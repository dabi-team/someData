2022

1

Adversarial Robustness of
Deep Neural Networks: A Survey
from a Formal VeriÔ¨Åcation Perspective

Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Zhe Hou,
Yan Xiao, Yun Lin, Jin Song Dong

2
2
0
2

t
c
O
1
1

]

R
C
.
s
c
[

2
v
7
2
2
2
1
.
6
0
2
2
:
v
i
X
r
a

Abstract‚ÄîNeural networks have been widely applied in security applications such as spam and phishing detection, intrusion
prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications.
Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for
trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the
reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in security and machine
learning. In this work, we survey existing literature in adversarial robustness veriÔ¨Åcation for neural networks and collect 39 diversiÔ¨Åed
research works across machine learning, security, and software engineering domains. We systematically analyze their approaches,
including how robustness is formulated, what veriÔ¨Åcation techniques are used, and the strengths and limitations of each technique. We
provide a taxonomy from a formal veriÔ¨Åcation perspective for a comprehensive understanding of this topic. We classify the existing
techniques based on property speciÔ¨Åcation, problem reduction, and reasoning strategies. We also demonstrate representative
techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.

Index Terms‚ÄîRobustness, veriÔ¨Åcation, security, adversarial machine learning, neural networks, deep learning.

(cid:70)

1 INTRODUCTION

D EEP learning is an artiÔ¨Åcial intelligence (AI) technique

that is regarded as one of the top technological break-
throughs in computer science [1]. It imitates the working
principle of the human brain in processing data and form-
ing knowledge patterns and produces promising results
for various tasks such as image classiÔ¨Åcation [2], speech
recognition [3], recommendation [4], and natural language
understanding [5]. Nowadays deep learning is increasingly
applied in many Ô¨Åelds where trustworthiness is critically
needed, such as cybersecurity [6], autonomous driving [7],
and the healthcare industry [8]. Particularly in cybersecurity,
deep learning has been used to perform intrusion detec-
tion [9], malware detection [10], phishing detection [11],
network trafÔ¨Åc analysis [12], etc. Neural networks, or more
speciÔ¨Åcally deep neural networks (DNNs), act as the key
technology for deep learning. Most neural networks are
designed to be deployed in practice where an arbitrary
input shall be accepted. This, however, brings a great un-
certainty in determining the behavior of a neural network
model even if it has been well-trained [13]. As a result,
the trustworthiness of neural networks has turned out to
be a new challenging topic in the research community,

‚Ä¢ M. H. Meng is with the National University of Singapore, and also with

the Institute for Infocomm Research, A*STAR, Singapore.
E-mail: menghs@i2r.a-star.edu.sg

S. G. Teo is with the Institute for Infocomm Research, A*STAR, Singapore.

‚Ä¢ G. Bai is with The University of Queensland, Australia.
‚Ä¢
‚Ä¢ Z. Hou is with the GrifÔ¨Åth University, Australia.
‚Ä¢ Y. Xiao, Y. Lin and J. S. Dong are with the National University of

Singapore.

‚Ä¢ G. Bai (Email: g.bai@uq.edu.au) and Y. Xiao (Email: dcsxan@nus.edu.sg)

are corresponding authors.

especially for those to be deployed in safety and security-
critical systems [14].

The genesis of neural network veriÔ¨Åcation and valida-
tion can be traced back to decades ago [15], in which the
veriÔ¨Åcation is referred to as correctness and the validation is
referred to as accuracy and efÔ¨Åciency. Adversarial examples
for neural network models were discovered by Szegedy
et al. [16] and later Goodfellow et al. [17] systematically
introduced an approach to intrude the robustness property
of a neural network to make it misclassify a tampered input
to a wrong result. Since then, neural networks are shown
vulnerable to adversarial attacks [18]; consequently, research
on the security of neural networks experienced explosive
growth. Many research projects study the attack and defense
of AI using adversarial training [19], [20], [21], [22]. As a
result, verifying the robustness of neural network models
against adversarial inputs becomes a strong demand in
machine learning research to earn trust from their users and
investors.

Thanks to a complete knowledge system of program
analysis that has been built since the last century, many tech-
niques designed for traditional program veriÔ¨Åcation, such as
constraint solving [23], reachability analysis [24], and abstract
interpretation [25], are adopted in reasoning neural network
models. On the other hand, researchers also attempt to
reduce the veriÔ¨Åcation to an optimization problem and
achieve uplifting progress by taking advantage of various
mathematics theories [26], [27], [28]. The progress made
in robustness veriÔ¨Åcation techniques not only increases in-
dustry practitioners‚Äô conÔ¨Ådence in the security of neural
network models but also stimulates the discussion of more
diverse property speciÔ¨Åcations in deep learning, such as

 
 
 
 
 
 
2

2022

fairness [29], monotonicity [30], and coverage [31].

An early survey under this theme [32] performs a
comparative study on various veriÔ¨Åcation methods, which
collects seven different approaches from the literature and
discusses the advantages as well as the shortcomings of each
approach. A recent survey [33] extensively studies 18 exist-
ing robustness veriÔ¨Åcation approaches and reproduces them
on a uniÔ¨Åed mathematical framework, therefore for the Ô¨Årst
time comprehensively sketching the benchmarking of vari-
ous veriÔ¨Åcation techniques. However, some latest progress
since 2019 has not been covered. Another survey [34] pro-
poses a taxonomy of robustness veriÔ¨Åcation in accordance
with existing robust training techniques and conducts a
uniÔ¨Åed evaluation toolbox for over 20 approaches. It takes
both deterministic and probabilistic veriÔ¨Åcation into account
and unveils the landscape of available techniques for ro-
bustness training and robustness veriÔ¨Åcation. Compared
with existing literature, this paper covers the state-of-the-art
robustness veriÔ¨Åcation techniques that have been published
in recent years. Moreover, we also propose a more compre-
hensive taxonomy to categorize the existing approaches and
analyze them from a formal veriÔ¨Åcation perspective, thereby
forming our insight into future research on this topic.

Contributions

In summary, our paper makes the following contributions:
‚Ä¢ This survey discusses adversarial robustness veriÔ¨Åcation
for deep neural networks from a perspective of formal
veriÔ¨Åcation including three key components: a property
formalization, a reduction framework, and available rea-
soning strategies. From this point of view, we show the
veriÔ¨Åcation methodology not only differs in reasoning
strategies but also varies with the author‚Äôs understand-
ing of neural network veriÔ¨Åcation as a problem.

‚Ä¢ We propose a taxonomy based on different components
under the formal veriÔ¨Åcation framework and accord-
ingly perform a classiÔ¨Åcation of the existing literature.
We establish the connection between different types of
approaches from the technical perspective and thereby
unveil the landscape as well as the latest progress. We
particularly analyze the strategy behind the implementa-
tion of diversiÔ¨Åed approaches and discuss their advan-
tages and shortcomings.

‚Ä¢ Considering many existing approaches are proposed for
speciÔ¨Åc use cases or experimental settings, it is hard
to directly analyze them in a comparative context. This
survey Ô¨Ålls the gap by assessing them on a more general
neural network model with a case study supported by
substantial visualization.

‚Ä¢ We suggest the future directions of deep neural network
veriÔ¨Åcation, including, improvement in completeness of
veriÔ¨Åcation, adoption of various types of complex mod-
els, leveraging optimization algorithms, and a generic
veriÔ¨Åcation framework.

Scope of This Survey

Considering the research on deep neural network property
veriÔ¨Åcation begins with the topic of adversarial robustness,
which is indeed the most discussed property so far, we
choose robustness veriÔ¨Åcation as the theme to carry out our

survey. In order to avoid information overload and present
this survey in an appropriate space, we only focus on
reviewing and analyzing research works that put robustness
veriÔ¨Åcation as the major contribution.

We collect the existing literature published in machine
learning, security, and software engineering domains in
recent years. We also resort to two sources to replenish the
literature. First, we take reference from the existing survey
papers on neural network veriÔ¨Åcation [32], [33], [34], and
thereby we have collected some representative approaches.
so that the representative approaches are not missed. Sec-
ond, we take them as the seeds and treat the counterparts
that have been cited and/or benchmarked in their papers as
the second source.

We also notice that most related studies simplify the
context of a deep neural network as a multilayered perceptron
(MLP) for classiÔ¨Åcation tasks that are equipped with three
typical activations (ReLU, sigmoid, and tanh), therefore we
only discuss robustness veriÔ¨Åcation of MLPs with three
activations applicable in this survey. To avoid ambiguity,
the term ‚Äúneural networks‚Äù is referred to as deep neural
networks with the assumption that it is in a multi-layer
architecture rather than a single neuron perceptron. We also
assume the robustness veriÔ¨Åcation is performed in a white-
box setting, which means the architecture and parameters of
the examined model are transparent to the veriÔ¨Åer.

As a result, we collect 39 research works that have
proposed an algorithm or a tool to verify the robustness
property, and we show them later in Table 1. For some
research works that do not discuss robustness veriÔ¨Åcation
based on a predeÔ¨Åned speciÔ¨Åcation, but from the perspec-
tive of robust training or Ô¨Ånding adversarial examples, we
only take their methodologies that apply to speciÔ¨Åcation-
based veriÔ¨Åcation into account.

2 BACKGROUND

This section introduces formal veriÔ¨Åcation and one of its
typical implementations called model checking, including its
key elements and processes. After that, we propose our
deÔ¨Ånition of property veriÔ¨Åcation by Ô¨Åtting the setting of
neural networks into the framework of formal veriÔ¨Åcation.

2.1 Formal VeriÔ¨Åcation

Formal veriÔ¨Åcation is deÔ¨Åned as a method to prove or
disprove the correctness of a speciÔ¨Åc software or hardware
system through formal methods of mathematics. As tradi-
tional testing is commonly used for Ô¨Ånding bugs through
an actual execution or simulation, formal veriÔ¨Åcation em-
phasizes proving the correctness in a modeling framework
referring to the examined system with regard to a certain
speciÔ¨Åcation or property ‚Äì and therefore formal veriÔ¨Åca-
tion requests an in-depth understanding of the system to
be examined, and usually comes with a higher cost than
traditional testing [35]. Model checking is a typical formal
veriÔ¨Åcation technique to systematically verify if a formally
deÔ¨Åned property holds in a Ô¨Ånite state model [36]. It is
designed to explore all possible system states exhaustively
and, in the end, provides a qualitative result for specifying
whether the target speciÔ¨Åcation holds in the system. In Fig. 1

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

3

property veriÔ¨Åcation of deep neural networks through the
lens of formal veriÔ¨Åcation. Here we treat a deep neural
network model as a function that maps an input to an
output, thereby we formalize the property speciÔ¨Åcation as
a logic formula involving both input and output. Given the
property speciÔ¨Åcation and available modeling framework
on hand, we can perform the veriÔ¨Åcation using diverse
reasoning strategies.

Let us take the adversarial robustness as an example.
Suppose we have a neural network ùëì
: ùëã ‚Üí ùëå that takes
an input ùë• and then outputs ùë¶ as the result of classiÔ¨Åcation.
With a limit of the maximum input perturbation Œîùë• given,
we assert the neural network is not robust throughout
the veriÔ¨Åcation if we observe an arbitrary (ùë•, ùë¶) from the
training set such that ùëì (ùë• + Œîùë•) ‚â† ùë¶; otherwise the robustness
is proved to be held in the given neural network.

Through the veriÔ¨Åcation above, we can know if a prop-
erty holds in the target model from the perspective of the
worst case. However, we gain no knowledge of the percent-
age of compromised inputs (that are successfully manipu-
lated by the attacker) within the entire input space, not even
to mention the analysis of possible approaches to improve.
For those reasons, quantitative reasoning is required to per-
form a more complex and in-depth property veriÔ¨Åcation for
neural network models. Counting the desirable outputs and
offering a probabilistic result is a typical step of quantitative
veriÔ¨Åcation that makes it differ from qualitative veriÔ¨Åcation.
For those properties with demographic characteristics, such
as fairness, quantitative reasoning could achieve better se-
mantic expressiveness than any qualitative approach. Since
we focus on robustness veriÔ¨Åcation, we assume the property
speciÔ¨Åcation is qualitatively deÔ¨Åned in this survey.

3 PROPERTY FORMALIZATION
The properties of a neural network could be captured from
the semantic context of its speciÔ¨Åcation. Most of the prop-
erties are input-output (IO) properties that request a speciÔ¨Åc
mapping relation between the input and output. Robustness
is one of the earliest IO properties that has been studied,
which requests the model‚Äôs output to be stable upon minor
modiÔ¨Åcations have been made to the input‚Äôs value [38],
[39]. In this section, we present our deÔ¨Ånition of adversarial
perturbations and provide a formalization of the robustness
property.

3.1 An Introduction to Perturbation

The perturbation of input can be categorized into two
types, namely global perturbation and regional perturbation,
as illustrated in Fig. 2 (middle). As its name implies, the
global perturbation is a vector of the same size as input
samples, and the adversarial input is obtained by overlaying
the perturbation on a benign input. The source of global
perturbation can be from normal distortion in actual deploy-
ments such as signal noise or weather conditions [?], or the
output of attack methods, such as projected gradient descent
(PGD) [20] or fast gradient sign method (FGSM) [17]. Regional
perturbation is usually a Ô¨Åxed pattern to be added to the
original input, which is also known as local perturbation
or ‚ÄúTrojan trigger‚Äù in an attack scenario. Due to the uncer-
tainty of the occurrence of the attack, veriÔ¨Åcation of security

Fig. 1. An overview of adversarial robustness veriÔ¨Åcation from a formal
veriÔ¨Åcation perspective

we present a classical schematic view of formal veriÔ¨Åcation
through model checking, which is composed of three key
components: a property formalization, a reduction framework,
and speciÔ¨Åcation reasoning.

A precise and unambiguous property speciÔ¨Åcation is
necessary for formal veriÔ¨Åcation. Property speciÔ¨Åcation is
a statement of property obtained through formalizing the
system requirements. It is often presented as a classical logic
formula to specify the relationship or restriction on different
system components or illustrated in a temporal logic formula
to enforce a rule over the system in terms of time. The reduc-
tion framework is a formal representation of the system to
be examined, which keeps all the key features and functions
to ensure a proper simulation of all possible states within
the system. To sum up, the property formalization deÔ¨Ånes
what the system is expected to do, and the system modeling
after reduction prescribes how the system behaves. Property
formalization and a proper reduction framework together
constitute the speciÔ¨Åcation phase of the veriÔ¨Åcation. These two
processes illustrate our understanding of system veriÔ¨Åcation
as a problem. Different understandings of a system may
result in different reasoning approaches available for us to
solve the problem.

With both system and property formalized during the
speciÔ¨Åcation phase, the veriÔ¨Åer performs the reasoning task
and outputs a result ‚Äì we call this stage the veriÔ¨Åcation phase.
A positive output means that the property speciÔ¨Åcation is
satisÔ¨Åed in the examined model. However, a negative out-
put sometimes could not be straightforwardly interpreted
as a violation of that property speciÔ¨Åcation, and therefore
an extra analysis is required. Since the reasoning usually
produces a counterexample together with a negative output,
we need to determine if the counterexample generated
through the simulation faithfully reÔ¨Çects the situation of the
actual system ‚Äì otherwise, it is concluded as a false negative
and another round of reasoning is needed after reÔ¨Åning the
reduction framework. Such a strategy to recursively reÔ¨Åne
the system speciÔ¨Åcation according to the counterexample
generated from the previous round is formally proposed
in [37] as the counterexample-guided abstraction reÔ¨Ånement
(CEGAR) approach.

2.2 Property VeriÔ¨Åcation of Deep Neural Networks

After introducing the basic concept of neural networks and
formal veriÔ¨Åcation theory, we can deÔ¨Åne the problem of

FormalizationReductionModelDeep Neural NetworkRequirementsProperty SpecificationVerificationResultReasoningAdversarial perturbationsRobustùëì()‚â†ùëì()ùëì()ùëì()=Not robustTested networkOutputInput1Input24

2022

3.3 Robustness against Adversarial Perturbations

Robustness describes the fault-tolerant capability of a com-
puter system to deal with erroneous inputs during execu-
tion. In machine learning, robustness is usually described
as the capacity of a model to consistently produce desirable
output given some perturbations (or distortions) have been
made on the input. As robustness is closely related to the
reliability and performance of a neural network model, it
is widely discussed in diverse branches of deep learning
applications [39].

A mutant input with perturbation that can alter the
model‚Äôs output is called an adversarial example ‚Äì therefore the
evaluation of the robustness of a deep neural network model
is to Ô¨Ånd the existence of such an adversarial example. In
analysis, robustness only makes sense if the scope of pertur-
bation is clearly deÔ¨Åned and the benign input is properly
speciÔ¨Åed. The veriÔ¨Åcation may not be able to truthfully
reÔ¨Çect the real robustness of the given neural network model
if the scope of perturbation is deÔ¨Åned negligibly (i.e., too
small). A too strong perturbation makes the adversarial
example sufÔ¨Åciently differs from the original input, in which
case it is reasonable for the model to classify the input as
a different output. As for the benign input, it could be an
image used for classiÔ¨Åcation or a vector of values within
a continuous domain. We then formalize the robustness
property of neural network models as follows:

DeÔ¨Ånition 1 (Robustness against adversarial perturbations).
Given a neural network model that maps a benign input ùë•ùëé to
the output ùë¶ùëé, and moreover produces an output ùë¶ùëè given an
adversarial input ùë•ùëè. Let (cid:107)¬∑(cid:107) ùëù be the function to calculate the ùêø ùëù-
norm distance. The robustness property Œ¶ against any adversarial
perturbation within the scope of Œîùë• is deÔ¨Åned as follows:

Œ¶(ùë•ùëé, ùë¶ùëé, ùë•ùëè, ùë¶ùëè, Œîùë•) def= (cid:0)(cid:107)ùë•ùëé ‚àí ùë•ùëè (cid:107) ùëù ‚â§ Œîùë• (cid:1) ‚Üí (ùë¶ùëé = ùë¶ùëè)

Other than the above deÔ¨Ånition that applies to the classi-
Ô¨Åcation of a discrete domain, the robustness property could
be further deÔ¨Åned with tolerant misclassiÔ¨Åcation or tolerant
misprediction [30], which is more practical in the case that the
output falls in a continuous domain (e.g., regression tasks)
or an aggregation of labels (e.g., top-K classiÔ¨Åcation). As this
paper focuses on the classiÔ¨Åcation scenario, we assume the
veriÔ¨Åcation does not tolerate errors in the output.

4 CORRECTNESS ASSESSMENT

From the view of program analysis, the objective of a static
checker is to Ô¨Ånd if a program prevents something that it
claims to avoid. The same idea also applies to the veriÔ¨Åca-
tion of neural network models. Intuitively, a positive ver-
iÔ¨Åcation outcome implies the neural network model could
always prevent violations of speciÔ¨Åc property if it claims
to do so and, on the contrary, a negative outcome claims
the neural network model fails to preserve the robustness
property. However, inconsistency between the claim and
the fact sometimes happens when the veriÔ¨Åcation is not
guaranteed to be sound and complete. This section discusses
soundness and completeness for a more comprehensive and
precise correctness assessment [41].

Fig. 2. Samples of regional (a) and global (b) adversarial perturbations
and measurement of magnitude, illustrated with attacks to the prediction
of the MNIST dataset

against Trojan attacks will be difÔ¨Åcult without knowing the
pattern, location, and size of triggers. Moreover, proving ro-
bustness against a speciÔ¨Åc Trojan pattern does not generalize
the property against another Trojan pattern even when both
patterns are on a similar scale. For those reasons, regional
perturbation is mainly studied in attack scenarios. We focus
on global perturbation in the remainder of this paper.

3.2 Perturbation Measurement

There are different ways to measure the magnitude (scope)
of input perturbations. In mathematics, a norm is a function
to map a vector‚Äôs distance to the origin into a non-negative
value. Here we treat input perturbations as vectors, and we
adopt the norm function to deÔ¨Åne the scope of perturbation,
and correspondingly calculate its magnitude.

Although both global and regional perturbations are
created with the same objective, which is to maximize the
chance of misclassiÔ¨Åcation meanwhile ensuring they are
visually indistinguishable, the measuring methods for dif-
ferent types of perturbations may differ. ùêø0 norm calculates
the number of input features that have been affected, i.e., the
size of perturbation, which is useful to describe the size of
a regional perturbation. ùêø1 norm (Manhattan distance) and
ùêø2 norm (Euclidean distance) measure the distance between
the benign and adversarial inputs, which are often used
to describe the accumulated inÔ¨Çuence of the perturbation.
ùêø‚àû norm of a perturbation records the greatest magnitude
among all elements of it as a vector. ùêø‚àû norm is widely used
in robustness veriÔ¨Åcation because it echoes the ùúñ value of
the FGSM [17], which regulates the scope of gradient as the
worst-case perturbation that leads to misclassiÔ¨Åcation. Fig. 2
demonstrates a sample perturbation in each type given
a benign input sample from the MNIST dataset [40] and
illustrates the value of different norm functions in deÔ¨Åning
the magnitude of input perturbation.

We highlight that the adversarial examples are not the
only source of input perturbations, which may also include
benign corruption (e.g., the inÔ¨Çuence of weather) and rea-
sonable distortion (e.g., signal noise, blur) [39]. However,
those cases are specially introduced in object recognition
applications and do not differ from the adversarial example
in the context of formal analysis. We assume adversarial
examples as the default violation of robustness in this work.

Benign image (28√ó28)Label: 1 (23.20%)Adversarial image #1Prediction: 7 (15.80%)Adversarial image #2Prediction: 8 (23.20%)Perturbation #1 (Regional)Perturbation #2 (Global)norm: 28norm: 19.6norm: 3.704norm: 0.7norm: 784norm: 196norm: 7norm: 0.25Benign image (28√ó28)Label: 1 (23.20%)++(a)(b)MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

5

Fig. 3. The cases of veriÔ¨Åcation outcomes classiÔ¨Åed in the statistical
hypothesis testing approach, illustrated with the classiÔ¨Åcation of the
CIFAR-10 dataset

4.1 Soundness and Completeness of the VeriÔ¨Åcation

A proof method is sound if every statement it can prove
is indeed true. It is complete if every true statement can
be proved. Given a neural network veriÔ¨Åer with something
undesired X that we wish to prevent. The veriÔ¨Åer is sound
if it never accepts X taking place in the neural network with
certain inputs. It is complete if it never rejects any input,
provided X does not happen in the neural network model.
Statistical hypothesis testing provides another good way
to understand soundness and completeness, where the hy-
pothesis represents the actual existence of an undesirable
event X deÔ¨Åned in the speciÔ¨Åcation of a neural network
model, and the decision indicates the actual outcome of the
veriÔ¨Åer [42]. A table of error speciÔ¨Åed for the veriÔ¨Åcation
is shown in Fig.3 with positive and negative hypothesis
columns being replaced by desirables and violations. With
such a framework being set up, we may understand the re-
lationship between soundness and completeness as follows:
‚Ä¢ A sound veriÔ¨Åcation prevents missed violations (false neg-
atives) but may tolerate false alarms (false positives) in
case the event X does not actually take place even if the
veriÔ¨Åer outputs a rejection. Simply put, the veriÔ¨Åcation
may report unsafe regarding the (possible) existence of
violation X, even though the system is indeed safe.

‚Ä¢ A complete veriÔ¨Åcation prevents false alarms (false posi-
tives). However, it may not be able to account for missed
violations (false negatives) ‚Äî which means there may
exist some violations not found by the veriÔ¨Åcation. The
veriÔ¨Åer may report the system is safe due to its failure to
Ô¨Ånd the violation X.

‚Ä¢ A sound and complete veriÔ¨Åcation implies a perfect pre-
diction, that only allows accepted desirables (true positives)
and caught violations (true negatives).

4.2 Trade-off in Practice

Verifying deep neural network models is shown as an NP-
hard problem in [23] and therefore it is challenging to imple-
ment veriÔ¨Åcation that is simultaneously sound, complete,
and able to terminate in a reasonable time.

In practice, one of those three requirements is usually
put aside to guarantee the other two characters. Some
approaches (e.g., [23], [43]) put aside termination to ensure
both soundness and completeness are preserved, which is
widely adopted in the early stage of neural network veriÔ¨Åca-
tion due to the size of neural networks is comparably small,
and the scale of veriÔ¨Åcation is limited. However, scalability
becomes a critical factor in the veriÔ¨Åcation of deeper and

Fig. 4. The sample neural network (ReLU activation) with weight and
bias parameters shown on edges and nodes respectively

more complicated neural networks. Considering that sound-
ness is more essential in program analysis (to prevent what
the system claims to prevent), recent veriÔ¨Åcation solutions
(e.g., [44], [45]) commonly choose to sacriÔ¨Åce completeness
to secure soundness and scalability.

Unlike the complete veriÔ¨Åcation that comes with an
exponential time complexity for the worst case, incom-
plete veriÔ¨Åers usually come with much better scalability
but risk the veriÔ¨Åcation from precision loss due to over-
approximation. Since the precision loss is accumulated layer
by layer, in the worst case, an incomplete veriÔ¨Åer may fail
to certify the robustness even though it terminates quickly.

4.3 Soundness of Floating-point Arithmetic

The soundness of Ô¨Çoating-point arithmetic is another factor
to determine the correctness of the veriÔ¨Åcation. Floating-
point is a commonly used data type in computing that
offers Ô¨Çexible demands in precision. It does not naturally
satisfy the axioms of real arithmetic, such as associativity
and distributivity [46]. For that reason, the Ô¨Çoating-point
soundness is a concern in verifying neural network models
on a larger scale. Unsound Ô¨Çoating-point arithmetic can
make the veriÔ¨Åcation suffer from false negatives. At the mo-
ment of writing this survey, only a few papers [44], [46], [47]
take Ô¨Çoating-point arithmetic into consideration. Since we
focus on studying robustness veriÔ¨Åcation methodology from
a formal veriÔ¨Åcation perspective, Ô¨Çoating-point arithmetic is
not included in our classiÔ¨Åcation criteria.

5 MODEL REDUCTION
This section discusses how to reduce the property veri-
Ô¨Åcation of a neural network model to another problem
with Ô¨Ånite states and an available solution. Such a reduc-
tion process is also known as modeling in model checking.
The veriÔ¨Åcation reduction reÔ¨Çects our understanding of the
veriÔ¨Åcation as a problem and meanwhile determines what
consequent approach or algorithm could be used to solve
it. As one of the three key processes deÔ¨Åned in Section 2.1,
reduction stands for a formal interpretation of the neural
network model to be examined.

We categorize the existing reduction methods into two
groups, namely optimization problem and reachability problem.
In case the veriÔ¨Åcation is treated as an optimization prob-
lem, we further classify the reduction of existing approaches
into SAT/LP encoding, MILP encoding, and QCQP/SDP en-
coding. For the second category, we also identify three sub-
classes from the existing works, namely layerwise propaga-
tion, abstract interpretation, and discretization. We present our

AcceptedAccepted Desirables(TRUE POSITIVE)Missed Violations(FALSE NEGATIVE)RejectedFalse Alarms(FALSE POSITIVE)Caught Violations(TRUENEGATIVE)Hypothesis (Truth)Decision (Test)Prediction: 8 (ship)CorrectPrediction: 0 (airplane)Wrongi.e., the model is robusti.e., the model is not robustDesirablesViolations(          +          )ùëìHidden Layer #1(w. ReLU Activation)Output LayerInput Layerùë•1ùë•2ùë•3ùë•4ùë•5ùë•6ùë•7ùë•8ùë•9ùë•10ùë•11ùë•12-10-1-1-11-1111111-0.5110-1Hidden Layer #2(w. ReLU Activation)6

2022

TABLE 1
A list of the surveyed works in neural network robustness veriÔ¨Åcation.

ClassiÔ¨Åcation‚Ä†

CONVEXADVERSARIAL, CONVDUAL* PyTorch
EXACTREACH*
MATLAB
SHERLOCK
DEEPVERIFY*, DUALITY*
ADVDEFENSE*, CERTIFY*
SDPVERIFY*
AI2

TensorFlow

TensorFlow

Not SpeciÔ¨Åed (C++)

Summary

#

Authorship,
Year & Reference

1 Pulina & Tacchella, 2010 [48]
2 Scheibler et al., 2015 [49]

3 Bastani et al., 2016 [50]

4 Cheng et al., 2017 [51]

Name of the Tool
& Link (if any)

NEVER

‚Äì
ILP*
MAXRESILIENCE*
PLANET

5 Ehler, 2017 [43]
6 Huang et al., 2017 [52]
7 Katz et al., 2017 [23]
RELUPLEX
8 Lomuscio & Maganti, 2017 [53] NSVERIFY*
9 Wong & Kolter, 2018 [26]

DLV

10 Xiang et al., 2017 [54]
11 Dutta et al., 2018 [55]

12 Dvijotham et al., 2018 [28]

13 Raghunathan et al., 2018 [27]

14 Raghunathan et al., 2018 [56]
15 Gehr et al., 2018 [25]
16 Singh et al., 2018 [46]
17 Wang et al., 2018 [47]
18 Wang et al., 2018 [57]
19 Weng et al., 2018 [58]

20 Xiang et al., 2018 [59]
21 Zhang et al., 2018 [60]
22 Bunel et al., 2018 [61]
23 Katz et al., 2019 [62]
24 Tjeng et al., 2019 [63]
25 Singh et al., 2019 [44]
26 Singh et al., 2019 [64]
27 Singh et al., 2019 [65]

DEEPZ

RELUVAL

NEURIFY

FAST-LIN, FAST-LIP
MAXSENSITIVITY*
CROWN

BAB, BABSB & RELUBAB

MARABOU

MIPVERIFY

DEEPPOLY

REFINEZONO

KPOLY, REFINEPOLY
CEGAR-MARABOU*
REACHNN

28 Yisrael et al., 2019 [45]
29 Huang et al., 2019 [66]
30 Xiang et al., 2020 [67]
31 Bak et al., 2020 [68]
32 Tran et al., 2019 & 2020 [69], [70] NNV

IGNNV

NNENUM

VENUS

33 Henriksen & Lomuscio, 2020 [71] VERINET
34 Botoeva et al., 2020 [72]
35 Dathathri et al., 2020 [73]
36 Tjandraatmadja et al., 2020 [74]
37 Fazlyab et al., 2020 [75]
38 Batten et al., 2021 [76]
39 Kouvaros & Lomuscio, 2021 [77] VENUS2¬ß

SDP-FO

DEEPSDP

FASTC2V, OPT2CV

LAYERSDP, FASTSDP

Supported
Framework

Shark (C++)

Not SpeciÔ¨Åed

Caffe

ConvNetJS

Caffe

Keras (Theano)

Not SpeciÔ¨Åed (C)

Not SpeciÔ¨Åed

TensorFlow & MATLAB

PyTorch & TensorFlow

Not SpeciÔ¨Åed (C, Python)

Not SpeciÔ¨Åed (C)

TensorFlow

TensorFlow

MATLAB

TensorFlow

PyTorch

TensorFlow

Julia

PyTorch & TensorFlow

PyTorch & TensorFlow

PyTorch & TensorFlow

Not SpeciÔ¨Åed

Not SpeciÔ¨Åc (Python)

MATLAB

Not SpeciÔ¨Åed (Python)

MATLAB

PyTorch

Keras (TensorFlow)

JAX

Not SpeciÔ¨Åed (C++)

MATLAB

Not SpeciÔ¨Åed

Not SpeciÔ¨Åed

Supported
Activation
ReLU Sigmoid Tanh SpeciÔ¨Åcation SAT MILP SDP LWP DI AI CR MO IA LA IR

Reasoning
Strategies

Reduction

Property

ùêø‚àû
IP
ùêø‚àû
ùêø

1
IP
1, ùêø
IP

2

ùêø

IP
ùêø‚àû
IP
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
2, ùêø‚àû
1, ùêø
2, ùêø‚àû
1, ùêø
ùêø‚àû
2, ùêø‚àû
1, ùêø
ùêø‚àû
IP
2, ùêø‚àû
1, ùêø
ùêø‚àû
ùêø‚àû
ùêø‚àû
1, ùêø‚àû
ùêø
IP

ùêø
ùêø

ùêø

ùêø

ùêø

IP

IP

IP
, ùêø‚àû
1
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû
ùêø‚àû

‚Ä† Property speciÔ¨Åcations include Ô¨Åxed input pattern (IP) or ùêø ùëù norms of adversarial perturbations. Reduction approaches are classiÔ¨Åed into Ô¨Åve
classes as shown in the table, including SAT/LP encoding (SAT), MILP encoding (MILP), QCQP/SDP encoding (SDP), layerwise propagation
(LWP), discretization (DI), and abstract interpretation (AI). Reasoning strategies are classiÔ¨Åed into Ô¨Åve classes as shown in the table, including
constraint reasoning (CR), mathematical optimization (MO), interval arithmetic (IA), linear approximation (LA), and iterative reÔ¨Ånement (IR).
¬ß The link is provided in the literature but not accessible as of May 2022.
* The name of the tool has not been explicitly deÔ¨Åned in the literature but is taken from the source code released, or commonly addressed in
subsequent research. The name is usually cited from the function name or the paper title.

classiÔ¨Åcation result over the collected literature in columns
9-14 of Table 1 and detail those reduction methods in the
remainder of this section.

To facilitate the analysis and discussion, we generate a
sample neural network model. As shown in Fig.4, the sam-
ple network is a fully-connected four-layer MLP with ReLU
activation, which is a typical neural network architecture
used for classiÔ¨Åcation tasks. To make it clear and consistent,
we call the basic elements within the sample neural network
as nodes. We accordingly separate each neuron within hid-
den layers into two nodes, which perform an afÔ¨Åne function
and activation function respectively. We create this sample
network based on the fact that the speciÔ¨Åc input [0, 0] would
always lead to the class corresponding to ùë•11. In addition,
we also claim factual robustness of the sample network when
dealing with ùêø‚àû perturbations that are less or equal to 1.0,
i.e., the value of ùë•11 is always greater than ùë•12 regardless of

any input perturbation within the range [‚àí1, 1].

5.1 Boolean SatisÔ¨Åability Problem (SAT) / Linear Pro-
gramming (LP) Encoding

Since a neural network model is analyzed in a white-box
setting and all the parameters are constants, it is intuitive
to perform the analysis by transforming the entire neural
network into a set of constraints in the form of propositional
logic and linking certain constraints containing the input
and output in one Boolean formula. We aim to prove the
non-existence of any violation in the examined model, and
thereby we can reduce the robustness veriÔ¨Åcation to a con-
straint satisfaction problem, or more speciÔ¨Åcally a Boolean
satisÔ¨Åability problem (colloquially known as SAT problem).
Alternatively, we can also set the negation of constraints
over the output nodes as the objective function and keep
the remaining constraints unchanged, thus the veriÔ¨Åcation

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

7

process is transformed into solving an optimization prob-
lem. In the case of all activation are piecewise-linear functions
(e.g., ReLU), this reduction approach is also known as linear
programming (LP) encoding.

As we are checking whether a neural network is robust,
the goal of reasoning over this encoding is to prove the
existence of such a solution that satisÔ¨Åes all the constraints,
rather than Ô¨Ånding the objective value. To this end, we sym-
bolize every node with a unique variable and then connect
those variables layer-by-layer according to the computa-
tional relation deÔ¨Åned in the network model. This reduction
approach (in SAT form) can be formalized as below.

DeÔ¨Ånition 2 (SAT/LP encoding). Given a neural network
ùëì with ùëõ nodes. To verify a property speciÔ¨Åcation of it,
model
ùëì can be formally reduced to a model M in form of a constraint
satisfaction problem, such as:

M(SAT/LP) = (cid:104)ùëâ, ùê∑, ùê∂(cid:105)
where ùëâ = {ùëâ1, . . . , ùëâùëõ} is a set of variables corresponding to the
nodes within ùëì ; ùê∑ = {ùê∑1, . . . , ùê∑ ùëõ} is a set specifying the value
domain (if any) of variables in ùëâ; and ùê∂ = {ùê∂1, . . . , ùê∂ùëö} is the set
of constraints, including the constraint reÔ¨Çecting the violation of
property speciÔ¨Åcation.

Fig.5 depicts the encoding of the sample network that
violates the robustness speciÔ¨Åcation, i.e., ùë•11 ‚â§ ùë•12. Because
ReLU is a piecewise-linear function that produces different
outputs from two input domains, we notice that each ReLU
activation function introduces a disjunctive logical (or) op-
eration, which bisects the veriÔ¨Åcation problem. As a result,
given ùëõ ReLU nodes in a neural network model, its SAT/LP
encoding creates a disjunction of 2
conjunctive normal
form (CNF) formulas (16 in the sample model) in total.

ùëõ

As we capture all nodes from the examined model and
truthfully encode both afÔ¨Åne and activation functions in
constraints, we consider this reduction approach preserves
both soundness and completeness for the next step of rea-
soning. Nevertheless, the nature of propositional logic is
only suitable to encode linear algebraic relations without ex-
ponential. It makes this reduction approach only applicable
to neural networks that use piecewise-linear activation, such
as ReLU or its variants; otherwise, the output of activation
must be relaxed by another linear or piecewise-linear func-
tion in advance.

5.2 Mixed-Integer Linear Programming (MILP) Encod-
ing

One signiÔ¨Åcant progress made on SAT/LP encoding is
achieved by adding an integer variable for each ReLU acti-
vation and thereby encodes the examined model to a mixed-
integer linear programming (MILP) problem. MILP is a variant
of LP that allows part of variables speciÔ¨Åed as integers.
Some recent papers [51], [53], [55], [63], [78] take advantage
of the integer setting of MILP, propose a subtle encoding
of the ReLU function into a set of four in-equations, and
thus eliminate the excessive disjunctive formulas in the
constraints.

We show how a canonical

integer setting could be
adopted in ReLU activation in DeÔ¨Ånition 3 and present
our formalization of MILP encoding in DeÔ¨Ånition 4. We

ùëâ = {ùë•1, ùë•2, ..., ùë•12 } , ùê∑ = {‚àí1 ‚â§ ùë•1 ‚â§ 1, ‚àí1 ‚â§ ùë•2 ‚â§ 1}
ùê∂ = {ùë•3 = ùë•1 + ùë•2, ùë•4 = ùë•1 ‚àí ùë•2 ‚àí 1,
( ( ùë•3 ‚â§ 0) ‚àß ( ùë•5 = 0)) ‚à® ( ( ùë•3 > 0) ‚àß ( ùë•5 = ùë•3)) ,
( ( ùë•4 ‚â§ 0) ‚àß ( ùë•6 = 0)) ‚à® ( ( ùë•4 > 0) ‚àß ( ùë•6 = ùë•4)) ,
ùë•7 = ‚àíùë•5 + ùë•6 ‚àí 1, ùë•8 = ùë•5 ‚àí ùë•6 ‚àí 1,
( ( ùë•7 ‚â§ 0) ‚àß ( ùë•9 = 0)) ‚à® ( ( ùë•7 > 0) ‚àß ( ùë•9 = ùë•7)) ,
( ( ùë•8 ‚â§ 0) ‚àß ( ùë•10 = 0)) ‚à® ( ( ùë•8 > 0) ‚àß ( ùë•10 = ùë•8)) ,

ùë•11 = ùë•9 ‚àí ùë•10 + 1, ùë•12 = ùë•10 ‚àí 0.5, ùë•11 ‚â§ ùë•12 }

Fig. 5. Boolean formula obtained through SAT encoding of the sample
network given the robustness speciÔ¨Åcation being violated

then demonstrate the corresponding encoding of the sample
network in Fig.6.

DeÔ¨Ånition 3 (Big-M encoding of ReLU). Consider given an
integer variable ùëë ‚àà {0, 1} and another sufÔ¨Åciently great constant
ùëÄ (which is guaranteed to be greater than the upper bound of
ReLU input), a ReLU function ùë¶ = ùëöùëéùë• (0, ùë•) could be encoded
by a CNF of four logical formulas, as shown below:

ùë¶ = ùëöùëéùë• (0, ùë•) ‚áî ((ùë¶ ‚â• ùë•) ‚àß (ùë¶ ‚â§ ùë• + ùëÄ ¬∑ ùëë)

‚àß (ùë¶ ‚â• 0) ‚àß (ùë¶ ‚â§ ùëÄ ¬∑ (1 ‚àí ùëë)))

where the case ùëë = 0 represents the ReLU is in active mode (ùë• > 0
and ùë¶ = ùë•), and conversely, ùëë = 1 indicates the ReLU is inactive
(ùë• < 0 and ùë¶ = 0).

DeÔ¨Ånition 4 (MILP encoding). Given a neural network model
ùëì with ùëõ nodes. To verify a property of it,
ùëì can be formally
modeled as a MILP problem presented as follows:
M(MILP) = (cid:10)ùëâ, ùëâùëñùëõùë° , ùê∑, ùê∂, Obj(cid:11)
where ùëâ = {ùëâ1, . . . , ùëâùëõ} is a set of variables corresponding to
the nodes within ùëì ; ùëâùëñùëõùë° is the set of integers that correspondingly
deÔ¨Åned to constraint ReLU activation outputs; ùê∑ = {ùê∑1, . . . , ùê∑ ùëõ}
is a set specifying the value domain (if any) of variables in ùëâ;
ùê∂ = {ùê∂1, . . . , ùê∂ùëö} is the set of constraints; and Obj is the objective
(minimum) inequation looking for the counterexample in case of
violation (i.e., Obj ‚â§ 0) occurs.

It

is worth noting that some existing approaches
(e.g., [27], [73], [76], etc) initiate their analysis from an LP
or MILP perspective and later transform the neural network
encoding into a semideÔ¨Ånite programming (SDP) problem.
We separate this encoding approach as an independent
category and detail it in Section 5.3.

The MILP encoding is proposed as a successor of
SAT/LP encoding. It retains both soundness and complete-
ness at the reduction stage. Moreover, it addresses our
concern about the ‚Äútoo many branches‚Äù issue in SAT/LP
encoding, especially when handling larger or deeper neu-
ral networks, and eventually brings potential improvement
regarding the scalability in neural network veriÔ¨Åcation.

5.3 Quadratically Constrained Quadratic Program
(QCQP) / SemideÔ¨Ånite Programming (SDP) Encoding

A quadratically constrained quadratic program (QCQP) is
an optimization problem in which both the objective func-
tion and the constraints are quadratic functions. In fact,
both LP and MILP encoding can be treated as variants of
QCQP where the quadratic objective function and quadratic

8

2022

constraints are absent. Solving a QCQP is NP-hard, too.
However, the problem can be solved by semideÔ¨Ånite pro-
gramming (SDP) if it is convex. For that reason, existing
studies categorized in this type are also known as SDP-
based approaches [33], [56], [73]. They mainly transform
the ReLU activation to convex quadratic equations, thereby
the veriÔ¨Åcation can be reduced to a convex QCQP after
including the linear constraints (i.e., afÔ¨Åne functions in the
neural network). The quadratic encoding of ReLU can be
deÔ¨Åned as follows.

DeÔ¨Ånition 5 (QCQP encoding of ReLU). Given a ReLU
function ùë¶ = ùëöùëéùë• (0, ùë•), we can encode it to a set of quadratic
constraints, as shown below:

ùë¶ = ùëöùëéùë• (0, ùë•) ‚áî ((ùë¶ ‚â• ùë•) ‚àß (ùë¶ ‚â• 0) ‚àß (ùë¶ (ùë¶ ‚àí ùë•) = 0))

Compared with LP and MILP encodings, the QCQP
encoding enables us to analyze quadratic relations of sym-
bolic (i.e.,
input and output of nodes) within a neural
network model. By applying the convex encoding for ReLU
activation, the QCQP can be solved with soundness and
completeness guarantee using off-the-shelf interior point
methods. The major concern with this type of encoding
comes from scalability. Interior point methods are shown
to be computationally expensive and could take up to an
ùëÇ (ùëõ6) complexity for a neural network model containing ùëõ
hidden units [73].

Existing papers that adopt QCQP encoding commonly
mitigate the scalability issue in two directions. They apply
various mathematical optimization to improve solving efÔ¨Å-
ciency and/or take advantage of the linear approximation
to further relax the original QCQP. Particularly, recent work
exploits linear approximation to extend the veriÔ¨Åcation with
a broader range of activation functions. Through the linear
approximation (to be detailed in Section 6.4), both tanh and
sigmoid functions can be approximately formulated as a set
of quadratic equations.

5.4 Layerwise Propagation

The layerwise propagation faithfully and precisely simu-
lates the computation of a neural network model to solve
the veriÔ¨Åcation as a reachability problem. It is intuitive and
has been adopted in some early approaches [49], [54].

Given the property speciÔ¨Åcation, we Ô¨Årst identify the
domain of concerned input (i.e., all the possible adversarial
inputs). Then we simulate the calculation based on those in-
puts until reaching the output layer. In the end, we compare
the propagated outputs with the domain of expected output
to determine if the property preserves in the target model.
We visualize the layerwise propagation process of verifying
the sample model in Fig. 7-(a).

The layerwise propagation at the reduction stage guar-
antees both soundness and completeness as it in fact does
not explicitly alter the neural network as the body to be
veriÔ¨Åed. The overall completeness and soundness are fully
determined by the following reasoning strategies, which we
will discuss in Section 6. Layerwise propagation usually
works well on deep models, however, suffers while dealing
with thick ones (i.e., lots of neurons per layer). Thus, the
reasoning based on the layerwise propagation reduction

minimize ùë•9 ‚àí 2 √ó ùë•10 + 1.5

s.t. ùë•1 + ùë•2 ‚â§ ùë•5 ‚â§ ùë•1 + ùë•2 + ùëÄ ¬∑ ùëë3,

ùë•5 ‚â§ ùëÄ ¬∑ (1 ‚àí ùëë3) ,
ùë•1 ‚àí ùë•2 ‚àí 1 ‚â§ ùë•6 ‚â§ ùë•1 ‚àí ùë•2 ‚àí 1 + ùëÄ ¬∑ ùëë4,
ùë•6 ‚â§ ùëÄ ¬∑ (1 ‚àí ùëë4) ,
ùë•5 + ùë•6 ‚àí 1 ‚â§ ùë•9 ‚â§ ùë•5 + ùë•6 ‚àí 1 + ùëÄ ¬∑ ùëë7,
ùë•9 ‚â§ ùëÄ ¬∑ (1 ‚àí ùëë7) ,
ùë•5 ‚àí ùë•6 ‚àí 1 ‚â§ ùë•10 ‚â§ ùë•5 ‚àí ùë•6 ‚àí 1 + ùëÄ ¬∑ ùëë8,
ùë•10 ‚â§ ùëÄ ¬∑ (1 ‚àí ùëë8) ,
‚àí 1 ‚â§ ùë•1 ‚â§ 1, ‚àí1 ‚â§ ùë•2 ‚â§ 1,
ùëë3, ùëë4, ùëë7, ùëë8 ‚àà {0, 1} , ùë•5, ùë•6, ùë•9, ùë•10 ‚â• 0,
ùëÄ > ùë•ùëñ ,

(1 ‚â§ ùëñ ‚â§ 12)

Fig. 6. MILP encoding of the sample network after equations simplifying,
with the objective expected to be positive

without any optimization or trick would be computationally
expensive, therefore leaving concern of scalability.

5.5 Discretization

The concern of scalability of layerwise propagation exists
since the early stage of neural network veriÔ¨Åcation. One
possible approach to address such a concern is to mitigate
the computational burden by discretization. Discretization is
a concept in applied mathematics that describes the process
of converting a continuous function into several discrete
pieces, which aims to achieve a Ô¨Ånite exploration within an
inÔ¨Ånite search space. The discretization has been adopted in
a veriÔ¨Åcation approach named DLV [52], which proposes to
discretize the perturbation from the inÔ¨Ånite input distribu-
tion. It propagates those discrete input vectors layer by layer
and determines whether the robustness is preserved or not
by observing the bounds of output. Fig. 7-(b) demonstrates
the veriÔ¨Åcation of the sample model through a discretiza-
tion.

Although the architecture of the examined model re-
mains unchanged, the domain of output completely de-
pends on the discretized input domain. Unless the scale of
input discretization has been set large enough to sufÔ¨Åciently
depicts all the key points in the output domain, such as the
convex hull of a hidden layer, there always exists a risk of
precision loss and consequently leads to missed violation
(false negative). Overall, discretization greatly simpliÔ¨Åes
the analysis, especially for a deep neural network that
takes comparably lower dimension inputs. However, as a
reduction process, it does not come with a guarantee of
completeness. The guarantee of soundness is challenging,
too, as it is mutually restricted by the granularity of dis-
cretization and the convexity of the computation at each
layer. However, many neural networks, particularly those
that use sigmoid or tanh activation, are not convex. For these
reasons, this reduction approach may suffer from the state-
space explosion issue and therefore does not scale to larger
neural network models [25], [47].

5.6 Abstract Interpretation

Abstract interpretation is a veriÔ¨Åcation approach that pro-
vides a formalism of approximation and abstraction in a
mathematical setting, which has been adopted in [25], [45],
[46] and their extensional works. An abstract interpretation

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

9

Fig. 7. Visualization of different reduction approaches in verifying robustness property of the sample model (abstraction interpretation with box
domain and reasoned by interval arithmetic in sub-Ô¨Ågure (a); discretization in sub-Ô¨Ågure (b) and forward reachability analysis in sub-Ô¨Ågure (c), with
red color area indicating false positive produced)

consists of an abstract domain, a pair of abstraction and con-
cretization functions, and a sound abstract semantic function in
form of an iterative Ô¨Åxed-point equation.

Abstract interpretation aims to simplify the veriÔ¨Åcation
problem at the reduction stage. Although the veriÔ¨Åcation is
still treated as a reachability problem, it is solved by reason-
ing an abstract system rather than the original neural net-
work model. The mainstream abstraction interpretation ap-
proaches symbolize each node of the neural network, apply
abstraction function over its computation, and ultimately
determine whether the property is preserved through the
output values after the concretization. Besides that, [45]
explores another direction of the abstract interpretation that
simpliÔ¨Åes the architecture of the examined model, rather
than approximating the output of neurons inside.

In [45], each hidden layer has been simpliÔ¨Åed to contain
at most four fully-connected neurons, literally one neuron
with the maximum positive weight and tends to bring
increment to the output written as ùëé [+,‚Üë], one neuron with
the minimize negative weight and tends to bring increment
to the output ùëé [‚àí,‚Üë], one neuron with the maximum positive
weight and tends to bring decrement to the output ùëé [+,‚Üì],
and lastly one neuron with the minimum negative weight
and tends to bring decrement to the output ùëé [‚àí,‚Üì]. Thus, both
upper and lower bounds of the output can be eventually ob-
tained by analyzing the abstract model. This approach gains
a scalability advantage in verifying neural networks that are
not very deep but built with a great number of hidden units
per layer. On the other hand, simplifying neural network
architecture through clustering the neurons‚Äô output does not
work well on models that adopt convergence functions as
activation, such as sigmoid and tanh (because they always
produce output within a certain range).

At the time of writing, DEEPPOLY [44] and its exten-
sional approaches are the most comprehensive, precise,
and practical solutions to perform veriÔ¨Åcation by abstract

interpretation. In the scheme of DEEPPOLY, each node is
transformed into an abstract element. An abstract element
is a tuple constituted by two polyhedral constraints and two
associate constants. The polyhedral constraints of a node are
made up of variables corresponding to those nodes con-
nected from the previous layer, and the associate constants
deÔ¨Åne its approximated upper and lower bounds after the
concretization. Take the sample network as an instance, the
robustness veriÔ¨Åcation is reduced to determine if, in its
concretized abstraction, the upper bound of ùë•12 is less than
the lower bound of ùë•11, which means ùë•11 > ùë•12 is always
true. We take [44] as an example and deÔ¨Åne the abstract
interpretation reduction as follows.

DeÔ¨Ånition 6 (Abstract interpretation). Given a neural network
model ùëì with ùëõ nodes. By deÔ¨Åning a polyhedral abstract domain
and approximating all the nodes accordingly, the veriÔ¨Åcation of ùëì
can be achieved by concretizing its abstraction ùëìùëé, written as:

M(Abst.) = ùëìùëé : (cid:8)‚àÄùëñ ‚àà [ùëõ] . ùëéùëñ = (cid:10)ùëé ‚â§
ùëñ and ùëé ‚â•

where ùëé ‚â§
ùëñ are polyhedral constraints of the abstract element
corresponding to the ùëñ-th node, and ùë¢ùëñ and ùëôùëñ are associated con-
stants that reÔ¨Çect the concretization of the ùëñ-th node‚Äôs abstraction.

ùëñ , ùëé ‚â•

ùëñ , ùë¢ùëñ, ùëôùëñ (cid:11)(cid:9)

In Fig. 7-(c), we demonstrate the veriÔ¨Åcation of the
sample model by applying a box domain (each node is
abstracted to a pair of lower and upper bounds) in the
abstract interpretation. As we can observe from the out-
put layer in the Ô¨Ågure, the veriÔ¨Åcation fails to determine
whether the property holds due to a too coarse approxi-
mation (the tiny red area of the output domain violates the
property speciÔ¨Åcation). That shows the given property spec-
iÔ¨Åcation could not be veriÔ¨Åed in the sample model through
this reduction approach, although it holds in the reality.
Throughout the history of applying abstract interpretation
in neural network veriÔ¨Åcation, the abstract domain and

OUTPUT LAYERINPUT LAYER-10ùë•1ùë•2ùë•3ùë•4ùë•5ùë•6ùë•7ùë•8-1-1-11-111111ùë•9ùë•10ùë•11ùë•121-0.5110-1(a)(b)(c)10

2022

corresponding abstraction functions have experienced evo-
lution towards a more precise approximation of the neural
network‚Äôs output, aiming to enhance the effectiveness of the
veriÔ¨Åcation through this reduction. Fortunately, the same
property can be successfully veriÔ¨Åed by later works such
as DEEPPOLY [44], which we will discuss in the next section
and visualize in Fig. 13.

Overall, abstract interpretation reduction enables ana-
lyzing the examined model‚Äôs behaviors in a less precise
but more efÔ¨Åcient way. Compared with the discretization
that performs analysis merely based on discrete values
of the input, abstract interpretation has an advantage in
soundness guarantee as it ensures that all possible values
of a given node are within the bounds obtained from over-
approximation.

6 SPECIFICATION REASONING

SpeciÔ¨Åcation reasoning discusses in detail which tool or
algorithm to be adopted in performing the veriÔ¨Åcation. The
choice of reasoning strategy closely relates to the reduction
approach applied to the examined model and determines
the overall veriÔ¨Åcation performance. In this section, we
classify available reasoning strategies into Ô¨Åve classes and
present the result in columns 15-19 of Table 1. With various
reduction approaches deÔ¨Åned in advance, we establish the
connection between each reasoning method and its applica-
ble reduction setting. Here we remark that one veriÔ¨Åcation
approach may adopt multiple speciÔ¨Åcation reasoning strate-
gies. Next, we detail each reasoning strategy and discuss its
advantages and shortcomings.

6.1 Constraint Reasoning

Constraint reasoning is an analysis approach implemented
by an off-the-shelf solver, and therefore it is also known
as solver reasoning. This reasoning option is designed to
take a set of constraints as input but does not restrict any
reduction setting applied in advance. Reasoning through
a satisÔ¨Åability modulo theories (SMT) solver is the most
intuitive way to perform veriÔ¨Åcation of a neural network in
SAT encoding. An SMT solver takes the SAT encoding of the
examined model as input and heuristically searches for the
best strategy to Ô¨Ånd a solution that satisÔ¨Åes all constraints.

Using an SMT solver to verify a large and deep neural
network containing hundreds or even thousands of neurons
is not an efÔ¨Åcient approach. With the worst case of expo-
nential time complexity, reasoning by an SMT solver usually
cannot guarantee to terminate within a reasonable time.
The high cyclomatic complexity constitutes another challenge.
As each ReLU node generates a disjunction due to its
piecewise-linear nature, reasoning a deep neural network
with ùëõ ReLU neurons in hidden layers could produce 2
sub-
problems. Due to the lack of a polynomial-time algorithm
for solving a CNF formula, constraint reasoning faces a
critical challenge of scalability. Dedicated tools using SMT
solvers to solve CNF encoding are shown only able to
handle small models containing 10-20 neurons in hidden
layers [79].

ùëõ

One mitigation is replacing the SMT solver with an
LP solver, which offers efÔ¨Åciency improvement in solving

although sharing the same worst case of time complexity.
For example, RELUPLEX [23] adopts an LP solver with
an in-house implementation of the simplex algorithm in
Ô¨Ånding the feasibility of constraints. The simplex algorithm
is known to be nondeterministic polynomial (NP) in time
complexity but a remarkably effective way to solve an
LP problem by a series of update and pivot operations. In
RELUPLEX, a ReLU function is treated as a special equation.
The value of ReLU output will be checked in each update
operation. To avoid the inÔ¨Ånite pivoting on variables within
one ReLU function (either its input or output), RELUPLEX
also adopts a branch-and-conquer strategy to split the origi-
nal problem into two sub-problems, with each sub-problem
corresponding to the active or inactive mode of that ReLU
node. The experiment of [23] demonstrates a successful
veriÔ¨Åcation of 10 properties on the ACAS Xu1 system. Owe
to splitting heuristics, RELUPLEX could successfully verify
neural network models with up to 300 ReLU nodes. It for
the Ô¨Årst time marks robustness veriÔ¨Åcation practical in actual
applications of deep neural networks. The latest progress of
SMT/LP solver-based veriÔ¨Åcation is MARABOU [62], which
brings a signiÔ¨Åcant improvement over its former release
RELUPLEX by introducing a symbolic tightening algorithm
and integrating an improved simplex algorithm.

Using a solver over the MILP encoding is another mitiga-
tion to address the scalability concern because it resolves the
‚Äúbranch explosion‚Äù issue in the SAT/LP encoding. Gurobi
is the most used solver to reason the MILP encoding (see
Table 2). Through the survey, we Ô¨Ånd [51], [53] might be the
genesis of encoding a neural network into a MILP problem
and reason by a solver. Later endeavors such as [55], [63],
[78] also make use of this set of encoding and reasoning
strategies to apply different solvers and attempt to support
more types of neural network models. According to the
benchmarking performed in [55], reasoning property speci-
Ô¨Åcation as a MILP problem brings signiÔ¨Åcant improvement
in time performance. It allows us to analyze larger models
containing up to 6,000 neurons.

Reasoning through a solver comes with an advan-
tage that natively guarantees soundness and completeness,
based on the premise that the solver is sound and complete.
However,
it faces scalability issues to a certain degree.
Considering the desirable sound and complete veriÔ¨Åca-
tion can only be achieved if both modeling and reasoning
approaches are sound and complete ‚Äì at this moment, a
solver-based veriÔ¨Åcation, especially the MILP solver-based
veriÔ¨Åcation, is still the best choice to verify a deep neural
network with a guarantee of completeness.

6.2 Mathematical Optimization

With the LP, MILP and SDP encoding selected, the veriÔ¨Å-
cation of a neural network could also be achieved through
various mathematical optimizations. Instead of generic pro-
gramming (e.g., Python or C) with a speciÔ¨Åc constraint
solver, approaches in this category are usually implemented
based on off-the-shelf mathematical programming and nu-
merical analysis platforms such as MATLAB.

1. ACAS stands for Airborne Collision Avoidance System, where
ACAS Xu is an experimental variant designed for the Remotely Piloted
Aircraft Systems (RPAS).

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

11

TABLE 2
List of research works that disclose solver usage for the reasoning

Authorship, Year & Reference
Solver(s)
HySAT
Pulina & Tacchella, 2010 (NEVER) [48]
Scheibler et al., 2015 [49]
iSAT
Cheng et al., 2017 (MAXRESILIENCE) [51]
CPLEX
miniSat, GLPK
Ehler, 2017 (PLANET) [43]
Huang et al., 2017 (DLV) [52]
Z3
Katz et al., 2017 (RELUPLEX) [23]
GLPK
Lomuscio & Maganti, 2017 (NSVERIFY) [53] Gurobi
Dutta et al., 2018 (SHERLOCK) [55]
Gurobi
Wang et al., 2018 (NEURIFY) [57]
Gurobi
Weng et al., 2018 (FAST-LIN) [58]
Gurobi
Bunel et al., 2018 (BAB, etc.) [61]
Gurobi
Katz et al., 2019 (MARABOU) [62]
GLPK
Tjeng et al., 2019 (MIPVERIFY) [63]
Cbc, CPLEX, Gurobi
Singh et al., 2019 (REFINEZONO) [64]
Gurobi
Singh et al., 2019 (KPOLY) [65]
Gurobi
Botoeva et al., 2020 (VENUS) [72]
Gurobi
Bak et al., 2020 (NNENUM) [68]
GLPK
Henriksen & Lomuscio, 2020 (VERINET) [71] Gurobi

* Approaches that claim their usage of solvers but do not disclose the
names are not listed.

One typical technique in this type is duality, which
theoretically enables us to certify the robustness property
by proving if the dual problem is solvable, although its
original form is not practically easy to be solved. Wong and
Kolter [26] proposed the Ô¨Årst work that falls in this class. It
illustrates the distortion over benign input as an adversarial
polytope and applies a MILP encoding for the examined
model. After that, it encodes the property violation into
the dual formulation of the LP problem that corresponds to
the original neural network model, and then applies convex
outer approximation (also known as relaxation) in solving the
dual problem. According to the duality theory, the original
problem (also known as primal) can be guaranteed with a
lower bound of the solution if its dual problem is found fea-
sible to be solved. Moreover, when the number of variables
blasts with the growth of the size of neural networks, the
dual formulation becomes comparably easier to be solved
than its primal. As a result, it provides a novel direction
to make the intractable veriÔ¨Åcation problem feasible to be
solved through mathematical optimization.

Duality of LP/MILP only supports neural networks
model with ReLU activation functions. Additionally,
in
case more non-piecewise-linear activation functions are sup-
ported or the examined neural network layers grow larger in
size, the veriÔ¨Åcation could suffer from false positives when
the property fails to be certiÔ¨Åed ‚Äì which owes to the gap
between the dual and primal problems.

There is another approach proposed by Raghunathan
et al. [27] that also adopts a similar idea of solving the
dual of the original problem. In that work, the encoding of
neural networks is presented as an SDP that is composed
of quadratic constraints. Besides that, it presents the dual
problem in the form of Lagrange dual function and uses
gradient descent to solve the optimization problem. Since
all mainstream activation functions are differentiable, this
approach supports a wide range of activation functions,
including sigmoid and ReLU. Nevertheless, that work does
not show good scalability, too. It only performs well on a
fully-connected MLP with up to two layers. Recent work

that applies a similar reasoning strategy to obtain the bound
of the SDP problem includes [56], [73], [75], [76]. Another
work by Dvijotham et al. [28] takes the limitation of previous
research into account and proposes a general dual approach
to support a broader class of neural networks. Compared
with [26], [27], it claims to obtain non-trivial robustness
bounds without the need for bound optimization during
adversarial training. However, the quality of the robustness
bound is traded off for the enhancement in scalability.

Reasoning by mathematical optimization is essentially
introduced as an optimization technique in robust training.
Unlike verifying a deep neural network model with a prede-
Ô¨Åned property speciÔ¨Åcation, this reasoning approach is more
effective in Ô¨Ånding the maximum robustness of a neural
network model and helping in further neural network op-
timization. Reasoning by mathematical optimization faces
challenges from both completeness and scalability. [75]
improves veriÔ¨Åcation scalability, however, only for neural
networks with up to Ô¨Åve layers. The disadvantage in scala-
bility makes mathematical optimization less solely applied
afterward. A recent approach [76] integrates mathematical
optimization with linear approximation (to be detailed in
Section 6.4) to enhance the overall efÔ¨Åciency, which presents
a promising direction for future research.

6.3 Interval Arithmetic

Interval arithmetic, also known as interval analysis or
bound propagation, is essentially developed as a classical
method to measure the bounds of errors in mathematical
computation [80]. It can also be used as an ideal method to
evaluate the bounds of neural network output. To the extent
of robustness speciÔ¨Åcation reasoning, the preservation of
robustness can be soundly certiÔ¨Åed once we establish the
bounds of its output, which are propagated from the input
perturbations, are within the acceptable range (i.e., the ex-
amined model does not misbehave).

Interval arithmetic has been adopted in early works [43],
[51] as scalability mitigation of SAT/LP solver-based veri-
Ô¨Åcation. By symbolizing all nodes in the examined neural
network model, each node can be represented as a compo-
sition of variables at its previous layer. Given predeÔ¨Åned
bounds of the input, the output of the neural network can
be evaluated through a layer-by-layer analysis. Considering
both afÔ¨Åne and ReLU functions are made up of simple arith-
metic operations, we can follow the basic rules of interval
operation to generate bounds for them [81].

Let ùëì

: R[ùêΩ ] ‚Üí R be the afÔ¨Åne function of a neuron
within hidden layers that executes the assignment ùë•ùëñ ‚Üê
(cid:0)ùë§ ùëó ¬∑ ùëé ùëó (cid:1), where ùë•ùëñ has ùêΩ neurons in the previous
ùëè + (cid:205) ùëó ‚àà [ùêΩ ]
layer connected to it, and ùëé ùëó stands for the output of the ùëó-th
neuron on the previous layer. Each ùëé ùëó has a pair of the lower
and upper bounds written as ùëé ‚â§
ùëó . The interval of ùë•ùëñ,
represented by (cid:2)ùë• ‚â§

ùëó and ùëé ‚â•
(cid:3), can be obtained as follows:

ùëñ , ùë• ‚â•
ùëñ

ùë• ‚â§
ùëñ = ùëè +

ùë• ‚â•
ùëñ = ùëè +

‚àëÔ∏Å

(cid:16)

ùëó ‚àà [ùêΩ ]

‚àëÔ∏Å

(cid:16)

ùëó ‚àà [ùêΩ ]

ùëöùëéùë•(0, ùë§ ùëó ) ¬∑ ùëé ‚â§

ùëó + ùëöùëñùëõ(0, ùë§ ùëó ) ¬∑ ùëé ‚â•
ùëó

ùëöùëéùë•(0, ùë§ ùëó ) ¬∑ ùëé ‚â•

ùëó + ùëöùëñùëõ(0, ùë§ ùëó ) ¬∑ ùëé ‚â§
ùëó

(cid:17)

(cid:17)

12

2022

Similarly, let ùëì : R ‚Üí R be the ReLU activation function
that executes the assignment ùëéùëñ ‚Üê ùëöùëéùë•(0, ùë•ùëñ), where ùë•ùëñ is
the output of an afÔ¨Åne function ahead and the input of the
activation function. The bounds of ùëéùëñ can be obtained as
follows:

ùëñ = ùëöùëéùë•(0, ùë• ‚â§
ùëé ‚â§

ùëñ ) , ùëé ‚â•

ùëñ = ùëöùëéùë•(0, ùë• ‚â•
ùëñ )

We remark that interval arithmetic does not take sym-
bolic dependency into account so it tends to generate a
larger interval for a deep neural network when the number
of layers increases. This phenomenon is also reÔ¨Çected in
Fig. 7-(c) that the interval obtained at the output layer spans
over the boundary of the objective (shown as the red color
region), thus it fails to verify the robustness preservation.
For that reason, even though interval arithmetic is simple
and fast in computation, it can only provide a coarse evalu-
ation of the examined neural network‚Äôs output.

To mitigate the issue caused by too coarse evaluation, an
iterative reÔ¨Ånement (to be detailed in Section 6.5) sometimes
is necessary to maintain the minimum bounds of neuron
output in practice. For example, we can keep the symboliza-
tion along with layerwise propagation and then simplify the
symbolic composition through backtracking to Ô¨Ånd a more
precise interval of each neuron. Counterexample-based re-
Ô¨Ånement is another useful strategy to reÔ¨Åne the interval.

As a reasoning strategy, the usage of interval arith-
metic is usually combined with linear approximation in
recent literature as scalability becomes the primary issue
in verifying larger and deeper neural networks. On the
other hand, integrating interval arithmetic with a proper
approximation method can also enable veriÔ¨Åcation for those
neural networks that use an activation without a piecewise-
linear character like sigmoid and tanh functions.

6.4 Linear Approximation

In deep neural network veriÔ¨Åcation, linear approximation
(also known as linear relaxation) aims to provide an es-
timated range of non-linear activation output. It is Ô¨Årstly
used as a complement to solver-based veriÔ¨Åcation to extend
its support with a broader range of activation functions [48],
[79]. Linear approximation is one of the primary reasoning
techniques for abstract interpretation modeling. By applying
linear approximation with layerwise symbolic analysis, it
is expected to get rid of reliance on SMT/MILP solvers
and achieve a veriÔ¨Åcation with provable soundness and
scalability.

Next, we choose three typical activation functions that
are used in most feedforward neural networks and present
the existing linear approximation algorithms proposed in
the literature. Some linear approximation algorithms also
correspond to the abstraction function (e.g., zonotope [46]
and polyhedron [44]) from the abstract interpretation per-
spective.

ReLU Function

Linear approximation of the ReLU function is Ô¨Årstly pro-
posed in [43] as a supplementary to the native LP modeling,
which replaces the piecewise feature of the ReLU function
by offering a pair of bounds, and thereby forms a constant
mapping relation between the input and output of ReLU

Fig. 8. ReLU function and its convex approximation of the upper bound
(a) and both the upper and lower bounds (b), the shaded regions
represent the range of value approximation.

Fig. 9. ReLU function and its convex approximation in DEEPPOLY for
case (3) where ùúÜ = 0 (a) and ùúÜ = 1 (b), the shaded regions represent
the range of value approximation.

regardless of its activation status (i.e., active or inactive).
Fig. 8-(a) illustrates an early linear approximation of ReLU
with an upper bound. Another similar approximation that
appears in [57], [58], [60] estimates both the upper and
lower bounds of ReLU output in case the activation mode
is unable to be determined (i.e., the input range spans over
zero), as shown in Fig. 8-(b).

(cid:68)

(cid:69)

ùëé ‚â§
ùëó , ùëé ‚â•

ùëó , ùëô ùëó , ùë¢ ùëó

The state-of-the-art linear approximation of the ReLU
function is proposed in DEEPPOLY [44] as the abstraction
function over a polyhedral domain. Recall that in DeÔ¨Å-
nition 6 we propose a sound approximation of a node‚Äôs
value by two polyhedral constraints (i.e., ùëé ‚â§, ùëé ‚â•) and two
: R ‚Üí R be a function
associate constants (i.e., ùëô, ùë¢). Let ùëì
that executes the assignment ùë•ùëñ ‚Üê max (cid:0)0, ùë• ùëó (cid:1) for ùëó = ùëñ ‚àí 1.
Given
, we can estimate the output in three
cases:
1) If ùëô ùëó (cid:62) 0, ùëé(cid:48) ‚â§
ùëñ
2) If ùë¢ ùëó (cid:54) 0, ùëé(cid:48) ‚â§
ùëñ
3) Otherwise ùëé(cid:48) ‚â§
ùëñ

(ùë•) = ùë• ùëó , ùëô (cid:48)
(ùë•) = 0, ùëô (cid:48)
(ùë•) = ùúÜ ¬∑ ùë• ùëó , ùëé(cid:48) ‚â•
ùëñ = ùúÜ ¬∑ ùë• ùëó ,
ùëñ
ùëñ = ùë¢ ùëó where ùúÜ ‚àà {0, 1} that minimizes the area of the
ùë¢(cid:48)
(cid:12)
resulting shape in the (ùë•ùëñ,ùë• ùëó )-plane. i.e., if
(cid:12), then
ùúÜ = 0; otherwise ùúÜ = 1. Fig.9 illustrates two possible
scenarios of the approximation in this case.
Compared with previous literature, the approximation
proposed in DEEPPOLY produces the smallest approxima-
tion region (shown as shadowed areas in Fig. 9), which
stands for the highest precision among all relevant works.

ùëñ = ùëô ùëó , and ùë¢(cid:48)
ùëñ = ùë¢(cid:48)
ùëñ = 0;
(ùë•) = ùë¢ ùëó

( ùë• ùëó ‚àíùëô ùëó)
(ùë¢ ùëó ‚àíùëô ùëó) , ùëô (cid:48)
(cid:12) (cid:62) (cid:12)
(cid:12)
(cid:12)
(cid:12)ùëô ùëó

(ùë•) = ùëé(cid:48) ‚â•
ùëñ
(ùë•) = ùëé(cid:48) ‚â•
ùëñ

ùëñ = ùë¢ ùëó ;

(cid:12)ùë¢ ùëó

Sigmoid and Tanh Functions
Linear approximation of the sigmoid function is Ô¨Årstly
proposed in [48] where the input and output of a sigmoid
function (ùúé) are mapped with intervals. By doing that, a
sigmoid function can be encoded into linear constraints
containing input and output variables, and then be reasoned
by an SMT/LP solver.

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

and ùúÜ(cid:48) = min (cid:0)ùëî(cid:48) (cid:0)ùë¢ ùëó (cid:1) , ùëî(cid:48) (cid:0)ùëô ùëó (cid:1)(cid:1), then:
(a) For the lower bound polyhedral constraint ùëé(cid:48) ‚â§
ùëñ
we have:

ùëé(cid:48) ‚â§
ùëñ

(ùë•) =

(cid:40)ùëî (cid:0)ùëô ùëó (cid:1) + ùúÜ (cid:0)ùë• ùëó ‚àí ùëô ùëó (cid:1)
ùëî (cid:0)ùëô ùëó (cid:1) + ùúÜ(cid:48) (cid:0)ùë• ùëó ‚àí ùëô ùëó (cid:1)

if ùëô ùëó (cid:62) 0
if ùëô ùëó < 0

13

(ùë•),

Fig. 10. Interval abstraction proposed to evaluate the output of the
sigmoid function, with input ranges over [‚àí2, 2] and ùëù = 0.5.

Fig. 11. The convex approximation of the tanh function proposed in
CROWN (a) and DEEPZ (b) when ùëô ùëó < 0 (cid:54) ùë¢ ùëó , the shaded regions
represent the range of over-approximation.

The output range of a sigmoid function is Ô¨Åxed as (0, 1)
and its derivation, ùúé(cid:48)(ùë•) = ùúé(ùë•)(1 ‚àí ùúé(ùë•)), has the maximum
value of 1/4 at which the input is zero. As shown in Fig.10,
if we divide the input domain as a series of intervals in
the length of ùëù, we can always approximate the bounds of
the sigmoid activation function with an arbitrary input ùë• as
0 ‚â§ ùúé (ùë• + ùëù) ‚àí ùúé (ùë•) ‚â§ ùëù/4. Thus we can initiate a sound
abstract interpretation with a box abstract domain.

A similar method could also be applied to the tanh
activation function since both of them have the maximum
derivation value at zero points (the maximum derivation of
ùë°ùëéùëõ‚Ñé (ùë•) is 1 where ùë• = 0). This idea closely relates to another
linear approximation by calculating the Lipschitz continuous
gradient to assess output bounds, which becomes commonly
adopted in later research such as [58], [66], [75].

One recent linear approximation technique for sigmoid
and tanh functions is proposed in DEEPZ [46] as a part of
zonotope abstract interpretation and later adopted in its suc-
ceeding works, such as DEEPPOLY [44]. Since both sigmoid
and tanh functions present an S-shape on the input-output
plane, they are always twice differentiable. Moreover, we
observe their derivatives keep growing with the input value
in the negative range, and conversely, declining with in-
put‚Äôs growth when the input becomes greater than zero.
According to that character, we can estimate the growing
slope of the output within a closed range determined by the
lower and upper bounds of the input. This approximation
is visualized in Fig. 12 and detailed as follows.

Let ùëî : R ‚Üí R be a function that performs ùë•ùëñ ‚Üê ùúé (cid:0)ùë• ùëó (cid:1)
(cid:69)
or ùë•ùëñ ‚Üê ùë°ùëéùëõ‚Ñé (cid:0)ùë• ùëó (cid:1)
for ùëó = ùëñ ‚àí 1. Given
,
we can approximate the new component ùëñ by setting two
associate constants ùëôùëñ = ùëî(ùëô ùëó ), ùë¢ùëñ = ùëî(ùë¢ ùëó ), and two polyhedral
constraints ùëé ‚â§
ùëñ and ùëé ‚â•
1) If ùë¢ ùëó = ùëô ùëó , then ùëé(cid:48) ‚â§
(ùë•) = ùëé(cid:48) ‚â•
ùëñ
ùëñ
2) Otherwise, we compute ùëé(cid:48) ‚â§
ùëñ

ùëñ according to two cases below:
(ùë•) = ùëî(ùëô ùëó );
(ùë•) and ùëé(cid:48) ‚â•
ùëñ

(ùë•) separately.

ùëó , ùëô ùëó , ùë¢ ùëó

ùëé ‚â§
ùëó , ùëé ‚â•

(cid:68)

We Ô¨Årst set ùúÜ =

ùëî(ùë¢ ùëó)‚àíùëî(ùëô ùëó)
ùë¢ ùëó ‚àíùëô ùëó

,

(b) For the upper bound polyhedral constraint ùëé(cid:48) ‚â•
ùëñ
we have:

(ùë•),

ùëé(cid:48) ‚â•
ùëñ

(ùë•) =

(cid:40)ùëî (cid:0)ùë¢ ùëó (cid:1) + ùúÜ (cid:0)ùë• ùëó ‚àí ùë¢ ùëó (cid:1)
ùëî (cid:0)ùë¢ ùëó (cid:1) + ùúÜ(cid:48) (cid:0)ùë• ùëó ‚àí ùë¢ ùëó (cid:1)

if ùë¢ ùëó (cid:54) 0
if ùë¢ ùëó > 0

Another approximation is introduced in CROWN
framework [60] and applied in later research such as [71].
It proposes a similar approximation of sigmoid and tanh
functions with DEEPZ but offers more precise bounds when
the input range spans over zero. Considering sigmoid and
tanh functions always be convex given the input is less
than zero, and be concave for the positive input, CROWN
estimates the upper and lower bounds of sigmoid or tanh
function at an arbitrary input point through a tangent line
that originates from the input bounds, and thereby con-
structs a quadrilateral boundary.

Here we take the tanh function (written as ùëî) as an exam-
ple. Suppose the upper and lower bounds of input are given
as [ùëô ùëó , ùë¢ ùëó ] such that ùëô ùëó < 0 < ùë¢ ùëó , we call the curve segment
corresponding to the input bounds ùëî [ùëô,ùë¢ ]. There must exist
a tangent line starting from either the lower bound or the
upper bound of the tanh function touching the curve itself.
As shown in Fig. 11(a), these two tangent lines constitute
the upper and lower bounds of linear approximation. Since
both tanh and sigmoid functions share the same shape of
curve but only differentiate in output range, we omit the
demonstration of the sigmoid function to save space.

Notwithstanding Ô¨Ånding the tangent line for each bound
could be computationally costly, it offers a sound estimation
with higher precision. CROWN [60] proclaims the Ô¨Ånding
of tangent line can be achieved through a binary-search
algorithm, which means a logarithmic time complexity on
average and therefore theoretically does not escalate the
overall difÔ¨Åculty of veriÔ¨Åcation. Compared with the approx-
imation proposed in CROWN [60], the zonotope abstraction
of sigmoid and tanh functions proposed in DEEPZ [46] does
not require Ô¨Ånding tangent lines in calculating the output
bounds. However, that approach produces a comparably
coarse approximation (see Fig. 11), where the slope at either
the lowest input or the highest input tends to be zero ‚Äì in
contrast CROWN [60] does not have such concern. Overall,
the linear approximation strategy of DEEPZ [46] has the
potential to achieve a better time efÔ¨Åciency in the practice
but sacriÔ¨Åces precision to a certain degree.

6.5 Iterative ReÔ¨Ånement

Iterative reÔ¨Ånement has been proposed as a complemen-
tary technique to reinforce incomplete analysis such as
over-approximation. Some existing approaches [44], [47],
[68], [71] adopt it to achieve a complete veriÔ¨Åcation de-
spite diverse incomplete reasoning strategies being used,
while most of the others take advantage of it to mitigate
the incompleteness. It is Ô¨Årstly used in [48] to mitigate

14

2022

Fig. 12. Visualization of the approximation of sigmoid/tanh activation that is proposed in DEEPZ based on its transformation in case (2) where ùëô ùëó (cid:62) 0
(a for sigmoid,d for tanh), ùë¢ ùëó < 0 (b for sigmoid,e for tanh), and ùëô ùëó < 0 (cid:54) ùë¢ ùëó (c for sigmoid,f for tanh), the shaded regions represent the range of
over-approximation.

false positives in searching for the violation of a property
speciÔ¨Åcation. The iterative reÔ¨Ånement strategy is inspired
by the counterexample-guided abstraction reÔ¨Ånement (CE-
GAR) in model checking. However, in the veriÔ¨Åcation of
neural networks, incomplete analysis usually does not offer
a counterexample when the property speciÔ¨Åcation is shown
unsatisÔ¨Åed. In that case, the goal of iterative reÔ¨Ånement is
to adjust the ‚Äúcoarseness‚Äù of the approximation for certain
nodes within the examined model and eventually validate
if the unsatisÔ¨Åed result is caused by a counterexample or it
is merely a false alarm.

Next, we categorize different techniques that have been

used as iterative reÔ¨Ånement by the existing approaches.

Constraint Replenishment

Constraint replenishment is widely-used when there is no
parameter available to improve the precision, and therefore
we have to bring more constraints for a less coarse analysis.
A common way to achieve that is by adding a complete
reasoning technique, for example referring the MILP encod-
ing to the ReLU function to introduce additional constraints
over its output, making use of these additional constraints
in the feedforward analysis, and in the end obtaining a less
coarse approximation of the neural network output [57],
[64]. Recent literature such as [64], [68], [71], [72], [77] mainly
perform linear approximation in the beginning, especially
for the purpose of relaxing non-linear activation in a linear
format, and then apply an off-the-shelf cross-platform solver
library (e.g., Gurobi) to execute the veriÔ¨Åcation.

Branch and Bound

Branch and bound (colloquially known as B&B, BaB) is an
algorithm design paradigm for solving NP-hard optimiza-
tion and interval analysis [82]. It is in nature a linear re-
laxation of non-linear computation for constraint reasoning.
The branching takes advantage of an efÔ¨Åcient searching strat-
egy to prioritize the ReLU nodes for the analysis, followed
by the bounding process that exploits the cutting-edge over-
approximation techniques to Ô¨Ånd the bounds of the node.

Branch-and-bound is Ô¨Årstly adopted in [48] as a parame-
ter justiÔ¨Åcation technique to exclude unsatisÔ¨Åed veriÔ¨Åcation
outcomes caused by spurious counterexamples. Since the
coarseness of the sigmoid approximation is determined by
the size of an interval of ùëù (see Fig. 10), higher precision can
be achieved by heuristically assigning a smaller value to ùëù. It
repeats until we can determine if the property speciÔ¨Åcation
is satisÔ¨Åed, otherwise a counterexample that violates the
property speciÔ¨Åcation is given.

A similar strategy has also been applied in approxi-
mating the ReLU activation in [23], [43], [61], [68], [76]
by bisecting its input domain. RELUPLEX [23] applies this
strategy in their simplex algorithm to split each ReLU
constraint into two constraints, i.e., the negative part that
always leads to a zero output, and the positive part that
remains unchanged through the ReLU computation. In [61],
branch-and-bound has been adopted with multiple reason-
ing options to achieve completeness to different degrees.
NNENUM [68] implements an enumerated searching algo-
rithm in Ô¨Ånding the sign of the ReLU input to reÔ¨Åne bounds
obtained from over-approximation and claims to achieve a
complete veriÔ¨Åcation in the end. Another recent work [76]
also takes advantage of this approach to aid constraint
reasoning, performing 50% better than the top candidates
in recent veriÔ¨Åcation competitions [83].

Multiple Neurons Analysis

Multiple neurons analysis is an emerging strategy that is
proposed to enhance precision and mitigate incompleteness.
It reÔ¨Ånes the spaces of each symbolic through a multiple
variate analysis, in an intra-layer or cross-layer manner. It
is Ô¨Årstly proposed in KPOLY [65] called k-polyhedral convex
hull approximation, which reasons not only the constraints
of feedforward propagation but also the linear relations
of multiple neurons in the same layer. In addition to its
previous work [44], KPOLY (by setting ùëò = 2) calculates
the bounds of sum (ùë•ùëñ + ùë• ùëó ) and difference (ùë•ùëñ ‚àí ùë• ùëó ) of two
nodes at each layer. As shown in Fig. 13, by analyzing two
nodes simultaneously, we can achieve higher precision than
other approaches with only a single neuron analysis [44]
along with the layer propagation. As a result, the size of
bounding boxes at the output layer drops exponentially
with the number of neurons to be analyzed at the same layer,
implying less unnecessary precision loss produced due to
the abstract interpretation.

Another recent paper [74] uses a similar strategy but
with different reduction and reasoning strategies. It applies
multivariate space reÔ¨Ånement in both layerwise propagation
(FASTC2V) and LP solving (OPTC2V), and thereby achieves
better completeness than previous approaches at the price of
sacriÔ¨Åce in scalability.

Compared with other iterative reÔ¨Ånement approaches,
nowadays multiple neurons analysis still heavily relies on
speciÔ¨Åc mathematical libraries or solving tools and therefore
faces challenges to be adopted in generic programming or
well-known machine learning frameworks (e.g., PyTorch,
TensorFlow, etc.).

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

15

Fig. 13. Visualization of layerwise bounds reÔ¨Ånement over the abstract interpretation of the sample neural network (Ô¨Åner bounds in orange color
are obtained through interval arithmetic based on polyhedral abstract domain aided with backtracking in DEEPPOLY, and bounds in blue colors are
further reÔ¨Åned by applying multiple variate analysis that is adopted in KPOLY)

7 FUTURE DIRECTIONS

The upsurge of research in user-guide neural network veri-
Ô¨Åcation started since the imperceptible input perturbation
is proved to cause misclassiÔ¨Åcation in several research
works [16], [17] and more importantly, in real-life applica-
tions [84]. We Ô¨Ånd dozens of works published during the
past decade that attempt to address this challenge and verify
that a neural network model is robust against adversarial in-
put perturbations. Next, we discuss a few potential research
directions that are potential to bring progress in the future.

Endeavor for a better completeness

Our research community has been relentlessly working to
strike a balance between completeness and scalability. To
resolve this dilemma, one potential solution could be a
heuristic combination of techniques used in both incomplete
and complete veriÔ¨Åers. In one of the most recent works by
Singh et al. [64], a methodology as a mixture of abstract
interpretation and MILP solving is proposed, where two
techniques jointly take effect during the veriÔ¨Åcation. After
the over-approximation is Ô¨Ånished on each layer, a solver
starts reÔ¨Åning the boundary obtained from approximation
with a timeout (because it is a complete veriÔ¨Åcation method
that does not guarantee a termination). That idea has pre-
liminarily proved to gain an advantage from both complete
and incomplete veriÔ¨Åers. However, due to the limitation
of the state-of-the-art MILP approaches, only piecewise-
linear activation (i.e., ReLU) is supported in that work,
which means there is still much work to do in the future
to implement a practical tool for the industry.

Adoption of various complex models

Most of the existing studies only perform the veriÔ¨Åcation
on MLP models, which is an elementary neural network
architecture from nowadays view. As the max-pooling func-
tion could be treated as a variant of the ReLU function
that outputs the maximum from multiple values, we are
pleased to see some recent studies (e.g., [25], [26], [44],
[70]) have adopted their veriÔ¨Åcation approaches to convo-
lutional neural networks (CNNs). However, deep neural
networks are not created only for image classiÔ¨Åcation. Ro-
bustness is equally important for voice recognition and text
prediction tasks, which are achieved by diverse complex
models, such as recurrent neural networks (RNNs) and
transformers. In that case, the challenges of soundness and
completeness trade-off and support of diverse types of
functions (not necessarily to be activation function) become

critical and urgent. Recent research has made effort on
extending existing reduction and reasoning techniques to
robustness veriÔ¨Åcation of RNNs [85], graph neural networks
(GNNs) [86], ùêø‚àû distance networks [87], variational auto-
encoders (VAEs) [73], and transformers [88]. We foresee the
existing approaches will be one day applied to more types
of complex models.

Leveraging optimization algorithms

Existing LP/MILP encoding-based methods rely on math-
ematical optimization formulations and attempt to Ô¨Ånd
the minimum value of an objective function with a large
number of constraints. If the minimum value is positive,
then the veriÔ¨Åcation answers true. While the state-of-the-
art is successful in exploring the optimization state-space
using advanced branching and relaxation techniques [76],
[77], there is still potential room for improvement. One
possibility is to leverage recent advancements of nature-
inspired optimization algorithms [89] at the reasoning stage.
Such algorithms are highly efÔ¨Åcient in Ô¨Ånding optimal val-
ues of extremely complex objective functions at the cost
that the results are often approximations rather than the
global maximum/minimum. In the setting of the veriÔ¨Å-
cation problem, this corresponds to trading completeness
for scalability. However, with a customized algorithm ded-
icated to veriÔ¨Åcation, it is possible to compute very close
approximations of the minimum value for a large deep
neural network in a signiÔ¨Åcantly shorter time than existing
methods. The above can be integrated into a method that
combines incomplete and complete algorithms and takes
advantage of both worlds √† la [64]. It is also worth exploring
the possibility of addressing different activation functions
using such optimization algorithms. The outcome of these
directions should be valuable despite being incomplete.

A generic veriÔ¨Åcation framework

The advancement of veriÔ¨Åcation techniques cannot beneÔ¨Åt
real-life AI applications unless it can be implemented in
generic programming frameworks and well supports main-
stream machine learning ecosystems. We Ô¨Ånd that much lit-
erature in this Ô¨Åeld releases a proof-of-concept that can only
work on particular models and mathematical programming
platforms like MATLAB. Only around 30% (13 out of 39,
see Table 1) of surveyed papers have implemented their
approaches based on TensorFlow and/or PyTorch, which
are the two most popular deep learning frameworks as of
2022 [90]. This phenomenon seriously limits the industrial
adoption of robustness veriÔ¨Åcation. We advocate that efforts

LegendÔºöBounds of interval arithmetic (w. box abstract domain)Bounds of DEEPPOLYafter refinementBounds of K-POLYafter multivariate refinement16

2022

need to be made for a more generic veriÔ¨Åcation framework.
Future research is suggested to release their techniques as
public packages that are compatible with the mainstream
deep learning ecosystems.

8 CONCLUSION

The veriÔ¨Åcation of neural networks is the art of problem-
solving. Its development engages persistent research efforts
from veriÔ¨Åcation, optimization, and machine learning com-
munities. In this survey, we collect 39 existing approaches to
verifying adversarial robustness and we propose our taxon-
omy for classifying those related works from a perspective
of formal veriÔ¨Åcation. We present our classiÔ¨Åcation result
regarding three key components of model checking includ-
ing the formalization of property speciÔ¨Åcation, reduction
of the examined model, and available reasoning strategies.
To gain an in-depth understanding of the rationality and
difference between those diverse approaches, we create a
sample multi-layered neural network model and provide
substantial visualization in the paper. We then discuss both
advantages and challenges that are yet to be addressed for
each approach. In the end, we share our outlook on the
future directions of deep neural network veriÔ¨Åcation.

REFERENCES

[1] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù nature, vol.

521, no. 7553, pp. 436‚Äì444, 2015.

[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Å-
cation with deep convolutional neural networks,‚Äù in Advances in
neural information processing systems, 2012, pp. 1097‚Äì1105.

[4]

[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., ‚ÄúDeep
neural networks for acoustic modeling in speech recognition: The
shared views of four research groups,‚Äù IEEE Signal processing
magazine, vol. 29, no. 6, pp. 82‚Äì97, 2012.
S. Deng, L. Huang, G. Xu, X. Wu, and Z. Wu, ‚ÄúOn deep learning
for trust-aware recommendations in social networks,‚Äù IEEE trans-
actions on neural networks and learning systems, vol. 28, no. 5, pp.
1164‚Äì1177, 2016.
I. Sutskever, O. Vinyals, and Q. V. Le, ‚ÄúSequence to sequence
learning with neural networks,‚Äù in Advances in Neural Information
Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, Eds., 2014, pp. 3104‚Äì3112.
[6] M. H. Vrejoiu, ‚ÄúNeural networks and deep learning in cyber
security,‚Äù Romanian Cyber Security Journal, vol. 1, no. 1, pp. 69‚Äì86,
2019.

[5]

[7] Y. Tian, K. Pei, S. Jana, and B. Ray, ‚ÄúDeeptest: Automated testing
of deep-neural-network-driven autonomous cars,‚Äù in Proceedings
of the 40th international conference on software engineering, 2018, pp.
303‚Äì314.

[8] B. Alipanahi, A. Delong, M. T. Weirauch, and B. J. Frey, ‚ÄúPredicting
the sequence speciÔ¨Åcities of dna-and rna-binding proteins by deep
learning,‚Äù Nature biotechnology, vol. 33, no. 8, pp. 831‚Äì838, 2015.

[9] N. Shone, T. N. Ngoc, V. D. Phai, and Q. Shi, ‚ÄúA deep learning
approach to network intrusion detection,‚Äù IEEE Transactions on
Emerging Topics in Computational Intelligence, vol. 2, no. 1, pp. 41‚Äì50,
2018.

[10] R. Nix and J. Zhang, ‚ÄúClassiÔ¨Åcation of android apps and malware
using deep neural networks,‚Äù in 2017 International Joint Conference
on Neural Networks (IJCNN), 2017, pp. 1871‚Äì1878.

[11] K. Shima, D. Miyamoto, H. Abe, T. Ishihara, K. Okada, Y. Sekiya,
H. Asai, and Y. Doi¬ß, ‚ÄúClassiÔ¨Åcation of url bitstreams using bag of
bytes,‚Äù in 2018 21st Conference on Innovation in Clouds, Internet and
Networks and Workshops (ICIN), 2018, pp. 1‚Äì5.

[12] Z. Okonkwo, E. Foo, Q. Li, and Z. Hou, ‚ÄúA CNN based encrypted
network trafÔ¨Åc classiÔ¨Åer,‚Äù in ACSW 2022: Australasian Computer
Science Week 2022, Brisbane, Australia, February 14 - 18, 2022,
D. Abramson and M. N. Dinh, Eds. ACM, 2022, pp. 74‚Äì83.

[13] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra,
‚ÄúWeight uncertainty in neural networks,‚Äù in International conference
on machine learning. PMLR, 2015, pp. 1613‚Äì1622.

[14] A. Shahrokni and R. Feldt, ‚ÄúA systematic review of software
robustness,‚Äù Information and Software Technology, vol. 55, no. 1, pp.
1‚Äì17, 2013.

[15] L.-M. Fu, Neural networks in computer intelligence. Tata McGraw-

Hill Education, 2003.
[16] C. Szegedy, W. Zaremba,

I. Sutskever,

J. Bruna, D. Erhan,
J. Goodfellow, and R. Fergus, ‚ÄúIntriguing properties of
I.
neural networks,‚Äù in 2nd International Conference on Learning
Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2014.
[Online]. Available: http://arxiv.org/abs/1312.6199

[17] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and
harnessing adversarial examples,‚Äù in 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun,
Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6572
[18] M. Stewart, ‚ÄúSecurity vulnerabilities of neural networks,‚Äù
2019, (Accessed 15-February-2022). [Online]. Available: https://
towardsdatascience.com/hacking-neural-networks-2b9f461ffe0b
[19] J. Jeong, S. Park, M. Kim, H. Lee, D. Kim, and J. Shin, ‚ÄúSmoothmix:
Training conÔ¨Ådence-calibrated smoothed classiÔ¨Åers for certiÔ¨Åed
robustness,‚Äù in Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelz-
imer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp.
30 153‚Äì30 168.

[20] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
‚ÄúTowards deep learning models resistant to adversarial attacks,‚Äù
in 6th International Conference on Learning Representations. Open-
Review.net, 2018.

[21] K. Leino, Z. Wang, and M. Fredrikson, ‚ÄúGlobally-robust neural
networks,‚Äù in International Conference on Machine Learning. PMLR,
2021, pp. 6212‚Äì6222.

[22] B. Zhang, T. Cai, Z. Lu, D. He, and L. Wang, ‚ÄúTowards certifying l-
inÔ¨Ånity robustness using neural networks with l-inf-dist neurons,‚Äù
in International Conference on Machine Learning. PMLR, 2021, pp.
12 368‚Äì12 379.

[23] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer,
‚ÄúReluplex: An efÔ¨Åcient smt solver for verifying deep neural net-
works,‚Äù in International Conference on Computer Aided VeriÔ¨Åcation.
Springer, 2017, pp. 97‚Äì117.

[24] W. Ruan, X. Huang, and M. Kwiatkowska, ‚ÄúReachability analysis
of deep neural networks with provable guarantees,‚Äù in Proceedings
of the 27th International Joint Conference on ArtiÔ¨Åcial Intelligence,
J. Lang, Ed.

ijcai.org, 2018, pp. 2651‚Äì2659.

[25] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaud-
huri, and M. Vechev, ‚ÄúAi2: Safety and robustness certiÔ¨Åcation
of neural networks with abstract interpretation,‚Äù in 2018 IEEE
Symposium on Security and Privacy.

IEEE, 2018, pp. 3‚Äì18.

[26] E. Wong and J. Z. Kolter, ‚ÄúProvable defenses against adversarial
examples via the convex outer adversarial polytope,‚Äù in Pro-
ceedings of the 35th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, J. G. Dy and
A. Krause, Eds., vol. 80. PMLR, 2018, pp. 5283‚Äì5292.

[27] A. Raghunathan, J. Steinhardt, and P. Liang, ‚ÄúCertiÔ¨Åed defenses
against adversarial examples,‚Äù in 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
[Online]. Available: https://openreview.net/forum?id=
2018.
Bys4ob-Rb

[28] K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, and P. Kohli, ‚ÄúA
dual approach to scalable veriÔ¨Åcation of deep networks,‚Äù in The
Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, vol. 1, 2018, p. 2.
[29] A. Albarghouthi, L. D‚ÄôAntoni, S. Drews, and A. V. Nori,
‚ÄúFairsquare: probabilistic veriÔ¨Åcation of program fairness,‚Äù Pro-
ceedings of the ACM on Programming Languages, vol. 1, no. OOPSLA,
pp. 1‚Äì30, 2017.

[30] S. A. Seshia, A. Desai, T. Dreossi, D. J. Fremont, S. Ghosh, E. Kim,
S. Shivakumar, M. Vazquez-Chanlatte, and X. Yue, ‚ÄúFormal spec-
iÔ¨Åcation for deep neural networks,‚Äù in International Symposium on
Automated Technology for VeriÔ¨Åcation and Analysis.
Springer, 2018,
pp. 20‚Äì34.

[31] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and
D. Kroening, ‚ÄúConcolic testing for deep neural networks,‚Äù in Pro-

MENG et al.: ADVERSARIAL ROBUSTNESS OF DEEP NEURAL NETWORKS - A SURVEY FROM A FORMAL VERIFICATION PERSPECTIVE

17

ceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering, 2018, pp. 109‚Äì119.

[32] W. Xiang, P. Musau, A. A. Wild, D. M. Lopez, N. Hamilton,
X. Yang, J. Rosenfeld, and T. T. Johnson, ‚ÄúVeriÔ¨Åcation for machine
learning, autonomy, and neural networks survey,‚Äù arXiv preprint
arXiv:1810.01989, 2018.

[33] C. Liu, T. Arnon, C. Lazarus, C. A. Strong, C. W. Barrett, and M. J.
Kochenderfer, ‚ÄúAlgorithms for verifying deep neural networks,‚Äù
Found. Trends Optim., vol. 4, no. 3-4, pp. 244‚Äì404, 2021.

[34] L. Li, X. Qi, T. Xie, and B. Li, ‚ÄúSok: CertiÔ¨Åed robustness for deep

neural networks,‚Äù arXiv preprint arXiv:2009.04131, 2020.
don‚Äôt

formal meth-
[35] H. Wayne,
ods?‚Äù
[On-
(Accessed
line]. Available: https://www.hillelwayne.com/post/why-dont-
people-use-formal-methods

15-January-2022).

January

people

‚ÄúWhy

2019,

use

[36] C. Baier and J.-P. Katoen, Principles of model checking. MIT press,

2008.

[37] E. Clarke, O. Grumberg, S.

Jha, Y. Lu, and H. Veith,
‚ÄúCounterexample-guided abstraction reÔ¨Ånement,‚Äù in International
Conference on Computer Aided VeriÔ¨Åcation. Springer, 2000, pp. 154‚Äì
169.

[38] N. Carlini and D. Wagner, ‚ÄúTowards Evaluating the Robustness of
Neural Networks,‚Äù in 2017 IEEE Symposium on Security and Privacy.
San Jose, CA, USA: IEEE, May 2017, pp. 39‚Äì57.

[39] D. Hendrycks and T. G. Dietterich, ‚ÄúBenchmarking neural net-
work robustness to common corruptions and perturbations,‚Äù in
7th International Conference on Learning Representations. OpenRe-
view.net, 2019.

[40] Y. LeCun and C. Cortes, ‚ÄúMNIST handwritten digit database,‚Äù

2010. [Online]. Available: http://yann.lecun.com/exdb/mnist/

[41] M. Hicks,

‚ÄúWhat
analy-
[Online].
sis)?‚Äù October 2017,
Available: http://www.pl-enthusiast.net/2017/10/23/what-is-
soundness-in-static-analysis/

is
(Accessed 20-January-2022).

soundness

static

(in

[42] B. Meyer,

‚ÄúSoundness
2019,

pre-
and
cision,‚Äù April
[On-
(Accessed
line]. Available: https://cacm.acm.org/blogs/blog-cacm/236068-
soundness-and-completeness-with-precision

completeness: With
19-January-2022).

[43] R. Ehlers, ‚ÄúFormal veriÔ¨Åcation of piece-wise linear feed-forward
neural networks,‚Äù in International Symposium on Automated Tech-
nology for VeriÔ¨Åcation and Analysis. Springer, 2017, pp. 269‚Äì286.

[44] G. Singh, T. Gehr, M. P√ºschel, and M. Vechev, ‚ÄúAn abstract
domain for certifying neural networks,‚Äù Proceedings of the ACM
on Programming Languages, vol. 3, no. POPL, pp. 1‚Äì30, 2019.
[45] Y. Yisrael Elboher, J. Gottschlich, and G. Katz, ‚ÄúAn abstraction-
based framework for neural network veriÔ¨Åcation,‚Äù in Computer
Aided VeriÔ¨Åcation.
Springer International Publishing, 2020, pp.
43‚Äì65.

[46] G. Singh, T. Gehr, M. Mirman, M. P√ºschel, and M. Vechev, ‚ÄúFast
and effective robustness certiÔ¨Åcation,‚Äù in Advances in Neural Infor-
mation Processing Systems, 2018, pp. 10 802‚Äì10 813.

[47] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, ‚ÄúFormal
security analysis of neural networks using symbolic intervals,‚Äù in
27th USENIX Security Symposium (USENIX Security 18), 2018, pp.
1599‚Äì1614.

[48] L. Pulina and A. Tacchella, ‚ÄúAn abstraction-reÔ¨Ånement approach
to veriÔ¨Åcation of artiÔ¨Åcial neural networks,‚Äù in International Con-
ference on Computer Aided VeriÔ¨Åcation. Springer, 2010, pp. 243‚Äì257.
[49] K. Scheibler, L. Winterer, R. Wimmer, and B. Becker, ‚ÄúTowards
veriÔ¨Åcation of artiÔ¨Åcial neural networks.‚Äù in MBMV, 2015, pp. 30‚Äì
40.

[50] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori,
and A. Criminisi, ‚ÄúMeasuring neural net robustness with con-
straints,‚Äù in Advances in neural information processing systems, 2016,
pp. 2613‚Äì2621.

[51] C.-H. Cheng, G. N√ºhrenberg, and H. Ruess, ‚ÄúMaximum resilience
of artiÔ¨Åcial neural networks,‚Äù in International Symposium on Auto-
mated Technology for VeriÔ¨Åcation and Analysis.
Springer, 2017, pp.
251‚Äì268.

[52] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, ‚ÄúSafety ver-
iÔ¨Åcation of deep neural networks,‚Äù in International Conference on
Computer Aided VeriÔ¨Åcation. Springer, 2017, pp. 3‚Äì29.

[53] A. Lomuscio and L. Maganti, ‚ÄúAn approach to reachability
analysis for feed-forward relu neural networks,‚Äù CoRR, vol.
abs/1706.07351, 2017. [Online]. Available: http://arxiv.org/abs/
1706.07351

[54] W. Xiang, H.-D. Tran, and T. T. Johnson, ‚ÄúReachable set com-
putation and safety veriÔ¨Åcation for neural networks with relu
activations,‚Äù arXiv preprint arXiv:1712.08163, 2017.

[55] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari, ‚ÄúOutput
range analysis for deep feedforward neural networks,‚Äù in NASA
Formal Methods Symposium. Springer, 2018, pp. 121‚Äì138.

[56] A. Raghunathan, J. Steinhardt, and P. Liang, ‚ÄúSemideÔ¨Ånite re-
laxations for certifying robustness to adversarial examples,‚Äù in
Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montr√©al, Canada, S. Bengio, H. M. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.,
2018, pp. 10 900‚Äì10 910.

[57] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, ‚ÄúEfÔ¨Åcient for-
mal safety analysis of neuralnetworks,‚Äù in Advances in Neural Infor-
mation Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.
Curran
Associates, Inc., 2018, pp. 6367‚Äì6377.

[58] L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel,
D. Boning, and I. Dhillon, ‚ÄúTowards fast computation of certiÔ¨Åed
robustness for relu networks,‚Äù in Proceedings of the 35th Interna-
tional Conference on Machine Learning, 2018, pp. 5276‚Äì5285.

[59] W. Xiang, H.-D. Tran, and T. T. Johnson, ‚ÄúOutput reachable set
estimation and veriÔ¨Åcation for multilayer neural networks,‚Äù IEEE
transactions on neural networks and learning systems, vol. 29, no. 11,
pp. 5777‚Äì5783, 2018, publisher: IEEE.

[60] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel,
‚ÄúEfÔ¨Åcient neural network robustness certiÔ¨Åcation with general
activation functions,‚Äù in Advances in neural information processing
systems, 2018, pp. 4939‚Äì4948.

[61] R. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar, ‚ÄúA
uniÔ¨Åed view of piecewise linear neural network veriÔ¨Åcation,‚Äù in
Proceedings of the 32nd International Conference on Neural Information
Processing Systems, 2018, pp. 4795‚Äì4804.

[62] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
P. Shah, S. Thakoor, H. Wu, A. Zelji¬¥c et al., ‚ÄúThe marabou frame-
work for veriÔ¨Åcation and analysis of deep neural networks,‚Äù in
International Conference on Computer Aided VeriÔ¨Åcation.
Springer,
2019, pp. 443‚Äì452.

[63] V. Tjeng, K. Y. Xiao, and R. Tedrake, ‚ÄúEvaluating robustness
of neural networks with mixed integer programming,‚Äù in 7th
International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [Online].
Available: https://openreview.net/forum?id=HyGIdiRqtm
[64] G. Singh, T. Gehr, M. P√ºschel, and M. T. Vechev, ‚ÄúBoosting
robustness certiÔ¨Åcation of neural networks,‚Äù in 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net, 2019. [Online]. Available:
https://openreview.net/forum?id=HJgeEh09KQ

[65] G. Singh, R. Ganvir, M. P√ºschel, and M. Vechev, ‚ÄúBeyond the
single neuron convex barrier for neural network certiÔ¨Åcation,‚Äù in
Advances in Neural Information Processing Systems, 2019, pp. 15 098‚Äì
15 109.

[66] C. Huang, J. Fan, W. Li, X. Chen, and Q. Zhu, ‚ÄúReachnn: Reacha-
bility analysis of neural-network controlled systems,‚Äù ACM Trans-
actions on Embedded Computing Systems (TECS), vol. 18, no. 5s, pp.
1‚Äì22, 2019.

[67] W. Xiang, H.-D. Tran, X. Yang, and T. T. Johnson, ‚ÄúReachable
set estimation for neural network control systems: A simulation-
guided approach,‚Äù IEEE Transactions on Neural Networks and Learn-
ing Systems, 2020.

[68] S. Bak, H.-D. Tran, K. Hobbs, and T. T. Johnson, ‚ÄúImproved
geometric path enumeration for verifying relu neural networks,‚Äù
in Proceedings of the 32nd International Conference on Computer Aided
VeriÔ¨Åcation. Springer, 2020.

[69] H.-D. Tran, D. Manzanas Lopez, P. Musau, X. Yang, L. V. Nguyen,
W. Xiang, and T. T. Johnson, ‚ÄúStar-based reachability analysis
of deep neural networks,‚Äù in International symposium on formal
methods. Springer, 2019, pp. 670‚Äì686.

[70] H.-D. Tran, X. Yang, D. Manzanas Lopez, P. Musau, L. V. Nguyen,
W. Xiang, S. Bak, and T. T. Johnson, ‚ÄúNnv: the neural network
veriÔ¨Åcation tool for deep neural networks and learning-enabled
cyber-physical systems,‚Äù in International Conference on Computer
Aided VeriÔ¨Åcation. Springer, 2020, pp. 3‚Äì17.

[71] P. Henriksen and A. Lomuscio, ‚ÄúEfÔ¨Åcient neural network veriÔ¨Å-
cation via adaptive reÔ¨Ånement and adversarial search,‚Äù in ECAI
2020.

IOS Press, 2020, pp. 2513‚Äì2520.

18

2022

[72] E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Mis-
ener, ‚ÄúEfÔ¨Åcient veriÔ¨Åcation of relu-based neural networks via
dependency analysis,‚Äù in Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence, vol. 34, no. 04, 2020, pp. 3291‚Äì3299.

[73] S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J. Uesato,
R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang, and
P. Kohli, ‚ÄúEnabling certiÔ¨Åcation of veriÔ¨Åcation-agnostic networks
via memory-efÔ¨Åcient semideÔ¨Ånite programming,‚Äù in Advances in
Neural Information Processing Systems, H. Larochelle, M. Ranzato,
Curran
R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33.
Associates, Inc., 2020, pp. 5318‚Äì5331.

[74] C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma, K. Patel,
and J. P. Vielma, ‚ÄúThe convex relaxation barrier, revisited: Tight-
ened single-neuron relaxations for neural network veriÔ¨Åcation,‚Äù
in Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato,
R. Hadsell, M. Balcan, and H. Lin, Eds., 2020, pp. 21 675‚Äì21 686.

[75] M. Fazlyab, M. Morari, and G. J. Pappas, ‚ÄúSafety veriÔ¨Åcation and
robustness analysis of neural networks via quadratic constraints
and semideÔ¨Ånite programming,‚Äù IEEE Transactions on Automatic
Control, 2020.

[76] B. Batten, P. Kouvaros, A. Lomuscio, and Y. Zheng, ‚ÄúEfÔ¨Åcient
neural network veriÔ¨Åcation via layer-based semideÔ¨Ånite relax-
ations and linear cuts,‚Äù in Proceedings of the 30th International Joint
Conference on ArtiÔ¨Åcial Intelligence, Z.-H. Zhou, Ed.
International
Joint Conferences on ArtiÔ¨Åcial Intelligence Organization, 8 2021,
pp. 2184‚Äì2190, main Track.

[77] P. Kouvaros and A. Lomuscio, ‚ÄúTowards scalable complete veriÔ¨Å-
cation of relu neural networks via dependency-based branching,‚Äù
in International Joint Conference on ArtiÔ¨Åcial Intelligence, 2021, pp.
2643‚Äì2650.

[78] M. Fischetti and J. Jo, ‚ÄúDeep neural networks and mixed integer
linear optimization,‚Äù Constraints, vol. 23, no. 3, pp. 296‚Äì309, 2018,
publisher: Springer.

[79] L. Pulina and A. Tacchella, ‚ÄúChallenging smt solvers to verify
neural networks,‚Äù Ai Communications, vol. 25, no. 2, pp. 117‚Äì135,
2012.

[80] H. Dawood, Theories of interval arithmetic: mathematical foundations
and applications. LAP Lambert Academic Publishing, 2011.
[81] G. Alefeld and G. Mayer, ‚ÄúInterval analysis: theory and applica-
tions,‚Äù Journal of computational and applied mathematics, vol. 121, no.
1-2, pp. 421‚Äì464, 2000.

[82] J. Clausen, ‚ÄúBranch and bound algorithms-principles and exam-
ples,‚Äù Department of Computer Science, University of Copenhagen, pp.
1‚Äì30, 1999.

[83] VNN-COMP, ‚ÄúVeriÔ¨Åcation of neural networks

competition
(vnn-comp 2020),‚Äù 2020, (Accessed 15-Janurary-2022). [Online].
Available: https://sites.google.com/view/vnn20/vnncomp
[84] L. Alan and B. Ryan, ‚ÄúTesla‚Äôs autopilot found partly to blame
for 2018 crash on the 405,‚Äù https://www.latimes.com/business/
story/2019-09-04/tesla-autopilot-is-found-partly-to-blame-for-
2018-freeway-crash, 2019, (Accessed 1-February-2022).

[85] X. Du, Y. Li, X. Xie, L. Ma, Y. Liu, and J. Zhao, ‚ÄúMARBLE: Model-
based robustness analysis of stateful deep learning systems,‚Äù
in Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering (ASE). ACM, Dec. 2020, pp. 423‚Äì
435.

[86] B. Wang, J. Jia, X. Cao, and N. Z. Gong, ‚ÄúCertiÔ¨Åed robustness
of graph neural networks against adversarial structural pertur-
bation,‚Äù in Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining, 2021, pp. 1645‚Äì1653.

[87] B. Zhang, D.

Jiang, D. He, and L. Wang, ‚ÄúBoosting the
certiÔ¨Åed robustness of l-inÔ¨Ånity distance nets,‚Äù arXiv preprint
arXiv:2110.06850, 2021.

[88] Z. Shi, H. Zhang, K. Chang, M. Huang, and C. Hsieh, ‚ÄúRobustness
veriÔ¨Åcation for transformers,‚Äù in 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. [Online]. Available:
https://openreview.net/forum?id=BJxwPJHFwS

[89] S. Mirjalili, J. S. Dong, and A. Lewis, Eds., Nature-Inspired Opti-
mizers - Theories, Literature Reviews and Applications, ser. Studies in
Computational Intelligence. Springer, 2020, vol. 811.

[90] R. O‚ÄôConnor,

‚ÄúPytorch vs TensorFlow in 2022,‚Äù https:
//www.assemblyai.com/blog/pytorch-vs-tensorÔ¨Çow-in-2022/,
2021, (Accessed 11-April-2022).

