2
2
0
2

p
e
S
5

]
E
S
.
s
c
[

2
v
6
2
0
2
1
.
3
0
2
2
:
v
i
X
r
a

1

Machine Learning Testing in an ADAS Case Study
Using Simulation-Integrated Bio-Inspired
Search-Based Testing
Mahshid Helali Moghadam∗‡, Markus Borg∗, Mehrdad Saadatmand∗, Seyed Jalaleddin
Mousavirad†, Markus Bohlin‡ and Bj¨orn Lisper‡
RISE Research Institutes of Sweden, Sweden
†Universidade da Beira Interior, Covilh˜a, Portugal
‡M¨alardalen University, Sweden

∗

Abstract

This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that
generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly
proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), (µ + λ) and
(µ, λ) evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed
and domain-speciﬁc cross-over and mutation operations tailored for the presentation model used for modeling the
test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out
an empirical evaluation and comparison with regard to the results of ﬁve participating tools in the cyber-physical
systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not
only represent a considerable improvement on the previous version but also prove to be effective and efﬁcient in
provoking a considerable number of diverse failure-revealing test scenarios for testing an ML-driven lane-keeping
system. They can trigger several failures while promoting test scenario diversity, under a limited test time budget,
high target failure severity, and strict speed limit constraints.

Machine Learning Testing, Search-Based Testing, Evolutionary Computation, Advanced Driver Assistance

Systems, Deep Learning, Lane-Keeping System

Index Terms

I. INTRODUCTION
Machine Learning (ML) nowadays is used in a wide range of application areas such as automotive
[1], [2], health care [3] and manufacturing [4]. Many of the ML-driven systems in these domains present
a high level of autonomy [5] and meanwhile are subject to rigorous safety requirements [6]. In 2018,
the European Commission (EC) published a strategy for trustworthy Artiﬁcial Intelligence (AI) systems
[7]. In this strategy, AI systems are deﬁned as “systems that display intelligent behavior by analyzing
their environment and taking actions–with some degree of autonomy–to achieve speciﬁc goals”. The EC
states that a trustworthy AI system must be lawful, ethical, and robust. Self-driving cars are examples of
safety-critical AI systems, which leverage various ML techniques such as Deep Neural Networks (DNN),
machine vision, and sensor data fusion. Meanwhile, in the context of automotive software engineering,
there is always a set of strict safety requirements to meet.

The quality assurance methodology for AI systems [8] is quite different from the conventional software
systems, since the included ML components in those systems are not explicitly programmed, they are
intended to learn from data and experience instead—called Software 2.0 [9]. In addition, in AI systems,
a part of the requirements is mainly seen as encoded implicitly in the data and the challenge of under-
speciﬁcity is common in requirements deﬁnitions. However, it is still highly expected to assure the ability
of the AI system to control the risk of hazardous events in particular in safety-critical domains. In this
regard, Hawkins et al. [8] introduced a methodology for assurance of ML in autonomous systems, called
AMLAS. It presents a systematic process for integrating safety assurance into the development of ML

 
 
 
 
 
 
2

systems and introduces veriﬁcation procedures at different stages, e.g., learning model veriﬁcation and
system-level (integration) veriﬁcation that happens after integrating the ML model into the system.

There is also a vigorous need for integration Veriﬁcation and Validation (V&V) of ML models deployed
in self-driving cars to make sure that they are safe and dependable. Many of the failures basically emerge
in the interplay between software containing ML components, hardware, and remote sensing devices, e.g.,
sensors, cameras, RADAR, and LiDAR technologies. Hardware-In-the-Loop (HIL), simulation-based and
ﬁeld testing are common approaches for system-level veriﬁcation of deployed ML models [8]. System-level
testing mainly targets deﬁning a set of operational scenarios that could lead to failures. In this regard, in the
ISO/PAS 21448 Safety of the Intended Function (SOTIF) standard [10]—which addresses complementary
aspects of functional safety in ISO 26262 [11]—simulation-based testing has been considered a proﬁcient
approach and a proper complementary solution to the on-road testing. Testing on real-world roads is costly,
does not scale to cover all the needed scenarios, and in addition, it is dangerous to create and execute
critical scenarios. The use of virtual prototyping allows testing and veriﬁcation at the early stages of the
development and offers the possibility of efﬁcient and effective testing. It can capture the whole of the
operational environment to a great extent using thoroughgoing physics-based simulators [?]. Recently, a
growing number of commercial and open-source simulators have been developed to support the need for
realistic simulation of self-driving cars [12]–[14]—we refer interested readers to a review by Rosique et
al. [15].

Research Challenge. In this study, we target an Advanced Driver-Assistance System (ADAS) that
provides lane-keeping assistance. Effective and efﬁcient system-level testing in simulation environments
requires sophisticated approaches to generate critical test scenarios. The critical test scenarios are those
that break or are close to break the safety requirements of the ADAS under test, which hence result in
safety violations. Generating effective test scenarios involves sampling from a large and complex set of
test inputs. Several authors have shown the potential of search-based software test generation techniques
to address this challenge. Various system-level testing techniques using different search-based testing
approaches [16]–[21] for different types of ADAS, relying on simulators, have been proposed in recent
years.

Research Contribution. In this paper, we present a bio-inspired computation-driven test generator,
called Deeper, for effective and efﬁcient generation of failure-revealing test scenarios to test a Deep
Neural Network (DNN)-based lane-keeping system in the BeamNG driving simulator. The test subject
is BeamNG.AI, the built-in ML-driven driving agent in the BeamNG simulator. In this study, a failure
is deﬁned in terms of episodes in which the ego car—driven by the BeamNG.AI agent—drives partially
outside the lane w.r.t a certain tolerance threshold. The tolerance threshold determines the percentage of
the car’s bounding box needed to be outside the lane to be regarded as a failure.

Deeper in its current version in this paper beneﬁts from the genetic algorithm (GA), (µ + λ) and
(µ, λ) evolution strategies (ES), and the particle swarm optimization (PSO) to generate failure-revealing
test scenarios, which are test roads in our study. The problem is basically regarded as an optimization
problem, and in order to generate the test scenarios that are of interest, we evaluate the quality of the
test scenarios using a ﬁtness (objective) function that guides the search process to maximize the detected
distance of the car from the center of the lane during driving of the car on the lane. The initial version
of Deeper [19] contained a test generator based on NSGA-II. In this paper, we extend Deeper with four
additional test generators based on GA, (µ + λ) and (µ, λ) ESs, and PSO. In the newly proposed test
generators, we leverage an initial quality population seed to boost the search process, and also propose
and develop domain-speciﬁc cross-over and mutation operations tailored for the presentation model used
for modeling the test scenarios in the search algorithms. We rely on the presentation model [22] used by
DeepJanus [22] based on Catmull-Rom cubic splines [23].

Empirical evaluation. In order to carry out an empirical evaluation, we use the setup provided by
the cyber-physical systems (CPS) testing competition1 at the IEEE/ACM 14th International Workshop on

1Available at https://github.com/se2p/tool-competition-av.

3

Search-Based Software Testing (SBST). Our experiments are designed to answer three main research
questions which are as follows:
RQ1: How capable are these test generators to trigger failures?
RQ2: How diverse are the generated failure-revealing test scenarios?
RQ3: How effectively and efﬁciently do the test generators perform? I.e., given a certain test budget, how
many test scenarios are generated, what proportion of the scenarios is valid, and ﬁnally what proportion
of the valid test scenarios leads to triggering failures?
We provide a comparative analysis on the performance of the proposed bio-inspired test generators in
Deeper and ﬁve counterpart tools all integrated into the BeamNG simulator. In this regard, we compare
the results of the proposed test generators in Deeper with ﬁve other test generator tools, Frenetic [24],
GABExploit and GABExplore [21], Swat [25], and also the earlier version of the Deeper (based on NSGA-
II) [19]—all participating tools in the CPS competition at SBST 2021. In order to do a fair comparison,
we consider the same experimental evaluation procedures as the original CPS tool competition. Our
experimental results show that ﬁrst, the newly proposed test generators in Deeper present a considerable
improvement on the previous version, second, they perform as effective and efﬁcient test generators
that can provoke a considerable number of diverse failure-revealing test scenarios w.r.t different target
failure severity (i.e., in terms of tolerance threshold), available test budget, and driving style constraints
(e.g, setting speed limits). For instance, in terms of the number of triggered failures within a given
test time budget and with less strict driving constraints, the (µ + λ) ES-driven test generator in Deeper
considerably outperforms other tools while keeping the level of promoted failure diversity quite close to
the counterpart tool with the highest number of detected failures in the competition. Meanwhile, as a
distinctive feature, none of the newly proposed test generators leaves the experiment without triggering
any failures, and in particular, they act as more reliable test generators than most of the other tools
for provoking diverse failures under a limited test budget and strict constraints. With respect to the test
effectiveness and efﬁciency, Deeper (µ + λ) ES-, PSO-, and GA-driven result in high effectiveness in
terms of the ratio of the number of detected failures to the generated valid test scenarios.

The rest of this paper is organized as follows: Section II presents background information on random
search techniques including evolutionary and swarm intelligence techniques. Section III presents the
problem formulation and the technical details of our proposed test generators in Deeper. Section IV
elaborates on the empirical evaluation, including the research method, experiments setup. Section V
discusses the results, answers to the RQs, and the threats to the validity of the results. Section VI
provides an overview of the related work, and ﬁnally, Section VII concludes the paper with our ﬁndings
and the potential research directions for future work.

II. BACKGROUND
Evolutionary and swarm intelligence algorithms are two main classes of random search techniques,
which are widely used in many different optimization problems. Genetic algorithms (GA), genetic
programming (GP), differential evolution (DE), and evolution strategies (ES) are the main categories
inside the family of evolutionary algorithms (EAs). Particle swarm optimization (PSO) is one of the
primary representatives of swarm intelligence algorithms [26].

Genetic algorithms is one of the most common nature-inspired optimization techniques. It starts with
a random population of individuals—each called a chromosome—representing a potential solution for
the problem. The objectives to be optimized in the problem are deﬁned in an objective function and the
quality of the solutions is measured via this function. It shows how ”well” each solution satisﬁes the
objective. The quality of each individual, which is also referred to as “ﬁtness”, is a main factor during
the evolution process. At each generation, a new population is formed based on the selected individuals
from the previous generation. Three operations are involved in forming the new generation, which are as
follows:

1) Selection, which mainly identiﬁes highly-valued individuals from the previous generation.

4

2) Crossover, which breeds “child” individuals by exchanging parts of the “parent” individuals. The

child individuals (offspring) are formed by selecting genes from each parent individual.

3) Mutation, which applies small random adjustments to the individuals.
Crossover and mutation operations are applied w.r.t user-set probabilities, and these two operations
might be used, either independently or jointly, to create new individuals to form a new population. The
resulting individuals are added to the new population. The ﬁtness values are calculated and stored for
each individual in this population. This process iterates each generation until stopping criteria are met,
e.g., a user-set number of generations or an allowed time budget is exhausted [20], [26].

Evolution strategy is another common class of EAs. It is commonly used in almost all ﬁelds of
optimization problems including discrete and continuous input spaces. ES also involves applying selection,
recombination, and mutation to a population of individuals over various generations to get iteratively
evolved solutions. Two canonical versions of ES are (µ/ρ + λ) and (µ/ρ, λ) evolution strategies. If ρ = 1,
we have ES cases without recombination, which are denoted by (µ + λ) and (µ, λ) ESs. In the case of
ρ = 1, the recombination is simply making a copy of the parent. In these notations, λ and µ indicate
the size of the offspring and population respectively. The main difference between GA and ES is related
to the selection step. In GA, at each iteration, the next generation is formed by selecting highly-valued
individuals, while the size of the population is kept ﬁxed [26]. In ES, a temporary population with the
size of λ is created and the individuals in this temporary population undergo crossover and mutation at
user-set probabilities regardless of their ﬁtness values. In (µ + λ) ES, then, both parents and the generated
offspring resulting from the temporary population are copied to a selection pool—with size (µ + λ)—and
a new population with size µ is formed by selecting the best individuals. While, in (µ, λ) ES, the new
generation with size µ is selected only from the offspring (with size λ). Therefore, a convergence condition
as µ < λ is required to guarantee an optimal solution [27].

Particle swarm optimization is one of the most common representative of the swarm intelligence (SI)
algorithms, which form a big class of nature-inspired optimization methods alongside the evolutionary
algorithms. The SI algorithms present the concept of collective intelligence, which is mainly deﬁned as a
collective behavior in a group of individuals that seem intelligent. SI algorithms have been inspired from
collective behavior and self-organizing interactions between living agents in the nature, e.g., ant colonies
and honey bees [28].

PSO is an optimization method simulating the collective behavior of certain types of living species. In
PSO, cooperation is an important feature of the system as each of the individuals changes its searching
pattern based on its own and others’ experiences. PSO starts with a swarm of random particles. Each
particle has a position and velocity vectors, which are updated w.r.t the local and global best values. The
best values get updated at each iteration. In the application of PSO, each particle (individual) represents a
potential solution and is often modeled as a vector containing n elements, in which each element represents
a variable of the problem that is being optimized. Like GA, PSO searches for the optimal solution through
updating solutions and creating subsequent generations, though without using evolution operators [29].
The position (the elements) and velocity of each particle are updated as follows:

P t+1 = P t + V t+1

V t+1 = wV t + c1r1(P t

best − P t) + c2r2(Gt

best − P t)

(1)

(2)

where P t and V t are the position and velocity of the particle at iteration t, respectively; P t
best
are the local best position of the particle and the global best one up to the iteration t. The ﬁrst part of (2)
is perceived as inertia, which indicates the tendency of the particle to keep moving in the same direction,
while the second part—which reﬂects a cognitive behavior—indicates the tendency towards the local best
position discovered by the particle and the last part—which is the social knowledge—reﬂects the tendency
to follow the best position found so far by other particles.

best and Gt

5

Fig. 1: A test scenario executed in the BeamNG simulator.

Therefore, at each iteration, the position of each particle is updated based on its velocity, and the velocity
is controlled by the inertia and accelerated stochastically towards the local and global best values. r1 and
r2 are random weights from range (0, 2] which adjust the cognitive and social acceleration. In (2), w is
inertia weight, which adjusts the ability of the swarm to change the direction and makes a balance between
the level of exploration and exploitation in the search process. A lower w leads to more exploitation of the
best solutions found, while a higher value of w facilitates more exploration around the found solutions.
c1 and c2 are the acceleration hyperparameters deﬁning to what extent the solutions are inﬂuenced by the
local best solutions and global best solution. These hyperparameters and the inertia weight could be static
or changed dynamically over the iterations. For instance, w = 0.72984 and c1 + c2 > 4 is a common
setup for a static conﬁguration of these parameters [30].

III. DEEPER: A BIO-INSPIRED SIMULATION-INTEGRATED TESTING FRAMEWORK
This section presents the technical details of Deeper and shows how it challenges a DNN-based lane-
keeping system in a self-driving car trained and tested in the BeamNG simulator environment [12]. The
subject system is a built-in AI driving agent encompassing a steering angle predictor (ML model) which
receives images captured by an onboard camera in the simulation environment. Then, the test inputs (test
cases) generated by Deeper are deﬁned as scenarios in which the car drives. Our target is to generate
diverse test scenarios triggering the misbehavior of the subject system. In this regard, we beneﬁt from bio-
inspired search-based techniques to explore the input space and generate highly-valued failure-revealing
test scenarios.

A. Test Scenario and failure speciﬁcation

Test scenarios are deﬁned as combinations of roads, the environment including e.g., the weather and
illumination, and the driving path, i.e., starting and end points and the lane to keep. Hereafter, we consider
scenarios involving a single asphalt road surrounded by green grass where the car is to drive on the right
lane, and the environment is set to a clear day with the overhead sun (See Fig. 1). Therefore, the focus of
Deeper is to generate diverse roads which trigger failures in the system under test. In this system, failure
is deﬁned in terms of an episode, in which the car drives partially outside the lane meaning that X% of
the car’s bounding box gets outside the lane. X is a conﬁgurable tolerance threshold for Deeper.

In the test scenarios, the road composes two ﬁxed-width lanes with a yellow center line and two white
lines separating the lanes from the non-drivable area. In BeamNG, each road is mainly described by a set
of points that are used by the simulation engine to render the road. The simulation engine accomplishes
the rendering by interpolating the points and creating a sequence of polygons on the points—provided
by the SBST competition setup 2. Notably, not every sequence of road points results in valid roads, so
each sequence of points is also validated against some initial geometrical constraints related to the road

2https://github.com/se2p/tool-competition-av.git.

6

polygons and some other domain-speciﬁc constraints. The main constraints are: 1) the start and end points
of the road shall be different, 2) the road shall be completely contained in the map used in the simulation,
3) the road shall not self-intersect, and 4) the road shall not contain too sharp turns that force the vehicle
to invade the opposite lane. To assure the satisfaction of these constraints, Deeper validates the generated
roads before getting executed and consequently, the invalid roads are not counted as failed test scenarios.
Road Representation Model: In order to convert the abstract road model into a proper set of points
that can be rendered by the simulation engine—as candidate solutions in the test generator—we rely on
the representation model used by DeepJanus [22] based on Catmull-Rom cubic splines [23]. Therefore,
each road is represented by two sets of points, control points and sample points. First, control points are
provided as an input speciﬁcation for a candidate road. Second, sample points are calculated using the
Catmull-Rom calculation algorithm. Third, the simulation engine uses the sample points, if they are valid,
to render the road. Fig. 2 shows the representation model of a road in terms of control and sample points
(2a and 2b) and the corresponding rendered road in the simulation (2c).

CP = (cid:104)C1, C2, · · · , Cm(cid:105) , Rimin ≤ Ci ≤ Rimax
Rimin, Rimax ∈ R

SP = (cid:104)S1, S2, · · · , Sn(cid:105) , SP = Catmull Rom Spline(CP )

Rimin ≤ Si ≤ Rimax
Rimin, Rimax ∈ R

(3)
(4)

(5)
(6)
(7)

(a) Control Points.

(b) Calculated Sample Points.

(c) The corresponding road in the simulation.

Fig. 2: The representation model of a road

7

B. Fitness Function

As indicated above, the focus is the generation of diverse failure-revealing test scenarios w.r.t the
intended tolerance threshold. The competition setup elaborates the positional data and detects the episodes
in which the car breaks out of the lane bounds, i.e., out of bound episodes (OBE). It computes the distance
of the car from the center of the lane in those OBE episodes and reports an OBE failure [31] each time
that the car drives outside the lane if the percentage of the area of the car that is outside the lane is bigger
than the intended threshold (referred to as X in Section III-A).

The problem is regarded as an optimization problem, and in order to generate valuable test scenarios
leading us to meet the target, we evaluate the quality of the test scenarios using a ﬁtness (objective)
function. In this regard, for each test scenario, the main objective of interest to be optimized is the
maximum detected distance of the car from the center of the lane during driving of the car on the lane.
So, more accurately the ﬁtness function that we want to minimize is as follows:

F itness F unction = (lane width)/2 − d(center polyline, car position)

(8)

where lane width is the width of the lane and d(spin polyline, car position) indicates the distance of
the car position from the central polyline (center) of the lane.

C. Bio-Inspired Search Algorithms

We are interested in sampling from the space of possible test scenarios in an effective way to generate
those that lead to the emergence of failures. This can be achieved by using an optimization algorithm to
guide the search by the ﬁtness function. Therefore, in order to ﬁnd the solutions of interest, we use bio-
inspired search-based algorithms, i.e., GA, (µ+λ) and (µ, λ) ESs, and PSO, guided by the ﬁtness function
introduced in Sec.III-B. These search algorithms are mainly modeled on the basis of population evolution
over time and they usually get started with the creation of a random population of solutions. In Deeper,
we leverage an initial quality population seed to boost the search process regarding the fact that the search
is done at a ﬁxed test budget. Throughout the development, the impact of different initial population seeds
was investigated. For instance, starting from an initial random population seed was not quite effective to
lead the search to ﬁnd the failure-revealing solutions w.r.t high tolerance thresholds within a reasonable
test budget. The quality population seed used in the current search algorithms of Deeper is a mix of some
valid random solutions and some extracted randomly from the solutions generated by a 5-hour execution
of the ﬁrst version of Deeper (based on NSGA-II [32]) that start from an initial random population seed.
Then, the generated solutions which could cause OBEs w.r.t a tolerance threshold τ ≥ 0.5 are considered
to be used to form the quality population seed.

1) Genetic Algorithm: The GA-driven test generator in Deeper starts with forming an initial population
by sampling from the quality population seed. Over various generations, new populations of test scenarios
are formed through applying crossover and mutation operations to the best ones selected from the previous
generations.

Selection: We use tournament selection for identifying the promising test scenarios, which have a high
probability to lead to failures and safety violations. In tournament selection, a subset of the population is
selected at random in each tournament and the best test scenario of the subset is picked. The number of
individuals participating in each tournament indicates the size of the tournament.

Crossover: We develop a domain-speciﬁc one-point crossover operation to create new test scenarios,
i.e., new test roads, from the ones selected from the previous generation. The proposed crossover operation
performs the segment exchange at the sets of control points, which means that a random point is selected
as the crossover point in the sets of control points in the parent roads, then the parts of the sets beyond
the crossover point are swapped between the parents, and accordingly, two new sets of control points for
two child roads are formed. The corresponding sample points for the generated child roads are calculated
using the Catmull-Rom calculation algorithm (See Fig. 3).

8

(a) Parent Road 1: Control Points.

(b) Child Road 1: Control Points.

(c) Child Road 1: Sample Points.

(d) Parent Road 2: Control Points.

(e) Child Road 2: Control Points.

(f) Child Road 2: Sample Points.

Fig. 3: Crossover operation on two sample roads

However, still, these resulting sets of points might not represent valid roads w.r.t the geometrical
constraints, so before adding these new test scenarios to the offspring, we also check their validity and
let only the resulting valid roads be added to the offspring. If both generated child roads are valid, then
both of them will be transferred to the offspring, while if one of them is valid, we keep the valid child
and another crossover point is tried to breed the parent roads and generate the second valid child.
Likewise, if none of the child roads are identiﬁed as valid roads, another crossover point is tried. All in
all, in order to generate two valid child roads from a crossover operation, at most ﬁve attempts to try
with different crossover points are done. If in the end, the attempts do not lead to valid child roads, the
whole process will be rolled back and the original parents will be added to the offspring.

Mutation: The mutation operation also targets the coordinate values of the control points. We use
Polynomial Bounded mutation, a bounded mutation operation for real-valued individuals which was used
in NSGA-II [32]. It features using a polynomial function for the probability distribution and a user-set
parameter, η, presenting the crowding degree of the mutation and adjusting the diversity in the resulting
mutant. A high value for η results in a mutant resembling the original solution, while a low η leads to
a more divergent mutant from the original. This domain-speciﬁc polynomial bounded mutation operation
selects randomly a point—mutation point—in the set of control points and mutates randomly the x or y
coordinate of the selected control point (See Fig. 4). Accordingly, the sample points are re-calculated for
the mutated set of control points, and their validity w.r.t the geometrical constraints are checked. In case
the mutant does not represent a valid road, another control point is tried. The GA-driven test generator
of Deeper is conﬁgured as presented in Algorithm 1.

2) Evolution Strategies: The ES-driven test generators in Deeper use two canonical (µ + λ) and (µ, λ)
ES algorithms. The ES-driven test generators start with initializing a population with µ test scenarios
sampled from the population seed. The ﬁtness value of each test scenario is calculated through rendering
them in the simulation. Next, a temporary population with size λ is formed by the reproduction of test
scenarios from the original population. This temporary population is used to create offspring by applying
the proposed domain-speciﬁc crossover or mutation operations (See Section III-C1 ) to its individuals
according to user-set crossover and mutation probabilities. In (µ + λ) ES, then both parents and the
resulting offspring are copied to a selection pool, and a new population with size µ is created by using
tournament selection. While, in (µ, λ) ES-driven test generator, the new population is selected only from

9

(a) Original road.

(b) Mutated road.

Fig. 4: Mutation operation on the road

Algorithm 1 GA-driven test generator in Deeper

1. Initialize a population of test scenarios (with size = 70) from the quality population seed
2. Evaluate the test scenarios through rendering them in the simulation and computing the ﬁtness
values
repeat

3. Select highly-valued scenarios using tournament selection (T ournament size = 3)
4. Create offspring by using crossover and mutation operations

4.1. Apply the domain-speciﬁc crossover operation (Crossover rate = 0.3)
4.2. Apply the domain-speciﬁc polynomial Bounded mutation operation (M utation rate = 0.7)

5. Evaluate the offspring

until reaching the end of the test budget (e.g., given time);
6. Collect the test scenarios revealing OBE failures

the offspring. Algorithm 2 presents the procedure of (µ + λ) ES-driven test generator, where µ = 70
and λ = 30. The procedure of (µ, λ) ES test generator is almost the same as (µ + λ) ES, and the
main difference is in the selection step, where the µ highly-valued individuals are selected only from the
offspring in (µ, λ) ES. However, with regard to the required convergence condition (µ < λ) in this case,
then those parameters in (µ, λ) ES test generator are conﬁgured as µ = 70 and λ = 100 (the size of the
main population is ﬁxed at both ES-driven test generators).

3) Particle Swarm Optimization: The PSO-driven test generator in Deeper starts with initializing a
population of particles from the quality population seed. Each test scenario is modeled as a particle, and
the set of control points represents the position vector of the particle that is updated according to Eq. (1)
and (2) over various generations. After each updating, the corresponding sample points for the updated
set of control points are calculated and the validity of the new set of road points is checked against the
geometrical constraints. The PSO-driven test generator is conﬁgured as presented in Algorithm 3 and
based on the following setting for the parameters, which is w = 0.8, c1 = 2.0, and c2 = 2.0.

IV. EMPIRICAL EVALUATION
We conduct an empirical evaluation of the proposed simulation-integrated bio-inspired test generators
in Deeper, by running experiments on an experimental setup based on a PC with 64-bit Windows 10 Pro,
Intel Core i7-8550U CPU @ 1.80GHz, 16GB RAM, Intel UHD Graphics 620, and BeamNG.tech driving
simulator together with the software requirements for running Deeper3.

3See requirements at https://github.com/mahshidhelali/Deeper ADAS Test Generator.git

10

Algorithm 2 (µ + λ) ES-driven test generator in Deeper

1. Initialize a population P with µ test scenarios sampled from the quality population seed
2. Evaluate the test scenarios in the population P through simulation and computing the ﬁtness values
repeat
3. Create a temporary population PT with size λ by reproduction of test scenarios from the
population P
4. Create an offspring by applying the crossover or mutation operation to the test scenarios in the
population PT (with crossover probability CxP = 0.3 and mutation probability M uP = 0.7)

4.1. choice = Random.random(), 0 < choice < 1
4.2. If choice < CxP then

Apply the domain-speciﬁc crossover operation

else if choice < CxP + M uP

Apply the domain-speciﬁc polynomial Bounded mutation operation

5. Evaluate the offspring
6. Select µ highly-valued test scenarios using tournament selection (T ournament size = 3) from
the original population P and the offspring

until reaching the end of the test budget (e.g., given time);
7. Collect the test scenarios revealing OBE failures

Algorithm 3 PSO-driven test generator in Deeper

1. Initialize a swarm of test scenario particles (with size = 70) from the quality population seed
2. Evaluate the particles of the swarm through simulation and computing the ﬁtness values
3. Select the global best particle w.r.t the ﬁtness value (Gbest)
repeat

for each test scenario particle P in the swarm do

4. Calculate the particle’s velocity according to eq. (2)
5. Update particle’s position according to eq. (1)
6. Evaluate the particle based on the ﬁtness function
if ﬁtness value of P is better than the local best of P, (Pbest), then

7. Update Pbest with P

end
if ﬁtness value of P is better than the global best, (Gbest), then

8. Update Gbest with P

end

end

until reaching the end of the test budget (e.g., given time);
9. Collect the test scenario particles revealing OBE failures

Test Subject: The system under test

is BeamNG’s built-in driving agent, BeamNG.AI. It

is an
autonomous agent utilizing optimization techniques to plan the driving trajectory according to the speed
limit while keeping the ego-car inside the road lane. It is equipped with a DL-based lane-keeping
ADAS. The DL-based lane-keeping system learns a mapping from the input of the onboard camera in
the simulated environment to the steering angle. It is based on the DAVE-2 architecture including a
normalization layer, ﬁve convolutional layers followed by three fully connected layers [33]. This test
subject has been used in previous research and also in the SBST 2021 cyber-physical tool competition
for evaluating test scenario generators [22], [31], [34], and moreover does not require manual training,
which can mitigate the threats to the validity of the results [34].

11

Fig. 5: An overview of the experimental setup

A. Research Method

We design and implement a set of experiments to answer the research questions:
1) RQ1: How capable are these test generators to trigger failures?
2) RQ2: How diverse are the generated failure-revealing test scenarios?
3) RQ3: How effectively and efﬁciently do the test generators perform, i.e., given a certain test budget
how many test scenarios are generated, what proportion of the scenarios is valid, and what proportion
of the valid test scenarios leads to triggering failures?

The experiments are simulation scenarios generated by a Python test scenario generator and executed
by the simulation engine. BeamNG.AI is the autonomous driving agent controlling the ego car in the
simulation (Fig. 5). In order to provide quantitative answers to the RQs, we use the following quality
criteria to assess the bio-inspired test scenario generators in Deeper:

• Detected Failures: The number of generated test scenarios that lead to failures, w.r.t the given tolerance

threshold.

• Failure Diversity: The dissimilarity between the test scenarios that lead to the failures. Generating
diverse failure-revealing test scenarios is of interest, since triggering the same failures multiple times
results in wasting the test budget, e.g., computation resources. In order to measure the failure diversity,
we rely on a two-step strategy adopted by the SBST 2021 tool competition. It extracts, ﬁrst, the road
segments related to the failures, then computes the sparseness, which is considered as the average
of the maximum Levenshtein distance [35] between those road segments.
The failure-related road segments are referred to as the parts of the road 30 meters before the OBE
and 30 meters after it, and accordingly, the sparseness is calculated as follows:

Sparseness =

(cid:80)

i∈OBEs maxj∈OBEs Lev dist(i, j)
|OBEs|

(9)

where Lev dist(i, j) indicates the weighted Levenshtein distance between the road segments.

• Test generation efﬁciency and effectiveness: It indicates how the test generator uses the given test
budget to generate the test scenarios, in particular how many test scenarios are generated in total,
what fraction of them are valid, and what fraction of the valid ones triggers failures.

Experiments: We design two sets of experiments as implemented in the SBST 2021 CPS tool
competition. In order to provide a comparative analysis, we compare the results of the proposed test
generators in Deeper with the presented test generators in the tool competition, i.e., Frenetic [24],
GABExploit and GABExplore [21], Swat [25], and also the earlier version of Deeper [19]. We run the
test generators on the test subject based on the same two experiment conﬁgurations as in the
competition, which are shown in table I. The SET1 of experiments provides a 5-hour test generation

budget, meanwhile sets the failing tolerance threshold to a high value, 0.95, and does not consider any
speed limit. This experiment conﬁguration might lead to a more careless style of driving. The SET2 of
the experiments allocates a shorter time budget for the test generation and considers a lower tolerance
threshold, 0.85, while imposing a speed limit of 70 km/h—promoting a more careful driving style. To
ensure a fair comparison, we run each tool the same number of times on the same dedicated machine.
We run each test generator 5 times in experiment conﬁguration SET1 and 10 times in SET2 and report
distributions of the results.

12

TABLE I: Experiment conﬁgurations

Name

Test Budget
(h)

Map
(m2)

Size

Speed Limit
(Km/h)

(careless

(cautious

SET1
driving)
SET2
driving)

5h

2h

200 × 200

None

200 × 200

70 Km/h

Failing
Tolerance
Threshold
(%)

0.95

0.85

This sections reports results corresponding to the three RQs.

V. RESULTS AND DISCUSSION

A. Detected Failures (RQ1)

Figure 6 reports the number of triggered failures by each of the tools in experiment conﬁguration SET1.
In this regard, (µ+λ) ES-driven test generator in Deeper could successfully trigger at least 2X more OBEs
than the highest record—held by GABExploit—in the competition. At the same time, Deeper (µ + λ)
ES showed more consistent performance over the runs, i.e., with lower standard deviation, compared to
GABExploit, which revealed highly different behavior across the runs (e.g., returning over 100 OBEs in
some runs, but failed to trigger any failure in other runs [34]). PSO- and GA-driven test generators in
Deeper, in half of the cases and also on average, triggered higher numbers of failures than the competition’s
test generators—except for GABExploit, which showed comparable results. Moreover, the PSO- and GA-
driven approaches were able to trigger the failures in which the ego-car invades the opposite lane of the
road—a type of failure that has been typically considered difﬁcult to trigger in most of the test generator
tools [34]. Lastly, in SET1, Deeper (µ, λ) ES clearly gave a better performance, in terms of the number
of detected failures, than Deeper NSGA-II, GABExplore, and Swat.

Similarly, ﬁgure 7 presents the number of triggered failures in experiment conﬁguration SET2. With
regard to the limited time budget, the speed limit of 70 km/h and the tolerance threshold 0.85, in
contrast to Deeper NSGA-II, GABExplore, GABExploit, and Swat, none of the newly proposed test
generators in Deeper left an experiment without triggering any failure or with a very low number of
failures (i.e., less than 3), which means they are able to detect failures even within a limited test budget
and strict constraints such as setting a speed limit (See Table II). It is noted that Deeper NSGA-II,
triggered just equal or less than 1 failure in a considerable number of
GABExplore, and Swat
experiments done based on conﬁguration SET2. Therefore, all
the new test generators in Deeper
outperform Deeper NSGA-II, GABExplore, and Swat w.r.t the number of triggered failures within a
limited test time and moreover the PSO-driven test generator results in a very comparable number of
detected failures to Frenetic. Additionally, in the experiment conﬁguration SET2, the GA-driven test
generator is still able to trigger the failures showing the invasion of the car to the opposite lane of the
road.

13

Fig. 6: Number of detected failures in SET1

Fig. 7: Number of detected failures in SET2

B. Diversity of Failures (RQ2)

Figure 6 depicts the diversity of the detected failures in SET1 in terms of the distribution of the
failures’ sparseness. All the new test generators in Deeper resulted in a considerable improvement on
failure sparseness compared to the ﬁrst version, Deeper NSGA-II in the competition. Meanwhile, among
the new test generators of Deeper, the PSO- and GA-driven test generators lead to a higher average
sparseness compared to the (µ + λ) and (µ, λ) ES approaches. Deeper PSO, (µ + λ), and GA also show a
higher sparseness than GABExploit—both on average and in almost half of the cases. However, still the
failure diversity—in terms of the special sparseness metric deﬁned by the competition—promoted by the
new test generators in Deeper are not as high as Frenetic, GABExplore, and Swat. Note that GABExplore
and Swat in 20% of the experiments did not report any sparseness ﬁgure, since they just triggered one
failure in each of those experiments.

Figure 9 shows the distribution of the failures’ sparseness in SET2. In the limited test budget and strict
driving constraints, all the newly test generators again show a big improvement on promoting failure
sparseness in comparison to the ﬁrst version, Deeper NSGA-II. In the meantime, Deeper PSO, (µ + λ),
and GA promote comparable levels of failure diversity, though more consistent, compared to GABExploit.
At the same time, GABExplore and Swat in around 70% of the experiments in SET2 did not provide any
sparseness ﬁgure, since they triggered just one or zero failure (See Table II).

14

TABLE II: Triggered failures in SET2

Test Generator

Exp.1

Exp.2

Exp.3

Exp.4

Exp.5

Exp.6

Exp.7

Exp.8

Exp.9

Deeper NSGA-II
Frenetic
GABExplore
GABExploit
Swat
Deeper PSO
Deeper (µ, λ) ES
Deeper (µ + λ) ES
Deeper GA

0
12
0
84
0
54
8
29
11

0
8
1
18
0
8
4
5
7

0
9
1
126
0
13
3
9
11

1
11
0
38
0
13
5
4
15

1
19
1
0
5
10
7
4
14

0
10
3
11
1
9
7
7
9

0
15
0
47
1
10
3
19
25

0
20
0
0
2
15
5
3
9

3
6
3
28
1
18
7
14
14

Exp.
10
0
23
0
5
2
12
9
5
20

Fig. 8: Failure diversity in terms of sparseness in SET1

C. Test Effectiveness and Efﬁciency (RQ3)

In SBST competition, test effectiveness and efﬁciency were indicated by how many test scenarios are
generated and what proportion of the scenarios is valid, given a certain test budget. They basically show
how well the test generator is able to utilize the test budget. Figures 10 and 11 report the average number
of total test scenarios, as well as the number of valid and invalid scenarios generated by each tool in
SET1 and SET2 respectively. Generally, the new test generators in Deeper utilize the test budget more
efﬁciently than the competition tools and generate a higher number of test scenarios within the given test
time. In this regard, Deeper PSO results in the highest efﬁciency (e.g., generates more than 650 scenarios
on average within 5 hours) among all the test generators in both experimental conﬁgurations. Regarding
the number of valid test scenarios, all the Deeper test generators along with Swat lead to an almost
comparable number of valid scenarios. However, with respect to the ratio of the valid test scenarios to
the total generated ones—called test effectiveness according to the competition evaluation—Swat, Deeper
NSGA-II, and GABExploit are the ones showing the highest result.

To answer RQ3 on test effectiveness and efﬁciency, in addition to the metrics deﬁned and used by
the competition, we deﬁned an extra metric, aggregated test effectiveness called effectiveness plus, which
indicates what proportion of the valid test scenarios leads to triggering failures. It is deﬁned as the ratio of
the triggered failures to the number of valid test scenarios and is intended to present the effectiveness of the
test generators w.r.t meeting the target—detecting failures. Figures 12 and 13 report the test effectiveness
plus for the test generators in SET1 and SET2 respectively. With respect to the effectiveness plus, Deeper
(µ + λ) ES-driven test generator results in the highest target-based effectiveness in SET1 and then Deeper
PSO and GA are the next most effective tools. In SET2, GABExploit shows the highest effectiveness
plus, while Frenetic, Deeper PSO and GA are the next most effective ones. It is worth noting that as

15

Fig. 9: Failure diversity in terms of sparseness in SET2

Fig. 10: Test generation effectiveness and efﬁciency in SET1

shown in Fig. 12 and 13 both Deeper PSO- and GA-driven test generators keep their effectiveness in
generating failure-revealing scenarios in both experimental conditions of SET1 and SET2, which means
they are effective test generators even within a limited test budget and strict driving constraints.

D. Threats to Validity

The evaluation of Deeper comes with a set of threats to construct, internal, and external validity of the

results.
Construct validity: The choices of the ﬁtness function—the distance of the car position from the center of
the lane—and also the metrics used for calculating the sparseness and indicating the diversity of the test
scenarios—weighted Levenshtein distance—in this study are domain-speciﬁc. However, we have based
our choices on the sound metrics adopted by other research works in the literature [22], [31], [34].
Internal validity: The randomized nature of the used bio-inspired algorithms could be a source of threats
to the internal validity of the results. In this regard, we follow the guidelines given by Arcuri and Briand
[36] for the evaluation and analysis of the results, and mitigate this threat by running the experiments
multiple times (e.g., 5 times in SET1 and 10 times in SET2), reporting the distribution and statistics of
the results (e.g., using Box plots to show the results), using same algorithm settings, e.g., population size,
crossover and mutation probabilities, in the proposed test generators.

16

Fig. 11: Test generation effectiveness and efﬁciency in SET2

Fig. 12: Test generation effectiveness plus in SET1

External validity: The choice of the test subject system is a potential threat to the external validity of
the results. However, the test system in this study is one of the main and commonly used systems in
self-driving cars, and furthermore different ML models with various quality levels (i.e., different accuracy
levels) could be deployed within the BeamNG.AI agent, and in this regard, the proposed test scenario
generation techniques can still be used. Nonetheless, it still offers one type of DL-based systems in self-
driving cars, and further studies are required to address the testing of other DL-driven systems, meanwhile,
we also keep the tool open for extensions, for example, to support the execution of test scenarios in other
simulators.

VI. RELATED WORK
Input data assurance alongside the model and integration testing [8], [37] are considered different test
levels investigated by various research works for ML systems. Model testing could be regarded as unit
testing for ML components and integration testing focuses on the issues emerging after the integration of
the ML model into the system. Regarding access to the test subject, there are black-box and white-box
testing approaches analogous to traditional non-ML systems. Black-box testing involves access only to the
ML inputs and outputs, while the white-box testing implies access to the internal architecture of the test
ML subject, code, hyperparameters, and training/test data. However, Riccio et al. in [37] also introduced

17

Fig. 13: Test generation effectiveness plus in SET2

another type of ML testing called data-box, which requires access to data plus everything that a black-box
test requires.

Test input data that can reveal failures in the test subject is the most common generated test artifact in
the literature related to testing of automotive AI systems [37]. Depending on the test level and the test
subject, the inputs could be images, for instance, as used in DeepTest [38], or test scenario conﬁgurations
as used in [20]. A brief overview of the most common techniques used to generate the test data is as
follows:

Input data mutation. This type of mutation involves generating new inputs based on the transformation
of the existing input data. For instance, DeepXplore [39] uses such input transformations to ﬁnd the inputs
triggering different behaviors between similar autonomous driving DNN models, while also striving to
increase the level of neuron coverage. Moreover, in many studies, those transformations are based on
metamorphic relations. DeepTest [38] applies different transformations to a set of seed images with the
aim of increasing neuron coverage and uses metamorphic relations to ﬁnd the erroneous behaviors of
different Udacity DNN models for self-driving cars. DeepRoad [40] uses a GAN-based metamorphic
testing technique to generate input images to test three autonomous driving Udacity DNN models. It
deﬁnes the metamorphic relations such that the driving behavior in a new synthesized driving scene is
expected to be consistent with the one in the corresponding original scene.

Test scenario manipulation. Another major category of the methods to generate test input data is
based on the manipulation and augmentation of the test scenarios. Most of the works in this category
use search-based techniques to go through the search space of the scenarios to ﬁnd the failure-revealing
or collision-provoking test scenarios. In this regard, simulators as a form of digital twins have played a
key role to generate and capture those critical failure-revealing test scenarios. Simulation-based testing
can act as an effective complementary solution to ﬁeld testing, since exhaustive ﬁeld testing is expensive,
meanwhile inefﬁcient, and even dangerous, in some cases. Recently, various high-ﬁdelity simulators such
as the ones using physics-based models (e.g., SVL simulator [41], Pro-SiVIC [42], and PreScan [43]) and
the ones based on game engines (e.g., BeamNG.tech [12] and CARLA [13]) have considerably contributed
to this area by providing the possibility of realistic simulations of functionalities in autonomous driving.
Accordingly, various testing approaches relying on the simulators have been presented in the literature
and in this regard, search-based techniques have been frequently used to address the generation of failure-
revealing test scenarios. Abdessalem et al. utilize multi-objective search algorithms such as NSGA-II
[44] along with surrogate models to ﬁnd critical test scenarios with fewer simulations and then at less
computation time for a pedestrian detection system. In a following study [18], they use MOSA [45]—a
many-objective optimization search algorithm— along with objectives based on the branch coverage and

18

some failure-based heuristics to detect undesired and failure-revealing feature interaction scenarios for
integration testing in a self-driving car. Further, in another study [46], they leverage a combination of
multi-objective optimization algorithms (NSGA-II) and decision tree classiﬁer models—referred to as a
learnable evolutionary algorithm—to guide the search-based process of generating critical test scenarios
and also to reﬁne a classiﬁcation model that can characterize the failure-prone regions of the test input
space for a pedestrian detection and emergency braking system. Haq et al. [?] use many-objective search
algorithms to generate critical test data resulting in severe mispredictions for a facial key-points detection
system in the automotive domain. Ebadi et al. [20] beneﬁt from GA along with a ﬂexible data structure to
model the test scenarios and a safety-based heuristic for deﬁning the objective function to test the pedestrian
detection and emergency braking system of the Baidu Apollo (an autonomous driving platform) within
the SVL simulator.

Regarding the impacts of the simulators in this area, Haq et al. [47] provide a comparison between
the results of testing DNN-based ADAS using online and ofﬂine testing. Their results clearly motivate an
increased focus on online testing as it can identify faults that never would be detected in ofﬂine settings—
whereas the opposite does not appear to be likely. Our current study responds to this call, and motivates
our work on systems testing in simulated environments. With regard to a different perspective, Borg et al.
[48] discuss the consistency between the test results obtained from running the same experiments based
on two different simulators and investigate the reproducibility of the results in both simulators. When
running the same testing campaign in PreScan and ESI Pro-SiVIC, the authors found notable differences
in the test outputs related to revealed safety violations and the dynamics of cars and pedestrians.

VII. CONCLUSION
Deeper in its extended version utilizes a set of bio-inspired algorithms, genetic algorithm (GA),
(µ + λ) and (µ, λ) evolution strategies (ES), and particle swarm optimization (PSO),
to generate
failure-revealing test scenarios for testing a DL-based lane-keeping system. The test subject is an AI
agent in BeamNG.tech’s driving simulator. The extended Deeper contains four new bio-inspired test
leverage a quality seed population and domain-speciﬁc cross-over and mutation
generators that
operations tailored for the presentation model used for modeling the test scenarios. Failures are deﬁned
as episodes where the ego car drives partially out of the lane w.r.t a certain tolerance threshold. In our
empirical evaluation we focused to answer three main questions as ﬁrst how many failures the test
generators can detect, second how much diversity they can promote in the failure-revealing test
scenarios, and third how effectively and efﬁciently they can perform, w.r.t different target failure severity
(i.e., in terms of tolerance threshold), available test budget, and driving style constraints (e.g, setting
speed limits). Our results show that the newly proposed test generators in Deeper present a considerable
improvement on the previous version and they are able to act as effective and efﬁcient test generators
that provoke a considerable number of diverse failure-revealing test scenarios for testing an ML-driven
lane-keeping system. They show considerable effectiveness in meeting the target, i.e., detecting diverse
failures, with respect to different target failures intended and constraints imposed. In particular, they act
as more reliable test generators than most of the counterpart tools for provoking diverse failures within
a limited test budget and with respect to strict constraints.

As some directions for future work, we plan to apply the proposed approaches to testing further types
of ML-based lane-keeping systems, i.e., more industrial ones and also in other state-of-the-art simulation
platforms. We also plan to extend the approaches by applying machine learning-based techniques such
as reinforcement learning or Generative Adversarial Networks (GANs) for empowering the discovery of
failure-revealing test scenarios.

ACKNOWLEDGMENT
This work has been funded by Vinnova through the ITEA3 European IVVES (https://itea3.org/project/
ivves.html) and H2020-ECSEL European AIDOaRT (https://www.aidoart.eu/) and InSecTT (https://www.

19

insectt.eu/) projects. Furthermore, the project received partially ﬁnancial support from the SMILE III
project ﬁnanced by Vinnova, FFI, Fordonsstrategisk forskning och innovation under the grant number:
2019-05871.

REFERENCES

[1] M. Borg, C. Englund, K. Wnuk, B. Duran, C. Levandowski, S. Gao, Y. Tan, H. Kaijser, H. L¨onn, and J. T¨ornqvist, “Safely entering
the deep: A review of veriﬁcation and validation for machine learning and a challenge elicitation in the automotive industry,” Journal
of Automotive Software Engineering, vol. 1, no. 1, pp. 1–19, 2019.

[2] M. Borg, J. Bronson, L. Christensson, F. Olsson, O. Lennartsson, E. Sonnsj¨o, H. Ebabi, and M. Karsberg, “Exploring the assessment
list for trustworthy ai in the context of advanced driver-assistance systems,” ser. 2021 IEEE/ACM 2nd International Workshop on Ethics
in Software Engineering Research and Practice (SEthics).

IEEE, 2021, pp. 5–12.

[3] E. J. Topol, “High-performance medicine: the convergence of human and artiﬁcial intelligence,” Nature medicine, vol. 25, no. 1, pp.

44–56, 2019.

[4] J. Lee, H. Davari, J. Singh, and V. Pandhare, “Industrial artiﬁcial

intelligence for industry 4.0-based manufacturing systems,”

Manufacturing letters, vol. 18, pp. 20–23, 2018.

[5] M. Cummings and D. Britton, “Regulating safety-critical autonomous systems: past, present, and future perspectives,” Living with

robots, pp. 119–140, 2020.

[6] S. Burton, I. Habli, T. Lawton, J. McDermid, P. Morgan, and Z. Porter, “Mind the gaps: Assuring the safety of autonomous systems

from an engineering, ethical, and legal perspective,” Artiﬁcial Intelligence, vol. 279, p. 103201, 2020.

[7] “Ethics guidelines

for

trustworthy AI,” European Commission, Available

at https://digital-strategy.ec.europa.eu/en/library/

ethics-guidelines-trustworthy-ai, Retrieved August, 2021.

[8] R. Hawkins, C. Paterson, C. Picardi, Y. Jia, R. Calinescu, and I. Habli, “Guidance on the assurance of machine learning in autonomous

systems (amlas),” arXiv preprint arXiv:2102.01564, 2021.

[9] Andrej Karpathy, “Software 2.0,” Available at https://karpathy.medium.com/software-2-0-a64152b37c35, Retrieved August, 2022.
[10] “Road Vehicles - Safety of the Intended Functionality,” International Organization for Standardization, Tech. Rep. ISO/PAS 21448:2019,

2019.

[11] “Road vehicles — Functional safety,” International Organization for Standardization, Tech. Rep. ISO 26262-1:2018, 2018.
[12] BeamNG GmbH, “BeamNG.research,” https://beamng.gmbh/research/, Retrieved March, 2022.
[13] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open urban driving simulator,” ser. Conference on robot

learning. PMLR, 2017, pp. 1–16.

[14] G. Rong, B. H. Shin, H. Tabatabaee, Q. Lu, S. Lemke, M. Moˇzeiko, E. Boise, G. Uhm, M. Gerow, S. Mehta et al., “Lgsvl simulator:
A high ﬁdelity simulator for autonomous driving,” ser. 2020 IEEE 23rd International Conference on Intelligent Transportation Systems
(ITSC).

IEEE, 2020, pp. 1–6.

[15] F. Rosique, P. J. Navarro, C. Fern´andez, and A. Padilla, “A systematic review of perception system and simulators for autonomous

vehicles research,” Sensors, vol. 19, no. 3, p. 648, 2019.

[16] R. B. Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing vision-based control systems using learnable evolutionary algorithms,”

ser. 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE).

IEEE, 2018, pp. 1016–1026.

[17] A. Gambi, M. Mueller, and G. Fraser, “Automatically testing self-driving cars with search-based procedural content generation,” ser.
Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 2019, pp. 318–328.
[18] R. B. Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter, “Testing autonomous cars for feature interaction failures using
IEEE, 2018,

many-objective search,” ser. 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE).
pp. 143–154.

[19] M. H. Moghadam, M. Borg, and S. J. Mousavirad, “Deeper at the sbst 2021 tool competition: Adas testing using multi-objective

search,” ser. 2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing (SBST).

IEEE, 2021, pp. 40–41.

[20] H. Ebadi, M. H. Moghadam, M. Borg, G. Gay, A. Fontes, and K. Socha, “Efﬁcient and effective generation of test cases for pedestrian
detection-search-based software testing of baidu apollo in svl,” ser. 2021 IEEE International Conference on Artiﬁcial Intelligence Testing
(AITest).

IEEE, 2021, pp. 103–110.

[21] F. Kl¨uck, L. Klampﬂ, and F. Wotawa, “Gabezier at the sbst 2021 tool competition,” ser. 2021 IEEE/ACM 14th International Workshop

on Search-Based Software Testing (SBST).

IEEE, 2021, pp. 38–39.

[22] V. Riccio and P. Tonella, “Model-based exploration of the frontier of behaviours for deep learning system testing,” ser. Proceedings
of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. ACM, 2020, pp. 876–888.

[23] E. Catmull and R. Rom, “A class of local interpolating splines,” ser. Computer aided geometric design. Elsevier, 1974, pp. 317–326.
[24] E. Castellano, A. Cetinkaya, C. H. Thanh, S. Klikovits, X. Zhang, and P. Arcaini, “Frenetic at the sbst 2021 tool competition,” ser.

2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing (SBST).

IEEE, 2021, pp. 36–37.

[25] D. Humeniuk, G. Antoniol, and F. Khomh, “Swat tool at the sbst 2021 tool competition,” ser. 2021 IEEE/ACM 14th International

Workshop on Search-Based Software Testing (SBST).

IEEE, 2021, pp. 42–43.

[26] A. Slowik and H. Kwasnicka, “Evolutionary algorithms and their applications to engineering problems,” Neural Computing and

Applications, pp. 1–17, 2020.

[27] H.-G. Beyer and H.-P. Schwefel, “Evolution strategies–a comprehensive introduction,” Natural computing, vol. 1, no. 1, pp. 3–52, 2002.
[28] M. Mavrovouniotis, C. Li, and S. Yang, “A survey of swarm intelligence for dynamic optimization: Algorithms and applications,”

Swarm and Evolutionary Computation, vol. 33, pp. 1–17, 2017.

[29] A. Slowik and H. Kwasnicka, “Nature inspired methods and their industry applications—swarm intelligence algorithms,” IEEE

Transactions on Industrial Informatics, vol. 14, no. 3, pp. 1004–1015, 2017.

20

[30] M. Clerc and J. Kennedy, “The particle swarm-explosion, stability, and convergence in a multidimensional complex space,” IEEE

transactions on Evolutionary Computation, vol. 6, no. 1, pp. 58–73, 2002.

[31] A. Gambi, M. M¨uller, and G. Fraser, “Asfault: Testing self-driving car software using search-based procedural content generation,” ser.
IEEE, 2019,

2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion).
pp. 27–30.

[32] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist multiobjective genetic algorithm: Nsga-ii,” IEEE transactions on

evolutionary computation, vol. 6, no. 2, pp. 182–197, 2002.

[33] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End

to end learning for self-driving cars,” arXiv preprint arXiv:1604.07316, 2016.

[34] S. Panichella, A. Gambi, F. Zampetti, and V. Riccio, “Sbst tool competition 2021,” ser. 2021 IEEE/ACM 14th International Workshop

on Search-Based Software Testing (SBST).

IEEE, 2021, pp. 20–27.

[35] V. I. Levenshtein et al., “Binary codes capable of correcting deletions, insertions, and reversals,” ser. Soviet physics doklady. Soviet

Union, 1966, pp. 707–710.

[36] A. Arcuri and L. Briand, “A hitchhiker’s guide to statistical tests for assessing randomized algorithms in software engineering,” Software

Testing, Veriﬁcation and Reliability, vol. 24, no. 3, pp. 219–250, 2014.

[37] V. Riccio, G. Jahangirova, A. Stocco, N. Humbatova, M. Weiss, and P. Tonella, “Testing machine learning based systems: a systematic

mapping,” Empirical Software Engineering, vol. 25, no. 6, pp. 5193–5254, 2020.

[38] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing of deep-neural-network-driven autonomous cars,” ser. Proceedings

of the 40th international conference on software engineering.

IEEE/ACM, 2018, pp. 303–314.

[39] K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox testing of deep learning systems,” ser. proceedings of the 26th

Symposium on Operating Systems Principles. ACM, 2017, pp. 1–18.

[40] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad: Gan-based metamorphic testing and input validation framework
for autonomous driving systems,” ser. 2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 2018, pp. 132–142.

[41] LG Electronics, “SVL Simulator,” https://www.svlsimulator.com/, Retrieved July, 2022.
[42] A. Belbachir, J.-C. Smal, J.-M. Blosseville, and D. Gruyer, “Simulation-driven validation of advanced driving-assistance systems,”

Procedia-Social and Behavioral Sciences, vol. 48, pp. 1205–1214, 2012.

[43] TASS International, “PreScan Simulator,” https://tass.plm.automation.siemens.com/prescan-overview, Retrieved July, 2022.
[44] R. Ben Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing advanced driver assistance systems using multi-objective search and
IEEE/ACM,

neural networks,” ser. Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering.
2016, pp. 63–74.

[45] A. Panichella, F. M. Kifetew, and P. Tonella, “Reformulating branch coverage as a many-objective optimization problem,” ser. 2015

IEEE 8th international conference on software testing, veriﬁcation and validation (ICST).

IEEE, 2015, pp. 1–10.

[46] R. B. Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing vision-based control systems using learnable evolutionary algorithms,”

ser. 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE).

IEEE, 2018, pp. 1016–1026.

[47] F. U. Haq, D. Shin, S. Nejati, and L. Briand, “Can ofﬂine testing of deep neural networks replace their online testing?” Empirical

Software Engineering, vol. 26, no. 5, pp. 1–30, 2021.

[48] M. Borg, R. B. Abdessalem, S. Nejati, F.-X. Jegeden, and D. Shin, “Digital twins are not monozygotic–cross-replicating adas testing in
two industry-grade automotive simulators,” ser. 2021 14th IEEE Conference on Software Testing, Veriﬁcation and Validation (ICST).
IEEE, 2021, pp. 383–393.

