2
2
0
2

r
a

M
8
2

]
L
P
.
s
c
[

2
v
8
5
6
9
0
.
3
0
2
2
:
v
i
X
r
a

Lupa: A Framework for Large Scale
Analysis of the Programming Language Usage

Anna Vlasova
JetBrains Research
anna.vlasova@jetbrains.com

Maria Tigina
JetBrains Research
ITMO University
maria.tigina@jetbrains.com

Ilya Vlasov
Saint Petersburg State University
ilyavlasov2011@gmail.com

Anastasiia Birillo
JetBrains Research
anastasia.birillo@jetbrains.com

Yaroslav Golubev
JetBrains Research
yaroslav.golubev@jetbrains.com

Timofey Bryksin
JetBrains Research
timofey.bryksin@jetbrains.com

ABSTRACT
In this paper, we present Lupa — a platform for large-scale analysis
of the programming language usage. Lupa is a command line tool
that uses the power of the IntelliJ Platform under the hood, which
gives it access to powerful static analysis tools used in modern IDEs.
The tool supports custom analyzers that process the rich concrete
syntax tree of the code and can calculate its various features: the
presence of entities, their dependencies, definition-usage chains,
etc. Currently, Lupa supports analyzing Python and Kotlin, but can
be extended to other languages supported by IntelliJ-based IDEs.
We explain the internals of the tool, show how it can be extended
and customized, and describe an example analysis that we carried
out with its help: analyzing the syntax of ranges in Kotlin.

ACM Reference Format:
Anna Vlasova, Maria Tigina, Ilya Vlasov, Anastasiia Birillo, Yaroslav Gol-
ubev, and Timofey Bryksin. 2022. Lupa: A Framework for Large Scale Anal-
ysis of the Programming Language Usage. In Proceedings of ACM Conference
(Conference’17). ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

1 INTRODUCTION
Software engineering becomes an ever more important part of our
life and enters almost all of the existing industries and fields of
study [11]. To answer the emerging challenges and to incorpo-
rate the earned experience, new programming languages continue
to be developed, and the existing ones are evolving [21, 23, 32].
This process can take many forms: new libraries that implement
novel techniques [28, 42], new language features [18], or even new
programming languages altogether [13, 31].

To help with this process, it is important to keep track of these
features, their changes, and their prevalence [19, 29, 35]. Narrower
topics can also be investigated, such as configuration settings [36],
popularity trends [12], popular testing practices [24], etc.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

Several approaches exist to study language features on a large
scale. Among these, two main groups can be highlighted. The first
one [30, 38–40] is focused on the meta-information about projects,
like the information about the number of stars or collaborators
on GitHub. These tools usually do not have access to the code of
the projects and therefore can not be used to study programming
languages. The second group of tools can take an existing dataset
and perform the analysis of the code itself [17, 22, 29, 35]. This is
important in a large number of cases, since the code contains a
rich set of information about how language constructs are used.
Unfortunately, such platforms mostly focus on just several promi-
nent languages and features within them, do not use industry-level
tools to parse and analyze the source code, and instead opt for
implementing their own specific pipelines. For this reason, while
such tools are in principle extendable, it is often difficult to actually
extend them in practice.

To overcome the described problems of the code-based tools,
in this paper, we present Lupa — an extendable framework for
analyzing fine-grained language usage built on top of the IntelliJ
Platform [26]. Lupa is a command line tool that uses the power of
the IntelliJ Platform [10] under the hood to perform code analysis
using the same industry-level tools that are employed in IntelliJ-
based IDEs [26], such as IntelliJ IDEA [4], PyCharm [9], or CLion [1].
This platform provides a rich concrete syntax tree called Program
Structure Interface (PSI) [6], and an API for its traversal (visitors,
name resolving, finding usages, etc.). Currently, our framework
supports the analysis of two languages: Python — a mature language
most popular in the data science and machine learning domains [3],
and Kotlin — a relatively young but quickly growing language [5].
The most important feature of Lupa is the ease of its extension.
The current implementation is written in Kotlin, but both Java
and Kotlin can be used to extend the tool. To perform a custom
analysis within the supported language, one needs only to add a
new analyzer: the necessary filters expressed in terms of PSI nodes.
For example, to detect all usages of a certain function, one needs
to visit all the nodes that correspond to function calls and filter
them by name. Lupa will then run this analysis on all projects in
the provided dataset and output the results. Our framework can
also be extended to any language that PSI supports (this includes
Java, JavaScript, PHP, C/C++, Go, and many others) [26].

To demonstrate the usefulness of Lupa and to provide an ex-
ample of its usage, we describe a study of range declarations in
Kotlin. In this analysis, we collected a dataset of 10 thousand Kotlin

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Anna Vlasova, Maria Tigina, Ilya Vlasov, Anastasiia Birillo, Yaroslav Golubev, and Timofey Bryksin

projects and wrote several analyzers for Lupa that studied different
syntactic ways of declaring ranges (e.g., X..Y, X.rangeTo(Y), etc.)
and the context of their use. To get the context of the usage, Lupa
traversed the syntax tree of the code to find the necessary parent
node (for example, to see that the range is used within an if condi-
tion). The results show that different kinds of declarations are used
unevenly and in different contexts. This information was used by
the development team of Kotlin to plan further experiments with
the syntax of ranges.

Lupa is available for researchers and practitioners on GitHub:
https://github.com/JetBrains-Research/Lupa. We believe that the
framework can be of use for researchers for a wide array of studies
on source code, as well as for the developers and maintainers of
libraries that are interested in how their features are used.

2 BACKGROUND
A lot of existing research has been dedicated to studying large
corpora of code [12, 15, 19, 24, 29, 35, 36], which involved developing
tools, frameworks, and platforms for their processing [17, 29, 30,
35, 38–40]. In this section, we list the most notable ones.

The first category of tools allows calculating various metrics
without code analysis, for example, the number of stars for a GitHub
repository. The tools in this category can be implemented as online
platforms such as MetricMiner [38] and SmartSHARK [40], or
as desktop applications such as PyDriller [39] and CodeDJ [30].
Online platforms from this category [38, 40] are limited to the
datasets that are loaded into their database. First of all, this draw-
back limits the scope of analysis to languages present in the dataset,
usually only popular ones. In addition, it is impossible to view the
data in dynamics, since the database contains a specific revision
of the data (which can also be outdated). At the same time, such
platforms allow performing analytical queries and filtering reposi-
tories without much effort, since the user needs to simply open a
website, write a query, and get the results.

Desktop tools [30, 39] are more difficult to use as they require a
laptop or a server with the suitable characteristics, free hard disk
space, and the effort required to install the tool. Such tools usually
work with GitHub API and allow not only running some queries
but also downloading repositories that satisfy these requests. These
tools can be adapted to new queries by extending filter engines
and can be used with specific languages and datasets. The main
disadvantage of such platforms is that the analysis in this case is
limited only to studying various types of meta-information about
the project, the authors, etc., which makes it impossible to study
source code in detail.

The second distinct group of tools usually analyze pre-gathered
datasets or have simple GitHub scrapers, and allow scanning code
files, even performing static analysis. One such popular tool is
Boa [17] — a language and an infrastructure for analyzing large-
scale datasets. This language allows building an abstract syntax
tree (AST) for code files and filtering projects by a simple parsing
of this tree. The tool can traverse the AST and filter its nodes by
text conditions, e.g., if a node has null in its name. Boa is actively
maintained [7] and popular among researchers [8], but it does
not have the ability to use other static analysis features, such as
searching for references or resolving variable types.

Another prominent tool is PyScan [35], which allows scanning
Python repositories, resolving all dependencies and types. The
tool defines 22 kinds of popular language constructs for Python
applications and analyzes their usage. The approach is based on
building an AST, designing patterns for each language feature of
interest, and using an external tool for dynamic type inference. Such
a tool is difficult to extend even inside Python, since to support new
patterns, it requires one to design the given pattern, as well as, if
necessary, implement additional tools for static code analysis.

In another work, Li developed a tool [29] aiming at building an
extended AST for Bash projects and analyzing popular constructs of
the language. The author uses a set of Python scripts to download
and filter GitHub data, and the IntelliJ Platform [10] to build the
tree and carry out static analysis. However, the tool is distributed
as a replication package with no possibilities for extension to other
languages and analyzers.

It can be seen that the described solutions generally only support
prominent languages (usually just one or several each), and also do
not use industry-level parsing capabilities, instead implementing
their own analysis tools (different ones for different languages),
which makes it more difficult to extend them to custom needs. To
overcome these drawbacks, we developed Lupa.

3 LUPA
Lupa is a platform for large-scale analysis of the programming
language usage. Specifically, Lupa is implemented as a plugin for
the IntelliJ Platform [10] that reuses its API to launch the IDE in
the background (without graphical user interface, GUI) and run
the necessary analysis on every project in the given dataset. The
workflow of Lupa is demonstrated in Figure 1. The user needs only
to write a set of instructions about which nodes of the syntax tree
they want to analyze, and launch the tool. In this section, we explain
the architecture and the tooling behind Lupa, describe the process
of extending it to custom analysis and visualization, provide several
example use cases, and discuss the existing limitations.

Figure 1: An operating pipeline of Lupa.

3.1 Internals
The tool is written in Koltin and operates as a plugin for the IntelliJ
Platform. Apart from traditional plugins, when the plugin operates
in the opened IDE via GUI, the IntelliJ Platform also supports the
so called headless mode. In this mode, the plugin operates as a
command line program that launches the IDE in the background,
without GUI, and then the IDE performs the necessary work.

.csv ﬁleRepositories from GitHubCore 
analyzersCustom analyzersCore visualizationCustom visualizersGathered statisticsLupa: A Framework for Large Scale Analysis of the Programming Language Usage

Conference’17, July 2017, Washington, DC, USA

This operation pipeline is extremely convenient for our task for
three main reasons. Firstly, the plugin can launch different IDEs,
meaning that it can support different languages. Secondly, this
allows us to use the full power of industry-level code analysis on
large datasets, like finding usages of functions, without having
to open each project manually. Thirdly, this allows us to use the
existing tooling without parsing or processing the code ourselves.
Mainly, it gives us access to a particularly useful part of the IntelliJ
Platform — Program Structure Interface (PSI). The PSI tree is a
rich concrete syntax tree that allows us to perform efficient and
deep static analysis on the opened project. Unlike a simple abstract
syntax tree, PSI contains information about the semantics of code,
can index the entire project to connect entities between different
files, resolve dependencies, build control flow graphs, etc.

Lupa operates as follows. To perform the analysis, the tool needs
two obvious components: a dataset and analyzers, i.e., sets of in-
structions of what PSI tree nodes need to be analyzed and how.
The tool takes the list of links to GitHub projects as input, checks
them for duplicates using GitHub API, and then downloads the
dataset. The analyzers are implemented using the tool’s infras-
tructure (see Section 3.2 for details), several analyzers can be run
together in a single passing of the dataset. When the arguments are
passed, the plugin can be run with one simple CLI command. It is
also possible to launch the tool in the batch mode. In this case, the
tool will process the given number of projects (the size of the batch
that can be configured), save all results individually, and then merge
them in the end. This can be used to save the results iteratively as
checkpoints in case of external problems when processing a large
dataset. Lupa launches the IDE in the background, which then
opens the projects in the batch one by one. This happens the same
way as if one would open the project in an IDE — the necessary
dependencies are automatically downloaded, indexed, and resolved,
the semantic and syntactic PSI models of the project are built.

When the project is opened, the plugin gains access to the PSI
tree of each necessary file. Then it simply executes instructions from
the analyzers and saves their results into a CSV file. When this is
done for all the files, the results are saved, the project is closed, and
Lupa moves to the next project in the batch. The final results can be
analyzed further or visualized: the plugin has several core functions
for visualization, e.g., plotting interactive bar charts for visualizing
CSV data that can be customized by the users via changing the
parameters of the chart or adding custom functionality, and then
used in Jupyter notebooks.

3.2 Extensions
The main feature of Lupa is that one can write the necessary ana-
lyzers themselves in a pretty straightforward way. Since the plugin
is written in Kotlin, this can be done in either Kotlin or Java. Specif-
ically, each analyzer is a class that describes a filter that should be
applied to the PSI tree to obtain the necessary result. This can be
as simple as “filter only numerals” or something more complex like
“find all the unreachable while loops and save their content”. While
the second example sounds complicated, in reality, it is merely a se-
ries of filters and checks applied to PSI nodes. The implementation
of this particular analyzer for Python code is presented in Figure 2.
The analyzer is written in Java, but can also be written in Kotlin.

Figure 2: An example of a simple analyzer written in Java.
It finds all the while loops, evaluates their conditions, and
collects their content if the condition is always false.

Besides accessing different PSI elements, Figure 2 highlights
the use of the evaluateAsBoolean method (a part of the IntelliJ
Platform) to check the boolean value of a statement. So while some-
thing trivial as while False can be found using regular expressions,
Lupa would be able to find something like while 2+2 != 4 as
well, which simpler tools would not be able to find. This example
analyzer demonstrates the power of PSI for code analysis.

The tool provides a flexible architecture that supports different
types of analyzers: simple filtering by node types, filtering by con-
text (the use of a given node in the scope of another node, e.g.,
using the print function within the try branch), and others. Lupa
is fully documented and contains a dozen examples of analyzers
that we used for our purposes. Also, importantly, several analyzers
can be run together in a single passing of the dataset.

Finally, the analyzers can be written not only for Python and
Kotlin, but also for any other language that PSI supports. Since
the IntelliJ Platform is used as the base for IDEs for different lan-
guages (JavaScript, TypeScript, PHP, Go, and others), all these major
languages can be analyzed. Doing this requires one to access the
necessary PSI elements for the chosen language and add appropriate
dependencies to the necessary parts of the IntelliJ Platform.

3.3 Use Cases
Lupa can be helpful in several different scenarios. Firstly, it can
be used to perform general large-scale studies about how various
languages are used [19, 29, 35]. This can be as simple as counting
the usage of specific keywords and constructs [29, 35], or something
more complex that involves dependencies between different entities,
referencing certain fields, etc. [19, 29]. This can be used to track the
evolution of a language or to compare them between each other.

In particular, tracking changes in a language can be useful in
education [14, 33, 34]. Languages often develop quite quickly, new
libraries and frameworks appear [20, 41], and at the same time,
new courses emerge for learning new technologies [37]. In this
case, Lupa can help teachers in choosing technologies to study and
finding examples of their usage.

Another way the framework can be useful is to track the usage
of a specific library [25, 27]. While the simple fact of its use can be
studied using, for example, Code Search on GitHub [2], Lupa allows

final public class WhileStatementAnalyzer        implements PsiAnalyzer<PyWhileStatement, String> {    // The analyzer method to find all unreachable while loops    @Nullable    @Override    public String analyze(@NotNull PyWhileStatement psiElement) {        // Evaluate the 'while' condition        Boolean result = PyEvaluator                .evaluateAsBoolean(psiElement                        .getWhilePart().getCondition());        // Check if the 'while' condition is always false        // and return its body        if (result != null && !result) {            return psiElement.getWhilePart().getText();        }        // Return null otherwise        return null;    }}Conference’17, July 2017, Washington, DC, USA

Anna Vlasova, Maria Tigina, Ilya Vlasov, Anastasiia Birillo, Yaroslav Golubev, and Timofey Bryksin

implementing more complex analyzers to see in which context
specific functions are used, what parameters they receive, what
types they work with, etc. This can be used by researchers in general
or by developers of a specific library themselves [25].

3.4 Limitations
Lupa has several important limitations that should be mentioned.
The main one is the operating speed of the tool. Deep code analysis
requires a longer time, since in order to process usages and connect
entities, it is necessary to index the project and resolve the depen-
dencies. Lupa can thus be most useful on datasets of moderate size.
In our studies, we used a dataset of 10 thousand Kotlin projects
from GitHub, running the tool on a laptop with the following char-
acteristics: 2.4 GHz 8-Core Intel Core i9 processor and 32 GB of
RAM. Running Lupa on this dataset took on average 6.5 seconds
per project for Kotlin. This time measurement does not take into
account downloading all the necessary libraries, since this needs to
be done only once.

The second limitation is that while Lupa is extendable, its scope
is still limited by only the languages that are supported by PSI.
The list of these languages is rather long, but it does not include
some specific languages, e.g., OCaml. Finally, while writing a new
analyzer is simple enough, it is still more difficult than writing
a script with a regular expression or an SQL query. Writing the
necessary analyzer in Java or Kotlin can be viewed as using a DSL of
sorts, however, PSI and the IntelliJ Platform are well-documented.
Even though these limitations exist, we believe that they do not
invalidate the usefulness of the tool, and that it can be of use for
researchers and practitioners in many cases.

4 EXAMPLE ANALYSIS OF KOTLIN RANGES
In our research group, we employed Lupa for a number of different
studies, both practical and theoretical. In this section, we describe
one of these studies that showcases the usefulness of the tool as
well as the potential diversity of its application.

Background. Kotlin supports several different ways to write
numerical ranges. There are three main options to define an ascend-
ing range (X..Y, X until Y, and X.rangeTo(Y)) and one main
way for descending ranges (X.downTo(Y)). Different options are
more convenient in different situations, and they also differ by their
rules of inclusion and exclusion of the borders. For the purposes of
language evolution, our colleagues from the Kotlin development
team wanted to see which of them are more popular and where.

Research questions. With this research, we wanted to answer
two questions. (1) Which ways of declaring ranges are popular?
(2) Are some ways more popular than others when used in a specific
context (loops, conditions, etc.)?

Dataset. To collect the data, we used the Web service developed
by Dabic et al. [16]. This tool provided us with a list of all GitHub
projects that have Kotlin as the main language, are not forks of other
projects, and have at least 10 stars. Using the tool, we downloaded
the projects in April of 2021, which resulted in 10,536 projects. We
deleted all duplicates, e.g., repositories that were moved, and the
final dataset contained 10,442 projects.

Methodology. To answer the research questions, we created
two different analyzers for Lupa that were then run simultaneously.

Both analyzers processed only Kotlin files, and firstly detected all
the necessary PSI nodes (functions that correspond to the four
types of range declarations). To answer the first question, these
nodes were simply counted. To answer the second question, the
second analyzer then traversed the parents of each node until it
found a node from the list of language constructs that were of
interest to the Kotlin development team: for loops, while loops, if
conditions, etc. This allowed us to see where the range is located in
the program’s body. Finally, we wrote some visualizations that built
the necessary charts. To do this, we extended the core visualization
functions from our framework.

Results. Popularity. Our analysis discovered more than 70 thou-
sand usages of ranges in the studied projects. Two of the four ways
of declaration are significantly more popular than the other two:
X..Y covers 52.2% of cases, X until Y covers 45.6% of cases, while
the other two syntactic forms only cover the remaining 2.2%. This
result can help the Kotlin development team focus their attention
on specific types of ranges in future language improvements.

Context. We discovered that the studied declarations are used
very differently in various contexts. While in general, as mentioned
above, X..Y and X until Y are more or less similar in popularity,
they are used within different language constructs. For example,
the former is more than three times as popular when used in if con-
ditions, while the latter is two times as popular when used in for
loops. Such specific distribution is very important for language de-
velopers, since it allows them to develop custom recommendations
in IDEs that depend on the context. An example would be different
code completion recommendations in different code contexts. It is
important to note that it is impossible to obtain accurate results
like these using simple regular expressions or full text search.

5 CONCLUSION
In this paper, we presented Lupa, a framework for large scale anal-
ysis of the programming language usage. Lupa is a plugin for the
IntelliJ Platform that launches the IDE in the background, which
then performs the necessary analysis. This allows Lupa to reuse a
lot of the platform’s industry-level abilities and features: rich con-
crete syntax tree, syntactic and semantic models, etc. This helps our
framework to overcome the drawbacks of many existing tools, since
it is integrated into a well-established and documented platform
and can be easily extended. To extend the tool for custom analysis,
a researcher only needs to write the necessary analyzer, i.e., a class
that details the filters that should be applied to the syntax tree of
the code. The tool currently supports the analysis of Kotlin and
Python, but can be extended to other languages as well.

To showcase Lupa, we developed a dozen analyzers and de-
scribed an example study. In it, we used the tool to study different
ways of defining ranges in Kotlin along with the context where
they are used. The results demonstrated the diversity among the
usage of ranges in Kotlin, and highlighted the usefulness of our
framework for such research.

In the future, we plan to extend Lupa to support more languages,
expand the list of pre-configured example analyzers, and develop
more custom visualizations to support the full pipeline of a large
scale analysis.

Lupa: A Framework for Large Scale Analysis of the Programming Language Usage

Conference’17, July 2017, Washington, DC, USA

REFERENCES
[1] [n.d.]. CLion. https://www.jetbrains.com/clion/. [Online; accessed 8-December-

2021].

[2] [n.d.]. Code Search on GitHub. https://docs.github.com/en/search-github/
searching-on-github/searching-code. [Online; accessed 10-January-2022].
[3] [n.d.]. Github Annual Report. https://octoverse.github.com/#top-languages-

over-the-years. [Online; accessed 8-December-2021].

[4] [n.d.].

IntelliJ IDEA. https://www.jetbrains.com/idea/.

[Online; accessed 8-

December-2021].

[5] [n.d.]. Kotlin, Estimated, SlashData Estimate: State of the Developer Nation
20th Edition, April 2021. https://developer-economics.cdn.prismic.io/developer-
economics/dbf9f36f-a31a-440a-9c22-c599cc235fa4_20th+edition+-
+State+of+the+developer+Nation.pdf. [Online; accessed 25-Juny-2021].

[6] [n.d.]. Program Structure Interface. https://plugins.jetbrains.com/docs/intellij/

psi.html. [Online; accessed 26-November-2021].

[7] [n.d.]. Publications about Boa. http://boa.cs.iastate.edu/papers/index.php#about.

[Online; accessed 8-December-2021].

[8] [n.d.]. Publications cited Boa. http://boa.cs.iastate.edu/papers/index.php#uses.

[Online; accessed 8-December-2021].

[9] [n.d.]. PyCharm. https://www.jetbrains.com/pycharm/.

[Online; accessed

8-December-2021].

[10] [n.d.]. The IntelliJ Platform. https://www.jetbrains.com/opensource/idea/. [On-

line; accessed 26-November-2021].

[11] Barry Boehm. 2006. A view of 20th and 21st century software engineering. In
Proceedings of the 28th international conference on Software engineering. 12–29.
[12] Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Understanding
the factors that impact the popularity of GitHub repositories. In 2016 IEEE In-
ternational Conference on Software Maintenance and Evolution (ICSME). IEEE,
334–344.

[13] Subham Bose, Madhuleena Mukherjee, Aditi Kundu, and Madhurima Banerjee.
2018. A comparative study: java vs kotlin programming in android application
development. International Journal of Advanced Research in Computer Science 9,
3 (2018), 41.

[14] Benjamin Canou, Roberto Di Cosmo, and Grégoire Henry. 2017. Scaling up func-
tional programming education: under the hood of the OCaml MOOC. Proceedings
of the ACM on Programming Languages 1, ICFP (2017), 1–25.

[15] Fragkiskos Chatziasimidis and Ioannis Stamelos. 2015. Data collection and
analysis of GitHub repositories and users. In 2015 6th International Conference on
Information, Intelligence, Systems and Applications (IISA). IEEE, 1–6.

[16] Ozren Dabic, Emad Aghajani, and Gabriele Bavota. 2021. Sampling Projects in
GitHub for MSR Studies. In 18th IEEE/ACM International Conference on Mining
Software Repositories, MSR 2021. IEEE, 560–564.

[17] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. 2013. Boa: A
language and infrastructure for analyzing ultra-large-scale software repositories.
In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 422–
431.

[18] Robert Dyer, Hridesh Rajan, Hoan Anh Nguyen, and Tien N Nguyen. 2013. A

large-scale empirical study of Java language feature usage. (2013).

[19] Robert Dyer, Hridesh Rajan, Hoan Anh Nguyen, and Tien N Nguyen. 2014. Mining
billions of AST nodes to study actual and potential usage of Java language features.
In Proceedings of the 36th International Conference on Software Engineering. 779–
790.

[20] Christoph Endres, Andreas Butz, and Asa MacWilliams. 2005. A survey of
software infrastructures and frameworks for ubiquitous computing. Mobile
Information Systems 1, 1 (2005), 41–80.

[21] Matheus Flauzino, Júlio Veríssimo, Ricardo Terra, Elder Cirilo, Vinicius HS Durelli,
and Rafael S Durelli. 2018. Are you still smelling it? A comparative study between
Java and Kotlin language. In Proceedings of the VII Brazilian symposium on software
components, architectures, and reuse. 23–32.

[22] Demetrio Guilardi, Jalves Nicácio, Bianca M Napoleão, and Fabio Petrillo. 2020.
AndroidPropTracker: mining lifetime properties of Android projects. In Proceed-
ings of the IEEE/ACM 7th International Conference on Mobile Software Engineering
and Systems. 23–26.

[23] Siim Karus and Harald Gall. 2011. A study of language usage evolution in open
source software. In Proceedings of the 8th Working Conference on Mining Software

Repositories. 13–22.

[24] Pavneet Singh Kochhar, Tegawendé F Bissyandé, David Lo, and Lingxiao Jiang.
2013. Adoption of software testing in open source projects–A preliminary study
on 50,000 projects. In 2013 17th european conference on software maintenance and
reengineering. IEEE, 353–356.

[25] Raula Gaikovina Kula, Daniel M German, Ali Ouni, Takashi Ishio, and Katsuro
Inoue. 2018. Do developers update their library dependencies? Empirical Software
Engineering 23, 1 (2018), 384–417.

[26] Zarina Kurbatova, Yaroslav Golubev, Vladimir Kovalenko, and Timofey Bryksin.
2021. The IntelliJ Platform: a Framework for Building Plugins and Mining
Software Data. In 2021 36th IEEE/ACM International Conference on Automated
Software Engineering Workshops (ASEW). IEEE, 14–17.

[27] Davy Landman, Alexander Serebrenik, and Jurgen J. Vinju. 2017. Challenges for
Static Analysis of Java Reflection - Literature Review and Empirical Study. 2017
IEEE/ACM 39th International Conference on Software Engineering (ICSE) (2017),
507–518.

[28] Yue Li, Tian Tan, Yulei Sui, and Jingling Xue. 2014. Self-inferencing reflection
resolution for Java. In European Conference on Object-Oriented Programming.
Springer, 27–53.

[29] Zheyang Li. 2021. An Empirical Study on Bash Language Usage in Github. Master’s

thesis. University of Waterloo.

[30] Petr Maj, Konrad Siek, Alexander Kovalenko, and Jan Vitek. 2021. CodeDJ:
Reproducible Queries over Large-Scale Software Repositories. In 35th European
Conference on Object-Oriented Programming (ECOOP 2021). Schloss Dagstuhl-
Leibniz-Zentrum für Informatik.

[31] Bruno Gois Mateus and Matias Martinez. 2019. An empirical study on quality of
Android applications written in Kotlin language. Empirical Software Engineering
24, 6 (2019), 3356–3393.

[32] Bruno Gois Mateus and Matias Martinez. 2020. On the adoption, usage and
evolution of Kotlin features in Android development. In Proceedings of the 14th
ACM/IEEE International Symposium on Empirical Software Engineering and Mea-
surement (ESEM). 1–12.

[33] Obaro Odiete, Tanvi Jain, Ifeoma Adaji, Julita Vassileva, and Ralph Deters. 2017.
Recommending programming languages by identifying skill gaps using analysis
of experts. a study of stack overflow. In Adjunct Publication of the 25th Conference
on User Modeling, Adaptation and Personalization. 159–164.

[34] Kevin R Parker, Joseph T Chao, Thomas A Ottaway, and Jane Chang. 2006. A
formal language selection process for introductory programming courses. Journal
of Information Technology Education: Research 5, 1 (2006), 133–151.

[35] Yun Peng, Yu Zhang, and Mingzhe Hu. 2021. An Empirical Study for Common
Language Features Used in Python Projects. In 2021 IEEE International Conference
on Software Analysis, Evolution and Reengineering (SANER). IEEE, 24–35.
[36] Gerald Schermann, Sali Zumberi, and Jürgen Cito. 2018. Structured informa-
tion on state and evolution of dockerfiles on github. In Proceedings of the 15th
International Conference on Mining Software Repositories. 26–29.

[37] Sergii Sharov, Vira Kolmakova, Tetiana Sharova, and Anatolii Pavlenko. 2021.

Analysis of MOOC on Programming for IT Specialist Training. (2021).

[38] Francisco Zigmund Sokol, Mauricio Finavaro Aniche, and Marco Aurélio Gerosa.
2013. MetricMiner: Supporting researchers in mining software repositories. In
2013 IEEE 13th International Working Conference on Source Code Analysis and
Manipulation (SCAM). IEEE, 142–146.

[39] Davide Spadini, Maurício Aniche, and Alberto Bacchelli. 2018. Pydriller: Python
framework for mining software repositories. In Proceedings of the 2018 26th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 908–911.

[40] Alexander Trautsch, Fabian Trautsch, Steffen Herbold, Benjamin Ledel, and Jens
Grabowski. 2020. The smartshark ecosystem for software repository mining. In
Proceedings of the ACM/IEEE 42nd International Conference on Software Engineer-
ing: Companion Proceedings. 25–28.

[41] Zhaobin Wang, Ke Liu, Jian Li, Ying Zhu, and Yaonan Zhang. 2019. Various frame-
works and libraries of machine learning and deep learning: a survey. Archives of
computational methods in engineering (2019), 1–24.

[42] Yury Zhauniarovich, Maqsood Ahmad, Olga Gadyatskaya, Bruno Crispo, and
Fabio Massacci. 2015. Stadyna: Addressing the problem of dynamic code updates
in the security analysis of android applications. In Proceedings of the 5th ACM
Conference on Data and Application Security and Privacy. 37–48.

