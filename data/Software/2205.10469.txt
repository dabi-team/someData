MASTERFUL: A TRAINING PLATFORM FOR COMPUTER VISION
MODELS

2
2
0
2

y
a
M
1
2

]

V
C
.
s
c
[

1
v
9
6
4
0
1
.
5
0
2
2
:
v
i
X
r
a

Samuel Wookey∗

Yaoshiang Ho ∗

Tom Rikert ∗

Juan David Gil Lopez ∗

Juan Manuel Muñoz Betancur∗

Santiago Cortes ∗

Ray Tawil ∗

Aaron Sabin ∗

Jack Lynch ∗

Travis Harper ∗

Nikhil Gajendrakumar ∗

ABSTRACT

Masterful is a software platform to train deep learning computer vision models. Data and model
architecture are inputs to the platform, and the output is a trained model. The platform’s primary
goal is to maximize a trained model’s accuracy, which it achieves through its regularization and
semi-supervised learning implementations. The platform’s secondary goal is to minimize the amount
of manual experimentation typically required to tune training hyperparameters, which it achieves via
multiple metalearning algorithms which are custom built to control the platform’s regularization and
semi-supervised learning implementations. The platform’s tertiary goal is to minimize the computing
resources required to train a model, which it achieves via another set of metalearning algorithms
which are purpose built to control Tensorﬂow’s optimization implementations. The platform builds
on top of Tensorﬂow’s data management, architecture, automatic differentiation, and optimization
implementations.

Keywords Computer Vision · Deep Learning · Machine Learning · regularization · semi-supervised learning ·
optimization · Tensorﬂow

1

Introduction

The ﬁeld of computer vision (CV) has relied on deep learning to achieve state-of-the-art results since the breakthrough
performance of AlexNet [Krizhevsky et al., 2012]. Numerous research advancements, primarily in model architecture
but also in training techniques, have continued to improve CV model performance. However, practitioners attempting
to apply research often run into challenges:

• Unique attributes of research datasets often do not apply to real world datasets. For example, CIFAR-10 and
Imagenet are balanced, have high entropy, and high cardinalities [Krizhevsky and Hinton, 2009, Deng et al.,
2009]. Research built on these datasets sometimes need adjustments to deal with the class imbalanced, low
entropy, and low cardinality datasets common in practice.

• An emphasis on model architectures leaves training techniques a less studied ﬁeld. For example, while data
augmentation was included in AlexNet, the arguably ﬁrst signiﬁcant paper to study data augmentation policies
in depth occurred in 2019, 7 years after the publication of AlexNet [Cubuk et al., 2018]. And the interplay of
adaptive optimizers and weight decay, a regularization technique used in the majority of state of the art CV
research papers, was misunderstood until 2019 [Loshchilov and Hutter, 2017].

• The training of models still requires extensive hyperparameter tuning. While some automatic hyperparam-
eter tuning techniques exist, the major approach is still manually guessing-and-checking hyperparameters
[Karpathy, 2022].

∗Afﬁliated with Masterful AI at time of relevant work. Corresponding author: yaoshiang@masterfulai.com.

 
 
 
 
 
 
• Deploying models into inference adds additional model size and latency constraints. For example, when
deploying deep learning CV models on to mobile devices, it is common to take a well trained model and
reduce its size via pruning and ﬁxed point quantization [Goyal et al., 2021].

The Masterful platform seeks to solve these problems to enable practitioners to more easily apply deep learning to their
CV problems. Masterful is guided by the following design choices.

• A focus on a domain of problems that encompasses the vast majority of computer vision tasks in practice, but
is nevertheless a small minority of the full expressiveness of underlying vectorized computing, neural network
design, and back propagation based ML platforms like Tensorﬂow and PyTorch [Abadi et al., 2015].

• A primary goal of model accuracy (or other goodness-of-ﬁt measures) by implementing state-of-the-art
regularization and semi-supervised learning (SSL) algorithms. We believe that this goal requires building an
integrated platform rather than implementing a collection of algorithms in isolation because algorithms are not
additive. Like an athlete who overtrains to the point of injury, applying two such algorithms may be worse
than no algorithms at all [Müller et al., 2019].

• A secondary goal of reducing developer time spent on hyper-parameter tuning. Key hyperparameters of
research papers are often grid-searched or manually guess-and-checked behind the scenes. And when applying
a technique to a new dataset or model architecture, new hyperparameters often must be discovered. Masterful’s
goal is to minimize the time spent on hyperparameter tuning by developing purpose-built algorithms to discover
these hyperparameters. These algorithms range from highly optimized implementations of search algorithms
speciﬁc to small subsets of the hyperparameters, to analytical, closed-form solutions.

• A tertiary goal of minimizing compute resources, e.g. GPU-hours. Compute resources are a major source of
training cost, our experiences has been that most practitioners utilize less than half of their compute resources.
Common mistakes we have seen in practice include using antiquated optimizers, confusing regularization and
optimization, using hyperparameters like Batch Normalization momentum inappropriate for their datasets,
inappropriately low batch sizes, and inappropriately low learning rates. We believe that the goal of optimizers
like Adam or LAMB have the sole purpose to minimize compute resources by converging a model’s weights to
the minimum loss value on training data in the minimum amount of time. It is outside the scope of optimization
to focus on generalization (e.g. accuracy on test data), which is the domain of regularization. This can be
confusing in practice - for example many optimizers implement decoupled weight decay, a regularization
technique. In our view, this simply means that it was convenient to implement a speciﬁc type of regularization
and optimization in the same software package. Indeed, decoupled weight decay has also been implemented
as a separate module, divorced from the optimizer [Loshchilov and Hutter, 2017]. Optimizers are the only way
to ﬁnd a minimum loss value but they are not the only approach to speeding up the overall task of optimization.
For example early stopping is also a method to reduce the use of compute resources [Prechelt, 1998].

Masterful is currently built to support model architectures implemented as Tensorﬂow 2 and Tensorﬂow Object Detection
API formats. And data formats implemented as Tensorﬂow Tensors, tf.data.Dataset, and Numpy arrays [Abadi
et al., 2015]. None of our algorithms are format speciﬁc. We plan to extend to PyTorch formats in the near future.

Masterful supports these common CV use cases:

• Binary classiﬁcation

• Single-label classiﬁcation (e.g. ImageNet, CIFAR-10)

• Multi-label classiﬁcation

• Detection (e.g. Pascal VOC, MS-COCO)

• Keypoint Detection (coming soon)

• Semantic Segmentation

• Instance Segmentation (coming soon)

2 Prior Work

Bello et al. [2021] (ResNet-RS) set a framework that improvements in model accuracy could be grouped into training
techniques (what we call optimization), regularization, and model architecture. It also proposed the view that model
architecture was given undue credit in prior work when in fact regularization and training techniques were responsible
for at least some of the apparently improvement from a model architecture.

2

Loshchilov and Hutter [2017] (Decoupled Weight Decay). First, it teased out the difference between regularization
and optimization, despite the fact that both can be implemented in a software package known as an "optimizer", such
as Adam [Kingma and Ba, 2014]. Second, it resolved a long-standing mystery as to why the Adam optimizer did not
produce SOTA results. Finally, its conclusions pointed to training techniques in general as an understudied ﬁeld, since
many SOTA advancements in model architecture including AlexNet, GoogLeNet, Resnet, and EfﬁcientNet did include
weight decay, but without the same in-depth study of the subject.

Cui et al. [2019] gave us an early hint that information theoretic approaches would be necessary to understand the
amount of information in training data.

AutoAugment Cubuk et al. [2019] and RandAug Cubuk et al. [2020] were major, signiﬁcant studies of data augmentation,
a subset of regularization. Both attempted to search the high dimensional space of data augmentations. RandAug in
particular attempted to collapse this space into a tractable size, on the order of 100 full training runs rather than the
15,000 of AutoAug. Our work with RandAug and its shortcoming in datasets other than ImageNet led us to develop our
own search algorithms to further reduce this search space while better adapting it to arbitrary model architectures and
datasets.

Cutmix, Mixup, Label Smoothing Regularization, and Distillation all used soft labels, which perhaps contain more
knowledge (aka "Dark Knowledge") than hard labels [Yun et al., 2019, Zhang et al., 2017, Szegedy et al., 2016, Hinton
et al., 2015, Müller et al., 2019].

Frechet Inception Distance introduced a powerful semantic similarity metric, a simpliﬁed variation of which we have
implemented to help collapse the search space of data augmentations by orders of magnitude compared to AutoAug
[Heusel et al., 2017].

McCandlish et al. [2018] (Gradient Noise Scale) showed that statistical analysis of gradients wasn’t limited solely to
the ﬁeld of optimizers, but also towards an information theoretic approach to determining the optimal batch size, rather
than a heuristical approach. This also leads to accurate sizing of compute resources.

Smith [2017] showed an empirical estimate of a max learning rate.

Self-Distillation as Instance-Speciﬁc Label Smoothing proposed the view that soft labels are actually a form of
Maximum A Posteriori optimization [Zhang et al., 2019].

Ho and Wookey [2019] describes the frequentist philosophy that underlies deep learning via Maximum Likelihood
Estimation.

Snorkel’s description of weak labeling combined with Distillation led us to our view that so-called "supervised training"
is nothing more than distillation, where a target model is distilling the knowledge found in a single or an ensemble of
larger, more powerful models: humans [Ratner et al., 2017].

Noisy Student Training Xie et al. [2020], SimCLR Chen et al. [2020], and Barlow Twins Zbontar et al. [2021] all
established simple approaches to set SOTA results in Semi-Supervised Learning.

Gidaris et al. [2018] provided an easy to reproduce example of unsupervised pretextual pretraining, the same framework
as SimCLR, Barlow Twins, and other contrastive approaches.

3 Model Architecture

The Masterful platform takes as input a model architecture. This allows the user the control a speciﬁc architecture that
will be appropriate for their inference constraints (size, latency, cost).

In general, it’s our belief that a modern family of architectures like ResNet-RS or EfﬁcientNet will be nearly optimal
for the vast majority of problems. In future work, we plan to simplify the task of model architecture selection through
rules based on power laws.

We are encouraged by work in transformers pretrained through generative techniques, which may help reduce the
translation invariance assumption / inductive bias, which may in turn prove to be superior backbones for detection and
segmentation than traditional convolutional neural networks [Carion et al., 2020, Bao et al., 2021].

4 Data

The Masterful platform takes data as an input. Since a key implementation of Masterful is SSL, we deﬁne data not only
as labeled data (images and labels), but also unlabeled data (raw images without labels).

3

Transfer learning can also be viewed as an approach to extract information from multiple sources of data [Pratt, 1992].
For example, by pretraining a backbone using a task like supervised pretraining or an unsupervised / self-supervised
technique like SimCLR using Imagenet, then ﬁne-tuning with labeled data from the target domain, we are essentially
training a model using two sources of data: Imagenet and the target domain.

5 Meta-learning

Masterful contains multiple meta-learners to discover the hyper-parameters to control Regularization, SSL, and
Optimization. We do not use any black box optimizer, such as those based on Bayesian optimization, because we
believe that each hyper-parameter is best discovered using it’s own meta-learning algorithm. Our meta-learners include
closed form solutions, grid search on pretextual problems, beam search, greedy search, and reinforcement learning.

We include discussion of our speciﬁc meta-learning algorithms in the respective sections of Regularization, SSL, and
Optimization.

6 Regularization

Masterful contains multiple algorithms to regularize a model. We deﬁne regularization as achieving accuracy (or other
goodness-of-ﬁt measures) on data the model has never seen, which the test dataset represents. This typically means
preventing the model from overﬁtting to the training dataset.

The regularization methods included in Masterful include:

• Data Augmentation, or image transforms. One view of data augmentation is that it reproduces invariances found
in the image capture process. For example, zooming in and out of an image is a common data augmentation.
If an object is a dog from ﬁve meters away from a camera, then that object is still a dog if the camera is ten
meters away. Yet each camera captures a different image: the closer camera captures an image with a larger
dog. This data augmentation technique obviously fails for abnormal ranges, such as microscopic ranges.

• Soft label techniques. Focusing on the labels rather than the images, forcing labels into continuous distributions

like (0.6, 0.4) appear to help regularize models as well in some cases.

• Weight Decay focuses on the weights of a models itself, based on the basic assumption from Rumelhart: "...the
simplest most robust network which accounts/or a data set will, on average, lead to the best generalization
to the population from which the training set has been drawn" [Hanson and Pratt, 1988]. Weight decay is
generally a single parameter, and yet modern optimizers recognize that different weights or layers of a model
beneﬁt from differential learning rates. To meta-learn a decay rate per weight or layer would be intractable,
but Masterful includes a novel technique to decay weights dynamically and separately from back-prop.

• Dropout is a commonly used regularization technique that in one interpretation by it’s original authors attempts

to efﬁciently approximate a "Bayesian gold standard" [Srivastava et al., 2014].

Regularization inside the Masterful platform involves picking the right techniques with the right magnitude. Given
transforms T and magnitudes M , the search space is of size |T ||M |. In practice this can easily grow to a search space of
1010 values, which is obviously too large of a search space to tractably search via brute force. Therefore the central
challenge is not any individual technique, but ﬁnding the right hyperparameters among all possible values.

Transforms are not independent of each other, so a greedy approach of ﬁnding an optimal magnitude for an individual
transform, next stacking on the next transform, etc., despite having a search space of only |T | × |M |, is ineffective.

In order to reduce the search space, we analyze each tuple (t, m) where t ∈ T and m ∈ M using a simpliﬁed variation
of Frechet Inception Distance and group each tuple by its distance [Heusel et al., 2017] into groups G. In the extreme,
this grouping would allow us to reduce the search space to simply |G|.

The implementations of the transforms are designed to run on GPU, making them also serve the goal of optimization.

7 Semi-Supervised Learning or SSL

SSL is the ability for a CV model to learn from both labeled and unlabeled images.

Masterful trains your model using unlabeled data through SSL techniques. Masterful’s approach draws from two of
the three major lineages of SSL: feature reconstruction and contrastive learning of representations. (Masterful does

4

not currently include techniques from the third major lineage, generative techniques aka image reconstruction). Three
state-of-the-art papers broadly deﬁne the techniques included in Masterful: Noisy Student Training, SimCLR, and
Barlow Twins [Xie et al., 2020, Chen et al., 2020, Zbontar et al., 2021].

The central challenge of productizing SSL is that research problems are often deﬁned narrowly and therefore research
results are not robust to messy, real world conditions. Deﬁning a narrow problem like “classiﬁcation on Imagenet on
Resnet50” allows many hyperparameters to be asserted. Masterful generalizes the basic concepts from SOTA research
to additional tasks like detection and segmentation, arbitrary data domains like overhead geospatial, and additional
model types.

8 Optimization

Optimization means ﬁnding a minima in the high dimension loss surface of the training set in the minimum computational
cost and wall-clock time.

Although optimization advancements sometimes do improve accuracy, we view those as ﬁxing bugs than introducing a
feature because, in general, running a low learning rate on a maximally high batch size without any tricks (namely,
the entire dataset, also known as Gradient Descent) will eventually ﬁnd the minimum loss value. Thus, the central
challenge of optimization is speed.

Formally, during training, the goal is to ﬁnd the best parameters θ of a neural network. "Best" means that these
parameters will yield the minimum loss value, calculated by the loss function J(θ). The typical loss function for
classiﬁcation, for example, is cross-entropy [Goodfellow et al., 2016].

During training, we compute the best direction and relative magnitude, or gradient, along which we should change our
parameters θ by using back-propagation. The gradient is mathematically guaranteed to be the direction of the steepest
descent.

8.1 Optimizers

Optimizers other than pure GD/SGD, like Momentum, RMSProp, Adam, LARS, and LAMB, include some statistical
information about the prior gradients to dynamically adjust the update for faster convergence [Tieleman and Hinton,
2017, Kingma and Ba, 2014, You et al., 2017]. The goal is to converge more quickly by using this statistical information.

For one update of parameters θ, optimization algorithms might calculate updates using all or less than all of the training
set:

• Gradient Descent calculates gradients against the entire dataset and does not analyze statistical information.

θ = θ − η.∇θJ(θ)

• Most optimizers calculate against less than the full dataset, the most basic being Stochastic Gradient Descent
(SGD). Note that we consider momentum a differnet optimizer than pure SGD, since momentum includes
statistical information about prior gradients.

θ = θ − η.∇θJ(θ; x(i:i+n); y(i:i+n))

8.2 Batch Size

Batch size refers to the number of training examples utilized in one iteration of neural network training. A crucial
implementation detail is that the batches must be selected randomly. Unfortunately, Tensorﬂow’s Dataset API’s shufﬂe
method provides an insufﬁciently random algorithm

Batch Size is an important hyper-parameter of optimization. The larger the batch size, the more information is contained
in each step of back propagation. GPUs are designed to parallelize computations, and the best way to parallelize
computation is to increase the batch size.

Finding the right batch size by trial and error is nearly intractable because it depends on the cardinality and entropy of
the dataset, the model architecture, GPU speciﬁcations and learning rate. If the batch size is too small, training runs
take longer than necessary. And if the batch size is too large, training is using more compute resources than necessary.

5

8.2.1 Intuition about Finding The Right Batch Size

In Gradient Descent, the gradient used is what we will call true gradient: an average of the the gradients from each
and every training example. The true gradient factors in all possible information, by deﬁnition. A batch gradient is the
average of just a batch of data, rather than the full training data.

When the batch size is very small, the batch gradient will be noisy estimate of the true network gradient because of high
variance in the gradient between batches. Doubling the batch size allows us to train in half the time without additional
steps of backprop. By contrast, when the batch size is very large, variance in the gradients between batches is very low.
The batch gradient nearly matches the true gradient. Doubling the batch size won’t lead to faster training because any
two randomly sampled batches will almost have similar gradients.

The optimal batch size, which is a trade off between wall clock time for training and the compute cost in terms of
hardware budget, occurs roughly when increasing batch size stops reducing the noisiness of the gradient - where the
variance of the gradient is at the same scale as the gradient.

Figure 1: Training time and compute cost are primarily determined by the number of optimization steps and the number
of training examples processed, respectively (Figure source: [McCandlish et al., 2018]).

Figure 2: Comparison between large and small batch training (Figure source: McCandlish et al. [2018]).

8.2.2 The Gradient Noise Scale

In McCandlish et al. [2018], the authors distilled the above intuition into a speciﬁc algorithm and demonstrated its
usefulness across many domains and applications, including MNIST, SVHN, CIFAR10, ImageNet, and Billion Word. It
starts by calculating a statistic called the gradient noise scale.

The objective of a deep neural network is to minimize the loss using an optimization algorithm such as stochastic
gradient descent. Total loss L(θ) over the entire distribution is given by an average over loss Lx(θ) associated with
each data point x. Therefore L(θ) = Ex∼p[Lx(θ)].
We minimize L(θ) using SGD-like optimizer by taking repeated steps in the direction of steepest descent of the loss
function at the current point. The gradient of the loss function, G(θ) = ∇L(θ), is mathematically guaranteed to be the

6

direction of its steepest descent. Calculating gradient over the entire distribution is expensive. Instead, we obtain an
estimate of the true gradient by averaging over a mini-batch of samples of size B.

Gest(θ) =

1
B

B
(cid:88)

i=1

∇θLxi(θ) ;

xi ∼ p

(1)

The above gradient estimate is like a random variable whose expected value (averaged over random batches) is given by
the true gradient. Covariance between gradient estimate of different batches is inversely proportional to batch size B;
As the batch size increases, variance in the gradients between batches is very low.

Ex1...B∼ρ[Gest(θ)] = G(θ)

covx1...B∼ρ(Gest(θ)) =

1
B

Σ(θ)

(2)

(3)

We want to ﬁnd the optimal batch size where increasing batch size stops reducing the noisiness of the gradient since
noise in the gradient is connected to the maximum improvement in true loss that we can expect from a single gradient
update. Let the current gradient update be V , it perturbs the model parameters θ by (cid:15)V , where (cid:15) is the step size. In the
next step, we minimize the loss with respect to new model parameters (θ − (cid:15)V ).

Where G denotes the true gradient and H denotes the true Hessian at parameter values θ.

L(θ − (cid:15)V ) ≈ L(θ) − (cid:15)GT V +

1
2

(cid:15)2V T HV

(4)

The above equation is derived from second-order Taylor series approximation of the function f (x0 + δx) is given by
f (x0 + δx) = f (x0) + δxT + 1
The step direction is given by a direction proportional to the gradient: → δx = αg ; where α is the learning rate
⇒ f (x0 + δx) ≈ f (x0) + αgT g + 1
2 α2gT Hg. Therefore, minimizing f (x0 + δx) with respect to α we can obtain
gT Hg . Using same approach, we get (cid:15)max = |G|2
that this learning rate should be the right term α = gT g

2 δxT Hδx

GT HG .

However, in reality we have access only to the noisy estimated gradient Gest from a batch of size B, thus the best we
can do is minimize the expectation E[L(θ) − (cid:15)Gest] w.r.t (cid:15).

E[L(θ) − (cid:15)Gest] = L(θ) − (cid:15)|G|2 +

1
2

(cid:15)2(GT HG + tr(HΣ)/B)

(5)

To ﬁnd the optimal learning rate, minimize the above equation w.r.t (cid:15) by setting the partial derivative of the above
equation w.r.t (cid:15) to 0.

−|G|2 + (cid:15)(GT HG +

tr(HΣ)
B

) = 0

−B|G|2 + (cid:15)(BGT HG + tr(HΣ)) = 0

(cid:15) =

B|G|2
(BGT HG + tr(HΣ)

=

|G|2
GT HG(1 + tr(HΣ)
BGT HG

=

|G|2
GT HG

(1 +

tr(HΣ)
GT HG

B )

Hence, the noise scale is deﬁned as

(cid:15)opt(B) =

|G|2
GT HG
1 + Bnoise

B

Bnoise =

tr(HΣ)
GT HG

7

(6)

(7)

The Hessian H is a matrix of all possible second derivatives for a function. As the model size increases, the noise scale
in the above equation requires some overhead to compute H. To simplify noise scale computation, let’s assume that the
optimization is perfectly well-conditioned i.e, H is a multiple of the identity matrix, then

Bsimple =

tr(Σ)
|G|2

(8)

Now the gradient noise scale can be computed with just gradients. Above equation says that the gradient noise scale is
equal to the sum of the variances of the individual gradient components, divided by the global norm of the gradient. It
is also a statistic to measure how close the estimated gradient is to the true gradient in L2 space. Heuristically, the noise
scale measures the variation in the data as seen by the model by taking expectation over individual data points. When
the noise scale is small, increasing the batch size becomes redundant, whereas when it is large, we can still learn more
from larger batches of data.

It is expensive in terms of wall clock time to calculate the gradient noise mentioned in equation 8. In the next section,
we discuss how McCandlish et al. [2018] calculate the gradient noise scale with less overhead.

8.2.3 Unbiased Estimate of the Simple Noise Scale with Less Overhead

Instead of accumulating gradients of all the batches, which is expensive, we estimate variances of the individual gradient
components and global norm of the gradient by accumulating gradients between two batch sizes Bsmall and Bbig (this
could be a multiple of Bsmall). Given estimates of |Gest|2 for both B = Bsmall and B = Bbig, we can obtain unbiased
estimates |(cid:37)|2 and S for |G|2 and tr(Σ) respectively:

Estimate of global norm of the gradient:

|(cid:37)|2 ≡

1
Bbig − Bsmall

(Bbig|GBbig |2 − Bsmall|GBsmall |2)

Estimate of variance of the individual gradient component:

S ≡

1
− 1
Bbig

1
Bsmall

(|GBsmall |2 − |GBbig |2)

(9)

(10)

The values S and |(cid:37)|2 computed at every iteration are not unbiased estimators for tr(Σ) and |G2| respectively. To
obtain unbiased estimators, we need to calculate the exponentially moving average SEM A and |(cid:37)|2

EM A as follows

|(cid:37)|2

EM At

= α|(cid:37)|2

t + (1 − α)|(cid:37)|2

EM At−1

SEM At = αSt + (1 − α)SEM At−1

(11)

(12)

Where α represents the decay coefﬁcient that weighs the contributions of the currently calculated value and of the
previous exponentially moving average. It is an additional hyper-parameter which needs to be tuned. The unbiased
estimation of the noise scale, which is a reasonable estimate of the optimal batch size to use during training, is

ˆBnoise =

SEM A
|(cid:37)|2

EM A

(13)

8.2.4 Results of Gradient Noise Scale

To validate the results of the paper, we (Masterful) trained EfﬁcientNetB0, using LAMB optimizer, on CIFAR-10 for
two different batch sizes. The starting learning rates are different for both batch sizes because a higher batch size
requires higher starting learning rate to fully take advantage of potential speedups in the optimization. See Learning
Rate Policy for a discussion on learning rates.

Two batch sizes we chose were:

• 64: A common default value.
• 1024: This is the batch size predicted by running the above gradient noise scale algorithm.

8

batch
size
64
64

learning
rate
1e-3
7e-3

training
time
7416 sec
1914 sec

# epochs ran within
training time
82
81

validation
training
accuracy
accuracy
97.92 % 70.68 %
98.64 % 71.51 %

As we can see from the above table:

• The baseline setup achieved 70.68% accuracy in 82 epochs over 7416 seconds.

• The experimental setup, trained with an optimized batch size, achieved equivalent results in 1914 seconds, or

75% less wall clock time than the baseline.

8.3 Learning Rate Policy

TBD.

9 Acknowledgements

We thank Shibani Santurkar for her personal correspondence with us regarding her work [Santurkar et al., 2018]

We thank Stephen José Hanson for personal correspondence on his discussions with Rumelhart on ideas around weight
decay (coauthored with Lorien Pratt) [Hanson and Pratt, 1988].

We thank Barrett Zoph for personal correspondence on his paper Resnet-RS (co-authored with Irwan Bello), in which
he shared additional details on their ablation study of weight decay in their paper on ResNet-RS. [Bello et al., 2021].

The format of this report was in part inspired by Papernot et al. [2016]

References

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. Advances in neural information processing systems, 25, 2012.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0,

University of Toronto, Toronto, Ontario, 2009.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image

database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.

Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation

policies from data. arXiv preprint arXiv:1805.09501, 2018.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

Adrej Karpathy. A recipe for training neural networks. http://karpathy.github.io/2019/04/25/recipe/,

2022.

Rishabh Goyal, Joaquin Vanschoren, Victor Van Acht, and Stephan Nijssen. Fixed-point quantization of convolutional

neural networks for quantized inference on embedded platforms. arXiv preprint arXiv:2102.02147, 2021.

Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat
Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorﬂow.org.

Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural

information processing systems, 32, 2019.

Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55–69. Springer, 1998.

Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, and
Barret Zoph. Revisiting resnets: Improved training and scaling strategies. Advances in Neural Information Processing
Systems, 34, 2021.

9

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of
samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9268–9277,
2019.

Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation
strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 113–123, 2019.

Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern

with a reduced search space.
Recognition Workshops, pages 702–703, 2020.

Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-
ization strategy to train strong classiﬁers with localizable features. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 6023–6032, 2019.

Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization.

arXiv preprint arXiv:1710.09412, 2017.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception
architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 2818–2826, 2016.

Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531, 2(7), 2015.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30,
2017.

Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training.

arXiv preprint arXiv:1812.06162, 2018.

Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications

of computer vision (WACV), pages 464–472. IEEE, 2017.

Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher:
Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 3713–3722, 2019.

Yaoshiang Ho and Samuel Wookey. The real-world-weight cross-entropy loss function: Modeling the costs of

mislabeling. IEEE Access, 8:4806–4813, 2019.

Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. Snorkel: Rapid
training data creation with weak supervision. In Proceedings of the VLDB Endowment. International Conference on
Very Large Data Bases, volume 11, page 269. NIH Public Access, 2017.

Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet
classiﬁcation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
10687–10698, 2020.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning

of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020.

Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via

redundancy reduction. In International Conference on Machine Learning, pages 12310–12320. PMLR, 2021.

Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image

rotations. arXiv preprint arXiv:1803.07728, 2018.

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
End-to-end object detection with transformers. In European conference on computer vision, pages 213–229. Springer,
2020.

Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254,

2021.

Lorien Y Pratt. Discriminability-based transfer between neural networks. Advances in neural information processing

systems, 5, 1992.

10

Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with back-propagation. Advances

in neural information processing systems, 1, 1988.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(56):1929–1958, 2014.
URL http://jmlr.org/papers/v15/srivastava14a.html.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. coursera: Neural networks

for machine learning. Technical Report, 2017.

Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint

arXiv:1708.03888, 2017.

Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help

optimization? Advances in neural information processing systems, 31, 2018.

Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie,
Yash Sharma, Tom Brown, Aurko Roy, et al. Technical report on the cleverhans v2. 1.0 adversarial examples library.
arXiv preprint arXiv:1610.00768, 2016.

11

