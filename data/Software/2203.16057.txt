SELF-SUPERVISED 360° ROOM LAYOUT ESTIMATION

A PREPRINT

Hao-Wen Ting

Cheng Sun

Hwann-Tzong Chen

National Tsing Hua University, Hsinchu 30013, Taiwan

ABSTRACT

We present the first self-supervised method to train 360° room layout estimation models without
any labeled data. Unlike per-pixel dense depth that provides abundant correspondence constraints,
layout representation is sparse and topological, hindering the use of self-supervised reprojection
consistency on images. To address this issue, we propose Differentiable Layout View Rendering,
which can warp a source image to the target camera pose given the estimated layout from the target
image. As each rendered pixel is differentiable with respect to the estimated layout, we can now train
the layout estimation model by minimizing reprojection loss. Besides, we introduce regularization
losses to encourage Manhattan alignment, ceiling-floor alignment, cycle consistency, and layout
stretch consistency, which further improve our predictions. Finally, we present the first self-supervised
results on ZilloIndoor and MatterportLayout datasets. Our approach also shows promising solutions
in data-scarce scenarios and active learning, which would have an immediate value in the real estate
virtual tour software. Code is available at https://github.com/joshua049/Stereo-360-Layout.

Keywords 360° room layout, self-supervised learning, differentiable rendering, multi-views 3D,
Manhattan world

1

Introduction

Automatic room layout reconstruction from 360° images is an in-demand technique for real-world applications like
real estate virtual tours. It also constitutes a crucial component of holistic 3D understanding [21] and extreme indoor
SfM [8]. To estimate high-quality room layout, recent methods mostly use deep-learning-based models to predict layout
boundaries, corners, or wall segments with steady progress on specialized model design, layout output representation,
and training objective formulation. However, deep models are data-hungry; annotating the layout is a demanding task,
which makes fully-supervised layout estimators costly to achieve reasonable results.

One affordable alternative is self-supervised learning with image reprojection consistency, where we can train the
models by using unlabeled images with visual overlap. In the past few years, self-supervised learning for monocular
dense depth estimation has achieved good results, with more techniques being proposed to reduce the performance gap
to the fully-supervised versions. However, on the task of 360° layout estimation, self-supervised learning is still an
unexplored area. To the best of our knowledge, this work is the first approach toward training a deep layout estimator
without labeled data.

A critical challenge of the self-supervised room layout model comes from the difference between the representations
of dense depth and room layout. The estimated depth map can easily be resized and compute the reprojected UV
coordinate on new camera poses for backward image warping. Conversely, the typical representation of room layout is
inherently sparse and topological. For instance, one popular method, HorizonNet [9], only regresses the positions of
wall-floor and wall-ceiling boundaries at each image column. Using these sparse points to train a model with image
reprojection loss can easily be stuck in the local optimum. The sparse points issue also exists in the recent flatten layout
depth regression technique [15]. Another category of layout representation is 2D probabilistic maps, e.g., floor and
ceiling segmentation maps [7, 17] or layout corner and boundary heat maps [24]. Unfortunately, it is unclear how to
design a differentiable process to compute the geometry coordinates from the probabilistic map-based predictions, so
we opt to adopt models that regress the flatten layout coordinates directly in this work. In sum, due to the characteristics

2

of layout representation, the existing self-supervised techniques for dense depth are not straightforwardly reproducible
for room layout estimation.

To overcome the challenge, we present Differentiable Layout View Rendering that warps images based on the layout
estimation in a differentiable manner. Specifically, given a target image, we first estimate the wall-floor and wall-ceiling
boundary positions for all image columns. We can then compute the 3D positions of all pixels in an image column
using the estimated boundary positions at the column as per the assumptions of the horizontal floor, horizontal ceiling,
and vertical wall. These 3D points can then be projected to a source image and bilinear sample the colors. As the
overall process is differentiable with respect to the estimated layout boundaries, we can end-to-end train a model to
minimize the photometric errors between the warped source image and the target image. Although this approach brings
self-supervised learning into the 360° room layout estimation task, it is still an ill-posed and challenging problem.
To facilitate this task, our idea is to incorporate prior knowledge and self-consistency into our models. We propose
training losses on the estimated layout boundaries to encourage i) Manhattan alignment, ii) ceiling-floor alignment, iii)
consistency between the estimated layout from original images and rendered images, and iv) consistency between the
stretched estimated layout and the estimated layout from stretched image. The effectiveness of all our designs is verified
quantitatively, and, for the first time, we accomplish self-supervised training of 360° room layout estimation models.

Our contributions are manifold and summarized as follows:

– We are the first to train deep 360° room layout models without labels.

– The proposed Differentiable Layout View Rendering bridges the gap between the dense representation of
depth and the flattened/sparse representation of 360° room layout, which makes the image reprojection loss
applicable.

– The introduced Manhattan alignment, ceiling-floor alignment, cycle consistency, and layout stretch consistency

losses are shown to be effective.

– The self-supervised pre-trained model is data-efficient, significantly improving results over the standard

ImageNet pre-trained.

– Our self-supervised model is also helpful in active learning. Training on the actively-queried labeled images

leads to much better results than randomly-selected labeled images.

2 Related work

360° room layout estimation. PanoContext [22] is one of the pioneering attempts to estimate the room layout
from a single 360° image, where conventional features like Orientation Map and Geometric Context are employed
to score and select the layout hypothesis. Later, LayoutNet [24] demonstrates the effectiveness of deep learning in
360° room layout estimation. Since then, deep-learning-based models and techniques have been thriving in this task.
One aspect of categorizing these methods is whether the predictions are based on probabilistic maps or regression
models. LayoutNet [24] and CFL [3] predict the heat maps of layout corners and boundaries. DuLa-Net [17] proposes
to use bird’s-eye views to sidestep 360° distortions and predict the ceiling-floor binary segmentation maps, as the
follow-up AtlantaNet [7] seeks to improve the network architecture. On the other hand, HorizonNet [9] makes good use
of the gravity alignment (with image y-axis) prior and proposes to regress the wall-floor and wall-ceiling boundary
positions at each image column. Their sequel HoHoNet [10] devises an advanced architecture and extends to more
modalities. Recently, LED2-Net [15] computes regression losses using the rendered layout depth instead of the image
positions, achieving even better results. The differentiable depth rendering module in LED2-Net only considers the
distance to walls of each image column, which is still sparse and is only used in training losses. Our differentiable
layout view rendering module computes the global 3D coordinates of all pixels for image reprojection. Due to the
limitation of space, we only mention a part of the relevant methods. We refer interested readers to Zou et al. [25] for a
comprehensive review of the recent progress of the 360° room layout estimation models.

One challenge of applying the probabilistic-map-based layout predictions to self-supervised learning is that non-
differentiable operations (e.g., peak-finding, binarization, contour-finding [11]) are typically required as the first step to
form layout polygons. Conversely, regression-based predictions can form polygons by simply connecting the adjacent
regressed points. Thus, we opt to use the regression-based layout representation [9] to devise our differentiable rendering
module for its inherently differentiable property and outstanding performance [15, 25] on complex scenes.

SSLayout360 [12] uses mean-teacher to simultaneously learn from labeled and unlabeled data, showing the effectiveness
of semi-supervised learning in 360° layout estimation, especially when labeled data is scarce. Our self-supervised
approach improves the predictions by providing better weight-initialization and does not have to change the training
pipeline for the unlabeled data. Combining self-supervised and semi-supervised learning could be helpful [1]; however,

3

Fig. 1: The proposed self-supervised scheme for 360° room layout estimation.

SSLayout360 does not release training code at the time of this preprint being done, and investigating the combination is
out of the scope of this work.

Self-supervised depth estimation. As densely labeled depth data are costly to acquire, self-supervised learning
has appeared as an affordable alternative to learning from large-scale unlabeled images with visual overlap. Two
popular forms of self-supervised learning for depth estimation are i) learning from stereo pairs with known relative
poses [4, 5, 16] and ii) learning from monocular videos with estimated poses between frames [6, 13, 18–20, 23]. By
minimizing the image reprojection errors, the models should learn to estimate depth for each pixel. However, these
methods are not directly applicable for 360° layout estimation due to the sparse representation of layout. We address
this issue by the proposed Differentiable Layout View Rendering. Besides, we consider a more constrained capturing
setup [2] where each room is captured by multiple 360° views with known relative poses and constant distance to the
floor.

3 Approach

3.1 Preliminaries

We demonstrate the proposed self-supervised learning mechanism for 360° room layout via HorizonNet [9]. First, the
floor-plane direction alignment [24] algorithm is applied to the input image as a standard pre-processing step, which
facilitates layout inference. Given an aligned image x ∈ RH×W ×3, HorizonNet then regresses the viewing angles ϕ(f)
and ϕ(c) of wall-floor and wall-ceiling boundaries at each image column, where we express them as W -dimensional
vectors ϕ(f) ∈ (−π/2, 0)W , ϕ(c) ∈ (0, π/2)W for the overall W image columns. We discard the auxiliary branch for
horizontal wall-wall classification as we do not figure out how to train it without labels, while ϕ(f), ϕ(c) are enough to
represent the room layout as shown in recent work [7, 15, 17]. In supervised learning, the labeled corners can be used to
derive the ground-truth boundaries for supervision.

3.2 Assumptions

To train a self-supervised 360° room layout model, we assume that each room is captured from more than one viewpoints
with constant distance to the floor and known relative poses. We also make the assumptions adopted by most previous
arts, i.e., one horizontal floor, one horizontal ceiling, vertical walls, and all 360° images are pre-processed by Manhattan
alignment algorithm.

3.3 Differentiable Layout View Rendering

Overview. Our setting of self-supervised learning does not need layout annotations. We only assume that multiple
panoramas of a room are available. The key idea is to train the layout estimation model by maximizing image reprojection
consistencies between different viewpoints. To this end, we present Differentiable Layout View Rendering, which warps

SourceTargetHorizonNetStretchedPano-StretchRenderedSource-TargetConsistencyCycle ConsistencyManhattan AlignmentCeiling-Floor ConsistencyLayout-StretchStretch ConsistencyPhotometric ConsistencyDifferentiable Layout View Rendering4

a source image x(s) to the target viewpoint using the estimated wall-floor ϕ(f) and wall-ceiling ϕ(c) boundaries from the
target image x(t). Besides, the rendered target image x(s→t) is differentiable with respect to ϕ(f), ϕ(c) so it is end-to-end
trainable. Our approach is illustrated in Fig. 1 and detailed below.

Pixel indices to UV coordinates. The UV coordinate system in this work is

where H, W are image height and width, and j, i are row index and column index.

uji = i/W · 2π , vji = (0.5 − j/H) · π ,

(1)

Constrained prediction ranges. The original boundary regression of HorizonNet is floating point, and the model
can finally learn the proper output range with supervised learning. However, the unconstrained boundaries could make
the training unstable if we adopt self-supervised learning, e.g., rendering a wall behind the viewing direction or too far
away. To address this concern, we apply sigmoid with a hand-crafted scaling to ensure proper ranges for the regressed
boundaries. Specifically, at each image column we obtain
(cid:16)˚ϕ(f)(cid:17)

ϕ(f) = −0.5π · Sigmoid

, ϕ(c) = 0.5π · Sigmoid

(cid:16)˚ϕ(c)(cid:17)

,

(2)

where ˚ϕ denotes HorizonNet’s unconstrained output.

Inference of distances to the floor and ceiling. The distances from the camera to the horizontal floor plane z = z(f)
and ceiling plane z = z(c) are necessary to the inference of 3D layout. In this work, we reconstruct up-to-a-scale 3D
and set z(f) = −1 for the entire scene. In case the camera height h(camera) and room height h(room) are provided, we set
z(c) = h(room)−h(camera)
. To enable a less constrained data setup when h(camera), h(room) are not annotated, the distance to
ceiling is determined by minimizing the mean squared error of the estimator z(f) cot ϕ(f)
i derived from ϕ(f), ϕ(c)
at each column i. Specifically, we plug in z(f) = −1 and solve the MSE for z(c) as

i tan ϕ(c)

h(camera)

z(c) = arg min

(cid:88)W

(cid:16)

i=1

− cot ϕ(f)

i tan ϕ(c)

i − z

z>0
1
W

(cid:88)W

i=1

=

− cot ϕ(f)

i tan ϕ(c)

i

,

(cid:17)2

(3a)

(3b)

where the Eq. (3b) is the closed form solution of the optimization problem in Eq. (3a). See the appendix for an
illustration and detail explanation.

Inference of 3D coordinates of all pixels as per the estimated layout. We parameterize the coordinate pji of a 3D
point observed by pixel (j, i) as

pji = dji · [cos uji, sin uji, tan vji] ,
where d is the distance between camera and the observed 3D point projected onto z = 0. The value of dji depends
on the estimated wall-floor ϕ(f)
i boundaries of the same image column i. See Fig. 2a for a better
illustration of the layout rendering. There are four segments in an image column separated by 0 and the two boundary
points. The dji of floor pixels, lower-wall pixels, upper-wall pixels, and ceiling pixels can be decided by

i and wall-ceiling ϕ(c)

(4)

dji =





vji ∈ [ϕ(c)
z(c) · cot vji ,
z(c) · cot ϕ(c)
, vji ∈ (0, ϕ(c)
i
vji ∈ (ϕ(f)
z(f) · cot ϕ(f)
,
i
vji ∈ (−0.5π, ϕ(f)
z(f) · cot vji ,

i , 0.5π) ;
i ) ;
i , 0) ;

i ] .

(5)

We now get the 3D coordinates of all pixels projected onto the estimated layout by Eq. (4) and Eq. (5).
As z(f) is a constant in this work, the gradients from all the floor pixels cannot backpropagate through the network.
On the other hand, ceiling pixels do not have this problem if we use Eq. (3) to determine z(c). Therefore, we may
alternatively fix z(c) and solve for z(f) like Eq. (3) to render another 3D layouts, and then use the two rendered layouts to
compute our self-supervised losses. However, we find in our pilot experiments that using the annotated z(c) and fixed
z(f) with both ceiling and floor pixels detached from network training yields better results, which suggests that wall
pixels are enough to guide network training. Thus, we choose to keep z(f) and z(c) as constant, and include only wall
pixels in the update of our model. For completeness’ sake, we still render all pixels but leave for future work to continue
investigating on this respect.

5

(a) Illustration of the rendered layout (blue solid lines for ceiling, upper-
wall, lower-wall, and floor) of an image column by our differentiable
rendering module (Sec. 3.3).

(b) Our loss encourages Manhattan alignment by
pulling the regressed layout boundary point P toward
the closest local x- or y-median. As P can only move
along OP , the distance is measured by the moving
distance on OP , so, in this example, P is guided to
align with x-median.

Fig. 2: Layout rendering and Manhattan alignment.

Warping source images to the target viewpoint. After obtaining the 3D coordinates p ∈ RH×W ×3 by projecting
all pixels from the target image onto the estimated target layout, we can warp a source image x(s) to the target viewpoint
using backward image warping:

x(s→t)
ji = x(s)

(cid:28)

(cid:18)

EquProj

Tt→s

(cid:20)p
⊺
ji
1

(cid:21)(cid:19)(cid:29)

,

(6)

where Tt→s ∈ R3×4, derived from the relative pose, transforms the target camera coordinate to the source camera
coordinate. The operator EquProj performs equirectangular projection, and ⟨·⟩ is bilinear sampling operator.

3.4 Losses for Self Supervision

Photometric loss. We use the Mean Squared Error (MSE) to measure the photometric dissimilarity between the target
image x(t) and the rendered image x(s→t) that is warped from the source image to the target viewpoint by Eq. (6). We
superimpose a validity mask m on the image to exclude the regions covered by the camera tripod from the computation
of the photometric loss:

Lphoto =

1
N

(cid:88)N

k=1

mk

(cid:13)
(cid:13)x(s→t)
(cid:13)

k

− x(t)
k

(cid:13)
2
(cid:13)
(cid:13)

,

(7)

where index k iterates through all the N pixels, and m activates pixels that are not covered by the camera tripod in both
the target and the warped image.

Cycle consistency loss. As an extra supervision signal on reprojection, the model predicts the rendered image and
enforces its consistency with the target image via the cycle consistency loss:

Lcycle =

1
W

(cid:88)W

(cid:88)

i=1

r∈{f,c}

(cid:16)

ϕ(s→t)(r)
i

− detach

(cid:17)(cid:17)2

(cid:16)

ϕ(r)
i

,

(8)

where W is image width and ϕ(s→t) is HorizonNet prediction for the rendered image x(s→t). We use detach to block
the gradient flow of backpropagation as we treat ϕ as a teacher with a more accurate result.

XY-plane projection. We first convert the estimated layout boundary ϕ to world coordinates to compute the geometry-
based losses. As all the boundary points on the floor and ceiling in a room share the same constant z component z(f), z(c),
we only care about their projected points onto the XY-plane:

b(r)
i =

(cid:104)

i , y(r)
x(r)

i

(cid:105)

(cid:104)

= z(r) ·

cot ϕ(r)

i cos ui, cot ϕ(r)

i sin ui

(cid:105)

for r ∈ {f, c} ,

(9)

where i is the column index (see Fig. 2a). All our geometry-based losses introduced below assume Manhattan world
and Manhattan alignment (Sec. 3.2).

lower-wallﬂoorupper-
wallceiling0-1zlocal x-medianlocal y-mediancamera centerlocal y-medianlocal x-medianO: camera centerP: layout boundary point6

Source-target consistency loss. Since the source and target views capture the same room, we encourage their layout
boundary points to be as close as possible on the world coordinates by
T (XY)
s→t

· b(s)(r), b(r)(cid:17)

ChamferDist

Lsrc-tgt =

(10)

(cid:88)

(cid:16)

,

r∈{f,c}

where T (XY)
coordinates. We use Chamfer distance to assess the distance between two sets of 2D points.

s→t ∈ R2×2 transforms the projected points b(s)(r) ∈ R2×W in source camera coordinates to target camera

Ceiling-floor consistency loss. The wall-ceiling and wall-floor boundaries of the same image column should be
projected onto the same point on the XY-plane, so we minimize
i − x(f)
x(c)

i − y(f)
y(c)

Lc-f =

(cid:88)W

(11)

(cid:17)2

(cid:17)2

+

(cid:16)

(cid:16)

,

i

i

1
W

i=1

to encourage the projected points to be close.

Manhattan alignment loss. As all the images are justified by Manhattan alignment algorithm, we can implement
Manhattan alignment regularization by minimizing the discrepancy to local x-median or y-median for each projected
boundary point. However, the naive implementation ignores the fact that the projected point can only move away or
toward the camera (illustrated in Fig. 2b). We thus compensate the slope factors for the x and y components:

LM =

1
W

W
(cid:88)

(cid:88)

i=1

r∈{f,c}

min

(cid:16)

(cid:110)(cid:12)
(cid:12)
(cid:12)

(cid:17)

i − ˆx(r)
x(r)

i

· sin ui

(cid:12)
(cid:12)
(cid:12) ,

(cid:16)

(cid:12)
(cid:12)
(cid:12)

(cid:17)

i − ˆy(r)
y(r)

i

· cos ui

(cid:111)

(cid:12)
(cid:12)
(cid:12)

,

(12)

where ˆx and ˆy are the local x-median and y-median.

Stretch consistency loss. We perform the Pano Stretch operation [9] to stretch images, and then compute the MSE
between the predicted layout from the stretched image and the stretched layout from the original image:
(cid:16)

(cid:17)(cid:17)(cid:17)2

(cid:16)

(cid:16)

(cid:88)W

(cid:88)

ϕ(S img)(r)
i

− S layout

detach

ϕ(r)
i

,

(13)

Lstretch =

1
W

i=1

r∈{f,c}

where ϕ(S img) is HorizonNet prediction for the randomly stretched image, and S layout stretches the layout with the
same stretching parameters as the corresponding image. Similar to cycle consistency loss, we treat ϕ as a teacher.

4 Experiments

4.1

Implementation Details

We adopt almost the same network architecture as HorizonNet [9]. Since our self-supervised pretraining only focuses
on boundary prediction, we revise the number of output channels from 3 to 2 so that the model predicts the boundary
channels only. For evaluation, due to the lack of corner prediction, we directly regard the boundary prediction as a
polygon of W vertices and follow the setting in HorizonNet to calculate the evaluation metrics. For self-supervised
pretraining, we use the Adam optimizer to train the network for 20 epochs with batch size 2 and learning rate 10−4. For
supervised fine-tuning, we use the same optimizer and learning rate, but adjust the number of epochs and batch size to
100 and 4.

4.2 Datasets

Zillow Indoor Dataset (ZInD) [2]: The ZInD dataset features multiple panoramas taken in one room, which could
be an excellent resource for self-supervised layout estimation based on multiview stereo. Nevertheless, we do not adopt
all panoramas for our pretraining and fine-tuning. The reason is that some of the panoramas include too many occluded
corners and current techniques often fail to estimate the layout accurately under such circumstances. ZInD has two
attributes that can be used to characterize the complexity of panoramas:

1. Simple: A panorama is considered simple if it does not contain consecutive occluded corners.
2. Primary: The primary panorama of a room is defined as the best view to annotate, usually the room center.

We filter the data based on these two attributes. First, we exclude scenes that are not ‘simple’. We then select a pair
of panoramas from each room. To ensure better visual overlap for self-supervised training, we require that each pair
consists of the primary panorama and a randomly selected non-primary panorama. The target view is randomly assigned
at the training stage. We follow the training-test split as ZInD. As a result, we obtain 28,828 labeled panoramas for
fully-supervised training, 5,799 unlabeled pairs of panoramas for self-supervised training, and 2,830 labeled panoramas
for evaluation.

7

Table 1: Improving fine-tuning on ZInD by self-supervised pretraining.

percentage of
labeled data
0

1% (= 248)

2% (= 451)

3% (= 673)

5% (= 1,139)

Self-supervised
pretraining
✓
✗
✓
✗
✓
✗
✓
✗
✓

3D IoU(%)↑ 2D IoU(%)↑ RMSE ↓

δ1 ↑

71.59
69.03
82.33
82.29
84.17
84.08
84.65
85.28
86.11

74.57
72.54
85.33
85.46
86.89
86.93
87.31
87.20
88.63

0.3054
0.2165
0.1847
0.1945
0.1673
0.1733
0.1648
0.1515
0.1468

0.9137
0.8325
0.9680
0.9629
0.9739
0.9709
0.9750
0.9754
0.9791

Table 2: Fine-tuning on MatterportLayout with self-supervised pretraining.

Method
ImageNet-pretrained
Self-pretrained

Method
ImageNet-pretrained
Self-pretrained

Method
ImageNet-pretrained
Self-pretrained

Method
ImageNet-pretrained
Self-pretrained

3D IoU (%) ↑
50 labels 100 labels 200 labels 400 labels 1650 labels
72.35
73.50

79.70
80.90

74.36
76.10

68.15
70.18

63.28
65.11

67.42
68.65

72.00
73.46

77.31
78.97

81.86
83.37

2D IoU (%) ↑
50 labels 100 labels 200 labels 400 labels 1650 labels
75.38
76.71
RMSE ↓
50 labels 100 labels 200 labels 400 labels 1650 labels
0.371
0.360
δ1 ↑
50 labels 100 labels 200 labels 400 labels 1650 labels
0.937
0.947

0.280
0.256

0.968
0.982

0.344
0.317

0.950
0.966

0.899
0.924

0.446
0.411

0.857
0.893

0.527
0.488

MatterportLayout [14,25]: MatterportLayout contains 2,295 labeled panorama images capturing real-world Manhat-
tan scenes with different numbers of layout corners. We subsample the official training split to conduct our data-efficient
experiments, and all the results are evaluated on the official test split.

4.3 Main Results

Improving the fine-tuning on ZInD with self-supervised pretraining. We aim to show that our self-supervised
pretraining can provide better weight initialization for fine-tuning the layout estimation model. We divide the experiment
into several parts. For each part, we split the training set into labeled data and unlabeled data. ZInD organizes the data in
several levels. The top level comprises the scenes, where each scene consists of several rooms. Each room then contains
several panoramas sharing the same layout. As our self-supervised pretraining requires pairs of panoramas taken in
the same room, we split the training set by the level of scenes, so the self-supervised training pairs adopted in the
experiment are guaranteed to be valid. Therefore, the supervised ratio X% means that X% of scenes in the training set
are picked as labeled data, while the rest (100-X)% of scenes are unlabeled data. We use the proposed self-supervised
learning mechanism with the unlabeled data to pretrain the model for 20 epochs. As the ZInD is a dataset of unfurnished
rooms, leading to faster convergence, we only finetune with the labeled data for 30 epochs. As Table 1 shows, we set
the supervised ratio to 1%, 2%, 3%, and 5%, corresponding to 248, 451, 673, and 1139 labeled data. We compare
our self-supervised model with the ImageNet-pretrained model under each condition. It is clear that our model with
self-supervised pretraining outperforms the ImageNet-pretrained model under data-scarce conditions where only a
small number of labeled data are available for fine-tuning. Although the performance gap between the two weight
initialization strategies decreases as the number of labeled data increases, our self-pretrained model still achieves better
performance for all the scenarios.

Improving the fine-tuning on MatterportLayout with self-supervision. As aforementioned, the ZInD dataset
contains mainly unfurnished rooms. To show that the pretrained model can generalize to furnished rooms, we adopt the
self-supervised model that is trained on the entire set of unlabeled data of ZInD to obtain the pretrained weights for the

8

Self-supervised only on ZInD

Fine-tuning on MatterportLayout with 100 labels

Fine-tuning on MatterportLayout with all labels

Fig. 3: Qualitative results. The red lines are ground-truth layouts, and the green lines are our predictions. Our self-
supervised model achieves impressive results on the ZInD dataset without training on a single layout label. When
fine-tuning on the MatterportLayout dataset, our self-supervised pre-trained achieves better visual results than the
ImageNet pre-trained (blue line), which is consistent with our quantitative results in Table 2.

evaluation on MatterportLayout. We follow the experiment settings of SSLayout360 [12] on MatterportLayout and
show the results in Table 2. For each experiment, we set the number of labeled images to 50, 100, 200, 400, and 1650
(= the entire labeled data). We select exactly the same labeled subset as SSLayout360. We compare our self-pretrained
model with ImageNet-pretrained model as the experiments on ZInD. Again, our method achieves better performance
under all conditions and metrics, suggesting that our method can provide effective weight initialization on cross-domain
datasets.

Active data selection. We find that the proposed Manhattan alignment loss and ceiling-floor consistency loss are good
indicators for assessing the uncertainties of our self-supervised model’s predictions without the need of ground-truth
labels. As a result, we can use them to build an efficient scheme for actively selecting the most critical data to be labeled
next. Given a pretrained model to be further fine-tuned for layout estimation, we let the model predict on each each
sample and calculate the sum of Manhattan alignment loss and ceiling-floor consistency loss as an uncertainty score. If
the datasets do not provide annotations of ceiling heights, we estimate the heights using Eq. (3). For a sample having
a higher uncertainty score, we consider its prediction less reliable; it is more useful if we acquire its labeled for the
next training phase. To evaluate such an active data selection scheme, we sort the samples by the uncertainty scores in
descending order and select the top 50, 100, 200, and 400 samples to form subsets for conducting data-efficiency tests
on MatterportLayout. Note that, in addition to the model pretrained by our method, the ImageNet-pretrained model is
also evaluated to verify if this data-selection scheme is also applicable to other models.

9

Table 3: Active data selection.
3D IoU (%) ↑

Self-pretrained Active Selection

50 labels 100 labels 200 labels 400 labels

-
-
✓
✓

-
✓
-
✓

53.60
61.26
61.10
66.37

59.79
65.49
64.16
69.48
2D IoU (%) ↑

64.68
70.14
66.67
71.44

71.08
73.04
72.33
74.89

Self-pretrained Active Selection

50 labels 100 labels 200 labels 400 labels

-
-
✓
✓

-
✓
-
✓

58.28
65.70
64.88
69.44

64.73
69.68
67.60
72.88

RMSE ↓

69.10
73.57
70.29
74.60

74.83
76.59
75.32
77.45

Self-pretrained Active Selection

50 labels 100 labels 200 labels 400 labels

-
-
✓
✓

-
✓
-
✓

0.707
0.546
0.555
0.448

0.589
0.465
0.505
0.401

δ1 ↑

0.492
0.392
0.459
0.375

0.371
0.342
0.363
0.338

Self-pretrained Active Selection

50 labels 100 labels 200 labels 400 labels

-
-
✓
✓

-
✓
-
✓

0.794
0.834
0.868
0.910

0.821
0.874
0.894
0.930

0.875
0.917
0.903
0.931

0.942
0.950
0.950
0.960

Table 4: Ablation of loss components.
photometric validity cycle src-tgt manhattan ceil-floor stretch 3D IoU(%)↑

✓
✓
✓
✓
✓
✓
✓

✓
✓
✓
✓
✓
✓

✓
✓
✓
✓
✓

✓
✓
✓
✓

✓
✓
✓

✓
✓

✓

53.61
55.43
56.82
64.07
68.68
70.21
71.59

Table 3 shows that our active data selection scheme is always better than random selection. Furthermore, since the data
are selected by the score derived from our self-supervised learning model, i.e., data selection is customized for our
self-pretrained model, we find that the fine-tuned model with our data selection scheme outperforms the fine-tuned
model with SSLayout360’s data split on the first 50 chosen labels as shown in Table 3. Meanwhile, as SSLayout360
does not mention how their subsequent data splits are chosen, we cannot conduct a fair comparison with SSLayout360’s
data-scarce results with ours achieved by the active data selection scheme. Nevertheless, it can be seen that our
Manhattan alignment loss and ceiling-floor consistency loss are effective to serve as informative metrics for active data
selection on layout estimation.

4.4 Ablation Experiments

The proposed layout estimation model comprises various learning mechanisms and modules to achieve self-supervision.
Therefore, we conduct extensive ablation experiments to verify the effectiveness of the proposed loss components. We
also show the results when room heights and camera heights are not provided.

Loss components. We progressively ablate the proposed losses and present the results in Table 4. The photometric
loss is the fundamental driving force behind our self-supervised layout estimation model. By training with only the
photometric loss, our model can already learn a rough understanding of room layout to achieve 55.43% IoU. We also
show that excluding pixels covered by camera tripods is reasonable, and the IoU is degraded without it. In addition to
photometric loss, we use cycle consistency to enrich the supervision coming from the rendered images, which improves
our results slightly.

10

Table 5: The performance of ceiling height inference.

Ceiling height annotation
✗
✓

3D IoU(%) ↑
69.62
71.59

2D IoU(%) ↑ RMSE ↓
0.3201
0.3054

71.23
74.57

δ1 ↑
0.8988
0.9137

The relative translations between views in the ZInD dataset are typically large; such a wide range of variation in
geometry also leads to noticeable view-dependent effects in appearance, e.g., the reflections on the floor vary from
place to place. However, the rendered images via warping are unable to reproduce the view-dependent effect, which
may hinder the performance if only the photometric loss is applied. We thus introduce the geometry-based loss, which
encourages the estimated layout polygons from the source and target views to be consistent on the world coordinates.
After adding the source-target consistency loss, our results are improved significantly from 56.82% to 64.07% IoU. It is
worth noting that training with only the source-target consistency loss fails to converge as the layout polygons are sparse
and the correspondences between vertices are missing. As a result, the models still mainly rely on the photometric loss,
while the source-target consistency acts as an auxiliary loss.

Finally, the proposed losses further guide the estimated layouts to be i) Manhattan-world aligned, ii) consistent between
ceiling and floor, and iii) consistent between different augmented stretches. Combining the three auxiliary losses, we
achieve another significant improvement from 64.07% to 71.59% IoU.

Inferring the ceiling height. The ratio of camera height to ceiling height is important information to reconstruct
up-to-a-scale 3D room layout. Table 5 presents the results of our self-supervised learning with and without the ceiling
height annotation (distance to floor is set to 1). Our method can automatically infer the ceiling height if not provided.
The result without ceiling height annotation only degrades slightly and is still ranked the third-best among all results in
Table 4. Thus, ceiling height annotation is not a crucial requirement for our approach.

5 Conclusion

Our method brings self-supervised learning into 360° room layout estimation. We propose a novel Differentiable
Layout View Rendering module that can warp images from different viewpoints in a differentiable manner, so we can
now train 360° layout estimation models with image reprojection loss. We also introduce several a priori alignment
and consistency losses to facilitate this challenging problem and improve the results. Our self-supervised models are
also demonstrated on the practical lower-shot fine-tuning and active-learning setups. A major limitation of this work
is that we only use empty rooms to conduct self-training. We hope our method can encourage more exploration on
self-supervised 360° layout estimation and extend to a less constrained data setup.

Appendices

The appendices contain i) detailed description and illustration of ceiling height inference (Appendix A), ii) breakdown
of quantitative results with different number of corners (Appendix B), iii) implementation detail for the training loss
weights, and iv) additional qualitative comparisons for ablation experiments.

A Details of ceiling height inference

11

z(f) = −1

− cot ϕ(f)
i

− cot ϕ(f)

i tan ϕ(c)

i

Fig. A.1: An illustration of ceiling height inference.

We provide an illustration in Fig. A.1 to help the reader better understand our ceiling height inference presented in
Sec. 3.3 of the main content. Recap that we reconstruct up-to-a-scale room layout and set z(f) = −1 (the leftmost plot)
in this work. In case the ratio of camera height to ceiling height is not provided by the dataset, we can infer the distance
to ceiling z(c) using model predictions ϕ(f), ϕ(c) ∈ RW for an input image with width W . Specifically, consider only
an image column i, the optimal distance to ceiling is z(f) cot ϕ(f)
i . We want to find z(c) that
minimizes the mean squared error of the estimator from each column, which is solved by Eq. (3) in the main paper
(provided below for the sake of being self-contained).

i = − cot ϕ(f)

i tan ϕ(c)

i tan ϕ(c)

z(c) = arg min

(cid:88)W

(cid:16)

i=1

− cot ϕ(f)

i tan ϕ(c)

i − z

(cid:17)2

z>0
1
W

(cid:88)W

i=1

=

− cot ϕ(f)

i tan ϕ(c)

i

.

B Detailed results for fine-tuning

We provide the detailed results under label-scarce conditions (fine-tune with 50 labels and 100 labels) shown in
Table B.1. We group the results by the ground-truth numbers of corners. The ‘overall’ column are the results averaged
over all samples. The results show that the self-supervised pretrained models outperform the ImageNet-pretrained
model on rooms with different numbers of corners. Our pre-training can moderately improve results on the simplest
cuboid rooms (with only four corners). On rooms with more complicated topology, our self-supervised pre-training
shows significant improvements, especially for rooms with more than ten corners and odd numbers of corners. The
results suggest that the proposed self-supervised approach can serve as a good pre-training strategy and provide a
weight initialization that can generalize well under label-scarce conditions.

C Detail of training loss weights

In addition to the main photometric loss Lphoto to guide self-supervised training, we introduce several auxiliary losses to
facilitate the challenging self-supervised task. The training loss weights for the auxiliary losses are set to 0.1, so the

12

Table B.1: Quantitative results on scenes with different numbers of corners by fine-tuning with 50 labels and 100 labels
on MatterportLayout dataset.

3D IoU(%) ↑

50 labels
Method

4

10+ odd overall
6
ImageNet-pretrained 64.04 61.32 66.46 59.80 53.77 63.28
65.65 65.35 68.31 63.24 56.59 65.11
2D IoU(%) ↑

Self-pretrained

8

ImageNet-pretrained 68.72 65.55 70.55 62.72 57.13 67.42
69.66 68.22 72.65 66.06 59.04 68.65
RMSE ↓

Self-pretrained

ImageNet-pretrained 0.506 0.487 0.547 0.626 0.606 0.527
0.463 0.432 0.545 0.563 0.572 0.488
δ1 ↑

Self-pretrained

ImageNet-pretrained 0.845 0.870 0.899 0.859 0.843 0.857
0.883 0.901 0.912 0.894 0.911 0.893

Self-pretrained

3D IoU(%) ↑

100 labels
Method

4

10+ odd overall
6
ImageNet-pretrained 70.78 66.63 70.81 60.48 55.09 68.15
71.26 73.27 71.88 63.96 60.74 70.18
2D IoU(%) ↑

Self-pretrained

8

ImageNet-pretrained 75.09 70.74 73.93 62.70 58.54 72.00
75.16 75.60 74.78 66.07 63.18 73.46
RMSE ↓

Self-pretrained

ImageNet-pretrained 0.394 0.408 0.521 0.604 0.578 0.446
0.380 0.335 0.495 0.518 0.502 0.411
δ1 ↑

Self-pretrained

ImageNet-pretrained 0.897 0.898 0.915 0.894 0.888 0.899
0.912 0.933 0.926 0.956 0.956 0.924

Self-pretrained

overall training objective is

L = Lphoto + 0.1 · (Lcycle + Lsrc-tgt + Lc-f + LM + Lstretch) .

D Qualitative results of ablation experiments

R: Lphoto, Lcycle. B: Lphoto, Lcycle, Lsrc-tgt. G: All losses.

Fig. D.1: Qualitative results of ablation experiments.

We provide qualitative comparisons for ablation experiments in Fig. D.1 to further demonstrate the effectiveness of
the proposed losses. The red lines are the results trained by the photometric loss and the cycle consistency loss, where
the model can finally learn a rough understanding of the room layout. However, there are two main issues when the
supervisions come only from the rendered image. First, the model mistakenly recognizes the space outside doors as part
of the room. Second, image warping cannot reproduce the view-dependent effects in appearances, such as reflection and
light conditions. As a result, the ambiguity of photometric consistency may hinder self-supervised performance.

We thus introduce the source-target consistency to encourage geometric consistency as an auxiliary to the main
photometric losses. We can see from the blue lines in Fig. D.1 that the issue of predictions outside doors is substantially
addressed. Consider an example that one of the panoramas is taken in front of an open door, while the other panorama is

taken far away from the door. The predicted layout for the first panorama usually has the aforementioned mistake, while
the predicted layout for the second panorama may depict the layout along with the door as it is hard to look outside the
door from a farther viewpoint.

Finally, the green lines Fig. D.1 are the results after adding the proposed Manhattan-world alignment, ceiling-floor
consistency, and layout stretch consistency losses, which regularize the results to be more in line with the layouts in the
real world. The provided qualitative comparisons explain that each loss in our method is designed not only to improve
the quantitative evaluation metrics but also to yield powerful techniques that address the issues in self-supervised
training, making our losses useful and explainable.

13

14

References

1. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton, G.E.: Big self-supervised models are strong semi-supervised learners.

In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) NeurIPS (2020) 2

2. Cruz, S., Hutchcroft, W., Li, Y., Khosravan, N., Boyadzhiev, I., Kang, S.B.: Zillow indoor dataset: Annotated floor plans with

360deg panoramas and 3d room layouts. In: CVPR. pp. 2133–2143 (2021) 3, 6

3. Fernandez-Labrador, C., F´acil, J.M., P´erez-Yus, A., Demonceaux, C., Civera, J., Guerrero, J.J.: Corners for layout: End-to-end

layout recovery from 360 images. IEEE Robotics Autom. Lett. pp. 1255–1262 (2020) 2

4. Garg, R., Kumar, B.G.V., Carneiro, G., Reid, I.D.: Unsupervised CNN for single view depth estimation: Geometry to the rescue.

In: ECCV (2016) 3

5. Godard, C., Aodha, O.M., Brostow, G.J.: Unsupervised monocular depth estimation with left-right consistency. In: CVPR (2017)

3

6. Godard, C., Aodha, O.M., Firman, M., Brostow, G.J.: Digging into self-supervised monocular depth estimation. In: ICCV. pp.

3828–3838 (2019) 3

7. Pintore, G., Agus, M., Gobbetti, E.: Atlantanet: Inferring the 3d indoor layout from a single $360ˆ\circ $ image beyond the

manhattan world assumption. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J. (eds.) ECCV. pp. 432–448 (2020) 1, 2, 3

8. Shabani, M.A., Song, W., Odamaki, M., Fujiki, H., Furukawa, Y.: Extreme structure from motion for indoor panoramas without

visual overlaps. In: ICCV. pp. 5703–5711 (2021) 1

9. Sun, C., Hsiao, C., Sun, M., Chen, H.: Horizonnet: Learning room layout with 1d representation and pano stretch data

augmentation. In: CVPR. pp. 1047–1056 (2019) 1, 2, 3, 6

10. Sun, C., Sun, M., Chen, H.: Hohonet: 360 indoor holistic understanding with latent horizontal features. In: CVPR. pp. 2573–2582

(2021) 2

11. Suzuki, S., Abe, K.: Topological structural analysis of digitized binary images by border following. Comput. Vis. Graph. Image

Process. pp. 32–46 (1985) 2

12. Tran, P.V.: Sslayout360: Semi-supervised indoor layout estimation from 360deg panorama. In: CVPR. pp. 15353–15362 (2021)

2, 8

13. Wang, C., Buenaposada, J.M., Zhu, R., Lucey, S.: Learning depth from monocular videos using direct methods. In: CVPR. pp.

2022–2030 (2018) 3

14. Wang, F., Yeh, Y., Sun, M., Chiu, W., Tsai, Y.: Layoutmp3d: Layout annotation of matterport3d. arxiv:2003.13516 (2020) 7

15. Wang, F., Yeh, Y., Sun, M., Chiu, W., Tsai, Y.: Led2-net: Monocular 360deg layout estimation via differentiable depth rendering.

In: CVPR. pp. 12956–12965 (2021) 1, 2, 3

16. Xie, J., Girshick, R.B., Farhadi, A.: Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks.

In: ECCV (2016) 3

17. Yang, S., Wang, F., Peng, C., Wonka, P., Sun, M., Chu, H.: Dula-net: A dual-projection network for estimating room layouts

from a single RGB panorama. In: CVPR. pp. 3363–3372 (2019) 1, 2, 3

18. Yang, Z., Wang, P., Wang, Y., Xu, W., Nevatia, R.: Lego: Learning edge with geometry all at once by watching videos. In:

CVPR. pp. 225–234 (2018) 3

19. Yang, Z., Wang, P., Xu, W., Zhao, L., Nevatia, R.: Unsupervised learning of geometry from videos with edge-aware depth-normal

consistency. In: AAAI (2018) 3

20. Yin, Z., Shi, J.: Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In: CVPR (2018) 3

21. Zhang, C., Cui, Z., Chen, C., Liu, S., Zeng, B., Bao, H., Zhang, Y.: Deeppanocontext: Panoramic 3d scene understanding with

holistic scene context graph and relation-based optimization. In: ICCV. pp. 12632–12641 (2021) 1

22. Zhang, Y., Song, S., Tan, P., Xiao, J.: Panocontext: A whole-room 3d context model for panoramic scene understanding. In:

ECCV. pp. 668–686 (2014) 2

23. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-motion from video. In: CVPR (2017) 3

24. Zou, C., Colburn, A., Shan, Q., Hoiem, D.: Layoutnet: Reconstructing the 3d room layout from a single RGB image. In: CVPR.

pp. 2051–2059 (2018) 1, 2, 3

25. Zou, C., Su, J., Peng, C., Colburn, A., Shan, Q., Wonka, P., Chu, H., Hoiem, D.: Manhattan room layout reconstruction from a

single $360ˆ{\circ }$ image: A comparative study of state-of-the-art methods. IJCV pp. 1410–1431 (2021) 2, 7

