A Systematic Comparison of Simulation Software for Robotic Arm
Manipulation using ROS2

Florent P. Audonnet1, Andrew Hamilton2 and Gerardo Aragon-Camarasa1,2

2
2
0
2

r
p
A
3
1

]

O
R
.
s
c
[

1
v
3
3
4
6
0
.
4
0
2
2
:
v
i
X
r
a

Abstract— Simulation software is a powerful tool for robotics
research, allowing the virtual representation of the real world.
However with the rise of the Robot Operating System (ROS),
there are new simulation software packages that have not
been compared within the literature. This paper proposes a
systematic review of simulation software that are compatible
with ROS version 2. The focus is research in robotics arm
manipulation as it represents the most often used robotic
application in industry and their future applicability to digital
twins. For this, we thus benchmark simulation software under
similar parameters, tasks and scenarios, and evaluate them
in terms of their capability for long-term operations, success
at completing a task, repeatability and resource usage. We
ﬁnd that there is no best simulation software overall, but two
simulation packages (Ignition and Webots) have higher stability
than other while, in terms of resources usage, PyBullet and
Coppeliasim consume less than their competitors.

I. INTRODUCTION

With the advent of deep learning technologies, current
research efforts have been focused on teaching robots how
to perform various tasks autonomously. However, a data-
driven approach is required to acquire and process the vast
amount of data to effectively teach a robot how to perform
a task which is unfeasible using a real robotic testbed. For
this, robot simulation software [1], [2], [3], [4], [5] have
been used to overcome the shortcomings of data-hungry AI
approaches and to allow the developer to obtain a constant
environment [6]. In a simulated environment the world can
be controlled, including aspects that would be impractical in
reality. There is also no risk of damaging the robot or human
operators, and simulations allow to control the time which
increases the speed of data collection.

Simulations are the gateway for Digital Twins, a high-
ﬁdelity representation of the physical world [7], and can
allow manufacturing to increase production and ﬂexibility
of supply chains. Therefore, digital
twinning consists of
interconnecting a simulation software to a real autonomous
robotic system in order to reduce the implementation time
of manufacturing process when changing a production line.
A recent example of a digital twin solution for a robotic
arm can be found in [8] where the authors used ROS (Robot
Operating System) [9] to achieve seamless operation between
the real and digital world. However, simulation software are

This research has been supported by EPSRC DTA No. 2605103 and

NVIDIA Corporation for the donation of the Titan Xp GPU.

1 School of Computing Science, University of Glasgow, G12 8QQ, Scot-
land, United Kingdom f.audonnet.1@research.gla.ac.uk
and gerardo.aragoncamarasa@glasgow.ac.uk

2 National Manufacturing Institute in Scotland, Scotland, United King-

dom andrew.w.hamilton@strath.ac.uk

Fig. 1. Overview of the Simulation Software and Their Capabilities

not perfect because their physics engines are not an accurate
representation of the real world. Furthermore, simulations
allow for perfect data capture with no noise which has
powered research in deep learning approaches for robotics.
In this paper, we propose to carry out a systematic bench-
mark of current simulation software (Figure 1) to investigate
their performance and suitability to perform different robotic
manipulation tasks using the ROS2 (Robot Operating System
version 2). ROS has become the de facto communication
platform for modern robotic systems. We choose ROS2
because it supports a wide array of devices (e.g. micro-
controllers) which enables the integration of Internet of
Things (IoT). The latter is a main requirement for developing
a working digital twin system. ROS2 can also be used to
bridge the gap between AI-enabled robots and real world
robot control. We choose robotic arms in this paper as they
are prevalent in automated manufacturing operations.

We consider 2 tasks for the robot arm to perform. The
ﬁrst task is about picking and placing an object which is a
common operation in industry. The second task consists of
throwing a cube into a pyramid. We chose this throwing
task as we aim to test the accuracy and repeatability of
the simulation software to decide its potential suitability for
building digital twins. Figure 2 shows an overview of the
tasks. We record the resources usage of each simulation
considered in this paper while performing each task in both

 
 
 
 
 
 
Fig. 2.
is a Throw task where the goal is to collapse a pyramid of 6 cubes by throwing a cube at it.

Simulation Tasks Progression over Time. Task 1 (top), is a Pick and Place task where the goal is to stack 3 columns of 5 cubes. Task 2 (bottom)

a headless and a graphical version. Our contributions consist
of proposing a systematic comparison of state-of-the-art
robotic arm simulation software using ROS2, the state-of-
the-art version of the robot operating system. Furthermore,
we develop an experimental methodology to evaluate robot
simulations software for long-term operations and their suc-
cess at completing a task. We also devised an experimental
validation system to evaluate the stability of robot simulation
software and their capacity to repeat a given task.

II. BACKGROUND

Benchmarking robotic simulation software can trace back
its origins to Oreback and Christensen [10]. They were the
ﬁrst to propose a methodology to test robotic simulations.
Their approach consisted of summarising the capabilities
of 3 simulators, considering quantitative values such as
supported OS or programming languages and qualitative
opinions such as the learning curve or the difﬁculty of
installation. They also recorded the amount of RAM used
while using the simulation software to control a real robot.
Kramer and Scheutz [11] extended [10] and developed a
comprehensive testing suite for open-source robot simulation
software. They devised a set of criteria based on the software-
development process and created a feature score based on
different properties such as usability, supported features (path
planning, voice recognition, etc.) and faults handling. They
performed a low-level implementation of a task on a real
robot and recorded the resource usage. However, the task is
scarcely described and was only repeated three times.

[11]. Staranowicz and Mariottini [12] provided the ﬁrst com-
parison of simulation software that used ROS as the commu-
nication framework to control a robot. They compared the
properties of three open source and commercial simulations.
They then demonstrated the capabilities of Gazebo [13], a
popular simulation software, with a real robot, effectively
creating a Digital Twin. However they neither recorded the
resources usage nor did they try a different simulator for the
real world task. Their work was then extended by Nogueira
[14] who compared 2 simulators and their integration with
ROS, the ease of building the world and the CPU usage.

Pitonakova et al. [15] adopted the methodology in [14].
They compared three simulators and then ran extensive tests
to record each simulator performance on tasks involving
multiple robotic arms. They recorded memory, CPU usage,
and the real time factor, meaning the speed at which the
simulation runs. This is vital for Digital Twining. It is also
vital for machine learning, as the faster the simulation runs
without compromising the physics, the faster the training of
a machine learning model would be. They performed each
tests with and without a Graphical User Interface (GUI) and
then compared the impact of rendering the simulation to a
screen. Ayala et al. [16] and Korber et al. [17] followed
the idea of recording resources usage during the running of
the experiment. After recapitulating the properties of each
simulator, they coded tasks and recorded, memory and CPU
usage. Korber et al. compared four simulation software on
robotic manipulation tasks while Ayala et al. only compared
3 for humanoid robot scenarios.

Before ROS [9], roboticists used the simulation software
as a middleware to send data and commands to the robot, e.g.

In this paper, we initially consider eight robot simulation
software but narrow our benchmark to ﬁve that support for

ROS2, including two simulation software that have not been
considered in the literature. We also propose to implement
a pick and place and a throwing tasks to investigate the
advantages and limitations for each simulation software,
their performance and, ultimately, their suitability for Digital
Twins.

III. MATERIALS AND METHODS

To evaluate and compare robotic simulation software, we
develop our methodology and experiments guided by the
following research questions:

RQ1 How does simulation software compare in terms
of supporting long-term operations while still suc-
ceeding at completing a given task?

RQ2 How repeatable is the simulation software under

the same scene and task constrains?

RQ3 Which simulation software would be more suitable
for machine learning research in terms of resource
usage and idle time?

A. Simulation Software

The above research questions inform our choice of the
simulation software investigated in this paper as shown on
Figure 1. Not all of the simulation software have support
for ROS2. For this paper, we have attempted to implement
our own ROS2 bridge but with limited success due to the
rapid development cycle of ROS2. For completeness, we
describe our experience while implementing the ROS2 bridge
for the simulations we do not use in this paper. Unity’s
existing bridge is limited as it does not support asynchronous
communications which are the underlying communication
paradigm in ROS2. Mujuco conﬂicts with ROS2 because
ROS2 multithreaded controller is incompatible with Mujuco
single threading nature. Finally, we had to drop Gazebo
because development efforts have turned to Ignition, and
there is currently an implementation error in the controller
code, causing our robot to move erratically1.

We also consider simulations that feature a headless mode.
This is because, a headless mode is critical in a machine
learning context (ref. RQ3). Therefore, we analyse the im-
pact of the GUI in terms of the resource usage. The robot
simulation software examined in this paper are:

1) Ignition [1] is a set of open source software libraries
which are arranged as multiple modular plugins written in
Ruby and C++. They have been developed by Open Robotics
since 2019. It has a similar communication principle to
ROS2. We chose this simulator as it is the successor of
Gazebo.

2)Webots [2] has been developed since 1998 by Cy-
berbotics, a spin-off company from EPFL (Swiss Federal
Institute of Technology Lausanne). It supports a wide range
of sensors and robot controllers out of the box, as well as
being well documented and including several examples ﬁles.
In Figure 1, it has partial headless support because it only
disables the simulation rendering. There is still a GUI visible.

1https://github.com/ros-simulation/gazebo ros2 control/issues/73

We considered it as it is one of the oldest simulation software
still being actively developed.

3) Isaac Sim [3] is a recent, Linux only, simulation
environment developed by Nvidia which runs on the PhysX
engine, and can be programmed in Python or C. By default,
it integrates machine learning capabilities, and has in-built
plugins to generate synthetic data for domain adaptation and
transfer learning. The latter is possible because of its ray
tracing capabilities which allow for a visual simulation as
close to reality. While it can be run in headless mode, this is
not possible while using their ROS2 extension since there is
an issue with the ROS2 plugin not loading when launched
from a python script instead of from the terminal.

4) PyBullet [4] is a Python-based simulation environment
based on the Bullet physics engine which has been in
development since 2016. It is popular for machine learning
research as it is lightweight and easy to use. For this paper,
we implemented a ROS2 compatible plugin since there is no
ofﬁcial ROS2 support.

5) Coppeliasim [5], previously known as V-REP, is a
partially closed source simulation environment, developed
since 2010. It can be programmed directly in Lua, or using
external controllers in 6 other languages. We decided to
include it in our research as it has been compared in previous
simulation software reviews, e.g. [14], [15], [18], [16], [19].

B. Data Capturing

For data capturing and recording, we adopt the metrics
mentioned in Section II, namely, the processor usage (CPU),
the memory usage (RAM) and the execution time (ref. RQ1
& 3). We also record task-speciﬁc data, such as the number
of cubes placed or moved (ref. RQ2 & 3). The execution
time is not mentioned in the literature but was added as
a metric for machine learning, in which the running time
can have major impact, as training a model involves several
iterations of the simulation. A delay of tens of seconds for
one iteration can turn into hours of difference for long train-
ing sessions. In order to accurately record each simulation,
we start recording 5 seconds before the simulation starts
and ends the recording 60 second after the task has ended.
We record processes only related to the simulation while
discarding the rest, such OS-speciﬁc processes.

C. Robotic Tasks

We consider 2 tasks, each divided into 2 sub-tasks, in
order to evaluate each simulator considered here. A sub-task
is repeated 20 times with the aim to reduce the variance
during data recording and to obtain an accurate statistical
characterisation of a simulation. In practice, we found that
more than 20 repetitions does not result
in a statistical
signiﬁcant difference. The 2 tasks along with their rationale
are summarised in Table I. The task execution logic is the
same for all simulations. We must note that we use the
default simulation parameters to setup these tasks. This is to
remove bias while implementing the tasks and avoid tuning
simulator speciﬁc parameters in order to obtain an objective
evaluation for each simulation software.

Name

Design

TABLE I
SUB-TASK OVERVIEW AND DESIGN RATIONALE

A robotic arm randomly takes 5 cm cubes from a table with
21 cubes arranged in a 7 × 3 grid. The task aim is to stack
them into 3 towers of 5 cubes as can be seen in Figure 2. We
consider 3 stacks in order to leave cubes on the table and to
allow for more diversity in each repetition. We set the limit to 5
stacked cubes due to the height of the table and the capabilities
of the robot.
We use the same setup as Task 1-A but without a GUI. This
was chosen as in a machine learning setting, experiments need
to be restarted multiple times and often run on a server with
no screen (ref. RQ3).
A robotic arm will pick up a cube and throw it towards a
pyramid of 6 cubes. The arm goes as far back as mechanically
possible and perform a throwing motion towards the pyramid
in front of it. Figure 2 shows the trajectory taken by the robot
during this task. The cube is released at 50% of the trajectory.
The pyramid is placed such that a successful throw at full
power will collapse it.
We follow the same design as Task 2-A, except without a GUI
(ref. RQ3).

Features
Tested

Friction,
Gravity,
Inertia

Friction,
Gravity,
Inertia,
Kinetic

Data Recorded and Rationale

.

This experiment addresses RQ2
which analyses the numbers of cubes
correctly placed. It will also test the
stability and suitability of the
simulation for long operations, as
recorded by the number of cubes still
in place at the end (ref. RQ1). The
idea of stacking cubes to analyse
performance is motivated from [17]

This task benchmarks the accuracy
and repeatability of the simulation
software and addresses RQ2. The
latter is carried out by recording the
number of cubes that are displaced
from their original position. This idea
has been inspired by a contributor to
Ignition2 demonstrating how to
interface ROS2 and Ignition.

Task 1-A: Pick and
Place

Task 1-B: Pick and
Place Headless

Task 2-A: Throwing

Task 2-B: Throwing
Headless

D. Robot Control

There are 3 methods to control a robot using ROS2,
namely, the joint controller, the joint trajectory follower and
the general purpose ROS controller. The joint controller sets
the position of the joints to a given joint angle using hardware
speciﬁc interfaces of a robot. This is the simplest method as
it provides no feedback to the controller. The joint trajectory
follower uses a ROS action client/server combination in
which the client sends the joint position for a given trajectory
as a list along with a delay. Then, the server continuously
sends the current value of the joints as a feedback mechanism
until the trajectory is completed. This method works well
in practice and we have implemented it for Coppeliasim,
PyBullet and Isaac Sim. For the Ignition and Webots, we
use the general purpose ROS controller (ros control)
[20], which is not implemented for the other simulations. It
provides a wrapper for the joint trajectory follower described
above, but also provides different methods of control such
as a velocity, effort or gripper controller.

IV. EXPERIMENTS

A. Methodology

We use a docker container with Nvidia Ubuntu 20.04
cudagl image for all simulators except for Isaac Sim that
cannot access the base graphics driver API when using
docker. Isaac Sim is thus executed in the base system from
where we run all experiments. ROS2 Foxy has been installed,
along with simulator speciﬁc packages. Docker has been
used to easily provide an image with all the necessary pack-
ages installed without conﬂict between different simulations.
It also allows for reproducibility of these experiments by
providing the same setup every time. The base system runs
an Intel I7 10700 with 32GB of RAM and an Nvidia GeForce

RTX 3060 with Ubuntu 20.04. We used psutil3 which is a
python package that records the CPU and RAM usage. Each
process was monitored at 10 Hz to minimise the resources
impact. For each simulator we used the recommended time
step, and we have ﬁxed all simulators to run in real time. We
use the Franka Panda robot, and its model and conﬁguration
ﬁles provided by MoveIt 2 [21]. The project repository can
be found at https://github.com/09ubberboy90/
ros2_sim_comp.

B. Implementation

The implementation comprises 4 components as shown in

Figure 3 and as noted below.

1 Simulator: launches the simulation software and spawns
the robot along with the gripper and arm controllers.
2 Pathﬁnder: launches Rviz (a ROS data visualisation

software) and MoveIt 2.

3 Controller: chooses which object to grab from the list
of collision objects advertised by MoveIt 2, shown in
Figure 3 as blue arrows. Then, the pick and place or
throwing task is executed accordingly.

4 The Object-Handler: spawns the objects and publishes
their position to the planning scene of MoveIt 2 at
2 Hz. We choose 2 Hz because the scene’s rate of
change in real
time does not deviate considerably.
Higher rates consume more CPU usage which impacts
the performance of all simulations in this paper.

In our implementation, both the arm and the gripper are
controlled using a position controller. We must note that
the gripper controller available in ROS1 has not yet been
ported to ROS2. The latter causes issues while grasping
objects in the simulations (except Webots) as the amount
of force is constant with no feedback. To mitigate this

2https://github.com/AndrejOrsula/ign moveit2
3https://pypi.org/project/psutil/

Fig. 3.

Implementation Diagram

issue, we command the gripper to close its gripper beyond
the optimal closing distance to ensure that the object is
grasped correctly. Webots does not have this issue because
it implements PID controller by default for each simulated
motor. These 4 components are launched using a single
ROS2 launch ﬁle with ad hoc delays to ensure everything
is started as Part 3 and 4 do not work unless part 2 is
launched. For each simulation software, we are using the
default physics time step. The physics engine is also the
default, except for Coppeliasim, in which we use the Newton
physics engine because the other supported physics engines
causes the gripper to fail to grip the cube.

C. Task 1 Experiments

Table II shows the result of task 1, which addresses RQ1
& RQ2. The reason the task times out (failure in Table II)
is because the ROS controller fails to start, or in the case of
Coppeliasim, the client refuses to connect to the simulation
server. The rest of the metrics only focus on the successful
attempts.

therefore,

Ignition and PyBullet did not have timeouts; however,
PyBullet performs signiﬁcantly worse at stacking 5 towers
than the other simulators as 15% of the cubes in average
(i.e 3 cubes) were correctly positioned at the end of the
simulation, and,
the robot does not encounter
scenarios where it collapses towers. Ignition and Webots
are the best performing simulations at executing the task
of stacking cubes, and at keeping the cubes in place. Cop-
peliasim and Isaac Sim, are carrying out the task well at
placing the cube in the right place but, tend to have situations
where the robot collapses the towers. Furthermore, while
Coppeliasim achieves 92% success of placing cubes, we
can observe that it often times out, and reduces its overall
success. We can also observe in Table II that there is no a
statistical signiﬁcant difference between headless and GUI
modes. These results suggest that Ignition (headless and
GUI) succeeds at completing the task more frequently using
the default parameters (ref. RQ1) and has less variation over
different attempts (ref. RQ2).

TABLE II
TASK 1 RESULTS

Name

Failure
(%)

Ignition
Ignition GUI
Isaac Sim GUI
Pybullet
Pybullet GUI
Coppeliasim
Coppeliasim GUI
Webots
Webots GUI

0
0
10
0
0
30
15
5
5

Cubes Placed
(%)

Cubes at the end
of the task (%)

Min Mean Max Min Mean Max
100
60
100
80
100
47
47
7
40
7
100
67
100
53
93
53
100
73

100
100
100
47
40
100
100
100
100

82
88
65
15
11
79
76
79
80

47
67
20
7
7
47
47
47
40

91
94
89
18
11
92
91
88
91

TABLE III
TASK 1 RESOURCES USAGE.

Name
Ignition
Ignition GUI
Isaac Sim GUI
Pybullet
Pybullet GUI
Coppeliasim
Coppeliasim GUI
Webots
Webots GUI

CPU (%)
202 ± 94
205 ± 51
134 ± 25
117 ± 41
140 ± 28
135 ± 42
100 ± 43
144 ± 72
162 ± 61

RAM (MB)
686 ± 313
775 ± 159
10070 ± 1885
663 ± 224
919 ± 168
860 ± 262
850 ± 379
1191 ± 550
1322 ± 410

Table III shows that PyBullet headless consumes fewer
resources overall, while Isaac Sim,
is the most memory
intensive simulation as it consumes 10 times more RAM
than the next simulator (Webots GUI). This is inline with
the current trend of Pybullet being used in machine learning
research (ref. RQ3). It is worth noting that Coppeliasim uses
fewer resources with a GUI than headless. We speculate that
this is because it was initially designed as a GUI application,
with headless support only added at a later date, thus having
received less development focus.

Figure 4 shows the spread of the start time and end time
for each simulation (ref. RQ3). As mentioned in IV-B, Isaac

TABLE IV
TASK 2 RESULTS

Name

Ignition
Ignition GUI
Isaac Sim GUI
Pybullet
Pybullet GUI
Coppeliasim
Coppeliasim GUI
Webots
Webots GUI

Failure
(%)
10
0
0
0
0
32
20
5
15

Cubes Moved %)
Min Mean Max
0
0
18
0
0
9
0
11
21

0
0
83
0
0
50
0
50
50

0
0
0
0
0
0
0
0
0

Fig. 4. Task 1: Mean task start and end time

Sim has to be started manually, thus the time that takes
the simulation to start is not captured in the plot. Ignition
takes the most time to load, because it requires an additional
movement to the start position that it has to do to start
the effort controller. Webots ﬁnishes the earliest with little
variation. Combined with its relatively high success rate
from Table II, Webots appears to be ideal for prototyping
robotic tasks and for machine learning experiments due to
its relatively high success rate from Table II and ﬁnishing
the task and simulation early with low variation. PyBullet,
on the other hand, takes the most time, and combined with its
high failure rate (with the default parameters), it may not be
suitable for machine learning as it would take more time to
run a single experiment. Similarly, further parameter tuning
would be required in order to obtain a stable simulation that
succeeds at completing the task.

D. Experiment 2

As shown in Table IV, which focuses on RQ1 & 3, only
Webots throws consistently. Isaac Sim consistently manages
to throw the cube but fails to hit the pyramid as the motion
behaviour is not consistent. We speculate that this is because
we did not tune the simulation parameters and used the
default values. Coppeliasim and PyBullet manages to hit
the pyramid, but the behaviour is rare as the few times the
arm manages to successfully perform the throwing motion,
the throw is not always at the same distance nor perfectly
aligned. Coppeliasim has a high timing out rate (failure
column in Table IV) due to the reasons mentioned in Section
IV-C. Finally, for Ignition, the success at hitting the pyramid
is zero. We observe that, in most cases, the cube falls in
transit, especially when the arm is as far back as possible
and starts to move at full speed for the throwing motion. At
this point, the robot and the cube have the highest moment of
inertia, and if the friction between the cube and the gripper
is not enough, the cube falls. We must note that we ﬁx the
friction parameter to explore the default capabilities for each
simulator. We also notice that there are instances when the
robot manages to throw the cube but does not hit the pyramid.

This is because the gripper controller had a delay in opening
its gripper, changing the thrown cube landing spot.

Table V shows similar results to task 1. Coppeliasim
uses the lowest amount of CPU while Ignition uses the
less memory. The CPU usage for all simulations observes
less variation. This could be due to the simplicity of the
world and the short time of execution. As mentioned in IV-
C, Coppeliasim still uses fewer resources with a GUI than
headless. Figure 5 shows similar start and end time for all
simulations, observing lower variations compared to task 1.
The reason for this is because the relatively short time of
execution and the low amount of path planning that can fail
and delay the execution. For this scenario, considering only
the time of execution will not have and impact on the choice
for a machine learning approach as the difference between
execution is minimal. If the resource usage is important,
then Coppeliasim should be considered for machine learning
tasks. Otherwise, a more successful simulation should be
considered such as Webots.

V. CONCLUSIONS & FUTURE WORK

In this paper, we have investigated current robot simulation
software performance and their suitability to perform two dif-
ferent robotic manipulation tasks. We have also developed a
methodology to systematically benchmark robot simulations
under similar parameters, tasks and scenarios. Based on our
experimental results, Webots appears to be the more suitable
for long-term operations while still succeeding at completing
a given task (ref. RQ1) and be able to replicate the same sim-
ulation conditions across attempts (ref. RQ2). Webots would
only be suitable for machine learning if the execution time
and resources are not a requirement while training machine
learning models (ref. RQ3). Ignition, while comparable to
Webots, is more suited to answer RQ1 & RQ3. RQ2 is only
satisﬁed if the task is slow moving and constant. We must
note that Ignition is still in development and some of the
challenges we encountered while implementing both tasks
and carrying out our experiments may be mitigated in the
future. Coppeliasim and PyBullet have less impact in terms
of resource usage and are the most suited to answer RQ3.
That is, Coppeliasim provides better stability for task success
at the cost of timing out more often. Finally, Isaac Sim only
satisﬁes RQ1, as the simulated scene was not repeatable
across attempts.

TABLE V
TASK 2 RESOURCES USAGE

Name
Ignition
Ignition GUI
Isaac Sim GUI
Pybullet
Pybullet GUI
Coppeliasim
Coppeliasim GUI
Webots
Webots GUI

CPU (%)
152 ± 99
137 ± 103
148 ± 48
139 ± 32
136 ± 31
115 ± 37
98 ± 28
141 ± 74
134 ± 72

RAM (MB)
524 ± 340
574 ± 371
11123 ± 1865
730 ± 174
828 ± 205
742 ± 185
838 ± 214
1275 ± 261
1207 ± 198

[5] E. Rohmer, S. P. N. Singh, and M. Freese, “V-rep: A versatile and
scalable robot simulation framework,” in 2013 IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2013, pp. 1321–1326.

[6] J. Banks, “1999: INTRODUCTION TO SIMULATION,” p. 7.
[7] Y. Lu, C. Liu, K. I.-K. Wang, H. Huang, and X. Xu, “Digital
Twin-driven smart manufacturing: Connotation,
reference model,
applications and research issues,” Robotics and Computer-Integrated
Manufacturing, vol. 61, p. 101837, Feb. 2020. [Online]. Available:
https://linkinghub.elsevier.com/retrieve/pii/S0736584519302480
[8] P. Tavares, J. Silva, P. Costa, G. Veiga, and A. Moreira, “Flexible Work
Cell Simulator Using Digital Twin Methodology for Highly Complex
Systems in Industry 4.0,” Nov. 2018, pp. 541–552.

[9] Stanford Artiﬁcial Intelligence Laboratory et al., “Robotic operating

system.” [Online]. Available: https://www.ros.org

[10] A. Oreb¨ack and H. I. Christensen, “Evaluation of Architectures for
Mobile Robotics,” Autonomous Robots, vol. 14, no. 1, pp. 33–49, Jan.
2003. [Online]. Available: https://doi.org/10.1023/A:1020975419546
for
autonomous mobile robots: A survey,” Autonomous Robots, vol. 22,
no. 2, pp. 101–132, Jan. 2007. [Online]. Available: http://link.springer.
com/10.1007/s10514-006-9013-8

and M. Scheutz,

[11] J. Kramer

“Development

environments

[12] A. Staranowicz and G. L. Mariottini, “A survey and comparison
of commercial and open-source robotic simulator software,” in
Proceedings of
the 4th International Conference on PErvasive
Technologies Related to Assistive Environments, ser. PETRA ’11.
New York, NY, USA: Association for Computing Machinery, May
2011, pp. 1–8. [Online]. Available: https://doi.org/10.1145/2141622.
2141689

[13] N. Koenig and A. Howard, “Design and use paradigms for Gazebo, an
open-source multi-robot simulator,” in 2004 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) (IEEE Cat.
No.04CH37566), vol. 3, Sep. 2004, pp. 2149–2154 vol.3.

[14] L. Nogueira, “Comparative Analysis Between Gazebo and V-REP

Robotic Simulators,” Tech. Rep., Dec. 2014.

[15] L. Pitonakova, M. Giuliani, A. Pipe, and A. Winﬁeld, “Feature and
Performance Comparison of the V-REP, Gazebo and ARGoS Robot
Simulators,” in Towards Autonomous Robotic Systems, M. Giuliani,
T. Assaf, and M. E. Giannaccini, Eds. Cham: Springer International
Publishing, 2018, pp. 357–368.

[16] A. Ayala, F. Cruz, D. Campos, R. Rubio, B. Fernandes, and
R. Dazeley, “A Comparison of Humanoid Robot Simulators: A
Quantitative Approach,” arXiv:2008.04627 [cs], Aug. 2020, arXiv:
2008.04627. [Online]. Available: http://arxiv.org/abs/2008.04627
[17] M. K¨orber, J. Lange, S. Rediske, S. Steinmann, and R. Gl¨uck,
“Comparing Popular Simulation Environments in the Scope of
Robotics and Reinforcement Learning,” arXiv:2103.04616 [cs], Mar.
2021, arXiv: 2103.04616. [Online]. Available: http://arxiv.org/abs/
2103.04616

[18] M. S. P. d. Melo, J. G. d. S. Neto, P. J. L. d. Silva, J. M. X. N.
Teixeira, and V. Teichrieb, “Analysis and Comparison of Robotics
3D Simulators,” in 2019 21st Symposium on Virtual and Augmented
Reality (SVR), Oct. 2019, pp. 242–251.

[19] S. Ivaldi, V. Padois, and F. Nori, “Tools for dynamics simulation
feedback,” arXiv:1402.7050
[Online]. Available: http:

of
robots: a survey based on user
[cs], Feb. 2014, arXiv: 1402.7050.
//arxiv.org/abs/1402.7050

[20] S. Chitta, E. Marder-Eppstein, W. Meeussen, V. Pradeep, A. R.
Tsouroukdissian, J. Bohren, D. Coleman, B. Magyar, G. Raiola,
M. L¨udtke, and E. F. Perdomo, “ros control: A generic and
framework for ROS,” Journal of Open Source
simple control
Software, vol. 2, no. 20, p. 456, Dec. 2017. [Online]. Available:
https://joss.theoj.org/papers/10.21105/joss.00456

[21] T. Coleman David, “Reducing the Barrier to Entry of Complex
a MoveIt! Case Study,” 2014, publisher:
[Online]. Available: https:

Robotic Software:
Universit`a degli
studi di Bergamo.
//aisberg.unibg.it//handle/10446/87657

[22] P. Chang and T. Padif, “Sim2Real2Sim: Bridging the Gap Between
Simulation and Real-World in Flexible Object Manipulation,” in 2020
Fourth IEEE International Conference on Robotic Computing (IRC),
Nov. 2020, pp. 56–62.

[23] N. Pitsillos, A. Pore, B. S. Jensen, and G. Aragon-Camarasa,
“Intrinsic Robotic Introspection: Learning Internal States From Neuron
Activations,” in 2021 IEEE International Conference on Development
and Learning (ICDL). Beijing, China: IEEE, Aug. 2021, pp. 1–7.
[Online]. Available: https://ieeexplore.ieee.org/document/9515672/

Fig. 5. Task 2: Mean task start and end time

From our review and experimental results, we found
that current robot simulation software could not be used
to develop a digital twin. This is because the simulators
considered in this paper cannot maintain a repeatable sim-
ulated scene over time. We hypothesise that a continuous
feedback mechanism is needed between the simulation and
reality similar to [22] in order to maintain an accurate
representation of the real environment. While this paper
focused on benchmarking robot simulation software, future
work consists of optimising each simulator to minimise
failure rates and maximise task completion, and benchmark
them accordingly. Additionally, the Unreal Engine plugin
for ROS2, has recently seen more development and could
potentially replace Unity in our original plan. We also aim to
speciﬁcally benchmark each simulation in a machine learning
context such as in [23] with the view to develop a digital twin
that can take advantage of a simulated environment to deploy
AI solutions for autonomous robotic systems.

REFERENCES

[1] O. Robotics, “https://ignitionrobotics.org/home.” [Online]. Available:

https://ignitionrobotics.org/home

[2] Webots, “http://www.cyberbotics.com,” open-source Mobile Robot
Simulation Software. [Online]. Available: http://www.cyberbotics.com
[3] N. Corporation, “Nvidia isaac sim.” [Online]. Available: https:

//developer.nvidia.com/isaac-sim

[4] E. Coumans and Y. Bai, “Pybullet, a python module for physics sim-
ulation for games, robotics and machine learning,” http://pybullet.org,
2016–2021.

