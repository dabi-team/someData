2
2
0
2

l
u
J

4
1

]
T
S
.
h
t
a
m

[

1
v
5
7
0
7
0
.
7
0
2
2
:
v
i
X
r
a

Adversarial Sign-Corrupted Isotonic Regression

Shamindra Shrotriya Matey Neykov

Department of Statistics & Data Science
Carnegie Mellon University
Pittsburgh, PA 15213

{sshrotri, mneykov}@andrew.cmu.edu

July 15, 2022

Abstract

Classical univariate isotonic regression involves nonparametric estimation under a
monotonicity constraint of the true signal. We consider a variation of this generating
process, which we term adversarial sign-corrupted isotonic (ASCI) regression. Under this
ASCI setting, the adversary has full access to the true isotonic responses, and is free to sign-
corrupt them. Estimating the true monotonic signal given these sign-corrupted responses
is a highly challenging task. Notably, the sign-corruptions are designed to violate mono-
tonicity, and possibly induce heavy dependence between the corrupted response terms.
In this sense, ASCI regression may be viewed as an adversarial stress test for isotonic re-
gression. Our motivation is driven by understanding whether eﬃcient robust estimation
of the monotone signal is feasible under this adversarial setting. We develop ASCIFIT, a
three-step estimation procedure under the ASCI setting. The ASCIFIT procedure is con-
ceptually simple, easy to implement with existing software, and consists of applying the
PAVA with crucial pre- and post-processing corrections. We formalize this procedure, and
demonstrate its theoretical guarantees in the form of sharp high probability upper bounds
and minimax lower bounds. We illustrate our ﬁndings with detailed simulations.

Table of Contents

1

Introduction

1.1 Adversarial sign-corrupted isotonic (ASCI) regression . . . . . . . . . . . . .

1.2

Interesting special cases of ASCI regression . . . . . . . . . . . . . . . . . . .

1.3 Motivation and focus of our work . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Prior and related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.5 Main contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6 Organization of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 ASCIFIT: A three-step estimation procedure for µ

1

3

3

4

5

6

7

7

7

7

 
 
 
 
 
 
2.1

Intuition for the three ASCIFIT steps . . . . . . . . . . . . . . . . . . . . . .

8

3 Analysis of ASCIFIT: Upper bounds

4 Lower bounds

5 Simulations

6 Conclusion

7 Acknowledgments

Appendix

10

11

12

13

13

17

2

1

Introduction

Isotonic regression is a classically studied nonparametric regression problem in which the
underlying signal satisﬁes a monotonicity constraint. In the univariate case, this regression
setup provides a ﬂexible nonparametric generalization to simple linear regression. That is, the
underlying signal may be non-linear, but still satisﬁes monotonicity as in the simple linear
model. The classically studied isotonic regression generating process is formally described in
Deﬁnition 1:

Deﬁnition 1 (Classical isotonic regression). We consider n observations, {Yi | i ∈ [n]}, where
each observation Yi is generated from the following model:

Yi = µi + εi
s.t. µ1 ≤ µ2 ≤ . . . ≤ µn
and εi

i.i.d.∼ N (cid:0)0, σ2(cid:1)

(1)

(2)

(3)

The statistical goal under this classical setup is to estimate the underlying signal vector
µ := (µ1, . . . , µn)(cid:62), subject the monotonicity constraint in Equation (2), while σ is an un-
known (nuisance) parameter. Throughout this paper we will adopt the convention, without
loss of generality, that the signal vector is monotonically increasing (as per Equation (2)).
Additionally we will assume that all estimation errors are computed under square loss (in
Euclidean metric), in high probability.

1.1 Adversarial sign-corrupted isotonic (ASCI) regression

Our work in this paper is motivated by a variation of the classical isotonic regression es-
timation problem, per Deﬁnition 1. We refer to this newly proposed model as adversarial
sign-corrupted isotonic (ASCI) regression. The generating process for this ASCI estimation
problem is formalized in Deﬁnition 2.

Deﬁnition 2 (Adversarial sign-corrupted isotonic (ASCI) regression). We consider n observa-
tions, {Ri | i ∈ [n]}, where each observation Ri is generated from the following model:

Ri = ξi(µi + εi)

s.t. 0 < η ≤ µ1 ≤ µ2 ≤ . . . ≤ µn
and εi
and ξi ∈ {−1, 1}

i.i.d.∼ N (cid:0)0, σ2(cid:1)

(4)

(5)

(6)

(7)

Remark 1. We note that the constant η > 0 is known to the observer and adversary as part
of the generating process. This means that the true signal is positive, since it is uniformly
bounded away from η. It is an artefact of our method but as we will see later in Examples 3
and 4, highly non-trivial estimation tasks are still contained under this constraint.

Remark 2. Throughout this paper we will interchangeably use the terms ASCI regression,
setting, setup, model, and generating process to refer to Deﬁnition 2.

By comparing Deﬁnitions 1 and 2, this ASCI regression generating process is a partial
generalization of the classical isotonic regression. It can be brieﬂy described as follows. Here
the classical isotonic regression responses, µi + εi in Equation (1), are sign-corrupted in a

3

manner chosen by an adversary, as captured by the multiplicative ξi terms. Here the ξi ∈
{−1, 1} are sign-corruptions for the true data generating process, i.e., Yi := µi + εi.
It is
important to note that the ξi ∈ {−1, 1} for each i ∈ [n], are chosen given that the adversary
has full access to the true responses, i.e., {µi + εi | i ∈ [n]}. As such, Equation (1) in the
classical isotonic regression setup represents a special case of Equations (4) and (7) by taking
a.s.= 1 for each i ∈ [n]. However, we note that in this ASCI setting, in Equation (5) the
ξi
monotonically increasing signal vector µ := (µ1, . . . , µn)(cid:62) is bounded below by η, which is
some ﬁxed and known positive constant. As such this represents a restriction of the classical
isotonic regression condition described in Equation (2). In summary, ASCI regression represents
both a restriction and relaxation of the classical isotonic regression generating process. We
will see why the restriction is necessary in this work, but we will later suggest possible ways
in which it can be relaxed in future work.

1.2

Interesting special cases of ASCI regression

Interestingly, we note that even some special cases of this ASCI regression setup can result in
highly non-trivial estimation tasks. Two particular ASCI regression special cases are formalized
in Examples 3 to 4.

Example 3 (Two-component Gaussian mixture ASCI regression special case). We consider n
observations, {Ri | i ∈ [n]}, where each observation Ri is generated from the following model:

Ri = ξiµi + εi

s.t. 0 < η ≤ µ1 ≤ µ2 ≤ . . . ≤ µn
and εi
and ξi

i.i.d.∼ N (cid:0)0, σ2(cid:1)
i.i.d.∼ Rademacher(p), p ∈ (0, 1), and ξi ⊥⊥ εi

(8)

(9)

(10)

(11)

i.i.d.∼ Rademacher(p) for each i ∈ [n], means that ξi = +1 with proba-
Remark 3. Note that ξi
bility p, and ξi = −1 with probability 1 − p. We note that the model deﬁned in Example 3 is
a special case of Deﬁnition 2. This is formally proved in Appendix B.3.

We note that in the univariate setting, Example 3 represents a generalization of the
two-component Gaussian mixture model studied in detail in Balakrishnan et al. [2017, Sec-
tion 3.2.1]. Our model generalizes their setting in the sense that we allow a diﬀerent mean,
i.e., µi, for each of the n univariate observations. Interestingly, in this more general univariate
mixture setting, our proposed ASCIFIT estimator (see Section 2) provides an eﬃcient alterna-
tive to the EM algorithm [Dempster et al., 1977]. Such models have extensive applications,
e.g., community detection [Giraud and Verzelen, 2018, Royer, 2017].

Example 4 (Non-convex ASCI regression special case). We consider n observations, {Ri | i ∈ [n]},
where each observation Ri is generated from the following model:

Ri = γi + εi

s.t. 0 < η ≤ |γ1| ≤ |γ2| ≤ . . . ≤ |γn|
and εi

i.i.d.∼ N (cid:0)0, σ2(cid:1)

(12)

(13)

(14)

Remark 4. Verifying that Example 4 is a special case of Deﬁnition 2 is not a priori obvious,
and is formally proved in Appendix B.4.

4

In Example 4, one can think of this model being generated from the ASCI model as per Def-
inition 2. In this special case, the adversary randomly chooses sign-corruptions independently
of the error terms, i.e., ξi ⊥⊥ εi for each i ∈ [n]. Under this setup, the resulting response term
in Equation (12) is the same as the classical isotonic regression response, as seen by comparing
to Equation (1). However, interestingly the adversarial sign-corruption is now absorbed into
the revised monotonicity constraint 0 < η ≤ |γ1| ≤ |γ2| ≤ . . . ≤ |γn|, per Equation (13). As a
result, this generating process is a highly non-convex constrained estimation problem. In this
case ASCIFIT will allow one to recover |γi|, where as PAVA will not provide any information on
the |γi| (or γi), given the non-convex constraint.

1.3 Motivation and focus of our work

With the ASCI regression setup clearly deﬁned, we turn our attention to describing the focus
of our analysis in this work. This is summarized by the following core question of interest:

Core question: Given the adversarial sign-corrupted isotonic (ASCI) re-
gression setup in Deﬁnition 2, can we ﬁnd a computationally eﬃcient esti-
mator for µ, and demonstrate its precise (non-asymptotic) statistical opti-
mality?

To the best of our knowledge the ASCI model, and our core question of interest, have not
been explicitly studied before in the literature. We note that this ASCI estimation problem
is inherently challenging, and thus interesting, for three main reasons, i.e., Challenge I –
Challenge III:

Challenge I (Dependent responses): in this estimation problem the adversary is free to
choose the sign-corruption terms ξi, after observing all samples, possibly resulting in a strong
dependence between the original isotonic responses. As such, any ASCI estimator must be able
to handle arbitrary dependence structure between the sign-corrupted responses.

Challenge II (Violating signal monotonicity): qualitatively speaking, the sign-corruptions
are in a sense ‘extreme’ in that by selectively changing the sign of the observations the ad-
versary fundamentally ‘attacks’ the isotonic monotonicity constraint directly. It is this convex
monotone constraint which classical isotonic estimators, i.e., PAVA, are designed to exploit.

Challenge III (Interesting special cases): The ASCI setting contains interesting non-
trivial special cases as described in Examples 3 and 4. Naively applying typical least squares
estimation techniques will be unable to provide any relevant information on the estimated
quantity of interest.

Given these three formidable challenges posed by ASCI regression, any computationally and
statistically eﬃcient estimator here needs to utilize new techniques to exploit the potential non-
convex structure in our setting. Our motivations here are thus driven by understanding the
robustness of existing isotonic regression estimators under such adversarial settings. Moreover,
for the ASCI setting to be worth studying, we wanted ensure practical algorithms for estimation
under this adversarial setting, with sharp minimax (worst-case) statistical guarantees, both
of which we were able to provide. We thus view the ASCI setting as stimulating prototype for
more such research into adversarial robustness in isotonic regression.

5

1.4 Prior and related work

As noted, to the best of knowledge our core question of interest, i.e., isotonic regression under
the proposed ASCI setup, has not been previously studied. Our work however builds on and
utilizes known estimators from the classical isotonic regression literature. As such we limit our
prior and related work summary on known risk bounds (and rates) for such isotonic regression
estimators, and the eﬃcient algorithms (i.e., the PAVA) and practical implementations thereof.

Isotonic regression (classical):

A lively historical overview of isotonic regression estimation from a computational lens
In brief, we note that the origins of isotonic
is given in de Leeuw et al. [2009, Section 1].
regression can be traced back to a number of independently written papers in the 1950s. In
particular it was studied by Ayer et al. [1955], Brunk [1955]. Such estimators for “ordered
parameters” were also analyzed in the series of papers van Eeden [1956, 1957a,b,c] which
culminated into a PhD thesis in by the same author [van Eeden, 1958]. Shortly thereafter
the articles [Bartholomew, 1959a,b] also investigated the related idea of hypothesis testing
under monotonicity constraints. We refer the interested reader to the classical comprehensive
references Barlow et al. [1972], Robertson et al. [1988], for further reading.

The classical isotonic regression setup per Deﬁnition 1 under square loss is a convex opti-
mization problem. As such, it has a unique solution, i.e., the Euclidean projection onto the
closed convex monotone cone given by the constraint in Equation (2). In this case, the non-
asymptotic risk bounds for the least squares estimator (LSE) of the monotone parameters µi
are of the order n−2/3 in sample complexity. This convergence rate has been established across
a number of papers including Birgé and Massart [1993], Chatterjee et al. [2015], Donoho [1990],
Meyer and Woodroofe [2000], van de Geer [1990, 1993], Wang [1996], Zhang [2002]. Broadly
speaking, these results typically vary in the generality of their underlying assumptions on the
normality or independence of the error terms in classical isotonic regression. As noted in the
excellent recent survey Guntuboyina and Sen [2018], the same risk rate for this (and for more
general) LSEs was established using an alternative approach in Chatterjee [2014]. Moreover,
in the case of minimax lower bounds, the matching risk rate (up to constant terms) for isotonic
regression was established in Chatterjee et al. [2015] and also in Bellec and Tsybakov [2015],
in both high probability and expectation terms.

Pool Adjacent Violators Algorithm (PAVA):

Rather remarkably, despite the nonparametric setup of classical isotonic regression, the
LSE in this case has an explicit ‘max-min’ formulation [Barlow et al., 1972, Equation (1.9)].
However, in practice it is eﬃciently computed using the pool adjacent violators algorithm
(PAVA). As described in Tibshirani et al. [2011] the PAVA in eﬀect estimates the ordered param-
eters by scanning through the (sorted) observations. For each adjacent pair of observations,
the monotonicity constraint is checked. If the constraint is ‘violated’ by a given observation,
the average of the observations is used as the estimate, with appropriate (minimal) back-
tracking to ensure that any restrospectively incurred violations are similarly corrected for.
Eﬃcient PAVA implementations, e.g., as described in Best and Chakravarti [1990], Grotzinger
and Witzgall [1984], have a computational complexity of O (n), where n is the sample size.
Since we will use the PAVA in just one step in our proposed three-step estimator for the ASCI
regression parameter µ, we will not detail it further here. However, such open-source PAVA
implementation details can be found in de Leeuw et al. [2009], Pedregosa et al. [2011].

6

1.5 Main contributions

Our contributions in this paper are twofold and are summarized as follows:

• Computable estimators with non-asymptotic upper bounds: We propose a com-
putationally eﬃcient three-step algorithm ASCIFIT, to estimate the required parameter
µ, under the ASCI setting. Our ASCIFIT estimator converges at a n−2/3 rate, with high
probability. We illustrate our ﬁndings with extensive numerical simulations.

• Sharp minimax lower bounds: we provide matching high probability lower bounds
(up to constant and log factors) under square loss, and thus demonstrate that our esti-
mators are minimax optimal in this sense.

In particular, our upper bound proofs involve rather subtle theoretical details about the
PAVA, and our use of method of moment techniques is quite unique in this literature. We believe
these proof techniques will be of independent interest to researchers in isotonic regression. In
particular, for similar adversarial estimation tasks, where traditional convex M-estimation
techniques are infeasible.

1.6 Organization of the paper

The rest of this paper is organized as follows. In Section 2 we introduce ASCIFIT, our three-
step estimation procedure for µ. In Section 3 we provide high probability upper bounds on
estimation rates using ASCIFIT. In Section 4 we establish sharp minimax lower bounds for the
parameter estimation in our ASCI setting. In Section 5 we provide extensive numerical ASCI
simulations, to illustrate our ﬁndings. In Section 6 we summarize our results and describe
exciting future research directions.

1.7 Notation

Throughout this paper, we typically use lowercase for scalars in R, e.g., (x, y, z, . . .), bold
lowercase for vectors, e.g., (x, y, z, . . .), and bold uppercase for matrices, e.g., (X, Y, Z, . . .).
We use (cid:46) and (cid:38) to mean ≤ and ≥, respectively, up to positive universal constants. We
denote a ∨ b := max {a, b} for each a, b ∈ R. We say that a sequence an := O (1) if there exists
C > 0, N ∈ N such that |an| < C for each n > N . Similarly, an = O (bn) iﬀ an
= O (1). We
bn
= o (1).
say that a sequence an = o (1) if an → 0 as n → ∞. Similarly, an = o (bn) iﬀ an
bn
We denote the ﬁnite set {1, . . . , n} by [n]. We deﬁne the indicator function IΩ(x) to take the
value 1 when x ∈ Ω ⊆ Rd, and 0 otherwise. We say that a function f : Ω → R is increasing,
if for all u, v ∈ Ω ⊂ R such that u ≤ v, implies f (u) ≤ f (v). We use strictly increasing in
the case where these inequalities are strict. Similarly we note that f is decreasing (or strictly
decreasing) when these respective inequalities are reversed. We provide a useful notation
summary table in Appendix A.1.

2 ASCIFIT: A three-step estimation procedure for µ

As per our core question of interest, we now turn our attention to ASCIFIT, i.e. our proposed
estimation procedure for µ, under the ASCI setup. The Folded Normal distribution, and in
particular its mean and variance, will be fundamental to ASCIFIT. As such, we ﬁrst formalize
the key properties of the Folded Normal distribution in Deﬁnition 5.

7

Deﬁnition 5 (Folded Normal distribution). Suppose R ∼ N (µ, σ2), and let T := |R|. We
then say that T ∼ FoldNorm(µ, σ), is a Folded Normal distribution. We denote the mean and
variance of T , by f (µ, σ) and g(µ, σ), respectively. They are given as follows:

f (µ, σ) := E (T ) = σ(cid:112)2/π exp(−µ2/(2σ2)) − µ(1 − 2Φ(µ/σ)).

g(µ, σ) := Var (T ) = µ2 + σ2 − f (µ, σ)2.

(15)

(16)

Remark 5. We refer the reader to Elandt [1961], Tsagris et al. [2014] for more details. We only
consider µ > η > 0 per Equation (5), and we use the shorthand notation f (µ, σ)2 := (f (µ, σ))2.
We now describe ASCIFIT, our three-step procedure to estimate µ under the ASCI setting,

as follows:

ASCIFIT: Three-step procedure to estimate µ under the ASCI setting

Step I (Pre-processing and PAVA):
Obtain an initial naive estimate of µ := (µ1, . . . , µn)(cid:62) by ﬁtting isotonic regression
(using the PAVA) on Ti := |Ri|. Denote these estimates by

(cid:98)µnaive := ( (cid:98)T1, . . . , (cid:98)Tn)(cid:62).

Step II (Second moment matching ):
Estimate σ in the following way. Pick the σ solving the following equation, and denote
the corresponding solution as

(cid:98)σ:

G(σ) := σ2 +

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 =

i=1

1
n

n
(cid:88)

i=1

T 2
i .

(17)

Here f −1(·, σ), denotes the inverse function of f (µ, σ) with respect to µ, when we hold
σ ﬁxed to the value σ.

Step III (Post-processing via plug-in):
From

(cid:98)µnaive in Step I, and

(cid:98)σ in Step II, compute

(cid:98)µi := f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ), for each i ∈ [n].

(cid:98)µasciﬁt := ((cid:98)µ1, . . . , (cid:98)µn)(cid:62) as follows:
(18)

2.1

Intuition for the three ASCIFIT steps

We now provide more precise intuition for each of the three ASCIFIT steps, i.e., Step I – Step
III.

Intuition for Step I:

Here, we begin with the pre-processing operation Ti := |Ri|. This serves the critical dual
purpose of removing the eﬀect of the sign-corruptions ξi, and also induces independence of the
resulting observations (T1, . . . , Tn)(cid:62). This helps directly address Challenge I and Challenge
II under the ASCI setup. To better understand this dual eﬀect, note that in the ASCI setup,
the ξi ∈ {−1, 1} may be arbitrarily chosen by the adversary (without a precise distributional
assumption). However, the critical information in our model is given by pre-processing each
observation, Ri, as Ti := |Ri|. More speciﬁcally we have that Ti = |ξi (µi + εi)| = |µi + εi|.
i.n.i.d.∼ FoldNorm (µi, σ), per
Since µi + εi

i.n.i.d.∼ N (cid:0)µi, σ2(cid:1), per Deﬁnition 5 we have that Ti

8

Deﬁnition 5. We note that our pre-processed observations {T1, . . . Tn} are all i.n.i.d.1, since
they have a common variance σ2 but varying means µi for each observation i ∈ [n]. Moreover,
ﬁtting an isotonic regression to Ti intuitively estimates f (µi, σ) which are the mean of each
Ti. This step is formally justiﬁed by the results of Zhang [2002].
Intuition for Step II:

(cid:80)n

i=1 µ2
i

, is σ2 + 1
n
i=1 µ2
i

This is motivated by second moment matching to estimate σ. Speciﬁcally, using the fact
(cid:80)n
that the expected value of 1
. The left hand side of Equation (17)
i=1 T 2
i
n
(cid:80)n
directly estimates the term σ2 + 1
. In Step II it is not clear a priori whether such
n
an inverse function f −1(·, σ) exists, or whether there exists a unique positive solution for σ
in Equation (17). We will demonstrate that both assertions are true, and that the unique
(cid:98)σ, to estimate σ, can be computed eﬃciently with a binary search approach. We
solution
would like to note here that estimating σ is not an easy problem (it is not by coincidence
that in classical isotonic regression that σ is viewed as a nuisance parameter). This diﬃculty
explains why we need to impose some additional assumptions on the vector µ and on σ later
on. Next, we provide the intuition on why we use the factor (cid:98)Ti ∨ f (η, σ) in Step II, for each
i ∈ [n]. This is summarized in Proposition 6.

Proposition 6 (Reason for the “∨f (η, σ)”-correction in Step II). The need for deﬁning the
∨f (η, σ) in Equation (17) in Step II in ASCIFIT, is that the solution to the problem

arg min
(cid:101)T1,..., (cid:101)Tn

n
(cid:88)

(Ti − (cid:101)Ti)2 s.t. f (η, σ) ≤ (cid:101)T1 ≤ . . . ≤ (cid:101)Tn,

i=1

is related to the solution to

arg min
(cid:98)T1,..., (cid:98)Tn

n
(cid:88)

i=1

(Ti − (cid:98)Ti)2 s.t. (cid:98)T1 ≤ . . . ≤ (cid:98)Tn,

as (cid:101)Ti := (cid:98)Ti ∨ f (η, σ).

(19)

(20)

To understand the signiﬁcance of Proposition 6, ﬁrst note that we apply the PAVA to the
Ti values in Step I. As such, the corresponding least squares PAVA estimates, (cid:98)Ti, actually
project onto the unconstrained monotone cone, as per Equation (20). However, in our setup
we actually want to solve the constrained non-negative monotone means, as per Equation (19).
Fortunately, this is not an issue since we can simply post hoc correct each of the ﬁtted uncon-
strained PAVA solutions as (cid:101)Ti := (cid:98)Ti ∨ f (η, σ), for each i ∈ [n]. This follows by adapting Németh
and Németh [2012, Corollary 1] to our ASCIFIT setup. From all of the above discussion, in-
tuitively it follows that the term σ2 + 1
i=1(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 in (17) also estimates
n
σ2 + 1
n

(cid:80)n
, which explains why in Step II we equate that term to 1
n

i=1 T 2
i

(cid:80)n

(cid:80)n

.

i=1 µ2
i
Intuition for Step III:

To understand the need for this step, one needs to realize that the PAVA will estimate the
means of Ti which are f (µi, σ). Hence in order for us to go back at the original µi scale, we
need to invert the value of the PAVA estimates (cid:98)Ti. Ideally we would use the true value of σ
(cid:98)σ as computed
in the inversion, but since it is unavailable to us, we use the plug-in estimate
in Step II. In addition the term “∨f (η, (cid:98)σ)” in Equation (18) is present, since by assumption
the value of each µi (or suﬃciently just µ1) must be at least η, after inverting.

1i.e., independent but not identically distributed.

9

3 Analysis of ASCIFIT: Upper bounds

We have now described the details and key intuition behind our three-step ASCIFIT estimator
(cid:98)µasciﬁt, for µ. We now turn our attention to formalizing this intuition into least squares
estimation risk bounds. More speciﬁcally, our end goal in this section is to describe our high
probability non-asymptotic upper risk bound for
(cid:98)µasciﬁt, and understand its dependence on
the sample complexity, and other ASCI parameters. We also provide summary sketch behind
the main proof techniques used and what insight they oﬀer for estimation purposes. Before we
state the results we will deﬁne the rate of convergence rn,2(µn, µ1, σ), which plays an important
role in all of the Theorems to follow. For an absolute constant C2 > 0 deﬁne

rn,2(µn, µ1, σ) := min

(cid:20)

2σ2C2
2 ,

27
4

(cid:18) µn − µ1
n

(cid:19) 2
3

(σC2)

4
3 +

2σ2C2
2
n

(1 + log n)

(cid:21)
.

(21)

Importantly, assuming that µn − µ1, σ are constants not scaling with the sample size n, we
have that rn,2(µn, µ1, σ) (cid:46) max (cid:8) (cid:16) σ2V
3 , σ2 log n
}, where V := µn − µ1, is the total variation
n
of the underlying monotone signal. With this essential background, we are ready to state our
ﬁrst result in Theorem 7.

(cid:17) 2

n

(cid:80)n

Theorem 7 (Equation (17) has a unique root). Assume that there exist constants ψ, Ψ, C > 0
i ≤ C, for each n ∈ N. In addition let rn,2(µn, µ1, σ) =
such that ψ ≤ σ ≤ Ψ and 1
n
o(1), where the quantity rn,2(µn, µ1, σ) is deﬁned in (21). Then for suﬃciently large n, δ =
, and γ = o (cid:0)n1/2(cid:1), under the ASCI setup, Equation (17) in ASCIFIT has
o
(cid:113) 1
n

(rn,2(µn, µ1, σ))−1(cid:17)
(cid:104)
0,

for σ with probability at least 1 − δ−1 − 2γ−2.

a unique root σ∗ ∈

i=1 T 2
i

i=1 µ2

(cid:80)n

(cid:16)

(cid:105)

(cid:80)n

(cid:80)n

(cid:80)n

(cid:113) 1
n

i=1 T 2
i

, evaluated at σ ∈ {0,

The key insight of Theorem 7 from a statistical perspective, is that our second moment
matching approach in Step II will ensure that our proposed estimator
(cid:98)σ, for σ, will be
unique with high probability. The core idea behind the proof of Theorem 7 is that the map
i=1(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 is monotone increasing over σ ≥ 0. This
σ (cid:55)→ G(σ) := σ2 + 1
n
enables the use of the intermediate value theorem to check that two endpoints of G(σ) −
i } have opposite sign with high probability. This
1
n
has important practical implications for estimation purposes. In eﬀect, it means that we can
(per Equation (17)) using a binary search
eﬃciently compute (cid:98)σ, by solving G(σ) = 1
i=1 T 2
i
approach between the two identiﬁed endpoints. We would like to mention that while using the
intermediate value theorem sounds like an easy task, it turns out that it is extremely hard to
verify that G(0) ≤ 1
, for which the bulk of the proof of Theorem 7 is dedicated to.
n
(cid:98)σ uniquely, it is
(cid:98)σ estimates σ. This is summarized in Theorem 8.
(cid:98)σ is close to σ). Under the assumptions of Theorem 7, we have that |σ − ˆσ| (cid:46)
Theorem 8 (
(δrn,2(µn, µ1, σ))1/2 + γn−1/2 with probability at least 1 − δ−1 − 2γ−2, where δ−1, γ−2 ∈ (0, 1).

Although Theorem 7 gives us a high probability bound on estimating

important to next understand how eﬃciently

i=1 T 2
i

i=1 T 2

(cid:80)n

(cid:80)n

n

(cid:80)n

From Theorem 8 we see that

(cid:98)σ converges to σ roughly at a n−1/3 rate. In both Theo-
rems 7 and 8, we require that there exist constants ψ, Ψ, C > 0 such that ψ ≤ σ ≤ Ψ and
i ≤ C, for each n ∈ N. For transparency, we note that such assumptions are an arte-
1
n
fact of our methodology and ensure that our risk bounds can be tightly controlled using the
second moment matching approach. Given the highly adversarial corruptions and non-convex

i=1 µ2

10

constraints that can arise under ASCI estimation, e.g., in Example 4, these are slightly stronger
assumptions required for classical convex isotonic regression setup. They eﬀectively represent
a trade-oﬀ for the ﬂexibility, and simplicity of using ASCIFIT under these adversarial settings,
whilst still ensuring precise control in the parameter estimation risk bounds.

(cid:98)σ in our post-processing correction for

(cid:98)µasciﬁt := ((cid:98)µ1, . . . , (cid:98)µn)(cid:62). That is, our ﬁnal estimate
for each µi, is given by
(cid:98)µi := f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ). With this explicit form and our tightly
controlled bounds in Theorem 7 and Equation (17) we are ﬁnally able derive the required least
(cid:98)µasciﬁt. This is summarized in Theorem 9. We will shortly discuss this
squares risk rate of
result further in Section 4 when we derive high probability minimax lower bounds.

Theorem 9 (
have that

(cid:98)µasciﬁt is close to µ). Under the assumptions of Theorem 7 and Theorem 8, we

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, ˆσ), ˆσ) − µi)2 (cid:46) δrn,2(µn, µ1, σ) + γ2n−1,

(22)

with probability at least 1 − δ−1 − 2γ−2.

Remark 6. We note that η is currently absorbed in our constants in Theorems 7 to 9. The
exact form is complicated (but the smaller the η the bigger the constants). For more details,
please refer to Appendix D.

4 Lower bounds

(cid:12) µ1 ≤ . . . ≤ µn

We now derive high probability minimax lower bounds under the ASCI setting. We accord-
ingly ﬁrst introduce the relevant related notation and deﬁnitions here for classes of mono-
tonic sequences. We denote S ↑ := (cid:8)µ := (µ1, . . . , µn)(cid:62) (cid:12)
(cid:9) to be the set of
all non-decreasing sequences. We deﬁne k(µ) ≥ 1, for µ ∈ S ↑, to be the integer such
that k(µ) − 1 is the number of inequalities µi ≤ µi+1 that are strict for i ∈ [n − 1] (i.e.,
the number of ‘jumps’ of µ). The class of bounded monotone functions are S ↑(V ∗) :=
(cid:8)µ ∈ S ↑ (cid:12)
(cid:12) V (µ) ≤ V ∗(cid:9), for some ﬁxed V ∗ ≥ 0, and V (µ) := µn − µ1, is the total vari-
ation of any µ ∈ S ↑. We focus on the ASCI-restricted class of monotone sequences, i.e.,
S ↑(V ∗, η, C) := (cid:8)µ ∈ S ↑(V ∗) (cid:12)
(cid:12) 1
n

i ≤ C, µ1 > η > 0(cid:9).
We closely follow the approach of Bellec and Tsybakov [2015, Proposition 4] but non-
trivially adapt it to our ASCI setting by ensuring the monotonicity constraint in Equation (5)
is satisﬁed in the lower bound construction. The proof uses well established techniques includ-
ing the Varshamov-Gilbert bound Tsybakov [2009, Lemma 2.9], and Fano’s Lemma arguments
using Tsybakov [2009, Theorem 2.7]. This leads to our minimax lower bound result in Propo-
sition 10.

i=1 µ2

(cid:80)n

Proposition 10 (Minimax lower bounds). Let n ≥ 2, V ∗ > 0 and σ > 0, and deﬁne
(cid:101)rn,2(V ∗, σ) := max (cid:8) (cid:16) σ2V ∗

n }. Then, there exist absolute constants c, c(cid:48) > 0 such that:

3 , σ2

(cid:17) 2

n

inf
ˆµ

sup
S↑(V ∗, η, C)

Prµ

(cid:18) 1
n

(cid:19)
(cid:107) (cid:98)µ − µ(cid:107)2 ≥ c(cid:101)rn,2(V ∗, σ)

> c(cid:48)

(23)

Crucially, Proposition 10 demonstrates that our high probability upper bounds for ASCIFIT
in Theorem 9 are sharp in the minimax sense, up to constants and log factors. This is evident
(cid:101)rn,2(V ∗, σ) to rn,2(µn, µ1, σ) per Equation (21).
by directly comparing

11

5 Simulations

We now demonstrate our ASCIFIT estimation algorithm in action through a variety of simu-
lations2. Speciﬁcally, for simulation purposes we consider n observations, {Ri | i ∈ [n]}, where
each observation Ri is generated from Example 3. For suﬃciently large n, the ASCI model
in Example 3 roughly translates to (1 − p)-proportion of observations being independently
sign-corrupted by the adversary. Moreover we assume per Equation (11) that the adversarial
sign-corruptions, ξi, are chosen independently of all true errors, εi, for each i ∈ [n]. The
true monotone signal is deﬁned to be µi := η + (1 − η) i−1
, for each i ∈ [n]. We run this
n
generating process over the following parameter grid: η := 0.2, p := 0.5, σ ∈ {0.5, 1, 1.5, 2},
n ∈ {100, 250, 500, 1000}. We perform 50 replications for each combination of simulation grid
(cid:98)µasciﬁt,
parameters. In each replication of this generating process we ﬁt the ASCIFIT estimator
for µ. The main summary result from running our simulation, is shown in Figure 1.

Figure 1: Mean (sample) MSE of ASCIFIT as a function of n, σ.

To clarify, given η = 0.2, p = 0.5, Figure 1 plots the sample mean-MSE, 1

n (cid:107) (cid:98)µasciﬁt − µ(cid:107)2,
over 50 ASCIFIT replications for each value of n ∈ {100, 250, 500, 1000}. Here the sample
mean-MSE is a useful simulation proxy for the least squares error, our core theoretical risk
measure of interest. This sample mean-MSE is plotted separately for each of the four sigma
values, σ ∈ {0.5, 1, 1.5, 2}. The mean-MSE value of each replication (± 2 standard errors) are
shown using error bars in an eﬀort to quantify replication uncertainty. The plot in Figure 1
is as expected in that all of the sample mean-MSE values show a steady decreasing trend in
n. Importantly the relative uncertainty in sample mean-MSE reduces in n, as seen by the
smaller error bars to the right of Figure 1. For smaller σ values, i.e., smaller variance in the
underlying generating model, we see a much lower sample mean-MSE on average compared
to higher σ-valued simulations. That is, our ASCIFIT estimator achieves better accuracy, with
smaller underlying variability in the model, on average when other factors are held constant.
(cid:98)µasciﬁt, actually ﬁts
the true signal µ, it is instructive to plot both directly on the original generating sample data.
This is seen for one instance over our parameter grid of simulations in Figure 2.

Finally, in order to precisely gauge how well the ASCIFIT estimator

Speciﬁcally, for η = 0.2, p = 0.5, n = 1000, σ = 1.5, Figure 2 plots the simulated true
generating process, µ, against the ASCIFIT estimator,
(cid:98)µasciﬁt. Additionally both the original
and sign-corrupted individual observations are plotted to emphasize the diﬃculty of this esti-

2Reproducible code for all ﬁgures in this paper is found at: https://github.com/shamindras/ascifit. All of the
simulation results in this section were run on a personal Macbook laptop with macOS, Intel Core i9 CPU, and
64GB RAM. The total runtime for a single run of all simulations is approximately 90 minutes.

12

Figure 2: Mean (sample) MSE of ASCIFIT as a function of n, σ.

(cid:98)µnaive.
mation task. Moreover, for comparison purposes we also plot the naive estimator, i.e.,
Here
(cid:98)µnaive represents the estimator by stopping at Step I in ASCIFIT. That is, estimating µ,
by simply ﬁtting isotonic regression (using the PAVA) on Ti := |Ri|. Furthermore, since p = 0.5,
as expected, on average roughly half of the true responses are adversarially sign-corrupted.
Despite this, one can see that ASCIFIT is relatively stable and reasonably recovers the true
signal. This shows more directly (in such an instance), the robustness of ASCIFIT under such
randomized adversarial sign-corruptions. Moreover since n = 1000, we can see that ASCIFIT
indeed ﬁts well with increasing sample complexity. In addition it highlights the importance of
Step II and Step III in ASCIFIT.

6 Conclusion

In this paper we have considered a variation of the original isotonic regression problem in
which the observations can be adversarially corrupted in their sign value. In this ASCI setting,
adversarially refers to the fact that the sign-corruptions can be chosen to have strong depen-
dence with the error terms in the original model. Our simple three-step estimation procedure,
ASCIFIT, is easy to implement with existing software and has sharp non-asymptotic minimax
guarantees on the estimation error, under square loss. For future directions we note that
that true signal is required to be strictly positive for our guarantees to hold. We believe this
restriction can be lifted if one uses unimodal regression instead of isotonic regression in Step
I. However, sharp risk guarantees need to ﬁrst be proven similar to Zhang [2002] under this
unimodal setting. It would also be interesting to see if the moment matching technique could
be extended subgaussian error terms. We leave these exciting directions for future work.

7 Acknowledgments

We would like to thank Arun Kumar Kuchibhotla, Alex Reinhart, Alessandro Rinaldo, Larry
Wasserman from the Carnegie Mellon University (CMU) Department of Statistics & Data
Science, and Yang Ning from the Cornell Department of Statistics & Data Science. Their
encouragement, and extensive feedback throughout this work greatly shaped the ﬁnal outcome.
This paper extensively utilizes the R statistical software [R Core Team, 2021] for conducting
simulations and plots. In particular, we relied primarily on the tidyverse [Wickham et al.,
2019] collection of R packages.

13

References

Ayer, M., Brunk, H. D., Ewing, G. M., Reid, W. T., and Silverman, E. (1955). An empirical
distribution function for sampling with incomplete information . Ann. Math. Statist., 26:641–
647.

Balakrishnan, S., Wainwright, M. J., and Yu, B. (2017). Statistical guarantees for the EM

algorithm: from population to sample-based analysis . Ann. Statist., 45(1):77–120.

Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972). Statistical
inference under order restrictions. The theory and application of isotonic regression . Wiley
Series in Probability and Mathematical Statistics. John Wiley & Sons, London-New York-
Sydney.

Bartholomew, D. J. (1959a). A test of homogeneity for ordered alternatives. Biometrika,

46(1-2):36–48.

Bartholomew, D. J. (1959b). A test of homogeneity for ordered alternatives. II. Biometrika,

46:328–335.

Bellec, P. C. (2018). Sharp oracle inequalities for least squares estimators in shape restricted

regression . Ann. Statist., 46(2):745–780.

Bellec, P. C. and Tsybakov, A. B. (2015).

Sharp oracle bounds for monotone and convex

regression through aggregation . J. Mach. Learn. Res., 16:1879–1892.

Best, M. J. and Chakravarti, N. (1990). Active set algorithms for isotonic regression; a unifying

framework. Math. Programming, 47(3, (Ser. A)):425–439.

Birgé, L. and Massart, P. (1993). Rates of convergence for minimum contrast estimators.

Probab. Theory Related Fields, 97(1-2):113–150.

Brunk, H. D. (1955). Maximum likelihood estimates of monotone parameters. Ann. Math.

Statist., 26:607–616.

Chatterjee, S. (2014). A new perspective on least squares under convex constraint. Ann.

Statist., 42(6):2340–2381.

Chatterjee, S., Guntuboyina, A., and Sen, B. (2015). On risk bounds in isotonic and other

shape restricted regression problems . Ann. Statist., 43(4):1774–1800.

de Leeuw, J., Hornik, K., and Mair, P. (2009).

Isotone Optimization in R: Pool-Adjacent-
Violators Algorithm (PAVA) and Active Set Methods . Journal of Statistical Software,
32(5):1–24.

Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete

data via the EM algorithm. J. Roy. Statist. Soc. Ser. B, 39(1):1–38. With discussion.

Donoho, D. L. (1990). Gelfand n-widths and the method of least squares. Preprint.

Elandt, R. C. (1961). The folded normal distribution: two methods of estimating parameters

from moments . Technometrics, 3:551–562.

14

Giraud, C. and Verzelen, N. (2018). Partial recovery bounds for clustering with the relaxed

K-means. Math. Stat. Learn., 1(3-4):317–374.

Grotzinger, S. J. and Witzgall, C. (1984). Projections onto order simplexes. Applied Mathe-

matics & Optimization, 12(1):247–270.

Guntuboyina, A. and Sen, B. (2018). Nonparametric shape-restricted regression. Statist. Sci.,

33(4):568–594.

Meyer, M. and Woodroofe, M. (2000). On the degrees of freedom in shape-restricted regression.

Ann. Statist., 28(4):1083–1104.

Németh, A. B. and Németh, S. Z. (2012). How to project onto the monotone nonnegative

cone using Pool Adjacent Violators type algorithms .

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,
Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine Learning in
Python. Journal of Machine Learning Research, 12:2825–2830.

R Core Team (2021). R: A Language and Environment for Statistical Computing. R Foundation

for Statistical Computing, Vienna, Austria.

Robertson, T., Wright, F. T., and Dykstra, R. L. (1988). Order restricted statistical inference.
Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical
Statistics. John Wiley & Sons, Ltd., Chichester.

Royer, M. (2017). Adaptive Clustering through Semideﬁnite Programming. pages 1795–1803.

Tibshirani, R. J., Hoeﬂing, H., and Tibshirani, R. (2011). Nearly-isotonic regression. Techno-

metrics, 53(1):54–61.

Tsagris, M., Beneki, C., and Hassani, H. (2014). On the Folded Normal Distribution. Mathe-

matics, 2(1):12–28.

Tsybakov, A. B. (2009). Introduction to nonparametric estimation. Springer Series in Statistics.
Springer, New York. Revised and extended from the 2004 French original, Translated by
Vladimir Zaiats.

van de Geer, S. (1990). Estimating a regression function. Ann. Statist., 18(2):907–924.

van de Geer, S. (1993). Hellinger-consistency of certain nonparametric maximum likelihood

estimators . Ann. Statist., 21(1):14–44.

van Eeden, C. (1956). Maximum likelihood estimation of partially or completely ordered

parameters . Statist. Afdeling Rep. S 207 (VP 9). Math. Centrum Amsterdam.

van Eeden, C. (1957a). A least squares inequality for maximum likelihood estimates of ordered

parameters . Nederl. Akad. Wetensch. Proc. Ser. A 60 = Indag. Math., 19:513–521.

van Eeden, C. (1957b). Maximum likelihood estimation of partially or completely ordered
parameters. I . Nederl. Akad. Wetensch. Proc. Ser. A. 60 = Indag. Math., 19:128–136.

15

van Eeden, C. (1957c). Maximum likelihood estimation of partially or completely ordered
parameters. II . Nederl. Akad. Wetensch. Proc. Ser. A. 60 = Indag. Math., 19:201–211.

van Eeden, C. (1958). Testing and estimating ordered parameters of probability distributions

. Mathematical Centre, Amsterdam.

Wang, Y. (1996). The L2 risk of an isotonic estimate. Comm. Statist. Theory Methods,

25(2):281–294.

Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund,
G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M.,
Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D.,
Wilke, C., Woo, K., and Yutani, H. (2019). Welcome to the tidyverse. Journal of Open
Source Software, 4(43):1686.

Zhang, C.-H. (2002). Risk bounds in isotonic regression. Ann. Statist., 30(2):528–555.

16

Appendix

Table of Contents

A Mathematical Preliminaries

A.1 Notation Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Useful miscellaneous results . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 The Folded Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . .

A.4 Properties of the folded normal mean: f (µ, σ) . . . . . . . . . . . . . . . .

A.5 Properties of the folded normal variance: g(µ, σ) . . . . . . . . . . . . . . .
A.6 Properties of the inverse folded normal mean: f −1(µ, σ) . . . . . . . . . .

A.7 Properties of: J(σ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.8 Properties of: G(σ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B Proofs of Section 1

B.1 Mathematical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2 Important Model Deﬁnitions

. . . . . . . . . . . . . . . . . . . . . . . . . .

B.3 Proof justiﬁcation for Example 3 . . . . . . . . . . . . . . . . . . . . . . . .

B.4 Proof justiﬁcation for Example 4 . . . . . . . . . . . . . . . . . . . . . . . .

C Proofs of Section 2

C.1 Mathematical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . .

C.2 Proof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Proofs of Section 3

D.1 Mathematical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.2 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.3 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.4 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Proofs of Section 4

E.1 Mathematical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.2 Proof of Proposition 40 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.3 Proof of Proposition 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

18

18

22

22

24

26

27

29

31

31

31

32

33

34

34

34

36

36

40

45

48

50

50

50

52

A Mathematical Preliminaries

In this appendix we provide detailed proofs of all key statements from the main paper. Since
our work relies a variety of core ideas from isotonic regression we ﬁrst introduce some common
deﬁnitions which will be referred to in subsequent proofs.

17

A.1 Notation Summary

To ensure that the Appendix is can be read in a standalone manner, we consolidate key
notation used in the paper in Table 1. Unless stated otherwise K ⊆ Rd is a closed, non-empty
convex set, and Ω ⊆ Rd.

Table 1: Notation and conventions used in our paper

Variables and inequalities

a ∧ b min {a, b} for each a, b ∈ R
a ∨ b max {a, b} for each a, b ∈ R

scalars x, y, z ∈ R
vectors x, y, z ∈ Rd

matrices X, Y, Z ∈ Rd×m

(cid:46) ≤ up to positive universal constants
(cid:38) ≥ up to positive universal constants

an = O (1)
an = O (bn)
an = o (1)
an = o (bn)
Xn = oP (1)
Xn = OP (1)

= O (1)

(∃C > 0)(∃N ∈ N)(∀n ≥ N )(|an| < C)
an
bn
(∀C > 0)(∃N ∈ N)(∀n ≥ N )(|an| < C)
an
bn
(∀ε > 0)(Pr (|Xn| ≥ ε) n→∞−−−→ 0)
(∀ε > 0)(∃C > 0)(∃N ∈ N)(∀n ≥ N )(Pr (|Xn| ≥ C) ≤ ε)

= o (1)

Functions and sets

[n]

{1, . . . , n}, for n ∈ N

Indicator function IΩ(x) Takes value 1 when x ∈ Ω, and 0 otherwise
ΠK : Rd → K (cid:96)2-projection of any x ∈ Rd onto K

f : Ω → R is increasing
f : Ω → R is decreasing

If ∀u, v ∈ Ω such that u ≤ v =⇒ f (u) ≤ f (v)
If ∀u, v ∈ Ω such that u ≤ v =⇒ f (u) ≥ f (v)

Φ : R → [0, 1] Cumulative density function of N (0, 1)
φ : R → R Probability density function of N (0, 1)

(cid:12) µ1 ≤ . . . ≤ µn

(cid:9)

S ↑ (cid:8)µ := (µ1, . . . , µn)(cid:62) (cid:12)
S ↑
+

(cid:8)µ ∈ S ↑ (cid:12)
S ↑(V ∗) (cid:8)µ ∈ S ↑ (cid:12)

(cid:12) µ1 ≥ 0(cid:9)
(cid:12) V (µ) ≤ V ∗(cid:9)

V (µ) µn − µ1 for µ ∈ S ↑
(cid:8)µ ∈ S ↑ (cid:12)

(cid:12) k(µ) ≤ k∗(cid:9)
S ↑(V ∗, η, C) (cid:8)µ ∈ S ↑(V ∗) (cid:12)
(cid:80)n
(cid:12) 1
n

S ↑
k∗

i=1 µ2

i ≤ C, µ1 > η > 0(cid:9)

A.2 Useful miscellaneous results

Here we prove some useful standard results that are used in several of the remaining proofs.
For reader convenience, we provide short proofs to ensure that our work is self-contained.

We start with some elementary inequalities, which will be used repeatedly. First, in
Lemma 11 we introduce a diﬀerencing inequality we use repeatedly to construct lower bounds.

18

Lemma 11 (Diﬀerence of squares lower bound). For each a, b, l ∈ R, such that b, l ≥ 0 and
a − b ≥ l, the following holds:

a2 − b2 ≥ la ≥ l2

(24)

Proof of Lemma 11. We proceed as follows. First note that since b, l ≥ 0 by assumption, we
have that a − b ≥ l ⇐⇒ a ≥ b + l ≥ 0. Now observe:

a2 − b2 = (a + b)(a − b)
≥ l(a + b)

≥ la
≥ l2

(since a − b ≥ l ≥ 0 by assumption.)
(since b ≥ 0 by assumption.)
(since a ≥ l)

as required.

Lemma 12 (Lower bound via diﬀerence of squares). For each a, b, C, K ∈ R, such that
b ≥ 0, a2 − b2 ≥ C > 0, a ∈ [0, K], the following holds:

a − b ≥

C
2K

(25)

Proof of Lemma 12. We proceed as follows. First note that since a2 − b2 ≥ C > 0 by as-
sumption, we have that a > 0, and hence a > b, K > 0 since both a, b are non-negative. Now
observe:

a2 − b2 ≥ C

(by assumption.)

=⇒ a − b ≥

≥

≥

C
a + b
C
2a
C
2K

(since a > 0, b ≥ 0 =⇒ a + b > 0.)

(since a ≥ b.)

(since a ≤ K.)

as required.

Lemma 13 (Maximum diﬀerence square inequality). For each a, b, c ∈ R such that b ≤ c the
following inequality holds:

((a ∨ b) − c)2 ≤ (a − c)2

(26)

Proof of Lemma 13. Under the assumption that a, b, c ∈ R such that b ≤ c, let d := a ∨ b. We
then observe:

(d − c)2 ≤ (a − c)2

⇐⇒ d2 − a2 ≤ 2dc − 2ac
⇐⇒ (d + a)(d − a) ≤ 2c(d − a)

(expanding and simplifying.)

(27)

So we need to equivalently prove that Equation (27). To that end we only need to consider 2
cases. Namely a ≥ b, and a < b. Note that in the ﬁrst case a ≥ b =⇒ d := a ∨ b = a. In this

19

case, both LHS/RHS of Equation (27) are 0, and the statement holds. Next consider the case
a < b. Here we have a < b =⇒ d := a ∨ b = b > a. We then observe the following:

a + d = a + b

≤ 2b

≤ 2c

(since d = b.)
(since a < b by assumption.)
(since b ≤ c by assumption.)

That is, we have that a + d ≤ 2c. Substituting back to Equation (27) we have that
(d + a)(d − a) ≤ 2c(d − a), which is what we wanted to show. Which completes the proof
Equation (26), as required.

Lemma 14 (Square sum inequality). For each a, b ∈ R the following holds:

(a + b)2 ≤ 2(a2 + b2)

Proof of Lemma 14. We proceed as follows:

(a + b)2 = a2 + 2ab + b2

(28)

(29)

≤ a2 + b2 + 2 |ab|
≤ a2 + b2 + 2(|a|2 + |b|2)
= 2(a2 + b2)

(since x ≤ |x| for each x ∈ R)
(by AM-GM we have 2 |ab| ≤ |a|2 + |b|2)
(30)

as required.

As a result of Lemma 14 we obtain Corollary 15.

Corollary 15. For random variables X1, X2 the following holds:

Var (X1 − X2) ≤ 2(Var (X1) + Var (X1))

(31)

Proof of Corollary 15. First let the centered versions of the random variables be denoted as

˜Xi := Xi − E (Xi) ,

for each i ∈ [2].

It then follows that:

(cid:17)

= Var
(cid:16)

Var (X1 − X2) = Var (X1 − X2 + E (X2) − E (X1))
(cid:16) ˜X1 − ˜X2
( ˜X1 − ˜X2)2(cid:17)
(cid:17)
1 + ˜X 2
2( ˜X 2
2 )
(cid:16) ˜X 2
(cid:17)

(cid:16) ˜X 2

= E

= E

+ E

= 2

(cid:17)(cid:17)

E

(cid:16)

(cid:16)

1

2

(since ˜X1, ˜X2 are both centered.)

(using Lemma 14)

(linearity of expectation.)

= 2(Var (X1) + Var (X2))

(since ˜X1, ˜X2 are both centered.)

as required.

The following is a standard result from real analysis, which we use repeatedly.

20

(32)

(33)

(34)

Lemma 16 (B-Lipschitz characterization via bounded derivative). Let f : I → R be contin-
uous and once diﬀerentiable, where I ⊆ R is an interval (possibly unbounded).

f is B-Lipschitz, with B > 0 ⇐⇒ (∃B > 0)(∀x ∈ R) : ((cid:12)

(cid:12)f (cid:48)(x)(cid:12)

(cid:12) ≤ B)

(35)

Proof of Lemma 16. We prove both directions. In both parts we assume that f : I → R be
continuous and once diﬀerentiable, where I ⊆ R is an interval (possibly unbounded).
( =⇒ ). Suppose that f is B-Lipschitz, with B > 0. We then have that, for some ﬁxed (but
arbitrary) c ∈ I:

=⇒

|f (x) − f (c)| ≤ B |x − c|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)f (cid:48)(c)(cid:12)
(cid:12) ≤ B

f (x) − f (c)
x − c
=⇒ (cid:12)

≤ B

(by deﬁnition of B-Lipschitz property.)

(taking limits as x → c.)

Since c ∈ I is arbitrary, indeed |f (cid:48)(x)| ≤ B, for each x ∈ I, as required.

( ⇐= ). Suppose that |f (cid:48)(x)| ≤ B, with B > 0. Further let x, y ∈ I, such that x < y.
Since f is diﬀerentiable on I, we have:

|f (x) − f (y)| ≤ (cid:12)

(cid:12)f (cid:48)(c)(cid:12)
≤ B |x − y|

(cid:12) |x − y|

(by the mean value theorem, for some c ∈ (x, y).)
(by assumption.)

Which implies that f is B-Lipschitz, as required.

Lemma 17 (Standard normal upper bound). Let φ(x), Φ(x) respectively denote the probability
density function, and cumulative density function of a standard normal variable. Then the
following inequality holds:

xφ(x)
2Φ(x) − 1

≤

1
2

, for each x ≥ 0

(36)

With equality if and only if x = 0.

Proof of Lemma 17. We ﬁrst note that at x = 0, that
type 0
0

. As such we have:

xφ(x)
2Φ(x)−1

is an indeterminate form of

lim
x→0

xφ(x)
2Φ(x) − 1

=

=

=

=

=

∂
limx→0
∂x xφ(x)
∂
limx→0
∂x 2Φ(x) − 1
limx→0 φ(x) + xφ(cid:48)(x)
limx→0 2φ(x)

limx→0 φ(x)
limx→0 2φ(x)
φ(0)
2φ(0)
1
2

21

(using L’Hospital’s rule.)

(by continuity of φ(x) at x = 0.)

(37)

With our given function now deﬁned to be 1
2
inequality. Observe that we can equivalently reformulate it as:

at x = 0, we now proceed to prove our given

Φ(x) − xφ(x) −

1
2

≥ 0

(38)

Setting h(x) := Φ(x) − xφ(x) − 1
2 = 0. We need to show that
2
h(x) ≥ 0, for each x ≥ 0, which will imply the result. We will show that h(x) is increasing,
i.e., or equivalently that h(cid:48)(x) ≥ 0, for each x ≥ 0. We then have that:

, we observe that h(0) = Φ(0) − 1

h(cid:48)(x) = φ(x) − (φ(x) + xφ(cid:48)(x))

= −xφ(cid:48)(x)

(cid:18)

= −x

−x

= x2φ(x)
≥ 0

2 x2(cid:19)

e− 1

1
√
2π

(using φ(x) := 1√
2π

e− 1

2 x2.)

(39)

Note that the inequality in Equation (39) is strict when x > 0 and equality holds if and only
if x = 0. This means the function is strictly increasing and bounded away from 0 when for
each x > 0, and equal to 0 only when x = 0, as required.

A.3 The Folded Normal Distribution

For convenience, we begin by quickly recalling the deﬁnition of the Folded Normal distribution.

Deﬁnition 5 (Folded Normal distribution). Suppose R ∼ N (µ, σ2), and let T := |R|. We
then say that T ∼ FoldNorm(µ, σ), is a Folded Normal distribution. We denote the mean and
variance of T , by f (µ, σ) and g(µ, σ), respectively. They are given as follows:

f (µ, σ) := E (T ) = σ(cid:112)2/π exp(−µ2/(2σ2)) − µ(1 − 2Φ(µ/σ)).

g(µ, σ) := Var (T ) = µ2 + σ2 − f (µ, σ)2.

Remark 7. We note that Equation (15) can be equivalently written as follows:

σ(cid:112)2/π exp(−µ2/(2σ2)) + µ(1 − 2Φ(−µ/σ))

(15)

(16)

(40)

Note that this equivalence follows from the symmetry of the standard normal CDF, i.e.,
Φ(x) = 1−Φ(−x) for each x ∈ R. For our purposes we typically use the form of Equation (15).

A.4 Properties of the folded normal mean: f (µ, σ)

Let’s start setting up some notation. First we note as previously Ti
:= |Ri| = |µi + εi|.
Where we then have Ti ∼ FoldNorm (cid:0)µi, σ2(cid:1). Now denote f (µi, σ) := E (Ti), for each i ∈ [n].
Moreover the Ti random variables are all mutually independent, but not identically distributed
(since their mean’s, i.e., f (µi, σ) diﬀer for each i ∈ [n]). Since we run PAVA on (T1, . . . , Tn)
we have the resulting estimators ( (cid:98)T1, . . . , (cid:98)Tn). We will also denote the population level error
terms for this transformed (mean centered) response as δi := Ti − f (µi, σ). We note that the
(δ1, . . . , δn) are all mutually independent, but not identically distributed.

22

Lemma 18 (Properties of the Folded Normal mean). Suppose R ∼ N (µ, σ2). Let T a.s.= |R|,
then T ∼ FoldNorm (cid:0)µ, σ2(cid:1) per Deﬁnition 5. We denote the mean of the Folded Normal
distribution by f (µ, σ) := E (T ). Given this setup, and ﬁxing σ > 0, we note the following
important properties of f (µ, σ):

f (µ, σ) ≥ 0 for each µ ∈ R
f (µ, σ) ≥ µ for each µ ∈ R
f (µ, σ) is strictly increasing in µ ∈ R>0

∂f (µ, σ)
∂µ

∈ (0, 1) is for each µ ∈ R>0

f (µ, σ) is 1-Lipschitz for each µ ∈ R>0
f (µ, σ)2 ≤ µ2 + σ2 for each µ ∈ R≥0

(41)

(42)

(43)

(44)

(45)

(46)

Additionally for µ1 ≤ . . . ≤ µn we have that the relationship holds for V (f, µ, σ), i.e., the total
variation of the mean of the Folded Normal distribution:

V (f, µ, σ) :=

n−1
(cid:88)

i=1

|f (µi+1, σ) − f (µi, σ)| ≤ µn − µ1

(47)

Proof of Lemma 18. We prove each property (Equations (41) to (47)) in turn. As per the
assumption σ > 0 is ﬁxed and that R ∼ N (cid:0)µ, σ2(cid:1) for µ ∈ R.
(cid:4)

(Proof of Equation (41).) We have that T := |R| ≥ 0 a.s. =⇒ f (µ, σ) := E (T ) = E (|R|) ≥ 0
(cid:4)
by the monotonicity of expectation, as required.

(Proof of Equation (42).) We have that R ≤ |R| a.s. =⇒ µ := E (R) ≤ E (|R|) = E (T ) =:
(cid:4)
f (µ, σ) again by the monotonicity of expectation, as required.

(Proof of Equation (43).) For any µ > 0 we have that:

f (µ, σ) := σ

=⇒

∂f (µ, σ)
∂µ

= −

(cid:114) 2
π
(cid:114) 2
π

µ
σ

exp

(cid:18)

−

(cid:18)

(cid:19)

µ2
2σ2
µ2
2σ2

−

exp

(cid:16)

− µ

1 − 2Φ

(cid:17)(cid:17)

(cid:16) µ
σ

(per Equation (15))

(cid:19)

− 1 + 2Φ

(cid:17)

(cid:16) µ
σ

+

(cid:17)

2µ
σ

φ

(cid:16) µ
σ

(cid:17)

(cid:16) µ
σ

− 1

= 2Φ

> 0

(cid:113) 2

(cid:16)

(cid:17)

π exp

= 2µ
(since µ
σ
(since µ, σ > 0 and Φ (cid:0) µ
σ

− µ2
2σ2

σ φ (cid:0) µ
σ
(cid:1) > 1
2

(cid:1))

)

as required.

(cid:4)

(Proof of Equation (44).) By the previous proof, we note that ∂f (µ,σ)
vious proof and noting that Φ(x) > 1
2
Combining both parts we have that ∂f (µ,σ)

for each x > 0, it follows that ∂f (µ,σ)
∂µ ∈ (0, 1), as required.

∂µ > 0. Also using the pre-
(cid:1) < 1.
∂µ = −1+2Φ (cid:0) µ
(cid:4)

σ

(Proof of Equation (45).) By the previous proof, we note that ∂f (µ,σ)

∂µ ∈ (0, 1) =⇒

(cid:12)
(cid:12)
(cid:12)

∂f (µ,σ)
∂µ

(cid:12)
(cid:12)
(cid:12) ≤

23

1 for each µ > 0. It follows by the mean value theorem, that f (µ, σ) is 1-Lipschitz as required.
(cid:4)

(Proof of Equation (46).) Observe that from Equation (16) we have that g(µ, σ) := Var (T ) =
µ2 + σ2 − f (µ, σ)2. Since Var (T ) ≥ 0, it follows that f (µ, σ)2 ≤ µ2 + σ2 for each µ ∈ R, as
(cid:4)
required.

(Proof of Equation (47).) Let i ∈ [n] be arbitrary. Now note that by the Equation (45)
property it follows that , we then have that:

V (f, µ, σ) :=

n−1
(cid:88)

|f (µi+1, σ) − f (µi, σ)|

i=1
n−1
(cid:88)

f (µi+1, σ) − f (µi, σ)

=

(by deﬁnition)

i=1

(using Equation (43) and monotonicity of µ1 ≤ . . . ≤ µn)
(by telescoping sum)

= f (µn, σ) − f (µ1, σ)
≤ |µn − µ1|
= µn − µ1

(using Equation (45))
(by monotonicity of µ1 ≤ . . . ≤ µn)

(cid:4)

as required.

Thus all properties speciﬁed in Equations (41) to (47) are now proved.

A.5 Properties of the folded normal variance: g(µ, σ)

Lemma 19 (Properties of the Folded Normal variance). Let T ∼ FoldNorm (cid:0)µ, σ2(cid:1) per Deﬁ-
nition 5, and let g(µ, σ) := Var (T ). Given this setup, and ﬁxing σ > 0, we note the following
properties of g(µ, σ):

g(µ, σ) ≤ σ2, for each µ ∈ R
g(µ, σ) ≥ g(0, σ), for each µ ∈ R>0
Var (cid:0)T 2(cid:1) = 4µ2σ2 + 2σ4, for each µ ∈ R

(48)

(49)

(50)

Proof of Lemma 19. We prove each properties speciﬁed in Equations (48) to (50) in turn.

(Proof of Equation (48).) We have for each µ ≥ 0

g(µ, σ) := µ2 + σ2 − f (µ, σ)2

(per Equation (16))

≤ σ2

(since f (µ, σ)2 ≥ µ2 using Equation (42))

as required.

(cid:4)

(Proof of Equation (49).) First note that g(0, σ) = σ2 − f (0, σ)2 = σ2 − (cid:0) 2
π

(cid:1) σ2.

It then

24

follows that:

g(µ, σ) ≥ g(0, σ)

⇐⇒ µ2 + σ2 − f (µ, σ)2 ≥ σ2 −

(cid:19)

(cid:18) 2
π

σ2

⇐⇒ µ2 +

(cid:19)

(cid:18) 2
π

σ2 ≥ f (µ, σ)2

(51)

We will then prove the equivalent statement Equation (51). Since µ, σ > 0 in our case, let
ν := µ
σ > 0 in what follows. Then dividing both sides of Equation (51) by ν we obtain:

Let us then deﬁne

(cid:114)

ν2 +

2
π

≥ ν(2Φ(ν) − 1) +

(cid:114) 2
π

e− ν2

2

(cid:114)

g(ν) :=

ν2 +

2
π

− ν(2Φ(ν) − 1) −

(cid:114) 2
π

e− ν2

2

Taking the derivative of g(ν) with respect to ν we obtain:

g(cid:48)(ν) =

(cid:113)

=

(cid:113)

ν

ν2 + 2
π
ν

ν2 + 2
π

− 2Φ(ν) + 1 −

√

2ν
2π

e− ν2

2 + ν

(cid:114) 2
π

e− ν2

2

− 2Φ(ν) + 1

As such all global and local extrema are obtained by setting g(cid:48)(ν) = 0, that is:

⇐⇒

= 2Φ(ν) − 1

g(cid:48)(ν) = 0
ν

(cid:113)

ν2 + 2
π

(52)

(53)

(54)

(55)

(56)

Now, ν = 0 is a clear solution, at which our function is exactly equal to 0. Also, we need to
look at ν = ∞, where we also have an identity. So we need to take care of other possible roots
= 2Φ(ν) − 1. Now observe that since when ν ≥ 0 the function Φ(ν) is
to the equation

(cid:113)

ν
ν2+ 2
π

concave and therefore 2Φ(ν) − 1 = 2(Φ(ν) − Φ(0)) ≤ 2νφ(0) =

solution ¯ν to the equation

This implies that ¯ν2 ≥ π

2 − 2

π

(cid:113)

= 2Φ(ν) − 1, we must have

ν
ν2+ 2
π
. Now, going back to the original function we need to show

¯ν
¯ν2+ 2
π

≤ ¯ν

(cid:113)

(cid:114)

¯ν2 +

2
π

≥ ¯ν(2Φ(¯ν) − 1) +

(cid:114) 2
π

e−¯ν2/2 =

¯ν2

(cid:113)

¯ν2 + 2
π

(cid:114) 2
π

+

e−¯ν2/2.

The latter is equivalent to ¯ν2 + 2
. since the function ν (cid:55)→ 2
¯ν2 + 2
π

π e¯ν2 ≥
π ≥ ¯ν2 +
π eν2 − ν2 is increasing for positive ν it suﬃces to check that

which is equivalent to 2

¯ν2 + 2
π

(cid:113) 2

π e−¯ν2/2(cid:113)

25

(cid:113) 2

π ν. Thus for any non-zero
.

(cid:113) 2
π

π e¯ν2 ≥ ¯ν2 + 2
2
true, and completes the proof, as required.

(since as we know from before ¯ν is at least that value). This is
(cid:4)

for ¯ν = π

2 − 2

π

π

(Proof of Equation (50).) By direct calculation we have:

Var (cid:0)T 2(cid:1) = E (cid:0)T 4(cid:1) − (cid:0)E (cid:0)T 2(cid:1)(cid:1)2
= E (cid:0)R4(cid:1) − (cid:0)E (cid:0)R2(cid:1)(cid:1)2
= (µ4 + 6µ2σ2 + 3σ4) − (µ2 + σ2)2
= 4µ2σ2 + 2σ4

(since T a.s.= |R|)
(2nd, 4th moments of N (µ, σ2))

as required.

(cid:4)

Thus all properties speciﬁed in Equations (48) to (50) are now proved.

A.6 Properties of the inverse folded normal mean: f −1(µ, σ)

Lemma 20 (Properties of the Folded Normal mean inverse). Suppose R ∼ N (µ, σ2). Let
T a.s.= |R|, then T ∼ FoldNorm (cid:0)µ, σ2(cid:1) per Deﬁnition 5. We denote the mean of the Folded
Normal distribution by f (µ, σ) := E (T ). Given this setup, and ﬁxing σ > 0, we note the
following important properties of f −1(u, σ) (which denotes the inverse with respect to µ function
of f (µ, σ) when σ is held ﬁxed):

∂
∂σ

f −1(u, σ) = −

f −1(u, σ) exists,
(cid:112)2/π exp(−µ2/(2σ2))
2Φ(µ/σ) − 1

,

∂
∂u

f −1(u, σ) = 1/(2Φ(µ/σ) − 1),

f −1(u, σ) is a Lipschitz function for each u > f (η, σ) > 0 for a ﬁxed σ,

where in the above u = f (µ, σ) (or in other words µ = f −1(u, σ)).

(57)

(58)

(59)

(60)

Proof of Lemma 20. We prove each properties speciﬁed in Equations (57) to (60) in turn.

(Proof of Equation (57).) Note that for a ﬁxed σ > 0 the function f (µ, σ) is invertible
(cid:4)
(as it is increasing, per Lemma 18), as required.

(Proof of Equation (58).) In order to ﬁnd the derivative of ∂
as follows:

∂σ f −1(·, σ), we can parametrize

u = f (µ, σ)

v = σ

(61)

(62)

We will use the inverse function theorem which says that under certain conditions µ =
F (u, v) = F (u, σ) and σ = G(u, v) = v, for some functions F and G. Note that for a
ﬁxed σ > 0 the function f (µ, σ) is invertible (per Equation (57)). Thus

∂
∂σ

f −1(u, σ) =

∂µ
∂σ

=

∂F (u, v)
∂v

= −

∂u
∂σ
J

= −

(cid:112)2/π exp(−µ2/(2σ2))
J

(63)

26

Where J is the Jacobian of the transformation
∂u
∂µ
∂v
∂µ

J =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂u
∂σ
∂v
∂σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 2Φ(µ/σ) − 1

> 0

(since µ ≥ η > 0)

It follows that

As required.

∂
∂σ

f −1(u, σ) = −

(cid:112)2/π exp(−µ2/(2σ2))
2Φ(µ/σ) − 1

(Proof of Equation (59).) We similarly evaluate the derivative ∂

∂u f −1(u, σ) as follows:

∂
∂u

f −1(u, σ) =

=

=

µ

∂
∂u
∂v
∂σ
J

1
2Φ(µ/σ) − 1

As required.

(Proof of Equation (60).) We note that Equation (59) implies that

∂
∂u

f −1(u, σ) ≤

1
2Φ(η/σ) − 1

(64)

(cid:4)

(65)

(66)

(67)

(cid:4)

(68)

since µ ≥ η > 0 under our setting. In this case, this holds for each u > f (η, σ) > 0, for a ﬁxed
σ. Since this derivative is bounded by this constant, it follows that f −1(u, σ) is
-
1
2Φ(η/σ)−1
(cid:4)
Lipschitz by applying Lemma 16.

Thus all properties speciﬁed in Equations (57) to (60) are now proved.

A.7 Properties of: J(σ)

Deﬁnition 21 (J(σ)). Let η > 0 be ﬁxed, and σ ≥ 0 per Equations (4) and (5), respectively.
We deﬁne the function, J : R≥0 → R, as:

J(σ) :=

(cid:40)0
σ

(cid:16) 1
2 − η/σφ(η/σ)

2Φ(η/σ)−1

(cid:17)

if σ = 0
otherwise

(69)

In order to prove the key properties of J(σ), we will ﬁrst need to prove a useful result in
Lemma 22.

Lemma 22. We deﬁne the function, M : R≥0 → R≥0, as:

M (x) :=

(cid:40) 1
2

xφ(x)
2Φ(x)−1

if x = 0
otherwise.

(70)

Note that M (0) = 1

2 , by Equation (37). Then M (x) is strictly decreasing for each x > 0.

27

Proof of Lemma 22. In order to show that M (x) is decreasing for each x > 0, we will show
that M (cid:48)(x) < 0 for each x > 0. To see this, ﬁrst observe that:

(2Φ(x) − 1) (φ(x) + xφ(cid:48)(x)) − 2xφ2(x)
(2Φ(x) − 1)2
(2Φ(x) − 1) (cid:0)φ(x) − x2φ(x)(cid:1) − 2xφ2(x)
(2Φ(x) − 1)2
(cid:0)(2Φ(x) − 1) (cid:0)1 − x2(cid:1) − 2xφ(x)(cid:1)

M (cid:48)(x) =

=

=

=

φ(x)
(2Φ(x) − 1)2
φ(x)
(2Φ(x) − 1)2

(cid:18)

(cid:18)

2

Φ(x) −

(cid:19)

1
2

(cid:0)1 − x2(cid:1) − 2xφ(x)

(cid:19)

.

(71)

(since φ(cid:48)(x) + xφ(x) = 0)

Now we see that:

(cid:18)

2

Φ(x) −

(cid:19)

1
2

= 2 (Φ(x) − Φ(0)) = 2

(cid:90) x

0

1
√
2π

e− t2

2 dt =

2
√
2π

(cid:90) x

0

e− t2

2 dt ≤

√

2x
2π

,

(72)

where the last inequality in Equation (72) followed from the fact that e− t2
It then follows that:

2 ≤ 1 for each t ≥ 0.

(cid:18)

(cid:18)

2

Φ(x) −

(cid:19)

1
2

(cid:0)1 − x2(cid:1) − 2xφ(x)

(cid:19)

=

≤

(cid:18) 2
√
2π

(cid:90) x

0

(cid:19)

e− t2

2 dt

(cid:0)1 − x2(cid:1) −

√

2x
2π

e− x2

2

√

2x
2π

(cid:0)1 − x2(cid:1) −

√

2x
2π

e− x2

2

(using Equation (72))

< 0,

(73)

Where Equation (73) followed by observing that since 1 − x2 < 1 − x2
Now since
that M (cid:48)(x) < 0, for each x > 0, as required.

for each x > 0.
(2Φ(x)−1)2 > 0 for each x > 0, we have by applying Equation (73) to Equation (72)

φ(x)

2 < e− x2

2

Lemma 23 (Properties of J(σ)). Let J(σ) be deﬁned as per Equation (69). Then J(σ) satisﬁes
the following properties:

J(σ) > 0 for each σ ∈ R>0 and 0 if and only if σ = 0
J(σ) is continuous for each σ ∈ R>0

For any 0 < σ1 < σ2, min

σ∈[σ1,σ2]

J(σ) ≥ σ1

(cid:18) 1
2

−

η/σ2φ(η/σ2)
2Φ(η/σ2) − 1

(cid:19)

> 0

(74)

(75)

(76)

Proof of Lemma 23. We prove each property (Equations (74) to (76)) in turn. Throughout
these proofs, we write:

J(σ) := J1(σ)J2(σ), where J1(σ) := σ, and J2(σ) :=

1
2

−

η/σφ(η/σ)
2Φ(η/σ) − 1

(77)

(Proof of Equation (74).) Observe that both J1(σ), J2(σ) are zero if and only if σ = 0. In the
case of J2(σ) this follows from Lemma 17. Now for σ > 0, J1(σ) := σ > 0, by assumption.
And the fact that J2(σ) > 0, for σ > 0 again follows directly from Lemma 17. As such,
J(σ) > 0 for each σ > 0, since it is the product of two strictly positive functions over this

28

support, as required.

(cid:4)

(Proof of Equation (75).) J1(σ) is continuous for σ > 0. Moreover since φ(x), Φ(x) for a
standard normal are continuous over their support, R, it follows that J2(σ) is also continuous
for σ > 0. As such, J(σ) is continuous for each σ > 0, since it is the product of two continuous
(cid:4)
functions, as required.

(Proof of Equation (76).) Note that for any two ﬁxed σ1, σ2, such that 0 < σ1 < σ2,
the interval [σ1, σ2] is compact. From Equation (75), we know that J(σ) is continuous for
σ > 0, and so it attains it’s minimum (and maximum) on this interval. Moreover, from
Equation (74), it follows that minσ∈[σ1,σ2] J(σ) > 0. Now we note that σ (cid:55)→ J1(σ) := σ,
is increasing in σ. Moreover for each σ > 0, we have that J2(σ) := 1
σ ), where
the function M is as deﬁned in Equation (70). Moreover it follows from Lemma 22 that
J2(σ) is strictly decreasing for each σ > 0. By the non-negativity of J(σ) over it’s do-
main, we have that J(σ) > J1(σ1)J2(σ2) for each σ ∈ [σ1, σ2]. From this we have that
(cid:16) 1
2 − η/σ2φ(η/σ2)
(cid:4)
minσ∈[σ1,σ2] J(σ) ≥ J1(σ1)J2(σ2) = σ1
2Φ(η/σ2)−1

> 0, as required.

2 − M ( η

(cid:17)

Thus all properties speciﬁed in Equations (74) to (76) are now proved.

A.8 Properties of: G(σ)

Deﬁnition 24 (G(σ)). Under the setup of ASCI generating process per Deﬁnition 2, and per
the ASCIFIT model we deﬁne the function, G : R≥0 → R, as:

G(σ) := σ2 +

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, σ), σ))2

(78)

Lemma 25 (Properties of G(σ)). Under the setup of ASCI generating process per Deﬁnition 2,
and with G(σ) deﬁned as per Deﬁnition 24, we note the following important properties of G(σ):

∂
∂σ

G(σ) =

(cid:32)

4
n

n
(cid:88)

i=1

σ

1
2

−

f −1( (cid:98)Ti, σ)/σφ(f −1( (cid:98)Ti, σ)/σ)1( (cid:98)Ti ≥ f (η, σ))
2Φ(f −1( (cid:98)Ti, σ)/σ) − 1

(cid:33)

,

G(σ) is increasing for σ ≥ 0, and strictly increasing for σ > 0.

(79)

(80)

Proof of Lemma 25. We prove each property (Equations (79) and (80)) in turn. Throughout
these proofs, J(σ) is as deﬁned in Deﬁnition 21, and G(σ) is as deﬁned in Deﬁnition 24.

29

(Proof of Equation (79).). Using the deﬁnition, we have:

∂
∂σ

G(σ)

= 2σ −

2
n

n
(cid:88)

i=1

(cid:112)2/π f −1( (cid:98)Ti, σ) exp(−f −1( (cid:98)Ti, σ)2/(2σ2))1( (cid:98)Ti ≥ f (η, σ))
2Φ(f −1( (cid:98)Ti, σ)/σ) − 1

(using Equation (58))

= 2σ −

4σ
n

n
(cid:88)

i=1

(cid:32)

1
2

−

f −1( (cid:98)Ti, σ)/σφ(f −1( (cid:98)Ti, σ)/σ)1( (cid:98)Ti ≥ f (η, σ))
2Φ(f −1( (cid:98)Ti, σ)/σ) − 1
f −1( (cid:98)Ti, σ)/σφ(f −1( (cid:98)Ti, σ)/σ)1( (cid:98)Ti ≥ f (η, σ))
2Φ(f −1( (cid:98)Ti, σ)/σ) − 1

=

4
n

n
(cid:88)

i=1

σ

as required.

(cid:33)

(81)

(cid:4)

(Proof of Equation (80).). Now per Lemma 17, we have that x (cid:55)→ xφ(x)/(2Φ(x) − 1) ≤ 1/2
∂σ G(σ),
for all x ≥ 0, and moreover it is decreasing for x > 0. Therefore the derivative, ∂
is bounded from below by 0. As such G(σ) is increasing in σ, for σ ≥ 0.
In fact, since
η, σ > 0, it follows that η
∂σ G(σ) is bounded from below by
σ > 0.
(cid:17)
(cid:16) 1
2 − η/σφ(η/σ)
It follows that G(σ) is
J(σ) := σ
(cid:4)
strictly increasing in σ, for σ > 0, as required.

In turn, we have that ∂
> 0, for each σ > 0, using Lemma 23.

2Φ(η/σ)−1

Thus all properties speciﬁed in Equations (79) and (80) are now proved.

30

B Proofs of Section 1

B.1 Mathematical Preliminaries

Lemma 26 (Symmetrization with Rademacher random variables). Suppose that ε is a sym-
metric distribution i.e. ε d= −ε, ξ ∼ Rademacher(α), with α ∈ [0, 1]. If ξ ⊥⊥ ε then ξε d= ε.

Proof of Lemma 26. Let us deﬁne Q := ξε. We then have the following:

Pr (Q ≥ q) := Pr (ξε ≥ q)

(since Q := ξε.)

= Pr (ξε ≥ q | ξ = −1) Pr (ξ = −1) + Pr (ξε ≥ q | ξ = 1) Pr (ξ = 1)

= Pr (−ε ≥ q) (1 − α) + Pr (ε ≥ q) (α)

= Pr (ε ≥ q) (1 − α) + Pr (ε ≥ q) (α)

= Pr (ε ≥ q) (1 − α + α)

= Pr (ε ≥ q)

(since ξ ∼ Rademacher(α).)
(since ξ ⊥⊥ ε.)
(since ε d= −ε.)

So we have that Q := ξε d= ε, as required.

The setting can be simpliﬁed if the adversary chooses the sign-corruptions independent
of the error terms. To see this, ﬁrst note that εi are centered (i.e. symmetric) Gaussian
random variables. Now, if the (ξ1, . . . , ξn) are picked independently from (ε1, . . . , εn), the
ASCI generating process response reduces to Ri = ξiµi + εi. That is our setting encompasses
this more simpliﬁed setting, and is shown formally in Corollary 27. Further, we note that in
a.s.= 1 then and µ1 ≤ µ2 ≤ . . . ≤ µn, then this is equivalent to the standard
the case where ξi
univariate isotonic regression setup.

Corollary 27. In the case where ξi ⊥⊥ εi, for each i ∈ [n] we have that the ASCI generating
process simpliﬁes to Ri = ξiµi + εi.

Proof of Corollary 27. We note that the underlying adversarial generating process is given by
Ri = ξi(µi + εi) = ξiµi + ξiεi, for each i ∈ [n]. Now since ξi ⊥⊥ εi we have by applying
i.i.d.∼ εi. And so the required adversarial model can be
Lemma 26 for each i ∈ [n] that ξiεi
written as Ri = ξiµi + εi, as required.

B.2

Important Model Deﬁnitions

First, we formally (redeﬁne) the generating model described in Example 3.

Deﬁnition 28 (Two-component Gaussian mixture ASCI special case from Example 3). We
consider n observations, {Ri | i ∈ [n]}, where each observation Ri is generated from the follow-
ing model:

Ri = ξiµi + εi

s.t. 0 < η ≤ µ1 ≤ µ2 ≤ . . . ≤ µn
and εi
and ξi

i.i.d.∼ N (cid:0)0, σ2(cid:1)
i.i.d.∼ Rademacher(p), p ∈ (0, 1), and ξi ⊥⊥ εi

31

(82)

(83)

(84)

(85)

Second, we formally deﬁne the generating model described in Example 4.

Deﬁnition 29 (Non-convex generating model from Example 4). We consider n observations,
{Ri | i ∈ [n]}, where each observation Ri is generated from the following model:

Ri = γi + εi

s.t. 0 < η ≤ |γ1| ≤ |γ2| ≤ . . . ≤ |γn|
and εi

i.i.d.∼ N (cid:0)0, σ2(cid:1)

(86)

(87)

(88)

Remark 8. From a simulation perspective, each γi is generated ﬁrst subject to Equation (87),
then ξi is sampled independently, and both are added to give each response Ri.
Third, we introduce an alternative model as per Deﬁnition 30.

Deﬁnition 30 (Alternative non-convex model).

Ri = ξiai + εi

i.i.d.∼ N (cid:0)0, σ2(cid:1)

s.t. 0 < η ≤ a1 ≤ a2 ≤ . . . ≤ an
and εi
and ξi = sgn (γi)
and ξi ⊥⊥ εi
and ai = |γi|

(89)

(90)

(91)

(92)

(93)

(94)

Finally, for convenience we recall Deﬁnition 2 as follows.

Deﬁnition 2 (Adversarial sign-corrupted isotonic (ASCI) regression). We consider n observa-
tions, {Ri | i ∈ [n]}, where each observation Ri is generated from the following model:

Ri = ξi(µi + εi)

s.t. 0 < η ≤ µ1 ≤ µ2 ≤ . . . ≤ µn
and εi
and ξi ∈ {−1, 1}

i.i.d.∼ N (cid:0)0, σ2(cid:1)

(4)

(5)

(6)

(7)

B.3 Proof justiﬁcation for Example 3

Proposition 31 (Justiﬁcation for Example 3). Under the model generating process described
in Example 3 (i.e., per Deﬁnition 28), the following model deﬁnition inclusion holds.

Deﬁnition 28 ⊆ Deﬁnition 2

(95)

Remark 9. Here, each deﬁnitional inclusion is to be read as the former generating model
deﬁnition being a special case of the latter generating model deﬁnition.

Proof of Example 3. Our basic strategy is to show each model inclusion in turn.

(Deﬁnition 28 ⊆ Deﬁnition 2). Observe that Equations (83) and (84) are deﬁnitionally equiv-
alent to Equations (5) and (6), respectively. Moreover we have that Equation (85) is a special
i.i.d.∼ Rademacher(p), p ∈
case of Equation (7). Finally, from Equation (85) we have that ξi

32

(0, 1), and ξi ⊥⊥ εi. Thus from Corollary 27, it follows that Equation (82) is a special case of
Equation (82).

In summary we have shown that Equation (95) holds, from which it follows that Example 3
(or equivalently Deﬁnition 28) is a special case of Deﬁnition 2, as required.

B.4 Proof justiﬁcation for Example 4

We now provide a formal proof justiﬁcation that Example 4 is a special case of the generating
process described in Deﬁnition 2.

Proposition 32 (Justiﬁcation for Example 4). Under the model generating process described
in Example 4, the following model deﬁnition inclusion holds.

Deﬁnition 29 = Deﬁnition 30 ⊆ Deﬁnition 2

(96)

Remark 10. As with Proposition 31, each deﬁnitional inclusion is to be read as the former
generating model deﬁnition being a special case of the latter generating model deﬁnition. In
the case of equivalence, we note that that both inclusions hold between the model deﬁnitions.

Proof of Example 4. Our basic strategy is to show each model inclusion in turn.

(Deﬁnition 29 = Deﬁnition 30) This follows by construction. Observe that Equations (92)
and (94) imply that ξiai = sgn (γi) |γi| = γi, so that Equations (86) and (89) are equivalent.
In addition from Equation (94), we have that ai = |γi| and thus Equations (87) and (90) are
equivalent, as are Equations (88) and (91). As such the equality is established between the
two generating model deﬁnitions.

(Deﬁnition 30 ⊆ Deﬁnition 2). Observe that by Equations (6) and (91) are deﬁnitionally
equivalent. Observe from Equation (92) that ξi = sgn (γi) ∈ {−1, 1} which is a special case of
Equation (7). For each observation i ∈ [n] using Equation (94) that by setting ai := µi that
Equations (5) and (90) are equivalent. Finally since ξi ⊥⊥ εi from Equation (93), we note that
Equation (89) is a special case of Equation (4) by applying Corollary 27 to the observation
Ri, for each i ∈ [n].

In summary we have shown that Equation (96) holds, from which it follows that Deﬁnition 29
is a special case of Deﬁnition 2, as required.

33

C Proofs of Section 2

C.1 Mathematical Preliminaries

Theorem 33 (Projection onto the nonnegative monotone cone). Suppose that S ↑ ⊆ Rn is the
monotone cone, that is,

S ↑ :=

µ := (µ1, . . . , µn)(cid:62) ∈ Rn (cid:12)
(cid:110)
(cid:12)
(cid:12) µ1 ≤ . . . ≤ µn

(cid:111)

.

and S ↑

+ ⊆ Rn is the nonnegative monotone cone, that is,
µ := (µ1, . . . , µn)(cid:62) ∈ S ↑ (cid:12)
(cid:111)
(cid:110)
(cid:12)
(cid:12) µ1 ≥ 0

S ↑
+ :=

.

Then for an arbitrary v ∈ Rn it holds that

ΠS↑

+

(v) = (ΠS↑(v))+ ,

where for any z ∈ Rn, z+ ∈ Rn stands for the lattice operation deﬁned by the order induced
by the nonnegative orthant in Rn. That is, we deﬁne the operation componentwise as (z+)i :=
(z)i ∨ 0 for each component index i ∈ [n].

Proof of Theorem 33. See Németh and Németh [2012, Corollary 1] for details.

Remark 11. In eﬀect, Theorem 33 basically states that in order to project onto the nonegative
monotone cone, K, one can instead ﬁrst project onto the monotone cone, W , ﬁrst, and then
take the non-negative part along each component. This is useful, since one can leverage
algorithms like the PAVA which already eﬃciently handle projection onto the unrestricted
monotone cone, W .

C.2 Proof of Proposition 6

Proposition 6 (Reason for the “∨f (η, σ)”-correction in Step II). The need for deﬁning the
∨f (η, σ) in Equation (17) in Step II in ASCIFIT, is that the solution to the problem

arg min
(cid:101)T1,..., (cid:101)Tn

n
(cid:88)

(Ti − (cid:101)Ti)2 s.t. f (η, σ) ≤ (cid:101)T1 ≤ . . . ≤ (cid:101)Tn,

i=1

is related to the solution to

arg min
(cid:98)T1,..., (cid:98)Tn

n
(cid:88)

i=1

(Ti − (cid:98)Ti)2 s.t. (cid:98)T1 ≤ . . . ≤ (cid:98)Tn,

as (cid:101)Ti := (cid:98)Ti ∨ f (η, σ).

(19)

(20)

Proof of Proposition 6. This follows along the following lines. First subtract f (η, σ) from all
˜Ti to bring the ﬁrst problem to

arg min
¯Ti

n
(cid:88)

((Ti − f (η, σ)) − ¯Ti)2 s.t. 0 ≤ ¯T1 ≤ . . . ≤ ¯Tn,

(97)

i=1

34

where ¯Ti = ˜Ti − f (η, σ). Now the solution to the unrestricted problem

arg min
T ∗
i

n
(cid:88)

i=1

((Ti − f (η, σ)) − T ∗

i )2 s.t. T ∗

1 ≤ . . . ≤ T ∗
n ,

i = ˆTi − f (η, σ). Next we apply Theorem 33, we see that ¯Ti = T ∗

i ∨ 0, so that ˜Ti =
i + f (η, σ)) ∨ f (η, σ) = ˆTi ∨ f (η, σ) which is what we

i ∨ 0 + f (η, σ) = (T ∗

is T ∗
¯Ti + f (η, σ) = T ∗
wanted to show.

35

D Proofs of Section 3

D.1 Mathematical Preliminaries

The key idea to prove this theorem here is to apply [Zhang, 2002, Theorem 2.2(ii)] to our
speciﬁc setting. To ensure our work is self-contained, we translate this result into the notation
of our paper:

Theorem 34 (Theorem 2.2 (ii) [Zhang, 2002]). Let Rn,p(f, µ, σ, σp) :=
|δi|p∨2(cid:17)

Let δi := Ti − f (µi, σ) be independent, with E (δi) = 0 and E

(cid:16)

(cid:80)n

(cid:16) 1
n
≤ σp∨2
p

i=1 E

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)Ti − f (µi, σ)

p(cid:17)(cid:17) 1
(cid:12)
p .
(cid:12)
(cid:12)

, p ≥ 1 then:

Rn,p(f, µ, σ, σp) ≤ 2

1

p σpCp min

(cid:40)



1,

3
2

3
(3 − p)+

(cid:18) V (f, µ, σ)
nσpCp

(cid:19) p

3

+

1
n

(cid:90) n

dx

0

(x ∨ 1)

p
2

(cid:41) 1
p


 (98)

where Cp are constants depending on p only in general.

Proof of Theorem 34. See Zhang [2002, Theorem 2.2(ii)] for details. Note that to translate
between our notation and theirs respectively, we have Ti ≡ yi, (cid:98)Ti ≡ (cid:98)fn(ti), f (µi, σ) ≡ f (ti), δi ≡
εi for each i ∈ [n].

Corollary 35 (Upper bound for R2
We then have:

n,2(f, µ, σ, σ2)). In our setting, deﬁne X := 1
n

(cid:16)

(cid:80)n

i=1

(cid:98)Ti − f (µi, σ)

(cid:17)2

.

E (X) ≤ min

(cid:34)
2σ2C2
2 ,

27
4

(cid:18) µn − µ1
n

(cid:19) 2
3

(σC2)

4
3 +

2σ2C2
2
n

(cid:35)

(1 + log n)

(99)

=: rn,2(µn, µ1, σ)

where C2 is a constant.

(cid:80)n

(cid:16)

(cid:17)2

i=1

(cid:98)Ti − f (µi, σ)

, then X = R2

Proof of Corollary 35. Since X := 1
n,2(f, µ, σ, σ2)2, by
n
deﬁnition in the setting of Theorem 34, assuming the relevant suﬃcient conditions are met.
We now need to check the suﬃcient condition for Theorem 34. Here we have, for each i ∈ [n],
that δi := Ti −f (µi, σ). Note that by deﬁnition E (δi) = E (Ti)−f (µi, σ) = 0. We observe that
(δ1, . . . , δn) are independent since the original responses, i.e. (R1, . . . , Rn) are independent by
assumption. And taking absolute values and centering are measurable transformations which
preserve their independence. We note that as per Zhang [2002, Theorem 2.2(ii)], we are
required to check the suﬃcient condition E
. In our case, with p = 2, this is
equivalent to showing that E (cid:0)δ2
i

. Then for each i ∈ [n] we have:

|δi|p∨2(cid:17)

(cid:1) ≤ σ2
2

≤ σp∨2
p

(cid:16)

(cid:16)

|δi|p∨2(cid:17)

E

= E (cid:0)δ2
i

(cid:1)

= Var (Ti)
=: g(µi, σ)
≤ σ2
=: σ2
2

36

(since p = 2.)

(since δi are mean centered Ti values.)
(by deﬁnition.)

(using Equation (48))

(100)

As required, by deﬁning σ2 := σ. So we meet this suﬃcient condition. Additionally observe
that

(cid:90) n

0

dx
(x ∨ 1)

=

=

(cid:90) 1

0
(cid:90) 1

0

dx
(x ∨ 1)

dx +

(cid:90) n

1

(cid:90) n

+

1
dx
x

dx
(x ∨ 1)

(by truncation)

Now, in our setting note that V (f, µ, σ) ≤ µn − µ1 using Equation (47), it follows that:

= 1 + log n

(101)

E (X) := R2

n,2(f, µ, σ, σ2)

(by deﬁnition.)

(cid:40)

≤ min

2σ2

2C2
2 ,

27
4

(cid:18) µn − µ1
n

(cid:19) 2
3

(σ2C2)

4
3 +

2C2
2σ2
2
n

(cid:90) n

0

dx
(x ∨ 1)

(cid:41)

(setting p = 2 in Theorem 34.)

(cid:40)

= min

2σ2

2C2
2 ,

27
4

(cid:18) µn − µ1
n

(cid:19) 2
3

(σ2C2)

4
3 +

2C2
2σ2
2
n

(cid:41)

(1 + log n)

(using Equation (101))

(cid:40)

= min

2σ2C2
2 ,

27
4

(cid:18) µn − µ1
n

(cid:19) 2
3

(σC2)

4
3 +

2σ2C2
2
n

(cid:41)

(1 + log n)

=: rn,2(µn, µ1, σ)

as required.

(since σ2 := σ per Equation (100))
(102)

Lemma 36 (Concentration for mean Folded Normal). In our setting we assume that 1
n
C, for each n ∈ N. Deﬁne X := 1
n

i=1 (Ti − f (µi, σ))2. We then have:

(cid:80)n

(cid:80)n

i=1 µ2

i ≤

|X − E (X)| ≤ 2γσ

(cid:114)

5σ2 + 4C
n
i=1 g(µi, σ) = 1
n

(cid:80)n

(103)

(cid:80)n

i=1

(cid:0)µ2

i + σ2 − f (µi, σ)2(cid:1).

with probability at least 1−γ−2, where E (X) = 1
n

Proof of Lemma 36. First we determine E (X) as follows:

E (X) := E

(cid:32)

1
n

n
(cid:88)

(Ti − f (µi, σ))2

(cid:33)

(by deﬁnition of X)

i=1
(cid:16)

E

(Ti − f (µi, σ))2(cid:17)

=

=

=

=

1
n

1
n

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

Var (Ti)

g(µi, σ)

(by since f (µi, σ) := E (Ti).)

(by since g(µi, σ) := Var (Ti).)

(cid:0)µ2

i + σ2 − f (µi, σ)2(cid:1)

(using Equation (16))

37

as required. Next we determine Var (X) as follows:

Var (X) := Var

(cid:32)

1
n

n
(cid:88)

(Ti − f (µi, σ))2

(cid:33)

(by deﬁnition of X)

(Ti − f (µi, σ))2(cid:17)

(by the independence of Ti)

i=1
(cid:16)

Var

=

1
n2

n
(cid:88)

i=1

Now note that for each i ∈ [n] we have:

Var

(cid:16)

(Ti − f (µi, σ))2(cid:17)

i − 2f (µi, σ)Ti

i + f (µi, σ)2 − 2f (µi, σ)Ti

(cid:1)
(cid:1) + Var (2f (µi, σ)Ti)(cid:1)
(cid:1) + 8f (µi, σ)2 Var (Ti)
(cid:1) + 8f (µi, σ)2g(µi, σ)
i σ2 + 2σ4) + 8f (µi, σ)2σ2

= Var (cid:0)T 2
= Var (cid:0)T 2
≤ 2 (cid:0)Var (cid:0)T 2
i
= 2 Var (cid:0)T 2
i
= 2 Var (cid:0)T 2
i
≤ 2(4µ2
= 8µ2
≤ 16f (µi, σ)2σ2 + 4σ4
i + σ2)σ2 + 4σ4
≤ 16(µ2
i σ2 + 20σ4
= 16µ2

i σ2 + 8f (µi, σ)2σ2 + 4σ4

(cid:1)

(by translation invariance.)

(using Corollary 15)

(since g(µi, σ) := Var (Ti))
(using Equations (48) and (50))

(using Equation (42))

(using Equation (46))

(104)

Therefore we have that

Var (X) =

≤

≤

≤

1
n2

1
n2

1
n2

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

Var

(cid:16)

(Ti − f (µi, σ))2(cid:17)

(cid:0)16f (µi, σ)2σ2 + 4σ4(cid:1)

(using Equation (42))

(cid:0)16µ2

i σ2 + 20σ4(cid:1)

(using Equation (104))

16Cσ2 + 20σ4
n

(assuming 1
n

(cid:80)n

i=1 µ2

i ≤ C, for each n ∈ N.)

From this it follows that:

Pr (|X − E (X)| ≥ t) ≤

Var (X)
t2

,

for each t > 0

(using Chebychev’s inequality)

≤

16Cσ2 + 20σ4
nt2

,

for each t > 0

(105)

It then follows that by setting the upper bound (RHS) to γ−2 ∈ (0, 1), that

16Cσ2 + 20σ4
nt2

=

1
γ2 =⇒ t = γσ

(cid:114)

5σ2 + 4C
n

We then have that |X − E (X)| ≤ 2γσ

(cid:113) 5σ2+4C
n

, with probability at least 1−γ−2, as required.

38

Our end goal is to to show a the following high probability result described in Theorem 37.

Theorem 37 (Concentration of ﬁtted Folded Normal).

1
n

n
(cid:88)

(cid:16)

i=1

(cid:98)Ti − f (µi, σ)

(cid:17)2

≤ δrn,2(µn, µ1, σ)

(106)

with probability at least 1 − δ−1.

repre-
Proof of Theorem 37. First to simplify notation we let X := 1
n
sent the quantity of interest. Observe that X ≥ 0 a.s. by deﬁnition, so that |X| a.s.= X. Then
for any t > 0 we have:

(cid:98)Ti − f (µi, σ)

i=1

(cid:80)n

(cid:16)

(cid:17)2

Pr (X ≥ t) ≤

E (X)
t
R2
n,2(f (µi, σ))
t
rn,2(µn, µ1, σ)
t
It then follows that by setting the upper bound (RHS) to δ−1 ∈ (0, 1), that

≤

≤

(by deﬁnition, per Corollary 35)

(using Corollary 35)

(by Markov’s inequality)

rn,2(µn, µ1, σ)
t

=

1
δ

=⇒ t = δrn,2(µn, µ1, σ)

We then have that |X| a.s.= X ≤ δrn,2(µn, µ1, σ), with probability at least 1 − δ−1, as required.

Lemma 38 (Concentration of 1
n
have:

(cid:80)n

i=1 T 2
i

). In our setting, deﬁne X := 1
n

(cid:80)n

i=1 T 2

i . We then

with probability at least 1 − γ−2, where E (X) = 1
n

|X − E (X)| ≤ 2γσ

(cid:114)

2σ2 + 4C
n
(cid:0)µ2

i=1

(cid:80)n

i + σ2(cid:1).

Proof of Lemma 38. Let X := 1
n

(cid:80)n

i=1 T 2
i

. First we determine E (X) as follows:

(107)

(by deﬁnition of X)

(by linearity of expectation.)

E (X) := E

(cid:32)

(cid:33)

1
n

n
(cid:88)

i=1

T 2
i

E (cid:0)T 2
i

(cid:1)

(cid:16)

Var (Ti) + (E (Ti))2(cid:17)

=

=

=

=

1
n

1
n

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:0)µ2

i + σ2 − f (µi, σ)2 + f (µi, σ)2(cid:1)

(using Equations (15) and (16))

(cid:0)µ2

i + σ2(cid:1)

39

as required. Next we determine Var (X) as follows:

Var (X) := Var

(cid:32)

(cid:33)

1
n

n
(cid:88)

i=1

T 2
i

n
(cid:88)

i=1
n
(cid:88)

Var (cid:0)T 2
i

(cid:1)

(cid:0)4µ2

i σ2 + 2σ4(cid:1)

1
n2

1
n2

=

=

≤

i=1
4Cσ2 + 2σ4
n

(by deﬁnition of X.)

(by the independence of Ti.)

(using Equation (50))

(assuming 1
n

(cid:80)n

i=1 µ2

i ≤ C, for each n ∈ N.)

From this it follows that:

Pr (|X − E (X)| ≥ t) ≤

Var (X)
t2

,

for each t > 0

(using Chebychev’s inequality)

≤

4Cσ2 + 2σ4
nt2

,

for each t > 0

(108)

It then follows that by setting the upper bound (RHS) to γ−2 ∈ (0, 1), that

4Cσ2 + 2σ4
nt2

=

1
γ2 =⇒ t = γσ

(cid:114)

2σ2 + 4C
n

We then have that |X − E (X)| ≤ 2γσ

(cid:113) 2σ2+4C
n

, with probability at least 1−γ−2, as required.

D.2 Proof of Theorem 7

(cid:80)n

Theorem 7 (Equation (17) has a unique root). Assume that there exist constants ψ, Ψ, C > 0
i ≤ C, for each n ∈ N. In addition let rn,2(µn, µ1, σ) =
such that ψ ≤ σ ≤ Ψ and 1
n
o(1), where the quantity rn,2(µn, µ1, σ) is deﬁned in (21). Then for suﬃciently large n, δ =
, and γ = o (cid:0)n1/2(cid:1), under the ASCI setup, Equation (17) in ASCIFIT has
o
(cid:113) 1
n

(rn,2(µn, µ1, σ))−1(cid:17)
(cid:104)
0,

for σ with probability at least 1 − δ−1 − 2γ−2.

a unique root σ∗ ∈

i=1 T 2
i

i=1 µ2

(cid:80)n

(cid:16)

(cid:105)

Proof of Theorem 7. First, under the ASCIFIT setup, we can rewrite Equation (17) as H(σ) =
0, where:

H(σ) := G(σ) −

1
n

n
(cid:88)

i=1

T 2
i .

G(σ) := σ2 +

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, σ), σ))2

i=1

(109)

(110)

(cid:105)
Our goal in this proof is to show that H(σ) = 0 has a solution σ∗ ∈
, which
occurs with high probability. We note that per Lemma 25 that G(σ) is increasing for σ ≥ 0

i=1 T 2
i

(cid:80)n

(cid:113) 1
n

0,

(cid:104)

40

and strictly increasing for σ > 0 (per Equation (80)). Moreover to see that the equation
H(σ) = 0 has a unique root we appeal to the Intermediate Value Theorem. Speciﬁcally we
are required to ﬁnd two values for σ, i.e. {σ1, σ2}, such that the following conditions hold:

G(σ2) ≥

G(σ1) ≤

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

T 2
i

T 2
i

By taking σ2 :=

(cid:113) 1
n

(cid:80)n

i=1 T 2
i

, we observe that a.s.:

G(σ2) =

1
n

n
(cid:88)

i=1

T 2
i +

n
(cid:88)

i=1

1
n
(cid:124)

(cid:32)

(cid:32)

f −1

(cid:98)Ti ∨ f

≥

n
(cid:88)

i=1

T 2
i

(cid:32)

n
(cid:88)

i=1

η,

1
n
(cid:123)(cid:122)
≥0 a.s.

(111)

(112)

(113)

(114)

(cid:33)

T 2
i

,

1
n

n
(cid:88)

i=1

T 2
i

(cid:33)(cid:33)2

(cid:125)

So indeed σ2 :=

i=1 T 2
i
claim that σ1 := 0 will satisfy Equation (112). First observe that:

satisﬁes the required condition in Equation (111). We now

(cid:113) 1
n

(cid:80)n

G(0) =

1
n

n
(cid:88)

i=1

f −1( (cid:98)Ti ∨ f (η, 0), 0)2 =

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2,

i=1

we then want to show that

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2 ≤

i=1

1
n

n
(cid:88)

i=1

T 2
i ,

(115)

(116)

holds with high probability, to be speciﬁed later.

Furthermore, since (cid:98)Ti ∨ η is the solution to an optimization problem we have that a.s.:

n
(cid:88)

i=1

( (cid:98)Ti ∨ η − η)(Ti − η) =

n
(cid:88)

i=1

( (cid:98)Ti ∨ η − η)2.

(117)

We see that Equation (117) holds since when you project any vector v ∈ Rn on a monotone
cone K ⊆ Rn, then ΠK(v)(cid:62)v = (cid:107)ΠK(v)(cid:107)2
per Bellec [2018, Equation 1.16]. Speciﬁcally, in
2
our case we have that K = S ↑
+ := {µ := (µ1, . . . , µn)(cid:62) ∈ Rn : 0 ≤ µ1 ≤ µ2 ≤ . . . ≤ µn},
and v := (T1 − η, . . . , Tn − η)(cid:62). We further observe that Equation (117) can be rewritten as

41

follows a.s.:

⇐⇒

n
(cid:88)

i=1

n
(cid:88)

( (cid:98)Ti ∨ η − η)2 =

i=1

( (cid:98)Ti ∨ η)2 + 2η

n
(cid:88)

( (cid:98)Ti ∨ η − η) + η2 =

n
(cid:88)

i=1
n
(cid:88)

i=1

i=1
n
(cid:88)

− η

( (cid:98)Ti ∨ η − η)(Ti − η)

(Equation (117))

( (cid:98)Ti ∨ η)Ti − η

n
(cid:88)

( (cid:98)Ti ∨ η − η)

i=1

(cid:98)Ti + η2

(expanding LHS/RHS.)

⇐⇒

⇐⇒

n
(cid:88)

i=1

n
(cid:88)

i=1

i=1
n
(cid:88)

( (cid:98)Ti ∨ η)Ti − η

( (cid:98)Ti ∨ η)2 =

i=1

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)2 =

( (cid:98)Ti ∨ η)Ti − η

(cid:32) n
(cid:88)

i=1

(cid:32) n
(cid:88)

i=1

Ti −

(cid:98)Ti −

n
(cid:88)

i=1

n
(cid:88)

i=1

(cid:33)

(cid:98)Ti ∨ η

(118)
(cid:33)

(cid:98)Ti ∨ η

,

(119)

where to go from Equation (118) to Equation (119) we used the fact that (cid:80)n
i=1 (cid:98)Ti.
This holds since we know that (cid:98)Ti are the PAVA solutions. Now we derive the following upper
bound a.s.:

i=1 Ti = (cid:80)n

(cid:32) n
(cid:88)

(cid:98)Ti −

1
n

(cid:33)

(cid:98)Ti ∨ η

n
(cid:88)

i=1

i=1
n
(cid:88)

i=1
n
(cid:88)

1
n

1
n
(cid:118)
(cid:117)
(cid:117)
(cid:116)

=

=

≤

i=1

1
n

n
(cid:88)

(cid:16)

i=1

(cid:98)Ti −

1
n

n
(cid:88)

i=1

f (µi, σ) +

1
n

n
(cid:88)

f (µi, σ) −

n
(cid:88)

(cid:98)Ti ∨ η

1
n

i=1
(cid:17)

f (µi, σ) − (cid:98)Ti ∨ η

(cid:16)

(cid:98)Ti − f (µi, σ)

(cid:17)

+

1
n

(cid:98)Ti − f (µi, σ)

(cid:17)2

i=1
(cid:16)

n
(cid:88)

i=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

1
n

n
(cid:88)

(cid:16)

i=1

(cid:98)Ti ∨ η − f (µi, σ)

(cid:17)2

,

(120)

(121)

where the transition between Equations (120) and (121) was by applying the Cauchy-Schwartz
inequality to each summand. Note that for each i ∈ [n], we have that f (µi, σ) ≥ µi ≥ η per
Equations (5) and (42). Then using Lemma 13 we have a.s.:

1
n

n
(cid:88)

(cid:16)

i=1

( (cid:98)Ti ∨ η) − f (µi, σ)

(cid:17)2

≤

1
n

n
(cid:88)

i=1

( (cid:98)Ti − f (µi, σ))2.

(122)

Hence by ﬁrst using the inequality in Equation (122) to upper bound Equation (121), we

can in turn upper bound the LHS of Equation (119) as follows a.s.:

1
n

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)2 ≤

1
n

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)Ti + 2η

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

( (cid:98)Ti − f (µi, σ))2 .

(123)

On the other hand we have by Theorem 37 that:

1
n

n
(cid:88)

(cid:16)

i=1

(cid:98)Ti − f (µi, σ)

(cid:17)2

≤ δrn,2(µn, µ1, σ),

(124)

42

with probability at least 1 − δ−1, for δ−1 ∈ (0, 1). Thus from Equation (123), we have:

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2 ≤

i=1

=

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

( (cid:98)Ti ∨ η)Ti + 2η (δrn,2(µn, µ1, σ))

( (cid:98)Ti ∨ η) (cid:98)Ti + 2η (δrn,2(µn, µ1, σ))

1
2

1
2 .

(using Equation (124))

(125)

Note that the ﬁnal equality in Equation (125) holds, since (cid:80)n

i=1( (cid:98)Ti ∨η) (cid:98)Ti,
by again since we know that (cid:98)Ti are the PAVA solutions. We then apply Cauchy-Schwartz to
this summand of Equation (125) to obtain the following upper bound with probability at least
1 − δ−1, for δ−1 ∈ (0, 1).

i=1( (cid:98)Ti ∨η)Ti = (cid:80)n

1
n

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

1
n

( (cid:98)Ti ∨ η)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:98)T 2
i + 2η (δrn,2(µn, µ1, σ))

1
2 ,

(126)

Now observe that since η > 0, the following holds a.s.:

η = |η| =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

η2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2

i=1

(127)

Then using Equation (127) we have the following:



η



(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)2 −

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1



(cid:98)T 2
i



(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

≤

( (cid:98)Ti ∨ η)2





(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

( (cid:98)Ti ∨ η)2 −

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1



(cid:98)T 2
i



=

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2 −

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:98)T 2
i .

(using Equation (127))

(128)

By applying the upper bound derived in Equation (126) to Equation (128) we obtain the

following:

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2 −

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:98)T 2
i ≤ 2 (δrn,2(µn, µ1, σ))

1
2 ,

(129)

with probability at least 1 − δ−1, for δ−1 ∈ (0, 1).

(cid:113)

(cid:80)n

Now we will show that

, which
will imply that for large n the value at 0 is smaller than the target value, i.e., G(σ1) := G(0) ≤
1
n

is a constant distance away from

as required per Equation (112).
i=1 T 2
i
On the other hand using Lemma 36, we have:

i=1 T 2
i

i=1 (cid:98)T 2
i

(cid:80)n

1
n

(cid:113) 1
n

(cid:80)n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

(Ti − f (µi, σ))2 −

i=1

1
n

n
(cid:88)

i=1

(cid:0)µ2

i + σ2 − f (µi, σ)2(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ l(γ, C, σ),

(130)

43

with probability at least 1−γ−2, where l(γ, C, σ) := γσ
in Equations (124) and (130) we then obtain:

(cid:113) 5σ2+4C
n

. Subtracting the inequalities

1
n

n
(cid:88)

i=1

T 2
i −

1
n

n
(cid:88)

i=1

(cid:98)T 2
i +

2
n

n
(cid:88)

( (cid:98)Ti − Ti)f (µi, σ)

i=1

≥

1
n

n
(cid:88)

i=1

(cid:0)µ2

i + σ2 − f (µi, σ)2(cid:1) − l(γ, C, σ) − δrn,2(µn, µ1, σ)

(131)

⇐⇒

1
n

n
(cid:88)

i=1

T 2
i −

1
n

n
(cid:88)

i=1

(cid:98)T 2
i

(cid:0)µ2

i + σ2 − f (µi, σ)2(cid:1)

n
(cid:88)

i=1

≥

1
n
(cid:32)

−

l(γ, C, σ) + δrn,2(µn, µ1, σ) +

(cid:33)

( (cid:98)Ti − Ti)f (µi, σ)

.

(132)

2
n

n
(cid:88)

i=1

Now, in order sharpen the lower bound in Equation (132), we upper bound the term 2
n
Ti)f (µi, σ) as follows:

(cid:80)n

i=1( (cid:98)Ti−

2
n

n
(cid:88)

i=1

( (cid:98)Ti − Ti)f (µi, σ)

n
(cid:88)

( (cid:98)Ti − Ti)(f (µi, σ) − (cid:98)Ti)

(since (cid:98)Ti are the PAVA solutions.)

( (cid:98)Ti − Ti)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(f (µi, σ) − (cid:98)Ti)2

(by Cauchy-Schwartz.)

(f (µi, σ) − Ti)2

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

(f (µi, σ) − (cid:98)Ti)2

(since (cid:98)Ti are PAVA, i.e., LSE solutions.)

=

≤

≤

2
n

2
n

2
n

i=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

= 2

1
n

n
(cid:88)

i=1

(f (µi, σ) − Ti)2

i=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(cid:16)

i=1

f (µi, σ) − (cid:98)Ti

(cid:17)2

≤ 2 (l(γ, C, σ)δrn,2(µn, µ1, σ))

1
2 ,

(133)

(134)

with probability at least with probability at least 1 − δ−1 − γ−2, by the union bound. Note
that to obtain Equation (134) we applied the bounds in Equations (146) and (130) to Equa-
tion (133). Now using the bound in Equation (134) in Equation (132) we conclude that:

1
n

n
(cid:88)

i=1

T 2
i −

1
n

n
(cid:88)

i=1

(cid:98)T 2
i

n
(cid:88)

i=1

g(µi, σ)

1
n
(cid:16)

≥

−

l(γ, C, σ) + δrn,2(µn, µ1, σ) + 2 (l(γ, C, σ)δrn,2(µn, µ1, σ))

(cid:17)

1
2

(135)

44

Now from Equation (49) we have that g(µ, σ) ≥ g(0, σ) = σ2 (cid:0)1 − 2
π
if σ ≥ ψ > 0, then

(cid:1), for each µ > 0. Hence

1
n

n
(cid:88)

i=1

T 2
i −

1
n

n
(cid:88)

i ≥ ψ2
(cid:98)T 2

i=1

(cid:18)

1 −

(cid:19)

(cid:16)

−

2
π

l(γ, C, σ) + δrn,2(µn, µ1, σ) + 2 (l(γ, C, σ)δrn,2(µn, µ1, σ))

(cid:17)

1
2

> 0

(cid:80)n

(cid:80)n

i − 1
n

i=1 (cid:98)T 2

i=1 T 2

i ≥ ψ2 (cid:0)1 − 2

(136)
(cid:1) > 0, with probability at least 1−δ−1 −γ−2. Hence under
1
n
the assumption that rn,2(f, µn, µ1, σ) = o(1), for suﬃciently large n the above will be bigger
(cid:113) 2σ2+4C
than a constant. Now by Lemma 38 we have 1
n
n
which is upper bounded by some constant for suﬃciently large n given our assumption that
i ≤ C, for each n ∈ N, and σ ≤ Ψ for some constants C, Ψ > 0. It follows that by
1
n
applying Lemma 12 to Equation (136) we have:

i + σ2) + 2γσ

i ≤ 1
n

i=1(µ2

i=1 T 2

i=1 µ2

(cid:80)n

(cid:80)n

(cid:80)n

π

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

T 2
i −

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(cid:98)T 2
i ≥ κ > 0,

(137)

for suﬃciently large n, where κ is some positive constant, with probability at least 1 − δ−1 −
2γ−2.

Going back to equation (129), it follows that required equation will have a solution between
(cid:113) 1
(cid:105)
, with probability at least with probability at least 1 − δ−1 − 2γ−2, as required.
n

(cid:80) T 2
i

(cid:104)

0,

D.3 Proof of Theorem 8

Lemma 39 (Upper and lower bounds for
such that ψ ≤ σ ≤ Ψ and 1
n
Deﬁnition 2, the following hold:

i=1 µ2

(cid:80)n

(cid:98)σ). Assume that there exist constants ψ, Ψ, C > 0
i ≤ C, for each n ∈ N. Then under the ASCI setting per

(cid:98)σ ≥ K1 with probability at least 1 − δ−1 − 2γ−2, for suﬃciently large n.
(cid:98)σ ≤ K2 with probability at least 1 − δ−1 − 2γ−2, for suﬃciently large n,
where K1, K2 > 0 are ﬁxed constants and γ−2, δ−1 ∈ (0, 1) are as in the proof of Theorem 7.

(138)

(139)

Proof of Lemma 39. We prove each property (Equations (138) and (139)) in turn.

(Proof of Equation (138).) We note that by assumption we have 0 < ψ ≤ σ ≤ Ψ. We
(cid:98)σ is positively bounded away from 0, with high probability. First,
now want to show that
observe that per Theorem 7 that
, with high probability.
Per Equation (115), we then have that:

(cid:98)σ uniquely solves G((cid:98)σ) = 1

i=1 T 2
i

(cid:80)n

n

G((cid:98)σ) − G(0) =

1
n

n
(cid:88)

i=1

(cid:98)T 2
i −

1
n

n
(cid:88)

( (cid:98)Ti ∨ η)2.

i=1

(140)

We then have by the Mean Value Theorem, and the fact that G(σ) is increasing for each σ ≥ 0
(cid:101)σ ∈ [0, (cid:98)σ] such that
(per Lemma 25), that there exists a
G((cid:98)σ) − G(0) = G(cid:48)((cid:101)σ)(cid:98)σ.

(141)

45

Now we have
follows that G(cid:48)(σ) ≤ 2(cid:101)σ ≤ 2(cid:98)σ. Using this and Equation (141), we see that:

(cid:101)σ ≤ (cid:98)σ, or equivalently that 2(cid:101)σ ≤ 2(cid:98)σ. Since G(cid:48)(σ) ≤ 2σ using Equation (81), it

G((cid:98)σ) − G(0) = G(cid:48)((cid:101)σ)(cid:98)σ ≤ (2(cid:98)σ) (cid:98)σ ≤ 2(cid:98)σ2,

(142)

(cid:80)n

Now using Equation (142) and the proof of Theorem 7 we have that G((cid:98)σ)−G(0) = 1

i −
i=1( (cid:98)Ti ∨ η)2 is positively bounded away from 0 with high probability. So it follows that
(cid:113) G((cid:98)σ)−G(0)
(cid:4)

> 0, with high probability, as required.

i=1 T 2

n

(cid:80)n

1
n
(cid:98)σ ≥

2

(cid:80)n

i=1 T 2
i

(Proof of Equation (139).) First, observe that per Theorem 7 that
, with high probability. By Deﬁnition 24 this implies that
1
n
probability. Moreover by Lemma 38 we have 1
n
with probability at least γ−1, where 1 − γ−2 for γ ∈ (0, 1). This in turn is bounded, in
high probability, by some constant, K2 > 0 for suﬃciently large n given our assumptions
(cid:4)
1
n

(cid:98)σ uniquely solves G((cid:98)σ) =
, with high
(cid:98)σ ≤ 1
i=1 T 2
i
(cid:113) 2σ2+4C
i + σ2) + 2γσ
n

i ≤ C, for each n ∈ N, and σ ≤ Ψ, as required.

i ≤ 1
n

i=1(µ2

i=1 T 2

i=1 µ2

(cid:80)n

(cid:80)n

(cid:80)n

(cid:80)n

n

Thus all properties speciﬁed in Equations (138) and (139) are now proved.

Theorem 8 (
(cid:98)σ is close to σ). Under the assumptions of Theorem 7, we have that |σ − ˆσ| (cid:46)
(δrn,2(µn, µ1, σ))1/2 + γn−1/2 with probability at least 1 − δ−1 − 2γ−2, where δ−1, γ−2 ∈ (0, 1).

Proof of Theorem 8. Recall our map G(σ) := σ2 + 1
n
deﬁned in Equation (110). We will ﬁrst try to show that G(σ) is close to 1
n
note that f −1(· ∨ f (η, σ), σ) is a L :=
that σ is a (both upper and lower) bounded quantity by assumption. Thus it follows that

i=1(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 as originally
. First
-Lipschitz function per Lemma 20 and the fact

1
2Φ(η/σ)−1

i=1 T 2
i

(cid:80)n

(cid:80)n

(cid:12)
(cid:12)f −1( (cid:98)Ti ∨ f (η, σ), σ) − f −1(f (µi, σ), σ)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ L

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ,
(cid:12) (cid:98)Ti ∨ f (η, σ) − f (µi, σ)

and therefore

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2

(143)

(144)

(cid:12)
(cid:12)f −1( (cid:98)Ti ∨ f (η, σ), σ) − f −1(f (µi, σ), σ)
(cid:12)

(cid:12)
2
(cid:12)
(cid:12)

( (cid:98)Ti ∨ f (η, σ) − f (µi, σ))2

(using Equation (143))

( (cid:98)Ti − f (µi, σ))2

(using Equation (43) and Lemma 13.)

In sum, we have established:

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2 ≤

i=1

L2
n

n
(cid:88)

( (cid:98)Ti − f (µi, σ))2,

i=1

We saw earlier by Theorem 37 we have that

1
n

n
(cid:88)

(cid:16)

i=1

(cid:98)Ti − f (µi, σ)

(cid:17)2

≤ δrn,2(µn, µ1, σ),

46

(145)

(146)

=

≤

≤

1
n

L2
n

L2
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

with probability at least 1 − δ−1, for δ−1 ∈ (0, 1). Combining Equations (145) and (146) we
have that

(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2 ≤ L2δrn,2(µn, µ1, σ)

(147)

1
n

n
(cid:88)

i=1

with probability at least 1 − δ−1, for δ−1 ∈ (0, 1). Thus by the triangle inequality, and reverse
triangle inequality we have

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 ∈

(cid:34)

1
n

n
(cid:88)

i=1

µ2

i − hn,

1
n

n
(cid:88)

i=1

i + L2δrn,2(µn, µ1, σ) + hn
µ2

(148)

(cid:35)

where hn := 2
1
i=1 µ2
n

i=1 µ2
i
i ≤ C, for each n ∈ N, we have that:

(cid:80)n

1
n

(cid:80)n

(cid:80)n

(cid:113) 1
n

(cid:113)

i=1(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2 . Given our assumption that

hn ≤ 2L (Cδrn,2(µn, µ1, σ))

1
2

(149)

with probability at least 1 − δ−1, for δ−1 ∈ (0, 1). Then combining Equations (148) and (149),
we have that there exists some l1 ∈ [−2, 2] for suﬃciently large n such that

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, σ), σ))2 =

1
n

n
(cid:88)

i=1

µ2

i + l1L (2Cδrn,2(µn, µ1, σ))

1
2 .

(150)

with probability at least 1 − 2δ−1, for δ−1 ∈ (0, 1), using the union bound.

Similarly, using Lemma 38 we have that there exists some l2(σ, C, γ) ∈ R such that

1
n

n
(cid:88)

i=1

i = σ2 +
T 2

1
n

n
(cid:88)

i=1

i + l2(σ, C, γ)n−1/2.
µ2

(151)

with probability at least 1 − γ−2, for γ−2 ∈ (0, 1). Moreover per Theorem 7 we have that
uniquely solves G((cid:98)σ) = 1

, with high probability. That is:

i=1 T 2
i

(cid:80)n

n

(cid:98)σ

G((cid:98)σ) = (cid:98)σ2 +

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ))2 =

1
n

n
(cid:88)

i=1

T 2
i .

(152)

Then combining Equations (151) and (152), we conclude that

|G(σ) − G(ˆσ)| ≤ l1L (2Cδrn,2(µn, µ1, σ))

1

2 + l2(σ, C, γ)n−1/2.

(153)

(cid:98)σ > σ and σ > (cid:98)σ.

We now consider two cases, namely

(cid:98)σ > σ,
we seek to show that G(cid:48)(ξ) ≥ K1 > 0, in high probability, for each ξ ∈ (σ, (cid:98)σ). Here K1
represents a positive constant. By Lemma 39 both σ and ˆσ are upper and lower bounded by
some constants which implies that ξ is also upper and lower bounded by some constants call
. As we argued
them C1 and C2, i.e., C1 ≤ ξ ≤ C2. Since G(cid:48)(ξ) ≥ J(ξ) = ξ
earlier J(ξ) is positive and since it is a continuous function and the set [C1, C2] is compact it
achieves its minimum, which is strictly positive. Hence G(cid:48)(ξ) ≥ K1 > 0.

In the ﬁrst case, with

2 − η/ξφ(η/ξ)

2Φ(η/ξ)−1

(cid:18)

(cid:19)

1

Similarly, in the second case, with σ > (cid:98)σ, we can also show that G(cid:48)(ξ) ≥ K2 > 0, in high

probability, for each ξ ∈ (σ, (cid:98)σ). Where again, K2 represents a positive constant.

47

Then by using the Mean Value Theorem we have that there exists some ξ ∈ (σ, (cid:98)σ) such
that |G(σ) − G((cid:98)σ)| = G(cid:48)(ξ) |σ − (cid:98)σ| > min(K1, K2) |σ − (cid:98)σ|. Thus from equation (153) we have

l1L (2Cδrn,2(µn, µ1, σ))

1

2 + l2(σ, C, γ)n−1/2 = |G(σ) − G((cid:98)σ)| ≥ min(K1, K2) |σ − (cid:98)σ| ,

(154)

and hence |σ − (cid:98)σ| (cid:46) l1L (2Cδrn,2(µn, µ1, σ))

1

2 + l2(σ, C, γ)n−1/2.

D.4 Proof of Theorem 9

Theorem 9 (
have that

(cid:98)µasciﬁt is close to µ). Under the assumptions of Theorem 7 and Theorem 8, we

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, ˆσ), ˆσ) − µi)2 (cid:46) δrn,2(µn, µ1, σ) + γ2n−1,

(22)

with probability at least 1 − δ−1 − 2γ−2.

Proof of Theorem 9. We will now consider 1
n
that a.s.:

(cid:80)n

i=1(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − µi)2. We observe

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − µi)2

i=1

(155)

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − f −1( (cid:98)Ti ∨ f (η, σ), σ))2

+

(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − f −1( (cid:98)Ti ∨ f (η, σ), σ))2 ,

i=1

(156)

where the transition between Equations (155) and (156) was by applying adding and sub-
tracting f −1( (cid:98)Ti ∨ f (η, σ), σ), then applying the triangle inequality, and ﬁnally applying the
Cauchy-Schwartz inequality to the cross product summand.

We now set to upper bound the Equation (156) further. First, we saw in Equation (147)
i=1(f −1( (cid:98)Ti ∨ f (η, σ), σ) − µi)2 ≤ L2δrn,2(µn, µ1, σ), with probability at least 1 − δ−1,

that 1
n
for δ−1 ∈ (0, 1). Next, we will tackle the term

(cid:80)n

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − f −1( (cid:98)Ti ∨ f (η, σ), σ))2.

(157)

Note that map σ (cid:55)→ f −1( (cid:98)Ti ∨ f (η, σ), σ) is a L :=
-Lipschitz per
Lemmas 16 and 20, and in addition both σ, ˆσ are upper and lower bounded by constants. It

≤

2/π exp(−µ2/σ2/2)
2Φ(µ/σ)−1

2/π
2Φ(η/σ)−1

√

√

48

≤

1
n

i=1
n
(cid:88)

i=1

1
n
(cid:118)
(cid:117)
(cid:117)
(cid:116)

+ 2

follows that

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − f −1( (cid:98)Ti ∨ f (η, σ), σ))2

≤

1
n

n
(cid:88)

i=1

|L(σ − (cid:98)σ)|2

= L2(σ − (cid:98)σ)2
(cid:46) 2Cδrn,2(µn, µ1, σ) + γ2n−1,

(using L-Lipschitz property.)

(158)

where Equation (158) follows from Theorem 8 with probability at least 1 − δ−1 − 2γ−2.

Then applying the upper bounds in Equations (157) and (158) appropriately to each cor-

responding summand of Equation (156), we conclude that

1
n

n
(cid:88)

i=1

(f −1( (cid:98)Ti ∨ f (η, (cid:98)σ), (cid:98)σ) − µi)2 (cid:46) δrn,2(µn, µ1, σ) + γ2n−1,

(159)

with probability at least 1 − δ−1 − 2γ−2.

49

E Proofs of Section 4

E.1 Mathematical Preliminaries

Since we adapt the lower bound construction from Bellec and Tsybakov [2015] for our ASCI
setting, we ﬁrst introduce the relevant related notation and deﬁnitions here ﬁrst for classes
of monotonic sequences. We denote S ↑ := (cid:8)µ := (µ1, . . . , µn)(cid:62) (cid:12)
(cid:9) to be the
set of all non-decreasing sequences. We deﬁne k(µ) ≥ 1, for µ ∈ S ↑, to be the inte-
ger such that k(u) − 1 is the number of inequalities µi ≤ µi+1 that are strict for i ∈
[n − 1] (i.e., number of jumps of µ). The class of monotone functions we will consider are
S ↑(V ∗) := (cid:8)µ ∈ S ↑ (cid:12)
(cid:12) V (µ) ≤ V ∗(cid:9), for some ﬁxed V ∗ ∈ R, and V (µ) = µn − µ1, is the
total variation of any µ ∈ S ↑. We also consider the restricted class of monotone sequences,
k∗ := (cid:8)µ ∈ S ↑ (cid:12)
i ≤ C, µ1 > η > 0(cid:9).
S ↑

(cid:12) k(µ) ≤ k∗(cid:9), and S ↑(V ∗, η, C) := (cid:8)µ ∈ S ↑(V ∗) (cid:12)
(cid:12) 1
n

(cid:12) µ1 ≤ . . . ≤ µn

i=1 µ2

(cid:80)n

E.2 Proof of Proposition 40

We follow directly the proof technique and construction from Bellec and Tsybakov [2015,
Proposition 4], but make suitable adaptations for our ASCI setup. We largely follow their
notation to help readers align the commonalities and diﬀerences in the underlying constructions
used. Our ﬁrst lower bound result is stated in Proposition 40.

Proposition 40 (Minimax lower bounds). Let n ≥ 2, V ∗ > 0 and σ > 0. There exist absolute
constants c, c(cid:48) > 0 such that for any positive integer k∗ ≤ n satisfying (k∗)3 ≤ 16n(V ∗)2
we
have

σ2

inf
ˆµ

sup
S↑
k∗ ∩ S↑(V ∗,η,C)

Prµ

(cid:18) 1
n

(cid:107) (cid:98)µ − µ(cid:107)2 ≥

(cid:19)

cσ2k
n

> c(cid:48)

(160)

(cid:113) σ2k∗
where η > 0 is a ﬁxed positive constant per Equation (5), C ≥ (V ∗)2+4γ2+2η2, γ := 1
n ,
8
Pµ denotes the distribution of (R1, . . . , Rn)(cid:62) satisfying Equation (4), and inf ˆµ is the inﬁmum
over all estimators.

Proof of Proposition 40. Let n be a multiple of k∗ ∈ N. Then for any ω, ω(cid:48) ∈ {0, 1}k∗, using
the Varshamov-Gilbert bound [Tsybakov, 2009, Lemma 2.9], there exists a set Ω ∈ {0, 1}k∗
such that:

0 = (0, . . . , 0)(cid:62) ∈ Ω,

log(|Ω| − 1) ≥

k∗
8

,

and DHAMM

(cid:0)ω, ω(cid:48)(cid:1) >

k∗
8

(161)

for any two distinct ω, ω(cid:48) ∈ Ω. For each ω ∈ Ω, deﬁne a vector uω ∈ Rn componentwise, for
each component index i ∈ [n] as follows:

(cid:5) V ∗

:=

(cid:4)(i − 1) k∗
n
2k∗

:= uω

i + η.

uω
i
¯uω
i

+ γω(cid:98)(i−1) k∗

n (cid:99)+1.

(162)

(163)

where γ := 1
8
we note that uω
i

(cid:113) σ2k∗
and η > 0 is a ﬁxed positive constant per Equation (5). Importantly
n
per Equation (162) is precisely as constructed in Bellec and Tsybakov [2015,

50

Proposition 4]. However, critically the construction in Equation (163) is adapted to our ASCI
setting, by componentwise translation by η > 0. More compactly, it is also convenient to
represent this construction as ¯uω := uω + η, where η := (η, . . . , η)(cid:62) ∈ Rn.

As per Bellec and Tsybakov [2015, Proposition 4] we ﬁrst note the following properties for
, for each i ∈ [n]. For any ω ∈ Ω, uω is a piecewise constant sequence with k (uω) ≤ k∗, uω
2k∗ , and by construction V (uω) ≤ V ∗. Thus,

uω
i
is a non-decreasing sequence because γ ≤ V ∗
uω ∈ S ↑

k∗ ∩ S ↑(V ) for all ω ∈ Ω.

Now we observe the following corresponding properties of the η-translated sequence ¯uω.
First note that since for any ω ∈ Ω, uω is a piecewise constant non-decreasing sequence, so is
, by translation invariance. Next, consider any arbitrary index j ∈ [n] relating to a ‘jump’
¯uω
j
in uω, i.e., uω

(note the strict inequality). We then have that:

j < uω

j+1

⇐⇒ uω

j < uω
uω
j + η < uω
j < ¯uω

⇐⇒ ¯uω

j+1

j+1
j+1 + η

(by assumption.)

(using Equation (163))

So any ‘jump’ in the original sequence uω corresponds to a jump in the η-translated sequence
¯uω. That is, we have k (¯uω) = k (uω) ≤ k∗. In addition, we note that

n − ¯uω
V (¯uω) = ¯uω
1
n + η) − (uω
= (uω
n − uω
= uω
1
= V (uω)
≤ V ∗

1 + η)

(by construction of uω.)

By construction we also have that ¯uω

1 ≥ 0 by construction
(in fact each component is non-negative). Finally, per our ASCI setting, we want to check if
i )2 ≤ C, for each n ∈ N. Given ¯uω we observe the
there exists a C > 0, such that 1
i=1 (¯uω
n
following for each component index i ∈ [n]:

1 + η ≥ η > 0, since uω

1 := uω

(cid:80)n

(¯uω

i )2 := (uω
(cid:16)
≤ 2

i + η)2
i )2 + η2(cid:17)
(uω

(cid:32) (cid:4)(i − 1) k
n
2k



(cid:5) V ∗

+ γω(cid:98)(i−1) k

n (cid:99)+1

(cid:33)2





+ η2

(using Equation (162))

(using Lemma 14)

(using Equation (162))

= 2



≤ 2

2


(cid:32) (cid:4)(i − 1) k
n

2k

(cid:33)2

(cid:5) V ∗

(cid:16)

+

γω(cid:98)(i−1) k

n (cid:99)+1

(cid:17)2


 + η2





(using Lemma 14)

(cid:32)
2

(cid:34)(cid:18) V ∗k
2k

(cid:19)2

≤ 2

(cid:35)

(cid:33)

+ γ2

+ η2

≤ (V ∗)2 + 4γ2 + 2η2

So indeed it follows from Equation (164) that:

(since i−1

n ≤ 1 for each i ∈ [n].)

(164)

(165)

1
n

n
(cid:88)

i=1

(¯uω

i )2 ≤ (V ∗)2 + 4γ2 + 2η2 =: C

51

So that we have ¯uω ∈ S ↑

k∗ ∩ S ↑(V ∗, η, C). Moreover, for any ω, ω(cid:48) ∈ Ω, we observe that:

(cid:12)
(cid:12)

(cid:12)¯uω − ¯uω(cid:48)(cid:12)

(cid:12)
(cid:12)

2

:=

+ η)

(cid:12)
2
(cid:12)
(cid:12)

(by construction ¯uω := uω + η.)

2
(cid:13)
(cid:13)
(cid:0)ω, ω(cid:48)(cid:1)

(cid:12)
(cid:12)(uω + η) − (uω(cid:48)
(cid:12)
(cid:13)uω − uω(cid:48)(cid:13)
(cid:13)
(cid:13)
γ2
k∗ DHAMM
γ2
8
σ2k∗
512n

=

=

≥

=

Set for brevity Pω = P¯uω . The Kullback-Leibler divergence DKL (Pω || Pω(cid:48)), between Pω and
for all ω, ω(cid:48) ∈ Ω. Thus,
Pω(cid:48), is equal to n
2σ2

(cid:13)uω − uω(cid:48)(cid:13)

2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:16)

DKL

Pω || P0

(cid:17)

=

γ2nDHAMM (0, ω)
2k∗σ2

≤

k∗
128

≤

log(|Ω| − 1)
16

(166)

Applying Tsybakov [2009, Theorem 2.7] with α = 1/16 completes the proof.

E.3 Proof of Proposition 10

From Proposition 40, in line with Bellec and Tsybakov [2015, Corollary 5], we immediately
obtain the following result in Proposition 10. Once again, we utilize the technique of Bellec
and Tsybakov [2015, Corollary 5] to obtain the following corollary. The important changes to
ensure that we adapt to our ASCI setting are captured in Proposition 40 and our proof thereof.

Proposition 10 (Minimax lower bounds). Let n ≥ 2, V ∗ > 0 and σ > 0, and deﬁne
(cid:101)rn,2(V ∗, σ) := max (cid:8) (cid:16) σ2V ∗

n }. Then, there exist absolute constants c, c(cid:48) > 0 such that:

3 , σ2

(cid:17) 2

n

inf
ˆµ

sup
S↑(V ∗, η, C)

Prµ

(cid:18) 1
n

(cid:19)
(cid:107) (cid:98)µ − µ(cid:107)2 ≥ c(cid:101)rn,2(V ∗, σ)

> c(cid:48)

(23)

of

(cid:17) 1

(cid:16) 16n(V ∗)2
σ2

Proof of Proposition 40. As per Bellec and Tsybakov [2015, Corollary 5], to prove this corollary
it is enough to note that if 16n(V ∗)2
≥ 1, by choosing k∗ in Proposition 40 as the integer part
(cid:16) σ2V ∗
n
is dominant, so that we need
σ2
, which is trivial (it follows from a reduction to the

Equation (23). On the other hand, if 16n(V ∗)2
to have the lower bound of the order σ2
n
bound for the class composed of two constant functions).

3 , we obtain the lower bound corresponding to

< 1 the term σ2
n

3 under the maximum in

(cid:17) 2

σ2

52

