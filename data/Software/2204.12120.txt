2
2
0
2

r
p
A
7
2

]
S
M

.
s
c
[

2
v
0
2
1
2
1
.
4
0
2
2
:
v
i
X
r
a

Automated Generation of High-Performance
Computational Fluid Dynamics Codes

Sandra Maci`aa,∗, Pedro J. Mart´ınez-Ferrera, Eduard Ayguad´ea, Vicen¸c
Beltrana

aBarcelona Supercomputing Center (BSC-CNS), Barcelona, Spain

Abstract

Domain-Speciﬁc Languages (DSLs) improve programmers productivity by de-
coupling problem descriptions from algorithmic implementations. However,
DSLs for High-Performance Computing (HPC) have two additional critical re-
quirements: performance and scalability. This paper presents the automated
process of generating, from abstract mathematical speciﬁcations of Computa-
tional Fluid Dynamics (CFD) problems, optimised parallel codes that perform
and scale as manually optimised ones. We consciously combine within Saiph,
a DSL for solving CFD problems, low-level optimisations and parallelisation
strategies, enabling high-performance single-core executions which eﬀectively
scale to multi-core and distributed environments. Our results demonstrate how
high-level DSLs can oﬀer competitive performance by transparently leveraging
state-of-the-art HPC techniques.

Keywords: Domain-Speciﬁc Languages, High-Performance Computing,
Computational Fluid Dynamics, Code Optimisation

1. Introduction

Scientiﬁc applications face the challenge of eﬃciently exploiting increasingly
complex parallel and distributed systems. Extracting high performance requires
deep expertise in parallel programming models, libraries and algorithms, and in-
depth knowledge of the target architecture. Hand-tuned codes are built under
this assumed knowledge, and therefore, able to provide both low-level optimised
and eﬃcient parallel implementations. Data structures, optimisations and par-
allelisation strategies are intertwined with the application code and exposed to
the compilers. Nevertheless, developing such codes is a time-consuming, te-
In this scenario, reaching high performance
dious and hardly reusable task.

∗Corresponding author
Email addresses: sandra.macia@bsc.es (Sandra Maci`a),

pedro.martinez-ferrer@bsc.es (Pedro J. Mart´ınez-Ferrer), eduard.ayguade@bsc.es
(Eduard Ayguad´e), vicenc.beltran@bsc.es (Vicen¸c Beltran)

 
 
 
 
 
 
appears detrimental to productivity and portability and unreasonable to expect
from scientists. Domain-Speciﬁc Languages (DSLs) have arisen as a separa-
tion of concerns approach through high-level abstraction layers to overcome
such diﬃculties. On the one hand, productivity and portability can be reached
by abstracting the application layer from the ﬁnal parallel low-level code. On
the other hand, since DSLs are restricted to speciﬁc problem domains, they
are built under assumptions, embodying domain knowledge that enables the
automated application of suitable computing methodologies. Hence, DSL can
transparently tackle performance at the set of algorithmic patterns they imple-
ment. However, this dissociation might blind the compilers and prevent using
state-of-the-art HPC techniques such as vectorisation and tiling or advanced
parallelisation strategies. Thus, pressure has moved to DSLs researchers ex-
pecting from their framework a high level of generality and abstraction while
delivering high-performance on par with hand-tuned codes.

This paper aims to enhance a DSL framework to automatically generate code
optimised to exploit an HPC cluster from high-level speciﬁcations. Saiph is a
DSL that targets the resolution of Computational Fluid Dynamics (CFD) prob-
lems through a high-level syntax and a generic numerical library implementing
explicit and Finite Diﬀerences Methods (FDM). We use Saiph [1, 2] as a DSL
platform and consequently focus on the CFD domain. However, the modular
design of the tool enables the present methodologies to be generalised for other
problem domains according to their computational needs. We proﬁt from the
layered design and adapt the build process for a new code generation, ensur-
ing the extraction and propagation of information from the input code to the
ﬁnal binary. From Saiph applications, we automate the combination of state-of-
the-art and advanced HPC techniques by generating an intermediate annotated
code exposing detailed information to the compiler. For that, we study such
techniques and determine (i) the information needed at the intermediate code
to enable it, (ii) how to extract and generate such information from a high-level
speciﬁcation and (iii) how to combine it with the rest of the techniques.

In particular, the combination of HPC techniques is a non-trivial task. Si-
multaneously vectorising, blocking, parallelising and distributing a loop requires
a harmonious combination ensuring aligned data access, good cache locality and
well-balanced domain partition at the same time. Our approach ensures the au-
tomated, generic and eﬀective combination of such techniques by coupling the
eﬃcient exploitation of three levels of HPC resources: single-core, multi-core
and cluster, in a bottom-up manner:

Single-core performance We target single-core performance by exploiting
parallelism and memory hierarchy through code-vectorisation and cache
locality enhancement using data-blocking mechanisms. To that end, we
explore suitable data layout, traversal and alignment, compiler hints and
compile-time evaluations.

Multi-core performance We inquire how to adapt the data-blocking for sub-
sequent appropriate shared-memory usage at the node level when exploit-
ing multi-core resources. We target diﬀerent shared-memory models such

2

as fork-join and tasking through OpenMP and OmpSs-2 [3] programming
models. Hence, we address multi-core parallelism for load imbalance or
data locality improvements. To the best of our knowledge, this work
presents an unprecedented approach for the automatic generation of task-
iﬁed codes within a DSL framework.

Distributed performance On top of it, we research how to extend distributed
executions based on MPI and scalable domain decomposition to preserve
the underlying optimisations. Moreover, combining distributed strate-
gies with multi-core parallel versions produces hybrid conﬁgurations with
diﬀerent interoperability characteristics. We study such hybrid conﬁgura-
tions using MPI and TAMPI [4] combined with fork-join and task models.

The paper makes the following contributions:

1. A DSL build process and code generation ensuring automated code anno-

tations and optimisations.

2. An eﬀective automated combination of state-of-the-art HPC techniques

and parallelisation strategies at single/multi-core and cluster levels.

2. Background

2.1. Computational Fluid Dynamics

CFD is a physics domain to solve problems related to ﬂuid ﬂows numeri-
cally. The governing equations modelling density-based CFD problems are the
Navier-Stokes equations and the equation of state. The former is a set of space-
time dependent Partial Diﬀerential Equations (PDEs) describing the motion of
a ﬂuid; the latter corresponds to a non-time derivative equation relating the
ﬂuid’s state variables. Depending on the equations’ unknowns, we incorpo-
rate other thermodynamic relations between state variables to close the system.
To unambiguously deﬁne a CFD problem, we state the equations’ system, the
space-time modelling scope and the initial and boundary ﬂuid conditions (ICs,
BCs). We use space-time discretisation methods for CFD system resolution.

2.1.1. Explicit and Finite Diﬀerence Method (FDM)

Explicit FDM deﬁnes a closed set of computation patterns involving ﬁnite
diﬀerence discretisations and explicit time solvers to solve a wide range of CFD
problems. FDM discretises a spatial domain mapping continuous ﬁeld informa-
tion into a Cartesian grid of points; points store ﬁeld values according to spa-
tial coordinates. Explicit methods discretise the temporal dimension through a
time-stepping loop at which each iteration deﬁnes the state of a system using
previous time-step states. In such methods, PDEs are approximated by alge-
braic equations at each spatial coordinate at each time-step. Computationally,
a time-step loop encloses a spatial traversal at which linear algebra and stencil
computations occur.

3

2.1.2. Challenges of HPC explicit FDM

Explicit FDM can beneﬁt from diﬀerent levels of optimisations to fully ex-

ploit HPC systems.

Single-core level. Algebraic operations and stencils calculations can beneﬁt from
basic compiler optimisations and code-vectorisation if the compiler has enough
information and the data structures are adequately aligned and traversed. More-
over, stencil calculations access neighbouring mesh point values from previous
time-iterations, so data-blocking mechanisms enabling data reuse can improve
data locality at the memory hierarchy.

Multi-core level. Within explicit FDM time-steps, new values are computed
from previous time iterations ones. Hence, at each time step, the absence of
data dependencies ensures the spatial loop’s embarrassing parallelism. Such
patterns can beneﬁt from shared-memory parallelism through parallel program-
ming model annotations. Explicit FDM codes must ensure balanced work parti-
tion and minimum synchronisation overheads. Moreover, the neighbouring data
dependencies across time-steps restrict the time loop parallelism, but the end
of each time step is not globally synchronised; distant spatial regions can proﬁt
from asynchronous parallel time progress.

Cluster level. The computation domain decomposition approach within explicit
FDM corresponds to a spatial domain distribution across available nodes. Thus,
each node is in charge of a portion of the initial domain, and data redundancy
and message passing mechanism manage dependencies. Such distribution must
be scalable and preserve and combine with lower-level optimisations.

In this paper, we consider all previous points together and detail their ap-

plication and combination.

2.2. Saiph

Saiph is a DSL easing the simulation of physical phenomena from the CFD
domain in HPC environments. Users specify CFD problems through high-level
constructs deﬁning systems of PDEs with time-space conﬁgurations. Mathemat-
ical speciﬁcations, Saiph codes, evaluations and output results can be accessed
online [5].

The DSL is embedded in Scala [6] and comprises two main divisions: the
Scala and the C++ layer. The Scala layer deﬁnes the language syntax; the C++
layer implements numerical methods and parallelisation strategies on separated
libraries forming the modules. Figure 1 illustrates the macroscopic compilation
ﬂow.
Input code is ﬁrst compiled to be parsed by the Scala layer producing
a C++ intermediate code. The C++ code is then linked to the C++ library,
and a second compilation produces the ﬁnal parallel binary. Because of this
dissociation, discretisation methods and algebraic kernels are generic enough to
support diﬀerent input problem conﬁgurations. Moreover, at the ﬁrst compila-
tion step, selecting a module determines the intermediate C++ generated code
with function calls to the matching library.

4

Figure 1: Saiph layered abstraction

At the Saiph intermediate output, PDEs are represented by equation trees
modelled as abstract graphs. Vertices correspond to mathematical operators
that relate the ﬂuid ﬁelds designated by leaves. At run-time, the numerical
library traverses these graphs for each spatial coordinate at each time-step,
applying basic parallelisation strategies. However, single-core performance is
far from optimal because graph data structures blind the compiler preventing
optimisations. This design oﬀers high productivity and extensibility but lim-
its Saiph from being a competitive HPC tool. This paper enhances Saiph to
leverage the speciﬁc domain knowledge and propagate it through the diﬀerent
compilation phases to achieve competitive and scalable performance. We focus
on generating a speciﬁc, detailed and optimisable spatial loop from the Scala
layer for the input applications within the explicit FDM modules. Moreover, we
implement advanced parallelisation strategies at the underlying C++ libraries,
embracing and boosting the spatial loop execution, where computations and
memory accesses happen iteratively.

3. New Design and implementation

We present a new design to extract and propagate information from the
high-level code to the underlying layers. Based on this information, we develop,
adopt and combine diﬀerent numerical parallel strategies. As stated in the
previous section, 2.1.1, the FDM-CFD algorithmic patterns occur within the
spatial loop, at the mesh traversal. To get a signiﬁcant beneﬁt in performance,
we focus on optimising such a loop.

3.1. Code generation

We present a generic build process ensuring productivity while enabling high-
performance. For an eﬃcient equation resolution, we remove the spatial loop

5

construct from the C++ library to generate it from the Scala layer encapsulating
the generated code in a lambda function. Figure 2 shows how we generate a
C++ lambda function from a high-level speciﬁcation of a PDE. From the input
heat equation code, we traverse the equation tree at the Scala layer and generate,
at the loop body, the corresponding method call for each vertex. Computations
and memory allocations and accesses remain at the C++ library, guaranteeing
their unique, eﬃcient implementation.

Figure 2: Saiph equation speciﬁcation, internal representation and code generation

To minimise mesh traversals, we group the resolution of the equations into
the same spatial loop. Still, the equation ﬁnal update depends on the equa-
tion’s nature determined by the left-hand side expression. We generate two
independent lambda functions grouping the spatial resolution of time-derivative
and non-time derivative equations, respectively. For each loop body, we enable
domain-speciﬁc optimisations, such as the common subexpression elimination
(CSE) at the Scala layer through the Lightweight Modular Staging (LMS) [7],
allowing partial results to be reused even for diﬀerent equations. Figure 3 ex-
empliﬁes this reuse for two equations grouped into the same lambda; tmp4 cor-
responds to the partial result of the common highlighted subexpression, so it is
used at both equation updates.

Figure 4 illustrates the overall execution workﬂow. Once the Scala layer
outputs the C++ generated code from the input one, we use the lambdas as
arguments of new setters methods from the C++ library, stating the resolution
function attributes. Once these attributes are deﬁned, we call them from the
library within the integration methods’ time loop. The lambdas executed at
each time-step act as links between layers: from the generated code scope their
capture the references of the ﬂuid ﬁelds deﬁned at the input code and call the
C++ library methods that state the calculations over them.

Since we generate speciﬁc spatial loops automatically, we can further detail

and annotate them to obtain the desired optimisation level.

3.2. Exploiting low-level optimisations

6

Equation(dt(T), k*lapla(T))Saiph Scala Layerλ functionEquation Syntax[Saiph specification]Equation Tree [Internal representation]Heat Equation[Mathematical specification]for(i = initMesh ￫ endMesh){  // laplacian vertex lapla()  tmp0 = lapla(T, i);  // variable access vertex k  tmp1 = var(k, i);  // times vertex ()*()  tmp2 = times(tmp0, tmp1);  // Eq update  updatePDE(T, i, tmp2);}lapla()Tdt()EqC++ generated code()*()kTFigure 3: Saiph partial results reuse at the equations code generation

3.2.1. Code eﬃciency

To beneﬁt from compiler optimisations, we want detailed programs using
simple structures and constant parameters. The extraction of semantic in-
formation from the input code enables compile-time evaluations leading to an
equivalent, more eﬃcient program. Linear algebra and stencil computations can
enormously beneﬁt from such transformations. Within the spatial loop, Saiph
equation resolution happens through function calls. At the C++ layer, those
functions perform variable accesses, algebraic operations, stencil computations
and integration updates. We implement them to encapsulate memory accesses
and basic mathematical operations coded through simple structures involving
conditional branches and few-iterations loops over problem variables. Moreover,
we deﬁne them as static inline functions, so these small, recurrent-used

7

 for(i = initMesh ￫ endMesh){   // variable access vertex u   tmp1 = var(u,i);   // variable access vertex rho_u   tmp2 = var(rho_u,i);   // times vertex ()*()   tmp3 = times(tmp1,tmp2);   // times vertex ()*()   tmp4 = times(0.5,tmp3);   // subtract vertex ()-()   tmp5 = subtract(rho_e,tmp4);           …   // addition vertex ()+()   tmp14 = add(tmp13,tmp4);   // Eqs updates   update(p,i,tmp8);   update(rho_e,i,tmp14); } p()*()Total energy relation[Mathematical specification]Reuse of partial resultλ functionrho_e()-()lambda1()-()()*()()*()0.5rho_uuEq()+()rho_e()*()rho()*()()*()0.5rho_uu()*()cvTSaiph Scala LayerEquation of state[Mathematical specification]EqEquation tree[Internal representation]Equation tree[Internal representation]C++ generated codeCommon subexpressionFigure 4: Saiph resolution workﬂow

kernels avoid being called and escape the associated overhead. We also generate
the clause #pragma forceinline recursive right before the spatial loop, at
the lambda function, to ensure the inline. Consequently, we enable context-
speciﬁc optimisations on the body of the inline functions. By deﬁning control
variables such as mesh dimensions, operands dimensions and stencil neighbour-
ing as literals, const, or constexpr, we expose them, and compile-time evalu-
ation automatically applies. To illustrate the internal implementation of such
functions, Listing 1 presents the function var, for accessing a variable at a par-
ticular spatial position and returning its value. For that, the function receives
four parameters: the pointer varBuff pointing the buﬀer at which the variable
is stored, the integer idx specifying the spatial position to access, the integer
varDims stating the dimensions of the variable and the pointer res indicating
where to store the result. Then, the var function involves a loop that performs
the desired variable access that can be unrolled if the variable’s dimensions,

8

varDims, are known at compile-time.

static inline real t∗ var(real t∗ varBuﬀ, int idx, int varDims, real t∗ res) {

for(int i = 0; i < varDims; ++i)

res[i] = varBuﬀ[idx∗varDims + i];

return res; }

Listing 1: C++ variable access kernel

Similarly, spatial derivative functions can be optimised since they iterate over
mesh dimensions and stencil accuracy control variables, potentially derived at
compile-time. Hence, we want the Scala layer to generate an intermediate code
with information over the control variables to beneﬁt from compiler optimisa-
tions. Mesh dimensions, ﬁelds dimensions and stencil accuracy are generated
as literals by retrieving user code’s information. We extract the information
for other kernels’ control variables when traversing the equation tree. At the
bottom-up graph traversal, from the Scala layer, we calculate speciﬁc semantic
information for each vertex from leaves’ known dimensions, depending on the
operator nature and children’s dimensions. We enhance the Scala layer to char-
acterise graphs and generate kernel calls with evaluable control variables. Figure
5 shows the tree characterisation of the heat equation and the speciﬁc gener-
ated lapla, var and times function calls for computing a laplacian, accessing a
scalar problem ﬁeld and performing a product operation, respectively. As we
illustrate in the ﬁgure, we generate the operand dimensions’ evaluation before
the operator call and allocate the temporal memory to store the partial result.
Thus, the calls to the kernels are speciﬁc and determined at compile-time to be
automatically optimised by the compiler.

3.2.2. Micro-architecture use

As hardware moves forward to boost executions, we adapt code to the ar-

chitecture design to well-exploit resources and core pipeline.

Vectorisation. Due to its embarrassing parallel nature, the generated spatial
loop can beneﬁt from auto-vectorisation [8] through hardware support (SIMD
length of vector units) and compiler loop dependence analysis. Still, compilers
need assistance in applying this optimisation: we leverage data alignment at the
C++ library level for each ﬁeld buﬀer by transparently allocating memory at
an address multiple of parameter alignment set to the cache line size, optimal
for memory movements. Apart from base pointer alignment, vectorisation relies
on known and aligned accesses. Within Saiph FDM modules, stencil compu-
tations represent the principal cause of memory access. Applying a stencil at
a speciﬁc mesh element (i, j, k) implies accessing its neighbours. For the
ﬁrst dimension, nX, the stencil involves contiguous memory accesses; the other
dimensions neighbouring have non-unit strides, computed from the (i, j, k)
element address plus an oﬀset multiple of nX. To maximise aligned accesses, we
apply a padding strategy; nX adds extra points to force its size to be multiple of
memory cache lines. Thus, each data row perfectly ﬁts into several cache lines

9

Figure 5: Saiph equation characterisation and vertex operator generated calls

ensuring contiguous or aligned stencil access. Moreover, cache lines size is mul-
tiple of the vector units instruction size (set through vectorSize parameter) so,
as long as the starting index of the spatial loop matches the start of a data row,
the spatial loop is vectorised without peeled or remainder loops. Figure 6 shows
this organisation of data into memory and the automated aligned access pattern
for a ﬁrst-order stencil computation in a 2D spatial domain. The neighbouring
access of the (i, j) element is contiguous for the ﬁrst dimension and aligned
for the second. The padding strategy enables aligned and vectorised memory
accesses.

To hint the compiler about data alignment, we automatically emit spe-
ciﬁc clauses at the intermediate code. We declare ﬁeld buﬀer pointers with
builtin assume aligned attributes for Intel or GNU,
respectively. We mark nX and the lower bound of the spatial loop as multiples
assume(nX%alignment==0) for Intel, nX & -alignment for
of alignment:

assume aligned or

10

lapla()[tmp1size = 1]dt()[Tsize]Eq()*()[tmp3size = max(tmp1size,tmp2size)]T[tmp0size = Tsize]T[Tsize]λ functionfor(idx = initMesh ￫ endMesh) {  // laplacian vertex lapla()  constexpr int tmp1size = 1;  real_t tmp1mem[tmp1size];  auto tmp1 = lapla(T,idx,Tsize,tmp1mem);  // variable access vertex k  constexpr int tmp2size = ksize;  real_t tmp2mem[tmp2size];  auto tmp2 = var(k, idx, ksize, tmp2mem);  // times vertex ()*()  constexpr int tmp3size = max(tmp1size, tmp2size);  real_t tmp3mem[tmp3size];  auto tmp3 = times(tmp1, tmp2, tmp1size,                           tmp2size, tmp3mem);  ...  updatePDE(T, idx, tmp3);}C++ Generated codeSaiph Scala Layerk[tmp2size = ksize]Figure 6: Saiph memory access pattern for a ﬁrst order stencil computation

GNU, and we add the portable pragma omp simd right before the loop. Sim-
ilarly, we use the clause omp declare simd at library kernels to enable their
SIMD versions.

Spatial blocking. Stencil data-locality improves if implemented by means of data
chunks; memory pieces ﬁtting into the cache optimise their reuse [9, 10]. For
that, we add blocking strategies at diﬀerent levels of the DSL. At the C++
library, once the mesh is discretised, we compute the number of local blocks
nbli per mesh dimension i through iterative cuts based on the L3size param-
eter (L3 cache size). We adopt a 2.5D blocking technique [11, 12]: blocking
non-contiguous dimensions while streaming the computations over the ﬁrst one
(nblX = 1, nblY (cid:54)= 1, nblZ (cid:54)= 1). The ﬁrst dimension counts on the padding
points, so the blocked spatial traversal maintains the innermost loop well-
conditioned for vectorisation. Once the number of blocks per dimension is set,
we automatically derive the block sizes bsi parameters. At the Scala layer, we
generate a code with a generic spatial loop skeleton able to apply the blocking
decisions that will take place at the C++ library. Listing 2 shows this generated
code with the nested loops that enable the parametrised blocked spatial traver-
sal; the three outer loops traverse the nblZ ∗ nblY ∗ nblX blocks while the three
inner loops traverse each block iteration space bsZ ∗ bsY ∗ bsX. Preceding the
loop, the generated code includes library method calls stating the loop bounds
(nblX, nblY , nblZ, bsX, bsY , bsZ).

3.3. Exploiting multi-core parallelism
3.3.1. Fork-join model

We transparently obtain a parallel code by generating the OpenMP clause
#pragma omp parallel for collapse(3) at the intermediate code, right be-
fore the embarrassingly parallel spatial loop. The clause envelops the loop so

11

that blocks are distributed among threads, each of them starting at a vectorisa-
tion beneﬁcial aligned index. Threads share the local memory, so we adapt the
blocking decisions; we increment the number of blocks nbli to have, at least, as
many blocks as working threads (parameter nThreads) ﬁtting simultaneously
into the cache. In such a way, there is enough parallel work to feed nThreads
simultaneously while the blocks they compute in parallel ﬁt into the L3 cache,
occupying less than L3size, enabling good locality.

3.3.2. Task model

This paradigm represents an appealing approach for HPC-CFD problems
[13, 14]. The model requires annotating the code with task constructs enclosing
pieces of code that will be asynchronously executed in parallel; we specify tasks
data-dependencies to ensure correct execution order. We develop two new DSL
modules using OpenMP and OmpSs-2 [3] programming models, respectively.
In both, we generate an annotated intermediate code creating tasks that en-
velop spatial block updates. This code generation happening at the Scala layer
precedes the allocation of problems ﬁelds. Thus, to state task dependencies
over the not-yet-allocated buﬀers, we use additional arrays whose elements are
sentinels representing blocks. At the C++ numerical library, we deﬁne such
structure, shown in Figure 7, as a multi-dimensional array of chars, with as
many dimensions as the input mesh and as many elements as the number of
blocks per dimension nbli plus two. Adding two elements per dimension allows
us to specify stencil dependencies generically without worrying about boundary
block cases.

Figure 7: Block mesh abstraction for the 2.5D blocking technique

We handle two of such structures mapping the source (read) and the destina-
tion (write) buﬀers, respectively, to state the time dependencies. Finally, we use
the pointers to the structures as lambda functions’ arguments to enable their
use at the spatial loop. Listing 2 shows the annotated loop generated within the
OmpSs2-module; the skeleton of the spatial nested loops remains generic and

12

3D input meshnXnY(nblY * bsY)nZ (nblZ * bsZ)3D sentinels structure(nX, nY, nZ)(1, nblY+2, nblZ+2)bsZbsYAbstract mapping between mesh blocks and rep elements1nblZ+2nblY+2we automatically generate a pragma for the creation of tasks just before each
block traversal.

for(int zb = 0; zb < nblZ; ++zb) {

for(int yb = 0; yb < nblY; ++yb) {

for(int xb = 0; xb < nblX; ++xb) {

#pragma oss task in(repSRC[xb][yb][zb]) in(repSRC[xb+1][yb][zb])

... out(repDST[xb][yb][zb])

for(int z = 0; z < bsZ; ++z) {

for(int y = 0; y < bsY; ++y) {
#pragma forceinline recursive
#pragma omp simd
for(int x = 0; x < bsX; ++x) {

Listing 2: Generated spatial loop skeleton with OmpSs-2 annotations

Tasks from the same time-step can be executed in parallel; hence, we adjust
the blocking decisions to have at least nThreads blocks ﬁtting simultaneously
into the cache. In such a way, the memory requirements from the tasks executed
in parallel do not exceed the cache size, maintaining a good memory locality.
Block updates correspond to computational tasks, labelled as A-tasks that sat-
isfy the relation A-tasks= nblX ∗ nblY ∗ nblZ ∗ niter where niter is the number
of time integration iterations of the simulation. Moreover, taskifying the spatial
loop requires protecting or applying the equivalent tasking strategy to other
computations at the exact buﬀer locations. Thus, we adapt BCs computations
to happen within tasks, labelled as B-tasks, at the same spatial blocks and use
abstracted dependencies to relate them to the ones from the lambda function.
We exploit the hidden temporal parallelism allowing asynchronous block time
progress. Figure 8 shows the Saiph task scheme for a generic block update. The
task A depends on previous B-tasks over neighbouring blocks. Once executed,
the corresponding B-task is ready for execution.

Figure 8: Explicit FDM task scheme for a 2D block update

3.4. Exploiting distributed parallelism

We transparently apply a domain decomposition approach at the Saiph C++
library to exploit distributed parallelism [15, 16]. We partition the mesh to dis-
tribute a similar workload across the available MPI ranks. For a highly scalable

13

distribution, we enable cuts in every dimension so to create as many mesh por-
tions as parameter nMPI which is the number of MPI ranks involved in the
execution. Naming nbgi as the number of blocks of the global mesh per i di-
mension, and nM P I the number of MPI ranks involved in the execution, our
domain decomposition satisﬁes nbgX ∗ nbgY ∗ nbgZ = nM pi. Hence, the num-
ber of cuts per dimension results from the factorisation of the nM P I number.
We use the MPI Dims create function [17] to automate the computation of pa-
rameters nbgi. Alternatively, we can choose the proportions of the distributed
blocks by manually setting the nbgi parameters.

After each MPI rank receives its domain portion, we manage dependencies
through data redundancy and message passing mechanisms. Figure 9 schema-
tises a local mesh portion of a 3D distributed mesh; local meshes are formed by
mesh points partition and halos, which correspond to the replicated informa-
tion from neighbours’ local meshes. We ensure a vectorisable contiguous spatial
traversal while minimising redundant computations by traversing ﬁrst dimen-
sion halos while avoiding last dimension halos updates. Moreover, to adjust to
the single/multi-core automated optimisations, we apply padding to the ﬁrst
contiguous dimension of each mesh portion.

We implement the communication of redundant data, or halos, to neigh-
bouring processes at the end of each step. Data is then automatically updated
and correctly accessed at future computational steps. We use the standard Mes-
sage Passing Interface (MPI) library for communications. Each MPI rank sends
its updated frontier mesh points and receives halos boundaries from neighbour
MPIs. When distributing the mesh, we prioritise partitioning the last dimension
to favour contiguous communication. When cutting in more than one dimen-
sion, we apply gathering and scattering techniques for the non-contiguous halos
messages.

Figure 9: Distributed local 3D mesh with padding technique

3.5. Exploiting hybrid parallelism

We mix multi-core and distributed parallelism within Saiph hybrid mod-
ules, combining fork-join/task model and message passing libraries to explore

14

HalosZHalosZHalosXHalosX + paddingXHalosYendLocal halosLocal mesh pointsSpatial loop boundinitHalosYdiﬀerent levels of parallelism simultaneously. We consciously combine them to
preserve previous optimisations and obtain eﬃcient conﬁgurations.

For that, we apply the domain decomposition from Section 3.4 and dis-
tribute the mesh among the available MPI ranks. Locally, we group parallel
computations into spatial blocks enabling the multi-core parallelism from Sec-
tion 3.3. Moreover, blocking and padding strategies from Section 3.2.2 provide
good memory usage and vectorisation. Similarly to computations, we group
boundary communication using the spatial block structure. Figure 10 illustrates
such adjusted techniques over a distributed local mesh.

Figure 10: Distributed local 3D mesh combining halos, 2.5D blocking and padding techniques

Combining fork-join and domain distribution does not imply interoperability
issues; we add a communication phase at the end of each time-step, where we
exchange halos. In contrast, combining tasks and domain partition require pro-
tecting communications using barriers or encapsulating them inside other tasks.
Similarly to mesh updates, we use the block structure from Figure 10 to encap-
sulate the halos exchange into C-tasks. We internally state C-tasks dependen-
cies, for a correct execution order, through the same sentinels’ structures from
Figure 7. We use the Task-Aware MPI Library (TAMPI) [4] to favour the inter-
operability between MPI and OpenMP/OmpSs-2 by eﬃciently executing MPI
operations from inside tasks constructs: the non-blocking TAMPI model avoids
barriers and the underuse of computational resources while allowing the safe
progress of the program execution. Thus, computation A/B and communica-
tion C tasks are asynchronously executed in parallel whenever the dependencies
are satisﬁed. Figure 11 shows the task ﬂow of a hybrid taskiﬁed time-step.

Mesh global and local blocking parameters (nbgX, nbgY , nbgZ) and (nblX,
nblY , nblZ) are automatically derived by Saiph to balance computations among
the available resources nM P I and nT hreads. However, the default parameters
can be modiﬁed, allowing the exploration of diﬀerent computational granulari-
ties. Regarding communication, we allow to group boundary block communica-
tion within the same C-task to proﬁt from established connections and thus, also
enable communication granularity exploration, through parameters (nCommX,
nCommY , nCommZ).

15

Local halosLocal mesh pointsSpatial loop boundSpatial blockbsYbsZnXnZnYendinitHalosXHalosX + paddingXHalosZHalosZHalosYHalosYnblX = 1nblY = 4nblZ = 3Figure 11: Explicit FDM distributed task scheme for a 2D block update

4. Evaluation

In this section we evaluate the quality of the code generated by Saiph and
the parallelisation strategies implemented. On section 4.1 we describe the con-
textual framework for such evaluation. Then, sections 4.2, 4.3 and 4.4 conduct
the Saiph performance evaluation at single-core, multi-core and cluster levels
respectively.

4.1. Methodology
4.1.1. Saiph apps

We use several Saiph applications [2] to evaluate our contributions. These
applications are open-source [5], and they include a reference output to vali-
date their correctness. Table 1 details the applications used and their memory
accesses and operations required to update a mesh point. Each application
involves a ﬁxed number of stencil operations, ops, computed with a certain
spatial accuracy, acc. This spatial accuracy is a Saiph parameter stated by
users to determine the numerical precision of stencils. Computationally, the
accuracy determines the number of neighbour points involved per stencil, which
determines the computational intensity of the application.

Table 1: Saiph apps speedups and details per mesh point update depending on stencil opera-
tions and accuracy (ops/acc)

App

Mesh points &
Mem. use (MB)

Mem access/point
x+ops(acc+1)

FLOP/point
y+ops(2( acc+ 1)+1)

Saiph-lambda
vs tree-traversal

1DSineWave
2DSmithHutton
2DInviscidVortex
3DHeat

100001 - 1.5
2001*1001 - 61
1601*1601 - 215
301*201*201 - 185

13+1(acc+1)
24+4(acc+1)
69+12(acc+1)
13+3(acc+1)

2+1(2(acc+1)+1)
8+4(2(acc+1)+1)
25+12(2(acc+1)+1)
2+3(2(acc+1)+1)

3,9x
13,9x
13,8x
4,5x

4.1.2. Hand-tuned codes

We develop hand-tuned codes for each of the Saiph applications tested. Such
codes explicitly implement the targeted single-core optimisations that Saiph

16

automatically applies. We validate the results produced by the optimised hand-
tuned codes and we use them as baselines to appraise the performance of Saiph
automatic optimisations.

4.1.3. Yask kernels

Yask [18] is a state-of-the-art framework for the exploration of the HPC
stencil-performance design space. The tool provides automatic stencil optimisa-
tions, including cache blocking, vector folding and vectorisation from high-level
stencil kernels speciﬁcations and generates, from the user code, parallel pro-
grams using multiple cores through OpenMP threads. Yask and Saiph tackle
diﬀerent domains and levels of abstractions but can be compared regarding the
quality of the code generated. To conduct such a comparison, we develop a
3D Heat equation kernel within Yask. For that, we use Yask version 3.05.06
[19] and compile the kernel using diﬀerent compilers with default optimisation
parameters.

4.1.4. Tests

All applications use double-precision ﬂoating-point formats. We ﬁrstly ad-
dress, on Section 4.2, the single-core absolute performance of Saiph generated
code using diﬀerent native compilers and compare such results against manu-
ally optimised codes and literature results. Then, on Sections 4.3 and 4.4, we
explore parallelisation strategies through scaling performance studies tackling
the three levels of HPC resources.

This evaluation procedure aims to validate the proposed collection of optimi-
sations and the strategies to apply and combine them automatically to demon-
strate the DSL scalability and high performance. Quantifying productivity and
comparing Saiph to other CFD tools is an extremely challenging task because
each framework targets a diﬀerent level of abstraction and provides diﬀerent
optimisations. This comprehensive evaluation escapes the scope of the paper,
and we contemplate it as future work.

4.1.5. Hardware

We run the applications on BSC’s Marenostrum 4 supercomputer [20]. Com-
pute nodes are equipped with two sockets, Intel Xeon Platinum 8160 CPU with
24 cores each, supporting vectorisation instructions up to AVX-512 and sharing
an L3 cache of 33MB. The experiments in Section 4.3 are limit to single socket
to avoid NUMA issues. For the same reason, the hybrid MPI+OmpSs-2 experi-
ments described on Section 4.4 use one MPI rank per socket. In this case, the 24
cores assigned to each rank are exploited through OpenMP/OmpSs-2 threads.

4.1.6. Parametric runs

Saiph runs are deﬁned by the set of parameters (alignment, vectorSize,
L3size, nThreads, nMPI, commBlocks). The ﬁrsts are hardware-dependent, and
we set them to 64 bytes (L3 lines size), 8 (double-precision ﬂoating-points ﬁtting
on AVX-512) and 33MB, respectively. Regarding computing resources, we select

17

nThreads and nMPI according to the execution test. Finally, the commBlocks
parameter allows exploring performance enhancement at hybrid TAMPI and
OmpSs-2 runs. We empirically set it to 4. Once set, parameters automatically
combine and determine the code transformations that lead to speciﬁc optimised
parallel code.

4.1.7. Compilers

Table 2 displays the compiler versions and corresponding ﬂags used to com-
pile Saiph, Yask and the hand-tuned codes. All the codes involving the OmpSs-2
programming model have been compiled using LLVM; for other codes, if nothing
is speciﬁed, binaries are built from Intel.

Table 2: Saiph intermediate compilers and ﬂags

Compiler-Versions Flags

ICC - 2020.1
icpc

GCC - 9.2.0
g++

LLVM- 13.0.0
ompss-2[21]
clang++

-O3 -qopenmp -qopenmp-simd
-inline-forceinline
-xCORE-AVX512
-qopt-zmm-usage=high

-O3 -fopenmp -fopenmp-simd
--forceinline -ftree-vectorize
-march=skylake-avx512

GCC ﬂags
[+] -fompss-2

4.2. Single-Core Performance

We test the performance of spatial loops for the applications of Table 1 using
a default stencil accuracy of 2. At this level, we choose the small problem sizes
from Table 1 to stress vector units and assert the generated code’s eﬀectiveness.

4.2.1. Saiph-lambda vs Saiph-tree-traversal

We start comparing the new method to solve Saiph’s equations based on
lambdas’ generated at compile-time against the original method based on travers-
ing equation trees at runtime. Results are presented in the last column of Ta-
ble 1. Saiph based on lambdas outperforms old implementations [1, 2] by a factor
of 4x to 14x depending on the length of the equation trees since the new im-
plementation avoid traversing it at each time step. This performance increase
comes from the fact that lambda functions generated at compile-time enable
native compiler optimisations that cannot be applied when tree-traversals take
place at runtime.

18

4.2.2. Hand-tuned code vs Yask kernel

To evaluate the quality of our hand-tuned codes introduced in section 4.1.2,
we compare the single-core performance of the 3D Heat equation application
manually implemented against Yask results. The application mainly involves
computing a second-order stencil, which can be easily speciﬁed using Yask.
Moreover, stencils represent the most challenging patterns to optimise within
Saiph. Thus, by proving the high quality of stencil computations, we demon-
strate the overall quality of the explicit FDMs for CFD. We confront the 3D Heat
equation hand-tuned code against the corresponding Yask kernel, using diﬀerent
compilers. Table 3 show the performance results of the diﬀerent implementa-
tions of the 3D Heat equation running on a single-core. Results are reported
using memory bandwidth, GFLOPS and Mpoints/s as absolute performance
metrics. While the ﬁrst two are implementation and optimisation dependent,
Mpoints/s is based on the problem size, a user-ﬁxed parameter. Hence, we focus
on this last metric for a more fair comparison across implementations.

Table 3: Single-core performance comparison between diﬀerent implementations of the Heat3D
application

Compiler

Heat3D app
implementation

GB/s GFLOPS Mpoints/s

icpc

g++

clang++

Hand-tuned
Yask
Saiph

Hand-tuned
Yask
Saiph

Hand-tuned
Yask
Saiph

99.01
61.41
96.43

32.79
24.25
27.82

97.09
58.92
98.14

12.87
9.92
12.54

4.26
3.90
3.61

12.62
9.49
12.76

574
524
559

190
230
161

562
586
569

Hand-tuned and Yask implementations deliver comparable results between
each other and when using Intel and LLVM compilers. However, when using the
Gnu C++ compiler, the Yask kernel runs about 56% slower than the same kernel
built with the Intel C++ compiler. This performance drop is higher for the
hand-tuned implementation, which shows a 67% of performance drop when using
g++. Overall, those results demonstrate that our manually developed codes
successfully encode optimisations beneﬁting explicit FDMs patterns. Moreover,
the compiler choice determines the performance of the ﬁnal binary.

4.2.3. Saiph vs hand-tuned codes

We appraise Saiph automated optimisations from implementations in Sec-
tion 3.2 against our manually optimised codes versions, using diﬀerent native
compilers. Figure 12 shows normalised results, taking the most performant run
as the baseline for Intel, GNU and LLVM compilers. At each barplot, the ﬁrst set

19

Figure 12: Normalised performance of single-core CFD spatial loops executions

of bars corresponds to hand-tuned loop bodies. The second set, Saiph-lambda,
refers to the new implementation based on the lambdas, described above, which
do not include any additional optimisation. We use this version as the base-
line for the last three code sets, incrementally enabling compile-time evaluations,
vectorisation and blocking. For icpc, the most optimised Saiph version provides
28x, 25x, 28x, and 26x of performance increase compared to the Saiph-lambda
results and 0.95x, 1.2x, 1.1x, and 0.98x against hand-tuned codes. The use of
other compilers shows similar results. Although the g++ compiler presents lower
performance results overall, Saiph optimised codes reach or surpass hand-tuned
ones’ performance. Such results illustrate how compilers can produce a more
eﬃcient binary when optimising the regular loop body structures from the Saiph
generated code than when receiving our hand-tuned constructs. Saiph gener-
ated codes appear to be more optimisation-friendly for some applications and
compilers than hand-tuned codes developed as human programmers. Moreover,
binaries built from g++ and clang++ show how vectorisation can be enabled
In such cases, we see a
without clauses, happening at +compOpts versions.
slight performance drop when using clauses: compiler transformations can pro-
duce diﬀerent codes depending on the order they are applied so that default
automated decisions can be preferable. However, beneﬁts, when such transfor-
mation is not guaranteed, justify the use of clauses. Finally, multi-dimensional
blocking shows low impact because substantial concurrency is necessary to push
the limits of the memory system [22]. However, memory pressure at the cache
level will appear for memory demanding cases under shared-memory parallelism.

4.2.4. Saiph vs Yask

Finally, we compare Saiph single-core optimised code against Yask results.
For that, we use the 3D Heat equation application and the fully automatically
optimised Saiph version. Table 3 adds Saiph performance results along with the
already evaluated hand-tuned and Yask ones. Numbers illustrate how Saiph
single-core performance is comparable with Yask results. Using the GNU C++
compiler, Saiph suﬀers from a higher drop in performance than Yask, compared
to Intel results. Binaries built from other compilers show competitive perfor-

20

01HandTunedSaiph-lambda+compOpts+vectClauses+blockingNormalizedperformanceicc01HandTunedSaiph-lambda+compOpts+vectClauses+blockinggcc01HandTunedSaiph-lambda+compOpts+vectClauses+blocking1DSineWave2DSmithHutton2DInviscid3DHeatllvmmance. Saiph single-core optimisations and their automatic application are thus
validated.

4.3. Multi-core Scaling Performance

We use the already evaluated Saiph optimised single-core results as base-
lines to address the spatial loop’s scalability at the multi-core level. We evalu-
ate the fork-join model from section 3.3 and verify that previous optimisations
are preserved. Figure 13 shows results up to a socket (24 cores) for the 3D
Heat application from Table 1 comparing Saiph vectorised and non-vectorised
implementations against Yask results for diﬀerent spatial accuracy orders.

Figure 13: Multi-core performance comparisons of Saiph and Yask OpenMP fork-join 3D Heat
application for several spatial accuracy orders

Figure 13 shows decreased performance at rising space order for all the codes
tested. Moving to higher accuracy implies increasing the stencil length, lead-
ing to more computations for every point update. Using the Heat application
and moving from a second to an eight space order corresponds to a theoretic
computer demand increase of 2.6× from 23 FLOP/point to 59 FLOP/point.
Saiph vectorised results for the 1-core executions give 420 Mpoints/s and 183
Mpoints/s at acc = 2 and acc = 8, respectively, representing a 2.3× of per-
formance decrease. Hence, the Saiph automated combination of low-level opti-
misations apply for diﬀerent space orders. Moreover, Saiph vectorised results
from Figure 13 outperform scalar ones by a factor of 4, taking advantage of
the vector instruction size. Finally, Saiph’s results are competitive compared to
Yask’s ones. Nevertheless, Yask optimisations give slightly better performance
at high spatial accuracy orders.

We conduct a rooﬂine analysis [23] using the Intel Advisor Rooﬂine tool [24]
to obtain computational performance, and memory bandwidth values for the

21

1000200030004000500012481624Performance(Mpoints/s)Mesh points 300x200x200acc = 2Saiph-Heat3D non vect   Saiph-Heat3D vectYask-Heat3D1000200030004000500012481624Mesh points 300x200x200acc = 41000200030004000500012481624Performance(Mpoints/s)CoresMesh points 300x200x200acc = 61000200030004000500012481624CoresMesh points 300x200x200acc = 8Saiph vectorised runs using 16 cores and a second-order spatial accuracy. Within
such runs, the spatial loop delivers 742.88 GB/s, surpassing the DRAM and L3
bandwidth peak performances of 114 GB/s and 353 GB/s, respectively. Stencil
computations are usually memory-bounded, but the working set of the analysed
application is small (∼ 185M B). Moreover, the Saiph blocking technique en-
ables the reuse of cached memory, crucial for surpassing the roof of L3 cache
bandwidth. Hence, the loop’s performance is not limited by the simultaneous
accesses of the 16 cores to the same L3 cache of 33 MB. A deeper analysis char-
acterises the loop as a cache-bound workload, where the L1 and L2 cache stalls
are the most signiﬁcant cause of performance loss. Saiph optimised results are
close to the machine peak performance. The Saiph automated combination of
low-level optimisation and shared-memory parallelism is then validated at dif-
ferent space orders. Saiph multi-core support provides competitive performance
close to Yask’ and the processor peak performance.

In order to evaluate the impact on performance of the problem size, we
perform several executions running the Heat application with diﬀerent mesh
sizes: 150x100x100 points occupying ∼ 24 MB, 300x200x200 points occupying
∼ 185 MB and 300x400x400 points occupying ∼ 738 MB. Results are presented
in Figure 14.

Figure 14: Multi-core performance comparisons of Saiph and Yask OpenMP fork-join 3D Heat
application for several mesh sizes

Figure 14 shows slightly better results for the minor mesh size ﬁtting into
the L3 cache. Saiph outputs similar results for bigger meshes, validating the
eﬃcacy of Saiph’s multi-dimensional blocking strategy under shared-memory
parallelism. Again, Saiph vectorised results are competitive with Yask’s ones.

We repeat the previous scaling study for the diﬀerent Saiph applications

22

100020003000400012481624Performance(Mpoints/s)CoresMesh points 150x100x100acc = 8Saiph-Heat3D non vect   Saiph-Heat3D vectYask-Heat3D100020003000400012481624CoresMesh points 300x200x200acc = 8100020003000400012481624Performance(Mpoints/s)CoresMesh points 300x400x400acc = 8from Table 1 using a second-order spatial accuracy. Figure 15 shows scaling
results for blocked and vectorised applications taking the optimised single-core
execution as a baseline. We add a linear scaling curve with an arbitrary origin
value to have the slope reference for linear scalability.

Figure 15: Fork-join parallel scalability at socket level

All runs from ﬁgures 13 and 14, scale up to 16 cores due to load imbalances
at the 24-cores executions. We assess that Saiph blocking technique ensures
good memory locality and enough parallel work for linear scaling performance.
However, load imbalances at the 24-cores executions stall the scalability as the
number of threads is not multiple of the number of local blocks. The number of
cores nThreads determines the blocks, so the performance of the parallel code is
suboptimal for ill-suited conﬁgurations; load imbalances appear if nThreads is
not multiple of the number of blocks. This fact can be hypothesised when using
24 threads and conﬁrmed with the corresponding trace on the left of Figure 16.
The execution on the right of Figure 16 shows how using the tasking module
can prevent such a performance drop.

Figure 16: Main mesh and BCs updates using 24 threads and fork-join (left) and tasks (right)
models

The shared-memory parallelism is ﬁrmly bound to the automatic loop block-
ing technique and the balanced work partition, leading to linear scalability un-
der the appropriate combination of parallel paradigms and hardware resources.
Saiph eases the exploration of such key parameters that aﬀect performance,

23

24816242481624Scaling factorCores1DSineWave2DSmithHutton2DInviscid3DHeatLinear Scalinggiving insight into the best running conﬁgurations. In the previous tests, we
realised that runs using 16 cores provide a similar performance as those using
24 cores but using less power. Similarly, Saiph modules permit us to compare
diﬀerent parallel paradigms to choose the best ﬁtting one.

4.4. Distributed Scaling Performance

Lastly, we evaluate the applications’ distributed parallelism through weak
and strong scaling studies. For that, we use bigger problem sizes and evaluate
memory bound workloads and communications overheads.

We present a weak scaling study for the evaluation of the distribution strat-
egy from sections 3.3 and 3.5. For distributed executions assessment, commu-
nications and required data manipulations play an essential role. Thus, we take
one-node executions (2 MPIs) as baselines for 2D applications and four-node
executions (8 MPIs, 3D mesh partition) for 3D use cases. As illustrated in Fig-
ure 17, the multidimensional applications of Table 1 present a linear scalability
of up to 32 nodes for pure MPI and hybrid MPI+OpenMP fork-join modules.

Figure 17: Weak scaling using diﬀerent parallel strategies

Finally, we carry out a strong scaling study for the 3D Heat equation appli-
cation. For that, we enlarge the spatial mesh, set the spatial stencil accuracy to
8 and compute 500 time-steps. According to Table 1, the new problem size of
2401*1201*1201 mesh points, occupy 51.6 GB (2 buﬀers read/write using double
precision) and involves 63 Tera memory accesses, 4.72 Tera stencils and a total
of 93 TFLOPs. Figure 18 displays the absolute performance behaviour from
4 to 32 nodes using and combining diﬀerent shared-memory and distributed
parallel paradigms from Saiph. We add a linear scaling curve with an arbitrary
origin value to have the slope reference for linear scalability. Pure MPI runs
use as many MPI processors as available cores in the nodes, while hybrid runs
bind MPI processors to sockets and use as many threads as cores in the socket.
Finally, the blocking technique is automatically applied to each local mesh to
use the most appropriate granularity for the multi-core parallel work.

24

24816322481632SpeedupNodesPure MPI248163224816322DSmithHutton2DInviscid3DHeatNodesMPI+OpenMPFigure 18: Strong scaling for the 3D Heat application using diﬀerent hybrid parallelisations

Distributing the same problem size across a diﬀerent number of nodes il-
lustrates the transition from memory-bound to compute-intensive workloads
ending up in scenarios where communications overheads become costly. Saiph
hybrid executions show linear scaling up to 16 nodes. When using 32 nodes, local
computations decreases while communications increase, producing imbalances
leading to a non-linear performance scaling. Overall, task versions deliver higher
performance and better scalability than the fork-join version by reducing com-
putation load imbalances. Concretely, the use of TAMPI presents higher per-
formance than the rest of Saiph versions, including the pure MPI one. TAMPI
allows computations and communications to overlap, increasing the overall per-
formance and scalability of the runs. By taking the four-node execution as a
reference, the TAMPI-OmpSs-2 module outperforms the MPI fork-join, tasks
and pure versions by a factor of 1.7x, 1.3x and 1.3x, respectively. Those re-
sults validate Saiph implementations and demonstrate its competitive results.
Diﬀerent modules allow a parallel exploration to select the most performant
paradigms.

5. Related Work

Many tools target the CFD domain to deliver performance, portability, or
productivity [25]. Mainly divided into two main branches, there are frameworks
tackling stencil computation performance and DSLs facing complete CFD, PDEs
system resolution.

In the ﬁrst group, diﬀerent existing frameworks provide automated low-level
stencil optimisations and parallel implementations. Liszt [26] is a DSL for un-
structured mesh computations. The language is designed for code portability
across heterogeneous platforms. Liszt users work at the numerical level giving
information to ensure that the compiler can infer data dependencies. Similarly,
for unstructured mesh computations, OPS/OP2 [27, 28] and PyOP2 [29] as
the Python extension, give an abstraction for stencil computations at CPUs,
GPUs, and distributed systems. Such DSLs are embedded in C/Fortran and

25

81632641284192838416768321536Performance(Gpoints/s)NodesCoresSaiph-MPI+OpenMP (fork-join)Saiph-MPI+OpenMP (tasks)Saiph-TAMPI+OmpSs-2Saiph-Pure MPILinear ScalingPython, respectively. Yask [18] is a C++ library also for automatic stencil
optimisations. Cache blocking, vector folding and vectorisation are automati-
cally applied. Moreover, Yask generates from the user code parallel programs
using multiple cores and distributed-memory parallelism. ExaSlang [30] is an-
other stencil-speciﬁc programming language that provides diﬀerent layers of
abstraction and exploits domain information for neighbouring access optimisa-
tions. Targeting ExaSlang, ExaStencils [31] is a code generator to generate all
optimised lower layers codes to automatically get optimal conﬁgurations and
implementations. The above tools cover a wide range of domains, designs, de-
velopment platforms, languages and hardware targeted, optimisations and par-
allelisation strategies. However, they lack the abstraction of the whole picture of
a CFD problem. While stencils are highly optimised, time integration methods
or ICs and BCs are not part of the language, leaving users to code the complete
parallel workﬂow.

The second approach uses a top-level abstraction expressing the CFD prob-
lem in terms of actual diﬀerential equations. Those tools focus on a particular
set of parallel numerical methods, leaving the implementation’s details to lower-
level libraries. This is the case of FEniCS [32], a complete simulation infras-
tructure for many real-world problems. FEniCS relies upon expressing PDEs
at the mathematical level using Python and C++ interfaces. The tool deﬁnes a
language for the Finite Element Method and generates parallel codes that can
be executed in parallel using MPI. Although FEniCS is a compelling solution
for many complex problems, it requires deep expertise in numerical methods
to be used. Similarly, Firedrake [33] uses the PyOP2 library for parallelising
user high-level code. OpenSBLI [34] focus on the solution of the compressible
Navier-Stokes equations. The tool uses symbolic Python to allow the speciﬁca-
tion of PDEs using Einstein notation which automatically discretises to generate
OPS code. Devito [35] uses similar symbolic Python mainly focusing on seis-
mic inversion problems. Devito optimisations include common sub-expression
elimination, vectorisation, blocking and multi-core and distributed parallelism.
While the ﬁrst type of tools lacks part of the PDEs resolution process, the
last usually rely on other frameworks to apply low-level optimisations and paral-
lelisation strategies. This top-down dependence can worsen the use of advanced
parallel paradigms such as the task model. Saiph comprehends the combination
of the above-targeted philosophies.
It oﬀers a CFD high-level syntax hiding
numerical complexities while combining low-level automatic optimisations and
parallelisation strategies leading to high-performance executions codes. As the
above tools, Saiph design is based on layers separating concerns, however, the
proposed execution model ensures the interaction between the high-level syntax
layer and the low-level modules enabling to intertwine user code and low-level
optimisations and parallelisation strategies.

6. Conclusions

In this work, we have enhanced Saiph, a high-level DSL for solving CFD
problems using FDM, to generate optimised algorithmic patterns that leverage

26

state-of-the-art optimisation techniques. This work demonstrates how to enable
and combine the most relevant optimisation techniques from a high-level speciﬁ-
cation code. We extend Saiph to exploit high-performance algorithmic patterns
found in FDM. While maintaining abstraction and generality, we establish the
procedure to generate codes exploiting single, multi-core, and cluster resources:
Saiph ensures high eﬃcient sequential runs enabling compiler optimisations and
vectorisation through automated compile-time evaluations, data-alignment and
padding strategies. Moreover, the DSL transparently provides scalable par-
allel codes preserving single-core optimisations; using an appropriate blocking
technique Saiph oﬀers enough and well-balanced multi-core parallelism while
enhancing data locality and preserving data alignment at each parallel chunk.
Tasking and fork-join parallelisation versions are available. Saiph automati-
cally distributes the mesh across several nodes but preserving the optimisations
applied at the core and node level. Thus, following a bottom-up approach,
we consciously combine such strategies from single-core to cluster level, pre-
serving their eﬀectiveness and obtaining high performance, competitive with
hand-tuned codes. We conclude that exploiting domain knowledge allows DSLs
to assume and retrieve information from the high-level code. Then, generic
underlying implementations of optimisations and parallelisation strategies can
be intertwined with the user code through the build process proposed. In such
a scenario, we automatically generated optimised codes using advanced paral-
lelisation strategies and diﬀerent parallel paradigms performing and scaling as
much as manually optimised ones. Further, due to the customisable nature of
Saiph, it is possible to conduct a performance exploration under diﬀerent paral-
lelisation strategies. Looking forward, we see the opportunity of transparently
applying the best strategy for each input application.

Acknowledgment

This research has received funding from the European Union’s Horizon
2020/EuroHPC research and innovation programme under grant agreement
N.955606 (DEEP-SEA), and is supported by the Spanish State Research Agency
- Ministry of Science and Innovation (contract PID2019-107255GB), and by the
Generalitat de Catalunya (2017-SGR-1414). This work is also supported by
the Ministry of Economy of Spain through Severo Ochoa Center of Excellence
Program (SEV-2015-0493).

References

[1] S. Maci`a, S. Mateo, P. J. Mart´ınez-Ferrer, V. Beltran, D. Mira, E. Ayguad´e,
Saiph: Towards a DSL for high-performance computational ﬂuid dynamics,
in: Proceedings of the Real World Domain Speciﬁc Languages Workshop,
2018, pp. 1–10.

[2] S. Maci`a, P. J. Mart´ınez-Ferrer, S. Mateo, V. Beltran, E. Ayguad´e, As-
sembling a high-productivity DSL for computational ﬂuid dynamics, in:

27

Proceedings of the Platform for Advanced Scientiﬁc Computing Confer-
ence, 2019, pp. 1–11.

[3] BSC, OmpSs-2, OmpSs project home page https://pm.bsc.es/
{\protect\discretionary{\char\hyphenchar\font}{}{}}ompss-2
(2021).

[4] K. Sala, X. Teruel, J. M. Perez, A. J. Pe˜na, V. Beltran, J. Labarta, In-
tegrating blocking and non-blocking MPI primitives with task-based pro-
gramming models, Parallel Computing 85 (2019) 153–166.

[5] BSC, Saiph CFD-benchmarkApp,

https://github.com/EulerStokes/CFD-benchmarkApp (2019).

[6] A. Moors, T. Rompf, P. Haller, M. Odersky, Scala-virtualized, in: Pro-
ceedings of the ACM SIGPLAN 2012 workshop on Partial evaluation and
program manipulation, 2012, pp. 117–120.

[7] T. Rompf, M. Odersky, Lightweight modular staging: a pragmatic ap-
proach to runtime code generation and compiled DSLs, GPCE ’10, ACM,
New York, NY, USA, 2010, pp. 127–136. doi:10.1145/1868294.1868314.
URL http://doi.acm.org/10.1145/1868294.1868314

[8] C. Yount, Vector folding:

Improving stencil performance via multi-
dimensional SIMD-vector representation, in: 2015 IEEE 17th International
Conference on High Performance Computing and Communications, IEEE,
2015, pp. 865–870.

[9] M. Wolfe, More iteration space tiling,

in: Proceedings of the 1989

ACM/IEEE conference on Supercomputing, 1989, pp. 655–664.

[10] S. Coleman, K. S. McKinley, Tile size selection using cache organization

and data layout, ACM SIGPLAN Notices 30 (6) (1995) 279–290.

[11] A. Nguyen, N. Satish, J. Chhugani, C. Kim, P. Dubey, 3.5-D blocking
optimization for stencil computations on modern CPUs and GPUs, in: SC
’10: Proceedings of the 2010 ACM/IEEE International Conference for High
Performance Computing, Networking, Storage and Analysis, 2010, pp. 1–
13.

[12] L. Renganarayana, M. Harthikote-Matha, R. Dewri, S. Rajopadhye, To-
wards optimal multi-level tiling for stencil computations, in: 2007 IEEE
International Parallel and Distributed Processing Symposium, IEEE, 2007,
pp. 1–10.

[13] J. M. C. Carpaye, J. Roman, P. Brenner, Design and analysis of a task-
based parallelization over a runtime system of an explicit ﬁnite-volume
CFD code with adaptive time stepping, Journal of computational science
28 (2018) 439–454.

28

[14] L. L. Nesi, L. M. Schnorr, P. O. A. Navaux, Design, implementation and
performance analysis of a CFD task-based application for heterogeneous
CPU/GPU resources, in: International Conference on Vector and Parallel
Processing, Springer, 2018.

[15] T. Chung, Computational ﬂuid dynamics, Cambridge university press,

2010.

[16] A. Afzal, Z. Ansari, A. R. Faizabadi, M. Ramis, Parallelization strategies
for computational ﬂuid dynamics software: state of the art review, Archives
of Computational Methods in Engineering 24 (2) (2017) 337–363.

[17] O. MPI, MPI Dims create man page, available at https://www.open-mpi.

org/doc/v3.1/man3/MPI_Dims_create.3.php (2020).

[18] C. Yount, J. Tobin, A. Breuer, A. Duran, YASK-Yet Another Stencil Ker-
nel: A framework for HPC stencil code-generation and tuning, in: 2016
Sixth International Workshop on Domain-Speciﬁc Languages and High-
Level Frameworks for High Performance Computing (WOLFHPC), IEEE,
2016, pp. 30–39.

[19] C. Yount, Developing stencil code using the YASK framework, http://

intel.github.io/yask/YASK-tutorial.pdf (2020).

[20] BSC, MareNostrum, available at https://www.bsc.es/marenostrum

(2021).

[21] BSC, LLVM-based compiler for OmpSs-2, https://github.com/bsc-pm/

llvm (2021).

[22] J. D. Little, A proof for the queuing formula: L= λ w, Operations research

9 (3) (1961) 383–387.

[23] S. Williams, Rooﬂine: An insightful visual performance model for ﬂoating-

point programs and multicore, ACM Communications.

[24] Intel, Intel Advisor Rooﬂine, https://www.intel.com/content/www/us/

en/developer/articles/guide/intel-advisor-roofline.html.

[25] I. Z. Reguly, G. R. Mudalige, Productivity, performance, and portability
for computational ﬂuid dynamics applications, Computers & Fluids 199
(2020) 104425.

[26] Z. DeVito, N. Joubert, F. Palacios, S. Oakley, M. Medina, M. Barrientos,
E. Elsen, F. Ham, A. Aiken, K. Duraisamy, et al., Liszt: a domain speciﬁc
language for building portable mesh-based pde solvers, in: Proceedings of
International Conference for High Performance Computing, Networking,
Storage and Analysis, 2011.

29

[27] I. Z. Reguly, G. R. Mudalige, M. B. Giles, D. Curran, S. McIntosh-Smith,
The OPS domain speciﬁc abstraction for multi-block structured grid com-
putations, in: WOLFHPC, 2014 Fourth International Workshop on, IEEE,
2014, pp. 58–67.

[28] G. R. Mudalige, I. Reguly, M. B. Giles, Auto-vectorizing a large-scale pro-
duction unstructured-mesh CFD application, in: Proceedings of the 3rd
Workshop on Programming Models for SIMD/Vector Processing, ACM,
2016, p. 5.

[29] F. Rathgeber, G. R. Markall, L. Mitchell, N. Loriant, D. A. Ham,
C. Bertolli, P. H. Kelly, PyOP2: A high-level framework for performance-
portable simulations on unstructured meshes, in: 2012 SC Companion:
High Performance Computing, Networking Storage and Analysis, IEEE,
2012, pp. 1116–1123.

[30] C. Schmitt, S. Kuckuk, F. Hannig, H. K¨ostler, J. Teich, ExaSlang: A
domain-speciﬁc language for highly scalable multigrid solvers, in: 2014
Fourth international workshop on domain-speciﬁc languages and high-level
frameworks for high performance computing, IEEE, 2014, pp. 42–51.

[31] C. Lengauer, S. Apel, M. Bolten, S. Chiba, U. R¨ude, J. Teich,
A. Gr¨oßlinger, F. Hannig, H. K¨ostler, L. Claus, et al., ExaStencils: Ad-
vanced multigrid solver generation, in: Software for Exascale Computing-
SPPEXA 2016-2019, Springer, Cham, 2020, pp. 405–452.

[32] M. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg,
C. Richardson, J. Ring, M. Rognes, G. Wells, The FEniCS project ver-
sion 1.5 3. doi:10.11588/ans.2015.100.20553.

[33] F. Rathgeber, D. A. Ham, L. Mitchell, M. Lange, F. Luporini, A. T. McRae,
G.-T. Bercea, G. R. Markall, P. H. Kelly, Firedrake: automating the ﬁnite
element method by composing abstractions, ACM Transactions on Mathe-
matical Software (TOMS) 43 (3) (2016) 1–27.

[34] G. R. Mudalige, I. Reguly, S. P. Jammy, C. T. Jacobs, M. B. Giles, N. D.
Sandham, Large-scale performance of a DSL-based multi-block structured-
mesh application for direct numerical simulation, Journal of Parallel and
Distributed Computing 131 (2019) 130–146.

[35] M. Lange, N. Kukreja, M. Louboutin, F. Luporini, F. Vieira, V. Pandolfo,
P. Velesko, P. Kazakas, G. Gorman, Devito: Towards a generic ﬁnite dif-
ference DSL using symbolic python, in: 2016 6th Workshop on Python for
High-Performance and Scientiﬁc Computing (PyHPC), IEEE, 2016, pp.
67–75.

30

