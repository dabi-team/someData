2
2
0
2

n
u
J

0
3

]
h
p
-
t
n
a
u
q
[

1
v
4
8
2
5
1
.
6
0
2
2
:
v
i
X
r
a

QuASK - Quantum Advantage Seeker with
Kernels

Francesco Di Marcantonio 1, Massimiliano
Incudini 2,4, Davide Tezza 3,4 and Michele Grossi 1

1*European Organization for Nuclear Research (CERN), Geneva,
1211, Switzerland.
2*Dipartimento di Informatica, Universit`a di Verona, Verona,
34137, Italy.
3*Dipartimento di Matematica, Universit`a di Trento, Povo, 38123,
Italy.
4*Data Reply s.r.l., Turin, 10126, Italy.

Contributing authors: massimiliano.incudini@univr.it;

Abstract

QuASK is a quantum machine learning software written in Python that
supports researchers in designing, experimenting, and assessing diﬀer-
ent quantum and classical kernels performance. This software is package
agnostic and can be integrated with all major quantum software packages
(e.g. IBM Qiskit, Xanadu’s Pennylane, Amazon Braket). QuASK guides
the user through a simple preprocessing of input data, deﬁnition and calcu-
lation of quantum and classical kernels, either custom or pre-deﬁned ones.
From this evaluation the package provides an assessment about poten-
tial quantum advantage and prediction bounds on generalization error.
Moreover, it allows for the generation of parametric quantum kernels that
can be trained using gradient-descent-based optimization, grid search, or
genetic algorithms. Projected quantum kernels, an eﬀective solution to
mitigate the curse of dimensionality induced by the exponential scaling
dimension of large Hilbert spaces, are also calculated. QuASK can further-
more generate the observable values of a quantum model and use them
to study the prediction capabilities of the quantum and classical kernels.

Keywords: Quantum Kernel, Kernel optimization, Software, Quantum
Machine Learning, Quantum Computing

1

 
 
 
 
 
 
2

QuASK - Quantum Advantage Seeker with Kernels

1 Introduction

Is there any clear beneﬁt in using quantum kernels [1] to enhance machine
learning applications? The deﬁnitive answer has still to be found [2]. The no-
free lunch theorem guarantees that there is no best model in machine learning
which outperforms all others, and eﬃciency is based on the type of training data
distribution. Thus, there is no hope for a “universal” quantum kernel. However,
quantum kernels have been proven to reach superior performance in learning
from quantum experiments [3, 4] and in solving artiﬁcially generated, classically
intractable classiﬁcation tasks [5]. Considering real-world data, they have been
used to classify cosmological data with performance comparable to the classical
Gaussian kernel [6]. Moreover, these can be eﬀectively implemented on NISQ
devices [6, 7]. Quantum kernels must be carefully tailored to avoid problems
related to the curse of dimensionality: the Hilbert space associated with a
kernel scales exponentially in the number of qubits, leading to exponentially
small inner products. This issue could be solved by projecting the data onto
low-dimensional subspaces using projected quantum kernels [3].

The process of designing, experimenting, and evaluating diﬀerent quantum
kernels can be nontrivial, resulting in computationally intensive and error-
prone calculations. The dataset must be appropriately preprocessed in order
to reduce the number of features to accommodate the available resources of
NISQ hardware and simulators. Furthermore, the features must be normalized
and rebalanced according to the class distribution. The choice of the unitary
transformation implementing the quantum feature map is arbitrary, as is the
observable choice in projected quantum kernels. In addition, this unitary trans-
formation may be parametric and optimization might be necessary according
to some criteria [8]. Finally, the quantum kernel must be evaluated accord-
ing to some metrics including the prediction accuracy, model complexity, and
geometric diﬀerence [3].

The whole process described above can be automated with the proposed
QuASK (Quantum Advantage Seeker with Kernel) library. This package, written
in Python, implements a (quantum) machine learning pipeline that takes care
of the most trivial but time-consuming part of working with quantum kernels.
QuASK is available either as an interactive CLI (Command-Line Interface) or
as a library whose components can be used to build new software. QuASK
allows the complete reproducibility of the experiments, saving the information
processed at any step of its pipeline. QuASK allows, also, the generation of
data from quantum processes in order to study the performance of classical and
quantum kernels using quantum data. Finally, in terms of results visualization,
QuASK can generate a simple accuracy plot of all the trained kernel machines.
https://github.com/

available

software

The

at

is

freely
CERN-IT-INNOVATION/QuASK.

QuASK - Quantum Advantage Seeker with Kernels

3

2 QuASK library

QuASK provides a library of elements that can be integrated with other projects.
The library is composed of the following software modules.

quask.datasets and quask.metrics

The quality of a quantum kernel can be empirically tested through the perfor-
mance of a kernel machine with respect to a certain dataset, by evaluating some
metrics. The quask.datasets module facilitates the researcher in choosing a
suitable dataset, providing some of the most popular datasets from OpenML
platform1 and custom datasets generated also from quantum experiments (the
latter allows us to reproduce results in [3]). The module quask.metrics con-
tains the metrics to compare and evaluate the kernels, including the kernel
polarity (i.e. Frobenius inner product between two Gram matrices), the target-
kernel alignment [9], the training and testing accuracy of the Support Vector
Machine with the precomputed kernel, the geometric diﬀerence [3] (which can
be used to ﬁnd a potential quantum advantage), and the model complexity [3].

quask.template pennylane and quask.template qiskit

φ(x(cid:48))
(cid:105)
|

The quantum kernel requires the deﬁnition of a feature map φ which is
implemented using a parameterized unitary transformation Uφ(x). The Quan-
tum Kernel Estimation algorithm [10] calculating the function k(x, x(cid:48)) =
is implemented through either the overlap test (Figure 1a) or the
φ(x)
(cid:104)
C-SWAP test (Figure 1b). The projected quantum kernel calculates classically
the inner product between two feature vectors φ(x1), φ(x2), each one being
the output of the quantum feature map φ(x) =
(Figure 1c).
The feature map crosses the quantum space ﬁrst through U(x) and projects
the data back in a classical representation when measuring with the Hermi-
tian operator H. QuASK contains both some notable unitary transformations
U from the literature and the code to use such unitary transformations as a
kernel function through one of the three methods described above. The user
can deﬁne its own unitary transformation and immediately get the kernel func-
tion. QuASK is agnostic with respect to the software framework used to deﬁne,
simulate and execute the quantum circuits: we have implemented some uni-
tary transformations in PennyLane (quask.template pennylane module) and
some in Qiskit (quask.template qiskit). This allows also the use of the dif-
ferent functionalities oﬀered by the diﬀerent frameworks. For example, noiseless
simulation with PennyLane can be speeded up using JAX2.

U †(x)HU (x)
|
|

0
(cid:105)

0
(cid:104)

quask.kernels
The module quask.kernels keeps track of the quantum kernel available
within the framework. The module quask.kernels contains the global object
the kernel register, which is a list of kernels. This list is already ﬁlled with

1OpenML is an open platform for sharing datasets, available at https://www.openml.org/
2JAX is an high-performance linear algebra library.

4

QuASK - Quantum Advantage Seeker with Kernels

(a)

(b)

(c)

(d)

Fig. 1: (1a)-(1b) Overlap test and C-SWAP test for Quantum Kernel Esti-
mation where U is the feature map associated with the quantum kernel. (1c)
Quantum circuit for the feature map associated with the projected kernel, the
Hermitian observable H can be arbitrary. (1d) Steps of the QuASK pipeline.
The processing phase can analyze the system to ﬁnd the best technique to
analyze the system.

all the quantum kernels derived from the unitary transformations in modules
quask.template xyz, while the user can add its own custom kernel too. The
kernels do not need to be quantum. Such module can use trainable kernels
(deﬁned in quask.template pennylane) which are parametric quantum cir-
cuits that depend on both the input and some trainable parameters. These
parameters are supposed to be trained with gradient-descent-based optimization
algorithms. The user can take advantage of eﬃcient optax optimization library.
Nonetheless, the user can manually tune the parameters or even run a grid
search. Furthermore, such an approach is extendable to optimization techniques
that change the topology of the circuit itself such as genetic algorithms.

σ⊗nz|0⟩⊗nU(x1)U†(x2)σz|0⟩HH|0⟩⊗nU(x1)|0⟩⊗nU(x2)H|0⟩⊗nU(x)GENERATE CLASSICAL AND QUANTUM DATAPREPROCESSPLOTPROCESS CLASSICAL AND QUANTUM KERNELSgeometric difference≪#sample∝#sampleapproximate dimensionlowhighcomplexity of classical modelslowhighcomplexity of quantum modelslowhighPOTENTIAL QUANTUM ADVANTAGEUSE CLASSICAL MLQuASK - Quantum Advantage Seeker with Kernels

5

3 QuASK script

QuASK can be run as a standalone, interactive executable. The software is run
with python3 -m quask <command>. The script implements the sequence of
operation described in Figure 1d. The available commands are:

get-dataset command
The script is used to retrieve one of the datasets available in quask.datasets
module. The output of the process is a pair of NumPy binary ﬁles representing
the feature data and the corresponding labels.

> python3.9 -m quask get-dataset
The available datasets are:

0: iris
1: Fashion-MNIST
2: liver-disorders
3: delta_elevators
4: Q_Fashion-MNIST_2_E1
5: Q_Fashion-MNIST_4_E2
6: Q_Fashion-MNIST_8_E3

Which dataset will you generate?: 0
Where is the output folder?: .
Saved file ./iris_X.npy
Saved file ./iris_y.npy

- classification - classical
- classical
- classification
- classical
- regression
- classical
- regression
- quantum
- regression
- quantum
- regression
- quantum
- regression

preprocess-dataset command

The script can modify the dataset in several ways. Firstly, the user can vertically
slice the dataset, keeping only a certain range of labels. Secondly, the user can
apply dimensionality reduction techniques. These are especially important in
the NISQ setting due to the lack of resources. The techniques available are
PCA for numerical data and FAMD for mixed numerical and categorical data3.
Thirdly, it is possible to ﬁx the possible imbalanceness of the classes using
random undersampling or random oversampling. When loading, the script
already shows some statistics about the dataset, both for classiﬁcation and
regression tasks, which can guide the user through the preprocessing. The
output of the process are the four ﬁles X train, y train, X test, y test which
can now be fed to some kernel machine.
> python3.9 -m quask preprocess-dataset
Choose a random seed: 2323
Choose a random seed for splitting training and testing set: 3232
Where is the output folder: .
Specify the path of classical feature data X (.npy array): iris_X.npy

Loaded X having shape (150, 4) (150 samples of 4 features)

Specify the path of classical feature data T (.npy array): iris_y.npy

Loaded Y having shape (150,) (150 labels)
Labels have type int64 thus this is a classification problem
The dataset has 3 labels being [0, 1, 2] distributed as it follows:

Class 0 is present 50 times (33.33 %)
Class 1 is present 50 times (33.33 %)
Class 2 is present 50 times (33.33 %)

Do you want to pick just the first two classes? [y/N]: y
Do you want to undersample the largest class? [y/N]: n

3FAMD, Factor Analysis for Mixed Data is implemented using Prince library (available at

https://github.com/MaxHalford/prince).

6

QuASK - Quantum Advantage Seeker with Kernels

Do you want to oversample the smallest class? [y/N]: n
Do you want to apply preprocessing to the features? [y/N]: y
Do you want to apply PCA (numerical data only)? [y/N]: y
How many components you want?: 2
Do you want to apply FAMD (both numerical and categorical data)? [y/N]: n
Do you want to scale each field from 0 to 1 (MinMaxScaler)? [y/N]: y
Which percentage of data must be in the test set?: 0.5
Saved file ./X_train.npy
Saved file ./y_train.npy
Saved file ./X_test.npy
Saved file ./y_test.npy

apply kernel command

The script is used to generate the Gram matrices of the training and testing sets
with respect to the selected kernel. The user can select to apply one of the kernels
(classical and quantum) provided by the Python object the kernel register.

> python3.9 -m apply-kernel
Where is X_train (npy file)?: X_train.npy
Where is y_train (npy file)?: y_train.npy
Where is X_test (npy file)?: X_test.npy
Where is y_test (npy file)?: y_test.npy
Do you want to apply a fixed kernel [Y] or a trainable one [N]? [y/N]: y
The available fixed kernels are:

0: linear_kernel
1: rbf_kernel
2: poly_kernel
3: zz_quantum_kernel
4: projected_zz_quantum_kernel
5: random_quantum_kernel
6: projected_random_quantum_kernel

Which kernel Gram matrix will you generate?: 0
Where is the output folder?: .
Saved file ./training_linear_kernel.npy
Saved file ./testing_linear_kernel.npy

The user can use trainable quantum kernels. In this case, the user speciﬁes
the embedding and the variational form (and its number of repetitions), the
optimizer (ADAM stochastic gradient descent or grid search), and the reward
function to maximize.

> python3.9 -m apply-kernel
Where is X_train (npy file)?: X_train.npy
Where is y_train (npy file)?: y_train.npy
Where is X_test (npy file)?: X_test.npy
Where is y_test (npy file)?: y_test.npy
Do you want to apply a fixed kernel [Y] or a trainable one [N]? [y/N]: n
Choose an embedding for your data (rx, ry, rz, zz): rx
Choose an embedding for your data (hardware_efficient, tfim, ltfim, zz_rx): tfim
Choose a number of layers: 1
Choose an optimizer (adam, grid): adam
Choose a reward metric to maximize (kernel-target-alignment, accuracy, geometric-differen

ce, model-complexity): accuracy

Choose a random seed: 434343
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun

for more info.)

...................................................................................
Where is the output folder?: .
Where is the output folder?: .
Saved file ./training_trainable_rx_tfim_1_adam_accuracy.npy
Saved file ./testing_trainable_rx_tfim_1_adam_accuracy.npy

QuASK - Quantum Advantage Seeker with Kernels

7

plot-metric command

The script calculates the accuracy of a kernel machine using the training and
testing Gram matrices given as input. The output is a plot comparing the
diﬀerent kernels. For each kernel matrix, the user speciﬁes the label that appears
at the x-axis of the plot. If multiple instances are speciﬁed with the same label
these are interpreted as i.i.d. random experiments and will contribute to the
error bars. The software also calculates, together with the accuracy, any metric
present in quask.metrics.
python3.9 -m quask plot-metric \
--metric accuracy \
--train-gram training_linear_kernel.npy --train-y Y_train.npy \
--test-gram testing_linear_kernel.npy --test-y Y_test.npy \
--label linear \
--train-gram training_rbf_01_kernel.npy --train-y Y_train.npy \
--test-gram testing_rbf_01_kernel.npy --test-y Y_test.npy \
--label rbf \
--train-gram training_rbf_001_kernel.npy --train-y Y_train.npy \
--test-gram testing_rbf_001_kernel.npy --test-y Y_test.npy \
--label rbf \
--train-gram training_poly_kernel.npy --train-y Y_train.npy \
--test-gram testing_poly_kernel.npy --test-y Y_test.npy \
--label poly \
--train-gram training_trainable_rx_tfim_1_adam_accuracy.npy \
--train-y Y_train.npy \
--test-gram testing_trainable_rx_tfim_1_adam_accuracy.npy --test-y
--label 'trainable quantum kernel'

Y_test.npy \

4 Conclusions

We have introduced QuASK, a tool supporting researchers in creating powerful
quantum kernels. The software takes care of the most time-consuming and error-
prone aspects of the experimentation. It exploits theoretical metrics in QML,
providing users with an environment to easily assess case for potential quantum
advantage. This package oﬀers the exciting perspective of testing these metrics
on real-wolrd datasets. The QuASK project will be extended in future versions
with a wider range of datasets and feature maps, both classical and quantum.

Acknowledgments

Icons in Figure 1d were made by eucalyp from www.ﬂaticon.com.

References

[1] Mengoni, R., Di Pierro, A.: Kernel methods in quantum machine learning.

Quantum Machine Intelligence 1(3), 65–71 (2019)

[2] Schuld, M., Killoran, N.: Is quantum advantage the right goal for quantum

machine learning? arXiv preprint arXiv:2203.01340 (2022)

[3] Huang, H.-Y., Broughton, M., Mohseni, M., Babbush, R., Boixo, S.,
Neven, H., McClean, J.R.: Power of data in quantum machine learn-
ing. Nature Communications 12(1) (2021). https://doi.org/10.1038/

8

QuASK - Quantum Advantage Seeker with Kernels

s41467-021-22539-9

[4] Huang, H.-Y., Broughton, M., Cotler, J., Chen, S., Li, J., Mohseni, M.,
Neven, H., Babbush, R., Kueng, R., Preskill, J., McClean, J.R.: Quantum
advantage in learning from experiments. Science 376(6598), 1182–1186
(2022). https://doi.org/10.1126/science.abn7293

[5] Liu, Y., Arunachalam, S., Temme, K.: A rigorous and robust quantum
speed-up in supervised machine learning. Nature Physics 17(9), 1013–1017
(2021)

[6] Peters, E., Caldeira, J., Ho, A., Leichenauer, S., Mohseni, M., Neven,
H., Spentzouris, P., Strain, D., Perdue, G.N.: Machine learning of high
dimensional data on a noisy quantum processor. npj Quantum Information
7(1), 1–5 (2021)

[7] Wang, X., Du, Y., Luo, Y., Tao, D.: Towards understanding the power of

quantum kernels in the nisq era. Quantum 5, 531 (2021)

[8] Hubregtsen, T., Wierichs, D., Gil-Fuster, E., Derks, P.-J.H., Faehrmann,
P.K., Meyer, J.J.: Training quantum embedding kernels on near-term
quantum computers. arXiv preprint arXiv:2105.02276 (2021)

[9] Cristianini, N., Shawe-Taylor, J., Elisseeﬀ, A., Kandola, J.: On kernel-
target alignment. Advances in neural information processing systems 14
(2001)

[10] Havl´ıˇcek, V., C´orcoles, A.D., Temme, K., Harrow, A.W., Kandala, A.,
Chow, J.M., Gambetta, J.M.: Supervised learning with quantum-enhanced
feature spaces. Nature 567(7747), 209–212 (2019)

