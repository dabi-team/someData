2
2
0
2

p
e
S
7

]
E
S
.
s
c
[

1
v
7
0
5
3
0
.
9
0
2
2
:
v
i
X
r
a

So Much in So Little: Creating Lightweight
Embeddings of Python Libraries

Yaroslav Golubev
JetBrains Research
Belgrade, Serbia
yaroslav.golubev@jetbrains.com

Egor Bulychev*
Huawei
Moscow, Russia
egor.bulychev@huawei.com

Egor Bogomolov
JetBrains Research
Paphos, Cyprus
egor.bogomolov@jetbrains.com

Timofey Bryksin
JetBrains Research
Limassol, Cyprus
timofey.bryksin@jetbrains.com

Abstract—In software engineering, different approaches and
machine learning models leverage different types of data: source
code, textual
information, historical data. An important part
of any project is its dependencies. The list of dependencies is
relatively small but carries a lot of semantics with it, which can
be used to compare projects or make judgements about them.

In this paper, we focus on Python projects and their PyPi
dependencies in the form of requirements.txt ﬁles. We compile
a dataset of 7,132 Python projects and their dependencies, as
well as use Git to pull their versions from previous years. Using
this data, we build 32-dimensional embeddings of libraries by
applying Singular Value Decomposition to the co-occurrence
matrix of projects and libraries. We then cluster the embeddings
and study their semantic relations.

To showcase the usefulness of such lightweight library embed-
dings, we introduce a prototype tool for suggesting relevant li-
braries to a given project. The tool computes project embeddings
and uses dependencies of projects with similar embeddings to
form suggestions. To compare different library recommenders, we
have created a benchmark based on the evolution of dependency
sets in open-source projects. Approaches based on the created
embeddings signiﬁcantly outperform the baseline of showing the
most popular libraries in a given year. We have also conducted
a user study that showed that the suggestions differ in quality
for different project domains and that even relevant suggestions
might be not particularly useful. Finally, to facilitate potentially
more useful recommendations, we extended the recommender
system with an option to suggest rarer libraries.

I. INTRODUCTION

A lot of tasks in software engineering research rely on
building representations of different entities: code snippets,
ﬁles, or entire projects. Such representations, or embeddings,
can be used in searching for similar repositories [1], suggesting
method names [2], or code completion [3]. However, software
engineering contains not only the code itself: it also leverages
textual information (e.g., READMEs, issues, docstrings), his-
torical data (e.g., project’s age and evolution), collaboration
data, licensing information, and much more. For instance, the
collaboration data can be used to assign reviewers [4] or track
learning in software teams [5].

*The work was carried out when the author worked at JetBrains.

One more source of valuable information about the project
is its dependencies. In a way, dependencies can be viewed as a
skeleton of a project. Oftentimes, one glance at them can give
you a pretty good idea of the project’s domain: web, machine
learning, media, etc. Thus, dependencies can potentially be
used to compare or differentiate between projects.

In this paper, we use dependencies to create embed-
dings of libraries and projects, and showcase their possible
usefulness by suggesting potentially interesting libraries to
developers of a given project. We compiled a dataset of
requirements.txt ﬁles of 7,132 Python projects, as well
as pulled up their versions from previous years where it
is possible. We have applied Singular Value Decomposition
(SVD) [6] to the co-occurrence matrix of projects and their
dependencies to obtain 32-dimensional vectors of libraries.
In contrast
to the approaches based on the study of in-
code imports [7], our technique does not require any code
processing and can be considered lightweight.

Firstly, we studied the meaningfulness of the obtained
embeddings by clustering them and manually inspecting the
resulting clusters. We found that the clusters themselves and
their relative positions reﬂect semantic differences between
libraries, with web- and application-related libraries laying
further away from machine learning and data science ﬁelds.

We also developed a system that employs the obtained
embeddings to suggest possible relevant libraries to a given
project. To search for similar projects, we consider several
ways to represent projects based on their dependencies. To
evaluate the performance of different approaches, we also
created a benchmark from the collected versions of require-
ments ﬁles. The benchmark measures the models’ accuracy in
suggesting libraries to the projects in a given year that were
actually added in the next year. The best results using the
extracted embeddings demonstrate the MRR (Mean Reciprocal
Rank) [8] of 0.189, while the baseline of suggesting the most
popular libraries only shows the MRR of 0.144.

To further evaluate the usefulness of the approach, we also
conducted a user study. We selected a repository from each of

 
 
 
 
 
 
the ﬁve domains of Python projects [9]: NLP, Web, Media,
Data Processing, and Scientiﬁc Calculations, and collected
the suggestions of different models. We then showed the
suggestions to eight Python developers and asked them to
evaluate their usefulness and relevance. The results suggest
that for certain domains, recommendations are better than for
others, and that oftentimes the suggestions are relevant but not
useful, in the sense that they might not be easily implemented
in a given project. This can happen, when, for example, the
approach suggests an analogous popular framework.

Finally, we propose another possible way of suggesting
libraries called the exploration, which consists in
relevant
facilitating the recommendation of rare libraries and therefore
possibly helping developers discover new tools.
The main contributions of this paper are:
• Approach. We proposed a novel approach to create
lightweight embeddings of Python libraries and projects
based on their co-occurence, and used these embeddings
to create a system for recommending relevant libraries
for Python projects.

• Dataset and benchmark. We collected a dataset of 7,132
Python projects and their dependencies, and used them to
compile a benchmark for the libraries recommendation
task. The benchmark contains 4,678 entries that consist
of a project’s current libraries and the set of libraries
added within the next year.

• Evaluation. We compared several different approaches
on the benchmark, with the embeddings-based models
demonstrating the best performance. Also, we conducted
a user evaluation that showed that the quality of predic-
tions varies by domains and that the suggestions might
be relevant and yet be difﬁcult to integrate in the project
that they are suggested for.

The collected dataset, the benchmark based on it, the source
code of our approach, and the CLI tool for recommend-
ing relevant libraries are available online: https://github.com/
JetBrains-Research/similar-python-dependencies.

The rest of the paper is organized as follows. Section II
describes the recent works related to software libraries and
entity embeddings. In Section III, we describe the collection
of our dataset and discuss its nature. Section IV describes
in detail the creation of the embeddings and the studying of
their semantics using clustering, while Section V describes
the proposed system for recommending relevant libraries, the
comparison of different models on the compiled benchmark,
as well as the user study and their results. In Section VI, we
discuss possible threats to the validity of our study, and, ﬁnally,
in Section VII, we draw our conclusions and discuss possible
directions of the future work.

II. RELATED WORK

Over the last decade, machine learning methods, and neural
networks in particular, have been applied in various ﬁelds, such
as natural language processing [10], image processing [11],
machine translation [12], recommendation systems [13]. Soft-
ware engineering (SE) domain is not an exception: different

neural network models show state-of-the-art results in many
SE tasks, e.g., method name prediction [14], bug localiza-
tion [15], commit message generation [16].

Depending on the task, the input of ML models can vary
greatly. However, any input should be transformed into a
numeric form in order to apply neural networks. A projection
of the input into numeric vectors, commonly called embed-
ding, can be trained alongside with training the network for
a speciﬁc task. Alternatively, approaches like word2vec [17]
build generic embeddings that can be further utilized in various
downstream tasks. The latter is of interest for us, since general
approaches are more suitable for our goal of exploring the
information contained in project dependencies.

Two common ways to build general embeddings are mod-
iﬁcations of word2vec [17] (e.g., FastText [18], GloVe [19])
and language models (LMs) [20]. In order to work, both ap-
proaches need sequences of items (e.g., words in text, queries
in search sessions). When the sequential information is missing
or is not reliable, as in the case of project dependencies, item
embeddings can be built based on the co-occurrence matrix.
A popular technique to transform a matrix of word-text co-
occurrence is Singular Value Decomposition (SVD) [6].

Theeten et al. [7] studied the idea of building embeddings
for software libraries. The authors collected a dataset of
projects in Java, JavaScript, and Python, extracted dependen-
cies from source code ﬁles, and trained word2vec-like model
on the sets of imports contained in the same ﬁle. This approach
relies on parsing the dependencies from code, and then training
a small neural network to obtain embeddings. Theeten et al.
also introduce a task of contextual code search: given a set of
anchor libraries, they suggest similar libraries based on their
embeddings. In order to evaluate the model’s performance for
this task, they use a somewhat artiﬁcial setup: for a set of
libraries, or context, the authors identify projects that contain it
and try to predict the rest of the dependencies of these projects.
Aside from building library embeddings, researchers ex-
plored the library migration graphs [21], studied the way
developers handle vulnerable dependencies [22] and the evo-
lution of dependencies [23]. We believe that all of these tasks
can beneﬁt from approaches that automate the extraction of
semantic information for software libraries.

III. DATASET

Our approach to retrieving semantic information for soft-
ware libraries relies on projects and the lists of their depen-
dencies. In our work, we chose Python as a target language.
The ﬁrst reason for this is Python’s popularity and the second
reason is that it has very convenient package management
systems. To keep the approach simple, we focused on the pip
package installer [24] and the PyPi index [25].

pip is a popular package installer for Python that supports
so-called requirement ﬁles. These ﬁles must adhere to certain
simple formatting rules and contain dependencies that are
necessary to run the given project. If such ﬁle is present, the
user can then run a single command to automatically install
or update all the necessary packages via pip.

A. Data collection

Since our method relies only on the lists of dependencies,
it does not require a lot of storage space or computational
resources. Thus, we decided to compile our own dataset with
fresh data. As a starting point, we took GHTorrent [26], a
large collection of GitHub data, more speciﬁcally, their dump
of July of 2020 [27]. To process it, we used a tool called
PGA-create [28] that was previously used to create a Public
Git Archive [29]. This tool processes the SQL dump to create
a CSV with a list of projects that allows for their convenient
ﬁltering. We selected projects with at least 50 stars, which
allows us to only work with at
least moderately popular
projects.

Then, we ran each of the obtained projects through GitHub

API and ﬁltered out the following repositories:

• repositories that now redirect to other repositories in the
dataset (since some repositories were moved or merged
after the GHTorrent dump was carried out);

• repositories, the main language of which is not Python
(to discard possible cases where Python is used as a
supporting tool only);

• forks.
This process resulted in 26,072 projects that we consider
to be of interest to us. pip requirements ﬁles are traditionally
named requirements.txt and saved in the root directory of the
project. Stemming from this convention, we detected all the
repositories that have such a ﬁle in the root of the HEAD
branch, and cloned them. This resulted in 7,876 repositories
cloned in November of 2020.

In order to analyze the evolution of project libraries, we
traversed the history of each project and collected versions
of requirements ﬁles from Novembers of previous years,
dating back to 2011. Naturally, the vast majority of projects
are younger than that, so older temporal slices have fewer
projects and fewer requirements ﬁles. The very ﬁnal step of
the data collection is to parse the collected requirement ﬁles.
We utilized a Python library called Requirements Parser [30],
which parses the ﬁle automatically and allows for a convenient
iteration over the dependencies. We skipped empty ﬁles and
ﬁles with all the requirements commented out. The ﬁnal tally
of the projects with parsed dependencies used in this study is
presented in Table I.

B. Libraries distribution

The collected dataset shows an extremely imbalanced dis-
tribution of libraries among the projects. Such a distribution
among the 7,132 projects in 2020 is shown in Figure 1.
Following the work by Theeten et al. [7], we conclude that the
distribution of libraries roughly follows the Zipf’s law. 89.3%
of all the dependencies are only encountered in 10 projects or
less, and as much as 54.6% appear only once. On the other end
of the spectrum, there is a handful of very popular libraries.
Top 10 most popular libraries are listed in Table II.1

1A lot of PyPi libraries will be mentioned in this paper. In order not to create
dozens of extra footnotes or references, let us say beforehand that you can ﬁnd
the information about them at https://pypi.org/project/name of the library/

TABLE I
THE AMOUNT OF PROJECTS WITH THEIR DEPENDENCIES
IN DIFFERENT TEMPORAL SLICES IN THE DATASET.

Year

Projects

Unique libraries

2011
2012
2013
2014
2015
2016
2017
2018
2019
2020

103
363
938
1,616
2,525
3,392
4,323
5,248
6,296
7,132

272
985
1,196
1,896
2,699
3,573
4,367
5,323
6,285
7,168

Fig. 1. The distribution of the dependencies among the projects in 2020.
The scale is logarithmic.

IV. EMBEDDINGS OF LIBRARIES

To use project dependencies as a source of information for
SE tasks, we need to represent them in a numerical way.
The dependency data is very sparse: each project depends
only on a handful of all libraries. Also, the distribution of
dependency frequencies is very skewed, with a small amount
of very popular libraries and a long tail of libraries which
occur as a dependency in few projects (see Figure 1).

Similar issues arise in the NLP (Natural Language Pro-
cessing) domain, where each document contains only a small
amount of all words, and frequently used words constitute a
small part of vocabulary. A common way to resolve both issues
is to use embeddings, dense numerical vectors that contain
information about word semantics. Following this idea, we
build embeddings of libraries and further analyze that they
are indeed meaningful and useful in the SE domain.

A. Building library embeddings

Following the early work in the word embeddings do-
main [31], we build the vectors with SVD (Singular Value De-
composition). As an alternative, we considered other popular
approaches to create embeddings such as language models [20]
or adaptation of word2vec [17]. However, both approaches
require sequential information to operate, whereas items in
requirement ﬁles do not have any particular order.

1101001000100001101001000Number of projects with the dependencyRank among the most popular dependenciesTABLE II
TEN MOST POPULAR LIBRARIES IN THE PROJECTS FROM 2020.

Library

# of projects % of all projects

requests
numpy
six
scipy
pyyaml
matplotlib
pillow
pandas
python-dateutil
tqdm

2,158
1,952
1,428
1,063
902
845
818
705
690
687

30.3%
27.4%
20%
14.9%
12.6%
11.8%
11.5%
9.9%
9.7%
9.6%

To build embeddings with SVD, we construct a co-
occurrence matrix M , where rows represent repositories (all
repositories from all versions) and columns represent libraries.
A matrix element MR,L equals to one if repository R directly
depends on library L, and zero otherwise. Then, we apply
SVD to decompose M into a product of three matrices U ,
Σ, and V . Σ is a matrix with singular values of M on its
main diagonal, rows of U are embeddings of repositories,
and columns of V are embeddings of libraries. Initially, the
dimensionality of embeddings equals to the total number of
repositories and libraries in the dataset, respectively. In order
to get small and meaningful vectors, we drop the components
that correspond to the smallest singular values, which leaves
us with embeddings of ﬁxed size d for both libraries and
repositories. In this study, we used the implementation of SVD
from scikit-learn [32]. We evaluated different dimensionalities
for this task and decided on using 32-dimensional embeddings,
more details on their comparison are presented in Section V-C.

B. Exploring embedding semantics

In order to evaluate whether the built embeddings of li-
braries contain semantic information, we clustered them with
the K-means algorithm [33] and explored the meaningfulness
of the constructed clusters. K-means works as follows. Firstly,
we ﬁx the number of clusters K and the algorithm chooses
K random cluster centers. In the main stage of K-means, we
iteratively assign items to the closest center to form clusters,
and recompute cluster centers as the mean of items in each
cluster. The implementation of K-means was also taken from
scikit-learn [32].

To choose K, we used the gap statistic technique [34].
We varied the number of clusters from 2 to 64, and for
each cluster partition computed the intra-cluster distance (i.e.,
the total distance from cluster vectors to the cluster center).
Then we computed the difference of intra-cluster distances
between the built cluster partitions and random clusterings of
the same size. With the increase of the number of clusters, the
difference increases, because more clusters allow to divide the
data more granularly. Figure 2 shows the resulting dependence.
We conclude that the difference begins to stabilize at K equals
44, hence we used this value to further study the vector space.
To analyze the clusters, for each cluster, we manually
inspected the most frequent libraries assigned to it, as well

Fig. 2. The dependence of the difference of intra-cluster distances between
the built cluster partitions and random clusterings of the same size on the
number of clusters when clustering the library embeddings with the K-Means
algorithm. The arrow indicates the saturation point.

as repositories, for which most of the dependencies belong to
this cluster. For convenience, we gave a short name to every
cluster. We also constructed a dendrogram shown in Figure 3
that illustrates the process of merging similar clusters into
larger groups. To build the dendrogram, we used the scikit-
learn [32] implementation of agglomerative clustering scheme
with cosine distance and average linkage.

It can be noticed that both the clusters and their relations
to each other are semantically sound. In the vast majority of
cases we had no trouble understanding and naming them. As
the dendrogram suggests, the clusters are divided into three
categories at the root.

The ﬁrst category consists of clusters 1 and 2, which are
the furthest away from the others. Cluster 1 is very broad
and domain-independent, it contains libraries that relate to
compatibility, for example, of Python versions. Cluster 2, in
contrast, is very speciﬁc and contains libraries used by the
OpenStack organization on GitHub.

The second large group of clusters contains Clusters 38–
44 and can be named Science and Machine Learning. The
presence of such a group can be expected, since Python is
extremely popular in scientiﬁc applications. In this group we
can see two clusters that deal with calculations, clusters that
deal with data analysis, statistics, and machine learning, as
well as libraries for plotting and visualizing data. It is within
this group of clusters that we can see the most popular data
science libraries like numpy, scipy, and pandas,
the most
popular libraries for machine learning and deep learning like
torch and scikit learn, and the most popular tools for plotting
graphs: matplotlib and seaborn.

Finally, the largest group of clusters contains Clusters 3–37
and might be called Web and applications. This group itself
divides into several other noteable sub-groups.

Aside from analyzing high-level information about the state
of the Python open-source ecosystem (e.g., the presence of
different domains and their relative sizes), the constructed
dendrogram can also provide more detailed insights. As an

01020304050607002505007501000125015001750Difference in intra-cluster distances with random clusteringNumber of clusters44 clustersFig. 3. The dendrogram of the constructed clusters.

1Interfaces for compatibility (six, fusepy, kitchen)2OpenStack (pbr, oslo.utils, oslo.conﬁg)3Zope (zc.buildout, zope.index, zope.app.basicskin)4Web utilities (pastedelpoy, html2text, pyramid)5Documentation, CLI (sphinx, pygments, docutils)6Graphics (pillow, pyopengl, pyproj)7Nose tests (nose, unittest2, nose_cov)8Distribution, SSH (boto, kombu, fabric)9Web applications (oauthlib, elasticsearch, sqlparse)10Django-based applications (gunicorn, psycopg2, south)11Django (django, djangorestframework, django_tables2)12XML, CSS, fonts (lxml, cssselect, pyquery)13PyObjC (pyobjc, pyobjc_core, pyobjc_framework_cocoa)14Eﬃcient data processing (pyparsing, tornado, jsonschema)15Machine Learning-based services (tensorﬂow, protobuf, keras)16HTTP, IPs, certiﬁcation (urllib3, certiﬁ, pysocks)17Server and API utilities (appdirs, attrs, aiohttp)18Authentication, connections (paramiko, websocket_client, rsa)19Cloud, AWS (boto3, botocore, s3transfer)20CLI (click, tabulate, pyaes)21Cryptography, encodings (cryptography, cﬃ, pyopenssl)22Python utilities and Apache (kazoo, cached_property, treap)23Databases, SQL (redis, pymongo, pymysql)24Low-level Web (sleekxmpp, pybluez, crontab)25UNIX servers (argparse, lockﬁle, python_daemon)26Web crawling + API (beautifulsoup4, youtube_dl, python_twitter)27Crawling Web (selenium, xlrd, tweepy)28Flask extensions and forms (sqlalchemy, ﬂask_sqlalchemy, wtforms)29Flask extensions (ﬂask_babel, ﬂask_mongoengine, ﬂask_oauthlib)30Web and remote control (ﬂask, paho_mqtt, pychromecast)31Packages, environments (wheel, setuptools, virtualenv)32CI tests (coverage, ﬂake8, pylint)33Pytest (pytest, pytest_cov, codecov)34Mock tests (mock, httpretty, freezegun)35Web frameworks (jinja2, werkzeug, peewee)36HTTP, requests (requests, slacker, tld)37CLI tooling (pyyaml, docopt, watchdog)38Visualization (matplotlib, wxpython, wordcloud)39Deep learning, NLP, image processing (torch, opencv_pyton, tqdm)40Data science (pandas, bokeh, dash)41Math, statistics, graphs (seaborn, statsmodels, sympy)42General machine learning (scikit_learn, nltk, hyperopt)43Scientiﬁc computations (scipy, cython, gensim)44High-performance computations (numpy, pyaudio, blosc)example of the latter, Tensorﬂow — one of the most popular
machine learning frameworks — belongs to Cluster 15, while
most of the other ML libraries belong to Clusters 37–44. After
a more detailed analysis of the repositories that depend on each
group of clusters, we found that Tensorﬂow is a more frequent
choice for online services that provide ML-based functionality
(e.g., face recognition or sentiment analysis in the cloud). This
insight also explains the relatively small distance between the
Tensorﬂow’s cluster and libraries related to AWS.

We conclude that the constructed library embeddings are
indeed meaningful and carry enough semantic information for
both high-level descriptive analysis and providing interesting
insights about the libraries. It leaves us with a strong basis to
further explore their practical applicability.

V. RECOMMENDING RELEVANT LIBRARIES
In order to prove the usefulness of the constructed library
embeddings in the SE domain, we employ the task of depen-
dency recommendation that was previously used by Theeten et
al. [7]. The rationale behind it is a rapid growth in the number
of open-sourced Python libraries: as the number of available
frameworks and tools grows, it is important to keep up with
the innovations and explore relevant libraries used in similar
projects.

Unfortunately, the previous work did not introduce a proper
way to compare the recommender systems. Thus, for the
evaluation of the proposed approach and the comparison of
different models, we need a benchmark with a ﬁxed deﬁnition
of a correct and an incorrect suggestion. In our work we
compiled such a benchmark using the historical data of the
previous versions of requirements ﬁles in our dataset.

However, such a benchmark is still synthetic to some degree
and does not fully capture the relevance and the usefulness of
the recommendation. To combat this, we also conduced a user
study about the suggested libraries. In this section, we will
describe the recommender systems that we used and both of
the mentioned evaluations.

A. Recommender systems

Firstly, we suggest a recommender system based on the
constructed library embeddings. The general idea of the the
approach is to ﬁnd repositories similar to the given one, look
at their dependencies and suggest those that are not already in
the query project. To ﬁnd the closest repositories, we need to
assign vectors to them as well.

We assign vectors to repositories as a mean of embeddings
of their dependencies. Then, given a repository R, we ﬁnd K
repositories closest to R according to cosine distance, where K
is a parameter chosen empirically. For each library L present
in these repositories that is not present in R, we compute their
relevance score as following:

scoreL = idf (L)α (cid:88)

(1 − dist(r, R))β,

(1)

r∈Nearest(R,L)

where α and β are hyperparameters, dist is cosine distance,
idf is inverse document frequency of the library in the entire

dataset [35], and summation is over repositories closest to
R that also contain L. The intuition behind this formula is
that we assign higher score to libraries that frequently occur
among repositories similar to R, and reweigh the score given
the library’s popularity represented by idf . Finally, the system
recommends libraries with the highest relevance score.

From here on out, we will refer to the system that represents
repositories as a mean of their libraries as Repositories by
Libraries Embeddings (RLE). To verify the adequacy of this
system, we compare it to three alternative approaches:

• Direct Repository Embeddings (DRE). Instead of assign-
ing the mean of dependency embeddings to each repos-
itory, we could use the embedding directly computed in
SVD (see Section IV-A). A downside of this approach is
its inability to handle new repositories: in order to build a
vector for them we would need to retrain the entire SVD
once again. Equation 1 stays the same.

• Jaccard. We could directly compute similarity of repos-
itories as a Jaccard distance [36] between sets of their
dependencies. While it might be more intuitive, searching
for similar repositories according to Jaccard distance
requires the computation of a set intersection with each
repository in the dataset, while for cosine distance we
can make the search asymptotically faster using K-D
Trees [37]. Equation 1 stays the same.

• Baseline. As a baseline, we use the recommendation of
the most popular libraries among all the repositories. This
recommendation is the same for any repository and pays
no attention to the query project.

B. Creating the benchmark

To create a benchmark for comparing the models and esti-
mate their best parameters, we decided to utilize the obtained
historical data, namely, the requirements ﬁles from different
years (see Table I). The idea is to compile the projects that
have actually added some libraries to them and try to suggest
these libraries.

More speciﬁcally, among the projects from 2011–2019 (i.e.,
all projects that have the next version), we selected all projects
that have:

• added at least one new library in the next version;
• did not change (add or remove) more than 10 libraries.
The second limitation removes only a handful of candidates,
but by manual checking we found a few cases where between
versions the project changed completely (moved to entirely
different domain), and trying to predict this is unreasonable.
Overall, the benchmark consisted of 4,678 projects. For
example, if project X had dependencies [A, B, C] in 2016
and dependencies [A, B, D] in 2017, the task would be: given
the repository with dependencies [A, B, C] and considering
only projects from 2016, suggest the dependency D.

When evaluating models on the benchmark, we only con-
sidered the neighbors from the projects in the same temporal
slice as the target project. The same goes for the baseline —
the list of the most popular libraries is calculated for the year

Fig. 4. The dependence of the RLE model’s MRR on the model parameters with the cosine similarity power ﬁxed at 0. The value in the square corresponds
to the Baseline.

of the query project. All of this is done to prevent the possible
cases of looking into the future.

For each of the models with each parameter conﬁguration,
we calculated the metrics commonly used for the recommen-
dation systems: precision among ﬁrst K suggestions, recall
among ﬁrst K suggestions, and MRR (Mean Reciprocal Rank).

C. Comparing the models on the benchmark

All the tested models, apart from the Baseline, allow us to
vary three parameters from Equation 1: α — the power of the
IDF term, β — the power of the cosine similarity term, and
K — the number of the closest repositories to consider.

• We varied the power of the IDF term from -3 to 2.
When this term is raised to the negative power, the model
facilitates recommending popular libraries, and when this
term is raised to the positive power, the model facilitates
suggesting more obscure, rare libraries. If the power is
equal to 0, the popularity of the library is not considered.
• We varied the power of the cosine similarity term from

0 (ignoring the distance altogether) to 2.

• We varied the amount of the nearest neighbors from 1 to

all of the repositories.

It should also be pointed out that using any of the models
with the terms’ powers equal to 0 and considering all the
repositories as neighbors results in the Baseline.

We discovered that the power of the similarity term does not
signiﬁcantly impact the metrics. However, the IDF term power
and the number of considered neighbors both did. Figure 4
shows the heatmap of the MRR value with a ﬁxed similarity
term power of 0. It can be seen that negative values of the IDF

term power demonstrate better results. A negative IDF term
power facilitates the model to predict more popular libraries,
and since the benchmark was built on historical data, such
libraries were naturally added more often. It can also be seen
that the best results are obtained when considering a moderate
number of neighbors (200–500), which backs up the idea of
using similar projects to make the suggestions. Finally, the
heatmap shows that the RLE model with optimal parameters
signiﬁcantly outperforms the Baseline.

MRR of the DRE model behaves very similarly, with the
best results obtained at the same parameters as the RLE model.
Probably, this has to do with the similar ways in which the
embeddings were obtained.

As for the Jaccard model, its relation to the powers of
the terms is the same, the main difference is that the best
results are obtained when considering 100 closest neighbors.
While RLE and DRE models utilize semantic similarity of
dependencies via embeddings and can identify similar projects
even when they have no dependencies in common, Jaccard
model relies on the explicit intersection of dependency sets.
Thus,
the number of meaningful neighbors to consider is
smaller in the Jaccard case.

Table III shows the overall results of different models with
the best parameters. All numbers are relatively low, because
the task is inherently very difﬁcult, but it can be seen that all
three models perform similarly and signiﬁcantly better than the
Baseline. This means that considering the closest repositories
is useful for the task, and each of the proposed models is
potentially usable. In practice, the RLE model has signiﬁcant
the
advantages over the others. As previously mentioned,

0,0480,0960,1170,1410,1620,1720,1790,1790,1800,1720,1590,1510,1490,0480,0970,1170,1400,1620,1720,1810,1810,1820,1760,1630,1510,1490,0480,0960,1130,1360,1550,1680,1780,1800,1840,1790,1640,1490,1470,0320,0740,0920,1140,1350,1500,1620,1700,1790,1780,1670,1470,1440,0230,0460,0630,0840,1050,1190,1290,1350,1450,1500,1480,1300,1250,0230,0260,0310,0440,0650,0840,0950,0970,1020,1080,1030,0830,07715102050100200300500100020005000All-3-2-1012Number of considered closest neighborsIDF term power0,0230,0550,0870,120,150,18MRRBASELINETABLE III
THE BEST RESULTS OF DIFFERENT MODELS ON THE BENCHMARK. α IS
THE POWER OF THE IDF TERM, β IS THE POWER OF THE COSINE
SIMILIARITY TERM, AND K IS THE NUMBER OF THE CONSIDERED
CLOSEST NEIGHBORS.

Model

Parameters

α

β

K

Prec@1

Rec@10 MRR

RLE
500
-1
DRE
500
-1
Jaccard
100
-1
Baseline — — —

2
2
2

0.104
0.105
0.104
0.075

0.184
0.193
0.195
0.140

0.185
0.189
0.186
0.144

Jaccard model requires signiﬁcantly more calculations as the
number of repositories grows, and the DRE model is not easily
expandable to new projects.

We built a simple CLI tool that runs the RLE model on
a user-deﬁned input. It requires only the requirements.txt ﬁle
of the target project and uses pre-calculated embeddings for
rapid searching of the nearest neighbors.

Having a benchmark also allowed us to compare different
dimensionalities of the SVD embeddings. We experimented
with ten different dimensionalities of the form 2n with n
ranging from 0 to 9. We considered both RLE and DRE
embeddings, and noticed that the quality of predictions sat-
urates at the dimensionality of 32. Increasing the number
of dimensions further does not improve the results on the
benchmark. Considering also the semantic meaningfulness of
the obtained embeddings (See Section IV-B), we believe that
32 dimensions is optimal for SVD in this task.

D. User study

The historical benchmark is a useful tool for comparing
the models and tuning hyperparameters. However, the ranging
metrics that we discussed do not guarantee practical value for
developers because of the synthetic nature of the compiled
benchmark. The goal of the recommendation system is to pro-
vide relevant and useful suggestions, and since these concepts
are inherently subjective, the evaluation should be carried out
on people. For this reason, we also conducted a user study.

Firstly, we needed to select repositories for the evaluation.
As discussed in Section IV-B, libraries can be assigned to
broader domains by their topic. To compile the user study,
we used ﬁve domains of Python projects mentioned by Lin
et al. [9]: NLP, Web, Media, Data processing, and Scientiﬁc
Calculations. As target repositories, we chose one repository
per domain from our dataset, speciﬁcally, from the 2020
temporal slice that was not present in the benchmark and,
in a sense, has no ground truth. Let us now describe the
repositories in each domain.

• NLP: Seq2seq Chatbot [38] – a lightweight implementa-

tion of a chatbot.

• Web: Octopus [39] – a server for accessing PowerShell

via HTTP(S).

• Media: Thumbnail Generator [40] – a tool for generating

thumbnail sprites from a video.

• Data Processing (DP): Fuel [41] – a framework for pro-
cessing datasets: working with popular formats, iterating,
editing.

• Scientiﬁc Calculations (SC): SciKit Kinematics [42] – a
collection of functions for working with 3D kinematics,
like rotation matrices and quaternions.

In our user study, we evaluated three best performing models
from Table III. We did not evaluate the baseline (the predic-
tions of which are analogous to libraries in Table II) in order
not to diffuse the user study with the same general suggestions,
but rather focus on different domain-speciﬁc suggestions.

For each of the ﬁve projects, we used the best perform-
ing models to generate Top-5 most relevant suggestions and
merged all of them into a single list in case of repetitions. All
participants were shown all the suggestions for a given project
in random order and were asked three questions about each
suggestion, with the answers being a scale from 1 to 5:

I Is this suggestion relevant to the topic of the project?

1 – not relevant at all, 5 – very relevant.
II Would you use this library in this project?

1 – no, the library either does not bring any value or
requires too much technical work to use in this project,
5 – yes, it is valuable and does not require signiﬁcant
efforts to start using in the present project.

III Would you consider using it if you started another

similar project?
1 – no, the library cannot bring any value to similar
projects, 5 – yes, the library can be useful as an addition
or as a replacement of one of the currently used tools.

In total, eight experienced Python developers participated
in our study. The averages of their answers are presented in
Table IV. Several observations can be made from them.

First of all, it can be seen that the results are very unequal
between domains. For three domains (NLP, Web, and Data
Processing), embeddings-based models demonstrated the av-
erage scores in questions I and III better than 3, indicating the
overall relevant and potentially useful recommendation. At the
same time, the results for the other two domains (Media and
Scientiﬁc Calculations) are noticeably and signiﬁcantly worse.
Even though in our work we only have one repository
per domain, during our preliminary experiments we have
seen similar cases for other repositories in these domains.
Figure 3 sheds the light on this discrepancy. All the domains
that showed better results are well presented in the clusters,
and at the same time, Media and Scientiﬁc Calculations are
both mentioned in just one cluster, which makes them under-
represented. The models consider 100 or even 500 closest
repositories, so if there are signiﬁcantly less related projects,
the unrelated projects might bring up unrelated suggestions.
This is exactly what happened to Thumbnail Generator: since
there are few Media-related projects in our dataset, the closest
projects contained a lot of weakly related repositories, and
the suggestions of all three models contained just generally
popular libraries, some of which you can see in Table II:
numpy, six, requests, etc.

TABLE IV
THE RESULTS OF THE USER STUDY. THE ANSWERS TO ALL THREE QUESTIONS FOR EACH OF THE LIBRARY PREDICTED BY A GIVEN MODEL WERE
AVERAGED FOR ALL THE PARTICIPANTS. THE LAST THREE COLUMNS SHOW THE AVERAGE OF THE RESULTS FOR ALL FIVE REPOSITORIES.

Domain

Question

By libraries
Direct
Jaccard

NLP

II

2.7
2.5
2.1

III

3.3
3.2
2.6

I

3.4
3.2
2.5

Web

II

2.3
2.2
1.6

I

3.3
3.1
1.8

III

3.1
3.0
1.8

Media

I

1.7
1.8
1.5

II

1.5
1.7
1.5

III

1.7
2.0
1.6

DP

II

2.8
2.9
3.3

I

3.4
3.6
4.0

III

3.2
3.6
3.9

SC

II

2.1
1.8
1.6

III

2.5
2.1
1.9

I

2.7
2.2
2.0

Average

I

2.9
2.8
2.4

II

2.3
2.2
2.0

III

2.8
2.8
2.3

Even though the two under-represented domains bring down
the overall average, the approach shows good results for the
domains that are popular in Python. While Table IV shows
the average results of all the suggested libraries, Table V
shows the score of the best library for every model in regard
to Question III about usefulness in a similar project. It can
be seen that for the three well-represented domains, Top-
5 suggestions of our embeddings-based models contained
libraries that were voted overwhelmingly useful.

TABLE V
THE HIGHEST SCORE OF A SINGLE BEST LIBRARY PREDICTED BY EACH
MODEL IN REGARD TO QUESTION III.

Domain

NLP Web Media

RLE
DRE
Jaccard

4.3
4.3
3.6

4.0
4.0
3.2

2.7
2.7
2.3

DP

4.6
4.3
4.6

SC

3.0
2.9
2.9

Another conclusion that can be drawn from Table IV
and Table V is that on average, embeddings-based models
demonstrate better results than the Jaccard model. The RLE
model was able to suggest all the libraries that received the
highest scores in all questions. Overall, this can be viewed as
another advantage of using embeddings for a given task.

Finally, Table IV demonstrates another important implica-
tion: for all the models in all the domains, and on average, the
score in Question II is lower than the scores in Questions I and
III. This means that using the suggestions in the given project
is more difﬁcult than in a similar project, and high relevance
does not guarantee the ability to use the library right away.

To investigate this further, we asked the participants to
comment on why they found certain libraries relevant but
not useful. There were two most common answers. Firstly,
all the suggested libraries are very popular and therefore the
include them might be voluntary. This is
decision to not
not unusual, because, as we discussed before, all the best-
performing models use the negative power of the IDF term,
and therefore suggest more popular libraries. Secondly, the
suggested libraries can be analogous to the ones that are
already in the project: even if they might bring additional
value, it might be difﬁcult to integrate them to a mature project.
Overall, the demonstrated results show the potential of the
proposed approach. It is important to keep in mind how little
information this method uses to make the decisions. However,
the results of the user study point to a fundamental discrepancy
between the relevance and the usefulness of the suggestions.

E. Exploration of libraries

As a ﬁnal part of our study, we decided to investigate
this problem a bit deeper. Let us demonstrate the discrepancy
between the relevance and the usefulness of the suggestions
by considering the ﬁrst of our studied projects, the ChatBot.
Its dependencies are somewhat typical for a Machine Learning
project and include numpy — a library for scientiﬁc calcula-
tions, scikit learn — a library for data analysis, and tensorﬂow
— one of the most popular machine learning frameworks.

Now let us look at

the recommendations made by the
RLE model. The top suggestions are relevant to the topic
of the project and include torch – another popular machine
learning framework, pandas – another data analysis tool, and
matplotlib – one of the most popular libraries for plotting and
visualization. Naturally, such suggestions are more relevant
than, for example, the Baseline, and can be helpful in search
engines or package management services, where they can
guess what the developer might search for. However, if we are
looking for libraries that can add something new to the project,
these suggestions turn out to be less useful: it is doubtful that
developers will rewrite the application in another framework.
If we go back and look at the distributions of libraries
demonstrated in Section III-B and in Figure 1,
they are
very unequal, with a handful of very popular libraries and
a long “tail” of rare ones. It might be of interest to suggest
such rare libraries to allow the developers actually discover
some potentially useful libraries. We refer to this mode of
recommendation as exploration.

The parameters of our models can facilitate such explo-
ration. Firstly, and more obviously, we can increase the power
of the IDF term, therefore incentivizing the models to predict
rarer libraries. Secondly, we can decrease the number of
considered closest neighbors, therefore making the possible
suggestions more precise.

Choosing the parameters of models for such a task is very
difﬁcult and requires complex manual evaluation. We leave
this for the future work, but demonstrate a couple of hand-
picked examples for the RLE model with the the IDF term
power of 3 and the number of closest neighbors of 50.

If we use these settings for the Chatbot, the ﬁrst ﬁve sug-
gestions include keras-bert – an implementation of the BERT
Language model, and two libraries that were encountered only
once in the entirety of our dataset: lemminﬂect – a library
for lemmatizing English words and language-tool-python – a
grammar and spelling checker. These libraries relate closely

to the topic of the repository and there is a very good chance
that the developer has not heard of them.

Of course, boosting up very rare libraries is more risky, and
other suggestions can be totally irrelevant. There are possible
ways of mitigating this, for example, clustering the repositories
and considering not just the closest projects, but only the
projects within the same cluster. Overall though, this approach
may be useful for discovering interesting libraries that are
otherwise shadowed by the most popular ones.

VI. THREATS TO VALIDITY

The general large-scale nature of our study gives way to

certain treats to validity.

Parsing. When parsing the requirements, we took into
account the possible upper and lower case, as well as the
fact that the symbols “-” and “ ” are interchangeable in the
pip formatting. However, there exist speciﬁc cases where the
same library can have different aliases. For example, a popular
Web scraping library beautifulsoup4 is often called by its short
name bs4, and the developers of the library have uploaded
a dummy placeholder package to the abbreviated name that
ensures the downloading of the same library and prevents
possible misunderstandings. If there are other such cases in the
dataset, our approach will not be able to match such aliases,
however, such cases are rare.

Python versions. In this work, we also do not differentiate
between Python3 and Python2 and can therefore suggest
deprecated packages. However, all the most popular libraries
are well-maintained and do not have this problem, whereas
the exploration suggesting mode is intrinsically unstable.
Moreover, when suggesting relevant libraries, only the project
versions from the same year are taken into account, which can
also prevent recommending old libraries.

Indirect dependencies. Some libraries have dependencies
of their own that do not need to be in the requirements ﬁles
or be explicitly used but are installed automatically. For this
reason, there is a possibility that our approaches might suggest
libraries that are already installed and used under the hood
in the developer’s project. However, the goal of suggesting
libraries is to advise the developer to take a look at the library,
and even if it is used under the hood, the developer might not
know it closely, and the suggestion might still be useful.

Choice of projects. In our user study, we only selected one
project per domain. This has to do with the fact that labeling
several predictions for several models for each repository
constitutes a lot of work. However, this only directly impacts
one of our observations that relates to the differences between
domains, while the other ones deal with the averaged data of
all ﬁve projects. We leave further extended experiments for
future work.

VII. CONCLUSION & FUTURE WORK

In this paper, we propose an idea of creating lightweight em-
beddings of libraries and projects based on their co-occurrence.
We gathered a dataset consisting of requirements ﬁles of
7,132 Python projects and pulled up their versions from the

years 2011–2020. Based on this data, we obtained dense 32-
dimensional vectors of libraries by applying SVD to the co-
occurrence matrix of projects and libraries.

We evaluated the semantic meaningfulness of the extracted
embeddings by clustering them with K-means and manually
inspecting the clusters. The clusters themselves and their
relative positioning demonstrate that the embeddings carry
with themselves a lot of meaning and can be used for making
relevant suggestions.

Based on this, we developed an approach to suggest rele-
vant libraries to the developers of a given project based on
the project’s dependencies. The idea of the approach is to
search for similar repositories and suggest their dependencies
for consideration. To identify similar repositories, we tried
embedding-based approaches and Jaccard similarity between
the dependency sets. We also used a baseline of simply
suggesting the most popular libraries overall.

To test and compare models, we compiled a benchmark
based on the historical data from our dataset. The mod-
els suggested libraries to the projects in a given year, and
tried to guess the actually added dependencies. The baseline
demonstrated the MRR of 0.144, while all similarity-based
models demonstrated similar results, with the highest MRR
of 0.189. We also implemented the approach based on library
embeddings as a CLI tool for others to try.

To complement the comparison of models on a synthetic
benchmark, we conducted a user study. We used the similarity-
based models to suggest libraries for ﬁve different projects,
and asked the participants to evaluate their relevance and
usefulness. The study showed that
the suggestion quality
varies between the project domains. Also, it turns out that the
recommendation of popular libraries might not actually bring
any new knowledge to the developer. Therefore, we introduced
another mode for our recommender system called exploration
that facilitates the system to score rare libraries higher.

Overall, we believe that the proposed embeddings might be
of interest and of use for various tasks. They are very simple
to obtain, require no text or code analysis, but at the same time
allow us to differentiate between different domains. There is
a number of ways to continue this research:

• It is possible to test the proposed approach for different

languages and package managers.

• There is a need to study the process of obtaining
embeddings in greater detail, and to compare different
approaches to this task.

• It is of interest to carry out a user study of the recom-
mendation models on a larger scale, considering different
ways of suggesting relevant libraries, including the explo-
ration mode.

We believe that it is very important to consider various
sources of information when analyzing the software ecosys-
tem, and hope that our work can be helpful in this regard.

ACKNOWLEDGMENTS

We would like to thank Viktor Tiulpin for his help and

expertise with GHTorrent and data collection.

REFERENCES

[1] E. Bogomolov, Y. Golubev, A. Lobanov, V. Kovalenko, and T. Bryksin,
“Sosed: a tool for ﬁnding similar software projects,” in 2020 35th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE), 2020, pp. 1316–1320.

[2] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating
sequences from structured representations of code,” in International
Conference on Learning Representations, 2019. [Online]. Available:
https://openreview.net/forum?id=H1gKYo09tX

[3] A. Svyatkovskiy, Y. Zhao, S. Fu, and N. Sundaresan, “Pythia: Ai-assisted
code completion system,” in Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining,
2019, p. 2727–2735.

[4] P. Thongtanunam, C. Tantithamthavorn, R. G. Kula, N. Yoshida, H. Iida,
and K. Matsumoto, “Who should review my code? a ﬁle location-based
code-reviewer recommendation approach for modern code review,”
in 2015 IEEE 22nd International Conference on Software Analysis,
Evolution, and Reengineering (SANER), 2015, pp. 141–150.

[5] V. Kovalenko, E. Bogomolov, T. Bryksin, and A. Bacchelli, “Building
implicit vector representations of individual coding style,” in Proceed-
ings of
the IEEE/ACM 42nd International Conference on Software
Engineering Workshops, 2020, p. 117–124.

[6] V. Klema and A. Laub, “The singular value decomposition: Its compu-
tation and some applications,” IEEE Transactions on Automatic Control,
pp. 164–176, 1980.

[7] B. Theeten, F. Vandeputte, and T. Van Cutsem, “Import2vec learning
embeddings for software libraries,” in Proceedings of the 16th Interna-
tional Conference on Mining Software Repositories, 2019, p. 18–28.

[8] N. Craswell, Mean Reciprocal Rank, 2009, pp. 1703–1703.
[9] W. Lin, Z. Chen, W. Ma, L. Chen, L. Xu, and B. Xu, “An empirical
study on the characteristics of python ﬁne-grained source code change
types,” in 2016 IEEE international conference on software maintenance
and evolution (ICSME), 2016, pp. 188–199.

[10] Y. Belinkov and J. Glass, “Analysis methods in neural language pro-
cessing: A survey,” Transactions of the Association for Computational
Linguistics, pp. 49–72, 2019.

[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in Neural Infor-
mation Processing Systems, 2012, pp. 1097–1105.

[12] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” CoRR, vol. abs/1409.0473, 2015.
[13] P. Covington, J. Adams, and E. Sargin, “Deep neural networks for
youtube recommendations,” in Proceedings of the 10th ACM Conference
on Recommender Systems, 2016.

[14] P. Fernandes, M. Allamanis, and M. Brockschmidt, “Structured neural
summarization,” in 7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.

[15] V. J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber,

“Global relational models of source code,” in ICLR, 2020.

[16] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, “Neural-
machine-translation-based commit message generation: How far are
we?” in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, 2018, p. 373–384.

[17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their composition-
ality,” in Advances in Neural Information Processing Systems, vol. 26,
2013, pp. 3111–3119.

[18] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” Transactions of the Association for
Computational Linguistics, pp. 135–146, 2017.

[19] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors
for word representation,” in Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), 2014,
pp. 1532–1543.

[20] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” CoRR,
vol. abs/1802.05365, 2018.

[21] C. Teyton, J. Falleri, and X. Blanc, “Mining library migration graphs,”
in 2012 19th Working Conference on Reverse Engineering, 2012, pp.
289–298.

[22] I. Pashchenko, H. Plate, S. E. Ponta, A. Sabetta, and F. Massacci,
“Vulnerable open source dependencies: Counting those that matter,”
in Proceedings of the 12th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement, 2018.

[23] G. Bavota, G. Canfora, M. D. Penta, R. Oliveto, and S. Panichella, “The
evolution of project inter-dependencies in a software ecosystem: The
case of apache,” in 2013 IEEE International Conference on Software
Maintenance, 2013, pp. 280–289.

[24] “Package installer for Python,” https://pip.pypa.io/en/stable/, [Online.

Accessed 05.09.2022].

[25] “Python

package

index,”

https://pypi.org/,

[Online. Accessed

05.09.2022].

[26] G. Gousios, “The ghtorrent dataset and tool suite,” in Proceedings of
the 10th Working Conference on Mining Software Repositories, 2013,
pp. 233–236.

[27] “GHTorrent dumps,” http://ghtorrent-downloads.ewi.tudelft.nl/mysql/,

[Online. Accessed 05.09.2022].

[28] “pga-create,”

https://github.com/src-d/datasets/tree/master/

PublicGitArchive/pga-create, [Online. Accessed 05.09.2022].

[29] V. Markovtsev and W. Long, “Public git archive: A big code dataset
for all,” in Proceedings of the 15th International Conference on Mining
Software Repositories, 2018, pp. 34–37.

[30] “Requirements Parser,”

https://pypi.org/project/requirements-parser/,

[Online. Accessed 05.09.2022].

[31] J. Steinberger and K. Jeˇzek, “Text summarization and singular value
decomposition,” in Advances in Information Systems, T. Yakhno, Ed.,
2005, pp. 245–254.

[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, pp. 2825–2830, 2011.

[33] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman,
and A. Y. Wu, “An efﬁcient k-means clustering algorithm: analysis and
implementation,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, pp. 881–892, 2002.

[34] R. Tibshirani, G. Walther, and T. Hastie, “Estimating the number of
clusters in a data set via the gap statistic,” Journal of the Royal Statistical
Society: Series B (Statistical Methodology), pp. 411–423, 2001.
[35] K. S. Jones, “A statistical interpretation of term speciﬁcity and its

application in retrieval,” Journal of Documentation, pp. 11–21, 1972.

[36] P. Jaccard, “The distribution of the ﬂora in the alpine zone.1,” New

Phytologist, vol. 11, no. 2, pp. 37–50, 1912.

[37] J. L. Bentley, “Multidimensional binary search trees used for associative

searching,” Commun. ACM, p. 509–517, 1975.

[38] “Seq2Seq Chatbot — Chatbot

in 200 lines of code using Ten-
sorLayer,” https://github.com/tensorlayer/seq2seq-chatbot/, [Online. Ac-
cessed 05.09.2022].

[39] “Octopus — Open source pre-operation C2 server based on Python and
[Online. Accessed

Powershell,” https://github.com/mhaskar/Octopus/,
05.09.2022].

[40] “Video

generator,”
video-thumbnail-generator/, [Online. Accessed 05.09.2022].

thumbnail

https://github.com/ﬂavioribeiro/

[41] “Fuel — A data pipeline framework for machine learning,” https:

//github.com/mila-iqia/fuel/, [Online. Accessed 05.09.2022].

[42] “scikit-kinematics — Python functions for working with 3D kinemat-
ics,” https://github.com/thomas-haslwanter/scikit-kinematics/, [Online.
Accessed 05.09.2022].

