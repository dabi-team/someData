Applicability of Software Reliability Growth
Models to Open Source Software

Radoslav Miˇcko
Faculty of Informatics
Masaryk University
Brno, Czechia
r.micko@mail.muni.cz

Stanislav Chren
Faculty of Informatics
Masaryk University
Brno, Czechia
chren@mail.muni.cz

Bruno Rossi
Faculty of Informatics
Masaryk University
Brno, Czechia
brossi@mail.muni.cz

2
2
0
2

n
u
J

4
2

]
E
S
.
s
c
[

2
v
9
9
5
2
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—Software Reliability Growth Models (SRGMs) are
based on underlying assumptions which make them typically
more suited for quality evaluation of closed-source projects and
their development lifecycles. Their usage in open-source software
(OSS) projects is a subject of debate. Although the studies
investigating the SRGMs applicability in OSS context do exist,
they are limited by the number of models and projects considered
which might lead to inconclusive results. In this paper, we present
an experimental study of SRGMs applicability to a total of 88
OSS projects, comparing nine SRGMs, looking at the stability of
the best models on the whole projects, on releases, on different
domains, and according to different projects’ attributes. With
the aid of the STRAIT tool, we automated repository mining,
data processing, and SRGM analysis for better reproducibility.
Overall, we found good applicability of SRGMs to OSS, but
with different performance when segmenting the dataset into
releases and domains, highlighting the difﬁculty in generalizing
the ﬁndings and in the search for one-ﬁts-all models.

Index Terms—Software Reliability Growth Models, Open
Source Software, Cumulative Software Failure Data, Mining
Software Repositories

I. INTRODUCTION

Software Reliability is one of the most important attributes
of software systems. Software Reliability Growth Models
(SRGMs) emerged in the ’70es when Jelinski and Moranda
proposed the JM model for ﬁtting cumulative failure data
aiming at providing a methodology to analyze and predict
project failures over time [1], [2].

However, during the years, the software development pro-
cess has been evolving considering more agile and open source
development processes, challenging many of the assumptions
of the models that were developed over time — like the fact
that the defect ﬁxing process does not introduce new defects or
that no new code is introduced during the testing process [3].
The goal of this paper is to investigate the applicability of
Software Reliability Growth Models (SRGM) to a large set of
Open Source Software (OSS) projects. For the analysis, we
adopted and customized STRAIT [4], a previously developed
tool to provide SRGM analysis. With the aid of the tool,
we ﬁt 9 different SRGM models to cumulative failure data
from 88 OSS projects. Overall, considering projects as a
whole and software releases, we ﬁt 1 053 SRGMs — that, to
our knowledge represents the largest empirical study on OSS

SRGMs. We further provide all the datasets and outputs from
the STRAIT tool [5]. The three main contributions of this
paper go into studying SRGMs in more detail in i) project
domains, ii) releases, all in the context of a iii) large-scale
study compared to related works.

i) Domains: we want to investigate whether different do-
mains might follow different lifecycles and have differ-
ent operational proﬁles thus resulting in varied issue-
reporting patterns, which might affect the SRGM anal-
ysis.

ii) Project releases: considering project releases can make
the reliability growth trend more prominent, thus affect-
ing the parameter estimation and the GoF. Some of the
related studies conducted the SRGM analysis on the issue
reports from the whole project history, some divided
the data based on releases. We wanted to replicate both
approaches on a larger sample of projects.

iii) Large scale:

in this paper, we demonstrate a large-
scale SRGM analysis (1 053 SRGMs, ∼400K software
defects) with the help of the STRAIT tool [4] to motivate
future replicability of the results and reuse of datasets in
different SRGM studies.

The paper is structured as follows. In Section II, we provide
a brief background for the SRGM process. In Section III
we discuss the peculiarities of OSS for the applicability of
SRGMs. In Section IV, we summarize the related work to
further motivate our study. Section V details the methodology
of the experiments with the results presented in Section VI.
We discuss the potential threats to validity in Section VII and
conclusions in Section VIII.

II. SOFTWARE RELIABILITY GROWTH MODELLING

During the testing and early operation phases of the software
life cycle, failure events are encountered. They are recorded
and the underlying faults that caused them are removed, which
results in a process called reliability growth. SRGMs are
regression-based models whose purpose is to estimate the
parameters of a mean value function m(t) based on the input
data: m(t) represents cumulative number of faults detected by
the given time t.

Accepted version of R. Micko, S. Chren and B. Rossi, ”Applicability of Software Reliability Growth Models to Open Source Software,” to appear in 48th
Euromicro Conference Series on Software Engineering and Advanced Applications (SEAA’22)

 
 
 
 
 
 
In this study, we consider nine models shown in Table I.
These are the most important models that were deﬁned over
time to study reliability growth in cumulative failure data.

TABLE I
LIST OF APPLIED SRGMS

Model
Goel-Okumoto (GO) [6]
Goel-Okumoto S-Shaped (GOS) [6]
Hossain-Dahiya (HD) [7]
Musa-Okumoto (MO) [8]
Duane (DU) [9]

Weibull (WE) [10]

Type
Concave
S-Shaped
Concave
Inﬁnite
Inﬁnite

Concave

Yamada Exponential (YE) [11]

Concave

Yamada Raleigh (YR) [11]
Log-Logistic (LL) [12]

S-Shaped
S-Shaped

µ(t)
a(1 − e−bt)
a(1− (1 + bt)e−bt)
a(1 − e−bt)/ (1 + ce−bt)
αln(βt + 1)
αtβ
a(1 − e−btc
a(1 − e−rα(1−e−βt ))
a(1 − e−rα(1−e−βt2/2))
a(λt)κ/ (1 + (λt)κ)

)

Based on the shape of m(t), we can classify SRGMs into

three main categories [1]

• Concave models – they assume that the total number of
faults in software is ﬁnite, and that it is possible to achieve
fault-free software in ﬁnite time.

• S-shaped models – they also assume that the total number
of faults is ﬁnite. and that early testing is not as effective
in fault discovery as the testing in the later stages.
Therefore, there is a period in which the number of faults
is increasing.

• Inﬁnite models – they assume that it is not possible to
develop fault-free software because during fault removal
we can introduce new ones.

The ﬁrst step of the SRGM analysis process is to prepare
the input data, such as bug reports obtained during testing or
operation. The reports should be processed ﬁrst to identify the
faults which caused the failures and only these faults should be
considered in SRGM analysis. The bug reports can be further
ﬁltered to better ﬁt into the SRGMs assumptions [13].

The second step is to perform a statistical test such as
Laplace’s trend test [14] to determine whether a trend of
decreasing number of failures can be observed in the data.
If such a trend cannot be observed, it means there is no
signiﬁcant reliability growth in the data and the SRGMs might
not provide reasonable results.

Next, the SRGM is selected for the application. It is recom-
mended that multiple models are used and the one describing
the data most accurately is selected. To get results from
SRGMs, the model’s parameters need to be estimated based
on input data. For parameter estimation, either the Maximum
likelihood method or the Minimum least-squares method are
typically used [1].

After parameter estimation, a goodness-of-ﬁt

test (GoF)
is performed to evaluate how well the model ﬁts the data
to choose the most appropriate SRGM. In related studies
(Table II), there are several most adopted GoF metrics: the
R-squared a.k.a. coefﬁcient of determination (R2) gives the
amount of the model explaining the variability of the data [10],

Akaike Information Criterion (AIC) provides the quality of the
model [15], Bayesian Information Criterion (BIC) represents
the posterior probability of a model given the data [15],
and Residual Standard Error (RSE) measuring the standard
deviation of the residuals in the regression model [16].

The selected SRGMs can be then used to predict a variety
of failure data, such as future failure intensity, number of
remaining faults, or testing effort required to achieve a given
reliability level.

III. SRGM & OSS

lifecycle. For instance,

The application of SRGMs in the OSS domain faces sev-
eral challenges mainly due to underlying assumptions of the
traditional SRGMs [1]. These models were designed with a
closed-source software environment reﬂecting the waterfall
the models assume a
development
perfect debugging process in which the detected fault
is
immediately removed. However, in the OSS context, such
assumption would be unrealistic due to the inﬂuence of various
factors, such as non-homogeneous groups of developers and
users both of which can contribute to the bug reports during
the development. Additionally, the contributors are equipped
with different skills, tools, and resources which may lead to
potential regressions with new bugs introduced and thus a
rather non-linear debugging process [17].

The main issues in the applicability of SRGMs to OSS are
based on the assumptions about the testing and faults repairing
process. Some of these assumptions might be violated in the
context of modern systems/development practices [3]:

• Faults are repaired immediately when they are discov-
ered – in reality, faults are not repaired immediately, but
this can be partially eliminated by ﬁltering out duplicated
reports with a combination of using only resolver reports.
• Fault repairs are perfect – fault repair likely introduces
new faults. Because the re-test for the code is not as
thorough as the initial testing, the new faults are less
likely to be identiﬁed.

• No new code is introduced during testing – new code
is frequently introduced throughout the entire test period
(faults repair and new features), especially for agile de-
velopment. This is accounted for in parameter estimation
since real faults discoveries are used. However, the shape
of the curve may be changed (i.e., make it less concave).
• Defects are only reported by the product testing group –
it does not apply to open source projects, where the whole
community of users and developers can submit reports.
• Each unit of time is equivalent – it can be accounted
for as long as test sequences are reasonably consistent.

• Tests represent operational proﬁle – often,

the op-
erational proﬁle is uncertain because of the lack of
usage statistics. However, nowadays, many applications
(including OSS projects) collect various usage/telemetry
data (e.g., execution steps/scenarios, frequency of certain
inputs, actions) so this might not be the issue anymore.

• Failures are independent – it is reasonable except when
a section of code has not been as thoroughly tested as
other code.

The impact of models’ assumption violations is difﬁcult to
estimate. For example, introducing a new functionality may
make the curve less concave, whereas test re-runs could make
it more concave.

Furthermore, in most cases, the testing period is not publicly
deﬁned for OSS. Ideally, the reports should be processed ﬁrst
to distinguish the faults which caused the failures, and only
these faults should be taken into account in SRGM analysis.
This process would be very time-consuming. Therefore, it
is possible to process the bug reports to ﬁlter out duplicate
entries. Afterward, reports are considered as a substitute for a
fault [13].

Given the uncertainties about the effects of violating model
assumptions,
the best strategy for OSS is to try multiple
models and see the models that could be better for the
prediction of failures.

IV. RELATED WORK

In the context of OSS, multiple studies focused on a
comparison of SRGMs to see which model ﬁts best the defect
data originating from OSS repositories. We list these studies
in Table II detailing the SRGMs used, the best performing
models,
the number of OSS projects investigated and the
GoF measures applied. Mohamed et al. [18] focused on the
defect classiﬁcation in OSS projects and how different types
of defects impact the SRGM analysis. Rahmani et al. [19],
Zhou et al. [20], Rossi et al. [21], Fengzhong et al. [22], and
Ullah et al. [23] studied bug arrival processes in OSS projects
in relation to SRGMs. Out of those, authors in [20] and [23]
compared the results also with the industrial and closed-source
datasets. Furthermore, the study in [19] investigated also the
correlation of bug arrival patterns and project popularity with
the SRGM results.

Interestingly, even though the studies overlapped in the ap-
plied SRGMs, the results and conclusions were often different.
For example, Rossi et al. [21] determined that WE is overall
the best performing model. Rahmani et al. [19] agreed with
the performance of WE in terms of GoF, but in terms of
predictive power, they considered WE as the worst model.
Zhou et al. [20] concluded that closed-source projects and
OSS exhibit similar bug arrival patterns making the traditional
SRGMs suitable for OSS datasets with WE being the best
model. On the other hand, Fengzhong et al. [22] argued that
traditional SRGMs were largely unsuitable and proposed a
new type of reliability model instead. Similarly, Wang et al.
[17] also proposed a new SRGM targeting the OSS projects
and showed that their new model outperformed the traditional
ones.

V. EMPIRICAL EVALUATION

The goal of the empirical evaluation is to Compare 9
SRGMs on 88 OSS Projects in terms of several GoF measures
deriving the performance of the models in the whole dataset,

TABLE II
SUMMARY OF RELATED STUDIES ABOUT THE APPLICATION OF SRGM IN
OSS.

Ref Models

Best Models

GOS, SCH, WE
GO, GOS

[19]
[18]
[20] WE
[21] WE, WES, GO, GOM
GOS, HD, YE
GO, HD, LP

[24]

WE
--
WE
WE

LP

[23] MO, HD, GO, GOS,
WE, GOM, LOG, YE
GO, GOS, ISS, PNZ,
PZ, WM, L, W

[17]

GOM, HD

W

OSS
Projects
5
2
8
3

GoF

R2
R2
R2
R2, AIC

1

6

3

AIC,
MSE
R2

R2,
MSE

*SCH - Schneidewind model [19], WES - Weibull S-Shaped model [25], GOM
- Gompertz model [26], LOG - Logistic model [27], LP - Logarithmic Poisson
Execution Time model [24], MSE - mean squared error, ISS - Inﬂection S-
Shaped model [28], PNZ - Pham-Nordmann-Zhang model [29], PZ - Pham-
Zhang model [30], WM - Wang-Mi model [31], L - Li model [32], W - Wang
model [17]

in the single domains, and considering for each project the
software releases.

A. Research Questions

RQ1 What is the ranking of the models based on GoF ? We
investigate the GoF values for all the models applied
to OSS projects. We compare mean/SD values for GoF
and run Kruskal-Wallis and Dunn’s tests to identify the
ranking over all the projects.

RQ2 How does the project’s domain affect the GoF of models?
Projects from different domains might indicate different
applicability of SRGMs. We will compare mean/SD val-
ues for GoF and run Kruskal-Wallis and Dunn’s tests to
identify the signiﬁcant differences of the models.
RQ3 Does project division into OSS releases change the ap-
plicability of SRGMs? We compare the applicability of
SRGMs to the whole project and to releases of the same
project. We compare mean/SD values for GoF to identify
this difference on whole projects and separately on their
releases.

RQ4 How much do different project attributes (like lines of
code, number of issue reports, number of faults) impact
on the applicability of SRGMs? The rationale is to
understand if these factors imply different applicability of
the models. We segment the projects according to three
categories depending on the levels of the attributes and
we compare the ranking of the models in terms of R2.

B. Data Collection

We used STRAIT [4] to mine data from GitHub bug
tracking repositories. To get a more heterogeneous sample of
projects, we have chosen the top ten projects from different
topics of the ”Topic Lists” and combined them with ten
more projects from the ”Trending List”. We focused on the
issues that were declared ”bug”, ”error”, ”fail”, ”fault” or
”defect” excluding any issue that was marked as ”duplicated”.
An example of part of the output of the STRAIT tool is

Fig. 2. QQplots of ANOVA’s residuals for all models R2

Fig. 1. Partial STRAIT Output for project Ansible for YR and LL models

available in Fig. 1. After ﬁltering the projects which were data
repositories, we deﬁned the ﬁnal set of 88 projects divided into
the categories shown in Table III. All the mined projects and
STRAIT’s outputs are available on Figshare [5]. More details
about the process are also available in [33].

TABLE III
THE NUMBER OF PROJECTS BASED ON THEIR CATEGORIES AND THE AVG.
NUMBER OF DEFECTS PER PROJECT.

Category

C1 Admin/monitoring
C2 Cryptocurrency
C3 DB and data analysis
C4 Multimedia

# Defects
(AVG.)
10012
2527
3327
5286

14
9
10
10

Category

C5 SW Development
C6 System/OS tools
C7 Text processing
C8 Web/Networking

# Defects
(AVG.)
1876
8671
833
1017

16
10
7
12

C. Methodology

To answer the research questions, we ﬁtted the nine SRGM
models on the cumulative number of failures for each project.
For models to converge, we started with the initial approx-
imation of parameters with 100K algorithm iterations. We
then used these initial parameters approximation as input for
the original nls - Nonlinear Least Squares [34] method. We
allowed a parallel run of parameter estimations in separate
threads, so all nine models were estimated at once, reducing
the overall time needed for analysis but increasing resource
requirements. We run STRAIT [4] in a Cloud environment
(16 threads / 128GB RAM) for increased performance.
Overall, we ﬁtted 792 SRGMs (88 projects x 9 models) for
RQ1, RQ2, RQ4 and additionally 261 SRGMs for software
releases (29 releases x 9 models) in RQ3.

To evaluate the Goodness-of-ﬁt (GoF) of the models to the
dataset, we used the GoF metrics described in Section II (R2,
AIC, BIC, and RSE). We then computed mean values and
standard deviation (σ) for GoF metrics for all the projects.

Initially, we planned to use the one-way Analysis of Vari-
ance (ANOVA) test [35] together with Tukey’s Honestly Sig-
niﬁcant Difference (HSD) test [36] to evaluate the differences

between the groups. However, there are several assumptions
for the application of ANOVA and HSD (measurements nor-
mally distributed, homogeneity of variance across groups,
independence) – in our case we evaluated whether R2 mea-
surements violate strongly any of the assumptions, as ANOVA
can be considered robust in case of small violations. After
performing Q-Q plots of ANOVA’s residuals (Fig. 2) and
Shapiro-Wilk normality test (W=0.382, p-value < 2.2e-16,
data non-normally distributed) the ﬁrst assumption on the mea-
surement indicator R2 was strongly violated. For this reason,
we opted for Kruskal-Wallis Test with Dunn post-hoc test with
multiple testing adjustment based on the Bonferroni correction.
The assumptions were not violated for AIC measurements,
but we still opted for the more conservative Kruskal-Wallis
test also for AIC. For the effect size, we used Eta squared:
η2[H] = (H − k + 1)/(n − k), where H is the Kruskal-Wallis
test result; k is the number of groups; n is the total number
of observations. The rule of thumb for η2 is 0.01− < 0.06
(small effect), 0.06− < 0.14 (moderate effect), and >= 0.14
(large effect).

VI. RESULTS

A. RQ1 - Ranking of Models based on GoF

To answer this RQ, we considered 792 SRGMs ﬁtted on
the whole dataset with 383 788 software defects. Considering
all projects’ average R2, the Log-logistic model (LL), Yamada
Raleigh (YR), Weibull (WE), and Hossain-Dahiya (HD) are
the best four models (Table IV). At the same time, Goel-
Okumoto S-Shaped (GOS) represents the worse model
in
terms of R2. However,
these considerations are based on
the average R2 results from ﬁtting all the models. We can
look at the distribution of R2 for more insights about the
performance of the models. The Kruskal-Wallis rank sum test
p-value ≤ 0.05 (large effect size η2[H] = 0.191) indicates
that there are statistically signiﬁcant differences between two
or more groups. Overall, we can consider the results of 95%
R2 Conﬁdence Interval (Fig. 3).

• GO, GOS, MO, YE have higher variance than other mod-
els and they also have worse performance compared to

the other models. Dunn post-hoc test shows signiﬁcantly
statistical differences between these and the other models;
• HD and YR are the models that have less variance in
terms of R2. Considering the whole dataset, these models
reach consistent results, slightly better compared to the
best model (LL) considering average R2. Dunn post-
hoc test reports signiﬁcant differences HD-LL (p-value =
0.001) and YR-LL (p-value = 0.008) while the difference
HD-YR is not signiﬁcant.

• The GOS model is the worse model in terms of average
and variance of R2. From the analysis on all the projects,
this seems the last model to be suggested for usage.
• Since each model has been applied to the same set of
projects, we can compare the AIC metric results: the
Kruskal-Wallis rank sum test reports that there are non-
signiﬁcant differences among the group of models (p-
value = 0.64).

TABLE IV
RQ1 - GOF OF MODELS (ORDERED BY BEST R2)

R2

AIC

RSE

Model
LL
YR
WE
HD
DU
YE
GO
MO
GOS

µ
0.979
0.977
0.976
0.973
0.963
0.937
0.935
0.931
0.896

σ
0.094
0.052
0.096
0.039
0.099
0.115
0.115
0.123
0.220

µ
3,374
3,806
3,411
3,769
3,708
3,889
3,896
3,883
3,767

σ
1771.464
1847.782
1782.277
1890.365
1947.489
2087.951
2078.743
2073.052
1942.052

µ
83.907
175.468
81.158
145.866
147.956
233.414
234.270
230.064
180.096

σ
188.828
445.723
164.656
380.665
395.543
604.541
604.028
606.065
438.825

The ﬁrst ranking of the Log-logistic (LL) model conﬁrms
the ﬁndings in [12] extending the result to other measures of
accuracy than the RSE and proving the LL model proposition
to capture the increasing/decreasing nature of the failure occur-
rence rate. Nevertheless, the Weibull (WE) model also proved
its precedence even over a signiﬁcant sample of projects.
However, if we look at the results from the Dunn’s test on
all 88 projects, the main consideration is that all models apart
GOS can be used to ﬁt the cumulative failure data with some
degree of conﬁdence.

Fig. 3. 95% Conﬁdence Intervals for R2 of the nine models (please note the
[1.0-0.8] range - not all datapoints are represented)

RQ1 Findings

Based on 792 ﬁtted SRGMs, considering the R2
metric LL, YR, WE, HD, DU are the best models.
GO, GOS, MO, YE show the highest variance than
other models. GOS is in general the worse model in
terms of R2.

B. RQ2 - Project Domains

To answer this RQ, we considered 792 SRGMs ﬁtted on the
whole dataset with 383 788 software defects and segmented
by categories presented in Table III. We analyzed the GoF of
the domains C1-C8 by ﬁtting the models on the cumulative
failures of the projects in each domain (Table V). All the
categories show signiﬁcant results according to Kruskal-Wallis
Test, apart C7 (text processing). The Log-logistic (LL), the
Weibull (WE), and the Yamada Raleigh (YR) are among the
top three models in terms of R2 for the majority of the project
domains. However, there are some distinctions to put forward:
GOS while generally worse in some categories (C1,C3,C4,C5)
reaches good performance in other categories (C6,C7,C8),
so the application can be considered domain-dependent. The
Database and data analysis (C3) domain presents interesting
results: the concave model Hossain-Dahiya (HD) model ﬁts
the best for projects of this domain, followed by LL and WE
models, but in general, the results of the majority of the models
are the worst compared to other domains, caused by the
low failure occurrence rates. We cannot compare AIC metric
results across domains (due to the application to different
projects), but when compared in the same domain Kruskal-
Wallis test does not report signiﬁcant differences among the
models.

RQ2 Findings

Divided by domains (C1-C8), the results in terms of
R2 metric report some models with a good consis-
tency across domains (e.g., LL, WE, YR). One ex-
ception is the database and data analysis domain (C3)
in which the lower failure occurrence rate leads to HD
model and GO, GOS, MO, YE as the worse models.
The GOS model, while statistically worse than all
other models when considering the whole dataset, has
some domains in which it has low variance and good
R2 rankings.

C. RQ3 - Project Releases

To answer this RQ, we used 63 SRGMs (7 projects with
releases ﬁtted by 9 models) and 261 SRGMs (29 releases, 9
models, 6 800 defects). We looked at the impact of considering
project releases compared to projects as a whole when ﬁtting
SRGM models to cumulative failure data. Project releases with
less than 20 issues indicated a notable bad ﬁt for SRGMs.
For such a low number of failure data, it was not feasible to
estimate the parameters of the models. For this reason, when

TABLE V
RQ2 - OVERVIEW OF THE GOF (R2) FOR THE PROJECT CATEGORIES AND INDIVIDUAL MODELS (TOP-3 MODELS HIGHLIGHTED).

C1
n = 126
≤ 0.05 (KWT)
η2 = 0.19
σ
µ
0.008
0.989
0.056
0.949
0.331
0.842
0.043
0.970
0.002
0.996
0.058
0.947
0.003
0.995
0.056
0.950
0.017
0.985

C2
n = 81
≤ 0.05 (KWT)
η2 = 0.25
σ
µ
0.016
0.982
0.018
0.971
0.114
0.949
0.021
0.977
0.009
0.993
0.018
0.970
0.009
0.993
0.018
0.971
0.014
0.986

C3
n = 90
≤ 0.05 (KWT)
η2 = 0.10
σ
µ
0.269
0.850
0.283
0.766
0.255
0.774
0.011
0.988
0.277
0.871
0.301
0.757
0.274
0.865
0.286
0.762
0.165
0.921

C4
n = 90
≤ 0.05 (KWT)
η2 = 0.19
σ
µ
0.027
0.976
0.061
0.952
0.259
0.845
0.072
0.968
0.007
0.993
0.059
0.947
0.008
0.993
0.061
0.952
0.018
0.984

C5
n = 144
≤ 0.05 (KWT)
η2 = 0.14
σ
µ
0.016
0.982
0.040
0.958
0.256
0.872
0.033
0.966
0.009
0.989
0.040
0.957
0.011
0.987
0.040
0.958
0.030
0.975

C6
n = 90
≤ 0.05 (KWT)
η2 = 0.19
σ
µ
0.020
0.976
0.027
0.970
0.008
0.987
0.028
0.982
0.009
0.994
0.026
0.967
0.008
0.994
0.027
0.970
0.009
0.987

C7
n = 63
> 0.05 (KWT)
η2 = 0.06
σ
µ
0.076
0.952
0.050
0.934
0.026
0.969
0.041
0.954
0.009
0.984
0.103
0.910
0.077
0.958
0.050
0.935
0.023
0.970

C8
n = 108
≤ 0.05 (KWT)
η2 = 0.13
µ
σ
0.080
0.960
0.079
0.945
0.017
0.980
0.008
0.988
0.055
0.994
0.021
0.952
0.008
0.993
0.028
0.969
0.012
0.985

Model
DU
GO
GOS
HD
LL
MO
WE
YE
YR

analyzing single releases, we omitted project releases with less
than 20 faults.

We divided the projects into two groups for the comparison
of GoF : R(eleases) – SRGMs ﬁtted to cumulative failures for
projects releases with failure data >20, and N(non)R(eleases)
– SRGMs ﬁtted to the cumulative failures for projects as
a whole. Considering the R group, the Kruskal-Wallis test
was signiﬁcant (p-value = 4.064e-05, moderate effect size,
η2[H] = 0.103), while for the NR group it was not signiﬁcant
(p-value = 0.437, small effect size η2[H] = 0.0006).

Four models with the highest means of R2 are ranked the
same (LL, WE, DU, HD) for both groups (Table VI). Using
releases or considering the project as a whole does not have
an impact on the best models that remain mostly stable (apart
YR which gains positions in the ranking). Considering AIC
and BIC all the top-3 models are ranked differently when
considering software releases. Considering RSE, LL and WE
remain the best models in both groups, with the YR model
that gains from the last model to the 3rd model in terms of
RSE. However, AIC and BIC penalize YR on the release data
signalling the model might be overﬁtting the data (higher R2
and RSE but lower AIC and BIC).

Looking at Dunn’s post-hoc test with Bonferroni adjustment
for R2 of the R group, the GOS model is considered to
be different from DU, LL, WE models – as well, YR is
than the LL model. Based on these
statistically different
results, we can deduce that there is not much difference from
the models to be selected at the release level, with GOS
worse in terms of the performance compared to the other
models (Table VII).

RQ3 Findings

Considering projects as a whole or releases mostly
does not have an impact on the rankings of the
models in terms of R2 (the top-3 models remain the
same). Considering AIC and BIC leads to different
rankings of models, signalling that some models such
as YR might be penalized for overﬁtting the data. As
per RQ1 results, the GOS model is the worse both
considering releases and projects as a whole.

TABLE VI
RQ3 - COMPARISON OF RANKINGS OF GOF METRICS MEANS
CONSIDERING RELEASES (R) OR WHOLE PROJECTS (NR).

R2

AIC

BIC

RSE

R
3
7
9
4
1
6
2
5
8

NR
3
8
9
4
1
7
2
6
5

R
6
4
9
2
5
8
7
3
1

NR
3
7
8
5
1
6
2
4
9

R
7
1
9
3
6
8
4
5
2

NR
3
7
8
5
1
6
2
4
9

R
3
7
8
4
1
6
2
5
9

NR
5
9
4
6
1
8
2
7
3

DU
GO
GOS
HD
LL
MO
WE
YE
YR

TABLE VII
RQ3 - RESULT OF DUNN’S TEST (BONFERRONI ADJ) (R2) – R GROUP

Models
GO-DU
GOS-DU
HD-DU
LL-DU
MO-DU
WE-DU
YE-DU
YR-DU
GOS-GO
HD-GO
LL-GO
MO-GO

p-value
1.000
0.017
1.000
1.000
1.000
1.000
1.000
0.538
0.117
1.000
1.000
1.000

Models
WE-GO
YE-GO
YR-GO
HD-GOS
LL-GOS
MO-GOS
WE-GOS
YE-GOS
YR-GOS
LL-HD
MO-HD
WE-HD

p-value
1.000
1.000
1.000
0.293
0.000
0.115
0.000
0.064
1.000
1.000
1.000
1.000

Models
YE-HD
YR-HD
MO-LL
WE-LL
YE-LL
YR-LL
WE-MO
YE-MO
YR-MO
YE-WE
YR-WE
YR-YE

p-value
1.000
1.000
1.000
1.000
1.000
0.010
1.000
1.000
1.000
1.000
0.061
1.000

D. RQ4 - Project Attributes

To answer this RQ, we considered all the 88 projects, thus
considering 792 SRGMs ﬁtted to 383 788 software defects.
We segmented the projects according to different attributes:

• lines of code/size (LOC) – the total code size;
• number of contributors (NOC) – the total number of

contributors;

• number of issue reports (NOI) – the total number of

issue reports created on project;

• number of faults (NOFA) – the total number of faults

from issue reports;

We divided these attributes into more distinct fragments
(Small (S) / Medium (M) / Large (L)). The threshold values

for these fragments were selected to have at least 10 projects
in each segment and to represent a signiﬁcant subdivision of
projects in the three ranges. We derived these thresholds from
the indications in previous research on typical domain-speciﬁc
source code metrics [37] and our evaluation of the distribution
of the metrics in the projects considered (Table VIII).

TABLE VIII
THRESHOLDS FOR CATEGORY FRAGMENTS

LOC
NOC
NOI
NOFA

Small
< 10,000
< 100
< 1,000
< 500

Medium

Large
10,000 to 100,000 > 100,000
> 300
> 10,000
> 5,000

100 to 300
1,000 to 10,000
500 to 5,000

Once the projects were segmented, we looked at the rank-
ings in terms of R2 between the ﬁtted models in the different
categories of projects attributes (Table IX):

• In all the projects in which either LOCS, NOC, NOI,
NOFA metrics are in the medium and higher ranges, the
LL model is usually ranked as the best model in terms of
R2. Where the values are instead on the small scale, the
YR models is, in the majority of the cases, the best model.
This seems to suggest that the two S-Shaped models (LL
and YR) have better results in terms of ﬁtting compared
to other models;

• We found that in most of the cases GOS (the other S-
Shaped model) is the worse model when considering the
division into categories (S,M,L). Also the MO model
reports some consistently lower results in terms of R2.
• We compared also the consistency of the rankings of the
models for the four categories LOC, NOC, NOI, NOFA in
terms of S,M,L division to see the stability of the rankings
in each of the categories. We used the Inter-Rater Agree-
ment (IRA) deﬁned as IRA = T A/(T R∗R)∗100 where
TA = # of agreements in the ratings, TR = # of ratings
given by each rater, R = # of raters (three in our case,
S,M,L). Overall, rankings are more stable in the LOC
categories with IRA = 40.7%. They are quite low in the
NOC categories with IRR = 25.9% and NOI categories
with IRA = 22.2%. While in the NOFA, there is a very
low consistency of the rankings, with only IRA = 3.7%.

RQ4 Findings

Considering the ranges of LOCs, NOCs, NOIs, and
NOFAs, YR and HD models behave the best in the
lower range categories, while LL and WE in those
with higher ranges. GOS, GO, and MO models are
often the worse models in R2 for different categories.
NOFA seems to have a high impact on the consis-
tency of the models’ rankings.

TABLE IX
RQ4 - COMPARISON OF RANKING OF THE TOP-2 AND BOTTOM-2 MODELS
(LOC, NOC, NOI, NOFA) BY MEAN R2 FOR S, M, L PROJECTS.

LOC
S M L
3
5
5
7
6
6
9
9
9
4
4
2
1
1
3
8
8
8
2
3
4
6
7
7
5
2
1

NOC
S M L
3
4
6
6
8
5
9
9
9
5
3
2
1
1
3
8
7
8
2
2
4
7
6
7
4
5
1

NOI
S M L
3
5
5
7
8
6
6
9
8
5
3
2
1
1
3
9
8
9
2
2
4
8
7
7
4
4
1

NOFA
S M L
3
4
5
7
8
6
5
9
9
6
5
1
2
1
3
9
7
8
1
2
4
8
6
7
4
3
2

DU
GO
GOS
HD
LL
MO
WE
YE
YR

VII. THREATS TO VALIDITY

We only used GoF metrics for the evaluation of the projects.
However, one model can have good GoF but bad predictive
power [1]. We plan in further extensions to look into the
predictive power of the models for the different domains. We
applied data cleaning and ﬁltering for the issues mined from
GitHub repositories, still, some issues might not be related
to the system’s faults. Furthermore, for ﬁtting the models we
used the minimum least square regression method to estimate
model parameters which is ﬁne for small to medium samples.
In contrast, the recommended method for parameter estimation
for a signiﬁcant sample (a large number of faults) is the
maximum likelihood method. However, for the size of the
projects and the comparison of the models, we considered
the minimum least square method as adequate. Furthermore,
there are many project parameters that can potentially affect
the results. We do not evaluate them in this study due to
space constraints, as the goal of this paper was to be more
exploratory than explanatory. However, there were previous
studies that looked at such parameters. For example, while
the programming language used can lead to certain types of
problems, overall, it might not have a signiﬁcant impact on
the number of issue reports. This was the conclusion of an
earlier study on 100,000 OSS projects [38]. On the same line,
the answers to the RQs are mostly based on the R2 indicator.
Even though we report AIC, BIC, and RSE, the stability of
the results across the different indicators would require a more
in-depth analysis.

VIII. CONCLUSION

In this paper, we conducted an empirical analysis of SRGMs
in the context of the OSS projects. While initial SRGMs
were developed without the consideration of agile models,
during the years the software development process has evolved
towards agile and open source development processes, chal-
lenging many of the assumptions of the models that were
developed over time. For this reason, we set up a large
empirical study for the application of SRGMs to OSS projects.
The main contribution of the study is the evaluation of SRGMs
in a large number of projects, considering domains and project
releases – overall, including 88 OSS projects and ∼400K
software defects, we consider 1 053 ﬁtted SRGMs.

Such a large number of ﬁtted SRGMs allowed us to in-
vestigate the applicability to OSS on a large scale, looking
at the stability of the best models on the whole projects,
on releases, on different domains, and according to different
projects’ attributes.

In general, the results of an extensive number of projects
can document the discrepancies in related works about the best
models identiﬁed. Even the worse model in terms of ﬁtting
the cumulative failure data on the whole projects can ﬁnd
good applicability in some domains or speciﬁc projects. As
such, ﬁnding a one-ﬁts-all model for OSS, while unrealistic,
should further motivate the future investigation of the attributes
related to OSS development that might have an impact on
the applicability of SRGMs to drive the selection of the
most appropriate models. In this sense, while the current
paper is exploratory, future works can be in the direction of
determining the best predictors from OSS projects that can
impact the applicability of SRGMs.

ACKNOWLEDGMENTS
The work was supported by ERDF ”CyberSecurity, Cy-
berCrime and Critical Information Infrastructures Center of
Excellence” (No. CZ.02.1.01/0.0/0.0/16 019/0000822).

Access to the CERIT-SC computing and storage facil-
ities provided by the CERIT-SC Center, provided under
the programme ”Projects of Large Research, Development,
and Innovations Infrastructures” (CERIT Scientiﬁc Cloud
LM2015085), is greatly appreciated.

REFERENCES

[1] M. R. Lyu, Handbook of software reliability engineering.

IEEE

Computer Society Press, 1996.

[2] Z. Jelinski and P. Moranda, “Software reliability research,” in Statistical
computer performance evaluation. Elsevier, 1972, pp. 465–484.
[3] A. Wood, “Software reliability growth models,” Tandem Computers Inc.,

Cupertino, CA 95014, Tech. Rep., 1996.

[4] S. Chren, R. Micko, B. Buhnova, and B. Rossi, “Strait: A tool for
automated software reliability growth analysis,” in 2019 IEEE/ACM 16th
International Conference on Mining Software Repositories (MSR), 2019,
pp. 105–110.

[5] R. Micko, S. Chren, and B. Rossi, “Software Reliability Growth
[Online].

Modelling (SRGM) 2022: 88 Projects Dataset,” 2022.
Available: https://doi.org/10.6084/m9.ﬁgshare.18794003

[6] A. L. Goel and K. Okumoto, “Time-dependent error-detection rate
model for software reliability and other performance measures,” IEEE
Transactions on Reliability, no. 3, pp. 206–211, 1979.

[7] S. A. Hossain and R. C. Dahiya, “Estimating the parameters of a
non-homogeneous poisson-process model for software reliability,” IEEE
Transactions on Reliability, pp. 604–612, 1993.

[8] J. D. Musa and K. Okumoto, “A logarithmic poisson execution time
model for software reliability measurement,” in Proceedings of the 7th
International Conference on Software Engineering.
Piscataway, NJ,
USA: IEEE Press, 1984, pp. 230–238.

[9] J. T. Duane, “Learning curve approach to reliability monitoring,” IEEE

Transactions on Aerospace, pp. 563–566, 1964.

[10] J. D. Musa, A. Iannino, and K. Okumoto, Software reliability: measure-

ment, prediction, application. McGraw-Hill, Inc., 1987.

[11] S. Yamada, M. Ohba, and S. Osaki, “S-shaped reliability growth
modeling for software error detection,” IEEE Transactions on Reliability,
vol. R-32, no. 5, pp. 475–484, 1983.

[12] S. S. Gokhale and K. S. Trivedi, “Log-logistic software reliability growth
model,” in Proceedings Third IEEE International High-Assurance Sys-
tems Engineering Symposium (Cat. No.98EX231), 1998, pp. 34–41.
[13] H. Koziolek, B. Schlich, and C. G. Bilich, “A large-scale industrial
case study on architecture-based software reliability analysis,” IEEE 21st
International Symposium on Software Reliability Engineering, pp. 279–
288, 2010.

[14] J. I. Ansell and M. J. Phillips, Practical Methods for Reliability Data

Analysis. Oxford University Press Inc., New York, 1994.

[15] K. P. Burnham and D. R. Anderson, “A practical information-theoretic
approach,” Model selection and multimodel inference, vol. 2, 2002.
[16] A. Barone. (2021) Residual sum of squares (rss). [Online]. Available:
https://www.investopedia.com/terms/r/residual-sum-of-squares.asp
[17] J. Wang, “Open source software reliability model with nonlinear fault
detection and fault introduction,” Journal of Software: Evolution and
Process, vol. 33, no. 12, p. e2385, 2021.

[18] S. M. Syed-Mohamad and T. McBride, “Reliability growth of open
source software using defect analysis,” in Int. Conference on Computer
Science and Software Engineering, vol. 2, 2008, pp. 662–667.

[19] C. Rahmani, A. Azadmanesh, and N. Lotﬁ, “A comparative analysis of
open source software reliability,” JSW, vol. 5, pp. 1384–1394, 12 2010.
[20] Y. Zhou and J. Davis, “Open source software reliability model: an em-
pirical approach,” ACM SIGSOFT software engineering notes, vol. 30,
no. 4, pp. 1–6, 2005.

[21] B. Rossi, B. Russo, and G. Succi, “Modelling failures occurrences of
open source software with reliability growth,” in IFIP International
Conference on Open Source Systems, vol. 319, 2010, pp. 268–280.
[22] F. Zou and J. Davis, “Analyzing and modeling open source software bug
report data,” in 19th Australian Conference on Software Engineering
(aswec 2008), 2008, pp. 461–469.

[23] N. Ullah, M. Morisio, and A. Vetro, “A comparative analysis of software
reliability growth models using defects data of closed and open source
software,” in 2012 35th Annual IEEE Software Engineering Workshop,
2012, pp. 187–192.

[24] Y. Tamura and S. Yamada, “Comparison of software reliability as-
sessment methods for open source software,” in 11th International
Conference on Parallel and Distributed Systems (ICPADS’05), vol. 2,
2005, pp. 488–492.

[25] V. Ivanov, A. Reznik, and G. Succi, “Comparing the reliability of
software systems: A case study on mobile operating systems,” Inf. Sci.,
vol. 423, pp. 398–411, 2018.

[26] K. Ohishi, H. Okamura, and T. Dohi, “Gompertz software reliability
model: Estimation algorithm and empirical validation,” Journal of Sys-
tems and software, vol. 82, no. 3, pp. 535–543, 2009.

[27] T. M. Khoshgoftaar and E. B. Allen, “Logistic regression modeling of
software quality,” International Journal of Reliability, Quality and Safety
Engineering, vol. 6, no. 04, pp. 303–317, 1999.

[28] M. Ohba, “Inﬂection s-shaped software reliability growth model,” in
Stochastic models in reliability theory. Springer, 1984, pp. 144–162.
[29] H. Pham, L. Nordmann, and Z. Zhang, “A general imperfect-software-
debugging model with s-shaped fault-detection rate,” IEEE Transactions
on reliability, vol. 48, no. 2, pp. 169–175, 1999.

[30] H. Pham and X. Zhang, “An nhpp software reliability model and its
comparison,” International Journal of Reliability, Quality and Safety
Engineering, vol. 4, no. 03, pp. 269–282, 1997.

[31] J. Wang and X. Mi, “Open source software reliability model with the
decreasing trend of fault detection rate,” The Computer Journal, vol. 62,
no. 9, pp. 1301–1312, 2019.

[32] X. Li, Y. F. Li, M. Xie, and S. H. Ng, “Reliability analysis and optimal
version-updating for open source software,” Information and Software
Technology, vol. 53, no. 9, pp. 929–936, 2011.

[33] R. Miˇcko, “Software reliability growth models for open source software
[online],” Master’s thesis, Masaryk University, Faculty of Informatics,
Brno, 2022.

[34] H. Huang and S. Huang, “Nonlinear regression analysis,” International

Encyclopedia of Education, pp. 339–346, 2010.

[35] R. Bevans. (2020) An introduction to the one-way anova. [Online].

Available: https://www.scribbr.com/statistics/one-way-anova/

[36] H. Abdi and L. J. Williams, “Tukey’s honestly signiﬁcant difference
(hsd) test,” Encyclopedia of research design, vol. 3, no. 1, pp. 1–5,
2010.

[37] A. Mori, G. Vale, M. Viggiato, J. Oliveira, E. Figueiredo, E. Cirilo,
P. Jamshidi, and C. Kastner, “Evaluating domain-speciﬁc metric thresh-
olds: An empirical study,” in 2018 IEEE/ACM International Conference
on Technical Debt (TechDebt), 2018, pp. 41–50.

[38] T. F. Bissyand´e, F. Thung, D. Lo, L. Jiang, and L. R´eveillere, “Popular-
ity, interoperability, and impact of programming languages in 100,000
open source projects,” in 2013 IEEE 37th annual computer software and
applications conference.

IEEE, 2013, pp. 303–312.

