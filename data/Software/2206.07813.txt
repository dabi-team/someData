2
2
0
2

n
u
J

5
1

]
E
S
.
s
c
[

1
v
3
1
8
7
0
.
6
0
2
2
:
v
i
X
r
a

1

Search-Based Testing Approach for Deep
Reinforcement Learning Agents

Amirhossein Zolfagharian, Manel Abdellatif, Lionel Briand, Mojtaba Bagherzadeh and Ramesh S

Abstract—Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various
decision-making problems such as autonomous driving, trading decision making and robotics. However, these algorithms have faced
great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially
critical errors. One of the ways to assess the safety of DRL agents is to test them in order to detect possible faults leading to critical
failures during their execution. This raises the question of how we can efﬁciently test DRL policies to ensure their correctness and
adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of
the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the
robustness of DRL agents rather than testing the compliance of agents’ policies with respect to requirements. Due to the huge state
space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, the exhaustive testing of DRL
agents is impossible.
In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL
agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models
and a dedicated genetic algorithm to narrow the search towards faulty episodes (i.e., sequences of states and actions produced by the
DRL agent). We apply STARLA on a Deep-Q-Learning agent which is widely used as a benchmark and show that it signiﬁcantly
outperforms Random Testing by detecting more faults related to the agent’s policy. We also investigate how to extract rules that
characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under
which the agent fails and thus assess the risks of deploying it.

Index Terms—Reinforcement Learning, Testing, Genetic Algorithm, Machine Learning, State Abstraction.

(cid:70)

1 INTRODUCTION

Reinforcement Learning (RL) algorithms have seen
tremendous research advances in recent years, both from
a theoretical standpoint and in their applications to solve
real-world problems. Reinforcement Learning (RL) [1] trains
an agent to make a sequence of decisions to reach a ﬁnal
goal and is therefore a technique gaining increasing interest
in many application contexts such as autonomous driving
and robotics. Deep Reinforcement Learning (DRL) tech-
niques [2], [3], [4], a branch of deep learning where RL poli-
cies are learnt using Deep Neural Networks (DNNs), have
been gaining more attention in recent years. However, like
for DNN components, their application in production en-
vironments requires effective and systematic testing, espe-
cially when used in safety-critical applications. For instance,
deploying a reinforcement learning agent in autonomous
driving systems entails major concerns around safety as we
should pay attention not only to the extent to which the
agent’s objectives are met, but also to damage avoidance [5].

One of the ways to assess the safety of DRL agents
is to test them in order to detect possible faults leading
to critical failures during their execution. By deﬁnition, a
fault in DRL-based systems corresponds to a problem in the
RL policy that may lead to the agent’s failure during the
execution. Since DRL techniques use DNNs, they inherit the
advantages and drawbacks of such models, making their
testing very challenging and time consuming. Furthermore,
DRL-based systems are based on a Markov Decision Process
(MDP) [6] that makes them stateful. They embed several

components including the agent, the environment and the
ML-based policy network. Testing a stateful system that
consists of several components is by itself a challenging
problem. It becomes even more challenging when ML
components and the probabilistic nature of the real-world
environments are considered.

Furthermore, in a DRL context, two types of faults can be
deﬁned: functional and reward faults. The former happens
when an RL agent takes an action that leads to an unsafe
state (e.g., a driverless car does not stop at a stop sign).
The latter occurs when an agent does not reach the desired
reward (e.g., when a driverless car reaches its destination
an hour late). Functional and reward faults are often in
tension as we can obtain a high reward while observing a
functional fault. For example, an unsafe action can help the
agent reach the desired state faster (e.g., when not stopping
at stop signs, the driverless car may reach its destination
sooner). This makes the detection of such types of fault
challenging, especially when the agent is well trained and
failures are rare. We should note that we focus our analysis
on functional faults since they are more critical. Thus, the
detection of reward faults is left out of the scope of the paper
for future work.

There are three types of testing approaches for deep
learning systems that depend on the required access levels
to the system under test: white-box, black-box, and data-
box testing [7], [8]. White-box testing requires access to the
internals of the DL systems and its training dataset. Black-
box testing does not require access to these elements and
considers the DL model as a black box. Data-box testing

 
 
 
 
 
 
requires access only to the training dataset.

Prior testing approaches for DL systems (including
DNNs) have focused on black-box and white-box testing [9],
[10], [11], [12], [13], [14], depending on the required access
level to the system under test. However, limited work has
been done on testing DRL-based systems in general and
using data-box testing methods in particular [10], [12], [14],
[15]. Relying on such type of testing is practically important
as testers often have no full access to the internals of RL-
based systems but have access to the training dataset of the
RL agent [16].

Most existing works on testing DRL agents are based
on adversarial attacks that aim at perturbing states of the
DRL environment [17]. However, adversarial attacks lead to
unrealistic states and episodes, and their main objective is to
test the RL agents’ robustness rather than testing the agents’
functionality (e.g., functional safety). In addition, a white-
box testing approach of DRL agents has been proposed,
focusing on fault localization in the source-code of DRL-
based systems [10]. However, this testing approach requires
full access to the internals of the DRL model, which are often
not available to testers, especially when the DRL model is
proprietary or provided by a third-party. Also, localizing
and ﬁxing faults in the DRL source code do not prevent
agent failures due to imperfect policies and the probabilistic
nature of the RL environment. Furthermore, because of the
huge state space, the high cost of test execution, and the
black-box nature of DNN models (policy networks), the
exhaustive testing of DRL agents is impossible.

In this paper, we focus on testing the policies of DRL-
based systems using a data-box testing approach, thus ad-
dressing the needs of many practical situations. We propose
STARLA, a Search-based Testing Approach for Reinforce-
ment Learning Agents, focused on testing the agent’s policy
by searching for faulty episodes as effectively as possible.
An episode is a sequence of states and actions resulting from
executing the RL agent. To create these episodes, we lever-
age evolutionary testing methods and rely on a dedicated
genetic algorithm to identify and generate functional-faulty
episodes [18]. We rely on state abstraction techniques [19],
[20] to group similar states of the agent and signiﬁcantly
reduce the state space. We also make use of ML models
to predict faults in episodes and guide the search towards
faulty episodes. We applied our testing approach on a
Deep-Q-Learning agent and more speciﬁcally the widely
known Cart-Pole benchmark from the openAI GYM envi-
ronment [21]. We show that our testing approach outper-
forms Random Testing as we ﬁnd signiﬁcantly more faults.
Overall, the main contributions of our paper are as

follows:

• We propose STARLA, a data-box search-based ap-
proach to test DRL agents’ policies by detecting
functional-faulty episodes.

• We propose a highly accurate machine learning-based
classiﬁcation of RL episodes to predict functional-faulty
episodes, which we use to improve the guidance of the
search. This is in part based on deﬁning and applying
the notion of abstract state to increase learnability.

• Based on a large-scale experiment, we show that
STARLA outperforms the Random Testing of DRL
agents as we detect signiﬁcantly more faults when

2

considering the same testing budget (i.e., same number
of the generated episodes).

• We provide online1 a prototype tool for our search-
based testing approach as well as all the needed data
and conﬁgurations to replicate our experiments and
results.
The remainder of the paper is structured as follows.
Section 2 presents the required background and establishes
the context of our research. Section 3 describes precisely our
research problem. Section 4 presents our testing approach.
Section 5 reports on our empirical evaluation and results.
Section 6 discusses the practical implications of our results.
Section 7 analyses the threats to validity. Finally, sections 8
and 9 contrast our work with related work and conclude the
paper, respectively.

2 BACKGROUND

Reinforcement Learning (RL) trains a model or an agent
to make a sequence of decisions to reach a ﬁnal goal. It is
therefore a technique gaining increasing interest in complex
autonomous systems. RL uses trials and errors to explore the
environment by selecting an action from a set of possible
actions. The actions are selected to optimize the obtained
reward. In the following, we will describe an example
application of RL in autonomous vehicles.

2.1 Deﬁnitions

To formally deﬁne our testing framework, we rely on a
running example and several key concepts that are intro-
duced in the following sections.

A running example. Assuming an Autonomous Vehicle
(AV) cruising on a highway, an RL agent (i.e., AV) receives
observations from an RGB camera (placed in front of the
car) and attempts to maximize its reward during the high-
way driving task that is calculated based on the vehicle’s
speed. The agent is given one negative/positive reward
per time step when the vehicle’s speed is below/above 60
MPH (Miles Per Hour). Available actions are turning right,
turning left, going straight, and no-action. The cruising
continues until one of the termination criteria is met: (1)
the time budget of 10 seconds is consumed, or (2) a collision
has happened.

Deﬁnition 1. (RL Agent Behaviour.) The behaviour of an
RL agent can be captured as a Markov Decision Process
(MDP) [6] (cid:104)S, A, T, R, γ(cid:105) where S and A denote a set of
possible states and actions accordingly, T : S×A×S −→ [0, 1]
refers to the transitions function, such that T (s(cid:48), a, s) deter-
mines the probability of reaching state s(cid:48) by performing ac-
tion a in state s, R : S × A −→ [0, Rmax] is a reward function
that determines the immediate reward for a pair of action
and state, and γ ∈ [0, 1] is the discount factor indicating the
difference of short-term and long-term reward [22].

The solution of an MDP is a policy π : S −→ A that
denotes the selected action given the state. The agent starts
from the initial state (s0 ∈ S) at time step t = 0 and then,
at each time step (ti, i ≥ 0), it takes an action (ai ∈ A)
according to the policy π that results in moving to a new
state si+1. Also, ri refers to the reward corresponding to the
action ai and state si that is obtained at the end of the step ti.

1. https://github.com/amirhosseinzlf/STARLA

Note that, there may not be a reward at each step, which in
that case is considered to be zero. Finally, (cid:80)i ri refers to the
accumulative reward until step ti.

Deﬁnition 2. (Episodes.) An episode e is a ﬁnite sequence of
pairs of states and actions, i.e., [(sj, aj)|sj ∈ S, aj ∈ A, 0 ≤
j ≤ n, n ∈ N], where the state of the ﬁrst pair is an initial
state, and the state of the last pair is an end state. An end
state is by deﬁnition, a state in which the agent can take
no more action. The accumulative reward of episode e is
(cid:80)|e| r, where |e| denotes the length of the episode. We refer
to the accumulated reward of episode e with r(cid:48)
e. A valid
episode is an episode where each state is reachable from the
initial state with respect to the transition function presented
in deﬁnition 1. Moreover, the episode is executable (i.e.,
consistent with the policy of an agent) if starting from the
same initial state and in each state, the selected action of the
agent is consistent with the action in the episode that we
want to execute.

Deﬁnition 3. (Faulty state.) A faulty state is a state in which
one of the deﬁned requirements (e.g., the autonomous ve-
hicle must not hit obstacles) does not hold, regardless of
accumulated reward in that state. A faulty state is often an
end state. In the context of the running example, a faulty
state is a state where a collision occurs.

Deﬁnition 4. (Faulty Episode.) We deﬁne two types of faulty
episodes:

• Functional fault: If an episode e contains a faulty state,
it is considered as a faulty episode of type functional.
A functional fault may lead to an unsafe situation in
the context of safety-critical systems (e.g., hitting an
obstacle in our running example).

e ≤ τ ),

• Reward fault: If the accumulative reward of episode
e is less than a predeﬁned threshold (r(cid:48)
it
is considered a faulty episode of type reward, i.e.,
the agent failed to reach the expected reward in the
episode. Intuitively, regarding our running example, if
we assume a reward fault threshold of τ = 100, then
each episode with a reward below 100 is considered to
contain a reward fault. In our running example, this
occurs when the AV agent drives at 25 MPH all the
time. As we mentioned earlier, the detection of this type
of fault is out of the scope of the paper and is left for
future work.

2.2 State Abstraction

State abstraction is a means to reduce the size of the state
space by clustering similar states to reduce the complexity
of the investigated problem [19], [20]. State abstraction can
be deﬁned as a mapping from an original state s ∈ S to an
abstract state sφ ∈ Sφ

φ : S −→ Sφ

(1)

3

2) Q∗-irrelevance abstraction: φ(s1) = φ(s2) if for all
available actions a ∈ A, Q∗(s1, a) = Q∗(s2, a), where
Q∗(s, a) is the optimal state-action value function that
returns the maximum expected reward from state s up
to the ﬁnal state when selecting action a in state s.
3) Model-irrelevance abstraction: φ(s1) = φ(s2) if for
any action a ∈ A and any abstract state sφ ∈
Sφ, R(s1, a) = R(s2, a) and also the transition
dynamics of the environment are similar, meaning
that (cid:80)
s(cid:48)∈φ−1(sφ) T (s(cid:48), a, s2)
where T (s(cid:48), a, s) returns the probability of going to
state s(cid:48) from state s performing action a, as deﬁned
in deﬁnition 1.

s(cid:48)∈φ−1(sφ) T (s(cid:48), a, s1) = (cid:80)

As we are testing RL agents in our work, we use in this
study the Q∗-irrelevance abstraction method since it repre-
sents the agent’s perception. We also choose this abstraction
method because it is more precise than π∗-irrelevance. In-
deed, π∗-irrelevance only relies on the predicted action, i.e.,
action with the highest Q∗-value, to compare two different
states, which makes it coarse. In contrast, the Q∗-irrelevance
relies on Q∗-values for all possible actions.

To clarify, assume two different states in the real world
for which our trained agent has the same Q-values. Given
the objective of testing the agent, it is logical to assume
these states to be similar as the agent has learned to predict
identical state-action values for both states (i.e., the agent
perceives both states to be the same).

Further, abstraction methods can be strict or approxi-
mate. Strict abstraction methods use a strict equality con-
dition when comparing states of state-action pairs, as pre-
sented above. Though they are more precise, they bring
limited beneﬁts in terms of state space reduction. In contrast,
more lenient abstraction methods can signiﬁcantly reduce
the state space but they may have inadequate precision.
Approximate abstractions relax the equality condition in
strict abstraction methods to achieve a balance between
state space reduction and precision. For example, instead
of Q∗(s1, a) = Q∗(s2, a), approximate abstraction methods
use the condition |Q∗(s1, a) − Q∗(s2, a)| < (cid:15), where (cid:15) is
a parameter to control the trade-off between abstraction
precision and state space reduction.

Another important property is transitivity as transitive
abstractions use a transitive predicate. For example, assume
that two states s1 and s2 are similar based on an abstraction
predicate and the same is true for s2 and s3. Then, we
should be able to conclude that s1 and s3 are similar.
Transitive abstractions are efﬁcient to compute and preserve
the near-optimal behaviour of RL agents [20]. Moreover, this
property helps create abstract classes more effectively.

Considering the properties that we explained above, we
use the following abstraction predicate φd that is transitive
and approximates the Q∗-irrelevance abstraction:

where the abstract state space is often much smaller than

the original state space.

Generally, there are three different classes of abstraction

methods in the RL context [23], [24]:

1) π∗-irrelevance abstraction: s1 and s2 are in the same
abstraction class φ(s1) = φ(s2), if π∗(s1) = π∗(s2),
where π∗ represents the optimal policy.

φd(s1) = φd(s2) ≡ ∀a ∈ A :

(cid:25)

(cid:24) Q∗(s1, a)
d

=

(cid:25)

(cid:24) Q∗(s2, a)
d

(2)

where d is a control parameter (abstraction level) that
can squeeze more states together when increasing and thus
reduce the state space signiﬁcantly. Intuitively, this method
discretizes the Q∗-values with buckets of size d.

4

3 PROBLEM DEFINITION

In this paper, we propose a systematic and automated
approach to test a DRL agent. In other words, considering
a limited testing budget, we aim to exercise the agent in
a way that results into detecting faulty episodes, if at all
possible. This requires ﬁnding faulty episodes in a large
space of possible episodes while satisfying a given testing
budget deﬁned as the number of executed episodes.

The exhaustive testing of DRL agents is impossible due
to the large state space, the black-box nature of DNN models
(policy networks), and the high cost of test execution. To
address the challenges mentioned above, we propose a ded-
icated search-based testing approach for RL agents that aims
at generating as many diverse faulty episodes as possible.
To create the corresponding test cases, we indeed leverage
meta-heuristics and most particularly genetic algorithms
that we tailor to the speciﬁc RL context.

3.1 RL Agent Testing Challenges

3.2 Assumptions

Since Deep Reinforcement Learning (DRL) techniques
use Deep Neural Networks (DNNs), they inherit the ad-
vantages and drawbacks of DNNs, making their testing
very challenging and time consuming [25], [26], [27], [28]. In
addition, RL techniques raise speciﬁc challenges for testing:

• Functional faults. The detection of functional faults in
DRL systems is challenging since relying only on the
agent’s reward is not always sufﬁcient to detect such
faults. Indeed, an episode with a functional fault can
reach a high reward. For example, by not stopping at
stop signs, the car may reach its destination sooner and
get a higher rewards if it is deﬁned based on the time of
arrival. Even if we consider a penalty for unsafe actions,
we can still have an acceptable reward for functional-
faulty episodes. Relying only on the agent’s reward
makes it challenging to identify such type of fault.

• State-based testing with uncertainty. Most traditional
ML models, including DNNs, are stateless. However,
DRL techniques are based on a Markov Decision Pro-
cess (MDP) that makes them stateful and more difﬁcult
to test. Also, an output of an RL agent is the result of
an interaction between the environment (possibly con-
sisting of several components, including ML ones) and
the agent. Testing a system with several components
and many states is by itself a challenging problem.
Accounting for ML components and the probabilistic
nature of real-world environments makes such testing
even more difﬁcult [29], [7].

• Cost of test execution. According to the above dis-
cussion, testing an RL agent requires the execution
of test cases by either relying on a simulator or by
replaying the captured logs of real systems. The latter
is often limited since recording a sufﬁcient number of
logs that can exhaustively include the real system’s
behaviour is impossible, especially in the context of
safety-critical systems, for which logs of unsafe states
cannot (easily) be captured. Thus, using a simulator for
testing DRL agents, speciﬁcally in the context of safety-
critical domains, is often inevitable. Despite signiﬁcant
progress made in simulation technology, high-ﬁdelity
simulators often require high computational resources.
Thus, testing DRL agents tends to be computationally
expensive [12], [29], [30].

• Focus on adversarial attacks. Most existing works on
testing DRL agents use adversarial attacks that are fo-
cused on perturbing states [13]. However, such attacks
lead to unrealistic states and episodes [31]. The main
goal of such attacks is to test the robustness of RL poli-
cies rather than testing the agents’ functionality [32],
[12], [33].

In this work, we focus on the testing of RL agents with
discrete actions and a deterministic policy, interacting with
a stochastic environment. A discrete action setting reduces
the complexity of the problem in deﬁning genetic search
operators, as we will see in the following sections. It also
reduces the space of possible episodes. Moreover, assuming
a deterministic policy and stochastic environment is realistic
since in many application domains (speciﬁcally in safety-
critical domains), randomized actions are not acceptable and
environments tend to be complex [34]. We further assume
that we do not have noisy rewards [35] [36] where an
adversary manipulates the reward to mislead the agent. We
build our work on model-free RL algorithms since they
are more popular in practice and have been extensively
researched [37], [38].

4 APPROACH

Genetic Algorithms (GA) are evolutionary search tech-
niques that imitate the process of evolution to solve opti-
mization problems, especially when traditional approaches
are ineffective or inefﬁcient [39]. In this research, as for many
other test automation problems, we use genetic algorithms
to test RL agents. This is accomplished through analyzing
the episodes performed by an RL agent in order to generate
and execute new episodes with high fault probabilities from
a large search space.

4.1 Reformulation as a Search Problem

We are dealing with a high number of episodes repre-
sented as sequences of pairs (deﬁnition 2) which are exe-
cuted to test an RL agent. To properly translate the process
into a search problem using a genetic algorithm, we need to
deﬁne the following elements.

• Individuals. Individuals consists of a set of elements
called genes. These genes connect together and form
an individual that is a solution. Here, individuals are
episodes complying with Deﬁnition 2 which should
ideally have a high probability of failure. Naturally,
each gene is represented by a pair of state and action.
• Initial Population. A set of individuals forms a popu-
lation. In our context, a population is a set of episodes.
However, it is imperative for the search to select a
diverse set of individuals to use as initial population.
The sampling process is detailed in section 4.3.

• Operators. Genetic operators include crossover, muta-
tion and selection [40] which are used to create the
next generation of episodes. In the crossover, we use
two episodes as input and create a new offspring with
hopefully a higher fault probability. We use the current
population as the input and select an episode for the

5

Figure 1: Overview of STARLA

crossover (using tournament selection). The selection
of such an episode is in relation to its ﬁtness. We then
select a crossover point randomly and search for a
matching episode and join it with the selected episode.
Mutation is an operator that adds diversity to our
solutions. An episode is selected using again the tour-
nament selection. Then, one pair is randomly selected
as the mutation point which is altered according to a
deﬁned policy that is detailed in section 4.5.2.
Selection is the last operator used in each generation.
It combines the episode from the last generation with
the newly created episode in a way that does not elim-
inate good solutions from previous generations. More
detailed explanations are provided in section 4.5.3.

• Fitness Function. The ﬁtness function should indirectly
capture how likely an episode is to be faulty. To that
end, we deﬁne a multi-objective ﬁtness [41] function
that we use to compare episodes and select the ﬁttest
ones. As further explained in section 4.4, we consider
different ways to indirectly capture an episode’s fault-
iness: (1) the reward loss, (2) the predicted probability
of observing functional faults in an episode based on
machine learning, and (3) the certainty level for the
actions taken in an episode.

• Termination criteria. This determines when the search
process should be ended. Different termination criteria
can be used such as the number of generations or iter-
ations, the search time, and convergence in population
ﬁtness. For the latter, the search stops when there is no
improvement above a certain threshold over a number
of newly generated episodes.

4.2 Overview of the Approach

As depicted in Figure 1, the main objective of STARLA is
to generate and ﬁnd episodes with high fault probabilities in
order to assess whether an RL agent can be safely deployed.
To apply a genetic algorithm for our problem, we ﬁrst
need to sample a diverse subset of episodes to use as the
initial population. In the next step, we use dedicated genetic
operators to create offspring to form the new population.
Finally, using a selection method, we transfer individuals
from the old population to the new one while preserving
the diversity of the latter. For each ﬁtness function, we have
a threshold value, and a ﬁtness function is satisﬁed if an
episode has a ﬁtness value beyond that threshold. We repeat
this process until all ﬁtness functions are satisﬁed, or until
the maximum number of generations is reached.

Algorithm 1 shows a high-level algorithm for the process
described above based on a genetic algorithm. Assuming P
is the initial population, that is a set of episodes containing
both faulty and non-faulty episodes, the algorithm starts

Algorithm 1: High-level genetic algorithm

Input: A set of episodes (P ) as the initial population,

solution archive α

Output: The updated archive α containing faulty

episodes

1 gen ←− 0
2 α ←− ∅
3 while F itSatisf ied = F alse & gen ≤ g do
4

F itness = F it(P )
Pnew ←− ∅
t1, t2 ←− ∅
rand = Random(0, 1)
if rand < c then

t1, t2 ←− Cross(P )
Pnew ←− Pnew ∪ {t1, t2}
tm ←− M ut(P ∪ {t1, t2}, m)
Pnew ←− Pnew ∪ tm
F itness = F it(Pnew)
U pdate F itSatisf ied
U pdate α
P ←− Select(P, Pnew)
gen ←− gen + 1

5

6

7

8

9

10

11

12

13

14

15

16

17

18 return α

an iterative process, taking the following actions at each
generation until the termination criteria are met (lines 3-17).
The search process is as follows:

1) We create a new empty population Pnew (line 5).
2) We create offspring using crossover (Cross) and muta-
tion (M ut), and add newly created individuals to Pnew
(lines 6-12).

3) We calculate the ﬁtness of the new population (line
13) and update the archive α and the condition
F itSatisf ied capturing whether all ﬁtness functions
are satisﬁed (line 14). The archive contains all solutions
that satisfy at least one of our three ﬁtness functions
(line 15).

4) If all ﬁtness functions are satisﬁed, we stop the process
and return the archive (i.e., set of solutions that satisfy
at least one ﬁtness function). Otherwise, the population
P is updated using the selection function and then we
move to the next generation (lines 16-17).

Furthermore, in our genetic search algorithm, we set the
crossover rate c to 75% and the mutation rate m to the 1
V
where V is the length of the selected episodes for mutation
based on the suggested parameters for genetic algorithms
in the literature [42]. In the following, we discuss in detail
each step of the search.

RL AgentData collection Extraction of sequences ofthe agent'sstates andactionsTraining the  ML model to predictfaulty episodesCreate newindividuals usingCrossover andMutationCalculate fitnessfunctionsRank individualsusing MOSASelect top rankindividuals and createnew generation Termination criteriasatisfied?Final EpisodesGenetic aglorithmYesNoTrained ML model topredict faulty episodesML-based fault predictionSamplingRL random  episodesRL training  episodesRL EpisodesML TrainingInitial population4.3 Initial Population

The initial population of our search problem is a set
containing |P | episodes. We use random executions of the
agent to build the initial population of the generic search.
Consequently, we initiate the environment with different
initial states in which we randomly change the alterable
parameters of the environment when available (e.g., chang-
ing the weather or time of the day in the running example,
changing the starting position of the car). We execute the
RL agent starting from the randomly selected initial states
and store the generated episodes as initial population of the
search.

4.4 Fitness Computations

A ﬁtness function quantitatively assesses the extent to
which an individual ﬁts the search objectives and is meant
to effectively guide the search. Recall that our objective is
to ﬁnd faulty episodes that can exhibit functional faults.
We therefore deﬁne the following three ﬁtness functions
to capture the extent to which an episode is close to being
faulty.

4.4.1 Reward

The ﬁrst ﬁtness function is meant to drive the search
towards episodes with low reward. The reward ﬁtness
function of an episode is deﬁned as follows.

6

In the search process, instead of maximizing P rf [e], we
minimize its negation f2(e) to (1) have a consistent mini-
mization problem across all ﬁtness functions, and (2) guide
the search towards ﬁnding episodes with a high probability
of functional faults.

4.4.3 Certainty Level

This ﬁtness function captures the level of certainty asso-
ciated with the actions taken in each state within an episode.
It is calculated as the average difference in each state-action
pair between the probability of the chosen action, assigned
according to the learned policy, compared to the second-
highest probability assigned to an action [43].

A higher accumulated certainty level across the sequence
of actions in an episode suggests that the agent is overall
more conﬁdent about the selected actions. A lower accumu-
lated certainty level, on the other hand, can guide our search
towards situations in which the agent is highly uncertain of
the selected action. Thus, it is relatively easier to lure the
agent to take another action, which makes these episodes
suitable for applying search operators.

The certainty level is calculated as shown below (Equa-
tion 5), where e is the given episode, |e| is its length, ai is
the selected action in state si, i.e., ai is the action with the
highest selection probability, Ai is the set of possible actions
in state si, and Pr(ai|si) is the probability of selecting ai in
state si.

f1(e) = r(cid:48)
e

(3)

(cid:80)|e|

i=1 (Pr(ai|si) −

f3(e) =

Pr(aj|si))

max
aj ∈Ai & j(cid:54)=i
|e|

(5)

where r(cid:48)

e is the accumulated reward of an episode e.
In the initial population, the rewards of selected episodes
are known. Also, when genetic operators are applied, we
calculate the reward of new individuals using the reward
function according to Deﬁnition 1. The search aims to min-
imize the reward of episodes over generations to guide the
search towards ﬁnding faulty episodes. As we explained
earlier, relying only on the agent’s reward is however not
always sufﬁcient to detect functional faults. For this reason,
we consider two other ﬁtness functions that we describe
in the following to further drive the search towards faulty
episodes.

4.4.2 Probability of Functional Fault

The second ﬁtness function captures the probability for
an episode to contain a functional fault. Such probability is
predicted using a Machine Learning (ML) model. The ﬁtness
function is deﬁned as follows:

f2(e) = 1 − P rf [e]

(4)

where P rf [e] is the probability of having a functional fault
in an episode e ∈ E, and E is the space of all possible
episodes. In the context of the running example, driving
very close to obstacles has a higher probability of reveal-
ing a functional fault (high probability of collision) than
an episode that maintains a safe distance from them. We
therefore want the ﬁrst episode to be favored by our search
over the second one. The ML model that we use takes an
episode as input, uses its state-action pairs as features, and
returns the probability of functional fault for that episode.
A detailed explanation of the probability prediction method
using ML is provided in section 4.4.4.

In our search algorithm, we aim to minimize this ﬁtness
function to guide the search towards ﬁnding episodes with
high uncertainty levels.

4.4.4 Application of Machine Learning for Estimating Prob-

abilities of Functional Faults

A machine learning algorithm is used to learn functional
faults and estimate their probabilities in episodes without
executing them. This model is expected to take episodes as
input and predict the probabilities of functional faults. The
labels of each episode are functional faulty or not faulty. We
choose Random Forest as a candidate modeling technique
because (1) it can scale to large numbers of features, and
(2) its robustness to overﬁtting has been well studied in the
literature [44], [45].

4.4.5 Preparation of Training Data

To build the above-mentioned machine learning model,
the training data are collected relying on episodes from
training and random executions of the RL agent. More pre-
cisely, our ML training dataset contains both faulty and non-
faulty episodes generated through the training and random
executions of the agent.
Episodes from RL training. We sample episodes from the
training phase of the agent to increase the diversity of the
dataset. We also include such episodes in case we do not
ﬁnd enough faulty episodes based on random executions.
Such process of providing data with different types of
episodes (i.e., functional-faulty and non-faulty) makes the
training of ML models possible. Since the training phase
of RL agents is exploratory, it contains a diverse set of

faulty and non-faulty episodes, which help learning and
increase model accuracy. One issue with sampling from the
training episodes is that they may not be consistent with the
ﬁnal policy of the trained agent. The agent may execute a
faulty episode during training because of (1) randomness in
action selection, due to the exploratory nature of the training
process, and (2) incomplete agent training. To alleviate this
issue, when sampling to form the training dataset of the ML
model, we give a higher selection probability to the episodes
executed in the later stages of the training, since they are
more likely to be consistent with the ﬁnal behaviour of the
trained agent.

Assuming a sequence of n episodes ([Ei : 1 (cid:54) i (cid:54) n])
that are explored during the training of the RL agent, the
probability of selecting episode Ei (Pr[Ei]) is calculated as
follows.

Pr[Ei] =

i
Σn
j=1j

(6)

We thus give a higher selection probability to episodes
executed in the later stage of the training phase of the agent
(Pr[E1] < Pr[E2] < ... < Pr[En]).
Episodes from random executions. To build the training
dataset of our ML model, we also include episodes gen-
erated through random executions of the agent to further
diversify the training dataset with episodes that are consis-
tent with the ﬁnal policy of the agent. In practice, we use the
episodes of the initial population of the generic search since
they have been already created with random executions of
the RL agent (section 4.3), thus minimizing the number of
simulations (and therefore the testing budget).

4.4.6 State Abstraction for Training Data

After collecting the training episodes, we need to map
each concrete state to its corresponding abstract state to
reduce the state space and thus enabling effective learn-
ing. Indeed, this is meant to facilitate the use of machine
learning with more abstract features. To do so, we rely on
the transitive Q∗-irrelevance abstraction method which was
described in section 2.2.

The state abstraction process is deﬁned in Algorithm 2.
The algorithm takes the concrete states as input and ﬁnds
abstract states sφd ∈ Sφ considering the abstraction function
of φd where d is the abstraction level. For each concrete
state, we try to ﬁnd the abstract state that corresponds
to the concrete state by calculating the Q∗-values of all
available actions, as described in section 2.2. If a match
with an abstract state of a previous concrete state that was
already processed is found, we assign the abstract state to
the concrete state. Otherwise, we create a new abstract state.

4.4.7 Feature Representation: Presence and Absence of

Abstract States

To enable effective learning, each episode consists of
state-action pairs, where the states are abstract states instead
of concrete states. To train the ML model, we determine
whether abstract states are present or not in episodes and
use this information as features. As described below, each
episode is encoded with a feature vector of binary values
denoting the presence (1) or absence (0) of an abstract state
Sφ
i in the episode and n is the total number of abstract states.

Algorithm 2: High-level algorithm to create ab-
stract states

7

Input: Set of states S, abstraction level d
Output: Abstract states Sφd

1 Sφd ←− ∅
2 len ←− 0
3 for si ∈ S do
4

if Sφd = ∅ then

5

6

7

8

9

10

11

12

13

14

len ←− len + 1
append si to Sφd
1

F ound = F alse
for j in range(1, len) do

if φ(si) = Sφd
then
j
append si to Sφd
j
F ound = T rue

if Found = False then
len ←− len + 1
append si to Sφd
len

15 return Sφd

episodei

1 Sφ
Sφ
2
1
0

· · · Sφ
i
0
· · ·

· · · Sφ
n
1
· · ·

The main advantage of this representation is that it is
amenable to the training of standard machine learning clas-
siﬁcation models. Furthermore, we were able to signiﬁcantly
reduce the feature space by grouping similar concrete states
through state abstraction, where the selected action of the
agent is the same for all concrete states within one abstract
class. As a result, considering n different abstract states, the
feature space of this representation is 2n. Note that we only
consider the abstract classes that have been observed in the
training dataset of the ML model, which we expect to be
rather complete. Further, in this feature representation, the
order of the abstract states in the episodes is not accounted
for, which might be a weakness if we are not able to
predict functional faults as a result. Empirical results will
tell whether the two above-mentioned potential problems
materialize in practice.

4.4.8 Multi-Objective Search

We need to minimize the above-mentioned ﬁtness func-
tions to achieve our goal and this is therefore a multi-
objective search problem. More speciﬁcally, our multi-
objective optimization problem can be formalized as fol-
lows:

min
x∈E

F (x) = (f1(x), f2(x), f3(x))

(7)

where E is the set of possible episodes in the search
space, F : E −→ R3 that consists of three real-value objective
functions f1(x), f2(x), f3(x), and R3 is the objective space
of our optimization problem.

4.5 Search Operators

We describe below three genetic operators. The ﬁrst
operator is crossover, which generates new offspring using
slicing and joining high-ﬁtness, selected individuals. The
second operator is mutation which introduces small changes

in individuals to add diversity to the population, thus
making the search more exploratory. Finally, the selection
operator determines which individuals survive to the next
generation. We provide below a detailed description of how
we deﬁned these operators.

4.5.1 Crossover

The crossover process is described in Algorithm 3. It uses
the population as input and creates offspring as output. It
begins by sampling an episode (line 2) with the sample
function. This function draws an episode using tournament
selection [46]. In a K-way tournament selection, K indi-
viduals are selected and we run tournament between the
selected individuals where ﬁtter individuals are more likely
to be selected for reproduction. Then, we randomly select a
crossover point (line 3) using the uniform distribution.

After ﬁnding the crossover point, we must ﬁnd a match-
ing pair (line 4). We do so by considering individuals in
the population containing the abstract state of the pair
selected as a crossover point. The search function tries
to ﬁnd a matching pair for the crossover point based on
the Q∗-irrelevance abstraction method (section 2.2). If no
matching pair is found (line 5), we repeat the process from
the beginning (lines 1-5). Otherwise, offspring are created
on lines 6-9. For example, let us assume that the selected
parent is as follows:

P arent = [(s1, a1), (s2, a2), ...,(sf −1, af −1), (sf , af )

, (sf +1, af +1), ..., (sm, am)]

(8)

where (sf , af ) is the pair selected as a crossover point.

The matching function tries to ﬁnd an episode containing
a pair which has a concrete state that belongs to the same
abstract class as state (sf ) to ensure the validity of the new
episode. Recall that all states in the same abstraction class
are perceived to be the same by the RL agent. Also, since
they have the same Q∗-values, their certainty level is the
same.

1, a(cid:48)
v, a(cid:48)

1), (s(cid:48)
2, a(cid:48)
v), ..., (s(cid:48)

match = [(s(cid:48)
, (s(cid:48)

2),..., (s(cid:48)
n, a(cid:48)
n)]
v and sf result into the same abstract state based
on our abstraction method. As a result the selected actions
are also the same.

where s(cid:48)

v−1, a(cid:48)

v−1)

(9)

The newly created offspring are:

of f spring1 = [(s1, a1), ..., (sf −1, af −1)
n, a(cid:48)

v), ..., (s(cid:48)

v, a(cid:48)

, (s(cid:48)

n)]

of f spring2 = [(s(cid:48)

1, a(cid:48)

1), ..., (s(cid:48)

v−1, a(cid:48)

v−1)

, (sf , af ), ..., (sm, am)]

(10)

(11)

The ﬁrst offspring contains the ﬁrst part of the matching
individual up to the crossover point with state sf −1 and the
second part is taken from the parent and vice versa for the
second offspring.

Based on the selected state abstraction method (sec-
tion 2.2), we create episodes that are more likely to be
valid though this is not guaranteed. Also, we may get

Algorithm 3: High-Level Crossover Algorithm

Input: Population P
Output: Offspring B1 and B2

8

1 do
2

3

parent ←− sample(P )
l ←− CrossoverP oint(parent)
match ←− search(P, parent[l])

4
5 while match = ∅
6 B1[0 : l] ←− match[0 : l]
7 B1[l : end] ←− parent[l : end]
8 B2[0 : l] ←− parent[0 : l]
9 B2[l : end] ←− match[l : end]
10 return B1, B2

inconsistent episodes (i.e., episodes that cannot be executed
by the RL agent). Furthermore, due to the high simulation
cost of the RL environment, we are not executing episodes
after applying crossover during the search. The validity
of the episodes in the ﬁnal archive is therefore checked
by executing the ﬁnal high-ﬁtness episodes. The execution
process is described in detail in section 4.6.

4.5.2 Mutation

The mutation operator starts with selecting an episode

using a K-way tournament selection.

Then a mutation point is randomly selected using the
uniform distribution. To ensure the exploratory aspect of
the mutation operator, we alter the state of the mutation
point using some image transformation methods selected
considering the environment and the learning task to pro-
duce realistic and feasible states [47], [9].

These transformations are context-dependent. They
should be realistic and representative of situations with
imprecise sensors or actuators, as well as external factors
that are not observable. In the case of our running example,
here are example transformations matching such situations:
changing the brightness and the contrast of the image,
adding tiny black rectangles to simulate dust on the camera
lens, changing the weather. Another example is the Cart-
Pole problem (section 5.2) where the task is balancing a pole
on a moving cart. For such environment, we rely on other
transformations such as slightly changing the position, the
velocity, and the angle of the pole.

After mutating the gene, we run the episode. Although
executing episodes is computationally costly, mutation is
infrequent and helps create valid and consistent episodes
exploring unseen parts of the search space. Then, the up-
dated episode is added to the population. Also, if we ﬁnd
any failure during the execution of the mutated episodes,
we mark such episodes as failing and exhibiting a functional
fault.

Assume that (1) the selected episode for mutation is eh,
(2) we select (sc, ac) from eh as a candidate pair, and (3) the
mutated/transformed state for sc is st
c. The mutated episode
em
h is then as follows:

eh = [(s1, a1), ..., (sc, ac), (sc+1, ac+1),

..., (sm, am)]

(12)

h = [(s1, a1), ..., (st
em

c, at

c), ...]

(13)

where the states after the mutation point are determined
from executing episode em
h .

4.5.3 Selection

To select the best individuals that minimize our ﬁtness
function, we use the Many Objective Sorting Algorithm
(MOSA) [42] as it is speciﬁcally tailored to our application
context, software test automation. MOSA is a dominance-
based multi-objective search algorithm based on NSGA-
II [48]. Although traditional dominance-based algorithms
like NSGA-II and SPEA-II [49] show poor performance in
problems with many objectives [50], MOSA performs well
even for a large number of objectives as it tries to generate
solutions that cover the ﬁtness functions separately, instead
of ﬁnding a well-distributed set of solutions (i.e., diverse
trade-offs between ﬁtness functions). Considering that there
are no dependencies between our three ﬁtness functions and
therefore no trade-offs to be found, we selected MOSA in
our work. Indeed, it is expected that minimizing each ﬁtness
function, regardless of the others, can individually result in
a faulty episode.

MOSA works as follows. In a similar way to NSGA-II,
MOSA starts from an initial population and generates at
each generation, new offspring using genetic operators (i.e.,
mutation and crossover). We calculate the ﬁtness value of
each individual in the population based on the three ﬁtness
functions that we described in section 4.4. MOSA then uses
a novel preference method to rank the non-dominant solu-
tions. In this preference method, the best solutions according
to each ﬁtness function, are rewarded with the rank = 0,
and the other solutions are ranked based on the traditional
non-dominated sorting in NSGA-II. During the transition to
a new population, we select the highest ranked individuals
using MOSA and add them to the new population without
any changes. We also transfer a subset of the individuals
with the highest ﬁtness from the previous population to
avoid losing the best solutions. Finally, an archive is used
to store the best individuals for each individual ﬁtness
function.

4.6 Execution of Final Results

After completing the execution of the genetic algorithm,
we obtain a population that contains episodes with a high
fault probability. We need to execute these ﬁnal episodes
to check their validity as well as their consistency with
the policy of the agent and whether they actually trigger
failures. We assume that an episode is consistent if the RL
agent is able to execute it. We retain failing episodes that are
both valid and consistent.

During the execution process, we may observe devia-
tions where the agent selects an action other than the action
in the episode. To deal with such deviations, we replace
the state observed by the agent with the corresponding
state from the episode and observe the selected action. For
example, let us assume that we want to execute an episode
e(cid:48) produced by STARLA where e(cid:48) = [(s(cid:48)
i ∈
A, 0 ≤ i ≤ n, n ∈ N]. We set the state of the simulator to
the initial state of the episodes s(cid:48)
0. Then we use the states

i ∈ S, a(cid:48)

i)|s(cid:48)

i, a(cid:48)

9

from the environment as input to the agent and we check
the action selected by the agent. If during the execution, the
agent selects an unexpected action ai (cid:54)= a(cid:48)
i at state si, we
i from episode e(cid:48) to drive the agent
replace si with state s(cid:48)
to select action a(cid:48)
i. If the action selected by the agent is not
a(cid:48)
i, we consider that episode e(cid:48) is invalid and we remove it
from the ﬁnal results.

Replacing states in the situation described above is
acceptable since (1) we assume that the environment is
stochastic, (2) states in episode e(cid:48) are real concrete states
generated in the environment, and (3) we noticed that the
states of the environment and in the episodes where devi-
ations occur are very similar. The latter is likely due to the
selection of the crossover point based on identical abstract
states. Indeed, we observed that 94% of the environment
and episode states where deviations occur in the Cart-Pole
environment, which we will describe in detail in section 5.2,
have a cosine distance lower than 0.25. Replacing similar
states where deviations of the agent occur is therefore sensi-
ble way to execute such episodes (if possible) since, in real-
world environments, we may anyway have incomplete or
noisy observations due to imperfect sensors.

5 EMPIRICAL EVALUATION

This section describes the empirical evaluation of our
approach, including research questions, the case study, the
experiments, and results.

5.1 Research Questions

Our empirical evaluation is designed to answer the

following research questions.

5.1.1 RQ1. Do we ﬁnd more faults than Random Testing

with the same testing budget?

We aim to study the effectiveness of our testing approach
in terms of the number of detected faults compared to
Random Testing. We want to compare the two approaches
with the same testing budget deﬁned as the number of
executed episodes during the testing phase since, given that
the cost of real-world RL simulations can be high (e.g.,
autonomous driving systems), this is the main cost factor.

5.1.2 RQ2. Can we rely on ML models to predict faulty

episodes?

We want to investigate in this research question whether
it is possible to predict faulty episodes using an ML classi-
ﬁer. Since during the search we do not execute all episodes,
we want to use the probabilities of functional faults that are
estimated by an ML classiﬁer as ﬁtness function to guide
our search towards ﬁnding faulty episodes.

5.1.3 RQ3. Can we learn accurate rules to characterize the

faulty episodes of RL agents?

One of the goals of testing an RL agent is to under-
stand the conditions under which the agent fails. This can
help developers assess the risks of deploying the RL agent
and focus its retraining. Therefore, we aim to investigate
the learning of interpretable rules that characterize faulty
episodes from the ﬁnal episodes that are executed once the
search is completed.

5.2 Case Study

We consider in our study a Deep-Q-Learning (DQN)
agent on the Cart-Pole balancing problem from the OpenAI
GYM environment2. We have chosen the Cart-Pole problem
because it is open source and widely used as a bench-
mark problem in the RL literature [51], [52]. We have also
considered this benchmark as it includes a large number
of concrete states. Furthermore, the simulations in such
environment are also fast enough to enable large-scale ex-
perimentation.

In the Cart-Pole balancing problem, a pole is attached to
a cart, which moves along a track. The movement of the
cart is bidirectional and restricted to a horizontal axis with
a deﬁned range. The goal is to balance the pole by moving
the cart left or right and changing the velocity of the cart.

As depicted in Figure 2, the state of the system is

characterized by four variables:
• The position of the cart
• The velocity of the cart
• The angle of the pole
• The angular velocity of the pole

Figure 2: Cart-Pole balancing problem

We provide a reward of +1 for each time step when the
pole is still upright. The episodes end in three cases: (1)
the cart is away from the center with a distance more than
2.4 units, (2) the pole’s angle is more than 12 degrees from
vertical, or (3) the pole remains upright during 200 time
steps.

We deﬁne functional faults in the Cart-Pole balancing
problem as follows. If in a given episode, the cart moves
away from the center with a distance above 2.4 units, re-
gardless of the accumulated reward, we consider that there
is a functional fault in that episode.

5.3 Implementation

We used Google Colab and stable baselines [53] to imple-
ment an RL agent. Our RL agent is based on a DQN policy
network [54] using standard setting of stable baselines (i.e.,
Double Q-learning [4], and dueling DQN [55]). Our RL
agent has been trained in the Cart-Pole environment for
50000 time steps. The average reward of the trained agent is
124. In general, the pole is upright over 124 time steps out
of a maximum of 200.

2. https://gym.openai.com/envs/CartPole-v1/

10

Finally, we execute the search algorithm for a maximum
of 10 generations. The mean execution time of STARLA on
Google Colab was 89 minutes.

5.4 Evaluation and Results

5.4.1 RQ1. Do we ﬁnd more faults than Random Testing

with the same testing budget?

We want to study in this research question the effective-
ness of STARLA in ﬁnding more faults than Random Testing
when we consider the same testing budget B, measured as
the number of executed episodes. To do so, we consider two
practical testing scenarios:

• Randomly executed episodes are available or inex-
pensive: In the ﬁrst scenario, we assume that we want
to further test a DRL agent provided by a third-party
organization. We assume that both training episodes
and some randomly executed episodes of the RL agent,
used for testing the agent, are provided by the third-
party. Therefore, we can extract ML training data and
an initial population from such episodes without using
our testing budget.
We can also consider another situation where the RL
agent is trained and tested using both a simulator and
hardware in the loop [56]. Such two-stage learning of
RL agents has been widely studied in the literature
where an agent is trained and tested on a simulator
in order to ”warm-start” the learning on real hard-
ware [56], [57]. Since STARLA produces episodes with
a high fault probability, we can use it to test the agent
when executed on real hardware to further assess the
reliability of the agent. In such situation, STARLA uses
prior episodes that have been generated on the simu-
lator to build the initial population and executes the
newly generated episodes on the hardware. In such
case, randomly executed episodes using a simulator be-
comes relatively inexpensive. Therefore, only episodes
that are executed with hardware in the loop and in
the real environment are accounted for in the testing
budget.
To summarize, when randomly executed episodes are
available or inexpensive, the testing budget B is equal
to the sum of (1) the number of mutated episodes
that have been executed during the search, and (2) the
number of faulty episodes generated by STARLA that
have been executed after the search.

• Randomly executed episodes are generated with
STARLA and should be accounted for in the testing
budget: In the second scenario, we assume that the
agent is trained and then tested by the same organiza-
tion using STARLA. Therefore, we have access to the
training dataset but need to use part of our testing
budget, using random executions, to generate the initial
population. More precisely, the total testing budget in
this scenario is equal to the sum of (1) the number of
episodes in the initial population that have been gen-
erated through random executions of the agent, (2) the
number of mutated episodes that have been executed
during the search, and (3) the number of faulty episodes
generated by STARLA that have been executed after the
search.

Cart VelocityPole AnglePoleCart PositionReferenceAngularVelocityBecause of randomness in our search approach and its
signiﬁcant execution time (section 5.3), we re-executed the
search algorithm 20 times and stored at each run the gener-
ated episodes and the executed episodes with mutation. We
computed the mean number of generated functional-faulty
episodes N (in our case N=5313) and the mean number of
mutated executed episodes M (in our case M=128) over the
20 runs. We analyzed the distribution of the total number
of functional faults identiﬁed with STARLA over the 20
runs. Then, we randomly selected (with replacement) 100
samples from the set of episodes that have been generated
with Random Testing. Each sample contained B episodes to
ensure that we had the same testing budget as in STARLA.
In the ﬁrst scenario, B is equal to 5441 (which corre-
sponds to the mean number of generated faulty episodes
and executed mutated episodes with STARLA over the
20 runs). On the other hand, in the second scenario, B is
equal to 6941 since, as explained earlier, this testing budget
accounts for the number of episodes in the initial population
which have been generated with Random Testing (1500).

We analyzed the distribution of the identiﬁed faults in
the two testing scenarios, compared it with STARLA, and
reported the results in Figure 3. We should note that in the
ﬁrst scenario, we only compute faults that are generated
with the genetic search. We do not consider faults that are
in the initial population since we assume that these episodes
are provided to STARLA and are not included in the testing
budget. In contrast, in the second scenario, we include in
the ﬁnal results of STARLA, the faulty episodes in the initial
population as they are part of our testing approach and are
included in the testing budget.

11

faults between STARLA and Random Testing, we use the
non-parametric Mann-Whitney U-test [58] and compute the
corresponding p-value in both testing scenarios.

In the ﬁrst scenario, we obtain a p-value equal to 9.69e-
13 while in the second scenario the p-value is 9.70e-13,
clearly indicating signiﬁcance with a p-value< 0.01. This
result shows that our approach signiﬁcantly outperforms
Random Testing in detecting functional faults in DRL-based
systems.

Answer to RQ1: We ﬁnd signiﬁcantly more func-
tional faults with STARLA than with Random Test-
ing using the same testing budget.

5.4.2 RQ2. Can we rely on ML models to predict faulty

episodes?

We investigate the accuracy of the ML classiﬁer in pre-
dicting faulty episodes of the RL agent. To this end, we use
Random Forest to predict the probability of functional faults
in a given episode.

To build our training dataset, we sampled 2111 episodes
including 1500 episodes generated through random exe-
cutions of the agent and 611 episodes from the training
phase of the agent (as described in section 4.3). 733 of these
episodes correspond to functional faults while 1378 episodes
are non-functional faulty. We trained a Random Forest model
using this dataset to predict functional faults.

d

0.005
0.01
0.05
0.1
0.5
1
5
10
50
100

Abstract
states

195073
146840
33574
15206
2269
1035
134
48
8
4

Accuracy

Precision Recall

63%
63%
73%
92%
95%
97%
84%
79%
77%
77%

39%
39%
81%
92%
95%
97%
84%
81%
78%
78%

63%
63%
73%
92%
95%
97%
84%
79%
77%
77%

Table 1: Prediction of functional faults with Random For-
est. The ﬁrst column represents the abstraction level
d, the second column shows the number of abstract
states for each abstraction level and we report the
accuracy, precision, and recall in the next columns.

Figure 3: Number of functional-faulty episodes gener-
ated with STARLA compared to Random Testing

As we can see from the boxplots, our approach outper-
forms Random Testing in detecting faults in both scenarios.
In fact, in the ﬁrst scenario, the average number of faulty
episodes detected by STARLA is 2291 while an average of
1789 faulty episodes is detected with Random Testing. In the
second scenario, where we consider a bigger testing budget,
STARLA outperforms Random Testing as well by identi-
fying, on average, 2861 faulty episodes compared to 2367
with Random Testing. To assess the statistical signiﬁcance of
the average difference of the number of detected functional

Because of the high number of concrete states in our
dataset (about 250 000), we need to reduce the state space
by using state abstraction to facilitate the learning process
of the Random Forest models. As presented in section 4.4.6,
we used the Q*-irrelevance state abstraction technique [20]
to reduce the state space. We experimented with several
values for the abstraction level d (section 2.2) and reported
the prediction results in terms of precision, recall, and ac-
curacy in Table 1. We obtained less abstract states when we
increased d because more concrete states were included in
the same abstract states. For each value of d, we considered a
new training dataset (with different abstract states). For each
dataset, we trained Random Forest by randomly sampling

Number of functional faulty episodesScenario 1Scenario 2STARLARandom TestingSTARLARandom Testing70% of the data for training and used the remaining 30% for
testing. The overall prediction results for functional faults
are promising. Indeed, the best results for the prediction
of functional faults yield a precision and recall of 97%. Also,
we observe that when we increase the state abstraction level,
the accuracy of the ML classiﬁers improves until it plateaus
and then starts to decrease as information that is essential
for classiﬁcation is lost. This highlights the importance of
ﬁnding a proper state abstraction level to (1) facilitate the
learning process of the ML classiﬁers, and (2) predict more
accurately functional faults. Note that, in our ﬁnal approach
we consider d equal to one as this abstraction level max-
imizes the accuracy of the ML model while signiﬁcantly
decreasing the total number of distinct abstract states in the
dataset.

Answer to RQ2: By using an ML classiﬁer (based
on Random Forest) combined with state abstraction,
we can accurately classify the episodes of RL agents
as having functional faults or not. Such classiﬁer
can therefore be used as ﬁtness functions in our
search. Finding a suitable level of state abstraction
is, however, essential to increase the learnability of
the ML classiﬁer and thus to improve the accuracy
of faults prediction results.

5.4.3 RQ3. Can we learn accurate rules to characterize the

faulty episodes of RL agents?

We investigate the learning of interpretable rules that
characterize faulty episodes to understand the conditions
under which the RL agent can be expected to fail. Con-
sequently, we rely on interpretable ML models, namely
Decision Trees, to learn such rules. We assess the accuracy
of decision trees and therefore our ability to learn accurate
rules based on the faulty episodes that we identify with our
testing approach. In practice, engineers will need to use such
an approach to assess the safety of using an RL agent and
target its retraining.

In our analysis, we consider a balanced dataset that
contains (1) faulty episodes created with STARLA, and (2)
non-faulty episodes obtained through random executions of
the RL agent. We consider the same proportions of faulty
and non-faulty episodes. Such dataset would be readily
available in practice to train decision trees. For training,
we use the same type of features as for the ML model
that was used to calculate one of our ﬁtness functions
(section 4.4.7). Each episode is encoded with a feature vector
of binary values denoting the presence (1) or absence (0) of
an abstract state in the episode. We rely on such features
since the ML model that we have used to predict functional
faults showed good performance using such representation.
Moreover, we did not rely on the characteristics of concrete
states to train the model and extract the rules due to (1) the
potential complexity of state characteristics in real-world RL
environments, and (2) Q∗-values matching abstract states
are more informative since they also capture the next states
of the agent and the optimal action (i.e., agent’s perception).
Since we simply want to explain the faults that we detect
by extracting accurate rules, we measure the accuracy of
the models using K-fold cross-validation. We repeat the

12

same procedure for all 20 executions of STARLA to obtain
a distribution of the accuracy of the decision trees. More
speciﬁcally, we study the distributions of precision, recall
and F1-scores for the detected faults and report the results
in Figure 4.

Figure 4: Accuracy of rules predicting faults

As we can see in the ﬁgure, we learned highly accurate
decision trees and therefore rules (tree paths) that char-
acterize faults in RL agents. Rules predicting faults have
a median precision of 98.75%, a recall of 98.90% and an
F1-score of 98.85%. The rules that we extract consist of
conjunctions of features capturing the presence or absence
of abstract states in an episode. We provide in the following
an example of a rule that we obtained:

12 and Sφ

R1: not(Sφ

5 ) and Sφ
where rule R1 states that an episode is faulty if there is no
concrete states in the episode that belong to abstract state Sφ
5
and we have at least two concrete states matching abstract
state Sφ

23, respectively.

12 and Sφ

23

From a practical standpoint, such highly accurate rules
can help developers understand, with high conﬁdence, the
conditions under which the agent fails. One can analyze
for example, the concrete states that correspond to abstract
states leading to faults to extract real-world conditions of
failure. For example, to interpret the rule R1, ﬁrst we extract
all faulty episodes following this rule. Then, we extract from
these episodes all concrete states belonging to the abstract
states that must be present according to R1, i.e., Sφ
12 and
Sφ
23, respectively. For abstract states for which the rule states
they should be absent (abstract state Sφ
5 in our example), we
extract the set of all corresponding concrete states from all
episodes in the ﬁnal dataset. Finally, for each abstract state
in the rule, we analyze the distribution of each characteristic
of the corresponding concrete states (i.e., the position of
the cart, the velocity, the angle of the pole and the angular
velocity) to interpret the situations under which the agent
fails.

Due to space limitation, we include the boxplots of
the mentioned distributions of states characteristics in our
replication package. Note that we did not directly rely on the
abstract states Q∗-values to understand the failing condi-

13

6 DISCUSSIONS

We propose in this paper STARLA, a search-based ap-
proach to detect faulty episodes of an RL agent. To the
best of our knowledge, this is the ﬁrst testing approach
focused on testing the agent’s policy and detect what we
call functional faults.

We rely on a small proportion of the training data of
an RL agent and do not need access to the internals of the
RL-based system, hence the data-box nature of our solution.
Our testing approach outperforms Random Testing of the
RL agent since we were able to ﬁnd signiﬁcantly more func-
tional faults with the same simulation budget. However,
Random Testing might outperform our testing approach in
simple environments with fast simulations since it typically
generates a much higher number of episodes, including
faulty ones. Nonetheless, RL agents are generally used in
complex environments, e.g., cyber-physical systems, where
the simulation and therefore test execution costs are very
high. Narrowing the search towards the faulty space of the
agent is therefore crucial to optimize RL testing in a scalable
way.

Relying on state abstraction helped us reduce the search
space and increase the learnability of ML models that we
used to (1) calculate the probabilities of functional faults,
and (2) extract and interpret the rules characterizing faulty
episodes. However, depending on the type of the RL task, in
practice, one needs to select the right state abstraction type
and level to effectively guide the search towards ﬁnding
faults in the RL agent. State abstraction has allowed us
to extract accurate rules predicting the presence of faults
and thus enables effective risk analysis. Though we investi-
gated different representations of features, such as encoding
episodes with sequences of abstract states, the accuracy
of the ML model was only slightly improved. Therefore,
it was considered not worth the additional complexity of
accounting for such sequential information.

We should note that the number of functional faults in
our case study is relatively high since the reward func-
tion of the RL agent does not help prevent such type of
fault (despite of the high average reward of the agent). As
mentioned in section 5.2, we have relied in this work on
the standard reward function of the Cart-Pole balancing
problem where the reward of the agent does not increase
when it crosses the borders (i.e, where there is a functional
fault and the episode terminates). We are using a standard,
widely used but artiﬁcial benchmark to validate our testing
approach but we expect the number of functional faults in
real world RL agents to be much lower. Indeed, in more
complex environments, a high penalty for functional faults
could be involved in the reward function to minimize them
and prevent safety violations. However, as we explained
in section 3.1, this is not always enough to prevent safety
critical situations since the agent’s reward could still be
relatively acceptable in the presence of functional faults.

We also investigated different sizes of the initial pop-
ulation (i.e., 500, 1050 and 3000) and obtained consistent
results: STARLA outperformed Random Testing in terms of
the total number of detected functional faults. Furthermore,
we observed that the number of detected faults increased
with the size of the initial population of the search. In our

Figure 5: Interpretation of Rule R1. Each cart repre-
sents one abstract state. The gray cart depicts the state
of the system in abstract state Sφ
5 , which should be
absent in the episode. The black carts represent the
presence of abstract states Sφ
23, respectively.
Having both latter states appearing in an episode and
not having the state on the left is highly likely to lead
to a fault.

12 and Sφ

tions of the agent since they are not easily interpretable. We
rely on the median values of the distribution of the states’
characteristics to illustrate each abstract state and hence the
failing conditions. We illustrate in Figure 5 such conditions.
Our analysis shows that the presence of abstract states
Sφ
12 and Sφ
23 represent situations where the cart is close to
the right border of the track and the pole strongly leans
towards the right. To compensate for the large angle of the
pole, as you can see in the ﬁgure, the agent has no choice
but to push the cart to the right, which results in a fault
because the border is crossed. Moreover, abstract state Sφ
5
represents a situation where (1) the angle of the pole is not
large, and (2) the position of the cart is toward the right
but not close to the border. In such situation, the agent
will be able to control the pole in the remaining area and
keep the pole upright without crossing the border, which
justiﬁes why such abstract state is absent in faulty episodes
that satisfy rule R1. Note that we only provide here an
example of a faulty rule from our case study. Different
rules that consist of more complex combinations of different
abstract states can be extracted and therefore analyzed. Such
interpretable rules can thus assist engineers in ensuring
safety and analyzing risks prior to deploying the agent.

Answer to RQ3: By using our search-based tech-
nique and interpretable ML models, such as Decision
Trees, we can accurately learn interpretable rules that
characterize the faulty episodes of RL agents. Such
rules can then serve as the basis for risk analysis
before deployment of the agent to avoid safety vio-
lations.

PoleCart  PositionReference  Position2.4Reference  Angle MaximumAngle 0case study, the accuracy of the faults prediction decreased
for training data sets with less than 670 episodes. In some
practical cases, costly simulations may lead to limited data
for testing RL agents with STARLA. Consequently, this can
affect the accuracy of ML models due to small ML training
datasets and thus the results of the genetic search (i.e., due
to small initial population). From a practical standpoint, the
size of the initial population is bounded by a predetermined
testing budget and can be determined according to the
accuracy of the ML model. Depending on the case study, we
may choose the size of the initial population that maximizes
the accuracy of the ML model while consuming a reasonable
portion of the testing budget.

We investigate in this paper the learning of interpretable
rules that characterize faulty episodes to understand the
conditions under which the RL agent can be expected to
fail. If accurate, these rules can help developers focus their
analysis on speciﬁc abstract states that lead to faults, and
analyze the risks related to the deployment of the RL
agent. For example, after analyzing the failing rules of the
agent, engineers can use abstract states leading to faults
to automatically ensure safety at run-time. The agent state
can be monitored to assess the risks and activate corrective
measures. To prevent a failure, for example, the agent can
be forced to avoid speciﬁc actions leading to states that can
violate safety requirements.

This paper takes a ﬁrst step towards testing RL agents
using data-box genetic search. Our proposed testing ap-
proach and associated results have several practical implica-
tions. The generated faulty episodes and the corresponding
rules that characterize them can be used for safety analysis
and retraining. Indeed, analyzing the states and actions in
the generated faulty episodes could help developers (1)
understand root-causes for faults in the RL agent, and (2)
analyze the safety risks at run-time based on the prevalence
of such root causes in practice and the consequences of
identiﬁed faults. Moreover, one can retrain the agent using
some of the generated faulty episodes, guided by the rules,
as a mechanism to improve the policy of the agent.

7 THREATS TO VALIDITY

We discuss in this section the different threats to the
validity of our study and describe how we mitigated them.

Internal threats concern the causal relationship between
the treatment and the outcome. Invalid episodes generated
by STARLA might threaten internal validity. To mitigate
this threat and to ensure the validity of the generated
ﬁnal episodes, we have relied on state abstraction and the
application of realistic state transformations when using
the search operators. For instance, for crossover, instead
of selecting random crossover points, we have used state
abstraction to ﬁnd a matching pair for the crossover point.
Furthermore, to ensure the validity and the exploratory
aspect of the mutation operator, we alter the state of the
mutation point using realistic state transformation methods
to produce realistic and feasible states that could happen
in the real-world environment. Finally, the validity of the
episodes is checked through executing them. Thus, we only
retain valid failing episodes in our ﬁnal results.

14

The choice of an inappropriate state abstraction method
and level might also be a threat. To mitigate it, we have
studied several state abstraction methods and have tried
different abstraction levels to train our ML model. We have
selected the best abstraction level that maximizes the accu-
racy of the model and signiﬁcantly decreases the number of
abstract states.
Conclusion threats are concerned with the relationship
between treatment and outcome. The randomness in our
search approach leads to the generation of different episodes
after each run of STARLA. To mitigate this threat, we have
considered in our experiments the execution of several runs
of our search method and studied the distribution of the
number of the detected faults for both our method and
Random Testing.
Reliability threats concern the replicability of our study
results. We rely on a publicly available RL environment
and provide online all the materials required to replicate
our study results. This includes the set of the executed and
generated episodes and the different conﬁgurations that we
used in our experiments.
External threats concern the generalizability of our study.
Due to the high computational expense of our experiments
and the lack of publicly available realistic RL agents, we
relied on one case study in this paper. However, we miti-
gated this threat by using a widely studied RL task which
is considered a valid benchmark problem in many RL-
related studies [59], [60], [61]. However, our approach is
customizable and can be applied on any other RL problem.
Furthermore, we aim as future work to apply our testing
approach on other RL problems to generalize more the
obtained results.

8 RELATED WORK

Several approaches have been proposed in the literature
to study the safety of RL agents during the training and exe-
cution phases. However, limited research has been targeted
at testing RL-based systems.

Nikanjam et al. [10] presented a taxonomy of DRL faults
and a tool to locate these faults in DRL programs. To build
the taxonomy, they analyzed DRL programs on Github, they
mined Stack Overﬂow posts and conducted a survey with
19 developers. They proposed DRLinter, a model-based fault
detection tool that relies on static analysis of DRL programs
and graph transformation rules to locate existing faults in
DRL source code. Although we have similar objectives,
this work greatly differs from ours as we detect faults
related to the execution of RL agents through search and
the generation of faulty episodes. Nonetheless, this work
may complement our approach and could be used as a root
cause analysis mechanism of the faults reported with our
search approach.

Trujillo et al. [7] studied the reliability of neuron cover-
age [62] in testing DRL systems. They studied the correlation
between coverage metrics and rewards obtained by two
different models of Deep Q-Network (DQN) that have been
implemented for the Mountain Car Problem [63]. They show
that neuron coverage is not correlated to the agent’s reward.
For instance, reaching high coverage does not necessarily
mean success in an RL task in terms of reward. They
also showed that maximum coverage is obtained through

excessive exploration of the agent, which leads to exploring
different actions that do not help maximize the agent’s
reward. Finally, they conclude that neuron coverage is not
suitable to guide the testing of DRL systems.

Several approaches have been proposed in the literature
to study the robustness of RL agents against adversarial
attacks [12], [13], [14]. For example, Ilahi et al. [12] studied
28 adversarial attacks on RL and provided a taxonomy of
existing attacks in the literature. They considered attacks
that rely on perturbing (1) the state space, (2) the reward
space, (3) the action space, and (4) the model space, where
one can perturb the learned parameters of the model. They
show that although many defense approaches are proposed
to increase the safety of DRL-based systems, the robustness
of such systems to all possible adversarial attacks is still an
open issue. This is because the proposed defense techniques
in the literature can respond to types of attacks they are built
for. Besides Moosavi Dezfooli et al. [64] argue that regardless
of the number of adversarial examples added to the training
data, they were able to generate new adversarial examples
to alter the normal behaviour of the system.

Huang et al. [13] studied the robustness of neural net-
work policies in presence of adversaries. They studied the
effectiveness of black-box and white-box adversarial attacks
on policy networks such as DQN [54], TRPO [65] and
A3C [66], trained on Atari games [67]. They showed that ad-
versarial attacks can signiﬁcantly degrade the performance
of the agent, even with small imperceptible perturbations.

Pan et al. [14] studied the robustness of the reinforcement
learning agent in the speciﬁc learning task of power system
control. They proposed a new adversary in both white-box
and black-box (using a surrogate model) scenarios. They
studied the effectiveness of their method and compared it
with random and weighted adversarial attacks previously
proposed for power system controls [68], [69]. Moreover,
they studied the robustness improvement of the agent
trained with adversarial training.

Other existing approaches in the literature [70], [15], [71],
[72], [73], [74], [75], [76], [77], [78] have proposed adversarial
training techniques to increase the robustness of RL agents
to adversarial attacks. For example, Pattanaik et al. [70]
proposed a training approach of DRL agents to increase
their robustness to gradient-based adversarial attacks. They
train the agent using adversarial samples generated from
gradient-based attacks. They show that adding noise to the
training episodes increases the robustness of the DRL agent
to adversarial attacks.

Tan et al. [15] also proposed an adversarial training
approach of DRL agents used for decision and control tasks.
The purpose of their training approach is to increase the
robustness of DRL agents against adversarial perturbations
to the action space (within speciﬁc attack budgets). Conse-
quently, they relied on gradient-based white-box adversarial
attacks during the training phase of a DRL agent. They show
that the proposed method increases the robustness of the
agent against similar attacks.

The above works differ from our testing approach as we
do not focus on the robustness of RL agents to adversarial
attacks. Rather, we test the policies of RL agents, without
using any of their internal information, by relying on genetic
search to effectively ﬁnd faulty episodes.

15

9 CONCLUSION

We propose in this paper STARLA, a data-box search-
based approach to test the policy of DRL agents by effec-
tively searching for faulty episodes. We rely on a dedicated
genetic algorithm to detect functional faults. We make use
of an ML model to predict DRL faults and guide the search
towards faulty episodes. To this end, we use state abstrac-
tion techniques to group similar states of the agent and
signiﬁcantly reduce the state space. This helped us increase
the learnability of the ML models and build customized
search operators. We showed that STARLA outperforms
Random Testing as we ﬁnd signiﬁcantly more faults when
considering the same testing budget. We also investigated
how to extract rules that characterize faulty episodes of
the RL agent using our search results. The goal was to
help developers understand the conditions under which the
agent fails and thus assess the risks of deploying the RL
agent.

As future work, we aim to detect other types of faults
such as reward faults and investigate the retraining of the
RL agent using the generated faulty episodes. We aim to
study the effectiveness of such episodes in improving the
agent’s policy. We also want to support the safety of RL-
based critical systems by providing mechanisms based on
ML and state abstraction to identify sub-episodes that may
lead to hazards or critical faults.

ACKNOWLEDGEMENTS

This work was supported by a research grant from
General Motors as well as the Canada Research Chair and
Discovery Grant programs of the Natural Sciences and
Engineering Research Council of Canada (NSERC).

REFERENCES

[1] M. A. Wiering and M. Van Otterlo, “Reinforcement learning,”
Adaptation, learning, and optimization, vol. 12, no. 3, p. 729, 2012.

[2] M. Sewak, Deep reinforcement learning. Springer, 2019.
[3] C. Lei, “Deep reinforcement learning,” in Deep Learning and Prac-

tice with MindSpore. Springer, 2021, pp. 217–243.

[4] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement

learning with double q-learning,” 2015.

[5] G. Dulac-Arnold, D. Mankowitz, and T. Hester, “Chal-
learning,” arXiv preprint

lenges of real-world reinforcement
arXiv:1904.12901, 2019.

[6] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dy-
namic Programming, 1st ed. USA: John Wiley & amp; Sons, Inc.,
1994.

[7] M. Trujillo, M. Linares-V´asquez, C. Escobar-Vel´asquez, I. Dus-
paric, and N. Cardozo, “Does neuron coverage matter for deep
reinforcement learning? a preliminary study,” in Proceedings of
the IEEE/ACM 42nd International Conference on Software Engineering
Workshops, 2020, pp. 215–220.
J. Lin, L. Xu, Y. Liu, and X. Zhang, “Black-box adversarial sample
generation based on differential evolution,” Journal of Systems and
Software, vol. 170, p. 110767, 2020.

[8]

[9] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing

of deep-neural-network-driven autonomous cars,” 2018.

[10] A. Nikanjam, M. M. Morovati, F. Khomh, and H. Ben Braiek,
“Faults in deep reinforcement learning programs: a taxonomy
and a detection approach,” Automated Software Engineering, vol. 29,
no. 1, pp. 1–32, 2022.

[11] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: a coverage-guided fuzz
testing framework for deep neural networks,” in Proceedings of
the 28th ACM SIGSOFT International Symposium on Software Testing
and Analysis, 2019, pp. 146–157.

[12] I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. Al-Fuqaha, D. T.
Huang, and D. Niyato, “Challenges and countermeasures for
adversarial attacks on deep reinforcement learning,” 2021.

[13] S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, and P. Abbeel,
“Adversarial attacks on neural network policies,” ArXiv, vol.
abs/1702.02284, 2017.

[14] A. Pan, Y. Lee, H. Zhang, Y. Chen, and Y. Shi, “Improving ro-
bustness of reinforcement learning for power system control with
adversarial training,” 2021.

[15] K. L. Tan, Y. Esfandiari, X. Y. Lee, S. Sarkar et al., “Robustifying
reinforcement learning agents via action space adversarial train-
ing,” in 2020 American control conference (ACC).
IEEE, 2020, pp.
3959–3964.

[16] T. Byun, S. Rayadurgam, and M. P. Heimdahl, “Black-box testing
of deep neural networks,” in 2021 IEEE 32nd International Sympo-
sium on Software Reliability Engineering (ISSRE).
IEEE, 2021, pp.
309–320.

[17] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and
M. Sun, “Tactics of adversarial attack on deep reinforcement
learning agents,” 2019.

[18] J. H. Holland, “Genetic algorithms,” Scientiﬁc american, vol. 267,

no. 1, pp. 66–73, 1992.

[19] R. Akrour, F. Veiga, J. Peters, and G. Neumann, “Regularizing
reinforcement learning with state abstraction,” in 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 534–539.

[20] D. Abel, D. Arumugam, L. Lehnert, and M. Littman, “State
abstractions for lifelong reinforcement learning,” in Proceedings
of
the 35th International Conference on Machine Learning, ser.
Proceedings of Machine Learning Research, J. Dy and A. Krause,
Eds., vol. 80.
[Online].
Available: http://proceedings.mlr.press/v80/abel18a.html

PMLR, 10–15 Jul 2018, pp. 10–19.

[21] “Gym library,” https://github.com/openai/gym, 2020.
[22] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[23] N. Jiang, “Notes on state abstractions,” 2018.
[24] L. Li, T. J. Walsh, and M. L. Littman, “Towards a uniﬁed theory of
state abstraction for mdps.” ISAIM, vol. 4, no. 5, p. 9, 2006.
[25] Y. Sun, X. Huang, D. Kroening, J. Sharp, M. Hill, and R. Ash-
more, “Testing Deep Neural Networks,” arXiv e-prints, p.
arXiv:1803.04792, Mar. 2018.

[26] Y. Sun, X. Huang, D. Kroening, J. Sharp, M. Hill, and R. Ashmore,
“Deepconcolic: Testing and debugging deep neural networks,” in
2019 IEEE/ACM 41st International Conference on Software Engineer-
ing: Companion Proceedings (ICSE-Companion), 2019, pp. 111–114.

[27] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and

D. Kroening, “Concolic testing for deep neural networks,” 2018.

[28] L. Ma, F.

Juefei-Xu, F. Zhang,

J. Sun, M. Xue, B. Li,
C. Chen, T. Su, L. Li, Y. Liu, and et al., “Deepgauge:
multi-granularity testing criteria for deep learning systems,”
Proceedings of
the 33rd ACM/IEEE International Conference on
Automated Software Engineering, Sep 2018. [Online]. Available:
http://dx.doi.org/10.1145/3238147.3238202

[29] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru,
S. Gowal, and T. Hester, “Challenges of real-world reinforcement
learning: deﬁnitions, benchmarks and analysis,” Machine Learning,
vol. 110, no. 9, pp. 2419–2468, 2021.

[30] T. Chaffre, J. Moras, A. Chan-Hon-Tong, and J. Marzat, “Sim-
to-real transfer with incremental environment complexity for
reinforcement learning of depth-based robot navigation,” arXiv
preprint arXiv:2004.14684, 2020.

[31] S. Ghamizi, M. Cordy, M. Gubri, M. Papadakis, A. Boystov,
Y. Le Traon, and A. Goujon, “Search-based adversarial testing and
improvement of constrained credit scoring systems,” in Proceed-
ings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2020, pp. 1089–1100.

[32] N. Carlini and D. Wagner, “Towards evaluating the robustness of
neural networks,” in 2017 ieee symposium on security and privacy
(sp).

IEEE, 2017, pp. 39–57.

[33] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C.-J.
Hsieh, “Robust deep reinforcement learning against adversarial
perturbations on state observations,” Advances in Neural Informa-
tion Processing Systems, vol. 33, pp. 21 024–21 037, 2020.

16

[35] X. Zhang, Y. Ma, A. Singla, and X. Zhu, “Adaptive reward-

poisoning attacks against reinforcement learning,” 2020.

[36] A. Rakhsha, X. Zhang, X. Zhu, and A. Singla, “Reward poisoning
in reinforcement learning: Attacks against unknown learners in
unknown environments,” 2021.

[37] P. Swazinna, S. Udluft, D. Hein, and T. Runkler, “Comparing
model-free and model-based algorithms for ofﬂine reinforcement
learning,” arXiv preprint arXiv:2201.05433, 2022.

[38] “Openai,”

https://spinningup.openai.com/en/latest/

spinningup/rl intro2.html, 2018, [Accessed 24 Jan. 2022.].

[39] I. Alsmadi, “Using genetic algorithms for test case generation and

selection optimization,” in CCECE 2010.

IEEE, 2010, pp. 1–4.

[40] D. Whitley, “A genetic algorithm tutorial,” Statistics and computing,

vol. 4, no. 2, pp. 65–85, 1994.

[41] T. Murata, H. Ishibuchi et al., “Moga: multi-objective genetic algo-
rithms,” in IEEE international conference on evolutionary computation,
vol. 1.

IEEE Piscataway, NJ, USA, 1995, pp. 289–294.
[42] A. Panichella, F. M. Kifetew, and P. Tonella, “Reformulating branch
coverage as a many-objective optimization problem,” in 2015
IEEE 8th International Conference on Software Testing, Veriﬁcation and
Validation (ICST), 2015, pp. 1–10.

[43] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and
M. Sun, “Tactics of adversarial attack on deep reinforcement learn-
ing agents,” in Proceedings of the 26th International Joint Conference
on Artiﬁcial Intelligence, ser. IJCAI’17.
AAAI Press, 2017, p.
3756–3762.

[44] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, p.
5–32, Oct. 2001. [Online]. Available: https://doi.org/10.1023/A:
1010933404324

[45] J. H. Friedman, “Greedy function approximation: a gradient boost-

ing machine,” Annals of statistics, pp. 1189–1232, 2001.

[46] B. L. Miller and D. E. Goldberg, “Genetic algorithms, selection
schemes, and the varying effects of noise,” Evol. Comput.,
vol. 4, no. 2, p. 113–131,
[Online]. Available:
jun 1996.
https://doi.org/10.1162/evco.1996.4.2.113

[47] K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated
whitebox testing of deep learning systems,” Commun. ACM,
[Online]. Available:
vol. 62, no. 11, p. 137–145, Oct. 2019.
https://doi.org/10.1145/3361566

[48] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and
elitist multiobjective genetic algorithm: Nsga-ii,” IEEE Transactions
on Evolutionary Computation, vol. 6, no. 2, pp. 182–197, 2002.
[49] V. J. Amuso and J. Enslin, “The strength pareto evolutionary
algorithm 2 (spea2) applied to simultaneous multi- mission wave-
form design,” in 2007 International Waveform Diversity and Design
Conference, 2007, pp. 407–417.

[50] R. Tanabe, H. Ishibuchi, and A. Oyama, “Benchmarking multi- and
many-objective evolutionary algorithms under two optimization
scenarios,” IEEE Access, vol. 5, pp. 19 597–19 619, 2017.

[51] S. Nagendra, N. Podila, R. Ugarakhod, and K. George, “Com-
parison of reinforcement learning algorithms applied to the cart-
pole problem,” in 2017 International Conference on Advances in
Computing, Communications and Informatics (ICACCI).
IEEE, 2017,
pp. 26–32.

[52] C. Aguilar-Ib´a ˜nez, J. Mendoza-Mendoza, and J. D´avila, “Stabiliza-
tion of the cart pole system: by sliding mode control,” Nonlinear
Dynamics, vol. 78, no. 4, pp. 2769–2777, 2014.

[53] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore,
P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Rad-
ford, J. Schulman, S. Sidor, and Y. Wu, “Stable baselines,” https:
//github.com/hill-a/stable-baselines, 2018.

[54] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforce-
ment learning,” arXiv preprint arXiv:1312.5602, 2013.

[55] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and
N. de Freitas, “Dueling network architectures for deep reinforce-
ment learning,” 2016.

[56] A. Marco, F. Berkenkamp, P. Hennig, A. P. Schoellig, A. Krause,
S. Schaal, and S. Trimpe, “Virtual vs. real: Trading off simulations
and physical experiments in reinforcement learning with bayesian
optimization,” in 2017 IEEE International Conference on Robotics and
Automation (ICRA).

IEEE, 2017, pp. 1557–1563.

[57] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238–1274, 2013.

[34] G. Dulac-Arnold, D. Mankowitz, and T. Hester, “Challenges of

[58] P. E. McKnight and J. Najab, “Mann-whitney u test,” The Corsini

real-world reinforcement learning,” 2019.

encyclopedia of psychology, pp. 1–1, 2010.

forcement learning for autonomous defence in software-deﬁned
networking,” ArXiv, vol. abs/1808.05770, 2018.

17

[59] V. Behzadan and W. H. Hsu, “Adversarial exploitation of policy

imitation,” ArXiv, vol. abs/1906.01121, 2019.

[60] V. Behzadan and W. Hsu, “Sequential triggers for watermarking of
deep reinforcement learning policies,” ArXiv, vol. abs/1906.01126,
2019.

[61] K. Chen, T. Zhang, X. Xie, and Y. Liu, “Stealing deep reinforcement
learning models for fun and proﬁt,” CoRR, vol. abs/2006.05032,
2020. [Online]. Available: https://arxiv.org/abs/2006.05032
[62] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
L. Li, Y. Liu et al., “Deepgauge: Multi-granularity testing criteria
for deep learning systems,” in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, 2018, pp.
120–131.

[63] R. S. Sutton, “Andrew g barto. reinforcement learning: An intro-

duction,” MIT press, 2018.

[64] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard,
“Universal adversarial perturbations,” CoRR, vol. abs/1610.08401,
2016. [Online]. Available: http://arxiv.org/abs/1610.08401
[65] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel,

“Trust region policy optimization,” 2017.

[66] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” 2016.

[67] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The
arcade learning environment: An evaluation platform for general
agents,” Journal of Artiﬁcial Intelligence Research, vol. 47, p. 253–279,
Jun 2013. [Online]. Available: http://dx.doi.org/10.1613/jair.3912
[68] A. Marot, I. Guyon, B. Donnot, G. Dulac-Arnold, P. Panci-
atici, M. Awad, A. O’Sullivan, A. Kelly, and Z. Hampel-Arias,
“L2rpn: Learning to run a power network in a sustainable world
neurips2020 challenge design,” 2020.

[69] L. Omnes, A. Marot, and B. Donnot, “Adversarial training for a
continuous robustness control problem in power systems,” in 2021
IEEE Madrid PowerTech.

IEEE, 2021, pp. 1–6.

[70] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary,
“Robust deep reinforcement learning with adversarial attacks,” in
Proceedings of the 17th International Conference on Autonomous Agents
and MultiAgent Systems, 2018, pp. 2040–2042.

[71] J. Kos and D. Song, “Delving into adversarial attacks on
on Learning
ICLR 2017, Toulon, France, April 24-26, 2017,
[Online].

deep policies,” in 5th International Conference
Representations,
Workshop Track Proceedings. OpenReview.net, 2017.
Available: https://openreview.net/forum?id=BJcib5mFe

[72] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary,
“Robust deep reinforcement learning with adversarial attacks,”
in 17th International Conference on Autonomous Agents and Multi-
agent Systems, AAMAS 2018, ser. Proceedings of the International
Joint Conference on Autonomous Agents and Multiagent Systems,
International Foundation for Autonomous Agents and
AAMAS.
Multiagent Systems (IFAAMAS), 2018, pp. 2040–2042, publisher
Copyright: © 2018 International Foundation for Autonomous
Agents and Multiagent Systems (www.ifaamas.org). All rights
reserved.; 17th International Conference on Autonomous Agents
and Multiagent Systems, AAMAS 2018 ; Conference date: 10-07-
2018 Through 15-07-2018.

[73] X. Y. Lee, Y. Esfandiari, K. L. Tan, and S. Sarkar, “Query-based
targeted action-space adversarial policies on deep reinforcement
learning agents,” in Proceedings of the ACM/IEEE 12th International
Conference on Cyber-Physical Systems, ser. ICCPS ’21. New York,
NY, USA: Association for Computing Machinery, 2021, p. 87–97.
[Online]. Available: https://doi.org/10.1145/3450267.3450537
[74] E. Vinitsky, Y. Du, K. Parvate, K. Jang, P. Abbeel, and A. M. Bayen,
“Robust reinforcement learning using adversarial populations,”
CoRR, vol. abs/2008.01825, 2020.
[Online]. Available: https:
//arxiv.org/abs/2008.01825

[75] K. L. Tan, Y. Esfandiari, X. Y. Lee, Aakanksha, and S. Sarkar,
“Robustifying reinforcement learning agents via action space
adversarial training,” CoRR, vol. abs/2007.07176, 2020. [Online].
Available: https://arxiv.org/abs/2007.07176

[76] V. Behzadan and A. Munir, “Mitigation of policy manipulation
attacks on deep q-networks with parameter-space noise,” ArXiv,
vol. abs/1806.02190, 2018.

[77] V. Behzadan and M. Arslan, “Whatever does not kill deep
stronger,” arXiv preprint

learning, makes

it

reinforcement
arXiv:1712.09344, 2017.

[78] Y. Han, B. I. P. Rubinstein, T. Abraham, T. Alpcan, O. Y. de Vel,
S. M. Erfani, D. Hubczenko, C. Leckie, and P. Montague, “Rein-

