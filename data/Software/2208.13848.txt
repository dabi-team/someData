ProspectNet: Weighted Conditional Attention for
Future Interaction Modeling in Behavior Prediction

Yutian Pang1†, Zehua Guo2, Binnan Zhuang2

2
2
0
2

g
u
A
9
2

]
I

A
.
s
c
[

1
v
8
4
8
3
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—Behavior prediction plays an important role in
integrated autonomous driving software solutions. In behavior
interactive behavior prediction is a less-
prediction research,
explored area, compared to single-agent behavior prediction.
Predicting the motion of interactive agents requires initiating
novel mechanisms to capture the joint behaviors of the interactive
pairs. In this work, we formulate the end-to-end joint prediction
problem as a sequential learning process of marginal learning
and joint learning of vehicle behaviors. We propose ProspectNet,
a joint learning block that adopts the weighted attention score
to model the mutual inﬂuence between interactive agent pairs.
The joint learning block ﬁrst weighs the multi-modal predicted
candidate trajectories, then updates the ego-agent’s embedding
via cross attention. Furthermore, we broadcast the individual
future predictions for each interactive agent into a pair-wise
scoring module to select the top K prediction pairs. We show that
ProspectNet outperforms the Cartesian product of two marginal
predictions, and achieves comparable performance on the Waymo
Interactive Motion Prediction benchmarks.

Index Terms—Interactive Motion Prediction, Autonomous

Driving, Trajectory Prediction

I. INTRODUCTION

Along with perception and planning, behavior prediction
forms an integrated end-to-end autonomous driving system
(ADS). In ADS, behavior prediction infers the future locations
of nearby dynamic objects identiﬁed by perception systems,
and the maneuver of the ego vehicle is further scheduled by
planning systems. The task of behavior prediction is chal-
lenging due to complex operation scenarios such as vehicle-
to-vehicle, vehicle-to-human, and vehicle-to-road interactions
under the various right-of-ways and driver intentions.

In behavior prediction, researchers are focusing on several
types of problems, as shown in Fig. 1. We name the model
for predicting the trajectory of a single agent without consid-
ering interacting agents a single marginal model. In motion
prediction literature, multimodality has become the standard
practice to capture uncertainties into the future [1]–[4], where
the marginal predictor outputs a set of trajectories (modes) to
represent different driver behavior intentions. Each trajectory
sequence is associated with a corresponding probability (mode
probability) as the prediction conﬁdence score. To overcome
the issue of mode collapse, post-hoc processing methods
such as non-maximum suppression [5] ﬁltering are proposed
to reject near-duplicated trajectories. In this work, we are

†Work done during an internship at Xsense.ai Inc.
1School for Engineering of Matter, Transport & Energy, Arizona State Uni-
versity. 975 S. Myrtle Ave, Tempe, AZ 85281. Yutian.Pang@asu.edu
2Behavior Prediction Team, Xsense.ai Inc. 15070 Avenue of Science #100,
San Diego, CA 92128. {zehuag1, binnanz}@xiaopeng.com

Fig. 1: A demonstration of different behavior prediction prob-
lems in a merging scenario. In single marginal prediction, the
model predicts one future trajectory sequence for each agent
without considering interactions. In multi-modal marginal pre-
diction, the model gives multiple future predictions for each
agent without considering the interactions. In multi-modal
joint prediction, the model considers the collisions and selects
the best potential routes. In multimodality settings, ⊕ marks
the mode with the highest probability.

particularly interested in the highly-interactive scenario of the
prediction systems of Autonomous Vehicles (AVs), and this
leads to the joint prediction of interactive vehicles, where
multiple realizations of self-consistent future trajectories are
generated. In our multi-modal joint prediction, the predictions
of one agent are conditionally inferred [6] on other agents’
predictions and vice versa. Therefore, our joint model learns
to generate predictions in a sense that are aware of both past
and future interactions.

Many recent publications adopt

the graph-based feature
extraction for interaction representation [5], [7], [8]. VectorNet
[8] encodes high-precision map polylines as graph features and
represents interactions via lower-order self-interactions and
higher-order agent graph interactions. Target-driven trajectory
prediction network [5] integrates VectorNet’s interaction repre-
sentation idea and further extends to multi-modal sense by pre-
sampling target points as anchors of predictions. HEAT adopts
edge-enhanced graph attention to model the interaction [7].
M2I [1] proposes a sequential procedure for interactive mo-
tion prediction, which includes a classiﬁcation of inﬂuencer-
reactor relations, and then a conditional predictor to generate
predictions for the reactor.

In this work, we propose an end-to-end trajectory prediction
framework for interactive motion prediction, build upon the

 
 
 
 
 
 
ﬁne-tuned marginal motion prediction model. Similar to the
marginal model, the proposed model takes embedded inputs
from both dynamic and static–moving objects histories (cars,
pedestrians, cyclists), roadmaps, etc. To bridge the gap be-
tween a marginal motion prediction and an interactive motion
prediction, we propose a novel weighted attention mechanism
for condition prediction of interactive agent trajectories and
named it ProspectNet. One of our innovation is the introduc-
tion of a weight bias term to the compatibility function of
simple attention mechanism. The weight bias term represents
the conditional importance of predicted trajectory candidates
from the interactive agent, so the joint encoding module can
learn the probability distribution of the marginal prediction
as a priori. A uniform probability importance score for each
lane segment is also introduced for normalization. Another
important building block of the end-to-end joint model is the
pair-wise scoring module to post-hoc rank the paired trajectory
candidates.

Our work explicitly models the interaction between inter-
acting agents, by a specially designed attention mechanism,
with a pair-wise trajectory candidates selection function for
candidate selection. We examine the effectiveness of the pro-
posed framework by comparing it with a ﬁne-tuned marginal
prediction model (Cartesian product between topK candidates
of two agents). The usage of joint learning block is ﬂexible
and can be extended to stacked interactions and multi-agents.
Different from existing literature, we do not have a heuristic-
based pre-labeling procedure for inﬂuencer/reactor with dif-
ferent types of relations. Moreover, we only use the vectorized
features in our implementation. To be more clear, we highlight
our contributions here: (a) We propose a joint learning block
where the weighted conditional attention mechanism is the
core component. The joint learning block is introduced to the
prediction embedding of the marginal model, as a sequential
process. (b) We propose a pair-wise scoring module to select
broadcasted topK candidate pairs from top N 2 pairs. (c) We
largely improves the prediction
show the marginal model
capability in various scenarios through visualizations.

II. LITERATURE REVIEW

Trajectory prediction is a popular research topic in various
engineering application domains including cars, pedestrians
[9], ships [10], aircraft [11], etc. In this section, we will
discuss the extensive motion prediction research for ground-
based autonomous systems. We will start from common prac-
tices in motion prediction Section II-A, then extend to agent
interaction modeling for conditional behavior prediction in
Section II-B.

A. Motion Prediction in Behavior Prediction

Context Encoding acts as the initial step to extract features
from raw input data, with deep neural networks (e.g., CNN
[12], [13], RNNs [7], GNNs [8], [14]) for unsupervised feature
extraction. The context encoding methods can be divided into
two categories, rasterized methods and vectorized methods.
Rasterized encoding treats the HD map and agent history

tracks as images and adopts a CNN-based encoder to construct
inputs to trajectory predictor/decoder. A typical practice is
to represent timestamps as image kernels. Multipath and its
variant [15], [16] use CNNs-based feature encoder for raster
images. IntentNet [13] develops CNN-based feature encoder
for both raster images and LiDAR points. DRF-Net [17] raster-
izes map elements and augments agent encoding as inputs to
the recursive discrete residual ﬂow module. In contrast, sparse
encoding (vectorized) methods treat each entity as a struc-
tured element set and adopt graph-based algorithm to extract
features and learn element interactions. Vectorized methods
develop rapidly with beneﬁts from reduced training difﬁculty
of fewer model parameters, and explicit scenario encoding for
improved explainability in structured feature representations.
VectorNet [8] is the pioneering research in this direction.
It explicitly encodes the sparse element representations of
agents/maps as graph features and adopts lower and higher-
order interaction learning modules for modeling agent-to-agent
and agent-to-map relations. TNT [5] and DenseTNT [18]
further extend this concept by using sparse/dense anchor/goal-
based trajectory predictors and considering multi-modality of
driving intent. LaneGCN and LaneRCNN [14], [19] focus on
modeling lane graph interactions and graph-to-graph interac-
tions. Additionally, M2I [1] proposes to combine rasterized
encoding and vectorized encoding for better feature represen-
tation and achieve state-of-the-art.

Target-Driven Trajectory Prediction TNT introduces pre-
sampled anchors from map segments and predicts trajectories
based on anchors with probabilities. TNT demonstrates the
efﬁciency of using the non-maximum suppression (NMS) [5]
threshold to reject near-duplicate trajectories, such that diverse
intents of the driver can be ﬁltered from trajectory candidates.
However, the sparse anchors in TNT fail to perfectly estimate
the probability of roads. DenseTNT [18] improves the sparse
goal-sampling procedure with dense goal probability estima-
tion on the map. In DenseTNT, the dense goals are selected
along the candidate road lanes that are classiﬁed based on the
distance between goals and lanes. In the goal training stage, the
goal probability is predicted through attention. The predicted
trajectories will be based on the topK selected goal candidates.
HOME [20] uses a similar goal-sampling idea but uses CNN
to generate the probability heatmap. Generally, the goal-based
idea has been explored in different settings toward various
objectives [14], [21].

Multi-Modality is an approach to provoke uncertainty in
behavior prediction. Multi-modal prediction aims to gener-
ate multiple plausible trajectories for the objective. Multi-
modal prediction is designed to handle uncertainty in motion
prediction. The uncertainty comes from diverse behaviors of
drivers, and dynamic trafﬁc scenarios. In practice, multi-modal
prediction is achieved by either randomized latent variables
[22], or directly sampled from probability distributions (e.g.,
GMM [6], [23]) to get diverse predictions. These methods are
sensitive to the model’s pre-deﬁned parameters/distributions.
Alternatively, anchor/goal-based approaches make predictions
based on the proposals and reduce computational difﬁculties

by narrowing down the solution search space. We follow the
second approach in this work.

B. Interaction Modeling in Conditional Behavior Prediction

Predicting trajectories considering interactions between two
or more agents remains an open question in the topic of
prediction. Existing methods can be divided into classical
methods and deep learning based methods. Classical methods
(e.g., Social Force models, Geometry-based methods) require
hand-crafted features to capture multi-agent behaviors. They
are less data-intensive with increased interpretability. Social
Force models (guided by virtual repulsive and attractive forces)
are built upon the assumption that pedestrians are mission-
driven for destination navigation and collision avoidance [24].
However, Social Force models perform poorly on pedes-
trian prediction tasks [25]. Geometry-based models adopt
optimization-based interactive prediction with the geometry
of each agent [26], [27]. Classical methods require extensive
feature engineering and are hard to generalize in different
scenes [28]. Deep learning based methods show satisfactory
performance compared with classical methods. Behavior CNN
captures crowd behaviors using CNN [29]. Social pooling
merges the latent space between nearby agents, which leads to
a socially-aware prediction. GNNs, self-attention, and Trans-
former mechanisms are frequently introduced to leverage agent
interactions [2], [22], [30]–[32]. Conditional predictions output
future agent trajectories by conditioning on the nearby agent’s
planned or future trajectories.

In this work, we propose ProspectNet, a novel attention
mechanism to explicitly represent conditional inference, in
multi-modality fashion. ProspectNet adopts attention to model
but interactions between interactive agent pairs. The prediction
of one agent is conditioned on all trajectory candidates of
another agent, with adjusted attention score bias to avoid
restricted search space and to represent candidate conﬁdence.
Different from the conditional trajectory predictor in M2I,
our proposed model doesn’t have a pre-labeled inﬂuencer
and reactor, yet the joint learning block is newly proposed
sequentially aligned after the marginal trajectory predictor.

III. METHODOLOGY

In this section, we discuss the problem setup and an
overview of ProspectNet for the interactive motion prediction
case. As shown in Fig. 2, our proposed framework contains
a marginal learning block, a joint learning block, and a pair-
wise scoring module. We adopt a similar structure to TNT [5]
as the baseline marginal learning block. For the simplicity of
illustration, we focus on the case of two interactive agents,
numbered 1 and 2. Firstly, each agent’s map and trajectory
encoding will be passed into the marginal learning block sep-
arately. The marginal learning block will output each agent’s
top N trajectory candidates, future trajectory embedding, and
map embedding. Then, the joint learning block takes agent A’s
future trajectory embedding, agent B’s prediction candidates,
agent A’s map embedding, and output updated agent A’s future
embedding. The embedding is further used to sample trajectory

candidates from the predictor of marginal
learning block.
Lastly, the top N trajectory candidates for each agent are
combined into N 2 pairs to be selected by the pair-wise scoring
module. The ﬁnal output dimension will be R2×K×T ×2, where
K is the number of modes, T is prediction timestamps, and the
last dimension is the coordinate dimension (2 in this work).

A. Problem Formulation

The objective of interactive motion prediction is to predict
two agents (A&B) future T timestamps trajectories col-
laboratively. Researchers have proposed several methods to
determine if the two agents are considered interactive. For
instance, CBP [6] proposes to use mutual
information to
quantify the dependence between two time series. Several
motion prediction datasets explicitly provide the interactive
label for agents in a scene [33], [34]. In this work, we primarily
focus on the interactive bahavior prediction problem for the
Waymo Motion Prediction Dataset. For a given scenario, the
future trajectory sequences for agent A are SA = {si
A ∈
RT ×2 : i ∈ {1, 2, ..., K}} for top K candidates, or i ∈
{1, 2, ..., N } for top N candidates before candidate selection.
We use xA = {mA, cA} to represent context encoding, which
consists of the map embedding mA ∈ RH×E and agent
embedding cA ∈ RT ×E, where H is the number of features,
and E is the embedding dimension. We use pA ∈ RK×1 to
denote the associate probability to each candidate of SA.

Algorithm 1 Joint Learning Process
Input: Agent embedding hA, hB; Context encoding each
coordinate xA, xB; Predicted trajectories SA, SB and
associated probabilities pA, pA.

Objective: Updated agent encoding ˆhA, ˆhB
1: for q in 0 to Q do
2:

Coordinate alignment: ˆsB ← sB
GRU embedding: ˜sB ← GRU( ˆsB)
3:
4: Q, K, V ← hA, ˜sB ⊕ xA, ˜sB ⊕ xA
5:

Uniﬁed standard probability score for context encoding
xA: pXA = (1/xA.dim[0]).repeat(xA.dim[−1])

6: Kb ← pB.repeat( ˜sB.dim[−1]) ⊕ pXA
ˆhA = WeightedAttention(Q, K, V, Kb)
7:
// Joint Learning for Agent A Above
Coordinate alignment: ˆsA ← sA
8:
GRU embedding: ˜sA ← GRU ( ˆsA)
9:
10: Q, K, V ← hB, ˜sA ⊕ xB, ˜sA ⊕ xB
11:

Uniﬁed standard probability score for context encoding
xB: pXB = (1/xB.dim[0]).repeat(xB.dim[−1])

12: Kb ← pA.repeat( ˜sA.dim[−1]) ⊕ pXB
13:

ˆhB = WeightedAttention(Q, K, V, Kb)
// Joint Learning for Agent B Above
if Q > 1 then

hA, hB ← ˆhA, ˆhB
(SA, pA), (SB, pB) ← Marginal(hA), Marginal(hB)

14:
15:

16:
end if
17:
18: end for
19: return ˆhA, ˆhB

Fig. 2: Overview of the ProspectNet: In (a), the interactive pair predictions from the marginal prediction model. First, The
marginal model parameter is ﬁne-tuned. Then, the topK candidates from topN candidates of each agent will be scored. Finally,
the topK pair candidate trajectories will be selected from the Cartesian product of K 2 paired candidates. In (b), the joint
learning block updates each agent’s marginal encoding with a weighted attention mechanism. The updated agent encoding is
further used to sample N candidates of each agent. The pair-wise scoring block selects topK pair candidate trajectories.

A = p(si
si

A|SB =

N
(cid:88)

i=0

si
B, xA),

si
A ∈ SA

(1)

Similarly, we apply the same notations to agent B and deﬁne

the conditional inference of agent B as in Equation (2).

B = p(si
si

B|SA =

N
(cid:88)

i=0

si
A, xB),

si
B ∈ SB.

(2)

It is worth noting the conditional prediction is conditioned
on all N sampled trajectory candidates, with the pair-wise
scoring module applied afterwards. Algorithm 1 listed the
pseudo-code for he joint learning process. The joint learning
module updates the agent embedding hA, hB with weight
adjusted cross-attention mechanism, and we name it weighted
attention. Q is the parameter determining the number of
stacked weighted attention to enhance interactive learning.

WeightedAttention =

softmax(QT K + QT Kb)
√
dk

V

(3)

In practice, the trajectory predictor makes predictions by
sampling from the learned latent spaces hA, hB ∈ RH×E.
The hidden spaces are heatmaps [18] to represent the possible
lanes the agent is following through an attention mechanism.
The goal samples are further inferred through a probability
estimation module. Finally, The predictor regresses the trajec-
tory sequences corresponding to each selected goal. For agent
A, the conditional inference is achieved by updating hA with
predicted trajectory candidates of agent B. That is, we are
looking for p( ˆhA) in Equation (4),

p( ˆhA) = p(hA| ˜sB, xA)

(4)

Fig. 3: Joint Learning Block (demonstration with agent A
only): We propose relative attention to consider the attention
score. The query (Q) matrix is the agent encoding. The key (K)
and value (V) metrics are agents A map encoding and aligned
agent B topN predictions. The weight for attention score is the
uniﬁed agent A map encoding probability and agent B topN
candidates probability.

B. Conditional Prediction

In this work, we use SA and SB to represent the set of
multi-modal predictions for each agent, and sA and sB to
represent an entry in the set. The problem of interactive mo-
tion prediction can be represented with conditional inference
problem, as in Equation (1).

where ˜sB is the trajectories sB ∈ SB shifted to agent
A’s coordinate. We inference p( ˆhA), p( ˆhB) through weighted
attention as in Equation (3). In Equation (3), Q is the agent’s
embedding, K and V are aligned interactive agent’s prediction
embedding concatenated with feature encoding. Kb is the
mode probability score for each agent prediction concatenated
with the normalized feature encoding score. Details are listed
in Algorithm 1.

C. Pair-Wise Scoring

The pair-wise scoring module is to select topK trajectory
candidate pairs from sampled trajectories. Figure 4 shows the
structure of the pair-wise scoring module. The broadcasted
topN 2 trajectory pairs (sA, sB) are concatenated with agent
encoding (ˆhA, ˆhB) as the input to the two-layer MLPs (ψ). In
trajectory scoring, we estimate the difference between ground
truth and predictions for the entire sequence. We use the
maximum entropy model to score topK trajectory pairs, similar
to VectorNet but with extended dimensions. For the maximum
entropy model, the loss is deﬁned as the cross-entropy loss
(ΘCE) between the predicted scores and the ground truth
scores (Equation (5)).

Fig. 4: Pair-Wise Scoring: A simple two-layer MLP is adopted
to output the pair-wise score. The MLP takes two inputs, the
broadcasted topN 2 trajectory pairs, and each agent’s encoding.

Lscore := ΘCE(ψ(sA, ˆhA, sB, ˆhB), D(sA, sB))

(5)

Different to single motion prediction in VectorNet, we used

averaged l∞ norm for trajectory pair in D.

D(sA, sB) =

exp(−(l∞(s, sGT))/2α)
s(cid:48) exp((−l∞(s(cid:48), sGT))/2α)

(cid:80)

(6)

where l∞(s, sGT) is the averaged inﬁnity norm of two

To determine the ﬁnal topK trajectory candidate pairs after
obtaining the trajectory score, we use the duplicate trajectory
rejection mechanism. We ﬁrst sort the trajectory candidate
pairs based on their score, then ﬁnd the next available can-
didate pair satisfying the separation threshold. We decay the
threshold by a factor and repeat this process if there are no K
pairs satisfying the threshold.

IV. EXPERIMENTS

In this section, we introduce the procedure we performed
to process the Waymo Motion Prediction Dataset for inter-
active prediction training as the feature engineering study of
this research Section IV-B. Finally, we show our interactive
prediction results Section IV-D.

A. Dataset

We use Waymo Motion Prediction Data [34] from the
Waymo Open Motion Dataset (WOMD) to demonstrate the
effectiveness of ProspectNet. WOMD is by far the largest pub-
lic dataset intended for motion prediction research. It contains
over 570 hours of driving data and spans over 1, 750km of
roadways. The essential problem in WOMD is to predict the
future 8 seconds positions given agents’ 1-second histories
sampled at 10Hz. WOMD provides test datasets for both
single and interactive predictions and online data challenges
for comparisons with state-of-the-art. However, the training
set is kept the same for different prediction problems, with up
to 8 prediction labels in each scene. Thus, the procedures for
WOMD raw data processing and the determination of inter-
active pairs are the two fundamental problems for interactive
behavior prediction.

B. Feature Engineering

Feature engineering is fundamental for deep learning prac-
tices. Good quality of data guarantees the success of the
data-driven model, while a larger amount of data helps with
generalizability. In this work, we perform feature processing
from two aspects, (a) The quality of target samples. (b) The
selection of interacting agents in the training set of Waymo
Motion Prediction Dataset.
Interaction Pairs In the WOMD training/validation set, the
interactive pairs are not explicitly given. Thus we need to
search at most C2
8 agent pairs for interactions. We use a
heuristic-based method to determine interaction pairs in the
WOMD training set. For any two predictable agents A and B
A and sGT
in a scene, their ground truth trajectories are sGT
A .
We ﬁnd the closest distance in the future locations of sGT
A
and sGT
A . If the closest l2 distance in future timestamps is
below the threshold (e.g., 5 meters in our case), we save
the pair feature as the training set for interactive modeling
(Equation (8)).

dmin = minT

t=10

l2(sGT

A , sGT

B ) < 5

(8)

interactive agents,

l∞(s, sGT) =

1
2

(l∞(sA, sGT

A ) + l∞(sB, sGT

B ))

Target Samples are the sampled destination points in target-
driven predictors. Target driven method helps with enhanc-
ing multi-modality by performing regression conditioned on

(7)

targets. Conversely, the training regressor suffers from mode
averaging. In this work, targets are the sampled locations
of (x, y) an agent probably will reach at the last timestamp
of T . As discussed in Section III-B, the trajectory predictor
predicts a distribution of targets as location choices in the ﬁnal
timestamp. Under the assumption that the vehicles will not
deviate far away from targets, we uniformly sample points
based on the map centerlines. If we denote the offsets to
centerlines as δx and δy. Then the sampled targets are Γ ∈
RN = {τ κ} = {(xκ, yκ) + (δxκ, δyκ) : κ ∈ {1, 2, ..., N }}. In
practice, we have several parameters controlling the sampling
procedure: (a) The number of targets we want to sample. (b)
The range of target samples (after coordinate alignment). (c)
The range radius for searchable lanes. (d) The range radius
for objects. For evaluation of target quality, we use the best
mode displacement (BMD) on WOMD validation set, which
is the minimum offsets of the sample targets. We can refer to
Appendix. A. for detailed experiment records. As in Figure 5,
we select a parameter set with lower BMD for both interactive
agents.

Fig. 5: Boxplot of BMDs for 14 parameter sets in targets
generation. Y-axis in meters.

C. Evaluation Metrics

TABLE I: Marginal Model v.s. Joint Model on the validation
set

Marginal
Joint

minADE minFDE MissRate OverlapRate mAP
0.045
0.816
0.115
0.826

12.596
8.118

4.499
3.012

0.378
0.416

aThe metrics are evaluated on the validation set.
bThe validation set only considers vehicles.

D. Results

As shown in Figure 2(b), the joint learning model learns
the interactions by updating the agent’s embedding h through
weighted attention, and we name it ProspectNet. The updated
agent embedding is further fed into the trajectory predictor to
get paired predictions. To demonstrate the improved capability
of our proposed method, we compare it with the marginal pre-
diction baseline model in Figure 2(a). We adapt the marginal
model for two agents by calculating the Cartesian Product of
mode probabilities to get the naive joint mode probability, and
we get the topK predicted trajectories for interactive pairs. We
compare the performance of Figure 2(a) and Figure 2(b) and
list the results in Table I. For this comparison, we ﬁrst ﬁne-tune
the marginal model towards single motion prediction state-of-
the-art, then use the same model parameters to train the joint
model for a fair comparison.

TABLE II: Comparison with other methods.

minADE minFDE MissRate OverlapRate mAP

Scene
Transformer
M2I
ProspectNet

Scene
Transformer
M2I
HeatRM4
Waymo
LSTM
Baseline

1.72

-
3.01

1.76

2.50
2.93

4.52

Validation Set

3.99

5.49
8.12

Test Set

4.08

5.65
7.20

12.40

0.49

0.55
0.83

0.50

0.57
0.80

0.87

-

-
-

0.28

0.39
0.46

0.56

0.11

0.18
0.12

0.10

0.16
0.07

0.01

aThe results here only consider vehicles.
bThe test set results are obtained from motion challenge leaderboard.

Similar to WOMD, we use the pair-wise evaluation metrics
for multi-modal motion prediction. minADE and minFDE
compute the minimum average/ﬁnal displacement error of the
best prediction mode. It is worth pointing out that the best
mode in minADE is not necessary the best mode in minFDE,
and vice versa. OverlapRate calculate the possible overlap
between the predicting agent and other agents in the best
mode by considering the Intersection over Union (IoU) for
each agent with shapes. MissRate computes the possibility
of neither mode making a correct prediction within a range of
the ground truth, with speed adjustment to the range threshold.
Mean average precision (mAP) is a general measure of the
model performance by calculating the area under the precision-
recall curve, where MissRate is used to deﬁne true positives.

We present the statistical comparison of ProspectNet with
other SOTA models on interactive motion prediction in
Table II. We show that ProspectNet achieves comparable
performance against other SOTA benchmarks. Additionally,
ProspectNet appears to have superior prediction capability,
even surpassing SceneTransformer, in terms of mAP. A large
portion of the beneﬁts come from the conditional inference
process. Additionally, we visualize several scenes as corner
cases to demonstrate the interaction-aware prediction in Fig-
ure 6. We show that the joint learning process can improve
high interaction predictions, especially in yielding/following
scenarios. In some scenarios, the joint model may give over-
conservative predictions to reduce potential conﬂict. These
types of behaviors may not always match the ground truth

(a) Marginal Model Prediction for Scenario ID: 360b21f165487005

(b) Joint Model Prediction for Scenario ID: 360b21f165487005

(c) Marginal Model Prediction for Scenario ID: 8dd03cf09f4ae271

(d) Joint Model Prediction for Scenario ID: 8dd03cf09f4ae271

(e) Marginal Model Prediction for Scenario ID: b1aece8720878be2

(f) Joint Model Prediction for Scenario ID: b1aece8720878be2

Fig. 6: Visualization of corner cases. Left: marginal learning predictions. Right: joint learning predictions. Ground truths are in dashed lines.
The multi-modal predictions of agents are marked in green and violet. Other nearby vehicles are in gold. The ﬁnal timestamp of the best
mode for both agents is marked with (cid:63), with a bounding box representing vehicle shapes. In (a)&(b), the right turn agent’s joint prediction
is yielding to the straight trafﬁc, matching the ground truth. (c)&(d) is the following scenario, the joint prediction of the following vehicle
is keeping a safe distance to the leading vehicle, matching the ground truth. In (e)&(f), the joint model forces agent A to have a right turn
intent to avoid high interactions.

but are reasonable, as in Figure 6(e)&(f). More corner cases
are included in Appendix B.

V. CONCLUSIONS

In summary, we propose an interactive motion prediction
model, ProspectNet. For interaction-aware prediction, the re-
alization of conditional inference is crucial. In ProspectNet, we
perform conditional inference within the joint learning block,
where we propose a novel weighted attention mechanism to
update the agent’s trajectory latent spaces from the marginal
learning model. The updated agent representation for both
agents is aware of all the topN predicted future positions

of the interacting agent. Consequently, ProspectNet outputs
interaction-aware of the sampled trajectories. We show that
our proposed model achieves performance improvement by
comparing it with a single motion prediction model through
experiments on the Waymo Motion Prediction Dataset. Ad-
ditionally, we show that ProspectNet achieves comparable
performance to the state-of-the-art. Furthermore, we visualize
the predictions for several scenarios in the test set. These
visualizations demonstrate the effectiveness of our proposed
learning framework.

Insights

ACKNOWLEDGMENT

Our method has limitations and can be further improved in

The authors thank the support from Xsense.ai Inc.

many aspects.

• We are expecting a performance improvement by ﬁne-
tuning the parameters of the interactive model. Currently,
we are using the same model parameters for both the
marginal model and joint model to show the effectiveness
of the proposed joint learning block. However, to achieve
a better performance, we can adjust
the embedding
dimensions, the number of stacked weighted attention
modules, the learning schedulers, etc. By performing an
extensive parametric study, the joint model performance
can be improved.

• During the feature processing, we are using the heuristic-
based method to select an interacting agent within the
training dataset. The heuristic largely impacted the quality
of the training set of the model, while the training set
is the most critical factor towards a good model. We
propose to try different feature selection metrics for the
preparation of the training data, for instance, mutual
information or degree of inﬂuence [6], [34].

• There are potential improvements in the pair-wise scoring
module. For instance, we can perform a parametric study
on the selection of norms on different timestamps. We can
alternate the l∞ norm to the Euclidean norm, or l0 norm,
and adjust the timestamps to calculate the difference.
• The trajectory selection function of non-maximum sup-
pression can be further investigated. At this point, we
can apply different speed penalized selection thresholds
on the driving direction and its tangent direction to reﬂect
the impact of the vehicle speed proﬁle.

• Due to the time limits, we haven’t looked into alternative
trajectory predictors to study the generalizability of the
proposed joint learning framework. Similar to this work,
ProspectNet can be applied to the latent space for any
marginal predictor.

The proposed end-to-end ProspectNet model can be ex-
tended to multi-agents interactive prediction. The joint learn-
ing block in Figure 2 can be generalized to conditional
encoding of multiple agents by extending the attention layer
dimension in Algorithm 1. In Algorithm 1,
the key-value
tensor pairs will be expand to include the predicted trajectories
of all interacting agents. For three interacting agents (A, B, C),
during updating the encoding of agent, K, V ← ˜sB ⊕ ˜sC ⊕xA.
Similarly, the biased key tensor pairs need to concatenate
predicted trajectory probabilities to match the dimension in
Equation (3). Also, the scoring function can be straightly ad-
justed to multi-agents candidates selection function. However,
the proposed joint scoring function has exponential complexity
increase in the broadcast phase of Figure 3. The experiments
of multiple interacting agents are beyond the scope of this
paper, and the WOMD behavior prediction dataset also limit
the problem setup to interactive agent pairs. We leave them as
the major studies to investigate in the future.

REFERENCES

[1] Q. Sun, X. Huang, J. Gu, B. C. Williams, and H. Zhao, “M2i: From
factored marginal trajectory prediction to interactive prediction,” arXiv
preprint arXiv:2202.11884, 2022.

[2] J. Ngiam, B. Caine, V. Vasudevan, Z. Zhang, H.-T. L. Chiang, J. Ling,
R. Roelofs, A. Bewley, C. Liu, A. Venugopal et al., “Scene transformer:
A uniﬁed multi-task model for behavior prediction and planning,” arXiv
e-prints, pp. arXiv–2106, 2021.

[3] Y. Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, “Multimodal motion
prediction with stacked transformers,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021, pp.
7577–7586.

[4] S. Casas, C. Gulino, S. Suo, and R. Urtasun, “The importance of
prior knowledge in precise multimodal prediction,” in 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2020, pp. 2295–2302.

[5] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y. Shen,
Y. Shen, Y. Chai, C. Schmid et al., “Tnt: Target-driven trajectory
prediction,” arXiv preprint arXiv:2008.08294, 2020.

[6] E. Tolstaya, R. Mahjourian, C. Downey, B. Vadarajan, B. Sapp, and
D. Anguelov, “Identifying driver interactions via conditional behavior
prediction,” in 2021 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2021, pp. 3473–3479.

[7] X. Mo, Z. Huang, and C. Lv, “Multi-modal interactive agent trajectory
prediction using heterogeneous edge-enhanced graph attention network,”
in Workshop on Autonomous Driving, CVPR, vol. 6, 2021, p. 7.

[8] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid,
“Vectornet: Encoding hd maps and agent dynamics from vectorized rep-
resentation,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 11 525–11 533.

[9] C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, “Spatio-temporal graph
transformer networks for pedestrian trajectory prediction,” in European
Conference on Computer Vision. Springer, 2020, pp. 507–523.
[10] L. P. Perera, P. Oliveira, and C. G. Soares, “Maritime trafﬁc monitoring
based on vessel detection,
tracking, state estimation, and trajectory
prediction,” IEEE Transactions on Intelligent Transportation Systems,
vol. 13, no. 3, pp. 1188–1200, 2012.

[11] Y. Pang, X. Zhao, J. Hu, H. Yan, and Y. Liu, “Bayesian spatio-
temporal graph transformer network (b-star) for multi-aircraft trajectory
prediction,” Knowledge-Based Systems, p. 108998, 2022.

[12] H. Cui, V. Radosavljevic, F.-C. Chou, T.-H. Lin, T. Nguyen, T.-K.
Huang, J. Schneider, and N. Djuric, “Multimodal trajectory predictions
for autonomous driving using deep convolutional networks,” in 2019
International Conference on Robotics and Automation (ICRA).
IEEE,
2019, pp. 2090–2096.

[13] S. Casas, W. Luo, and R. Urtasun, “Intentnet: Learning to predict
intention from raw sensor data,” in Conference on Robot Learning.
PMLR, 2018, pp. 947–956.

[14] W. Zeng, M. Liang, R. Liao, and R. Urtasun, “Lanercnn: Distributed
representations for graph-centric motion forecasting,” in 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2021, pp. 532–539.

[15] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple
probabilistic anchor trajectory hypotheses for behavior prediction,” arXiv
preprint arXiv:1910.05449, 2019.

[16] B. Varadarajan, A. Hefny, A. Srivastava, K. S. Refaat, N. Nayakanti,
A. Cornman, K. Chen, B. Douillard, C. P. Lam, D. Anguelov et al.,
“Multipath++: Efﬁcient information fusion and trajectory aggregation
for behavior prediction,” arXiv preprint arXiv:2111.14973, 2021.
[17] A. Jain, S. Casas, R. Liao, Y. Xiong, S. Feng, S. Segal, and R. Urtasun,
“Discrete residual ﬂow for probabilistic pedestrian behavior prediction,”
in Conference on Robot Learning. PMLR, 2020, pp. 407–419.
[18] J. Gu, C. Sun, and H. Zhao, “Densetnt: End-to-end trajectory prediction
from dense goal sets,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 15 303–15 312.

[19] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun,
“Learning lane graph representations for motion forecasting,” in Euro-
pean Conference on Computer Vision. Springer, 2020, pp. 541–556.

[20] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,
“Home: Heatmap output for future motion estimation,” in 2021 IEEE
International Intelligent Transportation Systems Conference (ITSC).
IEEE, 2021, pp. 500–507.

[21] C. Choi, A. Patil, and S. Malla, “Drogon: A causal reasoning framework
for future trajectory forecast,” arXiv preprint arXiv:1908.00024, vol. 2,
no. 3, p. 4, 2019.

[22] C. Tang and R. R. Salakhutdinov, “Multiple futures prediction,” Ad-
vances in Neural Information Processing Systems, vol. 32, 2019.
[23] S. Khandelwal, W. Qi, J. Singh, A. Hartnett, and D. Ramanan,
“What-if motion prediction for autonomous driving,” arXiv preprint
arXiv:2008.10587, 2020.

[24] D. Helbing and P. Molnar, “Social force model for pedestrian dynamics,”

Physical review E, vol. 51, no. 5, p. 4282, 1995.

[25] M. Kuderer, H. Kretzschmar, C. Sprunk, and W. Burgard, “Feature-based
prediction of trajectories for socially compliant navigation.” in Robotics:
science and systems, 2012.

[26] Y. Luo, P. Cai, A. Bera, D. Hsu, W. S. Lee, and D. Manocha,
“Porca: Modeling and planning for autonomous driving among many
pedestrians,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp.
3418–3425, 2018.

[27] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha, “Reciprocal n-
Springer, 2011, pp.

body collision avoidance,” in Robotics research.
3–19.

[28] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, “Stgat: Modeling spatial-
temporal interactions for human trajectory prediction,” in Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2019,
pp. 6272–6281.

[29] S. Yi, H. Li, and X. Wang, “Pedestrian behavior understanding and
prediction with deep neural networks,” in European Conference on
Computer Vision. Springer, 2016, pp. 263–279.

[30] N. Kamra, H. Zhu, D. K. Trivedi, M. Zhang, and Y. Liu, “Multi-agent
trajectory prediction with fuzzy query attention,” Advances in Neural
Information Processing Systems, vol. 33, pp. 22 530–22 541, 2020.
[31] L. L. Li, B. Yang, M. Liang, W. Zeng, M. Ren, S. Segal, and R. Urta-
sun, “End-to-end contextual perception and prediction with interaction
transformer,” in 2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2020, pp. 5784–5791.

[32] V. Kosaraju, A. Sadeghian, R. Mart´ın-Mart´ın, I. Reid, H. Rezatoﬁghi,
and S. Savarese, “Social-bigat: Multimodal trajectory forecasting using
bicycle-gan and graph attention networks,” Advances in Neural Infor-
mation Processing Systems, vol. 32, 2019.

[33] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kum-
merle, H. Konigshof, C. Stiller, A. de La Fortelle et al., “Interaction
dataset: An international, adversarial and cooperative motion dataset
in interactive driving scenarios with semantic maps,” arXiv preprint
arXiv:1910.03088, 2019.

[34] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai,
B. Sapp, C. R. Qi, Y. Zhou et al., “Large scale interactive motion
forecasting for autonomous driving: The waymo open motion dataset,”
in Proceedings of the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 9710–9719.

A. Target Generation Parameters

APPENDIX

This is to list the parametric study mentioned during the feature preparation process, as discussed in Sec IV-B.

TABLE III: Experiment Records for Target Samples

Parameter Set Number
#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
#11
#12
#13
#14

Number of Target
8000
8000
8000
10000
6000
8000
6000
6000
10000
12000
12000
12000
16000
20000

Range of Target
[-100.0, 50.0, -80.0, 80.0]
[-150.0, 100.0, -100.0, 100.0]
[-200.0, 100.0, -150.0, 150.0]
[-200.0, 100.0, -150.0, 150.0]
[-150.0, 100.0, -100.0, 100.0]
[-300.0, 200.0, -200.0, 200.0]
[-200.0, 100.0, -150.0, 150.0]
[-200.0, 100.0, -150.0, 150.0]
[-200.0, 100.0, -100.0, 100.0]
[-200.0, 100.0, -150.0, 150.0]
[-200.0, 100.0, -100.0, 100.0]
[-200.0, 100.0, -100.0, 100.0]
[-200.0, 100.0, -100.0, 100.0]
[-200.0, 100.0, -100.0, 100.0]

Radius of Lane
80
160
160
160
120
200
100
160
160
140
160
200
200
200

Radius of Object
60
120
120
120
100
200
100
120
120
100
120
200
200
200

BMD1:Mean
9.87
3.30
3.05
2.24
6.00
2.90
7.58
4.45
2.21
2.52
1.73
1.38
0.99
0.94

BMD1:Median
0.65
0.58
0.57
0.55
0.62
0.57
0.63
0.60
0.55
0.55
0.54
0.54
0.53
0.52

BMD1:75% BMD1:90% BMD2:Mean
40.60
3.28
2.36
1.84
21.29
2.11
30.22
8.68
1.83
2.04
1.67
1.56
1.47
1.47

1.89
1.10
1.07
1.00
1.33
1.06
1.50
1.20
1.00
1.02
0.96
0.94
0.91
0.91

9.54
3.02
2.81
1.96
5.69
2.73
7.24
4.31
1.94
2.12
1.47
1.23
0.87
0.81

BMD2:Median
0.58
0.51
0.50
0.48
0.55
0.51
0.56
0.54
0.48
0.48
0.47
0.47
0.46
0.46

BMD2:75% BMD2:90%

2.29
0.98
0.95
0.88
1.22
0.96
1.41
1.10
0.88
0.88
0.84
0.83
0.80
0.80

38.73
2.71
2.00
1.63
21.26
1.98
28.20
9.26
1.61
1.74
1.47
1.42
1.34
1.33

B. Interaction Scenarios Visualization

In this section, we visualize several scenarios in Waymo Interactive Motion Prediction Dataset. All lines are rotated back
to agent A’s coordinate system. The line legends can be found in the upper-left corner. For abbreviation, we use the marginal
model to stand for the Cartesian product of two single motion prediction model outputs and use the joint model to represent
the interactive motion prediction model.

• For marginal model visualization, each agent’s top 6 trajectory candidates are drawn, with the corresponding vehicle shape

visualized on the destination of the highest probability mode (Tpred = 8s).

• For joint model visualization, top 6 trajectory candidate pairs are drawn, with vehicle shape visualized on the destination

of the highest probability pairs (Tpred = 8s).

Moreover, we separate these visualizations into different categories for classiﬁed illustrations.

Joint model matches the ground truth and shows higher prediction capability.

(a) Marginal Model Prediction for Scenario ID: b4dfa76ba3ab3b02

(b) Joint Model Prediction for Scenario ID: b4dfa76ba3ab3b02

Fig. 7: During a highly interactive two-left-turn scenario, the joint prediction of agent A yields to agent B, matching the ground truth.

(a) Marginal Model Prediction for Scenario: cda124af5393dff4

(b) Joint Model Prediction for Scenario ID: cda124af5393dff4

Fig. 8: Agent A’s joint prediction is yielding to the straight trafﬁc by adjusting speed, matching the ground truth.

Joint modal deviates from ground truth due to high interaction.

(a) Marginal Model Prediction for Scenario: 6b011fa4a87377e3

(b) Joint Model Prediction for Scenario ID: 6b011fa4a87377e3

Fig. 9: The joint prediction model is aware of the two-left-turns scenario and matches the ground truth

(a) Marginal Model Prediction for Scenario: 4389c3b5fd11d3b7

(b) Joint Model Prediction for Scenario ID: 4389c3b5fd11d3b7

Fig. 10: The joint prediction model is aware of the two-left-turns scenario and matches the ground truth

(a) Marginal Model Prediction for Scenario: c23ca2936c737633

(b) Joint Model Prediction for Scenario ID: c23ca2936c737633

Fig. 11: The joint model predicts lane change in a two-right-turns scenario while the ground truth is not switching lanes.

(a) Marginal Model Prediction for Scenario: 48aa99ecbb9bbbb3

(b) Joint Model Prediction for Scenario ID: 48aa99ecbb9bbbb3

Fig. 12: The joint prediction is aware of the complicated situation but failed to make a correct prediction. agent A’s top mode hesitate
before the trajectory intersection point.

(a) Marginal Model Prediction for Scenario: 42cc950c45c6c043

(b) Joint Model Prediction for Scenario ID: 42cc950c45c6c043

Fig. 13: The joint model forces the prediction to change intent to avoid conﬂicts but deviates from the ground truth.

