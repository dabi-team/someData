Sequence Feature Extraction for Malware Family
Analysis via Graph Neural Network

Shun-Wen Hsiao∗ Po-Yu Chu∗
∗Dept. of Management Information Systems, National Chengchi University, Taipei, Taiwan
{hsiaom,109356020}@nccu.edu.tw

2
2
0
2

g
u
A
0
1

]

R
C
.
s
c
[

1
v
6
7
4
5
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Malicious software (malware) causes much harm to
our devices and life. We are eager to understand the malware
behavior and the threat it made. Most of the record ﬁles of
malware are variable length and text-based ﬁles with time
stamps, such as event log data and dynamic analysis proﬁles.
Using the time stamps, we can sort such data into sequence-
based data for the following analysis. However, dealing with
the text-based sequences with variable lengths is difﬁcult. In
addition, unlike natural language text data, most sequential data
in information security have speciﬁc properties and structure,
such as loop, repeated call, noise, etc. To deeply analyze the API
call sequences with their structure, we use graphs to represent
the sequences, which can further investigate the information
and structure, such as the Markov model. Therefore, we design
and implement an Attention Aware Graph Neural Network
(AWGCN) to analyze the API call sequences. Through AWGCN,
we can obtain the sequence embeddings to analyze the behavior
of the malware. Moreover, the classiﬁcation experiment result
shows that AWGCN outperforms other classiﬁers in the call-
like datasets, and the embedding can further improve the classic
model’s performance.

Index Terms—Graph Neural Network, Attention, Sequential

Data, Markov Model

I. INTRODUCTION

In recent years, malicious software (malware) has yielded
over 200 million per year [1]. Malware causes much trouble
in information security because of the massive amount and
rapid creation. According to the report [2], the number of
malware has exceeded 1,339 million. Furthermore, the hackers
can produce the malware through metamorphism, a technique
that mutates the malware compositions but with the same
purpose. Metamorphism may change the API (Application
Programming Interface) calls invoked with each run of the
infected program [3] and insert dummy or random calls (i.e.,
noise) for obfuscation. That is, malware can be massively
generated within a short period and can avoid being detected.
Consequently, the malware authors can produce over 12 mil-
lion new malware per month on average in the last two years.
A variety of malware poses different kinds of severe prob-
lems to the user device, including smartphones, laptops, and
desktops. For example, WannaCry, a branch of ransomware,
caused a worldwide impact on all walks of life in 2017.
WannaCry encrypts the data on the affected endpoint device
and demands the ransom payment in the Bitcoin. The statistics
point out that 300,000 systems in over 150 countries had been
damaged because of wildly and rapidly spreading speed [4].
For another instance, Mirai is malicious software that creates

botnets of IoT devices. Mirai drew the public attention after
it was used in distributed denial-of-service (DDoS) attack
against the website of Kreb on Security, which covers com-
puter security and cybercrime [5]. The DDoS attack launched
through Mirai caused the crash on high pageview websites,
including GitHub, Twitter, Reddit, etc. From the example
stated above, we realize how desperate damage can be caused
by the malware.

However, the current malware analysis approaches highly
rely on malware analysts’ domain knowledge and standard
operation process to identify the intention of malware. In
contrast, the metamorphism technique hugely enhances the
difﬁculty of malware analysis. The massive amount of mal-
ware and the sophisticated call sequence invoked by malware
makes the analysis difﬁcult. The complicated and innumerable
tasks overwhelm the analysts. Consequently, an automatic and
timely analysis method is indispensable to achieve proper
defense and lessen the inﬂiction of loss on the computer
devices.

To analyze the behavior of malware, static and dynamic
analysis methods are usually applied. Static analysis method
extracts program behavior form collected executable ﬁle, such
as Windows portable executable (PE) ﬁle or Linux executable
and linkable format (ELF) ﬁle [6]. Dynamic analysis method
executes malware in a sandbox and records the interaction
between malware and operation system [7]. These static and
dynamic malware proﬁle is usually unstructured, text-based,
temporal, and/or variable-length that makes it more difﬁcult
to analyze. In this paper, we focus on dynamic analysis
proﬁle that contains a sequence of call invocation executed
by malware.

It

is challenging to deal with variable length and non-
numeric sequential text data. In the past, several methods are
used when analyzing sequential text data, such as sequence
alignment [8], bag-of-words, or word2vec [9]. These methods
transform the text in the proﬁle into a numerical vector but
their transformation do not consider the structure of the text
sequence. In such way, a malware author may construct a
malware through the permutation of call sequence and the
insertion of noise fragments [10] to avoid being detected such
methods. Namely, unlike natural language, the call sequence
may contain certain structure, such as loop, repeated call,
and swap structure generated by the malware code. From
this point of view, when transforming the text into numerical
vector, we further consider to include the structure of the text

 
 
 
 
 
 
to better represent a malware proﬁle. This study investigates
using a graph, such as ﬁnite state machine or Markov model,
to represent the structure of the call sequence. We then embed
the structure and text together to represent a malware proﬁle.
We believe that such representation of proﬁles can help the
downstream security applications, for example, malware fam-
ily classiﬁcation, malware characteristics extraction, malware
behavior clustering.

In this study, we would like to develop a novel graphic
neural network to automatically embed variable-length, se-
quential, text-based data to a proper numerical vector with
preserving its hidden information in the sequence structure.
In addition, this method can highlight the most important sub-
sequence in the proﬁle when performing the downstream task.
In this way, we can visualize the importance of individual call
in the proﬁle. We anticipate the embedding methods should
outperform conventional text embedding methods [11], [12].
To achieve our goal, the following issues should be solved:
(1) how to represent a structure of a sequence and text
information while considering the sequence is variable-length
and contains noises; (2) how to embed the graph structure,
node (call features and information), and edge (call transition
probability and information) into a vector; (3) how to identify
the importance of each individual call in the sequences when
performing downstream task.

For the ﬁrst

issue, we design and implement a Graph
Generation Module that adopt the Markov model to transform
each sequence to a corresponding graph with the transition
probabilities learned from the data and the information of
calls. By our design, we can preserve the structure information
hidden in the original call sequences and handle the imbalance
length of sequences. For the second issue, we then feed the
graphs to the customized graph convolution neural network
(GCN). While training the neural network with each graph, the
node information between the consecutive calls in the graph
will be exchanged bidirectionally regarding to its transition
probability. The last layer of the customized GCN is designed
as the representation of the input graph. For the last issue,
we design a novel attention structure that can specify the
importance of each individual call from the training data
directly and automatically. This structure helps us to reveal the
importance of input feature and transfer the attended features
to the next convolution layers. At the end, we can obtain
the importance of each call of the sequence with the help
of graphic structure.

The contributions of this study are listed as follows.

• We develop a graph-based approach to dealing with the
embedding issue for the variable-length, sequential, text-
based sequences data whose structure contains loops,
repeated events, shifted sub-sequence, etc. We anticipate
that adopting graph representation is more appropriate
than using NLP approach to analyze such sequential data.
• We develop a GCN-based neural network to transform
the graphs to numerical vectors that can well perform the
representation of the information contained in the graphs

for the latter downstream task, such as malware family
classiﬁcation.

• We develop a novel attention structure directly on the
input
text (node) and in the convolution structure to
capture the attention that can help us reveal the crucial
sub-sequence of the input.

• We use the real-world malware datasets to demonstrate
the effectiveness of the proposed system compared with
the several embedding models and NN-based models. The
proposed Attention Aware Graph Convolution Network
(AWGCN) can better represent the sequence data and
outperform them on the downstream malware family
classiﬁcation tasks. On four datasets with more than
6,000+ malware samples, we have average 97.63% f1-
score which is around 2% of improvement better than
the 2-nd best models.

The rest of the paper is organized as follows. In Section II,
we review some background and related works. Section III is
the proposed design of AWGCN. The evaluations of AWGCN
are demonstrated in Section IV. In Section V, we discuss the
some insights and future works. At last, Section VI contains
some concluding remarks.

II. RELATED WORK

A. Text Embedding Algorithm

For the computation convenience, usually non-numeric data
(e.g., text and categorical data) are converted to a uniﬁed
numeric format before performing the downstream tasks. In
this section, we brieﬂy introduce several approaches to deal
with text data.

1) Bag-of-words and One-hot: Bag-of-words uses word oc-
currence frequency in the sequence to convert the original text-
based sequence. One-hot converts a word into a vector with
only one digit being one; others are zeros. The disadvantage
of such approaches is the curse of dimensionality if lots of
unique words are used.

2) Word2Vec [9]: There are two training models in
Word2vec; one is skip-gram, and another is CBOW. They
are both self-supervised learning method that uses sentences
to train the word vector. Skip-gram inputs a word to predict
the context; on the other hand, CBOW inputs the context to
predict the blanking word, like a cloze task. Because Word2vec
generate a single word embedding representation for each
word in the corpus [13], it can not deal with polysemy words.
3) Doc2Vec [14]: To overcome the shortcomings of bag-of-
words that lack of information of word ordering and sentence
structure. Le and Mikolov proposed Paragraph Vector, adding
paragraph identiﬁcation into the model. This mechanism takes
the sentence position in the paragraph into account.

4) RNN: Recurrent Neural Network (RNN) uses the pre-
vious hidden state to predict the next state, and is widely
used to analyze sequential data. Hochreiter and Schmidhuber
developed long short-term memory (LSTM) [15] networks
to improve original RNN and performed well in multiple
application domains.

5) Transformer [16]: A transformer is a deep learning
model consisting of self-attention mechanism and feed forward
neural network. The structure of transformer is the stack of
encoders and decoders. The design of transformer deprecates
the mechanism of RNN, namely, the prediction of input can
learn from any part of the original sequence. Transformer are
increasingly the model of choice for NLP problems, replacing
RNN models such as long short-term memory (LSTM). This
also led to the development of pretrained systems such as
BERT [17], [18].

B. Graph Neural Network

Convolution layer have been widely used for extracting
higher-level representation from image data for latter analysis,
for example, extracting facial features from a pixel image
for emotion detection. Convolution Neural Network (CNN)
has been proven well-performed in many computer vision
domains. However, analyzing image data by CNN relies on
the ﬁxed input order and neighbor pixels to correctly extract
higher-level features [19].

Nowadays, the concept of convolution has been extended to
other data type, such as graph who has no ﬁxed ordering and
has more complex neighboring relations. To analyze a graph,
e.g., knowledge graphs and molecular structure, convolutional
neural networks have been generalized on the graph domain
[20].

According to the taxonomy in [21], graph neural net-
works can be categorized into recurrent graph neural net-
works (RecGNNs), convolutional graph neural networks (Con-
vGNNs), graph autoencoders (GAEs), and spatial-temporal
graph neural networks (STGNNs). Moreover, GNNs can cope
with three kinds of graph analytics tasks with different
graph structures as input, including node-level, edge-level,
and graph-level tasks. Each category can be used to deal
with different main tasks as well. To classify the graph
with a distinct structure, we need to learn the whole graph
representation. Therefore, we focus on convolutional graph
neural networks in the research, which is further close to our
primary goal.

Given a graph G = (V, e), which consist of |V| vertices and
|e| edges, we can construct the adjacency matrix A, which
is a |V| x |V| matrix with (i, j) entry equaling to 1 if there
is an edge connecting vertex i and j and 0 otherwise. The
original graph convolutional network (GCN) [22] proposed
the following layer-wise propagation rule:
H (l+1) = σ( ˆAH (l)W (l))

(1)
2 , and ˜A = A + IN is the adjacency
where ˆA = ˜D− 1
matrix A with added self-connection. IN is the identity matrix,
˜D is the degree matrix and W is the trainable weight matrix. σ
denotes an activation function, such as ReLU, Softmax. H (l)
is the matrix of activation in the lth layer, and H 0 = X, which
is the feature matrix of the vertices in the graph.

2 ˜A ˜D− 1

We can visualize the information propagation in the graph
convolution layer in Fig. 1. GCN [22] assigns α depending on
each node’s degree. The nodes can acquire information of their

Fig. 1.
convolution layers.

The visualization of information propagation process in graph

neighbor nodes. The ﬁgure shows that the red node receives
the information from three blue nodes, and all the other nodes
receive information from their neighbors as well.

Owing to the success of GCN, a great deal of research
proposed their own propagation rule to improve the perfor-
mance further. For example, graph attention networks (GAT)
[23] assume the neighbors’ contribution is imbalanced due
to the importance being different to the central vertex. Thus,
GAT implements the attention mechanism to learn the relative
weights between two connected vertices before the message
propagation. Graph sample and aggregate (GraphSAGE) [24]
proposed a trick to improve the efﬁciency of taking the total
size of the neighboring vertices. GraphSAGE adopts sampling
to obtain a ﬁxed size of neighbors for each vertex and apply
an aggregation function to the embedding.

In previous work, graph convolutional network are applied
to graph tasks such as citation networks, social networks, bio-
chemical graphs, knowledge graphs, and others. Furthermore,
graph convolutional network are also used on text data like
fake news prediction. In fake news predictions, the graph data
are used to describe the information between the news authors.
Namely, the graph data is additional information in fake news
data. In our work, the text data (call) is regarded as a node;
that is, we use graph convolutional network directly on API
call data.

Our proposed model used the graph convolutional layers
to propagate the information between the vertices for the
following downstream tasks. Further detailed design of our
model is described in Section III.

C. API Call Sequence

API stands for application programming interface, a type
of software interface that speciﬁes how clients should interact
with software components [25]. Nowadays, many service
providers offer their API to allow users to extract or transport
the data [26], such as e-commerce website.

When users want to interact with the system, for example,
log on to the app or search the question via a browser, users
have made an API call. Deﬁnitely, not only interact with
browsers but also access to the system resources need the
help with API. For instance, user applications cannot access
hardware and system resources directly in Windows operating
system. Nonetheless, they can rely on the interfaces provided
by dynamic-link libraries [27].

Fig. 2. A partial sample of Windows API call sequence.

Fig. 3. A real example of API call sequence Markov model.

As stated above, the malware authors can create the user
program and use the API calls to perform malicious actions.
The Windows API function calls fall under various functional
levels such as network resources and libraries [28]. Hence, the
API call sequence can reﬂect the behavior of the execution
process, whether the process is malicious or benign. In other
words, we can extract information from the API call sequence
to represent the process; theoretically, it should be precise for
the following downstream tasks.

Figure 2 is a partial example of the collected malware
proﬁle. Besides the API name, an API call invocation has
time information so that we can link them as a sequence as
a whole. In addition, most of the dynamic analysis tool also
output certain extra information, such as call parameters and
return values.

D. Markov Model

to model

A Markov model

is a stochastic model

the
changing processing between each state. Markov chain is a
specialized Markov model, which describes a sequence of pos-
sible events transition with their probability. With the Markov
property, the probability of moving to the next state depends
only on the present state and not on the previous states [29].
The formal deﬁnition of Markov chain in mathematics is
shown in Eq. 2

P r(Xn+1 = x|X1 = x1, X2 = x2, ..., Xn = xn)
= P r(Xn+1 = x|Xn+1 = x)

(2)

four unique Win32 APIs, i.e., RegQueryValue, LoadLibrary,
RegSetValue and CreateFile. It has 26 calls in total. (Due to
the page limitation, we select a relativity short sample for
demonstration. On the average, the malware samples in our
different datasets have 50 – 364 calls in a proﬁle.)

III. PROPOSED SYSTEM

A. Overview

Figure 4 is the overview of the proposed system that
can input the raw malware proﬁles (in terms of API/system
call sequences) for graph representation, embedding, graph
attention and downstream malware family classiﬁcation.

The system has three main modules: Preprocessing, Graph
Generation Module and Graph Convolution Network. The
Preprocessing module parse the raw data (i.e., malware proﬁle)
and malware family labels to construct the call sequences of
each malware family. The Graph Generation Module accepts
all sequences and generate a Markov model graph with tran-
sition probability for each sequence. Vanilla Markov model
graphs can be plotted in this stage. The Graph Convolution
Network accepts the training data (i.e., the graphs) and the
training labels (i.e., corresponding malware family labels) to
perform graph embedding by AWGCN. The Graph Latent
Space represents the original call sequence with the graph
structure. The neighbor vectors in the Graph Latent Space
indicates that their original sequences use similar calls and
similar structure. At the end, a malware family classiﬁer can
classify a proﬁle into the corresponding class. We will use 20%
of the data for evaluation. In addition, the trained attention
weights can be used to specify the importance of each call in
the Markov model graph.

The detail design of each module is described as follows.

The notations used in this paper are listed in Table I.

B. Preprocessing Module

The collected datasets have their own format to represent the
call sequences, so preprocessing is needed. The Preprocessing
Module will parse the proﬁles and select necessary informa-
tion (e.g., malware hash value, timestamp, call name, call
parameters, call return values) to construct the call sequences
and identify its malware family label. The psudo-code of this
module is shown in Fig. 6. For each proﬁle, p, in the collected
proﬁle pool, P (having n proﬁles), a sequence s contains all
the information of this proﬁle, including the proﬁle hash value,
malware family label, and a sequence of call name and its
parameters. In our implementation, the sequences are stored
in a customized Python data structure.

C. Graph Generation Module

Our research uses the Markov chain to represent the tran-
sition between the API call (as shown in Fig. 3), each node
represent an unique call in the sequence) and transform the
original sequential data into graph format data. Figure 3
demonstrates the Markov chain of a real-world malware
sample used in our experiment. This demonstrated malware
belongs to Vobfus malware family, and it only invoked

We design a k-gram mechanism, which helps us to better
portray the feature when transforming the sequences. We use
Markov model to depict our original API call sequences be-
cause it contains a transition probability table that can describe
the transition of neighboring calls. The characteristic of API
call sequence is the transition between each call because API
call sequence represents the continuous occurrence of calls.

Fig. 4. The overall process of our purposed system.

Fig. 5. Graph Convolutional Network architecture

This characteristic is similar to the essence of Markov model;
thus, we adopt Markov model to transform the sequences.
Moreover, Markov model can preserve the short-term relation
of the calls and quantized transition probabilities, which can be
used in the next neural network module to enrich the informa-
tion of input sequences. In this form, transition probabilities
can be seen as the transfer of information in the following
graph neural network. In addition, if we want to preserve long-
term relations, we can adjust k-gram mechanism to fulﬁll. The
psudo-code of this module is shown in Fig. 7. We traverse each
sequence, s ∈ S, to record the transition between the calls and
add to g.A. In the meanwhile, we record the edge occurrence
in e.weight and divide by sum of v.edge.weight to calculate
the probability of each edge. To feed the graph data into the

following graph neural network, we use one-hot encoding to
transform the calls into vector format data g.X. We use one-
hot because it is a primary encoding method that can show
our following graph neural network capability. At the end, we
can also visualize the g, such as Figure. 3

D. Graph Convolution Network Module

In the beginning of our GCN model architecture, we design
a Feature Attention (F A) layer. We want
to capture the
important part of the input graphs through this layer. Next, we
use k-layers of graph convolution layers to extract the higher-
level node representations of the whole graph. The graph latent
space is the embedding of original s in malware proﬁle with
graph structure. In the last of this model, we apply a classiﬁer,
which map the extracted feature to the label space, softmax

TABLE I
THE TABLE OF NOTATION

Description
Proﬁle set
API call sequences set
The number of proﬁle ﬁles in P
The number of API call sequences in S
The i-th proﬁle, i = 1 ∼ n
The hash value of pi
The malware family label of pi
The API call sequence for pi, i = 1 ∼ |S|
The j-th call of si, j = 1 ∼ |si|
The name of call ci
j
The parameters of call ci
j
The graph of a s and a p, g=(V , E)
The graph set of all g
The vertex set of a graph g
The edge set of a graph g
A vertex in V
An edge from vertex va to vertex vb, va and vb ∈ V
The set of all edges starting from v
The weight of edge e
The input feature matrix of graph g
The adjacency matrix of graph g
The transition probability of the edges.
Weight matrix
The convolved feature matrix at l layer.
The feature attention matrix of g
The adjacency attention matrix of g
The attended feature matrix of g, g.X × g.F A = g.AF
The set of vertex in entire dataset.
Do one-hot encoding to the input.

Notation
P
S
|P |, n
|S|
pi
pi.hash
pi.label
si
ci
j
ci
j .name
ci
j .pars
g
G
V
E
v
e(va, vb)
v.edges
e.weight
g.X
g.A
g.Aattr
W
Hl
g.F A
g.AA
g.AF
v
OH()

Algorithm 1 Data Preprocessing

Input: A set of proﬁle P .
Output: A set of sequence S.

1: S = EmptySet()
2: for i ← 1 to i = |P | do
3:
4:
5:
6:
7:

si.hash = pi.hash
si.label = pi.label
si.seq = EmptyList()
for c ∈ pi do

append (c.name, c.pars) to si.seq

end for
add si to S

8:
9:
10: end for

Fig. 6. The pseudo code of data preprocessing

activation function for mutil-label classiﬁcation, and sigmoid
for binary classiﬁcation.

We take a graph G and the label P.label as input. In
Figure. 5, all the cuboid with diagonal dashed line is ﬁxed,
others are trainable. We use a graph as input to explain the
process in Graph Convolution Network Module. The input
feature matrix in Figure. 5, means the graph contains |V |
vertices, and each node has a d dimensional feature. We then
multiply the X with proposed trainable Feature Attention layer

Algorithm 2 Graph Generation Module

Input: A set of sequence S.
Output: G.
1: G = EmptySet()
2: for si ∈ S do
3:

gi.X = OH(v)
gi.A = EmptyList()
gi.Aattr = EmptyList()
for c.name ∈ si do
j.name, ci
append e(ci
e(ci
end if

j.name, ci

if e(ci

4:
5:
6:
7:

8:

end for
add gi to G

9:
10:
11:
12:
13: end for
14: for v in gi ∈ G do
15:
16:
17:
18: end for

freq = sum up vi.edge.weight
probs =Divide e.weight ∈ vi by freq
append probs to gi.Aattr

j+1.name) not in gi.A then
j.name, ci
j+1.name) to gi.A

j+1.name).weight += 1

Fig. 7. The pseudo code of Graph Generation Module

(F A) to capture the signiﬁcance of each node. As shown in
Figure. 5 feature attention part, we get the Attended Feature
Matrix (AF ).

After that, we pass |V |xd AF to designed three times graph
convolution to exchange the information between the vertices.
Before the dot product begins, we multiply the adjacency
matrix with proposed trainable Adjacency Attention. Hence,
we can further capture the connection importance of the graph.
As mentioned in II, if vertices are connected, the correspond-
ing adjacency matrix digit is marked as 1. Thus, we can
achieve the information exchange in the AF through the dot
product with attention attended adjacency matrix. Furthermore,
we use the transition probabilities, g.Aattr, calculate from
previous module to decide the ratio of the transfer between
connected vertices. In the last of a graph convolution, we
use a weight matrix whose dimension is dxk1 to transform
the feature dimension of nodes. Through this weight matrix,
we can project the original d dimension feature to a larger
k1, which means we can adopt more information from the
transferred feature matrix, this can help us learn more from
the original input graph as well.

After the ﬁrst graph convolution, we obtain a new AF
with dimension |v|xk1. We take this attended feature matrix
as input to the second convolution process. Same as the ﬁrst
convolution, we can adjust k2 to preserve more information
which may help the downstream classiﬁcation task. After the
convolution process, we apply fully connected layer, dropout
layer and use softmax to classify the multi-label task.

The pseudo code of Graph Convolution Network is shown

Algorithm 3 Graph Convolution Network

Input: G, P.label.
Output: Trained weight of W , g.F A, and g.AA.

1: for g ∈ G do
2:
3:
4: end for
5: for epoch do
6:
7:
8:
9: end for

Training set ← gi where i≥ r ∗ |S|
Testing set ← gi where r ∗ |S| <i≤ r ∗ |S|

(cid:46) r = ratio

AF ←g.X × FA
Hl+1 ← AA × g.A · Hl · Wl
Bcakpropagate and update weight

(cid:46) H0 = AF

Fig. 8. The pseudo code of Graph Convolution Network

in Fig. 8. We input the graphs G with the label P.label to
proposed Graph Convolution Network. In the beginning, we
split the whole dataset into training and testing with 80:20
ratio. We input our training graphs into the GCN, and provide
the g.X, g.A and g.Aattr to train the GCN. In each epoch,
we attend the g.X, convolve the AF three times and use the
classiﬁer to evaluate the correctness. Then do backpropagation
and update the weight for the training in next epoch. The
output of this module, besides the classiﬁcation result, we also
obtain the graph embedding and weight of W , F A and AA.
The embedding and weight can help us cluster the original
sequences and specify the important part of the graphs.

IV. EVALUATION

A. Data Set

As shown in Table II, ﬁve datasets are used in our ex-
periments. SynData is used for proof-of-concept
to make
sure the synthetic special calls can be identiﬁed. RanSyn is
sophisticated synthetic data to imitate the invocation of API
calls. WinMal and Oliverira are malware proﬁles that record
the Windows API calls invoked by malware samples. Syscall
records the Linux system calls by IoT malware. The Unique
Event column speciﬁes the number of unique call used among
all sequences in the dataset. The Classes column speciﬁes the
number of malware family labels in the dataset.

1) SynData: There are 300 different call sequences in 3
families. The length of each call sequence is 50. The purpose
of using synthetic data is to prove the correctness of our
attention mechanism. Therefore, we randomly generate 50
calls which is the number between 0 to 50. After that, we
replace the speciﬁc digit (21st to 25th) with the alphabet.
As shown in Fig. 9, in family 1, we replace the digit with
lowercase a, b, c, and d at 21st-24th digits; in family 2, replace
it with e, f, g, and h 21st-24th digits; and in family 3, replace
it with i, j, k, and l 21st-24th digits. Last, the 25th digit in
each family is the capital letter A. We simulate the ordinary
call with random digit and give each family a speciﬁc feature.
The capital letter A is the same feature across the family.

Therefore, we expect our model should only point out the
lowercase alphabets are more important after training.

Family 1: [’11’,’8’, ’a’, ’b’, ’c’, ’d’, ’A’,
’35’,’42’,’28, ’13’, ... ,’25’,’48’,’9’]
Family 2: [’14’,’8’, ’e’, ’f’, ’g’, ’h’, ’A’,
’12’,’35’,’21’,’18’, ... ,’28’,’45’,’1’]
Family 3: [’16’,’4’, ’i’, ’j’, ’k’, ’l’, ’A’,
’17’,’33’,’29’,’12’, ... ,’28’,’45’,’1’]

Fig. 9. Example of our synthetic API call sequence data

2) RanSyn: To imitate the invocation of calls in real-
world situations, we create another dataset – RanSyn (Random
generated Synthetic data). This dataset is more sophisticated
than Syndata. In this dataset, the position of the alphabet
subsequence is not ﬁxed, and the numeric digit is possibly
inserted between the alphabet subsequence. There are 10
percent The next digit of the numeric digit has a 10 percent
probability of being an alphabet digit and 90 percent of being a
numeric one. On the other hand, the next digit of the alphabet
digit has a 5 percent probability of being a numeric digit. In
addition, the alphabet subsequence may invoke not only one
time. The other setting is the same as Syndata.

3) RanMarkov: There are 400 different call sequences in
4 families. We build 4 different Markov chain at ﬁrst and use
random walk to traverse them to generate the call sequences
whose length is 250. For example, we use ﬁve same API
calls but with different transition probability and structures
in Figure 10. By this approach, the call sequences will have
the characteristic of the Markov chain and the call sequences
can be similar to the real world call sequences data.

Fig. 10. The synthesized Markov chain in RanMarkov dataset.

4) WinMal: There are 1,940 different call sequences in 14
families. The maximum length of our API call sequences is
20,365, minimum length is 15, and average length as shown in
Table II. Figure. 11 shows the number of API call sequences
in each malware family. The malware family contains the most
samples is Allaple which has 615 different proﬁles. In contrast,
Loring and Mydoom contain the least malware samples in
WinMal. On the average, the malware samples in the other
families have 22-312 proﬁles in WinMal. Because WinMal
is the real-world dataset, the imbalanced number of sample in
each family is natural. Therefore, we did not adjust the number

TABLE II
TABLE OF DATASETS

Dataset
SynData
RanSyn
RanMarkov
WinMal [30]
Syscall [30]
Oliveira [32]
aIn average.

Sequence Type
Synthesized Alphabet Sequence with Noise
Random Generated Synthesized Alphabet Sequence with Noise
Random Walk on Synthesized Win32 API Call Markov Chain
Win32 API Call Sequence [31]
Linux System Call Sequence
Win32 API Call Sequence from Cuckoo [35]

Dataset Size
360
360
400
1940
1208
3237

Seq. Length
50
50
250
275a
364a
100

Unique Event
64
64
5
26
121
307

Classes
3
3
4
14
4
2

of sample to be balance. We want to test the capability of
AWGCN in real-world imbalanced data.

Figure. 12 is a partial of real malware sample of API call
sequence and parameters. Each ﬁle has the process identiﬁca-
tion in the beginning of proﬁles, and the number with pound
sign is the timestamp. The content between two timestamps is
the API call and its parameters. From this sample we observe
several different calls, including LoadLibrary, RegQueryValue,
CreateFile, RegEnumValue. Each call has parameters to record
the event state, for example, LoadLibrary records the loaded
dynamic link library, CreateFile records the ﬁle stored path.
The malware samples in this dataset are available on [30] and
similar to [31].

Fig. 11. The API call sequences family distribution in WinMal

5) Syscall: The sequences in Syscall are longer than se-
quences in WinMal, and the number of unique calls is ﬁve
times more than WinMal as well. Syscall contains Linux
system call 1,208 sequences equally divided into four different
families that collected from IoT devices. Furthermore, some
English words are used in system call, such as access, open,
close, etc. Unlike the call name used in Windows, combine the
words directly, Linux system call has some daily used word
that may be identify by the language models. This dataset is
also available on [30].

6) Oliveira: The second Windows API call dataset is an
open access dataset from IEEE dataport [32]. This dataset con-
tains 42,797 malware API call sequences and 1,079 goodware
API call sequences. In our experiment, to avoid the baseline
model only predicting the sample sequences as malware label,
we balanced the distribution of labels. We randomly choose
2,158 malware sample, two times as goodware, the balance
data can help us to conﬁrm the correctness of the classiﬁcation
indicators. Each API call sequence is composed of the ﬁrst
100 non-repeated consecutive API calls associated with the
parent process. Author extract from the ’calls’ elements of
Cuckoo Sandbox report [35]. Each API call sequence has its
hash name, API call name (in integer type), and the label with
0 for goodware and 1 for malware. Therefore, we can use

3152 malware.exe
#209990000
LoadLibrary
lpFileName=C:\WINDOWS\system32\IMM32.DLL
Return=SUCCESS
#216910000
RegCreateKey
hKey=HKEY_CLASSES_ROOT\CLSID\
{4AEDBC33-8B19-7F8D-B932-E844B2219184}
Return=0
#216960000
RegSetValue
hKey=HKEY_CLASSES_ROOT\CLSID\
{4AEDBC33-8B19-7F8D-B932-E844B2219184}
type=REG_SZ
data=erbkznenwqlwkknq
Return=0
#216990000
RegCreateKey
hKey=HKEY_CLASSES_ROOT\CLSID\
{4AEDBC33-8B19-7F8D-B932-
E844B2219184}\LocalServer32
Return=0
#217010000
RegSetValue
hKey=HKEY_CLASSES_ROOT\CLSID\
{4AEDBC33-8B19-7F8D-B932-
E844B2219184}\LocalServer32
type=REG_SZ
data=C:\Documents and Settings\All Users\
Desktop\malware.exe
Return=0
#217260000
RegQueryValue
hKey=HKEY_LOCAL_MACHINE\System\
CurrentControlSet\Services\WinSock2\Parameters\
WinSock_Registry_Version
Return=0
type=REG_SZ
data=2.0
#217830000
CreateProcessInternal
lpCommandLine=C:\WINDOWS\system32\urdvxc.exe/
installservice
Return=SUCCESS
dwProcessId=3172
dwThreadId=3176

Fig. 12. A partial Example of Windows API call sequence

Fig. 13. AWGCN hypermeter: learning and dropout rate tuning

Fig. 14. Classiﬁcation confusion matrix of AWGCN in WinMal dataset.

binary classiﬁcation for downstream task to test our proposed
model.

B. Family Classiﬁcation Camparsion

In this experiment, we compare our Attention Aware GCN
(AWGCN) with text-based methods and the different classiﬁer
shown in Table III on four datasets. The number in ﬁrst row
is test accuracy, second row is f1-score, and third row is area
under curve (AUC) score.

Because the original synthetic data is too simple to predict
(each family contain speciﬁc feature), we increase the classi-
ﬁcation difﬁculty via making the sequences more complicated
by adding noise. We create noise sequences by adding some
noise calls into the original synthetic sequences. We add the
other two families speciﬁc features into the call sequences in
the family. For example, we add e, f, g, h, i, j, k, and l into
family 1, and so on. Moreover, we add a random integer call
between the original feature of the family, such as a, b, 5,
c, and d. Namely, each sequence contains the all the original
family features. In addition, the training data in this Syndata
and RanSyn are synthetic data without noise, we use 60 noise
sequences only in testing data to test the generalization ability
of the models.

For AWGCN, we use three graph convolution layers to
extract the information from the graph. We also experimented
with other settings to ﬁnd better parameters, as shown in
Figure. 13. We can observe the lower f1-score the point color
is close to blue. The peak of f1-score is close to red, and the
point of the peak is close to (0.005, 0.35). Therefore, we set the
learning rate as 0.005, dropout rate as 0.35, and the dimension
in each graph convolution layer as k1 = 128, k2 = 256, and
k3 = 64, respectively.

Performance. Table III presents the classiﬁcation result of
each model. AWGCN outperforms the other models on four
different datasets, especially on real-world malware dataset.
Figure. 14 show the confusion matrix of the classiﬁcation
result on WinMal dataset. The diagonal line represent the
correct predictions of the model. The families contain least

sample, Loring and Mydoom, AWGCN can perfectly classify
them into correct malware family. The most misclassiﬁcation
of AWGCN is between the family Loadmoney and Graftor.
According to introduce of these two malware family of the
anti-virus company Malwarebytes [36], [37],
the malware
Loadmoney are known to install adware, browser hijackers,
and potentially unwanted programs (PUPs) on Windows sys-
tems. The malware Graftor is a large family of malware
targeting Windows systems, most of which are Trojans, though
some are adware. These two family have preﬁx word adware
on the citation website. AWGCN classify the malware samples
via the behavior of malware, the misclassiﬁcation may be the
similar action but in two different families.

AWGCN is better than others because AWGCN investigates
the transition information from the original call sequences. For
in-depth analysis, sequence alignment can perform well if the
features are in a speciﬁc position. If some unique calls are
only used in a speciﬁc family, the primary classiﬁer with one-
hot encoding can be a fast approach with acceptable accuracy.
Text-based approaches also performed nice results, but they
are more suitable for language corpus than heavily repetitive
sequential data like API call sequence.

In Table IV, we use the same model to identify whether
AWGCN extracts the graph representation successfully or not.
We output the graph latent space vector (shown as blue vector
in Fig.4) to compare with the one-hot encoding data. As the
result show in Table IV, AWGCN latent space improve the
performance of classical models. In the experiment of SVM,
AWGCN enhance performance around 14 percent. This exper-
iment indicate that AWGCN can perform a well embedding
of original sequences and achieves signiﬁcant improvement in
extracting the representation.

C. Attention Mechanism

In this section, we evaluate our designed attention mecha-

nism via attention visualization.

As mentioned above, synthetic sequences are constructed by
ordinary calls (random numbers) and speciﬁc calls (alphabet

TABLE III
RESULTS OF FAMILY CLASSIFICATION TASK.

Encoding

Model

SeqAm

-

Seq. Alignment

OHSVM

One-hot

SVM

OHLR

One-hot

Logistic Regression

OHRF

One-hot

Random Forest

Engtk-BiLSTM

spaCy tokenizer

T2V-BiLSTM [33]

TextVectorization

W2V-GRU

Word2Vec

TkCNN

Tokenizer

Bert [34]

Bert

AWGCN

One-hot

Adjacecny

Bi-LSTM

Dense

Bi-LSTM

Dense

GRU

Dense

1D CNN

Dense

Dense

AWGCN

Dense

SynData
0.987
0.994
0.996
0.317
0.300
0.485
0.333
0.167
0.507
0.367
0.278
0.512
1.000
1.000
1.000
0.275
0.311
0.485
0.250
0.333
0.493
1.000
1.000
1.000
0.500
0.888
0.995
1.000
1.000
1.000

RanSyn
0.300
0.821
0.942
0.487
0.351
0.693
0.463
0.416
0.616
0.487
0.447
0.619
0.742
0.945
0.988
0.250
0.338
0.506
0.237
0.830
0.972
0.475
0.886
0.917
0.524
0.894
0.993
0.912
0.980
0.998

RanMarkov WinMal

0.900
0.900
0.973
0.738
0.703
0.910
0.738
0.703
0.910
0.738
0.703
0.910
1.000
1.000
1.000
0.996
0.997
0.999
1.000
1.000
1.000
0.986
0.994
0.999
0.850
0.937
0.989
1.000
1.000
1.000

0.759
0.759
0.963
0.812
0.791
0.973
0.855
0.852
0.981
0.873
0.871
0.982
0.914
0.906
0.992
0.882
0.833
0.972
0.914
0.886
0.989
0.919
0.905
0.990
0.911
0.922
0.983
0.927
0.941
0.995

Syscall Oliveira
0.401
0.401
0.755
0.945
0.945
0.988
0.948
0.948
0.992
0.948
0.948
0.992
0.964
0.965
0.995
0.930
0.949
0.990
0.933
0.923
0.990
0.958
0.962
0.995
0.935
0.950
0.991
0.966
0.966
0.996

0.485
0.485
0.571
0.739
0.741
0.811
0.864
0.862
0.907
0.904
0.904
0.970
0.941
0.974
0.973
0.922
0.959
0.955
0.837
0.843
0.819
0.933
0.969
0.965
0.945
0.977
0.974
0.992
0.998
0.997

∗The numbers in each row represent Testing Accuracy, F1-score, and AUC-score, respectively.

TABLE IV
COMPARISON BETWEEN ONE-HOT AND AWGCN LATENT SPACE

Encoding/Embedding

Model

SVM

One-hot

Logistic regression

Random Forest

SVM

AWGCN latent space

Logistic regression

Random Forest

Indicators
0.812
0.791
0.973
0.855
0.852
0.981
0.873
0.871
0.982
0.926
0.927
0.992
0.923
0.924
0.993
0.912
0.912
0.987

sequences). In other words, our designed mechanism should
the alphabet sub-sequence. In Fig. 15, we
only point out
visualize the attention weights on synthetic data. The upper left
is the beginning epoch of our training process, and the right-
hand side is the next epoch. We can easily observe that each
node’s attention weight changes when training. The weight
of ﬁrst epoch is random initialized with uniform distribution;
thus, the alphabet sequence seems to be insigniﬁcant, and the
attention hot spot is on ordinary calls. However, by the training

process moving on, the feature attention hot spot are gradually
transfer to the feature of the sequence. After the whole training
process, the attention weights on the bottom right are focused
on the speciﬁc features of the family (alphabet sequence).
Namely, the attention mechanism of AWGCN is trainable and
can precisely point out the signiﬁcant part of the graph after
training.

In Figure. 16, we demonstrate the real world malware
example in WinMal dataset. The result shows that RegSet-
Value, RegQueryValue, and CreateProcessInternal are more
critical in this malware sample. According to the report [38],
[39], this malware is a worm and named Allaple. The report
said that Allaple modiﬁes the registry to reference a unique
CLSID, in our original malware proﬁle, this malware call
the RegSetValue after create the registry key and activate the
malware.exe stored at Desktop. Furthermore, according to the
second report from Microsoft, this malware is a multi-threaded
worm and it copies itself to the Windows system folder using
the ﬁlename ”urdvxc.exe” or ”irdvxc.exe” and modiﬁes the
registry to load this copy when Windows is started. In our
proﬁles, RegQueryValue query the path mentioned in report
to CreateProcessInternal using the ﬁlename ”urdvxc.exe” to
create another process, as shown in Figure. 12. Moreover, the
call CreateFile calls the rsaenh.dll to encrypt at last which is
pointed out by our attention mechanism. In summary, attention
mechanism can properly ﬁgure out the critical part of the
input API call sequence and the result is similar to the report
provided by antivirus companies.

Fig. 15. The example of the attention transition of synthetic API call sequence.

Fig. 16. The attention result of real world malware sample in WinMal dataset.

In Figure. 17, we show the created multi-threaded process
from malware 3152. They have same hash value with different
sufﬁx identiﬁcation number and they similar behavior as
well. The difference between malware 3152 and 3172 is that
malware 3152 call CreateProcess to create another process for
its following behavior.

D. Representation of Malware Family

In this part, we visualize the graph latent space training
by AWGCN of the WinMal dataset. Because WinMal dataset
has 1,940 samples and is distributed in 14 families, which can
better represent the graph latent space, we use WinMal dataset
as an example to visualize the result.

Fig. 17. The sample of multi-threaded process of Allaple malware in WinMal
dataset.

In Figure. 18, we use t-SNE to do dimension reduction
in order to visualize. The t-SNE result in Figure. 18 is the
same space but in different angle. (You can access the auto
rotation and interaction t-SNE on this link, https://github.
com/ChuPoYu/14family latent space). In Figure. 18, we can
observe that most of the malware family are been separated
to different area in t-SNE. Only the partial samples in Load-
money and Graftor are overlapped (the purple and red dot in
right part of Fig. 18 at the bottom). The main reason is same as
mentioned above, there are some samples with similar action
but in different family label.

In Figure. 19, we perform a self organizing map (SOM)
clustering on our graph latent space vector. Most of the
malware proﬁle are clustered to a proper space, for instance,
the samples in Allaple family (the dot colored in ocean blue at
the bottom-right in Fig. 19). This result show that the samples
in Allaple have similar action; thus, they are clustered to closed
area. In contrast to Allaple, the samples in Loadmoney family
(the dot colored purple) are clustered to two different area.

Fig. 18. The malware family distribution in latent space of WinMal dataset.

Fig. 20. The SOM containing speciﬁc call.

model with other graph representation to further improve the
information preserving from original sequences. Last but not
least, we can perform AWGCN back to the text data to test the
generalization and the correctness of attention in the natural
language processing domain.

VI. CONCLUSION

In this paper, we provide a point of view from graphs to
deal with structural text-based API call sequential data. The
classiﬁcation experiment results show that using our designed
AWGCN to analyze the call-like sequential data outperforms
other methods and properly separates the data points in the
latent space. Also, our method conquers the disadvantage of
sequence alignment in coping with variable length sequences.
Furthermore, we implement the attention mechanism, which
can help us quickly point out the critical component of the
malware.

ACKNOWLEDGMENT

The work is partially supported by the Ministry of Science
and Technology, Taiwan under Grant No. MOST 109-2221-E-
004-007-MY3 and No. MOST 111-2218-E-001-001-MBK.

REFERENCES

[1] C. Beek et al., “McAfee Labs Threats Report August 2019,”
McAfee, Santa Clara, CA, Aug. 2019. Accessed: Jan. 1, 2022.
[Online]. Available: https://www.mcafee.com/enterprise/en-us/assets/
reports/rp-quarterly-threats-aug-2019.pdf

[2] Malware Statistics & Trends Report: AV-TEST. Accessed: Mar. 22,
2020. [Online]. Available: https://www.av-test.org/en/statistics/malware/
[3] S. Alam, R. N. Horspool, I. Traore, and I. Sogukpinar ”A framework for
metamorphic malware analysis and real-time detection,” Comput. Secur.,
vol. 48, pp. 212-233, Feb. 2015.

[4] M. Akbanov, V. G. Vassilakis, and M. D. Logothetis, ”WannaCry
Ransomware Analysis of Infection Persistence Recovery Prevention and
Propagation Mechanisms,” J. Telecommun. Inf. Technol., pp. 113-124,
Apr. 2019, doi: 10.26636/jtit.2019.130218.

[5] H. Sinanovic and S. Mrdovic, ”Analysis of Mirai malicious soft-
ware,” 2017 25th SoftCOM, pp. 1-5, Sep. 2017, doi: 10.23919/SOFT-
COM.2017.8115504.

Fig. 19. The SOM clustering with malware family label.

This indicate Loadmoeny family may have two branches, one
have similar action to Grafter, another one do different actions.
Because we want to know will the used calls affect the
malware family distribution. In Figure. 20, we use different
four keys to visualize the same SOM clustering, including
CopyFile, CreateThread, RegCreateKey, and Network related
calls (e.g. WinHttpConnect and InternetOpen). In Figure. 20,
the malware use network mainly separate to two area, and
they are belong to LoadMoney and Graftor. The malware use
CopyFile are mainly belong to Sytro, which is a Worm family
and can copy and spread themselves. Through Figure. 19 and
Figure. 20 , we can realize what may be the main action of
the malware and which family are they belong to.

V. DISCUSSION

The experiment results show that the proposed AWGCN
achieves strong family classiﬁcation results and learns the
graph embedding in a better
representation. Moreover,
AWGCN can make the inductive prediction for unseen se-
quence samples.

In the future, we can test AWGCN on different datasets
but have similar properties, such as event log data which
is highly repetitive. Furthermore, we can replace Markov

[32] A. Oliveira, ”Malware Analysis Datasets: API Call Sequences”, IEEE

Dataport, Oct. 23, 2019, doi: https://dx.doi.org/10.21227/tqqm-aq14.

[33] ”Text classiﬁcation with an RNN.” https://www.tensorﬂow.org/text/

tutorials/text classiﬁcation rnn (accessed Jun. 26, 2022).

[34] ”Classify text with BERT”. https://www.tensorﬂow.org/text/tutorials/

classify text with bert (accessed Jun. 26, 2022).

[35] ”Cuckoo”. https://cuckoosandbox.org/ (accessed Jun. 28, 2022)
[36] ”Adware.LoadMoney”.

https://blog.malwarebytes.com/detections/

adware-loadmoney/ (accessed Jun. 29, 2022)

[37] ”Adware.Graftor”.

https://blog.malwarebytes.com/detections/

[38] ”WORM

adware-graftor/ (accessed Jun. 29, 2022)
ALLAPLE.IK”.

https://www.trendmicro.com/vinfo/us/
threat-encyclopedia/malware/WORM ALLAPLE.IK/ (accessed Jul. 26,
2022)

[39] ”Worm:Win32/Allaple.A”.

https://www.microsoft.com/en-us/

wdsi/threats/malware-encyclopedia-description?Name=Worm:
Win32/Allaple.A&ThreatID=2147574777 (accessed Jul. 26, 2022)

[6] Y. Pan, X. Ge, C. Fang and Y. Fan, ”A Systematic Literature Review
of Android Malware Detection Using Static Analysis,” in IEEE Access,
vol. 8, pp. 116363-116379, 2020, doi: 10.1109/ACCESS.2020.3002842.
[7] M. Egele, T. Scholte, E. Kirda, and C. Kruegel. ”A survey on automated
dynamic malware-analysis techniques and tools”. ACM Comput. Surv.
vol. 44, pp. 6:1-6:42, Mar. 2008.

[8] R. C. Edgar and S. Batzoglou, ”Multiple sequence alignment,”
Curr. Opin. Struct. Biol., vol. 16, pp. 368-373, Jun, 2006, doi:
10.1016/j.sbi.2006.04.004.

[9] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ”Efﬁcient estimation of
word representations in vector space,” CoRR, vol. abs/1301.3781, 2013.
[10] R. Ronen, M. Radu, C. Feuerstein, E. Yom-Tov, and M. Ahmadi, ”Mi-
crosoft Malware Classiﬁcation Challenge,” 2018, arXiv:1802.10135v1
[cs.CR].

[11] Y. Ki, E. Kim, and H. K. Kim, ”A Novel Approach to De-
tect Malware Based on API Call Sequence Analysis,” Int. J. Dis-
trib. Sens. Netw., vol. 11, no. 6, pp. 659101, Jun. 2015, doi:
https://doi.org/10.1155/2015/659101.

[12] M. K. Shankarapani, S. Ramamoorthy, R. S. Movva, and S. Mukka-
mala, ”Malware detection using assembly and API call sequences,”
J. Comput. Virol., vol. 7, no. 2, pp. 107-119, May. 2011, doi:
https://doi.org/10.1007/s11416-010-0141-5.
(language model)”.
(language model) (accessed Jun. 26, 2022).

https://en.wikipedia.org/wiki/BERT

[13] ”BERT

[14] Q. Le and T. Mikolov, ”Distributed representations of sentences and

documents,” ICML, vol.32, no.2, pp.1188-1196, Jun. 2014.

[15] S. Hochreiter and J. Schmidhuber, ”Long Short-Term Memory,” Nerual

Comput., vol.9, pp.1735-1780, Nov. 1997.

[16] A. Vaswani et al., “Attention is all you need,” in Advances in Neural

Information Processing Systems, 2017, pp. 5998–6008.

[17] ”Transformer (machine learning model)”. https://en.wikipedia.org/wiki/
Transformer (machine learning model) (accessed Jun. 26, 2022).
[18] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ”BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding,” 2019,
arXiv:1810.04805v2 [cs.CL].

[19] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, ”An End-to-End Deep
Learning Architecture for Graph Classiﬁcation,” in AAAI, vol. 32, no.
1, Apr.2018.

[20] J. Bruna, W. Zaremba, A. Szlam, and Y, LeCun, ”Spectral Networks
and Deep Locally Connected Networks on Graphs,” 2014, arXiv:
1312.6203v3 [cs.LG].

[21] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang and P. S. Yu, ”A
Comprehensive Survey on Graph Neural Networks,” in IEEE Trans.
Neural Netw. Learn. Syst., vol. 32, no. 1, pp. 4-24, Jan. 2021, doi:
10.1109/TNNLS.2020.2978386.

[22] T. N. Kipf and M. Welling, ”Semi-Supervised Classiﬁcation with Graph
Convolutional Networks,” presented at ICLR, Toulon, France, Apr. 24-
26, 2017.

[23] P. Velickovic et al., ”Graph Attention Networks,” presented at ICLR,

Vancouver, B.c., Canada, Apr. 30-May. 3, 2018.

[24] W. L. Hamilton, R. Ying, and J. Leskovec, ”Inductive Representation
Learning on Large Graphs,” presented at NIPS, Long Beach, CA, USA,
Dec. 4-9, 2017.

[25] R. Martin, ”API Design for C++,” 1st ed. Netherlands: Elsevier, 2011.
[26] J. Wulf and I. Blohm, ”Fostering value creation with digital platforms:
A uniﬁed theory of the application programming interface design,”
Manag.
Inf. Syst., vol. 37, no. 1, pp. 251-281, Mar. 2020, doi:
10.1080/07421222.2019.1705514.

[27] E. Amer and I. Zelinka, ”A dynamic Windows malware detection
and prediction method based on contextual understanding of API call
sequence,” Comput. Secur., vol. 92, May. 2020, Art no. 101760.
[28] M. Alazab, S. Venkatraman, P. Watters, and M. Alazab, ”Zero-day
Malware Detection based on Supervised Learning Algorithms of API
call Signatures,” in Proc. AusDM., Dec. 1-2, 2011, pp. 171-181.
[29] ”Markov chain”. https://en.wikipedia.org/wiki/Markov chain (accessed

Jun. 26, 2022).

[30] National Center for High-performance Computing(NCHC) and Tai-
wan Computer Security Incident Response Team(TWCSIRT). ”Malware
Knowledge Base”. https://owl.nchc.org.tw/about.php (accessed May. 22,
2022).

[31] S. Hsiao and Y. Lee, ”NN-Based Feature Selection for Text-Based
Sequential Data”. PACIS 2020 Proceedings., pp. 238, Jun, 2020
https://aisel.aisnet.org/pacis2020/238.

