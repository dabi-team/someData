2
2
0
2

l
u
J

4
1

]
E
S
.
s
c
[

1
v
0
9
5
6
0
.
7
0
2
2
:
v
i
X
r
a

Attention: Not Just Another Dataset for
Patch-Correctness Checking

Yuehan Wang*
Department of Computer Science
University of Illinois at
Urbana-Champaign
USA, IL
yuehanw2@illinois.edu

Jun Yang*
Department of Computer Science
University of Illinois at
Urbana-Champaign
USA, IL
jy70@illinois.edu

Yiling Lou
Department of Computer Science
Purdue University
USA, IN
louyiling@pku.edu.cn

Ming Wen
School of Cyber Science and
Engineering
Huazhong University of Science and
Technology
China, Wuhan
mwenaa@hust.edu.cn

Lingming Zhang
Department of Computer Science
University of Illinois at
Urbana-Champaign
USA, IL
lingming@illinois.edu

ABSTRACT
Automated Program Repair (APR) techniques have drawn wide
attention from both academia and industry. Meanwhile, one main
limitation with the current state-of-the-art APR tools is that patches
passing all the original tests are not necessarily the correct ones
wanted by developers, i.e., the plausible patch problem. To date,
various Patch-Correctness Checking (PCC) techniques have been
proposed to address this important issue. However, they are only
evaluated on very limited datasets as the APR tools used for gen-
erating such patches can only explore a small subset of the search
space of possible patches, posing serious threats to external validity
to existing PCC studies. In this paper, we construct an extensive
PCC dataset (the largest labeled PCC dataset to our knowledge)
to revisit all state-of-the-art PCC techniques. More specifically,
our PCC dataset includes 1,988 patches generated from the recent
PraPR APR tool, which leverages highly-optimized bytecode-level
patch executions and can exhaustively explore all possible plausible
patches within its large predefined search space (including well-
known fixing patterns from various prior APR tools). Our extensive
study of representative PCC techniques on the new dataset has
revealed various surprising findings, including: 1) the assumption
made by existing static PCC techniques that correct patches are
more similar to buggy code than incorrect plausible patches no
longer holds, 2) state-of-the-art learning-based techniques tend to
suffer from the dataset overfitting problem, and 3) while dynamic
techniques overall retain their effectiveness on our new dataset,
their performance drops substantially on patches with more com-
plicated changes. Based on our findings, we also provide various
guidelines/suggestions for advancing PCC in the near future.

1 INTRODUCTION
Automated Program Repair (APR) [19, 26, 46, 78] aims to directly
fix software bugs with minimal human intervention and thus can

*The first two authors contributed equally to this work

substantially speed up software development. In recent years, vari-
ous APR techniques have been proposed and extensively studied
in academia [23, 28, 37, 39, 57, 75]; furthermore, APR techniques
have also drawn wide attention from the industry (e.g., Facebook
[44], Alibaba [11, 41], and Fujitsu [54, 55]). A typical APR technique
first generates various candidate patches based on different strate-
gies, such as template-based [28, 36, 37], heuristic-based [26, 78],
constraint-based [19, 46, 70] and learning-based [14, 35, 42] ones;
then, it validates all the candidate patches via software testing [23,
26, 42], static analysis [61], or even formal verification [13]. To date,
most APR systems leverage software testing for patch validation
due to the popularity and effectiveness of testing in practice.

Although test-based patch validation has been shown to be prac-
tical for real-world systems, it suffers from the test overfitting issue
– patches passing all the tests (i.e., plausible patches) may not al-
ways be semantically equivalent to the corresponding developer
patches [52]. The reason is that software tests can hardly cover all
possible program behaviors for real-world systems. Therefore, it is
usually recommended that the developers should manually inspect
the plausible patches to find the final correct ones. However, such
manual inspection can be extremely challenging and time consum-
ing given the large number of potentially plausible patches and
code complexity for real-world systems [23]. To relieve the burden
on developers, various techniques have been proposed to automate
Patch-Correctness Checking (PCC) – while static techniques aim to
infer patch correctness based on analyzing the patched code snip-
pets [30, 64, 66], dynamic techniques rely on runtime information
collected for the original and patched code versions to determine
patch correctness [69, 72]. In recent years, researchers have also
leveraged the recent advances in deep code embedding for PCC [60].
While existing PCC techniques have shown promising results,
they are mostly studied on limited PCC datasets since the APR tools
used for generating patches for such datasets can only explore a very
small portion of the possible patch search space due to the following
reasons. First, the leveraged APR tools have been shown to be
overfitting to the studied subjects, e.g., Defects4J V1.2 [18, 23]. Such

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

APR tools usually apply very aggressive patch pruning strategies
to fix more bugs on the studied subjects, but may fail to fix any
bugs on other subjects, e.g., the recent SimFix/CapGen tools [1, 64]
widely adopted in existing PCC datasets can only fix 0/2 more bugs
on a newer version of Defects4J with ∼200 more bugs [23]. In this
way, the possible plausible patches that should be generated are
also missed by such APR tools, and thus absent in the resulting PCC
dataset. Second, due to the efficiency issue, most studied APR tools
(e.g., SimFix) terminate patch exploration as soon as they find the
first plausible patch. In this way, a large number of possible plausible
patches can also be missed by such techniques. For example, even
including 21 APR tools, the current largest labeled PCC dataset [62]
only has 902 patches for Defects4J V1.2 in total. Third, most of the
PCC datasets are built on the popular Defects4J V1.2, making it
unclear whether the findings can generalize to other subjects. As
a result, it is totally unclear whether the promising experimental
results of existing PCC techniques can generalize to more realistic
datasets and real-world software development.

In this paper, we aim to revisit existing state-of-the-art PCC
techniques with a more extensive and real-world dataset to more
faithfully evaluate their effectiveness in practice. To this end, we
first construct an extensive dataset for PCC based on the recent
PraPR repair system [23]. We choose PraPR given the following
reasons: (1) its predefined patch search space is large since it ap-
plies popular well-known fixing templates from various prior APR
work [4, 8, 50, 56], (2) it is the only available APR tool (to our knowl-
edge) that can exhaustively explore the entire predefined patch search
space since it generates and validates patches based on its highly
optimized on-the-fly bytecode manipulation. In this way, PraPR
can generate all plausible patches within its clearly defined patch
search space, which can largely avoid the dataset overfitting is-
sue [23] and can work as an ideal candidate for constructing PCC
datasets. We apply PraPR to both the widely studied Defects4J V1.2
dataset [27] with 395 bugs and the latest Defects4J V2.0 dataset
with 11 additional subject systems and 401 additional bugs. In total,
our dataset contains 1,988 plausible patches, including 83 correct
patches and 1,905 overfitting patches after manual labeling. To
our knowledge, this is the largest labeled dataset for PCC. Then,
based on our new datasets, we perform an extensive study of prior
PCC techniques, including state-of-the-art static [30, 59, 64, 66],
dynamic [69, 72], and learning-based [60] PCC techniques. Our
empirical study reveals various surprising findings that the commu-
nity should be aware of: 1) the assumption made by existing static
PCC techniques that correct patches are more similar to buggy code
than incorrect plausible patches no longer holds, 2) state-of-the-art
learning-based techniques tend to suffer from the dataset overfit-
ting problem and 3) while dynamic techniques overall retain their
effectiveness on our new dataset, their performance drops substan-
tially when encountering patches with more complicated changes.
Based on our experimental findings, we also provide various guide-
lines/suggestions for further advancing PCC techniques in the near
future. Overall, this paper makes the following contributions:
• Real-world Dataset. We create a large-scale realistic dataset
for PCC. To our knowledge, this is the first dataset based on
exhaustive patch search space exploration, as well as the largest
labeled patch dataset for PCC to date.

• Extensive Study. We perform an extensive study of state-of-
the-art PCC techniques, including static, dynamic, and learning-
based ones, on our newly constructed dataset. In total, our ex-
periments cost 580 CPU days, including static analysis, code
embedding, test generation, and test execution for all patches
within our dataset.

• Practical Impacts. Our empirical study reveals various rather
surprising results that the community should be aware of, as
well as providing various practical guidelines for future PCC
work, e.g., the current dynamic PCC techniques can no longer
work for future advanced APR techniques capable of fixing more
real-world bugs, and should be largely improved. Our replica-
tion package is available at https://github.com/claudeyj/
patch_correctness_assessment [6].

2 BACKGROUND AND RELATED WORK
In this section, we first introduce the background on automated
program repair and patch correctness checking, and then motivate
our study by systematically revisiting datasets used in the literature.
Automated Program Repair. Automated Program Repair(APR) [19,
26, 28, 37, 46, 57, 75, 78] aims at fixing bugs with minimal human
intervention. Due to its promising future, to date, researchers have
proposed various APR techniques, which can be categorized accord-
ing to how patches are generated: (1) Heuristic-based APR [34, 51, 77]
leverages heuristics (e.g., random searching [51] or genetic program-
ming [34]) to explore the search space of patches; (2) Template-based
APR [28, 36, 37] incorporates patterns summarized from histori-
cal developer patches to guide patch generation; (3) Constraint-
based APR [19, 46, 70] leverages symbolic execution and constraint
solving to directly synthesize program patches; (4) Learning-based
APR [14, 35, 42] utilizes advanced learning techniques (e.g., neural
machine translation [10]) to generate patches based on existing
fixes. APR tools then leverage software testing [23, 26, 42], static
analysis [61], or even formal verification [13] to validate the patches.
Patch Correctness Checking. Most APR techniques leverage
software tests as the specifications for patch correctness, regarding
the plausible patches that can pass all tests as potentially correct.
However, such an assumption can be problematic, since tests often
fail to detect all possible bugs. Therefore, a plausible patch may be
overfitting to the existing tests and yet be incorrect. To address such
test-overfitting issue in APR, various Patch Correctness Checking
(PCC) [32] techniques have been proposed to differentiate overfit-
ting patches from correct patches.

There are two application scenarios for PCC techniques accord-
ing to whether oracle patch information is required. In the oracle-
based scenario, the plausible patches that exhibit different behaviors
from the oracle patch are deemed as overfitting. For example, ex-
isting oracle-based PCC techniques often generate new tests (e.g.,
Evosuite [22] or Randoop [49]) or invariants (e.g., Daikon [21]) to
characterize run-time behaviors of the oracle and plausible patches.
Note that oracle-based PCC is usually used for APR experimenta-
tion, and cannot be applied to real-world bug fixing where the oracle
patch information is unavailable. In contrast, oracle-absent PCC
can identify potential overfitting patches without oracle patches,
and thus can be applied in real-world bug fixing. Therefore, in this
work, we also focus on the oracle-absent PCC techniques.

Attention: Not Just Another Dataset for
Patch-Correctness Checking

According to whether dynamic patch execution information is
required, traditional PCC techniques can be categorized into static
and dynamic techniques. In particular, static techniques leverage
static code features [30, 64, 66] or pre-defined anti-patterns [59]
to differentiate overfitting patches from correct patches. Dynamic
techniques leverage run-time information, such as crash or memory-
safety issues used in Opad [72] and test execution traces used in
Patch-Sim [69]. Recently, researchers have also started to explore
advanced machine learning and deep learning techniques for PCC.
Seminal research [73] characterizes patch code with pre-defined
features, while more recent work [15, 60] directly encodes patch
code via embedding models (e.g., BERT [17] and Doc2Vec [29]) and
learns to predict the probability of each patch being overfitting.
Existing PCC Datasets. We revisit the existing peer-refereed lit-
erature on automated program repair and PCC based on the APR
living review [48] before 2022, and summarize the datasets used
in their evaluation. Table 1 presents the characteristics of datasets
used in existing work. In particular, Column “Studied technique”
presents the PCC techniques proposed/studied in each work. Col-
umn “Scale” presents the total number of patches; Column “Ratio”
presents the ratio between correct patches and overfitting patches
in each dataset. We further show how patches are collected in
Column “Dataset source”, including the subjects, the number of
developer patches or APR-generated patches. In addition, for the
patches generated by APR tools, we further check whether the
early stop mechanism is enabled during APR procedure as shown
in Column “Early stop”. From the table, we can find that although
involving various APR tools, subjects, and patches, existing PCC
datasets can still be insufficient for evaluating PCC, since they can
only explore a very small portion of the possible patch search space
due to the following reasons.

First, existing datasets mainly include the plausible patches gener-
ated by overfitting APR tools. As shown in recent work [18, 23], most
existing APR tools are overfitting to the bug benchmark Defects4J
V1.2 [27]. Such APR tools often leverage aggressive and specific
patch pruning strategies so as to fix more bugs on the studied sub-
jects, but may suffer from the dataset-overfitting issue [58]. For
example, the recent SimFix/CapGen tools [26, 64] can only fix 0/2
more bugs on additional ∼200 more bugs [23]; also, 11 APR tools
have been shown to perform substantially worse on other bench-
marks than the widely-used Defects4J V1.2 [18]. In this way, the
possible plausible patches that should be generated are missed by
such APR tools, and thus absent in the resulting PCC dataset.

Second, existing datasets only include the first few plausible patches
generated by APR tools due to the efficiency issue. APR tools (e.g.,
SimFix [26]) enable early stop mechanism, terminating patch exe-
cution as soon as they find the first plausible patch for the sake of
efficiency. As shown in the table, most APR tools leveraged in exist-
ing PCC datasets enable early stop. In this way, existing PCC dataset
may miss a large number of possible plausible patches that can be
potentially generated by these APR tools. In fact, even including 21
APR tools, the existing largest labeled PCC dataset [62] has only
902 plausible patches for Defects4J V1.2; meanwhile a single APR
(i.e., SequenceR [14]) can only generate at most 73 patches.

Third, most PCC datasets are built on the popular Defects4J V1.2,
making it unclear whether the findings can generalize to other subjects.
As shown in the table, the majority of existing PCC datasets are

Conference’17, July 2017, Washington, DC, USA

generated from the subjects in the most widely used benchmark
Defects4J V1.2. Therefore, it remains unknown how existing PCC
techniques perform on patches from other subjects.

To address the limitations above, in this work, we construct a
more extensive and real-world dataset so as to revisit all existing
PCC techniques. In particular, our dataset is built on the plausible
patches generated by the recent byte-code level APR tool PraPR [23].
We choose PraPR since 1) its predefined patch search space is large
since it applies popular well-known fixing templates from various
prior APR work [4, 8, 50, 56], and 2) it is the only available APR tool
(to our knowledge) that can exhaustively explore the predefined
patch search space due to its highly optimized on-the-fly bytecode
manipulation. In this way, PraPR can generate all plausible patches
within the clearly defined patch search space, which other APR
tools fail to reach. Furthermore, in addition to the most widely
used Defects4J V1.2 dataset [27], we further apply PraPR to the
most recent version of Defects4J dataset, i.e., Defects4J V2.0 with
11 additional subject systems and 401 additional bugs.
3 DATASET CONSTRUCTION
3.1 Subjects
Our datasets are constructed with patches generated for the subjects
in Defects4J [27] due to the following reasons: 1) Defects4J contains
hundreds of real bugs for real-world projects and has become the
most widely studied APR benchmark in the literature, 2) most prior
PCC studies were performed on Defects4J [27, 60, 62, 65, 69, 74],
thus enabling a more direct/fair comparison with prior work. We
include all subjects from Defects4J V1.2 (with 395 bugs from 6
projects) since it is the most widely used Defects4J version in prior
APR and PCC work [27, 60, 62, 64–66, 69, 74]. Furthermore, we
also study the latest Defects4J V2.0, which includes 11 additional
projects with 401 additional bugs, to study the dataset overfitting
issue of existing PCC work, i.e., whether the prior PCC experimental
results can generalize to the newer Defects4J version. Please refer
to our website [6] for detailed statistics of Defects4J versions.
3.2 Patch Collection
Plausible Patch Collection. For each studied buggy version from
Defects4J, we first run PraPR on it to generate all possible plausible
patches (i.e., the patches passing all the tests). Since PraPR achieves
high-speed program repair via on-the-fly bytecode manipulation,
the resuling plausible patches are in the bytecode level. We further
decompile these bytecode-level plausible patches into source-code-
level patches since existing PCC techniques work at the source-code
level. We make the following efforts to ensure the decompilation
process as precise as possible. First, we configure PraPR to include
all the required debugging information (including line numbers,
variable names, and source-file names) in the resulting bytecode-
level patches. Second, we leverage the state-of-the-art decompiler
JD-Core [2] to decompile patched bytecode files. Third, we only
locate the patched line in the decompiled file1, and patch this line
into the original buggy source file to obtain a potential patch at
the source-code level. For the cases with format/compilation issues
during decompilation, we manually fix them based on the patch
location and mutator information in the PraPR report [5]. Finally,
we perform the sanity check to further ensure the decompiled

1Note that PraPR only changes one line on bytecode level patches.

Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

Table 1: Datasets used in existing PCC work

Work

Studied technique Scale

Ratio
(correct/overfitting)

Subject source

Tan et al. [59]

Anti-patterns

289

30/259

CoREBench [12], GenProg [34]

Xin et al. [65]

DiffTGen

Le et al. [30]

Yang et al. [72]

Xin et al. [66]

S3

Opad

ssFix

Xiong et al. [69]

Patch-Sim

Ye et al. [74]

RGT

Wen et al. [64]

CapGen [64]

Xin et al. [67]

sharpFix [68]

89

85

449

153

139

638

202

82

10/79

25/60

22/427

122/31

29/110

257/381

28/174

56/26

Defects4J V1.2

100 bugs from 62 subjects

45 bugs from 7 subjects

Defects4J V1.2
exclude Mockito
Defects4J V1.2
exclude Closure, Mockito

Defects4J V1.2

Defects4J V1.2
exclude Closure, Mockito
Bugs.jar
127 bugs from 7 projects in Bugs.jar

Tian et al. [60]

Embedding-based

1,000

468/532

Defects4J V1.2

Wang et al. [62]

12 PCC techniques

902

248/654

Defects4J V1.2

Dataset source

Patch source
All from APR tools
GenProg, mGenProg [59], SPR [40], mSPR [59]
All from APR tools
jGenProg, jKali, NPol and HDRepair
All from APR tools
S3, Enumerative [53], CVC4 [53], Angelix [47]
All from APR tools
GenProg, AE [63], Kali[52], and SPR
All from APR tools
ssFix, jGenProg, jKali [45], Nopol, HDRepair [31], ACS
All from APR tools
jGenPro [45], Nopol [71], jKali, ACS [70], HDRepair
All from APR tools
ACS, Arja, CapGen, etc., 14 tools in total
All from APR tools
CapGen
All from APR tools
sharpFix, ssFix [66]
778 from APR tools
RSRepair-A [77], jKali, ACS, SimFix [26], TBar [38], etc., 17 tools in total
232 from developer patches (*Supplemented to balance the dataset)
All from APR tools
jGenProg, DynaMoth [20], SequencR [14], etc., 21 tools in total

Early stop

4/4

4/4

4/4

4/4

5/5

5/5

10/14

0/1

2/2

12/17

16/21

Table 2: Our patch datasets

Subjects
Dataset ID
𝑃𝑟𝑎𝑃𝑅𝑣1.2
Defects4J V1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
Defects4J V2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
Defects4J V2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0 Defects4J V2.0

APR Tools
PraPR
PraPR
PraPR + 21 tools
PraPR + 21 tools

#Patch
1,311
1,988
2,760
542

#Overfit
1,264
1,905
2,489
271

#Correct
47
83
271
271

source-code-level patches indeed pass all the tests. In total, 332 out
of 2,320 plausible bytecode-level patches have been filtered out, i.e.,
we finally obtain 1,988 plausible patches at the source-code level.
Patch Correctness Identification. For each plausible patch, we
then manually identify its correctness based on whether it is seman-
tically equivalent to the developer patch. In particular, we follow
the labeling procedure in previous work [39] and the patches that
satisfy the following criteria are labeled as correct: (1) the patches
that are syntactically identical to the developer patches, or (2) the
patches that are semantically equivalent to the developer patches
according to the rules summarized from existing work [39]. On the
contrary, the patches that cannot satisfy these criteria are labeled
as overfitting. We involve multiple participants with over-years
Java development experience in the manual labelling procedure:
two participants first label each plausible patch individually, and
a third participant would be introduced to resolve the conflicts by
discussing based on the corresponding bug report or issue link. In
total, among all the 1,988 plausible patches newly collected in this
work, 83 are correct and the remaining 1,905 are overfitting ones.

3.3 New Patch Datasets
The plausible patches newly collected above, constitute our main
dataset 𝑃𝑟𝑎𝑃𝑅𝑣2.0, which contains the 1,988 plausible patches gen-
erated by PraPR on Defects4J V2.0. Based on the main dataset, we
further construct different sub-datasets for more thorough study of
PCC work. Table 2 presents the details of all our datasets.

𝑃𝑟𝑎𝑃𝑅𝑣1.2. This dataset includes all the plausible patches gener-
ated by PraPR on Defects4J V1.2, including 1,311 plausible patches
in total. We separate this out of the main dataset to directly compare
with prior PCC studies on the same Defects4J version.

𝑀𝑒𝑟𝑔𝑒𝑣2.0. Although PraPR contains overlapping patch fixing
patterns with many other APR tools, it is still important to consider
patches generated by other APR tools for more comprehensive
PCC evaluation. Therefore, we further combine our 𝑃𝑟𝑎𝑃𝑅𝑣2.0 with
the existing largest labeled dataset derived from APR tools, i.e.,
𝑊 𝑎𝑛𝑔𝑣1.2 [62] that includes 902 plausible patches generated by 21
other existing APR tools. After carefully removing the duplicated
patches between 𝑊 𝑎𝑛𝑔𝑣1.2 and our dataset, we finally obtain 2,763
plausible patches in this merged dataset.

𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0. As shown in Table 2, the 𝑀𝑒𝑟𝑔𝑒𝑣2.0 dataset is largely
imbalanced, with the majority being overfitting patches. While PCC
studies frequently leverage imbalanced datasets (since current APR
tools cannot generate correct patches for many bugs), we further
construct a balanced dataset based on 𝑀𝑒𝑟𝑔𝑒𝑣2.0 for more thorough
PCC evaluations. More specifically, we keep all correct patches from
𝑀𝑒𝑟𝑔𝑒𝑣2.0 and randomly sample overfitting patches of the same
number as the correct ones. To mitigate the bias in randomness,
we repeat the random sampling process ten times and present the
average results on 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0.

To our knowledge, the datasets newly constructed in this work
are the largest labeled datasets for evaluating PCC. For example,
PraPR alone generates 1,988 plausible patches while the existing
largest PCC dataset includes only 902 plausible patches that are
generated from 21 APR tools. In addition, merging the patches
generated by PraPR with the existing datasets could further result
in a larger labeled dataset (i.e., 𝑀𝑒𝑟𝑔𝑒𝑣2.0). The large scale of our
new datasets is credited to the high efficiency and exhaustive search
strategies employed by PraPR, which help collect many plausible
patches that are missed by the other APR tools due to their efficiency
issues or their early termination after finding the first plausible
patch. For example, PraPR alone generates 39 unique plausible patches
for the Defects4J bug Math-28, while the other existing 21 APR tools
generate seven plausible patches in total (five of them are unique). The
main reason could be that other tools early stop after finding the
first plausible patch while PraPR exhaustively explores the whole
search space. Therefore, at least 34 out of the 39 plausible patches
generated by PraPR are not visited by the existing 21 APR tools.

Attention: Not Just Another Dataset for
Patch-Correctness Checking

4 STUDY DESIGN
4.1 Research Questions
Based on our newly constructed datasets, we aim to revisit all state-
of-the-art PCC techniques from different categories. To this end,
we aim to answer the following research questions:
• RQ1: How do static PCC techniques perform on our datasets?
• RQ2: How does learning-based PCC work perform on our datasets?
• RQ3: How do dynamic PCC techniques perform on our datasets?

4.2 Studied Techniques
Our study selects existing state-of-the-art techniques that are de-
signed for or can be adapted to the PCC task. Specifically, the se-
lected techniques can be broadly categorized into three categories,
including static, dynamic, and learning-based techniques. We only
include those techniques that do not require the oracle informa-
tion (i.e., the information of developer patches) since the practical
usefulness of those techniques requiring the oracle information is
significantly compromised [62].

Static Techniques. Wang et al. has empirically investigated

4.2.1
the effectiveness of static features extracted from three static tools [62],
namely ssFix [66], S3 [30] and CapGen [64], and then utilized such
features to check patch correctness. We follow their experiment
settings and adopt the same methodology to assess the correctness
of patches in our new dataset.

S3: S3 proposed six features to measure the syntactic and seman-
tic distance between patched code and the original buggy code [33].
Among them, AST differencing and cosine similarity are utilized
to prioritize and identify correct patches. Specifically, the sum of
these features is computed and further used as the suspiciousness
score. Patches with higher scores are deemed to be more likely to
be overfitting according to the previous study [9] which claims cor-
rect patches are more similar to the original buggy code, and thus
possess fewer modifications. In addition, we exclude the other three
features in this study following prior work [62]. Specifically, model
counting and output coverage are excluded since they cannot be
generalized to all the generated patches [62]. Besides, Anti-patterns
utilized in S3 is also excluded here since we utilize it as a stand-alone
tool in this study, following the previous study [62].

ssFix: ssFix focused on the token-based syntax representation
of code to identify syntax-related code fragments to generate cor-
rect patches. Specifically, structural token similarity and conceptual
token similarity are calculated to measure the similarity between
the buggy code and patched code. Similar to S3, the sum of these
features is used to rank patches. Patches with higher scores are
deemed more likely to be correct.

CapGen: CapGen designed three context-aware features to pri-
oritize correct patches over overfitting ones, namely the genealogy
similarity, variable similarity, and dependency similarity respec-
tively. Although such features are not initially proposed for check-
ing patch correctness, they can still be adapted to assess patch
correctness from the view of static features. Specifically, following
the existing study [62], the product of these similarity scores is
used to rank and prioritize patches, and patches with higher scores
are more likely to be correct. We follow previous studies and rank
patches according to the scores calculated by the above-mentioned
features.

Conference’17, July 2017, Washington, DC, USA

For the above three static techniques, we apply the Top-N strat-
egy to identify correct patches following previous work [62]. Specif-
ically, the Top-N prioritized patches are labeled as correct while
other patches as overfitting, where N is the number of correct
patches in our dataset following existing studies [62]. To mitigate
the effect of N, we also present the average ranking of the correct
patches per bug (i.e., denoted as AVR) to evaluate the ability to
prioritize correct patches (details shown in Section 4.3).

Anti-patterns: Anti-patterns provides a set of generic forbid-
den transformations, which can be applied on search-based repair
tools [59]. It has been shown that Anti-patterns could help obtain
program patches with higher qualities with minimal effort. To ap-
ply these Anti-patterns on our own dataset, we first map the Anti-
patterns to PraPR’s mutation rules [23] through manual inspection.
If the rules fall into a specific pattern, we deem those patches gen-
erated by the mutation rule as overfitting. Such a strategy is the
same as that adopted by the existing study [62]. In this way, we
observe that patches generated by PraPR mainly fall into four of
the seven anti-patterns, including A1: Anti-delete CFG Exit Node,
A2: Anti-delete Control Statement, A5: Anti-delete Loop-Counter and
A6: Anti-append Early Exit. Therefore, we classify the patches that
fall into these rules as overfitting and the others as correct.
4.2.2 Dynamic Techniques. Dynamic techniques are designed based
on the testing outcomes or the differences between the testing run-
time behaviors to check whether a patch is correct. Specifically,
we focus on state-of-the-art dynamic techniques widely studied in
the literature [62, 69, 72]:

Opad: Opad is designed based on the hypothesis that patches
should not introduce new crash or memory-safety problems [72].
Therefore, any patch violating those rules is regarded as overfit-
ting. Note that Opad is originally designed for C programs utilizing
fuzzing techniques to generate new test cases. To adapt it on Java,
we leverage two state-of-the-art test generation tools, i.e., Evo-
suite [22] and Randoop [49], to generate test cases based on the
buggy version. Opad based on Evosuite and Randoop are denoted as
E-Opad and R-Opad respectively, following the previous study [62].
Specifically, for Evosuite and Randoop, we generate 30 test suites
for each buggy program with a time budget of 600 seconds for each
test suite utilizing existing test generation module in Defects4J
framework. We finally successfully generate 44,937 test suites on
796 buggy programs in total. After test generation, we first run the
generated tests on the original buggy programs to remove flaky
tests. Specifically, any test that throws exceptions on original buggy
programs is regarded as a flaky test and removed. After that, we run
the remaining ones on the patched programs. If any crash occurs or
any exception is thrown, Opad will identify the patch as overfitting,
otherwise it will identify the patch as correct.

Patch-Sim: Patch-Sim is a similarity-based PCC tool, which
utilizes the tests generated by automated test generation tools (Ran-
doop in the original study [69] and both Randoop and Evosuite in
Wang et al.’s study [62]) as the test inputs. It assumes that tests
with similar executions are likely to have similar results. Therefore,
given that a correct patch may behave similarly on the passing
tests and differently on the failing ones compared with the buggy
program, Patch-Sim could assess patch correctness. Note that in the
original study, Xiong et al. used Randoop to generate test suites [69].
Besides, according to Wang et al.’s study [62], the tests generated

Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

by Randoop perform better than those by Evosuite. We, thus, chose
Randoop as the test generation tool in this study. To apply Patch-
Sim on 𝑃𝑟𝑎𝑃𝑅𝑣1.2, we generated 6,570 test cases for related projects
in total by ourselves. Combining with the test suites generated by
the existing study [62], we obtain 10,404 test cases for Patch-Sim.

Learning-based Techniques. Besides the traditional static and
4.2.3
dynamic PCC tools, we also include a novel technique for com-
parison: the learning-based approaches with different embedding
techniques (hereafter referred to as Embedding), in our study.

Embedding: Code Embedding is the technique that can transfer
source code into distributed representations in the form of fixed-
length vectors. Consequently, various software engineering tasks
can take advantage of such distributed representations by utiliz-
ing supervised machine learning algorithms. Tian et al. initially
proposed utilizing code embedding techniques to identify correct
patches among plausible ones [60]. They applied the pre-trained
models of BERT [17], CC2Vec [24], as well as a retrained model
of Doc2Vec [29], on their collected patch dataset, to investigate
the effectiveness of different code embedding techniques in PCC.
Specifically, they integrate those patches generated by Liu et al. and
the developer patches, and used them to train the embedding model.
They then evaluate the model’s effectiveness against those patches
collected by another study [69]. Their results reveal that the model
trained using Logistic Regression (LR) based on the embedding
learned by BERT is able to achieve the optimum performance in
patch correctness prediction. There are also other techniques utiliz-
ing embedding to classify patches[15, 16], but they mostly follow
highly similar ideas and cannot outperform Tian et al. ’s work [60].
Therefore, we utilize the best-performing model in Tian et al. ’s
study, denoted as BERT-LR, as the representative in this study for
further investigation. Specifically, we applied the model learned by
the BERT-LR method to our dataset, to investigate how it performs.
Following the experimental setting as adopted by the existing study,
we utilize the dataset used in [60] for training and our collected
benchmark for testing. Note that we filtered out those overlapped
patches between Tian’s and our datasets for the training process.
4.3 Metrics
Following the recent works on patch correctness checking [60, 62,
69, 72], we evaluate PCC techniques against the following metrics:
• TP: # of truly overfitting patches identified as overfitting.
• TN: # of truly correct patches identified as correct.
• FP: # of truly correct patches identified as overfitting.
• FN: # of truly overfitting patches identified as correct.
• 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇 𝑃/(𝑇 𝑃 + 𝐹 𝑃).
• 𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑇 𝑃/(𝑇 𝑃 + 𝐹 𝑁 ).
• 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 = 𝑇 𝑁 /(𝑇 𝑁 + 𝐹 𝑃).
• AUC: the Area Under the ROC Curve [25].
• AVR: the AVerage Ranking of correct patches for bugs which

have both overfitting and correct patches.

As suggested by previous studies [62, 69, 73, 76], PCC techniques
should not mistakenly exclude correct patches, i.e., the FP should
be as low as possible. However, when the dataset is imbalanced
(i.e., the number of overfitting patches is much larger than cor-
rect patches), 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 can be biased and overrated since 𝐹 𝑃 is
often much smaller than 𝑇 𝑃, thus posing significant effects to the
final results. Therefore, following the previous work on imbalanced

datasets [69], we further include the metric 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 , which calcu-
lates the ratio of correct patches that are also identified as correct
for complementary fair comparisons. In fact, 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 is crucial since
rejecting a correct patch may cause a bug to be unfixed. Besides,
we also utilize the AUC metric [25] since it is able to eliminate
the effects caused by imbalanced datasets. Note that AUC is not
applicable for rule-based techniques including Opad, Patch-Sim and
Anti-patterns since their results are discrete (i.e., either 0 or 1). For
the static techniques (S3, CapGen, and ssFix) based on feature score
ranking, we leverage AVR to show their average ranking of correct
patches, i.e., the smaller ranking is better. For example, given AVR
a(b), a is the average ranking of the first correct patch in each bug
while b is the average number of patches in each bug. In summary,
an effective PCC technique should achieve high 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛, 𝑅𝑒𝑐𝑎𝑙𝑙,
𝑅𝑒𝑐𝑎𝑙𝑙𝑐 , and AUC, while small AVR.
4.4 Threats to Validity
The threats to internal validity mainly lie in the implementation of
studied PCC techniques. To reduce such threats, we reuse existing
implementations whenever possible, and have carefully reviewed
all our code and script. In addition, to mitigate the bias in the
patch correctness labeling, we involve multiple participants, follow
widely-used criteria summarized from existing work [39], and have
released all our patches for public review. To mitigate the potential
bias of randomness in sampling, we repeat the sampling process for
ten times and use the average results. The threats to external validity
are mainly concerned with the generalizability of our findings. To
reduce such threats, we have constructed the largest PCC dataset
(to our knowledge), and also included a newer version of the widely
Defects4J V1.2 dataset with 400+ more bugs. The threats to construct
validity mainly lie in the metrics used in our study. To mitigate such
threats, we have included all popular metrics for PCC, including
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛, 𝑅𝑒𝑐𝑎𝑙𝑙, 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 , and AUC.

5 RESULT ANALYSIS
5.1 RQ1: Static Techniques

Static Code features. For studying all the static techniques,
5.1.1
we follow the same experimental setting as the recent study [62].
The detailed experimental results for the three studied techniques
are shown in Tables 3, 4 and 5, respectively. From the tables, we can
observe that compared with original scores on𝑊 𝑎𝑛𝑔𝑣1.2, the 𝑅𝑒𝑐𝑎𝑙𝑙
scores all increase while the 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 scores all substantially decrease
on the 𝑃𝑟𝑎𝑃𝑅𝑣1.2 and 𝑃𝑟𝑎𝑃𝑅𝑣2.0 datasets. One direct reason is that
the two new datasets are extremely imbalanced (overfitting patches
are far more than correct patches). Meanwhile, the extremely low
𝑅𝑒𝑐𝑎𝑙𝑙𝑐 scores (i.e., 5%-20% of correct patches are identified as cor-
rect) still demonstrate that such techniques can hardly be useful
in practice. The AUC scores for 𝑃𝑟𝑎𝑃𝑅𝑣2.0 are all below 50%, also
demonstrating that all three static techniques perform even worse
than a random classification model (with 50% AUC) [7] on our main
dataset. The experimental results on the more balanced 𝑀𝑒𝑟𝑔𝑒𝑣2.0
and 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0 datasets further confirm this observation. For ex-
ample, on the balanced dataset 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0, the 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 remains
similar to the prior study on 𝑊 𝑎𝑛𝑔𝑣1.2 [62] (i.e., ∼45%), while the
𝑅𝑒𝑐𝑎𝑙𝑙 substantially drops over 30 percentage points for all three
static techniques (e.g., from 78.75% to 46.49% for ssFix). The AUC
scores on this balanced dataset are also all lower than 50%. AVR is

Attention: Not Just Another Dataset for
Patch-Correctness Checking

Table 3: Performance of ssFix on different datasets

ssFix
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

TP
515
1221
1826
2242
126

FP
139
43
79
247
145

TN
109
4
4
24
126

FN precision
139
43
79
247
145

78.75% 78.75%
96.60% 96.60%
95.85% 95.85%
90.08% 90.08%
46.49% 46.49%

AVR
AUC
43.95% 60.61%
2.72(9.1)
8.51% 49.50% 6.77(10.69)
4.82% 45.23%
6.26(9.64)
8.86% 45.46% 6.12(12.87)
-
46.49% 45.88%

recall 𝑅𝑒𝑐𝑎𝑙𝑙𝑐

Conference’17, July 2017, Washington, DC, USA

1 @@ -453,4 +453,4 @@
2 -
3 +
4

if (this.rightBlock != null) {
if (false) {

RectangleConstraint c4 = new RectangleConstraint...

Listing 1: A PraPR patch replacing condition with false

Table 4: Performance of S3 on different datasets

Table 7: Static PCC on semantically equivalent patches

S3
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

TP
517
1222
1826
2250
120

FP
137
42
79
239
151

TN
111
5
4
32
120

FN precision
137
42
79
239
151

79.05% 79.05%
96.68% 96.68%
95.85% 95.85%
90.40% 90.40%
44.28% 44.28%

AVR
AUC
44.76% 61.57%
2.8(9.1)
10.64% 53.56% 8.15(10.69)
7.15(9.64)
4.82% 56.63%
7(12.87)
11.81% 45.03%
-
44.28% 44.57%

recall 𝑅𝑒𝑐𝑎𝑙𝑙𝑐

Table 5: Performance of CapGen on different datasets

CapGen
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

TP
510
1222
1827
2253
133

FP
144
42
78
236
138

TN
104
5
5
35
133

FN precision
144
42
78
236
138

77.98% 77.98%
96.68% 96.68%
95.91% 95.91%
90.52% 90.52%
49.08% 49.08%

AVR
AUC
2.14(9.1)
41.94% 64.04%
10.64% 47.80% 7.38(10.69)
6.02% 45.98%
6.62(9.64)
12.92% 50.34% 5.49(12.87)
-
49.08% 50.77%

recall 𝑅𝑒𝑐𝑎𝑙𝑙𝑐

Table 6: Average scores based on static features

Patches
𝑃𝑟𝑎𝑃𝑅𝑣2.0Correct
𝑃𝑟𝑎𝑃𝑅𝑣2.0Overfitting
𝑊 𝑎𝑛𝑔𝑣1.2Correct
𝑊 𝑎𝑛𝑔𝑣1.2Overfitting
Developer

ssFix
1.52
1.56
1.49
1.35
1.16

S3
10.60
15.73
11.71
26.35
40.64

CapGen
0.37
0.41
0.44
0.25
0.26

a better metric for simulating actual efforts of selecting one correct
patch from plausible patches. From the tables, we can observe that
for 𝑊 𝑎𝑛𝑔𝑣1.2dataset, developers would examine 2.14~2.80 patches
on average (depending on the chosen technique) until the first cor-
rect patch is found. However, when PraPR dataset is considered or
merged with 𝑊 𝑎𝑛𝑔𝑣1.2, the AVRs rise to 5.49~8.15. In other words,
when larger patch space is included, effectiveness of all similarity-
based static tools significantly degrades and the cost to identify
correct patch for each bug significantly increases.

With the aim to understand the reasons behind these results, we
further investigate the detailed raw patch scores computed by dif-
ferent tools as shown in Table 7. Note that for ssFix and CapGen, the
scores reveal the similarity between the buggy code and the patches;
while for S3, the scores represent the edit distance of the patches.
Such techniques are designed based on the widely-accepted com-
mon assumption that correct patches should be more similar to the
buggy code. To investigate the validity of such a widely-accepted as-
sumption, we split all the patches into four groups: 𝑃𝑟𝑎𝑃𝑅𝑣1.2correct
patches, 𝑃𝑟𝑎𝑃𝑅𝑣1.2 overfitting patches, 𝑊 𝑎𝑛𝑔𝑣1.2 correct patches and
𝑊 𝑎𝑛𝑔𝑣1.2 overfitting patches. Furthermore, we also include the de-
veloper patches for comparison. As shown in Table 7, if we focus
on the dataset of 𝑊 𝑎𝑛𝑔𝑣1.2, the average scores of correct patches
for ssFix and CapGen are 10.37% and 76% higher than the overfit-
ting patches respectively. Besides, the average score of the correct
patches for S3 is 55.56% lower than of the overfitting patches. A
Mann-Whitney U Test [43] is conducted to evaluate the signifi-
cance of the difference and it shows that the p-values are relatively
4.19 × 10−7, 3.00 × 10−11 and 9.72 × 10−9 for ssFix, CapGen and
S3 respectively. That is to say, correct patches in 𝑊 𝑎𝑛𝑔𝑣1.2 in gen-
eral share higher similarities with the buggy code and introduce
fewer modifications than overfitting patches. Such results actually
support the widely-accepted assumption, and can also explain why
these tools exhibit promising results on 𝑊 𝑎𝑛𝑔𝑣1.2 [62] and their
original publications [30, 64, 66].

Chart-13-mutant-6
TokenStrct(ssFix)
TokenConpt(ssFix)
ASTDist(S3)
ASTCosDist(S3)
VariableDist(S3)
VariableSimi(CapGen)
SyntaxSimi(CapGen)
SemanticSimi(CapGen)

mutate condition to false
0.829
0.773
6
0.025
3
0.5
0.245
0.011

remove if block
0.344
0.419
51
0.097
20
0.393
0.347
0.618

However, such an assumption might no longer be valid on our
new datasets. Specifically, on 𝑃𝑟𝑎𝑃𝑅𝑣2.0, for ssFix and CapGen, the
average similarity scores of the overfitting patches are even 2.63%
and 10.81% higher than those of the correct patches (the p-values
for Mann-Whitney Test are 0.070 and 0.106 respectively). Moreover,
if we compare with the correct developer patches, the average score
of ssFix on the overfitting patches in 𝑊 𝑎𝑛𝑔𝑣1.2 is also 14.66% higher
than that of the developer patches (with p-value of 1.01 × 10−11)
while average scores of ssFix and CapGen on 𝑃𝑟𝑎𝑃𝑅𝑣2.0overfitting
patches are 34.48% and 57.69% higher than that of the developer
patches (with p-values of 1.42 × 10−54 and 5.53 × 10−12 respec-
tively). Such results are contradictory to those observed merely
based on 𝑊 𝑎𝑛𝑔𝑣1.2 [62]. As for S3, although the results observed
on 𝑃𝑟𝑎𝑃𝑅𝑣2.0 share a similar trend with those on 𝑊 𝑎𝑛𝑔𝑣1.2, we can
still observe contradictory results if we compare with the developer
patches. Specifically, the average distance scores of S3 on developer
patches are 54.23% higher than overfitting patches in 𝑊 𝑎𝑛𝑔𝑣1.2 and
158.36% higher than overfitting patches in 𝑃𝑟𝑎𝑃𝑅𝑣2.0 (p-values of
1.87 × 10−25 and 4.92 × 10−81 respectively).

Finding 1: The widely-accepted assumption made by existing
similarity-based static techniques that correct patches in gen-
eral share higher similarities with the buggy code is no longer
valid on our new dataset with exhaustive patch generation.

Through further investigation, we observe that one major issue
of similarity-based static techniques is that they can produce diverse
similarity scores for semantic-equivalent patches. For instance,
Listing 1 shows a simple example patch. This patch replaces the
conditional expression simply with false, which is semantically
equivalent to removing the whole if block (including its large body).
The scores generated by the three static techniques are totally
different, as shown in Table 7. The left column displays the scores
of the original patch, i.e., replace the conditional expression with
false, while the right column denotes the scores of the simplified
patch, i.e., simply removing the whole if block. Among the eight
static features, the similarity scores of ssFix features of original
patch are significantly higher compared with the simplified patch
(i.e., the patch with the whole if block removed) 140.99% and 84.49%
respectively, while the edit distance scores for S3 of the original
patch have much lower scores than the simplified patch. For CapGen
features, the scores are also completely different from each other.
Such results provide stronger evidence that merely considering the

Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

if (childType.isDict()) {...
} else {

1 @@ -1311,7 +1311,7 @@
2
3
4 -
5 +
6
7

return;
} else if ...

if (n.getJSType() != null && parent.isAssign())
if (false && parent.isAssign())

Listing 2: A correct patch misclassified by Anti-patterns

syntactic similarity is inadequate for PCC while more advanced
semantics-based techniques should be proposed.

Finding 2: All three studied static techniques can compute
totally different scores for semantically equivalent patches,
indicating that future static PCC techniques should incorporate
more advanced semantics analysis.

Table 8: Performance of Anti-patterns

FP
37
10
28
56
56

TP
219
174
361
559
41

TN
211
37
55
215
215

𝑅𝑒𝑐𝑎𝑙𝑙
33.49%
13.77%
18.95%
22.46%
15.13%

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
85.08%
78.72%
66.27%
79.34%
79.34%

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
85.55%
94.57%
92.80%
90.89%
42.27%

Datasets
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

FN
435
1,090
1,544
1,930
230
5.1.2 Anti-patterns. Table 8 presents the results of Anti-patterns [59]
on our four new datasets, along with its original results in previ-
ous work [62] (i.e., 𝑊 𝑎𝑛𝑔𝑣1.2). From the table, we can observe that
Anti-patterns performs significantly worse on PraPR patches by
misidentifying a larger proportion of correct patches as overfitting
and omitting more overfitting patches. For example, compared to
the previous dataset, Anti-patterns exhibits both lower 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 and
𝑅𝑒𝑐𝑎𝑙𝑙 on 𝑃𝑟𝑎𝑃𝑅𝑣1.2. Although there seems to be an improvement
in 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛, it is actually caused by the largely imbalanced datasets,
i.e., the overwhelming number of overfitting patches over the cor-
rect ones. Actually, the results on the balanced dataset 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0
further confirm that Anti-patterns performs much worse in all the
used metrics than the results reported in prior work [62].

We further looked into the root cause of Anti-patterns’ poor
performance on our datasets, and illustrate that with two mis-
classification cases of Anti-patterns. Note that these two patches
are generated exclusively by PraPR and have not been included
in any previous dataset. Listing 2 is an FP example where Anti-
patterns mistakenly identifies a correct patch generated by PraPR
as overfitting. In particular, the correct patch shown in Listing 2 falls
in the anti-pattern A2 (i.e., Anti-delete Control Statement) which
disallows any removal of control statements (e.g., if-statements,
switch-statements, and loops). In fact, in order to enable general
and efficient searching attempts in the patch space, fixing operations
leveraged in existing APR (especially the basic fixing operations in
PraPR) are often much less sophisticated than manual fixes, which
may even appear “simple” and “crude” sometimes. Therefore, Anti-
patterns, which distinguishes overfitting and correct patches by
the pattern of manual patches, inherently becomes less effective
in identifying the developer-unlike but correct patches (which are
largely missed by prior APR but captured by PraPR).

Listing 3 presents an FN example that Anti-patterns mistak-
enly identifies overfitting patches generated by PraPR as correct.
The overfitting patch makes a subtle modification by removing
the method invocation calculateBottomInset (i.e., only one to-
ken changed). Such a modification does not fall into any existing
Anti-patterns, thus cannot be excluded during PCC. In particular,

1 @@ -524,7 +524,7 @@
2
3 -
4 +
5

...
double b = calculateBottomInset(h);
double b = h;
area.setRect(area.getX() + l, area.getY() + t, w - l - r, h - t - b);

Listing 3: An overfitting patch misclassified by Anti-
patterns

although this overfitting patch involves only a basic repair opera-
tion, it is exclusive in our dataset, and is not included in any other
existing dataset. This further confirms our motivation that the
dataset-overfitting issue and early-stop mechanism of most APR
tools result in insufficient plausible patches for evaluating PCC.

Finding 3: Anti-patterns performs much worse on our datasets
due to its limited capability of identifying (i) overfitting patches
with subtle modifications and (ii) correct patches with developer-
unlike modifications. Such problem is largely amplified on our
datasets due to PraPR’s exhaustive patch generation.

5.2 RQ2: Learning-based Techniques
Table 9 presents the results of the embedding-based technique [60]
on our datasets, i.e., 𝑃𝑟𝑎𝑃𝑅𝑣1.2 and 𝑃𝑟𝑎𝑃𝑅𝑣2.0. Since there is a large
overlap between the training set used by the embedding technique
and 𝑀𝑒𝑟𝑔𝑒𝑣2.0/𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0, i.e., they all include patches collected
by Liu et al. [39], we do not consider 𝑀𝑒𝑟𝑔𝑒𝑣2.0 or 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0 in
this RQ. For comparison, we also present the performance of the
embedding-based technique in its original paper (i.e., 𝑇𝑖𝑎𝑛𝑣1.2).
From the table, we can have the following observations.

First, the embedding-based technique tends to perform worse on
PraPR patches. In particular, compared to original results, AUC be-
comes lower on both 𝑃𝑟𝑎𝑃𝑅𝑣1.2 and 𝑃𝑟𝑎𝑃𝑅𝑣2.0. Note that although
there is an improvement in 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 and 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 , it does not neces-
sarily indicate that the embedding-based technique becomes more
effective, since our datasets are more imbalanced than previous ones
and AUC is a more reliable metric for such imbalanced datasets.
Second, it is also interesting to see that the embedding-based tech-
nique performs even worse on 𝑃𝑟𝑎𝑃𝑅𝑣2.0 than 𝑃𝑟𝑎𝑃𝑅𝑣1.2, i.e., AUC
further degrades on 𝑃𝑟𝑎𝑃𝑅𝑣2.0. This indicates that the embedding-
based technique is less effective on the additional bugs in 𝑃𝑟𝑎𝑃𝑅𝑣2.0.
To confirm the observation, we further present the results on the ex-
clusive bugs in 𝑃𝑟𝑎𝑃𝑅𝑣2.0 in the table, i.e., 𝑃𝑟𝑎𝑃𝑅𝑣2.0−𝑣1.2. The even
lower AUC (i.e., 67.80 %) further confirms that the embedding-based
technique suffers from the dataset overfitting issue: it performs
worse on the patches of subjects that are different from training set.
Thus, we encourage future learning-based PCC work to consider
across-dataset evaluation.

We then look into the potential reasons for such a decrement.
The intuition of embedding-based PCC techniques is that the cosine
similarity between the embedding vectors of correct patches and
buggy code should be larger than the cosine similarity between the
embedding vectors of overfitting patches and buggy code. Tian et

Table 9: Performance of the embedding-based technique

Datasets
𝑇 𝑖𝑎𝑛𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑃𝑟𝑎𝑃𝑅𝑣2.0−𝑣1.2

TP
85
725
1,020
295

FP
14
15
23
8

TN
16
27
53
26

FN
24
324
633
309

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
85.86%
97.97%
97.79%
97.36%

𝑅𝑒𝑐𝑎𝑙𝑙
77.98%
69.11%
61.71%
48.84%

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
53.33%
64.29%
69.74%
76.47%

AUC
76.50%
75.49%
72.49%
67.80%

Attention: Not Just Another Dataset for
Patch-Correctness Checking

Conference’17, July 2017, Washington, DC, USA

...

1 @@ -64,9 +64,6 @@
2
3 - if (property == null) {
4 -
return this;
5 - }
6

JsonFormat.Value format=findFormatO(serializers,property,handledType());...

Figure 1: Similarity distribution on different datasets

Table 10: Performance of E-Opad on different datasets

E-Opad
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

FN
562
1,116
1,638
2,145
223
Table 11: Performance of R-Opad on different datasets

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
100.00%
100.00%
99.26%
99.42%
96.00%

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
100.00%
100.00%
97.59%
99.26%
99.26%

𝑅𝑒𝑐𝑎𝑙𝑙
14.07%
11.71%
14.02%
13.82%
17.71%

TN
248
47
81
269
269

TP
92
148
267
344
48

FP
0
0
2
2
2

R-Opad
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣2.0
𝑀𝑒𝑟𝑔𝑒𝑣2.0
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0

TP
67
238
346
408
44

FP
0
12
15
15
15

TN
248
35
68
256
256

FN
587
1,026
1,559
2,081
227

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
100.00%
95.20%
95.84%
96.45%
74.58%

𝑅𝑒𝑐𝑎𝑙𝑙
10.24%
18.83%
18.16%
16.39%
16.24%

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
100.00%
74.47%
81.93%
94.46%
94.46%

al. [60] show that correct/overfitting patches in their dataset per-
fectly follow such an assumption. However, this assumption no
longer holds on the additional patches generated by PraPR. Figure 1
presents the distribution of the cosine similarity between the em-
bedding vectors of correct/overfitting patches and buggy code on
the original dataset in the embedding work [60] (i.e., 𝑇𝑖𝑎𝑛𝑣1.2) and
our datasets built on Defects4J V1.2 and additional bugs in Defects4J
V2.0 (i.e., 𝑃𝑟𝑎𝑃𝑅𝑣1.2 and 𝑃𝑟𝑎𝑃𝑅𝑣2.0−𝑣1.2). From the figure, we ob-
serve that totally differing from prior work [60], correct patches
and overfitting patches share very close distributions of similarity
scores on our datasets, which explains why the embedding-based
technique exhibits worse performance on PraPR patches. Further-
more, we can also observe that correct patches even tend to have
lower median similarity than overfitting patches on 𝑃𝑟𝑎𝑃𝑅𝑣2.0−𝑣1.2,
demonstrating that the assumption made by the embedding-based
work no longer holds on our new dataset.

Finding 4: The assumption made by the embedding-based
technique that the cosine similarity between the embeddings
of correct patches and buggy code should be larger than that
between the embeddings of overfitting patches and buggy code
no longer holds on this new dataset. Also, the embedding-based
technique tends to suffer from the dataset overfitting issue.

5.3 RQ3: Dynamic Techniques

5.3.1 Opad. The experimental result for the two Opad variants,
i.e., E-Opad (Opad with EvoSuite) and R-Opad (Opad with Ran-
doop), are shown in Table 10 and Table 11, respectively. We can
observe that except for R-Opad on 𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑣2.0, Opad achieves a
precision over 95% on all the datasets, while the achieved recall
ranges from 11.71% to 18.83%. The high precision with low recall
achieved by Opad is consistent with previous studies [62, 72]. Be-
sides, the 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 also significantly outperforms other techniques
with respect to precision. Such results indicate that Opad can iden-
tify most of the correct patches and rarely classifies correct patches
as overfitting ones. Different from the static and learning-based

Listing 4: A correct patch misclassified by Opad

techniques, the performance of Opad does not degrade much in
our new datasets. In other words, Opad is more stable than static
techniques. This falls into our intuition since such dynamic tools
based on test generation concern more towards code semantics in-
stead of syntactic elements. Meanwhile, similar to the findings from
prior work [62], Opad achieves a rather low recall on our datasets,
indicating that a substantial ratio of overfitting patches are not
detected. This is still a critical issue in PCC since manually filtering
out overfitting patches can be extremely costly for developers [69].

Finding 5: The performance of Opad overall tends to remain
similar on our new datasets, demonstrating the robustness of
such dynamic techniques. Meanwhile, consistent with the find-
ings in prior work, it achieves a rather low recall for incorrect
patches, which compromises its practical usefulness.

Although the precision achieved by Opad is still high even in
our new datasets, it can no longer achieve a precision of 100%,
i.e., a few correct patches are incorrectly identified as overfitting.
This is inconsistent with the results released in previous studies for
PCC [62, 72], where a precision of 100% can be always achieved by
Opad. Motivated by this, we further investigated the false positive
cases manually and made the following observations.

For instance, Listing 4 shows a correct patch which is identified
as overfitting by Opad. Specifically, this patch [3] removes a null
check for property. However, Opad is designed based on the tests
generated on the original buggy programs, thus being unaware of
such semantic changes. When stepping into method findFormatO
(at line 6) from a test, property is null, which is unexpected to
the tests generated on the original buggy program. Consequently,
a NullPointerException was thrown and in findFormatO, and
thus Opad mistakenly identified this patch as overfitting. Many
other similar cases have been observed for Opad. The reason could
be that some of the tests are generated on the buggy programs,
based on some incorrect contracts or preconditions. When the
semantics of a program change, those preconditions could be easily
broken and fail on a generated test unexpectedly.

Finding 6: The effectiveness of Opad might decay and it might
mistakenly identify correct patches as incorrect when the se-
mantic changes of the patches break certain conditions.

Since developer patches might even incur larger semantic changes,
we are curious to see whether such similar cases could also happen
on developer patches. Unfortunately, there are no existing studies
investigating Opad on developer patches to our best knowledge.
However, this is an important study since APR techniques can

Table 12: Opad/Patch-Sim on developer patches

E-Opad
R-Opad
Patch-Sim

TP
0
0
0

FP
111
167
65

TN
685
629
110

FN
0
0
0

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
86.06%
79.02%
62.86%

Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

...
if (length < 9) {...
} else {

1 @@ -483,9 +483,8 @@ public static int formatLongOctalOrBinaryBytes(
2
3
4 +
5 +
6 -
7
8

formatBigIntegerBinary(value, buf, offset, length, negative);
buf[offset] = (byte) (negative ? 0xff : 0x80);
return offset + length;

formatBigIntegerBinary(value, buf, offset, length, negative);}

Listing 5: A developer patch misclassified by Opad

potentially fix more and more bugs and produce more and more
developer patches in the near future. As shown in Table 12, E-Opad
and R-Opad achieve 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 of 86.05% and 79.02%, respectively,
which is significantly lower than that on the other datasets except
for 𝑃𝑟𝑎𝑃𝑅𝑣1.2. Specifically, Opad produces more false positive cases
and achieves lower 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 compared with previous study [62, 72].
We further manually checked some false positive cases in developer
patches, and Listing 5 shows an example. Specifically, the devel-
oper patch moved a statement into an else block with a condition
length >= 9. The failing test expects an IllegalArgumentException
triggered by invalid parameters of formatBigIntegerBinary. When
stepping into the method in Listing 5, the original statement throw-
ing that exception was not executed in the patched program. There-
fore, the program stepped over that statement and when executing
the next statement (line 7), the parameter offset is -1, triggering an
ArrayIndexOutOfBoundsException. Similar to the previous exam-
ple, Opad is not aware of such semantic changes and misclassified
that patch as overfitting. Actually, Opad could produce more false
positive cases when the patches get more complicated, i.e., have dif-
ferent code structure, logic or semantics. This can be an important
finding for the community since eventually APR techniques will be
able to fix more and more complicated bugs, producing more and
more complicated patches.

Finding 7: Opad cannot handle developer patches well, and
should be applied with caution on (future) automated APR tools
that can generate more complicated patches.

Table 13: Performance of Patch-Sim on different datasets

Datasets
𝑋𝑖𝑜𝑛𝑔𝑣1.2
𝑊 𝑎𝑛𝑔𝑣1.2
𝑃𝑟𝑎𝑃𝑅𝑣1.2

TP
62
249
210

FP
0
51
3

TN
29
186
7

FN
48
392
283

𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
100.00%
83.00%
98.59%

𝑅𝑒𝑐𝑎𝑙𝑙
56.36%
38.85%
42.60%

𝑅𝑒𝑐𝑎𝑙𝑙𝑐
100.00%
78.48%
70.00%

5.3.2 Patch-Sim. Table 13 presents the performance of Patch-Sim
on previous datasets (i.e., 𝑋𝑖𝑜𝑛𝑔𝑣1.2 from its original paper [69] and
𝑊 𝑎𝑛𝑔𝑣1.2 from the previous revisiting study [62]) and our dataset
(i.e., 𝑃𝑟𝑎𝑃𝑅𝑣1.2). Note that due to an implementation issue [69],
Patch-Sim does not support Defects4J V2.0 and subjects Closure
and Mockito from Defects4J V1.2. Therefore, in line with previous
work [69], we only consider the patches supported by Patch-Sim
from Defects4J V1.2. In total, 503 patches in 𝑃𝑟𝑎𝑃𝑅𝑣1.2 are used for
this evaluation, including 493 overfitting patches and 10 correct
patches. As shown in the table, Patch-Sim performs significantly
worse on our dataset 𝑃𝑟𝑎𝑃𝑅𝑣1.2 in all studied metrics. For exam-
ple, Patch-Sim is shown to achieve 100% 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 for overfitting
patches and 100% 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 for correct patches on its original dataset,
whereas on PraPR patches, both metrics are degraded. Such an
observation is mainly consistent with results in previous work [62].

Listing 6 presents an FP example that Patch-Sim mistakenly identi-
fies the correct patch as overfitting. Patch-Sim is designed based on
the mild consequence assumption that a passing test should behave
similarly on the correct patch and the buggy code. Therefore, the
patch substantially changing the behavior of passing tests would be
regarded as overfitting by Patch-Sim. In this example, the correct
patch modifies the condition, leading to significant changes in con-
trol flow: on the buggy code, we find that some passing tests can
enter the if block and then throw the exception, whereas on the
patch code these tests skip the if block and exception statement.
Therefore, Patch-Sim considers the behaviors of these passing tests
changed substantially, and further regards the patch as overfitting.
However, such an assumption is not true and easily broken if con-
sidering a large patch space, since correct patches are also likely
to introduce large behavioral changes whereas overfitting patches
can also induce little impact on passing tests.

1 @@ -195,7 +195,7 @@
...
2
if (fa * fb >= 0.0) {
3 -
4 +
if (fa * fb > 0.0D) {
5

throw new ConvergenceException...}

Listing 6: A correct patch misclassified by Patch-Sim

Finding 8: The assumption made by Patch-Sim that passing
tests should behave similarly on correct patch code and buggy
code can easily be broken when a larger patch space is consid-
ered, especially for the correct patches with non-trivial modifi-
cations on control flow.

So far, we have mainly evaluated Patch-Sim on the PraPR patches,
which include a limited number of correct patches due to the im-
plementation issues of Patch-Sim. To further investigate its perfor-
mance on additional correct patches, we also consider developer
patches. Table 12 presents the performance of Patch-Sim on 175
developer patches from Defects4J V1.2 that Patch-Sim can be suc-
cessfully applied to. It is interesting to find that Patch-Sim performs
even worse on developer patches with a 𝑅𝑒𝑐𝑎𝑙𝑙𝑐 of only 62.86%, i.e.,
mis-classifying 37.14% correct patches as incorrect. After manual
inspection, we find that developer patches often involve more so-
phisticated modifications and thus passing tests behave very differ-
ently from buggy code. For example, Listing 7 presents a developer
patch which is mistakenly identified as overfitting by Patch-Sim.
The patch modifies multiple lines, and several if-else conditional
statements. Thus, it significantly affects the control flow and ex-
ecution traces of some passing tests. In fact, it is prevalent that
developer patches contain such sophisticated modifications, and
thus the assumption made by Patch-Sim can easily be violated on
developer patches. Such results show that future PCC work should
include complex patches for evaluation since advanced APR tools
can generate more complex patches in the near future.

Finding 9: Patch-Sim misclassifies 30% correct patches as
incorrect on 𝑃𝑟𝑎𝑃𝑅𝑣1.2 and tends perform even worse with
38.14% misclassifion rate on complicated developer patches.
Our results also suggest that future dynamic PCC techniques
should consider more complex patches for evaluation.

Attention: Not Just Another Dataset for
Patch-Correctness Checking

if(start == seqEnd) {
return 0;

1 @@ -37,47 +37,53 @@
2 - if(... && index < seqEnd - 1 && ...) {
3 + if(... && index < seqEnd - 2 && ...) {...}
4 +
5 +
6 +
7
8 -
9 +
10 +
11 +
12
13 +
14 -
15 +

||(input.charAt(end) >= 'a' && input.charAt(end) <= 'f')
||(input.charAt(end) >= 'A' && input.charAt(end) <= 'F') ) )
...

boolean semiNext = (end != seqEnd) && (input.charAt(end) == ';');
return 2 + (end - start) + (isHex ? 1 : 0) + 1;
return 2 + (end - start) + (isHex ? 1 : 0) + (semiNext ? 1 : 0);

}
...
while(input.charAt(end) != ';')
while(end<seqEnd && ((input.charAt(end)>='0' && input.charAt(end)<='9')

Listing 7: A developer patch misclassified by Patch-Sim

6 CONCLUSION
We have constructed an extensive PCC dataset for revisiting all
state-of-the-art PCC techniques. Our new dataset leverages the
highly-optimized PraPR APR tool to return all possible plausible
patches within its large predefined patch search space (including
well-known fixing patterns from various prior APR tools) for more
realistic/thorough PCC evaluation. Our extensive study of repre-
sentative PCC techniques on the new dataset has revealed various
surprising findings and practical guidelines/suggestions for future
PCC work, including 1) various assumptions used by prior stud-
ies [9, 30, 62, 64, 66] no longer hold in our new dataset and seman-
tics analysis should be included to determine patch correctness,
2) state-of-the-art learning-based techniques tend to suffer from
the dataset overfitting problem, and 3) while dynamic techniques
overall tend to retain their effectiveness on our new dataset, their
performance drops substantially when encountering patches with
more complicated changes .

REFERENCES
[1] [n.d.].
[2] 2022. JD-Core. https://github.com/java-decompiler/jd-core.
[3] 2022. Patch Example. https://github.com/FasterXML/jackson-databind/commit/

323cd0b309697021f1883de1e53038d2d09fc160.
[4] 2022. Pitest tool. https://github.com/hcoles/pitest.
[5] 2022. PraPR Report. https://github.com/prapr/prapr#prapr-reports.
[6] 2022.

Replication Package.

https://github.com/anonymous0903/patch_

correctness.

[7] Altuna Akalin. 2020. Computational Genomics with r. Chapman and Hall/CRC.
[8] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge

University Press.

[9] Moumita Asad, Kishan Kumar Ganguly, and Kazi Sakib. 2019. Impact Analysis
of Syntactic and Semantic Similarities on Patch Prioritization in Automated
Program Repair. In 2019 IEEE International Conference on Software Maintenance
and Evolution (ICSME). 328–332. https://doi.org/10.1109/ICSME.2019.00050
[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-
chine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).

[11] Samuel Benton, Xia Li, Yiling Lou, and Lingming Zhang. [n.d.]. On the Effective-
ness of Unified Debugging: An Extensive Study on 16 Program Repair Systems.
([n. d.]).

[12] Marcel Böhme and Abhik Roychoudhury. 2014. Corebench: Studying complexity
of regression errors. In Proceedings of the 2014 international symposium on software
testing and analysis. 105–115.

[13] Liushan Chen, Yu Pei, and Carlo A. Furia. 2017. Contract-based program re-
pair without the contracts. In Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering, ASE 2017, Urbana, IL, USA, Oc-
tober 30 - November 03, 2017, Grigore Rosu, Massimiliano Di Penta, and Tien N.
Nguyen (Eds.). IEEE Computer Society, 637–647. https://doi.org/10.1109/ASE.
2017.8115674

[14] Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet,
Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-
sequence learning for end-to-end program repair. IEEE Transactions on Software

Conference’17, July 2017, Washington, DC, USA

Engineering (2019).

[15] Viktor Csuvik, Dániel Horváth, Ferenc Horváth, and László Vidács. 2020. Uti-
lizing Source Code Embeddings to Identify Correct Patches. In 2020 IEEE 2nd
International Workshop on Intelligent Bug Fixing (IBF). 18–25.

[16] Viktor Csuvik, Dániel Horváth, Márk Lajkó, and László Vidács. 2021. Exploring
Plausible Patches Using Source Code Embeddings in JavaScript. In 2021 IEEE/ACM
International Workshop on Automated Program Repair (APR). 11–18. https://doi.
org/10.1109/APR52552.2021.00010

[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
CoRR abs/1810.04805 (2018). arXiv:1810.04805 http://arxiv.org/abs/1810.04805

[18] Thomas Durieux, Fernanda Madeiral, Matias Martinez, and Rui Abreu. 2019.
Empirical Review of Java Program Repair Tools: A Large-Scale Experiment on
2,141 Bugs and 23,551 Repair Attempts. In Proceedings of the 27th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE ’19). https://arxiv.org/abs/1905.11973

[19] Thomas Durieux and Martin Monperrus. 2016. DynaMoth: dynamic code synthe-
sis for automatic program repair. In Proceedings of the 11th International Workshop
on Automation of Software Test, AST@ICSE 2016, Austin, Texas, USA, May 14-15,
2016, Christof J. Budnik, Gordon Fraser, and Francesca Lonetti (Eds.). ACM, 85–91.
https://doi.org/10.1145/2896921.2896931

[20] Thomas Durieux and Martin Monperrus. 2016. Dynamoth: dynamic code synthe-
sis for automatic program repair. In Proceedings of the 11th International Workshop
on Automation of Software Test. 85–91.

[21] Michael D. Ernst, Jake Cockrell, William G. Griswold, and David Notkin. 2001.
Dynamically discovering likely program invariants to support program evolution.
IEEE Transactions on Software Engineering 27, 2 (Feb. 2001), 99–123.

[22] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic test suite generation
for object-oriented software. In SIGSOFT/FSE’11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19) and ESEC’11: 13rd European
Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011.
[23] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical program
repair via bytecode mutation. In Proceedings of the 28th ACM SIGSOFT In-
ternational Symposium on Software Testing and Analysis, ISSTA 2019, Beijing,
China, July 15-19, 2019, Dongmei Zhang and Anders Møller (Eds.). ACM, 19–30.
https://doi.org/10.1145/3293882.3330559

[24] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: dis-
tributed representations of code changes. In ICSE ’20: 42nd International Confer-
ence on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, Gregg
Rothermel and Doo-Hwan Bae (Eds.). ACM, 518–529. https://doi.org/10.1145/
3377811.3380361

[25] Jin Huang and C.X. Ling. 2005. Using AUC and accuracy in evaluating learning
algorithms. IEEE Transactions on Knowledge and Data Engineering 17, 3 (2005),
299–310. https://doi.org/10.1109/TKDE.2005.50

[26] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen.
2018. Shaping program repair space with existing patches and similar code. In
Proceedings of the 27th ACM SIGSOFT international symposium on software testing
and analysis. 298–309.

[27] René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: a database of
existing faults to enable controlled testing studies for Java programs. In Interna-
tional Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA -
July 21 - 26, 2014, Corina S. Pasareanu and Darko Marinov (Eds.). ACM, 437–440.
https://doi.org/10.1145/2610384.2628055

[28] Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim, Jacques Klein,
Martin Monperrus, and Yves Le Traon. 2020. FixMiner: Mining relevant fix
patterns for automated program repair. Empir. Softw. Eng. 25, 3 (2020), 1980–2024.
https://doi.org/10.1007/s10664-019-09780-z

[29] Quoc V. Le and Tomás Mikolov. 2014. Distributed Representations of Sentences
and Documents. In Proceedings of the 31th International Conference on Machine
Learning, ICML 2014, Beijing, China, 21-26 June 2014 (JMLR Workshop and Con-
ference Proceedings, Vol. 32). JMLR.org, 1188–1196. http://proceedings.mlr.press/
v32/le14.html

[30] Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: syntax-and semantic-guided repair synthesis via programming by
examples. In Proceedings of the 11th Joint Meeting on Foundations of Software
Engineering. ACM, 593–604.

[31] Xuan Bach D Le, David Lo, and Claire Le Goues. 2016. History driven program
repair. In 2016 IEEE 23rd international conference on software analysis, evolution,
and reengineering (SANER), Vol. 1. IEEE, 213–224.

[32] Xuan-Bach Dinh Le, Lingfeng Bao, David Lo, Xin Xia, Shanping Li, and Corina S.
Pasareanu. 2019. On reliability of patch correctness assessment. In Proceedings of
the 41st International Conference on Software Engineering, ICSE 2019, Montreal,
QC, Canada, May 25-31, 2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle
(Eds.). IEEE / ACM, 524–535. https://doi.org/10.1109/ICSE.2019.00064

[33] Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: syntax- and semantic-guided repair synthesis via programming by
examples. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering, ESEC/FSE 2017, Paderborn, Germany, September 4-8, 2017, Eric Bodden,

Conference’17, July 2017, Washington, DC, USA

Yuehan Wang*, Jun Yang*, Yiling Lou, Ming Wen, and Lingming Zhang

Wilhelm Schäfer, Arie van Deursen, and Andrea Zisman (Eds.). ACM, 593–604.
https://doi.org/10.1145/3106237.3106309

[34] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012.
GenProg: A generic method for automatic software repair. IEEE Transactions on
Software Engineering 38, 1 (2012), 54–72.

[35] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: context-based code
transformation learning for automated program repair. In ICSE ’20: 42nd In-
ternational Conference on Software Engineering, Seoul, South Korea, 27 June
- 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 602–614.
https://doi.org/10.1145/3377811.3380345

[36] Kui Liu, Anil Koyuncu, Tegawendé F. Bissyandé, Dongsun Kim, Jacques Klein,
and Yves Le Traon. 2019. You Cannot Fix What You Cannot Find! An Investigation
of Fault Localization Bias in Benchmarking Automated Program Repair Systems.
In 12th IEEE Conference on Software Testing, Validation and Verification, ICST 2019,
Xi’an, China, April 22-27, 2019. IEEE, 102–113. https://doi.org/10.1109/ICST.2019.
00020

[37] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F. Bissyandé. 2019.
AVATAR: Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations.
In 26th IEEE International Conference on Software Analysis, Evolution and Reengi-
neering, SANER 2019, Hangzhou, China, February 24-27, 2019, Xinyu Wang, David
Lo, and Emad Shihab (Eds.). IEEE, 456–467. https://doi.org/10.1109/SANER.2019.
8667970

[38] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F Bissyandé. 2019. Tbar:
revisiting template-based automated program repair. In Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and Analysis. 31–42.
[39] Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawendé F. Bissyandé,
Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, and Yves Le Traon. 2020.
On the efficiency of test suite based program repair: A Systematic Assessment of
16 Automated Repair Systems for Java Programs. In ICSE ’20: 42nd International
Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020,
Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 615–627. https://doi.org/10.
1145/3377811.3380338

[40] Fan Long and Martin Rinard. 2015. Staged program repair with condition syn-
thesis. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software
Engineering. 166–178.

[41] Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao,
and Lu Zhang. 2020. Can automated program repair refine fault localization?
a unified debugging approach. In ISSTA ’20: 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, Virtual Event, USA, July 18-22,
2020, Sarfraz Khurshid and Corina S. Pasareanu (Eds.). ACM, 75–87. https:
//doi.org/10.1145/3395363.3397351

[42] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and
Lin Tan. 2020. CoCoNuT: combining context-aware neural translation models
using ensemble for program repair. In ISSTA ’20: 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, Virtual Event, USA, July 18-22,
2020, Sarfraz Khurshid and Corina S. Pasareanu (Eds.). ACM, 101–114. https:
//doi.org/10.1145/3395363.3397369

[43] Henry B Mann and Donald R Whitney. 1947. On a test of whether one of
two random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50–60.

[44] Alexandru Marginean, Johannes Bader, Satish Chandra, Mark Harman, Yue Jia,
Ke Mao, Alexander Mols, and Andrew Scott. 2019. SapFix: automated end-to-end
repair at scale. In Proceedings of the 41st International Conference on Software
Engineering: Software Engineering in Practice, ICSE (SEIP) 2019, Montreal, QC,
Canada, May 25-31, 2019, Helen Sharp and Mike Whalen (Eds.). IEEE / ACM,
269–278. https://doi.org/10.1109/ICSE-SEIP.2019.00039

[45] Matias Martinez and Martin Monperrus. 2016. Astor: A program repair library
for java. In Proceedings of the 25th International Symposium on Software Testing
and Analysis. 441–444.

[46] Matias Martinez and Martin Monperrus. 2018. Ultra-Large Repair Search Space
with Automatically Mined Templates: The Cardumen Mode of Astor. In Search-
Based Software Engineering - 10th International Symposium, SSBSE 2018, Mont-
pellier, France, September 8-9, 2018, Proceedings (Lecture Notes in Computer Sci-
ence, Vol. 11036), Thelma Elita Colanzi and Phil McMinn (Eds.). Springer, 65–86.
https://doi.org/10.1007/978-3-319-99241-9_3

[47] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
multiline program patch synthesis via symbolic analysis. In Proceedings of the
38th international conference on software engineering. 691–701.

[48] Martin Monperrus. 2020. The living review on automated program repair. (2020).
[49] Carlos Pacheco and Michael D Ernst. 2007. Randoop: feedback-directed random
testing for Java. In Companion to the 22nd ACM SIGPLAN conference on Object-
oriented programming systems and applications companion. 815–816.

[50] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: mutation-based fault
localization. Software Testing, Verification and Reliability 25, 5-7 (2015), 605–628.
[51] Yuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and Chengsong Wang. 2014. The
strength of random search on automated program repair. In Proceedings of the
36th International Conference on Software Engineering. ACM, 254–265.

[52] Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An analysis of
patch plausibility and correctness for generate-and-validate patch generation
systems. In Proceedings of the 2015 International Symposium on Software Testing
and Analysis. 24–36.

[53] Andrew Reynolds, Morgan Deters, Viktor Kuncak, Cesare Tinelli, and Clark
Barrett. 2015. Counterexample-guided quantifier instantiation for synthesis
in SMT. In International Conference on Computer Aided Verification. Springer,
198–216.

[54] Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R. Prasad. 2017. ELIXIR:
effective object oriented program repair. In Proceedings of the 32nd IEEE/ACM
International Conference on Automated Software Engineering, ASE 2017, Urbana,
IL, USA, October 30 - November 03, 2017, Grigore Rosu, Massimiliano Di Penta,
and Tien N. Nguyen (Eds.). IEEE Computer Society, 648–659. https://doi.org/10.
1109/ASE.2017.8115675

[55] Seemanta Saha, Ripon K. Saha, and Mukul R. Prasad. 2019. Harnessing evo-
lution for multi-hunk program repair. In Proceedings of the 41st International
Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31,
2019, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 13–24.
https://doi.org/10.1109/ICSE.2019.00020

[56] David Schuler and Andreas Zeller. 2009. Javalanche: Efficient mutation testing for
Java. In Proceedings of the 7th joint meeting of the European software engineering
conference and the ACM SIGSOFT symposium on The foundations of software
engineering. 297–298.

[57] Edward K. Smith, Earl T. Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure
worse than the disease? overfitting in automated program repair. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE
2015, Bergamo, Italy, August 30 - September 4, 2015, Elisabetta Di Nitto, Mark
Harman, and Patrick Heymans (Eds.). ACM, 532–543. https://doi.org/10.1145/
2786805.2786825

[58] Edward K Smith, Earl T Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure
worse than the disease? overfitting in automated program repair. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering. 532–543.
[59] Shin Hwei Tan, Hiroaki Yoshida, Mukul R. Prasad, and Abhik Roychoudhury.
2016. Anti-patterns in search-based program repair. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
727–738.

[60] Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li Li, Jacques Klein,
and Tegawendé F. Bissyandé. 2020. Evaluating Representation Learning of Code
Changes for Predicting Patch Correctness in Program Repair. In Proceedings of
the 35th IEEE/ACM International Conference on Automated Software Engineering.
ACM.

[61] Rijnard van Tonder and Claire Le Goues. 2018. Static automated program repair
for heap properties. In Proceedings of the 40th International Conference on Software
Engineering. 151–162.

[62] Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing Zou,
Xiaoguang Mao, and Hai Jin. 2020. Automated Patch Correctness Assessment:
How Far are We?. In 35th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020. IEEE,
968–980. https://doi.org/10.1145/3324884.3416590

[63] Westley Weimer, Zachary P Fry, and Stephanie Forrest. 2013. Leveraging program
equivalence for adaptive program repair: Models and first results. In 2013 28th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 356–366.

[64] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.
Context-aware patch generation for better automated program repair. In Pro-
ceedings of the 40th International Conference on Software Engineering, ICSE 2018,
Gothenburg, Sweden, May 27 - June 03, 2018, Michel Chaudron, Ivica Crnkovic,
Marsha Chechik, and Mark Harman (Eds.). ACM, 1–11. https://doi.org/10.1145/
3180155.3180233

[65] Qi Xin and Steven P. Reiss. 2017.

Identifying Test-Suite-Overfitted Patches
through Test Case Generation. In Proceedings of the 26th ACM SIGSOFT Inter-
national Symposium on Software Testing and Analysis (Santa Barbara, CA, USA)
(ISSTA 2017). Association for Computing Machinery, New York, NY, USA, 226–236.
https://doi.org/10.1145/3092703.3092718

[66] Qi Xin and Steven P. Reiss. 2017. Leveraging syntax-related code for automated
program repair. In Proceedings of the 32nd IEEE/ACM International Conference on
Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - November
03, 2017, Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen (Eds.). IEEE
Computer Society, 660–670. https://doi.org/10.1109/ASE.2017.8115676

[67] Qi Xin and Steven P. Reiss. 2019. Better Code Search and Reuse for Better
Program Repair. In Proceedings of the 6th International Workshop on Genetic
Improvement (Montreal, Quebec, Canada) (GI ’19). IEEE Press, 10–17. https:
//doi.org/10.1109/GI.2019.00012

[68] Qi Xin and Steven P Reiss. 2019. Revisiting ssFix for Better Program Repair. arXiv

preprint arXiv:1903.04583 (2019).

[69] Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, and Gang Huang. 2018.
Identifying patch correctness in test-based program repair. In Proceedings of the
40th International Conference on Software Engineering. ACM, 789–799.

Attention: Not Just Another Dataset for
Patch-Correctness Checking

Conference’17, July 2017, Washington, DC, USA

[70] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and
Lu Zhang. 2017. Precise condition synthesis for program repair. In Proceedings of
the 39th International Conference on Software Engineering, ICSE 2017, Buenos Aires,
Argentina, May 20-28, 2017, Sebastián Uchitel, Alessandro Orso, and Martin P.
Robillard (Eds.). IEEE / ACM, 416–426. https://doi.org/10.1109/ICSE.2017.45
[71] Jifeng Xuan, Matias Martinez, Favio Demarco, Maxime Clement, Sebastian Lame-
las Marcote, Thomas Durieux, Daniel Le Berre, and Martin Monperrus. 2017.
Nopol: Automatic repair of conditional statement bugs in java programs. IEEE
Transactions on Software Engineering 43, 1 (2017), 34–55.

[72] Jinqiu Yang, Alexey Zhikhartsev, Yuefei Liu, and Lin Tan. 2017. Better test cases
for better automated program repair. In Proceedings of the 11th Joint Meeting on
Foundations of Software Engineering. ACM, 831–841.

[73] He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2019.
Automated Classification of Overfitting Patches with Statically Extracted Code
Features. CoRR abs/1910.12057 (2019). arXiv:1910.12057 http://arxiv.org/abs/
1910.12057

[74] He Ye, Matias Martinez, and Martin Monperrus. 2021. Automated patch assess-
ment for program repair at scale. Empirical Software Engineering 26, 2 (2021),
1–38.

[75] Jooyong Yi, Shin Hwei Tan, Sergey Mechtaev, Marcel Böhme, and Abhik Roy-
choudhury. 2018. A correlation study between automated program repair and
test-suite metrics. In Proceedings of the 40th International Conference on Soft-
ware Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, Michel
Chaudron, Ivica Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 24.
https://doi.org/10.1145/3180155.3182517

[76] Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, and Martin
Monperrus. 2019. Alleviating patch overfitting with automatic test generation: a
study of feasibility and effectiveness for the Nopol repair system. Empir. Softw.
Eng. 24, 1 (2019), 33–67. https://doi.org/10.1007/s10664-018-9619-4

[77] Yuan Yuan and Wolfgang Banzhaf. 2018. ARJA: Automated Repair of Java Pro-
grams via Multi-Objective Genetic Programming. IEEE Transactions on Software
Engineering (2018).

[78] Yuan Yuan and Wolfgang Banzhaf. 2020. ARJA: Automated Repair of Java Pro-
grams via Multi-Objective Genetic Programming. IEEE Trans. Software Eng. 46,
10 (2020), 1040–1067. https://doi.org/10.1109/TSE.2018.2874648

