2
2
0
2

g
u
A
5
1

]
I

N
.
s
c
[

1
v
2
4
0
7
0
.
8
0
2
2
:
v
i
X
r
a

A Pipeline for DNS-Based Software Fingerprinting

Sebastian Schäfer
RWTH Aachen University
schaefer@itsec.rwth-aachen.de

Ulrike Meyer
RWTH Aachen University
meyer@itsec.rwth-aachen.de

ABSTRACT
In this paper, we present the modular design and implementation of
DONUT, a novel tool for identifying software running on a device.
Our tool uses a rule-based approach to detect software-specific DNS
fingerprints (stored in an easily extendable database) in passively
monitored DNS traffic. We automated the rule extraction process
for DONUT with the help of ATLAS, a novel tool we developed
for labeling network traffic by the software that created it. We
demonstrate the functionality of our pipeline by generating rules
for a number of applications, evaluate the performance as well as
scalability of the analysis, and confirm the functional correctness
of DONUT using an artificial data set for which the ground-truth
is known. In addition, we evaluate DONUT’s analysis results on a
large real-world data set with unknown ground truth.

1 INTRODUCTION
As networks today become increasingly complex it becomes more
and more important for network administrators, security consul-
tants, and pentesters to quickly and unintrusively obtain an overview
on the devices running in a specific network in an automated fash-
ion. Besides the general network topology, the information of in-
terest includes identifying the operating systems devices run on, as
well as the applications installed on them. An administrator in an
open network such as a university network can typically not install
administrative tools on every device used in the network. Never-
theless, he may, e.g., be interested in statistics related to the general
use of operating systems and applications used in his network.
Also, he may be interested in checking certain policies on which
applications are prohibited to be installed on self-administrated
devices, e.g., cloud storage applications due to privacy concerns
or crypto mining software. Security consultants and pentesters
may also be interested in this information in order to determine
potential targets within a network.

There are a variety of network scanning and mapping tools
such as [9, 18, 22] that enable the generation of such an overview.
These tools are either based on actively scanning the network or
based on deep packet inspection and are thus rather complex and
application specific. The same holds for other active approaches for
fingerprinting operating systems based on ICMP [4] or TCP SYN
packets [12]. Passive approaches for desktop computers so far focus
on fingerprinting operating systems (rather than applications) and
are based on flows [14, 21], TCP/IP header information [16], TLS
handshakes [11, 13], or DNS traffic [5, 19]. Only on the mobile side
fingerprinting of apps in general [7, 20, 25, 27] has received some
attention. Fingerprinting desktop applications based on passive
monitoring has not yet been explored beyond fingerprinting DNS
server software [6], browsers [11], broader process families [2], and
malware [3].

In this paper we present the design and implementation of a
pipeline for rule-based application fingerprinting, including tools

1

for automated network traffic labeling, rule-extraction, and analysis
of passively monitored DNS traffic. The Domain Oriented Network
Unmasking Tool (DONUT) covers the analysis part of the pipeline
and identifies applications running on devices based on rules that
represent application-specific patterns of domains queried by the
devices. The Rule-Extractor automatically extracts rules for DONUT
from application-labeled DNS traffic and obtains the labeled traf-
fic from our novel Automated Traffic Labeling Software (ATLAS).
Note that ATLAS is of interest independent of the context of the
pipeline presented in this paper as it allows for labeling any net-
work traffic on a Windows host with the application that initiated
the traffic on a per-packet basis. We demonstrate the rule extraction
process on a variety of commonly used applications and evaluate
the performance and functional correctness of DONUT’s analyzing
capabilities on a self-generated dataset with known ground truth.
We illustrate DONUT’s real-world applicability by discussing its
analysis results on a large-scale real world dataset from an open
university network for which the ground truth is unknown.

2 RELATED WORK
Fingerprinting operating systems or application software has been
studied using a multitude of different approaches. These approaches
can broadly be separated into approaches based on actively scan-
ning the network and those that are based on passive monitoring.
In addition, they differ greatly in what they fingerprint (operating
systems, or specific or general applications) based on which type
of traffic (e.g. HTTP or SSL/TLS). In the following, we first discuss
prior approaches for passive fingerprinting of operating systems
and then carry on with the passive approaches for fingerprinting
specific (mobile) applications. We then briefly discuss active fin-
gerprinting approaches and place our work in the more general
context of DNS privacy.

Most closely related to DONUT is the passive approach to fin-
gerprinting OSs based on DNS traffic presented in [19]. It is based
on characteristic queries and timing patterns. Another approach
based on characteristic domain names in DNS queries is [5], where
machine learning techniques are used to fingerprint OSs. Both of
these approaches are similar to our approach in that they focus on
passively monitored traffic. However, as opposed to DONUT these
approaches focus on operating systems alone, while DONUT also
fingerprints applications.

The other passive approaches for OS fingerprinting focus on
monitoring protocols other than DNS and are thus less well suited
to be extended to fingerprinting applications in general. For exam-
ple, in [13] user agent fingerprinting based on SSL/TLS handshakes
is used to determine the OS of a device. Extracting features for OS
classification from passive analysis of TCP/IP header information
was suggested in [16]. [31] uses Support Vector Machines for OS
detection and discovery of unknown fingerprints. In [1] a method
for automatic detection of packet features for OS fingerprinting

 
 
 
 
 
 
is presented. Other authors use network flow analysis for OS fin-
gerprinting [14, 21]. [26] use packet-level signatures to detect the
typical behavior of smart home devices. Besides OS fingerprint-
ing, there are some approaches for passively fingerprinting specific
desktop applications like DNS server software [6] based on flows or
browsers [11] based on TLS handshakes. Both of these approaches
focus on one specific type of application and cannot easily be ex-
tended to fingerprinting applications in general. Additional work
on application fingerprinting based on TLS [2, 3] was done by cre-
ating a large knowledge base containing TLS fingerprints together
with contextual data like processes, operating systems and IP ad-
dresses to train a classifier that is able to detect process families
and malware.

Other work focuses on fingerprinting of smartphone apps. [25]
develop a system to fingerprint mobile apps based on flows of en-
crypted traffic produced by them. [20] use key-value pairs in URLs,
HTTP header fields, and traffic flows to automatically generate
fingerprints for mobile apps. In [7] the identification of Android
apps based on HTTP traffic is investigated. [27] developed a semi-
supervised approach to fingerprint apps based on encrypted traffic,
which allows to fingerprint previously unseen apps.

DONUT allows for rule-based fingerprinting of applications
based on passively monitored DNS traffic. The rules of DONUT can
be automatically extracted from application-labeled network traffic
generated by our novel network traffic labeling tool ATLAS, which
makes our pipeline for application fingerprinting scalable.

Active approaches for fingerprinting operating systems include
[4], where ICMP is used and [12], which is based on TCP SYN
packets. More generally, network scanning and mapping tools such
as [9, 18, 22] allow for the detection of the components within
a network as well as the connections between them and thereby
determine the current topology of the network and gather various
types of information on the devices running in the network includ-
ing the operating systems they run on and applications installed on
them. These tools are typically based on actively scanning the net-
work in combination with deep packet inspection. As such, active
approaches are more invasive than the passive DNS-monitoring-
only approach taken by DONUT, which does not require any active
network access.

The privacy leakage generated by DNS traffic has been under
much discussion in recent years [8] and has led to novel suggestions
to enhance DNS to offer more privacy (e.g [10, 17, 28]) or to replace
DNS by a more privacy-preserving alternative. It has been shown
that DNS traffic allows for a re-identification of individual users [15].
However, how much information is revealed about the network
itself by the DNS traffic generated by the devices in the network is
less well studied so far. DONUT contributes to this discussion as it
demonstrates that passive monitoring of unprotected DNS traffic
at least reveals what software is installed on devices in a network
to everyone with access to the DNS traffic. Note that even if DNS
traffic is protected by DoH or DoT, the DoH or DoT server may still
infer this information as it has access to the full DNS traffic.

3 SETTING AND TOOL DESIGN
Before presenting our approach for data labeling and rule gener-
ation, we describe the setting in which DONUT is supposed to

2

Schäfer et al.

Figure 1: Possible deployment settings for DONUT: green
shapes depict possible deployment positions, red shapes de-
pict positions where DONUT cannot be used.

be used, its detailed design, and provide an overview on how it is
implemented. We start with the setting, then provide a high-level
description of the overall structure of the tool, describe its main
modules and explain how they interoperate.

3.1 Deployment setting
We start with a description of the deployment settings which DONUT
is designed for. The general goal is to capture and analyze DNS
traffic (including the IP addresses of the clients initiating the traffic)
in order to draw conclusions about the software that is running
on each device in a network. This information is mainly useful
for network administrators, e.g., to detect unwanted software in
Bring Your Own Device (BYOD)-networks or to provide additional
context about hosts when handling security incidents.

In general, DONUT monitors unencrypted domains as well as
the IP addresses of the querying hosts. Figure 1 depicts possible
deployment positions for DONUT using different combinations
of DoH/DoT with an internal or external DNS server. The green
shapes indicate positions where DONUT can be deployed, while the
red shapes indicate positions where either access to unencrypted
domains or access to IP addresses is missing. If an internal DNS
server is used, the monitoring can be co-located with the internal
DNS server. In this case, the internal DNS server could even apply
encryption with DoH/DoT and DONUT could still be used, e.g., by
the local network administrator. If an external DNS server is used,
DONUT can also be co-located with the server. However, it can only
be deployed internally if no DoH/DoT is used, e.g., at a local router.
In case a DNS resolver sits between the hosts and the DNS server,
e.g., if each subnet uses a separate resolver for caching, DONUT
can only be co-located with the resolver because the individual IP
addresses are not visible to the internal or external DNS server.

We consider DONUT’s main use-case to be a tool for analyzing
hosts in BYOD networks, especially university networks, where it
is common that a central DNS server is used by most of the hosts.
This allows to capture the DNS traffic at one place, assuming no
additional DNS resolver or NAT configuration is present between
the hosts and the DNS server. In such networks it is especially

A Pipeline for DNS-Based Software Fingerprinting

Figure 2: DONUT’s analysis pipeline including its most im-
portant modules. The arrows are labeled with ingoing and
outgoing data, and the circular arrows depict repeating com-
munication.

useful to be able to detect running applications because the devices
are not centrally managed, which makes it hard to detect unwanted
software like crypto-miners and to handle security incidents like
malware infections.

3.2 Design and Implementation of DONUT
DONUT takes DNS traffic (pcap files or streams) as input, analyzes
it based on pre-configured rules, and after some post-processing,
outputs information on the devices that produced the traffic. The
input traffic can be captured by tools like TShark[29] and can either
take place prior to the analysis or online. The DNS traffic can, e.g.,
be captured at the router of a network or at the DNS Server di-
rectly. The latter allows the usage even if encrypted DNS like DoH
is used. Depending on the rules used, DONUT’s output includes
the applications and services running on each device. Due to its
modularity, DONUT has the potential to also produce information
about operating systems or NAT configurations. However, in this
paper we focus on the core functionality of identifying applications.
To make the operation of DONUT as easy as possible, we devel-
oped a basic web-based user interface in addition to command-line
enabling visual inspection of the results.

DONUT uses a rule-based approach and is designed to be as mod-
ular and extensible as possible. In this paper, we discuss two types
of rules, namely CF-rules and Set-rules. A CF (context free)-rule
simply encodes a specific queried domain and adds a corresponding
label to all IPs querying that domain. The Set-rules then combine
these labels in a post-processing step to produce the final results,
i.e., the applications that most likely produced this combination of
DNS queries. Both types of rules are generated automatically based
on labeled data, which we describe in Section 4.

As depicted in Figure 2, DONUT is composed of three main mod-
ules, and an IP dictionary. The parser extracts the relevant fields
from each DNS packet, i.e., IP, domain name, and query type, from
the traffic and puts them into a processing queue. The analyzer mod-
ule, containing the CF-Matcher, pulls the packets successively from
the queue, checks whether they match to a CF-rule, and updates
DONUT’s data structure accordingly. In general, the CF-rules only
take information of a single DNS packet into account. More com-
plex rules, e.g., encoding sequences of packets, can be added in the
future due to DONUTs modularity. Finally, in the post-processing
module, the Set-matcher processes the stored labels according to
the Set-rules, and computes the list of identified applications for
each IP. The main data structure of DONUT is a dictionary for
each IP address active in the network traffic. In the following, we
will refer to this data structure as IP dictionary. All information

3

Figure 3: Structure of the CF-Matcher, which adds labels to
the IP dictionary depending on which rule has matched to
the host’s DNS queries.

Table 1: Entries of the CF-Matcher rules, which are com-
pared to each packet in the pcap file or stream.

Entry

Description

Domain name The domain name to match.
Query type
Label

The query type(s) of the packet to match.
A list of labels to indicate that this rule has matched.

gathered by the Analyzer, i.e., the labels added by the CF-rules, are
stored in that data structure. All rules and mappings to application
names are stored in configuration files.

The CF-Matcher analyzes domain names of single packets with-
out taking other context into account, as depicted in Figure 3. Ap-
plications use DNS to resolve the IP addresses of their remote
communication endpoints. Hence, the set of DNS queries sent by
an application can already be unique to this application and is thus
usable to fingerprint an application without taking additional in-
formation into account. Each CF-rule contains the entries listed
in table 1. The Domain name and the query type are compared
with all packets from the pcap file or stream by the analyzer. If the
entries match to a packet, the list of labels, which is usually just
one, is added to the dictionary of the corresponding IP address. In
case multiple applications query the same domain, multiple labels
for different applications can be added by one CF-rule.

The Set-Matcher operates as a post-processing step once the
analysis of the pcap file or stream is finished. Additionally, it can
be configured to be executed regularly during analysis to compute
preliminary results, e.g., every few minutes or after a certain amount
of packets. Its task is to combine all information gathered during
the analysis and compute corresponding implications, i.e., which
applications are running, based on the labels added by the CF-
Matcher. Each application is represented by one Set-rule, containing
the entries listed in Table 2. Since the Set-Matcher is not part of
the analyzer, its rules do not contain packet entries as prerequisites.
This implies, that the Set-rules can also be changed after the packet
analysis is already finished, as long as they only contain labels
of CF-rules that were used during the analysis. This allows for a
later reconfiguration of Set-rules as needed, e.g., changing of rule
parameters to improve false positives or false negatives.

The prerequisites of Set-rules are a combination of required la-
bels, optional labels, and a threshold. All required labels must be
present for a rule to match. Of the optional labels, at least the stated
minimum number of labels need to be present, in addition to the
required labels. The motivation for this differentiation is that appli-
cations often connect to a (potentially small) set of communication
endpoints, and therefore query the corresponding domains on each

Schäfer et al.

Figure 5: Complete fingerprinting pipeline, including data
generation with ATLAS, rule extraction, and analysis with
DONUT.

as long as it is possible to directly capture the traffic on the central
DNS server of the network.

4 RULE GENERATION
In this section, we describe our methodology for the automated
generation of rules for DONUT. We present our tool ATLAS used
for traffic labeling, describe the procedure for data generation for
a selection of applications to test our approach, and present the
process of rule extraction in detail.

The process for the generation of rules is composed of two parts,
namely (1) data generation and labeling, and (2) rule extraction.
The resulting rules are then used by DONUT for application finger-
printing. The complete pipeline is depicted in Figure 5. For labeling,
we developed the Automated Traffic Labeling Software (ATLAS),
which is used to label network packets by the process name that cre-
ated them with ground truth accuracy. We used ATLAS on multiple
virtual machines, where we installed and interacted with a number
of applications to generate the DNS traffic for labeling. Based on
this data, we used the Rule Extractor to generate rules for DONUT.

4.1 Labeling
In the following, we describe ATLAS in more detail. It monitors net-
work traffic and system events and currently it supports Windows
10. ATLAS includes a simple GUI to start and stop the monitoring
and to adjust settings, e.g., the network protocols which should be
monitored. The way network packets are handled differs between
operating systems, however, for future work we plan to add support
for other operating systems like Linux or MacOS. ATLAS is able to
label packets of all network protocols with the corresponding appli-
cation, of which we only need DNS for this paper. The tool consists
of mainly three components: event logging, packet capturing, and
packet labeling.

The event logging component uses the Windows Process Mon-
itor (procmon), which monitors system events in real-time, e.g.,
file system access or process activity. It logs all events occurring
on a machine together with the application that caused the event.
These events are then filtered for network events, and parsed, using
wtrace[30]. In addition to procmon, the logging component polls
tasklist.exe periodically to receive the process ID of each process.
Also, it activates and parses the DNS event log of Windows, which
is used to track the process IDs of applications calling the system’s
DNS resolver. For each event, the timestamp, process name and ID,
source and destination IP, as well as source and destination port is
forwarded to the labeling component via a queue. Simultaneously,

Figure 4: Structure of the Set-Matcher, which combines the
labels added by the CF-Matcher in order to conclude which
applications are running on a host.

Table 2: Entries of the Set-Matcher rules, which combine the
information gathered in the analysis step.

Entry

Description

Labels required to be present.

Required labels
Optional labels Optional labels.
Min. optional
ID implication

Minimum number of required optional labels.
ID of implied application.

start or at regular intervals during runtime. Other DNS queries
might only be triggered when a certain function of the application
is used, e.g., during updating or an actively initiated video call. The
idea is to configure domains observed in most runs or instances
during the rule generation process as required and all domains
queried during only a few runs as optional. The goal of optional
labels, which are chosen less conservative as required labels, is to
reduce the number of false positives, especially in case the list of
required labels is short. The minimum number of optional labels
can be used to adjust the risk of false negatives. Another use-case
for the concept of optional labels is to be able to include domains
that are also queried by other applications or are likely to be queried
by a user, e.g., while browsing the web. In this case, the minimum
number is used as a threshold to reduce false positives while the
required labels ensure that, e.g., domains likely to be queried by
a user alone are not sufficient for a Set-rule to be triggered. The
process of generating these rules is automated and described in
more detail in Section 4.

3.3 Limitations
We presented DONUT as a tool to fingerprint applications based on
DNS traffic. For obvious reasons, this works only for applications
that use DNS in a way that is distinguishable from other appli-
cations. However, even applications that can operate completely
offline often produce DNS traffic, e.g., when searching for updates.
If two applications, e.g., from the same vendor, query the same
set of DNS queries, it is not possible to distinguish them without
additionally analyzing other parameters like timing or order of
queries. Also, it is clear that our fingerprinting method can be cir-
cumvented by manually querying all domains used in CF rules to
obfuscate the actually running applications. However, our goal is
not to create a tool capable for a scenario where users actively try
to hide or manipulate their traffic, but rather to build a tool to gain a
quick overview of applications running in a network, e.g., to detect
unwanted software as a network administrator. As discussed in
Section 3, DONUT can still be used if the DNS traffic is encrypted,

4

A Pipeline for DNS-Based Software Fingerprinting

the capturing component uses TShark to capture network traffic in
real-time. For each packet, the frame number, timestamp, source
and destination IP, as well as source and destination port, are parsed
and forwarded to the labeling component via another queue.

The labeling component receives system events and network
packets and attempts to match each event to a packet. This is done
by comparing the IP addresses and ports of an event to the packets
from the queue. If all parameters match and the timestamps of
event and packet are within a small time frame, the packet with
the smallest time-delta is labeled with the corresponding process
name, and packet as well as event are deleted from their queue. If
the timestamps of event and packet are slightly off, it is possible
that the event used for labeling the packet is not the event that
caused the exact packet, e.g., if the corresponding process generates
many packets in fast succession. However, because the combination
of IP addresses and ports can only be used by one process at a
time, this still results in correctly labeled packets. Considering
DNS, applications use the DNS resolver of the operating system
to resolve a URL. Hence, since procmon reports which process
triggered a network event, this would result in all DNS packets
being labeled with the DNS resolver process. To solve this, the
DNS packets are compared to the DNS events also collected by
the logging component, the events with the smallest time delta
are matched, and the process ID in the DNS event is replaced by
the process name for the label, using the information collected
from tasklist.exe. In order to label packets without a preceding
system event, e.g., follow up packets of a TCP connection, the tool
additionally tracks all connections that were successfully labeled at
least once. Packets that cannot be matched directly with a system
event are compared to known connections, defined by source and
destination IP addresses and ports, in order to assign the correct
label.

The output of ATLAS consists of the pcap file captured by TShark
and a csv file which contains the frame number for each packet
from the pcap, timestamp, source and destination ports and IP,
process name and id, and the domain in case of DNS packets. In
summary, this results in a labeling coverage of almost 100%, which
would not be possible by simply polling open network connections
because of short-lived connections such as DNS. The only packets
that cannot be labeled are the ones without a preceding system
event, that belong to a connection, which was opened before the
labeling process started, and closed after the labeling stopped, e.g.,
TCP keepalive packets.

4.2 Data Generation and Application Selection
Next, we describe the process of generating network traffic for a
selection of applications. We use ATLAS to label this traffic in order
to automatically extract rules that can be used for fingerprinting
with DONUT. We briefly describe our data generation setup, moti-
vate the selection of applications, and outline how we interacted
with the applications in order to generate DNS traffic. We used
Proxmox Virtual Environment[24] where we set up virtual ma-
chines with Windows 10. On each VM we installed ATLAS as well
as the applications we want to generate labeled network traffic for.
Each application was installed on multiple VMs and interacted with
repeatedly in order to generate as much DNS traffic as possible.

5

We selected the applications based on two categories. First,
widely used applications that are installed on most office com-
puters, and second, applications that might be unwanted in some
networks due to organizational policies. We look at widespread
applications in order to test the generated rules on an unlabeled
real world dataset. With the second category, we aim to demon-
strate another practical use case of our fingerprinting approach, i.e.,
finding unwanted software in a network as an administrator. For
the "widespread" category, we installed the browsers Chrome, Fire-
fox, and Edge; Microsoft Office including Word, Excel, Powerpoint
and Outlook; Skype and Zoom as video conference software; and
Sophos as an antivirus solution. The latter was selected because
it is widespread in the RWTH university network for which we
obtained a large real-world dataset in order to test DONUT. As
potentially unwanted software we installed crypto-miners, namely
Easyminer and Nicehash; cloud storage software OneDrive and
Dropbox due to privacy concerns; the gaming platform Steam; and
Teamviewer because of its potential for unauthorized access. Addi-
tionally, all traffic from Windows was captured to generate rules
for the operating system processes itself.

We manually interacted with all applications, started and stopped
each applications repeatedly, and passively ran the applications for
multiple hours. For example, we created accounts for each applica-
tion that require a login, e.g., Outlook, Skype, and Zoom; used the
application’s main functionality, e.g., browsing, video calls, crypto
mining, or virus scans; and triggered updates. This most likely does
not result in the complete spectrum of possible DNS traffic for each
software, however, it generally produced a sufficient amount of traf-
fic to be used for rule extraction. In total, we generated and labeled
network traffic for 14 applications as well as Windows 10 resulting
in 989.922 packets in total, of which 61.407 are labeled DNS packets
containing 10.096 different queries. Note, that 8.514 of the 10.096
unique domains were queried by Sophos alone, because Sophos
excessively uses DNS to lookup IP addresses and presumably file
hashes during malware scans.

4.3 Rule Extraction
After covering the process for labeling and data generation, we now
describe our approach of automated rule extraction for DONUT’s
fingerprinting capabilities. The Rule-Extractor consists of three
main components, namely a parser, the CF-Rule generator, and the
Set-Rule generator. It also implements filtering functionality for
the data and rules. For each component, several parameters can be
adjusted to configure the logic for rule extraction.

The parser takes a number of csv files generated by ATLAS as
input and extracts all labeled DNS packets. The labels are com-
pared against a whitelist filter, which is the only part that requires
manual input. The whitelist specifies all processes that should be
considered for rule extraction, which is necessary for two reasons.
First, to exclude irrelevant processes, e.g., sub processes from the
operating system that are not of interest, and second, to combine
multiple process names into one application name. For example, we
combined the most common present processes of Windows 10 (e.g.,
svchost.exe and sihclient.exe) under one label identifying Windows
10. As another example, the application Teamviewer consists of
two processes, namely temviewer.exe and teamviewer_service.exe,

Table 3: Criteria used to decide which domain labels are used
in a Set-rule for one application.

Table 4: Configurable parameters used for Set-rule genera-
tion based on the criteria listed in Table 3.

Criterion

Description

Criterion

Description

Schäfer et al.

isUnique

occurence

Boolean value, specifying whether a domain
was queried by only one or multiple applica-
tions.
Value between 0 and 1, depicting the portion
of application instances in the dataset where
the domain was queried relative to the total
amount of instances of this application.

medianInterval Median interval between queries of the same
domain by one application.

Max. required labels in a Set-rule.
Max. optional labels in a Set-rule.

maxReqLabels
maxOptLabels
minReqOccurrence Min. occurrence value for required labels.
minOptOccurrence Min. occurrence value for optional labels.
minOptProbability

maxReqMedian
maxOptMedian
allowOptNonunique

Desired probability that the minimum number
of optional labels (see Table 2) are present,
based on the occurence value.
Max. median interval for required labels.
Max. median interval for optional labels.
If True, domains queried by multiple applica-
tions can be used for optional labels.

which we also combined under one label. All processes that are
not whitelisted are combined under the label other, in order to not
loose the information, that some domains queried by a whitelisted
process might also be queried by another one not on the whitelist.
Another filtering step of the parser is comparing all domains to
the Tranco list [23], containing the most popular websites. We do
this as an easy way to filter domains that are likely to be queried
by a user, e.g., through a browser, in order to lower false positives.
Additionally, we filter queries containing the local domain of the
network our data generation VMs operate in, to prevent that such
a domain ends up as required in a Set-rule, which would lead to
many false negatives in a different network environment.

The CF-rule generator creates a preliminary list of CF-rules based
on the parsed information. CF-rules only imply labels instead of
applications directly and some of them are not used in any Set-rule.
In this case, the corresponding CF-rule can be removed. However,
to allow for dynamic re-configuration of Set-rules after the analysis,
it makes sense to still use all available CF-rules to increase the num-
ber of usable labels for Set-rules. For each whitelisted application,
one CF-rule with a unique label for each queried domain of this
application is generated. The Set-rule generator creates one rule for
each application based on the labels from CF-rules. Which domains
are used for the Set-rule of one application depends on the three
criteria, shown in Table 3.

The boolean value isUnique indicates whether a domain was
queried by one or multiple applications in the dataset. In the latter
case, the domain’s label is disregarded as required label in the
Set-rule. However, non-unique queries can be used for optional
labels, since they only become relevant if all required labels of an
application are present as well.

Occurrence indicates, how many different instances of an appli-
cation in the dataset queried the domain. In case of, e.g., a browser,
most domains resulting from browsing the web will most likely have
a lower occurence value, if the dataset contains enough instances of
the browser with different browsing behavior. This is because not
all users browse the same websites, e.g., www.scholar.google.com
will only appear in a fraction of browser instances recorded. On
the other hand, domains that are queried by the browser without
user interaction, e.g., checking for updates, will have a higher oc-
curence value. In the Set-rule generator, this value is, e.g., used to

6

decide whether a label should be required or optional, as described
in Chapter 3.

The third criterion, medianInterval, represents how often a spe-
cific domain is queried by one application instance, defined by the
median interval between two repetitions of the same query. We use
median instead of average to reduce the influence of time frames
where an application is not running in the dataset. This value is
used to prioritize the domains, i.e., labels, that are used for a Set-rule.
The higher the frequency of a domain, the higher the chance that a
domain is queried by an application during the runtime of DONUT.
The three criteria are computed for each domain of an appli-
cation. Defined by configurable parameters, listed in Table 4, the
Set-rule generation works as follows. For an application, each la-
bel, corresponding to a domain of the application, is classified as
a candidate for required, optional, or nothing. If a label has an oc-
currence value larger than minReqOccurrence and a medianInterval
value lower than maxReqMedian, it is considered a candidate to be
a required label. The minReqOccurrence parameter ensures that the
domain was queried by a large enough portion of application in-
stances, while maxReqMedian ensures that the domain is expected
to be queried regularly. This ensures that each required domain
is expected to be queried by future instances of the application
and that it is expected to occur in a small enough time frame. The
same categorization is done for optional labels. However, with less
restricted parameters, since only a portion of optional labels need
to be present for a Set-rule to trigger.

Once the candidates for required labels are selected, the list is
sorted by their median interval in ascending order to prioritize
domains queried more frequently, such that the resulting Set-rule
can trigger as early as possible on live monitored traffic. If the list
contains more than maxReqLabels, the excess candidates are added
to the list of optional candidates and the most frequent ones are
selected as required labels. Finally, the list of optional candidates is
sorted by the median interval as well, and at most maxOptLabels
are selected.

For required labels, we only allow domains that were queried
by exactly one application in the dataset. For optional labels, the
parameter allowOptNonunique controls whether domains queried
by multiple applications are allowed. However, even if only unique

A Pipeline for DNS-Based Software Fingerprinting

Table 5: Evaluation datasets containing DNS traffic of real-
world and artificial networks.

Dataset

Source

Size

# Hosts

Validation set
Performance set RWTH university network
RWTH university network
Real-world set

Virtual machines

9.273 DNS packets
1M DNS packets
216M DNS packets

4
23.455
43.961

domains queried by exactly one application in the dataset are con-
sidered, there is a chance that a domain is also queried by an appli-
cation which is not in the dataset, or that a domain is queried by a
user instead of an application. By limiting the number of domains
in rules and prioritizing domains that are queried often and by
most application instances in the dataset, the chance of including
a user dependent domain in the rules is limited. Any domain can
be queried by a user manually, however, the goal of this pipeline is
not to be robust against active avoidance, e.g., if someone manually
queries domains of all DONUT rules.

The number of optional labels that need to be present in order
for a Set-rule to trigger is calculated based on minOptProbability,
representing the desired chance that this number of queries is
actually present based on the occurrence value. We interpret the
occurrence value as the chance that a domain is queried by an
application instance, as it represents the portion of instances in
the dataset which queried the domain. The minimum number of
optional labels is then equal to the maximum number n, such that
the chance of at least n domains being queried is greater than
minOptProbability.

In Section 5, we select a baseline set of parameters and discuss the
impact of each parameter on the accuracy measured on an artificial
validation dataset with known ground truth. The selected set of
parameters is then used to test DONUT on a real-world dataset
with unknown ground-truth.

5 EVALUATION
In the following, we present the evaluation of performance and
functionality of our pipeline. We first introduce the datasets used for
evaluation. Next, we discuss performance and resource consump-
tion of ATLAS, rule extraction, and DONUT. We analyze the impact
of different parameters for Set-rule extraction and evaluate the
correctness of the analysis on an artificial validation dataset with
known ground truth. Finally, we discuss the output of DONUT on
a large real world dataset for which the ground truth is unknown.

5.1 Evaluation Datasets
We use the three datasets for evaluation of performance and func-
tionality of our pipeline, listed in Table 5. The validation set was
manually created using four virtual machines in the same environ-
ment as descried in Section 4. We installed the same applications
that we created rules for on three virtual machines in random com-
binations, and interacted with them in a similar way. Additionally,
we installed all applications on a fourth VM, but used them for
only one minute before closing them again, in order to produce
edge cases for our evaluation. Secondly, we used a large real-world
dataset captured roughly one year before writing this paper on the
edge routers of the RWTH university network. The dataset contains

7

only DNS traffic and all IP addresses were anonymized with the
tool capsan1 before it was sent to us. We split this into a small set
for performance evaluation and a large set for functional testing.
The latter contains 7.5 hours of traffic produced by 43.961 hosts. We
have no ground-truth for this dataset. While this does not impact
the performance evaluation, we can only discuss the plausibility of
the analysis results on the real-world dataset and cannot evaluate
false positives or false negatives.

5.2 Performance
Next, we present the performance evaluation of ATLAS, rule-extraction,
and DONUT. The measurements were done on a virtual machine
running on a server with an AMD Epyc 7702P processor, with an
allocation of 8 cores and 16GB memory. ATLAS managed to label
everything in real-time with a small delay, because matching pack-
ets to events are searched for in a small time window. This was
achieved while labeling all network packets, however, peaks while
buffering videos for example, also caused the buffer of unlabeled
packets to rise temporarily. During labeling, CPU utilization ranged
between 5% and 15%, depending on the amount of packets in the
buffer. When only labeling DNS packets, which is sufficient in our
context, no delay occured because DNS corresponds to only a frac-
tion of the overall produced packets. Hence, we didn’t focus on
further optimizing the tool’s performance.

The rule extraction performance largely depends on the max-
imum number of optional labels for Set-rules, because of the ex-
ponential complexity of the calculation for minOptProbability de-
scribed in Section 4. However, even with 20 optional labels for the
number of applications we generated data for, the rule extraction
took less than 30 seconds. Note, that we did not focus on opti-
mizing performance for ATLAS and rule extraction yet, as their
performance is sufficient for our current experiments.

For DONUT, we measured the number of packets that can be
analyzed per second in order to determine whether traffic of large
networks can be analyzed in real-time. DONUT’s performance de-
pends on TShark, which is used as parser, and on the CF-Matcher.
Performance of the latter does not depend on the number of rules,
hence it scales well even with a large number of applications. On
average, the analysis on our performance dataset took 67 seconds,
corresponding to almost 15.000 packets analyzed per second. We
measured the performance with a varying amount of CF-rules.
However, the runtime stayed constant which makes DONUT scal-
able to support rules for many applications. The real-world RWTH
dataset contains 216M DNS queries with 44.000 active hosts over
the course of 7.5 hours which corresponds to 8.000 packets per
second on average. Even when the amount of traffic peaks tem-
porarily, DONUT’s performance is sufficient with a maximum of a
few seconds delay when traffic exceeds the capabilities and packets
need to be buffered. To scale our approach for even larger networks,
multiple instances of DONUT can run simultaneously, e.g., one for
each subnet. For our evaluation, one instance was sufficient.

5.3 Rule-extraction Parameters
Now we will discuss the impact of the parameters for Set-rule
extraction, listed in Table 4, on the classification, and determine a

1capsan: a pcap anonymizer, from https://github.com/jsiwek/capsan

Schäfer et al.

Figure 6: Histogram of median intervals between queries of
the same domain by one application (left), and occurrence
values of all domains with a median interval of 1400 and
less (right).

set of parameters to test DONUT on the real-world dataset. We start
with a baseline for the parameters and compute the false positives
and false negatives for different amounts of required and optional
labels for the Set-rules. Afterwards, we discuss the impact of the
parameters on the classification of our validation dataset.

For minOptProbability, we set a value of 0.5. Figure 6 (left) shows
the distribution of medianIntervals among all domains. Most do-
mains have a median interval between 1 and 1400, with occasional
peaks above 1400, hence we set maxReqMedian and maxOptMedian
to 1400. Since labels are prioritized by low median values, we do
not set a lower maximum for required labels. Figure 6 (right) shows
the distribution of occurrence values for all domains with a median
interval below 1400. Many domains have an occurrence value below
0.4, and another peak can be seen at a value of 1.0. We set minRe-
qOccurrence to 1.0, such that only domains that were queried by all
application instances are required to be present. For minOptOccur-
rence, we chose a value of 0.6, in order to require that more than
half of all application instances queried the domain during data
generation. We set allowOptNonunique to true, in order to allow for
more potential labels, especially for applications with a low total
number of queried domains. Since we have no way to determine
good values for maxReqLabels and maxOptLabels, we evaluate the
classification results of our validation set with all combinations of
0-10 required and optional labels.

5.4 Validation Set Results
We used DONUT to analyze the validation dataset using rules ex-
tracted with the described parameters. Figure 7 shows the false
positives with different combinations of maxReqLabels and max-
OptLabels. It shows that the number of required labels in Set-rules
has almost no impact on false positives, and for 3 or more optional
labels we observe zero false positives on the validation set. Figure
8 shows the false negatives for different number of labels, once
without the host where each application was only ran for one
minute (left), and once including this host (right). Without these
edge cases, roughly half of all combinations result in zero false neg-
atives. With the edge cases, the lowest number of false negatives
is five for all combinations, excluding the ones with zero optional
labels. The false negatives are caused by not detecting Dropbox,
Zoom, Teamviewer, Sophos, and Windows 10. Apparently, for these
applications and the operating system itself, the amount of DNS
traffic during the short time frame was not enough for detecting

Figure 7: Heatmap of false positives on the validation datset
for all combinations of 0-10 required and optional labels in
Set-rules.

Figure 8: Heatmap of false negatives on the validation
dataset, without short application run times (left) and with
short application runtimes (right), for all combinations of
0-10 required and optional labels in Set-rules.

them. However, we successfully detected the remaining 9 appli-
cations. We conclude that our fingerprinting works well on the
validation set, with zero false positives and only a few negatives
for applications monitored for a short period of time only. For the
real-world dataset, we decide to use a maximum of 4 required labels
and 7 optional labels for our Set-rules, to minimize the amount of
false negatives.

Next, we describe how changing the rule-extraction parameters
impacts the classification. For values of minReqOccurrence below
1.0 we observed an increment in false negatives, presumably be-
cause some of the additional required labels are not present in the
validation set. For minOptOccurrence, values below 0.5 caused more
false negatives, especially for the detection of Windows 10, while
values above 0.65 resulted in zero labels for Sophos. The reason for
this is that Sophos mostly queries domains containing IPs or file
hashes, each with low occurrence values, while we observed no
domain that was queried by all Sophos instances. With values above
0.5 for minOptProbability, we observed an increasing amount of
false negatives, especially for Windows 10. We observed no mean-
ingful impact when lowering maxReqMedian besides an increment
in false negatives below 120. Dropping maxOptMedian below 1200,
resulted in an increasing amount of false negatives for Windows
10. With allowOptNonunique set to false, we also observed more
false negatives. The same holds for not excluding domains from the

8

A Pipeline for DNS-Based Software Fingerprinting

Figure 9: Heatmap of false positives (left) and false negatives
(right) on the validation datset with a minOpt value capped
to 4, for all combinations of 0-10 required and optional la-
bels in Set-rules.

Table 6: Fingerprinting results of DONUT applied to the real-
world dataset, with and without capped minOpt.

Software

capped uncapped

Software

capped uncapped

Windows 10
Firefox
Office
Dropbox
Chrome
Sophos
Edge

4739
2903
1943
1045
878
861
840

1026
0
277
812
879
861
796

Onedrive
Skype
Zoom
Steam
Teamviewer
Nicehash
Easyminer

815
754
414
184
87
1
0

0
0
0
138
0
1
0

Tranco list. In general, changing the parameters mostly impacted
the amount of false negatives, while the amount of false positives
only increased when changing the parameters drastically.

When manually inspecting the extracted rules, we noticed that
false negatives mostly occurred for applications where the minOpt
value is equal to or slightly lower than the total amount of optional
labels. This could be an indication for overfitting, because we in-
teracted with the applications using repeatable patterns, instead of
collecting data from real-world machines. Therefore, we artificially
capped the value for minOpt as an attempt to further reduce false
negatives. With a maximum value of 4, we still end up with low
false positives while false negatives are reduced, as seen in Figure
9. We achieve the best results with zero required labels and 6-8
optional labels.

5.5 Real-world Dataset Results
We now briefly discuss the analysis results on the real-world RWTH
dataset using rules extracted as described above, with and without
a cap for minOpt. The analysis took 3:55 hours, which equals to
15.300 packets analyzed per second. Note, that the dataset was
recorded one year before rule generation, because we had no access
to a more recent one. Hence, our rules were generated for newer
versions of the applications. Also, because we extracted our rules
from a small artificially generated dataset and test on a real-world
dataset, our rules might be overfitting to the behavior of our VMs.
Table 6 shows DONUT’s detected fingerprints in the real-world
dataset. With capped minOpt, we detected considerably more fin-
gerprints. For some applications, the detections jumped from zero
to several hundreds or thousands. We assume that our rules are
overfitting because of the artificially generated data. In the capped

9

scenario, we detected Windows on 11% of all active hosts. Of the
hosts where no application was detected, 48% sent less than 100
DNS packets overall. Note, that the dataset contains desktops, lap-
tops, and mobile phones, while we only created rules for Windows.
We didn’t find many fingerprints for crypto miners, which can be
due to low popularity of these applications, or different behavior of
the applications in our analyzed version compared to the real-world
dataset. Our rule extraction only resulted in one label for Nicehash
and three for Easyminer. The applications with more detections
generally also queried more unique domains during data genera-
tion, resulting in more labels to work with. Nevertheless, DONUT
still managed to detect a considerable amount of fingerprints in the
dataset. Since we measured a low amount of false positives on our
validation set, while changing the parameters mainly affected false
negatives, we also assume a low false positive rate on this dataset.

6 CONCLUSION AND FUTURE WORK
In this paper, we presented the design and implementation of a
pipeline for DNS-based application fingerprinting. We presented
DONUT, a rule-based tool that analyzes DNS traffic in order to
fingerprint software. Additionally, we developed ATLAS, a tool
for automatically labeling network traffic based on the software
that produced it, which we used to generate labeled datasets for 13
applications as well as Windows 10. Both tools will become open
source alongside the publication of this paper. We automated the
process of rule extraction based on data labeled by ATLAS, such that
our pipeline can be easily extended to support more applications
without much manual effort. Our performance evaluation shows
that DONUT can analyze traffic of large networks at line-speed,
independent of the number of rules or supported applications. We
optimized the parameters used for automated rule-extraction and
evaluated the functional correctness of DONUT with the help of an
artificially generated dataset with known ground truth, resulting in
low false positives and false negatives, even in edge-case scenarios.
Additionally, we evaluated the results of DONUT on a large real-
world dataset.

To evaluate our automated pipeline for application fingerprint-
ing, the data from virtual machines was sufficient. For future work,
we plan to collect more realistic and diverse data from real-world
machines. As privacy concerns complicate the acquisition of la-
belled real-world data, we will simultaneously work on automating
the interaction with applications to make the generation of artificial
datasets more scalable. Also, we will further extend the function-
ality of ATLAS, by adding version information to the application
labels in order to extract rules for different versions of applications.
Finally, we plan to experiment with additional ways to parameterize
the Set-rule extraction to further increase accuracy.

A ETHICS
In this work, we used a real-world dataset containing DNS traffic
captured on the edge routers of the RWTH university network to
demonstrate the capabilities of our pipeline. Before we received ac-
cess to the data, all IP addresses were anonymized with the tool cap-
san to remove the possibility of linking domains to real users. In the-
ory, the network administrator who was responsible for anonymiz-
ing the traffic can reverse this process. However, we received the

Schäfer et al.

[27] T. van Ede, R. Bortolameotti, A. Continella, J. Ren, D. Dubois, M. Lindorfer, D.
Choffnes, M. van Steen, and A. Peter. 2020. FlowPrint: Semi-supervised mobile-
app fingerprinting on encrypted network traffic. In Network and Distributed
System Security Symposium (NDSS), Vol. 27.

[28] M. Wachs, M. Schanzenbach, and C. Grothoff. 2014. A censorship-resistant,
privacy-enhancing and fully decentralized name system. In CANS. Springer.
[29] Wireshark 2022. TShark, free command-line based network protocol analyzer.
Retrieved February 10, 2022 from www.wireshark.org/docs/man-pages/tshark.
html

[30] wtrace 2022. wtrace, open source tool for live recording trace events on Windows.

Retrieved February 10, 2022 from www.wtrace.net

[31] B. Zhang, T. Zou, Y. Wang, and B. Zhang. 2009. Remote operation system detection

base on machine learning. In FCST. IEEE.

dataset with a delay, such that mappings between IPs and users
were already deleted once we had access to the anonymized data.
Hence, even if the network administrator receives access to our
analysis results, it is impossible to map any information to a specific
user. Because the datasest might still contain sensitive information,
it will not be made available to the public.

B ACKNOWLEDGMENTS
The authors would like to thank Jens Hektor for providing anonymized
DNS data of the RWTH university network. This project has re-
ceived funding from the European Union’s Horizon 2020 research
and innovation programme under grant agreement No. 833418.

REFERENCES
[1] A. Aksoy, S. Louis, and M. Gunes. 2017. Operating system fingerprinting via

automated network traffic analysis. In CEC. IEEE.

[2] B. Anderson and D. McGrew. 2019. TLS beyond the browser: Combining end
host and network data to understand application behavior. In Proceedings of the
Internet Measurement Conference.

[3] B. Anderson and D. McGrew. 2020. Accurate TLS Fingerprinting using Destina-
tion Context and Knowledge Bases. arXiv preprint arXiv:2009.01939 (2020).
[4] O. Arkin. 2002. A remote active OS fingerprinting tool using ICMP. login 27, 2

(2002).

[5] D. Chang, Q. Zhang, and X. Li. 2015. Study on os fingerprinting and nat/tethering

based on dns log analysis. In RAIM. IRTF.

[6] R. Chitpranee and K. Fukuda. 2013. Towards passive DNS software fingerprinting.

In AINTEC. ACM.

[7] S. Dai, A. Tongaonkar, X. Wang, A. Nucci, and D. Song. 2013. NetworkProfiler:

Towards automatic fingerprinting of Android apps. In INFOCOM. IEEE.

[8] DNS Privacy 2022. DNS Privacy Project. Retrieved February 10, 2022 from

https://dnsprivacy.org/

[9] Famatech Corp 2022. Advanced IP Scanner, free network scanner to analyse LAN.

Retrieved February 10, 2022 from www.advanced-ip-scanner.com

[10] H. Federrath, K. Fuchs, D. Herrmann, and C. Piosecny. 2011. Privacy-preserving
DNS: analysis of broadcast, range queries and mix-based protection methods. In
ESORICS. Springer.

[11] B. Garn, D. Simos, S. Zauner, R. Kuhn, and R. Kacker. 2019. Browser fingerprinting

using combinatorial sequence testing. In HotSoS. ACM.

[12] L. Greenwald and T. Thomas. 2007. Toward Undetected Operating System

Fingerprinting. WOOT (2007).

[13] M. Husák, M. Čermák, T. Jirsík, and P. Čeleda. 2016. HTTPS traffic analysis
and client identification using passive SSL/TLS fingerprinting. EURASIP 2016, 1
(2016).

[14] T. Jirsík and P. Čeleda. 2014.

Identifying operating system using flow-based

traffic fingerprinting. In EUNICE. Springer.

[15] D. Kim and J. Zhang. 2015. You are how you query: Deriving behavioral finger-

prints from DNS traffic. In SecureComm. Springer.

[16] R. Lippmann, D. Fried, K. Piwowarski, and W. Streilein. 2003. Passive operating

system identification from TCP/IP packet headers. In DMSEC, Vol. 40. ICDM.

[17] Y. Lu and G. Tsudik. 2009. PPDNS: Privacy-preserving domain name system. In

S&P. IEEE.

[18] G. Lyon. 2018. Nmap, a free and open source network security and discovery scanner.

Retrieved May 13, 2019 from www.nmap.org

[19] T. Matsunaka, A. Yamada, and A. Kubota. 2013. Passive OS fingerprinting by

DNS traffic analysis. In AINA. IEEE.

[20] S. Miskovic, G. Lee, Y. Liao, and M. Baldi. 2015. AppPrint: automatic fingerprinting

of mobile applications in network traffic. In PAM. Springer.

[21] S. Mossel. 2010. Passive OS detection by monitoring network flows. (2010).
[22] Paessler AG 2022. PRTG, a commercial network monitoring tool. Retrieved

February 10, 2022 from www.nmap.org

[23] V. Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczyński, and W. Joosen.
2018. Tranco: A research-oriented top sites ranking hardened against manipula-
tion. arXiv preprint arXiv:1806.01156 (2018).

[24] Proxmox 2022. Proxmox, open source platform for virtualization. Retrieved

February 10, 2022 from www.proxmox.com

[25] V. Taylor, R. Spolaor, M. Conti, and I. Martinovic. 2016. Appscanner: Automatic
fingerprinting of smartphone apps from encrypted network traffic. In EuroS&P.
IEEE.

[26] R. Trimananda, J. Varmarken, A. Markopoulou, and B. Demsky. 2020. Packet-level
signatures for smart home devices. In Network and Distributed Systems Security
(NDSS) Symposium, Vol. 2020.

10

