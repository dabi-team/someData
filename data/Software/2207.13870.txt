2
2
0
2

l
u
J

8
2

]
S
D
.
s
c
[

1
v
0
7
8
3
1
.
7
0
2
2
:
v
i
X
r
a

Engineering faster double-array Aho–Corasick automata

Shunsuke Kanda1, Koichi Akabe1, and Yusuke Oda1,2

1LegalForce Research, Japan
2Center for Data-driven Science and Artiﬁcial Intelligence, Tohoku University, Japan

Abstract

Multiple pattern matching in strings is a fundamental problem in text processing applications such as regular ex-
pressions or tokenization. This paper studies eﬃcient implementations of double-array Aho–Corasick automata
(DAACs), data structures for quickly performing the multiple pattern matching. The practical performance of DAACs
is improved by carefully designing the data structure, and many implementation techniques have been proposed thus
far. A problem in DAACs is that their ideas are not aggregated. Since comprehensive descriptions and experimental
analyses are unavailable, engineers face diﬃculties in implementing an eﬃcient DAAC.

In this paper, we review implementation techniques for DAACs and provide a comprehensive description of
them. We also propose several new techniques for further improvement. We conduct exhaustive experiments through
real-world datasets and reveal the best combination of techniques to achieve a higher performance in DAACs. The
best combination is diﬀerent from those used in the most popular libraries of DAACs, which demonstrates that their
performance can be further enhanced. On the basis of our experimental analysis, we developed a new Rust library
for fast multiple pattern matching using DAACs, named Daachorse, as open-source software at https://github.com/
daac-tools/daachorse. Experiments demonstrate that Daachorse outperforms other AC-automaton implementations,
indicating its suitability as a fast alternative for multiple pattern matching in many applications.

1 Introduction

Multiple pattern matching in strings is a fundamental problem in text and natural language processing [33]. Given a
set of patterns and a text, the goal of this problem is to report all occurrences of patterns in the text. A representative
application is regular expressions, which provide a powerful way to express a set of search patterns. Another is
tokenization in unsegmented natural languages such as Japanese and Chinese, which partitions a sentence into shorter
units called tokens or words. Multiple pattern matching is essential in these applications, and its time eﬃciency is
crucial.

The Aho–Corasick (AC) algorithm [1] is a fast solution for multiple pattern matching. It uses an AC automaton and
performs the matching with 𝑂 (𝑛) character comparisons, where 𝑛 is the length of an input text. Despite the theoretical
guarantee, the practical performance of the AC algorithm is signiﬁcantly aﬀected by its internal data structure used
to represent the AC automaton [35]. Thus, carefully designing the internal data structure is vital to achieve faster
matching.

Double-array AC automata (DAACs) are AC-automaton representations for fast matching. The core component
is the double-array [2], a data structure to implement transition lookups in an optimal time. Prior experiments
[35] demonstrated that DAACs performed the fastest compared to various other representations. Thanks to their
time eﬃciency, double-array structures are used in a wide range of applications such as dictionary lookups [28, 46],
compressed string dictionaries [26], tokenization [27, 43], language models [36, 44], classiﬁcation [52], and search
engines [9].

To achieve a higher performance in DAACs, it is essential to carefully design the data structure with regard to the
target applications and data characteristics. In the three decades since the original idea of the double-array was proposed
by Aoe [2], many implementation techniques have been developed from diﬀerent points of view, such as scalability

1

 
 
 
 
 
 
[18, 25, 26], cache eﬃciency [24, 49], construction speed [32, 37, 47], and specializations [36, 44, 47]. In addition
to these academic studies, many open-source libraries have been developed, e.g., [20, 28, 40, 46], which sometimes
contain original techniques. The problem is that their ideas are not aggregated. Speciﬁcally, the comprehensive
descriptions and experimental analyses are unavailable, which makes it diﬃcult for engineers to implement an eﬃcient
DAAC.

Our contributions
In this paper, we review various implementation techniques in DAACs and provide a compre-
hensive description of them, including categorization for facilitating comparison (as summarized in Table 2). We also
propose several new techniques for further improvement. We provide an exhaustive experimental analysis through
real-world datasets and reveal the best combination of implementation techniques. The best combination is diﬀerent
from those used in the most popular libraries of DAACs [20, 40], demonstrating that their performance can be further
enhanced.

On the basis of our experimental analysis, we develop a new Rust library for fast multiple pattern matching using
DAACs, named Daachorse, as open-source software at https://github.com/daac-tools/daachorse under the Apache-2.0
or MIT license. Our experiments demonstrate that Daachorse outperforms other AC-automaton implementations,
indicating its suitability as a fast alternative for multiple pattern matching in many applications. Daachorse has been
plugged into Vaporetto [10], a Japanese tokenizer written in Rust. Vaporetto is a fast implementation of the pointwise
prediction method [34, 41, 42] that determines token boundaries using a discriminative model. The core step of
this method is feature extraction performed with the AC algorithm. Its running time signiﬁcantly aﬀects the entire
processing time, and the fast pattern matching provided by Daachorse assures the time eﬃciency of Vaporetto. For
example, Daachorse performs tokenization 2.6× faster than other implementations, as demonstrated in this paper.

2 Preliminaries

2.1 Basic deﬁnition and notation

A string is a ﬁnite sequence of characters over a ﬁnite integer alphabet Σ = {0, 1, . . . , |Σ| − 1}. Our strings always start
at position zero. The empty string 𝜀 is a string of length zero. Given a string 𝑃 of length 𝑛 ≥ 1, 𝑃[𝑖.. 𝑗) denotes the
substring 𝑃[𝑖], 𝑃[𝑖 + 1], . . . , 𝑃[ 𝑗 − 1] for 0 ≤ 𝑖 ≤ 𝑗 ≤ 𝑛. Specially, 𝑃[0..𝑖) is a preﬁx of 𝑃, and 𝑃[𝑖..𝑛) is a suﬃx of 𝑃
for 0 ≤ 𝑖 ≤ 𝑛. Let |𝑃| := 𝑛 denote the length of 𝑃. The same notation is applied to arrays. We denote the cardinality
of a set 𝐴 by | 𝐴|.

2.2 Multiple pattern matching
Given a set of strings D = {𝑃1, 𝑃2, . . . , 𝑃 | D | } and a string 𝑇, the goal of multiple pattern matching is to report all
occurrences {(𝑘, 𝑖, 𝑗) : 𝑃𝑘 ∈ D, 𝑃𝑘 = 𝑇 [𝑖.. 𝑗)}, where an occurrence (𝑘, 𝑖, 𝑗) consists of the index 𝑘 of the matched
pattern 𝑃𝑘 and the starting and ending positions 𝑖, 𝑗 appearing in 𝑇. Throughout this paper, we refer to D as a
dictionary, 𝑃𝑘 as a pattern, and 𝑇 as a text.

Table 1 shows an example of a dictionary D consisting of six patterns. In all examples throughout this paper, we
denote indices of patterns by upper-case letters instead of numbers and integer elements in Σ by lower-case letters.
Given the dictionary and text 𝑇 [0..6) = abacdd, the occurrences are (A, 0, 2), (B, 1, 2), (D, 1, 4), and (F, 4, 6).

2.3 Aho–Corasick algorithm

The AC automaton [1] is a ﬁnite state machine to ﬁnd all occurrences of patterns in a single scan of a text. The AC
automaton for a dictionary D is deﬁned as the 5-tuple (𝑆, Σ, 𝛿, 𝑓 , ℎ):

• 𝑆 = {0, 1, . . . , |𝑆| − 1} is a ﬁnite set of states, where each state is identiﬁed by an integer, and the initial state is

indicated by 0;

• Σ = {0, 1, . . . , |Σ| − 1} is the alphabet;

2

Table 1: A dictionary D of six patterns (used in examples throughout this paper). Patterns are indexed with upper-case
letters A, B, C, . . . instead of numbers in the examples. Elements in the integer alphabet Σ are denoted with lower-case
letters. Σ = {a = 0, b = 1, c = 2, d = 3}.

Index
A
B
C
D
E
F

Pattern
ab
b
bab
bac
db
dd

(a) AC automaton

(b) Trie

Figure 1: Examples of (a) an AC automaton for the dictionary of Table 1 and (b) its trie part. Transitions are depicted
by solid line arrows. 𝛿(0, b) = 2, 𝛿(2, a) = 5, and 𝛿(2, c) = −1. We depict the mappings of the failure function (except
𝑓 (4) = 2, 𝑓 (5) = 1, 𝑓 (6) = 2, 𝑓 (7) = 3, 𝑓 (8) = 4, and 𝑓 (𝑠) = 0 for
ones to the initial state) by dotted line arrows.
the other states 𝑠. Output states are shaded and associated with pattern indices (drawn from A, B, C, . . . ). ℎ(2) = {B},
ℎ(4) = {A,B}, ℎ(6) = {E,B}, ℎ(7) = {F}, ℎ(8) = {C,A,B}, ℎ(9) = {D}, and ℎ(𝑠) = ∅ for the other states 𝑠.

• 𝛿 : 𝑆 × Σ → 𝑆 ∪ {−1} is a transition function, where −1 is an invalid state id;

• 𝑓 : 𝑆 \ {0} → 𝑆 is a failure function; and

• ℎ : 𝑆 → P ({1, 2, . . . , |D|}) is an output function, where P (·) is the power set.

Figure 1a shows an example of the AC automaton.

The transition function 𝛿 is built on a trie for the dictionary D. The trie [16] is a tree automaton formed by merging
the preﬁxes of patterns in D. Figure 1b shows the trie in the AC automaton. A state 𝑠 in the trie represents any preﬁx
of patterns in D, and the preﬁx can be extracted by concatenating transition labels from the initial state to state 𝑠. We
denote by 𝜙(𝑠) the string represented by state 𝑠. For example, 𝜙(4) = ab and 𝜙(5) = ba in Figure 1b. The initial
state always represents the empty string 𝜀, i.e., 𝜙(0) = 𝜀. We call states satisfying 𝜙(𝑠) ∈ D output states. If a state
𝑠 does not indicate any other state with character 𝑐, 𝛿(𝑠, 𝑐) = −1 is deﬁned using the invalid state id −1. There is one
diﬀerence in the deﬁnition of 𝛿 between the AC automaton and trie: the AC automaton redeﬁnes special transitions
𝛿(0, 𝑐) := 0 for labels 𝑐 such that 𝛿(0, 𝑐) = −1 in the trie.

The failure function 𝑓 maps a state 𝑠 ∈ (𝑆 \ {0}) to another state 𝑡 ∈ (𝑆 \ {𝑠}) such that 𝜙(𝑡) is a longer suﬃx
of 𝜙(𝑠) than 𝜙(𝑡 (cid:48)) for 𝑡 (cid:48) ∈ (𝑆 \ {𝑠, 𝑡}). Note that, since the empty string 𝜀 is a suﬃx of any string, the initial state is
always one of the candidates. Let 𝐹 (𝑠) be a set of output states reached through only the failure function from state
𝑠. For example, 𝐹 (8) = {8, 4, 2} in Figure 1a. The output function ℎ(𝑠) is the set of pattern indices associated with
output states in 𝐹 (𝑠), i.e., ℎ(𝑠) = {𝑘 : 𝑃𝑘 ∈ D, 𝑃𝑘 = 𝜙(𝑡), 𝑡 ∈ 𝐹 (𝑠)}.

3

Algorithm 1: Multiple pattern matching using AC automaton.

: Text 𝑇 of length 𝑛

Input
Output : All occurrences of patterns {(𝑘, 𝑖, 𝑗) : 𝑃𝑘 ∈ D, 𝑃𝑘 = 𝑇 [𝑖.. 𝑗)}

1 𝑠 ← 0
2 for 𝑗 = 0, 1, . . . , 𝑛 − 1 do
for 𝑘 ∈ ℎ(𝑠) do
3
𝑖 ← 𝑗 − |𝑃𝑘 |
Output an occurrence (𝑘, 𝑖, 𝑗)

5

4

𝑠 ← 𝛿∗ (𝑠, 𝑇 [ 𝑗])

6
7 for 𝑘 ∈ ℎ(𝑠) do
8

𝑖 ← 𝑛 − |𝑃𝑘 |
Output an occurrence (𝑘, 𝑖, 𝑛)

9

Figure 2: BASE and CHECK implementing the transition function 𝛿 of Figure 1b. Σ = {a = 0, b = 1, c = 2, d = 3}.
The state ids are assigned to satisfy Equation (2). 𝛿(0, b) = 2 is simulated by BASE[0] + b = 2 and CHECK[2] = 0.
𝛿(2, d) = −1 is simulated by BASE[2] + d = 8 and CHECK[8] ≠ 2. The state id 7 is a vacant id because its element
does not represent any state of the original trie.

Matching algorithm Algorithm 1 shows the AC algorithm that performs multiple pattern matching using the AC
automaton. The algorithm uses the extended transition function 𝛿∗:

𝛿∗(𝑠, 𝑐) =

(cid:40)𝛿(𝑠, 𝑐)
𝛿∗ ( 𝑓 (𝑠), 𝑐)

if 𝛿(𝑠, 𝑐) ≠ −1
otherwise.

(1)

Given a text 𝑇, the AC algorithm scans 𝑇 character by character, visits states from the initial state 𝑠 = 0 with 𝛿∗,
and reports occurrences where patterns in ℎ(𝑠) are associated with each visited state 𝑠. For example, assume we are
scanning text 𝑇 [0..6) = abacdd with the AC automaton in Figure 1a. We ﬁrst move with ab from state 0 to state 4 by
𝛿∗ (0, a) = 1 and 𝛿∗ (1, b) = 4, and then report two occurrences, (A, 0, 2) and (B, 1, 2), associated with ℎ(4). Next, we
move to state 9 by 𝛿∗ (4, a) = 5 and 𝛿∗ (5, c) = 9 and report occurrence (D, 1, 4) associated with ℎ(9). Last, we move
to state 7 with 𝛿∗(9, d) = 3 and 𝛿∗(3, d) = 7 and report occurrence (F, 4, 6) associated with ℎ(7).

In the scanning, the number of states visited with 𝛿 and 𝑓 is bounded by 2𝑛 for a text of length 𝑛. The algorithm

runs in 𝑂 (𝑛 + 𝑜𝑐𝑐) time in the most eﬃcient case, where 𝑜𝑐𝑐 is the number of occurrences.

2.4 Double-array Aho–Corasick automata (DAACs)

The double-array [2] is a data structure to implement the transition function 𝛿 using two one-dimensional arrays:
BASE and CHECK (see Figure 2). The double-array arranges original states in 𝑆 onto BASE and CHECK and assigns
new state ids to the original states. An element of BASE and CHECK corresponds to an original state, and its array

4

12558001024455012345678910BASECHECKAlgorithm 2: Construction of BASE and CHECK from an original trie using a queue.

: Original trie

Input
Output : BASE and CHECK arrays representing the original trie

1 Reserve enough elements of BASE and CHECK
2 𝑄 ← {(0, 0)}
3 while 𝑄 ≠ ∅ do
4

Pop (𝑠, 𝑠(cid:48)) from 𝑄
𝐸 ← a set of labels of outgoing transitions from state 𝑠
if 𝐸 ≠ ∅ then

⊲ Queue to traverse the original trie, initialized with the initial state ids

⊲ 𝑠 and 𝑠(cid:48) denote state ids in the original trie and double-array, respectively

BASE[𝑠(cid:48)] ← integer 𝑏 such that 𝑏 + 𝑐 is a vacant id for each 𝑐 ∈ 𝐸
for 𝑐 ∈ 𝐸 do

⊲ Vacant search

5

6

7

8

9

10

11

3

4

5

6

(𝑡, 𝑡 (cid:48)) ← (𝛿(𝑠, 𝑐), BASE[𝑠(cid:48)] + 𝑐)
CHECK[𝑡 (cid:48)] ← 𝑠(cid:48)
Push (𝑡, 𝑡 (cid:48)) to 𝑄

12 Output BASE and CHECK

Algorithm 3: Implementation of 𝛿∗(𝑠, 𝑐) using DAAC.
1 repeat
2

𝑡 ← BASE[𝑠] + 𝑐
if CHECK[𝑡] = 𝑠 then

return 𝑡
else if 𝑠 = 0 then
return 0
𝑠 ← FAIL[𝑠]

7
8 until

oﬀsets indicate new state ids. The BASE and CHECK arrays are constructed so that new state ids satisfy the following
equations when 𝛿(𝑠, 𝑐) = 𝑡 (except for 𝑡 = −1):

BASE[𝑠] + 𝑐 = 𝑡 and CHECK[𝑡] = 𝑠.

(2)

Equation (2) enables us to look up a transition 𝛿(𝑠, 𝑐) in two very simple steps: (i) computing 𝑡 := BASE[𝑠] + 𝑐

and (ii) returning 𝑡 if CHECK[𝑡] = 𝑐 or −1 otherwise. The time complexity is 𝑂 (1).

Let 𝑆BC = {0, 1, . . . , |𝑆BC| − 1} be a set of state ids in a resulting double-array structure. The space complexity
of BASE and CHECK is 𝑂 (|𝑆BC|), where |𝑆BC| = |𝑆| in the best case and |𝑆BC| = |𝑆||Σ| in the worst case, as 𝑆BC can
contain unused ids to satisfy Equation (2). For example, state id 7 is unused in Figure 2. We call such state ids vacant
ids. Constructing BASE and CHECK as few vacant ids as possible is important for space eﬃciency.

Construction algorithm Constructing BASE and CHECK requires traversing an original trie from the root and
searching for values of BASE[𝑠] for each state 𝑠 to satisfy Equation (2). More formally, for a state 𝑠 having outgoing
transitions with 𝑘 labels 𝑐1, 𝑐2, . . . , 𝑐𝑘 , we search for a BASE value 𝑏 such that 𝑏 + 𝑐 𝑗 is a vacant id for each 1 ≤ 𝑗 ≤ 𝑘
and deﬁne BASE[𝑠] = 𝑏 and CHECK[𝑏 + 𝑐 𝑗 ] = 𝑠, which we call a vacant search. Algorithm 2 shows the construction
algorithm of BASE and CHECK. In Section 3.5, we will discuss approaches to accelerate vacant searches.

Extension to AC automaton We can extend the double-array to DAACs by introducing components for failure and
output functions. Figure 3 shows an example DAAC for the AC automaton in Figure 1a. The failure function 𝑓 is
implemented with array FAIL such that FAIL[𝑠] = 𝑓 (𝑠). Using the arrays BASE, CHECK, and FAIL, the extended
transition function 𝛿∗ is implemented as Algorithm 3.

The output function ℎ is implemented with three arrays: OUTPUT arranges values in ℎ(𝑠) for output states 𝑠, TERM
stores bit ﬂags to identify terminals of each set ℎ(𝑠) in OUTPUT, and OUTPOS[𝑠] stores the starting positions of each

5

Figure 3: DAAC for the AC automaton in Figure 1a.

set ℎ(𝑠) in OUTPUT. Using these arrays, we can extract ℎ(𝑠) by scanning OUTPUT[𝑖.. 𝑗] from the starting position
𝑖 = OUTPOS[𝑠] until encountering TERM[ 𝑗] = 1.

3 Implementation techniques in DAACs

In this section, we review implementation techniques to improve the DAAC performance and describe them based on
our categorization (summarized in Table 2).

3.1 Management of output sets

We ﬁrst describe how to manage output sets eﬃciently. Figure 3 shows a simple approach that arranges values in ℎ(𝑠)
in an array. We call this approach Simple. An advantage of Simple is the locality of reference that can extract ℎ(𝑠) by a
sequential scan. However, Simple can maintain many duplicate values in OUTPUT. For example, value B appears four
times in the OUTPUT of Figure 3. The length of OUTPUT is bounded by 𝑂 (|D| · 𝐾), where 𝐾 is the average length of
patterns in the dictionary. In the following, we present two additional approaches, Shared and Forest, to improve the
memory eﬃciency of Simple.

Shared arrangement To design Shared, we exploit the fact that ℎ(𝑡) ⊆ ℎ(𝑠) when state 𝑡 can be reached from state
𝑠 through only failure links. For example, ℎ(2) ⊆ ℎ(3) ⊆ ℎ(9) in Figure 3. This fact indicates that values in ℎ(𝑡)
can be represented as a part of the values in ℎ(𝑠). Shared implements the OUTPUT and TERM arrays while merging
such common parts. Figure 4a shows an example of Shared, where ℎ(2) and ℎ(3) are represented in the rear parts
of OUTPUT[3..5] for ℎ(9). Shared can extract ℎ(𝑠) in the same manner as Simple and does not lose the locality of
reference.

Forest representation A drawback of Shared is that it cannot remove all duplicate values, and the space complexity
of OUTPUT and TERM is not improved. For example, value B still appears twice in Figure 4a. Forest is an alternative
approach that maintains only unique pattern indices [7].

Multiple trees, or a forest, can be constructed from an AC automaton by chaining output states through the failure
function, which we call an output forest. The left part of Figure 4b shows such an output forest constructed from the
AC automaton in Figure 3. In the output forest, we can extract values in ℎ(𝑠) by climbing up the corresponding tree to
the root.

Forest constructs an output forest whose nodes are indexed by numbers from {0, 1, . . . , |D| − 1}. Our data
structure represents the forest using two arrays, OUTPUT and PARENT, such that OUTPUT stores pattern indices and
PARENT stores parent positions. Also, OUTPOS[𝑠] stores the node index corresponding to an output state 𝑠 in the

6

125580010244550002012430013569012345678910BASECHECKFAILOUTPOS10101100110123456789OUTPUTTERM(a) Shared

(b) Forest

Figure 4: Examples of approaches to store output sets of Figure 3.

AC automaton. ℎ(𝑠) is extracted by visiting OUTPUT[𝑖] from node 𝑖 := OUTPOS[𝑠] while updating 𝑖 := PARENT[𝑖]
until encountering the root. The right part of Figure 4b shows the data structure.

The advantage of Forest is that the space complexity of OUTPUT and PARENT is bounded by 𝑂 (|D|). However,
traversing an output forest requires random accesses, which sacriﬁces the locality of reference in Simple and Shared.
Also, the pointer array PARENT consumes a larger space than the bit array TERM.

3.2 Byte- and character-wise automata

When handling strings consisting of multibyte characters, there are two representations of strings:

• Bytewise represents strings as sequences of bytes in the UTF-8 format and uses byte values for transition labels.

The maximum value in the alphabet Σ is the maximum byte value appearing in input patterns.

• Charwise represents strings as sequences of code points in Unicode and uses code-point values for transition
labels. The maximum value in the alphabet Σ is the maximum code-point value appearing in input patterns.

For example, the Japanese string “世界” (meaning “the world”) is represented as a sequence of six bytes “0xE4,
0xB8, 0x96, 0xE7, 0x95, 0x8C” in Bytewise and a sequence of two code points “U+4E16, U+754C” in Charwise. The
alphabet Σ in Bytewise is {0x00, 0x01, . . . , 0xE7}, and the alphabet Σ in Charwise is {U+0000, U+0001, . . . , U+754C}.
In the following, we ﬁrst describe the advantages and disadvantages when constructing DAACs from strings in
the Bytewise or Charwise scheme; then, we present an approach called Mapped to overcome the disadvantages in the
Charwise scheme.

Advantages and disadvantages There is a trade-oﬀ between the alphabet size (or |Σ|) and the number of automaton
states (or |𝑆|): the alphabet size in Bytewise is bounded by 28 and is smaller than that in Charwise bounded by 221

7

540236012345678910OUTPOS01100110123456OUTPUTTERM010012345OUTPUTPARENT013524012345678910OUTPOS(since code points are drawn up to U+10FFFF), and the number of states in Charwise is lower than that in Bytewise.
For example, consider an AC automaton for the single pattern “世界”. The alphabet size in Bytewise is 232 (i.e., 0xE7
plus 1) while that in Charwise is 30029 (i.e., U+754C plus 1); thus, the number of states in Bytewise is seven while
that in Charwise is three.

Smaller alphabets enable DAACs to achieve faster construction because the possible number of outgoing transitions
from a state is suppressed, which facilitates vacant searches. Fewer states enable DAACs to achieve faster matching
because of suppressing the number of random memory accesses during a matching. Therefore, Bytewise is beneﬁcial
for faster construction, and Charwise is beneﬁcial for faster matching.

The memory usage of DAACs depends on the number of resulting double-array states |𝑆BC|, which is the sum of
the number of original states |𝑆| and that of vacant ids. Although Charwise can construct an AC automaton with fewer
states, its large alphabet can produce more vacant ids because of the diﬃculty in vacant searches. Thus, we can achieve
memory eﬃciency by constructing a DAAC with fewer vacant ids in the Charwise scheme.

Code mapping The Mapped approach can overcome the disadvantages in Charwise by mapping code points to
smaller integers [29]. This approach assigns code values to characters 𝑐 ∈ Σ with a certain frequency to assign smaller
code values to more frequent characters.

We give a formal description of Mapped. Let us denote by #(𝑐) the number of occurrences of character 𝑐 ∈ Σ in

patterns of D. We construct the mapping function 𝜋 : Σ → Σ 𝜋 such that:

• Σ 𝜋 = {−1, 0, 1, . . . , 𝜎 − 1}, where 𝜎 is the number of characters 𝑐 such that #(𝑐) ≠ 0;

• 𝜋(𝑐) = −1 for characters 𝑐 when #(𝑐) = 0; and

• 𝜋(𝑐) is the number of other characters 𝑐(cid:48) such that #(𝑐) < #(𝑐(cid:48)) (breaking ties arbitrarily).

We call 𝜎 the mapped alphabet size.

Also, Equation (2) is modiﬁed into

BASE[𝑠] + 𝜋(𝑐) = 𝑡 and CHECK[𝑡] = 𝑠.

(3)

Note that characters 𝑐 such that 𝜋(𝑐) = −1 are not used in transitions because (i) during construction, the characters do
not appear, and (ii) during pattern matching, we can immediately know whether transitions with the characters fail.

If Σ includes many characters 𝑐 such that #(𝑐) = 0, the mapped alphabet size 𝜎 becomes much smaller than |Σ|.
Moreover, occurrences of real-world characters are often skewed following Zipf’s law [51], indicating that most of the
characters in D are represented with small code values via the mapping 𝜋. Prior works have empirically demonstrated
that mapping 𝜋 reduces the resultant vacant ids and shortens the construction time [29, 36, 44].

However, we need to store an additional data structure for implementing the mapping 𝜋. We implement 𝜋 with an
array of length |Σ| such that the 𝑐-th element stores the value of 𝜋(𝑐). When the array consists of 221 four-byte integers,
it takes 8 MiB of memory.

3.3 Memory layout of arrays

This paper says that the data structure of DAACs consists of four arrays: BASE, CHECK, FAIL, and OUTPOS. In the
following, we describe the two memory layouts of the four arrays: Individual and Packed.

Individual layout The Individual layout maintains the four arrays individually, as shown in Figure 5a.

Packed layout Since the values of BASE[𝑠], CHECK[𝑠], FAIL[𝑠], and OUTPOS[𝑠] are accessed consecutively during
a matching, we should be able to improve the locality of reference by placing these values in a consecutive memory
array. The Packed layout represents states using a one-dimensional array STATE, where each element consists of the
four ﬁelds base, check, fail, and outpos corresponding to each array. Figure 5b illustrates the Packed layout.

8

(a) Individual

(b) Packed

Figure 5: Illustrations of memory layouts of arrays.

3.4 Array formats

Double-array implementations often use 4-byte integers to represent an oﬀset of arrays (e.g., [28, 46]). When BASE,
CHECK, FAIL, and OUTPOS are implemented as arrays of 4-byte integers, we call this format Basic. In the following,
we describe a more memory-eﬃcient format that implements CHECK as a byte array in the Bytewise scheme, called
Compact.

Compact format The Compact format stores transition labels in CHECK instead of array oﬀsets [49]. In other words,
CHECK[𝑡] stores 𝑐 instead of 𝑠 when 𝛿(𝑠, 𝑐) = 𝑡, and Equation (2) is modiﬁed into

However, for every state pair (𝑠, 𝑠(cid:48)), the following must be satisﬁed to avoid deﬁning invalid transitions:

BASE[𝑠] + 𝑐 = 𝑡 and CHECK[𝑡] = 𝑐.

BASE[𝑠] ≠ BASE[𝑠(cid:48)].

(4)

(5)

In the Bytewise scheme, the alphabet size is bounded by 256, and CHECK can be implemented as a byte array.
The Compact format can reduce the memory consumption of CHECK to 25% while maintaining the time eﬃciency in
matching. In the Packed layout, we assign three bytes to base, one byte to check, and four bytes to fail and outpos for
avoiding extra memory padding on a 4-byte aligned memory structure, following the original implementation [49].1

A drawback of Compact is that it makes it harder for vacant searches to avoid the duplication of BASE values for
satisfying Equation (5); indeed, this restriction can produce vacant ids that never satisfy Equation (5). For example, a
vacant id 𝑠 never satisﬁes Equation (5) when BASE values 𝑠, 𝑠 − 1, . . . , 𝑠 − |Σ| + 1 are already used. In Section 4.5, we
observe that such vacant ids can signiﬁcantly slow the running times of vacant searches.

3.5 Acceleration of vacant searches
Given a state having 𝑘 outgoing transitions with labels 𝑐1, 𝑐2, . . . , 𝑐𝑘 , a naïve vacant search is performed to verify if
𝑏 + 𝑐 𝑗 for 1 ≤ 𝑗 ≤ 𝑘 are vacant ids for each integer 𝑏 ≥ 0 until ﬁnding such a value 𝑏. However, for a maximum state
id 𝑁, this approach veriﬁes 𝑂 (𝑁) integers in the worst case, resulting in slow construction for a large automaton. In
the following, we describe three acceleration techniques for vacant searches: Chain, SkipForward, and SkipDense.

Chaining vacant ids The Chain technique constructs a linked list on vacant ids [32]. Let us denote 𝑀 vacant ids
by 𝑞1, 𝑞2, . . . , 𝑞 𝑀 , where 0 < 𝑞1 < 𝑞2 < · · · < 𝑞 𝑀 < 𝑁. Given a state having 𝑘 outgoing transitions with labels

1This modiﬁcation limits the maximum state id to 224 − 1. To represent state ids up to 229 − 1 with the same space usage, we can employ a
technique used in the Darts-clone library [46] that utilizes two types of oﬀset values to represent exact and rough positions. We did not use this
technique in our experiments because three bytes are enough to store the automata we tested.

9

B0...BN-1C0...CN-1F0...FN-1O0...ON-1BASECHECKFAILOUTPOSB0C0F0O0B1C1F1O1...BN-1CN-1FN-1ON-1STATE[0]STATE[1]STATE[N-1]Figure 6: BASE and CHECK implementing the transition function 𝛿 of Figure 1b based on Equation (6). The block
size 𝐵 is four, since |Σ| = 4 and 𝐵 = 2 (cid:100)log2 4(cid:101). Destination states 5 and 7 from state 1 are placed in the same block, i.e.,
(cid:98)5/4(cid:99) = (cid:98)7/4(cid:99) = 1.

𝑐1, 𝑐2, . . . , 𝑐𝑘 , Chain veriﬁes only BASE values 𝑏𝑖 such that 𝑏𝑖 = 𝑞𝑖 − min{𝑐1, 𝑐2, . . . , 𝑐𝑘 } for 𝑖 = 1, 2, . . . , 𝑀 while
visiting only vacant ids using the linked list. The number of veriﬁcations is bounded by 𝑂 (𝑀).

If there are few vacant ids, 𝑀 becomes much smaller than 𝑁, and vacant searches can be performed faster than with
the naïve approach. The resultant double-array structure with Chain is always identical to that with the naïve approach.

Skipping search blocks Before introducing SkipForward and SkipDense, we present a technique to partition array
elements into ﬁxed-size blocks [24, 46, 52]. The partitioning modiﬁes Equation (2) into

BASE[𝑠] ⊕ 𝑐 = 𝑡 and CHECK[𝑡] = 𝑠.

(6)

In other words, the bitwise-XOR operation (⊕) is used instead of the plus operation (+).

We introduce a constant parameter 𝐵 and partition array elements into blocks of size 𝐵, where the 𝑠-th element is

placed in the (cid:98)𝑠/𝐵(cid:99)-th block. We obtain the following theorem.

Theorem 1. Let 𝐵 = 2 (cid:100)log2 |Σ|(cid:101).2 When state ids are deﬁned using Equation (6), all destination states from a state are
always placed in the same block.

Proof. Computing BASE[𝑠] ⊕ 𝑐 is implemented by modifying the least signiﬁcant (cid:100)log2 |Σ|(cid:101) bits of BASE[𝑠]. For all
characters 𝑐 ∈ Σ, the results of BASE[𝑠] ⊕ 𝑐 have the same binary representation except the least signiﬁcant (cid:100)log2 |Σ|(cid:101)
bits. Computing (cid:98)𝑥/𝐵(cid:99) for an integer 𝑥 is implemented by shifting (cid:100)log2
𝐵(cid:101) = (cid:100)log2 |Σ|(cid:101) bits right, and the least
signiﬁcant (cid:100)log2 |Σ|(cid:101) bits of 𝑥 are omitted from the result. Therefore, (cid:98)(BASE[𝑠] ⊕ 𝑐)/𝐵(cid:99) = (cid:98)BASE[𝑠]/𝐵(cid:99) for all
(cid:3)
characters 𝑐 ∈ Σ, and all destination states 𝑡 from a state 𝑠 are placed in the (cid:98)BASE[𝑠]/𝐵(cid:99)-th block.

Figure 6 shows an example of BASE and CHECK constructed using Equation (6). The partitioning allows us
to perform vacant searches for blocks individually and set up conditions if we search each block. SkipForward and
SkipDense each use a diﬀerent approach to select which blocks to be searched.

SkipForward searches for only the last 𝐿 blocks with a constant parameter 𝐿 [46], as shown in Figure 7a. This idea
is inspired by the fact that backward state ids are more likely to be vacant when performing vacant searches from the
forward [32]. SkipForward can bound the number of veriﬁcations in a vacant search by 𝑂 (𝐿 · |Σ|) = 𝑂 (|Σ|), since 𝐿
is constant. A drawback of SkipForward is that blocks containing many vacant ids might be skipped, which degrades
the memory eﬃciency.

SkipDense introduces a constant threshold 𝜏 ∈ [0, 1] and categorizes blocks into two classes: if the proportion of
vacant ids in a block is no less than 𝜏, the block is categorized to the sparse class; otherwise, the block is categorized
to the dense class. SkipDense performs vacant searches for only sparse blocks, as shown in Figure 7b. SkipDense

2In the Mapped scheme, 𝜎 is used instead of |Σ |.

10

2456900021316601234567891011BASECHECK(a) SkipForward

(b) SkipDense

Figure 7: Illustrations of skipping techniques.

does not skip blocks that contain many vacant ids and is thus more memory-eﬃcient than SkipForward. However, the
number of veriﬁcations is not bounded by the alphabet size.3

The ideas of both SkipForward and SkipDense do not conﬂict with Chain. We therefore apply the technique of

Chain to SkipForward and SkipDense.

3.6 Traversal orders in construction

BASE and CHECK are constructed by traversing an original trie from the root and performing vacant searches, as
shown in Algorithm 2. Although this algorithm traverses the original trie using a queue in the breadth-ﬁrst order, we
can take an arbitrary order to traverse the trie. We can also take an arbitrary scan order of outgoing transitions for each
visited state (i.e., the loop order of 𝐸 at Line 8 in Algorithm 2).

The orders are related to the resultant arrangement of states in BASE and CHECK, and the arrangement aﬀects the
cache eﬃciency in transitions: a transition 𝛿(𝑠, 𝑐) = 𝑡 can be looked up quickly when states 𝑠 and 𝑡 are placed close
together (e.g., on the same cache line). Selecting the traversal order that improves the locality of reference during a
matching is crucial. In the following, we describe the four approaches: LexBFS, LexDFS, FreqBFS, and FreqDFS.

Breadth- and depth-ﬁrst searches There are two types of data structures to traverse the original trie. One uses a
queue and visits states with a breadth-ﬁrst search (as Algorithm 2), which we denote by BFS. The other uses a stack
and visits states with a depth-ﬁrst search, which we denote by DFS.

We consider performing vacant searches from the forward (i.e., attempting to use smaller state ids ﬁrst). The
resultant arrangements of states using BFS and DFS are expected to be as follows. With BFS, it is expected that
shallower states (i.e., ones around the initial state) are placed around the head of BASE and CHECK. If we often visit
shallow states during a matching, BFS can perform cache-eﬃciently. With DFS, in contrast, it is expected that deeper
states are placed close together. If we often visit deep states during a matching, DFS can perform cache-eﬃciently.

Figure 8 shows toy examples of resultant BASE and CHECK arrays with BFS and DFS. In BFS, shallower states
have smaller state ids (see Figure 8a), while in DFS, states whose depth is no less than two have adjacent ids with their
destination states (see Figure 8b). These examples show that BFS and DFS have better locality of reference when we
visit shallower and deeper states, respectively.

Lexicographical and frequency orders There are two orders to traverse outgoing transitions for each visited state:
Lex visits transitions in the lexicographical order of their labels, and Freq visits transitions in the decreasing order

3SkipDense is inspired by the idea in the Darts library [28], which skips forward elements in which the proportion of vacant ids is not less than a
certain threshold. SkipDense is a minor modiﬁcation of the Darts technique combined with block partitioning and enables vacant searches to select
target blocks more ﬂexibly.

11

The last      blocksVacant searchSparseSparseSparseSparseVacant searchVacant searchVacant search(a) BFS

(b) DFS

Figure 8: Examples of resultant BASE and CHECK arrays constructed with BFS and DFS for a dictionary of four
patterns aa, bbb, cccc, and ddddd. Σ = {a = 0, b = 1, c = 2, d = 3}. The state ids are determined based on Equation
(2). In the construction, vacant searches are performed from the forward to use smaller state ids ﬁrst. A set of outgoing
transition labels (or 𝐸) is scanned in the lexicographical order (i.e., the ﬁgures show the results of LexBFS and LexDFS
more precisely). In both cases, the initial state and its destination states have identical ids (i.e., states 0 to 4). This is
because the initial state id is always ﬁxed to 0, and its destination state ids are ﬁxed to BASE[0] to satisfy Equation (2).
The deeper states have diﬀerent ids in accordance with the order of visiting states.

12

155558881010110000123467810111301234567891011121314BASECHECK15568678910110000126389411121301234567891011121314BASECHECKTable 2: Summary of implementation techniques in DAACs. “Selected” indicates the techniques selected to be the
best through our experiments in Section 4 and are used in our Daachorse library.

(a) Approaches to store output sets (Section 3.1)

Technique

Simple
Shared
Forest

Brief description
Arranging pattern indices in a simple manner
Merging some common parts of pattern indices in Simple
Storing pattern indices in a forest structure

References
Conventional
This study
[7]

Selected

(cid:88)

(b) Schemes to handle strings of multibyte characters (Section 3.2)

Technique

Bytewise
Charwise
Mapped

Brief description
Slicing multibyte characters into byte sequences
Handling multibyte characters as code points in Unicode
Mapping code points in the frequency order

References
Conventional
Conventional
[29, 36]

Selected
(cid:88)

(cid:88)

(c) Memory layouts of double-arrays (Section 3.3)

Technique

Individual
Packed

Brief description
Maintaining arrays BASE, CHECK, FAIL, and OUTPOS individually
Arranging values in the arrays cache-eﬃciently

References
Conventional
e.g., [32, 49]

Selected

(cid:88)

(d) Formats of double-arrays in Bytewise scheme (Section 3.4)

Technique

Basic
Compact

Brief description
Implementing BASE, CHECK, FAIL, and OUTPOS as arrays of four-byte integers
Compressing CHECK into a byte array

References
Conventional
[49]

Selected

(cid:88)

(e) Approaches to accelerate vacant searches (Section 3.5)

Technique

Chain
SkipForward
SkipDense

Brief description
Visiting only vacant ids with a linked list
Searching for only the last 𝐿 blocks
Searching for only blocks in which the vacant proportion is no less than 𝜏

References
[32]
[32, 46]
This study

Selected

(cid:88)

(f) Traversal orders in construction (Section 3.6)

Technique

LexBFS
FreqBFS
LexDFS
FreqDFS

Brief description
Visiting states with breadth-ﬁrst search and transition labels in the lexicographical order
Visiting states with breadth-ﬁrst search and transition labels in the frequency order
Visiting states with depth-ﬁrst search and transition labels in the lexicographical order
Visiting states with depth-ﬁrst search and transition labels in the frequency order

References
Conventional
Conventional
Conventional
Conventional

Selected

(cid:88)

of the frequencies of their labels in D. In the Freq order, ids of states incoming with frequent transition labels are
determined earlier, and it is expected that those states are placed at the head of the array.

Possible combinations There are four possible combinations in the traversal order:

• LexBFS is BFS with Lex,

• FreqBFS is BFS with Freq,

• LexDFS is DFS with Lex, and

• FreqDFS is DFS with Freq.

3.7 Summary of implementation techniques

Table 2 summarizes the implementation techniques described in this section.

13

Table 3: Basic statistics of dictionaries.

(a) Average pattern length in bytes

(b) Average pattern length in characters

(c) Alphabet size in the Bytewise scheme

Dataset

EnWord
JaWord
JaChars

1K
5.0
5.7
4.9

10K
6.6
6.7
6.3

100K
7.1
8.7
7.3

1M
7.5
14.5
8.0

Dataset

EnWord
JaWord
JaChars

1K
5.0
1.9
1.8

10K
6.6
2.3
2.3

100K
7.1
3.0
2.7

1M
7.5
5.2
2.9

Dataset

EnWord
JaWord
JaChars

1K
226
233
233

10K
226
238
233

100K
239
239
238

1M
239
244
239

(d) Alphabet size in the Charwise scheme

(e) Mapped alphabet size in the Mapped scheme

Dataset

EnWord
JaWord
JaChars

1K
8,226
39,640
39,640

10K
9,632
57,345
40,845

100K
63,289
64,017
57,345

1M
65,532
1,048,766
64,017

Dataset

EnWord
JaWord
JaChars

1K
83
669
483

10K
104
2,262
1,648

100K
177
4,806
2,990

1M
472
10,809
4,773

Table 4: The number of occurrences in pattern matching per sentence.

Dataset

EnWord
JaWord
JaChars

1K
47.2
25.8
36.6

10K
93.4
38.0
54.6

100K
126.2
46.7
69.7

1M
150.8
49.0
79.9

4 Experimental analyses of DAAC techniques

4.1 Setup

We conducted all experiments on one core of a hexa-core Intel i7-8086K CPU clocked at 2.80 GHz in a machine with
64 GB of RAM (L1 cache: 32 KiB, L2 cache: 256 KiB, L3 cache: 12 MiB), running the 64-bit version of CentOS 7.5
based on Linux 3.10. All data structures were implemented in Rust. We compiled the source code by rustc 1.60.0 with
optimization ﬂag opt-level=3.

Datasets We use three real-world natural language datasets:

• EnWord is English word uni-grams in the Google Web 1T 5-Gram Corpus [8];

• JaWord is Japanese word uni-grams in the Nihongo Web Corpus 2010 [45]; and

• JaChars is Japanese character uni-, bi-, and tri-grams in the Nihongo Web Corpus 2010 [45].

In these datasets, each N-gram has its own frequency. We extract the most frequent 𝐾 N-grams from each dataset
and produce dictionaries D of 𝐾 patterns. We test 𝐾 = 103, 104, 105, 106 to observe scalability. Table 3 lists the basic
statistics of the dictionaries.

We evaluate the running times of pattern matching and the construction and memory usage of an AC automaton.
We use an English text from the Pizza&Chili Corpus [15] and a Japanese text from 13 diﬀerent subcorpora in the
Balanced Corpus of Contemporary Written Japanese (BCCWJ, version 1.1) [30] to measure pattern matching times for
the English and Japanese dictionaries, respectively. We randomly sample one million sentences (separated by lines)
from each text and produce a sequence of search texts 𝑇. Table 4 shows the number of occurrences during a pattern
matching for each dataset. Throughout the experiments, we report the total running time of a pattern matching for all
sentences, averaged on ten runs.

4.2 Analysis on approaches to store output sets

Table 5 shows the experimental results on the Simple, Shared, and Forest approaches presented in Section 3.1, while
ﬁxing the other settings to Bytewise, Packed, Basic, Chain, and LexDFS.

14

Table 5: Experimental results for approaches to store output sets: Simple, Shared, and Forest. For memory usage, the
total of OUTPUT and TERM is reported in Simple and Shared, and the total of OUTPUT and PARENT is reported in
Forest. The last two rows in Tables (a)–(c) show the diﬀerent ratios for each result.

(a) Length of OUTPUT (×103)

EnWord

JaWord

JaChars

Approach

Simple
Shared
Forest
Shared/Simple
Forest/Simple

1K
1.60
1.46
1.00
0.92
0.63

10K
29.2
26.2
10.0
0.90
0.34

100K
412
362
100
0.88
0.24

1M
4,988
4,377
1,000
0.88
0.20

1K
1.46
1.31
1.00
0.89
0.68

10K
18.7
16.7
10.0
0.89
0.53

100K
247
218
100
0.88
0.40

1M
3,615
3,132
1,000
0.87
0.28

1K
1.68
1.40
1.00
0.83
0.60

10K
21.6
17.7
10.0
0.82
0.46

100K
252
217
100
0.86
0.40

1M
2,779
2,497
1,000
0.90
0.36

(b) Memory usage (KiB)

EnWord

JaWord

JaChars

Approach

Simple
Shared
Forest
Shared/Simple
Forest/Simple

1K
12.5
11.4
11.7
0.92
0.94

10K
228
205
117
0.90
0.51

100K
3,219
2,830
1,172
0.88
0.36

1M
38,968
34,192
11,719
0.88
0.30

1K
11.4
10.2
11.7
0.89
1.02

10K
146
131
117
0.89
0.80

100K
1,932
1,703
1,172
0.88
0.61

1M
28,244
24,473
11,719
0.87
0.41

1K
13.1
10.9
11.7
0.83
0.89

10K
168
139
117
0.82
0.70

100K
1,971
1,692
1,172
0.86
0.59

1M
21,707
19,507
11,719
0.90
0.54

(c) Matching time (ms)

EnWord

JaWord

JaChars

Approach

Simple
Shared
Forest
Shared/Simple
Forest/Simple

1K
525
525
533
1.00
1.01

10K
609
613
596
1.01
0.98

100K
693
719
697
1.04
1.00

1M
1,088
1,148
1,088
1.06
1.00

1K
469
457
446
0.98
0.95

10K
646
653
626
1.01
0.97

100K
919
923
871
1.00
0.95

1M
1,266
1,304
1,259
1.03
0.99

1K
448
415
427
0.92
0.95

10K
700
668
663
0.95
0.95

100K
1,073
1,044
1,027
0.97
0.96

1M
2,853
2,822
2,796
0.99
0.98

(d) Average cardinality of ℎ (𝑠)

Dataset

EnWord
JaWord
JaChars

1K
1.37
1.42
1.63

10K
2.07
1.76
2.06

100K
3.16
2.20
2.42

1M
4.15
3.02
2.73

15

Table 6: Experimental results on memory eﬃciency in the Bytewise, Charwise, and Mapped schemes. The memory
usage is the total of BASE, CHECK, FAIL, and OUTPOS (and the mapping 𝜋 in Mapped). The last rows in Tables (a)
and (d) show the diﬀerent ratios for each result.

(a) Number of original states (×103)

Scheme

Bytewise
Charwise
Charwise/Bytewise

1K
2.74
2.74
1.00

EnWord

10K
25.6
25.6
1.00

100K
237
237
1.00

1M
2,129
2,124
1.00

1K
2.96
1.43
0.48

JaWord

10K
27.9
13.0
0.47

100K
305
142
0.46

1M
4,899
2,155
0.44

1K
1.73
1.12
0.65

JaChars

10K
18.5
10.9
0.59

100K
186
107
0.58

1M
1,790
1,052
0.59

(b) Proportion of vacant ids (%)

Scheme

Bytewise
Charwise
Mapped

EnWord

1K
2.6
66.6
2.7

10K
0.9
0.2
0.9

100K
0.1
0.0
0.2

1M
0.0
0.0
0.0

1K
3.7
96.4
30.2

JaWord

10K
0.8
77.3
20.5

100K
0.2
16.3
3.9

1M
0.1
0.7
1.1

1K
15.3
97.2
27.1

JaChars

10K
0.8
84.1
24.1

100K
0.2
68.0
12.6

1M
0.0
71.8
27.0

(c) Average number of outgoing transitions for an internal state.

Scheme

Bytewise
Charwise

1K
1.41
1.41

EnWord

10K
1.40
1.41

100K
1.44
1.44

JaWord

1M
1.50
1.50

1K
1.41
2.54

10K
1.43
2.79

100K
1.37
2.40

1M
1.20
1.60

1K
1.87
3.58

JaChars

10K
1.68
3.25

100K
1.75
3.83

1M
1.91
5.33

(d) Memory usage (KiB)

EnWord

JaWord

JaChars

Scheme

Bytewise
Charwise
Mapped
Charwise/Bytewise
Mapped/Bytewise
Mapped/Charwise

1K
44
128
76
2.91
1.73
0.59

10K
404
401
442
0.99
1.09
1.10

100K
3,712
3,707
3,959
1.00
1.07
1.07

1M
33,276
33,193
33,456
1.00
1.01
1.01

1K
48
619
187
12.89
3.89
0.30

10K
440
896
480
2.04
1.09
0.54

100K
4,780
2,646
2,554
0.55
0.53
0.97

1M
76,596
33,922
38,145
0.44
0.50
1.12

1K
32
619
179
19.34
5.59
0.29

10K
292
1,067
384
3.65
1.31
0.36

100K
2,908
5,240
2,144
1.80
0.74
0.41

1M
27,972
58,318
22,778
2.08
0.81
0.39

We ﬁrst focus on the statistics related to memory eﬃciency. Table 5a shows the length of OUTPUT, i.e., the
number of pattern indices stored in the data structure. The length in Forest is essentially the same as the number of
patterns and is much smaller than that in Simple, indicating that Simple maintains many duplicate values in OUTPUT.
Shared also reduces such duplicate values in Simple. Comparing Shared and Forest on their reduction ratios from
Simple, the diﬀerence becomes larger as the number of patterns increases, and Forest is more memory eﬃcient for
large dictionaries. Table 5b shows the memory usage of the data structures. Shared is the smallest when the number
of patterns is 1K, and Forest is the smallest in the other cases.

We next focus on the statistics related to the time eﬃciency of the AC algorithm. Table 5c reports the elapsed
time during a pattern matching, and as we can see, there is no signiﬁcant diﬀerence between the approaches. These
results indicate that the cache-eﬃcient scanning in Simple and Shared is unimportant. To clarify this, Table 5d shows
the average cardinality of ℎ(𝑠) for each dictionary. These results imply that only a few random accesses can arise in
Forest’s extraction and do not create a bottleneck.

4.3 Analysis on byte-wise and character-wise schemes

We evaluate the performances of the Bytewise, Charwise, and Mapped schemes presented in Section 3.2, while ﬁxing
the other settings to Forest, Packed, Basic, Chain, and LexDFS.

Table 6 shows the results on memory eﬃciency, where Table 6a lists the number of original states (i.e., |𝑆|) and
Table 6b reports the proportion of vacant ids (i.e., |𝑆|/|𝑆BC|), which we call vacant proportion. On JaWord and

16

Table 7: Experimental results on time eﬃciency in the Bytewise, Charwise, and Mapped schemes. The last rows in
Tables (a) and (c) show the diﬀerent ratios for each result.

(a) Number of visiting states during matching (×106)

Scheme

Bytewise
Charwise
Charwise/Bytewise

1K
108
108
1.00

EnWord

10K
104
104
1.00

100K
107
107
1.00

1M
111
111
1.00

1K
128
57
0.44

JaWord

10K
128
61
0.48

100K
129
61
0.47

JaChars

1M
129
61
0.47

1K
129
59
0.45

10K
129
62
0.48

100K
129
62
0.48

1M
131
62
0.48

Scheme

Bytewise
Charwise
Mapped
Charwise/Bytewise
Mapped/Bytewise
Mapped/Charwise

Scheme

Bytewise
Charwise
Mapped
Charwise/Bytewise
Mapped/Bytewise
Mapped/Charwise

1K
527
550
563
1.04
1.07
1.02

1K
0.12
0.14
0.09
1.18
0.80
0.68

(b) Matching time (ms)

EnWord

JaWord

JaChars

10K
597
628
654
1.05
1.10
1.04

100K
689
720
741
1.04
1.08
1.03

1M
1,057
1,098
1,135
1.04
1.07
1.03

1K
454
362
315
0.80
0.69
0.87

10K
644
444
407
0.69
0.63
0.92

100K
896
563
533
0.63
0.59
0.95

1M
1,254
809
762
0.65
0.61
0.94

1K
406
356
304
0.88
0.75
0.85

10K
648
507
424
0.78
0.65
0.84

100K
1,021
661
625
0.65
0.61
0.94

1M
2,735
1,812
1,677
0.66
0.61
0.93

(c) Construction time (ms)

EnWord

JaWord

JaChars

10K
1.34
0.79
0.81
0.59
0.60
1.03

100K
21
18
18
0.86
0.86
1.01

1M
244
199
204
0.81
0.83
1.03

1K
0.13
0.32
0.06
2.51
0.44
0.17

10K
1.44
1.09
1.50
0.76
1.04
1.37

100K
27
163
102
6.03
3.77
0.62

1M
387
1,829
1,251
4.72
3.23
0.68

1K
0.09
0.29
0.08
3.14
0.83
0.26

10K
1.2
2.3
1.4
1.92
1.22
0.64

100K
20
134
45
6.72
2.25
0.33

1M
275
21,184
9,107
77.1
33.1
0.43

JaChars, whose strings consist of multibyte characters, Charwise deﬁnes fewer states than Bytewise but more vacant
ids. Especially in JaChars, the vacant proportion in Charwise is always more than 68%. The large vacant proportions
are related to the average number of outgoing transitions from an internal state, as reported in Table 6c. The more
outgoing transitions there are, the easier it is for vacant searches to fail and the number of vacant ids to increase. The
large vacant proportions of Charwise, however, can be improved with the code mapping 𝜋, as the result of Mapped
demonstrates.

Table 6d reports the total memory usage of BASE, CHECK, FAIL, and OUTPOS (and the mapping 𝜋 in Mapped).
On EnWord, there is no signiﬁcant diﬀerence between the schemes when the number of patterns is no less than 10K.
On JaWord and JaChars, Mapped is always the ﬁrst or second smallest because of its fewer states and smaller vacant
proportions.

Table 7 shows the experimental results on time eﬃciency, where Table 7a reports the number of visiting states
during matching with 𝛿 and 𝑓 . On EnWord, whose characters are mostly single-byte ones, there is no signiﬁcant
diﬀerence between the schemes. On JaWord and JaChars, whose strings consist of multibyte characters, Charwise
(or Mapped) achieves half as many as Bytewise, resulting in halving the number of random accesses during a pattern
matching. Table 7b lists the elapsed time during a pattern matching. On JaWord and JaChars, Charwise and Mapped
are faster than Bytewise because of their fewer random accesses. Comparing Charwise and Mapped, there is no large
diﬀerence, although Mapped requires additional computations for the code mapping 𝜋.

Table 7c shows the elapsed time to compute two arrays, BASE and CHECK, from an original trie. On EnWord,
Charwise and Mapped are faster than Bytewise in most cases. On JaWord and JaChars, Charwise is much slower than
Bytewise, and its time performance is improved by the code mapping of Mapped; nevertheless, Mapped is still much
slower than Bytewise for large dictionaries. The time ineﬃciency of Charwise and Mapped is caused by their large
vacant proportion (as shown in Table 6b): Chain does not perform well when there are many vacant ids.

17

(a) Matching time (ms)

(b) Number of L3 cache misses occurring during a pattern matching

Figure 9: Comparison results on matching time for the Individual and Packed layouts.

4.4 Analysis on memory layouts

Figure 9 shows the experimental results for the Individual and Packed layouts presented in Section 3.3, while ﬁxing
the other settings to Forest, Bytewise, Basic, Chain, and LexDFS. Figure 9a shows the matching time. The greater the
number of patterns, the faster Packed becomes. Packed is ≈20% faster than Individual when the number of patterns is
106. The time eﬃciency of Packed stems from its cache-eﬃciency. Figure 9b shows the number of L3 cache misses
occurring during a pattern matching. When the number of patterns is no greater than 105, cache misses infrequently
occur because the data structure ﬁts in the cache memory. However, when the number of patterns is 106, Packed
achieves 38–47% cache misses compared to Individual.

4.5 Analysis on array formats

Table 8 shows the experimental results for the Basic and Compact formats presented in Section 3.4, while ﬁxing the
other settings to Forest, Bytewise, Packed, Chain, and LexDFS.

Table 8a reports the total memory usage of BASE, CHECK, FAIL, and OUTPOS. In all cases, the memory usage in
Compact is 75% of that in Basic because there is no diﬀerence in the resulting number of vacant ids (although Equation
(5) has to be satisﬁed). There is no signiﬁcant diﬀerence in the matching time, as Table 8b shows.

Table 8c reports the construction time. Compact is always slower because of Equation (5), especially on JaWord of
1M patterns. To clarify the slowdown, we investigated vacant ids for which vacant searches were mostly unsuccessful
and found that the top-ﬁve vacant ids could only accept the ﬁve transition labels 0xC0, 0xCF, 0xD0, 0xD1, and 0xFF
to satisfy Equation (5). Because these labels did not appear or were very few in the dictionary, vacant searches for the
vacant ids were unsuccessful many times, resulting in the slow construction. Consequently, while the Compact format
does not produce a particularly high number of vacant ids, some of them can signiﬁcantly slow down vacant searches
when using Chain.

18

103104105106Number of patterns020040060080010001200Matching time (ms)EnWord, BytewiseIndividualPacked103104105106Number of patterns0250500750100012501500Matching time (ms)JaWord, BytewiseIndividualPacked103104105106Number of patterns0500100015002000250030003500Matching time (ms)JaChars, BytewiseIndividualPacked103104105106Number of patterns02468Number of cache misses×106EnWord, BytewiseIndividualPacked103104105106Number of patterns0.00.20.40.60.81.01.2Number of cache misses×107JaWord, BytewiseIndividualPacked103104105106Number of patterns02468Number of cache misses×107JaChars, BytewiseIndividualPackedTable 8: Experimental results for the Basic and Compact formats. The memory usage is the total of BASE, CHECK,
FAIL, and OUTPOS. The last row in each table shows the diﬀerent ratios for each result.

(a) Memory usage (KiB)

Format

Basic
Compact
Compact/Basic

1K
44.0
33.0
0.75

EnWord

10K
404
303
0.75

100K
3,712
2,784
0.75

JaWord

JaChars

1M
33,276
24,957
0.75

1K
48.0
36.0
0.75

10K
440
330
0.75

100K
4,780
3,585
0.75

1M
76,544
57,408
0.75

1K
32.0
24.0
0.75

10K
292
219
0.75

100K
2,908
2,181
0.75

1M
27,972
20,979
0.75

(b) Matching time (ms)

EnWord

Format

Basic
Compact
Compact/Basic

1K
523
545
1.04

10K
600
606
1.01

100K
696
712
1.02

1M
1,082
1,052
0.97

1K
447
473
1.06

JaWord

10K
632
633
1.00

100K
879
893
1.02

1M
1,234
1,226
0.99

1K
406
415
1.02

JaChars

10K
651
609
0.94

100K
1,038
1,028
0.99

1M
2,733
2,439
0.89

(c) Construction time (ms)

EnWord

Format

Basic
Compact
Compact/Basic

1K
0.11
0.14
1.25

10K
1.06
1.30
1.23

100K
19.2
23.5
1.22

1M
207
249
1.20

1K
0.13
0.14
1.09

JaWord
100K
22.4
32.7
1.46

10K
1.19
1.39
1.17

JaChars

1M
331
182,015
549

1K
0.08
0.09
1.09

10K
0.81
0.91
1.12

100K
14.8
16.0
1.08

1M
200
226
1.13

Table 9: The average number of veriﬁcations per vacant search when using Chain. The number of patterns in each
dictionary is one million.

Method
Bytewise + Basic
Bytewise + Compact
Mapped

EnWord

JaWord

JaChars

2.8
4.6
2.5

2.8
5383.3
84.4

6.6
7.4
4934.5

4.6 Analysis on acceleration techniques for vacant searches

We ﬁrst investigate problematic cases in DAAC construction when using Chain. Table 9 shows the average number
of veriﬁcations per vacant search when using Chain. In the combination of Bytewise and Basic, the number is always
several, indicating that it is hard to improve vacant searches further even using SkipForward or SkipDense. However,
the number is signiﬁcantly higher in the cases of (i) Compact on JaWord and (ii) Mapped on JaChars. Case (i) is
because of Equation (5), as discussed in Section 4.5. Case (ii) is because of many vacant ids, as discussed in Section
4.3.

We next test SkipForward and SkipDense to improve the two cases. We test parameters 𝐿 = 4, 8, 12, 16, 20, 24
in SkipForward and 𝜏 = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5 in SkipDense. Figure 10a shows the experimental results for
the average number of veriﬁcations per vacant search. The left ﬁgure corresponds to case (i), where we can see
that SkipForward achieves several veriﬁcations with any parameter 𝐿 while maintaining similar vacant proportions.
SkipDense also achieves several veriﬁcations, although the vacant proportions increase depending on the parameter 𝜏.
In the right ﬁgure corresponding to case (ii), both SkipForward and SkipDense achieve fewer veriﬁcations than Chain.
SkipForward and SkipDense plot similar curves, and their performances have no signiﬁcant diﬀerence.

Figure 10b shows the experimental results for the construction times. The plots are similar to those in Figure 10a,
and both SkipForward and SkipDense achieve construction times within several seconds. These results indicate that we
should choose SkipForward or SkipDense in accordance with the desired purpose: if we want to control construction
times, SkipForward is best, and if we want to control memory usage, SkipDense is more suitable.

19

(a) The average number of veriﬁcations per vacant search

Figure 10: Experimental results for veriﬁcations and construction times when using Chain, SkipForward, or SkipDense.

(b) Construction time in seconds

20

020406080100Proportion of vacant elements (%)22242628210212Average number of verifications JaWord (Bytewise+Compact)ChainSkipForwardSkipDense020406080100Proportion of vacant elements (%)2829210211212Average number of verifications JaChars (Mapped)ChainSkipForwardSkipDense020406080100Proportion of vacant elements (%)20222426Construction time (sec)JaWord (Bytewise+Compact)ChainSkipForwardSkipDense020406080100Proportion of vacant elements (%)2120212223Construction time (sec)JaChars (Mapped)ChainSkipForwardSkipDenseFigure 11: Matching times in milliseconds for diﬀerent traversal orders.

4.7 Analysis on traversal orders

Figure 11 shows the experimental results of matching times for LexBFS, FreqBFS, LexDFS, and FreqDFS presented in
Section 3.6. In the Bytewise scheme, we ﬁx the other settings to Forest, Packed, Compact, and SkipForward (𝐿 = 16).
In the Mapped scheme, we ﬁx the other settings to Forest, Packed, and SkipForward (𝐿 = 16).

We compare the DFS and BFS orders. DFS is faster than BFS for JaWord and JaChars in Bytewise. To investigate
the reason, we show the number of times states are visited in each depth during a pattern matching in Figure 12. As we
can see, deeper states are often visited for JaWord and JaChars in Bytewise. The number of L3 cache misses occurring
during a pattern matching is shown in Table 10, where we can see that DFS is cache-eﬃcient in most cases. These
observations indicate that DFS enables cache-eﬃcient traversals on deeper states and is suitable for long patterns.

Comparing Lex and Freq, there is no signiﬁcant diﬀerence. Although we also tested other orders, such as a random
order, we did not observe any signiﬁcant diﬀerences. The reason is explained by the average number of outgoing
transitions for an internal state reported in Table 6c. The average number is the average size of 𝐸 (≠ ∅) in Algorithm
2. In all cases, the average number is several, indicating that the order to visit elements in 𝐸 is insigniﬁcant.

Figure 12: The number of visiting states at each depth during a pattern matching.

21

103104105106Number of patterns02004006008001000Matching time (ms)EnWord, BytewiseLexBFSLexDFSFreqBFSFreqDFS103104105106Number of patterns0200400600800100012001400Matching time (ms)JaWord, BytewiseLexBFSLexDFSFreqBFSFreqDFS103104105106Number of patterns050010001500200025003000Matching time (ms)JaChars, BytewiseLexBFSLexDFSFreqBFSFreqDFS103104105106Number of patterns020040060080010001200Matching time (ms)EnWord, MappedLexBFSLexDFSFreqBFSFreqDFS103104105106Number of patterns0200400600800Matching time (ms)JaWord, MappedLexBFSLexDFSFreqBFSFreqDFS103104105106Number of patterns02505007501000125015001750Matching time (ms)JaChars, MappedLexBFSLexDFSFreqBFSFreqDFS024681012141618Depth of visited state0.00.51.01.52.02.53.0#times of visiting states×107EnWord,BytewiseEnWord,MappedJaWord,BytewiseJaWord,MappedJaChars,BytewiseJaChars,MappedTable 10: The number of L3 cache misses occurring during a pattern matching with diﬀerent traversal orders (×103).
The number of patterns is ﬁxed to 106. The last row shows the diﬀerent ratios for each result.

Order

LexBFS
LexDFS
LexDFS/LexBFS

EnWord

JaWord

JaChars

Bytewise

Mapped

Bytewise

Mapped

Bytewise

Mapped

2,485
2,303
0.93

2,967
2,888
0.97

4,301
3,209
0.75

3,157
2,944
0.93

31,999
28,804
0.90

17,719
17,917
1.01

4.8 Comparison with other AC automata

On the basis of the above experimental analyses, we determine the best combinations of techniques and develop a new
Rust library, called Daachorse, containing the following two variants:

• Bytewise-Daachorse = Forest + Bytewise + Packed + Compact + SkipForward (𝐿 = 16) + LexDFS

• Charwise-Daachorse = Forest + Mapped + Packed + SkipForward (𝐿 = 16) + LexDFS

We compare Daachorse with the Aho-corasick library [19], which is the most popular implementation of AC
automata in Rust. This library provides two types of implementations: NFA-AC is a standard AC automaton (as
described in Section 2.3), and DFA-AC is a deterministic ﬁnite automaton compiled from NFA-AC. DFA-AC is an option
for faster matching, although it consumes a huge amount of memory.

Figure 13 shows the comparison results. For the matching times reported in Figure 13a, Bytewise-Daachorse
is always the fastest on EnWord, and Charwise-Daachorse is always the fastest on JaWord and JaChars. Although
DFA-AC is also fast when the number of patterns is 103, its performance degrades as the number of patterns increases.
For the construction times reported in Figure 13b, NFA-AC is always the fastest, and Bytewise and Charwise-Daachorse
approach NFA-AC. DFA-AC is always the slowest because compiling from NFA-AC takes time. For the memory usages
reported in Figure 13c, Bytewise- and Charwise-Daachorse are often the smallest; however, when the number of
patterns is 103, Charwise-Daachorse is larger than NFA-AC because of the memory consumption of the mapping 𝜋.

5 Application example: Japanese tokenization

Vaporetto [10], a supervised Japanese tokenizer written in Rust, is an application example of AC automata. Vaporetto
is an eﬃcient implementation of the pointwise prediction algorithm [31] and leverages the AC algorithm in its feature
extraction phase. To demonstrate the ability of Daachorse, we integrated the Daachorse and Aho-corasick libraries
with Vaporetto and evaluated the tokenization speeds.

We conducted these experiments in the same environment as the experiments in Section 4. To train the Vaporetto
model, we used 60K sentences with manually annotated short-unit-word boundaries in BCCWJ (version 1.1) [30] and
667K unique words in UniDic (version 3.1.0) [11], while specifying default parameters in Vaporetto. We performed
tokenization for 5.9M sentences in BCCWJ that are not used for the training and then measured the running time.

The experimental result showed that:

• Bytewise-Daachorse performed in 3.3 microseconds per sentence,

• Charwise-Daachorse performed in 2.9 microseconds per sentence,

• NFA-AC performed in 7.5 microseconds per sentence, and

• DFA-AC performed in 7.3 microseconds per sentence.

Daachorse was at most 2.6× faster than the Aho-corasick library. This result indicates that the time eﬃciency of

the AC algorithm is critical in Vaporetto’s tokenization, demonstrating the utility of our Daachorse.

22

(a) Matching time (ms)

(b) Construction time (ms)

(c) Memory usage (KiB)

Figure 13: Comparison results with the Daachorse and Aho-corasick libraries. (a) The matching time is plotted in a
linear scale because the time complexity is not linear over the number of patterns. (b) The construction time and (c)
the memory usage are plotted in a logarithmic scale because the complexities are linear over the number of patterns.

23

103104105106Number of patterns0500100015002000250030003500Matching time (ms)EnWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns01000200030004000Matching time (ms)JaWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns02000400060008000Matching time (ms)JaCharsBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns100101102103Construction time (ms)EnWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns100101102103104Construction time (ms)JaWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns100101102103Construction time (ms)JaCharsBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns102103104105106Memory usage (KiB)EnWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns102103104105106107Memory usage (KiB)JaWordBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC103104105106Number of patterns102103104105106Memory usage (KiB)JaCharsBytewise-DaachorseCharwise-DaachorseNFA-ACDFA-AC6 Related works

Double-array The two most popular implementations of DAACs are the Java library by hankcs [20] and the Go
library by Ruohang [40]. Both libraries are implemented in a similar manner and employ the same techniques: Simple
(although the memory layout is not identical), Charwise, Individual, SkipDense (𝜏 = 0.05, without Chain and the block
partitioning), and LexBFS. As demonstrated in Section 4, these techniques do not provide the best performances in
most cases, and the implementations could be improved.

Some studies have represented the integers of BASE and CHECK in a compressed space other than Compact
(described in Section 3.4). Fuketa et al. [18] proposed partitioning one double-array structure into several smaller ones
to implement BASE and CHECK consisting of 2-byte integers; however, as this approach produces many structures for
a large trie, many pointers must be maintained to connect the structures. Fuketa et al. [17] proposed eliminating BASE
by introducing additional character mappings, although its applications are limited to ﬁxed-length keywords such as zip
codes. Kanda et al. [25, 26] proposed approaches to compress the integers of BASE and CHECK through diﬀerential
encoding; however, time eﬃciency can degrade because of more computations.

Another approach to compress the double-array is to employ compact trie forms. The minimal-preﬁx (MP) trie
[3, 13] is an often-used form [2, 26, 49], and various methods to eﬃciently implement the MP-trie using the double-
array have been proposed [12, 24, 48, 50]. However, the MP-trie is specialized for dictionary lookups, which are done
by traversing a trie from the root to a leaf. Other compact forms such as double-tries [4] and directed acyclic word
graphs [47] cannot be applied to AC automata in principle.

Liu et al. [29] showed that double-array structures resulting from large alphabets (e.g., multibyte characters) can
include many vacant ids and are thus memory ineﬃcient. They proposed several approaches to address this problem
and demonstrated that code mapping with the frequency of characters is eﬀective on Chinese strings. The eﬃciency
of the code mapping was also demonstrated on 𝑁-gram sequences [36, 44].

Alternative trie representations Tries [16] have been studied since the 1960s, and there are many data structures
to represent. Since the AC automaton is a simple extension of the trie, we have many alternatives to the double-array.
The conventional data structures are a matrix form and a list form [6]. The matrix form uses a transition matrix of size
|𝑆| × |Σ| and performs a transition lookup in 𝑂 (1) time, while consuming a large space of 𝑂 (|𝑆||Σ|). The list form
stores a set of transition labels from each state in a sorted array and performs a transition lookup by binary search in
𝑂 (log |Σ|) time, while consuming a smaller space of 𝑂 (|𝑆|). The matrix and list forms have a time-space trade-oﬀ,
and the double-array harnesses both advantages by compressing the matrix form.

There are other data structures that utilize both advantages of the matrix and list forms. One is a hybrid approach that
uses the matrix form for states with many outgoing transitions and the list form for states with few outgoing transitions.
The aho-corasick library [19] employs the hybrid approach and is outperformed by Daachorse, as we demonstrated.
Another approach is hashing [6], which stores mappings from states to outgoing transitions in hash tables. This
approach can perform a transition lookup in 𝑂 (1) expected time while consuming 𝑂 (|𝑆|) space. However, searching
in the hash table often requires more computations than transition lookups in the double-array. Prior experiments [35]
demonstrated that DAACs outperformed AC automata with the hashing approach.

Compressed representations of tries have recently been proposed to store massive datasets in main memory.
Succinct tries [5] are representative data structures. Their memory usage achieves |𝑆| log2 |Σ| + 𝑂 (|𝑆|) bits of space
and is close to the information-theoretic lower bound [23]. However, the succinct tries employ many bit manipulations
in tree navigational operations, and their time eﬃciency is not competitive with that of double-arrays [25, 26]. More
compressed data structures, such as XBW [14] and Elias-Fano tries [38, 39], also have similar time bottlenecks and are
not suited to design fast AC automata.

Compressed representations of AC automata Another line of research proposes data structures to represent AC
automata (i.e., transition, failure, and output functions) in a compressed space. Belazzougui [7] proposed the ﬁrst
compressed data structure that supports a matching in optimal 𝑂 (𝑛 + 𝑜𝑐𝑐) time, while achieving |𝑆| log2 |Σ| + 𝑂 (|𝑆|)
bits of space. Hon et al. [21] achieved an entropy compressed space while matching time remains optimal. I et al. [22]
designed a matching algorithm working on grammar-based compressed AC automata. However, these studies were
accomplished through theoretical discussions, and we are unaware of any actual implementation.

24

7 Conclusion

In this paper, we provided a comprehensive description of implementation techniques in DAACs and experimentally
revealed the most eﬃcient combinations of the techniques to achieve higher performance. We also designed a
data structure of DAACs and developed a new Rust library for faster multiple pattern matching, called Daachorse.
Our experiments showed that, compared to other implementations of AC automata, Daachorse oﬀered a superior
performance in terms of time and space eﬃciency. As we demonstrated in the integration test with Vaporetto,
Daachorse has signiﬁcant potential to improve applications that employ multiple pattern matching. Our future work
will incorporate Daachorse in other applications to enable faster text processing.

Acknowledgement

We thank Shinsuke Mori for his cooperation and Keisuke Goto for his insightful review.

References

[1] Alfred V Aho and Margaret J Corasick. Eﬃcient string matching: An aid to bibliographic search. Communications

of the ACM, 18(6):333–340, 1975.

[2] Jun’ichi Aoe. An eﬃcient digital search algorithm by using a double-array structure. IEEE Transactions on

Software Engineering, 15(9):1066–1077, 1989.

[3] Jun’ichi Aoe, Katsushi Morimoto, and Takashi Sato. An eﬃcient implementation of trie structures. Software:

Practice and Experience, 22(9):695–721, 1992.

[4] Jun’ichi Aoe, Katsushi Morimoto, Masami Shishibori, and Ki-Hong Park. A trie compaction algorithm for a

large set of keys. IEEE Transactions on Knowledge and Data Engineering, 8(3):476–491, 1996.

[5] Diego Arroyuelo, Rodrigo Cánovas, Gonzalo Navarro, and Kunihiko Sadakane. Succinct trees in practice. In

Proceedings of the 12th Workshop on Algorithm Engineering and Experiments (ALENEX), pages 84–97, 2010.

[6] Nikolas Askitis. Eﬃcient data structures for cache architectures. PhD thesis, RMIT University, 2007.

[7] Djamal Belazzougui. Succinct dictionary matching with no slowdown.

In Proceedings of the 21st Annual

Symposium on Combinatorial Pattern Matching (CPM), pages 88–100, 2010.

[8] Thorsten Brants and Alex Franz. Web 1T 5-gram Version 1. Linguistic Data Consortium, 2006.

[9] Brazil Inc. Groonga: An open-source fulltext search engine and column store (12.0.3). https://groonga.org/,

2022.

[10] daac-tools. Vaporetto: Very Accelerated POintwise pREdicTion based TOkenizer (0.4.0). https://github.com/

daac-tools/vaporetto, 2022.

[11] Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka Uchimoto, and
Hanae Koiso. The development of an electronic dictionary for morphological analysis and its application to
Japanese corpus linguistics (in japanese). Japanese linguistics, 22:101–123, 2007.

[12] Tshering C Dorji, El-sayed Atlam, Susumu Yata, Mahmoud Rokaya, Masao Fuketa, Kazuhiro Morita, and Jun’ichi
Information

Aoe. New methods for compression of MP double array by compact management of suﬃxes.
Processing & Management, 46(5):502–513, 2010.

[13] John A Dundas. Implementing dynamic minimal-preﬁx tries. Software: Practice and Experience, 21(10):1027–

1040, 1991.

25

[14] Paolo Ferragina, Fabrizio Luccio, Giovanni Manzini, and S Muthukrishnan. Structuring labeled trees for optimal
succinctness, and beyond. In Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 184–193, 2005.

[15] Paolo Ferragina and Gonzalo Navarro. Pizza&Chili Corpus. http://pizzachili.dcc.uchile.cl/texts.html, 2005.

[16] Edward Fredkin. Trie memory. Communications of the ACM, 3(9):490–499, 1960.

[17] Masao Fuketa, Hiroya Kitagawa, Takuki Ogawa, Kazuhiro Morita, and Jun-ichi Aoe. Compression of double
array structures for ﬁxed length keywords. Information processing & management, 50(5):796–806, 2014.

[18] Masao Fuketa, Kazuhiro Morita, Toru Sumitomo, Shinkaku Kashĳi, Elsayed Atlam, and Jun-Ichi Aoe. A new
compression method of double array for compact dictionaries. International Journal of Computer Mathematics,
81(8):943–953, 2004.

[19] Andrew Gallant. aho-corasick: A fast implementation of Aho-Corasick in Rust (0.7.18). https://github.com/

BurntSushi/aho-corasick, 2021.

[20] hankcs. AhoCorasickDoubleArrayTrie: An extremely fast implementation of Aho Corasick algorithm based on

Double Array Trie (1.2.2). https://github.com/hankcs/AhoCorasickDoubleArrayTrie, 2020.

[21] Wing-Kai Hon, Tsung-Han Ku, Rahul Shah, Sharma V Thankachan, and Jeﬀrey Scott Vitter. Faster compressed

dictionary matching. Theoretical Computer Science, 475:113–119, 2013.

[22] Tomohiro I, Takaaki Nishimoto, Shunsuke Inenaga, Hideo Bannai, and Masayuki Takeda. Compressed automata

for dictionary matching. Theoretical Computer Science, 578:30–41, 2015.

[23] Guy Jacobson. Space-eﬃcient static trees and graphs. In Proceedings of the 30th IEEE Symposium on Foundations

of Computer Science (FOCS), pages 549–554, 1989.

[24] Shunsuke Kanda, Yuma Fujita, Kazuhiro Morita, and Masao Fuketa. Practical rearrangement methods for

dynamic double-array dictionaries. Software: Practice and Experience, 48(1):65–83, 2018.

[25] Shunsuke Kanda, Masao Fuketa, Kazuhiro Morita, and Jun’ichi Aoe. A compression method of double-array

structures using linear functions. Knowledge and Information Systems, 48(1):55–80, 2016.

[26] Shunsuke Kanda, Kazuhiro Morita, and Masao Fuketa. Compressed double-array tries for string dictionaries

supporting fast lookup. Knowledge and Information Systems, 51(3):1023–1042, 2017.

[27] Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. Applying conditional random ﬁelds to Japanese morpho-
logical analysis. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 230–237, 2004.

[28] Takuu Kudo. Darts: Double-ARray Trie System (0.32). http://chasen.org/~taku/software/darts/, 2008.

[29] Huidan Liu, Minghua Nuo, Long-Long Ma, Jian Wu, and Yeping He. Compression methods by code mapping
and code dividing for chinese dictionary stored in a double-array trie. In Proceedings of the 5th International
Joint Conference on Natural Language Processing (ĲCNLP), pages 1189–1197, 2011.

[30] Kikuo Maekawa, Makoto Yamazaki, Toshinobu Ogiso, Takehiko Maruyama, Hideki Ogura, Wakako Kashino,
Hanae Koiso, Masaya Yamaguchi, Makiro Tanaka, and Yasuharu Den. Balanced corpus of contemporary written
Japanese. Language resources and evaluation, 48(2):345–371, jun 2014.

[31] Shinsuke Mori, Yosuke Nakata, Graham Neubig, and Tatsuya Kawahara. Morphological Analysis with Pointwise

Predictors (in Japanese). Journal of Natural Language Processing, 18(4):367–381, 2011.

[32] Kazuhiro Morita, Masao Fuketa, Yoshihiro Yamakawa, and Jun’ichi Aoe. Fast insertion methods of a double-array

structure. Software: Practice and Experience, 31(1):43–65, 2001.

26

[33] Gonzalo Navarro and Mathieu Raﬃnot. Flexible pattern matching in strings: Practical on-line search algorithms

for texts and biological sequences. Cambridge university press, 2002.

[34] Graham Neubig, Yosuke Nakata, and Shinsuke Mori. Pointwise prediction for robust, adaptable Japanese
In Proceedings of the 49th Annual Meeting of the Association for Computational

morphological analysis.
Linguistics: Human Language Technologies, pages 529–533, 2011.

[35] Janne Nieminen and Pekka Kilpeläinen. Eﬃcient implementation of Aho–Corasick pattern matching automata

using unicode. Software: Practice and Experience, 37(6):669–690, 2007.

[36] Jun’ya Norimatsu, Makoto Yasuhara, Toru Tanaka, and Mikio Yamamoto. A fast and compact language model
implementation using double-array structures. ACM Transactions on Asian and Low-Resource Language Infor-
mation Processing, 15(4):27, 2016.

[37] Masaki Oono, El-Sayed Atlam, Masao Fuketa, Kazuhiro Morita, and Jun’ichi Aoe. A fast and compact elimination
method of empty elements from a double-array structure. Software: Practice and Experience, 33(13):1229–1249,
2003.

[38] Giulio Ermanno Pibiri and Rossano Venturini. Eﬃcient data structures for massive n-gram datasets. In Proceed-
ings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 615–624, 2017.

[39] Giulio Ermanno Pibiri and Rossano Venturini. Handling massive N-gram datasets eﬃciently. ACM Transactions

on Information Systems, 37(2), feb 2019.

[40] Feng Ruohang. ac: Aho-Corasick Automaton with Double Array Trie. https://github.com/Vonng/ac, 2019. The

latest version at May 24, 2022.

[41] Manabu Sassano. An empirical study of active learning with support vector machines for Japanese word
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics

segmentation.
(ACL), pages 505–512, 2002.

[42] Hiroyuki Shinnou. Deterministic japanese word segmentation by decision list method. In Proceedings of the 6th

Paciﬁc Rim International Conference on Artiﬁcial Intelligence (PRICAI), pages 822–822, 2000.

[43] Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou. Fast wordpiece tokenization. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
2089–2103, 2021.

[44] Makoto Yasuhara, Toru Tanaka, Jun ya Norimatsu, and Mikio Yamamoto. An eﬃcient language model using
double-array structures. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 222–232, 2013.

[45] Susumu Yata. Nihongo Web Corpus 2010 (NWC 2010). http://www.s-yata.jp/corpus/nwc2010/, 2010.

[46] Susumu Yata. A clone of Darts. https://github.com/s-yata/darts-clone, 2018. The latest version at May 24, 2022.

[47] Susumu Yata, Kazuhiro Morita, Masao Fuketa, and Jun’ichi Aoe. Fast string matching with space-eﬃcient word
graphs. In Proceedings of the 4th International Conference on Innovations in Information Technology (IIT), pages
79–83, 2008.

[48] Susumu Yata, Masaki Oono, Kazuhiro Morita, Masao Fuketa, and Jun’ichi Aoe. An eﬃcient deletion method for

a minimal preﬁx double array. Software: Practice and Experience, 37(5):523–534, 2007.

[49] Susumu Yata, Masaki Oono, Kazuhiro Morita, Masao Fuketa, Toru Sumitomo, and Jun’ichi Aoe. A compact
static double-array keeping character codes. Information Processing & Management, 43(1):237–247, 2007.

27

[50] Susumu Yata, Masaki Oono, Kazuhiro Morita, Toru Sumitomo, and Jun’ichi Aoe. Double-array compression
by pruning twin leaves and unifying common suﬃxes. In Proceedings of the 1st International Conference on
Computing & Informatics (ICOCI), pages 1–4, 2006.

[51] Zhou Yihan. Meaningfulness and unit of Zipf’s law: evidence from danmu comments. In Proceedings of the

20th Chinese National Conference on Computational Linguistics, pages 1046–1057, August 2021.

[52] Naoki Yoshinaga and Masaru Kitsuregawa. A self-adaptive classiﬁer for eﬃcient text-stream processing.

In
Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 1091–1102,
2014.

28

