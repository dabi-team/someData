2
2
0
2

g
u
A
4
2

]

C
D
.
s
c
[

1
v
5
4
7
1
1
.
8
0
2
2
:
v
i
X
r
a

AI-coupled HPC Workﬂows

Shantenu Jha,1, 2, ∗ Vincent R. Pascuzzi,2, † and Matteo Turilli1, 2, ‡

1Rutgers University, New Brunswick, NJ 08901

2Computational Science Initiative, Brookhaven National Laboratory, Upton, NY 11972, USA

(Dated: August 26, 2022)

Increasingly, scientiﬁc discovery requires sophisticated and scalable workﬂows. Workﬂows

have become the “new applications,” wherein multi-scale computing campaigns comprise

multiple and heterogeneous executable tasks. In particular, the introduction of AI/ML mod-

els into the traditional HPC workﬂows has been an enabler of highly accurate modeling, typi-

cally reducing computational needs compared to traditional methods. This chapter discusses

various modes of integrating AI/ML models to HPC computations, resulting in diverse types

of AI-coupled HPC workﬂows. The increasing need of coupling AI/ML and HPC across sci-

entiﬁc domains is motivated, and then exempliﬁed by a number of production-grade use cases

for each mode. We additionally discuss the primary challenges of extreme-scale AI-coupled

HPC campaigns—task heterogeneity, adaptivity, performance—and several framework and

middleware solutions which aim to address them. While both HPC workﬂow and AI/ML

computing paradigms are independently eﬀective, we highlight how their integration, and

ultimate convergence, is leading to signiﬁcant improvements in scientiﬁc performance across

a range of domains, ultimately resulting in scientiﬁc explorations otherwise unattainable.

I.

INTRODUCTION

Scientiﬁc discovery increasingly requires sophisticated and scalable workﬂows. Workﬂows have

become the “new applications,” wherein multi-scale computing campaigns comprise hundreds to

thousands of heterogeneous executable tasks.

Introducing AI/ML models into traditional high

performance computing (HPC) workﬂows has been an enabler of highly accurate modeling, and

has been demonstrated to be a promising approach for signiﬁcant performance improvements.

Advances in statistical algorithms and runtime systems have enabled extreme scale ensemble-

based applications [1] to overcome limitations of traditional monolithic simulations. However, in

spite of several orders of magnitude improvement in eﬃciency from these ensemble algorithms, the

complexity of phase space and dynamics for modest physical systems require additional orders of

∗ shantenu@bnl.gov
† pascuzzi@bnl.gov
‡ mturilli@bnl.gov

 
 
 
 
 
 
2

magnitude improvements and performance gains. Integration of traditional HPC workﬂows with

AI/ML methods holds real promise for overcoming such barriers [2].

In many application domains, the integration of AI/ML into a computational workﬂow is a fa-

vorable way to obtain large performance gains, and presents an opportunity to jump a generation

of simulation enhancements. For example, one can view the use of learned surrogates as a perfor-

mance boost that can lead to substantial speedups, as calculation of a prediction from a trained

network can be many orders of magnitude faster than full execution of the simulation [3, 4]. In

addition to the use of learning for advanced sampling as mentioned above, other simple examples

include the use of a surrogate to represent a chemistry potential [5], or a larger grain size to solve

the diﬀusion equation underlying cellular and tissue level simulations [6].

There are various modes (couplings) of integrating traditional HPC methods and simulations,

with AI/ML methodologies, resulting in diverse types of AI/ML “enhanced” HPC workﬂows. This

chapter provides an overview the various couplings and how they can result in the adaptive execu-

tion of workﬂow applications comprising heterogeneous tasks. We identify the core characteristics

of such workﬂow applications, as well as discuss state-of-art tools and workﬂow applications.

II. LEARNING EVERYWHERE PARADIGM

There are two classes of interplay between HPC and ML. In the ﬁrst, ML directly enhances

and impacts applications; in the second class, ML enhances the HPC environment on which those

applications operate. This chapter exclusively focuses on the former.

Central to the ﬁrst class, as well as the re-examination and overcoming the performance bar-

rier, is the need to integrate ML methodologies and HPC. In this approach – learning enhanced

simulations and campaigns – we include the use of neural surrogates, with a neural network di-

rectly predicting either the full results of simulations, or components thereof. This also includes

using learning methods to control and steer simulations, for example, eﬃcient campaigns that

steer ensembles smartly through phase space[7–10]. We have identiﬁed three high-level modes of

integrating ML with HPC [3, 11, 12]: ML-in-HPC, ML-out-HPC, and ML-about-HPC.

ML-in-HPC represents the scenario when an ML model is introduced in lieu of a component

of the HPC simulations, or possibly, in lieu of the total simulation itself, i.e., ML model serves as a

“total surrogate”. ML-out-HPC captures situations wherein a ML model resides “outside” of the

traditional HPC simulation loop, but dynamically controls the progression of the HPC workﬂow.

For example, Active Learning and Reinforcement Learning control of computational campaigns.

3

FIG. 1. Illustrating the three modes of ML-x-HPC: (1) ML-in-HPC: AI/ML surrogate models are used to

replace part or entire simulations. (2) ML-about-HPC: AI/ML complements traditional computational tasks

and possibly steers the tasks, improving their scientiﬁc results or eﬃciency (3) ML-out-HPC: A high-level

AI/ML based algorithm, such as active learning or reinforcement learning is used to dynamically control the

campaign, or steer the workﬂow as a whole (as opposed to just the tasks). Typically, ML-in and ML-about

are directly responsible for producing output for further analysis, while ML-out drives this production.

Finally, ML-about-HPC represents the situation where ML models are concurrent and coupled to

the main HPC tasks. Figure 1 illustrates these primary modes coupling AI/ML to HPC workﬂows.

These three modes are not mutually exclusive, and will increasingly be used collectively.

The learning everywhere paradigm [3, 11, 12] contends that increasingly, scientiﬁc applications

will achieve performance gains and methodological advances by using all three modes of combining

learning approaches with HPC simulation-based techniques. In the next section, we will discuss

multilevel drug selection as a canonical example of learning-everywhere paradigm, but additional

prominent examples include materials design and earth-systems modeling [4].

There are many open challenges that implementing and translating the paradigm to practice

poses. For example, how and where can ML eﬀectively enhance or accelerate HPC simulations?

How to make ML methods that work in tandem with HPC simulations scalable, robust, and

reliable? For a given computational campaign what is the optimal mix and execution plan of ML-

in, out and about HPC? Furthermore, there are system and software challenges and opportunities

AL/RLAgentHPCSimulationModel(s)TrainingMLSurrogateMLInferenceTAinaboutoutCampaign4

in combining ML and HPC systems software, hardware, and overall infrastructure. What are

the correct programming models and abstractions to manage the diverse “computational tasks”

viz., ML training & inference along with traditional HPC workloads? What runtime systems

are needed to manage the heterogeneous workload eﬀectively? What are the general motifs of

interaction between ML and HPC, and their inﬂuence on design of runtime systems?

A leitmotif of the learning everywhere paradigm is eﬀective performance, i.e., the perfor-

mance improvement obtained by substituting a traditional HPC method with an integrated HPC

and learning method. Eﬀective performance measures the improvement in application performance

metric (e.g., computational cost, improved time-to-solution, or the achievement of scientiﬁc objec-

tive) when using ML methods in conjunction with HPC, as compared to using HPC methods

stand-alone. For example, eﬀective performance can be measured as the time-to-solution ratio of

the traditional approach vs. the learning-enhanced approach. If a traditional parameter study ran

1000 simulations to determine an optimal engineering design, while a model-based optimizer pro-

duced the same optimum in 100 simulations, the eﬀective performance of the learning enhanced

application is 10.

If, additionally, the ML-based approximations in the simulation accelerated

computation by a factor of 10, the eﬀective performance would be 100. These orders-of-magnitude

increase in eﬀective performance as learning-enhanced high-performance computing takes root [13]

are at the heart of the motivation for this new paradigm of computation.

III. LEARNING EVERYWHERE EXAMPLES

This section provides an overview of various use cases and exemplar applications, across scien-

tiﬁc domains which couple HPC and ML/AI. Table I groups use cases and exemplar applications

using the three modes described in Sec. II. Use cases and applications were selected to provide a

representative overview of the ML techniques currently employed to couple ML with HPC, and to

cover diverse scientiﬁc domains in which this coupling is bringing innovation and unprecedented

performance improvement.

A. ML-in-HPC

Workﬂows in high energy physics are multi-scale, comprising quantum ﬁeld theoretic calcula-

tions, detector simulations, and classical reconstruction of physical objects. Each scale has consid-

erable computational requirements and, using only traditional methods, it is impossible to produce

5

Mode

Domain

Application Coupling Mechanism

ML-in-HPC

High Energy physics Atlfast3

Surrogate methods

Molecular Dynamics DeePMD-kit Surrogate methods

Atomistic simulations Proxima

Runtime surrogate tuning

ML-about-HPC

Material engineering Colmena

Runtime model (re)training/conﬁguration

RAS protein/Cancer MuMMI

Runtime ML-based selection

Cancer research

DeepHyper Automated machine learning

ML-out-HPC

Cyberinfrastructure

SIOX

Oﬄine ML

Materials Science

EXARL

ML-guided simulations

TABLE I. Examples of ML-x-HPC modes across scientiﬁc domains.

suﬃcient numbers of Monte Carlo events to maintain statistical adequacy with recorded data.

As such, ML techniques, including generative adversarial networks, are becoming an increasingly

attractive alternative to standard frameworks implementing step-by-step model predictions.

For example, Fig. 2 shows an illustration of the ATLAS Collaboration’s “fast” simulation frame-

work, Atlfast3 [14], where ML-based techniques are used in place of intensive Geant4 [15] simula-

tions. In this conﬁguration, surrogates are employed in lieu of full Geant4 simulations for speciﬁc

particle types, energies and subdetectors to reduce overall simulation times up to several orders of

magnitude. At the same time, those surrogates allow to maintain accurate detector modeling for

new physics searches, statistically-limited analyses, background processes, and detector upgrades.

Another signiﬁcant example of ML-in-HPC can be found in the traditional ab initio MD (AIMD)

methods for modeling atomistic phenomena. Due to demanding computational requirements (cubic

scaling in the number of electronic degrees of freedom), most AIMD applications are limited to

O(1000) atoms. However, AIMD plays a major role in addressing many issues related to, e.g., drug

discovery, complex chemical processes and nanotechnology. As such, tremendous eﬀorts have been

aﬀorded to more eﬃcient methods, including ML.

Jia et al. [5] oﬀer a powerful example of ML-in-HPC applied to AIMD. Jia’s approach employs

an ML-based simulation protocol which uses surrogates (Deep Potential MD) in conjunction with a

highly-optimized code (a GPU-accelerated DeePMD-kit [16]) to simulate O(108) nanosecond-long

trajectories in 24 hours. This record-setting accomplishment eﬃciently scaled to the whole 4,560

nodes of the Summit supercomputer, reaching double/mixed-single/mixed-half precision perfor-

mance of 91/162/275 PFLOPS. Compared to other state-of-the-art, Jia et al. showed more than

6

FIG. 2. Illustration of ML-in-HPC, showing a conﬁguration of diﬀerent surrogate modules replacing Geant4

in the ATLAS detector. Diﬀerent ML-based components can be employed, depending on the type of process

being modeled, speciﬁc subdetectors, particle types and energies. Image from [14].

O(10−3) reduction in time-to-solution (TTS) [s/step/atom].

Ref. [5] is a prime example of how ML-in-HPC, which is on the cusp of a paradigmatic change

as learning approaches inﬂuence the way both ODE and PDEs are being solved. For example,

Karniadakis et al [17] are investigating how to solve and discover new PDEs via deep learning.

For partial diﬀerential equations (PDEs), neural operators directly learn the mapping from any

functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in

contrast to classical methods which solve one instance of the equation [18].

B. ML-about-HPC

Replacing computationally intensive computations with surrogate approximators aims to reduce

TTS often by sacriﬁcing accuracy with respect to more complete models. Optimally balancing TTS

and accuracy is a non-trivial task, and the former is often neglected in order to reach the desired

accuracy. Proxima [19], provides real-time feedback from executing simulations and it has been

utilized to develop systematic and automated methods for dynamically tuning surrogate conﬁgu-

rations. An iterative simulation workﬂow, representative of the ML-about-HPC mode, evaluates

uncertainties associated with the use of surrogates, concurrently updating conﬁgurations based on

a distance metric to learn features and an accuracy metric to evaluate prediction. Between itera-

tions, surrogates evolve and replace less optimized ones, providing the coupling between concurrent

surrogate tuning and the main HPC campaign.

7

The Proxima framework has been demonstrated in a Mone Carlo sampling application, where

the ﬁrst-principles Hartree-Fock [20] prediction target is replaced with a Proxima-managed sur-

rogate. Mean absolute error (MAE) and TTS comparisons are made between Proxima and a

surrogate strategy with a ﬁxed distance threshold (based on scientiﬁc trial-and-error).

In cer-

tain scenarios, the ﬁxed strategy outperforms Proxima in terms of TTS. However, the utility of

user-deﬁned error bounds ensures more robust results with Proxima. By determining values for

surrogate conﬁgurations automatically during workﬂow execution, Proxima is able to satisfy error

bounds while achieving as much as 5.5x speedup in TTS.

Estimating properties of large collections of molecules is often necessary to ﬁnd candidates for

medical therapeutics, next-generation batteries, etc. However, the number of possible candidates

for a single application, and therefore the number of diﬀerent experimental conﬁgurations required

to test them all, is often intractable. Large-scale workﬂows have thus come to adopt methodologies

to provide an ML model training and retraining runtime to decide which computations to perform

based on previous outputs.

The Colmena framework [21] facilitates a user-deﬁned steering for workﬂow execution. Using

an example application involving an ML-guided search of 105 molecules with high resistance to

oxidation electrolyte design, the Colmena workﬂow provides components to actively train and

learn as simulations are executed. Colmena performs a concurrent execution mode, having ML

and traditional simulations running side-by-side throughout the workﬂow. Candidacy of molecules

is evaluated using ML models which are scored based on selection criteria and then are ordered

based on their score. Molecules appearing at the top of the ordered list reﬂect most suitable

candidates, and subsequently additional simulations are executed. Colmena is reported to ﬁnd

candidate molecules at rates 100 times that of traditional computational solutions, and scaling up

to 1024 nodes (65,636 cores) on Theta supercomputer.

A naturally more complicated scenario is the development of therapeutics. This type of R&D can

take years or decades to come to fruition due to the complicated computational modeling involved

in searches for candidate drugs and strict FDA approval procedure.

In the context of cancer

treatments, for example, it is suggested that Ras proteins are involved in nearly a third of all human

cancers in the US [22]; however, many physiochemical properties of Ras-Raf-membrane dynamics

are not fully understood. The inherent multi-scale nature of such processes makes computational

modeling challenging, and each scale is traditionally simulated separately.

The massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI) [23, 24]

couples three resolution scales with ML-based selection, eﬀectively promoting important conﬁgu-

8

rations from coarse-grained to all atomistic (highest resolution). The autonomy and full power of

MuMMI is realized through dynamic co-scheduling of tasks which is achieved by tying together

application and coordination layers of the workﬂow. MuMMI achieves a 98% GPU occupancy

for more than 83% of 600,000 node hours, coordinating 24,000 jobs and managing several ter-

abytes of data daily. Furthermore, the split architecture, separating the workﬂow application from

coordination, permits generalizability, making the infrastructure attractive beyond drug design.

C. ML-out-HPC

Increasing computational power helps to produce more rapidly predictive models, larger volumes

of collected data enables higher ﬁdelity predictions. However, improving models typically implies

introducing additional complexity such as substantially increasing trainable parameters. As such,

building ML models for complex diseases—such as cancer—involves a signiﬁcant amount of trial-

and-error, and intervention from both epidemiologists and ML experts, making diagnosis, detection,

prognosis and prediction extremely time-consuming tasks.

Work from Balaprakash et al. [25] introduces a reinforcement, learning-based neural architecture

search for autonomous deep learning development. By targeting speciﬁc class of cancer data, the

automated approach ﬁnds neural architectures requiring fewer trainable parameters—thus reducing

training time—which produce equivalent or better accuracy to manually ﬁnely-tuned architectures.

Scalability is demonstrated using 1024 nodes of the Theta supercomputer, with the best neural

architecture outperforming the manually designed network in terms of scientiﬁc results, and having

11.5× fewer trainable parameters and 2.5× faster training time. These results suggest ML-driven

neural architecture search has the potential to accelerate cancer research, allowing researchers in

the ﬁeld to automate neural architecture discovery using HPC.

To accommodate needs of scientists and non-ML experts, a recent eﬀort providing ﬂexible

user tools for distributed and scalable reinforcement learning (RL) is the EXARL [26] from the

Co-Design Center for Exascale Machine Learning Technologies (ExaLearn). EXARL enables inter-

facing with existing exascale applications—e.g., LAMMPS [27] and NWChem [28]—which domain

scientists can guide using RL algorithms and associated neural network architectures. In addition

to the miniGAN proxy application [29], the ExaLearn team has demonstrated its usefulness and

exercise its scalable RL to a block copolymer application [30]. This is generally a complicated prob-

lem as materials may evolve toward generic states or become trapped in a metastable state, and

thus requiring hundreds of experimental trials to reach a target state. By mapping this problem

9

to RL, wherein a NN is trained to update annealing temperatures for subseqeunt block copoly-

mer simulations, EXARL was able to show learning convergence for guiding the annealing process

to both equilibrium and non-equilibrium states. Ongoing and future work includes expanding to

new scientiﬁc domain use-cases, and enablement of further scaling and execution of multi-process

applications.

While HPC drives much of scientiﬁc research and discovery, the platforms themselves require

continuous performance analysis and optimizations to reach their full potential. This is particu-

larly diﬃcult for I/O systems which are commonly bottlenecks in computing systems, trailing in

performance with respect to computational capabilities by several orders of magnitude. This is

due to complexity of I/O systems, requiring intimate knowledge of the underlying components and

potentially thousands of parameters need to mutual optimization.

The SIOX Project [31] monitors, diagnoses and optimizes I/O system parameters of HPC plat-

forms. The modular design of SIOX provides plug-and-play capability, allowing to use diverse

monitoring tools for data production. Plugins use oﬄine ML to predict the performance gains or

losses for diﬀerent optimization actions and online ML to perform anomaly detection. SIOX imple-

ments actuator tasks to apply the selected optimizations and evaluator tasks to measure achieved

performance.

D. Learning EveryWhere: A Canonical Example

The three modes of coupling ML with HPC are not mutually exclusive.

In fact, the most

ambitious multi-scale or multi-stage campaigns involve all three modes. For example, considering

the universe of about 1068 possible drug compounds, eﬃcient and high throughput frameworks for

early stage drug discovery [32] are needed. In silico methodologies need to be improved to better

select lead compounds that can proceed to later stages of the drug discovery protocol accelerating

the entire process [33–35].

Innovations that integrate AI and simulation at multiple levels are

demonstrating promise in overcoming fundamental limitations.

We discuss IMPECCABLE as a representative campaign that is comprised of ML-in-, ML-out-,

and ML-about-HPC workﬂows, supplanting traditional HPC with learning everywhere. Although,

IMPECCABLE was developed for COVID19 therapeutics, the multi-stage and AI-HPC integrated

campaign is representative of a range of campaigns in material and molecule design, climate science,

inter alia.

The campaign consists of an iterative loop initiated with ML predictions (ML1), followed by

10

FIG. 3. IMPECCABLE is a virtual drug discovery pipeline, from hit to lead through to lead optimization.

The constituent components are deep-learning based surrogate model for docking (ML1), Autodock-GPU

(S1), coarse and ﬁne-grained binding free energies (S3-CG and S3-FG) and ML-enhanced MD simulations.

data processing stages S1, S2, S3. ML/AI techniques (ML1 and S2) interfaced with physics-

based methods estimate docking poses of compounds that are promising leads for a given protein

target (S1) and binding free-energy computations (S3). Put together, the campaign glues together

learning methods with innovative physics-based methods, with iterative algorithms allowing both

upstream and downstream feedback to overcome fundamental limitations of classical in silico drug

design [8]. It includes high-throughput structure-based protein-ligand docking simulations, followed

by iterative reﬁnements to these virtual screening results to ﬁlter out compounds that “show

promise” in biochemical or whole-cell assays, safety and toxicology tests.

ML techniques overcome the limitations of S1 and S3 by predicting the likelihood of binding be-

tween small molecules and a protein target (ML1), and accelerating the sampling of conformational

landscapes to bound the binding free-energy values for a given protein-ligand complex (S2). In-

terfacing ML approaches with physics-based models (docking and MD simulations), we achieve at

least three orders of magnitude improvement in the size of compound libraries that can be screened

with traditional approaches, while simultaneously providing access to binding free-energy calcula-

tions that can impose better conﬁdence intervals in the ligands selected for further (experimental

or computational) optimization.

High Throughput Docking (S1)Latent Space Representation andSteered Advanced Sampling(S2)Oﬄine Ensemble DockingEnhanced Sampling of protein target statesFine-GrainedBinding Aﬃnity(S3-FG)Improved BindingFree-Energy EstimatesML-predict DockingScores (ML1)Stability Measures/Features for Protein-LigandInteractionsCoarse GrainedBinding Aﬃnity (S3-CG)Coarse GrainedBinding Aﬃnity (S3-CG)Latent Space Representation andSteered Advanced Sampling(S2)Fine-GrainedBinding Aﬃnity(S3-FG)11

S1 is an example of ML-in-HPC mode(i.e., training and using a surrogate in lieu of compu-

tations), while S2 [36] represents a common instance of the ML-about-HPC mode. Although

ML-out-HPC was not implemented on HPC platforms at the time of publication, prototypes were

used to determine optimal allocation of computational resources across S1-S3 [37].

The impact of the algorithmic, methodological and infrastructural innovations resulted by mea-

suring both raw throughput—deﬁned as ligands per unit time, scientiﬁc performance—deﬁned as

eﬀective ligands sampled per unit time, as well as the quality of ligands selected [8, 9, 38, 39].

Thus, the IMPECCABLE [9, 38] drug discovery pipeline is the quintessential example of the learn-

ing everywhere paradigm.

IV. MACHINE LEARNING AND SCIENTIFIC WORKFLOW APPLICATIONS

The coupling of AI/ML methods to HPC simulations, poses unprecedented challenges to the

development of middleware systems to support the execution of scientiﬁc workﬂow applications on

increasingly heterogeneous computing platforms. We outline three main challenges, and discuss

how the six use cases introduced in §III address those challenges.

A. Challenges

Traditionally, scientiﬁc workﬂows were deﬁned as either High Throughput Computing (HTC)

or High Performance Computing (HPC). The former came to deﬁne the distributed workﬂows

of the grid era; the later epitomized by complex and large DAGs of processing. The increasing

importance and popularity of ensembles of HPC simulations, resulted in a convergence of these

two primary modes – high-throughput of high-performance computing (HT-HPC). And ultimately

workﬂows involving dependencies between large number of (parallel) tasks, and represented by

DAG task-graph. The current workﬂow middleware reﬂects these dominant paradigms and trends.

Moving forward, they will be supplanted by middleware systems which support ML coupling to

HT-HPC workﬂows at multiple levels of the application.

Integrating ML methods with HPC simulations, results in three primary classes of workﬂows:

(1) Hybrid HPC-HTC workﬂows; (2) ML-coupled workﬂows, discussed in §III; and (3) Edge-to-

center workﬂows, which typically involve integrating distributed ML with HPC workﬂows (e.g.,

with ML on the edge). This is rapidly becoming an increasingly important type of workﬂow with

distributed data production and ML execution, and their need to couple to large data-centers.

12

Unsurprisingly, coupling ML to HPC simulations also introduces many challenges — at ap-

plication, middleware and resource levels.

In this chapter we focus on three main middleware

development challenges related to resource and task execution management to realize the full po-

tential of ML for scientiﬁc workﬂow applications: (1) task heterogeneity; (2) adaptive execution;

and (3) application performance.

ML introduces multiple levels of task heterogeneity. Alongside traditional CPU, GPU and, pos-

sibly, multi-node MPI tasks, ML usually requires the execution of high-throughput function calls,

often implemented in an interpreted language as Python, and that may depend on datasets dis-

tributed across repositories managed by diverse organization and platforms. As a consequence,

the middleware that manages the execution of the workﬂow application, has to be able to concur-

rently schedule, place and execute MPI executables alongside Python functions with wildly varying

execution lifetimes— the former for hours, the latter for as little as fractions of seconds.

One of the main scientiﬁc reasons to use ML in workﬂow applications is to improve the analy-

sis that can be done on the data produced by part of the tasks of the workﬂow application. While

some tasks progress, ML models can be used to learn relevant features and better drive the progress

of the workﬂow at runtime. In order to leverage the potential of ML-based analysis, the workﬂow

application has to become adaptive, i.e., being able to integrate the results of ML inferences and

alter the workﬂow graph accordingly, and deﬁne the amount of learning to perform at runtime, es-

pecially when that amount cannot be known in advance, before execution [36]: simulations must

be paused and restarted with new starting points, and/or a diverse number, type or size of simula-

tions must be started to account for changed requirements, based on ML inferences. Further, ML

training can vary at runtime, both in amount per model and across multiple models, when used.

That has consequences for the capabilities of the workﬂow execution middleware. Alongside the

capability of traversing an acyclic direct graph (DAG) to produce a concrete execution plan, work-

ﬂow middleware has to update that DAG, pausing/restarting the execution of some of its nodes,

adding/removing some nodes, and/or dynamically changing the amount of resources allocated to

those nodes.

Finally, for ML to be useful it must enable improvements in both scientiﬁc and execution

performance. On one side, the use of ML modeling and inference needs to improve the scientiﬁc

computation that it drives, e.g., the accuracy and/or physically simulated duration. On the other

side, ML has to eﬀectively and eﬃciently use available resources when integrated within a workﬂow

application. Resource eﬃciency depends on both the amount of time those resources are used in

order to achieve the planned goal of the workﬂow application, and the percentage of available

resources utilized to achieve that goal. This means that the workﬂow execution middleware has

to manage the concurrent execution of heterogeneous tasks in a way that maximizes resource

utilization while minimizing the workﬂow application total time to completion.

13

B. Framework and Middleware Solutions

The ML-enabled workﬂow frameworks described earlier address some or all the challenges of

task heterogeneity, adaptive execution and framework’s performance (as opposed to scientiﬁc per-

formance), at diﬀerent levels of the middleware software stack.

Proxima [40] is implemented as a Python library used to wrap a Python function. Based on its

inputs, Proxima calculates when to infer via a surrogate model or running the wrapped function.

Inferring via a surrogate model is often faster than executing the wrapped function, resulting in

an overall speedup. Proxima continually monitors the function execution, dynamically adapting

the surrogate conﬁguration parameters and determining when to retrain the surrogate model at

runtime. While Proxima executes diﬀerent types of functions (inference, monitoring, evaluation,

conﬁguration and retraining), it is not optimized for HPC and does not concurrently execute

those diﬀerent functions at scale. Proxima implements adaptivity, by retraining at runtime and

parametrizing the model. Finally, Proxima performance as Python library is evaluated in terms of

Proxima logic, model (re)training, surrogate usage, and inference.

Colmena [21] a general-purpose Python library for steering ensembles of experiments on HPC

computing systems. Colmena is designed to execute diﬀerent types of tasks, including: simulation,

inference (via surrogate models) model training, and candidate generation. Diﬀerent from Prox-

ima, Colmena is designed to scale on HPC platforms, addressing the heterogeneity challenge by

coordinating the (possibly concurrent) execution of diﬀerent types of tasks. Similar to Proxima,

Colmena enables adaptivity via surrogate parameterization and (re)training. Colmena uses Parsl

as its runtime, avoiding a reimplementation of a ML-speciﬁc and general-purpose runtime capa-

bilities. Colmena’s performance is measured in terms of communication overheads (e.g., requests

or result object, and data input or output), and scaling performance with diﬀerent task duration,

result size, and number of workers.

EXARL [29] is a Python framework build on OpenAI Gym to enable the implementation of

arbitrary reinforcement learning (RL) algorithms and their execution at scale. EXARL implements

agents, each based on a learner/actors architecture in which each agent concurrently uses a scalable

number of learners. Leaners can be implemented as multi-process or MPI executables; multiple

agents can be executed concurrently. EXARL does not oﬀer speciﬁc capabilities for mapping

and launching its agents, relying on third party tools like, for example, batch system and an MPI

infrastructure. As such, EXARL does not support task heterogeneity and implementing adaptivity

requires coding capabilities on top of its agents. Performance is currently under evaluation, in terms

of scalability of the size of each learners, and number of concurrent learners and agents.

14

MUMMI [23, 24] is a Python workﬂow manager that coordinated the execution of massively

parallel multiscale simulations. MUMMI allows to coordinate the concurrent execution of macro-

and micro-scale simulation tasks, coupling them via ML methods to decide what space of the

macro-scale simulations should be explored by the micro-scale ones. MUMMI uses the Flux job

scheduler to coordinate the scheduling and execution of heterogeneous tasks on both CPUs and

GPUs, and the Maestro workﬂow plugin to interface its workﬂow manager component to Flux.

MUMMI enables adaptivity, allowing (re)training of ML models at runtime and using them for

steering the simulations. MUMMI’s performance is evaluated in terms of resource utilization and

number of concurrent simulations executed.

IMPECCABLE [8, 9] is a drug discovery pipeline that executes heterogeneous tasks (i.e., MD

simulations, ML training and inference) on both CPUs and GPUs at scale. Implemented using

RADICAL-Cybertools as workﬂow middleware and runtime systems, it also uses DeepDriveMD.

IMPECCABLE enables adaptivity by clustering MD trajectories to steer the ensemble of MD

simulations. This may include either starting new simulations (i.e., expanding the pool of initial

MD simulations), or killing unproductive MD simulations (i.e., simulations stuck in meta-stable

states).

IMPECCABLE also supports runtime evaluation of training of docking surrogate(s).

IMPECCABLE’s performance is evaluated in terms of resource utilization, framework’s overheads,

and total time to completion of the pipeline and each of its stages.

Importantly, the capabilities oﬀered by DeepDriveMD and RADICAL-Cybertools are portable

across use cases and computational campaigns. DeepDriveMD and RADICAL-Cybertools capa-

bilities which are utilized for IMPECCABLE, also allowed for coordinating the diverse simulations

coupled to ML models, and automate their execution at scale for the #COVIDIsAirborne [41]

campaign. Work is underway to use RADICAL-Cybertools to support workﬂow orchestration,

heterogeneous task execution and adaptivity at scale.

V. DISCUSSION

15

The success of ML-enabled HPC workﬂows brings to the forth several challenges and oppor-

tunities: (1) Engineering middleware and frameworks to support for ML-enabled HPC workﬂows;

(2) ML-HPC Benchmarks to measure both execution and eﬀective performance; (3) Online ML

model engineering, to name just a few.

Consistent with the current workﬂow application landscape, many ML methods are being im-

plemented as single-point software solutions, supporting speciﬁc user-facing interfaces, use cases

and HPC platforms. Nonetheless, as seen in §IV B, some solutions are built over existing mid-

dleware, seeking beneﬁts of well-engineered and general-purpose systems. Thus, one of the main

requirements in middleware engineering for ML and HPC will be to progressively separate the ap-

plications, framework, middleware and platform concerns, enabling ML support across the stack,

without having to code a plethora of independent solutions that all implement similar capabilities.

Another of the main items of the ML-enabled HPC workﬂow applications roadmap, is to promote

the integration among existing middleware solutions to support the development of domain-speciﬁc

ML frameworks. While the middleware layer should be domain-agnostic, oﬀering general-purpose

resource and runtime management capabilities, often domain scientists require frameworks tai-

lored to their programming models and abstractions. For example, some scientists may prefer a

conﬁguration-based interface to set their applications’ parameters, while others require an API to

manage parallelism at loops level. The goal will be to develop frameworks tailored to ML-enabled

workﬂow applications, that leverage runtime capabilities already available, and expose dedicated

abstractions to the users while hiding low-level details.

Currently, ﬁlesystem performance and implementation of in-memory data sharing are among the

main limitations faced by ML-driven workﬂow applications on HPC platforms. Often, ﬁlesystems

become bottlenecks for the I/O intensive operations required by ML, especially when performed

on data continuously generated at runtime. In-memory approach to data exchange among diverse

types of workﬂow tasks still requires using task-level capabilities[42]. That creates friction between

using middleware capabilities to implement coordination protocols and the need to implement those

protocols within the tasks themselves because of in-memory communication requirements. That

impedes a clean separation of concerns between middleware and tasks, hindering the development

of general-purpose, production-grade solutions.

Finally, with the growing number of datasets stored on cloud platforms and the need to lever-

age diverse programming and computation paradigms, integrating cloud and HPC resources has

16

become a priority. Developing robust and reliable solutions for such integrations is a sociotechnical

challenge. Socially, cloud and HPC resources leverage diﬀerent economic models, making diﬃcult

to reconcile two diﬀerent resource allocation processes. Technically, the HPC multitenant batch

systems with their non-elastic resource allocations, heavily biased towards large and long single

MPI jobs, does not match the platform, container and function as a service models implemented by

cloud providers. It will be important to develop resource brokering systems, designed to seamlessly

execute large-scale, ML-enabled workﬂow applications on diverse and heterogeneous resources.

Performance will be critical for the future development of ML-enabled workﬂow applications.

Steady-state performance and resource utilization for large-scale worfklows is a known challenge.

For example, workﬂows that helped advance research and response to COVID19 and underpinned

the Gordon Bell 2020 Special Prize for COVID19 ﬁnalists had impressive peak performance, but

modest steady-state performance. With increasing heterogeneity and temporal variation in the

duration of tasks and services – as can be expected with ML-coupled HPC workﬂows, improving

steady-state performance and resource utilization becomes challenging.

Eﬀective performance, and its measure of scientiﬁc improvement over other methods, will have to

be complemented by runtime performance to assure eﬀective and eﬃcient utilization of available

computing resources.

In that context, Benchmarks will play a fundamental role to drive both

software and platform development. Benchmarks will have to be accessible and recognized by

relevant scientiﬁc communities, enabling to compare performance among algorithmic methods and

application execution. Without those benchmarks, it will not be possible to converge towards

eﬀective algorithmic solutions and, importantly, asses how eﬃciently future platform architectures

will support ML-enabled workﬂows.

The learning everywhere examples show how ML methods will have to be integrated at multiple

levels within workﬂow applications and the middleware that enable their executions. Whereas the

real impact will arise from computational campaigns that integrate ML with HPC, ML methods

will also play a fundamental role for the middleware, improving online monitoring, tracing and

proﬁling. ML methods will also enable improved scheduling algorithms, essential for the eﬀective

placement of tasks at the upcoming exascale, and preemptive data staging and caching.

Acknowledgements

The authors would like to thank Jack Well and Tom Gibbs (NVIDIA), and Addi Malviya

Thakur (ORNL) for valuable suggestions on early drafts. SJ acknowledges Geoﬀrey Fox for many

useful discussions. SJ acknowledges funding from DOE (ECP CANDLE and ExaWorks, and DE-

SC0021352), as well as NSF-1931512 (RADICAL-Cybertools).

17

[1] Peter M Kasson and Shantenu Jha. Adaptive ensemble simulations of biomolecules. Current Opinion

in Structural Biology, 52:87 – 94, 2018. Cryo electron microscopy: the impact of the cryo-EM revolution

in biology • Biophysical methods.

[2] Eugen Hruska, Vivekanandan Balasubramanian, Shantenu Jha, and Cecilia Clementi. Extensible and

scalable adaptive sampling on supercomputers. Journal of Chemical Theory and Computation (ac-

cepted), 2020. https://arxiv.org/abs/1907.06954.

[3] Geoﬀrey Fox, James Glazier, JCS Kadupitiya, Vikram Jadhao, Minje Kim, Judy Qiu, James P Sluka,

Endre Somogy, Madhav Marathe, Abhijin Adiga, et al. Learning everywhere: Pervasive machine

learning for eﬀective high-performance computation. In IEEE International Parallel and Distributed

Processing Symposium Workshops, pages 422–429. IEEE, 2019. https://arxiv.org/abs/1902.10810.

[4] M. F. Kasim, D. Watson-Parris, L. Deaconu, S. Oliver, H. Hatﬁeld, D. H. Froula, G. Gregori, M. Jarvis,

S. Khatiwala, J. Korenaga, J. Topp-Mugglestone, E. Viezzer, and S. M. Vinko. Building high accuracy

emulators for scientiﬁc simulations with deep neural architecture search. Machine Learning: Science

and Technology, page in press, 2021.

[5] Weile Jia, Han Wang, Mohan Chen, Denghui Lu, Lin Lin, Roberto Car, E Weinan, and Linfeng Zhang.

Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine

learning. In SC20: International Conference for High Performance Computing, Networking, Storage

and Analysis, pages 1–14, 2020.

[6] J Luc Peterson, Rushil Anirudh, Kevin Athey, Benjamin Bay, Peer-Timo Bremer, Vic Castillo,

Francesco Di Natale, David Fox, Jim A Gaﬀney, David Hysom, et al. Merlin: Enabling machine

learning-ready hpc ensembles. arXiv preprint arXiv:1912.02892, 2019.

[7] Hyungro Lee, Matteo Turilli, Shantenu Jha, Debsindhu Bhowmik, Heng Ma, and Arvind Ramanathan.

Deepdrivemd: Deep-learning driven adaptive molecular simulations for protein folding.

In 2019

IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS), pages 12–19. IEEE, 2019.

[8] Hyungro Lee et al. Scalable hpc and ai infrastructure for covid-19 therapeutics. Platform for Advanced

Scientiﬁc Computing Conference (PASC ’21), July 5–9, 2021, Geneva, Switzerland. ACM, New York,

NY, USA, 2021.

[9] Aymen Al Saadi et al. Impeccable: Integrated modeling pipeline for covid cure by assessing better

leads. 50th International Conference on Parallel Processing (ICPP ’21), August 9–12, 2021, Lemont,

IL, USA. ACM, New York, NY, USA, 12 pages, 2021.

[10] Lorenzo Casalino et al. Ai-driven multiscale simulations illuminate mechanisms of sars-cov-2 spike

dynamics. International Journal of High-Performance Computing Applications (IJHPCA), 2021.

18

[11] Shantenu Jha and Geoﬀrey Fox. Understanding ML Driven HPC: Applications and Infrastruc-

ture.

In 2019 15th International Conference on eScience (eScience), pages 421–427. IEEE, 2019.

https://arxiv.org/abs/1909.02363.

[12] Geoﬀrey Fox and Shantenu Jha. Learning Everywhere: A Taxonomy for the Integration of Machine

Learning and Simulations. In 2019 15th International Conference on eScience (eScience), pages 439–

448. IEEE, 2019. https://arxiv.org/abs/1909.13340.

[13] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar, N.

Samatova, and V. Kumar. Theory-Guided data science: A new paradigm for scientiﬁc discovery from

data. IEEE transactions on knowledge and data engineering, 29(10):2318–2331, October 2017.

[14] G. et al. Aad. Atlfast3: The next generation of fast simulation in atlas. Computing and Software for

Big Science, 6(1):7, Mar 2022.

[15] S. Agostinelli et al. Geant4—a simulation toolkit. Nuclear Instruments and Methods in Physics Research

Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 506(3):250–303, 2003.

[16] Han Wang, Linfeng Zhang, Jiequn Han, and Weinan E. Deepmd-kit: A deep learning package for many-

body potential energy representation and molecular dynamics. Computer Physics Communications,

228:178–184, 2018.

[17] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep

learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential

equations. Journal of Computational physics, 378:686–707, 2019.

[18] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew

Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial diﬀerential equations.

arXiv preprint arXiv:2010.08895, 2020.

[19] Prasanna Balaprakash, Romain Egele, Misha Salim, Stefan Wild, Venkatram Vishwanath, Fangfang

Xia, Tom Brettin, and Rick Stevens. Proxima: Accelerating the integration of machine learning in

atomistic simulations. In Proceedings of the International Conference on Supercomputing, pages 242–

253. ACM, 2021.

[20] C F Fischer. Hartree–fock method for atoms. a numerical approach. 1 1977.

[21] Logan Ward, Ganesh Sivaraman, J Gregory Pauloski, Yadu Babuji, Ryan Chard, Naveen Dandu, Paul C

Redfern, Rajeev S Assary, Kyle Chard, Larry A Curtiss, et al. Colmena: Scalable machine-learning-

based steering of ensemble simulations for high performance computing. In 2021 IEEE/ACM Workshop

on Machine Learning in High Performance Computing Environments (MLHPC), pages 9–20. IEEE,

2021.

[22] Dhirendra K. Simanshu, Dwight V. Nissley, and Frank McCormick. Ras proteins and their regulators

in human disease. Cell, 170(1):17–33, 2017.

[23] Francesco Di Natale et al. A massively parallel infrastructure for adaptive multiscale simulations:

Modeling ras initiation pathway for cancer. In Proceedings of the International Conference for High

Performance Computing, Networking, Storage, and Analysis, pages 1–16. ACM/IEEE, 2019.

19

[24] Harsh Bhatia et al. Generalizable coordination of large multiscale workﬂows: Challenges and learnings

at scale. In Proceedings of the International Conference for High Performance Computing, Networking,

Storage, and Analysis, pages 1–16. ACM/IEEE, 2021.

[25] Prasanna Balaprakash, Romain Egele, Misha Salim, Stefan Wild, Venkatram Vishwanath, Fangfang

Xia, Tom Brettin, and Rick Stevens. Scalable reinforcement-learning-based neural architecture search

for cancer deep learning research. In Proceedings of the International Conference for High Performance

Computing, Networking, Storage and Analysis, pages 1–33, 2019.

[26] Vinay Ramakrishnaiah, Malachi Schram, Joshua Suetterlein, Jamal Mohd-Yusof, Sayan Ghosh, Yunzhi

Huang, Ai Kagawa, Christine Sweeney, and Shinjae Yoo. Easily extendable architecture for reinforce-

ment learning (exarl). https://github.com/exalearn/EXARL, 2020.

[27] A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu, W. M. Brown, P. S. Crozier, P. J.

in ’t Veld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida, C. Trott,

and S. J. Plimpton. LAMMPS - a ﬂexible simulation tool for particle-based materials modeling at the

atomic, meso, and continuum scales. Comp. Phys. Comm., 271:108171, 2022.

[28] E. Apr`a, E. J. Bylaska, W. A. de Jong, N. Govind, K. Kowalski, T. P. Straatsma, M. Valiev, H. J. J.

van Dam, Y. Alexeev, J. Anchell, V. Anisimov, F. W. Aquino, R. Atta-Fynn, J. Autschbach, N. P.

Bauman, J. C. Becca, D. E. Bernholdt, K. Bhaskaran-Nair, S. Bogatko, P. Borowski, J. Boschen,

J. Brabec, A. Bruner, E. Cau¨et, Y. Chen, G. N. Chuev, C. J. Cramer, J. Daily, M. J. O. Deegan,

T. H. Dunning, M. Dupuis, K. G. Dyall, G. I. Fann, S. A. Fischer, A. Fonari, H. Fr¨uchtl, L. Gagliardi,

J. Garza, N. Gawande, S. Ghosh, K. Glaesemann, A. W. G¨otz, J. Hammond, V. Helms, E. D. Hermes,

K. Hirao, S. Hirata, M. Jacquelin, L. Jensen, B. G. Johnson, H. J´onsson, R. A. Kendall, M. Klemm,

R. Kobayashi, V. Konkov, S. Krishnamoorthy, M. Krishnan, Z. Lin, R. D. Lins, R. J. Littleﬁeld, A. J.

Logsdail, K. Lopata, W. Ma, A. V. Marenich, J. Martin del Campo, D. Mejia-Rodriguez, J. E. Moore,

J. M. Mullin, T. Nakajima, D. R. Nascimento, J. A. Nichols, P. J. Nichols, J. Nieplocha, A. Otero-de-

la Roza, B. Palmer, A. Panyala, T. Pirojsirikul, B. Peng, R. Peverati, J. Pittner, L. Pollack, R. M.

Richard, P. Sadayappan, G. C. Schatz, W. A. Shelton, D. W. Silverstein, D. M. A. Smith, T. A.

Soares, D. Song, M. Swart, H. L. Taylor, G. S. Thomas, V. Tipparaju, D. G. Truhlar, K. Tsemekhman,
T. Van Voorhis, ´A. V´azquez-Mayagoitia, P. Verma, O. Villa, A. Vishnu, K. D. Vogiatzis, D. Wang,

J. H. Weare, M. J. Williamson, T. L. Windus, K. Woli´nski, A. T. Wong, Q. Wu, C. Yang, Q. Yu,

M. Zacharias, Z. Zhang, Y. Zhao, and R. J. Harrison. Nwchem: Past, present, and future. The Journal

of Chemical Physics, 152(18):184102, 2020.

[29] Jim Ang, Christine Sweeney, Michael Wolf, Austin Ellis, Sayan Ghosh, Ai Kagawa, Yunzhi Huang,

Siva Rajamanickam, Vinay Ramakrishnaiah, Malachi Schram, and Shinjae Yoo. Ecp report: Update

on proxy applications and vendor interactions. pages 1–3, 2020.

[30] Francis J Alexander, James Ang, Jenna A Bilbrey, Jan Balewski, Tiernan Casey, Ryan Chard, Jong

Choi, Sutanay Choudhury, Bert Debusschere, Anthony M DeGennaro, Nikoli Dryden, J Austin El-

lis, Ian Foster, Cristina Garcia Cardona, Sayan Ghosh, Peter Harrington, Yunzhi Huang, Shantenu

20

Jha, Travis Johnston, Ai Kagawa, Ramakrishnan Kannan, Neeraj Kumar, Zhengchun Liu, Naoya

Maruyama, Satoshi Matsuoka, Erin McCarthy, Jamaludin Mohd-Yusof, Peter Nugent, Yosuke Oyama,

Thomas Proﬀen, David Pugmire, Sivasankaran Rajamanickam, Vinay Ramakrishniah, Malachi Schram,

Sudip K Seal, Ganesh Sivaraman, Christine Sweeney, Li Tan, Rajeev Thakur, Brian Van Essen, Lo-

gan Ward, Paul Welch, Michael Wolf, Sotiris S Xantheas, Kevin G Yager, Shinjae Yoo, and Byung-Jun

Yoon. Co-design center for exascale machine learning technologies (exalearn). The International Jour-

nal of High Performance Computing Applications, 35(6):598–616, 2021.

[31] Julian M Kunkel, Michaela Zimmer, Nathanael H¨ubbe, Alvaro Aguilera, Holger Mickler, Xuan Wang,

Andriy Chut, Thomas B¨onisch, Jakob L¨uttgau, Roman Michel, et al. The siox architecture–coupling

automatic monitoring and optimization of parallel i/o. In International Supercomputing Conference,

pages 245–260. Springer, 2014.

[32] Regine S Bohacek, Colin McMartin, and Wayne C Guida. The art and practice of structure-based drug

design: A molecular modeling perspective. Medicinal research reviews, 16(1):3–50, 1996.

[33] Dinler A Antunes, Didier Devaurs, and Lydia E Kavraki. Understanding the challenges of protein

ﬂexibility in drug design. Expert opinion on drug discovery, 10(12):1301–1313, 2015.

[34] Yadi Zhou, Fei Wang, Jian Tang, Ruth Nussinov, and Feixiong Cheng. Artiﬁcial intelligence in COVID-

19 drug repurposing. The Lancet Digital Health, 2020.

[35] Justin S Smith, Adrian E Roitberg, and Olexandr Isayev. Transforming computational drug discovery

with machine learning and AI, 2018.

[36] Alexander Brace, Hyungro Lee, Heng Ma, Anda Trifan, Matteo Turilli, Igor Yakushin, Todd Munson,

Ian Foster, Shantenu Jha, and Arvind Ramanathan. Achieving 100x faster simulations of complex

biological phenomena by coupling ml to hpc ensembles, 2021.

[37] Hyun-Myung Woo, Xiaoning Qian, Li Tan, Shantenu Jha, Francis J Alexander, Edward R Dougherty,

and Byung-Jun Yoon. Optimal decision making in high-throughput virtual screening pipelines. arXiv

preprint arXiv:2109.11683, 2021.

[38] Austin Clyde et al. High throughput virtual screening and validation of a sars-cov-2 main protease

non-covalent inhibitor. Journal of Chemical Informatics and Modeling, 2021.

[39] Yadu Babuji, Ben Blaiszik, Tom Brettin, Kyle Chard, Ryan Chard, Austin Clyde, Ian Foster, Zhi Hong,

Shantenu Jha, Zhuozhao Li, et al. Targeting SARS-CoV-2 with AI-and HPC-enabled lead generation:

A ﬁrst data release. arXiv preprint arXiv:2006.02431, 2020.

[40] Yuliana Zamora, Logan Ward, Ganesh Sivaraman, Ian Foster, and Henry Hoﬀmann. Proxima: Ac-

celerating the integration of machine learning in atomistic simulations.

In Proceedings of the ACM

International Conference on Supercomputing, pages 242–253, 2021.

[41] Abigail Dommer et al. #covidisairborne: AI-enabled multiscale computational microscopy of delta

SARS-CoV-2 in a respiratory aerosol (preprint). 2021.

[42] Jong Youl Choi, Jeremy Logan, Kshitij Mehta, Eric Suchyta, William Godoy, Nick Thompson, Lipeng

Wan, Jieyang Chen, Norbert Podhorszki, Matthew Wolf, et al. A co-design study of fusion whole device

modeling using code coupling. In 2019 IEEE/ACM 5th International Workshop on Data Analysis and

Reduction for Big Scientiﬁc Data (DRBSD-5), pages 35–41. IEEE, 2019.

21

