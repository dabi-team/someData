2
2
0
2

p
e
S
7

]

C
D
.
s
c
[

1
v
8
7
8
2
0
.
9
0
2
2
:
v
i
X
r
a

GPU implementation of a ray-surface intersection algorithm in
CUDA (Compute Uniﬁed Device Architecture)

Raymond Leung

Australian Centre for Field Robotics
Faculty of Engineering
The University of Sydney

September 6, 2022

ABSTRACT

These notes accompany the open-source code published in GitHub which implements a GPU-based
line-segment, surface-triangle intersection algorithm in CUDA. It mentions some relevant works and
discusses issues speciﬁc to this implementation. The goal is to provide software documentation and
greater clarity on collision buffer management which is sometimes omitted in online literature. For
real-world applications, CPU-based implementations of the test are often deemed too slow to be
useful. In contrast, the code described here targets Nvidia GPU devices and offers a solution that is
vastly more efﬁcient and scalable. The main API is also wrapped in Python. This geometry test is
applied in various engineering problems, so the software developed can be reused in new situations.1

1 Motivation

The objective is to determine for each line segment li = (rstart
) whether it crosses a user-supplied surface,
i ∈ R3, and a mesh surface that comprises Nt
given Nr rays, each described by start and end points rstart
triangles, each described by a triplet tj = [tj,1, tj,2, tj,3] which references the three vertices v(tj) ≡ [vtj,1 , vtj,2, vtj,3 ]
from the collection V = {vn}1≤n≤Nv . The result is stored in a boolean array, y ∈ {0, 1}Nr by default. However,
the program can be conﬁgured to return the intersecting point and triangle for each surface-intersecting ray. In the
envisaged application, ray segments can point in arbitrary directions. Their starting points rstart
i may be all different.
There is no requirement for the rays to originate from a single or multiple shared light sources.

, rend
i
i ∈ R3 and rend

i

The principal motivation for writing the ray-surface intersection test code in C++/CUDA is to harness the power of
GPUs [5] and increase productivity in an R&D project that is geared towards geotechnical investigations / stratigraphic
modelling. As such, achieving real-time rendering performance (e.g. 25 fps) is not the real goal, rather the goal is to
surpass the speed of off-the-shelf CPU-based solutions, such as python’s vtk and pyvista packages. For instance, the
latter takes ∼ 60s to test 10M rays against a surface with approx. 30k triangles, even with OpenMP under the hood.2
A reasonable goal is to reduce this duration by 10 to 100 fold. In general, GPU programming requires a different
mindset and conceptual organisation; a good summary of its philosophy and applications can be found in [7].

1The practical applications of the ray-surface intersection test are well understood in computer graphics. For instance, it is used
to render photo-realistic objects in a scene [1] and evaluate user cursor interactions with a 3D terrain height map [2]. Apart from
these examples, it is also used in spatial algorithms in geoscience, for instance, to label geological domains in orebody grade-block
models [3]. Its reach extends to fabrication of nanoelectronic devices and 3D topography simulation for etching and deposition
processes, where ray-surface intersection tests are used to model particle transport and surface evolution [4].

2OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory
multiprocessing programming in C, C++, and Fortran, on many platforms, instruction-set architectures and operating systems,
including Solaris, AIX, FreeBSD, HP-UX, Linux, macOS, and Windows. It consists of a set of compiler directives, library routines,
and environment variables that inﬂuence run-time behavior (Wikipedia, 2022) [6].

 
 
 
 
 
 
GPU implementation of a ray-surface intersection algorithm in CUDA

2

Implementation

Our initial CUDA design choices were informed by ﬁndings reported by Jimenez et al. in [8] which inﬂuenced how
the problem was partitioned and translated to CUDA architecture [9, 10]. In accordance, each grid (j) loads data
pertaining to a single triangle (tj), the ﬁrst thread in each thread-block computes shared attributes such as the edges
of the triangle with a synchronisation barrier. The thread index (i) is mapped to a ray-segment (li+λB), it cycles over
multiplier λ ∈ Z until all rays have been considered, with stride B = 1024 being equal to the block size. Each
thread is responsible for applying the Moller-Trumbore test to each triangle-ray (j, i + λB) combination. A number
of algorithms—Badouel, Moller-Trumbore, Segura-Feito (tetrahedra sign test) and Jimenez et al. (barycentric coor-
dinates of segment endpoint)—were considered in [11] and the study in [8] demonstrated that Moller-Trumbore [12]
is the most efﬁcient general-purpose algorithm for detecting ray-triangle intersections, requiring only 1 division, 4
dot-products and 2 vector-products in the worst case. Although not speciﬁcally mentioned by Jimenez, our initial im-
plementation also included a naïve screening or culling step that checks for ray-triangle AABB (axis-aligned bounding
box) overlap before the exact test is applied. A major conclusion in Jimenez’s study [8] was that precalculating quan-
tities is “typically counterproductive in the GPU environment” as the time required to compute these values may be
smaller than the time required to fetch them from memory. In our experience, if the ray-segment bounding boxes were
precomputed in the context of the bounding-box prescreening strategy, it increases the throughput roughly by 25%. In
this preliminary implementation (v1), the cost is still signiﬁcant even with the screening step, as it applies the overlap
test to all (Nt × Nr) combinations. Nonetheless, it is faster than some of CPU-based alternatives. This is shown in
Table 1 where speed is measured in million intersection tests per second (Mi/s) using an Nvidia GTX Titan X GPU.

Table 1: Timing comparison for the preliminary implementation

Time (s)

Speed (Mi/s) GPU gain

CPU-based solutions
PyVista (ray_trace)
PyVista (multi_ray_trace) OpenMP with 8 logical processors
VTK (obbTree.IntersectWithLine)
Our preliminary GPU CUDA implementation
Jimenez-like strategy (v1)
Reference GPU implementation
Jimenez [8] Table 2 (original reported ﬁgure)
Jimenez [8] Table 2 (hardware adjusted)
† Reported ﬁgure using GeForce GTX560Ti.
‡ Adjusted ﬁgure using an estimated speedup factor of 3.52 assuming GTX Titan X is used today.
Test surface contains 29,284 triangles, 1 × 107 (10 million) rays were used.

761.5
4832.5
3171.3

384.5
60.6
92.3

2742.0†
9651.9‡

12340.5

23.7

–
–

16.2×
2.55×
3.89×

1.27×

All this illustrates is that our preliminary CUDA implementation [13] and Jimenez’ reference implementation achieve
quite similar performance. To achieve more signiﬁcant speedup, the screening step cannot be applied in a brute force
manner to all possible ray-triangle pairs. Embedding location information about the triangles in a hierarchical structure
is key to avoiding needless evaluations. Hence, our ﬁnal implementation (version v2) makes use of more advanced data
structures and performs tree search to eliminate possibilities of overlap. Online resources provide excellent practical
guidance. Our approach is guided by Marcus [14] which in turn is based on Apetrei’s fast, agglomerative approach
[15] for building a linear bounding volume hierarchy (LBVH [16]). The idea is to construct a binary radix tree for a
set of objects (viz., triangles on the mesh surface) which are sorted using morton codes. This allows the tree structure
to be traversed in parallel by individual threads, each associated with a particular ray-segment. A common choice is
to use the centroid of the triangle vertices, cj = [vtj,1 +vtj,2 +vtj,3 ]/3, to represent the location of each triangle.
The ﬁrst step is to compute the support intervals over {cj}. Then, each triangle centroid, cj ∈ R3, is quantised as
qj ∈ Z3 to utilise the range effectively (with up to 221 levels along the x, y and z axes). The quantised triangle
coordinates, [qj,x, qj,y, qj,z], are subsequently bit-wise interleaved and converted into 64-bit long unsigned Morton
code, zj. This has the effect that when all the codes (zj) are sorted, spatial locality will be preserved, which essentially
means the triangles are grouped / clustered locally following a Z-curve scanning pattern [17] in 3D space. Once the
triangles qt ≡ Q(ct) are mapped to Morton codes zj, they are sorted to correspond to leaf nodes. The second step is to
construct the binary radix tree (LBVH) by setting up left and right child node pointers for each parent in a bottom-up
manner starting from the leaf nodes (each associated with an individual triangle). Apetrei [15] devised a single-pass
procedure for propagating the index range and bounding box values from children to parents. This construction is
implemented as GPU device code; for details, refer to the bvhConstruct kernel in “bvh_structure.h”.

2

GPU implementation of a ray-surface intersection algorithm in CUDA

Once the LBVH is built, there needs to be a way for querying bounding-box overlap with individual line-segment
bounding box using the triangle binary radix tree. The relevant structures for the latter are stored in two device memory
arrays: internalNodes and leafNodes. On paper, the size of these are always Nt − 1 and Nt. However, it is convenient
to actually store the root node as the left child of the last internal node (in internalNodes[Nt − 1]). This pointer
is accessed as a shared variable in the relevent kernel launch (bvhIntersectionKernel) which provides the starting
point for searching the tree. At the thread-block level, each thread executes the device code bvhFindCollisions(...,
idx). This GPU code plays two major roles: a) it conducts efﬁcient overlap feasibility testing using bvhTraverse
(iterative tree search) to identify a small set of triangles capable of intersecting with each ray; b) it invokes the
lib_rsi::checkRayTriangleIntersection function to verify if there is an actual crossing between the ray and sur-
face. Pictures and further explanation of the BVH traversal function can be found in (Kerras, 2012) [18].

2.1 Program structure

The code repository contains four essential ﬁles.

• gpu_ray_surface_intersect.cu contains the main program which handles I/O, allocates host/device memory

and launches the relevant kernels.

• bvh_structure.h implements the bounding volume hierachy in the lib_bvh namespace. It deﬁnes __device__
functions and provides __global__ functions (such as bvhConstruct and bvhIntersectionKernel) used in main.

• rsi_geometry.h implements the Moller-Trumbore ray-triangle intersection algorithm (and associated alge-
braic operations) in the lib_rsi namespace. It provides the checkRayTriangleIntersection interface func-
tion used in lib_bvh::bvhFindCollisions.

• morton3D.h inherits some of the Morton code procedures written by Jeroen Baert which is distributed under

an MIT license.

2.2 Compilation

The code is compiled using
/usr/local/cuda/bin/nvcc gpu_ray_surface_intersect.cu -o gpu_ray_surface_intersect

2.3 Standard usage

The basic command is
./gpu_ray_surface_intersect ${vertices_file} ${triangles_file} ${rayfrom_file} ${rayto_file}

If the input ﬁles are named “vertices_f32”, “triangles_i32”, “rayFrom_f32”, “rayTo_f32” and located in ./input
relative to the source directory, the last four command line arguments may be omitted.
To suppress terminal output, the string silent may be added as the 5th argument.
./gpu_ray_surface_intersect ${vertices} ${triangles} ${rayfrom} ${rayto} silent

2.4 Extended usage

To return results in the form of (intersecting_rays, distances, intersecting_triangles, intersecting_points) in lieu
of a crossing_detected binary array, the string barycentric is added as the 6th argument.
./gpu_ray_surface_intersect ${vertices} ${triangles} ${rayfrom} ${rayto} default barycentric

To return results in the form of (num_ray_surface_intersecting_points) , use
./gpu_ray_surface_intersect ${vertices} ${triangles} ${rayfrom} ${rayto} default intercept_count

2.5 Required input

The input consists of four binary ﬁles in little-endian format.

• vertices contains 3 × Nvertices × sizeof(float32) bytes. The Nvertices surface vertices are arranged as

v1,x, v1,y, v1,z, v2,x, v2,y, v2,z, ....

• triangles contains 3 × Ntriangles × sizeof(int32) bytes. The Ntriangles surface triangles are arranged as

t1,1, t1,2, t1,3, t2,1, t2,2, t2,3, ....

3

GPU implementation of a ray-surface intersection algorithm in CUDA

• rayFrom contains 3 × Nrays × sizeof(float32) bytes. The Nrays line-segment start points are arranged as

1,x , rstart
rstart

1,y , rstart

1,z , rstart

2,x , rstart

2,y , rstart

2,z , ....

• rayTo contains 3 × Nrays × sizeof(float32) bytes. The Nrays line-segment end points are arranged as

1,x, rend
rend

1,y, rend

1,z, rend

2,x, rend

2,y, rend

2,z, ....

Sample input data may be generated using scripts/input_synthesis.py [13].

2.6 Python wrapper

For convenience, scripts/gpu_ray_surface_intersect.py implements a wrapper class PyGpuRSI that encapsulates the
functionality of gpu_ray_surface_intersect.cu. The notebook scripts/demo.ipynb illustrates how this is used. Part
A shows how the data manipulation, compilation, run and clean-up steps can be managed using a with statement. Part
B shows how the program can be conﬁgured to return (intersecting_rays, distances, intersecting_triangles,
intersecting_points) as python objects (numpy arrays) in reference to the extended usage described in Sec. 2.4. The
notebook scripts/experimental_feature.ipynb contains an example in Part C where (num_intersecting_points)
is returned instead. The odd parity test may be used to ﬁnd ray starting points that lie inside a closed surface.

3 Discussion

In this section, we comment on speciﬁc aspects of the implementation that may seem a little obscure.

3.1 Bounding box and node deﬁnitions

The axis-aligned bounding box (AABB) and node are deﬁned in rsi_geometry.h and bvh_structure.h, respectively.

struct AABB
{

float xMin , xMax , yMin , yMax , zMin , zMax ;

};

struct BVHNode
{

AABB bounds ;
BVHNode * childLeft , * childRight ;
BVHNode * parent ; (cid:63)
(cid:63)
BVHNode * self ;
int idxSelf , idxChildL , idxChildR , isLeafChildL , isLeafChildR ; (cid:63)
int triangleID ;
int atomic ;
int rangeLeft , rangeRight ;

};

Most of these ﬁelds are explained in Robbin Marcus’ blog [14], thus there is no point repeating except in noting that
attributes marked with (cid:63) are not strictly necessary. These are included for debugging purpose during development to
verify that parent points correctly to the parent node and one of its children nodes points to the current node self, for
example, when we trace and expand a portion of the tree using the testSimulateTreeExpansion function and examines
its contents with testPrintNode. The reason for having the redundant variables idxSelf, idxChildL, idxChildR,
isLeafChildL, isLeafChildR is to explicitly identify the array indices and knowing whether idxChild? refers to an
element in the internalNodes or leafNodes. This is because the BVHNode pointers become invalid when the structure
is copied from device to host memory, dereferencing them would lead to illegal memory access, so instead we use
testDecipherDescendent to interpret idxChild? to ﬁnd the corresponding address in host memory.

The attribute bounds obviously describes the spatial extent of the bounding box for triangles[triangleID] while
[rangeLeft, rangeRight] describes the range covering the node indices of its descendants. In our implementation,
an internal node is assigned a triangleID value of -1, the root node in particular is given a unique value of -2. All leaf
nodes have non-negative values 0 ≤ triangleID < Nt. The atomic variable is used during BVH construction (see
bvhUpdateParent) to ensure the internal nodes only compute the bounds from their children bounding boxes when
both children have been identiﬁed.

4

GPU implementation of a ray-surface intersection algorithm in CUDA

3.2 Bounding box collision detection

The non-trivial part of the implementation revolves around bvhFindCollisions and bvhInsert. Although there is an
excellent description of stack pointers in [18] which also explains the thinking behind BVH tree traversal in GPU,
details about the management of the collision list are somewhat lacking. Our intention here is to highlight one feasible
approach and discuss this in practical terms. Conceptually, the TRAVERSE step begins by comparing the bounding
box of a given ray-segment (henceforth denoted Ξ) with the bounds in the current node Ni. The process starts at
If Ξ intersects with the
the root node, and when an overlap occurs, the left and right child nodes are expanded.
left child’s bounding box, ΩLi, and the left child happens to be a leaf node, node[Li].triangleID is inserted into
the collision buffer. Similarly, if Ξ intersects with the right child’s bounding box, ΩRi which happens to be a leaf
node, node[Ri].triangleID is inserted into the collision buffer. This represents part 1 of 2 functions performed by
bvhTraverse—refer to code between 1 → and ← 1 .

__device__ void bvhTraverse( const AABB & queryBox , NodePtr & bvhNode ,

NodePtr * & stackPtr , CollisionList & hits )

{

NodePtr node ( bvhNode );
bool bufferFull ( false );
do 1 →
{

// check each child node for overlap
NodePtr childL = node - > childLeft ;
NodePtr childR = node - > childRight ;
bool overlapL = overlap ( queryBox , childL );
bool overlapR = overlap ( queryBox , childR );

// query overlaps a leaf node = > report collision
if ( overlapL && isLeaf ( childL ))

bufferFull = bvhInsert ( hits , childL - > triangleID ); (cid:63)

if ( overlapR && isLeaf ( childR ))

bufferFull |= bvhInsert ( hits , childR - > triangleID ); ← 1

// query overlaps an internal node = > traverse 2 →
bool traverseL = ( overlapL && ! isLeaf ( childL ));
bool traverseR = ( overlapR && ! isLeaf ( childR ));

if (! traverseL && ! traverseR )
node = * - - stackPtr ; // pop

else
{

node = ( traverseL ) ? childL : childR ;
if ( traverseL && traverseR )

* stackPtr ++ = childR ; // push

} ← 2

}
while ( node != NULL && ! bufferFull );

bvhNode = node ;

}

The problem is that STL containers and adaptors (such as std::vector and std::stack|queue) cannot be used in
device or kernel code as they lie outside the scope of a device function. Although it is possible to allocate memory
dynamically and adjust the amount of device memory available for the heap using CUDA runtime APIs, this would
be painfully slow if each thread allocates its own memory dynamically and memory is not coalesced. Thus, the
collision buffer really needs to be statically allocated using cudaMalloc. Effectively, access to the relevant portion of
the collision buffer is compartmentalised for each individual thread. The buffer size is ﬁnite and ﬁxed. For instance,
in the following listing, MAX_COLLISION may be set to 32, say. Since there is no telling how many triangle bounding
boxes would overlap with a given ray-segment, there needs to be a way to manage this triangleID INSERT operation
to prevent array overrun. This is accomplished with bvhInsert which returns true when the hits buffer occupancy is
nearing capacity. The ray-speciﬁc collision buffer occupancy state is reﬂected by the bufferFull thread-local variable

5

GPU implementation of a ray-surface intersection algorithm in CUDA

in (cid:63) above. The code between 2 → and ← 2 pops a node pointer from the stackPtr “queue”3 when the ray bounding-
box yields an empty intersection with both child bounding boxes. Otherwise, the ﬁrst child node is visited next, and
if the second child node also intersects with the ray-bounding box, this second child node is added to the stackPtr
queue. Normally, this process continues until the “queue” is empty, i.e. when the NULL pointer is encountered. In
our implementation, it also exits the do-while loop when a full buffer is imminent. Both input arguments NodePtr*
&bvhNode and CollisionList &hits are mutable.

struct CollisionList
{

uint32_t hits [ MAX_COLLISIONS ];
int count ;

};
__device__ bool inline bvhInsert( CollisionList & collisions , int value )
{

// insert value into the hits [] array . Returned value indicates
// if buffer is full ( true = > not enough room for two elements ).
collisions . hits [ collisions . count ++] = static_cast < uint32_t >( value );
return ( collisions . count < MAX_COLLISIONS - 1)? false : true ;

}

In the next listing, we see that bvhTraverse is wrapped in the device function bvhFindCollisions which performs
two critical roles. First, it performs broad-based ray-triangle bounding box collision detection. Second, it performs
the Moller-Trumbore check (an exact intersection test) on viable candidates (a small subset of triangles) for which a
ray-triangle crossing is possible. The code in 3 initialises the stackPtr array and sets up the bvhRoot as the ﬁrst
node to visit. This is consistent with the description in [18]. Where it differs, or perhaps just some design choices
speciﬁc to our implementation, is the coupling with checkRayTriangleIntersection in 4 instead of running coarse
and ﬁne-grained checks in separate stages. It applies the Moller-Trumbore test to the thread-speciﬁc ray-segment and
a set of triangle candidates reported in the collisions list. If this list contains all potential collision candidates (the
entire subset of triangles to test for), all is good. In the event the collisions buffer is full and there are more BVH nodes
remaining to check, the nextNode returned will not be NULL; this means the do-while loop will continue unless a
ray-triangle intersection has already been found by the Moller test. Another important observation is the internal state
(or stack content) will persist in memory if the bvhTraverse function is called a second time within the do-while loop.
As should be obvious, the ray-surface intersection result is afﬁrmed in (written to) detected[rayIdx].

__device__ void bvhFindCollisions( const float * vertices ,

const int * triangles ,
const float * rayFrom ,
const float * rayTo ,
const AABB * rayBox ,
const NodePtr bvhRoot ,
CollisionList & collisions ,
int * detected ,
int rayIdx )

{

// argument ‘ collisions ‘ provides access to a thread - local
// portion of device memory that holds the collisions array .

// allocate traversal stack from thread - local memory ,
// push NULL to indicate that there are no postponed nodes .
3 →
NodePtr stack [ MAX_STACK_PTRS ];
NodePtr * stackPtr = stack ;
* stackPtr ++ = NULL ;
NodePtr nextNode ( bvhRoot );
← 3

4 →
do {

collisions . count = 0;
bvhTraverse ( rayBox [ rayIdx ] , nextNode , stackPtr , collisions );

// check for actual intersections with the triangles found so far

3The stack pointer “queue” is actually a static array with known dimension at compile time.

6

GPU implementation of a ray-surface intersection algorithm in CUDA

int candidate = 0;
while (! detected [ rayIdx ] && ( candidate < collisions . count )) {

int triangleID = collisions . hits [ candidate ++];
c h e c k R a y T r i a n g l e I n t e r s e c t i o n ( vertices , triangles , rayFrom , rayTo ,

detected , rayIdx , triangleID );

}

}
while (( detected [ rayIdx ] == 0) && ( nextNode != NULL ));
← 4

}

To give an indication of the time cost associated with BVH construction in GPU, the following table shows the
cumulative time as each step of the process is added. Overall, it is quite inexpensive for a test surface with about 29k
triangles.

Table 2: Timing associated with triangle BVH construction

Incremental steps
rbxKernel (compute ray bounding boxes)
+ discretisation / normalisation of triangle coordinates
+ create Morton code for triangles
+ sort Morton code for triangles
+ bvhResetKernel (initialise BVH and leaf nodes)
+ bvhConstruct (update parent nodes, create binary radix tree)

Elpased time (ms)
4.32
5.47
7.75
24.23
24.62
24.94

3.3 The bvhIntersect kernel

Putting everything together, the bvhFindCollisions device code is launched through the bvhIntersectKernel. The
bvhRoot represents a shared attribute that is initialised by the ﬁrst thread (within a thread-block) with barrier synchro-
nisation. Akin to version 1 of our implementation, each thread (i) iterates over the rays with a stride of i+gridDim×B
where B = 1024 is the typical size of a thread-block, and 0 ≤ gridDim < Ngrid.

__global__ void bvhIntersectionKernel( const float * __restrict__ vertices ,

const int * __restrict__ triangles ,
const float * __restrict__ rayFrom ,
const float * __restrict__ rayTo ,
const BVHNode * __restrict__ internalNodes ,
const AABB * __restrict__ rayBox ,
CollisionList * __restrict__ raytriBoxHitIDs ,
int * __restrict__ detected ,
int numTriangles , int numRays )

{

// load BVH root node into shared memory
__shared__ NodePtr bvhRoot ;
__shared__ int stride ;
if ( threadIdx . x == 0) {

bvhRoot = internalNodes [ numTriangles -1]. childLeft ;
stride = gridDim . x * blockDim . x ;

}
__syncthreads ();

int threadStartIdx = blockIdx . x * blockDim . x + threadIdx . x ;
int bufferIdx = threadStartIdx ;
// iterate if numRays exceeds dimension of thread - block
for ( int idx = threadStartIdx ; idx < numRays ; idx += stride ) {

(cid:63)

if ( idx < numRays ) {

// access thread - specific collision array
CollisionList & collisions = raytriBoxHitIDs [ bufferIdx ];
bvhFi ndCollisions ( vertices , triangles , rayFrom , rayTo , rayBox ,

bvhRoot , collisions , detected , idx );

7

GPU implementation of a ray-surface intersection algorithm in CUDA

}

}

}

Two grid-size conﬁgurations, Ngrid ∈ {1, 16}, were considered. This requires device memory allocation as follows.

CollisionList * d_hitIDs ;
cudaMalloc (& d_hitIDs , nGrids * threadBlockSize * sizeof ( CollisionList ));

Essentially, it comes down to a memory and speed trade-off. As the timing measurements show, Ngrid = 1, results in a
4.7× improvement relative to version v1. However, with Ngrid = 16, it results in a 51× improvement relative to v1—
this emphasizes the importance of incorporating an accelerating (tree) structure. It exploits the fact that multiple blocks
can be run concurrently on a streaming multiprocessor. For each thread within a given grid-block, the bufferIdx in
line (cid:63) is key to stepping correctly into the dedicated portion of device memory that registers collision. Note: the
threadStartIdx incorporates the stride, a grid-block speciﬁc offset into the rays.

4

Indicative Results

4.1 Processing time

Table 3: Timing comparison for the ﬁnal implementation

Time (s)

Speed (Mi/s) GPU / CPU gain Change (vs ‡)

CPU-based solutions
PyVista (multi_ray_trace) with OpenMP
PyVista (ray_trace)
VTK (obbTree.IntersectWithLine)
Our GPU CUDA implementation
Jimenez-like strategy (v1)
- with BVH, Ngrid = 1 (v2.0)
- with BVH, Ngrid = 16 (v2.1) preferred
- with BVH, sorted rays and Ngrid = 16 (v2.2)

60.6
384.5
92.3

23.729
5.022
0.463
0.228

4832.5(cid:63)
761.5
3171.3

12340.5
58978.1
632071.5
1284357.7

1×
0.15×
0.656×

2.5×
12×
130×
265×

–
–
–

‡
4.7×
51×
104×

In terms of speed, the adopted implementation (v2.1) is 130× faster on a GeForce GTX Titan X GPU than the best
CPU-based solution which uses PyVista’s multi_ray_trace method. The difference is stark, it completes in 463ms
compared with >60s. Furthermore, the rays used for our testing have been randomised. If the rays were ﬁrst sorted
using Morton code, execution divergence can be minimised and the compute time can be halved to about 228ms.
However, there is a substantial cost associated with sorting 10M rays. So, one feasible use case would be to sort these
rays once and evaluate against multiple surfaces if the line segments remain ﬁxed throughout an experiment.

4.2 Memory use

The ﬁgures on page 9 show memory usage on the GeForce GTX Titan X GPU device when the program is
run. Apart from minor formatting, this output was produced by /usr/local/cuda/bin/nvprof --print-gpu-trace
./gpu_ray_surface_intersect. The main observation is that bvhIntersectionKernel uses 40 registers per thread (a
total of 40960 per block) which is less than the 65536 registers available (see Appendix A.1). This kernel uses 12
bytes of static shared memory per block which is tiny compared to the 49152 bytes available. In terms of device global
memory, it uses 267.8 MB in the test case which is about 2% of the 12GB available.

8

GPU implementation of a ray-surface intersection algorithm in CUDA

O b j e c t / FunctionName

O p e r a t i o n
S t a r t

D u r a t i o n

Regs SSMem DSMem
========================================================================================
[CUDA memset ]
2 3 8 . 9 4 ms

3 8 . 1 4 7MB 2 4 2 . 9 3GB/ s Dev / −

d _ c r o s s i n g D e t e c t e d

S r c / DstMem

T h r o u g h p u t

1 5 3 . 3 5 u s

B l o c k S z

G r i d

S i z e

−

−

−

−

−

−

−

−

−

−

−

−

−

17

−

[CUDA memcpy HtoD ] d _ v e r t i c e s

2 3 9 . 1 3 ms

1 7 . 0 8 8 u s

−

[CUDA memcpy HtoD ] d _ t r i a n g l e s

2 3 9 . 2 3 ms

3 0 . 4 9 7 u s

−

[CUDA memcpy HtoD ] d_rayFrom

2 3 9 . 5 0 ms

−
[CUDA memcpy HtoD ] d _ r a y T o
−

2 3 . 7 9 6 ms

2 2 . 9 0 7 ms

2 6 3 . 4 4 ms
[ k e r n e l c o d e ]
2 8 6 . 3 9 ms

l i b _ r s i : : r b x K e r n e l

4 . 2 0 8 9 ms ( 9 7 6 6 , )

[CUDA memcpy HtoD ] d _ m o r t o n

3 1 0 . 8 3 ms

2 1 . 7 9 3 u s

−

( 1 0 2 4 , )

−

[CUDA memcpy HtoD ] d _ s o r t e d T r i a n g l e I D s

3 1 0 . 8 7 ms
[ k e r n e l c o d e ]
3 1 0 . 8 9 ms
[ k e r n e l c o d e ]
3 1 0 . 9 8 ms
[ k e r n e l c o d e ]
3 1 1 . 2 0 ms

1 1 . 3 2 8 u s

−
l i b _ b v h : : b v h R e s e t K e r n e l

−

−

8 3 . 6 8 2 u s

( 2 9 , )

( 1 0 2 4 , )

26

2 0 7 . 9 4 u s

4 8 9 . 5 8 ms

b v h C o n s t r u c t K e r n e l

( 2 9 , )

( 1 0 2 4 , )

26
lib_bvh::bvhIntersectionKernel
40

( 1 0 2 4 , )

( 1 6 , )

−

−

−

−

−

−

−

−

1 7 5 . 3 8KB 9 . 7 8 8 0GB/ s

Pg / Dev

3 4 3 . 1 7KB 1 0 . 7 3 1GB/ s

Pg / Dev

1 1 4 . 4 4MB 4 . 6 9 6 5GB/ s

Pg / Dev

1 1 4 . 4 4MB 4 . 8 7 8 8GB/ s

Pg / Dev

0B

0B

−

−

−/ −

−

−

0B

0B

12B

−

−

0B

0B

0B

2 2 8 . 7 8KB 1 0 . 0 1 2GB/ s

Pg / Dev

1 1 4 . 3 9KB 9 . 6 3 0 2GB/ s

Pg / Dev

−

−

−

−

−

−

−/ −

−/ −

−/ −

[CUDA memcpy DtoH ] h _ c r o s s i n g D e t e c t e d

8 0 0 . 8 1 ms

3 . 2 8 4 2 ms

−

−

−

−

−

3 8 . 1 4 7MB 1 1 . 3 4 3GB/ s Dev / Pg

Regs : Number o f

r e g i s t e r s u s e d p e r CUDA t h r e a d . T h i s number

i n c l u d e s

r e g i s t e r s u s e d

i n t e r n a l l y by t h e CUDA d r i v e r / t o o l s and c a n be more t h a n what

t h e c o m p i l e r

shows

s h a r e d memory a l l o c a t e d p e r CUDA b l o c k .

SSMem : S t a t i c
DSMem: Dynamic s h a r e d memory a l l o c a t e d p e r CUDA b l o c k .
S r c / DestMem : The t y p e o f
A b b r e v i a t i o n s :

Pg= P a g e a b l e , Dev= D e v i c e

s o u r c e / d e s t i n a t i o n memory a c c e s s e d by memory o p e r a t i o n / copy

9

GPU implementation of a ray-surface intersection algorithm in CUDA

A Appendix: System / device properties

A.1 GPU speciﬁcation

CUDA Device Query ( Runtime API ) version ( CUDART static linking )

Detected 1 CUDA Capable device ( s )

Device 0: "GeForce GTX TITAN X"

CUDA Driver Version / Runtime Version
CUDA Capability Major / Minor version number :
Total amount of global memory :
(24) Multiprocessors , (128) CUDA Cores / MP :
GPU Max Clock rate :
Memory Clock rate :
Memory Bus Width :
L2 Cache Size :
Maximum Texture Dimension Size (x ,y , z )

11.2 / 11.2
5.2
12213 MBytes (12806062080 bytes )
3072 CUDA Cores
1076 MHz (1.08 GHz )
3505 Mhz
384 - bit
3145728 bytes
1 D =(65536) , 2 D =(65536 , 65536) ,
3 D =(4096 , 4096 , 4096)
1 D =(16384) , 2048 layers
2 D =(16384 , 16384) , 2048 layers
65536 bytes
49152 bytes

Maximum Layered 1 D Texture Size , ( num ) layers
Maximum Layered 2 D Texture Size , ( num ) layers
Total amount of constant memory :
Total amount of shared memory per block :
Total number of registers available per block : 65536
Warp size :
Maximum number of threads per multiprocessor :
Maximum number of threads per block :
Max dimension size of a thread block (x ,y , z ): (1024 , 1024 , 64)
Max dimension size of a grid size
Maximum memory pitch :
Texture alignment :
Concurrent copy and kernel execution :
Run time limit on kernels :
Integrated GPU sharing Host Memory :
Support host page - locked memory mapping :
Alignment requirement for Surfaces :
Device has ECC support :
Device supports Unified Addressing ( UVA ):
Device PCI Domain ID / Bus ID / location ID :
Compute Mode :

(x ,y , z ): (2147483647 , 65535 , 65535)
2147483647 bytes
512 bytes
Yes with 2 copy engine ( s )
No
No
Yes
Yes
Disabled
Yes
0 / 2 / 0

32
2048
1024

< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery , CUDA Driver = CUDART , CUDA Driver Version = 11.2 ,
CUDA Runtime Version = 11.2 , NumDevs = 1 , Device0 = GeForce GTX TITAN X
Result = PASS

NVIDIA - SMI driver version is 460.27.04.

A.2 Host machine

Linux OS (RedHat 7.9 x86_64) with 16x Intel® Xeon® Platinum 8259CL CPU @ 2.50GHz. Experiments with PyVista
and VTK were run on a Windows machine with Intel® CoreTM i7-8665U CPU @ 1.90GHz, 4 cores/8 logical processors
and 32 GB RAM.

B Source code

The open-source code is distributed under the BSD 3-clause license and is available at https://github.com/
raymondleung8/gpu-ray-surface-intersection-in-cuda [13].

10

GPU implementation of a ray-surface intersection algorithm in CUDA

Acknowledgements

This work is supported by the Australian Centre for Field Robotics and the Rio Tinto Centre for Mine Automation.

References

[1] Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically based rendering: From theory to implementation.

Morgan Kaufmann, 2016.

[2] Tim Sjöstrand. Efﬁcient intersection of terrain geometry in real-time applications. B.Eng. Thesis, University of

Gothenburg, 2017.

[3] Raymond Leung. Modelling orebody structures: Block merging algorithms and block model spatial restructuring
strategies given mesh surfaces of geological boundaries. Available at: https://doi.org/10.5311/JOSIS.
2020.21.582. Journal of Spatial Information Science, 21:137–174, 2020.

[4] Paul Ludwig Manstetten. Ray-surface intersection tests for particle transport. In Efﬁcient Flux Calculations for

Topography Simulation. PhD Thesis. Technischen Universität Wien, 2018.

[5] André R Brodtkorb, Trond R Hagen, and Martin L Sætra. Graphics processing unit (GPU) programming strate-

gies and trends in GPU computing. Journal of Parallel and Distributed Computing, 73(1):4–13, 2013.

[6] Wikipedia. OpenMP. URL https://en.wikipedia.org/wiki/OpenMP.
[7] John D Owens, Mike Houston, David Luebke, Simon Green, John E Stone, and James C Phillips. GPU comput-

ing. Proceedings of the IEEE, 96(5):879–899, 2008.

[8] Juan J Jiménez, Carlos J Ogáyar, José M Noguera, and Félix Paulano. Performance analysis for GPU-based ray-
triangle algorithms. In 2014 International Conference on Computer Graphics Theory and Applications (GRAPP),
pages 1–8. IEEE, 2014.

[9] Danilo De Donno, Alessandra Esposito, Luciano Tarricone, and Luca Catarinucci. Introduction to GPU com-
puting and CUDA programming: A case study on FDTD [EM programmer’s notebook]. IEEE Antennas and
Propagation Magazine, 52(3):116–122, 2010.

[10] David B Kirk and W Hwu Wen-Mei. Programming Massively Parallel Processors: A Hands-on Approach.

Morgan Kaufmann, 2016.

[11] Juan J Jiménez, Rafael J Segura, and Francisco R Feito. A robust segment/triangle intersection algorithm for

interference tests. Efﬁciency study. Computational Geometry, 43(5):474–492, 2010.

[12] Tomas Möller and Ben Trumbore. Fast, minimum storage ray-triangle intersection. Journal of Graphics Tools,

2(1):21–28, 1997.

[13] Raymond Leung. GPU implementation of a ray-surface intersection algorithm in CUDA. Source code available
at https://github.com/raymondleung8/gpu-ray-surface-intersection-in-cuda under the bsd 3-
clause license.

[14] Robbin Marcus. Real-time raytracing part 2.1. Published on 2015-10-29. Available at http://robbinmarcus.

blogspot.com/2015/12/real-time-raytracing-part-21.html.

[15] Ciprian Apetrei.

Fast and simple agglomerative LBVH construction.

In Rita Borgo and Wen
Tang, editors, EG UK Computer Graphics & Visual Computing. The Eurographics Association. Avail-
able at:
http://diglib.eg.org/bitstream/handle/10.2312/cgvc.20141206.041-044/041-044.
pdf?sequence=1&isAllowed=y, 2014.

[16] Wikipedia. Bounding volume hierarchy. URL https://en.wikipedia.org/wiki/Bounding_volume_

hierarchy.

[17] Wikipedia. Z-order (Lebesgue, Morton space ﬁlling) curve. URL https://en.wikipedia.org/wiki/

Z-order_curve.

[18] Tero Kerras. Thinking Parallel, Part II: Tree Traversal on the GPU. Published on 2012-11-26. Available at

https://developer.nvidia.com/blog/thinking-parallel-part-ii-tree-traversal-gpu.

11

