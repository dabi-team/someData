DESIGN OF A NOVEL KOREAN LEARNING APPLICATION FOR
EFFICIENT PRONUNCIATION CORRECTION

A PREPRINT

Francis Jeon
Information System
Hanyang University
Seoul, 222 Wangsimni-ro, Seongdong-gu, South Korea.

Minseon Kim
Educational Technology
Hanyang University
Seoul, 222, Wangsimni-ro, Seongdong-gu, Seoul, Republic of Korea, South Korea.

Hanseon Joo
Information System
Hanyang University
Seoul, 222, Wangsimni-ro, Seongdong-gu, Seoul, Republic of Korea, South Korea.

May 4, 2022

ABSTRACT

The Korean wave, which denotes the global popularity of South Korea’s cultural economy, contributes
to the increasing demand for the Korean language. However, as there does not exist any application
for foreigners to learn Korean, this paper suggested a design of a novel Korean learning application.
Speech recognition, speech-to-text, and speech-to-waveform are the three key systems in the proposed
system. The Google API and the librosa library will transform the user’s voice into a sentence and
MFCC. The software will then display the user’s phrase and answer, with mispronounced elements
highlighted in red, allowing users to more easily recognize the incorrect parts of their pronunciation.
Furthermore, the Siamese network might utilize those translated spectrograms to provide a similarity
score, which could subsequently be used to offer feedback to the user. Despite the fact that we were
unable to collect sufﬁcient foreigner data for this research, it is notable that we presented a novel
Korean pronunciation correction method for foreigners.

Keywords Deep learning, Siamese network, Pronunciation, Korean, System design

0.1 Introduction

The Korean wave (Hallyu) is the term used to describe the global reach of South Korea’s cultural economy, which
exports pop culture, entertainment, music, tv dramas, and ﬁlms [1]. Thanks to the Korean wave, the demand for
the Korean language is increasing all over the world. According to the latest statistics, about 160,000 students have
enrolled in Korean courses in 2020, with the goal of learning and mastering the language. Furthermore, with about
70 million Korean language speakers, Korean is indeed the 14th most spoken language in the world. In order to help
foreign students to learn Korean effectively, the Korean government established the King Sejong Institutes, which have
213 branches in 76 countries [2]. Furthermore, as shown in the graph below, the demand for K-pop and K-drama is
continuously increasing, so the demand for Korean is also expected to rise in the upcoming years.

The purpose of this research is to create a novel application for Korean teaching apps specialized in pronunciation
correction via voice AI technology. Although learning Korean pronunciation is not challenging since there is no tone

Design of a novel Korean learning application for efﬁcient pronunciation correction

A PREPRINT

Figure 1: Content Industry Exports from 2015 to 2020

and consonant and vowel sounds are simple, double consonant/vowel letters, triphthongs, and L/R pronunciations
are obstacles that should be overcome [3]. Therefore, we tend to develop applications that could help people learn
Korean pronunciation effectively. Furthermore, as mentioned above, since the demand for learning Korean would keep
increasing, this proposed software could aid people all over the world. The rest of this paper will be organized as
follows: in the Materials and Methods section, the overall systems of the applications will be discussed. Some of the
outcomes of the experiments will be discussed in the results section, and our main ﬁndings and summary of this paper
will be addressed in the conclusion section.

0.2 Materials and Methods

0.2.1 Proposed System

The overall framework of the suggested pronunciation correction system could be found in the below ﬁgure. It consists
of three main systems: speech recognition, speech-to-text, and speech-to-MFCC. Speech recognition will be conducted
through the user’s smartphone, and Google’s speech-to-text API would convert the gathered voice into a sentence. Then
the generated sentence will be compared against the proper sentence to identify the incorrect component. Furthermore,
a librosa, which is a python package for music and audio analysis will be utilized to visualize the waveform of the given
data. A similarity score is calculated between the waveforms of the user’s voice and proper voice, which could yield the
level and top percentage of the user’s ﬂuency.

0.2.2 Google Speech-to-Text

The Cloud Speech-to-Text API was introduced in April 2017, and the latest version which is based on the novel end-to-
end machine learning algorithms supports more than 20 languages and enhances the accuracy of speech recognition.
The ’latest short’ model is designed for short utterances of a few seconds, which is useful for capturing instructions
or other one-shot-directed speech situations [4]. The ’latest long’ model is for lengthy material like media or natural
speech and dialogues. In terms of fees, speech recognition for audio less than 60 minutes is free. It charges $0.006
every 15 seconds for audio transcriptions that are longer than that [5].

0.2.3 Audio Similarity Measure (Siamese Network)

The acoustic similarity will be evaluated by using the Siamese network. This network is a deep learning algorithm that
comprises two sub-networks that work together to produce a similarity, and the two sub-networks exchange parameters
including weight. The model is made up of a sequence of convolutional layers, each having a single channel, various

2

Design of a novel Korean learning application for efﬁcient pronunciation correction

A PREPRINT

Figure 2: Overall architecture of suggested system

size ﬁlters, and a ﬁxed stride of one. Since the input image is a pair, each image is put into a sub-network to create
two outputs, and distance is calculated using the two outputs. To be more speciﬁc, the sigmoid activation function is
applied to the assessed L1 distance after calculating the L1 distance of outputs from the two sub-networks. Finally, the
loss function for the Siamese network is binary cross-entropy. The similarity between the two data is signiﬁcant if the
output value is close to 1, and it is low if it is around 0 [6].

0.2.4 Data Description

Dataset from the AIhub was utilized to yield the expected result of our proposed system, and the dataset is available
at https://aihub.or.kr/opendata/keti-data/recognition-laguage/KETI-02-002 [7]. This dataset was
collected using an emotional conversation application, in which users naturally communicated with the application for a
certain period of time, and collected data were selected through speciﬁc procedures [7].

0.3 Expected Result

In order to derive the expected result, simple experiments were conducted. Firstly, the Google sound-to-text was applied
to some of the audio ﬁles from the given dataset, and this was utilized as the answer in our system. Then we read the
same sentence and converted it into a sentence via Google’s speech-to-text engine. As a consequence, even though we
are native Koreans, it was discovered that if we do not pronounce it correctly, it does not change to the right phrase.
Therefore, for the proposed system, the wrong pronunciation could be displayed as a red one, as illustrated in the ﬁgure
below, and this will make users identify the wrong pronunciations and correct them efﬁciently.

Figure 3: Examples of the experimental results of speech-to-text API

As demonstrated below, the same audio ﬁles were utilized for the second experiment, and they were successfully
converted to Mel-frequency cepstral coefﬁcients (MFCC). The ﬁrst MFCC representation is a correct description,
but the second is inaccurate as mentioned above. With these two spectrograms, the Siamese network could yield the
similarity score, which could be further utilized as giving feedback to the user. For instance, if the derived similarity

3

Design of a novel Korean learning application for efﬁcient pronunciation correction

A PREPRINT

score is higher than 0.9, he or she could be recognized as having the same pronunciation skills as the native Korean
speaker. Furthermore, as the range of scores is 0 to 1, the users could easily recognize their ranks among the application
users.

Figure 4: Visualization of converted MFCC from the answer

Figure 5: Visualization of converted MFCC from the user

0.4 Conclusion

The aim of this research is to propose a novel Korean teaching app, which is suitable for correcting pronunciation more
effectively. Speech recognition, speech-to-text, and speech-to-waveform are the three key systems in the proposed
system. The user’s voice will be converted to both sentence and MFCC through the Google API, and librosa library.
Then the app will show the user’s sentence and answer, displaying mispronounced parts in red, which will allow users
to more effectively identify the wrong parts of their pronunciation. Furthermore, the Siamese network might provide a
similarity score using those converted spectrograms, which could then be used to provide feedback to the user. Our
paper is signiﬁcant in that it proposed a new Korean pronunciation correction system for foreigners. However, there still
exists a limitation on our paper which should be resolved in further research; since we were unable to collect appropriate
data from non-native Korean speakers, it was impossible to effectively implement our suggested application.

0.5 Reference

[1] Martinroll. “Korean Wave (Hallyu) - Rise of Korea’s Cultural Economy and Pop Culture." Martin Roll, Martin
Roll, 21 Oct. 2021, martinroll.com/resources/articles/asia/korean-wave-hallyu-the-rise-of-koreas-cultural-economy-
pop-culture.

Tzoneva,

[2]
Demand
www.1stopasia.com/blog/korean-language-trends-growing-popularity-and-demand-in-2020/#:%7E:text=Recent%20statistics,learn%20and%20master%20the%20language.

Desi.
2020."

Trends
of

and
2021,

–
Asia,

Popularity

Language

Growing

“Korean

Pulse

Pulse

Asia,

Aug.

18

of

in

[3] Hooshmand, Dana. “How Hard Is Korean for English Speakers? Complete Analysis." Discover Discomfort,
Discover Discomfort, 19 Nov. 2021, discoverdiscomfort.com/how-hard-is-korean-for-english-speakers.

[4] “Speech-to-Text: Automatic Speech Recognition |." Google Cloud, Google Cloud, cloud.google.com/speech-
to-text/?utm_source=google&utm_medium=cpc&utm_campaign=japac-KR-all-en-dr-bkws-all-super-trial-e-dr-
1009882&utm_content=text-ad-none-none-DEV_c-CRE_507268848708-ADGP_Hybrid%20%7C%20BKWS%20-
%20EXA%20%7C%20Txt%20%7E%20AI%20%26%20ML%20%7E%20Speech-to-Text_Speech%20-
%20google%20speech%20to%20text-KWID_43700030970565817-kwd-21425535976&userloc_1009871-

4

Design of a novel Korean learning application for efﬁcient pronunciation correction

A PREPRINT

network_g&utm_term=KW_google%20speech%20to%20text&gclid=Cj0KCQjwyMiTBhDKARIsAAJ-
9VveDwNf6zqyBUC7izJIVaKVjCguXcNsy_QfWBpYQcbU0be_LSNAdAEaAuTFEALw_wcB&gclsrc=aw.ds.
Accessed 4 May 2022.

[5] “Pricing | Cloud Speech-to-Text |." Google Cloud, cloud.google.com/speech-to-text/pricing. Accessed 4 May 2022.

[6] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. "Siamese neural networks for one-shot image recogni-
tion." ICML deep learning workshop. Vol. 2. 2015.

[7] “Conversation voice dataset for emotion classiﬁcation" AI Hub, https://aihub.or.kr/opendata/keti-data
/recognition-laguage/KETI-02-002. Accessed 2 May 2022.

5

