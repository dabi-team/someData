2
2
0
2

n
u
J

3
1

]

G
L
.
s
c
[

1
v
4
5
0
6
0
.
6
0
2
2
:
v
i
X
r
a

Specifying and Testing k-Safety Properties
for Machine-Learning Models

Maria Christakis
MPI-SWS
Germany
maria@mpi-sws.org

Hasan Ferit Eniser
MPI-SWS
Germany
hfeniser@mpi-sws.org

Jörg Hoffmann
Saarland University, Saarland Informatics Campus
German Research Center for Artiﬁcial Intelligence (DFKI)
Germany
hoffmann@cs.uni-saarland.de

Adish Singla
MPI-SWS
Germany
adishs@mpi-sws.org

Valentin Wüstholz
ConsenSys
Germany
valentin.wustholz@consensys.net

Abstract

Machine-learning models are becoming increasingly prevalent in our lives, for
instance assisting in image-classiﬁcation or decision-making tasks. Consequently,
the reliability of these models is of critical importance and has resulted in the
development of numerous approaches for validating and verifying their robustness
and fairness. However, beyond such speciﬁc properties, it is challenging to specify,
let alone check, general functional-correctness expectations from models. In this
paper, we take inspiration from speciﬁcations used in formal methods, expressing
functional-correctness properties by reasoning about k different executions—so-
called k-safety properties. Considering a credit-screening model of a bank, the
expected property that "if a person is denied a loan and their income decreases, they
should still be denied the loan" is a 2-safety property. Here, we show the wide ap-
plicability of k-safety properties for machine-learning models and present the ﬁrst
speciﬁcation language for expressing them. We also operationalize the language in
a framework for automatically validating such properties using metamorphic test-
ing. Our experiments show that our framework is effective in identifying property
violations, and that detected bugs could be used to train better models.

1

Introduction

Due to the impressive advances in machine learning and the unlimited availability of data, machine-
learning (ML) models, e.g., neural networks, are rapidly becoming prevalent in our lives, for instance
by assisting in image-classiﬁcation or decision-making tasks. As a result, there is growing concern
about the reliability of these models in performing such tasks. For example, it could be disastrous if
an autonomous vehicle misclassiﬁes a street sign, or if a recidivism-risk algorithm, which predicts
whether a criminal is likely to re-offend, is unfair with respect to race. The research community is, of
course, aware of these issues and has devised numerous techniques to validate and verify robustness
and fairness properties of machine-learning models (e.g., [18, 14, 31, 1, 2, 37, 5, 15, 24, 13, 36, 35]).

Preprint. Under review.

 
 
 
 
 
 
Beyond such speciﬁc properties however, it is challenging to express general functional-correctness
expectations from such models, let alone check them, e.g., how can we specify that an image classiﬁer
should label images correctly? We take inspiration from speciﬁcations used in formal methods—so-
called hyperproperties [8]—capturing functional-correctness properties by simultaneously reasoning
about multiple system executions. For example, consider a credit-screening model of a bank. The
expected property that "if a person is denied a loan and their income decreases, they should still be
denied the loan", or conversely "if a person is granted a loan and their income increases, they should
still be granted the loan", is a 2-safety hyperproperty—we need two model executions to validate
its correctness. In contrast, the property that "a person with no income should be denied a loan"
is a standard (1-)safety property since it can be validated by individual model executions. Overall,
k-safety hyperproperties generalize standard safety properties in that they require reasoning about k
different executions.

Examples. To demonstrate the wide applicability of k-safety properties for ML models, we use
examples from ﬁve distinct domains throughout this paper:

• Tabular data. Consider the COMPAS dataset [20], which determines how likely criminals are
to re-offend. An expected hyperproperty for models trained on COMPAS could be that "if the
number of committed felonies for a given criminal increases, then their recidivism risk should
not decrease". Note that this is essentially monotonicity in an input feature, a special case of the
hyperproperties we consider here.

• Images. Using the MNIST dataset [21], which classiﬁes images of handwritten digits, an expected
hyperproperty could be that "if a blurred image is correctly classiﬁed, then its unblurred version
should also be correctly classiﬁed". Note that this is not monotonicity in a feature as whether
or not an image is blurred does not constitute part of the model input (i.e., the image); instead,
blurring may affect most, if not all, pixels.

• Speech. Similarly, for the SpeechCommand dataset [42], which classiﬁes short spoken commands,
an expected hyperproperty could be that "if a speech command with white noise is correctly
classiﬁed, then its non-noisy version should also be correctly classiﬁed".

• Natural language. The HotelReview dataset [22] is used for sentiment analysis of hotel reviews. An
expected hyperproperty could be that "if a review becomes more negative, then the sentiment should
not become more positive". Note that, again, making a review more negative may signiﬁcantly
affect the model input.

• Action policies. LunarLander is a popular Gym [4] environment consisting of a 2D-world with an
uneven lunar surface and a reinforcement-learning (RL) lander, which initially appears far above
the surface and moves downward. The goal is to navigate and land the lander on its two legs; if the
body ever touches the surface, the lander crashes. An expected hyperproperty could be that "if
the lander lands successfully, then decreasing the surface height (thus giving the lander more time
to land) should also result in landing successfully". Here, even a seemingly simple change to the
initial game state may result in signiﬁcant changes to subsequent states since the policy is invoked
repeatedly during the game.

Note that, in practice, such properties are deﬁned by users, thus expressing model expectations that
are deemed important in their particular usage scenario.

Related work. There is work on expressing k-safety properties for programs [32], but no prior work
has explored how to specify such properties for ML models and how to leverage these speciﬁcations
for automated testing. Numerous techniques verify speciﬁc functional-correctness properties of
models, such as robustness (e.g., [18, 14, 31, 3, 40]), fairness (e.g., [1, 2, 37]), and others (e.g.,
[19, 39]). There are also approaches for validating such properties (e.g., [5, 15, 24, 13, 36, 35]).
Although certain popular robustness and fairness properties do, in fact, constitute 2-safety properties
(e.g., slightly perturbing the pixels of an image should not change its classiﬁcation, or changing the
race of a criminal should not make them more or less likely to re-offend), none of this work targets
general hyperproperties. The most relevant work is by Sharma and Wehrheim [30], who introduce
veriﬁcation-based testing of monotonicity in ML models. As indicated above, a model is said to
be monotone with respect to an input feature if an increase in the feature implies an increase in the
model’s prediction, e.g., the higher the income, the larger the loan. In addition, Deng et al. [10, 9]
propose an approach for testing image-based autonomous-vehicle models against safety properties
deﬁned in domain-speciﬁc behaviour templates that resemble natural language.

2

Approach and contributions. In this paper, we show the wide applicability of k-safety properties
for ML models. We design a declarative, domain-agnostic speciﬁcation language, NOMOS ("law" in
Greek), for writing them. In contrast to existing approaches, NOMOS can express general k-safety
properties capturing arbitrary relations between more than one input-output pair; these subsume the
more speciﬁc relations of robustness, fairness, and monotonicity. Going a step further, we design a
fully automated framework for validating NOMOS properties using metamorphic testing [7, 29]. On a
high-level, our framework takes as input the model under test and a set of k-safety properties for the
model. As output, it produces test cases for which the model violates the speciﬁed properties. Note
that a single test case for a k-safety property consists of k concrete inputs to the model under test.
Under the hood, the harness generator component of the framework compiles the provided NOMOS
properties into a test harness, i.e., software that tests the given model against the properties. The
harness employs a test generator and an oracle component, for generating inputs to the model using
metamorphic testing and for detecting property violations, respectively.

In summary, this paper makes the following key contributions:

• We present NOMOS, the ﬁrst speciﬁcation language for expressing general k-safety hyperproperties

for ML models.

• We demonstrate the wide applicability of such properties through case studies from several domains

and the expressiveness of our language in capturing them.

• We design and implement a fully automated framework for validating such properties using

metamorphic testing.

• We evaluate the effectiveness of our testing framework in detecting property violations across
a broad range of different domains. We also perform a feasibility study to showcase how such
violations can be used to improve model training.

2 NOMOS Speciﬁcation Language

NOMOS allows a user to specify k-safety properties over source code invoking an ML model under
test. On a high level, a NOMOS speciﬁcation consists of three parts: (1) the precondition, (2) the
source code—Python in our implementation—invoking the model, and (3) the postcondition. Pre-
and postconditions are commonly used in formal methods, for instance, in Hoare logic [16] and
design by contract [25]. Here, we adapt them for reasoning about ML models and k-safety properties.

The precondition captures the conditions under which the model should be invoked, allowing the
user to express arbitrary relations between more than one model input. It is expressed using zero
or more requires statements, each capturing a condition over inputs; the logical conjunction of
these conditions constitutes the precondition. The source code may be arbitrary code invoking the
model one or more times to capture k input-output pairs. Finally, the postcondition captures the
safety property that the model is expected to satisfy. It is expressed using zero or more ensures
statements, each taking a condition that, unlike for the precondition, may refer to model outputs; the
logical conjunction of these conditions constitutes the postcondition.

Examples. As an example, consider the NOMOS speciﬁcation of Fig. 1a expressing the COMPAS
property described earlier. On line 1, we specify that we need an input x1, i.e., a criminal. Lines 2–4
get the ﬁrst feature of x1, which corresponds to the number of felonies, and assign it to variable v1;
in variable v2, we increase this number, and create a new criminal x2 that differs from x1 only with
respect to this feature, i.e., x2 has committed more felonies than x1. Line 5 speciﬁes a precondition
that the new criminal’s felonies should not exceed a sensible limit. Lines 6–7 declare two outputs, d1
and d2, that are assigned the model’s prediction when calling it with criminal x1 and x2, respectively
(see block of Python code on lines 8–11). Finally, on line 13, we specify the postcondition that the
recidivism risk of criminal x2 should not be lower than that of x1.

Fig. 1b shows the MNIST speciﬁcation. Given an image x1 (line 1), image x2 is its blurred version
(line 2), and variable v1 contains its correct label (line 3), e.g., retrieved from the dataset. Note that
functions such as blur and label extend the core NOMOS language and may be easily added by
the user. The postcondition on line 10 says that if the blurred image is correctly classiﬁed, then so
should the original image. Note that we deﬁned a very similar speciﬁcation for the SpeechCommand
property—instead of blur, we used function wNoise adding white noise to audio.

3

1 input x1 ;
2 var v1 := getFeat ( x1 , 1);
3 var v2 := v1 + randInt (1 , 10);
4 var x2 := setFeat ( x1 , 1 , v2 );
5 requires v2 <= 20;
6 output d1 ;
7 output d2 ;
8 {
9

d1 = predict ( x1 )
d2 = predict ( x2 )

10
11 }
12 # 0 - low , 1 - medium , 2 - high risk
13 ensures d1 <= d2 ;

1 input x1 ;
2 var x2 := blur ( x1 );
3 var v1 := label ( x1 );
4 output d1 ;
5 output d2 ;
6 {
7

d1 = predict ( x1 )
d2 = predict ( x2 )

8
9 }
10 ensures d2 == v1 == > d1 == v1 ;

(a) COMPAS 2-safety hyperproperty.

(b) MNIST 2-safety hyperproperty.

1 input x1 ;
2 input x2 ;
3 var v1 := getFeat ( x1 , 1);
4 var v2 := getFeat ( x2 , 1);
5 var v3 := strConcat ( v1 , v2 );
6 var x3 := setFeat ( x1 , 1 , v3 );
7 output d1 ;
8 output d3 ;
9 {
10

d1 = predict ( x1 )
d3 = predict ( x3 )

11
12 }
13 # 0 - pos , 1 - neg
14 ensures d1 <= d3 ;

1 input s1 ;
2 var s2 := relax ( s1 );
3 output o1 ;
4 output o2 ;
5 {
6

o1 , o2 = 0 , 0
for _ in range (10):

rs = randInt (0 , MAX_INT )

7

8

9

o1 += play ( s1 , rs )
o2 += play ( s2 , rs )

10
11 }
12 # 0 - lose , 1 - win
13 ensures o1 <= o2 ;

(c) HotelReview 2-safety hyperproperty.

(d) LunarLander 20-safety hyperproperty.

Figure 1: Example k-safety speciﬁcations in NOMOS.

The HotelReview speciﬁcation is shown in Fig. 1c. Note that a hotel review consists of a positive
and a negative section, where a guest describes what they liked and did not like about the hotel,
respectively. On lines 1–2, we obtain two reviews, x1 and x2, and in variables v1 and v2 on lines 3–4,
we store their negative sections (feature 1 retrieved with function getFeat). We then create a third
review, x3, which is the same as x1 except that its negative section is the concatenation of v1 and v2
(lines 5–6). The postcondition on line 14 checks that the detected sentiment is not more positive for
review x3 than for x1.

Finally, consider the LunarLander speciﬁcation in Fig. 1d. On line 1, we obtain an input s1, which is
an initial state of the game. Line 2 "relaxes" this state to obtain a new state s2, which differs from
s1 only in that the height of the lunar surface is lower. In the block of Python code that follows
(lines 5–11), we initialize outputs o1 and o2 to zero and play the game from each initial state, s1 and
s2, in a loop; o1 and o2 accumulate the number of wins. We use a loop because the environment is
stochastic—ﬁring an engine of the lander follows a probability distribution. Therefore, by changing
the environment random seed rs on line 8, we take stochasticity into account. In each loop iteration
however, we ensure that the game starting from s2 is indeed easier, i.e., that stochasticity cannot make
it harder, by using the same seed on lines 9–10. Note that function play invokes the policy multiple
times (i.e., after every step in the game simulator). Finally, line 13 ensures that, when playing the
easier game (starting with s2), the number of wins should not decrease. Since this property depends
on 20 model invocations, it is a 20-safety property! Conversely, we can also make the game harder by
"unrelaxing" the original initial state, i.e., increasing the surface height. In such a case, when playing
the harder game, the number of wins should not exceed the original number of wins.

Grammar. Fig. 2 provides a formal grammar for NOMOS (in a variant of extended Backus-Naur
form). The top-level construct is <spec> on lines 1–2.
It consists of zero or more import
statements—the curly braces denote repetition—to import source-code ﬁles containing custom
implementations for domain-speciﬁc functions, e.g., blur or wNoise, one or more input declarations,

4

:: = { <import> } <input> { <input> } { <var_decl> } { <precond> }

{ <output> } " { " <code> " } " { <postcond> }

:: = " import " <model_name> " ; "
:: = " input " <var_name> " ; "
:: = " var " <var_name> " := " <scalar_expr> " ; "
| " var " <var_name> " := " <record_expr> " ; "

:: = " requires " <bool_expr> " ; "
:: = " output " <var_name> " ; "
:: = " ensures " <bool_expr> " ; "

| <int_expr>
| <string_expr>
| " getFeat ( " <record_expr> " ," <int_expr> " ) "
| " label ( " <record_expr> " ) "
| " randInt ( " <int_expr> " ," <int_expr> " ) "
| " strConcat ( " <string_expr> " ," <string_expr> " ) "

:: = <bool_literal>

| <var_name>
| " ! " <bool_expr>
| <bool_expr> " && " <bool_expr>
| <scalar_expr> " == " <scalar_expr>
| <scalar_expr> " < " <scalar_expr>
| <record_expr> " == " <record_expr>

1 <spec>
2
3 <import>
4 <input>
5 <var_decl>
6
7 <precond>
8 <output>
9 <postcond>
10 <scalar_expr> :: = <bool_expr>
11
12
13
14
15
16
17 <bool_expr>
18
19
20
21
22
23
24 <record_expr> :: = <var_name>
25
26
27
28
29

| " setFeat ( " <record_expr> " ," <int_expr> " ," <scalar_expr> " ) "
| " blur ( " <record_expr> " ) "
| " wNoise ( " <record_expr> " ) "
| " relax ( " <record_expr> " ) "
| " unrelax ( " <record_expr> " ) "

Figure 2: The NOMOS grammar.

variable declarations, preconditions, output declarations, the source-code block, and postconditions.
We deﬁne these sub-constructs in subsequent rules (lines 4–9). For instance, a precondition (line 7)
consists of the token requires, a Boolean expression, and a semicolon. For brevity, we omit a
deﬁnition of <code>; it denotes arbitrary Python code that is intended to invoke the model under
test and assign values to output variables. We also omit the basic identiﬁers <model_name> and
<var_name>.

The grammar additionally deﬁnes various types of expressions needed in the above sub-constructs.
In their deﬁnitions, we use the | combinator to denote alternatives. In particular, we deﬁne scalar
(lines 10–16), Boolean (lines 17–23), and record expressions (lines 24–29). The latter are used
to express complex object-like values, such as images or game states. In these deﬁnitions, we
include extensions to the core language with domain-speciﬁc functions that support the application
domains considered in this paper—e.g., getFeat and setFeat retrieve and modify record ﬁelds,
respectively. Integer and string expressions are deﬁned as expected (see appendix), and we omit the
basic scalar expressions <bool_literal>, <int_literal>, and <string_literal>.

3 Metamorphic-Testing Framework for NOMOS Speciﬁcations

Metamorphic testing [7, 29] is a testing technique that addresses the lack of an existing oracle deﬁning
correct system behavior. Speciﬁcally, given an input, metamorphic testing transforms it such that the
relation between the outputs (i.e., the output of the system under test when executed on the original
input and the corresponding output when executed on the transformed input) is known. If this relation
between outputs does not actually hold, then a bug is detected. As a simple example, consider testing
a database system; given a query as the original input, assume that the transformed input is the same
query with weakened constraints. A bug is detected if the transformed query returns fewer results
than the original one, which is more restrictive. So far, metamorphic testing has been used to test ML
models from speciﬁc application domains, e.g., image classiﬁers [11, 34], translation systems [38],
NLP models [23], object-detection systems [41], action policies [12], and autonomous cars [33, 43].

In our setting, we observe that metamorphic testing is a natural choice for validating general k-safety
properties as these also prescribe input transformations and expected output relations. For instance, in
Fig. 1a, lines 2–5 describe the transformation to input x1 in order to obtain x2, and line 13 speciﬁes
the relation between the corresponding outputs. We, therefore, design the framework in Fig. 3 for
validating a model against a NOMOS speciﬁcation using metamorphic testing. The output of our

5

Figure 3: An overview of our testing framework.

framework is a set of (unique) bugs, i.e., test cases revealing postcondition violations. For Fig. 1a, a
bug would comprise two concrete instances of a criminal, c1 and c2, such that (1) c2 differs from c1
only in having more felonies, and (2) the recidivism risk of c2 is predicted to be lower than that of c1.

Under the hood, the harness generator component of the framework compiles the NOMOS speciﬁca-
tion into a test harness, i.e., a Python program that tests the model against the speciﬁed properties. Our
implementation parses NOMOS speciﬁcations using an ANTLR4 [26] grammar. After semantically
checking the parsed abstract syntax tree (AST), our framework translates the AST into the Python
program constituting the test harness. A snippet of the generated harness for the speciﬁcation of
Fig. 1a is shown in Fig. 4. The test harness employs a test generator and an oracle component, for
generating inputs to the model using metamorphic testing and for detecting postcondition violations,
respectively.

As shown in Fig. 4, the model is tested until a user-speciﬁed budget is depleted (line 1). In each
iteration of this loop, the test generator creates k model inputs that satisfy the given precondition, if
any (lines 3-11). Speciﬁcally, for every input declaration, the test generator randomly selects an
input from a source speciﬁed in the imported ﬁles (line 4)—note that import statements are not
shown here but are deﬁned on line 3 of Fig. 2. In our evaluation, we have used both the test set and
the output of an off-the-shelf fuzzer [12] as such input sources. The metamorphic transformation of
an input can be performed through var declarations, which are compiled into temporary variables in
the harness (lines 5–7). Before the test generator returns the k generated model inputs, the speciﬁed
precondition is checked; if it is violated, the process repeats until it holds (lines 9–11).

Next, the block of Python code in the speciﬁcation is executed (lines 12–14), and ﬁnally the oracle
component checks the postcondition (lines 16–21). On line 21, the oracle records each detected
bug and processes it for subsequent de-duplication. In particular, for each bug, the oracle hashes
any source of randomness in generating the model inputs (i.e., for the example of Fig. 4, there is
randomness on lines 4 and 6). Two bugs are considered duplicate if their hashes match, that is, if the

1 while budget > 0:
2

3

4

5

6

7

8

9

10

11

# test generator
x1 = compas . randInput ()
v1 = compas . getFeat ( x1 ,1)
v2 = v1 + compas . randInt (1 ,10)
x2 = compas . setFeat ( x1 ,1 , v2 )

if not ( v2 <= 20):

compas . precond_violtn += 1
continue

12

13

14

15

16

17

18

19

20

21

22

# code
d1 = compas . predict ( x1 )
d2 = compas . predict ( x2 )

# oracle
if d1 <= d2 :

compas . passed += 1

else :

compas . postcond_violtn += 1
compas . process_bug ()

budget -= 1

Figure 4: Snippet of generated harness for the speciﬁcation of Fig. 1a.

6

SpecificationModelTest GeneratorOracleNOMOS Testing  FrameworkTranslatorViolationsTest HarnessTable 1: Number of speciﬁed properties, violated properties, and unique bugs per dataset and model.

Dataset

Model

Properties
Speciﬁed Violated

Unique
Bugs

COMPAS

GermanCredit

MNIST
SpeechCommand
HotelReview
LunarLander

NN
DT
NN
DT
NN
NN
NN
RL

12
12
10
10
1
1
4
2

7
6
6
6
1
1
4
2

960.0
294.8
295.2
286.9
22.4
14.2
3288.0
3459.0

generated model inputs are equivalent. Note that we avoid comparing model inputs directly due to
their potential complexity, e.g., in the case of game states.

4 Experimental Evaluation

So far, we have demonstrated the expressiveness of NOMOS by specifying hyperproperties for models
in diverse domains. This section focuses on evaluating the effectiveness of our testing framework in
ﬁnding bugs in these models. We describe the benchmarks, experimental setup, and results. We also
present a feasibility study on how detected bugs can be used to improve model training.

Benchmarks. We trained models using six datasets from ﬁve application domains as follows:

• Tabular data. We used the COMPAS [20] and GermanCredit [17] datasets, which we pre-process.
For each dataset, we trained a fully connected neural network (NN) and a decision tree (DT). For
COMPAS, we achieved 74% (NN) and 72% (DT) accuracy, and for GermanCredit, 78% (NN) and
70% (DT). Note that, even though we report accuracy here, the achieved score does not necessarily
affect whether a speciﬁed property holds, i.e., a perfectly accurate model could violate the property,
whereas a less accurate model might not.

• Images. Using the MNIST dataset [21], we trained a fully connected neural network achieving

97% accuracy.

• Speech. We pre-processed the SpeechCommand dataset [42] to convert waveforms to spectrograms,
which show frequency changes over time. As spectrograms are typically represented as 2D-images,
we trained a convolutional neural network classifying spectrogram images. The model achieves
84% test accuracy.

• Natural language. For the HotelReview dataset [22], we used a pre-trained Universal Sentence
Encoder (USE) [6] to encode natural-language text into high dimensional vectors. USE compresses
any textual data into a vector of size 512 while preserving the similarity between sentences. We
trained a fully connected neural network of 82% accuracy on the encoded hotel reviews.

• Action policies. In LunarLander [4], touching a leg of the lander to the surface yields reward +100,
whereas touching the body yields −100; the best-case reward is over 200. We trained an RL policy
that achieves an average reward of 175 (after 1 million training episodes).

Experimental setup. For each of these models, we wrote one or more NOMOS speciﬁcations to
capture potentially desired properties (see appendix for a complete list). Each test harness used a
budget of 5000 (see line 1 of Fig. 4), that is, it generated 5000 test cases satisfying the precondition,
if any. We ran each harness with 10 different random seeds to account for randomness in the testing
procedure. Here, we report arithmetic means (e.g., for the number of bugs) unless stated otherwise.
In all harnesses except for LunarLander, the input source (e.g., line 4 of Fig. 4) is the test set. In the
LunarLander harness, the input source is a pool of game states that was generated by π-fuzz [12]
after fuzzing our policy for 2 hours.

Results. We speciﬁed 30 properties across all datasets. Tab. 1 provides an overview of the number
of speciﬁed properties, violated properties, and unique bugs per dataset and model. Our testing
framework was able to ﬁnd violations for all datasets, and in particular, for 24 of these properties.

7

Table 2: Minimum-bug and maximum-reward policies generated with normal and guided training.

Minimum-Bug Policy
Guided
Normal

Maximum-Reward Policy

Normal

Guided

Bugs

Rew.

Bugs

Rew.

Bugs

Rew.

Bugs

Rew.

19
12
20
19
28
8
21
17
14
9

230.8
155.5
257.0
170.2
83.7
237.4
224.8
15.0
263.5
128.1

19
7
12
19
16
6
12
7
9
2

242.0
160.1
254.4
170.2
62.9
208.9
254.7
220.2
209.0
144.4

27
16
32
29
29
11
29
24
14
16

232.0
157.2
277.3
175.0
137.2
243.6
240.8
181.7
263.5
158.7

23
8
16
27
34
13
21
12
23
7

261.5
197.0
279.0
184.5
167.7
256.2
264.1
221.6
242.4
217.0

Most property violations were exhibited through tens or hundreds of unique tests. This demonstrates
that our framework is effective in detecting bugs even with as few as 5000 tests per property; in
contrast, fuzzers for software systems often generate millions of tests before uncovering a bug.

The average number of bugs per property varies signiﬁcantly depending on the property, model, and
dataset (see appendix for details). For instance, for COMPAS, the average number of bugs ranges
from 0.5 to 619.7 when testing the NN classiﬁer against each of the twelve different properties.

There are six properties that were not violated by any model trained on the COMPAS and German-
Credit datasets. For four of these properties, we observed that the involved features almost never
affect the outcome of our models, thereby trivially satisfying the properties. In the remaining cases,
the training data itself seems to be sufﬁcient in ensuring that the properties hold for the models.

Feasibility study. Our results show that our framework is effective in detecting property violations.
But are these violations actionable? A natural next step is to use them for repairing the model under
test or incorporate them when training the model from scratch—much like adversarial training for
robustness issues. While a comprehensive exploration and discussion of such options is beyond the
scope of this work, we did perform a feasibility study to investigate whether the reported violations
are indeed actionable.

For this study, we selected LunarLander due to its higher complexity. On a high level, we incorporated
buggy game states, i.e., ones that resulted in property violations, during policy training. In particular,
we adjusted the existing training algorithm (PPO [28] implemented in the SB3 library [27]) to start
episodes not only from random initial states, but also from buggy states. As training progresses,
our guided-training algorithm gradually increases the probability of starting from buggy states. The
intuition behind this choice is to focus more on "ironing out" bugs toward the end of the training,
when the policy is already able to achieve decent rewards.

Under the hood, our guided-training algorithm tests the current policy at regular intervals (every 5
rollouts in our experiments), essentially alternating between training and testing phases. Any bugs
that are found during the latest testing phase are added to a pool of buggy states from which the
algorithm selects initial states during subsequent training phases. Note that we prioritize most recently
detected buggy states, but we also include older bugs to ensure the policy does not "forget" later on.

For our experiments, we trained 10 policies with each training algorithm, i.e., normal and guided.
Tab. 2 summarizes the policies that were generated during these training runs—each row corresponds
to a training run. In the four leftmost columns, we focus on policies with the fewest number of bugs.
The ﬁrst two columns show the number of bugs and reward for the minimum-bug policy generated
during each of the normal-training runs. Note that, for policies with the same number of bugs during
a run, we show the one with higher reward. Similarly, the third and fourth columns show the same
data for guided training. In the four rightmost columns, we focus on policies with the highest reward.
Again, for policies with the same reward, we show the one with fewer bugs.

Looking at the ﬁrst and third columns, no normal-training run achieves fewer bugs than the corre-
sponding guided-training run, and guided training results in fewer bugs in 8 out of 10 runs. Looking

8

Figure 5: Increase in reward and decrease in number of bugs over time for normal and guided training.

at the second and fourth columns, guided training does not result in signiﬁcantly lower rewards for the
minimum-bug policies; in 5 out of 10 runs, guided minimum-bug policies surpass, in terms of reward,
the corresponding normal policies. In addition, when looking at the fourth and sixth columns, 4 out
of 10 guided minimum-bug policies even surpass the normal maximum-reward policies. Similarly,
when considering the maximum-reward policies, guided training results in higher rewards in 9 out of
10 runs; in 7 runs, guided policies have fewer bugs; and 4 guided maximum-reward policies have
fewer bugs than the corresponding normal minimum-bug policies.

Fig. 5 shows the increase in reward and decrease in number of bugs over time both for normal and
guided training. The dark lines represent the mean values, and the lighter shaded areas denote the
90% conﬁdence interval. As expected, we observe that, for guided training, the number of bugs is
consistently lower without compromising on the achieved reward.

Overall, our experiments show that property violations can be useful not only for assessing the quality
of a model, but also for training better models. The latter is a promising direction for future work.

5 Conclusion and Outlook

We have presented the NOMOS language for specifying k-safety properties of ML models and an
automated testing framework for detecting violations of such properties. NOMOS is the ﬁrst high-level
speciﬁcation language for expressing general hyperproperties of models, subsuming more speciﬁc
ones such as robustness and fairness. We have demonstrated the wide applicability of such properties
through case studies from several domains and evaluated the effectiveness of our framework in
detecting property violations. Although users could manually write test cases or a test harness for
each desired property, this would be tedious, repetitive, and easy to get wrong; it would also be
difﬁcult to update and extend properties if needed. In contrast, our NOMOS speciﬁcations are concise
and enable users to think about properties on a higher level of abstraction.

There are several promising directions for future work. For the ML community, model repair and
guided training might be the most interesting direction for building on NOMOS and our testing
framework. One way to think about speciﬁcations is as a, possibly inﬁnite, source of training
examples. Our feasibility study has already provided some empirical evidence for how such examples
can be incorporated in the training process. However, more work is needed, and adversarial-training
techniques could be adapted to improve the effectiveness.

For the testing community, an interesting direction could be to explore more effective input-generation
techniques, such as coverage-guided testing. This may reduce the testing time or increase the number

9

0100200300400500600700800timesteps (x 1k)120010008006004002000200400reward101520253035bugsnormal trainingguided trainingof bugs that can be found within a given time budget. Such advances can be crucial for reducing the
testing overhead when performing guided training.

For the formal-methods community, a natural next step is to build veriﬁcation tools for certifying
that a property holds for all inputs. This could be particularly promising for models that are used in
safety-critical domains, such as autonomous driving.

We believe that NOMOS can bring these communities together to facilitate the development of
functionally correct ML models.

References

[1] Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and Aditya V. Nori. FairSquare: Probabilis-

tic veriﬁcation of program fairness. PACMPL, 1:80:1–80:30, 2017.

[2] Osbert Bastani, Xin Zhang, and Armando Solar-Lezama. Probabilistic veriﬁcation of fairness

properties via concentration. PACMPL, 3:118:1–118:27, 2019.

[3] Leonard Berrada, Sumanth Dathathri, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel,
Jonathan Uesato, Sven Gowal, and M. Pawan Kumar. Make sure you’re unsure: A framework
for verifying probabilistic speciﬁcations. In NeurIPS, pages 11136–11147, 2021.

[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,

and Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.

[5] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.

In S&P, pages 39–57. IEEE Computer Society, 2017.

[6] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah
Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope,
and Ray Kurzweil. Universal sentence encoder. CoRR, abs/1803.11175, 2018.

[7] Tsong Yueh Chen, S. C. Cheung, and Siu-Ming Yiu. Metamorphic testing: A new approach for

generating next test cases. Technical Report HKUST–CS98–01, HKUST, 1998.

[8] Michael R. Clarkson and Fred B. Schneider. Hyperproperties. In CSF, pages 51–65. IEEE

Computer Society, 2008.

[9] Yao Deng, Guannan Lou, James Xi Zheng, Tianyi Zhang, Miryung Kim, Huai Liu, Chen Wang,
and Tsong Yueh Chen. BMT: Behavior driven development-based metamorphic testing for
autonomous driving models. In MET@ICSE, pages 32–36. IEEE Computer Society, 2021.
[10] Yao Deng, Xi Zheng, Tianyi Zhang, Guannan Lou, Huai Liu, and Miryung Kim. RMT:
Rule-based metamorphic testing for autonomous driving models. CoRR, abs/2012.10672, 2020.
[11] Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R. P. Ja-
gadeesh Chandra Bose, Neville Dubash, and Sanjay Podder.
Identifying implementation
bugs in machine learning based image classiﬁers using metamorphic testing. In ISSTA, pages
118–128. ACM, 2018.

[12] Hasan Ferit Eniser, Timo P. Gros, Valentin Wüstholz, Jörg Hoffmann, and Maria Christakis.
Metamorphic relations via relaxations: An approach to obtain oracles for action-policy testing.
In ISSTA. ACM, 2022. To appear.

[13] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. Fairness testing: Testing software for

discrimination. In ESEC/FSE, pages 498–510. ACM, 2017.

[14] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and
Martin T. Vechev. AI2: Safety and robustness certiﬁcation of neural networks with abstract
interpretation. In S&P, pages 3–18. IEEE Computer Society, 2018.

[15] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-

sarial examples. In ICLR, 2015.

[16] C. A. R. Hoare. An axiomatic basis for computer programming. CACM, 12:576–580, 1969.
[17] Hans Hofmann.

https://archive.ics.uci.edu/ml/

The German credit dataset.
datasets/Statlog+(German+Credit+Data).

[18] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep

neural networks. In CAV, volume 10426 of LNCS, pages 3–29. Springer, 2017.

10

[19] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex:
An efﬁcient SMT solver for verifying deep neural networks. In CAV, volume 10426 of LNCS,
pages 97–117. Springer, 2017.

[20] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyzed
https://www.propublica.org/article/

the COMPAS recidivism algorithm, 2016.
how-we-analyzed-the-compas-recidivism-algorithm.

[21] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten

digits. http://yann.lecun.com/exdb/mnist.

[22] Jiashen Liu. 515K hotel reviews data in Europe. https://www.kaggle.com/datasets/

jiashenliu/515k-hotel-reviews-data-in-europe.

[23] Pingchuan Ma, Shuai Wang, and Jin Liu. Metamorphic testing and certiﬁed mitigation of

fairness violations in NLP models. In IJCAI, pages 458–465. ijcai.org, 2020.

[24] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR. OpenReview.net, 2018.

[25] Bertrand Meyer. Eiffel: The Language. Prentice-Hall, 1992.

[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference, 2nd Edition. O’Reilly Media, 2013.

[27] Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah
Dormann. Stable baselines3, 2019. https://github.com/DLR-RM/stable-baselines3.

[28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. CoRR, abs/1707.06347, 2017.

[29] Sergio Segura, Gordon Fraser, Ana B. Sánchez, and Antonio Ruiz Cortés. A survey on

metamorphic testing. TSE, 42:805–824, 2016.

[30] Arnab Sharma and Heike Wehrheim. Higher income, larger loan? Monotonicity testing of

machine learning models. In ISSTA, pages 200–210. ACM, 2020.

[31] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. An abstract domain for

certifying neural networks. PACMPL, 3:41:1–41:30, 2019.

[32] Marcelo Sousa and Isil Dillig. Cartesian Hoare logic for verifying k-safety properties. In PLDI,

pages 57–69. ACM, 2016.

[33] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. DeepTest: Automated testing of

deep-neural-network-driven autonomous cars. In ICSE, pages 303–314. ACM, 2018.

[34] Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail E. Kaiser, and Baishakhi Ray. Testing DNN
image classiﬁers for confusion & bias errors. In ICSE, pages 1122–1134. ACM, 2020.

[35] Florian Tramèr, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, Jean-Pierre Hubaux,
Mathias Humbert, Ari Juels, and Huang Lin. FairTest: Discovering unwarranted associations in
data-driven applications. In EuroS&P, pages 401–416. IEEE Computer Society, 2017.

[36] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. Automated directed fairness

testing. In ASE, pages 98–108. ACM, 2018.

[37] Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang. Perfectly parallel

fairness certiﬁcation of neural networks. PACMPL, 4:185:1–185:30, 2020.

[38] Improving Machine Translation Systems via Isotopic Replacement. Sun, zeyu and zhang, jie m.
and xiong, yingfei and harman, mark and papadakis, mike and zhang, lu. In ICSE. ACM, 2022.
To appear.

[39] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security
analysis of neural networks using symbolic intervals. In Security, pages 1599–1614. USENIX,
2018.

[40] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter.
Beta-CROWN: Efﬁcient bound propagation with per-neuron split constraints for neural network
robustness veriﬁcation. In NeurIPS, pages 29909–29921, 2021.

[41] Shuai Wang and Zhendong Su. Metamorphic object insertion for testing object detection

systems. In ASE, pages 1053–1065. IEEE Computer Society, 2020.

11

[42] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. CoRR,

abs/1804.03209, 2018.

[43] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid. DeepRoad:
GAN-based metamorphic testing and input validation framework for autonomous driving
systems. In ASE, pages 132–142. ACM, 2018.

12

A List of Appendices

Below, we provide a brief description of each section in the appendix.

• We provide additional grammar deﬁnitions in Appendix B.
• We include all NOMOS speciﬁcations for our case studies in Appendix C.
• We present additional details of our experimental results (e.g., number of bugs for each speciﬁca-

tion) in Appendix D.

• We present additional details of our experimental setup (e.g., details on model training and hardware

setup) in Appendix E.

B Additional Grammar Deﬁnitions

Grammar deﬁnition for integer expressions in NOMOS:

1 <int_expr>
2
3
4
5
6
7

:: = <int_literal>
| <var_name>
| " -" <int_expr>
| <int_expr> " + " <int_expr>
| <int_expr> " -" <int_expr>
| <int_expr> " * " <int_expr>
| <int_expr> " / " <int_expr>

Grammar deﬁnition for string expressions in NOMOS:

1 <string_expr> :: = <string_literal>
2

| <var_name>

We omit the basic scalar expressions <int_literal> and <string_literal>.

C Speciﬁcations

Below, we provide a description of each speciﬁed property:

Felony Inc If the number of committed felonies for a criminal increases, then their recidivism risk

should not decrease.

Felony Dec If the number of committed felonies for a criminal decreases, then their recidivism risk

should not increase.

Misdmnr Inc If the number of committed misdemeanors for a criminal increases, then their recidi-

vism risk should not decrease.

Misdmnr Dec If the number of committed misdemeanors for a criminal decreases, then their

recidivism risk should not increase.

Priors Inc If the number of priors for a criminal increases, then their recidivism risk should not

decrease.

Priors Dec If the number of priors for a criminal decreases, then their recidivism risk should not

increase.

Others Inc If the number of other crimes committed by a criminal increases, then their recidivism

risk should not decrease.

Others Dec If the number of other crimes committed by a criminal decreases, then their recidivism

risk should not increase.

IsRecid Set If a criminal becomes a recidivist, then their recidivism risk should not decrease.
IsRecid Unset If a criminal ceases to be a recidivist, then their recidivism risk should not increase.
IsVRecid Set If a criminal becomes a violent recidivist, then their recidivism risk should not de-

crease.

IsVRecid Unset If a criminal ceases to be a violent recidivist, then their recidivism risk should not

increase.

13

Crdt Amount Inc If the credit amount requested by a person increases, then they should not be

more likely to receive it.

Crdt Amount Dec If the credit amount requested by a person decreases, then they should not be

less likely to receive it.

Crdt Hist Inc If a person’s credit history worsens, then they should not be more likely to receive

credit.

Crdt Hist Dec If a person’s credit history improves, then they should not be less likely to receive

credit.

Empl Since Inc If a person’s employment years increase, then they should not be less likely to

receive credit.

Empl Since Dec If a person’s employment years decrease, then they should not be more likely to

receive credit.

Install Rate Inc If a person’s installment rate (as a percentage of their disposable income) increases,

then they should not be more likely to receive credit.

Install Rate Dec If a person’s installment rate (as a percentage of their disposable income) decreases,

then they should not be less likely to receive credit.

Job Inc If a person is promoted, then they should not be less likely to receive credit.

Job Dec If a person is demoted, then they should not be more likely to receive credit.

Blur If a blurred image is correctly classiﬁed, then its unblurred version should also be correctly

classiﬁed.

WNoise If a speech command with white noise is correctly classiﬁed, then its non-noisy version

should also be correctly classiﬁed.

Pos-1 Deleting the positive comments of a hotel review should not make it more positive.

Pos-2 If more positive comments are added to a hotel review, it should not become more negative.

Neg-1 Deleting the negative comments of a hotel review should not make it more negative.

Neg-2 If more negative comments are added to a hotel review, it should not become more positive.

Relax If the lander lands successfully, then decreasing the surface height (thus giving the lander

more time to land) should also result in landing successfully.

Unrelax If the lander fails to land, then increasing the surface height (thus giving the lander less

time to land) should also result in failing to land.

D Additional Details of our Experimental Results

In this section, we provide more detailed results on the number of unique bugs for each individual
speciﬁcation.

The results for COMPAS are shown in Tab. 3. Column 2 shows the average number of unique bugs
for the NN model and for each of the 12 speciﬁcations. Column 3 shows the same data for the DT
model. For properties "IsRecid Set" and "IsRecid Unset", we observed that the involved feature
"IsRecid" almost never affects the outcome of our models, thereby trivially satisfying the properties.

Tab. 4 shows similar results for GermanCredit. For properties "Install Rate Inc" and "Install Rate
Dec", we observed that the involved feature "Installment Rate" almost never affects the outcome of
our models, thereby trivially satisfying the properties.

Tab. 5 shows similar results for the remaining benchmarks.

E Additional Details of our Experimental Setup

E.1 Training Setup

For the COMPAS dataset, we trained a fully connected neural network and a decision tree classiﬁer.
The neural network is composed of 3 hidden layers of size 12, 9, and 9. We use the RMSprop

14

Table 3: Average number of unique bugs for each COMPAS speciﬁcation.

Speciﬁcation

Felony Inc
Felony Dec
Misdmnr Inc
Misdmnr Dec
Priors Inc
Priors Dec
Others Inc
Others Dec
IsRecid Set
IsRecid Unset
IsVRecid Set
IsVRecid Unset

Unique Bugs
DT
NN

42.9
0.0
619.7
4.0
0.5
0.0
289.0
3.0
0.0
0.0
0.9
0.0

3.5
0.0
0.0
0.0
90.0
97.2
91.1
8.0
0.0
0.0
5.0
0.0

Table 4: Average number of unique bugs for each GermanCredit speciﬁcation.

Speciﬁcation

Crdt Amount Inc
Crdt Amount Dec
Crdt Hist Inc
Crdt Hist Dec
Empl Since Inc
Empl Since Dec
Install Rate Inc
Install Rate Dec
Job Inc
Job Dec

Unique Bugs
DT
NN

0.0
0.0
78.1
122.8
13.5
30.9
0.0
0.0
47.9
2.0

122.3
75.4
8.0
31.2
9.1
40.9
0.0
0.0
0.0
0.0

algorithm for optimization. For decision-tree training, we set the max _depth parameter to 8. For
training, we shufﬂe the data and use 67% of it.

For the GermanCredit dataset, we trained a fully connected neural network and a decision tree
classiﬁer. The neural network is composed of 1 hidden layer of size 10, and we use the Adam
optimizer. In decision-tree training, we set the max _depth parameter to 6. For training, we shufﬂe
the data and use 67% of it.

For the MNIST dataset, we trained a fully connected neural network consisting of 3 hidden layers
(each with 30 neurons), and we use the Adam optimizer. We use the regular training set for training.

Table 5: Average number of unique bugs for all MNIST, SpeechCommand, HotelReview, and
LunarLander speciﬁcations.

Benchmark

Speciﬁcation Unique Bugs

MNIST Blur

SpeechCommand WNoise
HotelReview Pos-1
Pos-2
Neg-1
Neg-2
LunarLander Relax

Unrelax

15

22.4
14.2
861.1
876.1
756.2
794.6
124.5
3334.5

For the SpeechCommand dataset, we apply a number of pre-processing steps and infer a spectrogram
image for each audio ﬁle. We use 80% of the spectogram inputs for training a convolutional neural
network consisting of 2 convolutional layers with kernels (32x32x3) and (64x64x3), and a fully
connected layer of size 128. We use dropout for regularization and Adam for optimization.

The HotelReview dataset consists of over 515k reviews, and only ca. 85k of them are scored above
6 (out of 10)—labeled as positive in our evaluation. We sample the same number of inputs from
the ones that are labeled as negative to form a new dataset consisting of around 170k inputs. We
use 90% of them as training set. We use the USE model from Tensorﬂow Hub1. The USE-encoded
reviews are used to train a fully connected neural network with 2 hidden layers (256 and 128 neurons,
respectively). We use dropout for regularization and Adam for optimization.

For the LunarLander dataset, we use the default PPO implementation in the SB3 library for training
the agent.

We use ReLU activation functions in all neural networks.

We use Tensorﬂow v2.7 and the scikit-learn v1.0.2 framework for training neural networks and
decision trees, respectively.

E.2 Hardware Setup

We use a cluster with a Quadro RTX 8000 GPU and an Intel(R) Xeon(R) Gold 6248R CPU @
3.00GHz for training models and running tests. Running 5k tests takes a few seconds for decision
trees. It takes longer for neural networks, ranging from 5 to 20 minutes depending on the speciﬁcation
and the dataset. For LunarLander, it takes up to 4 hours.

The total amount of compute for all experiments is ca. 1 day on the above cluster.

1https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3

16

