2
2
0
2

n
u
J

1
1

]
E
S
.
s
c
[

1
v
0
8
4
5
0
.
6
0
2
2
:
v
i
X
r
a

CodeS: A Distribution Shift Benchmark Dataset for
Source Code Learning

Qiang Hu ∗
University of Luxembourg

Yuejun Guo ∗
University of Luxembourg

Xiaofei Xie
Singapore Management University

Maxime Cordy
University of Luxembourg

Lei Ma
University of Alberta

Mike Papadakis
University of Luxembourg

Yves Le Traon
University of Luxembourg

Abstract

Over the past few years, deep learning (DL) has been continuously expanding its
applications and becoming a driving force for large-scale source code analysis in
the big code era. Distribution shift, where the test set follows a different distribution
from the training set, has been a longstanding challenge for the reliable deployment
of DL models due to the unexpected accuracy degradation. Although recent
progress on distribution shift benchmarking has been made in domains such as
computer vision and natural language process. Limited progress has been made on
distribution shift analysis and benchmarking for source code tasks, on which there
comes a strong demand due to both its volume and its important role in supporting
the foundations of almost all industrial sectors. To ﬁll this gap, this paper initiates
to propose CodeS, a distribution shift benchmark dataset, for source code learning.
Speciﬁcally, CodeS supports 2 programming languages (i.e., Java and Python)
and 5 types of code distribution shifts (i.e., task, programmer, time-stamp, token,
and concrete syntax tree (CST)). To the best of our knowledge, we are the ﬁrst
to deﬁne the code representation-based (token and CST) distribution shifts. In
the experiments, we ﬁrst evaluate the effectiveness of existing out-of-distribution
(OOD) detectors and the reasonability of the distribution shift deﬁnitions, and
then measure the model generalization of popular code learning models (e.g.,
the pre-trained language model, CodeBERT) on classiﬁcation tasks using CodeS.
The results demonstrate that 1) only softmax score-based OOD detectors perform
well on CodeS, 2) distribution shift causes the accuracy degradation in all code
classiﬁcation models, 3) representation-based distribution shifts have a higher
impact on the model than others, and 4) pre-trained models are more resistant to
distribution shifts. We make CodeS publicly available, enabling follow-up research
on the quality assessment of code learning models.

1

Introduction

Source code is the fundamental element of software, which has been the pillar to support almost all
industrial sectors nowadays. The rapid advance of open source platforms (e.g., Github [1]), as well

∗ Equal Contribution.

Preprint. Under review.

 
 
 
 
 
 
as the large number of (both publically and industry internally) available software systems, bring
challenges to traditional software analysis techniques, but offer new opportunities in understanding
and reusing existing “big code” for software development by the data-driven approach. In particular,
to enable the large volume of source code analysis, there comes a new trend in both software
engineering and machine learning communities – machine learning for source code [2] in the last
decade where deep neural networks (DNNs) have been widely employed for source code learning
and achieved remarkable success in various tasks, e.g., code summarization [3], method naming [4],
and source code classiﬁcation [5].

In practice, the main activity of machine learning for source code remains at the stage of designing
effective code representation techniques and model architectures, which are evaluated on the pure
test data. Speciﬁcally, the most commonly used evaluation methodology of existing works is simply
splitting a dataset into training, validation, and test sets and testing a model on the test set. In this way,
the reported performance on the test set can only reﬂect the model performance on in-distribution data
since the test set follows a similar distribution as the training set. However, with the rapid evolution of
technology and software, new source code is programmed every day. Distribution shift often happens
in the new data after the model has been deployed in the wild, which becomes a big challenge, and
causes the model to produce unreliable predictions [6]. Here, distribution shift means that the test
set follows a different data distribution from the training set. For example, in code learning models,
compared to the training data, the new code data might be written by new developers (with different
programming styles or habits) or collected from new projects using different techniques. Therefore,
datasets with different types of distribution shifts are needed for the evaluation of code learning
models.

Some recent progress has been made on building data distribution shift on domains like computer
vision [7] and natural language processing [6] with data distribution shift has already gained con-
siderable attention and multiple benchmarks have been constructed. For example, CIFAR-10-C
[7] provides 15 types of algorithmically generated corruptions that generate data with different
distributions from CIFAR-10. Wilds [6] collects natural datasets with distribution shifts including
both image and text data in the wild. However, the study of data distribution shifts for the source
code domain is still at a very early stage. Although some distribution shifts on source code [6, 8]
are deﬁned, they only considered rather simple scenarios, e.g., shifts from cross projects or time
periods. The ﬁne-grained and important features of source code data, for example, the frequency of
represented token, are missed. Besides, existing datasets are not multi-language friendly because
they only support one speciﬁc programming language such as only Java or Python. The above issues
and challenges limit the use of these datasets and the generality of conclusions that were drawn.

In this paper, we introduce CodeS, a distribution shift benchmark dataset for source code learning.
Overall, CodeS covers ﬁve types of code-level distribution shifts from two perspectives, natural distri-
bution shift and representation-based distribution shift. For natural distribution shift, we introduce
cross task shift, cross programmer shift, and time period shift. For representation-based distribution
shift, we deﬁne token frequency shift and concrete syntax tree (CST) similarity shift. Speciﬁcally, to
build the datasets of CodeS, we ﬁrst collect three in-distribution sets: two are collected from existing
code classiﬁcation datasets (Java250 and Python800), and one is crawled from the AtCoder site [9]
by ourselves. Afterward, for each set, we generate ﬁve versions of data shift as described above.
Finally, CodeS contains 16 groups of in/out-of-distribution (ID/OOD) data with two programming
languages, i.e., Java and Python.

Based on the constructed datasets, we conduct experiments to investigate 1) the effectiveness of
existing OOD detectors in distinguishing ID and OOD source code data, 2) the rationality of the
deﬁnitions of source code distribution shift, and 3) the generalization ability of existing source code
models on classiﬁcation tasks. For the rationality exploration, we employ existing OOD detectors
to quantify the shift degree. Generally, the more different the two sets are, the easier the OOD
detector can distinguish them. The experimental results demonstrated that only the simple softmax
score-based OOD detector performs well on our dataset. The representation-based distribution shifts
have higher impact on the performance of the models, where the token distribution shift brings the
biggest accuracy degradation.

To summarize, the main contributions of this paper are:

2

Table 1: Comparison between our developed CodeS dataset and existing datasets.

Wilds [6]
Li et al. [8]
Nie et al. [10]
CodeS

Programming Language Task Programmer Time-stamp Token CST
(cid:35)
(cid:35)
(cid:35)
(cid:33)

Python
Java
Java
Python, Java

(cid:35)
(cid:35)
(cid:35)
(cid:33)

(cid:35)
(cid:33)
(cid:33)
(cid:33)

(cid:33)
(cid:33)
(cid:33)
(cid:33)

(cid:35)
(cid:33)
(cid:35)
(cid:33)

• We propose CodeS1, a benchmark dataset that provides ﬁne-grained distribution shift datasets for
source code learning. To the best of our knowledge, this is the ﬁrst benchmark that covers two
programming languages (Java and Python) and ﬁve types of distribution shifts.

• We perform a comprehensive evaluation of the effectiveness of existing OOD detectors and the

rationality of the deﬁned distribution shift.

• We performed an in-depth investigation on popular source code classiﬁcation models using CodeS
and demonstrated that code representation-based (token and CST) distribution shifts cause signiﬁ-
cant degradation in model performance.

2 Related Work

Source code learning. Since source code can be represented as text data (e.g., sequence of tokens)
and structural data (e.g., data ﬂow), deep neural networks have been employed for learning such
representations to solve different tasks in recent years [2]. In which, source code classiﬁcation
is one of the widely studied tasks for code understanding, and many source code classiﬁcation
applications have been proposed. Alon et al. [4] proposed the very early work, Code2Vec, which aims
at predicting the function name of a code snippet by learning the distributed code representations. Lu
et al. [11] proposed a large-scale dataset CodeXGLUE for code understanding. It supports 2 code
binary classiﬁcation tasks that are related to the privacy and vulnerability of code, clone detection
and defect detection. More recently, researchers from IBM [12] proposed CodeNet which contains
source code solution classiﬁcation tasks with multiple programming languages, e.g., Java and Python.
Different from existing works that only consider clean datasets without data shift, we construct the
dataset including both clean data and distribution shift data, which can be better used to measure the
performance of trained models from a more practical perspective.

Distribution shift in source code. Some works have studied the distribution shift in source code
data. Table 1 lists the overview of the comparison between existing datasets and CodeS. More
detailed, Wilds, proposed by Koh et al. [6], is the ﬁrst work that mentioned the data distribution shift
problem for code completion in source code learning. Wilds ﬁrst deﬁned the cross-project distribution
shift that the shift might come from the data biases of different code repositories. Besides, Li et al.
[8] conducted an empirical study to investigate the prediction uncertainty of models under three types
of distribution shifts, code collected from different projects, code written by different programmers,
and code collected across different time periods. The authors found that the uncertainty metrics
proposed in the computer vision domain are not fully applicable for source code tasks, e.g., code
summarization and code completion. Moreover, in the latest work, Nie et al. [10] discussed that the
trained models should be evaluated in different methodologies according to the real use cases in the
code summarization task. For example, in the evolution scenario, the developers should consider
if the model trained at time t0 can still be used at time t1 in the future, which can be another data
distribution shift. Compared to the above works, CodeS provides datasets with more comprehensive
and ﬁne-grained distribution shifts, i.e., more programming languages and more diverse data shift.
Furthermore, based on the benchmarks, we evaluate the rationality of our distribution shift deﬁnitions
using OOD detectors.

1The dataset is available at: https://github.com/testing-cs/CodeS.git

3

Figure 1: An example of task distribution shift. Two programs target different tasks using Python.

Figure 2: An example of programmer distribution shift. Both users submit programs to AtCoder
Beginner Contest: 058, Task A – A - ι⊥ l. Programming language: Python.

3 The CodeS Dataset

3.1 Examples of distribution shifts

We show two examples of code distribution shifts collected from the submissions on programming
contest site, AtCoder [9]. Figure 1 shows the ﬁrst distribution shift from the task difference. The
two simple programs were submitted to two contests targeting different tasks. The ID program is to
print Tak’s total accommodation fee, and the OOD one is to print the number of integers between
a and b. Accordingly, if a model is trained on submissions of Contest 044 (Task A) and never sees
the submissions of Contest 048, it is quite challenging in recognizing the unseen task. Such a task
difference will produce the data distribution shift. Figure 2 presents an example where the distribution
shift is from the programmer change. Concerning the programming habits, different programmers
might solve the same task with different code styles. For instance, suppose the ID program has
the if-else statement in one line, while the OOD uses the standard format where the conditions and
actions are separated by new lines. Thus, the orders of tokens from these two programs are different
which could introduce distribution shifts.

3.2 Dataset construction

CodeS contains three collections of data, Python75, Java250-S and Python800-S. Here, we introduce
the distribution shift deﬁnitions and how we build each set accordingly.

Distribution shift deﬁnition. We design ﬁve types of distribution shifts for source code from two
perspectives: natural distribution shift and representation-based distribution shift. Natural distribution
shift comes from the common code style changes that the model could face every day, which includes
the change of task, time period, and programmer. We utilize the submission tags (time, task, and user)
as shown in Figure 3 (Appendix B) to divide data into ID and OOD sets. On the other hand, since
source code is often transformed to different representations, such as the linear sequence of tokens
and concrete syntax tree (CST) [13, 14], we also consider distribution shifts from the representation

4

a, b, c = map(int, input().split())if a % c == 0:print((b-a)//c+1)else:print((b-(a//c+1)*c)//c+1)N, K, X, Y = [int(input()) for iin range(4)]fee = 0for iin range(1, N+1):if (i<= K):fee += Xelse:fee += Yprint(fee)Contest: 044, Task A –Takand Hotels(ABC Edit)Contest: 048, Task B –Between a and bIDprogramOODprogramDatatype:IDTask:Contest-004-TaskA:Takand Hotels(ABC Edit)Program:a, b, c = map(int, input().split())if a % c == 0:print((b-a)//c+1)else:print((b-(a//c+1)*c)//c+1)Datatype:OODTask:Contest-048-Task B:Between a and bProgram:N, K, X, Y = [int(input()) for iin range(4)]fee = 0for iin range(1, N+1):if (i<= K):fee += Xelse:fee += Yprint(fee)a, b, c = map(int, input().split())if a % c == 0:print((b-a)//c+1)else:print((b-(a//c+1)*c)//c+1)N, K, X, Y = [int(input()) for iin range(4)]fee = 0for iin range(1, N+1):if (i<= K):fee += Xelse:fee += Yprint(fee)Contest: 044, Task A –Takand Hotels(ABC Edit)Contest: 048, Task B –Between a and bIDprogramOODprogramDatatype:IDTask:Contest-004-TaskA:Takand Hotels(ABC Edit)Program:a, b, c = map(int, input().split())if a % c == 0:print((b-a)//c+1)else:print((b-(a//c+1)*c)//c+1)Datatype:OODTask:Contest-048-Task B:Between a and bProgram:N, K, X, Y = [int(input()) for iin range(4)]fee = 0for iin range(1, N+1):if (i<= K):fee += Xelse:fee += Yprint(fee)Datatype:IDProgrammer:penicillin0Program:a, b, c = map(int, input().split())print('YES') if b-a == c-b else print('NO')Datatype:OODProgrammer :juppyProgram:a, b, c = map(int, input().split())if b-a == c-b:print('YES')else:print('NO')aspect. Compared to the natural distribution shift, the representation-based shift is more ﬁne-grained
and can reﬂect the implicit difference in data features.

• Task distribution shift – ID data and OOD data target different tasks.
• Programmer distribution shift – ID data and OOD data target the same task but come from different
programmers. As mentioned in Section 3.1, due to the programming habit, the distribution shift
can happen across different programmers.

• Time distribution shift – ID code and OOD code target the same task but are written in different
time periods. Assessing [15] and maintaining [16] the model performance over time is critical to
ensure the reliability and security of models. This is because the code can be improved or new
users appear over time.

• Token distribution shift – ID code and OOD code target the same task but the frequencies of tokens
that appear to them are different. In source code learning, transforming code into numeral vectors
is fundamental to make it executable for deep learning models [17]. A linear sequence of tokens is
the typical and most important code representation, which is usually processed via tokenization,
or lexical analysis. A token is the basic unit of the representation and can be a function name, an
operator, or a punctuation sign. In practice, each token is represented by an integer to be compatible
with models. Given token sequences of two source code ﬁles, the straightforward difference is the
appearance of tokens.

• CST distribution shift – ID code and OOD code target the same task but have different CST
representations. The concrete syntax tree (also known as the parse tree or derivation tree) is another
popular code representation, which includes the syntactic structure of code ﬁles. The difference
between code ﬁles can be represented by the distance between CSTs.

Raw data preparation. The raw data of Python75 is collected from the AtCoder site [9], where
contests are continuously announced. Hence, there are many new submissions and new programmers
in the contests. We selected the submissions from 75 tasks in 25 contests based on the following
conditions: 1) the language should be Python3, and 2) the tasks have at least 1,000 submissions
and 3) the status of the submission is accepted (AC). In total, we collected 200,462 programs for
Python75. The detailed information of Python75 can be found in Appendix B. The raw data of
Java250-S and Python800-S are from Java250 and Python800 provided by the Project CodeNet [13].
In total, Java250-S and Python800-S contain 75000 and 240000 programs.

Shift dataset creation. For each type of distribution shift, we create a dataset consisting of the
training set, ID test set, and OOD test set extracted from the raw data. The training and ID test sets
share the same data distribution and both contribute to training a model. The OOD test set is used to
evaluate the generalization ability of the trained model. Table 2 shows the details of each dataset.

Table 2: Data information (number of classes, data size in each class) of each distribution shift
dataset.

Data collection

Python75
Java250-S
Python800-S

Programmer, Time, Token, CST

#Classes
75
250
800

#Training
732
180
180

#ID test
134
60
60

#OOD test
134
60
60

#ID Classes
65
200
640

#OOD classes
10
50
160

Task
#Training
846
225
225

#ID test
154
75
75

#OOD test
1000
300
300

• Task distribution shift – We ﬁrstly randomly divide all tasks into the given number (#ID Classes
and #OOD Classes in Table 2) of ID and OOD tasks. Next, in each ID task, we randomly select the
number of #Training and #ID test code ﬁles as the training and ID test data, respectively. Finally,
in each OOD task, a number of #OOD test code ﬁles are randomly selected as the OOD test data.
• Programmer distribution shift - We follow the strategy used by Li et al. [8] to prepare this type of
data. Speciﬁcally, for each task, we ﬁrst randomly select speciﬁc programmers and consider their
submissions as the OOD test data, the submissions from other users (at least two submissions) are
added into training and ID test sets.

• Time distribution shift - For each task, we sort the source code ﬁles according to the submission
time and take the newest ﬁles as OOD test data2. The earlier code ﬁles are randomly split into the
training and ID test sets, respectively.

2The time tag of Java250 and Python800 is unavailable, hence we do not create datasets with the time shift.

5

• Token distribution shift - For each task, we ﬁrst build a density distribution of each token sequence
based on the histogram (the bin number is equal to the total number of token types) [18]. Then we
discriminate the tokens that are only used by some sequences but never used by the others, based
on which the code ﬁles are divided into ID and OOD sets. The ID set is further randomly split into
the training and ID test sets. In this paper, we use the publicly available tokenizer tool provided by
Puri et al. [13] to generate the token representations. Examples can be found in Appendix C.

• CST distribution shift - For each task, we calculate the average distance between each ﬁle and
the others by the Robinson-Foulds distance between two CSTs. The ones with greater distances
are grouped into the OOD set, and the ones with smaller distances are grouped into the training
and ID test sets. We use the parse tree generator provided by Puri et al. [13] to obtain the CST
presentations. Examples can be found in Appendix C.

4 Experimental setup

We ﬁrst investigate the performance of different OOD detectors. Then we explore the shift degree
by different shift types based on OOD detectors and the model performance. Finally, we analyze
the generalization ability of modes with different code representations on classiﬁcation tasks when
dealing with distribution shifts.

4.1 DNNs

We consider 5 DNNs with different code representations. CNN (Sequence) is a model with both max
and average global pooling operations (doublePoolClassDNN). MLP (Bag) is a DNN with dense
layers (denseDNN). Both CNN (Sequence) and MLP (Bag) are provided by the Project CodeNet [13].
Besides, we apply 3 Pre-trained models (RoBERTa, CodeBERT, and GraphCodeBERT) provided by
the CodeXGLUE project [19] in our experiments. Each pre-trained model is ﬁne-tuned using the
training set. More detailed model information can be found in Appendix A.

4.2 Baseline

To investigate the effectiveness of different distribution shifts, we introduce the random manner to
split data as a baseline. Namely, we randomly select a given number of samples for the training, ID
test, and OOD test sets, respectively. The data size is the same as the programmer distribution shift as
shown in Table 2. By default, this type of dataset has no distribution shift.

4.3 OOD detector

Four widely used OOD detectors are included, Maximum Softmax Probability (MSP) [20], Out-
of-Distribution detector for neural networks (ODIN) [21], Mahalanobis [22], and Outlier Exposure
(OE) [23]. The ﬁrst three are simply softmax score-based given a pre-trained model, and the last one
requires training a new neural network. A more detailed introduction can be found in Appendix D.

4.4 Evaluation measures

Accuracy is a basic performance measure of a given model, which measures the percentage of data
being correctly classiﬁed. Given a pre-trained model, we measure its accuracy on the ID and OOD
test sets, respectively. In general, a model will have greater accuracy on ID test data than on the OOD
since its training data has the same distribution as the ID test set [6].

AUROC is short for the area under the receiver operating characteristics. This metric gives insights
into how different the ID and OOD test sets are as well as the model’s ability to distinguish between
ID and OOD data. An ideal OOD detector has the highest AUROC score of 100%.

6

5 Results Analysis

5.1 OOD detector analysis

Since the data with task distribution shift is real OOD data (these data are not in the classiﬁcation
target of the model), we can use ID/OOD data split based on task distribution shift and random split to
evaluate the performance of OOD detectors. A high AUROC score obtained by the detector indicates
that its discrimination ability is strong.

Table 3 shows the results. Concerning the random case, MSP, Mahalanobis, and OE detectors all
have a score of around 50 regardless of the dataset. However, the ODIN detector achieves a much
smaller (compared to 50) score (37.32) on Python800-S. Concerning the task distribution shift, MSP,
Mahalanobis, and OE detectors perform better than the ODIN detector by giving higher scores.
Particularly, ODIN outputs the lowest scores (less than 14) for Python800-S. The ﬁrst conclusion
we draw is that the ODIN detector performs the worst of all, which can not be used for our source
code data. Furthermore, we found that the simplest MSP detector produces the best result where the
difference between random and task distribution shift is larger than by the well-designed Mahalanobis
and OE. For example, in Python800-S, MSP achieves 78.88 for the task distribution shift, while
Mahalanobis and OE cannot well distinguish between ID and OOD sets by obtaining the AUROC
score close to 50. These results show that it’s still challenging for source code OOD detection. Thus,
the ﬁrst application of CodeS is: researchers can use our dataset to design better OOD detectors for
source code learning.

Table 3: AUROC scores by different OOD detectors. The ODIN and Mahalanobis detectors have a
parameter of perturbation magnitude ((cid:15)). Average means the average score of all parameter settings.

OOD detector

(cid:15)

MSP

0
0.0005
0.001
0.0014
0.002
0.0024
0.005
0.01
0.05
0.1
0.2
Average
0
0.01
0.005
0.002
0.0014
0.001
0.0005
Average

ODIN

Mahalanobis

OE

Python75
Random Task
91.33
61.40
61.41
61.42
61.42
61.44
61.45
61.50
61.59
62.31
63.10
64.71
61.98
71.31
75.82
69.20
76.16
70.54
78.28
72.04
73.34
61.11

50.16
62.82
62.83
62.83
62.83
62.84
62.84
62.86
62.90
63.50
64.55
67.20
63.46
51.78
47.83
49.77
50.04
49.07
47.32
48.12
49.13
49.64

Java250-S
Random Task
80.99
44.86
44.85
44.85
44.85
44.84
44.84
44.81
44.76
44.30
43.61
41.87
44.40
72.36
73.03
71.15
71.45
70.78
71.80
71.37
71.71
52.67

50.06
49.69
49.69
49.69
49.69
49.69
49.69
49.70
49.71
49.85
50.00
50.27
49.79
49.58
51.88
50.77
48.27
52.25
49.95
49.27
50.28
48.12

Python800-S
Random Task
78.88
13.12
13.13
13.13
13.13
13.14
13.14
13.16
13.19
13.43
13.63
13.74
13.27
51.21
49.26
54.79
51.82
53.14
51.82
48.23
51.47
50.45

49.67
37.64
37.63
37.63
37.63
37.62
37.62
37.59
37.55
37.20
36.73
35.73
37.32
49.55
51.85
53.23
47.22
48.55
47.41
52.63
50.06
50.00

5.2 Shift degree

To explore the reasonability of our distribution shift deﬁnitions, we check the shift degree caused
by each deﬁnition. Generally, task distribution shift should be the upper bound since it produces
real OOD data with new classes. Table 4 shows the AUROC score and model accuracy on different
distribution shifts. Concerning the model accuracy, regardless of the dataset, all types of distribution
shifts (except for random splitting) cause performance degradation. This is consistent with the general
ﬁnding that the distribution shift causes a performance drop [6].

7

Table 4: AUROC score and accuracy on different distribution shift. DNN: CNN (sequence). The
accuracy improvement ↑ and degradation ↓ on the OOD test set are also listed. Average: average
score of 4 OOD detectors. ODIN: (cid:15) = 0.0014, T = 1000, Mahalanobis: (cid:15) = 0.0014.

Shift type

AUROC
MSP ODIN Mahalanobis

OE

Average

ID test

OOD test

Accuracy (%)

Random
Task
Programmer
Time
Token
CST

Random
Task
Programmer
Token
CST

Random
Task
Programmer
Token
CST

50.16
91.33
48.20
57.90
82.82
77.89

50.06
80.99
53.72
68.65
60.32

49.67
78.88
58.10
64.47
49.74

62.83
61.42
83.23
76.80
71.16
70.43

49.69
44.85
49.36
52.28
41.85

37.63
13.13
46.34
39.24
48.02

Python75

49.07
70.54
50.21
50.00
56.70
57.84

49.64
61.11
48.33
50.22
52.61
57.84

Java250-S

52.25
70.78
46.33
49.47
50.98

48.55
53.14
54.30
50.52
48.26

48.12
52.67
49.22
52.63
51.53
Python800-S
50.00
50.45
62.45
50.00
50.00

52.93
71.10
57.49
58.73
65.82
66.00

50.03
62.32
49.66
55.76
51.17

46.46
48.90
55.30
51.06
49.01

96.91
96.95
96.53
97.51
97.50
96.98

84.23
87.89
85.86
89.01
86.69

77.91
82.50
77.22
79.27
77.94

97.01 (0.10 ↑)
-
96.47 (0.06 ↓)
92.64 (4.87 ↓)
61.04 (36.46 ↓)
71.17 (25.81 ↓)

84.43 (0.20 ↑)
-
82.84 (3.02 ↓)
65.85 (23.16 ↓)
75.91 (10.78 ↓)

78.40 (0.49 ↑)
-
66.94 (10.28 ↓)
53.83 (25.44 ↓)
77.82 (0.12 ↓)

Table 5: Average AUROC score and accuracy change over Python75, Java250-S, and Python800-S of
different shift types. ↑ and ↓ indicate an accuracy improvement and degradation on the OOD test set,
respectively.

Shift type

Random
Task
Programmer
Time
Token
CST

AUROC
MSP ODIN Mahalanobis
49.96
83.73
53.34
57.90
71.98
62.65

50.05
39.80
59.64
76.80
54.23
53.43

49.96
64.82
50.28
50.00
52.23
52.36

OE
49.25
54.74
53.33
50.22
51.75
53.12

Average
49.81
60.77
54.15
58.73
57.55
55.39

Accuracy change (%)

0.26 (↑)
-
-4.45 (↓)
-4.87 (↓)
-28.35 (↓)
-12.24 (↓)

On the other hand, concerning the AUROC scores, ﬁrst, we can see that only MSP can produce scores
with the same trend as the accuracy degradation. That means the greater the MSP score, the more
drastic the accuracy decline. This ﬁnding future demonstrates that MSP is the best OOD detector
among our considered ones. Then, we check the score of each deﬁnition. The results show that
compared with the real OOD task distribution shift, all the other deﬁnitions have smaller scores,
except the score produced by ODIN. This is reasonable since the model has some learned information
on these data. Additionally, Table 5 shows the average AUROC score and accuracy change over
Python75, Java250-S, and Python800-S. On average, the code representation-based (Token and
CST) distribution shifts cause greater performance degradation than the natural distribution shifts
(Programmer and Time) except for the task-based. More speciﬁcally, the token distribution shift
affects the model accuracy the most. The average AUROC score also indicates the representation-
based shifts are stronger than the natural ones. In conclusion, these results demonstrate that, in
CodeS, the natural distribution shifts only introduce negligible data bias, while the new proposed
representation-based distribution shifts are more challenging.

5.3 Model generalization analysis

Finally, we utilize CodeS to test the generalization ability of different models. Table 6 shows the
accuracy of ﬁve models on the ID and OOD test sets. Regardless of the dataset and model, the
accuracy of the randomly split ID and OOD test sets are similar (the greatest difference is 1.03% in
Java250, MLP (Bag)). By comparing each model, we observe that the three pre-trained language
models (RoBERTa, CodeBERT, and GraphCodeBERT) are more robust against the distribution shift

8

than CNN (Sequence) and MLP (Bag). For example, in Python800-S with token distribution shift,
all the pre-trained models can achieve more than 88% accuracy on the OOD test set while CNN
(Sequence) and MLP (Bag) only achieve by up to 53.43% accuracy. This indicates that pretraining
with diverse programming languages is helpful for producing models with better generalization
ability on distribution shifted datasets. For instance, CodeBERT is pre-trained on natural language-
programming language pairs in 6 programming languages (Python, Java, JavaScript, PHP, Ruby, and
Go). In addition, by the accuracy drop on the OOD test sets, we can draw the same conclusion as
the shift degree study – representation-based distribution shifts are more challenging. The second
usage of CodeS is: CodeS opens the challenge for future improving the generalization of source code
learning models.

Table 6: Model accuracy (%) on ID and OOD test sets given different DNNs and distribution shifts.
The accuracy improvement ↑ and degradation ↓ on the OOD test set are also listed.

Model

Random

Programmer

ID test

OOD test

ID test

OOD test

ID test
Python75
97.51
92.35
97.92
98.06
98.11
Java250-S
-
-
-
-
-
Python800-S
-
-
-
-
-

Time

Token

CST

OOD test

ID test

OOD test

ID test

OOD test

92.64 (4.87 ↓)
88.13 (4.22 ↓)
99.28 (1.36 ↑)
99.44 (1.38 ↑)
99.52 (1.41 ↑)

-
-
-
-
-

-
-
-
-
-

97.50
94.13
98.28
98.51
98.50

89.01
76.31
96.55
96.92
97.29

79.27
71.37
97.04
97.44
96.19

61.04 (36.46 ↓)
45.28 (48.85 ↓)
95.02 (3.26 ↓)
96.92 (1.59 ↓)
96.37 (2.13 ↓)

65.85 (23.16 ↓)
35.32 (40.99 ↓)
87.43 (9.12 ↓)
89.27 (7.65 ↓)
89.89 (7.4 ↓)

53.83 (25.44 ↓)
34.47 (36.90 ↓)
88.51 (8.53 ↓)
89.91 (7.53 ↓)
97.77 (1.58 ↑)

96.98
93.53
99.47
99.52
99.52

86.69
76.03
96.79
97.41
97.82

77.94
66.39
95.72
96.19
96.42

71.17 (25.81 ↓)
63.45 (30.08 ↓)
86.65 (12.82 ↓)
87.04 (12.48 ↓)
87.17 (12.35 ↓)

75.91 (10.78 ↓)
50.86 (25.17 ↓)
87.47 (9.32 ↓)
88.63 (8.78 ↓)
89.31 (8.51 ↓)

77.82 (0.12 ↓)
67.77 (1.38 ↑)
97.31 (1.59 ↑)
97.77 (1.58 ↑)
97.98 (1.56 ↑)

96.47 (0.06 ↓)
91.80 (1.13 ↓)
98.46 (0.11 ↑)
98.53 (0.23 ↑)
98.63 (0.23 ↑)

82.84 (3.02 ↓)
71.59 (0.81 ↓)
95.04 (0.49 ↓)
96.25 (0.04 ↓)
96.51 (0.05 ↓)

66.94 (10.29 ↓)
63.39 (3.90 ↓)
96.15 (0.42 ↑)
96.99 (0.80 ↑)
97.24 (0.98 ↑)

CNN (Sequence)
MLP (Bag)
RoBERTa
CodeBERT
GraphCodeBERT

CNN (Sequence)
MLP (Bag)
RoBERTa
CodeBERT
GraphCodeBERT

CNN (Sequence)
MLP (Bag)
RoBERTa
CodeBERT
GraphCodeBERT

96.91
92.22
98.31
98.37
98.49

84.23
70.58
94.87
95.67
96.02

77.91
66.90
96.09
96.48
96.69

97.01 (0.1 ↑)
92.07 (0.15 ↓)
98.13 (0.18 ↓)
98.29 (0.08 ↓)
98.34 (0.15 ↓)

84.43 (0.2 ↑)
71.61 (1.03 ↑)
95.26 (0.39 ↑)
96.10 (0.43 ↑)
96.34 (0.32 ↑)

78.40 (0.49 ↑)
67.00 (0.10 ↑)
96.38 (0.29 ↑)
96.74 (0.26 ↑)
96.89 (0.20 ↑)

96.53
92.93
98.35
98.30
98.40

85.86
72.40
95.53
96.29
96.56

77.23
67.29
95.73
96.19
96.26

6 Limitations

The effect of each distribution shift is tested on 3 datasets, which is the main limitation. Especially,
Java250-S and Python800-S are built on a smaller size of raw data than Python75. However, we
believe the conclusions will remain the same when using larger datasets. For example, Table 3 shows
that the shift degree by the task distribution shift on Java250-S and Python800-S is smaller than on
Python75. If the raw data size of Java250-S and Python800-S is larger, the difference in data features
(e.g., programmers, code representations) will increases. Accordingly, the shift effect will be greater.

7 Conclusion

This paper proposed CodeS, the ﬁrst distribution shift benchmark dataset that supports two pro-
gramming languages and both natural and representation-based distribution shifts. We evaluated
the effectiveness of existing OOD detectors, the shift degree introduced by different shift types, and
the generalization ability of famous code classiﬁcation models based on CodeS. We found that the
well-designed OOD detectors cannot be generalized to code data and representation-based distribution
shifts are more challenging than natural distribution shifts. CodeS provides the stage for future study
of source code learning, including OOD detection, model robustness enhancement, and more.

References

[1] Github, 2008. URL https://github.com//. Online; accessed 19 May 2022.

[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of
machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):81,
2018.

[3] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from
structured representations of code. In International Conference on Learning Representations,
2018.

9

[4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed
representations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1–29,
2019.

[5] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree
structures for programming language processing. In Thirtieth AAAI conference on artiﬁcial
intelligence, 2016.

[6] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure
Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
Wilds: a benchmark of in-the-wild distribution shifts.
In Marina Meila and Tong Zhang,
editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 5637–5664. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/koh21a.html.

[7] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. Proceedings of the International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.

[8] Yufei Li, Simin Chen, and Wei Yang. Estimating predictive uncertainty under program data

distribution shift. arXiv preprint arXiv:2107.10989, 2021.

[9] Atcoder, 2012. URL https://atcoder.jp/. Online; accessed 19 May 2022.

[10] Pengyu Nie, Jiyang Zhang, Junyi Jessy Li, Raymond J Mooney, and Milos Gligoric. Impact of

evaluation methodologies on code summarization. ACL, page (To Appear), 2022.

[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning
benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664,
2021.

[12] Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, et al. Codenet: A large-scale ai for
code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655, 2021.

[13] Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir
Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca
Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss.
Codenet: a large-scale ai for code dataset for learning a diversity of coding tasks, 2021.

[14] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code
recommendation via structural code search. Proc. ACM Program. Lang., 3(OOPSLA), oct 2019.
doi: 10.1145/3360578. URL https://doi-org.proxy.bnl.lu/10.1145/3360578.

[15] Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian Lü. Boosting operational
In Proceedings of the 2019 27th ACM Joint
dnn testing efﬁciency through conditioning.
Meeting on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, ESEC/FSE 2019, page 499–509, New York, NY, USA, 2019. Association
for Computing Machinery. ISBN 9781450355728. doi: 10.1145/3338906.3338930. URL
https://doi-org.proxy.bnl.lu/10.1145/3338906.3338930.

[16] Qiang Hu, Yuejun Guo, Maxime Cordy, Xiaofei Xie, Lei Ma, Mike Papadakis, and Yves
Le Traon. An empirical study on data distribution-aware test selection for deep learning
enhancement. ACM Transactions on Software Engineering and Methodology, January 2022.
ISSN 1049-331X. doi: 10.1145/3511598. URL https://doi-org.proxy.bnl.lu/10.
1145/3511598.

[17] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and Federica Sarro. A

survey on machine learning techniques for source code analysis, 2021.

10

[18] Bernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.

[19] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: a machine learning benchmark dataset for code
understanding and generation. CoRR, abs/2102.04664, 2021.

[20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?id=Hkg4TI9xl.

[21] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=H1VGkIxRZ.

[22] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, NIPS’18, page 7167–7177, Red Hook,
NY, USA, 2018. Curran Associates Inc.

[23] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with
In International Conference on Learning Representations, 2019. URL

outlier exposure.
https://openreview.net/forum?id=HyxCxhRcY7.

Organization of the supplementary material.
Section A presents the details of our artifacts.
Section B introduces the information of the raw data of Python75. In Section C, we give examples of
the data splitting based Token and CST-based distribution shift. Section D introduces the deﬁnitions
and parameters of our used OOD detectors. Section E and Section F present our experiment details
(computing resources) and additional results.

A Artifacts

All our artifacts (datasets, models, and OOD detectors) are released under an MIT license and
archived on GitHub: https://github.com/testing-cs/CodeS.git

Datasets & Docs The online resource includes our crawled raw data from AtCoder, the original
datasets of Java250 and Python800 downloaded from the Project CodeNet [13] and data descriptions.
We also release the corresponding datasets (source code ﬁles and tokens) with distribution shifts of
Python75, Java250, and Python800. The token representations are generated using the tokenizer tool3
provided by Puri et al. [13].

Model training/ﬁne-tuning CNN (Sequence) and MLP (Bag) are trained using the implementa-
tion4 by Project CodeNet with the same parameters (e.g., random seed, dropout rate). The training
set and ID test are involved in the training procedure for training and validation, respectively. The
epoch number is set to 100. All the trained CNNs are released on GitHub. Pre-trained models are
ﬁne-tuned using the implementation5 with the same parameters.

OOD detectors & Code We modify the original implementations of the ODIN6 and Mahalanobis7,
detectors to ﬁt for the TensorFlow framework. The OE detectors have the same DNN architectures
and we take the loss function from the original implementation8. The implementations of MSP,
ODIN, Mahalanobis, and all the OE detectors are released on GitHub.

3https://github.com/IBM/Project_CodeNet/tree/main/tools/tokenizer
4https://github.com/IBM/Project_CodeNet
5https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection
6https://github.com/facebookresearch/odin
7https://github.com/pokaxpoka/deep_Mahalanobis_detector
8https://github.com/hendrycks/outlier-exposure

11

Figure 3: Screenshot of the submission results at AtCoder. The considered tags when crawling data
are highlighted with a blue frame. The red ones are for the distribution shifts.

B Raw data from AtCoder

Figure 3 shows a screenshot of the submissions to AtCoder Beginner Contest 044. The task distribu-
tion shift, time distribution shift and programmer distribution shift are prepared based on the tags
"Task", "Submission Time", and "User". Table 7 shows the details of crawled data (the raw data of
Python75) from AtCoder.

Table 7: Raw data crawled from AtCoder beginner contests.

3149, 3354

Data sizes
2579, 2683

Contest ID Tasks
A, B
044
A, B, C 3130, 2740, 2104
045
A, B
046
A, B, C 3026, 2738, 2130
047
A, B, C 3288, 3235, 1716
048
A, B, C 3119, 2727, 3716
049
A, B, C 2991, 2768, 1270
050
A, B, C 3711, 5071, 2195
051
A, B, C 2920, 2908, 1395
052
A, B, C 2877, 3426, 1370
053
A, B, C 2811, 2389, 2899
054
A, B, C 3092, 3837, 1446
055
A, B, C 2795, 2167, 2056
056

Contest ID Tasks
057
058
059
060
061
062
063
064
065
066
067
068
In total

Data sizes
3207, 2691, 3543
2708, 2981, 2002
2722, 2900, 1304
2865, 3358, 1557

A, B, C
A, B, C
A, B, C
A, B, C
A, B, C, D 2874, 2992, 2444, 1043
2535, 2547
A, B
A, B, C
2825, 2983, 2277
A, B, C, D 3781, 2546, 2679, 1580
A, B, C, D 2472, 3438, 2016, 1017
A, B, C
A, B, C
A, B, C
75

2806, 2944, 2287
2516, 2950, 2079
2651, 4076, 2438
200462

C Examples of Token and CST splitting

Here, in addition to Section 3.2 in the main paper, we present more splitting information about the
token and CST distribution shifts. Figure 4 shows examples of the splitting by the frequency of tokens
based on the density distribution. By comparison, the ID and OOD codes have very different token
distributions. Figures 5 - 8 show the CST representations of 4 submissions. The distance between the
ﬁrst two and last two are 121 and 90, respectively.

D OOD detector

Here, we introduce the deﬁnitions of our used OOD detectors in the experiments. Let f be a
N -class classiﬁer and (x, y) be an sample with its true label y. Given x, f predicts a label ˆy =
{Si (x) , 1 ≤ i ≤ N } which indicates that x is most likely to belong to the ˆy class. Here,
arg max
i

Si (x) is the softmax output computed by:

Si (x) =

expfi(x)
N
(cid:80)
j=1

expfj (x)

12

(1)

(a) abc044-A

(b) abc044-B

(c) abc045-A

(d) abc045-B

(e) p02263

(f) p02264

(g) p02381

(h) p02388

(i) p02546

(j) p02548

(k) p02570

(l) p02577

Figure 4: Density distribution of ID and OOD code tokens. Each subﬁgure corresponds to token
representations of two code ﬁles in a given task (caption of the subﬁgure). x−axis: ID code tokens.
y−axis: OOD code tokens. First row: Python75. Second row: Java250-S. Last row: Python800-S.

fi (x) is the raw prediction (also named as logits in the literature) of x belonging to the ith class
predicted by f . In [21], Sˆy = max
{Si (x) , 1 ≤ i ≤ N } is also named as the maximum softmax
probability and softmax score. {Si (x) , 1 ≤ i ≤ N } is the softmax distribution of x generated by f .

i

- MSP The Maximum Softmax Probability is a baseline method proposed by Hendrycks and
Gimpel [20] to detect wrongly classiﬁed and OOD samples. The intuition behind MSP is that a model
tends to be rather conﬁdent (have a high softmax score) on correctly classiﬁed data. In concrete,
given the softmax distribution of a model’s prediction on a test sample, MSP identiﬁes the data as ID
if the softmax score is greater than a threshold and vice versa.

- ODIN Similar to MSP, the Out-of-DIstribution detector for Neural networks proposed by Liang
et al. [21] also takes advantage of the softmax distribution. It points out that by adding slight
perturbations to samples, the gap of softmax score between wrongly and correctly classiﬁed samples
becomes larger. Compared to MSP that directly uses the original data to obtain the softmax score,
ODIN ﬁrst preprocesses the data by adding a perturbation (cid:15) and then scales the softmax output
(Equation (1)) by:

Si (x) =

expfi(x)/T
N
(cid:80)
j=1

expfj (x)/T

13

(2)

010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070010203040506070.
1

n
o
i
s
s
i

m
b
u
s

f
o

n
o
i
t
a
t
n
e
s
e
r
p
e
r
T
S
C

:
5

e
r
u
g
i

F

14

.
2

n
o
i
s
s
i

m
b
u
s

f
o

n
o
i
t
a
t
n
e
s
e
r
p
e
r
T
S
C

:
6

e
r
u
g
i

F

15

.
3

n
o
i
s
s
i

m
b
u
s

f
o

n
o
i
t
a
t
n
e
s
e
r
p
e
r
T
S
C

:
7

e
r
u
g
i

F

16

.
4

n
o
i
s
s
i

m
b
u
s

f
o

n
o
i
t
a
t
n
e
s
e
r
p
e
r
T
S
C

:
8

e
r
u
g
i

F

17

where T is the temperature scaling parameter. Following the original implementation9, we use differ-
ent perturbation magnitudes ((cid:15) = 0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2)
and set T = 1000.

- Mahalanobis Lee et al. [22] proposed the Mahalanobis detector that learns a Gaussian dis-
tribution for each class. Then the detector calculates a conﬁdence score for a test sample by
measuring the Mahalannobis distance between the sample and the closest class-conditional Gaus-
sian distribution. The test data is ID if its conﬁdence score is greater than a threshold and
vice versa. Similar to ODIN, this detector also preprocesses the test samples with a pertur-
bation (cid:15). Following the original implementation10, we use different perturbation magnitudes
((cid:15) = 0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2).

- OE Different from the above three detectors, Hendrycks et al. [23] proposed to train a neural net-
work named Outlier Exposure to detect OOD data by using available ID and OOD data. Remarkably,
the OOD data for training is not necessarily to include the distribution of test OOD data, which makes
the OE detector more practical to generalize to unseen distributions.

E Experiment details

Computing infrastructure All CNN (Sequence) and MLP (Bag) relevant experiments were con-
ducted on a 3.00 GHz Intel Xeon Gold 5217 CPU with two RTX 8000 GPUs. The implementation
was based on the Tensorﬂow 2.5.1 framework. All pre-trained language model-based experiments
were performed on a high-performance computer cluster consisting of Dell C4140, 6 GPU nodes x 4
Nvidia V100 SXM2 32GB. We implemented the related experiments based on the PyTorch 1.6.0
framework.

F Additional results

In addition to Table 4 in the main paper, we present Table 8 and Table 9 to show the AUROC scores
of ODIN and Mahalanobis, respectively, with different parameter settings. Recall that ODIN includes
the perturbation magnitude (cid:15) and temperature T . Mahalanobis only includes (cid:15).

Table 8 lists the AUROC scores measured by ODIN with different perturbation magnitudes. We also
conducted experiments using different temperatures T = 1, 10, 100, 1000 and got the same results,
hence, the results are not listed here. In general, changing the perturbation magnitude gives the
same conclusion compared to Table 4 in the main paper. Additionally, using a different magnitude
may result in a very different AUROC score. For example, in Python75, the scores on the token
distribution shift are 77.39 and 71.12, respectively, when using (cid:15) = 0.2 and (cid:15) = 71.12.

Table 9 lists the AUROC scores measured by the Mahalanobis detector with different perturbation
magnitudes. Compared to Table 8, the result varies more than the ODIN detector and the standard
deviation ranges from 0.77 to 4.35. For example, in Python75, the AUROC scores are 78.28 and
69.20 when using (cid:15) = 0.001 and (cid:15) = 0.005, respectively. However, in most cases, we can still draw
the conclusion that the task difference introduces the greatest distribution shift to the dataset.

9https://github.com/facebookresearch/odin
10https://github.com/pokaxpoka/deep_Mahalanobis_detector

18

Table 8: AUROC results by the ODIN detector with different perturbation magnitudes ((cid:15)). The
highlighted column (cid:15) = 0.0014 is the same as Table 4 in the main paper. T = 1000. Min: minimum.
Max: maximum. Std: standard deviation.

Shift type

0

0.0005

0.001

0.0014

Magnitude
0.0024

0.002

Random
Task
User
Time
Token
CST

Random
Task
User
Token
CST

Random
Task
User
Token
CST

62.82
61.40
83.23
76.81
71.12
70.47

49.69
44.86
49.38
52.32
41.90

37.64
13.12
46.34
39.23
48.04

62.83
61.41
83.23
76.81
71.13
70.45

49.69
44.85
49.38
52.30
41.88

37.63
13.13
46.34
39.23
48.04

62.83
61.42
83.23
76.80
71.15
70.44

49.69
44.85
49.37
52.29
41.86

37.63
13.13
46.34
39.24
48.03

62.83
61.42
83.23
76.80
71.16
70.43

49.69
44.85
49.36
52.28
41.85

37.63
13.13
46.34
39.24
48.02

62.84
61.44
83.22
76.80
71.18
70.40

49.69
44.84
49.36
52.26
41.83

37.62
13.14
46.34
39.24
48.01

62.84
61.45
83.22
76.80
71.19
70.39

49.69
44.84
49.35
52.25
41.81

37.62
13.14
46.34
39.24
48.00

0.01

0.005
Python75
62.86
61.50
83.22
76.78
71.27
70.30
Java250
49.70
44.81
49.32
52.19
41.72
Python800
37.59
13.16
46.34
39.25
47.95

62.90
61.59
83.20
76.75
71.40
70.20

49.71
44.76
49.25
52.06
41.54

37.55
13.19
46.34
39.27
47.86

0.05

0.1

0.2

Min Max Average

Std

Statistics

63.50
62.31
83.08
76.52
72.57
69.12

49.85
44.30
48.71
51.06
40.11

37.20
13.43
46.31
39.35
47.16

64.55
63.10
83.03
76.63
74.11
69.01

50.00
43.61
48.12
49.92
38.38

36.73
13.63
46.22
39.30
46.38

67.20
64.71
83.72
79.69
77.39
71.12

50.27
41.87
47.17
47.91
35.33

35.73
13.74
45.92
38.74
45.12

62.82
61.40
83.03
76.52
71.12
69.01

49.69
41.87
47.17
47.91
35.33

35.73
13.12
45.92
38.74
45.12

67.20
64.71
83.72
79.69
77.39
71.12

50.27
44.86
49.38
52.32
41.90

37.64
13.74
46.34
39.35
48.04

63.46
61.98
83.24
77.02
72.15
70.21

49.79
44.40
48.98
51.53
40.75

37.32
13.27
46.29
39.21
47.51

1.35
1.05
0.17
0.89
1.97
0.61

0.19
0.92
0.72
1.42
2.11

0.60
0.23
0.13
0.16
0.95

Table 9: AUROC results by the Mahalanobis detector with different perturbation magnitudes ((cid:15)). The
highlighted column (cid:15) = 0.0014 is the same as Table 4. DNN: CNN (sequence). Min: minimum.
Max: maximum. Std: standard deviation.

Shift type

0

0.01

0.005

0.002

0.0014

0.001

0.0005 Min Max Average

Std

Magnitude

Statistics

Random
Task
User
Time
Token
CST

Random
Task
User
Token
CST

Random
Task
User
Token
CST

51.78
71.31
54.31
54.88
59.91
58.28

49.58
72.36
51.65
48.40
46.88

49.55
51.21
50.09
47.04
49.91

47.83
75.82
50.21
53.18
59.63
59.32

51.88
73.03
50.40
52.66
51.03

51.85
49.26
53.81
55.76
50.12

49.77
69.20
48.92
52.83
55.89
63.24

50.77
71.15
51.06
48.28
50.17

53.23
54.79
56.32
50.18
52.17

50.04
76.16
49.56
49.79
52.08
56.92

48.27
71.45
50.96
48.01
47.88

47.22
51.82
55.45
46.91
47.95

Python75

49.07
70.54
50.21
50.00
56.70
57.84

47.32
78.28
45.85
48.65
56.06
61.38

Java250

49.95
71.80
50.05
50.57
51.64

52.25
70.78
46.33
49.47
50.98
Python800
48.55
53.14
54.30
50.52
48.26

47.41
51.82
53.55
48.40
51.45

48.12
72.04
40.40
48.33
54.59
59.89

49.27
71.37
48.23
48.02
47.66

52.63
48.23
54.43
48.23
50.82

47.32
69.20
40.40
48.33
52.08
56.92

48.27
70.78
46.33
48.01
46.88

47.22
48.23
50.09
46.91
47.95

51.78
78.28
54.31
54.88
59.91
63.24

52.25
73.03
51.65
52.66
51.64

53.23
54.79
56.32
55.76
52.17

49.13
73.34
48.49
51.10
56.41
59.55

50.28
71.71
49.81
49.35
49.46

50.06
51.47
53.99
49.58
50.10

1.54
3.40
4.35
2.52
2.74
2.18

1.43
0.77
1.88
1.74
1.93

2.50
2.22
1.97
3.06
1.56

19

