2
2
0
2

y
a
M
7
2

]

A
N
.
h
t
a
m

[

1
v
3
7
2
4
1
.
5
0
2
2
:
v
i
X
r
a

A NEW SEMI-STRUCTURED ALGEBRAIC MULTIGRID METHOD ∗

VICTOR A. P. MAGRI† , ROBERT D. FALGOUT‡ , AND ULRIKE M. YANG§

Abstract. Multigrid methods are well suited to large massively parallel computer architectures
because they are mathematically optimal and display excellent parallelization properties. Since cur-
rent architecture trends are favoring regular compute patterns to achieve high performance, the
ability to express structure has become much more important. The hypre software library provides
high-performance multigrid preconditioners and solvers through conceptual interfaces, including a
semi-structured interface that describes matrices primarily in terms of stencils and logically struc-
tured grids. This paper presents a new semi-structured algebraic multigrid (SSAMG) method built
on this interface. The numerical convergence and performance of a CPU implementation of this
method are evaluated for a set of semi-structured problems. SSAMG achieves signiﬁcantly better
setup times than hypre’s unstructured AMG solvers and comparable convergence. In addition, the
new method is capable of solving more complex problems than hypre’s structured solvers.

Key words. algebraic multigrid, semi-structured multigrid, semi-structured grids, structured

adaptive mesh reﬁnement

AMS subject classiﬁcations. 65F08, 65F10, 65N55

1. Introduction. The solution of partial diﬀerential equations (PDEs) often

involves solving linear systems of equations

(1.1)

Ax = b,

where A ∈ RN ×N is a sparse matrix; b ∈ RN is the right-hand side vector, and
x ∈ RN is the solution vector.
In modern simulations of physical problems, the
number of unknowns N can be huge, e.g., on the order of a few billion. Thus, fast
solution methods must be used for Equation (1.1).

Multigrid methods acting as preconditioners to Krylov-based iterative solvers are
among the most common choices for fast linear solvers. In these methods, a multilevel
hierarchy of decreasingly smaller linear problems is used to target the reduction of
error components with distinct frequencies and solve (1.1) with O(N ) computations
in a scalable fashion. There are two basic types of multigrid methods [7]. Geometric
multigrid employs rediscretization on coarse grids, which needs to be deﬁned explicitly
by the user. A less invasive and less problem-dependent approach is algebraic multi-
grid (AMG) [27], which uses information coming from the assembled ﬁne level matrix
A to compute a multilevel hierarchy. The hypre software library [21, 15] provides
high-performance preconditioners and solvers for the solution of large sparse linear
systems on massively parallel computers with a focus on AMG methods. It features
three diﬀerent interfaces, a structured, a semi-structured, and a linear-algebraic inter-
face. Its most used AMG method, BoomerAMG [19], is a fully unstructured method,
built on compressed sparse row matrices (CSR). The lack of structure presents seri-
ous challenges to achieve high performance on GPU architectures. The most eﬃcient
solver in hypre is PFMG [2], which is available through the structured interface. It
is well suited for implementation on accelerators, since its data structure is built on

∗Submitted to the editors on July 15, 2021.
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Liv-

ermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-JRNL-834288-DRAFT.

†Lawrence Livermore National Laboratory (paludettomag1@llnl.gov).
‡Lawrence Livermore National Laboratory (falgout2@llnl.gov).
§Lawrence Livermore National Laboratory (yang11@llnl.gov).

1

 
 
 
 
 
 
2

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

grids and stencils, and achieves signiﬁcantly better performance than BoomerAMG
when solving the same problems [4, 14]; however, it is applicable to only a subset of
the problems that BoomerAMG can solve. This work presents a new semi-structured
algebraic multigrid (SSAMG) preconditioner, built on the semi-structured interface,
consisting of mostly structured parts and a small unstructured component.
It has
the potential to achieve similar performance as PFMG with the ability to solve more
complex problems.

There have been other eﬀorts to develop semi-structured multigrid methods. For
example, multigrid solvers for hierarchical hybrid grids (HHG) have shown to be highly
eﬃcient [6, 5, 17, 18, 22]. These grids are created by regularly reﬁning an initial,
potentially unstructured grid. Geometric multigrid methods for semi-structured tri-
angular grids that use a similar approach have also been proposed [25]. More recently,
the HHG approach has been generalized to a semi-structured multigrid method [24].
Regarding applications, there are many examples employing semi-structured meshes
which can beneﬁt from new semi-structured algorithms, e.g., petroleum reservoir sim-
ulation [16], marine ice sheets modeling [9], next-generation weather and climate
models [1], and solid mechanics simulators [26], to name a few. In addition, software
frameworks that support the development of block-structured AMR applications such
as AMReX [29, 30] and SAMRAI [20] can beneﬁt from the development of solvers for
semi-structured problems.

This paper is organized as follows. Section 2 reviews the semi-structured con-
ceptual interface of hypre, which enables the description of matrices and vectors that
incorporate information about the problem’s structure. Section 3 describes the new
semi-structured algorithm in detail. In section 4, we evaluate SSAMG’s performance
and robustness for a set of test cases featuring distinct characteristics and make com-
parisons to other solver options available in hypre. Finally, in section 5, we list
conclusions and future work.

2. Semi-structured interface in hypre. The hypre library provides three
conceptual interfaces by which the user can deﬁne and solve a linear system of equa-
tions: a structured (Struct), a semi-structured (SStruct) and a linear algebraic (IJ)
interface. They range from highly specialized descriptions using structured grids and
stencils in the case of Struct to the most generic case where sparse matrices are stored
in a parallel compressed row storage format (ParCSR) [12, 13]. In this paper, we focus
on the SStruct interface [12, 13], which combines features of the Struct and the IJ
interfaces and targets applications with meshes composed of a set of structured sub-
grids, e.g, block-structured, overset, and structured adaptive mesh reﬁnement grids.
The SStruct interface also supports multi-variable PDEs with degrees of freedom ly-
ing in the center, corners, edges or faces of cells composing logically rectangular boxes.
From a computational perspective, these variable types are associated with boxes that
are shifted by diﬀerent oﬀset values. Thus, we consider only cell-centered problems
here for ease of exposition. The current CPU implementation of SSAMG cannot
deal with problems involving multiple variable types yet; however, the mathematical
algorithm of SSAMG expands to such general cases.

There are ﬁve fundamental components required to deﬁne a linear system in the
SStruct interface: a grid, stencils, a graph, a matrix, and a vector. The grid is
composed of np structured parts with independent index spaces and grid spacing.
Each part is formed topologically by a group of boxes, which are a collection of cell-
centered indices, described by their “lower” and “upper” corners. Figure 1 shows an
example of a problem geometry that can be represented by this interface. Stencils

SEMI-STRUCTURED ALGEBRAIC MULTIGRID

3

Figure 1. A semi-structured grid composed of ﬁve parts. Part 4 (orange) consists of two boxes,
while the others consist of just a single box. Furthermore, Part 1 (green) has a reﬁnement factor of
two with respect to the other parts. The pairs (x, y) denote cell coordinates in the i and j topological
directions, respectively. Note that the indices of lower-left cells for each part are independent, since
the grid parts live in diﬀerent index spaces.

are used to deﬁne connections between neighboring grid cells of the same part, e.g., a
typical ﬁve-point stencil would connect a generic grid cell to itself and its immediate
neighbors to the west, east, south, and north. The graph describes how individual
parts are connected, see Figure 3 for an example. We have now the components
to deﬁne a semi-structured matrix A = S + U , which consists of structured and
unstructured components, respectively. S contains coeﬃcients that are associated
with stencil entries. These can be variable coeﬃcients for each stencil entry in each
cell within a part or can be set to just a single value if the stencil entry is constant
across the part. U is stored in ParCSR format and contains the connections between
parts. Lastly, a semi-structured vector describes an array of values associated with
the cells of a semi-structured grid.

3. Semi-structured algebraic multigrid (SSAMG). In the hypre package,
there is currently a single native preconditioner for solving problems with multiple
parts through the SStruct interface, which is a block Jacobi method named Split. It
uses one V-cycle of a structured multigrid solver as an approximation to the inverse
of the structured part of A. This method has limited robustness since it consid-
ers only structured intra-grid couplings in a part to build an approximation of A−1.
In this paper, we present a new solver option for the SStruct interface that com-
putes a multigrid hierarchy taking into account inter-part couplings. This method
is called SSAMG (Semi-Structured Algebraic MultiGrid).
It is currently available
in the recmat branch of hypre. This section deﬁnes coarsening, interpolation, and
relaxation for SSAMG (subsections 3.1, 3.2, and 3.4, respectively). It also describes
how coarse level operators are constructed (subsection 3.3) and discusses a strategy
for improving the method’s eﬃciency at coarse levels (subsection 3.5).

4

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

3.1. Coarsening. As in PFMG [2], we employ semi-coarsening in SSAMG. The
coarsening directions are determined independently for each part of the SStructGrid
to allow better treatment of problems with diﬀerent anisotropies among the parts.
The idea of semi-coarsening is to coarsen in a single direction of strong coupling such
that every other perpendicular line/plane (2D/3D) forms the new coarse level. For
an illustration, see Figure 3, where coarse points are depicted as solid circles.

In the original PFMG algorithm, the coarsening direction was chosen to be the
dimension with smallest grid spacing. This option is still available in hypre by allowing
users to provide an initial nd-dimensional array of “representative grid spacings” that
are only used for coarsening. However, both PFMG and SSAMG can also compute
such an array directly from the matrix coeﬃcients. In SSAMG, this is done separately
for each part, leading to a matrix W ∈ Rnp×nd , where np and nd denote the number
of parts and problem dimensions. Here, element Wpd is heuristically thought of as
a grid spacing for dimension d of part p, and hence a small value indicates strong
coupling.

To describe the computation of W in part p, consider the two-dimensional nine-
point stencil in Figure 2c and assume that AC > 0 (simple sign adjustments can be
made if AC < 0). The algorithm extends naturally to three dimensions. Note also that
both PFMG and SSAMG are currently restricted to stencils that are contained within
this nine-point stencil (27-point in 3D). The algorithm proceeds by ﬁrst reducing
the nine-point matrix to a single ﬁve-point stencil through an averaging process,
then computing the (negative) sum of the resulting oﬀ-diagonal coeﬃcients in each
dimension. That is, for the i-direction (d = 1), we compute

(3.1)

c1 =

(cid:88)

(i,j)

−(ASW + AW + ANW ) − (ASE + AE + AN E),

where the stencil coeﬃcients are understood to vary at each point (i, j) in the grid.
Here the left and right parenthetical sums contribute to the “west” and “east” co-
eﬃcients of the ﬁve-point stencil. The computation is analogous for the j-direction.
From this, we deﬁne

(3.2)

Wpd =

(cid:115) max
0≤i<nd
cd

ci

,

based on the heuristic that the ﬁve-point stencil coeﬃcients are inversely proportional
to the square of the grid spacing.

With W in hand, the semi-coarsening directions for each level and part are com-
puted as described in Algorithm 3.1. The algorithm starts by computing a bounding
box1 around the grid in each part, then loops through the grid levels from ﬁnest
(level 0) to coarsest (level nl). For a given grid level l and part p, the coarsening
direction d(cid:63) is set to be the one with minimum2 value in Wp (line 8). Then, the
bounding box for part p is coarsened by a factor of two in direction d(cid:63) (line 9) and
Wp,d(cid:63) is updated to reﬂect the coarser “grid spacing” on the next grid level (line 10).
If the bounding box is too small, no coarsening is done (line 7) and that part becomes
inactive. The coarsest grid level nl is the ﬁrst level with total semi-structured grid

1Given a set of boxes, a bounding box is deﬁned by the cells with minimum index (lower corner)

and maximum index (upper corner) over the entire set.

2In the case of two or more directions sharing the same value of Wpd, as in an isotropic scenario,

we set d(cid:63) to the one with smallest index.

SEMI-STRUCTURED ALGEBRAIC MULTIGRID

5

size less than a given maximum size smax, unless this exceeds the speciﬁed maximum
number of levels lmax.

Algorithm 3.1 SSAMG coarsening
1: procedure SSAMGCoarsen(W )
2:
3:
4:

for p = 1, np do

Compute part bounding boxes bboxp

end for
for l = 1, nl do

for p = 1, np do

if volume bboxp > 1 then
d(cid:63) = arg mind Wpd
Coarsen bboxp in direction d(cid:63) by a factor of 2
Wpd(cid:63) = 2 ∗ Wpd(cid:63)

5:
6:
7:
8:
9:

end if

10:
11:
12:
end for
13:
14: end procedure

end for

3.2. Interpolation. A key ingredient in multigrid methods is the interpolation
(or prolongation) operator P , the matrix that transfers information from a coarse
level in the grid hierarchy to the next ﬁner grid. The restriction operator R moves
information from a given level to the next coarser grid. For a numerically scalable
method, error modes that are not eﬃciently reduced by relaxation should be captured
in the range of P , so they can be reduced on coarser levels [7].

In SSAMG, we employ a structured operator-based method for constructing pro-
longation similar to the method used in [2]. It is “structured” because P is composed
of only a structured component; interpolation is only done within a part, not between
them. It is “operator-based” because the coeﬃcients are algebraically computed from
S and are able to capture heterogeneity and anisotropy. In hypre, P is a rectangular
matrix deﬁned by two grids (domain and range), a stencil, and corresponding stencil
coeﬃcients. In the case of P , the domain grid is the coarse grid and the range grid
is the ﬁne grid. Since SSAMG uses semi-coarsening, the stencil for interpolation con-
sists of three coeﬃcients that are computed by collapsing the stencil of A, a common
procedure for deﬁning interpolation in algebraic multigrid methods.

To exemplify how P is computed, consider the solution of the Poisson equation on
a cell-centered grid (Figure 2a) formed by a single part and box. Dirichlet boundary
conditions are used and discretization is performed via the ﬁnite diﬀerence method
with a nine-point stencil (Figure 2c). Assume that coarsening is in the i-direction
by selecting ﬁne grid cells with even i-coordinate index (depicted in darker red) and
renumbering them on the coarse grid as shown in Figure 2b. The prolongation oper-
ator connects ﬁne grid cells to their neighboring coarse grid cells with the following
stencil (see [11] for more discussion of stencil notation)

P ∼ (cid:2)PW 1 PE

c = (cid:2)PW ∗ PE
(cid:3)

c ⊕ (cid:2)∗
(cid:3)r1

1

∗(cid:3)r2
c ,

where

(3.3)

PW =

ASW + AW + ANW
AS + AC + AN

, and PE =

ASE + AE + AN E
AS + AC + AN

.

6

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

Figure 2. (a) and (b) show one example of ﬁne and coarse grids, respectively, also known as
range and domain grids for the purpose of prolongation. Coarsening is done in the i-direction, as
depicted by the darker cells in the ﬁne grid. (c) shows the stencil coeﬃcients of A relative to the
grid point (3, 3) from the ﬁne grid. Stencil coeﬃcients for a given grid point can be viewed as the
nonzero coeﬃcients of its respective row in a sparse matrix.

Here, r1 denotes the range subgrid given by the light-colored cells in Figure 2a, and r2
denotes the subgrid given by the dark-colored cells. For a ﬁne-grid cell such as (3, 3)
in Figure 2, interpolation applies the weights PW and PE to the coarse-grid unknowns
associated with cells (2, 3) and (4, 3) in the ﬁne-grid indexing, or (1, 3) and (2, 3) in the
coarse-grid indexing. For a ﬁne-grid cell such as (2, 3), interpolation applies weight 1
to the corresponding coarse-grid unknown.

When one of the stencil entries crosses a part boundary that is not a physical
boundary, we set the coeﬃcient associated with it to zero and update the coeﬃcient
for the opposite stencil entry so that the vector of ones is contained in the range
of the prolongation operator. Although this gives a lower order interpolation along
part boundaries, it limits stencil growth and makes the computation of coarse level
matrices cheaper, see section 3.3.
It also assures that the near kernel of A is well
interpolated between subsequent levels.

Another component needed in a multigrid method is the restriction operator,
which maps information from ﬁne to coarse levels. SSAMG follows the Galerkin
approach, where restriction is deﬁned as the transpose of prolongation (R = P T ).

3.3. Coarse level operator. The coarse level operator Ac in SSAMG is com-
puted via the Galerkin product P T AP . Since the prolongation matrix consists only
of the structured component, the triple-matrix product can be rewritten as

(3.4)

Ac = P T SP + P T U P,

where the ﬁrst term on the right-hand side is the structured component of Ac, and
the second its unstructured component. Note that the last term involves the mul-
tiplication of matrices of diﬀerent types, which we resolve by converting one matrix
type to the other. Since it is generally not possible to represent a ParCSR matrix in
structured format, we convert the structured matrix P to the ParCSR format. How-
ever, we consider only the entries of P that are actually involved in the triple-matrix
multiplication P T U P to decrease the computational cost of the conversion process.

If we examine the new stencil size for Ac, we note that the use of the two-point
interpolation operator limits stencil growth. For example, in the case of a 2D ﬁve-
point stencil at the ﬁnest level, the maximum stencil size on coarse levels is nine, and
for a 3D seven-point stencil at the ﬁnest level, the maximum stencil size on coarse
levels is 27.

We prove here that under certain conditions, the unstructured portion of the

(1,1)(5,5)(3,3)(a) Fine grid(1,1)(b) Coarse grid(2,5)AWASWANWANEAEASEANAS(c) Stencil of Acentered at (3,3)ACSEMI-STRUCTURED ALGEBRAIC MULTIGRID

7

Figure 3. Example of a graph of the matrix U and graph of matrix P derived from the semi-
structured grid shown in Figure 1. The graph of U is depicted by black-solid edges. The graph
of P consists of ﬁve unconnected subgraphs illustrated by the dotted multicolored lines. Lastly, the
boundary points are depicted by black-rimmed circles.

coarse grid operator stays restricted to the part boundaries and does not grow into
the interior of the parts. Note that we deﬁne a part boundary δΩi here as the set of
points in a part Ωi that are connected to neighboring parts in the graph of the matrix
U . For an illustration, see the black-rimmed points in Figure 3. Figure 3 shows the
graph of P for the semi-structured grid in Figure 1 and an example of a graph for the
unstructured matrix U .

Theorem 3.1. We make the following assumptions:

• The grid Ω consists of k parts: Ω = Ω1 ∪ ... ∪ Ωk, where Ωi ∩ Ωj = ∅.
• The grid has been coarsened using semi-coarsening.
• The interpolation P interpolates ﬁne points using at most two adjacent coarse
points aligned with the ﬁne points and maps coarse points onto themselves.
• The graph of the unstructured matrix U contains only connections between
boundary points, i.e., ui,j = 0 if i ∈ Ωm \ δΩm, m = 1, ..., k, or j ∈ Ωn \
δΩn, n = 1, ..., k, and there are no connections within a part, i.e., ui,j = 0 for
i, j ∈ Ωm, m = 1, ..., k.

Then the graph of the unstructured part Uc = P T U P also contains only connections
between boundary points, i.e., uc
i,j = 0 if i ∈ Ωc
n \
δΩc
i,j = 0 for i, j ∈
Ωc

n, n = 1, ..., k, and there are no connections within a part, i.e., uc
m, m = 1, ..., k..

m, m = 1, ..., k, or j ∈ Ωc

m \ δΩc

Proof. Since we want to examine how boundary parts are handled, we reorder
the interpolation matrix P and the unstructured part U , so that all interior points
are ﬁrst followed by all boundary points. The matrices P and U are then deﬁned as
follows:

(3.5)

P =

(cid:18) P I
P BI

(cid:19)

,

P IB
P B

U =

(cid:18) 0

0
0 U B

(cid:19)

.

Note that while U B maps δΩ onto δΩ, P B maps δΩc onto δΩ. Thus, in the extreme
case that all boundary points are ﬁne points, P IB and P B do not exist. Then, the

8

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

coarse unstructured part is given as follows:

(3.6)

Uc = P T U P =

(cid:18) (P BI )T U BP BI
(P B)T U BP BI

(P BI )T U BP B
(P B)T U BP B

(cid:19)

.

It is clear already that there is no longer a connection to P I and P IB, eliminating
many potential connections to interior points; however, we still need to investigate
further the inﬂuence of P BI and P B.

Since P BI , P B, and U B are still very complex due to their dependence on k
parts, we further deﬁne them as follows using the fact that P is deﬁned only on the
structured parts and U only connects boundary points of neighboring parts.

(3.7) P x =

P x
1

P x
2















,

. . .

P x
k

U B =









0

U B
1,2

U B
2,1
...
U B
k,1

0
. . .
...

...
. . .
. . .
U B

k,k−1









.

U B
1,k
...
U B
k−1,k
0

Note that while U B
ij maps δΩi to δΩj, only the coeﬃcients corresponding to edges in
the graph of U that connect points in δΩi to δΩj are nonzero, all other coeﬃcients
are zero. Then, (P x)T U BP y, where “x” and “y” can stand for “BI” as well as “B”,
is given by









(3.8)

0

(P x

1 )T U B

1,2P y

2

2,1P y

1

(P x

2 )T U B
...
k )T U B

(P x

k,1P y

1

0
. . .
...

...
. . .
. . .
k,k−1P y
k )T U B

(P x

k−1

1,kP y

k

(P x

1 )T U B
...
k−1)T U B

(P x

k−1,kP y

k

0









.

This allows us to just focus on the submatrices (P x
j . There are three potential
scenarios that can occur at the boundary between two parts due to our use of semi-
coarsening and a simple two-point interpolation (Figure 3):

i )T U B

ij P y

• all boundary points are coarse points as shown at the right boundary of part

2 and the left boundary of part 3;

• all boundary points are ﬁne points as at the right boundary of part 1 and 4;
• the boundary points are alternating coarse and ﬁne points as illustrated at

the right boundary of part 5.

i

i

Let us deﬁne P x

i |δΩij = I and P BI

i
|δΩij . If all points are ﬁne points, P B

i |δΩij as the matrix that consists of the rows of P x
i that correspond
to all boundary points in δΩi that are connected to boundary points in δΩj. If all
points are coarse points, P B
|δΩij = 0, since there are no connections
from the boundary to the interior for P BI
i |δΩij
does not exist, and P BI
|δΩij is a matrix with at most one nonzero element per row.
Since coarse points in Ωi adjacent to the ﬁne boundary points in δΩi become boundary
points of Ωc
i , e.g., see right boundary of part 1 or left and right boundaries of part
4, all nonzero elements in P BI
|δΩij are associated with a column belonging to δΩi.
i
In the case of alternating ﬁne and coarse points, P BI
|δΩij = 0, since there are no
connections from the boundary to the interior, and P B
i |δΩij is a matrix with at most
two nonzeros in the j-th and k-th columns, where j and k are elements of δΩc
i . Recall
that all columns in Uij belonging to points outside of δΩj and all rows belonging
to points outside of δΩi are zero. Based on this and the previous observations it is
clear that if all points are coarse or we are dealing with alternating ﬁne and coarse

i

SEMI-STRUCTURED ALGEBRAIC MULTIGRID

9

|δΩji = 0. Any additional nonzero coeﬃcients in P BI

points, the submatrices in 3.8 that involve P BI
|δΩij = 0 and
i
P BI
due to boundary
j
points next to other parts will be canceled out in the matrix product. It is also clear,
since the columns of P B
i , that the graph of the product
(P B
j and none to
the interior or to itself.

j only contains connections of points of δΩc

i pertain only to points in δΩc

i will be 0, since P BI

i to points of δΩc

i )T UijP B

or P BI

j

i

i

j

)T UijP BI

. Since we have already shown that P BI

Let us further investigate the case where all boundary points are ﬁne points. We
ﬁrst consider (P BI
|δΩij = 0 for
boundaries with coarse or alternating points leading to zero triple products in 3.8,
we can ignore these scenarios and assume that for both P BI
the boundary
points adjacent to each other are ﬁne points. Each row of P BI
|δΩij has at most one
nonzero element in the column corresponding to the interior coarse point connected
to the ﬁne boundary point. This interior point is also an element in δΩc
i . Therefore
the graph of the product (P BI
i to
points of δΩc
j and none to the interior or to itself. Finally, this statement also holds
for the triple products (P B
j using the same arguments
as above.

only contains connections of points of δΩc

i )T UijP BI

)T UijP BI

and (P BI

)T UijP B

and P BI

j

j

j

i

i

i

i

i

Note that the number of nonzero coeﬃcients in Uc can still be larger than those

in U , however the growth only occurs along part boundaries.

3.4. Relaxation. Relaxation, or smoothing, is an important element of multi-
grid whose task is to eliminate high frequency error components from the solution
vector x. The relaxation process at step k > 0 can be described via the generic
formula:

(3.9)

xk = xk−1 + ωM −1 (b − Axk−1) ,

where M −1 is the smoother operator and ω is the relaxation weight. In SSAMG, we
provide two pointwise relaxation schemes. The ﬁrst one is weighted Jacobi, in which
M −1 = D−1, with D being the diagonal of A. Moreover, ω varies for each multigrid
level and semi-structured part as a function of the grid-spacing metric W :

(3.10)

where

(3.11)

ωp =

2
3 − βp/αp

,

αp =

nd(cid:88)

d=0

1
W 2
pd

and βp =

nd(cid:88)

d = 0,
d(cid:54)=d(cid:63)

1
W 2
pd

.

The ratio βp/αp adjusts the relaxation weight to more closely approximate the optimal
weight for isotropic problems in diﬀerent dimensions. To see how this works, consider
as an example a highly-anisotropic 3D problem that is nearly decoupled in the k-
direction and isotropic in i and j. Because of the severe anisotropy, the problem is
eﬀectively 2D, so the optimal relaxation weight is 4/5. Since our coarsening algorithm
will only coarsen in either directions i or j, we get βp/αp = 1/2, and ωp = 4/5 as
desired.

The second relaxation method supported by SSAMG is L1-Jacobi. This method
is similar to the previous one, in the sense that a diagonal matrix is used to construct

10

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

the smoother operator; however, here, the i-th diagonal element of M equals the
L1-norm of the i-th row of A:

Mii =

N
(cid:88)

j=0

|Aij| .

This form leads to guaranteed convergence when A is positive deﬁnite, i.e., the error
propagation operator E = I − M −1A has a spectral radius smaller than one. We refer
to [3] for more details. This option tends to give slower convergence than weighted
Jacobi; however, a user-deﬁned relaxation factor in the range (1, 2/λmax(M −1A))
(λmax is the maximum eigenvalue) can be used to improve convergence.

To reduce the computational cost of a multigrid cycle within SSAMG, we also
provide an option to turn oﬀ relaxation on certain multigrid levels in isotropic (or
partially isotropic) scenarios. We call this option “skip”, and it has the action of
mimicking full-coarsening. For a given part, SSAMG’s algorithm checks if the coars-
(cid:1). If yes, then relaxation
ening directions for levels “l” and “l − nd” match (cid:0)d(cid:63)
is turned oﬀ (skipped) at level l.

l = d(cid:63)

l−nd

3.5. Hybrid approach. Since SSAMG uses semi-coarsening, the coarsening ra-
tio between the number of variables on subsequent grids is 2. In classical algebraic
multigrid, this value tends to be larger, especially when aggressive coarsening strate-
gies are applied. This leads to the creation of more levels in the multigrid hierarchy
of SSAMG when compared to BoomerAMG. Since the performance beneﬁts of ex-
ploiting structure decreases on coarser grid levels, we provide an option to transition
to an unstructured multigrid hierarchy at a certain level or coarse problem size cho-
sen by the user. This is done by converting the matrix type from SStructMatrix
to ParCSRMatrix at the transition level. The rest of the multigrid hierarchy is set
up using BoomerAMG conﬁgured with the default options used in hypre. With a
properly chosen transition level, this hybrid approach can improve performance. In
the non-hybrid case, SSAMG employs one sweep of the same relaxation method used
in previous levels.

4. Numerical results. In this section, we investigate convergence and perfor-
mance of SSAMG when used as a preconditioner for the conjugate gradient method
(PCG). We also compare it to three other multigrid schemes in hypre, namely PFMG,
Split, and BoomerAMG. The ﬁrst is the ﬂagship multigrid method for structured prob-
lems in hypre based on semi-coarsening [2, 8], the second, a block-Jacobi method built
on top of the SStruct interface [21], in which blocks are mapped to semi-structured
parts, and the last scheme is hypre’s unstructured algebraic multigrid method [19].
Each of these preconditioners has multiple setup parameters that aﬀect its perfor-
mance. For the comparison made here, we select those leading to the best solution
times on CPU architectures. In addition, we consider four variants of SSAMG in an
incremental setting to demonstrate the eﬀects of diﬀerent setup options described in
the paper. A complete list of the methods considered here is given below:
• PFMG: weighted Jacobi3 smoother and “skip” option turned on.
• Split: block-Jacobi method with one V-cycle of PFMG as the inner solver for

parts.

• BoomerAMG: Forward/Backward L1-Gauss-Seidel relaxation [3]; coarsening
via HMIS [10] with a strength threshold value of 0.25; modularized option for

3This is the default relaxation method of PFMG.

SEMI-STRUCTURED ALGEBRAIC MULTIGRID

11

computing the Galerkin product RAP ; one level (ﬁrst) of aggressive coars-
ening with multi-pass interpolation [28] and, in the following levels, matrix-
based extended+i interpolation [23] truncated to a maximum of four nonzero
coeﬃcients per row.

• SSAMG-base: baseline conﬁguration of SSAMG employing weighted L1-

Jacobi smoother with relaxation factor equal to 3/2.

• SSAMG-skip: above conﬁguration plus the “skip” option.
• SSAMG-hybrid: above conﬁguration plus the “hybrid” option for switching
to BoomerAMG as the coarse solver at the 10th level, which corresponds to
three steps of full grid reﬁnement in 3D, i.e., 512 times reduction on the
number of degrees of freedom (DOFs).

• SSAMG-opt: refers to the best SSAMG conﬁguration and employs the same
parameters as SSAMG-hybrid except for switching to BoomerAMG at the
7th level. This results in six pure SSAMG coarsening levels and reduction
factor of 64 on the number of DOFs.

Every multigrid preconditioner listed above is applied to the residual vector via a
single V(1,1)-cycle. The coarsest grid size is at most 8 in all cases where BoomerAMG
is used, it equals the number of parts for SSAMG-base and SSAMG-skip, and one for
PFMG. The number of levels in the various multigrid hierarchies changes for diﬀerent
problem sizes and solvers.

We consider four test cases featuring three-dimensional semi-structured grids,
diﬀerent part distributions, and anisotropy directions. Each semi-structured part is
formed by a box containing m × m × m cells. Similarly, in a distributed parallel
setting, each semi-structured part is owned by p × p × p unique MPI tasks, meaning
that the global semi-structured grid is formed by replicating the local m3-sized boxes
belonging to each part by p times in each topological direction. This leads to a total
of npp3 MPI tasks for np parts. We are particularly interested in evaluating weak
scalability of the proposed method for a few tasks up to a range of thousands of MPI
tasks. Thus, we vary the value of p from one to eight with increments of one.

For the results, we report the number of iterations needed for convergence, setup
time of the preconditioner, and solve time of the iterative solver. All experiments
were performed on Lassen, a cluster at LLNL equipped with two IBM POWER9
processors (totaling 44 physical cores) per node. However, we note that up to 32
cores per node were used in the numerical experiments to reduce the eﬀect of limited
memory bandwidth. Convergence of the iterative solver is achieved when the L2-
norm of the residual vector is less than 10−6||b||2. The linear systems were formed
via discretization of the Poisson equation through the ﬁnite diﬀerence method, and
zero Dirichlet boundary conditions are used everywhere except for the k = 0 boundary,
which assumes a value of one. The initial solution guess passed to PCG is the vector
of zeros. The discretization scheme we used leads to the following seven-point stencil:

(4.1)

A ∼ (cid:2)−γ(cid:3)





−β
−α 2(α + β + γ) −α
−β





(cid:2)−γ(cid:3)

where α, β, and γ denote the coeﬃcients in the i, j, and k topological directions. For
the isotropic problems, α = β = γ = 1, for the anisotropic cases we deﬁne their values
in section 4.2.

4.1. Test case 1 - cubes side-by-side. The ﬁrst test case is made of an iso-
tropic and block-structured three-dimensional domain composed of four cubes, where

12

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

each contains the same number of cells and refers to a diﬀerent semi-structured part.
Figure 4 shows a plane view of one particular case with cubes formed by four cells in
each direction. Regarding the solver choices, since PFMG works only for single-part
problems, we translated parts into diﬀerent boxes in an equivalent structured grid.
Note that such a transformation is only possible due to the simplicity of the current
problem geometry and is unattainable in more general cases such as those described
later in sections 4.3 and 4.4.

Figure 4. ij-plane cut of the three-dimensional base grid used for test case 4.1. There are no
adjacent parts in the k-direction. Colors denote diﬀerent parts, and the experiments showed in this
section are produced by equally reﬁning the base grid in all directions.

For the numerical experiments, we consider m = 128, which gives a local problem
size per part of 2, 097, 152 DOFs and a global problem size of 8, 388, 608 DOFs for 4
MPI tasks (p = 1). The largest problem we consider here, obtained when p = 8, has
a global size of about 4.3B DOFs.

Figure 5. Weak scalability results for test case 1. Three metrics are shown in the ﬁgure, i.e.,
setup phase times in seconds (left); solve phase times in seconds (middle), and number of iterations
(right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which varies from 4
(p = 1) up to 2048 (p = 8).

Figure 5 shows weak scalability results for this test case. Analyzing the itera-
tion counts, Split is the only method that does not converge in less than a maximum
iteration count of 100 for runs larger than 500M DOFs (p = 4). This lack of numeri-
cal scalability was already expected since couplings among parts are not captured in
Split’s multigrid hierarchy. The best iteration counts are reached by PFMG, which is

Part 0Part 2Part 1Part 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3Proc 0Proc 1Proc 2Proc 3(b) Grid partitioning (4 processes)(a) Base grid025651276810241280153617922048MPI tasks1234567Setup time [s]025651276810241280153617922048MPI tasks05101520253035Solve time [s]025651276810241280153617922048MPI tasks020406080100IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplitPFMGSEMI-STRUCTURED ALGEBRAIC MULTIGRID

13

natural since this method can take full advantage of the problem’s geometry. Notice-
ably, the iteration counts of SSAMG-opt follow PFMG closely, since part boundaries
are no longer considered after switching to BoomerAMG on the coarser levels and the
switch is done earlier here than in SSAMG-hybrid; the other SSAMG variants need
a higher number of iterations for achieving convergence, since the interpolation is of
lower quality along part boundaries. Lastly, the BoomerAMG preconditioner shows a
modest increase in iteration counts for increasing problem sizes, and this is common
in the context of algebraic multigrid.

Solve times are directly related to iteration counts. Since Split has a similar
iteration cost to the other methods but takes the largest number of iterations to
converge, it is the slowest option in solution time. For the same reason, the three
SSAMG variants except for SSAMG-opt are slower than the remaining precondition-
ers. Still, SSAMG-skip is faster than SSAMG-base despite showing more iterations
because the “skip”option reduces its iteration cost. The optimal variant SSAMG-opt
is able to beat BoomerAMG by a factor of 1.6x for p = 1 and 2.3x for p = 8. More-
over, SSAMG-opt shows little performance degradation with respect to the fastest
preconditioner (PFMG).

BoomerAMG is the slowest option when analyzing setup times. This is a result

of multiple reasons, the three most signiﬁcant ones being:

• BoomerAMG employs more elaborate formulas for computing interpolation,
which require more computation time than the simple two-point scheme used
by PFMG and SSAMG;

• the triple-matrix product algorithm for computing coarse operators imple-
mented for CSR matrices is less eﬃcient than the specialized algorithm em-
ployed by Struct and SStruct matrices4;

• BoomerAMG’s coarsening algorithm involves choosing ﬁne/coarse nodes on
the matrix graph besides computing a strength of connection matrix. Those
steps are not necessary for PFMG or SSAMG.

This is followed by Split, which should have setup times close to PFMG, but due
to a limitation of its parallel implementation, the method does not scale well with
an increasing number of parts. On the other hand, all the SSAMG variants show
comparable setup times, up to 2.8x faster than BoomerAMG. The ﬁrst two SSAMG
variants share the same setup algorithm, and their lines are superposed. SSAMG-opt
has a slightly slower setup for p ≤ 5 than SSAMG-base, but for p > 5 the setup
times of these two methods match. The fastest SSAMG variant by a factor of 1.2x
with respect to the others is SSAMG-hybrid, and that holds because it generates a
multigrid hierarchy with fewer levels than the non-hybrid SSAMG variants leading to
less communication overhead associated with collective MPI calls. The same argument
is true for SSAMG-opt; however, the beneﬁts of having fewer levels is outweighed by
the cost of converting the SStructMatrix to a ParCSRMatrix in the switching level.
Still, SSAMG-opt is 2.3x and 3x faster than BoomerAMG for p = 1 and p = 8,
respectively. Finally, PFMG yields the best setup times with a speedup of nearly 4.6x
with respect to BoomerAMG and up to 1.9x with respect to SSAMG.

We note that PFMG is naturally a better preconditioner for this problem than
SSAMG since it interpolates across part boundaries. However, this test case was
signiﬁcant to show how close the performance of SSAMG can be to PFMG, and we
demonstrated that SSAMG-opt is not much behind PFMG, besides yielding faster
solve and setup times than BoomerAMG.

4We plan to explore this statement with more depth in a following communication.

14

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

4.2. Test case 2 - anisotropic cubes. This test case has the same problem
geometry and sizes (m = 128) as the previous test case; however, it employs diﬀerent
stencil coeﬃcients (α, β, and γ) for each part of the grid with the aim of evaluating
how anisotropy aﬀects solver performance. Particularly, we consider three diﬀerent
scenarios (Figure 6) where the coeﬃcients relative to stencil entries belonging to the
direction of strongest anisotropy for a given part are 100 times larger than the re-
maining ones. The directions of prevailing anisotropy for each scenario are listed
below:

(A) “i” (horizontal) for all semi-structured parts.
(B) “i” for parts zero and two; “j” (vertical) for parts one and three.
(C) “i” for part zero, “j” for part three, and “k” (depth) for parts one and two.
Regarding the usage of PFMG for this problem, the same transformation mentioned
in section 4.1 apply here as well.

Figure 6. XY -plane cut of the three-dimensional grids used in test case 4.2. We consider
three anisotropy scenarios. Arrows indicate the direction of prevailing anisotropy in each part of the
grid, e.g., i-direction in scenario A. Diagonal arrows in the rightmost case indicate the k-direction.

Figure 7 shows the results referent to scenario A. The numerical scalabilities of the
diﬀerent methods look better than in the previous test case. This is valid especially
for the SSAMG variants, because the two-point interpolation strategy is naturally a
good choice for the ﬁrst few coarsening levels when anisotropy is present in the matrix
coeﬃcients. Such observation also applies to the less scalable Split method, explaining
the better behavior seen here with respect to Figure 5. Again, PFMG uses the least
number of iterations followed closely by SSAMG-opt and BoomerAMG.

Regarding solve times, SSAMG-opt is about 1.3x faster than BoomerAMG for
p ≤ 2, while for p > 2 these methods show similar times. The “skip” option of SSAMG
is not beneﬁcial for this case since the solve times of SSAMG-skip are higher than
SSAMG-base. In fact, such an option does not play a signiﬁcant role in reducing the
solve time compared to isotropic test cases. This is because coarsening happens in the
same direction for the ﬁrst few levels in anisotropic test cases, and thus relaxation is
skipped only in the later levels of the multigrid hierarchy where the cost per iteration
associated with them is already low compared to the initial levels. Moreover, the
omission of relaxation in coarser levels of the multigrid hierarchy can be detrimental
for convergence in SSAMG, explaining why SSAMG-skip requires more iterations
than SSAMG-base. Following the fact that PFMG is the method that needs fewer
iterations for convergence, it is also the fastest in terms of solution times. For setup
times, the four SSAMG variants show comparable results, and similar conclusions to
test case 1 are valid here. Lastly, the speedups of SSAMG-opt over BoomerAMG are
3.3x and 2.5x for p = 1 and p = 8, respectively.

(a) Scenario A(b) Scenario B(c) Scenario CSEMI-STRUCTURED ALGEBRAIC MULTIGRID

15

Figure 7. Weak scalability results for scenario A of test case 2. Three metrics are shown in the
ﬁgure, i.e., setup phase times in seconds (left); solve phase times in seconds (middle), and number
of iterations (right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which
varies from 4 (p = 1) up to 2048 (p = 8).

Results for scenario B are shown in Figure 8. The most signiﬁcant diﬀerence
here compared to scenario A are the results for PFMG. Particularly, the number
of iterations is much higher than in the previous cases. This is caused by the fact
that PFMG employs the same coarsening direction everywhere on the grid, and thus
it cannot recognize the diﬀerent regions of anisotropy as done by SSAMG. This is
clearly sub-optimal since a good coarsening scheme should adapt to the direction of
largest coupling of the matrix coeﬃcients. The larger number of iterations is also
reﬂected in the solve times of PFMG, which become less favorable than those by
SSAMG and BoomerAMG. Setup times of PFMG continue to be the fastest ones;
however, this advantage is not suﬃcient to maintain its position of fastest method
overall. The comments regarding the speedups of SSAMG compared to BoomerAMG
made for scenario A also apply here.

Figure 8. Weak scalability results for scenario B of test case 2. Three metrics are shown in the
ﬁgure, i.e., setup phase times in seconds (left); solve phase times in seconds (middle), and number
of iterations (right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which
varies from 4 (p = 1) up to 2048 (p = 8).

We conclude this section by analyzing the results, given in Figure 9, for the last
anisotropy scenario C. Since there is a mixed anisotropy conﬁguration in this case as in
scenario B, PFMG does not show a satisfactory convergence behavior. On the other

025651276810241280153617922048MPI tasks1234567Setup time [s]025651276810241280153617922048MPI tasks5101520253035Solve time [s]025651276810241280153617922048MPI tasks01020304050607080IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplitPFMG025651276810241280153617922048MPI tasks1234567Setup time [s]025651276810241280153617922048MPI tasks510152025303540Solve time [s]025651276810241280153617922048MPI tasks020406080IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplitPFMG16

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

hand, the SSAMG variants show good numerical and computational scalabilities,
and, particularly, SSAMG-opt shows similar speedups compared to the BoomerAMG
variants as discussed in the previous scenarios. When considering all three scenarios
discussed in this section, we note that SSAMG shows good robustness with changes
in anisotropy, and this an important advantage over PFMG.

Figure 9. Weak scalability results for scenario C of test case 2. Three metrics are shown in the
ﬁgure, i.e., setup phase times in seconds (left); solve phase times in seconds (middle), and number
of iterations (right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which
varies from 4 (p = 1) up to 2048 (p = 8).

4.3. Test case 3 - three-points intersection. In this test case, we consider a
grid composed topologically of three semi-structured cubic parts that share a common
intersection edge in the k-direction (Figure 10). Stencil coeﬃcients are isotropic,
but this test case is globally non-Cartesian. In particular, the coordinate system is
diﬀerent on either side of the boundary between parts 1 and 2. For example, an east
stencil coeﬃcient coupling Part 1 to Part 2 is symmetric to a north coeﬃcient coupling
Part 2 to Part 1.

Figure 10. ij-plane view of the base geometry for test case 4.3. Equally reﬁned instances of

this problem in all directions are used for obtaining the results.

For the numerical experiments of this section, we use m = 160, which gives a local
problem size per part of 4, 096, 000 DOFs, and a global problem size of 12, 288, 000
DOFs, when p = 1, i.e., three parts and MPI tasks. Figure 11 reports weak scalability
results for the current test case. As noted in section 4.1, it is not possible to recast
this problem into a single part; thus, we cannot show results for PFMG here.

025651276810241280153617922048MPI tasks12345678Setup time [s]025651276810241280153617922048MPI tasks510152025Solve time [s]025651276810241280153617922048MPI tasks0102030405060IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplitPFMGPart 0Part 1Part 2ijijijSEMI-STRUCTURED ALGEBRAIC MULTIGRID

17

Figure 11. Weak scalability results for test case 3. Three metrics are shown in the ﬁgure, i.e.,
setup phase times in seconds (left); solve phase times in seconds (middle), and number of iterations
(right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which varies from 3
(p = 1) up to 1536 (p = 8).

Examining the iteration counts reported in Figure 11, we see that SSAMG-opt
is the fastest converging option with the number of iterations ranging from 11, for
p = 1 (3 MPI tasks), to 14, for p = 8 (1536 MPI tasks). This is the best numerical
scalability among the other methods, including BoomerAMG. On the other hand, the
remaining SSAMG variants do not show such good scalability as in the previous test
cases. Once again, this is related to how SSAMG computes interpolation weights of
nodes close to part boundaries. In this context, we plan to investigate further how to
improve SSAMG’s interpolation such that the non-hybrid SSAMG variants can have
similar numerical scalability to SSAMG-opt. As in the previous test cases, the Split
method is the least performing method and does not converge within 100 iterations
for p ≥ 3 (Np ≥ 81).

Regarding solve times, SSAMG-opt is the fastest method since it needs the min-
imum amount of iterations to reach convergence. Compared to BoomerAMG, its
speedup is 1.3x for p = 1 and 1.1x for p = 8. SSAMG-skip shows solution times
smaller than SSAMG-base, and, here, the “skip” option is beneﬁcial to performance.
Lastly, looking at setup times, all SSAMG variants show very similar timings and
the optimal variant is up to 2.9x faster than BoomerAMG, proving once again the
beneﬁts of exploiting problem structure.

4.4. Test case 4 - structured adaptive mesh reﬁnement (SAMR). In the
last problem, we consider a three-dimensional SAMR grid consisting of one level of
grid reﬁnement, and thus composed of two semi-structured parts (Figure 12). The
ﬁrst one, in red, refers to the outer coarse grid, while the second, in blue, refers to
the reﬁned patch (by a factor of two) located in the center of the grid. Each part has
the same number of cells. To construct the linear system matrix for this problem,
we treat coarse grid points living inside of the reﬁned part as ghost unknowns, i.e.,
the diagonal stencil entry for these points is set to one and the remaining oﬀ-diagonal
stencil entries are set to zero. Inter-part couplings at ﬁne-coarse interfaces are stored
in the unstructured matrix (U ), and the value for the coeﬃcients connecting ﬁne
grid cells with its neighboring coarse grid cells (and vice-versa) is set to 2/3. This
value was determined by composing a piecewise constant interpolation formula with a
ﬁnite volume discretization rule. We refer the reader to the SAMR section of hypre’s
documentation [21] for more details.

0192384576768960115213441536MPI tasks24681012Setup time [s]0192384576768960115213441536MPI tasks102030405060Solve time [s]0192384576768960115213441536MPI tasks020406080100IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplit18

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

Figure 12. XY -plane cut of the three-dimensional semi-structured grid used in test case 4.4
when m = 8. The semi-structured parts represent two levels of reﬁnement and contain the same
number of cells.

The numerical experiments performed in this section used m = 128, leading to a
local problem size per part of 2, 097, 152 DOFs, and a global problem size of 4, 194, 304
DOFs, for p = 1 (Nprocs = 2). Figure 13 shows weak scalability results for this test
case. This problem is not suitable for PFMG, thus we do not show results for PFMG.

Figure 13. Weak scalability results for test case 4. Three metrics are shown in the ﬁgure, i.e.,
setup phase times in seconds (left); solve phase times in seconds (middle), and number of iterations
(right). All curves are plotted with respect to the number of MPI tasks, Nprocs, which varies from 2
(p = 1) up to 1024 (p = 8).

As in the previous test cases, Split does not reach convergence within 100 itera-
tions when p ≥ 3. Then, SSAMG-skip is the second least convergent option followed
by SSAMG-base. The best option is again SSAMG-opt with the number of iterations
ranging from 15 (p = 1) to 20 (p = 8). Furthermore, its iteration counts are practi-
cally constant for the several parallel runs, except for slight jumps located at p = 4
(Nprocs = 128) and p = 8 (Nprocs = 1024), which are present in SSAMG-hybrid as
well.

As noted before, solve times reﬂect the methods’ convergence performance. In
particular, the cheaper iterations of SSAMG-skip are not able to oﬀset the higher
number of iterations for convergence over SSAMG-base. That explains why these
two methods show very similar solve times. Split is the least performing option due
to its lack of robustness. SSAMG-opt and BoomerAMG have similar performance,
with SSAMG-opt slightly better for various cases, but BoomerAMG showing more

Part 0Part 101282563845126407688961024MPI tasks2345678Setup time [s]01282563845126407688961024MPI tasks51015202530Solve time [s]01282563845126407688961024MPI tasks020406080100IterationsSSAMG-baseSSAMG-skipSSAMG-hybridSSAMG-optBoomerAMGSplitSEMI-STRUCTURED ALGEBRAIC MULTIGRID

19

consistent performance here.

Setup times of the SSAMG variants are very similar. Listing them in decreasing
order of times, SSAMG-base and SSAMG-skip show nearly the same values, followed
by SSAMG-opt, and SSAMG-hybrid is the fastest option. The results for Split are
better here than in the previous test cases, and this is due to the small number of
semi-structured parts involved in this SAMR problem. Still, SSAMG leads to the
fastest options. The slowest method for setup is again BoomerAMG and SSAMG-opt
speedups with respect to it are 4x for p = 1 and 2.7x for p = 8.

5. Conclusions. In this paper, we presented a novel algebraic multigrid method,
built on the semi-structured interface in hypre, capable of exploiting knowledge about
the problem’s structure and having the potential of being faster than an unstruc-
tured algebraic multigrid method such as BoomerAMG on CPUs and accelerators.
Moreover, SSAMG features a multigrid hierarchy with controlled stencil sizes and
signiﬁcantly improved setup times.

We developed a distributed parallel implementation of SSAMG for CPU archi-
tectures in hypre. Furthermore, we tested its performance, when used as a precondi-
tioner to PCG, for a set of semi-structured problems featuring distinct characteristics
in terms of grid, stencil coeﬃcients, and anisotropy. SSAMG proves to be numerically
scalable for problems having up to a few billion degrees of freedom and its current
implementation achieves setup phase speedups up to a factor of four and solve phase
speedups up to 1.4x with respect to BoomerAMG.

For future work, we plan to improve diﬀerent aspects of SSAMG and its implemen-
tation. We will further investigate SSAMG convergence for more complex problems
than have been considered so far. We want to explore adding an unstructured com-
ponent to the prolongation matrix to improve interpolation across part boundaries
and evaluate how this beneﬁts convergence and time to solution. We also plan to add
a non-Galerkin option for computing coarse operators targeting isotropic problems
since this approach applied in PFMG has shown excellent runtime improvements on
both CPU and GPU. Finally, we will develop a GPU implementation for SSAMG.

Acknowledgments. This material is based upon work supported by the U.S.
Department of Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing
Research, Scientiﬁc Discovery through Advanced Computing (SciDAC) program.

REFERENCES

[1] S. Adams, R. Ford, M. Hambley, J. Hobson, I. Kavˇciˇc, C. Maynard, T. Melvin,
E. M¨uller, S. Mullerworth, A. Porter, M. Rezny, B. Shipway, and R. Wong,
LFRic: Meeting the challenges of scalability and performance portability in weather and
climate models, Journal of Parallel and Distributed Computing, 132 (2019), pp. 383–396,
https://doi.org/10.1016/j.jpdc.2019.02.007.

[2] S. F. Ashby and R. D. Falgout, A parallel multigrid preconditioned conjugate gradient al-
gorithm for groundwater ﬂow simulations, Nuclear Science and Engineering, 124 (1996),
pp. 145 – 159, https://doi.org/10.13182/nse96-a24230.

[3] A. H. Baker, R. D. Falgout, T. V. Kolev, and U. M. Yang, Multigrid smoothers for
ultraparallel computing, SIAM Journal on Scientiﬁc Computing, 33 (2011), pp. 2864–2887,
https://doi.org/10.1137/100798806.

[4] A. H. Baker, R. D. Falgout, T. V. Kolev, and U. M. Yang, Scaling Hypre’s Multigrid
Solvers to 100,000 Cores, Springer London, London, 2012, pp. 261–279, https://doi.org/
10.1007/978-1-4471-2437-5 13.

[5] B. Bergen, G. Wellein, F. H¨ulsemann, and U. R¨ude, Hierarchical hybrid grids: achieving
TERAFLOP performance on large scale ﬁnite element simulations, International Journal

20

V. A. P. MAGRI, R. D. FALGOUT, AND U. M. YANG

of Parallel, Emergent and Distributed Systems, 22 (2007), pp. 311–329, https://doi.org/
10.1080/17445760701442218.

[6] B. K. Bergen and F. H¨ulsemann, Hierarchical hybrid grids: data structures and core algo-
rithms for multigrid, Numerical Linear Algebra with Applications, 11 (2004), pp. 279–291,
https://doi.org/10.1002/nla.382.

[7] W. L. Briggs, V. E. Henson, and S. F. McCormick, A Multigrid Tutorial, Second Edition,
Society for Industrial and Applied Mathematics, second ed., 2000, https://doi.org/10.1137/
1.9780898719505.

[8] P. N. Brown, R. D. Falgout, and J. E. Jones, Semicoarsening multigrid on distributed
memory machines, SIAM Journal on Scientiﬁc Computing, 21 (2000), pp. 1823–1834,
https://doi.org/10.1137/S1064827598339141.

[9] S. L. Cornford, D. F. Martin, D. T. Graves, D. F. Ranken, A. M. Le Brocq, R. M.
Gladstone, A. J. Payne, E. G. Ng, and W. H. Lipscomb, Adaptive mesh, ﬁnite volume
modeling of marine ice sheets, Journal of Computational Physics, 232 (2013), pp. 529–549,
https://doi.org/10.1016/j.jcp.2012.08.037.

[10] H. De Sterck, U. M. Yang, and J. J. Heys, Reducing complexity in parallel algebraic
multigrid preconditioners, SIAM Journal on Matrix Analysis and Applications, 27 (2006),
pp. 1019–1039, https://doi.org/10.1137/040615729.

[11] C. Engwer, R. D. Falgout, and U. M. Yang, Stencil computations for PDE-based applica-
tions with examples from DUNE and hypre, Concurrency and Computation: Practice and
Experience, 29 (2017), p. e4097, https://doi.org/10.1002/cpe.4097.

[12] R. D. Falgout, J. E. Jones, and U. M. Yang, Conceptual interfaces in hypre, Future Gen-
eration Computer Systems, 22 (2006), pp. 239–251, https://doi.org/10.1016/j.future.2003.
09.006.

[13] R. D. Falgout, J. E. Jones, and U. M. Yang, The design and implementation of hypre,
a library of parallel high performance preconditioners,
in Numerical Solution of Par-
tial Diﬀerential Equations on Parallel Computers, A. M. Bruaset and A. Tveito, eds.,
Berlin, Heidelberg, 2006, Springer Berlin Heidelberg, pp. 267–294, https://doi.org/10.
1007/3-540-31619-1 8.

[14] R. D. Falgout, R. Li, B. Sj¨ogreen, L. Wang, and U. M. Yang, Porting hypre to heteroge-
neous computer architectures: Strategies and experiences, Parallel Computing, 108 (2021),
p. 102840, https://doi.org/10.1016/j.parco.2021.102840.

[15] R. D. Falgout and U. M. Yang, hypre: A library of high performance preconditioners, in
Computational Science — ICCS 2002, P. M. A. Sloot, A. G. Hoekstra, C. J. K. Tan, and
J. J. Dongarra, eds., Berlin, Heidelberg, 2002, Springer Berlin Heidelberg, pp. 632–641,
https://doi.org/10.1007/3-540-47789-6 66.

[16] B. Ganis, G. Pencheva, and M. F. Wheeler, Adaptive mesh reﬁnement with an en-
hanced velocity mixed ﬁnite element method on semi-structured grids using a fully coupled
solver, Computational Geosciences, 23 (2019), pp. 1573–1499, https://doi.org/10.1007/
s10596-018-9789-6.

[17] B. Gmeiner and U. R¨ude, Peta-scale hierarchical hybrid multigrid using hybrid paralleliza-
tion, in Large-Scale Scientiﬁc Computing, I. Lirkov, S. Margenov, and J. Wa´sniewski,
eds., Berlin, Heidelberg, 2014, Springer Berlin Heidelberg, pp. 439–447, https://doi.org/
10.1007/978-3-662-43880-0 50.

[18] B. Gmeiner, U. R¨ude, H. Stengel, C. Waluga, and B. Wohlmuth, Towards textbook eﬃ-
ciency for parallel multigrid, Numerical Mathematics: Theory, Methods and Applications,
8 (2015), p. 22–46, https://doi.org/10.4208/nmtma.2015.w10si.

[19] V. E. Henson and U. M. Yang, BoomerAMG: A parallel algebraic multigrid solver and pre-
conditioner, Applied Numerical Mathematics, 41 (2002), pp. 155–177, https://doi.org/10.
1016/S0168-9274(01)00115-5. Developments and Trends in Iterative Methods for Large
Systems of Equations - in memorium Rudiger Weiss.

[20] R. D. Hornung and S. R. Kohn, Managing application complexity in the SAMRAI object-
oriented framework, Concurrency and Computation: Practice and Experience, 14 (2002),
pp. 347–368, https://doi.org/10.1002/cpe.652.

[21] hypre: High performance preconditioners. http://www.llnl.gov/CASC/hypre/, https://github.

com/hypre-space/hypre.

[22] N. Kohl and U. R¨ude, Textbook eﬃciency: massively parallel matrix-free multigrid for the

stokes system, (2020), https://doi.org/10.48550/arXiv.2010.13513.

[23] R. Li, B. Sj¨ogreen, and U. M. Yang, A new class of AMG interpolation methods based on
matrix-matrix multiplications, SIAM Journal on Scientiﬁc Computing, 43 (2021), pp. S540–
S564, https://doi.org/10.1137/20M134931X.

[24] M. Mayr, L. Berger-Vergiat, P. Ohm, and R. S. Tuminaro, Non-invasive multigrid for

SEMI-STRUCTURED ALGEBRAIC MULTIGRID

21

semi-structured grids, 2021, https://doi.org/10.48550/arXiv.2103.11962.

[25] C. Rodrigo, F. J. Gaspar, and F. J. Lisbona, Multigrid methods on semi-structured grids,
Archives of Computational Methods in Engineering, 19 (2012), pp. 499–538, https://doi.
org/10.1007/s11831-012-9078-9.

[26] B. Runnels, V. Agrawal, W. Zhang, and A. Almgren, Massively parallel ﬁnite diﬀer-
ence elasticity using block-structured adaptive mesh reﬁnement with a geometric multigrid
solver, Journal of Computational Physics, 427 (2021), p. 110065, https://doi.org/10.1016/
j.jcp.2020.110065.

[27] K. St¨uben, A review of algebraic multigrid, Journal of Computational and Applied Mathemat-
ics, 128 (2001), pp. 281–309, https://doi.org/10.1016/S0377-0427(00)00516-1. Numerical
Analysis 2000. Vol. VII: Partial Diﬀerential Equations.

[28] U. M. Yang, On long-range interpolation operators for aggressive coarsening, Numerical Linear

Algebra with Applications, 17 (2010), pp. 453–472, https://doi.org/10.1002/nla.689.

[29] W. Zhang, A. Almgren, V. Beckner, J. Bell, J. Blaschke, C. Chan, M. Day, B. Friesen,
K. Gott, D. Graves, et al., AMReX: a framework for block-structured adaptive mesh
reﬁnement, Journal of Open Source Software, 4 (2019), pp. 1370–1370, https://doi.org/10.
21105/joss.01370.

[30] W. Zhang, A. Myers, K. Gott, A. Almgren, and J. Bell, AMReX: Block-structured adap-
tive mesh reﬁnement for multiphysics applications, The International Journal of High
Performance Computing Applications, 35 (2021), pp. 508–526, https://doi.org/10.1177/
10943420211022811.

