Assessing IT Architecture Evolution using Enriched
Enterprise Architecture Models

Christophe Ponsard
CETIC Research Centre
Gosselies, Belgium
christophe.ponsard@cetic.be

2
2
0
2

r
p
A
3
1

]
E
S
.
s
c
[

1
v
6
2
2
6
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—Enterprise Architecture (EA) help companies to
keep the evolution of their IT architecture aligned with their
business evolution using a set of complementary models ranging
from vision to infrastructure. In this paper, we explore how such
models can be exploited to best drive this co-evolution by helping
in validating different change management strategies. Consid-
ering state of the art techniques in service oriented and Cloud
architecture, we propose to enrich and improve the quality of the
information about the applications and software components on
various qualities (e.g. performance, scalability, security, technical
debt) in order to include them in the assessment process already
at the Enterprise Architecture level rather than discovering
them later in the change implementation phase. Our work is
experimented on the LabNaf EA framework and is illustrated
on a partial case study taken from an industrial project.

Index Terms—Architecture Evolution, Enterprise Architec-
ture, Business Alignment, Service Oriented Architecture, Non-
Functional Requirements, Tool Support

I. INTRODUCTION
Software evolvability is its ability to cope with future
change in a cost effective way, especially considering changes
in its environment, requirements and implementation technolo-
gies [1], [2]. Assessing this characteristic is highly valuable
for supporting strategic decision process, especially for long-
lived systems which, like most current systems, heavily rely
on software for their operation.

A system or software architecture can be deﬁned as ’the
fundamental organisation of a system embodied in its compo-
nents, their relationships to each other and to the environment,
and the principles guiding its design and evolution’ [3]. The
last words clearly show its key role in the evolution process.
Over the years software have become tightly integrated in
organisations to manage their information systems, starting
for simple operational processing then vertically evolving
towards to decision support and to strategic management.
Their generalisation across enterprises also required horizontal
integration to avoid functional silos and lead to the emergence
of Enterprise Resource Planning and Enterprise Application
Integration. Such frameworks have also evolved towards high
ﬂexibility and dynamics thanks to the use of Service Oriented
Architectures [4].

Of course organisations are also evolving in terms of goals,
structure and processes, triggering the need to change the
supporting software. Conversely, the adoption of new soft-
ware tools enable new form of organisation. Hence, both the
business and IT part of the organisation depicted in Figure

1 are actually co-evolving. In order to cover all aspects in
both dimensions, the notion of Enterprise Architecture (EA)
is deﬁned as the discipline for proactively and holistically
leading enterprise responses to disruptive forces by identifying
and analysing the execution of change toward desired business
vision and outcomes [5].

Fig. 1. General structure of an Enterprise Architecture [6]

Enterprise Architecture Framework (EAF) refers to any
framework, process, or methodology which informs how to
create and use an EA [7]. Although EAF can be questioned
about their usefulness, adequacy or value [8], they are widely
acknowledged as providing the tooling to gather both the as-
is and to-be states of the enterprise from all the involved
stakeholders and to deﬁne a strategic roadmap to conduct
the change. A wide variety of EAF has developed with
particular advantages and disadvantages, some historical like
Zachman [9], other still evolving like TOGAF [10]. They are
also complemented and supported by a variety of standard
modelling notations such as UML [11] and BPMN [12] as
well as business speciﬁc way to capture, analyse or visualise
enterprise data (e.g. PESTEL, Business Canvas, scorecards).
Such approaches and the underlying tooling is increasingly
relying on efﬁcient modelling platform which enables many
different analysis to determine the best road to take to drive
the change inside the organisation.

In the scope of our work, we focus on the evolution of
the software part of the system and also stay at the macro

 
 
 
 
 
 
that

level
i.e. high
is captured by enterprise architecture,
level architecture described in terms of application down to
component/service-level interactions but without considering
the internal implementation structure of the components. The
current trends are:

• EA models are becoming more and more semantically

rich and detailed

• frameworks are providing increasingly powerful analysis

capabilities

• DEVOPS tools are enabling a high level of automation
of the software lifecycle and runtime monitoring making
easy to harvest data about component qualities

This combination opens new perspectives for conducting
EA by considering more detailed software level information
earlier. Currently, it is often fully studied only when progress-
ing in the change implementation through speciﬁc projects
and some change might reveal impractical or more costly than
expected, with few alternatives at this stage. More precisely,
we are interested by the following research questions:

• RQ1 - What kind of analysis can be carried out from the
EA model to better drive the co-evolution of the business
and IT dimensions ?

• RQ2 - What useful information should be gathered about
software component and services in connection with the
elaboration of business level strategies ?

• RQ3 - What are the key issues (risks/costs) for a success-

ful model-based approach ?

This paper reports on-going work which only provides
partial answer to those questions at this stage at which we
are interested by feedback from the community. Our paper is
structured as follows. First section 2 gives some more context
about our practical research setting in terms of framework and
case study. Then section 3, 4 and 5 respectively detail some
partial answers, discussed in the light of our experiment and of
the work reported by others. Section 6 concludes and presents
the next steps of our research roadmap.

II. METHOD AND TOOLS

In order to explore the above research questions we used
the LabNaf EAF which is a plugin of Sparx EA [13], [14].
We preferred it over large commercial tools like Abacus [15]
or Alphabet [16] for the following reasons:

• it provides a semantically consistent EA approach based
on a strong underlying meta-model. This address reason-
ing limitations due to the variety of available and possibly
overlapping frameworks (e.g. Archimate [17], TOGAF
[10]) and modelling languages (e.g. UML, BPMN).
• it also deals with existing standards and takes a merger
approach for ensuring consistency. The ISO42010 is used
for software and system architecture [3].

• it is builds on top of standard UML extension capabilities
such as stereotypes and proﬁles but also provides more
powerful tool capabilities for analysis (queries/reports)
and automation/integration (powershell) making it an
interesting R&D instruments.

Fig. 2. Model of the global EA process at level 1 (including architecture)

The current evolution process is itself captured by the tool
and described at three levels of reﬁnement. Figure 2 shows
the top level process. The visible enterprise description cap-
tures all the enterprise assets including processes, functions,
information, application and infrastructure. Figure 3 details the
typical viewpoints for capturing application and service level
architectures. Note the third step of Figure 2 relates to project
implementation phase and drives evolution through speciﬁc
projects. It involves a number of design exploration steps that
result in a decision about the solution and its impact. Once
implemented it will update the enterprise description. A key
aspect is to be able to make sure relevant/high quality
architecture information is taken into account in the earlier
steps of strategy deﬁnition.

Fig. 3. Model of the reﬁned architecture selection process at level 2

On the IT side, we assume the availability of typical
DEVOPS tool stack composed of continuous integration (e.g.
Jenkins), software quality analysis (e.g. sonarqube), runtime
monitoring (e.g. NAGIOS, ELK, Promotheus...). Those tools
can generated the information for updating the EA repository.
To explore our research questions we rely on a few available
evolution and migration cases studies already provided by the
LabNaf framework. A local deployment of the platform is
used to study the experiment on how to enrich the existing

information on the repository and analyse it using relevant
tools. We also looked into the literature to identify problems
and solutions reported by others w.r.t. our research questions.

III. RQ1 - ANALYSING APPLICATION AND SERVICE
EVOLUTION STRATEGIES USING EA MODEL

A systematic review of software architecture evolution
research [18] identiﬁed ﬁve key themes. Without explicitly
referring to EA, the mentioned themes can be supporting by
such an approach:

• quality consideration during software architecture de-
sign and architectural quality evaluation can be partly
supported by the domain models relating to enterprise
(see Figure 3). However those need to be augmented
with quality attributes such as provided by the SQUarE
standard [19].

• economic valuation: EA support strategic decision mak-
ing are typically carried out in monetary terms although
it also support goal deﬁnition and different ways to assess
their satisfaction.

• architectural knowledge management: the EA repository
has a key role as centralised enterprise knowledge man-
agement tool.

• modelling techniques: as discussed in Section 2.

Fig. 4. Traceability from goal level to component level

So far we have identiﬁed and experimented with some of

the following analysis capabilities:

• traceability
goal/process
as shown in Figure 4.

impact

and
from business
to application/service (and conversely)

analysis

• global ranking of component quality across the applica-

tion landscape

• identiﬁcation of risk related to low quality component

and critical business function

• comparison of alternative architectures roadmaps with
different plateaus (evolution milestones) to achieve the
same global transformation with different tradeoffs and
strategies (e.g. component replacement, component cor-
rection, component wrapping,...)

Technically the analysis could be carried out with off-the-
shelf functions such as trace, impact matrix and gap analysis
for the simpler ones and with query and reporting tools for
more complex ones such as comparison of alternatives.

IV. RQ2 - GATHERING USEFUL INFORMATION FROM
RUNNING APPLICATIONS AND SERVICES

Once the useful information is identiﬁed comes the question
how to efﬁciently gather it. At this point we did not prototype
any connector between continuous integration and operation
monitoring tooling mentioned in Section 1. Beyond the tech-
nical issues, there are a several interesting issues requiring
attention and already discussed in the literature [20], [21]. The
later provide an interesting general check list that can drive
our work:

1) When does the EA model have to be updated ?

In our context, this cover different triggers. First, model
should be updated when new release are deployed, for
sure. In this case, some plateau or to-be become part
of the as-is. A number of internal qualities are also
available at that time if they are extracted from the
continuous integration platform. Second, observing the
running system will also tell us about its externally visi-
ble qualities. Those may degrade after a new deployment
and can then be related to such an event but it could
also be related to environmental causes (e.g. growing
number of users or environment becoming hostile due
to cybersecurity attacks). In this case, a difﬁculty is to
ﬁgure out when it becomes relevant to report, it might be
a progressive trend with progressive service degradation
or an accumulation of punctual failures more or less
impacting the business level.

2) Where does integrated data originate from (manual

entry/automated update)?
As we target software quality data, from the previous
point two main sources can be identiﬁed: development
time and run-time. A good level of automation can be
expected using continuous integration platform for the
former and runtime monitoring for the later. However
manual collection is probably also required for speciﬁc
attributes such as usability. Information collected from
some manual channels such as helpdesk can also be
analysed and fed into the system in a semi-automated
way. More collaborative approach have also been deﬁned

based on a collaboration platform with capabilities to
monitor the actual information demand and to maintain
the EA model at runtime [22].

3) Which part of the EA model should be updated ?
the relevant part

In our case,
is easy to identify in
the visible enterprise vision relating to application and
services (see Figure 3). It needs to be extended to capture
the quality attributes identiﬁed in RQ1.

4) How is the quality of the resulting EA model governed,

e.g. how are duplicate data entries avoided?
This issue is not yet covered but discussed in RQ3.

V. RQ3 - KEY ISSUES FOR A SUCCESSFUL MODEL-BASED
APPROACH

Modelling semantics. As already discussed in Section 2,
high quality analysis has to rely on clear semantics of the EA
modelling which is often quite light (box-and-arrow semantics,
information captured in different models in an unrelated way,
dealing with different granularity levels,...). We addressed this
risk by selecting the LabNaf framework. At this point, the
approach is quite convincing although it means some choice
have been made and need to be learned, e.g. explicit restriction
on Archimate. Enforcement is also not always explicit but
requires to run validation tools.

Model quality. A key point in EA is to make sure the models
get updated upon changes and more generally provides a
process to ensure the quality of its data. As in code a notion of
technical debt can be deﬁned and tracked. Internal consistency
can be checked using validation rules against completeness,
freshness or even semantic compliance of the information. On
the other side, update procedure should be triggered upon all
detected changes at project milestones or even on the running
system (reﬂecting some environment evolution). When possi-
ble automated process can be used, e.g. some process can ﬁx
some well identiﬁed errors but more importantly they can be
used to feed the model with information such as design time
or runtime quality measurements on the software components.
Cost of model maintenance. A major risk is that the model
gets too costly to maintain over time especially when growing
in size and complexity. Such cost are directly related to quality
assurance and the use of automation can help in keeping it
under control. Another control lever is to adapt the level of
precision of the EA model in relation with the identiﬁed risks.
This also motivates the selection of an approach supporting
different reﬁnement levels although it can be more complex.

VI. CONCLUSION AND ROADMAP

In this paper, we highlighted our on-going research to
enrich EA models with information about the applications and
software components on various qualities in order to better
drive the global EA evolution process. We identiﬁed some
research questions covering the kind of analyse required, the
data collection process and the effectiveness of the approach.
We also deﬁned an experimental setting and some case studies
to support our work and performed a partial literature survey.

The next steps of our research is to implement automated
connectors and start gathering operations data. Going beyond
our limited cases, our aim is to build a partial EA model of
our research centre and to collect data about key services such
as email server, website, internal applications relying on our
existing monitoring and development infrastructure. Based on
this, we can analyse how our current evolution roadmap is
aligned with what we gather and analyse, especially related
to an number of legacy yet business critical applications. We
also plan to investigate more powerful data analysis tools.

ACKNOWLEDGMENT

Thanks to Alain De Preter for giving early access to the
LabNaf Enterprise Architecture tooling, helping in deployment
and feedback on this work.

REFERENCES

[1] S. Cook, H. Ji, and R. Harrison, “Software evolution and software

evolvability,” 12 2000.

[2] S. Ciraci and P. van den Broek, “Evolvability as a quality attribute of
software architectures,” Journal of Physics: Conference Series, 01 2006.
[3] ISO/IEC, “42010 - standard for systems and software engineering -
recommended practice for architectural description of software-intensive
systems,” https://www.iso.org/standard/50508.html, 2011.

[4] M. H. Valipour et al., “A brief survey of software architecture concepts
and service oriented architecture,” in 2nd IEEE Int.. Conf. on Computer
Science and Information Technology, 2009.

[5] Gartner, “IT Glossary - Enterprise Architecture,” https://www.gartner.

com, 2014.

[6] G. Engels and M. Assmann, “Service-oriented enterprise architectures:
Evolution of concepts and methods,” in 12th International IEEE Enter-
prise Distributed Object Computing Conference, Sep. 2008.

[7] S. Watts, “Enterprise Architecture Frameworks (EAF): The Complete

Beginner’s Guide,” The Business of IT Blog, 2018.

[8] S. Kotusev, “Critical questions in enterprise architecture research,” Int.

J. Enterp. Inf. Syst., vol. 13, no. 2, pp. 50–62, Apr. 2017.

[9] J. A. Zachman, The Zachman Framework for Enterprise Architecture:
Zachman

Primer for Enterprise Engineering and Manufacturing.
International, 2003.

[10] The Open Group, “TOGAF - The Open Group Architecture Framework

V9,” The Open Group, USA, 2009.

[11] OMG, “Uniﬁed modeling language,” http://www.omg.org/spec/UML,

1997.

[12] OMG, “BPMN: Business Process Model And Notation V2.0,” https:

//www.omg.org/spec/BPMN/2.0, 2011.

[13] A. D. Preter, “Labnaf, all-in-one strategy & architecture framework,”

http://www.labnaf.one, 2018.

[14] Sparx, “Entreprise Architect 14,” https://sparxsystems.com/products/ea,

2018.

[15] Avolution, “Abacus - architecture-based analysis, communication and
understanding of systems,” https://www.avolutionsoftware.com/abacus,
2001.

[16] SoftwareAG, “Alfabet, Enterprise Architecture Management,” www2.

softwareag.com/corporate/products/aris alfabet/eam, 2015.

[17] The Open Group, “Archimate 3.1 speciﬁcation,” http://pubs.opengroup.

org/architecture/archimate3-doc, 2017.

[18] H. P. Breivold, I. Crnkovic, and M. Larsson, “A systematic review
of software architecture evolution research,” Information and Software
Technology, vol. 54, no. 1, 2012.

[19] ISO, “System and Software Quality Requirements and Evaluation

(SQuaRE),” https://iso25000.com, 2011.

[20] T. Br¨uckmann, V. Gruhn, and M. Pfeiffer, “Towards real-time monitor-
ing and controlling of enterprise architectures using business software
control centers,” in Software Architecture, 2011.

[21] M. Farwick et al., “A meta-model for automated enterprise architecture

model maintenance,” 09 2012.

[22] S. Roth, M. Hauder, and F. Matthes, “Collaborative evolution of enter-

prise architecture models,” vol. 1079, 10 2013.

