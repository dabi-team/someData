2
2
0
2

l
u
J

9
1

]
E
S
.
s
c
[

1
v
5
6
0
9
0
.
7
0
2
2
:
v
i
X
r
a

AUTOMATED BLACK-BOX BOUNDARY VALUE DETECTION

A PREPRINT

Felix Dobslaw
Dept. of Computer and System Sciences
Mid Sweden University
Östersund
felix.dobslaw@miun.se

Robert Feldt, Francisco Gomes de Oliveira Neto
Dept. of Computer Science and Engineering
Chalmers, and the University of Gothenburg
Gothenburg
robert.feldt@chalmers.se, francisco.gomes@cse.gu.se

July 20, 2022

ABSTRACT

The input domain of software systems can typically be divided into sub-domains for which the
outputs are similar. To ensure high quality it is critical to test the software on the boundaries between
these sub-domains. Consequently, boundary value analysis and testing has been part of the toolbox of
software testers for long and is typically taught early to students. However, despite its many argued
beneﬁts, boundary value analysis for a given speciﬁcation or piece of software is typically described
in abstract terms which allow for variation in how testers apply it.
Here we propose an automated, black-box boundary value detection method to support software
testers in systematic boundary value analysis with consistent results. The method builds on a metric
to quantify the level of boundariness of test inputs: the program derivative. By coupling it with search
algorithms we ﬁnd and rank pairs of inputs as good boundary candidates, i.e. inputs close together
but with outputs far apart. We implement our AutoBVA approach and evaluate it on a curated dataset
of example programs. Our results indicate that even with a simple and generic program derivative
variant in combination with broad sampling over the input space, interesting boundary candidates can
be identiﬁed.

Keywords information theory · boundary value analysis · boundary value exploration · program derivative

1

Introduction

Ensuring the quality of software is critical and while much progress has been made to improve formal veriﬁcation
approaches, testing is still the de facto method and a key part of modern software development. A central problem in
software testing is how to meaningfully and efﬁciently cover an input space that is typically very large. A fundamental,
simplifying assumption, is that even for such large input spaces there are subsets of inputs, called partitions or sub-
domains, that the software will handle in the same or similar way [1, 2, 3, 4]. Thus, if we can identify such partitions
we only need to select a few inputs from each partition to test and ensure they are correctly handled.

While many different approaches to partition testing have been proposed [1, 2, 5, 6] one that comes naturally to many
testers is boundary value analysis (BVA) and testing [7, 8, 9]. It is based on the intuition that developers are more likely
to get things wrong around the boundaries between input partitions, i.e. where there should be a change in how the
input is processed and in the output that is produced [9]. By analysing a speciﬁcation, testers can identify partitions and
boundaries between them. They should then select test inputs on either side of these boundaries and thus, ideally, verify
both correct behavior in the partitions as well as that the boundary between them is in the expected, correct place [9, 10].
But note that identifying the boundaries, and thus partitions, is the critical step; a tester can then decide whether to
focus only on them or to also sample some non-boundary inputs “inside” of each partition.

A problem with partition testing, in general, and boundary value analysis (BVA), in particular, is that there is no clear
and objective method for how to identify partitions nor the boundaries between them. Already Myers [7] pointed out the
difﬁculty of presenting a “cookbook” method and that testers need to be creative and adapt to the software being tested.

 
 
 
 
 
 
Automated Black-Box Boundary Value Detection

A PREPRINT

Later work describe BVA and partition testing as relying on either a partition model [10], categories/classiﬁcations
of inputs or environment conditions [5, 6], constraints [2, 5], or “checkpoints” [11] that are all to be derived from the
speciﬁcation. But they don’t guide this derivation step in detail. Several authors have also pointed to the fact that
existing methods don’t give enough support to testers [4] and that BVA is, at its core, a manual and creative process
that cannot be automated [6]. More recent work can be seen as overcoming the problem by proposing equivalence
partitioning and boundary-guided testing from formal models [12]. However, this assumes that such models are (readily)
available or can be derived, and then maintained, without major costs.

One alternative is to view and then provide tooling for using BVA as a white-box testing technique. Pandita et al [13]
use instrumentation of control ﬂow expressions and dynamic, symbolic execution to generate test cases that increases
boundary value coverage [14]. However, it is not clear how such boundaries, internal to the code, relate to the boundaries
that traditional, black-box BVA would ﬁnd. Furthermore, it requires instrumentation and advanced tooling which is not
always available and might be costly. It should be noted that white-box testing is limited to situations where source
code is available; black-box testing approaches do not have this limitation.

Empirical studies on the effectiveness of partition and boundary value testing do not provide a clear picture. While
early work claimed that random testing was as or more effective [3], they were later countered by studies showing
clear beneﬁts to BVA [15, 11]. A more recent overview of the debate also provided theoretical results on effectiveness
and discussed the scalability of random testing in relation to partition testing methods [16]. Regardless of the relative
beneﬁts of the respective techniques, we argue that improving partition testing and boundary value analysis has both
practical and scientiﬁc value; judging their value will ultimately depend on how applicable and automatic they can be
made.

Here we address the core problem of how to automate black-box boundary value analysis. We build on our recent
work that proposed a family of metrics to quantify the boundariness of software inputs [17] and combine it with
search and optimization algorithms in order to automatically detect promising boundary candidates. These can then
be (interactively) presented to testers and developers to help them explore meaningful boundary values and create
corresponding test cases [18]. Our search-based, black-box, and automated boundary value identiﬁcation method does
not require manual analysis of a speciﬁcation nor the derivation of any intermediate models. In fact, it can be used even
when no speciﬁcation nor models are available. Since it is based on generic metrics of boundariness it can be applied
even for software with non-numeric, structured, and complex inputs and/or outputs. We implement our AutoBVA
method as a tool and evaluate it on four different software under test (SUT).

The main contributions of this paper are:

• A generic method and implementation of automated boundary value analysis (AutoBVA), and
• The proposal and use of a simple and very fast variant of the program derivative [17] for efﬁcient search, and
• The comparison of simple but broadly sampling and two heuristic local-search algorithm within our tool, and
• An empirical evaluation of AutoBVA on four SUTs to its capabilities in identifying boundary candidates.

Our results show that the proposed method can be effective even when using a simple and fast program derivative
together with simple algorithms that just ensure a broad sampling of the input space. Furthermore, for some of the
investigated programs, more sophisticated meta-heuristic search algorithm can provide additional and complementary
value.

The rest of this paper is organized as follows. After providing a more detailed background and overview of related
work in Section 2, we present AutoBVA in Section 3. The empirical evaluation is detailed in Section 4 followed by
the results in Section 5. The results are discussed in Section 6.1 and the paper concludes in Section 7. Appendix A
and B contain details of two screening studies that supported AutoBVA meta-parameter choices for its detection and
summarization phases, respectively.

2 Related Work

In the following, we provide a brief background to boundary value analysis and the related partition testing concepts of
domain testing and equivalence partitioning.

White and Cohen [8] proposed a domain testing strategy that focuses on identifying boundaries between different
(sub-)domains of the input space and to ensure that boundary conditions are satisﬁed. As summarized by Clarke et
al [9]: “Domain testing exploits the often observed fact that points near the boundary of a domain are most sensitive to
domain errors. The method proposes the selection of test data on and slightly off the domain boundary of each path
to be tested.”. This is clearly connected to the testing method typically called boundary value analysis (BVA), ﬁrst

2

Automated Black-Box Boundary Value Detection

A PREPRINT

described more informally by Myers in 1979 but later also included in standards for software testing [7, 15, 19, 10].
Jeng and Weyuker [20] even describe domain testing as a sophisticated version of boundary value testing.

While White and Cohen [8] explicitly said their goal was to “replace the intuitive principles behind current testing
procedures by a methodology based on a formal treatment of the program testing problem” this has not lead to automated
tools and a BVA is typically performed manually by human testers. Worth noting is also that while boundary value
analysis is typically described as a black-box method [7, 15, 10], requiring a speciﬁcation, the White and Cohen papers
are less clear on this and their domain testing strategy could also be applied based on the control ﬂow conditions of an
actual implementation.

The original domain testing paper [8] made a number of simplifying assumptions such as the boundary being linear,
being deﬁned by “simple predicates”, and that test inputs are continuous rather than discrete. While none of these
limitations should be seen as fundamental they do leave a practitioner in a difﬁcult position since it is not explicit what
the method really entails when some or all of these assumptions are not fulﬁlled. Even though later formulations of
BVA as a black-box method [10] avoid these assumptions, they, more fundamentally, do not give concrete guidance to
testers on how to identify boundaries or the partitions they deﬁne.

As one example, the BCS standard [10] states that “(BVA) ...uses a model of the component that partitions the input
and output values of the component into a number of ordered sets with identiﬁable boundaries.” and that “a partition’s
boundaries are normally deﬁned by the values of the boundaries between partitions, however where partitions are
disjoint the minimum and maximum values in the range which makes up the partition are used” but do not give guidance
on where to ﬁnd or how to create such a partition model1 if none is already at hand. This problem was clear already
from Myers original description of BVA [7] which stated “It is difﬁcult to present a cookbook for boundary value
analysis, since it requires a degree of creativity and a certain amount of specialization to the problem at hand”.

Later efforts to formalize BVA ideas have not addressed this. For example, Richardson and Clarke’s [2] partition analysis
method makes a clear difference between partitions derived from the speciﬁcation versus from the implementation and
proposes to compare them but relies on the availability of a formal speciﬁcation and do not detail how partitions can be
derived from it. Jeng and Weyuker [20] proposed a simpliﬁed and generalized domain testing strategy with the explicit
goal of automation but then only informally discuss how automation based on white-box analysis of path conditions
could be done.

A very different approach is Pandita et al [13] that presents a white-box automated testing method to increase the
Boundary Value Coverage (BVC) metric (originally presented by [14]). The core idea is to instrument the software
under test with additional control ﬂow expressions to detect values on either side of existing control ﬂow expressions.
An existing test generation technique to achieve branch coverage, in [13] the dynamic symbolic execution test generator
Pex is used, can then be used to ﬁnd inputs on either side of a boundary. The experimental results were encouraging in
that BVC could be increased with 23% on average and also lead to increases (11% on average) in the fault-detection
capability of the generated test inputs.

There has been several studies that empirically evaluate BVA. While an early empirical study by Hamlet and Taylor [3]
found that random testing was more effective, it’s results were challenged in later work [15, 11]. Reid [15] investigated
three testing techniques on a real-world software project and found that BVA was more effective at ﬁnding faults both
than equivalence partitioning and random testing. Yin et al [11] compared a method based on checkpoints, manually
encoding qualitatively different aspects of the input space, combined with antirandom testing 2 to different forms of
random testing and found the former to be more effective. The checkpoint encoding can be seen as a manually derived
model of important properties of the inputs space and thus, indirectly, deﬁnes, potentially overlapping, partitions.

Even recent work on automating partition and boundary value testing has either been based on manual analysis or
required a speciﬁcation/model to be available. Hubner et al [12] recently proposed a novel equivalence class partitioning
method based on formal models expressed in SysML. An SMT solver is used to sample test inputs either inside or
on the border of identiﬁed equivalence classes. They compared multiple variants of their proposed technique with
conventional random testing. The one that sampled 50% of test inputs within and 50% on the boundaries between the
equivalence partitions performed best as measured by mutation score.

Related work on modeling the input space has also been done as a means to improve combinatorial testing. Boraznajy
et al [21] proposed to divide the problem into two phases where the ﬁrst models the input structure while the latter
models the input parameters. They propose that ideas from partition testing can be used for the latter stage. However,
for analysing the input structure they propose a manual process that can support only two types of input structures,

1An annex to the standard does provide an example of how to ﬁnd partitions and identify boundaries but the speciﬁcation used in

the example explicitly states the boundaries so the identiﬁcation task is trivial

2A form of diversity-driven test generation that also relates clearly to what is recently more commonly referred to as adaptive

random testing (ART).

3

Automated Black-Box Boundary Value Detection

A PREPRINT

Figure 1: AutoBVA framework for automated boundary value analysis.

either ﬂat (e.g. for command line parameters that have no obvious relation) or graph-structured (e.g. for XML ﬁles for
which the tree structure can be exploited).

We have previously proposed a family of metrics to quantify the boundariness of pairs of software inputs [17]. This is
a generalization of the classical deﬁnition of functional derivatives in mathematics, that we call program derivatives.
Instead of using a standard subtraction (“-”) operator to measure the distance between inputs and outputs we leverage
general, information theoretical results on how to quantify distance and diversity. We have previously used such
measures for increasing and evaluating the beneﬁts of test diversity [22, 23]. In a recent study, we used the program
derivatives to explore input spaces and visualise boundaries for testers and developers [18]. Here, we automate this
approach by coupling it to search and optimization algorithms.

In summary, early results on partition and boundary value analysis/testing require a speciﬁcation and do not provide
detailed advice or any automated method to ﬁnd boundary values. One automated method has been proposed but instead
requires a model of the system under test. One other method can automatically increase a coverage metric for boundary
values but is white-box and requires both instrumentation of the software under test as well as advanced test generation
based on symbolic execution. In contrast to existing research we propose an automated, black-box method to identify
boundary candidates that is simple and straightforward to implement.

3 Automated Boundary Value Analysis

We propose to automate boundary value analysis by a fully automated detection method which outputs a set of input/
output pairs that are then summarized and presented to testers. An overview of our proposed approach can be seen
in Figure 1. The two main parts are detection (on the left), which produces a list of promising boundary candidates,
which are then summarized and presented to the tester (on the right). The boundary value detection method is based on
two key elements: (1) search to explore (both globally and locally) the input domain coupled with (2) the program
derivative to quantify the boundariness of input pairs. While exploration acts as a generator of new boundary candidate
input pairs the program derivative acts as a ﬁlter and selects only promising candidate pairs for further processing. For
summarization, an archive is updated with the new pairs and only the unique and good boundary candidates are kept.
The ﬁnal list of promising candidates, in the archive, is then summarized and presented to the tester who can select the
most interesting and meaningful ones and turn them into test cases. For the summarization step we here propose to use
clustering to avoid showing multiple candidates that are very similar to each other.

Below we describe the three main parts of our approach: selection, search/exploration, and summarization. We start
with selection, since that is the critical step, and use a simple SUT, bytecount, to exemplify each step.

3.1 Selection: Program Derivative

We argue that the key problem in boundary value analysis is how to judge what is a boundary and what is not. If we
could quantify how close to a boundary a given input is we could then use a multitude of different methods to ﬁnd many
such inputs. Together such a set could be used to indicate where the boundaries of the software are.

4

Automated Black-Box Boundary Value Detection

A PREPRINT

In previous work [17], we proposed the program derivative for doing this type of boundariness quantiﬁcation. The
idea is based on a generalization of the classic deﬁnition of the derivative of a function in (mathematical) analysis. In
analysis, the deﬁnition is typically expressed in terms of one point, x, and a delta value, h, which together deﬁne a
second point after summation. The derivative is then the limit as the delta value approaches zero:

lim
h→0

f (x + h) − f (x)
(x + h) − x

= lim
h→0

f (x + h) − f (x)
h

We can see that this measures the sensitivity to change of the function given a change in the input value. A large
(absolute value of a) derivative indicates that the function changes a lot even for a very small change of the input. If the
function here instead was the software under test, and the small change in inputs would cross a boundary it is reasonable
that the output would also change more than if the change did not cross a boundary. We could then use this program
derivative to screen for input pairs that are good candidates to cross boundaries of a program.

Key to generalize from mathematical functions to programs is to realize that programs typically have many more than
one input and that their types can be very different from numbers. Also, there can be many outputs and their types might
differ from the types of the inputs. Instead of simply using subtraction (“−”) both in the numerator and denominator
we need two distance functions, one for the outputs (do) and one for the inputs (di). Also, rather than ﬁnding the
closest input, to actually calculate the derivative of a single input, for our purposes here we only need to quantify the
boundariness of any two, individual inputs. We thus deﬁne the program difference quotient (PDQ) for program P and
inputs3 a and b as [17]:

P DQdo,di(a, b) =

do(P (a), P (b))
di(a, b)

where P (x) denotes the output of the program for input x.

Since the PDQ is parameterized on the input and output distance functions this deﬁnes not a single but a whole family
of different measures. A tester can choose distance functions so as to capture meaningful differences in outputs and/or
inputs. In the original program derivative paper [17], the authors argued for compression-based distance functions as a
good and general choice. However, a downside with these are that they can be relatively costly to calculate which could
be a hindrance when used in the context of a search-based loop which will potentially require a very large number of
distance calculations. Also, compression-based distances using mainstream string compressors such as zlib or bzip2
might not work well for short strings, as commonly seen in testing.

In this work, we thus focus on using one of the least costly output distance functions one can think of: strlendist the
difference in length of outputs when printed as strings. Obviously, this distance function works regardless of the type of
the outputs involved. A downside is that it is coarse-grained and will not detect smaller differences in outputs of the
same length. Still, if a simple and fast distance function can sufﬁce to identify boundaries this can be a good baseline for
further investigation. Also, our framework is ﬂexible and can also use multiple distance functions for different purposes
in its different components. For example, one could use strlendist during search and exploration while using also
more ﬁne-grained NCD-based output distance when updating the archive or during summarization (see Figure 1).

For the input distance function this will typically vary depending on the software under test. If inputs can be represented
as numbers or a vector of numbers we can use simple substraction or a vector distance functions such as Euclidean
distance. For more complex input types one can use string-based distance functions like Normalised Compression
Distance (NCD) [22, 23, 17] or even simpler ones like Jaccard distance [24].

3.1.1 Example: program derivative for bytecount

We will exemplify our framework with the simple bytecount function that is one of the most copied snippets of code
on Stack Overﬂow but also known to contain a bug [25, 26]. It translates an input number of bytes into a human-readable
string with the appropriate unit, e.g. “MB” for megabytes etc. For example, for an input of 2099 it returns the output
”2.1 kB“ and for the input 9950001 it returns ”10.0 MB“.

Table 1 shows a set of manually selected examples of boundary candidate pairs, using a single input distance function
(subtraction) but two different output distances: strlendist and Jaccard(1), the Jaccard distance based on 1-
grams [24]. The Jaccard distance can be used as an approximation of compression distances but is fast to calculate and

3We here assume each input corresponds to a single argument of the program but by using distance functions that can handle also

multiple arguments the formulation becomes fully general.

5

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 1: Example of six boundary candidates for the bytecount SUT with their corresponding Program Difference
Quotient (PDQ) values for two different output distances.

Row

Input 1

Input 2 Output 1 Output 2

do1 (strlendist)

do2 (Jaccard(1))

di

P DQ1

P DQ2

1
2
3
4
5
6

9
999949999
99949
99949
99951
99948

10
999950000
99950
99951
99952
99949

9B
999.9 MB
99.9 kB
99.9 kB
100.0 kB
99.9 kB

10B
1.0 GB
100.0 kB
100.0 kB
100.0 kB
99.9 kB

1
2
1
1
0
0

0.75
0.63
0.43
0.43
0.0
0.0

1
1
1
2
1
1

1
2
1
0.50
0.0
0.0

0.75
0.63
0.43
0.21
0.0
0.0

applicable also for short strings. Correspondingly, in our example table, there are two different PDQ values and we
have sorted in descending order based on the P DQ2, i.e. that uses the Jaccard(1) output distance function.

Starting from the bottom of the table, on row 6, the bytecount output is the same for the input pair (99948, 99949).
This leads to PDQ values of 0.0 regardless of the output distance used. The PDQ values are zero also for the example
on row 5; even though the output has changed compared to row 6 they are still the same within the pair. We are thus in
a different partition, since the outputs differ from the ones on row 6, but we are not crossing any clear boundary and the
PDQ values are still zero.

The example on row 4 does show a potential boundary crossing. Even though the input distance is now higher, at 2, the
outputs differ so both PDQ values are non-zero. This example pair can be improved further though, by subtracting 1
from 99951 to get the pair (99949, 99950) shown on row 3. Since the denominator in the PDQ calculation is smaller,
the PDQ value is higher and we consider it to be a better boundary candidate. In fact this is the input pair with the
highest PDQ value of the ones that has these two outputs; it can thus be considered the optimal input pair to show the
“99.9 kB” to “100.0 kB” boundary.

Finally, the examples on rows 2 and 1 show input pairs for which the two PDQ measures used differ in their ranking.
While we can agree that both of these input pairs identify boundaries it is possible that different testers might have
different preferences on which one is the more preferred. Given that P DQ1, using the strlendist output distance, is
so simple and quick to compute we will use that in the experiments of this study. Future work can explore trade-offs in
the choice of distance functions as well as how to combine them. Note that regardless of how the input pairs have been
found they can be sorted and selected using different output distance functions when presented.

3.2 Exploration: Generation of Candidate Pairs

While the program derivative can help evaluate whether an input pair is a good candidate to detect a boundary, there is
a very large number of possible such pairs to consider. Thus, we need ways to explore the input space and propose
good candidate pairs, i.e. that have high program derivative values. A natural way to approach this is as a search-based
software engineering problem [27, 28, 29], with the program derivative as the goal/ﬁtness function and a search
algorithm that tries to optimize for higher program derivative values.

However, to identify the boundaries it is not enough to simply ﬁnd and return one candidate pair. Most software will
have multiple and different boundaries in their input domain. Furthermore, boundaries are typically stretched out over
(consecutive) sets of inputs. The search and exploration procedure we chose should thus output a set of input pairs
that are, ideally, spread out over the input domain (to ﬁnd multiple boundaries) as well as over each boundary (to help
testers identify where it is).

An additional concern when using search-based approaches is the shape of the ﬁtness landscape, i.e. how the ﬁtness
value changes over the space being searched [30, 31]. Many search and optimisation approaches assume or at least
beneﬁt from a smooth landscape, i.e. where small steps in the space lead to only a small change in the ﬁtness value. It
is not clear that we can assume this to be the case for our problem. The program derivative might be very high right
at the boundary while showing very little variation when inside the partitions on either side of the boundary. Worst
case, this could be a kind of needle-in-a-haystack ﬁtness landscape [31] where there is little to no indication within a
sub-domain to guide a search towards its edges, where the boundary is.

Given these differences compared to how search and optimisation has typically been applied in software testing we
formulate our approach in a generic sense. We can then instantiate it using different search procedures and empirically
study which ones are more or less effective. However, given that we are searching for a pair of two inputs we separate
the search component into two steps: a global and a local exploration/search step.

6

Automated Black-Box Boundary Value Detection

A PREPRINT

Algorithm 1 Automated Boundary Detection - AutoBD-2step

Input: Software under test SU T , Boundariness quantiﬁer Q, Sampling strategy SS, Boundary search BS
Output: Boundary candidates BC
1: BC = ∅
2: while stop criterion not reached do
3:
4:
5: BC = BC ∪ {c|c ∈ P BC ∧ Q.evaluate(c) > threshold(BC)}
6: end while
7: return BC

input = SS.sample(SU T, BC) # globally sample a starting point
P BC = BS.search(SU T, Q, input) # locally explore and detect potential candidate(s)

Algorithm 1 outlines this generic, 2-step method for automated boundary detection. Given a software under test (SU T ),
it returns a set of boundary candidate pairs (BC). It has three additional parameters: a way to quantify the boundariness
of pairs of inputs (Q), a (global) sampling strategy (SS) to propose starting points for local exploration, and a (local)
boundary search (BS) strategy. These exploration strategies capture two different types of sampling/search procedures.
The global one explores the input space of the SUT as a whole by sampling different input points (line 3). The local
strategy will then search from the starting point (line 4) and try to identify new potential boundary candidate pairs
according to the boundariness quantiﬁer.

Each of the potential new candidates (in P BC) are then evaluated and their boundariness compared to a threshold (line
5) and added to the ﬁnal set (BC) returned. The threshold function simply captures the fact that we might not use a
ﬁxed threshold but rather can allow more complex updating schemes where the threshold is based on the candidates that
have already been found.

We have split the search and exploration procedure into two for clarity and since they have two slightly different goals.
The goal of the global sampling strategy is to propose inputs that have not been previously considered (to avoid wasting
effort) and, ideally, that are likely to be close to a boundary. The goal for the local search strategy is rather to ensure that
only inputs that have a small distance to the starting input are considered. This division of labor can thus increase the
likelihood that we consider only input pairs that are close to each other, i.e. neighbors. Since this input distance value is
in the denominator of the program derivative the use of the local search strategy thus helps ensure the denominator is
small, which should lead to higher derivative values.

Since the scale of the derivative values that the algorithm is likely to encounter cannot always be known (it will depend
on the speciﬁc distance functions used) it is natural to decide the threshold (Algorithm 1, line 5) dynamically. For
example, the threshold could be taken as some percentile (say, 90%) of the boundariness values of the candidate set
saved so far. Alternatively, even more elaborate boundariness testing and candidate set update procedures can be used.
For example, rather than simply adding to the current set whenever a sufﬁciently good boundary value is found, we
could save a top-list of the highest boundariness values found so far. The update would then be generalized so that it
can also delete previously added candidate pairs that are no longer promising.

As a simple example, consider a SU T that takes a single, integer number as input. One variant of our general framework
could be to choose random sampling of integers as the global exploration strategy and use the addition and subtraction
of the values one (1) or two (2) in the local search step. If the global strategy sampled the integer 10, the boundariness
of four different candidate pairs would be probed in the local search: (10, 11), (10, 9), (10, 12), and (10, 8). If the
SU T in question was actually the above mentioned bytecount PBC would then return the candidate pair (10, 9) since
it has the highest PDQ value.

While the beneﬁts of creating candidate input pairs in two steps may not be obvious for this particular example it is easy
to imagine the local search step being more important when the inputs are of a complex and structured datatype, such as
for example an XML tree. If the search was done with a single, global exploration step it would be relatively unlikely
that the two inputs of a candidate pair would have a small distance. By doing this in two steps, we could ﬁrst sample
one XML tree and then explore its neighborhood in the input space by small mutations to it. However, for future work,
we don’t rule out exploring other ways to structure the search/exploration step; a single step that searches directly for a
pair with high program derivative is important future work.

In the experiments of this paper we will use one and the same (global) sampling strategy to sample starting points but
compare two different boundary search algorithms, namely Local Neighbor Sampling and Boundary Crossing Search.
Below we describe these different components in more detail.

7

Automated Black-Box Boundary Value Detection

A PREPRINT

3.2.1 Global Sampling

All inputs in the SUTs investigated in this paper are numbers. While we could use uniform sampling in valid ranges to
globally sample (Algorithm 1, line 3) such numbers we argue that in general it is better to sample uniformly over the
bits, i.e. bituniform sampling, making up the numbers; since there are many more large numbers, standard uniform
sampling would otherwise tend to favor larger numbers.

For a broad exploratory sampling we introduce a complementing technique that we call Compatible Type Sampling
(CTS), i.e. the argument-wise sampling based on compatible types per argument. An example of compatible types are
all integers types with speciﬁc bit size. For instance, in the Julia programming language we use in our experiments,
booleans (Bool), 8-bit (Int8) and 32-bit integers (Int32) types are compatible because they all are sub-types of Integer.

More details and justiﬁcation for the global sampling strategy we use here, bituniform sampling combined with CTS,
can be found in Appendix A. We do note that in general, more advanced test input generation strategies [32] can be used,
and adapting them to the SUT and its arguments will likely be important when further generalizing our framework.

3.2.2 Local Neighbor Sampling (LNS)

The simpler of the implementations of the here introduced local search strategies for Algorithm 1 (line 4), Local
Neighborhood Sampling (LNS), is presented as Algorithm 2. The basic idea with LNS is to structurally sample
neighboring inputs, i.e. inputs close to a given starting point i, to form potential candidate pairs including i. For that,
the algorithm processes mutations over all individual arguments (line 3) considering all provided mutation operators
mos (line 4). For integer inputs, the mutation operators are basic subtraction and addition. Outputs are produces for the
starting point (line 2) and each neighbor (line 6), to form the candidate pairs (line 7). Without ﬁltering, they are all
added to the set of potential boundary candidates (line 8), returned by the algorithm (line 11). LNS is used as a trivial
baseline implementation to better understand what is possible using AutoBD without sophisticated search.

Algorithm 2 search – Local Neighbor Sampling (LNS)

Input: Software under test SU T , mutation operators mos, Starting Point i
Output: potential boundary candidates P BC
1: P BC = ∅
2: o = SU T.execute(i)
3: for a ∈ arguments(SU T ) do
for mo ∈ mos[a] do
4:
5:
6:
7:
8:
9:
10: end for
11: return P BC

n = mo.apply(i, a)
on = SU T.execute(n)
c = (cid:104)i, o, n, on(cid:105)
P BC = P BC ∪ {c}

end for

3.2.3 Boundary Crossing Search (BCS)

Boundary Crossing Search (BCS) is the second local search method we introduce for comparison (see Algorithm 3).
It is a heuristic algorithm that uses a boundariness quantiﬁer Q, in our experiments the program derivative, to seek a
single potential boundary candidate that locally stands out in comparison to starting point i. For a random direction
(argument a in line 1 and mutation operator mo in line 2) a neighboring input inext gets mutated (line 3) and outputs for
both inputs produced (line 4) to deﬁne the initial candidate (line 5) for which the boundariness gets calculated (line 6).

Lines 7-12 describe the constraints and conditions that must hold true for a resulting boundary candidate c to locally
stand out. This search can be implemented in a variety of ways. We implemented a binary search that ﬁrst identiﬁes the
existence of a boundary crossing by taking larger and larger steps and calculating the difference ∆c to ﬁnd an input for
the state in which the boundariness is greater than ∆c, and thereby guaranteeing the necessary condition in line 124.
Once that is achieved, the algorithm squeezes that boundary to obtain c which is the nearest point to cinit that ensures
there is a notable local difference in neighboring inputs, and by that guaranteeing the neighboring constraint in line 8.

4A stop criterion returns the original point in case no difference could be picked up.

8

Automated Black-Box Boundary Value Detection

A PREPRINT

Algorithm 3 search – Boundary Crossing Search (BCS)

Input: Software under test SU T , Boundariness quantiﬁer Q, mutation operators mos, Starting Point i
Output: potential boundary candidates P BC
1: a = rand(arguments(SU T )) # select random argument
2: mo = rand(mos[a]) # select random mutation operator for argument
3: inext = mo.apply(i, a) # mutate input a ﬁrst time in single dimension
4: o = SU T.execute(i), onext = SU T.execute(inext) # produce outputs
5: cinit = (cid:104)i, o, inext, onext(cid:105) # instantiate initial candidate
6: ∆init = Q.evaluate(cinit) # calculate candidate distance
7: c = (cid:104)i1, o1, i2, o2(cid:105), with i1 obtained by a ﬁnite number of chained mutations mo of argument a over i, and
8:
9:
10:
11:
12:
13: return {c}

i2 = mo.apply(i1, a), and
o1 = SU T.execute(i1), and
o2 = SU T.execute(i2), and
∆c = Q.evaluate(c), and
∆c > ∆init or cinit = c

3.3 Summarization: Validity-Value Similarity Clustering

The boundary candidate set resulting from AutoBD-2step (Algorithm 1) can be extensive. However, humans information
processing is limited and more fundamentally many of the boundary pairs found can be similar to each other and
represent the same or a very similar boundary. The summary method(s) we choose should thus not only limit the
number of candidates presented, those candidates also need to be different and represent different groups of behaviors
and boundaries over the input space.

Furthermore, the goals for the summary step will likely differ depending on what the boundary candidates are to be used
for; comparing a speciﬁcation to actual boundaries in an implementation is a different use case than adding boundary
test cases to increase coverage. Thus we cannot provide one general solution for summarization and future work will
have to inspect different methods to cluster, select, prioritize but also visualise the boundary candidates.

In the following, we propose one particular, but very general, summarization method. Our hope is that it can show
several building blocks needed when creating summarization methods for speciﬁc use cases. But also that it can act
as a general, fallback method that can provide value regardless of the use case. This is also the method we use in the
experimental evaluation of the paper. We consider a general, black-box situation with no speciﬁcation available. Thus,
we only want to use information about the boundary candidates themselves. The general idea is to identify meaningful
clusters of similar candidates and then sample only one or a few representative candidates per cluster and present to the
tester.

For instance, Table 2 contains a subset of (nine) candidate pairs found by our method for the bytecount example
introduced above. Different features can be considered when grouping and prioritizing. We see that candidates differ at
least in terms of the type of their output, whether the outputs are valid return values or exceptions, as well as in the
actual values of inputs and/or outputs themselves. For example, both outputs for the candidate on row 2 are strings and
are considered normal, valid return values. On the other hand, for row 9 both outputs are (Julia) exceptions indicating
that a string of length 6 (”kM GT P E”) has been accessed at (illegal) positions 9 and 10, respectively.

We argue that the highest level boundary in boundary value analysis is that between valid and invalid output values.
Any time there is an exception thrown when the SU T is executed we consider the output to be invalid, if not the output
is valid5. Since we consider pairs of inputs there are two outputs per candidate that can be grouped in three, what we
call, validity groups:

• two valid outputs in the boundary pair (VV),
• one valid output and one invalid (error) output (VE), and
• two invalid (error) outputs (EE).

We use validity as the top-level variation dimension and produce individual summaries for each of these three groups.
In Table 2, the validity group is indicated in the rightmost column (VV for lines 1-7, VE in line 8, and EE in line 9).

5Note that in the implementation we have to clearly differentiate between the situation when an exception was thrown during
execution from the one where the function itself returns a value that is an exception. Otherwise, our framework could not be used for
functions that manipulate exceptions (without raising any exceptions while doing so).

9

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 2: A summary of boundary candidates found for bytecount by our method. Rows 1-7 are for the valid-valid (VV)
validity group of type String-String, row 8 for the valid-error (VE) group of type String/BoundsErrror, and row 9 for
error-error (EE) group of type BoundsError/BoundsError. The candidates of rows 1-7 are the shortest candidate pairs of
each identiﬁed cluster.

Row

Input 1

Input 2

Out 1

Out 2

Validity

1
2
3
4
5
6
7

8

9

false
9
-10
999949
99949999999999999
9950000000000001999
-1000000000000000000000000000000

true
9B
-9
999950
99950000000000000
9950000000000002000
-999999999999999999999999999999

falseB
10
-10B
999.9 kB
99.9 PB
9.9 EB
-1000000000000000000000000000000B

trueB
10B
-9B
1.0 MB
100.0 PB
10.0 EB
-999999999999999999999999999999B

999999999999994822656

999999999999994822657

1000.0 EB

BoundsError("kMGTPE", 7)

999999999999990520104160854016

999999999999990520104160854017

BoundsError("kMGTPE", 9)

BoundsError("kMGTPE", 10)

VV
VV
VV
VV
VV
VV
VV

VE

EE

Within each validity group we can consider other characteristics to further categorize candidates. For example, we
could use the type of the two outputs to create further (sub-)groups. This is logical since it is not clear that it is always
meaningful to compare the similarity of values of different types. However, for many SUTs and their validity groups
the types might not provide further differentiation, since they are often the same.

In the ﬁnal step we propose to cluster based on the similarity of features within the identiﬁed sub-groups. A type-speciﬁc
similarity function can be used or we can simply use a general, string-based distance function after converting the
values to strings. After calculating the pair-wise similarities (distances) we can create a distance matrix per sub-group.
This can then be used with any clustering method (in the experiments in this paper we used k-means clustering [33]).
From each cluster then we select one representative (or short) boundary pair to present to the tester. The distance matrix
can also be used with dimensionality reduction methods to visualise the validity-type sub-group. We thus call our
summarization method Validity-Value Similarity Clustering.

4 Empirical Evaluation

For evaluation, we run our framework using two different BVE search strategies (independent variable) on four different
types of SUTs (control variables) and study the sets of boundary candidates returned, in detail. Our SUTs are program
functions. Our focus is on evaluating to what degree the framework can ﬁnd many, diverse, and high-quality candidate
boundary pairs. Speciﬁcally, we address the following research questions:

• RQ1: Can large quantities of boundary candidates be identiﬁed? How do the different exploration strategies

compare against one another in terms of identifying unique boundary candidates?

• RQ2: Can the method robustly identify boundary candidates that cover a diverse range of behaviors?
• RQ3: To what extent can AutoBVA reveal relevant behavior or potential faults?

Through RQ1 we try to understand to what extent AutoBVA can pick up potential boundary candidates (PBC),
comparing two local search strategies. We compare both the overall quantities and quantities of uniquely identiﬁed
PBCs using a basic boundary quantiﬁer. Uniqueness here is measured in relation to the set of all PBCs for a SUT over
all experiments irrespective of the local search applied.

Through RQ2, we try to understand how well AutoBVA covers the space of possible boundaries between equivalence
partitions in the input space of varying behavior. For a given arbitrary SUT, we argue that there is no one-size-ﬁts-all
approach to extracting/generating “correct” equivalence partitions; many partitions and, thus, boundaries exist and
which ones a tester consider might depend on the particular speciﬁcation, the details with which it has been written,
the interests and experience of the tester etc. Therefore, we use our summarization method (Validity-Value Similarity
Clustering as described in Section 3.3) to group similar PBCs within each validity group (VV, VE and EE). We
answer RQ2 by comparing how the PBCs found by each exploration method cover those different clusters. Comparing
the coverage of these clusters allows us to interpret the behaviour triggered by the set of PBCs. For instance, even
though two boundary candidates identiﬁed for the Date constructor SUT, bc1 = (28-02-2021, 29-02-2021) and bc2 =
(28-02-2022, 29-02-2022), are different they do cover the same valid-error boundary6. It is thus not clear that ﬁnding
them both, or many similar boundary candidates showing a similar boundary, helps in identifying diverse boundary
behavior.

6Note that both pairs belong to the valid-error (VE) validity group as both 2021 and 2022 are not leap years and thus February

29th leads to an ArgumentError exception being thrown.

10

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 3: The different conﬁgurations used to run each of the four SUTs.

Parameter

Choice of values

Exploration strategy (ES)
SUTs
Sampling method (SS)
Boundariness quantiﬁer (PD)
Threshold
Mutation operators (m)
Stop criterion

LNS, BCS
bytecount, BMI-Value, BMI-Class, Date
bituniform + CTS activated
strlendist
0
increment/decrement (++/- -)
{30,600} seconds

The more quantitative approaches we use for RQ1 and RQ2 cannot really probe whether the candidates found are
of high quality, i.e. if the boundaries they indicate are ones that are important to test. For RQ3, we thus perform a
qualitative analysis of the identiﬁed PBCs by manually investigating each cluster, systematically sampling candidates
with varied output length in each and analysing them. Consequently, we analyse whether and how often AutoBVA can
identify relevant, rare and/or fault-revealing SUT behaviour.

In the following, we detail how we setup each stage of AutoBVA in our experiments.

4.1 Setup of Selection and Exploration Step

We perform an experiment to gather data to then answer our research questions. We analyse AutoBVA by contrasting
two exploration strategies (one factor - two levels) applied to four chosen SUTs (control variables). Prior to the main
experiment we performed two screening studies to conﬁgure (1) the (global) sampling strategy and (2) the clustering of
boundary candidates (see Appendices A and B).

The sampling method is ﬁxed as the one best performing conﬁguration from the screening study and uses both bituniform
sampling and CTS for all experiments (see Appendix A). For each SUT, we conducted two series of runs, one short for
30 seconds each and one longer for 600 seconds (10 minutes), to understand the convergence properties of AutoBVA.
While these running times are ad hoc, we selected them to at least loosely correspond to more direct (30 seconds)
and more ofﬂine (10 minutes) use by a tester. The boundariness quantiﬁer for all experiments is strlengdist, as output
distance, i.e. the length of the output when seen as a string. Since all input parameters of the SUTs in this study are
integer types, numerical distance is used (implicitly) as input distance, and we use mutation operators increment (add to
integer) and decrement (subtract from integer) during search and exploration. We repeat each experiment execution ten
times to account for variations caused by the pseudorandom number generator used during search. Table 3 summarises
the setup of the experiment.

4.2 Setup of Summarization Step

The dependent variables in our experiment are the number of unique candidates found (RQ1), the number of clusters
covered (RQ2), and the characteristics of interesting candidates (RQ3) found by each exploration approach. To
summarize a large set of boundary candidates we have to decide and extract a set of complementing features able to
group similar candidates and set apart those that are dissimilar. Ideally we want to have a generic procedure for this,
which can give good results for many different types of SUTs.

The program derivative seems a good candidate for a feature that helps separating by candidate differences within
boundaries. Another seemingly relevant aspect to be captured as a feature was the degree of similarity of a boundary
candidate outputs in relation to the overall set of outputs in the entire set of candidates, i.e. the uniqueness of an output.
Essentially, we want to select features so that boundary pairs with similar outputs are grouped together. The choice of
features is formalized below.

We implement the AutoBVA summarization by Validity-Value Similarity Clustering using k-means clustering [34]. We
choose k-means clustering because it is one of the simplest, well-studied, and understood clustering algorithms and is
also widely available [35]. Clustering was done per validity group to avoid mixing pairs that have very different output
types, namely: VV (String, String), VE (String, ArgumentError), and EE (ArgumentError, ArgumentError).

We span a feature-space over the boundary candidates to capture a diverse range of properties and allow for a diversiﬁed
clustering from which to sample representatives per cluster.

11

Automated Black-Box Boundary Value Detection

A PREPRINT

We extract features from the output differences between boundary candidates, since input distances within the boundary
candidates are already factored into the selection of the candidates. Moreover, outputs can easily be compared in their
“stringiﬁed” version using a generic information theoretical measure Q, typically a string distance function.

Our goal is that the features that span the space shall be generic and capture different aspects of the boundary candidates.
We therefore introduce two feature types: (1) WD captures the differences within a boundary candidate as the distance
between the ﬁrst and the second output; (2) U is a two-attribute feature that captures the uniqueness of a candidate based
on the distance between the ﬁrst (U1) and second (U2) output to the corresponding outputs of all other candidates in the
set.7 Considering Q as the distance measure chosen for the outputs, we deﬁne U and WD, for a boundary candidate
j ∈ BC, j = (outj1, outj2) as:

W Dj = Q(out1j, out2j)

Uj = (Uj1, Uj2); where Ujk =

(cid:88)

j(cid:48)∈BC

Q(outjk, outj(cid:48)k), k = 1, 2

To understand which combination of distance measures (Q) yields better clustering of boundary candidates, we
conducted a screening study using three different string distances to measure the distance between the outputs, namely,
strlendist, Levensthein (Lev) and Jaccard of length two (Jacc). These common metrics cover different aspects of string
(dis)similarity, each with their own trade-off. For instance, strlendist is efﬁcient but ignores the characters of both
strings, whereas Jacc compares combinations of characters but disregards speciﬁc sequences in which those characters
show up (e.g., missing complete names or identiﬁers); lastly, Lev is the least efﬁcient but more sensitive to differences
between the strings. Nonetheless, all three measures only consider lexicographic similarity and are not sensitive to
semantics, such as synonyms or antonyms.

For simplicity, the screening study was done only on the bytecount SUT. Details of the screening study and exam-
ples of features extracted from boundary candidates are presented in Appendix B. Our screening study reveals that
strlendist(WD) and Jaccard(WD, U) is the best combination of features and distance measures that yield clusters of
good ﬁt with high discriminatory power. Choosing those types of models yields more clusters that can be differentiated
with high accuracy, hence allowing for a more consistent comparison of cluster coverage between exploration strategies.
Moreover, clearer clusters are also useful in practice as they allow AutoBVA to suggest testers with more diverse
individual boundary candidates.

Formally, we create a feature Matrix M over boundary candidates of a SUT with each row i representing each attribute
from the features over each boundary candidate j, as deﬁned below.

M =

(cid:21)

(cid:20)W D
U

,

For this experiment, each M has four rows, one per attribute in the chosen features: strlendist(WD), Jaccard(WD),
Jaccard(U1) and Jaccard(U2). The number of columns (j) vary depending on the number of candidates found per
exploration approach and SUT. We run the clustering 100 times on each SUT and their corresponding feature matrices
M . To evaluate the coverage of the boundary candidates (RQ2), we choose the clustering discriminating best, i.e. the
one resulting in most clusters based on the top ﬁve percentile Silhouette scores.

We reduce the risk of cluster-size imbalance, and thereby improve on overall clustering quality, by selecting only a
diverse subset of boundary candidates for the clustering and assign the remaining boundary candidates to the clusters
with most similar diversity centers. For that we create an initial diversity matrix as of above using 1000 randomly
selected candidates8. We then remove the bottom 100 in terms of overall diversity (based on the sum of all normalized
diversity readings) and redo this by substituting them with 100 of the remaining candidates. Until no more candidates
exist, we repeat this step to receive M for clustering.

4.3 Description of SUTs

We investigate four SUTs, namely: a function to print byte counts, Body Mass Index (BMI) calculation as value and
category, and the constructor for the type Date. Those SUTs are comparable (i.e., unit-level that have integers as input)

7All distance values from Q are normalised between zero and one keep all features and corresponding attributes on the same

scale.

8For SUTs with fewer than that, we skip this step.

12

Automated Black-Box Boundary Value Detection

A PREPRINT

but each has peculiar properties and different sizes of input vectors. For instance, when creating Dates, the choice of
months affect which are the valid range for days (and vice-versa), whereas the result of a BMI classiﬁcation depends
on the combination of both input values (height and weight). Below, we explain the input, output and reasoning for
choosing each SUT. The code for each SUT is available in our reproduction package9.

bytecount(i1: Int): Receives an integer representing a byte value (i1). The function returns a human readable
string (valid) or an exception signalling whether the input is out of bound (invalid). The largest valid input are those
representable as Eta-bytes. Chosen for being the most copied code in StackOverﬂow. Moreover, the code has a fault in
the boundary values when switching between scales of bytes (e.g., from 1000 kB to 1.0 MB).

bmi_value(height: Int, weight: Int): Receives integer values for a person’s height (in cm) and weight (in
kg). The function returns a ﬂoating point value resulting from weight/(height/100)2 (valid) or an exception message
when specifying negative height or weight (invalid). Chosen because the output is a ratio between both input values,
i.e., different combinations of height and weight yield the same BMI values.

bmi_classification(height: Int, weight: Int): Receives integer values for a person’s height (in cm) and
weight (in kg). Based on the result of bmi_value(height: Int, weight: Int), the outcome is a string indicating
whether the person is Underweight, Healthy, Overweight or Obsese (valid). Otherwise, it returns an exception caused
by negative height or weight (invalid). Chosen because the boundaries between classes depend on the combination of
the input values chosen, leading to a variety of valid and invalid combinations.

date(year: Int, month: Int, day: Int): Receives an integer value representing a year, month and day. The
function returns the string for the speciﬁed Date in the proleptic Gregorian calendar (valid).10 Otherwise, it returns
speciﬁc exception messages for incorrect combinations of year, month, and day values (invalid). Chosen because Date
has many boundary cases that are conditional to the combination of outputs (e.g., the maximum valid day value vary
depending on the month, or the year during a leap year).

Our choice of SUT offers a gradual increase in the cognitive complexity of input, where the tester needs to understand
(1) individual input values, (2) how they will be combined according to the speciﬁcation, and (3) how changing them
impacts the behaviour of the SUT. For instance, when choosing input for year for the Date constructor, a tester can
simply choose arbitrary integer values (case ‘1’) or think of year values that interact with February, 29 to check leap
year dates (case ‘2’). Another example would be choosing test input for BMI, in which the tester needs to manually
calculate speciﬁc combinations of height and weight to verify all possible BMI classiﬁcations (case ‘3’). In parallel
to all those cases a tester would need to check the boundaries for the types used (e.g., maximum or minimum integer
values). Note that systematically thinking of inputs to reveal boundaries is a multi-faceted problem that depends on the
SUT speciﬁcation (e.g., what behaviours should be triggered), the values acceptable for the input type and the output
created independently of being a valid outcome or an exception.

5 Results and Analysis

Below we present the results and analyse them in relation to each of the research questions. The next section then
provides a more general discussion on lessons learnt, implications, as well as future work.

5.1 RQ1 - Boundary candidate quantities

Table 4 summarises the number of common and unique boundary candidates found by the two search strategies LNS
and BCS. For each SUT and search strategy it shows result for the 30 second and the 600 second runs, individually.
For each time control the mean and standard deviation, over the 10 repetitions, of the number of potential boundary
candidates as well as the number of unique candidates is listed. For example, we can see that BCS in a 600 second run
for the Date SUT ﬁnds on average 897.4 +/- 82.6 PBCs out of a total of 45456 unique ones (found over all runs and
search strategies). And overall, 7276 of the total 45456 PBCs were uniquely only found by BCS, i.e. in none of the
LNS runs any of these 7276 were found.

We see that overall a large number of boundary candidates can be found by both methods. Except for bytecount,
there is also a large increase in the number of candidates found as the execution time increases. While the number
of candidates found per second does taper off also for BMI-class, BMI and Julia Date (from 166, 552, and 81 per
second for the short runs to 117, 380, and 76 for the long runs, respectively), longer execution clearly has the potential
to identify more candidates. Since the 20 times longer execution time, for bytecount, only ﬁnds one additional

9https://github.com/feldob/repro_autobva
10We use the constructor from the Julia language as in https://docs.julialang.org/en/v1/stdlib/Dates/.

13

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 4: Descriptive statistics (µ ± σ) over the potential boundary candidates (PBC) found by both BCS and LNS. Total
refers to the size of the union set of the candidates found during the 20 executions of each strategy.

SUT

Strategy

30 seconds

600 seconds

bytecount LNS
BCS

BMI-class

BMI

Julia Date

LNS
BCS

LNS
BCS

LNS
BCS

Total

# PBC found

# Unique

Total

# PBC found

# Unique

57
57

4979
4979

16549
16549

2444
2444

9.5 ± 0.7
56.1 ± 0.6

623.9 ± 40.5
104.2 ± 9.3

2054.0 ± 61.3
209.3 ± 43.0

246.1 ± 13.7
21.6 ± 5.9

0
46

4194
543

58
58

12.0 ± 0.7
57.2 ± 0.4

70265
70265

8233.2 ± 196.6
1398.7 ± 45.2

14918
1126

227899
227899

27581.0 ± 265.4
2959.1 ± 52.9

2232
191

45456
45456

4351.6 ± 86.3
897.4 ± 82.6

0
44

59890
7631

205389
15895

37216
7276

candidate (58 total versus 57 for 30 seconds) it might be useful to terminate search and exploration when the rate of
new candidates found goes below some treshold.

For the bytecount SUT, only BCS ﬁnds a unique set of candidates, meaning that all boundaries identiﬁed by LNS
were also identiﬁed by BCS. This means that only 14 (58-44) of the all candidates found was found by LNS, even
after 10 runs of 600 seconds, and all of those were also found by BCS. For the other SUTs, LNS clearly ﬁnds more
candidates and more unique candidates, between 5 and 15 times more, depending on the SUT.

Thus, overall, LNS produces a higher quantity of boundaries. This is expected since it is essentially a random search
strategy with minimal local exploration. The effect can likely be explained by two reasons that may also interplay. First,
the low algorithmic overhead of the LNS search method enables it to make more calls to the underlying SUT given a
ﬁxed time budget. Second, a proportion of the BCS searches can fail and return no boundary candidates at all since
the input landscape does not regularly lead to changes in output partition by means of single-dimensional, i.e. to one
input, mutations. However, the quantity of boundary candidates might not directly translate to ﬁnding more diverse and
“better” boundary candidates; next we thus need to consider RQs 2 and 3.

Key ﬁndings (RQ1): Large quantities of boundaries can be identiﬁed by both exploration strategies. Overall, Local
Neighbor Search (LNS) ﬁnds more candidates and more unique candidates than Boundary Crossing Search (BCS)
for the more complex SUTs with multiple input arguments, while BCS ﬁnds larger numbers and more unique
candidates for the one-input SUT.

5.2 RQ2 - Robust coverage of diverse behaviors

Using our Validity-Value Similarity Clustering method, we obtained between 5 and 15 clusters across the different
SUTs and validity groups (VV, VE and EE). We summarise the coverage of those clusters per SUT, exploration strategy,
and execution time in Table 5. For example, we can see that for the Julia Date SUT, after we merged all candidates
found by any of the methods in any of the runs and clustered them we found a total of 11 clusters. In a 30 second run,
LNS covered 4.9 +/- 0.3 of them and covered one cluster that was not covered by BCS (in a 30 second run), while in a
600 second run, BCS covered 7.5 +/- 0.8 clusters and covered 6 that were not covered by LNS (in a 600 second run).

LNS shows consistent but modest cluster coverage growth with increasing running time. In other words, boundary
candidates found using LNS cover, on average, one or two more clusters when increasing the execution time from 30
seconds to 10 minutes. In contrast, BCS shows more cluster coverage improvement over time, where ﬁve additional
clusters were covered when searching for 10 minutes, both for BMI Classiﬁcation and Julia Date. It shows no such
growth for bytecount or for BMI but it has also “saturated” for these SUTs already after 30 seconds, i.e. it has covered
the total number of clusters found already after 30 seconds and thus have no potential for further growth.

For many cases, BCS and LNS cover the same clusters, but there are some exceptions. For instance, only BCS found
boundaries between the valid-error and error-error partitions for bytecount — clusters 7 and 8 in Table 6. In contrast,
and considering the 30-second search, only LNS identiﬁed candidates in BMI Classiﬁcation that cover the transitions
between (Underweight, Normal) and (Normal, Overweight) — clusters 8 and 13 in Table 8. However, importantly, with
increased execution time, BCS was the only strategy to ﬁnd unique clusters (ﬁnal column of Table 5). Particularly, if

14

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 5: Statistics over the potential boundary candidate clusters covered by both BCS and LNS. We also show the
number of clusters that are uniquely covered by each approach, for each execution time setting. The Total Clusters
column lists the total number of clusters found by the summarization method when run on all candidates found by any
method in any run.
SUT

Strategy Total

600 seconds

30 seconds

bytecount LNS
BCS

BMI-class

BMI

Julia Date

LNS
BCS

LNS
BCS

LNS
BCS

Clusters

# Found

# Unique

# Found

# Unique

8
8

15
15

5
5

11
11

5.1 ± 0.6
8.0 ± 0.0

13.7 ± 0.8
9.0 ± 0.7

5.0 ± 0.0
4.8 ± 0.4

4.9 ± 0.3
2.7 ± 1.1

0
2

2
0

0
0

1
1

6.0 ± 0.0
8.0 ± 0.0

15.0 ± 0.0
14.2 ± 0.8

5.0 ± 0.0
5.0 ± 0.0

5.0 ± 0.0
7.5 ± 0.8

0
2

0
0

0
0

0
6

we look at Julia Date (10 minutes), BCS covers 6 unique clusters — including two clusters with “valid” outputs but
unexpectedly long month strings. In RQ3, we further explain and compare these clusters for each SUT, and argue their
importance.

The comparisons above highlight the trade-off between time and effectiveness of the exploration strategies. Overall,
we see that LNS can be more effective in covering clusters in a short execution (BMI-class and Julia Data) but this is
not always the case (bytecount). And with more execution time, BCS generally catches up to LNS (BMI-class) and
often surpass it (bytecount and Julia Date) both on average and in the number of unique clusters that are uncovered.
Clearly, attributes of the SUTs will affect cost-effectiveness, e.g., number of arguments in the input, (complexity of)
speciﬁcation, or the theoretical number of clusters that could be obtained to capture boundary behavior.

From Table 5, we also note that the standard deviations are typically low so the method is overall robust to random
variations during the search. Still, we do note that the best method for Julia Date (BCS) only ﬁnds 7-8 of the total of 11
clusters. This is not so for the other three SUTs where it tends to ﬁnd all of the clusters in a 600 second run.

Key ﬁndings (RQ2): The identiﬁed boundary candidates cover a diverse range of boundary behaviors. While Local
Neighbor Sampling (LNS) can sometimes ﬁnd more clusters in a short (30 second) run, Boundary Crossing Search
(BCS) catches up and often ﬁnds both more diverse and more unique candidates in longer runs. The framework is
robust to random variations during the search and the number of unique behaviors found is mainly a function of the
execution time and the characteristics of the SUT.

5.3 RQ3 - Identifying relevant boundaries

None of the investigated SUTs have a formal speciﬁcation to which we can compare the actual behavior of the
implementations as highlighted by the identiﬁed boundary candidates. Formally, we can thus not judge if any of the
identiﬁed boundary candidate pairs indicate real faults.

Also, in practice, even if there was a formal speciﬁcation it might not be fully correct or at least not complete, i.e.
there might be situations/inputs for which it doesn’t fully specify the expected behavior. Human judgement would
then be needed to decide what, if anything, would need to be updated or changed in response to unexpected behavior
uncovered during testing. In industry it is more common with informal speciﬁcations consisting of requirements in
natural language, which can further exacerbate these problems. However, a relative beneﬁt of black-box, automated
boundary exploration with the techniques proposed here is that they can potentially help identify several of these
problems be them in the speciﬁcation (incompleteness or even incorrectness), the implementation (bugs), and/or both.
And even if no problems are identiﬁed the techniques we propose can help strengthen the test suite.

Below, we go through each SUT, in turn, and manually analyse the boundary pairs identiﬁed and if they actually did
uncover relevant (expected as well as unexpected) behavior or even indicate actual faults. We used the clusters identiﬁed
by the summarization process (see RQ2 above) as the starting point. For clusters of a size smaller than 50 boundary
pairs we went through all of them. For larger clusters we randomly sampled pairs, stratiﬁed by the total size of the

15

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 6: Representative candidates for each cluster for bytecount including search coverage for BCS. Rows marked
with an asterisk indicate clusters that are uniquely covered by BCS in a 600 seconds search.

ID Validity

Input 1

Group

Output 1

Input 2

Output 2 Cluster size BCS found

1
2
3
4
5
6
7∗
8∗

VV
VV
VV
VV
VV
VV
VE
EE

-1B
-1
-10B
-10
9.9 kB
9950
999B
999
99.9 kB
99949
falseB
false
99...56
1000.0 EB
99...16 BoundsError("kMGTPE", 9)

0B
0
-9B
-9
10.0 kB
9951
1.0 kB
1000
100.0 kB
99950
trueB
true
99...57
BoundsError("kMGTPE", 7)
99...17 BoundsError("kMGTPE", 10)

3
33
6
6
7
1
1
1

3
33
6
6
7
1
1
1

outputs and analysed them, in turn, from smaller sized sub-groups to larger ones until saturation, i.e. looking at least
at 50 pairs and going on further until no new, interesting or unexpected behavior was found. For additional detail we
calculated also program derivative values using the Jaccard (based on 2-grams) function as output distance and checked
all top-10 ranked pairs, per cluster. In the following, we highlight the key ﬁndings per cluster and SUT.

To support our reporting on the manual analysis we extracted tables with cluster representatives (see Tables 6 - 9).
Unfortunately, for brevity, some of the tables’ entries had to be shortened. The original values and details can be found
as supplementary material 11. Since the answers to RQ1 and RQ2 above indicated that BCS sometimes was more
effective (even if not always as efﬁcient as LNS) the tables has a column indicating how many of the total candidates
per cluster that was found by BCS.

5.3.1 Bytecount

Table 6 contains the representatives for the clusters identiﬁed for bytecount. All 6 members of cluster 4 for bytecount
are the very natural and expected boundaries where the output string sufﬁx changes from a lower value to the next,
e.g. from the smallest input pair of the cluster (999, 1000) with outputs (“999B”, “1.0 kB”) to the largest pair
(999949999999999999, 999950000000000000) with outputs (“999.9 PB”, “1.0 EB”). While the behavior is not
unexpected it is important that also these expected boundaries are identiﬁed. A tester can then more easily verify that
the implementation actually corresponds to what is expected.

The six members of cluster 3 has a similar pattern to the ones in cluster 4 but here the transition is within each output
string sufﬁx category for the transitions from 9.9 to 10.0, e.g. the input pair (99949999, 99950000) with outputs (“99.9
MB”, “100.0 MB”). Since the outputs in such pairs differ in length our output distance function detects them. Cluster 5
has the same six transitions but between 99.9 and 100.0, but also one extra boundary pair for the exabyte sufﬁx (“EB”).
Since this is the last sufﬁx class and thus does not switch over to the next sufﬁx at the value of “1000.0 EB”. Since it
is not obvious what the behavior at 1000.0EB should be, not all speciﬁcations might cover it, and thus it would be
important to test and check against expectations.

Cluster 1 contains three candidates all within the “B” byte sufﬁx group, covering the transitions from “-1B” to “0B”,
from “9B” to “10B”, and from “99B” to “100B”. While the transition from zero to negative one seems like a natural
boundary, one could argue that the other two boundaries are less fundamental, and are an artefact of our speciﬁc choice
of output distance function (string distance, here detecting the difference in lengths between "9" and "10" etc.). But the
extra cost for a tester to verify that they are there seem slight. In the general case, there is, of course, a cost involved in
having to screen very many candidate pairs. The transition from zero to negative inputs, however, should prompt a
tester to consider if this should really be allowed (in the speciﬁcation) or not.

The 33 members of cluster 2 are of more questionable relevance as they are all the transitions from “-9B” to “-10B”,
“-99B” to “-100B”, and so on up for every output string length between 2 up to 36. An argument can be made that it is
good for a tester to check if negative inputs should even be allowed and, if so, how they are to be handled. But having
more than a few such examples is probably not really adding extra insight, and the transition from 0 to -1 was already
covered by the candidate in cluster 1 above.

The ﬁnal valid-valid (VV) cluster (6) for bytecount contains the single pair (false, true) with outputs (“falseB”, “trueB”).
This comes from the fact that in Julia the “Bool” type is a subtype of “Integer” and our tested Julia implementation of
bytecount only speciﬁes that inputs should be integers; booleans are thus generated during the search and this pair is

11https://github.com/feldob/repro_autobva/tree/master/clusterings

16

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 7: Representative candidates for each cluster for Julia Date including search coverage for BCS. Rows marked
with an asterisk indicate clusters that are uniquely covered by BCS in a 600 seconds search. Some input values and
exception messages were abbreviated for brevity. ‘Err’ refers to ArgumentErrors in Julia due to months (Mon) or days
our of range (oor).

ID

1∗
2
3∗
4∗
5∗
6
7
8
9
10∗
11∗

Validity
Group

VV
VV
VV
VV
VV
VE
VE
EE
EE
EE
EE

Input 1

Output 1

Input 2

Output 2 Cluster

(-10000,2,3)
(-1,9,3)
(9999,5,9)
(75...81,2,21)
(16...92,3,22)
(0,2,0)
(330,5,0)
(-8,3,-1)
(0,0,92)
(0,4,99)
(0,9...9,0)

-10000-02-03
-0001-09-03
9999-05-09
25...50-60...91-02
99...99-18...68-20
Err("Day: 0 oor (1:29)")
Err("Day: 0 oor (1:31)")
Err("Day: -1 oor (1:31)")
Err("Mon: 0 oor (1:12)")
Err("Day: 99 oor (1:30)")
Err("Mon: 9...9 oor (1:12)")

(-9999,2,3)
(0,9,3)
(10000,5,9)
(75...82,2,21)
(16...93,3,22)
(0,2,1)
(330,5,1)
(-8,3,0)
(0,1,92)
(0,4,100)
(0,1...0,0)

-9999-02-03
0000-09-03
10000-05-09
-25...50-12...77-30
10...00-18...68-20
0000-02-01
0330-05-01
Err("Day: 0 oor (1:31)")
Err("Day: 92 oor (1:31)")
Err("Day: 100 oor (1:30)")
Err("Mon: 1...0 oor (1:12)")

size

8
115
14
1
5
11019
890
34465
871
8
3

BCS
found

8
38
13
1
5
1560
111
6373
108
7
3

found. Again, it is not clear if this input type should actually be allowed but we argue it is important for a tester to
know of this, implemented behavior to decide if it is good enough or needs to be addressed. Even if one decides to keep
this functionality in the implementation it seems valuable to add it as a testcase to the test suite, at least as a kind of
documentation.

There is a single valid-error (VE) cluster for bytecount (7) that has the single member (999999999999994822656,
999999999999994822657) where the ﬁrst output is the string “1000.0EB” while the latter throws the exception
BoundsError(”kM GT P E”, 7). The Julia exception indicates that the implementation tried to access the string
”kM GT P E”, of length 6, at position 7. Similarly there is a single error-error (EE) cluster (8) where the exception
thrown changes from BoundsError(”kM GT P E”, 9) to BoundsError(”kM GT P E”, 10). Having found 3 inputs
for which there are different kinds of BoundsErrors thrown it is then obvious that there will be other such transitions,
i.e. between the errors accessing the string at position 7 and those at position 8 etc. Since our output distance only
detects differences in length it doesn’t identify the transition from 7 to 8 or 8 to 9 but picks up the transition from 9 to
10. This shows some of the trade-offs in the selection of the output distance function; while the one we have chosen
here is very fast and does ﬁnd a lot of relevant boundary pairs more ﬁne-grained detection can be possible with more
sensitive output distance functions.

5.3.2 Julia Date

Cluster number 4 for the Julia Date SUT, shown in Table 7, contains a single boundary candidate pair which shows an
unexpected switch in the outputs despite both being valid Dates. The pair also has among the largest program derivative
values found overall (0.634). This candidate uses very large values for the year input parameter (757576862466481
and its successor), coupled with “normal” month and day values, but the outputs have no resemblance to the inputs
and also switches the sign for the year in the Date outputs (outputs are 252522163911150-6028347736506391-02 and
-252522163911150-12056695473012777-30, respectively). Even if such high values for the year parameter are not
very likely in most use cases, we argue that it is still useful for a developer or tester to know about this unexpected
behavior. They can then decide if and how to handle the case, i.e. to update either or both of the speciﬁcation and the
implementation or at least to document the behavior by adding a test case.

The pairs found in the valid-valid cluster 5 similarly all happen for large values of the year input parameter but differ
from cluster 4 in that the output Dates are more similar to each other and typically only differ in one of the Date
elements, e.g. year or month. Correspondingly the PD values are much lower (smaller variation around 0.20).

Cluster 1 contains pairs where all years are negative and switches from one order of magnitude (all nines, e.g. "-9999-
02-03", to the next one followed by zeros, e.g. "-10000-02-03"). Since the PD output distance is output string length
many such boundaries (for different number of nines) are found. While the outputs in this cluster have similarities to
the ones in cluster 5 above, the latter does not have inputs that corresponds to the outputs. For cluster 1 the input years
corresponds to the output years. Splitting these into two clusters thus makes sense.

The largest valid-valid cluster (2) contains many pairs that only differ in the month and day while the year always goes
from -1 to 0.

17

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 8: Representative candidates for each cluster for BMI classiﬁcation including search coverage for BCS.

ID Validity

Input 1

Group

Output 1

Input 2

Output 2 Cluster size BCS found

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

(1,0)
VV
(21,1)
VV
(26,1)
VV
(29,1)
VV
(29,2)
VV
(101,18)
VV
(101,30)
VV
(108,26)
VV
(115,32)
VV
(132,44)
VV
(133,41)
VV
(1015,3087)
VV
VV (10...88,18...37)
VE
VE

Underweight
Normal
Underweight
Underweight
Overweight
Underweight
Obese
Normal
Overweight
Obese
Overweight
Severely obese
Normal
(-1,0) DomErr("H or W negative...)
(1,-1) DomErr("H or W negative...)

(1,1)
(21,2)
(26,2)
(29,2)
(29,3)
(101,19)
(101,31)
(108,27)
(115,33)
(133,44)
(134,41)
(1016,3087)
(10...89,18...37)
(0,0)
(1,0)

Severely obese
Severely obese
Obese
Overweight
Severely obese
Normal
Severely obese
Overweight
Obese
Overweight
Normal
Obese
Underweight
Severely obese
Underweight

19
5
5
3
2
511
394
125
139
93
103
814
871
35176
35051

19
5
5
3
1
459
358
40
49
28
38
747
818
3809
3991

Clusters 6 and 7 both have pairs where one input leads to an ArgumentError while the other input leads to a valid Date.
Both of the clusters has errors that either complains about an invalid Day or and invalid Month. We couldn’t identify a
clear reason why these two clusters were not merged. Most likely it is just an artefact of the clustering method we used
in the summarisation step.

The remaining clusters all raise exceptions for both inputs. Cluster 11 has only 3 pairs which are all complaining about
invalid Month inputs, all of a Month transition from a number of nines to the next order of magnitude. Similarly, the
pairs of cluster 10 all complain about invalid Day inputs, all being variations of nines and the next order of magnitude.
This cluster is larger since there are more unique exception of this type. The error message depends on the month
since different months have differing numbers of allowed day ranges. Cluster 9 then has pairs where one input leads to
argument error for invalid month and the other one for invalid day. Cluster 8 then mainly has both inputs being invalid
day values although some combined pairs (both invalid month and invalid day) are also in this one.

5.3.3 BMI classiﬁcation

For the BMI classiﬁcation SUT (Table 8) there are many clusters that show the boundaries between “adjacent” output
classes, i.e. Underweight to Normal (clusters 6 and 13), Normal to Overweight (8 and 11), Overweight to Obese (9 and
10), and Obese to Severly Obese (7 and 12). There are two clusters for each such boundary and they differ only in the
order of the outputs, i.e. cluster 6 has the Underweight output ﬁrst while cluster 13 has Normal ﬁrst etc.

We can also note that these clusters are relatively large with the smallest one (cluster 10) containing 93 pairs up to the
largest one (cluster 13) cotaining 871 pairs. This is natural since the formula for calculating BMI allows for many
different actual inputs being right on the border between two adjacent output classes.

In contrast to the “natural” boundaries above, clusters 1 to 5 all contain fewer boundary candidates (from 2 to 19) but all
correspond to transitions between output classes that are unexpected. For example, cluster 1 contains extreme examples
of inputs that are very close but where the output class jumps all the way from Underweight to Severly obese. We
note that all of these clusters happen for very extreme input values and it is likely that we can address many of these
problems by putting limits on the valid ranges of each of the inputs. However, it is important that our method was able
to ﬁnd transition not only between some of these non-adjacent output classes but for several combinations of them.

Finally, the method identiﬁed a large number of valid-error pairs at either end of the output class adjacency scale.
Cluster 14 has pairs that go from Severly obese to an input Domain error where one of the values are negative, while
cluster 15 has pairs that go from the Underweight output class to input domain errors. The size of these clusters are very
large and it is not likely a tester would get much extra beneﬁt from having so many candidate pairs. Future work can
thus explore ways of focusing the search to avoid ﬁnding very many candidate pairs that ends up in the same cluster.

5.3.4 BMI

For the BMI SUT (Table 9) there are only ﬁve clusters identiﬁed with clusters 3 to 5 all having one input that leads to a
domain error raised while the other output is either NaN (cluster 3), Inﬁnity (4), or 0.0 (5). While cluster 3 is rare and

18

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 9: Representative candidates for each cluster for BMI including search coverage for BCS.

ID Validity Group

Input 1

Output 1

Input 2 Output 2 Cluster size BCS found

1
2
3
4
5

VV
VV
VE
VE
VE

(0,93)
(106,11)

Inf
9.8
(-1,0) DomErr("H or W negative...)
(-1,1) DomErr("H or W negative...)
(1,-1) DomErr("H or W negative...)

(1,93)
(106,12)
(0,0)
(0,1)
(1,0)

930000.0
10.7
NaN
Inf
0.0

5753
99113
2
95046
38449

299
11181
2
6534
4494

only happens for two speciﬁc input pairs, the two other clusters are very large. This reﬂects the fact that there are many
ways to create an inﬁnite output (height of zero and weight is any value) or zero output (weight is zero and height is any
value).

The valid-valid cluster 1 have pairs where the ﬁrst outputs inﬁnity while the second input leads to a normal, ﬂoating
point output. The largest cluster is the valid-valid cluster 2 which has pair with normal, ﬂoating point outputs that differ
only in their length. Of course, there are many such transitions and our system identiﬁes many of them, but it is not
clear that a tester would be helped by some of them more than others. Sorting just by length and including a few such
transitions will likely be enough.

5.3.5 Summary for RQ3

Taken together, our manual analysis of the identiﬁed clusters and their boundary candidates shows that the method we
propose can reveal both expected and unexpected behavior for each of the tested SUTs. Using bytecount as an example,
21 expected boundaries were automatically identiﬁed, divided into three main groups:

1. transitions between consecutive byte sufﬁx partitions, e.g. "999.9 kB" to "1.0 MB" (6 candidates),

2. transitions from zero to negative values, "0B" to "-1B" (1),

3. transitions within same byte sufﬁx partitions, e.g. "9B" to "10B", "9.9 MB" to "10.0 MB", and "99.9 GB" to

"100.0 GB" (14).

Of these we argue the ﬁrst two groups (1 and 2) are expected and natural while a tester can decide if and, if so, how
many from group 3 to include in the test suite. The method also identiﬁed four (4) boundaries for bytecount that we
argue were unexpected:

1. transition from "999.9 EB" to "1000.0 EB",

2. transition between boolean inputs "falseB" to "trueB",

3. transition from valid output, "1000.0 EB" for input 999999999999994822656,

to an exception,

BoundsError("kMGTPE", 7) for input 999999999999994822657,

4. transition

from

two

different

exceptions,

BoundsError("kMGTPE", 9)

for

999999999999990520104160854016

put
999999999999990520104160854017.

and

BoundsError("kMGTPE", 9)

for

in-
input

In hindsight, it is likely a tester can understand the reasons for these boundaries, but we argue that it is not obvious
from just looking at the implementation or a speciﬁcation that these boundaries are there. Even though the very simple
output distance function we have used cannot detect the additional error-error boundaries we can understand to be there
(between bounds error 7 and 9, for example) it would be relatively simple to ﬁnd them with a more focused search once
we know to look for them. This also points to future work investigating alternative output distance functions or even
hybrid search approaches that applies multiple distance functions, for different purposes.

For bytecount, another 33 boundary candidates were identiﬁed that were also unexpected but where we judge it less
likely that a tester would include them all in a testsuite. These are the transitions between different sizes of negative
inputs, e.g. from "-9B" to "-10B" and so on. Potentially, a tester might want to sample some of them to ensure proper
treatment by a reﬁned implementation but since the transition from zero to negative one has already been found the
additional value is relatively limited.

For the Julia Date SUT, several clusters where of more debatable value and in particular in the error-error group it is
likely that a tester would only have selected some typical examples from the identiﬁed clusters. While there were some
differences between the clusters they essentially just differed in whether the month or day inputs lead to an exception

19

Automated Black-Box Boundary Value Detection

A PREPRINT

being thrown. We do note that the boundary transitions for invalid day inputs covered all of the different months (30
to 31 valid-error transition for June, 31 to 32 for August, 28 to 29 for February etc). However, the clustering was not
enough to separate them out into individual groups which made the manual screening less efﬁcient.

For BMI-class, most clusters contained relevant boundaries. While all the expected boundaries between consecutive,
ordinal outputs (like Normal to Obese) where identiﬁed, the method also identiﬁed a large number of unexpected
boundaries between non-consecutive output categories.

For BMI, with its numerical outputs, a much larger number of candidates where identiﬁed. Still, the summarization
method successfully distilled them down to only 5 clusters, which made it relatively easy to manually screen them. Even
if expected and relevant boundaries were found it is harder, in this case, to deﬁne judge if there are other boundaries
that should be found in the large, valid-valid groups. It was not clear, in this case that the output distance function used
is ﬁne-grained enough to pick out important differences.

While all eight clusters for bytecount and BMI contained at least one boundary candidate that we argue a tester would
like to look at this was not the case for all the other SUTs. For example, for BMI-class several clusters differed only in
the order of the outputs. This should be reﬁned in future work on the method. There were also cases where clusters
seemed to have been unnecessarily split into multiple clusters for which we could not discern any clear pattern or
semantic reason. Most likely this is an artefact of the clustering method used and the features we used as its input. Still,
we argue that since the total number of clusters identiﬁed was relatively limited it would not be a major cost for testers
to screen them all.

While the number of identiﬁed candidates for bytecount was low (58), the clustering for summarization clearly helped
in identifying interesting boundary candidates. This was even more evident for the more complex SUTs where the total
number of candidates identiﬁed was very large; summarization is thus a necessity and clustering is one helpful way to
achieve it. Future work should investigate how to reﬁne the summarization further to decrease the number of candidates
a tester has to look at.

Key ﬁndings (RQ3): The AutoBVA method can successfully identify both expected and unexpected boundaries
without using a speciﬁcation or white-box information. While the value of identiﬁed candidates ultimately depends
on the tester, the summarization via clustering helped focus the manual screening. Further reﬁnement to the
summarization method should be investigated to try to minimize the number of different clusters a tester has to
manually inspect.

6 Discussion

Our results show that relevant boundaries in the input space and behavior of programs can be identiﬁed automatically,
without the need for a speciﬁcation nor for white-box analysis or instrumentation. This is important since it can help
make boundary value analysis and testing more automated and systematic. While these techniques for quality assurance
and testing have been advocated for long and sometimes even been required by standards and certiﬁcation bodies, prior
work has relied, for effective results, on the creativity and experience of the tester performing them.

By building on the vision from [17, 18] and coupling their proposal to simple search algorithms, the system we propose
here enables augmenting the testers performing boundary value analysis and testing by automatically identifying and
presenting them with candidate input pairs that are more likely to reveal actual boundaries.

Our experimental validation shows that for the investigated SUTs the system could identify a large number of boundary
candidate pairs that also cover a diverse range of behaviors. These results were also robust over multiple executions,
despite the stochastic nature of the algorithms used. Manual screening showed that many relevant (important) boundary
candidates were automatically identiﬁed, both those that could be expected for the investigated SUTs, and relevant but
unexpected ones, that we argue would have been harder for a tester to think of.

We investigated two different search strategies within our overall framework. The simpler one, Local Neighbor Search
(LNS) is more directly similar to random testing (in automated testing) but with a local exploration around a randomly
sampled point. It identiﬁed more boundary candidates but, even if given a longer execution time, didn’t ﬁnd as diverse
types of candidates as the other strategy. The latter, Boundary Crossing Search (BCS), was tailored speciﬁcally to the
problem at hand, by ﬁrst identifying inputs in two different input partitions and then “honing” in on the/any boundary
between them. BCS needs more computational resources but consistently found as many or more diverse clusters of
candidates than LNS.

20

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 10: Different actions a tester can take given one or a set of related automatically identiﬁed boundary candidates,
and which artefacts that are likely to change for each action.

Action

Speciﬁcation

Implementation Test suite

Skip
—
ExtendTests —
—
Debug
X
Clarify
X
Reﬁne

—
—
X
—
X

—
X
X
X
X

Regardless of the search strategy used, for our system to be useful to actual testers, a critical step is how to group and
summarize the set of candidates found. While this is not the main focus of this study, we show that basing the grouping
on the type of outputs of the pair and then clustering them based on their within- and between-pair distances, can be
helpful. However, our experiments also uncovered challenges in this approach that should be investigated in future
work, i.e. how to avoid showing too many groups (clusters) as well as candidates to a tester.

The key idea that our system builds upon is that of the program derivative, a way to quantify the rate of change for any
program around one of its inputs. A key choice we made in this study was to use a very fast but exceedingly simple
distance function for outputs. By simply comparing the length of the outputs in a pair, after stringifying them, we
can only detect a difference that leads to differing lengths. Clearly, this will not always be enough, for example for
functions where all outputs are of the same length. Given this major limitation, our results are encouraging; we can ﬁnd
relevant boundaries despite it. One reason is likely that if outputs differ in some way they also tend to differ also in their
length. Another reason is likely that by using such a fast but coarse distance function we can explore larger parts of the
search space. Even the fairly simple Jaccard string distance function, that would detect more ﬁne-grained differences
in outputs, would be at least an order of magnitude and possibly more slower. And more advanced methods like the
compression distances would be orders of magnitude slower yet. Future work should look into the trade-off between
fast but coarse and slower but more ﬁne-grained distance functions. We do note that the system need not select only one
distance function though; hybrid solutions could be tried that ﬁrst gets a coarser view of the boundaries and then zooms
in for further details in different sub-partitions.

While our results constitute evidence that the overall approach has value, it is methodologically difﬁcult to judge
how complete a set of boundaries the method can ﬁnd. Similarly, there is no clear baseline to compare to given that
traditionally boundary value analysis has been a manual activity that is heavily dependent on both the existence of a
speciﬁcation and the experience of the tester. Still, future work should perform user studies, both "competing" with
testers doing purely manual analysis and "in tandem" with testers to evaluate the added value of the automated system.

Our manual analysis of boundary candidates indicated that there are different types of candidates and they differ in
what type of action they are likely to lead to. Below we give some examples and discuss these actions in some more
detail. Finally, we then conclude the Discussion section by discussing limitations and threats to validity.

6.1 Boundary candidates types and tester actions

Table 10 show ﬁve different actions that a tester can typically take in relation to one or a set of similar boundary
candidates. The columns to the right shows the different artefacts that are likely to be affected in each case: the
speciﬁcation, the implementation, or the test suite.

A Skip action typically means that either the tester does not ﬁnd the candidate(s) relevant/useful, or it is already handled
in the correct way by the implementation, clear enough in the speciﬁcation, as well as tested well enough. Even in
the latter case, it is important that an automated testing method can identify also boundaries of this type; if it would
frequently miss them a tester might lose conﬁdence in the tool.

An ExtendTests action would happen when the identiﬁed behavior is both correct and according to the speciﬁcation
but not yet well covered by the test suite. The Debug action occurs when a fault is identiﬁed in the implementation,
i.e. the speciﬁcation describes what the correct behavior should be but the implementation does not comply. The
Clarify situation is a consequence of a behavior that is intended but not explicitly speciﬁed. Finally, the Reﬁne action is
a consequence of a boundary that is not desired but when the speciﬁcation is either incorrect or incomplete. In this
situation we both need to clarify the speciﬁcation and debug/ﬁx the implementation. For all three of the last actions the
test suite will also typically be extended to ensure the same problem does not occur in the future.

21

Automated Black-Box Boundary Value Detection

A PREPRINT

One type of boundary candidate we identiﬁed in manual analysis was the under-speciﬁed input range, i.e. for inputs
that are not clearly expected which leads to unexpected behavior. One example, for BMI-class, was the candidate with
outputs that move from underweight to severely obese by a change in body-weight from 0 to 1 kg for a person with a
height of 1 cm. Another example Julia Date, with a very large year input that lead to nonsensical outputs that showed
no resemblance to the inputs, likely due to some kind of overﬂow in the internal representation of date values.

This type of boundary would typically lead to either a Debug or a Reﬁne action, depending on whether an update of the
speciﬁcation is meaningful/necessary or not.

Other types of problems were identiﬁed in relation to the bytecount SUT. We used this SUT since it is known as
the most copied (Java) code snippet on StackOverﬂow and has several bugs as described in [26]: a rounding error,
correctness for large values over 64 bits, and the fact that negative inputs are not appropriately handled. The latter
was clearly identiﬁed, as described in our results above, can be seen as an under-speciﬁed problem and likely leads
to a Reﬁne action since it is not clear from the speciﬁcation how to handle negative inputs. The correctness for large
values is still a problem even though in Julia the Integer type includes also 128 bit integers and the BigInt type to handle
arbitrary large integers. However, the problem manifests in the boundary candidates that lead to BoundsErrors since
they try to access larger byte sufﬁx categories also after exabytes ("EB"). A Reﬁne or at least a Debug action is probably
called for. The boundary candidates capturing the rounding error of bytecount, e.g. kB to MB, were all identiﬁed in
the searches (cluster 5 in Table 6) and represents wrongly positioned boundaries. While it is not clear if the rounding
error can be directly spotted even given the boundary examples in the cluster, it is contained in all the boundaries of the
cluster. A diligent tester is likely to investigate key boundaries in depth and the fact that they are all in the same cluster
can help make this checking more likely. This will lead to a Debug action.

6.2 Threats to validity

Our analyses involve simple comparisons through descriptive statistics (RQ1 + RQ2) and a qualitative analysis of the
clusters (RQ3). We mitigate conclusion validity threats in our quantitative comparison by focusing on how the sampling
strategies complement each other rather than simply comparing which one is better. By analysing the uniqueness of
candidates and clusters we identiﬁed essential trade-offs. For RQ3, we do not perform an exhaustive inspection of all
candidates in larger clusters, hence there is a risk we miss important ones. We mitigate this risk by random sampling
and iterative analysis until saturation in conclusions.

We operationalize the constructs of our proposed approach with a limited set of treatments and dependent variables.
Consequently, the main construct validity threats are our choices of: (1) conﬁguration (string length for PD, mutation
operators, sampling methods, etc.), (2) SUTs with integer inputs, (3) measures of uniqueness, clustering, and coverage/
inspection of clusters. However, given the novelty of the approach we argue it would be very difﬁcult to analyse the
overall results if we used more advanced or just more choices for these contextual factors. Future work is needed to
better understand performance implications as these factors are varied.

We use clustering as an attempt to summarise boundary candidates into a presentable subset for testers. The “relevance/
importance of a boundary” is to a certain degree subjective, hence an optimal or perfect summarization is hardly
attainable. It should be mentioned that in a real-world situation, a single search run with sufﬁciently many boundary
candidates should sufﬁce to do both the clustering and extraction of summary - which will both be faster and simpler
than running many times over and building an overall clustering model over the entire set of found candidates. However,
the number of clusters found in each run was smaller than the total number for some SUTs, even for BCS, so the effect
of execution time needs further study in the future.

We mitigate internal validity threats by doing pilot studies and testing our instrumentation. The screening studies helped
us to identify feasible and consistent strategies for sampling candidates and clustering outputs, instead of going for
arbitrary choices. Encouraging was also that our method revealed a fault in our own implementation of the BMI value
function (Table 9)12 Regarding the veriﬁcation of the search procedure and system itself, we use automated unit tests in
Julia to mitigate faults in our implementation.13

Lastly, our results cannot be generalised beyond the scope of our experimental study, i.e., ﬁnding boundaries for
unit-level SUTs that take integers as input. On one hand, there are various aspects in AutoBVA that indicate it might
be generally applicable, such as the concept of the program derivative and basing on string distances rather than
type-speciﬁc distance function. However, more general test data generation algorithms, mutation operators, and distance
functions will need to be experimented with to increase the external validity.

12The implementation did not check for height or weight equals to zero, hence triggering a division by zero error which lead to a

NaN output.

13https://docs.julialang.org/en/v1/stdlib/Test/#Basic-Unit-Tests

22

Automated Black-Box Boundary Value Detection

A PREPRINT

7 Conclusions

While automated boundary value analysis and testing is often advocated or required for quality assurance, prior work
has relied mainly on the creativity and experience of the testers that perform them. Boundary value analysis has been a
manual activity. Automated solutions are needed to better support testers, increase efﬁciency, and to make boundary
value analysis more systematic. However, existing proposals have relied either on formal speciﬁcations, development of
additional models, or white-box analysis, limiting their applicability.

Here we have presented an automated and black-box approach to identify and summarize boundaries of programs.
It is based on coupling a boundary quantiﬁcation method to a search algorithm. We further proposed to use string
length distance as a very fast but coarse-grained boundary quantiﬁer, and proposed two different strategies to search for
boundary candidates. A clustering algorithm was used to summarize the identiﬁed candidates in groups based on the
similarity of their values.

We validated our approach on four SUTs with both single and multiple numbers as inputs. We quantitatively evaluated
how many candidates were found by the two search strategies as well as their uniqueness and diversity. A manual,
qualitative analysis of the identiﬁed boundary candidates was also performed to better understand their usefulness.

We ﬁnd that, even using one of the simplest possible boundary quantiﬁcation metrics, large quantities of boundary
candidates could be found by both strategies. While the simpler Local Neighbor Search found more and more unique
candidates than the Boundary Crossing Search strategy, the latter found more unique groups, if given more search time.
Even though our approach is stochastic it was robust over multiple, repeated executions. The manual analysis showed
that many both expected and unexpected boundaries were found. Both previously known bugs in the investigated
SUTs as well as new ones introduced by us were identiﬁed by the system. Based on our ﬁndings we outlined a
simple taxonomy of different actions the proposed system can prompt a tester to take; reﬁning either a test suite, an
implementation, or even the speciﬁcation.

While our results are promising, future work will have to consider more SUTs that are both more complex and have
non-numerical input parameters. It should also explore more elaborate search strategies that can search globally over
the input space as well as use the already identiﬁed candidates and groups to avoid unnecessary, repeated work.

References

[1] J. B. Goodenough and S. L. Gerhart, “Toward a theory of test data selection,” IEEE Transactions on software

Engineering, no. 2, pp. 156–173, 1975.

[2] D. J. Richardson and L. A. Clarke, “Partition analysis: A method combining testing and veriﬁcation,” IEEE

Transactions on Software Engineering, no. 12, pp. 1477–1490, 1985.

[3] D. Hamlet and R. Taylor, “Partition testing does not inspire conﬁdence (program testing),” IEEE Transactions on

Software Engineering, vol. 16, no. 12, pp. 1402–1411, 1990.

[4] M. Grindal, J. Offutt, and S. F. Andler, “Combination testing strategies: a survey,” Software Testing, Veriﬁcation

and Reliability, vol. 15, no. 3, pp. 167–199, 2005.

[5] T. J. Ostrand and M. J. Balcer, “The category-partition method for specifying and generating fuctional tests,”

Communications of the ACM, vol. 31, no. 6, pp. 676–686, 1988.

[6] M. Grochtmann and K. Grimm, “Classiﬁcation trees for partition testing,” Software testing, veriﬁcation and

reliability, vol. 3, no. 2, pp. 63–82, 1993.
[7] G. J. Myers, The art of software testing.
[8] L. J. White and E. I. Cohen, “A domain strategy for computer program testing,” IEEE transactions on software

John Wiley & Sons, 1979.

engineering, no. 3, pp. 247–257, 1980.

[9] L. A. Clarke, J. Hassell, and D. J. Richardson, “A close look at domain testing,” IEEE Transactions on Software

Engineering, no. 4, pp. 380–390, 1982.

[10] Standard, “BS 7925-2: Standard for software component testing (working draft),” British Computer Society

Specialist Interest Group in Software Testing, Standard, 2001.

[11] H. Yin, Z. Lebne-Dengel, and Y. K. Malaiya, “Automatic test generation using checkpoint encoding and anti-
random testing,” in PROCEEDINGS The Eighth International Symposium On Software Reliability Engineering.
Albuquerque: IEEE, 1997, pp. 84–95.

[12] F. Hübner, W.-l. Huang, and J. Peleska, “Experimental evaluation of a novel equivalence class partition testing

strategy,” Software & Systems Modeling, vol. 18, no. 1, pp. 423–443, 2019.

23

Automated Black-Box Boundary Value Detection

A PREPRINT

[13] R. Pandita, T. Xie, N. Tillmann, and J. De Halleux, “Guided test generation for coverage criteria,” in 2010 IEEE

International Conference on Software Maintenance. Timisoara: IEEE, 2010, pp. 1–10.

[14] N. Kosmatov, B. Legeard, F. Peureux, and M. Utting, “Boundary coverage criteria for test generation from
formal models,” in 15th International Symposium on Software Reliability Engineering. Rennes: IEEE, 2004, pp.
139–150.

[15] S. C. Reid, “An empirical analysis of equivalence partitioning, boundary value analysis and random testing,” in
Proceedings Fourth International Software Metrics Symposium. Albuquerque: IEEE, 1997, pp. 64–73.
[16] A. Arcuri, M. Z. Iqbal, and L. Briand, “Random testing: Theoretical results and practical implications,” IEEE

transactions on Software Engineering, vol. 38, no. 2, pp. 258–277, 2011.

[17] R. Feldt and F. Dobslaw, “Towards automated boundary value testing with program derivatives and search,” in

Search-Based Software Engineering. Tallinn: Springer International Publishing, 2019, pp. 155–163.
[18] F. Dobslaw, F. G. de Oliveira Neto, and R. Feldt, “Boundary value exploration for software analysis,” 2020.
[19] S. C. Reid, “Bs 7925-2: the software component testing standard,” in Proceedings First Asia-Paciﬁc Conference

on Quality Software. Hong Kong: IEEE, 2000, pp. 139–148.

[20] B. Jeng and E. J. Weyuker, “A simpliﬁed domain-testing strategy,” ACM Transactions on Software Engineering

and Methodology (TOSEM), vol. 3, no. 3, pp. 254–270, 1994.

[21] M. N. Borazjany, L. S. Ghandehari, Y. Lei, R. Kacker, and R. Kuhn, “An input space modeling methodology
for combinatorial testing,” in 2013 IEEE Sixth International Conference on Software Testing, Veriﬁcation and
Validation Workshops. Luxembourg: IEEE, 2013, pp. 372–381.

[22] R. Feldt, R. Torkar, T. Gorschek, and W. Afzal, “Searching for cognitively diverse tests: Towards universal
test diversity metrics,” in 2008 IEEE International Conference on Software Testing Veriﬁcation and Validation
Workshop. Lillehammer: IEEE, 2008, pp. 178–186.

[23] R. Feldt, S. Poulding, D. Clark, and S. Yoo, “Test set diameter: Quantifying the diversity of sets of test cases,” in
2016 IEEE International Conference on Software Testing, Veriﬁcation and Validation (ICST). Chicago: IEEE,
April 2016, pp. 223–233.

[24] P. Jaccard, “The distribution of the ﬂora in the alpine zone. 1,” New phytologist, vol. 11, no. 2, pp. 37–50, 1912.
[25] S. Baltes and S. Diehl, “Usage and attribution of stack overﬂow code snippets in github projects,” Empirical

Software Engineering, vol. 24, no. 3, pp. 1259–1295, Jun 2019.

[26] A. Lundblad, “The most copied stackoverﬂow snippet of all time is ﬂawed,” 2019, https://programming.guide/

worlds-most-copied-so-snippet.html.

[27] M. Harman and B. F. Jones, “Search-based software engineering,” Information and software Technology, vol. 43,

no. 14, pp. 833–839, 2001.

[28] W. Afzal, R. Torkar, and R. Feldt, “A systematic review of search-based testing for non-functional system

properties,” Information and Software Technology, vol. 51, no. 6, pp. 957–976, 2009.

[29] R. Feldt, “Generating multiple diverse software versions with genetic programming,” in Proceedings. 24th

EUROMICRO Conference (Cat. No. 98EX204), vol. 1. Västerås: IEEE, 1998, pp. 387–394.

[30] T. Smith, P. Husbands, and M. O’Shea, “Fitness landscapes and evolvability,” Evolutionary computation, vol. 10,

no. 1, pp. 1–34, 2002.

[31] T. Yu and J. F. Miller, “Through the interaction of neutral and adaptive mutations, evolutionary search ﬁnds a way,”

Artiﬁcial Life, vol. 12, no. 4, pp. 525–551, 2006.

[32] R. Feldt and S. Poulding, “Finding test data with speciﬁc properties via metaheuristic search,” in 2013 IEEE 24th
International Symposium on Software Reliability Engineering (ISSRE). Passadena: IEEE, 2013, pp. 350–359.
[33] A. Likas, N. Vlassis, and J. J. Verbeek, “The global k-means clustering algorithm,” Pattern recognition, vol. 36,

no. 2, pp. 451–461, 2003.

[34] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means clustering algorithm,” Journal of the royal

statistical society. series c (applied statistics), vol. 28, no. 1, pp. 100–108, 1979.

[35] D. Xu and Y. Tian, “A comprehensive survey of clustering algorithms,” Annals of Data Science, vol. 2, no. 2, pp.

165–193, 2015.

[36] P. J. Rousseeuw, “Silhouettes: a graphical aid to the interpretation and validation of cluster analysis,” Journal of

computational and applied mathematics, vol. 20, pp. 53–65, 1987.

[37] H. Xiong and Z. Li, “Clustering validation measures,” in Data Clustering. Chapman and Hall/CRC, 2018, pp.

571–606.

24

Automated Black-Box Boundary Value Detection

A PREPRINT

A Screening Study - Conﬁguring the Exploration Strategy

The exploration process has shown to be greatly inﬂuenced by a number of parameters - in particular regarding the
sampling strategy. A pre-study investigating two parameters was conducted, and is detailed below. The purpose was to
reduce the complexity of the main experiments without sacriﬁcing quality if possible. We here describe the experiments
and the verdict.

High level languages usually offer sampling based on a single concrete datatype (such as Int32 in Julia, representing
Integers of 32 bits). When activating CTS, the sampling is instead done based on the compatible types per argument.
For abstract data-types, the compatible types per argument can be derived from the type graph (see Julia’s type graph
for numbers in Fig. 2). In this study all input parameters are Integers. In Julia, Integers are an abstract type which
cannot be sampled from. Thus we use Integers of base 128 in this study as per default. CTS must contain concrete
types only, not abstract types, and a sampler for the type must be available. For instance, for Integer in Julia, the CTS in
accordance with the graph is

CT S(Integer) = {U Int8, U Int64, U Int32, U Int16, U Int128, Int8, Int64, Int32, Int16, Int128, BigInt, Bool}

Further, the use of CTS is not limited to abstract datatypes such as Integer, but can be extended to concrete types. For
Int16, and in accordance with the conversion rules of Julia, we may declare:

CT S(Int16) = {U Int8, Int8, Int16, Bool}

Sampling using CTS becomes a two-step process. For each argument, ﬁrst, a compatible type is selected, and then
the sampler for that type gets invoked. Two simple approaches to select the type for each sample is round-robin and
uniformly at random. Without loss of generality, we use the latter approach in this study.

We here investigate the impact of CTS (activated, deactivated) and the sampling method (uniform, bituniform), both
explained in 3.2.1, on the efﬁcacy of boundary exploration. We therefore apply AutoBD with LSN compared to BCS
according to Section 3 on bytecount for 30 and 60 seconds each for all possible conﬁgurations. As a boundariness
measure, strlendist of the outputs is applied. The results can be seen in Table 11.

Note that regular uniform sampling without CTS, what often is referred to as random search, almost never ﬁnds
any boundary candidates. First when activating CTS boundary candidates can be identiﬁed. The greatest number of
boundary candidates in all scenarios is though obtained when both CTS and bituniform sampling is activated. We
therefore limit the study to conﬁgurations with CTS activated and bituniform sampling. The implied limitations in
terms of generalizability are further discussed under 6.1.

B Screening Study - Clustering for Summarization

The goal of our screening study is to identify a combination of features (U and WD) and distance measures (strlendist,
Jaccard, and Levenshtein) that yield a good model for doing k-means clustering of boundary candidates found by
AutoBVA (as introduced and explained in section 4.2). Finding a good clustering is important to allow for an automated
and consistent comparison of types of boundary candidates, since we cannot deﬁne equivalence partitions manually for
the chosen SUTs. We also illustrate some examples of features extract from actual boundary candidates to clarify how
the features and attributes represent the different types of boundaries.

Figure 2: The type hierarchy for numbers in Julia showing the compatible types for Integer. Bool is an Integer.

25

NumberRealComplexRationalIntegerFixedPointAbstractIrrationalAbstractFloatUnsignedSignedBoolNormedFixedIrrationalFloat16Float64Float32BigFloatUInt8UInt16UInt32UInt64UInt128Int8Int16Int32Int64Int128BigIntAutomated Black-Box Boundary Value Detection

A PREPRINT

Table 11: The results for the screening over the bytecount SUT.
# Found (µ ± σ)
Time (s) Algorithm CTS

Sampling

30
30
30
30

30
30
30
30

60
60
60
60

60
60
60
60

LNS
LNS
LNS
LNS

BCS
BCS
BCS
BCS

LNS
LNS
LNS
LNS

BCS
BCS
BCS
BCS

uniform
(cid:88) uniform

bituniform
(cid:88) bituniform

uniform
(cid:88) uniform

bituniform
(cid:88) bituniform

uniform
(cid:88) uniform

bituniform
(cid:88) bituniform

uniform
(cid:88) uniform

bituniform
(cid:88) bituniform

0.0 ± 0.0
8.6 ± 0.9
7.0 ± 0.0
9.8 ± 0.8

0.0 ± 0.0
22.0 ± 1.0
55.8 ± 0.8
56.0 ± 0.7

0.0 ± 0.0
9.8 ± 0.4
7.8 ± 1.3
10.4 ± 0.9

0.0 ± 0.0
23.8 ± 0.4
56.0 ± 0.0
56.2 ± 0.4

In order to get a clustering of good ﬁt we evaluate k in the range of 1–10, and select the one having the highest Silhouette
score [36] which offers an overview of the cohesion within each cluster and the separation between different clusters
[37]. We use the default level of neighbors for orientation per data point of 15. We use Euclidean as distance metric
between feature vectors, and 200 max iterations as per the interface default. Silhouette values vary between +1 and −1,
where +1 indicates that the clusters are clearly distinguishable from each other; values of zero indicate that the clusters
are relatively close to each other such that there is little signiﬁcance in clustering the candidates. Lastly, −1 means
that the distance between candidates within the cluster is greater than the distance between different clusters, hence
indicating that the clustering performed is not appropriate and more distinct clusters are likely needed.

We repeated the runs 100 times per all combinations of features, totalling 64, and ranked the conﬁgurations according
to the silhouette mean, while selecting the model of maximum score per conﬁguration. The objective here was not to
ﬁnd perfect-ﬁt models/clusterings but to identify models of good ﬁt with high discriminatory power, because the more
clusters that can be differentiated with high accuracy, the more diverse the individual boundary candidates to present to
a tester.

The best models can be seen in Table 12. The selected feature-set was the one producing the largest number of clusters
among the top ﬁve percentile silhouette scores for k-Means over k ∈ 1, ..., 10, i.e. strlendist (WD) and Jaccard (WD
+ U). It strikes a good balance between modeling quality and ability to discriminate for the bytecount SUT that
serves for the training due to its low computational complexity and straight forward separation of boundaries within
the V domain. The clustering therefore uses the Jaccard metric for both WD and U. How/whether the feature space
generalizes well is a question for future work. The ﬁnal matrix with three features represented by four attributes over
which all clusterings in this paper are conducted is therefore:

M =






strlendist(W D)
Jaccard(W D)
Jaccard(U1)
Jaccard(U2)




 .

26

Automated Black-Box Boundary Value Detection

A PREPRINT

Table 12: The top 9 clustering conﬁgurations for bytecount with their respective average silhouette score and number of
clusters (sorted by Silhouette Score).

Conﬁguration

Silhouette Score Number of Clusters

strlendist (WD) + Jaccard (WD)
strlendist (WD) + Jaccard (WD + U)
strlendist (WD) + Jaccard (U)
Jaccard (WD + U)
strlendist (WD) + Levenshtein (U)
strlendist (WD) + Jaccard (WD + U) + Levenshtein (WD)
Levenshtein (WD) + Jaccard (U)
strlendist (WD) + Levenshtein (WD) + Jaccard (U)
strlendist (WD) + Jaccard (WD) + Levenshtein (U)

0.982
0.942
0.938
0.924
0.779
0.777
0.773
0.771
0.760

5
6
3
5
3
4
2
3
4

27

