Snow Mountain: Dataset of Audio Recordings
of The Bible in Low Resource Languages

Kavitha Raju∗, Anjaly V∗, Ryan Lish, Joel Mathew
{kavitha.raju,anjaly.v,joel}@bridgeconn.com
rslish@cobaltspeech.com

2
2
0
2

n
u
J

1

]
S
A
.
s
s
e
e
[

1
v
5
0
2
1
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Automatic Speech Recognition (ASR) has in-
creasing utility in the modern world. There are
a many ASR models available for languages
with large amounts of training data like En-
glish. However, low-resource languages are
poorly represented.
In response we create
and release an open-licensed and formatted
dataset1 of audio recordings of the Bible in
low-resource northern Indian languages. We
setup multiple experimental splits and train
and analyze two competitive ASR models to
serve as the baseline for future research using
this data.

1

Introduction

Automatic Speech Recognition (ASR) is a well-
studied problem that is motivated by multiple pop-
ular use cases like voice assistants (e.g. Amazon
Alexa, Apple Siri), live audio transcription and as
a pre-processing step for conversational machine
translation. Training models for ASR, however, are
limited by the availability of training data. Thus,
fewer ASR models are available for low resource
languages which usually do not have a sizable col-
lection of audio recording training data.

The Bible is a document that is translated
into numerous languages including multiple low-
resource languages with multiple ongoing transla-
tions. There is a parallel effort to produce audio-
recordings for the text translations. This is espe-
cially important since many of the remaining lan-
guages where the Bible is translated are commu-
nities with primarily oral learners. In fact, there
are translation projects underway that attempt to
directly translate the Bible orally without using
text.

Building ASR models for these very low re-
source languages is an important step to better

*Equal contribution.
1https://gitlab.bridgeconn.com/

software/research/datasets/snow-mountain

represent and include these communities in the
many technological advancements that are popular
among speakers of well represented languages.

In response to this need, in this paper we describe
and share freely-licensed audio recordings of 10
low resource languages of Northern India for The
Bible and train two different ASR models using the
dataset to serve as a baseline for future research.

2 Related Work

Efforts in the development of speech datasets in
Tamil, Telugu and Marathi are discussed in (Anu-
manchipalli et al., 2005) where they collected data
from about 560 speakers and trained acoustic mod-
els using the Sphinx 2 speech tool kit(Lamere et al.,
2003) in the three languages.

(Shrishrimal et al., 2012) discusses various
speech datasets developed in multiple Indian lan-
guages for ASR and Text-to-speech models. They
collect domain speciﬁc data in agriculture, market-
ing, travel and emergency services. It uses Hindi
training data recorded by 30 female speakers with
approximately 26 hours of speech recordings. A
speech dataset developed for Hindi from news bul-
letins is mentioned which is 3.5 hours and recorded
by 19 speakers (6 Male, 13 Females). Various
ongoing projects for creating speech corpora by
Linguistic Data Consortium for Indian Languages
(LDC-IL) are also underway.

A survey on the efforts made to develop speech
corpora in Indian languages is done in (Kurian,
2015).

(Deka et al., 2018) present an ongoing effort
in creation of speech corpora for under-resourced
languages of North-East India, namely, Assamese,
Bengali and Nepali.

Our dataset is unique in that it is the ﬁrst au-
dio recorded corpora available in some of the lan-
guages in the dataset while also being substantive
in terms of hours of recording when compared with
other audio recording datasets for low-resource lan-

 
 
 
 
 
 
Speaker Speaker
Gender Age

No. Language

Language

Bible
Portion Code
hin/hi
OT
Hindi
bgc
NT
Haryanvi
kfs
NT
Bilaspuri
dgo
NT
Dogri
bhd
NT
Bhadrawahi
gbk
NT
Gaddi
xnr
NT
Kangri
kfx
NT
Kulvi
mjl
Mandeali
NT
kfx-x-OSJ Speaker10 Male
Kulvi Outer Seraji NT
Speaker11 Male
bfz
NT
Pahari Mahasui

Speaker
ID
Speaker01 Female
Speaker02 Male
Speaker03 Male
Speaker04 Male
Speaker05 Male
Speaker06 Male
Speaker07 Male
Speaker08 Female
Speaker09 Female

1
2
3
4
5
6
7
8
9
10
11

43-45 yrs
43-45 yrs
45 -50 yrs
29 yrs
30 yrs
30 yrs
28-29 yrs
35-40 yrs
20 yrs
68 yrs
26-27 yrs

Table 1: Details of the languages and speakers in the dataset. The listed languages belong to the Indo-Aryan
language family. kfx-x-OSJ is a dialect of Kulvi dominant in the region of Outer Seraji in Himachal Pradesh

guages.

3 Dataset

The Snow Mountain dataset contains the audio
recordings (in .mp3 format) and the correspond-
ing text of The Bible in 11 Indian languages. The
recordings were done in a studio setting by native
speakers. Each language has a single speaker in
the dataset. Most of these languages are geograph-
ically concentrated in the Northern part of India
around the state of Himachal Pradesh. Being re-
lated to Hindi they all use the Devanagari script
for transcription. Details of the dataset are shown
in tables 1 and 2, including the languages’ infor-
mation (Eberhard et al., 2022), speaker details and
duration of audio.

The protestant Bible is composed of 66 canon-
ical books which are portions of text of various
sizes. These are split into two parts or testaments:
the Old Testament (OT) consists of 39 books and
the New Testament (NT) consists of 27 books. In
the dataset, in the place of using full Bible book
names, a 3-letter code is used. The tables 5 and
6 in appendix lists the Books in the Bible and the
code used for them (Societies, 2018). Each book
is further divided into chapters in which the text is
split into sentences known as verses. For example,
shown below are all the verses (marked at the be-
ginning of sentences) of chapter 23 of the Biblical
book of ’Psalms’ part of the Old Testament (OT).

Psalms 23:
1 The Lord is my shepherd; I shall not want.
2 He maketh me to lie down in green pastures: he
leadeth me beside the still waters.

3 He restoreth my soul: he leadeth me in the paths
of righteousness for his name’s sake.

4 Yea, though I walk through the valley of the
shadow of death, I will fear no evil: for thou art
with me; thy rod and thy staff they comfort me.

5 Thou preparest a table before me in the presence
of mine enemies: thou anointest my head with oil;
my cup runneth over.

6 Surely goodness and mercy shall follow me all
the days of my life: and I will dwell in the house
of the Lord for ever.

To make the dataset easier to use for training au-
tomatic models, we perform some post-processing
steps and make the data available in the following
ways (kept in separate directories):

3.1 Raw Data

The original audio recordings of the Bible in dif-
ferent languages are stored as one .mp3 ﬁle per
chapter of each Bible book. They are named in the
pattern <Book-code_ chapter-number.mp3>. The
recordings, along with the Bible text, sometimes
contain brief introductions to books and chapters
as well as recordings of section headings within
the chapters. We provide a timestamp ﬁle for each
chapter’s audio recording ﬁle that demarcates the
start times of different parts within. The timestamp
ﬁles have the same name as their corresponding
chapter recording ﬁle except that they have the .tsv
ﬁle extension.

The corresponding Bible text is stored in the Uni-
versal Scripture Format Markers (USFM) format
which is a popular format among Bible translation
agencies. For ease of consuming this data, we parse
the USFM ﬁle and provide the textual content in

No. Language

1
2
3
4
5
6
7
8
9
10
11

Hindi
Haryanvi
Bilaspuri
Dogri
Gaddi
Bhadrawahi
Kangri
Kulvi
Mandeali
Pahari Mahasui
Kulvi Outer Seraji

Chapters Duration Cleaned- Duration
(cleaned)
67.62
25.64
22.67
20.57
20.10
18.36
19.20
21.22
20.20
17.72
19.62

verses
22751
7702
7163
7735
7731
6891
7241
7033
6763
6929
6995

71.41
27.41
26.26
22.28
21.81
22.16
22.28
25.30
25.38
21.33
23.64

928
260
260
260
260
260
260
260
260
260
260

Short- Duration
verses
11511
3037
3022
4578
4769
4114
4368
3319
3353
4430
3816

(short)
22.60
6.29
6.19
9.14
9.31
8.10
8.65
6.76
6.83
8.68
7.49

Table 2: Language wise data size

a .csv format. This data is kept in the raw/text di-
rectory, one ﬁle per book, named with the 3-letter
book code.

3.2 Cleaned Data

In order to train ASR models with this data we per-
form the following pre-processing steps. First, we
remove instances of the recordings where we found
inconsistencies between the timestamps, text and
the audio recording ﬁle. Then, we break-up each
chapter recording ﬁle into separate verse recording
ﬁles which we store in the .wav format. The audio
ﬁles are named in the pattern <book-code_chapter-
number_ verse-number.wav>, and are referenced in
the path ﬁeld of every dataset ﬁle in the experiments
and the cleaned directories. Finally, we remove any
verse recording ﬁle which is more than 10 seconds
in duration, in the short_verses, to support training
ASR models that may hit ‘Out of memory’ errors
during training otherwise. These are kept in the
cleaned directory.

3.3 Experiment Splits

One of our goals in creating this dataset is to use
it to build ASR systems for very low resource lan-
guages. We want such a model to be trained with
minimal data so as to be useful in cases where Bible
translations are done orally ﬁrst. In such a scenario,
we assume the recordings of the Bible are available
but the text version needs to be produced. Thus,
eliciting little manual transcription, we desire to
train a reasonable ASR model that can then aid the
transcribers with the remaining work while being
able to incrementally re-train the model to steadily
improve output quality using manually corrected

data.

Another requirement for us to be able to use this
model effectively is that it should perform well on
a test set that is not similar to the dataset it has been
built from in terms of Bible books. The vocabulary
and style of language can vary considerably across
Bible Books. Thus, it becomes relevant to ensure
stable performance of the model when trained on
one type of text and then tested on a different type.
We create splits of the cleaned data (3.2) for
training and analysing the performance of ASR
models. The splits are available in the experiments
directory. The ﬁle names indicate the experiment
and the split category (see Table 7).

The ﬁrst category of the splits is based on the
size of the training data. For this we create training
splits of the sizes 500, 1000 and 2500 verses respec-
tively. Similarly, we create a split with only short
verses (those with less than or equal to 10 seconds
in length) which contain _short in their ﬁlenames.
We also create a split with the full data (_full). Each
of these are further divided into training and eval-
uation sets with an 8:2 ratio (e.g. train_1000 and
val_1000). For our experiments we use a single
disjoint test set of 500 verses (~1 hour) which is
speciﬁed by the ﬁle test_common.csv.

The other category for the splits is based on the
difference in writing styles between the Biblical
books. For this we create a training (and evaluation)
set using only the Gospels (MAT, MRK, LUK and
JHN) all of which describe the life of Jesus on earth
and then create different test sets from the book of
The Acts of the Holy Spirit (ACT- describes events
after the resurrection; test_acts.csv), the epistles
(ROM, 1CO, 2CO, GAL) , test_letters.csv and the

last books in the NT (1JN, 2JN, 3JN, JUD, REV)
test_lastbooks. These splits have been created after
analyzing the word overlap between the books and
identifying groups of most dissimilar books in the
NT of these languages.

4 Experiments

We train two different ASR models and compare
results on the the training splits deﬁned in section
3.3.

4.1 Methodology

4.1.1 Wav2vec XLS-R

We use the pretrained wav2vec2 2.0 (Baevski et al.,
2020) based XLS-R model (Babu et al., 2021) that
was released on HuggingFace 2 and ﬁne-tune it on
our data. XLS-R used almost half a million hours
of audio data in 128 languages for self-supervised
pre-training and provides pre-trained models with
300 million up to two billion parameters. XLS-R
learns contextualized speech representations by ran-
domly masking feature vectors before passing them
to a transformer network during self-supervised
pre-training. For ﬁne-tuning, a single linear layer
is added on top of the pre-trained network to train
the model on labelled data of the downstream tasks
such as speech recognition, speech translation and
audio classiﬁcation. In our use case, which is ASR,
we ﬁne-tune the base model separately for each
language.

Tokenizer: After basic pre-processing and
cleaning of text data, we create a CTC-tokenizer
(Hori et al., 2017) with the following characteris-
tics in its vocabulary: contains all unique letters of
the language, the space character replaced by “|”,
“UNK” token for unknown characters and “PAD”
for blank-token as required by the CTC algorithm.
Feature Extractor: Wav2Vec2FeatureExtractor
is used with the following conﬁguration, similar to
or as required by the base models: Sampling rate
16kHz, feature size 1, padding with 0.0 for shorter
inputs, with normalizing and returning attention
mask.

Data Collator: The data collator pads the input
dynamically to match sequence size to the longest
input in a batch and also treats the input values
and labels differently since they are of different
modalities.

Evaluation Metric: Word error rate (WER) is

used as the evaluation metric which is the predomi-
nant metric in ASR.

The pre-trained checkpoint of base model is
loaded with attention, hidden and feature projection
dropout = 0.0, mask time probability=0.05, no layer
drops and setting the CTC loss reduction="mean".
And the model’s feature extractor is frozen to
avoid further ﬁne-tuning the initial CNN layers that
extract acoustically meaningful but contextually
independent features from the raw speech signal.
Training: The following values were used alike
across all experiments: batches were grouped by
length with 8 batches per device, gradient accumu-
lation steps=2, evaluation strategy=’steps’, gradient
checkpointing=True, fp16 training with a learing
rate of 3e-4.

The following values were adjusted as per the
size of input dataset: training epochs; save, eval
and warmup steps.

Long input sequences require a lot of memory.
Since XLS-R is based on self-attention, the mem-
ory requirements scale quadratically with the input
length. In all our experiments we use only verses
that are 10 seconds or less in duration except in the
’Full dataset’ using which we were not able to train
the XLS-R model successfully.

The results shown in table 3 are those using the
1billion model3 which gave better results than the
300million model4 even though for training speed
and GPU memory usage the 300million model was
found to be better.

4.1.2 Kaldi Toolkit

Kaldi (Povey et al., 2011) is a free and open-
source speech recognition toolkit licensed under
the Apache 2.0 License5. It is primarily written
in C++.
Important features of Kaldi are: ﬁnite
state transducer-based framework (using OpenFst
toolkit (Allauzen et al., 2007)), extensive linear al-
gebra support with BLAS (Blackford et al., 2002)
and LAPACK (Anderson et al., 1999), extensible
design, non-restrictive license, availability of com-
plete recipes for building speech recognition sys-
tems and thorough test routines. Adding new fea-
ture without modifying the existing modules makes
Kaldi’s design extensible.

3https://huggingface.co/facebook/

wav2vec2-xls-r-1b

4https://huggingface.co/facebook/

wav2vec2-xls-r-300m

5https://www.apache.org/licenses/

2https://huggingface.co/facebook

LICENSE-2.0.html

Language Dataset

Wav2vec XLS-R

Kaldi

Hindi

Bilaspuri

Dogri

Haryanvi

Eval

-
3.59
8.40
10.83
12.58
-
14.40
16.08
18.45
23.56
-
13.74
18.68
18.35
23.81
-
25.63
25.67
33.67
33.15

Full data
Short verses
2500
1000
500
Full data
Short verses
2500
1000
500
Full data
Short verses
2500
1000
500
Full data
Short verses
2500
1000
500

Test

Test
w/o LM w/ LM
-
3.66
8.31
11.35
14.49
-
16.08
17.92
21.64
25.74
-
13.59
17.26
19.32
25.25
-
23.87
24.13
29.78
30.90

-
2.38
6.23
9.07
12.14
-
13.02
13.9
17.96
22.94
-
11.72
14.05
16.56
21.65
-
20.65
20.28
25.43
27.70

Eval

2.93
5.22
14.08
22.87
37.06
10.10
15.31
15.47
21.64
30.20
11.89
14.62
17.09
21.26
29.91
16.84
28.85
25.98
34.48
39.94

Test
w/ LM
3.36
5.18
12.12
21.23
36.31
10.26
16.16
16.21
23.58
31.79
11.91
14.63
16.63
22.95
30.26
18.79
24.14
25.07
30.45
37.66

Table 3: WER for the experiments using datasets of different sizes

We used the tedlium s5_r2 recipe6 to train our
Hindi acoustic model. Speciﬁcally, we used the lo-
cal/chain/tuning/run_tdnn_1g.sh script for prepar-
ing and training the TDNN-f(Povey et al., 2018)
model. We used the Hindi AM as a base model
for transfer learning to train the other languages.
For transfer learning, we roughly followed the ap-
proach used in Kaldi’s rm s5 recipe, namely the
local/chain/tuning/run_tdnn_wsj_rm_1c.sh script.
The following steps were involved in building an
ASR system using Kaldi.

Data preparation: Kaldi needs the input audio
ﬁles and their corresponding transcripts in a par-
ticular format for training and testing. For record-
ings of around 10 seconds duration, we need three
ﬁles: wav.scp, text, and utt2spk. For recordings
of longer duration an additional segments ﬁle is
required. The details of each of these ﬁles are:

• text ﬁle contains the transcriptions of each

utterance
Format: <utterance-id> <transcript>

• utt2spk ﬁle says, for each utterance, which

6https://github.com/kaldi-asr/kaldi/

blob/master/egs/tedlium/s5_r2

speaker (denoted by an ID) spoke it
Format: <utterance-id> <speaker-id>

• wav.scp ﬁle

Format: <recording-id> <extended-ﬁlename>
where the "extended-ﬁlename" may be an ac-
tual ﬁlename, or a command that produces a
.wav ﬁle

• Segments ﬁle

<utterance-id> <recording-id>

Format:
<segment-begin> <segment-end>
where the segment-begin and segment-end
are measured in seconds.

Lexicon and Language Model: Typical pro-
nunciation dictionaries map between the ortho-
graphical representation of a word and the sequence
of phones in its broad phonetic transcription. Typ-
ically symbols from systems such as ARPAbet or
X-SAMPA are used to represent the phones, but
creating a pronunciation dictionary from scratch
is a potentially time-consuming and expensive un-
dertaking. Annotators trained in phonetics and the
target language must manually write thousands of
entries, which can be used to train a grapheme-
to-phoneme (G2P) model that can generate auto-

Language Trainset

Wav2vec XLS-R

Bilaspuri Gospels
Dogri
Gospels
Haryanvi Gospels

Eval
14.58
12.66
23.24

Acts
22.92
16.90
34.90

Letters Lastbooks Eval
17.02
24.80
18.57
15.11
26.90
17.38
22.65
64.37
27.14

Acts
25.66
22.24
38.04

Kaldi
Letters Lastbooks
22.78
21.7
26.9

32.1
30.0
58.1

Table 4: WER for the Book-wise experiments

matic pronunciations for the rest of the words in the
dataset. Noting that northern Indian languages are
quite consistent in their orthography and pronunci-
ation, we decided to use the Unicode(Consortium,
2021) characters for Devanagari as our phone set.
This allows us to generate the entire lexicon with
a simple script, without the potential inconsisten-
cies and errors human annotators and G2P models
might introduce.

The Bible text is used to create a lexicon (or the
pronunciation dictionary) by listing all words in the
Bible along with its pronunciation, which are its
Unicode characters separated by a space character.

Figure 1: Excerpt of the Hindi lexicon used

The language model is built using the MIT
Language Modelling Toolkit7 (MITLM) using the
Bible text after removing punctuation and normal-
izing white space. This toolkit is a set of tools
designed for the efﬁcient estimation of statistical
n-gram language models. We exclude the evalua-
tion and test set from the language model training
and for the low resource languages, we include the
Hindi data for training the language model since
these are closely related.

Feature extraction: Kaldi’s feature extraction
and waveform-reading module creates standard
MFCC (Mel-frequency Cepstral Coefﬁcients) fea-
tures. Since this module requires the audio record-
ings in the .wav format we convert the .mp3 ﬁles
with a 16 KHz sampling frequency.

Acoustic modelling: Once the features are ex-
tracted, we train a GMM-HMM based acoustic

7https://github.com/mitlm/mitlm

model to generate alignments (phoneme-to-audio
alignments) for the training audio. Using this align-
ment data, a DNN (Deep Neural Network) based
acoustic model is trained. The acoustic model is
a factorized time delay neural network (TDNN-f)
(Povey et al., 2018) with 13 hidden layers, 9882368
parameters, and 3456 outputs. It is a "chain" model,
which uses the sequence-level objective function
known as lattice-free MMI (Povey et al., 2016).

4.2 Results

The Word Error Rate (WER) for the ASR models
trained on the different data splits are shown in
Tables 3 and 4. Due to an ’Out of Memory’ error
the ’Full data’ experiment could not be run using
the Wav2vec XLS-R model.

5 Discussion

Overall, both the trained ASR models show com-
parable results for the larger sized datasets (Ta-
ble 3). Wav2vec XLS-R outperforms Kaldi on
smaller sized datasets. Also, for Hindi, Wav2vec
XLS-R shows signiﬁcant improvements over Kaldi.
We attribute both of these observations to the power
of pre-training on a large corpus including Hindi
data. Also, using a language model, even if not
trained on any extraneous text (only the train set)
reasonably improves the WER.

We observe lower results in the experiments run
for Haryanvi language which we understand is due
to the poor data quality, speciﬁcally mismatches
between the timestamp and audio ﬁles.

The experiments on the books with different writ-
ing styles show poorer results. Factors that con-
tribute to this include the low word-level overlap
and the shift in discourse/content styles between
the train and test splits.

6 Future Work

We plan to run experiments for the remaining lan-
guages in the dataset and continue to release more
data in other languages as it becomes available to

us from partnering Bible translation agencies. An-
other direction is training multi-lingual ASR mod-
els to overcome limitations of the dataset being
single-speaker.

speech recognition. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 518–
529, Vancouver, Canada. Association for Computa-
tional Linguistics.

Cini Kurian. 2015. A review on speech corpus devel-
opment for automatic speech recognition in indian
languages. International Journal of Advanced Net-
working and Applications, 6(6):2556.

Paul Lamere, Philip Kwok, Evandro Gouvêa, Bhiksha
Raj, Rita Singh, William Walker, Manfred Warmuth,
and Peter Wolf. 2003. The cmu sphinx-4 speech
recognition system.

Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li,
Hainan Xu, Mahsa Yarmohammadi, and Sanjeev
Khudanpur. 2018. Semi-Orthogonal Low-Rank Ma-
In
trix Factorization for Deep Neural Networks.
Proc. Interspeech 2018, pages 3743–3747.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Na-
gendra Goel, Mirko Hannemann, Yanmin Qian, Petr
Schwarz, and Georg Stemmer. 2011. The kaldi
speech recognition toolkit. In In IEEE 2011 work-
shop.

Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-
gah Ghahremani, Vimal Manohar, Xingyu Na, Yim-
ing Wang, and Sanjeev Khudanpur. 2016. Purely
sequence-trained neural networks for asr based on
lattice-free mmi. In Interspeech, pages 2751–2755.

Pukhraj P Shrishrimal, Ratnadeep R Deshmukh, and
Vishal B Waghmare. 2012. Indian language speech
database: A review. International journal of Com-
puter applications, 47(5):17–21.

United Bible Societies. 2018. USFM Documentation.

References

Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efﬁcient weighted ﬁnite-state transducer
In Implementation and Application of Au-
library.
tomata, pages 11–23, Berlin, Heidelberg. Springer
Berlin Heidelberg.

E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Dem-
mel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, and D. Sorensen.
1999. LAPACK Users’ Guide, third edition. Society
for Industrial and Applied Mathematics, Philadel-
phia, PA.

Gopalakrishna Anumanchipalli, Rahul Chitturi, Sachin
Joshi, Rohit Kumar, Satinder Pal Singh, RNV
Sitaram, and SP Kishore. 2005. Development of in-
dian language speech databases for large vocabulary
speech recognition systems. In Proc. SPECOM.

Arun Babu, Changhan Wang, Andros Tjandra, Kushal
Lakhotia, Qiantong Xu, Naman Goyal, Kritika
Singh, Patrick von Platen, Yatharth Saraf, Juan Pino,
Alexei Baevski, Alexis Conneau, and Michael Auli.
2021. Xls-r: Self-supervised cross-lingual speech
representation learning at scale.

Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations.

L Susan Blackford, Antoine Petitet, Roldan Pozo,
Karin Remington, R Clint Whaley, James Demmel,
Jack Dongarra, Iain Duff, Sven Hammarling, Greg
Henry, et al. 2002. An updated set of basic linear
algebra subprograms (blas). ACM Transactions on
Mathematical Software, 28(2):135–151.

The Unicode Consortium. 2021. The unicode standard,

version 14.0.0.

Barsha Deka, Joyshree Chakraborty, Abhishek Dey,
Shikhamoni Nath, Priyankoo Sarmah, SR Nirmala,
and Samudra Vijaya. 2018. Speech corpora of un-
der resourced languages of north-east india.
In
2018 Oriental COCOSDA-International Conference
on Speech Database and Assessments, pages 72–77.
IEEE.

David M. Eberhard, Gary F. Simons, and Charles D
Fennig. 2022. Ethnologue: Languages of the World
twenty-ﬁfth edition.

Takaaki Hori, Shinji Watanabe, and John Hershey.
2017. Joint CTC/attention decoding for end-to-end

A Bible Books and 3-letter Codes

The tables 5 and 6 show the books in the protestant
Bible, and their codes used in the dataset.

B Sizes of each experiment set

Table 7 shows the number of verses in each experi-
ment split. The training and validation dataset are
split in an 8:2 ratio.

Comments
‘1 Moses’ in some Bibles
‘2 Moses’ in some Bibles
‘3 Moses’ in some Bibles
‘4 Moses’ in some Bibles
‘5 Moses’ in some Bibles

No. Book Code Book Name
GEN
1
EXO
2
LEV
3
NUM
4
DEU
5
JOS
6
JDG
7
RUT
8
1SA
9
2SA
10
1KI
11
2KI
12
1CH
13
2CH
14
EZR
15
NEH
16
EST
17
JOB
18
PSA
19
PRO
20
ECC
21
SNG
22
ISA
23
JER
24
LAM
25
EZK
26
DAN
27
HOS
28
JOL
29
AMO
30
OBA
31
JON
32
33 MIC
NAM
34
HAB
35
ZEP
36
HAG
37
38
ZEC
39 MAL

1 Kings or Kingdoms in Orthodox Bibles
2 Kings or Kingdoms in Orthodox Bibles
3 Kings or Kingdoms in Orthodox Bibles
4 Kings or Kingdoms in Orthodox Bibles
1 Paralipomenon in Orthodox Bibles
2 Paralipomenon in Orthodox Bibles
This is for Hebrew Ezra (1 Ezra, 1 Esdras)
Sometimes appended to Ezra; called 2 Esdras in the Vulgate

Genesis
Exodus
Leviticus
Numbers
Deuteronomy
Joshua
Judges
Ruth
1 Samuel
2 Samuel
1 Kings
2 Kings
1 Chronicles
2 Chronicles
Ezra
Nehemiah
Esther (Hebrew) This is for Hebrew Esther; for the longer Greek LXX Esther use ESG
Job
Psalms
Proverbs
Ecclesiastes
Song of Songs
Isaiah
Jeremiah
Lamentations
Ezekiel
Daniel (Hebrew) This is for Hebrew Daniel; for the longer Greek LXX Daniel use DAG
Hosea
Joel
Amos
Obadiah
Jonah
Micah
Nahum
Habakkuk
Zephaniah
Haggai
Zechariah
Malachi

150 Psalms in Hebrew
31 Proverbs, but 24 Proverbs in the Ethiopian Bible
3Qoholeth in Catholic Bibles; for Ecclesiasticus use SIR
Song of Solomon, or Canticles of Canticles in Catholic Bibles

The Book of Jeremiah; for the Letter of Jeremiah use LJE
The Lamentations of Jeremiah

Table 5: Books contained in the Old Testament(OT) of The Bible

No. Book Code Book Name
40 MAT
41 MRK
LUK
42
JHN
43
ACT
44
ROM
45
1CO
46
2CO
47
GAL
48
EPH
49
PHP
50
COL
51
1TH
52
2TH
53
1TI
54
2TI
55
TIT
56
PHM
57
HEB
58
JAS
59
1PE
60
2PE
61
1JN
62
2JN
63
3JN
64
JUD
65
REV
66

Comments
The Gospel according to Matthew
The Gospel according to Mark
The Gospel according to Luke
The Gospel according to John
The Acts of the Apostles
The Letter of Paul to the Romans
The First Letter of Paul to the Corinthians
The Second Letter of Paul to the Corinthians
The Letter of Paul to the Galatians
The Letter of Paul to the Ephesians
The Letter of Paul to the Philippians
The Letter of Paul to the Colossians

Matthew
Mark
Luke
John
Acts
Romans
1 Corinthians
2 Corinthians
Galatians
Ephesians
Philippians
Colossians
1 Thessalonians The First Letter of Paul to the Thessalonians
2 Thessalonians The Second Letter of Paul to the Thessalonians
1 Timothy
2 Timothy
Titus
Philemon
Hebrews
James
1 Peter
2 Peter
1 John
2 John
3 John
Jude
Revelation

The First Letter of Paul to Timothy
The Second Letter of Paul to Timothy
The Letter of Paul to Titus
The Letter of Paul to Philemon
The Letter to the Hebrews
The Letter of James
The First Letter of Peter
The Second Letter of Peter
The First Letter of John
The Second Letter of John
The Third Letter of John
The Letter of Jude
The Revelation to John; called Apocalypse in Catholic Bibles

Table 6: Books contained in the New Testament (NT) of The Bible

Dataset
all_verses.csv
short_verses.csv
test_common.csv
train_full.csv
val_full.csv
train_short.csv
val_short.csv
train_2500.csv
val_2500.csv
train_1000.csv
val_1000.csv
train_500.csv
val_500.csv
train_gospels.csv
val_gospels.csv
test_acts.csv
test_letters.csv
test_lastbooks.csv

Hindi Bilaspuri Dogri Haryanvi
22751
11511
500
17800
4451
8808
2203
2000
500
800
200
400
100
-
-
-
-
-

7702
3037
500
5757
1440
2029
508
2000
500
800
200
400
100
1482
371
334
356
132

7735
4578
500
5786
1447
3261
816
2000
500
800
200
400
100
1926
482
517
730
232

7163
3022
500
5328
1333
2017
505
2000
500
800
200
400
100
1360
340
396
370
137

Table 7: Number of verses in each dataset ﬁle

