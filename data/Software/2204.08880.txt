Highlights

Antipatterns in Software Classiﬁcation Taxonomies

Cezar Sas, Andrea Capiluppi

• Current software classiﬁcation datasets lack generalizability.

• We identiﬁed 7 antipatterns that pervade software classiﬁcation.

• Hierarchical aggregation of categories can reduce some of the antipat-

terns.

2
2
0
2

r
p
A
9
1

]
E
S
.
s
c
[

1
v
0
8
8
8
0
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Antipatterns in Software Classiﬁcation Taxonomies

Cezar Sasa,∗, Andrea Capiluppia

aBernulli Institute, University of Groningen, Nijenborgh 9, Groningen, 9747
AG, Netherlands

Abstract

Empirical results in software engineering have long started to show that
ﬁndings are unlikely to be applicable to all software systems, or any domain:
results need to be evaluated in speciﬁed contexts, and limited to the type of
systems that they were extracted from. This is a known issue, and requires
the establishment of a classiﬁcation of software types.

This paper makes two contributions: the ﬁrst is to evaluate the quality
of the current software classiﬁcations landscape. The second is to perform a
case study showing how to create a classiﬁcation of software types using a
curated set of software systems.

Our contributions show that existing, and very likely even new, classiﬁ-
cation attempts are deemed to fail for one or more issues, that we named
as the ‘antipatterns’ of software classiﬁcation tasks. We collected 7 of these
antipatterns that emerge from both our case study, and the existing classiﬁ-
cations.

These antipatterns represent recurring issues in a classiﬁcation, so we dis-
cuss practical ways to help researchers avoid these pitfalls. It becomes clear
that classiﬁcation attempts must also face the daunting task of formulating
a taxonomy of software types, with the objective of establishing a hierarchy
of categories in a classiﬁcation.

Keywords: classiﬁcation, software types, antipattern, taxonomy, machine
learning, natural language processing

∗Corresponding Author
Email addresses: c.a.sas@rug.nl (Cezar Sas), a.capiluppi@rug.nl (Andrea

Capiluppi)

Preprint submitted to Journal of Systems and Software

April 20, 2022

1. Introduction

In the context of empirical software engineering research, the main goal
of empirical papers is to achieve the generality of the results. The most com-
mon approach in doing so is to analyse projects having diﬀerent application
domains to decrease threats due to the generalizability of the results: as only
a few examples, the work in [1] analyses a collection of 200,000 mobile sys-
tems, [2] examples 1,500 GitHub systems based on their popularity, while [3]
is based on 165,000 GitHub projects based on the Travis CI. As a side eﬀect,
the domain, context and uniqueness of a software system have not been con-
sidered very often by researchers as driving factors for detecting similarities
or diﬀerences between software systems.

In parallel, there has been a call for ‘context-driven software engineering
research’ [4, 5]: in the testing and veriﬁcation ﬁelds, for example, the set of
assumptions is obviously speciﬁc to the systems under test, and those as-
sumptions are based on the type of system, the development process, and
other factors. Although the diversity and context of software systems have
received some attention in the past [6, 7], contemporary research in the com-
puting ﬁeld is almost entirely application-independent. This has not always
been the case - early in the computing era, ‘there were totally separate appli-
cation domains (for example, scientiﬁc and data processing) and the research
focus was often application-speciﬁc’ [8].

From the practitioners’ point of view, categories and types of software
systems are an important aspect to consider. Well known collaborative plat-
forms like GitHub, that contain very large amounts of software repositories,
show an increasing need to search and retrieve repositories based on their
semantics. As a solution, GitHub has started to propose a service called Top-
ics 1, that allows developers to annotate their projects manually, and other
users to search software via these topics. GitHub also provides the means
to create Collections 2 (previously named Showcases), that is a curated list
of topics where good quality repositories are grouped and showcased un-
der the same umbrella. However, both these solutions have various issues:
for example, Topics are only optional features to a hosted software project,
while GitHub does not suggest or restrict its usage in any way. As a result,
there are plenty of similar (or identical, with a diﬀerent morphological form)

1https://github.com/topics
2https://github.com/collections

2

topics, making the search less eﬀective. On the other hand, the Collections
list is manually curated; therefore, it is not scalable to all topics, reducing
the eﬀectiveness of ﬁnding repositories, especially those annotated with non-
popular topics. Furthermore, developers tend to not use these tools, or using
topics that are not helpful to retrieve their code (e.g., using programming
languages).

The call for context-driven software engineering research, easier retrieval
of relevant projects using semantics, and the extra burden put on the de-
velopers to label their project with all the correct labels requires a more
automated way to label software projects.

From past research and eﬀorts, there have been several approaches to
perform software classiﬁcation, and depending on what seed classiﬁcation
has been used as a stepping stone. In some cases, the seed was initiated with
a top-down approach, i.e., using an external classiﬁcation [9, 10]: researchers
would then use the categories (or labels) of the given classiﬁcation to ﬁt a
sample of software projects. In other cases, categories were generated by the
researchers [11], and the software projects assigned to the categories using
again a top-down approach. Finally, a bottom-up approach was used when
researchers used, as categories, the labels assigned by developers to their own
software [12].

Moreover, there are various artefacts that can be used to perform the
classiﬁcation, from README ﬁles, to the source code. These two approaches
are very diﬀerent in terms of diﬃculty, for example the README might be
lacking or containing irrelevant information about the repository content
like information regarding the building of the code. Whereas the source
code, there are hundreds or thousands of ﬁles, each containing some relevant
semantic information that needs to be aggregated keeping track not only
about the frequency but also about the interactions of the ﬁles.

The misclassiﬁcation, or lack of it, has various implications both on the
repository, and also on the research that makes use of the labels. The de-
veloper might struggle to ﬁnd contributors for their new and less popular
repositories, as these are unable to discover and use the code. Furthermore,
research that uses poorly labeled projects might infer wrong patterns and
give bad advice to practitioners.

In this work, we evaluate several existing software classiﬁcations proposed
in the literature. The selection criteria of these works comprise (i) research
papers that attempt a classiﬁcation of application domains, and (ii) research
works that made their data available. While analysing the resulting body of

3

research works, we came across a number of recurring issues that researchers
struggled with. These represent the most common antipatterns in classifying
software systems.

Similarly to the work in [13] that highlighted the pitfalls of mining soft-
ware repositories, the goal of our work is to analyse existing classiﬁcations
from past datasets, and to present a list of common antipatterns that re-
searchers encountered when creating them.

In this work we focus on the following research questions:

RQ1 - How is the quality of existing software classiﬁcation datasets?

RQ2 - What are the antipatterns of creating a software classiﬁcation dataset

or a taxonomy of software application domains?

RQ3 - How can we improve software classiﬁcations and move towards a
universal taxonomy that can be actively shared and used?

This paper presents two main contributions: ﬁrst, we perform a case
study attempting to create a classiﬁcation for software systems that mini-
mizes common issues present in current datasets’ classiﬁcation. Second, using
the acquired experience and inductive analysis, we distil a set of 7 common
antipatterns that researchers have encountered while attempting to classify
software systems. These antipatterns might have happened (and are likely
to happen again) when the researchers (i) create their own labels, (ii) use a
pre-deﬁned classiﬁcation of labels, or (iii) use the labels manually entered by
software developers. A visual representation of our pipeline is presented in
Figure 1. For the sake of replication, we have made all our data3 and code 4
publicly available.

The rest of this work is structured as follows: in Section 2 we will give
an overview of previous work with a focus on the used classiﬁcations. Using
the evidence obtained from the analysis of the datasets, in Section 3 we sum-
marise the antipatterns when creating a software classiﬁcation. In Section 4
we present a case study on creating a software classiﬁcation and resolving
In Section 5 we discuss the threats to validity to our work.
some issues.
Finally, we present our conclusions and discuss future developments for our
work in Section 6.

3http://doi.org/10.5281/zenodo.5018234
4https://github.com/SasCezar/ComponentSemantics/blob/dev-
semantic/componentSemantics/class term based classiﬁcation.ipynb

4

Figure 1: Pipeline used to identify the antipatterns in software application domain classi-
ﬁcation datasets.

2. Related Work and Existing Taxonomies

There have been several attempts in the literature focusing on software
classiﬁcation:
in our paper we choose to only focus on those performing a
classiﬁcation of application domains. In general, all those previous works use
their own datasets and diﬀerent classiﬁcations. This generates an even more
broad issue: it becomes hard to have a real applicability of these approaches,
or an agreement on a shared benchmark.

While this paper is not a systematic literature review, the analyzed works
have been selected using a similar approach to a systematic literature review.
We retrieved the past works that (1) focused on the classiﬁcation of software
into application domains, and that (2) are proposing a new dataset.
In
order to perform this query, we performed an initial search on computer
science bibliography services like dblp, Google Scholar, and Arxiv. We used
the following terms: ‘software categorization’, ‘software classiﬁcation’, ‘github
repository classiﬁcation’, and ‘software similarity’. We perform a ﬁrst stage
to validate the relevance of each work, we ﬁlter on the results using the title
and abstracts. The works that passed the ﬁrst ﬁltering were subsequently
used to perform a manual forward and backward snowballing for further
relevant papers. Works that are case limits, were kept if their method or
dataset can be used to perform software classiﬁcation (e.g., [14], [11]).

The list of papers that form the result of our search are listed in Table 2,
and it spans a window of 15 years (2006 to 2020). The approaches used
to perform the classiﬁcation task of software projects in the retrieved works
varies: from project metrics [15], to source code [9], and binary data [16]. In
this paper, we focus on the approaches based on:

5

QueryFilterDatasetCollectionAnalysisAntipatternsDatasetsRelevantPapersInitialRetrievalSnowballingBibliographicDatabasesAugmentedListTable 1: List of works divided by the diﬀerent data source used.

Data Source

Works

Source Code
Other Project Data

[18, 19, 9, 20, 21, 22, 23, 14, 24]
[25, 26, 10, 27, 28, 29, 30, 11]

(A) source code; and

(B) other project data (e.g., README ﬁles),

as we are interested in the classiﬁcation task using semantic information,
and structural (can be extracted from source code). Table 1 contains a list
of the works divided by their approach.

Below we provide, for each of the used information source a summary of
the representative works with a focus on the work’s dataset. A more in detail
review of the software classiﬁcation task landscape is presented in [17].

2.1. Source Code Approaches

One of the initial works on software classiﬁcation is MUDABlue [18],
which applied information retrieval techniques to classify software into 6
In particular, the authors used Latent Semantic
SourceForge categories.
Analysis (LSA), on the source code identiﬁers of 41 projects written in C.

Following MUDABlue, Tian et al. proposed LACT [19], an approach
based on Latent Dirichlet Allocation (LDA), a generative probabilistic model
that retrieves topics from textual datasets, to perform the classiﬁcation task
from the identiﬁers and comments in the source code. In addition, the au-
thors use a heuristic to cluster similar software. The authors use a dataset
of 43 examples divided in 6 SourceForge categories. The list of projects is
available in their paper.

A diﬀerent approach was adopted in [9], the authors used API packages,
classes, and methods names and extracted the words using the naming con-
ventions. Using the example in [31], the authors use information gain to
select the best attributes as input to diﬀerent machine learning methods for
the task of classifying 3,286 Java projects into 22 SourceForge categories.
Their dataset is not available anymore.

LeClair et al. [20] used a neural network approach. The authors use
the project name, function name, and the function content as input to a

6

C-LSTM [32], a combined model of convolutional and recurrent neural net-
works. Their dataset is made of 9,804 software projects, with the annotations
from the Debian packages repository. The authors only analysed programs
containing C/C++ source code, divided into 75 categories: many of these
categories have only a few examples, and 19 are duplicate categories with
diﬀerent surface form, more speciﬁcally ‘contrib/X’, where X is a category
present in the list.

CLAN [21] provides a way to detect similar apps based on the idea that
similar apps share some semantic anchors. Given a set of applications, the
authors create two terms-document matrices, one for the structural infor-
mation using the package and API calls, the other for textual information
using the class and API calls. Both matrices are reduced using LSA, then,
the similarity across all applications is computed. Lastly, the authors com-
bine the similarities from the packages and classes by summing the entries.
The data is not available. In [22], the authors propose CLANdroid, a CLAN
adaptation to the Android apps domain, and evaluate the solution on 14,450
Android apps. Their dataset is not available.

Another unsupervised approach was adopted by LASCAD [23], a lan-
guage agnostic classiﬁcation and similarity tool. As in LACT, the authors
used LDA over the source code, and further applied hierarchical clustering
with cosine similarity on the output topic terms matrix of LDA to merge
similar topics. The authors also proposed two datasets: an annotated one
consisting of 103 projects divided in 6 categories (from GitHub Collections)
with 16 programming languages (although many languages have only 1 ex-
ample), and an unlabeled one which is not available.

Taking a more Natural Language Processing (NLP) inspired approach,
based on the distributional hypothesis: ‘A word is characterized by the com-
pany it keeps’ [33], [14] proposed a neural network solution to create dense
representation (i.e., embeddings) of libraries. The authors used the co-
occurrences of import statements of libraries to learn a semantic space where
libraries that appear in the same context are close (similar) in the space. The
authors do not perform classiﬁcation, therefore, their dataset is not anno-
tated, however, the learned representation can be used to compute similarity
and also train a classiﬁcation model.

Diﬀerently from the previous works, [24] used the C++ keywords and
operators, represented as a binary matrix, as input to a convolutional neural
network to assign the correct category out of 6 in the computer science and
engineering ﬁeld. The dataset is made of 40,023 students written source

7

code for assignment/exams (short, single ﬁle, programs). Their dataset is
not publicly available.

2.2. Other Approaches

The following is a review of the works that have been based on software
artifacts other than source code. Following MUDABlue, Sally [25] used an
approach based on bytecode, the external dependencies of the project and
information from Stack Overﬂow to generate a tag cloud. Their dataset is
no longer available.

Sharma et al. [26] used a combined solution of topic modeling and genetic
algorithms called LDA-GA [34]. The authors apply LDA topic modeling on
the README ﬁles, and optimize the hyper-parameters using genetic al-
gorithms. While LDA is an unsupervised solution, humans are needed to
annotate the topics from the identiﬁed keywords. The authors release a list
of 10,000 examples annotated by their model into 22 categories, which was
evaluated using 400 manually annotated projects. It is interesting to notice
that half of the projects eventually end up in the ‘Other’ category, which
means that they are not helpful when training a new model.

ClassifyHub [10] used an ensemble of 8 na¨ıve classiﬁers, each using dif-
ferent features (e.g. ﬁle extensions, README, GitHub metadata and more)
to perform the classiﬁcation task. The authors use the InformatiCup 20175
dataset, which contains 221 projects unevenly divided into 7 categories.

Nguyen et al. [27] proposed CrossSim, an approach that uses the manifest
ﬁle and the list of contributors of GitHub Java projects: this data is used
to create a RDF graph where projects and developers are nodes, and edges
represent the use of a project by another or that a developers is contributing
to that project. The authors used SimRank [35] to identify similar nodes in
the graph. According to SimRank, two objects are considered to be similar
if they are referenced by similar objects.

HiGitClass [28] used an approach for modeling the co-occurrence of multi-
modal signals in a repository (e.g. user, name of repository, tags, README
and more). The authors performed the annotation according to a taxon-
omy (hierarchical classiﬁcation) that is given as an input with keyword for
each leaf node. The authors released a dataset with taxonomies for two do-
mains: an artiﬁcial intelligence (AI) taxonomy with 1,600 examples, and a

5https://github.com/informatiCup/informatiCup2017

8

bioinformatics (Bio) one with 876 projects.

Di Sipio et al. [29] used the content of the README ﬁles and source
code, represented using TFIDF, as input to a probabilistic model called
Multinomial Na¨ıve Bayesian Network to recommend possible topics. Given
its premises, the work is deﬁned as a multi-label classiﬁcation. The authors
used 120 popular topics from GitHub, and released a dataset of around 10,000
annotated projects in diﬀerent programming languages.

Repologue [30] also adopted a multimodal approach. The authors used
project names, descriptions, READMEs, wiki pages, and ﬁle names con-
catenated together as input to BERT [36], a neural language model, that
creates a dense vector representation (i.e., embeddings) of the input text.
Then, a fully connected neural network was applied to these embeddings to
predict multiple categories. Their dataset (currently unavailable) contains
152K repositories in various languages classiﬁed using 228 categories from
GitHub’s Collections, which should be similar as the ones from Di Sipio et
al. [29].

Finally, Borges et al. [11], albeit not performing a classiﬁcation of software
repositories, made a list of 2,500 projects (annotated in 6 domains/categories)
available for other researchers.

2.3. Summary of Related Work

Table 4 presents a summary of the datasets used in the literature. A
single ﬁle with all the categories and the number of examples for each of
the analyzed works is available in the replication package for inspection or
further analysis. We use the following attributes to describe and analyze
each dataset:

• Work: the publication details where the dataset is proposed or used

for the ﬁrst time;

• Year: the publication year;

• Data Source: the type of information used to perform the classiﬁca-
tion task from the software systems. This was further coded into the
following:

– Source Code: when the authors of the research directly used the
source code of a system to infer (i.e., bottom up) or assign (i.e.,
top down) the software categories;

9

– README : when the authors used the textual description of a
software project of the README ﬁle to infer or assign one or
more categories;

– Imports: when the authors focused on what external libraries have
been imported into a software project to infer or assign a category;

– Key and Op, when the authors used the predeﬁned or reserved
words of a speciﬁc programming language (e.g., C++), along with
the operators used in the source code;

– Multimodal [37]: when the authors used a combination of several

sources (e.g., Source Code and Wiki pages).

• Available: whether the dataset is available or not. We distinguish
whether the available part is the list of annotated projects, or the list
and the used ﬁles are;

• Task: the type of task that can be performed using the dataset. This

attribute has the following possibilities:

– Classiﬁcation: assign one of n mutually exclusive categories to the

input project;

– Multi-Label Classiﬁcation [38]: assign to a project one or more

categories from set of n;

– Hierarchical Classiﬁcation [39]: assign m of n categories as for
the Multi-Label problem, however there is a hierarchy among the
categories;

– Similarity: the task is to retrieve software that are similar to one

given as input;

– Representation Learning [40]: a more general case of the Sim-
ilarity, in this case the goal is to create a dense representation
(embedding) that preserves the similarities among projects, and
it can also be used for downstream tasks.

• Examples: the total amount of examples in the dataset;

• Categories: the number of diﬀerent categories used to classify the
software into, higher scores aren’t always better as we will see later on;

10

• Balance: the level of class balance in terms of examples. It is com-
puted using the Shannon Diversity Index (or Normalized Class En-
tropy [41] in Machine Learning), a normalized entropy [42] value:

Balance =

−

k
(cid:80)
i=1

n log ci
ci

n

log k

where the numerator is the entropy for a dataset of size n with k cat-
egories each of size ci, and the denominator is the perfect case of a
dataset with balanced categories, used to normalizes the values. The
results range between 0 (e.g., a completely unbalanced dataset with
only one category containing all the examples) and 1 (e.g., a perfectly
balanced dataset containing categories with the same amount of ex-
amples). A low score means that the dataset contains a large number
of categories that are not well represented in examples, and therefore
more diﬃcult to perform the classiﬁcation task when encountered. This
measure is not suitable for cases where there is a large amount of classes
with many examples, and only a few classes with a small number of
examples;

• Min: the number of examples for the class with the least amount of

representation;

• Max: the number of examples for the largest class in the dataset.

11

x
a
M

3
1

9

-

5
1
1
1

7
3
8

‡
0
7
6

9
1

4
3
5
3

6
2

-

-

-

-

1
2

5
1
6

0
0
1

0
0
1

9
3

-

(cid:70)
1
6
3

(cid:70)
0
1
2

-

9
6
7
0
1

2

4

-

3
0
3

3
0
1

5
8

5
9

-

1

7

-

-

(cid:70)
3
1
2
1

(cid:70)
7
2

-

3
1
7
4

-

8
4

1
6
2

0
0
1

-

1

8

)
‡
1
9
.
0
(

(cid:70)
7
8
.
0

(cid:70)
1
9
.
0

-

-

0
6
.
0

8
8
.
0

3
7
.
0

5
9
.
0

-

3
9
.
0

8
5
.
0

7
8
.
0

1

-

3
9
.
0

3
9
.
0

-

-

1
9
.
0

7
9
.
0

-

†
6
9
.
0

8
8
.
0

6

6

-

2
2

6

-

2
2

5
7

5

6

-

-

3
2

-

-

3

2

4
3
1

8
2
2

9
6

3
1

(cid:70)
3
1

(cid:70)
0
1

)
‡
0
6
3
,
5
(

0
1
3
,
8

6
8
2
,
3

0
0
5
2

0
5
4
,
4
1

0
0
0
,
0
1

4
0
8
,
9

8
0
2

1
4

3
4

3
0
1

2
8
5

-

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

y
t
i
r
a
l
i

m
S

i

y
t
i
r
a
l
i

m
S

i

y
l
n
O
t
s
i
L

y
l
n
O
t
s
i
L

s
e
Y

s
e
Y

s
e
Y

.
s
s
a
l
C

l
e
b
a
L
-
i
t
l

u
M

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

y
t
i
r
a
l
i

m
S

i

(cid:5)
y
l
n
O
t
s
i
L

s
e
Y

o
N

o
N

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

y
l
n
O
t
s
i
L

e
d
o
C
e
c
r
u
o
S

e
d
o
C
e
c
r
u
o
S

e
d
o
C
e
c
r
u
o
S

e
d
o
C
e
c
r
u
o
S

-

o
N

l
a
d
o
m

i
t
l

u
M

3
2
0
,
0
4

6
9
5
,
1

.
s
s
a
l
C

l
a
c
i

h
c
r
a
r
e
i
H

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

6
7
8

.
s
s
a
l
C

l
a
c
i

h
c
r
a
r
e
i
H

0
0
0
,
2
5
1

0
6
0
,
2
1

.
s
s
a
l
C

l
e
b
a
L
-
i
t
l
u
M

.
s
s
a
l
C

l
e
b
a
L
-
i
t
l
u
M

5
9
4

5
9
4

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

o
N

s
e
Y

s
e
Y

s
e
Y

o
N

s
e
Y

s
e
Y

g
n

i

n
r
a
e
L

.
r
p
e
R

i

g
n
d
d
e
b
m
E

e
d
o
C
e
c
r
u
o
S

e
d
o
C
e
c
r
u
o
S

l
a
d
o
m

i
t
l
u
M

p
O
d
n
a

y
e
K

l
a
d
o
m

i
t
l
u
M

l
a
d
o
m

i
t
l
u
M

l
a
d
o
m

i
t
l

u
M

E
M
D
A
E
R

s
t
r
o
p
m

I

E
M
D
A
E
R

E
M
D
A
E
R

e
d
o
C
e
c
r
u
o
S

-

6
0
0
2

9
0
0
2

2
1
0
2

4
1
0
2

6
1
0
2

6
1
0
2

7
1
0
2

7
1
0
2

8
1
0
2

8
1
0
2

8
1
0
2

9
1
0
2

9
1
0
2

9
1
0
2

9
1
0
2

0
2
0
2

0
2
0
2

1
2
0
2

1
2
0
2

]
8
1
[

e
u
l
B
A
D
U
M

]
9
1
[

T
C
A
L

]
1
2
[

N
A
L
C

]
9
[

.
l
a

t
e

z
e
u
q
s
a
V

]
1
1
[

.
l
a

t
e

s
e
g
r
o
B

]
2
2
[

d
i
o
r
d
N
A
L
C

]
6
2
[

.
l
a

t
e

a
m
r
a
h
S

(cid:62)
]
0
1
[

b
u
H
y
f
i
s
s
a
l
C

I

A

o
i
B

-

-

]
8
2
[

]
8
2
[

s
s
a
l
C
t
i

G
H

i

s
s
a
l
C
t
i

G
H

i

]
9
2
[

.
l
a

t
e

o
i
p
i
S

i

D

a
v
a
J
-
e
m
o
s
e
w
A

]
0
3
[

e
u
g
o
l
o
p
e
R

J
A
d
e
c
u
d
e
R

]
0
2
[

.
l
a

t
e

r
i
a
l
C
e
L

]
4
1
[

c
e
V
2
t
r
o
p
m

I

]
4
2
[

i
h
s
a
h
O

]
3
2
[

D
A
C
S
A
L

]
7
2
[

m
i
S
s
s
o
r
C

12

s
t
a
t
S

t
e
s
a
t
a
D

n
i
M

e
c
n
a
l
a
B

s
e
i
r
o
g
e
t
a
C

s
e
l

p
m
a
x
E

k
s
a
T

e
l

b
a
l
i
a
v
A

e
c
r
u
o
S

a
t
a
D

r
a
e
Y

k
r
o

W

e
r
u
t
a
r
e
t
i
l

n

i

d
e
s
u

s
t
e
s
a
t
a
d

t
n
e
r
e
ﬀ
d

i

e
h
t

f
o

y
r
a
m
m
u
S

:
2

e
l
b
a
T

e
r
u
s
a
e
m
e
h
t

r
o
f

d
e
s
u

s
e
l

p
m
a
x
e

f
o

r
e
b
m
u
n

e
h
t

o
t

m
u
s

t
o
n

o
d

e
l

b
a
t

r
e
p
a
p

e
h
t

n

i

s
r
e
b
m
u
n

o
s
l
a

d
n
a

,
s
l
e
b
a
l
-
i
t
l
u
M

†

t
e
s
a
t
a
d

e
h
t

n

i

s
e
l

p
m
a
x
e

e
h
t

f
o

f
l
a
h

t
s
o
m
l
a

s
n

i
a
t
n
o
c

t
a
h
t

s
s
a
l
c

’
s
r
e
h
t
O

‘

e
h
t

g
n
i
v
o
m
e
r

r
e
t
f
A
‡

]
9
1
[

T
C
A
L

n

i

e
l

b
a
l
i
a
v
a

s
i

t
e
s
a
t
a
d

d
e
c
u
d
o
r
p
e
r
A
(cid:5)

.
y
m
o
n
o
x
a
t

e
h
t

f
o

l
e
v
e
l

d
n
o
c
e
s

e
h
t

r
o
f

r
e
h
t
o

e
h
t

,
l
e
v
e
l

t
s
r
ﬁ

e
h
t

r
o
f

s
i

e
u

l
a
v

t
s
r
i
F

.
y
h
c
r
a
r
e
i
h

l
e
v
e
l

o
w
T

t
e
s
a
t
a
D
7
1
0
2

p
u
C
i
t
a
m
r
o
f
n
I

(cid:62)

(cid:70)

2.4. Related Work Content Analysis

The quantitative summarization of the previous section is not suﬃcient to
give us a complete idea of the datasets. In this section we present the content
of the categorizations in the selected datasets. We will give an overview of
their intended application, inferred from the labels, and discuss in more detail
the semantics of the labels using word embeddings.

We use fastText [43], a neural language model, as the method for ex-
tracting the vector representation of the category: this is because it can
handle out-of-vocabulary words, however, we obtained similar results also
with BERT [36] and StackOverﬂow [44] embeddings. In Figure 2, we can
see the distribution of similarities among categories, for each dataset. On
the one hand, it is diﬃcult to say anything deﬁnitive of the low similarity
outliers, as terms from diﬀerent domains might have low similarity; on the
other hand, for the high similarity ones, these mostly result in categories that
are in a hierarchical relationship or are highly related.

• MUDABlue: it has a very small categorization, with a focus on de-
velopers containing categories like ‘Compilers’, ‘Editor’, and a very
speciﬁc one ‘xterm’. However, it also contains labels that are not rel-
evant to the others, in particular ‘Boardgame’. Overall the terms are
not too similar between themselves, with the only outlier being the pair
‘xterm’ and ‘Compilers’ of 0.46. And the lowest similarity being among
the ‘Boardgame’ label, and ‘Editor’. Given its small size, we can see
the high spread as a lack of speciﬁcity.

• LACT: it has a similar domain as MUDABlue, but with more terms.
It is also more general, as it contains several terms that are broader and
less speciﬁc. We can ﬁnd: ‘Database’ and ‘Editor’, as in MUDABlue,
‘Terminal’, which can be considered a more general version of ‘xterm’.
The other terms are more cohesive compared to MUDABlue, for exam-
ple ‘E-Mail’ and ‘Chat’. In this case, we do not ﬁnd any outlier classes
with a high similarity, and the distribution is quite narrow.

• Vasquez:

it proposes a much more general taxonomy, as is a subset
of SourceForge. Its labels span multiple ﬁelds in the Computer Science
domain, some more general: ‘Scientiﬁc’, ‘Networking’, and ‘Security’:
others more speciﬁc: ‘Indexing’, and ‘Compilers’. Given its well deﬁned
focus, and a higher number of topics compared to previous dataset, we

13

‘Compilers’ and ‘Inter-
ﬁnd some labels that have a high similarity:
preters’ result in a similarity of 0.52 while ‘Networking’ and ‘Commu-
nication’ are at 0.48. The latter are co-hyponyms, so hyponyms that
share the same hypernym, while the former are related, as communi-
cation software use networking technologies.

• LASCAD: smaller compared to Vasquez et al., while still using Com-
puter Science labels, they are not as complete and their labels are more
sparse and less related to each other. The labels are: ‘Machine Learn-
ing’, ‘Data Visualization’, ‘Game Engine’, ‘Web Framework’, ‘Text Ed-
itor’, and ‘Web Game’. As expected from their surface form, the high
similarity outliers pairs are:
‘Web Game’ and ‘Game Engine’, with a
similarity of 0.60, and ‘Web Game’ and ‘Web Framework’ with 0.59.
These pairs, beside being related by sharing a term, are also related in
terms of usage.

• Ohashi: this is a very speciﬁc categorization, based on the domain of
science courses. The label set includes:
‘Combinatorial Optimization
Problems’, ‘Number Theory Problems’, ‘Shortest Path Problems’. The
overall high similarity between labels is due to the fact that all the
labels contain the ‘Problems’ term.

• Sharma:

it is a developers oriented classsiﬁcation. The terms cover
various areas, labels include:
‘Security’, ‘Music’, ‘Gaming and Chat
Engines’, and ‘Blogging’. Furthermore, there also some programming
languages like ‘Lua’ and ‘Ruby related’.

• ClassifyHub: as a more educational oriented dataset, its focus is not
well deﬁned, and it has high level labels in very loosely related domains:
‘Homework’, ‘Documents’, ‘Development’, ‘Education’, and ‘Website’.

• HiGitClass: their two datasets are very speciﬁc, one focusing on AI

subﬁelds, while the other is focused on Bioinformatics.

Labels in the AI dataset include: ‘Computer Vision’, ‘NLP’, and ‘Speech’
at level zero and ‘Image Generation’, ‘Super Resolution’, ‘Language
In Figure 2, we can see the similarity
Modeling’ at the ﬁrst level.
among the labels at all levels. The outliers are due to surface similarity
among the labels (e.g., ‘Text Classiﬁcation’ and ‘Image Classiﬁcation’).

14

As expected, the average similarity is higher, given the very speciﬁc do-
main which as mentioned, also means that some words are present in
multiple labels, increasing this score.

In the Bioinformatics dataset, labels include: ‘Computational Biology’
and ‘Data-Analytics’ at level zero, and ‘Sequence Analysis’, ‘Database
and Ontology’, and ‘System Biology’ at level one. This dataset contains
some labels that are representing two distinct concepts (e.g., ‘Database
and Ontology’), these labels are less informative when used as we are
unsure of which of the two concepts the annotated project belongs to.
The outliers show similar characteristics as for the AI dataset.

• Di Sipio: their categorization is the most general as it is a subset
of the most common GitHub Topics. The topics include application
domains like ‘Machine Learning’, ‘Database’, and ‘Operating System’.
Moreover, we ﬁnd programming languages like ‘Python’ and ‘Java’, and
also companies and services like ‘Google’ and ‘AWS’. Given the large
variety in labels, we also have many that are highly related to others.
We start with ‘Cryptocurrency’ and ‘Bitcoin’ with similarity 0.84; fol-
lowed by a list of database-related labels with similarity in the range
0.75 − 0.80, these are ‘PostgreSQL’, ‘SQL’, and ‘MySQL’, ‘NoSQL’,
‘MongoDB’. And also ‘Machine Learning’ with ‘Deep Learning’ having
a similarity of 0.77.

A complete list of the statistics and labels for each dataset is available in

our data replication package6.

2.5. Discussion

We gathered several insights analysing the results collected in Table 4:
ﬁrst, approximately one in three of the datasets are not publicly available;
similarly, the authors have only released the list of categories in one in three
of the datasets, which in most cases is a sub-sample of a larger classiﬁcation.
In both those cases, it is hard (if not impossible) to reproduce the steps that
lead to the classiﬁcation: the unclear pre-processing has in fact a direct eﬀect
on the performance of the empirical approach [45].

Second, we noticed the variance of the amount of examples and the result-
ing classiﬁcations: from 41 examples to 12K or 150K categories (although the

6https://zenodo.org/record/5018234

15

Figure 2: Cosine Similarity between labels using fastText embeddings.

latter are not publicly available), and from 6 to 134 or 228 (again, the latter
are unavailable). The higher bound of these stats shows acceptable numbers
for both the number of example and the number of diﬀerent categories.

Furthermore, from the inspection of the categories shows some issues: in
particular, they contain some categories that are not relevant to the intended
use of the dataset: software application domain classiﬁcation.

From the observations above, it becomes clear that most existing clas-
siﬁcations have fundamental issues that prevent them from being further
adopted by other researchers. While creating a new classiﬁcation, one should
not only be able to reproduce the steps performed by other researchers, but
also annotate the aspects that might represent common antipatterns for oth-
ers pursuing a similar goal.

Next, in Section 3, we collect all the practical insights gained from ana-
lyzing the datasets and systematically present issues that we found in other
classiﬁcations.

16

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−0.250.000.250.500.75MUDABlueLACTVasquezBorgesLASCADOhashiSharmaClassifyHubHiGitClass−AIHiGitClass−BIODi SipioSimilarity3. Antipatterns

Using the evidence observed during our case study, and an inductive
analysis of the state of the art classiﬁcation and taxonomies that comprises
the information summarized in Table 2, we highlight 7 antipatterns that
researchers have faced so far while creating a taxonomy or a classiﬁcation
of software. We also add a discussion to each, and a suggested solutions to
reduce the eﬀect of each of these antipatterns.

The analysis was performed with a particular focus on the following char-

acteristics:

• Coverage: each classiﬁcation has its own domain, that can be more
speciﬁc or general. With this requirement, we can evaluate if there are
missing categories given the domain of the classiﬁcation, and therefore
evaluate its completeness and usability in its domain;

• Cohesion: this also relates to the domain of the classiﬁcation, however,
with this requirement, we try to assess if there are categories that do
not belong to the domain;

• Consistency:

lastly, we check if the classiﬁcation has any other issue
that aﬀect its consistency like duplicate or overlapping categories, cat-
egories with a bad surface form (e.g. in [20] there is ‘contrib/math’ and
‘math’), or any other abnormality.

The results of the analysis is a list of 7 common antipatterns. Below, we
present a deﬁnition of each, and we will discuss them with instances of how
each was observed in past literature and give some suggestions on how they
can be ﬁxed from a practical point of view.

MT – Mixed Taxonomies: this issue happens when a label set contains
various categories from diﬀerent taxonomies (e.g., Programming Lan-
guages and Application Domains).

MG – Mixed Granularity: this issue emerges when categories are found
(or imposed) by researchers to a dataset, although those categories
belong to diﬀerent levels (e.g., family, species, etc.) of a taxonomy.

SC – Single Category: this is a very common issue in software classiﬁca-
tions, and it is based on simplifying a complex artifact like a software
system, and describing it with only one category.

17

NE – Non Exhaustive Categories: this issue is visible when the cat-
egories chosen (i.e., top-down) or extracted (i.e., bottom-up) by the
researchers don’t cover the entire spectrum of software categories.

NRC – Non Relevant Categories: this issue is visible when researchers
choose or extract a subset of categories that is not representative of the
domain of the classiﬁcation.

UJC – Unnecessarily Joined Categories: this issue occurs when re-
searchers arbitrarily join or use several categories as one, although those
are a compound of two or more diﬀerent domains.

SKC – Sink Category: this is another very common issue in software
classiﬁcation, and it manifests itself when researchers use a generic
category for all the software systems that do not ﬁt any of the available
categories.

3.1. Mixed Taxonomies - MT
Deﬁnition: The MT antipattern is deﬁned as a label set consisting of a mix-
ture of two or more diﬀerent taxonomies, each covering a diﬀerent domain.

Possible Root Cause: We hypnotize, that one of the causes of this, is the
fear of the creators of the dataset to exclude labels, which might make it less
appealing to the ﬁnal users.

Potential Side Eﬀects: This is problematic as the model has to perform two
or more tasks at the same time with only a single label per project. Having
multiple annotations for the same project is not a problem itself, the issue is
having them mutually exclusive as there might be labels diverging from each
other.

Concrete Examples: One very common additional part of the classiﬁca-
tion is based on having programming languages in the labels set. This is
common to [26, 20, 29], where, on average, 10% of the examples belong to
programming language categories. It is also common to have language spe-
ciﬁc frameworks or technologies as part of the label set, as for example found
in [29]. In some cases, we might ﬁnd a domain category like ‘Deep-Learning’,
and speciﬁc framework or technology labels like ‘Django’ or ‘AWS’ that are
part of the same classiﬁcations.

18

Potential Solution: A solution to this issue is to deﬁne the diﬀerent classiﬁ-
cations independently, and use them as separate tasks when training models.
Having projects annotated with diﬀerent classiﬁcations is useful as they can
be used as auxiliary tasks to a multi-task learning [46] model to improve
generalizations [47], and boost performance for all the tasks.

3.2. Mixed Granularity - MG
Deﬁnition: Having a dataset where some labels are very speciﬁc for a ﬁeld
and others are more general, or worse, when labels are in an ‘IS-A’ relation-
ship, without these relations being explicitly represented.

Possible Root Cause: This issue is caused by the diﬃculty in creating such
relations among all the labels in the categorization.

Potential Side Eﬀects: The former can make the model catch very spe-
ciﬁc terms, that are dependant on the sample in the dataset, to distinguish
between categories. The latter causes overlap between classes, making the
classiﬁcation harder or even impossible when having a single annotation for
the projects.

Concrete Examples: As examples of this antipattern in action, we observed
that [29] contains the ‘Cryptocurrency’ and ‘Bitcoin’ categories, that have a
similarity of 0.84. Similarly, [9] contains the ‘Compilers’ and ‘Interpreters’
categories with a similarity of 0.52. Even in the label sets where we could not
detect hierarchical relations among categories (for example in [9]), we also
observed more general categories like ‘Networking’ and ‘Web’ along with very
speciﬁc ones like ‘Interpreters’ and ‘Indexing’.

Potential Solution: A solution to this antipattern is to perform a reﬁnement
of the categories, and try to aggregate them, in a hierarchical fashion, as
we attempted in our case study. The beneﬁts are visible in Figure 2, we
have a lower number of outliers with high similarity. Moreover, a more
qualitative analysis to evaluate the extent of the issue, is to assign a position
in a taxonomic ranking scale, like the ones used in biology, for each category
and evaluate the number of diﬀerent ranks covered by the used taxonomy.

3.3. Single Category - SC

Software systems do not contain only one large feature or functionality,
but they are rather composed of many other smaller parts, each with its own

19

speciﬁc task. Most of the times software systems are labeled with only one
category, that limits the extent to which researchers can learn from it.

Deﬁnition: We deﬁne the Single Category antipattern as the annotation of
software with only one category, despite it being a mix of diﬀerent ones.

Possible Root Cause: This is caused by the annotation process being time
consuming and also by not being made by the developers, who will only need
to annotate their own project.

Potential Side Eﬀects: Diﬀerent components of a project, each with its own
domain, inﬂuence what a model learns, making it harder for it to assign a sin-
gle category to all components, especially when their semantic contribution
to the model is diﬀerent. This antipattern gets more evident when having
a Mixed Granularity (and also a Mixed Taxonomy) classiﬁcation, where one
category is contained in another, however the system is penalized by sug-
gesting the other category.

Concrete Examples: This antipattern was detected in all datasets except
for the one of Vasquez et al. [9] that performs multi-label classiﬁcation, and
[29, 30] that performs recommendation of GitHub Topics.

Potential Solution: While the solution for this is obvious (e.g., ‘annotate
each sample with multiple categories’), this is not that easy to achieve as it
requires extra eﬀort from researchers and developers during the annotation
phase. A less demanding approach to achieve the goal would be to adopt
the annotation approach as in the GitHub Topics, however those topics are
highly noisy as mentioned previously. Therefore, this antipattern requires
more attention in future works.

3.4. Non Exhaustive Categories - NE
Deﬁnition: A taxonomy where there are terms that have a common parent,
but one of them is lacking, is considered to be suﬀering of this anti-pattern.
For a classiﬁcation to be usable, it needs to cover the entire range in the
domain. This is dependant on the actual domain, and also changes over time,
and therefore has to be considered in the domain behind the classiﬁcation.

Possible Root Cause: There are various possible causes, one is that the
missing term was not existent, or very uncommon, at the time that the
taxonomy was created (e.g., the Deep Learning was not very common 20 years

20

ago). Another cause is the how the taxonomy was created, if a subsampling
of another one, then some terms might have been excluded by the process,
if the taxonomy is deﬁned top-down, then the knowledge of the domain by
the authors has a big impact on the presence or not of this antipattern.

Potential Side Eﬀects: Having a classiﬁcation that is too small or with
missing relevant categories is an issue as the classiﬁcation model performance
can be aﬀected based on what category is missing, as the category can be
easily diﬀerentiated if no similar ones are present. Moreover, will make the
approach less useful for general uses.

Concrete Examples: Some example from the previous work are the classiﬁ-
cations of [18, 19, 23] that are too small and lack many categories, and in [20],
where they have ‘Interpreters’ but not ‘Compilers’, they are also missing a
‘Security’/‘Cryptography’ related categories. In [29], a GitHub Topics based
classiﬁcation, we have ‘NLP’ but not ‘Computer Vision’ in spite of it having
134 categories.

Potential Solution: Solutions to this are limited, as mentioned previously,
application domains change over time with new categories appearing. A
possible solution is to have a coarser granularity, however, this might not be
a possibility in some cases, and will also reduce the utility of the classiﬁcation.

3.5. Non Relevant Categories - NRC
Deﬁnition: This antipattern is based on assigning very ﬁne-grained cat-
egories to a project. This means that researchers have in the past added
categories to their taxonomies that are too speciﬁc and non relevant.

Possible Root Cause: The presence of these categories can be caused by a
lack of a speciﬁc usage for the taxonomy; or a lack of reﬁnement of categories,
when subsampling them from a larger pool.

Potential Side Eﬀects: This antipattern has the eﬀect of making the classi-
ﬁcation task too simple, since the categories have very few shared terms (or
even none) with the others. This can be viewed as a special case of Mixed
Taxonomies: however, the categories that would be more relevant to those
are usually one or two, and, diﬀerently from Mixed Taxonomies, they are not
related to speciﬁc technologies or programming languages.

21

Concrete Examples: Examples of non relevant categories were found for
instance in [18], where categories are very diﬀerent between each other (e.g.,
‘Boardgame’ and ‘Editor‘); and in [10], where there are a ‘Development’
category and a ‘Homework’ one. Another example is from [29], where we
ﬁnd ‘Minecraft’ (i.e., a popular videogame) as a category.

Potential Solution: A possible workout for this antipattern would be to just
remove these categories and either discard the examples along with it, or try
to reassign them to a relevant one that belongs to the domain that is being
modeled.

3.6. Unnecessarily Joined Categories - UJC
Deﬁnition: This antipattern manifests itself in categories that join several
categories using the “and” conjunction (e.g., ‘Gaming and Chat Engines‘).
While this is a less common antipattern, having a category that is a conjunc-
tion of two unrelated categories is something to pay attention to.

Possible Root Cause: One cause for this is the high similarity between the
terms, and the low number of examples each of them have, therefore joining
them will make the number of examples higher.

Potential Side Eﬀects: The joint categories do not provide as much of an
information to the ﬁnal user as having a single term: which category of the
two in the conjunction does the project belong to?.

Concrete Examples: In [26] there are many examples of these joined cate-
gories: while some might be considered acceptable (for example, ‘Data Man-
agement and Analysis’) others form a weak combination (for example, ‘Gam-
ing and Chat Engines‘ or ‘Build and Productivity tools’), since they join
labels that belong to very diﬀerent categories.

Potential Solution: An easy solution for this antipattern would be for re-
searchers to avoid using conjunctions, or to use them only when the categories
are related. However, if the categories are indeed related, there should be a
more general (single) label to group them under, which is a more appropriate
solution.

22

3.7. Sink Category - SKC
Deﬁnition:. This is a very common antipattern to fall in, when dealing with
large classiﬁcations. This antipattern manifests itself with a category, used
as a super-label, that is applied to any software that doesn’t ﬁt any an-
other category in the classiﬁcation, but that still needs an annotation. The
most common one is to have a category named ‘Others’, or other synonyms.
However, there are other categories that might not be that obvious, like
‘Frameworks’, ‘Libs’ and so on.

Possible Root Cause:. While the ‘Other’ category is needed for the classiﬁ-
cation, it’s abuse and presence of the oter Sink Categories is eased by the
diﬃculty of annotating some projects, hence labeling them with a Sink Cat-
egory makes it easier.

Potential Side Eﬀects:. This sink category adds extra noise, as it might get
applied to the majority of projects contained in the pre-existing classiﬁcation.
This category might be also applied to projects that actually belong to other
categories, but that were not originally contained in the classiﬁcation; and
also can be used as a backup for harder to classify projects.

Concrete Examples:. Examples from previous work include:
in LeClair et
al. [20] has three of these which are ‘Libs’, ‘Utils’, and ‘Misc’ which total to
30% of the dataset size; in [11] they have a category called ‘Non-web libraries
and Frameworks’ containing 25% of their dataset’s examples. Lastly, [26] has
a category ‘Others’ containing 50% of the dataset examples.

Potential Solution:. This antipattern is a harder to avoid, and it was com-
monly found in our survey: the works that do not suﬀer from this were
usually dealing with small classiﬁcations, or very domain-speciﬁc ones.

3.8. Summary

In Table 3 we summarize, for each work, the antipatterns they have in
their classiﬁcation. We can notice the least easy to fall in antipatterns are
the NRC and UJC, and the most common are NE and SC which are also
the hardest to avoid as they require extra work in the annotation phase.
The most problematic issues, MT and MG are also quite common, with the
former being present in most of the larger and more general taxonomies.

We can also see that there is no perfect taxonomy: if one only considered
the amount of antipatterns contained in a dataset, they would select the

23

works of Di Sipio et al. [29], Zhang et al. (HiGitClass) [28], and Ohashi et
al. [24]. However, the latter two have very speciﬁc and closed domains, that
are more straightforward to create, but less useful to other researchers.

Table 3: Summary of the antipatterns in previous works.

T
M

G
M

C
S

E
N

C
R
N

C
J
U

C
K
S

(cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
(cid:51) (cid:51) (cid:51)
(cid:51)
(cid:51) (cid:51) (cid:51)

MUDABlue [18]
LACT [19]
Vasquez et al. [9]
Borges et al. [11]
LeClair et al. [20] (cid:51) (cid:51) (cid:51)
LASCAD [23]
Ohashi et al. [24]
Sharma et al. [26] (cid:51)
(cid:51)
ClassifyHub [10]
(cid:51)
HiGitClass [28]
Di Sipio et al. [29] (cid:51) (cid:51)

(cid:51) (cid:51) (cid:51)
(cid:51) (cid:51)
(cid:51) (cid:51)
(cid:51) (cid:51) (cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:51) (cid:51)

(cid:51)

In Section 4, we present an attempt at creating a classiﬁcation using a
new, bottom up taxonomy: we will annotate all the steps in doing so, and
try and address the limitations of existing classiﬁcations presented above.

4. Case Study

In this section we describe our attempt at creating an example classiﬁca-
tion, with real world usages, that minimises the general issues noted above.
This section describes the original source of the classiﬁcation (4.1), the man-
ual process that was used to reduce the categories in order to balance the
number of examples in each (4.2). Finally, in order to evaluate how dis-
tinct the categories are from each other, we evaluated the lexical similarity
between categories, described by their projects content (4.3.2).

4.1. Classiﬁcation Source

As a starting point (e.g., the ‘seed’) for the creation of our case study
dataset, we picked a pre-existing classiﬁcation, from a Java Awesome List

24

hosted on GitHub. Awesome Lists are curated repositories containing re-
sources that are useful for a particular domain: in our case we use Awesome-
Java 7, a GitHub project that aggregates overall 700 curated Java frameworks,
libraries and software organised in 69 categories. In an initial phase of clean-
ing, we removed tutorials and URLs to websites, obtaining 530 examples; we
also removed the projects that could not be analyzed (e.g., gives errors in the
pipeline, including: encoding, no keywords left to create an encoding, etc.).
The total of projects ﬁnally considered for our task was 495.

Using GitHub Topics8 could be an alternative to the selected Java Awe-
some List: however, the categories for the same list of projects is larger in
the former (around 1,000 labels) than in the latter (69 labels). Also, the
decision of using the Awesome Java list was to avoid using pre-existing clas-
siﬁcations or taxonomies. Beside the previously mentioned issues, other have
sporadically emerged in the past (e.g., in [20], where many examples of that
dataset come from secondary code that is not relevant to the main projects.
Moreover, Awesome-Java is an annotation of a closed ecosystem (Java de-
velopment), making it the seed of a small, but realistic, classiﬁcation. In fact
this process, when improved and automated, could be applied to GitHub’s
Topics annotations to obtain an unlimited source of distantly annotated ex-
amples. Lastly, the Awesome-Java repository is a collective eﬀort of more
than 300 contributors and continuous updates to the list, making it the go-to
source for more than 31K (as stars) developers when looking for a library.

However, the GitHub Topics, would be a better source for a more general
list of categories and a larger scale source of projects. However, there are
larger challenges as there are more than 100K GitHub Topics, hence, this
will move the focus of our study to unrelated issues.

4.2. Label Mapping

The Awesome-Java classiﬁcation contains 69 categories: on average, each
category contains 8 projects. Also, some of the categories represent either
general concepts (‘Science’) or detailed keywords (e.g., ‘Bean Mapping’).

As a result, the Awesome-Java categories make classiﬁcation tasks quite
challenging: therefore we decided to manually reduce the original categories,
in order to reduce the complexity, and avoid duplicates or synonyms. This

7https://github.com/akullpp/awesome-java
8https://github.com/topics

25

mapping was performed manually, in a hierarchical fashion, by one of the
authors, and resulted in a smaller set of 13 categories (Reduced AJ ): the
Label column of Table 4 lists the reduced categories that were obtained from
the original 69. The reductions were evaluated by the second author, and
disagreements on terms were resolved by discussion.

Table 4: Distribution of the number of examples for each category in the Reduced AJ.

Label

Projects

Introspection
CLI
Data
Development
Graphical
Miscellaneous
Networking
Parser
STEM
Security
Server
Testing
Web

Total

32
8
49
100
11
59
25
41
39
14
37
42
38

495

This reduction, in addition to increasing the amount of examples per class,
also helps with one of the issues that the original Awesome-Java presented,
that is the lack of a hierarchical relation between categories. Figure 3 shows a
visual representation of the reduction, and how this helps the establishment
of hierarchical links. Given three labels in the Awesome-Java taxonomy:
Natural Language Processing (‘NLP’), Computer Vision (‘CV’), and Machine
Learning (‘ML’); we reduce those three to an intermediate conceptual label
‘AI’. Together with two other categories, ‘Date/Time’ and ‘Geospatial’, we
assign all these to the ‘STEM’ category.

The initial and ﬁnal annotated labels are stored as a CSV ﬁle, and is

available in our replication package. The ﬁle has the following schema:

• project.name: name of the project;

26

Figure 3: Example of the reduction process. The blue rectangles are actual classes in
the initial or ﬁnal dataset, the grey one are intermediate logical classes used to aggregate
labels. The dots represent not well deﬁned intermediate classes.

• project.desc: short description of the project from Awesome-Java;

• project.link: URL to the GitHub repository;

• category: Awesome-Java annotation;

• category.desc: short description of the category from Awesome-Java;

• label: mapping of the original category into one of the reduced set.

4.3. Evaluation

We evaluate the quality of our approach of reduction on the Awesome-
Java taxonomy using both qualitatively and quantitative measure. We ﬁrst
compare the original and the reduced taxonomies using the introduced perils
and pitfalls. Furthermore, we measure the lexical similarity of classes.

4.3.1. Antipatterns

A summary of the antipatterns found in the original Awesome-Java and
the Reduced AJ is present in Table 5. The original Awesome-Java presents
several of the antipatterns identiﬁed in the taxonomies in previous works,
from non exhaustive label set (NE), to mixed granularity (MG) and mixed
taxonomies (MT). Therefore, we can be more conﬁdent that the processes

27

NLPCVMLGeospatialAISTEMDate/Time......we used to reduce these issues can be deployed to diﬀerent taxonomies as
well. Examples of the antipatterns found in Awesome-Java include:

• Mixed Taxonomy: examples of this antipatterns are the presence of

technologies like ‘Apache Commons’ in the list;

• Mixed Granularity: for examples we ﬁnd the label ‘Science’ with the la-
bel ‘Conﬁguration’, or ‘Development’ and ‘Compiler Compiler’. More-
over, there are labels that are in a ‘IS-A’ relationship, like ‘Mobile
development’ and ‘Development’.

• Non Exhaustive Categories: one examples is the lack of a ‘Audio Pro-
cessing’ category, while there are for ‘Computer Vision’ and ‘Natural
Language Processing’.

• Sink category: the ‘Apache Commons’ label contains many projects
that can be annotated with another label in the set, for example ‘Com-
mons CLI9’ can be annotated with the ‘Command Line Interface’ label.

The two main beneﬁts of the reduction process are the removal of the
Non Exhaustive (NE) label set issue and the removal of the Mixed Taxonomy
(MT). Another beneﬁt, although not completely removed as seen in Table 5,
is a marked decrease in the severity of the Mixed Granularity (MG) issue.

In our case study, most of these antipatterns have been resolved using
the label reduction process. However better results require tackling Mixed
Taxonomy (MT) issue:
its resolution required manual annotations of the
examples belonging to the problematic category (e.g., ‘Apache Commons’)
as the mere reduction would just map everything to Sink Category.

Table 5: Summary of the antipatterns in the original Awesome-Java and our Reduced AJ.

T
M

G
M

C
S

E
N

C
R
N

C
J
U

Awesome-Java (cid:51) (cid:51) (cid:51) (cid:51)
Reduced AJ

(cid:51) (cid:51)

C
K
S

(cid:51)
(cid:51)

Similarly for the other datasets, we also computed the similarity of the
labels using fastText, in 4 we can see the similarity before and after the
reduction. The lower average similarity is caused by a reduction in the terms

9https://commons.apache.org/proper/commons-cli/

28

in a hierarchical relationship, and also by a lower number of terms sharing a
common subword.

Figure 4: Cosine Similarity between labels using fastText embeddings.

4.3.2. Lexical Similarity between Categories

To evaluate the quality of our process, we evaluated the lexical similarity
between each category using the content of all projects belonging to that
category. This step is of fundamental importance, since it helps to evaluate
the quality of the mapping process into categories, and to give an empirical
evaluation of how similar categories are, before and after reduction. In order
to lexically represent the categories we used the TFIDF approach; in order
to measure the similarity of two categories, we used the cosine similarity. We
did not opted for embedding solutions like fastText or BERT as they are not
suited for our task. For example fastText is not designed for long document
embeddings, as it performs a mean operation over the embeddings to create
the ﬁnal representation, meaning that all the documents will converge to a
very similar embedding, resulting in very high similarities between all doc-
uments. With BERT, given the small amount of token it accepts as input
(512) we will have a similar issue, as we will need to combine the embeddings
of subset. Other, more code oriented solutions, like code2vec or CodeBERT
have issue as well. Code2vec is trained with the objective of encoding struc-
ture and semantic of the code, and not semantic of the word. CodeBERT
suﬀers of the issues of BERT.

Extraction of the category documents. For each project belonging to a cat-
egory, we created the category document using all the identiﬁers contained

29

lllllllllllllllllllllllllllReduced AJAwesome−Java−0.20.00.20.40.60.8Similarityin the source code ﬁles of the project belonging to that category. For the
extraction of the identiﬁers, we used the tree-sitter 10 parser generator tool.
The identiﬁers, without keywords, are extracted from the annotated concrete
syntax tree created using a grammar for Java code.

The identiﬁers were further processed by (1) separating the camel case
strings into words, (2) lower casing every word, and (3) removing common
Java terms that do not add much semantically (e.g., ‘main’, ‘println’, etc).
Lastly, we perform (4) lemming, which is a way to reduce amount of diﬀerent
terms in the vocabulary by removing the morphological diﬀerences in words
with the same root (e.g., ‘networking’ becomes ‘network ’).

Evaluation of the similarity between categories. These category documents
were used as an input to TFIDF, a statistical vectorization of words based on
the Bag of Words (BoW) model. Documents are considered as a collection
of words/terms, and converted to a vector by counting the occurrences of
every term. Diﬀerently from BoW, in TFIDF the words are weighted by
their total frequency in the collection of documents. This will result in a
list of vectors representing the lexical content of that category. We limit
the amount to the top 1,000 terms that have a max document frequency
lower than 0.8, hereby words that are present in less than 80% of the labels,
therefore ignoring common words.

We adopted the cosine similarity, a measure of similarity between two
vectors, in order to measure the similarity between all categories, and to
evaluate possible overlaps or large diﬀerences between them. The cosine
similarity, when using TFIDF, ranges from 0, diﬀerent content, to 1 identical
content. We compute the similarities between the categories of the original
ﬁner grained Awesome-Java classiﬁcation, and the Reduced AJ as well.

Results. The results in Figure 5 show the ﬁnal similarities for the reduced
classiﬁcation, while Figure 6 shows the similarities between the categories
of the original Awesome-Java. The initial impression is that the overall
similarity between categories is very low for both classiﬁcations: this is a clear
eﬀect of the pre-ﬁltering of the terms that are very frequent in all documents.
The second observation is that the mean similarity of the classiﬁcation with
69 labels is higher and has more variance: in particular there is an average
similarity of 0.0520 ± 0.0677, as compared to the reduced one of 0.0210 ±

10https://github.com/tree-sitter/tree-sitter

30

0.0290. This is also visible graphically in the heat map of Figure 6: the
brighter spots, therefore higher similarity are much more frequent there than
in the reduced classiﬁcation in Figure 5.

Discussion. The higher similarities of Figure 6 are caused by a combination
of diﬀerent factors:

1. the ﬁrst cause is the presence of the Mixed Granularity antipattern t.
For example, the similarity between the categories ‘Development’ and
‘Code Analysis’ is 0.45. These two were mapped into the ‘Development’
combined category of the reduced classiﬁcation.

2. the second cause of high similarity is the Single-Label antipattern. As
a result, some projects are labeled with one category, but their fea-
tures would require multiple labels. An example of this would be the
high similarity (0.68) between ‘Database’ and ‘Messaging’ which in
Awesome-Java is described as “Tools that help send messages between
clients to ensure protocol independency”. An example explaining this
high similarity is by considering ‘Apache Kafka’, a distributed event
streaming framework used for various tasks including data integration,
being categorized as ‘Messaging’ while still containing a high amount of
data management terms like ‘query’. This also remains in the reduced
classiﬁcation (Figure 6) for the ‘Database’ and ‘Networking’, in which
‘Messaging’ is mapped.

3. lastly, we also have to consider noise, given the smaller number of
examples per category in the original classiﬁcation, the documents used
might not be very representative of the category.

4.4. Discussion: moving towards a complete taxonomy

The purpose of a classiﬁcation, like the ones that we have summarised
in Table 2 is to organise similar items (in this case, software systems) into
categories for future reference. This could be for proactively recommending
systems [27], in order to generate a list of alternatives from the same category;
or for identiﬁcation of common patterns into the found categories [48].

On the other hand, the purpose of a taxonomy is to organise categories
into levels: for instance, in a taxonomy for biology, ‘class’ (say, “mammals”)
is placed on a diﬀerent level than ‘order’ (say, “carnivorous”). In the clas-
siﬁcation works that we analysed, we never observed an attempt to deﬁne

31

Figure 5: Cosine similarities between categories in the Reduced AJ. The last two rows are
the mean and max similarity per category.

at which level the categories are (except in [28]), or whether those should
be considered more or less generic or speciﬁc in the more general terms of a
taxonomy.

As a further analysis, and in order to evaluate how speciﬁc, general,
or mixed level the taxonomy is, we asked a group of 10 people belonging
to our research group. The pool of annotators is composed of PhDs, Post
Docs, and Professors in the Software Engineering ﬁeld to indicate whether
the categories illustrated in Table 4 should be placed in a higher or lower
level of a taxonomy. The questionnaire included 13 questions, one for each
topic in the Reduced AJ, where the annotators were asked to perform a rating
by assigning the topic into one of 5 levels, from 1 (very generic) to 5 (very
speciﬁc). We collected their responses and analysed them to determine if any
of the categories that were reduced from the Awesome-Java sample should
be considered a ‘family’, or ‘group’ or even a ‘species’ within a software
taxonomy.

The results of this preliminary qualitative analysis showed that speciﬁc
categories were placed fairly consistently among either the very general (e.g.,

32

IntrospectionDevelopmentCLIParserServerScience/EngineeringDataTestingMiscellaneousGraphicalWebNetworkingSecurity10.020.010.070.070.040.020.060.020.010.050.010.010.02100.010.0200.110.020.0100.020.0700.01010.010.010000.0100000.070.010.0110.010.030.010.020.050.020.0200.020.070.020.010.0110.010.040.060.0200.10.030.010.04000.030.0110.010.010.020.020.0100.010.020.1100.010.040.0110.010.0100.010.1700.060.0200.020.060.010.0110.020.010.0400.010.020.010.010.050.020.020.010.0210.060.0500.010.01000.0200.0200.010.061000.010.050.0200.020.10.010.010.040.05010.010.010.010.07000.0300.170000.01100.01000.020.010.0100.010.010.010.0101Mean0.110.10.080.10.110.090.110.10.10.090.10.10.08IntrospectionDevelopmentCLIParserServerScience/EngineeringDataTestingMiscellaneousGraphicalWebNetworkingSecurityMax0.070.110.010.070.10.040.170.060.060.060.10.170.020.00.20.40.60.81.0x
a
m

d
n
a

n
a
e
m

e
h
t

e
r
a

s
w
o
r

o
w
t

t
s
a
l

e
h
T

.
a
v
a
J
-
e
m
o
s
e
w
A

l
a
n

i
g
i
r
o

e
h
t

n

i

s
e
i
r
o
g
e
t
a
c

n
e
e
w
t
e
b

s
e
i
t
i
r
a
l
i

m

i
s

e
n
i
s
o
C

:
6

e
r
u
g
i
F

.
y
r
o
g
e
t
a
c

r
e
p

y
t
i
r
a
l
i

m

i
s

33

Bean MappingBuildBytecode ManipulationCachingCLICluster ManagementCode AnalysisCode CoverageCode GeneratorsCompilercompilerConfigurationCSP SolverCSVData StructuresDatabaseDate and TimeDependency InjectionDevelopmentDistributed ApplicationsDistributed TransactionsDistributionDocument ProcessingFinancialFormal VerificationFunctional ProgrammingGame DevelopmentGeospatialHigh PerformanceHTTP ClientsHypermedia TypesImageryIntrospectionJob SchedulingJSONJVM and JDKLoggingMachine LearningMessagingMicroserviceMiscellaneousMonitoringNativeNLPNetworkingORMPDFPerformance analysisApache CommonsOtherProcessesReactive librariesREST FrameworksScienceSearchSecuritySerializationServerTemplate EngineAsynchronousBDDFixturesFrameworksMatchersMockingUtilityWeb CrawlingWeb FrameworksMean0.050.060.060.070.050.070.040.070.060.060.050.030.050.080.070.040.070.080.060.090.080.070.050.060.030.040.050.040.10.050.060.040.040.060.080.070.090.050.120.130.090.050.050.090.090.040.060.130.10.030.060.110.030.040.040.050.080.080.090.050.040.130.050.070.10.030.1Bean MappingBuildBytecode ManipulationCachingCLICluster ManagementCode AnalysisCode CoverageCode GeneratorsCompilercompilerConfigurationCSP SolverCSVData StructuresDatabaseDate and TimeDependency InjectionDevelopmentDistributed ApplicationsDistributed TransactionsDistributionDocument ProcessingFinancialFormal VerificationFunctional ProgrammingGame DevelopmentGeospatialHigh PerformanceHTTP ClientsHypermedia TypesImageryIntrospectionJob SchedulingJSONJVM and JDKLoggingMachine LearningMessagingMicroserviceMiscellaneousMonitoringNativeNLPNetworkingORMPDFPerformance analysisApache CommonsOtherProcessesReactive librariesREST FrameworksScienceSearchSecuritySerializationServerTemplate EngineAsynchronousBDDFixturesFrameworksMatchersMockingUtilityWeb CrawlingWeb FrameworksMax0.20.170.180.230.670.290.450.190.190.240.160.080.220.370.680.120.280.450.150.310.50.230.130.210.110.140.370.180.440.140.270.150.290.190.610.670.540.680.440.360.240.230.10.420.390.160.280.360.330.230.50.310.070.260.160.130.310.240.530.280.20.360.180.530.610.090.280.00.20.40.60.81.0the ‘STEM’ category), or the very speciﬁc level (‘Introspection’, ‘CLI’). On
the other hand, several other categories were assigned uniformly to levels
2, 3 and 4, therefore being placed to middle-ground levels of the taxonomy,
depending on the assessor’s point of view. Figure 7 shows the visualisation
of what has initially emerged from the answers of our questionnaire. The
assignment of a topic to a level was performed using the majority voting,
those without majority are not presented.

This is further evidence that deﬁning categories for software systems faces
the challenging task of placing them in an overarching map: the ‘mixed levels’
antipattern will always aﬀect a classiﬁcation eﬀort, unless a more concerted
research eﬀort is conducted and shared, in order to build a taxonomy and
to place the categories in its levels. A magniﬁer of the the uniform distribu-
tion of some topics can be imputed to our methodology, the rating task is
more complex compared to a ranking to asses subjective characteristics [49].
Hence, the future work will focus on using better methods to rank topics.

Figure 7: Assignment of the categories to levels of a taxonomy

5. Threats to Validity

We use the classiﬁcation of Runeson et al. [50] for analyzing the threats to
validity in our work. We will present the construct validity, external validity,
and reliability. Internal validity was not considered as we did not examine
causal relations [50].

34

ClassPhylumOrderFamilySpeciesSTEMGraphicalMiscellaneousParsersCLIIntrospection5.1. Construct Validity

A construct threat in our work is the choice of the classiﬁcation for the
case study. However, given the wide variety of datasets, and the similarity
Awesome Java has regarding the issues with the state of the art classiﬁca-
tions, this threat is mitigated.

Another threat regards the way the reduction was performed. Having
a single annotator performing the reduction can increase the bias in the
selection of the resulting categories. We mitigated this threat by having
another author evaluate the resulting categories; furthermore, we collected
feedback from other colleagues regarding the same resulting categories.

5.2. External Validity

We reduce the external validity to a minimum by analyzing a large va-
riety of datasets. We analyzed 12 datasets, with a diﬀerent origin of the
base classiﬁcation: both bottom up, and top down classiﬁcations have been
considered for study. Moreover, these classiﬁcations are based on diﬀerent
domains, some more speciﬁc (e.g., bio-engineering) other more generic: this
should help alleviate this threat to validity.

5.3. Reliability

The analysis of the classiﬁcations and taxonomies is inherently subjec-
tive, as it involves natural language, and prior knowledge about the diﬀerent
application domains. We adopted objective tools, like semantic analysis, to
aid with the subjective analysis.

6. Conclusions and Future Work

In this work we evaluated the diﬀerent classiﬁcations used for the soft-
ware classiﬁcation task. The current classiﬁcations have issues that might
compromise generalizability of classiﬁcation models, moreover, there is no
general classiﬁcation that can be actively used (RQ1). We identiﬁed a list
of 7 common antipatterns that researchers encounter when creating a soft-
ware classiﬁcations for classifying systems into application domains (RQ2).
While the ideal case would be to avoid those antipatterns when creating a
classiﬁcation, this is quite diﬃcult, and a reﬁnement stage helps with the
reduction (but not the complete removal) of some of these issues. We pre-
sented a case study, using a real classiﬁcation, in which we mitigated some of

35

the antipatterns using a reduction of the categories (RQ3). The reduction
was performed manually in a hierarchical fashion.

As future works we plan to perform the similarity between the categories
content also for the other works in the literature. Furthermore, we plan
to perform analysis similar to the work of Sen et al. [51] for the Question
Answering (QA) task in the Natural Language Processing ﬁeld, where they
look at what clues QA models actually use to answer questions. We are
interested in checking if the models learn general terms of a speciﬁc domains,
or they pick up dataset speciﬁc clues that are not transferable to others.

We are also planning to create a taxonomy induced from all the GitHub
Topics given the variety of projects, and therefore application domains, that
are hosted on the platform. Given the large amount of terms, the hierarchi-
cal aggregation process needs to be automated. First, we plan to create a
ranking with a larger pool of annotators, and given the high disagreement in
our ranking case study, use a diﬀerent methodology: ranking from pairwise
comparisons, as is less complex for annotators [52]. Lastly, use the ranking to
create links between levels, to group terms from diﬀerent levels in the same
domain.

References

[1] I. J. Mojica, B. Adams, M. Nagappan, S. Dienst, T. Berger, A. E. Hassan, A
large-scale empirical study on software reuse in mobile apps, IEEE software
31 (2) (2013) 78–86.

[2] F. Wen, C. Nagy, G. Bavota, M. Lanza, A large-scale empirical study on code-
comment inconsistencies, in: 2019 IEEE/ACM 27th International Conference
on Program Comprehension (ICPC), IEEE, 2019, pp. 53–64.

[3] Y. Zhao, A. Serebrenik, Y. Zhou, V. Filkov, B. Vasilescu, The impact of
continuous integration on other software development practices: a large-scale
empirical study, in: 2017 32nd IEEE/ACM International Conference on Au-
tomated Software Engineering (ASE), IEEE, 2017, pp. 60–71.

[4] L. Briand, Embracing the engineering side of software engineering, IEEE soft-

ware 29 (4) (2012) 96–96.

[5] L. Briand, D. Bianculli, S. Nejati, F. Pastore, M. Sabetzadeh, The case for
context-driven software engineering research: Generalizability is overrated,
IEEE Software 34 (5) (2017) 72–75.

36

[6] C. Vassallo, S. Panichella, F. Palomba, S. Proksch, A. Zaidman, H. C. Gall,
Context is king: The developer perspective on the usage of static analysis
tools, in: R. Oliveto, M. D. Penta, D. C. Shepherd (Eds.), 25th International
Conference on Software Analysis, Evolution and Reengineering, SANER 2018,
Campobasso, Italy, March 20-23, 2018, IEEE Computer Society, 2018, pp. 38–
49. doi:10.1109/SANER.2018.8330195.
URL https://doi.org/10.1109/SANER.2018.8330195

[7] S. Easterbrook, J. Singer, M. D. Storey, D. E. Damian, Selecting empirical
methods for software engineering research, in: F. Shull, J. Singer, D. I. K.
Sjøberg (Eds.), Guide to Advanced Empirical Software Engineering, Springer,
2008, pp. 285–311. doi:10.1007/978-1-84800-044-5\_11.
URL https://doi.org/10.1007/978-1-84800-044-5_11

[8] R. L. Glass, I. Vessey, Contemporary application-domain taxonomies, IEEE

Softw. 12 (4) (1995) 63–76. doi:10.1109/52.391837.
URL https://doi.org/10.1109/52.391837

[9] M. Linares-V´asquez, C. Mcmillan, D. Poshyvanyk, M. Grechanik, On using
machine learning to automatically classify software applications into domain
categories, Empirical Softw. Engg. 19 (3) (2014) 582–618. doi:10.1007/
s10664-012-9230-z.

[10] M. Soll, M. Vosgerau, Classifyhub: An algorithm to classify github repos-
itories,
in: G. Kern-Isberner, J. F¨urnkranz, M. Thimm (Eds.), KI 2017:
Advances in Artiﬁcial Intelligence, Springer International Publishing, Cham,
2017, pp. 373–379.

[11] H. Borges, A. Hora, M. T. Valente, Understanding the factors that impact
the popularity of github repositories, in: 2016 IEEE International Conference
on Software Maintenance and Evolution (ICSME), 2016, pp. 334–344. doi:
10.1109/ICSME.2016.31.

[12] A. Capiluppi, N. Ajienka, Towards a dependency-driven taxonomy of software
types, in: Proceedings of the IEEE/ACM 42nd International Conference on
Software Engineering Workshops, 2020, pp. 687–694.

[13] E. Kalliamvakou, G. Gousios, K. Blincoe, L. Singer, D. M. Germ´an, D. E.
Damian, An in-depth study of the promises and perils of mining github, Em-
pir. Softw. Eng. 21 (5) (2016) 2035–2071. doi:10.1007/s10664-015-9393-5.
URL https://doi.org/10.1007/s10664-015-9393-5

37

[14] B. Theeten, F. Vandeputte, T. Van Cutsem, Import2vec: Learning embed-
dings for software libraries, in: Proceedings of the 16th International Confer-
ence on Mining Software Repositories, MSR 2019, 26-27 May 2019, Montreal,
Canada, 2019, pp. 18–28. doi:10.1109/MSR.2019.00014.
URL https://doi.org/10.1109/MSR.2019.00014

[15] C. Liu, D. Yang, X. Zhang, B. Ray, M. M. Rahman, Recommending github
projects for developer onboarding, IEEE Access 6 (2018) 52082–52094. doi:
10.1109/ACCESS.2018.2869207.

[16] J. Escobar-Avila, M. Linares-V´asquez, S. Haiduc, Unsupervised software cat-
egorization using bytecode, in: Proceedings of the 2015 IEEE 23rd Interna-
tional Conference on Program Comprehension, ICPC ’15, IEEE Press, 2015,
p. 229–239.

[17] M. Auch, M. Weber, P. Mandl, C. Wolﬀ, Similarity-based analyses on software
applications: A systematic literature review, Journal of Systems and Software
168 (2020) 110669. doi:https://doi.org/10.1016/j.jss.2020.110669.
URL
S0164121220301278

https://www.sciencedirect.com/science/article/pii/

[18] S. Kawaguchi, P. K. Garg, M. Matsushita, K. Inoue, Mudablue: an automatic
categorization system for open source repositories, in: 11th Asia-Paciﬁc Soft-
ware Engineering Conference, 2004, pp. 184–193. doi:10.1109/APSEC.2004.
69.

[19] K. Tian, M. Revelle, D. Poshyvanyk, Using latent dirichlet allocation for
automatic categorization of software, in: 2009 6th IEEE International Work-
ing Conference on Mining Software Repositories, 2009, pp. 163–166. doi:
10.1109/MSR.2009.5069496.

[20] A. LeClair, Z. Eberhart, C. McMillan, Adapting neural text classiﬁcation
for improved software categorization, in: 2018 IEEE International Confer-
ence on Software Maintenance and Evolution, ICSME 2018, Madrid, Spain,
September 23-29, 2018, IEEE Computer Society, 2018, pp. 461–472. doi:
10.1109/ICSME.2018.00056.
URL https://doi.org/10.1109/ICSME.2018.00056

[21] C. McMillan, M. Grechanik, D. Poshyvanyk, Detecting similar software ap-
plications, in: Proceedings of the 34th International Conference on Software
Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, ICSE ’12, IEEE
Computer Society, 2012, p. 364–374.
URL https://doi.org/10.1109/ICSE.2012.6227178

38

[22] M. L. V´asquez, A. Holtzhauer, D. Poshyvanyk, On automatically detecting
similar android apps, in: 24th IEEE International Conference on Program
Comprehension, ICPC 2016, Austin, TX, USA, May 16-17, 2016, IEEE Com-
puter Society, 2016, pp. 1–10. doi:10.1109/ICPC.2016.7503721.
URL https://doi.org/10.1109/ICPC.2016.7503721

[23] D. Altarawy, H. Shahin, A. Mohammed, N. Meng, Lascad : Language-
agnostic software categorization and similar application detection, Journal
of Systems and Software 142 (2018) 21–34. doi:https://doi.org/10.1016/
j.jss.2018.04.018.
URL https://doi.org/10.1016/j.jss.2018.04.018

[24] H. Ohashi, Y. Watanobe, Convolutional neural network for classiﬁcation
of source codes,
in: 13th IEEE International Symposium on Embedded
Multicore/Many-core Systems-on-Chip, MCSoC 2019, Singapore, Singapore,
October 1-4, 2019, 2019, pp. 194–200. doi:10.1109/MCSoC.2019.00035.

[25] S. Vargas-Baldrich, M. Linares-V´asquez, D. Poshyvanyk, Automated tagging
of software projects using bytecode and dependencies (n),
in: 2015 30th
IEEE/ACM International Conference on Automated Software Engineering
(ASE), 2015, pp. 289–294. doi:10.1109/ASE.2015.38.

[26] A. Sharma, F. Thung, P. S. Kochhar, A. Sulistya, D. Lo, Cataloging
in: Proceedings of the 21st International Conference
github repositories,
on Evaluation and Assessment in Software Engineering, EASE’17, Associ-
ation for Computing Machinery, New York, NY, USA, 2017, p. 314–319.
doi:10.1145/3084226.3084287.

[27] P. T. Nguyen, J. Di Rocco, R. Rubei, D. Di Ruscio, Crosssim: Exploiting
mutual relationships to detect similar oss projects, in: 2018 44th Euromi-
cro Conference on Software Engineering and Advanced Applications (SEAA),
2018, pp. 388–395. doi:10.1109/SEAA.2018.00069.

[28] Y. Zhang, F. F. Xu, S. Li, Y. Meng, X. Wang, Q. Li, J. Han, Higitclass:
in: 2019
Keyword-driven hierarchical classiﬁcation of github repositories,
IEEE International Conference on Data Mining (ICDM), 2019, pp. 876–885.
doi:10.1109/ICDM.2019.00098.

[29] C. Di Sipio, R. Rubei, D. Di Ruscio, P. T. Nguyen, A multinomial na¨ıve
bayesian (MNB) network to automatically recommend topics for github repos-
itories, in: Proceedings of the Evaluation and Assessment in Software Engi-
neering, EASE ’20, Association for Computing Machinery, New York, NY,

39

USA, 2020, p. 71–80. doi:10.1145/3383219.3383227.
URL https://doi.org/10.1145/3383219.3383227

[30] M. Izadi, S. Ganji, A. Heydarnoori, Topic recommendation for software
repositories using multi-label classiﬁcation algorithms, ArXiv abs/2010.09116
(2020). arXiv:2010.09116.

[31] S. Ugurel, R. Krovetz, C. L. Giles, What’s the code? automatic classiﬁca-
tion of source code archives, in: Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD
’02, Association for Computing Machinery, New York, NY, USA, 2002, p.
632–638. doi:10.1145/775047.775141.

[32] C. Zhou, C. Sun, Z. Liu, F. C. M. Lau, A C-LSTM neural network for text

classiﬁcation, CoRR abs/1511.08630 (2015). arXiv:1511.08630.
URL http://arxiv.org/abs/1511.08630

[33] J. Firth, Studies in Linguistic Analysis, Publications of the Philological Soci-

ety, Blackwell, 1957.
URL https://books.google.nl/books?id=JWktAAAAMAAJ

[34] A. Panichella, B. Dit, R. Oliveto, M. Di Penta, D. Poshynanyk, A. De Lucia,
How to eﬀectively use topic models for software engineering tasks? an ap-
proach based on genetic algorithms, in: 2013 35th International Conference
on Software Engineering (ICSE), 2013, pp. 522–531. doi:10.1109/ICSE.
2013.6606598.

[35] G. Jeh, J. Widom, Simrank: a measure of structural-context similarity, in:
Proceedings of the Eighth ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, July 23-26, 2002, Edmonton, Alberta,
Canada, ACM, 2002, pp. 538–543. doi:10.1145/775047.775126.
URL https://doi.org/10.1145/775047.775126

[36] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep
bidirectional transformers for language understanding, in: Proceedings of the
2019 Conference of the North American Chapter of the ACL: HLT, Volume
1, Association for Computational Linguistics, Minneapolis, Minnesota, 2019,
pp. 4171–4186.

[37] T. Baltrusaitis, C. Ahuja, L.-P. Morency, Multimodal machine learning: A
survey and taxonomy, IEEE Trans. Pattern Anal. Mach. Intell. 41 (2) (2019)
423–443. doi:10.1109/TPAMI.2018.2798607.
URL https://doi-org.proxy-ub.rug.nl/10.1109/TPAMI.2018.2798607

40

[38] G. Tsoumakas, I. Katakis, Multi-label classiﬁcation: An overview, Int. J.

Data Warehous. Min. 3 (2007) 1–13.

[39] A. D. Gordon, A review of hierarchical classiﬁcation, Journal of the Royal

Statistical Society: Series A (General) 150 (2) (1987) 119–137.

[40] Y. Bengio, A. Courville, P. Vincent, Representation learning: A review and
new perspectives, IEEE transactions on pattern analysis and machine intelli-
gence 35 (8) (2013) 1798–1828.

[41] A. Kalousis, J. Gama, M. Hilario, On data and algorithms: Understanding
inductive performance, Machine Learning 54 (3) (2004) 275–312. doi:10.
1023/B:MACH.0000015882.38031.85.
URL https://doi.org/10.1023/B:MACH.0000015882.38031.85

[42] C. E. Shannon, A mathematical theory of communication, The Bell system

technical journal 27 (3) (1948) 379–423.

[43] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, Enriching word vectors with
subword information, Transactions of the Association for Computational Lin-
guistics 5 (2017) 135–146. doi:10.1162/tacl_a_00051.
URL https://www.aclweb.org/anthology/Q17-1010

[44] V. Efstathiou, C. Chatzilenas, D. Spinellis, Word embeddings for the software
engineering domain, in: A. Zaidman, Y. Kamei, E. Hill (Eds.), Proceedings
of the 15th International Conference on Mining Software Repositories, MSR
2018, Gothenburg, Sweden, May 28-29, 2018, ACM, 2018, pp. 38–41. doi:
10.1145/3196398.3196448.
URL https://doi.org/10.1145/3196398.3196448

[45] A. K. Uysal, S. Gunal, The impact of preprocessing on text classiﬁ-
Information Processing & Management 50 (1) (2014) 104–112.

cation,
doi:https://doi.org/10.1016/j.ipm.2013.08.006.
URL
S0306457313000964

https://www.sciencedirect.com/science/article/pii/

[46] R. Caruana, Multitask learning, Machine Learning 28 (1) (1997) 41–75. doi:

10.1023/A:1007379606734.
URL https://doi.org/10.1023/A:1007379606734

[47] S. Ruder, An overview of multi-task learning in deep neural networks, arXiv

preprint arXiv:1706.05098 (2017).

41

[48] A. Capiluppi, N. Ajienka, The relevance of application domains in empirical
ﬁndings, in: B. Adams, E. Constantinou, T. Mens, K. Stewart, G. Robles
(Eds.), Proceedings of the 2nd International Workshop on Software Health,
SoHeal@ICSE 2019, Montreal, QC, Canada, May 28, 2019, IEEE / ACM,
2019, pp. 17–24.
URL https://dl.acm.org/citation.cfm?id=3355304

[49] P. Ye, D. S. Doermann, Active sampling for subjective image quality assess-
ment, in: 2014 IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, IEEE Computer
Society, 2014, pp. 4249–4256. doi:10.1109/CVPR.2014.541.
URL https://doi.org/10.1109/CVPR.2014.541

[50] P. Runeson, M. H¨ost, A. Rainer, B. Regnell, Case Study Research in Software

Engineering - Guidelines and Examples, Wiley, 2012.
URL
productCd-1118104358.html

http://eu.wiley.com/WileyCDA/WileyTitle/

[51] P. Sen, A. Saﬀari, What do models learn from question answering datasets?,
in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020, Association for Computational Linguistics,
2020, pp. 2429–2438. doi:10.18653/v1/2020.emnlp-main.190.
URL https://doi.org/10.18653/v1/2020.emnlp-main.190

[52] N. B. Shah, S. Balakrishnan, J. K. Bradley, A. Parekh, K. Ramchandran,
M. J. Wainwright, Estimation from pairwise comparisons: Sharp minimax
bounds with topology dependence, Journal of Machine Learning Research 17
(2016) 58:1–58:47.
URL http://jmlr.org/papers/v17/15-189.html

42

