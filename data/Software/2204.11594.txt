Addressing Leakage in Self-Supervised Contextualized Code Retrieval

Johannes Villmow and Viola Campos and Adrian Ulges and Ulrich Schwanecke
RheinMain University of Applied Sciences
Wiesbaden, Germany
{firstname.lastname}@hs-rm.de

Abstract

We address contextualized code retrieval, the
search for code snippets helpful to ﬁll gaps in
a partial input program. Our approach facil-
itates a large-scale self-supervised contrastive
training by splitting source code randomly into
contexts and targets. To combat leakage be-
tween the two, we suggest a novel approach
based on mutual identiﬁer masking, dedenta-
tion, and the selection of syntax-aligned tar-
gets. Our second contribution is a new dataset
for direct evaluation of contextualized code re-
trieval, based on a dataset of manually aligned
subpassages of code clones. Our experiments
demonstrate that our approach improves re-
trieval substantially, and yields new state-of-
the-art results for code clone and defect detec-
tion.

1

Introduction

AI-supported software development has experi-
enced growing interest recently (Lu et al., 2021).
Its most intensely researched tasks include code
auto-completion (Svyatkovskiy et al., 2020), natu-
ral language code search (Husain et al., 2019), and
code clone detection (Svajlenko and Roy, 2015).
All these tasks require a semantic understanding
of code, and are commonly tackled by pretrained
transformers (Vaswani et al., 2017).

Our focus is on a related task called contextu-
alized code search (Mukherjee et al., 2020b; Da-
hal et al., 2022): Given an incomplete piece of
code and a certain position of interest (e.g., the cur-
rent cursor position), a retriever searches for code
fragments that are relevant for ﬁlling in the miss-
ing piece. This setting aligns well with program-
mers’ workﬂow , and differs substantially from
the three above tasks: (1) In contrast to natural
language code search, retrievers in contextualized
code search can exploit local code context, which
may (but does not have to) include a comment de-
scribing the missing step. (2) Code generated by

autocompletion (e.g. from GitHub’s CodEx (Chen
et al., 2021)) is prone to subtle programming er-
rors that may alter the behaviour of the current
program, particularly if the current code status still
contains bugs or larger gaps. In contrast, contextu-
alized search leaves the developer in charge, and
the origin of a solution remains transparent. (3)
In contrast to clone detection, contextualized code
search is not targeted at semantically similar pieces
of code but pieces of code that complement each
other.

A key challenge with contextualized code search
is that supervised labels for relevant code pas-
sages are missing. Therefore, we bootstrap a self-
supervised learning process by drawing inspira-
tion from Cloze Tasks in natural language process-
ing (Lee et al., 2019): Given a large-scale dataset
containing a piece of code, we erase a random
block. We refer to this block as the target, and
to the rest as context. Together, they both form a
positive sample for contrastive learning.

Unfortunately, this approach suffers from leak-
age between context and target (see Figure 1): (1)
Both share common identiﬁers, (2) the target’s in-
dentation level matches the position of interest in
the query, and (3) if context and target divide a syn-
tactic primitive (e.g., a for-loop), the target can eas-
ily be identiﬁed by bracket matching the ones in the
context. Retrievers might exploit all these effects
and bypass semantic similarity. To this end, our
ﬁrst contribution is a novel approach towards self-
supervised code retrieval, which avoids the above
bias through de-leaking steps, namely mutual iden-
tiﬁer masking, dedentation, and target selection
aligned with the code’s syntactic primitives.

The second challenge we address is evaluation:
So far, the focus of evaluating code retrieval sys-
tems has been on natural language queries (which
can be bootstrapped from docstrings) (Husain et al.,
2019). Contextualized code retrieval has been eval-
uated only indirectly via inﬁlling quality (Lu et al.,

2
2
0
2

r
p
A
7
1

]
E
S
.
s
c
[

1
v
4
9
5
1
1
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Figure 1: Our approach bootstraps code pairs for contrastive learning from random source code blocks. To avoid
leakage, such as cutting syntactic primitives (red), we (a) select the target using the code’s syntactic structure
(green). We split (b) the code into context (gray) and target (green). We further (c) mutually mask identiﬁers
(yellow) and dedent the target (green arrow). Finally, we apply contrastive learning on the resulting code pairs (d).

2022), which poorly reﬂects the actual retrieval
quality. Therefore, our second contribution is a
rigorous evaluation of contextualized code retrieval
on a manually curated dataset based on aligned
code clones. We coin COCOS and make available
for future research. We demonstrate on said dataset
that retrieval quality beneﬁts substantially from our
de-leaking approach. Also, we achieve state-of-
the-art results on the related tasks code clone and
defect detection.

2 Approach

Our system takes arbitrary pieces of code
and bootstraps code pairs for contrastive learn-
ing. More speciﬁcally, given a piece of code
as a token sequence X=x1, . . . , xn, we se-
target code snippet Y =xi, . . . , xi+L with
lect
n, i, L ∈ N , obtaining a masked context version
X (cid:48)=x1, . . . , xi−1, xMASK, xi+L+1, . . . , xn.
To
X (cid:48), Y we prepend a language-speciﬁc CLS token.
To address the above mentioned leakages we uti-
lize the concrete syntax tree of a piece of code1.
The leaves of this tree consist of all code tokens in-
cluding whitespace tokens. First we propose tree-
based span selection (TS) and use a program’s
syntax tree to determine the cut-out target span Y
(see Figure 1). By selecting syntax tree nodes for re-
moval, we ensure to always remove a syntactically
complete piece, using the following procedure: We
sample the target’s length L from a normal distribu-
tion with µ=150 and σ=90. Then select a node n
in the syntax tree covering at most L leaves/tokens

1We use the tree-sitter library for parsing.

and iteratively expand the selection, either to n’s
parent, or by adding n’s direct siblings, until reach-
ing the desired size L. Adding siblings allows for
multiline targets spanning several statements (but
not a complete block). Second, we suggest mu-
tual identiﬁer masking (IM)2: By modifying the
syntax tree, we replace identiﬁers such as variable
names with special tokens (e.g. VAR1, VAR2)
dynamically during training, resulting in a more
structure-oriented approach. To minimize not only
masking but at the same time leakage we mask
only mutual identiﬁers present in both context and
target. We hide 90% of those mutual identiﬁers
randomly either in the context or in target code.
For 5% of context-target pairs we omit identiﬁer
masking overall.

Finally, we determine the indentation level of
the target code and dedent (DE) it, so that it has
indentation level zero.

2.1 Training

We encode both sequences with the same trans-
former encoder and obtain sequence embeddings
q, k ∈ Rd for context code X (cid:48) and target Y by
using the encoding of the CLS token. Following
Wang et al. (2021b) we optionally pretrain the trans-
former with alternating generation tasks such as
identiﬁer masking and span prediction3 . The re-
triever is then trained by optimizing the contrastive

2What is an identiﬁer is deﬁned in the grammar of a tree-
sitter parser and varies between programming languages. I.e.
we do not differentiate between variables, method names or
method calls.

3We omit identiﬁer detection and instead use our tree-

based span selection to generate large and small spans.

import mysql.connectordef totalSalary(id,name):  connection =       mysql.connector.connect(host=‘localhost’, user=‘root’,database=‘db’)  cursor = connection.cursor()  query = ("SELECT wage,bonus             FROM employees           WHERE emp_no = %s            AND emp_name = %s")  cursor.execute(query,   (id, name))  row = cursor.fetchone()  salary,bonus = row  return salary + bonus) 1 2 3 4 5 6 7 8 91011121314151617181920212223  query = ("SELECT wage,bonus             FROM employees           WHERE emp_no = %s            AND emp_name = %s",            id,name)  cursor.execute(query,   (id, name))  row = cursor.fetchone()  salary,bonus = row 1 2 3 4 5 6 7 8 9reduceleakageEncodercontrastive lossEncoder(a)(b)(c)(d)import mysql.connectordef totalSalary(VAR2,name):  connection =       mysql.connector.connect(host=‘localhost’, user=‘root’,database=‘db’)  cursor = connection.cursor()  return VAR1 + VAR3) 1 2 3 4 5 6 7 8 9101112131415mutual identifier maskingTree-based span selection  query = ("SELECT wage,bonus             FROM employees           WHERE emp_no = %s            AND emp_name = %s",            id,VAR1)  VAR2.execute(query,   (id, VAR1))  row = VAR2.fetchone()  salary,bonus = row 1 2 3 4 5 6 7 8 9Naive span selectiontargetcontextDedentInfoNCE (van den Oord et al., 2018) loss with in-
batch negative samples :

LΘ = −log

exp(f (q, k+)/τ ))
i=0 exp(f (q, k−

(cid:80)K−2

i )/τ ))

(1)

Where f is cosine similarity, K is the amount of
sequences in our batch and τ =0.1 temperature. To
obtain harder negative samples - which have been
found crucial for good retriever training (Ren et al.,
2021) - we form batches only with samples from
the same programming language.

3 Dataset

Our self-supervised code retrieval model is pre-
trained on 33M ﬁles in 16 programming languages
(see Appendix A). As code ﬁles tend to be large,
we truncate them using tree-based span selec-
tion (compare Section 2): Starting from a ﬁle we
randomly select sufﬁciently large spans of code
(length between 150 and 800 tokens). We remove
those segments from the original ﬁle, and feed the
shortened ﬁle as well as all individual segments as
inputs X into the learning process described in Sec-
tion 2. A special identiﬁer (similar to code folding
in the IDE) marks those positions in the original
ﬁle where segments have been removed.

3.1 Contextualized Code Retrieval Dataset

Evaluating contextualized code retrieval models is
hard, due to limited to no suitable evaluation data
being available indicating which subblocks in code
implement the same functionality. To address this
gap, we have created a new dataset based on Big-
CloneBench (Svajlenko and Roy, 2015) – a code
clone dataset that provides pairs of functions im-
plementing the same functionality4. We manually
select a sub-passage from a function as the target
and label which lines in the function’s clones match
this target (see Figure 3 - 5). By extracting these
targets and their surrounding contexts, we can eval-
uate how well a model retrieves targets implement-
ing the same functionality. Note that the original
target is not considered. We manually gather 387
context-target pairs implementing 20 randomly se-
lected functionalities and coin the resulting dataset
COCOS ( Contextualized Code Search).

4In that dataset two functions are considered clones, but
the shared functionality may only be a part of the function.
We use the shared functionality part as relevant targets and
obtain different contexts.

Model Features MAP NDCG P@1

P@3

P@10

TS, PT
TS, IM, DE
TS, IM, DE, PT

50.04
56.07
69.14

77.71
76.24
85.85

78.12
38.8
74.48

69.97
42.8
77.95

56.56
54.58
73.72

Table 1
Zeroshot code retrieval results for different deleaking steps:
Tree-based span selection (TS); mutual identiﬁer masking
(IM); dedenting (DE); pretraining (PT). Compare Section 2.

4 Evaluation

We evaluate our model on zero-shot code retrieval
on our COCOS dataset and on two similar code
understanding tasks from CodeXGlue (Lu et al.,
2021), speciﬁcally code clone detection and code
defect detection. During unsupervised training we
measure mean reciprocal rank (MRR) on 30k sam-
ples of the held out validation set (for which we
apply the same modiﬁcations). For all experiments
we report test results of the model with the highest
validation MRR.

4.1 Zeroshot Code Retrieval

Without further ﬁne-tuning we directly evaluate
our self-supervised model on the COCOS dataset
in a zero-shot setting. For each context we rank
all possible targets, exclude the original target. In
Table 1 we report mean average precision (MAP),
normalized discounted cumulative gain (NDCG)
and precision at k. The baseline trained without
deleaking steps dedentation and mutual identiﬁer
masking is able to retrieve samples with similar
identiﬁers (high precision at 1), but fails to con-
sistently retrieve all relevant targets. Instead our
approach – that uses deleaking steps – retrieves
more relevant samples. This is visible in Figure 2,
where our approach forms better clusters for both
contexts and targets.

4.2 Clone Detection

We evaluate our model on clone detection on
the POJ-104 dataset (Mou et al., 2016). The
POJ-104 dataset consists of C and C++ programs
for 104 problems from an open programming plat-
form (OJ). We follow the evaluation procedure
of CodeXGlue and report mean average precision
(MAP@R) with R=499. We ﬁnd that our model
outperforms state-of-the-art by a large margin.

4.3 Defect Detection

We evaluate our model on defect detection on the
Devign dataset (Zhou et al., 2019). It consists

Model

Clone

Defect

MAP@R Accuracy

RoBERTa (code)
CodeBERT
code2vec
PLBART
GraphCodeBERT
SynCoBERT
C-BERT
CodeT5
CoTexT

Ours

76.67
82.67
1.98
-
85.16
88.24
-
-
-

91.34

61.05
62.08
62.48
63.18
63.21
64.50
65.45
65.78
66.62

68.33

Table 2
Results on code clone detection on POJ-104 and defect de-
tection on Devign dataset.

of vulnerable C functions manually collected from
open source projects. The task is to predict whether
the function is vulnerable. Following CodeXGlue
we report accuracy.

5 Related Work

Given the success of pre-trained language mod-
els in NLP, some recent work has extended pre-
training to program syntax. Kanade et al. (2020)
and Feng et al. (2020) train a BERT encoder on
source code using masked language modeling. Guo
et al. (2021) propose GraphCodeBERT to incor-
porate structural information like data ﬂow. Be-
sides these encoder models, Svyatkovskiy et al.
(2020) and Liu et al. (2020) developed CodeGPT
and CugLM, both based on the transformer decoder
and pre-trained on pairs of natural language and
program code. Ahmad et al. (2021) and Wang et al.
(2021b) proposed PLBART and CodeT5, follow-
ing the architecture of BART and T5 respectively,
trained on functions in multiple programming lan-
guages paired with natural language comments.
SynCoBERT (Wang et al., 2021a) is trained on
various pre-training tasks on multi-modal data, in-
cluding code, comment and AST representations.
Guo et al. (2022) propose UniXcoder, which takes
a similar approach but employs a encoder-decoder
architecture instead of a single encoder.

In most of the above work, multiple modalities
have been applied, e.g. code and natural language
comments. In contrast to contextual code search,
this setup does not come with leakage, which is the
main concern of our work.

Code-to-code search A recent line of work uses
program code as additional context for natural lan-
guage queries to clarify the programmer’s intent.
FaCoY (Kim et al., 2018) extends the query with
related code from StackOverFlow and searches
similar code fragments in the source code index.
SCOTCH (Dahal et al., 2022) studies query aug-
mentation with surrounding source code to improve
search results. Jain et al. (2021) propose Contra-
Code, which introduces compiler-based semantic-
preserving code transformations as data augmenta-
tions and trains the neural network on a contrastive
learning objective comparing similar and dissimi-
lar code snippets. Plain code-to-code search is in-
vestigated in (Ragkhitwetsagul and Krinke, 2019),
where code fragments are used as query to ﬁnd sim-
ilar code clones in a code base. To improve perfor-
mance, multiple code representations are combined
to capture code structure at different levels. Aroma
(Luan et al., 2019) clusters candidate code and in-
tersects the snippets in each cluster to recommend
likely subsequent code for a given snippet.

Contextualized Code Search Mukherjee et al.
(2020a) address contextualized code search by de-
compiling code fragments into a simpler represen-
tation called SKETCH (Murali et al., 2017) to learn
a statistical search model. Lu et al. (2022) propose
ReACC and use partial code as search query in
the context of retrieval-augmented code comple-
tion. They focus on improving generation with
results from a contrastive retriever. Differing to
ours they do not train to retrieve partial targets,
but aim to retrieve the full augmented ﬁle insert-
ing dead code and renaming variables to combat
leakage. However, even with dead code insertion a
strong structural overlap between query and target
exists. Compared to ContraCode and ReACC our
steps towards leakage reduction are much simpler.

6 Conclusion

We have proposed a new approach towards unsu-
pervised code retrieval, which reduces leakage be-
tween randomly drawn targets and their contexts.
We also contribute a dataset COCOS, on which we
demonstrate via ablations that leakage reduction is
crucial for an efﬁcient training. Also, our approach
yields competitive representations for related tasks,
as demonstrated by new state-of-the-art results on
clone and defect detection. An interesting future
direction will be to combine our retriever with gen-
erators for a combined, unsupervised trainig.

References

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2021. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655–2668,
Online. Association for Computational Linguistics.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott
Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
Lukasz Kaiser, Mohammad Bavarian, Clemens Win-
ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-
ating large language models trained on code. CoRR,
abs/2107.03374.

Samip Dahal, Adyasha Maharana, and Mohit Bansal.
2022. Scotch: A semantic code search engine for
IDEs. In Deep Learning for Code Workshop.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, et al. 2020. Codebert: A
pre-trained model for programming and natural lan-
guages. arXiv preprint arXiv:2002.08155.

Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, and Jian Yin. 2022. Unixcoder: Uniﬁed cross-
modal pre-training for code representation. arXiv
preprint arXiv:2203.03850.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu
Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin,
Daxin Jiang, and M. Zhou. 2021. Graphcode-
bert: Pre-training code representations with data
ﬂow. ArXiv, abs/2009.08366.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search.

Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel,
Joseph Gonzalez, and Ion Stoica. 2021. Contrastive
code representation learning. In Proceedings of the
2021 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020. Learning and evaluating con-
textual embedding of source code. In Proceedings
of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine
Learning Research, pages 5110–5121. PMLR.

Kisub Kim, Dongsun Kim, Tegawendé Bissyandé, Eun-
jong Choi, Li Li, Jacques Klein, and Yves Le Traon.
2018. F a c o y: a code-to-code search engine. pages
946–957.

Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Conference of the Association for Computa-
tional Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
6086–6096. Association for Computational Linguis-
tics.

Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-
task learning based pre-trained language model for
the 35th
code completion.
IEEE/ACM International Conference on Automated
Software Engineering, ASE ’20, page 473–485, New
York, NY, USA. Association for Computing Machin-
ery.

In Proceedings of

Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-
won Hwang,
and Alexey Svyatkovskiy. 2022.
ReACC: A retrieval-augmented code completion
framework.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
ArXiv, abs/2102.04664.

Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen,
and Satish Chandra. 2019. Aroma: Code recom-
mendation via structural code search. Proc. ACM
Program. Lang., 3(OOPSLA).

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.
2016. Convolutional neural networks over tree struc-
tures for programming language processing. In Pro-
ceedings of the Thirtieth AAAI Conference on Artiﬁ-
cial Intelligence, AAAI’16, page 1287–1293. AAAI
Press.

Rohan Mukherjee, Swarat Chaudhuri, and Chris Jer-
maine. 2020a. Searching a database of source codes
using contextualized code search. Proc. VLDB En-
dow., 13(10):1765–1778.

Rohan Mukherjee, Chris Jermaine, and Swarat Chaud-
huri. 2020b. Searching a database of source codes
using contextualized code search. Proc. VLDB En-
dow., 13(10):1765–1778.

Vijayaraghavan Murali, Swarat Chaudhuri, and Chris
Jermaine. 2017. Bayesian sketch learning for pro-
gram synthesis. CoRR, abs/1703.05698.

Chaiyong Ragkhitwetsagul and Jens Krinke. 2019.
Siamese: Scalable and incremental code clone
search via multiple code representations. Empirical
Softw. Engg., 24(4):2236–2284.

Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,
QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong
Wen. 2021. RocketQAv2: A joint training method
for dense passage retrieval and passage re-ranking.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2825–2835, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.

Jeffrey Svajlenko and Chanchal K. Roy. 2015. Evalu-
ating clone detection tools with bigclonebench. In
2015 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pages 131–
140.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
IntelliCode Com-
and Neel Sundaresan. 2020.
pose: Code Generation Using Transformer, page
1433–1443. Association for Computing Machinery,
New York, NY, USA.

Aäron van den Oord, Yazhe Li, and Oriol Vinyals.
2018. Representation learning with contrastive pre-
dictive coding. CoRR, abs/1807.03748.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao
Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin
Jiang. 2021a.
Syncobert: Syntax-guided multi-
modal contrastive pre-training for code representa-
tion.

CodeT5:

Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H.
Identiﬁer-aware uniﬁed
Hoi. 2021b.
pre-trained encoder-decoder models for code under-
standing and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 8696–8708, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning
Du, and Yang Liu. 2019. <i>Devign</i>: Effective
Vulnerability Identiﬁcation by Learning Comprehen-
sive Program Semantics via Graph Neural Networks.
Curran Associates Inc., Red Hook, NY, USA.

A Pre-training Dataset Details

We crawl 237k active GitHub repositories with
more than 10 stars5 and perform per ﬁle dedupli-
cation. We keep ﬁles in programming languages
for which a tree-sitter parser is available (16 lan-
guages). The resulting dataset is shown in Table 3
and consists of ≈ 33M code ﬁles in 16 program-
ming languages. We select 570 repositories for
validation.

Language Training

Valid

Total

Java
JavaScript
C++
Python
C#
TypeScript
C
PHP
Go
Ruby
Rust
CSS
Scala
Haskell
OCaml
Julia

7,345,753
4,471,689
3,734,357
3,016,545
2,843,642
2,299,964
2,242,379
2,206,063
1,759,600
1,068,668
366,891
349,525
273,822
114,311
55,838
34,403

8,434
14,134
1,698
4,718
570
2,392
781
4,648
129
3,397
54
2,579
1,198
177
0
29

7,354,187
4,485,823
3,736,055
3,021,263
2,844,212
2,302,356
2,243,160
2,210,711
1,759,729
1,072,065
366,945
352,104
275,020
114,488
55,838
34,432

Table 3
Number of ﬁles in unsupervised pre-training dataset.

B Training Details

On all models and tasks we use the AdamW op-
timizer and linearly increase the learning rate for
10% of the training steps, along with a polynomial
decay for the remaining steps.

We train our unsupervised models for 500k steps
on a single A6000 GPU, with a peak learning rate
of 0.0001 and use a dynamic batch size so that
batches contain around 7000 tokens.

For clone and defect detection we ﬁne-tune our
model on the respective training set. Following
Wang et al. (2021b) we run a brief sweep over
learning rate, batch size and number of epochs and
report results of the model with highest validation
score, using the published evaluation code.

We will release our code in future.

5We consider a repository as active if there has been a pull

request between 04/21 and 09/21

public boolean extract(File f, String folder) {

Enumeration entries;
ZipFile zipFile;
try {

zipFile = new ZipFile(f);
entries = zipFile.getEntries();
[MASK]
zipFile.close();

} catch (IOException ioe) {

this.errMsg = ioe.getMessage();
Malgn.errorLog(

"{Zip.unzip} " + ioe.getMessage()

);
return false;

}
return true;

}

Figure 2: TSNE comparison between the embeddings of
the baseline model with leakage (top) and our steps for
leakage reduction have been applied (bottom).

Figure 3: Incomplete and masked query X (cid:48) from our
COCOS dataset. The [MASK] token denotes the current
position of interest (cursor). Code that extracts elements
from a zip ﬁle needs to be found. It can be seen that our
approach forms better clusters.

while (entries.hasMoreElements()) {

ZipArchiveEntry entry =

(ZipArchiveEntry) entries.nextElement();

if (entry == null) continue;
String path = folder + "/"

+ entry.getName().replace('\\', '/');

if (!entry.isDirectory()) {

File destFile = new File(path);
String parent = destFile.getParent();
if (parent != null) {

File parentFile = new File(parent);
if (!parentFile.exists()) {
parentFile.mkdirs();

}

}
copyInputStream(

zipFile.getInputStream(entry),
new BufferedOutputStream(

new FileOutputStream(destFile)

)

);

}

}

ArchiveEntry ae = zis.getNextEntry();
while(ae != null) {

//Resolve new file
File newFile = new File(

outputdir + File.separator + ae.getName()

);

//Create parent directories if not exists
if(!newFile.getParentFile().exists())
newFile.getParentFile().mkdirs();

if(ae.isDirectory()) { //create if not exists

if(!newFile.exists())

newFile.mkdir();

} else { //If file, write file

FileOutputStream fos = new FileOutputStream(

newFile);

int len;
while((len = zis.read(buffer)) > 0) {

fos.write(buffer, 0, len);

}
fos.close();

}

//Proceed to the next entry in the zip file
ae = zis.getNextEntry();

}

Figure 4: The masked section Y manually selected from
X (Figure 3). It has been dedented for better readability.

Figure 5: Possible solution that implements the same
functionality as the target in Figure 4

v = Context (Leakage)v = Target (Leakage)v = Context (No Leakage)v = Target (No Leakage)Decompress zip archive.Bubble Sort ArraySetup SGVSetup SGV Event HandlerInitialize Java Eclipse Project.Get Prime FactorsShuffle Array in PlaceLoad Custom FontCreate Encryption Key FilesPlay SoundTake Screenshot to FileEncrypt To FileOpen File in Desktop ApplicationGCDConvert Date String FormatConnect to DatabaseGet MAC Address StringParse CSV FileTest PalindromeWrite PDF File