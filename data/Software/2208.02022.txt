2

2

0

2

l

u

J

0

3

]

R

G

.

s

c

[

1

v

2

2

0

2

0

.

8
0
2
2
:
v
i
X
r
a

Graphical Abstract

A Brief Note on Building Augmented Reality Models for Scientiﬁc
Visualization

Mrudang Mathur, Josef M. Brozovich, Manuel K. Rausch

 
 
 
 
 
 
Highlights

A Brief Note on Building Augmented Reality Models for Scientiﬁc
Visualization

Mrudang Mathur, Josef M. Brozovich, Manuel K. Rausch

• We introduce an open-source pipeline to build and render augmented

reality (AR) models for scientiﬁc visualization.

• We showcase our pipeline by building static and dynamic AR models

by means of ﬁnite element simulation results.

• We simplify and automate the creation of AR models through python

scripts in Blender.

A Brief Note on Building Augmented Reality Models
for Scientiﬁc Visualization

Mrudang Mathura, Josef M. Brozovichb, Manuel K. Rauschb,c,d

aUniversity of Texas at Austin, Department of Mechanical Engineering, 204 E Dean
Keeton Street, Austin, 78712, TX, United States of America
bUniversity of Texas at Austin, Department of Aerospace Engineering and Engineering
Mechanics, 2617 Wichita Street, Austin, 78712, TX, United States of America
cUniversity of Texas at Austin, Department of Biomedical Engineering, 107 W Dean
Keeton Street, Austin, 78712, TX, United States of America
dUniversity of Texas at Austin, Oden Institute for Computational Engineering and
Sciences, 201 E 24th Street, Austin, 78712, TX, United States of America

Abstract

Augmented reality (AR) has revolutionized the video game industry by
providing interactive, three-dimensional visualization. Interestingly, AR tech-
nology has only been sparsely used in scientiﬁc visualization. This is, at least
in part, due to the signiﬁcant technical challenges previously associated with
creating and accessing such models. To ease access to AR for the scien-
tiﬁc community, we introduce a novel visualization pipeline with which they
can create and render AR models. We demonstrate our pipeline by means
of ﬁnite element results, but note that our pipeline is generally applicable
to data that may be represented through meshed surfaces. Speciﬁcally, we
use two open-source software packages, ParaView and Blender. The models
are then rendered through the <model-viewer> platform, which we access
through Android and iOS smartphones. To demonstrate our pipeline, we
build AR models from static and time-series results of ﬁnite element simu-
lations discretized with continuum, shell, and beam elements. Moreover, we
openly provide python scripts to automate this process. Thus, others may
use our framework to create and render AR models for their own research
and teaching activities.

Keywords: mixed reality, virtual reality, digital twin, metaverse, ﬁnite
elements

Preprint submitted to Elsevier

August 4, 2022

1. Introduction

Scientiﬁc data are often inherently three-dimensional. Examples include
imaging results or outputs from numerical methods. However, despite data’s
inherent three-dimensionality our standard visualization techniques remain
two-dimensional, as is the case with ﬁgures or videos. Additionally, current
visualization techniques lack interactivity.
In contrast, Augmented Real-
ity (AR) models can represent the complete spatial and temporal aspects of
data, are interactive in nature, and are easily accessible through smartphones.
That is, AR is a next-generation visualization technique that overlays com-
puter graphics directly into the physical space surrounding the user, thereby
creating an immersive experience[1]. While originally championed by the
entertainment, gaming, and computer graphics communities, AR now ﬁnds
several applications in manufacturing[2], medicine[3], and education[4].

Despite the clear beneﬁts of AR models, their adoption in research and
education has been limited[5, 6]. This is, at least in part, due to the use of
proprietary software and hardware previously needed to create and render
AR experiences, respectively[7]. Additionally, creating AR models often re-
quires specialized training in computer graphics and 3D modeling that is not
germane to most disciplines[8]. Thus, the objective of our current work is to
help the scientiﬁc community to overcome some of these challenges associated
with AR visualization. To do so, we introduce a novel open-source pipeline
to build and render AR models of mesh-based scientiﬁc data. For demonstra-
tion purposes we focus speciﬁcally on visualization of ﬁnite element results
but note that our work is equally applicable to many other disciplines and
data types.

2. Material and methods

Towards building and rendering AR models of mesh-based data, we pro-
pose an open-source pipeline using ParaView (Kitware Inc, Clifton Park,
NY), Blender (Blender Foundation BV, Amsterdam, The Netherlands), and
<model-viewer> (Google Inc, Mountain View, CA), see Figure 1. While
we focus on ﬁnite element analyses in this article, our pipeline can also be
used to create AR models of data from non-numerical experiments such as
imaging[9, 10]. Brieﬂy, we ﬁrst export our simulation results from a ﬁnite
element solver, for example Abaqus (v6.20-1, Dassault Systémes, Vélizy-
Villacoublay, France), to a ﬁle format compatible with ParaView. In Par-
aView, we then create and export a surface-only version of our simulation

2

Figure 1: Tricuspid valve ﬁnite element model rendered through novel AR visu-
alization pipeline: We transfer simulation results from a ﬁnite element solver, Abaqus in
this case, to ParaView, an open-source visualization package. In ParaView, we then spec-
ify engineering metrics to display and adjust the visualization colormap. Next, we export
a surface geometry of our results to Blender, a popular open-source 3D modeling software.
In Blender, we calibrate geometry scaling, translation, rotation, as well as lighting for AR
visualization and animate our time-series results. Finally, we export AR models that are
rendered through <model-viewer> on a smartphone.

mesh and data. Next, we transfer these surface meshes to Blender where
we calibrate model size, location, as well as orientation and create anima-
tions in case of time-series data. From Blender, we then export Android
and iOS-device compatible AR assets that we host on our website using
<model-viewer>. Finally, as a demonstration we build AR models contain-
ing element types commonly used in computational mechanics: continuum
elements and structural elements such as shells and beams. A detailed de-
scription of the post-processing steps is provided in the following sections.
Additionally, further details on software versions and ﬁle formats used are
provided in Appendix A and Appendix B, respectively.

2.1. Creating a surface mesh

We ﬁrst assemble our mesh-based results in the *.vtk ﬁle format. Cre-
ating *.vtk ﬁles is extensively documented in the literature and we direct
readers to an excellent guide detailing this process [11]. Next, we import the
*.vtk ﬁles in ParaView and calibrate the visualization to better represent
engineering metrics such as stress, strain, and displacement. To this end, we
ﬁlter our data and apply colormaps to our simulation mesh. We then gener-
ate a surface-only mesh of our results. Finally, we export the surface mesh
of our model as a binary *.ply ﬁle. Please note, we embed any engineering
metrics as vertex colors in these ﬁles. We provide example *.ply ﬁles for
continuum, shell, and beam elements on the GitHub repository supplemen-

3

Figure 2: Post-processing operations in Blender: We process surface geometries from
ParaView in three distinct ways based on AR model type and rending platform. To this
end, we classify AR models as static models, dynamic AR models for Android devices, and
dynamic AR models for iOS devices. We customize model size, position, and lighting in
all cases, adjust animation timing for time series data, and add vertex colors to all models
except dynamic models for iOS (see our discussion section for more detail for why we omit
colors in iOS dynamic models).

tal to this article. Additionally, see Appendix C for a list of ﬁlters used to
create surface meshes of the aforementioned elements in ParaView.

2.2. Building AR models

To create AR models from surface meshes of our ﬁnite element results,
we follow a multi-step process in Blender, as described in Figure 2. Here,
we employ a common process to build static AR models for all mobile plat-
forms. However, because Android and iOS devices use ARCore and ARKit
as rendering libraries, respectively, they require distinct treatment when vi-
sualizing time-varying data. Thus, our process diﬀers when animating our
time-series data results in dynamic AR models for Android and iOS devices.

2.2.1. Static models

To build a static AR model, we ﬁrst import the *.ply ﬁle in Blender
where we adjust model scaling, translation, and rotation. To visualize any
colormaps that we add to our model in ParaView, we ﬁrst create a "Mate-
rial" within Blender. "Materials" are data structures in Blender that store
and govern the appearance and texture of models. To our "Material", we
assign vertex colors that are embedded in the *.ply ﬁle. Next, we apply a

4

"Decimate Modiﬁer" to our geometry. This reduces the number of polygons
in our AR assets which, in turn, reduces their storage size. Furthermore,
we apply a "Solidify" modiﬁer to shell geometries. This enhances model vis-
ibility while rendering. Additionally, we adjust lighting around our model
in 3D space. Finally, we export an Android-compatible *.glb ﬁle through
Blender’s native exporter. To generate an iOS-compatible *.usdz ﬁle we
use the BlenderUSDZ add-on via the Blender GUI. To simplify the model
creation process, we have also automated the above described steps using
Python. See Supplementary Scripts 1a-c to create static AR models of
simulations with continuum, shell, and beam elements, respectively.

2.2.2. Dynamic models: Android

First, we follow the same steps used to create static models. That is, we
import all geometries of our time-series data, perform aﬃne transformations,
and add the appropriate colormaps to our models in Blender. Next, we create
a stop-motion animation of our time-series results. To this end, we ensure
that only a single geometry is visible in each frame of our animation. Thereby,
we emulate mesh motion between each animation frame. To achieve this, we
dynamically resize each geometry in our dataset over the time-span of the
animation. Finally, we adjust model lighting and exported a *.glb ﬁle of our
animation for rendering on Android devices. We provide Supplementary
Scripts 2a-c to automatically build dynamic AR models of the continuum,
shell, and beam element examples.

2.2.3. Dynamic models: iOS

To build dynamic models for iOS devices, we ﬁrst imported all *.ply ﬁles
into Blender. Next, we adjusted the scaling, translation, and rotation of our
models. We then animated our time-series data using Shape Keys. To this
end, Blender automatically builds a material point correspondence between
each mesh in our dataset. As a result, we exactly animate the motion of each
vertex in our model geometry over time. Please note, Shape Keys require
the number of polygons remains constant between meshes. Thus, special
attention should be paid to simulation results with self-contacting geometries
to ensure a constant polygon count. Next, we modiﬁed scene lighting and
exported an intermediate *.usdc ﬁle of our animated model. Finally, we
adjusted model scaling and converted our animated model to *.usdz ﬁles
in Reality Converter or usdzconvert in MacOS and Windows/Linux systems,

5

respectively. See Supplementary Scripts 3a-c to automate Shape Key
animations in Blender.

2.3. Hosting and rendering models

We use GitHub to host, access, and share our AR models. To this end, we
integrate <model-viewer> with our research webpage to render AR assets.
We chose <model-viewer> over other, similar, rendering platforms as it is free
to use, compatible with both Android and iOS devices, and can be accessed
without app download. Finally, we share links to our AR models through
QR codes. This choice is motivated by the relative ease of generating QR
codes, accessing them via smartphone, and by the possibility of embedding
QR codes in presentations, videos, and ﬁgures.

3. Results

To demonstrate our open-source visualization pipeline, we built AR mod-
els of three distinct ﬁnite element analyses from our research group, see Fig-
ure 3a. These include a dynamic model of the human tricuspid valve[12], a
bulge inﬂation test of tissue from a rat’s anterior tricuspid valve leaﬂet[13],
and the uniaxial extension of an undulated ﬁber network. These simulations
are discretized with continuum, shell, and beam elements, respectively. Here,
we present images of the reference and deformed conﬁguration of each sim-
ulation, as one would see in a scientiﬁc ﬁgure. Additionally in Figure 3b,
we provide QR codes leading to a webpage containing static and dynamic
AR models of each simulation. Readers are encouraged to scan these QR
codes and view the linked AR models as described by Figure 4. Using
our open-source pipeline, we successfully build and render AR models of the
aforementioned ﬁnite element analyses. Thereby, we provide scientists, in
general, and mechanicians, in particular, an avenue to create and share their
results in all spatial and temporal dimensions.

4. Discussion

Augmented reality represents the next frontier in personal computing
and has the capacity to transform scientiﬁc visualization. In this brief note,
we introduced an open-source visualization pipeline to build and render AR
models from ﬁnite element simulation results. Our pipeline integrates an
established and versatile tool in scientiﬁc visualization (Paraview) with a

6

Figure 3: AR models of ﬁnite element simulations discretized with continuum,
shell, and beam elements:(a) We present simulation results in the reference and de-
formed conﬁgurations which are overlaid with contours of engineering metrics, such as
maximum principal Cauchy stress and displacement magnitude. (b) Static and dynamic
models of each simulation can be accessed through the associated QR code.

popular 3D modeling tool (Blender) to standardize and simplify the process
of building AR models from scientiﬁc data. We then use an open-source AR
platform (<model-viewer>) to access and render our models on Android and
iOS devices. We also demonstrated our pipeline on results from ﬁnite element
simulations spatially discretized with continuum, shell, and beam elements.
Furthermore, we provide python scripts to automate the creation of those
same models. Thus, we consider this pipeline to be successful in achieving
our stated objectives: to democratize and simplify AR visualization of three-
dimensional data by means of ﬁnite element results. That is, we eschewed the
need for proprietary software, such as NVIDIA Omniverse, and expensive AR

7

Figure 4: Accessing AR models: To view AR models on a smartphone (a) we ﬁrst scan
the QR code and load the associated webpage, (b) we then click the <model-viewer> logo
to enable AR, and ﬁnally (c) we position the AR model in 3D space around us.

headsets, such as the Microsoft Hololens. Moreover, through the provided
python scripts, we signiﬁcantly reduce the training required to use an open-
source, industry-standard 3D modeling tool to create AR assets.

In addition to fulﬁlling these objectives, our pipeline can serve as a cor-
nerstone for future studies integrating scientiﬁc data with mixed-, virtual-,
and augmented-reality applications. Speciﬁcally concerning results from me-
chanical analyses we’d like to note that our methods can be easily extended
to Eulerian grids and do not have to be limited to Lagrangian analyses as
used in our examples[14, 15, 16, 17]. Furthermore, we can build AR mod-
els from isogeometric analysis results due to Blender’s in-built support for
NURBS[18, 19, 20]. Additionally, we may interface Blender with packages
such as Unity to design virtual reality experiences[21]. Finally, when com-
bined with image processing packages, our methods can be used to eﬀectively
visualize and interact with digital twins[22, 23].

Naturally, our methods are subject to certain limitations. Firstly, our
current method of creating dynamic *.usdz ﬁles precludes the use of custom
colormaps. This is currently a limitation of Apple’s underlying USD code-
base. We anticipate that dynamic vertex colors will be added in upcoming
software releases. Secondly, our current *.usdz animation technique requires
users to specify the number of times an animation should repeat. This in-

8

creases the ﬁle size of the AR asset, thereby making it harder to access over
slower internet connections. Moving forward, we aim to programmatically
loop *.usdz animations, similar to those in *.glb ﬁles. Finally, by using
<model-viewer> to render AR models, we require users to create both *glb
and *.usdz models of any scientiﬁc results they have. In the future, we antic-
ipate a greater cross-compatibility in ﬁles between Android and iOS systems,
thereby reducing this burden on content creators.

5. Conclusion

In conclusion, we introduced an end-to-end pipeline for building and ren-
dering AR models from scientiﬁc, mesh-based data[24].
Importantly, our
pipeline only uses open-source software packages and, thus, allows users to
freely access AR models on any smartphone through the internet. Further-
more, we showcased our pipeline by building AR models of ﬁnite element
analyses with three common element discretizations in used computational
mechanics. Moreover, we provided python scripts to automatically create
those models. Through this work, we hope to simplify and accelerate the
adoption of AR visualization in scientiﬁc visualization. Thereby enabling
researchers, educators, and students to gain a deeper understanding of com-
plex spatio-temporal results associated with their data.
Importantly, all
scripts and information necessary to reproduce our work are openly avail-
able through a GitHub repository listed under “Code Availability” below.

Acknowledgements

We appreciate support from the American Heart Association through
an award to Dr. Rausch (18CDA34120028) and a predoctoral fellowship
to Mrudang Mathur (902502), as well as the National Institutes of Health
through an award to Dr. Rausch (1R21HL161832). We would also like to
thank Soham M. Mane for sharing the results of his ﬁber network simulations
with us.

Disclosures

Manuel K. Rausch has a speaking agreement with Edwards Lifesciences.

None of the other authors have conﬂicts of interest to disclose.

9

Author Contributions

MKR and MM wrote the manuscript. MM and JMB developed the code.
MM produced all ﬁgures and tutorials. All authors reviewed the manuscript.

Code Availability

All supplementary scripts, surface geometries, and AR model examples

are available in the GitHub repository associated with this article.
URL: https://github.com/SoftTissueBiomechanicsLab/AR_Pipeline.git

Appendix A. Software requirements

Our proposed pipeline requires the use of multiple open-source software
and is, thus, subject to compatibility errors across diﬀerent software versions.
Therefore, we have compiled a list of stable and compatible software packages.
These packages, along with their download links, are detailed in the GitHub
repository associated with this article. The packages are:

• ParaView 5.10

• Blender 2.83

• BlenderUSDZ add-on

• Apple Reality Converter (for MacOS users)

• Apple usdzconvert utility (for Windows/Unix users)

Appendix B. File formats

To ensure the ﬂexibility of our methods to varied ﬁnite element solvers,
visualization packages, and mixed-reality platforms we use several industry-
standard and open-source ﬁle formats in our pipeline. Speciﬁcally, we employ
*.vtk ﬁles for storing and accessing numerical simulation data and *.ply ﬁles
to store model geometries. Moreover, we render our AR assets as *.glb and
*.usdz ﬁles for Android and iOS devices, respectively.

10

Appendix C. Generating surface meshes in ParaView

To generate surface meshes for results with continuum and shell elements

we use the following ﬁlters:

1. Filters > Alphabetical > Extract Surface
2. Filters > Alphabetical > Generate Surface Normals

For beam elements, we ﬁrst employ the Tube ﬁlter in ParaView viz.:

1. Filters > Alphabetical > Tube
2. Filters > Alphabetical > Extract Surface
3. Filters > Alphabetical > Generate Surface Normals

References

[1] J. Carmigniani, B. Furht, M. Anisetti, P. Ceravolo, E. Damiani,
M. Ivkovic, Augmented reality technologies, systems and applications,
Multimedia Tools and Applications 51 (1) (2011) 341–377. doi:10.
1007/s11042-010-0660-6.

[2] A. Nee, S. Ong, G. Chryssolouris, D. Mourtzis, Augmented reality ap-
plications in design and manufacturing, CIRP Annals 61 (2) (2012) 657–
679. doi:10.1016/j.cirp.2012.05.010.

[3] M. W. Chu, J. Moore, T. Peters, D. Bainbridge, D. McCarty, G. M.
Guiraudon, C. Wedlake, P. Lang, M. Rajchl, M. E. Currie, R. C. Daly,
B. Kiaii, Augmented Reality Image Guidance Improves Navigation for
Beating Heart Mitral Valve Repair, Innovations: Technology and Tech-
niques in Cardiothoracic and Vascular Surgery 7 (4) (2012) 274–281.
doi:10.1097/imi.0b013e31827439ea.

[4] K. N. Plunkett, A Simple and Practical Method for Incorporating Aug-
mented Reality into the Classroom and Laboratory, Journal of Chemi-
cal Education 96 (11) (2019) 2628–2631. doi:10.1021/acs.jchemed.
9b00607.

[5] J. Huang, S. Ong, A. Nee, Real-time ﬁnite element structural analysis in
augmented reality, Advances in Engineering Software 87 (2015) 43–56.
doi:10.1016/j.advengsoft.2015.04.014.

11

[6] C. Hedenqvist, M. Romero, R. Vinuesa, Improving the Learning of
Mechanics Through Augmented Reality, Technology, Knowledge and
Learning (0123456789) (jul 2021). doi:10.1007/s10758-021-09542-1.

[7] C. Sutherland, K. Hashtrudi-Zaad, R. Sellens, P. Abolmaesumi,
P. Mousavi, An Augmented Reality Haptic Training Simulator for
Spinal Needle Procedures, IEEE Transactions on Biomedical Engineer-
ing 60 (11) (2013) 3009–3018. doi:10.1109/TBME.2012.2236091.

[8] J. Huang, S. Ong, A. Nee, Visualization and interaction of ﬁnite element
analysis in augmented reality, Computer-Aided Design 84 (2017) 1–14.
doi:10.1016/j.cad.2016.10.004.

[9] J. Abderezaei, A. Pionteck, I. Terem, L. Dang, M. Scadeng, P. Morgen-
stern, R. Shrivastava, S. J. Holdsworth, Y. Yang, M. Kurt, Development,
calibration, and testing of 3D ampliﬁed MRI (aMRI) for the quantiﬁca-
tion of intrinsic brain motion, Brain Multiphysics 2 (September 2020)
(2021) 100022. doi:10.1016/j.brain.2021.100022.

[10] K. M. Moerman, C. A. Holt, S. L. Evans, C. K. Simms, Digital im-
age correlation and ﬁnite element modelling as a method to determine
mechanical properties of human soft tissue in vivo, Journal of Biome-
chanics 42 (8) (2009) 1150–1153. doi:https://doi.org/10.1016/j.
jbiomech.2009.02.016.

[11] W. Schroeder, K. Martin, B. Lorensen, The visualization toolkit, 4th

edn. kitware, New York (2006).

[12] M. Mathur, W. D. Meador, M. Malinowski, T. Jazwiec, T. A. Timek,
M. K. Rausch, Texas TriValve 1.0 : a reverse-engineered, open model
of the human tricuspid valve, Engineering with Computers (may 2022).
doi:10.1007/s00366-022-01659-w.

[13] W. D. Meador, M. Mathur, S. Kakaletsis, C.-Y. Lin, M. R. Bersi, M. K.
Rausch, Biomechanical phenotyping of minuscule soft tissues: An exam-
ple in the rodent tricuspid valve, Extreme Mechanics Letters 55 (2022)
101799. doi:10.1016/j.eml.2022.101799.

[14] R. Shad, A. D. Kaiser, S. Kong, R. Fong, N. Quach, C. Bowles, P. Kas-
inpila, Y. Shudo, J. Teuteberg, Y. J. Woo, A. L. Marsden, W. Hiesinger,

12

Patient-Speciﬁc Computational Fluid Dynamics Reveal Localized Flow
Patterns Predictive of Post–Left Ventricular Assist Device Aortic In-
competence, Circulation: Heart Failure 14 (7) (2021) E008034. doi:
10.1161/CIRCHEARTFAILURE.120.008034.

[15] M. R. Pfaller, J. Pham, N. M. Wilson, D. W. Parker, A. L. Marsden, On
the Periodicity of Cardiovascular Fluid Dynamics Simulations, Annals of
Biomedical Engineering 49 (12) (2021) 3574–3592. arXiv:2102.00107,
doi:10.1007/s10439-021-02796-x.

[16] K. Menon, R. Mittal, Flow physics and dynamics of ﬂow-induced pitch
oscillations of an airfoil, Journal of Fluid Mechanics 877 (McCroskey
1982) (2019) 582–613. doi:10.1017/jfm.2019.627.

[17] T. Berger, M. Kaliske, An arbitrary lagrangian eulerian formulation
for tire production simulation, Finite Elements in Analysis and Design
204 (2022) 103742. doi:https://doi.org/10.1016/j.finel.2022.
103742.

[18] B. Dortdivanlioglu, A. Krischok, L. Beirão da Veiga, C. Linder, Mixed
isogeometric analysis of strongly coupled diﬀusion in porous materials,
International Journal for Numerical Methods in Engineering 114 (1)
(2018) 28–46. doi:10.1002/nme.5731.

[19] K. M. Shepherd, X. D. Gu, T. J. Hughes, Isogeometric model recon-
struction of open shells via ricci ﬂow and quadrilateral layout-inducing
energies, Engineering Structures 252 (2022) 113602. doi:https://doi.
org/10.1016/j.engstruct.2021.113602.

[20] A. Chemin, T. Elguedj, A. Gravouil, Isogeometric local h-reﬁnement
strategy based on multigrids, Finite Elements in Analysis and Design 100
(2015) 77–90. doi:https://doi.org/10.1016/j.finel.2015.02.007.

[21] N. Bhatia, E. A. Müller, O. Matar, A GPU Accelerated Lennard-Jones
System for Immersive Molecular Dynamics Simulations in Virtual Real-
ity, in: Lecture Notes in Computer Science (including subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), Vol.
12191 LNCS, 2020, pp. 19–34. doi:10.1007/978-3-030-49698-2_2.

[22] R. Revetria, F. Tonelli, L. Damiani, M. Demartini, F. Bisio, N. Pe-
ruzzo, A Real-Time Mechanical Structures Monitoring System Based

13

On Digital Twin, Iot and Augmented Reality, in: 2019 Spring Simu-
lation Conference (SpringSim), Vol. 51, IEEE, 2019, pp. 1–10. doi:
10.23919/SpringSim.2019.8732917.

[23] E. Febrianto, L. Butler, M. Girolami, F. Cirak, Digital twinning of self-
sensing structures using the statistical ﬁnite element method (2021) 1–
23arXiv:2103.13729.

[24] C. M. Portela, J. R. Greer, D. M. Kochmann, Impact of node geometry
on the eﬀective stiﬀness of non-slender three-dimensional truss lattice
architectures, Extreme Mechanics Letters 22 (2018) 138–148. doi:10.
1016/j.eml.2018.06.004.

14

