2
2
0
2

l
u
J

2
1

]
E
S
.
s
c
[

2
v
9
8
6
3
0
.
7
0
2
2
:
v
i
X
r
a

Guiding the retraining of convolutional neural networks against
adversarial inputs

Francisco DurÃ¡n
francisco.javier.duran.lopez@upc.edu
Universitat PolitÃ¨cnica de Catalunya - BarcelonaTech
Barcelona, Catalunya, Spain
Universidad Nacional AutÃ³noma de MÃ©xico
Mexico City, Mexico

Silverio MartÃ­nez-FernÃ¡ndez
silverio.martinez@upc.edu
Universitat PolitÃ¨cnica de Catalunya - BarcelonaTech
Barcelona, Catalunya, Spain

Michael Felderer
michael.felderer@uibk.ac.at
University of Innsbruck
Innsbruck, Tyrol, Austria

Xavier Franch
xavier.franch@upc.edu
Universitat PolitÃ¨cnica de Catalunya - BarcelonaTech
Barcelona, Catalunya, Spain

Abstract
Background: When using deep learning models, there are many
possible vulnerabilities and some of the most worrying are the
adversarial inputs, which can cause wrong decisions (e.g., wrongly
classifying an image) with minor perturbations. Therefore, it be-
comes necessary to retrain these models against adversarial inputs,
as part of the software testing process addressing the vulnerability
to adversarial inputs. Furthermore, for an energy efficient testing
and retraining process, data scientists need support on which are
the best guidance metrics for reducing the adversarial inputs to use
as well as optimal dataset configurations.

Aims: We examined four guidance metrics for retraining deep
learning models, specifically with convolutional neural network
architecture, and three retraining configurations. Our goal is to im-
prove the convolutional neural networks against adversarial inputs
with regard to accuracy, resource utilization and time from the point
of view of a data scientist in the context of image classification.

Method: We conduced an empirical study in two datasets for im-
age classification. We explore: (a) the accuracy, resource utilization
and time of retraining convolutional neural networks by ordering
new training set by four different guidance metrics (neuron cover-
age, likelihood-based surprise adequacy, distance-based surprise
adequacy and random), (b) the accuracy and resource utilization
of retraining convolutional neural networks with three different
configurations (from scratch and augmented dataset, using weights
and augmented dataset, and using weights and only adversarial
inputs).

Results: We reveal that retraining with adversarial inputs from
original model weights and by ordering with surprise adequacy
metrics gives the best model w.r.t. accuracy, resource utilization
and time.

Conclusions: Although more studies are necessary, we recom-
mend data scientists to use the above configuration and metrics to
deal with the vulnerability to adversarial inputs of deep learning
models, as they can improve their models against adversarial inputs
without using many inputs. We also show that dataset size has an
important impact on the results.

CCS Concepts
â€¢ Computing methodologies â†’ Machine learning; Artificial in-
telligence; â€¢ Software and its engineering â†’ Software creation
and management.

Keywords
Neural networks, software testing, deep learning, adversarial inputs,
green AI

1 Introduction
In recent years, Deep Learning (DL) systems, defined as those soft-
ware systems with functionalities enabled by at least one DL model,
have become a widespread machine learning approach, due to their
outstanding capacity to solve complex problems. The use of DL
systems ranges from applications in autonomous driving systems
to applications in medical treatments, among others [6, 36, 39].
However, given their very nature, it is often the case that DL sys-
tems exhibit an unexpected behavior or produce anomalous results
when inferring with inputs differents to those used in the training
stage, due to their statistical and black box nature. These vulnerabil-
ities or system errors can lead to undesirable consequences when
integrated in real-world applications already in production.

The existence of vulnerabilities is commonplace in any type of
software system. In traditional systems, i.e., not embedding any DL
system, we may find a vast amount of testing approaches to ensure
a certain level of robustness. When the tests detect a problem, there
are specific solutions to fix the problem or at least mitigate the
effects to a large extent [15]. In contrast, testing DL systems is
completely different, due to their non-deterministic nature: the
â€œcorrectâ€ answer or â€œexpectedâ€ behaviour in response to a certain
input is often unknown at the time of training the DL model [2, 4].
Recently, one of the most worrying vulnerabilities in DL systems
are adversarial inputs, which are inputs created with slight modi-
fications to the original inputs of a dataset. These types of inputs
are often not distinguishable by the human from the input from
which they are generated, but they lead the DL system to produce
an incorrect output.

 
 
 
 
 
 
Given this scenario, DL system testing has become a relatively
new broad research field, in which every year the number of publi-
cations related to the subject increases [13, 26, 43], aiming at laying
the foundations of the tests to be able to compare research results
in a consistent and effective way. Different frameworks have been
created to facilitate the use of these methods [1, 9, 29, 32, 44] and
likewise a number of metrics have been created to compare the
properties of these testing methods [16, 23, 30].

This work focuses on improving the testing and retraining of
DL models in the presence of adversarial inputs. To this end, we
apply two different approaches: (i) using the information provided
by guidance metrics, which try to identify the most useful inputs
according to the criteria of each metric, and (ii) adding adversarial
inputs to augment the dataset employing different training inputs
from the original training set. Still, questions arise: When and how
to use a certain metric? What is the accuracy, resource utilization
and time of using such metrics? When and how to use a certain
retraining configuration? What is the accuracy and resource utiliza-
tion of using a certain retraining configuration? In this context, this
work aims at comparing both guidance metrics and config-
urations for retraining DL models with adversarial inputs
based on their accuracy, resource utilization and time. The
results of the research benefit data scientists to test their modelsâ€™
accuracy against adversarial inputs while keeping the trade-off
with the resource utilization of the retraining process. We evaluate
our method in one typical DL field application, namely image clas-
sification. For this reason, we focus on one particular type of DL
model, namely Convolutional Neural Networks (CNN), which are
widely used for image classification due to their high performance.
Since LeCun et al. introduced them [20], there has been significant
progress in the literature with respect to their accuracy even in
challenging datasets, as well as the explanation of their behaviour
and abstraction [19, 42].

The main contributions of this work are:

â€¢ A comparison, in terms of accuracy, resource utilization and
time, of four guidance metrics, including random selection,
and three configurations of retraining CNN with adversarial
inputs.

â€¢ A replication package, available online. 1

This document is structured as follows. Section 2 respectively
introduces the DL arquitecture used for the models, the guidance
metrics (Neuron Coverage (NC) and two Surprise Adequacy (SA)
metrics), and the adversarial attack Fast Gradient Sign Method
(FGSM) used to create the adversarial inputs. Section 3 describes
related work. Section 4 presents the research questions and the
study design. Section 5 presents the results. Section 6 presents the
discussions. Section 7 describes the threats to validity, and Section
8 draws conclusions and future work.

2 Background
In this section, we introduce: (i) the type of DL model we used,
namely Convolutional Neural Networks (CNN); (ii) the guidance
metrics used in our study, namely: Neuron Coverage (NC) and
two Surprise Adequacy (SA) metrics: Likelihood-based Surprise
Adequacy (LSA) and Distance-based Surprise Adequacy (DSA); (iii)
1Please refer to https://doi.org/10.5281/zenodo.5904550

DurÃ¡n et al.

the adversarial attack used to create adversarial inputs, namely Fast
Gradient Sign Method (FGSM).

2.1 Convolutional Neural Networks (CNN)
This is one of the most used architectures in image classification. A
CNN consists of an input layer, output layer and multiple hidden
layers. These hidden layers typically consist of convolutional layers,
pooling layers, normalization layers, fully connected layers, or other
type of layers to build more complex models. Each of the layers
contains a different level of abstraction for an image dataset and
the weights of the final model are obtained by backpropagation
[11, 20, 21].

2.2 Guidance metrics
Current approaches to DL systems testing evaluate them according
to a number of properties, either functional (such as accuracy or
precision) or non-functional (such as interpretability, robustness
or efficiency) [43]. In order to improve the DL system behaviour
with respect to these properties, it is important to have metrics to
compare the behaviors of the models; among the most used and
accepted metrics by the community are those related to neuron
coverage and to the surprise of new inputs regarding the model
[16, 25, 30, 40].

2.2.1 Neuron Coverage (NC) Pei et al. proposed the concept of NC
in 2017 [30] to measure the coverage of test data of a DL model and
to improve the generation of new inputs, arguing that the more
neurons are covered, the more network states can be explored,
having a greater possibility of defect detection [23]. The metric is
defined as follows. Let ğ· be a trained DL model, composed of a set
ğ‘ of neurons. The neuron coverage of input ğ‘¥ with respect to ğ· is
given by

ğ‘ğ¶ (ğ‘¥) =

| ğ‘› ğœ– ğ‘ | ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ (ğ‘›, ğ‘¥)|
|ğ‘ |

(1)

where ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ (ğ‘›, ğ‘¥) is true if and only if ğ‘› is activated when passing
ğ‘¥ to ğ·.

Surprise Adequacy (SA) Kim et al. [16] proposed SA as the
2.2.2
metric that measures the degree of surprise of the model when
it confronts new inputs with respect to training inputs, with the
hypothesis that a good test set should be â€œenoughâ€ but not too
surprising compared to the training set. Kim et al. presented two
surprise metrics, which we use in our study.

2.2.2.1 Likelihood-based Surprise Adequacy (LSA). Let ğ· be a
DL model trained on a set ğ‘‡ of inputs. The LSA of the input ğ‘¥ with
respect to ğ· is given by:

ğ¿ğ‘†ğ´(ğ‘¥) =

1
|ğ´ğ‘ğ¿ (ğ‘‡ğ· (ğ‘¥) )|

âˆ‘ï¸

ğ‘¥ğ‘¡ ğœ– ğ‘‡ğ· (ğ‘¥ )

ğ¾ğ» (ğ›¼ğ‘ğ¿ (ğ‘¥) âˆ’ ğ›¼ğ‘ğ¿ (ğ‘¥ğ‘– ))

(2)

where ğ›¼ğ‘ğ¿ (ğ‘¥) is the vector that stores the activation values of
neurons in the ğ¿ layer of ğ· when ğ‘¥ is entered, ğ‘‡ğ· (ğ‘¥) is the subset of
ğ‘‡ composed of all the inputs of the same class as ğ‘¥, ğ´ğ‘ğ¿ (ğ‘‡ğ· (ğ‘¥) ) =
ğ›¼ğ‘ğ¿ (ğ‘¥ğ‘– ) | ğ‘¥ğ‘– ğœ– ğ‘‡ğ· (ğ‘¥) and ğ¾ğ» is the Gaussian kernel function with
bandwidth matrix ğ» [25].

Guiding the retraining of convolutional neural networks against adversarial inputs

Table 1: Review of the retraining configurations used in related work (Cğ‘˜ refer to the configurations presented in Section 4)

Study

Configuration

Independent variables

Studied variables

Datasets

Pein et al.[30] Retraining of an original model
with 100 new error-inducing sam-
ples (Similar to C3).

DL model

Accuracy

Tian et al.[38] Retraining using synthetic images
and original training set. Does not
specify if it is done from scratch
(Similar to C1 and C2).

Kim et al.[16] Retraining with 100 new inputs
from different ranges of SA metrics
(Similar to C3).

Ma et al.[24]

Ma et al.[25] As C3, however on each iteration
they compute the metrics, instead
of calculating metrics once.
As C3, but when they add a new se-
lected batch, it starts from previous
retrained model; not from original
model weights.
C1, C2 and C3 (see Section 4).

This study

Image transformation

Layer of neurons used for
SA computation
Adversarial attack
Testing metric

Accuracy
MSE

Accuracy
MSE

Accuracy
Validation Loss

DL model

Accuracy

Guidance metric
Retraining configuration

Accuracy
Resource utilization
Time

GTSRB
Intel

MNIST
ImageNet
Udacity challenge
Contagio/VirusTotal
Drebin
Udacity challenge

MNIST
CIFAR-10
Udacity challenge
MNIST
Fashion-MNIST
CIFAR-10
MNIST
Fashion-MNIST
CIFAR-10

2.2.2.2 The Distance-based Surprise Adequacy (DSA). Based on
the distance between vectors representing the neuronal Activation
Traces of the given input and the training data (using Euclidean
distance). Let ğ· be a DL model trained on a set ğ‘‡ of inputs. The
DSA of the input ğ‘¥ with respect to ğ· is given by:

ğ·ğ‘†ğ´ (ğ‘¥) =

||ğ›¼ğ‘ (ğ‘¥) âˆ’ ğ›¼ğ‘ (ğ‘¥ğ‘)||
||ğ›¼ğ‘ (ğ‘¥ğ‘) âˆ’ ğ›¼ğ‘ (ğ‘¥ğ‘ )||

where:

ğ‘¥ğ‘ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ‘¥ğ‘– ğœ– ğ‘‡ğ· (ğ‘¥ )
ğ‘¥ğ‘ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›ğ‘¥ ğ‘— ğœ– ğ‘‡ \ğ‘‡ ğ· (ğ‘¥ )

||ğ›¼ğ‘ (ğ‘¥) âˆ’ ğ›¼ğ‘ (ğ‘¥ğ‘– )||

||ğ›¼ğ‘ (ğ‘¥ğ‘) âˆ’ ğ›¼ğ‘ (ğ‘¥ğ‘– )||

(3)

(4)

(5)

and where ğ· (ğ‘¥) is the predicted class of ğ‘¥ by ğ· and ğ›¼ğ‘ (ğ‘¥) is the
vector of activation values of all neurons of ğ· when confronted
with ğ‘¥ [25].

2.3 Adversarial attack: Fast Gradient Sign

Method (FGSM)

There are DL models whose dataset is increased when there is
not enough data. This can be done through techniques such as
obtaining new entries from those that already exist, carrying out
transformations to these including cuts and rotations, among oth-
ers [8, 14, 38]. Another way is using adversary examples, which C.
Szegedy discovered in 2013, when he noticed that several machine
learning and DL models are vulnerable to slightly different inputs
from those that are correctly classified, i.e. the adversarial inputs
[37]. This observation caused concerns because it goes against the
ability of these models to achieve a good generalization. In 2015,

Goodfellow introduced the FGSM method to be able to create adver-
sarial inputs in a relatively simple way, forcing the misclassification
of the input controlling the disturbance so that it is not perceptible
to a human, computed as follows [10]:

ğ‘¥âˆ— â† ğ‘¥ + ğœ€ â€¢ âˆ‡ğ‘¥ ğ½ (ğ‘“ , ğœƒ, ğ‘¥)

(6)

where ğ½ is the cost function used for the training of the model ğ‘“ in
the neighborhood of the training point ğ‘¥ for which the adversary
wants to force a wrong classification. The adversary example cor-
responding to the input ğ‘¥ that results from the method is denoted
as ğ‘¥âˆ—.

3 Related work
From the reviewed literature, there are many studies focused on
the creation of adversarial inputs in order to uncover debilities
and vulnerabilities of DL systems or models. Goodfellow et al. [10]
were the first to formulate a concrete definition of the adversarial
inputs and clarify their generalization in different architectures and
training sets. Additionally, they defined FGSM which allowed to
generate adversarial inputs. Other studies centered on being able
to detect such adversarial inputs for different purposes. Feinman
et al. [8] aimed at distinguishing adversarial samples from their
normal and noisy counterparts. Ma et al. [23] used their coverage
criteria to quantify defect detection ability using adversarial inputs
created with different adversarial techniques. Kim et al. [16] used
their proposed SA criteria to show that they could detect adversarial
inputs.

Nevertheless, there is scarce research focused on using adversar-
ial inputs in retraining with the purpose of improving the models

Table 2: The variables of the study

Description

Values or Formula

Scale

DurÃ¡n et al.

Testing metrics used to order the inputs for retraining NC, DSA, LSA, Random Nominal
The configuration used for retraining the models
against adversarial inputs
Accuracy using the new test set composed of the
original test set and an adversarial test set
obtained from the previous applying the FGSM
to each one of the original test inputs

C1, C2, C3

Nominal

Ratio

ğ‘‡ ğ‘ƒ +ğ‘‡ ğ‘
ğ‘‡ ğ‘ƒ +ğ‘‡ ğ‘ +ğ¹ ğ‘ƒ +ğ¹ ğ‘

Class

Independent

Name
Guidance metric

Retraining configuration

Accuracy

Dependent

ğ‘‡ ğ‘ƒ = True Positives; ğ‘‡ ğ‘ = True Negatives
ğ¹ ğ‘ƒ = False Positives; ğ¹ ğ‘ = False Negatives
ğ‘¢ = Input size used to obtain the maximum accuracy

Resource utilization

Other

Time
Dataset

ğ‘‡ = Number of total inputs
Time to obtain the metrics values
The datasets used for training the DL models

ğ‘¢
ğ‘‡

hh:mm:ss
GTSRB, Intel

Ratio

Interval
Nominal

[16, 30], see Table 1 for a summary. In these few works, we can
only identify one configuration used, similar to the configuration 3
presented in Section 4 of this work, in which the authors retrain
with only a few of adversarial inputs and do not clearly show the
steps for the retraining. Kim et al. [16] showed that sampling inputs
using SA for retraining can result in higher accuracy, Pei et al. [30]
showed that error-inducing inputs generated by DeepXplore can
be used for retraining to improve accuracy.

Most recently, Ma et al. [25] proposed to use testing metrics as a
retraining guide, looking for an answer on how to select additional
training inputs to improve the accuracy of the model. They used
the original training set starting from a previous trained model but
ordering these inputs following the metricsâ€™ guidance. Compared
to this work, we aim to do the retraining using the information of
an augmented dataset with adversarial inputs, not only the original
training set.

As the use of adversarial inputs has been proved to pro-
vide many improvements, we identified a research gap there
to take advantage of them during a retraining process. More-
over, we want to understand how to order an augmented
dataset with adversarial inputs guided by metrics so that
the retraining is efficient (with highest accuracy and using
fewest inputs) for data scientists. To the best of our knowledge,
our study is the first one that applies metrics to guide a retraining
using an augmented dataset with adversarial inputs in order to im-
prove the model accuracy against adversarial inputs and keeping the
trade-off with resource utilization, in order to obtain computational
benefits, which is key as the retraining phase is time consuming.

In our study, we consider an augmented dataset with adversar-
ial inputs obtained from the original training set using the FGSM
method. Therefore our study provides the following novel contri-
butions:

â€¢ A comparison between state-of-the-art guidance metrics for

a guided retraining.

â€¢ A comparison of three different configurations for doing a

retraining against adversarial inputs.

4 Empirical study

4.1 Research questions
The goal [5] of this research is to analyze guidance metrics and
retraining configurations for a retraining process of CNN models with
the purpose of comparing them with respect to DL testing properties
such as accuracy, resource utilization and time from the point of
view of a data scientist in the context of image classification.

Thus, we aim at answering the following research questions

(RQ):

â€¢ RQ1 - Does the use of guidance metrics impact the accu-
racy, the resource utilization and the time required for the
retraining of a CNN model?

â€¢ RQ2 - Does the configuration of the retraining of a CNN
model impact the accuracy and the resource utilization re-
quired for retraining this model?

4.2 Variables
Table 2 describes the variables of our study. With respect to the
dependent variables, some details follow:

â€¢ Accuracy, which measures the capability of the retraining
phase of providing higher accuracy against adversarial in-
puts. We plan to measure the accuracy of the CNN models
against an augmented test set, composed of the original test
set and an adversarial test set obtained from the previous
applying the FGSM to each one of the original test input.
â€¢ Resource utilization, which measures the input size to obtain

the highest accuracy during the retraining phase.

â€¢ Time, which quantifies the time to compute each of the

considered metrics for the corresponding dataset.

Another variable that influences our conclusions is the dataset,
we experiment with two different datasets. Hence, we define the
dataset as a nominal variable indicating the used dataset for training
and retraining the models. They are from different domains and
have different sizes.

Guiding the retraining of convolutional neural networks against adversarial inputs

Figure 1: Schema of our empirical study

Figure 2: Schema of retraining process

Train  setTestsetCNN model(W0) Traditional trainingRetraining processRetrained CNNmodel Image ClassificationGTSRBIntel DL Testing propertiesAccuracyResourceutilizationTime (i) Data collection and  preprocessing(ii) Model training(iii) Research outputTraditional trainingBest retraining guidance Guidance metric RetrainingconfigurationTrain  subsetTestsetFGSMAdvTrainAdvTest++==Test*Train*Ordering by: DSALSANCRandom RetrainingC1From scratchC2Train*(Ordered)CNN model(W0) C3CNN model(W0) Adv Train(Ordered)Step 1Step 2Step 3Step 4AdvTrainAdvTestTrainTestObtaining MetricsDSALSANCRandom Train*(Ordered)Step 34.3 Study design
Figure 1 shows the study design. First, the Data collection and pre-
processing which consists of the collection and preprocessing of raw
data. Second, the Model training which consists of the traditional
training and proposed retraining phase. Finally, we provide answers
to the RQs by analyzing our results w.r.t. DL testing properties.

4.3.1 Data collection and preprocessing: datasets We evaluate the
guidance metrics on CNNs using two multi-class, single-image
classification datasets. First, the German Traffic Sign Recognition
Benchmark (GTSRB), with 43 classes containing 39.208 unique
images of real traffic signs in Germany. It has been widely used
in DL research [6, 22, 36]. This type of images are characterized
by large changes of visual appearance for different causes (e.g.,
weather conditions). While humans can easily classify them, for
DL systems (e.g., autonomous driving systems) still is a challenge.
Second, the Intel Image Classification dataset with 6 classes
containing 17.034 labeled images of natural scenes around the world,
used in many studies as well [31, 33, 41]. It was provided by Intel
corporation to create another benchmark in image classification
tasks such as scene recognition [3]. This scene recognition task is
a daily task for humans and widely used application in industries
such as tourism or robotics, still an emerging field for computer
vision [27, 35].

The adversarial inputs are obtained using the FGSM method from
the foolbox library [32]. In the retraining the adversarial inputs
are from the original training set (see â€œAdv. Trainâ€ in Fig. 2 Step 1).
Another adversarial set is obtained from the original test set (see
â€œAdv. Testâ€ in Fig. 2 Step 1).

Using only one adversarial attack to create the adversarial inputs
can be a limitation of our work. Nevertheless, when Goodfellow
et al. [10] presented FGSM, he stated that this type of adversarial
examples can be generalized across architectures and training sets.
Being aware of other attack methods, we chose this method because
it is one of the most practical methods, and it is more widely used
than other attack methods such as Basic Iterative Method (BIM),
Jacobian-based Saliency Map Attack (JSMA), Projected Gradient
Descent (PGD) or Carlini-Wagner (CW), compared to CW in par-
ticular, FGSM is more time efficient. On top of that, we focus our
study in the retraining phase rather than creation of adversarial
inputs.

4.3.2 Model training The traditional training of a model consists
of using an available training set identified as â€œTrainâ€ in Figure 2
and then to evaluate the model against a test set identified by â€œTestâ€
in Figure 2. The traditional training is represented in the upper part
of Fig. 1 (ii) Model training phase. We plan to add in this phase a
retraining process as it is represented in the lower part of Fig. 1
(ii) Model training phase. This retraining uses adversarial inputs
guided by the metrics and three different configurations to improve
the original model, M, against adversarial inputs. After obtaining a
retrained model, M*, we evaluate the DL testing properties of this
retrained model. The retraining process is shown more extensively
in Figure 2. It comprises the following steps:

(1) Step 1. Create adversarial inputs for training and test-
ing: We obtain two adversarial sets applying FGSM. First,
to augment the training set, we apply FGSM to a subset of

DurÃ¡n et al.

the original training set, â€œTrainâ€, to obtain the â€œAdv. Trainâ€
set, the number of the adversarial inputs created is from a
small proportion of the entire â€œTrainâ€ set, thus we do not
increase much the artificial inputs used in retraining and
the naturalness is not diminished either. Putting these two
sets together, we obtain â€œTrain*â€ set. Second, to augment the
original test set, â€œTestâ€, we apply FGSM to the entire set to
obtain the â€œAdv. Testâ€ set as it is too small compared to the
augmented training set, â€œTrain*â€. Putting these â€œTestâ€ and
â€œAdv. Testâ€ sets together, we obtain â€œTest*â€ set.

(2) Step 2. Obtain guidance metrics: Based on the original
trained model, M, the â€œTrainâ€ set and the new adversarial
inputs for training â€œAdv. Trainâ€, we compute the different
metrics for the augmented training set, â€œTrain*â€.

(3) Step 3. Order inputs w.r.t. the guidance metrics: Accord-
ing to each metric, we order the inputs for the retraining.
With this, we expect that the retrained model, M*, will be
trained first with the images that are more difficult to classify
and more informative according to the metricsâ€™ value.
(4) Step 4. Retraining according to the configuration: We
implement the retraining in three different ways using or-
dered inputs according to the following configurations (see
Figure 2):
(a) C1: Starting from scratch using the new adversarial
inputs and original training set. The new training set
with the label â€œTrain*â€ in Fig. 2 from Step 1 is composed
of the â€œTrainâ€ and â€œAdv. Trainâ€ sets. The model M* is
retrained with this â€œTrain*â€ set ordered by highest score
of LSA, DSA, NC and Random, respectively, starting from
scratch.

(b) C2: Starting from the original model M using the
new adversarial inputs and original training set. The
new training set is the same â€œTrain*â€ set as in C1 with the
difference that the model M* is retrained from the original
model M weights.

(c) C3: Starting from the original model M using only
the new adversarial inputs. The new training set is only
the â€œAdv. Trainâ€ set. The model M* is retrained with this
set, also ordered by highest score of LSA, DSA, NC and
Random, respectively, starting from the original model M
weights.

For each configuration, in the retraining phase, we execute a
retraining of the models guided by the four metrics and for each
metric we obtain 20 data points as shown in Figures 3a - 3f. Each
retraining run for each data point is computed from their respective
initial weights. Also, we address randomness of resulted values by
executing the retraining randomly, and not through incremental
training.

4.3.3 Data analysis Figures 3a - 3f correspond to the accuracy of
the models against the augmented test set, â€œTest*â€. Table 3 shows
the accuracy against the same â€œTest*â€ set and resource utilization
of the experiments by dataset, configuration and metric used in
each case. The â€œOriginal accuracyâ€ column shows the accuracy of
the original model M against this new augmented dataset, which is
low due to the adversarial inputs, the â€œAccuracy w.r.t. augmented
test setâ€ column shows the highest accuracy during its respective

Guiding the retraining of convolutional neural networks against adversarial inputs

retraining and the â€œResource utilizationâ€ column shows the input
size to obtain that highest accuracy. Table 5 shows the time to com-
pute the considered metrics for every input of the corresponding
dataset.

In order to compare the impact that guidance metrics and retrain-
ing configurations have on the accuracy and resource utilization
of the retrained models, we select the best model, according to the
accuracy, during the retraining of the 20 data points for each combi-
nation of configuration and guidance metric (see marked points on
Figures 3a - 3f in Section 5). We obtain the accuracy and resource
utilization of these points and report these values in Tables 3 - 4 to
determine if they can be improved by guiding the retraining with
the studied metrics and using the studied retraining configurations.
In addition to that, we evaluate the time it takes to compute the
guidance metrics in Table 5.

We answer the RQs from two different angles. First, we compare
the accuracy changes against the test set augmented with adversar-
ial inputs in the 20 data points of the retrained models according to
each configuration and guidance metric. The test set is composed
of the original test set and adversarial inputs created with the same
attack method but using inputs of the original test set; this way we
ensure that the test data is never used during training or retraining.
Second, we compare the input size required to obtain the model
with highest accuracy between those data points according to each
configuration and guidance metric.

5 Results
In this section, we report the results of our empirical study, answer-
ing RQ1 and RQ2, and highlighting key takeaways.

To better describe the results, we use the Figures 3a - 3f and

Tables 3, 4 and 5, as explained in Section 4.3.3.

5.1 Does the use of guidance metrics impact
the accuracy, the resource utilization and
the time required for the retraining of a
CNN model? (RQ1)

Guidance metrics and accuracy. According to the Figures 3a - 3f, we
found in the experiments that SA metrics have the best selection of
inputs for the retraining as stated by the observed accuracy. Five
out of six guided retraining runs obtained the best accuracy using
SA metrics (highlighted with bold font in Table 3).

Guidance metrics and resource utilization. According to the results
of using C2 with the GTSRB dataset, the impact of the method can
be noticed with greater difference. Clearly, SA metrics can reach
a better model with less inputs, as much as 14.400/36.366 inputs
in the best case with 0.953 of accuracy, as shown in Figure 3c and
Table 3. On the other hand, this is not so clear when using Intel
dataset, which may be due to the size of the dataset: as there are
not enough inputs, the metrics cannot make a difference with just
a percentage of an already relatively small dataset. Nevertheless,
DSA was the best option according to Figure 3d.

Overall, SA metrics may be the best due to the fact that they
first identify the inputs that are most different from the previous
training inputs, so the model learns features that are different from
those previously learned, in order to correctly classify adversarial
inputs. On the other hand, for NC metric it may be more difficult

to identify good inputs for retraining because this metric identifies
inputs that cover more neurons according to certain thresholds,
which in retraining may not be significant.

Guidance metrics and time. We obtained the time to calculate
the metrics as stated in Table 5. To obtain NC values it takes more
than 7x in terms of time w.r.t. the time needed to obtain SA values,
which may be due to the library used and to a non-optimized metric
computation. In addition, the lower time required to get SA values
may be also due to the size of the datasets, as it is known that the
computation of SA metrics tends to quickly augment [40]. We can
observe this trend when comparing the ratio of NC time to DSA
time in each dataset. When we consider Intel dataset, computing
NC values takes 34x the DSA time, but when using GTSRB dataset,
this ratio reduces to almost 8x.

Key takeaways for RQ1:

â€¢ SA metrics have the best selection of inputs for the
retraining as stated by the observed accuracy.

â€¢ SA metrics can reach a better model with less inputs.
â€¢ To obtain NC values it takes more than 7x in terms
of time w.r.t. the time needed to obtain SA metrics.

5.2 Does the configuration of the retraining of
a CNN model impact the accuracy and the
resource utilization required for retraining
this model? (RQ2)

Configuration of the retraining and accuracy. According to the total
number of used inputs, as expected, C1 and C2 reached higher
values of accuracy than C3. Nevertheless, between these two con-
figurations, the benefits of using C2 are greater because with less
inputs, nearly less than a half in the best case (see Table 3), it is
possible to complete the retraining with high accuracy. Due to the
initial weights used in C2, this incremental training creates bias
towards the original training set, which can be the reason why C2
does not worsen its accuracy against inputs from the original test
set: the help of the adversarial inputs â€œAdv. Trainâ€ that augmented
the dataset makes this model better against the adversarial inputs
from the augmented test set â€œTest*â€.

Configuration of the retraining and resource utilization. According
to the resource utilization, C2 is the best option of the studied
configurations, nevertheless, using C3 shows some benefits. Table 4
shows the same values of Table 3 for C3, but for C2 the accuracy and
resource utilization are obtained in an specific data point, when the
model has only used the same input size for C3 (e.g., 5000 inputs
when using GTSRB and 3000 inputs when using Intel dataset).
Although C3 was expected to have lower accuracy than the other
two configurations, because of the available input size for retraining,
Table 4 shows that C3 can be a good option if we want to execute
a retraining with just few inputs. The values in the rows with
accuracy of C3 for both datasets are greater in their respective
values for C2.

We can compare C3 with C2 because the starting point is the
same original model M and we are executing a retraining with the
same number of inputs, but for C2 the inputs are chosen from the

DurÃ¡n et al.

(a) GTSRB dataset, C1.

(b) Intel dataset, C1.

(c) GTSRB dataset, C2.

(d) Intel dataset, C2.

(e) GTSRB dataset, C3.

(f) Intel dataset, C3.

Figure 3: Accuracy of the trained models.

Guiding the retraining of convolutional neural networks against adversarial inputs

Table 3: Accuracy against augmented test set (original test set and adversarial test set) and resource utilization when the
model reach the maximum accuracy for C1, C2 and C3 applied to each dataset

Dataset Config. 1 Original accuracy
GTSRB
Intel
GTSRB
Intel
GTSRB
Intel

0.589
C1
0.477
C1
0.589
C2
0.477
C2
0.589
C3
C3
0.477
1 Configuration of retraining.
Note: bold numbers indicate the accuracy and resource utilization for the model with highest accuracy during retraining.

Accuracy w.r.t. augmented test set
LSA
0.967
0.715
0.950
0.669
0.890
0.626

DSA
36366/36366
13300/14224
14400/36366
11900/14224
5000/5000
2400/3000

LSA
36366/36366
13300/14224
10800/36366
11200/14224
4000/5000
2400/3000

DSA NC
0.965
0.710
0.953
0.695
0.890
0.648

Random
0.966
0.710
0.936
0.686
0.898
0.630

0.963
0.705
0.927
0.676
0.889
0.605

NC
36366/36366
12600/14224
36366/36366
11200/14224
5000/5000
2850/3000

Resource utilization

Random
27000/36366
14224/14224
36366/36366
13300/14224
3500/5000
1500/3000

Table 4: Accuracy against augmented test set (original test set and adversarial test set) and resource utilization when the
model reach the maximum accuracy for C2 (data point with same number of inputs used in C3) and C3 applied to each
dataset

Dataset Config. Original accuracy
GTSRB
Intel
GTSRB
Intel

0.589
0.477
0.589
0.477

C2
C2
C3
C3

Accuracy w.r.t. augmented test set
LSA
0.860
0.590
0.890
0.626

DSA NC
0.860
0.595
0.890
0.648

Random
0.820
0.590
0.898
0.630

0.400
0.560
0.889
0.605

Resource utilization

LSA
5000/36366
3000/14224
4000/5000
2400/3000

DSA
5000/36366
3000/14224
5000/5000
2400/3000

NC
5000/36366
3000/14224
5000/5000
2850/3000

Random
5000/36366
3000/14224
3500/5000
1500/3000

Table 5: Time in hours (hh:mm:ss) to obtain the respective
values

Dataset
GTSRB
Intel

LSA values DSA values NC values Random
00:01:35
00:00:57

00:00:00
00:00:00

03:17:38
01:08:18

00:24:54
00:02:04

entire training set and adversarial set (â€œTrain*â€), while for C3 the
inputs are only from the adversarial set (â€œAdv Trainâ€).

Key takeaways for RQ2:

â€¢ The benefits of using C2 are greater because with
less inputs, nearly less than a half in the best case,
it is possible to execute a retraining obtaining high
accuracy.

â€¢ C3 can be a good option for the retraining in cases

in which we prioritize resource utilization.

6 Discussion
Our experimental results focused on the retraining of models against
adversarial inputs, taking into consideration the most used and prac-
tical state-of-the-art metrics: LSA, DSA, NC and Random. On top
of that, we considered the manner the retraining is done.

As observed in the results, we have verified that NC is not a
consistent metric to take into consideration to select inputs when
doing retraining. Previous studies, mainly Harel-Canada et al. [12],
have already stated that NC should not be trusted as a guidance
metric for DL testing. On the one hand, NC measures the proportion
of neurons activated in a model and it assumes that increasing this

proportion improves the quality of a test suit. But on the other hand,
Harel-Canada et al. showed that increasing NC value leads to fewer
defects detected, less natural inputs and more biased prediction
preferences, which is contrary to what we would expect when
selecting inputs for a retraining process to have a better model
and reduce the input size for that. In our study, the results confirm
that NC should not be trusted as a guidance metric. Furthermore,
Table 5 shows an evident disadvantage on time when computing
NC. Overall, different metrics rather than NC should be used when
doing guided retraining.

Ma et al. [25] found that uncertainty-based and surprise-based
metrics are the best at selecting retraining inputs and lead to im-
provements in the number of used inputs, up to twice faster than
random selection, in order to find an answer on how to select
additional training inputs to improve classification accuracy. Re-
garding SA metrics, we confirm the results obtained in that study:
SA metrics compared to the baseline of random selection and also to
NC, are better and lead to faster improvements with a satisfactory
model. We consider this work as a complement to the results of the
RQ about the selection for retraining of Ma et al., as we focused
our work on increment the modelâ€™s accuracy against adversarial
inputs. Although our results show that these metrics improve the
model faster (in terms of the input size used for retraining), we
have found that the size of the dataset greatly affects the results,
because when using small datasets it is not as fast. Ma et al. only
considered datasets with more than 50.000 available inputs to use in
the retraining (the entire training set), but in real world applications
is not always possible to have such large datasets. Unlike this work,
we have experimented with a smaller dataset in which the improve-
ments are minor (Intel dataset). Previous studies, our results of SA
metrics and new implementations of these metrics [17, 28, 40] aim

to be a baseline for test methods in DL testing: for data selection in
retraining processes, data generation, data selection, etc.

Additionally, an important finding is the variable that we also
considered: configuration of retraining. All the related works have
only experimented with their own way to execute the retraining
and even sometimes they are not explicit with how they did the
retraining process. As we observe in the results, the configuration
can change the performance of the model considerably. We have
uncovered a new challenge on finding efficient configurations to
retrain deep learning models. Considering the studied configura-
tions, using the best configuration studied in this work (C2) can
give data scientists a fast and efficient method of retraining.

When using the combination of DSA metric and C2 retraining
configuration, we observe that it can lead to results that are aligned
with green AI research, which refers to research that takes into
account computational costs to reduce the computational resources
spent [7, 34], as less inputs are needed for retraining and also less
time to compute the values of the metric w.r.t. the evaluated options
for both independent variables. We observe greater benefits of using
C2 over C3, as we give greater weight to the efficiency without
diminishing modelâ€™s accuracy, and encourage data scientists to
build greener DL models.

7 Threats to validity
In this section, we report the limitations of our empirical study and
some mitigation actions taken to reduce them as much as possible.
Regarding construct validity, we include two original models us-
ing CNN architecture trained in two different datasets, respectively,
in order to mitigate mono-operation bias. Derived from the metrics
selected, we use LSA, DSA, NC and Random metrics to guide the
retraining runs, which may have potential threats. However, these
metrics have been used in previous work as shown in Section 3,
lowering this threat. And according to threats related to the config-
urations, we have reviewed the relevant literature and searched for
retraining configurations using adversarial inputs.

Concerning conclusion validity, the quality of the DL models
and implementations depend on the experience of the develop-
ers. To mitigate this, we provide the implementations organized
and following the â€œCookiecutter Data Scienceâ€ project structure2,
making them as simple as possible. To increase reliability in our
study, we detail the procedure to reproduce our work: the process is
shown in Section 4, datasets and replication package are available
online. Also, we address the randomness of our results by starting
the retraining runs for each data point from their respective initial
weights (from original model weights for C2 and C3, and from
scratch for C1).

Two threats to internal validity are the implementation of the
studied DL models, as well as the computation of the metrics. We
used available replication packages from the authors of the metrics,
using the same configurations they used for the experiments to
minimize this risk. Also, different models are used with different
datasets, mitigating that the results of our study of guidance metrics
and configurations are caused accidentally.

2https://drivendata.github.io/cookiecutter-data-science

DurÃ¡n et al.

Threats to external validity stem mainly from the number of
datasets, models and the adversarial generation algorithm consid-
ered. Our results depend also on the datasets, type of architecture
considered and the device used for training. We believe our results
are applicable to image classification datasets. Some of these threats
are addressed by the use of two image datasets, several state-of-
the-art metrics and an adversarial attack widely used by scientific
community. Regarding the architecture type, as the adversarial in-
puts can be generalized across different of these [10] and we use
them for the retraining, we also believe that results for other archi-
tectures could be similar to the results for CNN architecture, only
further experiments can reduce this threat.

8 Conclusions and Future Work
In this work we have studied DL testing metrics for guiding retrain-
ing of models. We performed an empirical study with the metrics
and also considered three different configurations of retraining
against adversarial inputs and did a comparison of the metrics and
the configurations. In summary, we observe that (i) the models
are increasing their accuracy against the test set augmented with
adversarial inputs as it was sought in the objectives of this work,
and (ii) there are computational benefits of using certain metrics
and configurations.

The empirical study showed that the SA metrics (such as LSA
and DSA) as guidance for a retraining phase are useful for data sci-
entists when using the following configuration: an augmented train-
ing dataset with adversarial inputs, starting from original model
weights.

With the previous configuration and metrics, we can improve
the accuracy of the models against adversarial inputs by up to 61.8%
on the GTSRB dataset and up to 45.7% on the Intel dataset without
the need of using many inputs. Therefore, this can be done by using
39.6% of the inputs on the former dataset and 83.7% of the inputs on
the latter when using DSA. Using random can guide to similar levels
of accuracy but when using the recommended configuration, the
computational benefits of using SA are that less inputs are required
as discussed above in percentage. However, we do not recommend
the use of NC metric, and prevent that when considering another
configuration such as: using an augmented dataset with adversarial
inputs and starting from scratch, can be time consuming, as almost
100% of the inputs need to be used to obtain similar accuracy to the
recommended configuration.

Additionally, we revealed that the size of the dataset is important
when implementing the recommended metrics and configuration.
Taking this into account, we need to assess whether it would be
worth calculating the metrics when using only small datasets.

In the next step, the use of other adversarial attacks for the cre-
ation of adversarial inputs and reproduction of our experiments
on different datasets of varied sizes, unbalanced datasets and also
non-images datasets [18] are required to generalize our findings.
Particularly, experiments with other DL architectures are required
to confirm or reject that our findings can be used across different
architectures. Also, our results should be compared to guided re-
training runs using new refinements of SA [17, 28, 40] and other
testing metrics such as uncertainty-based metrics.

Guiding the retraining of convolutional neural networks against adversarial inputs

Data availability
Following open science principles, the replication package with
data and implementation can be found on https://doi.org/10.5281/
zenodo.5904550.

Acknowledgments
This work was partially supported by: the â€œUNAM-DGECI: Ini-
ciaciÃ³n a la InvestigaciÃ³n (verano-otoÃ±o 2021)â€ scholarship pro-
vided by Universidad Nacional AutÃ³noma de MÃ©xico (UNAM); the
DOGO4ML Spanish research project (ref. PID2020-117191RB-I00);
the â€œBeatriz Galindoâ€ Spanish Program BEAGAL18/00064; the Aus-
trian Science Fund (FWF): I 4701-N; and, the project Continuous
Testing in Production (ConTest) funded by the Austrian Research
Promotion Agency (FFG): 888127.

DurÃ¡n et al.

International Conference on Automated Software Engineering. 120â€“131.

[24] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama.
2018. MODE: automated neural network model debugging via state differential
analysis and input selection. In Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. 175â€“186.

[25] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon.
2021. Test selection for deep learning systems. ACM Transactions on Software
Engineering and Methodology (TOSEM) 30, 2 (2021), 1â€“22.

[26] Silverio MartÃ­nez-FernÃ¡ndez, Justus Bogner, Xavier Franch, Marc Oriol, Julien
Siebert, Adam Trendowicz, Anna Maria Vollmer, and Stefan Wagner. 2022. Soft-
ware Engineering for AI-Based Systems: A Survey. ACM Transactions on Software
Engineering and Methodology (TOSEM) 31, 2 (2022), 1â€“59.

[27] Alina Matei, Andreea Glavan, and EstefanÃ­a Talavera. 2020. Deep learning for
scene recognition from visual data: a survey. In International Conference on Hybrid
Artificial Intelligence Systems. Springer, 763â€“773.

[28] Tinghui Ouyang, Vicent Sant Marco, Yoshinao Isobe, Hideki Asoh, Yutaka Oiwa,
and Yoshiki Seo. 2021. Corner case data description and detection. arXiv preprint
arXiv:2101.02494 (2021).

[29] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Fein-
man, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, et al.
2016. Technical report on the cleverhans v2. 1.0 adversarial examples library.
arXiv preprint arXiv:1610.00768 (2016).

[30] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1â€“18.

[31] Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, and Mohammad Reza
Mohammadi. 2021. Wise-SrNet: A Novel Architecture for Enhancing Image
Classification by Learning Spatial Resolution of Feature Maps. arXiv preprint
arXiv:2104.12294 (2021).

[32] Jonas Rauber, Wieland Brendel, and Matthias Bethge. 2017. Foolbox: A python
toolbox to benchmark the robustness of machine learning models. arXiv preprint
arXiv:1707.04131 (2017).

[33] Sijin Ren and Cheryl Q Li. 2022. Robustness of transfer learning to image

degradation. Expert Systems with Applications 187 (2022), 115877.

[34] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai.

Commun. ACM 63, 12 (2020), 54â€“63.

[35] Priyal Sobti, Anand Nayyar, Preeti Nagrath, et al. 2021. EnsemV3X: a novel
ensembled deep learning architecture for multi-label scene classification. PeerJ
Computer Science 7 (2021), e557.

[36] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs.
computer: Benchmarking machine learning algorithms for traffic sign recognition.
Neural networks 32 (2012), 323â€“332.

[37] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[38] Y Tian, K Pei, S Jana, and B Ray. 2018.

ICSE 18: ICSE 18: 40th International
Conference on Software Engineering, May 27-June 3, 2018, Gothenburg, Sweden.
(2018).

[39] Thi Ngoc Trang Tran, Alexander Felfernig, Christoph Trattner, and Andreas
Holzinger. 2021. Recommender systems in the healthcare domain: state-of-the-
art and research issues. Journal of Intelligent Information Systems 57, 1 (2021),
171â€“201.

[40] Michael Weiss, Rwiddhi Chakraborty, and Paolo Tonella. 2021. A Review and
Refinement of Surprise Adequacy. arXiv preprint arXiv:2103.05939 (2021).
[41] Xizhi Wu, Rongzhe Liu, Hanqing Yang, and Zizhao Chen. 2020. An Xception Based
Convolutional Neural Network for Scene Image Classification with Transfer
Learning. In 2020 2nd International Conference on Information Technology and
Computer Application (ITCA). IEEE, 262â€“267.

[42] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolu-
tional networks. In European conference on computer vision. Springer, 818â€“833.
[43] JM Zhang, M Harman, L Ma, and Y Liu. 2019. Machine learning testing: survey,

landscapes and horizons. arXiv. arXiv preprint arXiv:1906.10742 (2019).

[44] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
framework for autonomous driving systems. In 2018 33rd IEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 132â€“142.

References
[1] Mohit Kumar Ahuja, Arnaud Gotlieb, and Helge Spieker. 2022. Testing Deep
Learning Models: A First Comparative Study of Multiple Testing Techniques.
arXiv preprint arXiv:2202.12139 (2022).

[2] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
Software engineering for machine learning: A case study. In 2019 IEEE/ACM 41st
International Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP). IEEE, 291â€“300.

[3] P Bansal. 2019.

Intel image classification. Available on https://www. kaggle.

com/puneet6060/intel-image-classification, Online (2019).

[4] Earl T Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2014.
The oracle problem in software testing: A survey. IEEE transactions on software
engineering 41, 5 (2014), 507â€“525.

[5] Victor R Basili, Gianluigi Caldiera, and H Dieter Rombach. 1994. The Goal
Question Metric Approach. Encyclopedia of Software Engineering-2 Volume Set.
Copyright by John Wiley & Sons, Inc (1994), 528â€“532.

[6] Roger Creus Castanyer, Silverio MartÃ­nez-FernÃ¡ndez, and Xavier Franch. 2021.
Integration of Convolutional Neural Networks in Mobile Applications. arXiv
preprint arXiv:2103.07286 (2021).

[7] Roger Creus Castanyer, Silverio MartÃ­nez-FernÃ¡ndez, and Xavier Franch. 2021.
Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener
AI? arXiv preprint arXiv:2109.15284 (2021).

[8] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017.
Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410
(2017).

[9] Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, and Alper Cakan. 2020.
Importance-driven deep learning system testing. In 2020 IEEE/ACM 42nd In-
ternational Conference on Software Engineering (ICSE). IEEE, 702â€“713.

[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).

[11] Tianmei Guo, Jiwen Dong, Henjian Li, and Yunxing Gao. 2017. Simple convo-
lutional neural network on image classification. In 2017 IEEE 2nd International
Conference on Big Data Analysis (ICBDA). IEEE, 721â€“724.

[12] Fabrice Harel-Canada, Lingxiao Wang, Muhammad Ali Gulzar, Quanquan Gu,
and Miryung Kim. 2020. Is neuron coverage a meaningful measure for testing
deep neural networks?. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 851â€“862.

[13] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun,
Emese Thamo, Min Wu, and Xinping Yi. 2020. A survey of safety and trust-
worthiness of deep neural networks: Verification, testing, adversarial attack and
defence, and interpretability. Computer Science Review 37 (2020), 100270.
[14] Lisa JÃ¶ckel, Michael KlÃ¤s, and Silverio MartÃ­nez-FernÃ¡ndez. 2019. Safe traffic
sign recognition through data augmentation for autonomous vehicles software.
In 2019 IEEE 19th International Conference on Software Quality, Reliability and
Security Companion (QRS-C). IEEE, 540â€“541.

[15] Mohd Ehmer Khan et al. 2011. Different approaches to white box testing tech-
nique for finding errors. International Journal of Software Engineering and Its
Applications 5, 3 (2011), 1â€“14.

[16] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE). IEEE, 1039â€“1049.

[17] Jinhan Kim, Jeongil Ju, Robert Feldt, and Shin Yoo. 2020. Reducing dnn labelling
cost using surprise adequacy: An industrial case study for autonomous driving.
In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 1466â€“
1476.

[18] Seah Kim and Shin Yoo. 2021. Multimodal Surprise Adequacy Analysis of Inputs
for Natural Language Processing DNN Models. In 2021 IEEE/ACM International
Conference on Automation of Software Test (AST). IEEE, 80â€“89.

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. Advances in neural information
processing systems 25 (2012), 1097â€“1105.

[20] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E
Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation applied
to handwritten zip code recognition. Neural computation 1, 4 (1989), 541â€“551.

[21] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.

[22] Attoumane Loukmane, Manuel GraÃ±a, and Mohammed Mestari. 2020. A Model
for Classification of Traffic Signs Using Improved Convolutional Neural Network
and Image Enhancement. In 2020 Fourth International Conference On Intelligent
Computing in Data Sciences (ICDS). IEEE, 1â€“8.

[23] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE

