2
2
0
2

n
u
J

0
1

]
L
D
.
s
c
[

1
v
7
6
3
5
0
.
6
0
2
2
:
v
i
X
r
a

Research Software Publication Policy Case Study

Nic Weber

University of Washington nmweber@uw.edu

Abstract. Research software is increasingly recognized as a vital com-
ponent of the scholarly record. Journals oﬀer authors the opportunity to
publish research software papers, but often have diﬀerent requirements
for how these publications should be structured and how code should
be veriﬁed. In this short case study we gather data from 20 Physical
Science journals to trace the frequency, quality control, and publishing
criteria for software papers. Our goal with the case study is provide a
proof-of-concept for doing descriptive empirical work with software pub-
lication policies across numerous domains of science and engineering.
In the narrative we therefore provide descriptive statistics showing how
these journals diﬀer in criteria required for archiving, linking, verifying,
and documenting software as part of a formal publication. The contri-
bution of this preliminary work is twofold: 1. We provide case study of
Physical Science research software publications over time; 2. We demon-
strate the use of a new survey method for analyzing research software
publication policies. In our conclusion, we describe how comparative re-
search into software publication policies can provide better criteria and
requirements for an emerging software publication landscape.

Keywords: Research Software, Scholarly Communications

1

Introduction

Research software is often developed by researchers for the speciﬁc purpose of
transforming, analyzing, and visualizing data, and is often necessary for produc-
ing novel results in a computational domain. Despite the important role of soft-
ware development in scientiﬁc and engineering research these contributions are
rarely recognized as unique and credit-worthy contributions to knowledge pro-
duction [4,13]. Multiple surveys have reported that researchers feel their work
would be impossible to conduct without time, access, and money to develop
research software [5]. However, research publications that present novel results
remain the key indicator of credit and acknowledgment of scholarly contribu-
tions. Previous work has described why this model of research publication is
incompatible with a current ecosystem where signiﬁcant eﬀort is devoted to de-
veloping software tools in order to produce novel research results [2,10]. Previous
scientometric work has also shown that software is often cited in traditional re-
search publications [9,11], but tracing and identifying these software mentions is
diﬃcult due to disparate practices in publishing and archiving research software
[7,14].

 
 
 
 
 
 
2

Weber

Software papers can help improve the recognition and reward of development
activities that are critical to advancing science and engineering. In a research
software paper, the main focus is describing and verifying software used for a
speciﬁc research task. This includes providing a description of the software’s in-
tended use, its architecture and design, providing documentation, and a proof of
concept for how the software can be used [8]). Scholarly journals have recently
started to give authors the ability to publish research software papers, but prac-
tices across research domains vary substantially in how these papers are to be
structured, and what information they must contain in order to be accepted for
publication. For example, the journal ‘Astronomy and Computing’ will accept
software-focused papers if they describe “outcomes (positive and negative) of the
practical application of informatics techniques within astronomy” [12]. Similarly,
in Political Science the ‘Journal of Information Technology & Politics’ oﬀers a
“workbench” paper where authors can describe research software in detail, but
only if the author’s also include preliminary results using the software [1]. Dedi-
cated journals, such as the Journal of Open Source Software (JOSS), SoftwareX,
and the Journal of Open Research Software (JORS) have also attempted to cre-
ate a lightweight model for software papers where authors can provide a general
description of code, testing, and veriﬁcation of a software contribution.

Despite a growing practice of research paper publishing and analysis of soft-
ware citation, there has been signiﬁcantly less attention paid to how domain prac-
tices in software paper publications diﬀer between journals. In this short paper
we seek to, generally, characterize the software publication landscape through a
survey of journal policies. We focus on software publication policies and practices
in the broad domain of Physical Sciences in order to provide a proof of concept
for how a large-scale survey of software paper policies can be conducted in the
future. In the following sections we describe our methods of data collection and
analysis, and then present results from analyzing 20 diﬀerent journal policies
related to software publications.

2 Methods

To construct a sample of journal policies to review we ﬁrst consulted a list of
journals that publish software papers from the Software Sustainability Institute
in the UK [6]. From this initial list we then used the Web of Science’s journal
subject-clustering tool to identify to create a domain identity for each journal.
Here, we focus on the domain of Physical Sciences which includes subject-clusters
such as “Earth and Planetary Sciences”, “Physics”, and “Astronomy”, and “Chem-
istry.” This resulted in 24 Physical Science journals. We then manually reviewed
each journal’s policy about software publication and eliminated 4 journals that
do explicitly mention they do not accept software-focused papers. Our ﬁnal list
of 20 journals can be found in the supplemental data to this paper. We then
modiﬁed an evaluation framework from Candela et al that focused on surveying
Data Journal policies [3] to be applicable to research software. In the follow-

Research Software Publication Policy Case Study

3

ing section, we explain each component of this modiﬁed framework and how it
relates to a software publication policy.

2.1 Analysis

We ﬁrst read all 20 journal’s publication policies, including the scope, directions
to authors, and criteria for acceptance. We then manually recorded the nature
of the publication, the instructions for a software paper’s preparation, whether
the journal oﬀered open access publication, and 10 criteria for how the software
were to be archived and made available in the future. Below, we explain each
area of analysis, and our results from analyzing 20 Physical Science journals.

Nature The nature of a journal is whether it only accepts software papers or
a broader mix of research publications. JOSS, as described above, would be a
“Pure” software journal because it only accepts research software publications,
whereas ‘Astronomy and Computing’ would be a “Mixed” journal because it
publishes research-driven papers as well as software papers. Results: All but one
of the journals in the Physical Science sample were Mixed journals. “Computing
and Software for Big Science” (Springer) was the only pure software journal that
we identiﬁed in this domain that was devoted purely to software publications.

Length We characterized each journal’s requirements for page length for a soft-
ware publication in order to better understand acceptance criteria for a software
paper’s contribution. We determined the length parameter by searching for ex-
plicitly stated limits in Author Instructions. If no explicit guidance was given
we then looked at the lengths of ﬁve articles focused explicitly on software. If
all articles were four or fewer pages, the journal was listed as having a “Short”
length requirement, but if any were more than four pages, the journal was listed
as “Normal”. Results: The journal “Computational Molecular Science” (Wiley)
was the only journal that contained “short” software papers. All of the remaining
journals in our sample neither set nor enforced a length restriction on software
papers.

Access status We recorded whether a journal was default open-access, or was
not open-access but allowed for authors to publish their work with an open-access
license for a fee. Results: 17 of the journals in our Physical Sciences domain clus-
ter oﬀered open-access publishing for a fee. Only three journals were by-default
licensed as open-access, these include “Journal of Cheminformatics” (Springer);
“Geoscientiﬁc Model Development” (EGU); and, “Computational Astrophysics
and Cosmology” (Springer).

Software paper requirements Next, we investigate requirements for pub-
lishing a software paper in a Physical Science journal. We ﬁrst read through the

4

Weber

“Author Instructions” of each journal and came up with an overarching classi-
ﬁcation where a journal either provided explicit, implicit, or no directions on
how to prepare a software paper for publication. An implicit set of requirements
stated that there was a unique structure that a software paper should follow,
but otherwise had no requirements for how the software was to be evaluated or
described in the paper.

Results: Four journals in our sample had an implicit set of requirements for
a software paper, and four had no requirements or structure for a software paper.
The remaining 12 journals all described explicit requirements for a software paper
to be accepted for publication. For the journals with explicit requirements, we
then created a set of categories and judged the strength of these requirements
on a three point scale, where 3 represents a category that was required for
publication, a 2 represents a suggested category, and 1 represents the categorical
requirement was not present in the journal’s policy for publishing a research
software paper. The categories that were documented for each journal include
the following:

– Archiving: Software papers are required to archive the software in an

openly-accessible repository, such as on Github, Zenodo, or Dataverse;

– Persistent link: The authors must obtain a persistent link (e.g. a DOI) to

the software repository where the software was stored;

– License: The software being described in the paper must have an explicit

license governing its reuse or distribution;

– Functionality: The paper required software authors to describe the in-

tended functionality of the software.

– Quality and Testing: The paper should describe how software tests (e.g.
Unit, Integration, Performance testing) or quality control was carried out on
the software.

– Dependencies: If the software required the installation or use of other

libraries the software dependencies must be communicated to readers.

– Documentation: The software being described must include documenta-

tion for conﬁguring, running, and using the software.

– Application: The software must provide a use case, or demonstration of

use through an application to a research problem.

– Future Development: Authors are required to describe goals of future
development, or how users can contribute to ongoing development eﬀorts.

In the following plot, we show the frequency distribution of each category
(ranked on a scale of 3-1 as described above). It is worth noting that no journal
in our sample had a requirement for each category that we analyzed. The most
frequently required aspects of a software paper were that dependencies were doc-
umented, documentation was provided for future use, and a license governing
distribution was described in the paper. Somewhat surprisingly more than half
of our overall sample require software to be archived in order to publish a soft-
ware paper. Perhaps this is an assumption that does not need to be formally
documented or required. However, we caution that without explicit directions to

Research Software Publication Policy Case Study

5

authors there is the possibility that journals are publishing papers that describe
software, but don’t actually provide access to that software in meaningful ways
that would enable their reuse in research. This is also reinforced by the fact
that more than half of our sample had no requirements for documenting future
development plans, or how the scientiﬁc community could make contributions
to a research software package.

Fig. 1. Frequency distribution for software paper requirements across 20 journals in
Physical Science

Software Paper Publishing Trends Next, we sought to understand the fre-
quency of software papers published across all domains of research. In order to
obtain this data, we again rely upon Hong’s list of journals that regularly pub-
lish research software papers [6], but in most of the journals that we analyzed
there was no clear distinction between a software paper and a research paper.
For journals that did not explicitly label a software publication, we read the
abstract of each paper, and then judged whether the author’s indicated the pri-
mary contribution of the paper was about software, or were the results of using
a speciﬁc piece of software. Below, we plot the publication frequency of software
papers published annually from 2014-2021.

As Figure 2 shows, there is a slight an upward trend in the number of re-
search software papers published during this seven-year window. Informatics,
Engineering, and Physical and Life Science journals have increasingly published
software papers (absent a decline in overall publications during 2020).

6

Weber

Fig. 2. Frequency distribution for software papers published from 2014-2021

3 Conclusions & Future Work

In this short paper we have described trends in software papers published in
20 Physical Science journals. We demonstrate that journals vary widely in their
requirements for publishing a software paper, with most journals suggesting that
dependency information and documentation are necessary components of a soft-
ware publication. Further, we demonstrate an increasing tendency for many do-
mains to publish research software papers. As we note in the ﬁndings described
above it is curious that a minimum standard for archiving software is absent in
over half of the journal policies that we analyzed. In future work, we will extend
this survey method to analyze requirements across additional domains of science,
engineering, and the humanities. This will help to substantiate the preliminary
ﬁndings of this paper and allow for cross-domain comparisons of software publi-
cation trends. The goal of our work, overall, is to rigorously describe an emerging
software publication landscape, to recommend best practices in creating author
instructions for research software papers, and to support ongoing methods de-
velopment in measuring the impact of scientiﬁc software development.

DATA ACCESSIBILITY Data behind this paper are available with a CC0
license in Dataverse. For more information on the broader project and software
used to conduct this analysis please see our project’s Github Repository.

References

1. Journal of

Information Technology & Politics Aims & Scope

(2022),
https://www.tandfonline.com/action/journalInformation?show=aimsScope&
journalCode=witp20

2. Anzt, H., Bach, F., Druskat, S., Löﬄer, F., Loewe, A., Renard, B.Y., Seemann,
G., Struck, A., Achhammer, E., Aggarwal, P., Appel, F., Bader, M., Brusch, L.,
Busse, C., Chourdakis, G., Dabrowski, P.W., Ebert, P., Flemisch, B., Friedl, S.,

Research Software Publication Policy Case Study

7

Fritzsch, B., Funk, M.D., Gast, V., Goth, F., Grad, J.N., Hegewald, J., Her-
mann, S., Hohmann, F., Janosch, S., Kutra, D., Linxweiler, J., Muth, T., Peters-
Kottig, W., Rack, F., Raters, F.H., Rave, S., Reina, G., Reißig, M., Ropinski, T.,
Schaarschmidt, J., Seibold, H., Thiele, J.P., Uekermann, B., Unger, S., Weeber,
R.: An environment for sustainable research software in Germany and beyond:
current state, open challenges, and call for action. F1000Research 9, 295 (Jan
2021). https://doi.org/10.12688/f1000research.23224.2, https://www.ncbi.
nlm.nih.gov/pmc/articles/PMC7845155/

3. Candela, L., Castelli, D., Manghi, P., Tani, A.: Data journals: A survey: Data Jour-
nals: A Survey. Journal of the Association for Information Science and Technol-
ogy 66(9), 1747–1762 (Sep 2015). https://doi.org/10.1002/asi.23358, https:
//onlinelibrary.wiley.com/doi/10.1002/asi.23358

4. Carver, J.C., Gesing, S., Katz, D.S., Ram, K., Weber, N.: Conceptualization of a US
Research Software Sustainability Institute (URSSI). Computing in Science Engi-
neering 20(3), 4–9 (May 2018). https://doi.org/10.1109/MCSE.2018.03221924,
conference Name: Computing in Science Engineering

5. Hettrick, S., Antonioletti, M., Carr, L., Chue Hong, N., Crouch, S., De Roure,
D.C., Emsley, I., Goble, C., Hay, A., Inupakutika, D., et al.: Uk research software
survey 2014 (2014)

6. Hong, N.C.: In which journals should I publish my software. Reference Source

(2014)

7. Howison, J., Bullard, J.: Software in the scientiﬁc literature: Problems
with seeing, ﬁnding, and using software mentioned in the biology lit-
Information Science and Tech-
erature. Journal of
https://doi.org/10.1002/asi.23538,
nology
https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23538,
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23538

the Association for

2137–2155

(2016).

67(9),

8. Katz, D.S., Chue Hong, N.P., Clark, T., Muench, A., Stall, S., Bouquin, D.,
Cannon, M., Edmunds, S., Faez, T., Feeney, P., Fenner, M., Friedman, M.,
Grenier, G., Harrison, M., Heber, J., Leary, A., MacCallum, C., Murray, H.,
Pastrana, E., Perry, K., Schuster, D., Stockhause, M., Yeston, J.: Recognizing
the value of software: a software citation guide. F1000Research 9,
1257 (Jan
2021). https://doi.org/10.12688/f1000research.26932.2, https://www.ncbi.
nlm.nih.gov/pmc/articles/PMC7805487/

9. Li, K., Lin, X., Greenberg, J.: Software citation, reuse and metadata consid-
erations: An exploratory study examining LAMMPS: Software Citation, Reuse
and Metadata Considerations: An Exploratory Study Examining LAMMPS.
Information Science and Technology
Proceedings of
53(1), 1–10 (2016). https://doi.org/10.1002/pra2.2016.14505301072, https:
//onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301072

the Association for

10. Niemeyer, K.E., Smith, A.M., Katz, D.S.: The Challenge and Promise of Software
Citation for Credit, Identiﬁcation, Discovery, and Reuse. Journal of Data and
Information Quality 7(4), 1–5 (Oct 2016). https://doi.org/10.1145/2968452,
https://dl.acm.org/doi/10.1145/2968452

11. Park, H., Wolfram, D.: Research software citation in the Data Citation In-
dex: Current practices and implications for research software sharing and
reuse. Journal of Informetrics 13(2), 574–582 (May 2019). https://doi.org/10.
1016/j.joi.2019.03.005, https://www.sciencedirect.com/science/article/
pii/S1751157718302372

8

Weber

12. Pasian, F.: Astronomy and Computing - Journal - Elsevier (2022), https://www.
journals.elsevier.com/journals.elsevier.com/astronomy-and-computing
13. Weber, N.: Finite and inﬁnite games: An ethnography of institutional logics in
research software sustainability. Proceedings of the Association for Information
Science and Technology 57(1) (Oct 2020). https://doi.org/10.1002/pra2.281,
https://onlinelibrary.wiley.com/doi/10.1002/pra2.281

14. Weber, N.M., Thomer, A.K.: Paratexts and documentary practices: Text mining
authorship and acknowledgment from a bioinformatics corpus pp. 597–624 (2019).
https://doi.org/10.4018/978-1-5225-8903-7.ch024

