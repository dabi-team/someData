GLAD: Neural Predicate Synthesis to Repair Omission Faults

Sungmin Kang
sungmin.kang@kaist.ac.kr
KAIST
Daejeon, Republic of Korea

Shin Yoo
shin.yoo@kaist.ac.kr
KAIST
Daejeon, Republic of Korea

2
2
0
2

r
p
A
4
1

]
E
S
.
s
c
[

1
v
1
7
7
6
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
Existing template and learning-based APR tools have successfully
found patches for many benchmark faults. However, our analysis of
existing results shows that omission faults pose a significant chal-
lenge to these techniques. For template based approaches, omission
faults provide no location to apply templates to; for learning based
approaches that formulate repair as Neural Machine Translation
(NMT), omission faults similarly do not provide the faulty code
to translate. To address these issues, we propose GLAD, a novel
learning-based repair technique that specifically targets if-clause
synthesis. GLAD does not require a faulty line as it is based on
generative Language Models (LMs) instead of machine translation;
consequently, it can repair omission faults. GLAD intelligently con-
strains the language model using a type-based grammar. Further,
it efficiently reduces the validation cost by performing dynamic
ranking of candidate patches using a debugger. Thanks to the shift
from translation to synthesis, GLAD is highly orthogonal to exist-
ing techniques: GLAD can correctly fix 16 Defects4J v1.2 faults that
previous NMT-based techniques could not, while maintaining a
reasonable runtime cost, underscoring its utility as an APR tool and
potential to complement existing tools in practice. An inspection of
the bugs that GLAD fixes reveals that GLAD can quickly generate
expressions that would be challenging for other techniques.

KEYWORDS
program repair, machine learning, debugging

ACM Reference Format:
Sungmin Kang and Shin Yoo. 2022. GLAD: Neural Predicate Synthesis to
Repair Omission Faults. In Proceedings of ACM SIGSOFT International Sym-
posium on Software Testing and Analysis (ISSTA 2022). ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/1122445.1122456

1 INTRODUCTION
In Automated Program Repair (APR), one seeks to automatically fix
faulty code given a specification of desired behavior (e.g. a logical
constraint or a test suite [15]). Researchers have attempted different
approaches to tackle the problem; for example, SequenceR [8] uses
deep learning models trained on fix data to automatically find
patches, while FixMiner [29] uses templates mined from human
patches and applies them to fix faulty code. APR is an active research

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/1122445.1122456

area, with more than 20 papers being published every year [39].
Given the large volume of APR research, it would be beneficial to
look at the performance of our community as a whole.

As many APR techniques are evaluated on the Defects4J [24]
dataset, we can assess the collective performance of the APR com-
munity by analyzing which faults have been fixed by any tool that
deals with Defects4J at the time of this writing. In short, we find
that since the introduction of the Defects4J dataset, 138 faults have
been fixed overall. More importantly, our analysis reveals that the
amount of added characters in the developer patch is strongly in-
dicative of APR success: while 91.7% of faults that require fewer
than 16 characters to be added are fixed, only 22.7% of faults that
require more than 32 characters to be added were ever fixed.

Inspecting the patches consisting of 16 to 63 added characters
but have never been found by any APR tools, we find that a large
portion contains omission faults, i.e. faults in which necessary code
is missing. Omission faults pose a problem to both template-based
and learning-based techniques. Template-based APR tools may fix
only a restricted set of omission faults such as adding null checks, as
there is no faulty code to apply templates to. Meanwhile, learning-
based techniques use the neural machine translation formulation,
which necessitates a faulty statement to translate into a fixed state-
ment, making omission faults difficult to fix. On the other hand,
language models (LMs) are a natural solution to repairing omis-
sion faults, as they are trained with a generative loss [46] and are
consequently better suited to handle omission faults.

To this end, we introduce GLAD (Grammar-based LAnguage
models with Debuggers) which specializes in fixing if statement
omission faults. To the best of our knowledge, GLAD is the first deep
learning-based technique that moves away from the translation
paradigm to tackle omission faults. GLAD utilizes trained language
models and type-based grammar rules to effectively synthesize and
modify predicates for if statements. Given an LM, GLAD guides it
to generate predicates by placing an if token at a given location,
and queries the LM to synthesize natural sequences that can follow.
A well-trained LM will synthesize predicate sequences, given the
presentation of the if token. For efficient search, we introduce a
variant of the standard beam search algorithm that incorporates a
type-based grammar and a vocabulary set. Constraining the out-
put of the LM allows GLAD to generate significantly more valid
expressions, enhancing its overall performance.

GLAD further narrows the space of potential patches by us-
ing a debugger, which can both provide static information such as
the legal members of an object, and dynamic information such as
evaluation results of expressions, on the fly. By using the values
of generated predicates on passing and failing tests, we can dy-
namically filter and rank potential patches without compiling or
executing them, leading to efficient repair behavior.

 
 
 
 
 
 
ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

We perform empirical analysis to demonstrate the utility of
GLAD. First, we present the faults that GLAD fixed, showing the
extent to which GLAD has expanded the set of fixed faults in De-
fects4J. Results show that GLAD could fix 16 faults in Defects4J
v1.2 that previous learning-based techniques could not, and overall
we report fixing eight faults that were never fixed by any APR tool
we examine. We verify that GLAD continues to generate correct
patches in the bugs added by Defects4J v2.0, implying the versatility
of GLAD’s repair abilities. We also find both the grammar-based
beam search and the debugger-based reranking make a significant
contribution towards reducing the space of patches to evaluate,
showing that GLAD can combine these information sources harmo-
niously to generate useful patches. An analysis on the runtime of
GLAD reveals that it fixes the majority of bugs in under 30 minutes,
confirming it is an effective repair tool; along with GLAD’s orthog-
onal results, this also demonstrates how GLAD could supplement
other repair techniques to expand the breadth of bugs that can be
repaired using APR techniques in practice.

Qualitatively inspecting the bugs that GLAD uniquely fixes, we
find that they require the synthesis of complex expressions that
are difficult to generate using existing techniques, establishing the
contribution of GLAD to the field of APR at large. Further, these
expressions are generated in less than 30 minutes, demonstrating
the efficiency of GLAD even when generating long predicates.

To summarize, our contribution is as follows:

• We present an analysis of faults in the Defects4J dataset that
have been fixed by all published APR techniques we could
identify, finding that the next level of difficulty in APR lies
in fixing omission faults;

• We introduce GLAD, which specializes in fixing if state-
ment omission faults using novel technical components such
as LMs (that can effectively generate omitted code), gram-
mar rules (that can effectively constrain LM output), and a
debugger (that can efficiently rank generated patches);
• We evaluate the repair performance of GLAD, showing that
it is orthogonal to existing APR techniques, fixing 16 new
faults relative to existing learning-based techniques and
eight faults that no APR tool we survey has fixed, while
managing to do this in a reasonable amount of time;

• We perform ablation studies to confirm that each component

we add contributes towards successful repair;

• We make our tool publicly available1.

The organization of the paper is as follows. In Section 2, we pro-
vide an overview of recent APR techniques, showing that many lack
the ability to successfully deal with omission faults. Concurrent
to this point, the data provided in Section 3 shows that omission
faults constitute the ‘next level’ of APR difficulty that the APR com-
munity should overcome. In Section 4, we detail GLAD, showing
how it can specifically deal with if-statement omission faults. We
pose the research questions in Section 5, with results presented in
Section 6 showing that GLAD fixes 16 faults never fixed by any
other learning-based APR tool, and is highly orthogonal to all APR
techniques we examine. Section 7 discusses threats to validity in
our experiments, and Section 8 concludes.

2 RELATED WORK
The academic context for our work is outlined in this section.

2.1 Automated Program Repair
Giving an overview of all program repair techniques is not within
the scope of this paper; those interested may refer to Gazzola et
al. [15] for a broad array of techniques. Instead, we briefly describe
techniques relevant to the purpose of GLAD.

2.1.1 Template-based Techniques. Since PAR [26], which intro-
duced eight fix patterns derived from common developer patches,
many new templates have been used to fix faults: AVATAR in-
troduces templates learned from static analysis tools [36], while
FixMiner automatically mines patch templates from corpora of
patch data [29]. TBar, among the strongest APR tools, collates tem-
plates used by existing APR tools and uses them to repair, resulting
in a single tool fixing 74 faults2 correctly [37]. While template-
based techniques have enjoyed strong performance, barring a few
common patterns that have been captured by templates, such as
adding null checkers (TBar FP2), template-based techniques are
ill-suited in solving omission faults. This is why, although every
possible output of the templates of TBar was evaluated on the cor-
rect position, not even plausible patches were made for omission
faults such as Closure-15 of Defects4J v1.2.

2.1.2 Learning-based Program Repair. Here, we focus on the deep
learning (or NMT) based program repair techniques. Inspired by
the success of NMT in the NLP field, Hata et al. [17] and Chen
et al. [8] use the seq2seq [51] architecture, formulating repair as
translation between faulty code and correct code. All work we
are aware of maintains this formulation [5, 34, 44, 62]; the current
best-performing repair technique, Recoder [62], comes closer to
synthesis by generating edits based on an edit grammar, but is
still far from full-fledged synthesis as it only allows one identifier
‘placeholder’ as part of its repair process. There is a fundamental dif-
ference between the NMT-based techniques and GLAD: NMT-based
APR techniques structurally rely on existing faulty statements to
generate patches, while GLAD only needs the code context leading
to the faulty location.

2.1.3 Condition Synthesis. There is a significant body of work tar-
geting condition synthesis; as condition synthesis often deals with
omission faults, most of these techniques are not strictly template-
based. ACS [57] uses predicate switching and syntactic code search
over a database to find fixing predicates; as such, ACS cannot fix
condition omissions with a predicate that never appears in the code
corpus, unlike GLAD. Nopol [12] and its successor Dynamoth [13]
first base their predicate synthesis on oracle mining, then use an
SMT solver and a debugger respectively to find the fixing predi-
cate. The assumptions that Nopol/Dynamoth make are inaccurate
at times, leading to significant test suite overfitting [39]. Finally,
JAID [6]/Restore [58] use state abstraction to identify ‘suspicious
program states’. Both are more restricted in the types of predicates
they can generate than GLAD, which can search through a larger
space efficiently thanks to language models.

1https://anonymous.4open.science/r/neural-pred-synth-4816/README.md

2c.f. Liu et al. [37], Tab. 5

GLAD: Neural Predicate Synthesis to Repair Omission Faults

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

2.2 Language Models
A statistical language model defines a probability distribution over
the space of sentences. That is, for every possible sentence, a lan-
guage model will provide the probability of such a sentence. Lan-
guage models are trained so that the probability of a training corpus
is maximized. Pretrained LMs are receiving a great amount of atten-
tion, thanks to their strong performance on few-shot learning [4].
LMs trained on source code are known to be able to measure natu-
ralness [19, 47] and perform autocompletion tasks [7, 28], so it is
natural that they could perform the role of a synthesizer as well.
We use byte-pair encoding (BPE) in this work, recently used in the
software engineering context by Karampatsis et al. [25], to handle
subtokenization. To the best of our knowledge we are the first to
primarily use a language model to perform repair.

2.3 Debuggers
While GLAD is the first learning-based APR technique to employ
debuggers, they have a history of use by APR tools. One early such
work is Zeller [61], which uses a debugger (GDB) to isolate failure-
relevant program states. Later, Galenson et al. [14] introduce Code-
Hint, which queries a debugger to identify available local variables,
among others. CodeHint requires user-specified constraints to op-
erate, making it difficult to compare with GLAD. Dynamoth [13]
uses debuggers to generate and evaluate expressions as well.

3 ANALYSIS OF EXISTING APR RESULTS
We begin by investigating the state-of-the-art in APR as a whole:
which faults in the Defects4J dataset are reported to be correctly
fixed by any APR tool? Following the criteria used by Liu et al. [39],
we survey papers in the community-maintained program-repair.org,
as well as in the living APR review maintained by Monperrus [45],
to identify Java-targeting APR techniques. We apply the following
criteria when gathering all faults fixed by the community:

• As APR performance is often measured under controlled
(i.e. perfect) fault localization [34, 39, 41] to gauge repair
performance without localization bias, when possible we use
repair results under such conditions, either from the paper
or from the results reported by Liu et al. [39].

• If the repair performance under perfect fault localization
is not available and the paper introducing the technique
provides specific Defects4J faults that were fixed, those are
directly used as the results;

• We consider two different TBar versions, 𝑇 𝐵𝑎𝑟𝑝 [37] and
TBar-10k [39], as the performance of TBar significantly
changes based on the stopping criteria.

Performing this survey, we identify 46 tools that fix Defects4J
faults; among those, we could retrieve fault-level repair results for
40 tools. Each APR tool, along with their fault localization (FL)
granularity, is reported in Table 1. The source for these results can
be found in the Appendix, Table 1. Overall, we find that there are
138 faults in Defects4J that have been fixed at least once, with easy
faults being fixed multiple times (e.g. the most fixed fault, Math-70,
was fixed by 25 tools), while others are fixed only a few times (e.g.
Math-71 was only fixed by SimFix).

Table 1: All tools used for the state-of-the-art APR analysis.
We could not find fault-level results for unused tools.

FL

Perfect-
Statement

#

23

Techniques

TBar-Time [37], TBar-10k [39], CURE [23],
SequenceR [8], SimFix [22], FixMiner [29], DLFix [34],
ACS [57], Ratchet [18], CODIT [5], AVATAR [36],
CoCoNuT [41], ARJA [60], jGenProg [42],
GenProg-A [60], jMutRepair [42], kPAR [35],
RSRepair-A [60], jKali [42], Kali-A [60], Dynamoth [13],
Nopol [12], Cardumen [43]
HDRepair [32], JAID [6], SketchFix [20], Restore [58]

4
11 Hercules [49], CapGen [53], PAR [26], SOFix [40],
PraPR [16], ConFix [27], VFix [59], iFixR [30],
LSRepair [38], GenPat [21], Recoder [62]
ssFix [56]3, NPEFix [11]4
Elixir [48], DeepRepair [54] LoopRepair [52], xPAR [32],
S3 [31], VarFix [55]

2
6

Method-given
No perfect info,
uses test spectra

Others
Unused in Study

We further obtain features for each fault based on the developer
patch and analyze if there is a feature that predicts whether a fault
will have been fixed by an APR tool at some point. If such a feature
exists, we can reasonably say that the feature correlates with APR
difficulty. We use the following features:

• We use the Defects4J-dissection dataset [50] which provides
8 numeric features: the number of files/classes/methods/lines
that are involved with the developer patch, the number of
added/removed/modified lines, and the number of chunks.
• We use the git diff tool to obtain token-level diffs, and in
turn derive characters added/removed features from them.
As the diffs are token-level, the number of characters added/re-
moved is always an overestimation.

Figure 1: Defects4J full fault distribution by feature, con-
trasted with the fixed fault distribution. The blue line rep-
resents the fixed fault proportion in a bin.

The raw data used in this section is made publicly available5, so
that the community can build upon it and perform other analyses.
Using these features, we plot the distribution difference between
all faults in Defects4J and those that have been fixed at least once.

3ssFix uses stack trace information.
4NPEFix has no fault localization.
5https://anonymous.4open.science/r/neural-pred-synth-4816/d4j_apr_results/fix_
data.csv

0315632551023Added Character # (log scale)0102030405060708090Count(a) Dist. of [Added Character #]0315632551023Deleted Character # (log scale)020406080100120140Count(b) Dist. of [Deleted Character #]True Bug DistAll FixedProportion0137153163Added Line # (log scale)020406080100120140Count(c) Dist. of [Added Line #]01371531Chunks in Patch (log scale)050100150200250300Count(d) Dist. of [Chunks in Patch]0.00.20.40.60.81.0Fixed Proportion by Bin0.00.20.40.60.81.0Fixed Proportion by Bin0.00.20.40.60.81.0Fixed Proportion by Bin0.00.20.40.60.81.0Fixed Proportion by BinISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

The results of this analysis are presented in Figure 1. First, note
that the amount of deletion (Figure 1(b)) does not have a consis-
tent relationship with the probability of a fault being patched (the
blue line fluctuates), suggesting that deletion is not particularly
relevant when accessing APR difficulty. However, the other three
features have a clear negative relationship with patch probability.
Following the fixed proportion (blue line) of Figure 1(a), for faults
whose patches contain a low number of added characters, the fixed
proportion is highest of all features, indicating almost all have been
fixed. As the added character number increases (and the graph
moves to the right), the percentage of faults that have been fixed
steeply decreases. Thus we suggest that the number of added char-
acters is a reasonable measure of APR difficulty. This implies that
synthesis is the bottleneck of program repair. As a result, the
point where the fixed ratio decreases in Figure 1(a) (15+ characters
added) constitutes the ‘next level’ of difficulty for APR.

Table 2: Unfixed ‘next level’ faults in each fault category.

Fault Size

total Weak om.

Strong. om.

Total om.

if om.

16-31
32-63

Overall

11
37

48

4 (36.4%)
9 (24.3%)

5 (45.5%)
23 (62.2%)

8 (72.7%)
31 (83.7%)

4 (36.4%)
13 (35.1%)

13 (27.1%)

28 (58.3%)

39 (81.3%)

17 (35.4%)

What types of faults constitute the ‘next level’? We first define
a few terms: a fault has ‘weak omission’ if the developer patch
contains a modification that only adds code to a certain statement.
In theory, existing NMT or template-based techniques can fix weak
omission faults, but the synthesis aspect can make fixing them diffi-
cult. A fault has ‘strong omission’ if a new statement is introduced.
Strong omission faults are structurally challenging to fix for exist-
ing NMT-based techniques; depending on what the inserted code
is, they can be difficult or impossible for existing template-based
techniques as well. Finally, a fault has ‘if omission’ if it has either
weak or strong omission and the added code is related to an if
predicate. We manually investigated all 48 unfixed faults within the
16-63 added character range. The results of this analysis, shown in
Table 2, indicate that 81.3% of all faults inspected have an omission
aspect; the faults in which a novel statement is added constitute
58.3% of unfixed faults in the range. Meanwhile the faults that have
if-statement omission amount to 35.4% (or 43.6% of all omission
faults), many of which require complex predicates that have re-
sisted synthesis attempts until now. This is notable as omission
faults, especially strong omission faults, are ill-handled by existing
template-based and learning-based techniques, as noted in Section 2.
As the leading techniques in the field struggle in fixing omission
faults, we believe a technique capable of efficiently complementing
these tools represents a meaningful step forward in APR.

4 APPROACH
We propose GLAD, a learning-based technique that specifically
aims to repair if-statement omission faults. By moving away from
the NMT paradigm (i.e., learning how to translate a faulty statement
to a correct statement), and instead using the context of the faulty
location to directly synthesize code, GLAD is well-adapted to fixing
omission faults unlike existing NMT-based techniques. We explain

our algorithm with the running example of the Defects4J fault
Closure-15, presented in Listing 1: the predicate n.isDelProp()
must be synthesized and combined with the {return true;} body.

Listing 1: Closure-15 developer patch, edited for brevity.
@@ -99 ,9 +99 ,9 @@ (...) {
return true ;

1
2
3 +
4 +
5 +
6
7

}
if (n. isDelProp ()) {

return true ;

}
for ( Node c = n. getFirstChild (); ...) {

if (...) {

An overview of the mechanism of GLAD is presented in Figure 2.
There are six steps in the repair pipeline. Before the repair proceeds,
a pretrained language model is fine-tuned on the faulty version
of the code so that the LM can pick up token usage patterns from
the target project (Figure 2(a)). Next, given a suspicious location,
the body of the method preceding the faulty location is tokenized,
and a ‘repair seed’, namely an if token along with an opening
parenthesis, is added to the end (Figure 2(b)). Prior to beam search,
the static information such as available identifiers and available
members to types are extracted via a debugger, to construct a gram-
mar constraining the language model (Figure 2(c)). The fine-tuned
language model will take cue from the injected if token and find
predicate candidates using grammar-aware beam search to generate
grammatically valid predicates (Figure 2(d)). Generated predicates
are evaluated by a debugger; the obtained dynamic information is
used to filter and rank generated patches, based on how correct
code should behave (Figure 2(e)). Finally, we add predetermined
bodies to the predicates, and evaluate the patches over tests for
verification (Figure 2(f)). The details of each step are explained in
the remainder of this section.

4.1 Preparation for Synthesis
This subsection describes the details of finetuning, tokenization of
the faulty context, and the addition of the repair seed.

Finetuning. Prior to repair, the pretrained language models
4.1.1
are trained on the unfixed version of the project, so that the LM can
‘get familiar’ with novel tokens (Figure 2(a)). This allows the LM
to pick up lexical patterns that are not in the pretraining dataset;
for example, the token isDelProp from the patch of Closure-15
never appears in the pretraining dataset we use, which may hurt
repair performance without finetuning. Indeed, finetuning has been
shown to be useful in increasing the performance of LMs [25].

4.1.2 Adding Repair Seed. The finetuned LM is capable of predict-
ing a sequence of tokens based on preceding context. To provide
context, given a suspicious location we first tokenize the method
content prior to the given location, resulting in a sequence of to-
kens representing the synthesis context. For Closure-15, Lines 1-2
are parsed to [..., ‘return’, ‘true’, ‘;’, ‘}’]. Given this
sequence, a ‘repair seed’ is appended to lead the language model to
generate appropriate predicates at that point (Figure 2(b)). When
the faulty location is not an if statement already, two tokens, ‘if’
and ‘(’, constitute the repair seed. As the language model likely
learned that predicates appear after an if token is provided, given
the context and the repair seed it assigns high probabilities to token

GLAD: Neural Predicate Synthesis to Repair Omission Faults

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Figure 2: Diagram of GLAD.

sequences that resemble predicates, and thus generates predicate-
resembling sequences during beam search. In the special case of
when the fault location points to an if condition itself, we remove
the last parenthesis from the if condition, prompting the language
model to explore alternative predicates.

Legal Identifier Extraction. A key ingredient for our grammar
4.1.3
is the set of legal identifiers at a suspicious location. To extract them
as in Figure 2(c), we perform the following steps:

(1) Build the AST of the faulty file using the javalang library6
and determine the scope of each identifier based on the
parent node class and the relative position of the node.
(2) Collect legal identifiers by selecting nodes such that the
fixing location is located in its scope. For example, in Listing 1
the scope of the method parameter n spans the entire method;
thus n is a legal identifier.

(3) For each identifier, get additional information such as its
type, bound methods, class fields, and the return type using
JDB analysis. For example, we record that the parameter n
is type Node and has methods such as isDelProp().

(4) From within the target file, we extract static classes that were

used and obtain static members of these classes.

Additionally, we allow a predetermined set of literals. First, the
primitive literals {null, 0, 1, true, false} are allowed and
treated equivalently to variables that have their corresponding
primitive types. These literals were chosen as they were the most
common literals in our training code corpus; the use of other lit-
erals (such as large integers) is often idiosyncratic, making them
inappropriate to add in our grammar rules which aim to be general.
In addition to these values, we allow GLAD to generate a string
placeholder of type java.lang.String; when generating concrete
patches, this placeholder is replaced with strings or characters that
occur within the target method.

4.2 Beam Search
To generate predicate candidates, we perform beam search (Fig-
ure 2(d)), which is a commonly used algorithm that uses trained
language models to synthesize a pool of likely candidates. We first
describe the grammar that constrains the space that beam search
can explore, then subsequently show how the grammar is integrated
into beam search to effectively find likely predicates.

6https://github.com/c2nes/javalang

4.2.1 Grammar Rules. To generate syntactically valid patches, we
constrain the expansion of the language model: namely, it must
follow grammar rules and only use the allowed vocabulary. Using
the legal vocabulary set obtained in the previous section, we deter-
mine what possibilities can be explored. Specifically, to maximize
the number of valid expressions generated, we keep track of the
type of each expression, and assure that each token added is type-
compatible with the preceding expression. For example, given an
expression completed up to if(n., we know that n is type Node,
and that given the member operator (.) member tokens are allowed
next. Node has members such as isDelProp or next, so we can
append these tokens knowing that the ensuing expression will be
valid. Using types has significant benefits, such as allowing GLAD
to recursively keep track of which member tokens are allowed at
each stage of expansion.

4.2.2 Grammar-directed Beam Search. Beam search is a heuristic
algorithm that performs a pruned breadth-first search over the
tree of possible sequence continuations. The pseudocode for our
grammar-constrained beam search is presented in Algorithm 1.
The initial context to the language model is created by adding the
tokenized content of the method preceding the faulty location to
the repair seed (Line 1), which allows the candidate patch list (𝐶𝑃)
to initialize (Line 2). The maximum number of expansions during
beam search is capped by a predetermined parameter 𝑙 (Line 3).
At each expansion step, all legal sequences stemming from the
current list of candidate expressions are considered (Line 6-19). For
each sequence candidate 𝐶 and its probability as judged by the
language model 𝑝𝐶 , if the current candidate is not complete (Line
7), the grammar first analyzes what tokens can follow the current
expression while keeping it valid in terms of type (Line 9).

If the grammar dictates that there is only one token possible,
we append tokens to the candidate until there are multiple options
(Lines 10-12). This is a novel process (not typically used in beam
search) that we use in conjunction with our grammar rules to save
timesteps that would be wasted expanding already determined
results, thus increasing search efficiency. For example, given the
expression n.isDelProp, we know based on type analysis that
isDelProp accepts no arguments; Lines 10-12 allow GLAD to add
the appropriate tokens ( and ) without using expansion timesteps.
Once there are multiple possible tokens to choose from, we query
the LM to obtain the probability of each possible token (Line 13).
For each of the legal tokens and their corresponding probabilities,

Extract vocabulary (c)'n', '.', …'!', 'check', …'visit', '(', …Perform beam search (d)Dynamic Reranking (e)'n', '.', …'!', 'check', …ValidateAdd body[… 'true', ';', '}',]Addseedtocontext (b)Patch testing (f)LanguageModelGrammarFaulty code,with fault locationFinetuning (a)['if', '(']JDBJDBJDB: Java Debugger…………n.isCall(), n.isDelProp()…+ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

Algorithm 1: Grammar-directed Beam Search

input : A trained language model 𝐿𝑀, a token grammar 𝐺,
context 𝐶𝑜𝑟𝑔, maximum length 𝑙 and beam width 𝑊

output : Candidate patches CP

1 𝐶𝑖𝑛𝑖𝑡 ← 𝐶𝑜𝑟𝑔 + GetRepairSeed(𝐶𝑜𝑟𝑔);
2 𝐶𝑃 ← [(𝐶𝑖𝑛𝑖𝑡 , 0)];
3 for 𝑖 in 1:𝑙 do
4

stageCandidates ← [];
stageCandidateProbs ← [];
for 𝐶, 𝑝𝐶 in 𝐶𝑃 do

if IsNotComplete(𝐶) then

newC ← C;
legalToks ← 𝐺 (𝐶);
while len(legalToks) = 1 do

newC ← newC+legalToks[0];
legalToks ← 𝐺 (𝐶);

legalTokProbs ← 𝐿𝑀 (𝑛𝑒𝑤𝐶, 𝑙𝑒𝑔𝑎𝑙𝑇 𝑜𝑘𝑠);
for 𝑡, 𝑝𝑡 in (legalToks, legalTokProbs) do
stageCandidates.append(𝐶 + 𝑡 );
stageCandidateProbs.append(𝑝𝐶 × 𝑝𝑡 );

else

stageCandidates.append(𝐶);
stageCandidateProbs.append(𝑝𝐶 );

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

𝐶𝑃 ← SortBySecond(stageCandidates, stageCandidateProbs);
𝐶𝑃 ← 𝐶𝑃 [:W];

21
22 return 𝐶𝑃;

we register them as potential beam candidates (Lines 14-16). At
the end of each stage, we pick the top 𝑊 beams by descending
order in probability and update the candidate patches, 𝐶𝑃 (Line 20-
21). The use of this grammar-aware beam search allows predicates
generated by GLAD to almost always be syntactically correct.

4.3 Dynamic Reranking using JDB
To further reduce the space of patches to be evaluated, we propose a
novel system based on dynamic information obtained from the Java
debugger, JDB, that can eliminate and rank patches (Figure 2(e)).

First, we observe that any correct patch must change failing test
behavior. For a side effect-free if statement addition patch, this
means that its predicate must evaluate to true at some point in all
failing tests. Formally, given the set of failing tests 𝐹 and a predicate
𝑝 to be added, assume a test 𝑒 ∈ 𝐹 that is executed a nonzero number
of times. If the set of timepoints in which 𝑝 is executed under test
𝑒 is 𝐼𝑒 , and the value of 𝑝 at test 𝑒 at timepoint 𝑖 ∈ 𝐼𝑒 is 𝑝 (𝑒, 𝑖), we
can denote the “behavior must change” criterion as in Equation (1).
Using its contrapositive, we filter out any if statement additions
in which the predicate always evaluates to false in a failing test.

(1)

[𝑝 is correct] → ∀𝑒 ∈ 𝐹, ∃𝑖 ∈ 𝐼𝑒 s.t. 𝑝 (𝑒, 𝑖) = 𝑇
For passing tests, there are no necessary or sufficient conditions
for correct patches, due to coincidental correctness [3]. Nonetheless,
a patch that minimally influences passing test behavior is intuitively
more likely to be the correct patch [12]. Given the set of passing tests
𝑃 that cover the faulty location, the probability of an if statement
addition with predicate 𝑝 being correct has an inverse correlation
with PassChange(𝑝), defined in Equation (2).

PassChange(𝑝) = |{𝑒 ∈ 𝑃 | ∃𝑖 ∈ 𝐼𝑒 s.t. 𝑝 (𝑒, 𝑖) = 𝑇 }|

(2)

Based on these two principles, we derive the score for the order-
ing of predicates to use in if-statement insertion in Equation (3).
As the principle is based on behavior change, how likely a predicate
is correct depends on how the predicate is used as well. If the pred-
icate is part of an if-statement complete with a new body added,
the predicate must evaluate to true in failing tests, while it is good
if it evaluates to false in passing tests. On the other hand, if the
predicate is used to wrap existing code, the opposite holds. As such,
we give multiple scores to each predicate, each corresponding to
a body template described in Section 4.4. In practice, Equation (3)
can lead to many ties; these are broken by the language model’s
likelihood score for each predicate.

(cid:40)

Score(𝑝) =

−∞
−PassChange(𝑝)

if ∃𝑒 ∈ 𝐹, ∀𝑖 ∈ 𝐼𝑒 𝑝 (𝑒, 𝑖) = 𝐹
else

(3)

To efficiently calculate predicate scores using Equation (3), GLAD
again uses the debugger. The debugger stops execution at the
fix location, allowing evaluation of generated expressions. Our
debugger use consists of two steps: one short pass to check which
expressions generated by the LM are actually Boolean and have no
side effects (the validation pass), and one longer pass to evaluate
predicate score according to Equation (3) (the ranking pass). As
there is no compile overhead, and the context preceding the faulty
location does not need to be executed multiple times, using the
debugger in the proposed manner allows a significant speedup
relative to executing each patch individually. The empirical effects
of this ranking are investigated in Section 6.3.

4.4 Patch Validation
At this point, we have generated and ranked a set of predicates; we
must now determine the bodies for these predicates to complete a
patch and validate it based on the test suite (Figure 2(f)).

Attaching a Body. We apply three templates for bodies: (BT1)
returning a literal or raising an exception within the failing test (as
does ACS [57]), (BT2) wrapping the code that follows the faulty
location and is within the same block with the predicate (as in
TBar [37] FP4.4), and (BT3) adding a clause to an existing predicate,
in which case there is no need to generate a new body. In Closure-
15, using BT1 allows making the correct patch. As our focus is on
predicate synthesis, we do not use complex techniques to synthesize
the body; however, the language model is theoretically capable of
synthesizing statements as well, and as such synthesizing the body
could be explored in future work.

Validation. All generated patches are sorted in order of descend-
ing score and evaluated relative to the test suite provided by De-
fects4J. A patch that passes all tests is plausible; plausible patches
are manually inspected to check for correctness. If the patch is
judged to be semantically equivalent to the developer patch, the
patch is deemed correct.

GLAD: Neural Predicate Synthesis to Repair Omission Faults

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

5 EXPERIMENTAL SETUP
5.1 Configurations
Fault Localization. We experiment with two FL settings: per-
5.1.1
fect statement fault localization and method-given fault localization.
Perfect fault localization allows measuring repair performance with-
out localization bias [39]; further it is widely used in prior learning-
based APR techniques [5, 8, 23, 41], allowing for fair comparison.
Method-given fault localization is also used to demonstrate the
versatility of GLAD, and for fair comparison with state-of-the-art
constraint-based techniques [6, 58]. In this formulation, the buggy
method is known, and GLAD iterates over fix locations within the
method using the Ochiai [1] suspiciousness order.

5.1.2 Language Model. The LM is pretrained on the Java-med
dataset gathered by Alon et al. [2]. We extract methods using
javalang and treat them as sentences to pretrain the LM; we dis-
card the less than 1% of methods and files that javalang fails to
parse. We perform tokenization using javalang, and further per-
form subtokenization using Byte-Pair Encoding (BPE) [25] with
approximately 5,000 pairs, so that the LM does not suffer from
out-of-vocabulary issues. Prior to training, data related to the De-
fects4J projects is purged. The LM is trained for five epochs on the
subtokenized pretraining dataset, which is enough for learning to
stabilize. Pretraining is an offline process that does not need to be
repeated for each fault. We use GRUs [9], as they are known to be
better than traditional recurrent neural networks and comparable
in performance to LSTMs [10] while being structurally simple. The
detailed architecture of our model is provided in the Appendix.

5.1.3 GLAD hyperparameters. During finetuning (Figure 2 (a)), the
language model is trained for one epoch on the faulty code, as more
training can quickly lead to overfitting due to the smaller size of
the faulty project. We perform beam search with a beam width 𝑊
of 10,000 and maximum length 𝑙 of 15 tokens, which empirically
leads to the best performance. The beam width is larger than prior
work [23]; GLAD can navigate this larger space thanks to dynamic
reranking. Dynamic reranking (Figure 2 (e)) is given a timeout
of 15 minutes, which was empirically determined to be sufficient.
We run GLAD for a maximum of three hours, although GLAD
often completes much earlier than that. After patch verification the
plausible patches are inspected for correctness, as is done in prior
work [6, 23]. We evaluate on 95 bugs from Defects4J v1.2 and 89
bugs newly introduced in Defects4J v2.0 for a total of 184 bugs that
include if statement faults.

5.1.4 Computational Resources. Experiments were run on eight-
core machines with Intel(R) Core(TM) i7-6700 CPU and NVIDIA
1080/3090 GPUs.

5.2 Research Questions
RQ1. Repair Effectiveness: How many omission faults can GLAD
successfully repair, and how distinct is its repair profile? Using the
two fault localization scenarios, we compare the performance of
GLAD on the widely studied Defects4J v1.2 with multiple baselines.
To emphasize how unique the suite of bugs that GLAD fixes are, we
first compare our performance using perfect fault localization with
(i) TBar employing perfect-FL [37], which represents an ensemble

of multiple template-based tools, and (ii) all reported fixes of deep
learning-based tools [5, 8, 18, 23, 34, 41, 62] that we are aware of. We
additionally compare our method-given repair performance with
the state-of-the-art condition generating repair tool Restore [58],
which evaluates under equivalent settings. Finally, we report the
number of times each bug was fixed by a technique listed in Table 1
and check how many bugs GLAD has fixed against the field as a
whole. If GLAD can fix previously unfixable bugs, it would indicate
that GLAD is indeed unique in its repair capabilities.
RQ2. Generalization: In order to mitigate external validity con-
cerns and show that GLAD can operate over a wide range of
projects, we evaluate GLAD using perfect fault localization on
the buggy projects that were newly introduced in Defects4J v2.0.
By verifying its performance against bugs in new projects, we hope
to show that GLAD is a general-purpose technique for handling if-
omission faults that is not overfitted to the widely studied Defects4J
v1.2 benchmark.
RQ3. Ablation Study: How much contribution does each of the
components of GLAD make to repair? To investigate, we remove
each component of GLAD and compare the performance to the full
model, using all 184 bugs studied in RQ1 and RQ2, as follows:

• -Finetuning. Finetuning can simply be omitted from GLAD,

as it is not intertwined with other repair components.
• -Grammar. Grammar can be removed by changing 𝐺 in Line
9 of Algorithm 1 to allow all tokens, resulting in unrestricted
beam search based on token naturalness alone.

• -Language Model. We modify Line 20 of Algorithm 1 to

sort at random, instead of using the LM.

Additionally, we analyze how much verification effort dynamic
ranking is saving, based on how the correct patch’s ranking im-
proved relative to when only using the likelihood score provided
by the LM.
RQ4. Efficiency: How efficient is GLAD in comparison to other
APR techniques? To investigate, we gather statistics about the
runtime of GLAD, and compare them with two other compara-
ble techniques that report their average runtime, CURE [23] and
Restore [58].
RQ5. Qualitative Analysis: What can we learn from both the
successful and unsuccessful patches generated by GLAD? With
RQ5, we qualitatively analyze how GLAD generates successful
patches. Further, we identify failure instances of GLAD to highlight
shortcomings and potential improvements in future work.

6 RESULTS
6.1 Repair Performance (RQ1)
Repair results on faults from Defects4J v1.2 are presented in Table 3.
GLAD can fix 28 faults correctly using perfect FL, and 19 faults
with method-given FL. More to the point, however, is how different
the bugs that GLAD fixes are. Relative to TBar, GLAD fixes 19 dif-
ferent bugs correctly, confirming that GLAD fixes a large group of
bugs with which template-based techniques struggle. Further, when
comparing with the entire set of bugs known to be fixed by deep
learning-based techniques (DL), GLAD fixes 16 different bugs un-
der the perfect fault localization scenario, significantly expanding
what is possible with learning-based APR techniques. While other
techniques do fix bugs that GLAD fixes in certain instances, they

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

Table 3: Faults correctly fixed by GLAD in Defects4J v1.2.
⃝/△/× stand for correctly/plausibly/not fixed, respectively.
The +C column shows the number of characters added by
the developer patch for each bug, and the DL column rep-
resents the known fix results of learning-based APR tech-
niques. On the right, GLAD-M shows the performance of
GLAD when the method is given, while RSTR is short for
Restore. Note the DL column does not have plausible results,
as many tools do not report their per-bug plausible fixes.

are generally within the 16-64 added character range (‘next level’
bugs in Section 3), suggesting GLAD met its stated goal.

Answer to RQ1: GLAD can fix a unique set of bugs, regardless
of the comparison group, fixing 16 bugs that previous learning-
based APR techniques could not fix and 8 that were never re-
ported to be correctly fixed. Further, the bugs that GLAD fixes
require some degree of synthesis, suggesting GLAD met its orig-
inal mission.

Fault ID +C GLAD TBar DL GLAD-M RSTR Ever

C-4
C-9
C-26
Cl-1
Cl-5
Cl-15
Cl-18
Cl-33
Cl-38
Cl-52
Cl-57
Cl-104
Cl-113
Cl-118
Cl-125
Cl-128
Cl-130
L-9
L-24
L-33
L-39
L-45
M-25
M-28
M-48
M-94
Mo-8
Mo-22
Mo-24
Mo-34
Mo-38

Total

17
37
21
29
39
33
45
33
19
24
41
20
29
39
33
36
35
185
24
25
68
49
86
95
48
15
40
34
58
49
75

-

⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
⃝
×
⃝
⃝
⃝
×
⃝
⃝
⃝
⃝
⃝
×
⃝
⃝
⃝

28

⃝
⃝
⃝
×
×
×
⃝
×
⃝
×
×
×
×
×
△
×
×
×
⃝
⃝
⃝
×
×
△
×
△
×
×
×
×
⃝

9

×
⃝
⃝
×
×
×
⃝
⃝
⃝
×
⃝
⃝
×
⃝
×
×
×
×
×
⃝
×
×
×
×
×
⃝
⃝
×
×
×
⃝

△
△
⃝
⃝
⃝
⃝
×
⃝
⃝
⃝
△
⃝
×
⃝
⃝
⃝
⃝
⃝
×
⃝
×
⃝
⃝
△
△
×
×
⃝
⃝
⃝
×

×
×
⃝
×
⃝
×
⃝
⃝
×
×
×
×
⃝
⃝
⃝
⃝
⃝
×
×
⃝
×
×
×
×
×
×
×
×
×
×
×

12

19

10

9
11
15
0
1
0
9
4
9
0
4
2
1
3
1
1
1
0
8
14
4
1
2
0
0
5
2
1
0
0
10

-

Table 4: Number of faults fixed by GLAD in Defects4J v2.0.
Baseline numbers marked by an asterisk are from Zhu et
al. [62], and are not directly comparable to those of GLAD.

Patch type TBar*

SimFix* Recoder* GLAD

Plausible
Correct

50
8

25
2

46
19

34
20

6.2 Generalization (RQ2)
The number of bugs that GLAD plausibly and correctly fixed within
the Defects4J v2.0 dataset are presented in Table 4. As shown, GLAD
continues to fix a significant number of bugs, even though previ-
ous work suggests bugs added in Defects4J v2.0 are more difficult
to repair [62]. While GLAD is shown to correctly fix the largest
number of bugs in the table above, these numbers are not directly
comparable as Zhu et al. [62] used GZoltar-based FL for APR evalu-
ation, which were not replicated in this study for consistency with
RQ1 and due to the fact that GZoltar failed on certain Defects4J
v2.0 bugs. Nonetheless, it is noteworthy that, while Zhu et al. [62]
report the performance of TBar and SimFix dropping significantly
when going from Defects4J v1.2 to v2.0, the performance of GLAD is
roughly consistent. As such, we would like to emphasize that GLAD
continues to successfully generate patches with complex expres-
sions in different projects: for example, GLAD generates the cor-
rect predicate (l<Integer.MIN_VALUE||l>Integer.MAX_VALUE)
for the bug Compress-46, despite the fact that the exact predicate
form never appears within the project.

Answer to RQ2: GLAD successfully generalizes to new bugs in-
troduced in Defects4J v2.0, showing its versatility and generality
as a repair tool.

generally fix bugs that add expressions to an already existing state-
ment (weak omission faults), or common cases such as null checks.
This trend continues when GLAD is evaluated without the exact
fix location (GLAD-M) under the method-given setting against Re-
store (RSTR); GLAD fixes 11 bugs correctly that Restore could not
under equivalent settings, suggesting that GLAD is indeed capable
of fixing a unique set of bugs. Even when compared with all 40 APR
tools we are aware of (Ever), overall GLAD fixes eight bugs for the
first time, underscoring the uniqueness of GLAD-generated solu-
tions. This is because these bugs often require complex expressions
to be synthesized (as is qualitatively explored in RQ5), which are
uniquely handled by GLAD. Finally, note the bugs that GLAD fixes

6.3 Component Ablation Study (RQ3)
The performance of each ablated model is presented in Table 5.
The Plaus. and Corr. columns represent the number of faults fixed
plausibly and correctly. The time presented on the Time column cor-
responds to the median time (i) finetuning the model on the target
project (Figure 2(a)), (ii) extracting the legal tokens and construct-
ing the grammar (Figure 2(c)), and (iii) performing beam search
(Figure 2(d)).

We find that all components that directly affect how the patches
are made contribute to repair performance. First, without finetun-
ing, the model generates a similar number of plausible patches,
but the total number of faults correctly fixed drops by nine. These

GLAD: Neural Predicate Synthesis to Repair Omission Faults

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Table 5: Faults fixed by GLAD with ablated models.

Method

Plaus. Corr. Time

GLAD
-Finetuning
-Grammar
-Language Model

75
71
48
13

48
39
21
1

-
34s
4s
69s

bugs generally involve tokens that are not within the pretrain-
ing dataset, indicating that finetuning indeed helps the language
model pick up within-project lexical patterns. It is worth noting,
however, that the performance drop is less severe than the other
ablated models; thus when using a larger LM that is too expen-
sive to retrain, it may be acceptable to use it without finetuning.
Without grammar, the number of correctly fixed faults drops by
about half, while the number of plausible patches drops by about
a third. As the LM is unaware of what expressions are acceptable
within the given project, its capabilities drop significantly, often
failing to generate necessary but uncommon tokens. This suggests
that LMs, and perhaps learning-based techniques in general, would
greatly benefit from project-specific static information that can
easily be acquired: incorporating such information may improve
the performance drastically at a low computational cost. Without
the LM (essentially random sampling from grammar-conforming
expressions), the number of plausibly fixed faults drops by 83% to
only 13 faults, while only one bug is correctly fixed. Thus, while
our grammar is powerful enough to fix some bugs with weak test
suites, the grammar alone does not lead to strong performance, and
guidance from the LM is necessary. Overall, these results show that
the synergy between the LM and the grammar enables GLAD to
generate complex expressions that fix previously unfixable faults.

Table 6: Ranking performance of GLAD regarding the cor-
rect patch with and without dynamic reranking (DR). MRR
is Mean Reciprocal Rank, acc@n denotes the correct patches
found within the top 𝑛 patches, Max. shows the worst rank-
ing of the correct patch over all faults correctly fixed, and
Time shows the execution time of the validity pass and rank-
ing pass separately.

MRR acc@1

acc@5 Max.

Time

GLAD 0.449
-DR 0.138

18
5

25
10

745
28321

13.5+119s
-

Meanwhile, Table 6 shows how our dynamic ranking enhances
the efficiency of GLAD. The verification effort needed to find the
correct expression decreases significantly; acc@1 and acc@5 in-
crease from 5 and 10 to 18 and 25, respectively. Importantly, thanks
to the dynamic ranking, the worst cases get substantially more
manageable. In the most drastic example, the correct patch would
have been placed at 28,321th place to be validated, but dynamic
ranking moves it up to the third place, reducing the necessary effort
by 4 degrees of magnitude and saving tens of hours in verification
time. This improvement, which allows GLAD to be time-efficient
as demonstrated in RQ4, is obtained by investing a little over two

minutes: 13.5 seconds on the quick validity pass, and two minutes
on dynamic ranking. Overall, dynamic ranking also plays an im-
portant role in making the repair cost of GLAD feasible even as it
generates unique expressions.

Answer to RQ3: Each component of GLAD is essential, con-
tributing independently to the final performance of the model
in a manner that is expected by their roles.

Table 7: Statistics regarding time (in minutes) until the first
plausible patch is generated using GLAD.

Repair type Min. Med. Mean

Perfect FL + GLAD 1.72
Perfect FL + CURE [23]
>2.5
Method-given + GLAD 3.08
1.1

Method-given + Restore [58]

9.38
-
18.59
10.4

17.76
16.5
38.65
38.25

6.4 Runtime (RQ4)
While the timeout given to GLAD is three hours per bug, we find
that the actual time taken to generate and verify a patch is much
shorter in general. The runtime required to fix a bug, along with
comparable baselines, is provided in Table 7. (The minimum fix
time of CURE is the amount of time they state it takes to generate
patches using beam search.) Looking at the Mean column, we find
that GLAD is capable of generating plausible patches in roughly
the same amount of time when compared to baselines under both
FL settings; indeed, even without the exact fault location, GLAD
can generate patches for more than half the bugs it plausibly fixed
in less than 20 minutes. The bugs fixed in this short time include
relatively complex predicates, as is shown in RQ5. These results
suggest that (i) GLAD is indeed effective at fixing complex bugs,
and that (ii) GLAD would be effective as part of an APR ensemble
to run when other APR tools fail.

Answer to RQ4: GLAD generates most of its correct patches in
a short time, suggesting GLAD is efficient when repairing bugs.

6.5 Qualitative Analysis (RQ5)
We look at examples of successful and unsuccessful patches gener-
ated by GLAD to answer RQ5.

Table 8: Correct patches for bugs only fixed by GLAD. -dev
represents the developer patch for the corresponding bug.

Patch ID

Patch Content

Cl-52-dev
Cl-52-GLAD
Mo-24-dev

Mo-24-GLAD

Mo-34-dev
Mo-34-GLAD

Append &&s.charAt(0)!=‘0’ to return len>0
Add if(len>0&&s.charAt(0)==‘0’) {return false;}
Add if(invocation.getMock() ==
invocation.getArguments()[0]) {return 0;}
Add if(invocation.getMock() ==
invocation.getArguments()[0]) {return 0;}
Append &&i.getArguments().length>k to condition
Add if(k==i.getArgumentsCount()) {break;}

Successful Cases. Correct patches that only GLAD can gener-
ate are presented in Table 8. All such bugs require the synthesis of

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

a relatively long predicate, which makes them fundamentally chal-
lenging for existing template-based and learning-based techniques.
Due to the large number of operators and identifiers/constants re-
quired to generate correct expressions, they pose a challenge to
constraint-based techniques as well, as a consequence of combinato-
rial space explosion. Regarding each fault, Closure-52 compares the
value of a method call with an argument to a character; to the best
of our knowledge, no learning-based technique has ever synthe-
sized patches requiring characters or string literals. For Mockito-24,
the patch requires a complex expression that involves the com-
parison of the results of two distinct method calls. Further, the
return type of invocation.getArguments() is an array, and thus
the patch needs to access the array element to make the correct
predicate. The fact that GLAD can successfully generate this expres-
sion is a testament to how grammar can help the language model
during synthesis. Finally, in Mockito-34, GLAD finds the method
getArgumentsCount(), which is more concise than the developer-
written expression getArguments().length while being seman-
tically equivalent; indeed, the LM prioritized using the available
method. Again, due to the relatively large number of distinct identi-
fiers involved in the expression (i, k, getArgumentsCount), which
also need to be appropriately matched using correct operators to
generate the correct expression, such a bug requires precise expres-
sion synthesis to be fixed.

As far as we know, these bugs are challenging for existing APR
tools to fix even with a large computational budget, due to their com-
plexity and components; yet, these complex bugs are fixed by GLAD
in less than thirty minutes. As such, these results demonstrate that
GLAD makes a significant contribution to the state-of-the-art in
APR.

Table 9: Incorrect patches generated by GLAD.

Patch ID

Patch Content

M-106-
dev
M-106-
GLAD
T-27-
dev
T-27-
GLAD

Add if(num.intValue() < 0) {
pos.setIndex(initialIndex); return null; }
Add if(num.intValue() < 0) { return null; }

Wrap block with if(sep.iAfterParser==null &&
sep.iAfterPrinter==null)
Add if(size <= sep.getClass().getInterfaces().length )
{ return null; }

Failing Cases. We present informative cases in which GLAD
fails to generate the correct patch, shown in Table 9. In Math-106,
we find that GLAD has actually generated the correct predicate
and return statement. Unfortunately, the patch is not correct, as the
statement pos.setIndex(initialIndex); was not added to the
body of the if statement. This is in fact a rather common situation:
we find that GLAD generates the correct predicate for 32 bugs that
it did not fix correctly. Thus along with the 48 bugs it fixes correctly,
GLAD generates the correct predicate for 80 bugs. This suggests
that an even greater number of omission faults could be solved by
further research into body synthesis; we hope to investigate this
prospect in follow-up research.

Regarding Time-27, note that the predicate requires synthesiz-
ing specific field accesses, such as ‘sep.iAfterPrinter’. This is
difficult for the language model, as (i) the token ‘iAfterPrinter’

never appears in the LM training data, and (ii) the token is never
accessed as a field within the Time project except in this instance.
Thus the LM assigns a low probability to the ‘sep.iAfterPrinter’
sequence, causing beam search to drop the candidate. Note that this
is a weakness that many other techniques share. For example, tech-
niques that use code snippets from other parts of the project [33, 58]
would equally struggle here due to the lack of precedents.

Answer to RQ5: A qualitative inspection reveals GLAD is capa-
ble of synthesizing expressions that other techniques would have
a great deal of difficulty doing so. Meanwhile, GLAD struggles
when there is little lexical precedent for a predicate, as many
other repair tools do.

7 THREATS TO VALIDITY
Internal validity relates to whether the experiments performed
in the study have precluded other possible explanations. In RQ1, we
compare with a large body of existing APR techniques to find that
GLAD can fix multiple faults that were not fixed by the community
despite multiple attempts by 40 different tools, suggesting that
GLAD is capable of fixing omission faults that pose a challenge to
other techniques. In RQ3, we establish that each component has a
significant effect on the final performance, minimizing the effects
from randomness.
External validity relates to whether the results of this work will
generalize to different subjects. We find that GLAD can fix patches
over multiple repositories written in Java: its performance was con-
sistent under similar settings when applied to different projects,
as demonstrated in RQ1/RQ2. Such results suggest GLAD can suc-
cessfully repair over a wide range of projects. Nonetheless, further
research is necessary to understand how far the performance of
GLAD generalizes. While the principles of GLAD are not derived
from any characteristics of the Java language itself, and as such it is
possible to implement GLAD on other programming languages, the
performance of GLAD under such conditions is currently unknown.

8 CONCLUSION
In this work, we analyze existing APR literature to identify which
faults APR tools should tackle next. Our results indicate that many
of the unfixed faults involve omission faults, and a sizable portion
of those omission faults are if-statement omission faults. With this
in mind, we propose GLAD, an APR technique that, given a fault
location, uses language models, a debugger, and grammar-based
beam search to synthesize and rank natural predicate candidates.
We find that GLAD is highly orthogonal to existing work, fixing 16
faults never fixed by previous learning-based tools, and fixing eight
faults no other tool has fixed. The performance is consistent even
when evaluated over a diverse set of projects, underscoring the
versatility of GLAD, while fixing bugs often takes less than thirty
minutes. We thus verify the utility of GLAD as a repair tool that
is effective at fixing bugs within its domain; the orthogonality of
GLAD makes it a good candidate for an ensemble of APR tools to
deploy as well. Our results invite further investigations into both
the capability of language models, and how to harness them to
achieve strong repair performance.

GLAD: Neural Predicate Synthesis to Repair Omission Faults

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

REFERENCES
[1] Rui Abreu, Peter Zoeteweij, and Arjan J.C. van Gemund. 2007. On the Accuracy of
Spectrum-based Fault Localization. In Testing: Academic and Industrial Conference
Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007). 89–
98. https://doi.org/10.1109/TAIC.PART.2007.13

[2] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations. https://openreview.net/forum?id=H1gKYo09tX
[3] R. A. Assi, Chadi Trad, Marwan Maalouf, and W. Masri. 2019. Coincidental

correctness in the Defects4J benchmark. Software Testing 29 (2019), n/a.

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural In-
formation Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901. https://proceedings.
neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[5] Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray.
2020. CODIT: Code Editing with Tree-Based Neural Models. IEEE Transactions
on Software Engineering (2020), 1–1. https://doi.org/10.1109/tse.2020.3020502

[6] Liushan Chen, Yu Pei, and Carlo A. Furia. 2017. Contract-based program repair
without the contracts. In 2017 32nd IEEE/ACM International Conference on Auto-
mated Software Engineering (ASE). 637–647. https://doi.org/10.1109/ASE.2017.
8115674

[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-
tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. arXiv:2107.03374 [cs.LG]

[8] Zimin Chen, Steve Kommrusch, Michele Tufano, L. Pouchet, D. Poshyvanyk,
and Monperrus Martin. 2019. SequenceR: Sequence-to-Sequence Learning for
End-to-End Program Repair. ArXiv abs/1901.01808 (2019).

[9] Kyunghyun Cho, B. V. Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Repre-
sentations using RNN Encoder-Decoder for Statistical Machine Translation. In
EMNLP.

[10] J. Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical
Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. ArXiv
abs/1412.3555 (2014).

[11] Benoit Cornu, Thomas Durieux, Lionel Seinturier, and Martin Monperrus. 2015.
NPEFix: Automatic Runtime Repair of Null Pointer Exceptions in Java. CoRR
abs/1512.07423 (2015). arXiv:1512.07423 http://arxiv.org/abs/1512.07423
[12] Favio DeMarco, Jifeng Xuan, Daniel Le Berre, and Martin Monperrus. 2014. Auto-
matic Repair of Buggy If Conditions and Missing Preconditions with SMT. In Pro-
ceedings of the 6th International Workshop on Constraints in Software Testing, Verifi-
cation, and Analysis (Hyderabad, India) (CSTVA 2014). Association for Computing
Machinery, New York, NY, USA, 30–39. https://doi.org/10.1145/2593735.2593740
[13] Thomas Durieux and Martin Monperrus. 2016. DynaMoth: Dynamic Code Synthe-
sis for Automatic Program Repair. In 2016 IEEE/ACM 11th International Workshop
in Automation of Software Test (AST). 85–91. https://doi.org/10.1109/AST.2016.021
[14] Joel Galenson, Philip Reames, Rastislav Bodik, Björn Hartmann, and Koushik
Sen. 2014. CodeHint: Dynamic and Interactive Synthesis of Code Snippets. In
Proceedings of the 36th International Conference on Software Engineering (Hyder-
abad, India) (ICSE 2014). Association for Computing Machinery, New York, NY,
USA, 653–663. https://doi.org/10.1145/2568225.2568250

[15] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019. Automatic Software
Repair: A Survey. IEEE Transactions on Software Engineering 45, 1 (2019), 34–67.
https://doi.org/10.1109/TSE.2017.2755013

[16] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical Program
Repair via Bytecode Mutation. In Proceedings of the 28th ACM SIGSOFT In-
ternational Symposium on Software Testing and Analysis (Beijing, China) (IS-
STA 2019). Association for Computing Machinery, New York, NY, USA, 19–30.
https://doi.org/10.1145/3293882.3330559

[17] Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to Generate
Corrective Patches using Neural Machine Translation. ArXiv abs/1812.07170
(2018).

[18] Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to Generate
Corrective Patches using Neural Machine Translation. CoRR abs/1812.07170
(2018). arXiv:1812.07170 http://arxiv.org/abs/1812.07170

[19] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering (Zurich, Switzerland) (ICSE ’12). IEEE Press,
837–847.

[20] Jinru Hua, Mengshi Zhang, Kaiyuan Wang, and Sarfraz Khurshid. 2018. SketchFix:
A Tool for Automated Program Repair Approach Using Lazy Candidate Gener-
ation. In Proceedings of the 2018 26th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering
(Lake Buena Vista, FL, USA) (ESEC/FSE 2018). Association for Computing Ma-
chinery, New York, NY, USA, 888–891. https://doi.org/10.1145/3236024.3264600
[21] Jiajun Jiang, Luyao Ren, Yingfei Xiong, and Lingming Zhang. 2019. Inferring
Program Transformations From Singular Examples via Big Code. In 2019 34th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
255–266. https://doi.org/10.1109/ASE.2019.00033

[22] J. Jiang, Yingfei Xiong, H. Zhang, Q. Gao, and X. Chen. 2018. Shaping program
repair space with existing patches and similar code. Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis (2018).
[23] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural
Machine Translation for Automatic Program Repair. In 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE). 1161–1173. https://doi.
org/10.1109/ICSE43902.2021.00107

[24] René Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Database
of Existing Faults to Enable Controlled Testing Studies for Java Programs. In
Proceedings of the 2014 International Symposium on Software Testing and Analysis
(San Jose, CA, USA) (ISSTA 2014). Association for Computing Machinery, New
York, NY, USA, 437–440. https://doi.org/10.1145/2610384.2628055

[25] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary Models for
Source Code. In Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering (Seoul, South Korea) (ICSE ’20). Association for Computing
Machinery, New York, NY, USA, 1073–1085. https://doi.org/10.1145/3377811.
3380342

[26] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
patch generation learned from human-written patches. In 2013 35th International
Conference on Software Engineering (ICSE). 802–811. https://doi.org/10.1109/
ICSE.2013.6606626

[27] Jindae Kim and Sunghun Kim. 2019. Automatic patch generation with context-
based change application. Empirical Software Engineering 24 (12 2019). https:
//doi.org/10.1007/s10664-019-09742-5

[28] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Pre-
diction by Feeding Trees to Transformers. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE). 150–162. https://doi.org/10.1109/
ICSE43902.2021.00026

[29] A. Koyuncu, K. Liu, Tegawendé F. Bissyandé, D. Kim, J. Klein, Monperrus Martin,
and Y. Le Traon. 2020. FixMiner: Mining relevant fix patterns for automated
program repair. Empirical Software Engineering 25 (2020), 1980–2024.

[30] Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim, Martin Monper-
rus, Jacques Klein, and Yves Le Traon. 2019. IFixR: Bug Report Driven Program
Repair. In Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineer-
ing (Tallinn, Estonia) (ESEC/FSE 2019). Association for Computing Machinery,
New York, NY, USA, 314–325. https://doi.org/10.1145/3338906.3338935

[31] Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: Syntax- and Semantic-Guided Repair Synthesis via Programming
by Examples. In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering (Paderborn, Germany) (ESEC/FSE 2017). Association for
Computing Machinery, New York, NY, USA, 593–604. https://doi.org/10.1145/
3106237.3106309

[32] Xuan-Bach D. Le, D. Lo, and Claire Le Goues. 2016. History Driven Program
Repair. 2016 IEEE 23rd International Conference on Software Analysis, Evolution,
and Reengineering (SANER) 1 (2016), 213–224.

[33] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer. 2012. GenProg: A Generic
Method for Automatic Software Repair. IEEE Transactions on Software Engineering
38, 1 (2012), 54–72. https://doi.org/10.1109/TSE.2011.104

[34] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based Code
Transformation Learning for Automated Program Repair. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering (Seoul, South
Korea) (ICSE ’20). Association for Computing Machinery, New York, NY, USA,
602–614. https://doi.org/10.1145/3377811.3380345

[35] Kui Liu, Anil Koyuncu, Tegawendé F. Bissyandé, Dongsun Kim, Jacques Klein,
and Yves Le Traon. 2019. You Cannot Fix What You Cannot Find! An Investigation
of Fault Localization Bias in Benchmarking Automated Program Repair Systems.
In 2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST).
102–113. https://doi.org/10.1109/ICST.2019.00020

ISSTA 2022, 18-22 July, 2022, Daejeon, South Korea

Kang and Yoo

Program Repair. Association for Computing Machinery, New York, NY, USA,
354–366. https://doi.org/10.1145/3468264.3468600

[56] Qi Xin and Steven P. Reiss. 2017. Leveraging syntax-related code for automated
program repair. In 2017 32nd IEEE/ACM International Conference on Automated
Software Engineering (ASE). 660–670. https://doi.org/10.1109/ASE.2017.8115676
[57] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu
Zhang. 2017. Precise Condition Synthesis for Program Repair. In Proceedings of the
39th International Conference on Software Engineering (Buenos Aires, Argentina)
(ICSE ’17). IEEE Press, 416–426. https://doi.org/10.1109/ICSE.2017.45

[58] T. Xu, L. Chen, Y. Pei, T. Zhang, M. Pan, and C. Furia. 5555. Restore: Retrospective
Fault Localization Enhancing Automated Program Repair.
IEEE Transactions
on Software Engineering 01 (apr 5555), 1–1. https://doi.org/10.1109/TSE.2020.
2987862

[59] Xuezheng Xu, Yulei Sui, Hua Yan, and Jingling Xue. 2019. VFix: Value-Flow-
Guided Precise Program Repair for Null Pointer Dereferences. In Proceedings
of the 41st International Conference on Software Engineering (Montreal, Quebec,
Canada) (ICSE ’19). IEEE Press, 512–523. https://doi.org/10.1109/ICSE.2019.00063
[60] Yuan Yuan and Wolfgang Banzhaf. 2020. Toward Better Evolutionary Program
Repair: An Integrated Approach. 29, 1, Article 5 (Jan. 2020), 53 pages. https:
//doi.org/10.1145/3360004

[61] Andreas Zeller. 2002. Isolating Cause-Effect Chains from Computer Programs.
In Proceedings of the 10th ACM SIGSOFT Symposium on Foundations of Software
Engineering (Charleston, South Carolina, USA) (SIGSOFT ’02/FSE-10). Association
for Computing Machinery, New York, NY, USA, 1–10. https://doi.org/10.1145/
587051.587053

[62] Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,
and Lu Zhang. 2021. A Syntax-Guided Edit Decoder for Neural Program Repair.
Association for Computing Machinery, New York, NY, USA, 341–353. https:
//doi.org/10.1145/3468264.3468544

[36] K. Liu, A. Koyuncu, D. Kim, and Tegawendé F. Bissyandé. 2019. AVATAR: Fixing
Semantic Bugs with Fix Patterns of Static Analysis Violations. 2019 IEEE 26th In-
ternational Conference on Software Analysis, Evolution and Reengineering (SANER)
(2019), 1–12.

[37] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F. Bissyandé. 2019. TBar:
Revisiting Template-Based Automated Program Repair (ISSTA 2019). Association
for Computing Machinery, New York, NY, USA, 31–42. https://doi.org/10.1145/
3293882.3330577

[38] Kui Liu, Anil Koyuncu, Kisub Kim, Dongsun Kim, and Tegawendé F. Bissyandé.
2018. LSRepair: Live Search of Fix Ingredients for Automated Program Repair. In
2018 25th Asia-Pacific Software Engineering Conference (APSEC). 658–662. https:
//doi.org/10.1109/APSEC.2018.00085

[39] Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawendé F. Bissyandé,
Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, and Yves Le Traon. 2020.
On the Efficiency of Test Suite Based Program Repair: A Systematic Assessment of
16 Automated Repair Systems for Java Programs. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering (Seoul, South Korea) (ICSE
’20). Association for Computing Machinery, New York, NY, USA, 615–627. https:
//doi.org/10.1145/3377811.3380338

[40] Xuliang Liu and Hao Zhong. 2018. Mining stackoverflow for program repair.
In 2018 IEEE 25th International Conference on Software Analysis, Evolution and
Reengineering (SANER). 118–129. https://doi.org/10.1109/SANER.2018.8330202
[41] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and
Lin Tan. 2020. CoCoNuT: Combining Context-Aware Neural Translation Models
Using Ensemble for Program Repair. In Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis (Virtual Event, USA)
(ISSTA 2020). Association for Computing Machinery, New York, NY, USA, 101–114.
https://doi.org/10.1145/3395363.3397369

[42] Matias Martinez and Martin Monperrus. 2016. ASTOR: A Program Repair
Library for Java (Demo). In Proceedings of the 25th International Symposium
on Software Testing and Analysis (Saarbrücken, Germany) (ISSTA 2016). As-
sociation for Computing Machinery, New York, NY, USA, 441–444.
https:
//doi.org/10.1145/2931037.2948705

[43] Matias Martinez and Martin Monperrus. 2018. Ultra-Large Repair Search Space
with Automatically Mined Templates: The Cardumen Mode of Astor: 10th Interna-
tional Symposium, SSBSE 2018, Montpellier, France, September 8-9, 2018, Proceedings.
65–86. https://doi.org/10.1007/978-3-319-99241-9_3

[44] E. Mashhadi and H. Hemmati. 2021. Applying CodeBERT for Automated Program
Repair of Java Simple Bugs. In 2021 2021 IEEE/ACM 18th International Confer-
ence on Mining Software Repositories (MSR) (MSR). IEEE Computer Society, Los
Alamitos, CA, USA, 505–509. https://doi.org/10.1109/MSR52588.2021.00063
[45] Martin Monperrus. 2020. The Living Review on Automated Program Repair.
(Dec. 2020). https://hal.archives-ouvertes.fr/hal-01956501 working paper or
preprint.

[46] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-

proving language understanding by generative pre-training. (2018).

[47] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli, and Premkumar Devanbu. 2016. On the "Naturalness" of Buggy Code.
In Proceedings of the 38th International Conference on Software Engineering (Austin,
Texas) (ICSE ’16). Association for Computing Machinery, New York, NY, USA,
428–439. https://doi.org/10.1145/2884781.2884848

[48] Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R. Prasad. 2017. Elixir:
Effective object-oriented program repair. In 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE). 648–659. https://doi.org/
10.1109/ASE.2017.8115675

[49] Seemanta Saha, R. Saha, and M. Prasad. 2019. Harnessing Evolution for Multi-
Hunk Program Repair. 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE) (2019), 13–24.

[50] Victor Sobreira, Thomas Durieux, Fernanda Madeiral, Martin Monperrus, and
Marcelo A. Maia. 2018. Dissection of a Bug Dataset: Anatomy of 395 Patches
from Defects4J. In Proceedings of SANER.

[51] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence

Learning with Neural Networks. In NIPS.

[52] Weichao Wang, Zhaopeng Meng, Zan Wang, Shuang Liu, and Jianye Hao. 2019.
LoopFix: an approach to automatic repair of buggy loops. Journal of Systems and
Software 156 (2019), 100–112. https://doi.org/10.1016/j.jss.2019.06.076

[53] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.
Context-Aware Patch Generation for Better Automated Program Repair. In Pro-
ceedings of the 40th International Conference on Software Engineering (Gothenburg,
Sweden) (ICSE ’18). Association for Computing Machinery, New York, NY, USA,
1–11. https://doi.org/10.1145/3180155.3180233

[54] Martin White, Michele Tufano, Matías Martínez, Martin Monperrus, and Denys
Poshyvanyk. 2019. Sorting and Transforming Program Repair Ingredients via
Deep Learning Code Similarities. In 2019 IEEE 26th International Conference
on Software Analysis, Evolution and Reengineering (SANER). 479–490. https:
//doi.org/10.1109/SANER.2019.8668043

[55] Chu-Pan Wong, Priscila Santiesteban, Christian Kästner, and Claire Le Goues.
2021. VarFix: Balancing Edit Expressiveness and Search Effectiveness in Automated

