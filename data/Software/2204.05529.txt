Forecasting SQL Query Cost at Twitter

Chunxu Tang, Beinan Wang, Zhenxiao Luo, Huijun Wu, Shajan Dasan, Maosong Fu, Yao Li, Mainak Ghosh,
Ruchin Kabra, Nikhil Kantibhai Navadiya, Da Cheng, Fred Dai, Vrushali Channapattan, and Prachi Mishra
Twitter, Inc.
San Francisco, USA
{chunxut, beinanw, zluo, huijunw, sdasan, mfu, yaoli, mghosh, rkabra, nnavadiya, dac, fdai, vrushali, prachim}@twitter.com

2
2
0
2

r
p
A
2
1

]

B
D
.
s
c
[

1
v
9
2
5
5
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—With the advent of the Big Data era, it is usually
computationally expensive to calculate the resource usages of a
SQL query with traditional DBMS approaches. Can we estimate
the cost of each query more efﬁciently without any computation
in a SQL engine kernel? Can machine learning techniques help
to estimate SQL query resource utilization? The answers are yes.
We propose a SQL query cost predictor service, which employs
machine learning techniques to train models from historical
query request logs and rapidly forecasts the CPU and memory
resource usages of online queries without any computation in a
SQL engine. At Twitter, infrastructure engineers are maintaining
a large-scale SQL federation system across on-premises and cloud
data centers for serving ad-hoc queries. The proposed service can
help to improve query scheduling by relieving the issue of imbal-
anced online analytical processing (OLAP) workloads in the SQL
engine clusters. It can also assist in enabling preemptive scaling.
Additionally, the proposed approach uses plain SQL statements
for the model training and online prediction, indicating it is both
hardware and software-agnostic. The method can be generalized
to broader SQL systems and heterogeneous environments. The
models can achieve 97.9% accuracy for CPU usage prediction
and 97% accuracy for memory usage prediction.
Index Terms—sql, database, machine learning

I. INTRODUCTION

In the data platform at Twitter,

there is a large effort
in pursuing high scalability and availability to fulﬁll
the
increasing needs for data analytics on the sea of data. Twitter
runs multiple large Hadoop clusters with over 300PB of data
[1], which are among the biggest in the world. To overcome
the performance issues in developing and maintaining SQL
systems with increasing volumes of data, we designed a
large-scale SQL federation system across on-premises and
cloud Hadoop clusters, paving the path for democratizing
data analytics and improving productivity at Twitter. The SQL
federation system is processing around 10PB of data daily.

During the daily operation of large-scale SQL systems, we
found that the lack of forecasting SQL query resource usages
is problematic. First of all, data system customers would like to
know the resource consumption estimation of their queries. We
received complaints that they wasted a considerable amount
of time waiting for ad-hoc queries to complete but ﬁnally
canceled them in frustration1. We found that there is a cor-
relation between the CPU time and wall clock time of ad-hoc
queries in Twitter OLAP workload. Once the query resource
is predicted, users can know approximately how long they will

1From an analysis of query request logs in a three-month session, 16.2%

of uncompleted queries are related to user cancellation.

wait. Second, according to the convoy effect, query scheduling
requires an estimate of the immediate workload in the SQL
system. Without proper query scheduling, the cluster can be
overwhelmed because of resource-consuming queries which
can easily occupy most resources in a cluster in as short as 10s.
Third, elastic scaling needs query resource usage forecasting.
Due to the rapid impact of resource-consuming queries, a SQL
system has to scale ahead of the actual processing of these
queries.

A query cost prediction system can provide the following

beneﬁts for large-scale SQL systems:

• Rapid estimate of CPU and memory usage of a
query for customers. They will have an intuitive idea
of resources a query may consume and an anticipated
billing associated.

• Improved query scheduling. With the forecasted re-
source usage of a query, we can differentiate whether
this is a resource-consuming query or a lightweight query.
This will help balance the workloads on different clusters,
before executing any queries in a SQL engine kernel.

• Enabling

preemptive

scaling. Because

resource-
intensive queries can occupy most resources of a cluster
in a short time, the prediction of resource usages can help
to trigger preemptive scaling if it alerts that the immediate
workload may overwhelm a preset conﬁgurable threshold.

As the query cost needs to be predicted before the query
is executed in a SQL engine, we do not leverage traditional
DBMS approaches such as the cost model [2]–[4]. By con-
trast, we propose to utilize machine learning techniques to
train two models from historical SQL query request logs for
CPU time and peak memory prediction. We categorize the
queries into various ranges according to their CPU time and
peak memory. Unlike some prior work [5]–[7] of generating
features from query plans, we extract features from plain SQL
statements. Our evaluation results show that the XGBoost
classiﬁer together with the TF-IDF vectorization achieves high
performance regarding accuracy as well as precision and recall
for each category.

This paper makes the following contributions:

1) Motivated by challenges that customers and engineers
are facing in a large-scale SQL system, we introduce
a SQL query cost prediction system for resource usage
estimation independent from SQL engines.

2) We harness supervised machine learning techniques to

 
 
 
 
 
 
forecast both CPU and memory resource usages of
individual SQL queries. As the proposed approach does
not have any speciﬁc hardware or software requirements,
it can be generalized to broader SQL systems and hetero-
geneous environments. Moreover, from our observation,
the P95 (95th percentile) of the query planning latency
in the SQL federation system is 9s, and the latency can
be as high as more than 30s. By contrast, the proposed
method only requires a constant time for the prediction
(around 200ms) for all types of SQL queries.

3) We deployed the trained models in the online production
environment for resource usage prediction. We observed
that the model accuracy may drop to around 92% in
four weeks and veriﬁed the concept drift. Moreover, we
implemented a quantitative analysis on the monitoring
results.

The remainder of this paper is organized as follows. We
discuss related work in Section II, describe the design of
the SQL federation system in Section III, demonstrate the
architecture of the query cost prediction system in Section
IV, explain the data preprocessing in Section V, and present
training and evaluation in Section VI. Section VIII concludes
the paper.

II. RELATED WORK

With the increasing volume of data, many distributed SQL
engines, targeting analyzing Big Data, emerged in the recent
decade. For example, Apache Hive [8] is a data warehouse
built on top of Hadoop, providing a SQL-like interface for
data querying. Spark SQL [9] is a module integrated with
Apache Spark, powering relational processing to Spark data
structures. Presto [10] is a distributed SQL engine, targeting
“SQL on everything”. It can query data from multiple sources
which is a major advantage. At the same time, there are
some commercial products such as Google BigQuery [11] (a
public implementation of Dremel [12], [13]), Amazon Redshift
[14], and Snowﬂake [15], acting as fully-managed cloud data
warehouses providing out-of-box data analytics experience.

Some prior work pioneers the study of resource prediction
and scaling in a database system. For example, Narayanan
et al. [16] proposed a Resource Advisor to answer “what-
if” questions about resource upgrades to predict the buffer
pool size on online transaction processing (OLTP) workloads.
Rogers et al. [17] described a “white-box” and a “black-box”
approach to build a resource provisioning framework on top
of an Infrastructure-As-A-Service cloud. Similarly, Das et al.
[18] presented an automated demand-driven resource scaling
approach by deriving a set of robust signals from the database
engine and combining them for better scaling.

Recently, some researchers began to utilize machine learn-
ing techniques to solve resource prediction problems. Some
work concentrated on optimized planning through identifying
and forecasting workload patterns. For example, Mozafari et
al. [19] developed the DBSeer system, employing statistical
models for resource and performance prediction. Ma et al.
[20] created a robust forecasting framework to predict the

expected arrival rate of queries based on historical data. They
studied Linear Regression, Recurrent Neural Network, and
Kernel Regression. Higginson et al. [21] utilized time series
analysis and supervised machine learning to identify traits for
database workload capacity planning.

Some other work focused on predicting metrics of a query,
such as query performance prediction, usually with the help
of query plans. For example, Gupta et al. [5] introduced a
classiﬁcation tree structure obtained from historical data of
queries with the query plan to predict the execution time of
a query. Ganapathi et al. [6] built machine learning models
with query plan feature vectors to predict metrics such as
elapsed time, disk I/Os, and message bytes. Akdere et al.
[7] evaluated predictive modeling techniques, ranging from
plan-level models to operator-level models. Marcus et al.
[22] introduced a plan-structured neural network to predict
query performance. Some work, such as [23], [24], and [25],
also extended the prediction to concurrent query performance
prediction.

The existing plan-based approaches rely on query planning
in SQL engines, indicating limitations under the scenario of
predicting query resource usages for query scheduling and
preemptive scaling when SQL engines are not involved. To
resolve this problem, we propose a machine learning approach
learning from plain SQL statements’ features to forecast both
CPU time and peak memory without dependency on any SQL
engines or query plans, fulﬁlling the requirements of query
scheduling and preemptive scaling.

III. SQL FEDERATION SYSTEM

Fig. 1: SQL federation system at Twitter.

In the SQL federation system shown in Figure 1, a query

processing ﬂow includes the following steps:

1) A client, such as a notebook or BI tool, sends a SQL

query to the router.

2) The router obtains the predicted resource usage of the

query from the query cost predictor.

3) Using both the projected resource usage and the cluster
statistics, the router determines a SQL engine cluster to
route the request.

4) The router sends the cluster’s URL endpoint, usually the
endpoint of the coordinator node in that cluster, back to
the client.

RouterCluster A: on premisesCluster B: cloudQuery Cost PredictionServing ClusterCluster StatisticsData StorageMetadataStorageNotebook/BI ToolsSQL Federation SystemFig. 2: The architectural design of the query cost prediction system.

5) The client sends the query to the speciﬁc cluster.
6) The cluster plans the query, fetches metadata from the
metadata storage, and scans necessary datasets from the
data storage.

7) Data results are aggregated and returned to the client.

The SQL federation system and peripherals contain the

following components:

Notebook/Business Intelligence (BI) tools. At Twitter, data
analysts and data scientists are employing various notebook
tools (e.g. Jupyter notebook and Apache Zeppelin) and BI
tools (e.g. Tableau and Looker) for data analytics and visual-
ization to gain insights from datasets. These tools send queries
to the SQL federation system to get the corresponding data
results.

SQL engine clusters. We utilize Presto as the core of
each SQL engine cluster. Meanwhile, Twitter engineering
is embarking on an effort to migrating ad-hoc clusters to
the Google Cloud Platform (GCP), aka the “Partly Cloudy”
[26]. Some of the SQL engine clusters have been migrated
to the GCP. For now, we are maintaining a hybrid-cloud
SQL federation system with both on-premises and cloud SQL
engine clusters.

Data storage. At Twitter,

the Hadoop Distributed File
System (HDFS) [27] is well-acknowledged and widely used.
Each SQL engine worker interacts with the HDFS for querying
data, which is usually stored in an Apache Parquet [28] format.
With the Partly Cloudy strategy, the data storage has also been
extended to the Google Cloud Storage (GCS).

Metadata storage [29]. The metadata storage is used to
provide metadata information, for example, how data ﬁles are
mapped to schemas and tables.

Router. The router sits between the clients and SQL engine
clusters, exposes a uniﬁed interface to the clients, hides cluster
conﬁguration details from the clients, and routes requests to
speciﬁc clusters. To more efﬁciently utilize the resources on
these clusters, the router also aids in balancing the workloads,
with the help of the query cost predictor. Besides collecting
real-time operational statistics such as P90 (90the percentile)
of query latency from SQL engine clusters, the router lever-

ages the prediction results from the query cost predictor to
improve query scheduling and enable preemptive scaling.

Query cost prediction serving cluster. This is the serving
cluster of the query cost prediction system. The predictor’s job
is to rapidly estimate the CPU and memory resources antici-
pated for a SQL query. Section IV discusses the architectural
design of this system.

IV. QUERY COST PREDICTION SYSTEM

Figure 2 illustrates the architectural design of the query cost

prediction system:

Request logs. The raw dataset

the query engine produces a request

is collected from query
request logs stored in the HDFS. For each SQL query sent
by a notebook/BI tool and processed in the SQL federation
log. Table I
system,
shows that each SQL request
log sample contains query-
related information including the unique identiﬁer, user name,
environment, query statement, etc. The logs in the recent three
months (90 days) are a good indicator for forecasting the
cost of online queries from our experiments. Such a typical
dataset consists of around 1.2 million records and more than
20 columns.

In Twitter’s data platform, most SQL statements in a typical
OLAP workload are SELECT statements, leveraged to query
datasets stored in various data sources such as HDFS and GCS.
Figure 3 illustrates the distribution of SQL statements based
on types. Besides writing SELECT statements, customers also
leverage CREATE statements to create temporary tables or
material views and OTHER statements mainly for metadata
querying. DELETE statements are rarely used.

Training cluster. The training cluster takes charge of the
computation with machine learning techniques. We train two
machine learning models-CPU model and memory model-
from historical request logs for CPU time and peak memory
prediction. First, we perform data cleaning and discretization
to the raw dataset, converting the continuous CPU time and
peak memory into buckets. Then, we apply vectorization
techniques to extract features from raw SQL statements and
employ classiﬁcation algorithms to obtain models.

Request LogsCPU TimePeak MemoryNotebook/BI Tools RouterCluster ACluster BSQL Federation SystemMemory ModelCPU ModelModel Training Data Cleaning &Discretization FeatureExtraction Training ClusterServing ClusterModel Repository??Query Cost Prediction SystemTABLE I: Samples of SQL query request logs. Sensitive information such as user names and concrete SQL statements were
hydrated and replaced. Some unrelated columns have been removed from the table.

query id
Uh0u6ScxuJ
HSJb3hSEe9
y2cysjWzKC
YqtRmXL8Gy
oMawUdJuHA
*: These are the columns utilized for model training.

cluster
cluster a
cluster b
cluster a
cluster a
cluster a

query*
sql1
sql2
sql3
sql4
sql5

user
alice
bob
bob
alice
charley

Fig. 3: Distribution of SQL statements in a typical Twitter’s
OLAP workload based on types.

Model repository. The model repository manages the mod-
els, including model storage and versioning. The models are
stored in a central repository such as GCS buckets.

Serving cluster. The serving cluster fetches models from
the model repository and wraps models into a web predictor
service, exposing RESTful APIs for external usages and
forecasting the CPU time and peak memory for online SQL
queries from the notebook/BI tools (for customers to have a
sense of the estimate of resource usages of their queries) and
the router (for query scheduling and preemptive scaling).

V. DATA PREPROCESSING

A. Data Cleaning and Discretization

Prior DBMS statistical approaches apply regression tech-
niques such as time series analysis to solve DBMS problems.
However, as the distribution of SQL query resource usage
follows a power law,
it poses challenges to conventional
regression approaches such as [30] and [31]. Moreover, we
noticed that the quick estimate of resource usages for cus-
tomers and the router for scheduling and scaling do not require
an accurately predicted value. For example, from our study, a
customer does not care much about whether the query will
cost 60 minutes or 61 minutes CPU time, but whether this
is a CPU-consuming query or a lightweight query that can
be completed in an acceptable range of wall clock time. For
query scheduling or scaling, we also do not need an accurate
value. Instead, we need to know whether a query is a low
resource-consuming query, a medium one, or a high one. This
judgment leads to the appliance of data discretization to the
raw dataset, transforming the continuous data to discrete data.
We also propose that this approach can be generalized to other
DBMS problems where an accurately predicted value can be
replaced by an approximate range.

Some prior work, such as [6] and [20], leverages clustering
to predict DBMS metrics. We tried applying the k-means

cpu time ms*
10143681
5903987
284392
53
179972

peak memory bytes*
1204117281
9038118972
1204117281
45056
118783230

datehour
2020021013
2020021411
2020021719
2020091516
2020110601

clustering algorithm, used in [20], to the dataset collected
from request logs in 3 months. Figure 4 shows an example
of categorizing 10,000 queries of peak memory and CPU
time in three clusters. However, we found that this clustering
approach did not work well. First of all, in query scheduling
and preemptive scaling, we usually plan the CPU and memory
resources separately instead of collectively. Moreover, we
found that the correlation between peak memory and CPU
time is only 0.256, indicating a query that consumes a large
amount of memory may only require a small chunk of CPU
time. But the algorithm tended to cluster queries with low CPU
time and those with low peak memory.

Fig. 4: Clustering of 10,000 samples of peak memory and
CPU time. Some samples with very high CPU time or peak
memory are removed from the ﬁgure.

How to group these queries? First of all, we select 5h and
1TB as boundaries for high CPU usage and memory usage
respectively, because these are prior thresholds applied in our
SQL federation system, which are obtained from our opera-
tional experience in running analytical queries. For the CPU
time, based on our DevOps experience, we consider queries
whose CPU time is less than 30s as lightweight queries. This
also helps us capture the large proportion of queries in the
range [0, 30s), which occupies more than 70% of queries. By
contrast, only 1% of queries fall in the range [30s, 1min).
For the peak memory, we found that the distribution tends to
be more evenly distributed. As a result, we generally evenly
categorize the queries whose peak memory is lower than 1TB
and select 1MB as the boundary for low and medium-memory-
consuming queries. In summary, we categorize the CPU time
into three ranges: [0, 30s), [30s, 5h), [5h, ); categorize the peak
memory into three ranges: [0, 1MB), [1MB, 1TB), [1TB, ).
Figure 5 shows the distribution of queries out of this partition.
As the queries are not evenly arranged especially for CPU
time usage, this may lead to an imbalanced classes issue in

020406080SELECTCREATEDELETEOTHERType of SQL statementsProportion (%)0100002000030000400005000002500005000007500001000000Peak memory (MB)CPU time (min)cluster123implies machine learning techniques can also help developers
to gain insights from large-scale SQL systems.

VI. MODEL TRAINING & EVALUATION

After preparing the features, we trained three types of
classiﬁers on the training dataset: Random Forest (RF), which
is an ensemble learning method; XGBoost, which is a tree-
based gradient boosting approach; and Logistic Regression,
which predicts probabilities of certain classes with a logistic
function. All of them are well-known for high computational
scalability and high interpretability. They have been widely
used for classiﬁcation tasks. The 3-fold cross-validation is used
to ﬁnd the optimal hyperparameters. From our experiments, a
typical training job on around 1.2 million query logs can be
completed in less than 5 hours in a machine with 8 CPU
cores and 64GB of memory. Then, we tested the trained
classiﬁers on the testing dataset. As the classes are imbalanced
and resource-consuming queries are of higher importance than
lightweight queries, we studied the precision and recall of each
class together with the overall accuracy for each of the CPU
and memory models. For the feature extraction, we compared
the word count approach and the TF-IDF approach. Table II
shows the comparison of these approaches.

From the evaluation results, the XGBoost model with the
TF-IDF approach outperforms other CPU models up to 3%
and other memory models up to 4% for the overall accuracy.
It achieves 97.9% accuracy for the CPU model and 97.0%
accuracy for the memory model. The Random Forest models
and XGBoost models have similar performance and are higher
than that of Logistic Regression models. Most TF-IDF-based
models have a little bit higher accuracy than their counterparts
based on word counts.

TABLE II: The model accuracy(%) of each classiﬁer.

Random Forest - Word Count
Random Forest - TF-IDF
XGBoost - Word Count
XGBoost - TF-IDF
Logistic Regression - Word Count
Logistic Regression - TF-IDF

CPU Memory
96.9
97.2
97.4
97.9
94.8
95.0

95.5
95.4
96.8
97.0
92.9
92.9

Accuracy is a popular metric to evaluate model perfor-
mance, but it is not the only one. In particular, considering the
imbalanced classes in the training dataset, shown in Figures 5a
and 5b, a high overall accuracy does not always indicate the
model is good at predicting all classes. A high capability of
classifying a class containing a dominant number of samples
can conceal the low accuracy of predicting classes with smaller
numbers of samples. To overcome the potential issue here,
we also considered the precision and recall of each class,
especially the classes representing CPU or memory-intensive
queries. In our work, precision is deﬁned as the proportion
of returned results that are relevant; recall (also known as
sensitivity) is deﬁned as the proportion of relevant cases that
are returned. Details of precision and recall of the XGBoost -
TD-IDF classiﬁer are shown in Tables III and IV.

(a) Distribution of queries in
CPU time ranges.

(b) Distribution of queries in
peak memory ranges.

Fig. 5: Distribution of queries.

the classiﬁcation which we need to pay attention to. More
evaluation details are discussed in Sections VI and VII. After
the dataset is transformed, we partition the dataset into a
training dataset (80%) and a testing dataset (20%).

B. Feature Extraction

For the transformed dataset with peak memory and CPU
time in categories, we apply vectorization techniques from
Natural Language Processing (NLP) to generate essential fea-
tures. Each SQL statement is mapped to a vector of numbers
for subsequent processing via vectorization, making it easier
for running classiﬁcation algorithms on text-based data. We
employ the bag-of-words (BoW) models, in which each word
is represented by a number such that a SQL statement can be
represented by a sequence of numbers. Word frequencies are
a typical representation. The term frequency-inverse document
frequency (TF-IDF) is another popular vectorization approach.
BoW models are known for their high ﬂexibility. They can be
generalized to a variety of text data domains.

Word embedding is another widely-used NLP approach
where words with similar meanings are represented by similar
vectors, usually through computing the joint probability of
words. In the word embedding, each word is mapped to a high-
dimensional vector, such that various deep learning algorithms
can be applied to solve NLP tasks. However, word embedding
brings challenges for domain-speciﬁc context [32], indicating
it may not ﬁt the context of Twitter SQL statements without
a pre-trained domain-speciﬁc embedding model. In our work,
considering there are extra training costs required for the word
embedding model, whereas our BoW-based models already
have high prediction accuracy and interpretability, we move
forward with the BoW-based models in our machine learning
pipeline.

BoW models are good at feature engineering for resource
usage prediction. They produce features without computation
in any SQL engines or communication with any metadata
stores. We do not use table-speciﬁc statistics for feature
engineering as this type of data requires additional costs for
analyzing SQL statements and fetching table-related metadata.
We also observed that with tree-based machine learning mod-
els where feature importance can easily be interpreted, some
SQL-related information such as the access to speciﬁc tables
and the limit of time ranges can be captured and reﬂected. This

0204060[0, 30s)[30s, 5h)[5h, )CPU time rangeProportion (%)010203040[0, 1MB)[1MB, 1TB)[1TB, )Peak memory bytes rangeProportion (%)TABLE III: The precision and recall for each class of the CPU
time model.

CPU time
[0, 30s)
[30s, 5h)
[5h, )

Precision
0.98
0.96
0.96

Recall
0.99
0.95
0.95

TABLE IV: The precision and recall for each class of the peak
memory model.

Peak memory
[0, 1MB)
[1MB, 1TB)
[1TB, )

Precision
0.98
0.92
0.97

Recall
0.99
0.91
0.98

From the tables, we can see that XGBoost achieves high
precision and recall for all classes besides the overall accuracy.
Particularly, it reaches no less than 0.95 of precision and recall
for resource-consuming queries: [5h, ) and [1TB, ). XGBoost
has the best performance, so we decided to use it in our online
environment. Moreover, Twitter has an internal distribution
of the open-sourced XGBoost JVM package, ﬁtting Twitter’s
technical stack. This is also an add-on advantage for making
this decision.

VII. MODEL SERVING & MONITORING

After the models are trained and tested, we encapsulate
the models into a web application for real-time serving. The
service, held in the serving cluster, is deployed in Aurora
containers on Mesos [33], which is the stack widely used
at Twitter. As each deployment unit is stateless, the appli-
cation’s scalability can be enhanced by increasing the number
of deployment replicas. The models are stored in a central
repository and fetched by the application when a deployment
unit starts. The service exposes two RESTful API endpoints
to forecast CPU time and peak memory of a SQL query. The
inference time is around 200ms.

When the models serve online requests, concept drift is
observed since unseen features emerge over time, deteriorating
the model performance. Hence, model monitoring and retrain-
ing are required under a given time granularity. From our
DevOps experience of the SQL federation system, the query
counts have high variance daily but are quite stable weekly. For
example, we usually observe many more queries on Mondays
than those on Fridays. Furthermore, a large number of queries
are scheduled to run weekly. To reduce the inﬂuence of outliers
or peaks in some speciﬁc workdays, we re-evaluate the online
models weekly.

Figure 6 illustrates the drifts of models in a real-time on-
line environment. Because resource-consuming queries weigh
more than lightweight queries, we also include the precision
and recall for resource-consuming ranges: [5h, ) and [1TB, ).
As time goes, the model metrics are gradually decreasing. For
example, at ﬁrst, the overall accuracy of the CPU model is as
high as around 98%. After four weeks, the accuracy shrank
to around 93%. The recall of the CPU time class [5h, ) has
dropped from 0.96 to 0.85. This is usually caused by unseen

features in the latest queries which are not captured by the
models we have trained. To mitigate the concept drift, we can
retrain the models when both the precision and recall for CPU
or memory-intensive queries are lower than 0.9. In Figure 6,
at week 3, the precision of the class [5h, ) is 0.88 and the
recall is 0.87. As both precision and recall are less than 0.9,
this will trigger retraining of the CPU model. To maintain the
consistency of training timestamps of the CPU and memory
models, we will also retrain and redeploy the memory model.
After the model retraining with the same hyperparameters, the
metrics will go back to a high level. This indicates our models
have a high adaptivity of new features.

(a) Changes of model accuracy.

(b) Changes of model preci-
sion for [5h, ) and [1TB, ).

(c) Changes of model recall for
[5h, ) and [1TB, ).

Fig. 6: Changes of model performance over time. RT: Re-
trained models.

We also observed that even though the CPU model accuracy
is higher than that of the memory model, the precision and
recall are lower and fall faster. For example, the precision of
both the CPU range [5h, ) and the memory range[1TB, ) is
higher than 0.95 initially. But after four weeks, the precision of
the CPU range has decayed to 0.86. By contrast, the precision
of the memory range is still 0.92. This is probably because the
setting of the CPU range [5h, ) is too high, causing the model
to fail to capture the features of CPU-consuming queries. As a
lesson learned from this study, we are considering tuning the
CPU range for this type of queries in the future.

VIII. CONCLUSION

In this paper, we introduced the SQL federation system in
the Twitter data platform and proposed a SQL query cost
prediction system. Unlike prior work, the proposed system
learns from plain SQL statements, builds machine learning
models from historical query request logs, and forecasts CPU
time and peak memory ranges. The evaluation shows that the
proposed system of the XGBoost classiﬁer with TF-IDF vec-
torization can achieve 97.9% accuracy for CPU time prediction
and 97% accuracy for memory usage prediction. We deployed

909294969801234WeekAccuracy(%)ModelCPUCPU-RTMemMem-RT0.800.850.900.9501234WeekPrecisionModelCPUCPU-RTMemMem-RT0.800.850.900.9501234WeekRecallModelCPUCPU-RTMemMem-RTthe models in the production environment and implemented a
quantitative analysis on the concept drift observed. This work
can help provide an estimate of resource usage to customers,
improve query scheduling, and enable preemptive scaling,
without dependency on SQL engines or query plans.

ACKNOWLEDGMENT

We would like to express our gratitude to everyone who
has served on Twitter’s Interactive Query team,
including
former team members Hao Luo and Yaliang Wang. We are
to Daniel Lipkin and Derek Lyon for their
also grateful
strategic vision, direction, and support to the team. Finally,
we thank Alyson Pavela, Julian Moore, and the anonymous
IC2E reviewers for their insightful suggestions that helped us
signiﬁcantly improve this paper.

REFERENCES

[1] P. Agrawal, “A new collaboration with Google Cloud,” 2018.
[Online]. Available: https://blog.twitter.com/engineering/en us/topics/
infrastructure/2018/a-new-collaboration-with-google-cloud.html

[2] W. Wu, Y. Chi, S. Zhu, J. Tatemura, H. Hacig¨um¨us, and J. F. Naughton,
“Predicting query execution time: Are optimizer cost models really unus-
able?” in 2013 IEEE 29th International Conference on Data Engineering
(ICDE).

IEEE, 2013, pp. 1081–1092.

[3] V. Leis, A. Gubichev, A. Mirchev, P. Boncz, A. Kemper, and T. Neu-
mann, “How good are query optimizers, really?” Proceedings of the
VLDB Endowment, vol. 9, no. 3, pp. 204–215, 2015.

[4] L. Baldacci and M. Golfarelli, “A cost model for Spark SQL,” IEEE
Transactions on Knowledge and Data Engineering, vol. 31, no. 5, pp.
819–832, 2018.

[5] C. Gupta, A. Mehta, and U. Dayal, “PQR: Predicting query execution
times for autonomous workload management,” in 2008 International
Conference on Autonomic Computing.

IEEE, 2008, pp. 13–22.

[6] A. Ganapathi, H. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. Jordan,
and D. Patterson, “Predicting multiple metrics for queries: Better deci-
sions enabled by machine learning,” in 2009 IEEE 25th International
Conference on Data Engineering.

IEEE, 2009, pp. 592–603.

[7] M. Akdere, U. Cetintemel, M. Riondato, E. Upfal, and S. B. Zdonik,
“Learning-based query performance modeling and prediction,” in 2012
IEEE 28th International Conference on Data Engineering.
IEEE, 2012,
pp. 390–401.

[8] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
P. Wyckoff, and R. Murthy, “Hive: A warehousing solution over a map-
reduce framework,” Proceedings of the VLDB Endowment, vol. 2, no. 2,
pp. 1626–1629, 2009.

[9] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley,
X. Meng, T. Kaftan, M. J. Franklin, A. Ghodsi et al., “Spark SQL:
Relational data processing in Spark,” in Proceedings of the 2015 ACM
SIGMOD international conference on management of data, 2015, pp.
1383–1394.

[10] R. Sethi, M. Traverso, D. Sundstrom, D. Phillips, W. Xie, Y. Sun,
N. Yegitbasi, H. Jin, E. Hwang, N. Shingte et al., “Presto: SQL on
everything,” in 2019 IEEE 35th International Conference on Data
Engineering (ICDE).

IEEE, 2019, pp. 1802–1813.

[11] K. Sato, “An inside look at Google BigQuery,” White paper, URL:
https://cloud. google. com/ﬁles/BigQueryTechnicalWP. pdf, 2012.
[12] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton,
and T. Vassilakis, “Dremel: Interactive analysis of web-scale datasets,”
Proceedings of the VLDB Endowment, vol. 3, no. 1-2, pp. 330–339,
2010.

[13] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton,
T. Vassilakis, H. Ahmadi, D. Delorey, S. Min et al., “Dremel: A decade
of interactive SQL analysis at web scale,” Proceedings of the VLDB
Endowment, vol. 13, no. 12, pp. 3461–3472, 2020.

[14] A. Gupta, D. Agarwal, D. Tan, J. Kulesza, R. Pathak, S. Stefani,
and V. Srinivasan, “Amazon Redshift and the case for simpler data
warehouses,” in Proceedings of the 2015 ACM SIGMOD international
conference on management of data, 2015, pp. 1917–1923.

[15] B. Dageville, T. Cruanes, M. Zukowski, V. Antonov, A. Avanes,
J. Bock, J. Claybaugh, D. Engovatov, M. Hentschel, J. Huang et al.,
“The Snowﬂake elastic data warehouse,” in Proceedings of the 2016
International Conference on Management of Data, 2016, pp. 215–226.
[16] D. Narayanan, E. Thereska, and A. Ailamaki, “Continuous resource
monitoring for self-predicting DBMS,” in 13th IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems.

IEEE, 2005, pp. 239–248.

[17] J. Rogers, O. Papaemmanouil, and U. Cetintemel, “A generic auto-
provisioning framework for cloud databases,” in 2010 IEEE 26th Inter-
national Conference on Data Engineering Workshops (ICDEW 2010).
IEEE, 2010, pp. 63–68.

[18] S. Das, F. Li, V. R. Narasayya, and A. C. K¨onig, “Automated demand-
driven resource scaling in relational database-as-a-service,” in Proceed-
ings of the 2016 International Conference on Management of Data,
2016, pp. 1923–1934.

[19] B. Mozafari, C. Curino, A. Jindal, and S. Madden, “Performance and
resource modeling in highly-concurrent OLTP workloads,” in Proceed-
ings of the 2013 acm sigmod international conference on management
of data, 2013, pp. 301–312.

[20] L. Ma, D. Van Aken, A. Hefny, G. Mezerhane, A. Pavlo, and G. J.
Gordon, “Query-based workload forecasting for self-driving database
management systems,” in Proceedings of the 2018 International Con-
ference on Management of Data, 2018, pp. 631–645.

[21] A. S. Higginson, M. Dediu, O. Arsene, N. W. Paton, and S. M. Embury,
“Database workload capacity planning using time series analysis and
machine learning,” in Proceedings of the 2020 ACM SIGMOD Interna-
tional Conference on Management of Data, 2020, pp. 769–783.
[22] R. Marcus and O. Papaemmanouil, “Plan-structured deep neural network
models for query performance prediction,” Proceedings of the VLDB
Endowment, vol. 12, no. 11, pp. 1733–1746, 2019.

[23] J. Duggan, U. Cetintemel, O. Papaemmanouil, and E. Upfal, “Perfor-
mance prediction for concurrent database workloads,” in Proceedings of
the 2011 ACM SIGMOD International Conference on Management of
data, 2011, pp. 337–348.

[24] J. Duggan, O. Papaemmanouil, U. Cetintemel, and E. Upfal, “Contender:
A resource modeling approach for concurrent query performance pre-
diction.” in EDBT, 2014, pp. 109–120.

[25] S. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica, “Ernest:
Efﬁcient performance prediction for large-scale advanced analytics,”
in 13th {USENIX} Symposium on Networked Systems Design and
Implementation ({NSDI} 16), 2016, pp. 363–378.

[26] J. Rottinghuis, “Partly Cloudy: The start of a journey into the cloud,”
2019. [Online]. Available: https://blog.twitter.com/engineering/en us/
topics/infrastructure/2019/the-start-of-a-journey-into-the-cloud.html
[27] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop
distributed ﬁle system,” in 2010 IEEE 26th symposium on mass storage
systems and technologies (MSST).

Ieee, 2010, pp. 1–10.

[28] J. L. Dem, “Graduating Apache Parquet,” 2015. [Online]. Available:
https://blog.twitter.com/engineering/en us/a/2015/graduating- apache-
parquet.html

[29] S. Krishnan, “Discovery and consumption of analytics data at Twitter,”
2016. [Online]. Available: https://blog.twitter.com/engineering/en us/
topics/insights/2016/discovery- and- consumption- of- analytics- data- at-
twitter.html

[30] A. Clauset, C. R. Shalizi, and M. E. Newman, “Power-law distributions
in empirical data,” SIAM review, vol. 51, no. 4, pp. 661–703, 2009.
[31] Q. Sun, W.-X. Zhou, and J. Fan, “Adaptive huber regression,” Journal
of the American Statistical Association, vol. 115, no. 529, pp. 254–265,
2020.

[32] F. Nooralahzadeh, L. Øvrelid, and J. T. Lønning, “Evaluation of domain-
speciﬁc word embeddings using knowledge resources,” in Proceedings
of the eleventh international conference on language resources and
evaluation (LREC 2018), 2018.

[33] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H.
Katz, S. Shenker, and I. Stoica, “Mesos: A platform for ﬁne-grained
resource sharing in the data center.” in NSDI, vol. 11, no. 2011, 2011,
pp. 22–22.

