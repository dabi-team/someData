2
2
0
2

p
e
S
9
2

]

D
S
.
s
c
[

2
v
7
0
8
3
0
.
9
0
2
2
:
v
i
X
r
a

Hardware Accelerator and Neural Network
Co-Optimization for Ultra-Low-Power Audio
Processing Devices

Gerum Christoph*
Department of Computer Science
University of T¨ubingen
christoph.gerum@uni-tuebingen.de

Frischknecht Adrian*
Department of Computer Science
University of T¨ubingen
adrian.frischknecht@uni-tuebingen.de

Hald Tobias
Department of Computer Science
University of T¨ubingen
tobias.hald@student.uni-tuebingen.de

Palomero Bernardo Paul
Department of Computer Science
University of T¨ubingen
paul.palomero-bernardo@uni-tuebingen.de

L¨ubeck Konstantin
Department of Computer Science
University of T¨ubingen
konstantin.luebeck@uni-tuebingen.de

Bringmann Oliver
Department of Computer Science
University of T¨ubingen
oliver.bringmann@uni-tuebingen.de

Abstract—The increasing spread of artiﬁcial neural networks
does not stop at ultralow-power edge devices. However, these very
often have high computational demand and require specialized
hardware accelerators to ensure the design meets power and
performance constraints. The manual optimization of neural
networks along with the corresponding hardware accelerators
can be very challenging. This paper presents HANNAH (Hard-
ware Accelerator and Neural Network seArcH), a framework for
automated and combined hardware/software co-design of deep
neural networks and hardware accelerators for resource and
power-constrained edge devices. The optimization approach uses
an evolution-based search algorithm, a neural network template
technique and analytical KPI models for the conﬁgurable Ultra-
Trail hardware accelerator template in order to ﬁnd an optimized
neural network and accelerator conﬁguration. We demonstrate
that HANNAH can ﬁnd suitable neural networks with minimized
power consumption and high accuracy for different audio clas-
siﬁcation tasks such as single-class wake word detection, multi-
class keyword detection and voice activity detection, which are
superior to the related work.

Index Terms—Machine Learning, Neural Networks, AutoML,

Neural Architecture Search

I. INTRODUCTION

Artiﬁcial intelligence is increasingly spreading into the
domain of always-on ultra-low power connected devices like
ﬁtness trackers, smart IoT sensors, hearing aids and smart
speakers. The limited power budget on these devices and
the high computational demand often mandates the use of
specialized ultra-low power hardware accelerators, specialized
to a speciﬁc application or application domain. Hardware
design, neural network training and optimized deployment
often require manual optimization by the system designers, who
need to deal with manifold often counter-directed issues. In this

This work has been partly funded by the EU and the German Federal
Ministry of Education and Research (BMBF) in the projects OCEAN12
(reference number: 16ESE0270).

*These authors contributed equally to this work.

Fig. 1: Overview of the HANNAH framework.

work, we propose HANNAH (Hardware Accelerator and Neural
Network seArcH) to automatically co-optimize neural network
architectures and a corresponding neural network accelerator.
The HANNAH design ﬂow is shown in Figure 1. Neural
Networks are instantiated and trained in the training component
employing quantization aware training and dataset augmen-
tation. Trained neural networks are then handed over to the
deployment component (Sec. III) for target code generation.
Here, the neural network is quantized to a low word width
representation the neural network operations are scheduled
and on-device memory is allocated for the neural network.
Along with the target network architecture, a specialized
hardware accelerator for the neural network processing is
instantiated from a conﬁgurable Verilog template. Hardware
dependent performance metrics like power consumption and
chip area are then either generated by running the neural
network on a gate-level simulation or estimated using an
analytical performance model. The evolution-based search
strategy (Sec. IV) implemented in the HANNAH optimizer
is then used to incrementally search the neural network and
hardware accelerator codesign space.

The main contributions of this paper are:
1) We present an end-to-end design ﬂow from neural
network descriptions down to synthesis and gate-level

HANNAH –TrainQATNetwork TopologiesData AugmentationHANNAH –DeploySchedulingQuantizationMemory AllocationUltraTrail–NPUMAC ArrayDistributed MemoryProgrammable Control UnitMetricsHANNAH –Optimize (Neural Architecture Search) 
 
 
 
 
 
power estimation.

2) We propose a model-guided hardware and neural network
co-optimization and architecture exploration ﬂow for
ultra-low power devices.

3) The combined ﬂow reaches state-of-the-art results on a

variety audio detection tasks.

II. RELATED WORK

In recent years, neural architecture search (NAS) had
an enormous success [1]–[3] in automating the process of
designing new neural network architectures. The early work on
neural architecture search did not take hardware characteristics
into account. More recent works (Hardware-Aware NAS) allow
to take the execution latency of a speciﬁc target hardware into
account and allow to optimize the neural networks for latency
as well as accuracy. There are several approaches applying
hardware/software co-design to hardware accelerators for neural
networks. In early works this involves manually designing
specialized neural network operations and a hardware architec-
ture [4]. In [5] and [6] reinforcement learning based NAS is
extended to include search for an accelerator conﬁguration on
FPGAs and optimize it for latency and area. Zhou et al. [7] for
the ﬁrst time combine differentiable neural architecture search
and the search for a neural architecture conﬁguration. All of
these approaches are not directly applicable to TinyML, as their
search spaces for neural networks and hardware accelerators
make it impossible to meet power budgets in the order of
<10 µW.

NAS for TinyML mostly focuses on searching neural
networks for microcontrollers. The search methods in [8],
[9] use genetic algorithms and Bayesian optimization to
optimize neural networks to ﬁt the constrained compute and
memory requirements of these devices. In [10] a weight
sharing approach is used to train a super network containing
multiple neural networks at once and a genetic algorithm,
is used to search for the ﬁnal network topology. A ﬁrst
approach to apply differentiable neural architecture search to
TinyML is presented in [11] it reaches state-of-the-art results
on the tinyMLPerf benchmarks, but requires relatively big
microcontrollers (˜500kB of memory) to execute the found
networks. These search methods would generally be applicable
to our neural network search problem, but they do not contain
a co-optimization strategy for also optimizing the hardware
architecture.

For ultra-low power audio processing hardware accelerators
or other edge devices with our intended target power there are
currently no hardware accelerators. So our main state of the
art comprises of manually designed hardware accelerators for
a speciﬁc neural network architecture. Manual optimization
of ultra-low power hardware has been used for all of the use-
cases in the experimental evaluation. Recent examples include
keyword spotting (KWS) [12]–[16], wake word detection
(WWD) [17], [18] and voice activity detection (VAD) [16],
[19], [20]. These approaches show impressive results, but all
require a work intensive and error-prone manual design process.

Fig. 2: Overview of the UltraTrail architecture.

III. NPU ARCHITECTURE AND ANALYTICAL MODELING

A. NPU architecture

The NPU architecture used in this work is based on UltraTrail
[12], a conﬁgurable accelerator designed for TC-ResNet. Fig.
2 shows the basic UltraTrail architecture. The accelerator
uses distributed memories to store the features (FMEM0-2),
parameters (W/BMEM) and local results (LMEM). Features,
parameters and internal results use a ﬁxed-point representation.
An array of multiply and accumulate (MAC) units calculates
the convolutional and fully-connect layers. A separate output
processing unit (OPU) handles post-processing operations
like bias addition, application of a ReLU activation function,
average pooling. A conﬁguration register stores the structure of
a trained neural network layer by layer. Each layer conﬁguration
contains, among others, information about the input feature
length, input and output feature channels, kernel size, and
stride, as well as the use of padding, bias addition, ReLU
activation, or average pooling.

The architecture of UltraTrail has been parameterized to suit
the executed neural networks as best as possible. At design
time, size and number of supported layers of the conﬁguration
register, the word width for the features, parameters, and
internal results, the size of the memories, the dimension
of the quadratic MAC array, and the number of supported
classiﬁcation classes can be modiﬁed. During runtime, the
programmable conﬁguration register allows the execution of
different neural networks.

B. Deployment

To execute a trained neural network with UltraTrail, a
schedule, a corresponding conﬁguration, and a sorted binary
representation of the quantized weights and bias values have to
be created. These steps are part of the automated deployment
ﬂow.

The deployment backend converts the PyTorch Lightning
model from HANNAH to ONNX and its graph model represen-
tation. A graph tracer iterates over the new representation and
determines the schedule and the layer-wise conﬁguration which
is loaded into the conﬁguration register of UltraTrail. The tracer
changes the node order in the obtained graph such that residual
connections are processed before the main branch to ensure a
correct schedule for the NPU as described in [12]. The graph
tracer also searches for exit branches for early exiting to handle
correct conﬁguration and treatment when the exit is not taken.

Control UnitFMEM 2FMEM 1FMEM 0InterconnectOPUMAC ArrayN×NLMEMBMEMWMEMCinFinWinBinRoutAlgorithm 1: Pseudocode of the NPU accelerator when
conﬁgured with an N × N -MAC array with K output
channels, C input channels, F ﬁlter size, S stride, I
input length, X = (cid:98)(I − 1)/S(cid:99) + 1 output length, and
full padding.

1 for k = 0 : (cid:100)K/N(cid:101) do
2

for c = 0 : (cid:100)C/N(cid:101) do
for f = 0 : F do

3

4

5

6

7

8

9

for x = 0 : X do

iidx ← x · S − (cid:98)F/2(cid:99) + f
if iidx ≥ 0 and iidx < I then

l(N)[x] += i(N)[c][iidx] · w(N×N)[k][c][f]

for x = 0 : X do

o(N)[k][x] = opu(l(N)[x])

The backend extracts weights and biases from the model.
These parameters are padded and reordered to ﬁt the expected
data layout before they get quantized to the deﬁned ﬁxed-point
format. The same happens to example input data provided by
HANNAH which is used as simulation input.

The word widths of the internal LMEM memory are
calculated by the largest ﬁxed-point format times two plus
the logarithm of the maximum input channels to avoid an
overﬂow.

After the schedule and number of weights and biases are
ﬁxed, the deployment backend generates the corresponding
22FDX memory macros. The backend determines the size and
word width of the weight and bias memory from the ﬁxed
numbers. The schedule gets analyzed to determine the size of
each FMEM so that the feature maps ﬁt perfectly for the trained
neural network. All memory sizes can be adjusted manually to
support other neural networks with more parameters or larger
feature maps. The LMEM size is determined by the largest
supported output feature map. All memory sizes and word
widths are set to the next possible memory macro conﬁguration.
Given the generated conﬁguration, weights and biases,
memory macros, example input data, and selected hardware
parameters, a simulation, synthesis, and power estimation are
run automatically if desired. The simulation results are fed
back to HANNAH and compared with the reference output to
validate the functional correctness of the accelerator.

C. NPU Models

The simulation and synthesis of UltraTrail are time-
consuming and therefore not feasible for hundreds to thousands
of possible conﬁgurations. To get an accurate yet fast evaluation
of UltraTrail, performance, area, and power consumption are
estimated using analytical models.

For latency estimation we adopt the cycle-accurate ana-
lytical model presented in [12] to conﬁgurable array sizes.
The pseudocode shown in Algorithm 1 visualizes the NPU
operations and memory accesses. The accelerator iterates over
C input and K output channels tiled by the MAC array size

N . One N × N patch of the weight array is fetched from the
weight memory per iteration of the next loop. In the innermost
loop N input channels are fetched from one of the FMEMs
and the current convolution outputs are accumulated in the
LMEM using a spatially unrolled matrix-vector multiplication
on the MAC array. To avoid misaligned memory accesses
and accelerate the computation, padding is implemented by
skipping the corresponding loop iterations instead of actual
zero padding of the feature maps. In the last loop, the OPU
fetches N output channels from the LMEM and the N results
are stored in a planned FMEM. The latency of the accelerator
without skipping of padded values can be easily estimated
using the following equation, by just counting the number of
loop iterations.

l = 1 +

(cid:24) C
N

(cid:25)

(cid:25)

(cid:24) K
N

·

· F · X

(1)

2

(cid:107)

Where the output length X can be derived from input
length I using X = (cid:98)(I − 1)/S(cid:99) + 1. The crucial part of
the performance model is to accurately calculate the number of
skipped loops in Line 7 of Algorithm 1. For the case iidx ≥ 0
(cid:106) (cid:98)F/2(cid:99)−1
+ 1
skipping happens in the ﬁrst ap,b =
iterations
S
(cid:7) at the start of
over x, the number of skipped executions is (cid:6) F
the loop, and decreases by S · i for each iteration over X. For
the case iidx < I the analytical model is similar but special
care must be taken if the input length I is not divisible by
the stride. In the last iteration over x, iidx takes the value
Imax = (X − 1) · S − (cid:98)F/2(cid:99) + F − 1 leading to an effective
padding of Cw,e = Imax + 1 − (cid:98)F/2(cid:99) − I. Loop skipping then
happens for the last ap,e =
iterations. The
number of skipped iterations is Cw,e at the end of the loop
and also decreases by s · i at each loop iteration. This leads
to an estimation of the total number of skipped MAC array
operations at the beginning #M ACnot,b and end #M ACnot,e
of the loop as:

(cid:106) (cid:98)Cw,e(cid:99)−1
S

+ 1

(cid:107)

#M ACnot,b =

ap,b−1
(cid:88)

(cid:23)

(cid:22) F
2

− s · i

#M ACnot,e =

i=0

ap,e−1
(cid:88)

i=0

And a total per layer latency of:

(cid:98)Cw,e(cid:99) − s · i

(2)

(3)

l = 1+

(cid:25)

(cid:24) C
N

(cid:25)

(cid:24) K
N

·

·F ·X −#M ACnot,e −#M ACnot,b (4)

The power model has two parts for SRAM power estimation
and for other NPU components. For every layer and memory,
the model calculates the number of read, write, and nop (idle)
operations. LMEM and input feature memory (IMEM) are
accessed at each cycle of the operation except for a single
setup cycle. The read cycles rimem and rlmem and write cycles
wlmem are the same as the layer latency l − 1. As the number of
memory accesses to the weights remain stationary during the

N

N

(cid:7)·(cid:6) K

innermost loop of Algorithm 1, the weight memory is accessed
only (cid:6) C
(cid:7)·F times (rwmem). Memory access to the output
feature memory (womem), the bias memory (rbmem), and in
the case of residual blocks the partial sum feature memory
(rpmem) is given as (cid:100)K/N (cid:101) · X. Idle times for the memories
are then calculated using the layer latencies and access times
for each memory: im = l − rm − wm

Furthermore, the combinational MAC array alone causes
many relevant glitches related to SRAM. Therefore,
the
glitching LMEM data input leads to a non-negligible power
increase. Based on some gate-level simulations for different
networks and array sizes, a linear regression for the number
of glitches (glmem) is used.

Finally, if the network latency on the accelerator L is below
the period P for real time operation, the memories are switched
to low power modes with reduced leakage for the remainder
of the period. The ﬁnal power consumption of the memories
is then estimated using:

(a)

(b)

Pmem =

+

+

L
P
(cid:88)

m
L
P

· P (glitch)

lmem · glmem

Fig. 3: A subgraph of the search space build from blocks and
layers (a) and the corresponding abstract conﬁguration (b).

(5)

L
P

(P (read)
m

· rm + P (write)

m

· wm + P (idle)

m

· im)

TABLE I: Neural Architecture Search Space

(cid:18)

· P (static)
m

+

1 −

· P (lp)
m

(cid:19)

L
P
, P (write)
m

m

The dynamic power P (read)

lmem and
static power in running- P (static) and low power-mode P (lp)
are extracted from the memory compiler for each memory
macro usable during the co-optimization and stored in a
database.

, P (glitch)

, P (idle)
m

Again, the model assumes a constant value for the control
unit as the power consumption is mainly independent of
the conﬁguration and executed neural network. The power
consumption of the MAC array and OPU is approximated
using a linear regression on MAC array size and word width
of the MAC array calibrated using gate-level simulations. Note,
that the dynamic power for non-memory modules is weighted
depending on the runtime per inference.

The analytical area model comprises an exact SRAM area
calculation and an estimation for the other NPU components
to estimate the cell area of the NPU after synthesis. The model
looks up the SRAM area in a small database containing all
necessary memory macros used during NAS. The SRAM area
estimation is by far the most important as it is responsible
for about 90 % of the total synthesis cell area. The control
unit is mainly independent of the conﬁguration and its area
is modelled as constant. The MAC array is estimated by a
linear growth of the word width and quadratic growth of the
MAC array dimension based on a minimum MAC unit area.
The OPU model is like the MAC array model however grows
linear with MAC array size.

IV. NEURAL NETWORK / HARDWARE CO-OPTIMIZATION

The co-optimization uses a block based search space for
neural network architectures. As shown in Figure 3a each

Level
Network
Network
Network
Block
Block
Block
Layer
Layer
Layer
Accelerator
Accelerator
Accelerator

Option
Word Width Features
Word Width Weights
Number of Blocks
Stride of Blocks
Type
Number of Convs
Kernel Size
Output Channels
Activation Function
Array Size
Memory Sizes
Word Widths

Choices
4,6,8
2,4,6,8
1-4
1,2,4,8,16
residual, forward
1-4
1,3,5,7,9,11
4,8,..,64
ReLU, None
2 × 2, 4 × 4, 8 × 8, 16 × 16
imputed
imputed

block is either a residual block with a main branch of a
conﬁgurable number of convolutional layers on the trunk and
a skip connection or a simple feed forward CNN block leaving
out the skip connection. We additionally search over layer,
block and network level hyperparameters like ﬁlter size, stride
and convolution sizes. The hardware accelerator search space
consists of the MAC array dimensions, the memory sizes
and multiplier word widths. The mac array size is optimized
using the search algorithm while the other metrics are derived
from the neural network parameters, during neural network
deployment. A full overview of the parameters is shown in
Table I.

The search for a target neural network and accelerator
architecture conﬁguration is implemented as an evolution-based
multi-objective optimization. As shown in Algorithm 2, the
search ﬁrst samples random neural network and hardware
conﬁguration from a joint search space S. The neural network
is then trained using quantization aware training, and the
trained neural network is then evaluated on the validation
set to obtain the accuracy metric. All further performance
metrics are estimated by the analytical hardware model as
described in Section III. At the end of an optimization step, the

CONV 9×1s =2BatchNormReLU+CONV 1×1s =2BatchNormReLUCONV 9×1s =1BatchNormReLUMinor 1Minor 2Minor 3Major 1Major Block 1●output channels = 24●stride = 2●branch = “residual”●Minor Blocks: (see below)Minor Block 1●size= 9●padding= true●batchnorm = true●activation= true●parallel = falseMinor Block 2●size= 9●padding= true●batchnorm = true●activation= false●parallel = falseMinor Block 3●size= 1●padding= true●batchnorm = true●activation= true●parallel = truesampled architecture parameters and the performance metrics
are added to the search history. After the initial population
size s has been reached, new architectures are derived from
the current population using element-wise mutations. During
search, HANNAH randomly samples from the following set
of mutations:

1) Add/remove a block
2) Change block type between residual and feed-forward
3) Add/remove a convolutional layer
4) Increase/decrease convolution size
5) Increase/decrease major block stride
6) Increase/decrease quantization word width
7) Increase/decrease MAC array size
8) Increase/decrease number of output channels
To select the ancestor of the next evaluation point. Similar to
current neural network search for TinyML on microcontrollers
[8], [9], we adopt randomized scalarization [21].

In this approach the architecture parameters are ranked

according to the following scalarization function:

f (M ) = max

mi ∈ M

λi

· mi

(6)

The λi are sampled from the uniform distribution over [0, 1
]
bi
where bi denotes a soft boundary for each target metric.
Choosing λi in this way ensures that candidates violating soft
targets are always ranked behind targets that satisfy a target,
while on the other hand encouraging the search to explore
different parts of the search space near the Pareto boundary.

V. EXPERIMENTAL EVALUATION

The HANNAH framework has been implemented using
current best practice libraries, using PyTorch version 1.10.1
[22].

Training hyperparameters have been set to ﬁxed values for
all experiments. All training runs use 30 epochs and use
a batch size of 128 samples per minibatch. The optimizer
used for training the network parameters is adamW with a
one-cycle learning rate scheduling policy using a maximum
learning rate of 0.005. All other optimizer and learning rate
parameters are left at the default values provided by PyTorch.
All searches are run on a machine learning cluster using
4 Nvidia RTX2080 Ti GPUs. We train 8 neural network
candidates in parallel which takes approximately 10 minutes.
The training uses noisy quantization aware training [23]. The
training sets are augmented using the provided background
noise ﬁles for keyword spotting and voice activity detection
and using random white noise for wake word detection. For
inference the batch norm weights are folded into the weights
and bias of the preceding convolutional layer [24].

As the hardware accelerator is set to operate at 250 kHz and
the neural networks are trained and evaluated with an input
time shift of 100 ms we set the latency constraint of the neural
network architectures to 25.000 cycles corresponding to the
maximum input shift used during training and evaluation. The
area constraints are set to 150 000 µm2 which is slightly less
than the conﬁguration used in [12] for KWS. The constraints for

*/

*/

*/

*/

Algorithm 2: Neural architecture and Hardware Accel-
erator Search
Input: Design Space S, Search Budget b, Population
Size s, Bounds B, Training Set T , Validation
Set V

Output: Search History H
/* Start with empty search history

1 H ← ∅
2 for n ∈ {1 ... b} do
3

if b ≤ s then

/* Sample Random Architecture and

Accelerator

a ← sample random(S)

else

/* Sample Fitness Function from target

bounds

f ← sample ﬁtness function(B)
/* Sort last p candidates according to

Fitness

P ← sort(H[−s :], f )
/* Apply random mutation to current best

architecture

a ← mutate random(last(P))
/* Quantization Aware Training
N ← train(a, T )
/* Evaluate trained architecture
error rate ← evaluate(N , V )
/* Estimate other metrics from hardware

model

power, latency, area ← hardware model(a)

/* Append architecture and metrics to
history

H.append(a, (error rate, power, latency, area))

*/

*/

*/

*/

*/

4

5

6

7

8

9

10

11

12

KWS are set to 5 µW maximum power and accuracy constraints
are set to 93.0 %. The other tasks use a power budget of 1 µW
and accuracy constraints of 95.0 %. All searches ran with
a search budget of 3000 individual architectures and use a
population size of 100.

We use the 22FDX technology by GlobalFoundries for
implementation with low-leakage standard cells and SRAMs
from Invecas. For synthesis and power estimation we use
Cadence Genus 20.10 and Cadence Joules 21.11, respectively.
For the average power consumption, we use two separate power
estimations for the inference with previous feature loading and
idle time. A weighted sum adds these two parts accordingly.
The load of weights and bias is not included as it must be
performed only once. It is evaluated at a 25 °C TT corner with
0.8 V supply voltage and no body bias voltage. The NPU uses
clock-gating and low-power modes of the SRAM during idle
times and waiting for the next inference to start.

A. Results of Neural Network Co-Optimization

Figure 4 shows a summary of the NAS results. For the
three evaluation datasets, the search focuses the evaluated

not consider any data dependent or parasitic induced power
consumption. The NPU models are valid to guide the neural
network co-optimization regarding performance and area with
high precision.

C. Comparison to related work

Note, that due to the differences in datasets, technology, and
the components contained in the system, a direct comparison is
only possible to a limited extent. All our implementations have a
positive slack that allows at least a clock frequency of 13.3 MHz
without changing the voltage nor the synthesized netlist. This
allows to reduce the latency but the power consumption would
increase linearly with the clock frequency. For our work,
the tables list the latency for feature loading and subsequent
inference, as well as the latency with applied power gating.
The corresponding power consumptions are also listed.

1) Keyword Spotting: Table II lists state-of-the-art KWS
accelerators and systems. [13] uses a recurrent neural network
with LSTM units to detect only four keywords and has a
higher power consumption than both of our variants. [14]
proposes a system with feature extraction and a hierarchical
accelerator setup including a sound detector, a KWS and a
speaker validation module to reduce power consumption. The
system in [15] also includes a feature extraction but nevertheless
with 41.3 µW has a higher power consumption than all other
works. Our LP and HA variants are based on [12] and achieve
the highest accuracy on GSCD while both variants still have a
lower power consumption.

2) Voice Activity Detection: State-of-the-art VAD accelera-
tors and systems are shown in Table III. Note that almost all
chosen datasets, technologies and latencies are different. Price
et al. [16] uses a VAD accelerator for a fully connected neural
network as a preliminary stage with a power consumption of
22.3 µW and a latency of at least 100 ms. [19] presents an ultra-
low power VAD accelerator for binary neural networks. The
power consumption varies between 2 and 8 µW depending on
the used word width with speech accuracy of up to 95 % with
10 dB of restaurant noise. [20] uses an analogue front end (AFE)
to calculate the features and so the overall system consumes
just about 1 µW. The accelerator itself has a low latency and
power consumption but also a low accuracy compared to other
works. Our variants have the lowest power consumption and
highest accuracies without noise and a latency of 100 ms.

3) Wake Word Detection: State-of-the-art WWD accelerators
and systems are shown in Table IV using the “Hey Snips”
dataset. [17] uses spiking neural networks combined with event-
driven clock and power gating to reduce the power consumption.
This is the only state of the art accelerator that can beat our
work in some of the accuracy and power values, but needs
a very specialized neural network and hardware accelerator
architecture. [18] proposes a system with accelerators for
feature extraction, VAD and binary RNNs. This system has
a lower accuracy and higher power consumption than our
accelerators.

Fig. 4: Summarized power, area, and accuracy results

architectures around the Pareto front of power and accuracy, and
is able to ﬁnd good trade-offs of power and accuracy. For further
analysis we select the Pareto points with lowest power (LP) and
highest accuracy (HA) which are shown in Figure 5. Half of
the chosen networks are simple feed-forward convolutional
neural networks, while the other half of neural networks
contain at least one residual block. The chosen networks
sizes and operator conﬁgurations reﬂect the difﬁculty of the
different machine learning tasks. The harder tasks use deeper
networks, fewer strides and generally smaller convolution
kernels, while the easier tasks use very shallow networks
with rather bit convolution kernels. These bigger ﬁlters are
needed to compensate for the extremely big strides used in
the operator conﬁgurations. In all of the networks the search
algorithm identiﬁed, possibilities to introduce bottlenecks, e.g.
convolutions with a larger number of output channels followed
convolutions with comparably fewer output channels.

B. NPU Models

Figure 6 shows the deviation in percentage points of the
predictions of the analytical models and the data obtained from
RTL simulation, synthesis and gate-level power estimation for
latency, cell area and power consumption, respectively.

The analytical performance model has no deviation as the

behavior of the hardware accelerator is modelled exactly.

The area model slightly underestimates by 2.41-4.06 per-
centage points because only the memory macros are exactly
modelled and the other components do not handle quadratic
growth. The deviation to the total synthesis area is additionally
about 3.6 %.

The power model uses the performance model to calculate
the inference and idle time. Regarding the gate-level power
consumption the power model results are in the range of
−9.19-−0.2 percent points. Like the area model, the power
model does not handle any non-linearity despite the array size.
In addition, the chosen minimum value for the MAC array
and OPU is conservative and has higher deviations for larger
conﬁgurations like the ones for KWS. Also, the model does

0510152025Power [W]86889092949698100Accuracy [%]TaskKWSVADWWDArea [m2]80000160000240000320000400000Fig. 5: Low power and high accuracy network conﬁgurations for the three search tasks (fc = fully connected layer, bn = batch
norm, relu = rectiﬁed linear unit, k=kernel size, c=number of output channels, s = stride)

TABLE II: Comparison of Results on Keyword Spotting Task

Technology
Area
Frequency
Latency

ESSCIRC’2018 [13]
65 nm
1.03 mm2 c
250 kHz
16 ms

ISSCC’2020 [14]
65 nm
2.56 mm2 c
250 kHz
16 ms

IEEE Access [15]
22 nm
0.75 mm2 c
250 kHz
20 ms

Voltage
DNN Structure
Word Width (Weights)
Word Width (Inputs)
MAC Array Size
Accuracya
F1-Scorea
Keywords
Power

0.6 V
LSTM+FC
4/8
8
-
-
90.00 % (TIMIT)b
4
5.0 µW

0.6 V
LSTM+FC
4/8
8
-
90.87 % (GSCD)
-
10
10.6 µW

0.55 V
CONV+FC
7
8
-
90.51 % (GSCD)
-
10
52 µW

ESWEEK’2020 [12]
22 nm
0.20 mm2 c
250 kHz
100 ms

High Accuracy
22 nm
0.126 mm2 d
250 kHz
100 ms
35.5 mse
0.8 V
CONV+FC
6
6
8 × 8
94.73 % (GSCD)
94.73 % (GSCD)
10
4.73 µW
10.4 µWf
e Active inference latency

Low Power
22 nm
0.118 mm2 d
250 kHz
100 ms
19.0 mse
0.8 V
TC-ResNet
6
6
8 × 8
93.54 % (GSCD)
93.54 % (GSCD)
10
4.17 µW
10.7 µWf

f Power when active

0.8 V
TC-ResNet
6
8
8 × 8
93.09 % (GSCD)
-
10
8.2 µW

a SNR ≥ 1000 dB

b Note the difference in datasets

c Area for layout/chip

d Cell area for synthesis

heterogeneous deployments and a larger class of DNNs for
intelligent sensor processing and perception in edge devices.

REFERENCES

[1] H. Pham et al., “Efﬁcient neural architecture search via parameters
PMLR,

sharing,” in International Conference on Machine Learning.
2018.

[2] B. Zoph et al., “Learning transferable architectures for scalable im-
age recognition,” in IEEE conference on computer vision and pattern
recognition, 2018.

[3] H. Liu et al., “Hierarchical representations for efﬁcient architecture

search,” arXiv preprint arXiv:1711.00436, 2017.

[4] Y. Yang et al., “Synetgy: Algorithm-hardware co-design for convnet
accelerators on embedded fpgas,” in 2019 International Symposium on
Field-Programmable Gate Arrays, 2019.

[5] M. S. Abdelfattah et al., “Best of both worlds: Automl codesign of a cnn
and its hardware accelerator,” in 57th ACM/IEEE Design Automation
Conference).

IEEE, 2020.

[6] W. Jiang et al., “Hardware/software co-exploration of neural architectures,”
IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 39, no. 12, pp. 4805–4815, 2020.

[7] Y. Zhou et al., “Rethinking co-design of neural architectures and hardware

accelerators,” arXiv preprint arXiv:2102.08619, 2021.

[8] E. Liberis, Ł. Dudziak, and N. D. Lane, “µnas: Constrained neural
architecture search for microcontrollers,” in 1st Workshop on Machine
Learning and Systems, 2021.

Fig. 6: Deviation of analytical models from gate-level simula-
tion results in percentage points.

VI. CONCLUSION AND FUTURE WORK

In this paper, we have presented HANNAH, a framework for
co-optimization of neural networks and corresponding hardware
accelerators. The framework is able to ﬁnd neural networks
to automatically ﬁnd and generate ultra-low power implemen-
tations on a set of three speech processing benchmarks. In
our future work we intend to extend this work to support

k=7,c=4,s=16,bnk=11,c=32,s=1,bnk=11,c=16,s=1,bn,reluk=6,c=32,s=16,bnk=1,c=52,s=1,bn,reluk=1,c=36,s=1,bn,reluk=1,c=36,s=16,bnfc, c=3k=1,c=4,s=16,bn,reluk=3,c=8,s=4,bn,reluk=11,c=8,s=1,bn,relufc, c=3k=1,c=4,s=16,bnk=7,c=8,s=4,bn,reluk=5,c=4,s=1,bnfc, c=2k=3,c=4,s=8,bnk=9,c=16,s=1,bn,reluk=5,c=8,s=1fc, c=2k=3,c=24,s=2,bn,reluk=11,c=24,s=1,bnk=7,c=64,s=4,bnfc, c=12k=1,c=24,s=2,bn,reluk=3,c=24,s=4,bn,reluk=3,c=40,s=1,bn,reluk=3,c=56,s=1,bn,reluk=7,c=24,s=2,bn,reluk=1,c=24,s=1,bn,reluk=11,c=16,s=1,bn,reluk=9,c=60,s=2,bn,relufc, c=12KWS HAKWS LPVAD HAVAD LPWWD HAWWD LP-0.15-3.130.00-5.70-4.060.00-0.02-3.130.00-3.98-4.110.00-7.71-2.570.00-9.19-2.410.00-10.00-9.00-8.00-7.00-6.00-5.00-4.00-3.00-2.00-1.000.00PowerAreaLatencyKWS_HAKWS_LPVAD_HAVAD_LPWWD_HAWWD_LPTABLE III: Comparison of Results on Voice Activity Detection Task

Technology
Area
Frequency
Latency

Voltage
DNN Structure
Word Width (Weights)
Word Width (Inputs)
MAC Array Size
Accuracy

ISSC’2017 [16]
65 nm
-
1.68 MHz-47.8 MHz
100 ms-500 ms

0.5 V-0.9 V
FC
16 (from [19])
16 (from [19])
-
10.0 % EER @7 dB
(AURORA2)

JSSC’2019 [19]
28 nm
-
-
10 ms

0.55 V
FC
1
1/4/8
-
95/92.0 % @10 dB
90/87.0 % @5 dB
85/80.0 % @−5 dB
(TIMIT, Noise-92)

GLSVLSI’2020 [20]
180 nm
0.62 mm2 (estimated NPU)
-
10 ms

0.55 V
FC
1
9 (ﬁrst layer), 1 (else)
-
84/85 % @10 dB
(Aurora4, DEMAND)

Power

22.3 µW

2/5/8 µW

1.01 µW@AFE+VAD
0.63 µW@VAD only

a Active inference latency

b Power when active

High Accuracy
22 nm
0.050 mm2
250 kHz
100 ms
9.1 msa
0.8 V
TC-ResNet
4
4
4 × 4
99.88 %
99.43 % @10 dB
99.40 % @5 dB
98.04 % @0 dB
(UW/NU,TUT)
851 nW
4.17 µWb

Low Power
22 nm
0.036 mm2
250 kHz
100 ms
4.5 msa
0.8 V
TC-ResNet
2
4
4 × 4
97.17 %
95.37 % @10 dB
93.04 % @5 dB
88.90 % @0 dB
(UW/NU,TUT)
435 nW
2.13 µWb

TABLE IV: Comparison of Results on Wake Word Detection Task

Technology
Area
Frequency
Latency

Voltage
DNN Structure
Word Width
(Weights)
Word Width
(Inputs)
MAC Array Size
Accuracy

arXiv’2020 [17]
65 nm
1.99 mm2
70 kHz
-

VLSI’2019 [18]
65 nm
6.2 mm2 (FE+NPU)
5-75 MHz
3.36 µs-1.91 ms

0.5-1.0 V
SNN
6

1

-
95.8 %@40 dB
˜90.5 %@0 dB

0.9-1.1 V
RNN
1

1

-
91.9 %

Power

75-220 nW

134 µW

a Active inference latency

b Power when active

High Accuracy
22 nm
0.046 mm2
250 kHz
100 ms
18.2 msa
0.8 V
TC-ResNet
4

Low Power
22 nm
0.036 mm2
250 kHz
100 ms
5.4 msa
0.8 V
CONV+FC
2

4

4

4 × 4
96.81 %
95.8 %@40 dB
94.6 %@0 dB
998 nW
4.15 µWb

4 × 4
95.64 %
95.3 %@40 dB
95.1 %@0 dB
435 nW
2.14 µWb

[9] I. Fedorov et al., “Sparse: Sparse architecture search for cnns on
resource-constrained microcontrollers,” Advances in Neural Information
Processing Systems, vol. 32, 2019.

[10] J. Lin et al., “Mcunet: Tiny deep learning on iot devices,” arXiv preprint

arXiv:2007.10319, 2020.

[11] C. Banbury et al., “Micronets: Neural network architectures for deploying
tinyml applications on commodity microcontrollers,” Proceedings of
Machine Learning and Systems, vol. 3, 2021.

[12] P. P. Bernardo et al., “Ultratrail: A conﬁgurable ultralow-power tc-resnet
ai accelerator for efﬁcient keyword spotting,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 39,
no. 11, pp. 4240–4251, 2020.

[13] J. S. Giraldo and M. Verhelst, “Laika: A 5uw programmable lstm
accelerator for always-on keyword spotting in 65nm cmos,” in IEEE 44th
IEEE, 2018, pp. 166–169.
European Solid State Circuits Conference.
[14] J. S. P. Giraldo et al., “Vocell: A 65-nm Speech-Triggered Wake-Up SoC
for 10-µW Keyword Spotting and Speaker Veriﬁcation,” IEEE Journal
of Solid-State Circuits, 2020.

[15] B. Liu et al., “An Ultra-Low Power Always-On Keyword Spotting
Accelerator Using Quantized Convolutional Neural Network and Voltage-
Domain Analog Switching Network-Based Approximate Computing,”
IEEE Access, vol. 7, pp. 186 456–186 469, 2019.

[17] D. Wang et al., “Always-on, sub-300-nw, event-driven spiking neu-
[16] M. Price, J. Glass, and A. P. Chandrakasan, “A Low-Power Speech
Recognizer and Voice Activity Detector Using Deep Neural Networks,”
IEEE Journal of Solid-State Circuits, vol. 53, pp. 66–75, Jan. 2018.

ral network based on spike-driven clock-generation and clock-and
power-gating for an ultra-low-power intelligent device,” arXiv preprint
arXiv:2006.12314, 2020.

[18] R. Guo et al., “A 5.1 pj/neuron 127.3 us/inference rnn-based speech
recognition processor using 16 computing-in-memory sram macros in
65nm cmos,” in 2019 Symposium on VLSI Circuits, 2019.

[19] M. Yang et al., “Design of an always-on deep neural network-based
1-µw voice activity detector aided with a customized software model for
analog feature extraction,” IEEE Journal of Solid-State Circuits, vol. 54,
no. 6, pp. 1764–1777, 2019.

[20] B. Liu et al., “A background noise self-adaptive vad using snr prediction
based precision dynamic reconﬁgurable approximate computing,” in 2020
Great Lakes Symposium on VLSI, 2020.

[21] B. Paria, K. Kandasamy, and B. P´oczos, “A ﬂexible framework for
multi-objective bayesian optimization using random scalarizations,” in
Uncertainty in Artiﬁcial Intelligence. PMLR, 2020.

[22] A. Paszke et al., “Pytorch: An imperative style, high-performance deep
learning library,” in Advances in Neural Information Processing Systems
32, H. Wallach et al., Eds. Curran Associates, Inc., 2019, pp. 8024–8035.
[23] A. Fan et al., “Training with quantization noise for extreme ﬁxed-point

compression,” arXiv preprint arXiv:2004.07320, 2020.

[24] B. Jacob et al., “Quantization and training of neural networks for efﬁcient
integer-arithmetic-only inference,” in IEEE Conference on Computer
Vision and Pattern Recognition, 2018.

