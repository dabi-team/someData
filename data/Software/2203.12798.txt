DPar2: Fast and Scalable PARAFAC2
Decomposition for Irregular Dense Tensors

Jun-Gi Jang
Computer Science and Engineering
Seoul National University
Seoul, Republic of Korea
elnino4@snu.ac.kr

U Kang
Computer Science and Engineering
Seoul National University
Seoul, Republic of Korea
ukang@snu.ac.kr

2
2
0
2

n
u
J

2

]

G
L
.
s
c
[

2
v
8
9
7
2
1
.
3
0
2
2
:
v
i
X
r
a

Abstractâ€”Given an irregular dense tensor, how can we ef-
ï¬ciently analyze it? An irregular tensor is a collection of
matrices whose columns have the same size and rows have
different sizes from each other. PARAFAC2 decomposition is a
fundamental tool to deal with an irregular tensor in applications
including phenotype discovery and trend analysis. Although
several PARAFAC2 decomposition methods exist, their efï¬ciency
is limited for irregular dense tensors due to the expensive
computations involved with the tensor.

In this paper, we propose DPAR2, a fast and scalable
PARAFAC2 decomposition method for irregular dense tensors.
DPAR2 achieves high efï¬ciency by effectively compressing each
slice matrix of a given irregular tensor, careful reordering of
computations with the compression results, and exploiting the ir-
regularity of the tensor. Extensive experiments show that DPAR2
is up to 6.0Ã— faster than competitors on real-world irregular
tensors while achieving comparable accuracy. In addition, DPAR2
is scalable with respect to the tensor size and target rank.

Index Termsâ€”irregular dense tensor, PARAFAC2 decomposi-

tion, efï¬ciency

I. INTRODUCTION

How can we efï¬ciently analyze an irregular dense tensor?
Many real-world multi-dimensional arrays are represented as
irregular dense tensors; an irregular tensor is a collection of
matrices with different row lengths. For example, stock data
can be represented as an irregular dense tensor; the listing
period is different for each stock (irregularity), and almost all
of the entries of the tensor are observable during the listing
period (high density). The irregular tensor of stock data is
the collection of the stock matrices whose row and column
dimension corresponds to time and features (e.g., the opening
price, the closing price, the trade volume, etc.), respectively. In
addition to stock data, many real-world data including music
song data and sound data are also represented as irregular
dense tensors. Each song can be represented as a slice matrix
(e.g., time-by-frequency matrix) whose rows correspond to the
time dimension. Then, the collection of songs is represented as
an irregular tensor consisting of slice matrices of songs each
of whose time length is different. Sound data are represented
similarly.

Tensor decomposition has attracted much attention from the
data mining community to analyze tensors [1]â€“[10]. Speciï¬-
cally, PARAFAC2 decomposition has been widely used for
modeling irregular tensors in various applications including

phenotype discovery [11], [12], trend analysis [13], and fault
detection [14]. However, existing PARAFAC2 decomposition
methods are not fast and scalable enough for irregular dense
tensors. Perros et al. [11] improve the efï¬ciency for handling
irregular sparse tensors, by exploiting the sparsity patterns of
a given irregular tensor. Many recent works [12], [15]â€“[17]
adopt their idea to handle irregular sparse tensors. However,
they are not applicable to irregular dense tensors that have
no sparsity pattern. Although Cheng and Haardt [18] improve
the efï¬ciency of PARAFAC2 decomposition by preprocessing
a given tensor, there is plenty of room for improvement in
terms of computational costs. Moreover, there remains a need
for fully employing multicore parallelism. The main challenge
to successfully design a fast and scalable PARAFAC2 decom-
position method is how to minimize the computational costs
involved with an irregular dense tensor and the intermediate
data generated in updating factor matrices.

In this paper, we propose DPAR2 (Dense PARAFAC2
decomposition), a fast and scalable PARAFAC2 decomposition
method for irregular dense tensors. Based on the charac-
teristics of real-world data, DPAR2 compresses each slice
matrix of a given irregular tensor using randomized Sin-
gular Value Decomposition (SVD). The small compressed
results and our careful ordering of computations consider-
ably reduce the computational costs and the intermediate
data. In addition, DPAR2 maximizes multi-core parallelism
by considering the difference in sizes between slices. With
these ideas, DPAR2 achieves higher efï¬ciency and scalability
than existing PARAFAC2 decomposition methods on irregu-
lar dense tensors. Extensive experiments show that DPAR2
outperforms the existing methods in terms of speed, space,
and scalability, while achieving a comparable ï¬tness, where
the ï¬tness indicates how a method approximates a given data
well (see Section IV-A).

The contributions of this paper are as follows.
â€¢ Algorithm. We propose DPAR2, a fast and scalable
PARAFAC2 decomposition method for decomposing ir-
regular dense tensors.

â€¢ Analysis. We provide analysis for the time and the space

complexities of our proposed method DPAR2.

â€¢ Experiment. DPAR2 achieves up to 6.0

faster running
time than previous PARAFAC2 decomposition methods

Ã—

 
 
 
 
 
 
(a) Trade-off

(b) Trade-off

(c) Trade-off

(d) Trade-off

Fig. 1. [Best viewed in color] Measurement of the running time and ï¬tness on real-world datasets for three target ranks R: 10, 15, and 20. DPAR2 provides
the best trade-off between speed and ï¬tness. DPAR2 is up to 6.0Ã— faster than the competitors while having a comparable ï¬tness.

TABLE I
SYMBOL DESCRIPTION.

Symbol

Description

k=1

{Xk}K
Xk
X(i, :)
X(:, j)
X(i, j)
X(n)
Qk, Sk
H, V
Ak, Bk, Ck
D, E, F
F(k)
Zk, Î£k, Pk
R
âŠ—
(cid:12)
âˆ—
(cid:107)
vec(Â·)

irregular tensor of slices Xk for k = 1, ..., K
slice matrix (âˆˆ Ik Ã— J)
i-th row of a matrix X
j-th column of a matrix X
(i, j)-th element of a matrix X
mode-n matricization of a tensor X
factor matrices of the kth slice
factor matrices of an irregular tensor
SVD results of the kth slice
SVD results of the second stage
kth vertical block matrix (âˆˆ RRÃ—R) of F(âˆˆ RKRÃ—R)
SVD results of F(k)EDT VSkHT
target rank
Kronecker product
Khatri-Rao product
element-wise product
horizontal concatenation
vectorization of a matrix

based on ALS while achieving a similar ï¬tness (see
Fig. 1).

â€¢ Discovery. With DPAR2, we ï¬nd that the Korean stock
market and the US stock market have different corre-
lations (see Fig. 12) between features (e.g., prices and
technical indicators). We also ï¬nd similar stocks (see
Table III) on the US stock market during a speciï¬c event
(e.g., COVID-19).

In the rest of this paper, we describe the preliminaries in
Section II, propose our method DPAR2 in Section III, present
experimental results in Section IV, discuss related works in
Section V, and conclude in Section VI. The code and datasets
are available at https://datalab.snu.ac.kr/dpar2.

II. PRELIMINARIES

In this section, we describe tensor notations, tensor opera-
tions, Singular Value Decomposition (SVD), and PARAFAC2
decomposition. We use the symbols listed in Table I.

A. Tensor Notation and Operation

We use boldface lowercases (e.g. x) and boldface capitals
(e.g. X) for vectors and matrices, respectively. In this paper,
indices start at 1. An irregular tensor is a 3-order tensor X
whose k-frontal slice X(:, :, k) is Xk
RIkÃ—J . We denote
k=1 instead of X where K is
irregular tensors by
the number of k-frontal slices of the tensor. An example

Xk

âˆˆ

}

{

K

Algorithm 1: Randomized SVD [20]
Input: A âˆˆ RIÃ—J
Output: U âˆˆ RIÃ—R, S âˆˆ RRÃ—R, and V âˆˆ RJÃ—R.
Parameters: target rank R, and an exponent q
1: generate a Gaussian test matrix â„¦ âˆˆ RJÃ—(R+s)
2: construct Y â† (AAT )qAâ„¦
3: QR â† Y using QR factorization
4: construct B â† QT A
5: ËœUÎ£VT â† B using truncated SVD at rank R
6: return U = Q ËœU, Î£, and V

is described in Fig. 2. We refer the reader to [19] for the
deï¬nitions of tensor operations including Frobenius norm,
matricization, Kronecker product, and Khatri-Rao product.

B. Singular Value Decomposition (SVD)

Â· Â· Â·

âˆˆ
ur

Singular Value Decomposition (SVD) decomposes A

âˆˆ
RIÃ—R is the left singular vector
RIÃ—J to X = UÎ£VT. U
matrix of A; U = (cid:2)u1
(cid:3) is a column orthogonal matrix
, uR are the eigenvectors
where R is the rank of A and u1,
of AAT. Î£ is an R
R diagonal matrix whose diagonal entries
are singular values. The i-th singular value Ïƒi is in Î£i,i where
RJÃ—R is the right singular
Ïƒ1
0. V
âˆˆ
â‰¥ Â· Â· Â· â‰¥
vector matrix of A; V = (cid:2)v1
(cid:3) is a column orthogonal
vR
Â· Â· Â·
matrix where v1,

, vR are the eigenvectors of ATA.

ÏƒR

Â· Â· Â·

Ïƒ2

â‰¥

â‰¥

Ã—

Randomized SVD. Many works [20]â€“[22] have introduced
RIÃ—J
efï¬cient SVD methods to decompose a matrix A
by applying randomized algorithms. We introduce a popular
randomized SVD in Algorithm 1. Randomized SVD ï¬nds a
RIÃ—(R+s) of (AAT )qAâ„¦ us-
column orthogonal matrix Q
âˆˆ
ing random matrix â„¦, constructs a smaller matrix B = QT A
R(R+s)Ã—J ), and ï¬nally obtains the SVD result U (= Q ËœU),
(
âˆˆ
ËœUÎ£VT .
Î£, V of A by computing SVD for B, i.e., B
Given a matrix A, the time complexity of randomized SVD
is O(IJR) where R is the target rank.

â‰ˆ

âˆˆ

Â· Â· Â·

C. PARAFAC2 decomposition

PARAFAC2 decomposition proposed by Harshman [23]
successfully deals with irregular tensors. The deï¬nition of
PARAFAC2 decomposition is as follows:

Deï¬nition 1 (PARAFAC2 Decomposition). Given a target
K
k=1 whose k-frontal slice
rank R and a 3-order tensor
Xk
}
{
RIkÃ—J for k = 1, ..., K, PARAFAC2 decomposition
is Xk
approximates each k-th frontal slice Xk by UkSkVT . Uk is
R, Sk is a diagonal matrix of the
a matrix of the size Ik

âˆˆ

Ã—

8408608809009200.700.720.740.76FMAUrbanUSStockKRStockActivityActionTraï¿¿icPEMS-SF0.60.70.80.91.01.11.21.31.40.00.51.0DPar2RD-ALSPARAFAC2-ALSSPARTAN1001000TotalRunningTime(sec)0.70.750.8FitnessğŸ”.ğŸÃ—ğŸ‘.ğŸÃ—Bestğ‘¹â†‘20100TotalRunningTime(sec)0.750.80.85FitnessğŸ‘.ğŸ“Ã—ğŸ‘.ğŸÃ—Best48TotalRunningTime(sec)0.930.950.97FitnessBestğŸ.ğŸ•Ã—ğŸ.ğŸ“Ã—481632TotalRunningTime(sec)0.90.920.94FitnessğŸ.ğŸÃ—ğŸ.ğŸ’Ã—BestAlgorithm 2: PARAFAC2-ALS [24]
Input: Xk âˆˆ RIkÃ—J for k = 1, ..., K
Output: Uk âˆˆ RIkÃ—R, Sk âˆˆ RRÃ—R for k = 1, ..., K, and

V âˆˆ RJÃ—R.

Parameters: target rank R
1: initialize matrices H âˆˆ RRÃ—R, V, and Sk for k = 1, ..., K
2: repeat
3:
4:

(cid:48)T
k â† XkVSkHT by performing

for k = 1, ..., K do
kP

compute Z(cid:48)
kÎ£(cid:48)
truncated SVD at rank R
(cid:48)T
Qk â† Z(cid:48)
k

kP

end for
for k = 1, ..., K do

Yk â† QT

k Xk

5:
6:
7:
8:
9:
10:

end for
construct a tensor Y âˆˆ RRÃ—JÃ—K from slices Yk âˆˆ RRÃ—J
for k = 1, ..., K
/* running a single iteration of CP-ALS
on Y */

11: H â† Y(1)(W (cid:12) V)(WT W âˆ— VT V)â€ 
12: V â† Y(2)(W (cid:12) H)(WT W âˆ— HT H)â€ 
13: W â† Y(3)(V (cid:12) H)(VT V âˆ— HT H)â€ 
14:
15:
16:
17: until the maximum iteration is reached, or the error ceases to

Sk â† diag(W(k, :))

for k = 1, ..., K do

end for

decrease;

18: for k = 1, ..., K do
19: Uk â† QkH
20: end for

size R
common for all the slices.

Ã—

R, and V is a matrix of the size J

R which are
(cid:3)

Ã—

The objective function of PARAFAC2 decomposition [23]

is given as follows.

min
{Uk},{Sk},V

K
(cid:88)

k=1

Xk

||

âˆ’

UkSkVT

2
F
||

k U = Î¦ for all k), and replace UT

For uniqueness, Harshman [23] imposed the constraint (i.e.,
UT
k with QkH where Qk
is a column orthogonal matrix and H is a common matrix for
all the slices. Then, Equation (1) is reformulated with QkH:

K
(cid:88)

k=1

min
{Qk},{Sk},H,V

Xk

||

âˆ’

QkHSkVT

2
F
||

(2)

Fig. 2 shows an example of PARAFAC2 decomposition for a
given irregular tensor. A common approach to solve the above
problem is ALS (Alternating Least Square) which iteratively
updates a target factor matrix while ï¬xing all factor matrices
except for the target. Algorithm 2 describes PARAFAC2-ALS.
First, we update each Qk while ï¬xing H, V, Sk for k =
1, ..., K (lines 4 and 5). By computing SVD of XkVSkHT
(cid:48)T
as Z(cid:48)
k , which minimizes
Equation (3) over Qk [11], [24], [25]. After updating Qk, the
remaining factor matrices H, V, Sk is updated by minimizing
the following objective function:

(cid:48)T
k , we update Qk as Z(cid:48)

kÎ£(cid:48)

kP

kP

min
{Sk},H,V

QT
||

k Xk

âˆ’

HSkVT

2
F
||

(3)

Minimizing this function is to update H, V, Sk using CP
RRÃ—JÃ—K whose k-th frontal
decomposition of a tensor Y

K
(cid:88)

k=1

âˆˆ

Example of PARAFAC2 decomposition. Given an irregular tensor
k=1, PARAFAC2 decomposes it into the factor matrices H, V, Qk,

Fig. 2.
{Xk}K
and Sk for k = 1, ..., K. Note that QkH is equal to Uk.
slice is QT
k Xk (lines 8 and 10). We run a single iteration of
CP decomposition for updating them [24] (lines 11 to 16). Qk,
H, Sk, and V are alternatively updated until convergence.

Iterative computations with an irregular dense tensor require
high computational costs and large intermediate data. RD-
ALS [18] reduces the costs by preprocessing a given tensor and
performing PARAFAC2 decomposition using the preprocessed
result, but
the improvement of RD-ALS is limited. Also,
recent works successfully have dealt with sparse irregular
tensors by exploiting sparsity. However, the efï¬ciency of their
models depends on the sparsity patterns of a given irregular
tensor, and thus there is little improvement on irregular dense
tensors. Speciï¬cally, computations with large dense slices Xk
for each iteration are burdensome as the number of iterations
increases. We focus on improving the efï¬ciency and scalability
in irregular dense tensors.

III. PROPOSED METHOD

In this section, we propose DPAR2, a fast and scalable
PARAFAC2 decomposition method for irregular dense tensors.

A. Overview

(1)

Before describing main ideas of our method, we present

main challenges that need to be tackled.
C1. Dealing with large irregular tensors. PARAFAC2 de-
composition (Algorithm 2) iteratively updates factor ma-
trices (i.e., Uk, Sk, and V) using an input tensor. Dealing
with a large input tensor is burdensome to update the
factor matrices as the number of iterations increases.
C2. Minimizing numerical computations and intermediate
data. How can we minimize the intermediate data and
overall computations?

C3. Maximizing multi-core parallelism. How can we paral-
lelize the computations for PARAFAC2 decomposition?
The main ideas that address the challenges mentioned above

are as follows:
I1. Compressing an input tensor using randomized SVD
considerably reduces the computational costs to update
factor matrices (Section III-B).

I2. Careful reordering of computations with the com-
pression results minimizes the intermediate data and the
number of operations (Sections III-C to III-E).

I3. Careful distribution of work between threads enables
DPAR2 to achieve high efï¬ciency by considering various
lengths Ik for k = 1, ..., K (Section III-F).

Its PARAFAC2 DecompositionA given irregulartensorPARAFAC2DecompositionFig. 3. Overview of DPAR2. Given an irregular tensor {Xk}K
k=1, DPAR2 ï¬rst compresses the given irregular tensor by exploiting randomized SVD. Then,
DPAR2 iteratively and efï¬ciently updates the factor matrices, Qk, H, Sk, and V, using only the compressed matrices, to get the result of PARAFAC2
decomposition.

As shown in Fig. 3, DPAR2 ï¬rst compresses each slice
of an irregular tensor using randomized SVD (Section III-B).
The compression is performed once before iterations, and only
the compression results are used at iterations. It signiï¬cantly
reduces the time and space costs in updating factor matrices.
After compression, DPAR2 updates factor matrices at each
iteration, by exploiting the compression results (Sections III-C
to III-E). Careful reordering of computations is required to
achieve high efï¬ciency. Also, by carefully allocating input
slices to threads, DPAR2 accelerates the overall process (Sec-
tion III-F).

B. Compressing an irregular input tensor

Xk
{

DPAR2 (see Algorithm 3) is a fast and scalable PARAFAC2
decomposition method based on ALS described in Algo-
rithm 2. The main challenge that needs to be tackled is to
minimize the number of heavy computations involved with a
K
k=1 consisting of slices Xk for
given irregular tensor
}
k = 1, ..., K (in lines 4 and 8 of Algorithm 2). As the number
of iterations increases (lines 2 to 17 in Algorithm 2), the heavy
computations make PARAFAC2-ALS slow. For efï¬ciency, we
preprocess a given irregular tensor into small matrices, and
then update factor matrices by carefully using the small ones.
Our approach to address the above challenges is to compress
K
k=1 before starting iterations. As
a given irregular tensor
}
shown in Fig. 4, our main idea is two-stage lossy compres-
sion with randomized SVD for the given tensor: 1) DPAR2
performs randomized SVD for each slice Xk for k = 1, ..., K
at target rank R, and 2) DPAR2 performs randomized SVD
for a matrix, the horizontal concatenation of singular value
matrices and right singular vector matrices of slices Xk.
Randomized SVD allows us to compress slice matrices with
low computational costs and low errors.

Xk
{

First Stage. In the ï¬rst stage, DPAR2 compresses a given
irregular tensor by performing randomized SVD for each slice
Xk at target rank R (line 3 in Algorithm 3).

â‰ˆ

Xk

AkBkCT
(4)
k
RIkÃ—R is a matrix consisting of left singular
RRÃ—R is a diagonal matrix whose elements are
RJÃ—R is a matrix consisting of

where Ak
âˆˆ
vectors, Bk
âˆˆ
singular values, and Ck
right singular vectors.

âˆˆ

Fig. 4.
Two-stage SVD for a given irregular tensor. In the ï¬rst stage,
DPAR2 performs randomized SVD of Xk for all k. In the second stage,
DPAR2 performs randomized SVD of M âˆˆ RJÃ—KR which is the horizontal
concatenation of CkBk.
K
compress a matrix M =
k=1(CkBk) which is the horizontal
(cid:107)
concatenation of CkBk for k = 1, ..., K. Compressing the
matrix M maximizes the efï¬ciency of updating factor matrices
H, V, and W (see Equation (3)) at
later iterations. We
RJÃ—KR by horizontally concatenating
construct a matrix M
CkBk for k = 1, ..., K (line 5 in Algorithm 3). Then, DPAR2
performs randomized SVD for M (line 6 in Algorithm 3):

âˆˆ

M = [C1B1;

where D
âˆˆ
vectors, E
âˆˆ
singular values, and F
right singular vectors.

âˆˆ

Â· Â· Â·

K
k=1(CkBk)

; CKBK] =

(5)
RJÃ—R is a matrix consisting of left singular
RRÃ—R is a diagonal matrix whose elements are
RKRÃ—R is a matrix consisting of

DEFT

â‰ˆ

(cid:107)

With the two stages, we obtain the compressed results D, E,
F, and Ak for k = 1, ..., K. Before describing how to update
factor matrices, we re-express the k-th slice Xk by using the
compressed results:

Xk

AkF(k)EDT

(6)

where F(k)

âˆˆ

RRÃ—R is the kth vertical block matrix of F:
ï£¹

ï£®

â‰ˆ

F =

ï£¯
ï£°

ï£º
ï£»

(7)

F(1)
...
F(K)

Since CkBk is the kth horizontal block of M and DEF(k)T
is the kth horizontal block of DEFT , BkCT
k corresponds to
F(k)EDT . Therefore, we obtain Equation (6) by replacing
BkCT

k with F(k)EDT from Equation (4).

Second Stage. Although small compressed data are gener-
ated in the ï¬rst step, there is a room to further compress the
intermediate data from the ï¬rst stage. In the second stage, we

In updating factor matrices, we use AkF(k)EDT instead
of Xk. The two-stage compression lays the groundwork for
efï¬cient updates.

Factor matrices of PARAFAC2 Decompositionusing the compressed results A given irregulartensorMatrices compressed by  exploiting randomized SVDS(cid:57)DS(cid:57)DS(cid:57)Dğ—!ğ—"concatenationğ€"ğ€!ğƒğ„ğ…#Stage 1Stage 2The preprocessed resultsâˆ¥$%!"ğ‚ğ’Œğğ’Œ=ğŒğğŸğ‚"#ğ‚ğŸğğŸğğŸğ‚%#ğ‚ğŸğğŸtransposetransposeC. Overview of update rule

Our goal is to efï¬ciently update factor matrices, H, V, and
Sk and Qk for k = 1, ..., K, using the compressed results
AkF(k)EDT . The main challenge of updating factor matrices
is to minimize numerical computations and intermediate data
by exploiting the compressed results obtained in Section III-B.
A naive approach would reconstruct ËœXk = AkF(k)EDT from
the compressed results, and then update the factor matrices.
However,
this approach fails to improve the efï¬ciency of
updating factor matrices. We propose an efï¬cient update rule
using the compressed results to 1) ï¬nd Qk and Yk (lines 5
and 8 in Algorithm 2), and 2) compute a single iteration of
CP-ALS (lines 11 to 13 in Algorithm 2).

There are two differences between our update rule and
PARAFAC2-ALS (Algorithm 2). First, we avoid explicit com-
putations of Qk and Yk. Instead, we ï¬nd small factorized
matrices of Qk and Yk, respectively, and then exploit the
small ones to update H, V, and W. The small matrices
are computed efï¬ciently by exploiting the compressed results
AkF(k)EDT instead of Xk. The second difference is that
DPAR2 obtains H, V, and W using the small factorized
matrices of Yk. Careful ordering of computations with them
considerably reduces time and space costs at each iteration.
We describe how to ï¬nd the factorized matrices of Qk and
Yk in Section III-D, and how to update factor matrices in
Section III-E.

D. Finding the factorized matrices of Qk and Yk

The ï¬rst goal of updating factor matrices is to ï¬nd the fac-
torized matrices of Qk and Yk for k = 1, ..., K, respectively.
In Algorithm 2, ï¬nding Qk and Yk is expensive due to the
computations involved with Xk (lines 4 and 8 in Algorithm 2).
To reduce the costs for Qk and Yk, our main idea is to exploit
the compressed results Ak, D, E, and F(k), instead of Xk.
Additionally, we exploit the column orthogonal property of
Ak, i.e., AT
k Ak = I, where I is the identity matrix.
We ï¬rst re-express Qk using the compressed results ob-
tained in Section III-B. DPAR2 reduces the time and space
costs for Qk by exploiting the column orthogonal property of
Ak. First, we express XkVSkHT as AkF(k)EDT VSkHT
by replacing Xk with AkF(k)EDT . Next, we need to obtain
left and right singular vectors of AkF(k)EDT VSkHT . A
naive approach is to compute SVD of AkF(k)EDT VSkHT ,
but there is a more efï¬cient way than this approach. Thanks to
the column orthogonal property of Ak, DPAR2 performs SVD
of F(k)EDT VSk HT
âˆˆ
RIkÃ—R, at target rank R (line 9 in Algorithm 3):

RRÃ—R, not AkF(k)EDT VSkHT

âˆˆ

k

F(k)EDT VSkHT SVD= ZkÎ£kPT
(8)
where Î£k is a diagonal matrix whose entries are the singular
values of F(k)EDT VSkHT , the column vectors of Zk and
Pk are the left singular vectors and the right singular vec-
tors of F(k)EDT VSkHT , respectively. Then, we obtain the
factorized matrices of Qk as follows:
Qk = AkZkPT
k

(9)

Algorithm 3: DPAR2
Input: Xk âˆˆ RIkÃ—J for k = 1, ..., K
Output: Uk âˆˆ RIkÃ—R, Sk âˆˆ RRÃ—R for k = 1, ..., K, and

V âˆˆ RJÃ—R.

Parameters: target rank R

1:

initialize matrices H âˆˆ RRÃ—R, V, and Sk for k = 1, ..., K
/* Compressing slices in parallel */

2: for k = 1, ..., K do
3:

compute AkBkCT
randomized SVD at rank R

k â† SVD(Xk) by performing

4: end for
5: M â† (cid:107)K
6: compute DEFT â† SVD(M) by performing randomized SVD

k=1(CkBk)

at rank R
/* Iteratively updating factor matrices */

7: repeat
8:
9:

for k = 1, ..., K do

compute ZkÎ£kPT
performing SVD at rank R

k â† SVD(F(k)EDT VSkHT ) by

end for
/* no explicit computation of Yk */
for k = 1, ..., K do
Yk â† PkZT

k F(k)EDT

10:

11:
12:
13:

end for
/* running a single iteration of CP-ALS
on Y */
compute G(1) â† Y(1)(W (cid:12) V) based on Lemma 1

compute G(2) â† Y(2)(W (cid:12) H) based on Lemma 2

14:
15: H â† G(1)(WT W âˆ— VT V)â€ 
16:
17: V â† G(2)(WT W âˆ— HT H)â€ 
18:
19: W â† G(3)(VT V âˆ— HT H)â€ 
for k = 1, ..., K do
20:
21:
22:
23: until the maximum iteration is reached, or the error ceases to

compute G(3) â† Y(3)(V (cid:12) H) based on Lemma 3

Sk â† diag(W(k, :))

(cid:46) Normalize V

end for

(cid:46) Normalize H

decrease;

24: for k = 1, ..., K do
25: Uk â† AkZkPT
26: end for

k H

where AkZk and Pk are the left and the right singular
vectors of AkF(k)EDT VSkHT , respectively. We avoid the
explicit construction of Qk, and use AkZkPT
k instead of Qk.
Since Ak is already column-orthogonal, we avoid performing
SVD of AkF(k)EDT VSkHT , which are much larger than
F(k)EDT VSkHT .

(6).

k AkF(k)EDT , we replace QT

k Xk (line 8 in Algorithm 2) as QT
of
directly
Instead
k with PkZT

Next, we ï¬nd the factorized matrices of Yk. DPAR2 re-
k AkF(k)EDT
expresses QT
computing
using Equation
QT
k . Then,
we represent Yk as the following expression (line 12 in
Algorithm 3):
Yk

k AkF(k)EDT = PkZT
QT
k F(k)EDT
= PkZT
Note that we use the property AT
k Ak = IRÃ—R, where IRÃ—R
is the identity matrix of size R
R, for the last equality.
By exploiting the factorized matrices of Qk, we compute Yk
without involving Ak in the process.

k AkF(k)EDT

k AT

k AT

â†

Ã—

Fig. 5. Computation for G(1) = Y(1)(W(cid:12)V). The rth column G(1)(:, r)
of G(1) is computed by

k=1 W(k, r) (cid:0)PkZT

EDT V(:, r).

k F(k)(cid:1)(cid:17)

(cid:16)(cid:80)K

E. Updating H, V, and W

(cid:12)

The next goal is to efï¬ciently update the matrices H, V,
and W using the small factorized matrices of Yk. Naively,
we would compute Y and run a single iteration of CP-
ALS with Y to update H, V, and W (lines 11 to 13 in
Algorithm 2). However, multiplying a matricized tensor and
a Khatri-Rao product (e.g., Y(1)(W
V)) is burdensome,
and thus we exploit the structure of the decomposed results
k F(k)EDT of Yk to reduce memory requirements and
PkZT
computational costs. In other word, we do not compute Yk,
k F(k)EDT in updating H, V, and W. Note
and use only PkZT
that the k-th frontal slice of Y, Y(:, :, k), is PkZT
k F(k)EDT .
VT V)â€ , we focus
V) based on Lemma 1.
on efï¬ciently computing Y(1)(W
A naive computation for Y(1)(W
V) requires a high com-
putational cost O(JKR2) due to the explicit reconstruction
of Y(1). Therefore, we compute that term without the recon-
struction by carefully determining the order of computations
and exploiting the factorized matrices of Y(1), D, E, Pk, Zk,
and F(k) for k = 1, ..., K. With Lemma 1, we reduce the
computational cost of Y(1)(W

V) to O(JR2 + KR3).

Updating H. In Y(1)(W

V)(WT W

(cid:12)
(cid:12)

(cid:12)

âˆ—

1.

Let

Lemma
with G(1)
(cid:16)(cid:16)(cid:80)K

k=1 W(k, r) (cid:0)PkZT

âˆˆ

(cid:12)
denote Y(1)(W

us
RRÃ—R. G(1)(:, r)
k F(k)(cid:1)(cid:17)

EDT V(:, r)

is
(cid:17)
.

(cid:12)
equal

V)
to
(cid:3)

Proof. Y(1) is represented as follows:

Y(1) = (cid:2)P1ZT

1 F(1)EDT

;

Â· Â· Â·
ï£®
k F(k)(cid:17)(cid:17)

; PKZT
EDT
...
O
k F(k)(cid:17)(cid:17) (cid:0)IKÃ—K

Â· Â· Â·
. . .

ï£¯
ï£°

ï£¹

KF(K)EDT (cid:3)
O
...
EDT

ï£º
ï£»

Â· Â· Â·
EDT (cid:1)

âŠ—

(cid:16)

(cid:16)

=

=

(cid:16)

(cid:16)

K
k=1
(cid:107)

K
k=1
(cid:107)

PkZT

PkZT

K. Then,

Ã—

where IKÃ—K is the identity matrix of size K
G(1) = Y(1)(W
V) is expressed as follows:
k F(k)(cid:17)(cid:17)
(cid:16)
(cid:16)
G(1) =

(cid:12)
PkZT

K
k=1

(cid:107)

Ã—
=

K
k=1

V(:, r))(cid:1)

EDT (cid:1) (cid:0)
R
r=1(W(:, r)
(cid:107)
k F(k)(cid:17)(cid:17) (cid:0)
R
r=1
(cid:107)

(cid:107)
(cid:0)IKÃ—K
âŠ—
(cid:16)
(cid:16)
EDT V(:, r)(cid:1)(cid:1)
PkZT
D) =
The mixed-product property (i.e., (A
âŠ—
BD)) is used in the above equation. Therefore, G(1)(:
AC
EDT V(:, r)(cid:1).
k F(k)(cid:1)(cid:1) (cid:0)W(:, r)
, r) is equal to (cid:0)
âŠ—
k F(k)(cid:1) EDT V(:
k=1 W(k, r) (cid:0)PkZT
We represent
, r) using block matrix multiplication since the k-th ver-

(cid:0)PkZT
K
k=1
(cid:107)
it as (cid:80)K

âŠ—
(cid:0)W(:, r)

âŠ—
B)(C

âŠ—

âŠ—

âˆˆ

âŠ—

(cid:12)

k=1

RR.

RKR is

k H(:, r)(cid:1).

Fig. 6. Computation for G(2) = Y(2)(W(cid:12)H). The rth column G(2)(:, r)
of G(2) is computed by DE (cid:80)K
tical block vector of (cid:0)W(:, r)
W(k, r)EDT V(:, r)

(cid:0)W(k, r)F(k)T ZkPT
EDT V(:, r)(cid:1)

âˆˆ
As shown in Fig. 5, we compute Y(1)(W

V) column by
column. In computing G(1)(:, r), we compute EDT V(:, r),
k F(k)(cid:1) for all k, and then perform ma-
sum up W(k, r) (cid:0)PkZT
trix multiplication between the two preceding results (line 14
in Algorithm 3). After computing G(1)
Y(1)(W
V),
â†
VT V)â€  where
we update H by computing G(1)(WT W
âˆ—
denotes the Moore-Penrose pseudoinverse (line 15 in Al-
â€ 
gorithm 3). Note that the pseudoinverse operation requires a
lower computational cost compared to computing G(1) since
the size of (WT W

RRÃ—R is small.

âˆ—
Updating V. In computing Y(2)(W

UT U)â€ ,
U) based on
we need to efï¬ciently compute Y(2)(W
Lemma 2. As in updating H, a naive computation for
U) requires a high computational cost O(JKR2).
Y(2)(W
U) with the cost O(JR2 +
We efï¬ciently compute Y(2)(W
KR3), by carefully determining the order of computations and
exploiting the factorized matrices of Y(2).

U)(WT W

VT V)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

âˆˆ

âˆ—

Lemma
with G(2)
(cid:16)(cid:80)K
DE

k=1

2.

Let

us

denote Y(2)(W
RJÃ—R. G(2)(:, r)
k H(:, r)(cid:1)(cid:17)

is

.

âˆˆ

(cid:0)W(k, r)F(k)T ZkPT

(cid:12)
equal

H)
to
(cid:3)

Proof. Y(2) is represented as follows:

Y(2) = (cid:2)DEF(1)T Z1PT
(cid:16)

;

; DEF(K)T ZKPT
1
K
Â· Â· Â·
(cid:17)
k=1F(k)T ZkPT
K
k
(cid:107)

(cid:3)

= DE
Then, G(2) = Y(2)(W

H) is expressed as follows:

G(2) = DE

(cid:16)

(cid:12)

k=1F(k)T ZkPT
K
k
(cid:107)

(cid:17)

ï£®

ï£¯
ï£°

W(1, 1)H(:, 1);
...
W(K, 1)H(:, 1);
G(2)(:, r) is equal to DE (cid:80)K
according to the above equation.

k=1

Ã—

Â· Â· Â·
...

; W(1, R)H(:, R)
...
; W(K, R)H(:, R)

ï£¹

ï£º
ï£»

Â· Â· Â·
(cid:0)W(k, r)F(k)T ZkPT

k H(:, r)(cid:1)

As shown in Fig. 6, we compute G(2)
H)
column by column. After computing G(2), we update V
by computing G(2)(WT W
HT H)â€  (lines 16 and 17 in
Algorithm 3).

Y(2)(W

â†

(cid:12)

âˆ—

Updating W. In computing Y(3)(V

HT H)â€ ,
H) based on Lemma 3. As
we efï¬ciently compute Y(3)(V
in updating H and V, a naive computation for Y(3)(V
H)
requires a high computational cost O(JKR2). We compute
H) with the cost O(JR2+KR3) based on Lemma 3.
Y(3)(V

H)(VT V

(cid:12)

(cid:12)

(cid:12)

âˆ—

(cid:12)

ğ!ğ™!"ğ…(!)ğ%ğ™%"ğ…(%)ğ–(1,ğ‘Ÿ)ğ„ğƒ"ğ•(:,ğ‘Ÿ)ğ†ğŸ(:,ğ‘Ÿ)ğ–(ğ¾,ğ‘Ÿ)ğ†ğŸâˆˆâ„â€™Ã—â€™, ğ–âˆˆâ„%Ã—â€™, ğ)ğ™)"ğ…())âˆˆâ„â€™Ã—â€™, ğ„ğƒ"ğ•âˆˆâ„â€™Ã—â€™ğ…!"ğ™!ğ!"ğƒğ„ğ‡(:,ğ‘Ÿ)ğ†ğŸ(:,ğ‘Ÿ)ğ–(ğ¾,ğ‘Ÿ)ğ†ğŸâˆˆâ„$Ã—&, ğƒğ„âˆˆâ„$Ã—&,ğ–âˆˆâ„â€™Ã—&, ğ…("ğ™(ğ("âˆˆâ„&Ã—&, ğ‡âˆˆâ„&Ã—&ğ–(1,ğ‘Ÿ)ğ…â€™"ğ™â€™ğâ€™"Algorithm 4: Careful distribution of work in DPAR2
Input: the number T of threads, Xk âˆˆ RIkÃ—J for k = 1, ..., K
Output: sets Ti for i = 1, ..., T .
1: initialize Ti â† âˆ… for i = 1, ..., T .
2: construct a list S of size T whose elements are zero
3: construct a list Linit containing the number of rows of Xk for

k = 1, ..., K

4: sort Linit in descending order, and obtain lists Lval and Lind
that contain sorted values and those corresponding indices

5: for k = 1, ..., K do
tmin â† argmin S
6:
l â† Lind[k]
7:
Ttmin â† Ttmin âˆª {Xl}
8:
S[tmin] â† S[tmin] + Lval[k]
9:
10: end for

(cid:107)

(cid:107)

âˆ’

âˆ’

âˆ’

k=1 (cid:107)

PkZT

HSkVT

k=1 (cid:107)
HSkVT

k F(k)EDT

AkF(k)EDT
(cid:107)

k=1 IkJR) and O((cid:80)K
Ë†Xk
Xk
2
F, we derive

and Ë†Xk = QkHSkVT . With this idea, we improve the efï¬-
ciency by computing (cid:80)K
2
F,
(cid:107)
not the reconstruction errors. Our computation requires the
time O(JKR2) and space costs O(JKR) which are much
lower than the costs O((cid:80)K
k=1 IkJ) of
naively computing (cid:80)K
2
F, respectively. From
(cid:107)
k F(k)EDT
PkZT
âˆ’
(cid:107)
Ë†Xk
2
F. Since the Frobenius norm is unitarily invariant, we
modify the computation as follows:
HSkVT
k F(k)EDT
QkPkZT
âˆ’
AkZkPT
k F(k)EDT
2
AkF(k)EDT
=
F
(cid:107)
âˆ’
(cid:107)
RRÃ—R since
where PT
k Pk and ZkZT
k are equal
Pk and Zk are orthonormal matrices. Note that the size of
PkZT
J which is much smaller
J of input slices Xk. This modiï¬cation
than the size Ik
completes the efï¬ciency of our update rule.

âˆ’
k F(k)EDT
k PkZT

k F(k)EDT and HSkVT is R

2
F
(cid:107)
QkHSkVT

PkZT
(cid:107)
=

QkHSkVT

2
F
(cid:107)

to I

Ë†Xk

=

Ã—

âˆ’

Ã—

2
F

âˆˆ

(cid:107)

(cid:107)

(cid:107)

F. Careful distribution of work

The last challenge for an efï¬cient and scalable PARAFAC2
decomposition method is how to parallelize the computations
described in Sections III-B to III-E. Although a previous
work [11] introduces the parallelization with respect to the K
slices, there is still a room for maximizing parallelism. Our
main idea is to carefully allocate input slices Xk to threads
by considering the irregularity of a given tensor.

The most expensive operation is to compute randomized
SVD of input slices Xk for all k; thus we ï¬rst focus on
how well we parallelize this computation (i.e., lines 2 to 4 in
Algorithm 3). A naive approach is to randomly allocate input
slices to threads, and let each thread compute randomized SVD
of the allocated slices. However, the completion time of each
thread can vary since the computational cost of computing
randomized SVD is proportional to the number of rows of
slices; the number of rows of input slices is different from each
other as shown in Fig. 8. Therefore, we need to distribute Xk
fairly across each thread considering their numbers of rows.
For i = 1, .., T , consider that an ith thread performs
randomized SVD for slices in a set Ti where T is the number
of threads. To reduce the completion time, the sums of rows

Fig. 7. Computation for G(3) = Y(3)(V (cid:12) H). G(3)(k, r) is computed
by (cid:0)vec (cid:0)PkZT
Exploiting the factorized matrices of Y(3) and carefully de-
termining the order of computations improves the efï¬ciency.

k F(k)(cid:1)(cid:1)T (cid:0)EDT V(:, r) âŠ— H(:, r)(cid:1).

3.

us

Let

Lemma
with G(3)
(cid:0)vec (cid:0)PkZT
âŠ—
denotes the vectorization of a matrix.

k F(k)(cid:1)(cid:1)T (cid:0)EDT V(:, r)

âˆˆ

RKÃ—R. G(3)(k, r)

H)
is
to
H(:, r)(cid:1) where vec(
)
Â·
(cid:3)

(cid:12)
equal

denote Y(3)(V

ï£®

Y(3) =

Proof. Y(3) is represented as follows:
(cid:0)vec (cid:0)P1ZT
1 F(1)EDT (cid:1)(cid:1)T
...
ï£¯
ï£¯
ï£°
KF(K)EDT (cid:1)(cid:1)T
(cid:0)vec (cid:0)PKZT
(cid:16)
(cid:16)
PkZT

=

(cid:16)

ï£¹

ï£º
ï£º
ï£»

(cid:16)

=

(cid:16)

vec

K
k=1
(cid:107)
K
k=1(DE
(cid:107)
(cid:16)
K
k=1
(cid:107)

(cid:16)

k F(k)EDT (cid:17)(cid:17)(cid:17)T
k F(k)(cid:17)(cid:17)T
PkZT
k F(k)(cid:17)(cid:17)(cid:17)T (cid:0)EDT

I) vec

âŠ—
(cid:16)
PkZT

(cid:1)

âŠ—

IRÃ—R
R. The property
I)vec(A)) is

=

vec
where IRÃ—R is the identity matrix of size R
Ã—
of the vectorization (i.e., vec(AB) = (BT
used. Then, G(3) = Y(3)(V
(cid:16)
G(3) =

K
k=1

(cid:16)
(cid:107)
R
r=1
(cid:107)

(cid:0)

Ã—

(cid:12)
vec

âŠ—
H) is expressed as follows:
(cid:16)
k F(k)(cid:17)(cid:17)(cid:17)T
PkZT
(cid:0)EDT V(:, r)
H(:, r)(cid:1)(cid:1)
âŠ—
k F(k)(cid:1)(cid:1)T (cid:0)EDT V(:, r)

âŠ—

H(:, r)(cid:1)

G(3)(k, r) is (cid:0)vec (cid:0)PkZT
according to the above equation.
We compute G(3) = Y(3)(V
H) row by row. Fig. 7 shows
(cid:12)
how we compute G(3)(k, r). In computing G(3), we ï¬rst
compute EDT V, and then obtain G(3)(k, :) for all k (line 18
in Algorithm 3). After computing G(3), we update W by
computing G(3)(VT V
denotes the Moore-
Penrose pseudoinverse (line 19 in Algorithm 3). We obtain Sk
whose diagonal elements correspond to the kth row vector of
W (line 21 in Algorithm 3).

HT H)â€  where

âˆ—

â€ 

â†

After convergence, we obtain the factor matrices, (Uk

AkZkPT

k H = QkH), Sk, and V (line 25 in Algorithm 3).

(cid:17)

Xk

(cid:16)(cid:80)K

Convergence Criterion. At the end of each iteration, we
determine whether to stop or not (line 23 in Algorithm 3) based
where Ë†Xk =
on the variation of e =
k=1 (cid:107)
QkHSkVT is the kth reconstructed slice. However, measuring
reconstruction errors (cid:80)K
2
Xk
F is inefï¬cient since it
(cid:107)
requires high time and space costs proportional to input slices
Xk. To efï¬ciently verify the convergence, our idea is to exploit
AkF(k)EDT instead of Xk, since the objective of our update
process is to minimize the difference between AkF(k)EDT

k=1 (cid:107)

2
F
(cid:107)

Ë†Xk

Ë†Xk

âˆ’

âˆ’

ğ†ğŸ‘(ğ‘˜,ğ‘Ÿ)ğ„ğƒ"ğ•(:,ğ‘Ÿ)ğ†ğŸ‘âˆˆâ„#Ã—%, ğ&ğ™&"ğ…(&)âˆˆâ„%Ã—%,ğ„ğƒ"ğ•âˆˆâ„%Ã—%,ğ‡âˆˆâ„%Ã—%ğ“‹â„¯ğ’¸ğ&ğ™&"ğ…&"ğ‡(:,ğ‘Ÿ)(a) US stock data

(b) KR stock data

Fig. 8. The length of temporal dimension of input slices Xk on US Stock
and Korea Stock data. We sort the lengths in descending order.
of slices in the sets should be nearly equal to each other. To
achieve it, we exploit a greedy number partitioning technique
that repeatedly adds a slice into a set with the smallest sum
of rows. Algorithm 4 describes how to construct the sets Ti
for compressing input slices in parallel. Let Linit be a list
containing the number of rows of Xk for k = 1, ..., K (line 3
in Algorithm 4). We ï¬rst obtain lists Lval and Lind, sorted
values and those corresponding indices, by sorting Linit in
descending order (line 4 in Algorithm 4). We repeatedly add
a slice Xk to a set Ti that has the smallest sum. For each
k, we ï¬nd the index tmin of the minimum in S whose ith
element corresponds to the sum of row sizes of slices in the
ith set Ti (line 6 in Algorithm 4). Then, we add a slice Xl to
the set Ttmin where l is equal to Lind[k], and update the list S
S[tmin] + Lval[k] (lines 7 to 9 in Algorithm 4).
by S[tmin]
Note that S[k], Lind[k], and Lval[k] denote the kth element of
S, Lind, and Lval, respectively. After obtaining the sets Ti for
i = 1, .., T , ith thread performs randomized SVD for slices in
the set Ti.

â†

After decomposing Xk for all k, we do not need to
consider the irregularity for parallelism since there is no com-
putation with Ak which involves the irregularity. Therefore,
we uniformly allocate computations across threads for all
k slices. In each iteration (lines 8 to 22 in Algorithm 3),
we easily parallelize computations. First, we parallelize the
iteration (lines 8 to 10) for all k slices. To update H,
V, and W, we need to compute G(1), G(2), and G(3)
in parallel. In Lemmas 1 and 2, DPAR2 parallelly com-
putes W(k, r) (cid:0)PkZT
k H(:, r)
for k, respectively. In Lemma 3, DPAR2 parallelly computes
(cid:0)vec (cid:0)PkZT

k F(k)(cid:1) and W(k, r)F(k)ZkPT

k F(k)(cid:1)(cid:1)T (cid:0)EDT V(:, r)

H(:, r)(cid:1) for k.

âŠ—

G. Complexities

We analyze the time complexity of DPAR2.

Lemma
(cid:16)(cid:16)(cid:80)K
O

4.

(cid:17)

Compressing
+ JKR2(cid:17)

time.

input

slices

takes

k=1 IkJR

(cid:16)(cid:80)K

(cid:17)

Proof. The SVD in the ï¬rst stage takes O
k=1 IkJR
since computing randomized SVD of Xk
takes
times
O(IkJR) time. Then, the SVD in the second stage takes
O (cid:0)JKR2(cid:1) due to randomized SVD of M(2) âˆˆ
RJÃ—KR.
Therefore, the time complexity of the SVD in the two stages
+ JKR2(cid:17)
is O

(cid:16)(cid:16)(cid:80)K

(cid:17)

.

k=1 IkJR

TABLE II
DESCRIPTION OF REAL-WORLD TENSOR DATASETS.

Dataset

Max Dim. Ik

Dim. J

Dim. K

Summary

FMA1 [26]
Urban2 [27]
US Stock3
Korea Stock4 [3]
Activity5 [28], [29]
Action5 [28], [29]
Trafï¬c6 [30]
PEMS-SF7

704
174
7, 883
5, 270
553
936
2, 033
963

2, 049
2, 049
88
88
570
570
96
144

7, 997
8, 455
4, 742
3, 664
320
567
1, 084
440

music
urban sound
stock
stock
video feature
video feature
trafï¬c
trafï¬c

Proof. For Yk, computing F(k)EDT VSkHT and performing
SVD of it for all k take O(JR2 +KR3). Updating each of H,
V, and W takes O(JR2 + KR3 + R3) time. Therefore, the
complexity for Yk, H, V, and W is O (cid:0)JR2 + KR3(cid:1).
is
of DPAR2
Theorem 1. The
O
where M is

complexity
+ JKR2 + M KR3(cid:17)

(cid:16)(cid:16)(cid:80)K

time

(cid:17)

k=1 IkJR
the number of iterations.

complexity

time
of

overall
summation

Proof. The
DPAR2
the
is
(see
cost
Lemma 4)
(see Lemma 5):
(cid:16)(cid:16)(cid:80)K
O
Note
.
that M JR2 term is omitted since it is much smaller than
(cid:16)(cid:80)K

+ JKR2 + M (JR2 + KR3)

iteration cost

and the
(cid:17)

compression

k=1 IkJR

the

of

(cid:17)

(cid:17)

k=1 IkJR

and JKR2.

Theorem 2. The size of preprocessed data of DPAR2 is
O

(cid:16)(cid:16)(cid:80)K

+ KR2 + JR

(cid:17)

(cid:17)

.

k=1 IkR

Proof. The size of preprocessed data of DPAR2 is proportional
to the size of E, D, Ak, and F(k) for k = 1, ..., K. The
R, respectively. For each
size of E and D is R and J
k,
R, respec-
tively. Therefore, the size of preprocessed data of DPAR2 is
O

the size of A and F is Ik

R and R

(cid:16)(cid:16)(cid:80)K

+ KR2 + JR

Ã—

Ã—

Ã—

(cid:17)

(cid:17)

.

k=1 IkR

IV. EXPERIMENTS

In this section, we experimentally evaluate the performance

of DPAR2. We answer the following questions:
Q1 Performance (Section IV-B). How quickly and accu-
rately does DPAR2 perform PARAFAC2 decomposition
compared to other methods?

Q2 Data Scalability (Section IV-C). How well does DPAR2
scale up with respect to tensor size and target rank?
Q3 Multi-core Scalability (Section IV-D). How much does
the number of threads affect the running time of DPAR2?
Q4 Discovery (Section IV-E). What can we discover from

real-world tensors using DPAR2?

A. Experimental Settings

We describe experimental settings for the datasets, competi-

tors, parameters, and environments.

Machine. We use a workstation with 2 CPUs (Intel Xeon
E5-2630 v4 @ 2.2GHz), each of which has 10 cores, and
512GB memory for the experiments.

Lemma 5. At each iteration, computing Yk and updating H,
V, and W takes O(JR2 + KR3) time.

Real-world Data. We evaluate the performance of DPAR2
and competitors on real-world datasets summarized in Table II.

020004000SortedStockIndex05000TimeLength02000SortedStockIndex020004000TimeLengthFig. 9.
each iteration, DPAR2 runs by up to 10.3Ã— faster than the second best method.

[Best viewed in color] (a) DPAR2 efï¬ciently preprocesses a given irregular dense tensor, which is up to 10Ã— faster compared to RD-ALS. (b) At

(a) Preprocessing time

(b) Iteration time

competitors on synthetic tensors. Given the number K of
slices, and the slice sizes I and J, we generate a synthetic
tensor using tenrand(I, J, K) function in Tensor Toolbox [31],
RIÃ—JÃ—K. We con-
which randomly generates a tensor X
âˆˆ
k=1 where Xk is equal to X(:, :, k) for
K
Xk
struct a tensor
}
{
k = 1, ...K.

Competitors. We compare DPAR2 with PARAFAC2 de-
composition methods based on ALS. All the methods includ-
ing DPAR2 are implemented in MATLAB (R2020b).

â€¢ DPAR2: the proposed PARAFAC2 decomposition model
which preprocesses a given irregular dense tensor and
updates factor matrices using the preprocessing result.
â€¢ RD-ALS [18]: PARAFAC2 decomposition which prepro-
cesses a given irregular tensor. Since there is no public
code, we implement it using Tensor Toolbox [31] based
on its paper [18].

â€¢ PARAFAC2-ALS: PARAFAC2 decomposition based on
ALS approach. It is implemented based on Algorithm 2
using Tensor Toolbox [31].

â€¢ SPARTan [11]: fast and scalable PARAFAC2 decompo-
sition for irregular sparse tensors. Although it targets on
sparse irregular tensors, it can be adapted to irregular
dense tensors. We use the code implemented by authors8.

Parameters. We use the following parameters.
â€¢ Number of threads: we use 6 threads except in Sec-

tion IV-D.

â€¢ Max number of iterations: the maximum number of

iterations is set to 32.

â€¢ Rank: we set the target rank R to 10 except in the trade-
off experiments of Section IV-B and Section IV-D. We
also set the rank of randomized SVD to 10 which is the
same as the target rank R of PARAFAC2 decomposition.
To compare running time, we run each method 5 times, and
report the average.

Fitness. We evaluate the ï¬tness deï¬ned as follows:

(cid:32) (cid:80)K

Xk

k=1 (cid:107)
(cid:80)K

1

âˆ’

(cid:33)

2
F
(cid:107)

âˆ’
Xk

Ë†Xk
2
F
(cid:107)

k=1 (cid:107)
where Xk is the k-th input slice and Ë†Xk is the k-th recon-
structed slice of PARAFAC2 decomposition. Fitness close to 1
indicates that a model approximates a given input tensor well.

8https://github.com/kperros/SPARTan

Fig. 10. The size of preprocessed data. DPAR2 generates up to 201Ã— smaller
preprocessed data than input tensors used for SPARTan and PARAFAC2-ALS.
FMA dataset1 [26] is the collection of songs. Urban Sound
dataset2 [27] is the collection of urban sounds such as drilling,
siren, and street music. For the two datasets, we convert each
time series into an image of a log-power spectrogram so
that their forms are (time, frequency, song; value) and (time,
frequency, sound; value), respectively. US Stock dataset3 is
the collection of stocks on the US stock market. Korea Stock
dataset4 [3] is the collection of stocks on the South Korea
stock market. Each stock is represented as a matrix of (date,
feature) where the feature dimension includes 5 basic features
and 83 technical indicators. The basic features collected daily
are the opening, the closing, the highest, and the lowest prices
and trading volume, and technical indicators are calculated
based on the basic features. The two stock datasets have the
form of (time, feature, stock; value). Activity data5 and Action
data5 are the collection of features for motion videos. The two
datasets have the form of (frame, feature, video; value). We
refer the reader to [28] for their feature extraction. Trafï¬c
data6 is the collection of trafï¬c volume around Melbourne,
and its form is (sensor, frequency, time; measurement). PEMS-
SF data7 contain the occupancy rate of different car lanes of
San Francisco bay area freeways: (station, timestamp, day;
measurement). Trafï¬c data and PEMS-SF data are 3-order
regular tensors, but we can analyze them using PARAFAC2
decomposition approaches.

Synthetic Data. We evaluate the scalability of DPAR2 and

1https://github.com/mdeff/fma
2https://urbansounddataset.weebly.com/urbansound8k.html
3https://datalab.snu.ac.kr/dpar2
4https://github.com/jungijang/KoreaStockData
5https://github.com/titu1994/MLSTM-FCN
6https://github.com/ï¬‚orinsch/BigTrafï¬cData
7http://www.timeseriesclassiï¬cation.com/

0.60.70.80.91.01.11.21.31.40.00.51.0DPar2RD-ALSPARAFAC2-ALSSPARTANFMAUrbanUSStockKRStockActivityActionTraï¿¿icPEMS-SFData0.1110100PreprocessingTime(sec)ğŸ”.ğŸ–Ã—ğŸğŸ.ğŸÃ—FMAUrbanUSStockKRStockActivityActionTraï¿¿icPEMS-SFData0.1110TimeperIteration(sec)ğŸğŸ.ğŸ‘Ã—ğŸ.ğŸ’Ã—ğŸ“.ğŸÃ—ğŸ‘.ğŸ–Ã—ğŸ.ğŸÃ—ğŸ.ğŸ—Ã—ğŸ‘.ğŸÃ—ğŸ.ğŸ‘Ã—FMAUrbanUSStockKRStockActivityActionTraï¿¿icPEMS-SFData0.010.1110100SizeofPreprocessedData(GB)ğŸğŸğŸÃ—ğŸğŸ–ğŸ—Ã—ğŸ–.ğŸ–Ã—ğŸ–.ğŸ–Ã—ğŸ“ğŸ‘.ğŸÃ—ğŸ’ğŸ.ğŸ—Ã—ğŸ—.ğŸ“Ã—ğŸğŸ’.ğŸÃ—0.60.70.80.91.01.11.21.31.40.00.51.0DPar2RD-ALSInputTensor(a) Scalability for tensor size

(b) Scalability for rank

(c) Machine Scalability

Fig. 11. Data scalability. DPAR2 is more scalable than other PARAFAC2 decomposition methods in terms of both tensor size and rank. (a) DPAR2 is 15.3Ã—
faster than the second-fastest method on the irregular dense tensor of the total size 1.6 Ã— 1010. (b) DPAR2 is 7.0Ã— faster than the second-fastest method even
when a high target rank is given. (c) Multi-core scalability with respect to the number of threads. TM indicates the running time of DPAR2 on the number
M of threads. DPAR2 gives near-linear scalability, and accelerates 5.5Ã— when the number of threads increases from 1 to 10.
B. Performance (Q1)

We evaluate the ï¬tness and the running time of DPAR2,

RD-ALS, SPARTan, and PARAFAC2-ALS.

Ã—

Trade-off. Fig. 1 shows that DPAR2 provides the best
trade-off of running time and ï¬tness on real-world irregular
tensors for the three target ranks: 10, 15, and 20. DPAR2
achieves 6.0
faster running time than the competitors for
FMA dataset while having a comparable ï¬tness. In addition,
DPAR2 provides at least 1.5
faster running times than the
Ã—
competitors for the other datasets. The performance gap is
large for FMA and Urban datasets whose sizes are larger than
those of the other datasets. It implies that DPAR2 is more
scalable than the competitors in terms of tensor sizes.

Preprocessing time. We compare DPAR2 with RD-ALS
and exclude SPARTan and PARAFAC2-ALS since only RD-
ALS has a preprocessing step. As shown in Fig. 9(a), DPAR2
is up to 10
faster than RD-ALS. There is a large performance
gap on FMA and Urban datasets since RD-ALS cannot avoid
the overheads for the large tensors. RD-ALS performs SVD
of the concatenated slice matrices
k , which leads to its
slow preprocessing time.

k=1XT
K

Ã—

(cid:107)

Iteration time. Fig. 9(b) shows that DPAR2 outperforms
competitors for running time at each iteration. Compared
to SPARTan and PARAFAC2-ALS, DPAR2 signiï¬cantly re-
iteration due to the small
duces the running time per
size of the preprocessed results. Although RD-ALS reduces
the computational cost at each iteration by preprocessing
a given tensor, DPAR2 is up to 10.3
faster than RD-
ALS. Compared to RD-ALS that computes the variation of
(cid:17)
(cid:16)(cid:80)K
for the convergence criterion,
DPAR2 efï¬ciently veriï¬es the convergence by computing
the variation of (cid:80)K
2
F, which
(cid:107)
affects the running time at each iteration. In summary, DPAR2
obtains Uk, Sk, and V in a reasonable running time even if
the number of iterations increases.

2
F
(cid:107)
k F(k)EDT

QkHSkVT

HSkVT

PkZT

k=1 (cid:107)

k=1 (cid:107)

Xk

âˆ’

âˆ’

Ã—

Size of preprocessed data. We measure the size of pre-
processed data on real-world datasets. For PARAFAC2-ALS
and SPARTan, we report the size of input irregular tensor
since they have no preprocessing step. Compared to an input
irregular tensor, DPAR2 generates much smaller preprocessed

Ã—

data by up to 201 times as shown in Fig. 10. Given input
slices Xk of size Ik
J, the compression ratio increases as the
number J of columns increases; the compression ratio is larger
on FMA, Urban, Activity, and Action datasets than on US
Stock, KR Stock, Trafï¬c, and PEMS-SF. This is because the
compression ratio is proportional to
Size of the preprocessed results â‰ˆ
IKR+KR2+JR =
R/J+R2/IJ+R/IK assuming I1 = ... =
IK = I; R/J is the dominant term which is much larger
than R2/IJ and R/IK.

Size of an irregular tensor

IJK

1

C. Data Scalability (Q2)

We evaluate the data scalability of DPAR2 by measuring the
running time on several synthetic datasets. We ï¬rst compare
the performance of DPAR2 and the competitors by increasing
the size of an irregular tensor. Then, we measure the running
time by changing a target rank.

J

Ã—

Ã—

1000

1000, 1000

K:
Ã—
1000

1000
{
2000, 2000

Tensor Size. To evaluate the scalability with respect to the
tensor size, we generate 5 synthetic tensors of the following
sizes I
Ã—
Ã—
2000, 2000
Ã—
Ã—
Ã—
= IK = I. Fig. 11(a)
. For simplicity, we set I1 =
4000
Â· Â· Â·
}
shows that DPAR2 is up to 15.3
faster than competitors on
all synthetic tensors; in addition, the slope of DPAR2 is lower
than that of competitors. Also note that only DPAR2 obtains
factor matrices of PARAFAC2 decomposition within a minute
for all the datasets.

Ã—
2000, 2000

1000
2000

Ã—
2000

Ã—

Ã—

Ã—

Â· Â· Â·

Rank. To evaluate the scalability with respect to rank, we
generate the following synthetic data: I1 =
= IK = 2, 000,
J = 2, 000, and K = 4, 000. Given the synthetic tensors,
we measure the running time for 5 target ranks: 10, 20, 30,
40, and 50. DPAR2 is up to 15.9
faster than the second-
fastest method with respect to rank in Fig. 11(b). For higher
ranks, the performance gap slightly decreases since DPAR2
depends on the performance of randomized SVD which is
designed for a low target rank. Still, DPAR2 is up to 7.0
Ã—
faster than competitors with respect to the highest rank used
in our experiment.

Ã—

D. Multi-core Scalability (Q3)

We generate the following synthetic data: I1 =

=
IK = 2, 000, J = 2, 000, and K = 4, 000, and evaluate the

Â· Â· Â·

1.01.52.02.53.03.54.01002Â£1003Â£100DPar2PARAFAC2-ALSSPARTanRD-ALS1091010TensorSize101102103RunningTime(sec)ğŸğŸ“.ğŸ‘Ã—1020304050Rank102103RunningTime(sec)ğŸ•.ğŸÃ—ğŸğŸ“.ğŸ—Ã—1246810NumberofThreads246ScaleUp:TM/T1Slope = 0.56(a) US stock data

(b) Korea stock data

Fig. 12. The similarity patterns of features are different on the two stock markets. (a) For US Stock data, ATR and OBV have a positive correlation with
the price features. (b) For Korea Stock data, they are uncorrelated with the price features in general.
multi-core scalability of DPAR2 with respect to the number
of threads: 1, 2, 4, 6, 8, and 10. TM indicates the running
time when using the number M of threads. As shown in
Fig. 11(c), DPAR2 gives near-linear scalability, and accelerates
5.5

patterns on the two datasets. On the US stock dataset, ATR
and OBV have a positive correlation with the price features.
On the Korea stock dataset, OBV has little correlation with the
price features. Also, ATR has little correlation with the price
features except for the closing price. These different patterns
are due to the difference of the two markets in terms of market
size, market stability, tax, investment behavior, etc.

when the number of threads increases from 1 to 10.

Ã—

E. Discoveries (Q4)

We discover various patterns using DPAR2 on real-world

datasets.

1) Feature Similarity on Stock Dataset: We measure the
similarities between features on US Stock and Korea Stock
datasets, and compare the results. We compute Pearson Cor-
relation Coefï¬cient (PCC) between V(i, :), which represents a
latent vector of the ith feature. For effective visualization, we
select 4 price features (the opening, the closing, the highest,
and the lowest prices), and 4 representative technical indicators
described as follows:

â€¢ OBV (On Balance Volume): a technical indicator for
cumulative trading volume. If todayâ€™s closing price is
higher than yesterdayâ€™s price, OBV increases by the
amount of todayâ€™s volume. If not, OBV decreases by the
amount of todayâ€™s volume.

â€¢ ATR (Average True Range): a technical indicator for
volatility developed by J. Welles Wilder, Jr. It increases
in high volatility while decreasing in low volatility.

â€¢ MACD (Moving Average Convergence and Diver-
gence): a technical
indicator for trend developed by
Gerald Appel. It indicates the difference between long-
term and short-term exponential moving averages (EMA).
â€¢ STOCH (Stochastic Oscillator): a technical indicator
for momentum developed by George Lane. It indicates
the position of the current closing price compared to the
highest and the lowest prices in a duration.

Fig. 12(a) and 12(b) show correlation heatmaps for US
Stock data and Korea Stock data, respectively. We analyze
correlation patterns between price features and technical indi-
cators. On both datasets, STOCH has a negative correlation
and MACD has a weak correlation with the price features.
On the other hand, OBV and ATR indicators have different

2) Finding Similar Stocks: On US Stock dataset, which
stock is similar to a target stock sT in a time range that a
user is curious about? In this section, we provide analysis by
setting the target stock sT to Microsoft (Ticker: MSFT), and
the range a duration when the COVID-19 was very active
(Jan. 2, 2020 - Apr. 15, 2021). We efï¬ciently answer the
question by 1) constructing the tensor included in the range, 2)
obtaining factor matrices with DPAR2, and 3) post-processing
the factor matrices of DPAR2. Since Uk represents temporal
latent vectors of the kth stock,
the similarity sim(si, sj)
between stocks si and sj is computed as follows:
(cid:1)
2
F

(10)
âˆ’
where exp is the exponential function. We set Î³ to 0.01 in
this section. Note that we use only the stocks that have the
Usj is deï¬ned only when the
same target range since Usi âˆ’
two matrices are of the same size.

sim(si, sj) = exp (cid:0)

Usi âˆ’
(cid:107)

Usj (cid:107)

Î³

Based on sim(si, sj), we ï¬nd similar stocks to sT using two
different techniques: 1) k-nearest neighbors, and 2) Random
Walks with Restarts (RWR). The ï¬rst approach simply ï¬nds
stocks similar to the target stock, while the second one ï¬nds
similar stocks by considering the multi-faceted relationship
between stocks.

k-nearest neighbors. We compute sim(sT , sj) for j =
1, ..., K where K is the number of stocks to be compared, and
ï¬nd top-10 similar stocks to sT , Microsoft (Ticker: MSFT).
In Table III(a), the Microsoft stock is similar to stocks of
the Technology sector or with a large capitalization (e.g.,
Amazon.com, Apple, and Alphabet) during the COVID-19.
Moodyâ€™s is also similar to the target stock.

Random Walks with Restarts (RWR). We ï¬nd similar
stocks using another approach, Random Walks with Restarts
(RWR) [32]â€“[35]. To exploit RWR, we ï¬rst a similarity graph

OPENINGHIGHESTLOWESTCLOSINGATRSTOCHOBVMACDOPENINGHIGHESTLOWESTCLOSINGATRSTOCHOBVMACDOPENINGHIGHESTLOWESTCLOSINGATRSTOCHOBVMACDOPENINGHIGHESTLOWESTCLOSINGATRSTOCHOBVMACDTABLE III
BASED ON THE RESULTS OF DPAR2, WE FIND SIMILAR STOCKS TO MICROSOFT (MSFT) DURING THE COVID-19. (A) TOP-10 STOCKS FROM
k-NEAREST NEIGHBORS. (B) TOP-10 STOCKS FROM RWR. THE BLUE COLOR REFERS TO THE STOCKS THAT APPEAR ONLY IN ONE OF THE TWO
APPROACHES AMONG THE TOP-10 STOCKS.

(a) Similarity based Result

(b) RWR Result

Rank

Stock Name

Sector

Rank

Stock Name

Sector

1
2
3
4
5
6
7
8
9
10

Adobe
Amazon.com
Apple
Moodyâ€™s
Intuit
ANSYS
Synopsys
Alphabet
ServiceNow
EPAM Systems

Technology
Consumer Cyclical
Technology
Financial Services
Technology
Technology
Technology
Communication Services
Technology
Technology

1
2
3
4
5
6
7
8
9
10

Synopsys
ANSYS
Adobe
Amazon.com
Netï¬‚ix
Autodesk
Apple
Moodyâ€™s
NVIDIA
S&P Global

Technology
Technology
Technology
Consumer Cyclical
Communication Services
Technology
Technology
Financial Services
Technology
Financial Services

based on the similarities between stocks. The elements of the
adjacency matrix A of the graph is deï¬ned as follows:

(cid:40)

A(i, j) =

sim(si, sj)
0

if i
= j
if i = j

(11)

We ignore self-loops by setting A(i, i) to 0 for i = 1, ..., K.
After constructing the graph, we ï¬nd similar stocks using
RWR. The scores r is computed by using the power itera-
tion [36] as described in [37]:

âˆ’

(1

â†

r(i)

c) ËœAT r(iâˆ’1) + cq

(12)
where ËœA is the row-normalized adjacency matrix, r(i) is the
score vector at the ith iteration, c is a restart probability,
and q is a query vector. We set c to 0.15, the maximum
iteration to 100, and q to the one-hot vector where the element
corresponding to Microsoft is 1, and the others are 0.

As shown in Table III, the common pattern of the two
approaches is that many stocks among the top-10 belong to
the technology sector. There is also a difference. In Table III,
the blue color indicates the stocks that appear only in one
of the two approaches among the top-10. In Table III(a), the
k-nearest neighbor approach simply ï¬nds the top-10 stocks
which are closest to Microsoft based on distances. On the
other hand, the RWR approach ï¬nds the top-10 stocks by
considering more complicated relationships. There are 4 stocks
appearing only in Table III(b). S&P Global is included since it
is very close to Moodyâ€™s which is ranked 4th in Table III(a).
Netï¬‚ix, Autodesk, and NVIDIA are relatively far from the
target stock compared to stocks such as Intuit and Alphabet,
but they are included in the top-10 since they are very close to
Amazon.com, Adobe, ANSYS, and Synopsys. This difference
comes from the fact that the k-nearest neighbors approach
considers only distances from the target stock while the RWR
approach considers distances between other stocks in addition
to the target stock.

DPAR2 allows us to efï¬ciently obtain factor matrices, and

ï¬nd interesting patterns in data.

V. RELATED WORKS

We review related works on tensor decomposition methods

for regular and irregular tensors.

Tensor decomposition on regular dense tensors. There
are efï¬cient tensor decomposition methods on regular dense
tensors. Pioneering works [38]â€“[43] efï¬ciently decompose a
regular tensor by exploiting techniques that reduce time and
space costs. Also, a lot of works [44]â€“[47] proposed scalable
tensor decomposition methods with parallelization to handle
large-scale tensors. However, the aforementioned methods fail
to deal with the irregularity of dense tensors since they are
designed for regular tensors.

PARAFAC2 decomposition on irregular tensors. Cheng
and Haardt [18] proposed RD-ALS which preprocesses a
given tensor and performs PARAFAC2 decomposition using
the preprocessed result. However, RD-ALS requires high
computational costs to preprocess a given tensor. Also, RD-
ALS is less efï¬cient
in updating factor matrices since it
computes reconstruction errors for the convergence criterion
at each iteration. Recent works [11], [12], [15] attempted
to analyze irregular sparse tensors. SPARTan [11] is a scal-
able PARAFAC2-ALS method for large electronic health
records (EHR) data. COPA [12] improves the performance
of PARAFAC2 decomposition by applying various constraints
(e.g., smoothness). REPAIR [15] strengthens the robustness of
PARAFAC2 decomposition by applying low-rank regulariza-
tion. We do not compare DPAR2 with COPA and REPAIR
since they concentrate on imposing practical constraints to
handle irregular sparse tensors, especially EHR data. However,
we do compare DPAR2 with SPARTan which the efï¬ciency
of COPA and REPAIR is based on. TASTE [16] is a joint
PARAFAC2 decomposition method for large temporal and
static tensors. Although the above methods are efï¬cient in
PARAFAC2 decomposition for irregular tensors, they concen-
trate only on irregular sparse tensors, especially EHR data.
LogPar [17], a logistic PARAFAC2 decomposition method, an-
alyzes temporal binary data represented as an irregular binary
tensor. SPADE [48] efï¬ciently deals with irregular tensors in
a streaming setting. TedPar [49] improves the performance of
PARAFAC2 decomposition by explicitly modeling the tempo-
ral dependency. Although the above methods effectively deal
with irregular sparse tensors, especially EHR data, none of
them focus on devising an efï¬cient PARAFAC2 decomposition

(cid:54)
method on irregular dense tensors. On the other hand, DPAR2
is a fast and scalable PARAFAC2 decomposition method for
irregular dense tensors.

VI. CONCLUSION

In this paper, we propose DPAR2, a fast and scalable
PARAFAC2 decomposition method for irregular dense tensors.
By compressing an irregular input tensor, careful reordering of
the operations with the compressed results in each iteration,
and careful partitioning of input slices, DPAR2 successfully
achieves high efï¬ciency to perform PARAFAC2 decomposi-
tion for irregular dense tensors. Experimental results show
that DPAR2 is up to 6.0
faster than existing PARAFAC2
decomposition methods while achieving comparable accuracy,
and it is scalable with respect to the tensor size and target rank.
With DPAR2, we discover interesting patterns in real-world
irregular tensors. Future work includes devising an efï¬cient
PARAFAC2 decomposition method in a streaming setting.

Ã—

ACKNOWLEDGMENT

of

funded

funded

Foundation

Korea(NRF)

This work was partly supported by the National
Research
by
MSIT(2022R1A2C3007921), and Institute of Information &
communications Technology Planning & Evaluation(IITP)
grant
by MSIT [No.2021-0-01343, Artiï¬cial
Intelligence Graduate School Program (Seoul National
University)] and [NO.2021-0-02068, Artiï¬cial Intelligence
Institute, Seoul
Innovation Hub (Artiï¬cial
National University)]. The Institute of Engineering Research
and ICT at Seoul National University provided research
facilities for this work. U Kang is the corresponding author.

Intelligence

REFERENCES

[1] Y.-R. Lin, J. Sun, P. Castro, R. Konuru, H. Sundaram, and A. Kelliher,
â€œMetafac: community discovery via relational hypergraph factorization,â€
in KDD, 2009, pp. 527â€“536.

[2] S. Spiegel, J. Clausen, S. Albayrak, and J. Kunegis, â€œLink prediction on
evolving data using tensor factorization,â€ in PAKDD. Springer, 2011,
pp. 100â€“110.

[3] J.-G. Jang and U. Kang, â€œFast and memory-efï¬cient tucker decom-
position for answering diverse time range queries,â€ in Proceedings of
the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining, 2021, pp. 725â€“735.

[4] S. Oh, N. Park, J.-G. Jang, L. Sael, and U. Kang, â€œHigh-performance
tucker factorization on heterogeneous platforms,â€ IEEE Transactions on
Parallel and Distributed Systems, vol. 30, no. 10, pp. 2237â€“2248, 2019.
[5] T. Kwon, I. Park, D. Lee, and K. Shin, â€œSlicenstitch: Continuous cp
IEEE, 2021, pp.

decomposition of sparse tensor streams,â€ in ICDE.
816â€“827.

[6] D. Ahn, S. Kim, and U. Kang, â€œAccurate online tensor factorization
for temporal tensor streams with missing values,â€ in CIKM â€™21: The
30th ACM International Conference on Information and Knowledge
Management, Virtual Event, Queensland, Australia, November 1 - 5,
2021, G. Demartini, G. Zuccon, J. S. Culpepper, Z. Huang, and H. Tong,
Eds. ACM, 2021, pp. 2822â€“2826.

[7] D. Ahn, J.-G. Jang, and U. Kang, â€œTime-aware tensor decomposition

for sparse tensors,â€ Machine Learning, Sep 2021.

[8] D. Ahn, S. Son, and U. Kang, â€œGtensor: Fast and accurate tensor
analysis system using gpus,â€ in CIKM â€™20: The 29th ACM International
Conference on Information and Knowledge Management, Virtual Event,
Ireland, October 19-23, 2020, M. dâ€™Aquin, S. Dietze, C. Hauff, E. Curry,
and P. CudrÂ´e-Mauroux, Eds. ACM, 2020, pp. 3361â€“3364.

[9] D. Choi, J.-G. Jang, and U. Kang, â€œS3cmtf: Fast, accurate, and scalable
method for incomplete coupled matrix-tensor factorization,â€ PLOS ONE,
vol. 14, no. 6, pp. 1â€“20, 06 2019.

[10] N. Park, S. Oh, and U. Kang, â€œFast and scalable method for distributed

boolean tensor factorization,â€ The VLDB Journal, Mar 2019.

[11] I. Perros, E. E. Papalexakis, F. Wang, R. W. Vuduc, E. Searles,
M. Thompson, and J. Sun, â€œSpartan: Scalable PARAFAC2 for large
& sparse data,â€ in SIGKDD. ACM, 2017, pp. 375â€“384.

[12] A. Afshar, I. Perros, E. E. Papalexakis, E. Searles, J. C. Ho, and J. Sun,
â€œCOPA: constrained PARAFAC2 for sparse & large datasets,â€ in CIKM.
ACM, 2018, pp. 793â€“802.

[13] N. E. Helwig, â€œEstimating latent trends in multivariate longitudinal
data via parafac2 with functional and structural constraints,â€ Biometrical
Journal, vol. 59, no. 4, pp. 783â€“803, 2017.

[14] B. M. Wise, N. B. Gallagher, and E. B. Martin, â€œApplication of parafac2
to fault detection and diagnosis in semiconductor etch,â€ Journal of
Chemometrics: A Journal of the Chemometrics Society, vol. 15, no. 4,
pp. 285â€“298, 2001.

[15] Y. Ren, J. Lou, L. Xiong, and J. C. Ho, â€œRobust

irregular tensor
factorization and completion for temporal health data analysis,â€ in
CIKM. ACM, 2020, pp. 1295â€“1304.

[16] A. Afshar, I. Perros, H. Park, C. Deï¬lippi, X. Yan, W. Stewart, J. Ho, and
J. Sun, â€œTaste: Temporal and static tensor factorization for phenotyping
electronic health records,â€ in Proceedings of the ACM Conference on
Health, Inference, and Learning, 2020, pp. 193â€“203.

[17] K. Yin, A. Afshar, J. C. Ho, W. K. Cheung, C. Zhang, and J. Sun,
â€œLogpar: Logistic parafac2 factorization for temporal binary data with
missing values,â€ in SIGKDD, 2020, pp. 1625â€“1635.

[18] Y. Cheng and M. Haardt, â€œEfï¬cient computation of the PARAFAC2

decomposition,â€ in ACSCC.

IEEE, 2019, pp. 1626â€“1630.

[19] T. G. Kolda and B. W. Bader, â€œTensor decompositions and applications,â€

SIAM Review, vol. 51, no. 3, pp. 455â€“500, 2009.

[20] N. Halko, P. Martinsson, and J. A. Tropp, â€œFinding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix
decompositions,â€ SIAM Review, vol. 53, no. 2, pp. 217â€“288, 2011.
[21] F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert, â€œA fast randomized al-
gorithm for the approximation of matrices,â€ Applied and Computational
Harmonic Analysis, vol. 25, no. 3, pp. 335â€“366, 2008.

[22] K. L. Clarkson and D. P. Woodruff, â€œLow-rank approximation and

regression in input sparsity time,â€ JACM, vol. 63, no. 6, p. 54, 2017.

[23] R. A. Harshman, â€œParafac2: Mathematical and technical notes,â€ UCLA
working papers in phonetics, vol. 22, no. 3044, p. 122215, 1972.
[24] H. A. Kiers, J. M. Ten Berge, and R. Bro, â€œParafac2â€”part i. a direct
ï¬tting algorithm for the parafac2 model,â€ Journal of Chemometrics: A
Journal of the Chemometrics Society, vol. 13, no. 3-4, pp. 275â€“294,
1999.

[25] G. H. Golub and C. F. Van Loan, Matrix computations.

JHU press,

2013, vol. 3.

[26] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, â€œFMA:
A dataset for music analysis,â€ in ISMIR, 2017. [Online]. Available:
https://arxiv.org/abs/1612.01840

[27] J. Salamon, C. Jacoby, and J. P. Bello, â€œA dataset and taxonomy for urban
sound research,â€ in Proceedings of the ACM International Conference
on Multimedia, MM â€™14, Orlando, FL, USA, November 03 - 07, 2014.
ACM, 2014, pp. 1041â€“1044.

[28] J. Wang, Z. Liu, Y. Wu, and J. Yuan, â€œMining actionlet ensemble for
action recognition with depth cameras,â€ in 2012 IEEE Conference on
Computer Vision and Pattern Recognition, Providence, RI, USA, June
16-21, 2012.

IEEE Computer Society, 2012, pp. 1290â€“1297.

[29] F. Karim, S. Majumdar, H. Darabi, and S. Harford, â€œMultivariate lstm-

fcns for time series classiï¬cation,â€ 2018.

[30] F. Schimbinschi, X. V. Nguyen, J. Bailey, C. Leckie, H. Vu, and
R. Kotagiri, â€œTrafï¬c forecasting in complex urban networks: Leveraging
big data and machine learning,â€ in Big Data (Big Data), 2015 IEEE
International Conference on.

IEEE, 2015, pp. 1019â€“1024.

[31] B. W. Bader, T. G. Kolda et al., â€œMatlab tensor toolbox version
[Online]. Available: https:

3.0-dev,â€ Available online, Oct. 2017.
//www.tensortoolbox.org

[32] J. Jung, J. Yoo, and U. Kang, â€œSigned random walk diffusion for
effective representation learning in signed graphs,â€ PLOS ONE, vol. 17,
no. 3, pp. 1â€“19, 03 2022.

[33] J. Jung, W. Jin, H. Park, and U. Kang, â€œAccurate relational reasoning in
edge-labeled graphs by multi-labeled random walk with restart,â€ World
Wide Web, vol. 24, no. 4, pp. 1369â€“1393, 2021.

[34] J. Jung, W. Jin, and U. Kang, â€œRandom walk-based ranking in signed
social networks: model and algorithms,â€ Knowl. Inf. Syst., vol. 62, no. 2,
pp. 571â€“610, 2020.

[35] W. Jin, J. Jung, and U. Kang, â€œSupervised and extended restart in random
walks for ranking and link prediction in networks,â€ PLOS ONE, vol. 14,
no. 3, pp. 1â€“23, 03 2019.

[36] L. Page, S. Brin, R. Motwani, and T. Winograd, â€œThe pagerank citation
ranking: Bringing order to the web.â€ Stanford InfoLab, Tech. Rep., 1999.
[37] J. Jung, N. Park, S. Lee, and U. Kang, â€œBepi: Fast and memory-efï¬cient
method for billion-scale random walk with restart,â€ in Proceedings of
the 2017 ACM International Conference on Management of Data, 2017,
pp. 789â€“804.

[38] J. Jang and U. Kang, â€œD-tucker: Fast and memory-efï¬cient

tucker
IEEE, 2020, pp. 1850â€“

decomposition for dense tensors,â€ in ICDE.
1853.

[39] O. A. Malik and S. Becker, â€œLow-rank tucker decomposition of large
tensors using tensorsketch,â€ in NeurIPS, 2018, pp. 10 117â€“10 127.
[40] Y. Wang, H. F. Tung, A. J. Smola, and A. Anandkumar, â€œFast and
guaranteed tensor decomposition via sketching,â€ in NeurIPS, 2015, pp.
991â€“999.

[41] C. E. Tsourakakis, â€œMACH: fast randomized tensor decompositions,â€ in

SDM, 2010, pp. 689â€“700.

[42] C. Battaglino, G. Ballard, and T. G. Kolda, â€œA practical randomized CP
tensor decomposition,â€ SIAM J. Matrix Anal. Appl., vol. 39, no. 2, pp.
876â€“901, 2018.

[43] A. Gittens, K. S. Aggour, and B. Yener, â€œAdaptive sketching for
fast and convergent canonical polyadic decomposition,â€ in ICML, ser.
Proceedings of Machine Learning Research, vol. 119.
PMLR, 2020,
pp. 3566â€“3575.

[44] A. H. Phan and A. Cichocki, â€œPARAFAC algorithms for large-scale
problems,â€ Neurocomputing, vol. 74, no. 11, pp. 1970â€“1984, 2011.
[45] X. Li, S. Huang, K. S. Candan, and M. L. Sapino, â€œ2pcp: Two-
phase CP decomposition for billion-scale dense tensors,â€ in 32nd IEEE
International Conference on Data Engineering, ICDE 2016, Helsinki,
Finland, May 16-20, 2016, 2016, pp. 835â€“846.

[46] W. Austin, G. Ballard, and T. G. Kolda, â€œParallel tensor compression

for large-scale scientiï¬c data,â€ in IPDPS, 2016, pp. 912â€“922.

[47] D. Chen, Y. Hu, L. Wang, A. Y. Zomaya, and X. Li, â€œH-PARAFAC:
hierarchical parallel factor analysis of multidimensional big data,â€ IEEE
Trans. Parallel Distrib. Syst., vol. 28, no. 4, pp. 1091â€“1104, 2017.
[48] E. Gujral, G. Theocharous, and E. E. Papalexakis, â€œSpade: S treaming
SIAM, 2020,

pa rafac2 de composition for large datasets,â€ in SDM.
pp. 577â€“585.

[49] K. Yin, W. K. Cheung, B. C. Fung, and J. Poon, â€œTedpar: Temporally de-
pendent parafac2 factorization for phenotype-based disease progression
modeling,â€ in SDM. SIAM, 2021, pp. 594â€“602.

