TimeEvolver:
A Program for Time Evolution With Improved Error Bound

Marco Michela, Sebastian Zellb

aDepartment of Physics, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel
bInstitute of Physics, Laboratory for Particle Physics and Cosmology, ´Ecole Polytechnique F´ed´erale de Lausanne, CH-1015 Lausanne,
Switzerland

2
2
0
2

y
a
M
0
3

]
h
p
-
t
n
a
u
q
[

1
v
6
4
3
5
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

We present TimeEvolver, a program for computing time evolution in a generic quantum system. It relies on well-
known Krylov subspace techniques to tackle the problem of multiplying the exponential of a large sparse matrix iH,
where H is the Hamiltonian, with an initial vector v. The fact that H is Hermitian makes it possible to provide an
easily computable bound on the accuracy of the Krylov approximation. Apart from eﬀects of numerical roundoﬀ,
the resulting a posteriori error bound is rigorous, which represents a crucial novelty as compared to existing software
packages such as Expokit [1]. On a standard notebook, TimeEvolver allows to compute time evolution with adjustable
precision in Hilbert spaces of dimension greater than 106. Additionally, we provide routines for deriving the matrix
H from a more abstract representation of the Hamiltonian operator.

Keywords: Numerical simulation, quantum mechanics, Krylov subspace, time evolution, Schr¨odinger equation,
unitary operator

Program summary

Program Title: TimeEvolver
Link to published article in Computer Physics Communications: doi.org/10.1016/j.cpc.2022.108374
CPC Library link to program ﬁles: doi.org/10.17632/vvwvng9w36.1
Developer’s repository link: github.com/marco-michel/TimeEvolver
Code Ocean capsule: codeocean.com/capsule/8431379
Licensing provisions: MIT
Programming language: C++
Supplementary material: An example which demonstrates the computation of time evolution in a concrete physical system
Nature of problem: Computing time evolution in a generic physical quantum system can be reduced to the numerical task of cal-
culating exp(−iHt)v. Here H is the Hamiltonian matrix, which is large and sparse, i corresponds to the imaginary unit, t denotes
time and the vector v represents the initial state. A program is needed to perform this computation eﬃciently. Since the use of
approximation methods is unavoidable, it is important to quantify as rigorously as possible the resulting error. Moreover, in or-
der to facilitate the application to various problems in physics, additional functionalities are needed, in particular for forming the
Hamiltonian matrix from a more abstract representation of the Hamiltonian operator.
Solution method: The program employs known Krylov subspace methods for calculation the exponential of the large sparse matrix
(−iHt) times the vector v. The Arnoldi algorithm is used to form the Krylov subspace and exponentiation of the resulting small
matrix is achieved by diagonalization. The fact that (−iHt) is anti-Hermitian makes it possible to calculate the error of the Krylov
approximation in terms of an easily-computable integral formula. This allows to choose a maximal size of the time step, after
which the method is restarted and a new Krylov subspace is formed, while respecting an adjustable error bound. It is rigorous up
to inaccuracies of a one-dimensional numerical integral and eﬀects of ﬁnite machine precision, for which we also give an estimate.
All linear algebra operations are performed with the Intel(cid:114) Math Kernel Library and Boost is used for numerical integration. The
methods for deriving the Hamiltonian matrix rely on a hashtable representation of Hilbert space.

Email addresses: michelma@post.bgu.ac.il (Marco Michel), sebastian.zell@epfl.ch (Sebastian Zell)

1

 
 
 
 
 
 
Contents

1 Introduction

1.1 Time Evolution in a Generic Physical Quantum System . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Krylov Subspace Methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Practical Computation of Time Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

2 The Method

.

2.1 Krylov Subspace .
.
2.2 Previous Work on Error Bounds
Improvement of Error Bound .
2.3
.
.
2.4 Pseudocode .

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.
.
.

.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.

.

3 Creation of Hamiltonian Matrix

4 Description of Program

5 Application of Program

5.1 Exemplary System .
.
5.2 Analysis of Performance .

.

.

.
.

.
.

.
.

.
.

.

.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Summary and Outlook

1. Introduction

2
2
3
4

5
5
5
7
8

8

11

12
12
14

16

1.1. Time Evolution in a Generic Physical Quantum System

We consider a generic physical quantum system. At a given time t, all information about it can be encoded in
a state vector v(t), i.e. knowledge of v(t) allows to predict the outcome of any conceivable measurement. All states
that a system can possibly assume are contained in a Hilbert space H. According to our understanding of physics,
our Universe is deterministic in the following sense. The state v(0) ∈ H at some initial time uniquely determines the
states v(t) ∈ H at all other times. A central task in physics is to infer v(t) from the knowledge of v(0).

In principle, this is very easy. The only additional ingredient that one needs is a Hamiltonian H, which is a self-
adjoint and time-independent operator on the Hilbert space H.1 Given v(0) and H, it follows from the Schr¨odinger
equation that the state at any other point in time t can be computed as

v(t) = e−iHtv(0) .

(1)

As said the vector v(t) represents a full solution, i.e. it completely determines the system. Thus, ﬁnding techniques for
calculating the r.h.s. of eq. (1) bears great relevance for all physical systems.

In some situations, analytic methods and approximations are available for performing this computation. For a
generic system, however, the only way to compute the r.h.s. of eq. (1) is to employ a numerical approach. Then v(0)
is a vector with d complex entries and H is a Hermitian d x d matrix, where d is the dimension of the Hilbert space H.
Each column of H encodes to which states a given basis state can transit. Typically, this number is much smaller than
d, i.e. H is a sparse matrix. Thus, one has to compute the product of an exponentiated anti-Hermitian sparse matrix
and a vector. Exaggerating slightly, one could say that solving any physical system reduces to this numerical task.

1Time-independence of the Hamiltonian ensures that energy is conserved. Therefore, any fundamental Hamiltonian must not depend on time.
Nevertheless, it is sometimes possible to describe certain subsystems of a bigger physical system in terms of an eﬀective time-dependent Hamilto-
nian. We shall not consider this case in the present paper.

2

1.2. Krylov Subspace Methods

Of course, a plethora of approaches exists to exponentiate a matrix. But the special nature of the problem (1) –
especially the fact that in the end knowledge of the full matrix e−iHt is not required – allows the use of techniques
that are signiﬁcantly more eﬃcient than generic routines for matrix exponentiation. Particularly important are Krylov
subspace methods, which were ﬁrst used in 1986 for the purpose of numerical time evolution [2]. We shall briefly
sketch them now, before we review them in more detail in section 2.1.

The m-dimensional Krylov subspace Km is deﬁned as

Km := span

(cid:110)
v(0), Hv(0), . . . , (H)m−1v(0)

(cid:111)

,

(2)

where m (cid:28) d. (Typically, one has m (cid:46) 100.) The key idea is to project the large sparse matrix H (in our case the
Hamiltonian) on the small subspace Km:

Hm := H|Km .
Since Hm represents a mapping from Km to Km, it corresponds to a small m x m matrix. Now the crucial step is to
eﬀectively replace

(3)

e−iHt → e−iHmt ,

i.e. to use instead of the exact result v(t) = e−iHtv(0) the Krylov approximation

˜v(t) = Vme−iHmtV T

mv(0) ,

where Vm stands for the embedding of Km in the full Hilbert space H. In doing so, we introduce the error

err(t) := v(t) − ˜v(t) .

(4)

(5)

(6)

Once matrix exponentiation is restricted to the small subspace Km as shown in eq. (5), it can be performed very fast
using standard methods. In the full Hilbert space H, only matrix-vector multiplications, as displayed in eq. (2), need to
be carried out. Thus, the eﬀectiveness of Krylov subspace methods is due the fact that no matrix-matrix multiplication
has to be performed in the large Hilbert space H.

We can estimate how large the advantage of Krylov subspace methods is. Clearly, a necessary requirement for the
feasibility of the calculation is that all intermediate results can be stored in (the memory of) a computer. It turns out
that this condition, and not runtime, indeed is the limiting factor in many practical applications. If one uses standard
matrix exponentiation, the biggest intermediate object is the matrix e−iHt, which unlike H is no longer sparse. So the
required storage capacity scales as the number of entries, i.e. d2. In contrast, the largest entities that are needed for
the Krylov method are the basis elements of the Krylov space (2), so the required memory scales as m d. In practice,
this leads to a gain in Hilbert space size of several orders of magnitude.

The Krylov subspace (2) is speciﬁcally adapted to the initial state v(0). Therefore, one can expect that for short
enough times t, the error (6) is small whereas it grows large once the state v(t) (as determined without any approxima-
tion) deviates suﬃciently from v(0). Thus, it becomes necessary to restart the procedure after a certain time interval,
i.e. to take ˜v(t), computed by the Krylov method, as initial state for a new Krylov subspace. With such a time stepping
procedure in place, it has become evident since the earliest numerical studies that in many circumstances the Krylov
method works surprisingly well already for values of m as small as 10 (see e.g. [2, 3, 4, 5]).

At the same time, these and subsequent investigations have lead to signiﬁcant conceptual progress in understanding
how these Krylov subspace techniques work. A possibly incomplete list of relevant studies includes [3, 4, 5, 6, 7, 8,
9, 10, 11, 12, 13], where it is important to mention that these publications have a wider scope than the present paper
in that they deal with the exponentiation of a generic sparse matrix. In the works cited above, a particularly relevant
object of study is to provide bounds on the error (6). In general, one can distinguish between a priori and a posteriori
error bounds. The ﬁrst ones can be evaluated without actually performing the Krylov technique while the second
ones rely on the result of the Krylov method. Correspondingly, a priori error bounds tend to be more general (and in
particular independent of the initial state v(0)) whereas a posteriori error bounds are often sharper.

However, both types of rigorous error bounds typically suﬀer from the problem that the function bounding the
error is diﬃcult to evaluate in practice. Therefore, a posteriori error estimates have been introduced in [2, 4]. They
play a twofold role in numerical implementations of Krylov subspace methods. First, they are used to determine the

3

time interval after which restarting of the Krylov procedure becomes necessary. Secondly, they can be used to estimate
the error of the ﬁnal result. Obviously, using error estimates instead of error bounds leads to the problem that one can
in general not be sure if the outcome of the Krylov method is indeed close to the true result.

1.3. Practical Computation of Time Evolution

As we have discussed, Krylov subspace methods represent an eﬃcient approximation scheme for calculating the
product of an exponentiated sparse matrix and a vector. Such a problem appears in a wide range of context.
In
particular, it is relevant for computing time evolution in a generic quantum system, as displayed in eq. (1). In this
case, the sparse matrix −iHt, which is exponentiated, is anti-Hermitian. From the perspective of Krylov methods, this
represents a special case, but in a physical context this encompasses the most general quantum system.

In order to actually compute the result of time evolution in a given physical system, a numerical implementation of
Krylov subspace methods is required. This crucial tool was provided by Sidje in 1998 when he published the software
package Expokit [1].2 Among other routines for matrix exponentiation, this program provides Krylov subspace meth-
ods for generic sparse matrices. Since certain simpliﬁcations occur for (anti-)Hermitian matrices, Expokit contains
functions that are speciﬁc to this special case, which is extremely important in the context of physics. In line with our
previous discussions, Expokit does not employ a rigorous error bound, but it relies on the error estimate proposed in
[4] and subsequent reﬁnements developed in [6, 7].

The lack of a rigorous error bound represents a serious drawback for any practical application. If the problem
at hand is such that one can compute the result by means of a diﬀerent method, then one can of course check the
validity of the error estimates. However, in such a case there is no need to employ Krylov subspace techniques in the
ﬁrst place. This is to say that Krylov approximation methods are only interesting if there is no other way of solving
the problem. But in this case it is impossible to make sure that the error estimates are still viable. Therefore, the
numerical results obtained from Krylov subspace methods would become signiﬁcantly more meaningful if a more
precise statement about the error, ideally a rigorous error bound, were available.

Since the release of Expokit, much progress has been made in the study of a posteriori error bounds and cor-
responding criteria for choosing the size of the time step [11, 12, 13, 14, 15, 16, 17, 18, 19]. To our knowledge,
Lubich was the ﬁrst one to point out that signiﬁcant simpliﬁcations occur in the physically relevant case in which
an anti-Hermitian matrix is exponentiated: The fact that the matrix norm of e−iHt is 1 leads to the existence of an
a posteriori error bound, which can be represented as an integral over quantities that are already known within the
Krylov approximation [14]. In this way, one obtains a formula for computing the error bound that is both rigorous
and easy to compute in practice. Of course, it is important to remark that a rigorous formula for bounding the error
of the Krylov approximation still does not lead to a fully rigorous statement about the accuracy of a numerical result.
The reason is that a ﬁnite machine precision inevitably causes additional roundoﬀ errors. Nevertheless, reducing the
uncertainty in the error to eﬀects of ﬁnite machine precision certainly represents an improvement.

The goal of the present work is to incorporate this knowledge, and in particular the error bound proposed in [14],
in a concrete numerical implementation of Krylov subspace methods. To this end, we have developed the software
package TimeEvolver, which is able to compute numerical time evolution while obeying an adjustable error bound.
It is rigorous up to eﬀects of roundoﬀ errors and a negligible uncertainty due to a ﬁnite step size in the evaluation of
the error integral. Moreover, we provide an estimate for the numerical roundoﬀ error. Since we specialize to anti-
Hermitian matrices from the outset, our program is considerably less general than Expokit. However, it is suﬃcient
for the most general application of quantum mechanics, namely time evolving a generic physical system. In turn,
we have aimed at making our program easy to use and to modify. Correspondingly, TimeEvolver is open-access and
based on free software. In this way, we hope to make Krylov subspace methods easily accessible for physicists from
a wide range of ﬁelds.

The outline of this paper is as follows. Section 2 starts with a more detailed review of known Krylov methods.
We pay particular attention to a posteriori error bounds and also present our estimate for the numerical rounding
error. Subsequently, we describe our concrete algorithm. Section (3) discusses the practical problem of deriving the
Hamiltonian matrix from a more abstract Hamiltonian operator. We specialize to the widely-used situation in which
the basis of Hilbert space is given by number eigenstates. Section 4 contains a description of TimeEvolver, i.e. the

2In particular, it is based on the theoretical investigations [3, 4, 5, 6, 7].

4

concrete implementation of our approach in C++. In section 5, we apply our program to an exemplary physical
system and study its performance. We summarize in section 6. Throughout, we try to be self-contained and not
assume speciﬁc prior knowledge. In this way, we hope to make our presentation also understandable to a physicist
without previous experience in the application of Krylov subspace techniques.

2. The Method

2.1. Krylov Subspace

Following [3, 4, 5], we shall review known Krylov subspace methods. The ﬁrst step is to compute a nested basis
of the Krylov spaces (2). This can be achieved using the Arnoldi algorithm (see e.g. [20] for a review). It simpliﬁes
for the special situation of Hermitian H, which we consider, and in this case is also known as Lanczos algorithm. With
the initialization

v1 := v(0) ,

h0,1 = 0 ,

v0 = 0 ,

we calculate for j = 1, . . . m:

w j =Hv j ,
w j =w j − h j−1, jv j−1 ,
h j, j =(cid:104)w j, v j(cid:105) ,
˜w j =w j − h j, jv j ,
(cid:113)

h j, j+1 =h j+1, j =
v j+1 = ˜w j/h j+1, j ,

(cid:104) ˜w j, ˜w j(cid:105) ,

(7)

(8a)

(8b)

(8c)

(8d)

(8e)

(8f)

where we use modiﬁed Gram-Schmidt orthogonalization since it is more stable numerically (see e.g. [21, 22, 20]). As
discussed in the introduction, it is important to note that no matrix-matrix multiplications H ∗ H appear but only much
cheaper matrix-vector multiplications H ∗ v j. Using the Arnoldi algorithm (8), we determine an orthogonal matrix
Vm := [v1, . . . vm] as well as the Hermitian matrix (Hm)i, j := hi, j. In the generic case, in which H not necessarily
Hermitian, (and with an appropriate generalization of eq. (8e)), Hm is an upper Hessenberg matrix, i.e. hi, j = 0 for
i > j+1. For Hermitian H, which we consider, Hm is tridiagonal. Nevertheless, we shall refer to Hm as the Hessenberg
matrix in what follows. The matrices Hm and Vm fulﬁll

where em is the unit vector with entry in the mth component. This implies that

HVm = VmHm + hm+1,mvm+1eT
m ,

Hm = V T

m HVm ,

(9)

(10)

i.e. Hm is the projection of H on the subspace Km, in line with eq. (3). So far, all statements have been exact. Now we
implement the approximation (4), which amounts to using Hm instead of the full H. Thus, the approximate result of
time evolving v(0) is

(11)
mv(0) = e1. Here and in the following we assume that the initial state vector v(0)
This coincides with eq. (5) since V T
is normalized: ||v(0)||= 1. The key point is that unlike H, the Hessenberg matrix Hm is small. Therefore, any standard
algorithm can be used to exponentiate it without signiﬁcant computational cost.

˜v(t) = Vme−iHmte1 .

2.2. Previous Work on Error Bounds

Next we shall discuss bounds on the error, err(t) := v(t) − ˜v(t), i.e. the diﬀerence of ˜v(t) and the result v(t) of an

exact time evolution (see eq. (6)). A series representation of it was derived in [4]:

err(t) = hm+1,m

∞(cid:88)

(cid:16)

k=1

eT
m

Φk(−iHmt)e1

(cid:17)

(−iHt)k−1 vm+1 ,

5

(12)

where Φ1(z) = (ez − 1)/z and Φk(z) can be deﬁned recursively (see [4]). It was proposed in [4] to use the ﬁrst term of
the series as error estimate:3
(cid:16)

||err(t)||≈

(cid:12)(cid:12)(cid:12)(cid:12)hm+1,m

eT
m

Φ1(−iHmt)e1

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12) .

(13)

Additionally, the suggestion was made to further simply eq. (13) by replacing Φ1(−iHmt) with e−iHmt, which leads to
[4]:
(cid:12)(cid:12)(cid:12)(cid:12)hm+1,m
The error analysis employed in Expokit is based on the series representation (12).4 Following the proposals [6, 7],
both the ﬁrst term (shown in eq. (13)) and the second term of the series (12) are used to compute an error estimate [1].
An important advantage of this approach is that it can also be applied in the general case when H is not Hermitian.

me−iHmte1
eT

||err(t)||≈

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12) .

(14)

(cid:16)

A complementary approach for studying the error of the Krylov method was suggested in [14] and subsequently
investigated in [15, 16, 17, 18, 19]. One can start from the observation that err(t) fulﬁlls a simple diﬀerential equation
[11, 12, 14, 15, 16, 17, 18, 19]. In order to derive it, we ﬁrst note that

˜v(cid:48)(t) = −iVmHme−iHmte1 = −iH ˜v(t) + ihm+1,m

(cid:16)

me−iHmte1
eT

(cid:17)

vm+1 ,

where we used (9) in the second step. Therefore, it follows that

err(cid:48)(t) = −iH err(t) − ihm+1,m

(cid:16)

me−iHmte1
eT

(cid:17)

vm+1 .

For the initial condition err(0) = 0, this diﬀerential equation is solved by

err(t) = −ihm+1,m

(cid:90) t

(cid:16)

0

me−iHmτe1
eT

(cid:17)

e−i(t−τ)Hvm+1dτ .

(15)

(16)

(17)

In order to ﬁnd an upper bound for the norm of err(t) from (17), one has to evaluate ||e−i(t−τ)Hvm+1||. For a generic,
i.e. non-Hermitian, matrix H this is very hard since it involves the exponential of a large matrix. Calculating it is
as diﬃcult as the initial problem (1) we set out to solve. Thus, formula (17) makes evident the typical diﬃculty of
rigorous error bounds: It is diﬃcult to compute the bounding function numerically.

This problem disappears, however, when H is Hermitian. Then ||e−i(t−τ)Hvm+1||= ||vm+1||= 1 and the bound eq. (17)
can be computed explicitly. It is very interesting that the case of a Hermitian matrix H, which is of extraordinary
relevance in physics, leads to such simpliﬁcations. We arrive at the error bound [14, 18, 19]

||err(t)||≤

(cid:90) t

0

(cid:12)(cid:12)(cid:12)hm+1,m(eT

me−iHmτe1)

(cid:12)(cid:12)(cid:12) .

dτ

(18)

The key point of this formula is that it only depends on quantities that are known in the Krylov algorithm or easy to
me−iHmτe1 can be calculated without
determine: The element hm+1,m is already computed in the Arnoldi procedure and eT
any signiﬁcant computational cost since Hm is already known and – as said before – the time needed to exponentiate
the small matrix Hm is negligible.

The only remaining question is how the integral in eq. (18) can be computed in practice. In [14] the proposal
was made to approximate it by evaluating the integrand at a small number of points. For example, the right-endpoint
rectangle rule represents an alternative way to arrive at the estimate (14) while the Simpson rule is expected to yield a
more accurate approximation [14]. Additionally, it was suggested to consider the limit of a small step size, in which
the integral can be performed analytically [19]. Evidently, a major drawback of such approximation methods is that
they introduce an unquantiﬁable uncertainty in formula (18) so that it no longer yields a rigorous error bound.

3Further justiﬁcations for using eq. (13) as error estimate were given in [13].
4We remark that Expokit employs a slightly modiﬁed version of the Krylov method [1]. In this approach, which was suggested in [4], eq. (11)
is altered so that it also uses the vector vm+1. In contrast, we shall stick in the present work to the original Krylov scheme as shown in eq. (11).
Therefore, all subsequent statement about error bounds refer to this unmodiﬁed Krylov method, but analogous results can also be obtained for the
modiﬁed approach (see in particular [4]).

6

2.3. Improvement of Error Bound

Whereas Expokit relies on the series representation (12) for estimating the accuracy of the Krylov approximation,
we shall in the present work use the integral (18) for bounding the error. For computing it numerically, we will employ
well-known double-exponential integration methods [23]. In our case of a one-dimensional continuous function on
a ﬁnite interval, they are known to achieve high accuracy at moderate computational costs (see e.g. [24, 25, 26]).
Dynamically adapting the number of points at which the integrand is evaluated, we can largely avoid uncertainties
that would result from replacing the integral (18) by simple approximations, such as the ones considered in [14].
In this way, inaccuracies due to the one-dimensional numerical integration become negligible as compared to other
sources of error, which we shall discuss shortly.

In general, we can distinguish two reasons why the Krylov method discussed in the present work does not yield
an exact result. The ﬁrst one, which we have discussed so far, is due to the Krylov approximation itself, i.e. the
replacement of the full Hamiltonian H by its projection Hm on the Krylov subspace (see eq. (3)). We can call this
error “analytic” since it would arise even if all calculations were performed with inﬁnite machine precision. As
shown in eq. (18), one can provide a rigorous formula for computing a posteriori bound on this analytic error. In any
numerical procedure, however, there is an inevitable second source of inaccuracy caused by a ﬁnite machine precision
– one can label the resulting error as “numerical”. We will not bound it rigorously but only provide an estimate.

Generically, the eﬀect of a ﬁnite machine precision is most signiﬁcant when a large number of numerical oper-
ations is applied consecutively (see e.g. [21, 22]). Correspondingly, we expect the Arnoldi/Lanczos algorithm to be
most important in the determination of the numerical error. The rounding error in the Lanczos procedure has been
studied intensely and rigorous bounds were derived [27, 28, 29, 30]. It follows from these analyses that the order of
magnitude of the numerical error can be bounded in terms of the two quantities

d ||H|| (cid:15) ,

m|| |H| || (cid:15) ,

(19)

where ||H|| is the 2-norm of the matrix H and (cid:15) is the machine precision. Moreover, |H| denotes a matrix obtained from
H by taking the absolute value of each element. A priori the two number in eq. (19) are independent but a common
bound for both of them is given by

d ||H||1 (cid:15) ,

(20)

where ||H||1 denotes the 1-norm of H and we used H¨older’s inequality for a symmetric matrix. We shall use the
quantity (20) as an estimate for the numerical error.

In summary, we develop further existing approaches to error analysis in Krylov methods, and in particular the
implementation in Expokit [1], in two ways. First, we use an exact formula for computing the error resulting from
the Krylov approximation. It is displayed in eq. (18) and can be evaluated in practice with only negligible additional
computational eﬀort. Apart from an insigniﬁcant inaccuracy due to a ﬁnite step size in numerical integration, formula
eq. (18) would lead to a rigorous a posteriori error bound if machine precision were inﬁnite. Secondly, we give an
estimate for roundoﬀ errors, which result from ﬁnite machine precision. It is shown in eq. (20). We must mention,
however, that our approach is only applicable to the special case of a Hermitian matrix H and therefore signiﬁcantly
less general than the method employed in Expokit.

Finally, we remark that formula (18) does not only give a rigorous formula for computing an upper bound on
the error of the Krylov approximation for a given time interval t. It can also be used to ﬁnd the optimal step size
for a given desired accuracy. As explained above, the step size denotes the time after which the Krylov procedure is
restarted, i.e. a new Krylov subspace is computed. The input data is the time t, at which we wish to evaluate v(t), and
an upper bound errmax on the error. This determines an upper bound on the error rate tolrate as follows:

Now we can evaluate the error formula (18) at diﬀerent times. The optimal step size tstep is the largest time for which
the resulting error rate is still below the desired bound:

tolrate = errmax

t

.

(21)

tstep = max

err(˜t)/˜t ≤ tolrate

˜t .

(22)

This allows us to use a minimal number of steps while obeying a given bound errmax on the error.

7

2.4. Pseudocode

We shall discuss more concretely our implementation of the algorithm sketched so far. It is displayed in pseu-

docode 1 and consists of repeating three steps:

1. We perform the Arnoldi algorithm as shown in eq. (8). We remark that instead of H, we use the matrix A = −iH
for computing the Krylov subspace. Consequently, the Hessenberg matrix is anti-Hermitian. Otherwise, this
part is completely standard. The only aspect we have not gone into so far is the special situation of a “lucky
breakdown”. Namely it can occur that the Krylov space Km, as deﬁned in eq. (2), has a dimension that is
smaller than m. In this case, an exact analytic result of orthonormalization, as displayed in line (8d), would
yield a vanishing h j+1, j for some value of j. In the numerical implementation, this corresponds to h j+1, j being
below some small threshold set by the machine precision. In such a case working in the Krylov subspace is no
longer an approximation but yields an exact result. This means that our problem is solved and we can stop the
algorithm. We shall not further discuss this special case.

2. We ﬁnd the optimal step size according to eq. (22) by computing the integral in the error formula (18) for
diﬀerent times tstep. We employ a double-exponential method [23] in the form of a tanh-sinh transformation
of the integral, which is thereafter evaluated by trapezoidal quadrature. In the concrete implementation, it has
proven eﬃcient to proceed as follows. In all iterations but the ﬁrst one, we can use the previous step size as
estimate for tstep, where we slightly reduce its value. Then we compute the resulting error according to the
integral (18) and compare it to tolrate · tstep. If it is larger, we half tstep and repeat the procedure until the error
is small enough. By using a slightly smaller value than the previous step size as initial estimate for tstep, we
achieve that usually no halving of tstep is required. Finally, we increase tstep in small substeps as long as the
resulting error is still small enough.

3. Using the step size tstep determined above, we compute v(tstep) according to eq. (11). At this point, we restart
the Krylov procedure, i.e. we go back to step 1, where v(tstep) now is the initial vector for the next iteration of
the algorithm.

For clarity of presentation, a number of additional functionalities and special cases are not shown in the pseu-

docode 1:

• It is important to point out that the fraction of runtime required for step 2 is generically small since computations
are only performed in the small Krylov subspace of dimension m. Nevertheless, one can gain a slight improve-
ment in performance by diagonalizing the Hessenberg matrix Hm at the beginning of step 2 (see e.g. [14]). If
this is done once, no matrix exponential is needed any more to compute the integrand of (18) at diﬀerent values
of its argument. We use this approach in our implementation.

• In the ﬁrst iteration, we have no previous step size which we could use as estimate for the next one. Therefore,
we slightly modify step 2 as follows. With an arbitrary initial guess for tstep, we compute the error. If it is small
enough, we repeatedly double tstep until the next doubling would make the error too large. Then we proceed
with the displayed procedure, i.e. we ﬁrst keep halving tstep if the error is too big and subsequently increase tstep
in small substeps.

• At the end of step 2, we compare the error computed according to eq. (18) with the estimate (20) of the numerical
error. If the latter is bigger, the program triggers a warning to alert the user that the analytic error bound may be
spoiled by ﬁnite machine precision.

• The above algorithm can trivially be extended not only to compute the ﬁnal vector but also to sample its values
at intermediate points of time: In step 3, one uses the known matrices Hm and Vm to compute the vector at any
ts with tnow ≤ ts ≤ tnow + tstep.

3. Creation of Hamiltonian Matrix

So far, we have shown how time evolution can be computed once the Hamiltonian matrix H is known. A problem
that one encounters in practical applications is that the matrix H is usually not given. Instead, the physical system and
the Hamiltonian are deﬁned in more abstract terms. In order to demonstrate how the matrix H can be derived in such

8

Input: Hamiltonian matrix H, normalized initial vector v, time t, maximal error errmax
Output: Approximation for exp(−iHt)v
A := −iH; tnow := 0; w := v; tstep = 0.1; tolrate = errmax/t;
while tnow < t do

//Step 1: create Krylov space
v1 := w;
for j := 1 : m do
p := Av j;
if j (cid:54)= 0 then

p := p − M j−1, jv j−1;
// M is diﬀerent name for Hessenberg matrix Hm

end
M j j := v j p; p := p − M j jv j;
no := ||p||2;
if no < tolrate then

// lucky breakdown

end
if j (cid:54)= m then

M j, j+1 := −no; M j+1, j := no;
v j+1 := p/no;

else

h := no;

end

end
//Step 2: ﬁnd optimal step size
f (t) := h |eT
errstep := integrate f (t) from 0 to tstep;
while errstep > tolrate · tstep do

m · exp(M · t) · e1|; tstep := 0.97 tstep ;

tstep = tstep/2;
errstep := integrate f (t) from 0 to tstep;

end
s = tstep/N SUBSTEPS; ns = 0; ∆err = 0;
while errstep + ∆err < tolrate · (tstep + ns ∗ s) do

errstep = errstep + ∆err; ns = ns + 1;
∆err = integrate f (t) from tstep + ns ∗ s to tstep + (ns + 1) ∗ s;

end
tstep = tstep + (ns − 1) · s;
ω := exp(M · tstep · e1);
//Step 3: perform step
w = V · ω; tnow := tnow + tstep;

end

Algorithm 1: Krylov-method

9

a situation, we shall focus on the widely-used description of physical systems in terms of number operators. We shall
briefly review it and then discuss how H can be derived.

Every physical system can be described as collection of interacting oscillator modes. We take their number to be
l , ˆal, where l = 1, . . . , L.

L and assume that L is ﬁnite.5 Each mode l possesses creation and annihilation operators ˆa†
In the case of bosons, they fulﬁll the algebra

and lead to the number operators

[ˆak, ˆa†

l ] = δkl ,

[ˆak, ˆal] = [ˆa†

k, ˆa†

l ] = 0 ,

l ˆal .
Now we can use the creation operators to form a basis of states:
(cid:17)n1 . . .

|n1, . . . , nL(cid:105) := (cid:16)

ˆnl = ˆa†

ˆa†
1

(cid:16)

ˆa†
L

(cid:17)nL |0(cid:105) ,

where |0(cid:105) is the vacuum and the numbers (n1, . . . , nL) characterize the occupation number of the diﬀerent modes:

In the non-relativistic limit, the total occupation number,

ˆnl |n1, . . . , nL(cid:105) = nl |n1, . . . , nL(cid:105) .

N =

L(cid:88)

l=1

nl ,

(23)

(24)

(25)

(26)

(27)

is conserved. In this case, not all tuples (n1, . . . , nL) are admissible and the set of states (25) is ﬁnite. Finally, the
Hamiltonian can be expressed as an operator in terms of the creation and annihilation operators. For example, the free
Hamiltonian for a system of L modes is given by

ˆH0 =

L(cid:88)

l=1

εl ˆa†

l ˆal ,

(28)

where εl are arbitrary real numbers.

In order to be able to apply TimeEvolver, one now needs to derive the Hamiltonian matrix H from an Hamiltonian
operator ˆH, which is expressed in terms of creation and annihilation operators. As a ﬁrst step, we order the basis
elements, i.e. we introduce an arbitrary but ﬁxed bijective function ind, which maps the admissible tuples (n1, . . . , nL)
on an index set I = {1, . . . , d}. For each basis element |n1, . . . , nL(cid:105), we now apply the Hamiltonian operator ˆH, which
in general leads to a superposition of basis elements:

ˆH |n1, . . . , nL(cid:105) =

(cid:88)

k

c(k) |n(k)

1 , . . . , n(k)

L (cid:105) .

Here c(k) are a complex numbers. Then we deﬁne for each non-zero c(k):

Hi, j = c(k) ,

where

i = ind−1 (|n1, . . . , nL(cid:105)) ,

j = ind−1 (cid:16)

1 , . . . , n(k)
|n(k)
L (cid:105)

(cid:17)

.

(29)

(30)

(31)

At last, the Hi, j deﬁne the elements of the Hamiltonian matrix.

Finally, we remark that the above procedure can be straightforwardly generalized to Hamiltonians that are deﬁned
in terms of other operators than creation and annihilation operators. To this end, one only needs to adapt the commu-
tation relations (23) and the mapping (25) of basis states and operators. Finally, superselection rules, such as number
conservation in our case (see eq. (27)), need to be taken into account in the enumeration of basis states.

5Otherwise the Hilbert space has inﬁnite dimension and our numerical approach is not applicable.

10

4. Description of Program

We have implemented our method in C++. Our program uses the following libraries. The core module relies
on the Intel(cid:114) Math Kernel Library (MKL)6 for linear algebra operations.7 Moreover, Boost8 is used for numerical
integration. These two libraries are the only necessary prerequisites for our program. Outside of this core functionality,
we employ Hierarchical Data Format version 5 (HDF5(cid:114))9 for convenient output of data. Finally, CMake10 is used to
build our program.

Our implementation of the Krylov subspace method is contained in the class krylovTimeEvolver in the folder
“core”. Its constructor requires all arguments that are needed for time evolution, in particular the Hermitian Hamilto-
nian matrix and the complex vector of the initial state, where the corresponding data types are deﬁned in matrixDataTypes.
Algorithm 8 is implemented in the method timeEvolve, where Step 1, i.e. the Arnoldi algorithm, is encapsulated
in arnoldiAlgorithm and Step 2, i.e. the determination of the maximal step size compatible with the desired error
bound, is performed in findMaximalStepSize.

For evaluating the error integral (18) in findMaximalStepSize numerically, we rely on an adaptive implemen-
tation of a double-exponential method provided by Boost. It increases the number of points, at which the integrand
is sampled, until an estimate of the relative error in evaluating the integral, which is incorporated in the library, falls
below 10−3. If this cannot be achieved with 15 reﬁnements, the program triggers a warning. In certain cases, this
sophisticated integration procedure can have a negative impact on runtime. Therefore, we have also implemented a
second integration method based on a non-adaptive Gauß-scheme (see e.g. [24].), which the user can choose to use.
This simpler and faster approach could be particularly useful for explorative investigations. If interesting eﬀects are
found, one can establish the validity of results with the more accurate double-exponential method. We remark, how-
ever, that the eﬀect of integration on runtime becomes negligible for large Hilbert spaces (d (cid:38) 105) so that in this case
there is no disadvantage in always using the more precise approach.

All core functionalities of TimeEvolver are contained in the krylovTimeEvolver-class. If users already have at
their disposal a Hamiltonian matrix, no parts of our program other than the class krylovTimeEvolver are relevant
for them. However, for the convenience of the user who does not yet have a Hamiltonian matrix, we have also added
the folder “helper”, which contains routines to form it. The class basis provides a representation of basis vectors in
terms of number eigenstates, as described in section 3. In particular, its creator forms all basis elements for a given
number K of modes and a given total occupation N. Moreover, it supports the case in which some of the modes have
a given ﬁxed maximal occupation number.

The class hamiltonian contains a representation of a Hamiltonian in terms of creation and annihilation operators
and provides the important routine createMatrix, which creates a Hamiltonian matrix according to the approach
described in section 3. Since the function ind is inverted frequently (see eq. (31)), we represent it by a hashtable,
which can perform this operation in constant time. We remark that the routine createMatrix can be easily adapted
to other choices of basis. This makes it possible to apply it to Hamiltonians that are not deﬁned in terms of creation
and annihilation operators.

Finally, we have included the folder “example”, which demonstrates the usage of TimeEvolver for a particular class
of Hamiltonians. These Hamiltonians were investigated in the study [31], which relies on the TimeEvolver for numer-
ical time evolution.11 The particular structure of the Hamiltonians is reﬂected in the class exampleHamiltonian.
The routine main contained in the class of the same name shows how all parts can be integrated and the TimeEvolver
is used. Results of this routine are shown in section 5.1.

6https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html
7We choose MKL because it provides LAPACK and sparse BLAS routines in one package as well as for performance reasons.
8https://www.boost.org/
9https://www.hdfgroup.org/solutions/hdf5
10https://cmake.org/
11More precisely, the prospect of using the TimeEvolver in the study [31] inspired us to create it.

11

5. Application of Program

5.1. Exemplary System

We will demonstrate the usage of TimeEvolver on the speciﬁc Hamiltonian used in [31], which we shall briefly
describe. The system consists of two subsystems of quantum oscillators. The ﬁrst set is comprised of the two modes
ˆa0 and ˆb0. We label their occupation numbers as n0 and m0. The second subsystem includes the modes ˆak and ˆa(cid:48)
k(cid:48) ,
where k = 1, . . . , K and k(cid:48) = 1, . . . , K(cid:48). These oscillators are assumed to be qubits, i.e. their occupation numbers nk
k(cid:48) are restricted to the values 0 and 1. In total the system features L = 2 + K + K(cid:48) modes. We can denote a number
and n(cid:48)
eigenstate as

|n0, m0, n1, . . . , nK, n(cid:48)

1, . . . , n(cid:48)

K(cid:48) (cid:105) .

(32)

As will become evident, the system is constructed in such a way that the occupation numbers in the two sets of
oscillators are conserved independently. Thus, the two numbers

N0 = n0 + m0 ,

Nm =

K(cid:88)

k=1

nk +

K(cid:48)
(cid:88)

k(cid:48)=1

n(cid:48)
k(cid:48)

(33)

do not change in the course of time evolution.

Apart from this property, the particular structure of the system is not important for the present study. For com-
pleteness, we shall nevertheless discuss it in more detail and outline its physical motivation. The concrete Hamiltonian
is given by

(cid:16)

ˆH = C0

ˆa†
0

ˆb0 + ˆb†

0 ˆa0

(cid:17) + (cid:15)m

(cid:32)

1 −

ˆn0
Nc

(cid:33) K(cid:88)

k=1

ˆnk

(cid:32)

1 −

+(cid:15)m

(cid:26) K(cid:88)

+Cm

ˆn0
Nc − ∆Nc
K(cid:48)
(cid:88)

(cid:33) K(cid:48)
(cid:88)

k(cid:48)=1

ˆn(cid:48)
k(cid:48)

f1(k, k(cid:48))

(cid:16)

ˆa†
k ˆa(cid:48)

k(cid:48) + h.c.

(cid:17)

k=1
K(cid:88)

k(cid:48)=1
K(cid:88)

+

k=1

K(cid:48)
(cid:88)

l=1
l>k
K(cid:48)
(cid:88)

k(cid:48)=1

l(cid:48)=1
l(cid:48)>k(cid:48)

+

f2(k, l)

(cid:16)

(cid:17)

ˆa†
k ˆal + h.c.

f3(k(cid:48), l(cid:48))

(cid:16)

(cid:48)†
k(cid:48) ˆa(cid:48)
ˆa

l(cid:48) + h.c.

(cid:17) (cid:27)

,

(34)

(35)

with

(cid:40)

fi(k, l) =
7(l + ∆li)5(cid:17)

Fi(k, l) − 1
Fi(k, l)

for Fi < 0.5
for Fi ≥ 0.5

,

where Fi(k, l) = (cid:16) √
mod 1. Moreover, we set ∆k1 = ∆k2 = 1, ∆k3 = K + 1, ∆l1 = ∆l3 =
K + 1 as well as ∆l2 = 1. The Hamiltonian ˆH depends on the real constants (cid:15)m, C0, Nc, ∆Nc, Cm, K as well as K(cid:48),
which fulﬁll Nc > 0, 0 < ∆Nc < Nc, K > 0 and K(cid:48) > 0. As initial state, we use

2(k + ∆ki)3 +

√

|in(cid:105) = |N0, 0, 1, . . . , 1
(cid:124)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:125)
Nm

, 0, . . . , 0(cid:105) ,

(36)

where we introduced N0 and Nm as additional parameters. They denote the number of particles in the mode ˆa0 and
the number of qubits that are set to 1, respectively. In accordance with eq. (33), they moreover determine the number

12

of particles in each of the two subsectors of the system. Unless speciﬁed otherwise, we use the following choice of
parameters:

√

(cid:15)m =

20 , N0 = Nc = 20 , ∆Nc = 12 , K = K(cid:48) = 4 ,
C0 = 1 , Cm = 1 , Nm = 2 .

(37)

Finally, we shall briefly explain why it is interesting to study the system (34), thereby elucidating the origin of
the concrete Hamiltonian. If the reader is only interested in the application of TimeEvolver, they can skip this part.
The original motivation comes from black hole physics. The corresponding line of research was initiated in [32] and
a complete list of references can be found in [31]. A generic problem in black hole physics is that exact calculations
beyond any semi-classical limit are very hard to perform. Therefore, it is interesting to look for analogue systems
which share important properties with black holes, but which are easier to study. Then one can try to draw conclusions
about a black hole by solving the analogue system. An important property of a black hole is its Bekenstein-Hawking
entropy [33], because of which it has a high capacity of information storage. This motivates the study of generic
quantum systems with an enhanced memory capacity.

Thus, we start from the question how a generic quantum system can achieve a high capacity of information storage.
We can quantify the latter by a microstate entropy K, which is deﬁned as the logarithm of the number of available
states. Two ingredients are suﬃcient. First, one needs K modes. The number of states corresponding to diﬀerent
occupation numbers of them (as deﬁned in eq. (25)) scales exponentially with K. For example, if occupation numbers
are restricted to 0 and 1, then K qubits leads to 2K microstates and hence an entropy on the order of K. Secondly, all
microstates must be nearly degenerate in energy. If this is not the case, they cannot be counted as microstates that
belong to one and the same macrostate.

Both of these requirements are fulﬁlled by the simple system [34]

ˆHprototype = (cid:15)m

(cid:32)
1 −

ˆn0
Nc

(cid:33) K(cid:88)

k=1

ˆnk .

(38)

It corresponds to a part of the Hamiltonian (34), and all deﬁnitions are as there. The qubits ˆa1, . . . , ˆaK are responsible
for information storage, i.e. generating the microstate entropy K – we can call them memory modes. If the mode ˆa0
were absent, however, the states corresponding to diﬀerent occupation number of the memory modes would not be
degenerate in energy. Indeed, there would be a large diﬀerence of K(cid:15)m between the energies of the empty and of the
full state. This changes once ˆa0 is populated. In a highly occupied state with (cid:104)n0(cid:105) = n0, we can use the Bogoliubov
approximation, ˆn0 ≈ n0, to derive the eﬀective energy gap of the ˆak-modes:

(cid:32)

(cid:15)eﬀ ≈

1 −

(cid:33)

n0
Nc

(cid:15)m .

(39)

We see that it is lowered, and for a critical occupation n0 = Nc all memory modes become gapless. Then the
microstates corresponding to diﬀerent occupation numbers of them become degenerate in energy and an entropy
on the order of K is achieved. We call this phenomenon, in which a high occupation number of one mode decreases
the energy gap of others, assisted gaplessness [35]. This is the reason why we choose N0 = Nc in the initial state (36).
These observations lead to a question about how the memory modes backreact on the time evolution of ˆa0. In order
to study it, we couple ˆa0 to another mode ˆb0, which results in the ﬁrst line of the Hamiltonian (34). If the memory
modes ˆak are empty, the occupation numbers n0 and m0 perform oscillations. Once the ˆak are occupied, however, the
oscillations get suppressed and n0 becomes tied to the initial state n0 = Nc. This eﬀect of memory burden [36] arises
because any deviation of n0 from Nc would destroy the gaplessness of the memory modes and hence be very costly in
energy, as is evident from eq. (39).

In [31], we studied the question if memory burden can be overcome. To this end, we introduced a second set
k(cid:48) , which becomes gapless at a diﬀerent occupation number n0 = Nc − ∆Nc. The corresponding
of memory modes ˆa(cid:48)
operators are displayed in the second line of the Hamiltonian (34). Moreover, we allowed transitions between and
among the two sets of memory modes, which explains why we have the remaining lines of eq. (34). In the presence of
a second set of memory modes, it becomes energetically possible to transit from the initial state (36), in which n0 = Nc

13

(a) Expectation value of the number operators in the ﬁrst sector,
which consists of ˆa0 and ˆb0

(b) Expectation value of the number operators in the second sector,
1, . . . , ˆa(cid:48)
which consists of ˆa1, . . . , ˆaK , ˆa(cid:48)
K(cid:48)

Figure 1: Time evolution of the initial state (36). The horizontal black line plot illustrates the total occupation numbers N0 and Nm in the two
subsector. They are conserved separately.

and only the ˆak are occupied, to a ﬁnal state, in which n0 = Nc − ∆Nc and only ˆa(cid:48)
k(cid:48) are occupied. In [31], we used
TimeEvolver to study if such transitions actually occur dynamically, and our conclusion was that they are generically
heavily suppressed. Applied to black holes, this ﬁnding has interesting implications for Hawking radiation [37].
Namely, it indicates that evaporation of a black hole slows down signiﬁcantly at the latest after it has lost half of its
mass.

5.2. Analysis of Performance

Now we shall use TimeEvolver to compute the time evolution of the initial state (36) with the Hamiltonian
(34). The corresponding code is included in the folder “example”. The Hamiltonian is represented by the class
exampleHamiltonian and the method main shows how TimeEvolver is applied. For the numerical precision on the
norm we set errmax = 10−8. As exemplary observables we use the expectation values of the number operators of all
quantum modes.

For the choice (37) of parameters, the occupation numbers as functions of time are displayed in Fig. 1. Moreover,
we evaluate the sums N0 and Nm of occupation numbers in the two sectors of the system, as deﬁned in eq. (33). That
N0 and Nm are indeed constant represents a ﬁrst consistency check. As a second consistency check we compute a
forward-backward example. To this end, we ﬁrst calculate time evolution of the state (36) up to t = 10, as displayed
in Fig. 1, and then use the result as initial state for another time evolution, in which we replace ˆH → − ˆH. Fig. 2
shows the result of the second computation. In accordance with quantum mechanical unitarity, we see that Fig. 2 is
the mirror image of Fig. 1. Moreover, we arrive again at the initial state with an error of 9.82 × 10−9 which is within
the requested error bound of 2.0 × 10−8.

Next, we shall analyze the runtime. All following benchmark results in this section were obtained on an Intel®
Core™i9-9900K Processor with 22GB RAM. First we compare the simulation wall time of the TimeEvolver with the
original Expokit implementation (in its MATLAB version).12 As a prototype system we choose (34) with the model
parameters set to K = K(cid:48) = 10, Nm = 5 and N0 = Nc = 100 and the remaining ones given by (37). This corresponds
to a Hamiltonian matrix of dimension 1, 565, 904 × 1, 565, 904. We evolve the initial state (36) up to t = 10, choose
for both implementations a Krylov space dimension of m = 40 and request an upper error bound of errmax = 10−7.
Restricted to one thread we measured the following mean simulation times averaged over ﬁve runs:

tTimeEvolver = 353 s ,

tExpokit = 1537 s .

(40)

The 2-norm diﬀerence between the ﬁnal states is ||v(10)TimeEvolver − v(10)Expokit||= 5.7 × 10−8, and thus consistent with
the error bounds. However, we remark that the estimate for the numerical error based on eq. (20) yields a larger value,
3.2 · 10−6, and so a warning is triggered in TimeEvolver. This warning also appears in some of the examples discussed

12We used MATLAB R2021b for the Benchmark.

14

246810t5101520ni246810t0.51.01.52.0ni(a) Expectation value of the number operators in the ﬁrst sector,
which consists of ˆa0 and ˆb0

(b) Expectation value of the number operators in the second sector,
1, . . . , ˆa(cid:48)
which consists of ˆa1, . . . , ˆaK , ˆa(cid:48)
K(cid:48)

Figure 2: Time-reversed evolution of the ﬁnal state shown in Fig. 1.

Figure 3: Simulation time as a function of Hilbert space dimension. The data points correspond to the parameter choices N0 = Nc = 100,
K = K(cid:48) = 4, 6, 8, 10, Nm = K/2 and tmax = 10, with the remaining ones set by (37). The last point has been obtained by setting K = K(cid:48) = 10 and
N0 = Nc = 139 to go to the memory limit of our speciﬁc machine. The blue straight line represents a ﬁt to the data, as described in the text. The
last point has not been included in the ﬁt.

subsequently. Finally, we note that using the fast integration based on Gauß-quadrature does not lead to a measurable
improvement in runtime due to a large Hilbert space dimension.

Our next goal is to study how runtime depends on the size of the system. Again using the Hamiltonian (34) as a
prototype model, we shall vary the number K of modes in the memory sector. We set K(cid:48) = K as well as Nm = K/2
and N0 = Nc = 100. Note that the amount of basis elements depends exponentially on the number of modes. The
simulation time as a function of the Hilbert space dimension is illustrated in Fig. 3. The last data point corresponds to
a Hilbert space dimension of 2, 170, 560 which was the largest one ﬁtting in the memory of the benchmark machine.
We note that the limitation on the Hilbert space dimension comes from the creation of the Hamiltonian matrix and
not from the actual time evolution. By making matrix creation more memory eﬃcient, one could slightly increase the
maximal dimension of Hilbert space. Finally, we note that for our speciﬁc prototype system the relation of Hilbert
space dimension d and simulation time t is polynomial. Performing a ﬁt of the form t = a · db to the data, we obtain
a = 8.1 × 10−6 and b = 1.23, where we excluded the last point due to overhead eﬀects of being at memory limit. The
result of the ﬁt is shown in Fig. 3 as blue straight line.

To conclude, we study the dependence of the simulation time on meta parameters of the algorithm itself, namely
the Krylov space dimension m and the requested tolerance on the norm error errmax. For this study, we set K = K(cid:48) = 8,

15

246810t5101520ni246810t0.51.01.52.0ni50001×1045×1041×1055×1051×1060.11101001000Hilbertspacedimensionsimulationwall-clocktime[s](a) Simulation time scaling with Krylov dimension m.

(b) Simulation time scaling with requested bound errmax on the
norm error.

Figure 4: Wall clock time scaling. The parameter of the model (34) were chosen to be K = K(cid:48) = 8, N0 = Nc = 100, Nm = 4 and tmax = 10 with the
remaining ones set by (37)

N0 = Nc = 100, Nm = 4 and tmax = 10 with the remaining ones set by (37). The results are displayed in Fig. 4. It is
evident from Fig. 4a that the runtime only depends mildly on the dimension of Krylov space. The range of acceptable
values of m is usually rather large and penalty on the simulation time is in most cases not drastic as long as it is not
unreasonably small or large. The wall-clock simulation time as a function of errmax is shown in Fig. 4b. We observe a
logarithmic dependence on the requested error bound.

6. Summary and Outlook

In this paper, we present the software package TimeEvolver. Its purpose it to compute the exponential of a large
sparse matrix that is multiplied with a vector. We specialize to the case in which the matrix is anti-Hermitian. While
this may seem like a peculiar choice from a mathematical point of view, it is of utter importance for physics: Any
calculation of time evolution can be reduced to this task of exponentiating an anti-Hermitian matrix. In this case, the
vector represents the initial state and the matrix is the imaginary unit i times the Hamiltonian.

It is well-known that an eﬃcient way to tackle this task consists in Krylov subspace methods, and also TimeEvolver
relies on them. In doing so, TimeEvolver goes beyond existing software packages, such as Expokit [1], in two ways.
First, the specialization to anti-Hermitian matrices makes it possible to incorporate in the numerical implementation a
rigorous formula for bounding the error of the Krylov approximation. Moreover, we provide an estimate for additional
uncertainties due to numerical roundoﬀ. An improved statement about the accuracy of the Krylov approach is of great
importance for the potential of TimeEvolver to make new discoveries. If our software leads to novel and potentially
unexpected results (as was the case in [31]), one needs to make sure that they are not artifacts of the numerical method.
Since TimeEvolver is most useful for systems and phenomena that cannot be studied with any other means, the validity
of new discoveries can only be established via an error analysis provided by the program itself.

The second advantage of TimeEvolver consists in its ease of use for applications to physics. In order to achieve
this, we provide routines to create a basis of possible states and to derive the Hamiltonian matrix from a more abstract
representation of the Hamiltonian. In doing so, we specialized to the widely-used case of Hamiltonians that are deﬁned
in terms of creation and annihilation operators, but it is straightforward to adapt our program to other operators.
Moreover, we have included a concrete physical example, documented all parts of the software package and devised a
streamlined installation and build procedure. Finally, our program is open-access and based on free software. In this
way, we hope that TimeEvolver can contribute to progress across various disciplines in physics.

As an outlook, we would like to point out ideas for future improvements of TimeEvolver. First, it is known that
a decrease of runtime for Krylov subspace methods can be achieved by employing GPU computing (see [38] for an
early implementation using NVIDIA CUDA(cid:114)). Secondly, parallelization is capable of bypassing the limitation on the
size of the Hilbert space, which arises from the requirement of storing the Hamiltonian matrix in a single memory, by
distributing it among many computing units. On a supercomputer, this makes it possible to study Hilbert spaces with
dimension close to 1010 [39]. It would be very interesting to combine a parallel implementation of Krylov subspace

16

105010050010005000050100150200250Krylovdimensionmsimulationwall-clocktime[s]10-910-810-710-610-510-40.00120222426errmaxsimulationwall-clocktime[s]techniques, such as the one in [39], with the error analysis of TimeEvolver.13

Declaration of Competing Interests

The authors declare that they have no known competing ﬁnancial interests or personal relationships that could

have appeared to inﬂuence the work reported in this paper.

Acknowledgments

We are grateful to Lukas Eisemann for collaboration in the initial stages of this project, and we thank Gia Dvali
for useful comments. Moreover, we are indebted to an anonymous referee for insightful feedback and many helpful
suggestions. This work was supported in part, by the Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germany’s Excellence Strategy via the Munich Center for Quantum Science and Technology
(EXC-2111 - 390814868) and the Excellence Cluster Origins (EXC-2094 - 390783311). The work of M.M. was
supported by a Minerva Fellowship of the Minerva Stiftung Gesellschaft f¨ur die Forschung mbH, the Israel Science
Foundation (grant No. 741/20) and by the Deutsche Forschungsgemeinschaft through a German-Israeli Project Co-
operation (DIP) grant ”Holography and the Swampland”. The work of S.Z. was supported by ERC-AdG-2015 grant
694896.

References

[1] R. B. Sidje, Expokit: A Software Package for Computing Matrix Exponentials, ACM Trans. Math. Softw. 24 (1) (1998) 130–156. doi:

10.1145/285861.285868.

[2] T. J. Park, J. C. Light, Unitary quantum time evolution by iterative Lanczos reduction, The Journal of Chemical Physics 85 (10) (1986)

5870–5876. doi:10.1063/1.451548.

[3] E. Gallopoulos, Y. Saad, On the Parallel Solution of Parabolic Equations, in: Proceedings of the 3rd International Conference on Supercom-

puting, ICS ’89, Association for Computing Machinery, New York, NY, USA, 1989, p. 17–28. doi:10.1145/318789.318793.

[4] Y. Saad, Analysis of Some Krylov Subspace Approximations to the Matrix Exponential Operator, SIAM Journal on Numerical Analysis

29 (1) (1992) 209–228. doi:10.1137/0729014.

[5] E. Gallopoulos, Y. Saad, Eﬃcient Solution of Parabolic Equations by Krylov Approximation Methods, SIAM Journal on Scientiﬁc and

Statistical Computing 13 (5) (1992) 1236–1264. doi:10.1137/0913071.

[6] R. Sidje, Parallel algorithms for large sparse matrix exponentials: Application to numerical transient analysis of Markov processes, PhD,

Rennes.

[7] B. Philippe, R. B. Sidje, Transient Solutions of Markov Processes by Krylov Subspaces, in: W. J. Stewart (Ed.), Computations with Markov

Chains, Springer US, Boston, MA, 1995, pp. 95–119. doi:10.1007/978-1-4615-2241-6_7.

[8] V. Druskin, L. Knizhnerman, Krylov subspace approximation of eigenpairs and matrix functions in exact and computer arithmetic, Numerical

Linear Algebra with Applications 2 (3) (1995) 205–217. doi:https://doi.org/10.1002/nla.1680020303.

[9] D. Stewart, T. Leyk, Error estimates for Krylov subspace approximations of matrix exponentials, Journal of Computational and Applied

Mathematics 72 (2) (1996) 359 – 369. doi:10.1016/0377-0427(96)00006-4.

[10] M. Hochbruck, C. Lubich, On Krylov Subspace Approximations to the Matrix Exponential Operator, SIAM Journal on Numerical Analysis

34 (5) (1997) 1911–1925. doi:10.1137/S0036142995280572.

[11] E. Celledoni, I. Moret, A Krylov projection method for systems of ODEs, Applied Numerical Mathematics 24 (2) (1997) 365 – 378, second

International Conference on the Numerical Solution of Volterra and Delay Equations. doi:10.1016/S0168-9274(97)00033-0.

[12] V. Druskin, A. Greenbaum, L. Knizhnerman, Using Nonorthogonal Lanczos Vectors in the Computation of Matrix Functions, SIAM Journal

on Scientiﬁc Computing 19 (1) (1998) 38–54. doi:10.1137/S1064827596303661.

[13] M. Hochbruck, C. Lubich, H. Selhofer, Exponential Integrators for Large Systems of Diﬀerential Equations, SIAM Journal on Scientiﬁc

Computing 19 (5) (1998) 1552–1574. doi:10.1137/S1064827595295337.

[14] C. Lubich, From Quantum to Classical Molecular Dynamics: Reduced Models and Numerical Analysis, European Mathematical Society,

2008. doi:10.4171/067.

[15] M. A. Botchev, V. Grimm, M. Hochbruck, Residual, Restarting, and Richardson Iteration for the Matrix Exponential, SIAM Journal on

Scientiﬁc Computing 35 (3) (2013) A1376–A1397. doi:10.1137/110820191.

[16] Q. Ye, Error Bounds for the Lanczos Methods for Approximating Matrix Exponentials, SIAM Journal on Numerical Analysis 51 (1) (2013)

68–87. doi:10.1137/11085935X.

[17] Z. Jia, H. Lv, A posteriori error estimates of Krylov subspace approximations to matrix functions, Numerical Algorithms 69 (1) (2015) 1–28.

arXiv:1307.7219, doi:10.1007/s11075-014-9878-0.

13Numerous approximate solution techniques have been developed for the study of Hilbert spaces with even larger sizes. Also in such a situation,

Krylov subspace techniques can be useful to compute time evolution (see e.g. [40] for a review.)

17

[18] H. Wang, Q. Ye, Error Bounds for the Krylov Subspace Methods for Computations of Matrix Exponentials, SIAM Journal on Matrix Analysis

and Applications 38 (1) (2017) 155–187. arXiv:1603.07358, doi:10.1137/16M1063733.

[19] T. Jawecki, W. Auzinger, O. Koch, Computable upper error bounds for Krylov approximations to matrix exponentials and associated ϕ-

functions, BIT Numerical Mathematics 60 (2020) 157–197. arXiv:1809.03369, doi:10.1007/s10543-019-00771-6.

[20] Y. Saad, Iterative Methods for Sparse Linear Systems, 2nd Edition, Society for Industrial and Applied Mathematics, 2003. doi:10.1137/

1.9780898718003.

[21] B. N. Parlett, The Symmetric Eigenvalue Problem, Society for Industrial and Applied Mathematics, 1998.

doi:10.1137/1.

9781611971163.

[22] N. J. Higham, Accuracy and Stability of Numerical Algorithms, 2nd Edition, Society for Industrial and Applied Mathematics, 2002. doi:

10.1137/1.9780898718027.

[23] H. Takahasi, M. Mori, Double Exponential Formulas for Numerical Integration, Publications of the Research Institute for Mathematical

Sciences 9 (3) (1974) 721–741. doi:10.2977/prims/1195192451.

[24] P. J. Davis, P. Rabinowitz, Methods of Numerical Integration, 2nd Edition, Academic Press, 1984. doi:https://doi.org/10.1016/

C2013-0-10566-1.

[25] M. Mori, M. Sugihara, The double-exponential transformation in numerical analysis, Journal of Computational and Applied Mathematics

127 (1) (2001) 287–296. doi:10.1016/S0377-0427(00)00501-X.

[26] D. H. Bailey, K. Jeyabalan, X. S. Li, A comparison of three high-precision quadrature schemes, Experimental Mathematics 14 (3) (2005)

317–329. doi:10.1080/10586458.2005.10128931.

[27] S. Kaniel, Estimates for some computational

techniques in linear algebra, Math. Comp. 20 (1966) 369–378.

doi:10.1090/

S0025-5718-1966-0234618-4.

[28] C. C. Paige, Computational Variants of the Lanczos Method for the Eigenproblem, IMA Journal of Applied Mathematics 10 (3) (1972)

373–381. doi:10.1093/imamat/10.3.373.

[29] C. C. Paige, Error Analysis of the Lanczos Algorithm for Tridiagonalizing a Symmetric Matrix, IMA Journal of Applied Mathematics 18 (3)

(1976) 341–349. doi:10.1093/imamat/18.3.341.

[30] C. Paige, Accuracy and eﬀectiveness of the Lanczos algorithm for the symmetric eigenproblem, Linear Algebra and its Applications 34

(1980) 235–258. doi:10.1016/0024-3795(80)90167-6.

[31] G. Dvali, L. Eisemann, M. Michel, S. Zell, Black hole metamorphosis and stabilization by memory burden, Phys. Rev. D 102 (10) (2020)

103523. arXiv:2006.00011, doi:10.1103/PhysRevD.102.103523.

[32] G. Dvali, C. Gomez, Black Holes as Critical Point of Quantum Phase Transition, Eur. Phys. J. C 74 (2014) 2752. arXiv:1207.4059,

doi:10.1140/epjc/s10052-014-2752-3.

[33] J. D. Bekenstein, Black holes and entropy, Phys. Rev. D 7 (1973) 2333–2346. doi:10.1103/PhysRevD.7.2333.
[34] G. Dvali, Critically excited states with enhanced memory and pattern recognition capacities in quantum brain networks: Lesson from black

holes arXiv:1711.09079.

[35] G. Dvali, M. Michel, S. Zell, Finding Critical States of Enhanced Memory Capacity in Attractive Cold Bosons, EPJ Quant. Technol. 6 (2019)

1. arXiv:1805.10292, doi:10.1140/epjqt/s40507-019-0071-1.

[36] G. Dvali, A Microscopic Model of Holography: Survival by the Burden of Memory arXiv:1810.02336.
[37] S. W. Hawking, Particle Creation by Black Holes, Commun. Math. Phys. 43 (1975) 199–220, [Erratum: Commun. Math. Phys. 46, 206

(1976)]. doi:10.1007/BF02345020.

[38] S. Velamparambil, S. MacKinnon-Cormier, J. Perry, R. Lemos, M. Okoniewski, J. Leon, GPU Accelerated Krylov Subspace Methods for
Computational Electromagnetics, in: 2008 38th European Microwave Conference, 2008, pp. 1312–1314. doi:10.1109/EUMC.2008.
4751704.

[39] M. Brenes, V. Varma, A. Scardicchio, I. Girotto, Massively parallel implementation and approaches to simulate quantum dynamics using
Krylov subspace techniques, Computer Physics Communications 235 (2019) 477–488. arXiv:1704.02770, doi:10.1016/j.cpc.2018.
08.010.

[40] S. Paeckel, T. K¨ohler, A. Swoboda, S. R. Manmana, U. Schollw¨ock, C. Hubig, Time-evolution methods for matrix-product states, Annals of

Physics 411 (2019) 167998. arXiv:1901.05824, doi:https://doi.org/10.1016/j.aop.2019.167998.

18

