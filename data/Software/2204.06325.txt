2
2
0
2

r
p
A
3
1

]
E
S
.
s
c
[

1
v
5
2
3
6
0
.
4
0
2
2
:
v
i
X
r
a

LESSONS LEARNED FROM REPLICATING A STUDY ON
INFORMATION-RETRIEVAL BASED TEST CASE PRIORITIZATION

A PREPRINT

Nasir Mehmood Minhas
Department of Software Engineering, Blekinge Institute of Technology
nasir.mehmood.minhas@bth.se

Mohsin Irshad
Ericsson Sweden AB, Karlskrona, Sweden
mohsin.irshad@ericsson.com

Kai Petersen
Department of Software Engineering, Blekinge Institute of Technology
University of Applied Sciences Flensburg, Germany
kai.petersen@bth.se

JÃ¼rgen BÃ¶rstler
Department of Software Engineering, Blekinge Institute of Technology
jurgen.borster@bth.se

ABSTRACT

Objective: In this study, we aim to replicate an artefact-based study on software testing to address
the gap. We focus on (a) providing a step by step guide of the replication, reï¬‚ecting on challenges
when replicating artefact-based testing research, (b) Evaluating the replicated study concerning its
validity and robustness of the ï¬ndings.
Method: We replicate a test case prioritization technique by Kwon et al. We replicated the original
study using four programs, two from the original study and two new programs. The replication
study was implemented using Python to support future replications.
Results: Various general factors facilitating replications are identiï¬ed, such as: (1) the importance
of documentation; (2) the need of assistance from the original authors; (3) issues in the maintenance
of open source repositories (e.g., concerning needed software dependencies); (4) availability of
scripts. We also raised several observations speciï¬c to the study and its context, such as insights
from using different mutation tools and strategies for mutant generation.
Conclusion: We conclude that the study by Kwon et al.
is replicable for small and medium
programs and could be automated to facilitate software practitioners, given the availability of
required information.

Keywords: Replication, Regression Testing, Technique, Test case prioritization, Information
retrieval, SIR

1

Introduction

Replications help in evaluating the results, limitations, and validity of studies in different contexts [53]. They also help
establishing or expanding the boundaries of a theory [10, 53].

 
 
 
 
 
 
Minhas et al.

A PREPRINT

During the previous four decades, software engineering researchers have built new knowledge and proposed new so-
lutions. Many of these lack consolidation [35] replication studies can help in establishing the solutions and expanding
the knowledge. Software engineering researchers have been working on replication studies since the 1990s. Still,
the number of replicated studies is small, and a more neglected area is the replication of software testing experi-
ments [9, 10, 35, 51]. Most software engineering replication studies are conducted for experiments involving human
participants; few replications exist for artefact-based experiments [10].

In the artefacts-based software engineering experiments, the majority of the authors use the artefacts from the software
infrastructure repository (SIR) [58]. Do et al.[13] introduced SIR in 2005 to facilitate experimentation and evaluation
of testing techniques and to promote replication of experiments and aggregation of ï¬ndings.

Researchers proposed different techniques to support regression testing practice, and there are various industrial eval-
uations of regression testing techniques. Adopting these techniques in practice is challenging because the results are
inaccessible for the practitioners [5]. Replications of existing solutions on regression testing can be helpful in this
regard, provided the availability of data and automation scripts of these replications.

Attempts have been made to replicate regression testing techniques. The majority of these replications are done by
the same group of authors who originally proposed the techniques [14â€“16]. There is a need for conducting more
independent replications in software engineering [13]. However, evidence of independent replications in regression
testing is low [10].

Overall, we would highlight the following research gaps concerning replications:

â€¢ Gap 1: Only a small portion of studies are replications: Among the reasons for a lower number of repli-
cations in software engineering is the lack of standardized concepts, terminologies, and guidelines [35].
Software engineering researchers need to make an effort to replicate more studies.

â€¢ Gap 2: Lack of replication guidelines: There is a need to work on the guidelines and methodologies to

support replicating the studies [12].

â€¢ Gap 3: Lack of replications in speciï¬c subject areas: Software testing as a subject area has been highlighted
as an area lacking replication studies [10]. According to Da Silva et al. [10] the majority of replication studies
focuses on software construction and software requirements. Despite a well-researched area, the number of
replication studies in software testing is at the lowest than other software engineering research areas according
to MagalhÃ£es et al. [12].

â€¢ Gap 4: Lack of studies on artefact-based investigations: Only a few replicated studies focused on artefact-
based investigations [10]. That is, the majority of studies focused on experiments and case studies involving
human subjects. Artefact-based replications are of a particular interest as they require to build and run scripts
for data collection (e.g., solution implementation and logging), and at the same time compile and run the
software systems, which are the subject of study.

Considering the gaps stated above, we formulate the following research goal:

Goal: To replicate an artefact-based study in the area of software testing, with a focus
on reï¬‚ecting on the replication process and the ability to replicate the ï¬ndings of the
original study.

To achieve our research goal, we present the results of our replication of an IR-based test case prioritization technique
proposed by Kwon et al. [36]. The authors introduced a linear regression model to prioritize the test cases target-
ing infrequently tested code. The inputs for the model are calculated using term frequency (TF), inverse document
frequency (IDF), and code coverage information [36]. TF and IDF are the weighing scheme used with information
retrieval methods [48]. The original studyâ€™s authors used open-source data sets (including SIR artefacts) to evaluate the
proposed technique. We attempted to evaluate the technique using four programs to see if the replication conï¬rms the
original studyâ€™s ï¬ndings. We selected two programs from the original study and two new cases to test the techniqueâ€™s
applicability on different programs.

Our research goal is achieved through the following:

1. Objective 1: Studying the extent to which the technique is replicable. Studying the extent to which the
technique is replicable and documenting the detail of all steps will help draw valuable lessons. Hence,
contributing with guidance for future artefact-based replications (Gap 2, Gap 4).

2

Minhas et al.

A PREPRINT

2. Objective 2: Evaluating the results of the original study [36]. Evaluating the results through the replication
provides an assessment of the validity and the robustness of the results of the original study. Overall, we
contribute to the generally limited number of replication studies (Gap 1) in general, and replication studies
focused on software testing in particular (Gap 3).

The organization of the rest of the paper is as follows: Section 2 provides a brief introduction to the concepts relevant to
this study. Section 3 presents a brief discussion of some replications carried out for test case prioritization techniques.
Along with the research questions and summary of the concepts used in the original study, Section 4 describes the
methodologies we have used to select the original study and conduct the replication. Threats to the validity of the
replication experiment are discussed in Section 4.6. Section 5 presents the ï¬ndings of this study, Section 6 provides
the discussion on the ï¬ndings of replication study, and Section 7 concludes the study.

2 Background

This section provides a discussion on the topics related to our investigation.

2.1 Regression testing

Regression testing is a retest activity to ensure that system changes do not affect other parts of the system negatively
and that the unchanged parts are still working as they did before a change [40, 58]. It is essential but expensive and
challenging testing activity [20]. Various authors have highlighted that testing consumes 50% of the project cost, and
regression testing consumes 80% of the total testing cost [20, 21, 25, 33]. Research reports that regression testing may
consume more than 33% of the cumulative software cost [34]. Regression testing aims to validate that modiï¬cations
have not affected the previously working code [14, 40].

Systems and Software Engineeringâ€“Vocabulary [28], deï¬nes regression testing as:

1. â€œSelective retesting of a system or component to verify that modiï¬cations have not caused unin-
tended effects and that the system or component still complies with its speciï¬ed requirements.â€
2. â€œTesting required to determine that a change to a system component has not adversely affected
functionality, reliability or performance and has not introduced additional defects.â€

For larger systems, it is expensive to execute regression test suites in full [40]. To cope with this, one of the suggested
solutions is test case prioritization. It helps to prioritize and run the critical test cases early in the regression testing
process. The goal of test case prioritization is to increase the test suiteâ€™s rate of fault detection [19].

A reasonable number of systematic literature reviews and mapping studies on various aspects of regression testing
provides evidence that regression testing is a well-researched area [3, 5, 7, 8, 11, 21, 23, 33, 34, 37, 45, 49, 54, 58, 59].
Despite a large number of regression testing techniques proposed in the literature, the adoption of these techniques
in the industry is low [18, 20, 46, 47]. The reasons are that the results of these techniques are not accessible for
practitioners due to the discrepancies in terminology between industry and academia [5, 20, 39]. There is a lack
of mechanisms to guide the practitioners in translating, analyzing, and comparing the regression testing techniques.
Furthermore, various authors use controlled experiments for their empirical investigations, and in most cases, it is hard
to assess that these experiments are repeatable and could ï¬t in an industrial setting [5]. Replication of empirical studies
could lead us to the desired solution, as it can help to conï¬rm the validity and adaptability of these experiments [53].

2.2 Replication

Replication is a means to validate experimental results and examine if the results are reproducible. It can also help
to see if the results were produced by chance or the results are the outcome of any feigned act [30]. An effectively
conducted replication study helps in solidifying and extending knowledge. In principle, replication provides a way
forward to create, evolve, break, and replace theoretical paradigms [35, 53]. Replication could be of two types 1) inter-
nal replication â€“a replication study carried out by the authors of the original study themselves, 2) external replication
â€“a replication study carried out by researchers other than the authors of the original study [35, 52].

In software engineering research, the number of internal replications is much higher than external replications [4, 10].
Da Silva et al. [10] reported in their mapping study that out of 133 included replication studies, 55% of the studies
are internal replications, 30% are external replications, and 15% are the mix of internal and external. Furthermore, the
results of 82% of the internal replications are conï¬rmatory, and the results of 26% of external replications conform to
the original studies [10]. From the empirical software engineering perspective, Shull et al. [53] classify replications as

3

Minhas et al.

A PREPRINT

exact and conceptual replication. In an exact replication, the replicators closely follow the procedures of the original
experiment, whereas, in a conceptual replication, the research questions of the original study are evaluated using a
different experimental setup. Concerning exact replication, if the replicators keep the conditions in the replication
experiment the same as the actual experiment, it would be categorized as exact dependent replication. If replicators
deliberately change the underlying conditions of the original experiment, it would be referred to as exact independent
replication. Exact dependent and exact independent replications could respectively be mapped to strict and differen-
tiated replications. A strict replication compels the researchers to replicate a prior study as precisely as possible. In
contrast, in a differentiated replication, researchers could intentionally alter the aspects of a previous study to test the
limits of the studyâ€™s conclusions. In most cases, strict replication is used for both internal and external replications
[35].

2.3

Information Retrieval

IR-based techniques are used to retrieve the userâ€™s information needs from an unstructured document collection. The
information needs are represented as queries [22, 57]. An information retrieval (IR) system is categorized by its
retrieval model because its effectiveness and utility are based on the underlying retrieval model [1]. Therefore, a
retrieval model is the core component of any IR system.

Amati [1] deï¬nes the information retrieval model as:

â€œA model of information retrieval (IR) selects and ranks the relevant documents with respect to a
userâ€™s query. The texts of the documents and the queries are represented in the same way, so that
document selection and ranking can be formalized by a matching function that returns a retrieval
status value (RSV) for each document in the collection. Most of the IR systems represent document
contents by a set of descriptors, called terms, belonging to a vocabulary V.â€

Some of the retrieval models are the vector space model (VSM), probabilistic relevance framework (PRF), binary
independence retrieval (BIR), best match version 25 (BM 25), and language modeling (LM). VSM is among the
popular models in information retrieval systems. It uses TF-IDF (term frequency and inverse document frequency) as
a weighing scheme [48].

Since the technique [36] we are replicating in this study uses the concepts of TF-IDF weighing scheme, we brieï¬‚y
present TF and IDF.

Term frequency (TF) and inverse document frequency (IDF) are statistics that indicate the signiï¬cance of each word in
the document or query. TF represents how many times a word appears in the document or query. IDF is an inverse of
document frequency (DF). The DF of a word indicates the number of documents in the collection containing the word.
Therefore a high IDF score of any word means that the word is relatively unique and it appeared in fewer documents
[22].

3 Related Work

Most of the replication studies on test case prioritization were conducted by the same group of authors, who primarily
re-validated/extended the results of their previously conducted experiments (see [14â€“16]). Below we discuss studies
that are closely related to our topic (i.e., test case prioritization).

Do et al. [16] conducted a replication study to test the effectiveness of the test case prioritization techniques origi-
nally proposed for C programs on different Java programs using the JUnit testing framework. The authorsâ€™ objective
was to test whether the techniques proposed for C programs could be generalized to other programming and testing
paradigms. The authors who conducted the replication study were part of the original studies, so by deï¬nition, it could
be referred to as an internal replication. However, concerning the implementation perspective, the replication study
would be regarded as differentiated replication.

Do and Rothermel [15] conducted an internal replication study to replicate one of their studies on test case prioriti-
zation. The original study used hand-seeded faults. In the replication study, the authors conducted two experiments.
In the ï¬rst experiment, the authors considered mutation faults. The goal was to assess whether prioritization results
obtained from hand-seeded faults differ from the results obtained by mutation faults. The authors used the same pro-
grams and versions used in the original study. They also replicated the experimental design according to the original
study. To further strengthen the ï¬ndings, later in the second experiment, the authors replicated the ï¬rst experiment
with two additional Java programs with different types of test suites.

4

Minhas et al.

A PREPRINT

Ouriques et al. [41] conducted an internal replication study of their own experiment concerning the test case prioriti-
zation techniques. In the original study, the authors experimented with programs closer to the industrial context. The
objective of the replication study was to repeat the conditions evaluated in the original study but with more techniques
and industrial systems as objects of study. Although the authors worked with the test case prioritization techniques,
they clearly stated that the methods examined in their research use a straightforward operation of adding one test case
at a time in the prioritized set. They do not use any data from the test case execution history, and hence, regression test
prioritization is not in the scope of their study.

Hasnain et al. [26] conducted a replication study to investigate the regression analysis for classiï¬cation in test case
prioritization. The authorsâ€™ objective to replicate the original study was to conï¬rm whether or not the regression model
used in the original study accurately produced the same results as the replicated study. Along with the program and
data set used in the original study, the authors also used an additional open-source Java-based program to extend the
original studyâ€™s ï¬ndings. It is an external replication study as all authors of the replication study are different from that
of the original study. The authors of the replicated study validated the results of the original study on an additional
dataset other than the one used in the original study, the replication is not strict.

In the above discussion of related studies, we learned that most replication studies conducted for test case prioritization
are primarily internal replications. We could only ï¬nd a single external replication study [26]. The authors of this
study conducted the replication of a classiï¬cation-based test case prioritization using regression analysis. Our study is
similar to this study based on the following factors, 1) our study is an external replication, 2) we also use two software
artefacts from the original study and two additional artefact. In many ways, our study is unique; for example, 1) we
are replicating a technique that focuses on less tested code, whereas Husnain et al. replicated a technique that is based
on fault classiï¬cation and non-faulty modules, 2) we have provided a step by step guide to support future replications,
and 3) we provide automated scripts to execute the complete replication study.

4 Methodology

For reporting the replication steps, we followed the guideline proposal provided by [6].
following for a replication study:

It suggests reporting the

1. Information about the original study (Section 4.2)

2. Information about the replication (Section 4.3.3)

3. Comparison of results to the original study (Section 5.2)

4. Drawing conclusions across studies (Section 7)

4.1 Research questions

In the presence of the constraint regarding experimental setup and data, we have to rely on the information presented
in the original study (see Section (4.2). We decided not to tweak the original studyâ€™s approach and followed the steps
proposed by the authors and executed the technique on one of the artefacts used by the authors. The differential aspects
of the replication experiment are the mutants and the automation of the major steps of the technique. According to the
classiï¬cation provided by Shull et al. [53], our work can be classiï¬ed as exact independent replication of the test case
prioritization technique presented in [36].

To achieve the objectives of the study we asked the following two research questions:

RQ1. To what degree is the study replication feasible given the information provided?

RQ1.1 To what degree is the study replicable with the programs used by the original authors?
RQ1.2 What is the possibility to replicate the study with the new programs?

The answer to RQ1 corresponds to Objective 1. While answering RQ1, the motive was to
see the possibility to replicate the technique presented in the original study using different
programs.

RQ2. Does the replication conï¬rm the ï¬ndings of the original study? The answer to RQ2 corresponds
to Objective 2. The motive of RQ2 was to see if the replication results conform to the ï¬nding
of the original study. To ensure that there should be no conscious deviation from the basic
technique, we followed the steps and used the tools mentioned in the original study. Finally,
we evaluated the replication results using the average percentage of fault detection (APFD) as
suggested by the original studyâ€™s authors.

5

Minhas et al.

A PREPRINT

4.2

Information about the original study

4.2.1 Selection of target

Selection of a target study for replication is a difï¬cult process, and often it is prone to biases due to various reasons
[44]. For example, clinical psychology research reports that authors tend to choose targets that are easy to set up and
execute [44]. The selection of target must be purpose-based, either by following systematic criteria (see, e.g., [44]) or
other justiï¬able reasons. In our case, the selection of the target is based on the needs identiï¬ed from our interaction
with industry partners [5, 39, 40] and reported facts in the related literature [54, 58].

For the selection of the target study, our ï¬rst constraint was test case prioritization, whereas the underlying criteria
were to search for a technique that can help control fault slippage and increase the fault detection rate. During our
investigations [40], we identiï¬ed that test case prioritization is among the primary challenges for practitioners, and
they are interested in ï¬nding techniques that can overcome their challenges and help them follow their goals (see also
[39]). Increasing the test suiteâ€™s rate of fault detection is a common goal of regression test prioritization techniques
[37, 42], whereas controlling fault slip through is among the goals of the practitioners [39, 40].

Our next constraint was selecting a study where authors used the SIR system to evaluate their technique(s). Singh et
al. [54] reported that out of 65 papers selected for their systematic reviews on regression test prioritization 50% are
using SIR systems. Yooo et al. [58] also reported that most of the authors evaluate their techniques using SIR artefacts.
They highlight that use of SIR systems allows replication studies.

The ï¬nal constraint was to select a target study that uses IR methods for the prioritization technique. Recent studies
report that test case prioritization techniques based on IR concepts could perform better than the traditional coverage-
based regression test prioritization techniques [43, 50].

We searched Google Scholar with the keywords â€œregression testingâ€, â€œtest case prioritizationâ€, â€œinformation retrieval
(IR)â€, â€œsoftware infrastructure repository (SIR)â€. Our searches returned 340 papers. After scanning the abstracts,
we learned that there is not a single technique that explicitly states controlling fault slippage as its goal. However,
the technique presented in [36] focused on less tested code, and the goal was to increase the fault detection rate of
coverage-based techniques using IR methods. Ignored or less tested code could be among the causes of fault slippage.
Therefore we considered the technique by Kwon et al. [36] for further evaluation. We evaluated this technique using
the rigor criteria as suggested by Ivarsson and Gorschek [29]. The authors suggest evaluating the rigor of empirical
studies based on context, design, and validity threats.

After considering all the factors mentioned above and applying the rigor rubrics, the study presented in [36] was used
as a target for replication.

4.2.2 Describing the original study

Kwon et al. [36] intended to improve the effectiveness of test case prioritization by focusing on infrequently tested
code. They argued that test case prioritization techniques based on code coverage might lack fault detection capability.
They suggested that using the IR-based technique could help overcome the limitation of coverage-based test case
prioritization techniques. Considering the frequency at which code elements have been tested, the technique uses a
linear regression model to determine the fault detection capability of the test cases. Similarity score and code coverage
information of test cases are used as input for the linear regression model. Kwon et al. [36] stated that the technique
they proposed is the ï¬rst of its type that considers less tested code and uses TF-IDF in IR for test case prioritization.
The authors claimed that their approach is also ï¬rst in using linear regression to weigh the signiï¬cance of each feature
regarding fault-ï¬nding. They divided the process into three phases, i.e., validation, training, and testing, and suggested
using the previous fault detection history or mutation faults as validation and training data.

6

Minhas et al.

A PREPRINT

Kwon et al. [36] suggested the following steps to implement the proposed technique:

1. Coverage of each test case

2. Set IDF threshold with validation data (previous or mutation faults)

3. Calculate TF/IDF scores of each test case

4. Use coverage and sum of TF/IDF of a test case as predictor values in the training

data

5. Use previous (mutation) faults as response values in the training data

6. Estimate the regression coefï¬cients (weight of each feature) with the training data

7. Assign predictor values (coverage and TF/IDF scores) to the model to decide the test

schedule

8. Run the scheduled test cases

To evaluate the proposed test case prioritization technique (IRCOV), [36] used four open-source Java programs XML-
Security (XSE), Commons-CLI (CCL), Commons-Collections (CCN), and Joda-Time (JOD). [36] highlighted that the
fault information of the programs was not sufï¬ciently available, and they were unable to evaluate their approach using
available information. Therefore, the authors simulated the faults using mutation. To generate the mutants, they used
the mutation framework MAJOR [31, 32]. To reduce the researcherâ€™s bias and achieve reliable results, they applied
ten fold validation and divided the mutation faults into ten subsets and assigned each subset to training, validation, and
test data.

4.2.3 Concepts used in the original study

The original study [36] makes use of IR concepts. It views a â€œdocumentâ€ as a test case, â€œwordsâ€ as elements covered
(e.g., branches, lines, and methods), and â€œqueryâ€ as coverage elements in the updated ï¬les. TF and IDF scores of the
covered elements determine their signiï¬cance to a test case. The number of times a test case exercises a code element
is counted as a TF value. The document frequency (DF) represents the number of test cases exercising an element.
IDF is used to ï¬nd the unique code elements as it is the inverse of DF.

Since the focus of the proposed technique was on less-tested code, the IDF score has more signiï¬cance, and it is
required to minimize the impact of TF. To minimize the impact of TF score on the test case prioritization, they used
Boolean values for TF (i.e., ğ‘‡ ğ¹ = 1 if a test case covers the code element, ğ‘‡ ğ¹ = 0 otherwise). To assign an IDF score
to a code element the IDF threshold is used. [36] deï¬ne the IDF threshold as:

â€œThe maximum number of test cases considered when assigning an IDF score to a code element.â€

The IDF threshold is decided by the validation data that consists of faults and related test cases from the previous test
execution history or mutation faults.

Finally, the authors used the similarity score between a test case (document) and the changed element (query) to
indicate the test cases related to modiï¬cations. The similarity score is measured using the sum of TF-IDF scores of
common elements in the query.

4.2.4 Key ï¬ndings of the original study

Using four open-source Java programs, the authors compared their technique with random ordering and standard code-
coverage-based methods (i.e., line, branch, and method coverage). They measured the effectiveness using Average
Parentage of Fault Detection (APFD).

The authors concluded that their technique is more effective as it increased the fault detection rate by 4.7% compared
to random ordering and traditional code coverage-based approaches.

4.3

Information about the replication

We ï¬rst present contextual information, i.e. data availability (Section 4.3.1) and division of the roles during the
replication (Section 4.3.2). Thereafter, we describe how the replication steps were implemented (Section 4.3.3).

7

Minhas et al.

A PREPRINT

4.3.1 Authorsâ€™ consent and Data availability

We contacted the original authors to get their consent and ask for any help to replicate their work. We asked them if
they can share their experimental package and data with us. We received a reply from one of the corresponding authors
and were informed that they do not have any backups related to this study since they conducted this study a few years
ago. However, they do not have any objection to the replication of their work.

4.3.2 Roles involved

All four authors of this study were given a speciï¬ed role in the replication. The ï¬rst and second authors jointly selected
the candidate study. The ï¬rst author conceptualized the whole replication process, including the logical assessment
of the study to be replicated. The second author who is an industry practitioner set up the environment according to
the requirements of the programs. Both the ï¬rst and second authors jointly performed the replication steps. The ï¬rst
author provided his input for every step, while the second author carried out the actual implementation. The third and
fourth authors reviewed study design and implementation steps.

4.3.3 Replication steps

We aimed to make an exact replication of the original study, and therefore we followed the procedure strictly as
presented in the original study [36]. The original study IRCOV was built using total and additional line, branch, and
method coverage. However, [36] stated that the results of IRCOV with total and additional coverage were similar.
Therefore we only used the total coverage to built the IRCOV models. The sequence of events followed in the
replication experiment are shown in Figure 1.

Replication objects: We built IRCOV models based on three coverage approaches (i.e., Line, Branch, and Method
coverage). We aimed to build the IRCOV model using four programs, two from the original study and two new
programs. Table 1 presents the details of the programs used in the replication of IRCOV. The programs are Common
CLI, XML security, Commons email, and Log4j. We were able to implement IRCOV with Commons CLI, but due
to various constraints discussed in Section 5, we failed to replicate IRCOV with XML security, Commons email, and
Log4j.

Table 1: Programs used in replication

Program

Version

LOC

Test
Classes

Used
[36]

in

Repository

Commons CLI
XML Security
Commons Email
Log4j

1.1, 1.2
2.2.3
master
master

13210
21315
83154
169646

23
172
20
63

Yes
Yes
No
No

SIR & GitHub
SIR & GitHub
GitHub
SIR & GitHub

We selected Commons CLI and XML security as these were used in the original study. Commons CLI1 is a library
providing an API parsing of command-line arguments. XML-security2 for Java is a component library implementing
XML signature and encryption standards. To see if the technique (IRCOV) is replicable with other programs, we
selected Commons Email and Log4J. Commons Email3 is built on top of the JavaMail API, and it aims to provide an
API for sending email.
Log4j4 is a Java based logging utility. Log4j 2 was released in 2014 to overcome the limitations of its predecessor
version Log4j 1. We obtained the programs from GitHub and used the test suites provided with the programs.

Mutant generation: The fault information of the programs was not available, and therefore we used mutation faults
insteadâ€“the authors of the original study used mutation faults. For the mutation, we used the tool (MAJOR) [31, 32].

Partitioning mutants into training, validation, and test sets: As per the description in the original study, we classiï¬ed
the mutants into training, validation, and test sets (10%, 10%, and 80%, respectively). To classify the data, we used
an online random generator5. We applied the ten-fold validation technique to ensure the reliability of the results and

1https://commons.apache.org/proper/commons-cli/
2http://santuario.apache.org/javaindex.html
3https://commons.apache.org/proper/commons-email/
4https://logging.apache.org/log4j/2.x/
5https://approsto.com/random-line-picker/

8

Minhas et al.

A PREPRINT

Figure 1: Steps followed to replicate the original study

avoid any bias. To create ten folds of each data set (i.e., training, validation, and test sets), we wrote automation scripts
[27].

IDF threshold: The purpose of setting up an IDF threshold is to ensure that prioritized test cases should detect faults
in less tested code elements. The IDF threshold is decided using validation data containing information of faults and
of test cases detecting the faults. To calculate the IDF threshold the authors of the original study [36] suggested using
a ratio from 0.1 to 1.0 in Equation 1.

ğ¼ ğ·ğ¹ ğ‘‡ â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ = ğ‘›ğ‘œ ğ‘œ ğ‘“ ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ğ‘ğ‘ ğ‘’ğ‘  Ã— ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ

(1)

We trained the regression model with each threshold using validation data and selected the ratio that led to the min-
imum training error for the IDF threshold. Based on the minimum training error, Table 2 presents the chosen values
for the IDF threshold of all ten folds of Commons CLI. We assigned IDF values to only those code elements whose
DF was not above the IDF threshold.
Calculating TF and IDF score: As suggested in the original study [36], we use Boolean values for TF (i.e., ğ‘‡ ğ¹ = 1 if
the test case covers the element, ğ‘‡ ğ¹ = 0 otherwise). The purpose to ï¬x the TF values as 0 or 1 was to ensure that only
test case would be prioritized that are targeting less tested code. The IDF score is more signiï¬cant in this regard. As
suggested in the original study [36], we used Equation 2 to calculate the IDF score.

9

Using training datacalculate coverage ofeach test casePartition the mutants intotraining, validation, and testsets (10%, 10%, & 80%).Generate mutants*using tool Set IDF thresholdusing validation data(mutation faults)Using training data and IDFthreshold set IDF scores,also set TF scoresUsing TF-IDF scorescalculate similarity scoresEstimate the regressioncoefficients Assign predictor values (Coverage andsimilarity scores) along with the coefficientsto the model to decide the test scheduleRun the scheduled test cases10 Foldscompleted?StopYesNo* Mutants are required ifprevious faults are notavailableClasses of theprograms to be testedStartMinhas et al.

A PREPRINT

ğ¼ ğ·ğ¹ = 1 + ğ‘™ğ‘œğ‘”

(cid:18)

# ğ‘œ ğ‘“ ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ğ‘ğ‘ ğ‘’ğ‘ 
# ğ‘œ ğ‘“ ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘ğ‘ğ‘ ğ‘’ğ‘  ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘” ğ‘¡â„ğ‘’ ğ‘’ğ‘™ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡

(cid:19)

(2)

Similarity score: The similarity score directly contributes to the IRCOV model. In the regression model (see Equation
4), ğ‘¥2 refers to the similarity score of each test case. We have calculated the similarity scores using Equation 3 as
suggested in [36].

ğ‘†ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘¡, ğ‘) =

âˆ‘ï¸

ğ‘’ âˆˆğ‘¡âˆ©ğ‘

ğ‘¡ ğ‘“ âˆ’ ğ‘–ğ‘‘ğ‘“ e,t

(3)

Since TF values are 1 or 0 (i.e., if a test case excises a code element, then TF is 1; otherwise, it is 0), practically
similarity scores are the sum of IDF scores of the elements covered by a particular test case.
Coverage information: The coverage measure is aslo used in the regression model. In Equation 4, ğ‘¥1 refers to the
coverage size of each test case. To measure code size (line of code) and coverage of each test case, we used JaCoCo6.

IRCOV model: We used Equation 4 for the linear regression model as suggested in the original study [36].

ğ‘¦ = ğœƒ0 + ğœƒ1ğ‘¥1 + ğœƒ2ğ‘¥2

(4)

In Equation 4, ğ‘¥1 is the size of the coverage data for each test case, and ğ‘¥2 refers to the similarity score of each test
case. The value of y represents each test caseâ€™s fault detection capability, which is proportional to the number of
previous faults detected by the test case. In the regression model, three coefï¬cients need to be calculated (i.e., ğœƒ0, ğœƒ1,
& ğœƒ2). Here ğœƒ0 represents the intersect, whereas, to calculate ğœƒ1 and ğœƒ2 [36] suggested using Equation 5, which uses ğ‘¦
value and respective values of ğ‘¥1 and ğ‘¥2. Here ğ‘¦ could be calculated using Equation 6, where as ğ‘¥1 and ğ‘¥2 respectively
represent the size of coverage and similarity scores of each test case.

ğ‘¡â„ğ‘’ğ‘¡ğ‘ = (ğ‘‹ğ‘‡ ğ‘‹)âˆ’1 ğ‘‹ğ‘‡ (cid:174)ğ‘¦

ğ‘¦ =

ğ‘›
âˆ‘ï¸

ğ‘›=1

ğ‘“ i
ğ‘™ğ‘œğ‘”(ğ‘¡i) + 1

(5)

(6)

Prioritization based on fault detection capability: After having the values of coefï¬cients and variables of regression
model (i.e., ğœƒ0, ğœƒ1, ğœƒ2, ğ‘¥1, and ğ‘¥2 ), we determined the fault detection capability of each test case using the IRCOV
model (see Equation 4). Finally, we arranged the test cases in the descending order of the calculated fault detection
capability.

Evaluating the technique: After having a prioritized set of test cases, we ran them on the 50 faulty versions of each
fold we created using test data set of mutants. To evaluate the results, we used the average percentage of fault detection
(APFD) (see Equation 7).

ğ´ğ‘ƒğ¹ ğ· = 1 âˆ’

ğ‘‡ ğ¹1 + ğ‘‡ ğ¹2 + .... + ğ‘‡ ğ¹N
ğ‘›ğ‘š

+

1
2ğ‘›

(7)

4.4 Analysis of the replication results

We implemented IRCOV for line, branch, and method coverage. As described above, for all coverage types, we
calculated the APFD values for each fold, we also captured the intermediate results (see Table 2).

To compare the replication and the original study results, we translated the APFD values for Commons CLI from the
original study. Then we plotted the APFD values of the original and replication study in the box plot, a statistical tool
to visually summarize and compare the results of two or more groups [14, 55]. Box plot of APFD values enabled us
to visually compare the replication and original study results.

6https://www.eclemma.org/jacoco/

10

Minhas et al.

A PREPRINT

Figure 2: Steps to automate the replication of IRCOV

To triangulate our conclusions, we applied hypothesis testing. We used Wilcoxon signed-rank test to compare the
results of IRCOV original and IRCOV replication results. Also in the original study [36] used Wilcoxon signed-rank
test to compare the IRCOV results with the baseline methods. Wilcoxon signed-rank test is suitable for paired samples
where data is the outcome of before and after treatment. It measures the difference between the median values of paired
samples [24]. In our case, we were interested in measuring the difference between the median APFD values of IRCOV
original and IRCOV replication. Therefore, the appropriate choice to test our results was Wilcoxon singed-rank test.

We tested the following hypothesis:

H0LC: There is no signiï¬cant difference in the median APFD values of original and replication study

using line coverage.

H0BC: There is no signiï¬cant difference in the median APFD values of original and replication study

using branch coverage.

H0MC: There is no signiï¬cant difference in the median APFD values of original and replication study

using method coverage.

4.5 Automation of Replication

The replication was implemented using Python scripts. They are available [27]. Figure 2 presents the details of
automation steps for the replication of IRCOV. The original studyâ€™s authors proposed that ten-fold-based execution
is needed (when historical data is not available) to evaluate their original technique. Therefore, our implementation
(fold_generator) [27] generates ten folds of the object program at the ï¬rst stage. Thereafter, it generates ï¬fty faulty
versions of each fold, whereas each version contains 5-15 mutants (faults). After generating the faulty versions, the
script makes the corresponding changes in the code. Finally, the tests are executed, and their results are extracted.
Later, using the test results, we calculate the APFD values of each fold. The calculation of APFD values is the only
step not handled in our script. We used excel sheets to calculate APFD values.

11

Generate 10-Fold  Sets (Each set  contains 50-faultyprograms having  5-15 mutants) and Thresholdvalues set Implement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsFaulty  Program 2Faulty  Program 50Faulty  Program 1Implement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsFaulty  Program 2Faulty  Program 50Faulty  Program 1Implement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsImplement Codechanges accordingto MutantsFaulty  Program 2Faulty  Program 50Faulty  Program 1Parse 50 filescontaining Mutantsusing  Python scriptFold 1Parse 50 filescontaining Mutantsusing  Python scriptFold 2Parse 50 filescontainingMutants using  Python scriptFold 10Execute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsExecute TestCases &Store ResultsCollectResults ofEach faultyprogramCollectResults ofEach faultyprogramCollectResults ofEach faultyprogramFold 1Fold 2Fold 103-9Folds3-9Folds3-9Folds3-9Folds3-9FoldsAPFD Values for each faultyprogram of 10-FoldsPython ScriptManual ProcessCalculateCoverageCalculateSimilarityScoresTrain Model andGetPrioritized SetMinhas et al.

A PREPRINT

4.6 Threats to validity

4.6.1 Internal validity

Internal validity refers to the analysis of causal relations of independent and dependent variables. In our case, we have
to see if the different conditions affect the performance of IRCOV. IRCOV depends upon two inputs, coverage of each
test case and a similarity score calculated based on TF-IDF. We used the test cases available within the programs.
Therefore, we do not have any control over the coverage of these test cases. However, the choices of mutants can
impact the similarity score. To avoid any bias, we generated the mutants using a tool and used a random generator to
select the mutants for different faulty versions of the programs. Furthermore, we trained IRCOV sufï¬ciently before
applying it to test data by following the tenfold validation rule. Since we measured the performance of IRCOV using
the APFD measure, the results of the successful case were not signiï¬cantly different from the original studyâ€™s results.
Therefore we can argue that our treatment did not affect the outcome of IRCOV. Hence minimized the threats to the
internal validity.

4.6.2 Construct validity

Construct validity is concerned with the underlying operational measures of the study.
In our case, since it is a
replication study and we followed the philosophy of exact replication [53]. Therefore, if the original study suffers of
any aspects of construct validity, the replication may do so. For instance, the use of mutation faults could be a potential
threat to the construct validity because of the following two reasons

â€¢ Mutation faults may not be representative of real faults.
â€¢ Possible researchersâ€™ bias concerning the nature of mutation faults.

Concerning the ï¬rst reason, the use of mutation faults to replace the real faults is an established practice and researchers
claim that mutation faults produce reliable results and hence can replace the real faults [2, 15]. To avoid any bias, we
used an automated mutation tool to generate the mutants. Also to select the mutants for validation, training, and
test set we used an automated random selector. Hence no human intervention was made during the whole process.
Furthermore, we discussed the strengths and weaknesses of different tools.

4.6.3 External validity

External validity is the ability to â€generalize the results of an experiment to industrial practiceâ€ [56]. The programs
used in the replication study are small and medium-sized Java programs. Therefore, we can not claim the generaliz-
ability of results to large-scale industrial projects. The results produced in replication align well with the results of the
original study. However, we could not demonstrate the use of the technique on the new programs.

5 Results

This section presents the ï¬ndings from the replication. The results are organized according to research questions listed
in Section 4.

5.1 RQ1. Degree to which the replication is feasible to implement.

The ï¬rst goal was to see if it is possible to replicate the IRCOV technique described in the study [36].

Out of the four replication attempts, we successfully replicated the IRCOV technique with the Commons CLI project.
However, with the other three projects (i) XML security, (ii) Commons email, (iii) Log 4j, the replication was either
partially successful or unsuccessful due to the reasons elaborated in the following.

Successful replication implementation: We successfully replicated IRCOV with Commons CLI. After going through
the steps presented in Section 4.3.3, for every fold, we were able to calculate the respective coverage information and
similarity score of each test case. Table 2 presents the intermediate results for the replication of IRCOV with Commons
CLI. These include, training error, chosen value of IDF threshold, regression coefï¬cient ğœƒ0, coverage weight ğœƒ1, and
wight for similarity score ğœƒ2).
To evaluate the performance of IRCOV, we have calculated APFD values for all ten folds of each coverage type
(branch, line, and method) (see Table 3). For branch coverage, the APFD value ranges from 0.547 to 0.873, whereas
the average (median) APFD value for branch coverage is 0.747. The APFD values for line coverage range from 0.609
to 0.873, and the average APFD value for line coverage is 0.809. Finally, the APFD value for method coverage ranges

12

Minhas et al.

A PREPRINT

Table 2: Simulation parameters for Commons CLI. (MC = Method coverage, LC = Line coverage, & BC = Branch
coverage)

Fold
Name

Fold1

Fold2

Fold3

Fold4

Fold5

Fold6

Fold7

Fold8

Fold9

Fold10

Coverage
Type

Training Error

IDF Threshold

ğœƒ 0

ğœƒ 1

ğœƒ 2

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

MC
LC
BC

1.0694
0.9770
0.8876

0.3195
0.3533
0.3567

0.6411
0.6404
0.6405

0.4783
0.4551
0.4947

0.1838
0.1856
0.1876

0.2247
0.1795
0.1549

0.1382
0.1364
0.1390

0.2020
0.2046
0.2046

0.1490
0.1532
0.1517

0.0339
0.0339
0.0343

7
2
2

5
6
5

6
6
6

6
6
6

5
4
4

2
2
2

10
10
10

6
6
6

6
6
6

10
10
10

-0.3478
0
0

-0.7976
-0.6084
-0.3386

-0.0286
-0.0498
-0.0380

-0.0687
-0.1086
0.1240

0.0309
0.0859
0.1406

-0.5284
-0.4869
-0.3978

-0.1479
-0.0833
0.0028

0.4389
0.3401
0.3286

0.1652
0.0540
0.0862

-0.1253
-0.1127
-0.0920

0.0187
0
0

0.0323
0.0088
0.0178

0.0008
0.0004
0.0008

0.0097
0.0032
0.0045

0.0068
0.0018
0.0038

0.0194
0.0052
0.0119

0.0115
0.0030
0.0065

-0.0024
-0.0001
-0.00001

-0.0032
-0.0002
-0.0012

0.0017
0.0004
0.0007

0.1426
0
0

-0.1472
-0.1343
-0.2095

0.0796
0.0736
0.0736

0.1677
0.1365
0.1683

0.0558
0.0612
0.0516

0.3548
0.3470
0.3149

-0.0141
-0.0234
-0.0235

0.0839
0.0715
0.0694

0.1473
0.1344
0.1434

0.0267
0.0261
0.0278

from 0.549 to 0.864, and the average APFD for method coverage is 0.772. These results show that the IRCOV model
performed best for the line coverage as the mean APFD for line coverage is highest among the coverages.

Partial or unsuccessful replication: Our ï¬rst unsuccessful replication was concerning XML security. We did not ï¬nd
all the program versions used in the original study (Study [36]). Therefore, we decided to use the versions that have
slightly similar major/minor release versions. We downloaded available XML security versions 1, 1.5 and 2.2.3. The
ï¬rst two downloaded versions (version 1 and version 1.5) were not compiling due to the unavailability of various
dependencies. The logs from the compilation failures are placed in folder â€œLogsXmlSecuritâ€ available at [38].

We were able compile the third XML security version 2.2.3, but we could not continue with it, because this version
contained several failing test cases (see [38]). With already failing test cases it was difï¬cult to train the model correctly
and get the appropriate list of prioritized test cases.

The second unsuccessful replication attempt was executed on Commons email. This time the replication was un-
successful because of faulty mutants generated by the mutant software. For instance, it suggested replacing variable
names with â€™nullâ€™ (see Listing 1 & 2). The actual code was ğ‘¡â„ğ‘–ğ‘ .ğ‘›ğ‘ğ‘šğ‘’ = ğ‘›ğ‘¢ğ‘™ğ‘™; while after mutant injection, the code
turned to ğ‘¡â„ğ‘–ğ‘ .ğ‘›ğ‘¢ğ‘™ğ‘™ = ğ‘›ğ‘¢ğ‘™ğ‘™.

Listing 1: Faulty mutant generated by the tool

3 5 :EVR: < IDENTIFIER ( j a v a . l a n g . S t r i n g ) >: <DEFAULT> : o r g . a p a c h e . commons . m a i l .
B y t e A r r a y D a t a S o u r c e @ s e t N a m e ( j a v a . l a n g . S t r i n g ) : 2 1 4 : name |== > n u l l

Listing 2: Code generated after the insertion of faulty mutant

13

Minhas et al.

A PREPRINT

p u b l i c v o i d setName ( f i n a l S t r i n g name )

{

/ /
t h i s . n u l l =

t h i s . name = n u l l ;

O r i g i n a l c o d e
/ / S u b s t i t u t e d by m u t a n t g e n e r a t o r }

n u l l ;

Another type of faulty instances were when MAJOR suggested to modify a line in the code that resulted in Java
compilation errors (such as "unreachable statement"). There were several such faulty mutants that made the program
fail to compile, and hence no further processing was possible. The detail of all faulty mutants is available in the folder
â€œCommonsEmailâ€ at [38].

We also made unsuccessful attempts to change the mutant generator to rectify this problem. However, each mutant
generator presented a new set of problems. The lessons learnt from usage of different mutant generators are described
in next section.

The third replication attempt was executed on the program Log4j. We followed all the steps (using automatic scripts)
proposed by the authors of the original study. We successfully generated the mutants for this program. However, the
replication was stopped at the point when the steps to train the model failed. The proposed approach in the original
study is based on the coverage information of each code class and test-class. This time the issue was caused by the
low coverage of the test cases. During the training of the model, we realized that because of low coverage of the
test cases, we were unable to calculate the values of regression coefï¬cients, and as a result, we could not generate
the prioritized set of test cases. We developed a Jupyter notebook to describe each steps of this partially successful
replication (see [27]). Compared to the other programs selected in this study, with 169646 LOC, Log4J is a large
program. Thus, a lot of time was needed to train the model for Log4J. For all ten folds, with ï¬fty faulty versions of
each fold and with ï¬ve to ï¬fteen faults in each faulty version, it required approximately 60 hours to train the model.

Key ï¬ndings: Concerning RQ1, the replication was only feasible in one of four cases;
the key reasons are listed below.

1. The inability to use the system under test was caused by compatibility issues

(unavailability of system versions and dependencies).

2. Already failing test cases made the replication fail.

3. Mutant generators created issues in running the replication, workarounds

were difï¬cult to implement.

4. Test cases require a certain level of coverage to train the model.

5. More effort is required to train the model for large-sized programs.

Table 3: APFD values for all ten folds of each coverage type

Folds

Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Fold 6
Fold 7
Fold 8
Fold 9
Fold 10

Branch
Cover-
age

Line
Cover-
age

Method
Cover-
age

0.865
0.790
0.613
0.755
0.721
0.829
0.839
0.585
0.594
0.803

0.874
0.816
0.646
0.757
0.725
0.796
0.841
0.610
0.548
0.736

0.874
0.866
0.643
0.816
0.715
0.829
0.839
0.610
0.622
0.803

14

Minhas et al.

A PREPRINT

5.2 RQ2. Comparison of the results to the original study.

Figure 3 presents the APFD boxplots of the original and replication study for Commons CLI. Boxplots with blue
patterns represent the original study results, and boxplots with gray patterns represent the replication study results.
We can see that in all cases, the APFD values of the original study are slightly better compared to the values of the
replication. We applied statistical tests to detect whether the results of the replication and the original study differ.

Figure 3: APFD Boxplots for IRCOV Original vs IRCOV Replication

IRCBO= IRCOV Branch coverage original, IRCBR= IRCOV Branch Coverage Replication
IRCLO= IRCOV Line coverage original, IRCLR= IRCOV Line coverage replication
IRCMO= IRCOV Method coverage original, IRCMR=IRCOV Method coverage replication

To compare the replication results for branch, line, and method coverage of Commons CLI with the original studyâ€™s
results, we applied Wilcoxon singed-rank test. The results are signiï¬cant if the p-value is less than the level of
signiï¬cance [17]. In our case, the difference between the two implementations would be signiï¬cant if the p-value is
less than 0.05.

Table 4 presents the results of statistical test. The p-value for branch coverage is 0.475, which is greater than 0.05
(signiï¬cance level). Therefore, we can not reject the null hypothesis. That means we can not show a signiï¬cant
difference in the APFD values for branch coverage of Commons CLI between the replication and the original study.

Similarly, the p-value for line coverage is 0.415, greater than the set signiï¬cance level. Based on the statistical results,
we can not reject the null hypothesis. This implies that we can not show a signiï¬cant difference in the APFD values
for the line coverage of Commons CLI between the replication and the original study.

Finally, the p-value for method coverage is 0.103, based on this result, we can not reject the null hypothesis. Therefore
no signiï¬cant difference in the APFD values for the method coverage of Commons CLI between the replication and
the original study.

Table 4: Statistical results of replication compared to the original study for Commons CLI.

Coverage

Branch
Line
Method

ğ›¼

0.05
0.05
0.05

p-value

95% Conf. Int.

0.475
0.415
0.103

0.646 - 0.816
0.668 - 0.845
0.652 - 0.827

15

0,300,400,500,600,700,800,901,00IRCBOIRCBRIRCLOIRCLRIRCMOIRCMRMinhas et al.

A PREPRINT

From the t-test results, we can conclude that for all three coverage types (branch, line, and method), we did not ï¬nd
any signiï¬cant difference between the replication and the original study. Therefore, we can state that the replication
experiment did not deviate from the original result to a degree that would lead to the test detecting a signiï¬cant
difference.

Key ï¬ndings: Concerning RQ2, we compared the replication results of the successful
case (i.e., Commons CLI) with the original studyâ€™s results. Below are the key ï¬ndings
for RQ2.

1. The statistical test did not detect a signiï¬cant difference in the APFD val-
ues of the replication and the original study concerning the three coverage
measures investigated.

2. We conclude that the results of the original study are veriï¬able for Commons

CLI.

6 Discussion

6.1 Lessons learned of replicating artefact-based studies in software testing

We replicated the study presented in [36] with the intent of promoting artefact-based replication studies in software
engineering, validating the correctness of the original study, and exploring the possibilities to adopt regression testing
research in the industry.

Overall, it is essential to capture and document assumptions and constraints concerning the techniques that are repli-
cated, as well as the conditions for being able to run a replication. We highlight several factors of relevance that were
observed.

Conditions concerning System under Test (SuT) complexity: From the replication results, we learned that besides the
various constraints, the technique (IRCOV) presented in [36] is replicable for small and medium programs provided
the availability of context information. The technique with its current guidelines is difï¬cult to implement with large-
size programs because it requires a signiï¬cant amount of effort to train the model. For example, the restriction of
10-folds, ï¬fty faulty versions for every fold, and 5 to 15 faults in every faulty version would require a substantial effort
(approximately 60 hours) to train the model for large-size programs. This limitation can be managed by reducing the
number of faulty versions for each fold, but this may degrade the accuracy and increase the training error.

Conditions concerning the characteristics of the test suite: Test cases available with Log4j 2 have low coverage,
limiting the chance of correctly training the model and generating a reasonable prioritization order of the test cases.
Coverage is one of the primary inputs required for the test case prioritization using the IRCOV model. Another
problem we encountered was the presence of already failing test cases in one of the versions of XML security. Test
If a handful of test cases fail
cases are used to calculate the coverage score and similarity scores of the project.
(as in XML security version 2.2.3), wrong coverage information and similarity scores are calculated. This results
in the wrong prioritization of test cases as well faulty training of the model (which is used to identify prioritized
test cases). Another drawback with failing test cases concerns the use of mutations. If tests are already failing and
when mutants are introduced, then the effectiveness is unreliable as tests are already failing because of other issues.
Further conditions may be of relevance in studies focusing on different aspects of software testing. Here, we would
highlight how important it is to look for these conditions and document them. This is also of relevance for practice, as
it demonstrates under which conditions a technique may or may not be successful in practice.

Availability of experimental data for artefact-based test replications: One of the constraints regarding the replicability
of the IRCOV technique is the availability of experimental data. For example, the authors of the original study [36]
stated that they used in-house built tools to conduct the experiment but, they did not provide any source of these tools,
also not including details of the automation tools. Therefore, it took a signiï¬cant effort to set up the environment to
replicate IRCOV with the ï¬rst program. There are various limitations concerning the data sets and tools required to
work with the recommended steps. Regarding data sets, we have recorded the ï¬ndings in Section 5. These include the
compatibility of SIR artefacts. For example, because of various dependencies, we faced difï¬culties while working with
XML security version 1. While working with version 2.2.3 of XML security, we encountered errors in the version.
Therefore, we could not collect the coverage information. Ultimately, we were unable to replicate the technique with
any of the versions of XML security.

16

Minhas et al.

A PREPRINT

No Mutation
Tool
1 Major7

2

ğœ‡Java8

3

Jester9

4

Jumble10

5

PIT11

Table 5: Comparison of mutant generators

Beneï¬ts

Challenges

(i) Easy to use.
(ii)
Most commonly used
mutant generator.

(i) Faulty mutant generated. (ii) Needs
upgrade to latest Java versions (iii) Doc-
umentation needs improvement.

(i) IDE plugin available
(ii) User decides what
types of mutants can be
generated.

(i) Exporting mutants separately is not
supported (ii) Does not support latest
Java versions (iii) GUI crashes often
while generating mutants.

Two types of Jester ver-
sions, a complete ver-
sion and a simple ver-
sion.

Latest update is more than 10 years ago.
We were unable to generate mutations
or start the program despite of follow-
ing all steps.

(i) Support recent Java
Inte-
versions.
gration with IDE Sup-
ported.

(ii)

The most
recent and
complete mutant gener-
ator. Mutants are gen-
erated and tests are exe-
cuted. A report is gen-
erated for the user.

Unable to generate mutants despite fol-
lowing examples. Latest update was 6
years ago.

(i) Unable to export the mutants.
(ii)
(iii)
Lack of diversity in the mutants.
Each execution produced exact same
mutants.

Reï¬‚ections on mutant generators: In the absence of failure data, the authors of the original study suggested using
mutation faults, and they used the MAJOR mutation tool to generate the mutants. In one of our cases (Commons
Email), the mutation tool (MAJOR) generated inappropriate mutants that led to the build failure. Therefore, no further
progress was possible with this case.

To overcome the difï¬culty with replication of project 3 (Commons Email), we tried different open-source mutation
generators available. Each of these presented various beneï¬ts and challenges that are documented in Table 5. After
trying out different mutation tools, we learned that among the available options, MAJOR is an appropriate tool for
Java programs, as it generates the mutants dynamically.

Reï¬‚ections on the IRCOV technique: Besides the various limitations highlighted earlier, the IRCOV technique is
replicable, and the replication results of the successful case (Commons CLI) show that the original authorsâ€™ claim
regarding the performance of the IRCOV technique was veriï¬able. The technique presented in the original study can
be valuable from the industry perspective because of its focus on prioritizing test cases detecting faults in less tested
code while taking coverage of test cases into account during the prioritization process. It can help the practitioners
work with one of their goals (i.e., controlled fault slippage). Looking at regression testing in practice, the practitioners
recognize and measure the coverage metric [40]. The only information that needs to be maintained in the companies
is failure history. In the presence of actual failure data, we do not need to use the mutants to train the IRCOV model
extensively, and we can reduce the number of faulty versions for each fold and the number of folds.

Overall, pursuing the ï¬rst RQ provided us with a deeper insight into the various aspects and challenges related to
external replication. The lessons learned in this pursuit are interesting and to provide recommendations in the context
of replication studies in software engineering. From the existing literature, it was revealed that the trend of replication
studies in software engineering is not encouraging [9, 10]. The studies report that the number of internal replications
is much higher than external replications [4, 10]. While searching the related work, we observed that in the software

7https://mutation-testing.org/
8https://cs.gmu.edu/ offutt/mujava/
9http://jester.sourceforge.net/
10http://jumble.sourceforge.net/
11https://pitest.org/

17

Minhas et al.

A PREPRINT

testing domain, compared to the internal replications, external replications are few in numbers. There could be sev-
eral reasons for the overall lower number of replication studies in software engineering, but we can reï¬‚ect on our
experiences concerning the external replications as we have undergone an external replication experiment.

One issue we would like to highlight is the substantial effort needed to implement the replication. Replication effort
can be substantially reduced with more detailed documentation of the original studies, the availability of appropriate
system versions and their dependencies, and the knowledge about prerequisites and assumptions. Better documenta-
tion and awareness of conditions may facilitate a higher number of replications in the future.

6.2 General lessons learned for artefact-based replications

Table 6 provides an overview of challenges we encountered during the replications. It lists the possible impact of
each challenge on the results of replication, and the table also presents a list of recommendations for researchers. The
following provides a brief discussion on the lessons learned in this study.

Table 6: Recommendations drawn from the challenges/lessons learned

Challenge

Impact

Recommendation

Documentation of orig-
inal experimental setup

Replicators have to invest
additional effort to under-
stand the context of
the
study.

Original authors need to
maintain/publish a compre-
hensive documentation of
experimental setup.

Collaboration with the
authors of original stud-
ies

In the absence of exper-
imental data and support
from original authors can
make the replication pro-
cess more complicated.

Issues with the open
source data sets

Replication
experiments
may fail due to these issues.

System under
Test
(SuT) and tools com-
patibility issues

Any compatibility issue of
the tools required to repli-
cate the original experiment
can create a bottleneck for
the replication.

In the event of request from
the replicators the authors
of the original study pro-
vide assistance in the form
of essential information re-
garding the original experi-
ment.

Open source repositories
need to maintained and be
up to date.

Such tools (e.g., Mutation
tools in our case) need to be
maintained to make them
compatible with new devel-
opment frameworks. The
same applies to the system
under test.

Documenting the original experiment: The authors of the original studies need to maintain and provide comprehensive
documentation concerning the actual experiment. The availability of such documents will help the independent repli-
cators understand the original studyâ€™s context. In the absence of such documentation, the replicators need to invest
more effort to understand the original studyâ€™s context. In this regard, we suggest using open source repositories to
store and publish the documentation. The documentation may contain the detail of the experimental setup, including
the tools used to aid the original experiment, automation scripts (if used/developed any), and the internal and ï¬nal
results of the study. Furthermore, the authors can also include detail about any special requirements or considerations
that need to be fulï¬lled for the successful execution of the experiment.

Collaboration with the original authors: Because of page limits posed by the journals and conferences, every aspect
of the study can not be reported in the research paper. Sometimes, the replicators need assistance from the original
authors regarding any missing aspect of the study. Therefore, it is essential that in case of any such query from the
replicators, the original studyâ€™s authors must willingly assist them. Such cooperations can promote replication studies
in software engineering. In our opinion, lack of collaboration is one reason for fewer replication studies in software
engineering. However, it is important to still conduct the replications as independently as possible due to possible
biases (i.e., avoiding to turn an external replication into an internal one).

18

Minhas et al.

A PREPRINT

Maintaining open source repositories: Open-source repositories (one example being SIR) provide an excellent op-
portunity for researchers to use the data sets while conducting software engineering experiments. A large number
of researchers have beneï¬ted from these data sets. We learned that some of the data sets available in repositories
are outdated and need to be maintained. Such data sets are not helpful, and studies conducted using these data sets
would be complex to adopt/replicate. It is therefore essential that authors explicitly state the the versions they used in
their own studies. In addition, we recommend that authors of original studies as well as replications ensure that the
dependencies or external libraries are stored to avoid that the system under test can not be used in replications.

Tools compatibility: In many cases, the authors need to use open source tools to assist the execution of their exper-
iment. Such tools need to be well maintained and updated. In case of compatibility issues, these tools can hinder
the replication process. For example, the study we replicated uses a mutation tool (MAJOR). Although it is one of
the best choices among the available options, the tool generated inappropriate mutants for one of our cases due to
some compatibility issues. Ultimately, after a signiï¬cant effort, we had to abandon the replication process for that
case. Here, we also would like to highlight that one should document the versions of the tools and libraries used (also
including scripts written by the researchers - e.g., in Phython).

Documenting successes and failures in replications: Besides the signiï¬cance of documenting every aspect of the
original experiment, recording every single event of replication (success & failure) is critical for promoting future
replications and industry adoptions of research. We recommend storing the replication setups and data in open source
repositories and providing the relevant links in the published versions of the articles.

Automation of replication: A key lesson learned during the replication of the original study is that the documentation of
the setup and execution of replication could be automated with the help of modern tools and programming languages.
This automation will help in reproducing the original results and analysis for researchers reviewing or producing the
results from the studies. We have provided programming scripts that describe and documented all the steps (and the
consequences of these steps).

7 Conclusions

This article reports the results of a replication of the test case prioritization technique using information retrieval (IR)
concepts proposed initially by [36]. We replicated the original study using four Java programs: Commons CLI, XML
security, Commons email, and Log4j. We selected two programs from the original study, and the other two were new.
We aimed to answer the two research questions. In RQ1, the aim was to see if the technique is replicable, and in RQ2,
we aimed to see if the replication results conform to the ones presented in the original study.

We have faced various challenges while pursing RQ1, these challenges are related to the availability of original ex-
perimental setup, collaboration with the original authors, system under test, test suites, and compatibility of support
tools. We learned that the technique is replicable for small programs subject to the availability of context information.
However, it is hard to implement the technique with the larger programs because it requires a substantial effort to train
it for a larger program.

To verify the original studyâ€™s results (RQ2), we compared the replication results for Commons CLI with the ones
presented in the original study. These results validated the original studyâ€™s ï¬ndings as the statistical test conï¬rms
no signiï¬cant difference between the APFD values of the replication and the actual experiment. However, we must
say that our results partially conformed with the original study because we could not replicate the technique with all
selected artefacts due to missing dependencies, broken test suites, and other reasons highlighted earlier.

The technique can be helpful in the industry context as it prioritizes the test cases that target the less tested code. It
can help the practitioners to control fault slippage. However, it needs some improvements in training and validation
aspects to scale the technique to the industry context. To support the future replications/adoption of IRCOV, we have
automated the IRCOV steps using Python (Jupyter notebook).

We plan to work with more artefacts with actual faults to test the techniqueâ€™s (IRCOV) effectiveness in the future, and
we plan to see the possibilities of scaling it up for larger projects. In addition to that, we want to evaluate our proposed
guidelines (under lessons learned) using different studies from industrial contexts.

Author contributions All authors have contributed to every phase of this study, i.e., the conception of the idea, implementation,
and manuscript writing. All authors read and approved the ï¬nal manuscript, and they stand accountable for all aspects of this workâ€™s
originality and integrity.

19

Minhas et al.

A PREPRINT

Acknowledgement

This work has in parts been supported by ELLIIT; the Swedish Strategic Research Area in IT and Mobile Communications.

References

[1] G. Amati. Information Retrieval Models, pages 1523â€“1528. Springer, New York, NY, 2009.

[2] J. H. Andrews, L. C. Briand, and Y. Labiche.

Is mutation an appropriate tool for testing experiments?

In

Proceedings of the 27th international conference on Software engineering, pages 402â€“411, 2005.

[3] A. Bajaj and O. P. Sangwan. A systematic literature review of test case prioritization using genetic algorithms.

IEEE Access, 7:126355â€“126375, 2019.

[4] R. M. Bezerra, F. Q. da Silva, A. M. Santana, C. V. Magalhaes, and R. E. Santos. Replication of empirical studies
in software engineering: An update of a systematic mapping study. In 2015 ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM), pages 1â€“4. IEEE, 2015.

[5] N. bin Ali, E. EngstrÃ¶m, M. Taromirad, M. R. Mousavi, N. M. Minhas, D. Helgesson, S. Kunze, and
M. Varshosaz. On the search for industry-relevant regression testing research. Empirical Software Engineer-
ing, pages 1â€“36, 2019.

[6] J. C. Carver. Towards reporting guidelines for experimental replications: A proposal.

In 1st international

workshop on replication in empirical software engineering, volume 1, pages 1â€“4. Citeseer, 2010.

[7] C. Catal. On the application of genetic algorithms for test case prioritization: a systematic literature review. In
Proceedings of the 2nd international workshop on Evidential assessment of software technologies, pages 9â€“14.
ACM, 2012.

[8] C. Catal and D. Mishra. Test case prioritization: a systematic mapping study. Software Quality Journal,

21(3):445â€“478, 2013.

[9] M. Cruz, B. BernÃ¡rdez, A. DurÃ¡n, J. A. Galindo, and A. Ruiz-CortÃ©s. Replication of studies in empirical software

engineering: A systematic mapping study, from 2013 to 2018. IEEE Access, 8:26773â€“26791, 2019.

[10] F. Q. Da Silva, M. Suassuna, A. C. C. FranÃ§a, A. M. Grubb, T. B. Gouveia, C. V. Monteiro, and I. E. dos
Santos. Replication of empirical studies in software engineering research: a systematic mapping study. Empirical
Software Engineering, 19(3):501â€“557, 2014.

[11] O. Dahiya and K. Solanki. A systematic literature study of regression test case prioritization approaches. Inter-

national Journal of Engineering & Technology, 7(4):2184â€“2191, 2018.

[12] C. V. de MagalhÃ£es, F. Q. da Silva, R. E. Santos, and M. Suassuna. Investigations about replication of empirical
studies in software engineering: A systematic mapping study. Information and Software Technology, 64:76â€“101,
2015.

[13] H. Do, S. Elbaum, and G. Rothermel. Supporting controlled experimentation with testing techniques: An infras-

tructure and its potential impact. Empirical Software Engineering, 10(4):405â€“435, 2005.

[14] H. Do, S. Mirarab, L. Tahvildari, and G. Rothermel. The effects of time constraints on test case prioritization: A

series of controlled experiments. IEEE Transactions on Software Engineering, 36(5):593â€“617, 2010.

[15] H. Do and G. Rothermel. On the use of mutation faults in empirical assessments of test case prioritization

techniques. IEEE Transactions on Software Engineering, 32(9):733â€“752, 2006.

[16] H. Do, G. Rothermel, and A. Kinneer. Empirical studies of test case prioritization in a junit testing environment.

In 15th international symposium on software reliability engineering, pages 113â€“124. IEEE, 2004.

[17] J.-B. Du Prel, G. Hommel, B. RÃ¶hrig, and M. Blettner. Conï¬dence interval or p-value?: part 4 of a series on

evaluation of scientiï¬c publications. Deutsches Ã„rzteblatt International, 106(19):335, 2009.

[18] E. D. Ekelund and E. EngstrÃ¶m. Efï¬cient regression testing based on test history: An industrial evaluation. In
Proceedings of IEEE International Conference on Software Maintenance and Evolution, ICSME, pages 449â€“457,
2015.

[19] S. Elbaum, A. G. Malishevsky, and G. Rothermel. Test case prioritization: A family of empirical studies. IEEE

transactions on software engineering, 28(2):159â€“182, 2002.

[20] E. EngstrÃ¶m and P. Runeson. A qualitative survey of regression testing practices. In Proceedings of the 11th
International Conference on Product-Focused Software Process Improvement PROFES, pages 3â€“16, 2010.

20

Minhas et al.

A PREPRINT

[21] E. EngstrÃ¶m, P. Runeson, and M. Skoglund. A systematic review on regression test selection techniques. Infor-

mation & Software Technology, 52(1):14â€“30, 2010.

[22] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In Proceedings of the 27th annual
international ACM SIGIR conference on Research and development in information retrieval, pages 49â€“56, 2004.
[23] M. Felderer and E. Fourneret. A systematic classiï¬cation of security regression testing approaches. International

Journal on Software Tools for Technology Transfer, 17(3):305â€“319, 2015.

[24] J. D. Gibbons. Location tests for single and paired samples (sign test and wilcoxon signed rank test), 1993.
[25] M. J. Harrold and A. Orso. Retesting software during development and maintenance. In Proceedings of the

Frontiers of Software Maintenance Conference, pages 99â€“108, 2008.

[26] M. Hasnain, I. Ghani, M. F. Pasha, I. H. Malik, and S. Malik. Investigating the regression analysis results for
classiï¬cation in test case prioritization: A replicated study. International Journal of Internet, Broadcasting and
Communication, 11(2):1â€“10, 2019.

[27] M. Irshad. Automation scripts to replicate ircov. https://github.com/MohsinIr84/replicationStudy/,

2021.

[28] ISO/IEC/IEEE.

International standard - systems and software engineeringâ€“vocabulary.

ISO/IEC/IEEE

24765:2017(E), pages 1â€“541, Aug 2017.

[29] M. Ivarsson and T. Gorschek. A method for evaluating rigor and industrial relevance of technology evaluations.

Empirical Software Engineering, 16(3):365â€“395, 2011.

[30] N. Juristo and O. S. GÃ³mez. Replication of Software Engineering Experiments, pages 60â€“88. Springer Berlin

Heidelberg, 2012.

[31] R. Just. The major mutation framework: Efï¬cient and scalable mutation analysis for java. In Proceedings of the

2014 international symposium on software testing and analysis, pages 433â€“436, 2014.

[32] R. Just, F. Schweiggert, and G. M. Kapfhammer. Major: An efï¬cient and extensible tool for mutation analysis
in a java compiler. In 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE
2011), pages 612â€“615. IEEE, 2011.

[33] R. Kazmi, D. N. A. Jawawi, R. Mohamad, and I. Ghani. Effective regression test case selection: A systematic

literature review. ACM Comput. Surv., 50(2):29:1â€“29:32, 2017.

[34] M. Khatibsyarbini, M. A. Isa, D. N. Jawawi, and R. Tumeng. Test case prioritization approaches in regression

testing: A systematic literature review. Information and Software Technology, 93:74â€“93, 2018.

[35] J. L. Krein and C. D. Knutson. A case for replication: synthesizing research methodologies in software engi-
neering. In RESER2010: proceedings of the 1st international workshop on replication in empirical software
engineering research, pages 1â€“10. Citeseer, 2010.

[36] J.-H. Kwon, I.-Y. Ko, G. Rothermel, and M. Staats. Test case prioritization based on information retrieval
concepts. In 2014 21st Asia-Paciï¬c Software Engineering Conference, volume 1, pages 19â€“26. IEEE, 2014.
[37] J. A. P. Lima and S. R. Vergilio. Test case prioritization in continuous integration environments: A systematic

mapping study. Information and Software Technology, 121:106268, 2020.

[38] N. M. Minhas and M. Irshad. Data set used in the replication of an ir based test case prioritization techniques

(ircov). https://data.mendeley.com/drafts/ccnzpxng54, 2021.

[39] N. M. Minhas, K. Petersen, N. Ali, and K. Wnuk. Regression testing goals-view of practitioners and researchers.
In 24th Asia-Paciï¬c Software Engineering Conference Workshops (APSECW), pages 25â€“32. IEEE, 2017.
[40] N. M. Minhas, K. Petersen, J. BÃ¶rstler, and K. Wnuk. Regression testing for large-scale embedded software
development â€“ exploring the state of practice. Information and Software Technology, 120:106254, 2020.
[41] J. F. S. Ouriques, E. G. Cartaxo, and P. D. Machado. Test case prioritization techniques for model-based testing:

a replicated study. Software Quality Journal, 26(4):1451â€“1482, 2018.

[42] R. Pan, M. Bagherzadeh, T. A. Ghaleb, and L. Briand. Test case selection and prioritization using machine

learning: a systematic literature review. Empirical Software Engineering, 27(2):1â€“43, 2022.

[43] Q. Peng, A. Shi, and L. Zhang. Empirically revisiting and enhancing ir-based test-case prioritization. In Pro-
ceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 324â€“336,
2020.

[44] M.-M. Pittelkow, R. Hoekstra, J. Karsten, and D. van Ravenzwaaij. Replication target selection in clinical
psychology: A bayesian and qualitative reevaluation. Clinical Psychology: Science and Practice, 28(2):210,
2021.

21

Minhas et al.

A PREPRINT

[45] D. Qiu, B. Li, S. Ji, and H. K. N. Leung. Regression testing of web service: A systematic mapping study. ACM

Comput. Surv., 47(2):21:1â€“21:46, 2014.

[46] A. Rainer and S. Beecham. A follow-up empirical evaluation of evidence based software engineering by un-
In Proceedings of the 12th International Conference on Evaluation and Assessment in

dergraduate students.
Software Engineering, pages 78â€“87, 2008.

[47] A. Rainer, D. Jagielska, and T. Hall. Software engineering practice versus evidence-based software engineering
research. In Proceedings of the ACM Workshop on Realising evidence-based software engineering (REBSE â€™05),
pages 1â€“5, 2005.

[48] T. Roelleke. Information retrieval models: Foundations and relationships. Synthesis Lectures on Information

Concepts, Retrieval, and Services, 5(3):1â€“163, 2013.

[49] R. H. Rosero, O. S. GÃ³mez, and G. D. R. Rafael. 15 years of software regression testing techniques - A survey.

Int. J. Software Eng. Knowl. Eng., 26(5):675â€“690, 2016.

[50] R. K. Saha, L. Zhang, S. Khurshid, and D. E. Perry. An information retrieval approach for regression test
prioritization based on program changes. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, volume 1, pages 268â€“279. IEEE, 2015.

[51] A. Santos, S. Vegas, M. Oivo, and N. Juristo. Comparing the results of replications in software engineering.

Empirical Software Engineering, 26(2):1â€“41, 2021.

[52] M. Shepperd, N. Ajienka, and S. Counsell. The role and value of replication in empirical software engineering

results. Information and Software Technology, 99:120â€“132, 2018.

[53] F. J. Shull, J. C. Carver, S. Vegas, and N. Juristo. The role of replications in empirical software engineering.

Empirical software engineering, 13(2):211â€“218, 2008.

[54] Y. Singh, A. Kaur, B. Suri, and S. Singhal. Systematic literature review on regression test prioritization tech-

niques. Informatica (Slovenia), 36(4):379â€“408, 2012.

[55] D. F. Williamson, R. A. Parker, and J. S. Kendrick. The box plot: a simple visual method to interpret data. Annals

of internal medicine, 110(11):916â€“921, 1989.

[56] C. Wohlin, P. Runeson, M. HÃ¶st, M. C. Ohlsson, B. Regnell, and A. WesslÃ©n. Experimentation in software

engineering. Springer Science & Business Media, 2012.

[57] S. Yadla, J. H. Hayes, and A. Dekhtyar. Tracing requirements to defect reports: an application of information

retrieval techniques. Innovations in Systems and Software Engineering, 1(2):116â€“124, 2005.

[58] S. Yoo and M. Harman. Regression testing minimization, selection and prioritization: a survey. Softw. Test.,

Verif. Reliab., 22(2):67â€“120, 2012.

[59] A. Zarrad. A systematic review on regression testing for web-based applications. JSW, 10(8):971â€“990, 2015.

22

