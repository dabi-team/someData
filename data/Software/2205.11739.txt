2
2
0
2

y
a
M
4
2

]
E
S
.
s
c
[

1
v
9
3
7
1
1
.
5
0
2
2
:
v
i
X
r
a

Deep Learning Meets Software Engineering:
A Survey on Pre-Trained Models of Source Code

Changan Niu1 , Chuanyi Li1 , Bin Luo1 and Vincent Ng2
1State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
2Human Language Technology Research Institute, University of Texas at Dallas, Richardson, Texas, USA
niu.ca@outlook.com, {lcy,luobin}@nju.edu.cn, vince@hlt.utdallas.edu

Abstract

Recent years have seen the successful application
of deep learning to software engineering (SE). In
particular, the development and use of pre-trained
models of source code has enabled state-of-the-art
results to be achieved on a wide variety of SE tasks.
This paper provides an overview of this rapidly ad-
vancing ﬁeld of research and reﬂects on future re-
search directions.

1 Introduction
Once upon a time the state of software intelligence in soft-
ware engineering (SE) was very rudimentary, with many of
the decisions supported by gut feeling and at best through
consultation with senior developers [Hassan and Xie, 2010].
As a wealth of data has been generated in the software de-
velopment and evolution lifecycle over the years, the soft-
ware development and evolution paradigm has also shifted
from human experience-based to data-driven decision mak-
ing. While AI researchers are fully aware of the impact deep
learning has on AI application domains such as computer vi-
sion and natural language processing (NLP), many are not
aware of the extensive and successful applications of deep
learning technologies to SE tasks in recent years.

Though successful,

the application of deep learning is
not without challenges. One such challenge concerns the
need for a large, typically costly-to-obtain, annotated train-
ing set to train the millions or even billions of parameters in
deep neural networks. To address this data annotation bot-
tleneck, NLP researchers have come up with an idea that
can arguably be considered a breakthrough in recent deep
learning research, namely pre-training [Dai and Le, 2015;
Howard and Ruder, 2018; Peters et al., 2018]. Rather than
training a model from scratch (i.e., with randomly initial-
ized network weights), which typically requires a lot of task-
speciﬁc annotated data, one can ﬁrst pre-train it on one or
more so-called self-supervised tasks (i.e., tasks for which
annotated data can be automatically generated and there-
fore large amounts of training data are readily available)
so that its weights encode general linguistic and common-
sense knowledge about language, and then the resulting pre-
trained model can be ﬁne-tuned to learn the target task us-
ing (a potentially small amount of) task-speciﬁc annotated

training data in the usual supervised manner. A large num-
ber of pre-trained language models have been developed and
widely used in NLP, such as BERT [Devlin et al., 2018], XL-
Net [Yang et al., 2019], RoBERTa [Liu et al., 2019], ELEC-
TRA [Clark et al., 2019], GPT-2 [Radford et al., 2019], T5
[Raffel et al., 2020], and BART [Lewis et al., 2020].

Can these pre-trained models be applied to SE tasks? Since
source code can be viewed as a sequence of code tokens in the
same way that natural language (NL) can be viewed as a se-
quence of word tokens, we can in principle retrain these mod-
els on source code and apply them to SE tasks. In practice,
this is not ideal, as there are code-speciﬁc characteristics that
may not be properly taken into account by these models. For
instance, source code is not as homogeneous as NL: it is com-
posed of both the code in a function body, which is written in
programming language (PL), as well as optional comments
written in NL. Treating both code and comments in a uniform
manner (i.e., as a sequence of tokens) may not be the best
way to exploit the two sources of information. In addition,
code has syntactic structures (as deﬁned in Abstract Syntax
Trees (ASTs)) and semantic structures (as deﬁned in Control
Flow Graphs (CFGs)). While a few syntax-aware pre-trained
models are recently developed in the NLP community (e.g.,
Xu et al. [2021]), the majority of existing pre-trained models
fail to exploit structured information. Consequently, SE re-
searchers have developed a number of pre-trained models of
source code (CodePTMs) that take into account the charac-
teristics speciﬁc to source code in the past few years.

Our goal in this paper is to raise the awareness of the AI au-
dience on the impact that AI technologies — in this case the
development and use of pre-trained models — have on SE, an
important AI application domain, speciﬁcally by providing
them with a survey of the recent development of CodePTMs
and their successful application to SE tasks. We believe this
survey will be of particular interest to (1) NLP researchers,
especially those focusing on text summarization and genera-
tion, since many SE tasks (e.g., code summarization) involve
NL generation; and (2) applied machine learning researchers,
since the development of these models could have a big im-
pact on SE. Though our target audience is AI researchers, we
believe this paper could also be of high interest for the SE
technology providers, raising their awareness on the added
value AI technology could have in augmenting SE tooling to
leverage the increasing complexity of software systems.

 
 
 
 
 
 
Type

I-O

Task Deﬁnition

WB

ET

BD

CD

CC

FD

CR

VM

CT

CS

CP

Wrong Binary Operator: Check if a given piece of code contains any
incorrect binary operators.
Exception Type: Predict the precise exception type.
Bug Detection / Defect Detection: Check if a given function contains a
defect.
Clone Detection: Determine whether two code snippets are semantically
equivalent.
Code Classiﬁcation: Classify the category of a given function.
Function-Docstring Mismatch: Determine whether a given function and
the docstring correspond to each other.
Code-to-Code Retrieval: Retrieve semantically similar code for a given
piece of query code.
Variable-Misuse Localization and Repair: Identify the location of a mis-
used variable and return the correct one.
Cloze Test: Predict the masked token from code.
Code Search / Text-to-Code Retrieval: Find the most relevant piece of
code from a set of candidates for a given natural language description.

Code Completion: Predict the missing/following token(s) of a given code
context.

C-V

C-C

NL-C

C-C

TL

Code Translation: Translate the code in one programming language to
the code in another programming language.

Bug Fixing: Repair buggy code by generating the correct version.

BF
MG Mutant Generation: Inject in working code a mutant for a real bug.
AG

Assert Generation: Generate a correct unit test assert statement.

Und.

Gen.

C-NL

NL-C

SU

MN

CG

Code Summarization / Code Documentation: Generate a textual descrip-
tion that describes the functionality of a function.

Method Naming / Extreme Code Summarization: Predict the function
name of a given function body.
Code Generation: Generate code given a natural language description.

ID - Dataset

Metrics

K1 - Kanade et al. [2020]

Acc

K1 - Kanade et al. [2020]
D1 - Devign [2019]
P1 - Pradel et al. [2018]
B1 - BigCloneBench [2014]
C1 - CLCDSA [2019]
P2 - POJ-104 [2016]

Acc
Acc
Acc
F1
P/R/F1
Acc/MAP@R

K1 - Kanade et al. [2020]

Acc

C1 - CLCDSA [2019]
P2 - POJ-104 [2016]

Acc/MRR/NDCG
MAP@R

V1 - Vasic et al. [2019]

Acc

D2 - De Sousa et al. [2021]
Acc
C2 - CodeSearchNet [2019]
MRR
C3 - AdvText [2021]
MRR/F1/Acc
S1 - Svyatkovskiy et al. [2020] RL/EditSim.
L1 - Liu et al. [2020]
A1 - Alon et al. [2020]
C4 - Chen et al. [2018]
T1 - TransCorder [2020]
C1 - CLCDSA [2019]
T2 - Tufano et al. [2019b]
T3 - Tufano et al. [2019a]
W1 - Watson et al. [2020]
C2 - CodeSearchNet [2019]
H1 - Haque et al. [2020]
H2 - Hu et al. [2018a]
H3 - Hu et al. [2018b]
M1 - Miceli et al. [2017]
A2 - Allamanis et al. [2016]
E1 - ETH Py150 [2016]
C5 - CONCODE [2018]

Acc
Acc@k
BLEU/Acc/CBLEU
Acc
BLEU/RL/CIDER
BLEU/Acc/CBLEU
Acc
Acc@k
BLEU
BLEU/RL
BLEU
BLEU/METEOR
BLEU
P/R/F1
P/R/F1
BLEU/Acc/CBLEU

Table 1: Categorization of the 18 SE tasks to which CodePTMs have been applied.

2 SE Tasks, Datasets, and Evaluation Metrics
SE studies problems concerning the design, development,
maintenance, testing, and evolution of software systems. Ta-
ble 1 enumerates the key SE tasks to which pre-trained mod-
els have been applied. As can be seen in the ﬁrst two columns,
we classify each task along two dimensions: (1) whether the
task concerns understanding (Und.) or generation (Gen.);
and (2) the type of input assumed by the task and the type
of output produced (I-O), where C, NL, and V denote code,
natural language, and extracted/predicted value, respectively.
In addition, Table 1 shows for each task the bench-
mark dataset(s) and the corresponding evaluation met-
ric(s). These metrics are fairly standard.
For retrieval
and classiﬁcation tasks, metrics such as Acc (Accu-
racy [Kanade et al., 2020]), Acc@k (Accuracy computed
over the top k predicted answers [Watson et al., 2020]),
Precision(P)/Recall(R)/F1 [Naﬁ et al., 2019], MRR (Mean
Reciprocal Rank [Husain et al., 2019]), MAP@R (Mean
Average Precision [Mou et al., 2016]), and NDCG (Nor-
malized Discounted Cumulative Gain [Naﬁ et al., 2019])
For generation tasks, metrics de-
are typically used.
veloped in the NLP community for summarization and
such as BLEU [Papineni et al., 2002],
translation tasks,
ROUGE-L
ME-
TEOR [Hu et al., 2018b], CIDER [Zhang et al., 2021], and
EditSim [Svyatkovskiy et al., 2020] (an edit distance-based
metric), as well as variants developed in the SE community,

[Haque et al., 2020],

(RL)

such as CodeBLEU (CBLEU) [Ren et al., 2020], are used.

3 CodePTMs
In this section, we provide an overview of 20 CodePTMs re-
cently developed in the SE community. To enable the reader
to better understand their similarities and differences, as well
as their relative strengths and weaknesses, we classify them
along four dimensions, as described below.

3.1 Architecture
First, existing CodePTMs differ in terms of the underlying
network architecture. To understand network architectures,
we need to brieﬂy introduce the concepts of encoding and
decoding. An encoder encodes an input sequence as a ﬁxed-
length vector representation, whereas a decoder generates an
output sequence based on the representation of an input.

Rather than designing new network architectures, SE re-
searchers base the design of CodePTMs on existing ar-
these architectures can be divided
chitectures. Broadly,
(1) Long Short-Term Memory
into four categories:
(LSTM [Hochreiter and Schmidhuber, 1997]), which is a
classical recurrent neural network architecture, (2) Trans-
former (TF) [Vaswani et al., 2017], which is a comparatively
newer encoder-decoder architecture1 that is faster to train and

1Recall that an encoder-decoder architecture is commonly used
for sequence-to-sequence tasks, where the encoder encodes an input

Type

LM

MLM

DAE

CTL

CA

SA

CN

CS

C
M
A

N
L
P

S
E

Task
FLM [2020]
FNP [2021]
BiLM [2020]
BMLM [2021]
WWM [2020]
MASS [2022]
SMLM [2021]
DAE [2021]
NSP [2020]
RTD [2020]
IMLM [2020]
SIMLM [2021b]
IT [2021b]
CCL [2021]
EP [2021]
NOP [2021]
BDG [2021b]
MNG [2022]
NA [2021]
TMLM [2021]
VGVAE [2021]
CAP [2022]
CLR [2021]
PD [2021]
ACP [2021]

CNS MCL [2021a]

Full Name and Description

Forward LM: maximizes the conditional probabilities of all the words by taking their previous words as contexts.
Future N-gram Prediction: a variant of FLM that involves predicting the next n (n > 1) tokens simultaneously instead of one token.
Bidirectional LM: combines a forward LM and a backward LM, and jointly maximizes the likelihood of the tokens both directions.
Basic version of MLM: randomly masks a certain percentage of tokens in the input, then predicts the masked tokens.
Whole Word Masking: if a word is masked, mask all subwords/tokens in it; then predict these masked tokens.
MAsked Seq2Seq: reconstructs the sentence fragment given the remaining part of the sentence in the encoder-decoder framework.
Seq2Seq MLM: randomly masks a set of token spans in the input and sequentially predicts them in the encoder-decoder framework.
Denoising Auto-Encoding: corrupts the input (by masking, deleting tokens, etc.) and uses the model to recover the original input.
Next Sentence Prediction: determines whether two given sentences (i.e., logical lines of code) are coherent.
Replaced Token Detection: identiﬁes the replaced tokens in the input (i.e., tokens produced by a small generator network).
Identiﬁer MLM: an adaptation of MLM to source code that masks only the identiﬁers in the code text.
Seq2Seq IMLM: an adaptation of Seq2Seq MLM to source code that masks only the identiﬁers in the code text.
Identiﬁer Tagging: determines if the input token at each position is an identiﬁer or not via binary classiﬁcation.
Code Contrastive Learning: minimizes/maximizes the distances between the representations of similar/dissimilar code snippets.
Edge Prediction: masks the edges connecting randomly selected nodes in a DFG, then predicts the masked edges.
Node Order Prediction: randomly changes the order of some nodes in an AST, then determines if a change occurs.
Bimodal Dual Generation: generates a NL summary if code is given, and generates code if NL is given.
Method Name Generation: generates the sub-token sequence of the method name based on a given method body.
Node Alignment: samples nodes in a DFG, masks the edge connecting each node to its code token, then predicts the masked edges.
Tree MLM: masks some terminal nodes/identiﬁers in ASTs/code on encoder/decoder side, then generates complete code sequence.
vMF-Gaussian Variational Autoencoder: disentangles code semantics from code syntax under the supervision of a masked AST.
Code-AST Prediction: determines whether the given code and AST correspond to each other.
Cross-Language Reconstruction: reconstructs the code snippet in one PL from functionally equivalent code snippets in other PLs.
Posterior Distribution: reduces difference in distributions of functionally equiv. code snippets in different PLs over code semantics.
Attentive Code Position: predicts the node type of a code token in an AST through an attention mechanism.
Multi-modal Contrastive Learning: maximizes/minimizes the representation similarity between positive/negative samples.

Table 2: Categorization and description of the pre-training tasks used by existing CodePTMs.

can better capture long-distance dependencies than LSTM;
(3) Transformer-Encoder (TE), which corresponds to the
architecture of the encoder part of TF; and (4) Transformer-
Decoder (TD), which corresponds to the architecture of the
decoder part of TF. While it is possible to use encoder-
only models (such as TE) and decoder-only models (such
it has
as TD) for sequence-to-sequence (seq2seq) tasks,
been shown to be disadvantageous and impractical to do
so [Niu et al., 2022]. In particular, encoder-only models and
decoder-only models are disadvantaged when applied to gen-
eration/decoding and classiﬁcation tasks, respectively.

3.2 Modality
When using a neural model to process source code, be-
ing able to integrate the NL embedded in the code (e.g.,
documentations, variable names) and the code structure
(e.g., ASTs) can improve the model’s ability to understand
the code [Ernst, 2017; Hu et al., 2018b; LeClair et al., 2019;
Z¨ugner et al., 2021]. Therefore, the use of NL and code struc-
ture as inputs in addition to the code itself has become a com-
mon practice in CodePTMs. As Code, NL, and Structure dif-
fer in representation and processing, they can be viewed as
features of different input modalities. Hence, along the sec-
ond dimension, we divide CodePTMs into three categories
— unimodal (Uni), bimodal (Bi), and multimodal (Multi) —
based on the number of input modalities they employ.

When a model employs more than one input modality, we
can either (1) concatenate the features extracted from differ-
ent modalities to form a single training instance or (2) use
the features extracted from different modalities to create dif-
ferent training instances. We refer to these two strategies as
Together and Standalone, respectively. As can be imagined,

sequence as a ﬁxed-length, typically task-speciﬁc, representation,
and the decoder then generates an output sequence token by token
based on the input and the tokens that have been generated so far.

an advantage of Together over Standalone is that the former
allows cross-modal representations to be learned by a model.

3.3 Pre-Training Tasks
Along the third dimension, we differentiate CodePTMs based
on the tasks used to pre-train them. At a high level, we can
divide these tasks into two categories depending on whether
the task originates in NLP (NLP) or is speciﬁcally designed
for source code (SE), as shown in Table 2.

As can be seen from the table,

the NLP pre-training
tasks can be subdivided into four categories: (1) Language
modeling (LM) [Qiu et al., 2020], which refers to the col-
lection of tasks that aim to predict a given word given
the surrounding context; (2) Masked Language Modeling
(MLM) [Devlin et al., 2018], which refers to the collection
of tasks that aim to predict the masked tokens; (3) Denois-
ing Auto-Encoding (DAE) [Lewis et al., 2020], which aim to
recover the original (i.e., uncorrupted) text from corrupted
text; and (4) Contrastive Learning (CTL) [Jain et al., 2021],
which allows a model to learn which data points are similar
or different. The SE pre-training tasks, on the other hand,
can be subdivided into three categories according to their in-
put modalities: (1) Code-Aware (CA) tasks, which aim to
mine latent information from code text; (2) Structure-Aware
(SA) tasks, which aim to learn representations of the code
structure; and (3) Cross-Modal-Aware (CMA) tasks, which
seek to acquire knowledge from multiple input modalities.
The CMA tasks can be further subdivided into three cate-
gories based on which input modalities are involved, namely
Code-NL (CN), Code-Structure (CS) and Code-NL-Structure
(CNS).

When more than one task is used to pre-train a CodePTM,
the tasks involved can be learned simultaneously (i.e., each
data instance supports all of the tasks involved2 and the task

2A data instance supports a task if the task’s loss can be com-

Arch. Mod.

LSTM

TE

TD

Uni
Bi

Uni

Bi

Multi

Uni

Uni

Pre-Training Tasks
NLP CA SA CMA
X

X

X

X X
X
X X
X
X
X X X X
X

X

X

X

Bi

TF

X X

Multi X

X
X X
X

PL

CodePTM

SE Understanding Tasks

SE Generation Tasks

WB ET BD CD CC FD CR VM CT CS CP TL BF MG AG SU MN CG

Mono
Multi

Mono

SCELMo
CodeDisen
CuBERT
C-BERT
JavaBERT
CugLM
CodeBERT

Multi
Multi
Mono OSCAR
Multi GraphCodeBERT
Multi
Multi GPT-C
Multi DOBF
Mono DeepDebug
Mono T5-learning

SynCoBERT

Multi

Multi
Multi
Multi

PLBART
CoTexT
ProphetNet-Code
CodeT5
TreeBERT
SPT-Code

K1 K1

P1

D1

C1

C1

P2

V1

C1

P2

P2

P2

D2

L1

C2

C2
C2,C3

S1

C3

B1
D1 B1

B1

D1 B1
D1

D1 B1

C2

C2

C4 T2
C4

T1

C4

T2
T2 T3 W1 H1
C2
C2
C2
C2
H2 A2,E1

T2

C5
C5

C5

C4 T2

C2 A1 C4 T2

C2,H3,M1

Table 3: Categorization of existing CodePTMs along four dimensions and their performances on downstream SE tasks. If a CodePTM is
applied to a task, we list the ID of the benchmark dataset on which the CodePTM was evaluated (see Table 1 for the ID associated with each
dataset), boldfacing the ID if the CodePTM achieved SOTA results on the corresponding dataset.

losses can be jointly minimized), sequentially (i.e., the model
is ﬁrst trained on the ﬁrst task for a speciﬁed number of steps
and then trained on the remaining tasks one by one), or al-
ternately (i.e., the tasks are randomly optimized as batches of
the data instances corresponding to a particular task are se-
lected at random during training). Hence, simultaneous pre-
training holds the strictest requirements on the data and the
tasks because it requires that for each data instance, all the
pre-training tasks can be completed in one forward propaga-
tion such that their losses can be added to form the ﬁnal op-
timization objective and jointly minimized during backward
propagation.
In other words, if it can perform simultane-
ous pre-training, it will also be possible to perform sequen-
tial/alternate pre-training but not vice versa. Nevertheless,
the selection of a pre-training strategy in existing CodePTMs
seems random when multiple options are available3.

3.4 Programming Languages
Along the last dimension, we categorize CodePTMs depend-
ing on whether they are pre-trained on one PL (Monolingual
(Mono)) or multiple PLs (Multilingual (Multi)).

3.5 Categorization and Pre-Training Details

The ﬁrst ﬁve columns of Table 3 categorize 20 CodePTMs
along the four dimensions discussed in the previous subsec-
tions, namely Architecture (Arch.), Modality (Mod., Pre-
Training Tasks, and Programming Languages (PL). We be-
lieve this categorization can help the reader better understand
the similarities and differences between different CodePTMs.

puted based on the instance. For example, a code-only data in-
stance (i.e., a code snippet without the paired docstring) supports
both MLM and NSP because the losses of both tasks can be calcu-
lated based on the code snippet. However, it does not support BDG
because the code-docstring alignment is needed by BDG.

3For example, IT in CodeT5 can be pre-trained simultaneously

with any of the other tasks, but it is still pre-trained alternatively.

Note, however, that Table 3 only provides a high-level
categorization of the CodePTMs. For instance, we still do
not know which two input modalities are used by a bimodal
CodePTM, and neither do we know which PLs are used to
pre-train a multilingual CodePTM. Table 4 ﬁlls this gap by
providing the details of how each CodePTM is pre-trained.
Speciﬁcally, CodePTM cites the paper that proposed each
CodePTM, whereas Input, Objective, and Dataset show the
input modalities, the pre-training tasks, and the PLs involved
in pre-training each CodePTM. The datasets can be divided
into four types, namely, GitHub Repos (a dataset obtained
from GitHub, e.g., JS GitHub Repos is a dataset built by
GitHub JavaScript repositories), BigQuery (a platform that
includes activity from over 3M open source GitHub reposito-
ries, e.g., “Python from BigQuery” is the dataset collected
by querying Python functions on BigQuery), CodeSearch-
Net [Husain et al., 2019] (a dataset that is obtained by scrap-
ing open-source repositories and pairing individual functions
with their docstrings and which includes more than 6.4M
codes of 6 PLs including Java, Python, JavaScript, PHP, Go
and Ruby), and CLCDSA [Naﬁ et al., 2019] (a dataset col-
lected from Online Judge (OJ) sites across four PLs (i.e., Java,
Python, C# and C++) where functionally similar solutions
written in different PLs are available for a given problem).

4 Discussion
Next, we explore the relationship between CodePTMs (Sec-
tion 3) and SE tasks (Section 2). The right half of Table 3
depicts this relationship by showing whether a CodePTM has
been applied to a particular SE task, and if so, which bench-
mark dataset(s) it has been evaluated on and whether state-of-
the-art (SOTA) results have been achieved. Below we discuss
our key observations, which are based in part on Table 3 and
in part on conclusions drawn from the literature.
Architecture. As can be seen in Table 3, TE-based
CodePTMs are applied mostly to Understanding tasks,

Input

Objective

Dataset

CodePTM
SCELMo [2020]
CodeDisen [2021]
CuBERT [2020]
C-BERT [2020]
JavaBERT [2021]
CugLM [2020]
CodeBERT [2020]
OSCAR [2021]
GraphCodeBERT [2021]
SynCoBERT [2021a]
GPT-C [2020]
DOBF [2021]
DeepDebug [2021]
T5-learning [2021]

Code
Code + AST Seq
Code
Code
Code
Code
Code + Doc
IR + AEI
Code + Doc + DFG Nodes
Code + Doc + AST Seq
Code
Code
Code
Code

BiLM
VGVAE + CLR + PD + ACP
BMLM + NSP
WWM
BMLM
IMLM + NSP + FLM
BMLM & RTD
BMLM + CCL
BMLM + EP + NA
BMLM + IT + TEP + MCL
FLM
SIMLM(Seq2Seq IMLM)
SMLM(Seq2Seq MLM)
SMLM(Seq2Seq MLM)

PLBART [2021]

Code & Posts

DAE (masking / deletion / inﬁlling)

CoTexT [2021]

Code + Doc

SMLM(Seq2Seq MLM)

ProphetNet-Code [2021]

Code & Doc

CodeT5 [2021b]

TreeBERT [2021]
SPT-Code [2022]

Code + Doc

Code + AST Paths
Code + Names + AST Seq

FNP
SMLM(Seq2Seq MLM) / IT /
SIMLM(Seq2seq IMLM) / BDG
TMLM + NOP
CAP & MASS & MNG

JS GitHub Repos
CLCDSA
Python from BigQuery
C GiHub Repos
Java GitHub Repos
Java, TS GitHub Repos
CodeSearchNet
C/C++ GitHub Repos
CodeSearchNet (Bimodal)
CodeSearchNet
Python, C#, JS/TS GitHub Repos
Java, Python from BigQuery
Java GitHub Repos
CodeSearchNet (Java)
Java, Python GitHub Repos
StackOverﬂow Posts
CodeSearchNet
Java, Python from BigQuery
CodeSearchNet (Bimodal)
CodeSearchNet
C, C# from BigQuery
Java, Python from BigQuery
CodeSearchNet

Dataset Size

150K Files
26K Functions
7.4M Files
5.8GB
3M Files
617K Files
6.5M Functions
500K Functions
2.3M Functions
6.5M Functions
4.7M Files
11.5M Files
8M Files
1.5M Functions
680M Functions
47M Posts
6.5M Functions
6.4M Functions
2.3M Functions
6.5M Functions
1.85M Functions
21.3M Files
6.5M Functions

Table 4: Details of how the CodePTMs are pre-trained. The pre-training scheme employed for each CodePTM is characterized by (1) the
input modalities (if multiple modalities are involved, they can be handled via a Together (+) or Standalone (&) strategy); (2) the pre-training
objectives (if multiple pre-training objectives are involved, they can be learned jointly (+), sequentially (&), or alternately (/)); (3) the dataset
on which the CodePTM is pre-trained; and (4) the size of the dataset.

whereas TD- and TF-based CodePTMs are applied mostly
to Generation tasks. This is understandable. As mentioned
in Section 3.1, encoder-only models are disadvantaged when
applied to Generation tasks. The reason is that they can only
map an input sequence to an output sequence with a priori
known length, but for Generation tasks the output length is
typically not known a priori.
In contrast, the presence of
decoders in TD- and TF-based CodePTMs naturally makes
them more suited to Generation tasks.
Modality. We make two modality-related observations.
First, for CodePTMs that use structured information as input
(e.g., features extracted from DFGs, ASTs, and AEI4), re-
moving such information from the input always reduces their
performances on downstream SE tasks [Guo et al., 2021;
Zhang et al., 2021; Wang et al., 2021a; Jiang et al., 2021].

Second,

the use of NL as an input modality appears
to contribute positively to model performance on a down-
in the input or out-
stream task only if NL is present
put of the task [Feng et al., 2020; Niu et al., 2022]. Oth-
erwise, the use of NL could lead to a performance dete-
rioration [Phan et al., 2021; Niu et al., 2022]. For example,
CodeT5, which is pre-trained using NL and Code, achieves
SOTA results on all the NL-related SE tasks to which it is
applied (e.g., TL, SU, and MN), but it is surpassed by Syn-
CoBERT, which is pre-trained only on Code, in performance
on CD, a Code-related-only task.
Pre-training tasks. We make two pre-training tasks-related
observations. First, after ﬁne-tuning on task-speciﬁc train-
ing data, a pre-trained model generally yields better re-
sults on SE downstream tasks than its ”no pre-training”

4AEI (Abstract Environment Information) describes a program’s

semantics with a mathematical characterization of its behaviors.

counterpart that
is trained only on task-speciﬁc training
data, and the discrepancy in their performances is es-
pecially obvious when the amount of task-speciﬁc train-
ing data is small [Zhou et al., 2021; Kanade et al., 2020;
Buratti et al., 2020; Roziere et al., 2021]. This is true even
when the underlying pre-trained model is taken from the NLP
domain, such as RoBERTa, without pre-training it again on
source code [Feng et al., 2020; Ahmad et al., 2021].

Second, keeping the pre-training task’s type as similar as
possible to that of the downstream task tends to yield the best
results. Theoretically, pre-training will be beneﬁcial for a
downstream task precisely when the knowledge learned dur-
ing pre-training can be successfully exploited when the model
learns the downstream task. Such knowledge transfer tends
to be more effective if the pre-training task is closer to the
downstream task. For instance, for Understanding tasks, it is
better to use a pre-training task that is also an Understanding
task, such as MLM. Note that MLM is used by all the mod-
els that achieve SOTA results on Understanding tasks, such
as CuBERT. In contrast, (I)MASS, which focuses on Genera-
tion, tends to work much better as a pre-training task than (I)
MLM, which focuses on Understanding, on seq2seq down-
stream tasks such as code summarization [Jiang et al., 2021].
Programming languages. We make two PL-related obser-
vations. First, knowledge transfer tends to be a lot more ef-
fective if a CodePTM is trained on a PL that is syntactically
similar to the one used in the downstream task. In contrast,
knowledge learned by a CodePTM from PLs that are syn-
tactically different from the one used in the downstream task
may even lead to the performance degradation. For instance,
PLBART, which is pre-trained on Java and Python, performs
better on C# code translation but worse on PHP code sum-
marization than RoBERTa, a PTM that is trained on NL text

only. The reason is that C# is syntactically similar to Java,
while PHP has a syntax mismatch with Java and Python.

Second, multilingual pre-training and ﬁne-tuning gener-
ally yield better results than their monolingual counterparts.
For example, CodeT5, which is pre-trained on 6 PLs, out-
performs T5-learning and DeepDebug, both of which are
In addition,
only pre-trained on Java, on code translation.
when performing multilingual pre-training, language-aware
pre-training, where the training instances that belong to dif-
ferent PLs are being differentiated (by adding language-
speciﬁc symbols to the input or appending a language-type
embedding to each token, for instance), tend to yield a pre-
trained model that can better discriminate between PLs than
language-agnostic pre-training, as demonstrated via GPT-C
on code completion.

5 How Effective are CodePTMs?
CodePTMs have been successfully applied to a variety of SE
tasks, but how effective are they? To enable the reader to
gain insights into this question, we present some quantita-
tive results in this section. More speciﬁcally, we show in Ta-
ble 5 the best result achieved by a CodePTM on each com-
monly used evaluation dataset for each SE task (see the ”Best
CodePTM” column). To help the reader gauge the effective-
ness of CodePTMs, we show in the ”Best non-CodePTM”
column the best result achieved by an approach that does not
involve pre-training on each dataset. As can be seen, many of
the best non-CodePTM-based approaches are neural models
that involve Tree-LSTM and Transformer, for instance. The
last column of the table shows for each dataset the relative er-
ror reduction rate, which is computed as the error reduced by
the best-performing CodePTM relative to the error made by
the best non-CodePTM-based system on the dataset. A pos-
itive value indicates that the SOTA result is achieved using
a CodePTM. As can be seen, the SOTA results on all of the
datasets are achieved using CodePTMs, with the relative er-
ror reduction rates ranging from 0.9–78.7 when expressed in
percentages. These results provide suggestive evidence that
CodePTMs are a promising approach to a wide variety of SE
tasks. Nevertheless, it is clear that CodePTMs are more ef-
fective at relative error reduction on certain SE tasks/datasets
than other tasks/datasets. Additional analysis is needed to de-
termine the reason.

6 Concluding Remarks
Though CodePTMs have proven their success in SE, we be-
lieve that they have not reached their full potential. In this
section, we outline some promising future directions.

6.1 Thinking beyond NLP
Tokenization and embedding. Currently, CodePTMs use
the tokenization and embedding methods developed in NLP.
For example, they use SentencePiece as the tokenizer as well
as token and position embeddings. However, code is not ex-
actly the same as NL: code contains different types of lexi-
cal tokens such as variables, control symbols, and keywords.
We speculate that NLP tokenization and embedding methods
would not yield optimal performances for CodePTMs, and

CS

CR

CD

Task DS Best CodePTM

Best non-CodePTM
73.8 (GREAT [2020])
49.5 (Transformer [2020])
62.4 (code2vec [2021])
95.0 (FA-AST [2020])
81.0 (Tree-LSTM [2019] )
96.6 (ProGraML [2021])
91.0 (Transformer [2020])
16.6 (Pontes et al. [2018] )
82.4 (MISIM [2020] )
80.5 (BiLSTM [2020])
−
41.9 (Transformer [2021])

82.3 (CuBERT [2020])
WB K1
79.1 (CuBERT [2020])
ET K1
65.7 (CodeT5 [2021b])
BD D1
97.4 (SynCoBERT [2021a])
B1
90.0 (CodeDisen [2021])
C1
98.0 (OSCAR [2021])
CC P2
98.0 (CuBERT [2020])
FD K1
31.6 (CodeDisen [2021])
C1
88.2 (SynCoBERT [2021a])
P2
95.2 (CuBERT [2020])
VM V1
94.4 (JavaBERT [2021])
CT D2
74.0 (SynCoBERT [2021a])
C2
38.1 (SynCoBERT [2021a]) −
C3
82.8 (GPT-C [2020])
S1
−
71.7 (Transformer-XL [2019])
81.9 (CugLM [2020])
L1
24.7 (SLM [2020])
26.5 (SPT-Code [2022])
A1
35.4 (Transformer [2021])
66.4 (CodeT5 [2021b])
C4
34.7 (Transformer [2021])
41.8 (DOBF [2021])
T1
25.8 (Tree-LSTM [2019])
29.7 (CodeDisen [2021])
C1
12.7 (S2S+COPY [2021])
18.3 (CodeT5 [2021b])
BF T2
17.0 (Tufano [2019a])
28.0 (T5-learning [2021])
MG T3
65.0 (Watson et al. [2020])
AG W1 66.0 (T5-learning [2021])
15.5 (Transformer [2020])
19.0 (Haque et al. [2020])
19.7 (GNN+GRU [2020])
48.2 (AST-Trans [2022])
34.7 (AST-Trans [2022])
57.5 (GNN+GRU [2020])
34.4 (GNN+GRU [2020])
12.2 (Iyer et al. [2019])

19.7 (CodeT5 [2021b])
C2
21.0 (T5-learning [2021])
H1
20.4 (TreeBERT [2021])
H2
49.1 (SPT-Code [2022])
H3
M1 36.1 (SPT-Code [2022])
60.1 (TreeBERT [2021])
A2
39.0 (TreeBERT [2021])
E1
22.3 (CodeT5 [2021b])
CG C5

MN

SU

TL

CP

ER

32.4
58.6
8.8
48.0
47.3
43.2
78.7
17.9
32.9
75.4

55.2

36.0
2.4
47.9
10.9
5.3
6.5
13.3
2.9
4.9
2.5
0.9
1.6
2.1
6.1
7.0
11.5

Table 5: Relative error reduction rates achieved by CodePTMs
”DS” shows the commonly used evalua-
on SE tasks/datasets.
tion datasets for each SE task (see Table 1 for details on these
datasets). ”Best CodePTM” shows the best result achieved to date
by a CodePTM on the corresponding dataset and the name of the
CodePTM. ”Best non-CodePTM” shows the best result achieved to
date by an approach that does not involve pre-training on the corre-
sponding dataset and the name of the approach (note that “−” indi-
cates that non-CodePTM-based approaches have not been applied to
the corresponding dataset). ”ER” shows the relative error reduction
rate for each dataset. Information on the evaluation metric used for
each dataset can be found in Table 1.

recommend that researchers look into the possibility of de-
veloping code-speciﬁc versions of these methods.
Pre-training methods. Pre-training tasks that can better
exploit code-speciﬁc characteristics (e.g., code structure, the
presence of branches, and the use of different identiﬁers taken
from a largely unrestricted vocabulary to express the same
meaning) may be needed in order to train more powerful
CodePTMs. Most of the existing SE-speciﬁc pre-training
tasks (see Section 3.3) still do not completely step outside
the NLP mindset.
IMLM, for example, is just a version
of MLM that masks identiﬁers, and in fact, pre-training on
IMLM has even yielded worse results than pre-training on
MLM for DOBF [Roziere et al., 2021]. We believe that the
design of code-speciﬁc pre-training methods is currently lim-
ited in part by the NLP tokenization and embedding methods
that are currently in use, and that a fundamental overhaul in
the design of code-speciﬁc pre-training methods that involves
designing code-speciﬁc tokenization and embedding methods
will likely be needed.

6.2 Learning Code Form and Functionality
Code has both form, which is deﬁned by combinations of par-
ticular code identiﬁers, and function, which is independent of

any particular code identiﬁers [Jain et al., 2021]. Note that
the CodePTMs listed in Table 4 all learn representations of
source code from the “form” instead of the “function” per-
spective. Learning code functionality, however, will undoubt-
edly help CodePTMs understand the code better and achieve
higher performances on SE tasks. So we believe that design-
ing CodePTMs that can learn both code form and code func-
tionality would be a valuable research direction.

6.3 Adaptation to Downstream Tasks

Currently, ﬁne-tuning is the primary method for transferring
the knowledge acquired during pre-training to downstream
tasks. However, ﬁne-tuning can be inefﬁcient because all
model parameters need to be updated. To mitigate this prob-
lem, the NLP community has proposed several solutions,
such as (1) model reprogramming (i.e., freezing the origi-
nal parameters of the PTMs and adding small ﬁne-tunable
adaption modules for speciﬁc tasks [Chen, 2022]), (2) us-
ing prompt tuning [Brown et al., 2020], and (3) using model
compression (e.g., pruning and knowledge distillation). How
to adapt or extend these methods for ﬁne-tuning CodePTMs
is a promising research direction.

6.4 CodePTMs for Niche Applications

Rather than attempting to design a single CodePTM that
works well on all SE tasks, we recommend that special-
ized CodePTMs be designed for different classes of SE tasks
(e.g., Understanding vs. Generation). Our recommendation
is based on our earlier observations that different model de-
sign choices may be better suited for different kinds of tasks.
For instance, theoretically speaking, TE-based models tend to
work better than TD- and TF-based models on Understand-
ing tasks, whereas the reverse is generally true for Genera-
tion tasks. One may even go as far as designing task-speciﬁc
CodePTMs. The reason is that having pre-training tasks that
are more similar to the downstream task at hand could enable
a more effective transfer of the knowledge acquired during
pre-training, as discussed previously. We believe that special-
ized CodePTMs have an additional advantage: they tend to be
smaller and hence may be more efﬁcient, potentially allowing
us to address the efﬁciency issues associated with model ar-
chitecture (Section 6.1) and ﬁne-tuning (Section 6.3).

6.5 Uniﬁed Evaluation and Analysis

Our understanding of the strengths and weaknesses of exist-
ing CodePTMs is currently limited by the tasks on which they
are evaluated. To better understand CodePTMs, it is impor-
tant to conduct a systematic evaluation of all CodePTMs on
all the benchmark datasets associated with the 18 SE tasks
we discussed.
In addition to a comprehensive quantitative
evaluation, a qualitative analysis that involves analyzing the
common errors made by each model would be important.

Acknowledgments

We thank the three anonymous reviewers for their helpful
comments on an earlier draft of this paper. This work was
supported in part by the National Natural Science Foundation

of China (No. 61802167) and the US National Science Foun-
dation (Grant IIS-1528037). Any opinions, ﬁndings, conclu-
sions or recommendations expressed in this paper are those of
the authors and do not necessarily reﬂect the views or ofﬁcial
policies, either expressed or implied, of the funding agencies.
Chuanyi Li is the corresponding author.

References
[Ahmad et al., 2021] Wasi Ahmad, Saikat Chakraborty,
Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-training
In NAACL-
for program understanding and generation.
HLT, 2021.

[Allamanis et al., 2016] Miltiadis Allamanis, Hao Peng, and
Charles Sutton. A convolutional attention network for ex-
treme summarization of source code. In ICML, 2016.
[Alon et al., 2020] Uri Alon, Roy Sadaka, Omer Levy, and
Eran Yahav. Structural language models of code. In ICML,
2020.

[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learn-
ers. In NeurIPS, 2020.

[Buratti et al., 2020] Luca Buratti, Saurabh Pujar, Mi-
haela Bornea, Scott McCarley, Yunhui Zheng, Gae-
tano Rossiello, Alessandro Morari, Jim Laredo, Veronika
Thost, Yufan Zhuang, et al. Exploring software natural-
ness through neural language models. arXiv:2006.12641,
2020.

[Chen et al., 2018] Xinyun Chen, Chang Liu, and Dawn
Song. Tree-to-tree neural networks for program transla-
tion. In NeurIPS, 2018.
[Chen, 2022] Pin-Yu Chen.

reprogramming:
Resource-efﬁcient cross-domain machine learning. arXiv
preprint arXiv:2202.10629, 2022.

Model

[Clark et al., 2019] Kevin Clark, Minh-Thang Luong,
Quoc V Le, and Christopher D Manning.
Electra:
Pre-training text encoders as discriminators rather than
generators. In ICLR, 2019.

[Coimbra et al., 2021] David Coimbra, Soﬁa Reis, Rui
Abreu, Corina P˘as˘areanu, and Hakan Erdogmus. On using
distributed representations of source code for the detection
of c security vulnerabilities. CoRR, 2021.

[Dai and Le, 2015] Andrew M. Dai and Q. V. Le. Semi-

supervised sequence learning. In NIPS, 2015.

[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang,
Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond
a ﬁxed-length context. In ACL, 2019.

[de Sousa and Hasselbring, 2021] Nelson Tavares de Sousa
and Wilhelm Hasselbring.
Training a
transformer-based model for the java programming lan-
guage. arXiv:2110.10404, 2021.

Javabert:

[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understand-
ing. arXiv:1810.04805, 2018.

[Drain et al., 2021] Dawn Drain, Chen Wu, Alexey Svy-
atkovskiy, and Neel Sundaresan. Generating bug-ﬁxes
In Proceedings of the
using pretrained transformers.
5th ACM SIGPLAN International Symposium on Machine
Programming, 2021.

[Ernst, 2017] Michael D. Ernst. Natural language is a pro-
gramming language: Applying natural language process-
ing to software development. In 2nd Summit on Advances
in Programming Languages, 2017.

[Feng et al., 2020] Zhangyin Feng, Daya Guo, Duyu Tang,
Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-
trained model for programming and natural languages. In
EMNLP: Findings, 2020.

[Guo et al., 2021] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin
Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sun-
daresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graph-
codebert: Pre-training code representations with data ﬂow.
In ICLR, 2021.

[Haque et al., 2020] Sakib Haque, Alexander LeClair,
Lingfei Wu, and Collin McMillan.
Improved automatic
summarization of subroutines via attention to ﬁle context.
In MSR, 2020.

[Hassan and Xie, 2010] Ahmed E Hassan and Tao Xie. Soft-
ware intelligence: the future of mining software engineer-
ing data. In FSE/SDP workshop, 2010.

[Hellendoorn et al., 2020] Vincent J Hellendoorn, Charles
Sutton, Rishabh Singh, Petros Maniatis, and David Bieber.
Global relational models of source code. In ICLR, 2020.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
Computation, 1997.

[Howard and Ruder, 2018] Jeremy Howard and Sebastian
Ruder. Universal language model ﬁne-tuning for text clas-
siﬁcation. In ACL, 2018.

[Hu et al., 2018a] Xing Hu, Ge Li, Xin Xia, David Lo, and
Zhi Jin. Deep code comment generation. In ICPC, 2018.
[Hu et al., 2018b] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai
Lu, and Zhi Jin. Summarizing source code with transferred
api knowledge. In IJCAI, 2018.

[Husain et al., 2019] Hamel Husain, Ho-Hsiang Wu, Tiferet
Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of seman-
tic code search. arXiv:1909.09436, 2019.

[Iyer et al., 2018] Srinivasan Iyer, Ioannis Konstas, Alvin
Cheung, and Luke Zettlemoyer. Mapping language to code
in programmatic context. In EMNLP, 2018.

[Iyer et al., 2019] Srinivasan Iyer, Alvin Cheung, and Luke
Zettlemoyer. Learning programmatic idioms for scalable
semantic parsing. In EMNLP, 2019.

[Jain et al., 2021] Paras Jain, Ajay Jain, Tianjun Zhang,
Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. Con-
trastive code representation learning. In EMNLP, 2021.
[Jiang et al., 2021] Xue Jiang, Zhuoran Zheng, Chen Lyu,
Liang Li, and Lei Lyu. Treebert: A tree-based pre-trained
model for programming language. In UAI, 2021.

[Kanade et al., 2020] Aditya Kanade,

Gogul Balakrishnan, and Kensen Shi.
evaluating contextual embedding of source code.
ICML, 2020.

Petros Maniatis,
Learning and
In

[Karampatsis and Sutton, 2020] Rafael-Michael Karampat-
sis and Charles Sutton. Scelmo: Source code embeddings
from language models. arXiv:2004.13214, 2020.

[LeClair et al., 2019] Alexander LeClair, Siyuan Jiang, and
Collin McMillan. A neural model for generating natu-
ral language summaries of program subroutines. In ICSE,
2019.

[LeClair et al., 2020] Alexander LeClair, Sakib Haque,
Lingfei Wu, and Collin McMillan. Improved code sum-
marization via a graph neural network. In ICPC, 2020.
[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal,
Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart:
Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. In
ACL, 2020.

[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,
Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
Roberta: A robustly optimized bert pretraining approach.
arXiv:1907.11692, 2019.

[Liu et al., 2020] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin.
Multi-task learning based pre-trained language model for
code completion. In ASE, 2020.

[Lu et al., 2021] Shuai Lu, Daya Guo, Shuo Ren, Junjie
Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
Lidong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, MING GONG, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU.
CodeXGLUE: A machine learning benchmark dataset for
code understanding and generation. In NeurIPS Datasets
and Benchmarks Track (Round 1), 2021.

[Mastropaolo et al., 2021] Antonio Mastropaolo, Simone
Scalabrino, Nathan Cooper, David Nader Palacio, Denys
Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. Study-
ing the usage of text-to-text transfer transformer to support
code-related tasks. In ICSE, 2021.

[Miceli-Barone and Sennrich, 2017] Antonio

Valerio
Miceli-Barone and Rico Sennrich. A parallel corpus of
python functions and documentation strings for automated
In IJCNLP,
code documentation and code generation.
2017.

[Mou et al., 2016] Lili Mou, Ge Li, Lu Zhang, Tao Wang,
and Zhi Jin. Convolutional neural networks over tree struc-
In AAAI,
tures for programming language processing.
2016.

[Naﬁ et al., 2019] Kawser Wazed Naﬁ, Tonny Shekha Kar,
Banani Roy, Chanchal K Roy, and Kevin A Schneider.
Clcdsa: cross language code clone detection using syn-
tactical features and api documentation. In ASE, 2019.
[Niu et al., 2022] Changan Niu, Chuanyi Li, Vincent Ng, Ji-
dong Ge, Liguo Huang, and Bin Luo. Spt-code: Sequence-
to-sequence pre-training for learning source code repre-
sentations. arXiv:2201.01549, 2022.

[Panthaplackel et al., 2021] Sheena Panthaplackel, Miltiadis
Allamanis, and Marc Brockschmidt. Copy that! editing
sequences by copying spans. In AAAI, 2021.

[Papineni et al., 2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. Bleu: a method for au-
tomatic evaluation of machine translation. In ACL, 2002.
[Peng et al., 2021] Dinglan Peng, Shuxin Zheng, Yatao Li,
Guolin Ke, Di He, and Tie-Yan Liu. How could neural
networks understand programs? In ICML, 2021.

[Peters et al., 2018] Matthew E. Peters, Mark Neumann,
Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton
Lee, and Luke Zettlemoyer. Deep contextualized word
representations. In NAACL-HLT, 2018.

[Phan et al., 2021] Long Phan, Hieu Tran, Daniel Le, Hieu
Nguyen, James Anibal, Alec Peltekian, and Yanfang Ye.
Cotext: Multi-task learning with code-text transformer.
arXiv:2105.08645, 2021.

[Pontes et al., 2018] Elvys Linhares Pontes, St´ephane Huet,
Andr´ea Carneiro Linhares, and Juan-Manuel Torres-
Moreno. Predicting the semantic textual similarity with
siamese cnn and lstm. In TALN, 2018.

[Pradel and Sen, 2018] Michael Pradel and Koushik Sen.
Deepbugs: A learning approach to name-based bug de-
tection. Proceedings of the ACM on Programming Lan-
guages, 2018.

[Qi et al., 2021] Weizhen Qi, Yeyun Gong, Yu Yan, Can
Xu, Bolun Yao, Bartuer Zhou, Biao Cheng, Daxin Jiang,
Jiusheng Chen, Ruofei Zhang, et al. Prophetnet-x: Large-
scale pre-training models for english, chinese, multi-
lingual, dialog, and code generation. arXiv:2104.08006,
2021.

Language models are unsupervised multitask learners. In
OpenAI Blog, 2019.

[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam
Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits
of transfer learning with a uniﬁed text-to-text transformer.
JMLR, 2020.

[Raychev et al., 2016] Veselin Raychev, Pavol Bielik, and
Martin Vechev. Probabilistic model for code with decision
trees. In OOPSLA, 2016.

[Ren et al., 2020] Shuo Ren, Daya Guo, Shuai Lu, Long
Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming
Zhou, Ambrosio Blanco, and Shuai Ma.
Codebleu:
a method for automatic evaluation of code synthesis.
arXiv:2009.10297, 2020.
[Roziere et al., 2020] Baptiste

Marie-Anne
Lachaux, Lowik Chanussot, and Guillaume Lample.
Unsupervised translation of programming languages.
In
NeurIPS, 2020.

Roziere,

[Roziere et al., 2021] Baptiste

Marie-Anne
Lachaux, Marc Szafraniec, and Guillaume Lample.
Dobf: A deobfuscation pre-training objective for pro-
gramming languages. arXiv:2102.07492, 2021.

Roziere,

[Shido et al., 2019] Yusuke Shido, Yasuaki Kobayashi, Ak-
ihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Mat-
sumura. Automatic source code summarization with ex-
tended tree-lstm. In IJCNN, 2019.

[Svajlenko et al., 2014] Jeffrey Svajlenko, Judith F Islam,
Iman Keivanloo, Chanchal K Roy, and Mohammad Ma-
mun Mia. Towards a big data curated benchmark of inter-
project code clones. In ICSME, 2014.

[Svyatkovskiy et al., 2020] Alexey Svyatkovskiy, Shao Kun
Deng, Shengyu Fu, and Neel Sundaresan. Intellicode com-
pose: Code generation using transformer. In ESEC/FSE,
2020.

[Tang et al., 2022] Ze Tang, Xiaoyu Shen, Chuanyi Li, Ji-
dong Ge, Liguo Huang, Zhelin Zhu, and Bin Luo. Ast-
trans: Code summarization with efﬁcient tree-structured
attention. In ICSE, 2022.

[Tufano et al., 2019a] Michele Tufano,

Jevgenija Pan-
tiuchina, Cody Watson, Gabriele Bavota, and Denys
Poshyvanyk. On learning meaningful code changes via
neural machine translation. In ICSE, 2019.

[Tufano et al., 2019b] Michele Tufano, Cody Watson,
Gabriele Bavota, Massimiliano Di Penta, Martin White,
and Denys Poshyvanyk. An empirical study on learn-
ing bug-ﬁxing patches in the wild via neural machine
translation. TOSEM, 2019.

[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu,
Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained
models for natural language processing: A survey. Science
China Technological Sciences, 2020.

[Vasic et al., 2019] Marko Vasic, Aditya Kanade, Petros Ma-
niatis, David Bieber, and Rishabh singh. Neural program
repair by jointly learning to localize and repair. In ICLR,
2019.

[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon
Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,

Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In NeurIPS, 2017.

[Wang et al., 2020] Wenhan Wang, Ge Li, Bo Ma, Xin Xia,
and Zhi Jin. Detecting code clones with graph neural net-
work and ﬂow-augmented abstract syntax tree. In SANER,
2020.

[Wang et al., 2021a] Xin Wang, Yasheng Wang, Fei Mi,
Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin
Liu, and Xin Jiang. Syncobert: Syntax-guided multi-
modal contrastive pre-training for code representation.
arXiv:2108.04556, 2021.

[Wang et al., 2021b] Yue Wang, Weishi Wang, Shaﬁq Joty,
and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code understanding
and generation. In EMNLP, 2021.

[Watson et al., 2020] Cody Watson, Michele Tufano, Kevin
Moran, Gabriele Bavota, and Denys Poshyvanyk. On
learning meaningful assert statements for unit test cases.
In ICSE, 2020.

[Xu et al., 2021] Zenan Xu, Daya Guo, Duyu Tang, Qinliang
Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun
Quan, Daxin Jiang, and Nan Duan. Syntax-enhanced pre-
trained model. In ACL/IJCNLP (1), 2021.

[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang,
Jaime Carbonell, Russ R Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. NeurIPS, 32, 2019.

[Ye et al., 2020] Fangke Ye, Shengtian Zhou, Anand Venkat,
Ryan Marcus, Nesime Tatbul, Jesmin Jahan Tithi, Ni-
ranjan Hasabnis, Paul Petersen, Timothy Mattson, Tim
Kraska, et al. Misim: A neural code semantics similar-
ity system using the context-aware semantics structure.
CoRR, 2020.

[Zhang et al., 2021] Jingfeng Zhang, Haiwen Hong, Yin
Zhang, Yao Wan, Ye Liu, and Yulei Sui. Disentangled
code representation learning for multiple programming
languages. In ACL: Findings, 2021.

[Zhou et al., 2019] Yaqin Zhou, Shangqing Liu, Jingkai
Siow, Xiaoning Du, and Yang Liu. Devign: Effective vul-
nerability identiﬁcation by learning comprehensive pro-
gram semantics via graph neural networks. In NeurIPS,
2019.

[Zhou et al., 2021] Xin Zhou, DongGyun Han, and David
Lo. Assessing generalizability of CodeBERT. In ICSME,
2021.

[Z¨ugner et al., 2021] Daniel Z¨ugner, Tobias Kirschstein,
Michele Catasta, Jure Leskovec, and Stephan G¨unnemann.
Language-agnostic representation learning of source code
from structure and context. In ICLR, 2021.

