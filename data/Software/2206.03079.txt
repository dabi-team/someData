2
2
0
2

n
u
J

7

]

R
C
.
s
c
[

1
v
9
7
0
3
0
.
6
0
2
2
:
v
i
X
r
a

An Empirical Study of IoT Security Aspects at
Sentence-Level in Developer Textual Discussions

Nibir Mandal and Gias Uddin

DISA Lab, University of Calgary, Canada.

Abstract

Context: IoT is a rapidly emerging paradigm that now encompasses almost
every aspect of our modern life. As such, ensuring the security of IoT de-
vices is crucial. IoT devices can diﬀer from traditional computing (e.g., low
power, storage, computing), thereby the design and implementation of proper
security measures can be challenging in IoT devices. We observed that IoT
developers discuss their security-related challenges in developer forums like
Stack Overﬂow (SO). However, we ﬁnd that IoT security discussions can also
be buried inside non-security discussions in SO.
Objective: In this paper, we aim to understand the challenges IoT develop-
ers face while applying security practices and techniques to IoT devices. We
have two goals: (1) Develop a model that can automatically ﬁnd security-
related IoT discussions in SO, and (2) Study the model output (i.e., the
security discussions) to learn about IoT developer security-related challenges.
Method: First, we download all 53K posts from StackOverﬂow (SO) that
contain discussions about various IoT devices, tools, and techniques. Second,
we manually labeled 5,919 sentences from 53K posts as 1 or 0 (i.e., whether
they contain a security aspect or not). Third, we then use this benchmark to
investigate a suite of deep learning transformer models. The best performing
model is called SecBot. Fourth, we apply SecBot on the entire 53K posts
and ﬁnd around 30K sentences labeled as security. Fifth, we apply topic
modeling to the 30K security-related sentences labeled by SecBot. Then we
label and categorize the topics. Sixth, we analyze the evolution of the topics
in SO.
Results: We found that (1) SecBot is based on the retraining of the deep
learning model RoBERTa. SecBot oﬀers the best F1-Score of .935, (2) there
are six error categories in misclassiﬁed samples by SecBot. SecBot was mostly

Preprint submitted to Journal IST

June 8, 2022

 
 
 
 
 
 
wrong when the keywords/contexts were ambiguous (e.g., ‘gateway’ can be a
security gateway or a simple gateway), (3) there are 9 security topics grouped
into three categories: Software, Hardware, and Network, and (4) the highest
number of topics belongs to software security, followed by network security
and hardware security.
Conclusion: IoT researchers and vendors can use SecBot to collect and
analyze security-related discussions from developer discussions in SO. The
analysis of nine security-related topics can guide major IoT stakeholders like
IoT Security Enthusiasts, Developers, Vendors, Educators, and Researchers
in the rapidly emerging IoT ecosystems.

Keywords:

IoT, security, Stack Overﬂow, Deep Learning, Empirical Study

1. Introduction

Internet of Things (IoT) is a rapidly emerging paradigm that is deﬁned as
the connection between places and physical objects (i.e., things) over the In-
ternet via smart computing devices [9, 37]. This technological revolution now
encompasses almost every aspect of our modern life, such as smart homes,
cars, voice-enabled intelligent devices, and so on [4, 57]. Indeed, according to
Statistica reports [1], the number of “smart” connected devices was 15 billion
in 2015 and it is projected to be 75 billion by 2025 (i.e., 400% increase in
ten years). As such, it is not surprising that interest in the IoT technologies
is growing among developers to develop new architectures, tools, and tech-
niques and to make those devices intelligent based on data analytics [50, 87].
The pervasiveness of IoT-based solutions in our daily lives has raised con-
cerns about security in IoT [28, 32, 33]. Such concerns for IoT devices can
be related to the implementation/adoption of security protocols (e.g., zigbee
chain reaction [62]) and roles (e.g., authentication [36]), communication over
the network (e.g., cross-device inference [28]), as well as the underlying hard-
ware [40]. Indeed, ensuring the security of billions/trillions of IoT devices
requires eﬀorts of an unprecedented nature, unlike anything we have seen
before [64]. As such, it is important to understand the challenges IoT devel-
opers face during their adoption of security principles and practices for IoT
devices so that we can design eﬀective techniques to address these challenges.
With interest in IoT growing, we observe discussions of IoT developers
in online forums like Stack Overﬂow (SO). SO is a large online community
where millions of developers ask and answer questions. To date, there are

2

around 120 million posts and 12 million registered users on SO [54]. Previ-
ously, several research has been conducted to analyze SO posts, e.g., to an-
alyze discussions on big data [11], concurrency [3], programming issues [14],
blockchain development [86], microservices [13], and security [89]. However,
we are aware of no research that analyzed IoT security-related discussions
on SO, although such insight can complement existing IoT literature, which
so far has mainly used surveys to understand the issues and needs of IoT
practitioners [9, 37, 4].

We observe that IoT developers are active in SO and they discuss the
pros and cons of any IoT tools, devices, and technologies. Moreover, they
are aware of the new release of IoT devices and there are suﬃcient discus-
sions to get a full overview of security details in IoT devices. For example,
a developer asked about ‘arduino’ security details in SO question Q48854031.
The discussion session contains multiple security concerns for ‘arduino’ where
well-known developers share their valuable knowledge, including implementa-
tion support. This discussion is helpful for both new and advanced developers
as they get insightful information and various aspects of the IoT[6]. Besides
this, some recent studies indicate that they are currently facing challenges
while developing or implementing IoT devices due to the intricacies and sensi-
tivity of security details [73]. Thus, the developers require proper knowledge
and standard implementation details to develop a secure device or tools. As
IoT developers are following SO and sharing knowledge with others subtly,
the community can follow the statutes available in SO while developing the
security details of an IoT product. Moreover, the community can form a
standard for developing IoT tools, which they can propagate through SO
discussions. This will be helpful for the community to build a secure device
and the users to have a secure device. In addition, the newcomer can gather
IoT security knowledge from SO, which will help them to build more robust
devices in the future, as the security standard will be continuously upgraded
in the future. This will be beneﬁcial for the vendors as well, because they will
get direct feedback about their product from the community, and this will
help them to keep up with the market demands. Therefore, the IoT security
discussion in SO has great value for IoT stakeholders.

A major problem in supporting the automated analysis of IoT security
discussions from SO is that such security discussions can be buried inside
technical discussions of developers. For example, consider the answer to the
SO question Q33004863 in Figure 1. The question concerns two security soft-
ware libraries (i.e., APIs), both of which contain the implementation of the

3

Figure 1: An IoT discussion in a Stack Overﬂow answer about TinyDTLS security and
usability aspects (Q33004863)

‘Datagram Transport Layer Security (DLTS)’ protocol. The DLTS is a com-
munication protocol to provide security for software applications that operate
in packet-switched networks where a datagram is a basic transfer unit. A
datagram has a header and payload section. The protocol uses datagrams in
a stateless manner to prevent malicious attacks like eavesdropping, tamper-
ing, or message forgery. The asker of Q33004863 is particularly interested in
the DLTS implementation in the tinyDLTS library because he needs to use
the protocol in his IoT device. However, he is also worried about whether
the tinyDLTS API oﬀers limited features due to constrained resources and
data size. The answer to the question does conﬁrm that the tinyDLTS has
limited security features, e.g., it does not support X.509 certiﬁcates. The
last line of the answer, however, notes that the API is usable. Indeed, while
the ﬁrst three sentences in the answer are related to the security features of
the tinyDLTS API, the last line is about the usability aspect of the API.
Therefore, if we simply are interested in learning about security aspects of
software in SO posts, we may want to only consider the ﬁrst three sentences
of the answer. This means that sentence-level security aspect detection in
SO posts can be more precise and less noisy than post-level analysis. There-
fore, we need automated techniques to precisely detect security aspects in
SO developer discussions. Previously, Uddin and Khomh [83] developed a
supervised shallow machine learning classiﬁer to detect security aspects at
sentence-level in SO posts. However, the classiﬁer was not trained and tested
on IoT-related sentences. This can aﬀect the performance of the classiﬁer

4

Usability/Design AspectSecurity AspectQuestionTagsAccepted Answermodel as IoT security is diﬀerent from traditional security. For example, in
the IoT domain, X.509 indicates a public key certiﬁcate that is related to
security. But in our traditional computation, we can consider this an entity
or a number. A generic classiﬁer model can’t discern such security details
from non-security discussions as the model is trained and designed to have
generic knowledge.
In fact, a security-speciﬁc model can not discern such
discussions. Thus, we need to design a classiﬁer model that has IoT domain
knowledge. Moreover, the classiﬁer that Uddin et al.[81] used also did not
use recent advances in deep learning transformer models, which are found
to outperform shallow learning models in software engineering (SE) datasets
(e.g., sentiment classiﬁcation [91]).

As a ﬁrst step towards developing techniques to support the detection and
analysis of IoT security discussions in SO at sentence-level, in this paper, we
adopt two phases.

• Phase 1. We studied the eﬀectiveness of a suite of deep learning (DL)

models to detect security aspects on IoT discussions in SO.

• Phase 2. We studied the IoT topics found in the security-related
sentences in SO, where the security-related sentences are detected by
the best performing model from Phase 1.

In Phase 1, our focus is to study and ﬁnd the best performing machine
learning (ML) model that we can use to automatically detect security-related
IoT discussions in SO. Detection of security aspects is the key to study IoT
security-related discussions in SO. This is because, to the best of our knowl-
edge, there is no existing technique to identify the security aspect of IoT
posts. So, We collect 53K posts from SO based on 78 IoT tags (e.g., Ar-
duino, Raspberry-pi). Then, we create an IoT security dataset of 5919 sen-
tences from the IoT posts to assess the performance of deep learning (DL)
models. In phase 1, we answered a total of three research questions.
RQ1. How do the deep learning models perform to detect IoT
security aspects in the benchmark?

We studied four optimized transformers- i.e., BERT[31], RoBERTa[48],
XLNet[90], and DistilBERT[55] for this research question. Transformer-
based advanced language models [43] are found to outperform other models
in text classiﬁcation (e.g., sentiment detection in SE [91]). In addition, re-
cent studies suggest that a domain-speciﬁc DL model is more eﬀective than
a general-purpose deep learning model [72]. Thus, for this research question,

5

we also employ the BERTOverﬂow model, which has been pre-trained on
SO dumps [71]. Indeed, in our security aspect detection, we ﬁnd that the
transformer models outperform the baseline SVM, which was found to be
the most eﬀective in security aspect detection by Uddin and Khomh [81] in a
general purpose API review dataset. RoBERTa shows the best performance
among the four transformer models, with an F1-Score of 0.935. We name
this trained model SecBot.
RQ2. What are the error categories in the misclassiﬁed cases of
the best performing model?

From the model’s perspective, we analyze the misclassiﬁcation of SecBot.
It helps us get insights into the model and ﬁgure out the challenges of detect-
ing security aspects in IoT discussions. We ﬁnd that out of 5,919 samples in
the IoT security dataset, SecBot misclassiﬁes 139 sentences. We will analyze
all 139 sentences. We discover six types of erroneous categories (i.e., ‘Am-
biguous Keywords’, ‘Ambiguous Context’, ‘Implicit Context’, ‘Code Parsing
Error’, ‘Text Parsing Error’, and ‘Infrequent/unknown Keywords’) in those
samples.

In Phase 2, we apply SecBot to all sentences in our 53K IoT posts to
detect security-related sentences. Out of the total 672K sentences, SecBot
found around 30K security-related sentences. We studied the topics devel-
opers discussed about IoT security by answering two research questions.
RQ3. What are topics in the IoT security discussions collected from
the entire IoT SO dataset using the best performing model?

Topic modeling excavates useful information in a large dataset. In our
case, we can learn about the security topics that developers ask, post, and
answer in SO. Using SecBot, we found around 30K security-speciﬁc sen-
tences. We apply topic modeling to the 30K sentences to learn about IoT
security topics in developer discussions. We ﬁnd 9 topics grouped into three
categories: Software, Network, and Hardware. The largest number of topics
belong to the software category, followed by the network category and the
hardware category.
RQ4. How do the IoT security topics evolve in SO?

Evolution studies of IoT security topics can be helpful for both developers,
vendors, and investors to keep themselves up to date. IoT developers can
learn about recent tools and technology where vendors and investors can
learn about the developer’s interests in any technology or tool. We ﬁnd that
while software has the greatest number of topics, in recent years since 2017,

6

Figure 2: Overview of our empirical study

we see an almost equal number of security-related sentences for both network
and software-related topics. This ﬁnding indicates that we need to equally
prioritize IoT security research and development eﬀorts for IoT software and
networking related problems.
Replication Package: We share all the data and and all the source to
replicate our study in https://bit.ly/3nL2Sj5.

2. Empirical Study Setup

In this section, we ﬁrst oﬀer a schematic overview of the overall empir-
In Section 2.2, we discuss the how we
ical study approach (Section 2.1).
collect and process our IoT datasets from SO and in Section 2.3, we discuss
how we create our benchmark of 5919 sentences that we used to investi-
gate the accuracy of our IoT security classiﬁcation models in detecting IoT
security-related sentences in SO. We then discuss how we ﬁne-tune the ﬁve
transformer models during our study (Section 2.6) and oﬀer details of the
performance metrics used to report the accuracy of the models (Section 2.7).

2.1. Schematic Overview of Study

In Figure 2, we show the overview of our empirical study and the for-
mulation of our research questions. First, we collect IoT-relevant data (i.e.,
posts, comments, and answers) from SO dumps, applying some ﬁlters on
SO-tags. Then we apply some preprocessing (i.e., sentence tokenization) to

7

BERTOverflowRQ3the dataset. To detect security-related sentences, we use ﬁve DL transformer
models: BERT, RoBERTa, XLNet, DistilBERT, and BERTOverﬂow on the
dataset. In RQ1, we evaluate ﬁve of the transformers (BERT, RoBERTa,
XLNet, BERTOverﬂow, and DistillBERT) to ﬁnd the best performer based
on both domain-neutral and domain-speciﬁc word embedding. Our next
objective is to understand the weaknesses of the security detectors of the
best-performing model from RQ1. So we collect the predictions of the best
security detectors. Then, we analyze the error categories of the best per-
forming model from RQ1 in RQ2. In RQ4, we apply topic modeling to ﬁnd
the most discussed topics and their distribution in the IoT security domain.
Then we calculate the popularity of each topic category in RQ5 to understand
the evolution of these topics.

2.2. IoT Data Collection from Stack Overﬂow (SO)

Our research started in 2020, when the latest SO data dump available was
the September 2019 data dump from SO. Given that it is now 2022, newer
SO data dumps are now available, e.g., the Jan 2022 dump. However, when
we compared the SO dump of September 2019 with the SO data dump of Jan
2022, we observed several discrepancies like missing/deleted posts in the Jan
2022 data dump compared to the dump of September 2019. As such, in this
paper, we analyzed both data dumps, i.e., September 2019 and January 2022.
While we answer the research questions in our empirical study using the SO
data dump of September 2019 in Section 3, we compared the major results
from our empirical study between the two SO data dumps (i.e., Sept 2019
vs Jan 2022) in Section 4. For both of the data dumps, our data collection
process remains the same. For clarity, we describe our data collection process
below by referring to SO data dump of September 2019.

We follow Uddin et al. [84] to collect SO posts related to IoT discussions.
First, we download the SO data dump [68] of September 2019. The SO
dataset includes all posts for 11 years between 2008 and September 2019. In
total, it has 46,767,375 questions and answers. Out of those, around 40%
are questions and 60% are answers. Out of the answers, around 34% are
accepted. Second, we determine the list of potential IoT security tags in
SO, following Yang et al. [89]. We identify three most popular IoT related
tags, i.e., ‘iot’, ‘arduino’, and ‘raspberry-pi’ in SO. As these three have the
maximum number of posts in IoT domains, our intuition is that these tags
should cover the widest range of security-related topics. Thus, we select these
three tags as our primary tags. We denote those as Tinit. Then we collect

8

candidate tags TA related to Tinit by analyzing tags that are labeled at least
one of those tags Tinit. As this step includes all IoT tags, it makes sure that
all security related tags are also added for the study. Following previous
research [89, 12], we systematically ﬁlter out irrelevant tags and ﬁnalize a
list of all potential IoT tags T as follows. For each tag t in TA, we compute
its signiﬁcance and relevance as follows.

Signiﬁcance µ(t) =

#of Questions with tag t in P
#of Questions with tag t in SO dump

Relevance ν(t) =

#of Questions with tag t in P
#of Questions in P

(1)

(2)

Here, P denotes all questions with tags Tinit. Our 49 experiments with a
broad range of threshold values of µ and ν show that µ = 0.3 and ν =
0.001 allow for a more relevant set of IoT tags. The threshold values are
consistent with previous work [89, 3]. We ﬁnally got 75 IoT tags, including
security-related tags. Third, we found a total of 81,651 posts (questions
and answers). Out of which, around 48% are questions (i.e., 39,305) and
52% (42,346) are answers. Out of the answers, around 33% (13,868) are
accepted. Following previous research [12, 14], we only consider the questions
and accepted answers to the questions to avoid noise. Our ﬁnal dataset B
consists of a total of 53,173 posts (39,305 questions, 13,868 accepted answers).
Given our focus is to study IoT security discussions, it is important that
we can pick as many IoT related discussions as possible that could be relevant
to IoT security. Intuitively, if an IoT developer is worried about security of
IoT, he/she may add ‘security’ or ‘iot security’ as a tag in SO. However,
we did not ﬁnd any tag as ‘iot security’ in SO. We found a tag ‘security’
in IoT related posts in SO, which was accompanied by one of the 75 tags.
For example, Q48607181 discusses about the possibility of REST API in IoT
over SSH. The question has both ‘iot and ‘security’ as tag. However, as we
discuss in Section 3, most of the IoT posts in SO do not have a ‘security’ tag
associated, even if the question contains security-related discussions.

Developer discussion mostly consists of textual discussion, such as how
to connect Arduino wiﬁ modules. However, developers sometimes seek pro-
gramming support and resource sharing links. Sometimes, developers post
building logs with images. Thus, a post may contain code-snippets, text,
urls, etc. This makes SO posts more complex. In our investigation, we fo-
cus on only textual discussion, and thus we remove all types of discussion,

9

Table 1: Summary stats and agreements for the IoT Security dataset

Summary

Agreement

Dataset Sentences #Security Kappa Percent

Benchmark 5919
384

Judgemental

1049 (17.7%)
40 (10.5%)

0.94
0.88

97.6%
95.6%

such as source codes, logging, urls, etc., except text. We use ‘beautifulsoup’
library to ﬁlter out text, source codes, logging sentences, and urls. This
step ensures that our dataset contains only text. However, there is a small
amount of inline source code that can’t be ﬁltered out. For example, the SO
question, Q7383606, has the following line: “...It was suggested that I try char
msg[] = myString.getChars();...”. After nltk tokenization, we ﬁnd that only
a few sentences contain such inline codes. Therefore, our IoT dataset is only
applicable for textual discussion.

2.3. Creation of IoT Security Aspect Benchmark

We study the performance accuracy of the studied DL transformer models
to detect IoT security-related sentences in SO. Thus, we create a benchmark
dataset of 5,919 sentences from our IoT dataset of 53K posts. First, we use
nltk [52] to segregate sentences from the 53K IoT posts, which results in
672,678 sentences (i.e., an average of 13 sentences per post). Then we ran-
domly select 5,919 non-overlapping sentences and label those. This sample
size is statistically signiﬁcant with a 99.99% conﬁdence level and 2.5 conﬁ-
dence interval. The ﬁrst two authors then manually label each sentence as 1
(i.e., contains security discussion) or 0 (otherwise). Prior to labeling, the au-
thors consult together to establish a coding guideline. The second author has
a PhD and has published peer-reviewed high-impact papers in software secu-
rity. Following that, each author separately labels all sentences based on the
guidelines. The agreement analysis is computed between the authors based
on two metrics (Cohen Kappa and Percent Agreement). The disagreements
are resolved by mutual discussion between the authors. Finally, the random
samples have 1049 sentences labeled as security (17.7%). As a post consists
of multiple sentences (i.e., security and non-security related sentences) and
security-related sentences can be anywhere inside a post, the positive rate
(% of security-related samples) is low. One prior study on SO security was
[82], where there were only 4% (197 samples)
conducted by Uddin et al.

10

security-related samples out of 4522 samples. In Table 1, we show summary
statistics of the IoT Security dataset. The last two columns of Table 1 show
the agreement level between the two coders during their manual labeling.
The Cohen kappa value is 0.94 and the percent agreement is 97.6%. Accord-
ing to Viera at el. [85], a kappa value between 0.81 and 0.99 denotes a perfect
agreement.

2.4. Creation of IoT Judgmental Dataset

In the previous section, we discussed how our benchmark dataset is cre-
ated. We train our model on the benchmark dataset and select the best
performing model. However, model performance on the benchmark dataset
does not give us the guarantee that the model will perform similarly when
tested in diﬀerent environments. Therefore, we generate a new dataset to
cross-check the best performing model. To select the size of the dataset, we
follow Burmeister et al.
[21] and ﬁnd that a minimum of 384 samples are
needed to represent the whole IoT dataset of 672678 samples with a conﬁ-
dence of 95% and an interval of 5. We ﬁrst exclude all 5919 samples from the
IoT dataset and then collect 384 samples randomly. Finally, we label those
384 samples. We call this newly created dataset as “Judgemental dataset”.
The statistics for this dataset are shown in the Table 1. The Judgemental
dataset contains approximately 10% security-related samples. We use this
dataset to judge our best performing deep model on the benchmark dataset.

2.5. Baseline Models

In our experiments, we use two baseline models, i.e., Support Vector
Machine (SVM) [10] and Logistic Regression (Logit) [58] that are used by
Uddin et al.
[82] for software aspect classiﬁcation in SO. For classiﬁcation
tasks, both SVM and Logit provide probabilistic values that lie between 0 and
1 for a given input. However, these models take input as numerical values.
As we are dealing with textual content, we ﬁrst convert text samples into
numerical values. We use Term Frequency-Inverse Document Frequency (TF-
IDF) [92] to get a numerical value for a sample text. The inverse proportion of
the frequency of a word in a given document to the percentage of documents
the word appears in is used by TF-IDF to generate values for each word in
a document. Words with high TF-IDF numbers imply a strong relationship
with the document they appear in. We convert all samples to numerical
values using TF-IDF and then feed this input to the baseline models. All
hyparparameters are used similar to Uddin et al. [82].

11

Name

BERT

XLNet

RoBERTa

DistilBERT

Table 2: Studied Deep Learning Models

Rationale to Study

BERT is designed to learn pre-trained contextual word repre-
sentation by running of large volume of unlabeled texts. The
transformer mechanism allows the encoder and decoder to see
the entire input sequence at once.
XLNet oﬀers better training methodology and uses more data
for pre-training than BERT, which could increase its accuracy
in learning.
RoBERT uses more data and computation power to learn
from the data. It also oﬀers better training methodology than
BERT and XLNet. As a result, RoBERTa outperforms both
BERT and XLNet on benchmark datasets (e.g., GLUE).
DistilBERT is also a lightweight version of BERT in terms of
the learned architecture and word embedding, while retaining
more than 95% of BERT performance.

BERTOverﬂow BERTOverﬂow is an in-domain pretrained BERT. It is pre-
trained on SO dumps that include code snippets. As a result,
it oﬀers better performance on software domain-speciﬁc tasks.

2.6. Hypertuning of Studied Deep Learning Transformer Models

We investigate ﬁve state-of-the-art transformer models: (1) BERT, (2) RoBERTa,

(3) XLNet, (4) DistillBERT, (5) BERTOverﬂow

Transformers [43] are attention-based DL models that can handle se-
quential data. These trained models are used for various language-speciﬁc
downstream tasks after ﬁne-tuning the pre-trained models for the speciﬁc
task-related domain (e.g., sentiment detection, machine translation). Our
selection of these ﬁve transformer models is based on previous ﬁndings in
SE classiﬁcation tasks. BERT has shown massive improvement over other
deep models for sentiment analysis tools in SE [53][19]. Other variants of
BERT, i.e., RoBERTa, XLNet, etc., are also used in the SE classiﬁcation
task. RoBERTa, the most optimized BERT model, has shown better per-
formance in multiple SE classiﬁcation tasks. For example, Uddin et al. [80]
found that RoBERTa performed better than other transformer models in six
diﬀerent datasets for sentiment detection. In another recent study, Dai et al.
[30] showed RoBERTa’s superiority in aspect level software sentiment anal-

12

Table 3: Architecture details of variants of BERT Model (L = Layers, H = Hidden, D =
Heads, P = Parameters)

Architecture Used Model

L H D

P

BERT

XLNet
RoBERTa
DistilBERT

bert-base-
uncased
xlnet-base-cased
roberta-base
distilbert-base-
uncased

12

768

12

110M

12
12
6

768
768
768

12
12
12

110M
125M
66M

BERTOverﬂow jeniya/bertoverﬂow 12

768

12

110M

Figure 3: BERT Classiﬁcation Processing

ysis. Moreover, we ﬁnd that other transformer models, such as DistilBERT
and XLNet, also perform as well as the BERT model [15]. Besides this,
Tabassum et al. [71] found that the SO domain speciﬁc model BERTOver-
ﬂow had improved the performance of SO related classiﬁcation tasks. This
motivated us to study these ﬁve models for our IoT security aspect classiﬁ-
cation task. We leave the exploration of recent state-of-the-art models like
GPT2 and T5 as our future work. In Table 2, we show the details of the ﬁve
studied models in this paper.
Fine-Tuned Architecture. Table 3 shows the architectural details of the
used Transformer models from the hugging face transformer library, an open-
source community centered on pretrained transformer libraries. BERT ar-

13

Dense(2)SoftMaxDense(128)OutputEmbedded InputTanhDropout(.25)BERT TokenizerInput SentenceBERT EmbeddingLayersBERT Classiﬁca onLayerClassiﬁca on LabelClassiﬁca onSecurityNon-SecurityBERT Classiﬁca on LayerTable 4: Hyperparameters of the deep learning models on the IoT Security dataset
Batch Size Epochs Optimizer Learning Rate
Model

BERT
RoBERTa
XLNet
DistilBERT
BERTOverﬂow

32
16
8
8
32

3
2
3
3
2

Adam
Adam
Adam
Adam
Adam

2e-5
1e-5
2e-5
2e-5
1e-5

chitecture provides the contextual embedding for each word in a sentence.
For classiﬁcation purposes, a classiﬁer is used at the top of the model that
takes the embedding of sentences as input and outputs the predicted weights
for each category (Security and Non-Security). Figure 3 (right) shows the
used architecture of the classiﬁer. It consists of a dense layer with T anh ac-
tivation followed by a dropout layer, a dense layer with Sof tM ax activation.
Figure 3 (left) shows how we used the BERT architecture for our security
aspect detection. First, a tokenizer takes the input sentences and represents
them as tokenized sentences. Then, these tokenized sentences are fed into
the BERT model. Finally, the embedded sentences are passed through the
classiﬁer to get the ﬁnal output (security or non-security).

Following the sequence classiﬁer model from hugging face, we apply T anh
activation. In addition, we use the dropout layer to reduce overﬁtting. To
ﬁnetune the model on our dataset, we initialize the classiﬁer randomly where
the BERT is initialized from pretrained models.

For each deep learning model, following literature in text classiﬁcations [5,
38, 31], we tuned the following hyperparameters for optimal performance:
(1) Batch size, (2) Number of hidden layers, neurons, units, ﬁlters, epochs
(3) Optimizer, Activation, and loss functions, and (4) Learning rate. To
tune these hyparparameters, we use standard Grid search technique [17]. We
select a set of batch sizes (i.e., 8, 16, and 32), epochs (i.e., 2, and 3), and
learning rates (i.e., 1e-5, 2e-5, and 3e-5) as suggested in hugging face. We run
the models on the dataset for each set of hyperparameters. Then we choose
the best hyperparameter set for each model based on their performances. For
example, we ﬁnd the best performance of BERT for batch size of 32, epoch
of 3, and learning rate of 2e-5. In Table 4, we show the values of selected
hyperparameters in our deep learning model after tuning. Our replication
package details the hyperparameter values with source code.

14

Data Processing. We prepare our dataset with only textual discussion.
However, there may be some urls. These unwanted texts may hinder the
model’s performance. Thus, we check for URLs and remove them using
regular expressions. For implementation, we use Python’regex’ library. In
addition to this, we remove all stop words. As the importance of stop words
is low and the frequency of these words is high, these words can obscure
other signiﬁcant words. Moreover, recent studies related to sentiment analy-
sis found that stop words removal is an eﬀective step in improving a model’s
performance [35]. Therefore, we remove all stop words using ‘nltk’ [52] li-
brary.
Model Features. We used the pre-trained embedding in BERT models,
which learns additional contextual information for a given domain (e.g., in
our case, security in developer discussions) during the training phase. The
combination of pre-trained and domain-speciﬁc learning into such pre-trained
models oﬀers more contextual information during classiﬁcation tasks than
traditional DL models. We add ‘EOS’ and ‘CLS’ to each sentence and then
tokenize all sentences using a pretrained BERT-Tokenizer. We padded each
sentences to 100 length with ‘zero’ padding.

2.7. Performance Metrics

We analyze the performance of the classiﬁcation models for security as-
pect detection using ﬁve standard metrics in information retrieval [49]: Pre-
cision (P), Recall (R), F1-score (F1), Accuracy (Acc), and Matthews Corre-
lation Coeﬃcient (MCC).

1. Precision (P) is the fraction of sentences labeled as ‘HasSecurity’ (i.e.,
the sentence contains security discussions) out of all the sentences labeled
as ‘HasSecurity’.

P =

T P
T P + F P

(3)

Here TP = Correctly classiﬁed a sentence as containing discussion about
security, FP = Incorrectly classiﬁed as containing discussion about secu-
rity,

2. Recall (R) is the fraction of all ‘HasSecurity’ sentences that are success-
fully identiﬁed out of all sentences that contain security discussions.

R =

T P
T P + F N

15

(4)

Here FN = Incorrectly classiﬁed as not containing discussion about se-
curity.

3. F1-score is the harmonic mean of precision (P) and Recall (R).

F 1 = 2 ∗

P ∗ R
P + R

(5)

4. Accuracy (Acc) is the fraction of all correctly classiﬁed out of all records.

Acc =

T P + T N
T P + T N + F P + F N

(6)

Here TN = Correctly classiﬁed as not containing discussion about secu-
rity.

5. Matthews Correlation Coeﬃcient (MCC) is the linear correlation be-
tween all ‘HasSecurity’ labeled sentences and the sentences that contain
security discussion. It takes into account all possible scenarios- i.e., cor-
rectly identiﬁed security-related sentences, incorrectly identiﬁed security-
related sentences, correctly identiﬁed non-security-related sentences, and
incorrectly identiﬁed non-security-related sentences.

M CC =

T P ∗ T N − F P ∗ F N
(cid:112)(T P + F P )(T P + F N )(T N + F P )(T N + F N )

(7)

6. Area Under the Curve (AUC) is the area under the Receiver Operat-
ing Characteristics (ROC) curve. An ROC curve is a graphical plot to
illustrate the ability of a binary classiﬁer (such as ours).

T P R =

T P
T P + F N

, F P R =

F P
F N + T P

AU C =

(cid:90) 1

x=0

T P R(F P R−1(x))dx

(8)

F1-score is preferred over metrics like accuracy (Acc) when evaluation dataset
is imbalanced. For example, our IoT benchmark dataset is imbalanced with
lower number of sentences labeled as 1 (i.e., ‘HasSecurity’ = 1) than sentences
labeled as 0. Following standard practices in Literature [91, 83], we use F1-
score to determine and report the best model.

16

3. Empirical Study Results

In this section, we answer four research questions (RQ) using SO data
dump of September 2019. We compare the results against those found in SO
data dump of January 2022 in Section 4.

RQ1. How do the deep learning pre-trained transformer models perform

to detect IoT security aspects in the benchmark? (Section 3.1)

RQ2. What are the error categories in the misclassiﬁed cases of the best

performing model? (Section 3.2)

RQ3. What are topics in the IoT security discussions collected from the
entire IoT SO dataset using the best performing model? (Sec-
tion 3.3)

RQ4. How do the IoT security topics evolve in SO? (Section 3.4)

3.1. RQ1 How do the deep learning pre-trained transformer models perform

to detect IoT security aspects in the benchmark?

3.1.1. Motivation

Pretrained transformer models have already shown dominating perfor-
mances over shallow and deep models in software engineering [91]. Moreover,
recent studies on software engineering have found the eﬀectiveness of domain-
speciﬁc embedding (e.g., BERT, RoBERTa, XLNet, etc.) than a generic one.
For example, Tabassum et al.
[71] shows that the BERTOverﬂow model
that is pre-trained on Stack Overﬂow dumps beats the performance of ef-
ﬁcient transformers like RoBERTa on developer discussions such as Github
and Stack Overﬂow. Therefore, we investigate the possibility of improving
the baseline performance to detect security aspects in IoT discussions using
studied Transformer Models, e.g., BERT.

3.1.2. Approach

We ﬁrst use the newly created IoT Security dataset from Section 2.3 to
get the best performing transformer model. For experimentation, similar
to Uddin and Khomh [83], we use a stratiﬁed k-fold cross-validation ( k =
10). Each fold divides dataset D into k disjoint folds, each of approximately
equal size. Di ∩ Dj = φ ∀ i, j ∈ k and i (cid:54)= j and D1 ∪ D2 ∪ ... ∪ Dk = D.
To avoid overﬁtting: we train the models k times each time we hold one of
the k folds for validation, and we use the rest k − 1 folds for training. The

17

Figure 4: 10-Fold cross validation technique used in our study

In our experiments, we set K as 10.

value of k is a hyperparameter.
In
Figure 4, we show 10-Fold cross validation techniques. We run the model
10 times. In each iteration, we train the model on nine diﬀerent folds and
test the trained model on the last fold. Then, we calculate the average
performance of these 10 iterations. For example, according to the Figure 4,
at the ﬁrst iteration, we pick ‘Fold-2’, ‘Fold-3’, ‘Fold-4’, ‘Fold-5’, ‘Fold-6’,
‘Fold-7’, ‘Fold-8’, ‘Fold-9’, and ‘Fold-10’ to train the model, and we test this
model on the ‘Fold-1’. Therefore, in our case, we ﬁne tune our model during
each training (i.e., k times) and then test the tuned model on the test fold. To
tune the hyperparameter, we follow grid search [67] tuning techniques. For
example, we set learning rate as 1e-5, 2e-5, and 3e-5 and select the one that
has highest performance for each model. For the two baselines (Logit and
SVM), we hyper tuned all the parameters used by Uddin and Khomh [83, 81].
We ﬁnd the performance of transformer models on benchmark dataset and
select the best performing model based on the F1-Score. Then, we test the
trained model on our Judgmental dataset to check either the model learn to
generalize the security discussion in IoT domain.

3.1.3. Results

In Table 5, we report the performance of two baselines and the ﬁve ad-
vanced language-based pre-trained transformer models on the IoT security
dataset. For each model, we report six performance metrics from Section 2.7.
Among the two shallow learning baselines, SVM oﬀers the best performance
for all six metrics, outperforming Logits. SVM shows an F1-score of 0.866,
while Logits shows an F1-score of 0.864. For the other ﬁve performance met-

18

Fold-3Fold-4Fold-5Fold-6Fold-7Fold-8Fold-9Fold-10Fold-1Fold-2Fold-3Fold-4Fold-5Fold-6Fold-7Fold-8Fold-9Fold-10Fold-1Fold-2Fold-3Fold-4Fold-5Fold-6Fold-7Fold-8Fold-9Fold-10Fold-1Fold-2Fold-3Fold-4Fold-5Fold-6Fold-7Fold-8Fold-9Fold-10Fold-1Fold-2Fold-3Fold-4Fold-5Fold-6Fold-7Fold-8Fold-9Fold-10Fold-1Fold-2 Train DataTest DataFoldsIteration-1Iteration-2Iteration-3Iteration-10Table 5: Performance of security aspect detection of deep transformer models in the IoT
Security dataset [83]

Type

Model Name

Acc

P

R

F1 AUC MCC

Transformer BERT

0.928
0.962
0.930 0.941 0.935 0.962
RoBERTa
0.952
0.922
XLNet
0.950
0.907
DistilBERT
0.957
BERTOverﬂow 0.976 0.938

0.976
0.977
0.972
0.969

0.921
0.914
0.933

0.921
0.921
0.928

0.939

0.934

Baseline

SVM
Logit

0.953
0.952

0.876
0.875

0.856
0.853

0.866
0.864

0.915
0.913

Performance Change in RoBERTa Over Baselines

0.919
0.922
0.904
0.895
0.919

0.838
0.835

Over SVM
Over Logit

2.5% 6.2% 9.9% 8.0% 5.1% 10.0%
2.6% 6.3% 10.3% 8.2% 5.4% 10.4%

rics - Accuracy, Precision, Recall, AUC, and MCC, the improvements are
around .01%, .01%, .02%, .02%, and .04% respectively. We note that the
performance for both models is high. The higher performance of these mod-
els on our dataset is due to the following reasons: Classiﬁcations can often be
more straightforward for security aspects, i.e., parsing of words could suﬃce
(e.g., secure, attack, and hacking). So, non-contextual models like SVM and
Logits can detect security aspects with high precision and recall.

All ﬁve transformer models show better performances in terms of all met-
rics than the baseline models from Uddin [81] (SVM and Logit). The last
two rows in Table 5 show the performance change (in percentages) we ob-
served in the best performing transformer model (RoBERTa) compared to
the two baselines. For example, the best performing transformer model is
RoBERTa with an F1-score of 0.935, which outperforms the baselines SVM
by 8.0%, and Logits by 8.2%. Higher performances mean that transformers
are precisely (i.e., around 6% higher precision) discovering (i.e., around 10%
higher recall) security-speciﬁc sentences. It is because these models can un-
derstand the contextual meaning in a sentence. For example, the following
non-security related sentence “My goal is to create a sort of password-like
system where if you push the right buttons in the right order then a button
will light up.” The comments describe the procedure of a system using the
references ‘password like’ that have no relation with security. The baselines

19

model erroneously labeled it as 0 due to the misleading keyword- ‘password
like’ where the transformer models discerned the context. Hence, it found no
security threats in this sentence. This superiority of these models over base-
lines is not surprising given that the transformer models are generally found
to have oﬀered better performance than state-of-the-art classiﬁers for mul-
tiple other text classiﬁcation tasks in software engineering (e.g., sentiment
detection [91]).

Among the transformers, RoBERTa shows the best performance with an
F1-Score of 0.935, while DistilBERT is the lowest with an F1-Score of 0.914.
F1-Score for the other three models- e.g., BERT, BERTOverﬂow, and XL-
Net are 0.934, 0.933, and 0.921 respectively. There is a tiny gap between
the performance of RoBERTa and BERT. It is because the architecture and
training procedures of these two models are almost similar. The modiﬁ-
cation, dynamic changes of the masking pattern (in RoBERTa) instead of
static masking (in BERT) during the training phase, could be the reason
behind the subtle improvement in performance. XLNet improves the per-
formances by increasing the training to handle the dependency between the
words and capture long-distance information. In our dataset, each sentence
is simple and small in size as well. So, large-scale training on the dataset
causes overﬁtting and lower performance than BERT. The performance drop
of DistilBERT is understandable as the distillation of the BERT model is
obviously lower performing than its original model.

Although BERTOverﬂow has the same architecture as BERT and is
trained on the StackOverﬂow dumps, the performance of this model is lower
than RoBERTa and BERT. RoBERTa and BERT model beat BERTOver-
ﬂow by 0.21% and 0.11% in terms of F1-Score. However, BERTOverﬂow has
the best precision of 0.938 among the transformer models. This result indi-
cates that BERTOverﬂow predicts security-speciﬁc sentences precisely (i.e.,
better precision) but lacks in dicoverability (i.e., lower recall and F1-Score) in
comparison to RoBERTa and BERT. We look into the cause of BERTOver-
ﬂow’s performance drop. Our ﬁnding in this scenario is that BERTOver-
ﬂow pretrained on StackOverﬂow dumps containing code snippets, so the
model faced some security-related words (e.g., ’private‘, ’protected‘, ’bug‘,
’break‘, ’lock‘, ’key‘) in more programming contexts than security contexts.
The model fails to learn the correct embedding for these words during the
ﬁne-tuning or training phase. For example, consider the following security-
speciﬁc sentence “It will be helpful if anyone states which are keys to lock
so that in addition in my python script for locking phone programmatically.”

20

Table 6: Spearman Rank Correlation Coeﬃcient for RoBERTa and BERT pair and
RoBERTa and BERTOverﬂow pair.

Model Pair
RoBERTa & BERT
RoBERTa & BERTOverﬂow

Correlation Coeﬃcient, ρ p-value
< 0.0001
< 0.0001

0.941
0.940

BERTOverﬂow failed to embed security context for this sentence and labeled
it as 0- i.e., non-security.

Our intuitions indicate that RoBERTa is the best security-discussion
detector, which is also supported by the experimental results. However,
RoBERTa has only a 0.1% and 0.2% lead over the second and third best
models, which is not a signiﬁcant diﬀerence. Thus, we perform a statistical
analysis to understand the diﬀerence in results between RoBERTa and the
other two models. We calculate the Spearman Rank Correlation Coeﬃcient
[88] for the results of RoBERTa and BERT, and RoBERTa and BERTOver-
ﬂow. We show the result in Table 6. For both pairs, the correlation scores
are as high as 0.94. However, the results of RoBERTa and BERT are more
correlated (0.1% higher) than the results of RoBERTa and BERTOverﬂow.
Following Leclezio et al. [45], the scores indicate that the results of RoBERTa
are strongly correlated with the results of BERT and BERTOverﬂow. Never-
theless, the score also indicates that there is a diﬀerence between the results
of RoBERTa and other models, as the score is far less than 1.

From these experiments and analysis of the results, we ﬁnd RoBERTa
is the best performing deep learning model. We further test this trained
RoBERTa model on our judgemental dataset. We ﬁnd that the model per-
forms similarly on the judgemental dataset. RoBERTa has a precision of 0.95,
a recall of 0.85, and a F1-Score of 0.9. We report the performance in the
Figure 5. Compared to the performance on the benchmark dataset, model
has better precision but lower recall, which results in a slightly lower F1-
Score on the judgemental dataset. This result indicates that the model has
learned to generalize security discussion in IoT domain. Thus, our RoBERTa
model is reliable enough to carry on further experiments. From now on, we
refer to this best-performing transformer model, RoBERTa, as SecBot in
the remainder of the paper.

21

Figure 5: Performance of RoBERTa on bechmark and judgemental dataset

3.2. RQ2 What are the error categories in the misclassiﬁed cases of the best

performing model?

3.2.1. Motivation

Our observation in the previous research question (RQ1) is that the
most robust generic model Secbot (RoBERTa) outperforms domain-speciﬁc
BERTOverﬂow. Although improvement is subtle, the statistical signiﬁcance
in Table 6 suggests there are enough diﬀerences in both model’s predictions.
Our intuition behind this RQ is that lack of domain knowledge in Secbot
(RoBERTa) should have some impact on its performance. Thus, we perform
a qualitative study of the misclassiﬁcations of the best-performing model
(i.e., SecBot) to detect security aspects in IoT discussions. Proper reasoning
of misclassiﬁcation will strengthen our claim that the model performance is
not arbitrary. Moreover, this gives us leverage to conﬁne our model’s vul-
nerabilities in detecting security aspects in IoT domains. In the future, this
will provide a scope for the researcher to learn about the possible avenues
for improvement of the model.

3.2.2. Approach

In our benchmark of 5,919 samples, SecBot was wrong in 139 cases (false
positive or false negative). We analyzed these 139 samples from our IoT
security dataset as follows. We applied 10-fold cross-validation to our entire
dataset following steps similar to Section 3.1.2. As such, we were able to use

22

Infrequent/Unknown Keywords
(12.2%S 17S)

Ambiguous Context
(20.1%S 28S)

Code Parsing Issues
(5.8%S 8S)

Ambiguous Keywords
(37.4%S 58S)

Implicit Context
(17.3%S 24S)
Text Parsing Issues
(7.2%S 10S)

Figure 6: Distribution of SecBot error categories in the total 139 sentences (S) from IoT
Security dataset

the entire 5,919 sentences for training and testing. At the end of the 10th
iterations, we ﬁnd a total of 139 misclassiﬁcations in the IoT dataset by the
RoBERTa model used for SecBot. Then we manually analyze all the 139
sentences to determine the reason for misclassiﬁcation. The manual analy-
sis process consists of three stages. Both of the authors consulted together
(over Skype) on all 139 sentences to give a textual label that could explain
the reason for misclassiﬁcation. For example, the following sentence is er-
roneously labeled as 1 (i.e., contains security discussion) by SecBot “I have
attempted to set it so that it loads as the root user each time with no luck.”
After careful comparison of this sentence with other sentences in our IoT
samples, we determine that the misclassiﬁcation is due to the ambiguity in
context, which arose due to the words- ‘attempted to set’ and ‘root user’ in
the sentence. We label the misclassiﬁcation reason as ‘Ambiguous Context’;

3.2.3. Results

In Figure 6, we show the misclassiﬁcation categories in all 139 sentences
collected from the IoT Security dataset. Overall, we ﬁnd six error categories:
1. Ambiguous Keywords, 2. Ambiguous Context, 3. Implicit Context, 4. In-
frequent/Unknown Keywords, 5. Text Parsing Issues, and 6. Code Parsing
Issues. The most observed error category is ‘Ambiguous Keywords’, which
we observe in 37.4% of all 139 sentences (i.e., 58 sentences). The category
‘Ambiguous Context’ is observed the second most times (20.1% sentences),
followed by the category ‘Implicit Context’ (17.3% sentences), ‘Infrequen-

23

t/Unknown Keywords’ (12.2% sentences), ‘Text Parsing Issues’ (7.2% sen-
tences) and Code Parsing Issues (5.8% sentences), respectively. We discuss
the error categories below.

(1) Ambiguous Keywords. This type of error occurred due to the am-
biguity of a word in the dataset. We found two scenarios where this type of
error occurred. When a sentence oﬀers insuﬃcient context to detect security
concerns, thus, prediction mostly depends on the keywords present in that
sentence. In the worst-case scenario, these keywords are found in both se-
curity and non-security contexts during training. The model hardly guessed
correctly when this intriguing scenario occurred. For example, SecBot erro-
neously labeled the following sentence as 1, due to the ambiguous presence
of keywords like ‘signed’- “I’m not sure how to handle signed bytes.” It
got confused as ‘signed’ word associated with various security-related terms
(e.g., ‘signed device’, ‘self-signed cert’, ‘signed aws’) and also non-security
speciﬁc sentences (e.g. ‘signed bit’, ‘signed number’) as well. So, the model
erroneously predicted this programming-related non-security sentence as a
security-speciﬁc sentence. Secondly, this error occurred when a keyword was
predominantly associated with security-speciﬁc sentences during training and
pretraining phases. But in the erroneous cases, the keyword was found in a
non-security speciﬁc scenario or vice versa. Consider the sentence- “I am try-
ing to add the option to change the wi-ﬁ password, but that part of the form
does not seem to be available to the code. ” SecBot labeled this sentence as 1
(i.e., security), but the underlying scenario is about the conﬁguration of wiﬁ.
SecBot was confused due to the keywords ‘password’ and ‘change’ –which
are predominantly found in the security-speciﬁc sentences during training.

(2) Ambiguous Context. These errors occur when the underlying con-
text is too ambiguous to understand, i.e., security-speciﬁc. Such ambiguity
occurred because more information from surrounding sentences was needed.
Consider the following sentence: “I hope that someone here can help me, I
want to control my arduino uno by sending it commands from a c++ program
that performs some basic face recognition.” SecBot labeled the sentence as
1, while it is 0 in the manual label. It is because the context is about con-
trolling an IoT device that performs face recognition using some commands.
While the manual labeling phase considered the sentences non-security, the
model found it a possible security breach due to the context of controlling a
security device.

(3) Implicit Context. SecBot was unable to discern the underlying se-
curity context when it was implicit. Consider the following sentence: “The

24

main diﬀerence from software development standpoint of ios private api that
such apis aren’t declared in header ﬁles.” Here, the context is about ios pri-
vate API which can only accessed and used by Apple developers. The ele-
mentary context is that developers can not use any API without importing,
but implicitly it means that this API is restricted for non-Apple developers.
As such, SecBot labels it as 0, whereas the benchmark manual label is 1.

Sentences with implicit security contexts are diﬃcult to capture for a fully
automated system as domain knowledge experts can only bisect them from
other sentences.

(4) Infrequent/Unknown Keywords. This type of error occurred
when the model encountered a word that was not present during the train-
ing or was present so sporadically that it could not learn the word correctly.
But that word had profound signiﬁcance in understanding the sentence. For
example, the following sentence is labeled as 0 by SecBot: “The user should
have a iam access.”. Here, ‘iam access’– an AWS access token—is the key-
word to classify the sentence correctly. SecBot encountered this unknown
keyword during the testing phase and misclassiﬁed the sentence. Consider
another instance: “See the hid usage tables, v1.12 document for more infor-
mation about hid usage values.” The sentence contains a less frequent word,
‘hid’ is present. SecBot could not learn the weights for this keyword correctly,
and so it labeled the sentence as 0, where manual labeling considered it as 1.
(5) Text Parsing Issues. This error occurred due to the incompe-
tence of the model to parse the textual contents correctly in a sentence.
For example, SecBot erroneously labels the following sentence as 0, even
though it is about the enablement of secure socket layer (SSL) in the Rasp-
bian Stretch device: “The version of Openssl on Raspbian Stretch is 1.0.2,
but according to Openssl website, these versions should be binary compatible
(https://www.openssl.org/policies/releasestrat.html).” While SecBot knew from
training that SSL is related to the security aspect, SSL in this sentence is
denoted as Openssl, which is unknown to SecBot.

(6) Code Parsing Issues. These errors are very few compared to the
other categories. This error occurred due to the presence of code snippets
and inline code in the samples. For example, SecBot is unable to deter-
mine security threats related to input injection noted in the code block that
is provided as text in this sentence: “In Package tag add . . . In Capabil-
ities add . . . ¡rescap:Capability Name=“inputInjection” /¿¡rescap:Capability
Name=“inputInjectionBrokered” /¿”. This type of parsing error is inevitable
for automated models like SecBot because security-speciﬁc context can be

25

buried anywhere in a code snippet.

3.3. RQ3 What are topics in the IoT security discussions collected from the

entire IoT SO dataset using the best performing model?

We automatically detect security topics in the sentences labeled as security-

speciﬁc by SecBot in our IoT dataset.

3.3.1. Motivation

The rapid growth and usage of IoT devices, tools, and technologies have
become prominent threats to data protection, security, and privacy.
IoT
developers encounter various security challenges while using these devices
and tools. It is vital to understand those challenges in order to improve and
design more secure devices and tools. So, the understanding of topics in the
SO IoT security discussion can be helpful to infer those challenges.

3.3.2. Approach

First, we apply SecBot on the entire 672,678 sentences that we collected
from our 53K IoT posts (Section 2.2). SecBot returned 30,595 sentences as 1,
i.e., those containing security discussions. Second, we preprocess this dataset
as follows. We removed stopwords, extreme words (e.g., words with less than
20 frequencies), most frequent words (e.g., words with more than 2000 fre-
quencies), short tokens, and letter accents. The stopwords are collected from
NLTK [52]. We applied lemmatization to sentences to get root form of each
word. Third, we fed the preprocessed sentences into the topic modeling algo-
rithm Latent Dirichlet Allocation (LDA) [20] available via Gensim [60]. LDA
takes a set of texts as input and outputs a list of topics to group the texts into
K topics. This K is a hyperparameter of the algorithm. To get the optimal
value of K, we used standard practice as originally proposed by Arun at el[7].
This approach suggests that the optimal number of topics has the highest
coherence among them, i.e., the more coherent the topics, the better they can
encapsulate the underlying concepts. To determine this coherence metric, we
use the standard c v metric as originally proposed by R¨oder et al. [61]. We
use the topic coherence metric as available in Gensim package [60]. To de-
termine our optimal number of topics, we use grid search technique. We ﬁst
select a set of values for K. Then we run our LDA algorithm for each value
of K and measured the coherence metric. Finally, we choose the value of K
that is associated with the highest coherence score. In our experiment, we
set K = 1,4,8,9, 10,11, 12, 16,20,24,28,32, 36, 40 and got maximum coherence

26

Hardware
(7.9%S 1T)

Network
(34.8%S 3T)

Software
(57.3%S 5T)

Figure 7: Distribution of sentences (S) and topics (T) per topic category

for K = 9. Thus, we picked 9 as our optimal number of topics. LDA also
uses two hyperparameters, α and β to control the distribution of words in
[18].
topics and the distribution of topics in posts following Biggers et al.
Following previous work [14, 11, 3, 18, 63, 89], we thus use standard values
50/K and 0.01 for the two hyper parameters. Fourth, we label each topic a
name by manually analyzing a random sample of at least 50 sentences per
topic. We do not limit ourselves to a ﬁnite number of sentences per topic
during the labeling to ensure that the assigned label is reliable. The ﬁrst
and second authors virtually meet to label the topic manually. First, both
authors review a number of sentences for a topic and come up with possible
topic labels. Then both authors share their thoughts on the possible topic
labels. If both authors have the same thoughts, they label the topic with
a suitable title; otherwise, they go over the sentences again and label the
topics together. For example, the following sentence is about the certiﬁcate
validation for diﬀerent users, “. . . Found the answer as I was trying to import
the certiﬁcate with the Root and was testing with a diﬀerent user.” Another
sentence assigned to the same topic is “ . . . The commands where I input the
SSID and Password are the ones causing trouble.”. We thus label the topic as
‘User/Device Authorization’. Fifth, we assign each topic a category, where
a category denotes a higher order grouping of the topics. For example, we
assign the topics ‘User/Device Authorization’ and ‘Crypto/Encryption Sup-
port’ to the category ‘Software’, because the topics contain discussion about
the support of software in IoT devices to ensure security. The topic ‘Se-
cure Transaction’ is assigned to the category ‘Network, because it contains
discussions about the securing of transactions/communications between IoT
devices/services over the network.

27

Table 7: Distribution of IoT Security Topics by Total #of Sentences

Category Topic Name

# of Sentences Distribution(%)

Hardware

IoT Device Conﬁg./Set-up

Software

Storage/Database Management
Vulnerability/Attack
Framework/SDK-based Security
User/Device Authorization
Crypto/Encryption Support

Network

Secure Transaction
Secure Connection Conﬁg.
IoT Hub Federation

2402

4463
3808
3169
3101
3004

3855
3648
3145

7.9

14.6
12.4
10.4
10.1
9.8

12.6
11.9
10.3

3.3.3. Results

We found 9 IoT security-related topics, which are grouped into three
high-level categories: Software, Network, and Hardware. Figure 7 shows
the distribution of these three high-level categories. The Software category
contains the greatest number of topics (5) and covers more than 57% of
all sentences.
In Table 7, we show the 9 topics based on the distribution
of sentences in IoT security dataset. The distribution column indicates the
percentage of sentences in our IoT security dataset that cover this topic.
The topic ‘IoT Device Conﬁg’ from the Hardware category covers the low-
est number of sentences among all topics (2402 sentences, around 7.9%). It
contains discussions of programming and conﬁguration challenges IoT de-
velopers face while using Bluetooth/BLE/Microchip interfaces in their IoT
devices to setup a service (e.g., setting up a barcode scanner in rpi2 to verify
coupon Q30721018). Qi denotes a question with an ID i in SO. The topic ‘Stor-
age/Database Management’ from the Software category covers the highest
number of sentences (4463 sentences, around 14.6%). This topic contains dis-
cussions about developers’ problems to database set up, conﬁguration (e.g.
Q30073381), and attack like SQL Injection (e.g. Q6826176), Weak Authentica-
tion(e.g. Q5503812), Privilege abuse(e.g. Q39232652), and etc.

The other four topics in the software category are: 1. Mitigation tech-
niques of vulnerabilities/attacks in IoT devices (3808 sentences, around 12.4%),
2. Discussions about the vulnerability of speciﬁc approaches in frameworks
(3169 sentences, around 10.4%), 3. Authorization of users and IoT devices
(3101 sentences, around 10.1%), and 4. Documentation and usages of Cryp-

28

tography/Encryption support (3004 sentences, around 9.8%).

The Network category has the second highest number of topics (3) and
it covers 34.8% of all sentences. Two of the three topics under the Network
category are also among the top four topics (each with around 12% of sen-
tences). The ﬁrst two topics are ‘Secure Transaction’ and ‘Secure Connec-
tion Conﬁguration’, which contain discussions about ensuring security over
secure payments and secure copy/transfer of digital ﬁles via/among IoT de-
vices. The last network topic is ’IoT Hub Federation’. Questions in this
topic concern the cloud-based setup of IoT devices, e.g., using Azure/AWS/-
Google cloud. An IoT hub is a managed service in the cloud that acts as a
central messaging service to allow bi-directional communication between an
IoT application and the smart devices it manages (e.g., connect to MQTT
with authenticated Cognito credential Q51093154).

3.4. RQ4 How do the IoT security topics evolve in SO?
3.4.1. Motivation

We investigate how the popularity of the three IoT security topic cate-
gories has changed over time by determining the total number of new sen-
tences created per topic category over time. A new sentence can only be
created if a new post is created. Therefore, the evolution of the topics over
time oﬀers us information about the relative popularity of the categories and
whether a category is discussed more over time than the other categories.

3.4.2. Approach

For each of the 30K security-related sentences in our dataset, we identify
the ID of the SO post from where the sentence is detected. We consider the
creation time of the SO post to be the sentence creation time. We group the
30K sentences by the three topic categories. For each category, we compute
the total number of sentences created every six months based on the creation
time. Following Wan et al. [86], we report the evolution of IoT security topics
using the metric Topic Absolute Impact. Topic absolute impact is calculated
by determining the total number of sentences created per the 9 IoT topics
every six months since 2008. We do this by ﬁrst assigning each sentence to
its most dominant topic, i.e., the topic with which the sentence shows the
highest correlation score. We then assign the creation time to a sentence as
the creation time of the post where the sentence is found. Finally, for each
topic, we put the sentences into time-buckets, where each bucket contains
all sentences created within a time window (e.g., from Jan 2009 till June

29

Figure 8: #New sentences per IoT security topic category each month

2009, etc.). We further analyze the absolute growth to correlate the growth
in real world contexts for each topic. It is possible that developers might be
inﬂuenced by the release of new IoT APIs, tools, or devices. As such, we
ﬁgure out the impact of those real-world contexts in our developer discussion
in SO. We ﬁrst identify some major releases in the IoT domain during the
period from 2009 to 2019. Then, we ﬁnd all the spikes in our absolute
growth chart for each topic. Next, we collect all time frames of six months
(e.g., January 2010 to July 2010) in which the spikes took place. To identify
related real-world events (i.e., major updates and releases), we identify all
major updates and releases during that time frame for each topic. Finally,
we manually check the availability of discussions for all identiﬁed events in
our IoT security dataset during that time frame. Based on that, we either
count an event as a striking contributor to IoT discussions or discard the
event. For example, during the time interval between January 2014 and July
2014, both ‘vue.js’ and ‘Riot.js’ were released. However, we ﬁnd that our
IoT dataset has enough discussion for ‘Riot.js’ only during January 2014 and
July 2014. As a result, we discard ‘vue.js’ and count ‘Riot.js’ as a striking
event.

3.4.3. Results

Figure 8 shows the absolute growth of the three IoT security topic cate-
gories over time by showing the number of new sentences per category every

30

Intel Galelio ReleaseRaspberry Pi 2 ReleaseArduino Uno Wifi rev 2 ReleaseIBM WatsonReleaseAWS IoT ReleaseFirebase ReleaseRiot.js ReleaseDjango 1.2.6 ReleaseKali Linux 2016 Gnome ReleaseDjango 1.10.8  Releasesix months. While the software category has the most new sentences among
the three categories, the network category started to catch up starting from
2017. The number of discussions under Software shows an upswing between
January 2011 and July 2011, when Django 1.2.6, a free open-source web
framework that encourages rapid development, was launched. Some web
security tools (e.g., ‘XSS’, ‘memory-cache-backed session’ and ‘CSRF bto-
ken’) were ﬁrst introduced in this release. Many discussions in 2014 in our
dataset are related to Riot.js. Similarly, the release of Kali Linux in 2006 and
Django 1.10.8 gave a spike in software related security discussions. Overall,
the periods between June 2011 and June 2014 showcase the rapid adoption
of IoT-based SDKs/framekwors (e.g., rpi2 versions). IoT security concerns
are raised at the same time due to the emergence of ransomware like Cryp-
toLokcer and banking Trojans.

At the beginning of 2010, the network topic category got high priority
due to internet connectivity and cloud storage. A high spike was found at
that time. Firebase, a real-time cloud database suited for IoT applications,
was released in May 2012. As such, we found many related security discus-
sions across the Network category during this time. Similarly, we found the
release of IoT hubs like AWS IoT in late 2015 and IBM Watson IoT in 2017
contributed to many network-related discussions during the respective peri-
ods. Recently, the emergence of online transactions and IoT cloud storage
solutions has also contributed to the increased popularity of the network
category (#new sentences).

Hardware related discussions showed continuous growth over the years.
Many hardware supported security modules like ﬁngerprint locks, smart
doors, biometric devices, etc., have emerged during our study period. We
found the release of popular open-source IoT hardware has increased the rate
of discussion about that hardware. For example, the release of Intel Galelio
back in mid-2012 produced a spike between June 2013 and December 2013.
Similarly, after the release of ‘Rasberry Pi 2’ in February 2016 and ‘Arduino
Uno Wiﬁ rev 2’ in February 2017, spikes in the curve were visible.

In conclusion, the evolution of IoT security topic categories and subse-

quent topics can provide insight into new, emerging, or existing threats.

4. Discussion

For our experiments, we use SO data dumps up to September 2019. How-
ever, we also investigate the most recent data dumps of SO. We use the online

31

Stack Exchange data archive to get the most recent SO data dump, which
was for January 2022. We collect IoT datasets from this data dump by fol-
lowing all the steps we described in Section 2.2. We found that many SO
posts related to IoT were deleted by SO in the January 2022 data dump
that were previously available in the September 2019 data dump. As such,
we ﬁrst analyze the extent of such deleted posts in the January 2022 dump
(in Section 4.1). We then compare the results of our empirical studies for
RQ3 (recall Section 3.3) and RQ4 (recall Section 3.4) between the two data
dumps, i.e., Sept 2019 and Jan 2022 in Sections 4.2, 4.3.

4.1. The “Case” of Post Deletion by Stack Overﬂow

As per the data dumps of September 2019, there are around 53K IoT
related posts, whereas the most recent data dumps (January 2022) have only
45K IoT related posts. This indicates that most recent data dumps have
fewer IoT-related posts even though our trend charts from Figure 8 based on
Sept 2019 data dump showed that IoT-related discussions have an upward
trend in recent times. We investigate this drop in the number of posts in
Jan 2022 data dumps. We identiﬁed all the missing posts that are available
in the September 2019 data dumps. Then, we check the recent activity
and status of these missing posts. We ﬁnd that these posts were deleted in
between September 2019 and Jan 2022. As a result, these posts appear in
the 2019 data dumps but do not appear in recent data dumps. For example,
‘ESP32’ tagged SO question Q56098708 was created in May 2019, but later it
was deleted. We further apply Secbot to the new IoT data and ﬁnd only 6K
security-related sentences. Compared to the 30K security-related discussion
in RQ3(Section 3.3), these results are quite low. This is because the size of the
dataset is smaller and most of the posts related to the security discussions are
removed. For example, an encryption-related post, Q11501569, that contains
multiple security-related discussions, has been removed in recent data dumps.

4.2. Prevalence of IoT Security Topics After 2019

Following the RQ3.3, we apply topic modeling on 6k sentences we got
from the Jan 2022 data dump. We ﬁnd that the optimal number of topics
for this dataset is 6. Then we manually label these six topics. The top-
ics are:
‘IoT Device Conﬁg/Set-up’, ‘Secure Connection’, ‘Secure Transac-
tion’, ‘Framework/SDK-based Security’, ‘Authentication Management’, and
‘Crypto/Encryption Support’. We further investigate how these topics diﬀer

32

Figure 9: Topic distribution in February 2022 SO dumps and September 2019 SO dumps

from previously found topics in RQ3 (Section 3.3). Although ﬁve topics are
present in our previous ﬁndings, ‘Authentication Management’ is diﬀerent
from what we found previously in ‘User/Device Authentication’.
‘Authen-
tication Management’ is comprised of discussion related to authentication
mechanisms, devices, techniques, protocols, implementation supports, etc.,
whereas ‘User/Device Authentication’ only consists of user or device level
authentication. On the contrary, we didn’t ﬁnd any discussion related to
‘Storage/Database Management’, ‘Vulnerability/Attacks’, or ‘IoT Hub Fed-
eration’. This happens because of the removed posts in recent data dumps.
We further categorize these topics into Software, Hardware, and Network
group. We ﬁnd 3 software-related topics (i.e., ‘Framework/SDK-based Se-
curity’, ‘Crypto/Encryption Support’, and ‘Authentication Management’),
2 network-related topics (i.e., ‘Secure Connection’ and ‘Secure Transaction’
), and a hardware related topic (i.e., ‘IoT Device Conﬁg/Set-up’). There
are 22.3% hardware, 37.2% network, and 40.5% software related security-
discussions in the February 2022 data dumps. Finally, we compare the topic
distribution in the most recent data dumps (i.e., February 2022) and the
September 2019 data dumps. We show the comparison in Figure 9. The rel-
ative percentage of hardware-related discussions is higher in February 2022
data dumps than in September 2019 data dumps. Although the network
topic holds similar percentages in both data dumps, the software topic has
a lower value in the February 2022 data dumps. This is because most of

33

0.00%20.00%40.00%60.00%HardwareNetworkSoftwareFebruary 2022 Data DumpsSeptember 2019 Data DumpsFigure 10: Absolute growth of each IoT security topic category in each month for most
recent data dumps (February 2022)

the software-security related IoT posts were removed in the February 2022
data dumps, and in recent times, developers are discussing hardware-security
more than they did in past times.

4.3. Evolution of IoT Security Topics

We analyze how the topics found in the Jan 2022 data dump evolved over
the time period. We ﬁrst calculate the absolute growth of the topics following
RQ4 (Section 3.4). Then we generate a trend chart showing how the three
topics (i.e., Hardware, Software, and Network) evolve over the time period
between 2011 and 2022. We report the trend chart in Figure 10. From this
chart, we ﬁnd that security related discussions follow an upward trend for all
three topics (i.e., hardware, software, and network) before 2019, similar to
our previous ﬁndings in Section 3.4. However, there is a downward slope after
2019 for all three topics. One of the possible reasons for this downward swing
could be the lack of releases of any software, hardware, and network related
devices, tools, technologies, etc. during this period. Another contradictory
ﬁnding is that network and software related discussions have almost similar

34

# of Sentences050100150Jan-12Jan-14Jan-16Jan-18Jan-20Jan-22HardwareSoftwareNetwork8% Found
out of 30K
sentences using
frequently used 10 security tags

92% Missing

Figure 11: IoT security sentences dicoverability via frequently used SO security tags

numbers, but our previous ﬁnding says there are more software-related dis-
cussions than network ones. This could be because of the removed posts in
the February 2022 dumps.

4.4. Discoverability of IoT security sentences

Our study data is collected from SO, by identifying questions in SO la-
beled as one of the 75 IoT related tags. As we noted in Section 2.2, the 75
tags were previously used to learn IoT developer discussions in SO by Uddin
et al. [84], who found that the most popular and most frequently discussed
topics were related to the adoption of security measures in IoT devices (e.g.,
secure messaging). In addition, the same dataset was also used by Uddin [79]
to determine the prevalence of security-related discussions at the sentence
level. Both studies show that the dataset contains diverse discussions about
security for IoT. Note that not all the 75 IoT tags have explicit reference
to any security measures, but we observed that many of the questions (and
their answers) labeled as the tags contained security discussions. In this sec-
tion, we oﬀer quantitative evidence on the extent of security discussions in
questions labeled as non-security tags as follows.

We investigate whether SO security-speciﬁc tags cover all IoT security
sentences. Thus, we pick 10 frequently used security-related tags, i.e., ‘secu-
rity’, ‘ssh’, ‘ssl’, ‘passwords’, ‘authentication’, ‘authorization’, ‘encryption’,
‘cryptography’, and ‘hash’ in SO. Then, we collect all posts from our 53K

35

IoT posts with at least one of the tags. In the collected posts, we only man-
age to ﬁnd 8% of 30K sentences, i.e., 92% of our IoT security sentences are
not discoverable using the 10 tags (see Figure 11). We also investigate the
missing posts in the latest data dumps (January 2022) using these 10 se-
curity tags. The result shows the numbers are similar, only 10% posts can
be discovered with these 10 tags in 2022 data dumps. This indicates that
SO security tags do not cover all security discussions. However, our security
discussion includes sentences that are tagged with security tags.

5. Implications of Findings

The ﬁndings from our study can guide the following major stakeholders

in the rapidly emerging IoT ecosystems:

1. IoT Security Enthusiasts to learn about IoT security aspects from

IoT developer discussions,

2. IoT Vendors to oﬀer tools and techniques to support security in IoT

devices,

3. IoT Developers to determine the current trends in IoT security based
on developer discussions and to use the information to guide future
development needs,

4. IoT Educators to develop tutorials and documentation to educate

security principles to IoT practitioners, and

5. IoT Researchers to improve the detection of IoT security aspect in de-
veloper discussions and to study how to properly address the diﬃculty
in the adoption of security practices for IoT devices and techniques.

We discuss the implications below.

IoT Security Enthusiasts. IoT enthusiasts can be IoT practitioners or
simply IoT market watchers, but they nevertheless form a crucial part of the
IoT ecosystem in various ways (e.g., buying/analyzing IoT devices/solutions,
etc.). The development and adoption of IoT-based solutions by developers is
facilitated by the exponential growth of IoT devices, software, and platforms.
Our increasingly interconnected digital world relies on smart devices built
using the IoT, which means security for IoT devices is paramount and it
is the job of IoT developers to adopt proper security measures for their

36

Figure 12: Relative Growth of security related sentences over time

solutions. Indeed, if we compare the number of new security sentences per
month to all sentences in the month in our 53K SO posts, we observe a slow
but relative growth of IoT security-related discussions, especially after 2016
(see Figure 12). This means that interest in IoT security is increasing among
IoT developers in SO. Therefore, IoT security enthusiasts can beneﬁt from
the security discussions by IoT developers in SO. As we noted in Section 1
(Figure 1), security discussions in SO IoT posts can be buried inside general
or non-security IoT discussions. Therefore, to oﬀer security-speciﬁc insights
from SO IoT discussions, we need tools to automatically detect those. The
high performance of our developed tool, SecBot, is suited to meet the needs.
IoT Vendors. IoT vendors need to support IoT developers with proper
and usable secure IoT techniques. To determine what is appropriate and
what is working, though, the vendors need to know the problems faced by
IoT developers. Forums like SO have become a go-to place to look for solu-
tions to their technical problems. Thus it has become essential for the IoT
vendors to keep an eye on this open discussion forum. Our research can help
them in two ways. First, the high precision and recall of our tool SecBot

37

Growth(%)t
e
s
a
t
a
d
T
o
I

r
u
o

n
i

s
c
i
p
o
t

y
t
i
r
u
c
e
s
T
o
I

f
o

y
t
l

u
c
ﬃ
d

i

d
n
a

y
t
i
r
a
l

u
p
o
p

n
e
e
w
t
e
b

ﬀ
o
e
d
a
r
T

:
3
1

e
r
u
g
i
F

38

Storage/Database ManagementIoT Vulnerability/AttackSecure TransactionUser/Device AuthorizationSecure Connection ConfigIoT Device ConfigIoT Hub FederationFramework/SDK Based SecurityCrypto/EncryptionSupport7509501150135015501750195012%14%16%18%20%22%24%26%28%30%Popularity  of Posts with IoT Security TopicsAverage View Count of Questions Per IoT TopicDifficulty of Posts with IoT Security Topics (% questions withount an accepted answer)will help IoT vendors reliably ﬁnd security-related issues in SO. They can
analyse those discussions and take a decision accordingly. Second, our ﬁnd-
ings from RQ4 (Section 3.4) show that IoT developers are aware of major
releases and immediately discuss them more frequently. Thus, we can say
the vendors can get quick feedback from the developers about their most
recent release of any IoT device, tools, techniques, or software. This could
play a vital role in their upcoming releases. For example, if there are multiple
bugs or issues, they can ﬁx those bugs in their next release or oﬀer a quick
release ﬁxing those bugs. Overall, such insights can be crucial for vendors
to improve their oﬀerings and to compare the solutions of their competitors.
Hence, they could be well ahead of their competitors.

In Figure 13, we show a bubble chart to show the tradeoﬀ between the
popualrity vs diﬃculty of IoT security posts. The chart is constructed as
follows. Out of the 30K IoT security sentences in our dataset, we pick the
sentences that we ﬁnd in the SO question posts. For each of the 9 IoT secu-
rity topics, we pick the sentences that we ﬁnd in the SO question posts. For
the total number of distinct questions thus found for a given IoT security
topic, we analyze two metrics as follows: (1) Popularity analysis based on
the average view counts of the associated questions to the topic. (2) Dif-
ﬁculty analysis to get an accepted answer to a question by computing the
percentage of question per topic that remain without an accepted answer.
Both of the metrics have been used previously in literature to determine the
popularity and diﬃculty of topics generated from SO posts. The x-axis in
Figure 13 shows the diﬃculty score of each IoT security topic, and the y-axis
shows the popularity score of the IoT security posts. The size of each topic
bubble is based on the number of distinct questions assigned to the topic.
Therefore, the more right a topic in the chart, the more diﬃcult it is in SO.
Topics like ‘Framework/SDK Based Security’, and ‘Storage/Database Man-
agement’, and ‘IoT Device Conﬁg’ are among the most diﬃcult. The topic
‘IoT Device Conﬁg’ is among the most diﬃcult as well as the most popular
IoT topics. Therefore, IoT vendors can accelerate the adoption of security in
IoT Microschip and Bluetooth (e.g., BLE) devices, which will then increase
the chance of getting answers to the problems.

IoT Developers. The security of IoT devices and solutions is of paramount

importance. As the hacker community is looking for the single opportunity
to breach the IoT system or device, the developers should not consider ig-
noring any corner of security vulnerability which can be exploited. We thus
need IoT developers to stay aware of the IoT security trends and to adopt

39

prevalent IoT tools. The developer community is one of the best places that
could help them keep pace with the cutting edge technologies that can’t be
breached, at least for that time frame. Such insights can beneﬁt from such
precise, speciﬁc, but automatically mined security knowledge to stay aware
of the trends related to IoT security. In addition, such knowledge can also
help them make better decisions, like picking a popular IoT security tech-
nique over another. For example, in Figure 13, we ﬁnd that the topic ‘IoT
Hub Fedearation’ is the least diﬃcult, while also not very popular. This
means that IoT developers leveraging cloud infrastructure to implement se-
curity methods for their IoT devices are not ﬁnding enough support from SO
developers. The topic ‘Secure Transaction’ between IoT devices or between
IoT and non-IoT devices contain a large number of questions, which are also
popular. This means that IoT developers can devote time to learn and imple-
ment secure transactions using their IoT devices, and they can also inquire
with other IoT developers in SO about the problems they face.

The developer community cannot rely on new technologies every time,
as new technologies can be challenging, complex, or incompatible with the
current environment. For example, a developer asks how he can create a dig-
ital signature function PDF from Hardware Security Modules (HSM) using
C# in SO question Q54922093. However, the discussion section suggests that
he can’t use HSM directly in C# instead he needs to go through ‘CAPI’ or
‘CNG’ api or ‘ncryptoki’. Developers could face these challenges in the mid-
dle of any project, which could cost more time or, at worst, they could end up
switching to another framework or tool. This scenario can be avoided if de-
velopers gather the necessary information about the required apis, devices, or
software. Developer forum like SO is the ideal place to collect those informa-
tion. Our research provides information about IoT security topics. Further
mining of their discussed topics such as which features they are discussing,
what are the sentiment of those discussion, etc. will be helpful for everyone
to learn about an api, tools, frameworks, software, or devices. From this in-
formative feedback, the developer community can decide whether they adopt
the new technology or not. Thus, they can select their developing tools and
frameworks wisely and minimize the risk of a project.

IoT Security Educators. Our research ﬁndings are more promising for
IoT educators in many ways. From RQ4 (Section 3.4), we learn that IoT
topics are getting more attention and thus the responsibility of the IoT edu-
cators is also getting escalated. As the IoT security demands are increasing,
the educator must prepare quality contents to motivate the newcomers and

40

to enlight current practitioners. Thus, to create content, they can follow the
topics we ﬁnd in RQ3 (Section 3.3). They should also be quite selective in
selecting the most recent techniques and tools over the old ones. Otherwise,
the learners will lack the new tools, which will eventually end up going back-
ward instead of forward. SO can be a helping hand here for them. As SO
contains IoT security discussions about all those 9 topics, they can collect
that data to analyse and prepare their lessons. Moreover, as we previously
presented, one can ﬁnd enough discussions related to the most recent releases
of any IoT security tools, software, and devices. The educator can also pro-
vide these insights regarding the most recent IoT tools, software, and devices
into their lesson. This will also be beneﬁcial for the learner.

IoT security educators can use the bubble chart in Figure 13 to prior-
itize their eﬀorts to develop security tutorials and documentation for IoT
developers and practitioners. For example, the topic ‘Secure Connection
Conﬁg’ has the highest number of questions. It is also among the most pop-
ular topics, while also less diﬃcult than most topics in terms of getting an
accepted answer. Therefore, IoT security educators can analyze the ques-
tions related to user/device authorization in SO to develop comprehensive
tutorials for developers. The topic ‘Crypto/Encryption Support’ has the sec-
ond most questions, while it is the most popular yet one of the most diﬃcult
topics. Therefore, the IoT security educators can analyze the questions in
SO and consult IoT resources to ensure that IoT developers can learn about
the risks associated with leaving IoT ports open and how they can conﬁgure
them properly to ensure security.

IoT Researchers This research creates a vast space for the researcher to
go forward in multiple dimensions. Security aspect detection is a major task
that we accomplish in this paper. However, this task requires more attention
in the future. We ﬁnd our Secbot has a maximum performance of 0.935,
which seems like a good result considering no prior security detector has
achieved that far. But when we ﬁgure out the errors made by our Secbot, we
ﬁnd multiple implicit contexts, ambiguous contexts, and ambiguous keywords
related errors. These errors can further be researched to extract informative
information like the identiﬁcation of key phrases or words that cause ambigu-
ity. Individual research can be conducted on how these key words or phases
can be detected automatically. In another way, these error categories give us
a way of improving the performance of Secbot. The researcher can approach
this in three ways. First, they can add IoT contexts to Secbot. For example,
they can use some rules based patterns to extract information and then feed

41

it to the Secbot. Second, they can add more samples to reduce the sparsity
of security-aspects in the dataset. For this, one may consider only adding
the samples that are close to those error category types. They can even try
to make a balanced dataset to see how the performance changes. Third, they
can try a deep model that can only discern samples related to those error
categories, and then they can apply ensemble Secbot and the deep model.

Besides this, we ﬁnd that domain-speciﬁc BERTOverﬂow fails to perform
as well as the generic model Secbot. Additionally, the correlation coeﬃcient
score from Table 6 suggests that the results of BERTOverﬂow vary from the
results of Secbot. This could be an interesting topic to delve deeply into.
The researcher can explore the misclassiﬁcation made by BERTOverﬂow to
compare the results between Secbot and BERTOverﬂow. If there is a sig-
niﬁcant diﬀerence in error categories between these two models, they can
consider developing an ensemble model. Another direction of research explo-
ration could be developing a purely domain-based security aspect detector.
Although BERTOverﬂow has knowledge about StackOverﬂow, compared to
its overall knowledge, SO knowledge is not suﬃcient to represent it as an
intelligent SO expert. Thus, an entirely SO knowledge-based model may be
more suitable in security-aspect detection.

IoT researchers can analyze the security discussions to learn about the
speciﬁc challenges that IoT developers are facing based on their real-world
experience. Such insights can be useful for researchers to invent new tech-
niques and tools for IoT security. It is important that research in IoT security
can be inﬂuenced by the emerging trends in IoT security. As we observed in
Section 3.4 (Figure 8), we see an almost equal number of sentences per the
the two topic categories (Software and Network) starting from January 2017.
This means that security research in the IoT needs to put equal emphasis
on both software and network security. Similarly, discussions about secured
hardware for IoT devices are also increasing over time, although not as much
as the software and network topics.
IoT security researchers need to de-
sign and develop innovative techniques to secure IoT software and networks.
One of the 9 topics is ‘Vulnerability/Attack Concerns’ in IoT devices, which
points to the issues IoT developers are facing with regards to addressing spe-
ciﬁc vulnerabilities in their IoT-based solutions. As we ﬁnd in Figure 13, this
topic is also considerably popular and diﬃcult. Therefore, IoT software and
hardware security researchers can use the discussions to develop tools and
techniques. Data management and storage are subject to the topic ‘Secure
Data Management’, which can beneﬁt from research in database security.

42

Additionally, the research community can conduct rigorous research on
IoT security topics such as what types of questions developers asked in each
topic category, how the new release aﬀects the developers, what factors de-
velopers discussed most about any new release, etc.

6. Threats to Validity

Internal validity threats relate to the authors’ bias while conducting
the analysis. We mitigated the bias in our benchmark creation process and
topic labeling processes by computing agreements (security sentences) and
labeling together (topic modeling). During the topic labeling, the authors
communicated over Skype and using Google drive to reduce individual bi-
ases. The agreement between the two coders is above 95% all the time. We
use a standard random sampling technique to reduce locality biases in our
benchmark dataset. The machine learning models are trained, tested, and
reported using standard practices. There was no common data between the
training and test sets. We shuﬄe the training and testing datasets in each it-
eration to introduce a new scenario every time. We perform cross-validation
on the entire dataset to make sure our assessment is not biased towards any
subsets. We follow standard hyperparameter tuning to minimize the eﬀects
of hyperparameters on the ﬁnal results.

Construct validity threats relate to the selection and creation of IoT
security dataset. As we include all tags related to IoT tags, security-related
tags are also present there. However, there may be some security tags that
have not been included in the dataset creation process. In the future, SO
may include more IoT and security tags, which could challenge our dataset.
However, as we discussed in Section 4.4, we observed that IoT developers in
SO did not constrain their security-related discussions in SO to only questions
labeled as security-related tags.
In fact, more than 90% of our security-
related sentences are not covered by the security-speciﬁc tags in our dataset.
As such, in Section 5, we discussed the implications of our automated tool
to detect security-related sentences in SO, which could ﬁnd such sentences
even in SO IoT tags that do not explicitly refer to any security issues. Beside
this, construction validity threats relate to the diﬃculty of ﬁnding data to
create our IoT security-related sentences. Our benchmark creation process
was exhaustive, as we processed more than 53K posts from SO. The evolution
of IoT topic categories considers post-creation time as sentence creation time.
A post can be edited after its creation with new sentences, but we observed

43

less than 1% of such cases in our dataset. We ﬁnd security-speciﬁc 30K
sentences out of 672K sentences in the IoT dataset using the best performing
SecBot model. During the topic modeling phase, we observed only a few
non-security related sentences. Further investigation can be carried on. This
may give us more insights about the model, which we leave as our future
task. In addition, we select a topic for each sentence based on the higher
coherence score of the topics. During the topic labeling step, we label each
topic by analyzing the topic top 30 words and posts associated with the label.
This approach is consistent with the related work that also analyzed topics
in SO posts [12, 2].

External validity threats relate to the generalizability of our ﬁndings.
Our developed model, SecBot, demonstrates high accuracy. As such, the
model is expected to oﬀer good performance for other developer forums.
There is a possibility that IoT developers may discuss diﬀerent types of se-
curity discussions in other forums. For example, we found that most of the
IoT developers discuss Database/Storage related security in SO. It is possible
that in other forums, they discuss more about encryption. A detailed eval-
uation of SecBot in other forum data is our future work. In summary, our
model exhibits good performance in this research direction, but the results
should not be taken as an automatic implication of the same result in general.
An extensive analysis of the diverse nature of challenges and characteristics
can validate the transposition of the results to other domains. Besides this,
SecBot is designed to work on textual datasets. However, SO contains code-
snippets, logging, and urls which are ﬁltered out during our dataset creation.
Therefore, if SecBot is applied to SO posts containing source codes, logging,
or urls, the model’s performance may drop.

7. Related Work

Related work can broadly be divided into Studies to understand and

Techniques to detect/mitigate IoT security issues.
Studies. Literature in IoT so far has focused on surveys of IoT techniques
and architectures [65, 4], the underlying middleware solutions (e.g., Hub) [25],
the use of big data analytics to make smarter devices [50], the design of secure
protocols and techniques [4, 44, 93] and their applications on diverse domains
(e.g., eHealth [51]), the Industrial adoption of IoT [47], and the evolution and
visions related to IoT technologies [57, 66]. The unauthorized inference of
sensitive information from/among IoT devices is a prevalent concern[22].

44

We are aware of no previous research that focused on understanding IoT
security discussions in SO. In SE, topic modeling is used to learn aspects like
software logging [46], feature location [29, 56], traceability linking [59, 8], soft-
ware and source code evolution [41, 76, 75], source code categorization [77],
code refactoring [16], defect analysis [27], and various software maintenance
tasks [69, 70]. The SO posts are subject to topic modeling to understand
concurrency [3], big data [11] and chatbot issues [2]. Yang et al. [89] studied
security-topics in SO. They used SO tags to identify mobile security which
might miss many security discussions which were not tagged as any security
related tags. Our studies resolve this issue by following Uddin et al.
[84]
to collect all IoT posts. Next, they applied topic modeling to the security
posts. As the recent studies have found that larger documents may have
multiple topics, LDA topic modeling is unstable for such cases [26]. We thus
use sentence level topic modeling. We develop a precise security detector,
SecBot, to identify security related sentences from SO posts and apply LDA
topic modeling on our security dataset. Unlike Yang et al. [89], we focus on
IoT security topics.
Techniques. IoT devices can be easy target for cyber threats [93, 34]. As
such, signiﬁcant research eﬀorts are underway to improve IoT security. Au-
tomated IoT security and safety measures are studied in Soteria [23], IoT-
Guard [24]. Encryption and hashing technologies make communication more
secure and certiﬁed [74]. Many authorization techniques for IoT are proposed
like SmartAuth [78]. For smart home security, IoT security techniques are
proposed like Piano [36], smart authentication [39], and cross-App Interfer-
ence threat mitigation [28]. Session management and token veriﬁcation are
used in web security to prevent intruder getting information. Attacks on
Zigbee, an IEEE speciﬁcation used to support interoperability can make IoT
devices vulnerable [62]. Smart gateway for IoT is proposed to tackle mali-
cious attack [42]. To the best of our knowledge, our developed SecBot+ is the
ﬁrst DL model that can be used to automatically detect IoT security-related
developers discussions. IoT researchers can gain insights to oﬀer increased
support/security for a problematic IoT device as observed in the developer
discussions.

8. Conclusions

The rapid adoption of IoT-based solutions has raised concerns about the
security of IoT devices and communications. As such, it is important to

45

understand the problems developers discuss when discussing their usage of
IoT security tools and techniques in online technical forums like SO. With a
view to automatically detecting developers’ security discussions at the gran-
ularity level of sentences, in this paper, we have investigated a total of ﬁve
advanced pre-trained language-based deep learning techniques (e.g., BERT,
RoBERTa). The best-performing model, SectBot, based on RoBERTa, oﬀers
an F1-score of 0.935 to detect IoT security discussions. We use SecBot to
automatically mine all the 30K IoT security-related sentences from the 53K
IoT posts from SO. We apply topic modeling to the 30K sentences to ﬁnd
IoT security topics in developer discussions. We observed 9 topics that are
grouped into three categories: Software, Network, and Hardware. Our devel-
oped tools and study ﬁndings can guide automated collection and analysis
of IoT security problems in developer discussions.

References

[1] Number of IoT devices 2015-2025.

[2] A. Abdellatif, D. Costa, K. Badran, R. Abdalkareem, and E. Shihab. Chal-
lenges in chatbot development: A study of stack overﬂow posts. In 17th In-
ternational Conference on Mining Software Repositories, October 5–6, 2020,
Seoul, Republic of Korea. New York, NY, USA. ACM, 2020.

[3] S. Ahmed and M. Bagherzadeh. What do concurrency developers ask
about?: A large-scale study using stack overﬂow. In Proceedings of the 12th
ACM/IEEE International Symposium on Empirical Software Engineering and
Measurement, page Article No. 30, 2018.

[4] A. Al-Fuqaha, M. Guizani, M. Mohammadi, M. Aledhari, and M. Ayyash.
Internet of things: A survey on enabling technologies, protocols, and applica-
tions. IEEE Communications Surveys & Tutorials, 17(4):2347–2376, 2015.

[5] A. S. M. Alharbi and E. de Doncker. Twitter sentiment analysis with a deep
neural network: An enhanced approach using user behavioral information.
Cognitive Systems Research, 54:50–61, 2019.

[6] M. Aly, F. Khomh, and S. Yacout. What do practitioners discuss about
iot and industry 4.0 related technologies? characterization and identiﬁcation
of iot and industry 4.0 categories in stack overﬂow discussions. Internet of
Things, 14:100364, 2021.

46

[7] R. Arun, V. Suresh, C. E. V. Madhavan, and M. N. N. Murthy. On ﬁnding the
natural number of topics with latent dirichlet allocation: some observations.
In Proceedings of the 14th Paciﬁc-Asia conference on Advances in Knowledge
Discovery and Data Mining, pages 391–402, 2010.

[8] H. U. Asuncion, A. U. Asuncion, and R. N. Tylor. Software traceability with
topic modeling. In Proc. 32nd Intl. Conf. Software Engineering, pages 95–104,
2010.

[9] L. Atzori, A. Iera, and G. Morabito. The internet of things: A survey. Com-

puter Networks, 54(15):2787–2805, 2010.

[10] M. Awad and R. Khanna. Support Vector Machines for Classiﬁcation, pages

39–66. Apress, Berkeley, CA, 2015.

[11] M. Bagherzadeh and R. Khatchadourian. Going big: A large-scale study on
what big data developers ask. In Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE 2019, pages 432–442,
New York, NY, USA, 2019. ACM.

[12] M. Bagherzadeh and R. Khatchadourian. Going big: a large-scale study on
what big data developers ask. In Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, pages 432–442, 2019.

[13] A. Bandeira, C. A. Medeiros, M. Paixao, and P. H. Maia. We need to talk
about microservices: an analysis from the discussions on stackoverﬂow. In
2019 IEEE/ACM 16th International Conference on Mining Software Reposi-
tories (MSR), pages 255–259. IEEE, 2019.

[14] A. Barua, S. W. Thomas, and A. E. Hassan. What are developers talking
about? an analysis of topics and trends in stack overﬂow. Empirical Software
Engineering, pages 1–31, 2012.

[15] H. Batra, N. S. Punn, S. K. Sonbhadra, and S. Agarwal. Bert-based sentiment
analysis: A software engineering perspective. In C. Strauss, G. Kotsis, A. M.
Tjoa, and I. Khalil, editors, Database and Expert Systems Applications, pages
138–148, Cham, 2021. Springer International Publishing.

[16] G. Bavota, R. Oliveto, M. Gethers, D. Poshyvanyk, and A. D. Lucia. Method-
book: Recommending move method refactorings via relational topic models.
IEEE Transactions on Software Engineering, 40(7):671–694, 2014.

47

[17] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization.

J. Mach. Learn. Res., 13:281–305, 2012.

[18] L. R. Biggers, C. Bocovich, R. Capshaw, B. P. Eddy, L. H. Etzkorn, and N. A.
Kraft. Conﬁguring latent dirichlet allocation based feature location. Journal
Empirical Software Engineering, 19(3):465–500, 2014.

[19] E. Biswas, M. E. Karabulut, L. Pollock, and K. Vijay-Shanker. Achieving
reliable sentiment analysis in the software engineering domain using bert. In
2020 IEEE International Conference on Software Maintenance and Evolution
(ICSME), pages 162–173, 2020.

[20] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal

of machine Learning research, 3(Jan):993–1022, 2003.

[21] E. Burmeister and L. M. Aitken. Sample size: How many is enough? Aus-

tralian Critical Care, 25(4):271–274, 2012.

[22] Z. B. Celik, L. Babun, A. K. Sikder, H. Aksu, G. Tan, P. D. McDaniel, and
In 27th

A. S. Uluagac. Sensitive information tracking in commodity IoT.
USENIX Conference on Security Symposium, pages 1687 – 1704, 2018.

[23] Z. B. Celik, P. D. McDaniel, and G. Tan. SOTERIA: automated iot safety
and security analysis. In USENIX Conference on Usenix Annual Technical
Conference, pages 147 – 158, 2018.

[24] Z. B. Celik, G. Tan, and P. D. McDaniel. IoTGuard: Dynamic enforcement
of security and safety policy in commodity iot. In Network and Distributed
System Security Symposium, page 15, 2019.

[25] M. A. Chaqfeh and N. Mohamed. Challenges in middleware solutions for the
internet of things. In International Conference on Collaboration Technologies
and Systems (CTS), pages 21–26, 2012.

[26] U. Chauhan and A. Shah. Topic modeling using latent dirichlet allocation:

A survey. ACM Comput. Surv., 54(7), sep 2021.

[27] T.-H. Chen, S. W. Thomas, M. Nagappan, and A. E. Hassan. Explaining
In 9th working conference on mining

software defects using topic models.
software repositories, pages 189–198, 2012.

[28] H. Chi, Q. Zeng, X. Du, and J. Yu. Cross-app interference threats in smart
homes: Categorization, detection and handling. In 50th Annual IEEE/IFIP

48

International Conference on Dependable Systems and Networks, pages 411–
423, 2020.

[29] B. Cleary, C. Exton, J. Buckley, and M. English. An empirical analysis of
information retrieval based concept location techniques in software compre-
hension. Empirical Software Engineering, 14:93–130, 2009.

[30] J. Dai, F. Pan, Z. Shou, and H. Zhang. RoBERTa-IAN for aspect-level sen-
timent analysis of product reviews. Journal of Physics: Conference Series,
1827(1):012079, mar 2021.

[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. Technical report,
https://arxiv.org/abs/1810.04805, 2018.

[32] W. Ding and H. Hu. On the safety of IoT device physical interaction control.
In ACM SIGSAC Conference on Computer and Communications Security,
pages 832 – 846, 2018.

[33] S. Edwards and I. Profetis. Hajime: Analysis of a decentralized internet worm

for IoT devices. Rapidly Networks, 16:18, 2016.

[34] M. Frustaci, P. Pace, G. Aloi, and G. Fortino. Evaluating critical security
IEEE Internet of

issues of the iot world: Present and future challenges.
Things Journal, 5(4):2483 – 2495, 2017.

[35] K. V. Ghag and K. Shah. Comparative analysis of eﬀect of stopwords removal
on sentiment classiﬁcation. In 2015 International Conference on Computer,
Communication and Control (IC4), pages 1–6, 2015.

[36] N. Z. Gong, A. Ozen, Y. Wu, X. Cao, R. Shin, D. Song, H. Jin, and X. Bao.
PIANO: Proximity-based user authentication on voice-powered internet-of-
things devices. In 37th International Conference on Distributed Computing
Systems, pages 2212 – 2219, 2017.

[37] J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami. Internet of things (iot):
A vision, architectural elements, and future directions. Future generation
computer systems, 29(7):1645–1660, 2013.

[38] Z. Hameed and B. Garcia-Zapirain. Sentiment classiﬁcation using a single-

layered bilstm model. IEEE Access, 8:73992–74001, 2020.

49

[39] W. He, M. Golla, R. Padhi, J. Ofek, M. D¨urmuth, E. Fernandes, and B. Ur.
Rethinking access control and authentication for the home internet of things
(IoT). In 27th USENIX Conference on Security Symposium, pages 255 – 272,
2018.

[40] G. Ho, D. Leung, P. Mishra, A. Hosseini, D. Song, and D. A. Wagner. Smart
locks: Lessons for securing commodity internet of things devices.
In 11th
ACM on Asia Conference on Computer and Communications Security, pages
461 – 472, 2016.

[41] J. Hu, X. Sun, D. Lo, and B. Li. Modeling the evolution of development
topics using dynamic topic models. In IEEE 22nd International Conference
on Software Analysis, Evolution, and Reengineering, pages 3–12, 2015.

[42] S. R. Hussain, S. Nirjon, and E. Bertino. Securing the insecure link of internet-
of-things using next-generation smart gateways. In IEEE International Con-
ference on Distributed Computing in Sensor Systems, pages 66–73, 2019.

[43] N. Kant, R. Puri, N. Yakovenko, and B. Catanzaro. Practical text classiﬁca-
tion with large pre-trained language models. arXiv preprint arXiv:1812.01207,
2018.

[44] M. A. Khan and K. Salah. Iot security: Review, blockchain solutions, and
open challenges. Future Generation Computer Systems, 82:395–411, 2018.

[45] L. Leclezio, A. Jansen, V. Whittemore, and P. De Vries. Pilot validation of
the tuberous sclerosis-associated neuropsychiatric disorders (tand) checklist.
Pediatric Neurology, 52, 10 2014.

[46] H. Li, T.-H. P. Chen, W. Shang, and A. E. Hassan. Studying software logging
using topic models. Empirical Software Engineering, 23:2655–2694, 2018.

[47] Y. Liao, E. de Freitas Rocha Loures, and F. Deschamps. Industrial internet of
things: A systematic literature review and insights. IEEE Internet of Things
Journal, 5(6):4515–4525, 2018.

[48] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov. RoBERTa: A robustly optimized bert
pretraining approach. Technical report, https://arxiv.org/abs/1907.
11692, 2019.

[49] C. D. Manning, P. Raghavan, and H. Sch¨utze. An Introduction to Information

Retrieval. Cambridge Uni Press, 2009.

50

[50] M. Marjani, F. Nasaruddin, A. Gani, A. Karim, I. A. T. Hashem, A. Siddiqa,
and I. Yaqoob. Big iot data analytics: Architecture, opportunities, and open
research challenges. IEEE Access, 5(1):5247 – 5261, 2017.

[51] D. Minoli, K. Sohraby, and B. Occhiogrosso. Iot security (IoTSec) mecha-
nisms for e-health and ambient assisted living applications. In IEEE/ACM
International Conference on Connected Health: Applications, Systems and
Engineering Technologies, pages 13–18, 2017.

[52] NLTK.

Sentiment Analysis.

http://www.nltk.org/howto/

sentiment.html, 2016.

[53] M. Obaidi and J. Kl¨under. Development and application of sentiment analysis
tools in software engineering: A systematic literature review. In Evaluation
and Assessment in Software Engineering, EASE 2021, page 80–89, New York,
NY, USA, 2021. Association for Computing Machinery.

[54] S. Overﬂow. Stack Overﬂow Questions. https://stackoverflow.com/

questions/, 2020. Last accessed on 14 November 2020.

[55] K. Pipalia, R. Bhadja, and M. Shukla. Comparative analysis of diﬀerent
transformer based architectures used in sentiment analysis. In 2020 9th Inter-
national Conference System Modeling and Advancement in Research Trends
(SMART), pages 411–415, 2020.

[56] D. Poshyvanyk, Y.-G. Gu´eh´eneuc, A. Marcus, G. Antoniol, and V. T. Rajlich.
Feature location using probabilistic ranking of methods based on execution
scenarios and information retrieval. IEEE Transactions on Software Engi-
neering, 33(6):420–432, 2007.

[57] K. Pretz. The next evolution of the internet. IEEE Magazine The institute,

50(5), 2013.

[58] A. Prinzie and D. Van den Poel. Random forests for multiclass classiﬁcation:
Random multinomial logit. Expert Systems with Applications, 34(3):1721–
1732, 2008.

[59] S. Rao and A. C. Kak. Retrieval from software libraries for bug localization:
a comparative study of generic and composite text models. In 8th Working
Conference on Mining Software Repositories, page 43–52, 2011.

[60] R. ˇReh˚uˇrek and P. Sojka. Software framework for topic modelling with large
corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for
NLP Frameworks, pages 45–50, 2010.

51

[61] M. R¨oder, A. Both, and A. Hinneburg. Exploring the space of topic coherence
measures. In Proceedings of the Eighth ACM International Conference on Web
Search and Data Mining, pages 399–408, 2015.

[62] E. Ronen, A. Shamir, A.-O. Weingarten, and C. O’Flynn. IoT goes nuclear:
Creating a zigbee chain reaction. In IEEE Symposium on Security and Pri-
vacy, pages 195 – 212, 2017.

[63] C. Rosen and E. Shihab. What are mobile developers asking about? a large
scale study using stack overﬂow. Journal Empirical Software Engineering,
21(3):1192–1223, 2016.

[64] V. Sekar, S. Seshan, Y. Agarwal, and C. Xu. Handling a trillion (unﬁxable)
ﬂaws on a billion devices: Rethinking network security for the internet-of-
things. In 14th ACM Workshop on Hot Topics in Networks, 2015.

[65] P. Sethi and S. R. Sarangi. Internet of things: Architectures, protocols, and

applications. Journal of Electrical and Computer Engineering, 2017, 2017.

[66] N. Sharma, M. Shamkuwar, and I. Singh. The history, present and future
with iot. Internet of Things and Big Data Analytics for Smart Generation,
154(1):27–51, 2019.

[67] B. H. Shekar and G. Dagnew. Grid search-based hyperparameter tuning and
classiﬁcation of microarray cancer data. In 2019 Second International Confer-
ence on Advanced Computational and Communication Paradigms (ICACCP),
pages 1–8, 2019.

[68] Stack Exchange, Inc. Stack Exchange Data Dump. https://archive.

org/details/stackexchange, 2019.

[69] X. Sun, B. Li, H. Leung, B. Li, and Y. Li. Msr4sm: Using topic models
to eﬀectively mining software repositories for software maintenance tasks.
Information and Software Technology, 66:671–694, 2015.

[70] X. Sun, B. Li, Y. Li, and Y. Chen. What information in software historical
repositories do we need to support software maintenance tasks? an approach
based on topic model. Computer and Information Science, pages 22–37, 2015.

[71] J. Tabassum, M. Maddela, W. Xu, and A. Ritter. Code and named entity
recognition in stackoverﬂow. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics (ACL), 2020.

52

[72] W.-H. Tai, H. T. Kung, and X. Dong. exbert: Extending pre-trained mod-
els with domain-speciﬁc vocabulary under constrained training resources. In
FINDINGS, 2020.

[73] L. Tawalbeh, F. Muheidat, M. Tawalbeh, and M. Quwaider. Iot privacy and

security: Challenges and solutions. Applied Sciences, 10(12), 2020.

[74] P. Tedeschi, S. Sciancalepore, A. Eliyan, and R. D. Pietro. Like: Lightweight
certiﬁcateless key agreement for secure iot communications. IEEE Internet
of Things Journal, 7(1):621–638, 2020.

[75] S. W. Thomas, B. Adams, A. E. Hassan, and D. Blostein. Modeling the
In 8th working conference on

evolution of topics in source code histories.
mining software repositories, pages 173–182, 2011.

[76] S. W. Thomas, B. Adams, A. E. Hassan, and D. Blostein. Studying software
evolution using topic models. Science of Computer Programming, 80(B):457–
479, 2014.

[77] K. Tian, M. Revelle, and D. Poshyvanyk. Using latent dirichlet allocation for
automatic categorization of software. In 6th international working conference
on mining software repositories, pages 163–166, 2009.

[78] Y. Tian, F. Thung, A. Sharma, and D. Lo. APIBot: question answering bot
for api documentation. In Proc. 32nd IEEE/ACM International Conference
on Automated Software Engineering, pages 153–158, 2017.

[79] G. Uddin. Security and machine learning adoption in iot: A preliminary study
of iot developer discussions. In Proc.IEEE/ACM 3rd International Workshop
on Software Engineering Research and Practices for the IoT, pages 36–43,
2021.

[80] G. Uddin, Y.-G. Gueheneuc, F. Khomh, and C. K. Roy. An empirical study
of the eﬀectiveness of an ensemble of stand-alone sentiment detection tools
for software engineering datasets. 2021.

[81] G. Uddin and F. Khomh. Automatic summarization of API reviews. In Proc.
32nd IEEE/ACM International Conference on Automated Software Engineer-
ing, page 12, 2017.

[82] G. Uddin and F. Khomh. Automatic opinion mining from API reviews from
stack overﬂow. IEEE Transactions on Software Engineering, page 35, 2018.
Under review (Major Revision).

53

[83] G. Uddin and F. Khomh. Automatic opinion mining from API reviews from
stack overﬂow. IEEE Transactions on Software Engineering, page 35, 2019.

[84] G. Uddin, F. Sabir, Y.-G. Gu´eh´eneuc, O. Alam, and F. Khomh. An empirical
study of iot topics in iot developer discussions on stack overﬂow. Empirical
Software Engineering, 26, 11 2021.

[85] A. J. Viera and J. M. Garrett. Understanding interobserver agreement: The

kappa statistic. Family medicine, 37(4):360–363, 2005.

[86] Z. Wan, X. Xia, and A. E. Hassan. What do programmers discuss about
blockchain? a case study on the use of balanced lda and the reference archi-
tecture of a domain to capture online discussions about blockchain platforms
across stack exchange communities. IEEE Transactions on Software Engi-
neering, (1):24, 2019.

[87] M. Weyrich and C. Ebert. Reference architectures for the internet of things.

IEEE Software, 33(1):112–116, 2016.

[88] R. Winters, A. Winters, and R. G. Amedee. Statistics: A brief overview.

Ochsner Journal, 10(3):213–216, 2010.

[89] X.-L. Yang, D. Lo, X. Xia, Z.-Y. Wan, and J.-L. Sun. What security questions
do developers ask? a large-scale study of stack overﬂow posts. Journal of
Computer Science and Technology, 31(5):910–924, 2016.

[90] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le.
XLNet: Generalized autoregressive pretraining for language understanding.
Technical report, https://arxiv.org/abs/1906.08237, 2020.

[91] T. Zhang, B. Xu, F. Thung, S. A. Haryono, D. Lo, and L. Jiang. Senti-
ment analysis for software engineering: How far can pre-trained transformer
models go? In IEEE International Conference on Software Maintenance and
Evolution, pages 70–80, 2020.

[92] W. Zhang, T. Yoshida, and X. Tang. A comparative study of tf*idf, lsi
and multi-words for text classiﬁcation. Expert Systems with Applications,
38(3):2758–2765, 2011.

[93] Z.-K. Zhang, M. C. Y. Cho, C.-W. Wang, C.-W. Hsu, C.-K. Chen, and
S. Shieh.
In
IEEE 7th International Conference on Service-Oriented Computing and Ap-
plications, pages 230–234, 2014.

Iot security: Ongoing challenges and research opportunities.

54

