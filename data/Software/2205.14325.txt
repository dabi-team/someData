2
2
0
2

y
a
M
8
2

]

G
L
.
s
c
[

1
v
5
2
3
4
1
.
5
0
2
2
:
v
i
X
r
a

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM
classiﬁcation via mixed-integer optimization

Ryuta Tamura1,2*, Yuichi Takano3 and Ryuhei Miyashiro4

1Graduate School of Engineering, Tokyo University of
Agriculture and Technology, 2-24-16 Naka-cho, Koganei-shi,
184-8588, Tokyo, Japan.
2October Sky Co., Ltd., Zelkova Bldg., 1-25-12 Fuchu-cho,
Fuchu-shi, 183-0055, Tokyo, Japan.
3Faculty of Engineering, Information and Systems, University of
Tsukuba, 1-1-1 Tennodai, Tsukuba-shi, 305-8573, Ibaraki, Japan.
4Institute of Engineering, Tokyo University of Agriculture and
Technology, 2-24-16 Naka-cho, Koganei-shi, 184-8588, Tokyo,
Japan.

*Corresponding author(s). E-mail(s): r.tamura.cbc@gmail.com;

Abstract

We study the mixed-integer optimization (MIO) approach to feature
subset selection in nonlinear kernel support vector machines (SVMs) for
binary classiﬁcation. First proposed for linear regression in the 1970s,
this approach has recently moved into the spotlight with advances in
optimization algorithms and computer hardware. The goal of this paper
is to establish an MIO approach for selecting the best subset of fea-
tures for kernel SVM classiﬁcation. To measure the performance of
subset selection, we use the kernel–target alignment, which is the dis-
tance between the centroids of two response classes in a high-dimensional
feature space. We propose a mixed-integer linear optimization (MILO)
formulation based on the kernel–target alignment for feature subset
selection, and this MILO problem can be solved to optimality using opti-
mization software. We also derive a reduced version of the MILO problem
to accelerate our MILO computations. Experimental results show good
computational eﬃciency for our MILO formulation with the reduced
problem. Moreover, our method can often outperform the linear-SVM-
based MILO formulation and recursive feature elimination in prediction
performance, especially when there are relatively few data instances.

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Feature subset selection for kernel SVM classiﬁcation

Keywords: Feature subset selection, Support vector machine, Mixed-integer
optimization, Kernel–target alignment, Machine learning

1 Introduction

1.1 Overview and related work

Support vector machines (SVMs) are a family of sophisticated pattern recogni-
tion methods based on optimal separating hyperplanes. This method was ﬁrst
devised for binary classiﬁcation by Boser et al. [11] in combination with the
kernel method [1] for nonlinear data analyses. Since then, SVMs have attracted
considerable attention in various scientiﬁc ﬁelds because of their solid theoret-
ical foundations and high generalization ability [15, 21, 70]. Kernel methods
have been extended to a variety of multivariate analyses (e.g., principal com-
ponent analysis, cluster analysis, and outlier detection) [60, 61], and they have
also been applied to dynamic portfolio selection [63, 64].

Feature subset selection involves selecting a subset of relevant features
used in machine learning models. Such selection helps to understand the
causality between predictor features and response classes, and it reduces the
data collection/storage costs and the computational load of training machine
learning models. Moreover, the prediction performance can be improved
because overﬁtting is mitigated by eliminating redundant features. Because
of these beneﬁts, algorithms for feature subset selection have been exten-
sively studied [17, 29, 46, 47]. These algorithms can be categorized into ﬁlter,
wrapper, and embedded methods. Filter methods (e.g., Fisher score [31] and
relief [13, 40]) rank features according to evaluation criteria before the training
phase. Wrapper methods (e.g., recursive feature elimination [31]) search for
better subsets of features through repeated training of subset models. Embed-
ded methods (e.g., L1-regularized estimation [33]) provide a subset of features
as a result of the training process.

We address the mixed-integer optimization (MIO) approach to feature sub-
set selection. First proposed for linear regression in the 1970s [2], this approach
has recently gained attention with advances in optimization algorithms and
computer hardware [6, 19, 32, 41, 69]. Compared with many heuristic opti-
mization algorithms, the MIO approach has the advantage of selecting the best
subset of features with respect to given criterion functions [53, 54, 65]. MIO
methods for feature subset selection have been extended to logistic regres-
sion [7, 59], ordinal regression [55, 58], count regression [57], dimensionality
reduction [4, 74], and elimination of multicollinearity [5, 8, 66, 67]. MIO-
based high-performance algorithms have also been designed for feature subset
selection [9, 10, 22, 34, 35, 42].

Several prior studies have dealt with feature subset selection in linear SVM
classiﬁcation. A typical approach involves approximating the L0-regularization

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

3

term (or the cardinality constraint) for subset selection by the concave expo-
nential function [12], the L1-regularization term [12, 75, 77], and convex
relaxations [16, 26], and the L0-regularization term can be handled more accu-
rately by DC (diﬀerence of convex functions) algorithms [24, 44]. Meanwhile,
Maldonado et al. [49] proposed exact MIO formulations for feature subset
selection in linear SVM classiﬁcation, and Labb´e et al. [43] applied a heuristic
kernel search algorithm to the MIO problem; Lagrangian relaxation [25] and
generalized Benders decomposition [3] have also been used to handle large-
scale MIO problems. However, since these algorithms are focused on linear
classiﬁcation, they cannot be applied to nonlinear classiﬁcation based on the
kernel method.

The feature scaling approach has been studied intensively for feature sub-
set selection in kernel SVM classiﬁcation [18, 27, 48, 51, 76]. This approach
introduces feature weights in a kernel function and updates them iteratively
in the gradient descent direction. Other algorithms for feature subset selec-
tion in kernel SVM classiﬁcation include the ﬁlter method based on local
kernel gradients [36], local search algorithms [50, 52, 72], and metaheuristic
algorithms [37]. Several performance measures for kernel SVM classiﬁers have
been used in feature subset selection. Chapelle et al. [18] designed gradient
descent methods for minimizing various performance bounds on generaliza-
tion errors, Neumann et al. [56] proposed DC algorithms to maximize the
kernel–target alignment [20], Wang [72] considered the kernel class separabil-
ity in feature subset selection, and Jim´enez-Cordero et al. [39] used software
for nonlinear optimization to obtain good-quality solutions to their min-max
optimization problem. To our knowledge, however, no prior studies have devel-
oped an exact algorithm to compute the best subset of features in terms of a
given performance measure for nonlinear kernel SVM classiﬁcation.

1.2 Contribution

The goal of this paper is to establish a practical MIO approach to select-
ing the best subset of features for nonlinear kernel SVM classiﬁcation. In line
with Neumann et al. [56], we use the kernel–target alignment [20] as an objec-
tive function for subset selection. Also known as the distance between two
classes (DBTC) [62] in a high-dimensional feature space, the kernel–target
alignment has many applications in various kernel-based machine learning
algorithms [73]. First, we introduce an integer nonlinear optimization (INLO)
formulation based on the kernel–target alignment for feature subset selec-
tion. Next, we reformulate the problem as a mixed-integer linear optimization
(MILO) problem, which can be solved to optimality using optimization soft-
ware. We also derive a reduced version of the MILO problem to accelerate our
MILO computations.

We assess the eﬃcacy of our method through computational experiments
using real-world and synthetic datasets. With the real-world datasets, our
MILO formulations oﬀer clear computational advantages over the INLO for-
mulation. In addition, the problem reduction oﬀers highly accelerated MILO

Springer Nature 2021 LATEX template

4

Feature subset selection for kernel SVM classiﬁcation

computations. With the synthetic datasets, our method often outperforms the
linear-SVM-based MILO formulation [49] and recursive feature elimination [31]
in terms of accuracy for both classiﬁcation and subset selection, especially
when there are relatively few data instances.

1.3 Organization and notation

In Section 2, we review SVMs for binary classiﬁcation and introduce the
kernel–target alignment. In Section 3, we present our MIO formulations based
on the kernel–target alignment for feature subset selection. We report the
computational results in Section 4 and conclude in Section 5.

Throughout this paper, we denote the set of consecutive integers ranging
from 1 to n as [n] := {1, 2, . . . , n}. We write a p-dimensional column vector as
x := (xj)j∈[p] ∈ Rp, and an m × n matrix as X := (xij )(i,j)∈[m]×[n] ∈ Rm×n.

2 Support vector machines for binary

classiﬁcation

In this section, we review linear and kernel SVMs for binary classiﬁcation [15,
21, 70]. We then introduce the kernel–target alignment [20, 56] to be used for
feature subset selection.

2.1 Linear SVM classiﬁcation

We address the task of correctly assigning a binary class label ˆy ∈ {−1, +1}
to each p-dimensional feature vector x := (xj )j∈[p] ∈ Rp. We begin with the
following linear classiﬁer:

f (x) = w⊤x + b =

p

j=1
X

wjxj + b,

(1)

where the bias term b ∈ R and the weight vector w := (wj )j∈[p] ∈ Rp are
parameters to be estimated. We predict a class label based on the sign of the
classiﬁer (1) as

f (x) < 0 ⇒ ˆy = −1,
f (x) > 0 ⇒ ˆy = +1.
Suppose that we are given a training dataset {(xi, yi) | i ∈ [n]} containing
n data instances, where xi := (xij )j∈[p] ∈ Rp and yi ∈ {−1, +1} for each
i ∈ [n]. All data instances are correctly separated by Eq. (2) using the linear
classiﬁer (1) when

(2)

(

yif (xi) > 0

(i ∈ [n]),

(3)

but this is usually impossible. For this reason, the linear separability condi-
tion (3) is relaxed by introducing a vector ξ := (ξi)i∈[n] ∈ Rn of classiﬁcation
errors.

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

5

The soft-margin SVM minimizes the weighted sum of the L2-regularization

term kwk2

2 =

p
j=1 w2

j and classiﬁcation errors as

P

minimize

1
2

n

kwk2

2 + C

ξi

i=1
X

subject to yi(w⊤xi + b) ≥ 1 − ξi

(i ∈ [n]),
ξi ≥ 0
b ∈ R, w ∈ Rp, ξ ∈ Rn,

(i ∈ [n]),

(4)

(5)

(6)

(7)

where C ∈ R+ is a user-deﬁned parameter representing the misclassiﬁcation
penalty. Throughout this paper, each optimization formulation lists decision
variables on the last line (e.g., Eq. (7)).

The Lagrangian dual form of problem (4)–(7) is expressed as

n

maximize

αi −

i=1
X
n

1
2

n

n

i=1
X

h=1
X

subject to

αiyi = 0,

αiαhyiyhx⊤

i xh

i=1
X
0 ≤ αi ≤ C (i ∈ [n]),
α ∈ Rn,

(8)

(9)

(10)

(11)

where α := (αi)i∈[n] ∈ Rn is a vector composed of Lagrange multipliers. Note
n
i=1 αiyixi holds from the optimality condition; therefore, the
that w =
linear classiﬁer (1) is rewritten as

P

f (x) = w⊤x + b =

n

i=1
X

αiyix⊤

i x + b.

(12)

2.2 Kernel SVM classiﬁcation
We consider a feature map φ : Rp → X , which nonlinearly transforms the
original feature vector x into a high-dimensional feature vector φ(x) in a
feature space X . A simple example with x = (x1, x2)⊤ is given by

φ(x) = (x1, x2, x2

1, x1x2, x2

2)⊤.

We also deﬁne the kernel function

k(x, x′) = φ(x)⊤φ(x′),

(13)

(14)

Springer Nature 2021 LATEX template

6

Feature subset selection for kernel SVM classiﬁcation

which is the inner product in a feature space. The associated kernel matrix is
written as

K :=

k(xi, xh)

∈ Rn×n.

(i,h)∈[n]×[n]

(15)

We replace the original feature vector x with the high-dimensional feature
vector φ(x) in problem (8)–(11) and the corresponding classiﬁer (12). The
kernel SVM is then formulated as the following optimization problem:

(cid:16)

(cid:17)

n

maximize

αi −

i=1
X
n

1
2

n

n

i=1
X

h=1
X

subject to

αiyi = 0,

αiαhyiyhk(xi, xh)

i=1
X
0 ≤ αi ≤ C (i ∈ [n]),
α ∈ Rn,

where the associated nonlinear classiﬁer is given by

n

f (x) =

αiyik(xi, x) + b.

i=1
X

We consider the Gaussian kernel function deﬁned as

k(x, x′) = exp

−γkx − x′k2
2

= exp

−γ

(xj − x′

j )2

p

(16)

(17)

(18)

(19)

(20)

,

(21)

j=1
X

!

(cid:0)

(cid:1)
where γ ∈ R+ is a user-deﬁned scaling parameter. It is known that
the Gaussian kernel function (21) corresponds to the inner product in an
inﬁnite-dimensional feature space.

Consequently, if the kernel function (14) is computable (e.g., Eq. (21)),
the nonlinear classiﬁer (20) can be obtained from solving the Lagrangian dual
problem (16)–(19). Called the kernel method, this procedure avoids having to
deﬁne feature maps explicitly (e.g., Eq. (13)).

2.3 Kernel–target alignment
For the class label vector y := (yi)i∈[n] ∈ {−1, +1}n, the kernel–target
alignment [20, 56] of the kernel matrix (15) is deﬁned as

ˆA(K, yy⊤) :=

K • (yy⊤)
nkKkF

=

n
h=1 yiyhk(xi, xh)
n
h=1 k(xi, xh)2

n
P
i=1

.

(22)

n
i=1

n
P
pP

P

Following Neumann et al. [56], we focus on only the numerator of Eq. (22) in
view of the boundedness of the kernel function (21).

 
Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

7

The index set of data instances in each class y ∈ {−1, +1} is denoted by

N (y) := {i ∈ [n] | yi = y}.

We also deﬁne a vector of class labels divided by each class size as

ψ := (ψi)i∈[n] :=

yi
|N (yi)|

(cid:18)

(cid:19)i∈[n]

∈ Rn.

(23)

Then, it follows from Eqs. (14) and (23) that the kernel–target alignment [56]
for the class label vector ψ is expressed as

n

n

i=1
X

h=1
X

ψiψhk(xi, xh)

1
|N (−1)|

Xi∈N (−1)

φ(xi) −

1
|N (+1)|

Xi∈N (+1)

φ(xi)

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

.

(24)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

This corresponds to the squared Euclidean distance between the centroids of
negative and positive classes in a high-dimensional feature space X ; in other
words, the class separability in a feature space can be measured by Eq. (24).
In Section 3, we use the kernel–target alignment (24) as an objective function
to be maximized for subset selection.

3 Mixed-integer optimization formulations for

feature subset selection

In this section, we present our MIO formulations based on the kernel–target
alignment (24) for feature subset selection. We also apply some problem
reduction techniques to our MIO formulations.

3.1 Integer nonlinear optimization formulation

For subset selection, we assume that all features are standardized as

xij = 0 and

n
i=1 x2
ij
n

P

= 1

n

i=1
X

(25)

for all j ∈ [p].

Let z := (zj)j∈[p] ∈ {0, 1}p be a vector composed of binary decision vari-
ables for subset selection; namely, zj = 1 if the jth feature is selected, and
zj = 0 otherwise. We then consider the following subset-based Gaussian kernel

Springer Nature 2021 LATEX template

8

Feature subset selection for kernel SVM classiﬁcation

function:

kz(x, x′) := exp

−γ

zj(xj − x′

j)2

p

j=1
X

.

!

(26)

To select the best subset of features for kernel SVM classiﬁcation, we
maximize the kernel–target alignment (24) based on the subset-based Gaus-
sian kernel function (26). This problem is formulated as the following INLO
problem:

n

n

maximize

ψiψh exp

−γ

i=1
X
p

h=1
X

subject to

zj ≤ θ,

j=1
X
z ∈ {0, 1}p,

p

j=1
X

zj(xij − xhj)2

!

(27)

(28)

(29)

where θ ∈ [p] is a user-deﬁned subset size parameter.

However, a globally optimal solution to problem (27)–(29) is very diﬃ-
cult to compute because its objective function (27) is a nonlinear nonconvex
function.

3.2 Mixed-integer linear optimization formulations

Theorem 1 states that the INLO problem (27)–(29) can be reformulated as
the following MILO problem:

n

n

maximize

ψiψheih,p+1

i=1
X
p

h=1
X

subject to

zj ≤ θ,

j=1
X
eih1 = 1
zj = 0 ⇒ eih,j+1 = eihj

(i ∈ [n], h ∈ [n]),

zj = 1 ⇒ eih,j+1 = exp

(i ∈ [n], h ∈ [n], j ∈ [p]),
−γ(xij − xhj)2

· eihj

(i ∈ [n], h ∈ [n], j ∈ [p]),
, z ∈ {0, 1}p,

e ∈ Rn×n×(p+1)
+

(cid:0)

(cid:1)

(30)

(31)

(32)

(33)

(34)

(35)

where e := (eihj)(i,h,j)∈[n]×[n]×[p+1] ∈ Rn×n×(p+1)
is an array of auxiliary non-
negative decision variables. Here, Eqs. (33) and (34) are logical implications,
which can be imposed by using indicator constraints implemented in modern
optimization software.

+

 
 
Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

9

Theorem 1 Let (e∗, z∗) be an optimal solution to the MILO problem (30)–(35).
Then, z∗ is also an optimal solution to the INLO problem (27)–(29).

Proof For each z ∈ {0, 1}p, we can construct a feasible solution (e, z) to prob-
lem (30)–(35) by using Eqs. (33)–(34) recursively from Eq. (32). Therefore, the
feasible region of z in problem (30)–(35) is the same as in problem (27)–(29).
Consequently, it is necessary to prove only that, in Eqs. (27) and (30),

eih,p+1 = exp

for all (i, h) ∈ [n] × [n]. Note that

p

−γ

zj(xij − xhj )2

j=1
X









exp

−γzj(xij − xhj)2

=

(cid:16)

(cid:17)

exp(0) = 1

exp

(

−γ(xij − xhj)2

if zj = 0,
if zj = 1.

Therefore, the constraints (33) and (34) can be integrated into

(cid:16)

(cid:17)

eih,j+1 = exp

−γzj(xij − xhj)2

· eihj

for all j ∈ [p]. By substituting this equation recursively, we obtain

(cid:16)

(cid:17)

eih,p+1 =

p

j=1
Y

= exp

which completes the proof.

exp

−γzj(xij − xhj)2

∵ Eq. (32)

(cid:16)

p

(cid:17)

−γ

zj(xij − xhj)2

j=1
X

,









(cid:3)

Note also that problem (30)–(35) can be equivalently rewritten without

logical implications as

n

n

maximize

ψiψheih,p+1

i=1
X
p

h=1
X

subject to

zj ≤ θ,

j=1
X
eih1 = 1
− M zj ≤ eih,j+1 − eihj ≤ M zj

(i ∈ [n], h ∈ [n]),

− M · (1 − zj) ≤ eih,j+1 − exp

≤ M · (1 − zj)

e ∈ Rn×n×(p+1)
+

, z ∈ {0, 1}p,

(i ∈ [n], h ∈ [n], j ∈ [p]),
−γ(xij − xhj )2
(i ∈ [n], h ∈ [n], j ∈ [p]),

· eihj

(cid:0)

(cid:1)

(36)

(37)

(38)

(39)

(40)

(41)

where M ∈ R+ is a suﬃciently large positive constant (e.g., M = 1 due to
Eqs. (32)–(34)).

Springer Nature 2021 LATEX template

10

Feature subset selection for kernel SVM classiﬁcation

3.3 Problem reduction

Let us deﬁne the following index sets of instance pairs:

H := {(i, h) ∈ [n] × [n] | i < h},

H+ := {(i, h) ∈ [n] × [n] | i < h, ψiψh > 0},
H− := {(i, h) ∈ [n] × [n] | i < h, ψiψh < 0}.

Then, Theorem 2 proves that the MILO problem (36)–(41) can be reduced to
the following MILO problem:

maximize

ψiψheih,p+1

X(i,h)∈H
p

subject to

zj ≤ θ,

((i, h) ∈ H),

j=1
X
eih1 = 1
eih,j+1 − eihj ≤ 0
− Mihjzj ≤ eih,j+1 − eihj

((i, h) ∈ H+, j ∈ [p]),

((i, h) ∈ H−, j ∈ [p]),

eih,j+1 − exp

−γ(xij − xhj)2

· eihj ≤ Mihj · (1 − zj)

((i, h) ∈ H+, j ∈ [p]),
(cid:0)

(cid:1)

0 ≤ eih,j+1 − exp

−γ(xij − xhj )2

· eihj

((i, h) ∈ H−, j ∈ [p]),

(cid:0)

(cid:1)

e ∈ R|H|×(p+1)
+

, z ∈ {0, 1}p,

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

where

Mihj := 1 − exp

−γ(xij − xhj)2

(i ∈ [n], h ∈ [n], j ∈ [p]).

(50)

Theorem 2 Let (e∗, z∗) be an optimal solution to the reduced MILO problem (42)–
(49). Then, z∗ is also an optimal solution to the INLO problem (27)–(29).

(cid:0)

(cid:1)

Proof Because of Theorem 1, it is necessary to prove only that problem (36)–(41),
which is equivalent to problem (30)–(35), can be reformulated as problem (42)–(49).
We begin by focusing on the objective function (36). Note that Eq. (27) can be

decomposed as

n

n

Xh=1

i=1
X
n

ψiψh exp

−γ



p

zj (xij − xhj)2

j=1
X


ψiψh exp





p

j=1
X

−γ





zj (xij − xhj )2

.





=

ψ2

i + 2

i=1
X

X(i,h)∈H

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

11

This implies that the objective function (36) can be replaced with Eq. (42).
Accordingly, the unnecessary decision variables (i.e., eihj for (i, h) 6∈ H) and the
corresponding subset of constraints (38)–(40) can be deleted from the problem.

Next, we consider constraints (39) and (40). It is clear from Eqs. (32)–(34) that

0 ≤ exp

−γ(xij − xhj)2

· eihj ≤ eih,j+1 ≤ eihj ≤ 1.

(cid:16)
Therefore, it follows that

− Mihj ≤ −

1 − exp

−γ(xij − xhj)2

(cid:16)

0 ≤ eih,j+1 − exp

|

(cid:16)

Mihj
−γ(xij − xhj)2
{z

(cid:17)

(cid:17)(cid:17)

·eihj ≤ eih,j+1 − eihj ≤ 0,

(cid:16)

(cid:17)

(cid:16)

}
· eihj ≤

1 − exp

−γ(xij − xhj)2
(cid:16)

Mihj

(cid:17)(cid:17)

·eihj ≤ Mihj .

This implies that the feasible region remains the same even if the constraints (39)
and (40) are tightened as

{z

}

|

− Mihj zj ≤ eih,j+1 − eihj ≤ 0,

0 ≤ eih,j+1 − exp

−γ(xij − xhj)2

· eihj ≤ Mihj · (1 − zj ).

(51)

(52)

(cid:16)

(cid:17)

When ψiψh > 0, the left inequalities in Eqs. (51) and (52) are redundant because
eih,j+1 is maximized by the objective function (36). Similarly, when ψiψh < 0, the
right inequalities in Eqs. (51) and (52) are redundant. As a result, constraints (45)–
(cid:3)
(48) are obtained, thus completing the proof.

We conclude this section by highlighting the diﬀerences between the MILO
problem (36)–(41) and its reduced version (42)–(49). The number of continuous
decision variables is reduced from (p + 1)n2 (Eq. (41)) to (p + 1)n(n − 1)/2
(Eq. (49)), and the number of inequality constraints is reduced from 4pn2
(Eqs. (39) and (40)) to pn(n − 1) (Eqs. (45)–(48)). Also, the big-M values are
equal (e.g., M = 1) in Eqs. (39) and (40), whereas they are set to the smaller
values (50) in Eqs. (46) and (47).

4 Computational experiments

In this section, we report the results of computations to evaluate the eﬃcacy of
our method for feature subset selection in kernel SVM classiﬁcation. First, we
conﬁrm the computational eﬃciency of our MIO formulations using real-world
datasets, and then we examine the prediction performance of our method for
feature subset selection using synthetic datasets.

All computations were performed on a Windows computer with two Intel
Xeon E5-2620v4 CPUs (2.10 GHz) and 128 GB of memory using a single
thread.

4.1 Experimental design for real-world datasets

We downloaded four real-world datasets for classiﬁcation tasks from the UCI
Machine Learning Repository [23]. Table 1 lists the datasets, where n and p are

Springer Nature 2021 LATEX template

12

Feature subset selection for kernel SVM classiﬁcation

the numbers of data instances and candidate features, respectively. Categorical
variables with two categories were treated as dummy variables, and those with
more than two categories were transformed into sets of dummy variables. In the
Zoo and Parkinsons datasets, the names of data instances were deleted. In the
Hepatitis dataset, we removed four variables containing more than 10 missing
values, and then data instances containing missing values. In the Soybean
dataset, variables with the same value in all data instances were eliminated.
The Zoo and Soybean datasets have multiple response classes, so the positive
label (i.e., yi = +1) was given to classes 1 and 2 in the Zoo dataset and to
classes D1 and D4 in the Soybean dataset, and the negative label (i.e., yi = −1)
was given to the other classes.

Table 1 Real-world datasets

Name

n

p Original dataset [23]

Hepatitis
Zoo
Parkinsons
Soybean

138
101
195
47

15 Hepatitis
Zoo
16
Parkinsons
22
Soybean (Small)
45

We compare the computational eﬃciency of the following MIO formulations

for feature subset selection in kernel SVM classiﬁcation:

INLO-K: INLO formulation (27)–(29);
MILO-K: MILO formulation (36)–(41) with M = 1;
RMILO-K: reduced MILO formulation (42)–(49).

The MILO problems were solved using the optimization software IBM ILOG
CPLEX 20.1.0.0 [38], where algorithms for solving relaxed subproblems on
each node were set to the interior-point method instead of the dual simplex
method. To increase numerical stability, the big-M values for RMILO-K were
set as

Mihj = min{1 − exp

−γ(xij − xhj )2

+ 0.1, 1.0} (i ∈ [n], h ∈ [n], j ∈ [p]).

(53)
(cid:1)
The INLO problem, which cannot be handled by CPLEX because of its non-
linear objective function, was solved by the optimization software Gurobi
Optimizer 9.5.0 [28] using the general constraint EXP function.

(cid:0)

Many algorithms have been proposed for tuning SVM hyperparame-
ters [71]. Based on the sigest method [14], we estimated an appropriate value
of the scaling parameter γ in the subset-based Gaussian kernel function (26)
as follows:

ˆγ :=

median of

(θ/p) ·

p
j=1(xij − xhj)2 | (i, h) ∈ H

1

n

P

.

(54)

o

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

13

We then set γ = βˆγ with the scaling factor β ∈ {0.25, 1.00, 4.00}.

The following column labels are used in Tables 2–9:

ObjVal: value of the objective function (27);
OptGap: absolute diﬀerence between lower and upper bounds on the optimal
objective value divided by the lower bound;
| ˆS|: subset size of selected features;
Time: computation time in seconds.

A computation was terminated if it did not complete within 10000 s; in those
cases, the best feasible solution found within 10000 s was taken as the result.

4.2 Results for real-world datasets

Tables 2–5 give the computational results of the three MIO formulations for
the real-world datasets. First, we focus on the results for the Hepatitis dataset
(Table 2). The INLO formulation (INLO-K) always reached the time limit of
10000 s, and therefore its ObjVal values were often very small. In contrast,
our reduced MILO formulation (RMILO-K) solved all the problem instances
completely within the time limit. Moreover, RMILO-K ﬁnished computations
much sooner than did the original MILO formulation (MILO-K); for example,
the computation times of MILO-K and RMILO-K for (θ, β) = (3, 0.25) were
9498.3 s and 716.6 s, respectively.

For the Zoo dataset (Table 3), RMILO-K was still much faster than the
other formulations, whereas the diﬀerences in ObjVal among the three formu-
lations were relatively small. For the Parkinsons dataset (Table 4), although
the three formulations failed to ﬁnish computations within the time limit,
RMILO-K often attained the largest ObjVal and smallest OptGap values. For
the Soybean dataset (Table 5), INLO-K and MILO-K often reached the time
limit, whereas RMILO-K solved all the problem instances to optimality. These
results show that our MILO formulations oﬀer clear computational advantages
over the INLO formulation, and the problem reduction oﬀers highly accelerated
MILO computations.

Next, we examine how the two user-deﬁned parameters (θ, β) aﬀected the
MILO computations. The computation time of RMILO-K was longer when θ
was large; this is reasonable because the number of feasible subsets of features
increases with θ. Also, the computation time of RMILO-K was often longest
with β = 1.00; this implies that solving MILO problems is computationally
expensive when the scaling parameter γ is tuned appropriately by Eq. (54).

4.3 Experimental design for synthetic datasets

We prepared synthetic datasets based on the MADELON dataset [30] from
the NIPS 2003 Feature Selection Challenge. Speciﬁcally, we supposed that
there were θ∗ relevant features and p − θ∗ irrelevant features. The relevant
features were generated using the NDCC (normally distributed clusters on
cubes) data generator [68], which is designed to create datasets for nonlinear

Springer Nature 2021 LATEX template

14

Feature subset selection for kernel SVM classiﬁcation

binary classiﬁcation. The expansion factor “exp” is used in the NDCC data
generator to stretch the covariance matrix of multivariate normal distributions;
as the expansion factor increases, two classes overlap and become diﬃcult to
discriminate. The irrelevant features were drawn randomly from the standard
normal distribution. All these features were standardized as in Eq. (25).

We used n data instances as a training dataset for each combination
(n, p, exp, θ∗) of parameter values. For this training dataset, we selected a
subset ˆS of features. The accuracy of subset selection is measured by the
:= |S∗ ∩ ˆS|/|S∗| and
F1 score, which is the harmonic average of Recall
Precision := |S∗ ∩ ˆS|/| ˆS| as follows:

SetF1 :=

2 · Recall · Precision
Recall + Precision

,

where S∗ is the set of relevant features.

By means of the training dataset, we trained SVM classiﬁers with the
selected subset ˆS of features. We then evaluated the prediction performance
by applying the trained classiﬁer to a testing dataset consisting of suﬃciently
many data instances. Let ˆyi( ˆS) be the class label predicted for the ith data
instance. The classiﬁcation accuracy for the testing dataset is calculated as

ClsAcc :=

|{i ∈ ˜N | yi = ˆyi( ˆS)}|
| ˜N |

,

where ˜N is the index set of testing data instances. We repeated this process
10 times and give average values in Tables 6–9.

We compare the prediction performance of the following methods for

feature subset selection:

MILO-L: MILO formulation (MILP2 [49]) for linear SVM classiﬁcation;
RFE-K: recursive feature elimination [31] for kernel SVM classiﬁcation;
RMILO-K: our reduced MILO formulation (42)–(49) for kernel SVM
classiﬁcation.

The MILO formulation (MILP2 [49]) was proposed for feature subset selection
in linear SVM classiﬁcation. The recursive feature elimination was imple-
mented using the caret package in the R programming language. The MILO
problems were solved using the optimization software IBM ILOG CPLEX
20.1.0.0 [38], where the interior-point method was used to solve relaxed
subproblems. The big-M values for RMILO-K were set as in Eq. (53).
For a selected subset ˆS of features, SVM classiﬁers were trained using the
sklearn.svm.LinearSVC function (MILO-L) and the sklearn.svm.SVC func-
tion (RFE-K and RMILO-K) in the Python programming language. We set
the misclassiﬁcation penalty parameter as C = 1, which performed well for
our synthetic datasets. We also used the scaling parameter γ = ˆγ, which was
tuned by Eq. (54) for the subset-based Gaussian kernel function (26).

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

15

4.4 Results for synthetic datasets

Tables 6–9 show the computational results of the three methods for feature
subset selection for the synthetic datasets. Recall that the tables show average
values over 10 repetitions, with standard errors of the ClsAcc and SetF1 values
in parentheses, where the best ClsAcc and SetF1 values for each problem
instance (n, p, exp, θ∗) are given in bold. Note also that where the tables show
“>10000.0” in the column labeled “Time,” the computation reached the time
limit of 10000 s at least once out of 10 repetitions.

Table 6 gives the results for the expansion factor exp = 25 and the subset
size θ = θ∗ = 3. When n = 50, our kernel-based MILO method (RMILO-K)
achieved good accuracy for both classiﬁcation (ClsAcc) and subset selection
(SetF1). When n = 100, the kernel-based recursive feature elimination (RFE-
K) performed relatively well. On the whole, the linear-SVM-based MILO
method (MILO-L) performed the worst.

Table 7 gives the results for the expansion factor exp = 25 and the subset
size θ = θ∗ = 5. When n = 50, RMILO-K maintained good accuracy for both
classiﬁcation and subset selection. When n = 100, RFE-K and MILO-L had
the best accuracy for classiﬁcation and subset selection, respectively. However,
RMILO-K selected fewer than half the features selected by MILO-L. Accord-
ingly, it is also the case that RMILO-K delivered overall good performance
with relatively few features.

Table 8 gives the results for the expansion factor exp = 100 and the subset
size θ = θ∗ = 3. In this case, MILO-L outperformed the other kernel-based
methods in terms of the classiﬁcation accuracy. In other words, this dataset
was compatible with linear SVM classiﬁers. On the other hand, the accuracy
for classiﬁcation and subset selection was higher for RMILO-K than for RFE-K
overall.

Table 9 gives the results for the expansion factor exp = 100 and the subset
size θ = θ∗ = 5. In this case, RMILO-K and RFE-K attained good classiﬁcation
accuracy when n = 50 and n = 100, respectively. As for the subset selection
accuracy, although MILO-L had the overall best performance, RMILO-K with
fewer features outperformed RFE-K on the whole.

These results show that our MILO formulation delivers good prediction
performance, especially when there are relatively few data instances. One of the
main reasons for this is that the kernel–target alignment, which is the distance
between the centroids of two response classes, is a performance measure that
is robust against small datasets. Also, our MILO formulation can outperform
recursive feature elimination in terms of the subset selection accuracy.

5 Conclusion

This paper dealt with feature subset selection for nonlinear kernel SVM clas-
siﬁcation. First, we introduced the INLO formulation for computing the best
subset of features based on the kernel–target alignment, which is the distance
between the centroids of two response classes in a high-dimensional feature

Springer Nature 2021 LATEX template

16

Feature subset selection for kernel SVM classiﬁcation

space. Next, we reformulated the problem as a MILO problem and then devised
some problem reduction techniques to solve the problem more eﬃciently.

In computational experiments conducted using real-world and synthetic
datasets, our MILO problems were solved in much shorter times than was the
original INLO problem, and the computational eﬃciency was improved by our
reduced MILO formulation. Our method often attained better classiﬁcation
accuracy than did the linear-SVM-based MILO formulation [49] and recur-
sive feature elimination [31], especially when there were relatively few data
instances.

It is known that feature subset selection for maximizing the kernel–target
alignment leads to nonconvex optimization [56]. To our knowledge, we are the
ﬁrst to transform this subset selection problem into a MILO problem, which
can be solved to optimality using optimization software. Note that if we try
to solve the original nonconvex optimization problem exactly, then we cannot
avoid numerical errors caused by its nonlinear objective function. In contrast,
our method oﬀers globally optimal solutions to small-sized problems without
such numerical errors, and the obtained optimal solutions can be used to eval-
uate the solution quality of other algorithms. We also expect our formulation
techniques to be applicable to other nonconvex optimization problems whose
structures are similar to that of our problem.

A future direction of study will be to develop an eﬃcient algorithm spe-
cialized for directly solving our INLO problem (27)–(29). Another direction of
future research will be to devise MIO formulations for feature subset selection
using other kernel functions or other performance measures of kernel SVM
classiﬁers.

Acknowledgements

This work was partially supported by JSPS KAKENHI Grant Numbers
JP21K04526 and JP21K04527.

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

17

Table 2 Results for Hepatitis dataset: (n, p) = (138, 15)

β Method

ObjVal

OptGap

| ˆS|

Time

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

0.180
0.236
0.236

0.389
0.389
0.411

151.1%
0.0%
0.0%

190.6%
171.6%
0.0%

2 >10000.0
9498.3
3
716.6
3

2 >10000.0
2 >10000.0
2168.6
3

0.000 >1000.0%
0.0%
0.489
0.0%
0.489

0 >10000.0
7127.1
2
1872.9
2

0.000
0.215
0.215

0.344
0.360
0.392

0.406
0.463
0.463

624.8%
0.0%
0.0%

165.7%
229.3%
0.0%

278.8%
162.7%
0.0%

0 >10000.0
7787.1
5
760.5
5

4 >10000.0
5 >10000.0
5135.0
4

1 >10000.0
3 >10000.0
2476.5
2

Table 3 Results for Zoo dataset: (n, p) = (101, 16)

β Method

ObjVal OptGap

| ˆS|

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

0.303
0.303
0.303

0.916
0.916
0.916

1.445
1.445
1.445

0.278
0.278
0.278

0.657
0.726
0.726

1.333
1.333
1.333

0.0%
0.0%
0.0%

0.0%
0.0%
0.0%

0.0%
0.0%
0.0%

1.6%
0.0%
0.0%

32.5%
0.0%
0.0%

8.7%
0.0%
0.0%

Time

1624.7
342.7
55.5

1690.1
388.4
81.5

9489.9
315.3
29.7

3
3
3

3
3
3

2
2
2

5 >10000.0
379.1
5
89.1
5

5 >10000.0
852.9
5
291.2
5

3 >10000.0
503.3
3
62.1
3

θ

3

0.25

1.00

4.00

5

0.25

1.00

4.00

θ

3

0.25

1.00

4.00

5

0.25

1.00

4.00

θ

3

θ

3

Springer Nature 2021 LATEX template

18

Feature subset selection for kernel SVM classiﬁcation

Table 4 Results for Parkinsons dataset: (n, p) = (195, 22)

β Method

ObjVal

OptGap

| ˆS|

Time

0.25

1.00

4.00

5

0.25

1.00

4.00

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

0.000 >1000.0%
0.000 >1000.0%
81.2%
0.284

0 >10000.0
0 >10000.0
3 >10000.0

0.000 >1000.0%
0.000 >1000.0%
206.3%
0.316

0 >10000.0
0 >10000.0
2 >10000.0

0.000 >1000.0%
0.154 >1000.0%
0.000 >1000.0%

0 >10000.0
3 >10000.0
0 >10000.0

0.000 >1000.0%
965.5%
0.127
88.6%
0.251

0 >10000.0
3 >10000.0
5 >10000.0

0.000 >1000.0%
0.160 >1000.0%
268.9%
0.276

0 >10000.0
5 >10000.0
2 >10000.0

0.000 >1000.0%
0.158 >1000.0%
0.000 >1000.0%

0 >10000.0
5 >10000.0
0 >10000.0

Table 5 Results for Soybean dataset: (n, p) = (47, 45)

β Method

ObjVal

OptGap

| ˆS|

Time

0.25

1.00

4.00

5

0.25

1.00

4.00

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

INLO-K
MILO-K
RMILO-K

0.316
0.316
0.316

20.1%
22.7%
0.0%

3 >10000.0
3 >10000.0
346.0
3

0.137 >1000.0%
0.0%
0.926
0.0%
0.926

3 >10000.0
6669.9
3
618.2
3

1.419
1.451
1.451

0.300
0.267
0.300

0.711
0.737
0.870

0.966
1.391
1.391

26.3%
0.0%
0.0%

15.1%
547.6%
0.0%

60.6%
167.1%
0.0%

84.8%
20.9%
0.0%

3 >10000.0
7055.2
3
144.0
3

5 >10000.0
5 >10000.0
2302.6
5

5 >10000.0
5 >10000.0
5054.1
5

4 >10000.0
4 >10000.0
414.7
4

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

19

Table 6 Results for the synthetic dataset (exp = 25 and θ = θ∗ = 3)

n

50

p Method

ClsAcc

SetF1

OptGap

| ˆS|

10 MILO-L

0.867 (±0.007)
RFE-K
0.898 (±0.019)
RMILO-K 0.935 (±0.005)

0.833 (±0.056)
0.667 (±0.055)
0.820 (±0.020)

20 MILO-L

0.849 (±0.010)
0.905 (±0.014)
RFE-K
RMILO-K 0.931 (±0.004)

0.700 (±0.078)
0.690 (±0.052)
0.807 (±0.025)

0.0% 3.0
—
2.3
0.0% 2.1

0.0% 3.0
2.1
—
0.0% 2.2

Time

0.1
13.0
46.8

0.2
13.2
785.2

30 MILO-L

0.837 (±0.011)
RFE-K
0.849 (±0.036)
RMILO-K 0.935 (±0.006)

0.633 (±0.078)
0.647 (±0.062)
0.827 (±0.032)

0.4
0.0% 3.0
—
13.4
1.8
5.3% 2.3 >10000.0

100

10 MILO-L

RFE-K
RMILO-K

0.866 (±0.006)
0.944 (±0.009)
0.934 (±0.010)

0.833 (±0.056)
0.880 (±0.033)
0.810 (±0.043)

20 MILO-L

0.865 (±0.006)
0.922 (±0.018)
RFE-K
RMILO-K 0.933 (±0.010)

0.833 (±0.056)
0.830 (±0.047)
0.810 (±0.043)

0.0% 3.0
—
2.4
0.0% 2.1

0.0% 3.0
2.2
—
0.0% 2.1

0.1
13.0
378.4

0.4
13.2
4660.3

30 MILO-L

RFE-K
RMILO-K

0.866 (±0.006)
0.922 (±0.018)
0.906 (±0.011)

0.833 (±0.056)
0.830 (±0.047)
0.650 (±0.050)

0.0% 3.0
2.2
—

0.4
13.4
130.9% 1.5 >10000.0

Table 7 Results for the synthetic dataset (exp = 25 and θ = θ∗ = 5)

n

p Method

ClsAcc

SetF1

OptGap

| ˆS|

50

10 MILO-L

RFE-K
RMILO-K

0.872 (±0.008)
0.885 (±0.008)
0.875 (±0.011)

0.720 (±0.044)
0.618 (±0.069)
0.661 (±0.030)

20 MILO-L

0.854 (±0.008)
RFE-K
0.860 (±0.007)
RMILO-K 0.873 (±0.011)

0.640 (±0.040)
0.466 (±0.048)
0.661 (±0.030)

0.0% 5.0
2.7
—
0.0% 2.5

0.0% 5.0
—
2.0
0.0% 2.5

Time

<0.1
18.9
72.3

0.2
19.1
3472.5

30 MILO-L

0.849 (±0.007)
RFE-K
0.860 (±0.007)
RMILO-K 0.871 (±0.011)

0.600 (±0.030)
0.466 (±0.048)
0.643 (±0.029)

0.7
0.0% 5.0
—
19.3
2.0
74.6% 2.4 >10000.0

100

10 MILO-L

RFE-K
RMILO-K

0.892 (±0.004)
0.896 (±0.010)
0.886 (±0.010)

0.880 (±0.033)
0.643 (±0.062)
0.643 (±0.029)

0.0% 5.0
—
2.8
0.0% 2.4

0.1
18.9
524.5

20 MILO-L

RFE-K
RMILO-K

0.880 (±0.006)
0.890 (±0.010)
0.879 (±0.009)

0.780 (±0.047)
0.557 (±0.063)
0.625 (±0.027)

0.4
0.0% 5.0
—
19.1
2.2
72.0% 2.3 >10000.0

30 MILO-L

RFE-K
RMILO-K

0.872 (±0.007)
0.887 (±0.009)
0.864 (±0.003)

0.640 (±0.027)
0.538 (±0.053)
0.540 (±0.024)

0.0% 5.0
2.9
—

1.2
19.3
187.5% 2.0 >10000.0

Springer Nature 2021 LATEX template

20

Feature subset selection for kernel SVM classiﬁcation

Table 8 Results for the synthetic dataset (exp = 100 and θ = θ∗ = 3)

n

p Method

ClsAcc

SetF1

OptGap

| ˆS|

50

10 MILO-L

RFE-K
RMILO-K

0.800 (±0.012)
0.720 (±0.019)
0.757 (±0.025)

0.833 (±0.056)
0.687 (±0.044)
0.853 (±0.066)

20 MILO-L

RFE-K
RMILO-K

0.774 (±0.023)
0.707 (±0.022)
0.746 (±0.025)

0.767 (±0.071)
0.640 (±0.055)
0.820 (±0.066)

0.0% 3.0
—
2.6
0.0% 2.6

0.0% 3.0
2.7
—
0.0% 2.6

Time

0.1
13.1
88.3

0.2
13.3
1807.0

30 MILO-L

RFE-K
RMILO-K

0.757 (±0.021)
0.699 (±0.023)
0.699 (±0.020)

0.700 (±0.060)
0.637 (±0.059)
0.653 (±0.057)

0.0% 3.0
2.4
—

0.4
13.5
226.9% 2.6 >10000.0

100

10 MILO-L

RFE-K
RMILO-K

0.816 (±0.008)
0.793 (±0.009)
0.808 (±0.006)

0.867 (±0.054)
0.820 (±0.043)
0.920 (±0.033)

0.0% 3.0
—
2.6
0.0% 2.6

0.1
13.1
645.2

20 MILO-L

RFE-K
RMILO-K

0.810 (±0.008)
0.785 (±0.008)
0.802 (±0.008)

0.833 (±0.056)
0.780 (±0.032)
0.887 (±0.040)

0.0% 3.0
2.4
—

0.5
13.3
134.8% 2.6 >10000.0

30 MILO-L

RFE-K
RMILO-K

0.807 (±0.008)
0.774 (±0.008)
0.726 (±0.028)

0.800 (±0.054)
0.733 (±0.022)
0.687 (±0.072)

0.0% 3.0
2.5
—

0.3
13.5
709.0% 2.1 >10000.0

Table 9 Results for the synthetic dataset (exp = 100 and θ = θ∗ = 5)

n

50

p Method

ClsAcc

SetF1

OptGap

| ˆS|

10 MILO-L

0.793 (±0.008)
0.809 (±0.007)
RFE-K
RMILO-K 0.822 (±0.005)

20 MILO-L

0.780 (±0.007)
RFE-K
0.805 (±0.008)
RMILO-K 0.820 (±0.006)

0.700 (±0.061)
0.535 (±0.047)
0.582 (±0.020)

0.620 (±0.070)
0.498 (±0.040)
0.575 (±0.022)

0.0% 5.0
3.0
—
0.0% 2.2

Time

<0.1
19.2
138.7

0.0% 5.0
2.6
—

0.2
19.4
178.3% 2.3 >10000.0

30 MILO-L

0.770 (±0.008)
RFE-K
0.810 (±0.009)
RMILO-K 0.822 (±0.005)

0.520 (±0.053)
0.513 (±0.038)
0.557 (±0.010)

0.0% 5.0
2.3
—

1.1
19.3
295.3% 2.2 >10000.0

100

10 MILO-L

RFE-K
RMILO-K

0.808 (±0.004)
0.836 (±0.009)
0.827 (±0.007)

0.740 (±0.043)
0.639 (±0.049)
0.571 (±0.000)

0.0% 5.0
—
3.0
0.0% 2.0

0.1
19.2
708.6

20 MILO-L

RFE-K
RMILO-K

0.808 (±0.005)
0.836 (±0.007)
0.826 (±0.007)

0.720 (±0.053)
0.552 (±0.028)
0.571 (±0.000)

0.0% 5.0
3.0
—

0.6
19.5
195.2% 2.0 >10000.0

30 MILO-L

RFE-K
RMILO-K

0.804 (±0.005)
0.830 (±0.007)
0.826 (±0.007)

0.540 (±0.052)
0.518 (±0.030)
0.571 (±0.000)

0.0% 5.0
3.6
—

0.9
19.3
336.5% 2.0 >10000.0

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

21

References

[1] Aizerman, M. A., Braverman, E. M., & Rozonoer, L. I. (1964). Theo-
retical foundations of potential function method in pattern recognition.
Automation and Remote Control, 25(6), 917–936.

[2] Arthanari, T. S. & Dodge, Y. (1981). Mathematical Programming in

Statistics. Wiley.

[3] Aytug, H. (2015). Feature selection for support vector machines using
generalized Benders decomposition. European Journal of Operational
Research, 244(1), 210–218.

[4] Berk, L. & Bertsimas, D. (2019). Certiﬁably optimal sparse principal
component analysis. Mathematical Programming Computation, 11(3),
381–420.

[5] Bertsimas, D. & King, A. (2016). An algorithmic approach to linear

regression. Operations Research, 64(1), 2–16.

[6] Bertsimas, D., King, A., & Mazumder, R. (2016). Best subset selection
via a modern optimization lens. The Annals of Statistics, 44(2), 813–852.

[7] Bertsimas, D. & King, A. (2017). Logistic regression: From art to science.

Statistical Science, 32(3), 367–384.

[8] Bertsimas, D. & Li, M. L. (2020). Scalable holistic linear regression.

Operations Research Letters, 48(3), 203–208.

[9] Bertsimas, D., Pauphilet, J., & Van Parys, B. (2020). Sparse regression:
Scalable algorithms and empirical performance. Statistical Science, 35(4),
555–578.

[10] Bertsimas, D., Pauphilet, J., & Van Parys, B. (2021). Sparse classiﬁcation:
A scalable discrete optimization perspective. Machine Learning, 110(11),
3177–3209.

[11] Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992, July). A training algo-
rithm for optimal margin classiﬁers. In Proceedings of the Fifth Annual
Workshop on Computational Learning Theory (pp. 144–152).

[12] Bradley, P. S. & Mangasarian, O. L. (1998, July). Feature selection via
concave minimization and support vector machines. In Proceedings of the
Fifteenth International Conference on Machine Learning (pp. 82–90).

[13] Cao, B., Shen, D., Sun, J. T., Yang, Q., & Chen, Z. (2007, June). Fea-
ture selection in a kernel space. In Proceedings of the 24th International
Conference on Machine Learning (pp. 121–128).

Springer Nature 2021 LATEX template

22

Feature subset selection for kernel SVM classiﬁcation

[14] Caputo, B., Sim, K., Furesjo, F., & Smola, A. (2002, December).
Appearance-based object recognition using SVMs: Which kernel should I
use? In Proceedings of NIPS Workshop on Statistical Methods for Compu-
tational Experiments in Visual Processing and Computer Vision, Whistler
(Vol. 2002).

[15] Cervantes, J., Garcia-Lamont, F., Rodr´ıguez-Mazahua, L., & Lopez, A.
(2020). A comprehensive survey on support vector machine classiﬁcation:
Applications, challenges and trends. Neurocomputing, 408, 189–215.

[16] Chan, A. B., Vasconcelos, N., & Lanckriet, G. R. (2007, June). Direct con-
vex relaxations of sparse SVM. In Proceedings of the 24th International
Conference on Machine Learning (pp. 145–153).

[17] Chandrashekar, G. & Sahin, F. (2014). A survey on feature selection

methods. Computers & Electrical Engineering, 40(1), 16–28.

[18] Chapelle, O., Vapnik, V., Bousquet, O., & Mukherjee, S. (2002). Choos-
ing multiple parameters for support vector machines. Machine Learning,
46(1), 131–159.

[19] Cozad, A., Sahinidis, N. V., & Miller, D. C. (2014). Learning surrogate
models for simulation-based optimization. AIChE Journal, 60(6), 2211–
2227.

[20] Cristianini, N., Kandola, J., Elisseeﬀ, A., & Shawe-Taylor, J. (2006). On
kernel target alignment. In Innovations in Machine Learning (pp. 205–
256). Springer, Berlin, Heidelberg.

[21] Cristianini, N. & Shawe-Taylor, J. (2000). An Introduction to Support
Vector Machines and Other Kernel-based Learning Methods. Cambridge
University Press.

[22] Dedieu, A., Hazimeh, H., & Mazumder, R. (2021). Learning sparse clas-
siﬁers: Continuous and mixed integer optimization perspectives. Journal
of Machine Learning Research, 22(135), 1–47.

[23] Dua, D. & Graﬀ, C. (2019). UCI Machine Learning Repository [http://
archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of
Information and Computer Science.

[24] Gaudioso, M., Gorgone, E., & Hiriart-Urruty, J. B. (2020). Feature
selection in SVM via polyhedral k-norm. Optimization Letters, 14(1),
19–36.

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

23

[25] Gaudioso, M., Gorgone, E., Labb´e, M., & Rodr´ıguez-Ch´ıa, A. M. (2017).
Lagrangian relaxation for SVM feature selection. Computers & Opera-
tions Research, 87, 137–145.

[26] Ghaddar, B. & Naoum-Sawaya, J. (2018). High dimensional data classi-
ﬁcation and feature selection using support vector machines. European
Journal of Operational Research, 265(3), 993–1004.

[27] Grandvalet, Y. & Canu, S. (2002, January). Adaptive scaling for feature
selection in SVMs. In Proceedings of the 15th International Conference
on Neural Information Processing Systems (pp. 569–576).

[28] Gurobi Optimization (2021). Gurobi Optimizer Reference Manual, version

9.5, Gurobi Optimization.

[29] Guyon, I. & Elisseeﬀ, A. (2003). An introduction to variable and feature
selection. Journal of Machine Learning Research, 3(Mar), 1157–1182.

[30] Guyon, I., Gunn, S., Ben-Hur, A., & Dror, G. (2004). Result analysis of
the NIPS 2003 feature selection challenge. Advances in Neural Information
Processing Systems, 17.

[31] Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. A. (Eds.). (2008). Feature

Extraction: Foundations and Applications (Vol. 207). Springer.

[32] Hastie, T., Tibshirani, R., & Tibshirani, R. J. (2020). Best subset, for-
ward stepwise or lasso? Analysis and recommendations based on extensive
comparisons. Statistical Science, 35(4), 579–592.

[33] Hastie, T., Tibshirani, R., & Wainwright, M. (2019). Statistical Learning
with Sparsity: The Lasso and Generalizations. Chapman and Hall/CRC.

[34] Hazimeh, H. & Mazumder, R. (2020). Fast best subset selection: Coordi-
nate descent and local combinatorial optimization algorithms. Operations
Research, 68(5), 1517–1537.

[35] Hazimeh, H., Mazumder, R., & Saab, A. (2021). Sparse regression at
scale: Branch-and-bound rooted in ﬁrst-order optimization. Mathematical
Programming, to appear.

[36] Hermes, L. & Buhmann, J. M. (2000, September). Feature selection for
support vector machines. In Proceedings 15th International Conference
on Pattern Recognition. ICPR-2000 (Vol. 2, pp. 712–715). IEEE.

[37] Huang, C. L. & Wang, C. J. (2006). A GA-based feature selection and
parameters optimization for support vector machines. Expert Systems
with Applications, 31(2), 231–240.

Springer Nature 2021 LATEX template

24

Feature subset selection for kernel SVM classiﬁcation

[38] IBM (2020). IBM ILOG CPLEX Optimization Studio 20.1.0 [https://

www-01.ibm.com/software/commerce/optimization/cplex-optimizer/].
IBM.

[39] Jim´enez-Cordero, A., Morales, J. M., & Pineda, S. (2021). A novel embed-
ded min-max approach for feature selection in nonlinear support vector
machine classiﬁcation. European Journal of Operational Research, 293(1),
24–35.

[40] Kira, K. & Rendell, L. A. (1992, July). The feature selection problem:
Traditional methods and a new algorithm. In Proceedings of the Tenth
National Conference on Artiﬁcial Intelligence (pp. 129–134).

[41] Konno, H. & Yamamoto, R. (2009). Choosing the best set of vari-
ables in regression analysis using integer programming. Journal of Global
Optimization, 44(2), 273–282.

[42] Kudo, K., Takano, Y., & Nomura, R. (2020). Stochastic discrete ﬁrst-order
algorithm for feature subset selection. IEICE Transactions on Information
and Systems, 103(7), 1693–1702.

[43] Labb´e, M., Mart´ınez-Merino, L. I., & Rodr´ıguez-Ch´ıa, A. M. (2019).
Mixed integer linear programming for feature selection in support vector
machine. Discrete Applied Mathematics, 261, 276–304.

[44] Le Thi, H. A., Le, H. M., & Dinh, T. P. (2015). Feature selection in
machine learning: An exact penalty approach using a diﬀerence of convex
function algorithm. Machine Learning, 101(1), 163–186.

[45] Lee, I. G., Zhang, Q., Yoon, S. W., & Won, D. (2020). A mixed inte-
ger linear programming support vector machine for cost-eﬀective feature
selection. Knowledge-Based Systems, 203, 106145.

[46] Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R. P., Tang, J., &
Liu, H. (2017). Feature selection: A data perspective. ACM Computing
Surveys, 50(6), 94:1–94:45.

[47] Liu, H. & Motoda, H. (Eds.). (2007). Computational Methods of Feature

Selection. CRC Press.

[48] Maldonado, S. & L´opez, J. (2018). Dealing with high-dimensional class-
imbalanced datasets: Embedded feature selection for SVM classiﬁcation.
Applied Soft Computing, 67, 94–105.

[49] Maldonado, S., P´erez, J., Weber, R., & Labb´e, M. (2014). Feature selec-
tion for support vector machines via mixed integer linear programming.
Information Sciences, 279, 163–175.

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

25

[50] Maldonado, S. & Weber, R. (2009). A wrapper method for feature
selection using support vector machines. Information Sciences, 179(13),
2208–2217.

[51] Maldonado, S., Weber, R., & Basak, J. (2011). Simultaneous feature selec-
tion and classiﬁcation using kernel-penalized support vector machines.
Information Sciences, 181(1), 115–128.

[52] Mangasarian, O. L. & Kou, G. (2007, October). Feature selection for
nonlinear kernel support vector machines. In Seventh IEEE International
Conference on Data Mining Workshops (ICDMW 2007) (pp. 231–236).
IEEE.

[53] Miyashiro, R. & Takano, Y. (2015). Subset selection by Mallows’ Cp: A
mixed integer programming approach. Expert Systems with Applications,
42(1), 325–331.

[54] Miyashiro, R. & Takano, Y. (2015). Mixed integer second-order cone
programming formulations for variable selection in linear regression.
European Journal of Operational Research, 247(3), 721–731.

[55] Naganuma, M., Takano, Y., & Miyashiro, R. (2019). Feature subset
selection for ordered logit model via tangent-plane-based approximation.
IEICE Transactions on Information and Systems, 102(5), 1046–1053.

[56] Neumann, J., Schn¨orr, C., & Steidl, G. (2005). Combined SVM-based
feature selection and classiﬁcation. Machine Learning, 61(1–3), 129–150.

[57] Saishu, H., Kudo, K., & Takano, Y. (2021). Sparse Poisson regression via

mixed-integer optimization. PloS ONE, 16(4), e0249916.

[58] Sato, T., Takano, Y., & Miyashiro, R. (2017). Piecewise-linear approxi-
mation for feature subset selection in a sequential logit model. Journal of
the Operations Research Society of Japan, 60(1), 1–14.

[59] Sato, T., Takano, Y., Miyashiro, R., & Yoshise, A. (2016). Feature
subset selection for logistic regression via mixed integer optimization.
Computational Optimization and Applications, 64(3), 865–880.

[60] Sch¨olkopf, B. & Smola, A. J. (2002). Learning with Kernels: Support
Vector Machines, Regularization, Optimization, and Beyond. MIT Press.

[61] Shawe-Taylor, J. & Cristianini, N. (2004). Kernel Methods for Pattern

Analysis. Cambridge University Press.

[62] Sun, J., Zheng, C., Li, X., & Zhou, Y. (2010). Analysis of the distance
between two classes for tuning SVM hyperparameters. IEEE Transactions

Springer Nature 2021 LATEX template

26

Feature subset selection for kernel SVM classiﬁcation

on Neural Networks, 21(2), 305–318.

[63] Takano, Y. & Gotoh, J. (2011). A nonlinear control policy using kernel
method for dynamic asset allocation. Journal of the Operations Research
Society of Japan, 54(4), 201–218.

[64] Takano, Y. & Gotoh, J. (2014). Multi-period portfolio selection using
kernel-based control policy with dimensionality reduction. Expert Sys-
tems with Applications, 41(8), 3901–3914.

[65] Takano, Y. & Miyashiro, R. (2020). Best subset selection via cross-

validation criterion. TOP, 28(2), 475–488.

[66] Tamura, R., Kobayashi, K., Takano, Y., Miyashiro, R., Nakata, K., &
Matsui, T. (2017). Best subset selection for eliminating multicollinearity.
Journal of the Operations Research Society of Japan, 60(3), 321–336.

[67] Tamura, R., Kobayashi, K., Takano, Y., Miyashiro, R., Nakata, K., &
Matsui, T. (2019). Mixed integer quadratic optimization formulations for
eliminating multicollinearity based on variance inﬂation factor. Journal
of Global Optimization, 73(2), 431–446.

[68] Thompson, M. E. (2006). NDCC: Normally distributed clustered datasets
on cubes [www.cs.wisc.edu/dmi/svm/ndcc/]. Computer Sciences Depart-
ment, University of Wisconsin, Madison.

[69] Ustun, B. & Rudin, C. (2016). Supersparse linear integer models for
optimized medical scoring systems. Machine Learning, 102(3), 349–391.

[70] Vapnik, V. (1998). Statistical Learning Theory. Wiley Interscience.

[71] Wainer, J. & Fonseca, P. (2021). How to tune the RBF SVM hyper-
parameters? An empirical evaluation of 18 search algorithms. Artiﬁcial
Intelligence Review, 1–27.

[72] Wang, L. (2008). Feature selection with kernel class separability. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 30(9), 1534–
1546.

[73] Wang, T., Zhao, D., & Tian, S. (2015). An overview of kernel alignment
and its applications. Artiﬁcial Intelligence Review, 43(2), 179–192.

[74] Watanabe, A., Tamura, R., Takano, Y., & Miyashiro, R. (2021). Branch-
and-bound algorithm for optimal sparse canonical correlation analysis.
Optimization Online.

[75] Weston, J., Elisseeﬀ, A., Sch¨olkopf, B., & Tipping, M. (2003). Use of the
zero norm with linear models and kernel methods. The Journal of Machine

Springer Nature 2021 LATEX template

Feature subset selection for kernel SVM classiﬁcation

27

Learning Research, 3, 1439–1461.

[76] Weston, J., Mukherjee, S., Chapelle, O., Pontil, M., Poggio, T., & Vapnik,
V. (2000, January). Feature selection for SVMs. In Proceedings of the
13th International Conference on Neural Information Processing Systems
(pp. 647–653).

[77] Zhu, J., Rosset, S., Hastie, T., & Tibshirani, R. (2003, December). 1-
norm support vector machines. In Proceedings of the 16th International
Conference on Neural Information Processing Systems (pp. 49–56).

