Noname manuscript No.
(will be inserted by the editor)

Automatically Categorising GitHub Repositories by
Application Domain

Francisco Zanartu · Christoph
· Bruno Cartaxo · Hudson
Treude (cid:0)
Silva Borges · Pedro Moura · Markus
Wagner · Gustavo Pinto

the date of receipt and acceptance should be inserted later

Abstract GitHub is the largest host of open source software on the Internet.
This large, freely accessible database has attracted the attention of practition-
ers and researchers alike. But as GitHub’s growth continues, it is becoming
increasingly hard to navigate the plethora of repositories which span a wide
range of domains. Past work has shown that taking the application domain into
account is crucial for tasks such as predicting the popularity of a repository
and reasoning about project quality. In this work, we build on a previously an-
notated dataset of 5,000 GitHub repositories to design an automated classiﬁer
for categorising repositories by their application domain. The classiﬁer uses
state-of-the-art natural language processing techniques and machine learning

Francisco Zanartu
The University of Adelaide, Australia
E-mail: francisco.zanartu@adelaide.edu.au

(cid:0) Corresponding author - Christoph Treude
The University of Melbourne, Australia
E-mail: christoph.treude@unimelb.edu.au

Bruno Cartaxo
Federal Institute of Pernambuco, Brazil
E-mail: email@brunocartaxo.com

Hudson Silva Borges
Federal University of Mato Grosso do Sul, Brazil
E-mail: hsborges@facom.ufms.br

Pedro Moura
Federal Institute of Pernambuco, Brazil
E-mail: ppomoura@gmail.com

Markus Wagner
The University of Adelaide, Australia
E-mail: markus.wagner@adelaide.edu.au

Gustavo Pinto
Federal University of Par´a, Brazil & Zup Innovation, Brazil
E-mail: gpinto@ufpa.br

2
2
0
2

l
u
J

0
3

]
E
S
.
s
c
[

1
v
9
6
2
0
0
.
8
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Francisco Zanartu et al.

to learn from multiple data sources and catalogue repositories according to
ﬁve application domains. We contribute with (1) an automated classiﬁer that
can assign popular repositories to each application domain with at least 70%
precision, (2) an investigation of the approach’s performance on less popular
repositories, and (3) a practical application of this approach to answer how the
adoption of software engineering practices diﬀers across application domains.
Our work aims to help the GitHub community identify repositories of interest
and opens promising avenues for future work investigating diﬀerences between
repositories from diﬀerent application domains.

1 Introduction and Motivation

GitHub is the largest host of open source code on the Internet (Kim et al.,
2021) and has piqued the interest of practitioners and researchers, many of
whom have struggled to bring structure to the plethora of repositories avail-
able on the platform. For example, there are limited means available to sep-
arate repositories containing engineered software projects from other repos-
itories, such as personal projects or those that use GitHub for free cloud
storage (Kalliamvakou et al., 2014; Munaiah et al., 2017). To make it easier
for users to identify relevant repositories for their wide variety of use cases,
GitHub has been adding features to its service, such as README ﬁles, top-
ics tags, and showcases (where contributors describe, add keywords, and label
their repository). However, these features are insuﬃcient for many use cases.
For example, while achieving generalizability of the results is the primary
objective of many empirical papers, modern computing research is largely ap-
plication domain independent (Capiluppi et al., 2020). Application domains
are the sections of reality for which a software system is designed. Their im-
portance relies on their serving as the starting point for actual state analysis
and usually includes domain-speciﬁc language, meaning that developers in this
domain think about their project in a speciﬁc way, with particular terms and
concepts (Z¨ullighoven, 2004). Application domains are not a feature currently
implemented by GitHub to catalogue repositories. Previous work has found
that repository quality indicators, such as object-oriented metrics, can be “ex-
tremely sensitive to application domains” (Capiluppi and Ajienka, 2019), and
that the application domain is an important factor in predicting repository
popularity (Borges et al., 2016). Furthermore, since documentation of GitHub
repositories is often incomplete (Prana et al., 2019), information about the
application domain of a repository can be crucial to gain a high-level under-
standing of its content and purpose. Borges et al. (2016) manually annotated
5,000 GitHub repositories according to their application domain, distinguish-
ing repositories into Application Software, System Software, Web
Libraries and Frameworks, Non-Web Libraries and Frameworks,
Software Tools, and Documentation. In this paper, we use this previ-
ously annotated dataset to develop a machine learning solution to automati-

Automatically Categorising GitHub Repositories by Application Domain

3

cally classify GitHub repositories by their application domain. In particular,
our contributions are the following:

– an automated classiﬁer which can assign popular GitHub repositories to an
application domain with at least 70% precision for each of the application
domains;

– an investigation of the performance of the approach on a newly annotated

dataset of less popular repositories; and

– a study that puts our classiﬁer into practice by investigating how recent and
popular repositories, associated with diﬀerent application domains, diﬀer
in their adoption of software engineering practices, such as automation,
refactoring, and code ownership.

Our work aims to help researchers and practitioners identify rele-
vant repositories for their use cases, oﬀering a trade-oﬀ between accu-
racy and volume to many who have manually annotated repositories using
Borges et al. (2016)’s classiﬁcation (Bi et al., 2021, 2022; Coelho and Valente,
2017; De Stefano et al., 2020, 2022; Hata et al., 2022; Hayashi et al., 2019;
Rehman et al., 2022; Tamburri et al., 2019) or intend to use it in future
work (Bayati and Heidary, 2019; De Stefano et al., 2022; Wu, 2020; Wu et al.,
2020a,b; Zhang et al., 2020). For example, Bi et al. (2022) manually annotated
1,000 repositories in terms of their application domain using the Borges et
al. taxonomy to investigate release note production and usage of release notes
in practice, and Hayashi et al. (2019) manually annotated 969 repositories in
terms of their application domain to investigate the impacts of daylight saving
time on software development. Both of these had to invest substantial manual
eﬀort in the annotation task that our work automates. In addition, our inves-
tigation of diﬀerences in the adoption of software engineering practises across
application domains opens up promising avenues for future work, aiding in
the achievement of generalisable results for the selected software engineering
domain. All data and scripts are available on Zenodo.1

The remainder of this paper is structured as follows. We introduce the
dataset used as the starting point of this work in Section 2 and our research
questions in Section 3. We outline our methods for data collection, data pre-
processing, and data analysis in Section 4 and also provide a characterisation
of the data in the same section. Sections 5 through 6 provide answers to our
research questions. Section 7 discusses our results and Section 8 the threats
to the validity of this work, and Section 9 places our work in the context of
existing literature. Section 10 concludes this paper and describes future work.

2 Application Domains

We use the existing dataset curated by Borges et al. (2016) as the starting
point for our work to build an automated classiﬁer for categorising GitHub

1 https://doi.org/10.5281/zenodo.6423599

4

Francisco Zanartu et al.

Web Libraries and Frameworks

Non-Web Libraries and Frameworks

1,522

1,429

Software Tools

963

Application Software

Documentation

428

427

System Software

179

Fig. 1 Distribution of application domains in the original dataset

repositories. To help repository owners and clients understand how their repos-
itories are performing in a competitive open-source development market, the
authors developed a multiple linear regression classiﬁer to predict the number
of stars of GitHub repositories. One of the features used in their prediction
model is the repositories’ application domain, which the authors determined
by manually annotating 5,000 repositories with their respective application
domain. Figure 1 summarises the distribution of the application domains in
this dataset.

The dataset contains eight features for each repository in the sample. In
addition to stars and the application domain, it includes repository names, the
number of forks, the main programming language, a short description, URL,
and the growth pattern. The six application domains are explained by the
authors as follows (Borges et al., 2016):

– Application Software: systems that provide functionalities to end-
users, like browsers and text editors (e.g., wordpress/wordpress and
adobe/brackets).

– System Software: systems that provide services and infrastructure to
other systems, like operating systems, middleware, servers, and databases
(e.g., torvalds/linux and mongodb/mongo).

– Web Libraries and Frameworks (e.g., twbs/bootstrap and angu-

lar/angular.js).

– Non-Web Libraries and Frameworks (e.g., google/guava and

facebook/fresco).

– Software Tools: systems that support software development tasks, like
IDEs, package managers, and compilers (e.g., homebrew/homebrew and
git/git).

– Documentation: repositories with documentation, tutorials, source code

examples, etc. (e.g., iluwatar/java-design-patterns).

We use this annotated dataset for our work on developing an automated
classiﬁer for the application domain of a GitHub repository and as the starting
point of our investigation of diﬀerences among the application domains. The
Borges et al. dataset is publicly available on Zenodo.2

2 https://doi.org/10.5281/zenodo.804474

Automatically Categorising GitHub Repositories by Application Domain

5

3 Research Questions

We ask two research questions (RQ) to guide this work, focusing on auto-
matically classifying GitHub repositories and evaluating the performance and
potential applications of the classiﬁer.
RQ1 How accurately can we automatically classify GitHub repositories?

We investigate the classiﬁer performance on the dataset annotated by
Borges et al. (2016) and on our newly annotated dataset of less popular reposi-
tories, since the original dataset considered only the most popular. In addition,
we investigate to what extent the classiﬁer performance improves if we merge
closely related classes that cause confusion for the classiﬁer and the human
annotators.
RQ1.1 What is the impact of adding diﬀerent data sources on the classiﬁer

performance?

RQ1.2 How accurately can state-of-the-art machine learning techniques auto-

matically classify popular GitHub repositories?

RQ1.3 What is the impact of merging closely related classes of GitHub repos-

itories?

RQ1.4 What is the importance of diﬀerent features for the classiﬁcation?
RQ1.5 How accurately can state-of-the-art machine learning techniques auto-

matically classify less popular GitHub repositories?

The results of RQ1 provide an understanding of the features that the model
considers critical to obtaining the results. These ﬁndings should be taken into
account when explaining results in repositories that may contain incomplete
documentation, lowering the classiﬁer’s performance.

RQ2 investigates diﬀerences between repositories from diﬀerent application

domains from the point of view of practical applications of the classiﬁer:

RQ2 How does the adoption of software engineering practices diﬀer between

application domains?

The research question focuses on automation, refactoring, and code own-
ership, which are some of the most widely adopted software engineering prac-
tices. We pose distinct RQs for each. Ultimately, RQ2 aims to demonstrate
how the classiﬁcation of GitHub repositories could be used to answer addi-
tional research questions in the future.

RQ2.1 How does the adoption of GitHub Actions diﬀer between application

domains?

RQ2.2 How does the extent of refactoring diﬀer between application domains?
RQ2.3 How does code ownership diﬀer between application domains?

4 Methodology

In this section, we describe our procedure for data collection and characterise
the collected data. We also outline our methods for data pre-processing and
data analysis.

6

Francisco Zanartu et al.

Table 1 Textual and categorical features of GitHub repositories

Feature

Description

Description

README File

Topics

Licence

A short text that should brieﬂy tell the pub-
lic what it is contained in the repository.
The text ﬁle that GitHub shows when
someone navigates to a repository. It com-
monly contains a set of helpful information
about the repository and usage informa-
tion.
Topics are labels that let users explore
repositories by those labels, thus creating
subject-based connections between GitHub
repositories.
A licence is a fundamental part of the deﬁ-
nition of open-source software.

Labels

Prog. Languages GitHub provides repository statistics about
the programming languages used in ﬁles
within a repository.
Labels are intended to help users categorise
issues, pull requests, and discussions. There
is also a list of nine default labels that cre-
ate a standard workﬂow in a repository,
such as a bug, documentation, and dupli-
cate.
Contributors are the usernames of non-
anonymous users who contribute to GitHub
repositories.
Several ﬁles and folders could make up a
repository; this feature lists all the sub-
folder and ﬁle names in the root folder.

Sub-folders/ﬁles

Contributors

4.1 Data collection

We use the Borges et al. (2016) data set as the basis of our work. We then
used the GitHub API3 via the PyGitHub library4 to enrich and expand this
dataset.

Our choice of features to extract for each repository was inspired by previ-
ous work on processing GitHub repositories, such as Prana et al. (2019) and
Zhang et al. (2017).

Table 1 provides a description of the text and categorical features that we
obtained from GitHub to construct our dataset. README ﬁles, descriptions,
and labels are further processed using natural language processing techniques,
which we will describe below.

We also consider numeric features such as releases, stars, and forks, sum-

marised in Table 2.

3 https://docs.github.com/en/rest
4 https://github.com/PyGithub/PyGithub

Automatically Categorising GitHub Repositories by Application Domain

7

Table 2 Numerical features of GitHub repositories

Feature

Description

Stars

Releases Releases are tags that mark a speciﬁc point
in the repository history. This feature is the
count of those particular points.
A star is a bookmark or display of appreci-
ation for a repository.
Forks are individual copies of another user’s
repository. Forking allows making changes
to the source code without aﬀecting the
original root repository.

Forks

Table 3 Removal of repositories no longer available

Application Domain

Samples

% Previous Diﬀ.

Web Libs & Frameworks
Non-Web Libs & Frameworks
Software Tools
Application Software
Documentation
System Software

1,522
1,429
963
428
427
179

31
29
19
9
9
4

Total

4,948

100

1,535
1,439
972
437
433
184

5,000

13
10
9
9
6
5

52

4.2 Data characterisation

After completing the data collection described in the previous section, we
conducted a manual inspection to remove repositories from the dataset that
are no longer maintained, as indicated by descriptions or README ﬁles where
the original content was replaced with messages such as “This repository is
no longer maintained, has been removed, or is deprecated.” This step did not
substantially aﬀect the distribution of application domains, but was necessary
to avoid misleading values in the training data, such as empty repositories
labelled with a particular application domain. As a result of this step, the
sample was reduced to 4,948, as shown in Table 3.

Figure 1 shows that the dataset is imbalanced. The most frequent appli-
cation domain, Web Libraries and Frameworks, as well as Non-Web
Libraries and Frameworks, account for 60% of the dataset. The least fre-
quent application domain, System Software, accounts for only 4% of the
dataset.

To further characterise the data, Table 4 shows how many values are miss-
ing because the owners of the repository did not provide this information. The
table supports Zhang et al. (2019)’s discovery that the majority of GitHub
repositories lack topic tags.

8

Francisco Zanartu et al.

Table 4 Missing values per feature

Data source

Values Missing

(%)

Description
README File
Topics
Licence
Programming Languages
Labels
Contributors
Sub-folders/ﬁles
Releases
Stars
Forks

4,914
4,948
2,335
4,301
4,898
4,720
4,942
4,948
4,948
4,948
4,948

34
0
2,613
647
50
228
6
0
0
0
0

0.7
0
52.8
13.1
1
4.6
0.1
0
0
0
0

batchﬁle
dockerﬁle
makeﬁle
ruby

c
html
objective-c
shell

c++ css
java
other

javascript
python

1

0.8

0.6

0.4

0.2

0

a

t

n

s

u m e
L i b

c

b

D o
W e

n

t i o
F r

&

r

s
k
f t w a

e

r

a m e w o
S

o

n

N o

o

o l s
e m S
L i b
b

f t w a
&

s

s

n

k

r
t i o

e

r
a m e w o
F r
a
p l i c
A p

T o
t
s
y
- W e

S

e

r

f t w a

o

S

Fig. 2 Most frequent programming languages per application domain

Most of the GitHub repositories in our dataset use a licence to share their
code.5 They use ﬁve programming languages on average, contain 22 sub-folders
and ﬁles in their root folder, and have 95 non-anonymous contributors per
repository. Table 5 summarises this information and displays the number of
unique words provided by each data source, with “Sub-folders/ﬁles” and “Con-
tributors” being high-dimensional data sources. Note that each word in each
of the textual sources potentially becomes a feature used in the classiﬁer, i.e.,
we employed one-hot encoding.

5 Note that we only count repositories that have GitHub’s licence feature activated. Some

repositories declare their licence informally inside of a README ﬁle.

Automatically Categorising GitHub Repositories by Application Domain

9

apache 2.0
bsd 3-cl new/revised
gnu aﬀ. gen. pub. 3.0
gnu general public 3.0 mit
no licence

other

bsd 2-cl ‘simpl.’
cc zero 1.0 universal
gnu general pub. 2.0

1

0.8

0.6

0.4

0.2

0

a

t

n

s

u m e
L i b

c

b

D o
W e

n

t i o
F r

&

r

s
k
f t w a

e

r

a m e w o
S

o

n

N o

o

o l s
e m S
L i b
b

f t w a
&

s

s

n

k

r
t i o

e

r
a m e w o
F r
a
p l i c
A p

T o
t
s
y
- W e

S

e

r

f t w a

o

S

Fig. 3 Most frequent licences per application domain

The programming languages used might also be an indicator of the applica-
tion domain of a repository. For example, Web Libraries and Frameworks
account for JavaScript, HTML, and CSS as the languages that are the most
used. System Software, on the other hand, comes in a variety of languages
(e.g., Shell, Ruby, C, C++). However, the other application domains do not
diﬀer substantially. Figure 2 presents this information in visual form.

Figure 3 shows that the MIT licence is the most frequently adopted li-
cence in the dataset, with the exception of the application domain System
Software. Documentation repositories frequently lack a licence.

Table 5 shows that Software Tools and System Software are the ap-
plication domains with the most non-anonymous contributors. The box plot
in Figure 4 shows the number of non-anonymous contributors per application
domain, with a pronounced presence of outliers. Only System Software
appears to be free of outliers, which is likely due to the small sample size (4%
of the dataset). Contributors are a critical component of collaborative open-
source software development. While the majority of non-anonymous contribu-
tors have made few contributions, others collaborate on multiple repositories.
These contributors, in particular, are of interest for this research because we
might be able to establish that a particular contributor is more likely to con-
tribute to repositories of a particular application domain.

With this data characterisation in mind, we next identify suitable pre-
processing techniques for each feature or group of features in such a way that
could be interpretable for a machine learning classiﬁer. At ﬁrst sight, we can
identify text data from README ﬁles, repository description, and the labels

10

Francisco Zanartu et al.

Table 5 Values of textual features

s
e
g
a
u
g
n
a
L

6
4
4
5
10
3

e
c
n
e
c
i
L

1
1
1
1
1
1

s
c
i
p
o
T

4
3
3
3
4
3

s
e
l
ﬁ
/
s
r
e
d
l
o
f
-
b
u
S

21
23
18
22
29
18

s
r
o
t
u
b
i
r
t
n
o
C

87
76
72
105
147
83

Application Software
Documentation
Non-Web Libs & Frameworks
Software Tools
System Software
Web Libs & Frameworks

Average number of features
Unique features

5
336

1
24

3
6,036

22
27,752

95
239,364

Fig. 4 Non-anonymous contributors per application domain

used to classify issues, pull requests, and discussions. Contributors, program-
ming languages, topic keywords, sub-folders/ﬁles names, and type of licence
can be interpreted as categorical data for each repository, and ﬁnally, as nu-
merical data, we will use the number of forks, stars, and releases. Hence, the
challenge for our next step will be how to pre-process and combine all these
features into an interpretable dataset for our classiﬁer.

4.3 Data pre-processing

Textual data, i.e., README ﬁles, repository descriptions, and labels, require
the application of natural language processing (NLP) techniques. There are
several techniques for pre-processing and analysing natural language data,
such as Bag-of-words, TF-IDF, or Word2Vec, as well as transformer mod-

Automatically Categorising GitHub Repositories by Application Domain

11

els, which have achieved state-of-the-art performance in a wide range of
NLP tasks (Devlin et al., 2018; Vaswani et al., 2017). The Sentence Trans-
formers Library is a low-cost option for obtaining the embedding represen-
tation of our text data that is not big enough to train a deep learning
model (Reimers and Gurevych, 2019).

Before pre-processing text data, the README ﬁles are cleaned of HTML
and Markdown tags as well as duplicated white spaces. Then, the embed-
ding representation is obtained using the pre-trained model ‘all-mpnet-base-
v2’ which is an all-round model tuned from many use cases and exhibits the
best average performance on a diverse benchmark dataset. This model returns
a 768-dimensional vector for each sample.

We note that categorical data, such as topic labels assigned by reposi-
tory maintainers, is mostly sparse due to the employed one-hot encoding,
thus contradicting our objective of ﬁnding recurrent features that correlate
with our application domains. Therefore, we perform feature selection, which
has the added beneﬁt of decreasing the chances of overﬁtting and leads to
simpler models that generalise better (M¨uller and Guido, 2016). According
to the Scikit-learn library (Pedregosa et al., 2011), model-based feature selec-
tion keeps only the most important features by studying not only univariate
statistically signiﬁcant relationships between each feature and label, but also
the combination of all features with an importance measure greater than the
provided threshold. We chose LinearSVM with C = 0.01 as the threshold to
reduce sparsity in this work.

For contributors, we only used the 50 most proliﬁc ones from each ap-
plication domain as input to our model. Unsurprisingly, the distribution of
contributions to repositories in diﬀerent application domains follows a power-
law distribution, with the vast majority of developers only contributing to a
single repository and thus not providing us with useful information to learn
from for classifying other repositories.

4.4 Data analysis

the
To ﬁnd an adequate solution for RQ1, we needed to address
problem of combined algorithm selection and hyperparameter
tuning
(CASH) (Thornton et al., 2013). The solution to the CASH problem intends
to address two problems for automated machine learning (AutoML) methods:
There is no unique machine learning algorithm that performs well on every
dataset, and some machine learning algorithms need hyper-parameter optimi-
sation to achieve good results (Feurer et al., 2019).

AutoML libraries usually require multiple trials of diﬀerent conﬁgurations
or long training times to ﬁnd accurate models and outperform a tuned random
forest baseline benchmark (Gijsbers et al., 2019). In this context, a new state-
of-the-art approach, the fast and lightweight AutoML library (FLAML), has
been shown to signiﬁcantly outperform the top ranked AutoML libraries on

12

Francisco Zanartu et al.

Table 6 Impact of adding diﬀerent data sources

Data Source

Precision Recall

F1 score Accuracy

Description only
README only
Textual data only
Textual and categorical data
Textual, categorical and numerical data

0.58
0.54
0.64
0.64
0.70

0.50
0.48
0.55
0.58
0.58

0.52
0.50
0.56
0.60
0.61

0.61
0.65
0.67
0.69
0.72

a large set of open-source AutoML benchmarks (Wang et al., 2021). We use
FLAML to answer RQ1.

To answer RQ1, we investigate which algorithm FLAML has chosen as the
most suitable for our dataset and then analyse feature importance measures
appropriate for this algorithm. The methodology to answer RQ2 is described
in detail in Section 6.

5 RQ1: Classiﬁer Accuracy

In this section, we provide the answer to our ﬁrst research question: How
accurately can we automatically classify GitHub repositories?

5.1 RQ1.1 What is the impact of adding diﬀerent data sources on classiﬁer
performance?

As described in Section IV-D, we used FLAML for our experiments. To inves-
tigate the impact of adding diﬀerent data sources as input to the classiﬁcation,
we begin with only description tokens and then concatenate additional sources
such as README ﬁles and labels as text data. Then we combine text data
with categorical variables, i.e., contributors, programming languages, topics,
contents, and licences, and ﬁnally we combine text data, categorical data, and
numerical data, i.e., forks, stars, and releases.

Table 6 shows the results of these experiments. FLAML was conﬁgured
to run for 1,000 seconds with 10-fold cross-validation, and due to the imbal-
anced nature of this dataset, we chose the macro F1 score as the validation
metric (He and Ma, 2013). Although the complexity of the data increases as
more features are added, the evaluation metric improves steadily as more fea-
tures are added. For all the described scenarios, FLAML chose Light Gradient
Boosted Machine (LGBM)6 as the best estimator over Random forest, XG-
boost, Extra tree, and Linear regression, based on a 90/10 training testing
split. The resulting dataset is a 1,084 dimensional vector for each of the 4,948
samples.

6 https://lightgbm.readthedocs.io/en/latest/

Automatically Categorising GitHub Repositories by Application Domain

13

Table 7 Confusion matrix achieved with FLAML, column labels are ordered in the same
way as row labels

Application Software
Documentation
Non-Web Libs & Frameworks
Software Tools
System Software
Web Libs & Frameworks

20
1
5
3
0
1

0
26
4
3
1
3

8
2
108
10
3
14

11
2
10
69
7
7

0
0
2
1
6
0

4
12
14
10
1
127

Table 8 Final classiﬁer performance

Application Domain

Precision Recall

F1 score

Application & System Software
Documentation
Non-Web Libs & Frameworks
Software Tools
Web Libs & Frameworks

0.73
0.84
0.76
0.72
0.74

0.59
0.74
0.77
0.66
0.86

0.65
0.79
0.77
0.68
0.80

5.2 RQ1.2: How accurately can state-of-the-art machine learning techniques
automatically classify popular GitHub repositories?

From answering the previous research question, we learned that more features
will likely improve the classiﬁer’s performance, but since adding features also
leads to a more complex model, we explore the impact of ﬁne-tuning the clas-
siﬁer in this research question. Furthermore, we use the synthetic minority
oversampling technique (SMOTE) to handle the imbalance of the training
data (Lemaˆıtre et al., 2017). SMOTE balanced the samples to 1,370 for each
class, and since we now have a balanced dataset to train, we changed the vali-
dation metric to the area under the receiver operating curve (ROC-AUC) in its
one-vs-rest conﬁguration (OVR). ROC-AUC-OVR helps maximise the detec-
tion of correct samples (Fawcett, 2006). As described in Section 4.4, we used
FLAML for our experiments. Based on the results of RQ1, we also restricted
the search space to only LGBM and lengthened the search time to 30,000
seconds. Table 7 shows the initial confusion matrix achieved with FLAML,
for an overall accuracy of 0.72 based on a 90/10 training testing split. The
table reveals that much of the confusion was between the application domains
for Application Software and Software Tools—the precision of the
approach in predicting these application domains is below 0.7 and a macro
averaged F1 score of 0.64. If we compare our test results with a ZeroR clas-
siﬁer, i.e., a classiﬁer that always selects the most frequent class, LGBM was
roughly 2.3 times more accurate (0.72 vs. 0.31) than ZeroR.

5.3 RQ1.3: What is the impact of merging classes of GitHub repositories?

Taking into account that the deﬁnitions of Application Software and
Software Tools are closely related (see Section 2, Software Tools are

14

Francisco Zanartu et al.

Table 9 Feature importance

Feature Group

Example

Importance

Textual features
Categorical features
Numerical features

README ﬁle
Sub-folders/ﬁles
Stars

94.9%
4.0%
1.1%

essentially a sub-category of Application Software), and that these two
application domains caused confusion for our classiﬁers, we decided to merge
those two application domains and rename them Application & System
Software. Note that both application domains account for a relatively small
number of repositories. Anecdotally, we also observed diﬃculties in annotating
these as separate application domains during our annotation to answer RQ1.4.
We run FLAML with the same conﬁguration as RQ1.2, and the synthetic bal-
ancing via SMOTE was still applied to the dataset.

Table 8 shows that the results now improve considerably: the precision
values for all application domains are above 0.7, with an overall accuracy of
0.75 and a macro averaged F1 score of 0.74. LGBM was approximately 2.4
times more accurate (0.75 vs. 0.31) than the ZeroR classiﬁer.

5.4 RQ1.4: Feature Importance

Table 9 shows the importance of the diﬀerent feature groups for the clas-
siﬁer. According to the LGBM documentation, the importance of a feature
is calculated as the number of times the feature is used in a model. 94.9%
of the prediction is explained by text data contained in README ﬁles, de-
scriptions, and labels. Categorical data represents 4.0% of the prediction, with
sub-folders/ﬁles in the root folder and programming languages as the most im-
portant categorical data sources, while licences, contributors, and topics are
not enough to explain 1% of the total score together. Lastly, the combination
of numerical data sources, stars, forks, and releases, explains the remaining
1.1% of the feature importance score.

5.5 RQ1.5: How accurately can state-of-the-art machine learning techniques
automatically classify less popular GitHub repositories?

Given that our classiﬁer learns from data such as a repository’s README ﬁle,
we expect that it would ﬁnd it challenging to classify less popular repositories,
which might be poorly documented. To investigate the extent of this expected
deterioration, we reached out to the authors of Borges et al. (2016) to help us
annotate a randomly selected set of 50 less popular repositories from GitHub
using the same annotation process that was used for the original dataset. The
less popular repositories had a median of 169.5 stars and 41 forks each, com-
pared to 2,866 stars and 460 forks for the popular dataset curated by Borges

Automatically Categorising GitHub Repositories by Application Domain

15

et al. As expected, the performance deteriorates, with the overall accuracy
dropping to 0.58. LGBM was roughly 2.2 times more accurate (0.58 vs. 0.26)
than the ZeroR classiﬁer.

RQ1 Summary

After merging two of the less common application domains that confuse
human annotators and our classiﬁer, we are able to automatically de-
termine the application domain of a GitHub repository with a minimum
precision of 0.7. The ﬁnal classiﬁer relies mostly on textual features, with
categorical and numerical features playing a much smaller role. When the
classiﬁer is applied to a newly created dataset of 50 less popular reposi-
tories, the performance is lower.

6 RQ2: Diﬀerences between Domains

To investigate to what extent diﬀerences between application domains are
relevant from a software engineering perspective, we focused on three soft-
ware engineering practices—automation, refactoring, and code ownership—
and analysed how these diﬀer across application domains. Since this anal-
ysis required the processing of additional data, in particular related to code
changes, we took the opportunity to download a new version of the 2,000 most
popular repositories on GitHub. The number 2,000 was chosen to balance ef-
fort, in particular, for downloading all corresponding commits, and size of the
data. Then, we contrasted these new popular repositories with the Borges et
al. dataset and removed overlapping repositories, leaving us with 893 new un-
seen repositories. We downloaded all commit data from these 893 repositories
on 21 March 2022. We excluded from this analysis the following four outlier
repositories with more than 200,000 commits each: chromium/chromium,
aosp-mirror/platform frameworks base, llvm/llvm-project and
Homebrew/homebrew-core. We then automatically classiﬁed the repos-
itories using our classiﬁer.

Before presenting the results of the forthcoming sub-questions, we intro-
duce a high-level view of observable diﬀerences between repositories from dif-
ferent application domains. We use the following nine features related to soft-
ware development practices (see details in the following subsections) for each
repository:

1. Number of refactoring commits
2. Number of non-refactoring commits
3. Total number of commits
4. Refactoring ratio
5. Number of major contributors
6. Number of minor contributors
7. Total number of contributors
8. Ownership ratio

16

Francisco Zanartu et al.

Fig. 5 Density distributions across application domains

9. A boolean value indicating whether the repository uses automation

With the help of T-distributed Stochastic Neighbor Embedding (t-SNE),
a tool to visualize high-dimensional data, we projected all data into 2d, and
then visualised the distributions for each domain in Figure 5. We can see that
each of them occupies a slightly diﬀerent space, with some overlaps, which is
consistent with our classiﬁer results. Visually, Documentation appears to
have more unique characteristics in this view compared to other application
domains. In the next sections, we investigate this more systematically.

6.1 RQ 2.1: How does the adoption of GitHub Actions diﬀer between
application domains?

GitHub Actions are GitHub’s answer to the increasing demand for automa-
tion in software repositories. Kinsman et al. (2021) identiﬁed that the ﬁve

Automatically Categorising GitHub Repositories by Application Domain

17

Table 10 Adoption of GitHub Actions across application domains

Application Domain

Adopted

Not Adopted

Application & System Software
Documentation
Non-Web Libs & Frameworks
Software Tools
Web Libs & Frameworks

Total

55
93
97
104
97

446

(70%)
(29%)
(56%)
(68%)
(61%)

(50.2%)

24
232
75
49
63

443

(30%)
(71%)
(44%)
(32%)
(39%)

(49.8%)

categories of GitHub actions that are the most frequently used are Continu-
ous Integration, Utilities, Deployment, Publishing, and Code Quality. Because
GitHub Actions standardise the use of automation by GitHub repositories,
this feature aﬀords us the opportunity to investigate diﬀerences in GitHub
Actions adoption across application domains.

Table 10 shows the results of this analysis. 50.2% of the repositories in
this dataset have adopted GitHub Actions, indicating how quickly the com-
munity has adopted this new feature. Chi-square tests of independence were
performed to examine the relation between an application domain and the
adoption of GitHub Actions. Following Kim (Kim, 2017), we declare that an
eﬀect size (ES) = 0.1 is small, ES = 0.3 medium, and ES = 0.5 large (Kim,
2017). The relation between these variables was signiﬁcant for Documenta-
tion (χ2 = 93.838, p = .000, phi = .325), Software Tools (χ2 = 22.583,
p = .000, phi = .0159), Application & System Software (χ2 = 12.282,
p = .000, phi = .118) and Web Libraries and Frameworks (χ2 = 8.031,
p = .005, phi = .095). For Non-Web Libraries and Frameworks, the
relation between these variables was not signiﬁcant (χ2 = 3.006, p = .083,
phi = .058).

6.2 RQ 2.2: How does the extent of refactoring diﬀer between application
domains?

Refactoring is the process of restructuring existing source code with the in-
tention of improving the structure, design, and implementation of the code,
but not its functionality. To analyse whether the extent of refactoring diﬀers
between application domains, we used Ratzinger’s list of keywords (Ratzinger,
2007) to identify refactoring commits from their commit message: “refac-
tor”, “restruct”, “clean”, “not use”, “unus”, “reformat”, “import”, “remov”,
“replac”, “split”, “reorg”, “renam”, and “move” (Ratzinger, 2007). In total,
3,165,159 commits were analysed.

Table 11 shows the distributions of refactoring commits across application
domains. Chi-square tests of independence were performed to examine the
relation between an application domain and the extent of refactoring. The
tests revealed that the relationship between these variables was signiﬁcant, but
with a small eﬀect, for all the studied application domains: Documentation

18

Francisco Zanartu et al.

Table 11 Refactoring commits across application domains

Application Domain

Refactoring

Not Refactoring

Application & System Software
Documentation
Non-Web Libs & Frameworks
Software Tools
Web Libs & Frameworks

99k
25k
77k
138k
59k

(14%)
(6%)
(15%)
(13%)
(12%)

618k
386k
431k
914k
419k

(86%)
(94%)
(85%)
(87%)
(88%)

Total

398k

(13%)

2,767k

(87%)

(χ2 = 18462.478, p = .000, phi = .076), Software Tools (χ2 = 455.497, p =
.000, phi = .012), Application & System Software (χ2 = 1306.458, p =
.000, phi = .002), Non-Web Libraries and Frameworks (χ2 = 3630.422,
p = .000, phi = .034) and Web Libraries and Frameworks (χ2 = 20.087,
p = .000, phi = .003).

6.3 RQ 2.3: How does the code ownership diﬀer between application
domains?

Bird et al. (2011) developed ownership measures with the intuition that the
number of times a developer works on a software component increases the de-
veloper’s knowledge of that component. They deﬁned the proportion of owner-
ship as the ratio of the number of commits the contributor has made compared
to the total number of commits. They also diﬀerentiated major contributors as
developers whose ownership is at or above 5% and minor contributors whose
ownership is below 5%.

Figure 6 illustrates the ownership probability density estimation (i.e., ratio
of contributors identiﬁed as major contributors) for each application domain.
To determine diﬀerences in these distributions, Mann-Whitney U tests were
conducted on pairs of application domains. The tests revealed similar dis-
tributions for Software Tools (Mdn = 0.45), Non-Web Libraries and
Frameworks (Mdn = 0.49) and Application & System Software (Mdn
= 0.47), but signiﬁcant diﬀerences between all pairs of application domains
for Documentation (Mdn = 0.56) and Web Libraries and Frameworks
(Mdn = 0.54), all with small eﬀect sizes.

RQ2 Summary

Software repositories belonging to diﬀerent application domains diﬀer
in their adoption of software engineering practices. Almost all of the
relationships that we investigated for the practices of automation, refac-
toring, and code ownership revealed statistically signiﬁcant diﬀerences
between application domains.

Automatically Categorising GitHub Repositories by Application Domain

19

Fig. 6 Ownership distributions across application domains

7 Discussion

The purpose of this study is to help researchers and practitioners cata-
logue GitHub repositories using an automated classiﬁer across ﬁve appli-
cation domains. The advantage of this approach is that these domains
would otherwise be unavailable as a GitHub feature, diﬃcult to replicate
for a novice in the domain, and highly time-consuming to re-create. As
mentioned in the Introduction, many researchers have manually classiﬁed
GitHub repositories using the Borges et al. taxonomy (e.g., Bi et al. (2021,
2022); Coelho and Valente (2017); De Stefano et al. (2020, 2022); Hata et al.
(2022); Hayashi et al. (2019); Rehman et al. (2022); Tamburri et al. (2019))—
our classiﬁers aim to eliminate the need for such manual work.

Strengths of this high-level application domain taxonomy include the abil-
ity to combine it with other ﬁltering methods, such as topic tags, to help
narrow down the search of particular repositories. On the other hand, taxon-
omy and automated classiﬁer enable discovery of common characteristics of
projects in a speciﬁc domain, leading to generalisable results.

From our results of RQ1, we learned that more data sources as features
were beneﬁcial in improving our classiﬁer performance and that it was hard to
ﬁnd features to separate Application Software from Software Tools.
We observe that separating these two categories was not a trivial task, even for
a domain expert, so we ended up merging them. Textual data, i.e., descriptions
and README ﬁles, are the most important drivers to catalogue a repository.
Maintainers looking for contributors should make sure that their README ﬁle

20

Francisco Zanartu et al.

is detailed and up-to-date, since it is the main source for classiﬁers to categorise
a repository. README ﬁles are also the starting point for developers when
encountering a new repository (Prana et al., 2019). We hypothesise that poor
descriptions might inﬂuence lower results for less popular repositories, but
further research is needed to corroborate this.

RQ2 results show how our classiﬁer should be used rather than manually
annotating the repositories. The cataloguing time took only a fraction of the
time of doing it manually and the initial data was easily complemented with
more data. The ﬁndings revealed some overlap between application domains,
with a clear distinction for Documentation, which, despite having a signiﬁcant
relationship between the adoption of GitHub domains and the extent of refac-
toring commits, has the lowest rates of adoption of GitHub actions (Table 10)
and of extent of refactoring (Table 11), as well as a dissimilar distribution of
code ownership. As such, the reliability of these ﬁndings is based on their abil-
ity to be easily reproduced by anyone, fulﬁlling our earlier promise of aiding
in the achievement of generalisable results.

8 Threats to Validity

The dynamic nature of GitHub is a threat to the validity of this work. What
was considered a popular repository by Borges et al. (2016) may not be a
popular repository today, and that deﬁnition inﬂuenced what was considered
less popular in order to test the accuracy of this model. This dynamic nature
also inﬂuenced why some repositories could not be downloaded to answer RQ2
and whether their presence might have inﬂuenced the results.

The application domains used in this work are based on a manually an-
notated dataset with all the imperfections that might be present in terms of
bias and errors. However, every eﬀort has been made to keep those criteria in
place throughout the work in order to make it as consistent as possible. The
descriptions of what should be considered one or another application domain
leave some room for interpretation, despite the fact that manually annotating
5,000 repositories is a substantial eﬀort and there are few opportunities to use
such a valuable resource to create a classiﬁer as this one. This threat was mit-
igated by the authors of this classiﬁcation who debated diﬃcult decisions and
the involvement of Borges et al. in the annotation of less popular repositories
to ensure consistency across the annotations.

Further threats to validity exist in the fact that keywords to ﬁnd refactor-
ing commits can be imperfect or incomplete, despite us relying on a previously
published set of keywords for the identiﬁcation of such commits. In addition,
contributors can make their commits under more than one user name, aﬀecting
code ownership metrics, and part of a repository’s history may not have been
completely migrated to what is contained in GitHub, aﬀecting results. All of
these are diﬃcult to control, and interpretations of results must take these
into account. Finally, to answer RQ2, four repositories were removed from the
dataset. Due to the large number of commits compared to the other repos-

Automatically Categorising GitHub Repositories by Application Domain

21

itories, these four repositories chromium/chromium, llvm/llvm-project
and Homebrew/homebrew-core may have aﬀected the extent of the refac-
toring results for the Software tools application domain, as well as aosp-
mirror/platform frameworks base may have impacted the results for
the Non-web libraries and frameworks application domain.

9 Related Work

To our knowledge, our classiﬁer is the ﬁrst automated tool to predict the
application domain of a GitHub repository. It follows in the long line of work
on classifying other aspects of GitHub repositories, such as the content of
README ﬁles or a repository’s topic tags.

In 2014, when there were around 10 million repositories on GitHub,
Kalliamvakou et al. (2014) predicted the importance of this platform for re-
searchers, as well as the diﬃculties in distinguishing real software repositories
from repositories for personal use. Although features such as topic tags are im-
plemented on GitHub, most repositories do not use them (Zhang et al., 2019).
Therefore, researchers have been investigating approaches to assist GitHub
users in ﬁnding repositories of their interest and repository owners in improv-
ing their documentation.

Following this reasoning, Sharma et al. (2017) extracted descriptive seg-
ments of text from 10,000 README ﬁles in GitHub repositories. They pre-
processed those descriptive segments with standard natural language process-
ing techniques, i.e., tokenisation, stop word removal, and stemming, and eval-
uated them using the Latent Dirichlet Allocation Genetic Algorithm (LDA-
GA), to identify a set of topics together with the repositories belonging to
those topics. This is a promising approach with a resulting F1 score of 0.7 and
can be helpful in predicting missing topic data for some repositories. However,
the process involves manual analysis, which makes it diﬃcult to operate on
new data.

Ma et al. (2018) proposed an automated solution based on machine learn-
ing techniques to identify diﬀerent types of software artefacts, such as require-
ments documents, system elements, veriﬁcations, or tasks. On a test dataset,
they were able to automatically categorise documentation-related software
artefacts with an average precision of 0.76 and a recall of 0.75 by combin-
ing several machine learning techniques through ensemble learning.

Zhang et al. (2019) proposed a hierarchical classiﬁcation that considers
multimodal signals in a repository, i.e., user names, descriptions, tags, and
README ﬁles. They proposed a framework composed of three key modules: a
graph containing those multimodal signals deﬁned as “heterogeneous informa-
tion network”, keyword enrichment, and pseudo-document generation. Finally,
the framework was tested on two GitHub datasets containing separated data
from the machine learning and bioinformatics research communities against
six hierarchical classiﬁcation algorithms, achieving outperforming results.

22

Francisco Zanartu et al.

Prana et al. (2019) conducted a qualitative study of README ﬁles in
GitHub repositories and developed a classiﬁer to automatically identify eight
categories of README sections and achieved an F1 score of 0.746. They ﬁnd
that discussion about “What” and “How” of a repository is quite common,
while information about purpose and status is limited or missing. Then, they
labelled sections of README ﬁles using badges and tested the classiﬁcation
outcome with the opinions of twenty software professionals, who in the major-
ity perceived ease of information discovery. Finally, they provide recommen-
dations to repository owners to improve their documentation quality.

Di Sipio et al. (2020) trained a multinomial Na¨ıve Bayes network with TF-
IDF vectors computed from README ﬁles to predict a list of topics. Then,
they combine those results with discovered programming languages using the
GuessLang tool to deliver a resulting list of recommended topics. According
to the analysis of their results, the model performs well with a low number of
topics to predict, i.e., one or two. However, their results decrease notably as
the number of recommended topics increases.

Complementing these papers, our work focuses on the automated classi-
ﬁcation of repositories according to their application domain, based on the
domains established by the work of Borges et al. (2016).

10 Conclusion and Future Work

Past research has found that the application domain of a GitHub repository is
an important piece of information, for example, to predict the popularity of a
repository (Borges et al., 2016) or as a parameter when assessing the quality
of the code contained in the repository (Capiluppi and Ajienka, 2019). Yet,
determining the application domain of a GitHub repository is a non-trivial
task.

In this work, we use an existing dataset of 5,000 GitHub repositories as
a starting point for building a classiﬁer which can automate this task. Each
repository in the dataset has been manually annotated with its application
domain, and we augment this dataset with textual, categorical, and numerical
features that capture the characteristics of each of the repositories.

We show that our automated classiﬁer is able to assign popular reposito-
ries to their application domain with at least 70% precision for each of the
ﬁve domains, and we explore the impact of various design choices in the cre-
ation of the classiﬁer in detail. We then apply the classiﬁer to a new dataset
of repositories to investigate to what extent repositories associated with dif-
ferent application domains diﬀer from a software engineering perspective. In
particular, we investigate the adoption of software engineering practices, such
as automation and refactoring, across the application domains, uncovering
signiﬁcant diﬀerences.

Future Work. To further improve the performance of the classiﬁer, future
research should consider additional options to address the issue of incomplete

Automatically Categorising GitHub Repositories by Application Domain

23

documentation in some repositories. Although topic tags have a high predic-
tive capacity, they are not used in most repositories studied. These techniques
are also applicable when dealing with less popular repositories, such as those
discussed in RQ 1.4. For a repository with little documentation, our classi-
ﬁer might be able to help users understand the repository by identifying its
application domain.

Promising results can be obtained by combining classiﬁers and datasets;
for example, one classiﬁer could predict application domains using only text
data, while the other classiﬁer would use the remaining data sources, perhaps
one for categorical data and another for numerical data. This proposed method
can be helpful in diminishing the complexity of each dataset-classiﬁer pair and
will likely further improve the results.

Meanwhile, our classiﬁer enables other applications that need to quickly
and automatically sift through the large number of repositories on GitHub. We
found that the adoption of software engineering practices diﬀers depending on
the application domain—this has implications for practitioners and educators
alike. For example, one interesting question to explore would be whether the
observed diﬀerences imply that students need to be taught diﬀerently depend-
ing on their intended application domain or whether practitioners working in
certain application domains should be encouraged to adopt the practices of
others. Our classiﬁer enables the collection of a large number of repositories
to drive such work.

References

Bayati S, Heidary M (2019) Time series analysis of open source projects pop-
ularity. In: Proceedings of the Workshop on E-Business, Springer, pp 77–86
Bi T, Xia X, Lo D, Aleti A (2021) A ﬁrst look at accessibility issues in pop-
ular GitHub projects. In: Proceedings of the International Conference on
Software Maintenance and Evolution, pp 390–401

Bi T, Xia X, Lo D, Grundy J, Zimmermann T (2022) An empirical study
of release note production and usage in practice. IEEE Transactions on
Software Engineering 48:1834–1852

Bird C, Nagappan N, Murphy B, Gall H, Devanbu P (2011) Don’t touch my
code! Examining the eﬀects of ownership on software quality. In: Proceed-
ings of the ACM SIGSOFT Symposium and the European Conference on
Foundations of Software Engineering, pp 4–14

Borges H, Hora A, Valente MT (2016) Understanding the factors that impact
the popularity of GitHub repositories. In: Proceedings of the International
Conference on Software Maintenance and Evolution, pp 334–344

Capiluppi A, Ajienka N (2019) The relevance of application domains in em-
pirical ﬁndings. In: Proceedings of the International Workshop on Software
Health, pp 17–24

Capiluppi A, Ajienka N, Ali N, Arzoky M, Counsell S, Destefanis G, Miron
A, Nagaria B, Neykova R, Shepperd M, Swift S, Tucker A (2020) Using the

24

Francisco Zanartu et al.

lexicon from source code to determine application domain. In: Proceedings
of the International Conference on Evaluation and Assessment in Software
Engineering, pp 110–119

Coelho J, Valente MT (2017) Why modern open source projects fail. In: Pro-
ceedings of the ACM SIGSOFT Joint Meeting on the Foundations of Soft-
ware Engineering, pp 186–196

De Stefano M, Pecorelli F, Tamburri DA, Palomba F, De Lucia A (2020) Splic-
ing community patterns and smells: A preliminary study. In: Proceedings of
the International Workshop on Software Health, pp 703–710

De Stefano M, Iannone E, Pecorelli F, Tamburri DA (2022) Impacts of software
community patterns on process and product: An empirical study. Science of
Computer Programming 214:102731

Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:181004805

Di Sipio C, Rubei R, Di Ruscio D, Nguyen PT (2020) A multinomial na¨ıve
bayesian (MNB) network to automatically recommend topics for GitHub
repositories. In: Proceedings of the International Conference on Evaluation
and Assessment in Software Engineering, pp 71–80

Fawcett T (2006) An introduction to ROC analysis. Pattern Recognition Let-

ters 27(8):861–874

Feurer M, Klein A, Eggensperger K, Springenberg JT, Blum M, Hutter F
(2019) Auto-sklearn: eﬃcient and robust automated machine learning. In:
Automated Machine Learning, Springer, Cham, pp 113–134

Gijsbers P, LeDell E, Poirier S, Thomas J, Bischl B, Vanschoren J (2019) An
open source AutoML benchmark. In: Proceedings of the ICML Workshop
on Automated Machine Learning

Hata H, Novielli N, Baltes S, Kula RG, Treude C (2022) GitHub Discussions:
An exploratory study of early adoption. Empirical Software Engineering
27(1):1–32

Hayashi J, Higo Y, Matsumoto S, Kusumoto S (2019) Impacts of daylight
saving time on software development. In: Proceedings of the International
Conference on Mining Software Repositories, pp 502–506

He H, Ma Y (2013) Imbalanced learning: foundations, algorithms, and appli-

cations. John Wiley & Sons

Kalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D
(2014) The promises and perils of mining GitHub. In: Proceedings of the
Working Conference on Mining Software Repositories, pp 92–101

Kim HY (2017) Statistical notes for clinical researchers: Chi-squared test and

Fisher’s exact test. Restorative Dentistry & Endodontics 42(2):152

Kim J, Wi J, Kim Y (2021) Sequential recommendations on GitHub repository.

Applied Sciences 11(4):1585

Kinsman T, Wessel M, Gerosa MA, Treude C (2021) How do software devel-
opers use GitHub Actions to automate their workﬂows? In: Proceedings of
the International Conference on Mining Software Repositories, pp 420–431

Automatically Categorising GitHub Repositories by Application Domain

25

Lemaˆıtre G, Nogueira F, Aridas CK (2017) Imbalanced-learn: A python tool-
box to tackle the curse of imbalanced datasets in machine learning. Journal
of Machine Learning Research 18(1):559–563

Ma Y, Fakhoury S, Christensen M, Arnaoudova V, Zogaan W, Mirakhorli M
(2018) Automatic classiﬁcation of software artifacts in open-source applica-
tions. In: Proceedings of the International Conference on Mining Software
Repositories, pp 414–425

M¨uller AC, Guido S (2016) Introduction to machine learning with Python: a

guide for data scientists. O’Reilly Media, Inc.

Munaiah N, Kroh S, Cabrey C, Nagappan M (2017) Curating GitHub for
engineered software projects. Empirical Software Engineering 22(6):3219–
3253

Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blon-
del M, Prettenhofer P, Weiss R, Dubourg V, et al. (2011) Scikit-learn: Ma-
chine learning in python. Journal of Machine Learning Research 12:2825–
2830

Prana GAA, Treude C, Thung F, Atapattu T, Lo D (2019) Categorizing the
content of GitHub readme ﬁles. Empirical Software Engineering 24(3):1296–
1327

Ratzinger J (2007) Space-software project assessment in the course of evolu-

tion. PhD thesis, Vienna University of Technology

Rehman I, Wang D, Kula RG, Ishio T, Matsumoto K (2022) Newcomer oss-
candidates: Characterizing contributions of novice developers to GitHub.
Empirical Software Engineering 27(5):1–20

Reimers N, Gurevych I (2019) Sentence-bert: Sentence embeddings using
siamese bert-networks. In: Proceedings of the Conference on Empirical
Methods in Natural Language Processing and the International Joint Con-
ference on Natural Language Processing, pp 3982–3992

Sharma A, Thung F, Kochhar PS, Sulistya A, Lo D (2017) Cataloging GitHub
repositories. In: Proceedings of the International Conference on Evaluation
and Assessment in Software Engineering, pp 314–319

Tamburri DA, Palomba F, Serebrenik A, Zaidman A (2019) Discovering com-
munity patterns in open-source: a systematic approach and its evaluation.
Empirical Software Engineering 24(3):1369–1417

Thornton C, Hutter F, Hoos HH, Leyton-Brown K (2013) Auto-weka: Com-
bined selection and hyperparameter optimization of classiﬁcation algo-
rithms. In: Proceedings of the International Conference on Knowledge Dis-
covery and Data Mining, pp 847–855

Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser  L,
Polosukhin I (2017) Attention is all you need. In: Proceedings of the Inter-
national Conference on Neural Information Processing Systems, pp 6000—
-6010

Wang C, Wu Q, Weimer M, Zhu E (2021) Flaml: A fast and lightweight automl

library. Proceedings of Machine Learning and Systems 3:434–447

Wu Y (2020) Exploring the relationship between dockerﬁle quality and project
characteristics. In: Proceedings of the International Conference on Software

26

Francisco Zanartu et al.

Engineering: Companion Proceedings, pp 128–130

Wu Y, Zhang Y, Wang T, Wang H (2020a) Characterizing the occurrence of
dockerﬁle smells in open-source software: An empirical study. IEEE Access
8:34127–34139

Wu Y, Zhang Y, Wang T, Wang H (2020b) Dockerﬁle changes in practice: A
large-scale empirical study of 4,110 projects on GitHub. In: Proceedings of
the Asia-Paciﬁc Software Engineering Conference, pp 247–256

Zhang Y, Lo D, Kochhar PS, Xia X, Li Q, Sun J (2017) Detecting similar
repositories on GitHub. In: Proceedings of the International Conference on
Software Analysis, Evolution and Reengineering, pp 13–23

Zhang Y, Xu FF, Li S, Meng Y, Wang X, Li Q, Han J (2019) Higitclass:
Keyword-driven hierarchical classiﬁcation of GitHub repositories. In: Pro-
ceedings of the International Conference on Data Mining, pp 876–885

Zhang Y, Wang H, Wu Y, Hu D, Wang T (2020) GitHub’s milestone tool:
A mixed-methods analysis on its use. Journal of Software: Evolution and
Process 32(4):e2229

Z¨ullighoven H (2004) Object-oriented construction handbook: Developing
application-oriented software with the tools & materials approach. Elsevier

