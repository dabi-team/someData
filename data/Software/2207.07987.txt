A tool for emulating neuromorphic architectures
with memristive models and devices

Jinqi Huang, Spyros Stathopoulos, Alex Serb, and Themis Prodromakis
Email: {j.huang, s.stathopoulos, a.serb, t.prodromakis}@soton.ac.uk
Centre for Electronics Frontiers, Electronics and Computer Science, University of Southampton, UK

2
2
0
2

l
u
J

6
1

]
E
N
.
s
c
[

1
v
7
8
9
7
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Memristors have shown promising features for en-
hancing neuromorphic computing concepts and AI hardware
accelerators. In this paper, we present a user-friendly soft-
ware infrastructure that allows emulating a wide range of
neuromorphic architectures with memristor models. This tool
empowers studies that exploit memristors for online learning
and online classiﬁcation tasks, predicting memristor resistive
state changes during the training process. The versatility of the
tool is showcased through the capability for users to customise
parameters in the employed memristor and neuronal models as
well as the employed learning rules. This further allows users
to validate concepts and their sensitivity across a wide range of
parameters. We demonstrate the use of the tool via an MNIST
classiﬁcation task. Finally, we show how this tool can also be
used to emulate the concepts under study in-silico with practical
memristive devices via appropriate interfacing with commercially
available characterisation tools.

Index Terms—memristor, neuro-inspired computing, neuro-
morphic computing, neural networks, online learning, online clas-
siﬁcation, neuromorphic emulator, large scale characterisation

I. INTRODUCTION

With recent interests in AI-related research, neuromorphic
designs draw much attention because of their biological plau-
sibility and high computational efﬁciency. Inspired by biol-
ogy, neuromorphic designs beneﬁt from efﬁcient spike-based
computation, data sparsity, and temporal coding. Dedicated
neuromorphic hardware designs such as [1], [2] overcome Von
Neumann bottleneck by applying an in-memory computing
design scheme, further allowing the ability to handle a large
amount of data. Meanwhile, memristors [3], as a type of
non-volatile devices, have been rapidly developed and have
shown promising features for enhancing neuromorphic com-
puting: extreme downscaling [4], high density integration [5],
low switching energy with high switching rate [6], multi-
bit storage [7], and physical properties similar to biological
synapses [22]. Memristors have become a popular candidate
replacing SRAMs in in-memory computing -based dedicated
neuromorphic hardware by storing information as resistive
states (RSs). By directly applying Ohm’s Law and Kirchhoff’s
Current Law, the current from the wordline side of a memristor
array can be regarded as the dot product of two vectors which
are represented as input voltages applied to the bitline side
and memristor conductance respectively. As examples, ref
[9] showed the potential of employing memristor arrays in
Bayesian inference, and ref [10] presented memristor crossbar
serving as a dot-product engine for accelerating neural net-
works. Ref [11], [12], [13], [14], and [15] further performed

matrix multiplication acceleration using memristor crossbar
arrays in feedforward neural networks, convolutional neural
networks, and long short-term memory.

To further exploit the potential of memristors for enhancing
neuromorphic systems, a dedicated simulation tool for fast
validation of concepts and prediction of device states is
needed. However, existing simulators such as MNSIM [16]
and NeuroSim [17] focus more on circuit-level behaviours,
aiming at providing performance estimation in hardware. The
simulator targeting general simulation of the system architec-
ture with memristor models is still missing.

In this paper, we present a user-friendly software-based tool
that enables simulations of neuromorphic computing with a
wide range of neural network architectures, neuron models,
learning rules and memristor settings. This tool allows users to
validate the concept of exploiting memristors in neuromorphic
systems, and to explore the parameter sensitivity of the sys-
tem. Employed neuron models, learning rules and memristor
models further allow users to customise their neuromorphic
architectures by simply tuning parameters. A built-in analysis
tool will also assist users to visualise inference results and
internal variables for fast validation and debugging. Finally,
this tool can also interface with commercially available char-
acterisation tools such as [18] to emulate designs with real
devices.

II. DESIGN ARCHITECTURE

A. Design overview

Our work is a python-based software simulator aiming at
performing simulation for neuromorphic architectures with
memristor models. There are memristor models, neuron mod-
els, and learning rules embedded in the tool so that
the
simulation of the whole system performing real machine
learning tasks can be easily set up by providing input data and
system conﬁguration parameters. Figure 1 shows the workﬂow
of this work. There are three independent input ﬁles: conﬁg-
uration ﬁle, connectivity matrix ﬁle, and stimuli ﬁle. The
conﬁguration ﬁle provides parameters at both system level
(including network size, network depth, and neuron numbers
in each layer), and device level (such as memristor ﬁtting
parameters), to conﬁgure a neuromorphic architecture. The
connectivity matrix ﬁle deﬁnes the connectivity of neurons
and assigns memristors as synapses between neurons. The
stimuli ﬁle stores incoming spikes fed to input neurons at
each time step. After all input ﬁles are loaded to the tool,

 
 
 
 
 
 
config file

connectivity 
matrix file

stimuli file
(test/training)

read RSs

neuron core

inference 
results

t = tStep?

analysis tool

update RSs

plasticity core

t (cid:1) t + 1

No

Yes

training?

virtual memristor array

memristor-based NN simulator workflow

Fig. 1. System workﬂow. Icons in different colors indicate different ﬁles in the system.

memristors models in ’virtual memristor array’ are activated
and initialised. Meanwhile, input spikes are decoded from
the stimuli ﬁle and sent
to the neuron model placed in
’neuron core’. In a single inference, ’neuron core’ reads
memristor RSs from ’virtual memristor array’, and updates
membrane voltages according to the speciﬁc neuron model
selected by users. Fire history is then appended according to
the determined threshold. If training is enabled, memristor RSs
are also updated by the chosen learning rule in ’plasticity core’
before starting another inference. The same process will repeat
until all epochs have ﬁnished. Updated membrane voltages,
weights and ﬁre history can be loaded to a built-in analysis
tool for further visualisation and debugging.

There are two usage scenarios for this tool: one is to
explore the sensitivity of the memristor-base neuromorphic
system to a speciﬁc parameter. This tool allows users to
monitor how memristor RS changes given different values
of the target parameter. Another usage scenario is to validate
algorithms, including neuron models and learning rules. Users
can investigate a wide range of neuron models and learning,
by selecting an example ’neuron core’/’plasticity core’ or
writing their own model using example ﬁles as templates. In
both scenarios, the tool provides users options to customise
their own neuromorphic architectures at both system level and
device level.

B. Neuron models and learning rules

There are a wide range of neuron models and learning
rules provided by this tool, including leaky integrate-and-ﬁre
(LIF) [20] and Izhikevich neuron [21] for neuron models
and spiking-timing-dependent-plasticity (STDP) [22], back-
propagation (BP) [23], tempotron [24], and direct random
target projection (DRTP) [25] for learning rules. Those options
cover different application scenarios and design choices, from
unsupervised learning to supervised learning, from rate-coding
temporal coding, and from two-layer to multi-layer. This tool
also allows users to explore their algorithms by simply using
examples as templates.

The main workﬂow of neuron model and learning rule is as
follow: the neuron model placed in the ’neuron core’ updates

membrane voltages and determines neurons ﬁring states, and
the ﬁring history is then sent to the learning rule placed in
the ’plasticity core’ to update weights if training is enabled.
Once the condition of weight updating is met, pulses are sent
to the ’virtual memristor array’ to trigger weight updating. All
actions are taken within one time step, and the same process
will be repeated until the time step is equal to pre-set trail
number.

C. Memristor models

Memristor models embedded in the tool are essential to
provide a precise prediction for RSs given different triggering
conditions. Here we use one empirical model [19] which
reﬂects the main switching characteristics of real memristor
devices as an example, but this tool can be compatible with
other user-deﬁned memristor models as well to enable more
ﬂexibility. This memristor model can ideally emulate any
type of device provided different values of parameters. The
model explained how memristor resistance switching rate is
dependent on bias voltages and current RSs. Model equations
can be found below:

dR
dt

=

= m(R, v)

Ap(−1 + e

(

An(−1 + e

|v|

tp )(rp(v) − R)2
tn )(R − rn(v))2

|v|

if v > 0, R < rp(v)

if v ≤ 0, R ≥ rn(v)

(1)

Where term Ap,n(−1 + e

|v|
tp,n ) explains the exponential
dependency of the switching rate on bias voltage with Ap,n
and tp,n as scaling factors and ﬁtting parameters respectively.
Term (rp,n(v)−R)2 gives a ﬁtting of second-order dependency
of switching rate on current resistive states. Internal variables
rp,n represent resistive operating range given a bias voltage v
with a ﬁrst-order ﬁtting function using ﬁtting parameters a0p,
a1p, a0n, a1n:

r(v) =

rp(v) = a0p + a1pv
rn(v) = a0n + a1nv

if v > 0
if v ≤ 0

(

0

22

%
y
c
a
r
u
c
c
a

i

g
n
n
a
r
t

i

i

s
n
o
s
n
e
m
d

i

(A)

.
.
.

4
8
4

100

90

80

70

60

50

40

30

20

10

0

0

2000

4000

epochs

(C)

6000

8000

10000

(E)

(F)

(G)

s
n
o
r
u
e
n

t

u
p
n

i

4
8
4

.
.
.

(B)

(D)

s
n
o
r
u
e
n

t

u
p

t

u
o

0
1

k(cid:0)
2

.

4
2

k(cid:2)
6
2

.

2

Fig. 2. Simulation for handwritten digits recognition performed in selector-based and selectorless memristor arrays. (A) Input images. (B) Neural network
architecture. (C) Accuracy curves. (D) Weight update curves. (E) Memristor resistive states before training, (F) after training in selector-based array, and (G)
in selectorless array.

When a classiﬁcation task is executed, ’virtual memristor ar-
ray’ takes user-deﬁned parameters and initialises the memristor
model. When a pulse is sent from the ’plasticity core’ during
the training phase, a resulting resistance is updated according
to the model expressions and is returned to ’neuron core’ when
starting a read operation.

D. Weights updating scheme

After an expected ∆W is calculated in ’plasticity core’
during the training phase, there is a critical step to write new
weights to memristors. The weight update scheme used in
this tool is to use ’predict-write-verify’ loops until memristors
resistive states converge to expected values. Firstly, when ∆W

is attained, the new expected RS is calculated based on the
current RS and the weight mapping scheme. The resulting
resistance is then calculated given options of pulse parameter
sets provided by users. Next, set the pulse with parameters that
lead to memristor RS closest to the expected before starting
a read operation to verify. Finally, repeat the same process
until the new resistance is within the R tolerance chosen by
users, or the number of updating loops hits the maximum
steps provided by users. The R tolerance is calculated as
(Rexpected−Rreal)/Rexpected to make sure the ﬁnal memristor
RSs are within an acceptable range, and the maximum step
number is used to prevent the inﬁnite loop if memristor RSs

 
 
 
 
 
 
 
never converge to steady values due to the read noise.

III. APPLICATION EXAMPLE

In this section, an application example using this tool to
simulate handwritten digit recognition with MNIST dataset
in both selector-based and selectorless memristor arrays will
be presented and discussed. We used memristor parameters
extracted from TiOx-based devices [7] using extraction method
proposed by [26]. We simulated with bias voltages ranging
from ±0.9V to ±1.2V to predict how real devices behave in
a real-world classiﬁcation task. We are presenting two biasing
scenarios: a) a scenario for selector based devices where bias
is only applied to the selected device and b) a scenario with
half-voltage mitigation for unselected devices to represent the
selectorless case. In this task, original 28 × 28 grey-scale
images are cropped to 22 × 22 and converted to binary format
(see Figure 2 (A), pink and light grey pixels represent ’1’ and
’0’ respectively). The neural network used in this task is a
484 × 10 winner-take-all network with leaky integrate-and-
ﬁre neuron model and gradient descent learning rule, as it is
shown in Figure 2 (B). We map weights as device conductance
without normalisation as most memristor-based neuromorphic
designs do. Some key parameters used in this task are listed in
Table I. To start the task, the memristor arrays are initialised
to high resistance (low weights). In the ideal situation, weights
in stimulated synapses will increase weight gradually, while
those in non-stimulated synapses will stay high. Input images
are ﬁrstly unrolled to 484-dimensional vectors whose bits are
sent to each input neuron as 0-or-1 input spikes. If more than
one output neurons are above the threshold, the one with
the highest membrane voltage will ﬁre while others will be
inhibited. The network is ﬁrstly trained for 10000 epochs with
a minibatch size of 100. Next, 2000 test data from a separated
test set are sent to the network. We did tests for selector-
based memristor array and selectorless memristor array as
well as a version storing weights directly in the computer
memory instead of memristors as the baseline. The accuracy
curves can be found in Figure 2 (C). As it can be seen in the
training accuracy curves, the general training accuracy of the
baseline is 83.55%, and selector-based array and selectorless
array achieved 82% and 61.55% test accuracy respectively.
This indicates two things: ﬁrstly the accuracy of selector-based
array is mainly limited by factors out of memristor devices;
secondly, there is a 20% accuracy gap between selectorless
and selector-based versions.

To further explore what made the 20% accuracy gap,
the weight change curves for synapses connected between
stimulated or non-stimulated input neuron and the target output
neuron for both versions is displaced in Figure 2 (D), where
synapse 384-6 represents a stimulated synapse and 10-6 non-
stimulated. A stimulated synapse in the selector-based version
(the dark green line) shows an increasing tendency during
the whole training process, while in selectorless version (the
dark purple line) weight was unexpectedly decreased when no
update should occur (see the small window for the ampliﬁed
region between epoch 0 and 200). This is due to unexpected

TABLE I
KEY PARAMETERS USED IN THE APPLICATION EXAMPLE

Parameters
array size
threshold
learning rate
Ap
An
tp
tn
a0p
a0n
a1p
a1n
R tolerance
max update steps

Values
100×100
10 mV
3.5 × 10−6
0.21389
-0.81302
1.6591
1.5148
37087
43430
-20193
34333
0.1%
5

weight updates in inactive devices caused by half bias voltages
when target devices in the same bitlines or wordlines are
inhibited. The impact of unwanted decrease is accumulated as
many devices share the same bitlines or wordlines. For non-
stimulated synapse (10-6), weight changes in selector-based
version are trivial, while in selectorless version the synapse
weight also decreased due to half bias voltages. However, even
in the selectorless version, the weights in stimulated synapses
are still slightly higher than those in the non-stimulated. The
small weight differences still make the network able to learn
images and be able to distinguish images in some cases.
Figure 2 (E) - (G) show memristor resistive states before (E)
and after training for both selector-based (F) and selectorless
(G) versions. The small weight differences in the selectorless
version are not distinguishable in a colourmap spanning a wide
range, therefore we display resistive states instead. In Figure
2 array is initialised to ∼11k, and selector-based array has
memristors with high resistance (∼11k) in the background and
with low resistance (∼2.2k) in the middle to learn the digits.
In the selectorless version, resistive states of memristors in
the background have reached very high values (∼24k), while
those in the middle also have very high values (∼20k) but are
slightly smaller than those in the background.

IV. DISCUSSION

In this paper, we show a software-based tool for emulating
memristor-based neuromorphic architectures. This tool can
work as a standalone simulator to allow users to explore
different neuron models, learning rules, memristor models,
different numbers or types of memristor devices, different
neural network architectures, and different neuromorphic ap-
plications by simply providing different values of parameters
or interfacing user-deﬁned models with the tool. This tool
can also be connected to commercially available memristor
characterisation instruments to further emulate designs with
real memristor devices. As an example application, we show
simulation results of handwritten digit recognition in both
selector-based and selectorless memristor arrays, and we fur-
ther explore how and why the performance gap exists.

REFERENCES

[1] F. Akopyan et al., ”TrueNorth: Design and Tool Flow of a 65 mW 1
Million Neuron Programmable Neurosynaptic Chip,” IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems, vol. 34,
no. 10, pp. 1537-1557, 2015, doi: 10.1109/TCAD.2015.2474396.
[2] M. Davies et al., ”Loihi: A Neuromorphic Manycore Processor with
On-Chip Learning,” IEEE Micro, vol. 38, no. 1, pp. 82-99, 2018, doi:
10.1109/MM.2018.112130359.

[3] L. Chua, ”Memristor-The missing circuit element,” IEEE Transac-
tions on Circuit Theory, vol. 18, no. 5, pp. 507-519, 1971, doi:
10.1109/TCT.1971.1083337.

[4] A. Khiat, P. Ayliffe, and T. Prodromakis, ”High Density Crossbar Arrays
with Sub- 15nm Single Cells via Liftoff Process Only,” Scientiﬁc Re-
ports, vol. 6, no. 1, p. 32614, 2016/09/02 2016, doi: 10.1038/srep32614.
[5] S. Pi et al., ”Memristor crossbar arrays with 6-nm half-pitch and 2-nm
critical dimension,” Nature Nanotechnology, vol. 14, no. 1, pp. 35-39,
2019/01/01 2019, doi: 10.1038/s41565-018-0302-0.

[6] B. J. Choi et al., ”High-Speed and Low-Energy Nitride Memris-
tors,” Advanced Functional Materials, vol. 26, 05/01 2016, doi:
10.1002/adfm.201600680.

[7] S. Stathopoulos et al., ”Multibit memory operation of metal-oxide bi-
layer memristors,” Scientiﬁc Reports, vol. 7, no. 1, p. 17532, 2017/12/13
2017, doi: 10.1038/s41598-017-17785-1.

[8] T. Serrano-Gotarredona, T. Masquelier, T. Prodromakis, G. Indiveri,
and B. Linares-Barranco, ”STDP and STDP variations with memristors
for spiking neuromorphic learning systems,” (in English), Frontiers in
Neuroscience, Focused Review vol. 7, no. 2, 2013-February-18 2013,
doi: 10.3389/fnins.2013.00002.

[9] A. Serb, E. Manino, I. Messaris, L. Tran-Thanh, and T. Prodromakis,

”Hardware-level Bayesian inference,” 2017.

[10] M. Hu et al., ”Memristor-Based Analog Computation and Neural Net-
work Classiﬁcation with a Dot Product Engine,” Advanced Materials,
vol. 30, 01/10 2018, doi: 10.1002/adma.201705914.

[11] Z. Wang et al., ”In situ training of feed-forward and recurrent convolu-
tional memristor networks,” Nature Machine Intelligence, vol. 1, no. 9,
pp. 434-442, 2019/09/01 2019, doi: 10.1038/s42256-019-0089-1.
[12] P. Yao et al., ”Fully hardware-implemented memristor convolutional
neural network,” Nature, vol. 577, no. 7792, pp. 641-646, 2020/01/01
2020, doi: 10.1038/s41586-020-1942-4.

[13] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou,
”Memory devices and applications for in-memory computing,” Nature
Nanotechnology, vol. 15, no. 7, pp. 529-544, 2020/07/01 2020, doi:
10.1038/s41565-020-0655-z.

[14] C. Li et al., ”Long short-term memory networks in memristor crossbar
arrays,” Nature Machine Intelligence, vol. 1, no. 1, pp. 49-57, 2019/01/01
2019, doi: 10.1038/s42256-018-0001-4.

[15] S. Ambrogio et al., ”Equivalent-accuracy accelerated neural-network
training using analogue memory,” Nature, vol. 558, no. 7708, pp. 60-67,
2018/06/01 2018, doi: 10.1038/s41586-018-0180-5.

[16] L. Xia et al., ”MNSIM: Simulation platform for memristor-based neuro-
morphic computing system,” 2016 Design, Automation & Test in Europe
Conference & Exhibition (DATE), 2016, pp. 469-474.

[17] P. Chen, X. Peng and S. Yu, ”NeuroSim: A Circuit-Level Macro Model
for Benchmarking Neuro-Inspired Architectures in Online Learning,”
in IEEE Transactions on Computer-Aided Design of Integrated Cir-
cuits and Systems, vol. 37, no. 12, pp. 3067-3080, Dec. 2018, doi:
10.1109/TCAD.2018.2789723.

[18] Arc Instruments, Memristor Characterisation Platform User man-

ual,2017.

[19] Y. Messaris, A. Serb, A. Khiat, S. Nikolaidis, and T. Prodromakis, ”A

compact Verilog-A ReRAM switching model,” 03/03 2017.

[20] L. F. Abbott, ”Lapicque s introduction of the integrate-and-ﬁre model

neuron (1907),” Brain Research Bulletin, vol. 50, pp. 303-304, 1999.

[21] E. M. Izhikevich, ”Simple model of spiking neurons,” in IEEE Trans-
actions on Neural Networks, vol. 14, no. 6, pp. 1569-1572, Nov. 2003,
doi: 10.1109/TNN.2003.820440.

[22] H. Markram, J. L¨ubke, M. Frotscher, and B. Sakmann, ”Regu-
lation of Synaptic Efﬁcacy by Coincidence of Postsynaptic APs
and EPSPs,” Science, vol. 275, no. 5297, pp. 213-215, 1997, doi:
doi:10.1126/science.275.5297.213.

[23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ”Learning repre-
sentations by back-propagating errors,” Nature, vol. 323, no. 6088, pp.
533-536, 1986/10/01 1986, doi: 10.1038/323533a0.

[24] R. G¨utig and H. Sompolinsky, ”The tempotron: a neuron that learns
spike timing–based decisions,” Nature Neuroscience, vol. 9, no. 3, pp.
420-428, 2006/03/01 2006, doi: 10.1038/nn1643.

[25] C. Frenkel, M. Lefebvre, and D. Bol, ”Learning Without Feed-
back: Fixed Random Learning Signals Allow for Feedforward Train-
ing of Deep Neural Networks,” (in English), Frontiers in Neuro-
science, Original Research vol. 15, no. 20, 2021-February-10 2021, doi:
10.3389/fnins.2021.629892.

[26] I. Messaris et al., ”A TiO2 ReRAM parameter extraction method,” 2017
IEEE International Symposium on Circuits and Systems (ISCAS), 2017,
pp. 1-4, doi: 10.1109/ISCAS.2017.8050789.

This figure "fig1.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2207.07987v1

