2
2
0
2

r
p
A
3
1

]
E
M

.
t
a
t
s
[

1
v
2
3
6
6
0
.
4
0
2
2
:
v
i
X
r
a

RECURRENT EVENT ANALYSIS IN THE PRESENCE OF REAL-TIME HIGH
FREQUENCY DATA VIA RANDOM SUBSAMPLING

WALTER DEMPSEY

Abstract. Digital monitoring studies collect real-time high frequency data via mobile

sensors in the subjects’ natural environment. This data can be used to model the impact of

changes in physiology on recurrent event outcomes such as smoking, drug use, alcohol use,

or self-identiﬁed moments of suicide ideation. Likelihood calculations for the recurrent event

analysis, however, become computationally prohibitive in this setting. Motivated by this,

a random subsampling framework is proposed for computationally eﬃcient, approximate

likelihood-based estimation. A subsampling-unbiased estimator for the derivative of the

cumulative hazard enters into an approximation of log-likelihood. The estimator has two

sources of variation: the ﬁrst due to the recurrent event model and the second due to

subsampling. The latter can be reduced by increasing the sampling rate; however, this

leads to increased computational costs. The approximate score equations are equivalent to

logistic regression score equations, allowing for standard, “oﬀ-the-shelf” software to be used

in ﬁtting these models. Simulations demonstrate the method and eﬃciency-computation

trade-oﬀ. We end by illustrating our approach using data from a digital monitoring study

of suicidal ideation.

1. Introduction

Advancement in mobile technology has led to the rapid integration of mobile and

wearable sensors into behavioral health [Free et al., 2013]. Take HeartSteps, for example, a

mobile health (mHealth) study designed to increase physical activity in sedentary adults

[Klasnja et al., 2019]. Here, a Jawbone sensor is used to monitor step count every minute

of the participant’s study day. Of interest in many mHealth studies is the relation of such

real-time high frequency sensor data to an adverse, recurrent event process. In a smoking

Date: April 15, 2022.
Key words and phrases. recurrent events; probabilistic subsampling; estimating equations; high frequency

time series; logistic regression.

1

 
 
 
 
 
 
2

WALTER DEMPSEY

cessation mHealth study [Spring, 2019], for example, the relation between a time-varying

sensor-based measure of physiological stress and smoking lapse is of scientiﬁc interest.

In a suicidal ideation mHealth study [Kleiman et al., 2018], the relation of electrodermal

activity (EDA) and accelerometer with self-identiﬁed moments of suicidal ideation is of

scientiﬁc interest.

The goal of this paper is two-fold: (1) to discuss the appropriate choice of statistical

model for joint high-frequency sensor data and the recurrent event data, and (2) to

construct a simple, easy-to-implement method for parameter estimation and inference.

For (1), we discuss an important issue regarding measurement-error models when paired

with recurrent event outcomes. For (2), we introduce a random subsampling procedure

that has several beneﬁts. First, the resulting inference is unbiased; however, there is

a computation-eﬃciency trade-oﬀ.

In particular, a higher sampling rate can decrease

estimator variance at the cost of increased computation. We show via simulations that the

beneﬁts of incredibly high sampling rates is often negligible, as the contribution to the

variation is small relative to the variation in the underlying stochastic processes. Second,

derived estimating equations are optimal, implying loss of statistical eﬃciency is only due

to subsampling procedure and not the derived methodology. Finally, implementation can

leverage existing, standard software for functional data analysis and logistic regression,

leading to fast adoption by domain scientists.

2. Recurrent event process and associated high frequency data

Suppose n subjects are independently sampled with observed event times Ti = {Ti,1

, . . . , Ti,ki
over some observation window [0, τi] for each subject i = 1, . . . , n. Assume the event times
are ordered, i.e., Ti, j < Ti,j(cid:48) for j < j(cid:48). The observation window length, τi, is the censoring

}

time and is assumed independent of the event process. Let Ni(t) denote the associated
counting process of Ti; that is, Ni(t) = (cid:80)ki
j=1 1[Ti, j < t]. In this section, we assume a single-
dimensional health process xi = {xi(s)}
0<s<τi for each participant is measured at a dense grid

of time points. Accelerometer, for example, is measured at a rate of 32Hz (i.e., 32 times

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

3

per second). Electrodermal activity (EDA), on the other hand, is measured at a rate of

4Hz (i.e., 4 times per second). Given the high frequency nature of sensor data, this paper

assumes the process is measured continuously.

= HN
i,t

Let HNX
i,t

i,t be the σ-ﬁeld generated by all past values (Ni(s), xi(s))0≤s≤t. In this
paper, the instantaneous risk of an event at time t is assumed to depend on the health

⊗ HX

process, time-in-study, and the event history through a fully parametric conditional hazard

function:

(1)

(cid:16)
t

(cid:12)(cid:12)(cid:12) HNX

i,t

; θ

(cid:17)

hi

= lim
δ→0

δ−1 pr

(cid:16)
Ni(t + δ) − Ni(t) > 0 | HNX
i,t

(cid:17)

,

where θ is the parameter vector. For high frequency physiological data, we assume that

current risk is log-additive and depends on a linear functional of the health process over

some recent window of time and some pre-speciﬁed features of the counting process; that

is,

(2)

(cid:16)

hi

t | HNX
i,t

; θ

(cid:17)

(cid:32)

= h0(t; γ) exp

gt

(cid:17)(cid:48)

(cid:16)
HN
i,t

α +

(cid:33)
xi(s)β(s)ds

(cid:90) t

t−∆

and gt(HN

where h0(t; γ) is a parametrized baseline hazard function, ∆ is an unknown window-length,
i,t) ∈ Rp is a p-length feature vector summarizing the event-history and time-in-
(cid:82) t
t−∆ xi(s)β(s)ds reﬂects the unknown linear functional

study information. The ﬁnal term

form of the impact of the time-varying covariate on current risk.

i,t) ∈
An alternative to (2) would be to construct features from the sensor data history ft(HX
Rq and incorporated these features in the place of the ﬁnal term. Our current approach

builds linear features of HX

i,t directly from the integrated history, avoiding the feature

construction problem – a highly nontrivial issue for high frequency time-series data. The

main caveat is the additional parameter ∆; however, as long as the estimated ˆ∆ exceeds ∆,

then resulting estimation is unbiased albeit at a loss of eﬃciency. Moreover, sensitivity

analysis can be performed to determine how choice of ˆ∆ aﬀects inference. One limitation

of the approach presented here is that only fully parametric hazard models may be ﬁt to

4

WALTER DEMPSEY

the data. However, a spline model for the log baseline hazard aﬀords suﬃcient model

ﬂexibility.

2.1. Measurement-error models with event processes. One potential criticism for (1) is

that the health process may be measured with error. A common mathematical strategy
for joint models is to consider an unobservable, latent process ηi such that Ti ⊥⊥ xi | ηi, i.e.,

the two processes are conditionally independent given the latent trajectory. For example,
take ηi to be a zero-mean Gaussian process with

(3)

xi(t) = ηi(t) + (cid:15)i(t),

and log hi(t | η) = log h0(t) + gt

(cid:17)(cid:48)

(cid:16)
HN
i,t

α +

(cid:90) t

t−∆

ηi(s)β(s)ds

where (cid:15)i(t) is a white-noise measurement error term. Thus, ηi(t) is the “true and unobserved

value of the longitudinal outcome” [Rizopoulos, 2010, Sec. 2.1, pp.3]. The conditional
survival function of the jth event Tj given xi and all prior events T−j := {T1

, . . . , Tj−1

} is

(cid:16)

pr

Tj > t + s

(cid:12)(cid:12)(cid:12) H X

i,τi

(cid:17)
, T−j = t−j, T j > t

(cid:32)

= E

exp

(cid:90) t+s

(cid:32)

−

hi(u | η)du

(cid:33)

(cid:12)(cid:12)(cid:12) xi, T−j = t−j

(cid:33)

,

t

where the expectation is a Gaussian integral, albeit inﬁnite-dimensional. For most choices

of covariance structure, if the white-noise error term is non-zero then the above calculation

will show the conditional survival function depends not only on past x-values, but also on

future x-values, i.e., x and T do not satisfy independent evolution [Dempsey and McCullagh,

2019]. This is quite unnatural, as (3) suggests the instantaneous risk of an event at time t

depends on future values of the sensor process. To ensure independent evolution, we
set (cid:15)(t) ≡ 0 and treat xi as a Gaussian process measured without a white-noise error term.

Our procedure will only rely on the Gaussian assumption when x is not fully observed

(see Section 5.2).

2.2. Likelihood calculation. For the sake of notational simplicity, we leave the depen-
implicit, and write hi(t; θ). The component

dency of the conditional hazard function on HNX
i,t

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

5

of the log-likelihood related to the event process is then given by

Ln(θ) =

n(cid:88)

i=1




ki(cid:88)

j=1

(cid:16)
hi

(cid:16)
Ti, j; θ

(cid:17)(cid:17)

log



− Hi (τi; θ)

where Hi(τi; θ) = (cid:82) τi
score equations Un(θ) = 0 yields the maximum likelihood estimator ˆθ, where

hi(t; θ)dt is the cumulative hazard function. Solving the associated

0

Un(θ) =

n(cid:88)

i=1




ki(cid:88)

j=1

i (Ti, j; θ)
h(1)
hi(Ti,j; θ)



i (τi; θ)

− H(1)

,

with h(1)

i (Ti, j; θ) and H(1)

i (τi; θ) are derivatives with respect to θ.

In classical joint models [Henderson et al., 2000, Tsiatis and Davidian, 2004], time-

varying covariates xi(t) are observed only intermittently at appointment times. In our
current setting, maximizing the likelihood is computationally prohibitive since for any θ
we must compute the cumulative hazard functions Hi(τi; θ) which require integration
(cid:82) t
of hi(t; θ) given by (2) which itself depends on the integral
t−∆ xi(s)β(s)ds that is a function
of the unknown functional parameter β(·). That is, the risk model now depends on an

integrated past history of the time-varying covariate which leads to severe increase in

computational complexity.

2.3. Probabilistic subsampling framework. To solve the computational challenge we

employ a point-process subsampling design to obtain unbiased estimates of the derivative

of the cumulative hazards for each subject. The subsampling procedure treats the collected

sensor data as a set of potential observations. Suppose covariate information is sampled

counting process features gt(HN

at times drawn from an independent inhomogeneous Poisson point process with known
intensity πi(t). At a subsampled time t, the windowed covariate history {xi(t − s)}

0≤s≤∆ and
i,t) are observed. Optimal choice of πi(t) is beyond the scope
of this paper; however, simulation studies have suggested setting the subsampling rate
proportional to the hazard function hi(t; θ).

An estimator is design-unbiased if its expectation is equal to that parameter under the
probability distribution induced by the sampling design [Cassel et al., 1977]. Let Di ⊂ [0, ti]

6

WALTER DEMPSEY

denote the random set of subsampled points. Note, by construction, this random set is
distinct from the set of event times with probabilty one, i.e., pr(Ti ∩ Di = ∅) = 1. Under
subsampling via πi(t), one may compute a Horvitz-Thompson estimator of the derivative
i (u; θ)/πi(u). An alternative design-unbiased
of the cumulative hazard ˆH(1)

i (τi; θ) = (cid:80)
estimator of the derivative of the cumulative hazards is given by

u∈Di h(1)

(4)

ˆH(1)

i (τi; θ) =

(cid:88)

u∈(Ti∪Di)

i (u; θ)
h(1)
πi(u) + hi(u; θ)

Equation (4) is the estimator suggested by Waagepetersen [2008]. This estimator depends

on the superposition of the event and subsampling processes. Proposition 3.6 shows the

estimator for θ associated with using (4) is the most eﬃcient within a suitable class of

estimators for the derivative of the cumulative hazard function (including the Horvitz-

Thompson estimator). Therefore, we restrict our attention to (4) for the remainder of this

paper. Letting

(5)

wi(t; θ) =

πi(t)
πi(t) + hi(t; θ)

,

the resulting approximate estimating equations can be re-written as

(6)

ˆUn(θ) =

n(cid:88)

i=1





(cid:88)

u∈Ti

wi(u; θ)

h(1)(u; θ)
h(u; θ)

−

(cid:88)

u∈Di

wi(u; θ)

i (u; θ)
h(1)
πi(u)





.

Equation (6) represents the approximate score functions built via plug-in of the design-

unbiased estimator of the derivative of the cumulative hazard given in (4).

Remark 2.1 (Connection to design-based estimation). The approximate score equations given

by (6) arise in design-based inference of point processes. Design-based inference is common for

spatial point processes [Waagepetersen, 2008] where the spatial varying covariate is observed

at a random sample of locations.

It is common in mobile health where ecological momentary

assessments [Rathbun, 2012, Rathbun and Shiﬀman, 2016] are used to randomly sample individuals

at various time-points to assess their emotional state. In the current setting, we leverage these

ideas to form a subsampling protocol that can substantially reduce computationally complexity.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

7

Therefore, the purpose is quite diﬀerent. Moreover, the dependence of the intensity function on the

recent history of sensor values leads to additional complications that must be addressed.

3. Longitudinal functional principal components within event-history analysis

Probabilistic subsampling converts the single sensor stream xi into a sequence of func-

tions observed repeatedly at sampled times Di and event times Ti. Such a data structure is

commonly referred to as longitudinal functional data [Xiao et al., 2013, Goldsmith et al., 2015].

Given the large increase in longitudinal functional data in recent years, corresponding

analysis has received much recent attention [Morris et al., 2003, Morris and Carroll, 2006,

Baladandayuthapani et al., 2008, Di et al., 2009, Greven et al., 2010, Staicu et al., 2010, Chen

and M ¨uller, 2012, Li and Guan, 2014]. Here, we combine work by Park and Staicu [2015]

and Goldsmith et al. [2011] to construct a computationally eﬃcient penalized functional
method for solving the estimation equations ˆUn(θ).

3.1. Estimation of the windowed covariate history. We start by deﬁning X(t, s) = x(t − s)
to be the sensor measurement 0 ≤ s ≤ ∆ time units prior to time t ∈ Ti ∪ Di. We use the
sandwich smoother [Xiao et al., 2013] to estimate the mean µy(t, s) = Ey[X(t, s)] where
the expectation is indexed by whether t is an event (y = 1) or subsampled (y = 0) time

respectively. Alternative bivariate smoothers exist, such as the kernel-based local linear

smoother [Hastie et al., 2009], bivariate tensor product splines [Wood, 2006], and the

bivariate penalized spline smoother [Marx and Eilers, 2006]. The sandwich smoother was

chosen for its computational eﬃciency and estimation accuracy. We then deﬁne ˜X(t, s) =
X(t, s) − ˆµy(t, s) to be the mean-zero process at each time t ∈ Ti ∪ Di.

As in Park and Staicu [2015], deﬁne the marginal covariance by

(cid:48)
Σy(s, s

) =

(cid:90) τ

0

(cid:48)
cy((T, s), (T, s

)) fy(T)dt.

for 0 ≤ s, s(cid:48) ≤ ∆, where cy((t, s), (t, s(cid:48))) is the covariance function of the windowed covariate
history X(t, ·) and fy(T) is the intensity function for event (y = 1) and subsampled (y = 0)
times respectively. Estimation of Σy occurs in two steps. For simplicity, we present the

8

WALTER DEMPSEY

steps for subsampled times (i.e., y = 0) but the steps are the same for event times as well.

First, the pooled sample covariance is calculated at a set of grid points:

˜Σ

0(sr, sr(cid:48)) =


n(cid:88)


i=1

|Di|



−1 


n(cid:88)

(cid:88)

i=1

t∈Di



˜X(t, sr) ˜X(t, sr(cid:48))

.



Due to our concern over independent evolution as discussed in section 2.1, we do not

elements of ˆΣ

assume that each observation is observed with white noise and therefore the diagonal
0 are not inﬂated. Second, the estimator ˆΣ is further smoothed again using the
sandwich smoother [Xiao et al., 2013]. Note Park and Staicu [2015] smooth the oﬀ-diagonal

elements, while here we smooth the entire pooled sample covariance matrix. All negative

eigenvalues are set to zero to ensure positive semi-deﬁniteness. The result is used as an

estimator ˆΣ

0 for the pooled covariance Σ

0.

Next, we take the spectral decomposition of the estimated covariance function; let
k (s), ˆλ(0)
}k≥1 be the resulting sequence of eigenfunctions and eigenvalues. The key

k

{ ˆψ(0)

beneﬁt of the marginal covariance approach is that it allows us to compute a single,

time-invariant basis expansion; this reduces the computational burden by avoiding the

three dimensional covariance function (i.e., covariance depends on t) and associated

spectral decomposition in methods considered by Chen and M ¨uller [2012]. Using the
Karhunen-Lo`eve decomposition, we can represent X(t, s) for t ∈ Ti ∩ Di by

X(t, s) = ˆµy(t, s) +

∞(cid:88)

k=1

i,k (t) ˆψ(y)
ˆc(y)

k (s) ≈ ˆµy(t, s) + c(y)

i

(cid:62) ˆψ(y)(s)

(t)

i,k (t) = (cid:82) t

(s))(cid:62),
where ˆc(y)
and Kx < ∞ is the truncation level of the inﬁnite expansion. Following Goldsmith et al.

(t))(cid:62), ˆψ(y)(s) = ( ˆψ(y)

i,1 (t), . . . , c(y)
i,Kx

1 (s), . . . , ˆψ(y)

k (s)ds, c(y)

˜Xi(t, s) ˆψ(y)

(t) = (c(y)

t−∆

Kx

i

[2011], we set Kx to satisfy identiﬁability constraints (see Section 3.2 for details). In subse-
quent sections, we leave the dependence on y (i.e., whether t ∈ Ti or ∈ Di) implicit unless

required for notational simplicity.

3.2. Estimation of β(s). The next step of our method is modeling β(s). Here, we leverage

ideas from the penalized spline literature [Ruppert et al., 2003, Wood, 2003]. Let φ(s) =

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

9

{φ

[b1

1(s), . . . , φKb(x)} be a spline basis and assume that β(s) = (cid:80)Kb
, . . . , bKb](cid:62). Thus, the integral in (2) can be restated as

j=1 bjφj(s) = φ(t)b where b =

(cid:90) t

t−∆

X(t, s)β(s)ds ≈

ˆµ(t, s) + c(t)

(cid:105)
(cid:62) ˆψ(s)

× [φ(s)b] ds

(cid:90) t

(cid:104)

t−∆
= [M

(cid:62)
i,t

(cid:62)
+ c(t)

J ˆψ,φ]b

where Mt = (M1,t, . . . , MKb
with the (k, l)th entry is equal to

,t), M j,t = (cid:82) t
(cid:82) ∆
0

t−∆ ˆµ(t, s)φj(s), and J ˆψ,φ is a Kx × Kb dimensional matrix
ˆψk(s)φl(s)ds [Ramsay and Silverman, 2005].

Given the basis for β(t), the model depends on choice of both Kb and Kx. We follow Rup-
pert [2002] by choosing Kb large enough to prevent under-smoothing and Kx ≥ Kb to satisfy

identiﬁability constraints. While our theoretical analysis considers truncation levels that
depend on n, in practice, we follow the simple rule of thumb and set Kb = Kx = 35. As

long as the choices of Kx and Kb are large enough, their impact on estimation is typically

negligible. Below, we will exploit a connection between (6) and score equations for a

logistic regression model. Before moving on, we introduce some additional notation.

Deﬁne

(7)

(cid:16)
t | HNX
i,t

(cid:17)
; θ

hi

≈ exp

(cid:16)
(cid:62)
Z
t

γ + gt

(cid:17)(cid:48)

(cid:16)
HN
i,t

α + M

(cid:62)

i,tb + C

(cid:17)
(cid:62)
i,tJ ˆψ,φb

= exp

(cid:16)
W

(cid:62)
i,t

(cid:17)
θ

,

where θ = (γ, α, b) and exp(Z(cid:62)
γ) =: h0(t) is the parameterized baseline intensity function.
t
We write ˜Un(θ) to denote the approximate score function when substituting in (7) for (2).

3.3. Connection to logistic score functions. We next establish a connection between the
above approximate score equations ˜Un(θ) and the score equations for a logistic regression

model. We can then exploit this connection to allow the model to be ﬁt robustly using

standard mixed eﬀects software [Ruppert, 2002, McCulloch and Searle, 2001].

10

WALTER DEMPSEY

Lemma 3.1. Under weights (5) and the log-linear intensity function (7), the approximate
score function ˜Un(θ) is equivalent to

n(cid:88)

(cid:88)

i=1

t∈Ti∪Di





1[t ∈ Di] −

1 + exp

(cid:104)

−

1
(cid:16)
˜W(cid:62)
i,t

θ + log πi(t)

(cid:17)(cid:105)





˜Wi,t

where ˜Wi,t = −Wi,t. This is the score function for logistic regression with binary re-
sponse Yi(t) for t ∈ Ti ∪ Di and i ∈ [n] where Yi(t) = 1[t ∈ Ti], oﬀset log πi(t), and

covariates ˜Wi,t.

This connection established by Lemma 3.1 between our proposed methodology and

logistic regression allows us to leverage “oﬀ-the-shelf” software. The main complication is

pre-processing of the functional data; however, these additional steps can also be taken care

of via existing software. Therefore, the entire data analytic pipeline is easy-to-implement

and requires minimal additional eﬀort by the end-user. To see this, we brieﬂy review the

proposed inference procedure.

Remark 3.2 (Inference procedure review). Given observed recurrent event and high
frequency data {Ti, xi}n

i=1,

(a) For each i ∈ [n], sample non-event times as a time-inhomogeneous Poisson point

process with intensity according to πi(t)

(b) Estimate mean µy(t, s) for 0 ≤ s ≤ ∆ at all event times t ∈ ∪n

i=1Ti and sampled

non-event times t ∈ ∪n

i=1Di.
(c) Compute marginal covariance across event times, Σ
(d) Compute eigendecomposition { ˆψ(y)
k
(e) Use the eigendecomposition to construct Wi,t for all i ∈ [n] and t ∈ Di ∪ Ti
(f) Perform logistic regression with binary outcome {Yi(t)} and oﬀset of log πi(t).

} of marginal covariance Σy

1, and non-event times, Σ

, ˆλ()
k

0.

Before demonstrating the methodology via simulation in Section 4 and a worked example

in Section 6, we provide a theoretical analysis of our current proposal.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

11

3.4. Theoretical analysis. Our theoretical analysis requires assumptions regarding the

subsampling procedure, the event process, and the functional data. We state these

assumptions and then our main theorems. We start by assuming there exists a τ < ∞ such
that all individuals are no longer at risk (i.e., τi < τ for all i). Moreover, deﬁne Ri(t) to be
the at-risk indicator for participant i, i.e., Ri(t) = 1[t ∈ (0, τi)]. Asymptotic theory provided

in Lemma 3.5 will be proven under regularity conditions A-E in [Andersen et al., 1993, pp.

420–421] along with the following additional assumptions:

Assumption 3.3 (Event process assumptions). We assume the following holds:

(E.1) The subsampling rate is both lower and upper bounded for all at-risk times; that

is, 0 < L < πi(t) < U < ∞ for all i = 1, 2, . . . and t ∈ [0, τ] such that Ri(t) = 1

(E.2) There exists a nonnegative deﬁnite matrix Ξ(θ) such that

−1Ξn(θ) = n
n

−1

(cid:90) τ

n(cid:88)

i=1

0

wi(t; θ) ×





i (t; θ)(cid:62)

i (t; θ)h(1)
h(1)
hi(t; θ)





× Ri(t)dt

P→ Ξ(θ).

(E.3) There exists M such that |Wi, j,t| < M for all (i, j, t).
(E.4) For all j, k

(cid:90) τ

n(cid:88)

−1

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

d2
dθ jdθk

hi(t; θ

0)

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Ri(t)dt

P→ C < ∞

as n → ∞.

i=1

0

We also require several assumptions due to the truncation of the Karhunen-Lo`eve

decomposition that represents X(t, s).

Assumption 3.4 (Functional assumptions [Park and Staicu, 2015]). The following assump-

tions are standard in prior work on longitudinal functional data analysis [Park and Staicu,

2015, Yao et al., 2005, Chen and M ¨uller, 2012]:

(A.1) X = {X(t, s) : (t, s) ∈ T × S} is a square integrable element of the L2(T × S)

(A.2) The subsampling and conditional intensity rate functions fy(T) are continuous

and sup | fy(T)| < ∞.

(A.3) E[X(t, s)X(t, s(cid:48))X(t(cid:48), s)X(t(cid:48), s(cid:48))] < ∞ for each s, s(cid:48) ∈ [0, ∆] and 0 < t, t(cid:48) < τ.

12

WALTER DEMPSEY

(A.4) E[(cid:107)X(t, ·)(cid:107)4] < ∞ for each 0 < t < τ.

Finally, for simplicity, we assume that there exists b(cid:63) such that β(t) = φ(t)b(cid:63); that is,
the true function β(t) sits in the span of the spline basis expansion. With the necessary

assumptions stated, we can now state Lemma 3.5, which provides asymptotic theory.

Lemma 3.5. Under Assumption 3.3, Assumption 3.4, and ∆ known, for large n the
estimator ˆθn is consistent; moreover,

√

n( ˆθ − θ) → N(0, Ξ(θ)

−1)

where

h(1)(s; θ) × h(1)(s; θ)(cid:62)
h(s; θ)
and τ is the random censoring time of the event process.

w(s; θ) ×

Ξ(θ) =

0

(cid:90) τ

(cid:34)

(cid:35)

ds.

An estimator for Ξ(θ) is

(8)

ˆΞ(θ) = n

−1

n(cid:88)

(cid:88)

i=1

t∈Ti∪Di

wi(t; θ)(1 − wi(t; θ))





i (t; θ)
h(1)
hi(t; θ)





×





i (t; θ)
h(1)
hi(t; θ)

(cid:62)





For the log-linear intensity model, the sampling-unbiased estimator for ˆΞ(θ) is equivalent

to the Fisher information for the previously described logistic regression model. This

implies that subsampling from an inhomogeneous Poisson process, standard logistic

regression software can be used to ﬁt the recurrent event model by specifying an oﬀset
equal to log πi(t). Not only this, Lemma 3.6 shows weights (5) are optimal within a

particular class of weighted estimating equations.

Lemma 3.6. If the event process is an inhomogeneous Poisson point process with inten-

sity h(t; θ) and subsampling occurs via an independent, inhomogeneous Poisson point
process with intensity π(t), then ˆUn(θ) are optimal estimating functions (i.e., most eﬃcient)

in the class of weighted estimating functions given by (6) replacing (5) by any weight
function wi(t; θ). This class includes the Horvitz-Thompson estimator under w(s; θ) = 1.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

13

Proposition 3.6 ensures the only loss of statistical eﬃciency is due to subsampling and not

using a suboptimal estimation procedure given subsampling.

3.5. Computation versus statistical eﬃciency tradeoﬀ. Under assumption 3.4, we next

consider the statistical eﬃciency of our proposed estimator when compared to complete-

data maximum likelihood estimation. While subsampling introduces additional variation,

it may signiﬁcantly reduce the overall computational burden.

It is this trade-oﬀ that

we next make precise. In particular, we consider the following choice of subsampling

rate, π(t) = c × h(t; θ) for c > 0. That is, the subsampling rate is proportional to the intensity

function with time-independent constant c > 0. Under this subsampling rate, the weight

function (5) is equal to c/(c + 1). Under Lemma 3.5,

Ξ(θ) = c
c + 1

(cid:90) τ

0

h(1)(t; θ)h(1)(t; θ)(cid:62)
h(t; θ)

dt = c
c + 1

Σ(θ)

where Σ(θ) is the Fisher information of the complete-data maximum likelihood estimator.
Therefore the relative eﬃciency is c/(1 + c). For an upper bound H = maxt∈(0,τ) h(t; θ), if we
set π(t) = c × H, then the relative eﬃciency can be lower bounded by c/(c + 1).

Sensor measurements occur multiple times per second. Suppose the intensity rate is

bounded above by 1 and the unit time scale was hours. If we then subsample the data at

a rate of 10 times per hour, then we have a lower bound on the eﬃciency of 0.909. For a

4Hz sensor, this reduces the number of samples per hour from 4 × 60 × 60 = 14, 400 per

hour to on average 10 per hour. While the computational complexity of logistic regression

is linear in the number of samples, we get 1440 times reduction in the data size at the cost

of a 0.909 statistical eﬃciency. If we sample 100 times per hour, then the eﬃciency loss is

only 0.999, with a 144 times reduction in data size. Table 1 provides additional examples

for a 4Hz and 32Hz sensor rate respectively. The data reduction depends on this rate;

however, the lower bound on statistical eﬃciency does not because the subsampling rate

only depends on the upper bound of the intensity function. In particular, if the events are

rare then subsampling rate can be greatly reduced with no impact to statistical eﬃciency.

14

WALTER DEMPSEY

Sensor rate

4Hz (EDA)

32Hz (ACC)

Subsampling
constant (c)
5
10
100
5
10
100

0.5
5760
2880
288

1
2880
1440
144

Upper bound on intensity rate per hour Statistical
eﬃciency
5
0.833
576
0.909
288
0.990
29
0.833
46080 23040 7680 4608
0.909
23040 11520 3840 2304
0.990
230
2304

10
288
144
14
2304
1152
115

3
960
480
48

1152

384

Table 1. Data reduction (total # of measurements divided by expected num-
ber of subsampled measurements) given sensor rate, subsampling constant
and an upper bound on the intensity rate.

3.6. Penalized functional regression models. Recall theoretical results were proven un-
der the assumption that there exists b(cid:63) such that β(t) = φ(t)b(cid:63). To make this assumption

plausible, we set Kb large enough (but less than Kx to ensure identiﬁability) to ensure
the spline basis expansion is suﬃciently expressive. However, in practice, such a choice

of Kb may lead to overﬁtting the data. Following Goldsmith et al. [2011], we choose the
polynomial spline model and set φ(t) = b0
chosen knots and assume {bj}Kb
j=3

j=3 are the
∼ N(0, σ2I) to induce smoothness on the spline model.

j=3(t − κj)3 where {κj}Kb

+ b1t + b2t2 + (cid:80)Kb

Combining the penalized spline formulation with Lemma 3.1 establishes a connection

between our approximate score equations and solving a generalized mixed eﬀects logistic

regression with oﬀset.

3.7. Conﬁdence intervals for β(t). Due to the connection with generalized mixed eﬀects

models, we can leverage existing inferential machinery to obtain variance-covariance
estimates of model parameters. That is, if ˆΣbb is the Kb × Kb dimensional matrix ob-

tained by plugging in the estimates of variance components into the formula for the
variance of ˆb, then the standard error for estimate at time t0 – i.e., ˆβ(t0) = φ(t0) ˆb – is given
φ(t0) ˆΣbbφ(t0)(cid:62). Then the approximate 95% conﬁdence interval can be constructed

by

(cid:113)

as ˆβ(t0) ± 1.96

φ(t0) ˆΣbbφ(t0)(cid:62).

(cid:113)

We acknowledge two important limitations of conﬁdence intervals obtained via this

approach. First, penalization may lead to conﬁdence intervals that perform poorly in

regions where ˆβ(t) is oversmoothed. Second, we ignore the variability inherent in the

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

15

longitudinal functional principal component analysis; that is, our estimates ignore the
variability in estimation of eigenfunctions ˆψ as well as the coeﬃcients ˆci,k(t). Joint modeling

could be considered as in Crainiceanu and Goldsmith [2010], however, this is beyond the

scope of this article.

4. Simulation study

We next assess the proposed methodology via a simulation study. Here, we assume each

individual is observed over ﬁve days where each day is deﬁned on the unit interval [0, 1]

with 1000 equally spaced observation times per day. We deﬁne X(t) at the grid of

observations as a mean-zero Gaussian process with covariance

(cid:48)
Σ(t, t

) =

σ2
Γ(v)2ν−1

√

(cid:32)

2ν|t − s|
ρt

(cid:33)ν

√

(cid:32)

Kv

(cid:33)

2ν|t − s|
ρt

where Kv is the modiﬁed Bessel function of the second kind. We set ν = 1/2, σ2 = 1,
and ρ = 0.3 as well as set Kb = Kx = 35. For simplicity, we assume Σ is known in
computation of the eigendecompositions. Given {X(t)}

0≤t≤1 for a given user-day, we
generate event times according a chosen hazard function h(t; θ). To mimic our real data,

we set

(cid:32)

h(t; θ) = exp

θ

0

+

(cid:90) ∆

(cid:33)
X(t − s)β(s)ds

.

0

We set ∆ to mimic a 30-minute window for a 12-hour day. We set θ

= log(5/1000) to set

0

a baseline risk of approximately 5 events per day. We consider two choices of β(s): (1)

β
0

+ exp(−β

1s), and (2) β

1

∗ sin

(cid:16)

2π s
∆

(cid:17)
− π/2

.

We generate 1000 datasets, each consisting of 500 user-days. For a given simulated

user-day, we randomly sample non-event times using a Poisson process with rate of a
halfhour. We use the proposed methodology to construct the estimate ˆβi,0.5(t) for the

ith simulated dataset; we then subsample the sampled non-event times with thinning

probability 1/2, 1/4 and 1/8. This results in randomly sampled non-event times given by

a Poisson process with rates of an hour, two hours, and four hours. We can construct the
corresponding estimates: ˆβi,1(t), ˆβi,2(t), and ˆβi,4(t) respectively. Subsampling allows us to

16

WALTER DEMPSEY

compare the variance due to subsampling as compared to the variance due to sampling

fewer non-event times.

1
1000

(cid:80)1000
i=1

error (MISE) deﬁned as

Since we are primarily interested in accuracy, we report the mean integrated squared
(cid:82) ∞
0 ( ˆβi, j(s) − β(s))2ds for j = 0.5, 1, 2, 4 where β(s) ≡ 0 for
s > ∆. The MISE is deﬁned in this manner to account for settings where ∆ is unknown. Next,
ˆβi,j(t) denote the average estimate for j = 0.5, 1, 2, 4. Then the squared
let ¯βj(t) = 1
1000
(cid:82) ∞
(cid:80)1000
0 ( ˆβi,j(s) − ¯βj(s))2ds.
j=1
(cid:82) ∞
0 ( ˆβi, j(s) − ˆβi,0.5(s))2ds. Table 3 show the
MISE decomposed into the variance and squared biased as well as the subsampling

(cid:80)1000
i=1
(cid:82) ∞
0 ( ¯βj(s) − β(s))2ds and the variance is given by 1

The subsampling variance is deﬁned as 1
1000

bias is given by

(cid:80)1000
i=1

1000

variance. To allow fair comparisons across the two choices of β(s), all reported numbers

are scaled by the integrated square of the true function

(cid:82) ∞
0

β(s)2ds. The average runtime

(in seconds) is also reported.

Table 3 demonstrates that the variance does increase as the sampling rate decreases.

However, the rate of increase in the MISE is quite low relative. In the second case study, for

example, the MISE increases by 6% while the run time is 3 times faster. In the ﬁrst case study,

the MISE remains roughly constant but the run time is 4.7 times faster. This highlights the

eﬃciency-computation trade-oﬀ. Extrapolating to a complete data analysis, we can see

that the run time will continue to increase signiﬁcantly with minimal improvement in the

mean integrated squared error.

4.1. Impact of ∆. A concern with the proposed approach is the selection of window-

length, ∆. Here we investigate the impact of misspeciﬁcation of the window length
· sin (2πs/∆ − π/2) and the true window length (∆(cid:63)) is set to 32-minutes.

for β(t) = β
1

See Appendix C.1 for a similar discussion for β(t) = β

0 exp

(cid:0)−β

(cid:1)
1s

. As in the previous

simulation, we generate 1000 datasets per condition each consisting of 500 user-days. For

each simulation, we analyze the data using window lengths ∆ ∈ (26, 29, 32, 35, 37).

When the window length is too large, i.e., ∆ > ∆(cid:63), then asymptotically the estimation
is unbiased as β(t) ≡ 0 for t > ∆; however, we incur a penalty in ﬁnite samples, especially

for settings where the function is far from zero near t = ∆. We ﬁnd the MISE increases the

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

17

Subsampling variance -

Variance
Squared Bias
MISE
Avg. runtime (secs)

Subsampling variance -

Variance
Squared Bias
MISE
Avg. runtime (secs)

0.5

7.9 × 10−3
9.8 × 10−2
1.1 × 10−1
317

1.0 × 10−2
6.9 × 10−2
7.9 × 10−2
36

Sampling rate

1
9.0 × 10−7
9.2 × 10−3
9.5 × 10−2
1.0 × 10−1
177
2.9 × 10−6
1.3 × 10−2
6.5 × 10−2
7.9 × 10−2
23

2
2.9 × 10−6
1.0 × 10−2
9.3 × 10−2
1.0 × 10−1
104
8.9 × 10−6
2.0 × 10−2
6.0 × 10−2
8.0 × 10−2
15

4
5.8 × 10−6
1.30 × 10−2
9.2 × 10−2
1.0 × 10−1
68
2.0 × 10−5
3.1 × 10−2
5.4 × 10−2
8.4 × 10−2
12

Table 2. Mean-integrated squared error, varaince, squared bias, and sub-
sampling variance for β(s) given by (1) and (2) respectively.

absolute error |∆ − ∆(cid:63)| increases, while the variance decreases as a function of ∆. While
the MISE increased for ∆ < ∆(cid:63), the pointwise estimation error remains low for t < ∆.
This does not hold for ∆ > ∆(cid:63), where instead we see parameter attenuation, i.e., a bias
towards zero in the estimates at each 0 < t < ∆. To capture this, we deﬁne a partial
(cid:82) ˜∆
0 ( ˆβi,j(s) − ˆβ(s))2ds where ˜∆ = min(∆, ∆(cid:63)), which is the MISE on the subset

MISE as
0 < t < min(∆, ∆(cid:63)) and scaled for comparative purposes.

∆
˜∆

∆ =

26

29

32

35

37

MISE

0.233
0.244

0.5
0.400

4
0.413

0.239
0.251

0.225
0.240

Sampling rate
2
1
0.409
0.404
Variance 1.1 × 10−2 1.3 × 10−2 1.5 × 10−2 1.9 × 10−2
0.229
P-MISE
0.243
MISE
Variance 9.2 × 10−3 1.1 × 10−2 1.2 × 10−2 1.8 × 10−2
0.109
P-MISE
0.103
MISE
Variance 8.0 × 10−3 8.8 × 10−3 1.0 × 10−2 1.4 × 10−2
0.103
P-MISE
0.383
MISE
Variance 7.0 × 10−3 7.7 × 10−3 9.3 × 10−3 1.1 × 10−2
0.270
P-MISE
0.588
MISE
Variance 6.0 × 10−3 6.5 × 10−3 7.9 × 10−3 1.0 × 10−2
0.470
P-MISE

0.269,
0.589

0.273
0.589

0.269
0.591

0.107
0.105

0.105
0.384

0.117
0.105

0.105
0.386

0.111
0.103

0.103
0.384

0.470

0.473

0.469

Table 3. Mean-integrated squared error (MISE), variance, and partial MISE
as a function of ∆(cid:63) and sampling rate when true ∆ = 32 minutes.

18

WALTER DEMPSEY

5. Extensions

In this section, we demonstrate the ﬂexibility of the proposed approach by exploring

extensions in several important directions to ensure these methods are robust for practical

use with high frequency data. This section will continue to leverage the connection to

generalized functional linear models provided by Lemma 3.1.

5.1. Multivariate extensions. In this section, we extend our model to the case of mul-
tiple functional regressors. That is, suppose L health processes, i.e., xi = {xi(t) =
(xi,1(t), . . . , xi,L(t))}

0<s<τi, for each participant is measured at a dense grid of time points.

In the suicidal ideation case study, for example, accelerometer is measured at a rate of

32Hz while electrodermal activity (EDA) is measured at a rate of 4Hz. A multivariate

extension of our model (1) is given by

(9)

(cid:16)
t | HNX
i,t

(cid:17)
; θ

hi

(cid:32)

= h0(t; γ) exp

gt

(cid:17)(cid:48)

(cid:16)
HN
i,t

α +

(cid:90) t

t−∆

1

xi,1(s)β

1(s)ds + · · · +

(cid:33)
xi,L(s)βL(s)ds

(cid:90) t

t−∆L

The approach given in Section 3.3 extends naturally to the multivariate functional setting.
For each functional regressor, we estimate the pooled sample covariance Σy,l for y ∈ {0, 1}
ˆψ(y)
ˆλ(y)
k,l (s) ˆψk,l(t) be the spectral decomposition
and l = 1, . . . , L as in Section 3.1. Let
k,l
Xl(t, s)βl(t) =
of ˆΣy,l. Then xi, j(t) is approximated using a truncated Karhunen-Loeve
(cid:20)
Ml,t + cl(t)(cid:62)J ˆψ(y)

(cid:80)∞
k=1

(cid:82) t
t−∆

bl.

,φ

(cid:21)

l

l

l

5.2. Missing data. Sensor data can often be missing for intervals of time due to sensor

wearing issues.

In the suicidal ideation case study, for example, there are 2139 self-

identiﬁed moments of distress across all 91 participants. Of these, 1289 event times had

complete data for the prior thirty minutes, 1984 had fraction of missing data on a ﬁne grid

less than 30%, and 1998 had fraction of missing data on a ﬁne grid less than 10%.

Missing data is a critical issue because ci,k(t) cannot be estimated if X(s, t) is not observed
for all s ∈ [0, ∆]. Moreover, standard errors should reﬂect the uncertainty in these

coeﬃcients when missing data is prevalent. Goldsmith et al. [2011] suggest using best

linear unbiased predictors (BLUP) or posterior modes in the mixed eﬀects model to

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

19

estimate ci,k(t); however, this is ineﬀective when there is substantial variability in these

estimates. To deal with this, Crainiceanu and Goldsmith [2010] take a full Bayesian

analysis. Yao et al. [2005] introduced PACE as an alternative frequentist method. Petrovich

et al. [2018] shows that for sparse, irregular longitudinal, the imputation model should

not ignore the outcome variable Yi(t).

Here we present an extension of Petrovich et al. [2018] to our setting by leveraging

Lemma 3.1 and the marginal covariance estimation procedure to construct a multiple
imputation procedure. Let xi(t) denote incomplete sensor data at time t (i.e., at times {si,r}kit
r=1
in [0, ∆]. Then

E[Xi(s, t) | Yi(t) = y, xi(t)] = µy(s, t) + a

(cid:62)

i,t(s)Bi,t(xi(t) − µi(t))

Xi(s, t), Xi(s

(cid:48), t) | Yi(t) = y, xi(t)

(cid:1) = Σt(s, s

(cid:48)

(cid:62)
) − ai,t(s)

(cid:48)
Bi,tai,t(s

)

(10)

(11)

(cid:0)

Cov

where we have

ai,t(s)

(cid:62) =





, s)

Σt(si,1
...

Σt(si,kit

, s)





; B

−1
i,t

=





Σt(si,1
Σt(si,1
...

, si,1) Σt(si,1
, si,2)
. . .
, si,2)

· · ·
...

· · ·

Σt(si,kit

, si,kit)





,

j=1 and µy(s, t) is the mean of X(t, s) from group y,
µi(t) = E[xi(t) | Yi(t) = y] = {µy(s j, t)}r
and Σt(s(cid:48), s) is the covariance between X(s(cid:48), t) and X(s, t) for s(cid:48), s ∈ [0, ∆] and t ∈ R+. To
account for overlap between ∆-windows for t ∈ Ti ∪ Di - i.e., avoid imputing values at a
particular time to diﬀerent values – imputation is performed sequentially over this set.

5.2.1. Multiple imputation under uncongeniality. Multiple imputation yields valid frequen-

tist inferences when the imputation and analysis procedure are congenial [Meng, 1994];

the above procedure is derived for function-on-scalar multiple imputation for binary

outcomes, which ignored the joint nature of recurrent event analysis in the presence of

high frequency sensor data. The main advantage of the above imputation framework is

its simplicity and approximate congeniality when events are rare and the sampling rate

is low. The main disadvantage is that the above framework is uncongenial under many

20

WALTER DEMPSEY

events and/or high sampling rates. Again, congeniality ensures good frequentist coverage

properties. A key question is whether we can use the above imputation methods within a

general procedure to handle uncongeniality.

To address this, we use the recommendation from Bartlett and Hughes [2020] and

consider a method that ﬁrst bootstraps a sample from the dataset and then apply multiple

imputation to each bootstrap sample. This general approach was originally proposed

by Shao and Sitter [1996] and Little and Rubin [2002]. We suppose B bootstraps and M
imputations per bootstrap; let ˆθb,m denote the estimator for the mth imputation of the bth
bootstrap. The point estimator is given by (B)−1 (cid:80)B
ˆθb,m. To con-
b=1

ˆθb where ˆθb = M−1 (cid:80)M
m=1

(cid:80)B

1
B(M−1)

i.e., MSW =

struct the conﬁdence interval, we require mean sum of squares with and between boostraps,
b=1 M · ( ˆθb − ˆθ)( ˆθb − ˆθ)(cid:62)
m=1( ˆθb,m − ˆθb)( ˆθb,m − ˆθb)(cid:62) and MSB = 1
B−1
respectively. Then the estimator of the variance-covariance matrix of ˆθ is given by
MSB − MSW/M. We obtain the varaiance for β(t) by φ(t) ˆΣB,Mφ(t)(cid:62). We fol-
ˆΣB,M =

(cid:80)M

(cid:80)B

b=1

(cid:17)

(cid:16) B+1
BM

low Bartlett and Hughes [2020] and construct conﬁdence intervals based on Satterthwaite’s

degrees of freedom, which here is given by

(cid:62)
ˆν = φ(t) ˆΣB,Mφ(t)

The bootstrap followed by multiple imputation procedure has been studied extensively

by Bartlett and Hughes [2020] and is robust to uncongeniality. The main disadvantage of

this approach is its considerable computational intensity. Recall likelihood calculations

were computationally prohibitive by themselves, so combining with bootstrap and MI

would further increase this large-scale computation. The random subsampling framework

thus simpliﬁes handling of missing data via connections to function-on-scalar multiple

imputation by Petrovich et al. [2018] as well as to bootstrap to handle uncongeniality

by Bartlett and Hughes [2020]. Ignoring the computational time of bootstrap sampling, the

computational time for the ﬁrst choise in the simulation study with B = 200 bootstraps and

M = 2 imputations per bootstrap leads to 35 hours for a sampling rate of 0.5 compared to

7 hours for a sampling rate of 4, which highlights the beneﬁts of the proposed framework.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

21

5.3. Multilevel models. The approach can be extended to multilevel models with func-

tional regressors, which are critical in mobile health where a high degree of individual
variation is often observed. Let bi ∼ N(0, σ2

b), then the multilevel extension of (1) is

(12)

(cid:16)
t | HNX
i,t

hi

(cid:17)

; θ, bi

≈ exp

(cid:16)
(cid:62)
Z
t

γ + gt

(cid:17)(cid:48)

(cid:16)
HN
i,t

(cid:104)

α +

(cid:62)
M
i,t

(cid:62)
+ C
i,tJ ˆψ,φ

(cid:105)

(cid:17)
(β + bi)

= exp

(cid:16)
W

(cid:62)
i,t

(cid:62)
θ + Z
i,tbi

(cid:17)

,

where Zi,t = Mi,t + Ci,tJ ˆψ,φ. Lemma 3.1 implies that the random subsampling framework
applied to equation (12) leads to a penalized logistic mixed-eﬀects model. As far as the
authors are aware, the combination of mixed-eﬀects and L2-penalization on a subset of

parameters has not been addressed in existing packages. To address this, we derive a

penalized adaptive Gaussian quadarture (PADQ) for estimation. For conciseness, the

algorithm is presented in Appendix B.

6. A worked example: Adolescent psychiatric inpatient mHealth study

During an eight month period in 2018, 91 psychiatric inpatients admitted for suicidal risk

to Franciscan Children’s Hospital were enrolled in an observational study. Study data were

graciously provided by Evan Kleiman and his study team (https://kleimanlab.org).

Each study participant wore an Empatica E4 [Garbarino et al., 2014], a medical-grade

wearable device that oﬀers real-time physiological data. On each user-day, participants

were asked to self-identify moments of suicidal distress. At these times, the participant

was asked to press a button on the Empatica E4 device. The timestamp of the button

press was recorded. One of the primary study goals was to assess the association between

sensor-based physiological measurements and self-identiﬁed moments of suicidal distress.

In particular, the scientiﬁc question is whether there are early indicators of escalating

distress by monitoring physiological correlates.

A key concern is whether all moments of suicidal distress are recorded. To ensure this,

clinical staﬀ interviewed participants in the evening who were then asked to review their

button press activity. Any events that were identiﬁed as incorrect button press activity

were removed. At the end of the 30-day study period, the average number of button

22

WALTER DEMPSEY

presses per day was 2.42 with a standard deviation of 2.62. Investigation of the button

press data shows low button-press counts prior to 7AM and a sharp drop oﬀ by 11PM.

This demonstrates an additional concern: events can only occur when the individual is

at-risk, i.e., (A) the individual is wearing the Empatica E4 and (B) is currently awake. To

deal with (A) and (B), here we deﬁne each study day to begin at 9AM and end at 8PM.

Figure 1. User button-presses (red) versus time since study entry (in hours).
The black mark indicates the ﬁnal sensor measurement time.

Figure 1 visualizes button presses versus time since study entry for each user. Day 30 is

assumed to censor the observation process. A black mark signals dropout before day 30.

Figure 1 shows the potential heterogeneity in button press rates between users and study

days. To assess whether there is between user or between study day variation, Table 4

presents a two-way ANOVA decomposition of the button press counts as a function of

participant and day in study. The ANOVA decomposition demonstrates high variation

with day in study and across users.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

23

DF Sum Sq Mean Sq F value
88
Participant
Day in Study 29
672

2675.4
443.3
2304.2

30.4
15.3
3.4

Residuals

8.9
4.5

Pr(>F)
< 2 × 10−16
3.9 × 10−13

Table 4. ANOVA decomposition of daily button press counts

Here, we focus on two physiological processes – (1) electrodermal activity (EDA), a

measure of skin conductance measured at 4Hz, and (2) activity index [Bai et al., 2016],

a coordinate-free feature built from accelerometer data collected at 32Hz that measures

physical movement. Electrodermal activity can be signiﬁcantly impacted by external

factors (e.g., room temperature). To account for the high between user-day variation,

we analyze EDA standardized per study-day and device. The individual EDA and AI

trajectories are highly variable which tends to obscure patterns and trends. In Figure 3, the

mean trajectories of EDA and AI are plotted in reverse time from button press timestamps,

which shows sharp changes in EDA and AI in the 10 minutes prior to button presses.

(a) Electrodermal activity (EDA)

(b) Activity Index (AI)

Figure 2. Average scaled EDA and AI in the 30 minutes prior to button
presses

6.1. Complete-case analysis. Inspection of Figures 2a and 2b suggest setting ∆ = 30

minutes is adequate to capturing the proximal impact of EDA and AI on the risk of

a button press. To ensure minimal loss of eﬃciency, the subsampling rate was set to

24

WALTER DEMPSEY

once every ﬁfteen minutes. Given the daily button press rate, this ensures an average

of 44 non-events to 2.5 events per day. Based on Table 1, this ensures we can achieve a

substantial data reduction at a minimal loss of eﬃciency. After sampling non-event times,

complete-case analyses are performed, i.e., sampled times where sensors included in the

model have any level of missing data are ignored.

We analyzed activity index (AI) and electrodermal activity separately as given in

equation (2) as well as jointly in a multivariate model as in equation (9). Results did not

change substantially. Figure 3a and 3b presents the estimates from the separate analyses

with their associated 95% conﬁdence intervals. We highlight in gray the statistically

signiﬁcant regions. Here, we see that standardized EDA is not associated with increased

risk of button press in our population-level model, while activity index sees a positive

association in the ﬁnal few minutes prior to a button press. In section ??, we investigate

whether the population-level results are sensitive to missing data and whether individual-
level coeﬃcients, β(t) + bi(t), eﬀects show stronger dependence on EDA and/or AI for

certain individuals.

(a) Electrodermal activity (EDA)

(b) Activity Index (AI)

Figure 3. β(t) for ACC and EDA with 95% CI; solid line is complete-case
analysis, while dotted line is for

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

25

7. Discussion

In this paper, we have presented a methodology for translating a diﬃcult functional

analysis with recurrent events problem into a traditional logistic regression. The trans-

lation leveraged subsampling and weighting techniques, speciﬁcally the use of weights

suggested by Waagepetersen [2008], along with ﬂexible functional data analysis methods

of Goldsmith et al. [2011] with marginal covariance methods for longitudinal functional

data of Park and Staicu [2015]. The proposed methodology abides by the comment of

Robert Gentleman on big data: “make data as small as possible as fast as possible.” Sub-

sampling and weighting converts the problem to well-known territory which allowed us

to leverage existing software. We show limited loss of eﬃciency when the subsampling

is properly tuned to the event rates. Important extensions to an online sampling algo-

rithm, optimal weighting when the Poisson point process assumption does not hold, and

non-linear functional data methods are considered important future work.

References

P.K. Andersen, O. Borgan, R.D. Gill, and N. Keiding. Statistical models based on counting

processes. 1993.

Jiawei Bai, Chongzhi Di, Luo Xiao, Kelly R. Evenson, Andrea Z. LaCroix, Ciprian M.

Crainiceanu, and David M. Buchner. An activity index for raw accelerometry data and

its comparison with other activity metrics. PLOS ONE, 11(8):1–14, 08 2016. doi: 10.1371/

journal.pone.0160644. URL https://doi.org/10.1371/journal.pone.0160644.

V. Baladandayuthapani, B.K. Mallick, M. Young Hong, J.R. Lupton, N.D. Turner, and

R.J. Carroll. Bayesian hierarchical spatially correlated functional data analysis with

application to colon carcinogensis. Biometrics, 64(1):64–73, 2008.

Jonathan W Bartlett and Rachael A Hughes. Bootstrap inference for multiple imputation

under uncongeniality and misspeciﬁcation. Statistical Methods in Medical Research, 2020.

C-M. Cassel, S¨arndal, and J.H. Wretman. Foundations of inference in survey sampling. Wiley,

New York, 1977.

26

WALTER DEMPSEY

K. Chen and H.-G. M ¨uller. Modeling repeated functional observations.

Journal of the

American Statistical Association, 107(500):1599–1609, 2012.

C. Crainiceanu and J. Goldsmith. Bayesian functional data analysis using winbugs. Journal

of Statistical Software, 32:1–33, 2010.

W. Dempsey and P. McCullagh. Vital variables and survival processes. Submitted, 2019.

C.Z. Di, C.M. Crainiceanu, B.S. Caﬀo, and N.M. Punjabi. Multilevel functional principal

component analysis. Annals of applied statistics, 3(1):458–488, 2009.

C. Free, G. Phillips, L. Galli, L. Watson, L. Felix, P. Edwards, V. Patel, and A. Haines. The

eﬀectiveness of mobile-health technology-based health behaviour change or disease

management interventions for health care consumers: A systematic review. PLOS

Medicine, 10(1):1–45, 2013.

M. Garbarino, M. Lai, D. Bender, R. W. Picard, and S. Tognetti. Empatica e3 — a wear-

able wireless multi-sensor device for real-time computerized biofeedback and data

acquisition. In 2014 4th International Conference on Wireless Mobile Communication and

Healthcare - Transforming Healthcare Through Innovations in Mobile and Wireless Technologies

(MOBIHEALTH), pages 39–42, 2014.

J. Goldsmith, J. Bobb, C. Crainiceanu, B. Caﬀo, and D. Reich. Penalized functional

regression. Jounal of Computational and Graphical Statistics, 20(4):830–851, 2011.

J. Goldsmith, V. Zipunnikov, and J. Schrck. Generalized multilevel functional-on-scalar

regression and principal component analysis. Biometrics, 71(2):344–353, 2015.

S. Greven, C. Crainiceanu, B. Caﬀo, and D. Reich. Longitudinal functional principal

component analysis. Electronic journal of statistics, 4:1022–1054, 2010.

T. Hastie, R. Tibshirani, and J Friedman. The elements of statistical learning. Springer Series

in Statistics. Springer, 2009.

R. Henderson, P. Diggle, and A. Dobson. Joint modeling of longitudinal measurements

and event time data. Biostatistics, 1:465–480, 2000.

P. Klasnja, S. Smith, N. Seewald, A. Lee, K. Hall, B. Luers, E. Hekler, and S.A. Murphy.

Eﬃcacy of contextually tailored suggestions for physical activity: A micro-randomized

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

27

optimization trial of heartsteps. Annals of Behavioral Medicine, 53:573–582, 2019.

Evan M. Kleiman, Brianna J. Turner, Szymon Fedor, Eleanor E. Beale, Rosalind W. Picard,

Jeﬀ C. Huﬀman, and Matthew K. Nock. Digital phenotyping of suicidal thoughts.

Depression and Anxiety, 35(7):601–608, 2018.

Y. Li and Y. Guan. Functional principal component analysis of spatio-temporal point

processes with applications in disease surveillance.

Journal of the american statistical

association, 109(507):1205–1215, 2014.

R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data. Wiley, 2nd edition,

2002.

B.D. Marx and P.H. Eilers. Low-rank scale-invariant tensor product smooths for generalized

additive mixed models. Biometrics, 62(4):1025–1036, 2006.

Charles E McCulloch and Shayle R. Searle. Generalized, Linear and Mixed Models. Wiley,

New York, 2001.

X.L. Meng. Multiple-imputation inferences with uncongenial sources of input (with

discussion). Statistical Science, 10:538–573, 1994.

J.S. Morris and R.J. Carroll. Wavelet-based functional mixed models. Journal of the Royal

Statistical Society, Series B, 68(2):179–199, 2006.

J.S. Morris, M. Vannucci, P.J. Brown, and R.J. (2003) Carroll. Wavelet-based nonparametric

modeling of hierarchical functions in colon carginogenesis.

Journal of the American

Statistical Association, 98(463):573–583, 2003.

S.Y. Park and A.M. Staicu. Longitudinal functional data analysis. Stat, 4(1):212–226, 2015.

J. Petrovich, M. Reimherr, and C. Daymont. Functional regression models with highly

irregular designs. 2018.

J. Ramsay and B. Silverman. Functional data analysis. Springer, New York, 2005.

S. Rathbun. Optimal estimation of poisson intensity with partially observed covariates.

Biometrika, 100:277–281, 2012.

S. Rathbun and S. Shiﬀman. Mixed eﬀects models for recurrent events data with par-

tially observed time-varying covariates: Ecological momentary assessment of smoking.

28

WALTER DEMPSEY

Biometrics, 72:46–55, 2016.

D. Rizopoulos. Jm: An r package for the joint modeling of longitudinal and time-to-event

data. Journal of Statistical Software, 35:1–33, 2010.

D. Ruppert. Selecting the number of knots for penalized splines. Journal of Computational

and Graphical Statistics, 11(4):735–757, 2002.

D. Ruppert, M. Wand, and R. Carroll. Semiparametric Regression. Cambridge University

Press, Cambridge, 2003.

Jun Shao and Randy R Sitter. Bootstrap for imputed survey data. Journal of the American

Statistical Association, 91(435):1278–1288, 1996.

B. Spring. Sense2stop: Mobile sensor data to knowledge. https://clinicaltrials.gov/

ct2/show/NCT03184389, 2019.

A.-M. Staicu, C.M. Crainiceanu, and R.J. Carroll. Fast methods for spatially correlated

multilevel functional data. Biostatistics, 11(2):177–194, 2010.

A.A. Tsiatis and M. Davidian. Joint modeling of longitudinal and time-to-event data: an

overview. Statistica Sinica, 14:809–834, 2004.

Rasmus Waagepetersen. Estimating functions for inhomogeneous spatial point processes

with incomplete covariate data. Biometrika, 95(2):351–363, 2008.

S. Wood. Generalized additive models: an introduction with R. Chapman & Hall, London,

2003.

S.N. Wood. Low-rank scale-invariant tensor product smooths for generalized additive

mixed models. Biometrics, 62(4):1025–1036, 2006.

L. Xiao, Y. Li, and D. Ruppert. Fast bivariate p-splines: the sandwich smoother. Journal of

the Royal Statistical Society: Series B, 75(3):5770–599, 2013.

F. Yao, H.-G. M ¨uller, and J.-L Wang. Functional data analysis for sparse longitudinal data.

Journal of the American Statistical Association, 100(470):577–590, 2005.

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

29

Appendix A. Derivation of the design-unbiased score equations

Proof of Lemma 3.1. Recall d

dθ log hi(t; θ) = h(1)

i

(t;θ)

hi(t;θ) . Under the log-linear intensity function

given by (7), d

dθ log hi(t; θ) = Wi,t and

d
dθ log (πi(t) + hi(t; θ)) =

Therefore,

(cid:17)

θ

exp

(cid:16)
W(cid:62)
i,t
(cid:16)
W(cid:62)
πi(t) + exp
i,t

(cid:17) Wi,t

θ

˜Un(θ) =

=

=

n(cid:88)

i=1

n(cid:88)

i=1

n(cid:88)

i=1











(cid:88)

t∈Ti

(cid:88)

t∈Ti

(cid:88)

t∈Ti

Wi,t −

(cid:88)

t∈Ti∪Di

(cid:17)

θ

(cid:16)
W(cid:62)
exp
i,t
(cid:16)
W(cid:62)
πi(t) + exp
i,t





(cid:17) Wi,t

θ

πi(t)
(cid:16)
W(cid:62)
πi(t) + exp
i,t

(cid:17) Wi,t −
θ

(cid:88)

t∈Di

wi(t; θ)Wi,t −

(cid:88)

t∈Di

(1 − wi(t; θ))Wi,t





(cid:17)Wi,t

θ

(cid:17)
θ

exp

(cid:16)
W(cid:62)
i,t
(cid:16)
W(cid:62)
πi(t) + exp
i,t




n(cid:88)

(cid:88)

= −

[1[t ∈ Di] − wi(t; θ)] Wi,t

i=1

t∈Ti∪Di


1[t ∈ Di] −

(cid:88)

t∈Ti∪Di

n(cid:88)

=

i=1

1 + exp

1
(cid:17)
(cid:16)
−( ˜W(cid:62)
θ + log(πi(t)))
i,t





˜Wi,t.

where ˜Wi,t = −Wi,t. This is exactly the score equation for logistic regression with oﬀ-
(cid:3)
set log πi(t).

Proof of Lemma 3.5. Deﬁne the joint counting process

Mi(t) = Ni(t) −

(cid:90) t

0

(hi(s; θ) + πi(s))Ri(s)ds.

where Ni(t) is the counting process with jumps at all t ∈ Ti ∪ Di. Asymptotic consistency

is guaranteed [Andersen et al., 1993, Theorem VI.1.1] if

(13)

−1 ˜Un(θ
n

0)

P→ 0,

30

and

(14)

and

(15)

lim
n→∞

P

WALTER DEMPSEY

−1 d
n
dθ(cid:62)

˜Un(θ)

(cid:12)(cid:12)(cid:12)θ=θ0

= Ξ(θ),

d2
dθjdθk

˜Un(θ)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

< M for all j, k and all θ ∈ Θ
0

= 1

(cid:33)

(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

−1

n

To prove (13), we decompose ˜Un(θ

0) into three terms

(16)

−1 ˜Un(θ
n

0) = n

−1Un(θ

0) + n

−1 (cid:16)

ˆUn(θ

(cid:17)
0) − Un(θ)

+ n

−1 (cid:16)

˜Un(θ

0) − ˆUn(θ

(cid:17)
0)

where, recall, ˆUn(θ
the approximation C(cid:62)

0) are the logistic score equations with
i,tJφ,ψb. Andersen et al. [1993] (Theorem VI.1.1) show that n−1Un(θ

Xi(t, s)β(s)ds and ˜Un(θ

0) with
0) →

(cid:82) ∞
0

0. The second term

−1 (cid:16)
n

ˆUn(θ

0) − Un(θ

(cid:17)
0)

= 1
n

(cid:90) τ

n(cid:88)

i=1

0

h(1)(s; θ
0)
πi(s) + h(s; θ

0)

dMi(s).

where h(·; θ

0) is given by (2). Lenglart’s inequality implies the second term converges in

probability to zero under Assumptions (E.1) and (E.2). The third term satisﬁes

∞(cid:88)

θ +

ˆci,k(t)

(cid:90) t

t−∆

(cid:62)
ˆψk(s)φ(s)



b + log(πi(t))

− g

(cid:16)
˜W

(cid:62)
i,t

θ + log(πi(t))

(cid:17) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

−1 (cid:13)(cid:13)(cid:13) ˆUn(θ
n
n(cid:88)

≤Mn

−1

(cid:13)(cid:13)(cid:13)
0) − ˜Un(θ
0)


˜W

(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

g

t∈Ti∪Di

(cid:62)
i,t

i=1

≤ Mn

−1

= M
4

−1

n

n(cid:88)

(cid:88)

i=1

t∈Ti∪Di

n(cid:88)

(cid:88)

sup
x∈R

(cid:48)

(cid:13)(cid:13)(cid:13)g
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

i=1

t∈Ti∪Di
(cid:90) t

(cid:90) τ

Kb(cid:88)

0

l=1

t−∆

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

→ M
4

k=Kx+1

(cid:13)(cid:13)(cid:13) ×

(x)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

∞(cid:88)

ˆci,k(t)

k=Kx+1

Kb(cid:88)

(cid:90) t

l=1

t−∆

ˆψk(s)φl(s)bl

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Kb(cid:88)

×

l=1

(cid:90) t

t−∆



X(t, s) −

Kx(cid:88)

k=1



ˆci,k(t)ψk(s)

φl(s)bl

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)



X(t, s) −

Kx(cid:88)

k=1



ˆci,k(t)ψk(s)

φl(s)bl

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

( f0(t) + f1(t))dt

where M = sup (cid:107) ˜Wi,t(cid:107), g(x) = 1/(1 + exp(−x)) is the expit function. The second inequality is
due to the Taylor remainder theorem. The third is due to g(cid:48)(x) = ex/(1 + ex)2 ≤ e0/(1 + e0)2 =

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

31

1/4, reordering the summation, and rewriting the error term in terms of the diﬀerence
between X(t, s) and the approximation with truncation level Kx. Letting n → ∞ allows
us to re-write the outside sums in terms of the integrated diﬀerence where the integral
is with respect to the joint event and sampling distributions ( f0(t) + f1(t)). Finally, under

Assumptions 3.4, Park and Staicu [2015] show

X(t, s) −

Kx(cid:88)

k=1

ˆci,k(t)ψk(s)

P→ 0

as Kx → ∞, which implies the third term goes to zero as the truncation level goes to

inﬁnity. The same argument can be used in conjunction under assumptions (E.1) and (E.4)

to prove (14).

To prove (15), we start with equation (6), which we rewrite here for completeness:

ˆUn(θ) =

n(cid:88)

i=1





(cid:88)

u∈Ti

wi(u; θ)

h(1)(u; θ)
h(u; θ)

−

(cid:88)

u∈Di

wi(u; θ)

i (u; θ)
h(1)
πi(u)





.

Let N(e)

i (t) and N(s)
and Di respectively. Then

i (t) be the event and subsampling counting processes with jumps at t ∈ Ti

(17)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

−1

n

d2
dθjdθk

˜Un(θ)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤ 1
n

(cid:90) τ

n(cid:88)

i=1

0

wi(t; θ)×Hin(t)Ri(t)dN(e)

i (t)+ 1
n

(cid:90) τ

n(cid:88)

i=1

0

wi(t; θ)
πi(t)

×Gin(t)Ri(t)dN(e)

i (t)

where Hin(t) and Gin(t) are from Condition VI.1.1(E) in [Andersen et al., 1993, pp. 421].

The ﬁrst term converges in probability to a ﬁnite quantity by Andersen et al. [1993].
Since wi(t; θ)/πi(t) = (πi(t) + hi(t; θ))−1, the second term which simpliﬁes to

1
n

(cid:90) τ

n(cid:88)

i=1

0

Gin(t)
πi(t) + hi(t; θ)

Ri(t)dN(e)

i (t) ≤ 1
L · n

(cid:90) τ

n(cid:88)

i=1

0

Gin(t)Ri(t)dN(e)

i (t)

by Assumption (E.1) (i.e., the subsampling rate is lower bounded by L). The right hand

side is the optional variation of the local square integrable martingale

1
L · n

(cid:90) t

n(cid:88)

i=1

0

G1/2
in (s)dN(e)

i (s)

32

WALTER DEMPSEY

This martingale has predicable variation process

1
L · n

(cid:90) t

n(cid:88)

i=1

0

Gin(s)πi(s)Ri(s)ds

Since both processes have the same limits, conditions VI.1.1(E) in Andersen et al. [1993]

and Assumption (E.1) imply that the second term on the right hand side of (17) also
converges to a ﬁnite quantity as n → ∞ for every Kx which completes the proof.

P→ θ

This proves ˆθn

0 as n → ∞ and Kx → ∞. Note that the argument holds for any choice
of weight functions wi(t; θ); implying convergence holds more broadly for all approximate
(cid:3)

score equations in (6).

Proof of Asymptotic Normality. Consider a Taylor-expansion of ˜Un(θ) centered at θ
ˆθn ∈ Θ

0. We can write as

0 when

˜Un( ˆθn) = ˜Un(θ

0) − ( ˆθn − θ

0)

∂
∂θ

˜Un(θ) |θ=θ0

−( ˆθn − θ

0)

p(cid:88)

( ˆθln − θl0)

l=1

∂2
∂θ∂θl

˜Un(θ) |θ=θ(cid:63)

0 = ˜Un(θ

0) + ( ˆθn − θ

0)





∂
∂θ

˜Un(θ) |θ=θ0

+1
2

p(cid:88)

( ˆθln − θl0)

l=1

˜Un(θ) |θ=θ(cid:63)





n1/2( ˆθn − θ

0) =





1
n

∂
∂θ

˜Un(θ) |θ=θ0

+ 1
2n

p(cid:88)

( ˆθln − θl0)

l=1

∂2
∂θ∂θl

˜Un(θ) |θ=θ(cid:63)

−1/2 ˜Un(θ
n

0)

∂2
∂θ∂θl

−1



where θ(cid:63) is on the line segment between ˆθn and θ

0. First, the term

∂
∂θ

1
n

˜Un(θ) |θ=θ0

+ 1
2n

p(cid:88)

l=1

( ˆθln − θl0)

∂2
∂θ∂θl

˜Un(θ) |θ=θ(cid:63)

P→ Ξ(θ

0)

under conditions VI.1.1(A)–(E) in Andersen et al. [1993] and Assumptions (E.1)–(E.4).

To see this, note the ﬁrst term converges to Ξ(θ
probability by pM(cid:107)θ(cid:63) − θ
0
Euclidean norm. The second term therefore goes to zero since (cid:107)θ(cid:63) − θ

0) while the second term is bounded in
(cid:107) for some ﬁnite constant M independent of θ(cid:63) where (cid:107) · (cid:107) is the

(cid:107) → 0 as n → ∞.

0

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

33

We now consider n−1/2 ˜Un(θ

0). Recall ˜Un(θ

0) can be decomposed into three terms as

in (16). Here, we assume Kx → ∞ so the ﬁnal term is negligible. This leaves two terms

Un(θ

0) =

(cid:90) τ

n(cid:88)

i=1

0

h(1)(u; θ)
h(u; θ)

dM(e)

i (t)

i (t) and N(s)

where N(e)
with jumps at times t ∈ Ti and t ∈ Di respectively. and

i (t) are the orthogonal event and subsampling counting processes

( ˆUn(θ

0) − Un(θ

0)) =

=

=

(cid:90) τ

n(cid:88)

i=1
n(cid:88)

0
(cid:90) τ

i=1
n(cid:88)

0
(cid:90) τ

i=1

0

h(1)(s; θ
0)
πi(s) + h(s; θ

0)

dMi(s)

h(1)(s; θ
0)
πi(s) + h(s; θ

(1 − w(s; θ

dM(e)

i (s) +

0)
0)) · h(1)(s; θ
0)
hi(s; θ
0)

(cid:90) τ

n(cid:88)

i=1

0

h(1)(s; θ
0)
πi(s) + h(s; θ
(cid:90) τ

n(cid:88)

dM(e)

i (s) +

i=1

0

dM(s)

i (s)

0)

w(s; θ

0) · h(1)(s; θ
0)
πi(s)

dM(s)

i (s)

Taking diﬀerence with Un(θ

0), the ﬁrst term becomes

(cid:90) τ

n(cid:88)

i=1

0

w(s; θ

0) · h(1)(s; θ
0)
hi(s; θ
0)

dM(e)

i (s)

These are two orthogonal square integrable martingales with quadratic variation given

by

and

(cid:90) τ

n(cid:88)

i=1

0

(cid:90) τ

n(cid:88)

i=1

0

(cid:34)

(cid:34)

w2(s; θ

0)

w2(s; θ

0)

h(1)(s; θ
0)
hi(s; θ
0)

(cid:35)

(cid:34)

×

(cid:35)(cid:62)

h(1)(s; θ
0)
hi(s; θ
0)

hi(s)ds

0)

h(1)(s; θ
πi(s)

(cid:35)

(cid:34)

×

(cid:35)(cid:62)

0)

h(1)(s; θ
πi(s)

πi(s)ds

34

WALTER DEMPSEY

respectively. Using the deﬁnition w(s; θ

0) = πi(s)/(πi(s) + hi(s; θ

0)), we have

1
n

(cid:90) τ

n(cid:88)

i=1

0

(cid:34)

w2(s; θ

0)

(cid:35)

(cid:34)

×

(cid:35)

0)

×

h(1)(s; θ
0)
hi(s; θ
0)
(cid:34)

h(1)(s; θ
πi(s)

h(1)(s; θ
0)
hi(s; θ
0)
(cid:34)

w2(s; θ

0)

(cid:35)(cid:62)

hi(s)ds

(cid:35)(cid:62)

0)](cid:62)

(cid:35)

0)

πi(s)ds

h(1)(s; θ
πi(s)
(cid:34)
1 + h(s; θ
0)
πi(s)
(cid:34)πi(s) + h(s; θ
0)
πi(s)

ds

0)](cid:62)

(cid:35)

ds

w2(s; θ

0)

h(1)(s; θ

0)[h(1)(s; θ
hi(s; θ
0)

w2(s; θ

0)

h(1)(s; θ

0)[h(1)(s; θ
hi(s; θ
0)

(cid:90) τ

n(cid:88)

+ 1
n

0

i=1
(cid:90) τ

n(cid:88)

= 1
n

= 1
n

= 1
n

i=1
n(cid:88)

0
(cid:90) τ

i=1
n(cid:88)

0
(cid:90) τ

i=1

0

w(s; θ

0)

h(1)(s; θ

0)[h(1)(s; θ
hi(s; θ
0)

0)](cid:62)

ds → Ξ(θ

0).

Returning to the Taylor-expansion, under conditions VI.1.1(A)–(E) in Andersen et al. [1993]

and Assumptions (E.1)–(E.4), we can apply Rebolledo’s martingale central limit theorem

so that

√

n( ˆθn − θ

0)

D→ N(0, Ξ(θ

−1 × Ξ(θ

0) × Ξ(θ

0)

−1)

0)

which completes the proof.

(cid:3)

Appendix B. Penalized logistic mixed-effects model

Here we derive a penalized adaptive Gaussian quadrature (PADQ) to ﬁt the penalized
logistic mixed-eﬀects model. Here, for clarity, we write Yij ∈ {0, 1} to be the sequence
of binary indicators for participant i = 1, . . . , n and j indexes the event and subsampled

times. We write Wij to denote the time-varying covariate. Then the score equations for the
penalized logistic regression with oﬀset log(πij) is written as

n(cid:88)

ki(cid:88)

(cid:32)

i=1

j=1

yij −

1
1 + exp(−Wijθ − log(πij))

(cid:33)

Wij − 1
σ2

Kb(cid:88)

k=3

βk

k=3 (i.e., Bθ =
, 0)). Let Wi be a ki × p matrix with the jth row equal to Wij. Let µ(Wi; θ, πi) be

Let B be the matrix that extracts the entries in θ corresponding to {βk}Kb
(0, β
3
the vector of length ki with the jth entry equal to µ(Wij; θ, πij) =

1+exp(−Wθ−log(π)). Finally, let

, . . . , βKb

1

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

35

Yi = (yi1
(cid:80)n
i=1 W(cid:62)

, . . . , yiki and Ei(θ) = Yi − µ(Wi; θ, πi). Then we can succinctly write the above as
i Ei(θ) − 1

σ2 Bθ.

We derive the iteratively re-weighted least squares algorithm next. First, take the

derivative with respect to θ to construct the Hessian





−

n(cid:88)

ki(cid:88)

i=1

j=1

WijW(cid:62)

(1 + exp(−Wijθ − log(πij)))2 exp(−Wijθ − log(πij)) + B

σ2

ij





.

Deﬁne the matrix Vi(θ) be the diagonal matrix with the ( j, j)th entry equal to µ(Wij; θ, πij) ·
i Vi(θ)Wi + B
(1−µ(Wij; θ, πij). Then we can express the Hessian more succinctly as −
Combining these, starting at an initial estimate, the Newton-Rhapson update given current
parameter value ˆθk is given by

i=1 W(cid:62)

(cid:104)(cid:80)n

σ2

(cid:105)

.

ˆθk+1

= ˆθk +





n(cid:88)

i=1

(cid:62)
W

i Vi( ˆθk)Wi + B
σ2





−1 

n(cid:88)


i=1

(cid:62)
W

i Ei( ˆθk) − 1

σ2 B ˆθk





To learn σ2, we use the connection to the mixed-eﬀects literature. Fixing θ, the log-
2 log(σ2). Diﬀerentiating
likelihood components related to σ2 is given by − 1
2σ2
and setting equal to zero yields the estimator ˆσ2 = θ(cid:62)Bθ
B1 . The complete algorithm is then
alternating between Netwon-Rhapson for ﬁxed σ2 to estimate θ and then estimating σ2 for

θ(cid:62)Bθ − B1

ﬁxed θ using the above formula.

B.1. Estimation of random-eﬀects via Newton-Rhapson. In standard logistic mixed-
eﬀects model, with bi ∼ N(0, Ψ ) and bi ∈ Rq, then the likelihood is given by

n(cid:89)

(cid:90) ki(cid:89)

i=1

j=1

(cid:32) exp(Wijθ + Zijbi + log πij)
1 + exp(Wijβ + Zijbi + log πij)


(cid:16)
Wijθ + Zijbi + log πij

(cid:17)

yij

exp



ki(cid:88)

(cid:16)

j=1

(cid:33)yij (cid:32)

1
1 + exp(Wijβ + Zijbi + log πij)

(cid:33)1−yij

φ(bi; σ2

b)dbi

− log

(cid:16)
1 + exp

(cid:16)

Wijθ + Zijbi + log πij

(cid:17)(cid:17)(cid:17)

(cid:62)
− b
i

Ψ −1bi/2





dbi

(cid:90)

n(cid:89)

i=1

(cid:90)

n(cid:89)

∝

∝

i=1

(cid:104)

(cid:105)
g(θ, bi, πij, Ψ )

dbi,

exp

36

WALTER DEMPSEY

where the second and third line ignore the constant (2π)q/2|Ψ |−1/2. Diﬀerentiating the
function g(θ, bi, πij, Ψ ) with respect to bi, the connection to the prior section can be used.

Re-deﬁning terms appropriately allows us to express the derivatives as

∂g(θ, bi, πi, Ψ )
∂bi
∂g(θ, bi, πi, Ψ )
∂b2
i

= ZiEi(θ, bi) − Ψ −1bi

= −

(cid:104)

ZiVi(θ, bi)Zi + Ψ −1(cid:105)

For ﬁxed θ, the function g is stictly concave function of bi and therefore we can estimate
the unique maximum ˆbi via Newton-Rhapson method as follows:

ˆb(k+1)

i

= ˆb(k)
i

+

(cid:104)

ZiVi(θ, ˆb(k)

i )Zi + Ψ −1(cid:105)−1 (cid:16)

ZiEi(θ, ˆb(k)

i ) − Ψ −1 ˆb(k)

i

(cid:17)

Fixing ˆbi, we then need to estimate Ψ and θ. Here we employ adaptive gaussian

quadrature (cites) as it has been shown that for infrequent events the Laplace approximation
i Vi(θ, bi)Zi + Ψ −1, zj and wj for
tends to underestimate eﬀects (cites). Let R(cid:62)
j = 1, . . . , NGQ be the weights for the one-dimensional Gaussian quadrature rule. Let
i zk, and Wk = exp((cid:107)zk(cid:107)2)
zk = (zk1

l=1 wkl. Then the AGQ rule is

, . . . , zkq), ˜bik = ˆbi + R−1

i Ri = Z(cid:62)

(cid:81)q

given by

(cid:90)

(cid:90)

=

exp

(cid:2)

(cid:3)
g(θ, bi, πi, Ψ )

dbi

|Ri|−1 exp

(cid:104)

g(θ, ˆbi + R

−1
i z, πi, Ψ ) + (cid:107)z(cid:107)2/2

(cid:105)

exp(−(cid:107)z(cid:107)2/2)dz

=(2π)q/2|Ri|−1

NGQ(cid:88)

· · ·

NGQ(cid:88)

=1

j1

jq=1

(cid:104)

(cid:105)
g(θ, ˜bik, πi, Ψ )

Wk

exp

The score equations


NGQ(cid:88)

NGQ(cid:88)

· · ·

NGQ(cid:88)

=1

j1

jq=1



=1

j1

jq=1

NGQ(cid:88)

· · ·

(cid:104)

(cid:105)
g(θ, ˜bik, πi, Ψ )

Wk

exp


−1



(cid:16)

exp

(cid:104)

(cid:105)
g(θ, ˜bik, πi, Ψ )

Wk

(cid:17) ∂
∂θ g(θ, ˜bik, πi, Ψ )

RECURRENT EVENT ANALYSIS WITH FUNCTIONAL COVARIATES VIA RANDOM SUBSAMPLING

37

Deﬁne ω(θ, ˜bik, πi, Ψ ) to be the weights. Then we can express score and Hessian as:

ω(θ, ˜bik, πi, Ψ ) ·

∂
∂θ g(θ, ˜bik, πi, Ψ )

ω(θ, ˜bik, πi, Ψ ) ·

∂2

∂θ∂θ(cid:62) g(θ, ˜bik, πi, Ψ ) +

(cid:88)

k

ω(θ, ˜bik, πi, Ψ ) ·

(cid:34) ∂
∂θ g(θ, ˜bik, πi, Ψ )

(cid:35) (cid:34) ∂

∂θ g(θ, ˜bik, πi, Ψ )

(cid:35)(cid:62)

ω(θ, ˜bik, πi, Ψ )


∂

∂θ g(θ, ˜bik, πi, Ψ )





(cid:88)

k

ω(θ, ˜bik, πi, Ψ )


∂

∂θ g(θ, ˜bik, πi, Ψ )

(cid:62)

(cid:88)

k

(cid:88)

k

−





(cid:88)

k

When NGQ = 1, the AGQ approximation is equivalent to a Laplacian approximation.

Appendix C. Additional simulations and case study details

C.1. Impact of ∆. Here we investigate the impact of misspeciﬁcation of the window

length for β(t) = β

0 exp

(cid:0)−β

1s

(cid:1)
. To do this, we generate 1000 datasets per condition each

consisting of 500 user-days. For each simulation, we analyze the data using window

lengths ∆ ∈ (26, 29, 32, 35, 37).

When the window length is too large, i.e., ∆ > ∆(cid:63), then asymptotically the estimation
is unbiased as β(t) ≡ 0 for t > ∆; however, we incur a penalty in ﬁnite samples, especially

for settings where the function is far from zero near t = ∆. We ﬁnd the MISE increases
and variance slightly decreases as ∆ increases. While the MISE increased for ∆ < ∆(cid:63), the
pointwise estimation error remains low for t < ∆. This does not hold for ∆ > ∆(cid:63), where

instead we see parameter attenuation, i.e., a bias towards zero in the estimates at each

0 < t < ∆, which is captured by the partial MISE.

Department of Biostatistics, University of Michigan, 1415 Washington Heights, Ann Arbor, MI 48109,

USA

Email address: wdem@umich.edu

38

WALTER DEMPSEY

∆ =

26

29

32

35

37

MISE
Variance
P-MISE
MISE
Variance
P-MISE
MISE
Variance
P-MISE
MISE
Variance
P-MISE
MISE
Variance
P-MISE

Sampling rate
2
1
0.5
6.6 × 10−2
6.1 × 10−2
5.9 × 10−2
12.4 × 10−2
1.6 × 10−2
1.3 × 10−2
7.8 × 10−2
7.3 × 10−2
7.1 × 10−2
7.1 × 10−2
7.0 × 10−2
6.9 × 10−2
2.0 × 10−2
1.5 × 10−2
1.1 × 10−2
7.7 × 10−2
7.5 × 10−2
7.5 × 10−2
8.1 × 10−2
8.0 × 10−2
8.1 × 10−2
1.9 × 10−2
1.4 × 10−2
1.1 × 10−2
8.1 × 10−2
8.0 × 10−2
8.1 × 10−2
8.8 × 10−2
8.8 × 10−2
9.0 × 10−2
2.2 × 10−2
1.5 × 10−2
1.1 × 10−2
8.1 × 10−2
8.5 × 10−2
8.2 × 10−2
9.9 × 10−2
10.3 × 10−2 10.1 × 10−2
2.0 × 10−2
1.4 × 10−2
1.1 × 10−2
9.0 × 10−2
9.2 × 10−2
9.4 × 10−2

4
7.4 × 10−2
3.6 × 10−2
8.8 × 10−2
7.8 × 10−2
3.4 × 10−2
8.4 × 10−2
8.7 × 10−2
3.3 × 10−2
8.7 × 10−2
9.2 × 10−2
3.0 × 10−2
8.4 × 10−2
10.1 × 10−2
3.0 × 10−2
9.0 × 10−2

Table 5. Mean-integrated squared error (MISE), variance, and partial MISE
as a function of ∆(cid:63) and sampling rate when true ∆ = 32 minutes.

