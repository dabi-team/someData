2
2
0
2

l
u
J

1
2

]
L
C
.
s
c
[

1
v
7
9
3
0
1
.
7
0
2
2
:
v
i
X
r
a

CODET: CODE GENERATION WITH GENERATED TESTS

Bei Chen∗, Fengji Zhang∗, Anh Nguyen∗, Daoguang Zan, Zeqi Lin,
Jian-Guang Lou, Weizhu Chen
Microsoft Corporation
{beichen, v-fengjzhang, anhnguyen, v-dazan,
zeqi.lin, jlou, wzchen}@microsoft.com

ABSTRACT

Given a programming problem, pre-trained language models such as Codex have
demonstrated the ability to generate multiple different code solutions via sam-
pling. However, selecting a correct or best solution from those samples still re-
mains a challenge. While an easy way to verify the correctness of a code so-
lution is through executing test cases, producing high-quality test cases is pro-
hibitively expensive. In this paper, we explore the use of pre-trained language
models to automatically generate test cases, calling our method CODET: CODE
generation with generated Tests. CODET executes the code solutions using the
generated test cases, and then chooses the best solution based on a dual execu-
tion agreement with both the generated test cases and other generated solutions.
We evaluate CODET on ﬁve different pre-trained models with both HumanEval
and MBPP benchmarks. Extensive experimental results demonstrate CODET can
achieve signiﬁcant, consistent, and surprising improvements over previous meth-
ods. For example, CODET improves the pass@1 on HumanEval to 65.8%, an
increase of absolute 18.8% on the code-davinci-002 model, and an absolute 20+%
improvement over previous state-of-the-art results.

1

INTRODUCTION

Thanks to the recent advances in pre-training techniques, many large language models have been
pre-trained for code generation, e.g., Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), IN-
CODER (Fried et al., 2022a), CODEGEN (Nijkamp et al., 2022) and PolyCoder Xu et al. (2022),
as well as bringing code generation into real-world applications such as Copilot1. While these ad-
vanced pre-trained models are able to generate many different solutions for a programming problem
via sampling, it remains a challenge to select a single correct solution from multiple generated candi-
dates. Taking the HumanEval benchmark (Chen et al., 2021) as an example, Codex has a pass@100
(pass if one or more among 100 generated solutions for a given problem can pass the corresponding
test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5%2. This huge gap
makes it imperative to explore how to pick the correct or best solution from multiple candidates.

A simple way to verify if a solution is correct is to execute it and then check if it passes all cor-
responding test cases. Such an execution-guided approach has been extensively applied to many
code-related tasks, such as code generation (Chen et al., 2021; Li et al., 2022b; Shi et al., 2022),
code translation (Roziere et al., 2021), and program synthesis (Chen et al., 2018; Ellis et al., 2019).
The challenge, however, is that preparing a sufﬁcient number of high-quality test cases to cover all
the corner cases is prohibitively expensive and inefﬁcient. In real-world applications like Copilot, it
is troublesome if users are required to provide test cases when they are using a code generation tool.
To address these challenges, we explore approaches to automatically produce test cases for arbitrary
programming problems and then use them to quickly verify any solution.

Although pre-trained models such as Codex have been used to generate code solutions, we start by
designing an elaborate instruction as prompt, asking the same language model to automatically gen-
erate large amounts of test cases for each programming problem, as illustrated in Figure 1. Second,

∗The ﬁrst three authors contributed equally.
1https://github.com/features/copilot
2Results on the HumanEval benchmark with code-cushman-001. More results can be found in Section 4.1.

1

 
 
 
 
 
 
Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the
pre-trained language model. The best code solution is then selected by a dual execution agreement.

we execute each of the generated solutions on the generated test cases to associate each solution
with all the test cases it can pass. Third, we apply a dual execution agreement on both the solutions
and the test cases. We believe a solution could get support from both the test cases and the other
solutions. The more test cases a solution can pass, the better the solution is. Meanwhile, if there is
another test-driven sibling solution that could pass the exact same test cases as the current solution, it
is likely that the two solutions have the same functionality, although with different implementations.
We regard the sibling solutions as supporting each other where a larger number of sibling solutions
can directly contribute to the correctness of a solution. Finally, we calculate a ranking score based
on this dual execution agreement and produce the best solution. We call our method CODET: CODE
generation with generated Test-driven dual execution agreement.

Although CODET is simple and efﬁcient, without any need of either labelled data or additional
rankers, its performance is surprisingly exceptional. We evaluate CODET on ﬁve different pre-
trained models: three OpenAI Codex models (Chen et al., 2021), INCODER (Fried et al., 2022b), and
CODEGEN (Nijkamp et al., 2022), as well as two established benchmarks: HumanEval (Chen et al.,
2021) and MBPP (Austin et al., 2021). Extensive experimental results show CODET can effectively
select the correct solution, boosting the pass@1 score signiﬁcantly: HumanEval (33.5% → 44.5%
with code-cushman-001 and 47.0% → 65.8% with code-davinci-002), and MBPP (45.9% → 55.4%
with code-cushman-001 and 58.1% → 67.7% with code-davinci-002). Furthermore, combining
code-davinci-002 and CODET outperforms previous state-of-the-art methods by a large margin, e.g.,
HumanEval: 42.7% (Inala et al., 2022) → 65.8%. Our work will be publicly available at https:
//github.com/microsoft/CodeT.

2 METHODOLOGY

The task of code generation is to solve a programming problem: generate code solution x based
on context c. As shown in Figure 2, context c contains natural language problem descriptions in
the form of code comments, and a code snippet that includes statements such as imports and the
function header. A code solution is a code snippet that solves the programming problem described
in the context. Generally, we sample a set of code solutions, denoted as x = {x1, x2, · · ·, xK}, based

Figure 2: Code generation and test case generation: an example from the HumanEval benchmark.
Example input-output cases are removed from the context.

2

Test Case GenerationPre-trained Language ModelDual ExecutionAgreementCode Solution 1Code Solution 2……Test Case 1Test Case 2……CodeGeneration+Instruction A Programming ProblemThe Best Code SolutionfromtypingimportListdefhas_close_elements(numbers: List[float], threshold: float) -> bool:""" Check if in given list of numbers, are any two numbers closer to each other than given threshold."""pass# check the correctness of has_close_elementsassertContext 𝑐Instruction 𝑝foridx, eleminenumerate(numbers):foridx2, elem2inenumerate(numbers):ifidx!= idx2:distance= abs(elem-elem2)ifdistance< threshold:returnTruereturnFalseCode GenerationCode Solution 𝑥1Code Solution 𝑥2Code Solution 𝑥𝐾…assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == TrueTest Case GenerationTest Case 𝑦1Test Case 𝑦2Test Case 𝑦𝑀…Figure 3: A simple example of the programming problem “return the square of a number”.

on the context c using a pre-trained language model M, which can be formulated as x = M(c).
Our goal is to select the best code solution ˆx from the set of generated code solutions x, where
ˆx is the most likely solution to correctly solve the given programming problem. To this end, we
propose CODET in the hope of unleashing the inherent power of the pre-trained language model M.
Speciﬁcally, we use M to generate test cases for the programming problems (Section 2.1), and then
select the best code solution ˆx based on a dual execution agreement (Section 2.2).

2.1 TEST CASE GENERATION

We leverage the pre-trained language model M to generate both code solutions and test cases. When
generating test cases, to tell the model that we want to generate test cases rather than code solutions,
we add an instruction p as a prompt following the context c. As shown in Figure 2, we construct
instruction p using the following parts: a “pass” statement as a placeholder of the function body,
a comment “check the correctness of [entry point]” to clarify the intention of generating test cases,
and an “assert” to kick off the test case generation. The process of test case generation can be
formulated as y = M(concat(c, p)), where y = {y1, y2, · · ·, yM } denotes a set of test cases and
concat is the concatenation operation. It is worth noting that we remove all example input-output
cases from the context c to avoid exposing real test cases.

2.2 DUAL EXECUTION AGREEMENT

In this subsection, we try to answer the question: given the code solutions x and the test cases y,
how do we select the solution ˆx that is most likely correct?

First, we execute each of the generated code solutions on the generated test cases. Then, the most
straightforward way is to score each code solution by the number of test cases it can pass. However,
we found this naive method is not good enough3. A simple example is shown in Figure 3. There
are three code solutions and ﬁve test cases for the programming problem “return the square of a
number”. The highest scoring code solution is x3, which passes four test cases. x3, however, is ob-
viously not the correct solution, since it returns the double of a number, not its square. As observed,
although x1 and x2 are two different solutions, they are both correct with the same functionality of
returning the square of a number. Hence, it is reasonable for them to group together. By adding up
the scores of x1 and x2, they will be selected based on a combined score of 6.

Based on this idea, we propose our approach CODET to perform what we call dual execution agree-
ment. Formally, for each code solution x ∈ x, we execute it with all test cases in y. If two code
solutions can pass a same set of test cases, then they are sibling solutions with the same functional-
ity; thus, we can put them into the same cluster. In this way, all code solutions in x can be divided
into several clusters, denoted as x = {x1, x2, · · ·, xN }. Let W = {wij} be an N ×M matrix to rep-
resent the execution results, where N is the number of code solution clusters and M is the number of
test cases. If code solutions in cluster xi can pass the test case yj, then wij = 1; otherwise, wij = 0.
The basic idea of the dual execution agreement is that a good code solution should be agreed upon
by both the test cases and the other solutions: (1) The more test cases it can pass, the better the
solution is. (2) A larger number of sibling solutions can directly contribute to the correctness of the

3Results can be found in Section 4.1.

3

ELT layoutCode Solutionsreturna**2returna*areturna*2Test Casesassertnum_square(1) == 1assertnum_square(2) == 4assertnum_square(1) == 2assertnum_square(3) == 6𝑠𝑐𝑜𝑟𝑒=3𝑠𝑐𝑜𝑟𝑒=3𝑠𝑐𝑜𝑟𝑒=4𝑠𝑐𝑜𝑟𝑒=6assertnum_square(0) == 0𝑥1𝑥2𝑥3𝑦1𝑦2𝑦3𝑦4𝑦5solution. Hence, we deﬁne the score of each cluster xi to be:

f (xi) = ri

M
(cid:88)

j=1

wijg0(yj),

(1)

where g0(yj) = 1
M denotes the initial normalized score of test case yj and ri is the square root of the
code solution number in the cluster xi. We use the square root to reduce the impact caused by code
solutions due to the intuition that the number of code solutions is less important than the number
of test cases. For example, there may be one code solution that can pass ﬁve test cases, whereas
another ﬁve code solutions may pass only one test case. We intuitively consider that the former may
be more likely correct. Finally, we get the best code solution ˆx by selecting any code solution from
the highest scoring cluster. In addition, if we want to obtain k code solutions, we can select one code
solution from each of the top k highest scoring clusters.

In CODET, each test case contributes equally to the score of the code solution. So the question
arises: do different test cases have different levels of importance? Intuitively, if a test case can be
passed by more code solutions, the more likely the test case is to be correct. To this end, we further
propose CODET-Iter to consider the importance of test cases in an iterative manner. Inspired by the
work for bipartite graph ranking (Kleinberg, 1999; He et al., 2016; Yang et al., 2020), the scores of
xi and yj can be calculated as:

f (xi) =αri

M
(cid:88)

j=1

wijg(yj) + (1 − α)f0(xi),

g(yj) =β

N
(cid:88)

i=1

wijf (xi) + (1 − β)g0(yj),

(2)

where f0(xi) = ri/ (cid:80)N
n=1 rn is the initial normalized score of xi, and α and β are hyper-parameters
to be set between [0, 1] to account for the importance of the initial scores. The scores of xi and yj
are calculated iteratively using Equation 2. For convergence, the scores of all code solution clusters
are normalized after each iteration; the same applies to the test cases as well.

3 EXPERIMENTAL SETUP

In this section, we introduce the experimental setup, including the pre-trained language models, the
benchmarks for code generation, the evaluation metrics, and the implementation details.

3.1 MODELS

Our main experiments are based on Codex (Chen et al., 2021), which is a descendant of GPT-3
(Brown et al., 2020). Codex is proﬁcient in understanding the provided context and generating
functional programs, and it has been successfully applied to many programming tasks (Drori et al.,
2021; Pearce et al., 2021; Sarsa et al., 2022). We use three Codex models with different sizes
provided by OpenAI: code-cushman-001, code-davinci-001, and code-davinci-002.

Furthermore, we include two publicly available pre-trained models: INCODER (Fried et al., 2022a)
and CODEGEN (Nijkamp et al., 2022). INCODER is a uniﬁed generative model that can perform left-
to-right code generation and code inﬁlling (editing) via the causal mask language modelling training
objective (Aghajanyan et al., 2022). CODEGEN is a family of large-scale language models trained
on natural language and programming data to perform conversational program synthesis. We take
use of the INCODER 6.7B version (INCODER-6B) and the CODEGEN 16B Python mono-lingual
version (CODEGEN-MONO-16B).

3.2 BENCHMARKS

We conduct experiments on two public code generation benchmarks using zero-shot settings.

HumanEval is a code generation benchmark consisting of 164 hand-written Python programming
problems, covering subjects of language comprehension, reasoning, algorithms, and simple math-
ematics (Chen et al., 2021). As shown in Figure 2, the context of each problem may include the

4

natural language description in the form of a comment, a function header, and statements like im-
ports. Each problem includes a canonical solution and several ground truth test cases. To be clear,
the original context of each problem may include example input-output cases, which are removed in
our experiments to avoid exposing real test cases.

MBPP (sanitized version) contains 427 crowd-sourced Python programming problems, ranging
from the basic usage of standard library functions to problems that require nontrivial external knowl-
edge (Austin et al., 2021). Originally, each problem includes the natural language problem descrip-
tion, a function header, a canonical solution, and several ground truth test cases. We follow Hu-
manEval to construct the context for MBPP, which contains a well-formed function header and its
natural language description in the form of a multi-line comment.

3.3 METRICS

We use the metric pass@k for performance evaluation and take advantage of ground truth test cases
to determine the functional correctness of code solutions. For each problem, k code solutions are
If any of the k code solutions pass all ground truth test cases, the
produced as the ﬁnal result.
problem is considered solved. Then pass@k is the percentage of solved problems. Following Chen
et al. (2021), pass@k can be formulated as:

(cid:34)

pass@k :=

E
Problems

1 −

(cid:35)

,

(cid:1)
(cid:0)n−c
k
(cid:1)
(cid:0)n
k

(3)

where n ≥ k is the sampling number and c ≤ n is the number of correct code solutions.

3.4

IMPLEMENTATION DETAILS

For experiments with Codex models, we set the top p to 0.95 and set the max generation length to
300. To get the results of baseline pass@1, the temperature is set to 0 and the sampling number n is
set to 1. For other results, the temperature is set to 0.8 and the sampling number n is set to 100. That
is, for baseline pass@10 and pass@100, we use sampling number n = 100. For CODET, we select
the top k code solutions as mentioned in Section 2.2 and use n = k. The timeout of executing a test
case is set to 0.1 seconds. The hyper-parameters α and β in CODET-Iter (Equation 2) are both set
to 0.9, and the iteration number is set to 3. For code solution post-processing, we follow Chen et al.
(2021) to truncate the generated content by ﬁve stop sequences: “\nclass”, “\ndef”, “\n#”, “\nif”,
and “\nprint”. For test case post-processing, we extract the ﬁrst ﬁve assertions that conform to the
Python syntax for each generated sample. A valid assertion should start with “assert” and contain
the name of the corresponding entry point function.

For experiments with INCODER and CODEGEN models, we use the HuggingFace transformers li-
brary (Wolf et al., 2019). The setup and post-processing procedure are the same as in the Codex
experiments, except that the baseline pass@1 results are obtained by picking the sample with the
highest mean log-probability from n = 100 samples with a small temperature close to 0. To speed
up our experiments, we run both models with half precision.

4 EXPERIMENTAL RESULTS

In this section, we evaluate CODET on ﬁve different pre-trained models and two benchmarks to
verify its effectiveness, followed by a deep analysis on the quality of generated test case and different
design choices in CODET.

4.1 MAIN RESULTS

Table 1 summarizes the experimental results of various Codex models on both the HumanEval and
the MBPP benchmarks. Using HumanEval in the Baseline column as an example, we ﬁnd our
reproduced results of the 12B code-cushman-001 model are slightly better than that of the Codex-
12B model reported in the original Codex paper (Chen et al., 2021) (33.5% vs. 28.6% in pass@1
and 77.4% vs. 72.3% in pass@100). We conjecture that code-cushman-001 is a better 12B pre-
trained model than the original Codex-12B model. On the other hand, if we compare the pass@100

5

Methods

Baseline

CODET

CODET-Iter

k

1

10

100

1

2

10

1

2

10

code-cushman-001
code-davinci-001
code-davinci-002

code-cushman-001
code-davinci-001
code-davinci-002

33.5
39.0
47.0

45.9
51.8
58.1

54.3
60.6
74.9

66.9
72.8
76.7

77.4
84.1
92.1

79.9
84.1
84.5

HumanEval
44.5 11.0
50.2 11.2
65.8 18.8

50.1
58.9
75.1

MBPP

65.7 11.4
75.8 15.2
86.6 11.7

45.2 11.7
48.5 9.5
65.2 18.2

55.4 9.5
61.9 10.1
67.7 9.6

61.7
69.1
74.6

72.7 5.8
79.3 6.5
81.5 4.8

54.9 9.0
62.1 10.3
67.9 9.8

50.9
57.9
75.2

61.1
69.4
73.7

66.0 11.7
76.4 15.8
86.8 11.9

72.7 5.8
79.6 6.8
80.5 3.8

Table 1: Pass@k (%) on the HumanEval and MBPP benchmarks with Codex. The numbers in red
indicate the absolute improvements of our methods over baseline on pass@1 and pass@10. For
baseline pass@1, the temperature is set to 0 and sampling number is set to 1; for others, temperature
is set to 0.8 and sampling number is set to 100. We do not show CODET pass@100, since it is the
same as the baseline pass@100.

Models

pass@1

pass@2

pass@10

code-cushman-001
code-davinci-001
code-davinci-002

29.9 −3.6
35.0 −4.0
58.4 11.4

36.6
46.0
65.1

59.5 5.2
70.2 9.6
86.1 11.2

Table 2: Pass@k (%) on the HumanEval with simply counting the number of passed test cases. The
numbers in red indicate the absolute improvements over baseline on pass@1 and pass@10.

to pass@1, it is clear that the former is signiﬁcantly better than the latter, indicating the potential to
select the best code solution from the 100 generated samples.

When we compare the CODET column with the Baseline column, CODET achieves an absolute
improvement of about 10% over the baseline. For HumanEval, the improvements are consistently
above 10%. Surprisingly, even for the strongest baseline, code-davinci-002, the improvement is
18.8%, boosting the pass@1 to 65.8% – a 20+% absolute improvement over the best previously
reported results (Inala et al., 2022). We attribute this larger improvement to the higher quality of test
cases generated by code-davinci-002, providing a deeper analysis in the following section.

We also report pass@2 and pass@10 of CODET to further show its superiority. The pass@2 results
of CODET are close to the baseline pass@10 results. Meanwhile, the improvement on pass@10
is also consistently over 10% in the HumanEval benchmark. Since CODET is performed on the
100 generated code solutions, its pass@100 performance is the same as that of the baseline. For
the MBPP benchmark, we continue to see consistent and signiﬁcant improvements, although the
magnitude of the improvements is slightly less than that of HumanEval. Using the code-davinci-002
as an example, the pass@1 improves by 9.6%.

In addition, we compare the performance between CODET-Iter and CODET. The results show that
they are comparable without any signiﬁcant difference. We conjecture it might be unnecessary to
consider the importance of test cases in this way, or an obviously good test case with a high score
can pass many different solutions without introducing differentiation to rank the code solutions. We
will leave the further study of a more complicated iterative approach for future work.

As mentioned in section 2.2, a straightforward way to score a code solution is to simply count the
number of test cases it can pass. Nevertheless, this method highly depends on the overall quality of
generated test cases and completely elides the agreement between code solutions. We evaluate this
method on the HumanEval benchmark using Codex and provide the results in Table 2. It clearly
shows that its performance is signiﬁcantly and consistently worse than CODET, with only code-
davinci-002 gaining improvement on pass@1 over the baseline. This observation again demonstrates
the importance and the rationality of CODET.

6

Methods

k

Baseline

CODET

CODET-Iter

1

10

100

1

2

10

1

2

10

HumanEval

INCODER-6B†
CODEGEN-MONO-16B†
INCODER-6B
CODEGEN-MONO-16B

15.2
29.3
16.4
29.7

27.8
49.9
28.3
50.3

47.0 −
75.0 −
47.5
73.7

20.6 4.2
36.7 7.0

INCODER-6B†
INCODER-6B
CODEGEN-MONO-16B

19.4 −
21.3
42.4

46.5
65.8

−
66.2
79.1

MBPP
−
34.4 13.1
49.5 7.1

−
−
27.6
44.7

−
43.9
56.6

−
−
37.1 8.8
59.3 9.0

−
−
20.9 4.5
37.5 7.8

−
58.2 11.7
68.5 2.7

−
34.0 12.7
50.5 8.1

−
−
27.6
44.9

−
43.6
56.8

−
−
37.1 8.8
59.5 9.2

−
57.0 10.5
67.5 1.7

Table 3: Pass@k (%) on the HumanEval and MBPP benchmarks with INCODER and CODEGEN.
The numbers in red indicate the absolute improvements of our methods over baseline on pass@1
and pass@10. We also list the baseline results from Fried et al. (2022a) and Nijkamp et al. (2022)
for reference (denoted by †), where the settings of context are not exactly the same as ours.

Models

Average Median

code-cushman-001
code-davinci-001
code-davinci-002
INCODER
CODEGEN

410.7
381.9
391.1
390.1
55.6

429.0
388.0
402.0
400.0
42.0

Table 4: The average and median numbers of syntactically correct test cases for each problem gen-
erated by various models on the HumanEval benchmark.

4.2 RESULTS OF INCODER AND CODEGEN

To further verify the effectiveness of CODET, we include the experimental results of INCODER-6B
and CODEGEN-MONO-16B, as shown in Table 3. It is obvious CODET can signiﬁcantly improve
the pass@1, with absolute improvements in the range of 4.2% to 13.1%. INCODER-6B achieves
the most improvement with a gain of 13.1% on the MBPP benchmark. Similar to the experimental
results of Codex, the pass@2 results are close to the baseline pass@10, with exceptionally close
CODET and CODET-Iter results. All the results demonstrate that CODET can boost the performance
of various pre-trained language models consistently.

4.3 ANALYSIS ON TEST CASES

The test cases are vital since the core idea of CODET is based on test-driven execution. Hence, in this
subsection, we would like to analyze test cases by answering the following two research questions.

Q1. What is the quality of the generated test cases?

For test case generation, we generate 100 samples for each problem and extract the ﬁrst ﬁve syntacti-
cally correct test cases for each sample, meaning each problem is equipped with up to 500 generated
test cases. Table 4 summarizes the average and median numbers of the test cases for each problem
on HumanEval. Almost all models could generate considerable number of syntactically correct test
cases, while CODEGEN generates plenty of unexpected noise.

We leverage the canonical solutions provided by the HumanEval benchmark to evaluate the correct-
ness of generated test cases. Each test case is an assert statement, and we only consider it correct if
its assert condition evaluates as true to the canonical solution. Figure 4a summarizes the distribution
of test case accuracy. The horizontal axis represents the test case accuracy value for each problem.
The vertical axis represents the probability density of problems with the corresponding accuracy
value. We can see that the test cases generated by code-davinci-002 are of high accuracy, while
those generated by INCODER are relatively inaccurate.

7

(a)

(b)

Figure 4: The distributions of (a) test case accuracy and (b) toxicity rate for each programming
problem on the HumanEval benchmark. Test cases of a model are of better quality if they have
higher accuracy and lower toxicity rate.

Methods

k

CODET

CODET-Iter

1

2

10

1

2

10

HumanEval

47.1 2.6
code-cushman-001
52.0 1.8
code-davinci-001
26.8 6.2
INCODER-6B
CODEGEN-MONO-16B 47.7 11.0

58.6 8.5
62.9 4.0
30.4 2.8
54.9 10.2

71.2 5.5
78.1 2.3
40.8 3.7
71.0 11.7

45.9 0.7
52.6 4.1
26.2 5.3
47.1 9.6

56.1 5.2
61.7 3.8
29.8 2.2
54.3 9.4

69.7 3.7
77.5 1.1
40.2 3.1
70.7 11.2

MBPP

59.7 4.3
code-cushman-001
64.3 2.4
code-davinci-001
50.3 15.9
INCODER-6B
CODEGEN-MONO-16B 60.0 10.5

64.8 3.1
71.7 2.6
55.4 11.5
67.6 11.0

75.5 2.8
80.5 1.2
64.5 6.3
76.5 8.0

59.7 4.8
64.3 2.2
48.0 14
58.6 8.1

64.4 3.3
71.0 1.6
52.8 9.2
65.6 8.8

74.8 2.1
79.9 0.3
63.3 6.3
75.5 8.0

Table 5: Pass@k (%) on the HumanEval and MBPP benchmarks with code-cushman-001, code-
davinci-001, INCODER, and CODEGEN using the test cases generated by code-davinci-002. The
numbers in orange indicate the absolute improvements of pass@k using code-davinci-002 test cases
over that using their own generated test cases.

Besides accuracy, we also introduce the toxicity rate to evaluate the quality of test cases. We consider
a test case to be “toxic” if there exists a generated code solution that passes it, but the canonical
solution does not. Toxic test cases may hinder the execution agreement of CODET and lead to
performance degradation. Figure 4b summarizes the distribution of test case toxicity rate, from
which we can see that the toxicity rate highly correlates to the test case accuracy with respect to
different models. The proportion of toxic test cases for code-davinci-002 is much smaller than that
for INCODER, which explains the minor performance improvement of INCODER on the HumanEval
benchmark by CODET when compared to code-davinci-002.

Q2. Can better test cases further boost the performance of mediocre models?

As analyzed above, code-davinci-002 is the most capable model for generating high-quality test
cases. Hence, we conduct an experiment to boost the code generation of the other four models
(code-cushman-001, code-davinci-001, INCODER, and CODEGEN) using test cases generated by
code-davinci-002. Table 5 summarizes the performance gain shown by different models on the Hu-
manEval and MBPP benchmarks. In general, compared to the results of using their own generated
test cases, the results of using test cases generated by code-davinci-002 show signiﬁcant improve-
ments. For code-cushman-001 and code-davinci-001, the absolute improvements are in the range of
1.8% to 4.3% on CODET pass@1, while for INCODER and CODEGEN, the range is from 6.2% to
15.9%. This indicates that potentially correct code solutions generated by mediocre models can be
further exploited by adopting better test cases.

8

0.00.10.20.30.40.50.60.70.80.91.0Test Case Accuracy0.00.10.20.30.4Problem Probability Densitycushman-001davinci-001davinci-002INCODERCODEGEN0.00.10.20.30.40.50.60.70.80.9Test Case Toxicity Rate0.00.20.40.6Problem Probability Densitycushman-001davinci-001davinci-002INCODERCODEGENFigure 5: The baseline pass@100 and CODET
pass@1 with code-cushman-001 at different tem-
perature settings.

Figure 6: The CODET results of three Codex
models with and without constraint on the num-
ber of code solutions.

4.4 ANALYSIS ON DESIGN CHOICE

Temperature The temperature hyper-parameter has great impact on the quality of generated code
solutions and test cases when using top p sampling. We use a high temperature of 0.8 in our main
experiments since CODET could beneﬁt from a larger number of diverse samples. To investigate
the sensitivity of CODET on the temperature, we perform an ablation study by using a range of
temperatures to report the results of baseline pass@100 and CODET pass@1. Figure 5 shows the
results of code-cushman-001 on the HumanEval benchmark at different temperature settings. We
can ﬁnd that a higher temperature does improve the baseline pass@100 and CODET pass@1, and
CODET achieves a good performance when temperature is set to 0.8.

Importance of code solutions As mentioned in Section 2.2, we deﬁne ri to be the square root
of the code solution number in the cluster xi, because we believe the number of passed test cases
is more valuable than the size of code solution clusters. For validation, we perform an ablation
study by comparing the performance of CODET with the “sqrt”, “log” functions, and without any
constraint (“linear”) on the number of code solutions. Figure 6 shows the results of three Codex
models on HumanEval. We can conclude that reducing the importance of code solutions improves
the performance of CODET, indicating our design of ri is reasonable.

5 RELATED WORK

Code Generation with Large Models Recently, a number of large pre-trained language mod-
els have been proposed for code generation. Beneﬁting from billions of trainable parameters and
massive publicly available source code, models could achieve surprisingly good performance. For
instance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competi-
tors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot
to provide real-time coding suggestions. In addition to the private-access AlphaCode and Codex
models, there are also open-source code generation models like GPT-Neo (Black et al., 2021), GPT-
J (Wang & Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022), PolyCoder (Xu et al., 2022),
CODEGEN (Nijkamp et al., 2022), and INCODER (Fried et al., 2022a). In our study, we take advan-
tage of the Codex inference API provided by OpenAI as well as the two competitive open-source
models CODEGEN and INCODER to perform zero-shot code generation.

Automatic Test Case Generation Automated test case generation for programming problems
can reduce the effort of writing test cases manually by developers. Early works including Ran-
doop (Pacheco et al., 2007), EvoSuite (Fraser & Arcuri, 2011), MOSA (Panichella et al., 2015),
DynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically
generate test cases for statically typed programming languages like Java. The later proposed Pyn-
guin (Lukasczyk & Fraser, 2022) could handle dynamically typed language like Python. Never-
theless, they are all search-based heuristics methods, which have limitations to the diversity and
quantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano
et al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019)

9

temperature35%45%55%65%75%85%0.10.20.30.40.50.60.70.80.91.0Baseline pass@100 CODET pass@1Codex modelspass@130%40%50%60%70%cushman-001davinci-001davinci-002linearw/ sqrtw/ logand T5 (Raffel et al., 2020) ﬁne-tuned on labelled data for test case generation. Unlike previous
works that require heuristics rules or model training, we directly sample test cases from powerful
code generation models like Codex in a zero-shot manner with elaborate prompts.

Code Selection from Multiple Samples Despite large models having achieved great performance
in code generation, the models need to sample many times to ﬁnd the correct answer, which brings
the challenge to select the correct ones from multiple samples. Recently, several approaches were
proposed to tackle this issue. In the domain of solving math word problems, Cobbe et al. (2021) gen-
erated many candidate solutions and chose the one with highest rank by a trained veriﬁer. Shen et al.
(2021) proposed to jointly train the generator and ranker through a multi-task framework. In the do-
main of general purpose code generation, Inala et al. (2022) trained a fault-aware ranker to increase
the pass@1 accuracy of code generation models. Besides training an additional veriﬁer/ranker, Shi
et al. (2022) and Li et al. (2022b) proposed to leverage the execution information by ranking the
similarity of outputs based on the given inputs. As for the input data, Shi et al. (2022) employed the
test cases provided by the benchmark, while Li et al. (2022b) trained an additional generator. The
idea of ranking based on agreement also appears in the domain of reasoning (Wang et al., 2022; Li
et al., 2022a). Unlike previous works that require either model training or pre-existing test cases to
rank the generated code solutions, we let the large models generate test cases for themselves and
then rank the solutions based on their test-driven dual execution agreement.

6 CONCLUSION AND FUTURE WORK

In this paper, we propose a simple yet effective approach, called CODET, leveraging pre-trained
language models to generate both the code solutions and the test cases. CODET executes the code
solutions using the test cases and chooses the best solution based on the dual execution agreement.
We demonstrate dual agreement with both the test cases and other solutions is critical to the success
of CODET and perform thorough analysis on the quality of generated test cases and their impact to
CODET. Meanwhile, experimental results clearly demonstrate the superiority of CODET, improving
the pass@1 numbers signiﬁcantly on both the HumanEval and the MBPP benchmarks. Furthermore,
the combination of code-davinci-002 and CODET surprisingly improves the pass@1 on HumanEval
to 65.8%, an absolute 20+% improvement over the previous sate-of-the-art results. In future work,
we will explore how to combine the ideas of CODET with training a ranker. Another direction is to
extend CODET to other code-related tasks.

ACKNOWLEDGEMENT

We would like to thank Davis Mueller and Jade Huang for proofreading the paper and providing
insightful comments.

REFERENCES

Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,
Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. Cm3: A
causal masked multimodal model of the internet. arXiv preprint, 2022.

Andrea Arcuri. Many independent objective (mio) algorithm for test suite generation. In Interna-

tional symposium on search based software engineering, pp. 3–17. Springer, 2017.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au-
toregressive Language Modeling with Mesh-Tensorﬂow. Zenodo, March 2021. doi: 10.5281/
zenodo.5297715. URL https://doi.org/10.5281/zenodo.5297715.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

10

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In Interna-

tional Conference on Learning Representations, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training veriﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.

Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu,
Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates
university math problems by program synthesis and few-shot learning at human level. arXiv
preprint arXiv:2112.15594, 2021.

Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama.
Write, execute, assess: Program synthesis with a repl. Advances in Neural Information Processing
Systems, 32, 2019.

Gordon Fraser and Andrea Arcuri. EvoSuite: automatic test suite generation for object-oriented soft-
ware. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference
on Foundations of software engineering, pp. 416–419, 2011.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint, 2022a.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint arXiv:2204.05999, 2022b.

Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian Wang. Birank: Towards ranking on bipartite

graphs. IEEE Transactions on Knowledge and Data Engineering, 29(1):57–71, 2016.

Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnaci´on, Shuvendu K
Lahiri, Madanlal Musuvathi, and Jianfeng Gao. Fault-aware neural code rankers. arXiv preprint
arXiv:2206.03865, 2022.

Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM),

46(5):604–632, 1999.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-
arXiv preprint
training for natural language generation,
arXiv:1910.13461, 2019.

translation, and comprehension.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the
advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022a.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R´emi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation
with alphacode. arXiv preprint arXiv:2203.07814, 2022b.

Stephan Lukasczyk and Gordon Fraser. Pynguin: Automated unit test generation for python. arXiv

preprint arXiv:2202.05218, 2022.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and

Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint, 2022.

Carlos Pacheco, Shuvendu K Lahiri, Michael D Ernst, and Thomas Ball. Feedback-directed random
test generation. In 29th International Conference on Software Engineering (ICSE’07), pp. 75–84.
IEEE, 2007.

11

Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. Reformulating branch coverage
as a many-objective optimization problem. In 2015 IEEE 8th international conference on software
testing, veriﬁcation and validation (ICST), pp. 1–10. IEEE, 2015.

Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. Automated test case generation
as a many-objective optimisation problem with dynamic selection of the targets. IEEE Transac-
tions on Software Engineering, 44(2):122–158, 2017.

Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt.
Can openai codex and other large language models help us ﬁx security bugs? arXiv preprint
arXiv:2112.02125, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.

Baptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guil-
laume Lample. Leveraging automated unit tests for unsupervised code translation. arXiv preprint
arXiv:2110.06773, 2021.

Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic generation of programming
exercises and code explanations with large language models. arXiv preprint arXiv:2206.11861,
2022.

Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate &
rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.

Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural lan-

guage to code translation with execution. arXiv preprint arXiv:2204.11454, 2022.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit
test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617, 2020.

Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Natural language processing with trans-

formers. ” O’Reilly Media, Inc.”, 2022.

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency
improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s trans-
formers: State-of-the-art natural language processing. ArXiv, 2019.

Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of

large language models of code. In Deep Learning for Code Workshop, 2022.

Kai-Cheng Yang, Brian Aronson, and Yong-Yeol Ahn. Birank: Fast and ﬂexible ranking on bipartite

networks with r and python. Journal of open source software, 5(51), 2020.

12

