Exploring the Impact of Code Style in Identifying
Good Programmers

Rafed Muhammad Yasir
Institute of Information Technology
University of Dhaka
Dhaka, Bangladesh
bsse0733@iit.du.ac.bd

Dr. Ahmedul Kabir
Institute of Information Technology
University of Dhaka
Dhaka, Bangladesh
kabir@iit.du.ac.bd

2
2
0
2

n
u
J

3
2

]
E
S
.
s
c
[

2
v
1
9
8
0
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Code style reﬂects the choice of textual represen-
tation of source code. This study, for the ﬁrst time, explores
whether code style can be used to identify good programmers
with a vision that recruitment process in the software industry
can be improved. For analysis, solutions from Google Code Jam
were selected. The study used cluster analysis to ﬁnd association
between good programmers and style clusters. Furthermore,
supervised machine learning models were trained with stylistic
features to predict good programmers. Results reveal that, al-
though association between programmers with particular clusters
could not be concluded, supervised learning models can predict
good programmers.

Index Terms—code style, style cluster, good programmer iden-

tiﬁcation

I. INTRODUCTION

Code style is the choice of textual representation of source
code, which does not affect the behavior of program execution
[1]. Figure 1 shows two code snippets that are functionally
same but having different code styles. The impact of code
style is seen in areas of software engineering such as software
maintenance [2] and speed of software development [3]. This
paper explores how code style can be used to identify good
programmers .

(a) Style 1

(b) Style 2

Fig. 1: Two functionally same code snippets written in
different styles

Recruiting programmers in the software industry is an
important process as it directly affects software construction.
Recruitment usually involves solving a set of problems in an
exam. The more problems an applicant can solve, the more
”good” that programmer is considered. However, current re-
cruitment processes do not consider that even a good program-
mer may sometimes fail to solve a problem in an exam which
he could have solved otherwise under normal circumstances.
As a result, unfair judgement could occur in some cases. If

a relation between code style and good programmers can be
established, the stylistic metrics could be used as additional
features to improve recruitment process. Additionally, it can
also be used to analyze software repositories to identify the
better programmers that maintain them. This study is an initial
attempt to determine whether code style can be used to identify
good programmers for such purposes.

The dataset chosen for this study were the solutions col-
lected from Google Code Jam (GCJ) [4]. 30 stylistic met-
rics were extracted from the codes and used as features
for analysis. Two methods of analysis were used. Firstly,
clustering algorithms were applied on the data to discover
style clusters and whether programmers could be attributed
to a particular cluster. Secondly, supervised machine learning
models were trained to predict good programmers. The models
were evaluated using recall, F1-macro and area under curve
of ROC (AUC-ROC).

Results show that, although style clusters were found, there
were no particular cluster to which good programmers could
be associated with. However, supervised machine learning
models showed that good programmers can be predicted to
some extent. Based on the evaluating metrics, a Balanced
Random Forest achieved the best result with an average of
0.65 recall, 0.51 macro-F1 and 0.69 AUC-ROC.

II. RELATED WORK

To the best of our knowledge this is the ﬁrst time code style
has been used to identify good programmers. Early researches
conducted by Oman and Cook proposed a taxonomy for code
style to help people grasp a coherent view on the basis and
application of code styles [5]. The four major categories
of their taxonomy are general practices, typographic style,
control structure style and information structure style. They
also concluded in further researches that code style is more
it can affect areas such as code
than cosmetic and that
comprehension [6].

Caliskan et al. proposed a Code Stylometry Feature Set
(FCFS) with which they performed source code authorship
is language agnostic and
attribution [7]. Their feature set
can be used for other programming languages as well. With
their method they achieved 94% accuracy in classifying 1600
authors and 98% accuracy in classifying 250 authors. They

 
 
 
 
 
 
concluded that
this method can help in the identiﬁcation
of authors of malicious programs, ghostwriting detection,
software forensics and copyright investigation.

Mirza and Cosma explored the suitability of using code style
in detecting plagiarism in the BlackBox dataset [8]. BlackBox
is a project that collects data from users of the BlueJ which is
a Java IDE [8]. Their study showed that code style is suitable
for detecting plagiarism.

Regarding evaluating software projects, Zou et al. explored
how code style inconsistency can affect pull request integration
in projects on Github [3]. Upon studying 117 public reposito-
ries they concluded that code styles with certain criteria can
affect both acceptance of pull requests and the time cost of
integrating a pull request.

Mi and Yu conducted a study on stylistic inconsistency in
software projects [2]. They proposed a collection of stylistic
metrics for c++ projects and used these metrics to analyze
small-scale Github projects. By using hierarchical agglom-
erative clustering they showed that stylistic differences exist
between source ﬁles in a project. They concluded that using
the degree of stylistic inconsistency as a basis, code compre-
hensibility and software maintainability could be improved in
the future.

Several tools have been developed that checks stylistic in-
consistencies and helps programmers improve code style. Ala-
Mutka et al. developed style++ that helps students learn good
c++ programming conventions [9]. M¨akel¨a et al. developed
Japroach that checks whether Java programs have a particular
style and if style related issues exist in them [10]. Ogura et
al. developed stylecoordinator to decrease inconsistency and
improve readability in code [1].

III. METHODOLOGY

A. Dataset Description

The solutions collected from Google Code Jam (GCJ) [4]
were used as the dataset for analysis. GCJ is an annual
programming contest held by Google. GCJ is selected as
its data is publicly available and it can loosely resemble
problem solving exams in recruitment processes. Professional
programmers, students and hobbyists participate in GCJ from
all over the world. Therefore, not only does the dataset consist
of source code from varying sources, they also solve the same
problem which makes comparative study possible. The contest
consists of seven rounds, each progressively harder than the
previous. The rounds are- Qualiﬁcation round, Round 1A,
Round 1B, Round 1C, Round 2, Round 3 and World Finals.
We consider the programmers who reached at least Round
as ”good” programmers because participating in these ﬁnal
two rounds requires passing the previous rounds with a good
number of accepted solutions.

Although GCJ accepts solutions in many programming
languages, C++ was selected as the preferred language for
evaluation as it is more popular among participants and has the
highest number of submissions. Each problem of the contest
has two validation sets- small input and large input set. A solu-
tion for the large validation set is a valid solution for the small

TABLE I: Number of Participants in a Round

Qualiﬁcation round
Round 2
Round 3
World Finals

2015
10744
1650
266
22

2016
11401
1641
296
20

2017
11342
1824
286
21

input set but not vice versa. For our analysis, the solutions of
the small input were taken as it has more submissions and
it would also be redundant if both the solutions were taken.
Some solutions could not be taken for analysis as the language
encoding consisted of non standard characters.

The solutions of the contests held in the years 2015, 2016
and 2017 are chosen for experimentation. Problems from
Round 1A, Round 1B, Round 1C and World Finals are
excluded from the dataset for analysis. Round 1 is excluded
because participating in it is not compulsory and thus it lacks
submissions from all participants. World Finals is excluded
because the number of ﬁnalists is too minimal to take into
consideration for analysis. Table I shows the number of
participants in each rounds of the contests.

From the collected data, layout and lexical stylistic features
were extracted based on [7]. Abstract syntax tree based fea-
tures were omitted as these features are not withing the con-
trolling bounds of a programmer. Furthermore, term frequency
based features were also excluded as they largely depend on
the corpus being used. Following these criteria 30 stylistic
features were extracted. The features are listed in table II.

B. Approach

The setups for exploring the effects code style in classifying
good programmers are discussed in this section. Two methods
were used for this purpose- (1) clustering techniques and (2)
supervised machine learning algorithms .

1) Analyzing Using Clustering Techniques: Clustering is
a method of partitioning objects into homogeneous groups
on the basis of similarity among those objects [11]. t-SNE
is one such algorithm that can discover the potential number
of clusters in a dataset with high dimensions [12]. For each
problem in the contests, t-SNE graphs were plotted with the
intent of ﬁnding groups that conform to a particular style.
Each data point in the plots represent a solution submitted by
a programmer. The data points were labelled as-

• Red: reached World Finals
• Green: reached Round 3
• Light blue: other programmers

The plots provided an estimate for the number of clusters and
the distribution of good programmers in the clusters.

To further validate the clustering provided by t-SNE, Hier-
archical Agglomerative Clustering (HAC) with Ward linkage
was performed and dendrograms were plotted. HAC is a
clustering algorithm that treats every data point as a cluster
and they are gradually merged to form a single cluster [13].
The number of clusters indicated by the dendrograms were
matched with the number of clusters indicated by t-SNE before
further analysis were performed.

TABLE II: Feature Description of Dataset

Feature
numTabs/length

numSpaces/length

numEmptyLines/
length
whiteSpaceRatio

newLineBefore
OpenBrace

tabsLeadLines

avgLineLength
stdDevLineLength

numkeyword/length

numTernary/length

numTokens/length

numUniqueTokens/
length
numComments/length

numLineComments/
length
numBlockComments/
length
numLiterals/length

numMacros/length

nestingDepth

numFunctions/length

avgParams
stdDevNumParams

Deﬁnition
Number of tabs divided by ﬁle length in
characters
Number of space characters divided by ﬁle
length in characters
Number of empty lines divided by ﬁle
length in characters
Ratio between the number of whitespace
characters (spaces, tabs, and newlines) and
non-whitespace characters
Ratio between the number of code blocks
preceded by a newline character and not
preceeded by a newline character
Ratio between the number of lines preceded
by a tab and not preceded by a tab
Average length of each line
Standard deviation of the lengths of each
line
Number of occurrences of keyword divided
by ﬁle length in characters, where keyword
is one of if, else, else-if, for, while, do,
break, continue, switch, case (10 different
features)
Number of ternary operators divided by ﬁle
length in characters
Number of word tokens divided by ﬁle
length in characters
Number of unique keywords used divided
by ﬁle length in characters
Number of comments divided by ﬁle length
in characters
Number of line comments divided by ﬁle
length in characters
Number of comments divided by ﬁle length
in characters
Number of string, character, and numeric
literals divided by ﬁle length in characters
Number of preprocessor directives divided
by ﬁle length in characters
Highest depth of control statements and
loops
Number of functions divided by ﬁle length
in characters
Average number of parameters of functions
Standard deviation of the number of param-
eters of functions

To analyze the properties of the t-SNE clusters, solutions
of each problem in the dataset were clustered using K-Means
With K=number of clusters estimated by t-SNE. The solutions
in the data were then labeled based on the cluster they
belonged to. This labeled data was ﬁtted on a Random Forest
Classiﬁer to obtain the feature importance of the tree. Based on
the feature importance of the tree it was concluded what style
groups are present and whether good programmers belong to
a particular style group.

2) Analyzing using Supervised Machine Learning Algo-
rithms: Supervised learning is a method of training a model
that can make predictions based on labeled data [14]. For pre-
dicting good programmers the following models were trained-
Logistic Regression (LR), Support Vector Classiﬁer (SVC), K-
Nearest Neighbors (KNN), Decision Tree (DT) and Random
Forest (RF). A Dummy classiﬁer was also trained to act as a
performance baseline for comparison [15]. The models were

trained for each problems in the dataset. Table I shows that
the number of participants in Round 3 is far less than the
participants in previous rounds. That is, the proportion of
”good” programmers in the dataset is much less in comparison
to the other programmers. This makes the classiﬁcation an
imbalanced classiﬁcation problem [16]. To balance the training
data up-sampling technique SMOTE [17] was used prior to
training the above mentioned models. Furthermore, Balanced
Random Forest (BRF) and RUS Adaboost classiﬁer (RUSAda)
were also trained which performs under-sampling to balance
training data [18]. For bias free results, all trained models were
K-fold cross validated.

IV. EXPERIMENTAL ANALYSIS

A. Performance Evaluation

The clusters created for analysis were evaluated empirically.
Although the analyzed dataset had labels and results could
be evaluated using a metric, it was not done as evaluating
clustering algorithms using labels is not recommended [19].

For the supervised algorithms recall, macro-F1 and Area
Under Curve of ROC (AUC-ROC) were used to evaluate
the models. Recall is the measure of the fraction of good
programmers correctly identiﬁed as good programmers [20].
is calculated as equation (1). F1 is an evaluation
Recall
metric measured by combining precision and recall, and it
is calculated as (3) [20]. Macro-F1 is an arithmetic mean of
the per class F1 scores. It has been selected as an evaluation
criteria as the training data was imbalanced, and Macro-F1
is a good metric for imbalanced data [21]. AUC-ROC is
the area under a ROC curve that allows comparison between
models [20]. Apart from these evaluation metrics, the balanced
accuracy was also reported. Balanced accuracy is deﬁned as
the average of recall obtained on each class [22]

Recall =

T rue P ositive
T rue P ositive + F alse N egative

P recision =

T rue P ositive
T rue P ositive + F alse P ositive

F 1 =

2 ∗ P recision ∗ Recall
P recision + Recall

(1)

(2)

(3)

B. Results and Discussion

The results of analysis of the contests of 2015, 2016 and
2017 are similar. Therefore only the results of one year (2016)
are shown in this section.

Figure 2 shows the t-SNE clusters of all the problems in
the dataset of the year 2016. The caption of each image
shows the round and the problem number. From the graphs
it can be said that 4 clusters exist in each problem. The most
important features of the clusters determined by a Random
Forest Classiﬁer is shown in Table III. All other features had
an importance less than 0.05. It is seen that newLineBefore-
OpenBrace and tabsLeadLines are the most prominent features
in separating the clusters. Upon manual inspection of the codes
of the clusters these combinations were found:

(a) Qualiﬁcation round - 565238

(b) Qualiﬁcation round - 563469

(c) Qualiﬁcation round - 563631

(d) Qualiﬁcation round - 573860

(e) Round 2 - 567760

(f) Round 2 - 572360

(g) Round 2 - 571844

(h) Round 2 - 571860

(i) Round 3 - 574081

(j) Round 3 - 563236

(k) Round 3 - 563461

(l) Round 3 - 512508

Fig. 2: t-SNE Style Clusters of GCJ 2016

TABLE III: Feature importance of Clusters

Features
newLineBeforeOpenBrace
tabsLeadLines
numTabs/length
numSpaces/length

Importance
0.282
0.163
0.151
0.103

• new line before opening braces, tabs lead lines
• no new line before opening braces, tabs lead lines
• new line before opening braces, whitespace lead lines
• no new line before opening braces, whitespace lead lines
Although style clusters were found, the good programmers
were almost equally distributed among them. Thus we cannot
conclude that good programmers belong to a particular style
cluster.

The results of prediction of good programmers in each prob-
lems using supervised learning algorithms are averaged and
showed in Table IV. BRF, LR, SVC and RUSAda performed
better than the dummy model which indicates that some
patterns can be identiﬁed by the models that can be used to
predict good programmers. Also BRF outperformed all models
in terms of Recall, macro F1 and ROC-AUC. While different

studies have used code style for various aspects such as author
identiﬁcation and plagiarism detection, none of the studies
have dealt with good programmer identiﬁcation. Therefore we
cannot compare our results with existing studies. However,
the results can induce further research on how recruitment
processes can be improved with our results.

TABLE IV: Prediction Results of Supervised Learning Models

Model

Recall macro-F1

ROC-AUC

BRF
LR
SVC
RUSAda
Dummy
KNN
DT
RF

0.650
0.641
0.601
0.510
0.485
0.469
0.287
0.185

0.511
0.523
0.523
0.50
0.412
0.494
0.525
0.539

0.695
0.692
0.689
0.626
0.499
0.593
0.542
0.664

Balanced
Accuracy
0.645
0.651
0.639
0.590
0.489
0.565
0.542
0.537

V. THREATS TO VALIDITY
This section presents aspects that may threat the validity of

the study:

• Internal validity: The algorithmic implementations of
the models provided by [23] and [24] were used for

[7] A. Caliskan-Islam, R. Harang, A. Liu, A. Narayanan, C. Voss, F. Ya-
maguchi, and R. Greenstadt, “De-anonymizing programmers via code
stylometry,” in 24th {USENIX} Security Symposium ({USENIX} Secu-
rity 15), 2015, pp. 255–270.

[8] O. M. Mirza, M. Joy, and G. Cosma, “Style analysis for source code
plagiarism detection—an analysis of a dataset of student coursework,”
in 2017 IEEE 17th international conference on advanced learning
technologies (ICALT).

IEEE, 2017, pp. 296–297.

[9] K. Ala-Mutka, T. Uimonen, and H.-M. Jarvinen, “Supporting students
in c++ programming courses with automatic program style assessment,”
Journal of Information Technology Education: Research, vol. 3, no. 1,
pp. 245–262, 2004.

[10] S. M¨akel¨a and V. Lepp¨anen, “Japroch: A tool for checking programming

style,” Kolin Kolistelut—Koli Calling 2004, p. 151, 2004.

[11] S. C. Johnson, “Hierarchical clustering schemes,” Psychometrika,

vol. 32, no. 3, pp. 241–254, 1967.

[12] G. C. Linderman and S. Steinerberger, “Clustering with t-sne, provably,”
SIAM Journal on Mathematics of Data Science, vol. 1, no. 2, pp. 313–
332, 2019.

[13] K. Sasirekha and P. Baby, “Agglomerative hierarchical clustering
algorithm-a,” International Journal of Scientiﬁc and Research Publi-
cations, vol. 83, p. 83, 2013.

[14] S. J. Russell and P. Norvig, Artiﬁcial intelligence: a modern approach.

Malaysia; Pearson Education Limited,, 2016.

[15] Scikit-learn. Dummy classsiﬁer. [Online]. Available: https://scikit-learn.
org/stable/modules/generated/sklearn.dummy.DummyClassiﬁer.html
[16] N. Japkowicz and S. Stephen, “The class imbalance problem: A system-
atic study,” Intelligent data analysis, vol. 6, no. 5, pp. 429–449, 2002.
[17] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:
synthetic minority over-sampling technique,” Journal of artiﬁcial intel-
ligence research, vol. 16, pp. 321–357, 2002.
Ensembled methods.

[18] Imbalanced-learn.

[Online].
https://imbalanced-learn.readthedocs.io/en/stable/api.html#
module-imblearn.ensemble

Available:

[19] I. F¨arber, S. G¨unnemann, H.-P. Kriegel, P. Kr¨oger, E. M¨uller, E. Schu-
bert, T. Seidl, and A. Zimek, “On using class-labels in evaluation of
clusterings,” in MultiClust: 1st international workshop on discovering,
summarizing and using multiple clusterings held in conjunction with
KDD, 2010, p. 1.

[20] D. M. Powers, “Evaluation: from precision, recall and f-measure to roc,

informedness, markedness and correlation,” 2011.

[21] B. Wu, S. Lyu, and B. Ghanem, “Constrained submodular minimization
learning,” in

for missing labels and class imbalance in multi-label
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.
[Online].

Avail-
accuracy.
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.

[22] Scikit-learn.
able:
balanced accuracy score.html

Balanced

[23] ——. Scikit learn. [Online]. Available: https://scikit-learn.org/stable/
[24] Imbalanced-learn.

[Online]. Available:

Imbalanced-learn.

https:

//imbalanced-learn.readthedocs.io/en/stable/index.html

analysis. Any implementation errors of these algorithms
may affect the validity of the results.

• External validity: The analysis was done on the source
ﬁles of the GCJ dataset. Therefore the ﬁndings of this
study may not be generally applicable to contests of other
formats. Furthermore, as only C++ codes were selected
for analysis, it cannot be said whether stylistic features of
other programming languages will show similar results.
Additionally, the criteria for deﬁning a good programmer
is subjective and could be deﬁned in other ways depend-
ing on the context. In such contexts our results will not
be generalized.

To ensure reliability of the study,

the results of anal-
ysis are made publicly available in Jupyter notebooks at
github.com/rafed123/GcjStyleAnalysis.

VI. CONCLUSION

This paper explores whether code style can be used to
identify good programmers with a vision that the program-
mer recruitment process can be improved. The study was
conducted on C++ solutions from the Google Code Jam
contest. Clustering techniques such as t-SNE and hierarchical
agglomerative clustering were used to discover whether style
clusters exist and if good programmers could be attributed to
any of them. Although four style clusters were found, good
programmers could not be associated to a particular cluster.
However, supervised machine learning showed that stylistic
attributes can be used to predict good programmers. Seven
machine learning models were trained and evaluated using
recall, macro-F1 and AUC-ROC. A Balanced Random Forest
yielded the best results with 0.650 recall, 0.511 macro-F1 and
0.695 ROC-AUC. The results indicate that code style can be
used as a measure to identify good programmers.

In future, how the prediction of good programmers using
code style can be effectively combined with the current
recruitment process will be explored. Furthermore, what values
of a stylistic metric indicates a good programmer can also be
explored. Additionally, our results could be further improved
using other techniques.

REFERENCES

[1] N. Ogura, S. Matsumoto, H. Hata, and S. Kusumoto, “Bring your own
coding style,” in 2018 IEEE 25th International Conference on Software
Analysis, Evolution and Reengineering (SANER).
IEEE, 2018, pp. 527–
531.

[2] Q. Mi, J. Keung, and Y. Yu, “Measuring the stylistic inconsistency
in software projects using hierarchical agglomerative clustering,” in
Proceedings of the The 12th International Conference on Predictive
Models and Data Analytics in Software Engineering. ACM, 2016,
p. 5.

[3] W. Zou, J. Xuan, X. Xie, Z. Chen, and B. Xu, “How does code style
inconsistency affect pull request integration? an exploratory study on
117 github projects,” Empirical Software Engineering, pp. 1–33, 2019.
[4] Google. Past contests, google code jam. [Online]. Available: https:

//code.google.com/codejam/past-contests

[5] P. W. Oman and C. R. Cook, “A programming style taxonomy,” Journal

of Systems and Software, vol. 15, no. 3, pp. 287–301, 1991.

[6] ——, “Typographic style is more than cosmetic,” Communications of

the ACM, vol. 33, no. 5, pp. 506–520, 1990.

