2
2
0
2

n
u
J

0
2

]
E
S
.
s
c
[

1
v
4
1
5
9
0
.
6
0
2
2
:
v
i
X
r
a

CURRENTLY UNDER REVIEW

1

Ethics in AI through the Developer’s Prism:
A Socio-Technical Grounded Theory
Literature Review and Guidelines

Aastha Pant, Rashina Hoda, Chakkrit Tantithamthavorn, Burak Turhan

Abstract— The term ‘ethics’ is widely used, explored, and debated in the context of developing Artiﬁcial Intelligence (AI) systems. In
recent years, there have been numerous incidents that have raised the proﬁle of ethical issues in AI development and led to public
concerns about the proliferation of AI technology in our everyday lives. But what do we know about the views and experiences of those
who develop these systems – the AI developers? We conducted a Socio-Technical Grounded Theory Literature Review (ST-GTLR) of
30 primary empirical studies that included AI developers’ views on ethics in AI to derive ﬁve categories that discuss AI developers’
views on AI ethics: developer’s awareness, perception, needs, challenges, and approach. These are underpinned by multiple codes
and concepts that we explain with evidence from the included studies. Through the steps of advanced theory development, we also
derived a set of relationships between these categories and presented them as ﬁve hypotheses, leading to the ’theory of ethics in AI
through the developer’s prism’ which explains that developers’ awareness of AI ethics directly leads to their perception about AI ethics
and its implementation as well as to identifying their needs, and indirectly leads to identifying their challenges and coming up with
approaches (applied and potential strategies) to overcome them. The theory provides a landscape view of the key aspects that concern
AI developers when it comes to ethics in AI. We also share an agenda for future research studies and recommendations for
developers, managers, and organisations to help in their efforts to better implement ethics in AI.

Index Terms—Grounded theory literature review, GTLR, artiﬁcial intelligence, ethics, AI ethics, socio-technical grounded theory, STGT,
socio-technical grounded theory literature review, ST-GTLR, developers, software engineering

(cid:70)

1 INTRODUCTION

E Thics refers to “the moral principles that govern the behav-

iors or activities of a person or a group of people” [1]. Ethics
can also be deﬁned as “the way an individual behaves and
the values they hold” [2]. Ethics can be applied to computers
and machines. The process of attributing moral values and
ethical principles to machines to resolve ethical issues they
encounter, and enabling them to operate ethically is a form
of applied ethics [3]. ’AI ethics’ refers to ”the principles of
developing AI to interact with other AIs and humans ethically
and function ethically in society” [4].

Ethical consideration in AI is very important [5] as
demonstrated by several prominent incidents in recent years
where lack of ethical consideration has resulted in severe
consequences. In a recent example, software developers
were upset due to GitHub’s unauthorised and unlicensed
use of copyright source code as training data for their
machine learning powered GitHub Copilot product. It is an
example of an ethical issue as the product was injecting
source code derived from copyright sources into the cus-
tomers’ software without informing them of the license of
the original source code [6].

In another case, the founder of the Algorithmic Justice
League (AJL) found that facial recognition algorithms work

• Pant, Hoda and Tantithamthavorn are with Faculty of IT, Monash
University, Melbourne, Australia E-mail: aastha.pant@monash.edu,
rashina.hoda@monash.edu, chakkrit@monash.edu

• Turhan is with Faculty of IT, University of Oulu, Oulu, Finland

E-mail: burak.turhan@oulu.ﬁ

Manuscript received June 2022

best on white men and worst on black women. The head
of AJL mentioned that there is no guarantee that such
issues are solved as there are no accountability mechanisms.
The case study reported the issue of racial bias as well as
accountability issues in AI [7]. In 2018, Amazon Inc. ceased its
new recruitment tool when the machine learning specialists
of the company found out that the tool was biased against
women. The tool preferred male candidates over female
candidates during the recruitment process [8]. Likewise,
in the Netherlands, the court passed a law to restrict the
government from deploying the AI-based social security
fraud detection system as it was not transparent on how the
model calculated the fraud risks. This lack of transparency
of the model violated the people’s fundamental rights [9].

These and many such case studies compelled us to
believe that ethical consideration in the development of AI
is signiﬁcant and necessary in today’s time and motivated
us to explore the area of ethics in AI. We were interested in
exploring the broader context of ethics in AI to understand
the lay of the land, and inform future studies into more
focused areas. In particular, we were interested in exploring
this important area from the perspective of those closest to
it, and indeed in one of the best positions to bring about
changes and improvements, – the AI developers. Given the
increasing prominence of the topic of ethics in AI, and
growing research interest, exploring existing literature to
gain an understanding seemed like a reasonable approach.
To understand developers’ views on AI ethics as
presented in literature, we conducted a socio-technical
grounded theory literature review or ST-GTLR, a literature

 
 
 
 
 
 
CURRENTLY UNDER REVIEW

2

review method that applies the principles and practices
of socio-technical grounded theory (STGT) as proposed by
Hoda [10] to the high-level framework of grounded theory
literature review (GTLR) as proposed by Wolfswinkel et
al. [11]. We applied GTLR’s ﬁve step framework of deﬁne,
search, select, analyse, and present with concrete data analysis
and theory development steps of socio-technical grounded
theory. Our experience suggests that while an ST-GTLR can
be conducted for most contexts, it is particularly suited
to niche and emerging topics where there may not be
enough literature accumulated over time to conduct a sys-
tematic literature review but where the topic is signiﬁcant
for the practice and research communities to fundamen-
tally understand and unpack. An added advantage of the
method is its focus on rigorous analysis and the devel-
opment of evidence-based theoretical concepts or mature
theory, to guide research and practice in the area. Another
reason for conducting an ST-GTLR is its methodological
alignment with empirical socio-technical grounded theory
(STGT) studies [10], since that is the next step of our wider
research program.

We ﬁrst deﬁned the overarching question, What do we
know from literature about how developers view ethics in
AI? Then, we developed an ST-GTLR protocol to ﬁnd and
analyse primary empirical studies investigating developers’
views on AI ethics. After rounds of searching and ﬁltering,
we rigorously analysed data from 30 primary empirical
studies using STGT techniques such as open coding, targeted
coding, constant comparison, and memoing and advanced the-
ory development [10].

GTLR studies have been conducted in domains such
as health [12], [13], [14], banking [15], and education [16].
GTLR has also been used to study topics in information
systems research [17], on the use of information and com-
munication technology (ICT) such as the techniques used by
social media inﬂuencers and their potential for inﬂuencing
the public regarding environmental awareness [18], and
national ICT policy challenges for developing countries [19].
To the best of our knowledge, this is the ﬁrst ST-GTLR,
and as such we present the guidelines for conducting an ST-
GTLR, including describing its context of use, steps and
procedures, as well as details of our application in this review
study, in Section 3. This is followed by presenting the ﬁnd-
ings of our ST-GTLR, including ﬁve categories discussing
AI developers’ views on AI ethics: awareness, perception,
needs, challenges, and approach, detailed in Section 4. We
derived a set of relationships between these categories in
ﬁve hypotheses that, together with the key categories, make
up the ‘theory of ethics in AI through the developers’ prism’,
explained in Section 5 (H1-H5). The hypotheses lead to the
theory of ethics in AI through the developer’s prism, which we
visually represent using the developer’s prism metaphor.
We also present insights and recommendations in Section 6,
followed by reﬂections in Section 7, evaluation is Section 8,
threats and limitations in Section 9 and conclusion in Section
10. The main contributions of this paper are:

• A source of gathered information on AI developers’

views on and understanding of ethics in AI.

• A theory of ethics in AI through the developer’s prism that
presents the key categories as developers’ awareness,

perception, needs, challenges, and approach, and associ-
ated hypotheses.

• A guidance for practitioners who require a better un-
derstanding of the requirements and factors affecting
ethics implementation in AI.

• A set of recommendations for future research in the
area of ethics implementation in AI from developers’
perspective.

• Guidelines for socio-technical grounded theory liter-
ature review (ST-GTLR) through an example appli-
cation.

.

2 BACKGROUND – ETHICS IN AI
There are numerous and divergent views on the topic of
ethics in AI [G4], [20], [21], as it has being increasingly
applied in various contexts and industries [G20]. AI prac-
titioners and researchers seem to have mixed perspectives
about AI ethics. Some believe that there is no rush to
consider AI related ethical issues as AI has a long way from
being comparable to human capabilities and behaviours [4].
While others conclude that AI systems must be developed
by considering ethics as they can have enormous societal
impact [5], [22]. Although the viewpoints vary from prac-
titioner to practitioner, most conclude that AI ethics is an
emerging and widely discussed topic and a current relevant
issue of the real world [23].

A number of studies conducted in the area of ethics in AI
have been conceptual and theoretical in nature [G15]. Criti-
cally, there are copious numbers of guidelines on AI ethics,
making it challenging for AI developers to decide which
guidelines to follow. Unsurprisingly, studies have been con-
ducted to analyse the ever-growing list of speciﬁc AI princi-
ples [G10], [G17], [4]. For example, Jobin et al. [24] reviewed
84 ethical AI principles and guidelines and concluded that
only ﬁve AI ethical principles – transparency, fairness, non-
maleﬁcence, responsibility and privacy – are mostly discussed
and followed. Fjeld et al. [25] reviewed 36 AI ethical princi-
ples and reported that there are eight key themes of AI ethics
– privacy, accountability, safety and security, transparency and
explainability, fairness and non-discrimination, human control of
technology, professional responsibility, and promotion of human
values. Likewise, Hagendorff [21] analysed and compared
22 AI ethical guidelines to examine their implementation
in the practice of research, development, and application of
AI systems. Some review studies focused on exploring the
challenges and potential solutions in the area of ethics in
AI, e.g. [26], [27]. The desire to set ethical guidelines in AI
has been enhanced due to increased competition between
organisations to develop robust AI tools [23]. Among them,
only a few guidelines indicate an oversight or enforcement
mechanism [28].

Another prominent area of focus have been studies that
were conducted to discuss the existing gap between research
and practice in the ﬁeld of ethics in AI. Smith et al. [29]
conducted a review study to identify gaps of ethics research
and practice of ethical data-driven software development
and highlighted how ethics can be integrated into the de-
velopment of modern software. Similarly, Shneiderman [30]
provided 15 recommendations to bridge the gap between

CURRENTLY UNDER REVIEW

3

ethical principles of AI and practical steps for ethical gov-
ernance. Overall, existing studies seem to primarily focus
on either analysing the plethora of ethical AI principles or
ﬁlling the gap between research and practice.

process with elaborated analysis stage (Figure 1). We also
include the details of the application of these guidelines and
demonstrate how we conducted the ST-GTLR in the context
of our review study.

We also came across solution-based papers and papers
discussing models, frameworks, and methods for AI de-
velopers to enhance their AI ethics implementation. For
example, an article by Vakkuri et al. [31] presents the AI
maturity model for AI software, whereas another article
by Vakkuri et al. [32] discusses the ECCOLA method for
implementing ethically aligned AI systems. Likewise, there
are papers presenting the toolkit to address fairness in ML
algorithms [33] and transparency model to design transpar-
ent AI systems [34]. However, we did not include these
papers in the list of our seed papers because they do not
provide insights on the developer’s views on ethics in AI.

We believe it is essential to study developers’ views
and understanding about AI ethics as they are the ones
developing the AI systems and are one of the best places
to bring about improvements in practice. While the impor-
tance of understanding developers’ views on AI ethics has
been acknowledged [G4], limited attention has been paid to
investigating it. In particular, there are not enough research
articles focusing on the topic to effectively conduct a sys-
tematic literature review or mapping study. For example,
a keyword search on prominent SE databases resulted in
under 1% return in ﬁnding relevant papers. This is mainly
because there are not enough papers that primarily inves-
tigate AI developers’ views on ethics in AI. Papers that do
include this as part of their ﬁndings are difﬁcult to identify
without a full read through, making it ineffective and im-
practical when dealing with thousands of papers. Hence, we
turned to grounded theory literature review (GTLR) [11] as
a review approach for this niche but signiﬁcant and rapidly
emerging topic. While the overarching review framework
from Wolfswinkel et al. helped frame the review process,
we found ourselves having to work through the concrete
application details using the practices of socio-technical
grounded theory (STGT).

3 REVIEW METHODOLOGY- GUIDELINES & AP-
PLICATION

Socio-Technical Grounded Theory Literature Review
(ST-GTLR) is an iterative and responsive literature re-
view method that applies the ﬁve-step framework of
deﬁne, search, select, analyse and present described in
the original GTLR guidelines by Wolfswinkel et al.
[11] with concrete data analysis (and optional theory
development) steps of socio-technical grounded theory
[10]. A GTLR is particularly well suited to studying
niche and emerging research areas to gain in-depth
understanding, establish theoretical foundations, and
make practical recommendations.

3.1 Context of Use

We begin by listing our recommendations on when, why,
and how to use the ST-GTLR method, based on our practical
experiences.

• When to use: Due to its iterative and responsive na-
ture, ST-GTLR is particularly suited to niche and/or
new and emerging topics where there is not enough
literature on the speciﬁc topic (rather, relevant infor-
mation is contained within and spread across papers
focusing on related topics) but where the topic is
signiﬁcant for the practice and research communities
to fundamentally understand and pack, such as is
common in the dynamic discipline of software engi-
neering.

• Why use: Due to its focus on rigorous analysis and
theory development, an ST-GTLR study leads to rich
ﬁndings and evidence-based theories that can serve
as research agendas for the emerging area and as
guides for practice.

• How to use: ST-GTLR can be applied as a review
method to produce original ﬁndings and theories,
reported as standalone papers. Due to ST-GTLR’s
inherent methodological alignment, it can also be used
as a pre-cursor to empirical STGT studies in the same
topic area.

We performed an ST-GTLR to undertake an in-depth
exploration of the ethics in AI from the perspective of devel-
opers1 working in the ﬁeld of AI. The main objective of our
review study was to enable systematic and evidence-based
development of a theoretical framework, understand the
various facets of the practice area, to guide future research
in this emerging area, and to present recommendations for
practice. Figure 1 shows an overview of ST-GTLR, adapted
from [11] with further details of the analyse step where
STGT’s data analysis [10] is most explicitly applied.

To the best of our knowledge, this is the ﬁrst ST-GTLR
study in software engineering. As such, we take the oppor-
tunity to describe the details of each of the stages in the
following sub-sections and reﬂect on our experiences.

3.2 Deﬁne

The ﬁrst step is to formulate the initial review protocol,
including determining the scope of the study by deﬁning
inclusion and/or exclusion criteria, publication period, and
search items, followed by databases to search in, research
questions, and search strings. Following this approach, we
deﬁned our inclusion and exclusion criteria, publication
period, language of the articles to be included followed
by the guiding research question (RQ), search string and
appropriate sources and speciﬁc search items with the aim

In this section, we present the guidelines for conducting
an ST-GTLR, including a deﬁnition of ST-GTLR (see deﬁni-
tion box), the context of use, the basic steps and procedures
of the ST-GTLR method and a diagram of the ST-GTLR

1. The term ’developers’ in our study include AI programmers, prac-
titioners, engineers, specialists, experts, designers, and stakeholders.
We use the terms ’AI developers’ and ’developers’ interchangeably
throughout our study.

CURRENTLY UNDER REVIEW

4

Fig. 1: Socio-Technical Grounded Theory Literature Review (ST-GTLR) process combining GTLR [11] and STGT [10].

Fig. 2: Examples of STGT data analysis using open and targeted coding [10]

of obtaining maximum relevant primary empirical studies.
The RQ was formulated to gain an overall understanding of
ethics in AI from the perspective of AI developers, as:

What do we know from literature about how developers

view ethics in AI?

Four popular digital databases, namely, ACM Digital
Library (ACM DL), IEEE Xplore, SpringerLink and Wiley Online
Library (Wiley OL) were used as sources to identify the rele-
vant literature. These databases have been regularly used to
conduct reviews on human aspects of software engineering,
e.g., [35], [36]. Initially, we searched for relevant studies

which were published in journals and conferences only and
for which full texts were available.

Key terms were selected from the research title to de-
velop search queries. The ﬁrst key terms were ‘ethics’ + ’AI’
+ ‘developers’ as the aim of our study is to ﬁnd AI devel-
oper’s views about ethics in AI. Then, synonyms of the key
search terms were used to retrieve more relevant primary
studies. The search terms were linked with ‘AND’ and ‘OR’
Boolean operators when developing the ﬁnal search string.
The purpose of the ‘AND’ operator was to concatenate the
key terms whereas the ‘OR’ operator linked the synonyms.

CURRENTLY UNDER REVIEW

TABLE 1: Formulation of search string

First search string: (“ethics” OR “trust” OR “morals” OR “fairness” OR
“responsib*”) AND (“artiﬁcial intelligence” OR “AI”
OR “machine learning”) AND (“software developer” OR
“software practitioner” OR “data scientist” OR “machine
learning” OR “software engineer” OR “programmer”)

Final search string: (”ethic*” OR ”moral*” OR ”fairness”) AND (”artiﬁcial
intelligence” OR ”AI” OR ”machine learning” OR ”data
science”) AND (”software developer” OR ”software
practitioner” OR ”programmer”)

Six candidate search strings were developed and exe-
cuted on the four online databases before one was ﬁnalised.
Table 1 shows the initial and the ﬁnal search strings created
for this study. As the ﬁnalised search string returned an
extremely large number of primary studies (N=9,806), we
restricted the publication period from January 2010 to June
2021, in all four databases, as the topic of ethics in AI has
been gaining rapid prominence in the last ten years. Table
2 shows the seed protocol of this study, including inclusion
and exclusion criteria. For example, an inclusion criterion
was that the study should be written in English, and be
empirically based, presenting evidence of AI developers’
views on ethics in AI. The exclusion criteria included: stud-
ies such as workshop articles, short papers which were less
than four pages, books, gray literature, theses, unpublished
and incomplete work. It also included studies written in
language other than English and duplicate articles. Like-
wise, studies that presented only the concept of ethics in
AI without empirical evidence, review papers, and studies
discussing topics irrelevant to the RQ were also considered
grounds for exclusion.

3.3 Search

The second stage involves performing the actual search
using the review protocol deﬁned in the ﬁrst stage [11].
We performed the search using our seed review protocol
presented in Table 2. The search process was iterative and
time-consuming because some essential synonyms of the
search terms were missing initially, and we had to revisit
the deﬁne stage again and again before moving to the next
stage (e.g. see the initial and ﬁnal search strings developed
for this study in Table 1).

3.4 Select

The selection of sample texts is done in this stage. The
steps include (i) ﬁltering out doubles, (ii) reﬁning sample
based on title and abstract, (iii) reﬁning sample based on
full text, (iv) scanning forward and backward citations, and
(v) checking if new articles come up in the last step as these
steps are iterative [11]. If new articles come up in the last
step, researchers should go to step (i) and follow other steps
accordingly. If no new articles come up, then that will be the
ﬁnal sample to be included in the study.

Following these guidelines, we obtained a total of 1,244
primary articles (ACM DL: 273, IEEEX: 355, SpringerLink:
543 and Wiley OL: 73) using the seed review protocol as
shown in Table 2 and the ﬁnal search string. After ﬁltering
out the duplicates, we were left with 982 articles. As per
the Wolfswinkel et al. guidelines, the next step was to reﬁne

5

the whole sample based on title and abstract. We tried this
approach for the ﬁrst 200 articles each that came up in ACM
DL, IEEEX and SpringerLink and all 73 articles in Wiley OL to
get a sense of the number of relevant articles to our research
question. We read the abstract of the articles whose title
seemed relevant to our research topic and tried to apply
the inclusion and exclusion criteria (in Table 2) to select the
relevant articles. We quickly realised that selection based on
title and abstract was not working well. This is because the
presence of the key search terms (e.g., ethics AND AI AND
developer) was rather common and did not imply that the
paper would include the developers’ perspective on ethics
in AI. We found ourselves having to scan through full texts
to judge relevance to our RQ. Despite the effort involved,
the return on investment was low, e.g. for every 100 papers
read, we found only one or two relevant to our study, i.e.,
that presented the AI developers’ views on ethics in AI.

From a quick reading of 673 papers, we obtained only
10 primary articles that were relevant to our research
topic. Many articles, albeit interesting, did not present the
AI developers’ views on ethics in AI. So, we decided to
ﬁnd more relevant articles through snowballing of articles.
Snowballing of those 10 articles via forward citations and
backward citations was done until 31 December 2021 to ﬁnd
more relevant articles and enrich the quality. Snowballing
seemed to work better for us than the traditional search
approach. We modiﬁed the seed protocol accordingly, to
include papers published in other databases and those pub-
lished beyond journals and conferences, including students’
theses, reports, and papers uploaded to ArXiv. The rest of
the protocol remained the same. The ﬁnal review protocol
used in this study is presented in Table 2. In this way, we
obtained 20 more relevant articles, taking the total number
of primary articles to 30.

The select step of scanning through the full contents
of 673 articles was very tedious with very low return on
investment (e.g. only 10 relevant studies obtained). In hind-
sight, we would have done better to start with a set of seed
papers that were collectively known to the research team or
those obtained from some quick searches on Google Scholar.
What we did next by proceeding from the seed papers to
cycles of snowballing, was more practical, productive, and
in line with the iterative GT approach as a form of applied
theoretical sampling.

3.5 Analyse

We applied Socio-Technical Grounded Theory (STGT) to
conduct our review because its socio-technical research
framework is customised to ﬁt the unique socio-technical
contexts of domains such as software engineering and arti-
ﬁcial intelligence [10].

•

•

Socio-technical phenomenon: The topic of studying
ethics in AI from the developers’ viewpoint presents
a distinctly socio-technical phenomenon, where “the
social and technical aspects are interwoven in a way that
studying one without due consideration of the other makes
for an incomplete investigation and understanding” [10].
Socio-technical domain and actors: Our domain of in-
vestigation was artiﬁcial intelligence, and we were

CURRENTLY UNDER REVIEW

TABLE 2: Seed and Final ST-GTLR Protocols

6

Seed ST-GTLR Protocol

Final ST-GTLR Protocol

Digital
Databases

Search
Items

ACM Digital Library
IEEE Xplore
SpringerLink
Wiley Online Library

Journal articles
Conference papers
Full Text

Language

English

Publication Period

January 2010 to June 2021

(”ethic*” OR ”moral*” OR “fairness”) AND
(”artiﬁcial intelligence” OR ”AI” OR ”machine learning”
OR ”data science”) AND (”software developer” OR
”software practitioner” OR ”programmer”)

Each study must be a full text published journal article or
conference paper
Studies that are written in English
Studies that are empirical
Studies that present AI developers’ views on ethics in AI

Search
String

Inclusion
Criteria

Exclusion
Criteria

No limitations on databases

Journal articles
Conference papers
Students’ theses
Reports
Papers on arXiv
Full Text

English

January 2010 to December 2021

Snowballing applied in later iterations

Each study must be a full text published journal article,
conference paper, students’ thesis, report or paper on arXiv
Studies that are written in English
Studies that are empirical
Studies that present AI developers’ views on ethics in AI

Workshop articles, short papers (less than 4 pages),
books, gray literature, theses, unpublished and incomplete work
Studies written in language other than English
Review papers
Duplicate articles
Theoretical or conceptual studies on ethics in AI (non-empirical)
AI related topics that do not include developers’ perspectives

Workshop articles, short papers (less than 4 pages),
books, gray literature and incomplete work
Studies written in language other than English
Review papers
Duplicate articles
Theoretical or conceptual studies on ethics in AI (non-empirical)
AI related topics that do not include developers’ perspectives

•

•

studying the viewpoints of actors in that domain, the
AI developers, both being socio-technical.
Socio-technical researchers: Our team included re-
searchers with the requisite technical background in
software engineering and AI, social science (e.g.,
traditional GT experience), empirical and qualitative
research, philosophical foundations, and theory de-
velopment.
Socio-technical data, tools, and techniques: We were us-
ing qualitative data gathered from relevant SE/AI
literature and the STGT data analysis techniques.

STGT includes basic data analysis procedures such as
open coding, constant comparison, and memoing that are
common to all GT variants and advanced data analysis
procedures such as options of targeted data collection and
analysis (DCA) and theoretical structuring or structured
integration, depending on the re-
DCA and theoretical
searchers’ choice of emergent or structured modes of theory
development respectively [10]. In our ST-GTLR, we applied
open coding, constant comparison, memoing in the basic
stage and targeted DCA and theoretical structuring in the
advanced stages using the emergent mode of theory develop-
ment.

The qualitative data included primarily the context and
ﬁndings covered in the primary studies, including excerpts
of raw underlying empirical data included in the papers.
Data was analysed iteratively in small batches. At ﬁrst, we
analysed the qualitative data of 10 articles that were ob-
tained in the initial phase. We used the standard techniques
of STGT data analysis such as open coding, constant com-
parison, and memoing for those 10 articles and advanced

techniques of STGT data analysis such as targeted coding
on the rest 20 articles, followed by theoretical structuring.
The right-hand side of Figure 1 shows the details of the
data analysis in the Analyse stage of the ST-GTLR process,
as applied in this study. This approach of data analysis is
rigorous and helped us to obtain multidimensional results
that were original, relevant and dense, as evidenced by the
depth of the categories and underlying concepts (presented
in Section 4). The techniques of the STGT data analysis are
explained in the following section. We also obtained layered
understanding and reﬂections through reﬂective practices
like memo writing, which are presented in Section 6.

3.5.1 The Basic Stage – Open Coding
We performed open coding to generate codes from the qual-
itative data of the initial set of 10 articles. Open coding was
done for each line of the Findings section of the included
articles to ensure we did not miss any information and
insights related to our RQ. The length of the qualitative data
varied from article to article. For example, some articles had
an in-depth and long Findings section whereas some had
short sections. Open coding for some articles consumed a
lot of time and led to hundreds of codes whereas a limited
number of codes were generated for some other articles.

Similar codes were grouped into concepts and similar
concepts into categories using constant comparison. Exam-
ples of the application of STGT for Data Analysis [10] to
generate codes, concepts and categories are exhibited in
Figure 2 and a number of quotations from the original pa-
pers are included in the Findings section, providing strength
of evidence [10]. The process of developing concepts and
categories was iterative. As we read more papers, we reﬁned

CURRENTLY UNDER REVIEW

7

2. We performed targeted coding in chunks of two to three
sentences or short paragraphs that seemed relevant to our
emergent ﬁndings, instead of the line by line coding, and
continued with constant comparison. This process was a
lot faster than open coding. The codes developed using
targeted coding were placed under relevant concepts, and
new concepts were aligned with existing categories in the
same Google spreadsheet.

In this stage, our memos became more advanced in
the sense that they helped identify relationships between
the categories and develop hypotheses. We continued with
targeted data collection and analysis until we reached a
point of diminishing results, described as theoretical sat-
uration, where analysing new papers served to validate
the emerging theory rather than lead to new insights or
categories.

To structure the emergent theory, we made use of a
developer’s prism metaphor that the research team iden-
tiﬁed during one of the discussions and coding workshops.
A triangular prism is useful for analysing and reﬂecting
light. An ordinary triangular prism can separate white light
into its constituent colours, called a spectrum. White light
entering a prism is bent, or refracted, and the light separates
into its constituent wavelengths, representing red, orange,
yellow, green, blue, indigo, and violet. Using the prism
metaphor, we see that the topic of ethics in AI looks like
a single ray of white light. But, when it is viewed through
a developer perspective, i.e., when it enters the developer’s
prism, the monochromatic ray of white light can be seen to
separate into its constituent wavelengths. The wavelengths
here refer to the spectrum of ﬁve distinct aspects – developer
awareness, perception, needs, challenges and approach. In
simple words, while the topic of ‘ethics in AI’ may look like
a single phenomenon, seen from the developer’s prism, it is
rather a multi-faceted and complex phenomenon composed
of a spectrum of distinct aspects, represented by each of
the categories. Figure 4 shows a visual representation of the
theory using the developer’s prism metaphor.

3.6 Present

The last step of the ST-GTLR process is to present the
ﬁndings. The key ﬁndings are presented through textual
descriptions accompanied by original quotations from the
included primary studies. Furthermore, Wolfswinkel et al.
recommends, ”presenting ﬁndings using visualisations such as
diagrams can help reach a wider audience” [11]. To visualise
the ﬁndings of the 30 primary empirical studies, we created
model diagrams (Figures 4 and 5).

4 FINDINGS – KEY CATEGORIES

Five key categories emerged from the analysis: (1) Devel-
oper Awareness, (2) Developer Perception, (3) Developer Needs,
(4) Developer Challenges and (5) Developer Approach. Taken
together, they represent the aspects AI developers are con-
cerned with when considering ethics in AI. We describe each
of the ﬁve key categories, their underlying concepts, and
use quotes from the included primary studies by attributing
them to paper IDs G1 to G30. Figure 4 shows a visualisation
of the prism metaphor. Figure 5 shows the theory diagram

Fig. 3: An example of a memo arising from the code (”prin-
ciples vs practice gap”) labelled [C1]

the emerging concepts and categories based on the new
insights obtained. The coding was performed by the ﬁrst
author in Google docs to begin with, followed by Google
spreadsheet as the number of codes and concepts started
growing. Both these formats enabled the ease of reviewing
and providing feedback by the second author, and were ac-
companied by detailed discussions leading to reﬁnements.
Each code was numbered as C1,C2,C3 and labelled with the
paper ID (e.g. G1, G2, G3) that it belonged to, to enable
tracing and improve retrospective comprehension of the
underlying contexts.

While the open coding led to valuable results in the
form of codes, concepts, and categories, memoing helped
us reﬂect on the insights related to the most prominent
codes, concepts, and emerging categories. We also wrote
reﬂective memos to document our reﬂections on the pro-
cess of performing an ST-GTLR using an STGT approach.
These insights and reﬂections are presented in Section 6. An
example of a memo created for this study is presented in
Figure 3.

3.5.2 Advanced Stage – Theory Development

The codes and concepts generated from open coding in the
basic stage led to the emergence of ﬁve categories: Developer
Awareness, Developer Perception, Developer Needs, Developer
Challenges and Developer Approach. Once these categories
were generated, we proceeded to identify new papers using
forward and backward snowballing in the advanced stage
of theory development.

Since our topic under investigation was rather broad
to begin with, an emergent mode of theory development
seemed appropriate in the next, advanced stage of STGT
[10]. This decision was further conﬁrmed as the categories
identiﬁed were distinct and well supported but the links
between them, which deﬁne the shape or structure of a the-
ory, were still unclear. We proceeded to iteratively perform
targeted data collection and analysis on more papers. Reﬂec-
tions captured through memoing and snowballing served
as an application of theoretical sampling when dealing with
published literature, similar to how it is applied in primary
STGT studies.

Targeted coding involves generating codes that are rele-
vant to the preliminary but strong concepts and categories
[10]. For example, see the emergence of a new code cultural
norms realisation during targeting coding that supported the
concept human limitations, which in turn led to the category
Developer Awareness identiﬁed in the basic stage, in Figure

CURRENTLY UNDER REVIEW

8

including the ﬁve key categories, each of which are detailed
in this section, and the ﬁve hypotheses, which are explained
in Section 5.

4.1 Developer Awareness

The ﬁrst category that emerged was Developer Awareness.
This category emerged from three underlying concepts:
developer awareness of AI ethics & principles, developer awareness
of human limitations, and reasons for developer awareness.

4.1.1 Developer Awareness of AI Ethics & Principles
Most articles reported that the developers participating in
their study were aware of ethics, including its importance
[G1], [G4], [G5], [G17] and its relevance [G18] in AI.

AI developers were aware of their roles and responsibilities
in implementing ethics during AI system development [G7].
They were aware that they play the most important role in
shaping the ethics that is embedded in an AI system [G23].
Govia [G16] mentioned that a participant was aware of the
philosophical theories of AI ethics. The participant stated
the following:
˚ ”If you’re familiar with various philosophical theories of
ethics, a lot of them involve either satisfying constraints based on
rules, Kantian deontological ethics, or optimising some function,
Utilitarian like Mill or Bentham...now these sorts of optimisation
are actually very important in computer science in general, also
in artiﬁcial intelligence.”– AI specialist from [G16].

Transparency is one of the ethical principles of AI that
is discussed widely by AI developers [G1], [G23] and a
highlighted topic of discussion in academia [G6]. AI de-
velopers were aware of the term ’transparent AI’ and it
was recognised as a goal during AI development [G2],
[G9], [G18]. Mark et al. [G17] mentioned that a participant
was aware of the transparency law which helped them to
determine what data needs to be public and what data
needs to be private during the development of an AI system.
Another participant in that study [G17] was also aware
of transparency in AI and aimed at making transparent
systems. The participant mentioned the following:
˚ ”You might want to make it transparent for all citizens.”– AI
expert from [G17].

AI developers are aware of the term ’fairness’ which is
an ethical principle of AI [G2]. AI developers are aware of
this principle and work towards abolishing fairness issues
in AI systems [G3]. Similarly, AI developers acknowledged
that they are aware of accountability of AI systems and
its importance [G3], [G6], and they felt responsible for the
harms caused by their system [G10]. A study [G8] stated
that ’responsibility’ as an ethical principle achieved the
highest rank in terms of relevance in AI and it affects
other ethical principles of AI. They were also aware that
they possessed sensitive customer data and so they actively
considered accountability in relation to cyber-security and
data management [G5].

Privacy is another ethical principle that AI developers
were aware of and discussed widely. Privacy of data and
information was identiﬁed as one of the major concerns of
organisations [G6], [G27]. One of the participants in a study
[G6] stated that:
˚ ”And one of the ﬁrst questions is privacy; that is, these algo-
rithms that you are presenting, where are they going to be run?

What will their information requirements be?”– AI practitioner
from [G6].

Developers also seemed to be aware of the gap that exists
between ethical principles and practice of implementing AI
ethics. Ibanez et al. [G6] reported that a participant stated
the following:
˚ ”They sent two hundred pages of what it should be today from
the European Union, but then in reality, what can be applied?
What is the reality of companies, and what is practical?”– AI
practitioner from [G6].

4.1.2 Developer Awareness of Human Limitations

Participants in a study [G7] mentioned that they were aware
of their own limitations in implementing ethics in AI. They
reported that sometimes the limitations of their foresight
and intention resulted in the development of a faulty and
unethical AI system. Orr et al. [G7] mentioned that a partic-
ipant stated the following:
˚ ”We are developing systems that are better than human ... only
to discover as time goes on, that maybe they make things worse.
And I don’t think that, that is a cynical thing to say. I think it is
just a reﬂection of how every technology innovation has unfolded
so far. What we need to do, as designers, is be aware that we could
be designing the system that works and changes people’s lives,
or you could be designing the system that makes people’s lives
worse.” – AI practitioner from [G7].

On the other hand, some studies highlighted the lack
of such awareness and assumed ethical behaviour, without
addressing conscious and unconscious biases. For example,
a study on developers’ challenges of addressing ethical
issues of AI presented various challenges that AI developers
face in addressing AI ethical issues [G24]. The study [G24]
reported that AI developers lacked self-reﬂection on being
able to recognise their own biases and responsibility which
hampers AI ethics implementation. A participant in that
study mentioned the following:
˚ ”Most of us think we’re ethical and we operate with a very bad
ethical premise that says I’m a good person and evil is caused by
evil people. I’m not an evil person. So I don’t have to worry about
it. So when I write the algorithm, I’m a good software engineer.
I don’t even have to question this. I’m doing a ﬁne job.” – AI
engineer from [G24].

Another participant in that study mentioned that they
lacked awareness about their cultural norms while making
ethical decisions during AI system development:
˚ ”The cultural norms that we have, but don’t even realise we
have, that we use in order to make decisions about what’s right
and wrong in context. It’s very difﬁcult for any software system,
even a really advanced one, to transcend its current context. It’s
locked into however it was framed, in whatever social norms were
in place amongst the developers at the time it was built.” – AI
engineer from [G24].

Likewise, other participants stated that they did not
always have a diverse and broad perspective to build inclu-
sive AI technologies which affects the ethics implementation
in AI:
˚ ”I’m in a niche market and I do the photo recognition software
and I’m an old white guy. So the only people I recognise are white
males with beards. And that happens in the software, we know it’s
happened and we’ve framed out the ethics.” – AI engineer from
[G24].

CURRENTLY UNDER REVIEW

9

Fig. 4: Visual representation of the theory of ethics in AI using the developer’s prism metaphor

Fig. 5: Theory diagram resulting from theoretical structuring, representing ﬁve key categories and ﬁve hypotheses

Participants from another study [G25] mentioned that
they are aware of their lack of knowledge about ethics
and ethical guidelines to be followed during AI system
development. They also mentioned that they were aware
of the ways to overcome their limitations such as participat-
ing in mandatory employee training programs, attending
ethics-focused design workshops to overcome, and reading
published materials on ethics.

4.1.3 Reasons for Developer Awareness

Using the STGT data analysis procedures, in addition to
the types of awareness, we were also able to identify some
reasons for developer awareness. For example, several par-
ticipants in a study [G3] concluded that organisational pres-
sure is the main reason for awareness of transparency which
led them to make transparent AI systems. Organisational
pressure leads to transparency of the ML system which
is also beneﬁcial for the organisation. A participant of the

study [G3] stated the following:
˚ ”We have better buy-in”. – ML practitioner from [G3], when
the logic of their ML systems is provided to the users.

Several studies concluded that AI developers were
forced by laws and regulations to be aware of AI ethics [G10],
[G5]. For example, the General Data Protection Regulation
(GDPR) [G6], [G27]. Following are the statements made by
the participants about GDPR in a study [G6]:
˚ ”The ﬁrst thing is the GDPR.” – AI practitioner from [G6].
˚ ”We learned from GDPR to have sensitive data in a separate
sensitive database that can only be accessed by several users or
with special permissions.” – AI practitioner from [G6].

Another study [G1] concluded that medical domain has
the most strict regulations which is the reason for medical
AI developers to be aware of AI ethics. On the ﬂip side, the
same regulations seemed to limit their awareness of ethical
issues. A participant in that study [G1] who was involved
in developing a medical AI system stated that,

CURRENTLY UNDER REVIEW

10

˚ ”We have in-house quality measurements and these regulation
requirements are very strict, so therefore these things pretty much
come as a given for us. And of course if you think about it the other
way, we consequently think about these things [ethics] even less
because we already have such clear regulations and requirements
for what we do.” – AI designer from [G1].

Other reasons for AI developers’ awareness of ethics
were also mentioned by participants such as AI developers’
personal interest and experiences that help them in gaining AI
ethical knowledge [G6],
˚ ”The typical ones, I know them but on a personal level and
[based on] curiosity.”– AI practitioner from [G6].

Another participant in a study [G6], who was the Director
of AI and had 14 years of experience in Technology Consulting
Services, stated the following:
˚ ”The White Paper of the European Commission on Artiﬁcial
Intelligence, well, I know it in my case, but it is also a bit because
of my proﬁle and past.” – AI practitioner from [G6].

Customer complaints about AI ethical issues and negative
media coverage of AI systems were other reasons for develop-
ers’ awareness about AI ethics [G2]. A participant in a study
[G2] suggested a reactive approach to awareness:
˚ ”How do you know the unknowns that you’re being unfair
towards? You just have to put your model out there, and then
you’ll know if there’s fairness issues if someone raises hell online.”
– ML practitioner from [G2].

4.2 Developer Perception

The second category is Developer Perception. This category
emerged from four underlying concepts: developer perception
of AI ethics & principles, developer perception of users, developer
perception of data, and developer perception of AI systems. This
category of developer perception is related to the developer
awareness category (as presented in the previous section), in
that awareness was a prerequisite to forming perceptions.
However, the perception category goes beyond acknowl-
edging the existence of something and captures developers’
views and opinions about it, including held notions and
beliefs. For example, it includes shared perceptions about
the relative importance of ethical principles in developing
AI systems, who is considered accountable for applying and
upholding them, and the perceived cost of implementing
ethics in AI.

4.2.1 Developer Perception of AI Ethics & Principles

Perception about the importance of ethics varied. Some AI
developers perceived ’ethics’ as very important in develop-
ing an AI system [G1], [G29], [G20]. A study [G1] reported
that AI developers acknowledged the importance of AI
ethics. In the paper, when participants were asked if ethics
is useful in AI, all (N=6) of them answered ”Yes”.

In contrast, many AI developers do not consider ethics as
the most important element during AI system development.
A study [G7] mentioned that AI developers consider only
speciﬁc ethical principles important whereas another study
[G9] mentioned that AI developers are less concerned about
ethics as a whole in AI and more concerned about the use-
fulness and viability of their products. Ethics was perceived
as a secondary concern [G7] and as other’s problem [G1]. The
study [G10] concluded that AI developers perceive ethics

as a non-functional requirement of AI systems. Yet others
approached it with a minimalist approach. For example, a
participant (AI practitioner) in a study [G7] shared:
˚ ”The very minimum that you have to adhere to is the law. So,
we start by ensuring that everything that we do, or our clients do
is legal. Then we have to decide whether or not it is appropriate,
which could be considered ethical or fair.”– AI practitioner from
[G7].

Developing responsible AI was seen as building posi-
tive relations between organisations and human beings by
minimising inequality, improving well-being, and ensuring
data protection and privacy. However, when it comes to the
relative importance of the ethical principles, it was a divided
house. Some AI developers think that AI systems must be
fair in every way [G11]. Others think that fairness issues
in AI systems must not only be minimised but completely
avoided [G8]. A study [G8] concluded that protection of
data privacy is the second most important principle that
comes after transparency. While other studies – [G6] and
[G10] – concluded that privacy is the most important ethical
principle in AI system development.

There were also differing opinions about who should be
responsible for ethics in AI. For example, a participant in a
study [G30] mentioned that:
˚ ”When you think about who’s accountable for AI that they’re
using in the public sector. When something goes bad, who do you
point the ﬁnger at? If you got the human being out of the loop or
maybe it’s never out of the loop? But how do you decide who bears
the cost in a bad experience?” – AI practitioner from [G30].

Others had strong opinions, such as a participant of a
study [G10], who stated that ethics cannot be outsourced,
suggesting it is ultimately the AI developers’ responsibility.
There was a notion that AI developers are responsible for
maintaining data privacy in AI systems. AI developers
perceived the importance of privacy from the user’s point
of view. A participant in a study [G6] quoted the following:
˚ ”There you have the data of people, their addresses, you even
have precious information, about when they are at home or not,
private data, and making proper use of them is essential.”– AI
practitioner from [G6].

Some others think that accountability issues are different
for different AI systems [G6]. For example, one perception
was that both users and AI developers are responsible
for maintaining the accountability of an AI system [G7],
[G26]. Technical knowledge of AI developers help them in
producing accountability whereas lack of user’s knowledge
about AI algorithms has a negative effect on accountability
of an AI system. A participant in a study [G7] shared the
following:
˚ “Accountability does not come up in any of our client discus-
sions. It does not come up as you would think. It is because they
don’t understand what they don’t understand. How many people
will know in detail how AI algorithms work, and who has actually
practiced it to understand the nuances of an AI algorithm?”– AI
practitioner from [G7].

In another study [G20], participants suggested stricter
mechanisms for enforcing accountability beyond relying on
individuals. They mentioned that AI is a very powerful tech-
nology which needs ethical regulations. Ethical regulations
provide an ethical framework that guides them during the
development of an AI system. A participant in the study

CURRENTLY UNDER REVIEW

11

[G20] mentioned the following:
˚ ”We have regulations on all kinds of technology, and I think
that there’s no special reason for why AI should be less regulated
than anything else.”– AI expert from [G20].

Another interesting opinion shared was to do with the
perceived cost of applying ethics in AI development. For exam-
ple, too much ethical accountability was perceived as having
a negative impact on business and organisational growth. A
participant in a study [G6] stated the following:
˚ ”If I have to be very “ethical”, accuracy will also be affected.
Then I think there is a dilemma there, in the end, of how ethical I
am and how much business I am losing.”– AI practitioner from
[G6].

Likewise, a study by Morley et al. [G29] reported that
improvement of social impact is the beneﬁt of a pro-ethical
AI design as perceived by majority of their participants
but incurring additional costs such as resource costs and
additional time is a perceived disadvantage.

4.2.2 Developer Perception of Users
AI developers have perceptions about users’ nature, technical
abilities, drivers, and their role in the context of ethics in AI.
Participants in a study [G3] perceived that users only like to
communicate if there is any chance of an incident occurring.
Otherwise, they don’t care. Similarly, the participants in a
study [G3] reported on the users’ tendencies to judge an AI
model based on personal factors:
˚ ”People tend to lose faith if their personally preferred risk
indicators aren’t in a model, even without looking at performance
of results.” – ML practitioner from [G3].

Some AI developers perceive that users are not curious
about the working of AI systems because ethical techni-
calities of an AI system are irrelevant to them [G5]. A
participant in one study [G5] stated that:
˚ ”Nobody wants to listen to ethics-related technical stuff. It’s
not relevant to the users.” – AI developer from [G5].

Users’ lack of AI knowledge is one of the reasons that
they have no interest in ethics of AI according to the percep-
tion of a participant in a study [G7]:
˚ ”Accountability doesn’t come up in any of our client discus-
sions. It doesn’t come up as you would think. It is because they
don’t understand what they don’t understand. How many people
will know in detail how AI algorithms work, and who has actually
practiced it to understand the nuances of an AI algorithm?” – AI
practitioner from [G7].

Another study [G6] reported that users are concerned
about ethics in AI and ethical issues only when it impacts
their business. Likewise, a study [G14] reported that users
are skeptical and worried when it comes to the publication
of their personal data in AI systems:
˚ ”So, there will be a lot of scepticism, because everyone is afraid
that personal data will be published. So, everyone has quite a bit
of respect.” – AI expert from [G14].

Participants in a study [G7] discussed the role of users in
ethics in AI. An AI developer stated that it is essential to
get users’ needs and requirements before developing an AI
system as it creates ethical parameters for them. Likewise,
participants in a study [G7] perceived that the growth of
an AI company is based on users. Users are likely to sue a
company if ethical issues of an AI system are not addressed
by the company:

˚ ”[Companies] that aren’t transparent or ethical, eventually, or
you would hope, end up being prosecuted or sued or you know,
all citizens as a whole would choose not to engage with them
because they’ve been identiﬁed as an untrustworthy organisation.
Because, trust becomes the currency on which we trade upon. And
will be more so as AI embeds itself in everything that we do.” –
AI practitioner from [G7].

Similarly, participants in a study [G7] perceived users
as autonomous agents and stated that encounters between
users and AI systems cannot be fully controlled through AI
design. They perceived that users have an equal responsi-
bility as developers for AI outcomes. A participant in that
study stated the following:
˚ ”We were a technology provider, so we didn’t make those
decisions. It is the same as someone who builds guns for a living.
You provide the gun to the guy who shoots it and kills someone in
the army, but you just did your job and you made the tool.” – AI
practitioner from [G7].

This statement is supported by another study [G10]
which concludes that users are accountable for their own
safety.

4.2.3 Developer Perception of Data

AI developers consider data as an important aspect in
implementing ethics in AI [G5], [G6]. A participant in a
study [G5] perceived that data handling is an essential step
that enhances the development of an ethical AI system:
˚ ”It’s really important how you handle any kind of data that
you preserve it correctly, among researchers, and don’t hand it out
to any government actors. I personally can’t see any way to harm
anyone with the data we have though.” – AI developer from [G5].
The developer’s na¨ıve perception of the potential for harm
(or lack thereof) is worth noting in the above example.

Along with that, participants in a study [G2] highlighted
the importance of data collection and curation in an AI
system development. They mentioned that collecting sufﬁ-
cient data from sub-populations and balancing them during
curation of data sets is essential to minimising ethical issues
of an AI system. On the other hand, Stahl et al. [G18]
reported that participants avoided getting personal data of
users or minimised its collection as much as possible as so
that no ethical issues related to data privacy arise during AI
system development.

4.2.4 Developer Perception of AI Systems

AI developers have different perception about AI systems.
The participants in a study [G1] perceived that every AI
system has some ethical issues initially and they take actions
to either avoid or mitigate them. The participants in another
study [G3] perceived physical harm of an AI system as
important and relevant but not any other harm. One of the
participants mentioned the following:
˚ ”What could it affect-the distribution of funds in a region,
or could it result in a school taking useless action. It does have
its own risks, but no one is going to die because of it.” – ML
practitioner from [G3].

AI developers perceive AI as just another feature at least
for considering the ethical side of the things [G4]. Whereas,
participants in a study [G7] thought AI as a socio-technical
system and not just a technical system:
˚ ”There is not really such a thing as an autonomous agent, it

CURRENTLY UNDER REVIEW

12

has kind of become important to say. It is now a socio-technical
system, not just a technical system.” – AI practitioner from [G7].
Orr et al. [G7] commented on the perceived limitations
of AI systems, suggesting that they are so complicated that
sometimes, they are not able to minimise ethical issues
despite trying their best:
˚ ”I can say, yeah OK that was a fault, but this is how we did the
safety analysis. And I can see that this was missed, not because we
were negligent, but just because it is so complicated. In this case,
somebody died, but we did have the right ethical framework. But
sometimes accidents happen. I think that is the kind of argument
that you are going to have to make.” – AI practitioner from [G7].
Participants in another study [G9] also perceived AI
systems as only concepts and prototypes so they did not feel
accountable for the design of an AI system. These kinds of
perceptions of AI developers denote that their AI systems
are as ethical as the humans that create them. Likewise,
studies [G12], [G14] reported that AI systems will always
have some biases as humans create those systems:
˚ ”The machine will always have biases, always being created by
a programmer, and the programmer has prejudices.” – AI expert
from [G12].

4.3 Developer Needs

The review highlighted different needs of AI developers
which can help them enhance ethics implementation in AI
systems. This category was underpinned by concepts such
as AI ethics & principles needs and human needs.

4.3.1 AI Ethics & Principles Needs

Developers in the included primary studies identiﬁed a
number of needs. For example, the need for a universal
ethics deﬁnition was highlighted, as it fulﬁls the gap between
the on-going academic discussion and the industry and
enhances AI ethics implementation [G1], [G6], [G13]. A
participant in a study [G1] mentioned the following:
˚ ”I actually try to use the word ‘ethics’ as little as possible
because it’s the kind of word that everyone understands in their
own way, and so they can feel that it’s not relevant to what we’re
doing at all.” – AI designer from [G1].

Along with that, a participant in a study [G6] stated that
there is a gap between practices and principles which needs to
be fulﬁlled:
˚ ”They sent two hundred pages of what it should be today from
the European Union, but then in reality, what can be applied?
What is the reality of companies, and what is practical?” – AI
practitioner from [G6].

Developers in [G1] and [G6] reported that participants
expressed the needs of tools or methods to translate principles
into practice. A participant in that study stated the following:
˚ ”I think we read them all because they are coming out.
There are many in the “stratosphere”. That is when you read the
principles and say, how do I translate them in practice? It gets
more complicated.”– AI practitioner from [G6].

Another study [G19] concluded that there is a lack of
tools that supports continuous assurance of AI ethics. A
participant in a study [G19] stated that it was challenging
for them as they had to rely on manual practice to manage
ethics principles during AI system development. The partic-
ipant stated the following:

˚ ”We had to go through a lot of data and make sure that there
was not a single frame with a person in it.” – AI scientist from
[G19].

AI developers face communication challenges that affect
ethics implementation in AI systems [G2], [G3], [G15]. Par-
ticipants in studies [G2], [G3] expressed the need for tools to
facilitate communication between AI model developers and
data collectors. All these points conclude that there is a
need of tools that can help AI developers to successfully
implement ethics during AI system development.

While the lack of practical tools are repeatedly identiﬁed,
other participants in a study [G6] had an opposite view on
the gap between principles and practice. They expressed
the need for more principles as they have much practice. A
participant of that study mentioned the following,
˚ ”There is much practice but few principles.” – AI practitioner
from [G6].

4.3.2 Human Needs

There are few needs related to AI developers that affect
ethics implementation in an AI system. A participant in a
study [G2] mentioned that there is a need for effective commu-
nication between AI developers as it supports ethics imple-
mentation. Similarly, participants in a study [G3] reported
that they are in need of external perception and opinions
of external parties on their AI software. It helps them to
know the ethical issues of the software. On the other hand,
participants in a study [G5] reported that they need more dis-
cussion of their responsibilities and AI development to help
them become aware of ethical issues and minimise them.
However, Chivukula et al. [G28] reported that participants
didn’t feel responsible anymore as they were already doing
their jobs ethically. One of the participants mentioned:
˚ ”I’m starting to feel like it’s not the responsibility of us
anymore because I think all of us are already thinking from that
perspective.” – AI practitioner from [G28].

4.4 Developer Challenges

The third key category is Developer Challenges. The review
shows that AI developers face several challenges while
implementing ethics during AI system development which
are related to four underlying concepts of ethics implementa-
tion challenges, user challenges, AI system challenges, and data
challenges.

4.4.1 Ethics Implementation Challenges

A number of challenges related to implementing AI ethics
were reported, including knowledge gaps, gaps between
principles and practice, ethical trade-offs including business
value considerations, and challenges to do with implement-
ing speciﬁc ethical principles such as transparency, privacy,
and accountability.

Participants in a study [G1] reported that they have dif-
ﬁculty in conceptualising ethics, i.e., it is challenging for them
to talk about ethics because the term ’ethics’ is understood
differently by different people. A participant in a study [G1]
stated that,
˚ ”I actually try to use the word ‘ethics’ as little as possible
because it’s the kind of word that everyone understands in their

CURRENTLY UNDER REVIEW

13

own way, and so they can feel that it’s not relevant to what we’re
doing at all.” – AI designer from [G1].

Likewise, Govia [G16] mentioned that it was difﬁcult for
the participants to talk about ethics because they did not
have a clear concept about ethics in AI. A participant in that
study stated the following:
˚ “It’s hard for me to talk about ethics because I don’t really
understand it that well to be quite honest with you; and that’s
probably the same for a lot of computer scientists, artiﬁcial
intelligence researchers — that we’re not too clear on what ethics
is.” – AI specialist from [G16].

Another participant acknowledged the need to possess

relevant background in ethics:
˚ ”I don’t know if I am qualiﬁed yet to really make professional
thoughts about it. I don’t have an ethics background. I have a
computer science background which maybe gives me insight into
some areas of it, but certainly does not give me the full picture.”
– AI specialist from [G16].

Similarly, a participant in a study [G18] reported that
they were technology experts but did not have any knowl-
edge and background of ethics. However, they mentioned
that they were extremely aware of privacy concerns in
AI use, highlighting the interesting relationship between
developer awareness, perception, and challenges.

Some AI developers are concerned about the lack of
discussion of implementing AI ethics in the industry. Dif-
ferent types of challenges are mentioned and solutions are
discussed in theory but there is no demonstration of those
solutions in practice [G1], [G3]. Translation of AI principles
into practice is a challenge for AI developers:
˚ ”I think we read them all because they are coming out.
There are many in the “stratosphere”. That is when you read the
principles and say, how do I translate them in practice? It gets
more complicated.” – AI practitioner from [G6].

AI developers are also challenged over making ethical

choices during the design of an AI system:
˚ ”Quite often we will make trade-offs naively and in line with
our own experiences and expectations and fail to understand the
implications of those trade-offs for others. We can assess all of the
trade-offs, but we still don’t weigh them in impartial ways.” – AI
practitioner from [G7].

Likewise, AI developers are challenged to implement
ethics in AI as there is a lack of tools or methods for
implementing ethics. For example, in a study [G1], when AI
developers were asked, ”Do your AI development practices take
into account ethics, and if yes, how?”, all respondents (N=6)
answered ”No”. This indicated that AI companies lack clear
tools and methods for implementing ethics in AI.

A number of challenges were mentioned to do with im-
plementing speciﬁc ethical principles such as transparency,
privacy, and accountability. For example, although trans-
parency is perceived as an important ethical principle of
AI, developers have challenges in maintaining transparency.
These challenges arise both in the sense of transparency
of systems and the development process [G1]. Providing
transparency to customers is challenging [G6]:
˚ ”There’s generally little transparency everywhere because it
is hard to make that transparent to the customer I think it is
still challenging to give that security and transparency.”– AI
practitioner from [G6].

A study [G3] highlighted that communicating the perfor-
mance of designed AI systems is challenging sometimes due
to cost and business value considerations, which hampers
transparency of an AI system. Cost is one of the major
challenges in maintaining transparency in AI [G26]:
˚ ”Releasing source code of AI to maintain transparency does
not happen often because it costs money to do, you have to spend
time to clean it up, to maintain it, to publish it and so on. Second,
you decrease the commercial value of it usually.” – AI scientist
from [G26].

Similarly, developers were found to be challenged to

maintain accountability of an AI system:
˚ ”How to clarify responsibilities and what are the standards
or regulations? A machine cannot take responsibility by itself,
as a human being can.” – AI developer from [G22]. At the
same time, some of them felt they did not have control
over the way an AI technology is used [G26]. Dividing
responsibilities among the teams was seen as one of the
major challenges of AI developers [G27].

4.4.2 User Challenges

AI developers face challenges related to users and clients
during the implementation of ethics in AI systems. Partici-
pants in a study [G1] mentioned that lack of interaction with
end users causes uncertainty about user problems:
˚ ”So that’s of course one question: how to make it clear for them
that there are some uncertainties there so that they don’t expect
the information to always be perfect. But I don’t really know how
much of a problem this is - I haven’t really spoken to our end
users.” – AI designer from [G1].

An AI practitioner in a study [G7] reported that an
imbalance is developed between AI developers and users
because the priorities of their work are set by senior mem-
bers of the company:
˚ ”Senior executives don’t understand machine learning models
that their data scientists are producing. Here are the parameters
and here is what is actually, here is what matters. You have told me
to maximise proﬁts so, it really just comes down to [maximising
proﬁt].” – AI practitioner from [G7].

4.4.3 AI System Challenges

AI systems create challenges for AI developers while im-
plementing ethics. Unpredictability of an AI system is a
major challenge for AI developers [G1], [G4], [G7] and they
take actions to avoid, mitigate or prevent unpredictable
behaviors that take place [G1]. A participant in a study [G1]
stated that,
˚ ”An example of such an action can be ML management by
means of using different sets of training data or limiting its
utilisation. ”...we have even cut some functionalities of the system
in order to make it more predictable, which has reduced the amount
of unexplained results we have gotten out of it in practice we’ve
been able to explain all of the faulty results so far.” – AI designer
from [G1].

Similar thought was shared by a participant in another

study [G24] who mentioned that,
˚ ”In terms of unpredictability, there is a lack of ‘work looking at
scenarios of unintended consequences’ precisely because ‘we don’t
know the unintended consequences of the decision-making of the
machine.” – AI engineer from [G24].

CURRENTLY UNDER REVIEW

14

There are external causes of AI system unpredictabil-
ity such as cyber-security threats [G1]. Likewise, client’s
needs such as proﬁt maximisation and attention optimisa-
tion cause unpredictable system behaviour that ultimately
develops ethical issues. A participant in the study [G7]
stated the following:
˚ ”It’s not that we thought what we were doing was safe, it’s just
that, certain inbuilt desires to increase clicks, to increase attention,
to maximise advertising was our primary motivation. You did not
have to think about any other consequences.” – AI practitioner
from [G7].

Not all organisations and their AI developers have
fallback plans for solving ethical issues developed by an
unpredictable AI system [G4]. Therefore, it can be concluded
that it is challenging for AI developers to solve ethical issues
that are developed by unpredictable AI system behaviours.
AI developers are using some strategies to avoid such
unpredictability issues of an AI system. However, there is
a need for methods and tools which can mitigate or prevent
unpredictable behaviour of an AI system. Such methods or
tools will help in minimising AI ethical risks.

4.4.4 Data Challenges

Some of the challenges shared by participants across the
primary studies were related to data. For example, the
quality of dataset used in AI algorithms is considered one
of the main factors affecting fairness of an AI system.
Some AI developers mentioned that it is challenging for
them to collect quality datasets as they are not given full
control over the data collection process [G2]. Lack of proper
engagement of users with the product also hampers the data
collection process. Participants in that study also mentioned
that challenges to get additional training dataset to ensure
AI fairness arise due to the team’s blind spots. Holstein et
al. [G2] reported that the participants recalled cases where
users complained about a globally deployed AI system that
recognised popular celebrities of some countries and did
not recognise others. One of the participants [G2] quoted
the following:
˚ ”It sounds easy to just say like, ‘Oh, just add some more
images in there,’ but there’s no person on the team that actually
knows what all of [these celebrities] look like, for real if I noticed
that there’s some celebrity from Taiwan that does not have enough
images in there, I actually don’t know what they look like to go
and ﬁx that. But Beyonc´e, I know what she looks like.” – ML
practitioner from [G2].

On the other hand, in some cases data privacy issues
were seen to induce risk aversion and impose barriers to
better data usage:
˚ ”My perception is that companies do take great care of their
information, to the point that they often prefer not to generate
value from information [rather] than to expose their information
to a risk of leakage.” – AI practitioner from [G6].

4.5 Developer Approach

The review of empirical studies provided insights into the
approaches used by AI developers to implement ethics dur-
ing AI system development. This category is underpinned
by two key concepts, applied strategies and possible strategies,
to enhance ethics implementation in AI. Applied strategies

refer to the techniques or ways that AI developers reported
using to enhance the ethics implementation in AI, whereas
possible strategies are the recommendations or potential
solutions discussed by AI developers to enhance the ethics
implementation in AI.

4.5.1 Applied Strategies

AI developers use different strategies to enhance ethics im-
plementation in AI. Participants in a study [G1] mentioned
that organisations implemented proactive strategies to en-
hance ethics implementation in AI development. One of
the strategies used was speculating socio-ethical impacts by AI
developers before developing an AI system. This helped AI
developers become aware of AI ethics and ethical issues that
may arise in future and have a plan if the issues arise [G5].
Similarly, analysing a hypothetical situation of unpredictability
was a strategy used to solve unpredictable behaviour of an
AI system [G1].

Likewise, group discussion with fellow colleagues was
another strategy used by AI developers to be aware of
issues [G9]. Some AI
AI ethics and address AI ethical
developers reported merging ethical and legal considerations
to ensure no illegal actions have been taken during AI
system development [G7]. In this strategy, ethics remained
a secondary concern. A participant in a study [G7] stated:
˚ ”The very minimum that you have to adhere to is the law. So,
we start by ensuring that everything that we do, or our clients do
is legal. Then we have to decide whether or not it is appropriate,
which could be considered ethical or fair.” – AI practitioner from
[G7].

Participants in a study [G18] reported several strategies
that they used to enhance ethics implementation in AI such
as responsible data science, stakeholder engagement, ethics review
boards, and following codes of ethics and standards of practice.
AI developers were also involved in setting customised reg-
ulations in the company and played an essential role in the
development of AI ethics. This strategy was used to enhance
ethics implementation by developing comprehensive and
well-deﬁned guidelines of AI ethics for the company [G7].
Similarly, a participant in a study [G17] mentioned that they
had an interaction and collaborative discussion with policy
makers and legal teams of the company to ensure that their
algorithms were abiding by the legislation. This denotes that
AI companies focused on ensuring their algorithms were
legally ﬁt before deployment.

AI developers discussed some strategies and methods
that they used to maintain transparency and accountability
of AI systems. Code documentation was the primary proactive
strategy of creating transparency during development of an
AI system and to track the actions and people involved
[G15]. Conducting audits was the other important strategy
used by AI developers to solve transparency issues [G1],
[G5]. RESOLVEDD, which stands for R: Review; E: Estimate;
S: Solutions; O: Outcomes; L: Likely; V: Values; E: Evaluate;
D: Decide; D: Defend, was a proactive strategy used by AI
developers to increase transparency and ensure that the eth-
ical considerations of the team members were documented.
Similarly, documenting decisions made by AI developers in
order to track decisions back to individuals when needed
was one of the strategies used to enhance accountability
[G10].

CURRENTLY UNDER REVIEW

15

Collecting as much training data as possible was one of
the proactive strategies of AI developers to minimise biases
in the system [G15]. Speculating possible fairness issues in
an AI system before deploying it was the other strategy
used to minimise fairness issues [G2] in AI. However, some
companies did not use proactive strategies to maintain
transparency of AI systems but addressed transparency
issues only when it impacted their business [G6]. Some AI
developers just followed what is legal and shifted the ethical
responsibilities to policy makers and legislative authorities
[G7]. At the same time, some AI developers used the reac-
tive strategy of apologising to customers if any unpredictable
ethical issue arises [G1].

4.5.2 Potential Strategies

In addition to sharing experiences of tried and tested strate-
gies, developers also discussed potential strategies that
they thought could improve ethics in AI. A study [G10]
concluded that appointing one individual to implement
ethics during AI development is not a good option. The
whole AI development team must be involved in the process
of ethics implementation. Educating AI developers about AI
ethics and including diverse people in the development team
help AI developers to implement ethics in the system and
build ethical AI [G21]. Having internal governance such as
ethics committees in an organisation to establish AI ethical
standards and employing AI auditors help AI developers were
other possible strategies that can provide AI developers an
opportunity to work closely with ethicists so that they can
verify if ethics is being implemented appropriately during
AI system development [G21]. On the other hand, a partic-
ipant in a study [G28] stated that educating business owners
with ethics training and education instead of them because
they focus on their business growth rather than ethics in AI.
The participant said:
˚ ”More education for business owners and people in other
parts of businesses to be a responsible business owner. Don’t push
these agendas. You think making more money quickly is the most
important part of your business.” – AI practitioner from [G28].
Similarly, a participant mentioned that organisations
should work on treating ethics properly by having a cultural
shift in the organisation:
˚ ”It’s not just a designer process or designer inﬂuence at this
point of time, but it’s a cultural shift that has to happen in the
organisation on how they treat ethics.” – AI practitioner from
[G28].

AI developers provided some potential strategies on
enhancing the transparency and fairness of AI software.
Recognising transparency as a goal is not sufﬁcient, pur-
suing it formally is important [G5]. Likewise, a participant
in a study [G10] mentioned that tackling ethical issues
during design and development of an AI system to enhance
system transparency is good. Using explanatory mechanisms
during AI design and development phases is recommended
to enhance system transparency [G6]. Hiring employees
who belong to different communities and ethnic groups
can enhance the chance of spotting biases within a team.
Others suggested including fairness-focused quizzes in the
interview processes can be useful to hire people who can
detect fairness issues in an AI system [G2]:
”No one person on the team [has expertise] in all types of bias

especially when you take into account different cultures. It would
be helpful to somehow pool knowledge of potential fairness issues
in speciﬁc application domains across teams with different back-
grounds, who have complementary knowledge and blind spots.” –
[G2].

5 HYPOTHESES AND THEORY

A grounded theory is not limited to a set of descriptive
categories – it also explains the key relationships between
those categories, such as through a set of interrelated hy-
potheses [10], [37]. We have presented the ﬁve key categories
– Developer Awareness, Developer Perception, Developer Needs,
Developer Challenges and Developer Approach – in Section
4, that emerged through the data analysis of 30 primary
studies. In this section, we present the relationships between
those categories as ﬁve interrelated hypotheses. We also
analyse how changes in one category impacts the changes
in another category. By analysing the relationships between
categories facilitated by memoing, we derived the grounded
theory of ethics in AI through the developer’s prism. Figure
5 presents the theory diagram resulting from theoretical
structuring, with key categories and hypotheses (H1-H5).

(cid:144)1: Developer Awareness is an antecedent to, but does

not guarantee positive, Developer Perception.
Awareness and perception are related yet distinct. AI de-
velopers’ awareness is an antecedent to their perception but
awareness does not guarantee improved perception of the
importance of AI ethics. In fact, awareness about AI ethics
was seen to lead to varying perception about it. For example,
a participant in a study was aware of the term ‘AI ethics’ and
speciﬁc principles of AI ethics [G1] and perceived ethics as
a very important aspect in AI. On the other hand, another
participant of that study was also aware of the term ‘AI
ethics’ but that participant considered ‘ethics’ as a secondary
concern in AI [G1].

(cid:144)2: Developer Awareness can lead to identifying De-

veloper Needs.
AI developers’ awareness about ethics can lead to a recog-
nition of their needs. For example, their awareness about
human limitations in the context of implementing AI may
lead to them acknowledging the need to overcome those
limitations. Similarly, awareness of the lack of foresight,
broad perspective and self-reﬂection [G7] on the one hand
can be seen to be addressed by recognising the need for
effective communication to discuss AI ethics and ethical
issues [G2] and the need to own responsibility of ethics
implementation [G5].

(cid:144)3: Developer Challenges lead to identifying Developer

Needs.
AI developers’ challenges lead to identifying their needs.
For example, a participant in a study [G3] mentioned about
the challenge they face during ethics implementation in
AI due to lack of communication between AI developers
and data collectors. Therefore, a need of tools to facilitate
effective communication between AI developers and data
collectors was highlighted in the study [G3]. Similarly, some
AI developers shared challenges in implementing ethics in
AI due to lack of tools to help them during ethics imple-
mentation [G3], [G9]. At the same time, the need for tool
support for ethics implementation in AI was highlighted

CURRENTLY UNDER REVIEW

16

by participants in studies [G9], [G1] to overcome these
challenges.

(cid:144)4: Developer Perception can generate Developer Ap-

proach.
AI developers’ perceptions can lead to generating ap-
proaches, such as possible strategies, to enhance the process.
For example, a participant in a study [G4] perceived that
ethics must be implemented systematically in AI. Another
study reported that developing systematic implementation
guidelines can be a possible strategy to enhance AI ethics
implementation [G21]. Likewise, another participant in a
study [G4] perceived that ethics must not be outsourced
and developers developing the AI system must implement
ethics. This perception about the ethics implementation
may generate a possible strategy such as providing ethics
implementation training to AI developers so that they do
not have to outsource the ethics implementation to the third
party [G21].

(cid:144)5: Developer Challenges can be overcome by Devel-

oper Approach.
AI developers’ challenges can be overcome by their ap-
proaches. For example,
lack of communication between
colleagues was one of the challenges mentioned by a par-
ticipant in a study [G3]. Practicing group discussion with
colleagues and other members of a company to discuss AI
ethics and ethical issues was one of the approaches used by
a participant in a study [G1]. Lack of knowledge about AI
ethics was one of the challenges identiﬁed by a participant in
a study [G16] which can be overcome by an approach such
as setting own ethical regulations by a company to make
the employees aware and knowledgeable about AI ethics,
shared in [G1].

Theory of Ethics in AI through the Developer’s Prism:
Taken together, the key categories and hypotheses form
the theory which explains that developers’ awareness
of AI ethics directly leads to their perception about AI
ethics and its implementation as well as to identifying
their needs, and indirectly leads to identifying their
challenges and coming up with approaches (applied and
potential strategies) to overcome them. As with most
grounded theories, our theory is meant to be modiﬁable
with future empirical evidence as the landscape of
ethics in AI changes over time.

6 DISCUSSION AND RECOMMENDATIONS

Our ﬁndings contribute to the academic discussion by ex-
ploring the studies that have included the views and under-
standing of AI developers about ethics. As we conducted an
ST-GTLR and used the STGT approach for data analysis,
we got an opportunity to rigorously review the primary
empirical studies relevant to our research question and
develop a multi-faceted theory. We now discuss some of the
insights captured through memoing and team discussions,
accompanied by recommendations.

› Ethics in AI – whose problem is it anyway? Par-
ticipants of the primary studies had a different perception
of AI ethics and its implementation. Most studies included

in our research concluded that AI developers perceived
ethics as an essential aspect in AI [G5], [G20]. However,
some participants of the primary studies had an opposite
perception about ethics in AI. A participant of a study [G1]
stated that discussion on AI ethics does not affect most peo-
ple, except for AI ethics discussions in massive companies
like Google. Likewise, we found that the participants of a
study [G4] perceived ethics as a non-functional requirement
in AI, and it was implemented externally [G23]. However,
a participant in a study [G4] stated that ethics could not
be ”outsourced”, and it should be implemented by AI
developers who are developing the software. The diverse
perspective of the participants about the implementation
of ethics in AI serves to highlight the complex nature of
the topic and why organisations struggle to implement AI
ethics.

Likewise, there were also different views on the account-
ability for ethics in AI. An AI practitioner in a study [G30]
mentioned the uncertainty in deciding responsible people
when some ethical issues arise in AI systems. We perceive
that organisations decide who to be held accountable for
ethics in AI, and it varies from organisation to organisation.
For example, ACM Code of Ethics clearly puts the respon-
sibility on professionals who develop these systems. On
the other hand, AI developers perceive that physical harm
caused by AI systems is essential and needs to be considered
rather than any other harms [G3]. This statement is alarming
as it hints that developers are not worried about ethical
harms in an AI system and prioritise only physical harms.

Recommendations for Practice

(cid:6) Given the diverse perspectives on who owns ac-
countability of considering ethics in AI systems de-
velopment and potential ethical issues arising from
AI system use, it is important for AI development
teams, that are usually multidisciplinary in nature,
managers, and organisations at large to have open
discussions about such issues at their workplace
[G5]. For example, this can be done through organis-
ing discussion panels, guest seminars by ethics and
ethical AI experts, and hosting open online forums
for employees to discuss such topics. Another ap-
proach is to collate the challenges speciﬁc to the
organisation and see how they map to selected ethi-
cal frameworks, as was conducted at the Australia’s
national scientiﬁc research agency (CSIRO) [G26].
(cid:6) Developer discussions can be followed by strategic
and organised attempts to reconcile perspectives,
e.g. teams collaboratively selecting an existing or
creating a bespoke ethical framework, and drafting
practical approaches to implement them in their spe-
ciﬁc project contexts [G7], many of which may be
application domain speciﬁc.

(cid:6) Following hypothesis H1 (developer awareness is an
antecedent to, but does not guarantee positive, developer
perception.), improved awareness may not equate to
more positive perception about the importance of
ethics in AI. This has implications for the efforts
made in industry and research to improve the aware-
ness of ethics in AI. Such efforts need to be carefully
crafted to cater not only to those who are likely

CURRENTLY UNDER REVIEW

17

to go away with positive perceptions but to also
address the scepticism, concerns, and questions of
those who may harbour negative perceptions as a
result of increased awareness. In other words, aware-
ness programs need to focus on building positive
perceptions by addressing developer challenges and
needs.

(cid:6) We recommend proactive awareness as evidenced in
our review, such as driven by personal interest and
experiences [G6], organisational needs [G3], and reg-
ulations such as the General Data Protection Regula-
tion (GDPR) [G6]. Whereas reactive awareness, driven
by customer complaints about AI ethical issues and
negative media coverage [G2], is not desirable.
(cid:6) Similarly, we recommend proactive strategies such as
speculating socio-ethical impacts by AI developers prior
to developing an AI system [G5], analysing hypothet-
ical situation of unpredictability to solve unpredictable
behaviour of an AI system [G1], following codes of
ethics and standards of practice [G18], including diverse
people in the development team [G21], and having
internal governance such as ethics committees in an
organisation to establish AI ethical standards [G21].
(cid:6) Finally, there is also a need to consider accountability
at the organisation and industry level. For example,
Ibanez et al. [G6] reported that there is a need for
ethical governance that can help them solve account-
ability issues.

Recommendations for Education

(cid:6) Educators should include the topic of ethics of AI
software development in the curriculum to educate
future AI developers about ethical principles, guide-
lines, and practical aspects as presented in this paper.
They can give students the opportunities to express
their perceptions about the topic through group dis-
cussions and debates.

(cid:6) Students should be explicitly trained about the ac-
countability of considering ethics in AI system de-
velopment. For example, through being assigned
projects to develop AI systems where they are given
the opportunity to decide the accountability of the
ethical issues arising in the system they develop.

› Ethics-critical domains lead the way. Comparisons
were made between the medical ﬁeld and the IT ﬁeld in
terms of the awareness of ethical regulations in AI [G5].
Participants mentioned that developers developing AI used
in the medical ﬁeld are more aware of ethics because the
medical ﬁeld has more strict laws and regulations than IT.
This hints that awareness of AI ethics depends on domain
speciﬁcity. Domains such as medical and health are more
ethics-aware than others and lead the way in ethics aware-
ness and implementation.

Recommendations for Practice

(cid:6) The IT domain can learn from the advances in im-
proving the awareness of and implementing ethics
in the medical domain. This includes digital, virtual,
mobile, and tele-health areas, as well as AI systems
developed in other domains.

(cid:6) Labelling certain domains as safety-critical and
equating that with ethics-critical, can be a ﬂawed
argument
leading to perceptions that domains
such
traditionally considered non-safety-critical,
as gaming and social media, can be held to lower
standards and expectations when it comes to ethics
implementation. We know from multiple cases of
cyber-bullying and intelligent games encouraging
self-harm in young adults that this would be a
mistake. We recommend that all domains should
aim to be ethics-critical domains.

› Research can help in fundamental and practical ways.
We found that AI developers have different perceptions
about AI systems that affect the implementation of ethics.
A participant of a study perceived the AI system as a
socio-technical system and not just a technical system. So,
the participant considered ethics a vital aspect of AI [G4].
However, a participant in another study perceived AI as
a highly complex system. The participant mentioned that
sometimes, they could not solve the ethical issues that arise
in an AI system due to its complexity. It compels them to
avoid ethics and give some excuses if required to defend
the issues arising in the system [G7]. The participants’
perspectives on AI systems indicate that the implementation
of ethics depends on how developers perceive AI ethics.

Recommendations for Research
Based on our review ﬁndings and insights, we recom-
mend research, including empirical, review, and solutions
and tools development, into the following topics.

(cid:6) Unpacking fundamental schools of ethics (or meta-
ethics theories) such as Utilitarianism, Divine Law,
and how they impact developers perspectives and
approaches.

(cid:6) Investigating each of the facets of our theory –
developer awareness, perception, challenges, needs, and
approach – individually and in depth, as much re-
mains to be understood about how these manifest
in practice.

(cid:6) Investigating speciﬁc ethical AI principles, such as –
human-central values, human societal and environ-
mental well-being, reliability and safety, contestabil-
ity – individually and in depth.

(cid:6) Developing frameworks and recommender systems
for appropriate ethical AI principles selection and
application, suited to the unique facets of the appli-
cation domain (e.g. health, cybersecurity, entertain-
ment) and AI system.

(cid:6) Reviewing tools available to AI developers to imple-
ment ethics in AI systems, including their evaluation
and feedback for improvement.

(cid:6) Designing solutions in terms of tools and guidelines
to address the developer challenges, working in close
collaboration with the developers and managers.
(cid:6) Investigating and explaining the users’ view of ethics
in AI, for example, through a similar ST-GTLR ap-
proach as applied in this review to address the de-
velopers’ view.

(cid:6) Understanding the interplay between the role of
developers and users in implementing ethics in the

CURRENTLY UNDER REVIEW

18

development and use of AI systems, including hu-
man limitations, biases, and strengths.

(cid:6) Understanding the role of rules, regulation, legisla-
tion, and laws in enforcing ethical AI principles and
guidelines, including enablers and barriers, to guide
policy development.

(cid:6) Better understanding the reasons for developer aware-
ness such as laws and regulations, GDPR, personal ex-
periences, as developer awareness directly or indirectly
leads to other categories like developer perception,
needs, challenges and approach of ethics and its
implementation in AI.

7 REFLECTIONS
7.1 Selecting a Review Method

A systematic review is deﬁned as “a means of evaluating
and interpreting all available research relevant to a particular
research question, topic area, or phenomenon of interest” [38].
SLR is a popular review method in software engineering
[39]. Kitchenham derived the SLR guidelines from three
guidelines in the medical domain, adapting them to the
speciﬁc context of software engineering research. A key
feature of an SLR is its focus on comprehensive and fair
reviewing achieved through systematic application of a
predeﬁned search strategy to report research that does and
does not support the preferred research hypothesis. The
main reasons for performing systematic reviews include
summarising existing literature, identifying gaps in litera-
ture, and providing a framework to position new research
[38]. The SLR process supports a top-down, carefully pre-
designed, sequential, and speciﬁcation driven approach to
reviewing.

Initially, we wanted to conduct an SLR to analyse and
synthesize existing primary studies on ethics in AI as
viewed by AI developers. However, the emerging nature
of the research area and niche topic meant we obtained a
minimal number of studies that focused on the AI develop-
ers’ views of ethics in AI. On the other hand, if we went
broad with the search terms, e.g., combinations of (”ethics”
OR ”trust” OR ”morals” OR ”fairness” OR ”responsib*”) AND
(”artiﬁcial intelligence” OR ”AI” OR ”machine learning”) AND
(”software developer” OR ”software practitioner” OR ”data sci-
entist” OR ”machine learning” OR ”software engineer” OR
”programmer”), it led to too many papers to ﬁlter through
manually with very low return on investment in terms of
relevant papers. An SLR did not appear to be the right ﬁt
for purpose. Nevertheless, we were convinced that this new
and emerging topic was worth investing deeper into. We
then decided to explore Grounded Theory Literature Re-
view (GTLR) because of its iterative approach that provides
ﬂexibility in identifying related works from within the same
area and allows to reﬁne the review protocols to accom-
modate related works from other areas or other credible
sources of information. We started by attempting to apply
the ﬁve GTLR steps proposed by Wolfswinkel et al. [11].
However, we had to work our way through the concrete
application details ourselves, and ended up adapting it for
implementation in our socio-technical research context.

Table 3 summarises the differences between an SLR
and an ST-GTLR. Researchers interested in understanding

a socio-technical research topic that is niche and emerging,
and deriving evidence-based theoretical models, and poten-
tially theories, from the ﬁndings along with recommenda-
tions for practice, are recommended to apply the ST-GTLR
guidelines presented in this paper.

7.2 Implementing an ST-GTLR

While the original GTLR framework proposed by Wolf-
swinkel et al. [11] was a good place to start, and sample
studies in other domains existed (e.g. [12], [13], [16]), many
aspects of how to apply the high-level ﬁve step framework
were still fuzzy. For example, it was not clear how the itera-
tions were meant to work. Some studies were seen to apply
a sequential approach [17], [13] while others attempted
some iterations [18]. Similarly, while Wolfswinkel et al.
noted an application of the traditional Strauss-Corbinian
GT and claimed “Grounded Theory aids in building theory
when performing a literature review: by focusing on phenom-
ena through a rigorous concept-centric approach” [40], there
was little concrete guidance available on how to develop
theory from literature as a data source. Additionally, the
analysis of the raw data – in this case derived from the
empirical evidence presented in primary studies – needed
a socio-technical treatment because of its socio-technical
research context, described in Section 3.5. For these rea-
sons, we turned to the concrete steps and procedures of
socio-technical grounded theory for basic data analysis and
advanced theory development [10]. We found ourselves
working through the details guided by the team’s collective
experience of conducting literature reviews (both systematic
and informal) and grounded theory studies and the second
author’s STGT expertise [10].

The original STGT guidelines [10] provide guidance on
conducting primary empirical research studies in socio-
technical contexts, where the research team both collects the
empirical evidence from the source and analyses it, to: (a)
derive descriptive ﬁndings through a limited application of
STGT for data analysis, or (b) derive mature theories through
a full STGT study. STGT studies do not work with literature
as a data source. The role of literature in an STGT study is
captured under the practices of lean and targeted literature
reviews, conducted in the basic and advanced stages of the
study respectively.

In implementing an ST-GTLR, we have extended the
GTLR framework with the use of STGT analysis steps and
procedures to a conduct secondary review study, where the
empirical evidence was originally collected by the research
teams behind the included primary studies, which in turn
were selected and analysed by the review team. Combining
the original GTLR ﬁve step framework with the concrete
STGT analysis steps allowed us to implement an iterative
yet rigorous literature review.

This process was challenging in many ways. The lack of
concrete guidelines to conduct a GTLR study motivated us
to develop and apply the ST-GTLR guidelines. The main
challenge in this process was ﬁnding articles relevant to
our research topic. Due to this, we performed forward
and backward snowballing of the seed articles. This step
was challenging as we had to go through every study in
depth. Besides, we performed open and targeted coding

CURRENTLY UNDER REVIEW

19

TABLE 3: Comparison of Systematic Literature Review (SLR) [38] and our Socio-Technical Grounded Theory Literature
Review (ST-GTLR)a

Systematic Literature Review (SLR)

Deﬁnition

Systematic review to present comprehensive ﬁnd-
ings on well researched topics.

Context of use

Comprehensive coverage of well researched topics
to establish the state-of-the-art.

Socio-Technical Grounded Theory Literature Re-
view (ST-GTLR)

Rigorous review to present multi-dimensional ﬁnd-
ings and develop theoretical foundations for niche
and emerging topics.
In-depth coverage to establish theoretical founda-
tions and periodic sense of the lay of the land,
specially to establish an early sense of where the
ﬁeld is headed for niche and emerging topics.

Approach

Top-down/deductive, mostly sequential, and spec-
iﬁcation driven.

Bottom-up/inductive, iterative, and responsive.

Steps

Phase 1. Planning the review
1. Identify need for review
2. Develop review protocol
Phase 2. Conducting the review
1. Identiﬁcation of research
2. Selection of primary studies
3. Study quality assessment
4. Data extraction and monitoring
5. Data synthesis (e.g. meta and thematic analysis)
Phase 3. Reporting the review

Iterative steps of:
(cid:23) Deﬁne/reﬁne RQ(s) and protocolb
(cid:23) Conduct Search
(cid:23) Select articles
(cid:23) Conduct STGT data analysis
(cid:23) (optional) Develop theory or theoretical models
(cid:23) Present ﬁndings

Outcomes

Full coverage ﬁndings and meta ﬁndings.

Advantages

Repeatable, reproducible, breadth of coverage.

Limitations

Considerable effort involved, possibility of biases,
requires established topics, can be monotonous.

In-depth descriptive ﬁndings, theoretical models,
and theories.
Credible, rigorous, theoretical, depth of ﬁndings,
can work with established and new topics.
Considerable effort involved, possibility of biases,
difﬁcult to replicate outcomes.

abased on Wolfswinkel et al. ﬁve step review process [11] and Hoda’s STGT [10]. bprotocol includes in-built quality criteria

manually in the Analyse stage. Reading each sentence of
the Findings section and developing codes and concepts
was time-consuming and tedious. We had to take multiple
breaks and refresh our minds while doing data analysis.
Our advice would be the same for those who perform the
ST-GTLR as this is a rigorous review, unlike other review
methods. Also, validating codes and concepts with team
members is recommended to achieve better results during
data analysis. Another challenge was to develop concepts
and categories from the codes, as we got hundreds of codes
from data analysis. Grouping similar codes to develop con-
cepts and similar concepts into categories can be complex
process. We recommend using software like Nvivo for this
process to save time and energy. Overall, our experience
of developing these ST-GTLR guidelines and its application
allowed us to explore a new option in the space of con-
ducting literature reviews, one especially suited to niche
and emerging research areas where applying traditional
systematic review approaches are particularly challenging.
Using a pragmatic approach, this paper presents the ST-
GTLR guidelines through sharing the concrete details of its
implementation in a real project as well as presenting the
ﬁndings of our ST-GTLR on ethics in AI so researchers can
beneﬁt from both – the guidelines for method application
and an example outcome.

8 EVALUATION

The application of STGT is evaluated using the criteria of
credibility and rigour, which we deﬁne and demonstrate
in the context of conducting a review below. In case of
literature reviews applying STGT, these criteria still apply,
but the indicative questions underlying them are adapted as
follows:

– How were the primary studies selected? We selected
the primary studies using the seed review protocol,
described in Section 3 and summarised in Table 2.
– How were the iterations applied? Our initial itera-
tion included 3 papers which we used to pilot our
STGT data analysis, followed by another 7 papers.
Together, these form the basic stage of the STGT data
analysis. The advanced stage of STGT’s theory devel-
opment involved the identiﬁcation and analysis of
the remaining 20 papers through iterations of read-
ing the title, abstract, and full text for relevance and
inclusion/exclusion, and performing snowballing on
them.

– How were memos written and used? Memos were
written throughout and used to drive theoretical
sampling and theory development, as explained in
Section 3 and Figure 3.

For manuscripts presenting mature theories, additional in-
formation should be provided.

CURRENTLY UNDER REVIEW

20

– How was theoretical sampling applied? We applied
theoretical sampling through forward and backward
snowballing.

– How were the review protocols reﬁned through
the iterations? The search string was trialled until
a ﬁnal string was decided upon (Table 1). Initially,
we deﬁned the search period to limit it to papers
published between January 2010 and June 2021 to
make the review manageable (see seed protocol in
Table 2). Later, to retrieve more papers, we revised
the seed review protocol to include papers up to
December 2021, relaxed the inclusion criteria, and
enabled snowballing (see ﬁnal protocol in 2).

– Which mode of theory development was applied?
We applied an emergent mode of theory develop-
ment, structuring the emergent theory later in the
review, as explained in Section 3.5.2.

– How was theoretical saturation achieved? When
the last couple of papers served to conﬁrm the key
ﬁndings and did not lead to the identiﬁcation of any
signiﬁcant or new insight, we ﬁnished coding for this
project.

– What research paradigm was used and why? To
maintain a keen focus on the real-world issues of
ethics in AI, we applied a pragmatic approach.

A mature theory in a STGT study must be novel, useful,
parsimonious, and modiﬁable according to the STGT evalua-
tion guidelines presented by Hoda [10]. Our grounded theory
of viewing ethics in AI through the developer’s prism is a mature
theory as it fulﬁlls the evaluation criteria of an STGT study
as follows:

• Novel: Our theory presents novel ﬁndings of the ﬁve
key categories and ﬁve hypotheses connecting them.
• Useful: Our theory will be useful to AI developers
and managers in helping them understand the im-
portance of awareness and perception as critical ﬁrst
steps to acknowledging needs and challenges, and
leading to applied and suggested approaches to deal-
ing with those challenges. It is useful to researchers
as a research agenda to explore the key categories
and their underlying concepts in future empirical
studies.

• Parsimonious: Our theory is parsimonious as it ex-
plains the complex phenomena of ethics in AI in a
simple and elegant way using the developer’s prism
metaphor. The theory breaks down the seemingly
single phenomenon of ethics in AI as a set of ﬁve
categories, each representing a phenomenon worthy
of full investigation, and a set of ﬁve interrelated
hypotheses. The theory is represented visually in
Figures 4 and 5.

• Modiﬁable: Our theory can be modiﬁed in the future
with new empirical evidence as developer aware-
ness, perception, needs, challenges, and approaches
to do with ethics in AI change over time.

9 THREATS AND LIMITATIONS

We now discuss some of the threats and limitations of our
study. Unlike an SLR, an ST-GTLR study does not aim to

achieve completeness. Rather, it focuses on capturing the
‘lay of the land’ through identifying the key aspects of
the topic and presenting rich explanations and nuanced
insights. As such, while the process of an ST-GTLR can be
replicated, the results – the resulting descriptive ﬁndings
and hypotheses – are not easily reproducible. Furthermore,
our search and select steps for identifying the seed papers
and subsequent snowballing may have resulted in missing
some relevant papers. This threat is highly dependent on the
list of keywords selected for the study and the limitations
of the search engines. To minimise the risk of this threat,
we used an iterative approach to develop the search strings
for the study. Initially, we chose the key terms from our
research title and added their synonyms to develop the ﬁnal
search strings which returned the most relevant studies. For
example, we included ’fairness’ in our ﬁnal search string
because when we used only the term ’ethics’, we obtained
zero articles in two databases (ACM DL and Wiley OL). It is
one of the limitations of our study because using this term
in our search string may have returned the studies focusing
on developers’ views on the fairness of AI systems.

Finding enough empirical articles related to our research
topic was another challenge. Due to this, we relaxed the
review protocol during snowballing of articles. This led us
to include the articles that were published in venues other
than journals and conferences. We also had to use studies
uploaded on ArXiv as our seed papers due to the lack
of enough peer-reviewed studies relevant to our research
topic. Given the increasing trend of researchers sharing
their manuscripts early on ArXiv while they are still under
review, it is a useful resource to ﬁnd the latest research
on emerging topics. The ability to ascertain the authors
and their afﬁliations lends credibility to the source. Direct
citations to ArXiv has increased steadily from 2000 to 2013 in
Scopus indexed scholarly publications. For example, ArXiv
has been cited the most by mathematics [41].

When applying the STGT approach [10] to analyse the
qualitative data of primary studies, we performed open
coding and targeted coding only on the Findings sections
of the studies that presented the empirical evidence we
were interested in. We did not ﬁnd examples of tools
(software/framework/models) that AI developers use to
implement ethics in AI. A study [G10] mentioned that
there is an existence of various tools to enhance AI ethics
implementation, however, no details about the tools were
mentioned. Following a broad and inductive approach, we
were not speciﬁcally looking for information on tools. How-
ever, it was still surprising that not a lot was mentioned
by developers in this area. Future reviews and studies can
be conducted to understand to what extent, why, and how
tools are used in the context of implementing ethics in AI.

To minimise the threats associated with a single coder,
the second author regularly reviewed the codes and dis-
cussed the coding process. All ﬁndings were reviewed
and discussed by the four authors regularly who asked
critical questions, helping develop nuanced concepts and
categories. The hypotheses were derived by analysing the
relationships between the ﬁve categories based on the em-
pirical ﬁndings from 30 primary articles which limits the
scope of our theory but it remains open to modiﬁcation with
more empirical evidence in future.

CURRENTLY UNDER REVIEW

21

10 CONCLUSION AND FUTURE WORK
AI systems are as ethical as the humans developing them. It
is critical to understand how the humans in the trenches,
the AI developers, view the topic of ethics in AI if we
are to a lay ﬁrm theoretical foundation for future work in
this area. With this in mind, we formulated the research
question: What do we know from literature about how developers
view ethics in AI? To address this, we conducted a socio-
technical grounded theory literature review (ST-GTLR) by
applying the overarching framework of grounded theory
literature review (GTLR) introduced by Wolfswinkel et al.
[11] with the concrete steps of the socio-technical grounded
theory (STGT) method for data analysis and theory devel-
opment [10], on 30 primary empirical studies. Since there
were not many empirical studies addressing our niche topic
and RQ exclusively, a grounded theory-based iterative and
responsive review approach worked well to identify and
extract relevant content from across multiple studies (that
mainly focused on related topics), while the application of
STGT enabled rigour analysis and theory development. A
contribution of this paper is the ST-GTLR guidelines for con-
ducting review studies in software engineering, especially
on niche and emerging topics.

Through applying STGT’s basic stage, we identiﬁed
ﬁve categories of developer awareness, developer perception,
developer challenges, developer challenges, and developer needs.
Through applying its emergent theory development mode,
we identiﬁed ﬁve hypotheses that link the categories: H1:
developer awareness is an antecedent to, but does not guaran-
tee positive, developer perception; H2: developer awareness can
lead to identifying developer needs; H3: developer challenges
lead to identifying developer needs; H4: developer perception
can generate developer approach; H5: developer challenges can
be overcome by developer approach. Taken together, the cate-
gories and hypotheses form the theory of ethics in AI through
the developer’s prism that explains the seemingly single phe-
nomenon of ethics in AI as a complex set of interrelated
phenomena using the developer’s prism metaphor. The
theory explains that developers’ awareness of AI ethics
directly leads to their perception about AI ethics and its
implementation as well as to identifying their needs, and
indirectly leads to identifying their challenges and coming
up with approaches (applied and potential strategies) to
overcome them.

We also share practical recommendations for AI devel-
opers, managers, and organisations. The theory serves as a
research agenda for the community, where future work can
focus on investigating and explaining each of the phenom-
ena of developer awareness, perception, challenges, needs,
and approach. Future empirical studies can focus on im-
proving the understanding and implementation of ethics in
AI and recommend practical approaches to minimise ethical
issues such as mitigating human biases in AI development
through frameworks and tools development.

ACKNOWLEDGMENTS
Aastha Pant is supported by the Faculty of IT Ph.D. scholar-
ship by Monash University. C. Tantithamthavorn is partially
supported by the Australian Research Council’s Discovery
Early Career Researcher Award (DECRA) funding scheme

(DE200100941). Also, the authors would like to thank Prof.
John Grundy for his constructive feedback on the paper.

LIST OF INCLUDED STUDIES
G1 V. Vakkuri, K.-K. Kemell,

J. Kultanen, M. Siponen, and P.
Abrahamsson, “Ethically aligned design of autonomous systems:
Industry viewpoint and an empirical study,” arXiv preprint
arXiv:1906.07946, 2019.

G2 K. Holstein, J. Wortman Vaughan, H. Daume III, M. Dudik, and H.
Wallach, ”Improving fairness in machine learning systems: What
do industry practitioners need?,” in Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems, pp. 1–16,
May 2019, doi: https://doi.org/10.1145/3290605.3300830.

G3 M.Veale, M.VanKleek, and R.Binns, “Fairness and accountability
design needs for algorithmic support in high-stakes public sector
decision-making,” in Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems, pp. 1–14, April 2018, doi:
https://doi.org/10.1145/3173574.3174014.

G4 V. Vakkuri, K.-K. Kemell,

J. Kultanen, and P. Abrahamsson,
“The current state of industrial practice in artiﬁcial intelligence
ethics,” IEEE Software, vol. 37, no. 4, pp. 50–57, April 2020,
doi:10.1109/MS.2020.2985621.

G5 V. Vakkuri, K.-K. Kemell, and P. Abrahamsson, “Implementing
ethics in AI: Initial results of an industrial multiple case study,”
in International Conference on Product-Focused Software Process
Improvement, pp. 331–338, Nov. 2019, doi: //doi.org/10.1007/978-
3-030-35333-9-24.

G6 J. C. Ibanez and M. V. Olmeda, “Operationalising AI ethics: How
are companies bridging the gap between practice and principles?
An exploratory study,” AI & Society, pp. 1–25, Aug. 2021, doi:
https://doi.org/10.1007/s00146-021-01267-0.

G7 W. Orr and J. L. Davis, “Attributions of ethical responsibility
by artiﬁcial
intelligence practitioners,” Information, Communi-
cation & Society, vol. 23, no. 5, pp. 719–735, Jan. 2020, doi:
10.1080/1369118X.2020.1713842.

G8 L. Rothenberger, B. Fabian, and E. Arunov, “Relevance of ethical
guidelines for artiﬁcial intelligence– A survey and evaluation,”
in Proceedings of the 27th European Conference on Informa-
tion Systems, Stockholm & Uppsala, Sweden, Jun. 8-14, 2019,
https://aisel.aisnet.org/ecis2019 rip/26.

G9 V. Vakkuri, K.-K. Kemell, and P. Abrahamsson, “Ethically aligned
design: An empirical evaluation of the Resolvedd-strategy in soft-
ware and systems development context,” in 2019 45th Euromicro
Conference on Software Engineering and Advanced Applications
(SEAA), pp. 46–50, 28-30 Aug. 2019, doi: 10.1109/SEAA.2019.00015.
G10 S. Kelley, “Employee perceptions of effective AI principle adop-

tion,” ResearchGate preprint, 2021.

G11 Madaio, M.A., et al., ”Co-designing checklists to understand
organizational challenges and opportunities around fairness in
AI,” in Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems, pp. 1-14, 25-30 Apr. 2020, doi:
http://dx.doi.org/10.1145/3313831.3376445.

G12 Addis, C. and M. Kutar, ”AI management an exploratory survey
of the inﬂuence of GDPR and FAT principles”, in 2019 IEEE Smart-
World, Ubiquitous Intelligence & Computing, Advanced & Trusted
Computing, Scalable Computing & Communications, Cloud &
Big Data Computing, Internet of People and Smart City Inno-
vation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI),
pp. 342-347, 19-23 Aug. 2019, doi: 10.1109/SmartWorld-UIC-ATC-
SCALCOM-IOP-SCI.2019.00102.

G13 Baker-Brunnbauer, J, ”Management perspective of ethics in artiﬁ-
cial intelligence,” AI and Ethics, vol. 1, no. 2, pp. 173-181, May 2021,
doi: https://doi.org/10.1007/s43681-020-00022-3.

G14 Frick, N.R., et al., ”Design requirements for AI-based services
enriching legacy information systems in enterprises: A managerial
perspective,” in Proceedings of the 31st Australasian Conference
on Information Systems (ACIS), pp.1-4, Dec. 2020.

G15 Seah, J. and M. Findlay, ”Communicating Ethics across the AI
Ecosystem,” SMU Centre for AI & Data Governance Research
Paper, Jul. 2021.

G16 L. Govia, “Coproduction, ethics and artiﬁcial

intelligence: A
perspective from cultural anthropology,” Journal of Digital
Social Research, vol. 2, no. 3, pp. 42–64, Nov. 2020, doi:
https://doi.org/10.33621/jdsr.v2i3.53.

G17 R. Mark and G. Anya, “Ethics of using smart

city AI
four large European cities,” The

and big data: The case of

CURRENTLY UNDER REVIEW

22

ORBIT Journal, vol. 2, no. 2, pp. 1–36,
https://doi.org/10.29297/orbit.v2i2.110

Jun. 2019, doi:

G18 B. C. Stahl, J. Antoniou, M. Ryan, K. Macnish, and T. Jiya,
“Organisational responses to the ethical
in-
telligence,” AI & Society, vol. 37, pp. 1–15, Feb. 2021, doi:
https://doi.org/10.1007/s00146-021-01148-6.

issues of artiﬁcial

G19 Q. Lu, L. Zhu, X. Xu, J. Whittle, D. Douglas, and C. Sanderson,
“Software engineering for responsible AI: An empirical study and
operationalised patterns,” 2021, arXiv preprint arXiv: 2111.09478.

G20 M. Kessing, “Fairness in AI: Discussion of a uniﬁed approach
to ensure responsible AI development,” Master dissertation, KTH
Royal Institute of Technology, Stockholm, Sweden, 2021.

G21 T. Karakash, “The double-edged razor of machine learning al-
gorithms in marketing: beneﬁts vs. ethical concerns,” Bachelor
dissertation, University of Twente, Enschede, Netherlands, 2021.

G22 T. Q. Sun and R. Medaglia, “Mapping the challenges of artiﬁcial
intelligence in the public sector: Evidence from public healthcare,”
Government Information Quarterly, vol. 36, no. 2, pp. 368–383,
April 2019, doi: https://doi.org/10.1016/j.giq.2018.09.008.

G23 B. C. Stahl, et al., “Artiﬁcial
ishing–beyond principles
Business Research, vol. 124, pp. 374– 388,
https://doi.org/10.1016/j.jbusres.2020.11.030.

intelligence for human ﬂour-
for machine learning,” Journal of
Jan. 2021, doi:

G24 E. Christodoulou and K. Iordanou, “Democracy under attack:
Challenges of addressing ethical issues of AI and big data for
more democratic digital media and societies,” Frontiers in Political
Science, vol. 3, pp. 71, Jul. 2021, doi: 10.3389/fpos.2021.682945.
G25 Chivukula, S.S., et al., ”Identity claims that underlie ethical aware-
ness and action,” in Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems, pp. 1-13, May 2021, doi:
10.1145/3411764.3445375.

G26 C. Sanderson et al., “AI ethics principles in practice: Perspectives of
designers and developers,” arXiv preprint arXiv:2112.07467, 2021.
G27 M. Ryan et al., “Research and practice of AI ethics: A case study
approach juxtaposing academic discourse with organisational real-
ity,” Science and Engineering Ethics, vol. 27, no. 2, pp. 1–29, Mar.
2021, doi: https://doi.org/10.1007/s11948-021-00293-x.

G28 S. S. Chivukula et al., “Dimensions of UX practice that shape ethical
awareness,” in Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems, pp. 1–13, 25-30 Apr. 2020, doi:
http://dx.doi.org/10.1145/3313831.3376459.

G29 Morley,

J., et al., ”Operationalising AI ethics: Barriers, en-
ablers and next steps,” AI & Society, pp. 1-13, Oct. 2021, doi:
https://doi.org/10.1007/s00146-021-01308-8.

G30 Slota, S.C., et al., ”Many hands make many ﬁngers to point:
Challenges in creating accountable AI,” AI & Society, p. 1-13, Nov.
2021, doi: https://doi.org/10.1007/s00146-021-01302-0.

REFERENCES

[1] B. Nalini, “The hitchhiker’s guide to AI ethics,” 2020.

Last

accessed 15 September 2021.

[2] L. Iacovino, “Ethical principles and information professionals:
Theory, practice and education,” Australian Academic & Research
Libraries, vol. 33, no. 2, pp. 57–74, 2002.

[3] M. Anderson and S. L. Anderson, Machine ethics. Cambridge

University Press, 2011.

[4] K. Siau and W. Wang, “Artiﬁcial intelligence ethics: Ethics of AI
and ethical AI,” Journal of Database Management, vol. 31, no. 2,
pp. 74–87, 2020.

[5] N. Bostrom and E. Yudkowsky, “The ethics of artiﬁcial intelli-
gence,” The Cambridge Handbook of Artiﬁcial Intelligence, vol. 1,
pp. 316–334, 2014.

[6] D. Obasanjo, “Abandoning github,” 2021. Last accessed 9 Septem-

ber 2021.

[7] K. Johnson, “The movement to hold AI accountable gains more

steam,” 2021. Last accessed 18 October 2021.

[8] N. Martin, “Are AI hiring programs eliminating bias or making it

worse?,” 2018. Last accessed 2 August 2021.

[9] A. Dhinakaran, “Overcoming AI’s transparency paradox,” 2021.

Last accessed 18 October 2021.

[10] R. Hoda, “Socio-technical grounded theory for software engineer-
ing,” IEEE Transactions on Software Engineering, pp. 1–1, 2021.
[11] J. F. Wolfswinkel, E. Furtmueller, and C. P. Wilderom, “Using
grounded theory as a method for rigorously reviewing literature,”
European Journal of Information Systems, vol. 22, no. 1, pp. 45–55,
2013.

[12] F. Nunes, N. Verdezoto, G. Fitzpatrick, M. Kyng, E. Gr ¨onvall, and
C. Storni, “Self-care technologies in HCI: Trends, tensions, and
opportunities,” ACM Transactions on Computer-Human Interaction
(TOCHI), vol. 22, no. 6, pp. 1–45, 2015.

[13] C. Meuwly, T. Chowdhury, N. Sandu, E. Golanov, P. Erne,
T. Rosemann, and B. Schaller, “Deﬁnition and diagnosis of the
trigeminocardiac reﬂex: A grounded theory approach for an up-
date,” Frontiers in Neurology, vol. 8, p. 533, 2017.

[14] N. Khanna, J. MacCormack, B. Kutsyuruba, S. McCart, and
J. Freeman, “Discovering critical factors for youth thriving: Using
grounded theory rigorous review method,” From the 2014 Rosa
Bruno-Jofr´e Symposium in Education (RBJSE), vol. 9, pp. 35–50, 2015.
[15] A. R. Montazemi and H. Qahri-Saremi, “Factors affecting adoption
of online banking: A meta-analytic structural equation modeling
study,” Information & Management, vol. 52, no. 2, pp. 210–226, 2015.
[16] K.-I. Andersson, “Developing a theory of open access: A grounded
theory based literature review,” Master dissertation, University of
Bor˚as, 2016.

[17] S. Utulu, K. Sewchurran, and B. Dwolatzky, “Systematic and
grounded theory literature reviews of software process improve-
ment phenomena: Implications for IS research,” in Proceedings of
the Informing Science and Information Technology Education Confer-
ence, pp. 249–279, Informing Science Institute, 2013.

[18] O. Okuah, B. M. Scholtz, and B. Snow, “A grounded theory
analysis of the techniques used by social media inﬂuencers and
their potential for inﬂuencing the public regarding environmental
awareness,” in SAICSIT ’19: Proceedings of the South African Insti-
tute of Computer Scientists and Information Technologists, pp. 1–10,
Association for Computing Machinery, 2019.

[19] F. Makoza, “National ICT policy challenges for developing coun-
tries: A grounded theory informed literature review,” International
Journal of Technology Policy and Law, vol. 3, no. 2, pp. 107–130, 2019.
[20] B. Mittelstadt, “Principles alone cannot guarantee ethical AI,”
Nature Machine Intelligence, vol. 1, no. 11, pp. 501–507, 2019.
[21] T. Hagendorff, “The ethics of AI ethics: An evaluation of guide-

lines,” Minds and Machines, vol. 30, no. 1, pp. 99–120, 2020.

[22] J. Bryson and A. Winﬁeld, “Standardizing ethical design for ar-
tiﬁcial intelligence and autonomous systems,” Computer, vol. 50,
no. 5, pp. 116–119, 2017.

[23] H. Vainio-Pekka, “The role of explainable AI in the research ﬁeld
of AI ethics: Systematic mapping study,” Master dissertation,
University of Jyv¨askyl¨a, 2020.

[24] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of
AI ethics guidelines,” Nature Machine Intelligence, vol. 1, no. 9,
pp. 389–399, 2019.

[25] J. Fjeld, N. Achten, H. Hilligoss, A. Nagy, and M. Srikumar,
“Principled artiﬁcial intelligence: Mapping consensus in ethical
and rights-based approaches to principles for AI,” Berkman Klein
Center Research Publication, no. 2020-1, 2020.

[26] T. Jameel, R. Ali, and I. Toheed, “Ethics of artiﬁcial intelligence: Re-
search challenges and potential solutions,” in 2020 3rd International
Conference on Computing, Mathematics and Engineering Technologies
(iCoMET), pp. 1–6, IEEE, 2020.

[27] A. A. Khan, S. Badshah, P. Liang, B. Khan, M. Waseem, M. Niazi,
and M. A. Akbar, “Ethics of AI: A systematic literature review of
principles and challenges,” arXiv preprint arXiv:2109.07906, 2021.

[28] “AI ethics guidelines global inventory,” 2020. Last accessed 10

August 2021.

[29] M. J. Smith, J. A. Mitchell, S. Blajeski, B. Parham, M. M. Har-
rington, B. Ross, B. Sinco, D. M. Brydon, J. E. Johnson, G. S.
Cuddeback, et al., “Enhancing vocational training in corrections: A
type 1 hybrid randomized controlled trial protocol for evaluating
virtual reality job interview training among returning citizens
preparing for community re-entry,” Contemporary Clinical Trials
Communications, vol. 19, p. 100604, 2020.

[30] B. Shneiderman, “Bridging the gap between ethics and practice:
Guidelines for reliable, safe, and trustworthy human-centered
AI systems,” ACM Transactions on Interactive Intelligent Systems,
vol. 10, no. 4, pp. 1–31, 2020.

[31] V. Vakkuri, M. Jantunen, E. Halme, K.-K. Kemell, A. Nguyen-Duc,
T. Mikkonen, and P. Abrahamsson, “Time for AI (ethics) maturity
model is now,” arXiv preprint arXiv:2101.12701, 2021.

[32] V. Vakkuri, K.-K. Kemell, and P. Abrahamsson, “Eccola- A method
for implementing ethically aligned AI systems,” in 2020 46th Eu-
romicro Conference on Software Engineering and Advanced Applications
(SEAA), pp. 195–204, IEEE, 2020.

CURRENTLY UNDER REVIEW

23

Burak Turhan, PhD (Bo ˜gazic¸i University), is a
Professor of Software Engineering at the Univer-
sity of Oulu and an Adjunct Professor (Research)
in the Faculty of IT at Monash University. His re-
search focuses on empirical software engineer-
ing, software analytics, quality assurance and
testing, human factors, and (agile) development
processes. He is a Senior Associate Editor of
Journal of Systems and Software, an Associate
Editor of ACM Transactions on Software Engi-
neering and Methodology and Automated Soft-
ware Engineering, an Editorial Board Member of Empirical Software En-
gineering, Information and Software Technology, and Software Quality
Journal, and a Senior Member of ACM and IEEE. For more information,
please visit: https://turhanb.net.

[33] A. Castelnovo, R. Crupi, G. Del Gamba, G. Greco, A. Naseer,
D. Regoli, and B. S. M. Gonzalez, “Befair: Addressing fairness in
the banking sector,” in 2020 IEEE International Conference on Big
Data (Big Data), pp. 3652–3661, IEEE, 2020.

[34] H. Felzmann, E. Fosch-Villaronga, C. Lutz, and A. Tam `o-Larrieux,
“Towards transparency by design for artiﬁcial intelligence,” Sci-
ence and Engineering Ethics, vol. 26, no. 6, pp. 3333–3361, 2020.
[35] D. Hidellaarachchi, J. Grundy, R. Hoda, and K. Madampe, “The
effects of human aspects on the requirements engineering process:
A systematic literature review,” IEEE Transactions on Software En-
gineering, 2021.

[36] H. Perera, W. Hussain,

J. Whittle, A. Nurwidyantoro,
D. Mougouei, R. A. Shams, and G. Oliver, “A study on the
prevalence of human values in software engineering publications,
2015-2018,” in 2020 IEEE/ACM 42nd International Conference on
Software Engineering (ICSE), pp. 409–420, IEEE, 2020.

[37] B. G. Glaser, The grounded theory perspective III: Theoretical coding.

Mill Valley, CA: Sociology Press, 2005.

[38] B. Kitchenham and S. Charters, “Guidelines for performing sys-
tematic literature reviews in software engineering,” Technical Re-
port EBSE, vol. 2, 2007.

[39] B. Kitchenham, R. Pretorius, D. Budgen, O. P. Brereton, M. Turner,
M. Niazi, and S. Linkman, “Systematic literature reviews in
software engineering– A tertiary study,” Information and Software
Technology, vol. 52, no. 8, pp. 792–805, 2010.

[40] A. Strauss and J. Corbin, Basics of qualitative research. Sage publica-

tions, 1990.

[41] X. Li, M. Thelwall, and K. Kousha, “The role of arXiv, RePEc,
SSRN and PMC in formal scholarly communication,” Aslib Journal
of Information Management, vol. 67, no. 6, pp. 614–635, 2015.

Aastha Pant is a Ph.D. candidate at Monash
University, Melbourne, Australia. She received
in Computer Engineering de-
her Bachelor
gree from NIILM university,
India. She com-
pleted her Master in Business Research from
University of Southern Queensland, Australia.
to her Ph.D. candidature, she was in
Prior
academia as a teaching assistant. Her
re-
search interests include ethics in Artiﬁcial
Intelligence, socio-technical aspects of soft-
ware engineering, software systems and cy-
ber security. More details of her
research can be found at,
https://www.researchgate.net/proﬁle/Aastha-Pant-3 Contact her at:
aastha.pant@monash.edu.

Rashina Hoda is an Associate Professor in Soft-
ware Engineering at Monash University, Aus-
tralia. Rashina specialises in human-centered
empirical software engineering and has intro-
duced socio-technical grounded theory (STGT)
for software engineering. She received an ACM
SIGSOFT Distinguished Paper Award (2017)
and Distinguished Reviewer Award (2020). She
serves as an Associate Editor of the IEEE Trans-
actions on Software Engineering and the PC co–
chair of the SEIS track of ICSE2023. Previously,
she served on the IEEE Software Advisory Board, as Associate Editor
of Journal of Systems and Software, CHASE 2021 PC co–chair and
XP2020 PC co–chair. More details on https://rashina.com. Contact her
at rashina.hoda@monash.edu.

Chakkrit Tantithamthavorn is a Senior Re-
search Fellow in the Faculty of Information Tech-
nology, Monash University, Australia. He is rec-
ognized as the most impactful early-career SE
researcher based on a bibliometric assessment
of software engineering (2013-2020), and re-
ceived numerous prestigious awards including
a 2021 ACM SIGSOFT Distinguished Paper
Award, a 2020 ARC’s Discovery Early Career
Researcher Award (DECRA), and a 2016 Japan
Society for the Promotion of Science (JSPS-
DC2) Research Fellowship. More about him and his work is available
online at http://chakkrit.com. Contact him at Chakkrit@monash.edu.

