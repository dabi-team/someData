2
2
0
2

g
u
A
4
2

]
E
S
.
s
c
[

1
v
1
7
2
1
1
.
8
0
2
2
:
v
i
X
r
a

Long Code for Code Search

Fan Hu1*, Yanlin Wang2†‡, Lun Du3, Hongyu Zhang4, Shi Han3, Dongmei Zhang3, Xirong Li1
1School of Information, Renmin University of China
2School of Software Engineering, Sun Yat-sen University
4The University of Newcastle
3Microsoft Research Asia

{hufan hf, xirong}@ruc.edu.cn {lun.du, shihan, dongmeiz}@microsoft.com
hongyu.zhang@newcastle.edu.au, wangylin36@mail.sysu.edu.cn

Abstract

Thanks to the Transformer-based pretraining models, the per-
formance of code search has been improved signiﬁcantly.
However, due to the restriction of multi-head self-attention
and GPU memory, there is a limit on the input token length.
The existing pretrained code models, such as GraphCode-
BERT, CodeBERT, RoBERTa (code), take the ﬁrst 256 tokens
by default, which makes them unable to represent the com-
plete information of long code (i.e., code that is greater than
256 tokens). Unlike the long text document that can be re-
garded as a whole with complete semantics, the semantics of
long code is discontinuous as a piece of long code may con-
tain different code modules. Therefore, it is unreasonable to
directly apply the long text processing methods to long code.
To tackle the long code problem, we propose MLCS (Mod-
eling Long Code for Code Search) to obtain a better repre-
sentation for long code. Our experimental results show the
effectiveness of MLCS for long code retrieval. With MLCS,
we could use Transformer-based pretraining models to model
long code without changing their internal structure and re-
pretraining. Through AST-based splitting and attention-based
fusion methods, MLCS achieves an overall mean reciprocal
ranking (MRR) score of 0.785, outperforming the previous
state-of-the-art result of 0.713 on the public CodeSearchNet
benchmark.

1

Introduction

With a good code search technique, developers can search
code snippets with natural language to boost software devel-
opment. Recently, it has been shown that Transformer-based
code pre-training techniques, such as CodeBERT (Feng
et al. 2020a), CoCLR (Huang et al. 2021), MuCos (Du et al.
2021) and GraphCodeBERT (Guo et al. 2021), could signiﬁ-
cantly improve code search performance via self-supervised
pre-training on large-scale code corpus.

However, an inherent limitation of such approaches is that
the computational and memory complexity of self-attention
in the original Transformer neural network scales quadrati-
cally with the input length, typically limiting input length to
around 512 tokens. In order to run properly on common ma-
chines such as 2 V100s, GraphCodeBERT and CodeBERT

*Work done during internship at Microsoft Research Asia.
†Yanlin Wang is the corresponding author.
‡Work done at Microsoft Research Asia

take the ﬁrst 256 tokens of code snippets and truncate tokens
beyond these 256 tokens. However, this length limitation
would cause accuracy problem especially for long code. For
example, when we checked the retrieval results of Graph-
CodeBERT and the text matching method BM25(Robertson
and Zaragoza 2009), we found that BM25 has higher perfor-
mance for some long code snippets with key information po-
sitions at the end. As shown in Figure 1, the keywords “Ten-
sor” and “patches” appear after 256 tokens, which are dis-
carded by GraphCodeBERT. Therefore, GraphCodeBERT
ranks the code in the 21,148 places, while the traditional text
matching method BM25 can recognize it well.

We further conducted empirical studies on GraphCode-
BERT in publicly used CodeSearchNet dataset (Husain et al.
2019), and found that as the ground-truth code length of
the query increased, the search performance gradually de-
creased. A similar problem in the ﬁeld of natural language
processing is the long text problem. Many approaches have
been proposed to address this, like hierarchical processing
(Zhang, Wei, and Zhou 2019), sparse attention (Child et al.
2019; Beltagy, Peters, and Cohan 2020), and segment-level
recurrence (Dai et al. 2019). There are two problems with
applying them for long code directly. First, these methods
modify the internal structure of the Transformer, then the ex-
isting pre-training parameters may be invalid. Second, long
code is different from long text, which is the highly struc-
tured language. Unlike the long text document which can
be regarded as a whole with complete semantics, the code
semantics are discontinuous, and different functions are dis-
tributed in different places. Therefore, dividing long code
can eliminate interference and better preserve semantic in-
formation. Our goal is to split the long code with program-
ming structure and directly enable Transformer-based pre-
training models with long code modeling capabilities with-
out changing their internal structure and re-pretraining.

Therefore, we present MLCS (Modeling Long Code for
Code Search) to deal with long code and get better code rep-
resentations. As described in Figure 3, we ﬁrst split the long
code into a code piece set. Then we use the sliding win-
dow method to obtain a partially overlapping code block
set. The existing code encoders are used to obtain the em-
beddings of code blocks. At last, we fuse the embeddings to
get the long code representations. Through extensive exper-

 
 
 
 
 
 
on code search mainly apply information retrieval (IR) tech-
niques directly, which regard code search as a text match-
ing task. Query and code snippets are both regarded as plain
text. The traditional text matching algorithms include BOW
(bag-of-words) (Sch¨utze, Manning, and Raghavan 2008),
Jaccard (Jaccard 1901), TF-IDF (term frequency-inverse
document frequency) (Robertson and Jones 1976), BM25
(best match 25, an improved version of TF-IDF) (Robert-
son and Zaragoza 2009) and extended boolean model (Lv
et al. 2015). Since code length has little effect on modeling
complexity, text matching methods can directly encode long
code without truncation.

After the large-scale pre-training model BERT (Devlin
et al. 2019) was proposed, Feng et al. (Feng et al. 2020b)
proposed CodeBERT, which is a model pre-trained on unla-
beled source code and comments. After ﬁne-tuning on text-
code paired dataset, CodeBERT achieved good performance
on natural language-based code search (Du et al. 2021; Gu
et al. 2022). Huang et al. (Huang et al. 2021) further intro-
duced a contrastive learning method dubbed CoCLR to en-
hance query-code matching. Sun et al. (Sun et al. 2022) pro-
posed a context-aware code translation technique that trans-
lates code snippets into natural language descriptions and
a shared word mapping function. Chai et al. (Chai et al.
2022) adapted few-shot meta-learning to code search. Re-
cently, Guo et al. (Guo et al. 2021) proposed a structure-
aware pre-training model GraphCodeBERT and achieved
the state-of-the-art search performance on CodeSearchNet.
However, long code must be truncated due to the quadratic
computational complexity of the Transformer.

2.2 Neural Code Representation with Code

Structure

Recently, some neural code representation methods that
leverage code structure such as AST have been proposed
and achieved strong performance (Shi et al. 2022; Tao et al.
2021; Wang et al. 2021). For example, MMAN (Wan et al.
2019a) uses a multi-modal attention fusion layer to fuse the
representations of AST and CFG. ASTNN (Zhang et al.
2019) and CAST (Shi et al. 2021) split a large AST into
a sequence of small statement trees, and encodes the state-
ment trees to vectors by capturing the lexical and syntacti-
cal knowledge of statements. TBCAA (Chen, Ye, and Zhang
2019) leverages a tree-based convolution network over API-
enhanced AST. GraphCodeBERT (Guo et al. 2021) utilizes
variable relations extracted from AST in pretraining tasks.
In this work, we aim to model the structural information of
the long code.

2.3 Transformer for long text

In the ﬁeld of natural language processing, the approaches
to scale up attention and improve original Transformer for
long text are divided into four categories: sparse attention
(Child et al. 2019; Correia, Niculae, and Martins 2019;
Beltagy, Peters, and Cohan 2020; Kitaev, Kaiser, and Lev-
skaya 2019; Roy et al. 2021; Ainslie et al. 2020), recurrence
(Dai et al. 2019), hierarchical mechanisms (Zhang, Wei, and
Zhou 2019), and compressed attention (Ye et al. 2019; Guo

Figure 1: One case of BM25 and GraphCodeBERT methods.
We show the top1 results returned by models. Green and
red boxes represent correct and incorrect returned results,
respectively. Key words are highlighted in blue.

iments, we ﬁnd the proposed AST-based splitting method
and attention-based fusion method outperform other split-
ting and fusion methods. Due to the different code block
numbers obtained by different code snippets, we designed
a combine-divide module for acceleration. On the Code-
SearchNet dataset, MLCS achieves the state-of-the-art re-
sults with an overall mean reciprocal ranking (MRR) score
of 0.785, which is 10.1% higher than GraphCodeBERT.
The contributions of this work can be summarized as:
• Empirical ﬁnding and veriﬁcation of the difﬁculty for

modeling long code in existing code search models.

• We propose MLCS and design an AST-based splitting
and attention-based fusion method. we also design a
combine-divide module for acceleration.

• Through extensive experiments, we show the effective-
ness of the proposed MLCS in six programming lan-
guages.

2 Related work

2.1 Code search methods
Early explorations (Nie et al. 2016; Yang and Huang 2017;
Rosario 2000; Hill, Pollock, and Vijay-Shanker 2011; Sat-
ter and Sakib 2016; Lv et al. 2015; Van Nguyen et al. 2017)

Query:  Return a  tensorcontaining the patches. GraphCodeBERT: GT rank 21148.BM25: GT rank 1.defread_image_file(data_dir,image_ext,n):defPIL2array(img):returnnp.array(img.getdata(),dtype=np.uint8).reshape(64,64)...forfpathinlist_files:img=Image.open(fpath)foryinrange(0,1024,64):forxinrange(0,1024,64):patch=img.crop((x,y,x+64,y+64))patches.append(PIL2array(patch))returntorch.ByteTensor(np.array(patches[:n]))defpatch(self,*args,**kwargs):returnsuper(Deposit,self).patch(*args,**kwargs)Table 1: The code search performance (MRR) of different ground-truth code token lengths. We set the code truncation length
from 50 to 400. Dataset: CodeSearchNet python.

Code token length

#Queries

[129, 332)
[332, 508)
[509, 785)
[785, 1196)
[1196, 1995)

2984
2984
2984
2983
2983

Code truncation length

50

0.6701
0.6487
0.6235
0.5761
0.5591

100

0.7228
0.7113
0.6663
0.6329
0.6051

200

0.7265
0.7292
0.7027
0.6830
0.6447

256

0.7271
0.7272
0.7111
0.6974
0.6547

300

0.7269
0.7274
0.7128
0.7048
0.6628

400

0.7266
0.7289
0.7155
0.7108
0.6711

et al. 2019). Sparse Attention limits each token to attend to
a subset of the other tokens. Recurrence incorporates ele-
ments of recurrent neural networks into Transformer models
to lengthen their attention span. Hierarchical Mechanisms
model long input text hierarchically from sentences to para-
graphs. Compressed Attention selectively compresses cer-
tain parts of the input.

But these methods may not be suitable for highly struc-
tured code. For a well-designed program, the code within
the same module (e.g., a function) is tightly connected, and
the interactions between different modules are loosely cou-
pled (i.e., the guideline of high cohesion and low coupling).
But a piece of long text in natural language is usually coher-
ent. Therefore, this paper proposes MLCS, which divides the
long code according to the code structure and fuses the code
block embeddings with an attention-based method.

3 Motivation: The Long Code Problem

3.1 Preliminaries
Given a query Q and a codebase, the goal of code search
is to ﬁnd the best code snippet C that matches the query Q
from the codebase. For a current bi-encoder deep-learning
model, we ﬁrst transform query Q and the code snippets C
to query and code tokens with the tokenizer such as BPE
(Sennrich, Haddow, and Birch 2016). Then we transform the
token ids of the query Q and the code snippets C to vector
representations eq and ec by neural network encoders, and
calculate the similarity (or distance) measures in Euclidean
space such as Cosine similarity or Euclidean distance to ob-
tain the cross-modal similarity score s. The calculation can
be formalized as follows:






eq = Γ(tokenizer(Q))
ec = Γ(cid:48)(tokenizer(C)), C ∈ Codebase
s = sim(eq, ec)

(1)

where Γ and Γ(cid:48) are two well-trained neural network en-
coders learned from labeled paired data.

3.2 The Long Code Problem
If the query or code is long, we generally use the truncation
method to deal with it. For example, GraphCodeBERT takes
the ﬁrst 128 query tokens and the ﬁrst 256 code tokens by
default. In order to explore whether the truncation method
lead to loss of information, we make statistics for the token

(a) Query.

(b) Code.

Figure 2: The token length of query and code on Code-
SearchNet python test set.

length of query and code on CodeSearchNet python test set.
As shown on Figure 2, the query with a token length greater
than 128 only accounts for 11%, while the code with a token
length greater than 256 accounts for 91%. It can be seen that
for the code snippets with the token length greater than 256,
truncation will lead to loss of information.

To explore the difference in search performance of Graph-
CodeBERT for query subsets with different ground truth
(GT) code lengths, we divide the CodeSerarchNet(CSN)
python test subset into 5 query sets according to differ-
ent ground-truth code token lengths. We calculate the mean
reciprocal ranking of GraphCodeBERT for different Code
truncation lengths, and the results are shown in Table 1.

We observe that as the ground-truth code token length in-
creases (from top to bottom), the search performance gradu-
ally deteriorates, indicating that long code snippets are hard
cases for GraphCodeBERT. As the Code truncation length
increases (from left to right), the search performance of short
code snippets (token length is less than 332) remains basi-
cally unchanged. While the search performance of long code
snippets (token length is more than 509) has been greatly im-
proved, indicating that if the long code is simply truncated,
a lot of useful information may be lost.

4 MLCS
We describe the details of MLCS in this section, includ-
ing the model architecture, the splitting methods, the fusion
methods, and the combine-divide method for accelerating
the inference.

4.1 Model architecture
We introduce our MLCS in this section. The overall pipeline
is shown in Figure 3. Given a code snippet C, we want to

89%11%token length <= 128token length > 1289%91%token length <= 256token length > 256(a) AST-based code splitting.

(b) Slidding window and fusion.

Figure 3: The pipeline of our proposed splitting and fusion architecture.

obtain a strong code representation ec. As shown in Figure
3(a), we ﬁrst split the code snippet into a code piece set:

P = Split(C) = {p1, p2, . . . , pn}.

(2)

Then we use the sliding window method to obtain a partially
overlapping code block set:

B = SlidingWindow(P ) = {b1, b2, . . . , bk}.

(3)

Assuming the window size is w and the step is s, then the
code block number is k = (cid:98) n−w
s + 1(cid:99), where (cid:98)·(cid:99) refers to
round down. We use a code encoder (such as GraphCode-
BERT) e to obtain the embeddings of k code blocks:

eB = {eb1 , eb2, . . . , ebk }.
At last, a fusion method is used to fuse the k embeddings to
get the code representation ec:

(4)

ec = Fusion(eB)

(5)

4.2 Splitting methods
To obtain the code piece set, we try four splitting methods,
i.e., space-based splitting, token-based splitting, line-based
splitting, and our designed AST-based splitting. Space-based
splitting is simply splitting by space, for example “def
read image ﬁle” is divided into {‘def’, ‘read image ﬁle’}.

For AST-based splitting, we parse a source code
into an Abstract Syntax Tree with tree sitter1, and visit
this AST by preorder traversal. For composite structure

1https://github.com/tree-sitter/py-tree-sitter

(i.e., f or, if, def , etc.), as shown in Figure
3(a), we
deﬁne the set of AST nodes {head block, body}, where
head block is for splitting the header and body of nested
statements such as if and W hile statements, and body for
the method declaration. Each time a composite structure is
encountered, a splitting mark is inserted before and after
head block. In this way, a large AST is divided into a se-
quence of non-overlapping subtrees. According to the split-
ting of AST, we split the original code to the code piece set
P .

4.3 Fusion methods
Meanpooling / Maxpooling. As shown in Figure 3(a), to
fuse the embeddings of k code blocks, a naive way is to take
the mean/max of their embeddings:

ec = Mean/Max({eb1, eb2, . . . , ebk })
(6)
However, a potential drawback of meanpooling is that each
code block contributes equally to the ﬁnal result regardless
of their quality. Meanwhile, for maxpooling, the block em-
bedding with max value has the largest contribution. Thus
we make a natural extension and further propose weighted
embedding methods.

Attention-based fusion. As the code blocks are not
equally important for representing the long code. We con-
sider giving each block embedding a self-adaptive weight α
for fusion, i.e.,

ec =

k
(cid:88)

i

αiebi.

(7)

defread_image_file(data_dir,image_ext,n):defPIL2array(img):returnnp.array(img.getdata(),dtype=np.uint8).reshape(64,64)...forfpathinlist_files:foryinrange(0,1024,64):forxinrange(0,1024,64):patch=img.crop((x,y,x+64,y+64))patches.append(PIL2array(patch))returntorch.ByteTensor(np.array(patches[:n]))Functiondefinitionnameread_image_filebodyparametersname…Return statementFunctiondefinition…leftForstatement…………rightbody……defread_image_file(data_dir,image_ext,n):defPIL2array(img):returnnp.array(img.getdata(),dtype=np.uint8).reshape(64,64)...forfpathinlist_files:foryinrange(0,1024,64):forxinrange(0,1024,64):patch=img.crop((x,y,x+64,y+64))patches.append(PIL2array(patch))returntorch.ByteTensor(np.array(patches[:n]))Functiondefinitionnameread_image_filebodyparametersnamebodyReturn statementFunctiondefinition…INForstatement…………body…Return statementdefread_image_file(data_dir,image_ext,n):defPIL2array(img):returnnp.array(img.getdata(),dtype=np.uint8).reshape(64,64)...forfpathinlist_files:foryinrange(0,1024,64):forxinrange(0,1024,64):patch=img.crop((x,y,x+64,y+64))patches.append(PIL2array(patch))returntorch.ByteTensor(np.array(patches[:n]))def …def PIL2array… return np.array…for fpathin…img= Image. …...def …def PIL2array… return np.array…for fpathin…img= Image. …...GraphCodeBERTGraphCodeBERTGraphCodeBERT...FusionmoduleMethodDeclarationSliding windowCode encoderCode representationNode kindprogramStart pointEnd pointchildrenRead TextNodekindprogram…ReturnStatement…childrenNodekindrange……Figure 5: The batch processing combine-divide method. (cid:172)
and (cid:173) refer to combination and division methods.

Table 2: Data statistics of CodeSearchNet.

Code Language Training

Dev

Test

# Codebase

Python
PHP
Go
Java
JavaScript
Ruby

251,820
241,241
167,288
164,923
58,025
24,927

13,914
12,982
7,325
5,183
3,885
1,400

14,918
14,014
8,122
10,955
3,291
1,261

43,827
52,660
28,120
40,347
13,981
4,360

(a) One layer attention with
mean / max.

(b) Two layer attention with
mean / max.

Figure 4: The attention-based fusion methods.

Inspired by attention-based multiple instance learning (MIL)
(Ilse, Tomczak, and Welling 2018), we compute the weights
{α1, . . . , αk} as follow,

{a1, . . . , ak} = sof tmax(Linear({eb1 , . . . , ebk })).

(8)

For one layer attention, Linear refers to a fully connected
layer that transforms the dimension to 1. For two layer atten-
tion, Linear refers to two fully connected layers that ﬁrst
transform the dimension to 128 and then transform the di-
mension to 1. As shown in Figure 4(a) and Figure 4(b),
we also consider to combine the attention and meanpooling
/ maxpooling methods, i.e.,

5 Experimental design

5.1 Datasets

We conduct experiments on the widely used CodeSearchNet
(Husain et al. 2019) dataset, which includes six program-
ming languages, i.e., , Ruby, JavaScript, Go, Python, Java,
and PHP. The data statistics is shown in Table 2. Following
(Guo et al. 2021), we ﬁlter low-quality queries and expand
the retrieval set to the whole code corpus. Note that we use
the CodeSearchNet Python dataset which contains 14,918
queries and 43,827 code candidates for most of the follow-
ing experiments.

ec =

k
(cid:88)

i

(αiebi ) + Mean/Max({eb1 , eb2, . . . , ebk }). (9)

5.2 Evaluation metrics

4.4 Batch Processing

For improving the inference efﬁciency on large datasets, it is
necessary to design the batch processing method to encode
multiple long code snippets simultaneously. As described in
Section 4.1, we obtain multiple code blocks from a long
code snippet. However, since the number of resulting code
blocks varies for different long code snippets, it is difﬁcult
to make general batch processing.

We design a combine-divide method to tackle this prob-
lem. As shown in Figure 5, assuming the batch size is 3
(a code batch including three code snippets), the resulting
number of blocks is 2, 3, 1, respectively. We ﬁrst combine
the six code blocks as a block batch, and build a map M
that maps the code index to the block index. Then we input
the block batch into the code encoder in parallel to get block
embeddings. At last, according to M , we divide embeddings
into three groups and input them into the fusion module to
get different code representations.

We use two popular automatic criteria: MRR (Mean Recip-
rocal Ranking) and R@k (top-k accuracy, k=1, 5, 10, 100).
They are commonly used for in previous code search stud-
ies (Lv et al. 2015; Gu, Zhang, and Kim 2018; Sachdev et al.
2018; Husain et al. 2019; Feng et al. 2020b; Huang et al.
2021; Guo et al. 2021).

Mean Reciprocal Ranking (MRR) is a criterion to evalu-
ate systems that return a ranked list of answers to queries.
Assume that rank is the position of the highest-ranked an-
swer (1, 2, 3, . . . , N for N answers returned in a query). For
multiple queries Q, the Mean Reciprocal Rank is the mean
of the Q reciprocal ranks:

MRR =

1
Q

Q
(cid:88)

i=1

1
ranki

(10)

The higher the MRR value, the better the code search per-
formance.

R@k measures the percentage of queries for which the
correct result exists in the top-k ranked results (Wan et al.

EncoderLinearSoftmax…Encoder1×𝑑1×𝑑𝑘×𝑑𝑘×1𝑘×11×𝑑1×𝑑1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)LinearSoftmaxMean/MaxtanhLinear1×𝑑1×𝑑𝑘×𝑑1×𝑑Mean/Max1×𝑑1×𝑑𝑘×𝑑𝑘×128𝑘×11×𝑑1×𝑑𝑘×128𝑘×1Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)EncoderLinearSoftmax…Encoder1×𝑑1×𝑑𝑘×𝑑𝑘×1𝑘×11×𝑑1×𝑑1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)LinearSoftmaxMean/MaxtanhLinear1×𝑑1×𝑑𝑘×𝑑1×𝑑Mean/Max1×𝑑1×𝑑𝑘×𝑑𝑘×128𝑘×11×𝑑1×𝑑𝑘×128𝑘×1Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:2869)ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘(cid:3038)Table 3: The search performance of different MLCS variants . Dataset: CodeSearchNet Ruby.

GraphCodeBERT

MLCS-SpaceSplitting

MLCS-TokenSplitting

MLCS-LineSplitting

MLCS-ASTSplitting

Window Step Splitting Fusion

MRR R@1 R@5 R@10 R@100

–

256
256
256
256
256
256
128
64

256
128
64

64
32
16

64
32
16

–

128
128
128
128
128
128
64
32

128
64
32

32
16
8

32
16
8

–

–

0.6948

Space Maxpooling
0.6919
Space Meanpooling
0.6929
0.6940
Space Attention (two layer)
Space Attention (two layer) + Mean 0.7490
0.6989
Space Attention (one layer)
0.7495
Space Attention (one layer) + Mean
0.7545
Space Attention (one layer) + Mean
0.7431
Space Attention (one layer) + Mean

Token Attention (one layer) + Mean
Token Attention (one layer) + Mean
Token Attention (one layer) + Mean

Line Attention (one layer) + Mean
Line Attention (one layer) + Mean
Line Attention (one layer) + Mean

AST Attention (one layer) + Mean
AST Attention (one layer) + Mean
AST Attention (one layer) + Mean

0.7752
0.7606
0.7352

0.7635
0.7537
0.7498

0.7539
0.7762
0.7744

59.3

58.5
58.3
58.7
66.3
59.6
66.1
66.2
65.1

68.4
67.2
62.8

67.3
66.1
65.5

65.7
68.8
68.8

82.1

82.0
83.0
83.4
85.2
82.2
86.3
87.5
85.6

89.1
87.5
87.2

88.2
87.2
86.9

91.4
89.1
88.7

87.3

87.2
87.4
87.1
88.9
86.8
89.0
90.2
88.7

91.9
91.3
90.6

91.3
90.3
90.3

95.0
92.0
91.4

96.5

95.2
95.6
94.8
94.4
95.0
94.3
95.2
94.0

96.0
95.6
95.0

95.6
95.2
95.0

97.6
96.4
96.3

Table 4: The MRR comparison between baseline (Graph-
CodeBERT) and MLCS in different ground-truth code to-
ken lengths. Dataset: CodeSearchNet python. The compari-
son results of other programming languages are provided in
the supplementary material.

Code token
length

[129, 332)
[332, 508)
[509, 785)
[785, 1196)
[1196, 1995)

# Queries Baseline MLCS

2984
2984
2984
2983
2983

0.7271
0.7272
0.7111
0.6974
0.6547

0.7372
0.7598
0.7422
0.7440
0.7315

2019b; Kilickaya et al. 2017; Sun et al. 2022), which is com-
puted as follows:

R@k =

1
Q

Q
(cid:88)

i=1

δ (ranki ≤ k)

(11)

where δ(·) denotes a function which returns 1 if the input is
true and returns 0 otherwise. The higher the R@k value, the
better the code search performance.

5.3 Experimental settings
Our baseline is GraphCodeBERT. The parameters of code
and natural language encoders are initialized by GraphCode-
BERT. For training, we randomly select 6 code blocks from
the divided code blocks of one long code. The training batch
size is 64. For evaluation, we use all divided code blocks
of one long code. The evaluated batch size is 256. All ex-
periments are conducted on a machine with Intel Xeon E5-
2698v4 2.2Ghz 20-Core CPU and four Tesla V100 32GB
GPUs.

Figure 6: The time cost of different batching methods.

6 Experimental Results
6.1 Exploring the best MLCS conﬁguration w.r.t

code splitting and fusing methods

To ﬁnd the best splitting and fusing methods of MLCS,
we varied our architecture in different code split methods
and fusing methods, measuring the change in search perfor-
mance. Since the CodeSearchNet ruby dataset is relatively
small, we choose to perform experiments on the ruby pro-
gramming language. We present these results in Table 3.

In Table 3 rows MLCS-SpaceSplitting, we try different
fusing methods described in Section 4.3. Compared to
GraphCodeBERT Baseline, using one of the fusing methods
alone cannot achieve signiﬁcant performance improvement.
Since meanpooling is better than maxpooling, we fuse the
attention method with meanpooling, and the search perfor-
mance is signiﬁcantly improved. For Attention (one layer) +
Mean fusion method, MRR and R@1 are improved by 7.9%
and 11.5%, respectively. In the subsequent experiments, we
choose Attention (one layer) + Mean as the fusion method.

010020030040050060070080090048163264Time cost (s)Batch SizeWithout batchingBlock batchingTable 5: The MRR on six languages of the CodeSearchNet dataset. MLCS here refers to MLCS-ASTSplitting with window
size 32 and step 16.

Model / Method Ruby

Javascript

NBow
CNN
BiRNN
selfAtt

0.162
0.276
0.213
0.275

RoBERTa
0.587
RoBERTa (code) 0.628
CodeBERT
0.679
GraphCodeBERT 0.703

0.157
0.224
0.193
0.287

0.517
0.562
0.620
0.644

Go

0.330
0.680
0.688
0.723

0.850
0.859
0.882
0.897

Python

0.161
0.242
0.290
0.398

0.587
0.610
0.672
0.692

Java

0.171
0.263
0.304
0.404

0.599
0.620
0.676
0.691

Php

0.152
0.260
0.338
0.426

0.560
0.579
0.628
0.649

Overall

0.189
0.324
0.338
0.419

0.617
0.643
0.693
0.713

MLCS

0.776 (10.4%↑) 0.742 (15.2%↑) 0.921 (2.7%↑) 0.754 (8.9%↑) 0.768 (11.1%↑) 0.748 (15.3%↑) 0.785 (10.1%↑)

In Table 3, we try different code split methods de-
scribed in Section 4.2 rows MLCS-SpaceSplitting, MLCS-
TokenSplitting, MLCS-LineSplitting, MLCS-ASTSplitting.
For space and token based splitting methods, we set the win-
dow size from 64 to 256 due to the ﬁner granularity of the
division. Conversely, for line and AST based split methods,
we set the window size from 16 to 64. We observe that AST
based split method has outstanding performance, achieving
the best MRR and R@1 with a window size of 32. In the fol-
lowing experiments, MLCS refers to MLCS-ASTSplitting
with a window size of 32 and Attention (one layer) + Mean
fusion method.

6.2

Improvement of the proposed MLCS for code
with different lengths

To explore the improvement of the proposed MLCS for code
snippets with different lengths, we report the search perfor-
mance of baseline method GraphCodeBERT and MLCS un-
der different ground-truth code token lengths. The compari-
son results are shown in Table 4.

As we can see, the retrieval performance of each query
subset has been improved, especially the improvement of
long code retrieval results. We attribute this to two aspects.
First, the fusion module of MLCS captures and incorporates
information from various parts of the long code. Second, the
splitting of code can be seen as a method of data enhance-
ment. In summary, MLCS helps us get a stronger code repre-
sentation, which greatly improves the retrieval performance.

6.3 Effectiveness of the combine-divide method

As we described in Section 4.4, we design a batch pro-
cessing method to encode multiple codes with the differ-
ent number of code blocks simultaneously, thus can accel-
erate the speed of inference. To verify the effectiveness, we
test our model in two different ways: without batching and
combine-divide. Note that the batching has no effect on the
performance but changes the efﬁciency a lot.

We report the inference time with different batch size
on Figure 6. The dataset is CodeSearchNet python. We
observe as the batch size increases, the inference time of
the combine-divide method decreases, while the inference
time of the without batching method keeps consistent. The

combine-divide method shows a signiﬁcant improvement
over the without batching method.

6.4 Comparison with the state-of-the-art methods

in multiple programming languages

For a fair and reproducible comparison, we choose the base-
lines for comparison that meet the following three crite-
ria: 1) The source code is publicly available; 2) The over-
all model is adaptable to all the six programming lan-
guages on the CodeSearchNet dataset; 3) The paper is peer-
reviewed if it is published as a research paper. As a result,
we select four deep end-to-end approaches: NBow, CNN,
BiRNN, and SelfAtnn proposed by Husains et al. (Hu-
sain et al. 2019) and ﬁve pre-training-based approaches:
RoBERTa (Liu et al. 2019), RoBERTa (code) (Feng et al.
2020a), CodeBERT (Feng et al. 2020a), and GraphCode-
BERT (Guo et al. 2021).

The MRR results are shown in Table 5. We observe
that MLCS outperforms all methods in the six programming
languages. The average MRR of MLCS is 0.785, which is
10.1% higher than the best previous method GraphCode-
BERT. Note that the conclusion still holds for the recall met-
ric. The recall results can be found in the supplementary ma-
terial.

7 Conclusion
In this paper, we attack the problem of modeling long code
for code search. We propose MLCS to obtain a better rep-
resentation for long code. With our designed AST-based
splitting and attention-based fusion methods, MLCS signiﬁ-
cantly improves the search performance, especially for long
code. We believe this work opens up new possibilities for
code search.

References
Ainslie, J.; Onta˜n´on, S.; Alberti, C.; Cvicek, V.; Fisher, Z.;
Pham, P.; Ravula, A.; Sanghai, S.; Wang, Q.; and Yang, L.
2020. ETC: Encoding Long and Structured Inputs in Trans-
formers. In EMNLP.
Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer:
The long-document transformer. arXiv.

Chai, Y.; Zhang, H.; Shen, B.; and Gu, X. 2022. Cross-
Domain Deep Code Search with Few-Shot Meta Learning.
arXiv.
Chen, L.; Ye, W.; and Zhang, S. 2019. Capturing source code
semantics via tree-based convolution over API-enhanced
AST. In Proceedings of the 16th ACM International Con-
ference on Computing Frontiers.
Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.
Generating long sequences with sparse transformers. arXiv.
Correia, G. M.; Niculae, V.; and Martins, A. F. 2019. Adap-
tively Sparse Transformers. In EMNLP-IJCNLP.
Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J. G.; Le, Q.; and
Salakhutdinov, R. 2019. Transformer-XL: Attentive Lan-
guage Models beyond a Fixed-Length Context. In ACL.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In NAACL-HLT.
Du, L.; Shi, X.; Wang, Y.; Shi, E.; Han, S.; and Zhang, D.
2021. Is a Single Model Enough? MuCoS: A Multi-Model
Ensemble Learning Approach for Semantic Code Search. In
Proceedings of the 30th ACM International Conference on
Information & Knowledge Management, 2994–2998.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong,
M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; and Zhou, M.
2020a. CodeBERT: A Pre-Trained Model for Programming
and Natural Languages.
In Cohn, T.; He, Y.; and Liu, Y.,
eds., EMNLP.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong,
M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; and Zhou, M.
2020b. CodeBERT: A Pre-Trained Model for Programming
and Natural Languages.
In Cohn, T.; He, Y.; and Liu, Y.,
eds., EMNLP.
Gu, W.; Wang, Y.; Du, L.; Zhang, H.; Han, S.; Zhang,
D.; and Lyu, M. R. 2022. Accelerating Code Search with
arXiv preprint
Deep Hashing and Code Classiﬁcation.
arXiv:2203.15287.
Gu, X.; Zhang, H.; and Kim, S. 2018. Deep code search. In
ICSE.
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou,
L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; Tufano, M.; Deng,
S. K.; Clement, C. B.; Drain, D.; Sundaresan, N.; Yin, J.;
Jiang, D.; and Zhou, M. 2021. GraphCodeBERT: Pre-
training Code Representations with Data Flow. In ICLR.
Guo, Q.; Qiu, X.; Liu, P.; Shao, Y.; Xue, X.; and Zhang, Z.
2019. Star-Transformer. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,
Volume 1 (Long and Short Papers), 1315–1325. Association
for Computational Linguistics.
Hill, E.; Pollock, L.; and Vijay-Shanker, K. 2011. Improving
source code search with natural language phrasal represen-
tations of method signatures. In ASE. IEEE.
Huang, J.; Tang, D.; Shou, L.; Gong, M.; Xu, K.; Jiang,
D.; Zhou, M.; and Duan, N. 2021. CoSQA: 20,000+ Web
Queries for Code Search and Question Answering. In ACL.

Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and
Brockschmidt, M. 2019. Codesearchnet challenge: Evalu-
ating the state of semantic code search. arXiv.
Ilse, M.; Tomczak, J.; and Welling, M. 2018. Attention-
based deep multiple instance learning. In ICML, 2127–2136.
Jaccard, P. 1901. ´Etude comparative de la distribution ﬂorale
dans une portion des Alpes et des Jura. Bull Soc Vaudoise
Sci Nat, 547–579.
Kilickaya, M.; Erdem, A.; Ikizler-Cinbis, N.; and Erdem, E.
2017. Re-evaluating Automatic Metrics for Image Caption-
ing. In Lapata, M.; Blunsom, P.; and Koller, A., eds., EACL.
Association for Computational Linguistics.
Kitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer:
The Efﬁcient Transformer. In International Conference on
Learning Representations.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
2019. Roberta: A robustly optimized bert pretraining ap-
proach. arXiv.
Lv, F.; Zhang, H.; Lou, J.-g.; Wang, S.; Zhang, D.; and Zhao,
J. 2015. Codehow: Effective code search based on api un-
derstanding and extended boolean model (e). In ASE.
Nie, L.; Jiang, H.; Ren, Z.; Sun, Z.; and Li, X. 2016. Query
expansion based on crowd knowledge for code search. IEEE
Transactions on Services Computing, 771–783.
Robertson, S.; and Zaragoza, H. 2009. The probabilistic rel-
evance framework: BM25 and beyond. Now Publishers Inc.
Robertson, S. E.; and Jones, K. S. 1976. Relevance weight-
ing of search terms. Journal of the American Society for
Information science, 129–146.
Rosario, B. 2000. Latent semantic indexing: An overview.
Techn. rep. INFOSYS, 1–16.
Roy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2021.
Efﬁcient content-based sparse attention with routing trans-
formers. Transactions of the Association for Computational
Linguistics, 9: 53–68.
Sachdev, S.; Li, H.; Luan, S.; Kim, S.; Sen, K.; and Chandra,
S. 2018. Retrieval on source code: a neural code search. In
MAPL.
Satter, A.; and Sakib, K. 2016. A search log mining based
query expansion technique to improve effectiveness in code
search. In ICCIT, 586–591. IEEE.
In-
Sch¨utze, H.; Manning, C. D.; and Raghavan, P. 2008.
troduction to information retrieval, volume 39. Cambridge
University Press Cambridge.
Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-
chine Translation of Rare Words with Subword Units.
In
ACL.
Shi, E.; Wang, Y.; Du, L.; Chen, J.; Han, S.; Zhang, H.;
Zhang, D.; and Sun, H. 2022. On the evaluation of neural
In Proceedings of the 44th Interna-
code summarization.
tional Conference on Software Engineering, 1597–1608.
Shi, E.; Wang, Y.; Du, L.; Zhang, H.; Han, S.; Zhang, D.;
and Sun, H. 2021. CAST: Enhancing Code Summarization
with Hierarchical Splitting and Reconstruction of Abstract
Syntax Trees. In EMNLP.

Sun, W.; Fang, C.; Chen, Y.; Tao, G.; Han, T.; and Zhang, Q.
2022. Code Search based on Context-aware Code Transla-
tion. arXiv.
Tao, W.; Wang, Y.; Shi, E.; Du, L.; Han, S.; Zhang, H.;
Zhang, D.; and Zhang, W. 2021. On the evaluation of com-
mit message generation models: an experimental study. In
2021 IEEE International Conference on Software Mainte-
nance and Evolution (ICSME).
Van Nguyen, T.; Nguyen, A. T.; Phan, H. D.; Nguyen, T. D.;
and Nguyen, T. N. 2017. Combining word2vec with revised
In ICSE-C,
vector space model for better code retrieval.
183–185. IEEE.
Wan, Y.; Shu, J.; Sui, Y.; Xu, G.; Zhao, Z.; Wu, J.; and Yu,
P. S. 2019a. Multi-modal Attention Network Learning for
Semantic Source Code Retrieval. In ASE, 13–25.
Wan, Y.; Shu, J.; Sui, Y.; Xu, G.; Zhao, Z.; Wu, J.; and Yu,
P. S. 2019b. Multi-modal Attention Network Learning for
Semantic Source Code Retrieval. In ASE.
Wang, Y.; Shi, E.; Du, L.; Yang, X.; Hu, Y.; Han, S.; Zhang,
H.; and Zhang, D. 2021. CoCoSum: Contextual Code Sum-
marization with Multi-Relational Graph Neural Network.
arXiv preprint arXiv:2107.01933.
Yang, Y.; and Huang, Q. 2017. IECS: Intent-enforced code
search via extended Boolean model. Journal of Intelligent
& Fuzzy Systems, 2565–2576.
Ye, Z.; Guo, Q.; Gan, Q.; Qiu, X.; and Zhang, Z. 2019. Bp-
transformer: Modelling long-range context via binary parti-
tioning. arXiv preprint arXiv:1911.04070.
Zhang, J.; Wang, X.; Zhang, H.; Sun, H.; Wang, K.; and Liu,
X. 2019. A novel neural source code representation based on
abstract syntax tree. In 2019 IEEE/ACM 41st International
Conference on Software Engineering (ICSE).
Zhang, X.; Wei, F.; and Zhou, M. 2019. HIBERT: Document
Level Pre-training of Hierarchical Bidirectional Transform-
ers for Document Summarization. In ACL.

