Digital Nets and Sequences for Quasi-Monte Carlo
Methods

2
2
0
2

l
u
J

7
2

]

A
N
.
h
t
a
m

[

1
v
2
0
8
3
1
.
7
0
2
2
:
v
i
X
r
a

HONG, Hee Sun

A thesis submitted in partial fulﬁllment of the requirements

for the degree of

Doctor of Philosophy

May 2002

Hong Kong Baptist University

 
 
 
 
 
 
Abstract

Quasi-Monte Carlo methods are a way of improving the eﬃciency of Monte Carlo
methods. Digital nets and sequences are one of the low discrepancy point sets used
in quasi-Monte Carlo methods. This thesis presents the three new results pertaining
to digital nets and sequences:
implementing randomized digital nets, ﬁnding the
distribution of the discrepancy of scrambled digital nets, and obtaining better quality
of digital nets through evolutionary computation. Finally, applications of scrambled
and non-scrambled digital nets are provided.

i

Contents

Abstract

Table of Contents

List of Figures

List of Tables

Chapter 1

Introduction

1.1 Quasi-Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . .

1.2

Integration Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Digital nets and Sequences . . . . . . . . . . . . . . . . . . . . . . . .

1.4

(t, m, s)-Nets and (t, s)-Sequences . . . . . . . . . . . . . . . . . . . .

1.5 Randomized Quasi-Monte Carlo Methods . . . . . . . . . . . . . . . .

1.6 Outline of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . .

i

ii

v

x

1

2

4

5

6

7

8

Chapter 2

Implementing Randomized Digital (t, s)-Sequences

12

2.1 Owen’s Scrambling . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

2.2 Faure-Tezuka’s Tumbling . . . . . . . . . . . . . . . . . . . . . . . . .

14

2.3

Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

ii

2.3.1 The Method of Scrambling . . . . . . . . . . . . . . . . . . . .

15

2.3.2 Generators for the Randomized Nets

. . . . . . . . . . . . . .

18

2.3.3 Time and Space complexity . . . . . . . . . . . . . . . . . . .

20

2.4 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

Chapter 3 The Distribution of the Discrepancy of Scrambled Digital

(t, m, s)-Nets

30

3.1 The Distribution of the Squared Discrepancy of Scrambled Nets . . .

31

3.2 Fitting The Distribution Of The Square Discrepancy . . . . . . . . .

32

3.3 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

Chapter 4

t-parameter Optimization for Digital (t, m, s)-nets by Evo-

lutionary Computation

45

4.1 Digital (t, m, s)-Nets and (t, s)-Sequences . . . . . . . . . . . . . . . .

46

4.2 Evolutionary Computation . . . . . . . . . . . . . . . . . . . . . . . .

47

4.3 Searching Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

4.3.1 Environment Setting . . . . . . . . . . . . . . . . . . . . . . .

51

4.3.2

Searching Strategy . . . . . . . . . . . . . . . . . . . . . . . .

52

4.3.3 Objective Function . . . . . . . . . . . . . . . . . . . . . . . .

53

4.4 Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54

4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62

Chapter 5 Application

67

5.1 Multivariate Normal Distribution . . . . . . . . . . . . . . . . . . . .

67

5.2 Physics Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71

iii

5.3 Multidimensional Integration . . . . . . . . . . . . . . . . . . . . . . .

72

Chapter 6 Conclusion

Bibliography

78

80

iv

List of Figures

1.1 Dimensions 7 and 8 of the a) glp and b) randomly shifted glp for N = 256 10

1.2 Dimensions 2 and 5 of the a) Sobol’ and b) Scrambled Sobol’ sequences

for N = 256 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

2.1 Dimensions 6 and 7 of the a) Faure, b) Scrambled Faure, and c)Tumbled

Faure sequences for N = 256 . . . . . . . . . . . . . . . . . . . . . . .

24

2.2 The histogram of the square discrepancy for the Owen-scrambled Sobol’

net for K = 10, 20, and 30 for s = 2 and N = 210. . . . . . . . . . . .

25

2.3 The scaled root mean squared discrepancy with α = 2 as a function of

N for the Owen-scrambled Sobol’ net for K = 10 (x), 20 (o), 30 (*),

and unscrambled (.) for s = 2. . . . . . . . . . . . . . . . . . . . . . .

26

2.4 Same as Figure 2.3 for s = 5.

. . . . . . . . . . . . . . . . . . . . . .

26

2.5 The scaled root mean squared discrepancy with α = 2 as a function of

N for the Faure ((cid:3)), the Owen-scrambled Faure (

), and the Faure-

⋄

Tezuka-Tumbled Faure (*) nets for s = 5. . . . . . . . . . . . . . . . .

27

2.6 The scaled root mean square discrepancy with α = 1 as a function

of N for the Owen-scrambled Sobol’ (

) and the Niederreiter-Xing (

)

∗

·

nets for s = 1 and 10.

. . . . . . . . . . . . . . . . . . . . . . . . . .

28

v

2.7 The scaled root mean square discrepancy with α = 2 as a function

of N for Owen-scrambled Sobol’ (

) and Niederreiter-Xing (

) nets for

∗

·

s = 1 and 10.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

2.8 The scaled the root mean square discrepancy with α = 2 as a function

of N for non-scrambled Sobol’ (

) and Niederreiter-Xing (

) nets for

∗

·

s = 1 and 10.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

3.1 The empirical distribution of the square discrepancy (3.4) of a scram-

bled Sobol’ net and its ﬁtted distribution for s = 2 and N = 24, 27, 210

from right to left.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

3.2 Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Sobol’ net with s = 2 and N = 27. 36

3.3 The empirical distribution of the square discrepancy (3.4) of a scram-

bled Sobol’ net and its ﬁtted distribution for s = 5 and N = 24, 27, 210

from right to left.

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

3.4 Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Faure’ net with s = 3 and N = 27. 37

3.5 The empirical distribution of the square discrepancy (3.4) of a scram-

bled Faure net and its ﬁtted distribution for s = 3 and N = 33, 37 from

right to left.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

3.6 Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Faure net with s = 3 and N = 37. 38

vi

3.7 The empirical distribution of the square discrepancy (3.4) of a scram-

bled Niederreiter-Xing net and its ﬁtted distribution for s = 2 and

N = 24, 210 from right to left.

. . . . . . . . . . . . . . . . . . . . . .

38

3.8 Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Niederreiter-Xing net with s = 8

and N = 210.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

3.9 The empirical distribution of the square discrepancy of scrambled Sobol’

net for s = 1 and N = 20,

· · ·

, 210 from right to left. . . . . . . . . . .

40

3.10 The empirical distribution of the square discrepancy of the points from

Latin hypercube sampling for s = 1 and N = 20,

, 210 from right to

· · ·

left. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

3.11 The empirical distribution of D2

j (P ) for the (3,9,5)-net of scrambled

Sobol’ (solid) and (2,9,5)-net of scrambled Niederreiter-Xing (dashed).

Top ﬁgure shows j = 2, 5, 1 and the bottom ﬁgure shows j = 3, 4 from

right to left.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

3.12 The empirical distribution of the square discrepancy (3.4) of a ran-

domly shifted rank-1 lattice and its ﬁtted distribution for s = 5 and

N = 24, 27, 210 from right to left. . . . . . . . . . . . . . . . . . . . . .

44

3.13 A Q-Q plot of the empirical distribution of the squared discrepancy

versus the ﬁtted distribution for a randomly shifted rank-1 lattice with

s = 5 and N = 27.

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

4.1 Dimensions 16 and 18 of the a) EC and b) Scrambled EC’ sequences

for N = 256. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

65

vii

4.2 The scaled root mean square discrepancy with α = 2 as a function

of N for Owen-scrambled EC (o) and Niederreiter-Xing (

) nets for

∗

s = 18 and 32. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

4.3 The scaled root mean square discrepancy with α = 2 as a function of

N for non-scrambled EC’ (o) and Niederreiter-Xing (

) nets for s = 18

∗

and 32. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

66

5.1 Box and whisker plots of scaled errors, E/ǫ, for 50 randomly chosen

test problems (5.1). For each dimension s results from left to the four

algorithms as listed in the text.

. . . . . . . . . . . . . . . . . . . . .

68

5.2 Box and whisker plots of the computation times in seconds for 50

randomly chosen test problems (5.1). For each dimension s results

from left to the four algorithms as listed in the text. . . . . . . . . . .

69

5.3 Box and whisker plots of the number of points used for solving 50

randomly chosen test problems (5.1). For each dimension s results

from left to right, the four algorithms as listed in the text.

. . . . . .

70

5.4 The relative error obtained in approximating (5.2) for s = 25 for

FINDER’s generalized Faure algorithm (

). The box and whisker

◦

plots show the performance of the randomly scrambled Sobol’ points

and randomly shifted extensible rank-1 lattice, for the values of N =

4, 8, . . . , 218.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

5.5 The relative error of the numerical integration of (5.3) for scrambled

Sobol(

∗

) and scrambled EC(o) nets for s = 14 and 24. . . . . . . . . .

73

viii

5.6 The digit accuracy of the numerical integration of Oscillatory func-

tions for EC (.), scrambled EC (o), Niederreiter-Xing (+), scrambled

Niederreiter-Xing (

∗

), and imbedded Niederreiter-Xing ((cid:3)) nets. . . .

5.7 Same as 5.6 for the numerical integration of Product Peak functions.

74

75

5.8 Same as 5.6 for the numerical integration of Corner Peak functions.

.

75

5.9 Same as 5.6 for the numerical integration of Gaussian functions.

. .

76

5.10 Same as 5.6 for the numerical integration of C o functions.

. . . . . .

76

5.11 Same as 5.6 for the numerical integration of Discontinuous functions.

77

ix

List of Tables

2.1 Time required to compute low discrepancy points . . . . . . . . . . .

21

4.1 The T (m, s)-table for generator matrices of EC . . . . . . . . . . . .

55

4.2 Comparison of T (m, s) values between generator matrices of Sobol’

and EC for s = 40 and m = 20.

. . . . . . . . . . . . . . . . . . . . .

56

4.3 Comparison of T (m, s) values between generator matrices of Niederreiter-

Xing and EC for s = 18 and m = 20.

. . . . . . . . . . . . . . . . . .

57

4.4 Comparison of T (m, s) values between generator matrices of Niederreiter-

Xing and EC for s = 32 and m = 20.

. . . . . . . . . . . . . . . . . .

58

4.5 The T avg

r

and T max

r

values for r = 1, 2 and 18 of generator matrices of

Niederreiter-Xing and EC for smax = 18, and mmax = 20.

. . . . . . .

60

4.6 The T avg

r

and T max

r

values for r = 1, 2 and 32 of generator matrices of

Niederreiter-Xing and EC for smax = 32, and mmax = 20.

. . . . . . .

61

4.7 Comparison of t values from [CLM+99, Sch99b] and T (m, s) values of

generator matrices from EC. . . . . . . . . . . . . . . . . . . . . . . .

63

x

Chapter 1

Introduction

We consider the problem of approximating I, the integral of a function f over the

s-dimensional unit cube [0, 1)s:

I =

f (y)dy

Z[0,1)s

Computing such a multidimensional integral is important since it has wide applica-

tions in ﬁnance [CM96, PT96], physics and engineering [Kei96, PT97] and statistics

[Fan80, Gen92, Gen93]. Numerical integration methods, such as tensor product New-

ton Cotes and Gaussian quadrature, become impractical as s increases since the error

of the such methods converges like O(N −p/s) where p is the smoothness of function.

Monte Carlo methods are often a popular choice to estimate such integrals. They

use the sample mean of the integrand evaluated on a set, P , with N randomly selected

independent points drawn from a uniform distribution on [0, 1)s :

ˆIN =

1
N

f (y).

y∈P
X

Unfortunately the convergence rate of the quadrature error for Monte Carlo methods,

O(N −1/2), is relatively low.

1

To overcome the lower convergence rate of Monte Carlo methods, quasirandom

sequences have been introduced. The basic quasirandom sequences replace a random

set P by a carefully chosen deterministic set that is more uniformly distributed on

[0, 1)s, and it often yields a convergence rate of O(N −1logs−1N). The quadrature

methods based on low discrepancy sets are called quasi-Monte Carlo methods [Nie92,

Tez95, HL98, Fox99].

This chapter introduces the basic deﬁnitions of quasi-Monte Carlo methods and

the outline of the remaining thesis.

1.1 Quasi-Monte Carlo Methods

The most important property of quasirandom sequences is an equidistribution prop-

erty. The quality of such uniformity for quasirandom sequences is measured by the

discrepancy, which is a distance between the continuous uniform distribution on [0, 1)s

and the empirical distribution of the yi for i = 1,

, N. First we deﬁne the empirical

· · ·

distribution of the sequence as

FN (y) =

1
N

N

i=1
X

1[yi,∞)(y),

(1.1)

where 1A is the indicator function of the set A. We deﬁne the uniform distribution

on [0, 1)s as

s

Funif(y) =

yj, y = (y1,

j=1
Y

, ys)T

∈

· · ·

[0, 1)s.

(1.2)

The discrepancy arises from the worst case error analysis of quasi-Monte Carlo

quadratures. The Koskma-Hlawka inequality, which provides an important theo-

retical justiﬁcation for the quasi-Monte Carlo quadratures, uses the discrepancy to

2

provide an upper bound quadrature error of quasi-Monte Carlo methods:

ˆIN −

|

I

| ≤

D∗(P )VHK(f )

(1.3)

where VHK is said to have bounded variation f in the sense of Hardy and Krause. If

VHK(f ) <

, then f

∞

∈

BV HK. For further understanding of VHK see Niederreiter

[Nie92]. D∗(P ) indicates the star discrepancy which will be deﬁned next.

The star discrepancy is the most widely studied discrepancy and it is deﬁned as

follows:

D∗(P ) = sup

y∈[0,1)s |

Funif(y)

FN (y)

=

Funif −

FN ||∞.

||

|

−

(1.4)

The star discrepancy can be thought as a special case (p =

) of Lp-star discrep-

∞

ancy which is deﬁned by

D∗

p(P ) =

Funif −

FN ||p, 1

||

≤

p

.

≤ ∞

(1.5)

Another discrepancy, which will be used in this thesis, given by [Hic96], called a

generalized L2-discrepancy, is

D2(P ) =

1 +

−

1
N 2

where the notation

y

{

}

N −1

s

(

r=1 "−
Y

i,j=0
X

γ2)α
−
(2α)!

B2α(

yir −

{

yjr}

α

) +

γ2k
(k!)2 Bi(yir)Bi(yjr)

, (1.6)

#

Xk=0
means the fractional part of a number. The positive integer

α indicates the degree of smoothness of integrands in the underlying Hilbert space,

and the parameter γ measures the relative importance given to the uniformity of

the points in low dimensional projections of the unit cube versus high dimensional

projections of the unit cube. The reproducing kernel leading to this discrepancy is

for a Hilbert space of integrands whose partial derivatives up to order α in each

3

coordinate direction are square integrable [HH99]. It is known that the root mean

square discrepancy for scrambled (t, m, s)-nets decays as O(N −3/2+ǫ) as N

→ ∞

[HH99, HY01] for α

≥

2. Bi(y) is the ith Bernoulli polynomial (see [AS64]). The

ﬁrst few Bernoulli polynomials, which are used in this thesis, are B0(y) = 1 and

B1(y) = y

1
2

,

−

B2(y) = y2

B3(y) = y3

B4(y) = y4

−

−

−

y +

1
6

,

3
2

y2 +

1
2

y,

2y3 + y2

1
30

.

−

See [Hic96] for further discussion about the Lp-star discrepancy and several other

discrepancies.

The following two sections introduce two important families of quasi-Monte Carlo

methods which are :

• integration lattices [Nie92, Slo85], and

• digital nets and sequences [Nie92].

1.2

Integration Lattices

Rank-1 lattices, also known as good lattice point (glp) sets, were introduced by

Korobov[Kor59] and have been widely studied since then. See [SJ94, Nie92] for

further details. The formula for the node points of a rank-1 lattice is simply

P =

ih/N

}

{{

: i = 0,

, N

1

,

}

−

· · ·

(1.7)

where N is the number of points, h is an s-dimensional generating vector of integers

The formula for a lattice is rather simple. However ﬁnding a good generating vector

4

h that makes the lattice have low discrepancy for the chosen N and s is not trivial.

Recently the formula for lattice is extended to an inﬁnite sequence [HHLL01]. This

is done by using the radical inverse function, Qb(i). For any integer b

2, let any

≥

non-negative integer i be represented in base b as i =

i3i2i1, where the digits ik

· · ·

take on values between 0 and b

−

1. The function Qb(i) ﬂips the digits about the

decimal point, i.e.,

Qb(i) = 0.i1i2i3 · · ·

( base b) =

ikb−k.

∞

Xk=1

(1.8)

The sequence

Qb(i) : i = 0, 1,

{

· · · }

is called the van der Corput sequence. An inﬁnite

sequence of imbedded lattices is deﬁned by replacing i/N by Qb(i), i.e.,

yi =

Qb(i)h
}

{{

: i = 0, 1,

,

· · · }

(1.9)

where the ﬁrst N points of this sequence are a lattice whenever N is a power of b

[HHLL01].

1.3 Digital nets and Sequences

Digital nets and sequences [Lar98b, Nie92] are one widely used types of low discrep-

ancy point sets.

Let b

≥

2 denote a prime number. For any non-negative integer i =

i3i2i1(base b),

· · ·

1 vector ψ(i) as the vector of its digits, i.e., ψ(i) = (i1, i2, . . .)T .

we deﬁne the

∞ ×
For any point z = 0.z1z2 · · ·

(base b)

∈

[0, 1), let φ(z) = (z1, z2, . . .)T denote the

1 vector of the digits of z. Let C1, . . . , Cs denote predetermined

∞ ×

∞ × ∞

generator matrices. The digital sequence in base b is

y0, y1, y2, . . .
}

{

, where each

5

yi = (yi1, . . . , yis)T

∈

[0, 1)s is deﬁned by

φ(yij) = Cjψ(i),

j = 1, . . . , s, i = 0, 1, . . . .

(1.10)

Here all arithmetic operations take place in mod b. Thus, the right side of (1.10)

should not give a vector ending in an inﬁnite trail of b

1s.

−

Digital nets and sequences are the special cases of (t, m, s)-nets and (t, s)-sequences

and further explanation is given in Chapter 4.

1.4

(t, m, s)-Nets and (t, s)-Sequences

The concept of t in (t, m, s)-nets and (t, s)-sequences provides one way to measure the

quality of low discrepancy sequences. See [Nie92] for more detail explanations. Here

we provide a brief outline of elementary intervals of (t, m, s)-nets and (t, s)-sequences.

Let s

≥

1 and b

≥

of [0, 1)s of the form

2 be integers. An elementary interval in base b is a subinterval

s

r=1 (cid:20)
Y

ar
bkr

,

ar + 1
bkr

(cid:19)

ar < bkr . Such an elementary interval has volume b−(k1+···+ks).

with integer kr ≥

0, 0

≤

Let m

≥

0 be an integer. A ﬁnite set of N = bm points from [0, 1)s is a (t, m, s)-net

in base b if every elementary interval in base b with the volume bt−m contains exactly

bt points of a set, where t is a nonnegative integer. Smaller t values imply a better

equidistribution property for the net. Obviously the best case is when t = 0, that is

every b-ary box of volume 1/N contains exactly one of the N points of the set.

For a given t

≥

0, an inﬁnite sequence of points from [0, 1)s is a (t, s) sequence in

base b if for all integers k

0 and m

≥

≥

t the ﬁnite sequence

ykbm+1,

{

· · ·

, y(k+1)bm

is

}

a (t, m, s)-net in base b.

6

The constructions of (t, m, s)-nets and (t, s)-sequences in base b = 2 were in-

troduced by Sobol’ [Sob67]. Later Faure [Fau82] provided (0, m, s)-nets and (0, s)-

sequences for prime bases b

≥

s. The general deﬁnitions are given by Niederreiter

[Nie87]. Mullen, Mahalanabis and Niederreiter [Nie95] provide tables of attainable

t-values for nets.

The advantage of using nets taken from (t, s)-sequences is that one can increase

N through a sequence of values N = λbm, 1

λ < b, and all the points used in ˆIλbm

≤

are also used in ˆI(λ+1)bm . Owen [Owe97a] introduces the (λ, t, m, s)-nets to describe

such sequences.

The initial λbm points of a (t, s)-sequence are well distributed. For integers

m, t, b, λ with m

0, 0

t

≤

≥

≥

m, and 1

≤

λ < b, a sequence yi of λbm points

is called a (λ, t, m, s)-net in base b if every b-ary box of volume bt−m contains λbt

points of the sequence and no b-ary box of volume bt−m−1 contains more than bt

points of the sequence.

For functions of bounded variation in the sense of Hardy and Krause, the nu-

merical integration by averaging over the points of a (t, m, s)-net has an error of

order N −1(log N)s−1. Niederreiter [Nie92] discusses more precise error bounds for

(t, m, s)-nets and (t, s)-sequences.

1.5 Randomized Quasi-Monte Carlo Methods

Quasi-Monte Carlo methods can obtain a better convergence rate than Monte Carlo

methods, because the points are chosen to be more uniform. However deterministic

quasi-Monte Carlo methods have disadvantages compared with Monte Carlo methods.

7

First, quasi-Monte Carlo methods are statistically biased since the points are chosen

in certain deterministic way. Therefore the mean of the quadrature estimate is not

the desired integral. Second, quasi-Monte Carlo methods do not facilitate simple

error estimates while Monte Carlo methods provide straightforward probabilistic error

estimates.

Randomizing quasi-Monte Carlo point is one way to overcome such disadvantages

while still preserving their higher convergence rates. One may think of randomized

quasi-Monte Carlo as a sophisticated variance reduction technique. There are two

types of randomization. One is adding the same s-dimensional random shift to every

point. Let P be the original low discrepancy point set, then Psh =

ih/N + ∆
}

{

:

i = 0,

, N

1

}

−

· · ·

, where ∆ is a random vector uniformly distributed on [0, 1)s. This

type of randomization, which was introduced in [CP76], is often used with lattice rule

because shifted lattice rules retain their lattice structure. However, shifted nets do

not necessarily remain as nets. Another type of randomization, which was proposed

by Art Owen [Owe95], is a more sophisticated method that randomly permutes the

digits of each point. And it often applies to nets because scrambled nets retain their

nets properties. However, scrambled lattice rules do not necessary remain lattice

rules. The more detail discussions on Owen’s scrambling can be found in Chapter 2.

1.6 Outline of the Thesis

In this chapter we have introduced the necessary background of quasi-Monte Carlo

methods.

8

Chapter 2 discusses the randomization of the quasi-Monte Carlo methods specif-

ically randomizing digital nets. Detailed descriptions of Owen’s randomization (or

scrambling) and Faure-Tezuka’s randomization (or tumbling) are provided. The ac-

tual implementation of randomization for digital (t, s)-sequences and some numerical

results are presented.

Chapter 3 explores the distribution of the discrepancy of scrambled digital (t, m, s)-

nets which is based on recent theory. We ﬁt the empirical distribution by a sum of chi

squared random variables. The distribution of the discrepancy of randomized lattice

points is presented also.

Finding better digital nets is the main content of Chapter 4. Here we use optimiza-

tion methods to ﬁnd the generator matrices for digital (t, m, s)-nets. Evolutionary

computation is introduced as a tool for a ﬁnding better net.

Chapter 5 presents the applications of the sequences that we have generated.

Various problems are explored and the performance of the new sequences is compared

with other existing low discrepancy sequences.

Finally the thesis ends with some concluding remarks.

9

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.1

0.2

0.3

0.4

0.5
a

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5
b

0.6

0.7

0.8

0.9

1

Figure 1.1: Dimensions 7 and 8 of the a) glp and b) randomly shifted glp for N = 256

10

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.1

0.2

0.3

0.4

0.5
a

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5
b

0.6

0.7

0.8

0.9

1

Figure 1.2: Dimensions 2 and 5 of the a) Sobol’ and b) Scrambled Sobol’ sequences

for N = 256

11

Chapter 2

Implementing Randomized Digital

(t, s)-Sequences

This chapter introduces randomized digital (t, s)-sequences. Section 2.1 explains the

details of Owen-scrambling and Section 2.2 introduces Faure-Tezuka-tumbling. Sec-

tion 2.3 mainly deals with our method of scrambling that is nearly as general as

Owen’s scrambling and the detailed explanation of the implementation. Section 2.4

explains our eﬀorts to improve the eﬃciency of the generation time of the scrambled

and non-scrambled sequences and discusses the time and the complexity of our algo-

rithm. Section 5 presents various numerical results on the discrepancy for the newly

generated randomized (t, s)-sequences.

2.1 Owen’s Scrambling

Owen [Owe95, Owe97a, Owe97b, Owe98, Owe99] proposes scrambled (t, s)-sequences

as a hybrid of Monte Carlo and quasi-Monte Carlo methods. This clever randomiza-

12

tions of quasi-Monte Carlo methods can combine the best of both by yielding higher

accuracy with practical error estimates. The following is a detailed description of

Owen’s scrambling.

Let

x0, x1, x2, . . .
}

{

denote the randomly scrambled sequence proposed by Owen.

Let xijk denote the kth digit of the jth component of xi, and similarly for yijk. Then

xij1 = πj(yij1),

xij2 = πyij1(yij2),

xij3 = πyij1,yij2(yij3),

. . . ,

xijk = πyij1,yij2,··· ,yijk−1(yijk), . . .

where the πa1a2... are random permutations of the elements in mod b chosen uniformly

and mutually independently. Owen [Owe98] provides a geometrical description to

help visualize his scrambling as follows: The rule for choosing xij is like cutting the

unit cube into b equal parts along the xj axis and then reassembling these parts in

random order to reform the cube. The rule for choosing xij2 is like cutting the unit

cube into b2 equal parts along the xj axis, taking them as b groups of b consecutive

parts, and reassembling the b parts within each groups in random order. The rule

for xijk involves cutting the cube into bk equal parts along the xj axis, forming bk−1

groups of b equal parts, and reassembling the b parts within each group in random

order.

Owen [Owe95, Owe97b] states that a randomized net inherits certain equidistri-

bution properties of (t, m, s)-nets by proving the following propositions:

Proposition 1

yi}
in base b with probability 1.

If

{

is a (λ, t, m, s)-net in base b then

xi}

{

is a (λ, t, m, s)-net

13

Proposition 2

Let y be a point in [0, 1)s and let x be the scrambled version of

y as described above. Then x has the uniform distribution on [0, 1)s.

2.2 Faure-Tezuka’s Tumbling

Faure and Tezuka [FT00] proposed another type of randomizing digital nets and

sequences. However the eﬀect of the Faure-Tezuka-randomization can be thought

as re-ordering the original sequence, rather than permuting its digits like the Owen-

scrambling. Thus we refer to this method as Faure-Tezuka-tumbling as suggested

by Art Owen (personal communication). The following describes details of Faure-

Tezuka-tumbling.

For any m, λ = 0, 1, . . ., let i = λbm, . . . , (λ+1)bm

1. Then ψ(i) takes all possible

−

values in its top m rows. By the same token LT ψ(i) + e takes on all possible values in

its top m rows, but not necessarily in the same order. Therefore, the Faure-Tezuka-

tumbled (t, m, s)-net,

zλbm, . . . , z(λ+1)bm−1}

{

, obtained by replacing ψ(i) of (1.10) by

LT ψ(i) + e is just the same set as the original (t, m, s)-net,

yνbm, . . . , y(ν+1)bm−1}

,

{

given by (1.10) for some ν.

2.3

Implementation

There is a cost in scrambling (t, s)-sequences. The manipulation of each digit of

each point requires more time than for an unscrambled sequence. In addition Owen’s

scrambling can be rather tedious because of the bookkeeping required to store the

many permutations. Here we present an alternative method that is only slightly less

general than Owen’s proposal but is an eﬃcient method of scrambling that minimizes

14

this cost. This is called an Owen-scrambling to recognize that it is done in the spirit

of Owen’s original proposal.

2.3.1 The Method of Scrambling

Let L1,

· · ·

be an

∞ ×

, Ls, be nonsingular lower triangular

∞ × ∞

matrices and let e1,

, es

· · ·

1 vector. Assume that for all j any linear combination of columns of LjCj

plus ej does not end in an inﬁnite trail of b

1s. A particular Owen-scrambling

xi}

{

−

of a digital sequence

yi}

{

is deﬁned as

φ(xij) = Ljφ(yij) + ej = LjCjψ(i) + ej,

j = 1, . . . , s, i = 0, 1, . . . .

(2.1)

The left multiplication of each generator matrix Cj by a nonsingular lower triangular

matrix Lj yields a new generator matrix that satisﬁes the same condition (4.1) as the

original. The addition of an ej just acts as a digital shift and also does not alter the

t-value. Therefore the scrambled sequence is also a digital (t, s)-sequence.

To get a randomly scrambled sequence one chooses the elements of the Lj and ej

randomly. The resulting randomized sequence has properties listed in the theorem

below. Although these properties are not equivalent to the Owen’s original random-

ization, they are suﬃcient for the analysis of scrambled net quadrature rules given in

[Owe95, Hic96, Owe97a, Owe97b, Owe98, HH99, Yue99, YM99, HY01].

{

xi}
, Ls, e1,

Theorem 1 Let

be an Owen-scrambling of a digital (t, s)-sequence where ele-

ments of L1,

· · ·

, es are all chosen randomly and independently. The di-

· · ·

agonal elements of L1,

· · ·

, Ls are chosen uniformly on

1, . . . , b

{

1

}

−

, and the other

elements are chosen uniformly on Zb. Then this randomly scrambled sequence satis-

ﬁes the following properties almost surely:

15

i. xijk is uniformly distributed on Zb;

ii.

if yijh = yℓjh for h < k and yijk 6

= yℓjk, then

a. xijh = xℓjh for h < k

b.

(xijk, xℓjk) are uniformly distributed on

(x, y)

{

∈

Z2

b : x

= y

;

}

c. xijh, xℓjh are independent for h > k

iii.

(xij, xℓj) is independent from (xir, xℓr) for j

= r

Proof The qualiﬁcation “almost surely” is required to rule out the zero probability

event that for some j there exists a linear combination of columns of LjCj plus ej

that ends in an inﬁnite trail of b

−

1s. Assertion iii. follows from the fact that the

elements of Lj, ej, Lr, and er are chosen independently of each other. Now the other

two assertions are proved.

Note from (2.1) that xijk = lT

jkφ(yij) + ejk, where lT

jk is the kth row of Lj, and

all arithmetic operations are assumed to take place in the ﬁnite ﬁeld. Since ejk is

distributed uniformly on Zb, it follows that xijk is also distributed uniformly on Zb.

To prove ii. consider φ(xij)

φ(xℓj) = Lj[φ(yij)

−

−

φ(yℓj)]. Under the assumption

of ii. the ﬁrst k

1 rows of [φ(yij)

−

−

φ(yℓj)] are zero, which implies that the ﬁrst k

1

−

rows of φ(xij)

φ(xℓj) are also zero, since Lj is lower triangular. This immediately

jk[φ(yij)

xℓjk = lT

−
implies iia. Furthermore, xijk −
−
ljkk is the kth diagonal element of Lj. Since yijk −
nonzero element of Zb, it follows that xijk −
Zb. Combined with i., this implies iib. For h > k it follows that xijh −
lT
jh[φ(yij)
yℓjk 6

φ(yℓj)] = ljkk(yijk −
yℓjk 6

. The fact that yijk −

+ ljhk(yijk −

φ(yℓj)] =

yℓjk) +

· · ·

· · ·

−

yℓjk), where

= 0, and ljkk is a random

xℓjk is a random nonzero element of

xℓjh =

= 0, and

16

6
6
ljhk is uniformly distributed on Zb, combined with i. now imply iic.

To understand why the randomization given by (2.1) is not as rich as that origi-

nally prescribed by Owen, consider randomizing yi11, the ﬁrst digit of yi1 for diﬀerent

values of i. The yi11 take on b diﬀerent values, and there are b! possible permutations

of these values. However, the formula given by (2.1) is xi11 = l111yi11 + e11, where

ljhk denotes the h, k element of Lj, and ejk denotes the kth element of ej. Since there

are only b

−

1 possible values for l111 and only b possible values for e11, this formula

cannot give at most b(b

1) permutations.

−

The Faure-Tezuka-tumbling [FT00] randomizes the digits of i before multiplying

by the generator matrices. Let L, be a nonsingular lower triangular

matrix,

∞ × ∞

and let e be an

∞ ×

1 vector with a ﬁnite number of nonzero elements. A particular

tumbling,

zi}

{

, of a digital sequence is deﬁned as

φ(zij) = Cj[LT ψ(i) + e],

j = 1, . . . , s, i = 0, 1, . . . .

(2.2)

A particular scrambling-tumbling,

xi}

{

, of a digital sequence is deﬁned as

φ(xij) = LjCj[LT ψ(i) + e] + ej,

j = 1, . . . , s, i = 0, 1, . . . .

(2.3)

in order to obtain random Faure-Tezuka-tumbling the elements of L and e are chosen

randomly.

The scrambling method described here applies only to digital (t, s)-sequences.

Since all known general constructions of (t, m, s)-nets and (t, s)-sequences are digital

constructions [Sob67, Fau82, Nie88, Lar98a, Lar98b, NX98], this restriction is not

serious. The algorithm we implement here builds on the algorithms of [BF88, BFN92]

and includes the recent digital Niederreiter-Xing sequences by [NX96, NX98].

17

2.3.2 Generators for the Randomized Nets

Generators for scrambled Sobol’ [Sob67], Faure [Fau82], and Niederreiter [Nie88]

points have been programmed following the Fortran codes of [BF88, BFN92]. Below

the diﬀerences and improvements that we have made are highlighted and explained.

The code for the Niederreiter points has been extended to higher dimension by Giray

¨Okten, and we have employed this extension. A generator for Niederreiter-Xing

points has been coded based on the generator matrices provided by [Pir02].

The diﬀerent scrambled or non-scrambled sequences given by (1.10), (2.1), and

(2.2) are just special cases of (2.3). Thus, (2.3) is the basic algorithm to be imple-

mented, with diﬀerent choices given to the user as to how to choose the matrices

L1, . . . , Ls, L and the vectors e1, . . . , es, e.

In practice one cannot store all the entries of an

matrix. Assuming that

∞ × ∞

bmmax is the maximum amount of points that is required in one run, and that K is

the number of digits to be scrambled. Then, one can restrict L1, . . . , Ls to K

mmax

×

matrices, C1, . . . , Cs, L to mmax ×
to a mmax ×
but Section 2.4 provides evidence that this is not necessary.

mmax matrices, e1, . . . , es to K

1 vector. One might think that it is best to scramble all available digits,

1 vectors, and e

×

The value of Kmax, the maximum possible value of K, is 31, which corresponds

to the number of bits in the largest integer available. When b = 2, the algorithms for

generating digital sequences may be implemented more eﬃciently by storing an array

of bits as an integer. For the scrambled Sobol’ generator the value of s, the maximum

dimension is 40, the same as in [BF88]. For the scrambled Faure sequence, s = 500,

and for the scrambled Niederreiter generator s = 318. For the Niederreiter-Xing

18

points S = 16 based on the generator matrices available so far. The program may be

modiﬁed to allow for higher dimensions as the generator matrices become available.

To save time some matrices and vectors are premultiplied so that (2.3) becomes

φ(xij) = ˜Cjψ(i) + ˜ej,

j = 1, . . . , s, i = 0, 1, . . . .

(2.4)

where ˜Cj = LjCjLT , and ˜ej = LjCje + ej.

Another time saving feature is to use a gray code [Lic98]. Recursively deﬁne the

1 vector function ˜ψ(i) as follows:

∞ ×

˜ψ(0) = (0, 0, . . .)T

˜ψ(i + 1) = ˜ψ(i)

δk,ˆki+1,

−

where ˆki = min

k :

{

ib−k
⌊

= ib−k

⌋ 6

,

}

δkl is the Kronecker delta function, and where again all arithmetic operations take

place in the ﬁnite ﬁeld. For example, if b = 3, then

˜ψ(1) = (2, 0, 0 . . .)T ,

˜ψ(2) = (1, 0, 0 . . .)T ,

˜ψ(3) = (1, 2, 0 . . .)T ,

...

Note that only one digit of ˜ψ changes as the argument is increased by one. Replacing

ψ(i) by ˜ψ(i) in (2.4) still results in a scrambled digital sequence; it just has the eﬀect

of re-ordering the points. The eﬃciency advantage is that the digits of the i + 1st

point can be obtained from those of the ith point by the iteration:

φ(xi+1,j) = φ(xij)

˜Cjˆki+1,

−

19

(2.5)

where ˜Cjl is the lth column of ˜Cj.

The implementations of the scrambled Sobol’ and Niederreiter sequences closely

followed the structures of the original program and our changes were rather minor.

However, the structure of the Faure sequence generator has been altered more sub-

stantially to improve eﬃciency. The scrambled Sobol’, Niederreiter, and Niederreiter-

Xing generators all have base b = 2, which allows the use of logical operations instead

of additions and multiplications.

2.3.3 Time and Space complexity

This section considers the number of operations and the amount of memory required

to generate the ﬁrst N points of a scrambled (t, s)-sequence with bm−1 < N

bm. The

≤

space required to store the scrambled generator matrices is O(smK), where K

m

≥

is the number of digits scrambled. The time required to generate and pre-multiply

the matrices is O(sm2K). To generate one new point from the scrambled sequence

requires just O(sK) operations according to (2.5), so the total number of operations

to generate all points is O(sNK). Thus, the preparations steps take proportionally

negligible time as m

→ ∞

. Note that for unscrambled points the corresponding time

and space complexities can be obtained by replacing K by m.

The following table shows time comparison of sequence generating time of un-

scrambled and scrambled digital point sets using K = m + 15. The times have been

normalized so that the time to generate the Sobol’ set equals 1. From the table it

shows that generating time for scrambled Sobol’, Niederreiter and Faure sequences

are about 2 times more than that of the original sequences based on the same number

of extra digit scrambling. This is due the fact that one must manipulate K digits of

20

Table 2.1: Time required to compute low discrepancy points

Algorithm SOBOL SSOBOL NIED SNIED Old FAURE FAURE SFAURE

Time

1

2

2

2

10

3

6

each number, not just m. The new Faure generator takes about 1/3 the time of the

original one due to the use of the gray code.

2.4 Numerical Results

The discrepancy used in here is the squared discrepancy (1.6) and it has been scaled

by a dimension-dependent constant so that the root mean square scaled discrepancy

of a simple random sample is N −1/2.

Figures 2.2 shows the histogram of the distribution of the square discrepancy of

the particular Owen-scrambled Sobol’ sequence for the diﬀerent numbers of scrambled

digits, K. K has been chosen to be 10, 20, and 30 for s = 2 and m = 10 with

200 independent replications. The range of the x-axis is are 10−15, . . . , 10−6 in this

ﬁgure. From the ﬁgure, there is only little distinction in the distribution of the square

discrepancy for K = 20 and 30. However a larger number of scrambled digits produces

a wider range of discrepancies. From Figure 2.3 the root mean square discrepancies

for K = 20 and 30 are nearly the same. However the root mean square discrepancy

for K = 10 looses the beneﬁt from scrambling as m increases.

Figures 2.3 and 2.4 plot the discrepancy of the particular Owen-scrambled Sobol’

sequence for diﬀerent choices of K and the choices are same as before. The choices

21

of dimension are s = 2 and 5. These ﬁgures show the root mean square discrep-

ancy of 100 diﬀerent replications. There is almost no diﬀerence in the root mean

square discrepancy between scrambling K = 20 or 30 digits. However for K = 10

the discrepancy looses its superior performance as N increases, which indicates an

insuﬃcient number of scrambled digits. Also notice that choosing the number of

scrambled digits is independent from s.

Figure 2.5 plots the discrepancies of the Faure sequence, the scrambled Faure

sequence, and the tumbled Faure sequence. The number of scrambled digits, K, is

chosen to be 20, and 100 diﬀerent replications have been performed. The choice of

dimension is s = 5. Since the Faure sequence takes a base b as a smallest prime

number which is equal to or greater than s, b assigned to be 5. Therefore N is chosen

to be λ5m, where λ = 1,

, b

−

· · ·

1 and m = 1,

· · ·

, 4. From Figure 2.5 scrambled

sequences perform the best among all sequences and the tumbled sequence performs

better than the original Faure sequence. Also notice that the scrambling and tumbling

procedure help to ﬂatten out the humps, which are present in the original sequence

when the value of λ

= 1.

Figures 2.6–2.8 show the root mean square discrepancies of randomly scrambled

(t, m, s)-nets in base 2 with N = 2m points that have been calculated by using 100

diﬀerent replications. The choices of dimension are s = 1 and 10. The number of

scrambled digits, K, is chosen to be 31.

Theory predicts that the root mean square discrepancy of nets should decay as

O(N −1) for α = 1, and as O(N −3/2) for α = 2. Figures 2.6–2.8 display this behavior,

although for larger s the asymptotic decay rate is only attained for large enough N.

However, even for smaller N the discrepancy decays no worse than O(N −1/2), the

22

6
decay rate for a simple Monte Carlo sample.

The discrepancy of unscrambled nets does not attain the O(N −3/2) decay rate

for α = 2. Although for these experiments all possible digits were scrambled, our

experiment suggests that scrambling about K = m + 10 digits is enough to obtain

the beneﬁts of scrambling. Scrambling more than this number does not improve the

discrepancy but increases the time required to generate the scrambled points.

23

1

0.5

0

0

1

0.5

0

0

1

0.5

0

0

0.1

0.2

0.3

0.4

0.1

0.2

0.3

0.4

0.1

0.2

0.3

0.4

0.5
a

0.5
b

0.5
c

0.6

0.7

0.8

0.9

1

0.6

0.7

0.8

0.9

1

0.6

0.7

0.8

0.9

1

Figure 2.1: Dimensions 6 and 7 of the a) Faure, b) Scrambled Faure, and c)Tumbled

Faure sequences for N = 256

24

10−15

10−14

10−13

10−12

10−11

10−10
K = 10

10−9

10−8

10−7

10−6

10−5

10−15

10−14

10−13

10−12

10−11

10−10
K = 20

10−9

10−8

10−7

10−6

10−5

10−15

10−14

10−13

10−12

10−11

10−10
K = 30

10−9

10−8

10−7

10−6

10−5

Figure 2.2: The histogram of the square discrepancy for the Owen-scrambled Sobol’

net for K = 10, 20, and 30 for s = 2 and N = 210.

25

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

y
c
n
a
p
e
r
c
s
D

i

−6

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.3: The scaled root mean squared discrepancy with α = 2 as a function of N

for the Owen-scrambled Sobol’ net for K = 10 (x), 20 (o), 30 (*), and unscrambled

(.) for s = 2.

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

y
c
n
a
p
e
r
c
s
D

i

−6

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.4: Same as Figure 2.3 for s = 5.

26

0
10

−1

10

−2

10

−3

10

−4

10

y
c
n
a
p
e
r
c
s
D

i

−5

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.5: The scaled root mean squared discrepancy with α = 2 as a function of

N for the Faure ((cid:3)), the Owen-scrambled Faure (

), and the Faure-Tezuka-Tumbled

⋄

Faure (*) nets for s = 5.

27

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

y
c
n
a
p
e
r
c
s
D

i

N−0.5

N−1

−8

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.6: The scaled root mean square discrepancy with α = 1 as a function of N

for the Owen-scrambled Sobol’ (

) and the Niederreiter-Xing (

) nets for s = 1 and

∗

·

10.

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

y
c
n
a
p
e
r
c
s
D

i

N−0.5

N−1.5

N−2

−8

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.7: The scaled root mean square discrepancy with α = 2 as a function of N

for Owen-scrambled Sobol’ (

) and Niederreiter-Xing (

) nets for s = 1 and 10.

∗

·

28

1
10

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

y
c
n
a
p
e
r
c
s
D

i

N−0.5

N−1

N−1.5

N−2

−8

10

0
10

1
10

2
10
N

3
10

4
10

Figure 2.8: The scaled the root mean square discrepancy with α = 2 as a function of

N for non-scrambled Sobol’ (

) and Niederreiter-Xing (

) nets for s = 1 and 10.

∗

·

29

Chapter 3

The Distribution of the

Discrepancy of Scrambled Digital

(t, m, s)-Nets

Recently, Loh [Loh01] proved that the scrambled net quadrature obeys a central limit

theorem. This chapter studies the empirical distribution of the square discrepancy of

scrambled nets and compares it to what one would expect from Loh’s central limit

theorem. Here we use scrambling techniques mentioned in Chapter 2. Organization of

the chapter is as follows. Section 3.1 provides a general description of the discrepancy

with respect to the error of the quadrature and derives the theoretical asymptotic

distribution of the square discrepancy of scrambled nets. Section 3.2 explains the

procedure for ﬁtting the empirical distribution of the square discrepancy of scrambled

nets to the theoretical asymptotic distribution. Section 3.3 discusses the experimental

results.

30

3.1 The Distribution of the Squared Discrepancy

of Scrambled Nets

The discrepancy measures the uniformity of the distribution of a set of points and

it can be interpreted as the maximum possible quadrature error over a unit ball of

integrands. Since the discrepancy depends only on the quadrature rule, it is often

used to compare diﬀerent quadrature rules.

Let K(x, y) be the reproducing kernel for some Hilbert space of integrands whose

domain is [0, 1)s. To approximate the integral

f (x) dx

Z[0,1)s

one may use the quasi-Monte Carlo quadrature rule

f (z),

1
N

Xz∈P

where P

⊂

[0, 1)s is some well-chosen set of N points. The error of this rule is

Err(f ) =

f (x) dx

Z[0,1)s

1
N

−

f (z),

Xz∈P

and the square discrepancy is

D2(P ) = Errx(Erry(K(x, y))),

where Errx implies that the error functional is applied to the argument x [Hic99].

The reproducing kernel may be written as the (usually inﬁnite) sum

K(x, y) =

φν(x)φν(y),

ν
X

31

where

φν}

{

is a basis for the Hilbert space of integrands [Wah90]. This implies that

the discrepancy may be written as the sum [HW01]

D2(P ) =

[Err(φν, P )]2.

ν
X

By Loh’s central limit theorem it is known that each Err(φν, P ) is asymptotically

distributed as a normal random variable with mean zero. However, the Err(φν, P )

are in general correlated.

By making an orthogonal transformation one may write

D2(P )

∞

≈

ν=1
X

βνXν,

(3.1)

are some

β2 · · ·
then D2(P )/β

· · ·

where the Xν are independent χ2(1) random variables, and the β1 ≥
constants. Note that if β1 =

= βn = β and 0 = βn+1 = βn+2 =

· · ·

is approximately distributed as χ2(n).

3.2 Fitting The Distribution Of The Square Dis-

crepancy

Since equation (3.1) involves an inﬁnite sum it must be further approximated in

order to be computationally feasible. This is done by approximating all but the ﬁrst

n terms by a constant:

where

D2(P )

n

≈

ν=1
X

∞

βνXν + c

(3.2)

Here E denotes the expectation.

c = E[

βνXν].

ν=n+1
X

32

Let Z denote the random variable given by the square discrepancy of a scrambled

net, and let W denote the random variable given by the right hand side of (3.2).

Let GZ and GW denote the probability distribution functions of these two random

variables, respectively. Ideally, one would ﬁnd the βν and c that minimize some loss

function involving GZ and GW , but these distributions are not known exactly. Thus,

the optimal βν and c are found by minimizing

M

i=1
X

[log( ˆG−1

Z (pi))

−

log( ˆG−1

W (pi)]2.

(3.3)

Here,

and

pi = (2i

−

1)/(2M), for i = 1, . . . , M,

ˆG−1

Z (pi) = Z(i),

the ith order statistic from a sample of M scrambled net square discrepancies. Also,

ˆG−1

W (pi) = W(ki),

the kth

i order statistic from a sample of L independent and identically distributed

drawings of W , where

ki = iL/M

(L

−

−

M)/(2M),

and L is an odd multiple of M. The logarithm is used in (3.3) because for a ﬁxed

N and s the square discrepancy values can vary by a factor of 100 or more, and it is

not good for the larger values to unduly inﬂuence the ﬁtted distribution.

Fitting the distribution of the square discrepancy by minimizing (3.3) relies on

several approximations. The central limit theorem is invoked to obtain (3.1). The

inﬁnite sum is replaced by a ﬁnite one in (3.2). The probability distributions of each

33

side of (3.2) are approximated by Monte Carlo sampling, and the two probability

distributions are compared at only a ﬁnite number of points. In spite of these ap-

proximations the ﬁtted distribution matches the observed distribution of the square

discrepancy rather well.

3.3 Numerical Results

The discrepancy considered here is (1.6). With α = 2 and γ = 1 the discrepancy can

be rewritten as

D2(P ) =

1 +

−

1
N 2

N

s

y,y′∈P
X

j=1 (cid:26)
Y

1 + γ

B1(yj)B1(y′
(cid:20)

j) +

1
4

B2(yj)B2(y′
j)

1
24

−

B4({yj −

y′

j})

(3.4)

(cid:21)(cid:27)

Figures 3.1 and 3.3 show the empirical distribution (dashed line) of the square

discrepancy of scrambled nets and its ﬁtted value (thin line). The ﬁt is based on

M = 1000 replications of the scrambled Sobol’ [BF88, HH01], Niederreiter-Xing, and

Faure nets, L = 11000, and n = 3 or 4. The optimization was done by using MAT-

LAB’s fminsearch function. The ﬁts are good in the middle of the distributions,

but oﬀ in the tails. Moreover the Q-Q plots of the empirical and ﬁtted distributions

bear this out.

There are several humps in the empirical distribution of the square discrepancy

for s = 2. Moreover as N increases, the number of humps increases. However, these

humps are less noticeable as the dimension increases.

The humps can be explained as follows: The scrambling technique used here, as

34

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−12

−10

10

−8

10

−6

10

−4

10

−2

10

D2(P)

Figure 3.1: The empirical distribution of the square discrepancy (3.4) of a scrambled

Sobol’ net and its ﬁtted distribution for s = 2 and N = 24, 27, 210 from right to left.

described in Chapter 2, scrambles the generator matrices of these digital nets and

also gives them a digital shift. Since this technique preserves the digital nature of the

nets, it can be shown that for l = 0, 1, . . . the sum of the digits of points numbered

lb2, . . . , (l + 1)b2

−

1, is zero modulo b. However, for Owen’s original scrambling

this is not necessarily the case. For example, in one dimension Owen’s scrambling

is equivalent to Latin hypercube sampling. Figures 3.9 and 3.10 show the empirical

distribution of the square discrepancy for a scrambled one-dimensional Sobol’ net and

for Latin hypercube sampling. Since the base of the Sobol’ sequence is 2 the diﬀerence

in the two graphs emerges for N

≥

b2 = 4. Note that although the distributions of

the square discrepancy for Owen’s original scrambling and the variant used here are

somewhat diﬀerent, the means of the two distributions are the same.

35

−5

10

−6

10

)

(

P
2
D

−7

10

−8

10

−9

10

−9

10

−8

10

−7

10
FD2(P)

−6

10

−5

10

Figure 3.2: Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Sobol’ net with s = 2 and N = 27.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−9

−8

10

−7

10

−6

10

−5

10

−4

10

−3

10

−2

10

D2(P)

Figure 3.3: The empirical distribution of the square discrepancy (3.4) of a scrambled

Sobol’ net and its ﬁtted distribution for s = 5 and N = 24, 27, 210 from right to left.

36

−5

10

)

(

P
2
D

−6

10

−7

10

−8

10

−8

10

−7

10

−6

10

−5

10

FD2(P)

Figure 3.4: Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Faure’ net with s = 3 and N = 27.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−12

−10

10

−8

10

−6

10

−4

10

−2

10

D2(P)

Figure 3.5: The empirical distribution of the square discrepancy (3.4) of a scrambled

Faure net and its ﬁtted distribution for s = 3 and N = 33, 37 from right to left.

37

−7

10

−8

10

−9

10

−10

10

−11

10

)

P

(

2
D

−12

10

−12

10

−11

10

−10

10

−9

10

−8

10

−7

10

FD2(P)

Figure 3.6: Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Faure net with s = 3 and N = 37.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−7

−6

10

−5

10

−4

10
D2(P)

−3

10

−2

10

−1

10

Figure 3.7: The empirical distribution of the square discrepancy (3.4) of a scrambled

Niederreiter-Xing net and its ﬁtted distribution for s = 2 and N = 24, 210 from right

to left.

38

−5

10

)

P

(

2
D

−6

10

−7

10

−7

10

−6

10
FD2(P)

−5

10

Figure 3.8: Q-Q plot of the empirical distribution of the square discrepancy versus

the ﬁtted distribution for a scrambled Niederreiter-Xing net with s = 8 and N = 210.

39

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−14

−12

10

−10

10

−8

10

−6

10

−4

10

−2

10

0
10

D2(P)

Figure 3.9: The empirical distribution of the square discrepancy of scrambled Sobol’

net for s = 1 and N = 20,

, 210 from right to left.

· · ·

The square discrepancy can be written as a sum of polynomials in γ:

D2(P, K) =

(γ

+

γ0) ˜D2

1(P, K) + (γ

γ0)(γ

−

−

γ1) ˜D2

2(P, K)

−

+ (γ

γ0)

(γ

−

· · ·

−

· · ·

γs−1) ˜D2

s(P, K),

Then the sum of polynomials can be rearranged as

D2(P, K) = γD2

1(P, K) + γ2D2

2(P, K) +

+ γsD2

s(P, K),

· · ·

where D2

j measures the uniformity of all j-dimensional projections of P [Hic98]. A

computationally eﬃcient method for obtaining all of the D2

j is to compute D2(P, K)

for s diﬀerent values of γ0,

· · ·

γs−1 and then perform polynomial interpolation. Specif-

ically Newton’s divided diﬀerence formula has been applied for polynomial interpo-

lation.

Figure 3.11 plots the distribution of D2

j for 5-dimensional scrambled Sobol’ and

40

scrambled Niederreiter-Xing nets [NX96, NX98] for j = 1, . . . , 5. The t-values for

these nets are 3 and 2 respectively. From this ﬁgure the scrambled Sobol’ net has

smaller D2

j for j = 1, 2, 5 and the scrambled Niederreiter-Xing sequence has smaller

D2

j for j = 3, 4. This means that for integrands that can be well approximated by

sums of one and two-dimensional functions, the Sobol’ net will tend to give small

quadrature error even though it has larger t value.

Here the empirical distribution of randomly shifted lattice points has been in-

vestigated as well (see Figures 3.12 and 3.13). Here we use a Korobov type rank-1

lattice with generator η = 17797 [HHLL01]. Although there is no known theory on

the distribution of the randomly shifted lattice points, the ﬁtted distribution of the

form used for nets seems to work well for large enough N.

41

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−14

−12

10

−10

10

−8

10

−6

10

−4

10

−2

10

0
10

D2(P)

Figure 3.10: The empirical distribution of the square discrepancy of the points from

Latin hypercube sampling for s = 1 and N = 20,

, 210 from right to left.

· · ·

42

1

0.8

0.6

0.4

0.2

0
10

−12

−11

10

−10

10

−9

10

−8

10

−7

10

−6

10

−5

10

D2
j

(P)

1

0.8

0.6

0.4

0.2

0
10

−12

−11

10

−10

10

−9

10

−8

10

−7

10

−6

10

−5

10

D2
j

(P)

Figure 3.11: The empirical distribution of D2

j (P ) for the (3,9,5)-net of scrambled

Sobol’ (solid) and (2,9,5)-net of scrambled Niederreiter-Xing (dashed). Top ﬁgure

shows j = 2, 5, 1 and the bottom ﬁgure shows j = 3, 4 from right to left.

43

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
10

−6

−5

10

−4

10

−3

10

−2

10

−1

10

D2(P)

Figure 3.12: The empirical distribution of the square discrepancy (3.4) of a randomly

shifted rank-1 lattice and its ﬁtted distribution for s = 5 and N = 24, 27, 210 from

right to left.

)

(

P
2
D

−3

10

−3

10
FD2(P)

Figure 3.13: A Q-Q plot of the empirical distribution of the squared discrepancy

versus the ﬁtted distribution for a randomly shifted rank-1 lattice with s = 5 and

N = 27.

44

Chapter 4

t-parameter Optimization for

Digital (t, m, s)-nets by

Evolutionary Computation

So far most (t, m, s)-nets are generated by number theoretic methods. However, in

this chapter we use optimization methods to generate a digital (t, m, s)-net. This

chapter explains a procedure for ﬁnding good generator matrices of digital (t, m, s)-

nets. Here we consider imbedded generator matrices, that is one set of generator

matrices is used for any m

mmax and s

≤

≤

smax. Many well known generator ma-

trices of digital (t, m, s)-nets are imbedded matrices, namely Sobol’ and Niederreiter.

Such imbedded matrices have a certain advantage that the user needs only one set of

matrices rather than several diﬀerent sets of matrices that work for diﬀerent s or m.

Finding good imbedded nets is more diﬃcult than ﬁnding nets with ﬁxed m and s.

In Chapter 1 we introduced the deﬁnition of t as the quality measure of (t, m, s)-

nets and (t, s)-sequences in terms of an elementary interval in base b. In this chapter

45

the quality parameters t are expressed as the number of linearly independent rows of

generator matrices for digital (t, m, s)-nets, which can be computed from the gener-

ator matrices. Section 4.2 introduces the basic concept of evolutionary computation

that is used for solving optimization problems for ﬁnding better nets. Section 4.3

explains the detail methodology. Finally we report on some numerical results of new

generator matrices by making comparisons to other well known generator matrices

of digital (t, m, s)-nets.

4.1 Digital (t, m, s)-Nets and (t, s)-Sequences

For b a prime number the quality of a digital sequence is expressed by its t-value,

which can be determined from the generator matrices. For any positive integer m let

jmk be the row vector containing the ﬁrst m columns of the kth row of Cj. Let t
cT

be an integer, 0

t

≤

≤

m, such that for all s-vectors d = (d1, . . . , ds) of non-negative

integers with d1 +

+ ds = m

t,

−

· · ·

the vectors cjmk, k = 1, . . . , dj, j = 1, . . . , s

where Zb is a ﬁnite ﬁeld in mod b. Then for any non-negative integer ν and any

are linearly independent over Zb.

(4.1)

λ = 0, . . . , b

1 with λ

b

−

≤

−

(1.10) is a (λ, t, m, s)-net in base b.

(ν mod b), the set

yνbm, . . . , y(ν+λ)bm−1}
(Note that a (1, t, m, s)-net is the same as a

deﬁned by

{

(t, m, s)-net.) If the same value of t holds for all non-negative integers m, then the

digital sequence is a (t, s)-sequence.

An eﬃcient algorithm is available for the determination of the quality parameter

of a digital net in the binary case. More detailed explanations can be found in

46

[Sch99a].

4.2 Evolutionary Computation

Evolutionary Computation [Hei00] is an algorithm which is based on the principles

of a natural evolution as a method to solve parameter optimization problems. An

evolutionary algorithm (EA) constructs a population of candidate solutions for the

problem at hand. The solutions are evolving by iteratively applying a set of stochastic

operators, (or genetic operators) known as recombination, mutation, reproduction

and selection. During the iterating process (or evolving process) each individual in

the population receives a measure of its ﬁtness in the environment.

In EA each individual represents a potential solution to the problem at hand, and

in any evolution program, is implemented as some (possibly complex structure) data

structure. Each solution is evaluated to give some measures of its “ﬁtness”, then the

new population is formed by selecting the more ﬁt individuals. Some members of the

new population undergo transformations by means of “genetic” operators to form a

new solution. There is a unary transformation (mutation type), which creates new

individuals by a small change in a single individual. Higher order transformation

(crossover type) creates new individuals by combining parts from several individu-

als. After some number of generations the program converges to a solution which

might be a near-optimum solution. Traditionally the data structure of EA is binary

representations and an operator set consisting only of binary crossover and binary

mutation which is not suitable for many applications.

However a richer set of data structure (for chromosome representation) together

47

with an expanded set of a genetic operators has been introduced. The chromosomes

need not be represented by bit-strings. Instead it could be ﬂoating number represen-

tation. The alternation process including other “genetic” operators appropriates for

the given structure and the given problem. These variations include variables length

string, richer than binary string. These modiﬁed genetic operators meet the needs of

particular applications.

The following is a detailed description of stochastic operators in EA.

• Recombination perturbs the solution by decomposing distinct solutions and

then randomly mixes their parts to form a novel solution.

• Mutation might play important role that introduces further diversity while the

algorithm is running by randomly (or stochastically) perturbing a candidate

solution, since a large amount of diversity is usually introduced only at the

start of the algorithm by randomizing the genes in the population.

• Selection evaluates the ﬁtness value of the individuals and purges poor solutions

from population. It resembles the ﬁtness of survivors in nature.

• Reproduction focuses attention on high ﬁtness individuals and replicates the

most successful solutions found in a population.

The resulting process tends to ﬁnd globally optimal solutions to the problem much

in the same way as the natural populations of organisms adapt to their surrounding

environment.

A variety of evolutionary algorithms has been proposed. The major ones are

genetic algorithms [Gol89], evolutionary programming [Fog95], evolutionary strate-

48

gies [Kur92], and genetic programming. They all share the common conceptual base

of simulating the evolution of individual structures via processes of stochastic op-

erators which are mentioned earlier. They have been applied to various problems

where there was no other known problem solving strategy, and the problem domain

is NP-complete. That is the usual place where EAs solve the problem by heuristically

ﬁnding solutions where others fail.

49

PSEUDO CODE

Algorithm EA is

*********************************************************************

// start with an initial time

Time := 0;

// initialize a usually random population of individuals

initpopulation Pop;

// evaluate ﬁtness of all initial individuals in population

evaluate Pop;

// test for termination criterion (time, ﬁtness, etc)

While not done do

// select sub-population for oﬀspring production

SPop := selectparents Pop;

// recombine the ”genes” of selected parents

recombine SPop;

// perturb the mated population randomly or stochastically

mutate SPop;

// evaluate its new ﬁtness

evaluate SPop;

// select the survivors from the actual ﬁtness

Pop := survive Pop,SPop;

// increase the time counter

Time := Time+1;

*********************************************************************

50

4.3 Searching Method

Searching for ﬁnding good generator matrices can be considered as solving a combina-

torial optimization problem. As the size of s and m increases, the possible solutions

of the problem become exponentially large, O(bm2s). Evolutionary computation is

one of the methods for solving such problem. In this section we describe the search-

ing procedure for ﬁnding good digital (t, m, s)-nets by an evolutionary computation

strategy.

4.3.1 Environment Setting

Considering generator matrix Cj where j = 1,

1 Cj,1,2 Cj,1,3



, s.

· · ·

· · ·

· · ·
...

Cj,1,m

Cj,2,m
...

1 Cj,m−1,m



















· · ·

· · ·

1

· · ·

Cj =

Cj,2,3

. . .

· · ·

0
...

0

0

1
...

· · ·

· · ·

















First, we deﬁne an individual as A = (A1,

, As). Each sub population Aj can

be expressed as Aj = (Aj(1),

, Aj(r))

∈

· · ·

Z2 where r is the size of cells. Here we

consider the matrix Cj as a m

×

m upper triangular matrix with all diagonal entries

are one. Notice that we only consider the case b = 2. Then the size of r becomes

51

(mm

−

m)/2. The following explains the correspondence between Aj and Cj.

Aj(1) = Cj,1,2,

Aj(2) = Cj,1,3,

· · ·

Aj(m−1) = Cj,1,m,

Aj(m) = Cj,2,3,

· · ·

Aj(r) = Cj,m−1,m.

4.3.2 Searching Strategy

In previous section, we deﬁne an individual Aj. However looking at optimization

problem as a whole, the population becomes Ai

j where i = 1,

, u and u is the

· · ·

number of individuals. In this problem we choose u = 400. The followings explain

the detail setting of stochastic operators.

(a) Recombination : Two points crossover has been adopted. The recombination

only occurs for the same j among Ai

js. The selection of the locations of crossover is

chosen randomly within ﬁrst half for the ﬁrst location and second half for two second

location.

(b) Mutation : 5 % of randomly selected positions are assigned to perform muta-

tion.

(c) Selection : Best 30 % of individuals are selected based on a given objective

function.

52

(d) Reproduction : The best individuals are selected from the current generation

to remain as the part of the next generation. And the rest of the next generation is

produced based on the replication of the best individuals of the current generation.

Since we are keeping the best previous individuals as themselves, aging has been

introduced that each individual will be removed when the individuals are survived

more than certain length of period.

4.3.3 Objective Function

There are various objective functions that can be used in this problem. Here we

select an objective function as the sum of t values for s dimensional projections. The

details are following.

Let T (m, s, C1,

· · ·

, Csmax) be the smallest t for which C1,

, Cs generates a (t, m, s)-

· · ·

net that the t values of the digital (t, m, s)-net generated by

C1m,

· · ·

, Csm,

s = 1,

, smax ; m = 1,

, mmax

· · ·

· · ·

where Cjm is the top m

m sub-matrix.

×

For given C1,

· · ·

, Csmax, we can use these matrices to generate (T (m, s), m, s)-

nets and extend the nets for m

mmax and s

smax.

≤

≤

Finally the objective function is written as

T (C1,

· · ·

, Csmax) =

smax

mmax

s=1
X

m=1
X

T (m, s; C1,

, Csmax).

· · ·

Our goal is ﬁnding the generator matrices that obtain the smaller T (C1,

, Csmax)

· · ·

values.

53

4.4 Numerical Results

The newly obtained digital (t, m, s)-net by an evolutionary computation is named as

the EC net. The exact quality parameters t of the generator matrices were calculated

by using the program provided by Schmid [Sch99a]. Notice that initial populations

are selected partially at random and partially by taking Sobol’ generator matrices and

randomly choosing the Cj matrices. Table 4.1 shows the T (m, s) values of generator

matrices of EC sequence. Here the T (m, s) is deﬁned as following:

T (m, s) = max
m′≤m
s′≤s

T (m′, s′, C1,

, Csmax).

· · ·

The same deﬁnition is applied for the remaining section.

Table 4.2 shows the comparison of T (m, s) values between generator matrices of

Sobol’ and EC nets for s = 40 and m = 20. The T (C1, . . . , Csmax) value for Sobol’

sequence is 5421 and EC sequence is 5381. For the comparison of overall T (m, s)

values, if the two methods have the same T (m, s) values then the entry is marked

as *. But if one net is better, which means having a smaller T (m, s) value, then it

is marked as the initial of the better net. Also if the diﬀerence of T (m, s) value is

greater than 1 then the diﬀerence is indicated in front of the initial. For example, 2E

means the generator matrices of EC sequence has a smaller T (m, s) value than Sobol’

sequence by 2. From the table 4.2, there are some entries that Sobol’ is better than

EC and vice versa. However EC sequence has smaller T (C1, . . . , Csmax) values than

the Sobol’ sequence. The EC sequence tends to have smaller T (m, s) values that the

Sobol’ sequence for larger m and s.

We also compare the generator matrices of Niederreiter-Xing sequence with EC

sequence. However, notice that the generator matrices of Niederreiter-Xing sequence

54

Table 4.1: The T (m, s)-table for generator matrices of EC

s m 1
0
1
0
2
0
3
0
4
0
5
0
6
0
7
0
8
0
9
0
10
0
11
0
12
0
13
0
14
0
15
0
16
0
17
0
18
0
19
0
20
0
21
0
22
0
23
0
24
0
25
0
26
0
27
0
28
0
29
0
30
0
31
0
32
0
33
0
34
0
35
0
36
0
37
0
38
0
39
0
40

2
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

3
0
0
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

4
0
0
1
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

5
0
0
1
2
2
3
3
3
3
3
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

6
0
0
1
3
3
3
3
3
4
4
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5
5

7
0
0
1
3
3
4
4
4
4
4
5
5
5
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6

8
0
0
1
3
3
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
7

9
0
0
1
3
3
5
5
5
6
6
6
6
6
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7

10
0
0
1
3
3
5
5
6
6
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
8
8

11
0
0
1
3
4
6
6
7
7
7
7
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
9
9

12
0
0
1
3
4
6
6
7
7
8
8
8
8
8
8
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
10
10

13
0
0
1
3
5
6
6
7
7
9
9
9
9
9
9
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

14
0
0
1
3
5
6
6
8
8
9
10
10
10
10
10
10
10
10
10
10
10
10
11
11
11
11
11
11
11
11
11
11
11
11
11
11
11
11
11
11

15
0
0
1
3
5
7
7
9
9
9
10
10
10
11
11
11
11
11
11
11
11
11
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12

16
0
0
1
3
5
7
7
9
9
9
10
10
10
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
13
13
13
13
13
13
13

17
0
0
1
3
5
7
7
9
9
9
10
10
10
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
14
14
14
14
14
14
14

18
0
0
1
3
5
7
7
9
10
10
11
11
11
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
15
15
15
15
15
15
15

19
0
0
1
3
5
7
8
9
11
11
11
12
12
13
13
13
13
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
15
15
15
15
15
15
15

20
0
0
1
3
5
7
8
9
11
11
11
13
13
13
13
13
13
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15
15

55

Table 4.2: Comparison of T (m, s) values between generator matrices of Sobol’ and

EC for s = 40 and m = 20.

10

11

12

13

14

15

16

17

18

19

20

S

*

*

*

*

E 2E 2E E E

E 2E 2E E

E 2E 2E E

S

*

*

*

*

*

*

*

*

*

*

*

E E

E E

E E

E E

E E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

E

S

*

E

E

E

E

S

S

S

S

S

S

S

S

S

S

S

S

S

*

*

*

*

*

*

*

*

E

E

E

E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

S

*

*

E 2E

E 2E

E 2E

E 2E

*

*

*

*

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

S

S

S

S

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

*

*

*

*

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

S

S

S

*

*

*

*

s m 1

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

2

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

3

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

4

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

5

*

6

*

7

*

8

*

* S S *

* S *

*

* S * S

* S * S

* S * S

* S * S

* S * S

* S *

* S *

* S *

*

*

*

9

*

*

*

*

S

S

S

S

*

S

*

*

*

*

*

*

* E

* E

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

* S S *

* E

S

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

S

*

*

*

*

E E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

* S S *

* E

E E E

* S S *

* E

E E E

*

*

*

*

*

*

*

*

* S *

* E

E E E

*

*

*

*

*

*

*

*

*

* E

E E E

* E * E

E E E

* E E 2E E E E

* E E 2E E E E

* E E 2E E E E

* E E E

*

* E E

*

*

*

*

E

E

56

Table 4.3: Comparison of T (m, s) values between generator matrices of Niederreiter-

Xing and EC for s = 18 and m = 20.

s m 1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

E 2E 3E 4E 5E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E 6E

E 2E 3E 4E 5E 6E 6E 6E 6E 6E 7E 8E 8E 8E 8E 8E 8E 8E 8E 8E

E E 2E 3E 4E 5E 5E 5E 5E 5E 6E 7E 7E 7E 7E 7E 7E 7E 7E 7E

E E

E 2E 3E 3E 3E 3E 3E 3E 4E 5E 5E 6E 6E 6E 6E 6E 6E 6E

E E

E 2E 3E 3E 3E 3E 3E 3E 3E 4E 3E 4E 4E 4E 4E 5E 5E 5E

E E

E 2E 2E 3E 2E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E

E

E

E

E

E

E

E

E

E

E

E

E 2E 3E 2E E

E 2E 3E 2E E

E 2E 2E 2E E

E 2E 2E 2E E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

*

E

E

E

E

E

E

E

E

*

*

*

E

E

*

*

*

*

*

E

E

E

E

*

*

E

E

E

E

E

E

E

E 2E 2E 3E 3E 4E 4E 4E 4E 4E

E 2E 2E 3E 3E 4E 4E 4E 4E 4E

E

E

E

E

E

E

E

E 2E 2E 3E 3E 3E

E 2E E

E 2E 2E 2E E

E

*

*

*

*

*

N

E 2E 3E 3E 3E 4E

*

E 2E 2E 3E 4E

E 2E 2E 2E 3E 4E 4E 3E 2E 2E

E 2E 2E 2E 3E 4E 4E 3E 2E 2E

E 2E 2E 2E 2E 2E E

E 2E 2E 2E 2E 2E E

E

E

E

E

E

E

E 2E 2E 2E E

E 2E 2E 2E E

E 2E 2E 2E E

E

E

E

E

E

E 2E

E 2E

E 2E

E 2E

*

*

are originally not constructed as imbedded matrices like EC and Sobol’ sequences

do. Therefore there are diﬀerent generator matrices available for diﬀerent s. Here we

compute the T (m, s) values of the generator matrices of Niederreiter-Xing like other

imbedded matrices. Two matrices are chosen for comparison, “nxs18m30” for s = 18

and “nxs32m40” for s = 32. The generator matrices are obtained from [Pir02].

From tables 4.3 and 4.4, the EC sequence obtains smaller T (m, s) values than

Niederreiter-Xing sequence, and the diﬀerences are larger for smaller s. It is under-

standable that the generator matrices are constructed best for ﬁxed dimension like

s = 18 and 32 in this example. However the tables show EC sequence even obtains

smaller T (m, s) values for smax.

We also investigate the T (m, s) values of the lower dimensional projections for

EC and Niederreiter-Xing sequences. Tables 4.5 and 4.6 show the three diﬀerent

57

Table 4.4: Comparison of T (m, s) values between generator matrices of Niederreiter-

Xing and EC for s = 32 and m = 20.

s m 1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

E 2E 3E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E

E 2E 3E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E 4E

E E 2E 3E 3E 3E 3E 3E 4E 4E 4E 4E 4E 5E 5E 5E 5E 5E 5E 5E

E E

E 2E 2E 2E 3E 3E 3E 3E 4E 5E 6E 6E 6E 6E 6E 6E 6E 6E

E E

E 2E 2E 2E 3E 3E 3E 3E 3E 4E 4E 4E 4E 4E 4E 4E 4E 4E

E E

E 2E E 2E 2E 2E 3E 3E 2E 2E 3E 3E 2E 2E 3E 4E 5E 6E

E 2E 2E 2E 3E 3E 2E 3E 3E 3E 2E 2E 3E 4E 4E 5E

E 2E 2E 2E 3E 2E E 2E 2E E

E 2E 3E 4E

E 2E 2E 2E 2E E 2E 2E E

E 2E 2E 2E E

E 2E 2E E

E 2E 2E E

E 2E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

*

*

*

*

*

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

*

E

*

*

*

E

E

*

*

E

E

E

*

*

E

E 2E

E 2E

E 2E

*

*

N 2E 2E N N

*

*

*

*

*

*

*

*

N

N

*

*

*

*

*

*

*

*

N N N

N N N

N N

N N

N N

N N

N N

N N

*

*

E

E

E

E

E

E

E

E

N

N

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

E

E

E

E

*

*

E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

E

E

E

N

N

N

N

N

*

*

*

*

*

*

*

*

*

*

E

E

E

E

E

E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

E

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

N

N

N

*

E

E

E

E

E

E

E

E

*

*

*

*

*

*

*

*

*

*

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

E

E

*

*

*

*

*

*

*

*

*

*

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

E 2E E

58

dimensional projections.

T avg
r

(C1, . . . , Csmax) =

1
smax
r

T max
r

(cid:1)
(cid:0)
(C1, . . . , Csmax) = max
u⊆1:smax
|u|=r

T (m, r; Cj, j

u⊆1:smax
X
|u|=r
T (m, r; Cj, j

u)

∈

u)

∈

where r = 1, 2, and 18 for smax = 18 and r = 1, 2, and 32 for smax = 32.

From tables 4.5 and 4.6 the generator matrices of EC has smaller t values for

most entries. It implies that EC sequences has better equidistribution property for

lower as well as higher dimensional projections. The t values of T (m, 18, NXP rop)

and T (m, 32, NXP rop) are taken from [Pir00, Pir02]. Notice that T (m, 18, NXP rop)

values are usually smaller than T (m, 18, NX), because T (m, 18, NXP rop) is obtained

by applying propagation rule [Nie99] to the generator matrices.

Figures 4.2 and 4.3 plot the root mean square discrepancy of randomly scrambled

and unscrambled Niederreiter-Xing and EC sequences. The discrepancy is the same

discrepancy used in Chapter 2. Also 100 random replications are performed for the

scrambled one. The choices of dimension are s = 18 and s = 32. The number of

scrambled digits is chosen to be 31. From the ﬁgures for the unscrambled case the

EC sequence has a smaller discrepancy than the Niederreiter-Xing sequence. But the

convergence rate of the discrepancy seems to be nearly the same. However for the

scrambled case the EC sequence shows better convergence rate than the Niederreiter-

Xing sequence.

The table 4.7 is the comparison of T (m, s, EC) and the updated t values from

the tables [CLM+99, Sch99b], denoted A. Here, we made comparisons between a

T (m, s, EC) value and the t value for s + 1, because the EC net can be extended to

s + 1 by simply adding a skewed diagonal matrix to generator matrices of EC. It is

59

Table 4.5: The T avg

r

and T max

r

values for r = 1, 2 and 18 of generator matrices of

Niederreiter-Xing and EC for smax = 18, and mmax = 20.

m

T avg
1

(EC)

1

0

2

0

3

0

T avg
1

(N X)

0.4

0.6

0.9

4

0

1

0

4

5

0

6

0

7

0

8

0

9

0

10

0

1.3

1.6

1.2

1.4

1.6

1.3

0

5

0

6

0

5

0

6

0

5

0

3

0

2

0

3

T max
1

(EC)

T max
1

(N X)

T avg
2

(EC)

0

1

0

T avg
2

(N X)

0.6

T max
2

(EC)

T max
2

(N X)

T (m, 18, EC)

T (m, 18, N X)

T (m, 18, N XP rop)

0

1

0

1

1

0.5

0.9

1.2

1.6

1.8

1.9

2.2

2.4

2.5

1

1

2

1

2

2

1.6

2

3

2

3

3

2

3

4

3

4

4

2.6

3.1

4

5

4

5

4

5

6

5

6

5

3

5

6

5

6

6

3.4

3.4

3.2

6

7

6

7

7

7

7

7

7

7

8

8

7

8

8

m 11

12

13

14

15

16

17

18

19

20

T avg
1

(EC)

T avg
1

(N X)

T max
1

(EC)

T max
1

(N X)

T avg
2

(EC)

T avg
2

(N X)

T max
2

(EC)

T max
2

(N X)

T (m, 18, EC)

T (m, 18, N X)

T (m, 18, N XP rop)

0

1

0

4

0

0

0

0

0

0.9

0.9

0.9

0.7

0.7

0

4

0

4

0

5

0

4

0

5

0

1

0

6

0

0

0

1.2

1.2

1.1

0

7

0

8

0

7

2.8

2.9

3.2

3.2

3.3

3.4

3.5

3.6

3.6

3.7

3.5

3.7

3.9

4.2

3.9

3.9

4.2

4.7

5.0

4.9

7

9

8

9

9

7

10

9

10

9

8

11

10

11

10

7

12

10

12

11

8

13

11

13

12

9

14

12

14

12

9

10

11

12

13

13

14

14

12

13

11

10

14

14

14

11

11

15

15

15

60

Table 4.6: The T avg

r

and T max

r

values for r = 1, 2 and 32 of generator matrices of

Niederreiter-Xing and EC for smax = 32, and mmax = 20.

m

T avg
1

(EC)

T avg
1

(N X)

T max
1

(EC)

T max
1

(N X)

T avg
2

(EC)

T avg
2

(N X)

T max
2

(EC)

T max
2

(N X)

T (m, 32, EC)

T (m, 32, N X)

T (m, 32, N XP rop)

T max
1

(EC)

T max
1

(N X)

T avg
2

(EC)

T avg
2

(N X)

T max
2

(EC)

T max
2

(N X)

T (m, 32, EC)

T (m, 32, N X)

T (m, 32, N XP rop)

7

0

1

0

5

1

0

2

0

3

0

4

0

5

0

6

0

0.4

0.6

0.9

0.8

0.6

0.8

8

0

9

0

10

0

1.1

1.1

1.1

0

1

0

0

4

3

0

2

0

3

0

4

0

3

0

4

0

4

0.5

0.8

1.2

1.6

1.9

2.1

2.4

0.9

1.5

1.9

2.3

2.3

2.5

0

1

0

1

1

1

2

1

2

2

2

3

2

3

3

3

4

3

4

4

4

4

4

4

4

5

5

5

5

5

3

6

6

6

6

6

3.2

6

7

6

7

7

m 11

12

13

14

15

16

17

18

0

4

2.6

3.5

7

8

7

8

8

19

0

0

1

0

5

0

1.1

0

5

0

1

0

6

0

0

0

0

0.9

0.8

0.3

0.6

0.7

0

4

0

4

0

3

0

4

0

3

3.2

3.3

3.3

3.5

3.6

3.8

3.8

3.9

3.5

3.7

3.9

4.2

4.0

4.3

4.3

4.5

4.8 4.9

11

11

12

12

12

12

12

12

13

13

11

11

11

11

13

13

13

14

13

14

12

12

14

14

14

8

9

8

9

9

8

9

9

9

9

9

9

10

10

10

10

10

11

11

11

61

0

5

2.8

3.7

7

9

7

9

9

20

0

1

0

4

4

12

13

15

15

15

T avg
1

(EC)

0

T avg
1

(N X)

0.8

not surprising to see that t values from the references exhibit smaller t values than

T (m, s) values, since t values which from [CLM+99, Sch99b] are the best attainable

t values for ﬁxed (t, m, s)-net. For smaller s and larger m the diﬀerence are large.

However propagation rules [Nie99] can be adopted for further improvement for the

EC net. Notice that as s increases the diﬀerence becomes smaller and there are some

places that T (m, s) values and the t values are same.

4.5 Discussion

The new EC net has a smaller T (m, s) value than the Sobol’ net for higher dimension

and larger m. The new sequence shows smaller T (m, s) values in overall comparing

to Niederreiter-Xing sequence which has been considered as a imbedded net. For the

chosen two generator matrices of the Niederreiter-Xing net, the new net still shows

smaller T (m, s) values in some m for smax. Moreover the new net shows smaller t

values for lower dimensional projections as well.

However there is a disadvantage of ﬁnding the generator matrices by optimization

method, since we only can ﬁnd the generator matrices of a (t, m, s)-net. In contrast

the ﬁnite size of generator matrices for the Sobol’ and Niederreiter-Xing sequences

can be extended indeﬁnitely.

Our computation was carried out on a Unix station in C. The new generator

matrices were obtained after less than 100 generations. The actual time was about

2 weeks. There is a problem of increasing m due to the computation time, since the

computation time is more sensitive to m than s. Partially it is due to increasing

numbers of possible solutions by a factor for 2. Also the computing time for t sharply

62

Table 4.7: Comparison of t values from [CLM+99, Sch99b] and T (m, s) values of
generator matrices from EC.

s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

m 1
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

A

A

A

A

7
*
*
*

8
*
*
*

9
*
*
*

19
*
*
*

18
*
*
*

16
*
*
*

15
*
*
*

17
*
*
*

12
*
*
*

14
*
*
*

10
*
*
*

11
*
*
*

20
13
6
5
4
3
2
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
2A 2A 2A 2A 2A 2A 2A 2A 2A 2A 2A 2A 2A 2A 2A
A
* A A
A
A
2A 2A 3A 3A 3A 3A 3A 3A 3A 3A
*
* A A
2A 2A 2A 2A 3A 3A 3A 3A 4A 4A 4A 4A 4A 4A
A
A
* A A
2A 2A 2A 2A 3A 3A 3A 3A 4A 4A
A
A
2A 2A
A
A
*
*
2A 2A 2A 3A 3A 2A 3A 4A 4A 4A 4A 4A 4A
A
*
A
A
*
*
2A 2A 2A 3A 2A 2A 3A 3A 3A 3A 4A 5A 5A
A
A
A
A
*
*
2A 3A 3A 3A 4A 3A 3A 3A 2A 2A 3A 3A
A
A
A
*
*
A
A
A
A
A 2A 2A
*
*
2A 2A 2A 3A 3A 4A 4A 3A 2A 2A 2A 2A
2A 2A 3A 3A 3A 4A 3A 3A 2A 2A 2A 3A
A
A
A 2A 2A
*
*
2A 2A 2A 3A 2A 3A 4A 3A 3A 2A 2A 2A 3A
A
A 2A 2A
*
*
2A 2A 2A 2A 2A 3A 3A 4A 4A 5A 4A 3A 3A
A
A 2A 2A
*
*
2A 2A 2A 2A 2A 3A 3A 4A 4A 5A 4A 3A 3A
A
2A
A
*
*
*
2A 2A 2A 2A 3A 3A 3A 3A 4A 5A 4A 3A 3A
A
2A
A
*
*
*
2A 2A 2A 2A 3A 3A 3A 3A 4A 4A 3A 3A
A
A
2A
A
*
*
*
2A 2A 3A 3A 3A 3A 3A 3A 4A 5A
2A
A
A
2A
A
*
*
*
2A 2A 3A 3A 3A 3A 3A 3A 4A 5A
2A
A
A
2A
A
*
*
*
2A 2A 3A 2A 3A 3A 3A 2A 3A 4A
2A
A
2A
A
*
*
*
A
2A 2A 3A 2A 3A 3A 3A 2A 2A 3A
2A
A
2A 2A
A
*
*
*
2A 2A 2A 2A 2A 3A 3A 2A 2A 2A
2A
A
2A 2A
A
*
*
*
2A 2A 2A 3A 3A 3A 3A 2A 2A 2A
A
A
2A 2A
A
*
*
*
A
2A 2A 3A 3A 2A 2A
2A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 3A 3A 2A 2A
2A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
2A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
2A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
2A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
A
A
A
A
2A 2A
A
*
*
*
A
2A 2A 2A 3A 2A 2A
A
A
A
A
2A
A
*
*
*
*
*
A
2A 2A 2A 2A
A
A
A
A
2A
A
*
*
*
*
2A 2A 2A 2A
*
A
A
A
A
A
2A
A
*
*
*
*
2A 2A 2A 2A 2A 2A 2A
A
A
A
A
2A
A
*
*
*
*
2A 2A 2A 2A 2A 2A 2A
A
A
A
A
2A
A
*
*
*
*
2A 2A 2A 2A 2A 2A
A
A
A
A
A
2A
A
*
*
*
*
A
2A 2A 2A 2A 2A 2A
A
A
A
A
2A
A
*
*
*
*
A
2A 2A 2A 2A 2A 2A
A
A
A
A
2A
A
*
*
*
*
A
2A 2A 2A 2A 2A 2A 2A 2A
A
2A
A
*
*
*
*
A
A
2A 2A 2A 2A 2A 2A 2A 2A
A
2A 2A
A
*
*
*
*

A
A
A
A
A
A
A
A
A
A
A
A
A
A
*
*
*
*
*
*
*
A
A

A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A

A
A

63

increases as m increases.

Also notice that the new generator matrices are from the initial populations which

are partially taken from the generator matrices of the Sobol’ sequence. Therefore the

new matrices are more likely to be better than the Sobol’ net which is constructed

by the number theoretic method.

Choosing an objective function is another diﬃcult issue, In this thesis we consider

the total sum of t values as an objective function. This leads the new net to have

smaller t values in larger s and m. There are many diﬀerent objective functions to be

considered such as looking at the sum of t values for diﬀerent dimensional projections

or/and looking at the maximum t values of their projections.

Finding the generator matrices based on the Niederreiter-Xing sequence is another

interesting thing to try, because the generator matrices of the Niederreiter-Xing se-

quence are full matrices and it is the most recently developed one. Nevertheless it

is easier to ﬁnd better generator matrices based on the existing matrices rather than

the complete new matrices.

64

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.1

0.2

0.3

0.4

0.5
a

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5
b

0.6

0.7

0.8

0.9

1

Figure 4.1: Dimensions 16 and 18 of the a) EC and b) Scrambled EC’ sequences for

N = 256.

65

0
10

y
c
n
a
p
e
r
c
s
D

i

−1

10

−2

10

0
10

1
10

2
10
N

3
10

4
10

Figure 4.2: The scaled root mean square discrepancy with α = 2 as a function of N

for Owen-scrambled EC (o) and Niederreiter-Xing (

) nets for s = 18 and 32.

∗

2
10

1
10

y
c
n
a
p
e
r
c
s
D

i

0
10

−1

10

−2

10

0
10

1
10

2
10
N

3
10

4
10

Figure 4.3: The scaled root mean square discrepancy with α = 2 as a function of N

for non-scrambled EC’ (o) and Niederreiter-Xing (

) nets for s = 18 and 32.

∗

66

Chapter 5

Application

5.1 Multivariate Normal Distribution

Consider the following multivariate normal probability

1
Σ
(2π)s

|

|

Z[a,c]

e− 1

2

θT Σ−1θ dθ,

(5.1)

where a and c are known s-dimensional vectors that deﬁne the interval of integra-

p

tion, and Σ is a given s

×

s positive deﬁnite covariance matrix. One or more of the

components of a and c may be inﬁnite. Since the original form is not well-suited

for numerical quadrature, Alan Genz [Gen92, Gen93] proposed a transformation of

variables that result in an integral over the s

1 dimensional unit cube. This trans-

−

formation is used here.

The choices of the parameters in (5.1) are made as in [Gen93, HH97]:

a1 =

= as =

,

−∞

· · ·

cj i.i.d. uniformly on [0, √s],

Σ generated randomly as in [Gen93].

Four diﬀerent types of algorithms are compared for this problem:

67

r
o
r
r

E
d
e
a
c
S

l

1
10

0
10

−1

10

−2

10

−3

10

Tol = 0.0001

3

6

9
s

12

15

Figure 5.1: Box and whisker plots of scaled errors, E/ǫ, for 50 randomly chosen test

problems (5.1). For each dimension s results from left to the four algorithms as listed

in the text.

i. the adaptive algorithm MVNDNT of Alan Genz, which can be found at

http://www.sci.wsu.edu/math/faculty/genz/software/mvndstpack.f

ii. a Korobov rank-1 lattice rule implemented as part of the NAG library,

iii. a randomly-shifted extensible rank-1 lattice sequence of Korobov type [HH97,

HHLL01] with generator vector (1, 17797, . . . , 17797s−1), and

iv. the scrambled Sobol’ sequence described here.

The periodizing transformation x′

2xj −
integrand for the second and third algorithms ii. and iii., as it appears to increase the

, j = 1, . . . , s has been applied to the

j =

1

|

|

68

 
Tol = 0.0001

5
10

4
10

3
10

s
t
n
o
P

i

f
o

r
e
m
b
u
N

2
10

3

6

9
s

12

15

Figure 5.2: Box and whisker plots of the computation times in seconds for 50 ran-

domly chosen test problems (5.1). For each dimension s results from left to the four

algorithms as listed in the text.

accuracy of these two algorithms. The computations were carried out in Fortran on

an Unix workstation in double precision. An absolute error tolerance of ε = 10−4 was

chosen and compared to the absolute error E. Error estimation for each algorithm

is described in [HHLL01]. Because the true value of the integral is unknown for this

test problem, the Korobov algorithm with ε = 10−8 was used to be “exact” value

for computing error. For each s, 50 test problems were generated randomly. Figure

5.1, 5.2, and 5.3 show the box and whisker plots of the scaled absolute error E/ǫ, the

number of points used, and the computation time in seconds for the four methods

with various dimensions. The boxes are divided by the median and contain the middle

69

 
 
i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

1
10

0
10

−1

10

−2

10

−3

10

Tol = 0.0001

3

6

9
s

12

15

Figure 5.3: Box and whisker plots of the number of points used for solving 50 ran-

domly chosen test problems (5.1). For each dimension s results from left to right, the

four algorithms as listed in the text.

half of the values. The whiskers show the full range of values and the outliers are

plotted as

.

∗

The scaled error indicates how conservative the error estimate is.

Ideally the

scaled error should be close to but not more than one. If the scaled error is greater

than one, then the error estimation is not careful enough. However, if the scaled

error is much smaller than one, then the algorithm is wasting time by being too

conservative. From the Figures 5.1, 5.2 and 5.3 all four methods perform reasonably

well in error estimation. The scrambled Sobol’ points, however, use fewer function

evaluations and less computation time. Also, their performance is less dependent on

70

 
the dimension.

5.2 Physics Problem

The following multidimensional integral arising in physics problems is considered by

Keister [Kei96]:

s

cos(

x
k

k

ZRs

)e−kxk2

dx = πs/2

cos



Z[0,1]s

[Φ−1(yj)2]
2



dy,

(5.2)

v
u
u
t



j=1
X



where

k·k

denotes the Euclidian norm in Rs, and Φ denotes the standard multivariate

Gaussian distribution function. Keister provides an exact formula for the answer and

compared the quadrature methods of McNamee and Stenger [MS67] and Genz and

Patterson [Gen82, Pat68] for evaluating this integral. Papageorgiou and Traub [PT97]

applied the generalized Faure sequence from their FINDER library to this problem.

The exact value of the integral is reported in [Kei96, PT97].

Since the methods of McNamee and Stenger and Genz and Patterson performed

much worse than FINDER’s generalized Faure sequence, they are ignored here. In-

stead we compare the scrambled Sobol’ sequences implemented here to the perfor-

mance of FINDER and the extensible lattice rules described before. To be consistent

with the results reported in [PT97] the relative error is computed as a function of

N. Figure 5.4 shows the results of the numerical experiments for dimension s = 25.

Box and Whisker plots show how well 50 randomized nets and lattice rules perform.

The three algorithms appear to be quite competitive to each other. In some cases

randomized Sobol’ performs better than the other two sequences.

71

1
10

0
10

−1

10

r
o
r
r

E
e
v
i
t
a
e
R

l

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

100

l
101

l
102

l
103
N

l
104

l
105

106

Figure 5.4: The relative error obtained in approximating (5.2) for s = 25 for

FINDER’s generalized Faure algorithm (

). The box and whisker plots show the

◦

performance of the randomly scrambled Sobol’ points and randomly shifted extensi-

ble rank-1 lattice, for the values of N = 4, 8, . . . , 218.

5.3 Multidimensional Integration

Consider the following multidimensional integration problem:

s

Z[0,1)s

[1 + aj(yj −

j=1
Y

1
2

)]dy,

(5.3)

where the exact value of the integration is 1. We numerically compute this problem

for the case aj = 0.4 + j

10 where j = 1,

· · ·

, s with scrambled Sobol’ and EC nets.

Figure 5.5 show the root mean square relative error of (5.3) for the scrambled Sobol’

and scrambled EC nets. Figure 5.5 plots the root mean square relative error computed

as a function of N for s = 14 and 24. For scrambling 100 diﬀerent replications are

72

 
made. Figure 5.5 shows that the scrambled EC sequence has smaller relative error

than the scrambled Sobol’ sequence for all choices of N.

1
10

0
10

−1

10

−2

10

−3

10

−4

10

r
o
r
r

E
e
v
i
t
a
e
R

l

−5

10

0
10

1
10

2
10

3
10
N

4
10

5
10

6
10

Figure 5.5: The relative error of the numerical integration of (5.3) for scrambled

Sobol(

∗

) and scrambled EC(o) nets for s = 14 and 24.

Numerical experiments on multidimensional integrations are performed by using

Genz’ test function package [Gen87]. We test his six integral families, namely Os-

cillatory, Product Peak, Corner Peak, Gaussian, C 0 function, and Discontinuous for

ﬁve diﬀerent sequences. The compared sequences are Niederreiter-Xing, scrambled

Niederreiter-Xing, EC, scrambled EC, and imbedded Niederreiter-Xing sequences.

For imbedded Niederreiter-Xing sequence which we choose the generator matrices

“nxs20m30”. The choices of parameters are made as in [Gen87]. For scrambled se-

quences, 20 diﬀerent replications are performed. The digit accuracy is obtained by

computing max(log(relative error),0)). Therefore if the digit accuracy is zero

then the relative error is greater than 1. The larger value of the digit accuracy im-

73

 
plies the better performance. From Figures 5.6-5.11 it is shown that scrambled nets

perform almost always the best. Considering non-scrambled sequences the EC se-

quence shows the better performance as the dimension of the problem increases. The

Niederreiter-Xing sequence tends to perform well in lower dimension, and the imbed-

ded Niederreiter-Xing sequence always performs worse than the Niederreiter-Xing

sequence. This is expected because the Niederreiter-Xing sequence are not designed

as imbedded sequences. We also test the problems with Sobol’ and scrambled Sobol’

sequences. The performances of Sobol’ and EC sequences are nearly the same.

Oscillatory

7

6

5

4

3

2

1

y
c
a
r
u
c
c
A

t
i

i

g
D

0

4

6

8

10

12
s

14

16

18

20

Figure 5.6: The digit accuracy of the numerical integration of Oscillatory functions

for EC (.), scrambled EC (o), Niederreiter-Xing (+), scrambled Niederreiter-Xing

), and imbedded Niederreiter-Xing ((cid:3)) nets.

(

∗

74

 
Product Peak

9

8

7

6

5

4

3

2

1

y
c
a
r
u
c
c
A

t
i
g
D

i

0

4

8

12
s

16

20

Figure 5.7: Same as 5.6 for the numerical integration of Product Peak functions.

Corner Peak

3

2.5

2

1.5

1

0.5

y
c
a
r
u
c
c
A

t
i
g
D

i

0

4

8

12
s

16

20

Figure 5.8: Same as 5.6 for the numerical integration of Corner Peak functions.

75

 
 
Gaussian

6

5.5

5

4.5

4

3.5

y
c
a
r
u
c
c
A

t
i
g
D

i

3

4

8

12
s

16

20

Figure 5.9: Same as 5.6 for the numerical integration of Gaussian functions.

C0 Function

9.5

9

8.5

8

7.5

7

6.5

6

5.5

y
c
a
r
u
c
c
A

t
i
g
D

i

5

4

8

12
s

16

20

Figure 5.10: Same as 5.6 for the numerical integration of C o functions.

76

 
 
Discontinuous

5

4.5

4

3.5

3

y
c
a
r
u
c
c
A

t
i

i

g
D

2.5

4

6

8

10

12
s

14

16

18

20

Figure 5.11: Same as 5.6 for the numerical integration of Discontinuous functions.

77

 
Chapter 6

Conclusion

In Chapter 2 the randomization of Owen and of Faure and Tezuka have been im-

plemented for the best known digital (t, s)-sequences. Both types of randomization

involve multiplying the original generator matrices by randomly chosen matrices and

adding random digital shifts. Base 2 sequences can be generated faster by taking the

advantage of the computer’s binary arithmetic system. Using a gray code also speeds

up the generation process. The cost of producing a scrambled sequence is about twice

the cost of producing a non-scrambled one.

Scrambled sequences often have smaller discrepancies than their non-scrambled

counterparts. Moreover, random scrambling facilitates error estimation. On test

problems taken from the literature scrambled digital sequences performed as well as

or better than other quadrature methods (see Chapter 5).

In Chapter 3 the empirical distribution of the square discrepancy of scrambled

digital nets has been ﬁt to a mixture of chi-square random variables as suggested

by the central limit theorem by Loh [Loh01]. Apart from some technical diﬃculties

that have been discussed there is a good ﬁt between the empirical and theoretical

78

distributions. Although the digital scrambling schemes used in [Mat98, HH01, YH02]

gives the same mean square discrepancy as Owen’s original scheme, the distribution of

the mean square discrepancies varies somewhat. It seems that the square discrepancy

has a smaller variance for Owen’s scheme.

It is shown how one may study the

uniformity of lower dimensional projections of sets of points by decomposing the

square discrepancy into pieces, D2

j . In particular, Sobol’ nets seem to have better

uniformity for low dimensional projections than the more recent Niederreiter-Xing

nets.

In Chapter 4 new digital (t, m, s)-nets are generated by using the evolutionary

computation method. The new net shows better equidistribution properties for large

s and m compared to the the Sobol’ sequence. We also compare the exact quality

parameters t of the new nets with the Niederreiter-Xing nets, where we consider the

Niederreiter-Xing nets as an imbedded net. It is shown that the new net has overall

smaller t values for diﬀerent dimensional projections. For the s = 18 and 32, the

new net has better t values than the Niederreiter-Xing net in some m for smax. By

testing the new nets with multidimensional integration problems, we ﬁnd that the

new nets performs better than Sobol’ and Niederreiter-Xing nets for certain problems.

Chapter 5 shows some promising results that it is possible to improve existing nets

by the optimization method.

79

Bibliography

[AS64]

M. Abramowitz and I. A. Stegun, editors. Handbook of Mathematical

Functions with Formulas, Graphs and Mathematical Tables. U. S. Gov-

ernment Printing Oﬃce, Washington, DC, 1964.

[BF88]

P. Bratley and B. L. Fox. Algorithm 659: Implementing Sobol’s quasiran-

dom sequence generator. ACM Trans. Math. Software, 14:88–100, 1988.

[BFN92] P. Bratley, B. L. Fox, and H. Niederreiter. Implementation and tests of

low-discrepancy sequences. ACM Trans. Model. Comput. Simul., 2:195–

213, 1992.

[CLM+99] A. T. Calyman, K. M. Lawrence, G. L. Mullen, H. Niederreiter, and

N. J. A. Sloane. Updated tables of parameters of (t, m, s)-nets. J. Comb.

Designs, 7:381–393, 1999.

[CM96]

R. E. Caﬂisch and W. Morokoﬀ. Valuation of mortgage backed securi-

ties using the quasi-Monte Carlo method.

In International Association

of Financial Engineers First Annual Computational Finance Conference,

1996.

80

[CP76]

R. Cranley and T. N. L. Patterson. Randomization of number theoretic

methods for multiple integration. SIAM J. Numer. Anal., 13:904–914,

1976.

[Fan80]

K. T. Fang. Experimental design by uniform distribution. Acta Math.

Appl. Sinica, 3:363–372, 1980.

[Fau82]

H. Faure. Discr´epance de suites associ´ees `a un syst`eme de num´eration (en

dimension s). Acta Arith., 41:337–351, 1982.

[FHN00] K. T. Fang, F. J. Hickernell, and H. Niederreiter, editors. Monte Carlo

and Quasi-Monte Carlo Methods 2000. Springer-Verlag, Berlin, 2000.

[Fog95]

D. B. Fogel, editor. Evolutionary Computation:Toward a New Philosophy

of Machine Intelligence. IEEE Press, Piscataway, NJ, 1995.

[Fox99]

B. L. Fox. Strategies for Quasi-Monte Carlo. Kluwer Academic Publishers,

Boston, 1999.

[FT00]

H. Faure and S. Tezuka. Another random scrambling of digital (t, s)-

sequences. In Fang et al. [FHN00], pages 242–256.

[Gen82]

A. Genz. A Lagrange extrpolation algorithm for sequences of approxi-

mations to multiple integrals. SIAM J. Sci. Stat. Comput., 3:160–172,

1982.

[Gen87]

A. Genz. A package for testing multiple integration subroutines.

In

P. Keast and G. Fairweather, editors, Numerical Integration: Recent De-

81

velopments, Software and Applications, pages 337–340. D. Reidel Publish-

ing, Dordrecht, 1987.

[Gen92]

A. Genz. Numerical computation of multivariate normal probabilities. J.

Comput. Graph. Statist., 1:141–150, 1992.

[Gen93]

A. Genz. Comparison of methods for the computation of multivariate

normal probabilities. Computing Science and Statistics, 25:400–405, 1993.

[Gol89]

D. E. Goldberg, editor. Genetic Algorithmsin Search, Optimization and

Machine Learning. Addison-Wesley, 1989.

[Hei00]

J. Heitk¨otter. http://www.cs.bham.ac.uk/mirrors/ftp.de.uu.net/ec/clife/

www/.

[HH97]

F. J. Hickernell and H. S. Hong. Computing multivariate normal probabil-

ities using rank-1 lattice sequences. In G. H. Golub, S. H. Lui, F. T. Luk,

and R. J. Plemmons, editors, Proceedings of the Workshop on Scientiﬁc

Computing, pages 209–215, Hong Kong, 1997. Springer-Verlag, Singapore.

[HH99]

F. J. Hickernell and H. S. Hong. The asymptotic eﬃciency of randomized

nets for quadrature. Math. Comp., 68:767–791, 1999.

[HH01]

H. S. Hong and F. J. Hickernell.

Implementing scrambled digital nets.

Workshop on the Complexity of Multivariate Problems, October 4–8,

1999, Hong Kong, 2001. submitted to ACM TOMS.

82

[HHLL01] F. J. Hickernell, H. S. Hong, P. L’´Ecuyer, and C. Lemieux. Extensible lat-

tice sequences for quasi-Monte Carlo quadrature. SIAM J. Sci. Comput.,

22:1117–1138, 2001.

[Hic96]

F. J. Hickernell. The mean square discrepancy of randomized nets. ACM

Trans. Model. Comput. Simul., 6:274–296, 1996.

[Hic98]

F. J. Hickernell. A generalized discrepancy and quadrature error bound.

Math. Comp., 67:299–322, 1998.

[Hic99]

F. J. Hickernell. What aﬀects the accuracy of quasi-Monte Carlo quadra-

ture? In Niederreiter and Spanier [NS99], pages 16–55.

[HL98]

P. Hellekalek and G. Larcher, editors. Random and Quasi-Random Point

Sets, volume 138 of Lecture Notes in Statistics. Springer-Verlag, New

York, 1998.

[HW01]

F. J. Hickernell and H. Wo´zniakowski. The price of pessimism for multi-

dimensional quadrature. J. Complexity, 17:625–659, 2001.

[HY01]

F. J. Hickernell and R. X. Yue. The mean square discrepancy of scrambled

(t, s)-sequences. SIAM J. Numer. Anal., 38:1089–1112, 2001.

[Kei96]

B. D. Keister. Multidimensional quadrature algorithms. Computers in

Physics, 10:119–122, 1996.

[Kor59]

N. M. Korobov. The approximate computation of multiple integrals. Dokl.

Akad. Nauk. SSR, 124:1207–1210, 1959. (Russian).

83

[Kur92]

F. Kursawe, editor. Evolutionary Strategies for Vector Optimization. Na-

tional Chiao Tung University, Taipei, 1992.

[Lar98a] G. Larcher. Digital point sets: Analysis and applications. In Hellekalek

and Larcher [HL98], pages 167–222.

[Lar98b] G. Larcher. On the distribution of digital sequences. In H. Niederreiter,

P. Hellekalek, G. Larcher, and P. Zinterhof, editors, Monte Carlo and

quasi-Monte Carlo methods 1996, volume 127 of Lecture Notes in Statis-

tics, pages 109–123. Springer-Verlag, New York, 1998.

[Lic98]

J. Lichtner.

Iterating an α-ary gray code. SIAM J. Discrete Math.,

11(3):381–386, 1998.

[Loh01] W. L. Loh. On the asymptotic distribution of scrambled net quadrature,

2001. submitted for publication.

[Mat98]

J. Matousek. On the L2-discrepancy for anchored boxes. J. Complexity,

14:527–556, 1998.

[MS67]

J. McNamee and F. Stenger. Construction of fully symmetric numerical

integration formulas. Numer. Math., 10:327–344, 1967.

[Nie87]

H. Niederreiter.

Point sets and sequences with small discrepancy.

Monatsh. Math., 104:273–337, 1987.

[Nie88]

H. Niederreiter. Low discrepancy and low dispersion sequences. J. Number

Theory, 30:51–70, 1988.

84

[Nie92]

H. Niederreiter. Random Number Generation and Quasi-Monte Carlo

Methods. CBMS-NSF Regional Conference Series in Applied Mathemat-

ics. SIAM, Philadelphia, 1992.

[Nie95]

H. Niederreiter. New developments in uniform pseudorandom number and

vector generation. In Niederreiter and Shiue [NS95], pages 87–120.

[Nie99]

H. Niederreiter. Constructions of (t, m, s)-nets.

In Niederreiter and

Spanier [NS99], pages 70–85.

[NS95]

H. Niederreiter and P. J.-S. Shiue, editors. Monte Carlo and Quasi-Monte

Carlo Methods in Scientiﬁc Computing, volume 106 of Lecture Notes in

Statistics. Springer-Verlag, New York, 1995.

[NS99]

H. Niederreiter and J. Spanier, editors. Monte Carlo and Quasi-Monte

Carlo Methods 1998. Springer-Verlag, Berlin, 1999.

[NX96]

H. Niederreiter and C. Xing. Quasirandom points and global function

ﬁelds. In S. Cohen and H. Niederreiter, editors, Finite Fields and Appli-

cations, number 233 in London Math. Society Lecture Note Series, pages

269–296. Cambridge University Press, 1996.

[NX98]

H. Niederreiter and C. Xing. Nets, (t, s)-sequences and algebraic geometry.

In Hellekalek and Larcher [HL98], pages 267–302.

[Owe95] A. B. Owen. Randomly permuted (t, m, s)-nets and (t, s)-sequences. In

Niederreiter and Shiue [NS95], pages 299–317.

85

[Owe97a] A. B. Owen. Monte Carlo variance of scrambled equidistribution quadra-

ture. SIAM J. Numer. Anal., 34:1884–1910, 1997.

[Owe97b] A. B. Owen. Scrambled net variance for integrals of smooth functions.

Ann. Stat., 25:1541–1562, 1997.

[Owe98] A. B. Owen. Scrambling Sobol’ and Niederreiter-Xing points. J. Com-

plexity, 14:466–489, 1998.

[Owe99] A. B. Owen. Monte Carlo, quasi-Monte Carlo, and randomized quasi-

Monte Carlo. In Niederreiter and Spanier [NS99], pages 86–97.

[Pat68]

T. N. L. Patterson. The optimum addition of points to quadrature for-

mulae. Math. Comp., 22:847–856, 1968.

[Pir02]

G. Pirsic. http://www.dismat.oeaw.ac.at/pirs/niedxing.html, 02.

[Pir00]

G. Pirsic. A software implementation of niederreiter-xing sequences. In

Fang et al. [FHN00], pages 434–445.

[PT96]

A. Papageorgiou and J. F. Traub. Beating Monte Carlo. Risk, 9(6):63–65,

1996.

[PT97]

A. Papageorgiou and J. F. Traub. Faster evaluation of multidimensional

integrals. Computers in Physics, 11:574–578, 1997.

[Sch99a] W. Ch. Schmid. The exact quality parameter of nets derived from sobol’

and niederreiter sequences. In eds O. Illiev et al., editor, Recent Advances

in Numerical Methods and Applications, pages 287-295. World Scientiﬁc,

1999.

86

[Sch99b] W. Ch. Schmid. Improvements and extensions of the ”salzburg tables” by

using irreducible polymomials. In Niederreiter and Spanier [NS99], pages

476–447.

[SJ94]

I. H. Sloan and S. Joe. Lattice Methods for Multiple Integration. Oxford

University Press, Oxford, 1994.

[Slo85]

I. H. Sloan. Lattice methods for multiple integration. J. Comput. Appl.

Math., 12 & 13:131–143, 1985.

[Sob67]

I. M. Sobol’. The distribution of points in a cube and the approximate

evaluation of integrals. U.S.S.R. Comput. Math. and Math. Phys., 7:86–

112, 1967.

[Tez95]

S. Tezuka. Uniform Random Numbers: Theory and Practice. Kluwer

Academic Publishers, Boston, 1995.

[Wah90] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia,

1990.

[YH02]

R. X. Yue and F. J. Hickernell. The discrepancy and gain coeﬃcients of

scrambled digital nets. J. Complexity, 18:135–151, 2002. to appear.

[YM99]

R. X. Yue and S. S. Mao. On the variance of quadrature over scrambled

nets and sequences. Statist. Probab. Lett., 44:267–280, 1999.

[Yue99]

R. X. Yue. Variance of quadrature over scrambled unions of nets. Statist.

Sinica, 9:451–473, 1999.

87

