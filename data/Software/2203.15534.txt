Design and Implementation of Knowledge Base for
Runtime Management of Software Deﬁned
Hardware

Hongkuan Zhou†, Ajitesh Srivastava†, Rajgopal Kannan‡, Viktor Prasanna†

†University of Southern California

‡Army Research Lab-West

Los Angeles, USA
{hongkuaz, ajiteshs, rajgopak, prasanna}@usc.edu

2
2
0
2

r
a

M
8
2

]
E
S
.
s
c
[

1
v
4
3
5
5
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—Runtime-reconﬁgurable software coupled with re-
conﬁgurable hardware is highly desirable as a means to-
wards maximizing runtime efﬁciency without compromising pro-
grammability. Compilers for such software systems are extremely
difﬁcult
to design as they must leverage different types of
hardware at runtime. To address the need for static and dynamic
compiler optimization of workﬂows matched to dynamically
reconﬁgurable hardware, we propose a novel design of the central
component of a dynamic software compiler for software deﬁned
hardware. Our comprehensive design focuses not just on static
knowledge but also on semi-supervised extraction of knowledge
from program executions and developing their performance
models. Speciﬁcally, our novel dynamic and extensible knowledge
base 1) continuously gathers knowledge during execution of
workﬂows 2) identiﬁes optimal
implementations of workﬂows
on optimal (available) hardware conﬁgurations. It plays a hub
role in storing information from, and providing information to
other components of the compiler, as well as the human analyst.
Through a rich tripartite graph representation, the knowledge
base captures and learns extensive information on decomposition
and mapping of code steps to kernels and mapping of kernels
to available hardware conﬁgurations. The knowledge base is
implemented using the C++ Boost Library and is capable of
quickly processing ofﬂine and online queries and updates. We
show that our knowledge base can answer queries in 1ms
regardless of the number of workﬂows it stores. To the best of
our knowledge, this is the ﬁrst design of a dynamic and extensible
knowledge base to support compilation of high-level languages
to leverage arbitrary reconﬁgurable platforms.

Index Terms—Dynamic compiler, Reconﬁgurable architec-

tures, Knowledge management

I. INTRODUCTION

The modern world is driven by information - the increasing
availability of information must be matched by large scale
computational algorithms for processing this data. With the
emergence of diverse, heterogeneous system architectures, a
major challenge for algorithm developers is balancing the
tradeoff between algorithm runtime efﬁciency and ease of
implementation on available hardware. Consider two extreme
cases - general purpose CPUs versus application speciﬁc
integrated circuits (ASICs). Developers can write in a concise
high-level programming language to efﬁciently implement
algorithms on CPUs. Thus the former offers very good pro-
grammability but (relatively) low compute efﬁciency. Con-
versely, ASICs, being specialized, maximize runtime efﬁ-
ciency at the cost of poor programmability. Indeed, due to

the low ﬂexibility of ASICs, only high priority algorithms are
considered for implementation. For other algorithms, one has
to sacriﬁce compute efﬁciency by implementing on alternative
hardware like ﬁeld programmable gate arrays (FPGAs) or
CPUs. These algorithms can run much slower than on ASICs.
This poses the question, how to balance the tradeoff between
programmability and run-time efﬁciency for reconﬁgurable
hardware which runs the spectrum between CPUs and ASICs.

A. SDH Program

To achieve both high programmability and compute ef-
ﬁciency, the key is to build a system consisting of tightly
coupled hardware and software. The DARPA Software De-
ﬁned Hardware (SDH) [1] program aims to combine runtime-
reconﬁgurable hardware with a dynamic (reconﬁgurable) soft-
ware compiler that achieves comparable compute efﬁciency to
ASICs but without the associated cost and time of develop-
ment, or application ﬁeld-limitations. The target algorithms of
the DARPA SDH program are a selection of data-intensive
workﬂows from the domains of image, video, audio, text,
signal, and graph.

Hardware in the DARPA SDH program is expected to be
conﬁgurable for executing different types of computations as
efﬁciently as ASICs (under different conﬁgurations). Con-
comitantly, the software should be able to compile from high-
level languages such as Python to generate executable code as
well as the optimal hardware conﬁguration. The main feature
of this coupled software-hardware system is that it allows a
workﬂow to change the hardware conﬁguration at runtime.
Further, the system is expected to be capable of reusing the
similar hardware conﬁguration in executed workﬂows for new
workﬂows. This requires the system to be able to store, analyze
and retrieve historical logs on compilation and execution. The
ultimate goal of the SDH program is to be able to take
advantage of data-dependent optimizations that are even better
than today’s ASICs.

B. Main Contributions

To address the need for dynamic compiler optimizations
of workﬂows, we propose a novel dynamic and extensible
knowledge base which 1) continuously gathers knowledge

 
 
 
 
 
 
during execution of workﬂows 2) identiﬁes optimal imple-
mentations of workﬂows on optimal (available) hardware
conﬁgurations, given the sequence of their constituting steps,
based on compile-time and run-time queries.

i.e.,

Our knowledge base is dynamic and extensible,

it
keeps updating its contents at runtime so that the knowledge
stored is more accurate, and it evolves with time. We conduct
experiments showing that the query time of the knowledge
base is linear with its size. Combined with the result that
a small number of algorithmic building blocks (kernels) can
cover most workﬂows, we conclude that our knowledge base
does not need to be large, and so it can quickly answer queries
from other components of the compiler.

To summarize, our main contributions are as follows:
1) We propose a rich labeled tripartite network representa-
tion of the knowledge base, that gathers the knowledge
of optimized implementations of key algorithmic steps
and kernels on various parameterized hardware.

2) We design a query interface so that other components
of the compiler can interact with the knowledge base at
compile-time as well as execution-time.

3) We present the implementation of the knowledge base

using C++ Boost library.

4) We demonstrate that the knowledge base is capable of
answering queries within 1ms even with large number
of workﬂows.

5) We demonstrate that a small number of kernels can
capture majority of workﬂows, suggesting that our
knowledge base can be lightweight.

II. BACKGROUND AND RELATED WORK

The key to build the software deﬁned hardware that can
achieve both high programmability and efﬁciency is to build
compilers for high-level programming language. Such com-
pilers compile the programs before and during runtime. The
compilers generates hardware conﬁguration when the compu-
tation and communication patterns changes in the programs.

A. DDARING Project

One of the approaches for the dynamic hardware and
software compilers of the SDH [1] program is the DDARING
project [2]. The knowledge base is designed as a center
component
in the DDARING project. Figure 1 shows an
overview of the technical approach of the DDARING project.
The goal is to accelerate the workﬂows for data-intensive
analyses to achieve near-ASIC performance. At
the same
time, the DDARING project can achieve the productivity that
analysts have come to expect from modern high level problem-
solving environments such as Julia and Python. The DDAR-
ING project encapsulates a uniﬁcation of software compilation
and hardware reconﬁguration techniques, and is comprised
of six components: the programming model, the knowledge
base, the static high-level optimizer, the dynamic high-level
optimizer, the auto-tuner and proﬁling, reconﬁguration and
deployment system. The workﬂows are ﬁrst ﬁtted into a
programming model and compiled statically. At runtime, the

dynamic optimizer changes the conﬁguration of the hardware
to achieve optimal compute efﬁciency. This process of static
and dynamic compilation needs the supporting information
from the metadata of the workﬂow and status on previous
executions.

Speciﬁcally, developers would implement some workﬂows
in a high-level programming language, i.e., Python. For the
computational intensive parts that consume most execution
time in the workﬂows,
the developers instead implement
these parts in the programming model built for DDARING
called Intrepydd. There are also many pre-implemented high
efﬁciency functions in Intrepydd language that the developers
can use directly. Since most time of the workﬂows is spent
in the computational intensive parts, this helps the developers
to leverage the maximum convenience of high-level program-
ming language. For performance, the Intrepydd code is ﬁrst
optimized by a static data-aware optimizer then optimized by
a dynamic kernel reoptimizer and imported to the Python
program. These two optimizers endow these computational
intensive parts to leverage the advantages of reconﬁgurable
hardware and to achieve near-ASIC performance.

In the DDARING project, the key component is the knowl-
edge base. To support the static data-aware optimizer and
dynamic kernel reoptimizer, the knowledge base captures both
the meta-data in the workﬂows and historical performance of
similar workﬂows. To achieve high performance, these two
optimizers compile and reconﬁgure the hardware at runtime
when necessary. The knowledge base also treats the knowledge
in a ﬁne-grained level to uphold these ﬁne-grained optimizers.
One workﬂow is divided into many parts while similar parts
in different workﬂows are stored and analyzed together in the
knowledge base. At compilation and runtime, the knowledge
base gathers information and answers queries from other
components. This requires the knowledge base to respond
rapidly even with a large amount of stored contents. To store
ﬁne-grained knowledge and to respond in real time are the
two main challenges when designing and implementing the
knowledge base.

B. Related Work

Task scheduling in heterogeneous hardware systems has
been widely researched [3]–[6]. Since the introduction of
reconﬁgurable hardware systems, there also has been consider-
able research done in the ﬁeld of task scheduling and hardware
mapping [7]–[10]. Compilers for reconﬁgurable hardware need
to co-optimize processor conﬁguration and code using a itera-
tive procedure. The problem is that this optimization space is
much larger than what traditional compilers have to contend
with. Hence, it is necessary to have a knowledge base that
remembers the explored design space and give advice on
further space.

Existing research on predicting the performance of a work-
ﬂow tends to focus on accurate prediction before execution
based on the matadata or its reference performance on other
hardware conﬁguration. For instance, [11] proposed an ac-
curate way to predict GPU performance based on reference

Fig. 1. Overview of DDARING Project [2]

Fig. 2.
DDARING Project

Interactions of the Knowledge Base with Other Components in the

performance and data access pattern on CPU. However, once
the workﬂows have been executed on GPU several times, the
compiler should gather these real performance metrics and
analyze these data to predict more accurately for the next
runs. Existing compilers designed for heterogeneous archi-
tectures, such as HEFT [12], MATEHa [13], etc. decompose
the workﬂows into separate tasks and optimize the task graph
execution.

These approaches assume a prior knowledge of communi-
cation and computation models and task decomposition. On
the contrary, the knowledge base differentiates from these
approaches that it is more comprehensive. The knowledge
base focuses not just on optimal mapping but also on semi-
supervised extraction of knowledge from the workﬂows and
development of the performance models. The knowledge base
also captures the historical runtime performance data and
updates based on the analyses from these data. This allows the
knowledge base to evolve across executions of different work-
ﬂows and to gain performance with the captured knowledge.
Eventually, the knowledge base aids the compiler to achieve
near-ASIC performance on software deﬁned hardware.

III. DESIGN

To construct

the knowledge base, we view a workﬂow
as a composition of several recurring time-consuming code
segments which perform some speciﬁc tasks or operations. We
denote such code segments as kernel primitives. The knowl-
edge base stores extensive information about decomposition
of distinct code modules (steps) into kernels and the optimal
mapping of kernels to the underlying reconﬁgurable hardware.

A. Representation

The knowledge base comprises of a rich tripartite graph
representation G(V1, V2, V3, E) (TGR) as shown in ﬁgure 3.
The ﬁrst layer V1 consists of Domain Speciﬁc Language level
(DSL) steps. A step is a pattern of computation that can
be mapped to one or more kernels. The set V1 is the steps

discovered in a codebase which includes a wide range of
data intensive workﬂows. Each u ∈ V1 is attributed with a
feature vector (cid:126)ua = ({P}, {M}) to quantitatively represent
computational and data access patterns of the step (or part
of the workﬂow) such as access pattern irregularity, data
precision, computation to communication ratio and etc. A
workﬂow is a ﬁnite-state-machine of the steps in V1. A step
may have multiple variants in terms of kernels.

Fig. 3. Tripartite Representation of Knowledge Base

Nodes in set V2 represent bare bone tasks or kernels
that form the core of the knowledge base. The connectivity
between a step u ∈ V1 and the set of kernels t ∈ V2
delineates the lower level decomposition of u. Each edge
e = (u, t) will be attributed with a tuple (we, did). The weight
assignment we encodes meta-data of u to be passed to t. The
indexing function did enables the proposed knowledge base
representation to support more than one mapping of a step. For
instance, a convolution operation can be represented as 1) a
sequence of frequency domain transformation, multiplication

IngestOutputAnalyzeMapOutputDatasetsInputDatasetsProgModelStatic High-Level Optimizer Dynamic Low-Level Auto Tuning Knowledge Base AlgorithmsC&C1C&C2C&C3AnalystTimeDataKBProgramming ModelStatic High-Level OptimizerIdentify “operators”Identify Kernel to HW mappingsDynamic High-Level OptimizerIdentify optimized mapping costs Auto TuningObtain feature space for optimizationProfiling, ReconfigurationUpdate modelsKernelsHardwareConfigurationsStepsWorkflowMappingstepto kernelPerformance modeland inverse transformation steps (did = 0), or 2) a series
of sliding window dot products (did = 1). Frequently co-
occurring kernels may be merged to form one kernel for
optimized implementations.

The key characteristic of the TGR is that it exposes the
resource requirements of the kernels that allow the knowledge
base to obtain optimal program decomposition and task order-
ing across different segments of a given workﬂow.

The kernels t ∈ V2 are computationally primitive units
and feature mappings to the underlying reconﬁgurable hard-
ware platform and the building blocks in it. The hardware
conﬁgurations and building blocks are represented as vertices
v ∈ V3. Note that a kernel can be mapped to one hardware
conﬁguration (T , C) in multiple ways (T ) with different
performance costs (C). This is desirable as it would provide
ways to schedule a task even if the optimal hardware block
for execution is busy. Vertices in V3 also store their cost of
reconﬁguration which is taken into account while computing
the optimal mapping and overall cost of kernel execution.

Figure 4 shows how the knowledge base stores the node
classiﬁcation workﬂow. This workﬂow is manually proﬁled
and its knowledge is inserted into the knowledge base by
human hands. As in the ﬁrst layer V1, the workﬂow is decom-
posed into four steps: sampling of the neighbors; evaluating
the loss function; applying the gradients; and inferencing on
test nodes. The ﬁrst three steps are executed recursively until
some termination criteria, followed by the execution of the
last step. Based on the metadata of each steps, sampling is
mapped to the BFS tree. Loss and gradient are each mapped to
the corresponding neural network propagation. Note the dotted
line from inference to forward propagation to the FPGA. Since
inference does not required high precision as its metadata
suggests, the knowledge base captures it and maps it to the
FPGA hardware through some quantization technique to ﬁt
the FPGA resources.

Fig. 4. Running the Node Classiﬁcation Workﬂow using the Knowledge Base

B. Creation

The creation of knowledge base consists of two parts: ofﬂine

discovery and dynamic updates.

For ofﬂine discovery, we start with some chosen typical
workﬂows. These workﬂows are ﬁrst decomposed into steps
and the steps are then mapped to all possible hardware conﬁg-
urations. We manually proﬁle and construct the performance
model for each pair and enter the model into the knowledge
base. To begin the construction of the knowledge base, we have
two types of kernel: (i) kernels with well-known optimized
mappings to hardware such as dense matrix computation,
sparse-matrix computation, FFT, etc. (ii) kernels for which op-
timal mappings are not known. We apply clustering algorithm
to ﬁnd associations of Type (ii) kernels with Type (i) kernels
so that we can identify mapping of the input program to the
kernels. Intuitively, if an unknown step P has high similarities
with a known step M , it is likely that the optimal hardware
conﬁguration for M will achieve high performance on P as
well. However, instead of using the conﬁguration of a single
step, we use the similarities with multiple steps obtained by
clustering, to generate performance cost estimates for kernels
in P for possible conﬁgurations v ∈ V3. We also take into
account the fact that a step is a sequence of kernels, and
the ordering may result in different preferences of hardware
conﬁgurations. To address such sequential nature of workﬂows
that can be solved by using sequence labeling algorithms. In
the SDH [1] program, we proﬁle the codebases provided by
the HIVE [14] and D3M corpora and create the knowledge
base accordingly.

For dynamic updates, the proﬁling and reconﬁguration com-
ponent in the DDARING project starts with the performance
model obtained in ofﬂine discovery. As it analyzes and runs
the program, it will update its own prediction model and
also share the updates with the knowledge base dynamically.
Thus, the knowledge base will not only be able to update
existing cost models, but also create kernels and learn optimal
conﬁgurations at runtime.

C. Interaction with Other Components

The knowledge base is the hub of the DDARING project.
All other components provide inputs to, or consult queries
from the knowledge base. Figure 2 shows the interactions
between the knowledge base and the other ﬁve components.
Speciﬁcally,

• Programming Model: The knowledge base obtains input
datasets and supervision from analyst and bootstrap. The
knowledge base analyzes the programming model
to
specify and segments steps into kernels.

• Static High-Level Optimizer: Static High-Level Opti-
mizer consults the knowledge base to identify conﬁgu-
rations for key steps. The knowledge base also uses the
Static High-Level Optimizer in the ofﬂine discovery.
• Dynamic High-Level Optimizer: Dynamic High-Level
Optimizer consults the knowledge base to obtain map-
pings of kernels to hardware building blocks and to
construct reconﬁguration cost models and to perform low-
level optimizations.

KernelsHardwareConfigurationsBFS Tree16 coresTree Interconnections3840CUDA coresAccelerators with Coherent MemoryForward Prop.Backward Prop.FPGA1451k logic cells……SamplingForward Prop.Forward Prop.Forward Prop.Forward Prop.Forward Prop.Loss CalculationStepsGradientInferencingNode Classification• Auto Tuning: Auto Tuning consults the knowledge base
to enumerate the set of available optimizations and con-
ﬁgurations to ﬁnd optimal tuning of relevant parameters.
• Proﬁling, Reconﬁguration: Proﬁling, Reconﬁguration and
Deployment Platform interacts with the knowledge base
to obtain a previously learned performance model and
update it online.

These input and output are expected to occur sequentially
in each stage of the workﬂows. The interactions between
programming model and static high-level optimizer happen
before starting the execution of the workﬂow. The interaction
between dynamic high-level optimizer happens before the
execution of a kernel. The interaction between auto tuning
happens at the runtime of a kernel. The interaction between
proﬁling and reconﬁguration happens after a kernel ﬁnishes
execution.

IV. IMPLEMENTATION

The knowledge base is implemented using the Boost Graph
Library [15], which provides generic and STL-like graph
interface and graph components for C++. We choose to use
C++ to implement the knowledge base since the code to exe-
cute on reconﬁgurable hardware is also C++. To ensure good
extendibility both in contents and functionality, the knowledge
base is stored as a collection of objects. We implement ﬁve
C++ classes:

• Step t: represents steps in a workﬂow. This class stores
the static features of the step and supplies the features to
the map from a step to a kernel.

• Kernel t: represents identiﬁed and unidentiﬁed kernels.
• Hardware t:

represents hardware conﬁguration. This
class stores hardware characteristics needed when pre-
dicting the performance model.

• Kernel map t: represents the map from step to kernel.
This class stores the learnable models give advice on
whether one kernel can be used to accomplish the step.
• Performance model t: represents the map from kernel
to hardware conﬁgurations. This class stores the learn-
able models that predict the execution time and energy
requirement of executing one kernel on the hardware
conﬁguration based on the metadata of the workﬂow. This
model also give advice on which hardware conﬁguration
to use for a given kernel.

To realize the tripartite representation of the knowledge
base, we use adjacency list with customized vertex properties
and edge properties. As discussed in section III-C, we do
not consider concurrent queries or updates to the knowledge
base. Hence we do not add any locks to these classes or the
tripartite graph. Each node in the graph represents either a
step, a kernel or a hardware conﬁguration. For node and edge
properties, there are indicators distinguishing different classes
of the nodes, as well as pointers that point to the subclass (or
parent class) of the node. The deﬁnition of the Boost graph,
vertex properties and node properties are shown in listing 1.
The graph representation and the classes can be saved and

// one vertex captures a step, a kernel

or a hardware

typedef struct vertex_properties
{

bool is_step = false, is_kernel = false

, is_hardware = false;

int id;
Kernel_t *kernel;
Hardware_t *hardware;
Step_t *step;

}vertex_properties_t;

// one edge captures a performance model

or a kernel map

typedef struct edge_properties
{

bool is_performance_model = false,

is_kernel_map = false;

int id;
Performance_model_t *performance_model;
Kernel_map_t *kernel_map;

}edge_properties_t;

typedef boost::adjacency_list<boost::vecS
, boost::vecS, boost::undirectedS,
vertex_properties_t, edge_properties_t
> graph_t;

Listing 1. Code Snippet on Deﬁning the Knowledge Base

loaded to or from an XML ﬁle so that the identiﬁed knowledge
can be stored and reused. This is helpful when the developers
want to load some pre-learned knowledge base and update it
after discovering new kernels and performance models.

V. PERFORMANCE

The knowledge base should be able to answer queries from
other components in the compiler in real-time. We evaluate
the performance of the knowledge base using three types of
queries,

• Type 1 query: For a given step in the knowledge base,

return all kernels linked with that step.

• Type 2 query: For a given kernel and a given hardware
conﬁguration in the knowledge base, return the perfor-
mance model for executing the kernel on the hardware
conﬁguration.

• Type 3 query: For a given kernel in the knowledge base,
return the performance models for executing the kernels
on all hardware conﬁgurations.

We measure the average query time of these three types
of queries in different sizes of knowledge bases. For each
step, we assume that there are k connections to k kernels,
where k is chosen randomly from the geometric distribution
with probability p. The average number 1/p represents the
average number of kernels connected to one step. We ﬁx the

number of hardware conﬁgurations to be h since the type of
hardware is ﬁxed in most scenario. Figure 5 shows the average
query time for different sizes of the knowledge base. We can
see that the query time increases linearly with the number of
steps in the knowledge base. Type 3 query need the most time
while type 2 query need the least time. All queries could be
answered in 1ms even in a signiﬁcantly large knowledge base
(in reality, the number of steps in the current knowledge base
is less than 200). The peak in average query time is due to
the characteristics of cache.

Fig. 6. Uncoverage of SDH Workﬂows with Number of Kernels: Comparison
of real values with the simulation-based predictions.

VI. DISCUSSION AND CONCLUSION

We proposed a tripartite graph representation design and the
C++ implementation of the knowledge base in the DDARING
[2] project. The knowledge base takes input of datasets of
codes, metadata, prediction model updates, and outputs map-
pings from steps to kernels and predictions of performance
such as execution time, needed by other components in the
DDARING project. As a part of the knowledge base, in a sepa-
rate work, we have also developed light-weight neural-network
based performance models to predict the execution times of
kernels [16]. Our knowledge base is capable of responding to
queries from other parts of the compiler within 1 ms even for
large number of kernels. Although, we experimentally show
that only using few kernels the knowledge base can cover a
large number of workﬂows. In the next step of the DDARING
project, we will show speedup results of executing workﬂows
by the reconﬁgurable hardware and DDARING compiler using
the knowledge base.

We believe that the knowledge base that gathers information
on the metadata of the programs and learns mappings and
performance models is central to compiler optimizations on
reconﬁgurable devices. With the knowledge base, compilers
also beneﬁt when scheduling for non-reconﬁgurable devices
such as the CPU and GPU. Even though the knowledge base
is designed to support the DDARING project, our framework
is generic and can be applied to most hardware platforms and
programs.

ACKNOWLEDGEMENT

This material is based on work supported by the Defense
Advanced Research Projects Agency (DARPA) under BAA
Number HR001117S0055. Any opinions, ﬁndings and con-
clusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views
of DARPA.

Fig. 5. Average query time for different size of the knowledge base with
probability p = 2/3 and number of hardware conﬁgurations h = 10.

We deﬁne the coverage of a workﬂow as the fraction of
its
identiﬁed kernels in the knowledge base linked to all
decomposed steps. For example, a workﬂow is decomposed
into two steps S1 and S2. S1 is linked to kernel K1 and S2
is linked to kernel K2 and K3. If the knowledge base stores
two kernels K1 and K2, we say that the coverage of this
workﬂow in this knowledge base is 2/3 and the uncoverage of
this workﬂow is 1/3. To model the coverage, we assume that
when a randomly selected workﬂow wants to run a kernel, the
kernel is selected base on preferential attachment rule where
the probability of selecting one kernel is proportional to the
sum of frequency of this kernel and a ﬁxed parameter λ.

Figure 6 shows the predicted and real uncoverage of all
workﬂows in the SDH [1] codebases. The predicted uncov-
erage ﬁts the real one accurately with λ = 2.5. We can also
see that the coverage increases exponentially with the number
of kernels in the knowledge base, which means that a small
knowledge base is enough to serve most of the workﬂows.

Although the average query time increases linearly with the
size of the knowledge base, we do not need a large knowledge
base when there is a large number of workﬂows. Combined
with the fact that the query time of knowledge base is linear
with the size of stored kernels, we can conclude that the
knowledge base can answer queries rapidly regardless of the
number of workﬂows captured in that knowledge base.

025050075010001250150017502000Number of Steps0.00.20.40.60.81.01.2Average Query Time (ms)Average Query Time w.r.t Knowledge Base SizeType 1 QueryType 2 QueryType 3 QueryREFERENCES

[1] W. Shen, “Software deﬁned hardware (sdh),” https://www.darpa.mil/

program/software-deﬁned-hardware.

[2] V. Sarkar, “Dynamic data-aware reconﬁguration, integration and gener-

ation (ddaring).”

[3] C. Augonnet, S. Thibault, R. Namyst, and P.-A. Wacrenier, “Starpu:
a uniﬁed platform for task scheduling on heterogeneous multicore
architectures,” Concurrency and Computation: Practice and Experience,
[Online]. Available: https:
vol. 23, no. 2, pp. 187–198, 2011.
//onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1631

[4] T. Yang and A. Gerasoulis, “Pyrros: Static task scheduling and code
generation for message passing multiprocessors,” in The 6th ACM Int’l
Conf. on Supercomputing, 1992, pp. 428–437.

[5] H. Topcuoglu, S. Hariri, and Min-You Wu, “Task scheduling algorithms
for heterogeneous processors,” in Proceedings. Eighth Heterogeneous
Computing Workshop (HCW’99), April 1999, pp. 3–14.

[6] H. El-Rewini, H. H. Ali, and T. Lewis, “Task scheduling in multipro-
cessing systems,” Computer, vol. 28, no. 12, pp. 27–37, Dec 1995.
[7] A. Ahmadinia, C. Bobda, D. Koch, M. Majer, and J. Teich, “Task
scheduling for heterogeneous reconﬁgurable computers,” in Proceedings
of the 17th Symposium on Integrated Circuits and System Design, ser.
SBCCI ’04. New York, NY, USA: ACM, 2004, pp. 22–27. [Online].
Available: http://doi.acm.org/10.1145/1016568.1016582

[8] J. Resano, D. Mozos, D. Verkest, and F. Catthoor, “A reconﬁgurable
manager for dynamically reconﬁgurable hardware,” IEEE Design Test
of Computers, vol. 22, no. 5, pp. 452–460, Sep. 2005.

[9] K. Danne and M. Platzner, “A heuristic approach to schedule periodic
real-time tasks on reconﬁgurable hardware,” in International Conference
on Field Programmable Logic and Applications, 2005., Aug 2005, pp.
568–573.

[10] K. Bondalapati and V. K. Prasanna, “Loop pipelining and optimization

for run time reconﬁguration,” in IPDPS Workshops, 2000.

[11] I. Baldini, S. J. Fink, and E. Altman, “Predicting gpu performance from
cpu runs using machine learning,” in 2014 IEEE 26th International Sym-
posium on Computer Architecture and High Performance Computing,
Oct 2014, pp. 254–261.

[12] H. Topcuoglu and S. H. and, “Performance-effective and low-complexity
task scheduling for heterogeneous computing,” IEEE Transactions on
Parallel and Distributed Systems, vol. 13, no. 3, pp. 260–274, March
2002.

[13] L. De Giusti, F. Chichizola, M. Naiouf, and A. De Giusti, “Mapping
tasks to processors in heterogeneous multiprocessor architectures: The
mateha algorithm,” in 2008 International Conference of the Chilean
Computer Science Society, Nov 2008, pp. 85–91.

[14] W. Shen, “Hierarchical identify verify exploit (hive),” https://www.darpa.

mil/program/hierarchical-identify-verify-exploit.

[15] “Boost c++ libraries,” https://www.boost.org/.
[16] N. Zhang, A. Srivastava, and V. K. Prasanna, “Portable lightweight deep
learning models for kernel performance,” in International Workshop on
Performance, Protability and Productivity in HPC (P3HPC)(submited),
2019.

