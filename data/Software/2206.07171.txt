2
2
0
2

g
u
A
8

]

V
C
.
s
c
[

2
v
1
7
1
7
0
.
6
0
2
2
:
v
i
X
r
a

Automated image analysis in large-scale cellular
electron microscopy: A literature survey

Anusha Aswatha,b,∗, Ahmad Alsahafb, Ben N. G. Giepmansb, George
Azzopardia

aBernoulli Institute of Mathematics, Computer Science and Artiﬁcial Intelligence,
University Groningen, Groningen, The Netherlands
bDept. Biomedical Sciences of Cells and Systems, University Groningen, University Medical
Center Groningen, Groningen, The Netherlands

Abstract

Large-scale electron microscopy (EM) datasets generated using (semi-) auto-
mated microscopes are becoming the standard in EM. Given the vast amounts
of data, manual analysis of all data is not feasible, thus automated analysis is
crucial. The main challenges in automated analysis include the annotation that
is needed to analyse and interpret biomedical images, coupled with achieving
high-throughput. Here, we review the current state-of-the-art of automated
computer techniques and major challenges for the analysis of structures in cel-
lular EM. The advanced computer vision, deep learning and software tools that
have been developed in the last ﬁve years for automatic biomedical image anal-
ysis are discussed with respect to annotation, segmentation and scalability for
EM data. Integration of automatic image acquisition and analysis will allow for
high-throughput analysis of millimeter-range datasets with nanometer resolu-
tion.

Keywords: Electron microscopy, segmentation, supervised, unsupervised,
machine learning, deep learning, AI

1. Large-scale cellular EM

Electron microscopy (EM) is widely used in life sciences to study tissues,
cells, subcellular components and (macro) molecular complexes at nanometer
scale. Two-dimensional (2D) EM aids in diagnosis of diseases, but routinely it
still depends upon biased snapshots of areas of interest. Automated pipelines
for collection, stitching and open access publishing of 2D EM have been pio-
neered for both transmission EM (TEM) images (Faas et al., 2012) as well as
scanning TEM (STEM) (Sokol et al., 2015) for acquisition of areas up to 1mm2
at nanometer-range resolution, Table. 1. Nowadays, imaging of large areas at

∗Corresponding author:
Email address: a.aswath@rug.nl (Anusha Aswath)

 
 
 
 
 
 
Figure 1: Typical large-scale EM allows to analyze tissue at a high resolution. Overview
and snapshots of several cellular, subcellular and macromolecular structures of which up to a
million can be present per dataset (de Boer et al., 2020). Bars: red 10µm; green 1µm; white
0.5µm. Full access to digital zoomable data at full resolution is via http://www.nanotomy.org.

high resolution is entering the ﬁeld as a routine method and is provided by most
EM manufacturers. This we term nanotomy, for nano-anatomy (Ravelli et al.,
2013; de Boer et al., 2020; Dittmayer et al., 2021). The large-scale images allow
for open access world-wide data sharing; see nanotomy.org1for more than 40
published studies and the accessible nanotomy data.

A typical nanotomy dataset has a size of 5-25GB at 2.5nm pixel size. Nan-
otomy allows scientists to pan and zoom through diﬀerent tissues or cellular
structures in a Google Earth-like manner (Fig. 1). Large-scale 2D EM provides
unbiased data recording to discover events such as pathogenesis of diseases at the
supra-cellular level for morphological changes. Moreover, nanotomy allows for
the quantiﬁcation of subcellular hallmarks. With state-of-the-art 2D EM tech-

1www.nanotomy.org
2https://myscope.training/

2

Table 1: Major large-scale EM techniques. Note that the biomaterial for the techniques below
is stained with heavy metals to generate contrast. For further information see the MyScope
website2 and the papers reviewed by Peddie and Collinson (2014) and Titze and Genoud
(2016).

2D EM

Methodology

Transmission Electron
Microscopy (TEM)

Scanning Electron Microscopy
(SEM)

A wideﬁeld electron beam illuminates an ultra-thin spec-
imen and transmitted electrons are detected at the other
side of the sample. The structure that is electron dense
appears dark and others appear lighter depending on
their (lack of) scattering.

The raster scanning beam interacts with the material
and can result in back scattering or formation of sec-
ondary electrons. Their intensity reveals sample infor-
mation.

Scanning Transmission
Electron Microscopy (STEM)

SEM on ultrathin sections and using a detector for the
transmitted electrons.

3D EM

Serial section TEM (ssTEM)
or SEM (ssSEM)

Serial Block-face scanning
EM (SB SEM)

Focused Ion Beam SEM
(FIB-SEM)

Volume EM technique for examining 3D ultrastructure
by scanning adjacent ultrathin (typical 60-80 nm) sec-
tions using TEM or SEM, respectively.

The block face is scanned followed by removal of the
top layer by a diamond knife (typical 20-60 nm) and
the newly exposed block face is scanned. This can be
repeated thousands of times.

Block face imaging as above, but sections are repeatedly
removed by a focused ion beam that has higher precision
than a knife (typically down to 4 nm) suitable for smaller
volumes.

nology, such as multibeam scanning EM (Eberle et al., 2015; Ren and Kruit,
2016), up to 100 times faster acquisition and higher throughput allows for imag-
ing of tissue-wide sections in the range of hours instead of days. For a side by
side example of single beam versus multibeam nanotomy, see de Boer and Giep-
mans (2021). Given the automated and faster image acquisition in 2D EM a
data avalanche (petabyte-range per microscope/month) will soon be a reality.
Automated large-scale three-dimensional (3D) or volume EM (vEM), which
creates a stack of images, is also booming as reviewed by Peddie and Collinson
(2014); Titze and Genoud (2016); Peddie et al. (2022) (Table. 1). Examples
include whole-cell volume reconstruction of up to 35 cellular organelle classes
(Heinrich et al., 2021). The data acquisition time is not the bottleneck anymore,
but data analysis is. Note that one person needed two weeks to manually label
a fraction (1µm3) of the imaged volume. The whole cell could take 60 person
years. Hence, the urgent need for automatic image analysis.

Semantic segmentation of EM images is the automatic process of mapping
each pixel to known or newly discovered classes of subcellular structures. The
challenges of automatic segmentation of EM images are due to the rendering in
one color channel (grayscale) of subcellular structures with diﬀerent sizes, shapes

3

and heteregoneous appearances, further surrounded by complicated structures.
2D EM datasets only have neighbouring (X/Y axes) context as reference for
structural analysis. The vEM datasets leverage knowledge from adjacent sec-
tions (Z−axis) that share high resemblance, and therefore aids in better re-
construction of the volume. Additionally, large-scale 2D EM images are more
spatially distributed across cells than traditional EM micrographes of a selected
region or vEM that occupy a smaller spatial area, despite both large-scale 2D
EM and vEM containing gigabytes of information.
It requires global image
processing without losing coherent information.

Here, we review automated methods for large scale EM image segmentation

and analysis.

2. Deep learning and segmentation

Deep learning has become the state-of-the-art methodology for many com-
puter vision tasks, including segmentation. This has been a paradigm shift
when compared to traditional segmentation methods, which were generally per-
formed using machine learning classiﬁers such as Adaboost, Random forests,
and Support Vector Machines (SVMs), which required domain-oriented, hand-
crafted features as inputs (Table 2). The introduction of Deep Neural Networks
(DNNs) has enabled automatic extraction of hierarchical representations from
raw image data (LeCun et al., 2015). Convolutional Neural Networks (CNN)
have signiﬁcantly advanced image-related tasks such as classiﬁcation, detection,
and segmentation with end-to-end learning from feature extraction to predic-
tion.

A typical CNN consists of convolutional ﬁlters or weight sharing ﬁlter banks
in each layer of the neural network. Convolutional ﬁlters are linear functions
that are used for low-level basic operations, such as blurring and edge detection.
A 2D convolutional ﬁlter uses a 2D matrix, referred to as a kernel, centered on
a local region in a given image and applies a linear operation between the kernel
coeﬃcients and the respective pixel values in the local region concerned. The
resulting scalar value is the response of the ﬁlter to the considered region. The
ﬁlter is then slided across the whole image to compute the responses at every
location. All responses form what is known as a response or a feature map which
has the same size as the input image if the sliding is done one pixel at a time. 3D
ﬁlters operate in a similar way but use 3D kernels and are applied to 3D volumes
of vEM images. In a CNN, the ﬁlters allow for weight sharing throughout the
image capturing structured data using translational and rotational invariance
(Krizhevsky et al., 2012; Yamashita et al., 2018). The spatial extent of the
connectivity of a ﬁlter with local pixels is called the receptive ﬁeld or the kernel
size. The response maps obtained by such ﬁlters are then processed by a non-
linear activation function, before being downsampled by a pooling unit in order
to learn abstract representations (LeCun et al., 2015). Finally, the response
maps are fed to a fully connected layer which determines a label for the given
image.

4

Trainable

Linear

Non-linear

Weak and
Strong
learners

Ensemble
modeling

s
r
o
t
p
i
r
c
s
e
D

s
r
e
ﬁ
i
s
s
a
l
C

s
r
e
n
r
a
e
l
-
a
t
e

M

g
n
i
n
r
a
e
L

p
e
e
D

Table 2: Deﬁnitions of common terms used in the literature of machine/deep learning frame-
works.

Terms

Deﬁnition

Notes

Handcrafted Functions that are implemented using domain ex-
pertise for the extraction of features.

Examples include the Canny edge detector, Harris corner detector, and
more sophisticated ones, such as HOG, LBP, SIFT, and SURF. (Dalal
and Triggs, 2005; Wang and He, 1990; Lowe, 2004)

Functions that learn features from given training
sets. Such descriptors can be contour-, color- or
texture-based.

The trainable COSFIRE approach is an example of contour- and color-
based trainable descriptor (Azzopardi and Petkov, 2012; Gecer et al.,
2017).

A model that separates a given feature space with a
linear boundary. In a 2D space the model is simply
a line, while a plane and hyperplanes are used for
3D and higher dimensional spaces.

Logistic regression and Support Vector Machines (SVMs) are examples
of linear classiﬁers. The latter learns a linear classiﬁer after transform-
ing the input features to a higher dimensional space. (Dreiseitl and
Ohno-Machado, 2002; Boser et al., 1992)

A model that learns a non-linear function to sepa-
rate classes. The degree of non-linearity is arbitrary.
Generalization error tends to increase with increas-
ing non-linearity. Regularization is a technique used
to keep a good tradeoﬀ between non-linearity and
generalization.

Neural networks (NNs), such as deep NNs (DNNs), fully connected
NNs (FCNNs) and convolutional NNs (CNNs), are end-to-end models
that learn features from the training set and a non-linear function that
maps the input to output via the learned features (Glorot and Bengio,
2010)

A weak learner is a model that performs just better
than random guessing, while a strong learner is a
model that achieves high performance.

Decision trees are data sttructures that deﬁne classes. Decision trees
with with very few and with many layers can be considered as weak
and strong learners, respectively (Drucker and Cortes, 1995)

The combination of outcomes of multiple learners
to achieve better predictive performance while re-
ducing the risk of overﬁtting.

Deep
neural
networks
(DNNs)

Deep learning is a subset of machine learning, that
uses a neural network with three or more layers.
Such networks automatically extract input features
from unstructured data unlike conventional ma-
chine learning that use hand-crafted features.

Convolutional
Neural
Networks
(CNN)

DNNs with convolutional layers. Convolutional lay-
ers consist of nodes that apply linear ﬁlters by scan-
ning a given image in overlapping blocks of pixels
to extract features. Besides convolutions each layer
includes a nonlinear activation function followed by
down sampling.

Occurs when the partial derivatives of the loss func-
tion (error between the ground truth and predic-
tion) in a learning algorithm (e.g. gradient descent)
become very close to zero. The learning algorithm
is unable to back-propagate useful gradient infor-
mation.

Boosting aggregates outputs of diﬀerent weak learners in an itera-
tive manner. Bagging trains weak learners concurrently on subsets
of independent and identically distributed data (IID) sampled with re-
placement. Random forest; a group of decision trees that learn with
bagging and random feature sampling (Breiman, 2001).

Popular deep neural networks are Convolutional neural networks
(CNNs), Recurrent neural networks (RNNs), Long-short Term mem-
ory (LSTMs), and Autoencoders (Hochreiter and Schmidhuber, 1997;
Masci et al., 2011).

Examples include AlexNet (Krizhevsky et al., 2012) an early deep
CNN designed to classify a thousand categories of images (ImageNet).
VGG (Simonyan and Zisserman, 2014) reduced the number of param-
eters by using smaller ﬁlters multiple times between layers. Residual
Neural network (ResNet) (He et al., 2016) trains deeper networks ef-
ﬁciently using residual blocks that connect outputs of stacked layers
to the block’s input layer with identity skip connections.

Caused by activation functions (e.g. sigmoid) that map a large input
space to the limited range [0,1], leading to very small derivatives even
when the input change is large. Using activation functions (e.g. ReLU)
that avoid small derivatives is an eﬀective way to address the vanishing
gradient problem (Krizhevsky et al., 2012).

Deeper additional layers that provide the network
power to calculate a more complex function with-
out increasing the training set may lead to a perfor-
mance saturation and eventually degradation.

ResNets allow for uninterrupted ﬂow of information from previous lay-
ers to subsequent ones, realising a complex function with less param-
eters and hence less risk of both degradation and vanishing gradients
(He et al., 2016)

5

gradient

s Vanishing
N
N
D
h
t
i
w
s
e
g
n
e
l
l
a
h
C

Degradation
in DNNs

Progress in the development of CNNs has led to a plethora of applications
including the automatic analysis of medical images. The CNN designed by
Ciresan et al. (2012), for instance, was used for the segmentation of neuronal
membranes in stacks of EM images. The images were segmented by predicting
the label of each local region or patch covered by a convolutional ﬁlter in a slid-
ing window approach. Despite the method’s success - winning the 2012 ISBI3
EM segmentation challenge - the method has two major limitations. First, the
sliding window approach is slow as it suﬀers from redundancy due to the pro-
cessing of large overlaps between adjacent patches. Second, there is always a
trade-oﬀ between the size of the patches (context) and full-resolution prediction.
It turns out that localisation ability decreases with an increasing context due to
downsampling by the many max pooling layers. Improvements in the semantic
segmentation of EM images continued with the development of the Fully Con-
volutional Network (FCN) (Long et al., 2015). FCN allows for variable sized
images as input by replacing the fully connected layers of a standard CNN with
fully convolutional maps (Fig. 2). Now, the spatial maps in the last layer of an
FCN correspond to certain local patches or pixels of an input image depending
on the network depth.

Encoder-decoder architectures of the FCN type have been catalysts in pro-
viding better localization and use of larger context. The decoder also captures
multi-scale features using skip connections to fuse feature maps from shallow
layers, providing larger context. Skip connections bypass some of the neural
network layers and take the output of one layer as the input to the subsequent
ones. As a result, an alternative and shorter path is provided for backprop-
agating the error of the loss function, which also contributes in avoiding the
vanishing gradient and the degradation problems in deep networks (Table 2).
In principle, skip connections allow for a better upsampling in higher layers. For
instance, the symmetric U-Net architecture transfers full feature maps from the
encoder to the decoder paths, achieving the best segmentation results of neu-
ronal membranes in EM images in the ISBI 2015 challenge (Ronneberger et al.,
2015). SegNet was proposed to transfer only the pooling indices between the en-
coder and decoder paths to reduce the load on memory (Badrinarayanan et al.,
2017). Nevertheless, U-Net captures more complex information from the stored
encoder layer outputs concatenated using skip connections for upsampling and
thus outperforms SegNet in terms of accuracy.

The concept of convolutional output layers in FCNs enables the conversion of
popular DNNs, such as AlexNet, VGGNet, ResNet and GoogleNet (Inception-
v1) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; He et al., 2016;
Szegedy et al., 2015), into encoder-decoder architectures for semantic segmen-
tation. DNNs are universal approximators that can realise a complex function
with even two layers of the network. Shallow neural networks with a few layers
are not adequate to learn robust models due to overﬁtting. Deeper networks
with small receptive ﬁelds, such as VGG-16 and AlexNet, became popular for

3IEEE International Symposium on Biomedical Imaging.

6

Figure 2: Encoder-decoder networks for FCN and U-Net. Each of the ﬁrst four convolutional
layers in the encoder are followed by the nonlinear activation function ReLU and max pooling.
The last layer uses a softmax function to assign a probability class score to each pixel. The
FCN decoder includes an upsampling component that is linearly combined with the low-level
feature maps in the third convolutional layer of the encoder. The sizes of these feature maps
are 4 times less than the size of the input image I (denoted by I/4). Finally, there is a
direct upsampling from I/4 to the original size of I followed by softmax for classiﬁcation.
The symmetrical U-Net architecture shares the features maps in the encoder with the decoder
path together with skip connections.

image classiﬁcation due to their generalization ability. DNNs have a large num-
ber of parameters to learn using the loss function (or error) that quantiﬁes the
penalty of the predicted values with respect to the desired ones. The addition
of layers to make an architecture deeper brings other scientiﬁc challenges, such
as the vanishing gradient and network degradation (Table. 2). Methods to ad-
dress these challenges with training deeper networks include diﬀerent strategies
in initializing network parameters (Glorot and Bengio, 2010), training networks
in multiple stages (Simonyan and Zisserman, 2014), and using companion loss
functions or auxiliary supervision in the middle layers (Szegedy et al., 2015).
The most recent and important networks in computer vision are called the
Residual Neural Networks or ResNet that overcame the vanishing gradient and
the degradation issues simultaneously (Table. 2).

Spatial pyramid pooling and dilated (or atrous) convolutions were intro-
duced in the DeepLab family of segmentation architectures for the purpose of
larger context capture. DeepLab models addressed the challenges of achieving
robustness for diﬀerent scales of classes and considering larger context without
increasing computational complexity (Chen et al., 2014, 2017a,b, 2018). More-
over, multi-scale context aggregation using the Pyramid Scene Parsing Network

7

FCN64Iconv1128I/2conv2256I/4conv3512I/8fctoconvKI/4KI/4+KI/4KISoftmaxU-Net64Iconv1128I/2conv2256I/4conv3512I/8conv4256256256I/4128128128I/2646464IISoftmaxConvolutionallayerReLUactivationPoolinglayerUpsamplinglayerSkipconnectionLayerSoftmax(PSP-Net) or spatial attention using the attention U-Net have also gained pop-
ularity for their ability to capture larger context (Zhao et al., 2017; Oktay et al.,
2018). The ability to deal with varying scales while capturing more context is
particularly important in EM analysis where the neighbourhood of a structure
may have an impact in determining a precise delineation.

3. Literature search

The following search query was used in both Pubmed and Web of Science on
words in titles (TI) only, restricted to 2017-2021: TI=((electron microscopy OR
EM) AND (segmentation OR supervised OR unsupervised OR self-supervised))
NOT cryo. Cryo-EM (Kucukelbir et al., 2014) was excluded because it involves
molecule datasets as opposed to cellular EM. Results from the query that are
beyond our scope were excluded. A detailed review of the resulting 28 papers
(Table 3) is given in terms of the data annotation and studied structures, the
segmentation approaches, and computational scalability.

4. Data, anatomical structures and annotation

Early examples of automated segmentation include the reconstruction of
brain tissues for connectomics, which is the map of neuronal cell bodies and their
connections, the synapses. The pioneer work on automated neuronal membrane
segmentation by Ciresan et al. (2012) showed the success of neural networks
on EM images in the ISBI 2012 challenge. This has motivated more research
activity in this direction, resulting in key segmentation architectures such as U-
Net (Ronneberger et al., 2015). Properties of the most commonly used datasets
are shown in Table 4.

Large-scale connectome datasets mostly focused on sub-cellular components
related to brain cells such as synapses, pre- and post-synaptic sites, axons and
mitochondria (Takemura et al., 2015; Kasthuri et al., 2015). The open organelle
project provides free access to high resolution 3D EM datasets and their seg-
mentations for analysing intracellular sub-structures and organelles 4. With the
challenge to automatically reconstruct tissues at cellular level, several organelles
besides mitochondria, namely nucleus, plasma membrane, endoplasmic rectic-
ulum, nuclear envelope, vesicles, lysosomes, endosomes and microtubules have
now become the focus in open-source datasets (Heinrich et al., 2021).

Detailed annotation of large-scale EM images is required for automatic im-
age segmentation and analysis of structures. Three approaches of annotations
are considered: 1) human annotators; 2) software tools for biologists; and 3)
specialized microscopy imaging modalities.

4https://www.openorganelle.org/

8

Table 3: Survey papers. Abbreviations used - S (Supervised), UN (Unsupervised), SS (Semi-
supervised), F (FIB-SEM), SS (Serial section), SB (SB-SEM).

Method

Type Dataset

EM type

Structures

Study

2D 3D F SS SB TEM

Residual Deconvolutional Network
(RDN)

DeepEM3D

Feature selection and boosting

Pre-trained networks

2D Convolutional Neural Network
(CNN)

3D Residual FCN

Two-stream U-Net

Random forest

Fully Convolutional Network (FCN)

Fully Residual U-Net (FRU-Net)

3D Convolutional Neural Network
(CNN)

Residual Neural Network (ResNet)

S

S

S

S

S

S

UN

S

S

S

S

S

Morphological operators/superpixels

UN

Random forest

U-Net, ResNet, HighwayNet, DenseNet

DenseUNet

Fully residual CNN (FR-CNN)

HighRes3DZMNet

U-Net, autoencoder

DeepACSON

2D-3D hybrid network

3D U-Net

CDeep3EM, EM-Net, PReLU-Net,
ResNet

Hierarchial encoder-decoder (HED-Net)

S

S

S

S

S

UN

S

S

S

S

S

Generative adversarial network (GAN)

UN

Annotation-crowd-sourcing, ‘Etch a Cell’

U-Net

Hierarchical view ensemble (HIVE) Net

S

SS

S

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

9

Membranes

Membranes

Fakhry et al. (2016)

Zeng et al. (2017)

Mitochondria, synapses, membranes Cetina et al. (2018)

Neurophil, axons

Mitochondria

Drawitsch et al. (2018)

Oztel et al. (2018)

Mitochondria

Xiao et al. (2018)

Mitochondria, synapses

Berm´udez-Chac´on et al. (2019)

Glomular basement membrane

Cao et al. (2019)

Mitochondria

Membranes

Dietlmeier et al. (2019)

G´omez-de Mariscal et al. (2019)

Cells, mitochondria, membranes

Guay et al. (2019)

Neural cell body, cell nucleus

Jiang et al. (2019)

Mitochondria, membranes

Karaba˘g et al. (2019)

Mitochondria

Membranes

Mitochondria

Membranes

Peng and Yuan (2019)

Urakubo et al. (2019)

Cao et al. (2020)

He et al. (2020)

Mitochondria, endolysosomes

Mekuˇc et al. (2020)

Mitochondria

Axons

Peng et al. (2020)

Abdollahzadeh et al. (2021)

Cells, granules, mitochondria

Guay et al. (2021a)

Up to 35 sub-cellular structures

Heinrich et al. (2021)

Mitochondria

Khadangi et al. (2021b)

Mitochondria

Luo et al. (2021)

Cytoplasmic caspids

Shaga Devan et al. (2021)

ER, mitochondria, nucleus

Spiers et al. (2021)

Membranes

Takaya et al. (2021)

Axons, nuclei, mitochondria

Yuan et al. (2021)

Acquisition Dataset

Region

Imaged tissue parameters (x,y,z)

Reference

Table 4: Commonly re-used 3D EM datasets for image analysis improvement.

ssTEM

ssSEM

ISBI 2012 /
Drosophila I VNC

Nervous cord
(Drosophila)

ISBI 2013 /
SNEMI3D

Cerebral
cortex (Mouse)

ssSEM

Kasthuri dataset

SB SEM

HeLa cells

FIB-SEM

EPFL Mouse
Hippocampus

FIB-SEM

FIB-25

Neocortex
(Mouse)

Cultured cells
(Human)

CA1
Hippocampus
(Mouse)

Optic lobe
(Drosophila)

Volume (µm)

Pixels

Pixel size (nm)

2 × 2 × 1.5

512 × 512 × 30

4 × 4 × 50

6 × 6 × 6

1024 × 1024 × 100

6 × 6 × 30

103 × 103 × 130

1024 × 1024 × 2000

3 × 3 × 30

1 × 1 × 0.2

8192 × 8192 × 518

10 × 10 × 50

5 × 5 × 5

2048 × 1536 × 1065

5 × 5 × 5

64 × 66 × 81

6426 × 6623 × 8090

10 × 10 × 10

Ciresan et al.
(2012)

Arganda-
Carreras et al.
(2013)

Kasthuri et al.
(2015)

Iudin et al.
(2016)

Lucchi et al.
(2013)

Takemura
et al. (2015)

4.1. Human annotators

Human annotation is split into two categories, annotations by one or a
few experts, and collaborative annotations by large groups of experts and non-
experts, so-called crowd-sourced annotation. Projects such as Etch-a-Cell en-
able annotation through crowd-sourcing (Spiers et al., 2021) that allows vol-
unteers to participate in large-scale annotation tasks digitally, with the aid of
tutorials and other guided workﬂows. For example, volunteers are invited to
annotate certain structures, such as endoplasmic reticulum (ER), mitochondria
or nuclei, in order to collect ground truth segmentation labels for supervised
machine learning algorithms. As human segmentations can be erroneous due to
imprecise delineation of structures or bias, consensus between various volunteer
segmentations is often used as the ground truth. Expert annotations are more
accurate but require more time and resources. For example, labeling structures
in 30 platelets from a small fraction of the imaged samples required 9 months
from two human annotators (Guay et al., 2021a). Also, the large scale con-
nectomics project required extensive labelling of ground truth data (Plaza and
Funke, 2018), and the proofreading5 of the ground truth dataset took around 5
years in the work of Takemura et al. (2015).

4.2. Biomedical image analysis software

Biomedical image analysis software applications are black box tools for many
users to annotate or proofread EM datasets without having to know the under-

5Proofreading refers to the manual correction of segmented (manual or automatic) image

data.

10

lying mechanisms6. Large-scale connectome reconstruction prompted the devel-
opment of various tools for proofreading and analysis of the reconstructed data.
All the tools are mostly developed for connectome proofreading and analysis
for serial sectioned image stacks and block-face images. Such tools mostly come
with 2D viewers and 2D annotations using brush/ﬂood-ﬁll functions with 3D
viewers for visualization (Table 5). Generally, the reconstructed or annotated
maps are corrected by reading data from external servers. The data storage for-
mat in the server and the mechanism of data distribution to the client determine
how well the tool can scale for annotation or proofreading of larger volumes.

No single software tool is typically enough for the entire automated im-
age analysis pipeline. UNI-EM bundles several oﬀ-the-shelf tools for annota-
tion, segmentation, proofreading or multi-view rendering into a single package
(Urakubo et al., 2019). Entire segmentation workﬂow is included using DOJO,
CNN models such as U-Net, ResNet, Highway Net, DenseNet and ﬂood ﬁlling
networks (Srivastava et al., 2015; Huang et al., 2017; Januszewski et al., 2018)
and an additional 3D surface-mesh based renderer. DOJO is a web-based tool
for large-scale annotation and proofreading (Haehn et al., 2014). The package
also includes monitoring tools such as TensorBoard for performance monitoring
(Man´e et al., 2015). The user does not need programming skills to perform
segmentation.

TrackEM2 is a software tool for optimized manual and semi-automatic re-
construction of neuronal circuits (Xiao et al., 2018; Jiang et al., 2019; Ciresan
et al., 2012; Li et al., 2018). A companion tool for TrackEM2 is CATMAID,
which is for data storage, management and collaborative annotation of large-
scale 3D ssTEM datasets using a client-server model (Saalfeld et al., 2009).
However, 3D neurite tracing to the next lateral 2D image plane obtained from
a server can be less optimal due to low bandwidth and network latency issues.
To overcome this challenge, webKnossos uses a 3D SB SEM dataset storage and
transmission in the form of small 3D voxels, as used in Knossos (a standalone
data annotation application for connectomics) (Helmstaedter et al., 2011). Web-
Knossos is a cloud/browser-based 3D annotation tool for large-scale distributed
data analysis (Boergens et al., 2017). An example of open-source demo for dense
connectome reconstruction using webKnossos is given by Motta et al. (2019). 3D
voxel based storage and access is also provided by VAST (Volume Annotation
and Segmentaion Tool) to handle petabyte range image data. Rich metadata
based information can be generated for segmented objects and be grouped or
organised in a tree structure for importing. Similar to CATMAID, VAST has a
data management framework with speciﬁc ﬁle format conversion before analysis
(Berger et al., 2018). A VAST format compatible dataset is publicly available
to annotate the mitochondria dataset by Kasthuri et al. (2015)7.

The FlyEM project provides open-source tools which aim to fully reconstruct
the neural connectivity of the Drosophilia ﬂy brain using EM imaging (Scheﬀer

6http://www.biii.eu/
7lichtman.rc.fas.harvard.edu/vast/

11

Table 5: Reviewed biomedical image analysis software for manual or semi-automatic image
segmentation, annotation and proofreading.

Feature/
Software

Visualization
client

Annotation
client

Collaborative
annotation

Scalability Train/Test

Data
con-
version
format

UNI-EM 2D, 3D/DOJO

2D DOJO,
Mesh

3D

Web-based

PNG/TIFF High

DNN

webKnossos 2D,

2D/webKnossos

Web-based

Knossos

High

3D/webKnossos

FlyEM

2D, 3D/NeuTu

2D/NeuTu

Web-based

DVID

High

TrackEM2

2D, 3D/TrackEM2

2D/TrackEM2

Web-based

CATMAID Moderate

VAST

Ilastik

2D, 3D/VAST

2D/VAST

Central server

2D, 3D/ Ilastik

2D/ Ilastik

VAST

HDF5

High

Low

PNG/TIFF Low

Weka

2D/ ImageJ

2D/ ImageJ

IMOD

2D/ IMOD

-

-

-

-

-

-

-

-

Random
forest

Random
forest

Inclusion
of custom
algorithm

Python
script

Python
script

-

-

-

-

Java script

-

Camera
speciﬁc

Undeﬁned

-

et al., 2020). One of the tools, NeuroProof (Plaza, 2014), introduces focused
and faster proofreading using automated segmentation and prior information
(synapse connectivity). Other tools, like Raveler and NeuTu, allow interactive
proofreading in a distributed and scalable manner (Farm, 2014; Zhao et al.,
2018). Distributed, Versioned, Image-oriented Dataservice (DVID) provides a
web-based API for key-value based image labels for eﬃcient storage and faster
access. DVID facilitates the distribution and access of data through a common
format to access all datasets using a higher-level API (Katz and Plaza, 2019).
Ilastik and Trainable Weka Segmentation (TWS) are plugins in Fiji to
segment synaptic junctions and mitochondria (Sommer et al., 2011; Arganda-
Carreras et al., 2017).
Ilastik originally focused on the work of segmenting
synaptic junctions and was further developed into a platform that includes fast
interactive training through shallow classiﬁers (random forests) (Berg et al.,
2019). Trainable Weka Segmentation (TWS) allows training of a random forest
classiﬁer for binary segmentation and provides options to select the features,
load a stored classiﬁer to visualize results on test images along with its per-
formance metrics. Both are extensively used for labeling neuronal 3D images.
IMOD can semi-automatically register 3D EM serial sections and was used
to segment structures such as small extra-cellular vesicles (G´omez-de Mariscal
et al., 2019).

The annotation tools show a trend towards semi-automated segmentation
on web frameworks for scalable data access along with providing collaborative
proofreading. For further information on accelerating computing and scalable
data acesss for data analysis refer Section 6.

12

4.3. Correlative Light and Electron microscopy (CLEM)

Fluorescence microscopy can help to provide identifying information to EM
images and can be used to improve automated analysis. CLEM is used to
identify structures targeted with ﬂuorescent probes in µm resolution images at
(sub)cellular context from EM (de Boer et al., 2015). Drawitsch et al. (2018)
performed CLEM for 40-50µm EM sections for a 3D connectome dataset. The
workﬂow uses the full multi-color space of LM to identify the most-likely axon
out of all reconstructed axons in large scale EM to best match with the LM-
imaged axonal ﬂuorescent signal. Besides webKnossos, which they used for
manual reconstruction of axons that took 5300 work hours for a 1 × 1 × 0.1 mm3
layer of the mouse neocortex, further speedup using partially automatic methods
proposed in (Berning et al., 2015; Dorkenwald et al., 2017; Staﬄer et al., 2017;
Januszewski et al., 2018) is conceivable. Other attempts for 2D CLEM are based
on EM workﬂows or markers to speed up image registration or alignment.

5. Segmentation approaches

The taxonomy (Fig. 3 illustrates the main segmentation methodologies re-

viewed in this section.

5.1. Supervised learning

Supervised learning in segmentation refers to the family of machine learn-
ing algorithms that use a set of annotated images (training data) to create a
computational model that can segment structures in unseen images (test data).
The training set is used by the algorithm to determine the model’s parameters
in such a way as to to maximize the model’s generalization ability.

5.1.1. End-to-end learning

End-to-end learning refers to the use of gradient-based methods to adjust the
parameters of a complex deep neural network based on a loss function applied on
the network’s prediction (Glasmachers, 2017). In principle, increasing network
complexity - e.g. by increasing the number of layers - allows for more complex
functions to be learned. In practice, however, there are technical challenges that
limit a network’s learning capacity, like vanishing gradients and network degra-
dation (Table 2). The ResNet architecture attempts to counter such limitations
with residual blocks (He et al., 2016). Skip connections enable the design of
very deep networks while propagating the gradient of the loss function through
a lower number of layers. Encoder-decoder architectures, especially U-Nets,
have enabled end-to-end learning for segmentation. ResNet blocks have become
a standard in encoder and decoder networks for biomedical image segmentation.

13

Figure 3: Categorization of deep learning methods used for EM segmentation. Gray shad-
ing indicates which components belong to which category (supervised, semi-supervised or
unsupervised).

Neuronal membranes and ResNets. Various variants of the U-Net architecture
using residual blocks have been proposed for segmenting neuronal structures
such as membranes, neural cell bodies and cell nucleus. FusionNet and Fully
residual CNN (FR-CNN) use residual blocks at each level outperforming the
standard U-Net architecture and FCN for membrane segmentation (Quan et al.,
2016; He et al., 2020). Dense skip connections that connect each layer to ev-
ery other layer in a feed-forward manner used in DenseUNets, a combination
of U-Net and DenseNet (Cao et al., 2020), achieved competitive segmentation
performance on the ISBI 2012 EM dataset. The deconvolutional network of
Noh et al. (2015) introduced learnable unpooling layers, was used in combina-
tion with residual blocks in the in the Residual deconvolutional network (RDN)
(Fakhry et al., 2016). RDN shows minimal inconsistencies in continuity of mem-
brane detection across the 3D slices on the ISBI 2013 dataset. ResNet with
atrous convolutions was used for the segmentation of neural cell bodies and
nuclei. Additionally, the multi-scale contextual feature integration outperforms
U-Net and Deeplab v3+ (Jiang et al., 2019).

Mitochondria and eﬃcient 3D networks. Mitochondrial distribution inside a cell
and alterations in its shape are related to degeneration or cellular death. High-
resolution automatic analysis is required to study the physiological changes in
mitochondria. Both 2D and direct 3D networks for reconstruction work well
on isotropic FIB SEM images. A 2D image analysis pipeline for mitochondria
segmentation from FIB-SEM dataset was performed by direct upsampling of the
encoder features (Oztel et al., 2018). A hybrid 2D-3D network by Xiao et al.
(2018) refers to the use of 3D convolutions at the start and end of an encoder-
decoder network. Using 2D max-pooling instead of 3D in the same network

14

helped in capturing anisotropy for SB SEM datasets. Auxiliary supervision in
the mid-level features was key for better deep supervision to avoid the vanishing
gradient problem. As compared to architectures like U-Net and 3D U-Net, lesser
proofreading was needed due to better accuracy.

The 3D networks have millions of parameters to train and are computa-
tionally intensive. Multiple views of a 3D stack, three orthogonal views and a
ﬁnal branch to calculate context information from one of the views was used in
HIVE-Net, a multi-task pseudo 3D residual network (Yuan et al., 2021). Ex-
periments show that the HIVE-Net with lesser number of parameters achieves
state-of-the art performance compared to deep learning models such as U-Net,
3D U-Net or the hybrid 2D-3D network (Xiao et al., 2018).

Shape-based prior for segmentation. Shape information is used for faster and
regularized segmentation of axons, mitochondria and nuclei to take into account
the heterogeneity in large volumes of 3D EM datasets. DeepACSON, a Deep
learning-based AutomatiC Segmentation of axONs, achieves segmentation of
lower resolution larger ﬁeld of view images that miss distinctive image features
using shape information (Abdollahzadeh et al., 2021). Faster analysis of low
resolution images on par with the high-resolution ones was made possible using
shape information, such as ovality of mitochondria, tubularity of axons and
circularity of nuclei.

Two-stage networks for shape-based discriminative feature training is per-
formed by using the Hierarchical Encoder Decoder (HED) network (Luo et al.,
2021). Based on the eccentricity (elliptic or circular shape) of mitochondria,
ground-truth labels are sub-categorized into two sets for training; a ﬁrst stage
multi-task network and a second network for the full labels. Shape informa-
tion priors used in such networks show less false positives and fewer missed
detections in the segmentation of mitochondria when compared with U-Net, 3D
U-Net, and HIVE-Net (Yuan et al., 2021).

Whole cell 3D reconstruction. Robust and scalable automatic methods for
whole-cell 3D reconstruction are required to study intricate organisation of
thousands of structures inside a cell. FIB-SEM blocks from 5 diﬀerent cell
types and EM preparation methods were trained for around 35 diﬀerent cellular
organelles. The results show that a diverse set of all samples used for training
improves generalizability more than training on only one speciﬁc target sample.
Such comprehensive datasets and the trained models for 3D reconstruction of
cells are made open-source (Heinrich et al., 2021) to allow the exploration of
local cellular interactions and their intricate arrangements.

5.1.2. Ensemble learning

Ensemble learning methods combine outputs of multiple predictions for bet-
ter robustness. Pixel- or voxel-wise averaging and majority or median voting
are amongst the main aggregation methods. DeepEM3D uses deep inception-
residual modules in the encoding path and multi-scale contextual feature ag-
gregation in the decoding path (Zeng et al., 2017). Variants of the DeepEM3D

15

architecture use ensemble learning to combine several models that are trained
on neuronal boundaries with diﬀerent thicknesses. Voxel-wise averaging of the
predictions account for misalignment and anisotropy in the ssSEM image stacks.
Random forests (Table 2) use ensemble learning for improved generalization.
A stack of Random forests was investigated for the segmentation of glomerular
basement membrane from TEM images (Cao et al., 2019). Zoom-view random
forests based on N groups of membrane intensity images and one full-view ran-
dom forest taking M pixels sampled from all N groups were trained to capture
diﬀerent imaging conditions. The method of using two-level integrated Random
forests enhances generalization on diﬀerent gray-scale intra-image variations and
diﬀerent morphologies of the membrane. Cetina et al. (2018) used the PIBoost
algorithm (Fern´andez-Baldera and Baumela, 2014), a multi-class generaliza-
tion of AdaBoost with weak binary learners, for simultaneous segmentation of
synapse with mitochondria and mitochondria with membranes. Better accuracy
was obtained for both isotropic and anisotropic stacks (SB SEM) due to better
representation ability and robustness to class imbalance.

Structured prediction are modelling techniques that forecast a set of values
rather than a scalar value. In the segmentation context, structured prediction
methods seek the joint prediction of the label of the pixel under consideration
as well as the labels of the extended neighbourhood. The hierarchical approach
by Peng and Yuan (2019) uses an iterative procedure to ﬁne-tune the segmenta-
tion based on handcrafted features preserving neighborhood structure and class
labels determined in previous iterations. Structured and cascaded approaches
tend to improve the segmentation results by reducing the number of false posi-
tives and false negatives.

An ensemble of randomly initialized instances of the same network with each
trained on less than 1% of the SB-SEM volume for seven classes in platelet cells
yielded the best results to structural variations in large datasets, and outper-
formed individual 2D and 3D U-Net approaches (Guay et al., 2021a). Multiple
network outputs were also combined using a workﬂow for binary EM segmenta-
tion provided by the EM-stellar platform (Khadangi et al., 2021b). The network
architectures chosen for experimentation were CDeep3EM, EM-Net, PReLU-
Net, ResNet, SegNet, U-Net and VGG-16 (Haberl et al., 2018; Khadangi et al.,
2021a; He et al., 2015). A cross-evaluation using a heatmap of diﬀerent eval-
uation metrics and networks shows that conﬁguring an ensemble of various
architectures is required to obtain the best results. Khadangi et al. (2021b) also
demonstrated that no single deep architecture performs consistently well, and
that is why ensemble approaches may have an edge over individual methods.

5.1.3. Transfer learning

Transfer learning adapts the knowledge acquired from one dataset to an-
other, and is used when an application has insuﬃcient amount of training sam-
ples. The pre-trained model is ﬁne-tuned, usually in the ﬁnal layers, with the
training samples of the new dataset. Through transfer learning, the same neural
network segmentation pathways have been used across biological systems in 3D
(Guay et al., 2019). A three-class segmentation was proposed by Mekuˇc et al.

16

(2020) for the delineation of mitochondria, endolysosomes, and background.
The domain information of the larger number of mitochondrial structures with
texture similar to endolysosomes were used to learn a binary classiﬁer against
background. The weights were initialized for training a three-class model where
only the last layer was ﬁne-tuned to distinguish between the classes. Transfer
learning thus enables the modeling of a dataset even when they are limited.

Fine-tuning a pre-trained network comes with the risk of over-ﬁtting to the
few labeled training examples of the new application. This challenge has opened
up new research avenues, namely, few-shot learning and domain adaptation.
Few-shot is a meta-learning approach that “learns to learn” from a given pre-
trained model (Shaban et al., 2017). A pre-trained model is made to learn the
similarity or diﬀerence between classes on the available few labeled samples,
referred to as the support set (Dong and Xing, 2018). Complex non-linear mi-
tochondrial morphology was captured using active feature selection and boost-
ing (Dietlmeier et al., 2019). The VGG-16 model pretrained on the ImageNet
dataset was used as a feature extractor for extracting hypercolumns that con-
tain the activations of all CNN layers for each pixel. Hypercolumns were passed
through a linear regressor for actively selecting features. Only 20 patches or
blocks were used from a FIB-SEM stack for training a gradient based boosting
classiﬁer (XGBoost). By actively selecting features and learning using far less
training data or even using a single training sample (single-shot) one can obtain
competitive segmentation accuracy.

Domain adaptation is another form of transfer learning, where the source to
target datasets share the same labels (classes) but have a diﬀerent data distribu-
tion. Changes in data distribution can be due to slightly diﬀerent experimental
parameters during EM imaging or due to the imaging of diﬀerent tissue types or
body locations. Roels et al. (2019) aimed to learn a latent space with a shared
encoder in such a way that the source (annotated samples) and target (few an-
notated samples) representations are aligned in that feature space. Transferring
knowledge from isotropic to anisotropic SEM images is possible using learning
in latent space for domain adaptation.
In contrast, Berm´udez-Chac´on et al.
(2018) proposed a two networks, jointly trained using a diﬀerential loss function
to regularize two U-Nets (one for each domain) to avoid domain shift. Only
10% of labeled target data was required for domain adaptation to achieve state-
of-the-art performance when compared to a U-Net trained on fully annotated
data.

5.2. Semi-supervised learning

Semi-supervised approaches use unlabeled data for training along with a
small set of labeled data (Zhu and Goldberg, 2009). The distribution patterns
from unlabeled data used in training models help to generalize more than su-
pervised learning from the few labeled samples. Semi-supervised methods learn
to improve the model performance in subsequent iterations using pseudo labels
generated from the output of the pre-trained model. Incremental learning se-
lects best features by iteratively adding decision trees to the classiﬁer. (Utgoﬀ,

17

1989) demonstrated that adding only relevant decision trees can incrementally
improve the prediction without needing to go back and retrain the model.

Label propagation in images of a 3D stack using pseudolabels from predic-
tions of a trained network was performed in an incremental setting (Takaya
et al., 2021). The experimental results conclude that the generalization perfor-
mance using supervised learning on 3D EM does not perform well when com-
pared with a sequential semi-supervised segmentation (4S) approach. The 4S
approach improved the performance of the network by reducing false positives
along with improving segmentation accuracy when compared with U-Net. Such
a semi-supervised approach applied on sequential EM images (having strong
correlation between images in stacks) reduced the annotation eﬀort by experts.

5.3. Unsupervised approaches

Unsupervised approaches are categorized into two groups: the more tradi-
tional relies on image processing and thresholding without involving learning
algorithms, i.e., by Karaba˘g et al. (2019) who used traditional image process-
ing algorithms to detect nuclear envelopes. Low-pass ﬁltering, edge detection,
dilation to connect disjoint edges, super-pixel analysis followed by smoothing
and ﬁlling of holes form an unsupervised pipeline for nuclear envelope detec-
tion. The pipeline performed better than the four deep learning models VGG16,
ResNet18, U-Net and Inception-ResNetv2 investigated by Szegedy et al. (2017).
The cell nucleus that becomes smaller at the edges than the middle 2D slices
lead to a highly imbalanced images for training the deep architectures. How-
ever, the learning-free unsupervised approach assumes that the cell nucleus is
always located at the center of the three dimensional stack and that nuclear en-
velope is darker than nucleus and surrounding background, and therefore such
learning-free approaches may not be suﬃciently robust in generalization.

The more advanced group of unsupervised approaches conﬁgure models from
unlabeled data. Unsupervised domain adaptation with self-supervision (self-
generated labels) was used to determine pivot locations in the target dataset
with no labels that characterise regions of mitochondria or synapse (Berm´udez-
Chac´on et al., 2019). The target domain locations from the correspondences of
similar structures were converted to heatmaps, for adapting the model based
on a two-stream U-Net (Berm´udez-Chac´on et al., 2018). The results were con-
sistent with those obtained under fully annotated samples trained on U-Net or
semi-supervision (use of transfer learning). No new annotation eﬀort in case
of domain shifts was required for volumes of FIB-SEM from diﬀerent mouse
specimens.

Adversarial learning trains networks to predict the same output for two
datasets, one source and the other being the adversarial perturbed data, for
which the latter gives a diﬀerent output in spite of belonging to the same class.
Each algorithm can use a diﬀerent approach, such as sharing weights across
domains (Peng et al., 2020) or use of generative samples from a generator to
confuse the discriminator as is done by Generative Adversarial Networks, or
GANs for short (Goodfellow et al., 2014). Adversarial learning is also used for
domain adaptation to learn non-discriminating features for robustness to shift in

18

data distributions (Peng et al., 2020). Domain-invariant features in the encoder
are learnt through a reconstruction auto-encoder in an unsupervised manner. As
the target has no labels, the shared decoder features are still not discriminative
to the target domain for which the proposed method uses adversarial learn-
ing in the decoder stage. GANs use adversarial learning to generate synthetic
training samples with the same statistics as the source training data. Image
synthesis using a GAN generates images with similar distribution but varied
object conﬁguration for a three-class detection from a TEM dataset (Shaga De-
van et al., 2021). Synthetic images speed up automatic image analysis even
when large training datasets are not available, thus improving the performance
signiﬁcantly.

5.4. Performance evaluation metrics

Segmentation is evaluated using pixel-based matching or segment-based
matching for binary segmentation. When the segmentation gives a unique in-
dex to each object it is called instance segmentation, evaluated by penalising
overlaps with other individual segments.

Common pixel-based matching measures are accuracy, true positive rates
and false positive rates (Jiang et al., 2019). Precision, recall (sensitivity), speci-
ﬁcity and F1-score (harmonic mean of precision and recall) are the most basic
performance measures used in various studies to quantify the eﬀectiveness of
2D and 3D segmentation methods (Xiao et al., 2018; Dietlmeier et al., 2019;
Khadangi et al., 2021b; Takaya et al., 2021). Most binary image segmentation
tasks suﬀer from class imbalance as the background class is much larger than
the objects of interest. To address class imbalance, methods such as Intersection
over Union (IoU) or Jaccard index, which determine the similarity between the
ground-truth and predicted sets, are more appropriate. The Dice similarity co-
eﬃcient (DSC) addresses class imbalance by only considering the segmentation
class for evaluation. The DSC and JAC are the most commonly used measures
for performance evaluation in EM (Mekuˇc et al., 2020; Xiao et al., 2018; Yuan
et al., 2021; Luo et al., 2021; Cao et al., 2019; Peng and Yuan, 2019; Berm´udez-
Chac´on et al., 2018; Peng et al., 2020). The conformity coeﬃcient used by Xiao
et al. (2018) is a global similarity score with more discrimination capabilities
than Jaccard or DSC (Chang et al., 2009). To calculate how far the segmented
structure is from the ground truth, besides the Jaccard index, the Hausdorﬀ
distance is another sensible measure (Karaba˘g et al., 2019). The latter is the
spatial distance between two sets, and apart from matching segments, it also
takes into account the pixel/voxel localisation. Common metrics used for in-
stance segmentation in (Yuan et al., 2020; Luo et al., 2021) are the Aggregated
Jaccard Index (AJI) and the Panoptic Quality (PQ) (Kumar et al., 2017; Kir-
illov et al., 2019), which account for under- and over-segmentation more accu-
rately than the Jaccard index and DSC. The Jaccard curve, which was inspired
by the precision/recall and receiver operating characteristic (ROC) curves, is a
measure that quantiﬁes the quality of a segmentation result without involving
any thresholds (Cetina et al., 2018). Fig. 4 illustrates how such measures are
computed.

19

Figure 4: Common performance metrics for binary segmentation. For semantic segmentation,
the overall overlap of the ground truth (GT) mask with prediction (PR) is compared without
diﬀerentiating between objects of the foreground class. For instance segmentation, each GT
component is matched with only one PR component, the one with which it has the largest
intersection. The PR component A overlaps with two GT components, a and b, but is matched
only with a due to a larger overlap. The Aggregated Jaccard Index (AJI) takes the sum of the
intersections of all matched GT and PR components divided by the sum of their unions plus
the unmatched PR components. The Panoptic Quality (PQ) is the sum of the IoU ratios of all
associated GT-PR pairs (i.e. TPs) divided by the sum of all TPs and half of the unmatched
GT and PR components. The symbol |.| indicates the area of the component concerned.

Boundary matching and information theoretic scores have emerged as two
important metrics to evaluate neuronal boundary maps (Unnikrishnan et al.,
2007; Arbelaez et al., 2010; Arganda-Carreras et al., 2015). The most popu-
lar ones are similarity-oriented measures between two clusters for paired labels
instead of pixel-wise errors. Boundary maps are transformed to segmentations
by ﬁnding connected components. The rand index quantiﬁes the similarity be-
tween the results of two clusters by taking the ratio of the sum of the total
number of pairs of points in agreement and pairs of points in disagreements
with respect to the total number of pairs between the two clusters. Another
measure is based on what are known as the split and merge errors. The split
error is computed by taking two randomly selected voxels belonging to the same
segment in the ground truth and assessing them based on the joint probability
of belonging to the same region in the segmentation result. The merge error is
based on whether two voxels predicted as belonging to the same segment do ac-
tually belong together. The Rand F-score is then the weighted harmonic mean
of the merge and split errors. Metrics like foreground-restricted rand scoring
and foreground-restricted information theoretic scoring after border thinning
are the state-of-the-art metrics for neuronal boundary segmentation (Cao et al.,
2020; Zeng et al., 2017; He et al., 2020; Cetina et al., 2018; Khadangi et al.,

20

2021b).

6. Scalability and performance matters

Bioimage analysis software tools8 use high-performance computing for scal-
able processing. Scalability refers to the data processing frameworks and compu-
tational resources that can handle big data. Scalable machine or deep learning is
not limited by the algorithms involved, but the supporting infrastructure which
is vast and complex (Sculley et al., 2015).

6.1. Distributed computing

Distributed computing divides a single problem into many parts, controlled
by a master node but processed in diﬀerent computing units (worker nodes).
More databases or processing nodes can be added to the system, rather than
using a single server with many nodes that is not used at all times. Fault toler-
ance nodes address hardware failures known as worker nodes or master replica
(Fig. 5). Users or clients access and process data remotely in diﬀerent systems of
the network. Biomedical image analysis tools for neuronal reconstruction such
as VAST, NeuTu, webKnossos, TrackEM2 (CATMAID) use distributed storage
and processing.

Detailed metrics to evaluate for synapse connectivity were introduced in
VAST for evaluating petabyte range datasets for 3D EM. A software ecosystem
in VAST is used to evaluate two large datasets (Takemura et al., 2015, 2017) de-
ployed in a scalable cluster-based solution using Apache-Spark (Zaharia et al.,
2010). The latter is an open source data processing framework to store and
process data in batch or real-time across clusters of computers. Distributed
Versioned Image-oriented data service (DVID) provides branched or distributed
versioning in connectomics reconstruction workﬂow for collaborating proofread-
ers from any part of the world (Katz and Plaza, 2019). NeuTu is a client program
that uses DVID as its scalable image database for large-scale, collaborative 3D
neuronal reconstruction proofreading. A backend Hadoop framework used by
Yuan et al. (2020) allows for distributed data storage of large amounts of data.
The scaling of clusters with more generated EM data and redundancy of data
in various clusters provides reliable data management and integrity.

Decentralized computing, modeled after Google Maps (Rasmussen, 2005),
was introduced in the Collaborative Annotation Toolkit for Massive Amounts of
Image Data (CATMAID) (Saalfeld et al., 2009). It uses in-browser decentralized
annotation of large biological stacks. Immediate or message passing from farther
nodes in the network make images accessible to the user (Fig. 5). Projects, image
stack information, and annotations (metadata) are stored in a centralized server
for cross-referencing and collaboration. WebKnossos uses decentralized systems
for image storage of 3D cubes and is implemented for an eﬃcient in-browser
access and reconstruction (Boergens et al., 2017).

8http://www.biii.eu/

21

Figure 5: Workﬂow of bioimage software tools. Data or compute access through web appli-
cation servers to cluster nodes or cloud entity. An overview of the design for centralised and
decentralised computing shows many users accessing concurrently data or compute nodes at
the same time in the network.

6.2. Cloud computing

Cloud computing is the on-demand provision of servers, applications, net-
working capabilities, and hardware resources on the internet (Kagadis et al.,
2013). There are three models of cloud service. The infrastructure as a service
(IaaS) model only includes computing resources, networking, and storage. The
platform as a service (PaaS) model includes the application design, testing and
development tools, middleware, operating systems, and databases. Finally, the
software as a service (SaaS) model facilitates the availability of all application
services to users from any device with an internet connection.

CDeep3EM (Haberl et al., 2018), a pre-conﬁgured cloud-based implementa-
tion of the DeepEM3D CNN for image segmentation, is publicly available on
Amazon Web Services (AWS). EM-stellar (Khadangi et al., 2021b) is a Jupyter
notebook platform hosted on Google Colab with ready access to cloud comput-
ing resources. Colab is a framework for developers to interface with the existing

22

notebooks and cloud infrastructure to evolve networks for speciﬁc microscopy
image processing. The aim of the ZeroCostDL4Mic project by von Chamier
et al. (2021) is to make deep learning for microscopy analysis accessible to users
with no coding experience by leveraging the open, cloud-based Google Colab
resources. State-of-the-art networks are provided as notebooks for segmenta-
tion, object detection, denoising, and super-resolution microscopy, along with
quantitative tools to analyse model performance and optimize it along with data
augmentation and transfer learning options.

7. Challenges and future trends

CNNs for segmentation are the most popular methods for automatic fea-
ture extraction and pixel-wise reconstruction in EM images. A notable example
is the U-Net architecture. Techniques for structural segmentation improve ro-
bustness through additional supervision or ensemble techniques for detecting
less false positives and false negatives. Whole cell 3D reconstruction for seg-
menting many organelles in volume EM datasets provide public datasets for
reuse (Heinrich et al., 2021; Guay et al., 2021a). Large public datasets are
not available for training and new datasets generated from EM techniques lack
labels for supervised learning. Few-shot learning for segmentation has shown
promising results in context of noisy labels and for incremental learning (Dong
and Xing, 2018; Liang et al., 2022; Tao et al., 2020). Newer methodologies
of semi-supervised and unsupervised learning techniques are becoming more
appealing. Semi-supervised and unsupervised methods that can segment new
datasets with minimal annotations allow these methods to scale to larger EM
datasets. Larger datasets scale well on deeper networks such as transformers
that use built-in self-attention between patches which can prove useful for large-
scale 2D EM segmentation (Dosovitskiy et al., 2020; Zheng et al., 2021).

Self-supervised methods with the contrastive learning framework have been
used to learn similar or dissimilar pairs from data, namely SimCLR (Simple
Framework for Contrastive Learning of Visual Representations) and MoCo (Mo-
mentum Contrastive Learning) (Chen et al., 2020; Ciga et al., 2020). Networks
that are initialized either randomly or by being pretrained on large datasets,
such as ImageNet, do not perform better as compared to pre-training using
MoCo (Guay et al., 2021b; Casser et al., 2018; Perez et al., 2014; Mekuˇc et al.,
2020). Self-supervised methods can also beneﬁt from multi-modal EM data,
such as CLEM. In fact, Seifert et al. (2020) give further insight on the potential
of deep learning for automatic image registration for CLEM. Other multi-modal
training data that can be used for EM segmentation in the future is ColorEM
or EDX information (Pirozzi et al., 2018).

The ability to segment new datasets with minimal annotations allows these
methods to scale to larger EM datasets generated from state-of-the-art technolo-
gies, such as multi-beam scanning EM. Other aspects of scalability are improv-
ing due to the widespread use of distributed or cloud computing. Accessibility
by users to such computing resources has also improved through the continued
development of bioimage analysis software. Pathologists, for instance, can now

23

use oﬀ-the shelf cloud applications for data access and analysis. Distributed
computing can also be performed on the cloud which makes it even more ap-
pealing as all required resources can be remotely scaled with ever increasing
EM data. Large-scale EM can also beneﬁt more from the Google Maps based
decentralized processing approach in a similar manner to improved data access
for vEM.

Indexing of segmented structures is important for fast retrieval purposes in
practical applications, but it is hardly addressed in this literature. The idea is to
enable a domain expert to query a database in three diﬀerent ways, namely label-
based, image-based, and proximity-based. A label-based query would require
the user to specify the label of a structure of interest, image-based would require
the submission of an image example of a structure of interest and, proximity-
based would require the speciﬁcation of the distance and direction between a
set of structures of interest. Such functionality would enable domain experts to
enhance their interaction with a database with large volumes of large-scale EM
images.

All research data should be Findable, Accessible, Interoperable and reusable
(FAIR) for both machine and people (Wilkinson et al., 2016). Findability re-
quires globally identiﬁable data resource associated with rich metadata, whereas
accessibility provides an open, free and universally implementable protocol along
with authorization mechanisms for accessing protected data. Interoperability is
the data format or knowledge representation language that helps machines un-
derstand or be compatible with a service operating on the digital resource. Re-
usability requires that an individual or even machine can decide if the resources
are useful for any task based on its license terms.

8. Conclusion

Automated image analysis techniques for EM are evolving in accordance
with the recent advancements in imaging technologies. For instance, automated
large-scale 2D EM makes greater demands on the capture of global context by
segmentation algorithms, without the aid of 3D information available in volume
EM. Given that the lack of fully annotated data in medical imaging will persist
and is likely to be compounded by the exponential generation of image data, we
suspect that semi-supervised and self-supervised approaches will play a bigger
role in the segmentation of EM data in the future.

9. Acknowledgement

This project has received funding from the Centre for Data Science and Sys-
tems Complexity at the University of Groningen9. Part of the work has been
sponsored by ZonMW grant 91111.006; the Netherlands Electron Microscopy

9www.rug.nl/research/fse/themes/dssc/

24

Infrastructure (NEMI), NWO National Roadmap for Large-Scale Research In-
frastructure of the Dutch Research Council (NWO 184.034.014); the Network
for Pancreatic Organ donors with Diabetes (nPOD; RRID:SCR014641), a col-
laborative T1D research project sponsored by JDRF (nPOD: 5 − SRA − 2018 −
557 − Q − R) and The Leona M. & Harry B. Helmsley Charitable Trust (Grant
2018P G − T 1D053). The content and views expressed are the responsibil-
ity of the authors and do not necessarily reﬂect the oﬃcial view of nPOD.
Organ Procurement Organizations (OPO) partnering with nPOD to provide
research resources are listed in http://www.jdrfnpod.org/for-partners/
npod-partners/.

References

Abdollahzadeh, A., Belevich, I., Jokitalo, E., Sierra, A., Tohka, J., 2021. Deep-
acson automated segmentation of white matter in 3d electron microscopy.
Communications biology 4, 1–14.

Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., 2010. Contour detection and
hierarchical image segmentation. IEEE transactions on pattern analysis and
machine intelligence 33, 898–916.

Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri, K.W., Schindelin, J.,
Cardona, A., Sebastian Seung, H., 2017. Trainable Weka Segmentation: a
machine learning tool for microscopy pixel classiﬁcation. Bioinformatics 33,
2424–2426. doi:10.1093/bioinformatics/btx180.

Arganda-Carreras, I., Seung, H., Vishwanathan, A., Berger, D., 2013. 3d seg-

mentation of neurites in em images challenge-isbi 2013.

Arganda-Carreras, I., Turaga, S.C., Berger, D.R., Cire¸san, D., Giusti, A., Gam-
bardella, L.M., Schmidhuber, J., Laptev, D., Dwivedi, S., Buhmann, J.M.,
et al., 2015. Crowdsourcing the creation of image segmentation algorithms
for connectomics. Frontiers in neuroanatomy , 142.

Azzopardi, G., Petkov, N., 2012. Trainable cosﬁre ﬁlters for keypoint detection
and pattern recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence 35, 490 – 503. doi:10.1109/TPAMI.2012.106.

Badrinarayanan, V., Kendall, A., Cipolla, R., 2017. Segnet: A deep convolu-
tional encoder-decoder architecture for image segmentation. IEEE transac-
tions on pattern analysis and machine intelligence 39, 2481–2495.

Berg, S., Kutra, D., Kroeger, T., Straehle, C.N., Kausler, B.X., Haubold, C.,
interactive

Schiegg, M., Ales, J., Beier, T., Rudy, M., et al., 2019. Ilastik:
machine learning for (bio) image analysis. Nature Methods 16, 1226–1232.

Berger, D.R., Seung, H.S., Lichtman, J.W., 2018. Vast (volume annotation and
segmentation tool): eﬃcient manual and semi-automatic labeling of large 3d
image stacks. Frontiers in neural circuits 12, 88.

25

Berm´udez-Chac´on, R., Alting¨ovde, O., Becker, C., Salzmann, M., Fua, P., 2019.
Visual correspondences for unsupervised domain adaptation on electron mi-
croscopy images. IEEE transactions on medical imaging 39, 1256–1267.

Berm´udez-Chac´on, R., M´arquez-Neila, P., Salzmann, M., Fua, P., 2018. A
domain-adaptive two-stream u-net for electron microscopy image segmenta-
tion, in: 2018 IEEE 15th International Symposium on Biomedical Imaging
(ISBI 2018), IEEE. pp. 400–404.

Berning, M., Boergens, K.M., Helmstaedter, M., 2015. Segem: eﬃcient image

analysis for high-resolution connectomics. Neuron 87, 1193–1206.

de Boer, P., Giepmans, B.N., 2021. State-of-the-art microscopy to understand
islets of langerhans: what to expect next? Immunology and Cell Biology 99,
509–520.

de Boer, P., Hoogenboom, J., Giepmans, B., 2015. Correlated light and electron

microscopy: ultrastructure lights up! Nature Methods 12, 503–513.

de Boer, P., Pirozzi, N.M., Wolters, A.H., Kuipers, J., Kusmartseva, I., Atkin-
son, M.A., Campbell-Thompson, M., Giepmans, B.N., 2020. Large-scale elec-
tron microscopy database for human type 1 diabetes. Nature communications
11, 1–9.

Boergens, K.M., Berning, M., Bocklisch, T., Br¨aunlein, D., Drawitsch, F.,
Frohnhofen, J., Herold, T., Otto, P., Rzepka, N., Werkmeister, T., et al.,
2017. webknossos: eﬃcient online 3d data annotation for connectomics. na-
ture methods 14, 691–694.

Boser, B.E., Guyon, I.M., Vapnik, V.N., 1992. A training algorithm for optimal
margin classiﬁers, in: Proceedings of the ﬁfth annual workshop on Computa-
tional learning theory, pp. 144–152.

Breiman, L., 2001. Random forests. Machine learning 45, 5–32.

Cao, L., Lu, Y., Li, C., Yang, W., 2019. Automatic segmentation of pathological
glomerular basement membrane in transmission electron microscopy images
with random forest stacks. Computational and mathematical methods in
medicine 2019.

Cao, Y., Liu, S., Peng, Y., Li, J., 2020. Denseunet: densely connected unet
for electron microscopy image segmentation. IET Image Processing 14, 2682–
2689.

Casser, V., Kang, K., Pﬁster, H., Haehn, D., 2018. Fast mitochondria segmen-

tation for connectomics. arXiv preprint arXiv:1812.06024 .

Cetina, K., Buenaposada, J.M., Baumela, L., 2018. Multi-class segmentation of
neuronal structures in electron microscopy images. BMC bioinformatics 19,
1–13.

26

von Chamier, L., Laine, R.F., Jukkala, J., Spahn, C., Krentzel, D., Nehme,
E., Lerche, M., Hern´andez-P´erez, S., Mattila, P.K., Karinou, E., et al., 2021.
Democratising deep learning for microscopy with zerocostdl4mic. Nature com-
munications 12, 1–18.

Chang, H.H., Zhuang, A.H., Valentino, D.J., Chu, W.C., 2009. Performance
measure characterization for evaluating neuroimage segmentation algorithms.
Neuroimage 47, 122–135.

Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2014. Se-
mantic image segmentation with deep convolutional nets and fully connected
crfs. arXiv preprint arXiv:1412.7062 .

Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2017a.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous
convolution, and fully connected crfs. IEEE transactions on pattern analysis
and machine intelligence 40, 834–848.

Chen, L.C., Papandreou, G., Schroﬀ, F., Adam, H., 2017b. Rethink-
arXiv preprint

ing atrous convolution for semantic image segmentation.
arXiv:1706.05587 .

Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H., 2018. Encoder-
decoder with atrous separable convolution for semantic image segmentation,
in: Proceedings of the European conference on computer vision (ECCV), pp.
801–818.

Chen, T., Kornblith, S., Norouzi, M., Hinton, G., 2020. A simple framework for
contrastive learning of visual representations, in: International conference on
machine learning, PMLR. pp. 1597–1607.

Ciga, O., Martel, A.L., Xu, T., 2020. Self supervised contrastive learning for

digital histopathology. arXiv preprint arXiv:2011.13971 .

Ciresan, D., Giusti, A., Gambardella, L., Schmidhuber, J., 2012. Deep neural
networks segment neuronal membranes in electron microscopy images. Ad-
vances in neural information processing systems 25, 2843–2851.

Dalal, N., Triggs, B., 2005. Histograms of oriented gradients for human de-
tection, in: 2005 IEEE computer society conference on computer vision and
pattern recognition (CVPR’05), Ieee. pp. 886–893.

Dietlmeier, J., McGuinness, K., Rugonyi, S., Wilson, T., Nuttall, A., O’Connor,
N.E., 2019. Few-shot hypercolumn-based mitochondria segmentation in car-
diac and outer hair cells in focused ion beam-scanning electron microscopy
(ﬁb-sem) data. Pattern recognition letters 128, 521–528.

Dittmayer, C., Goebel, H.H., Heppner, F.L., Stenzel, W., Bachmann, S., 2021.
Preparation of samples for large-scale automated electron microscopy of tissue
and cell ultrastructure. Microscopy and Microanalysis 27, 815–827.

27

Dong, N., Xing, E.P., 2018. Few-shot semantic segmentation with prototype

learning., in: BMVC.

Dorkenwald, S., Schubert, P.J., Killinger, M.F., Urban, G., Mikula, S., Svara,
F., Kornfeld, J., 2017. Automated synaptic connectivity inference for volume
electron microscopy. Nature methods 14, 435–442.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-
terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,
2020. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 .

Drawitsch, F., Karimi, A., Boergens, K.M., Helmstaedter, M., 2018. Fluoem,
virtual labeling of axons in three-dimensional electron microscopy data for
long-range connectomics. Elife 7, e38976.

Dreiseitl, S., Ohno-Machado, L., 2002. Logistic regression and artiﬁcial neural
network classiﬁcation models: a methodology review. Journal of biomedical
informatics 35, 352–359.

Drucker, H., Cortes, C., 1995. Boosting decision trees. Advances in neural

information processing systems 8.

Eberle, A., Mikula, S., Schalek, R., Lichtman, J., Tate, M.K., Zeidler, D.,
2015. High-resolution, high-throughput imaging with a multibeam scanning
electron microscope. Journal of microscopy 259, 114–120.

Faas, F.G., Avramut, M.C., M. van den Berg, B., Mommaas, A.M., Koster,
A.J., Ravelli, R.B., 2012. Virtual nanoscopy: generation of ultra-large high
resolution electron microscopy maps. Journal of Cell Biology 198, 457–469.

Fakhry, A., Zeng, T., Ji, S., 2016. Residual deconvolutional networks for brain
electron microscopy image segmentation. IEEE transactions on medical imag-
ing 36, 447–456.

Farm, J., 2014. Raveler.

Fern´andez-Baldera, A., Baumela, L., 2014. Multi-class boosting with asymmet-

ric binary weak-learners. Pattern Recognition 47, 2080–2090.

Gecer, B., Azzopardi, G., Petkov, N., 2017. Color-blob-based cosﬁre ﬁlters for
object recognition. Image and Vision Computing 57, 165–174. doi:https:
//doi.org/10.1016/j.imavis.2016.10.006.

Glasmachers, T., 2017. Limits of end-to-end learning, in: Asian Conference on

Machine Learning, PMLR. pp. 17–32.

Glorot, X., Bengio, Y., 2010. Understanding the diﬃculty of training deep
feedforward neural networks, in: Proceedings of the thirteenth international
conference on artiﬁcial intelligence and statistics, JMLR Workshop and Con-
ference Proceedings. pp. 249–256.

28

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y., 2014. Generative adversarial nets. Advances in
neural information processing systems 27.

Guay, M.D., Emam, Z.A., Anderson, A.B., Aronova, M.A., Pokrovskaya, I.D.,
Storrie, B., Leapman, R.D., 2021a. Dense cellular segmentation for em using
2d–3d neural network ensembles. Scientiﬁc reports 11, 1–11.

Guay, M.D., Emam, Z.A., Anderson, A.B., Aronova, M.A., Pokrovskaya, I.D.,
Storrie, B., Leapman, R.D., 2021b. Dense cellular segmentation for em using
2d–3d neural network ensembles. Scientiﬁc reports 11, 1–11.

Guay, M.D., Emam, Z.A., Anderson, A.B., Leapman, R.D., 2019. Transfer
learning for eﬃcient segmentation of subcellular structures in 3-d electron
microscopy. Biophysical Journal 116, 288a.

Haberl, M.G., Churas, C., Tindall, L., Boassa, D., Phan, S., Bushong,
E.A., Madany, M., Akay, R., Deerinck, T.J., Peltier, S.T., et al., 2018.
Cdeep3m—plug-and-play cloud-based deep learning for image segmentation.
Nature methods 15, 677–680.

Haehn, D., Knowles-Barley, S., Roberts, M., Beyer, J., Kasthuri, N., Lichtman,
J.W., Pﬁster, H., 2014. Design and evaluation of interactive proofreading
tools for connectomics.
IEEE transactions on visualization and computer
graphics 20, 2466–2475.

He, J., Xiang, S., Zhu, Z., 2020. A deep fully residual convolutional neural
network for segmentation in em images. International Journal of Wavelets,
Multiresolution and Information Processing 18, 2050007.

He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving deep into rectiﬁers: Sur-
passing human-level performance on imagenet classiﬁcation, in: Proceedings
of the IEEE international conference on computer vision, pp. 1026–1034.

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 770–778.

Heinrich, L., Bennett, D., Ackerman, D., Park, W., Bogovic, J., Eckstein, N.,
Petruncio, A., Clements, J., Pang, S., Xu, C.S., et al., 2021. Whole-cell
organelle segmentation in volume electron microscopy. Nature , 1–6.

Helmstaedter, M., Briggman, K.L., Denk, W., 2011. High-accuracy neurite
reconstruction for high-throughput neuroanatomy. Nature neuroscience 14,
1081–1088.

Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural com-

putation 9, 1735–1780.

29

Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely
connected convolutional networks, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4700–4708.

Iudin, A., Korir, P.K., Salavert-Torres, J., Kleywegt, G.J., Patwardhan, A.,
2016. Empiar: a public archive for raw electron microscopy image data.
Nature methods 13, 387–388.

Januszewski, M., Kornfeld, J., Li, P.H., Pope, A., Blakely, T., Lindsey, L.,
Maitin-Shepard, J., Tyka, M., Denk, W., Jain, V., 2018. High-precision auto-
mated reconstruction of neurons with ﬂood-ﬁlling networks. Nature methods
15, 605–610.

Jiang, Y., Xiao, C., Li, L., Chen, X., Shen, L., Han, H., 2019. An eﬀective
encoder-decoder network for neural cell bodies and cell nucleus segmentation
of em images, in: 2019 41st Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC), IEEE. pp. 6302–6305.

Kagadis, G.C., Kloukinas, C., Moore, K., Philbin, J., Papadimitroulas, P., Alex-
akos, C., Nagy, P.G., Visvikis, D., Hendee, W.R., 2013. Cloud computing in
medical imaging. Medical physics 40, 070901.

Karaba˘g, C., Jones, M.L., Peddie, C.J., Weston, A.E., Collinson, L.M., Reyes-
Aldasoro, C.C., 2019. Segmentation and modelling of the nuclear envelope of
hela cells imaged with serial block face scanning electron microscopy. Journal
of Imaging 5, 75.

Kasthuri, N., Hayworth, K.J., Berger, D.R., Schalek, R.L., Conchello, J.A.,
Knowles-Barley, S., Lee, D., V´azquez-Reina, A., Kaynig, V., Jones, T.R.,
et al., 2015. Saturated reconstruction of a volume of neocortex. Cell 162,
648–661.

Katz, W.T., Plaza, S.M., 2019. Dvid: distributed versioned image-oriented

dataservice. Frontiers in neural circuits 13, 5.

Khadangi, A., Boudier, T., Rajagopal, V., 2021a. Em-net: Deep learning for
electron microscopy image segmentation, in: 2020 25th International Confer-
ence on Pattern Recognition (ICPR), IEEE. pp. 31–38.

Khadangi, A., Boudier, T., Rajagopal, V., 2021b. Em-stellar: benchmarking
deep learning for electron microscopy image segmentation. Bioinformatics 37,
97–106.

Kirillov, A., He, K., Girshick, R., Rother, C., Doll´ar, P., 2019. Panoptic seg-
mentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 9404–9413.

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classiﬁcation with
deep convolutional neural networks. Advances in neural information process-
ing systems 25, 1097–1105.

30

Kucukelbir, A., Sigworth, F.J., Tagare, H.D., 2014. Quantifying the local reso-

lution of cryo-em density maps. Nature methods 11, 63–65.

Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.,
2017. A dataset and a technique for generalized nuclear segmentation for
computational pathology. IEEE transactions on medical imaging 36, 1550–
1560.

LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. nature 521, 436–444.

Li, W., Liu, J., Xiao, C., Deng, H., Xie, Q., Han, H., 2018. A fast forward 3d
connection algorithm for mitochondria and synapse segmentations from serial
em images. BioData mining 11, 1–15.

Liang, K.J., Rangrej, S.B., Petrovic, V., Hassner, T., 2022. Few-shot learning

with noisy labels. arXiv preprint arXiv:2204.05494 .

Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for
semantic segmentation, in: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 3431–3440.

Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints.

International journal of computer vision 60, 91–110.

Lucchi, A., Li, Y., Fua, P., 2013. Learning for structured prediction using
approximate subgradient descent with working sets, in: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1987–
1994.

Luo, Z., Wang, Y., Liu, S., Peng, J., 2021. Hierarchical encoder-decoder with
soft label-decomposition for mitochondria segmentation in em images. Fron-
tiers in Neuroscience 15.

Man´e, D., et al., 2015. Tensorboard: Tensorﬂow’s visualization toolkit. Re-

trieved October 8, 2021.

G´omez-de Mariscal, E., Maˇska, M., Kotrbov´a, A., Posp´ıchalov´a, V., Matula,
P., Mu˜noz-Barrutia, A., 2019. Deep-learning-based segmentation of small
extracellular vesicles in transmission electron microscopy images. Scientiﬁc
reports 9, 1–10.

Masci, J., Meier, U., Cire¸san, D., Schmidhuber, J., 2011. Stacked convolutional
auto-encoders for hierarchical feature extraction, in: International conference
on artiﬁcial neural networks, Springer. pp. 52–59.

Mekuˇc, M.ˇZ., Bohak, C., Hudoklin, S., Kim, B.H., Kim, M.Y., Marolt, M.,
et al., 2020. Automatic segmentation of mitochondria and endolysosomes in
volumetric electron microscopy data. Computers in biology and medicine 119,
103693.

31

Motta, A., Berning, M., Boergens, K.M., Staﬄer, B., Beining, M., Loomba, S.,
Hennig, P., Wissler, H., Helmstaedter, M., 2019. Dense connectomic recon-
struction in layer 4 of the somatosensory cortex. Science 366.

Noh, H., Hong, S., Han, B., 2015. Learning deconvolution network for seman-
tic segmentation, in: Proceedings of the IEEE international conference on
computer vision, pp. 1520–1528.

Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,
K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al., 2018. Attention u-net:
Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 .

Oztel, I., Yolcu, G., Ersoy, I., White, T.A., Bunyak, F., 2018. Deep learning
approaches in electron microscopy imaging for mitochondria segmentation.
International Journal of Data Mining and Bioinformatics 21, 91–106.

Peddie, C.J., Collinson, L.M., 2014. Exploring the third dimension: volume

electron microscopy comes of age. Micron 61, 9–19.

Peddie, C.J., Genoud, C., Kreshuk, A., Meechan, K., Micheva, K.D., Narayan,
K., Pape, C., Parton, R.G., Schieber, N.L., Schwab, Y., et al., 2022. Volume
electron microscopy. Nature Reviews Methods Primers 2, 1–23.

Peng, J., Yi, J., Yuan, Z., 2020. Unsupervised mitochondria segmentation in
em images via domain adaptive multi-task learning. IEEE Journal of Selected
Topics in Signal Processing 14, 1199–1209.

Peng, J., Yuan, Z., 2019. Mitochondria segmentation from em images via hier-
archical structured contextual forest. IEEE journal of biomedical and health
informatics 24, 2251–2259.

Perez, A.J., Seyedhosseini, M., Deerinck, T.J., Bushong, E.A., Panda, S., Tas-
dizen, T., Ellisman, M.H., 2014. A workﬂow for the automatic segmentation
of organelles in electron microscopy image stacks. Frontiers in neuroanatomy
8, 126.

Pirozzi, N.M., Hoogenboom, J.P., Giepmans, B.N., 2018. Colorem: analyti-
cal electron microscopy for element-guided identiﬁcation and imaging of the
building blocks of life. Histochemistry and cell biology 150, 509–520.

Plaza, S.M., 2014. Focused proofreading: eﬃciently extracting connectomes

from segmented em images. arXiv preprint arXiv:1409.1199 .

Plaza, S.M., Funke, J., 2018. Analyzing image segmentation for connectomics.

Frontiers in neural circuits 12, 102.

Quan, T.M., Hildebrand, D.G., Jeong, W.K., 2016. Fusionnet: A deep fully
residual convolutional neural network for image segmentation in connec-
tomics.

32

Rasmussen, L., 2005. Keynote: google maps and browser support for rich web

applications. Lecture Notes in Computer Science 3579, 7.

Ravelli, R.B., Kalicharan, R.D., Avramut, M.C., Sjollema, K.A., Pronk, J.W.,
Dijk, F., Koster, A.J., Visser, J.T., Faas, F.G., Giepmans, B.N., 2013. De-
struction of tissue, cells and organelles in type 1 diabetic rats presented at
macromolecular resolution. Scientiﬁc reports 3, 1–6.

Ren, Y., Kruit, P., 2016. Transmission electron imaging in the delft multibeam
scanning electron microscope 1. Journal of Vacuum Science & Technology B,
Nanotechnology and Microelectronics: Materials, Processing, Measurement,
and Phenomena 34, 06KF02.

Roels, J., Hennies, J., Saeys, Y., Philips, W., Kreshuk, A., 2019. Domain
adaptive segmentation in volume electron microscopy imaging, in: 2019 IEEE
16th International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp.
1519–1522.

Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks
for biomedical image segmentation, in: International Conference on Medical
image computing and computer-assisted intervention, Springer. pp. 234–241.

Saalfeld, S., Cardona, A., Hartenstein, V., Tomanˇc´ak, P., 2009. Catmaid: col-
laborative annotation toolkit for massive amounts of image data. Bioinfor-
matics 25, 1984–1986.

Scheﬀer, L.K., Xu, C.S., Januszewski, M., Lu, Z., Takemura, S.y., Hayworth,
K.J., Huang, G.B., Shinomiya, K., Maitlin-Shepard, J., Berg, S., et al., 2020.
A connectome and analysis of the adult drosophila central brain. Elife 9.

Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaud-
hary, V., Young, M., Crespo, J.F., Dennison, D., 2015. Hidden technical
debt in machine learning systems. Advances in neural information processing
systems 28.

Seifert, R., Markert, S.M., Britz, S., Perschin, V., Erbacher, C., Stigloher, C.,
Kollmannsberger, P., 2020. Deepclem: automated registration for correlative
light and electron microscopy using deep learning. F1000Research 9, 1275.

Shaban, A., Bansal, S., Liu, Z., Essa, I., Boots, B., 2017. One-shot learning for

semantic segmentation. arXiv preprint arXiv:1709.03410 .

Shaga Devan, K., Walther, P., von Einem, J., Ropinski, T., A Kestler, H.,
Read, C., 2021. Improved automatic detection of herpesvirus secondary en-
velopment stages in electron microscopy by augmenting training data with
synthetic labelled images generated by a generative adversarial network. Cel-
lular Microbiology 23, e13280.

Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-

scale image recognition. arXiv preprint arXiv:1409.1556 .

33

Sokol, E., Kramer, D., Diercks, G.F., Kuipers, J., Jonkman, M.F., Pas, H.H.,
Giepmans, B.N., 2015. Large-scale electron microscopy maps of patient skin
and mucosa provide insight into pathogenesis of blistering diseases. Journal
of Investigative Dermatology 135, 1763–1770.

Sommer, C., Straehle, C., Koethe, U., Hamprecht, F.A., 2011. Ilastik: Interac-
tive learning and segmentation toolkit, in: 2011 IEEE international sympo-
sium on biomedical imaging: From nano to macro, IEEE. pp. 230–233.

Spiers, H., Songhurst, H., Nightingale, L., de Folter, J., Community, Z.V.,
Hutchings, R., Peddie, C.J., Weston, A., Strange, A., Hindmarsh, S., et al.,
2021. Deep learning for automatic segmentation of the nuclear envelope in
electron microscopy data, trained with volunteer segmentations. Traﬃc .

Srivastava, R.K., Greﬀ, K., Schmidhuber, J., 2015. Highway networks. arXiv

preprint arXiv:1505.00387 .

Staﬄer, B., Berning, M., Boergens, K.M., Gour, A., van der Smagt, P., Helm-
staedter, M., 2017. Synem, automated synapse detection for connectomics.
Elife 6, e26414.

Szegedy, C., Ioﬀe, S., Vanhoucke, V., Alemi, A.A., 2017. Inception-v4, inception-
resnet and the impact of residual connections on learning, in: Thirty-ﬁrst
AAAI conference on artiﬁcial intelligence.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions, in:
Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pp. 1–9.

Takaya, E., Takeichi, Y., Ozaki, M., Kurihara, S., 2021. Sequential semi-
supervised segmentation for serial electron microscopy image with small num-
ber of labels. Journal of Neuroscience Methods 351, 109066.

Takemura, S.y., Aso, Y., Hige, T., Wong, A., Lu, Z., Xu, C.S., Rivlin, P.K.,
Hess, H., Zhao, T., Parag, T., et al., 2017. A connectome of a learning and
memory center in the adult drosophila brain. Elife 6, e26975.

Takemura, S.y., Xu, C.S., Lu, Z., Rivlin, P.K., Parag, T., Olbris, D.J., Plaza,
S., Zhao, T., Katz, W.T., Umayam, L., et al., 2015. Synaptic circuits and
their variations within diﬀerent columns in the visual system of drosophila.
Proceedings of the National Academy of Sciences 112, 13711–13716.

Tao, X., Hong, X., Chang, X., Dong, S., Wei, X., Gong, Y., 2020. Few-shot
class-incremental learning, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 12183–12192.

Titze, B., Genoud, C., 2016. Volume scanning electron microscopy for imaging

biological ultrastructure. Biology of the Cell 108, 307–323.

34

Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007. Toward objective evaluation
of image segmentation algorithms. IEEE transactions on pattern analysis and
machine intelligence 29, 929–944.

Urakubo, H., Bullmann, T., Kubota, Y., Oba, S., Ishii, S., 2019. Uni-em:
An environment for deep neural network-based automated segmentation of
neuronal electron microscopic images. Scientiﬁc reports 9, 1–9.

Utgoﬀ, P.E., 1989. Incremental induction of decision trees. Machine learning 4,

161–186.

Wang, L., He, D.C., 1990. Texture classiﬁcation using texture spectrum. Pattern

recognition 23, 905–910.

Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J., Appleton, G., Axton, M.,
Baak, A., Blomberg, N., Boiten, J.W., da Silva Santos, L.B., Bourne, P.E.,
et al., 2016. The fair guiding principles for scientiﬁc data management and
stewardship. Scientiﬁc data 3, 1–9.

Xiao, C., Chen, X., Li, W., Li, L., Wang, L., Xie, Q., Han, H., 2018. Automatic
mitochondria segmentation for em data using a 3d supervised convolutional
network. Frontiers in neuroanatomy 12, 92.

Yamashita, R., Nishio, M., Do, R.K.G., Togashi, K., 2018. Convolutional neural
networks: an overview and application in radiology. Insights into imaging 9,
611–629.

Yuan, J., Zhang, J., Shen, L., Zhang, D., Yu, W., Han, H., 2020. Massive
data management and sharing module for connectome reconstruction. Brain
Sciences 10, 314.

Yuan, Z., Ma, X., Yi, J., Luo, Z., Peng, J., 2021. Hive-net: Centerline-aware
hierarchical view-ensemble convolutional network for mitochondria segmen-
tation in em images. Computer Methods and Programs in Biomedicine 200,
105925.

Zaharia, M., Chowdhury, M., Franklin, M.J., Shenker, S., Stoica, I., 2010. Spark:
Cluster computing with working sets, in: 2nd USENIX Workshop on Hot
Topics in Cloud Computing (HotCloud 10).

Zeng, T., Wu, B., Ji, S., 2017. Deepem3d: approaching human-level perfor-
mance on 3d anisotropic em image segmentation. Bioinformatics 33, 2555–
2562.

Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017. Pyramid scene parsing
network, in: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2881–2890.

Zhao, T., Olbris, D.J., Yu, Y., Plaza, S.M., 2018. Neutu: software for collabo-
rative, large-scale, segmentation-based connectome reconstruction. Frontiers
in Neural Circuits 12, 101.

35

Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J.,
Xiang, T., Torr, P.H., et al., 2021. Rethinking semantic segmentation from
a sequence-to-sequence perspective with transformers, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pp. 6881–
6890.

Zhu, X., Goldberg, A.B., 2009. Introduction to semi-supervised learning. Syn-

thesis lectures on artiﬁcial intelligence and machine learning 3, 1–130.

36

