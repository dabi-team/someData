2
2
0
2

r
a

M
7
2

]
E
S
.
s
c
[

2
v
7
0
7
9
0
.
3
0
2
2
:
v
i
X
r
a

M2TS: Multi-Scale Multi-Modal Approach Based on Transformer
for Source Code Summarization

Yuexiu Gao
Shandong Normal University
Jinan, China

Chen Lyu∗
Shandong Normal University
Jinan, China

ABSTRACT
Source code summarization aims to generate natural language de-
scriptions of code snippets. Many existing studies learn the syn-
tactic and semantic knowledge of code snippets from their token
sequences and Abstract Syntax Trees (ASTs). They use the learned
code representations as input to code summarization models, which
can accordingly generate summaries describing source code. Tradi-
tional models traverse ASTs as sequences or split ASTs into paths
as input. However, the former loses the structural properties of
ASTs, and the latter destroys the overall structure of ASTs. There-
fore, comprehensively capturing the structural features of ASTs in
learning code representations for source code summarization re-
mains a challenging problem to be solved. In this paper, we propose
M2TS, a Multi-scale Multi-modal approach based on Transformer
for source code Summarization. M2TS uses a multi-scale AST fea-
ture extraction method, which can extract the structures of ASTs
more completely and accurately at multiple local and global lev-
els. To complement missing semantic information in ASTs, we
also obtain code token features, and further combine them with
the extracted AST features using a cross modality fusion method
that not only fuses the syntactic and contextual semantic informa-
tion of source code, but also highlights the key features of each
modality. We conduct experiments on two Java and one Python
datasets, and the experimental results demonstrate that M2TS out-
performs current state-of-the-art methods. We release our code at
https://github.com/TranSMS/M2TS.

CCS CONCEPTS
• Software and its engineering → Software maintenance tools.

KEYWORDS
Source code summarization, Transformer, Neural network, Deep
learning

ACM Reference Format:
Yuexiu Gao and Chen Lyu. 2022. M2TS: Multi-Scale Multi-Modal Approach
Based on Transformer for Source Code Summarization. In 30th International
Conference on Program Comprehension (ICPC ’22), May 16–17, 2022, Virtual

∗Corresponding author. Email: lvchen@sdnu.edu.cn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ’22, May 16–17, 2022, Virtual Event, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3527907

Event, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
3524610.3527907

1 INTRODUCTION
During software development and maintenance, more than half of
the time is spent on program understanding and related tasks [1].
In most of these tasks, developers resort to looking at comments to
comprehend the meaning of source code [2]. However, the writing
of comments is often overlooked in software development, result-
ing in poor quality comments being obtained by developers [3, 4].
Source code summarization technology is the generation of brief
natural language descriptions for source code [5]. Its emergence
not only frees developers from handwritten comments but also in-
creases the efficiency and reduces the cost of software development
[6–9].

The earliest approaches to source code summarization are tradi-
tional, which rely mainly on templates [10–13]. Sridhara et al. [14]
applied a method for creating artificial templates where they used
the Software Word Usage Model (SWUM) to generate comments for
Java methods. Haiduc et al. [15, 16] used an Information Retrieval
(IR) approach to generate summaries, they applied a Vector Space
Model (VSM) to search summaries from similar code fragments.
Although these methods have achieved specific results, they are
overly dependent on naming normality and will not work if no
similar code fragments are given in the codebase.

With the rapid development of deep learning, extensive research
on source code summarization has emerged [17–20]. Lyer et al. [21]
used an attention-based sequence to sequence model to generate
summaries for source code. Later, in order to learn the structural
features of source code, some studies have used API [17] sequences
or ASTs [22] as input, Figure 1(a) shows a code snippet and its
AST. Hu et al. [23, 24] and LeClair et al. [25] fed AST sequences
obtained by traversal into the encoder-decoder model to generate
summaries. Alon et al. [26] extracted paths from ASTs to help
generate comments. Although these studies are carried out on ASTs,
they have destroyed the structural properties and integrity of ASTs.
Both the encoder and decoder used in the above studies were RNNs
(LSTM or GRU) [27, 28]. To address the inability of RNNs to capture
the long dependencies arising from using long sequences as input,
Vaswani et al. [29] proposed a Transformer model, which is effective
on neural machine translation tasks. Ahmad et al. [30] improved
on the ordinary Transformer and obtained excellent results using
only source code as input of the model. Most of the above works
[24, 25, 31, 32] have shown that using ASTs and source code as
input can improve the quality of generated summaries, so we also
use two modalities for study in this paper.

Two factors that influence the quality of summary generation

can be identified from the above studies:

 
 
 
 
 
 
ICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

AST structural feature extraction. The treatment of ASTs
in the previous studies mainly includes traversing them into se-
quences or using AST paths. Hu et al. [23, 24] used AST sequences
obtained by a Structure-Based Traversal (SBT) method as input,
which only extracts the AST structure globally without considering
the relationships between nodes, losing the structural properties
of ASTs [33]. In addition, LeClair et al. [31] used GNN to embed
the AST graph, but the embedding of ASTs using 2-Hop GNN was
unable to extract the overall structure of ASTs. So the first problem
we need to solve is how to extract the structural features of ASTs
more comprehensively.

Multi-modal feature fusion. There are numerous studies in
the source code summarization that use multi-modal features of
source code for modeling, where the multi-modal refers to multiple
different representations of source code. For example, Hu et al. [24],
Zhang et al. [34], LeClair et al. [31] used the AST and source code
for their research, and the semantic and syntactic information of
code can be learned using these two modalities. In the existing
multi-modal fusion methods, most of them use the traditional at-
tention mechanism to sum up different modality features [24, 25].
However, this method allocates attention only within modalities
while ignoring the impact of inter-modal importance on the gen-
erated summaries, failing to highlight critical information in the
source code. Therefore, how to better fuse multiple modal features
is the second issue we need to address.

For the first problem, we use multiple scales of ASTs (i.e., the
different power matrices obtained from the AST adjacency matrix)
combined with the Graph Convolutional Neural Network (GCN)
for feature extraction. Since the adjacency matrix only represents
the relationship between neighboring nodes while not obtaining
the information of non-neighboring nodes, the multi-scale method
can ensure the comprehensive AST structural features extracted at
multiple local and global levels. For the second problem, we use a
new cross modality feature fusion method that not only fuses the
syntactic and contextual semantic knowledge of source code, but
also highlights key features that help generate summaries.

In this paper, synthesizing the above issues, we propose a multi-
scale multi-modal approach based on Transformer for source code
summarization. First, we propose a multi-scale AST feature ex-
traction method that uses GCN to embed ASTs at multiple scales.
Then the embedded AST and code tokens vectors are encoded re-
spectively, and these encoded features are fused using a new cross
modality feature fusion method. Finally, the fused features are fed
into the decoder with the encoded code tokens features to generate
summaries.

We conduct extensive experiments on the three datasets, and
the experimental results demonstrate that M2TS obtains better
scores than current state-of-the-art methods on BLEU, METEOR,
ROUGE_L and CIDER evaluation metrics. For example, our model
obtains a 57.87% ROUGE_L score on the Java dataset used by Hu
et al. [17], which is 3.8% higher than the current state-of-the-art
method (SG-Trans), and 33.84% BLEU-4 score on the Python dataset
used by Barone et al. [35], which is 5.1% higher than SG-Trans.
In addition, we also conduct ablation experiments on M2TS, and
the results show that our proposed multi-scale and multi-modal
method is essential for improving the model performance.

Our main contributions are as follows:

• We propose a Transformer-based multi-scale multi-modal
approach for source code summarization (M2TS) that com-
prehensively extracts semantic and syntactic structural fea-
tures of source code, resulting in higher quality generated
summaries.

• We present a multi-scale AST feature extraction method,
which can extract the structure of ASTs more completely
and accurately from both multiple local and global levels.
• We design a new cross modality feature fusion method that
not only highlights the structural features of source code but
also learns the contextual relevance between code tokens.
• We conduct extensive experiments including quantitative
and qualitative comparisons that show the effectiveness of
M2TS when compared with other state-of-the-art methods.

2 BACKGROUND AND MOTIVATION
2.1 The Transformer Structure
The Transformer is a relatively popular model in recent years [29],
and it has been widely used in fields such as natural language
processing and computer vision. The following are the details of
several significant structures used in the Transformer.

2.1.1 Multi-head Attention. The structure is the main component
in the Transformer model, and it solves the long dependency prob-
lem of sequences. Multi-head attention is the set of multiple self-
attentions. In self-attention, the input vector is respectively mul-
tiplied with the weights 𝑊 𝑞,𝑊 𝑘,𝑊 𝑣 to obtain three new vectors
𝑄, 𝐾 and 𝑉 . While in multi-head attention, multiple 𝑄, 𝐾 and 𝑉
are obtained by multiplying the input vector with several different
weights. The following is the calculation formula of multi-head
attention:

𝑄𝑖 = 𝑋𝑊 𝑞

𝑖 , 𝐾𝑖 = 𝑋𝑊 𝑘

𝑖 , 𝑉𝑖 = 𝑋𝑊 𝑣
𝑖 ,

(1)

ℎ𝑒𝑎𝑑𝑖 = 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄𝑖, 𝐾𝑖, 𝑉𝑖 ) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (

𝑄𝑖𝐾𝑇
𝑖
√︁𝑑𝑘

)𝑉𝑖,

(2)

𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑 (𝑄, 𝐾, 𝑉 ) = 𝐶𝑜𝑛𝑐𝑎𝑡 (ℎ𝑒𝑎𝑑1, . . . , ℎ𝑒𝑎𝑑𝑛)𝑊 𝑂,
where 𝑋 is the input vector, 𝑖 denotes the 𝑖_𝑡ℎ head, and 𝑛 denotes
the total number of heads [29].

(3)

2.1.2 Position Encoding. Since the Transformer does not use RNNs,
it cannot use the order information of words. Therefore, positional
encoding is used in the Transformer to preserve the position of
the word in the sequence. We use 𝑃𝐸 to denote position encoding.
The dimension of 𝑃𝐸 is kept consistent with the input vector. The
position encoding can be obtained by training or formula. The
Transformer use the following formula to get the position encoding:

𝑃𝐸 (𝑝𝑜𝑠,2𝑖) = sin(𝑝𝑜𝑠/100002𝑖/𝑑 ),

(4)

𝑃𝐸 (𝑝𝑜𝑠,2𝑖+1) = cos(𝑝𝑜𝑠/100002𝑖/𝑑 ),
where 𝑝𝑜𝑠 denotes the position of the word in the sentence, 𝑑
denotes the dimension of 𝑃𝐸, 2𝑖 denotes the even dimension, and
2𝑖 + 1 denotes the odd dimension [29].

(5)

M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization

ICPC ’22, May 16–17, 2022, Virtual Event, USA

nodes with a common parent are far apart by traversal (for example,
children nodes 4 and 7 of node 3), and the other is that the sequence
may not be restored to a unique AST. To solve the above problems,
Hu et al. [23] proposed the SBT method, the sequence of Figure 1(a)
was obtained by using this method as (0 (1 (2) 2) 1 (3 (4 (5) 5 (6) 6)
4 (7 (8 (9) 9) 8) 7) 3) 0, which can restore a unique AST. However,
compared with the traditional traversal methods, the sequences
obtained by the SBT method are longer, which inevitably leads to
the long dependence problem when encoding. In addition, this way
of using AST sequences as input of model destroys the structural
properties of ASTs.

In the methods of using ASTs paths, each path of ASTs is usu-
ally obtained and then encoded [26]. We can get several paths of
Figure 1(a) as: 0→1→2, 0→3→4→5, 0→3→4→6, 0→3→7→8→9.
The AST paths are encoded in such a way that they preserve the
hierarchical relationships of nodes, but this method also has cer-
tain limitations. For example, the children nodes 4 and 5 of node 2,
which are in two pathways respectively, result in the correlation
of the two nodes being lost when encoded. Therefore, this method
breaks the integrity of the AST structure to some extent.

The last is embedding ASTs using GNNs [33, 37], and here we
take GCN as an example. As the number of GCN layers increases,
each node can aggregate a more extensive range of information
from its neighbors, thus focusing on a broader range of local fea-
tures. For example, two layers of GCN are used for Figure 1(a) AST,
and node 0 can aggregate not only the information of nodes 1 and
3 but also the information of nodes 2, 4 and 7 indirectly. Although
GCN can extract the local structure of ASTs [38], it ignores the
global information of ASTs.

To address the problems in the above studies, we propose a
multi-scale AST feature extraction method. The multi-scale refers
to the different power matrices of the AST adjacency matrix by
dot product. These power matrices have the special property that
the 𝑚_𝑡ℎ power matrix represents how many schemes there are
between nodes arriving through 𝑚 edges. For example, the element
1 in the red font of 𝐴2 in Figure 1(b) indicates that one scheme is
reached between nodes 3 and 5 through two edges. Based on the
obtained power matrices, it can be seen from the AST that a certain
node can establish a specific local structure with other nodes. As
shown in Figure 1, the 𝐴2 in Figure 1(b) shows that reaching node
3 through two edges are nodes 1, 5, 6, 8, and node itself. It can be
seen from the Figure 1(a) AST that these edges can represent the
syntactic structure of the node "IfStatement" (the orange dashed
line indicates the reachable paths), and this local structure can be
extracted by embedding the 𝐴2 matrix using GCN. Unlike general
GCN which only uses the adjacency matrix to embed the local
features of ASTs. In contrast, our method can represent multiple
different local structures of nodes by using multiple scales of ASTs,
and then using the weighted summation method introduced in
Section 3.1 also can extract the global structural features of ASTs.

3 OUR APPROACH
Our proposed approach M2TS mainly consists of a Multi-Scale
AST feature extraction module (MSA), a cross modality feature
fusion module (ACF, i.e., AST and Code Fusion), and a Combined
Decoder (CD). Among them, the MSA module embedding ASTs

(a) A Java method and its AST

(b) The adjacency matrix and its quadratic matrix

Figure 1: A motivating example. (a) is a Java method and its
AST, the type of nodes is in boldface and its corresponding
value of nodes is in italics, the number represents the ID of
AST nodes; (b) represents the adjacency matrix correspond-
ing to AST in (a) and its quadratic matrix.

2.2 Motivating Example
To explain that our proposed multi-scale method is effective, we
use a specific example for illustration in this subsection. We have
learned that code structures are crucial for generating summaries
by analyzing previous studies [23, 26, 31, 36]. In the research on the
representation of ASTs syntax structural features, we summarize
three common methods:

(1) Traversal of ASTs;
(2) Using ASTs paths;
(3) Embedding ASTs using Graph Neural Networks.

Although the above three methods have achieved certain results,
they also have corresponding shortcomings. In the following, we
will use Figure 1 to illustrate the weaknesses of these methods and
demonstrate the advantages of our new method. For convenience,
we use the node ID to represent the entire node in the following
exposition.

The first is to traverse ASTs to sequences. The AST sequence
that can be obtained using traditional traversal methods (e.g., prior-
order traversal) for Figure 1(a) is 0 1 2 3 4 5 6 7 8 9. However, this
traversal method has two significant problems, one is that the two

MethodDeclaration(canceledit)StatementExpression(putbackeditingcellview)IfStatement(if)BinaryOperationMemberReference(listener)Literal(null)BlockStatementStatementExpression(listener.ondragend)MethodInvocationMethodInvocationpublic void canceledit( ) { putbackeditingcellview( ) ; if ( listener != null ) { listener . ondragend( ) ; } }(!=){2}{5}{1}{4}{6}{3}{7}{8}{9}{0}0101000000101000000001000000001000100100000101100000001000000000100000000100001000000001010000000010𝐴2010100100020100000010100000000103011010100030010000010110000001011000100010020100010000200000000101𝐴2ICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

Figure 2: The overall framework of our approach

at multiple scales can extract structural features of the code more
completely and accurately. The ACF module fuses features of two
modalities after encoding, which not only highlights the structural
information of the code but also learns the contextual semantic
relatedness between code tokens. Finally, the fused features and the
encoded code tokens features are fed into the combined decoder
to generate summaries. The decoder can be used for information
supplementation. The overall framework of M2TS is illustrated in
Figure 2.

3.1 Multi-Scale AST Feature Extraction
In this paper, we use GCN to embed the multiple scales AST. We
have learned that the best results can be achieved by using a two-
layer GCN for the node classification problem [39], so in this paper,
we also use a two-layer GCN for the AST embedding. Unlike the
general multi-layer GCN, we add the output of the upper layer
graph convolution to the output of the current layer using a residual
connection. Each graph convolution layer is calculated using the
following formula:

𝐻 (𝑙+1) = 𝜎 ( ^𝐴𝐻 (𝑙)𝑊 (𝑙) ) + 𝐻 (𝑙),

(6)

where 𝑙 denotes the number of layers of the graph convolution, ^𝐴
denotes the normalized adjacency matrix, 𝐻 (𝑙) denotes the output
of the previous layer of the graph convolution, 𝑊 (𝑙) denotes a
weight matrix, and 𝜎 is a nonlinear activation function [39].

When inputting the first layer of graph convolution, we use the
BERT pre-training [40] to embed the "type" and "value" of nodes
as the initial feature vector 𝐻 (0) . Here the vectors obtained by pre-
training are fixed and do not participate in the whole optimization
process of the model.

From Section 2, according to the particular property of the power
matrices, we regard the adjacency matrix 𝐴 and the power matri-
ces 𝐴2, . . . , 𝐴𝑛 obtained by dot product as the first, second, and
𝑛_𝑡ℎ scales of the AST, respectively. First, we input the first scale
representation 𝐴 of the AST combined with 𝐻 (0) into the two-
layer GCN to obtain 𝑍1. After that, the second scale representation
𝐴2 combined with the previous scale output 𝑍1 is input into the
two-layer GCN to obtain 𝑍2. By analogy, the output 𝑍𝑛−1 of the
𝑛 − 1_𝑡ℎ scale representation 𝐴𝑛−1 is combined with the 𝑛_𝑡ℎ scale
representation 𝐴𝑛 into the same layer GCN to obtain the output
𝑍𝑛. Finally, we weight the output of each scale and sum them up
to obtain the final AST embedding representation 𝑍 . For example,
when the number of scales 𝑛 is set to three, the AST embedding
vector is calculated as:

𝑍 = 𝛼𝑍1 + 𝛽𝑍2 + (1 − 𝛼 − 𝛽)𝑍3,
where the values of 𝛼 and 𝛽 will be determined in the experiments.
We use multiple scales AST representation in the MSA module,
which extracts AST structural features at multiple local and global
levels.

(7)

3.2 Cross Modality Feature Fusion
To obtain more code features, we use the source code and AST
modalities as input to M2TS. In order to fuse the features of two
modalities, in this paper we use a new cross modality feature fusion
method. The two modalities features need to be encoded before
fusion. We input the embedded vector 𝑍 obtained from the MSA
module into the AST encoder for encoding. Here we use a multi-
head attention mechanism in the Transformer and a two-layer fully
connected network (Feed Forward block in Figure 2) as the primary
constituent structure of the AST encoder. Before inputting into the
encoder, we need to obtain the position encoding of the embedded
feature of each node to preserve the position information of nodes.

Code EncoderCodeCommentsCode SnippetsASTEncoderAST……MSANode embedding𝐴𝐴2𝐴𝑛GCNGCNGCNCodeAttentionACFAttentionMasked-NLAttentionFeed ForwardCDTokens features…………OutputsAST features…………ACFTokens sequenceTokens embedding…………M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Figure 3: Basic structure of the two encoders. 𝑁 indicates the
number of layers

After encoding, we get the feature 𝑍 ′. For the code tokens, we
encode them using a code encoder, which has a similar structure to
the AST encoder. Before entering the code encoder, we use word
embedding to represent the code tokens. Then adding the word
vectors and the position encoding of the tokens input to the code
encoder to get the output 𝑀. It is worth noting that although the
two encoders have similar structures, they are aimed at inputs
of different modality features, so their parameters are optimized
separately during the training process. The basic structure of these
two encoders is shown in Figure 3.

Given the encoded AST feature 𝑍 ′ and the code tokens feature 𝑀,
we fuse them into a unified representation using the ACF module,
which is similar to the self-attention mechanism in the Transformer.
We regard the AST feature 𝑍 ′ as the query and the code tokens
feature 𝑀 as the key and value. Specifically, we first represent 𝑍 ′
and 𝑀 into 𝑄 𝑓 , 𝐾𝑓 and 𝑉𝑓 by 1 × 1 convolution, respectively [41]:
𝑄 𝑓 = 𝐶𝑜𝑛𝑣1×1 (𝑍 ′), 𝐾𝑓 = 𝐶𝑜𝑛𝑣1×1 (𝑀), 𝑉𝑓 = 𝐶𝑜𝑛𝑣1×1 (𝑀),
(8)
where 𝑄 𝑓 , 𝐾𝑓 , 𝑉𝑓 retain the same size as the original input and then
calculate the fused feature by the following formula:

𝐹 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (

𝑄 𝑓 𝐾𝑇
𝑓
√︁𝑑𝑘

)𝑉𝑓 ,

(9)

where 𝑑𝑘 we set to 64.

Finally, we employ a residual connection to add the embedded
AST representation 𝑍 and the fused feature 𝐹 to obtain the final
output 𝐹 ′. The detailed process of the ACF module is shown in
Figure 4.

3.3 Combined Decoder
We input the encoded code tokens and fused features into the
combined decoder to generate summaries. In the training phase,
the CD module consists of three attention blocks and a two-layer
fully connected network: Masked-NL Attention, Code Attention,
ACF Attention and Feed-Forward. As with the two encoders used, a
normalization is added after each block output in the decoder. In the
following, we describe in detail the input and output of information
in the CD module [29, 42].

Figure 4: ACF module fusion process

Generally, we input the natural language descriptions after word
embedding and positional encoding into the Masked-NL Attention
block, where the inputs 𝑄, 𝐾, and 𝑉 are all Natural Language (NL)
embedding. This representation is a way of computation in the self-
attention mechanism. After that, using the output 𝑆1 of Masked-NL
Attention block as 𝑄 and the encoded code tokens feature 𝑀 as
𝐾 and 𝑉 input to the Code Attention block to get the output 𝑆2.
Subsequently, we input the output 𝑆2 of the Code Attention block
as 𝑄 and the output 𝐹 ′ obtained from the ACF module as 𝐾, 𝑉 into
the ACF Attention block to get the output 𝑆.

The probability of generating summaries is then calculated using
a softmax activation function after passing through a two-layer
fully connected network. Using this decoder allows for information
supplementation. In the CD module, we aim to minimize the cross-
entropy loss function and update the parameters in the model using
optimization algorithms such as gradient descendent. The following
is the loss function:

𝐿(𝑙𝑜𝑠𝑠) = −

1
𝑁

𝑁
∑︁

𝑛
∑︁

log 𝑝 (𝑦 ( 𝑗)

𝑖

),

(10)

𝑖=1
where 𝑁 is the total number of training data, 𝑛 is the length of each
target summary, 𝑦 ( 𝑗)
is the 𝑗_𝑡ℎ word in the 𝑖_𝑡ℎ sentence, and
𝑝 (𝑦 ( 𝑗)
𝑖

) denotes the probability of generating the 𝑗_𝑡ℎ word.

𝑗=1

𝑖

4 EXPERIMENTAL SETUP
4.1 Datasets and Preprocessing
We conduct experiments on two Java and one Python datasets.
For the two Java datasets, we call them JAH and JAL, and for the
Python dataset we call it PYB. Specifically, we cite the 78422 pairs
of <Java methods, comments> used by Hu et al. [17] as the JAH
dataset1, which is collected in the repositories created on GitHub.
The JAL dataset2 is a collection of 2.1 million pairs of java methods
and summaries on the publicly available GitHub by LeClair et al.
[35]. Accordingly, we use the JDK3 compiler to parse the source
code in the two Java datasets into ASTs [24]. The PYB dataset4 we
used is created by Barone et al. [43], and it contains 108726 pairs of

1https://github.com/xing-hu/TL-CodeSum
2http://leclair.tech/data/funcom/
3http://www.eclipse.org/jdt/
4https://github.com/EdinburghNLP

Multi-HeadAttentionAdd & NormFeed ForwardAdd & Norm×NInputs embedding𝑸𝒇𝑲𝒇,𝑽𝒇Self-AttentionFusion-FeatureAST FeatureTokens FeatureConv𝟏×𝟏Conv𝟏×𝟏××AST Embedding×Element-wise MultiplyResidualConnectionICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

Table 1: Statistical analysis of the three datasets

Dataset

Code Length

NL Length

AST(Node)

AST(Depth)

AvgC

UniC

AvgL UniL MaxN AvgN MaxD AvgD

JAH
JAL
PYB

120.99
65.23
57.12

23501
207458
125584

17.71
8.21
9.82

29645
25341
31116

2126
1024
1522

50
41
87

58
26
32

8.2
9.4
13.1

python functions and their comments from the GitHub open source
repositories. For Python functions, we use the Treelib5 toolkit to
parse them as ASTs [43]. In this paper, we use two Java datasets
to explore the ability of M2TS to generate summaries for the same
language source code of different lengths.

Before the experiments, we preprocess the three datasets. We
set the same length limits for code snippets and summaries as
in [17, 35, 43]. The statistics for the three datasets are shown in
Table 1. The parameters AvgC and AvgL in the table indicate the
average length of code tokens and natural language, respectively,
and the UniC and UniL indicate the number of unique tokens in the
source code and natural language. In addition to code snippets and
comments, we perform a statistical analysis of ASTs corresponding
to the three datasets. The MaxN and AvgN denote the maximum
and average number of nodes, and the MaxD and AvgD denote the
maximum and average depth, respectively. To maintain the same
segmentation settings as the compared baselines [17, 35, 43], we
split the PYB dataset into training, validation and testing sets in
the proportion of 6 : 2 : 2, and the JAH and JAL datasets in the
proportion of 8 : 1 : 1. To make the training and testing sets disjoint,
we remove the duplicated samples from the testing set [32]. In
addition, we ensure that no identical samples exist for the two Java
datasets. In common with [30, 32], we split the source code tokens
in the form of CamelCase and snake case in the three datasets
into their respective subtokens. We show that this code tokens
segmentation improves the quality of the generated summaries.

In addition, we use a special symbol <PAD> to fill token se-
quences in the datasets with less than the maximum length and
truncate sequences with greater than the maximum length. We set
the maximum vocabulary size for source code and summaries to
50k, and tokens out of range are represented using the UNK [32]. In
particular, since the adjacency matrix cannot be obtained for ASTs
of a single node, we remove ASTs with less than two nodes in the
datasets. We also remove empty or one-word summaries because
they do not fully describe the meaning of code snippets.

4.2 Experimental Settings
We train M2TS using the SGD optimizer [44] and set the initial
learning rate to 0.0001. We set the minimum batch size and dropout
to 32 and 0.2, respectively. We set the multi-head attention mecha-
nism heads and the layers in both encoder and decoder to 8 and 6,
𝑑_𝑚𝑜𝑑𝑒𝑙 to 512, 𝑑_𝑓 𝑓 to 2048, 𝑑_𝑣 and 𝑑_𝑘 to 64. We train M2TS
for at most 200 epochs and adopt early stop if the validation per-
formance does not improve after 20 epochs. We leverage a beam
search during generating summaries and set the beam size to 5.
We set the initial feature vector dimension of the MSA module to

5https://treelib.readthedocs.io/en/latest/

768 and the nonlinear activation function in GCN to the ReLu. The
experiments in this paper are all conducted on the Intel(R) Xeon(R)
Gold 5218 CPU @ 2.30GHz, 128GB RAM and TITAN RTX 24G GPU
platform.

4.3 Evaluation Metrics
Similar to existing work [22–24, 32, 33], in this paper we evalu-
ate the quality of the generated summaries using four evaluation
metrics: BLEU [45], ROUGE_L [46], METEOR [47] and CIDER [48].
They have commonly used metrics in tasks such as machine trans-
lation and text summarization.

BLEU measures the average n-gram precision between the ref-
erence and generated summaries, with brevity penalty for short
sentences [49]. In detail, the BLEU score is computed as:

𝐵𝐿𝐸𝑈 − 𝑁 = 𝐵𝑃 · 𝑒𝑥𝑝

𝑁
∑︁

𝑛=1

𝜔𝑛 log 𝑝𝑛,

(11)

where 𝑝𝑛 is the score of 𝑛 −𝑔𝑟𝑎𝑚𝑠 in the generated sentences which
are present in the reference sentences, 𝜔𝑛 is the uniform weight
1/𝑁 and 𝐵𝑃 is brevity penalty. In this paper, we set 𝑁 to 4, which
is the maximum number of grams.

METEOR is used to measure how the model captures the con-
tent from the reference sentences in the generated sentences [22].
Its calculation formula is as follows:

𝑀𝐸𝑇 𝐸𝑂𝑅 = (1 − 𝛾 · 𝑓 𝑟𝑎𝑔𝛽 ) ·

𝑃 · 𝑅
𝛼 · 𝑃 + (1 − 𝛼) · 𝑅

,

(12)

where the 𝑃 and 𝑅 are the precision and recall, the 𝑓 𝑟𝑎𝑔 is a frag-
mentation fraction. The default values of the parameters 𝛾, 𝛽, 𝛼 are
0.5, 3.0 and 0.9, respectively.

ROUGE_L evaluates how much reference text appears in the
generated text. Based on the longest common subsequence(LCS), it
uses F-score which is the harmonic mean of precision and recall
values [49]. Suppose 𝑀 and 𝑁 are generated and reference sentences
of lengths 𝑎 and 𝑏, then:

𝑃𝐿 =

𝐿𝐶𝑆 (𝑀, 𝑁 )
𝑎

, 𝑅𝐿 =

𝐿𝐶𝑆 (𝑀, 𝑁 )
𝑏

,

𝐹𝑅𝑂𝑈 𝐺𝐸_𝐿 =

(1 + 𝛽2)𝑃𝐿 · 𝑅𝐿
𝑅𝐿 + 𝛽2𝑃𝐿

,

(13)

(14)

where the 𝛽 is set to 1.2 as in [22], and the 𝐹𝑅𝑂𝑈 𝐺𝑅_𝐿 is the value
of ROUGE_L.

CIDER is a consensus-based evaluation metric for measuring
image captioning quality [32]. Then, the CIDER calculation formula
is as follows:

𝐶𝐼 𝐷𝐸𝑅(𝑐, 𝑠) =

𝑁
∑︁

𝑛=1

𝑤𝑛CIDER𝑛 (𝑐, 𝑠),

(15)

where the 𝑐 and 𝑠 are the generated and reference sentences, 𝑛 is set
from 1 to 4 and the CIDER𝑛 (𝑐, 𝑠) score for 𝑛 − 𝑔𝑟𝑎𝑚 is computed
using the average cosine similarity between 𝑐 and 𝑠.

Among them, BLEU and ROUGE_L are based on precision and
recall, respectively, while METEOR not only considers both, but also
includes functions not found in other metrics, such as synonyms
matching and homographs, etc. CIDER looks at whether the model
captures key information.

M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 2: Comparison of our proposed approach with the baseline approaches

Approach

JAH

JAL

PYB

BLEU-4 METEOR ROUGE_L CIDER BLEU-4 METEOR ROUGE_L CIDER BLEU-4 METEOR ROUGE_L CIDER

CODE-NN
26.07%
Hybrid-DeepCom 38.55%
38.86%
Code2Seq
40.24%
Code+GNN+GRU
42.55%
V-Transformer
43.42%
NeuralCodeSum
45.10%
MMTrans
45.21%
SG-Trans
M2TS
46.84%

13.97%
23.28%
23.76%
24.25%
25.31%
26.04%
26.90%
27.11%
28.93%

40.20%
51.36%
51.85%
52.83%
53.14%
54.05%
55.21%
55.76%
57.87%

0.855
1.254
1.360
1.855
2.041
2.035
2.346
2.301
2.573

16.25%
20.74%
21.50%
22.13%
27.41%
28.26%
29.30%
30.49%
31.63%

8.63%
9.41%
10.23%
11.62%
16.47%
18.25%
18.06%
18.81%
20.06%

38.24%
40.31%
41.48%
43.32%
45.47%
47.14%
46.96%
47.28%
49.31%

0.901
1.021
1.175
1.536
2.068
2.185
2.267
2.362
2.504

16.58%
21.14%
21.86%
23.48%
30.58%
31.07%
32.36%
32.21%
33.84%

9.08%
9.64%
10.33%
12.31%
17.68%
18.98%
18.24%
19.56%
21.83%

36.97%
37.80%
38.13%
39.85%
43.04%
45.50%
46.25%
46.34%
47.92%

0.988
1.284
1.357
1.766
2.002
2.238
2.316
2.230
2.411

4.4 Baselines
We compare M2TS with the existing source code summarization
methods that are directly related to our work and described in detail
below.

• CODE-NN: The method is first proposed by Iyer et al. [21]
using an end-to-end deep learning method. It is a typical
encoder-decoder model that uses code tokens embedding as
the output of the encoder and uses LSTM for the decoder
combined with an attention mechanism to generate sum-
maries.

• Hybrid-DeepCom: This method, proposed by Hu et al. [24],
uses the code token sequences and the AST sequences ob-
tained via the SBT method as input to the Seq2Seq model,
which can extract semantic and structural information of
the code, respectively.

• Code2Seq: Alon et al. [26] used randomly paired paths of
ASTs as model input and encoded each path as a fixed-length
vector using LSTM. Finally, the summaries are generated at
the decoder side with an attention mechanism.

• Code+GNN+GRU: This work is first used by LeClair et
al. [31] to use GNN for the code summarization task. They
used GNN to embed ASTs and then input them into GRU
in combination with the tokens features, after which an
attention-based decoder generates summaries.

• V-Transformer: To solve the long dependency problem
that occurs when sequences are encoded, Vaswani et al. [29]
proposed a Transformer model that entirely uses the self-
attention mechanism, which can outperform previous mod-
els such as RNNs by using only the source code as input.
• NeuralCodeSum: Ahmad et al. [30] used relative position
encoding and copy mechanisms to improve the ordinary
Transformer, which is the first approach to combine the
Transformer with the code summarization task.

• MMTrans: Yang et al. [33] used two modalities of ASTs to
generate code summaries. They used the AST sequences ob-
tained via the SBT method and the embedding representation
obtained using GCN to extract the structure of ASTs.

• SG-Trans: Gao et al. [37] injected local semantic information
and global syntactic structure into the self-attentive module
in the Transformer as an inductive bias to better capture the
hierarchical features of source code.

5 RESULTS AND ANALYSIS
Our study aims to assess whether M2TS outperforms the latest
baselines and why the multi-scale and multi-modal method can
improve the quality of the generated summaries. In this section, we
give the experimental results and analysis of the proposed research
problems.

5.1 RQ1: How does our approach perform

compared to the baselines?

We conduct experiments on two Java and one Python datasets, and
the obtained experimental results are shown in Table 2. When repro-
ducing baselines, we try our best to follow the data processing steps
of baselines to prepare the input and set the hyper-parameters to be
as consistent as possible with this paper. In addition, beam search
is used for all baselines to ensure a fair comparison of experiments.
As can be seen from the table, the metric scores obtained by
M2TS on the three datasets are better than the latest baselines. For
example, M2TS obtains a 46.84% BLEU-4 score, 28.93% METEOR
score, 57.87% ROUGE_L score and 2.573 CIDER score on the JAH
dataset. The METEOR score obtained by our model on the PYB
dataset is 11.6% higher than the baseline SG-Trans, which indicates
that M2TS is better than the method used by SG-Trans to integrate
code structure information in the Transformer. In addition, M2TS
obtained higher metric scores than the baseline MMTrans on the
three datasets. This is because MMTran does not use the source
code as input, and although the reasons are given in the paper,
the critical role of the source code still cannot be denied based on
previous studies [33]. We use source code and ASTs as input and
fuse the two modality features, thus achieving better metric scores
than MMTrans. The BLEU-4 score obtained by M2TS on the JAH
dataset is 3.42 higher than NeuralCodeSum. Furthermore, through
experiments, we find that M2TS uses 1.6 times more parameters
than NeuralCodSum, so this degree of improvement is worthwhile
with a slight difference in parameters. V-Transformer and Neural-
CodeSum yield higher scores than Code+GNN+GRU in the JAL
dataset; the results confirm that the Transformer can handle long
sequence input problems well. The score of Code+GNN+GRU is
improved compared to Code2Seq and Hybrid-DeepCom, and on
the JAL dataset Code+GNN+GRU is 13.6% higher than Code2Seq
on the METEOR metric and 7.5% higher than Hybrid-DeepCom on
the ROUGE_L metric. It can be seen that embedding the AST in the
form of a graph is better than using the AST sequences and AST

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

Table 3: Comparison of setting different scales on the three datasets

Approach

Scale

JAH

JAL

PYB

BLEU-4 METEOR ROUGE_L CIDER BLEU-4 METEOR ROUGE_L CIDER BLEU-4 METEOR ROUGE_L CIDER

M2TS

M2TS (weights)
M2TS w/o BERT
M2TS (G-GCN)
M2TS (attention)

1
2
3
4
5
3
3
3
3

43.63%
44.82%
46.31%
45.72%
45.08%
46.84%
46.13%
45.96%
45.75%

26.97%
27.58%
28.10%
27.21%
26.83%
28.93%
28.52%
28.34%
27.86%

55.39%
57.03%
57.24%
56.96%
55.23%
57.87%
57.17%
57.03%
56.58%

2.100
2.374
2.501
2.485
2.390
2.573
2.519
2.524
2.431

29.15%
29.83%
30.57%
29.64%
28.79%
31.63%
30.47%
30.81%
29.64%

18.13%
18.74%
19.35%
18.86%
18.17%
20.06%
19.36%
19.57%
18.62%

47.20%
47.84%
48.46%
47.83%
46.65%
49.31%
48.50%
48.15%
47.94%

2.305
2.376
2.425
2.324
2.255
2.504
2.417
2.446
2.386

31.68%
32.82%
33.25%
32.80%
31.75%
33.84%
33.21%
33.05%
32.47%

19.23%
19.75%
20.37%
19.87%
19.11%
21.83%
20.75%
20.96%
19.33%

44.40%
45.48%
46.34%
45.27%
45.63%
47.92%
47.11%
46.81%
46.24%

2.184
2.279
2.368
2.231
2.304
2.411
2.335
2.314
2.290

paths, as the AST graph contains more structural information. In
addition, we also compare with the earliest deep learning method
CODE-NN, and it is not hard to find that inputting only source
code into the Seq2Seq model yields much lower scores than Hybrid-
DeepCom. This also confirms that the structural features of ASTs
are significant for the source code summarization task. As can be
seen from the data throughout Table 2, the quality of the generated
summaries is directly related to the completeness and accuracy of
the code structure extraction and the comprehensiveness of the
code modality features. Furthermore, Table 2 shows that the metric
scores obtained on the PYB and JAL datasets are generally lower
than those obtained on the JAH dataset, which may be related to
the source code themselves or ASTs parsed by them.

5.2 RQ2: How does the number of scales in the
MSA module affect the performance of
M2TS?

In the multi-scale AST feature extraction method, we use the power
matrices of the adjacency matrix to represent the different scales
of ASTs. This section explores the effect of different scales on the
experimental results on the three datasets.

By conducting experiments on the three datasets, we find that
the highest metric scores are obtained when the number of scales
is 3. As can be seen from Table 3, the BLEU-4 score obtained with
the number of scales set to 3 is 6.1% higher than that obtained
with a scale of 1 on the JAH dataset. It can be seen that using
three scales provides a more comprehensive representation of the
structural features of source code, making the generated summaries
more fluent in syntax. In the other three evaluation metrics of the
corresponding datasets, the scores of scale 3 are also the highest.
As the number of scales increases, more structural features can
be extracted from ASTs. When the number of scales is set to 4
or 5, the scores of the four evaluation metrics are decreased. We
consider that when too many scales are set, some noisy data will be
included (e.g., some duplicate paths information will be introduced),
resulting in poor summaries obtained at decoding.

In addition, instead of directly summing the outputs of each
scale in the multi-scale AST feature extraction module, we assign
different weights to them. The reason for doing this is that we use
the output of the previous scale as input to the following scale,
and simply adding up the output of each scale would contain some
duplicate information. From Table 3, it is seen that the metric scores

Table 4: Comparison of setting different heads on the JAH
dataset

Approach
V-Transformer
NeuralCodeSum
MMTrans
SG-Trans

M2TS

Head-size BLEU-4 METEOR ROUGE_L CIDER

8
8
8
8
2
4
6
8
10

42.55%
43.42%
45.10%
45.21%
43.68%
45.80%
46.01%
46.83%
45.94%

25.31%
26.04%
26.90%
27.11%
27.81%
28.01%
28.29%
28.93%
27.86%

53.14%
54.05%
55.21%
55.76%
56.92%
56.84%
57.13%
57.87%
56.91%

2.041
2.035
2.346
2.301
2.457
2.512
2.507
2.573
2.462

obtained by using the weighting method when setting the same
scale are higher. Taking the PYB dataset as an example, the weighted
metric scores obtained are 3.4% higher on ROUGE_L than the un-
weighted M2TS. It is experimentally verified that the best quality of
the generated summaries is obtained when the output weights of
the three scales are set to 0.1, 0.2, 0.7 respectively. Since we use the
output of the previous scale as the input of the next scale, it makes
sense to set the output weights for the third scale. In particular,
it can be seen from the experimental results that the best results
are obtained by the multi-scale method for different code length
datasets, so it is known that our method has strong universality.

5.3 RQ3: How do the multi-head attention

mechanism heads affect the performance
of M2TS?

M2TS uses the multi-head attention mechanism and the fully con-
nected network in the Transformer as the basic structure of the
encoder and decoder. Using multiple heads in the multi-head atten-
tion mechanism allows extra attention to be applied to different
parts of the input information. However, too many or too few heads
can impact the experimental results. To explore the optimal number
of heads, we set the number of heads to 2, 4, 6, 8, and 10 for experi-
ments. The results find that the highest metric scores are achieved
for the three datasets when the number of heads is 8. For conve-
nience, in Table 4 we only list the experimental results obtained by
setting different head numbers on the JAH dataset. As shown from
the table, M2TS is 6.7% higher than the baseline SG-Trans on ME-
TEOR when the same number of heads is set. The score increases

M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 5: Qualitative example of the different models’ perfor-
mance on the three datasets

Dataset

Example

protected void addtogui(jpanel gui, jtextfield b, string cmd) {

b.setactioncommand(cmd);
b.addactionlistener(this);
gui.add(b);

JAH

}
NeuralCodeSum: adds the feature to the attribute object.
M2TS (1): adds a attribute of the object.
M2TS (attention): adds a feature to the attribute.
M2TS: adds a feature to the gui attribute of the object.
Reference: adds a feature to the gui attribute of the layer object.

def traverse_tree(course):

queue = [course]
while(len(queue)>0):
node = queue.pop()
queue.extend(node.get_children())

reture True

PYB

NeuralCodeSum: load the descriptor from the descriptor.
M2TS (1): load the node descriptor.
M2TS (attention): load a descriptor to the course.
M2TS: load every descriptor in the course.
Reference: load every descriptor in course.

public int getSquareWidth() {

if(skin == null) {

return 0;

}
return skin.graphicWidth;

JAL

}
NeuralCodeSum: returns the current skin of the current called width.
M2TS (1): returns the current skin of square.
M2TS (attention): returns the width of the scaled skin.
M2TS: returns the current width of square from scaled skin.
Reference: returns the current width of square from the scaled skin.

with the number of heads until the number of heads is set to 8, after
which it starts to decrease so that more heads in the multi-head
attention is not better. We consider that when too many heads are
set, it leads to distraction and thus destroys the correlation between
the data. The other baselines are not listed in Table 4 because they
do not use the multi-head attention mechanism.

5.4 Ablation Study
We further conduct ablation experiments to verify the effects of
the multi-scale feature extraction method and the cross modality
fusion method on M2TS. The results are shown in the bottom half
of Table 3.

Analysis of using the BERT pre-training. In the multi-scale
AST feature extraction module, we embed the "type" and "value"
of nodes using the BERT pre-training method, and input them
into GCN as the initial feature vector of nodes. In this section, we
compare it with the random initialization method for nodes. As
seen in Table 3, higher metric scores are obtained using BERT than
the random initialization method on the three datasets. This is since
that AST nodes contain code tokens information, and embedding
them as the initial feature vector of nodes can preserve code tokens
semantic features as much as possible.

Analysis of GCN with the residual connection. To extract
the structural features of ASTs more completely and accurately,

we improve the general GCN in the multi-scale method by using
a residual connection to sum the output of the previous graph
convolution layer with the production of the current layer. In order
to verify the effectiveness of this method, we use residual-free GCN
with it for ablation analysis. The M2TS (G-GCN) represents using
the general GCN in Table 3, and the results show that the GCN with
residual connection works better. This may be because the general
GCN computes fewer message-passing iterations when embedding,
resulting in some loss of features [38].

Analysis of the cross modality fusion. To confirm that the
ACF module is better than using the traditional fusion method, we
fuse AST and code tokens features using two traditional attention
mechanisms as in [24, 31]. Then the fused features are fed into the
decoder to generate summaries, where we use the GRU for our
decoder. The ablation comparison experiments find that the met-
ric scores obtained using the ACF module are higher on the three
datasets. As shown in Table 3, the M2TS (attention) denotes the
summary generated using the traditional fusion method, and the
ACF module yields 4.9% higher CIDER scores on the JAL dataset
than using the traditional method. This shows that our cross modal-
ity fusion method can highlight the critical information in each
modality to help generate summaries.

5.5 Qualitative Analysis
Since the statistics do not fully reveal the predictive power of the
model, we further discuss the quality of the summaries generated
by M2TS through qualitative analysis. Table 5 shows the summaries
generated by the different models for the examples in the three
datasets, where the model M2TS (1) denotes the summaries gen-
erated when the scale is set to 1. We chose the NeuralCodeSum
for comparison because this baseline played a key transition role
throughout the study, with subsequent studies MMTrans and SG-
Trans being based on the Transformer. From Table 5, we can observe
that M2TS can generate better descriptions for the given programs.
Specifically, for the example in the JAH dataset, the keyword "gui"
is missing in the results of the baseline NeuralCodeSum and ab-
lation analysis but can be generated accurately by our model. In
addition, for the examples in the PYB and JAL datasets, our model
can generate semantically coherent phrases "every descriptor" and
"current width", which is not possible with other listed baselines.
M2TS generates high-quality summaries for the following reasons:
first, embedding ASTs at multiple scales enables more comprehen-
sive extraction of syntactic structural information of ASTs, then
the fusion of two modalities, ASTs and code tokens, using the ACF
module can learn the contextual semantic relevance of code tokens
while highlighting structural features of source code. Due to the
two main components, the generated summaries are semantically
more fluently compared to NeuralCodeSum and other baselines as
seen in the examples.

6 RELATED WORK
6.1 Source Code Summarization
Source code summarization aims to generate short summaries for
code snippets. Existing code summarization approaches can be
divided into traditional and based on deep learning.

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

Among the traditional methods of source code summarization,
the earliest is the approach based on artificial templates. Sridhara
et al. [14] used the SWUM to create a rule-based approach to gen-
erating summaries for Java methods. Haiduc et al. [15, 16] applied
text retrieval techniques and latent semantic search to select es-
sential keywords from source code and treated these keywords
as comments. Hindle et al. [50] addressed the naturalness of code
languages and proved that source code could be modelled by prob-
abilistic models. Subsequently, Dana et al. [51] combined statistical
probability models such as topic models and n-gram models to
predict comments for Java methods [24].

Since deep learning can automatically learn pattern features
from large-scale data, some studies have experimented with deep
learning-based approaches to generate code summaries [22, 52–54].
Inspired by Neural Machine Translation (NMT), some studies have
looked at code summarization as a variant of the translation task.
Lyer et al. [21] proposed a Seq2Seq model based on attention mech-
anism, where they used token embedding vectors as the output
of the encoder and then used the LSTM as the decoder to gener-
ate summaries. With the advent of code-structured representation
ASTs, some research has begun to revolve around them, with Hu
et al. [17, 23], Zhang et al. [34], and LeClair et al. [25] successively
using ASTs as input to the encoder-decoder model. In addition,
Wan et al. [22] used deep reinforcement learning to enhance code
summarization as the way to address exposure bias during coding
[55]. LeClair et al. [31] used both AST and source code to generate
comments for Java methods, which differ from their work published
in 2019 ICSE [25] in that they studied AST graphs rather than AST
sequences. With the emergence of the Transformer model [29],
Ahmad et al. [30] improved it to generate summaries for both Java
and Python language. Due to the power of the Transformer model,
much work has subsequently emerged around it [37, 49, 56, 57].

Compared with the above works, our approach can comprehen-
sively extract the structural features of source code and fuse code
token and AST features in a cross modality fusion method to obtain
the best performance than the above state-of-the-art methods.

6.2 Graph Convolutional Neural Network
GCN is a graph embedding algorithm used more often in recent
years. Its core idea is the weighted summation of the feature values
of nodes themselves and their neighboring nodes [39], where the
feature value refers to the content information of nodes. Each layer
of graph convolution has two main inputs: the output of the pre-
vious layer of graph convolution and the adjacency matrix. Since
there is no previous layer output when inputting the first layer of
graph convolution, the input is the initialization feature vector of
nodes. Before inputting the GCN, the adjacency matrix needs to be
normalized, and the following is the formula:

^𝐴 = ˜𝐷−1/2 ˜𝐴 ˜𝐷−1/2,
where ˜𝐴 = 𝐴 + 𝐼 , 𝐼 denotes the unit matrix, which is added to
consider the information of the nodes themselves. ˜𝐷 is the degree
matrix of ˜𝐴, and ^𝐴 is the normalized adjacency matrix.

(16)

In this paper, we use the BERT pre-training [40] to embed nodes
and input the embedding vectors as the initial feature vectors of
nodes along with the normalized adjacency matrix into the GCN.
In addition, we add a residual connection to the general GCN to

prevent features loss when performing the next graph convolution
layer.

7 THREATS TO VALIDITY
We have identified the following threats to validity:

• Evaluation method. In this paper, we use four popular
automatic evaluation metrics to assess the similarity of gen-
erated and reference sentences. Although they are compu-
tationally fast and widely used in this field [31], they have
some limitations, such as the valid summary generated by
the model may not be aligned with the reference summary
or the generated summary is aligned with the reference sum-
mary but not readable. Therefore, it is necessary to evaluate
generated summaries from additional perspectives, such as
human evaluation [22, 24].

• Hardware limitations. Our experiments cannot perform
hyper-parameter optimizations of the model due to hardware
limitations [31]. In Table 2, our model or baselines may be
affected by certain hyper-parameters (e.g., learning rate and
batch size), caused different metric scores or rankings. This
is an unavoidable problem in deep learning research. The
hyper-parameters we set when reproducing the work in
baselines should match their descriptions as much as possible
to mitigate this problem [22].

• Dataset type. We use the Java and Python datasets to vali-
date the feasibility of the model in this paper. Although our
proposed multi-scale AST feature extraction method is set
at the same scales in these three datasets, we do not know
whether this setup is equally applicable to other datasets.
Therefore, it is necessary to conduct experiments on more
large datasets (e.g., C, C#) to verify the reliability of this
method.

8 CONCLUSION
In this paper, we propose a multi-scale multi-modal approach based
on the Transformer for source code summarization (M2TS), which
present a multi-scale AST feature extraction method to extract the
structural features of source code more completely and accurately
at multiple local and global levels. In addition, we use a new cross
modality feature fusion method to fuse the encoded AST features
with the code tokens features, highlighting the structural informa-
tion of source code and learning the contextual semantic relevance
between code tokens. Finally, we input the fused features and the
encoded code tokens features into the combined decoder to gener-
ate summaries. In particular, our extensive experiments on two Java
and one Python datasets demonstrate the effectiveness of M2TS,
and the results show that it outperforms the related approaches.

ACKNOWLEDGMENTS
This work is financially supported by the Natural Science Founda-
tion of Shandong Province, China (ZR2021MF059, ZR2019MF071),
National Natural Science Foundation of China (61602286, 61976127)
and Special Project on Innovative Methods (2020IM020100).

M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization

ICPC ’22, May 16–17, 2022, Virtual Event, USA

REFERENCES
[1] Thomas A Corbi. Program understanding: Challenge for the 1990s. IBM Systems

Journal, 28(2):294–306, 1989.

[2] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan-
ping Li. Measuring program comprehension: A large-scale field study with
professionals. IEEE Transactions on Software Engineering, 44(10):951–976, 2017.
[3] Andrew J Ko, Brad A Myers, and Htet Htet Aung. Six learning barriers in end-user
programming systems. In 2004 IEEE Symposium on Visual Languages-Human
Centric Computing, pages 199–206. IEEE, 2004.

[4] Andrew J Ko, Brad A Myers, Michael J Coblenz, and Htet Htet Aung. An ex-
ploratory study of how developers seek, relate, and collect relevant information
during software maintenance tasks. IEEE Transactions on software engineering,
32(12):971–987, 2006.

[5] Thomas D LaToza, Gina Venolia, and Robert DeLine. Maintaining mental models:
a study of developer work habits. In Proceedings of the 28th international conference
on Software engineering, pages 492–501, 2006.

[6] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. Evalu-
ating source code summarization techniques: Replication and expansion. In 2013
21st International Conference on Program Comprehension (ICPC), pages 13–22.
IEEE, 2013.

[7] Paul W McBurney and Collin McMillan. Automatic source code summarization
of context for java methods. IEEE Transactions on Software Engineering, 42(2):103–
119, 2015.

[8] Janice Singer, Timothy Lethbridge, Norman Vinson, and Nicolas Anquetil. An
examination of software engineering work practices. In CASCON First Decade
High Impact Papers, pages 174–188. 2010.

[9] Martin Röscheisen, Michelle Baldonado, Kevin Chang, Luis Gravano, Steven
Ketchpel, and Andreas Paepcke. The stanford infobus and its service layers:
Augmenting the internet with higher-level information management protocols.
Digital Libraries in Computer Science: The MeDoc Approach, pages 213–230, 1998.
[10] Edmund Wong, Taiyue Liu, and Lin Tan. Clocom: Mining existing source code
for automatic comment generation. In 2015 IEEE 22nd International Conference
on Software Analysis, Evolution, and Reengineering (SANER), pages 380–389. IEEE,
2015.

[11] Edmund Wong, Jinqiu Yang, and Lin Tan. Autocomment: Mining question
and answer sites for automatic comment generation. In 2013 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE), pages 562–567.
IEEE, 2013.

[12] Paige Rodeghero, Cheng Liu, Paul W McBurney, and Collin McMillan. An
eye-tracking study of java programmers and application to source code summa-
rization. IEEE Transactions on Software Engineering, 41(11):1038–1054, 2015.
[13] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, and Sidney
D’Mello. Improving automated source code summarization via an eye-tracking
study of programmers. In Proceedings of the 36th international conference on
Software engineering, pages 390–401, 2014.

[14] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-
Shanker. Towards automatically generating summary comments for java methods.
In Proceedings of the IEEE/ACM international conference on Automated software
engineering, pages 43–52, 2010.

[15] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. Supporting program compre-
hension with source code summarization. In 2010 acm/ieee 32nd international
conference on software engineering, volume 2, pages 223–226. IEEE, 2010.
[16] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. On the use of
automated text summarization techniques for summarizing source code. In 2010
17th Working Conference on Reverse Engineering, pages 35–44. IEEE, 2010.
[17] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source
code with transferred api knowledge. In Proceedings of the 27th International
Joint Conference on Artificial Intelligence, pages 2269–2275, 2018.

[18] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. Multi-
scale self-attention for text classification. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 7847–7854, 2020.

[19] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. Retrieve and refine: exemplar-
based neural comment generation. In 2020 35th IEEE/ACM International Confer-
ence on Automated Software Engineering (ASE), pages 349–360. IEEE, 2020.
[20] Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. Exploiting method names to
improve code summarization: A deliberation multi-task learning approach. arXiv
preprint arXiv:2103.11448, 2021.

[21] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Sum-
marizing source code using a neural attention model. In Proceedings of the 54th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 2073–2083, 2016.

[22] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S Yu. Improving automatic source code summarization via deep reinforce-
ment learning. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, pages 397–407, 2018.

[23] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In
2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC),

pages 200–20010. IEEE, 2018.

[24] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation
with hybrid lexical and syntactical information. Empirical Software Engineering,
25(3):2179–2217, 2020.

[25] Alexander LeClair, Siyuan Jiang, and Collin McMillan. A neural model for gener-
ating natural language summaries of program subroutines. In 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE), pages 795–806. IEEE,
2019.

[26] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating se-
quences from structured representations of code. arXiv preprint arXiv:1808.01400,
2018.

[27] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural

computation, 9(8):1735–1780, 1997.

[28] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase repre-
sentations using rnn encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In Advances in neural information processing systems, pages 5998–6008, 2017.
[30] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
A transformer-based approach for source code summarization. arXiv preprint
arXiv:2005.00653, 2020.

[31] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. Improved
In Proceedings of the 28th

code summarization via a graph neural network.
International Conference on Program Comprehension, pages 184–195, 2020.
[32] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. Retrieval-
based neural source code summarization. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE), pages 1385–1397. IEEE, 2020.
[33] Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei, Xiaoxue Ma,
and Miao Zhang. A multi-modal transformer-based code summarization approach
for smart contracts. arXiv preprint arXiv:2103.07164, 2021.

[34] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. A novel neural source code representation based on abstract syntax tree.
In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),
pages 783–794. IEEE, 2019.

[35] Alexander LeClair and Collin McMillan. Recommendations for datasets for source

code summarization. arXiv preprint arXiv:1904.02660, 2019.

[36] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim
Sheinin. Graph2seq: Graph to sequence learning with attention-based neural
networks. arXiv preprint arXiv:1804.00823, 2018.

[37] Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, and Xin Xia.
Code structure guided transformer for source code summarization. arXiv preprint
arXiv:2104.09340, 2021.

[38] Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. Integrating tree path
in transformer for code representation. Advances in Neural Information Processing
Systems, 34, 2021.

[39] Thomas N Kipf and Max Welling. Semi-supervised classification with graph

convolutional networks. arXiv preprint arXiv:1609.02907, 2016.

[40] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.

[41] Junke Wang, Zuxuan Wu, Jingjing Chen, and Yu-Gang Jiang. M2tr: Multi-modal
multi-scale transformers for deepfake detection. arXiv preprint arXiv:2104.09770,
2021.

[42] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. Treegen:
A tree-based transformer architecture for code generation. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 34, pages 8984–8991, 2020.
[43] Antonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python
functions and documentation strings for automated code documentation and
code generation. arXiv preprint arXiv:1707.02275, 2017.

[44] Herbert Robbins and Sutton Monro. A stochastic approximation method. The

annals of mathematical statistics, pages 400–407, 1951.

[45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method
for automatic evaluation of machine translation. In Proceedings of the 40th annual
meeting of the Association for Computational Linguistics, pages 311–318, 2002.

[46] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text

summarization branches out, pages 74–81, 2004.

[47] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evalua-
tion with improved correlation with human judgments. In Proceedings of the acl
workshop on intrinsic and extrinsic evaluation measures for machine translation
and/or summarization, pages 65–72, 2005.

[48] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-
based image description evaluation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 4566–4575, 2015.

[49] Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu
Zhang, and Dongmei Zhang. Cocosum: Contextual code summarization with
multi-relational graph neural network. arXiv preprint arXiv:2107.01933, 2021.

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Gao, et al.

[50] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
On the naturalness of software. Communications of the ACM, 59(5):122–131, 2016.
[51] Dana Movshovitz-Attias and William Cohen. Natural language models for pre-
dicting programming comments. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 35–40,
2013.

[52] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and
Tadayuki Matsumura. Automatic source code summarization with extended
tree-lstm. In 2019 International Joint Conference on Neural Networks (IJCNN),
pages 1–8. IEEE, 2019.

[53] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks
over tree structures for programming language processing. In Thirtieth AAAI
Conference on Artificial Intelligence, 2016.

[54] Paul W McBurney and Collin McMillan. Automatic documentation generation
via source code summarization of method context. In Proceedings of the 22nd
International Conference on Program Comprehension, pages 279–290, 2014.
[55] Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip Yu,
and Guandong Xu. Reinforcement-learning-guided source code summarization
via hierarchical attention. IEEE Transactions on software Engineering, 2020.
[56] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
Günnemann. Language-agnostic representation learning of source code from
structure and context. arXiv preprint arXiv:2103.11318, 2021.

[57] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint arXiv:2002.08155,
2020.

