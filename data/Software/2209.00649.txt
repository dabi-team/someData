Addressing Hidden Imperfections in Online Experimentation

Jeﬀrey Wong
Apple

Jasmine Nettiksimmons
Apple

Jiannan Lu
Apple

Katherine Livins
Apple

2
2
0
2

g
u
A
5
2

]
E
S
.
s
c
[

1
v
9
4
6
0
0
.
9
0
2
2
:
v
i
X
r
a

1 Introduction

Randomized controlled trials (RCTs) have long been
the gold standard for causal inference in scientiﬁc ﬁelds,
particularly in biomedicine (Rubin 2008; Imbens and
Rubin 2015). However, it is well known that in many
cases the ideal RCT cannot be conducted. For example,
some patients in medical trials pass away or become
otherwise untrackable (Rubin 2006), and others may
not fully comply with their assigned treatments (Efron
and Feldman 1991; Frangakis and Rubin 2002; Jin and
Rubin 2008).

Technology companies are increasingly using RCTs
as part of their development process. Despite hav-
ing ﬁne control over engineering systems and data in-
strumentation, these RCTs can still be imperfectly ex-
ecuted.
In fact, online experimentation suﬀers from
many of the same biases seen in biomedical RCTs in-
cluding opt-in and user activity bias (Wang et al. 2019),
selection bias (Dasgupta et al. 2012; Dmitriev et al.
2016; Xie et al. 2021), non-compliance with the treat-
ment (Deng and Hu 2015), and more generally, chal-
lenges in the ability to test the question of interest
(Gupchup et al. 2018). The result of these imperfec-
tions can lead to a bias in the estimated causal eﬀect,
a loss in statistical power, an attenuation of the eﬀect,
or even a need to reframe the question that can be an-
swered.

This paper aims to make practitioners of experi-
mentation more aware of imperfections in technology-
industry RCTs, which can be hidden throughout the
engineering stack or in the design process. We recom-
mend designers of experiments to be vigilant and iterate
together with product and user experience designers to
reconcile learning goals with minimization of burden on
end consumers. We will demonstrate the need for ma-
ture thought with a recurring example. As we describe
the challenges, we will discuss what is possible to infer
from a RCT using a diﬀerence of means, and whether
the original business question can be answered. Later,
we will show an analytics strategy to minimize any gaps
between the two. We oﬀer practical guidance on how
to

1. Design and scope the experiment.

2. Instrument the experimentation funnel.

3. Proactively monitor imperfections in measure-

ment.

4. Adjust statistical analysis of an experiment to mit-

igate imperfections.

These concepts will be illustrated with a running ex-
ample that assumes on-device treatment assignment.
This scenario has broad challenges that many prac-
titioners may be unfamiliar with. Server-side experi-
ments suﬀer from some of the same imperfections and
can also beneﬁt from these guidelines.

2 Designing and Scoping the

Experiment

Experimenters need to be vigilant and thoughtful in
experiment design. Conclusions from experiments can
suﬀer from opt-in bias, selection bias, non-compliance
with the treatment, or eﬀect attenuation. To start a
rigorous experiment, we must ﬁrst frame a business
question with an appropriate target. Afterwards, the
designer of the experiment must think carefully about

1. How will the users’ experience be randomized?

2. How will a user trigger that randomized experi-

ence?

3. What is the broader target population, and how

will a subset of users enter the experiment?

4. Are there any mechanisms that create an unequal
randomization in the treatment and control expe-
riences?

Multiple iterations across teams may be required on
these questions. Experiment design, user experience
design, and business goals can conﬂict with each other,
and the need to balance them may lead to experiments
that are diﬃcult to conclude. Natural limitations in
engineering and design can create an imperfect experi-
ment that aﬀects the scope of the conclusion, and the
generalizability of the results.

First, we must start with a question. Throughout
this paper we will rely on a recurring example about an
online store that sells products in a mobile app. The
product pages have images, and the store has released
an update to their app that enables the product pages

1

 
 
 
 
 
 
to include video. The question is simply “How does the
new video for product X aﬀect sales?”

Given this background, the designer of the experi-
ment must design a randomized controlled trial to un-
derstand the causal eﬀect of the video on sales. The
naive design is to randomly show the video to a frac-
tion of the online traﬃc, then compute sales metrics
for users that watched the video and those who did not
watch the video. The analysis would take the diﬀer-
ence in the means of sales and report that as the causal
eﬀect of the video. While this seems like a reasonable
experiment, there are many hidden imperfections
that aﬀect the scope and validity of the results.
It is the designer’s duty to state these imperfections,
and to scope the results of the experiment clearly.

Imperfections arise when studying how a user gets to
see the video. It is discovered that a series of events
must happen:

1. The user must be on the latest version of the app.

2. If the video feature is enabled, the app must down-
load a conﬁguration onto the device, allowing it to
arrange the product pages. It is possible that this
download fails.

3. The user must land on the product page for X.

4. On the product page, the user will see a video
player. They must click on it in order to play it.

Among the imperfections are (1) the possibility that
users do not play the video, (2) selection bias due to the
treatment conﬁguration failing to download, while the
control is conﬁgured without failure, and (3) changes
in the types of users that land on the product page.
Simply computing a diﬀerence in means for everyone
that was in the experiment, regardless of playing the
video, will decrease statistical power, and simply re-
moving users assigned to the treatment that did not
watch the video, or could not download the treatment
conﬁguration successfully, will result in bias.
In the
next sections, we will show how randomizing, trigger-
ing, and entering the experiment aﬀect the scope of the
conclusion. Planning and analytical support are needed
to address the imperfections and have a robust conclu-
sion.

2.1 Randomizing the Experiment

We are tempted to report the eﬀect of the video by
taking the diﬀerence between users who have played
the video, and users who did not. However, this is not
a randomized partition of users that the experiment de-
signer can create apriori. Instead, the app can only use
randomization to control the availability of the video.
We deﬁne this as the intent to treat (ITT), which can
be reported using a diﬀerence in means without bias

2

as long as the randomized assignment was logged inde-
pendently of whether the user successfully downloaded
the video conﬁguration. When analyzing the ITT ef-
fect, the scope of the conclusion should be qualiﬁed
to “the eﬀect of making the video available is ...”, in-
stead of “the eﬀect of watching the video is ...”. The
designer should clearly raise this diﬀerence. Later, we
will show that despite this challenge in experiment de-
sign, it is possible to approximate the treatment eﬀect
from watching the video using propensity weights.

2.2 Triggering the Experience

The video in our example is only relevant for users that
landed on the product page for X, an event known as
the trigger (Kohavi et al. 2009). Users that did not
land on the product page for X are irrelevant for the
study, because it is not possible for the video to cause
a change in sales. Including these users would attenu-
ate the eﬀects the business cares about, and the inabil-
ity to identify these users is an imperfection of the ex-
periment. Scoping the analysis to only triggered users
allows experimenters to focus the impact analysis on
users that would be impacted. While improving statis-
tical power, restricting to the triggered population al-
ters the scope of the conclusion to “Among users that
viewed the product page for X, the eﬀect of making the
video available is ...”.

2.3 Biases from Entering the Experi-

ment

An experiment makes inference about a broader tar-
get population, and a subset of users enter the exper-
iment in order to gather data. We must examine how
someone enters in order to tell if the sample is repre-
sentative of the target. There are two important gating
mechanisms: users must be on the latest version of the
app, and treatment users’ apps must download a con-
ﬁguration ﬁle onto their device. Each has a diﬀerent
impact on the conclusion of the experiment. For the
ﬁrst, the users on the most recent version may not be
representative of the target population; they may skew
towards early adopters or more active users. There-
fore, results may not generalize and lack external valid-
ity. Within this skew, randomization can still partition
these users equally, so scoping inference to users that
entered the experiment is still internally valid. If skew
exists, then the scope of the conclusion must be quali-
ﬁed with “Among users that were already on the most
recent version of the app...”.

The second gating mechanism induces an activation
bias that is more dangerous than the ﬁrst, and adds
additional complexity to the intent to treat and trig-
ger analysis. Failing to download the conﬁguration is a
one-sided bias, because the control group does not need
video conﬁguration. This is a threat to internal validity.

Additionally, the failure probability could be correlated
with sales, for example both may be a function of hav-
ing a strong WiFi signal. This correlation means that a
comparison between the the subset of treatment users
that activated, and all control users, is likely to be con-
founded. To avoid confounding, we must be able to
identify users that entered the experiment even if they
did not download the conﬁguration; if we can only iden-
tify users after they download the conﬁguration, then
there is an activation bias. The consequence of this
bias changes the scope of the question from “What is
the eﬀect of users having the ability to interact with a
video on product X’s page?” to “What is the eﬀect of
enabling a user’s app to download a conﬁguration ﬁle
to enable videos on product X’s page?”.

2.4 Scoping the Conclusion

Finally, combining all the qualiﬁcations together, the
most precise answer to the business question is either:

1. “Among current users on the most recent version
of the app who landed on the product page for
X, the intent to treat eﬀect of making the video
available is ...”.

2. “Among current users on the most recent version
of the app, the intent to treat eﬀect of enabling a
user’s app to download video conﬁguration is ...”.

Designing an experiment requires several iterations.
During the process, it is important to know what we
can conclude, and what we cannot. The result here
is internally valid for a limited target population that
could have entered the experiment. Naturally, the ex-
perimenter will wonder how to generalize the result out-
side of the experiment. To do this, we must instrument
a well structured experiment funnel.

3 Instrumenting the Experimentation

Funnel

Keeping track of the various diﬀerent scopes for the
conclusion is hard. Instrumenting a funnel allows us to
methodically construct what is safe to infer and what is
not. The funnel contains ﬁve diﬀerent stages below, and
is later used to measure diﬀerent eﬀects in an unbiased
way. We recommend recording all ﬁve stages.

3

1. The targeted population is the pool of users the
experimenters wish to study. These would be cur-
rent users of the latest version of the mobile app.

2. The allocated population is a subset of the tar-
geted population, containing users who are ran-
domly drawn to be part of an experiment. These
would be current users of the latest version of the
mobile app that were randomly selected to enter
the experiment.
It is agnostic to the treatment
and control experiences. All allocated users are as-
signed into a treatment group or a control group.

3. The activated population is a subset of the allo-
cated population, containing users who have suc-
cessfully received the experiment conﬁguration ap-
propriate for their assignment. These would be
users that were able to pull the video conﬁgura-
tions successfully.

4. The triggered population is a subset of the allo-
cated population, but does not need to be a subset
of the activated population. Triggered users have
have met all the criteria to be treated. These would
be users that landed on the product page for X. It
is agnostic to the treatment and control experi-
ences. The app will intend to treat all triggered
users, referencing their assignment.

5. The treated population is a subset of the acti-
vated and triggered population, containing users
who have experienced their assigned treatment.

The illustration of the funnel above shows that some-
one who is assigned to the treatment can progress
through the funnel diﬀerently than someone who is as-
signed to the control. Therefore, it is crucial to log all
ﬁve stages in order to have a balanced comparison for
the analysis of causal eﬀects.

The analysis population shrinks going down this fun-
nel. When planning statistical power,
it should be
noted that power is a function of the number of treated
users, not the number of allocated users.

Using this funnel,

important rates that aﬀect the

treatment eﬀect can be calculated.

1. Allocation rate is the percent of targeted users

that allocated.

2. Activation rate is the percent of allocated users

that activated.

3. Trigger rate is the percent of allocated users that

triggered.

4. Compliance rate is the percent of users who were
both assigned to treatment and triggered that are
treated.

Monitoring these rates is important as they can
change the result of the experiment. For example, if
either the trigger rate or the compliance rate declines,
then the ITT eﬀect will be attenuated and the vari-
ance will increase, making it harder to detect an eﬀect.
If the rate ﬂuctuates over time, it can make results
that were signiﬁcant become not signiﬁcant. The trig-
ger rate is also important for the practicality of the new
treatment. Suppose the new video in our app generates
a 10% increase in sales among users that land on the
product page for X. But only 1% of customers land on
that product page, then the total eﬀect on sales is a
more humble 0.1%.

4 Monitoring Imperfections

The many hidden imperfections discussed in this pa-
per are challenging because the experimenter may not
be aware of them, and the imperfections may not be
avoidable. Experimentation teams need to be prepared
to balance experiment design with user experience de-
sign. One way to proactively monitor the health of the
experiment design is to employ sample ratio mismatch
(SRM) checks (Crook et al. 2009) throughout the exper-
imentation funnel, and verify whether the funnel rates
are balanced in treatment and control groups. This
should be done on the allocation, activation, and trig-
ger steps in the experimentation funnel. In particular, if
SRM fails in the trigger, we should investigate whether
the treatment feature has performance issues that can
change user engagement. If SRM fails in the activation,
we should check the size of the conﬁguration ﬁle, which
may cause failed downloads.

Within the experimentation funnel, the trigger event
is the most important event because it occurs immedi-
ately before the randomized assignments go into aﬀect.
Reporting the intent to treat eﬀect among the triggered
population, including users that did not activate, is al-
ways internally valid, even when other eﬀects need to
be scrutinized. This is because the triggered population
is a child of the allocated population, neither of which
are unequally aﬀected by the assignment.

5 Adjusting Statistical Analysis

The experiment funnel with SRM checks guarantees
that the ITT eﬀect among triggered users can be an-
alyzed using a diﬀerence in means. While safe, infer-

ence on the triggered population is highly inﬂuenced by
the trigger rates. This intent to treat eﬀect also does
not show the eﬀect of playing the video. We can de-
rive other types of eﬀects using methods inspired by
propensity weights (Rosenbaum and Rubin 1983).

that also trigger

treatment
(cid:80)

The ITT eﬀect among users allocated into the
i∈T, T yi −
1
i∈C, T yi where T,T is the set of users assigned
nC,T
to the treatment that triggered, and C,T is the set of
users assigned to the control that triggered.

1
nT,T

(cid:80)

is

We may wish to report the ITT eﬀect for a diﬀer-
ent reference distribution of users, not necessarily the
distribution among triggered users. Say there are two
groups of users, labeled as g1 and g2, which are deﬁned
by a feature vector. The ratio between g1 and g2 users
in the reference may be 1:2, but 1:10 among triggered
users, caused by trigger rates of 10% and 50% respec-
tively. Analysis of the triggered population does not
reﬂect the reference population well due to the low rep-
resentation of g1 users. Through product changes, the
business may wish to know what the ITT eﬀect would
be if the ratio of triggered g1 to g2 users was the same
as the ratio in the allocation or target populations. Let
T (gi) be the probability that an allocated user reaches
the trigger, and T (cid:48)(gi) be a new trigger rate. The ITT
eﬀect with a new trigger rate will be

1
nT, T’

(cid:88)

i∈T, T

yiT (cid:48)(gi)
T (gi)

−

1
nC, T’

(cid:88)

i∈C, T

yiT (cid:48)(gi)
T (gi)

.

The experimenter

In the simple case where T (cid:48)(gi) = 1, the ITT eﬀect is
eﬀectively measured against the allocation distribution.
should pay attention to the
broader target population. While the funnel guides us
to analyze the trigger population, the business goal is
to make inference on the target population. We can re-
purpose the previous formula by replacing trigger rates
with allocation rates.

Finally, instead of computing an ITT eﬀect based
on assignment, we may want to measure the eﬀect of
playing the video among triggered users. We compute

1
nT,T + nC,T

(cid:88)

i∈Triggered

wiyi
p(gi)

−

(1 − wi)yi
1 − p(gi)

where wi is a ﬂag for whether a user played the video,
and p(gi) is the propensity for the user to play the video.

6 Conclusion

We have illustrated that the ideal experiment is not al-
ways possible, even in online AB testing with its ﬁne
grained control over most aspects of the system. With
our guidelines, we provide a way to communicate what
is safe to conclude, and what is not. In our example,
answering the desired question: “what is the impact

4

of product videos on sales” among the target popula-
tion can be restricted by selection bias and treatment
compliance. When no activation bias exists, a safe and
precise conclusion could be “among current users on
the most recent version of the app who landed on the
product page for X, the intent to treat eﬀect of making
the video available is ...”.

Awareness of imperfections and the ability to ad-
dress them hinges on logging the full experiment funnel.
Monitoring funnel rates with SRMs is crucial; an exper-
iment designer cannot take it on faith that an experi-
ment is executed perfectly. Contextualizing the broader
impact of the treatment given the trigger and compli-
ance rates also relies on the funnel. Without the funnel,
activation bias can force us to draw conclusions about
the eﬀect of a user’s app being allowed to display videos
rather than the eﬀect of a user being allowed to engage
with a video. By using reweighting and the funnel, we
can make conclusions on the original allocation or tar-
get populations, and approximate the eﬀect of playing
the video.

We recommend experiment designers go through the
exercise of discussing the target population, how users
enter the experiment, how they trigger the experiment,
whether the treatment is random or requires user in-
put, and whether there are any diﬀerential biases. Af-
terwards, the experiment funnel and logging steps can
be constructed. Having this structure will enable re-
sponsible and well scoped conclusions.

References

[Cro+09] Thomas Crook et al. “Seven pitfalls to
avoid when running controlled experiments
on the web”. In: Proceedings of the 15th
ACM SIGKDD international conference
on Knowledge discovery and data mining.
2009, pp. 1105–1114.

[Das+12] Anirban Dasgupta et al. “Overcoming
browser cookie churn with clustering”. In:
Proceedings of the ﬁfth ACM international
conference on Web search and data mining.
2012, pp. 83–92.

[DH15]

Alex Deng and Victor Hu. “Diluted treat-
ment eﬀect estimation for trigger analysis
in online controlled experiments”. In: Pro-
ceedings of the Eighth ACM International
Conference on Web Search and Data Min-
ing. 2015, pp. 349–358.

[Dmi+16] Pavel Dmitriev et al. “Pitfalls of long-term
online controlled experiments”. In: 2016
IEEE international conference on big data
(big data). IEEE. 2016, pp. 1367–1376.

[EF91]

[FR02]

[Gup+18]

[IR15]

[JR08]

Bradley Efron and David Feldman. “Com-
pliance as an explanatory variable in clini-
cal trials”. In: Journal of the American Sta-
tistical Association 86 (1991), pp. 9–17.

Constantine E Frangakis and Donald B
Rubin. “Principal stratiﬁcation in causal
58 (2002),
In: Biometrics
inference”.
pp. 21–29.

Jayant Gupchup et al. “Trustworthy ex-
perimentation under telemetry loss”. In:
Proceedings of the 27th ACM International
Conference on Information and Knowledge
Management. 2018, pp. 387–396.

Guido W Imbens and Donald B Rubin.
Causal inference in statistics, social, and
biomedical sciences. Cambridge University
Press, 2015.

Hui Jin and Donald B Rubin. “Principal
stratiﬁcation for causal inference with ex-
tended partial compliance”. In: Journal of
the American Statistical Association 103
(2008), pp. 101–111.

[Koh+09] Ron Kohavi et al. “Controlled experiments
on the web: survey and practical guide”.
In: Data mining and knowledge discovery
18.1 (2009), pp. 140–181.

[RR83]

[Rub06]

[Rub08]

Paul R Rosenbaum and Donald B Rubin.
“The central role of the propensity score
in observational studies for causal eﬀects”.
In: Biometrika 70 (1983), pp. 41–55.

“Causal

Donald B Rubin.
inference
through potential outcomes and principal
stratiﬁcation: application to studies with”
censoring” due to death”. In: Statistical
Science (2006), pp. 299–309.

Donald B Rubin. “For objective causal in-
ference, design trumps analysis”. In: The
Annals of Applied Statistics 2 (2008),
pp. 808–840.

[Wan+19] Yu Wang et al. “On heavy-user bias in
a/b testing”. In: Proceedings of the 28th
ACM International Conference on Infor-
mation and Knowledge Management. 2019,
pp. 2425–2428.

[Xie+21]

Yuxiang Xie et al. “How to Measure Your
App: A Couple of Pitfalls and Remedies
in Measuring App Performance in Online
Controlled Experiments”. In: Proceedings
of the 14th ACM International Conference
on Web Search and Data Mining. 2021,
pp. 949–957.

5

