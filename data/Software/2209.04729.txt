The Switch from Conventional to SDN: The Case
for Transport-Agnostic Congestion Control

Ahmed M. Abdelmoniem and Brahim Bensaou

1

2
2
0
2

p
e
S
0
1

]
I

N
.
s
c
[

1
v
9
2
7
4
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—To meet

the timing requirements of

interactive
applications, the no-frills congestion-agnostic transport protocols
like UDP are increasingly deployed side-by-side in the same
network with congestion-responsive TCP. In cloud platforms,
even though the computation and storage is totally virtualized,
they lack a true virtualization mechanism for the network (i.e.,
the underlying data centers networks). The impact of such lack
of isolation services, may result into frequent outages (for some
applications) when such diverse trafﬁcs contend for the small
buffers in the commodity switches used in data centers. In
this paper, we explore the design space of a simple, practical
and transport-agnostic scheme to enable a scalable and ﬂexible
end-to-end congestion control in data centers. Then, we present
the the shortcomings of coupling the monitoring and control
of congestion in the conventional system and discuss how a
Software-Deﬁned Network (SDN) would provide an appealing
alternative to circumvent the problems of the conventional system.
The two systems implements a software-based congestion control
mechanisms that perform monitoring, control decisions and
trafﬁc control enforcement functions. Both systems are designed
with a major assumption that the applications (or transport
protocols) are non-cooperative with the system, ultimately mak-
ing it deployable in existing data centers without any service
disruption or hardware upgrade. Both systems are implemented
and evaluated via simulation in NS2 as well as real-life small-scale
test-bed deployment and experiments.

Index Terms—Congestion Control, Data Center Networks,
Rate Control, Open vSwitch, Software Deﬁned Networks, Vir-
tualization

I. INTRODUCTION

To achieve isolation among tenants and use resources more
efﬁciently, resource virtualization has become a common prac-
tice in today’s public data centers. In most cases, each tenant
is provisioned with virtual machines assigned with dedicated
virtual CPU cores, memory, storage, and a virtual network
interface card (NIC) that sends trafﬁc over the underlying shared
physical NIC. Typically, tenants can not assume predictability
nor measurability of bounds on network performance, as no
mechanisms are deployed to explicitly allocate and enforce
bandwidth in the cloud. Nevertheless, cloud operators can
provide tenants with better virtual network management thanks
to the recent developments in control plane functions. For
example, Amazon introduced “Virtual Private Cloud (VPC)”
[3] to allow easy creation and management of tenant’s private
virtual network. VPC can be viewed as an abstraction layer

Ahmed M. Abdelmoniem is currently with School of EECS, Queen Mary
University of London and CS Department, Assiut University, Egypt. E-mail:
ahmed.sayed@qmul.ac.uk

Brahim Bensaou is currently with Department of Computer Science and

Engineering, HKUST, Hong Kong SAR, PRC.

This work combines works published in IEEE ICC 2016 [1] and IEEE LCN

2017 [2].

running on top of the non-isolated, shared network resources of
AWS’ public cloud. Additionally, Software Deﬁned Networking
(SDN) [4] has been effectively deployed to drive inter- and
intra-data center communications with added features to make
the virtualization and other network aspects easy to manage. For
example, both Google [5] and Microsoft [6] have deployed fully
operational SDN-based WAN networks to support standard
routing protocols as well as centralized trafﬁc engineering
between their data centers.

In contrast, the data plane in intra-datacenter networks
has seen little progress in managing bandwidth to overcome
congestion, improve efﬁciency, and apportioning it adequately
to provide isolation between competing tenants to meet their
target performance requirements. In principle, isolation can
simply be achieved through static reservation [7–11], where
tenants can enjoy a predictable, congestion-free network
performance. However, static reservations lead to inefﬁcient
utilization of the network capacity. To avoid such pitfall, tenants
should be assigned a minimum bandwidth using the so-called
hose model [12] which abstracts the network between the VMs
of one tenant as a single virtual switch (vSwitch). In such
setup, different VMs may reside on any physical machine in
the datacenter, yet, each VM should be able to send trafﬁc at the
full virtual port rate as determined by the vSwitch abstraction
layer. Such VMs should enjoy the allocated rate regardless of
the trafﬁc patterns of co-existing VMs and/or the nature of the
workload generated by competing VMs.

The following are the necessary elements that can be

incorporated together for this purpose:

• An intelligent and scalable VM admission mechanism
within the datacenter for VM placement where minimum
bandwidth is available. To facilitate this, topologies with
bottlenecks at the core switches (such as uplink over-
subscription or a low bisection bandwidth) should be
avoided if possible.

• A methodology to fully utilize the available high bisection
bandwidth (e.g., a load balancing mechanism and/or multi-
path transport/routing protocols).

• A rate adaptation technique to ensure conformance of
VM sending rates to their allocated bandwidth, while
penalizing misbehaving ones.

A number of interesting research works have investigated
more or less successfully the ﬁrst two elements of this frame-
work [13–25]. In [13, 14], highly scalable network topologies
offering a 1:1 over-subscription and a high bisection bandwidth
were proposed. These topologies are shown to be easily
deployable in practice and can simplify the VM placement at
any physical machine with sufﬁcient bandwidth to support the

 
 
 
 
 
 
VM. Efﬁcient routing and transport protocols [15, 16] were
designed for DCN to achieve a high utilization of the available
capacity. Finally, in terms of trafﬁc control, much of the recent
work [26, 27] focused on restructuring TCP congestion control
and its variants to efﬁciently utilize and fairly share bandwidth
among ﬂows (in homogeneous deployments). However, these
techniques fall short of providing true isolation among tenants
(e.g., a tenant may gain more bandwidth by opening parallel
connections or by using aggressive transport protocol like
UDP). It is common, in multi-tenant environments, that non-
homogeneous transport protocols co-exist leading to starvation
of the cooperative ones [28]. To illustrate this problem simply,
we conduct a set of simulations in which we compare the
performance of a tagged ECN-enabled TCP (NewReno) ﬂow
that competes head-to-head i) against another TCP ﬂow of
the same type, ii) against another ﬂow using a TCP variant
designed for data centers (i.e., DCTCP which is deployed in a
number of private data centers [29]), and; iii) against another
congestion-agnostic transport protocol (i.e., UDP which is used
in MemCacheD clusters of Facebook [30]); the results from
the three experiments are superimposed and presented on the
same graphs. Similar to what was already known from the
Internet, Figure 1 shows that, homogeneous TCP deployments
in data centers can achieve fairness, in contrast to heterogeneous
deployments. In particular, we observe in Figure1a that when
TCP is not responsive to ECN markings, it enjoys almost ≈ 0%
its share during the periods where it competes against DCTCP
or UDP. In contrast, being responsive to ECN markings helps
TCP improve its performance but still ultimately the fairness is
not achieved. In Figure 1b, we observe that even when TCP is
ECN-enabled it still loses ≈ 60% and ≈ 72% of its fair share
to DCTCP and UDP, respectively.

In this paper, we rely on the SDN capability of most data
center switching devices to propose a generic congestion control
(SDN-GCC) mechanism to address this issue. We ﬁrst introduce
the idea behind SDN-GCC in Section III, then discuss our
proposed methodology and present SDN-GCC framework in
Section VI. We show via ns2 simulation how SDN-GCC
achieves its requirements with high efﬁciency in Section VIII,
then present testbed experiments in Section IX1. Finally, we
conclude the paper in Section XI.

II. TRANSPORT ISOLATION PROBLEM

With the recent introduction of a signiﬁcant number of
new transport protocols designed for DC networks in addition
to the existing protocols,
the following three challenges
emerged: i) most such protocols are agnostic to the nature
of the VM aggregate trafﬁc demands leading to inefﬁcient
distribution of the network capacity among competing VMs
(for instance a VM could gain more bandwidth by opening
parallel TCP connections); ii) many versions of TCP co-
exist in DC networks (e.g., TCP NewReno/MacOS, compound
TCP/Windows, Cubic TCP/Linux, DCTCP/Linux, and so on),
leading to further inefﬁciency in addition to unfairness, and; iii)

1Simulation and implementation code can be requested from the authors
or downloaded from the following repository after they are made publicly
available: http://github.com/ahmedcs/SDN-GCC

2

(a) ECN disabled

(b) ECN enabled

Figure 1: The instantaneous and mean goodput of the tagged TCP
ﬂow (in the legends) while competing with 3 TCP, DCTCP
or UDP senders. The link capacity is 1Gbps. In interval
[0,10] only the competitors are active, in [10,20] all the
ﬂows are active and in [20,30] only the tagged TCP ﬂow
is active.

many DC applications rely on UDP to build custom transport
protocols (e.g., [30]), that are not responsive to congestion
signals, which exacerbate the unfairness to the point of causing
starvation to congestion-responsive ﬂows. While such problems
have been revealed in the context of Internet communications,
two decades ago, recent studies [28, 29] have conﬁrmed that
such problems of unfairness and bandwidth inefﬁciency also
exist in DCNs despite their characteristically small delays,
small buffers and different topologies from those found in the
Internet. As a consequence, a new solution to the problems of
congestion in DC networks is needed. Such solution must be
attractive to cloud operators and cloud tenants alike.

In particular, with the emergence of software deﬁned
networking, we see an opportunity to invoke the powerful
control features and the global scope provided by SDN to
revisit the problem from a different perspective, with additional
realistic design constraints. As such we propose a solution with
the following intuitive design requirements: R1) Simplicity:
to be readily deployable in existing production data centers;
R2) Transport-agnosticism: to be effective regardless of the
transport protocol; R3) Transparency: requires no changes to
the tenant’s OS (in the VM) and makes no assumption of
any advanced network hardware capability other than those
available in commodity SDN switches; R4) Load-effectiveness:
creates a minimal processing overhead on the end-host.

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (416)DCTCP (319)UDP (153)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (416)DCTCP (375)UDP (359)All of today’s communication infrastructure from hardware
devices to communication protocols have been designed with re-
quirements derived from the global Internet. As a result to cope
with scalability and AS autonomy, the decentralized approach
has been adopted, relinquishing all intelligence to end systems.
Yet, to enable responsiveness to congestion regardless of the
transport protocol capabilities, in time-scales that commensurate
with data center delays, it is preferable to adopt centralized
control as it provides a global view of congestion and is
known to achieve far better performance [31, 32]. Nevertheless
to reconcile existing hardware and protocols (designed for
distributed networks) with the centralized approach, we impose
design requirements R1-R4 on SDN-GCC. As such the core
design of SDN-GCC relies on outsourcing the congestion
control decisions to the SDN controller while the enforcement
of such decisions is carried out by the end-hosts hypervisors.

III. INTRODUCTION TO HYGENICC

To enable responsiveness to congestion regardless of the
transport protocol, one needs to return to the fundamentals
and put the burden of congestion control in principle where it
belongs: in the network layer. As such, in principle, such
congestion control mechanism must be transparent to the
transport layer protocol. However, to reconcile the principle
with the practice, design requirements R1-R4 must be fulﬁlled
and thus HyGenICC outsources its congestion control building
blocks to the hypervisor.

To meet requirement R1, HyGenICC can be implemented
either as a hypervisor-level shim-layer or as an added feature
to any of the current commercial virtual switches’ data-path
module. The job of the added shim-layer to the hypervisor
is to enforce per-VM rate control without VM cooperation
nor any knowledge about its trafﬁc patterns, workloads, or
used transport protocol (TCP/UDP). To this end, HyGenICC
maintains a rate allocation mechanism at each server to partition
the available uplink bandwidth among VMs locally at the
sending and receiving servers. In each such server, HyGenICC
only needs to maintain state information per VM which meets
design requirement R4. HyGenICC deploys a simple hypervisor-
to-hypervisor (IP-to-IP) congestion control mechanism that
relies on ECN markings (readily available in commodity
switches) to infer core network congestion. HyGenICC operates
at the IP level and does not interact directly with the VMs,
which meets requirements R1, R2 and R3. In addition, when
detecting a highly congested path in the core network towards a
destination (via ECN), HyGenICC performs admission control
by refraining from accepting any further connections to this
destination VM until the congestion subsides. Our design is
highly scalable, responsive, work conserving and since it is IP
based, it enforces the allocated bandwidth even in the presence
of highly dynamic and changing trafﬁc patterns and transport
protocols. The rate allocator resolves the contention among
tens-to-hundreds of co-located VMs at the servers, while the
congestion control mechanism addresses the contention in the
network core and pushes it back to the sources. HyGenICC
also allows administrators to assign per-VM weights which
directly affect the bandwidth reservation for the VMs making

3

Table I: Flow attributes and variables tracked in our mechanism

Entry name (VM-to-VM)
𝑠𝑜𝑢𝑟 𝑐𝑒
𝑑𝑒𝑠𝑡
𝑜𝑢𝑡 𝑝𝑎𝑐𝑘𝑒𝑡 𝑐𝑜𝑢𝑛𝑡
𝑖 𝑝𝑟 𝑝𝑎𝑐𝑘𝑒𝑡 𝑐𝑜𝑢𝑛𝑡
𝑒𝑐𝑛 𝑝𝑎𝑐𝑘𝑒𝑡 𝑐𝑜𝑢𝑛𝑡

Variable name (per VM)
𝑟 𝑎𝑡𝑒
𝑏𝑢𝑐𝑘𝑒𝑡
𝑡𝑜𝑘𝑒𝑛𝑠

Description
IP address of source VM
IP address of destination VM
Sent packets count
Received packets with “IPR-bit” mark
Received packets with ECN mark
Description
The share rate or speed of NIC
The capacity of the token bucket in bytes
The number of available tokens to be used for transmission

it appealing from cloud providers’ perspective as it enables
easier and more tangible bandwidth pricing and accounting.

IV. PROPOSED METHODOLOGY
First we discuss HyGenICC by imagining the datacenter
network as contained within one end-host where the VMs are
connected via a single virtual switch. Then, we extend this
design to operate in a network of end-hosts where the datacenter
fabric is treated as black box that generates congestion signals
whenever congestion is experienced. In a single virtual switch
connecting all VMs, bandwidth contention happens at the
output link to the destination when multiple senders compete
to send through the same output port of the virtual switch.
The virtual switch need to distribute the available physical
port’s capacity among VMs and ensure compliance of the VMs
with the allocated shares. Hence it needs a mechanism that
detects and accounts for active VMs and apply rate limiters
on a per-VM basis to share the bandwidth among them.

HyGenICC deploys a ﬂow table (for congestion control
purpose) to track state information shown in Table I on a
VM-to-VM granularity (i.e., source VM-destination VM pairs).
In addition, per-VM token-bucket state is used to enforce the
VM’s share of bandwidth.

A. VM detection and bandwidth allocation

As soon as a VM’s port becomes active (sending or receiving
trafﬁc), an associated entry is created in the ﬂow table.
Whenever a new VM becomes active on a given NIC, the NIC’s
nominal capacity is redistributed among the token buckets
of active VMs to account for the new one. This is done by
readjusting the rate and bucket size of all active VMs’ token
buckets on that NIC. Any extra trafﬁc sent by the VM in excess
of its share is simply dropped and resent later by the transport
layer or otherwise a per-VM queue is used for holding the trafﬁc
for later transmission whenever the tokens are regenerated2.

B. Congestion Control Mechanism

In practice, congestion may always happen within the
network as shown in Figure 2, if the network is over-subscribed
or does not provide full bisection bandwidth. HyGenICC
therefore relies on readily available features in switches
hardware3 , to convey congestion signals to the sources. To be

2We have experimented with both approaches and the queuing mechanism
achieves slightly better performance which did not motivate its usage due to
management and memory overhead.

3Most current commodity switches used in data centers are equipped with
QoS mechanisms like Strict Priority (SP), Weighted Fair Queuing (WFQ) and
Weighted Random Early Detection (WRED) in addition to the ability of ECN
marking of IP packets.

4

Ethernet headers) and piggybacks explicitly the number of
remaining ECN marks on the identiﬁcation ﬁeld of this IP
packet. The IP protocol ﬁeld is destined to an unused number
that has meaning only for HyGenICC.

At the sender, to match the current sending rate to the
congestion level in the network, upon receiving “IPR-bit”
marks or the special packet, the source decreases the VM’s
current allocated rate in proportion to the amount of marks
and gradually increases the rate when no congestion bits are
received in a period.

Figure 2: HyGenICC high-level system design

V. IMPLEMENTATION

more abstract, HyGenICC treats the datacenter network as a
black box in which source servers inject trafﬁc and the black
box generates ECN marks in response to congestion towards
the receivers. ECN marks are a fast proactive mechanism that
can help in quickly detecting any congestion from a shared
queue when buffers exceed a pre-conﬁgured queue occupancy
threshold along a packet’s path.

HyGenICC uses the ﬂow table to track, for each source-
destination pair, the number of IP packets received with
congestion notiﬁcation marks, regardless of the type of transport
protocol (TCP, UDP, or otherwise). This information is a
valuable indication of the level of congestion along the path
between the source VM and the destination VM starting at that
particular NIC. Since HyGenICC implements a network-layer
congestion control, any ECN or other marking used to track
congestion is cleared before delivering the datagrams to the
VM4. In addition, to force universal ECN marking along the
path, all outgoing packets are marked with the ECN-enabled
bit. HyGenICC typically creates a network layer congestion
control loop between hypervisors and is fully transparent to
the overlying VM transport protocol.

At the receiver side, upon receiving ECN marks, HyGenICC
needs to reﬂect the information back to the source to trigger
reduction of the sending rate of that particular source VM.
To avoid introducing any additional overhead and hinder the
operation of any on-path middle-boxes by introducing a new
protocol, we propose to piggyback the information on any
returning data. For this we identify three types of trafﬁc
ﬂows: TCP, which is by default bidirectional, other non-TCP
bidirectional trafﬁc and ﬁnally unidirectional trafﬁc; for the
three categories of trafﬁc, we propose to use the unused reserved
bit in the IP header “IPR-bit” of any reverse packet to reﬂect
the ECN marking synchronously to the origin. While this might
be sufﬁcient for the ﬁrst two categories of trafﬁc to carry all
marking back to the source, for the third category, there might
be a dramatic imbalance in the forward trafﬁc and reverse
trafﬁc leading to some proportion of forwarded markings not
being reﬂected back. As a solution HyGenICC crafts a special
small IP packet with header only (20 bytes of IP and 14 for

As explained above, HyGenICC needs two mechanisms: rate
limiters at the source server and congestion controller that run
from source to destination server. These mechanisms can either
be implemented in software, or hardware or a combination of
both as necessary. We simpliﬁed the design and concepts of
HyGenICC so that the built system is able to maintain line
rate performance at 1-10Gb/s while reacting quickly to deal
with congestion within a datacenter’s short RTT time scale.

A. HyGenICC sender

HyGenICC sender processing is described in Algorithm 1. At
the senders HyGenICC tracks the rate, the number of tokens,
the depth of the bucket and the ﬁll-rate variables per-VM
per-NIC where the per-VM rate limiters are implemented as
counting token buckets that have a rate 𝑅(𝑖, 𝑗) each, a bucket
capacity 𝐵(𝑖, 𝑗) each and number of tokens 𝑇 (𝑖, 𝑗) each. In
addition, the sender will also handle the received congestion
signals from different destinations on a per-source basis.

1) Rate Allocation: Initially, the installed on-system NICs
are probed and the values of their nominal data rate 𝑅(𝑖), bucket
capacity 𝐵(𝑖) and tokens 𝑇 (𝑖) are calculated correspondingly.
Thereafter, when packets start ﬂowing from each source VM,
NIC capacities are redistributed and a new capacity share
“𝐶𝑎 𝑝𝑎𝑐𝑖𝑡𝑦 𝑆ℎ𝑎𝑟𝑒” is calculated and used to update the entries
for each active VM in the rate, tokens and bucket matrices and
the VM is marked as currently active on all outgoing physical
NICs.

After a certain time of inactivity5, the bucket entries for a
VM are reset and its allocation is reclaimed and redistributed
among currently active VMs. As shown in Table I, ﬂow-table
entries are established immediately after arrival of the ﬁrst
packet using source-destination IP address6. First, on arrival or
departure of each packet 𝑃, its outgoing port 𝑗 and incoming
port 𝑖 are detected. The current value of available tokens 𝑇 (𝑖, 𝑗)
is retrieved and replenished based on the elapsed time since
the last transmission. Then, using the new 𝑇 (𝑖, 𝑗), the packet is
allowed for transmission if 𝑇 (𝑖, 𝑗) ≥ 𝑠𝑖𝑧𝑒( 𝑝𝑘𝑡), in this case the
packet length is deducted from 𝑇 (𝑖, 𝑗), otherwise the packet is
dropped.

4Supposedly, If the tenants are willing to deploy ECN in their overlay
networks then our mechanism should not clear ECN mark and let the transport
layer handle it as well. We believe the two would not conﬂict rather complement
each other as shown in the simulations.

5Inactivity timeout is set to 1 sec in simulations.
6We track the state of the communicating VM pairs not

individual
ﬂows plus we ﬁnd that the operations involved do not add burden on the
hypervisor/vswitch.

IPR bitECN bitHyGenICCSenderHyGenICCReceiverServer1Server2HypervisorORvSwitchNIC1VM2VM1VM3NIC2DCNAlgorithm 1: HyGenICC Sender Algorithm
1 Function Packet Departure(𝑃, 𝑖, 𝑗)
2

look up ﬂow entry 𝑓 in ﬂow table
𝑇 (𝑖, 𝑗) = 𝑇 (𝑖, 𝑗) + 𝑅(𝑖, 𝑗) × (𝑛𝑜𝑤() − 𝑓 .𝑠𝑒𝑛𝑡𝑡𝑖𝑚𝑒)
𝑇 (𝑖, 𝑗) = 𝑀 𝐼 𝑁 (𝐵(𝑖, 𝑗), 𝑇 (𝑖, 𝑗)) if
𝑇 (𝑖, 𝑗) ≥ 𝑆𝑖𝑧𝑒(𝑃) then

𝑇 (𝑖, 𝑗) = 𝑇 (𝑖, 𝑗) − 𝑆𝑖𝑧𝑒(𝑃) 𝑓 .𝑠𝑒𝑛𝑡𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤()
Enable ECN Capable bits (ECT) in IP header

3

4

5

else

Drop the packet

6 Function Packet Arrival(𝑃, 𝑖, 𝑗)
7

look up ﬂow entry 𝑓 in ﬂow table if Packet is
congestion feedback message then

8

9

10

11

𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 = 𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 + 𝑖𝑛𝑡 (𝑃.𝑑𝑎𝑡𝑎)
𝑓 .𝑟𝑏𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 = 𝑡𝑟𝑢𝑒
𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤() Drop the packet

else

if Packet is “IPR-bit” marked then
𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 = 𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 + 1
𝑓 .𝑟𝑏𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 = 𝑡𝑟𝑢𝑒
𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤()
Clear the mark and forward to the VM

12 Function Timer timeout
13

forall ﬂow 𝑓 in 𝐹𝑙𝑜𝑤𝑇 𝑎𝑏𝑙𝑒 do

14

15

16

17

18

19

20

21

22

23

24

25

26

if 𝑛𝑜𝑤() − 𝑓 .𝑠𝑒𝑛𝑡𝑡𝑖𝑚𝑒 ≥ 1𝑠𝑒𝑐 then

𝑓 .𝑎𝑐𝑡𝑖𝑣𝑒 = 𝑓 𝑎𝑙𝑠𝑒
Reset 𝑓 entry in Flow Table
Redistribute NIC capacity among active ﬂows

forall Active ﬂow 𝑓 in 𝐹𝑙𝑜𝑤𝑇 𝑎𝑏𝑙𝑒 do

if now() - f.feedbacktime≥ Congestion Timeout
then

𝑓 .𝑟𝑏𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 = 𝑓 𝑎𝑙𝑠𝑒
if 𝑓 .𝑟𝑏𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 == 𝑓 𝑎𝑙𝑠𝑒 then

𝑅(𝑖, 𝑗) = 𝑅(𝑖, 𝑗) + 𝑠𝑐𝑎𝑙𝑒(𝑁 𝐼𝐶 𝐶𝑎 𝑝)

else

if 𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 ≥ 0 then

𝑅(𝑖, 𝑗) =
𝑅(𝑖, 𝑗) − ( 𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 × 𝑠𝑐𝑎𝑙𝑒(𝑁 𝐼𝐶 𝐶𝑎 𝑝))

else

𝑅(𝑖, 𝑗) = 𝑅(𝑖, 𝑗) + 𝑠𝑐𝑎𝑙𝑒(𝑁 𝐼𝐶 𝐶𝑎 𝑝)

𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 = 0

𝑅(𝑖, 𝑗) =
𝑀 𝐴𝑋 (0, 𝑀 𝐼 𝑁 (𝐶𝑎 𝑝𝑎𝑐𝑖𝑡𝑦 𝑆ℎ𝑎𝑟𝑒, 𝑅(𝑖, 𝑗)))

2) Congestion Reaction: The sender module reacts on
regular intervals to incoming “IPR-bit” and cuts the sending rate
in proportion to the amount of marking received. Hence, sources
causing congestion in the network will receive “IPR-bit” signals
and will react by decreasing their sending rates proportionally
until the congestion subsides and congestion signals start
disappearing at which time sources start to gradually increase
their rates. The process will increase the rate conservatively, and

5

if no feedback arrives within 𝐶𝑜𝑛𝑔𝑒𝑠𝑡𝑖𝑜𝑛 𝑇𝑖𝑚𝑒𝑜𝑢𝑡 seconds,
the rate is increased fast until it reaches its “Capacity Share”
or an “IPR-bit” is detected again. Function ”scale(NIC Cap)“
is used to scale the amount of rate increase and decrease to
account for a single packets transmission over a single RTT (i.e.,
1000 bytes over 1 Gb/s in average RTT range of 100𝜇s-10ms
would give us ≈ 80-8 Mb/s of increments).

B. HyGenICC receiver

At the receiver, HyGenICC needs to track incoming conges-
tion ECN marks from the network on a per-source-destination
basis and feed this information back by piggybacking it on
outgoing packets heading back to corresponding sources. Hence,
the operations of the receiver is quite simple and does not incur
much processing overhead onto incoming trafﬁc. The receiver
processing is described in Algorithm 2.

Algorithm 2: HyGenICC Receiver Algorithm
1 Function Packet Arrival(𝑃, 𝑖, 𝑗)
2

look up ﬂow entry 𝑓 in ﬂow table if Packet is
ECN marked then

3

4

5

𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 = 𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 + 1
Clear the mark and forward to the VM
if now() - f.feedbacksenttime ≥ feedback timeout
then

Create IP feedback message and send to 𝑓 .𝑠𝑜𝑢𝑟𝑐𝑒

𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 𝑠𝑒𝑛𝑡𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤()
𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 = 0

6 Function Packet Departure(𝑃, 𝑖, 𝑗)
7

look up ﬂow entry 𝑓 in ﬂow table if

𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 ≥ 1 then

8

Set “IPR-bit” ﬂag in IP header
𝑓 . 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 𝑠𝑒𝑛𝑡𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤()
𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 = 𝑓 .𝑒𝑐𝑛𝑚𝑎𝑟 𝑘 𝑠 − 1

Each incoming packet is checked for ECN mark and the
number of packets with and without the mark are traced in
the ﬂow table, Table I, and immediately the ECN mark is
cleared before re-injecting the packet in the normal packet
processing path. For each ECN marked packet, an IPR-bit
mark is reﬂected in the ﬁrst available outgoing packet to that
destination (it could be a TCP ACK if the ﬂow is TCP or a
UDP reply data packet) until all the ECN marks are cleared.
However, when ingress and egress trafﬁc are out of balance on
a given ﬂow, non-reﬂected ECN marks may start to accumulate
at the receiver, to address this issue, we periodically use an
explicit ICMP-like feedback packet to convey the remaining
amount of ECN marks to the source. On a regular intervals
close to an RTT, we scan through the ﬂow table asynchronously
for any ﬂow with remaining ECN marks and that has not sent
any feedback for a period of 𝐹𝑒𝑒𝑑𝑏𝑎𝑐𝑘 𝑇𝑖𝑚𝑒𝑜𝑢𝑡. If any is
found, then an IP packet is created with unused protocol ID
value and the current value of ECN marks added as a 2-bytes
payload of this packet addressed to the source of the ﬂow. This
event is infrequent and unlikely to exist but if so, will not

6

of all active VMs’ token buckets on that NIC. Then we need
to extend the allocation of single hypervisor to account for
the in-network congestion caused by a network of hypervisors
managing tenants’ VMs.

Table II: Variables and parameters used by the SDN controller

application and end-host shim-layer of SDN-GCC

Parameter name
𝑇𝑜
𝑇𝑐
𝑇𝑖
Variable name (Shim-Layer)
𝑠𝑜𝑢𝑟 𝑐𝑒
𝑣 𝑝𝑜𝑟 𝑡
𝑟 𝑎𝑡𝑒
𝑏𝑢𝑐𝑘𝑒𝑡
𝑡𝑜𝑘𝑒𝑛𝑠
𝑠𝑒𝑛𝑡 𝑡𝑖𝑚𝑒

Variable name (SDN APP)
𝑆𝑊 𝐼𝑇 𝐶 𝐻
𝑆𝑊 𝐼𝑇 𝐶 𝐻 𝑃𝑂𝑅𝑇
𝐷𝑆𝑇 𝑆𝑅𝐶
𝐼 𝑃𝑇 𝑂𝑃𝑂𝑅𝑇
𝑀 𝐴𝑅𝐾 𝑆

Description
Timeout for Flow inactivity period
Timeout for Congestion grace period
Timeout of Congestion monitor period
Description
IP address of source VM
virtual port connecting VM
The allocated sending rate
The capacity of the token bucket in bytes
The number of available tokens
The time-stamp of last transmission
Description
List of the controlled SDN switches
List of the ports on the switches
List of destinations to sources pairs
List of IP to switch port pairs
ECN marks reading of for each switch port

In practice, congestion may always happen within the data
center network, if the network is over-subscribed or does
not provide full bisection bandwidth. SDN-GCC in an effort
to account for this limitation, relies on readily available
functionality in SDN switches to convey congestion events
to the controller. To elaborate more, SDN-GCC controller can
keep a centralized record of congestion statistics by periodically
collecting state information from the switches as shown in
Table II. ECN marking is chosen as a fast live-congestion
indication to signal the onset of possible congestion at any
shared queue. However, Usage of RED and ECN marking
could be avoided if drop-tail AQM keeps statistics of backlog
exceeding a certain pre-set threshold.

SDN-GCC application running on top of the network OS,
keeps record of each network-wide state information (e.g.,
congestion points). Hence, it can infer the bottleneck queues
based on this information and make intelligent decisions
accordingly. Whenever necessary, it sends special congestion
notiﬁcations to the shim-layer to adjust the sending rate of the
affected VM. Upon receiving any congestion notiﬁcation The
shim-layer reacts by adjusting VM’s rate-limiter proportionally
to the congestion level in the network and gradually increases
the rate when no more congestion messages are received.

VII. DESIGN AND IMPLEMENTATION

As explained above, SDN-GCC needs two components:
shim-layer at the servers and the control application that runs
on top of the network OS. These mechanisms can either be
implemented in software, or hardware or a combination of
both as necessary. We simpliﬁed the design and concepts of
SDN-GCC so that the built system is able to maintain line rate
performance at 1-10Gb/s while reacting quickly to deal with
congestion within a reasonable time.

A. SDN-GCC End-Host Shim-Layer

SDN-GCC shim-layer processing is described in Algorithm
3. The major variables it tracks are the rate, the number of

Figure 3: SDN-GCC high-level system design: 1) congestion point;
2) network statistics; 3) congestion tracking; 4) congestion
notiﬁcation; 5) rate adjustment.

incur much network overhead as the packet size would be 36
bytes (14-bytes Ethernet header + 20-bytes IP header + 2-bytes
payload data). To compress further the explicit feedback, the
2 bytes payload can be piggybacked instead in the IP header
identiﬁcation ﬁeld.

VI. PROPOSED METHODOLOGY

Figure 3 shows SDN-GCC’s system design which is broken
down into two parts: a network application that runs on the
SDN controller (network OS). It is responsible for monitoring
network states by querying the switches periodically via SDN’s
standard southbound API and signaling congestion; and a
hypervisor-based shim-layer, that is responsible of enforcing
per-VM rate control in response to congestion notiﬁcation by
the control application. The following scenario sketches the
SDN-GCC cycle: 1) Whenever the total incoming load exceeds
the link capacity, the link (in-red) becomes congested implying
that senders are exceeding their allocated rates. 2) SDN-
switches sends to the network OS periodic keep-alive and
statistics through the established control plane between them
(e.g., OpenFlow or sFlow). Whenever necessary, the switch
would report the amount of congestion experienced by each
output queue of its ports. 3) The SDN-GCC application co-
located with the network OS (or alternatively communicating
via the north-bound API) tracks congestion events in the
network. 4) SDN-GCC application communicates with the
SDN-GCC shim-layer of the sending servers whose VMs
are causing the congestion. 5) SDN-GCC shim-layer takes
a corrective action by adjusting the rate-limiter of the target
VM.

We start from a single end-host (hypervisor) connecting all
VMs where bandwidth contention happens at the output link
(i.e., when multiple senders compete to send through the same
output NIC of the virtual switch). The hypervisor needs to
distribute the available NIC’s capacity among VMs and ensure
compliance of the VMs’ weights with the allocated shares.
Hence it employs a mechanism to apply rate limiters on a per-
VM basis. Table II shows the variables needed to implement
a per-VM token-bucket rate limiter. Ideally, when a virtual
port becomes active, its variables are initialized and the NIC’s
nominal capacity is redistributed among the rate limiters of
currently active VMs by readjusting the rate and bucket size

Network Operating System (NOS)North-Bound APISouth-Bound APIControllerRoutingFirewallSDN-GCC...Shim-LayerHypervisorNICVM2VM1VM31234R3R2R15Algorithm 3: SDN-GCC Shim-layer
1 Function 𝑁𝑜𝑟𝑚𝑎𝑙 𝑃𝑎𝑐𝑘𝑒𝑡 𝐴𝑟𝑟𝑖𝑣𝑎𝑙 (𝑃, 𝑠𝑟𝑐, 𝑑𝑠𝑡)
/* i is NIC and j is VNIC index
*/
2
𝑇 (𝑖, 𝑗) = 𝑇 (𝑖, 𝑗) + 𝑅(𝑖, 𝑗) × (𝑛𝑜𝑤() − 𝑓 .𝑠𝑒𝑛𝑡 𝑡𝑖𝑚𝑒);
𝑇 (𝑖, 𝑗) = 𝑀 𝐼 𝑁 (𝐵(𝑖, 𝑗), 𝑇 (𝑖, 𝑗));
Enable ECN-capable bits (ECT) in IP header;
if 𝑇 (𝑖, 𝑗) ≥ 𝑆𝑖𝑧𝑒(𝑃) then

4

6

5

3

𝑇 (𝑖, 𝑗) = 𝑇 (𝑖, 𝑗) − 𝑆𝑖𝑧𝑒(𝑃);
𝑠𝑒𝑛𝑡 𝑡𝑖𝑚𝑒(𝑖, 𝑗) = 𝑛𝑜𝑤();

else

Queue until token regeneration OR Drop;

11 Function 𝐶𝑜𝑛𝑡𝑟𝑜𝑙 𝑃𝑎𝑐𝑘𝑒𝑡 𝐴𝑟𝑟𝑖𝑣𝑎𝑙 (𝑃, 𝑖, 𝑗)
12

if Packet has congestion notiﬁcation message then

𝑚𝑎𝑟 𝑘 𝑠 = 𝑖𝑛𝑡 (𝑚𝑠𝑔);
if 𝑚𝑎𝑟 𝑘 𝑠 ≥ 0 then

𝑐𝑜𝑛𝑔 𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 (𝑖, 𝑗) = 𝑡𝑟𝑢𝑒;
𝑒𝑙𝑎 𝑝𝑠𝑒𝑑 𝑡𝑖𝑚𝑒 = 𝑛𝑜𝑤() − 𝑐𝑜𝑛𝑔 𝑡𝑖𝑚𝑒(𝑖, 𝑗);
𝑚𝑎𝑟 𝑘𝑠
𝑚𝑎𝑟 𝑘 𝑟𝑎𝑡𝑒 =
𝑒𝑙 𝑝𝑎𝑠𝑒𝑑 𝑡𝑖𝑚𝑒 ;
𝑅(𝑖, 𝑗) = 𝑅(𝑖, 𝑗) − (𝑚𝑎𝑟 𝑘 𝑟𝑎𝑡𝑒 × 𝑠𝑐𝑎𝑙𝑒(𝐶));
𝑅(𝑖, 𝑗) = 𝑀𝑎𝑥(𝑅𝑚𝑖𝑛, 𝑅(𝑖, 𝑗));
𝑐𝑜𝑛𝑔 𝑡𝑖𝑚𝑒(𝑖, 𝑗) = 𝑛𝑜𝑤();

else

Send to normal packet processing;

23 Function 𝑆𝑡𝑎𝑡𝑒 𝑈 𝑝𝑑𝑎𝑡𝑒 𝑇𝑖𝑚𝑒𝑜𝑢𝑡 ()
24

forall i in NICs and j in VNICs do

if 𝑛𝑜𝑤() − 𝑐𝑜𝑛𝑔 𝑡𝑖𝑚𝑒(𝑖, 𝑗) ≥ 𝑇𝑐 then
𝑐𝑜𝑛𝑔 𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 (𝑖, 𝑗) = 𝑓 𝑎𝑙𝑠𝑒;

else

if 𝑛𝑜𝑤() − 𝑠𝑒𝑛𝑡 𝑡𝑖𝑚𝑒(𝑖, 𝑗) ≥ 𝑇𝑜 then

𝑎𝑐𝑡𝑖𝑣𝑒(𝑖, 𝑗) = 𝑓 𝑎𝑙𝑠𝑒;
redistribute NIC capacity among active ﬂows;

if 𝑎𝑐𝑡𝑖𝑣𝑒(𝑖, 𝑗) && 𝑐𝑜𝑛𝑔 𝑑𝑒𝑡𝑒𝑐𝑡𝑒𝑑 (𝑖, 𝑗) then

𝑅(𝑖, 𝑗) = 𝑅(𝑖, 𝑗) + 𝑠𝑐𝑎𝑙𝑒(𝐶);
𝑅(𝑖, 𝑗) = 𝑀 𝐼 𝑁 (𝐸 (𝑖), 𝑅(𝑖, 𝑗));

7

8

9

10

13

14

15

16

17

18

19

20

21

22

25

26

27

28

29

30

31

32

33

tokens and the depth of the bucket variables per-VM per-NIC
where the per-VM rate limiters are implemented as counting
token buckets where virtual NIC 𝑗 has a rate 𝑅(𝑖, 𝑗), bucket
depth 𝐵(𝑖, 𝑗) and number of tokens 𝑇 (𝑖, 𝑗) on physical NIC
𝑖. In addition, the shim-layer will also translate the received
congestion message from the controller on a per-source basis.
Initially, the installed on-system NICs are probed and the
values of their nominal data rate 𝑅(𝑖), and bucket size 𝐵(𝑖) are
calculated. Thereafter, when the ﬁrst packet is intercepted from
a new VM, NIC capacity is redistributed and a equal-share of
capacity “𝐸 (𝑖)” is calculated. The new value 𝐸 (𝑖) is used to
re-distribute the allocated rate for each active VM and then the
new VM is marked as active7 As shown in Table II, the state of

7Typically, after a certain time of inactivity (e.g., 1 sec in our simulation),
the variables used for VM tracking are reset and the rate allocations are
redistributed among currently active VMs.

7

the communicating VM is tracked only through token bucket
and congestion speciﬁc variables. The shim-layer algorithm
shown in Algorithm 3 is located at the forwarding stage of
the stack, on arrival or departure of a packet 𝑃, it detects
the packet’s outgoing port 𝑗 and incoming port 𝑖. Before 𝑃’s
departure, the available tokens 𝑇 (𝑖, 𝑗) is refreshed based on
the elapsed time since the last transmission. The packet is
then cleared for transmission if 𝑇 (𝑖, 𝑗) ≥ 𝑠𝑖𝑧𝑒(𝑃), in which
case 𝑠𝑖𝑧𝑒(𝑃) is deducted from 𝑇 (𝑖, 𝑗). Otherwise, 𝑃 is simply
dropped8. The shim-layer intercepts only the special congestion
message.

For each incoming notiﬁcation,

the algorithm cuts the
sending rate in proportion to the rate of marking received
(capped by 𝑅𝑚𝑖𝑛) which is a parameter set by the operator.
Typically, this values is chosen with respect to the minimal
bandwidth guarantee provided by the operator. Hence, as
sources cause more congestion in the network, the amount
of marks received increases and as a result the sending rates
of such sources decrease proportionally until the congestion
subsides. When congestion messages become less frequent, or
after a pre-determined timer 𝑇𝑐 elapses, the algorithm starts
to gradually increase the VMs’ source rate conservatively.
The rate is increased until it reaches 𝐸 (𝑖), or congestion is
perceived again, leading to another reduction cycle. Function
”scale(C)” is used to scale the amount of rate increase and
decrease proportional to the NICs rate and to smooth out large
variations in rate dynamics.

B. SDN Network Application

SDN-GCC relies on a SDN network application to probe for
congestion statistics on a regular basis from the queues of the
SDN switches in the network. The application sends notiﬁcation
messages towards the VMs that are causing congestion on
a given port. This is accomplished by building a special
message with those particular VMs as destinations with the
data indicating the amount of marking they have caused. These
messages are never delivered to the VMs and are actually
intercepted by the shim-layer in the hypervisor. For simplicity,
we assume that each of the involved VMs contribute equally to
the congestion, and hence, the marks are divided equally among
source VMs. SDN-GCC Controller shown in Algorithm 4 is an
event-driven mechanism that handles two major events: packet
arrivals of unidentiﬁed ﬂows (miss-entries) from switches and
congestion monitoring timer expiry to trigger warning messages
to the involved sources if necessary.

1) Upon a packet arrival: extract the necessary information
to establish source to destination 𝐷𝑆𝑇 𝑆𝑅𝐶 relationship
and destination to port relationship 𝐼 𝑃𝑇𝑂𝑃𝑂𝑅𝑇. This
is necessary to establish associations between congested
ports and corresponding sources. In addition, The timer for
congestion monitoring is re-armed if it was not already.
2) Congestion monitoring timer expiry: for each switch
𝑠𝑤, the controller probes for marking statistics through
OpenFlow or sFlow protocols by calling function
𝑟𝑒𝑎𝑑 𝑚𝑎𝑟 𝑘 𝑠(𝑠𝑤). The new marking rate of each switch

8Packets could be queued for later transmission, however, this approach

adds a large overhead on the end-hosts

Algorithm 4: SDN-GCC SDN application
1 Function 𝑃𝑎𝑐𝑘𝑒𝑡 𝐴𝑟𝑟𝑖𝑣𝑎𝑙 (𝑃, 𝑠𝑟𝑐, 𝑑𝑠𝑡)
2

if P is an IP packet of a new source-destination pair then

𝐷𝑆𝑇 𝑆𝑅𝐶 [𝑃.𝑠𝑟𝑐] = 𝑃.𝑑𝑠𝑡;
𝐼 𝑃𝑇𝑂𝑃𝑂𝑅𝑇 [𝑃.𝑠𝑟𝑐] = 𝑃.𝑖𝑛 𝑝𝑜𝑟𝑡;
if Timer is not active then start 𝐶 𝑀 𝑇𝑖𝑚𝑒𝑟 (𝑇𝑖) ;

6 Function 𝐶𝑜𝑛𝑔𝑒𝑠𝑡𝑖𝑜𝑛 𝑀𝑜𝑛𝑖𝑡𝑜𝑟 𝑇𝑖𝑚𝑒𝑜𝑢𝑡 ()
7

forall 𝑠𝑤 𝑖𝑛 𝑆𝑊 𝐼𝑇𝐶𝐻 do

𝑠𝑤 𝑚𝑎𝑟 𝑘 𝑠 = 𝑟𝑒𝑎𝑑 𝑚𝑎𝑟 𝑘 𝑠(𝑠𝑤);
forall 𝑝 𝑖𝑛 𝑆𝑊 𝐼𝑇𝐶𝐻 𝑃𝑂𝑅𝑇 do

𝛼 = 𝑀 𝐴𝑅𝐾𝑆[𝑠𝑤] [ 𝑝] − 𝑠𝑤 𝑚𝑎𝑟 𝑘 𝑠[ 𝑝];
𝑀 𝐴𝑅𝐾𝑆[𝑠𝑤] [ 𝑝] = 𝑠𝑤 𝑚𝑎𝑟 𝑘 𝑠[ 𝑝];
if 𝛼 > 0 then

𝐷𝑆𝑇 𝐿𝐼𝑆𝑇 = 𝑔𝑒𝑡 𝑎𝑙𝑙 𝑑𝑠𝑡 (𝑠𝑤, 𝑝) ;
forall 𝑑𝑠𝑡 𝑖𝑛 𝐷𝑆𝑇 𝐿𝐼𝑆𝑇 do

𝑆𝑅𝐶 𝐿𝐼𝑆𝑇 [𝑑𝑠𝑡] = 𝑔𝑒𝑡 𝑎𝑙𝑙 𝑠𝑟𝑐(𝑑𝑠𝑡);
𝛽 = 𝛽 + 𝑠𝑖𝑧𝑒(𝑆𝑅𝐶 𝐿𝐼𝑆𝑇 [𝑑𝑠𝑡]);

if 𝛽 > 0 then
𝑚 = 𝛼
𝛽 ;
forall 𝑑𝑠𝑡 𝑖𝑛 𝐷𝑆𝑇 𝐿𝐼𝑆𝑇 do

forall 𝑠𝑟𝑐 𝑖𝑛 𝑆𝑅𝐶 𝐿𝐼𝑆𝑇 [𝑑𝑠𝑡] do

𝑚𝑠𝑔 = 𝑀𝑆𝐺 ( 𝑚 , 𝑑𝑠𝑡 , 𝑠𝑟𝑐 );
𝑠𝑒𝑛𝑑 𝑚𝑠𝑔 𝑡𝑜 𝑠𝑟𝑐;

3

4

5

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

Restart 𝐶 𝑀 𝑇𝑖𝑚𝑒𝑟 (𝑇𝑖);

port 𝑝 is calculated [33]. For each port, if there are
new markings (due to congestion), then the controller
needs to advertise this to all related sources. Thus we
ﬁrst retrieve the destination list of this port via function
𝑔𝑒𝑡 𝑎𝑙𝑙 𝑑𝑠𝑡 (𝑠𝑤, 𝑝) and then for each destination retrieve
the sources using 𝑔𝑒𝑡 𝑎𝑙𝑙 𝑠𝑟𝑐(𝑑𝑠𝑡). The controller now
piggybacks on any outgoing control message or crafts an
IP message consisting of an Ethernet Header (14 bytes),
an IP header (20 bytes), and a payload (2-byte) containing
the number of ECN marks that have been observed in
the last measurement period, divided by the number of
sources. This message is created for each source concerned
(sending through the port 𝑝 experiencing congestion) and
sent with the source IP of the dest. VM and dest. IP of
the source VM (which allows the shim-layer to identify
the appropriate forwarding ports of source VM).

C. Implementation and Practical Issues

Any trafﬁc sent by the VM in excess of its share can
either be queued or simply dropped and resent later by the
transport/application layer. In the former case, an extra per-
VM queue is used for holding the trafﬁc for later transmission
whenever the tokens are regenerated. We tested both approaches
and the queuing mechanism turned out to achieve marginally
better performance which, in view of the complexity it adds,
did not motivate its need. If ECN marking is not in use end-
to-end, all outgoing data packets must be marked with the
ECT bit. In addition, the shim-layer needs to clear any ECN
marking used to track congestion before delivering the packets
to the target VMs.

8

SDN-GCC is distributed between the network application
and the shim-layer with very low computational complexity and
can be integrated easily in any network whose infrastructure is
based on SDN. Due to recent advancement of memory speeds,
the throughput of internal forwarding (e.g., Open vSwitch
(OvS)) of commercial desktop/server is 50-100 Gb/s, which is
fast enough to handle 10’s of concurrent VMs sharing a single
or few physical links. Hence, the overhead of the shim-layer
functions added to the OvS would not overload the CPU. In
addition, the shim layer at the hypervisor require operations
of 𝑂 (1) per packet, as a result the additional overhead is
insigniﬁcant. The network application is also of low complexity
making it ideal for fast response to congestion (within few
milliseconds time scale).

In multi-path routing, the SDN application with global view
can evaluate congestion on a path-by-path basis. Consequently,
the shim-layer can adapt rates to each path which it can identify
via the 5-tuple hash of the packets. Finally, the control plane
communications in SDN networks is typically out-of-band
i.e., different from the data plane [34], hence fast reaction
to congestion is possible and notiﬁcation messages are not
interrupted by in-network middle-boxes.

In data centers, the internet-facing incoming/outgoing con-
nections are typically isolated from intra-data center connec-
tions via connection splitting at the front-end or proxy servers.
Hence, the intra-data center congestion is primarily attributed
to ﬂows running in the data center

VIII. SIMULATION ANALYSIS

In this section, we study the performance of the proposed
scheme via ns2 simulation in network scenarios with a high
bandwidth-low delay. We examine the performance of a tagged
VM that uses New-Reno TCP with SACK-enabled. The tagged
TCP connection competes with other VMs running similar
New-Reno TCP, DCTCP, or UDP in four cases: 1) a setup
that uses RED AQM with non-ECN enabled TCP; 2) a setup
that uses RED AQM with ECN enabled TCP; 3) a setup
that uses HyGenICC as the trafﬁc control mechanism; and
4)
a setup that uses proposed SDN-GCC framework. For
HyGenICC, there is a single parameter settings of timeout
interval for updating ﬂow rates which should be larger than
a single RTT, in the simulation this value is set to 500 𝜇s
(i.e., 5 RTTs). However, in the case of SDN-GCC, timeout
interval for congestion monitoring and reporting application of
the controller is set to a large value of 5ms (i.e., 50 RTTs). We
set the minimum rate 𝑅𝑚𝑖𝑛 to 1% of the share of the sender
VM on the underling NIC. In all simulation experiments, we
adjust RED parameters to perform ECN marking based on
the instantaneous queue length at the threshold of 20% of the
buffer size.

A. Simulation Setup

We use ns2 version 2.35 [35] which we patched to include the
different mechanisms. We use in all our simulation experiments
link speed of 1 Gb/s for stations, small RTT of 100 𝜇s and
the default TCP 𝑅𝑇𝑂𝑚𝑖𝑛 of 200 ms. We use a single-rooted
tree (Dumbbell) topology with single bottleneck link of 1Gb/s

9

UDP are more aggressive. Figure 1b suggests that ECN can
partially ease the problem, however the achieved throughput
reaches the allocated share only when the competitor uses the
same TCP protocol. This can be attributed to the fact that
TCP reacts conservatively to ECN marks unlike DCTPC which
reacts proportionally to the fraction of ECN marks. Simulation
with a static rate limit of 250 Mb/s (fair-share), show that
a central rate allocator assigning rates per VM can achieve
perfect rate allocation with no work-conservation (Utilization
is 250 Mb/s in period 3). Figure 4a shows that HyGenICC
thanks to its distributed and live adaptive rate limiters, can
respond effectively to congestion events. Finally, Figure 4b
suggest a similar result as HyGenICC can be achieved with the
help of a regular control messaging from a central controller
whenever necessary. Hence, SDN-GCC can efﬁciently leverage
its global view of network status to dynamically adjust the rate
limiters controlling the competing ﬂows that cause congestion
and yet achieve work conservative high network utilization.

8 ≈ 125𝑀𝑏, ≈ 1𝐺𝑏

SDN based schemes are questioned for their scalability which
is currently under active research [36]. We conduct the previous
experiment however we increase the number of competitors to
7, 15 and 31. Figure 5 suggests that SDN-GCC can scale well
with an increasing number of senders. The tagged TCP ﬂow
and competing ﬂows, starting at 10𝑡 ℎ, adjust their rates due
to the incoming control messages when the controller starts
observing congestion in the network. The adjustment messages
trigger ﬂow rate changes up and down until they reach the
equilibrium point where sources start oscillating slightly around
the target share of ≈ 1𝐺𝑏
16 ≈ 62.5𝑀𝑏 and
≈ 1𝐺𝑏
32 ≈ 31.25𝑀𝑏 for the 8, 16 and 32 sources respectively.
In SDN environments, controller delays are also of major
concern. To study the sensitivity of the system to the control
loop delays, We conduct the previous experiment involving 4
senders however we vary the control loop delay to a smaller
control delay of 10 RTTs and larger delays of 100 RTT and
500 RTT. Figure 6a shows that to achieve faster convergence
smaller switch-controller-hypervisor delay is always preferable.
Figure 6b and 6c shows that ﬂows oscillations and convergence
period increases as the controller delay increases to 10ms and
50ms. This shows that the performance of rate control schemes
such as SDN-GCC depends on ensuring that the controller can
react within moderate delays.

IX. TESTBED IMPLEMENTATION OF SDN-GCC

We have implemented SDN-GCC Control application as a
separate application program in python for any python-based
controller (e.g., Ryu [37] SDN framework in our testbed). Since,
most popular cloud management software (e.g., OpenStack) use
OpenvSwitch [38] as their end-host (hypervisor) networking
layer. We implemented SDN-GCC shim-layer as a patch to the
Kernel data-path module of OvS. We added the token-bucket
rate limiters and the congestion message handler (i.e., the shim-
layer) in the packet processing pipeline in the data-path of OvS.
In a virtualized environment, OvS forwards the trafﬁc for inter-
VM, intra-Host and inter-Host communications. This leads to
an easy and straightforward way of deploying the shim-layer
at the end-hosts by only applying a patch and recompiling

(a) HyGenICC enabled network

(b) SDN-GCC enabled network

Figure 4: The instantaneous and mean goodput of the tagged TCP
ﬂow while competing with 3 senders that use either TCP,
DCTCP or UDP. In interval [0,10] the competitors are
active, in [10,20] all ﬂows are active and in [20,30] only
the tagged TCP ﬂow is active.

at the destination, and run the experiments for a period of 30
sec. The buffer size of the bottleneck link is set to be more
than the bandwidth-delay product in all cases (100 Packets),
the IP data packet size is 1500 bytes.

B. Simulation Results and Discussion

For clarity, we ﬁrst consider a toy scenario with 4 elephant
ﬂows (the tagged ﬂow and 3 competitors). In the experiments,
the tagged FTP ﬂow uses TCP NewReno and competes either
with 3 FTP ﬂows using TCP NewReno, DCTCP or 3 CBR
ﬂows using UDP. Competitors start and ﬁnish at the 0𝑡 ℎ and
20𝑡 ℎsec respectively, while the tagged ﬂow starts at the 10𝑡 ℎsec
and continues until the end. Hence, from 0 to 10s (period 1)
only the competitors occupy the bandwidth, from 10s to 20s
(period 2) the bandwidth is shared by all ﬂows, and from 20s to
30s (period 3) the tagged ﬂow uses the whole bandwidth. This
experiment demonstrates work conservation, sharing fairness,
and convergence speed of SDN-GCC compared to other setups.
Figure 4 shows the instantaneous goodput of the tagged TCP
ﬂow along with the mean goodput with respect to its competitor
(in the legend, the optimal average goodput of tagged TCP
would be 0Mb/s for period 1, 250Mb/s for period 2, 1000Mb/s
for period 3 and 416Mb/s for all the periods). As shown in
Figure 1a, without any explicit rate allocation mechanisms and
without ECN ability, TCP struggles to grab any bandwidth
when competing with DCTCP and UDP ﬂows as DCTCP and

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (406)DCTCP (410)UDP (410)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (407)DCTCP (407)UDP (405)10

(a) 8 senders scenario

(b) 16 senders scenario

(c) 32 senders scenario

Figure 5: The instantaneous and mean goodput of the tagged TCP ﬂow competing with 7, 15 and 31 senders using TCP, DCTCP or UDP

(a) 1ms monitor interval

(b) 10ms monitor interval

(c) 50ms monitor interval

Figure 6: The instantaneous and mean goodput of the tagged TCP ﬂow while competing with either TCP, DCTCP or UDP using a control

period of 1ms (10RTT), 10ms (100RTT) or 50ms (500RTT) is used.

in the network as shown in Figure 7. Various iperf and/or
Apache client/server processes are created and associated with
their own virtual ports on the OvS at the end-hosts. This allows
for scenarios with large number of ﬂows in the network to
emulate a data center with various co-existing applications. The
base RTT is ≈200 𝜇s without queuing and ≈1 ms with queuing,
hence we set the monitoring/sampling interval to conservative
values larger than ≈1 ms (≈5 times the base RTT).

We run a scenario in which TCP and UDP elephant ﬂows
are competing for bandwidth and to test the agility of SDN-
GCC, a burst of mice TCP ﬂows is introduced to compete for
bandwidth in a short-period of time. We ﬁrst generate 7 TCP
iperf ﬂows and another 7 UDP iperf ﬂows from each sending
rack for 20 secs resulting in 42 (2 × 7 × 3 = 42) elephants at the
bottleneck. At the 10𝑡 ℎsec, we use Apache Benchmark [40] to
request ”index.html” webpage (10 times) from each of the 7
web servers on each sending rack (7 × 6 × 3 = 126 in total).
Figure 8a and 8b show that the TCP elephants are able to grab
their share of bandwidth regardless of the existence of non-
well-behaved UDP trafﬁc. In addition, Figure 8c suggests that
mice ﬂows still beneﬁt from SDN-GCC by achieving a smaller
and nearly smooth (equal) ﬂow completion time on average
with a smaller standard deviation demonstrating SDN-GCC’s
effectiveness in apportioning the link capacity. In summary,
SDN-GCC effectively tackles congestion and allocates the
capacity among various ﬂow types as expected.

To summarize this simulation and experimental study, SDN-
GCC seems to be able to efﬁciently allocate the bandwidth

(a) The testbed topology

(b) The actual testbed

Figure 7: A real testbed for experimenting with SDN-GCC framework

the OvS kernel module, introducing minimal impact on the
operations of production DC networks with no need for a
complete shutdown. Speciﬁcally, deployment can be carried
out by the management software responsible for admission and
monitoring of the data center.

We set up a testbed as shown in Figure 7. All machines’
internal and the outgoing physical ports are connected to the
patched OvS. We have 4 racks consisting of 7 servers each
(rack 1, 2 and 3 are senders and rack 4 is receiver) all servers
are installed with Ubuntu Server 14.04 LTS running kernel
v3.16 and are connected to the ToR switch through 1 Gbps
links. Similarly, the machines are installed with the iperf [39]
program for creating elephant ﬂows and the Apache web server
hosting a single webpage ”index.html” of size 11.5KB for
creating mice ﬂows. We setup different scenarios to reproduce
both incast and buffer-bloating situations with bottleneck link

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (374)DCTCP (374)UDP (366)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (342)DCTCP (342)UDP (337)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (347)DCTCP (347)UDP (336)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (412)DCTCP (412)UDP (412)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (404)DCTCP (404)UDP (400)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (373)DCTCP (373)UDP (347)Rack 1Rack 2Rack 3CoreToRRack 4ControllerBottleneckControl11

(a) AVG TCP goodput

(b) AVG elephant UDP goodput

(c) AVG FCT for mice

Figure 8: Testbed experiment involving 126 TCP mice competing against 21 TCP (same variant) and 21 UDP elephants

among various ﬂow types and alleviate possible congestion in
the network core.

X. RELATED WORK

HyGenICC [1] can be comparable or complementary to
a number of works on cloud network resource allocation
that have been proposed recently. Seawall [41] is a system
proposed for sharing network bandwidth, it provides per-VM
max-min weighted fair share using explicit feedback end-to-
end congestion notiﬁcation based on losses for rate adaptation.
Seawall requires modiﬁcations to network stack which incurs a
large overhead and may interfere with middleboxes operations.
SecondNet [7] is designed to divide network among tenants
and enforce rate limits, but is limited to providing static
bandwidth reservation between pairs of VMs. Oktopus [8]
argues for predictability by enforcing a static hose model using
rate limiters. It computes rates using a pseudo-centralized
mechanism, where VMs communicate their pairwise bandwidth
consumption to a tenant-speciﬁc centralized coordinator. This
control plane overhead limits reaction times to more than 2
seconds which is inadequate for the fast changing and dynamic
trafﬁc nature in datacenters. FairCloud [42] designs better
policies for sharing bandwidth and explored fundamental trade-
offs between network utilization, minimum guarantees and
payment proportionality, for a number of sharing policies.
EyeQ [43] provides per-VM max-min weighted fair shares in
the context of a full bisection bandwidth datacenter topology
where congestion is limited to the ﬁrst and the last hops. By
simplifying rate limiters and coupling congestion control to
make them dynamic entities rather than static, HyGenICC
can achieve similar objectives as these proposals in an easy
to deploy manner with minimal CPU and network overhead.
HyGenICC is designed to operate with commodity infras-
tructure and traditional protocols used by current production
datacenter/cloud, to be a readily deployable solution. Finally,
HyGenICC can leverage the popularity of Open vSwitch (OvS)
usage by cloud management frameworks like openstack to
implement its mechanism with minor modiﬁcations to OvS
that do not require any new protocols, software and hardware.
A number of recent proposals implemented different system
designs for cloud network resources allocation. “Seawall” [41]
is a system designed solely for sharing network capacities
by achieving per-VM max-min weighted allocations using

explicit end-to-end feedback messaging for rate adaptation.
Seawall adds new encapsulation protocol to network stack on
top of transport headers which incurs a large processing and
messaging overhead as well as rendering it into a non middle-
box friendly solution. “Secondnet” [7] is a system proposed
to divide network among tenants via rate limits enforcements,
however, it only supports static bandwidth reservation among
tenants’ VMs. “EyeQ” [43] adopts per-VM max-min weighted
fair sharing in the context of a full bisection bandwidth
datacenter topology. Its downside is the design assumption that
congestion is limited to ﬁrst and last hops. “RWNDQ” [44, 45]
is a fair-share allocation AQM for TCP in data centers, however
it resolves contention among ﬂows using TCP as their transport.
RWNDQ also shares the drawback of low deployment potential
which is common to all switch solutions like [1, 46, 47].
“HyGenICC” [1] is a hypervisor-based IP-based congestion
control mechanism that relies on a collaborative information
exchange between hypervisors. The solution involves the use
of ECN marking as congestion indication which is aggregated
and fed back between hypervisors to enable network bandwidth
partitioning through dynamic (adaptive) rate limiters. In spite
of the appealing performance gains achieved. In general, these
mechanisms have some or all of the following drawbacks:

1) Security: Introduction of new protocols or using reserved
headers may interfere with the operation of the middle-
boxes (e.g., HyGenICC uses IP reserved bits known as
“Evil-bit” [48] which was used for security testing).
2) Overhead: Flow tracking and feedback packets crafting
on a per VM-to-VM basis adds burden to the hypervisor’s
processing overhead.

3) Locality: The lack of global

information about

the
dynamic network conditions results into hypervisor-based
solutions to react only to the perceived VM-to-VM
congestion (e.g., transient and non-persistent) rather than
the network-wide congestion state.

4) Mutli-Path: VM-to-VM packets are not guaranteed to
take the same path when multi-path routing (e.g., ECMP)
is used, leading to under estimation of the actual VM-to-
VM congestion by the end-points.

Recently, SDN has seen a growing number of deployments
for intra- and inter-data center networks [49–51]. SDN was
also invoked to address complex congestion events such as

051015202530Elpehant Goodput (Mb/s)0.00.20.40.60.81.0CDFRED-ECNSDN-GCC101520253035404550Elpehant Goodput (Mb/s)0.00.20.40.60.81.0CDFRED-ECNSDN-GCC0.00.10.20.30.40.50.60.70.80.9Average Response Time (sec)0.00.20.40.60.81.0CDFRED-ECNSDN-GCCTCP-Incast in data centers [2, 32, 52–56]. Hence, we address
the aforementioned drawback of the former schemes by taking
advantage of the rich information, ﬂexibility and global scope
provided by the SDN framework. We show that SDN-GCC [2]
is a middle-box and mutli-path friendly solution that achieves
similar design goals with lesser deployment overhead and lower
CPU and network overhead. This is achieved by leveraging
simple rate limiters and incorporating a network-aware SDN
controller towards building a dynamic adaptive system. The
essence of SDN-GCC is to address the increasing trend and
shift to SDN infrastructures while keeping traditional transport
protocols unchanged in current production data centers.

XI. CONCLUSION AND FUTURE WORK
In this paper, we set to build a system that relies of the
pervasive availability of SDN capable switches in data centers
to provide a centralized congestion control mechanism with
a small deployment overhead onto production data centers.
Our system achieves better bandwidth isolation and improved
application performance. SDN-GCC is a SDN framework that
can enforce efﬁcient network bandwidth allocation among
competing VMs by employing simple building blocks such
as rate limiters at the hypervisors along with an efﬁcient
SDN application. SDN-GCC is designed to operate with low
overhead, on commodity hardware, and with no assumption of
tenant’s cooperation which makes a great composition for the
deployment in SDN-based data center networks. SDN-GCC
was shown, via simulation and deployment, to efﬁciently divide
network bandwidth among active VMs, by enforcing the target
rates, regardless of the transport protocols in use.

REFERENCES
[1] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, “HyGenICC:
Hypervisor-based Generic IP Congestion Control for Virtualized
Data Centers,” in Proceedings of IEEE ICC, 2016.

[2] A. M. Abdelmoniem and B. Bensaou, “Enforcing Transport-
Agnostic Congestion Control via SDN in Data Centers,” in
IEEE Local Computer Networks (LCN), (Singapore), October
2017.
[3] Amazon,

(VPC).”

Private

Virtual

“AWS

Cloud

http://aws.amazon.com/vpc/.

[4] Open Networking Foundation, “SDN Architecture Overview,”

tech. rep., ONF TR-504, Nov 2014.

[5] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh,
S. Venkata, J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. H¨olzle,
S. Stuart, and A. Vahdat, “B4: Experience with a globally-
deployed software deﬁned wan,” in ACM SIGCOMM, 2013.
[6] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill,
M. Nanduri, and R. Wattenhofer, “Achieving high utilization
with software-driven wan,” in Proceedings of SIGCOMM, 2013.
[7] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu,
and Y. Zhang, “Secondnet: A data center network virtualization
architecture with bandwidth guarantees,” in ACM CoNext, 2010.
[8] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron, “Towards
predictable datacenter networks,” ACM CCR, vol. 41, 2011.
[9] R. Gajjala, S. Banchhor, A. M. Abdelmoniem, A. Dutta,
M. Canini, and P. Kalnis, “Huffman coding based encoding
techniques for fast distributed deep learning,” in DistributedML
workshop - CoNEXT, 2020.

[10] A. M. Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini,
“An Efﬁcient Statistical-based Gradient Compression Technique
for Distributed Training Systems,” in Machine Learning and
Systems (MLSys), 2021.

12

[11] A. M. Abdelmoniem and M. Canini, “Towards mitigating
device heterogeneity in federated learning via adaptive model
quantization,” in ACM EuroMLSys, 2021.

[12] N. G. Dufﬁeld, P. Goyal, A. Greenberg, P. Mishra, K. K.
Ramakrishnan, and J. E. van der Merive, “A ﬂexible model for
resource management in virtual private networks,” in Proceedings
of ACM SIGCOMM, 1999.

[13] M. Al-Fares, A. Loukissas, and A. Vahdat, “A scalable, com-
modity data center network architecture,” in SIGCOMM, 2008.
[14] B. A. Greenberg, J. R. Hamilton, S. Kandula, C. Kim, P. Lahiri,
A. Maltz, P. Patel, S. Sengupta, A. Greenberg, N. Jain, and D. A.
Maltz, “VL2: a scalable and ﬂexible data center network,” in
Proceedings of ACM SIGCOMM, 2009.

[15] T. Benson, A. Akella, A. Shaikh, and S. Sahu, “Cloudnaas:
A cloud networking platform for enterprise applications,” in
Proceedings of ACM Symposium on Cloud Computing, 2011.

[16] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and
M. Handley, “Improving datacenter performance and robustness
with multipath TCP,” in Proceedings of ACM SIGCOMM, 2011.
[17] A. M. Abdelmoniem, H. Susanto, and B. Bensaou, “Taming
Latency in Data centers via Active Congestion-Probing,” in IEEE
ICDCS, 2019.

[18] H. Susanto, B. L. Ahmed M. Abdelmoniem, Honggang Zhang,
and D. Towsley, “A Near Optimal Multi-Faced Job Scheduler
for Datacenter Workloads,” in IEEE ICDCS, 2019.

[19] A. M. Abdelmoniem and B. Bensaou, “Curbing Timeouts for
TCP-Incast in Data Centers via A Cross-Layer Faster Recovery
Mechanism,” in IEEE INFOCOM, 2017.

[20] A. M. Abdelmoniem and B. Bensaou, “Hysteresis-based Active
Queue Management for TCP Trafﬁc in Data Centers,” in IEEE
INFOCOM, 2019.

[21] H. Susanto, A. M. Abdelmoniem, H. Jin, and B. Bensaou, “Creek:
Inter many-to-many coﬂows scheduling for datacenter networks,”
in IEEE International Conference on Communications (ICC),
2019.

[22] J. Dong, H. Yin, C. Tian, A. M. Abdelmoniem, H. Zhou,
B. Bai, and G. Zhang, “Uranus: Congestion-proportionality
among slices based on weighted virtual congestion control,”
Computer Networks, vol. 152, pp. 154–166, 2019.

[23] A. M. Abdelmoniem and M. Canini, “DC2: Delay-aware
Compression Control for Distributed Machine Learning,” in
IEEE INFOCOM, 2021.

[24] A. M. Abdelmoniem, H. Susanto, and B. Bensaou, “Reducing
latency in multi-tenant data centers via cautious congestion
watch,” in 49th International Conference on Parallel Processing
- ICPP, ICPP ’20, (New York, NY, USA), Association for
Computing Machinery, 2020.

[25] A. M. Abdelmoniem and B. Bensaou, “T-RACKs: A Faster
Recovery Mechanism for TCP in Data Center Networks,”
IEEE/ACM Transactions on Networking, vol. 29, no. 3, 2021.

[26] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan, “Data center TCP
(DCTCP),” ACM SIGCOMM CCR, vol. 40, p. 63, 2010.
[27] H. Wu, Z. Feng, C. Guo, and Y. Zhang, “ICTCP: Incast
congestion control for TCP in data-center networks,” IEEE/ACM
Transactions on Networking, vol. 21, 2013.

[28] S. M. Irteza, A. Ahmed, S. Farrukh, B. N. Memon, and I. A.
Qazi, “On the coexistence of transport protocols in data centers,”
in Proceedings of IEEE ICC, 2014.

[29] G. Judd, “Attaining the promise and avoiding the pitfalls of TCP
in the datacenter,” in Proceedings of USENIX NSDI, 2015.
[30] R. Nishtala, H. Fugal, and S. Grimm, “Scaling memcache at

facebook,” Proceedings of 10th USENIX NSDI, 2013.

[31] Y. Lu and S. Zhu, “SDN-based TCP Congestion Control in Data
Center Networks,” in Proceedings of IEEE IPCCC, 2015.
[32] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, “SICC:
SDN-based Incast Congestion Control for Data Centers,” in
Proceedings of IEEE ICC, 2017.

[33] Open Networking Foundation, “OpenFlow Switch Speciﬁcation

V1.5.1,” tech. rep., ONF TS-025, Mar 2015.

[34] A. Panda, C. Scott, A. Ghodsi, T. Koponen, and S. Shenker,

“CAP for networks,” in ACM HotSDN workshop, 2013.

[35] NS2,

network
http://www.isi.edu/nsnam/ns.

“The

simulator

ns-2

project.”

[36] M. Karakus and A. Durresi, “A survey: Control plane scalability
issues and approaches in Software-Deﬁned Networking (SDN),”
Computer Networks, vol. 112, pp. 279 – 293, 2017.

[37] Ryu Framework Community, “Ryu: a component-based software
deﬁned networking controller.” http://osrg.github.io/ryu/.

13

in an Action: Rapid Precise Packet Loss Notiﬁcation in Data
Center,” in Proceedings of USENIX NSDI, pp. 17–28, 2014.

[47] A. J. Abu, B. Bensaou, and A. M. Abdelmoniem, “A Markov
Model of CCN Pending Interest Table Occupancy with Interest
Timeout and Retries,” in IEEE International Confereence on
Communications (ICC), 2016.

[48] C. Bellovin, “The security ﬂag in the ipv4 header,” 2003.

https://www.ietf.org/rfc/rfc3514.txt.

[49] Scott Raynovich, “A Look at Key SDN Deployments.”

https://www.sdxcentral.com/articles/analysis/key-sdn-
deployments/2016/06/.

[38] OpenvSwitch,

“Open

Virtual

Switch

project.”

[50] N. Feamster, J. Rexford, and E. Zegura, “The Road to SDN,”

http://openvswitch.org/.

[39] iperf,

“The TCP/UDP Bandwidth Measurement Tool.”

https://iperf.fr/.

[40] Apache.org, “Apache HTTP server benchmarking tool.”

http://httpd.apache.org/docs/2.2/programs/ab.html.

[41] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha,
“Sharing the data center network,” in USENIX NSDI, 2011.
[42] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Rat-
nasamy, and I. Stoica, “FairCloud: Sharing the Network in Cloud
Computing,” ACM SIGCOMM CCR, vol. 42, pp. 187–198, 2012.
[43] V. Jeyakumar, M. Alizadeh, D. Mazi`eres, B. Prabhakar, C. Kim,
and A. Greenberg, “Eyeq: Practical network performance isola-
tion at the edge,” in Proceedings of USENIX NSDI, 2013.
[44] A. M. Abdelmoniem and B. Bensaou, “Reconciling Mice and
Elephants in Data Center Networks,” in IEEE International
Conference on Cloud Networking (CloudNet), 2015.

[45] A. M. Abdelmoniem and B. Bensaou, “Efﬁcient Switch-Assisted
Congestion Control for Data Centers: an Implementation and
Evaluation,” in IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2015.

[46] P. Cheng, F. Ren, R. Shu, and C. Lin, “Catch the Whole Lot

Queue, vol. 11, pp. 20–40, 2013.

[51] I. F. Akyildiz, A. Lee, P. Wang, M. Luo, and W. Chou, “A
roadmap for trafﬁc engineering in SDN-OpenFlow networks,”
Computer Networks, vol. 71, pp. 1–30, 2014.

[52] A. M. Abdelmoniem and B. Bensaou, “Incast-Aware Switch-
Assisted TCP Congestion Control for Data Centers,” in IEEE
Global Communications Conference (GlobeCom), 2015.
[53] A. M. Abdelmoniem, B. Bensaou, and V. Barsoum, “IncastGuard:
An Efﬁcient TCP-Incast Congestion Effects Mitigation Scheme
for Data Center Network,” in IEEE International Conference on
Global Communications (IEEE GlobeCom),, 2018.

[54] A. J. Abu, B. Bensaou, and A. M. Abdelmoniem, “Leveraging
the Pending Interest Table Occupancy for Congestion Control
in CCN,” in IEEE Local Computer Networks (LCN), 2016.
[55] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, “Mitigating
incast-tcp congestion in data centers with sdn,” Annals of
Telecommunications, vol. 73, no. 3, pp. 263–277, 2018.
[56] A. M. Abdelmoniem, Y. M. Abdelmoniem, and B. Bensaou, “On
Network Systems Design: Pushing the Performance Envelope
via FPGA Prototyping,” in IEEE international Conference on
Recent Trends in Computer Engineering (IEEE ITCE), 2019.

