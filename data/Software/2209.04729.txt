The Switch from Conventional to SDN: The Case
for Transport-Agnostic Congestion Control

Ahmed M. Abdelmoniem and Brahim Bensaou

1

2
2
0
2

p
e
S
0
1

]
I

N
.
s
c
[

1
v
9
2
7
4
0
.
9
0
2
2
:
v
i
X
r
a

Abstractâ€”To meet

the timing requirements of

interactive
applications, the no-frills congestion-agnostic transport protocols
like UDP are increasingly deployed side-by-side in the same
network with congestion-responsive TCP. In cloud platforms,
even though the computation and storage is totally virtualized,
they lack a true virtualization mechanism for the network (i.e.,
the underlying data centers networks). The impact of such lack
of isolation services, may result into frequent outages (for some
applications) when such diverse trafï¬cs contend for the small
buffers in the commodity switches used in data centers. In
this paper, we explore the design space of a simple, practical
and transport-agnostic scheme to enable a scalable and ï¬‚exible
end-to-end congestion control in data centers. Then, we present
the the shortcomings of coupling the monitoring and control
of congestion in the conventional system and discuss how a
Software-Deï¬ned Network (SDN) would provide an appealing
alternative to circumvent the problems of the conventional system.
The two systems implements a software-based congestion control
mechanisms that perform monitoring, control decisions and
trafï¬c control enforcement functions. Both systems are designed
with a major assumption that the applications (or transport
protocols) are non-cooperative with the system, ultimately mak-
ing it deployable in existing data centers without any service
disruption or hardware upgrade. Both systems are implemented
and evaluated via simulation in NS2 as well as real-life small-scale
test-bed deployment and experiments.

Index Termsâ€”Congestion Control, Data Center Networks,
Rate Control, Open vSwitch, Software Deï¬ned Networks, Vir-
tualization

I. INTRODUCTION

To achieve isolation among tenants and use resources more
efï¬ciently, resource virtualization has become a common prac-
tice in todayâ€™s public data centers. In most cases, each tenant
is provisioned with virtual machines assigned with dedicated
virtual CPU cores, memory, storage, and a virtual network
interface card (NIC) that sends trafï¬c over the underlying shared
physical NIC. Typically, tenants can not assume predictability
nor measurability of bounds on network performance, as no
mechanisms are deployed to explicitly allocate and enforce
bandwidth in the cloud. Nevertheless, cloud operators can
provide tenants with better virtual network management thanks
to the recent developments in control plane functions. For
example, Amazon introduced â€œVirtual Private Cloud (VPC)â€
[3] to allow easy creation and management of tenantâ€™s private
virtual network. VPC can be viewed as an abstraction layer

Ahmed M. Abdelmoniem is currently with School of EECS, Queen Mary
University of London and CS Department, Assiut University, Egypt. E-mail:
ahmed.sayed@qmul.ac.uk

Brahim Bensaou is currently with Department of Computer Science and

Engineering, HKUST, Hong Kong SAR, PRC.

This work combines works published in IEEE ICC 2016 [1] and IEEE LCN

2017 [2].

running on top of the non-isolated, shared network resources of
AWSâ€™ public cloud. Additionally, Software Deï¬ned Networking
(SDN) [4] has been effectively deployed to drive inter- and
intra-data center communications with added features to make
the virtualization and other network aspects easy to manage. For
example, both Google [5] and Microsoft [6] have deployed fully
operational SDN-based WAN networks to support standard
routing protocols as well as centralized trafï¬c engineering
between their data centers.

In contrast, the data plane in intra-datacenter networks
has seen little progress in managing bandwidth to overcome
congestion, improve efï¬ciency, and apportioning it adequately
to provide isolation between competing tenants to meet their
target performance requirements. In principle, isolation can
simply be achieved through static reservation [7â€“11], where
tenants can enjoy a predictable, congestion-free network
performance. However, static reservations lead to inefï¬cient
utilization of the network capacity. To avoid such pitfall, tenants
should be assigned a minimum bandwidth using the so-called
hose model [12] which abstracts the network between the VMs
of one tenant as a single virtual switch (vSwitch). In such
setup, different VMs may reside on any physical machine in
the datacenter, yet, each VM should be able to send trafï¬c at the
full virtual port rate as determined by the vSwitch abstraction
layer. Such VMs should enjoy the allocated rate regardless of
the trafï¬c patterns of co-existing VMs and/or the nature of the
workload generated by competing VMs.

The following are the necessary elements that can be

incorporated together for this purpose:

â€¢ An intelligent and scalable VM admission mechanism
within the datacenter for VM placement where minimum
bandwidth is available. To facilitate this, topologies with
bottlenecks at the core switches (such as uplink over-
subscription or a low bisection bandwidth) should be
avoided if possible.

â€¢ A methodology to fully utilize the available high bisection
bandwidth (e.g., a load balancing mechanism and/or multi-
path transport/routing protocols).

â€¢ A rate adaptation technique to ensure conformance of
VM sending rates to their allocated bandwidth, while
penalizing misbehaving ones.

A number of interesting research works have investigated
more or less successfully the ï¬rst two elements of this frame-
work [13â€“25]. In [13, 14], highly scalable network topologies
offering a 1:1 over-subscription and a high bisection bandwidth
were proposed. These topologies are shown to be easily
deployable in practice and can simplify the VM placement at
any physical machine with sufï¬cient bandwidth to support the

 
 
 
 
 
 
VM. Efï¬cient routing and transport protocols [15, 16] were
designed for DCN to achieve a high utilization of the available
capacity. Finally, in terms of trafï¬c control, much of the recent
work [26, 27] focused on restructuring TCP congestion control
and its variants to efï¬ciently utilize and fairly share bandwidth
among ï¬‚ows (in homogeneous deployments). However, these
techniques fall short of providing true isolation among tenants
(e.g., a tenant may gain more bandwidth by opening parallel
connections or by using aggressive transport protocol like
UDP). It is common, in multi-tenant environments, that non-
homogeneous transport protocols co-exist leading to starvation
of the cooperative ones [28]. To illustrate this problem simply,
we conduct a set of simulations in which we compare the
performance of a tagged ECN-enabled TCP (NewReno) ï¬‚ow
that competes head-to-head i) against another TCP ï¬‚ow of
the same type, ii) against another ï¬‚ow using a TCP variant
designed for data centers (i.e., DCTCP which is deployed in a
number of private data centers [29]), and; iii) against another
congestion-agnostic transport protocol (i.e., UDP which is used
in MemCacheD clusters of Facebook [30]); the results from
the three experiments are superimposed and presented on the
same graphs. Similar to what was already known from the
Internet, Figure 1 shows that, homogeneous TCP deployments
in data centers can achieve fairness, in contrast to heterogeneous
deployments. In particular, we observe in Figure1a that when
TCP is not responsive to ECN markings, it enjoys almost â‰ˆ 0%
its share during the periods where it competes against DCTCP
or UDP. In contrast, being responsive to ECN markings helps
TCP improve its performance but still ultimately the fairness is
not achieved. In Figure 1b, we observe that even when TCP is
ECN-enabled it still loses â‰ˆ 60% and â‰ˆ 72% of its fair share
to DCTCP and UDP, respectively.

In this paper, we rely on the SDN capability of most data
center switching devices to propose a generic congestion control
(SDN-GCC) mechanism to address this issue. We ï¬rst introduce
the idea behind SDN-GCC in Section III, then discuss our
proposed methodology and present SDN-GCC framework in
Section VI. We show via ns2 simulation how SDN-GCC
achieves its requirements with high efï¬ciency in Section VIII,
then present testbed experiments in Section IX1. Finally, we
conclude the paper in Section XI.

II. TRANSPORT ISOLATION PROBLEM

With the recent introduction of a signiï¬cant number of
new transport protocols designed for DC networks in addition
to the existing protocols,
the following three challenges
emerged: i) most such protocols are agnostic to the nature
of the VM aggregate trafï¬c demands leading to inefï¬cient
distribution of the network capacity among competing VMs
(for instance a VM could gain more bandwidth by opening
parallel TCP connections); ii) many versions of TCP co-
exist in DC networks (e.g., TCP NewReno/MacOS, compound
TCP/Windows, Cubic TCP/Linux, DCTCP/Linux, and so on),
leading to further inefï¬ciency in addition to unfairness, and; iii)

1Simulation and implementation code can be requested from the authors
or downloaded from the following repository after they are made publicly
available: http://github.com/ahmedcs/SDN-GCC

2

(a) ECN disabled

(b) ECN enabled

Figure 1: The instantaneous and mean goodput of the tagged TCP
ï¬‚ow (in the legends) while competing with 3 TCP, DCTCP
or UDP senders. The link capacity is 1Gbps. In interval
[0,10] only the competitors are active, in [10,20] all the
ï¬‚ows are active and in [20,30] only the tagged TCP ï¬‚ow
is active.

many DC applications rely on UDP to build custom transport
protocols (e.g., [30]), that are not responsive to congestion
signals, which exacerbate the unfairness to the point of causing
starvation to congestion-responsive ï¬‚ows. While such problems
have been revealed in the context of Internet communications,
two decades ago, recent studies [28, 29] have conï¬rmed that
such problems of unfairness and bandwidth inefï¬ciency also
exist in DCNs despite their characteristically small delays,
small buffers and different topologies from those found in the
Internet. As a consequence, a new solution to the problems of
congestion in DC networks is needed. Such solution must be
attractive to cloud operators and cloud tenants alike.

In particular, with the emergence of software deï¬ned
networking, we see an opportunity to invoke the powerful
control features and the global scope provided by SDN to
revisit the problem from a different perspective, with additional
realistic design constraints. As such we propose a solution with
the following intuitive design requirements: R1) Simplicity:
to be readily deployable in existing production data centers;
R2) Transport-agnosticism: to be effective regardless of the
transport protocol; R3) Transparency: requires no changes to
the tenantâ€™s OS (in the VM) and makes no assumption of
any advanced network hardware capability other than those
available in commodity SDN switches; R4) Load-effectiveness:
creates a minimal processing overhead on the end-host.

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (416)DCTCP (319)UDP (153)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (416)DCTCP (375)UDP (359)All of todayâ€™s communication infrastructure from hardware
devices to communication protocols have been designed with re-
quirements derived from the global Internet. As a result to cope
with scalability and AS autonomy, the decentralized approach
has been adopted, relinquishing all intelligence to end systems.
Yet, to enable responsiveness to congestion regardless of the
transport protocol capabilities, in time-scales that commensurate
with data center delays, it is preferable to adopt centralized
control as it provides a global view of congestion and is
known to achieve far better performance [31, 32]. Nevertheless
to reconcile existing hardware and protocols (designed for
distributed networks) with the centralized approach, we impose
design requirements R1-R4 on SDN-GCC. As such the core
design of SDN-GCC relies on outsourcing the congestion
control decisions to the SDN controller while the enforcement
of such decisions is carried out by the end-hosts hypervisors.

III. INTRODUCTION TO HYGENICC

To enable responsiveness to congestion regardless of the
transport protocol, one needs to return to the fundamentals
and put the burden of congestion control in principle where it
belongs: in the network layer. As such, in principle, such
congestion control mechanism must be transparent to the
transport layer protocol. However, to reconcile the principle
with the practice, design requirements R1-R4 must be fulï¬lled
and thus HyGenICC outsources its congestion control building
blocks to the hypervisor.

To meet requirement R1, HyGenICC can be implemented
either as a hypervisor-level shim-layer or as an added feature
to any of the current commercial virtual switchesâ€™ data-path
module. The job of the added shim-layer to the hypervisor
is to enforce per-VM rate control without VM cooperation
nor any knowledge about its trafï¬c patterns, workloads, or
used transport protocol (TCP/UDP). To this end, HyGenICC
maintains a rate allocation mechanism at each server to partition
the available uplink bandwidth among VMs locally at the
sending and receiving servers. In each such server, HyGenICC
only needs to maintain state information per VM which meets
design requirement R4. HyGenICC deploys a simple hypervisor-
to-hypervisor (IP-to-IP) congestion control mechanism that
relies on ECN markings (readily available in commodity
switches) to infer core network congestion. HyGenICC operates
at the IP level and does not interact directly with the VMs,
which meets requirements R1, R2 and R3. In addition, when
detecting a highly congested path in the core network towards a
destination (via ECN), HyGenICC performs admission control
by refraining from accepting any further connections to this
destination VM until the congestion subsides. Our design is
highly scalable, responsive, work conserving and since it is IP
based, it enforces the allocated bandwidth even in the presence
of highly dynamic and changing trafï¬c patterns and transport
protocols. The rate allocator resolves the contention among
tens-to-hundreds of co-located VMs at the servers, while the
congestion control mechanism addresses the contention in the
network core and pushes it back to the sources. HyGenICC
also allows administrators to assign per-VM weights which
directly affect the bandwidth reservation for the VMs making

3

Table I: Flow attributes and variables tracked in our mechanism

Entry name (VM-to-VM)
ğ‘ ğ‘œğ‘¢ğ‘Ÿ ğ‘ğ‘’
ğ‘‘ğ‘’ğ‘ ğ‘¡
ğ‘œğ‘¢ğ‘¡ ğ‘ğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡
ğ‘– ğ‘ğ‘Ÿ ğ‘ğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡
ğ‘’ğ‘ğ‘› ğ‘ğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡

Variable name (per VM)
ğ‘Ÿ ğ‘ğ‘¡ğ‘’
ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡
ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ 

Description
IP address of source VM
IP address of destination VM
Sent packets count
Received packets with â€œIPR-bitâ€ mark
Received packets with ECN mark
Description
The share rate or speed of NIC
The capacity of the token bucket in bytes
The number of available tokens to be used for transmission

it appealing from cloud providersâ€™ perspective as it enables
easier and more tangible bandwidth pricing and accounting.

IV. PROPOSED METHODOLOGY
First we discuss HyGenICC by imagining the datacenter
network as contained within one end-host where the VMs are
connected via a single virtual switch. Then, we extend this
design to operate in a network of end-hosts where the datacenter
fabric is treated as black box that generates congestion signals
whenever congestion is experienced. In a single virtual switch
connecting all VMs, bandwidth contention happens at the
output link to the destination when multiple senders compete
to send through the same output port of the virtual switch.
The virtual switch need to distribute the available physical
portâ€™s capacity among VMs and ensure compliance of the VMs
with the allocated shares. Hence it needs a mechanism that
detects and accounts for active VMs and apply rate limiters
on a per-VM basis to share the bandwidth among them.

HyGenICC deploys a ï¬‚ow table (for congestion control
purpose) to track state information shown in Table I on a
VM-to-VM granularity (i.e., source VM-destination VM pairs).
In addition, per-VM token-bucket state is used to enforce the
VMâ€™s share of bandwidth.

A. VM detection and bandwidth allocation

As soon as a VMâ€™s port becomes active (sending or receiving
trafï¬c), an associated entry is created in the ï¬‚ow table.
Whenever a new VM becomes active on a given NIC, the NICâ€™s
nominal capacity is redistributed among the token buckets
of active VMs to account for the new one. This is done by
readjusting the rate and bucket size of all active VMsâ€™ token
buckets on that NIC. Any extra trafï¬c sent by the VM in excess
of its share is simply dropped and resent later by the transport
layer or otherwise a per-VM queue is used for holding the trafï¬c
for later transmission whenever the tokens are regenerated2.

B. Congestion Control Mechanism

In practice, congestion may always happen within the
network as shown in Figure 2, if the network is over-subscribed
or does not provide full bisection bandwidth. HyGenICC
therefore relies on readily available features in switches
hardware3 , to convey congestion signals to the sources. To be

2We have experimented with both approaches and the queuing mechanism
achieves slightly better performance which did not motivate its usage due to
management and memory overhead.

3Most current commodity switches used in data centers are equipped with
QoS mechanisms like Strict Priority (SP), Weighted Fair Queuing (WFQ) and
Weighted Random Early Detection (WRED) in addition to the ability of ECN
marking of IP packets.

4

Ethernet headers) and piggybacks explicitly the number of
remaining ECN marks on the identiï¬cation ï¬eld of this IP
packet. The IP protocol ï¬eld is destined to an unused number
that has meaning only for HyGenICC.

At the sender, to match the current sending rate to the
congestion level in the network, upon receiving â€œIPR-bitâ€
marks or the special packet, the source decreases the VMâ€™s
current allocated rate in proportion to the amount of marks
and gradually increases the rate when no congestion bits are
received in a period.

Figure 2: HyGenICC high-level system design

V. IMPLEMENTATION

more abstract, HyGenICC treats the datacenter network as a
black box in which source servers inject trafï¬c and the black
box generates ECN marks in response to congestion towards
the receivers. ECN marks are a fast proactive mechanism that
can help in quickly detecting any congestion from a shared
queue when buffers exceed a pre-conï¬gured queue occupancy
threshold along a packetâ€™s path.

HyGenICC uses the ï¬‚ow table to track, for each source-
destination pair, the number of IP packets received with
congestion notiï¬cation marks, regardless of the type of transport
protocol (TCP, UDP, or otherwise). This information is a
valuable indication of the level of congestion along the path
between the source VM and the destination VM starting at that
particular NIC. Since HyGenICC implements a network-layer
congestion control, any ECN or other marking used to track
congestion is cleared before delivering the datagrams to the
VM4. In addition, to force universal ECN marking along the
path, all outgoing packets are marked with the ECN-enabled
bit. HyGenICC typically creates a network layer congestion
control loop between hypervisors and is fully transparent to
the overlying VM transport protocol.

At the receiver side, upon receiving ECN marks, HyGenICC
needs to reï¬‚ect the information back to the source to trigger
reduction of the sending rate of that particular source VM.
To avoid introducing any additional overhead and hinder the
operation of any on-path middle-boxes by introducing a new
protocol, we propose to piggyback the information on any
returning data. For this we identify three types of trafï¬c
ï¬‚ows: TCP, which is by default bidirectional, other non-TCP
bidirectional trafï¬c and ï¬nally unidirectional trafï¬c; for the
three categories of trafï¬c, we propose to use the unused reserved
bit in the IP header â€œIPR-bitâ€ of any reverse packet to reï¬‚ect
the ECN marking synchronously to the origin. While this might
be sufï¬cient for the ï¬rst two categories of trafï¬c to carry all
marking back to the source, for the third category, there might
be a dramatic imbalance in the forward trafï¬c and reverse
trafï¬c leading to some proportion of forwarded markings not
being reï¬‚ected back. As a solution HyGenICC crafts a special
small IP packet with header only (20 bytes of IP and 14 for

As explained above, HyGenICC needs two mechanisms: rate
limiters at the source server and congestion controller that run
from source to destination server. These mechanisms can either
be implemented in software, or hardware or a combination of
both as necessary. We simpliï¬ed the design and concepts of
HyGenICC so that the built system is able to maintain line
rate performance at 1-10Gb/s while reacting quickly to deal
with congestion within a datacenterâ€™s short RTT time scale.

A. HyGenICC sender

HyGenICC sender processing is described in Algorithm 1. At
the senders HyGenICC tracks the rate, the number of tokens,
the depth of the bucket and the ï¬ll-rate variables per-VM
per-NIC where the per-VM rate limiters are implemented as
counting token buckets that have a rate ğ‘…(ğ‘–, ğ‘—) each, a bucket
capacity ğµ(ğ‘–, ğ‘—) each and number of tokens ğ‘‡ (ğ‘–, ğ‘—) each. In
addition, the sender will also handle the received congestion
signals from different destinations on a per-source basis.

1) Rate Allocation: Initially, the installed on-system NICs
are probed and the values of their nominal data rate ğ‘…(ğ‘–), bucket
capacity ğµ(ğ‘–) and tokens ğ‘‡ (ğ‘–) are calculated correspondingly.
Thereafter, when packets start ï¬‚owing from each source VM,
NIC capacities are redistributed and a new capacity share
â€œğ¶ğ‘ ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦ ğ‘†â„ğ‘ğ‘Ÿğ‘’â€ is calculated and used to update the entries
for each active VM in the rate, tokens and bucket matrices and
the VM is marked as currently active on all outgoing physical
NICs.

After a certain time of inactivity5, the bucket entries for a
VM are reset and its allocation is reclaimed and redistributed
among currently active VMs. As shown in Table I, ï¬‚ow-table
entries are established immediately after arrival of the ï¬rst
packet using source-destination IP address6. First, on arrival or
departure of each packet ğ‘ƒ, its outgoing port ğ‘— and incoming
port ğ‘– are detected. The current value of available tokens ğ‘‡ (ğ‘–, ğ‘—)
is retrieved and replenished based on the elapsed time since
the last transmission. Then, using the new ğ‘‡ (ğ‘–, ğ‘—), the packet is
allowed for transmission if ğ‘‡ (ğ‘–, ğ‘—) â‰¥ ğ‘ ğ‘–ğ‘§ğ‘’( ğ‘ğ‘˜ğ‘¡), in this case the
packet length is deducted from ğ‘‡ (ğ‘–, ğ‘—), otherwise the packet is
dropped.

4Supposedly, If the tenants are willing to deploy ECN in their overlay
networks then our mechanism should not clear ECN mark and let the transport
layer handle it as well. We believe the two would not conï¬‚ict rather complement
each other as shown in the simulations.

5Inactivity timeout is set to 1 sec in simulations.
6We track the state of the communicating VM pairs not

individual
ï¬‚ows plus we ï¬nd that the operations involved do not add burden on the
hypervisor/vswitch.

IPR bitECN bitHyGenICCSenderHyGenICCReceiverServer1Server2HypervisorORvSwitchNIC1VM2VM1VM3NIC2DCNAlgorithm 1: HyGenICC Sender Algorithm
1 Function Packet Departure(ğ‘ƒ, ğ‘–, ğ‘—)
2

look up ï¬‚ow entry ğ‘“ in ï¬‚ow table
ğ‘‡ (ğ‘–, ğ‘—) = ğ‘‡ (ğ‘–, ğ‘—) + ğ‘…(ğ‘–, ğ‘—) Ã— (ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘“ .ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’)
ğ‘‡ (ğ‘–, ğ‘—) = ğ‘€ ğ¼ ğ‘ (ğµ(ğ‘–, ğ‘—), ğ‘‡ (ğ‘–, ğ‘—)) if
ğ‘‡ (ğ‘–, ğ‘—) â‰¥ ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘ƒ) then

ğ‘‡ (ğ‘–, ğ‘—) = ğ‘‡ (ğ‘–, ğ‘—) âˆ’ ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘ƒ) ğ‘“ .ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤()
Enable ECN Capable bits (ECT) in IP header

3

4

5

else

Drop the packet

6 Function Packet Arrival(ğ‘ƒ, ğ‘–, ğ‘—)
7

look up ï¬‚ow entry ğ‘“ in ï¬‚ow table if Packet is
congestion feedback message then

8

9

10

11

ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ = ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ + ğ‘–ğ‘›ğ‘¡ (ğ‘ƒ.ğ‘‘ğ‘ğ‘¡ğ‘)
ğ‘“ .ğ‘Ÿğ‘ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤() Drop the packet

else

if Packet is â€œIPR-bitâ€ marked then
ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ = ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ + 1
ğ‘“ .ğ‘Ÿğ‘ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’
ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤()
Clear the mark and forward to the VM

12 Function Timer timeout
13

forall ï¬‚ow ğ‘“ in ğ¹ğ‘™ğ‘œğ‘¤ğ‘‡ ğ‘ğ‘ğ‘™ğ‘’ do

14

15

16

17

18

19

20

21

22

23

24

25

26

if ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘“ .ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’ â‰¥ 1ğ‘ ğ‘’ğ‘ then

ğ‘“ .ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
Reset ğ‘“ entry in Flow Table
Redistribute NIC capacity among active ï¬‚ows

forall Active ï¬‚ow ğ‘“ in ğ¹ğ‘™ğ‘œğ‘¤ğ‘‡ ğ‘ğ‘ğ‘™ğ‘’ do

if now() - f.feedbacktimeâ‰¥ Congestion Timeout
then

ğ‘“ .ğ‘Ÿğ‘ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’
if ğ‘“ .ğ‘Ÿğ‘ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ == ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’ then

ğ‘…(ğ‘–, ğ‘—) = ğ‘…(ğ‘–, ğ‘—) + ğ‘ ğ‘ğ‘ğ‘™ğ‘’(ğ‘ ğ¼ğ¶ ğ¶ğ‘ ğ‘)

else

if ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ â‰¥ 0 then

ğ‘…(ğ‘–, ğ‘—) =
ğ‘…(ğ‘–, ğ‘—) âˆ’ ( ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ Ã— ğ‘ ğ‘ğ‘ğ‘™ğ‘’(ğ‘ ğ¼ğ¶ ğ¶ğ‘ ğ‘))

else

ğ‘…(ğ‘–, ğ‘—) = ğ‘…(ğ‘–, ğ‘—) + ğ‘ ğ‘ğ‘ğ‘™ğ‘’(ğ‘ ğ¼ğ¶ ğ¶ğ‘ ğ‘)

ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ = 0

ğ‘…(ğ‘–, ğ‘—) =
ğ‘€ ğ´ğ‘‹ (0, ğ‘€ ğ¼ ğ‘ (ğ¶ğ‘ ğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ‘¦ ğ‘†â„ğ‘ğ‘Ÿğ‘’, ğ‘…(ğ‘–, ğ‘—)))

2) Congestion Reaction: The sender module reacts on
regular intervals to incoming â€œIPR-bitâ€ and cuts the sending rate
in proportion to the amount of marking received. Hence, sources
causing congestion in the network will receive â€œIPR-bitâ€ signals
and will react by decreasing their sending rates proportionally
until the congestion subsides and congestion signals start
disappearing at which time sources start to gradually increase
their rates. The process will increase the rate conservatively, and

5

if no feedback arrives within ğ¶ğ‘œğ‘›ğ‘”ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘‡ğ‘–ğ‘šğ‘’ğ‘œğ‘¢ğ‘¡ seconds,
the rate is increased fast until it reaches its â€œCapacity Shareâ€
or an â€œIPR-bitâ€ is detected again. Function â€scale(NIC Cap)â€œ
is used to scale the amount of rate increase and decrease to
account for a single packets transmission over a single RTT (i.e.,
1000 bytes over 1 Gb/s in average RTT range of 100ğœ‡s-10ms
would give us â‰ˆ 80-8 Mb/s of increments).

B. HyGenICC receiver

At the receiver, HyGenICC needs to track incoming conges-
tion ECN marks from the network on a per-source-destination
basis and feed this information back by piggybacking it on
outgoing packets heading back to corresponding sources. Hence,
the operations of the receiver is quite simple and does not incur
much processing overhead onto incoming trafï¬c. The receiver
processing is described in Algorithm 2.

Algorithm 2: HyGenICC Receiver Algorithm
1 Function Packet Arrival(ğ‘ƒ, ğ‘–, ğ‘—)
2

look up ï¬‚ow entry ğ‘“ in ï¬‚ow table if Packet is
ECN marked then

3

4

5

ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  = ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  + 1
Clear the mark and forward to the VM
if now() - f.feedbacksenttime â‰¥ feedback timeout
then

Create IP feedback message and send to ğ‘“ .ğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’

ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤()
ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  = 0

6 Function Packet Departure(ğ‘ƒ, ğ‘–, ğ‘—)
7

look up ï¬‚ow entry ğ‘“ in ï¬‚ow table if

ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  â‰¥ 1 then

8

Set â€œIPR-bitâ€ ï¬‚ag in IP header
ğ‘“ . ğ‘“ ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤()
ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  = ğ‘“ .ğ‘’ğ‘ğ‘›ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  âˆ’ 1

Each incoming packet is checked for ECN mark and the
number of packets with and without the mark are traced in
the ï¬‚ow table, Table I, and immediately the ECN mark is
cleared before re-injecting the packet in the normal packet
processing path. For each ECN marked packet, an IPR-bit
mark is reï¬‚ected in the ï¬rst available outgoing packet to that
destination (it could be a TCP ACK if the ï¬‚ow is TCP or a
UDP reply data packet) until all the ECN marks are cleared.
However, when ingress and egress trafï¬c are out of balance on
a given ï¬‚ow, non-reï¬‚ected ECN marks may start to accumulate
at the receiver, to address this issue, we periodically use an
explicit ICMP-like feedback packet to convey the remaining
amount of ECN marks to the source. On a regular intervals
close to an RTT, we scan through the ï¬‚ow table asynchronously
for any ï¬‚ow with remaining ECN marks and that has not sent
any feedback for a period of ğ¹ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜ ğ‘‡ğ‘–ğ‘šğ‘’ğ‘œğ‘¢ğ‘¡. If any is
found, then an IP packet is created with unused protocol ID
value and the current value of ECN marks added as a 2-bytes
payload of this packet addressed to the source of the ï¬‚ow. This
event is infrequent and unlikely to exist but if so, will not

6

of all active VMsâ€™ token buckets on that NIC. Then we need
to extend the allocation of single hypervisor to account for
the in-network congestion caused by a network of hypervisors
managing tenantsâ€™ VMs.

Table II: Variables and parameters used by the SDN controller

application and end-host shim-layer of SDN-GCC

Parameter name
ğ‘‡ğ‘œ
ğ‘‡ğ‘
ğ‘‡ğ‘–
Variable name (Shim-Layer)
ğ‘ ğ‘œğ‘¢ğ‘Ÿ ğ‘ğ‘’
ğ‘£ ğ‘ğ‘œğ‘Ÿ ğ‘¡
ğ‘Ÿ ğ‘ğ‘¡ğ‘’
ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡
ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ 
ğ‘ ğ‘’ğ‘›ğ‘¡ ğ‘¡ğ‘–ğ‘šğ‘’

Variable name (SDN APP)
ğ‘†ğ‘Š ğ¼ğ‘‡ ğ¶ ğ»
ğ‘†ğ‘Š ğ¼ğ‘‡ ğ¶ ğ» ğ‘ƒğ‘‚ğ‘…ğ‘‡
ğ·ğ‘†ğ‘‡ ğ‘†ğ‘…ğ¶
ğ¼ ğ‘ƒğ‘‡ ğ‘‚ğ‘ƒğ‘‚ğ‘…ğ‘‡
ğ‘€ ğ´ğ‘…ğ¾ ğ‘†

Description
Timeout for Flow inactivity period
Timeout for Congestion grace period
Timeout of Congestion monitor period
Description
IP address of source VM
virtual port connecting VM
The allocated sending rate
The capacity of the token bucket in bytes
The number of available tokens
The time-stamp of last transmission
Description
List of the controlled SDN switches
List of the ports on the switches
List of destinations to sources pairs
List of IP to switch port pairs
ECN marks reading of for each switch port

In practice, congestion may always happen within the data
center network, if the network is over-subscribed or does
not provide full bisection bandwidth. SDN-GCC in an effort
to account for this limitation, relies on readily available
functionality in SDN switches to convey congestion events
to the controller. To elaborate more, SDN-GCC controller can
keep a centralized record of congestion statistics by periodically
collecting state information from the switches as shown in
Table II. ECN marking is chosen as a fast live-congestion
indication to signal the onset of possible congestion at any
shared queue. However, Usage of RED and ECN marking
could be avoided if drop-tail AQM keeps statistics of backlog
exceeding a certain pre-set threshold.

SDN-GCC application running on top of the network OS,
keeps record of each network-wide state information (e.g.,
congestion points). Hence, it can infer the bottleneck queues
based on this information and make intelligent decisions
accordingly. Whenever necessary, it sends special congestion
notiï¬cations to the shim-layer to adjust the sending rate of the
affected VM. Upon receiving any congestion notiï¬cation The
shim-layer reacts by adjusting VMâ€™s rate-limiter proportionally
to the congestion level in the network and gradually increases
the rate when no more congestion messages are received.

VII. DESIGN AND IMPLEMENTATION

As explained above, SDN-GCC needs two components:
shim-layer at the servers and the control application that runs
on top of the network OS. These mechanisms can either be
implemented in software, or hardware or a combination of
both as necessary. We simpliï¬ed the design and concepts of
SDN-GCC so that the built system is able to maintain line rate
performance at 1-10Gb/s while reacting quickly to deal with
congestion within a reasonable time.

A. SDN-GCC End-Host Shim-Layer

SDN-GCC shim-layer processing is described in Algorithm
3. The major variables it tracks are the rate, the number of

Figure 3: SDN-GCC high-level system design: 1) congestion point;
2) network statistics; 3) congestion tracking; 4) congestion
notiï¬cation; 5) rate adjustment.

incur much network overhead as the packet size would be 36
bytes (14-bytes Ethernet header + 20-bytes IP header + 2-bytes
payload data). To compress further the explicit feedback, the
2 bytes payload can be piggybacked instead in the IP header
identiï¬cation ï¬eld.

VI. PROPOSED METHODOLOGY

Figure 3 shows SDN-GCCâ€™s system design which is broken
down into two parts: a network application that runs on the
SDN controller (network OS). It is responsible for monitoring
network states by querying the switches periodically via SDNâ€™s
standard southbound API and signaling congestion; and a
hypervisor-based shim-layer, that is responsible of enforcing
per-VM rate control in response to congestion notiï¬cation by
the control application. The following scenario sketches the
SDN-GCC cycle: 1) Whenever the total incoming load exceeds
the link capacity, the link (in-red) becomes congested implying
that senders are exceeding their allocated rates. 2) SDN-
switches sends to the network OS periodic keep-alive and
statistics through the established control plane between them
(e.g., OpenFlow or sFlow). Whenever necessary, the switch
would report the amount of congestion experienced by each
output queue of its ports. 3) The SDN-GCC application co-
located with the network OS (or alternatively communicating
via the north-bound API) tracks congestion events in the
network. 4) SDN-GCC application communicates with the
SDN-GCC shim-layer of the sending servers whose VMs
are causing the congestion. 5) SDN-GCC shim-layer takes
a corrective action by adjusting the rate-limiter of the target
VM.

We start from a single end-host (hypervisor) connecting all
VMs where bandwidth contention happens at the output link
(i.e., when multiple senders compete to send through the same
output NIC of the virtual switch). The hypervisor needs to
distribute the available NICâ€™s capacity among VMs and ensure
compliance of the VMsâ€™ weights with the allocated shares.
Hence it employs a mechanism to apply rate limiters on a per-
VM basis. Table II shows the variables needed to implement
a per-VM token-bucket rate limiter. Ideally, when a virtual
port becomes active, its variables are initialized and the NICâ€™s
nominal capacity is redistributed among the rate limiters of
currently active VMs by readjusting the rate and bucket size

Network Operating System (NOS)North-Bound APISouth-Bound APIControllerRoutingFirewallSDN-GCC...Shim-LayerHypervisorNICVM2VM1VM31234R3R2R15Algorithm 3: SDN-GCC Shim-layer
1 Function ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğ‘ƒğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ´ğ‘Ÿğ‘Ÿğ‘–ğ‘£ğ‘ğ‘™ (ğ‘ƒ, ğ‘ ğ‘Ÿğ‘, ğ‘‘ğ‘ ğ‘¡)
/* i is NIC and j is VNIC index
*/
2
ğ‘‡ (ğ‘–, ğ‘—) = ğ‘‡ (ğ‘–, ğ‘—) + ğ‘…(ğ‘–, ğ‘—) Ã— (ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘“ .ğ‘ ğ‘’ğ‘›ğ‘¡ ğ‘¡ğ‘–ğ‘šğ‘’);
ğ‘‡ (ğ‘–, ğ‘—) = ğ‘€ ğ¼ ğ‘ (ğµ(ğ‘–, ğ‘—), ğ‘‡ (ğ‘–, ğ‘—));
Enable ECN-capable bits (ECT) in IP header;
if ğ‘‡ (ğ‘–, ğ‘—) â‰¥ ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘ƒ) then

4

6

5

3

ğ‘‡ (ğ‘–, ğ‘—) = ğ‘‡ (ğ‘–, ğ‘—) âˆ’ ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘ƒ);
ğ‘ ğ‘’ğ‘›ğ‘¡ ğ‘¡ğ‘–ğ‘šğ‘’(ğ‘–, ğ‘—) = ğ‘›ğ‘œğ‘¤();

else

Queue until token regeneration OR Drop;

11 Function ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ ğ‘ƒğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ´ğ‘Ÿğ‘Ÿğ‘–ğ‘£ğ‘ğ‘™ (ğ‘ƒ, ğ‘–, ğ‘—)
12

if Packet has congestion notiï¬cation message then

ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  = ğ‘–ğ‘›ğ‘¡ (ğ‘šğ‘ ğ‘”);
if ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  â‰¥ 0 then

ğ‘ğ‘œğ‘›ğ‘” ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ (ğ‘–, ğ‘—) = ğ‘¡ğ‘Ÿğ‘¢ğ‘’;
ğ‘’ğ‘™ğ‘ ğ‘ğ‘ ğ‘’ğ‘‘ ğ‘¡ğ‘–ğ‘šğ‘’ = ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘ğ‘œğ‘›ğ‘” ğ‘¡ğ‘–ğ‘šğ‘’(ğ‘–, ğ‘—);
ğ‘šğ‘ğ‘Ÿ ğ‘˜ğ‘ 
ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘Ÿğ‘ğ‘¡ğ‘’ =
ğ‘’ğ‘™ ğ‘ğ‘ğ‘ ğ‘’ğ‘‘ ğ‘¡ğ‘–ğ‘šğ‘’ ;
ğ‘…(ğ‘–, ğ‘—) = ğ‘…(ğ‘–, ğ‘—) âˆ’ (ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘Ÿğ‘ğ‘¡ğ‘’ Ã— ğ‘ ğ‘ğ‘ğ‘™ğ‘’(ğ¶));
ğ‘…(ğ‘–, ğ‘—) = ğ‘€ğ‘ğ‘¥(ğ‘…ğ‘šğ‘–ğ‘›, ğ‘…(ğ‘–, ğ‘—));
ğ‘ğ‘œğ‘›ğ‘” ğ‘¡ğ‘–ğ‘šğ‘’(ğ‘–, ğ‘—) = ğ‘›ğ‘œğ‘¤();

else

Send to normal packet processing;

23 Function ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’ ğ‘ˆ ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ ğ‘‡ğ‘–ğ‘šğ‘’ğ‘œğ‘¢ğ‘¡ ()
24

forall i in NICs and j in VNICs do

if ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘ğ‘œğ‘›ğ‘” ğ‘¡ğ‘–ğ‘šğ‘’(ğ‘–, ğ‘—) â‰¥ ğ‘‡ğ‘ then
ğ‘ğ‘œğ‘›ğ‘” ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ (ğ‘–, ğ‘—) = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’;

else

if ğ‘›ğ‘œğ‘¤() âˆ’ ğ‘ ğ‘’ğ‘›ğ‘¡ ğ‘¡ğ‘–ğ‘šğ‘’(ğ‘–, ğ‘—) â‰¥ ğ‘‡ğ‘œ then

ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’(ğ‘–, ğ‘—) = ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’;
redistribute NIC capacity among active ï¬‚ows;

if ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’(ğ‘–, ğ‘—) && ğ‘ğ‘œğ‘›ğ‘” ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ (ğ‘–, ğ‘—) then

ğ‘…(ğ‘–, ğ‘—) = ğ‘…(ğ‘–, ğ‘—) + ğ‘ ğ‘ğ‘ğ‘™ğ‘’(ğ¶);
ğ‘…(ğ‘–, ğ‘—) = ğ‘€ ğ¼ ğ‘ (ğ¸ (ğ‘–), ğ‘…(ğ‘–, ğ‘—));

7

8

9

10

13

14

15

16

17

18

19

20

21

22

25

26

27

28

29

30

31

32

33

tokens and the depth of the bucket variables per-VM per-NIC
where the per-VM rate limiters are implemented as counting
token buckets where virtual NIC ğ‘— has a rate ğ‘…(ğ‘–, ğ‘—), bucket
depth ğµ(ğ‘–, ğ‘—) and number of tokens ğ‘‡ (ğ‘–, ğ‘—) on physical NIC
ğ‘–. In addition, the shim-layer will also translate the received
congestion message from the controller on a per-source basis.
Initially, the installed on-system NICs are probed and the
values of their nominal data rate ğ‘…(ğ‘–), and bucket size ğµ(ğ‘–) are
calculated. Thereafter, when the ï¬rst packet is intercepted from
a new VM, NIC capacity is redistributed and a equal-share of
capacity â€œğ¸ (ğ‘–)â€ is calculated. The new value ğ¸ (ğ‘–) is used to
re-distribute the allocated rate for each active VM and then the
new VM is marked as active7 As shown in Table II, the state of

7Typically, after a certain time of inactivity (e.g., 1 sec in our simulation),
the variables used for VM tracking are reset and the rate allocations are
redistributed among currently active VMs.

7

the communicating VM is tracked only through token bucket
and congestion speciï¬c variables. The shim-layer algorithm
shown in Algorithm 3 is located at the forwarding stage of
the stack, on arrival or departure of a packet ğ‘ƒ, it detects
the packetâ€™s outgoing port ğ‘— and incoming port ğ‘–. Before ğ‘ƒâ€™s
departure, the available tokens ğ‘‡ (ğ‘–, ğ‘—) is refreshed based on
the elapsed time since the last transmission. The packet is
then cleared for transmission if ğ‘‡ (ğ‘–, ğ‘—) â‰¥ ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘ƒ), in which
case ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘ƒ) is deducted from ğ‘‡ (ğ‘–, ğ‘—). Otherwise, ğ‘ƒ is simply
dropped8. The shim-layer intercepts only the special congestion
message.

For each incoming notiï¬cation,

the algorithm cuts the
sending rate in proportion to the rate of marking received
(capped by ğ‘…ğ‘šğ‘–ğ‘›) which is a parameter set by the operator.
Typically, this values is chosen with respect to the minimal
bandwidth guarantee provided by the operator. Hence, as
sources cause more congestion in the network, the amount
of marks received increases and as a result the sending rates
of such sources decrease proportionally until the congestion
subsides. When congestion messages become less frequent, or
after a pre-determined timer ğ‘‡ğ‘ elapses, the algorithm starts
to gradually increase the VMsâ€™ source rate conservatively.
The rate is increased until it reaches ğ¸ (ğ‘–), or congestion is
perceived again, leading to another reduction cycle. Function
â€scale(C)â€ is used to scale the amount of rate increase and
decrease proportional to the NICs rate and to smooth out large
variations in rate dynamics.

B. SDN Network Application

SDN-GCC relies on a SDN network application to probe for
congestion statistics on a regular basis from the queues of the
SDN switches in the network. The application sends notiï¬cation
messages towards the VMs that are causing congestion on
a given port. This is accomplished by building a special
message with those particular VMs as destinations with the
data indicating the amount of marking they have caused. These
messages are never delivered to the VMs and are actually
intercepted by the shim-layer in the hypervisor. For simplicity,
we assume that each of the involved VMs contribute equally to
the congestion, and hence, the marks are divided equally among
source VMs. SDN-GCC Controller shown in Algorithm 4 is an
event-driven mechanism that handles two major events: packet
arrivals of unidentiï¬ed ï¬‚ows (miss-entries) from switches and
congestion monitoring timer expiry to trigger warning messages
to the involved sources if necessary.

1) Upon a packet arrival: extract the necessary information
to establish source to destination ğ·ğ‘†ğ‘‡ ğ‘†ğ‘…ğ¶ relationship
and destination to port relationship ğ¼ ğ‘ƒğ‘‡ğ‘‚ğ‘ƒğ‘‚ğ‘…ğ‘‡. This
is necessary to establish associations between congested
ports and corresponding sources. In addition, The timer for
congestion monitoring is re-armed if it was not already.
2) Congestion monitoring timer expiry: for each switch
ğ‘ ğ‘¤, the controller probes for marking statistics through
OpenFlow or sFlow protocols by calling function
ğ‘Ÿğ‘’ğ‘ğ‘‘ ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘ (ğ‘ ğ‘¤). The new marking rate of each switch

8Packets could be queued for later transmission, however, this approach

adds a large overhead on the end-hosts

Algorithm 4: SDN-GCC SDN application
1 Function ğ‘ƒğ‘ğ‘ğ‘˜ğ‘’ğ‘¡ ğ´ğ‘Ÿğ‘Ÿğ‘–ğ‘£ğ‘ğ‘™ (ğ‘ƒ, ğ‘ ğ‘Ÿğ‘, ğ‘‘ğ‘ ğ‘¡)
2

if P is an IP packet of a new source-destination pair then

ğ·ğ‘†ğ‘‡ ğ‘†ğ‘…ğ¶ [ğ‘ƒ.ğ‘ ğ‘Ÿğ‘] = ğ‘ƒ.ğ‘‘ğ‘ ğ‘¡;
ğ¼ ğ‘ƒğ‘‡ğ‘‚ğ‘ƒğ‘‚ğ‘…ğ‘‡ [ğ‘ƒ.ğ‘ ğ‘Ÿğ‘] = ğ‘ƒ.ğ‘–ğ‘› ğ‘ğ‘œğ‘Ÿğ‘¡;
if Timer is not active then start ğ¶ ğ‘€ ğ‘‡ğ‘–ğ‘šğ‘’ğ‘Ÿ (ğ‘‡ğ‘–) ;

6 Function ğ¶ğ‘œğ‘›ğ‘”ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘€ğ‘œğ‘›ğ‘–ğ‘¡ğ‘œğ‘Ÿ ğ‘‡ğ‘–ğ‘šğ‘’ğ‘œğ‘¢ğ‘¡ ()
7

forall ğ‘ ğ‘¤ ğ‘–ğ‘› ğ‘†ğ‘Š ğ¼ğ‘‡ğ¶ğ» do

ğ‘ ğ‘¤ ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘  = ğ‘Ÿğ‘’ğ‘ğ‘‘ ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘ (ğ‘ ğ‘¤);
forall ğ‘ ğ‘–ğ‘› ğ‘†ğ‘Š ğ¼ğ‘‡ğ¶ğ» ğ‘ƒğ‘‚ğ‘…ğ‘‡ do

ğ›¼ = ğ‘€ ğ´ğ‘…ğ¾ğ‘†[ğ‘ ğ‘¤] [ ğ‘] âˆ’ ğ‘ ğ‘¤ ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘ [ ğ‘];
ğ‘€ ğ´ğ‘…ğ¾ğ‘†[ğ‘ ğ‘¤] [ ğ‘] = ğ‘ ğ‘¤ ğ‘šğ‘ğ‘Ÿ ğ‘˜ ğ‘ [ ğ‘];
if ğ›¼ > 0 then

ğ·ğ‘†ğ‘‡ ğ¿ğ¼ğ‘†ğ‘‡ = ğ‘”ğ‘’ğ‘¡ ğ‘ğ‘™ğ‘™ ğ‘‘ğ‘ ğ‘¡ (ğ‘ ğ‘¤, ğ‘) ;
forall ğ‘‘ğ‘ ğ‘¡ ğ‘–ğ‘› ğ·ğ‘†ğ‘‡ ğ¿ğ¼ğ‘†ğ‘‡ do

ğ‘†ğ‘…ğ¶ ğ¿ğ¼ğ‘†ğ‘‡ [ğ‘‘ğ‘ ğ‘¡] = ğ‘”ğ‘’ğ‘¡ ğ‘ğ‘™ğ‘™ ğ‘ ğ‘Ÿğ‘(ğ‘‘ğ‘ ğ‘¡);
ğ›½ = ğ›½ + ğ‘ ğ‘–ğ‘§ğ‘’(ğ‘†ğ‘…ğ¶ ğ¿ğ¼ğ‘†ğ‘‡ [ğ‘‘ğ‘ ğ‘¡]);

if ğ›½ > 0 then
ğ‘š = ğ›¼
ğ›½ ;
forall ğ‘‘ğ‘ ğ‘¡ ğ‘–ğ‘› ğ·ğ‘†ğ‘‡ ğ¿ğ¼ğ‘†ğ‘‡ do

forall ğ‘ ğ‘Ÿğ‘ ğ‘–ğ‘› ğ‘†ğ‘…ğ¶ ğ¿ğ¼ğ‘†ğ‘‡ [ğ‘‘ğ‘ ğ‘¡] do

ğ‘šğ‘ ğ‘” = ğ‘€ğ‘†ğº ( ğ‘š , ğ‘‘ğ‘ ğ‘¡ , ğ‘ ğ‘Ÿğ‘ );
ğ‘ ğ‘’ğ‘›ğ‘‘ ğ‘šğ‘ ğ‘” ğ‘¡ğ‘œ ğ‘ ğ‘Ÿğ‘;

3

4

5

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

Restart ğ¶ ğ‘€ ğ‘‡ğ‘–ğ‘šğ‘’ğ‘Ÿ (ğ‘‡ğ‘–);

port ğ‘ is calculated [33]. For each port, if there are
new markings (due to congestion), then the controller
needs to advertise this to all related sources. Thus we
ï¬rst retrieve the destination list of this port via function
ğ‘”ğ‘’ğ‘¡ ğ‘ğ‘™ğ‘™ ğ‘‘ğ‘ ğ‘¡ (ğ‘ ğ‘¤, ğ‘) and then for each destination retrieve
the sources using ğ‘”ğ‘’ğ‘¡ ğ‘ğ‘™ğ‘™ ğ‘ ğ‘Ÿğ‘(ğ‘‘ğ‘ ğ‘¡). The controller now
piggybacks on any outgoing control message or crafts an
IP message consisting of an Ethernet Header (14 bytes),
an IP header (20 bytes), and a payload (2-byte) containing
the number of ECN marks that have been observed in
the last measurement period, divided by the number of
sources. This message is created for each source concerned
(sending through the port ğ‘ experiencing congestion) and
sent with the source IP of the dest. VM and dest. IP of
the source VM (which allows the shim-layer to identify
the appropriate forwarding ports of source VM).

C. Implementation and Practical Issues

Any trafï¬c sent by the VM in excess of its share can
either be queued or simply dropped and resent later by the
transport/application layer. In the former case, an extra per-
VM queue is used for holding the trafï¬c for later transmission
whenever the tokens are regenerated. We tested both approaches
and the queuing mechanism turned out to achieve marginally
better performance which, in view of the complexity it adds,
did not motivate its need. If ECN marking is not in use end-
to-end, all outgoing data packets must be marked with the
ECT bit. In addition, the shim-layer needs to clear any ECN
marking used to track congestion before delivering the packets
to the target VMs.

8

SDN-GCC is distributed between the network application
and the shim-layer with very low computational complexity and
can be integrated easily in any network whose infrastructure is
based on SDN. Due to recent advancement of memory speeds,
the throughput of internal forwarding (e.g., Open vSwitch
(OvS)) of commercial desktop/server is 50-100 Gb/s, which is
fast enough to handle 10â€™s of concurrent VMs sharing a single
or few physical links. Hence, the overhead of the shim-layer
functions added to the OvS would not overload the CPU. In
addition, the shim layer at the hypervisor require operations
of ğ‘‚ (1) per packet, as a result the additional overhead is
insigniï¬cant. The network application is also of low complexity
making it ideal for fast response to congestion (within few
milliseconds time scale).

In multi-path routing, the SDN application with global view
can evaluate congestion on a path-by-path basis. Consequently,
the shim-layer can adapt rates to each path which it can identify
via the 5-tuple hash of the packets. Finally, the control plane
communications in SDN networks is typically out-of-band
i.e., different from the data plane [34], hence fast reaction
to congestion is possible and notiï¬cation messages are not
interrupted by in-network middle-boxes.

In data centers, the internet-facing incoming/outgoing con-
nections are typically isolated from intra-data center connec-
tions via connection splitting at the front-end or proxy servers.
Hence, the intra-data center congestion is primarily attributed
to ï¬‚ows running in the data center

VIII. SIMULATION ANALYSIS

In this section, we study the performance of the proposed
scheme via ns2 simulation in network scenarios with a high
bandwidth-low delay. We examine the performance of a tagged
VM that uses New-Reno TCP with SACK-enabled. The tagged
TCP connection competes with other VMs running similar
New-Reno TCP, DCTCP, or UDP in four cases: 1) a setup
that uses RED AQM with non-ECN enabled TCP; 2) a setup
that uses RED AQM with ECN enabled TCP; 3) a setup
that uses HyGenICC as the trafï¬c control mechanism; and
4)
a setup that uses proposed SDN-GCC framework. For
HyGenICC, there is a single parameter settings of timeout
interval for updating ï¬‚ow rates which should be larger than
a single RTT, in the simulation this value is set to 500 ğœ‡s
(i.e., 5 RTTs). However, in the case of SDN-GCC, timeout
interval for congestion monitoring and reporting application of
the controller is set to a large value of 5ms (i.e., 50 RTTs). We
set the minimum rate ğ‘…ğ‘šğ‘–ğ‘› to 1% of the share of the sender
VM on the underling NIC. In all simulation experiments, we
adjust RED parameters to perform ECN marking based on
the instantaneous queue length at the threshold of 20% of the
buffer size.

A. Simulation Setup

We use ns2 version 2.35 [35] which we patched to include the
different mechanisms. We use in all our simulation experiments
link speed of 1 Gb/s for stations, small RTT of 100 ğœ‡s and
the default TCP ğ‘…ğ‘‡ğ‘‚ğ‘šğ‘–ğ‘› of 200 ms. We use a single-rooted
tree (Dumbbell) topology with single bottleneck link of 1Gb/s

9

UDP are more aggressive. Figure 1b suggests that ECN can
partially ease the problem, however the achieved throughput
reaches the allocated share only when the competitor uses the
same TCP protocol. This can be attributed to the fact that
TCP reacts conservatively to ECN marks unlike DCTPC which
reacts proportionally to the fraction of ECN marks. Simulation
with a static rate limit of 250 Mb/s (fair-share), show that
a central rate allocator assigning rates per VM can achieve
perfect rate allocation with no work-conservation (Utilization
is 250 Mb/s in period 3). Figure 4a shows that HyGenICC
thanks to its distributed and live adaptive rate limiters, can
respond effectively to congestion events. Finally, Figure 4b
suggest a similar result as HyGenICC can be achieved with the
help of a regular control messaging from a central controller
whenever necessary. Hence, SDN-GCC can efï¬ciently leverage
its global view of network status to dynamically adjust the rate
limiters controlling the competing ï¬‚ows that cause congestion
and yet achieve work conservative high network utilization.

8 â‰ˆ 125ğ‘€ğ‘, â‰ˆ 1ğºğ‘

SDN based schemes are questioned for their scalability which
is currently under active research [36]. We conduct the previous
experiment however we increase the number of competitors to
7, 15 and 31. Figure 5 suggests that SDN-GCC can scale well
with an increasing number of senders. The tagged TCP ï¬‚ow
and competing ï¬‚ows, starting at 10ğ‘¡ â„, adjust their rates due
to the incoming control messages when the controller starts
observing congestion in the network. The adjustment messages
trigger ï¬‚ow rate changes up and down until they reach the
equilibrium point where sources start oscillating slightly around
the target share of â‰ˆ 1ğºğ‘
16 â‰ˆ 62.5ğ‘€ğ‘ and
â‰ˆ 1ğºğ‘
32 â‰ˆ 31.25ğ‘€ğ‘ for the 8, 16 and 32 sources respectively.
In SDN environments, controller delays are also of major
concern. To study the sensitivity of the system to the control
loop delays, We conduct the previous experiment involving 4
senders however we vary the control loop delay to a smaller
control delay of 10 RTTs and larger delays of 100 RTT and
500 RTT. Figure 6a shows that to achieve faster convergence
smaller switch-controller-hypervisor delay is always preferable.
Figure 6b and 6c shows that ï¬‚ows oscillations and convergence
period increases as the controller delay increases to 10ms and
50ms. This shows that the performance of rate control schemes
such as SDN-GCC depends on ensuring that the controller can
react within moderate delays.

IX. TESTBED IMPLEMENTATION OF SDN-GCC

We have implemented SDN-GCC Control application as a
separate application program in python for any python-based
controller (e.g., Ryu [37] SDN framework in our testbed). Since,
most popular cloud management software (e.g., OpenStack) use
OpenvSwitch [38] as their end-host (hypervisor) networking
layer. We implemented SDN-GCC shim-layer as a patch to the
Kernel data-path module of OvS. We added the token-bucket
rate limiters and the congestion message handler (i.e., the shim-
layer) in the packet processing pipeline in the data-path of OvS.
In a virtualized environment, OvS forwards the trafï¬c for inter-
VM, intra-Host and inter-Host communications. This leads to
an easy and straightforward way of deploying the shim-layer
at the end-hosts by only applying a patch and recompiling

(a) HyGenICC enabled network

(b) SDN-GCC enabled network

Figure 4: The instantaneous and mean goodput of the tagged TCP
ï¬‚ow while competing with 3 senders that use either TCP,
DCTCP or UDP. In interval [0,10] the competitors are
active, in [10,20] all ï¬‚ows are active and in [20,30] only
the tagged TCP ï¬‚ow is active.

at the destination, and run the experiments for a period of 30
sec. The buffer size of the bottleneck link is set to be more
than the bandwidth-delay product in all cases (100 Packets),
the IP data packet size is 1500 bytes.

B. Simulation Results and Discussion

For clarity, we ï¬rst consider a toy scenario with 4 elephant
ï¬‚ows (the tagged ï¬‚ow and 3 competitors). In the experiments,
the tagged FTP ï¬‚ow uses TCP NewReno and competes either
with 3 FTP ï¬‚ows using TCP NewReno, DCTCP or 3 CBR
ï¬‚ows using UDP. Competitors start and ï¬nish at the 0ğ‘¡ â„ and
20ğ‘¡ â„sec respectively, while the tagged ï¬‚ow starts at the 10ğ‘¡ â„sec
and continues until the end. Hence, from 0 to 10s (period 1)
only the competitors occupy the bandwidth, from 10s to 20s
(period 2) the bandwidth is shared by all ï¬‚ows, and from 20s to
30s (period 3) the tagged ï¬‚ow uses the whole bandwidth. This
experiment demonstrates work conservation, sharing fairness,
and convergence speed of SDN-GCC compared to other setups.
Figure 4 shows the instantaneous goodput of the tagged TCP
ï¬‚ow along with the mean goodput with respect to its competitor
(in the legend, the optimal average goodput of tagged TCP
would be 0Mb/s for period 1, 250Mb/s for period 2, 1000Mb/s
for period 3 and 416Mb/s for all the periods). As shown in
Figure 1a, without any explicit rate allocation mechanisms and
without ECN ability, TCP struggles to grab any bandwidth
when competing with DCTCP and UDP ï¬‚ows as DCTCP and

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (406)DCTCP (410)UDP (410)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (407)DCTCP (407)UDP (405)10

(a) 8 senders scenario

(b) 16 senders scenario

(c) 32 senders scenario

Figure 5: The instantaneous and mean goodput of the tagged TCP ï¬‚ow competing with 7, 15 and 31 senders using TCP, DCTCP or UDP

(a) 1ms monitor interval

(b) 10ms monitor interval

(c) 50ms monitor interval

Figure 6: The instantaneous and mean goodput of the tagged TCP ï¬‚ow while competing with either TCP, DCTCP or UDP using a control

period of 1ms (10RTT), 10ms (100RTT) or 50ms (500RTT) is used.

in the network as shown in Figure 7. Various iperf and/or
Apache client/server processes are created and associated with
their own virtual ports on the OvS at the end-hosts. This allows
for scenarios with large number of ï¬‚ows in the network to
emulate a data center with various co-existing applications. The
base RTT is â‰ˆ200 ğœ‡s without queuing and â‰ˆ1 ms with queuing,
hence we set the monitoring/sampling interval to conservative
values larger than â‰ˆ1 ms (â‰ˆ5 times the base RTT).

We run a scenario in which TCP and UDP elephant ï¬‚ows
are competing for bandwidth and to test the agility of SDN-
GCC, a burst of mice TCP ï¬‚ows is introduced to compete for
bandwidth in a short-period of time. We ï¬rst generate 7 TCP
iperf ï¬‚ows and another 7 UDP iperf ï¬‚ows from each sending
rack for 20 secs resulting in 42 (2 Ã— 7 Ã— 3 = 42) elephants at the
bottleneck. At the 10ğ‘¡ â„sec, we use Apache Benchmark [40] to
request â€index.htmlâ€ webpage (10 times) from each of the 7
web servers on each sending rack (7 Ã— 6 Ã— 3 = 126 in total).
Figure 8a and 8b show that the TCP elephants are able to grab
their share of bandwidth regardless of the existence of non-
well-behaved UDP trafï¬c. In addition, Figure 8c suggests that
mice ï¬‚ows still beneï¬t from SDN-GCC by achieving a smaller
and nearly smooth (equal) ï¬‚ow completion time on average
with a smaller standard deviation demonstrating SDN-GCCâ€™s
effectiveness in apportioning the link capacity. In summary,
SDN-GCC effectively tackles congestion and allocates the
capacity among various ï¬‚ow types as expected.

To summarize this simulation and experimental study, SDN-
GCC seems to be able to efï¬ciently allocate the bandwidth

(a) The testbed topology

(b) The actual testbed

Figure 7: A real testbed for experimenting with SDN-GCC framework

the OvS kernel module, introducing minimal impact on the
operations of production DC networks with no need for a
complete shutdown. Speciï¬cally, deployment can be carried
out by the management software responsible for admission and
monitoring of the data center.

We set up a testbed as shown in Figure 7. All machinesâ€™
internal and the outgoing physical ports are connected to the
patched OvS. We have 4 racks consisting of 7 servers each
(rack 1, 2 and 3 are senders and rack 4 is receiver) all servers
are installed with Ubuntu Server 14.04 LTS running kernel
v3.16 and are connected to the ToR switch through 1 Gbps
links. Similarly, the machines are installed with the iperf [39]
program for creating elephant ï¬‚ows and the Apache web server
hosting a single webpage â€index.htmlâ€ of size 11.5KB for
creating mice ï¬‚ows. We setup different scenarios to reproduce
both incast and buffer-bloating situations with bottleneck link

051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (374)DCTCP (374)UDP (366)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (342)DCTCP (342)UDP (337)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (347)DCTCP (347)UDP (336)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (412)DCTCP (412)UDP (412)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (404)DCTCP (404)UDP (400)051015202530Time (s)020040060080010001200Tagged Flow Throughput (Mb/s)Competitor (Mean Thr)TCP (373)DCTCP (373)UDP (347)Rack 1Rack 2Rack 3CoreToRRack 4ControllerBottleneckControl11

(a) AVG TCP goodput

(b) AVG elephant UDP goodput

(c) AVG FCT for mice

Figure 8: Testbed experiment involving 126 TCP mice competing against 21 TCP (same variant) and 21 UDP elephants

among various ï¬‚ow types and alleviate possible congestion in
the network core.

X. RELATED WORK

HyGenICC [1] can be comparable or complementary to
a number of works on cloud network resource allocation
that have been proposed recently. Seawall [41] is a system
proposed for sharing network bandwidth, it provides per-VM
max-min weighted fair share using explicit feedback end-to-
end congestion notiï¬cation based on losses for rate adaptation.
Seawall requires modiï¬cations to network stack which incurs a
large overhead and may interfere with middleboxes operations.
SecondNet [7] is designed to divide network among tenants
and enforce rate limits, but is limited to providing static
bandwidth reservation between pairs of VMs. Oktopus [8]
argues for predictability by enforcing a static hose model using
rate limiters. It computes rates using a pseudo-centralized
mechanism, where VMs communicate their pairwise bandwidth
consumption to a tenant-speciï¬c centralized coordinator. This
control plane overhead limits reaction times to more than 2
seconds which is inadequate for the fast changing and dynamic
trafï¬c nature in datacenters. FairCloud [42] designs better
policies for sharing bandwidth and explored fundamental trade-
offs between network utilization, minimum guarantees and
payment proportionality, for a number of sharing policies.
EyeQ [43] provides per-VM max-min weighted fair shares in
the context of a full bisection bandwidth datacenter topology
where congestion is limited to the ï¬rst and the last hops. By
simplifying rate limiters and coupling congestion control to
make them dynamic entities rather than static, HyGenICC
can achieve similar objectives as these proposals in an easy
to deploy manner with minimal CPU and network overhead.
HyGenICC is designed to operate with commodity infras-
tructure and traditional protocols used by current production
datacenter/cloud, to be a readily deployable solution. Finally,
HyGenICC can leverage the popularity of Open vSwitch (OvS)
usage by cloud management frameworks like openstack to
implement its mechanism with minor modiï¬cations to OvS
that do not require any new protocols, software and hardware.
A number of recent proposals implemented different system
designs for cloud network resources allocation. â€œSeawallâ€ [41]
is a system designed solely for sharing network capacities
by achieving per-VM max-min weighted allocations using

explicit end-to-end feedback messaging for rate adaptation.
Seawall adds new encapsulation protocol to network stack on
top of transport headers which incurs a large processing and
messaging overhead as well as rendering it into a non middle-
box friendly solution. â€œSecondnetâ€ [7] is a system proposed
to divide network among tenants via rate limits enforcements,
however, it only supports static bandwidth reservation among
tenantsâ€™ VMs. â€œEyeQâ€ [43] adopts per-VM max-min weighted
fair sharing in the context of a full bisection bandwidth
datacenter topology. Its downside is the design assumption that
congestion is limited to ï¬rst and last hops. â€œRWNDQâ€ [44, 45]
is a fair-share allocation AQM for TCP in data centers, however
it resolves contention among ï¬‚ows using TCP as their transport.
RWNDQ also shares the drawback of low deployment potential
which is common to all switch solutions like [1, 46, 47].
â€œHyGenICCâ€ [1] is a hypervisor-based IP-based congestion
control mechanism that relies on a collaborative information
exchange between hypervisors. The solution involves the use
of ECN marking as congestion indication which is aggregated
and fed back between hypervisors to enable network bandwidth
partitioning through dynamic (adaptive) rate limiters. In spite
of the appealing performance gains achieved. In general, these
mechanisms have some or all of the following drawbacks:

1) Security: Introduction of new protocols or using reserved
headers may interfere with the operation of the middle-
boxes (e.g., HyGenICC uses IP reserved bits known as
â€œEvil-bitâ€ [48] which was used for security testing).
2) Overhead: Flow tracking and feedback packets crafting
on a per VM-to-VM basis adds burden to the hypervisorâ€™s
processing overhead.

3) Locality: The lack of global

information about

the
dynamic network conditions results into hypervisor-based
solutions to react only to the perceived VM-to-VM
congestion (e.g., transient and non-persistent) rather than
the network-wide congestion state.

4) Mutli-Path: VM-to-VM packets are not guaranteed to
take the same path when multi-path routing (e.g., ECMP)
is used, leading to under estimation of the actual VM-to-
VM congestion by the end-points.

Recently, SDN has seen a growing number of deployments
for intra- and inter-data center networks [49â€“51]. SDN was
also invoked to address complex congestion events such as

051015202530Elpehant Goodput (Mb/s)0.00.20.40.60.81.0CDFRED-ECNSDN-GCC101520253035404550Elpehant Goodput (Mb/s)0.00.20.40.60.81.0CDFRED-ECNSDN-GCC0.00.10.20.30.40.50.60.70.80.9Average Response Time (sec)0.00.20.40.60.81.0CDFRED-ECNSDN-GCCTCP-Incast in data centers [2, 32, 52â€“56]. Hence, we address
the aforementioned drawback of the former schemes by taking
advantage of the rich information, ï¬‚exibility and global scope
provided by the SDN framework. We show that SDN-GCC [2]
is a middle-box and mutli-path friendly solution that achieves
similar design goals with lesser deployment overhead and lower
CPU and network overhead. This is achieved by leveraging
simple rate limiters and incorporating a network-aware SDN
controller towards building a dynamic adaptive system. The
essence of SDN-GCC is to address the increasing trend and
shift to SDN infrastructures while keeping traditional transport
protocols unchanged in current production data centers.

XI. CONCLUSION AND FUTURE WORK
In this paper, we set to build a system that relies of the
pervasive availability of SDN capable switches in data centers
to provide a centralized congestion control mechanism with
a small deployment overhead onto production data centers.
Our system achieves better bandwidth isolation and improved
application performance. SDN-GCC is a SDN framework that
can enforce efï¬cient network bandwidth allocation among
competing VMs by employing simple building blocks such
as rate limiters at the hypervisors along with an efï¬cient
SDN application. SDN-GCC is designed to operate with low
overhead, on commodity hardware, and with no assumption of
tenantâ€™s cooperation which makes a great composition for the
deployment in SDN-based data center networks. SDN-GCC
was shown, via simulation and deployment, to efï¬ciently divide
network bandwidth among active VMs, by enforcing the target
rates, regardless of the transport protocols in use.

REFERENCES
[1] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, â€œHyGenICC:
Hypervisor-based Generic IP Congestion Control for Virtualized
Data Centers,â€ in Proceedings of IEEE ICC, 2016.

[2] A. M. Abdelmoniem and B. Bensaou, â€œEnforcing Transport-
Agnostic Congestion Control via SDN in Data Centers,â€ in
IEEE Local Computer Networks (LCN), (Singapore), October
2017.
[3] Amazon,

(VPC).â€

Private

Virtual

â€œAWS

Cloud

http://aws.amazon.com/vpc/.

[4] Open Networking Foundation, â€œSDN Architecture Overview,â€

tech. rep., ONF TR-504, Nov 2014.

[5] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh,
S. Venkata, J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. HÂ¨olzle,
S. Stuart, and A. Vahdat, â€œB4: Experience with a globally-
deployed software deï¬ned wan,â€ in ACM SIGCOMM, 2013.
[6] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill,
M. Nanduri, and R. Wattenhofer, â€œAchieving high utilization
with software-driven wan,â€ in Proceedings of SIGCOMM, 2013.
[7] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu,
and Y. Zhang, â€œSecondnet: A data center network virtualization
architecture with bandwidth guarantees,â€ in ACM CoNext, 2010.
[8] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron, â€œTowards
predictable datacenter networks,â€ ACM CCR, vol. 41, 2011.
[9] R. Gajjala, S. Banchhor, A. M. Abdelmoniem, A. Dutta,
M. Canini, and P. Kalnis, â€œHuffman coding based encoding
techniques for fast distributed deep learning,â€ in DistributedML
workshop - CoNEXT, 2020.

[10] A. M. Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini,
â€œAn Efï¬cient Statistical-based Gradient Compression Technique
for Distributed Training Systems,â€ in Machine Learning and
Systems (MLSys), 2021.

12

[11] A. M. Abdelmoniem and M. Canini, â€œTowards mitigating
device heterogeneity in federated learning via adaptive model
quantization,â€ in ACM EuroMLSys, 2021.

[12] N. G. Dufï¬eld, P. Goyal, A. Greenberg, P. Mishra, K. K.
Ramakrishnan, and J. E. van der Merive, â€œA ï¬‚exible model for
resource management in virtual private networks,â€ in Proceedings
of ACM SIGCOMM, 1999.

[13] M. Al-Fares, A. Loukissas, and A. Vahdat, â€œA scalable, com-
modity data center network architecture,â€ in SIGCOMM, 2008.
[14] B. A. Greenberg, J. R. Hamilton, S. Kandula, C. Kim, P. Lahiri,
A. Maltz, P. Patel, S. Sengupta, A. Greenberg, N. Jain, and D. A.
Maltz, â€œVL2: a scalable and ï¬‚exible data center network,â€ in
Proceedings of ACM SIGCOMM, 2009.

[15] T. Benson, A. Akella, A. Shaikh, and S. Sahu, â€œCloudnaas:
A cloud networking platform for enterprise applications,â€ in
Proceedings of ACM Symposium on Cloud Computing, 2011.

[16] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and
M. Handley, â€œImproving datacenter performance and robustness
with multipath TCP,â€ in Proceedings of ACM SIGCOMM, 2011.
[17] A. M. Abdelmoniem, H. Susanto, and B. Bensaou, â€œTaming
Latency in Data centers via Active Congestion-Probing,â€ in IEEE
ICDCS, 2019.

[18] H. Susanto, B. L. Ahmed M. Abdelmoniem, Honggang Zhang,
and D. Towsley, â€œA Near Optimal Multi-Faced Job Scheduler
for Datacenter Workloads,â€ in IEEE ICDCS, 2019.

[19] A. M. Abdelmoniem and B. Bensaou, â€œCurbing Timeouts for
TCP-Incast in Data Centers via A Cross-Layer Faster Recovery
Mechanism,â€ in IEEE INFOCOM, 2017.

[20] A. M. Abdelmoniem and B. Bensaou, â€œHysteresis-based Active
Queue Management for TCP Trafï¬c in Data Centers,â€ in IEEE
INFOCOM, 2019.

[21] H. Susanto, A. M. Abdelmoniem, H. Jin, and B. Bensaou, â€œCreek:
Inter many-to-many coï¬‚ows scheduling for datacenter networks,â€
in IEEE International Conference on Communications (ICC),
2019.

[22] J. Dong, H. Yin, C. Tian, A. M. Abdelmoniem, H. Zhou,
B. Bai, and G. Zhang, â€œUranus: Congestion-proportionality
among slices based on weighted virtual congestion control,â€
Computer Networks, vol. 152, pp. 154â€“166, 2019.

[23] A. M. Abdelmoniem and M. Canini, â€œDC2: Delay-aware
Compression Control for Distributed Machine Learning,â€ in
IEEE INFOCOM, 2021.

[24] A. M. Abdelmoniem, H. Susanto, and B. Bensaou, â€œReducing
latency in multi-tenant data centers via cautious congestion
watch,â€ in 49th International Conference on Parallel Processing
- ICPP, ICPP â€™20, (New York, NY, USA), Association for
Computing Machinery, 2020.

[25] A. M. Abdelmoniem and B. Bensaou, â€œT-RACKs: A Faster
Recovery Mechanism for TCP in Data Center Networks,â€
IEEE/ACM Transactions on Networking, vol. 29, no. 3, 2021.

[26] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan, â€œData center TCP
(DCTCP),â€ ACM SIGCOMM CCR, vol. 40, p. 63, 2010.
[27] H. Wu, Z. Feng, C. Guo, and Y. Zhang, â€œICTCP: Incast
congestion control for TCP in data-center networks,â€ IEEE/ACM
Transactions on Networking, vol. 21, 2013.

[28] S. M. Irteza, A. Ahmed, S. Farrukh, B. N. Memon, and I. A.
Qazi, â€œOn the coexistence of transport protocols in data centers,â€
in Proceedings of IEEE ICC, 2014.

[29] G. Judd, â€œAttaining the promise and avoiding the pitfalls of TCP
in the datacenter,â€ in Proceedings of USENIX NSDI, 2015.
[30] R. Nishtala, H. Fugal, and S. Grimm, â€œScaling memcache at

facebook,â€ Proceedings of 10th USENIX NSDI, 2013.

[31] Y. Lu and S. Zhu, â€œSDN-based TCP Congestion Control in Data
Center Networks,â€ in Proceedings of IEEE IPCCC, 2015.
[32] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, â€œSICC:
SDN-based Incast Congestion Control for Data Centers,â€ in
Proceedings of IEEE ICC, 2017.

[33] Open Networking Foundation, â€œOpenFlow Switch Speciï¬cation

V1.5.1,â€ tech. rep., ONF TS-025, Mar 2015.

[34] A. Panda, C. Scott, A. Ghodsi, T. Koponen, and S. Shenker,

â€œCAP for networks,â€ in ACM HotSDN workshop, 2013.

[35] NS2,

network
http://www.isi.edu/nsnam/ns.

â€œThe

simulator

ns-2

project.â€

[36] M. Karakus and A. Durresi, â€œA survey: Control plane scalability
issues and approaches in Software-Deï¬ned Networking (SDN),â€
Computer Networks, vol. 112, pp. 279 â€“ 293, 2017.

[37] Ryu Framework Community, â€œRyu: a component-based software
deï¬ned networking controller.â€ http://osrg.github.io/ryu/.

13

in an Action: Rapid Precise Packet Loss Notiï¬cation in Data
Center,â€ in Proceedings of USENIX NSDI, pp. 17â€“28, 2014.

[47] A. J. Abu, B. Bensaou, and A. M. Abdelmoniem, â€œA Markov
Model of CCN Pending Interest Table Occupancy with Interest
Timeout and Retries,â€ in IEEE International Confereence on
Communications (ICC), 2016.

[48] C. Bellovin, â€œThe security ï¬‚ag in the ipv4 header,â€ 2003.

https://www.ietf.org/rfc/rfc3514.txt.

[49] Scott Raynovich, â€œA Look at Key SDN Deployments.â€

https://www.sdxcentral.com/articles/analysis/key-sdn-
deployments/2016/06/.

[38] OpenvSwitch,

â€œOpen

Virtual

Switch

project.â€

[50] N. Feamster, J. Rexford, and E. Zegura, â€œThe Road to SDN,â€

http://openvswitch.org/.

[39] iperf,

â€œThe TCP/UDP Bandwidth Measurement Tool.â€

https://iperf.fr/.

[40] Apache.org, â€œApache HTTP server benchmarking tool.â€

http://httpd.apache.org/docs/2.2/programs/ab.html.

[41] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha,
â€œSharing the data center network,â€ in USENIX NSDI, 2011.
[42] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Rat-
nasamy, and I. Stoica, â€œFairCloud: Sharing the Network in Cloud
Computing,â€ ACM SIGCOMM CCR, vol. 42, pp. 187â€“198, 2012.
[43] V. Jeyakumar, M. Alizadeh, D. Mazi`eres, B. Prabhakar, C. Kim,
and A. Greenberg, â€œEyeq: Practical network performance isola-
tion at the edge,â€ in Proceedings of USENIX NSDI, 2013.
[44] A. M. Abdelmoniem and B. Bensaou, â€œReconciling Mice and
Elephants in Data Center Networks,â€ in IEEE International
Conference on Cloud Networking (CloudNet), 2015.

[45] A. M. Abdelmoniem and B. Bensaou, â€œEfï¬cient Switch-Assisted
Congestion Control for Data Centers: an Implementation and
Evaluation,â€ in IEEE International Performance Computing and
Communications Conference (IPCCC), Dec. 2015.

[46] P. Cheng, F. Ren, R. Shu, and C. Lin, â€œCatch the Whole Lot

Queue, vol. 11, pp. 20â€“40, 2013.

[51] I. F. Akyildiz, A. Lee, P. Wang, M. Luo, and W. Chou, â€œA
roadmap for trafï¬c engineering in SDN-OpenFlow networks,â€
Computer Networks, vol. 71, pp. 1â€“30, 2014.

[52] A. M. Abdelmoniem and B. Bensaou, â€œIncast-Aware Switch-
Assisted TCP Congestion Control for Data Centers,â€ in IEEE
Global Communications Conference (GlobeCom), 2015.
[53] A. M. Abdelmoniem, B. Bensaou, and V. Barsoum, â€œIncastGuard:
An Efï¬cient TCP-Incast Congestion Effects Mitigation Scheme
for Data Center Network,â€ in IEEE International Conference on
Global Communications (IEEE GlobeCom),, 2018.

[54] A. J. Abu, B. Bensaou, and A. M. Abdelmoniem, â€œLeveraging
the Pending Interest Table Occupancy for Congestion Control
in CCN,â€ in IEEE Local Computer Networks (LCN), 2016.
[55] A. M. Abdelmoniem, B. Bensaou, and A. J. Abu, â€œMitigating
incast-tcp congestion in data centers with sdn,â€ Annals of
Telecommunications, vol. 73, no. 3, pp. 263â€“277, 2018.
[56] A. M. Abdelmoniem, Y. M. Abdelmoniem, and B. Bensaou, â€œOn
Network Systems Design: Pushing the Performance Envelope
via FPGA Prototyping,â€ in IEEE international Conference on
Recent Trends in Computer Engineering (IEEE ITCE), 2019.

