2
2
0
2

g
u
A
5

]
E
S
.
s
c
[

2
v
3
2
2
0
1
.
7
0
2
2
:
v
i
X
r
a

1

Fairness Testing: A Comprehensive Survey and
Analysis of Trends

Zhenpeng Chen, Jie M. Zhang, Max Hort, Federica Sarro, Mark Harman

Abstract—Software systems are vulnerable to fairness bugs and frequently exhibit unfair behaviors, making software fairness an
increasingly important concern for software engineers. Research has focused on helping software engineers to detect fairness bugs
automatically. This paper provides a comprehensive survey of existing research on fairness testing. We collect 122 papers and
organise them based on the testing workﬂow (i.e., the testing activities) and the testing components (i.e., where to ﬁnd fairness bugs)
for conducting fairness testing. We also analyze the research focus, trends, promising directions, as well as widely-adopted datasets
and open source tools for fairness testing.

Index Terms—Software fairness, fairness testing, survey, trend, analysis

(cid:70)

1 INTRODUCTION
In recent years, the wide adoption of software systems in
social-critical, human-related tasks, such as hiring [1], credit
assessment [2], criminal justice [3], and disease detection [4],
has increased awareness of fairness bugs and raised surging
concerns about them.

Fairness bugs refer to any imperfection in a software
system that causes a discordance between the existing and
the required fairness conditions. These bugs have been fre-
quently reported, related to sensitive attributes (also called
protected attributes) such as sex, race, age, and occupation
[5]. For example, the recidivism assessment software used
by US courts was found to falsely ﬂag black defendants as
future criminals compared to white defendants [6]; com-
mercially released facial-analysis software also exhibited
skin-type and gender bias1 [8]. These fairness bugs may
bring unethical and unacceptable consequences, particularly
disadvantaging minorities and protected groups.

A lot of effort from various research communities has
been devoted to detecting and reducing fairness bugs. The
growing importance of software fairness has prompted the
continued expansion of the fairness research community. A
series of new conferences and workshops are now dedicated
to this topic, including FairWare [9], FAcct [10], and SE4RAI
[11]. Through these venues, researchers, practitioners, and
policy makers together explore the fundamental question:
How to engineer fair software systems? This has greatly stimu-
lated the growth of software fairness publications.

• Zhenpeng Chen is with the Department of Computer Science, University
College London, London, United Kingdom. E-mail: zp.chen@ucl.ac.uk
Jie M. Zhang is with the Department of Informatics, King’s College
London, London, United Kingdom. E-mail: jie.zhang@kcl.ac.uk

•

• Max Hort is with the Department of Computer Science, University Col-
lege London, London, United Kingdom. E-mail: max.hort.19@ucl.ac.uk
Federica Sarro is with the Department of Computer Science, University
College London, London, United Kingdom. E-mail: f.sarro@ucl.ac.uk

•

• Mark Harman is with the Department

of Computer Science,
University College London, London, United Kingdom. E-mail:
mark.harman@ucl.ac.uk

1The literature treats “bias” and “unfairness” as synonyms, refer-

ring to the opposite of “fairness” [7].

Fairness is most pertinent to Machine Learning (ML)
software [12], but also applicable more generally in Software
Engineering (SE) [5]. It was ﬁrst studied as a software
engineering concern by Finkelstein et al. [13]. From the
Software Engineering (SE) perspective, fairness is a non-
functional software property that needs to be a ﬁrst-class
entity in the entire software engineering process, rather
than an after-the-system-is-built concern [5], [14]. Ahmad
et al. [15] regarded fairness as an increasingly important
requirement that needed to be considered in requirements
engineering of ML software; Alidoosti [16] argued that there
is a need to consider fairness in the software architecture
design process; Albarghouthi et al. [17] framed fairness as
correctness properties for program veriﬁcation; Zhang et al.
[12] described fairness as a non-functional software property
that deserves substantial testing effort; Salimi et al. [18]
considered fairness as the objective of software repair.

In this paper, we focus on the fundamental branch of
software fairness research: fairness testing. Fairness testing
is closely bound to other activities in the software en-
gineering process. It reveals fairness bugs introduced in
software implementation, tests whether software systems
satisfy fairness requirements both before and after software
deployment, guides software repair to ﬁx fairness bugs, etc.
As a part of software testing activities, fairness testing
is speciﬁc to revealing the discordance between existing
and required fairness conditions of a given software system.
Some aspects of fairness testing are shared with well-known
solutions already widely adopted in the software testing
literature. However, compared to traditional software test-
ing, fairness testing primarily targets statistically-orientated,
non-deterministic ML software, which poses unique chal-
lenges arising from the test oracle problem [12]. In addition,
there is an increasing number of fairness deﬁnitions, some of
which are even conﬂicting or difﬁcult to satisfy simultane-
ously in the engineering process [19], [20]. For certain social-
critical application scenarios, domain-speciﬁc knowledge is
required for fairness testing, and thus legal practitioners
and policy makers may need to be involved in the testing
process.

 
 
 
 
 
 
2

Fig. 1: Cumulative number of fairness testing publica-
tions.

Due to the increasing importance of fairness testing and
its unique challenges, the literature has witnessed a consid-
erable upsurge in work in this area. Figure 1 shows the cu-
mulative number of publications about fairness testing over
the years to 2022. Overall, we can see evidence of growing
interest in this topic, thereby demonstrating the timeliness of
this survey. From the ﬁgure, we can observe that more than
85% of fairness testing publications appeared since 2019,
demonstrating the emergence of this new software testing
domain - fairness testing.

This paper provides a comprehensive survey of fairness
testing research. We focus on ML software, which is primary
target of the software fairness literature. We collect papers
from SE venues as well as venues for artiﬁcial intelligence,
computer security, database, etc. We organize the collected
papers according to two aspects: fairness testing workﬂow
(i.e., how to test) and fairness testing components (i.e., where
to ﬁnd fairness bugs). Additionally, we analyze research
trends and identify research opportunities for the research
community working on fairness testing. We also summarize
the publicly accessible datasets and open source tools for
fairness testing.

There has been previous work surveying the fairness
literature. Mehrabi et al. [7] and Pessach and Shmueli [21]
surveyed fairness research on ML algorithms. Hort et al.
[22] provided a survey of bias mitigation methods for ML
classiﬁers. Sun et al. [23], Berk et al. [3], and Pitoura [24]
surveyed techniques for improving fairness in speciﬁc ML
tasks: natural language processing, criminal justice risk as-
sessment, and ranking. Tushev et al. [25] surveyed software
design strategies for fairness of digital sharing economy
applications. Hutchinson and Mitchell [26] introduced the
history of assessment tests of fairness over 50 years across
multiple disciplines such as education and hiring. Zhang
et al. [12] surveyed ML testing more generally, in which
fairness was considered as one of many testing properties.
To date, only one survey [27] focused on reviewing the
fairness literature through the lens of software engineering.
It is a systematic literature review that aims to characterize
the research state of ML software fairness, which covers
20 fairness testing papers. Different from these previous
papers, we focus on fairness testing, which plays a funda-

Fig. 2: Structure of this paper.

mental role in assessing bias issues for ML software. To the
best of our knowledge, this is the ﬁrst comprehensive survey
of the literature on fairness testing.

To summarize, we make the following contributions in

this paper:

• We provide a comprehensive survey of 122 fairness
testing papers across various research communities.
• We deﬁne fairness bug and fairness testing, and provide
an overview of the testing workﬂow and the testing
components related to fairness testing.

• We position fairness testing within the software engi-

neering process.

• We summarize the publicly available datasets and open
source tools for fairness testing, to provide a navigation
for researchers and practitioners interested in this ﬁeld.
• We analyze research trends and identify promising
research opportunities for fairness testing, with the goal
of stimulating and facilitating further research.
Figure 2 illustrates the structure of this paper. The de-

tailed survey schema is described in Section 3.

2 PRELIMINARIES

In this section, we ﬁrst introduce the most widely-adopted
deﬁnitions of fairness. We then provide a deﬁnition of
fairness bug and fairness testing through the lens of SE.
We also describe the testing workﬂow (how to test) and
the testing components (where to test) of fairness testing.
In addition, we compare fairness testing with traditional
software testing to help clarify its unique characteristics,
and position fairness testing within the wider software
engineering process.

TABLE 1: Widely-adopted fairness deﬁnitions.

Name
Counterfactual fairness [30]
Fairness through unawareness [31]
Fairness through awareness [32]
Demographic parity [33]
Equalized odds [34]
Equal opportunity [34]

Fairness type
Individual fairness
Individual fairness
Individual fairness
Group fairness
Group fairness
Group fairness

2.1 Deﬁnition of Fairness

The deﬁnition of fairness determines the fairness condition
that software systems are required to satisfy. Researchers
and practitioners have previously proposed and explored
various fairness deﬁnitions [20], [28], [29].

In this section, we aim to introduce the deﬁnitions most
widely adopted in the literature (listed in Table 1), mainly
falling in two types: individual fairness and group fairness.
Individual fairness requires software to produce similar
predictive outcomes for similar individuals, while group
fairness requires software to treat different groups similarly.
In the context of software fairness, a population is par-
titioned into a privileged group and an unprivileged group
based on a sensitive attribute (also called protected attribute),
which refers to the sensitive characteristic that needs to be
protected against unfairness, such as sex, race, age, and
physical and mental disability.

2.1.1 Individual Fairness

We ﬁrst introduce three widely-adopted deﬁnitions of indi-
vidual fairness.

Counterfactual fairness [30]. A software system satisﬁes
counterfactual fairness if, for an individual, its prediction in
the real world is the same as that in the counterfactual world
where the individual belongs to a different demographic
group (i.e., the sensitive attribute becomes different). This
deﬁnition interprets fairness from a causal perspective: the
variables other than the sensitive attribute are controlled,
and thus unfairness comes from the variants in the sensitive
attribute.

Fairness through unawareness [31]. A software system
is fair through unawareness so long as any sensitive at-
tribute is not explicitly used in its prediction process. Ideally,
blindness to the sensitive attribute would make the outcome
unaffected by it, thus achieving fairness. However, some-
times non-sensitive attributes employed in the prediction
process may contain information correlated to the sensitive
attribute, thereby still leading to potential discrimination
despite lack of any awareness of any sensitive attribute [35].
Fairness through awareness [32]. A software system
satisﬁes fairness through awareness if it produces similar
predictions for similar individuals. Speciﬁcally, any two
individuals who are similar with respect to a similarity
metric deﬁned for a particular task should receive a similar
outcome.

2.1.2 Group Fairness

We introduce three widely-adopted deﬁnitions of group
fairness. To help illustrate the formalization of group fair-
ness, we use A to denote the sensitive attribute, with 1 as the
privileged and 0 as the unprivileged. Let Y be the original

3
prediction label and ˆY the predicted label, with 1 as the
favorable outcome and 0 as the unfavorable.

Demographic parity [33]. Demographic parity, also
known as statistical parity, requires the likelihood of a
favorable outcome to be the same among different demo-
graphic groups. In other words, a software system satisﬁes
demographic parity if P [ ˆY = 1|A = 1] = P [ ˆY = 1|A = 0].
Equalized odds [34]. Equalized odds means that the
probability of a person in the favorable class being correctly
assigned a favorable outcome and the probability of a per-
son in an unfavorable class being incorrectly assigned a fa-
vorable outcome should both be the same for the privileged
and unprivileged groups. In other words, the prediction is
independent of the sensitive attribute when the target label
Y is ﬁxed: P [ ˆY = 1|A = 0, Y = y] = P [ ˆY = 1|A = 1, Y =
y], y ∈ {0, 1}.

Equal opportunity [34]. Equal opportunity states that
the privileged and the unprivileged groups have equal true
positive rates. In other words, the prediction made by the
software is independent of the sensitive attribute when the
target label Y is ﬁxed as 1: P [ ˆY = 1|A = 0, Y = 1] = P [ ˆY =
1|A = 1, Y = 1].

We use the widely-adopted binary classiﬁcation task as
the example to illustrate the group fairness deﬁnition, but
these deﬁnitions can also be applied to other ML tasks with
appropriate adjustments. For example, for the ranking task,
practitioners can measure whether members of different
groups have the same proportional representation among
the desirable outcome (e.g., in a top position in the ranking).
More about fairness deﬁnitions can be found in re-
cent work on surveying, analyzing, and comparing them.
Mitchell et al. [29] presented a reasonably consistent catalog
of fairness deﬁnitions from the literature; Verma and Rubin
[19] explained the rationale behind existing fairness deﬁni-
tions and investigated the relationship among them through
a case study; Castelnovo et al. [20] analyzed the differences,
implications, and orthogonality between existing fairness
deﬁnitions; Hutchinson and Mitchell [26] traced the 50-year
history of fairness deﬁnitions in the areas of education,
hiring, and ML; Mehrabi et al. [7] created a taxonomy of
fairness deﬁnitions proposed for ML software.

These fairness deﬁnitions are encoded in software engi-
neering activities and affect the entire software engineering
process (see Section 2.6). Practitioners can select appropriate
fairness deﬁnitions as requirements according to their appli-
cation scenarios, and use the selected fairness requirements
to guide software development activities such as design,
testing, repair, and deployment. For fairness testing, these
deﬁnitions are particularly encoded in the test oracles (see
Section 4.2). Although these fairness deﬁnitions are not
strictly independent of each other, it has been demonstrated
that they cannot be satisﬁed at the same time [19], [20], [36].

2.2 Deﬁnition of Fairness Bug and Fairness Testing

A software bug is an imperfection in a computer pro-
gram that causes a discordance between the existing and
the required conditions [37]. According to this deﬁnition,
previous work [12] deﬁnes “ML bug” as any imperfection
in an ML artefact that causes a discordance between the
existing and the required conditions, and “ML testing” as

any activity designed to reveal ML bugs. In line with the SE
nomenclature, we deﬁne fairness bug and fairness testing as
follows.
• Deﬁnition 1 (Fairness Bug). A fairness bug refers to any
imperfection in a software system that causes a discordance
between existing and required fairness conditions.

The required fairness condition depends on the fairness
deﬁnition adopted by the requirements of the software
under test. Existing papers also use other terms such as
fairness defects [5] and fairness issues [13] to describe such
imperfection. In this paper, we use “fairness bug” as a
representative of all these related terms, because “bug” has
a more general meaning [37].

The existence of fairness bugs results in the unfairness
(also called bias [38], [39]) of software systems, motivating
research work into techniques to help detect these bugs, i.e.,
fairness testing.
• Deﬁnition 2 (Fairness Testing). Fairness testing refers to any
activity designed to reveal fairness bugs.

Following the recent ML testing survey [12], we consider
two aspects of the emerging testing domain: testing workﬂow
and testing component. From the testing workﬂow angle, the
testing activities may include test oracle identiﬁcation, test
input generation, test adequacy evaluation, etc. From the
testing component angle, software systems typically rely
on ML to make predictions, thus fairness bugs may exist
not only in the prediction algorithms, but also in the data
used for training ML models. In this paper, we survey and
organize the fairness testing literature according to these
two aspects.

2.3 Fairness Testing Workﬂow

The fairness testing workﬂow refers to how to perform
fairness testing with different testing activities. This section
dives into the fairness testing workﬂow and describes its
key activities.

Figure 3 presents the workﬂow of fairness testing. Soft-
ware engineers determine and specify the expected fairness
requirements for the software under test through require-
ments engineering. Test inputs are either sampled from the
collected data or generated automatically; test oracles are
identiﬁed and generated based on the fairness requirements.
Software engineers execute test inputs on the software un-
der test to check whether the test oracles are violated. En-
gineers evaluate the adequacy of the tests, i.e., their ability
to revealing fairness bugs. Meanwhile, the test execution
results yield a bug report to help engineers reproduce,
locate, and ﬁx fairness bugs. The fairness testing process
can be repeated to ensure that the repaired software works
well with regard to fairness.

The fairness testing literature mainly tackles test input
generation (Section 4.1) and test oracle identiﬁcation (Sec-
tion 4.2), leaving other activities as open challenges and
research opportunities for the community (discussed in
Section 8).

2.4 Fairness Testing Components

The fairness testing components refer to where to ﬁnd fair-
ness bugs. Traditional software testing aims to detect bugs in
the code. In the fairness literature, researchers use the term

4

“algorithmic fairness” [28], [29], [40], [41] to indicate that
fairness bugs often exist in the algorithms of ML systems.
According to the fairness literature, we consider algorithms
an important testing component of fairness testing.

ML systems are enabled or assisted by ML models
trained using large-scale data with learning algorithms written
based on ML frameworks (e.g., scikit-learn [42], TensorFlow
[43], and Keras [44]). In other words, an ML system is the
result of the interaction among several components (i.e.,
data, algorithm, and ML framework) that are closely bonded
with each other. Therefore, developers need to conduct
fairness testing on each of them to ensure a fair outcome.

Algorithm Testing: Algorithm testing reveals fairness
bugs based on the input-output behaviors of ML software
systems. On the one hand, algorithm testing can detect
fairness bugs introduced by a particular part of the al-
gorithm, including improper data processing [45], training
algorithm selection [38], and hyper-parameter settings [46].
On the other hand, algorithm testing can view the software
under test as a combined entity and detect fairness bugs in
the algorithm of the software no matter which part causes
these bugs, which we call “fairness integration testing” in this
paper. The algorithm can be tested in a white, black, or
gray-box manner. Black-box testing is a technique of testing
without having any knowledge of the internal working of
ML software (e.g., code and data); white-box testing tests an
ML software system taking into consideration its internal
working; gray-box testing is to test with limited knowledge
of the internal working of the software under test [47].

Data Testing. ML software is developed following the
data-driven programming paradigm. Therefore, data deter-
mine the decision logic of ML software to a large extent [12],
and data bias is considered a main root cause of ML software
bias [48]. Data testing aims to detect different types of
data bias, including checking whether the labels of training
data are biased (label bias) [35], whether the distribution
of training data implies an unexpected correlation between
the sensitive attribute and the outcome label (selection bias)
[49], whether the features of training data contain bias
(feature bias) [50].

Framework Testing. ML frameworks play an important
role in ML software development, as they support engi-
neers with designing, training, and validating ML models.
Therefore, a lot of studies [51], [52], [53] have detected
ML framework bugs that lead to problems in the ﬁnal
ML software systems. However, these bugs are primarily
related to ML performance (e.g., accuracy). To the best of our
knowledge, to date, there has been no framework testing
work that detects fairness bugs.

2.5 Software Testing vs. Fairness Testing

Fairness testing is an emerging software testing domain,
which aims at revealing fairness bugs in ML software.
Compared to traditional software testing, fairness testing
has its unique characteristics. This section summarizes the
relationship and difference between traditional software
testing and fairness testing from several aspects.

(1) Components to test. As described in Section 2.4,
traditional software testing detects bugs in the code, while
fairness testing can detect fairness bugs in algorithms, data,
and ML frameworks.

5

Fig. 3: Workﬂow of fairness testing.

(2) Test input. In traditional software testing, test inputs
are usually the input data for detecting bugs in the code. For
detecting fairness bugs in algorithms, test inputs can include
data instances, while for detecting fairness bugs in data, test
inputs can include learning programs. Moreover, fairness
is a relative concept that considers different demographic
groups at the same time, so the test inputs are often in the
form of paired data instances, each of which is in a different
group, making fairness testing an instance of metamorphic
testing [54].

(3) Test oracle. Traditional software testing often as-
sumes the presence of a test oracle and determines the oracle
beforehand. For a given input, it compares the output of
the software under test with the output that the oracle de-
termines, to reveal software bugs. Compared to traditional
software testing, the oracle problem for fairness testing is
more challenging.

On the one hand, the ground-truth labels of inputs
need to be manually conﬁrmed, which is time-consuming
and labor-intensive. Moreover, because fairness testing is
often conducted for social-critical, human-related tasks, the
manual conﬁrmation needs to be treated with caution and
requires domain-speciﬁc knowledge about the application
scenarios, and even help from legal practitioners and policy
makers.

On the other hand, even if we have the actual labels of
inputs, it is still challenging to determine the concrete oracle
for fairness testing, because there is no ﬁrm consensus on
what is fair and how to formalize fairness.

In addition, it is difﬁcult to determine the test oracle for
an individual input, because fairness is a relative concept
that considers different demographic groups at the same
time. For example, engineers cannot judge whether a system
is fair to women if they are unaware of the outcomes that the
system provides to men. In practice, metamorphic relations
and statistical measurements are adopted to tackle the oracle
problem of fairness testing [48], [55].

(4) Test adequacy. Test adequacy criteria measure
whether existing tests exercise certain aspects of the soft-
ware under test. In traditional software testing, various test
adequacy criteria have been proposed and adopted, such as
line coverage, branch coverage, and dataﬂow coverage [56],
which measure the degree to which the code of a system
is executed by a test suite. However, we did not ﬁnd any
research on evaluating test adequacy nor coverage concepts
for fairness bug revelation. Therefore, there is a need of new
test adequacy criteria for fairness testing.

(5) Testers. Traditional software testing is performed
by test engineers. Considering that fairness bugs may be
caused by data or algorithms, data scientists and algorithm
designers may also need to take on the role of testers for fair-
ness testing. In addition, fairness is an important software
requirement that has been encoded in laws, regulations,
and policies, so legal practitioners, compliance ofﬁcers, and
policy makers may also contribute to test the fairness of
software systems.

2.6 Fairness Testing in Software Engineering Process

This section positions fairness testing within the entire soft-
ware engineering process.

Fairness is a non-functional software property that needs
to be a ﬁrst-class entity in the entire software engineering
process, rather than an after-the-system-is-built concern [5],
[14]. Figure 4 illustrates the fairness-aware software engi-
neering process. Indeed, fairness inﬂuences each software
engineering activity. Fairness testing plays an important role
both before and after the deployment of software systems,
called ofﬂine fairness testing and online fairness testing, respec-
tively.

Requirements: Software engineers need to (1) elicit fair-
ness requirements through interviews, workshops, policy
analysis, etc. (requirements elicitation), (2) perform require-
ments triage and prioritization, taking possible conﬂicts be-
tween different fairness requirements into account (require-
ments analysis), (3) represent and store fairness requirements
in a well-organized fashion to facilitate effective communi-
cation and change management (requirements speciﬁcation),
and (4) trace, evaluate, and deal with the fairness require-
ments changes (requirements management).

Design: With requirements in hand, the next step for
software engineers is to decide the software architectural
styles and design patterns to meet these requirements. In re-
cent years, ethics has received increasing attention from the
SE community [57], [58], [59], [60], and thus been employed
to guide the software design decisions. As an important
principle of ethics [61], fairness needs be considered in the
software design process.

Implementation: Software engineers develop and im-
plement the fairness-aware software systems through com-
puter programming.
Implementation involves fairness-
related tasks such as generating or identifying/selecting
fairness-aware algorithms.

Ofﬂine Testing: Engineers can use historical or gener-
ated data to perform online testing to check whether the

3 SURVEY METHODOLOGY

In this section, we introduce the survey scope, the paper
collection approach, and the organization of our survey.

6

3.1 Survey Scope

We aim to deﬁne, collect, and curate the disparate literature,
arguing and demonstrating that there does, indeed, exist a
coherent area of research in the ﬁeld that can be termed
“fairness testing”.

We apply the following inclusion criteria when collecting
papers. The papers that satisfy any of these criteria are
included in this survey.
• The paper introduces the general idea of fairness testing

or one of the related aspects of fairness testing.

• The paper presents an approach, study, framework, or tool

that targets at fairness testing.

We do not include papers about the issues of fairness
in network systems and hardware systems. Moreover, we
ﬁlter out papers that are about fairness deﬁnitions, but
do not consider them in the context of testing. We also
do not include papers about gender diversity/inclusion
and cognitive bias in software development, because our
survey focuses on software engineering product fairness,
not software engineering process fairness.

3.2 Paper Collection

We collected papers by using keyword searching on the
DBLP publication database [70], which covers arXiv (a
widely-used open-access archive), 1,803 journals, and 5,836
academic conferences in computer science. DBLP is widely
used in software engineering surveys [12], [71], [72], [73],
[74], and a recent survey [12] demonstrates that papers
collected from other popular publication databases are a
subset of those collected from DBLP.

We deﬁned the keywords through a trial-and-error pro-
cedure [75] performed by the ﬁrst two authors and a dis-
cussion among the ﬁrst four authors. The ﬁnal keywords
used for searching included (“fair” OR “bias” OR “dis-
crimination”) AND (“software” OR “learning” OR “bug”
OR “defect” OR “fault” OR “algorithm” OR “test” OR
“detect” OR “evaluat”). As a result, we conducted a total
of 3 × 9 = 27 searches on DBLP on March 17, 2022, and
obtained 5,694 hits. Then, the ﬁrst two authors manually
inspected each hit paper to check whether it is in the scope
of our survey, and selected 59 relevant papers.

We further performed snowballing [76] based on the col-
lected papers, to ensure a high coverage of fairness testing
papers, because keyword searching might omit relevant pa-
pers. Speciﬁcally, we employed both backward snowballing
and forward snowballing [76]. In backward snowballing, we
analyzed the references in each collected paper and iden-
tiﬁed those lying in our scope; in forward snowballing, we
identiﬁed papers of our interest from those that cite the
collected papers with the help of Google Scholar. We re-
peated the snowballing process until we reached a transitive
closure ﬁxed point; no new relevant papers were identiﬁed.
We collected additional 54 papers through snowballing.

To ensure that our survey is comprehensive and accu-
rate, we also contacted the authors of the 113 papers we

Fig. 4: Role of fairness testing in fairness-aware software
engineering process. Fairness testing (highlighted in gray)
plays an important role both before and after software
deployment.

system meets the fairness requirements. Because such data
usually fail to fully represent future data, ofﬂine testing is
unable to cover and test some circumstances that may be
problematic in real-world application scenarios.

Repair: Software engineers produce ﬁxes that can be
adopted to repair the system. The ﬁxes often both explain
the reason of unfairness and provide a possible solution to
improve software fairness. The process of software repair
for ﬁxing fairness bugs is also called “bias mitigation” in
the literature [22], [38], [62]. Testing needs to be repeated
after bias mitigation, to check whether the repaired software
meets the required fairness conditions.

Deployment: The deployment process may introduce
fairness bugs. For example, recently, there is a great demand
of deploying deep learning (DL) software to platforms with
limited computing resources such as mobile devices [63],
[64]. However, because DL software is often computation-
intensive and thus cannot be directly executed on such plat-
forms, it has been common during the deployment process
to compress the DL model. The compression may result in
software unfairness [65], [66], [67].

Online Testing: Online testing employs run-time moni-
toring to keep checking whether the software can still yield
fair outcomes for real-world input data [68]. During this
process, user feedback is also a common data source to
detect fairness bugs. Another typical way for online testing
is A/B testing [69], which splits users to use the new and the
old software and thus compares the two versions. The A/B
testing results can help software engineers determine which
version of software is better and devote more resources to
the better one.

Maintenance: If fairness bugs are detected in online
testing, software maintenance is needed to modify and up-
date the software in order to improve software fairness. In
addition, due to the dynamic environments where software
systems operate and the changing needs of stakeholders,
fairness requirements may evolve. Software engineers need
to maintain the software to keep up with the changing
fairness requirements and customer needs.

TABLE 2: Statistics of the collected papers.

Hits
Keyword
fair | bias | discrimination + software
204
fair | bias | discrimination + learning
2,052
fair | bias | discrimination + bug
17
fair | bias | discrimination + defect
40
fair | bias | discrimination + fault
84
fair | bias | discrimination + algorithm 1,305
fair | bias | discrimination + test
353
fair | bias | discrimination + detect
893
fair | bias | discrimination + evaluat
746
59
After manual inspection
113
After snowballing
122
After collecting author feedback

collected via keyword searching and snowballing. We asked
them to check whether our description about their work is
correct. They also pointed us to 14 additional papers, among
which 9 papers met our inclusion criteria and were further
included in the collected paper repository.

Table 2 shows the statistics of the paper collection pro-
cess. In summary, we consider 59 + 54 + 9 = 122 papers for
this survey.

3.3 Paper Organization

As shown in Table 3, we review the collected papers in the
following way:

Section 4 (Fairness Testing Workﬂow): We organize
the collected papers from the testing workﬂow angle, i.e.,
we summarize existing fairness testing work according to
test input generation and test oracle identiﬁcation. For
each dimension, we further categorize related work. For
example, we categorize test input generation work into
random test input generation, search-based test input gener-
ation, veriﬁcation-based test input generation, and domain-
speciﬁc test input generation.

Section 5 (Fairness Testing Components): We organize
the collected papers from the testing component angle,
i.e., we summarize existing work according to data testing
and algorithm testing. For each of the two dimensions, we
perform a more ﬁne-grained categorization. For example,
for data testing, we introduce the testing effort with regard
to fairness bugs in the data features, data labels, and data
distribution.

Section 6 (Datasets and Tools): We summarize the pub-
licly available datasets and open source tools for fairness
testing, to provide a navigation for (1) researchers who
would like to engage in fairness testing research, and (2) test
engineers who would like to select an appropriate dataset or
tool for their use case.

Section 7 (Research Trends and Distributions): We
analyze the research trends of fairness testing and compare
the numbers of publications in different research venues,
ML categories, data types, and fairness categories, and
classify existing fairness testing techniques according to the
software access level (white-box or black-box).

Section 8 (Research Opportunities): Finally, we discuss
open challenges and highlight potential research avenues in
fairness testing.

4 FAIRNESS TESTING WORKFLOW
We ﬁrst introduce existing techniques that support the key
activities involved in fairness testing, i.e., test input genera-
tion and test oracle identiﬁcation.

7

4.1 Test Input Generation

In the area of fairness testing, test input generation aims
to automatically produce instances that can induce discrim-
ination and reveal fairness bugs of software systems. We
organize relevant research based on the techniques adopted.

4.1.1 Random Test Input Generation

Galhotra et al. [55], [77] presented the ﬁrst test input gener-
ation approach for detecting fairness bugs. Speciﬁcally, they
proposed Themis, which generates values for non-sensitive
attributes randomly, and then iterates over the values for
sensitive attributes. Themis can also use the behavior of the
system under test on test inputs to measure the frequency
of discriminatory instances in the input space.

4.1.2 Search-based Test Input Generation

Despite the effectiveness of Themis, random generation may
lead to a low success rate of the discriminatory input gener-
ation [78], so the following fairness testing work [78], [79],
[80], [81], [82], [83], [84] generates test inputs using search-
based techniques. Search-based test generation uses meta-
heuristic search techniques to guide the generation process
and make this process more efﬁcient and effective [85],
[86], [87]. It has been employed in an increasing number
of fairness testing techniques to explore the input space of
the software under test.

Udeshi et al. [79] proposed Aequitas, a two-phase search-
based individual discriminatory instance generation ap-
proach. In the global search phase, Aequitas randomly
searches for discriminatory instances in the input space. In
the local search phase, Aequitas perturbs the discriminatory
instances identiﬁed in the global phase to search their neigh-
bors. It uses three strategies, i.e., random, semi-directed, and
fully directed, to update the probability used to guide the
selection of attributes to perturb.

Aggarwal et al. [80] presented Symbolic Generation (SG),
which combines symbolic generation and local explain-
ability for search-based discriminatory instance generation.
First, SG uses a local model explainer to construct a decision
tree to approximate the decision-making process of the ML
software under test. Then, SG leverages symbolic execution
to cover different paths in the decision tree to discover
discriminatory inputs, and perturbs these discovered inputs
to search for their neighborhood in the input space, thus
generating more discriminatory inputs.

Xie and Wu [81] relied on reinforcement learning to
achieve an optimal black-box search strategy for individual
fairness test input generation. This approach regards the
ML model under test as a part of the environment of
reinforcement learning. The reinforcement learning agent
takes actions to the environment to produce discriminatory
inputs, and then observes the state and feedback from the
environment in terms of a reward. Through iterations of
such interactions, the agent learns an optimal policy to
generate discriminatory inputs with high efﬁciency.

TABLE 3: Organization of the collected papers.

8

Category

Classiﬁcation

Section

Fairness Testing Workﬂow

Fairness Testing Components

Analysis and Summary

Test input generation

Test oracle identiﬁcation

Data testing

Algorithm testing

Datasets and Tools

Analysis

4.1

4.2

5.1

5.2

6

7&8

Topic
Random test input generation
Search-based test input generation
Veriﬁcation-based test input generation
Domain-speciﬁc test input generation
Metamorphic relations as test oracles
Statistical measurements as test oracles
Detection of feature bias
Detection of label bias
Detection of selection bias
Testing data processing
Testing hyper-parameters
Testing ML model internals
Testing fairness repair algorithms
Testing compression algorithms
Fairness integration testing
Public datasets
Open source tools
Research trends
Research distributions
Research opportunities

Fan et al. [78] proposed ExpGA, an explanation-guided
discriminatory instance generation approach. First, ExpGA
uses interpretable methods to search for seed instances that
are more likely to derive discriminatory instances by slightly
modifying feature values than other instances. Then, with
the seed instances as inputs, ExpGA employs a genetic
algorithm to efﬁciently generate a large amount of discrim-
inatory offspring.

Perera et al. [88] presented SBFT, a search-based fairness
testing approach for regression-based ML systems. They
ﬁrst deﬁned fairness degree, which is measured as the
maximum difference in the predicted values for all pairs of
instances that are similar apart from the sensitive attribute.
Then they searched for test inputs that reveal the fairness
degree using a genetic algorithm.

In addition to these techniques, there have been several
search-based test input generation approaches speciﬁcally
proposed for Deep Neural Networks (DNNs), such as Ad-
versarial Discrimination Finder (ADF) [82], Efﬁcient Indi-
vidual Discriminatory Instances Generator (EIDIG) [83], and
NeuronFair [84], which are described as follows.

Zhang et al. [82], [89] proposed ADF, a gradient-guided
search-based discriminatory instance generation approach
for DNNs. It contains two search phases. In the global
search phase, ADF locates the discriminatory instances near
the decision boundary by iteratively perturbing a seed
input towards the decision boundary with the guidance
of gradient. In the local search phase, ADF further uses
gradients as the guidance to search the neighborhood of the
found individual discriminatory instances to discover more
discriminatory instances.

EIDIG [83] inherits and improves ADF by integrating
a momentum term into the iterative search for identifying
discriminatory instances. The momentum term enables the
memorization of the previous trend and helps to escape
from local optima, which ensures a higher success rate of
ﬁnding discriminatory instances. In addition, EIDIG reduces
the frequency of gradient calculation by exploiting the prior
knowledge of gradients to accelerate the local search phase.

Zheng et al. [84] proposed NeuronFair, which uses the
identiﬁed biased neurons to guide the generation of dis-
criminatory instances for DNNs. First, NeuronFair identiﬁes
the biased neurons that cause discrimination via neuron
analysis. Then, it generates discriminatory instances with
the optimization object of increasing the ActDiff (activation
difference) values of the biased neurons. Finally, NeuronFair
uses the generated discriminatory instances as seeds and
perturbs them to generate more discriminatory instances
near the seeds.

Tao et al. [90] proposed RULER for fairness testing and
repair of DNNs. For a given input, RULER searches for
individual discriminatory instances with different pertur-
bation constraints on sensitive and non-sensitive attributes.
Speciﬁcally, the perturbation constraints require that sen-
sitive attributes shall be within their valid value ranges,
and that non-sensitive ones shall be in the neighborhood
of the original input within a small bound. The generated
instances are then used in training to improve DNN fairness.

4.1.3 Veriﬁcation-based Test Input Generation

Since our survey focuses on fairness testing, we do not
discuss work focusing solely on formal veriﬁcation of fair-
ness properties (see e.g., [17], [91], [92], [93], [94], [95],
[96]). Nevertheless, there is a research stream focusing on
generating discriminatory test inputs through fairness veri-
ﬁcation using Satisﬁability Modulo Theories (SMT) solving.
These techniques are called veriﬁcation-based test input
generation [97].

[97],

Sharma et al.

[98] proposed two veriﬁcation-
based test input generation techniques for fairness, named
fairCheck and MLCheck. Both fairCheck and MLCheck ap-
proximate the black-box model under test by a white-box
model, based on its predictions. Then, the fairness property
and the white-box model are translated to logical formulae
by SMT solvers. Test cases are then automatically generated
attempting to violate the speciﬁed fairness property with
the help of the SMT solver Z3 [99] to check for satisﬁability.

4.1.4 Domain-speciﬁc Test Input Generation

Recently, an increasing number of approaches have been
proposed for test input generation in speciﬁc application
domains. These approaches aim to generate natural inputs
that belong to the data distribution of a practical application
scenario. This section introduces such domain-speciﬁc test
input generation in two typical domains: natural language
processing and computer vision.
Natural language processing. The test input generation
for Natural Language Processing (NLP) systems is mainly
based on the metamorphic relation that a fair NLP system
should produce the same result for two pieces of text that
differ only in sensitive attributed-related features.

D´ıaz et al. [100] collected sentences containing the word
“old” and replaced “old” (as well as “older” and “oldest”)
with “young” (as well as “younger” and “youngest”). The
sentence pairs were used as test inputs to detect age-related
fairness bugs in sentiment analysis systems.

Another method, which is commonly used in fairness
testing for NLP systems, is to manually design a set of
templates for generating test inputs. These handcrafted
templates often consist of short sentences with placeholders,
such as “<person> goes to the school in our neighborhood”. Test
inputs can be generated by instantiating the placeholders.
Kiritchenko and Mohammad [101] created 11 such tem-
plates for gender2 and race, and pre-deﬁned values for the
placeholder <person> as common African American female
or male ﬁrst names, common European American female
or male ﬁrst names, and noun phrases referring to females
(e.g., “my daughter”) or males (e.g., “my son”).

Similarly, Sheng et al. [102], Huang et al. [103], Dhamala
et al. [104] designed templates for detecting fairness bugs
in natural language generation systems. These templates
are sentence preﬁxes containing a placeholder that can be
replaced by different values of a sensitive attribute, such as
“<XYZ> worked as”, where <XYZ> could be “The {woman,
man, Black person, White person, gay person, straight person}”.
Mehrabi et al. [105] created templates to detect the dif-
ference in the ability of name entity recognition systems
to recognize male and female names. The templates are
sentences that start with a placeholder for names followed
by a sentence that represents a human-like activity.

Wang et al. [106] created 30 templates to test machine
translation systems with regard to a speciﬁc type of fairness
bugs, i.e., the inability to determine the gender of a name
correctly. Each template includes a person name slot that
can be replaced with a name from a list of male and female
names.

Sharma et al. [107] manually designed templates to test
gender-related fairness bugs in natural language inference
systems, which aim to determine whether a hypothesis is
true, false, or undetermined given a premise. These tem-
plates are gender-speciﬁc hypothesis with the placeholder
<gender>, such as “This text talks about a <gender> occu-
pation”, where <gender> corresponds to male or female.

Another well-known approach is CheckList [108], which
has been proposed for producing test cases to evaluate NLP

2Sex and gender are different concepts that are often used inter-
changeably. We keep the original usage of the two words in each paper
to reserve ﬁdelity.

9

systems with respects to their capabilities beyond accuracy.
Fairness is of these capabilities. Like previous work, Check-
List relies on a small number of predeﬁned templates for
generating test sentences.

Although handcrafted templates successfully detect fair-
ness bugs in NLP systems, researchers argue that the gen-
erated test inputs relying on them may be simplistic and
limited [109]. To tackle this problem, Ma et al. [54] proposed
an automated framework MT-NLP to generate discrimi-
natory inputs for NLP systems. First, MT-NLP identiﬁes
human-related tokens in the input text using advanced
NLP techniques. Second, it uses word analogy techniques
in manipulating word embeddings to mutate the identiﬁed
tokens and thus generate the test inputs. Finally, it uses
language ﬂuency metrics to rule out unrealistic test inputs.
Asyroﬁ et al. [110] proposed an automated template
creation approach BiasFinder for producing more diverse
and complex test inputs to better uncover fairness bugs in
NLP systems. BiasFinder ﬁrst uses NLP techniques, such
as coreference resolution and named entity recognition, to
automatically identify all the words (e.g., person names,
gender pronouns, and gender nouns) associated to demo-
graphic characteristics in given texts. Then, it replaces the
identiﬁed words with placeholders to transform these texts
into templates. Finally, it ﬁlls in placeholders in each tem-
plate with concrete values of demographic characteristics to
produce a large number of text mutants and test whether
the metamorphic relationship is satisﬁed for the mutants in
the system under test.

Ezekiel et al. [111] proposed ASTRAEA, a grammar-
based fairness testing approach for generating a large num-
ber of discriminatory inputs for NLP systems automatically.
ASTRAEA contains the input grammars covering various
NLP tasks (i.e., coreference resolution, sentiment analysis,
and mask language modeling) and bias (e.g., gender bias,
religion bias, and occupation bias). It randomly explores
the input grammars to generate initial test inputs, and
mutates the words associated with the sensitive attribute
using alternative tokens. Finally, ASTRAEA checks whether
the generated test inputs and their mutants satisfy the
metamorphic relations.

Sun et al. [112], [113] proposed TransRepair and CAT,
which can be applied for test input generation for fairness
testing of machine translation systems. For each input sen-
tence, TransRepair conducts sentence mutations via context-
similar word replacement; CAT identiﬁes and conducts
word replacement using isotopic replacement. The mutants
as well as the original input sentence are used as test inputs
for the machine translation system under test.
Computer vision. To detect fairness bugs in a Computer
Vision (CV) system, researchers typically check how the
output changes when the sensitive attribute of the person
in the input image changes (e.g., transforming from dark
hair to light hair) while keeping everything else constant.
The test input generation for CV systems is underpinned by
this idea.

To perform image transformations, it is common to use
Generative Adversarial Networks (GANs) [114], which can
generate superﬁcially authentic images and has been widely
adopted in ML testing [12]. However, it is challenging for
conventional GANs to generate the precise changes to the

images as fairness testing requires. Taking changing the hair
color as an example, it is difﬁcult to change the hair color
without changing the hair style or other features of the face.
To tackle this problem, there is some recent effort in
adapting and improving conventional GANs for test in-
put generation of CV systems. Denton et al. [115] used
a progressive GAN [116] to mutate attributes for a given
image by linear interpolations of latent variables. Joo and
K¨arkk¨ainen [117] used the FaderNetwork architecture [118]
that inputs speciﬁc known attributes of an image separately
to the generator. Zhang et al. [119] adopted CycleGAN
[120], whose objective function limits the changes to non-
sensitive attributes, in order to generate discriminatory in-
puts. Denton et al. [121] built a face generative model that
maps latent codes, sampled from a ﬁxed prior distribution,
to images, and then infers directions in latent code space
that correspond to manipulations of a particular sensitive
attribute in pixel space. They generated input images by
traversing these inferred directions.

However, these approaches ignore the causal relation-
ships between attributes when generating discriminatory
images. To generate test inputs that may be encountered
in the real world, it is important to consider the down-
stream changes caused by changing a sensitive attribute.
For example, for a chest MRI classiﬁcation system, age of
the patients may affect the relative size of their organs
[122]. Therefore, it is not realistic to change the age of a
patient without considering the causal relationship between
age and the organ size. Based on this insight, Dash et
al. [122] proposed ImageCFGen, a fairness testing method
that combines knowledge from a causal graph and uses an
inference mechanism in a GAN-like framework to generate
discriminatory images.

4.2 Test Oracle Identiﬁcation

Given an input for a software system, the challenge of
distinguishing the corresponding desired behaviour from
the potentially incorrect behavior is called the “test oracle
problem” [123]. The test oracle of fairness testing enables
the judgement of whether a fairness bug exists. Compared
to traditional software testing, the test oracle problem for
fairness testing is more challenging, as described in Section
2.5. We ﬁnd that existing work employs two types of test
oracles for fairness testing: metamorphic relations and sta-
tistical measurements.

4.2.1 Metamorphic Relations as Test Oracles
A metamorphic relation is a relationship between the soft-
ware input change and the output change that we expect to
hold across multiple executions [123].

Suppose a system that implements the function sin(x),
then sin(x) = sin(π − x) is a metamorphic relation. This
relation can be used as a test oracle to help detect bugs.
If sin(x) differs from sin(π − x), we can conclude that the
system under test has a bug without the need for examining
the speciﬁc values output by the system.

Metamorphic relations have been widely studied to
uncover fairness bugs. Speciﬁcally, existing work mainly
performs fairness-related metamorphic transformation on
the input data or training data of ML software, and ex-
pects these transformations do not change or yield expected

10

changes in the prediction. Next, we classify and discuss this
work according to whether the metamorphic transforma-
tions operate on sensitive attributes.

(1) Metamorphic transformations based on mutating
sensitive attributes. For classiﬁcation systems, the most
common metamorphic relation used for fairness testing is
that pairs of instances with different sensitive attributes
but similar non-sensitive attributes should receive the same
classiﬁcation outcome. For example, we expect that a fair
loan application system produces the same decision for two
applicants differing only in their gender.

This metamorphic relation has been widely used to test
the fairness of software systems and to guide the fairness
test generation process in tabular data classiﬁcation [55],
[77], [79], [80], [81], [82], [83], [89], text classiﬁcation [54],
[101], [103], [106], [107], [108], [110], [111], image classiﬁ-
cation [84], [115], [117], [119], [121], [122], etc. For tabular
data, researchers can directly select the sensitive attributes
that they care about in the dataset (e.g., gender and race),
and mutate the sensitive attribute values (e.g., from “male”
to “female”) of instances to detect whether the software vi-
olates the fairness metamorphic relation for these instances.
For text, researchers need to identify the entities related
to the target sensitive characteristics and transform all the
identiﬁed entities at the same time to generate legitimate test
inputs in the real world. For example, for gender, we need
to detect and transform the person name, gender pronoun,
gender noun, etc. For images, researchers mainly leverage
state-of-art deep learning techniques such as Generative
Adversarial Network (GAN) [114] to transform images
across the sensitive attributes. The methods for changing
the sensitive attributes for different types of data have been
introduced in details in Section 4.1.

For software systems that perform regression tasks, the
prediction outcomes are continuous values, not concrete
labels, making it challenging to determine the metamorphic
relations. Speciﬁcally, it is difﬁcult to determine whether
two predicted continuous outcomes are sufﬁciently different
to judge that the software under test has fairness bugs.
To tackle this problem, Udeshi et al. [79] used a threshold
to determine the metamorphic relations, i.e., the outcome
difference of two similar instances that differ in the sensitive
attribute needs to be smaller than the manually-speciﬁed
threshold. Similarly, Perera et al. [88] proposed the concept
of fairness degree, which is measured as the maximum dif-
ference in the predicted values for all pairs of instances that
are similar apart from the sensitive attribute. The fairness
degree can be used and speciﬁed to construct metamorphic
relations and guide the test input generation.

For software systems that perform generation tasks, it is
more challenging to determine the metamorphic relations.
For example, for natural language generation systems, it is
difﬁcult to evaluate whether the generated text is identical
or similar. To measure text similarity, researchers [102], [103]
have applied existing natural language processing tech-
niques, including sentiment classiﬁcation, perplexity and
semantic similarity measurement, and regard classiﬁcation,
on machine-generated text. As a result, the metamorphic
relations require that pairs of inputs that are identical apart
from the sensitive attribute should obtain generated text
with the same sentiment polarity, perplexity, semantics, and

regard. For machine translation systems, which generate
translations based on the input sentences, Sun et al. [112],
[113] generated test oracles relying on the metamorphic rela-
tionship between translation inputs and translation outputs.
Speciﬁcally, translation outputs for the original input sen-
tence and its mutants with regard to sensitive characteristics
should have a certain degree of consistency modulo the
mutated words. They used similarity metrics that measure
the degree of consistency between translated outputs as test
oracles.

(2) Metamorphic transformations based on mutating
non-sensitive attributes. Metamorphic relations for fairness
testing can also be based on the mutation of non-sensitive
attributes.

D´ıaz et al. [100] presented a metamorphic testing ap-
proach to detecting age-related fairness bugs in sentiment
analysis systems through mutating common adjectives, in-
stead of mutating words related to age. They used the
word embedding technique to produce a set of “older” and
“younger” analogs for common adjectives. For example,
they found that in one embedding “stubborn” – “young”
+ “old” gives “obstinate”, while “stubborn” – “old” +
“young” gives “courageous”. They thus used “obstinate” as
the “old” variant of “stubborn”, “courageous” the “young”
variant, and checked whether the use of the two variants
in the input sentence inﬂuences the output of sentiment
analysis systems. If so, they judged that the systems under
test might lead to fairness bugs with regard to age.

Rajan et al. [124] proposed metamorphic testing for
speech recognition systems to detect fairness bugs. Specif-
ically, they identiﬁed eight metamorphic transformations
(e.g., noise, drop, and low/high pass ﬁlter), which are
common in real life, for speech signals. Then they calculated
the increase in error rates of speech recognition for different
demographic groups after metamorphic transformations.
The recognition system has fairness bugs if the difference in
the increase for different groups exceeds a given threshold.
Sharma and Wehrheim [125] presented metamorphic
transformations to test the fairness of the learning phase
of ML software. These transformations are applied to the
training data of ML models, including changing the order-
ing of data rows/columns, permuting the feature names,
and replacing categorical features by numerical ones. The
learning phase is considered fair if the application of the
transformations results in equivalent predictors.

4.2.2 Statistical Measurements as Test Oracles

Researchers have proposed various statistical fairness mea-
surements according to different fairness deﬁnitions and
requirements. Although these measurements are not direct
oracles for fairness testing, they provide a quantitative way
for test engineers to evaluate the fairness of the software
under test. For example, for demographic parity, researchers
calculate the favorable rate among different demographic
groups and detect fairness violations by comparing these
rates. If the rate difference, called Statistical Parity Differ-
ence (SPD) in the software fairness literature [35], [38], [48],
[50], [126], is beyond a threshold, the software under test is
identiﬁed as containing fairness bugs.

There are a large number of existing statistical fairness
measurements. A complete description of each is beyond the

11

scope of this survey. For example, to date, the IBM AIF360
toolkit [127] has provided more than 70 statistical measure-
ments for fairness. Verma and Rubin [19] have surveyed
and categorized widely-adopted statistical measurements
for fairness. Here, we extend their categorization based
on our collected papers, and introduce each category with
representative measurements.

Measurements based on predicted outcomes. Some
measurements are calculated based on the predicted out-
comes of the software for privileged and unprivileged
groups. For example, the aforementioned Statistical Parity
Difference (SPD) [33] measures the difference in favorable
rates among different demographic groups; Disparate Im-
pact (DI) [128] measures the ratio of the favorable rate of the
unprivileged group against that of the privileged group.

Measurements based on predicted and actual out-
comes. Some measurements not only consider the predicted
outcomes for different demographic groups, but also com-
pare them with the actual outcomes recorded in the col-
lected data. For example, the Equal Opportunity Difference
(EOD) [34] measures the difference in the true-positive rates
of privileged and unprivileged groups, where the true-
positive rates are calculated by comparing the predicted
and actual outcomes. Another widely-adopted measure-
ment that lies in this category is Average Odds Difference
(AOD) [34], which refers to the average of the false-positive
rate difference and the true-positive rate difference between
unprivileged and privileged groups.

Measurements based on predicted probabilities and
actual outcomes. Some measurements take the predicted
probability scores and actual outcomes into account. For
example, for any given predicted score, the calibration
measurement calculates the difference in the probability of
having a favorable outcome for privileged and unprivileged
groups [129]; the measurement of balance for positive class
calculates the difference of average predicted probability
scores in the favorable class between privileged and unpriv-
ileged groups [130].

Measurements based on neuron activation. As DNNs
are widely used in software systems to support the decision-
making process, researchers have started to leverage the
internal behaviors of DNNs to design statistical fairness
measurements. Tian et al. [131] proposed a new statistical
measurement based on neuron activation for DNNs. First,
they computed a neuron activation vector for each label
class based on the test inputs. Speciﬁcally, for a class c, each
element of its neuron activation vector represents how fre-
quently a corresponding neuron is activated by all members
in the test inputs belonging to class c. Then they computed
the distance between neuron activation vectors of different
classes as the fairness measurement. If two classes do not
show a similar distance with regards to a third class, they
consider that the DNN under test contains fairness bugs.

Measurements based on situation testing. Some re-
searchers designed statistical measurements to approximate
situation testing, which is a legal experimental procedure of
seeking for pairs of instances that are with similar charac-
teristics apart from the sensitive attribute value, but obtain
different prediction outputs [132]. Thanh et al. [132] lever-
aged the k-nearest neighbor classiﬁcation to approximate
situation testing. They ﬁrst divided the dataset into the

privileged group and the unprivileged group, based on the
sensitive attribute. Then, for each instance r in the dataset,
they found the k-nearest neighbors in the two groups and
denoted them as sets Kp and Ku, respectively. Finally,
they calculated the proportions of instances, for which the
outcome is the same as r in Kp and Ku, and measured the
difference between the two proportions. If the difference is
larger than a given threshold, the instance r is considered
unfairly treated. Zhang et al. [133] improved the measure-
ment proposed by Thanh et al. [132]. They designed a new
distance function that measures the distance between data
instances, to improve the k-nearest neighbor classiﬁcation.
Their function considers only the set of attributes that are
identiﬁed as the direct causes of the outcome by Causal
Bayesian Networks [134].

Measurements based on optimal transport projections.
Several measurements [135], [136], [137] are proposed based
on optimal transport projections [138], which seek for a
transformation map between two probability measures.
Black et al. [135] mapped the set of women in the data
to their male correspondents, with the optimal transport
projection to minimize the sum of the distances between
a woman and the man to which she is mapped (called her
counterpart). Then they extracted the positive ﬂipset, which
contained the women with favorable outcomes whose coun-
terparts are not. They also extracted the negative ﬂipset,
which was the set of women with unfavorable outcomes
whose counterparts are favorable. Finally, they calculated
the size difference of the positive and the negative ﬂipsets
to measure the unfairness of the system under test.

Measurements for ranking systems. It is difﬁcult to
apply the aforementioned measurements directly to rank-
ing systems, which are also used extensively online. Such
ranking systems are typically relied on as critical tools for
decision making across various domains such as ﬁltering
for hiring and university admissions [139], [140]. To tackle
this problem, some work reduced the ranking problem to a
classiﬁcation problem, and then applied existing statistical
measurements. For example, researchers [141], [142], [143]
used the statistical parity difference as the fairness require-
ment, and measured whether members of different groups
have the same proportional representation among desirable
outcomes, e.g., in a top position in the ranking. Another
typical type of statistical metric is based on pairwise fairness
[144], [145], [146], which requires a ranking system to ensure
that the likelihood of a clicked item being ranked above
another relevant unclicked item is the same across groups.

Using statistical measurements as test oracles poses

unique challenges for fairness testing:

It is difﬁcult to determine the threshold for statistical
measurements to detect the fairness bugs. For example, for
the aforementioned SPD, it would be too strict to consider
the software under test to be fair only when SPD equals to 0.
In practice, practitioners can set a threshold for the measure-
ment under consideration [147]. If the measurement result
for the software under test is above or below the speciﬁed
threshold, the software is considered to have a fairness bug.
Although the threshold could be empirically speciﬁed by
the requirements engineers, it is challenging to determine
the appreciate threshold for each fairness measurement.

12

To alleviate this problem, researchers attempt to use
statistical testing, based on the measurements to detect fair-
ness bugs. Tram`er et al. [148] proposed FairTest to analyze
the associations between software outcomes and sensitive
attributes. The software under test is deemed to have a
fairness bug, if the associations are statistically signiﬁcant.
Taskesen et al. [136] and Si et al. [137] employed a statistical
hypothesis test for the fairness measurements based on op-
timal transport projection. DiCiccio et al. [147] presented a
non-parametric permutation testing approach for assessing
whether a software system is fair in terms of a fairness
measurement. The permutation test is used to test the null
hypothesis that a system has equitable performance for two
demographic groups (e.g., male or female) with respect to
the given measurement.

In addition, some researchers construct the baseline for
the fairness measurement, and detect fairness bugs by com-
paring the measurement value with the baseline. Zhao et
al. [149] used the fairness measurements calculated based
on training data as their baseline against which to evaluate.
Speciﬁcally, they used the obtained ML model to annotate
unlabeled data instances, and revealed situations when
the ML process ampliﬁed existing bias by comparing the
fairness measurements on training data and those on the
annotated dataset. Wang and Russakovsky [150] showed
that the bias ampliﬁcation measurement proposed by Zhao
et al. [149] conﬂated different types of bias ampliﬁcation
and failed to account for varying base rates of sensitive
attributes. Then they proposed a new, decoupled metric for
measuring bias ampliﬁcation, which takes into account the
base rate of each sensitive attribute and disentangles the
directions of ampliﬁcation. Wang et al. [151] presented a
fairness testing approach for visual recognition systems that
predicted action labels for images containing people. They
trained two classiﬁers to predict gender from a set of ground
truth labels and model predictions. The difference in the
predictability of the two models indicated whether the ML
process introduced fairness bugs.

It is difﬁcult to determine which measurements to
use to ensure the sufﬁciency of testing, considering a
plethora of candidate fairness measurements available.
To tackle this problem, researchers have investigated the
relationship among different fairness measurements. If a
cluster of fairness assessments measure very similar things,
test engineers can use a single metric from the cluster, which
largely simpliﬁes the testing process. To this end, Majumder
et al. [36] used the clustering algorithm and correlation
analysis to group existing fairness measurements. Verma
and Rubin [19] divided fairness measurement into different
groups based on the theoretical deﬁnitions.

Cachel and Rundensteiner [152] presented FINS, a fair-
ness auditing framework for subset selection (i.e., select-
ing a subset of items), which is integral to AI-enabled
decision-making applications including shortlisting items,
top-k queries, data summarization, and clustering. FINS
provides a uniﬁed easy-to-understand interpretation across
different fairness measurements for subset selection, and
develops guidelines to support stakeholders in determining
which measurements are relevant to their problem context
and fairness objectives.

5 FAIRNESS TESTING COMPONENTS

Existing fairness testing work primarily resides in data
testing and algorithm testing.

5.1 Data Testing

ML software is developed following the data-driven
paradigm. This paradigm makes ML software vulnerable
to fairness bugs present in data. Speciﬁcally, fairness bugs
in data can be learned and propagated throughout the ML
software development pipeline, leading to the creation of
biased and unfair software systems. To tackle this problem,
approaches have been proposed that target the ML software
training data. They detect the bias in data features, data
labels, and data distribution.

(1) Detection of feature bias. Feature bias occurs when
some features in the training data are highly related to the
sensitive attribute, and these correlated features can thus
become the root cause of software unfairness [50]. Zhang
et al. [126] explored how the feature set inﬂuences ML
software fairness. The results showed that the feature set
plays a signiﬁcant role in fairness, which motivates the
fairness work about testing data features.

To detect which features result in fairness bugs, it is
intuitive to think that the software discriminates against
a certain demographic group because it takes the demo-
graphic information (i.e., the sensitive attribute) into ac-
count during the training and prediction process [35]. To test
whether the sensitive attribute is the root cause of fairness
bugs, Chakraborty et al. [35] removed sensitive attribute
information from the data and found that the obtained ML
software exhibits a similar level of unfairness as before.

A similar ﬁnding has been derived from a real-world
application: In 2016, the same-day delivery service pro-
vided by Amazon was demonstrated to discriminate against
neighborhoods in which there resided a disproportionally
high number of black people [153]. The ML model under-
pinning this service did not consider race information, but
the presence of correlated attributes in the training data
meant that bias remained possible nevertheless. That is,
“Zipcode” information used for model training turned out
to be highly correlated with race, and the ML model induced
race information from it.

To tackle this problem, Li et al. [50] aimed to identify
all the biased features that are correlated with sensitive
attributes. Speciﬁcally, they applied linear regression to
analyze the association between each feature and sensitive
attributes, and identiﬁed those features that may thereby
induce bias.

Peng et al. [154] used logistic regression and decision tree
algorithms as models to extrapolate the correlations among
dependent variables that might cause bias in training data.
Black et al. [135] employed optimal transport projections
[138] to map the data instances belonging to the unprivi-
leged group to the ones in the privileged group (i.e., the
counterparts). Then they extracted the positive ﬂipset (i.e.,
the set of the unprivileged group members with favorable
outcomes whose counterparts were unfavorable). They also
computed the negative ﬂipset (i.e., the set of the unpriv-
ileged group members with unfavorable outcomes whose

13

counterparts were favorable). Then they analyzed the mem-
bers of the positive and the negative ﬂipsets to determine
which features contributed to inconsistent classiﬁcations.

(2) Detection of label bias. Label bias occurs when
the process that produces the outcome labels is inﬂuenced
by factors that are not germane to the determination of
the labels [49]. The data for developing ML software are
often historically collected over many years. In the collection
process, the data labels are typically determined by human
beings or algorithms. As a result, these labels can end on
encoding human and algorithmic bias.

To mitigate label bias, Chakraborty et al. [35], [48]
leveraged situation testing to identify biased data points
and remove them from the training data. Speciﬁcally, they
divided the dataset into the privileged and unprivileged
groups based on the sensitive attribute. Then, they trained
two separate models on the data in the two groups. For each
training data instance, they checked the prediction of the
two models obtained. If the two models produced different
results, there is a probability that the label of the data point
would be biased.

Chen and Joo [155] detected label bias in common
datasets for facial expression recognition. They demon-
strated that many expression datasets contain signiﬁcant
label bias between different gender groups, especially when
it comes to the expressions of happiness and anger. They
also found that traditional fairness repair methods cannot
fully mitigate such bias in trained models.

(3) Detection of selection bias. Selection bias occurs
when the processing of sampling training data introduces
an unexpected correlation between the sensitive attribute
and the outcome [49]. For example, the Compas dataset
[156] can be used for predicting whether defendants will re-
offend within two years. It is widely studied in the software
fairness literature, and has been demonstrated to contain
unintended correlations between race and recidivism [49].

To detect selection bias, researchers mainly test the distri-
bution of the data with regard to the sensitive attribute and
the outcome. Chen et al. [39] tested whether the training
data satisfy the “We Are All Equal” worldview, which holds
the belief that there is no statistical association between the
outcome and the sensitive attribute, and has been widely
advocated in the literature and law [157]. Speciﬁcally, they
tested whether the favorable rates of privileged and unpriv-
ileged groups are equal. Chakraborty et al. [48], [158] not
only analyzed the difference in the favorable rates between
privileged and unprivileged groups, but also compared the
numbers of data instances in the two groups.

K¨arkk¨ainen and Joo [159] detected bias in public face
datasets, and found that these datasets are strongly biased
toward Caucasian faces, whereas other races (e.g., Latino)
are signiﬁcantly underrepresented. Such bias would tend to
risk the introduction of fairness bugs into any facial analytic
system trained on them, and limit the applicability of these
systems.

Wang et al. [160] detected the selection bias in visual
datasets along three dimensions: object-based bias, gender-
based bias, and geography-based bias. Object-based detec-
tion considers statistics about object size, frequency, con-
text, and diversity of object representation; gender-based
detection reveals the stereotypical portrayal of people of

different genders; geography-based detection concerns the
representation of different geographic locations.

Torralba and Efros [161] investigated the computer vi-
sion datasets. To test whether existing datasets are really
unbiased representations of the real world, they evaluated
how well an object detector trained on one dataset general-
izes when tested on a representative set of other datasets.

Yang et al. [162] collected perceived demographic at-
tributes on a popular dataset for face detection [163] and
observed skewed demographic distributions. The face de-
tectors trained based on this dataset thus exhibited demo-
graphic bias measured by performance disparity between
different groups.

Mambreyan et al. [164] analyzed the datasets used for lie
detection, and found that these datasets contain signiﬁcant
sex bias. Speciﬁcally, the percentage of instances labelled
with lies for females is larger than that for males in the
dataset. They further analyzed the effect of such bias on lie
detection. Speciﬁcally, they trained a classiﬁer to predict the
sex of the identity appearing in a video, and used the sex
as a proxy for lie, i.e., predicting lie for females and truth
for males. This deception detector simulated a classiﬁer
that used nothing but dataset bias. The results showed that
the performance of this biased classiﬁer was comparable
to the state-of-the-art and thus suggested that the recent
techniques claiming almost perfect results may exploit the
dataset bias.

5.2 Algorithm Testing

Algorithms may encode data processing, decision-making
logic, and run-time conﬁgurations (e.g., hyper-parameters
for ML) [63]. Each part of the algorithm may introduce
fairness bugs to the ﬁnal software system. Algorithm testing
can be applied to determine which part of the algorithm
causes the unfairness.

(1) Testing data processing. It is a common practice in
ML software to include data processing stages to manip-
ulate and transform the training data for the downstream
learning tasks. Biswas and Rajan [45] and Valentim et al.
[165] tested whether data processing methods introduce
fairness bugs using causal reasoning. Speciﬁcally, they em-
ployed each commonly-used data processing method as
an intervention into the development process of ML soft-
ware and kept other settings unchanged. Then they found
that certain pre-processing methods do, indeed, introduce
fairness bugs into ML software, while other pre-processing
methods may improve software fairness.

(2) Testing hyper-parameters. Some researchers argue
that the hyper-parameters speciﬁed in the ML programs
play an important role in software fairness, and test whether
different hyper-parameter settings result in different levels
of software fairness. The testing process is considered as a
search-based problem, whose goal is to ﬁnd optimal settings
in the hyper-parameter space. Chakraborty et al. [35], [166]
proposed Fairway, which combines the situation testing
method with a multi-objective optimization technique. Be-
cause there is often a trade-off between fairness and ML per-
formance [167], Fairway leverages sequential model-based
optimization [168] to search for hyper-parameters that make
ML software as fair as possible while also not degrading
other performance measures.

14

Similarly, Tizpaz-Niari et al. [46] took both fairness and
accuracy into account. They proposed Parfait-ML, which
provides three dynamic search algorithms (independently
random, black-box evolutionary, and gray-box evolution-
ary) to approximate the Pareto front of hyper-parameters
that trades fairness and accuracy. Parfait-ML not only pro-
vides a statistical debugging method to localize hyper-
parameters that systematically inﬂuence fairness, but also
contains a fairness repair method to ﬁnd improved hyper-
parameter conﬁgurations that simultaneously maximize
fairness and accuracy.

(3) Testing ML model internals. Some studies aim to test
the internals of the learned ML models. Zhang et al. [169]
detected the paths related to fairness in the decision tree
and random forest models and repaired them. Speciﬁcally,
they used a MaxSMT solver to decide which paths in the
tree could be ﬂipped or reﬁned, with both fairness and
semantic difference as hard constraints. Then they reﬁned
the decision tree paths and changed their leaf labels as
needed to output a repaired fair model as a solution. The
approach is sound and complete with respect to the input
dataset.

As for the emerging DL models, researchers detect the
neurons (the fundamental units of DNNs) that are respon-
sible for unfair outcomes. NeuronFair [84] identiﬁed the
biased neurons that are responsible for unfairness through
neuron analysis, and then generated discriminatory in-
stances with the goal of increasing the activation difference
values of the biased neurons. It has good performance in
terms of interpretability, generation effectiveness, and data
generalization.

DeepFAIT [119] applied signiﬁcance testing to detect
the activation differences of neurons for instances in the
privileged and the unprivileged groups, to identify fairness-
related neurons.

Vig et al. [170] employed Causal Mediation Analysis
(CMA) to test which parts (such as neurons or attention
heads) of a DNN model are causally implicated in its unfair
predictions. CMA [171] is a classic technique that measures
how a treatment effect is mediated by intermediate variables
(i.e., mediators). Consider each neuron in a neural network
to be an intermediate mediator, the neuron is affected by
the input and, in turn, affects the model output; however,
there also exist direct pathways from the input to the output
that do not pass through the neuron. CMA enables them to
detect the direct and indirect effects of targeted neurons on
the ﬁnal unfair predictions.

Gao et al.

[172] proposed FairNeuron for DNNs.
FairNeuron ﬁrst uses a neuron slicing technique to identify
conﬂict paths, i.e., the paths that contain a lot of neurons
that select sensitive attributes to make predictions. Then it
uses these paths to identify biased instances that trigger the
selection of sensitive attributes. Finally, it retrains the model
by selective training. In the selective training process, for the
identiﬁed biased instances, FairNeuron forces the conﬂict
paths to learn all features that are important for prediction
rather than the biased ones; for the other instances, FairNeu-
ron keeps the original training way.

(4) Testing fairness repair algorithms. As fairness has
been an increasingly important requirement for software
systems, engineers may include the fairness repair algo-

rithms (i.e., bias mitigation algorithms) in their programs
to ensure the fairness of their software systems. Some
researchers focus on testing whether these fairness repair
algorithms reduce fairness bugs without introducing side
effects (e.g., accuracy decrease).

Biswas and Rajan [173] applied seven fairness repair
algorithms to 40 top-rated ML models collected from a
crowd sourced platform, and then compared the individual
fairness, group fairness, and ML performance before and
after these algorithms were applied.

Qian et al. [174] applied fairness repair techniques on
ﬁve widely-adopted ML tasks, and investigated the variance
of fairness and ML performance of these techniques, i.e.,
whether identical runs with a ﬁxed seed produce differ-
ent results. The results showed that most fairness repair
techniques have some other undesirable impacts on the
ML software such as reducing accuracy, increasing fairness
variance, or increasing accuracy variance.

Zhang and Sun [175] evaluated existing fairness repair
techniques on DNNs and found that they may improve
fairness by paying the price of huge accuracy drop. These
techniques may even worsen both fairness and accuracy.
Then they proposed an approach that adaptively chooses
the fairness repair method for a DNN based on causality
analysis [176].

Hort et al. [38] proposed a benchmarking framework
named Fairea. Existing work often measured the impacts of
fairness repair algorithms on fairness and ML performance,
separately. In this way, it was unclear whether the improved
fairness was simply the unavoidable cause of ML perfor-
mance loss. To tackle this problem, Fairea provided a uniﬁed
baseline to evaluate and compare the fairness-performance
trade-off of different repair methods. Hort et al. [38] used
Fairea to empirically evaluate state-of-the-art fairness repair
methods implemented in the IBM AIF360 tooklit [177] with
different settings. They investigated whether ML algorithms
(e.g., RF and SVM), decision-making tasks, and fairness
deﬁnitions inﬂuence the detection of fairness bugs.

Chen et al. [178] employed Fairea to conduct a large-
scale, comprehensive empirical evaluation of 17 representa-
tive bias mitigation methods from both ML and SE com-
munities, evaluated with 12 ML performance metrics, 4
fairness metrics, and 24 types of fairness-performance trade-
off measurements, applied to 8 widely-adopted benchmark
software decision/prediction tasks.

Hort and Sarro [179] observed another side effect of fair-
ness repair: it could cause loss of discriminatory behaviours
of anti-protected attributes. Anti-protected attributes refer
to the attributes that one might want the ML decision to
depend upon (e.g., students with homework should receive
higher grades).

Orgad et al. [180], [181] evaluated the fairness repair
approaches for NLP models from two aspects: extrinsic
bias (performance difference across different demographic
groups) and intrinsic bias (bias in models’ internal repre-
sentations, e.g., word [182] or sentence [183] embeddings).
They found that the two types of bias may not correlated
with each other and that the choice of bias measurement
and dataset can affect the evaluation results signiﬁcantly.

(5) Testing compression algorithms. A computation-
intensive DL software system can be executed efﬁciently

15

on PC platforms with the GPU support, but it cannot be
directly deployed and executed on platforms with limited
computing power, such as mobile devices [63]. To tackle this
problem, model compression algorithms are proposed to
represent DL models in a smaller size with minimal impact
on their performance [64]. Common model compression
algorithms include quantisation (representing the weight
values of DL models using a smaller data type), pruning
(eliminating redundant weights that contribute little to the
model behaviors), and knowledge distillation (transferring
knowledge from a large model to a smaller one) [184].
The wide adoption of model compression in DL software
motivates researchers to detect fairness bugs introduced by
model compression algorithms.

Because model compression is often applied to DL
models with large sizes, existing fairness testing of model
compression algorithms is typically applied to complex NLP
models [65] and computer vision models [65], [66], [67],
[185]. Hooker et al. [66] showed that pruning and quanti-
zation can amplify gender bias when classifying hair color
on a computer vision dataset. Xu and Hu [186] tested the
effect of distillation and pruning on the bias of generative
language models, and presented empirical evidence that
distilled models exhibited less bias. Stoychev and Gunes [67]
detected fairness bugs introduced by different model com-
pression algorithms on various facial expression recognition
systems, and did not observe a consistent ﬁnding across
different systems.

(6) Fairness integration testing. Fairness testing work
that detects fairness bugs in software system as a whole
is referred to as fairness integration testing, which can be
performed in a black-box or white-box fashion. We dis-
tinguish between white-box testing and black-box testing
according to the access level to the training data and internal
knowledge of ML models.

Black-box testing. Black-box testing detects the fairness
bugs of ML software without the need of training data and
knowing the internal structure of the model.

The typical stream of fairness testing work uses statis-
tical measurements to uncover the fairness bugs in black-
box models based on their prediction behaviors. Tram`er
et al. [148] analyzed the associations between prediction
outcomes and sensitive attributes to detect if fairness bugs
exist; Biswas and Rajan [173] used existing statistical mea-
surements for individual fairness and group fairness to
reveal fairness bugs in public ML models.

In addition, as described in Section 4.2.1, several re-
searchers have detected fairness bugs by performing meta-
morphic transformations of the software inputs. They check
whether the transformations result in unexpected changes in
the predictions. Most of the work uses black-box testing. For
example, the Themis tool [55], [77] randomly generates test
inputs and checks whether the software system produces
the same output for every two individuals who differ only
in sensitive attribute values. Similarly, Aequitas [79] and
ExpGA [78] search the input space of the software for
discriminatory instances that reveal unfair predictions.

Black-box testing is frequently employed to detect fair-
ness bugs in complex software systems or commercial soft-
ware whose internal knowledge is unseen to testers, such as
NLP, computer vision, and ranking systems.

TABLE 4: Public datasets for fairness testing.

Description
Income prediction
Re-offence prediction
Credit risk level prediction
Credit risk level prediction
Credit risk level prediction
Subscribing a term deposit or not
Health care needs prediction
Heart health prediction
Final year grade prediction
Individual survival prediction

Dataset
Adult Income
Compas
German Credit
Default Credit
Home Credit
Bank Marketing
Mep
Heart Health
Student Performance
Titanic
Communities and Crime Crime prediction
Diabetes
Heritage Health
Fraud
US Executions
CelebA
LFW
BUPT-Transferface
VGGFace
WIDER FACE
FairFace
Pilot Parliaments
Casual-Conversations
WinoST
EEC
WinoBias
IMDB

Readmission prediction
Staying in the hospital or not
Fraud detection
Execution prediction
Face detection/recognition, etc.
Face recognition
Face recognition
Face detection
Face detection
Facial analytics
Face detection
Computer vision and audio tasks Video
Speech translation
Sentiment analysis
Coreference resolution
Text analytics

Data Type
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Tabular
Image
Image
Image
Image
Image
Image
Images

Speech audios
Text
Text
Text

Size
45,222
6,167
1,000
30,000
37,511
30,488
15,830
303
649
891
2,215
100,000
147,473
1,100
1,437
202,599
10,000
1,300,000 Race
2,600,000
32,203
108,501
1,270
45,000
3,888
8,640
3,160
50,000

Sensitive Attribute(s)
Sex, Race
Sex, Race
Sex
Sex
Sex
Age
Race
Age
Sex
Sex
Race, Age
Race
Age
Age
Sex, Race
For researchers to decide
Sex, Race

For researchers to decide
For researchers to decide
Sex, Race, Age
Sex, Race
Sex, Age, Skin tone
Gender
Sex, Race
Sex
For researchers to decide

16

Link
[187]
[156]
[188]
[189]
[190]
[191]
[192]
[193]
[194]
[195]
[196]
[197]
[198]
[199]
[200]
[201]
[202]
[203]
[204]
[163]
[205]
[206]
[207]
[208]
[209]
[210]
[211]

Many researchers designed text templates to detect fair-
ness bugs in various NLP systems, including sentiment
analysis systems [101], [110], machine translation systems
[106], text generation systems [102], [103], [104], natural
language inference systems [107], name entity recognition
systems [105]. This work has been described in Section
4.1.4 in details. For example, Asyroﬁ et al. [110] employed
BiasFinder (a fairness testing method described in Section
4.1.4) to build a template for the input text of sentiment
analysis systems, and used the template to generate mutant
texts. Then, they analyzed whether the system makes the
same prediction for equivalent mutants to reveal fairness
bugs.

For computer vision systems, state-of-the-art fairness
techniques [114], [115], [116], [117], [119], [121], [122] tend
to use GAN-based algorithms to generate images that differ
in sensitive attributes and detect whether the computer
vision systems make the different decisions for equivalent
image mutants. These techniques have been described in
Section 4.1.4. For example, Zhang et al. [119] employed
the CycleGAN [120] algorithm to limit the changes to non-
sensitive attributes of images, to generate discriminatory
inputs for computer vision systems.

Ranking systems are also often black-box and thus tested
in a black-box manner [141], [142], [143], [144], [145], [146],
[212]. For example, some researchers [141], [142], [143] mea-
sured whether members of different groups have the same
proportional representation in a top ranking position, based
on the ranking outputs of the system under test.

Several black-box testing techniques approximate the
black-box software using a white-box model, so that white-
box testing techniques can be applied. For example, Ag-
garwal [80] constructed a decision tree to approximate
the decision-making process of the black-box ML software

under test, using a local model explainer. Then, they used
symbolic execution-based test input generation to discover
discriminatory inputs. Sharma and Wehrheim [98] ﬁrst ap-
proximated the black-box software by a white-box model
based on the model behaviors. Then, they developed a
property-based testing mechanism for fairness checking
where the speciﬁc fairness requirement can be speciﬁed
using an assume-assert construct. Test cases were then
automatically generated attempting to violate the speciﬁed
fairness property.

White-box testing. White-box model testing detects fair-
ness bugs by inspecting the training data or the internal
structure and information of the ML model visible to the
test engineers.

Some work leverages the training data to reveal un-
fair predictions for the software under test. For example,
Chakraborty et al. [213] proposed an explanation method
based on k-nearest neighbors, to uncover bias in ML soft-
ware predictions. Speciﬁcally, for each test instance pre-
dicted as the unfavorable, they obtained the k-nearest neigh-
bors with the favorable label in the training data. Then, they
compared the distribution of these neighbors with the test
instance to ﬁnd out and explain the bias.

Several studies make use of the internal information of
the ML model to test the software system. For example, ADF
[82], [89] and EIDIG [83] leveraged the gradient information,
which is a vector of DNNs that speciﬁes the direction in
which loss function has the steepest ascent. Speciﬁcally, ADF
[82], [89] ﬁrst searched for the discriminatory instances near
the decision boundary of DNNs, and then used the gra-
dients of DNNs to guide the search of the neighborhood of
the discovered discriminatory instances for more test inputs;
EIDIG [83] reduced the frequency of gradient calculation to
accelerate the search process.

6 DATASETS AND TOOLS

This section describes the public datasets and open source
tools for fairness testing to provide a quick navigation for
researchers and practitioners.

6.1 Public Datasets

This section lists the most widely-adopted public datasets
in the literature. Table 4 shows the information of these
datasets, including the sizes, data types, sensitive attributes,
usage scenarios, and access links. Most of them are tabular
data, which can be used for traditional ML classiﬁers. Re-
cently, with the popularity of natural language processing
and computer vision, text and image datasets have also
emerged. These datasets are primarily collected from social
media platforms such as Twitter. Some datasets come with
constraints on their use, which researchers need to consider
when adopting them. For example, the well-known image
dataset CelebA [201] is allowed for non-commercial research
purposes only.

These datasets cover different

sensitive attributes,
among which sex, race, and age are the most common ones.
There are also datasets (i.e., CelebA, VGGFace, and IMDB)
without explicit sensitive attributes, for which researchers
can determine the sensitive attributes (such as hair color
and skin tone) based on the application scenario under
investigation.

For a more comprehensive overview of the available
fairness datasets, the reader is referred to the work of Le
Quy et al. [214] and Fabris et al. [215]. The former surveyed
the tabular datasets for fairness research; the latter extended
the survey scope to unstructured data (e.g., text and im-
ages), covering datasets in various domains such as social
sciences, computer vision, health, economics, business, and
linguistics.

6.2 Open source Testing Tools

There is a recent proliferation of open source tools for
supporting fairness testing. Nevertheless, Lee and Singh
[249] demonstrated that there is a steep learning curve for
practitioners to use these fairness tools. Currently, there is
no available guidance on tool adoption [249].

To this end, we summarize 30 open source tools for fair-
ness testing in this section, to facilitate fairness researchers
and practitioners to choose appropriate fairness tools. The
details of these tools are shown in Table 5. Overall, Table 5
covers fairness testing tools for general ML software (e.g.,
FairTest [148] and Themis [55]), DL systems (e.g., ADF [82]
and EIDIG [83]), natural language processing systems (e.g.,
ASTRAEA [111] and BiasFinder [110]), and computer vision
systems (e.g., REVISE [160]).

7 RESEARCH TRENDS AND DISTRIBUTIONS

In Figure 1, we have shown that fairness testing is experi-
encing a dramatic increase in the number of publications.
This section further analyzes the research trends and distri-
butions of fairness testing.

17

Fig. 5: Research distribution among different venues.

7.1 Research Venues

We ﬁrst describe research trends in terms of the research
communities engaging in fairness testing. Fairness testing
has been studied in the machine learning community since
2008 [250] and also within the software engineering com-
munity since the same year [13]. In 2008, Pedreschi et al.
[250] proposed the notion of discriminatory classiﬁcation
rules as a criterion to detect the potential unfairness in data
mining systems. Since 2017, more and more research com-
munities have started to study fairness testing. For example,
Galhotra et al. [55] presented the ﬁrst ML fairness testing
approach in the software engineering community and won
the Distinguished Paper Award of ESEC/FSE 2017; Zhao et
al. [149] detected bias in the datasets and ML models for
visual recognition tasks, and won the Best Paper Award of
EMNLP 2017; D´ıaz et al. [100] detected age-related bias in
sentiment analysis systems and won the Best Paper Award
of CHI 2018. These three best paper awards, as well as
being a credit to their authors, also demonstrate both the
signiﬁcant level of interest and the high quality of research
on fairness in three different research communities:

• Software Engineering (ESEC/FSE)
• Natural Language Processing (EMNLP)
• Human-Computer Interaction (CHI)
Figure 5 shows the distribution of the collected papers
across different research venues. Overall, most (54.1%) of the
fairness testing papers are published in artiﬁcial intelligence
venues, such as AAAI, IJCAI, ACL, EMNLP, CVPR, ECCV,
and KDD; 29.5% are published in software engineering
venues, such as ICSE, ESEC/FSE, ASE, ISSTA, ICST, and
TSE. Additionally, we ﬁnd that other research communities,
such as computer security and database communities, are
also embracing fairness testing, demonstrating the impor-
tance of our survey to a broad audience.

7.2 Machine Learning Categories

In this section, we dive deeper into ML software to investi-
gate the research trend of fairness testing in each ML cate-
gory. Speciﬁcally, following previous work [12], we classify
the collected papers into two categories: those targeting DL
software and those for general ML software.

Among the 122 papers, 66 papers (54.1%) conduct fair-
ness testing for DL software, while 56 papers (45.9%) cater
to general ML software. The large volume of publications on
fairness testing for DL software may be attributed to several
reasons. On the one hand, DL becomes increasingly perva-
sive, being used in a wide range of software applications

TABLE 5: Open source tools for fairness testing.

Usage scenario
Tool [ref]
General ML software
FairTest [148]
General ML software
Themis [55]
General ML software
Aequitas [79]
General ML software
ExpGA [78]
General ML software
fairCheck [98]
General ML software
MLCheck [97]
General ML software
LTDD [50]
General ML software
Fair-SMOTE [48]
General ML software
xFAIR [154]
General ML software
Fairway [35]
General ML software
Parfait-ML [46]
General ML software
Fairea [38]
General ML software
IBM AIF360 [177]
General ML software
scikit-fairness [228]
LiFT [229]
General ML software
SageMaker Clarify [231] General ML software
General ML software
FairVis [233]
BiasAmp→ [150]
General ML software
Tree-based classiﬁers
FairRepair [169]
Regression-based ML software
SBFT [88]
DL software
ADF [82]
DL software
EIDIG [83]
DL software
NeuronFair [84]
DL software
DeepInspect [131]
DL software
CMA [170]
DL systems
FairNeuron [172]
NLP systems
ASTRAEA [111]
NLP systems
MT-NLP [54]
NLP systems
BiasFinder [110]
CV systems
REVISE [160]
Subset selection systems
FINS [152]

Description
Analyzing associations between outcomes and sensitive attributes
Black-box random discriminatory instance generation
Black-box search-based discriminatory instance generation
Black-box search-based discriminatory instance generation
Veriﬁcation-based discriminatory instance generation
Veriﬁcation-based discriminatory instance generation
Detecting which data features and which parts of them are biased
Detecting biased data labels and data distributions
Extrapolation of correlations among data features that might cause bias
Detecting biased data labels and optimal hyper-parameters for fairness
Searching for hyper-parameters optimal to ML software fairness
Testing fairness repair algorithms
Examining and mitigating discrimination and bias in ML software
Examining and mitigating discrimination and bias in ML software
Examining and mitigating discrimination and bias in ML software
Measuring bias that occurs in each stage of the ML life cycle
Visual analytics for discovering intersectional bias in ML software
Analyzing whether ML exacerbates bias from the training data
Fairness testing and repair for tree-based models
Search-based fairness testing for regression-based ML systems
White-box search-based discriminatory instance generation for DNNs
White-box search-based discriminatory instance generation for DNNs
White-box search-based discriminatory instance generation for DNNs
White-box fairness measurement for DNNs
Detecting which parts of DNNs are responsible for unfairness
Detecting neurons and data instances responsible for unfairness
Grammar-based discriminatory instance generation for NLP systems
Template-based discriminatory instance generation for NLP systems
Template-based discriminatory instance generation for NLP systems
Detecting object-, gender-, and geography-based bias in CV datasets
Group fairness testing for subset selection tasks

18

Link
[216]
[217]
[218]
[219]
[220]
[221]
[222]
[223]
[224]
[225]
[226]
[227]
[127]
[228]
[230]
[232]
[234]
[235]
[236]
[237]
[238]
[239]
[240]
[241]
[242]
[243]
[244]
[245]
[246]
[247]
[248]

Fig. 6: Cumulative number of fairness testing papers on
general machine learning and deep learning.

and thus attracting interest from the research community.
On the other hand, compared to traditional ML algorithms
(e.g., regression and decision tree), DL is less interpretable
[251], making it harder to reason about fairness in a direct
manner.

We further analyze the number of publications for both
categories over years. Figure 6 illustrates the cumulative
number of fairness testing papers in general ML and DL. We
ﬁnd that there is a trend of moving from testing general ML
software to testing DL software. Before 2021, fairness testing
research mainly focused on general ML. Since 2021, the
number of papers on DL has increased notably, surpassing
publications on general ML.

Despite the increasing popularity of DL and fairness test-
ing for DL, a striking ﬁnding is that only 44.4% of fairness

Fig. 7: Distribution of different data types in fairness
testings.

testing papers in software engineering venues target DL
software. It indicates that there remain many opportunities
for software engineering researchers to use their testing
expertise to tackle fairness bugs in emerge DL software.

7.3 Data Types

In this section, we explore the research trends of fairness
testing in applications with different data types. Among the
122 papers that we collect, 7 do not have experiments with
speciﬁc datasets. Thus, we use the remaining 115 papers to
analyse the data type distribution.

As shown in Figure 7, among the 115 papers, 97.4%
focus on testing software applications that take tabular data
(50.4%), text (22.6%), and images (24.3%) as inputs. Fairness
testing for other data types (e.g., speech and video) is still
not well investigated.

We next dive deep to the research trends in the major
data types (for tabular data, text, and images). Figure 8
presents the cumulative number of fairness testing papers
on applications for tabular data, text, and images. We ob-
serve that although fairness testing for text- and image-

19

black, or gray-box testing). Among all the collected papers,
103 are about algorithm testing. We observe that 72.8% of the
103 papers provide black-box testing, while only 27.2% are
white-box testing. Only the Parfait-ML tool [46] supports
both black-box and gray-box manners for testing hyper-
parameters. Compared to black-box testing, white or gray-
box testing requires the knowledge of training data or the
internal of software systems. However, fairness testing of-
ten applies to human-related, social-critical systems whose
internal information cannot be disclosed to the public due
to privacy concerns or legal policies.

8 RESEARCH OPPORTUNITIES

Fairness testing remains in a relatively embryonic state.
Research in this area is experiencing rapid growth, so there
are plenty of open research opportunities. In this section,
we outline the challenges for fairness testing and present
promising research directions and open problems.

8.1 Absence of or with Multiple Sensitive Attributes

Fairness testing in absence of sensitive attribute informa-
tion. Existing fairness testing techniques rely on the exis-
tence of sensitive attributes, but in practice, this information
might be unavailable or imperfect for many reasons [252].
On the one hand, the data may be collected in a setting
where the sensitive attribute information is unnecessary,
undesirable, or even illegal, considering the recent released
regulations such as GDPR (General Data Protection Regu-
lation) [253] and CCPA (California Consumer Privacy Act)
[254]. On the other hand, users may withhold or modify
sensitive attribute information, for example, due to privacy
concerns or other personal preferences.

To tackle this issue, a straightforward solution is to ﬁrst
use existing demographic information inference techniques
(e.g., gender inference, race inference, and age inference) to
infer the sensitive attribute and then apply fairness testing
techniques. However, existing inference techniques may not
be fully satisfactory, and their application scenarios remain
limited [255]. Moreover, building a model to infer sensitive
information leaves open the possibility that the model may
ultimately be used more broadly, with possibly unintended
consequences [252]. Therefore, more research is needed to
tackle the fairness testing in absence of sensitive attribute
information.
Fairness testing at the intersection of multiple sensitive
attributes. Software systems may have multiple sensitive
attributes that need to be considered at the same time. How-
ever, existing fairness testing work often tackles a single
sensitive attribute at a time. To the best of our knowledge,
there has been little work that explores fairness testing for
compounded or intersectional effects of multiple sensitive
attributes [90], [233], leaving an interesting research oppor-
tunity for the community. Take test input generation as
an example. In real-world applications, software systems
may encounter input instances that trigger fairness bugs
related to multiple sensitive attributes simultaneously, but
such inputs may not be covered by existing test input gen-
eration techniques. Speciﬁcally, existing techniques generate
discriminatory instances only for one speciﬁed sensitive

Fig. 8: Cumulative number of fairness testing papers on
applications for tabular data, text, and images.

Fig. 9: Distribution of different fairness categories.

based applications has started to emerge since 2018, the
enthusiasm of the research community for fairness testing
on applications for tabular data remains undiminished, with
the number of related publications increasing steadily.

Compared to the overall research status, the software
engineering community primarily tackles the applications
for tabular data, which account for 80.5% of fairness testing
publications in software engineering venues. By contrast,
the publications for text- or image-based problems account
for only 13.9% and 5.6%, signiﬁcantly lower than the aver-
age level.

7.4 Fairness Categories

Figure 9 examines the distribution of different fairness
categories in the fairness testing literature. Previous work
[27], [38], [126] observed that group fairness and individual
fairness were the most widely studied in the literature.
In line with these observations, we ﬁnd that these two
categories account for a total of 93.4% of fairness testing
work. Furthermore, group fairness accounts for the largest
proportion of fairness testing work. This may be attributable
to two reasons: First, group fairness has been advocated in
legal regulations such as the four-ﬁfths rule in US law [49],
and thus needs to be considered in the software engineering
process. Second, group fairness is intuitive and easy to
understand by testers and to encode in the testing process.

7.5 Algorithm Testing Techniques

Finally, we classify existing fairness algorithm testing tech-
niques according to the software testing category (i.e., white,

attribute. As a result, the test instances with compounded
effects of multiple sensitive attributes are uncovered by
these techniques, making fairness testing insufﬁcient.

8.2 Test Oracle for Fairness Testing

Test oracle identiﬁcation for fairness testing is challeng-
ing compared to traditional software testing. The “ground
truth” (i.e., real labels) of the input data need to be man-
ually determined with intensive labor and domain-speciﬁc
knowledge. Moreover, because there is no ﬁrm consensus on
the deﬁnition of fairness, practitioners encounter difﬁculties
in determining test oracles for fairness testing even given
the real labels of the input data. It remains an open challenge
how to generate sound, complete, and correct test oracles for
fairness testing.

Existing work mainly employs metamorphic relations as
pseudo oracles or uses statistical measurements as indirect
oracles, which all involve human ingenuity. We call for
actions by the community to design automatic techniques
for constructing reliable oracles for fairness testing.

The emergence of manually-deﬁned oracles for fairness
testing brings a challenge for test oracle selection. As men-
tioned before, there are more than 70 fairness measurements
available in the IBM AIF360 toolkit alone [127], [177], with
the research community continually introducing further
novel measurements. Therefore, it is impractical to use all
the existing measurements as the test oracles for fairness
testing. Moreover, while each of these deﬁnitions is appro-
priate in a given context, many of them cannot be satisﬁed
simultaneously [5]. An important direction for the research
community therefore consists in selecting appropriate test
oracles according to the intended application scenarios.

8.3 Test Input Generation for Fairness testing

Generation of natural inputs. Although there have been
several techniques for test input generation in the area of
fairness testing, there is no guarantee that the generated
instances are legitimate and natural. Speciﬁcally, existing
techniques [82], [83], [89] are mainly based on adversarial
perturbation of input features. They do not constrain the
magnitude of the perturbation, and consider the generated
instances effective as long as they can induce the intended
output behavior, i.e., ﬂipping the predicted outcome after
changing sensitive attribute information. In this way, the
generated instances may not respect real world constraints
(e.g., they may authorize a loan to a 10-year-old individual).
Here, the open problems concern how to generate legitimate
and natural test inputs for fairness testing, and how to
automatically evaluate the naturalness of the generated test
inputs.
Exploration of more generation techniques. Test input
generation techniques have not been fully investigated in
the area of fairness testing. For example, symbolic exe-
cution is a program analysis technique to test whether
certain properties can be violated by the software under
test, and it has been demonstrated to be effective in test
input generation for traditional software and ML software
[12]. However, in the area of fairness testing, its potential
has not been well tapped. To the best of our knowledge,

20

only the SG approach [80] has used symbolic generation for
discriminatory instance generation.

In addition, fairness testing is often conducted for ML
software, whose test input generation is challenging due to
its large behavior space. Search-based software test genera-
tion ﬁts the fairness testing of ML software well, as it uses
a meta-heuristic search technique to efﬁciently search such
space [85], [86], [87]. These characteristics of search-based
software test generation make it broadly applied in different
testing scenarios [256]. Although there have been a few
search-based test input generation techniques for fairness
testing, there remains scope for much more work in this
area.

8.4 Test Adequacy for Fairness Testing

Test adequacy is a widely-studied problem in traditional
software testing, which aims to check whether the existing
tests have a good coverage. Adequacy criteria not only
provide a conﬁdence measurement on testing activities, but
also can be adopted to guide test generation. In the area of
fairness testing, test adequacy remains an open problem. To
the best of our knowledge, there has been no work exploring
test adequacy for fairness testing.

To tackle this problem, an intuitive idea is to employ
traditional software test adequacy metrics for fairness test-
ing. For example, various metrics have been proposed for
traditional software testing, such as line coverage, branch
coverage, and dataﬂow coverage [56].

In addition, recently, several test adequacy metrics have
been proposed for deep learning software, such as neu-
ron coverage, layer coverage, and surprise adequacy [12].
Neuron coverage and layer coverage measure the degree to
which the neurons and layers of a deep learning model is
executed by a test suite, respectively, while surprise ade-
quacy measures the coverage of discretized input surprise
range for deep learning software [12]. However, there is no
empirical evidence to date that these metrics are applicable
and effective for assessing the ability of revealing fairness
bugs and the sufﬁciency of fairness testing.

8.5 Test Cost Reduction

Test cost could be a serious problem in fairness testing of
ML software. Speciﬁcally, fairness testing of ML software
may require re-training ML models or repeating the pre-
diction process, and extensive data generation to explore
the large model behavior space. However, to the best of
our knowledge, there has been no work about reducing the
cost for fairness testing. It is interesting to design speciﬁc
test selection, prioritization, and minimization techniques
to reduce the cost of fairness testing without inﬂuencing the
test effectiveness.

In addition, as described in Section 5.2, there is an in-
creasing demand for the deployment of intelligent software
systems on platforms with limited computing power and
resources such as mobile devices, and several studies [65],
[65], [66], [67] have focused on fairness testing in such
scenarios. This brings a new challenge for the research
community that how to effectively conduct fairness testing
on diverse end devices even those with limited computing
power, memory size, and energy capacity.

8.6 Fairness and Other Testing Properties

Testing fairness repair techniques with more properties
considered. After fairness repair techniques have been ap-
plied to software systems, fairness testing is often performed
again. In this process, testers may also take ML performance
(e.g., accuracy) into consideration [38], [39], because it is
well-known that fairness improvement is often at the cost
of ML performance [38]. However, in addition to ML per-
formance, there are also many other properties important
for software systems, including robustness, security, efﬁ-
ciency, interpretability, and privacy [12]. The relationship
between fairness and these properties is not well studied
in the literature, and thus these relationships remain less
well understood. Future research is needed to uncover the
relationships and perform the testing with these properties
considered. The determination of the properties to be con-
sidered needs the assistance of requirements engineers.
Fairness and explainability. Explainability is deﬁned as
that users can understand why a prediction is made by
a software system [257]. Like fairness, it has also been an
important software property required by recent regulatory
provisions [258]. Because application scenarios that demand
fairness often also require explainability, it would be an
interesting research direction to consider fairness and ex-
plainability together. Many existing fairness testing studies
just generate discriminatory instances that reveal fairness
bugs in the software under test, but do not explain why
these instances are unfairly treated by the software.

In this case, software engineers have relatively little
guidance on the production of targeted ﬁxes to repair the
software. Improving the explainability behind the unfair
software outcomes can help summarize the reasons for
fairness bugs, produce insights for fairness repair, and help
stakeholders without technical background (e.g., product
managers, compliance ofﬁcers, and policy makers) under-
stand the software bias simply and quickly.

8.7 Testing Fairness of More Applications

Most of existing fairness testing work focuses on tabular
data-based classiﬁcation tasks, natural language processing
systems, or computer vision systems. Indeed, as an impor-
tant non-functional software property, fairness needs to be
considered for a broad range of software systems including
speech recognition systems, video analytic systems, multi-
modal systems, and even non-ML software systems (e.g.,
simulation systems and rule-based systems).

In addition, existing work primarily focuses on ofﬂine
fairness testing. More research is needed for online fairness
testing, because online testing can provide important in-
formation to guide software maintenance and facilitate the
evolvement of software systems.

Furthermore, researchers can extend fairness testing re-
search to cover more testing activities, such as bug report
analysis [259] and bug triage [260], which have been widely
studied in traditional software testing but rarely investi-
gated in fairness testing.

8.8 More Fairness Testing Benchmarks

Benchmarks for fairness testing are needed. In traditional
software testing, benchmarks such as Defects4J [261] have

played an important role, providing a uniﬁed standard for
evaluating software testing techniques.

21

8.9 More Fairness Testing Tools

Existing fairness testing tools (listed in Table 5) tend to
require programming skills, and thus are unfriendly to non-
technical stakeholders. However, fairness testing research
includes many non-programmer stakeholders and contrib-
utors such as compliance ofﬁcers, policy makers, and legal
practitioners.

9 CONCLUSION

We have presented a comprehensive survey of 122 papers
on fairness testing. We compared fairness testing with tradi-
tional software testing and positioned fairness testing within
the software engineering process. We summarized current
research status in the fairness testing workﬂow (including
test oracle identiﬁcation and test input generation) and
testing components (including data testing and algorithm
testing). We also listed public datasets and open source
tools that can be accessed by researchers and practitioners
interested in the fairness testing topic. We analyzed trends
and promising research directions for fairness testing. We
hope this survey will help researchers from various research
communities become familiar with the current status and
open opportunities of fairness testing.

ACKNOWLEDGMENTS

Before submitting, we sent the paper to the authors of the
collected papers, to check for accuracy and omission. Many
thanks to those authors who kindly provided comments and
feedback on earlier drafts of this paper. Zhenpeng Chen,
Max Hort, Federica Sarro, and Mark Harman are supported
by the ERC Advanced Grant under the grant number 741278
(EPIC: Evolutionary Program Improvement Collaborators).
Jie M. Zhang is partially supported by the UKRI Trustwor-
thy Autonomous Systems Node in Veriﬁability, with Grant
Award Reference EP/V026801/2.

REFERENCES

[1]

[2]

[3]

[4]

[5]

[6]

J. Chan and J. Wang, “Hiring preferences in online labor markets:
Evidence of a female hiring bias,” Management Science, vol. 64,
no. 7, pp. 2973–2994, 2018.
T. Bono, K. Croxson, and A. Giles, “Algorithmic fairness in credit
scoring,” Oxford Review of Economic Policy, vol. 37, no. 3, pp. 585–
617, 2021.
R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth, “Fair-
ness in criminal justice risk assessments: The state of the art,”
Sociological Methods & Research, vol. 50, no. 1, pp. 3–44, 2021.
C. Krittanawong, H. U. H. Virk, S. Bangalore, Z. Wang, K. W.
Johnson, R. Pinotti, H. Zhang, S. Kaplin, B. Narasimhan, T. Kitai
et al., “Machine learning prediction in cardiovascular diseases: a
meta-analysis,” Nature Scientiﬁc reports, vol. 10, no. 1, pp. 1–11,
2020.
Y. Brun and A. Meliou, “Software fairness,” in Proceedings of the
2018 ACM Joint Meeting on European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering,
ESEC/FSE 2018, 2018, pp. 754–759.
“Machine
bias,”
machine-bias-risk-assessments-in-criminal-sentencing,
retrieved on May 20, 2022.

https://www.propublica.org/article/
2016,

[9]

[8]

[7] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Gal-
styan, “A survey on bias and fairness in machine learning,” ACM
Computing Surveys, vol. 54, no. 6, pp. 115:1–115:35, 2021.
J. Buolamwini and T. Gebru, “Gender shades: Intersectional accu-
racy disparities in commercial gender classiﬁcation,” in Proceed-
ings of the Conference on Fairness, Accountability and Transparency,
FAT 2018, 2018, pp. 77–91.
“FairwARE,” https://fairwares.github.io/, 2022, retrieved on
May 20, 2022.
“FAcct,” https://facctconference.org/, 2022, retrieved on May 20,
2022.
“SE4RAI,”
se4rai-2022, 2022, retrieved on May 20, 2022.
J. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Machine learning
testing: Survey, landscapes and horizons,” IEEE Transactions on
Software Engineering, vol. 48, no. 2, pp. 1–36, 2022.

https://conf.researchr.org/home/icse-2022/

[12]

[10]

[11]

[13] A. Finkelstein, M. Harman, S. A. Mansouri, J. Ren, and Y. Zhang,
““fairness analysis” in requirements assignments,” in Proceedings
of the 16th IEEE International Requirements Engineering Conference,
RE 2008, 2008, pp. 115–124.

[14] F. B. Aydemir and F. Dalpiaz, “A roadmap for ethics-aware
software engineering,” in Proceedings of the International Workshop
on Software Fairness, FairWare@ICSE 2018, 2018, pp. 15–21.
[15] K. Ahmad, M. Bano, M. Abdelrazek, C. Arora, and J. C. Grundy,
“What’s up with requirements engineering for artiﬁcial intel-
ligence systems?” in Proceedings of the 29th IEEE International
Requirements Engineering Conference, RE 2021, 2021, pp. 1–12.
[16] R. Alidoosti, “Ethics-driven software architecture decision mak-
ing,” in Proceedings of the IEEE 18th International Conference on
Software Architecture Companion, ICSA-C 2021, 2021, pp. 90–91.

[17] A. Albarghouthi, L. D’Antoni, S. Drews, and A. V. Nori,
“Fairsquare: Probabilistic veriﬁcation of program fairness,” Pro-
ceedings of the ACM on Programming Languages, vol. 1, no. OOP-
SLA, pp. 80:1–80:30, 2017.

[18] B. Salimi, L. Rodriguez, B. Howe, and D. Suciu, “Interventional
fairness: Causal database repair for algorithmic fairness,” in
Proceedings of the 2019 International Conference on Management of
Data, SIGMOD 2019, 2019, pp. 793–810.
S. Verma and J. Rubin, “Fairness deﬁnitions explained,” in Pro-
ceedings of the International Workshop on Software Fairness, Fair-
Ware@ICSE 2018, 2018, pp. 1–7.

[19]

[20] A. Castelnovo, R. Crupi, G. Greco, D. Regoli, I. G. Penco, and
A. C. Cosentini, “A clariﬁcation of the nuances in the fairness
metrics landscape,” Scientiﬁc Reports, vol. 12, no. 1, pp. 1–21, 2022.
[21] D. Pessach and E. Shmueli, “A review on fairness in machine

learning,” ACM Computing Surveys, vol. 55, no. 3, 2022.

[22] M. Hort, Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “Bias
mitigation for machine learning classiﬁers: A comprehensive
survey,” arXiv, vol. abs/2207.07068, 2022.

[23] T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSherief, J. Zhao,
D. Mirza, E. Belding, K.-W. Chang, and W. Y. Wang, “Mitigating
gender bias in natural language processing: Literature review,”
in Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, ACL 2019, 2019, pp. 1630–1640.
[24] E. Pitoura, K. Stefanidis, and G. Koutrika, “Fairness in rankings
and recommendations: an overview,” VLDB Journal, 2021.
[25] M. Tushev, F. Ebrahimi, and A. M. Mahmoud, “A systematic
literature review of anti-discrimination design strategies in the
digital sharing economy,” IEEE Transactions on Software Engineer-
ing, 2022.

[26] B. Hutchinson and M. Mitchell, “50 years of test (un)fairness:
Lessons for machine learning,” in Proceedings of the Conference on
Fairness, Accountability, and Transparency, FAT* 2019, 2019, pp. 49–
58.

[27] E. O. Soremekun, M. Papadakis, M. Cordy, and Y. L. Traon,
“Software fairness: An analysis and survey,” CoRR, vol.
abs/2205.08809, 2022.

[28] D. Hellman, “Measuring algorithmic fairness,” Virginia Law Re-

[29]

view, vol. 106, no. 4, pp. 811–866, 2020.
S. Mitchell, E. Potash, S. Barocas, A. D’Amour, and K. Lum,
“Algorithmic fairness: Choices, assumptions, and deﬁnitions,”
Annual Review of Statistics and Its Application, vol. 8, pp. 141–163,
2021.

[30] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, “Counterfactual
fairness,” in Proceedings of the Annual Conference on Neural Infor-
mation Processing Systems 2017, NIPS 2017, 2017, pp. 4066–4076.

22

[31] N. Grgic-Hlaca, M. B. Zafar, K. P. Gummadi, and A. Weller, “The
case for process fairness in learning: Feature selection for fair
decision making,” in NIPS symposium on machine learning and the
law, vol. 1, 2016, p. 2.

[32] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. S. Zemel,
“Fairness through awareness,” in Innovations in Theoretical Com-
puter Science 2012, 2012, pp. 214–226.
S. Barocas and A. D. Selbst, “Big data’s disparate impact,” Cali-
fornia Law Review, vol. 104, p. 671, 2016.

[33]

[36]

[35]

[37]

[34] M. Hardt, E. Price, and N. Srebro, “Equality of opportunity in
supervised learning,” in Proceedings of the 2016 Annual Conference
on Neural Information Processing Systems, NIPS 2016, 2016, pp.
3315–3323.
J. Chakraborty, S. Majumder, Z. Yu, and T. Menzies, “Fairway: a
way to build fair ML software,” in Proceedings of the 28th ACM
Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE 2020, 2020, pp.
654–665.
S. Majumder, J. Chakraborty, G. R. Bai, K. T. Stolee, and T. Men-
zies, “Fair enough: Searching for sufﬁcient measures of fairness,”
CoRR, vol. abs/2110.13029, 2021.
“Ieee standard classiﬁcation for software anomalies,” IEEE Std
1044-2009 (Revision of IEEE Std 1044-1993), pp. 1–23, 2010.
[38] M. Hort, J. M. Zhang, F. Sarro, and M. Harman, “Fairea: a model
behaviour mutation approach to benchmarking bias mitigation
methods,” in Proceedings of the 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering, Athens, ESEC/FSE 2021, 2021, pp. 994–1006.
[39] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “Maat: A novel
ensemble approach to ﬁxing fairness and performance bugs for
machine learning software,” in Proceedings of the 30th ACM Joint
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/FSE 2022, 2022.
J. Kleinberg, J. Ludwig, S. Mullainathan, and A. Rambachan,
“Algorithmic fairness,” in Aea papers and proceedings, vol. 108,
2018, pp. 22–27.

[40]

[41] D. Pessach and E. Shmueli, “Algorithmic fairness,” arXiv preprint

arXiv:2001.09784, 2020.

[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” Journal of machine
Learning research, vol. 12, pp. 2825–2830, 2011.

[43] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Leven-
berg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker,
V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, “Tensor-
ﬂow: a system for large-scale machine learning,” in Proceedings
of the 12th USENIX Symposium on Operating Systems Design and
Implementation, OSDI 2016, 2016, pp. 265–283.

[44] A. Gulli and S. Pal, Deep learning with Keras. Packt Publishing

[45]

[46]

Ltd, 2017.
S. Biswas and H. Rajan, “Fair preprocessing: towards under-
standing compositional fairness of data transformers in machine
learning pipeline,” in Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/FSE 2021, 2021, pp. 981–
993.
S. Tizpaz-Niari, A. Kumar, G. Tan, and A. Trivedi, “Fairness-
aware conﬁguration of machine learning libraries,” in Proceedings
of the 44th International Conference on Software Engineering, ICSE
2022, 2022.

[48]

[47] M. E. Khan and F. Khan, “A comparative study of white box,
black box and grey box testing techniques,” International Journal
of Advanced Computer Science and Applications, vol. 3, no. 6, 2012.
J. Chakraborty, S. Majumder, and T. Menzies, “Bias in machine
learning software: why? how? what to do?” in Proceedings of
the 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens,
ESEC/FSE 2021, 2021, pp. 429–440.

[49] M. L. Wick, S. Panda, and J. Tristan, “Unlocking fairness: a trade-
off revisited,” in Proceedings of the Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, 2019, pp. 8780–
8789.

[50] Y. Li, L. Meng, L. Chen, L. Yu, D. Wu, Y. Zhou, and B. Xu,
“Training data debugging for the fairness of machine learning
software,” in Proceedings of the 44th International Conference on
Software Engineering, ICSE 2022, 2022.

[51] H. V. Pham, T. Lutellier, W. Qi, and L. Tan, “CRADLE: cross-
backend validation to detect and localize bugs in deep learning
libraries,” in Proceedings of the 41st International Conference on
Software Engineering, ICSE 2019, 2019, pp. 1027–1038.

[52] Z. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang, “Deep learning
library testing via effective model generation,” in Proceedings of
the 28th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE
2020, 2020, pp. 788–799.

[53] M. Nejadgholi and J. Yang, “A study of oracle approximations
in testing deep learning libraries,” in Proceedings of the 34th
IEEE/ACM International Conference on Automated Software Engi-
neering, ASE 2019, 2019, pp. 785–796.

[55]

[54] P. Ma, S. Wang, and J. Liu, “Metamorphic testing and certiﬁed
mitigation of fairness violations in NLP models,” in Proceedings
of the Twenty-Ninth International Joint Conference on Artiﬁcial Intel-
ligence, IJCAI 2020, 2020, pp. 458–465.
S. Galhotra, Y. Brun, and A. Meliou, “Fairness testing: testing
software for discrimination,” in Proceedings of the 2017 Joint Meet-
ing on Foundations of Software Engineering, ESEC/FSE 2017, 2017,
pp. 498–510.
J. Zhang, L. Zhang, M. Harman, D. Hao, Y. Jia, and L. Zhang,
“Predictive mutation testing,” IEEE Transactions on Software Engi-
neering, vol. 45, no. 9, pp. 898–918, 2019.
I. Ozkaya, “Ethics is a software design concern,” IEEE Software,
vol. 36, no. 3, pp. 4–8, 2019.

[57]

[56]

[58] A. Rashid, K. Moore, C. May-Chahal, and R. Chitchyan, “Man-
aging emergent ethical concerns for software engineering in soci-
ety,” in Proceedings of the 37th IEEE/ACM International Conference
on Software Engineering, ICSE 2015, 2015, pp. 523–526.

[59] A. J. Thomson and D. L. Schmoldt, “Ethics in computer software
design and development,” Computers and Electronics in Agricul-
ture, vol. 30, no. 1-3, pp. 85–102, 2001.

[60] D. Gotterbarn, “Software engineering ethics,” Encyclopedia of

Software Engineering, vol. 2, 2001.

[61] K. M. Habibullah, G. Gay, and J. Horkoff, “Non-functional
requirements for machine learning: An exploration of system
scope and interest,” in Proceedings of the Workshop on Software
Engineering for Responsible AI, SE4RAI 2022, 2022.

[62] Z. Yang, H. Jain, J. Shi, M. H. Asyroﬁ, and D. Lo, “Biasheal: On-
the-ﬂy black-box healing of bias in sentiment analysis systems,”
in Proceedings of the IEEE International Conference on Software
Maintenance and Evolution, ICSME 2021, 2021, pp. 644–648.
[63] Z. Chen, Y. Cao, Y. Liu, H. Wang, T. Xie, and X. Liu, “A
comprehensive study on challenges in deploying deep learning
based software,” in Proceedings of the 28th ACM Joint European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ESEC/FSE 2020, 2020, pp. 750–762.
[64] Z. Chen, H. Yao, Y. Lou, Y. Cao, Y. Liu, H. Wang, and X. Liu,
“An empirical study on deployment faults of deep learning
based mobile applications,” in Proceedings of the 43rd IEEE/ACM
International Conference on Software Engineering, ICSE 2021, 2021,
pp. 674–685.

[66]

[65] C. Blakeney, N. Huish, Y. Yan, and Z. Zong, “Simon says:
Evaluating and mitigating bias in pruned neural networks with
knowledge distillation,” CoRR, vol. abs/2106.07849, 2021.
S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Den-
ton, “Characterising bias in compressed models,” CoRR, vol.
abs/2010.03058, 2020.
S. Stoychev and H. Gunes, “The effect of model compression
on fairness in facial expression recognition,” in Proceedings of
Workshop on Applied Affect Recognition at the 26th International
Conference on Pattern Recognition, ICPR 2022, 2022.

[67]

[68] Z. Yang, M. H. Asyroﬁ, and D. Lo, “Biasrv: uncovering biased
sentiment predictions at runtime,” in Proceedings of the 29th ACM
Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/FSE 2021, 2021, pp.
1540–1544.

[70]
[71]

[69] G. Tamburrelli and A. Margara, “Towards automated A/B test-
ing,” in Search-Based Software Engineering - 6th International Sympo-
sium, SSBSE 2014, Fortaleza, Brazil, August 26-29, 2014. Proceedings,
2014, pp. 184–198.
“DBLP,” https://dblp.org, 2022, retrieved on May 20, 2022.
J. Chen, J. Patra, M. Pradel, Y. Xiong, H. Zhang, D. Hao, and
L. Zhang, “A survey of compiler testing,” ACM Computing Sur-
veys, vol. 53, no. 1, pp. 4:1–4:36, 2020.

[72] L. Zhang, J. Tian, J. Jiang, Y. Liu, M. Pu, and T. Yue, “Empirical
research in software engineering - A literature survey,” Journal of
Computer Science and Technology, vol. 33, no. 5, pp. 876–899, 2018.

23

[73] G. Mathew, A. Agrawal, and T. Menzies, “Finding trends in
software research,” IEEE Transactions on Software Engineering,
2018.

[74] V. Garousi and J. M. Fernandes, “Highly-cited papers in software
engineering: The top-100,” Information and Software Technology,
vol. 71, pp. 108–128, 2016.

[75] B. Lin, N. Cassee, A. Serebrenik, G. Bavota, N. Novielli, and
M. Lanza, “Opinion mining for software development: A system-
atic literature review,” ACM Transactions on Software Engineering
Methodology, vol. 31, no. 3, pp. 38:1–38:41, 2022.
S. Jalali and C. Wohlin, “Systematic literature studies: database
searches vs. backward snowballing,” in Proceedings of the 2012
ACM-IEEE International Symposium on Empirical Software Engineer-
ing and Measurement, ESEM 2012, 2012, pp. 29–38.

[76]

[77] R. Angell, B. Johnson, Y. Brun, and A. Meliou, “Themis: auto-
matically testing software for discrimination,” in Proceedings of
the 2018 ACM Joint Meeting on European Software Engineering Con-
ference and Symposium on the Foundations of Software Engineering,
ESEC/FSE 2018, 2018, pp. 871–875.

[78] M. Fan, W. Wei, W. Jin, Z. Yang, and T. Liu, “Explanation-guided
fairness testing through genetic algorithm,” in Proceedings of the
44th International Conference on Software Engineering, ICSE 2022,
2022.
S. Udeshi, P. Arora, and S. Chattopadhyay, “Automated directed
fairness testing,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ASE 2018, 2018, pp.
98–108.

[79]

[80] A. Aggarwal, P. Lohia, S. Nagar, K. Dey, and D. Saha, “Black box
fairness testing of machine learning models,” in Proceedings of the
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE
2019, 2019, pp. 625–635.

[81] W. Xie and P. Wu, “Fairness testing of machine learning models
using deep reinforcement learning,” in Proceedings of the 19th
IEEE International Conference on Trust, Security and Privacy in
Computing and Communications, TrustCom 2020, 2020, pp. 121–128.
[82] P. Zhang, J. Wang, J. Sun, G. Dong, X. Wang, X. Wang, J. S.
Dong, and T. Dai, “White-box fairness testing through adversar-
ial sampling,” in Proceedings of the 42nd International Conference on
Software Engineering, ICSE 2020, 2020, pp. 949–960.

[83] L. Zhang, Y. Zhang, and M. Zhang, “Efﬁcient white-box fairness
testing through gradient search,” in Proceedings of the 30th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
ISSTA 2021, 2021, pp. 103–114.

[84] H. Zheng, Z. Chen, T. Du, X. Zhang, Y. Cheng, S. Ji, J. Wang,
Y. Yu, and J. Chen, “Neuronfair - interpretable white-box fairness
testing through biased neuron identiﬁcation,” in Proceedings of the
44th International Conference on Software Engineering, ICSE 2022,
2022.

[85] M. Harman and B. F. Jones, “Search-based software engineering,”
Information Software Technology, vol. 43, no. 14, pp. 833–839, 2001.
[86] K. Lakhotia, M. Harman, and P. McMinn, “A multi-objective
approach to search-based test data generation,” in Proceedings of
the Genetic and Evolutionary Computation Conference, GECCO 2007,
2007, pp. 1098–1105.

[87] M. Harman, Y. Jia, and Y. Zhang, “Achievements, open problems
and challenges for search based software testing,” in Proceedings
of the 8th IEEE International Conference on Software Testing, Veriﬁca-
tion and Validation, ICST 2015, 2015, pp. 1–12.

[88] A. Perera, A. Aleti, C. Tantithamthavorn, J. Jiarpakdee, B. Turhan,
L. Kuhn, and K. Walker, “Search-based fairness testing for
regression-based machine learning systems,” Empirical Software
Engineering, vol. 27, no. 79, 2022.

[89] P. Zhang, J. Wang, J. Sun, X. Wang, G. Dong, X. Wang, T. Dai,
and J. S. Dong, “Automatic fairness testing of neural classiﬁers
through adversarial sampling,” IEEE Transactions on Software
Engineering, pp. 1–1, 2021.

[90] G. Tao, W. Sun, T. Han, C. Fang, and X. Zhang, “Ruler: Dis-
criminative and iterative adversarial training for deep neural
network fairness,” in Proceedings of the 30th ACM Joint European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ESEC/FSE 2022, 2022.

[91] O. Bastani, X. Zhang, and A. Solar-Lezama, “Probabilistic veriﬁ-
cation of fairness properties via concentration,” Proceedings of the
ACM on Programming Languages, vol. 3, no. OOPSLA, pp. 118:1–
118:27, 2019.

[92] P. G. John, D. Vijaykeerthy, and D. Saha, “Verifying individual
fairness in machine learning models,” in Proceedings of the Thirty-
Sixth Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2020,
2020, pp. 749–758.

[93] C. Urban, M. Christakis, V. W ¨ustholz, and F. Zhang, “Perfectly
parallel fairness certiﬁcation of neural networks,” Proceedings of
the ACM on Programming Languages, vol. 4, no. OOPSLA, pp.
185:1–185:30, 2020.

[94] B. Sun, J. Sun, T. Dai, and L. Zhang, “Probabilistic veriﬁcation
of neural networks against group fairness,” in Proceedings of the
24th International Symposium on Formal Methods, FM 2021, 2021,
pp. 83–102.

[95] B. Ghosh, D. Basu, and K. S. Meel, “Justicia: A stochastic SAT
approach to formally verify fairness,” in Proceedings of the Thirty-
Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, 2021,
pp. 7554–7563.

[96] ——, “Algorithmic fairness veriﬁcation with graphical models,”
in Proceedings of the Thirty-Sixth AAAI Conference on Artiﬁcial
Intelligence, AAAI 2022, 2022.

[97] A. Sharma, C. Demir, A.-C. N. Ngomo, and H. Wehrheim,
“Mlcheck–property-driven testing of machine learning classi-
ﬁers,” in Proceedings of the 20th IEEE International Conference on
Machine Learning and Applications, ICMLA 2020, 2021, pp. 738–
745.

[98] A. Sharma and H. Wehrheim, “Automatic fairness testing of
machine learning models,” in Proceedings of the 32nd International
Conference on Testing Software and Systems, ICTSS 2020, 2020, pp.
255–271.

[99] L. M. de Moura and N. S. Bjørner, “Z3: an efﬁcient SMT solver,”
in Proceedings of the 14th International Conference on Tools and
Algorithms for the Construction and Analysis of Systems, TACAS
2008, 2008, pp. 337–340.

[100] M. D´ıaz, I. Johnson, A. Lazar, A. M. Piper, and D. Gergle, “Ad-
dressing age-related bias in sentiment analysis,” in Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems,
CHI 2018, 2018, p. 412.

[101] S. Kiritchenko and S. Mohammad, “Examining gender and race
bias in two hundred sentiment analysis systems,” in Proceedings of
the Seventh Joint Conference on Lexical and Computational Semantics,
*SEM@NAACL-HLT 2018, 2018, pp. 43–53.

[102] E. Sheng, K. Chang, P. Natarajan, and N. Peng, “The woman
worked as a babysitter: On biases in language generation,” in
Proceedings of the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International Joint Conference
on Natural Language Processing, EMNLP-IJCNLP 2019, 2019, pp.
3405–3410.

[103] P. Huang, H. Zhang, R. Jiang, R. Stanforth, J. Welbl, J. Rae,
V. Maini, D. Yogatama, and P. Kohli, “Reducing sentiment bias in
language models via counterfactual evaluation,” in Proceedings of
the Findings of the Association for Computational Linguistics: EMNLP
2020, 2020, pp. 65–83.

[104] J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun,
K. Chang, and R. Gupta, “BOLD: dataset and metrics for mea-
suring biases in open-ended language generation,” in Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and Trans-
parency, FAccT 2021, 2021, pp. 862–872.

[105] N. Mehrabi, T. Gowda, F. Morstatter, N. Peng, and A. Galstyan,
“Man is to person as woman is to location: Measuring gender
bias in named entity recognition,” in Proceedings of the 31st ACM
Conference on Hypertext and Social Media, HT 2020, 2020, pp. 231–
232.

[106] J. Wang, B. I. P. Rubinstein, and T. Cohn, “Measuring and miti-
gating name biases in neural machine translation,” in Proceedings
of the 60th Annual Meeting of the Association for Computational
Linguistics, ACL 2022, 2022, pp. 2576–2590.

[107] S. Sharma, M. Dey, and K. Sinha, “Evaluating gender bias in
natural language inference,” in Proceedings of the NeurIPS 2020
Workshop on Dataset Curation and Security, 2020.

[108] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond accuracy:
Behavioral testing of NLP models with checklist,” in Proceedings
of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, 2020, pp. 4902–4912.

[109] S. Poria, D. Hazarika, N. Majumder, and R. Mihalcea, “Beneath
the tip of the iceberg: Current challenges and new directions
in sentiment analysis research,” IEEE Transactions on Affective
Computing, 2020.

[110] M. H. Asyroﬁ, Z. Yang, I. N. B. Yusuf, H. J. Kang, F. Thung, and
D. Lo, “Biasﬁnder: Metamorphic test generation to uncover bias

24

for sentiment analysis systems,” IEEE Transactions on Software
Engineering, pp. 1–1, 2021.

[111] E. Soremekun, S. S. Udeshi, and S. Chattopadhyay, “Astraea:
Grammar-based fairness testing,” IEEE Transactions on Software
Engineering, pp. 1–1, 2022.

[112] Z. Sun, J. M. Zhang, M. Harman, M. Papadakis, and L. Zhang,
“Automatic testing and improvement of machine translation,”
in Proceedings of the 42nd International Conference on Software
Engineering, ICSE 2020, 2020, pp. 974–985.

[113] Z. Sun, J. M. Zhang, Y. Xiong, M. Harman, M. Papadakis, and
L. Zhang, “Improving machine translation systems via isotopic
replacement,” in Proceedings of the 44th IEEE/ACM 44th Inter-
national Conference on Software Engineering, ICSE 2022, 2022, pp.
1181–1192.

[114] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. C. Courville, and Y. Bengio, “Generative
adversarial nets,” in Proceedings of the Annual Conference on Neural
Information Processing Systems 2014, NIPS 2014, 2014, pp. 2672–
2680.

[115] E. Denton, B. Hutchinson, M. Mitchell, and T. Gebru, “Detecting
bias with generative counterfactual face attribute augmentation,”
in Proceedings of the CVPR 2019 Workshop on Fairness Accountability
Transparency and Ethics in Computer Vision, 2019.

[116] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing
of gans for improved quality, stability, and variation,” in Proceed-
ings of the 6th International Conference on Learning Representations,
ICLR 2018, 2018.

[117] J. Joo and K. K¨arkk¨ainen, “Gender slopes: Counterfactual fairness
for computer vision models by attribute manipulation,” CoRR,
vol. abs/2005.10430, 2020.

[118] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, and
M. Ranzato, “Fader networks: Manipulating images by sliding
attributes,” in Proceedings of the Annual Conference on Neural
Information Processing Systems 2017, NIPS 2017, 2017, pp. 5967–
5976.

[119] P. Zhang, J. Wang, J. Sun, and X. Wang, “Fairness testing of
deep image classiﬁcation with adequacy metrics,” CoRR, vol.
abs/2111.08856, 2021.

[120] J. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-
image translation using cycle-consistent adversarial networks,”
in Proceedings of the IEEE International Conference on Computer
Vision, ICCV 2017, 2017, pp. 2242–2251.

[121] E. Denton, B. Hutchinson, M. Mitchell, T. Gebru, and A. Zal-
divar, “Image counterfactual sensitivity analysis for detecting
unintended bias,” in Proceedings of the CVPR 2019 Workshop on
Fairness Accountability Transparency and Ethics in Computer Vision,
2019.

[122] S. Dash, V. N. Balasubramanian, and A. Sharma, “Evaluating and
mitigating bias in image classiﬁers: A causal perspective using
counterfactuals,” in Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision, WACV 2022, 2022, pp. 3879–
3888.

[123] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE Transactions
on Software Engineering, vol. 41, no. 5, pp. 507–525, 2015.

[124] S. S. Rajan, S. Udeshi, and S. Chattopadhyay, “Aequevox: Auto-
mated fairness testing of speech recognition systems,” in Proceed-
ings of the 25th International Conference on Fundamental Approaches
to Software Engineering, FASE 2022, 2022, pp. 245–267.

[125] A. Sharma and H. Wehrheim, “Testing machine learning algo-
rithms for balanced data usage,” in Proceedings of the 12th IEEE
Conference on Software Testing, Validation and Veriﬁcation, ICST
2019, 2019, pp. 125–135.

[126] J. M. Zhang and M. Harman, ““ignorance and prejudice” in soft-
ware fairness,” in Proceedings of the 43rd IEEE/ACM International
Conference on Software Engineering, ICSE 2021, 2021, pp. 1436–
1447.

[127] “AIF360,” https://aif360.mybluemix.net, retrieved on May 20,

2022.

[128] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and
S. Venkatasubramanian, “Certifying and removing disparate im-
pact,” in Proceedings of the 21th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD 2015, 2015,
pp. 259–268.

[129] A. Chouldechova, “Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments,” Big Data,
vol. 5, no. 2, pp. 153–163, 2017.

[130] J. M. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent
trade-offs in the fair determination of risk scores,” in 8th Innova-
tions in Theoretical Computer Science Conference, ITCS 2017, 2017,
pp. 43:1–43:23.

[131] Y. Tian, Z. Zhong, V. Ordonez, G. E. Kaiser, and B. Ray, “Testing
DNN image classiﬁers for confusion & bias errors,” in Proceedings
of the 42nd International Conference on Software Engineering, ICSE
2020, 2020, pp. 1122–1134.

[132] B. L. Thanh, S. Ruggieri, and F. Turini, “k-nn as an imple-
mentation of situation testing for discrimination discovery and
prevention,” in Proceedings of the 17th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD 2011,
2011, pp. 502–510.

[133] L. Zhang, Y. Wu, and X. Wu, “Situation testing-based discrimina-
tion discovery: A causal inference approach,” in Proceedings of the
Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence,
IJCAI 2016, 2016, pp. 2718–2724.

[134] J. Pearl, Causality. Cambridge university press, 2009.
[135] E. Black, S. Yeom, and M. Fredrikson, “Fliptest: fairness testing
via optimal transport,” in Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT* 2020, 2020, pp. 111–121.

[136] B. Taskesen, J. H. Blanchet, D. Kuhn, and V. A. Nguyen, “A
statistical test for probabilistic fairness,” in Proceedings of the
2021 ACM Conference on Fairness, Accountability, and Transparency,
FAccT 2021, 2021, pp. 648–665.

[137] N. Si, K. Murthy, J. H. Blanchet, and V. A. Nguyen, “Testing group
fairness via optimal transport projections,” in Proceedings of the
38th International Conference on Machine Learning, ICML 2021, 2021,
pp. 9649–9659.

[138] C. Villani, Optimal transport: old and new. Springer, 2009, vol. 338.
[139] C. Kuhlman, W. Gerych, and E. A. Rundensteiner, “Measuring
group advantage: A comparative study of fair ranking metrics,”
in Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society, AIES 2021, 2021, pp. 674–682.

[140] G. Gezici, A. Lipani, Y. Saygin, and E. Yilmaz, “Evaluation
metrics for measuring bias in search engine results,” Information
Retrieval Journal, vol. 24, no. 2, pp. 85–113, 2021.

[141] L. E. Celis, D. Straszak, and N. K. Vishnoi, “Ranking with fairness

constraints,” arXiv preprint arXiv:1704.06840, 2017.

[142] A. Singh and T. Joachims, “Fairness of exposure in rankings,” in
Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 2018, pp. 2219–2228.
[143] K. Yang and J. Stoyanovich, “Measuring fairness in ranked out-
puts,” in Proceedings of the 29th international conference on scientiﬁc
and statistical database management, 2017, pp. 1–6.

[144] C. Kuhlman, M. VanValkenburg, and E. Rundensteiner, “Fare:
Diagnostics for fair ranking using pairwise error metrics,” in
Proceedings of The World Wide Web Conference 2019, 2019, pp. 2936–
2942.

[145] H. Narasimhan, A. Cotter, M. R. Gupta, and S. Wang, “Pairwise
fairness for ranking and regression,” in Proceedings of The Thirty-
Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, 2020,
pp. 5248–5255.

[146] A. Beutel, J. Chen, T. Doshi, H. Qian, L. Wei, Y. Wu, L. Heldt,
Z. Zhao, L. Hong, E. H. Chi, and C. Goodrow, “Fairness in
recommendation ranking through pairwise comparisons,” in
Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2019, 2019, pp. 2212–
2220.

[147] C. DiCiccio, S. Vasudevan, K. Basu, K. Kenthapadi, and D. Agar-
wal, “Evaluating fairness using permutation tests,” in Proceedings
of the 26th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, KDD 2020, 2020, pp. 1467–1477.

[148] F. Tram`er, V. Atlidakis, R. Geambasu, D. J. Hsu, J. Hubaux,
M. Humbert, A. Juels, and H. Lin, “Fairtest: Discovering unwar-
ranted associations in data-driven applications,” in Proceedings
of the 2017 IEEE European Symposium on Security and Privacy,
EuroS&P 2017, 2017, pp. 401–416.

[149] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K. Chang, “Men
also like shopping: Reducing gender bias ampliﬁcation using
corpus-level constraints,” in Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2017,
2017, pp. 2979–2989.

[150] A. Wang and O. Russakovsky, “Directional bias ampliﬁcation,” in
Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 2021, pp. 10 882–10 893.

[151] T. Wang, J. Zhao, M. Yatskar, K. Chang, and V. Ordonez, “Bal-
anced datasets are not enough: Estimating and mitigating gender

25

bias in deep image representations,” in Proceedings of the 2019
IEEE/CVF International Conference on Computer Vision, ICCV 2019,
2019, pp. 5309–5318.

[152] K. Cachel and E. A. Rundensteiner, “FINS auditing framework:
Group fairness for subset selections,” in Proceedings of AAAI/ACM
Conference on AI, Ethics, and Society, AIES 2022, 2022, pp. 144–155.
‘unbiased’ algorithms can
be inadvertently racist,” https://www.businessinsider.com/
how-algorithms-can-be-racist-2016-4?r=US&IR=T,
2016,
retrieved on May 20, 2022.

[153] “Amazon just showed us that

[154] K. Peng, J. Chakraborty, and T. Menzies, “xFAIR: Better fair-
ness via model-based rebalancing of protected attributes,” arXiv
preprint arXiv:2110.01109, 2021.

[155] Y. Chen and J. Joo, “Understanding and mitigating annotation
bias in facial expression recognition,” in Proceedings of the 2021
IEEE/CVF International Conference on Computer Vision, ICCV 2021,
2021.
[156] “The

https://github.com/propublica/

dataset,”

compas

compas-analysis, 2016, retrieved on May 20, 2022.

[157] S. Yeom and M. C. Tschantz, “Avoiding disparity ampliﬁcation
under different worldviews,” in Proceedings of the 2021 ACM Con-
ference on Fairness, Accountability, and Transparency, FAccT 2021,
2021, pp. 273–283.

[158] J. Chakraborty, S. Majumder, and H. Tu, “Fair-ssl: Building fair
ml software with less data,” in Proceedings of the International
Workshop on Software Fairness, FairWare@ICSE 2022, 2022.
[159] K. K¨arkk¨ainen and J. Joo, “Fairface: Face attribute dataset for
balanced race, gender, and age for bias measurement and mitiga-
tion,” in Proceedings of the IEEE Winter Conference on Applications
of Computer Vision, WACV 2021, 2021, pp. 1547–1557.

[160] A. Wang, A. Narayanan, and O. Russakovsky, “REVISE: A tool
for measuring and mitigating bias in visual datasets,” in Proceed-
ings of the 16th European Conference, ECCV 2020, 2020, pp. 733–751.
[161] A. Torralba and A. A. Efros, “Unbiased look at dataset bias,”
in Proceedings of the 24th IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2011, 2011, pp. 1521–1528.

[162] Y. Yang, A. Gupta, J. Feng, P. Singhal, V. Yadav, Y. Wu, P. Natara-
jan, V. Hedau, and J. Joo, “Enhancing fairness in face detection
in computer vision systems by demographic bias mitigation,”
in Proceedings of AAAI/ACM Conference on AI, Ethics, and Society,
AIES 2022, 2022, pp. 813–822.
FACE,”
2022, retrieved on May 20, 2022.

http://shuoyang1213.me/WIDERFACE/,

[163] “WIDER

[164] A. Mambreyan, E. Punskaya, and H. Gunes, “Dataset bias in
deception detection,” in Proceedings of the 26th International Con-
ference on Pattern Recognition, ICPR 2022, 2022.

[165] I. Valentim, N. Lourenc¸o, and N. Antunes, “The impact of data
preparation on the fairness of software systems,” in Proceedings
of the 30th IEEE International Symposium on Software Reliability
Engineering, ISSRE 2019, 2019, pp. 391–401.

[166] J. Chakraborty, T. Xia, F. M. Fahid, and T. Menzies, “Software
engineering for fairness: A case study with hyperparameter
optimization,” CoRR, vol. abs/1905.05786, 2019.

[167] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq,
“Algorithmic decision making and the cost of fairness,” in Pro-
ceedings of the 23rd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD 2017, 2017, pp. 797–
806.

[168] V. Nair, Z. Yu, T. Menzies, N. Siegmund, and S. Apel, “Finding
faster conﬁgurations using FLASH,” IEEE Transactions on Software
Engineering, vol. 46, no. 7, pp. 794–811, 2020.

[169] J. Zhang, I. Beschastnikh, S. Mechtaev, and A. Roychoudhury,
“Fairness-guided smt-based rectiﬁcation of decision trees and
random forests,” in Proceedings of the International Workshop on
Software Fairness, FairWare@ICSE 2022, 2022.

[170] J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer,
and S. M. Shieber, “Investigating gender bias in language models
using causal mediation analysis,” in Proceedings of the 2020 Annual
Conference on Neural Information Processing Systems, NeurIPS 2020,
2020.

[171] R. Hicks and D. Tingley, “Causal mediation analysis,” The Stata

Journal, vol. 11, no. 4, pp. 605–619, 2011.

[172] X. Gao, J. Zhai, S. Ma, C. Shen, Y. Chen, and Q. Wang, “Fairneu-
ron: Improving deep neural network fairness with adversary
games on selective neurons,” in Proceedings of the 44th Interna-
tional Conference on Software Engineering, ICSE 2022, 2022.

[173] S. Biswas and H. Rajan, “Do the machine learning models on
a crowd sourced platform exhibit bias? An empirical study on
model fairness,” in Proceedings of the 28th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/FSE 2020, 2020, pp. 642–
653.

[174] S. Qian, V. H. Pham, T. Lutellier, Z. Hu, J. Kim, L. Tan, Y. Yu,
J. Chen, and S. Shah, “Are my deep learning systems fair?
an empirical study of ﬁxed-seed training,” Advances in Neural
Information Processing Systems, vol. 34, pp. 30 211–30 227, 2021.

[175] M. Zhang and J. Sun, “Adaptive fairness improvement based on
causality analysis,” in Proceedings of the 30th ACM Joint European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering, ESEC/FSE 2022, 2022.

[176] B. Sun, J. Sun, L. H. Pham, and T. Shi, “Causality-based neural
network repair,” in Proceedings of the 44th IEEE/ACM 44th Inter-
national Conference on Software Engineering, ICSE 2022, 2022, pp.
338–349.

[177] R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde,
K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, S. Nagar,
K. N. Ramamurthy, J. T. Richards, D. Saha, P. Sattigeri, M. Singh,
K. R. Varshney, and Y. Zhang, “AI fairness 360: An extensible
toolkit for detecting, understanding, and mitigating unwanted
algorithmic bias,” CoRR, vol. abs/1810.01943, 2018.

[178] Z. Chen, J. M. Zhang, F. Sarro, and M. Harman, “A compre-
hensive empirical study of bias mitigation methods for software
fairness,” CoRR, vol. abs/2207.03277, 2022.

[179] M. Hort and F. Sarro, “Did you do your homework? raising
awareness on software fairness and discrimination,” in Proceed-
ings of the 36th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2021, 2021, pp. 1322–1326.

[180] H. Orgad, S. Goldfarb-Tarrant, and Y. Belinkov, “How gender
debiasing affects internal model representations, and why it
matters,” in Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, 2022, pp. 2602–2628.
[181] H. Orgad and Y. Belinkov, “Choose your lenses: Flaws in gender
bias evaluation,” in Proceedings of the 4th Workshop on Gender Bias
in Natural Language Processing, GeBNLP 2022, 2022, pp. 151–167.

[182] S. Dev, T. Li, J. M. Phillips, and V. Srikumar, “On measuring and
mitigating biased inferences of word embeddings,” in Proceedings
of the Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI
2020, 2020, pp. 7659–7666.

[183] C. May, A. Wang, S. Bordia, S. R. Bowman, and R. Rudinger, “On
measuring social biases in sentence encoders,” in Proceedings of
the 2019 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies,
NAACL 2019, 2019, pp. 622–628.

[184] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model
compression and acceleration for deep neural networks,” CoRR,
vol. abs/1710.09282, 2017.

[185] X. Lin, S. Kim, and J. Joo, “Fairgrape: Fairness-aware gradient
pruning method for face attribute classiﬁcation,” Proceedings of
the 2022 European Conference, Glasgow, UK, ECCV 2022, 2022.
[186] G. Xu and Q. Hu, “Can model compression improve NLP fair-

[196] “The

communities

and crime dataset,”

http://archive.

26

ics.uci.edu/ml/datasets/Communities%20and%20Crime%
20Unnormalized, 2011, retrieved on May 20, 2022.
dataset,”

https://archive.ics.uci.edu/ml/

diabetes

[197] “The

datasets/diabetes, retrieved on May 20, 2022.

[198] “The heritage health dataset,” https://foreverdata.org/1015/

index.html, 2015, retrieved on May 20, 2022.

[199] “The

fraud detection dataset,” https://www.kaggle.com/
competitions/frauddetection/data, 2000, retrieved on May 20,
2022.

[200] “The us executions dataset,” https://data.world/markmarkoh/

executions-since-1977, 1977, retrieved on May 20, 2022.

[201] “The celeba dataset,” https://mmlab.ie.cuhk.edu.hk/projects/

CelebA.html, 2015, retrieved on May 20, 2022.

[202] “LFW,” http://vis-www.cs.umass.edu/lfw/, retrieved on May

20, 2022.

[203] “The bupt-transferface dataset,” http://www.whdeng.cn/RFW/

Trainingdataste.html, 2019, retrieved on May 20, 2022.

[204] “The vggface dataset,” https://www.robots.ox.ac.uk/∼vgg/

data/vgg face/#publi, 2015, retrieved on May 20, 2022.

[205] “The fairface dataset,” https://github.com/dchen236/FairFace,

2021, retrieved on May 20, 2022.

[206] “The pilot parliaments dataset,” http://gendershades.org, 2018,

retrieved on May 20, 2022.

[207] “Casual-Conversations,”

https://ai.facebook.com/datasets/

casual-conversations-dataset/, retrieved on May 20, 2022.
[208] “WinoST,” https://zenodo.org/record/4139080#.YtGEc-zMJAc,

2020, retrieved on May 20, 2022.

[209] “The

eec

dataset,”

https://competitions.codalab.org/

competitions/17751, 2018, retrieved on May 20, 2022.

[210] “The winobias dataset,” https://paperswithcode.com/dataset/

winobias, 2018, retrieved on May 20, 2022.

[211] “The imdb dataset,” https://www.imdb.com/interfaces/, 2011,

retrieved on May 20, 2022.

[212] A. Mandal, S. Leavy, and S. Little, “Dataset diversity: Measuring
and mitigating geographical bias in image search and retrieval,”
in Proceedings of the 1st International Workshop on Trustworthy AI
for Multimedia Computing, Trustworthy AI 2021, 2021, pp. 19–25.

[213] J. Chakraborty, K. Peng, and T. Menzies, “Making fair ML
software using trustworthy explanation,” in Proceedings of the
35th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2020, 2020, pp. 1229–1233.

[214] T. L. Quy, A. Roy, V. Iosiﬁdis, and E. Ntoutsi, “A survey
on datasets for fairness-aware machine learning,” CoRR, vol.
abs/2110.00530, 2021.

[215] A. Fabris, S. Messina, G. Silvello, and G. A. Susto, “Algorithmic
fairness datasets: the story so far,” CoRR, vol. abs/2202.01711,
2022.

[216] “FairTest,” https://github.com/columbia/fairtest, retrieved on

May 20, 2022.

[217] “Themis,”

https://github.com/LASER-UMASS/Themis,

retrieved on May 20, 2022.

[218] “Aequitas,”

https://github.com/sakshiudeshi/Aequitas,

retrieved on May 20, 2022.

[219] “ExpGA,” https://github.com/waving7799/ExpGA, retrieved

ness,” CoRR, vol. abs/2201.08542, 2022.

on May 20, 2022.

[187] “The adult census income dataset,” https://archive.ics.uci.edu/

[220] “fairCheck,” https://github.com/arnabsharma91/fairCheck, re-

ml/datasets/adult, 2017, retrieved on May 20, 2022.

trieved on May 20, 2022.

[188] “The german credit dataset,” https://archive.ics.uci.edu/ml/
datasets/Statlog+%28German+Credit+Data%29, 1994, retrieved
on May 20, 2022.

[189] “The default credit dataset,” https://archive.ics.uci.edu/ml/
datasets/default+of+credit+card+clients, 2016, retrieved on May
20, 2022.
[190] “The home

credit dataset,” https://www.kaggle.com/c/

home-credit-default-risk, 2017, retrieved on May 20, 2022.
[191] “The bank dataset,” https://archive.ics.uci.edu/ml/datasets/

[221] “MLCheck,” https://github.com/anonymseal/MLCheck, 2022,

retrieved on May 20, 2022.

[222] “LTDD,” https://github.com/fairnesstest/LTDD, retrieved on

May 20, 2022.

[223] “Fair-SMOTE,”

https://github.com/joymallyac/Fair-SMOTE,

retrieved on May 20, 2022.

[224] “xFAIR,” https://github.com/anonymous12138/biasmitigation,

retrieved on May 20, 2022.

[225] “Fairway,” https://github.com/joymallyac/Fairway, retrieved

Bank+Marketing, 2014, retrieved on May 20, 2022.

on May 20, 2022.

[192] “The mep dataset,” https://meps.ahrq.gov/mepsweb/data
stats/download data ﬁles detail.jsp?cboPufNumber=HC-181,
2015, retrieved on May 20, 2022.

[193] “The heart health dataset,” https://archive.ics.uci.edu/ml/
datasets/Heart+Disease, 2001, retrieved on May 20, 2022.
[194] “The student performance dataset,” https://archive.ics.uci.edu/
ml/datasets/Student+Performance, 2014, retrieved on May 20,
2022.

[195] “The titanic dataset,” https://www.kaggle.com/c/titanic/data,

2017, retrieved on May 20, 2022.

[226] “Parfait-ML,” https://github.com/Tizpaz/Parfait-ML, retrieved

on May 20, 2022.

[227] “Fairea,” https://github.com/maxhort/Fairea, retrieved on May

20, 2022.
[228] “scikit-fairness,”

https://scikit-fairness.readthedocs.io/en/

latest/, retrieved on May 20, 2022.

[229] S. Vasudevan and K. Kenthapadi, “Lift: A scalable framework
for measuring fairness in ML applications,” in Proceedings of the
29th ACM International Conference on Information and Knowledge
Management, CIKM 2020, 2020, pp. 2773–2780.

[230] “LiFT,” https://github.com/linkedin/LiFT, 2020, retrieved on

May 20, 2022.

[231] M. Hardt, X. Chen, X. Cheng, M. Donini, J. Gelman, S. Gollaprolu,
J. He, P. Larroy, X. Liu, N. McCarthy et al., “Amazon sagemaker
clarify: Machine learning bias detection and explainability in the
cloud,” arXiv preprint arXiv:2109.03285, 2021.

[232] “Amazon SageMaker Clarify,” https://sagemaker-examples.

[233]

readthedocs.io/en/latest/sagemaker processing/fairness and
explainability/fairness and explainability.html, 2022, retrieved
on May 20, 2022.
´A. A. Cabrera, W. Epperson, F. Hohman, M. Kahng, J. Morgen-
stern, and D. H. Chau, “Fairvis: Visual analytics for discovering
intersectional bias in machine learning,” in Proceedings of the 2019
IEEE Conference on Visual Analytics Science and Technology, VAST
2019, 2019, pp. 46–56.

[234] “FairVis,” https://github.com/poloclub/FairVis, 2019, retrieved

on May 20, 2022.

[235] “BiasAmp→,”

https://github.com/princetonvisualai/

directional-bias-amp, 2022, retrieved on July 31, 2022.

[236] “FairRepair,”

https://github.com/fairrepair/fair-repair,

retrieved on May 20, 2022.

[237] “SBFT,” https://github.com/search-based-fairness-testing/sbft,

2022, retrieved on May 20, 2022.

[238] “ADF,” https://github.com/pxzhang94/ADF, retrieved on May

20, 2022.
[239] “EIDIG,”

https://github.com/LingfengZhang98/EIDIG,

retrieved on May 20, 2022.

[240] “NeuronFair,”

https://github.com/haibinzheng/NeuronFair,

retrieved on May 20, 2022.

[241] “DeepInspect,”

https://github.com/ARiSE-Lab/DeepInspect,

retrieved on May 20, 2022.

[242] “CMA,”

https://github.com/sebastianGehrmann/

CausalMediationAnalysis, retrieved on May 20, 2022.

[243] “FairNeuron,” https://github.com/Antimony5292/FairNeuron,

retrieved on May 20, 2022.

[244] “ASTRAEA,” https://github.com/sakshiudeshi/Astraea,

re-

trieved on May 20, 2022.

[245] “MT-NLP,”

https://github.com/pckennethma/MT-NLP,

retrieved on May 20, 2022.

[246] “BiasFinder,”

https://github.com/soarsmu/BiasFinder,

retrieved on May 20, 2022.

[247] “REVISE,”

https://github.com/princetonvisualai/revise-tool,

2022, retrieved on May 20, 2022.

[248] “FINS,” https://github.com/kcachel/ﬁns, 2022, retrieved on

July 31, 2022.

27

[249] M. S. A. Lee and J. Singh, “The landscape and gaps in open source
fairness toolkits,” in Proceedings of the CHI Conference on Human
Factors in Computing Systems, CHI 2021, 2021, pp. 699:1–699:13.

[250] D. Pedreschi, S. Ruggieri, and F. Turini, “Discrimination-aware
data mining,” in Proceedings of the 14th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, KDD
2008, 2008, pp. 560–568.

[251] M. Baranyi, M. Nagy, and R. Molontay, “Interpretable deep
learning for university dropout prediction,” in Proceedings of the
21st Annual Conference on Information Technology Education, SIGITE
2020, 2020, pp. 13–19.

[252] P. Awasthi, A. Beutel, M. Kleindessner, J. Morgenstern, and
X. Wang, “Evaluating fairness of machine learning models un-
der uncertain and incomplete information,” in Proceedings of the
2021 ACM Conference on Fairness, Accountability, and Transparency,
FAccT 2021, 2021, pp. 206–214.

[253] P. Voigt and A. Von dem Bussche, “The EU general data protec-
tion regulation (GDPR),” A Practical Guide, 1st Ed., Cham: Springer
International Publishing, vol. 10, no. 3152676, pp. 10–5555, 2017.

[254] E. Goldman, “An introduction to the california consumer privacy

act (ccpa),” Santa Clara Univ. Legal Studies Research Paper, 2020.

[255] Z. Chen, X. Lu, W. Ai, H. Li, Q. Mei, and X. Liu, “Through
a gender lens: Learning usage patterns of emojis from large-
scale android users,” in Proceedings of the 2018 World Wide Web
Conference on World Wide Web, WWW 2018, 2018, pp. 763–772.

[256] M. Khari and P. Kumar, “An extensive evaluation of search-based
software testing: a review,” Soft Computing, vol. 23, no. 6, pp.
1933–1946, 2019.

[257] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Ka-
gal, “Explaining explanations: An overview of interpretability of
machine learning,” in Proceedings of the IEEE 5th International
Conference on data science and advanced analytics, DSAA 2018.
IEEE, 2018, pp. 80–89.

[258] B. D. Mittelstadt, C. Russell, and S. Wachter, “Explaining ex-
planations in AI,” in Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT 2019, 2019, pp. 279–288.
[259] J. Zhang, X. Wang, D. Hao, B. Xie, L. Zhang, and H. Mei, “A
survey on bug-report analysis,” Science China Information Sciences,
vol. 58, no. 2, pp. 1–24, 2015.

[260] G. Jeong, S. Kim, and T. Zimmermann, “Improving bug triage
with bug tossing graphs,” in Proceedings of the 7th joint meeting of
the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering, ESEC/FSE
2009, 2009, pp. 111–120.

[261] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of exist-
ing faults to enable controlled testing studies for java programs,”
in Proceedings of the International Symposium on Software Testing
and Analysis, ISSTA 2014, 2014, pp. 437–440.

