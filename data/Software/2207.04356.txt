JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

A Comparative Study of Self-supervised Speech
Representation Based Voice Conversion

Wen-Chin Huang, Member, IEEE, Shu-Wen Yang, Tomoki Hayashi, and Tomoki Toda

2
2
0
2

l
u
J

0
1

]

D
S
.
s
c
[

1
v
6
5
3
4
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—We present a large-scale comparative study of self-
supervised speech representation (S3R)-based voice conversion
(VC). In the context of recognition-synthesis VC, S3Rs are
attractive owing to their potential to replace expensive supervised
representations such as phonetic posteriorgrams (PPGs), which
are commonly adopted by state-of-the-art VC systems. Using
S3PRL-VC, an open-source VC software we previously devel-
oped, we provide a series of in-depth objective and subjective
analyses under three VC settings: intra-/cross-lingual any-to-one
(A2O) and any-to-any (A2A) VC, using the voice conversion
challenge 2020 (VCC2020) dataset. We investigated S3R-based
VC in various aspects, including model type, multilinguality, and
supervision. We also studied the effect of a post-discretization
process with k-means clustering and showed how it improves in
the A2A setting. Finally, the comparison with state-of-the-art VC
systems demonstrates the competitiveness of S3R-based VC and
also sheds light on the possible improving directions.

Index Terms—voice conversion, self-supervised learning, self-

supervised speech representation

I. INTRODUCTION

V OICE conversion (VC) refers to a technique that converts

one type of speech to another while preserving the
underlying spoken contents [1], [2]. VC has a wide variety
of applications, including accent conversion [3], personalized
speech synthesis [4], [5], and speaking-aid device support [6]–
[8]. In this work, we focus on the most widely investigated
application of VC: speaker conversion, which refers to con-
verting speech from a source speaker to a target speaker [9].
A widely-studied approach to VC aims at constructing a
black-box function that directly maps source features into
those of the target, as depicted in the top of Figure 1. Early
studies employed statistical models such as Gaussian mixture
models (GMMs) to represent such a function [10], [11]. To
train the model, an alignment process with dynamic time
warping must be performed beforehand [12], which requires
access to a parallel training set containing utterances of the
same linguistic contents from both source and target. To avoid
the costly parallel data collection process, CycleGAN-based
VC [13] was proposed to ﬁnd the mapping function without
explicit alignment using adversarial learning.

In recent years, a different strategy that has been gaining
attention is to decompose the conversion function by disentan-
gling the spoken contents from the others factors in speech,
as depicted in the bottom of Figure 1. This is a reﬂection
of the deﬁnition of VC: from the information perspective,

Fig. 1: Top: black-box based voice conversion; bottom: decom-
position by content disentanglement based voice conversion.

VC can be performed by ﬁrst extracting the spoken contents
from the source speech, and then synthesizing the converted
speech from the extracted contents with the characteristics
of the target. Formally, starting from the source speech X,
a recognizer (or encoder) ﬁrst extracts the spoken contents,
H, which is then consumed by the synthesizer (or decoder) to
generate the converted speech, Y:

Y = Synth(H), H = Recog(X).

(1)

Methods that implement this paradigm can be categorized
based on how the two modules are optimized. For instance, a
line of work tries to optimize the recognizer and synthesizer
simultaneously by using an autoencoding objective. In this
framework, the ability of the encoder to extract linguistic
contents are ensured by employing various information bot-
tleneck, including variational autoencoder [14]–[16], vector
quantization [17], [18] and instance normalization [19].

In contrast, many have proposed to optimize the two mod-
ules separately, and such an approach is often referred to
as recognition-synthesis (rec-syn) based VC1. For instance,
in the latest voice conversion challenge 2020 (VCC2020)
[21], a baseline system and several top performing systems
implemented such a framework [22]–[26]. It was shown in the
challenge results that systems based on rec-syc VC were supe-
rior to autoencoder-based methods that trains the two modules
concurrently in terms of both naturalness and similarity. Since
these systems employed automatic speech recognition (ASR)
the
models as the recognizer module,

is believed that

it

W.-C. Huang is with the Graduate School of Informatics, Nagoya Univer-

sity, Japan. E-mail: wen.chinhuang@g.sp.m.is.nagoya-u.ac.jp
S.-W. Yang is with National Taiwan University, Taiwan.
T. Tomoki and T. Toda are with Nagoya University, Japan.

1To our konwledge, the term recognition-synthesis was ﬁrst deﬁned in [20],
which was only referred to VC systems composed by an ASR model and a
speaker-dependent synthesizer. In this work, we deﬁne it to be any VC system
that separately trains the recognizer and synthesizer.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

text data and the ASR objective function form a stronger
information bottleneck for preserving the linguistic contents
than the constraints used in the autoencoding framework.

One disadvantage of using ASR models as the recognizer
module is the expensive dataset collection process. In low-
resource settings such as the cross-lingual VC scenario [21],
labeled datasets can be especially hard to collect. Therefore,
researchers have resorted to unsupervised or the so-called self-
supervised speech representation (S3R) learning paradigm,
where a large-scale unlabeled data is used to learn rich,
compact speech representations.

In addition to its label-free property, S3R based VC is also
attractive in it being a good probing task for S3R analysis.
Based on the information perspective of VC presented above,
we may hypothesize that a good representation H in Eq. 1
should be rich in content but contains little to none speaker
information. As a result, an S3R model
that can extract
all-purpose speech representations may not be an optimal
choice for VC. For instance, a well-known S3R, wav2vec
2.0 [27], has been shown to be powerful in not only ASR
but also speaker and language recognition [28], implying that
it encodes rich content, speaker and language information.
Under our hypothesis, it may not be the best representation
for VC. Such analyses may help researchers reach a better
understanding of different S3R models.

In this paper, we present a comperative study of S3R-based
VC. Our experiments were conducted using S3PRL-VC [29],
an open-source VC software2 we previously developed that
extended the SUPERB benchmark and the S3PRL toolkit [30].
We conducted a large-scale evaluation, both objectively and
subjectively, to analyze S3R-based VC systems from various
aspects, including:

• Task: Experiments were conducted under three kinds
of settings:
intra-/cross-lingual any-to-one (A2O) VC,
where the system converts from an unseen speaker to
a seen speaker of the same/different language, and intra-
lingual any-to-any (A2A) VC, where both the source and
target speakers are unknown during training. We used the
VCC2020 dataset to unify the dataset condition, and to
provide comparison with top systems in the challenge.
• Model type: We implemented models used in the top
systems in VCC2018 [31] and VCC2020 [24], which al-
lows us to compare with the top systems in the respective
years.

• Multilinguality: We validatethe cross-lingual

transfer
ability of S3Rs using the cross-lingual VC task. Fur-
thermore, using the wav2vec 2.0 model, we compared
the performance when trained on a mono-lingual and a
multi-lingual dataset.

• Supervision: We provided results of supervised represen-
tations based systems using the same tasks and models, so
we can understand the impact of supervision in recognizer
training.

• Discretization: Although continuous features were used
as default in the SUPERB benchmark, our initial inves-

2https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/

a2o-vc-vcc2020

Fig. 2: The training and conversion procedures of recognition-
synthesis based VC.

tigation showed that they do not provide the sufﬁcient
disentanglement needed in the A2A setting. We then
investigated a k-means based discretization used in [32],
and provided a comprehensive ablation study.

This work aims to contribute to not only the VC ﬁeld but
also the S3R ﬁeld. The contributions to the respective ﬁelds
are summarized as follows:

• VC: We aim at a uniﬁed, comprehensive study of S3R-
based VC. Although getting increasingly popular in the
VC ﬁeld in recent years [32]–[36], each paper used their
own experimental setting, including different datasets,
models and evaluation protocol. As a result, it is difﬁ-
cult to compare different techniques to further identify
drawbacks of current methods. Through this work, we
hope to shed lights on a holistic understanding of the
S3R-based VC framework, and provide a stepping stone
for future VC researchers.

• S3R: We ﬁnd VC suitable for investigating the dis-
entanglement ability of S3R models. Most downstream
tasks test one ability of the S3R model at a time, either
the capability to encode rich and compact local content
information (speech recognition, keyword spotting, etc.)
or the power to represent global characteristics (speaker
veriﬁcation, emotion recognition, etc.) As stated above,
these two abilities at once.
we suspect VC can test
Moreover, although we focus on speaker conversion in
this work, by changing a task setting,
is possible
to inspect the ability of the S3R model to disentangle
different global attributes, such as accent or speaking
style.

it

II. BACKGROUND AND RELATED WORKS

A. Recognition-synthesis based voice conversion

Figure 2 illustrates the training and conversion processes in
rec-syn based VC. The recognizer is ﬁrst trained on a multi-
speaker dataset, which can be either labeled or unlabeled.
A common practice is to perform training in a speaker-
independent fashion, which ensures the model’s ability to

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

encode the speech representation, H, from any unseen speaker.
Using the VC training dataset, DVC, the synthesizer is trained
to reconstruct the acoustic features from H. Depending on
the setting, the VC training dataset can be either a small
target speaker dataset or a multi-speaker dataset, which we
will describe later. In the conversion phase, the converted
features, Y, are generated following Eq. 1. The recognizer
takes the source speech as input and extracts the S3Rs, which
is consumed by the synthesizer to generate the converted
acoustic features. Finally, a waveform synthesizer (ex. neural
vocoder) generates the converted waveform.

In the literature, many types of intermediate representations
have been used as H, all of which have their own respective
pros and cons. Table I presents a comparison of the features
based on various aspects. In the following we introduce three
widely-used categories.

1) Text: Text is a straight-forward choice, as one can simply
concatenate a pretrained ASR and text-to-speech (TTS) model.
In VCC2020, one of the baseline systems called ASR+TTS
[22] and the top system of the intra-lingual task [23] both
adopted text as the intermediate representation, and achieved
outstanding performance in terms of similarity. This is mainly
owing to the discrete and token-level nature of text. Since
prosodic information including the speaking rate and the pitch
pattern are discarded after recognition, the synthesizer needs to
use a powerful model like sequence-to-sequence (seq2seq) to
reconstruct the target characteristics. However, this approach
suffers from mispronunciation when the accuracy of the ASR
and TTS model is insufﬁcient, as shown in [22]. There are also
VC scenarios where the source style needs to be preserved,
such as singing VC [37].

2) Phonetic posteriorgrams or bottleneck features: Pho-
netic posteriorgrams (PPGs) were ﬁrst applied to VC in [38].
PPGs represent the frame-wise posterior probabilities of each
phonetic class, which are derived from the acoustic model
(AM) of an HMM based ASR model. The training target of
the AM are phoneme labels, so only the output of the last layer
of the AM has the physical meaning of PPG, but some have
proposed to use the ouptut from other layers. For example,
the system in [39] used the output before the softmax layer
and referred to them as bottleneck features (BNFs). Either
PPGs or BNFs are frame-level continuous features, thus better
perserve the linguistic contents and can help produce high-
quality speech. The top system in VCC2018 [31] and the top
system in VCC2020 task 2 [24] both adopted this feature.
However,
the frame-level nature makes the conversion of
speaking rate difﬁcult. Efforts needed for the frame-level labels
of the ASR dataset also raised the difﬁculty of constructing
the system.

3) Self-supervised speech representations: To reduce the
labeling cost of training ASR models, applying S3Rs to VC
has become increasing popular. Being free from labeled data
not only reduces the labeling cost, but also makes it possible
to use more unlabeled datasets and work under low-resource
settings. S3Rs have been applied to a wide verity of VC
settings, including any-to-one VC [33], many-to-many VC
[32], any-to-any VC [34], [35] and cross-lingual VC [36].

The typical usage of S3R models is to extract continuous

TABLE I: A comparison of intermediate representations in
recognition-synthesis based voice conversion.

Representation

Text

Extractor
Training data
Resolution
Continuous?
Examples

Phonetic
Posteriorgram

ASR model
labeled data

Self-supervised
speech representations
self-supervised model
unlabeled data

token level
discrete
[22], [23]

frame level

continuous
[24]–[26]

can be either
[32]–[36], [42]

features for downstream tasks. However, due to the lack of
supervision, continuous S3Rs lack the ability to fully separate
contents from other factors such as speaker identity, resulting
in poor performance in the A2A setting [29]. One way to
provide the sufﬁcient disentanglement is through discretiza-
tion, as shown in [40]. Certain S3R models such as VQVAE
[17] or vq-wav2vec [41] are able to generate discrete outputs
due to their architecture, thus some have therefore proposed
VC systems based on them [33], [42]. However, not all
S3R models have such discretization design. Recently, [32]
proposed to apply a k-means based post-discretization process
on the continuous S3Rs. The learned discrete units were shown
to be effective in a many-to-many VC setting.

B. Self-supervised speech representation learning

In recent years, self-supervised learning has been the state-
of-the-art approach in various research ﬁelds. It implies a
principle that ﬁrst pretrains an upstream model that learns
general knowledge by solving self-supervised tasks on a large
amount of unlabeled data, followed by ﬁne-tuning prediction
layers on various downstream tasks3. When applied to speech,
S3Rs are expected to capture linguistic, speaker, prosodic, and
semantic information of speech. In the literature, though with
different network architectures, S3Rs are commonly grouped
by their objective functions. Generative modeling incorporates
language model-like training losses to predict unseen regions
(such as future or masked frames), in order to maximize the
likelihood of the observed data. Examples include APC [43],
VQ-APC [44], Mockingjay [45], TERA [46], and NPC [47].
Discriminative modeling aims to discriminate (or contrast)
the target unseen frame with randomly sampled ones, which
is equivalent to mutual information maximization. Examples
include CPC [48], [49], wav2vec [50], vq-wav2vec [41],
wav2vec 2.0 [27] and HuBERT [51]. Finally, multi-task learn-
ing applies multiple objectives, including waveform gener-
ation, prosody features regression and contrastive InfoMax.
PASE+ [52] is the most representative approach.

III. TASKS DESIGN

A. General description of VCC2020

All experiments in this work are benchmarked on the
VCC2020 dataset [21]. There are two tasks in VCC2020, with
intra-lingual VC being task 1 and cross-lingual VC being task

3In the context of S3R-based VC, the recognizer is represented and the
synthesizer are represented by the S3R upstream model and the downstream
prediction layers, respectively. In the remainder of this paper, we will use
these two terms interchangeably.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

TABLE II: Summary of the data conditions in VCC2020.

Task

Task 1

Task 2

Training phase

Conversion phase

Source

Target

Source

Converted

70 Eng.
utterances

70 Eng.
utterances

70 Man./Ger./Fin.
utterances

25 Eng.
utterances

25 Eng.
utterances

2. The data conditions are summarized in Table II. The two
tasks share the same two English male and female source
speakers. The target speakers include two male and two female
English speakers for task 1, and one male and one female
speaker each of Finnish, German, and Mandarin for task 2.
For each speaker, 70 utterances (roughly ﬁve minutes) in their
respective languages and contents are provided, and there are
25 test sentences for evaluation. During conversion, the source
speech (which is in English) is converted as if it was uttered
by the target speaker while keeping the linguistic contents
unchanged.

B. Intra-lingual and cross-lingual any-to-one VC

We ﬁrst consider the two tasks in VCC2020 under the
A2O setting. A2O VC aims to convert from any arbitrary
speech into that of a predeﬁned target speaker. As mentioned
in II-A, the ability to encode H from any unseen speaker
is ensured by the common practice of training S3Rs on a
multi-speaker dataset. In the A2O setting, the VC training
dataset in Figure 2 is the target speaker dataset, Dtrg. The
synthesizer is trained to reconstruct the acoustic feature from
H. As described in Secion III-A, the language of Dtrg is
English and Finnish/German/Mandarin in the intra-lingual and
cross-lingual setting, respectively.

A2O VC is a good probing task to investigate several
characteristics of an upstream S3R model. A fundamental
requirement of VC is the linguistic consistency, so there is
a positive correlation between the VC performance of an S3R
model and its ability to faithfully encode H. Also, during the
synthesizer training in cross-lingual VC, the S3R model may
fail to generalize to X from a non-English target speaker since
most existing S3R models are trained with English datasets
only. It is worthwhile to examine the ability of mono-lingual
S3R models to transfer to different languages.

C. Intra-lingual any-to-any VC

We then extend the VCC2020 dataset for an A2A scenario,
also known as zero-shot VC. A2A VC attempts to convert to a
target speaker where Dtrg is so limited (less than one minute)
such that ﬁne-tuning in infeasible. In this setting, the DVC used
to train the A2A VC model is a separate multi-speaker dataset.
As in the A2O setting, the synthesizer is trained to reconstruct
the acoustic feature from H. However, due to the speaker-
independent nature of S3Rs, H does not provide sufﬁcient
speaker information to recover the speaker information. Thus,
the input is augmented with a speaker embedding, s, extracted
by an off-the-shelf speaker encoder, which is pretrained on an

Fig. 3: The models implemented in this work. Left: the simple
model. Middle: the simple model with an AR loop. Right: the
Tacotron2 model, with extension to an any-to-any model by
accepting a d-vector as the speaker embedding.

automatic speaker veriﬁcation (ASV) dataset and objective.
In training, the speaker embedding extracted from the target
waveform is used. During conversion, given Dtrg, s is formed
as an average of each embedding from each utterance. We
may then rewrite Eq. 1 as:

(2)

Y = Synth(H, s), H = Recog(X), s = SpkEnc(Dtrg).
A2A VC helps us investigate how complete can an S3R
model ﬁlter out the speaker information, which is an important
ability in rec-syn based VC. We explain why the A2O setting
cannot explore this ability well. Imagine the scenario where
a lot of speaker information remains in the S3R. Since the
training target is always the target speaker dataset, it is possible
that the model removes the speaker information ﬁrst then
inject back to the output. However, in the A2A VC scenario,
the training target is drawn randomly from the multi-speaker
dataset, thus a “speaker-free” S3R is more demanding. That
is to say, during conversion, if an S3R model encodes rich
speaker information, then the source speaker information in
X will conﬂict with the target speaker attributes injected by
the synthesizer, which hurts the VC performance.

IV. IMPLEMENTATIONS

A. Recognizers (upstream models)

Table III depicts the list of S3Rs we compared in this
work, which are the upstream models supported in S3PRL
at the date of publication. For a complete list of information
(training data, architecture, objective, etc.), refer to [30]. All
upstreams are trained with English data, mostly LibriSpeech
[53] or LibriLight [54]. In addition to the S3Rs, two extra
upstreams were included: (1) mel-spectrogram, “mel”, and (2)
“PPG (TIMIT)”, which is trained supervisedly on the TIMIT
dataset [55].

B. Synthesizer model implementation

Log mel fbanks was selected as the target acoustic feature.
We implemented several models to resemble top systems of
past VCCs, as illustrated in Figure 3. We avoid expensive
model components like attention [56] because (1) fast bench-
marking is a key requirement of SUPERB/S3PRL, and (2)
the frame-level feature used in this framework frees us from

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

the following two additional techniques shown in the right of
Figure 4. Both methods try to describe one feature vector with
multiple k-means models (i.e. multiple indices) to increase the
degree of freedom. In the experimental section, we provide a
complete investigation of these two techniques.

1) Cluster ensemble: Using an ensemble of k-means mod-
els with different codebook sizes can capture different granu-
larity, and each k-means model can provide complementary
information to back each other up. Speciﬁcally, given a
continuous feature vector hi, we use NCE k-means models to
generate NCE indices: [z1i, z2i, . . . , zNCEi], where the codebook
of n-th model has size Kn clusters. Each Kn should be set to
different numbers, so that different k-means models can learn
to capture different levels of detail.

2) Product quantization: Product quantization (PQ) is a
technique where the feature space is partitioned into multiple
subspaces, and each subspace is quantized separately using
different k-means models. Speciﬁcally, given a continuous
feature vector hi ∈ Rd, we ﬁrst partition it into NPQ sub-
vectors: [h1i, h2i, . . . , hNPQi] where each subvector has size
hni ∈ Rd/NPQ. Then, each subvector is consumed by a separate
k-means model to generate NPQ indices: [z1i, z2i, . . . , zNPQi].
The k-means models can be of different numbers of clusters
as done in cluster ensemble, but for simplicity, here we set all
k-means models to have equal number of clusters.

D. Other implementation setups

1) Any-to-any VC settings: The dataset used to train the
A2A VC model is the VCTK dataset [61]. For the speaker
encoder, we used the d-vector model [62] trained on a mix
of datasets, including LibriSpeech, VoxCeleb 1 [63] and 2
[64]. For the post-discretization process, following [32], all k-
means models are trained on the LibriSpeech clean-100h set
[53]. Although some studies use intermediate layer outputs for
discretization [32], [65], for simplicity, we use the last outputs
for all S3R models.

2) Waveform synthesizer: We used the HiFi-GAN [66], a
state-of-the-art parallel real-time neural vocoder. For the A2O
setup, we mixed the data of all 14 speakers in VCC2020 with
the VCTK dataset, while for the A2A setup we used only the
VCTK dataset.

V. EXPERIMENTAL EVALUATION RESULTS

In this section, we ﬁrst describe the evaluation metrics
(Section V-A). Then we provide a series of complete ob-
jective evaluations and a large-scale listening test to analyze
continuous feature-based S3R-based VC and to compare with
state-of-the-art systems (Section V-D). The aspects we in-
vestigate include the synthesizer model type (Section V-B),
multilinguality (Section V-C) and supervision (Section V-E).
We ﬁnally examine the effectiveness of the post-discretization
process (Sections V-G and V-H).

A. Evaluation metrics and protocols

1) Objective evaluation: We employed the following three
objective evaluation metrics, all of which measure different

the post-discretization process overview. Top
Fig. 4: Left:
right: the cluster ensemble technique with 3 k-means models
using different numbers of clusters. Bottom right: the product
quantization techniques with 4 partitions.

changing the temporal structure. For discrete inputs generated
by the methods described in Section IV-C, they are embedded
using lookup tables ﬁrst.

• Simple: We start from the model used by the top system
in VCC2018 [31]. The simple model consists of a single
layer feed-forward network (FFN), two long short-term
memory layers with projection (LSTMP), and a linear
projection layer.

• Simple-AR: As autoregressive (AR) modeling has been
shown to be effective in speech synthesis [57], we added
an AR loop to the simple model. At each time step, the
previous output is consumed by the ﬁrst LSTMP layer.
Dropout is essential in the AR loop to avoid exposure
bias brought by teacher-forcing [58], [59].

• Taco2-AR: We increase the model complexity by using
a model architecture similar to that of Tacotron 2 [60],
which resembles the model used by the top system in
VCC2020 [24]. Different from Tacotron 2, the attention
module was not used as it was reported to be useless in
[24].

C. Post-discretization process for any-to-any VC

In our initial investigations [29], using continuous features
cannot satisfy the disentanglement requirement in the A2A
scenario. As a result, most S3Rs fail to convert the speaker
identity, as we show in later sections. We thus provide an
extension to match the tendency in the A2A setting to that in
A2O.

We impose a stronger information bottleneck by adopting
the post-discretization process proposed in [32]. Speciﬁcally,
as illustrated in the left of Figure 4, the k-means clustering
algorithm takes the continous features H returned by the
recognizer, and returns corresponding discrete indices Z using
a codebook of size K trained with a separate dataset
in
advance.

However, in our preliminary experiments, the method pro-
posed in [32] performs poorly when applied to certain S3Rs.
The generated speech often suffers from poor the intelligibility,
even when using a large codebook. We suspect that the infor-
mation bottleneck introduced by discretization is too strong.
To offer more expressive power, inspired by [51], we employ

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

TABLE III: Objective evaluation results on different VC settings over various S3Rs using continuous features. For MCD and
WER, the smaller the better; for ASV, the higher the better.

Upstream

mel
PPG (TIMIT)
PASE+ [52]
APC [43]
VQ-APC [44]
NPC [43]
Mockingjay [45]
TERA [46]
Modiﬁed CPC [49]
DeCoAR 2.0 [67]
wav2vec [50]
vq-wav2vec [41]
wav2vec 2.0 Base [27]
wav2vec 2.0 Large
HuBERT Base [51]
HuBERT Large

Simple
MCD WER
48.5
69.0
5.0
8.6
10.8
39.0
31.3
11.4
9.4
7.4
14.0
13.4
24.7
12.5
5.5
5.6

8.41
7.78
9.29
8.67
8.12
7.74
8.58
8.60
8.71
8.31
7.45
7.41
7.80
7.64
7.70
7.54

Intra-lingual A2O
Simple-AR

Taco2-AR

Cross-lingual A2O
Taco2-AR

Intra-lingual A2A
Taco2-AR

ASV MCD WER
22.7
8.92
59.00
58.9
7.83
85.50
5.7
9.52
26.75
7.1
8.73
48.00
7.4
8.37
81.25
21.1
8.15
92.75
9.5
8.74
51.00
6.0
8.67
46.50
7.0
8.87
40.00
6.4
8.33
54.75
95.50
4.9
7.64
7.24
11.6
91.00
5.0
7.77
92.75
9.0
7.67
81.75
4.7
7.79
89.25
5.6
7.54
95.00

ASV MCD WER
38.3
8.47
49.75
33.6
7.18
95.25
30.6
8.66
26.00
27.2
8.05
41.75
22.4
7.84
60.50
30.4
7.86
76.75
35.1
8.29
47.00
25.1
8.21
42.50
26.2
8.41
30.00
17.1
7.83
53.00
10.1
7.45
90.50
7.08
98.75
13.4
10.5
7.50
86.50
15.8
7.63
82.75
8.0
7.47
84.25
9.0
7.22
93.00

ASV WER
39.0
77.25
51.0
99.75
36.3
63.20
33.9
87.25
28.4
94.25
37.6
94.75
39.2
79.75
29.2
83.75
35.3
71.00
26.8
90.75
13.9
98.25
100.00
21.0
14.9
98.00
22.7
97.25
13.5
98.50
15.9
99.25

ASV MCD WER
4.2
9.49
46.67
8.31
12.9
84.67
4.2
9.85
34.67
3.5
9.57
52.33
4.0
9.43
68.00
4.4
9.39
59.00
5.0
9.43
46.00
5.2
9.31
49.33
4.1
9.61
32.83
4.0
9.28
59.33
3.5
8.77
75.83
88.83
4.2
8.47
3.2
9.03
82.17
4.1
8.99
78.00
3.4
9.19
82.33
3.0
9.13
86.50

ASV
19.50
83.50
8.00
23.25
22.00
21.00
25.00
18.75
10.75
27.00
40.00
73.25
27.00
22.25
23.25
27.75

aspects of a VC system. For the cross-lingual A2O task, we
did not report the MCD results.

• MCD: The mel cepstrum distortion (MCD) is an intru-
sive, L2-norm based metric based on the mel cepstrum
coefﬁcient (mcep) which measures the general perfor-
mance:

as the upper bound. We used an open-source toolkit [70] that
implemented the ITU-T Recommendation P.808 [71] to screen
unreliable ratings obtained through the Amazon Mechanical
Turk (Mturk). We recruited more than 280 listeners from the
United States and had each sample rated by ﬁve different
participants on average. Audio samples are available online5.

MCD[dB] =

(cid:118)
(cid:117)
(cid:117)
(cid:116)2

10
log 10

K
(cid:88)

(mcep(c)

d − mcep(t)

d )2,

d=1

where K is the dimension of the mceps and mcep(c)
and
d
mcep(t)
represent the d-th dimensional coefﬁcient of the
d
converted mceps and the target mceps, respectively. The
WORLD vocoder [68] was used to extract the mceps.
• WER: The word error rate (WER) is a non-intrusive mea-
sure of the intelligibility and the linguistic consistency of
the converted speech. We used a pretrained wav2vec 2.0
model4.

• ASV: The accept rate from a pretrained ASV model
measures whether the speaker identity is converted by
calculating the cosine similarity using speaker embed-
dings [69]. Speciﬁcally, the cosine similarity of the d-
vectors extracted from each converted utterance and the
corresponding reference are calculated. We then report
the percentage of the testing utterances whose cosine
similarity exceeds a pre-calculated threshold.

2) Subjective evaluation: For the subjective test, we asked
listening participants to evaluate two common aspects in VC:
naturalness and similarity. Listeners were asked to evaluate the
naturalness on a ﬁve-point scale. For conversion similarity, a
natural target speech and a converted speech were presented,
and listeners were asked to judge whether the two samples
were produced by the same speaker on a four-point scale.
For each system, a total of 80 utterances (5 random × 16
conversion pairs) were evaluated. Recordings of the target
speakers were also included in the naturalness test and served

(3)

B. Comparison of different synthesizer model types

We ﬁrst investigate the impact of using different synthesizer
models described in Section IV-B in the intra-lingual A2O
setting, as shown in Table III. First, only by adding the AR
loop to the Simple model, most S3Rs beneﬁt from large
improvements in WER. With Taco2-AR, all S3Rs except
PASE+ and modiﬁed CPC achieved an ASV accept rate higher
80%, while all S3Rs suffered from a degradation in WER.
This shows that increasing the model capacity can signiﬁ-
cantly improve the speaker similarity, while sacriﬁcing the
intelligibility. However, we would like to emphasize that: (1)
the WER is a strict measurement of intelligibility, and human
can actually recognize better than machine. After listening
to the samples, our internal percepion was that compared to
simple-AR, the quality was greatly improved and intelligibility
degradation was not as serious as shown in the table. (2) the
Taco2-AR model yields the best MCD scores, which, as we
will show later, correlates better with subjective naturalness
and similarity. (3) we empirically found the training time of
the three models similar. Based on these reasons, we decided
to use the Taco2-AR model for the succeeding tasks and
comparisons.

C. Investigation on model multilinguality

Next, we assess the VC performance of S3R models in the
cross-lingual setting. Looking again at Table III, we ﬁrst ﬁnd
S3Rs trained on a mono-lingual corpus can still work well in
the cross-lingual setting, demonstrating their ability to transfer
across languages. However, compared with the intra-lingual

4Performance and APIs can be found at https://huggingface.co/facebook/

wav2vec2-large-960h-lv60-self

5https://unilight.github.io/Publication-Demos/publications/s3prl-vc/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

TABLE IV: Comparison of wav2vec 2.0 trained on mono-
lingual data and cross-lingual data in the cross-lingual A2O
scenario, using the Taco2-AR model. The results of wav2vec
2.0 Large are extracted from Table III.

Upstream
wav2vec 2.0 Large
XLSR [72]

Training data size
LibriLight 60k hr
56k hr from 53 languages

WER
22.7
24.2

ASV
78.00
72.50

A2O task, it could be clearly observed that all S3Rs degraded
in terms of both the WER and ASV accept rate in the cross-
lingual setting. In VCC2020, it was also reported that cross-
lingual VC is indeed a harder task than intra-lingual VC, as
the listening test results of all participating teams were much
worse.

To further investigate the impact of the training data lan-
guage, in Table IV we report the results of XLSR [72], a model
that has the same architecture as wav2vec 2.0 Large but trained
on a mixture of datasets from 53 language, resulting in 56k
hours of data. We found that compared to wav2vec 2.0 Large
trained on mono-lingual data, XLSR was not particularly good.
We suspect that when the training set is large enough, the
model can already capture the variations among all languages
such that a multilingual dataset will not be needed. Also, since
the source language during conversion is English, it is possi-
ble that monolingual models are sufﬁcient. It is worthwhile
investigating this point by considering a different setting in
the future, such as converting from non-English languages.

TABLE V: Comparison with state-of-the-art systems. All
upstreams use the Taco2-AR model. Nat. and Sim. stand for
naturalness and similarity, respectively. Both Nat. and Sim.
are the higher the better. The objective results (MCD, WER,
ASV) are extracted from Table III.

System

MCD WER

ASV

Nat.

Sim.

mel
PPG (TIMIT)
PASE+
APC
VQ-APC
NPC
Mockingjay
TERA
Modiﬁed CPC
DeCoAR 2.0
wav2vec
vq-wav2vec
wav2vec 2.0 B.
wav2vec 2.0 L.
HuBERT B.
HuBERT L.
USTC-2018† [31]
USTC-2020 [23]
SRCB [25]
CASIA [26]
ASR+TTS [22]
Target

PPG (TIMIT)
vq-wav2vec
HuBERT L.
USTC-2018 [31]
USTC-2020 [24]
SRCB [25]
CASIA [26]
ASR+TTS [22]
Target

Intra-lingual A2O
77.25
99.75
63.20
87.25
94.25
94.75
79.75
83.75
71.00
90.75
98.25
100.00
98.00
97.25
98.50
99.25
99.00
100.00
92.00
98.25
100.00
–

38.3
33.6
30.6
27.2
22.4
30.4
35.1
25.1
26.2
17.1
10.1
13.4
10.5
15.8
8.0
9.0
6.5
5.4
11.5
11.0
8.2
0.7

Cross-lingual A2O
84.67
88.83
86.50
97.67
96.00
78.67
91.67
67.83
–

51.0
21.0
15.9
5.6
7.6
8.6
10.5
34.5
–

8.47
7.18
8.66
8.05
7.84
7.86
8.29
8.21
8.41
7.83
7.45
7.08
7.50
7.63
7.47
7.22
–
6.98
8.90
7.13
6.48
–

–
–
–
–
–
–
–
–
–

2.61 ± .11
3.32 ± .10
2.58 ± .12
2.92 ± .11
3.08 ± .10
2.98 ± .11
2.81 ± .12
2.91 ± .12
2.74 ± .11
3.04 ± .11
3.40 ± .05
3.59 ± .10
3.36 ± .06
3.26 ± .10
3.48 ± .10
3.47 ± .10
4.20 ± .08
4.41 ± .07
4.16 ± .08
4.25 ± .08
3.84 ± .09
4.57 ± 0.14

2.79 ± .08
3.28 ± .08
3.13 ± .08
4.17 ± .06
4.27 ± .07
4.34 ± .07
4.11 ± .07
2.51 ± .08
4.48 ± 0.12

35% ± 3%
58% ± 4%
31% ± 3%
43% ± 4%
40% ± 4%
46% ± 3%
42% ± 4%
37% ± 4%
33% ± 3%
43% ± 4%
52% ± 2%
59% ± 4%
51% ± 2%
50% ± 4%
55% ± 4%
54% ± 4%
55% ± 4%
82% ± 3%
68% ± 3%
61% ± 4%
75% ± 3%
–

43% ± 3%
44% ± 3%
41% ± 3%
34% ± 3%
43% ± 3%
34% ± 3%
45% ± 3%
39% ± 3%
–

D. Comparing with state-of-the-art systems using subjective
evaluation

We then compared S3R-based VC models with state-of-the-
art systems. USTC-2018 [31], USTC-2020 [23], [24]6, SRCB
[25], CASIA [26] were top systems in VCC2020, all of which
adopted PPGs, synthesizer pretraining on a multi-speaker
dataset, and AR vocoders. Notably, they used thousands of
hours of internal data for training. ASR+TTS [22] was the
seq2seq+non-AR vocoder baseline in VCC2020. S2VC [35]
is the STOA system for A2A VC. The results are shown in
Table V. We summarize our observations as follows:

• vq-wav2vec outperformed all other upstreams in the
subjective test, with a 3.59 naturalness and 59% similarity
in the intra-lingual A2O setting.

• In the A2O settings, there was still a naturalness gap
between vq-wav2vec and other VCC2020 top systems
(3.59 v.s. 4.16-4.25, 3.28 v.s. 4.11-4.34). As for similarity,
vq-wav2vec was on par with USTC-2018 and CASIA in
the intra-lingual A2O setting, and achieved top in the
cross-lingual setting.

• In the A2A setting, vq-wav2vec was on par with S2VC in
similarity, while being signiﬁcantly better in naturalness.
Our system is therefore the new state-of-the-art in S3R-
based A2A VC.

6USTC’s systems used text and PPG for the intra-lingual and cross-lingual

tasks, respectively.

12.7
PPG (TIMIT)
4.2
vq-wav2vec
S2VC† [35]
12.4
†: Systems generate 16kHz, so MCD is not calculable and direct score
comparison should be made with caution.

3.41 ± .08
3.58 ± .09
2.90 ± .09

8.32
8.47
–

Intra-lingual A2A
84.25
73.25
71.50

34% ± 4%
28% ± 3%
29% ± 3%

TABLE VI: Linear correlation coefﬁcients between different
metrics.

Metric
MCD
WER
ASV
Nat.
Sim.

MCD WER
0.678
–
–
–
–

–
–
–
–
–

ASV
-0.934
-0.640
–
–
–

Nat.
-0.968
-0.808
0.910
–
–

Sim.
-0.961
-0.587
0.911
0.932
–

E. Impact of supervision

Although top systems using PPG greatly outperformed vq-
wav2vec in naturalness,
they used AR vocoders and the
system was trained on large internal datasets, so the impact
of supervision is not yet clear. To this end, we compared vq-
wav2vec result with “PPG (TIMIT)” and the same vocoder.
From Table V, we ﬁrst ﬁnd “PPG (TIMIT)” has a high WER
and a low naturalness score, shoing that it was indeed of low
quality. Nonetheless, in all three settings, “PPG (TIMIT)” can
achieve similar or higher similarity scores than vq-wav2vec.
This shows that supervision greatly contributes to similarity,
especially in a difﬁcult setting like A2A VC. This also
shows that the ability of current S3Rs to disentangle speaker
information is still limited when compared to PPG, and can

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

TABLE VII: Results of HuBERT Base and Mockingjay using
the cluster ensemble and the product quantization techniques
in the any-to-any scenario, with the Taco2-AR model. The best
numbers within the same upstream are in bold face.

Upstream

HuBERT Base

Mockingjay

1

# clusters (Kn) NPQ
50
100
200
50+100
50+200
100+200
50+100+200
50
100
200
100
200
50+100+200
100
200

2

2

1

MCD WER
22.0
10.3
10.2
10.2
8.6
8.8
7.9
12.7
8.2
7.2
77.4
73.1
59.7
64.5
55.4

8.41
8.25
8.32
8.37
8.28
8.29
8.40
8.37
8.23
8.32
9.12
9.10
9.02
9.07
8.95

ASV
79.50
83.50
84.25
86.25
84.25
83.25
85.00
84.50
86.25
81.75
63.00
63.25
61.00
59.00
61.75

be further improved in the future. That being said, we can still
achieve good performance without supervision if the S3R was
designed properly.

F. Justify the objective metrics with correlation analysis

Conducting a subjective test whenever a new S3R is devel-
oped cannot meet the fast benchmark requirement of SUPERB.
Therefore, we examine if the objective measures align well
with human perception. Using the intra-lingual A2O results
over different upstreams, we calculated pairwise linear corre-
lation coefﬁcients. Results in Table VI suggested that MCD
best aligned with both naturalness and similarity. Note that
in this correlation analysis, we considered systems that used
the same synthesizer and neural vocoder. Since the correlation
result is strongly affected by the pool of methods evaluated in
a listening test, this good correlation could be observed only
in such a homogeneous condition. That is to say, as long as the
synthesizer and the vocoder are the same, we can safely use
the objective measures to compare different upstreams. This
implication is very useful for the benchmarking requirement
of SUPERB.

G. Investigation of the post-discretization process

In Table VII, we report results of applying cluster ensem-
ble and PQ on two upstreams, namely HuBERT Base and
Mockingjay, in the A2A setting. First, we can observe that the
intelligibility (WER) improves when the number of k-means
model in the ensemble increases. That is to say, using two
k-means models is better than using one, and using three is
even better. The intelligibility is also improved when using
PQ, and the improvement is consistent across all numbers of
clusters. However, using more k-means models in both cluster
ensemble and PQ means to loosen the speaker information
bottleneck, which can harm the conversion similarity (ASV)
as well as MCD. Finally, an interesting ﬁnding is that by
only partitioning into two feature subvectors, the MCD and
WER are still better than using an ensemble of three k-means
models, suggesting that PQ is a more effective method then

TABLE VIII: Results of HuBERT Base and Mockingjay vary-
ing the number of partitions (NPQ) in the product quantization
technique. The number of clusters is set to 200 in all k-means
models. The task is any-to-any VC, and the model is the
Taco2-AR model.

Upstream

HuBERT Base

Mockingjay

NPQ
1
2
4
8
16
32
64
128
256
1
2
4
8
16
32

MCD WER
10.2
7.2
5.8
3.5
4.1
3.6
3.8
3.9
4.2
73.1
55.4
37.8
20.2
12.8
8.6

8.32
8.32
8.39
8.35
8.31
8.41
8.45
8.38
8.37
9.10
8.95
9.09
9.14
9.25
9.37

ASV
84.25
81.75
84.00
84.50
78.00
75.00
75.25
74.00
74.75
63.25
61.75
52.50
39.25
34.75
29.75

Fig. 5: Visualizing the effect of number of partitions. Left:
HuBERT Base. Right: Mockingjay.

cluster ensemble. This is consistent with the ﬁnding in [51].
We thus use PQ in the following experiments.

Based on the observations in Table VII, we then investigate
how much speaker information are leaked when the number
of partitions increases. Table VIII shows the results varying
the number of partitions using HuBERT and Mockingjay,
trend. For
and Figure 5 is a visualization of the overall
HuBERT Base, we can ﬁrst observe a diminishing returns
effect in WER. That is to say, the WER stops to improve
when NPQ is large enough. We can also observe that the
conversion accuracy stays at a similar level when NPQ is small,
and starts to drop when NPQ gets larger. These observtions
show that we can ﬁnd an optimal NPQ such that the WER
is optimized while maintaining a similar level of conversion
accuracy. However, for Mockingjay, both WER and ASV are
monotonically decreasing, such that we cannot ﬁnd such an
optimal point by only looking at these two metrics. As a result,
we resolve to MCD to ﬁnd the optimal NPQ.

H. Comparison of continuous and discrete features

Finally, we compare the results in the A2A setting when
using continuous and discrete features. Since there are too
many hyperparemeters that can be tuned, we applied the PQ
technique and set the number of clusters to be 200, and we

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

TABLE IX: Results on any-to-any VC with continuous and
discrete features over various upstreams. The results using
continuous features are extracted from Table III.

Continuous

Discrete

Upstream

PASE+
APC
VQ-APC
NPC
Mockingjay
TERA
Modiﬁed CPC
DeCoAR 2.0
wav2vec
vq-wav2vec
wav2vec 2.0 B.
wav2vec 2.0 L.
HuBERT B.
HuBERT L.
†: Fails to be trained.

MCD WER
4.2
3.5
4.0
4.4
5.0
5.2
4.1
4.0
3.5
4.2
3.2
4.1
3.4
3.0

9.85
9.57
9.43
9.39
9.43
9.31
9.61
9.28
8.77
8.47
9.03
8.99
9.19
9.13

ASV MCD WER
81.7
8.92
8.00
22.4
8.66
23.25
21.0
8.42
22.00
46.0
8.78
21.00
55.4
8.95
25.00
37.1
8.40
18.75
13.8
8.69
10.75
–
–
27.00
15.2
8.34
40.00
73.25
22.5
8.49
54.3
8.90
27.00
67.7
8.97
22.25
4.1
8.31
23.25
8.23
7.4
27.75

ASV
74.00
81.25
85.50
74.50
61.75
67.00
75.50
–†
86.50
82.50
75.75
72.75
78.00
86.25

searched the best NPQ between 1, 2 and 4 with the lowest
MCD. We report the results in Table IX. It can be clearly
observed that the post-discretization process indeed serves as a
strong speaker information bottleneck as the ASV scores of all
S3Rs are signiﬁcantly higher than the continuous counterpart.
As described in Section IV-C, most S3Rs suffer from poor
intelligibility even with the PQ technique. However, certain
S3Rs still achieved an acceptable balance of intelligibility
and conversion similarity, resulted in MCD values lower than
that of the best performing continuous S3R (8.47 from vq-
wav2vec), such as VQ-APC, wav2vec, HuBERT Base and
HuBERT Large.

VI. DISCUSSION AND CONCLUSION

In the paper, we presented a comparative study of S3R-
based VC. We used S3PRL-VC, an extension of the S3PRL
toolkit that focused on the VC downstream task. We evaluated
the S3Rs under the context of VC, and provided a series of
in-depth analysis in various aspects including the synthesizer
model type, different VC tasks, supervision and discretization.
We also compared with the state-of-the-art VC systems in
VCC2020, and showed that there is still room for improvement
in terms of quality and similarity.

Readers from different research communities can gain indi-
vidual insights from this work. From the VC perspective, in
S3PRL-VC, to meet the fast benchmarking requirement, some
techniques that were shown to be effective were not applied,
such as ﬁne-tuning target speaker-dependent vocoders [31],
[73], training the synthesizer with waveform domain losses
[32], [74], or ﬁne-tuning the vocoder with ground truth aligned
synthesis [60], [66], [75]. That is to say, the performance can
be further optimized. In addition, applications to other VC
tasks such as emotional VC, expressive VC, singing VC and
VC for speaking aid devices are also worth investigating.

From the S3R perspective, we have shown that there are
certain challenges that are required by VC, such as the
preservation of the spoken contents and the disentanglement
of speaker information. It is therefore worthwhile to continue
to use VC as a probing task when designing new S3R models.

Finally, we would like to discuss the special position of VC
in the context of the recent SUPERB [76] activities. SUPERB
is a collection of benchmark resources that aims to evaluate
S3Rs across various speech tasks, with an assumption in
mind that different representations should outperform others in
different tasks due to their pretext-task nature. However, in the
original version that consisted of only 10 discriminative tasks,
it turned out that wav2vec 2.0 and HuBERT outperformed all
other S3Rs. This dominance was broken after the introduction
of VC, where vq-wav2vec was shown to be the best in the
A2O setting, due to its disentangling ability.

This ﬁnding has several important implications. First, it
shows that VC can be used to examine the disentanglement
performance of a S3R, and there is a need for disentanglement
if one tries to develop an universal representation, which not
yet exists. Also, we hope this work can serve as a good
initiative for future S3R researchers to emphasize on the
disentanglement performance of their model, without hurting
the scores on other tasks like ASR and ASV. This could
have a bigger impact on the community compared to pursuing
incremental improvements on other tasks.

ACKNOWLEDGMENT

We would like to thank the S3PRL/SUPERB team for the
fruitful discussions. This work was partly supported by JSPS
KAKENHI Grant Number 21J20920, JST CREST Grant Num-
ber JPMJCR19A3, and a project, JPNP20006, commissioned
by NEDO, Japan.

REFERENCES

[1] S. H. Mohammadi and A. Kain, “An overview of voice conversion

systems,” Speech Communication, vol. 88, pp. 65–82, 2017.

[2] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of voice con-
version and its challenges: From statistical modeling to deep learning,”
IEEE/ACM TASLP, vol. 29, pp. 132–157, 2021.

[3] S. Aryal, D. Felps, and R. Gutierrez-Osuna, “Foreign accent conversion

through voice morphing.” in Proc. Interspeech, 2013, pp. 3077–3081.

[4] A. Kain and M. W. Macon, “Spectral voice conversion for text-to-speech

synthesis,” in Proc. ICASSP, vol. 1, 1998, pp. 285–288.

[5] J. Latorre, V. Wan, and K. Yanagisawa, “Voice expression conversion
with factorised hmm-tts models,” in Proc. Interspeech, 2014, pp. 1514–
1518.

[6] K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano, “Speaking-aid
systems using gmm-based voice conversion for electrolaryngeal speech,”
Speech Communication, vol. 54, no. 1, pp. 134–146, 2012.

[7] T. Toda, K. Nakamura, H. Saruwatari, K. Shikano et al., “Alaryngeal
speech enhancement based on one-to-many eigenvoice conversion,”
IEEE/ACM TASLP, vol. 22, no. 1, pp. 172–183, 2014.

[8] K. Tanaka, T. Toda, G. Neubig, S. Sakti, and S. Nakamura, “A hybrid
approach to electrolaryngeal speech enhancement based on spectral
subtraction and statistical voice conversion.” in Proc. Interspeech, 2013,
pp. 3067–3071.

[9] D. Childers, B. Yegnanarayana, and K. Wu, “Voice conversion: Factors
responsible for quality,” in Proc. ICASSP, vol. 10, 1985, pp. 748–751.
[10] Y. Stylianou, O. Cappe, and E. Moulines, “Continuous probabilistic
transform for voice conversion,” IEEE TSAP, vol. 6, no. 2, pp. 131–
142, 1998.

[11] T. Toda, A. W. Black, and K. Tokuda, “Voice Conversion Based
on Maximum-Likelihood Estimation of Spectral Parameter Trajectory,”
IEEE TASLP, vol. 15, no. 8, pp. 2222–2235, 2007.

[12] M. Abe, S. Nakamura, K. Shikano, and H. Kuwabara, “Voice conversion
through vector quantization,” in Proc. ICASSP, vol. 1, 1988, pp. 655–
658.

[13] T. Kaneko and H. Kameoka, “Cyclegan-vc: Non-parallel voice conver-
sion using cycle-consistent adversarial networks,” in Proc. European
Signal Processing Conference (EUSIPCO), 2018, pp. 2100–2104.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

[14] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013.

[15] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, “Voice
conversion from non-parallel corpora using variational auto-encoder,” in
Proc. APISPA ASC, 2016, pp. 1–6.

[16] ——, “Voice conversion from unaligned corpora using variational
autoencoding wasserstein generative adversarial networks,” in Proc.
Interspeech, 2017, pp. 3364–3368.

[17] A. van den Oord, O. Vinyals, and k. Kavukcuoglu, “Neural discrete
representation learning,” in Advances in Neural Information Processing
Systems 30, 2017, pp. 6306–6315.

[18] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura,
“VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec
Inverter for Zerospeech Challenge 2019,” in Proc. Interspeech, 2019,
pp. 1118–1122.

[19] J.-C. Chou, C.-C. Yeh, and H.-Y. Lee, “One-shot voice conversion by
separating speaker and content representations with instance normaliza-
tion,” in Proc. Interspeech, 2019, pp. 664–668.

[20] J. Zhang, Z. Ling, and L. Dai, “Non-Parallel Sequence-to-Sequence
Voice Conversion With Disentangled Linguistic and Speaker Repre-
sentations,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 28, pp. 540–552, 2020.

[21] Y. Zhao, W.-C. Huang, X. Tian, J. Yamagishi, R. K. Das, T. Kinnunen,
Z. Ling, and T. Toda, “Voice Conversion Challenge 2020 - Intra-
lingual semi-parallel and cross-lingual voice conversion -,” in Proc. Joint
Workshop for the BC and VCC 2020, 2020, pp. 80–98.

[22] W.-C. Huang, T. Hayashi, S. Watanabe, and T. Toda, “The Sequence-to-
Sequence Baseline for the Voice Conversion Challenge 2020: Cascading
ASR and TTS,” in Proc. Joint Workshop for the BC and VCC 2020,
2020, pp. 160–164.

[23] J.-X. Zhang, L.-J. Liu, Y.-N. Chen, Y.-J. Hu, Y. J., Z.-H. Ling, and L.-R.
Dai, “Voice Conversion by Cascading Automatic Speech Recognition
and Text-to-Speech Synthesis with Prosody Transfer,” in Proc. Joint
Workshop for the BC and VCC 2020, 2020, pp. 121–125.

[24] L.-J. Liu, Y.-N. Chen, J.-X. Zhang, Y. Jiang, Y.-J. Hu, Z.-H. Ling, and L.-
R. Dai, “Non-Parallel Voice Conversion with Autoregressive Conversion
Model and Duration Adjustment,” in Proc. Joint Workshop for the BC
and VCC 2020, 2020, pp. 126–130.

[25] Q. Ma, R. Liu, X. Wen, C. Lu, and X. Chen, “Submission from SRCB
for Voice Conversion Challenge 2020,” in Proc. Joint Workshop for the
BC and VCC 2020, 2020, pp. 131–135.

[26] L. Zheng, J. Tao, Z. Wen, and R. Zhong, “CASIA Voice Conversion
System for the Voice Conversion Challenge 2020,” in Proc. Joint
Workshop for the BC and VCC 2020, 2020, pp. 136–139.

[27] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A
framework for self-supervised learning of speech representations,” in
Proc. NeruIPS, 2020.

[28] Z. Fan, M. Li, S. Zhou, and B. Xu, “Exploring wav2vec 2.0 on speaker
veriﬁcation and language identiﬁcation,” in Proc. Interspeech, 2021, pp.
1509–1513.

[29] W.-C. Huang, S.-W. Yang, T. Hayashi, H.-Y. Lee, S. Watanabe, and
T. Toda, “S3PRL-VC: Open-Source Voice Conversion Framework with
Self-Supervised Speech Representations,” in Proc. ICASSP, 2022, pp.
6552–6556.

[30] S.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y.
Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng,
K.
t. Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe,
A. Mohamed, and H.-Y. Lee, “SUPERB: Speech processing Universal
PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198.
[31] L.-J. Liu, Z.-H. Ling, Y. Jiang, M. Zhou, and L.-R. Dai, “WaveNet
Vocoder with Limited Training Data for Voice Conversion,” in Proc.
Interspeech, 2018, pp. 1983–1987.

[32] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu,
A. Mohamed, and E. Dupoux, “Speech Resynthesis from Discrete
Disentangled Self-Supervised Representations,” in Proc. Interspeech,
2021, pp. 3615–3619.

[33] W.-C. Huang, Y.-C. Wu, T. Hayashi, and T. Toda, “Any-to-One
Sequence-to-Sequence Voice Conversion using Self-Supervised Discrete
Speech Representations,” in Proc. ICASSP, 2021, pp. 5944–5948.
[34] Y. Y. Lin, C.-M. Chien, J.-H. Lin, H.-Y. Lee, and L.-S. Lee, “Frag-
mentVC: Any-to-Any Voice Conversion by End-to-End Extracting and
Fusing Fine-Grained Voice Fragments With Attention,” in Proc. ICASSP,
2021, pp. 5939–5943.

[35] J.-H. Lin, Y. Y. Lin, C.-M. Chien, and H.-Y. Lee, “S2VC: A Frame-
work for Any-to-Any Voice Conversion with Self-Supervised Pretrained
Representations,” in Proc. Interspeech, 2021, pp. 836–840.

[36] W.-C. Huang, T. Hayashi, X. Li, S. Watanabe, and T. Toda, “On Prosody
Modeling for ASR+ TTS based Voice Conversion,” in Proc. ASRU, 2021.
[37] S. Liu, Y. Cao, S. Kang, N. Hu, X. Liu, D. Su, D. Yu, and H. Meng,
“Transferring Source Style in Non-Parallel Voice Conversion,” in Proc.
Interspeech, 2020, pp. 4721–4725.

[38] L. Sun, K. Li, H. Wang, S. Kang, and H. Meng, “Phonetic posterior-
grams for many-to-one voice conversion without parallel data training,”
in Proc. ICME, 2016, pp. 1–6.

[39] J. Zhang, Z. Ling, L. Liu, Y. Jiang, and L. Dai, “Sequence-to-Sequence
Acoustic Modeling for Voice Conversion,” IEEE/ACM TASLP, vol. 27,
no. 3, pp. 631–644, 2019.

[40] D.-Y. Wu and H.-y. Lee, “One-Shot Voice Conversion by Vector Quan-

tization,” in Proc. ICASSP, 2020, pp. 7734–7738.

[41] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-Supervised

Learning of Discrete Speech Representations,” in Proc. ICLR, 2020.

[42] Y.-H. Peng, C.-H. Hu, A. Kang, H.-S. Lee, P.-Y. Chen, Y. Tsao, and
H.-M. Wang, “The Academia Sinica Systems of Voice Conversion for
VCC2020,” in Proc. Joint Workshop for the BC and VCC 2020, 2020,
pp. 180–183.

[43] Y.-A. Chung and J. Glass, “Generative Pre-Training for Speech with
Autoregressive Predictive Coding,” in Proc. ICASSP, 2020, pp. 3497–
3501.

[44] Y.-A. Chung, H. Tang, and J. Glass, “Vector-Quantized Autoregressive
Predictive Coding,” in Proc. Interspeech, 2020, pp. 3760–3764.
[45] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, “Mockingjay:
Unsupervised Speech Representation Learning with Deep Bidirectional
Transformer Encoders,” in Proc. ICASSP, 2020.

[46] A. T. Liu, S.-W. Li, and H.-y. Lee, “TERA: Self-Supervised Learning
of Transformer Encoder Representation for Speech,” IEEE/ACM TASLP,
vol. 29, pp. 2351–2366, 2021.

[47] A. H. Liu, Y.-A. Chung, and J. Glass, “Non-Autoregressive Predictive
Coding for Learning Speech Representations from Local Dependencies,”
in Proc. Interspeech, 2021, pp. 3730–3734.

[48] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with
contrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.
[49] M. Rivi`ere, A. Joulin, P.-E. Mazar´e, and E. Dupoux, “Unsupervised
Pretraining Transfers Well Across Languages,” in Proc. ICASSP, 2020,
pp. 7414–7418.

[50] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec: Un-
supervised Pre-Training for Speech Recognition,” in Proc. Interspeech,
2019, pp. 3465–3469.

[51] W.-N. Hsu, Y.-H. H. Tsai, B. Bolte, R. Salakhutdinov, and A. Mohamed,
“HuBERT: How Much Can a Bad Teacher Beneﬁt ASR Pre-Training?”
in Proc. ICASSP, 2021, pp. 6533–6537.

[52] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro,
J. Trmal, and Y. Bengio, “Multi-task self-supervised learning for robust
speech recognition,” in Proc. ICASSP, 2020, pp. 6989–6993.

[53] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: An
ASR corpus based on public domain audio books,” in Proc. ICASSP,
2015, pp. 5206–5210.

[54] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar´e,
J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., “Libri-light:
A benchmark for asr with limited or no supervision,” in Proc. ICASSP,
2020, pp. 7669–7673.

[55] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett,
“DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM.
NIST speech disc 1-1.1,” NASA STI/Recon technical report n, vol. 93,
p. 27403, 1993.

[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is All you Need,” in Proc. NIPS,
2017, pp. 5998–6008.

[57] X. Wang, S. Takaki, and J. Yamagishi, “An autoregressive recurrent
mixture density network for parametric speech synthesis,” in Proc.
ICASSP, 2017, pp. 4895–4899.

[58] ——, “Autoregressive Neural F0 Model for Statistical Parametric Speech

Synthesis,” IEEE/ACM TASLP, vol. 26, no. 8, pp. 1406–1419, 2018.

[59] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,
Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis,
R. Clark, and R. A. Saurous, “Tacotron: Towards end-to-end speech
synthesis,” in Proc. Interspeech, 2017, pp. 4006–4010.

[60] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,
Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgian-
nakis, and Y. Wu, “Natural TTS Synthesis by Conditioning WaveNet on
MEL Spectrogram Predictions,” in Proc. ICASSP, 2018, pp. 4779–4783.
[61] C. Veaux, J. Yamagishi, and K. MacDonald, “CSTR VCTK Corpus:
English Multi-speaker Corpus for CSTR Voice Cloning Toolkit,” 2017.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

[62] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-
Dominguez, “Deep neural networks for small footprint text-dependent
speaker veriﬁcation,” in Proc. ICASSP, 2014, pp. 4052–4056.

[63] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Voxceleb: Large-
scale speaker veriﬁcation in the wild,” Computer Speech & Language,
vol. 60, p. 101027, 2020.

[64] J. S. Chung, A. Nagrani, and A. Zisserman, “VoxCeleb2: Deep Speaker

Recognition,” in Proc. Interspeech, 2018, pp. 1086–1090.

[65] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, “Unsupervised Speech

Recognition,” in Proc. NeulIPS, 2021.

[66] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative Adversarial
Networks for Efﬁcient and High Fidelity Speech Synthesis,” in Proc.
NeurIPS, vol. 33, 2020, pp. 17 022–17 033.

[67] S. Ling and Y. Liu, “Decoar 2.0: Deep contextualized acoustic repre-
sentations with vector quantization,” arXiv preprint arXiv:2012.06659,
2020.

[68] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A Vocoder-Based
High-Quality Speech Synthesis System for Real-Time Applications,”
IEICE Transactions on Information and Systems, vol. 99, pp. 1877–
1884, 2016.

[69] R. K. Das, T. Kinnunen, W.-C. Huang, Z.-H. Ling, J. Yamagishi, Z. Yi,
X. Tian, and T. Toda, “Predictions of Subjective Ratings and Spooﬁng
Assessments of Voice Conversion Challenge 2020 Submissions,” in
Proc. Joint Workshop for the BC and VCC 2020, 2020, pp. 99–120.

[70] B. Naderi and R. Cutler, “An Open Source Implementation of ITU-T
Recommendation P.808 with Validation,” in Proc. Interspeech, 2020, pp.
2862–2866.

[71] ITU-T Recommendation P.808, Subjective evaluation of speech quality

with a crowdsourcing approach, Std., 2018.

[72] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, “Un-
supervised Cross-Lingual Representation Learning for Speech Recogni-
tion,” in Proc. Interspeech, 2021, pp. 2426–2430.

[73] T. Hayashi, A. Tamamori, K. Kobayashi, K. Takeda, and T. Toda, “An
investigation of multi-speaker training for WaveNet vocoder,” in Proc.
ASRU, 2017, pp. 712–718.

[74] S. Liu, Y. Cao, X. Wu, L. Sun, X. Liu, and H. Meng, “Jointly
Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice
Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams,” in
Proc. Interspeech, 2019, pp. 714–718.

[75] W.-C. Huang, Y.-C. Wu, H.-T. Hwang, P. L. Tobing, T. Hayashi,
K. Kobayashi, T. Toda, Y. Tsao, and H.-M. Wang, “Reﬁned WaveNet
Vocoder for Variational Autoencoder Based Voice Conversion,” in Proc.
European Signal Processing Conference (EUSIPCO), 2019, pp. 1–5.

[76] H.-S. Tsai, H.-J. Chang, W.-C. Huang, Z. Huang, K. Lakhotia, S.-w.
Yang, S. Dong, A. Liu, C.-I. Lai, J. Shi, X. Chang, P. Hall, H.-J.
Chen, S.-W. Li, S. Watanabe, A. Mohamed, and H.-y. Lee, “SUPERB-
SG: Enhanced speech processing universal PERformance benchmark for
semantic and generative capabilities,” in Proc. ACL (Volume 1: Long
Papers), May 2022, pp. 8479–8492.

