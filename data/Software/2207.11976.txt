Empirical Software Engineering manuscript No.
(will be inserted by the editor)

Diﬀerential testing for machine learning: an analysis
for classiﬁcation algorithms beyond deep learning

Steﬀen Herbold · Steﬀen Tunkel

2
2
0
2

l
u
J

5
2

]
E
S
.
s
c
[

1
v
6
7
9
1
1
.
7
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Context: Diﬀerential testing is a useful approach that uses diﬀerent
implementations of the same algorithms and compares the results for software
testing. In recent years, this approach was successfully used for test campaigns
of deep learning frameworks.
Objective: There is little knowledge on the application of diﬀerential testing
beyond deep learning. Within this article, we want to close this gap for clas-
siﬁcation algorithms.
Method: We conduct a case study using Scikit-learn, Weka, Spark MLlib, and
Caret in which we identify the potential of diﬀerential testing by consider-
ing which algorithms are available in multiple frameworks, the feasibility by
identifying pairs of algorithms that should exhibit the same behavior, and
the eﬀectiveness by executing tests for the identiﬁed pairs and analyzing the
deviations.
Results: While we found a large potential for popular algorithms, the feasibility
seems limited because often it is not possible to determine conﬁgurations that
are the same in other frameworks. The execution of the feasible tests revealed
that there is a large amount of deviations for the scores and classes. Only a
lenient approach based on statistical signiﬁcance of classes does not lead to a
huge amount of test failures.
Conclusions: The potential of diﬀerential testing beyond deep learning seems
limited for research into the quality of machine learning libraries. Practitioners
may still use the approach if they have deep knowledge about implementations,
especially if a coarse oracle that only considers signiﬁcant diﬀerences of classes
is suﬃcient.

Steﬀen Herbold
Institute of Software and System Engineering, TU Clausthal, Germany
E-mail: steﬀen.herbold@tu-clausthal.de

Steﬀen Tunkel
Institute of Computer Science, University of Goettingen, Germany
E-mail: steﬀen.tunkel@stud.uni-goettingen.de

 
 
 
 
 
 
2

Steﬀen Herbold, Steﬀen Tunkel

Keywords machine learning · software testing · diﬀerential testing

1 Introduction

With diﬀerential testing, multiple implementations of the same algorithm are
executed with the same data. Inconsistencies between the results are indicators
for bugs. In recent years, this idea was adopted to deﬁne test oracles for deep
learning (e.g., Pham et al., 2019; Wang et al., 2020; Guo et al., 2020; Asyroﬁ
et al., 2020). This works well, because there are multiple versatile frameworks
such as PyTorch (Paszke et al., 2019) and TensorFlow (Abadi et al., 2016) that
allow the deﬁnition of exactly the same neural network structures and training
procedures. Beyond deep learning, this idea was also successfully applied to lin-
ear regression (McCullough et al., 2019), to cases where the optimal solutions
were known as oracle beforehand. Due to the importance of linear regression
as a basic technique, there are many powerful implementations of this, which
enabled the deﬁnition of diﬀerential testing. Moreover, the known optimal so-
lution served as an additional test oracle. However, even though diﬀerential
testing was successful for deep learning and linear regression, it is not obvious
that this should also be the case for other machine learning algorithms. For
example, while the general concept of random forests Breiman (2001) is well-
deﬁned,1 other aspects depend on the implementation, e.g., which decision
tree algorithm is used and how the sub-sampling can be conﬁgured. Whether
we can expect random forest implementations to behave the same depends
on how developers navigate these options, e.g., if they implement the same
variants in a hard-coded way or if they expose conﬁguration options through
hyperparameters. Due to this, it is unclear if and how diﬀerent implementa-
tions of the same algorithms can be directly compared to each other. With
deep learning, this problem does not exist: network structures, training pro-
cedures, loss functions, and optimization algorithms are all conﬁgured by the
user of the framework through the API. Moreover, while some deviations are
always expected with randomized algorithms like the training algorithms for
(deep) neural networks, other algorithms are deterministic and should lead to
exactly the same results, which was also not yet considered.

Thus, while we know that diﬀerential testing can be useful to deﬁne pseudo
oracles (Davis and Weyuker, 1981) for the quality assurance of algorithms, we
lack knowledge for other types of machine learning tasks beyond deep learning
and linear regression. Within our work, we close this gap for classiﬁcation
algorithms and investigate the following research question.

Research Question: What is the potential, feasibility, and eﬀectiveness of
diﬀerential testing for classiﬁcation algorithms beyond deep learning, i.e.,
for techniques like decision trees, random forests, or k-nearest neighbor
classiﬁcation.

1 bootstrap sampling of instances, subsets of features for decision

Diﬀerential testing for machine learning

3

We investigate our research question within an exploratory case study
of four machine learning frameworks: Scikit-learn (Pedregosa et al., 2011),
Weka (Frank et al., 2016), Spark MLlib(Meng et al., 2016), and Caret (Kuhn,
2018). We use a methodology with three phases within our study. First, we
identify for which algorithms we ﬁnd multiple implementations. Through this,
we evaluate the potential, i.e., for how many algorithms diﬀerential testing
could possibly work, due to the availability of multiple implementations in
large frameworks. Second, we compare the algorithms in detail to understand
if it is possible to conﬁgure them in the same way using their public inter-
faces through the deﬁnition of appropriate hyperparameters. Through this,
we evaluate the feasibility, i.e., if we can actually exploit the potential be-
cause implementations are suﬃciently similar to serve as pseudo oracles and
identify inconsistencies. Third, we implement diﬀerential testing for the algo-
rithms for which we are able to ﬁnd identical conﬁgurations. Through this,
we evaluate the eﬀectiveness of diﬀerential testing, i.e., the capability to ﬁnd
inconsistencies with the goal to reveal potential bugs.

Through this study, we contribute the following to the state-of-the-art.

– We found that while there is a large potential for diﬀerential testing, due
to diﬀerent implementations of the same algorithms, it is diﬃcult to har-
ness the potential and ﬁnd feasible combinations of algorithms that should
behave the same. This is due to the diversity in hyperparameters between
the implementations.

– We observe many deviations between implementations that should yield
similar results. The number of deviations indicates that there is a large
amount of noise in the results, which makes it impossible for us to pick up
a reliable signal to identify possible bugs.

– Nevertheless, we also found that for experts that want to test a speciﬁc
algorithm, an approach with a lenient test oracle that only considers if the
classiﬁcations are signiﬁcantly diﬀerent, could be useful.

The remainder of this article is structured as follows. We deﬁne our ter-
minology and notations in Section 2 followed by a discussion of the related
work in Section 3. Then, we present our case study in Section 4, including our
subjects, methodology, and results for each of the three phases. We discuss the
implications of our results in Section 5, followed by the threats to validity in
Section 6 before we conclude in Section 7.

2 Terminology and Notations

Within this article, we focus on binary classiﬁcation. Formally, we have in-
stances xi = (xi,1, ..., xi,m) ∈ F ⊆ Rm with labels yi ∈ {0, 1} for i = 1, ..., n.
We say that F is the feature space. The binary set {0, 1} represents the classes
and we note that the classes are considered as categories and can be replaced
by any other binary set, e.g., {f alse, true} or {-1, +1}. A classiﬁcation model
tries to ﬁnd a function f : F → {0, 1} such that f (xi) ≈ yi. Often, this is done

4

Steﬀen Herbold, Steﬀen Tunkel

score : F → R such that
by estimating scores for both classes c ∈ {0, 1} as f c
f (xi) = arg maxc∈{0,1} f c
score(xi). We note that the scores are often probabil-
ities of classes and the class assignment can also be optimized using diﬀerent
thresholds for scores. However, neither the question if the scores represent
probabilities nor the optimization of thresholds is relevant for to work and not
further discussed.

Based on the above deﬁnitions, a classiﬁcation algorithm A is an algorithm
that takes as input training instances (X train, Y train) = (xtrain
), i =
1, ..., ntrain and outputs functions f, fscore = A(X train, Y train). We further
deﬁne X test, Y test = (xtest
, ytest), i = 1, ..., ntest as test data. When we discuss
the comparison of two algorithms, we refer to them as A1 and A2 with the re-
spective functions f 1, f 1
score as result of the training. Moreover,
we use the notation 1condition for the indicator function, which is one when
the condition is fulﬁlled and zero otherwise.

score, f 2, and f 2

, ytrain
i

i

i

We note that we use the terms false negative and false positive not with
respect to the classiﬁcation of the algorithms, as is common in literature about
machine learning. Instead, we use these terms in their common meaning in the
software testing literature: a false positive is a software test that fails, even
though there is no wrong behavior and, vice versa, a false negative is a software
test that misses wrong behavior it should detect.

3 Related Work

We restrict our discussion of related work to other studies that utilize multi-
ple implementations of the same algorithms for the deﬁnition of software tests.
There are several recent surveys on software engineering and software testing
for machine learning, to which we refer readers for a general overview (Zhang
et al., 2020; Braiek and Khomh, 2020; Giray, 2021; Mart´ınez-Fern´andez et al.,
2021). This includes other approaches for addressing the oracle problem for
machine learning, like metamorphic testing (i.e., systematic changes to input
data such that the expected changes in output are known, e.g., Murphy et al.
(2008); Xie et al. (2011); Ding et al. (2017)), using crashes as oracle (e.g. Her-
bold and Haar, 2022), deﬁning numeric properties that can be checked (e.g.
Karpathy, 2018) or trying to map the performance of predictions to require-
ments to derive tests (e.g. Barash et al., 2019). Further, there is also a diﬀerent
type of diﬀerential testing approach that is used for the testing of the robust-
ness of models (e.g., Pei et al., 2019; Guo et al., 2021), i.e., similar deep learning
models or inputs were used as pseudo oracles. However, such tests of speciﬁc
models are not within the scope of our work, which considers the testing of
the underlying libraries that implement machine learning algorithms and not
the testing of learned models.

One method to solve the test oracle problem of machine learning (e.g. Mur-
phy et al., 2007; Groce et al., 2014; Marijan and Gotlieb, 2020) is to imple-
ment a diﬀerential testing between multiple implementations of the same algo-
rithm Murphy et al. (2007) and use the diﬀerent implementations as pseudo-

Diﬀerential testing for machine learning

5

oracles (Davis and Weyuker, 1981). While this idea is not new for machine
learning and was already shown to work for the MartiRank algorithm (Gross
et al., 2006) by Murphy et al. (2007), there was only little follow up work in
this direction until recently.

Most notably, this idea gained traction for the testing of deep learning
frameworks. Pham et al. (2019) developed the tool CRADLE which used, e.g.,
TensorFlow (Abadi et al., 2016) and Theano (Theano Development Team,
2016) as backends. The comparison of results of outputs between diﬀerent
implementations was eﬀective at ﬁnding errors, even though only few diﬀerent
deep learning models were used. Following this study, there were many papers
who build upon this work, e.g., LEMON by Wang et al. (2020) that proposes
a mutation approach to cover more diﬀerent network architectures during the
testing, AUDEE by Guo et al. (2020) that uses a genetic algorithm to generate
eﬀective tests, and CrossASR by Asyroﬁ et al. (2020) which applies a similar
concept for the testing of automated speech recognition models. Beyond deep
learning, the idea was also used by McCullough et al. (2019) for the comparison
of diﬀerent implementations of linear regression algorithms and shown to be
eﬀective. However, we note that while McCullough et al. (2019) compared
diﬀerent implementations to each other, they use analytic solutions as ground
truth, i.e., they did not really use the implementations themselves as pseudo
oracles.

In comparison to prior work, our focus is not on the diﬀerential testing of a
single algorithm (Murphy et al., 2007; McCullough et al., 2019) or deep learn-
ing. Instead, we consider this question broadly for classiﬁcation algorithms.
With the exception of multilayer perceptrons, we exclude deep learning, which
can also be used for classiﬁcation, as these are already studied.2 We further
note that neither Murphy et al. (2007) nor McCullough et al. (2019) studied
classiﬁcation algorithms, but rather a ranking algorithm and a regression al-
gorithm. To the best of our knowledge, this is, therefore, the ﬁrst work that
considers diﬀerential testing for non-deep learning classiﬁcation algorithms.

4 Case Study

Within this section, we discuss the main contribution of our work: a case study
about four machine learning libraries that explores the usefulness of diﬀeren-
tial testing for classiﬁcation algorithms. In the following, we ﬁrst discuss the
subjects of our study followed by a description of our methodology, including
the variables we measure and the research methods we use. Then, we pro-
ceed with the presentation of the results for each phase of our case study. We

2 Multilayer perceptrons are commonly found in general purpose machine learning li-

braries but do not provide the same ﬂexibility as deep learning frameworks.

6

Steﬀen Herbold, Steﬀen Tunkel

provide our implementation of the tests and additional details through our
replication kit online.3

4.1 Subjects

We used purposive sampling (Patton, 2014) for our case study based on four
criteria. First, we wanted to use subjects implemented in diﬀerent program-
ming languages. One important use case of diﬀerential testing is to validate
the behavior of new implementations of algorithms, which includes the im-
plementation in a new programming language. Second, all subjects should
be implemented by, to the best of our knowledge, independent project teams.
Overlap between teams could have an adverse aﬀect on our study of diﬀerential
testing, because our results could be aﬀected by the same developers working
on multiple implementations. There is a non-negligible likelihood that such a
developer would make similar design and implementation choices, which could
lead to an overestimation of similarities between independent implementa-
tions. Third, subjects should cover multiple algorithms. If our subjects are too
small, this would negatively aﬀect our research as the likelihood of ﬁnding the
same algorithms multiple times would decrease. Fourth, the libraries should
not focus on deep learning. Since our goal is to provide a perspective beyond
deep learning, we speciﬁcally exclude libraries whose main use case is deep
learning, even if the methods provided by these libraries could also be used to
implement, e.g., a linear logistic regression model. Based on these criteria, we
selected four such libraries as shown in Table ??.

4.2 Methodology

Our case study was conducted in three phases, as depicted in Figure 1. Each
phase provides insights into a diﬀerent aspect of the usefulness of diﬀerential
testing. Moreover, each phase builds on the results of the previous phase.

In the ﬁrst phase, we evaluate the basic assumption of diﬀerential test-
ing: we have multiple implementations of the same algorithm. To evaluate
this, we analyze each framework and extract the classiﬁcation algorithms. We
then compare these lists to each other to determine overlaps. The number
of overlaps determines the potential for the diﬀerential testing, because this
means that multiple implementations are available. For all frameworks, we
exclude pure meta learners from our analysis. Meta learners are wrappers
around other classiﬁers (e.g., generic boosting (Freund and Schapire, 1997) or
bagging (Breiman, 1996)), i.e., they propagate the learning to other classiﬁers

3 https://github.com/sherbold/replication-kit-2022-non-dl-diff-test - We will
create a long-term archive on Zenodo of the replication kit which we reference here in
case of acceptance.

4 https://www.hitachivantara.com/en-us/products/data-management-

analytics/pentaho-platform.html

Diﬀerential testing for machine learning

7

Framework Version

Language Description

Scikit-learn

1.0.1

Python

Weka

3.8.5

Java

Spark MLLib

3.0.0

Scala

Caret

6.0-90

R

Scikit-learn (Pedregosa et al., 2011) is a pop-
ular machine learning library for Python and
one of the reasons for the growing popularity
of python as a language for data analysis.
Weka is a popular machine learning library for
Java which had the ﬁrst oﬃcial release in 1999.
The library is very mature and has been used
by researchers and practitioners for decades
and is, e.g., part of the Pentaho business in-
telligence software.4
Spark MLLib (Meng et al., 2016) is the ma-
chine learning component of the rapidly grow-
ing big data framework Apache Spark Zaharia
et al. (2010) that is developed with Scala.
Caret (Kuhn, 2018) provides common inter-
faces for many learning algorithms imple-
mented in R. Through this, Caret provides
a harmonized Application Programing Inter-
face (API) that allows using many learning al-
gorithms that are implemented with diﬀerent
APIs as part of diﬀerent R packages. Thus,
Caret greatly reduces the diﬃculty of trying
out diﬀerent algorithms to solve a problem.

Table 1: Overview of the machine learning libraries selected for our study.

1

e
s
a
h
P

2

e
s
a
h
P

3

e
s
a
h
P

Collect Classiﬁcation Algorithms

Lists of Algorithms

Match algorithms

Algorithms that are in > 3 frameworks

Compare API documentation

Algorithms suitable for diﬀerential testing

Run diﬀerential tests

Results of diﬀerential testing

Fig. 1: Methodology Overview

8

Steﬀen Herbold, Steﬀen Tunkel

called base classiﬁers. Thus, meta learners require special consideration: we
must ﬁrst establish that there are other equal algorithms, because otherwise
it would be unclear if diﬀerences observed when comparing meta learners are
due to deviations between the meta learners, or due to diﬀerences between the
base classiﬁers.

In the second phase, we evaluate if we actually manage to implement dif-
ferential tests. This is non-trivial due to the hyperparameters of algorithms.
Hyperparameters allow the users of a machine learning algorithm to conﬁgure
the training and/or the resulting model, e.g., by restricting sizes, enforcing
regularization, or even by selecting which variant of an algorithm is used.
While the same is true for deep learning, there is a big diﬀerence: with deep
learning, every user of a library has to conﬁgure the hyperparameters for each
neural network and training. On the one hand, this increases the burden on
the developers, as this is not pre-conﬁgured. On the other hand, there are
no restrictions and thus, deﬁning an equivalent set of hyperparameters is not
problematic. This is diﬀerent with other algorithms. For example, there are
many diﬀerent decision tree variants, such as CART (Brieman et al., 1984),
ID3 (Quinlan, 1986), and C4.5 (Quinlan, 1993). A random forest could be
based on each of these variants, in which case the random forests could not
be directly compared. However, even the same variant of a decision tree is not
always the same, as concrete implementations may not implement the original
speciﬁcation or they may have added more features, e.g., for avoiding overﬁt-
ting through limiting tree depths or requiring a certain number of instances to
make decisions. Thus, within the second phase we compare diﬀerent implemen-
tations based on their API documentations and determine for which overlaps
we manage to ﬁnd matching conﬁgurations. For Caret, we also included the
API documentation of the R packages that are providing the implementa-
tion that is wrapped by Caret in our analysis. Furthermore, our analysis also
includes the reasons for cases where we fail to match the hyperparameters.

In the third phase, we implement a simple diﬀerential testing approach.

We use four data sets as input.

– UNIFORM: 200 randomly generated instances, with ten features that are
uniform in the interval [0, 1] with two classes, such that 50% of the data
are in each class. The separation between the classes is based on a 10-
dimensional rectangle, i.e., a clear separation between the classes is possi-
ble.

– RANDOM: 200 randomly generated instances, where the features are uni-
form within the interval [0, 1] with two classes that are randomly assigned.
Thus, no reasonable classiﬁcation is possible.

– BC: The breast cancer data from the UCI repository has 569 instances 30
numeric features for cancer prediction. The features are normalized to the
interval [0, 1].

– WINE: The wine data from the UCI repository has 178 instances with 11
numeric features for wine quality prediction. The features are normalized
to the interval [0, 1].

Diﬀerential testing for machine learning

9

The ﬁrst two data sets are reused from our prior work (Herbold and Haar,
2022) for the smoke testing of algorithms. With UNIFORM, we can test if
diﬀerent implementations perform the same, for data that is relatively easy
to learn based on informative features. With RANDOM, we test if the im-
plementations also yield the same results, in case the data is not informative.
This scenario can be considered “more diﬃcult”, as the resulting model is
more or less random. Still, the “randomness” should depend on the algorithm
and, ideally, diﬀerent implementations still yield the same results. The other
two data sets are popular data sets that are often used as examples for using
machine learning models. All data sets are randomly split into 50% training
data and 50% test data.

The data we use have two aspects in common: they have two classes and
the features are within well-bounded and numerically not problematic feature
spaces. Consequently, we are implementing a relatively simple diﬀerential test-
ing scenario, i.e., we do not intentionally stress the algorithms.5 Our rationale
for this is that we want to test the general feasibility of diﬀerential testing
to ﬁnd errors due to deviations between implementations. If our data would
target corner cases, our results could overestimate the eﬀectiveness because dif-
ferences might be larger for extreme data or be polluted by algorithms crashing
failures due to numeric issues. Moreover, classiﬁcation with more classes either
uses exactly the same algorithms (e.g., decision trees and neural networks) or
a combination of multiple binary models (e.g, one-vs-follower approach with
support vector machines). However, we may also underestimate the diﬀerence
between algorithms for multiple classes.

We use four criteria to evaluate the outcome of the diﬀerential tests for
two algorithms A1 and A2. The ﬁrst two criteria measure if the results are
equal, i.e., if two diﬀerent implementations yield the same results. To evaluate
this, we consider the number of diﬀerences between the classes predicted by
the two algorithms deﬁned as

∆ =

n
(cid:88)

i=1

1f 1(xi)(cid:54)=f 2(xi)

(1)

as the ﬁrst criterion and the number of deviations between the scores deﬁned
as

n
(cid:88)

∆score =

1f 1

score(xi)(cid:54)=f 2

score(xi)

(2)

i=1

as the second criterion. Please note that we consider scores to be equal, if the
diﬀerence is less than 0.001. Smaller diﬀerences are almost always irrelevant
and may easily be due to diﬀerences with how ﬂoating point operations are
implemented. If we were to enforce exact equality of scores, this could inﬂate
the false positive test results.

5 This is also the reason why we only use the test cases UNIFORM and RANDOM from
our prior work (Herbold and Haar, 2022). Our analysis of crashes showed that such data
does not lead to crashes, which would make the analysis of the eﬀectiveness of the diﬀerential
testing more diﬃcult.

10

Steﬀen Herbold, Steﬀen Tunkel

Additionally, we often cannot expect that two results of diﬀerent imple-
mentations are exactly the same, e.g., because an aspect of the algorithm is
randomized. However, the results should not diﬀer too much and we should
still get roughly the same result in terms of overall performance. We evalu-
ate this by looking at the signiﬁcance of the diﬀerences between the classes
and scores and derive the two remaining criteria. The third criterion consid-
ers the diﬀerences in the scores. Since the scores are continuous values and
we cannot make general assumptions regarding their distribution, we use the
Kolmogorov-Smirnoﬀ test. Thus, we check if the scores of the two models have
the same distribution. The fourth criterion applies this concept to the classes.
Similarly, we use the Chi-Squared test to compare the classes to check if the
diﬀerences in classiﬁcations are signiﬁcant. We reject the null hypotheses of
the tests (no diﬀerence) if we observe a p-value of less than 0.05. We do not
conduct any correction of repeated tests, as this correction would be dependent
on the number of tests that are executed, i.e., become a parameter of the test
campaign. This is hard to implement in practice. However, this means that we
expect that about 5% of the signiﬁcant diﬀerences we observe to be false posi-
tives, because the p-value follows a uniform distribution if the null hypothesis
is true. If and how this aﬀects the diﬀerential testing will be discussed based
on our results.

While it may seem counterintuitive that we use four criteria to evaluate
the eﬀectiveness of software tests, these criteria allow us not only to answer
the general question if diﬀerential tests are eﬀective, but also how lenient the
comparison of the results must be in order to not yield false positives. This is
due to the nature of machine learning. Figure 2 shows two cases, where correct
implementations of the same algorithm can yield diﬀerent optimal outcomes.
In both examples, the absolute diﬀerences we observe (the ﬁrst two criteria) are
not suitable to determine that a test failed, i.e., one result is wrong. However,
the diﬀerences between the outcomes should not be statistically signiﬁcant.
Ideally, we hope to see the limits of both absolute comparisons (criteria one
and two) and statistical comparison (criteria three and four) for the diﬀerential
testing as criteria for passing or failing diﬀerential tests.

Additionally, we apply all four criteria to both the training and the test
data. Both training and test data have advantages and disadvantages. The
training data has the advantage that it is readily available. However, software
tests that only rely on training data may miss cases in which the implemen-
tations does not correctly generalize beyond the training data. This is the
advantage of test data, as the computed functions for scoring and classiﬁ-
cation are evaluated on an independent data set. However, this may also be
prone to more false positives, because equally good results on the training data
may lead to diﬀerent results on the test data (see Figure 2). Since we use the
same amount of training and test data, the likelihood of diﬀerences between
implementations on the training and test data is not impacted by the amount
of data available.

Diﬀerential testing for machine learning

11

Fig. 2: Two examples of data sets, where the dotted and dashed lines represent
possible linear separations of the data that could, e.g., be computed by a
logistic regression. Both models are equally good in terms of accuracy.

Algorithm 1: Diﬀerential testing algorithm
Input: A1, A2, (X train, Y train), (X test, Y test)
Output: Number of diﬀerences ∆train, ∆test, ∆train

score, ∆test

score and p-values

KS , ptest
ptrain

KS , ptrain
χ2

, ptest
χ2

// Run training
f 1, f 1
f 2, f 1

score ← A1(X train, Y train)
score ← A2(X train, Y train)

// Criterion 1: Absolute differences between classes
∆train ← (cid:80)ntrain
i=1
∆test ← (cid:80)ntest
1
i=1

f 1(xtrain
i

)(cid:54)=f 2(xtrain

)(cid:54)=f 2(xtest

1

)

)

i

f 1(xtest
i

i

// Criterion 2: Absolute differences between scores
∆train

1

score(xtrain
f 1

)(cid:54)=f 2

score(xtrain

)

i

i

score ← (cid:80)ntrain
score ← (cid:80)ntest
1

i=1

i=1

∆test

score(xtest
f 1

i

)(cid:54)=f 2

score(xtest

i

)

score(X train), f 2
score(X test), f 2

// Criterion 3: Significance of difference between scores
ptrain
KS ← KS.test(f 1
score(X test))
KS ← KS.test(f 1
ptest
// Criterion 4: Significance of difference between classifications
ptrain
χ2 ← χ2.test(f 1(X train), f 2(X train))
χ2 ← χ2.test(f 1(X test), f 2(X test))
ptest

score(X train))

4.3 Phase 1: Overlap of Algorithms

Table 2 shows the overlapping algorithms of the four machine learning frame-
works we found using the API docs. We grouped the algorithms by their under-
lying paradigm, e.g., Naive Bayes or Decision Tree. The ﬁrst row demonstrates
the reason for this. While Scikit-learn provides unique classes for diﬀerent
variants of Naive Bayes, Spark MLlib has only a single class. However, the
implementation in Spark MLlib can be conﬁgured to use diﬀerent variants,

2.01.51.00.50.00.51.01.52.02.01.51.00.50.00.51.01.52.0Two equally good linear separations of four points86420246321012Two equally good linear separations betweendisjunctive point clouds12

Steﬀen Herbold, Steﬀen Tunkel

e.g., Gaussian Naive Bayes or Multinomial Naive Bayes. Our data shows that
there is a signiﬁcant overlap:

– Naive Bayes, Decision Trees, Random Forest, Support Vector Machines,
Multilayer Perceptrons, and Logistic Regression can be found in all frame-
works;

– a trivial classiﬁer, k-Nearest Neighbor, and Gradiant Boosting in at least

three of the frameworks; and

– a Perceptron, Stochastic Gradient Descent, Gaussian Process, LDA, QDA,
Nearest Centroid, Extra Trees, and Logistic Model Trees are present in two
frameworks.

– Additionally, there are 47 algorithms which are only implemented by a

single framework (see Table 5 in the appendix).

While we are not aware of any scientiﬁc data regarding this, the eight algo-
rithms that can be found in all frameworks are all well-known and frequently
mentioned in lists of techniques that should be learned.6 There seems to be
a link between the potential for diﬀerential testing and the popularity of ap-
proaches. Intuitively, this makes sense, because developers of machine learning
frameworks likely implement popular methods ﬁrst. For example, Scikit-learn
has a popularity/maturity based inclusion criterion based on the number of
citations.7 Another interesting aspect is that there are sometimes multiple im-
plementations for the same algorithm within a single framework. For Scikit-
learn, Weka, and Spark MLlib, these are diﬀerent variants of an algorithm.8
For Caret, this is not the case. The reason for this is that Caret is a meta
framework, i.e., a framework that wraps the implementations of diﬀerent R
packages to provide a common API. Due to this, there are sometimes imple-
mentations from diﬀerent R packages for the same algorithm, e.g., in the ﬁrst
row from the packages naive bayes and nb. Consequently, there is even a pos-
sibility for diﬀerential testing within the Caret package, without even using
other frameworks.

There is a large potential for diﬀerential testing beyond deep learning.
This potential is strongest for well-known and popular algorithms, while
newer or specialized algorithms are not good candidates for diﬀerential
testing.

4.4 Phase 2: Feasible Subset

Table 3 reports the feasible combinations of algorithms that we believe should
be the same, based on the API documentation. Overall, we identiﬁed three

6 Some examples from Kaggle.com: https://www.kaggle.com/general/253858 https:
https://www.kaggle.com/getting-started/

//www.kaggle.com/getting-started/96750
268971

7 https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms
8 For every rule, there is an exception, in this case the RidgeClassiﬁer from Scikit-learn,

which is a special case of the LogisticRegression.

Diﬀerential testing for machine learning

13

l

p
m
n
o
m

,
y
a
c
e
D
t
h
g
i
e

W
p
m

l

,

l

p
m

,

L
M
y
a
c
e
D
t
h
g
i
e

W
p
m

l

,
t
s
o
C
0
.
5
C

,
t
s
o
C
t
r
a
p
r

,
s
e
l
u
R
0
.
5
C

,
e
e
r
T
0
.
5
C

r
e
ﬁ

i
s
s
a
l
C
e
e
r
T
n
o
i
s
i
c
e
D

-
e
D

,
8
4
J

,
t
r
a
C
e
l
p
m
S

i

r
e
ﬁ
i
s
s
a
l
C
e
e
r
T
n
o
i
s
i
c
e
D

,
e
e
r
t
c

,
0
.
5
C

,
2
t
r
a
p
r

,

E
S
1
t
r
a
p
r

,
t
r
a
p
r

,
e
r
o
c
S
t
r
a
p
r

i

e
n
h
c
a
M

t
r
a
b

,
2
e
e
r
t
c

,

m
v
s
F
R
O

,
e
g
d
i
r
F
R
O

,
s
l
p
F
R
O

,
g
o
l
F
R
O

,
f
r

,
t
s
i
r
o
b
R

,
r
e
g
n
a
r

,

F
R
l
a
n
d
r
o

i

,
s
e
l
u
R
f
r

,
l
a
b
o
l
g
F
R
R

,

F
R
R

,
f
r
s
w

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

-
d
ﬀ
e
o
H

,

p
m
u
t
S
n
o
i
s
i
c

t
s
e
r
o
F
m
o
d
n
a
R

e
e
r
T
g
n

i

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

b
n
a
m

,

b
n

,
s
e
y
a
b

e
v
i
a
n

,

b
n
w
a

s
e
y
a
B
e
v
i
a
N

,
l
a
i
m
o
n
i
t
l

u
M

s
e
y
a
B
e
v
i
a
N

,

B
N
t
n
e
m
e
l
p
m
o
C

,

B
N

i
l
l

u
o
n
r
e
B

t
e
r
a
C

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

s
e
y
a
B
e
v
i
a
N

,

B
N
n
a
i
s
s
u
a
G

,

B
N
l
a
i
m
o
n
i
t
l
u
M

B
N
l
a
c
i
r
o
g
e
t
a
C

,
t
s
o
C
l
a
i
d
a
R
m
v
s

,
l
a
i
d
a
R
m
v
s

,
g
n
i
r
t
S
m
u
r
t
c
e
p
S
m
v
s

,
2
r
a
e
n
L
m
v
s

i

i

,
r
a
e
n
L
m
v
s

,
y
l
o
P
m
v
s

,
a
m
g
i
S
l
a
i
d
a
R
m
v
s

-
d
n
u
o
B
m
v
s

,
s
t
h
g
i
e

W

l
a
i
d
a
R
m
v
s

,
g
n
i
r
t
S
o
p
x
E
m
v
s

-

m
v
s
s
l

,
l
a
i
d
a
R
m
v
s
s
l

,
s
t
h
g
i
e

W

i

r
a
e
n
L
m
v
s

,
g
n
i
r
t
S
e
g
n
a
r

2
s
t
h
g
i
e

W

i

r
a
e
n
L
m
v
s

,
3
r
a
e
n
L
m
v
s

i

i

,
r
a
e
n
L
m
v
s
s
l

,
y
l
o
P

t
s
e
r
o
f
c

C
V
S
r
a
e
n
L

i

O
M
S

C
V
S

,

C
V
S
u
N

,

C
V
S
r
a
e
n
L

i

,

L
M
p
m

l

,

D
G
S
p
m

l

,
t
u
o
p
o
r
D
s
a
r
e
K
p
m

l

,
t
s
o
C
t
u
o
p
o
r
D

r
e
ﬁ
i
s
s
a
l
C

-
s
a
r
e
K
p
m

l

,
y
a
c
e
D
s
a
r
e
K
p
m

l

,
t
s
o
C
y
a
c
e
D
s
a
r
e
K
p
m

l

-
n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

r
e
ﬁ
i
s
s
a
l
C
P
L
M

-
e
P

,
a
d
r

,
a
d
l
r

,
a
d
n
L

i

,
a
d
l
r
r

,
a
d
s

,

A
D
L
e
s
r
a
p
s

,
a
d
l
s

,
a
d

l

,

A
D
L
p
e
t
s

,
a
d
l
c
o
l

,
a
d
M

l

,
2
a
d
p

,
a
d
p

,

A
D
L
d
e
z
i
l
a
n

i

r
a
e
n
L
r
p
s
s
u
a
g

,
y
l
o
P
r
p
s
s
u
a
g

,
l
a
i
d
a
R
r
p
s
s
u
a
g

A
D
Q
p
e
t
s

,
v
o
C
a
d
Q

,
a
d
q

a
d
d

l

,
a
d
F
R

,
2
a
d

l

m
a
p

s
e
e
r
T
a
r
t
x
e

T
M
L

n
n
k

,

n
n
k
k

,

n
n
s

m
b
g

,
o
2
h
m
b
g

l
l

u
n

r
e
ﬁ
i
s
s
a
l
C
T
B
G

n
o
r
t
p
e
c
r
e
P
d
e
t
o
V

D
G
S

R
o
r
e
Z

k
B
I

r
e
ﬁ
i
s
s
a
l
C
g
n
i
t
s
o
o
B
t
n
e
i
d
a
r
G

r
e
ﬁ
i
s
s
a
l
C
s
r
o
b
h
g
i
e
N
K

r
e
ﬁ

i
s
s
a
l
C
y
m
m
u
D

r
e
ﬁ
i
s
s
a
l
C
D
G
S

n
o
r
t
p
e
c
r
e
P

s
i
s
y
l
a
n
A
t
n
a
n
m

i

i
r
c
s
i
D
r
a
e
n
L

i

r
e
ﬁ
i
s
s
a
l
C
s
s
e
c
o
r
P
n
a
i
s
s
u
a
G

T
M
L

s
i
s
y
l
a
n
A
t
n
a
n
m

i

i
r
t
s
i
D
c
i
t
a
r
d
a
u
Q

r
e
ﬁ
i
s
s
a
l
C
e
e
r
T
a
r
t
x
E

d
i
o
r
t
n
e
C
t
s
e
r
a
e
N

r
l
o
p

,
r
l
p

,
c
i
t
s
i
g
o
L
g
e
r

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

c
i
t
s
i
g
o
L
e
l
p
m
S

i

,
c
i
t
s
i
g
o
L

r
e
ﬁ
i
s
s
a
l
C
e
g
d
R

i

,

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

Table 2: Overlapping algorithms between frameworks based on a scan of al-
gorithm names in the API docs. For Scikit-learn, Weka, and Spark MLlib we
report the names of the classes, in which the algorithms are implemented. For
Caret, we report the name of the method within Caret.

14

Steﬀen Herbold, Steﬀen Tunkel

variants of Naive Bayes (GNB, KDENB, MNB), two variants of the random
forest (RF1 with ﬁxed depth and RF2 without ﬁxed depth), three variants
of Support Vector Machines (LSVM with a linear kernel, PSVM with a poly-
nomial kernel, and RBFSVM with a RBF kernel), the Multilayer Perceptron
(MLP), the trivial classiﬁer (DUMMY), the k-Nearest Neighbor (KNN) algo-
rithm, and three variants of Logistic Regression (LR without regularization,
RIDGE with ridge regularization and LASSO with lasso regularization).

This means we found feasible combinations for all candidates with imple-
mentations in at least three of the four frameworks, with the exception of
the gradient boosting trees. The reason we failed here is because the Spark
MLlib API does not specify which kind of decision tree is implemented. Even
beyond this, while we identiﬁed many diﬀerent decision trees when analyzing
the potential, we could not ﬁnd any feasible combination of implementations.
For CART this almost worked, but was not possible because Caret did not
allow passing the appropriate parameters to the underlying library. Another
issue was the options that were conﬁgurable through hyperparameters (incl.
defaults) and hard coded values. In all cases, we needed to carefully consider
the hyperparameters to set them to appropriate values to behave the same
as with the other frameworks. There was no case, where the out-of-the box
default parameters between two implementations were equal for all implemen-
tations. As part of this analysis, we tried to identify hyperparameters, that
can be set with all implementations and that should have the same eﬀect. We
tried to maximize the number of such equal hyperparameters, but often only
found few, if any. In case the hyperparameters were not available in all frame-
works, we tried to determine which value was used by the implementation from
the API. Often, this was impossible because such implicit assumptions on the
algorithm conﬁguration that are not exposed to users through the API as con-
ﬁgurable hyperparameters are also not documented. Overall, this means that
the feasible subsets do not cover the full capabilities of the frameworks, i.e, it
is not possible for any of the algorithms to test the full set of hyperparameters
within our case study.

A notable aspect of ﬁnding feasible combinations is that a good knowledge
about the algorithms is required. For example, the diﬀerent implementations
of the RIDGE group use diﬀerent variants to deﬁne the strength of the regu-
larization. While the Scikit-learn implementation LogisticRegression uses the
inverse regularization strength 1
2·α , all other implementations, including the
Scikit-learn implementation RidgeClassiﬁer directly use α as regularization
strength. Without understanding the underlying mathematical principles, de-
riving suitable tests is not possible. However, we do not believe that this is, in
practice, a restriction on the feasibility of such tests: developers who actively
work on such algorithms, either through quality assurance or development,
should have the required background on the techniques anyways.

The potential for diﬀerential testing can often be operationalized in form
of feasible combinations that should yield the same behaviour. However,

Diﬀerential testing for machine learning

15

Fig. 3: Diﬀerences observed for pairs of libraries. The labels show the pair of
libraries and the number of tests executed. For example, “CARET, SKLEARN
(160)” means that we ran 160 tests where one classiﬁer was from either frame-
work.

such tests can only cover a subset of the implemented functionality, be-
cause of diﬀerences between the conﬁguration options that are exposed
as hyperparameters.

4.5 Phase 3: Test Execution

We executed the tests for all pairs of algorithms we identiﬁed in the second
phase, i.e., 87 pairs of algorithms with the parameters deﬁned in Table 6 in the
appendix on the four data sets with both training and test data for evaluation,
i.e., we have 87 pairs · 4 data sets · 2 training/test data = 696 comparisons. We
observed diﬀerence between the classiﬁcations for 457 pairs (65.6%), 67 (9.6%)
diﬀerences were signiﬁcant. Since some algorithms cannot compute scores,9 we
only have 432 pairs with scores. We observed diﬀerences between the scores for
313 pairs (72.4%), 141 (32.6%) were signiﬁcant. For better insights, we looked
at the results for each group of algorithms in detail. Table 4 summarizes the
results for each group. We refer to the algorithms by their framework. In case
there are multiple implementations per framework, we also provide the name
of the implementations.

9 All algorithms in the groups LSVM, PSVM, RBFSVM, the algorithm ranger from the

RF1 and RF2 groups, and the algorithm RidgeRegression from the RIDGE group.

0.00.20.40.60.81.0Percentage of testsCARET, CARET    (80)CARET, SKLEARN   (160)CARET, SPARK    (72)CARET, WEKA   (152)SKLEARN, SKLEARN     (8)SKLEARN, SPARK    (64)SKLEARN, WEKA   (112)SPARK, WEKA    (48)FrameworksTests with deviations in classes by library0.00.20.40.60.81.0Percentage of testsTests with significant 2-test results by library0.00.20.40.60.81.0Percentage of testsCARET, CARET (32)CARET, SKLEARN (88)CARET, SPARK (48)CARET, WEKA (96)SKLEARN, SPARK (48)SKLEARN, WEKA (80)SPARK, WEKA (40)FrameworksTests deviations in scores by library0.00.20.40.60.81.0Percentage of testsTests with significant Kolmogorov-Smirnoff-testresults by library16

Steﬀen Herbold, Steﬀen Tunkel

b
n

,
s
e
y
a
b

e
v
i
a
n

b
n

,
s
e
y
a
b

e
v
i
a
n

t
e
r
a
C

f
r

,
t
s
i
r
o
b
r

,
r
e
g
n
a
r

t
s
i
r
o
b
r

,
r
e
g
n
a
r

l
l

u
n

n
n
k

l
a
i
d
a
R
m
v
s

y
l
o
P
m
v
s

3
r
a
e
n
L
m
v
s

i

,
2
r
a
e
n
L
m
v
s

i

i

,
r
a
e
n
L
m
v
s

C
V
S
r
a
e
n
L

i

O
M
S

O
M
S

O
M
S

s
e
y
a
B
e
v
i
a
N

l
a
i
m
o
n
i
t
l
u
M

s
e
y
a
B
e
v
i
a
N

B
N
l
a
i
m
o
n
i
t
l
u
M

t
s
e
r
o
F
m
o
d
n
a
R

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

t
s
e
r
o
F
m
o
d
n
a
R

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

s
e
y
a
B
e
v
i
a
N

s
e
y
a
B
e
v
i
a
N

b
i
l

L
M
k
r
a
p
S

s
e
y
a
B
e
v
i
a
N

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

p
u
o
r
G

B
N
n
a
i
s
s
u
a
G

B
N
G

C
V
S

C
V
S

C
V
S

M
V
S
F
B
R

B
N
E
D
K

B
N
M

1
F
R

2
F
R

M
V
S
L

M
V
S
P

c
i
t
s
i
g
o
L
g
e
r

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

r
l
o
p

,
r
l
p

,
c
i
t
s
i
g
o
L
g
e
r

r
l
p

,
c
i
t
s
i
g
o
L
g
e
r

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

c
i
t
s
i
g
o
L

c
i
t
s
i
g
o
L

R
o
r
e
Z

r
e
ﬁ
i
s
s
a
l
C
y
m
m
u
D

Y
M
M
U
D

k
B
I

r
e
ﬁ
i
s
s
a
l
C
s
r
o
b
h
g
i
e
N
K

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

N
N
K

R
L

r
e
ﬁ
i
s
s
a
l
C
e
g
d
R

i

,

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

E
G
D
R

I

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

O
S
S
A
L

*
D
G
S
p
m

l

,
*
p
m

l

r
e
ﬁ
i
s
s
a
l
C
n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

r
e
ﬁ
i
s
s
a
l
C
P
L
M

P
L
M

Table 3: Feasible combinations with at least three implementations. Param-
eters are ommited and can be found in Table 6 in the Appendix. The mlp
classiﬁers of Caret could not be executed because they require an R package
which is not on CRAN anymore.

Diﬀerential testing for machine learning

17

Group

Summary of test results

GNB

KDENB

MNB

RF1

RF2

Classes are equal or almost equal, with small diﬀerences for Weka. Scores
are almost always diﬀerent, except for the Caret-naive bayes and Caret-
nb, and the Spark MLlib and Scikit-learn implementations, which are
equal. No diﬀerences are signiﬁcant.

Small but insigniﬁcant diﬀerences in classes and scores between the Caret-
naive bayes and Caret-nb. Large diﬀerences to Weka, that are signiﬁcant
for both classes and scores on about half of the data sets.

Classes and scores are equal, with one exception: the Scikit-learn imple-
mentation sometimes classiﬁes one instance diﬀerently. These diﬀerences
are in cases where the score is almost exactly 0.5 and for one framework
slightly smaller and for the other framework slightly larger than 0.5, e.g.,
for Scikit-learn 0.499 and for Weka 0.501.

Results are never equal, with larger diﬀerences on test than on training
data. The diﬀerences between classes are not signiﬁcant, except in two
cases on the RANDOM data, where the classes are signiﬁcantly diﬀerent.
The diﬀerences between the scores are almost always signiﬁcant.

Results are never equal, with larger diﬀerences on test than on training
data. The diﬀerences of the classes are not signiﬁcant, except in two cases
on the RANDOM data. The scores between Weka and Caret-rboist is
only signiﬁcant once on the WINE data. The scores of Scikit-learn are
signiﬁcantly diﬀerent most of the time.

LSVM

Results are never equal, with mostly small and insigniﬁcant diﬀerences.
The exception is Spark MLlib, which has large diﬀerence to the other
implementations that are signiﬁcant on the RANDOM and WINE data.

PSVM

Scikit-learn and Weka are equal. Caret yields diﬀerent results, but these
diﬀerences are only signiﬁcant on the UNIFORM data.

RBFSVM Scikit-learn and Weka are equal. Caret yields diﬀerent results and these

diﬀerences are almost always signiﬁcant.

MLP

On the RANDOM and UNIFORM data, the classes are almost equal be-
tween all implementations with no signiﬁcant diﬀerence. On the WINE
and BC data there are large and signiﬁcant diﬀerences, where Weka dis-
agrees with the other frameworks. The diﬀerences between scores are al-
ways large and signiﬁcant.

DUMMY The classes are always equal, the scores depend on the implementation of
the trivial model: Caret and Weka have the same approach, Scikit-learn
always disagrees.

KNN

LR

RIDGE

LASSO

The classiﬁcations are equal or almost equal between all implementations.
The scores of Scikit-learn and Caret are also equal or almost equal, with
the exception of the WINE data. Here, Caret is equal to Weka instead.
On the other data sets, Weka has a large and signiﬁcant diﬀerence from
the other implementations.

The classes are equal or almost equal. The scores are almost always equal,
except on the BC data, where we observe signiﬁcant diﬀerences between
all implementations.

The classes and scores of Weka, Scikit-learn-LogisticRegression and Caret-
plr are equal. The other implementations have small and insigniﬁcant dif-
ferences for the classes and large and signiﬁcant deviations for the scores.

The classiﬁcations of Caret and Scikit-learn are equal or almost equal.
Spark MLlib has large diﬀerences, but they are only signiﬁcant on the
RANDOM data. The scores between all implementations are diﬀerent.
The diﬀerences between Caret and Scikit-learn are mostly not signiﬁcant,
Spark MLlib is always signiﬁcantly diﬀerent.

Table 4: Overview of the results of the execution of the diﬀerential tests.

18

Steﬀen Herbold, Steﬀen Tunkel

Fig. 4: Diﬀerences observed with the data sets we used.

Overall, these results paint a complex picture regarding the eﬀectiveness.
For example, we found no patterns regarding which frameworks often disagree
with each other, except for a tendency that the deviations between diﬀerent
implementations wrapped by Caret are mostly not signiﬁcant (Figure 3). We
also found no pattern regarding which data sets are responsible for the de-
viations (Figure 4). Moreover, there is no group of implementations in which
both classes and scores are equal. We observe so many absolute diﬀerences that
we cannot really conclude anything regarding concrete bugs. Nevertheless, we
found a couple of repeating aspects.

– Perhaps unsurprisingly, non-deterministic algorithms like random forests
or MLPs can only be compared using statistical signiﬁcance. This seems
to work well for the classiﬁcations using the χ2 test. However, our results
indicate that the scores should not be compared, as the diﬀerences are very
often signiﬁcant.

– Perhaps surprisingly, we often observe diﬀerent classiﬁcations with deter-
ministic algorithms, even to the point where we cannot state which algo-
rithms are likely correct. For example, the small diﬀerences for GNB makes
it impossible to determine, without a detailed triage of all involved algo-
rithms, to infer if any of the implementations contain a bug causing the
small deviations, or if this is natural due to implementation choices. Most
of the time these diﬀerences are not statistically signiﬁcant, which further
raises the question if investigating these issues would be worth the eﬀort.

0.00.20.40.60.81.0Percentage of testsBCRANDOMUNIFORMWINEData setTests with deviations in classes by data set0.00.20.40.60.81.0Percentage of testsData setTests with significant 2-test results by data set0.00.20.40.60.81.0Percentage of testsBCRANDOMUNIFORMWINEData setTests with deviations in scores by data set0.00.20.40.60.81.0Percentage of testsData setTests with significant Kolmogorov-Smirnoff-testresults by data setDiﬀerential testing for machine learning

19

– Scores often depend on the implementation. Even for a “simple” algorithm
like LR, we observe signiﬁcant diﬀerences. Thus, we cannot recommend
to use diﬀerential testing for scores, as there is a very large risk of false
positives.

We also note that our choice to use a threshold for the p-value without
correction is not without consequences. For the Kolmogorov-Smirnoﬀ tests
between the scores, we have over 32% results in which we observe signiﬁcant,
i.e., far more than the expected 5% of false positives. Since there are too many
diﬀerences between scores anyways, the impact of possible false positives is
limited. For the Chi-Squared, we have only about 10% signiﬁcant diﬀerence.
Consequently, the randomness of the p-value distribution if the null hypothesis
is true would explain about half of the signiﬁcant diﬀerences we observe. This
adds another wrinkle to our already hard to interpret results. However, we
would argue that for deterministic algorithms, only very small diﬀerences are
acceptable, and anything large enough to be picked up as a signiﬁcant signal
by the Chi-Squared test actually points towards a signiﬁcant diﬀerence be-
tween deterministic implementations. For randomized algorithms, a potential
solution would be to use repeated tests, e.g., 100 diﬀerent UNIFORM samples.
One could then observe if the number of signiﬁcant deviations is less than the
expected false positive rate of 5%, which would indicate acceptable diﬀerences,
or if it is large which would indicate a signiﬁcant diﬀerence.

Due to these issues, we believe that diﬀerential testing may be used by
developers who already know the details of their implementation, to compare
their work with other implementations. However, whether this is more eﬀective
and/or eﬃcient than directly only debugging a single implementation is not
clear without further study. Using diﬀerential testing as pseudo-oracle (Davis
and Weyuker, 1981) to automatically drive test campaigns between tools seems
to have only a limited potential.

The results of the tests indicate that an eﬀective and eﬃcient usage of
diﬀerential testing between implementations is mostly not possible on an
absolute level, i.e., expecting equal results. Tests for signiﬁcant diﬀerences
of classiﬁcations seem to be the only feasible option, but should only be
used with proper care regarding false positive and require that tester are
already experts for the implementations under test.

5 Discussion

Overall, our results show that while we have a large potential, it is very dif-
ﬁcult to ﬁnd feasible conﬁgurations of the algorithms. When we then execute
these tests, we observe that there are many diﬀerences, to the point where we
cannot distinguish between possible problems and normal deviations. We saw
that especially the scores depend a lot on the implementation. Even if we just
consider whether the binary classiﬁcations are equal, we observed too many
diﬀerences. There is no reason to believe that this problem should not exist

20

Steﬀen Herbold, Steﬀen Tunkel

for more classes as well. While we expected that there would be some diﬀer-
ences between the frameworks, the magnitude of these diﬀerences surprised
us. We note that the data sets we use are neither very large, nor known to be
numerically challenging.

Consequently, we believe that diﬀerential testing is just not possible be-
tween frameworks, unless a very lenient test oracle is used that avoids the
large amount of false positives, e.g., using the Chi-Squared tests. However,
this comes at the cost of a larger chance of false negatives as well, because
unexpected small diﬀerences between classes and any kind of diﬀerence be-
tween scores cannot be detected. The cases where this would also be possible
are rather the exception then the rule. We can only speculate regarding the
reasons for this. However, we believe that there are four main drivers for the
diﬀerences we observe:

– The deﬁnition of the algorithms in the literature may not be suﬃciently
detailed to guarantee that everything is implemented in exactly the same
way. While we believe that this could sometimes be a reason, this cannot
explain all diﬀerences we observe. For example, LSVM have a clear mathe-
matical deﬁnition which is well known and should lead to the same optimal
result, except in few corner cases. In comparison, no two implementations
yield exactly the same results.

– It is possible that the API documentations are incomplete and there are
hidden values of hyperparameter that are hard-coded in the implementa-
tions but not visible through the documentation. However, in this case we
should see clearer patterns in the deviations, e.g., one algorithm deviating
from the others on all data sets. While we sometimes observe this, there are
relatively few cases were only one implementation disagrees with all others.
Usually, many implementations disagree with each other within one group.
– Optimizations for the numerical stability, run-time, or memory eﬃciency
could lead to subtle changes in the algorithms which could explain the
deviations between the implementations. The pattern of deviations should
be the same as before, i.e., one algorithm disagrees with the others, which
we often do not observe very often.

– The diﬀerent programming languages, including the libraries available,
could also have an impact. While, e.g., the diﬀerent pairs for GNB in-
dicate that this may be due to such an eﬀect, we also observe many cases
that indicate that this is not an issue. For example, the diﬀerent R imple-
mentations that Caret oﬀers often disagree with each other as well, even
though they are, presumably, based on the same native R features. Hence,
while this could be a factor, we believe that this only plays a minor role.

Unfortunately, we could only determine how often the above factors are the
reason for diﬀerences through an extremely time-consuming manual analysis
of the source code of the machine learning libraries, possibly even including
libraries on which they are based, e.g., to understand the numerics involved.
Such a large scale comparative audit is not possible with our resources, because

Diﬀerential testing for machine learning

21

we would basically need to achieve similar skills as the core developers for each
of the four frameworks.

Regardless of our lack of evidence, we believe that all of the above rea-
sons provide a reasonable explanation for the results of our experiments. We
believe that each of the reasons is a driver for some of the cases in which
we observe diﬀerences, especially those where we do not observe signiﬁcant
diﬀerences. Unfortunately, this also means that the diﬀerential testing is not
eﬀective as automated oracle, because we would need to invest a huge amount
of eﬀort to conduct in-depth analysis of diﬀerences between most pairs of algo-
rithms, which is not feasible. Consequently, this shows the limited capability
of diﬀerential testing to provide an exact “speciﬁcation” through a second
implementation that provides a reliable oracle for deciding the correctness of
software tests.

We also believe that our results are not contradicting that diﬀerential test-
ing works well with deep learning frameworks, e.g., PyTorch (Paszke et al.,
2019) and TensorFlow (Abadi et al., 2015): exact equality is not expected, due
to the randomized nature of the training, equality of scores is also not expected
because the decision boundaries of neural networks can also easily slightly
change with diﬀerent initialization. Thus, there is a built-in uncertainty of the
training of neural networks which means that the diﬀerential testing is only
concerned with checking if the classiﬁcation stays roughly the same, which we
also found to work. While these diﬀerences might be made slightly worse by
the implementation diﬀerences or similar as we describe above, they do not
hinder the diﬀerential testing, because exact equality is not the expectation,
anyways.

This is also in line with the fairly randomized algorithms we have within
our data set, i.e., the random forest and the MLP. Thus, one can argue that the
diﬀerential testing works beyond deep learning, if the algorithms are random-
ized and the only expectation is that the diﬀerences between classiﬁcations
are not signiﬁcant. However, subtle bugs that only aﬀect a small amount of
data can likely not be detected this way. For the algorithms in our data that
are deterministic and often implement a formula or optimization approach, we
should be able to observe a stronger type of equality, i.e., exactly the same
results or even scores, which is not the case due to the reasons listed above,
which in the end mean that the speciﬁcation of the algorithms are imprecise.
It follows that diﬀerential testing requires either a relaxed view on the ex-
pected equality or stronger speciﬁcations of the behavior of machine learning
algorithms. Consequently, we answer our research question as follows:

While there is a large potential and it is possible to derive feasible tests,
exhaustive testing of all parameter combinations or algorithms is not
possible. More importantly, due to many false positive diﬀerences, the
diﬀerential testing does not lead to a suitable automated test oracle if
exact or almost exact equality is expected. For randomized algorithms,

22

Steﬀen Herbold, Steﬀen Tunkel

where some diﬀerences are expected anyways, diﬀerential testing can still
be eﬀective, but only if bugs cause large diﬀerences.

6 Threats to validity

We report the threats to the validity of our work following the classiﬁcation
by Cook et al. (1979) suggested for software engineering by Wohlin et al.
(2012). Additionally, we discuss the reliability as suggested by Runeson and
H¨ost (2009).

6.1 Construct Validity

There are crucial design choices in each phase of of our case study that could
aﬀect our results. We describe these threats below, including why the believe
that the alternatives would not lead to a better construct.

The construct of the ﬁrst phase of our study is based on a coarse grouping
of algorithms by the general approach, e.g., grouping decision trees together.
While we have a good reason for this, i.e., that some algorithms have diﬀerent
variants in one implementation, while other have these variants over multiple
implementation, this construct may lead to an overestimation of the potential.
An indicator that this happens to some degree are the decision trees, where we
have a huge group for the potential, but no feasible variants. The alternative
design choice would have been a more ﬁne-grained grouping (i.e., by concrete
algorithm variant) and then possibly repeating the same implementation mul-
tiple times. However, we believe that this option has one major drawback, i.e.,
it would lead to merging aspects of the second phase of our study with the
ﬁrst phase. The more ﬁne-grained grouping is only possibly by studying the
hyperparameters through the API documentation, which is precisely what we
do in the second phase, where we not only achieve a more ﬁne-grained group-
ing that accounts for algorithm variants, but at the same time check if the
variants are suﬃciently similar to provide equal sets of hyper parameters.

The construct of the second phase of our study is based on studying the
API documentation of algorithms to ﬁnd equal conﬁgurations. While we be-
lieve that this is the only practically feasible choice, there is a certain risk
involved with relying on the documentation. If the documentation does not
document all parameters or fails to describe all relevant aspects, we may miss
feasible candidates. The alternative would be to directly check the source code
to understand all options (incl. hard-coded choices). However, this would es-
sentially be the same as an in-depth code review of the implementations under
test, which we believe is not realistic, neither for us as researchers as part of
a study, nor for practitioners as part of a test campaign.

The construct of the third phase of our study is based on four diﬀerent
comparisons that consider equality of scores and classes, as well as signiﬁ-
cance of diﬀerences. While we believe that these are the reasonable criteria to

Diﬀerential testing for machine learning

23

determine if a diﬀerential test fails, other options might also work, e.g., con-
sidering the mean absolute deviation of scores. However, such an approach has
the drawback that at threshold is required that deﬁnes how much deviation is
acceptable. We believe that our approach based on statistical tests achieves a
similar result, but with a stronger mathematical foundation. Another poten-
tial issue with our construct is that implementations may also randomly lead
to unstable results, even with the same data and hyperparameters (including
a ﬁxed random seed). To account for this, we asserted that the results remain
the same with ten randomly generated data sets for all algorithms. We did not
observe any deviations between the classes and the scores.

6.2 Internal Validity

We do not believe that there are threats to our conclusions regarding the poten-
tial, feasibility, or eﬀectiveness, because these conclusions are directly derived
from our quantitative evidence. However, our discussion regarding potential
reasons is not derived from this evidence, but rather from our knowledge about
software development and machine learning. Consequently, we may overesti-
mate or underestimate and of the issues we discuss as possible reasons in
Section 5 and also miss other reasons. However, we clearly state this limita-
tion to be transparent to readers of this article. The only feasible alternative
would be not discuss the reasons for our results at all, which we believe would
be a larger problem than possible wrong speculative reasoning, if this is clearly
marked as such.

6.3 External Validity

While we consider four machine learning libraries in our study, it is unclear
if the results would hold for other libraries as well or if a larger eﬀectiveness
with more feasible combinations would be possible. However, we do not believe
that this is likely, because we already considered large and mature libraries.
Additionally, caret is a meta framework that means we actually included many
diﬀerent R libraries in our study.

We also cannot be certain how our results generalize to other types of
machine learning, e.g., clustering or regression. However, we note that these
algorithms are similar in their structure to classiﬁcation algorithms. For exam-
ple, we see no reason why there should be notable diﬀerences in the testability
between a random forest regression and a random forest classiﬁer or between a
k-Nearest Neighbor classiﬁer and a k-Means clustering. Thus, we believe that
our results provide a strong indication for the expectations beyond classiﬁca-
tion, i.e., that exactly the same results usually cannot be expected but a more
coarse comparison could be possible.

24

6.4 Reliability

Steﬀen Herbold, Steﬀen Tunkel

A signiﬁcant amount of our work was the manual analysis of the APIs of four
machine learning frameworks we studied carried out together by the authors.
We cannot rule out the possibility that we missed algorithms (phase 1) or
feasible combinations (phase 2). We believe that the threat for the ﬁrst phase
is negligible, because all the API documentation of all framework speciﬁcally
contains categories for supervised and/or classiﬁcation algorithms. For the
second phase, we read all API docs and all parameters as pair to minimize the
likelihood of missing aspects. Moreover, the results of the second phase were
validated by implementing the feasible combinations later on, which required
us to double check all results. Thus, while we may have missed something, we
do not think that it is likely that we missed so many algorithms or feasible
combinations that this would have an eﬀect on our conclusions.

7 Conclusion

Within this paper, we evaluate the potential, feasibility, and eﬀectiveness of
diﬀerential testing for classiﬁcation algorithms beyond deep learning. While
we found that there is a huge potential, especially for popular algorithms, we
found that it was already diﬃcult to identify feasible tests such that the API
documentation indicated that the behavior of two implementations should be
the same. When we then executed the feasible tests, we found so many dif-
ferences between the results, that we cannot ﬁnd a signal in the noise, i.e.,
identify true positive diﬀerences that indicate actual bugs in an implementa-
tions among all the false positive tests results with diﬀerences due to other
reasons. However, our results indicate that for experts it may still be possible
to use a relatively lenient approach based on signiﬁcant diﬀerences between
classiﬁcation results to determine if there are bugs within a software. However,
the amount of false positive seems to be too large to be useful for researchers
as automated pseudo-oracle to evaluate the eﬀectiveness of testing approach
for such algorithms.

References

Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS,
Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving
G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Man´e
D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner
B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Vi´egas
F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X (2015)
TensorFlow: Large-scale machine learning on heterogeneous systems. URL
https://www.tensorflow.org/, software available from tensorﬂow.org
Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat
S, Irving G, Isard M, Kudlur M, Levenberg J, Monga R, Moore S, Murray

Diﬀerential testing for machine learning

25

DG, Steiner B, Tucker P, Vasudevan V, Warden P, Wicke M, Yu Y, Zheng
X (2016) Tensorﬂow: A system for large-scale machine learning. In: Pro-
ceedings of the 12th USENIX Conference on Operating Systems Design and
Implementation, USENIX Association, Berkeley, CA, USA, OSDI’16, pp
265–283, URL http://dl.acm.org/citation.cfm?id=3026877.3026899
Asyroﬁ MH, Thung F, Lo D, Jiang L (2020) Crossasr: Eﬃcient diﬀerential
testing of automatic speech recognition via text-to-speech. In: 2020 IEEE
International Conference on Software Maintenance and Evolution (ICSME),
pp 640–650, DOI 10.1109/ICSME46990.2020.00066

Barash G, Farchi E, Jayaraman I, Raz O, Tzoref-Brill R, Zalmanovici M (2019)
Bridging the gap between ml solutions and their business requirements us-
ing feature interactions. In: Proceedings of the 2019 27th ACM Joint Meet-
ing on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, Association for Computing Machin-
ery, New York, NY, USA, ESEC/FSE 2019, p 1048–1058, DOI 10.1145/
3338906.3340442, URL https://doi.org/10.1145/3338906.3340442
Braiek HB, Khomh F (2020) On testing machine learning programs. Jour-
nal of Systems and Software 164:110542, DOI https://doi.org/10.1016/j.jss.
2020.110542, URL http://www.sciencedirect.com/science/article/
pii/S0164121220300248

Breiman L (1996) Bagging predictors. Mach Learn 24(2):123–140, DOI 10.

1023/A:1018054314350

Breiman L (2001) Machine Learning

45(1):5–32, DOI

10.1023/a:

1010933404324, URL https://doi.org/10.1023/a:1010933404324

Brieman L, Friedman J, Olshen R, Stone C (1984) Classiﬁcation and regression

trees. Belmont (CA): Wadsworth

Cook TD, Campbell DT, Day A (1979) Quasi-experimentation: Design & anal-

ysis issues for ﬁeld settings, vol 351. Houghton Miﬄin Boston

Davis MD, Weyuker EJ (1981) Pseudo-oracles for non-testable programs. In:
Proceedings of the ACM ’81 Conference, ACM, New York, NY, USA, ACM
’81, pp 254–257, DOI 10.1145/800175.809889, URL http://doi.acm.org/
10.1145/800175.809889

Ding J, Kang X, Hu XH (2017) Validating a deep learning framework by
metamorphic testing. In: 2017 IEEE/ACM 2nd International Workshop on
Metamorphic Testing (MET), pp 28–34, DOI 10.1109/MET.2017.2

Frank E, Hall MA, Witten IH (2016) The WEKA Workbench. Online Ap-
pendix for ”Data Mining: Practical Machine Learning Tools and Techniques.
Morgan Kaufmann

Freund Y, Schapire RE (1997) A decision-theoretic generalization of on-
line learning and an application to boosting. Journal of Computer
and System Sciences 55(1):119 – 139, DOI http://dx.doi.org/10.1006/
jcss.1997.1504, URL http://www.sciencedirect.com/science/article/
pii/S002200009791504X

Giray G (2021) A software engineering perspective on engineering ma-
chine learning systems: State of the art and challenges. Journal of Sys-
tems and Software 180:111031, DOI https://doi.org/10.1016/j.jss.2021.

26

Steﬀen Herbold, Steﬀen Tunkel

111031, URL https://www.sciencedirect.com/science/article/pii/
S016412122100128X

Groce A, Kulesza T, Zhang C, Shamasunder S, Burnett M, Wong WK, Stumpf
S, Das S, Shinsel A, Bice F, McIntosh K (2014) You are the only possible
oracle: Eﬀective test selection for end users of interactive machine learning
systems. IEEE Trans Softw Eng 40(3):307–323, DOI 10.1109/TSE.2013.59,
URL http://dx.doi.org/10.1109/TSE.2013.59

Gross P, Boulanger A, Arias M, Waltz D, Long PM, Lawson C, Anderson
R, Koenig M, Mastrocinque M, Fairechio W, Johnson JA, Lee S, Doherty
F, Kressner A (2006) Predicting electricity distribution feeder failures using
machine learning susceptibility analysis. In: Proceedings of the 18th Confer-
ence on Innovative Applications of Artiﬁcial Intelligence - Volume 2, AAAI
Press, IAAI’06, pp 1705–1711, URL http://dl.acm.org/citation.cfm?
id=1597122.1597127

Guo J, Zhao Y, Song H, Jiang Y (2021) Coverage guided diﬀerential adversarial
testing of deep learning systems. IEEE Transactions on Network Science and
Engineering 8(2):933–942, DOI 10.1109/TNSE.2020.2997359

Guo Q, Xie X, Li Y, Zhang X, Liu Y, Li X, Shen C (2020) Audee: Automated
testing for deep learning frameworks. In: 2020 35th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE), pp 486–498
Herbold S, Haar T (2022) Smoke testing for machine learning: sim-
ple tests
severe bugs. Empirical Software Engineering
27(2), DOI 10.1007/s10664-021-10073-7, URL https://doi.org/10.1007/
s10664-021-10073-7

to discover

Karpathy A (2018) Cs231n: Convolutional neural networks for visual recogni-

tion. URL https://cs231n.github.io/neural-networks-3/

Kuhn M (2018) caret: Classiﬁcation and Regression Training. URL https:

//CRAN.R-project.org/package=caret, r package version 6.0-80

Marijan D, Gotlieb A (2020) Software testing for machine learning. In: Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 34, pp 13576–
13582

Mart´ınez-Fern´andez S, Bogner J, Franch X, Oriol M, Siebert J, Trendowicz A,
Vollmer AM, Wagner S (2021) Software engineering for ai-based systems: A
survey. ACM Transaction on Software Engineering and Methodology

McCullough BD, Mokﬁ T, Almaeenejad M (2019) On the accuracy of linear
regression routines in some data mining packages. WIREs Data Mining and
Knowledge Discovery 9(3):e1279, DOI https://doi.org/10.1002/widm.1279,
URL
https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/
widm.1279,
https://wires.onlinelibrary.wiley.com/doi/pdf/10.
1002/widm.1279

Meng X, Bradley J, Yavuz B, Sparks E, Venkataraman S, Liu D, Freeman J,
Tsai D, Amde M, Owen S, et al. (2016) Mllib: Machine learning in apache
spark. The Journal of Machine Learning Research 17(1):1235–1241

Murphy C, Kaiser GE, Arias M (2007) An approach to software testing of

machine learning applications. In: SEKE, vol 167

Diﬀerential testing for machine learning

27

Murphy C, Kaiser GE, Hu L, Wu L (2008) Properties of machine learning
applications for use in metamorphic testing. In: SEKE, vol 8, pp 867–872
Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z,
Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison
M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S (2019)
Pytorch: An imperative style, high-performance deep learning library. In:
Wallach H, Larochelle H, Beygelzimer A, d'Alch´e-Buc F, Fox E, Garnett
R (eds) Advances in Neural Information Processing Systems 32, Curran
Associates, Inc., pp 8024–8035, URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf

Patton MQ (2014) Qualitative research & evaluation methods: Integrating

theory and practice. Sage publications

Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blon-
del M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cour-
napeau D, Brucher M, Perrot M, Duchesnay E (2011) Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research 12:2825–2830
Pei K, Cao Y, Yang J, Jana S (2019) Deepxplore: Automated whitebox testing
of deep learning systems. Commun ACM 62(11):137–145, DOI 10.1145/
3361566, URL https://doi.org/10.1145/3361566

Pham HV, Lutellier T, Qi W, Tan L (2019) Cradle: Cross-backend validation
to detect and localize bugs in deep learning libraries. In: 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE), pp 1027–
1038, DOI 10.1109/ICSE.2019.00107

Quinlan JR (1986) Induction of decision trees. Machine learning 1(1):81–106
Quinlan JR (1993) C4.5: Programs for Machine Learning. Morgan Kaufmann

Publishers Inc., San Francisco, CA, USA

Runeson P, H¨ost M (2009) Guidelines for conducting and reporting case study
research in software engineering. Empirical Software Engineering 14(2):131–
164

Theano Development Team (2016) Theano: A Python framework for fast com-
putation of mathematical expressions. arXiv e-prints abs/1605.02688, URL
http://arxiv.org/abs/1605.02688

Wang Z, Yan M, Chen J, Liu S, Zhang D (2020) Deep learning library test-
ing via eﬀective model generation. In: Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, Association for Computing Ma-
chinery, New York, NY, USA, ESEC/FSE 2020, p 788–799, DOI 10.1145/
3368089.3409761, URL https://doi.org/10.1145/3368089.3409761
Wohlin C, Runeson P, H¨ost M, Ohlsson MC, Regnell B, Wesslen A (2012)
Experimentation in Software Engineering. Springer Publishing Company,
Incorporated

Xie X, Ho JW, Murphy C, Kaiser G, Xu B, Chen TY (2011) Testing
and validating machine learning classiﬁers by metamorphic testing. Jour-
nal of Systems and Software 84(4):544 – 558, DOI https://doi.org/10.
1016/j.jss.2010.11.920, URL http://www.sciencedirect.com/science/

28

Steﬀen Herbold, Steﬀen Tunkel

article/pii/S0164121210003213, the Ninth International Conference on
Quality Software

Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica I (2010) Spark:
cluster computing with working sets. In: Proceedings of the 2nd USENIX
Conference on Hot Topics in Cloud Computing (HotCloud)

Zhang JM, Harman M, Ma L, Liu Y (2020) Machine learning testing: Survey,
landscapes and horizons. IEEE Transactions on Software Engineering pp
1–1

A Additional Results

Within this appendix, we report additional results for all phases of the case study. Table 5
shows a complete list of algorithms from all frameworks and how they overlap. Table 6
shows how the hyperparameter must be conﬁgured for all feasible combinations we identiﬁed.
Table 7 provides additional details for the results of the execution of the diﬀerential tests.
Figure 3 shows how often we observed diﬀerences between implementations grouped by the
framework they were implemented in. Figure 4 shows how often we observed diﬀerences
grouped by the data set we used to execute the tests. Figure 5-18 show for each algorithm
we tested the distributions of ∆ and ∆score, normalized by the number of instances in the
data set n.

Diﬀerential testing for machine learning

29

l

p
m
n
o
m

,
y
a
c
e
D
t
h
g
i
e

W
p
m

l

,

l

p
m

,

L
M
y
a
c
e
D
t
h
g
i
e

W
p
m

l

,

L
M
p
m

l

,
t
s
o
C
t
r
a
p
r

,
s
e
l
u
R
0
.
5
C

,
e
e
r
T
0
.
5
C

,
t
r
a
p
r

,
e
e
r
t
c

,
e
r
o
c
S
t
r
a
p
r

,
0
.
5
C

,
2
t
r
a
p
r

,
t
s
o
C
0
.
5
C

,

E
S
1
t
r
a
p
r

i

e
n
h
c
a
M

t
r
a
b

,
2
e
e
r
t
c

,
s
e
l
u
R
f
r

,
l
a
b
o
l
g
F
R
R

,

F
R
R

,
f
r
s
w

,
f
r

,
t
s
i
r
o
b
R

,
r
e
g
n
a
r

,

F
R
l
a
n
d
r
o

i

,
e
g
d
i
r
F
R
O

,
s
l
p
F
R
O

,
g
o
l
F
R
O

t
s
e
r
o
f
c

,

m
v
s
F
R
O

,
l
a
i
d
a
R
m
v
s

,
g
n
i
r
t
S
m
u
r
t
c
e
p
S
m
v
s

,
a
m
g
i
S
l
a
i
d
a
R
m
v
s

,
t
s
o
C
l
a
i
d
a
R
m
v
s

,
2
r
a
e
n
L
m
v
s

i

i

,
r
a
e
n
L
m
v
s

,
y
l
o
P
m
v
s

-
l
a
i
d
a
R
m
v
s

,
g
n
i
r
t
S
o
p
x
E
m
v
s

,
g
n
i
r
t
S
e
g
n
a
r
d
n
u
o
B
m
v
s

,
s
t
h
g
i
e

W

-
a
R
m
v
s
s
l

,
s
t
h
g
i
e

W

i

r
a
e
n
L
m
v
s

i

,
r
a
e
n
L
m
v
s
s
l

,
y
l
o
P
m
v
s
s
l

,
l
a
i
d

2
s
t
h
g
i
e

W

i

r
a
e
n
L
m
v
s

,
3
r
a
e
n
L
m
v
s

i

-
s
a
r
e
K
p
m

l

,
t
s
o
C
y
a
c
e
D
s
a
r
e
K
p
m

l

,
t
s
o
C
t
u
o
p
o
r
D
s
a
r
e
K
p
m

l

,
y
a
c
e
D

,

D
G
S
p
m

l

,
t
u
o
p
o
r
D
s
a
r
e
K
p
m

l

r
e
ﬁ
i
s
s
a
l
C
e
e
r
T
n
o
i
s
i
c
e
D

,

p
m
u
t
S
n
o
i
s
i
c
e
D

,
8
4
J

,
t
r
a
C
e
l
p
m
S

i

e
e
r
T
g
n
d
ﬀ
e
o
H

i

r
e
ﬁ

i
s
s
a
l
C
e
e
r
T
n
o
i
s
i
c
e
D

B
N
l
a
c
i
r
o
g
e
t
a
C

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

t
s
e
r
o
F
m
o
d
n
a
R

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

C
V
S
r
a
e
n
L

i

O
M
S

C
V
S

,

C
V
S
u
N

,

C
V
S
r
a
e
n
L

i

r
e
ﬁ
i
s
s
a
l
C
n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

r
e
ﬁ
i
s
s
a
l
C
P
L
M

.
s
c
o
d

I
P
A
e
h
t

n
i

s
e
m
a
n
m
h
t
i
r
o
g
l
a

f
o

n
a
c
s

a

n
o

d
e
s
a
b

s
k
r
o
w
e
m
a
r
f

n
e
e
w
t
e
b

s

m
h
t
i
r
o
g
l
a

g
n

i

p
p
a
l
r
e
v
O

:
5

e
l

b
a
T

b
n
a
m

,

b
n

,
s
e
y
a
b

e
v
i
a
n

,

b
n
w
a

s
e
y
a
B
e
v
i
a
N

-
e
v
i
a
N

,
l
a
i
m
o
n
i
t
l

u
M

s
e
y
a
B
e
v
i
a
N

,

B
N
t
n
e
m
e
l
p
m
o
C

,

B
N

i
l
l

u
o
n
r
e
B

t
e
r
a
C

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

s
e
y
a
B

,

B
N
n
a
i
s
s
u
a
G

,

B
N
l
a
i
m
o
n
i
t
l
u
M

r
l
o
p

,
r
l
p

,
c
i
t
s
i
g
o
L
g
e
r

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

c
i
t
s
i
g
o
L
e
l
p
m
S

i

,
c
i
t
s
i
g
o
L

-
i
s
s
a
l
C
e
g
d
R

i

,

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

n
n
k

,

n
n
k
k

,

n
n
s

m
b
g

,
o
2
h
m
b
g

l
l

u
n

r
e
ﬁ

i
s
s
a
l
C
T
B
G

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

n
o
r
t
p
e
c
r
e
P
d
e
t
o
V

R
o
r
e
Z

k
B
I

r
e
ﬁ
i
s
s
a
l
C
g
n
i
t
s
o
o
B
t
n
e
i
d
a
r
G

r
e
ﬁ
i
s
s
a
l
C
s
r
o
b
h
g
i
e
N
K

r
e
ﬁ
i
s
s
a
l
C
y
m
m
u
D

n
o
r
t
p
e
c
r
e
P

r
e
ﬁ

30

Steﬀen Herbold, Steﬀen Tunkel

,
y
l
o
P
r
p
s
s
u
a
g

,
l
a
i
d
a
R
r
p
s
s
u
a
g

i

r
a
e
n
L
r
p
s
s
u
a
g

,
a
d
l
r
r

,
a
d
s

,

A
D
L
e
s
r
a
p
s

,
a
d
l
s

,

A
D
L
d
e
z
i
l
a
n
e
P

,
a
d
r

,
a
d
l
r

,
a
d
n
L

i

,
a
d
l
c
o
l

l

,
a
d
F
R

,
a
d
M

l

,
2
a
d
p

,
a
d
p

,
2
a
d

l

,
a
d

l

,

A
D
L
p
e
t
s

A
D
Q
p
e
t
s

,
v
o
C
a
d
Q

,
a
d
q

s
e
e
r
T
a
r
t
x
e

T
M
L

a
d
d

m
a
p

-
k
e
d
w

i

,
s
l
p
m

i
s

,
s
l
p

,
s
l
p
l
e
n
r
e
k

,
s
l
p
s

e
t
e
r
c
s
i
D
b
n

m
l
g
R
s
l
p

,
s
l
p
g

s
l
p
l
e
n
r
e

-
e
n
x
m

,
t
e
n
x
m

,
t
e
N
N
a
c
p

,

n
n
d

m
o
n
i
t
l
u
m

,
t
e
N
N
v
a

,
t
e
n
n

,

m
a
d
A
t

,
a
d
r
d
h

,
a
d
d
h

,
a
d
f

,
a
d
n
b

i

,
a
d
h

i
a
d
m
a

,
a
d
m

r

,
a
d
m

s

,
a
d
m

,
l
a
i
d
a
R
d
w
d

i

,
r
a
e
n
L
d
w
d

,

d
w
d
s

y
l
o
P
d
w
d

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

t
e
r
a
C

b
i
l

L
M
k
r
a
p
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

5

e
l
b
a
T

a
k
e
W

D
G
S

r
e
ﬁ
i
s
s
a
l
C
s
s
e
c
o
r
P
n
a
i
s
s
u
a
G

n
r
a
e
l
-
t
i
k
i
c
S

r
e
ﬁ
i
s
s
a
l
C
D
G
S

s
i
s
y
l
a
n
A
t
n
a
n
m

i

i
r
c
s
i
D
r
a
e
n
L

i

e
e
r
T
m
o
d
n
a
R

e
e
r
T
P
E
R

e
l

b
a
T
n
o
i
s
i
c
e
D

r
e
p
p
R

i

R
e
n
O

T
R
A
P

r
a
t
S
K

t
e
N
s
e
y
a
B

T
M
L

s
i
s
y
l
a
n
A
t
n
a
n
m

i

i
r
t
s
i
D
c
i
t
a
r
d
a
u
Q

r
e
ﬁ
i
s
s
a
l
C
e
e
r
T
a
r
t
x
E

d
i
o
r
t
n
e
C
t
s
e
r
a
e
N

r
e
ﬁ
i
s
s
a
l
C
e
v
i
s
s
e
r
g
A
e
v
i
s
s
a
P

r
e
ﬁ
i
s
s
a
l
C
s
r
o
b
h
g
i
e
N
s
u
d
a
R

i

Diﬀerential testing for machine learning

31

,

M
L
G
m
o
d
n
a
r

,

I

C
A
p
e
t
S
m
l
g

,

m
l
g

s
s
e
o
L
m
a
g

,
e
n

i
l

p
S
m
a
g

,

m
a
b

,

m
a
g

o
2
h

t
e
n
m
l
g

,
t
e
n
m
l
g

,

L
M
B
G
H
F

.

,

E
V
A
L
S

,

.

W
S
C
B
R
F

I

.

H
C
S
C
B
R
F

p
C
t
s
e
r
o
F
n
o
i
t
a
t
o
r

,
t
s
e
r
o
F
n
o
i
t
a
t
o
r

m
l
g
s
e
y
a
b

n
a
t
w
a

,

h
c
r
a
e
S
n
a
t

,

n
a
t

l
a
i
d
a
R
p
m
b
v

a
c
m
S
R

i

,
a
c
m
S
C

i

h
t
r
a
E
v
c
g

,

h
t
r
a
e

A
D
D
f
b
r

,
f
b
r

t
s
e
v
r
a
H
e
d
o
n

h
c
r
a
e
S
b
n

e
e
r
t
v
e

n
n
w
o

I

M
R
P

t
e
N
l
a
n
d
r
o

i

A
S
D
t
r
a
p

t
e
n
e
a
s
m

s
s
a
l
c
o
t
o
r
p

q
v
l

s
n
r
e
F
r

c
c
o
r

f
y
x

e
v
i
t
a
l
u
m
u
C
m
l
g
v

o
i
t
a
R
t
n
o
C
m
l
g
v

t
a
C
j
d
A
m
l
g
v

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

e
e
r
T
b
g
x

i

,
r
a
e
n
L
b
g
x

,

T
R
A
D
b
g
x

t
s
o
o
b
p
e
e
d

m
l
e

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

5

e
l
b
a
T

t
e
r
a
C

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

32

Steﬀen Herbold, Steﬀen Tunkel

t
e
r
a
C

d
i
a
h
c

g
e
r
g
o
l

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

5

e
l
b
a
T

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

Diﬀerential testing for machine learning

33

1
=
e
z
i
s
.
e
d
o
n
n
m

i

.

t
s
i
r
o
b
r

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

d
e
r
i
u
q
e
r

e
h
t

y
b

d
e
w
o
l
l
o
f

c
i
l
a
t
i

e
r
a

s
e
m
a
n

m
h
t
i
r
o
g
l
A

.
s
n
o
i
t
a
t
n
e
m
e
l

p
m

i

e
e
r
h
t

t
s
a
e
l

t
a

h
t
i
w

s
n
o
i
t
a
n
b
m
o
c

i

e
l
b

i
s
a
e
F

:
6

e
l
b
a
T

s
n
a
e
m

s
i
h
t

,

m
h
t
i
r
o
g
l
a

n
a

r
o
f

d
e
ﬁ
i
c
e
p
s

t
o
n

e
r
a

s
r
e
t
e
m
a
r
a
p

f
I

i

.
g
n
n
a
e
m
e
m
a
s

e
h
t

e
v
a
h
w
o
r

e
m
a
s

e
h
t

n

i

s
r
e
t
e
m
a
r
a
P

.
s
r
e
t
e
m
a
r
a
p

r
e
h
t
o

e
h
t

r
o
f

s
r
e
t
e
m
a
r
a
p

e
h
t

g
n
i
s
u

y
b

d
e
v
e
i
h
c
a

s
i

t
a
h
w
s
a

e
m
a
s

e
h
t

s
i

t
l
u
a
f
e
d

e
h
t

t
a
h
t

s
e
t
a
c
i
d
n

i

n
o
i
t
a
t
n
e
m
u
c
o
d

I
P
A
e
h
t

t
a
h
t

.
s
n
o
i
t
a
t
n
e
m
e
l
p
m

i

l
l
a

n
i

g
n
i
n
a
e
m
e
m
a
s

e
h
t

h
t
i
w
e
l
b
a
r
u
g
ﬁ
n
o
c

s
i

r
e
t
e
m
a
r
a
p

e
h
t

t
a
h
t

e
t
a
c
i
d
n

i

i

x

s
e
u

l
a
v

r
e
t
e
m
a
r
a
p

e
h
T

.
s

m
h
t
i
r
o
g
l
a

E
S
L
A
F
=
l
e
n
r
e
k
e
s
u

s
e
y
a
b

e
v
i
a
n

t
e
r
a
C

E
S
L
A
F
=
l
e
n
r
e
k
e
s
u

E
U
R
T
=
l
e
n
r
e
k
e
s
u

1
=
t
s
u
j
d
a

0
=
e
c
a
l
p
a
l

s
e
y
a
b

e
v
i
a
n

E
U
R
T
=
l
e
n
r
e
k
e
s
u

1
=
t
s
u
j
d
a

0
=
e
c
a
l
p
a
l

1
=
t
s
u
j
d
a

0
=
e
c
a
l
p
a
l

b
n

1
=
t
s
u
j
d
a

0
=
e
c
a
l
p
a
l

b
n

1
x
=
s
e
e
r
t
.

m
u
n

2
x
=
y
r
t

m

3
x
=
h
t
p
e
d
.
x
a
m

i

n
i
g
=
e
l
u
r
t
i
l

p
s

r
e
g
n
a
r

n
a
i
s
s
u
a
g
=
e
p
y
T
l
e
d
o
m

b
i
l

L
M
k
r
a
p
S

s
e
y
a
B
e
v
i
a
N

s
e
y
a
B
e
v
i
a
N

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

p
u
o
r
G

B
N
n
a
i
s
s
u
a
G

B
N
G

s
e
y
a
B
e
v
i
a
N

K

-

B
N
E
D
K

l
a
i
m
o
n
i
t
l
u
m
=
e
p
y
T
l
e
d
o
m

s
e
y
a
B
e
v
i
a
N

l
a
i
m
o
n
i
t
l
u
M
s
e
y
a
B
e
v
i
a
N

B
N
l
a
i
m
o
n
i
t
l
u
M

B
N
M

3
x

h
t
p
e
d
-

2
x
K

-

1
x

I
-

1
x
=
s
r
o
t
a
m

i
t
s
e

n

2
x
=
s
e
r
u
t
a
e
f

3
x
=
h
t
p
e
d

x
a
m

x
a
m

t
s
e
r
o
F
m
o
d
n
a
R

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

1
F
R

34

Steﬀen Herbold, Steﬀen Tunkel

2
x
=
d
e
x
i
F
d
e
r
p

1
=
e
d
o
N
n
m

i

3
x
=
l
e
v
e
L
n

1
x
=
e
e
r
T
n

1
x
=
s
e
e
r
t
.

m
u
n

i

n
i
g
=
e
l
u
r
t
i
l

p
s

2
x
=
y
r
t

m

r
e
g
n
a
r

t
e
r
a
C

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

6

e
l
b
a
T

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

p
u
o
r
G

2
x
K

-

1
x

I
-

1
x
=
s
r
o
t
a
m

i
t
s
e

n

2
x
=
s
e
r
u
t
a
e
f

x
a
m

t
s
e
r
o
F
m
o
d
n
a
R

r
e
ﬁ
i
s
s
a
l
C
t
s
e
r
o
F
m
o
d
n
a
R

2
F
R

1
=
e
z
i
s
.
e
d
o
n
n
m

i

.

2
x
=
d
e
x
i
F
d
e
r
p

1
=
e
d
o
N
n
m

i

1
x
=
e
e
r
T
n

t
s
i
r
o
b
r

1
x
=
e
e
r
t
n

2
x
=
y
r
t

m

r
a
e
n
i
L
m
v
s

1
x
=
u
a
t

f
r

2
r
a
e
n
i
L
m
v
s

1
x
=
t
s
o
c

1
0
0
.
0
=
l
o
t

1
0
0
.
0
=
n
o
l
i
s
p
e

3
r
a
e
n
i
L
m
v
s

1
x
=
t
s
o
c

2
L
=
s
s
o
L

y
l
o
P
m
v
s

1
x
=
C

2
x
=
e
e
r
g
e
d

1
x
=
m
a
r
a
P
g
e
r

C
V
S
r
a
e
n
i
L

∗
l
e
n
r
e
K
y
l
o
P
k
-

1
x
C

O
M
S

r
a
e
n

i
l

=
l
e
n
r
e
k

1
x
=
C

C
V
S

M
V
S
L

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

∗
l
e
n
r
e
K
y
l
o
P
k
-

2
x
E

-

O
M
S

1
x
C

y
l
o
p
=
l
e
n
r
e
k

2
x
=
e
e
r
g
e
d

1
x
=
C

C
V
S

M
V
S
P

Diﬀerential testing for machine learning

35

)
0
,
1
x
(
c
=
s
m
a
r
a
P
c
n
u
F
n
r
a
e
l

3
x
=
t
i
x
a
m

1
x
=
e
z
i
s

2
x
=
e
z
i
S
p
e
t
s

3
x
=
r
e
t
I
x
a
m

1
x
=
s
r
e
y
a
l

d
g
=
r
e
v
l
o
s

1
x

2
x

h
-

L
-

3
x
N

-

0
M

-

I
-

1
x
=
s
e
z
i
s

2
x
=

t
i
n

i

r
e
y
a
l

n
e
d
d
h

i

e
t
a
r

i

g
n
n
r
a
e
l

3
x
=
r
e
t
i

x
a
m

0
.
0
=
m
u
t
n
e
m
o
m

c
i
t
s
i
g
o
l
=
n
o
i
t
a
v
i
t
c
a

d
g
s
=
r
e
v
l
o
s

0
.
0
=
a
h
p
l
a

*
p
l
m

r
e
ﬁ
i
s
s
a
l
C
n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

n
o
r
t
p
e
c
r
e
P
r
e
y
a
l
i
t
l
u
M

r
e
ﬁ
i
s
s
a
l
C
P
L
M

P
L
M

3
x
=
s
h
c
o
p
e

x
a
m

0
=
m
u
t
n
e
m
o
m

2
x
=
e
t
a
r

n
r
a
e
l

*
D
G
S
p
l
m

1
x
=
e
z
i
s

0
=
a
m
m
a
g

0
=
a
d
b
m
a
l

1
=
s
t
a
e
p
e
r

0
=
g
e
r
2
l

l
l

u
n

c
i
t
s
i
g
o
L
g
e
r

0
=
t
s
o
c

1
x
=
k

n
n
k

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

0
0
0
0
1
=
r
e
t
I
x
a
m

0
=
m
a
r
a
P
g
e
r

e
g
a
p

t
x
e
n

n
o

d
e
u
n
i
t
n
o
c

c
i
t
s
i
g
o
L

0
R

-

0
0
0
0
1
M

-

1
x
K

-

k
B
I

0
0
0
0
1
=
r
e
t
i

x
a
m

t
n
e
u
q
e
r
f

t
s
o
m
=
y
g
e
t
a
r
t
s

r
e
ﬁ
i
s
s
a
l
C
s
r
o
b
h
g
i
e
N
K

N
N
K

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

1
x
=
s
r
o
b
h
g
i
e
n

n

R
L

R
o
r
e
Z

r
e
ﬁ
i
s
s
a
l
C
y
m
m
u
D

Y
M
M
U
D

1
0
0
.
0
=
l
o
t

l
a
i
d
a
R
m
v
s

1
=
e
l
a
c
s

2
x
=
a
m
g
i
s

1
x
=
C

t
e
r
a
C

1
0
0
.
0
=
l
o
t

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

6

e
l
b
a
T

b
i
l

L
M
k
r
a
p
S

a
k
e
W

n
r
a
e
l
-
t
i
k
i
c
S

p
u
o
r
G

∗
l
e
n
r
e
K
F
B
R
k
-

2
x
G

-

O
M
S

1
x
C

1
=
a
m
m
a
g

2
x
=
a
m
m
a
g

f
b
r
=
l
e
n
r
e
k

1
x
=
C

C
V
S

M
V
S
F
B
R

36

Steﬀen Herbold, Steﬀen Tunkel

l
a
m

i
r
p

2
L
=
s
s
o
l

1
0
0
0
.
0
=
n
o
l
i
s
p
e

c
i
t
s
i
g
o
l
=
d
o
h
t
e
m

c
i
t
s
i
g
o
L
g
e
r

1
x
=
t
s
o
c

0
=
a
d
b
m
a
l

c
i
b
=
p
c

r
l
p

r
l
o
p

l
a
m

i
r
p

2
L
=
s
s
o
l

1
0
0
0
.
0
=
n
o
l
i
s
p
e

1
x
=
a
d
b
m
a
l

r
l
p

c
i
t
s
i
g
o
L
g
e
r

1
x
=
t
s
o
c

c
i
b
=
p
c

1
0
0
0
.
0
=
n
o
l
i
s
p
e

1
L
=
s
s
o
l

t
o
n

l

d
u
o
c

s
r
e
ﬁ
i
s
s
a
l
c

l

p
m

t
e
r
a
C
e
h
T

.
r
o
t
c
e
V
t
r
o
p
p
u
s
.
s
n
o
i
t
c
n
u
f
.
s
r
e
ﬁ
i
s
s
a
l
c
.
a
k
e
w
e
g
a
k
c
a
p

e
h
t

n

i

d
e
t
a
c
o
l

e
r
a

s
l
e
n
r
e
k
M
V
S

a
k
e
W

∗

.
e
r
o
m
y
n
a
N
A
R
C
n
o

e
l
b
a
l
i
a
v
a

t
o
n

s
i

h
c
i
h
w

,
e
g
a
k
c
a
p
R
4
N
N
C
F

e
h
t

e
r
i
u
q
e
r

y
e
h
t

e
s
u
a
c
e
b

d
e
t
u
c
e
x
e

e
b

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

0
0
0
0
1
=
r
e
t
I
x
a
m

1
x
=
m
a
r
a
P
g
e
r

1
=
m
a
r
a
P
t
e
N
c
i
t
s
a
l
e

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

0
0
0
0
1
=
r
e
t
I
x
a
m

1
x
=
m
a
r
a
P
g
e
r

c
i
t
s
i
g
o
L

1
x
R

-

0
0
0
0
1
M

-

S
-

0
0
0
0
1
=
r
e
t
i

x
a
m

)
1
x

·
2
(
/
1
=
C

2
l
=
y
t
l
a
n
e
p

0
0
0
0
1
=
r
e
t
i

x
a
m

1
x
=
a
h
p
l
a

r
e
ﬁ
i
s
s
a
l
C
e
g
d
i
R

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

E
G
D
R

I

0
0
0
0
1
=
r
e
t
i

x
a
m

1
l
=
y
t
l
a
n
e
p

1
x
/
1
=
C

n
o
i
s
s
e
r
g
e
R
c
i
t
s
i
g
o
L

O
S
S
A
L

t
e
r
a
C

b
i
l

L
M
k
r
a
p
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

–

6

e
l
b
a
T

a
k
e
W

S
-

e
n
o
n
=
y
t
l
a
n
e
p

n
r
a
e
l
-
t
i
k
i
c
S

p
u
o
r
G

Diﬀerential testing for machine learning

37

Table 7: Feasible combinations with at least three implementations. Algorithm
names are italic followed by the required parameters. Parameters in the same
row have the same meaning. If parameters are not speciﬁed for an algorithm,
this means that the API documentation indicates that the default is the same
as what is achieved by using the parameters for the other algorithms. The
parameter values xi indicate that the parameter is conﬁgurable with the same
meaning in all implementations.

Group

Test results

GNB

KDENB

MNB

The classes of all implementations are equal, except Weka, which yields
diﬀerent classes on up to 2% of the instances on the RANDOM, WINE,
and BC data. These diﬀerences are not signiﬁcant. The scores of the two
pair naive bayes and nb, as well as the pair Scikit-learn Spark MLlib are
equal. For all other pairs, we often observe large numbers of deviations
between the scores on up to 89% of the instances, both on the test and
training data. However, these diﬀerences are also not signiﬁcant, i.e., we
observe the same distributions when all instances are considered, but the
scores of the individual instances depend on the implementation.

We observe diﬀerences between all implementations. The diﬀerences be-
tween the R packages naive bayes and nb are small, with respect to ∆,
i.e, we observe only few diﬀerences in classiﬁcations with at most 1%.
However, we observe big diﬀerences in scores, on both training and test
data. However, none of these diﬀerences are signiﬁcant. The diﬀerences
to the Weka implementation are large: we never observed the exact same
classiﬁcation with ∆ showing diﬀerences on between 3% and 27% of the
data. These diﬀerences are signiﬁcant in about half of the time, i.e., on
the WINE data and the test date of UNIFORM. We also observe large
diﬀerences in the scores, including cases where every single score is diﬀer-
ent. Similar to the classiﬁcations, the diﬀerences in scores are signiﬁcant
one the WINE data, the test data of UNIFORM, and, in addition, the
test data of RANDOM, but only between the nb package and WEKA.

We did not observe any diﬀerences between the Weka and Spark MLlib
implementation. We also did not observe any diﬀerences in the scores
between all three implementations. However, the Scikit-learn implemen-
tation classiﬁes one instance diﬀerently in ten of 21 comparisons. These
diﬀerences are in cases where the score is almost exactly 0.5 and for one
framework slightly smaller and for the other framework slightly larger
than 0.5, e.g., for Scikit-learn 0.499 and for Weka 0.501. In this case, we
do not consider the scores diﬀerent, but the classes are diﬀerent regardless.

continued on next page

38

Steﬀen Herbold, Steﬀen Tunkel

Group

Test results

Table 7 – continued from previous page

RF1

RF2

LSVM

PSVM

The results of the random forests with a ﬁxed depth of ﬁve are never ex-
actly the same. Between 0.7% and 40% of classiﬁcations are diﬀerent. We
note that the diﬀerences are larger on the test data than on the training
data. The diﬀerences between the two R packages and Scikit-learn are sim-
ilar to each other. The deviations between these three implementations
and Weka are about twice as large. However, these diﬀerences between
the classiﬁcations are mostly not signiﬁcant, with two exceptions on the
RANDOM data, where ranger deviates signiﬁcantly from Scikit-learn and
weka. For the scores, we observe that the Weka and Rborist have devi-
ations on between 28% and 82% of the instances. However, these large
diﬀerences are only signiﬁcant on the training data of WINE. The scores
of Weka imlementation and Rborist have deviations from Scikit-learn on
between 52% and 100% of the instances. These diﬀerences are almost
always signiﬁcant. We note that package ranger does not support scores.

The results of the random forest without a ﬁxed depth are also almost
never exactly the same, with two exceptions, once for Scikit-learn and
the R package ranger, and once for Scikit-learn and the R package rf.
Otherwise, there are diﬀerences for up to 38% of the instances. Same as
for the random forest with ﬁxed depth, the diﬀerences are larger on the
test data. However, in most cases, these diﬀerences are not signiﬁcant,
except on the WINE data, where ranger is signiﬁcantly diﬀerent from all
other implementations. For the scores diﬀerences on between 23% and
87% of the instances. However, none of these diﬀerences are signiﬁcant,
i.e., while the scores are diﬀerent, their distributions are not.

None of the linear SVMs implementations lead to equal results. The svm-
linear and svmlinear2 from Caret are almost equal, with only a single
instance that is classiﬁed diﬀerently on the WINE data. The Scikit-learn
and Weka implementations are also almost equal with two single instance
misclassiﬁed on the test data of UNIFORM and WINE. The diﬀerences
between most other pairs of implementations are similar, with between
1% and 11% of the instances deviating from each other. The clear outlier
is the Spark MLlib implementation, which yields a diﬀerent class on 38%
to 44% of the instances on the RANDOM data, and also diﬀerences on
12% to 16% of the WINE instances. These outliers of Spark MLlib are
the only signiﬁcant deviations of the linear SVM.

The Scikit-learn and Weka implementations of the polynomial SVM are
equal. The Caret implementation yields diﬀerent classiﬁcations for 17%
to 27% of the instances. However, this diﬀerence is only signiﬁcant on the
UNIFORM data.

RBFSVM The Scikit-learn and Weka implementations of the RBF SVM are equal.
The care implementation yields diﬀerent classiﬁcations for 6% to 52% of
the instances. This diﬀerence is always signiﬁcant, except training data of
BC.

continued on next page

Diﬀerential testing for machine learning

39

Group

Test results

Table 7 – continued from previous page

MLP

All three MLPs lead to the same classiﬁcations on the RANDOM data.
The Scikit-learn and Weka implementations are also also equal on the
UNIFORM data. The Scikit-learn and Spark MLlib implementation clas-
sify one instance diﬀerently on the UNIFORM data. However, they are
equal on the BC and WINE data. The diﬀerence of Scikit-learn and Spark
MLlib to Weka is on the BC and WINE data is signiﬁcant, with diﬀerences
on between 34% and 51% of the instances. The scores of all three imple-
mentations are signiﬁcantly diﬀerent from each other, with deviations on
between 53% and 100% of the instances.

DUMMY The trivial classiﬁers always have the same classes. The scores depend on
the implementation of the trivial model: Caret and Weka have the same
approach and always yield the same results. Scikit-learn uses a diﬀerent
approach and disagrees with the two other scores. These disagreements
are signiﬁcant.

KNN

LR

RIDGE

LASSO

The nearest neighbor algorithms have the same classes, except on the
WINE data, where about 1% of the instances are classiﬁed diﬀerently
between all combinations of frameworks. These diﬀerences are not signif-
icant. The scores of Scikit-learn and Caret are also equal, except on the
WINE data, where 5% of the instances have diﬀerent scores. This diﬀer-
ence is signiﬁcant. On WINE, the scores of Caret are equal to those of
Weka. On the other data sets, Weka has signiﬁcantly diﬀerent scores from
Caret and Scikit-learn, with 91% to 100% of instances receiving diﬀerent
scores.

The logistic regressions have the same classes, except on the test data
of BC, where about 3% of the instances are classiﬁed diﬀerently. These
diﬀerences are not signiﬁcant. Similarly, the scores are always equal for
all implementations on the UNIFORM, RANDOM, and WINE data, as
well as for Scikit-learn, Weka, and Spark MLlib on the BC training data.
We observe diﬀerences on between 3% and 9% of the instances on the
remaining tests on the BC data. The diﬀerences between the scores are
signiﬁcant.

The Weka, the LogisticRegression from Scikit-learn, and the Caret model
plr yield identical for both classiﬁcations and scores results. The diﬀer-
ences between the classes predicted by the other pairs of implementations
are between 0.3% and 4%, i.e., relatively small and not statistically sig-
niﬁcant. The scores are inconsistent and have large and signiﬁcant devi-
ations of on between 65% and 99% of the instances. The RidgeClassiﬁer
of Scikit-learn does not compute scores.

The Caret and Scikit-learn implementation yield almost the same classes,
with up to three instances classiﬁed diﬀerently. These diﬀerences are not
signiﬁcant. The diﬀerences to Spark MLlib are large and between 4% and
43% of instances are classiﬁed diﬀerently. However, these diﬀerenes are
only signiﬁcant on the RANDOM data. The scores are diﬀerent for be-
tween 67% and 98% of the instances for Caret and Scikit-learn. However,
the diﬀerence is only signiﬁcant on the training data of BC. The scores of
Spark MLlib are diﬀerent on at least 99% and 100% of the instance, i.e.,
almost always. These diﬀerences are signiﬁcant.

40

Steﬀen Herbold, Steﬀen Tunkel

Fig. 5: Deviations observed with GNB.

Fig. 6: Deviations observed with KDENB.

Fig. 7: Deviations observed with MNB.

Fig. 8: Deviations observed with RF1.

0.00.20.40.60.81.0/nCARETnaivebayes, CARETnbCARETnaivebayes, SKLEARNCARETnaivebayes, SPARKCARETnaivebayes, WEKACARETnb, SKLEARNCARETnb, SPARKCARETnb, WEKASKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nGNB0.00.20.40.60.81.0/nCARETnaivebayes, CARETnbCARETnaivebayes, WEKACARETnb, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nKDENB0.00.20.40.60.81.0/nSKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nMNB0.00.20.40.60.81.0/nCARETnaivebayes, CARETnbCARETnaivebayes, SKLEARNCARETnaivebayes, SPARKCARETnaivebayes, WEKACARETnb, SKLEARNCARETnb, SPARKCARETnb, WEKASKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nGNBDiﬀerential testing for machine learning

41

Fig. 9: Deviations observed with RF2.

Fig. 10: Deviations observed with LSVM.

Fig. 11: Deviations observed with PSVM.

Fig. 12: Deviations observed with RBFSVM.

0.00.20.40.60.81.0/nCARETranger, CARETrboistCARETranger, SKLEARNCARETranger, WEKACARETrboist, SKLEARNCARETrboist, WEKASKLEARN, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nRF20.00.20.40.60.81.0/nCARETsvmlinear2, CARETsvmlinear3CARETsvmlinear2, CARETsvmlinearCARETsvmlinear2, SKLEARNCARETsvmlinear2, SPARKCARETsvmlinear2, WEKACARETsvmlinear3, CARETsvmlinearCARETsvmlinear3, SKLEARNCARETsvmlinear3, SPARKCARETsvmlinear3, WEKACARETsvmlinear, SKLEARNCARETsvmlinear, SPARKCARETsvmlinear, WEKASKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairLSVMDataTestTraining0.00.20.40.60.81.0/nCARET, SKLEARNCARET, WEKASKLEARN, WEKAPairPSVMDataTestTraining0.00.20.40.60.81.0/nCARET, SKLEARNCARET, WEKASKLEARN, WEKAPairRBFSVMDataTestTraining42

Steﬀen Herbold, Steﬀen Tunkel

Fig. 13: Deviations observed with MLP.

Fig. 14: Deviations observed with DUMMY.

Fig. 15: Deviations observed with KNN.

Fig. 16: Deviations observed with LR.

0.00.20.40.60.81.0/nSKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nMLP0.00.20.40.60.81.0/nCARET, SKLEARNCARET, WEKASKLEARN, WEKAPairDUMMYDataTestTraining0.00.20.40.60.81.0/nCARET, SKLEARNCARET, WEKASKLEARN, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nKNN0.00.20.40.60.81.0/nCARETplr, SKLEARNCARETplr, SPARKCARETplr, WEKASKLEARN, SPARKSKLEARN, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nLRDiﬀerential testing for machine learning

43

Fig. 17: Deviations observed with RIDGE.

Fig. 18: Deviations observed with LASSO.

0.00.20.40.60.81.0/nCARETplr, CARETreglogisticCARETplr, SKLEARNlogisticCARETplr, SKLEARNridgeCARETplr, SPARKCARETplr, WEKACARETreglogistic, SKLEARNlogisticCARETreglogistic, SKLEARNridgeCARETreglogistic, SPARKCARETreglogistic, WEKASKLEARNlogistic, SKLEARNridgeSKLEARNlogistic, SPARKSKLEARNlogistic, WEKASKLEARNridge, SPARKSKLEARNridge, WEKASPARK, WEKAPairDataTestTraining0.00.20.40.60.81.0score/nRIDGE0.00.20.40.60.81.0/nCARET, SKLEARNCARET, SPARKSKLEARN, SPARKPairDataTestTraining0.00.20.40.60.81.0score/nLASSO