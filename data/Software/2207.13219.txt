Dalorex: A Data-Local Program Execution and
Architecture for Memory-bound Applications

Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff and Margaret Martonosi
Department of Computer Science, Princeton University
Princeton, New Jersey, USA
Email: {movera, esin.tureci, wentzlaf, mrm}@princeton.edu

2
2
0
2

g
u
A
2

]

R
A
.
s
c
[

2
v
9
1
2
3
1
.
7
0
2
2
:
v
i
X
r
a

fail

Abstract—Applications with low data reuse and frequent ir-
regular memory accesses, such as graph or sparse linear algebra
workloads, fail to scale well due to memory bottlenecks and poor
core utilization. While prior work with prefetching, decoupling,
or pipelining can mitigate memory latency and improve core
utilization, memory bottlenecks persist due to limited off-chip
bandwidth. Approaches doing processing in-memory (PIM) with
Hybrid Memory Cube (HMC) overcome bandwidth limitations
to achieve high core utilization due to poor task
but
scheduling and synchronization overheads. Moreover, the high
memory-per-core ratio available with HMC limits strong scaling.
We introduce Dalorex, a hardware-software co-design that
achieves high parallelism and energy efﬁciency, demonstrating
strong scaling with >16,000 cores when processing graph and
sparse linear algebra workloads. Over the prior work in PIM,
both using 256 cores, Dalorex improves performance and energy
consumption by two orders of magnitude through (1) a tile-
based distributed-memory architecture where each processing
tile holds an equal amount of data, and all memory operations
are local; (2) a task-based parallel programming model where
tasks are executed by the processing unit that is co-located with
the target data; (3) a network design optimized for irregular
trafﬁc, where all communication is one-way, and messages do not
contain routing metadata; (4) novel trafﬁc-aware task scheduling
hardware that maintains high core utilization; and (5) a data
placement strategy that improves work balance.

This work proposes architectural and software innovations to
provide—to our knowledge—the fastest design for running graph
algorithms while still being programmable for other domains.

I. INTRODUCTION
System designs are increasingly exploiting heterogeneous,
accelerator-rich designs to scale performance due to the slow-
ing of Moore’s Law [40] and the end of Dennard Scaling [21],
[52]. While compute-bound workloads thrive in an accelerator-
rich environment, memory-bound workloads, such as graph
algorithms and sparse linear algebra, present the following
challenges: (a) a low compute-per-data ratio, with little and
unpredictable data reuse; (b) frequent ﬁne-grained irregular
memory accesses that make memory hierarchies inefﬁcient;
(c) atomic accesses, synchronization, and inherent load imbal-
ance resulting in poor utilization of computing resources.

Recent work proposed using accelerators that pipeline the
different stages of graph processing in hardware [1], [14],
[19], [43], [47], [51]. Other works have used general-purpose
cores with decoupling and prefetching, to mitigate memory
latency issues [18], [38], [42], [45], [57], and coalescing, to
reduce the serialization caused by atomic updates [41]. These
solutions, however, fail to avoid costly data movements and are

Fig. 1.
Program execution of three sequential graph-processing steps in a
cache hierarchy (left), and Dalorex (right). Instead of moving data with little
reuse, Dalorex invokes tasks where the data is local—reducing movement.

eventually bottlenecked by memory bandwidth. The proposals
in the area of Processing-in-memory (PIM) enjoy a higher
memory bandwidth by processing data near DRAM [2], [69],
[74]. However, their memory technology choice constrains
the storage-per-core ratio, limiting the level of parallelism—
their
as we demonstrate later in this paper. In addition,
parallelization schemes suffer from load imbalance and syn-
chronization overheads that undermine core utilization.

Opportunities: We started our work by examining what
features are necessary to execute graph workloads in a scalable
manner. We observed that to maximize throughput (edges
the solution should: (1) minimize
processed per second),
data movement, which is the performance bottleneck and the
most signiﬁcant energy cost; (2) exploit the spatio-temporal
parallelism of operations to improve work balance and re-
source utilization; (3) avoid serializing features such as global
synchronization barriers and read-modify-write atomic opera-
tions; and (4) avoid frontier redundancy and data staleness to
minimize the number of edges explored (work-efﬁciency).

Our Approach: We present Dalorex, a highly scalable
and energy-efﬁcient hardware-software co-design that exploits
these opportunities by optimizing across the computing stack
without sacriﬁcing ISA programmability. Our architecture
design is a 2D-array of homogeneous tiles, each containing an
SRAM scratchpad, a thin in-order core (with no cache) called

1

L1 hitMain Memory (DRAM)Traditional Shared-memory: Bring data to the computeDalorex:  Migrate compute to the dataT1T2T3LLC missLLC hitCorePrivate Scratchpad (SRAM) T1Task invocationT3Local accessT2Local accessLocal accessData movementLLC SliceCoreL1$ 
 
 
 
 
 
the Processing Unit (PU), a task scheduling unit feeding the
PU, and a router connected to the network-on-chip (NoC). Our
programming model enables further parallelism by splitting
the sequential code inside a parallel-loop iteration into tasks
at each pointer indirection (Fig. 2). Tasks execute at the tile
containing the data to be operated on. Correct program order is
guaranteed because new tasks are spawned at the completion
of the parent task by placing their parameters into the NoC.
Since the spawned tasks are independent and execute in
any order, Dalorex achieves high synchronization-free spatio-
temporal parallelism. We designed the task scheduling unit
(TSU) because—in this decentralized task-ﬂow model—work
efﬁciency and core utilization highly depend on the order of
task executions. The TSU also coordinates PUs and routers to
make task invocations non-blocking and non-interrupting.

Dalorex distributes all dataset arrays in equal chunks across
all tiles. Since these chunks are scattered using lower-order
bits, Dalorex achieves good workload and NoC trafﬁc balance,
despite the irregular access patterns of sparse workloads. As
a result of this data partitioning, each tile owns a set of nodes
(with no copies of data), and all operations are atomic by
design. This also enables Dalorex to have a decentralized
frontier, such that we can remove the global barrier between
epochs of graph algorithms. In addition, the data-local execu-
tion of tasks removes the round-trip time and costly energy
of accessing memory. It also removes access contention and,
thus, serialization. Dalorex does not use virtual memory.

Fig. 1 illustrates how the distributed memory architecture
of Dalorex (right panel) minimizes data movement compared
to architectures with hierarchical shared-memory structures
(left panel). In Dalorex, each piece of data is only accessed
by one PU. Instead of bringing data to the PUs, they send
task-invoking messages to the tile containing the data to be
processed next. Dalorex exploits the inherent pointer indirec-
tion in sparse data formats to route a task-invocation message.
The tile’s PU executes the task corresponding to the message
received. In architectures with a traditional memory hierarchy,
sparse applications result in overwhelming data movement:
data-reuse distance is highly irregular, and thus, cache thrash-
ing leads to more than 50% of all memory accesses missing
in the cache levels and going to main memory [38]. Shared-
memory architectures also carry the overhead of cache coher-
ence, virtual memory, and atomic operations.

The technical contributions of this paper are:
• A data-local execution model where each processing tile
holds an equal amount of data—operating on local data
makes all updates atomic and minimizes data movement.
• A programming model that unleashes parallelism and
improves work balance by splitting the code into tasks—
based on data location—while preserving program order.
• A tiled distributed-memory architecture connected by a
NoC optimized for irregular communication (headerless
task-routing based on the index of the array they access).
• A hardware unit (TSU) that removes task-invocation over-
heads and schedules tasks to maximize core utilization
and work efﬁciency by sensing the network trafﬁc.

Fig. 2. Code of the Single Source Shortest Path (SSSP) algorithm, and how
it is split into Dalorex tasks based on indirect memory accesses.

We evaluate Dalorex and demonstrate that:
• Our architecture and data layout improves performance
by 6.2× over the best ISA-programmable prior work [2].
On top of that, our task invocation scheme improves by
4.7×, and our uniform data placement and trafﬁc-aware
scheduling, 4.4× more. Finally, removing the barriers and
upgrading the NoC provides an extra 1.8×, totaling a
compound 221× geomean improvement, across four key
graph applications, using equal processor count (256).
• Regarding energy, the compound improvements of using
SRAM (16×), our architecture and data layout (5.2×),
and TSU (3.9×) total 325×, in geomean.

• Dalorex achieves strong scaling with over 16,000 process-
ing tiles, reaching the parallelization limit of the datasets.
• Our data distribution and parallelization do not require
pre-processing to improve work balance across tiles.
Dalorex remains agnostic to the dataset characteristics.

II. BACKGROUND AND MOTIVATION

Dalorex is designed to accelerate applications that are
memory-bound due to irregular access patterns caused by
pointer indirection. Although this paper focuses on graph
algorithms, Dalorex is applicable to other domains such as
sparse linear algebra. We demonstrate this by evaluating sparse
matrix-vector multiplication.

A. Graph Algorithms and their Memory Access Patterns

Graphs are represented using adjacency matrices where
rows/columns represent vertices and values, weighted edges.
Since columns contain mostly zero values (most vertices have
few connections), these matrices are stored in formats like
Compressed-Sparse-Row (CSR) using four arrays. The non-
zero elements are accessed via pointer indirection.

Fig. 2 shows the code for Single Source Shortest Path
(SSSP) and the corresponding Dalorex tasks. In a regular
memory hierarchy, the accesses to neighbor vertex data in
the innermost loop (line 8) result in many cache misses
and thus, costly accesses to DRAM [38]. Moreover,
the
source vertex data (lines 3 and 4) and the neighbor
index (line 6) are also accessed indirectly, and the utility
of the cache depends on the number of neighbors and the
workload distribution. In Dalorex, the code is split at each level

2

1 2 3 4 5 6 7 8 9 10 11 12 13TASK 1   TASK 2TASK 3 while not frontier.isEmpty()   parallel for (v : frontier)    node_dist = dist[v]    startInd, endInd = ptr[v], ptr[v+1]    for i in range(startInd, endInd):      neighbor = edges[i]      new_dist = node_dist + edge_val[i]      curr_dist = dist[neighbor]      if (new_dist < curr_dist):        dist[neighbor] = new_dist        new_frontier.push(neighbor) frontier = new_frontier new_frontier = []search as a hardware pipeline [1], [19], [43], [47], [51]. Poly-
graph [14] generalized prior accelerator designs to perform
any of their algorithmic variants and optimize work efﬁciency
based on dataset characteristics and Fifer [43] offers a dynamic
temporal pipelining to achieve load-balancing. While effective
for hiding latency and increasing load-balancing, these ap-
proaches remain highly energy inefﬁcient due to excessive data
movement and are ultimately limited by DRAM bandwidth.

To increase memory bandwidth and reduce data movement,
Tesseract [2] proposes in-memory graph processing by intro-
ducing cores into the logic layer of a 3D Hybrid Memory
Cube (HMC) [24], [48]. Tesseract utilizes remote procedure
calls executed by cores located near the data, similar to
the execution migration literature [36], [54]. However, the
performance of this PIM-based approach is limited because:
(a) Their vertex-based data distribution causes load imbalance
since the highly variable vertex degree in graphs cause dif-
ferent amount of work per core; (b) Tesseract remote calls
are interrupting, incurring 50-cycle penalties, and the solution
to avoid these interrupts proposed by GraphQ [74] employs
barriers for batch communication, causing high synchroniza-
tion overheads; (c) HMC-based architectures are constrained
in the number of cores per cube, which is tied to the number
of vaults. In this work, as we explore the limits of graph
parallelization, we demonstrate that the energy-optimal storage
size per tile is in the kilobytes range, much smaller than what
HMC offers.

2) Software Techniques: Software solutions to accelerating
graph applications include optimizing data placement, work
efﬁciency, and parallelization schemes based on the target
hardware system [70]. For distributed systems, GiraphUC [20]
puts forward a Barrierless-Asynchronous-Parallel model that
reduces message staleness and removes global synchronization
barriers. However, GiraphUC is not optimized for data locality
and has high communication costs. Pregel [37] does parallelize
tasks such that each iteration is executed where the data is
local, but the data is distributed in a vertex-centric manner,
resulting in inherent load-balancing problems in addition to
communication overhead.

D. Manycore Architectures & Memory Bandwidth

Large-scale parallel processing can be done using many
small processing elements. From Systolic arrays [25], [26],
[32] and streaming architectures [23], [58], [59], to modern
manycores [4], [15], [17], [67], challenges remain in the data
supply on memory-bound applications.

Recently,

some industry products have utilized large
amounts of SRAM to achieve high on-chip bandwidth and thus
higher performance [9], [29]. Aside from energy consumption,
there are architectural advantages to using a scratchpad mem-
ory per core, e.g., low latency, dedicated access, and scalable
memory bandwidth (more cores means more memory ports).

III. DALOREX: A HARDWARE-SOFTWARE CO-DESIGN

Dalorex minimizes data movement and maximizes resource
(1) a data distribution that allows for only

utilization with:

Fig. 3. Program order and synchronization for Bulk Synchronous Parallel
(BSP), left, versus Dalorex model (right). The yellow columns are the tasks
executed in each tile (arrows indicate program order). Within them, the color
indicates the task type, based on the code in Fig. 2, while the letters represent
different vertex iterations. The dotted boxes on the Dalorex side mean that
tasks from other iterations may interleave.

of pointer indirection, leading to a series of tasks. For example,
TASK 1 accesses arrays dist and ptr (tuple of size #vertices),
while TASK 2 accesses arrays edge and edge value (tuple of
size #edges), and TASK 3 accesses dist.

As Fig. 3 illustrates, Dalorex allows spatial

interleav-
ings within vertex iterations while preserving program order
through sequential invocation of the tasks. The classical time-
wise interleavings of parallel-loop iterations are also allowed.
Finally, instead of using a centralized frontier, Dalorex uses
local frontiers, which removes the synchronization overheads
of frontier insertions and global barriers. As a result, our novel
programming model maximizes program parallelization.

B. Algorithmic variants

There are two modes of processing graph data: pulling data
for a vertex from its neighbors or pushing data to its neigh-
bors [7]. While pull-based algorithms tend to require higher
memory communication, push-based algorithms often require
atomic operations. When atomic operations are mitigated,
push-based algorithms have the advantage of reduced commu-
nication overhead. A hybrid version, direction-optimized BFS
[5], and its variants can offer faster convergence. However,
they incur a storage overhead and need heuristics, as they may
access either the source or the destination of an edge. Although
pull-based algorithms can be executed on the Dalorex architec-
ture, we focus on push-based algorithms due to their reduced
communication and work efﬁciency since Dalorex eliminates
the need for atomic operations.

C. Prior Work

We describe several pipelining and data-movement-reducing
techniques and lay out what aspects of these designs limit their
performance at very large parallelizations.

1) Hardware Techniques: Recent works have used decou-
pling to overlap data fetch and computation by running ahead
in the loop iterations to bring the data asynchronously [38],
[42], [45], [57]. To accomplish this, they perform program
slicing on each software thread, creating a software pipeline
effect. Others have proposed accelerators to perform the graph

3

cccCentralized FrontierLocal  Frontier  New FrontierGlobalbarrier to swapthe newfrontierTile 1Tile 2Tile 3aacbabccaaabbbTile 1Tile 2Tile 3Local  Frontier Local  Frontier Explore next vertex iteration asynch. Classical parallelizationDalorex: data-local execution Timeblocal memory operations;
(2) a programming model that
allows for intra-loop spatial and inter-loop timewise interleav-
ings; (3) a homogeneous tile-based architecture that optimizes
irregular data-access patterns, where each tile includes a local
memory, a router, and a processing unit. Program execution is
orchestrated by the task scheduling unit.

## These constants are filled when the program is loaded
param TILEID
param NODES_PER_CHUNK
param EDGES_PER_CHUNK
## Local chunk of the dataset arrays
var dist[NODES_PER_CHUNK]
var ptr[NODES_PER_CHUNK]
var edge_idx[EDGES_PER_CHUNK]
var edge_values[EDGES_PER_CHUNK]

A. Data distribution

As mentioned before, graphs and sparse matrices are often
stored in formats like CSR using four data arrays. In Dalorex,
these arrays are divided equally across all tiles and stored in
their private memories, making each tile responsible for opera-
tions only on its local data. For example, the edge_values
array has as many elements as edges (E) in a graph. This
array is split as in Listing 1 so that each of the T tiles has
EDGES_PER_CHUNK (E/T ) adjacent elements, e.g., the ﬁrst
tile contains elements from 0 to EDGES_PER_CHUNK-1.

Ours is the ﬁrst work that distributes an adjacency matrix
in this manner; the usual approach is to do a 2D distribution
of the matrix [8], where each computing element gets a rect-
angular subset of the matrix to compute. Although designed
to minimize communication, 2D distribution presents some
challenges: a subset of a sparse matrix is hyper-sparse (making
row or column sparse formats storage-inefﬁcient), and the
resulting chunks do not have an equal number of non-zeros
(different storage needs). Because Dalorex has all memory
equally distributed across tiles, its design embraces inter-tile
communication by making it part of the programming model.

B. Programming model

The BSP model parallelizes graph algorithms either at the
outer loop (processing vertices in the frontier) or the inner loop
(processing the neighbors of the frontier vertices) since these
can be processed in any order. We posit that if the iterations of
the inner loop are performed in program order, the location of
the execution can be altered. Dalorex preserves the instruction
order within an iteration since subsequent tasks are invoked by
the parent task at completion. Since only one tile has access
to each data chunk, coherence is not an issue.

Adapting a graph kernel for Dalorex involves splitting
the inner loop iteration into multiple tasks at each pointer
indirection. As we have seen in Fig. 2, this results in three
tasks for SSSP, where each task produces the array index to
be accessed by the next task. Additionally, graph algorithms
require a fourth task to explore the frontier vertices (Listing 1).
Every tile contains the same code and can perform any task.
From the program execution timeline perspective, after a tile
performs a task, it sends the output of the task (i.e., the input
for the next task) to the tile containing the data to be operated
next, thus preserving sequential order.

From the point of view of an individual tile, inputs for
any task may arrive in any order into their corresponding
task-speciﬁc queues. The execution order of different tasks
is determined by the TSU—described in Section III-E.

const FRONTIER_LEN = NODES_PER_CHUNK/32
## Bitmap frontier and memory-stored variables
var frontier[FRONTIER_LEN] = [0,...,0]
var blocks_in_frontier = 0
var t1_new_node = True
var neighbor_begin = 0;

##Configure network channels between tasks and their queues
CQ1 = channel(q_len=128, target=T2, enc=EDGES_PER_CHUNK)
CQ2 = channel(q_len=1024,target=T3, enc=NODES_PER_CHUNK)

## Declaring a task requires the length of its IQ and
## whether its parameters are loaded before the invocation
task T1 [32] ():

#T1 params aren’t pre-loaded. We read from the IQ of T1
node_id = peek(IQ1.head)
if (t1_new_node):

neighbor_begin = ptr[node_id]

neighbor_end = ptr[node_id+1]
## Split msg if range crosses chunk limits, or > MAX_T2
partial_end = min(neighbor_end, TILEID*NODES_PER_CHUNK)
partial_end = min(partial_end, neighbor_begin+MAX_T2)
while (!CQ1.full && neighbor_begin < partial_end):

CQ1 = neighbor_begin ## global idx for tile address
CQ1 = (partial_end % NODES_PER_CHUNK) ## local idx
CQ1 = dist[node_id]
neighbor_begin = partial_end

## We pop node_id if the whole range was pushed to CQ1
t1_new_node = (neighbor_begin == partial_end)
if (t1_new_node):
pop(IQ1.head)

# Task parameters are loaded by TSU before the task begins
task T2 [128] (neighbor_begin, neighbor_end, node_dist):

for i in range(neighbor_begin,neighbor_end):
##Writing to a Channel Queue sends data to the network

CQ2 = edge_idx[i]
CQ2 = edge_values[i] + node_dist

task T3 [2048] (new_dist, neigh_id):

curr_dist = dist[neigh_id]
if (new_dist < curr_dist):

dist[neigh_id] = new_dist

## Insert vertex into Local Frontier
blk_id = neigh_id >> 5;

blk_bits = frontier[blk_id];
frontier[blk_id] = mask_in_bit(blk_bits, neigh_id)
if (blk_bits == 0): ##only add newly active blocks

blocks_in_frontier++
IQ4 = blk_id

# T4 re-explores the local frontier queue
task T4 [FRONTIER_LEN] ():

frontier_block = peek(IQ4)
while(blocks_in_frontier > 0 && !IQ1.full):
blk_bits = frontier[frontier_block]
block_base = frontier_block << 5;
while(blk_bits > 0 && !IQ1.full):
idx = search_msb(blk_bits)
blk_bits = mask_out_bit(blk_bits, idx)
vertex = block_base + idx
IQ1 = vertex

# If all the no pending vertices in the block we
# remove the block from the frontier queue
if (blk_bits == 0):

pop(IQ4)
blocks_in_frontier--
frontier_block = peek(IQ4)

Listing 1. Pseudo-code of the SSSP algorithm adapted to use the Dalorex
programming model.

4

C. Program Flow and Synchronization

Dalorex programs do not have a main. Instead, tiles await
the task parameters to arrive in the corresponding input queue,
and PUs are invoked by TSU to process them. A task invokes
the next task by placing the task parameters into an output
queue (OQ). An OQ can be either a task-input queue (IQ) if
the next task operates over data residing in the same tile or a
channel queue (CQ), which puts a message into the network.
Listing 1 contains the code of the Dalorex-adapted SSSP
kernel. To start running the SSSP example, only the tile
containing the root of the graph-search receives a message to
invoke task1. This task obtains the range array indices that
contain the neighbors of vertex_id. If this range crosses the
border of a chunk, a separate message is sent to each tile with
the corresponding begin and end indices. Similarly, the range
is split if the length is bigger than a constant MAX_T2, that is
set to guarantee that task2 can execute without exceeding
the capacity of CQ2. However, task1 does not have this
guarantee and needs to check explicitly that CQ1 does not
overﬂow. If CQ1 ﬁlls before sending all the messages for
vertex_id range, a ﬂag is set and task1 ends early after
updating the begin index. The next time task1 is invoked
it continues operating on the same vertex_id since it was
not popped from IQ1. Note that vertex_id was explicitly
loaded with peek, as opposed to task2 and task3, where
the task parameters are implicitly popped from their IQs by
TSU (Section III-E).

Continuing from Listing 1, task2 calculates the new
distances to all the neighbors of vertex_id from root using
their edge values and sends this value to the owner of task3
data. Then, task3 checks whether the distance of neigh_id
from the root is smaller than the previously stored value. If
so, neigh_id needs to be inserted in the frontier.

When placing the parameters of the next task into a CQ,
the ﬁrst ﬂit is the index of the distributed array to be accessed
so that the message is routed to the tile containing the data for
that index (details in Section III-E). Sending tasks to other tiles
is akin to the non-blocking remote function calls employed by
Tesseract [2]. Unlike Tesseract, Dalorex task invocations are
non-interrupting when received.

Dalorex does not use a barrier after each epoch to explore
the new global frontier. Instead, each tile has a local frontier,
thus allowing for continuous execution ﬂow of tasks.

The local frontier is a bitmap that accumulates the updates
to the vertices that a tile owns (the ones within its chunk of
the vertex array). task4 is responsible for re-exploring the
local frontier. To avoid iterating over every 32-vertex block in
the bitmap frontier when task4 is invoked, task3 pushes
the ID of a new block to be explored (blk_id) into IQ4.
task4 then reads from its queue in FIFO order and pushes
the vertices into IQ1 so that they are processed again.

Termination: The program ends when all tiles are idle. This
is determined by aggregating a hierarchical, staged, idle signal
from all the tiles (co-located with the clock and reset signals).
Similar to a loosely-coupled accelerator [12], [49], the host

gets an interrupt when the global idle signal is set, to notify
that the work is completed.

Synchronization: Although we strive to avoid synchro-
nization between graph epochs, Dalorex also supports global
synchronization by reusing the chip idle signal. To have syn-
chronization per epoch, task3 should not push new frontier
vertices into the IQ of task4 and just add them to the bitmap
frontier. When all vertices are processed, the host detects that
the chip is idle, and it sends a message to all tiles to trigger
task4 and explore the new epoch. We characterize perfor-
mance with and without epoch synchronization in Section V.
Host: The host is a commodity CPU, that arranges the
load of the program binary and the dataset from disk to the
chip. The program, composed of tasks and memory-mapped
software conﬁgurations, is distributed as a broadcast and is
identical for all the allocated tiles. The data is distributed such
that every tile receives an equal-size chunk of each array.

Dalorex does not use virtual memory, although the host
processor can still use virtual addressing within its memory.
This avoids the overheads of address translation, which are
exacerbated in graph applications due to irregular memory
accesses. Another advantage of having private, uncontested
access to memory is not needing to deal with memory coher-
ence or consistency issues.

D. Graph size vs Chip size

Dalorex uses a homogeneous 2D layout of tiles. Because the
size of a tile’s local memory is determined at fabrication time,
the aggregated chip storage scales linearly with the computing
capacity (see Section V-B for optimal storage to compute
ratio). We envision Dalorex to be deployed at the edge with
a 16 × 16, 512 MB, 151mm2 chip, or at data-centers with a
256 × 256, 128GB, wafer-scale integration.

Processing Larger Graphs: Building on previous work on
distributed graph processing [28], [61], we propose using
graph partitioning to split larger graphs into multiple parts,
where each part is computed on a separate Dalorex chip.
The aggregated storage on chip determines the maximum size
of a graph partition. Smaller graphs can also be computed
concurrently in rectangular subsets of tiles within a chip,
uncontested, as they do not share hardware resources. The
tiles that are not allocated to run programs are switched off.
Selecting the number of tiles a program runs on has a lower
bound: it should run a subset of Dalorex that is large enough
to ﬁt the dataset into the aggregated memory capacity. It can
also choose to use a larger number of tiles to parallelize even
further. Section V shows that Dalorex scales close to linearly
until the parallelization limits are hit when a tile handles less
than a thousand vertices.

E. Dalorex Hardware

Fig. 4 shows the structure of a tile in the Dalorex archi-
tecture. The Processing Unit (PU) is a very power- and area-
efﬁcient unit that resembles a single-issue in-order core but
without a memory management unit or L1 cache, as the data
is accessed directly from the scratchpad memory. The tile

5

Fig. 4. Organization of a processing tile. TSU indicates the Processing Unit (PU) the next task to execute based on the occupancy of the task input queues
(IQ). The router connects TSU with the network. TSU has a read-write port to the local memory to write incoming network data (push) to IQs and pop
outgoing data from the channel queues (CQs).

area is dominated by the scratchpad SRAM, which contains
the data arrays, the code, and the input/output queue entries.
The queues are implemented as circular FIFOs using the
scratchpad. Queue sizes are conﬁgured at runtime based on
the number of entries speciﬁed next to the task declaration
(see Listing 1). A queue entry can be either 32 or 64 bits,
depending on the chip’s target total memory size. (A 32-bit
Dalorex can process graphs of up to 232 edges.)

The Task Scheduling Unit (TSU) is the key hardware
unit of Dalorex’s hardware-software co-design. It contains the
task conﬁgurations and scheduling policy and handles the
queues’ head and tail pointers. The TSU has a read-write
port into the scratchpad for pushing data from the router
buffers to the input queue. From the other side, the tail and
head pointers are exposed to software through Queue Speciﬁc
Registers (QSR). QSRs allow the PU to read or write from
its queues with a register operation, avoiding instructions to
calculate the address. A read from a QSR results in a load from
the scratchpad using the corresponding queue’s head pointer
provided by the TSU. This read also triggers the update of the
head pointer in hardware at the Task Queue Status table.

The TSU is responsible for invoking tasks based on the
status of the IQ and OQs. Tasks cannot block, so TSU invokes
a task only if its OQ has more than sixteen free entries (this
sets the minimum conﬁgurable queue size) and IQ is not
empty. TSU needs to arbitrate when two or more tasks have
non-empty IQs. The queues’ occupancy acts as a sensor for
the TSU to decide which task to prioritize. A task can have
three modes of priority based on the occupancy of its IQ and
OQ: high priority if its IQ is nearly full, medium priority if its
OQ is nearly empty, and low priority otherwise. When two or
more tasks have high/medium priority, the one with a larger
IQ/OQ size takes precedence.

After

testing several

static priorities and round-robin
schemes, we found that this occupancy-based priority worked
best for two reasons: (1) The primary source of network
contention is end-point back-pressure, so preventing IQs from
being full eases contention; (2) Reaching high resource utiliza-

tion relies on tiles giving each other work, so keeping OQs
non-empty is beneﬁcial. These heuristics are micro-coded as
event-condition-action rules and are set based on the reaction
time that it takes to prioritize a task and prevent the IQ from
getting full or the OQ empty. When all IQs are empty, TSU
disables the clock of the PU, to save power.

During program execution, when a task outputs the param-
eters for the next task to execute on another tile, a channel
queue places the data into the network, which delivers the
data to the destination. A network channel always connects
a channel queue with a task’s IQ. Messages can be composed
of several ﬂits, each being a parameter of the task to be called.
The network communication is composed of ﬂits traveling in
different logical channels that share the same network-on-chip
(NoC). A ﬂit has the same size as a queue entry, which is the
width of the PU’s ALU and the memory addresses. In our
experiments, we evaluated a 32-bit Dalorex.

The routers have bi-directional ports to north, south, east,
west, and towards the tile’s TSU. Routing is determined by
the router based on the data in the ﬁrst ﬂit of a message,
which we call the head. Since the head ﬂit always contains a
dataset-array index, and these arrays are statically distributed
across the chip, this index is used to obtain the destination tile.
The TSU’s channel table contains the sizes of the local chunks
and the number of parameters (ﬂits) of each message type. The
head encoder uses that information to calculate the destination
tile and the local index (modulo the chunk size). The encoder
also uses the width of the Dalorex rectangle to obtain the
X/Y coordinates. Depending on the width and height of the
chip, the upper bits of the head ﬂit encode the destination tile
(log2(width) + log2(height)). Routers compare incoming head
ﬂits with their local X/Y tile ID to determine where to route
it next. If routed to the TSU, the head decoder removes the
head ﬂit’s tile-index bits before pushing to the IQ.

The payload-based routing saves network trafﬁc as it does
not use metadata. The length of messages at each channel
is known, and its ﬂits are always routed back to back since
a route (from input to output port) opens with the ﬁrst ﬂit

6

Task Scheduling Unit (TSU) RouterProcessing Unit   Scratchpad Memory               (SRAM)TaskSchedulingUnitQueuesE  N  Tasks CodeData ArraysProcessing Tile S Read from InputQ Write to OutputQ Read/Write ArraysControl signals  to/from TSU  Instr. read/decode    Update PC T buffersTask  Control  LogicRouter-TSU Buffers InputQ push ArbitrationWrite to  MemRead from  MemOutputQ pop ArbitrationTask Conf & Queue Status buffer_id to write  mem_addr to wr OQ tail/head addr Next task addrCompleted  task Event signals from the PU32  Queue push/popIQ tail/head addr 32  buffers buffersbuffers +Head Decod.Head EncoderX_ID, Y_ID Width, HeightTile ConfigurationTask IDCode Addr Tail/Head/ Len/BaseNum. ParamOutput Queue123../../32/0x..../../128/0x..../../2048/0x..0x100x..  0x..  032CQ1CQ2IQ4 Occupancy  Full?  Channel Conf & Queue Status +Ch. ID Tail/Head/ Len/BaseChain LengthHead Encod. 12../../128/0x.. ../../1024/0x..EdgeNode3240x..  0IQ1../../1024/0x..- Occupancy  Full?  Addr & control signals to the PU  bufffer_id to rdmem_addr to read ---CLK  to PUClock-gateon whileactive tasksWbuffersand closes after the corresponding number of ﬂits has left the
router. Interleaving ﬂits from two messages going to the same
output port on the same channel is not allowed. The router
can route to different output ports simultaneously. However, it
arbitrates by doing a round-robin between the messages from
inputs ports that want to route to the same output port.

Channels buffers: In addition to identifying the task type,
communicating tasks in different channels prevents deadlocks.
Although channels might contend to use the NoC, the routers
contain buffers per channel so that a clogged channel does
not block others. Each router has a pool of buffer slots per
outbound direction shared between the channels. The size of
this pool is a tapeout parameter, but the number of buffer
slots per channel is software-conﬁgurable, as are the sizes of
the input/output queues.

F. Communication Network

Since Dalorex’s programming model uses task invocations
that do not return a value, communication is one way only,
resembling a software pipeline. Therefore, the communication
latency between the sender and receiver tiles does not con-
tribute to the execution time if the pipeline is full, i.e. task
invocations are continuous. However, the throughput would
suffer if bubbles are formed due to network contention.

Graphs vertices have variable degrees. Some vertices—often
referred to as hot vertices—have much higher vertex-degree
than others. The channel buffers and queues mitigate the
communication peaks caused by hot vertices and help keep
the pipeline effect. Dalorex relies on random data distribution,
so the number of hot vertices per tile is relatively uniform.
Should the graph be sorted by vertex degree, we build the
global CSR so that consecutive vertices fall into different tiles.
This uniformity avoids excessive end-point contention at the
TSU with messages from the router.

We observed that the network topology can be another
source of contention. Dalorex uses a 2D-torus to avoid the
contention towards the center we observed on a 2D-mesh.
Our 2D-torus is a wormhole network with dimension-ordered
routing. It also implements a local bubble routing to avoid
the ring deadlock. This network can be fabricated with nearly
tiles at a
equidistant wires by having consecutive logical
distance of two in the silicon. A 32-bit 2D-torus is 50% bigger
than a 2D-mesh, but the torus provides twice the bisection
bandwidth (BB) and 33% less number of hops [46].

Since we target the design of Dalorex to be scalable even
with hundreds of tiles per dimension (tens of thousands of
tiles), scalability of network utilization is part of the design
goal. Scaling Dalorex up to the next power-of-two tiles per
dimension results in four times the total tile count, but BB
only doubles. While latency is greatly hidden by the pipeline
effect, BB is critical for scalability since the vertex updates are
irregular and result in communication to any tile. This disparity
increases the contention with network sizes. To overcome that,
we also explore ruche networks [27], [46].

Ruche networks are long physical wires that bypass routers.
They increase the router radix and decrease the latency from

source to destination. For example, in a network with a ruche
factor of 2, a tile could route to their immediate neighbors
or tiles at a distance of 2. A full ruche network of factor R
increases the BB by a factor of (R − 1)× over the underlying
network, so increasing sizes of R with larger network sizes
can compensate for the previously observed BB decrease.
Section V provides a performance characterization of torus
vs. mesh, with and without ruche networks.

G. Scalable Memory Bandwidth

Large scratchpads are more area-efﬁcient with modern,
smaller, FinFET transistor nodes [10], [16], [63]. This allows
packing more SRAM on-chip than ever before. A real-world
example is the Wafer-Scale integration of Cerebras, which
enables 40GB of on-chip SRAM storage [35].

Having a dedicated scratchpad memory per tile enables
immediate, uncontested access to memory. A memory port has
a width equal to a network ﬂit. The PU can make one memory
read, and one write per cycle. This is leveraged by instructions
writing to QSR from data arrays. The PU has another port to
fetch instructions from the scratchpad. While the scratchpad
has many banks, to save per-access energy, not all banks need
to have all ports, e.g. the instruction port can exist only for a
fraction of the local memory, setting a limit to the code size.
Since each PU has all the data it needs locally, the total
memory bandwidth increases linearly with the number of tiles,
unlike many modern hardware architectures.

IV. EVALUATION METHODOLOGY

Below we describe the workloads we evaluated, and we
explain the simulation methodology, including the power and
area models for both Dalorex and the prior work.

A. Applications and Datasets

In addition to four graph algorithms, we evaluated one
sparse linear algebra kernel to demonstrate the generality of
our approach for memory-bound applications.

We adapted the following codes from the GAP bench-
marks [6] and GraphIt [70], splitting the program into tasks
at each indirect memory access: Breadth-First Search (BFS)
determines the number of hops from a root vertex to all
vertices reachable from it; Single-Source Shortest Path (SSSP)
ﬁnds the shortest path from the root to each reachable vertex;
PageRank ranks websites based on the potential ﬂow of users
to each page [33]; Weakly Connected Components (WCC)
ﬁnds and labels each set of vertices reachable from one to
all others in at least one direction (implemented using graph
coloring [55]); Sparse Matrix-Vector Multiplication (SPMV)
multiplies a sparse matrix with a dense vector.

We perform the evaluation using real-world networks and
synthetic datasets. We use several different sizes of synthetic
RMAT graphs [34] of up to 67M vertices (V) and 1.3B
edges (E), with up to 12GB of memory footprint, as well as
real-world graphs: LiveJournal (V=5.3M, E=79M), Wikipedia
(V=4.2M, E=101M) and Amazon (V=262K, E=1.2M).

7

B. Methodology

We built a C++ simulator to evaluate Dalorex due to the
novelty of the execution model and the simplicity of the
architecture. This simulator is cycle-accurate for both message
routing and task execution. While simulating the network,
messages ﬂow from tile to tile from source to destination,
spending one cycle at each step. Our simulations are vali-
dated to provide correct program outputs over sequential x86
executions of the applications we evaluate [6].

Current architectures pay an enormous energy cost asso-
ciated with accessing off-chip memory [44]. Reading data
locally consumes nearly two orders of magnitude less energy
than moving an equivalent block of data across 15mm of on-
chip wire [22] and three orders of magnitude less energy than
bringing the data from off-chip DDR3 [60].

Power and Area Model: With the latest technology, SRAM
can achieve very high densities in FinFET, ranging from 29.2
Mb/mm2 in 7nm [63] to 47.6 Mb/mm2 in 5nm [10]. New
technologies like FinFET or CMOS-ULVR show that leakage
power can get as low as 1.7nW per cell [31], 5µW per 8KB
(8192 bytes) macro [64], or 16.9µW for a 32KB macro at
7nm [63]. Although 5nm is achievable today [16], we will
be using 7nm for our area and power models since this is a
more mature transistor process, and we were able to ﬁnd more
reliable numbers for SRAM and logic. Reading a bank of data
consumes 5.8pJ, and writing it 9.1pJ, with an access time of
0.82 ns [63]. Thus, we use a 1GHz frequency for Dalorex to
ﬁt the memory access time within the cycle length.

In addition to area-efﬁcient SRAM technology, Dalorex uses
processing units that resemble slim cores. We estimate the
PU area considering the RISC-V Celerity, Snitch, and Ariane
cores [15], [66], [68]. To determine the dynamic and leakage
power of the single-issue in-order core, we use the reports from
Ariane [65] and transistor power-scaling ratios to calculate the
energy of those operations on a 7nm process [56], [62].

Regarding the NoC, we explore a 2D-Torus and a 2D-mesh,
with and without ruche networks. These are modeled hop by
hop between the routers, making the simulator precise for
cycle count and energy. We use 8pJ as the energy to move
a 32-bit ﬂit one millimeter [39] and assume the energy of
moving a ﬂit at the router to be similar to an ALU operation.
The NoC area is calculated based on Ou et al. [46]. These NoC
options are explored in Fig. 8, and for other results Dalorex
uses a regular torus NoC for grids up to 32×32 (1024 tiles)
and a torus NoC with ruche channels for larger grids.

C. Comparison with the State-of-the-Art

We looked at the literature in graph accelerators, i.e., those
with a specialized hardware pipeline to process the different
stages of graph traversing, and focused on Polygraph [14]—
the latest work. We evaluated the code that the authors kindly
provided and experimentally conﬁrmed that Polygraph’s per-
formance plateaus with conﬁgurations larger than 16 cores,
while Dalorex continues to scale. This is expected since
that conﬁguration already saturates the 512GB/s of DRAM

bandwidth provided by their 8 memory controllers using High-
Bandwidth Memory (HBM).

However, PIM-based proposals have a memory system that
scales with the amount of computing resources. From these
works, we compare ourselves with Tesseract [2]. We have sim-
ulated a more recent PIM-based approach, GraphQ [74], which
reported 3.3× performance and 1.8× energy improvements on
average over Tesseract. However, despite communicating with
the authors and using their methodology, we were unable to
obtain the cycle and energy values from simulation outputs
that matched their results. We do not include GraphQ in our
discussion because we obtained 3.2× and 4.6× larger runtime
and energy consumption (in geomean) than in [74].

We evaluate Tesseract using a Hybrid Memory Cube (HMC)
conﬁguration of 16 cores per cube (one per vault), aggregating
a total of 256 cores among the 16 cubes. To match their core
count, we use a 16 × 16 Dalorex grid with 4.2MB of memory
per tile. To simulate Tesseract, we follow their methodology
by using the Zsim simulator [53] with a 3D-memory power
model [50] (based on Micron’s disclosure for HMC). For the
energy spent by the cores, we use the same power model
as Dalorex—based on a 7nm transistor node. We compare
runtime and energy performance for the duration of the graph
processing time, not considering loading the dataset from disk
to HMC or to the Dalorex chip. Although Tesseract already
showed signiﬁcant speedups over server-class OoO systems,
we collected the cycle count of our x86 server while validating
the output of graph applications, to conﬁrm that both Tesseract
and Dalorex perform much better thanks to their scalable
memory bandwidth (MBW).

V. RESULTS

A. Improvements over the HMC-based prior work

This section demonstrates the gains in performance and
energy efﬁciency of Dalorex over Tesseract [2], both using
an equal amount of cores (256). We break down the impact
of the different optimizations of Dalorex by starting with
Tesseract and then evaluating Dalorex by adding one feature
at a time to gradually reach full Dalorex.

We provision Tesseract with a 2MB private cache per core
(512 MB of aggregate SRAM) and remove DRAM refresh
energy, to approximate the impact of using distributed SRAM
on performance and energy (Tesseract − LC). We evaluate
Dalorex without TSU by using the program ﬂow of Tesseract
(with remote vertex updates interrupting) to study the impact
of vertex-based (Tesseract) versus all-arrays-equal (Dalorex)
data and task splitting (Data − Local); We add a basic TSU
to invoke tasks with round-robin scheduling to evaluate the
impact of non-blocking and non-interrupting communication
(Basic-TSU). We distribute the data arrays by lower order
bits instead of higher order bits to evaluate the impact of
data placement. (Uni f orm − distr). We add the scheduling
policy that gives priority to tasks based on queue occupancy
(Tra f f ic − aware). We use a 2D-Torus NoC instead of a 2D-
Mesh (Torus − NoC). Finally, we removed the global barrier
synchronization after each epoch, reaching Dalorex − f ull.

8

Section V-C further analyzes the impact of the NoC by

testing one more design that beneﬁts larger Dalorex grids.

Fig. 5.
Improvements in performance and energy efﬁciency achieved with
the addition of large caches to Tesseract, and six Dalorex optimizations,
normalized to Tesseract. All conﬁgurations use 256 processing cores. X-axis
studies four datasets. Y-axis is logarithmic, higher is better. Note that since
PageRank necessitates per-epoch synchronization, the last datapoint (Dalorex-
full) still uses a global barrier; thus, the bars do not change.

Performance:

Fig. 5 (above) shows that Dalorex sub-
stantially outperforms Tesseract across all datasets and ap-
plications. The dataset-chunking strategy and the data-local
execution on Dalorex’s homogeneous architecture improves
performance by 6.2× (Data − Local). On top of that, non-
blocking and non-interrupting task invocation through TSU
improve performance by 4.7×, uniform data placement im-
proves another 2.6×, and trafﬁc-aware scheduling 1.7× more.
Finally, removing the barriers and upgrading the NoC pro-
vides an extra 1.8×, totaling a compound 221× geomean
improvement. Synchronization in graph workloads causes each
epoch to take as long as the slowest tile’s execution, increasing
the total runtime. WCC, having more epochs, beneﬁts the
most from barrierless processing. Similarly, Wikipedia (WK)
beneﬁts most from removing the barrier as its graph structure
leads to more epochs.

Energy: Fig. 5 (below) shows that the performance im-
provement of Dalorex over Tesseract, together with its power-
efﬁcient design, yields a very large improvement
in en-
ergy consumption. The compound improvements from using
SRAM technology (16×), our architecture and data layout
(5.2×), and TSU (3.9×),
total a geomean of 325×. The
breakdown shows that the energy of refreshing DRAM has

9

the biggest impact on Tesseract (also concluded in their paper).
Upgrading the NoC to a 2D-Torus increases energy usage by
12% geomean, besides being 40% faster. This is because the
Torus network is more power hungry and has longer wires.

Area: The 16 × 16 Dalorex with a 4.2MB memory per
tile uses much less chip area (305mm2) than the aggregated
area of the 16 cubes of Tesseract (3616mm2). The prior work
based on HMC is constrained by the number of cores that
can be integrated into the logic die of a memory cube—often
one per vault. Since a cube contains 8GB, a core accesses a
512MB DRAM vault. Most of this DRAM space is unused
with the datasets that Tesseract evaluated (the empty bitlines
of DRAM are switched off to save power). However, larger
datasets would have a much bigger runtime in the HMC-based
design since it cannot have more cores without increasing the
number of memory cubes.

HMC is also limited in scalability due to the large power
density, which makes it more challenging to cool
in 3D
integrations [73]. In Dalorex, power is evenly distributed
and so power-density stays below 300mW/mm2 for all our
experiments. This is much below the limits of air-cooled 2D
chips (∼1.5W/mm2 [71]). Another limitation of the HMC-
lower inter-cube communication
based approaches is that
bandwidth made their authors consider graph partitioning per
cube, limiting the scalability of each partition.

To summarize, such large across-the-board improvements
are achieved with a conjunction of optimizations at different
levels of the software-hardware stack. Dalorex: (1) reduces
data movement with its fully data-local program execution
model, where only task parameters are moved, and there
is no round-trip; (2) task invocations are natively supported
through the TSU, so there are no interrupt overheads as in
prior remote procedure calls; (3) does not require barrier
synchronization between tiles for BFS, WCC, and SSSP;
(4) coalesces vertex updates into each tile’s local frontier,
minimizing redundancy; (5) utilizes SRAM to store the tile-
distributed dataset, making data access immediate and energy-
efﬁcient for the challenging ﬁne-grain irregular accesses. Also,
SRAM is offered at ﬁner sizes than DRAM. This enables
Dalorex to use a few megabytes of memory per tile, which
we found to be energy-optimal in the scaling experiments of
the next section.

B. Dalorex Scales Beyond Thousands of Tiles

We evaluate strong scaling for BFS with increasing sizes of
Dalorex grids, and four RMAT datasets of size 216, 222, 225
and 226 vertices (average ten edges per vertex).

Performance Scaling: The upper plot of Fig. 6 shows the
runtime (in cycles) of executing each dataset on Dalorex with
an increasing number of tiles by multiples of four. The most
exciting result of this analysis is that Dalorex has a close to
linear scaling until it hits the parallelization limit. This occurs
when the chunk of data per tile is ∼1,000 vertices, indicating
that performance is not limited by MBW (as is the case for
all previous work) but tiles starving for work.

AZWKLJR22100101102103PerformanceImprovementBFSAZWKLJR22100101102103WCCAZWKLJR22100101102103PageRankAZWKLJR22100101102103SSSPTesseractTesseract-LCData-LocalBasic-TSUUniform-DistrTraﬃc-AwareTorus-NoCDalorexAZWKLJR22100101102103EnergyImprovementBFSAZWKLJR22100101102103WCCAZWKLJR22100101102103PageRankAZWKLJR22100101102103SSSPTesseractTesseract-LCData-LocalBasic-TSUUniform-DistrTraﬃc-AwareTorus-NoCDalorexFig. 7. Throughput in terms of executed instructions and edges per second,
and the average on-chip memory bandwidth (MBW) that was used to achieve
that. The X-axis is the size of the Dalorex grid used when analyzing strong
scaling RMAT-26, ranging from 256 to 16.384 tiles. The Y-axis is logarithmic.

Fig. 8. Performance improvement of Torus and Torus-Tuche over Mesh. The
X-axis shows the datasets used for each of the applications evaluated. RMAT-
26 runs on 64×64 tiles, and the rest 16×16.

starve for tasks. While approaching the parallelization limit of
a dataset, energy increases because of the leakage energy of
the additional PUs that are not fully utilized.

Throughput: Fig. 7 shows the number of edges and oper-
ations processed per second as a measure of the throughput
and the average aggregated MBW by all tiles. We study this
with doubling counts on X- and Y-dimension and for all
of our benchmarks running our biggest RMAT dataset. We
observe that both throughput and utilized MBW grow until the
largest conﬁguration we simulated: 128 × 128 (214, i.e., 16384
tiles). This last conﬁguration reaches 2 tera-operations/s, using
over 10 terabytes/s of MBW. The throughput and MBW of
Dalorex for graph applications are beyond reach for prior
work in hardware accelerators using HBM and the 3D-memory
integration that Tesseract proposed. Dalorex MBW is possible
by having all the memory storage distributed across tiles and
linearly increasing the number of memory ports with the tile
count. The fact that the MBW does not saturate allows the PUs
to process vertices and edges at the very high rates shown in
Fig. 7. Note that what we reported is the average MBW of the
whole program and not the maximum utilized at a given time.
The peak MBW available only increases with the number of
tiles, and it is 131TB/s on a 128 × 128 Dalorex.

C. Characterizing the Network-on-Chip (NoC)

We analyze Dalorex with the three NoC types that we con-
sidered in Section III-F: 2D-mesh, 2D-torus, and combining
the torus with ruche channels. Fig. 8 shows the performance
improvement of both torus options over the mesh. Using a

Fig. 6. Analysis of the runtime (cycles) and energy consumption (Joules) of
BFS for four RMAT datasets and scaling core counts.

Energy Scaling: To understand the optimal scratchpad size
for energy consumption, we analyzed energy as the number
of tiles increases and the required scratchpad size decreases.
Each scratchpad stores the dataset-chunk, the program binary,
and the queues. Fig. 6 (bottom) shows the total energy spent to
run BFS, for different numbers of tiles. Next to each datapoint
is the memory used by each tile (in KB).

As the tile count increases and the dataset-chunk per tile
becomes smaller, energy ﬁrst decreases to a minimum and then
increases. This is mainly driven by leakage power. Since the
aggregated memory capacity remains nearly constant in this
experiment, the SRAM leakage power becomes less signiﬁcant
with larger parallelizations. Total energy keeps decreasing
while the cores are fully utilized. The deﬂection point, i.e.,
minimal energy execution for each dataset, arrives when the
amount of data per tile is ∼10,000 vertices. This optimal range
is invariant with dataset size. The smallest dataset (RMAT-16)
reaches this point very early, being already past that with 64
tiles, where each tile holds ∼1,000 vertices.

Fig. 6 shows that the performance keeps scaling beyond
the energy-optimal conﬁguration. Unlike clusters, where per-
formance degrades with small messages between computing
nodes, Dalorex communication is at a word granularity. Scal-
ing is not limited by MBW but only when the workload per
tile is so low that we lose the pipeline effect and the PUs

10

100101102103104Corecount1041051061071081091010Runtime(Cycles)Programruntime:BFSonRMATdatasetsRMAT16RMAT22RMAT25RMAT26100101102103104Corecount10−410−310−210−1100101102Energy(Joules)1109291863523198578215955315211398286372957251445Energyusage:BFSonRMATdatasetsRMAT16RMAT22RMAT25RMAT262821021221410910101011101210131014ScalingRMAT-26ThoughputBFS2821021221410910101011101210131014WCC2821021221410910101011101210131014PageRank2821021221410910101011101210131014SSSP2821021221410910101011101210131014SPMVEdges/sOperations/sAvg.Mem.BW(Bytes/s)WKLJR22R260123456789PerformanceImprovementBFSWKLJR22R260123456789WCCWKLJR22R260123456789PageRankWKLJR22R260123456789SSSPWKLJR22R260123456789SPMVMeshTorusTorus-Ruche(a) 2D-Mesh

(b) 2D-Torus network

Fig. 9. Heatmaps of the utilization of PUs and routers while running SSSP
on RMAT-22. The 16×16 tiles are connected by a mesh (above) or by a torus
(below).

16 × 16 torus is nearly twice as fast as a mesh, on average,
for the smaller datasets (Wikipedia, LiveJournal, and RMAT-
22), which justiﬁes the area cost of an additional 0.2% of the
total chip area (using 4MB tiles).

We generated heatmaps of the utilization of PUs and routers
(as a percentage of the total runtime) to visually demonstrate
the advantage of torus over mesh. Fig. 9 shows that
the
contention towards the center of the mesh (above) clogs the
NoC and makes the PUs starve for tasks, whereas the torus
(below) has a uniform router utilization, unleashing the full
potential of the PUs.

As shown in Fig. 8, the improvement of the torus NoC
becomes even greater for the 64 × 64 grid used to evaluate
RMAT-26. We found ruche with torus to only improve per-
formance on the large grid. Despite that the ruche-torus NoC
uses more than twice the area of a regular torus (extra cost
of 1.2% area) and higher power, the performance gains at the
64 × 64 grid justify its use. Although not shown here, we have
found ruche combined with mesh not to be as effective as torus
alone, despite its larger area cost.

Fig. 10 breaks down the energy consumed by Dalorex. We
use a 16 × 16 grid to run WK, LJ and RMAT-22, and 64 × 64
to run RMAT-26. In Dalorex, the network consumes the most
energy: the larger the network, the longer the average distance
traveled to update a vertex, therefore a greater share of the total
energy consumption. This is expected because Dalorex uses
energy-efﬁcient memories and very simple processing units
(PU). PUs are also not actively waiting for messages to arrive
but are powered off by TSU when idle.

VI. FURTHER RELATED WORK & DISCUSSION

Due to storage limitations, graph networks that have trillions
of edges inevitably need to be partitioned and processed by

11

Fig. 10. Breakdown of the energy consumed by the computing logic, the
SRAM cells, and the network communication (including routing and wire
energy). The Y-axis is the percentage of total energy spent running the
program. The X-axis shows the datasets used for each of the applications
evaluated. RMAT-26 runs on 64x64 tiles, while the rest on 16x16 tiles.

multiple systems. However, within each system (where each
partition is processed), distributed graph processing frame-
works [3], [37], [61], [72] are bottlenecked by the memory
hierarchies in current computer architectures.

Dalorex, and other HMC-based approaches for graph pro-
cessing [2], [69], [74] behave as a large accelerator with re-
spect to the host CPU. Loosely-coupled accelerators for graph
applications have been investigated before, e.g. [14], [19],
[47]. However, ASIC accelerators cannot execute the variety
of workloads that ISA-programmable processors can. Dalorex
is not restricted to executing graph applications and can be
used for other domains since it is software programmable.

Unlike the prior work, which assumes an HMC technology
that has not been adopted, there is already an example of an
architecture where all the memory is distributed on a chip
that is being commercialized by Cerebras [9], [35] to speed
up Machine Learning applications.

As we show in this paper, the Dalorex execution model
and design decisions like network type, local-memory size,
and task scheduling unleash an outstanding graph processing
performance for distributed-memory architectures.

Another example of an architecture that could use Dalorex
is a regular memory hierarchy with large L1 caches per tile,
leveraging prior work in cache-scratchpad duality [11], [13],
[30] to provide a hybrid solution that would beneﬁt both cache-
averse and cache-friendly workloads.

VII. CONCLUSION

Dalorex is designed to massively parallelize applications
that are memory-bound due to the irregular memory ac-
cesses arising from pointer indirection. Since Dalorex is ISA-
programmable, it is applicable to any application ﬁtting in the
task-based programming model, although it is most advanta-
geous for those bottlenecked by pointer indirection or atomic
operations, e.g., histogram.

In this paper, we demonstrate strong scaling on four graph
algorithms and one sparse linear algebra kernel. We found
that reaching the parallelization limits requires: (1) a uniform
work balance, which we achieve with an equal amount of
(2) architectural and programming support to
data per tile;
invoke remote tasks natively with no message overhead;
(3)

01234567891011121314150123456789101112131415PUutilization(%oftime)02040608010001234567891011121314150123456789101112131415Routerutilization(%oftime)02040608010001234567891011121314150123456789101112131415PUutilization(%oftime)02040608010001234567891011121314150123456789101112131415Routerutilization(%oftime)020406080100WKLJR22R26020406080100EnergyBreakdown(%oftotal)BFSWKLJR22R26020406080100WCCWKLJR22R26020406080100PageRankWKLJR22R26020406080100SSSPWKLJR22R26020406080100SPMVLogicMemoryNetworka trafﬁc-aware arbitration of tasks; (4) a NoC that minimizes
contention, where a torus is superior to the mesh; and (5)
a NoC that scales bisection bandwidth with very large tile
counts on a 2D silicon, where we found ruche channels
compelling. Together with the data partitioning scheme that
allows barrierless frontiers, these optimizations make Dalorex
two orders of magnitude faster and more energy-efﬁcient than
the state-of-the-art in PIM-based graph processing.

REFERENCES

[1] M. Abeydeera and D. Sanchez, “Chronos: Efﬁcient speculative paral-
lelism for accelerators,” in Proceedings of the Twenty-Fifth International
Conference on Architectural Support for Programming Languages and
Operating Systems, 2020, pp. 1247–1262.

[2] J. Ahn, S. Hong, S. Yoo, O. Mutlu, and K. Choi, “A scalable processing-
in-memory accelerator for parallel graph processing,” in Proceedings of
the 42nd Annual International Symposium on Computer Architecture,
2015, pp. 105–117.

[3] “Apache Giraph,” The Apache Software Foundation, http://giraph.

apache.org/.

[4] J. Balkind, M. McKeown, Y. Fu, T. Nguyen, Y. Zhou, A. Lavrov,
M. Shahrad, A. Fuchs, S. Payne, X. Liang, M. Matl, and D. Went-
zlaff, “OpenPiton: An open source manycore research framework,” in
ASPLOS. ACM, 2016, pp. 217–232.

[5] S. Beamer, K. Asanovic, and D. Patterson, “Direction-optimizing
breadth-ﬁrst search,” in SC ’12: Proceedings of the International Con-
ference on High Performance Computing, Networking, Storage and
Analysis, 2012, pp. 1–10.

[6] S. Beamer, K. Asanovi´c, and D. Patterson, “The gap benchmark suite,”

arXiv preprint arXiv:1508.03619, 2015.

[7] M. Besta, M. Podstawski, L. Groner, E. Solomonik, and T. Hoeﬂer,
“To push or to pull: On reducing communication and synchronization
in graph computations,” CoRR, vol. abs/2010.16012, 2020. [Online].
Available: https://arxiv.org/abs/2010.16012

[8] E. G. Boman, K. D. Devine, and S. Rajamanickam, “Scalable matrix
computations on large scale-free graphs using 2d graph partitioning,”
in Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis, 2013, pp. 1–12.

[9] Cerebras Systems Inc., “The second generation wafer scale en-
gine,” https://cerebras.net/wp-content/uploads/2021/04/Cerebras-CS-2-
Whitepaper.pdf.

[10] T.-Y. J. Chang, Y.-H. Chen, W.-M. Chan, H. Cheng, P.-S. Wang, Y. Lin,
H. Fujiwara, R. Lee, H.-J. Liao, P.-W. Wang, G. Yeap, and Q. Li, “A
5-nm 135-mb sram in euv and high-mobility channel ﬁnfet technology
with metal coupling and charge-sharing write-assist circuitry schemes
for high-density and low-¡italic¿v¡/italic¿¡sub¿min¡/sub¿ applications,”
IEEE Journal of Solid-State Circuits, vol. 56, no. 1, pp. 179–187, 2021.
[11] C. C. Chou, A. Jaleel, and M. K. Qureshi, “Cameo: A two-level memory
organization with capacity of main memory and ﬂexibility of hardware-
managed cache,” in 2014 47th Annual IEEE/ACM International Sympo-
sium on Microarchitecture.

IEEE, 2014, pp. 1–12.

[12] E. Cota, G. D. Guglielmo, P. Mantovani, and L. Carloni, “An analysis
of accelerator coupling in heterogeneous architectures,” in Proceedings
of the 52nd Design Automation Conference (DAC), 2015.

[13] E. G. Cota, P. Mantovani, and L. P. Carloni, “Exploiting private local
memories to reduce the opportunity cost of accelerator integration,” in
Proceedings of the 2016 International Conference on Supercomputing,
2016, pp. 1–12.

[14] V. Dadu, S. Liu, and T. Nowatzki, “Polygraph: Exposing the value
of ﬂexibility for graph processing accelerators,” in 2021 ACM/IEEE
48th Annual International Symposium on Computer Architecture (ISCA).
IEEE, 2021, pp. 595–608.

[15] S. Davidson, S. Xie, C. Torng, K. Al-Hawai, A. Rovinski, T. Ajayi,
L. Vega, C. Zhao, R. Zhao, S. Dai, A. Amarnath, B. Veluri, P. Gao,
A. Rao, G. Liu, R. K. Gupta, Z. Zhang, R. Dreslinski, C. Batten,
and M. B. Taylor, “The celerity open-source 511-core risc-v tiered
accelerator fabric: Fast architectures and design methodologies for fast
chips,” IEEE Micro, vol. 38, no. 2, pp. 30–41, 2018.

[16] S. Enjapuri, D. Gujjar, S. Sinha, R. Halli, and M. Trivedi, “A 5nm
wide voltage range ultra high density sram design for l2/l3 cache
applications,” in 2021 34th International Conference on VLSI Design

and 2021 20th International Conference on Embedded Systems (VLSID),
2021, pp. 151–156.

[17] Esperanto Technologies, “Esperanto’s et-minion on-chip risc-v cores,”

https://www.esperanto.ai/technology/.

[18] T. J. Ham, J. L. Arag´on, and M. Martonosi, “DeSC: Decoupled supply-
compute communication management for heterogeneous architectures,”
in MICRO. ACM, 2015.

[19] T. J. Ham, L. Wu, N. Sundaram, N. Satish, and M. Martonosi,
“Graphicionado: A high-performance and energy-efﬁcient accelerator
for graph analytics,” in Proceedings of the 49th Annual International
Symposium on Microarchitecture,
[Online].
Available: https://doi.org/10.1109/MICRO.2016.7783759

ser. MICRO, 2016.

[20] M. Han and K. Daudjee, “Giraph unchained: Barrierless asynchronous
parallel execution in pregel-like graph processing systems,” Proceedings
of the VLDB Endowment, vol. 8, no. 9, pp. 950–961, 2015.

[21] J. L. Hennessy and D. A. Patterson, “A new golden age for computer
architecture,” Communications of the ACM, vol. 62, no. 2, pp. 48–60,
2019.

[22] R. Ho, K. W. Mai, and M. A. Horowitz, “The future of wires,”

Proceedings of the IEEE, vol. 89, no. 4, pp. 490–504, 2001.

[23] H. Hoffmann, “Stream algorithms and architecture,” Ph.D. dissertation,

Massachusetts Institute of Technology, 2003.

[24] “Hybrid Memory Cube (HMC),” http://www.hybridmemorycube.org,

2018.

[25] K. T. Johnson, A. R. Hurson, and B. Shirazi, “General-purpose systolic

arrays,” Computer, vol. 26, no. 11, pp. 20–31, 1993.

[26] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
S. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-datacenter
performance analysis of a tensor processing unit,” in Proceedings of the
44th annual international symposium on computer architecture, 2017,
pp. 1–12.

[27] D. C. Jung, S. Davidson, C. Zhao, D. Richmond, and M. B. Taylor,
“Ruche networks: Wire-maximal, no-fuss nocs: Special session paper,”
in 2020 14th IEEE/ACM International Symposium on Networks-on-Chip
(NOCS).

IEEE, 2020, pp. 1–8.

[28] G. Karypis and V. Kumar, “Metis: A software package for partitioning
unstructured graphs,” Partitioning Meshes, and Computing Fill-Reducing
Orderings of Sparse Matrices, Version, vol. 4, no. 0, 1998.

[29] S. Knowles, “Graphcore,” in 2021 IEEE Hot Chips 33 Symposium

(HCS).

IEEE, 2021, pp. 1–25.

[30] S. Kumar, H. Zhao, A. Shriraman, E. Matthews, S. Dwarkadas, and
L. Shannon, “Amoeba-cache: Adaptive blocks for eliminating waste in
the memory hierarchy,” in 2012 45th Annual IEEE/ACM International
Symposium on Microarchitecture.

IEEE, 2012, pp. 376–388.

[31] T. S. Kumar and S. L. Tripathi, “Process evaluation in ﬁnfet based 7t
sram cell,” Analog Integrated Circuits and Signal Processing, pp. 1–7,
2021.

[32] H. T. Kung and C. E. Leiserson, “Systolic arrays for (vlsi).” Carnegie-

Mellon University, Tech. Rep., 1978.

[33] P. Lawrence, B. Sergey, R. Motwani, and T. Winograd, “The pagerank
citation ranking: Bringing order to the web,” Stanford University,
Technical Report, 1998.

[34] J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, and Z. Ghahra-
mani, “Kronecker graphs: An approach to modeling networks,” Journal
of Machine Learning Reseach (JMLR), vol. 11, pp. 985–1042, Mar.
2010.

Chips 33 Symposium (HCS).

[35] S. Lie, “Multi-million core, multi-wafer ai cluster,” in 2021 IEEE Hot
IEEE Computer Society, 2021, pp. 1–41.
[36] E. Lockerman, A. Feldmann, M. Bakhshalipour, A. Stanescu, S. Gupta,
D. Sanchez, and N. Beckmann, “Livia: Data-centric computing
throughout the memory hierarchy,” in Proceedings of the Twenty-Fifth
International Conference on Architectural Support for Programming
Languages and Operating Systems, ser. ASPLOS ’20. New York,
NY, USA: Association for Computing Machinery, 2020, p. 417–433.
[Online]. Available: https://doi.org/10.1145/3373376.3378497

[37] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser,
and G. Czajkowski, “Pregel: a system for large-scale graph processing,”
in Proceedings of the 2010 ACM SIGMOD International Conference on
Management of data, 2010, pp. 135–146.

[38] A. Manocha, T. Sorensen, E. Tureci, O. Matthews, J. L. Arag´on,
and M. Martonosi, “Graphattack: Optimizing data supply for graph
applications on in-order multicore architectures,” ACM Transactions on
Architecture and Code Optimization (TACO), vol. 18, no. 4, pp. 1–26,
2021.

12

[39] M. McKeown, A. Lavrov, M. Shahrad, P. J. Jackson, Y. Fu, J. Balkind,
T. M. Nguyen, K. Lim, Y. Zhou, and D. Wentzlaff, “Power and energy
characterization of an open source 25-core manycore processor.” in
HPCA, 2018, pp. 762–775.

[40] G. E. Moore, “Cramming more components onto integrated circuits,”

Proceedings of the IEEE, vol. 86, no. 1, pp. 82–85, 1998.

[41] A. Mukkara, N. Beckmann, and D. Sanchez, “PHI: Architectural Sup-
port for Synchronization-and Bandwidth-Efﬁcient Commutative Scatter
Updates,” in Proceedings of the 52nd annual IEEE/ACM international
symposium on Microarchitecture (MICRO-52), October 2019.

[42] Q. M. Nguyen and D. Sanchez, “Pipette: Improving core utilization on
irregular applications through intra-core pipeline parallelism,” in 2020
53rd Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO).

IEEE, 2020, pp. 596–608.

[43] Q. M. Nguyen and D. Sanchez, “Fifer: Practical acceleration of
irregular applications on reconﬁgurable architectures,” in MICRO-54:
54th Annual IEEE/ACM International Symposium on Microarchitecture,
ser. MICRO ’21. New York, NY, USA: Association for Computing
Machinery, 2021, p. 1064–1077. [Online]. Available: https://doi.org/10.
1145/3466752.3480048

[44] T. M. Nguyen and D. Wentzlaff, “Morc: A manycore-oriented com-
pressed cache,” in Proceedings of the 48th International Symposium on
Microarchitecture, 2015, pp. 76–88.

[45] M. Orenes-Vera, A. Manocha, J. Balkind, F. Gao, J. L. Arag´on, D. Went-
zlaff, and M. Martonosi, “Tiny but mighty: designing and realizing
scalable latency tolerance for manycore socs.” in ISCA, 2022, pp. 817–
830.

[46] Y. Ou, S. Agwa, and C. Batten, “Implementing low-diameter on-
chip networks for manycore processors using a tiled physical design
methodology,” in 2020 14th IEEE/ACM International Symposium on
Networks-on-Chip (NOCS).

IEEE, 2020, pp. 1–8.

[47] M. M. Ozdal, S. Yesil, T. Kim, A. Ayupov, J. Greth, S. Burns, and
O. Ozturk, “Energy efﬁcient architecture for graph analytics accelera-
tors,” ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp.
166–177, 2016.

[48] J. T. Pawlowski, “Hybrid memory cube (hmc),” in 2011 IEEE Hot Chips

23 Symposium (HCS).

IEEE, 2011, pp. 1–24.

[49] L. Piccolboni, P. Mantovani, G. Di Guglielmo, and L. P. Carloni,
“Broadening the exploration of the accelerator design space in embedded
scalable platforms,” in HPEC.

IEEE Press, 2017.

[50] S. H. Pugsley, J. Jestes, H. Zhang, R. Balasubramonian, V. Srinivasan,
A. Buyuktosunoglu, A. Davis, and F. Li, “Ndc: Analyzing the impact of
3d-stacked memory+ logic devices on mapreduce workloads,” in 2014
IEEE International Symposium on Performance Analysis of Systems and
Software (ISPASS).
IEEE, 2014, pp. 190–200.

[51] S. Rahman, N. Abu-Ghazaleh, and R. Gupta, “Graphpulse: An event-
driven hardware accelerator for asynchronous graph processing,” in 2020
53rd Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO).

IEEE, 2020, pp. 908–921.

[52] K. Rupp, “42 Years of Microprocessor Trend Data,” https://www.
karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/, 2018.
[53] D. Sanchez and C. Kozyrakis, “Zsim: Fast and accurate microarchitec-
tural simulation of thousand-core systems,” ACM SIGARCH Computer
architecture news, vol. 41, no. 3, pp. 475–486, 2013.

[54] K. S. Shim, M. Lis, M. H. Cho, O. Khan, and S. Devadas, “System-level
optimizations for memory access in the execution migration machine
(em2),” CAOS, 2011.

[55] G. M. Slota, S. Rajamanickam, and K. Madduri, “BFS and coloring-
based parallel algorithms for strongly connected components and
related problems,” in 2014 IEEE 28th International Parallel and
Distributed Processing Symposium, Phoenix, AZ, USA, May 19-23,
2014.
IEEE Computer Society, 2014, pp. 550–559. [Online]. Available:
https://doi.org/10.1109/IPDPS.2014.64

[56] A. Stillmaker and B. Baas, “Scaling equations for the accurate prediction
of cmos device performance from 180 nm to 7 nm,” Integration, vol. 58,
pp. 74–81, 2017.

[57] N. Talati, K. May, A. Behroozi, Y. Yang, K. Kaszyk, C. Vasiladiotis,
T. Verma, L. Li, B. Nguyen, J. Sun, J. M. Morton, A. Ahmadi, T. Austin,

M. O’Boyle, S. Mahlke, T. Mudge, and R. Dreslinski, “Prodigy: Im-
proving the memory latency of data-indirect irregular workloads using
hardware-software co-design,” in 2021 IEEE International Symposium
on High-Performance Computer Architecture (HPCA).
IEEE, 2021, pp.
654–667.

[58] M. Taylor, W. Lee, S. P. Amarasinghe, and A. Agarwal, “Scalar operand
networks,” IEEE Transactions on Parallel and Distributed Systems,
vol. 16, no. 2, pp. 145–162, 2005.

[59] M. B. Taylor, J. Kim, J. Miller, D. Wentzlaff, F. Ghodrat, B. Greenwald,
H. Hoffman, P. Johnson, J.-W. Lee, W. Lee et al., “The raw micropro-
cessor: A computational fabric for software circuits and general-purpose
programs,” IEEE micro, vol. 22, no. 2, pp. 25–35, 2002.

[60] M. Technology, “Ddr3/ddr4 system-power calculator,” https://www.

micron.com/support/tools-and-utilities/power-calc.

[61] Y. Tian, A. Balmin, S. A. Corsten, S. Tatikonda, and J. McPherson,
“From” think like a vertex” to” think like a graph”,” Proceedings of the
VLDB Endowment, vol. 7, no. 3, pp. 193–204, 2013.

[62] Q. Xie, X. Lin, Y. Wang, S. Chen, M. J. Dousti, and M. Pedram,
“Performance comparisons between 7-nm ﬁnfet and conventional bulk
cmos standard cell libraries,” IEEE Transactions on Circuits and Systems
II: Express Briefs, vol. 62, no. 8, pp. 761–765, 2015.

[63] Y. Yokoyama, M. Tanaka, K. Tanaka, M. Morimoto, M. Yabuuchi,
Y. Ishii, and S. Tanaka, “A 29.2 mb/mm2 ultra high density sram macro
using 7nm ﬁnfet technology with dual-edge driven wordline/bitline and
write/read-assist circuit,” in 2020 IEEE Symposium on VLSI Circuits,
2020, pp. 1–2.

[64] H. Yoshida, Y. Shiotsu, D. Kitagata, S. Yamamoto, and S. Sugahara,
“Ultralow-voltage retention sram with a power gating cell architecture
using header and footer power-switches,” IEEE Open Journal of Circuits
and Systems, vol. 2, pp. 520–533, 2021.

[65] F. Zaruba and L. Benini, “The cost of application-class processing:
Energy and performance analysis of a linux-ready 1.7-ghz 64-bit risc-
v core in 22-nm fdsoi technology,” IEEE Transactions on Very Large
Scale Integration (VLSI) Systems, vol. 27, no. 11, pp. 2629–2640, Nov
2019, https://github.com/openhwgroup/cva6.

[66] F. Zaruba and L. Benini, “Ariane: An open-source 64-bit RISC-V
techni-
application class processor and latest
cal talk at the RISC-V Workshop https://www.youtube.com/watch?v=
8HpvRNh0ux4.

improvements,” 2018,

[67] F. Zaruba, F. Schuiki, and L. Benini, “Manticore: A 4096-core risc-
v chiplet architecture for ultraefﬁcient ﬂoating-point computing,” IEEE
Micro, vol. 41, no. 2, pp. 36–42, 2020.

[68] F. Zaruba, F. Schuiki, T. Hoeﬂer, and L. Benini, “Snitch: A tiny pseudo
dual-issue processor for area and energy efﬁcient execution of ﬂoating-
point intensive workloads,” IEEE Transactions on Computers, 2020.

[69] M. Zhang, Y. Zhuo, C. Wang, M. Gao, Y. Wu, K. Chen, C. Kozyrakis,
and X. Qian, “Graphp: Reducing communication for pim-based graph
processing with efﬁcient data partition,” in 2018 IEEE International
Symposium on High Performance Computer Architecture (HPCA).
IEEE, 2018, pp. 544–557.

[70] Y. Zhang, M. Yang, R. Baghdadi, S. Kamil, J. Shun, and S. Amarasinghe,
“Graphit: A high-performance graph dsl,” Proceedings of the ACM on
Programming Languages, vol. 2, no. OOPSLA, pp. 1–30, 2018.
[71] P. Zhou, J. Hom, G. Upadhya, K. Goodson, and M. Munch, “Electro-
kinetic microchannel cooling system for desktop computers,” in Twen-
tieth Annual IEEE Semiconductor Thermal Measurement and Manage-
ment Symposium (IEEE Cat. No. 04CH37545).
IEEE, 2004, pp. 26–29.
[72] X. Zhu, W. Chen, W. Zheng, and X. Ma, “Gemini: A computation-
centric distributed graph processing system,” in 12th {USENIX} sym-
posium on operating systems design and implementation ({OSDI} 16),
2016, pp. 301–316.

[73] Y. Zhu, B. Wang, D. Li, and J. Zhao, “Integrated thermal analysis
for processing in die-stacking memory,” in Proceedings of the Second
International Symposium on Memory Systems, 2016, pp. 402–414.
[74] Y. Zhuo, C. Wang, M. Zhang, R. Wang, D. Niu, Y. Wang, and X. Qian,
“Graphq: Scalable pim-based graph processing,” in Proceedings of the
52nd Annual IEEE/ACM International Symposium on Microarchitecture,
2019, pp. 712–725.

13

