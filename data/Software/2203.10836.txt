1

2
2
0
2

r
a

M
1
2

]

C
D
.
s
c
[

1
v
6
3
8
0
1
.
3
0
2
2
:
v
i
X
r
a

A Model and Survey of Distributed Data-Intensive Systems

ALESSANDRO MARGARA, Politecnico di Milano, Italy
GIANPAOLO CUGOLA, Politecnico di Milano, Italy
NICOLÒ FELICIONI, Politecnico di Milano, Italy
STEFANO CILLONI, Politecnico di Milano, Italy

Data is a precious resource in today’s society, and is generated at an unprecedented and constantly growing
pace. The need to store, analyze, and make data promptly available to a multitude of users introduces
formidable challenges in modern software platforms. These challenges radically transformed all research fields
that gravitate around data management and processing, with the introduction of distributed data-intensive
systems that offer new programming models and implementation strategies to handle data characteristics such
as volume, velocity, heterogeneity, and distribution. Each data-intensive system brings its specific choices
in terms of data model, usage assumptions, synchronization, processing strategy, deployment, guarantees
in terms of consistency, fault tolerance, ordering. Yet, the problems data-intensive systems face and the
solutions they propose are frequently overlapping. This paper proposes a unifying model that dissects the
core functionalities of data-intensive systems, and precisely discusses alternative design and implementation
strategies, pointing out their assumptions and implications. The model offers a common ground to understand
and compare highly heterogeneous solutions, with the potential of fostering cross-fertilization across research
communities and advancing the field. We apply our model by classifying tens of systems and this exercise
guides interesting observations on the current state of things and on open research directions.

CCS Concepts: • Information systems → Parallel and distributed DBMSs; Middleware for databases;
Distributed database transactions; Stream management.

Additional Key Words and Phrases: Data-intensive systems, distributed systems, data management, data
processing, big data, model, taxonomy

ACM Reference Format:
Alessandro Margara, Gianpaolo Cugola, Nicolò Felicioni, and Stefano Cilloni. 2022. A Model and Survey
of Distributed Data-Intensive Systems. ACM Comput. Surv. 1, 1, Article 1 (March 2022), 67 pages. https:
//doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
As data guides the decision-making process of increasingly many human activities, software
applications become data-intensive [53]. They handle large amounts of data produced by humans
as well as sensors, devices, and other software systems. They perform complex data analysis
to rapidly extract valuable knowledge from the application environment. They take automated
decisions in near real-time. They serve content to a multitude of users over wide geographical
areas. The challenges of data-intensive applications come from the volume, velocity and variety of
data they have to manage and they call for distributed software systems that exploit the resources

Authors’ addresses: Alessandro Margara, alessandro.margara@polimi.com, Politecnico di Milano, Italy; Gianpaolo Cugola,
gianpaolo.cugola@polimi.com, Politecnico di Milano, Italy; Nicolò Felicioni, nicolo.felicioni@mail.polimi.com, Politecnico
di Milano, Italy; Stefano Cilloni, stefano.cilloni@mail.polimi.com, Politecnico di Milano, Italy.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Association for Computing Machinery.
0360-0300/2022/3-ART1 $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

 
 
 
 
 
 
1:2

Margara et al.

of interconnected computers to efficiently store, query, analyze, and serve data to customers.
Unfortunately, managing data in distributed systems is notoriously difficult, since it introduces issues
related to communication, concurrency and synchronization, deployment of data and computational
tasks on physical nodes, replication and consistency, handling of partial failures [93]. To address
these issues, the last two decades have seen a flourishing of platforms to support the development
and operation of data-intensive applications with programming and execution models that abstract
away some of the concerns related to data management and processing at scale. These systems
originate from research and development efforts in various communities, in particular those working
on database and distributed systems.

Background: different research lines addressing overlapping problems. In the database community,
the mutating requirements brought by data-intensive applications put the traditional (relational)
data model and implementation strategies under question. The increasing complexity and vol-
ume of data demanded for flexibility and scalability. Internet-scale applications demanded for
geographically-replicated stores, supporting a multitude of users concurrently reading and updat-
ing the data store [7]. In these contexts, communication and synchronization costs may become
the main bottleneck [16]. Workload characteristics changed as well: analytical tasks emerged and
complemented query tasks [84]. In response to these challenges, researchers first investigated so
called NoSQL solutions [39] that trade strong consistency and transactional guarantees in favor of
flexibility and scalability. More recently, the complexity of writing applications with weak guaran-
tees inspired a renaissance of transactional semantics [85], coupled with programming, design, and
implementation approaches to make them more scalable [37, 87, 88, 91].

In parallel, within the distributed systems research area, the increasing availability of data
fostered the development of new data processing systems that exploit the compute capabilities of
cluster infrastructures to extract valuable information from large quantities of data. Pioneered by
MapReduce [41], many systems organize the computation into a dataflow graph of operators that
apply functional transformations on their input data. This dataflow model promotes distributed
computations: operators may run simultaneously on different machines (task parallelism), and
multiple instances of each operator may process independent portions of the input data in parallel
(data parallelism). Developers only specify the behavior of operators and data partitioning, and
the runtime automates the deployment of operators, the exchange of data, and the re-execution
of lost computations due to software of hardware failures. Over the years, a multitude of systems
adopted and revised this processing model in terms of programming abstractions (e.g., support
for streaming data and iterative computations) as well as design and implementation choices (e.g.,
strategies to associate operators to physical machines and to exchange data across operators). Other
systems bring alternative programming models that are suitable to specific domains, such as graph
processing [66] or machine learning [1].

In general, the challenging demands of data-intensive applications are continuously pushing
the limit of technology and lead researchers and practitioners to experiment with new design and
implementation strategies and to build novel solutions that go beyond traditional categories [86].
For instance, many dataflow platforms offer abstractions to process relational data, thus becoming
suitable execution engines for complex relational queries and crossing the boundaries of database
technologies. Data stores such as VoltDB [88] and S-Store [30] aim to support near real-time
processing of incoming data within a relational database core. Messaging services such as Kafka [54]
offer persistency, querying, and processing capabilities [20].

Motivations and contributions. In summary, a multitude of distributed data-intensive systems
proliferated over the years. Each brings its specific choices in terms of data and storage model, usage
assumptions, communication, synchronization and processing strategy, deployment, guarantees in

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:3

terms of data consistency, fault tolerance, ordering. Yet, the problems they face and the solutions
they propose are frequently overlapping. In this scenario, it is difficult for users to grasp the subtle
differences between systems, evaluate their benefits and limitations, and select the most suitable
ones for a given application. Also, it is hard for researchers in the area to identify common design
choices and key distinguishing factors, and understand their consequences on the system behavior.
Moving from these premises, this paper proposes a model for data-intensive systems that integrates
all key design and implementation choices into a unifying view, thus highlighting recurring choices
as well as distinctive features of each system. Our model characterizes data-intensive systems by
introducing a collection of abstract components that cooperate to offer the system functionalities.
For each component, we precisely define: (i) the assumptions it relies upon; (ii) the functionalities
it provides; (iii) the guarantees it offers; (iv) the possible strategies for its design and its concrete
implementation, highlighting their implications. From the model, we derive a list of classification
criteria that we use to describe tens of state-of-the-art data-intensive systems, organizing them
into groups that highlight their similarities.

Our work contributes to the research areas that gravitate around data-intensive systems in
various ways: (i) it enables a precise and unbiased comparison of their features; (ii) it offers a global
view of the research in the field, organized around well defined classification criteria; (iii) it promotes
cross-fertilization between communities, defining a vocabulary and conceptual framework that
they can use to exchange ideas and solutions to common problems; (iv) it highlights consolidated
results and open challenges, and helps identifying promising research directions.

Paper outline. The paper is organized as follows: Sec. 2 presents our unifying model of data-
intensive systems. Sec. 3 classifies current systems based on the criteria that emerge from the
model. Sec. 4, Sec. 5, and Sec. 6 apply the model to the classes of systems we identified: pure
data management systems, pure data processing systems, and other systems that propose new or
hybrid programming and execution models. Sec. 7 discusses the main messages we extract from
our work, shows connections with related fields, and points out promising research directions.
Sec. 8 concludes the paper summarizing its contributions.

2 A UNIFYING MODEL FOR DATA-INTENSIVE SYSTEMS
This section presents a model that captures the core functionalities of data-intensive systems and
associates them to abstract components. From the model, we derive fine-grained classification
criteria that we summarize in Table 1, Table 2, and Table 3. We use the criteria in Sec. 3, Sec. 4, Sec. 5,
and Sec. 6 to build a taxonomy and to survey data-intensive systems. Throughout this section,
when providing concrete examples of systems to explain the concepts in the model, we denote as
data management systems (DMSs) those that are primarily designed to store some state and expose
functions to query and mutate such state, while we denote data processing systems (DPSs) those
that enable expensive transformation and analysis of static (batch) or dynamic (streaming) data.
This terminology is coherent with the taxonomy in Sec. 3.

2.1 Functional model
Fig. 1 depicts the functional model of a data-intensive system. The resulting classification criteria
are in Table 1. External clients interact with the system by registering and starting driver programs.
As part of its execution, a driver program can invoke one or more jobs, which are the largest units
of execution that can be offloaded onto the distributed computing infrastructure made available
by the data-intensive system. A driver program may execute client-side or system-side, depending
on the specific system. Some systems decouple activation from registration (we say that driver
execution time is on start) while others do not (we say that driver execution time is on registration).

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:4

Margara et al.

Fig. 1. Functional model of a data-intensive system.

Functional

Jobs definition

Jobs compilation

Driver execution

Driver execution
time

Invocation of jobs

Sources

Sinks
State

Deployment

client-side /
system-side
on registration / on
start
synchronous /
asynchronous
no / passive / active /
both
no / yes
no / yes
cluster / wide area /
hybrid

Jobs definition API
Execution plan
definition
Task
communication
Execution plan
structure
Iterations
Dynamic creation
Nature of jobs

State management

Data parallel API
Placement-aware
API

library / DSL

explicit / implicit

explicit / implicit

dataflow / workflow

yes / no
no / yes
one-shot / continuous
absent / explicit /
implicit
yes / no

yes / no

Jobs compilation
time
Use static resources
info
Use dynamic
resources info

on driver registration /
on driver execution

no / yes

no / yes

Jobs deployment and execution

Granularity of
deployment

Deployment time

Use dynamic
resources info
Management of
resources

job-level / task-level

on job compilation /
on task activation

no / yes

system-only / shared

Table 1. Classification criteria: functional components, jobs definition, compilation, deployment, execution.

To exemplify, in a DMS a driver program may be a stored procedure that combines code expressed
in some general purpose programming language with one or more queries (the jobs) expressed
in the language offered by the engine (e.g., SQL). Stored procedures typically run system-side
every time a client activates them (on start). Their benefits may include pre-compilation of jobs
on registration and reduced latency in the communication between the driver program and the
jobs. Similarly, in a DPS, the driver program may be a piece of Java code that spawns one or more
distributed computations (the jobs) written using the API offered by the data processing engine. In
this context, the driver program will typically run system-side on registration.

The data-intensive system runs on the distributed computing infrastructure as a set of worker
processes, hosted on the same or different nodes (physical or virtual machines). We model the
processing resources offered by workers as a set of slots. Jobs are compiled into elementary units
of execution that we denote as tasks and run sequentially on slots. Jobs consume input data and
produce output data, possibly accessing (reading and modifying) some state stored inside the
distributed computing infrastructure. State is split (partitioned and replicated) across workers, such
that each of them is responsible for a state portion. In our model, data elements are immutable and
are distributed through communication channels (dark gray arrows in Fig. 1 and Fig. 2) that we
collectively refer to as the data bus. Notice that the data bus also distributes jobs invocations. Indeed,
our model emphasizes the dual nature of invocations and data, which can both carry information
and trigger jobs execution. We exploit this duality to capture commonalities between systems that
are frequently hidden behind heterogeneous styles of interaction.

Jobs invocations may be either synchronous, if the driver program waits for jobs completion before
making progress, or asynchronous, if the driver program continues to execute after submitting the
invocation. In both cases, invocations may return some result to the driver program, as indicated
by the bidirectional lines in Fig. 1. In some systems, jobs also consume data from external sources
and produce data for external sinks. We distinguish between passive sources, which consist of static
datasets that jobs can access during their execution (for instance, a distributed filesystem or a

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

ClientsData-intensive systemregisterstartDriver programSourcesSinksjobsinvocation& datadatadataDistributed computing infrastructureNodeWorkerStateSlotNodeWorkerStateSlotNodeWorkerStateSlotdata busA Model and Survey of Distributed Data-Intensive Systems

1:5

Fig. 2. Jobs definition, compilation, deployment, and execution.
database), and active sources, which produce new data dynamically and may trigger job execution
(for instance, a messaging system). To exemplify, stored procedures in a DMS invoke (synchronously
or asynchronously, depending on the specific system) one or more queries (the jobs) during their
execution. Invocations also carry input data in the form of parameters passed to the invocation.
Queries can access (read-only queries) and modify (read-write queries) the state of the system, and
return query results. In batch DPSs such as MapReduce, jobs read input data from passive sources
(for instance, a distributed filesystem), apply functional transformations that do not involve any
mutable state, and store the resulting data into sinks (for instance, the same distributed filesystem
or a database). In stream DPSs, jobs run indefinitely and make progress when active sources provide
new input data. We say that input data activates a job. In this case, jobs may preserve some state
across activations.

We characterize the distributed computing infrastructure based on its deployment. In a cluster
deployment all nodes belong to the same cluster or data center, which provides high bandwidth
and low latency for communication. Conversely, in a wide area deployment nodes can be spread in
different geographical areas, a choice that increases the latency of communication and may impact
the possibility of synchronizing and coordinating tasks. For this reason we also consider hybrid
deployments, when the system adopts a hierarchical approach, exploiting multiple fully-functional
cluster deployments that are loosely synchronized with each other.

2.2 Jobs: from definition to execution
This section concentrates on jobs, following their lifecycle from definition to execution (see Fig. 2).
Jobs are defined inside a driver program (Sec. 2.2.1), compiled into an execution plan of elementary
tasks (Sec. 2.2.2), which are deployed and executed on the distributed computing infrastructure
(Sec. 2.2.3). The resulting classification criteria are in Table 1.

Jobs definition. Jobs are defined inside driver programs. Frequently, driver programs include
2.2.1
multiple jobs and embed the logic that coordinates their execution. For instance, stored procedures
(driver programs) in DMSs may embed multiple queries (jobs) within procedural code. Similarly,
DPSs invoke analytic jobs from a driver program written in a standard programming language with
a fork-join execution model. Notably, some systems implement iterative algorithms by spawning a
new job for each iteration and by evaluating termination criteria within the driver program. Jobs
are expressed using programming primitives (jobs definition API ) with heterogeneous forms. For
instance, relational DMSs rely on SQL (a domain specific language - DSL), while DPSs usually
offer libraries for various programming languages. Many systems support different forms, giving
developers the opportunity to choose the most suitable for the application at hand. Jobs are compiled
into an execution plan of elementary units of deployment and execution called tasks. Tasks run
on slots and (i) they exchange data over the data bus (dark gray arrows in Fig. 2) according to
the communication schema defined in the execution plan; (ii) they may access the state portion
of the worker they are deployed on. We say that the execution plan definition is explicit if the

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

JobscompilationJobsdeploymentJobsExecutionplanTask ATask BTask CTask DTask EDistributed computing infrastructureDriver programWorkerState (portion)SlotTask ASlotTask BWorkerState (portion)SlotTask ASlotTask BWorkerState (portion)SlotTask DSlotTask EWorkerState (portion)SlotTask CSlot1:6

Margara et al.

programming primitives directly specify the individual tasks to be carried out as part of the jobs
and their logical dependencies. The definition is instead implicit if the logical plan is compiled from
a higher-level declarative specification. To exemplify, the dataflow formalism adopted in many
DPSs provides an explicit definition of the logical plan, while SQL, and most query languages,
provide an implicit definition. With an explicit definition of the logical plan, the communication
between tasks can itself be explicit or implicit. In the first case, the system APIs include primitives
to send and receive data across tasks, while in the latter case the exchange of data is implicit. The
execution plan structure can be a generic workflow, where there are no restrictions to the pattern
of communication between tasks, or a dataflow, where tasks need to be organized into an acyclic
graph and data can only move from upstream tasks to downstream tasks. When present, we also
highlight further structural constraints. For instance, the execution plan of the original MapReduce
system forces data processing in two phases: a map and a reduce. Iterations within the execution
plan may or may not be allowed, as in the case of the original MapReduce. We say that a system
supports dynamic creation of the plan if it enables spawning new tasks during execution: dynamic
creation gives the flexibility of defining or activating part of the execution plan at runtime, which
may be used to support control flow constructs.

Jobs can be either one-shot or continuous. One-shot jobs are executed once and then terminate.
We use the term invoke here: as invoking a program twice leads to two distinct processes, invoking
a one-shot job multiple times leads to separate executions of the same code. For instance, queries
in DMSs are typically one-shot jobs and indeed multiple invocations of the same query lead to
different one-shot jobs. Instead, continuous jobs persist across invocations. In this case we use
the term activate to highlight that the same job is repeatedly activated by the arrival of new data.
This happens in stream DPSs, where continuous jobs are activated when new input data comes
from active sources. As detailed in Sec. 2.4, the key distinguishing factor of continuous jobs is their
ability to persist some private task state across activations. By definition, this option is not available
for one-shot jobs, since each invocation is independent from the other.

State management in jobs may be absent, explicit, or implicit. For instance, state management is
absent in batch DPSs, which define jobs in terms of functional transformations that solely depend on
the input data. State management is explicit when the system provides constructs to directly access
state elements to read and write them. For instance, queries in relational DMSs provide select
clauses to retrieve state elements and insert and update clauses to store new state elements and
update them. State management is implicit when state accesses are implicit in job definition. For
instance, stream DPSs manage state implicitly through ad-hoc operators such as windows that
record previously received data and use it to compute new output elements.

Another relevant characteristic of the programming model is the supports for data parallel
operations. With data parallelism, a computation is defined for a single element and it is automati-
cally executed on many elements in parallel. Data parallelism is central in many systems, and in
particular in DPSs as (i) it simplifies the definition of a job by letting developers focus on individual
elements; (ii) it promotes formulating programs that are amenable to parallel execution as the
tasks operating on different elements are independent. Systems supporting data parallelism apply
partitioning strategies to both data and state (when available). We will return on these concepts
when discussing data and state management (see Sec. 2.3 and Sec. 2.4).

Virtually all data-intensive systems aim to optimize local access to data and state elements, and
reduce inter-task communication. Towards this goal, some systems offer placement aware API that
enable developers to suggest suitable placement strategies based on the expected workload.

Jobs compilation. The process of compiling jobs into an execution plan may either start
2.2.2
on driver registration or on driver execution. The first case models situations where the driver

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:7

program is registered in the system and can be executed multiple times, as in the case of stored
procedures. The second case happens in DPSs, which usually offer a single command to submit and
execute a program. Jobs compilation may rely on static information about the resources available
in the distributed computing infrastructure. For instance, data parallel operators are converted
into multiple tasks that run the same logical step in parallel: the concrete number of tasks is
typically selected depending on the available processing resources (overall number of CPU cores).
Compilation may also use dynamic information about resources. For instance, a join operation
may be compiled into a distributed hash-join or into a sort-merge join depending on the current
cardinality and distribution of the elements to join (dynamic information).

Jobs deployment and execution. Jobs deployment is the process of allocating the tasks of
2.2.3
a job execution plan onto slots. For instance, the execution plan in Fig. 2 consists of seven tasks
and each of them is deployed on a different slot. Tasks tagged A and B exemplify data-parallel
operations, each executed by two tasks in parallel. Deployment can be performed with job-level or
with task-level granularity. Job-level granularity is common when the deployment takes place on
job compilation, while task-level granularity is used when the deployment (of individual tasks) takes
place on task activation. It is important to note that the above classification is orthogonal to the
nature of jobs (one-shot or continuous) as defined above. One-shot jobs may be: (i) entirely deployed
on job compilation, or (ii) progressively, as their input data is made available by previous tasks
in the execution plan. The first choice is frequent in DMSs, while the latter characterizes several
DPSs. Similarly, continuous jobs may be: (1) fully deployed on compilation, with their composing
tasks remaining available onto slots, ready to be activated by incoming data elements, or (2) their
tasks may be deployed individually when new input data becomes available and activates them. In
this case the same task is deployed multiple times, once for each activation: systems that follow
this strategy minimize the overhead of deployment by accumulating input data into batches and
deploying a task only once for the entire batch, as well exemplified by the micro-batch approach of
Spark Streaming [98]. As we discuss in Sec. 2.3, task-level deployment requires a persistent data
bus, to decouple task execution in time. If the data bus is not persistent, all tasks in the execution
plan need to be simultaneously deployed to enable the exchange of data.

The deployment process always exploits static information about the resources available in the
computing infrastructure, like the address of workers and their number of slots. Some systems also
exploit dynamic information, such as the current load of workers and the location of data. This is
typically associated to task-level scheduling on activation, where tasks are deployed when their
input data is available and they are ready for execution.

Finally, the deployment process may have a system-only or shared management of resources.
System-only management only considers the resources occupied by the data-intensive system.
Shared management takes global decisions in the case multiple software systems share the same
distributed computing infrastructure. For instance, it is common to use external resource managers
such as Yarn for task deployment in cluster environments.

2.3 Data management
This section studies the characteristics of data elements and the data bus used to distribute them. The
resulting classification criteria are in Table 2. Recall that in our model data elements are immutable,
meaning that once they are delivered through the data bus they cannot be later updated or modified.
Also, they are used to represent both data and invocations, as they carry some payload and may
trigger the activation of tasks. Data elements may be structured, if they have an associated schema
determining the number and type of fields they contain, or unstructured, otherwise. Structured data
is commonly found in DPSs, when input datasets or data streams are composed of tuples with a fixed

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:8

Margara et al.

Data management

State management

Group atomicity

Elements structure

Temporal elements
Bus connection
type
Bus
implementation
Bus persistency
Bus partitioning
Bus replication
Bus interaction
model

no / general / domain
specific
no / yes

direct / mediated

persistent / ephemeral
no / yes
no / yes

push / pull / hybrid

Elements structure

Storage medium

Storage structure
Task state
Shared state
Partitioned
Replication
Replication
consistency
Replication
protocol
Update
propagation

no / general / domain
specific
memory / disk / hybrid
/ service

no / yes
no / yes
no / yes
no / backup-only / yes

weak / strong

leader / consensus /
conflict resolution

state / operations

Causes for aborts
Protocol
Assumptions

system / job
blocking / coord. free

Group isolation

Level
Implementation
Assumptions

blocking / coord. free
lock / timestamp

Delivery and order

Delivery
guarantees
Nature of
timestamps

Order guarantees

at most once / at least
once / exactly once

n.a. / ingestion / event

n.a. / eventually /
always

Table 2. Classification criteria: data, state, and guarantees.

structure. The structure of elements may reflect on the data bus, with assumptions of homogeneous
data elements (same schema) in some communication channels. For instance, DPSs typically assume
homogeneous input and homogeneous output data for each task. We further distinguish between
systems that accept general structured data and systems that assume domain-specific data models,
as in the case of relational data, time series, or graph-shaped data. Finally, data may or may not have
a temporal dimension: this is particularly relevant for stream DPSs, where it is used for time-based
analysis. We will return to this topic later in Sec. 2.6, where we detail how the temporal dimension
influences the order in which tasks analyze data elements.

The data bus can either consist of direct connections between the communicating parties or it can
be mediated by some middleware service. Accordingly, the actual implementation may range from
TCP links (direct connection) to various types of middleware systems (mediated connection), like
message queuing or a distributed storage services. While direct connections are always ephemeral,
various mediated connections are persistent. In the first case, receivers need to be active when the
data is transmitted over the bus and they cannot retrieve the same elements later in time. In the
second case, elements are preserved inside the bus and receivers can access them multiple times
and at different points in time. For instance, DPSs that implement job-level deployment usually
adopt a direct (and ephemeral) data bus made of TCP connections among tasks. Conversely, DPSs
that deploy tasks independently (task-level deployment) require a persistent and mediated data bus
(for instance, a distributed filesystem or a persistent messaging middleware) where intermediate
tasks can store their results for downstream tasks.

In many systems, the data bus provides communication channels where data elements are
logically partitioned based on some criterion, for instance, randomly or based on the value of a
given field in the case of structured data. The use of a partitioned data bus is common in DPSs,
where it is associated to data-parallel operators: the programmer specifies the operator for the data
elements in a single partition, but the operator is concretely implemented by multiple (identical)
tasks that work independently on different partitions; a partitioned data bus distributes data to
the correct task according to the partitioning criterion in use. A persistent data bus may also be
replicated, meaning that the same data elements are stored in multiple copies. Replication may serve
two purposes: improve performance, for instance by enabling different tasks to consume the same
data simultaneously from different replicas, or fault tolerance, to avoid losing data in the case one
replica becomes unavailable. We will discuss fault tolerance in greater details in Sec. 2.7. Currently
available systems that replicate the data bus always implement a single-leader replication schema,
where one leader replica is responsible for receiving all input data and for updating the other 𝑓
(follower) replicas. The update is synchronous (or semi-synchronous), meaning that the addition
of an input data element to the data bus completes when the data element has been successfully
applied to all 𝑓 follower replicas (or to 𝑟 < 𝑓 replicas, if the update is semi-synchronous). The

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:9

data bus offers an interaction model that is push if the sender delivers data to recipients, or pull if
receivers demand data to senders. Hybrid approaches are possible in the presence of a mediated
bus, a common case being a push approach between the sender and the mediated data bus and a
pull approach between the data bus and the recipients.

2.4 State management
Differently from data, state elements are mutable, meaning their content may change over time.
Upon invocation, tasks read input data elements from the data bus and write output results on the
data bus. In absence of state (stateless tasks), the results produced only depend on the input data
elements read, but many systems support stateful tasks, whose results also depend on some mutable
state that they can read and modify during their execution. As already discussed for data elements,
state elements may also be unstructured or structured, and in the second case they may rely on
domain-specific data models. State may be stored on different storage media: (1) many systems
store the entire state in-memory, and replicate it to disk only for fault tolerance; (2) other systems
use disk storage, or (3) hybrid solutions, where state is partially stored in memory for improved
performance and flushed to disk to scale in size; (4) some systems rely on an external storage
service, as common in cloud-based systems that split their core functionalities into independently
deployed services. Some recent work investigates the use of persistent memory [60], but these
solutions are not employed in currently available systems. The storage structure indicates the data
structure used to represent state on the storage media. This structure is heavily influenced by the
expected access pattern: for instance, relational state may be stored row-wise, to optimize access
element by element (common in data management workloads) or column-wise, to optimize access
attribute by attribute (common in data analysis workloads, for instance to compute and aggregation
function over an attribute). Many DMSs use indexed structures such as B-trees or log-structured
merge (LSM) trees to rapidly identify individual elements. B-trees offer superior read performance
by eagerly updating the index on write. LSM trees improve write performance by storing recent
updates in a sequential log, but require scanning the log to reconcile results on read.

Data-intensive systems may support two types of state: task state, which is private to a single task,
and shared state, which can be accessed by multiple tasks. Task state is only relevant in continuous
jobs, where it can survive across multiple activations of the same task. Indeed, it is widely used in
stream DPSs to implement stateful operators such as windows. The availability of these types of
state deeply affects the design and implementation of the system. Shared state is central in DMSs,
where two tasks (for example, an insert and a select query) can write and read simultaneously
from the same state (for example, a relational table). Conversely, most DPSs avoid shared state to
simplify parallel execution. Frequently, batch DPSs do not offer any type of state, while stream
DPSs only offer task state: as its access is restricted to a single task (sequential unit of execution), it
does not require any concurrency control mechanism.

In our model, workers are responsible for storing separate portions of the shared state space,
such that tasks may only access elements stored on the shared state portion of the worker they
are deployed on, while to retrieve shared state elements deployed on remote workers they need
to transfer them over the data bus. Splitting of the shared state among workers may respond to
a criterion of partitioning. For instance, partitioning enables DMSs to scale beyond the limited
memory capacity of a single node, but also to run tasks belonging to the same or different jobs
(queries) in parallel on different partitions. Besides partitioning, many data-intensive systems adopt
replication. As in the case of data bus replication, state replication may also serve two purposes:
(i) reduce read access latency, by allowing multiple workers to store a copy (replica) of the same
state elements locally; (ii) provide durability and fault tolerance, that is, avoid potential loss in
the case of software and hardware failures. We discuss the characteristics and strategies for state

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:10

Margara et al.

replication here, and we return on its use for fault tolerance later in Sec. 2.7. First, we consider if
the replication is backup-only, meaning that replicas are only used for fault tolerance and cannot
be accessed by tasks during execution, or not. If tasks can access state elements from multiple
replicas, different replication consistency models are possible, which define what state values may
be observed by tasks when accessing from multiple replicas. Replication models have been widely
explored in database and distributed systems theory [95]. For the goal of our analysis, we only
distinguish between strong and weak consistency models, where the former require synchronous
coordination among the replicas while the latter do not. This classification approach is also in line
with the recent literature that denotes models that do not require synchronous coordination as
being highly available [16]1. Intuitively, strong consistency models are more restrictive and use
coordination to avoid anomalies that may arise when tasks access elements simultaneously from
different replicas. In practice, most data-intensive systems that adopt a strong consistency model
provide sequential consistency, a model that ensures that accesses to replicated state are the same
as if they were executed in some serial order. Sequential consistency simplifies reasoning on the
state of the system, as it hides concurrency by mimicking the behavior of a sequential execution.
In terms of implementation, we distinguish two main classes of mechanisms to achieve strong
consistency: in leader-based algorithms all state updates are delivered to a single replica (leader)
that decides their order; in consensus-based algorithms replicas use quorum-based or distributed
consensus protocols to agree on the order of state accesses. Systems that adopt a weak consistency
model typically provide eventual consistency, where updates to state elements are propagated
asynchronously, which may lead to (temporary) inconsistencies between replicas. This includes
write-write conflicts, if the same elements are updated simultaneously in different replicas. For this
reason, weak consistency is typically coupled with automated conflict resolution algorithms, which
guarantee that all replicas solve conflicts in the same way and eventually converge to the same
state. A popular approach to conflict resolution are conflict-free replicated datatypes, which expose
only operations that guarantee deterministic conflict resolution in the presence of simultaneous
updates [80]. Finally, replication protocols may employ two approaches to propagate updates:
state-based or operation-based (also known as active replication). In state-based replication, when a
task updates a value in a replica, the new state is propagated to the other replicas. In operation-
based replication, the operation causing the update is propagated and re-executed at each replica:
this approach may save bandwidth but it may spend more computational resources to re-execute
operations at each replica.

2.5 Tasks grouping
Several systems offer primitives to identify groups of tasks and provide additional guarantees for
such groups, which we classify as group atomicity (Sec. 2.5.1) and group isolation (Sec. 2.5.2). The
resulting classification criteria are in Table 2. Atomicity ensures no partial failures for a group of
tasks: they either all fail or all complete successfully. Isolation limits the ways in which running
tasks can interact and interleave with each other. In DMSs, these concerns are considered part of
transactional management, as transactions provide atomicity, consistency, isolation, and durability.
In our model, we discuss consistency constraints as part of group atomicity in the next section,
while we integrate durability with fault tolerance and discuss it in Sec. 2.7.

2.5.1 Group atomicity. Atomicity ensures that a group of tasks either entirely succeeds or entirely
fails. We use the established jargon of database transactions and we say that a task (or group of tasks)
either commits or aborts. If the tasks commit, all the effects of their execution, and in particular all
their changes to the shared state, become visible to other tasks. If the tasks abort, none of the effects

1We adopt the same approach when discussing atomicity and isolation guarantees for groups of tasks in Sec. 2.5.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:11

of their execution becomes visible to other tasks. We classify group atomicity along two dimensions.
First, we consider the possible causes for aborts and we distinguish between system-driven or job-
driven. System-driven aborts derive from non-deterministic hardware or software failures, such as
a worker crashing or running out of memory. Job-driven aborts are part of a job definition and
are triggered if job completion may lead to a logic error. For instance, database consistency and
integrity constraints are a form of job-driven aborts. Second, we consider how systems implement
group atomicity. Atomicity is essentially a consensus problem [63], where tasks need to agree on a
common outcome: commit or abort. The established protocol to implement atomicity is two-phase
commit. In this protocol, one of the participants takes the role of a coordinator. In a first phase, the
coordinator collects all votes (commit or abort) from participants. In a second phase, it distributes
the common decision to all participants. Notice that this protocol is not robust against the failure of
the coordinator. However, data-intensive systems typically adopt orthogonal mechanisms to deal
with failures, as discussed in Sec. 2.7. Most importantly, two-phase commit is a blocking protocol
as participants cannot make progress before receiving the global outcome from the coordinator.
For these reasons, some systems adopt simplified, coordination free protocols that reduce or avoid
coordination under certain assumptions. Being specific to individual systems, we discuss such
protocols as part of our survey in Sec. 4.

2.5.2 Group isolation. Group isolation constrains how tasks belonging to different groups can
interleave with each other, and is classically organized into levels [4]. Serializable isolation requires
the effects of execution to be the same as if all groups were executed in some serial order, one after
the other, with no interleaving of tasks. Weaker levels of isolation enable some disciplined form
of concurrency that may lead to some anomalies in the results that clients can observe. In this
work we do not enter the details of specific isolation levels. In line with the approach adopted for
replication consistency and for atomicity, we consider only two broad classes of isolation levels:
those that require blocking coordination between tasks and those that are coordination free (usually
referred to as being highly-available in the literature [16]). This is also motivated by the systems
under analysis, which either provide strong isolation levels (typically, serializable) or do not provide
isolation at all. Implementation-wise, strong isolation is traditionally achieved with two classes
of coordination protocols: lock-based and timestamp-based [21]. With lock-based protocols, tasks
acquire non-exclusive or exclusive locks to access shared resources (shared state in our model) in
read-only or read-write mode. Lock-based protocols may incur distributed deadlocks: to avoid them,
protocols implement detection or prevention schemes that abort and restart groups in the case of
deadlock. Timestamp-based protocols generate a serialization order for groups before execution,
and then the tasks need to enforce that order. Basic timestamp protocols abort and re-execute
groups when they try to access shared resources out of order. Multi-version concurrency control
protocols reduce the probability of aborts by storing multiple versions of shared state elements,
and allowing tasks to read old versions when executed out of order. Optimistic concurrency control
protocols allow out of order execution of tasks, but check for conflicts before making the effects of
a group of tasks visible to other tasks. Finally, a few systems adopt simplified protocols that reduce
or avoid coordination under certain assumptions. As in the case of group atomicity, we discuss
these protocols as part of our survey of systems in Sec. 4.

2.6 Delivery and order guarantees
The effects of input data and invocations (from driver programs and from sources) become visible
(to driver programs and sinks) though (i) the output they produce, and (ii) the changes to the system
state (if any), which determine the results of other invocations. Delivery and order guarantees
define how external actors (driver programs, sources, and sinks) observe these effects. Both topics

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:12

Margara et al.

are crucial for distributed systems and have been widely explored in the literature. Here, we only
focus on the key concepts that characterize the behavior of the systems we analyzed, and we offer a
description that embraces different styles of interaction, from invocation-based (as in DMSs queries)
to data-driven (as in stream DPSs). The resulting classification criteria are in Table 2.

Delivery focuses on the effects of a single input 𝐼 (data element or invocation). Under at most
once delivery, the system behaves as if 𝐼 was either received and processed once or never. Under
at least once delivery, the system behaves as if 𝐼 was received and processed once or more than
once. Under exactly once delivery, the system behaves as if 𝐼 was received and processed once and
only once. Notice that it is in general impossible to deliver an input exactly once in a distributed
environment. Nevertheless, a system can behave as if the input was processed exactly once under
some assumptions: the most common are that driver programs and sources can resubmit the input
upon request (to avoid loss of data), while sinks can detect duplicate output results and discard
them (to avoid duplicate processing and output). To exemplify, DMSs offer exactly once delivery
when they guarantee group atomicity through transactions: in this case, a job entirely succeeds
or entirely fails, and in the case of a failure the system either notifies the driver program (that
may retry until success) or internally retries, allowing the jobs to be executed exactly once. DPSs
offer exactly once delivery by replaying data from sources (or from intermediate results stored in
a persistent data bus) in the case of a failure. In presence of continuous jobs (stream processing),
systems also need to avoid duplicating the effects of processing on task state when replaying data:
to do so, they often discard the effects of previous executions by reloading an old task state from a
checkpoint, before replaying data (see also the role of checkpoint on fault tolerance in Sec. 2.7).

Order focuses on multiple data elements or invocations and defines in which order their effects
become visible. Order ultimately depends on the nature of timestamps physically or logically
attached to data elements. In some systems no timestamp is associated to data elements and in these
cases no ordering guarantees are provided. Conversely, especially when data elements represent
occurrences of relevant events in the application domains, they have an associated timestamp that
can be set by the original source or by the system when it first receives the element. We rely on
established terminology, and we refer to the former case as event time and to the latter case as
ingestion time [9]. When a timestamp is provided, systems may ensure that the associated order
is guaranteed always or ultimately. Systems in the first class wait until all data elements before a
given timestamp become available and then process them in order. To do so, they typically rely
on a contract between the sender components and the data bus, where sender components use
special elements (denoted as watermarks) to indicate that all elements up to a given time 𝑡 have
been produced, and the data bus delivers elements up to time 𝑡 in the correct order. Systems in the
second class execute elements out-of-order, but retract previously issued results and correct them
when they receive new data with an older timestamp. Thus, upon receiving all input data up to
time 𝑡, the system ultimately returns the correct results. Notice that this mechanism requires the
actors receiving output data to tolerate temporarily incorrect results. According to our definitions
above, retraction is not compatible with exactly once delivery, as retraction changes the results
provided to sinks after they have already been delivered, thus breaking the illusion that they have
been produced once and only once.

2.7 Fault tolerance
Fault tolerance is the ability of a system to tolerate failures that may occur during its execution. We
consider hardware failures, such as a disk failing or a node crashing or becoming unreachable, and
non-deterministic software failures, such as a worker exhausting node’s memory as it competes with
other processes. We assume that the logic of jobs is correct, which guarantees that the re-execution
of a failed job does not deterministically lead to the same failure. Our minimal unit of fault is the

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:13

Fault tolerance

Dynamic reconfiguration

Detection
Scope
Computation recovery

State recovery

Guarantees for state
Assumptions

master-worker / consensus
comput - task state - shared state
absent / job / task
checkp. (indep./coord./per-activ.) /
log (WAL/CL) / repl.
none / valid / same

Goal
Automated
Mechanisms
Restart

no / yes
state migr. / task migr.
no / yes

Table 3. Classification criteria: fault tolerance and dynamic reconfiguration.

worker and we assume the typical approach to tolerate failures that involves first detecting the
failure and then recovering from it. The classification criteria for fault tolerance are in Table 3.

Fault detection is a problem of group membership: given a group of workers, determine a unique
(sub)set that is active and can communicate to execute jobs. Systems address the problem either
using a master-worker approach, which assumes one entity with a special role (master) that cannot
fail and can supervise normal workers, or using a distributed protocol, e.g., Paxos or Raft consensus.
After a failure is detected, fault recovery brings the system into a state from which it can resume
working (execute jobs) with the intended semantics. We describe the recovery process by focusing on
five aspects: scope, computation recovery, state recovery, guarantees, and assumptions. Depending
if tasks are stateless or stateful and if they can share state or not, the scope of recovery may involve
recovering the computation of failing tasks, the task state of failing tasks, and/or the shared state
portions held by failing workers. Computation recovery may be absent, in which case failing jobs
are simply discarded and the system offers at most once delivery (see Sec. 2.6). Otherwise, the
system recovers the computation by restarting it: we distinguish between systems that need to
restart an entire job and systems that can restart individual tasks. DMSs typically restart entire jobs
to satisfy transactional guarantees (in particular, atomicity) that require a job to either entirely
succeed or entirely fail. Some DPSs (those using a persistent data bus to save the intermediate
results of tasks) may restart only failed tasks. Restarting a computation requires that input data
and invocations are persisted and replayable, and that duplicate output data can be detected and
discarded by sinks (if the system wants to ensure exactly once delivery, see Sec. 2.6). To replay
input data and invocations, systems either rely on replayable sources, such as persistent message
services, or keep a log internally (see the discussion on command logging below).

To recover state, systems may rely on checkpointing, logging, and replication. Frequently, they
combine these mechanisms. Checkpointing involves saving a copy of the entire state to durable
storage. When a failure is detected, the last available checkpoint can be reloaded and the execu-
tion of jobs may restart from that state. Different workers may take independent checkpoints or
they may coordinate, for instance by using the distributed global snapshot protocol by Chandy
and Lamport [31] to periodically save the state of the entire system on stable storage. A third
alternative (per-activation checkpoint) is sometimes used for continuous jobs to save task state: at
each activation, a task stores its task state together with the output data. This approach essentially
transforms a stateful task into a stateless task, where state is encoded as a special data element
that the system receives in input at the next activation. In practice, per-activation checkpoint is
used in presence of a persistent data bus to store checkpoints. Frequent checkpointing may be very
resource consuming and affect the response time of the system. Logging is an alternative approach
that saves either individual operations or state changes rather than the entire content of the state.
These two forms of logging are known in the database literature as: (i) command logging (CL)
persists input data and invocations, and in the case of failure re-processes the same input to restore
state [65]; (ii) write ahead log (WAL) persists state changes coming from tasks to durable storage
before they are applied, and in the case of a failure re-applies the changes in the log to restore the
state [68]. Notice that logs may grow unbounded as new invocations enter the system. Accordingly,
they are always complemented with (infrequent) checkpoints. In the case of failure, the state is

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:14

Margara et al.

restored from the last checkpoint and then the log is replayed from the checkpoint on. Finally,
systems may replicate state portions in multiple workers. In this case, a state change performed by
a task succeeds only after the change has been applied to a given number of replicas 𝑟 . This means
that the system can tolerate the failure of 𝑟 − 1 replicas without loosing the state and without the
need to restore it from a checkpoint. As already discussed in Sec. 2.4, the same replicas used for
fault tolerance may also be used by tasks during normal processing to improve state access latency.
The recovery mechanisms above provide different guarantees on the state of the system after
recovery. It can be any state, a valid system state, or the same state the system was before failing.
In any state recovery, the presence of a failure may bring the system to a state that violates some
of the invariants for data and state management that hold for normal (non failing) executions: for
instance, the system may drop some input invocations. A valid state recovery mechanism brings
the system to a state that satisfies all invariants, but it may differ from all of those traversed before
the failure: for instance, a system that provides serializability for groups of tasks may recover by
re-executing groups of tasks in a different (but still serial) order. Depending on the system, clients
may be able or not to observe the differences between the two states (before and after the failure):
for instance, in a DMS, two read queries before and after the failure may observe different states.
A same state recovery mechanism brings the system in the same state it was before the failure.
Replication, write-ahead logging, and per-activation checkpointing bring the system to the same
state it was prior to fail, while independent checkpointing only guarantees to bring the system
back to a valid state, as it may happen that the latest checkpoints of each task do not represent a
consistent cut of the system and must be discarded in search of previous, consistent checkpoints.
The same happens for command logging, due to different interleavings in the original execution
and in the recovery phase that may lead to different (albeit valid) states. As a final aspect related to
recovery we consider the assumptions under which the various mechanisms operate. They range
from assuming no more than 𝑘 nodes can fail simultaneously, as in the case of replication, to the
need for data sources to be replayable and for duplicate data to be detectable in sinks.

2.8 Dynamic reconfiguration
With dynamic reconfiguration we refer to the ability of a system to modify the deployment and
execution of jobs on the distributed computing infrastructure at runtime. The corresponding
classification criteria are in Table 3. Reconfiguration may be driven by different goals, which
may involve providing some minimum quality of service, for instance in terms of throughput
or response time and/or minimizing the use of resources to cut down operation costs. It may be
activated manually or be automated, if the system can monitor the use of resources and make
changes that improve the state of the system with respect to the intended goals. The reconfiguration
process may involve different mechanisms: state migration across workers, for instance to rebalance
shared state portions if they become unbalanced; task migration, to change the association of tasks
to slots, including the addition or removal of slots, to add computational resources if the load
increases or release them when they are not necessary. State migration is common in DMSs, where
the distribution of shared state across workers may affect performance. Task migration is instead
used in DPSs in presence of continuous jobs, where tasks are migrated across invocations. In both
cases, the migration may adapt the system to the addition or removal of slots. Some systems can
continue operating during a reconfiguration process, while other systems need to be temporarily
stop and restart the jobs they are running: this approach appears in some system that adopt job-level
deployment: in this case, reconfiguration takes place by saving the current state, restarting the
whole system, and restoring the last recorded state.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:15

Data management §4

Data
intensive

Data processing §5

NoSQL §4.2

NewSQL §4.3

Dataflow with
task deployment §5.2

Dataflow with
job deployment §5.3

Graph processing §5.4

Computations on
data management §6.1

Other §6

New programming models – §6.2

Hybrid systems §6.3

Key-value §4.2.1

Wide-column §4.2.2

Document §4.2.3

Time-series §4.2.4

Graph §4.2.5

Key-value §4.3.1

Structured / relational §4.3.2

Object §4.3.3

Graph §4.3.4

Incremental computations §6.1.1

Long-running jobs §6.1.2

Graph processing §6.1.3

Stateful dataflow – §6.2.1

Reactors – §6.2.2

Time-based protocols

Deterministic execution

Explicit partition./repl.

Primary-based protocols

Fig. 3. Organization of the survey

3 SURVEY OF DATA-INTENSIVE SYSTEMS
The next sections use the model in Sec. 2 to survey data-intensive systems. We organize our
discussion in a hierarchical way, as shown in Fig. 3. At a first level, we distinguish between data
management systems (DMSs, Sec. 4), data processing systems (DPSs, Sec. 5), and other systems
that do not clearly fall into any of the two categories (Sec. 6). For each category, we further group
systems in a way that emphasizes their key commonalities with respect to the classification criteria
in Sec. 2. This allows us to easily describe all the systems that belong to the same group together.
For DMSs, we first distinguish between NoSQL systems (Sec. 4.2), which offer weak semantics for
replication and group operations, and NewSQL systems (Sec. 4.3), which offer strong semantics.
Within each class, we further classify systems based on the way they represent state elements
and on the protocols they use to implement strong semantics (when available). For DPSs, we
observed a clear distinction between dataflow systems that perform task-level deployment (Sec. 5.2)
and dataflow systems that perform job-level deployment (Sec. 5.3), while a third class of systems
focuses on graph data structures and algorithms (Sec. 5.4). As for the remaining systems, we may
distinguish among those that implement data processing or computational abstractions on top of a
DMS (Sec. 6.1), those that aim to offer new programming models (Sec. 6.2), and hybrid systems that
try to integrate data processing and management capabilities within a unified solution (Sec. 6.3).
Table 4 and Table 5 show how the classes of systems we identified map on the classification criteria
of our model. Next sections provide a step-by-step comment on the values in these tables.

4 DATA MANAGEMENT SYSTEMS
Data management systems (DMSs) offer the abstraction of a mutable state store that many jobs can
access simultaneously to query, retrieve, insert, and modify elements. Differently from data process-
ing systems, they mostly target lightweight jobs, which do not involve computationally expensive
data transformations and are short-lived. Since their advent in the seventies, relational databases
represented the standard approach to data management, offering a uniform data model (relational),
query language (SQL), and execution semantics (transactional). Over the last two decades, new
requirements and mutating operational conditions, brought the idea of a unified approach to data
management to an end [86, 87]: new applications emerged with different needs in terms of data and
processing models, for instance, to store and retrieve unstructured data; scalability concerns related

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:16

Data management

Data processing

NoSQL

NewSQL

Task depl

Jobs depl

Graph

Comput on
data manag

Other
New prog
models

Margara et al.

Hybrid

sys*

reg*

Driver
exec
Driver
exec time
Invocation
of jobs

Sources

Sinks

State

Deployment

client
+sysSP
reg
+startSP
sync
(+async)

sys dep

sys dep

yes

sys dep

client
+sysSP
reg
+startSP
sync
(+async)

no

no*

yes

sys dep

Jobs def API

class dep

class dep

Exec plan def

Task comm

Exec plan struct
Iterations
Dyn creation

impl*

impl

worfkl*
no*
no*

impl*

impl*

worfkl*
no*
no*

Nature of jobs

one-shot

one-shot

State man.

Data par. API
Placem-aware API

Jobs compil time
Use static res info
Use dyn res info

Granular of depl
Depl time
Use dyn res info
Managem of res

expl

no
no

on exec*
yes
no

job*
compil*
no
sys*

expl

no*
no*

on exec*
yes
sys dep

job
compil
no
sys*

Elem struct

Temp elem

Bus conn type

Bus impl

Bus persist
Bus partition
Bus repl
Bus interact

Elem struct
Stor medium
Stor struct

Task state

Shared state
Partitioned

Replication

Repl consist

Repl protocol
Upd propag

class dep

class dep

sys dep

direct

no

direct

gen (+spec)
noB
yesS
mediated*

net chan

net chan*

fs (+RAM)

ephem
yes
no
pull

class dep
sys dep
sys dep

no

yes
yes
yes/
backup
weak/
conf
lead/cons+confl res
op*

ephem
yes
no
pull*

class dep
sys dep
sys dep

no

yes
yes
yes/
backup

strong

lead/cons
op*

Functional components

sys/conf

sys dep

reg

reg

sys dep
passB
actS
yes
noB
yesS
cluster

async*
passB
actS
yes
noB
yesS
cluster

Jobs definition
lib
(+DSL)
expl
(+impl)

lib
(+DSL)
expl
(+impl)

impl

impl

dataflow
dataflow
sys dep
sys dep
no
no*
one-shotB
one-shotB
contS
contS
absentB
absentB
implS
implS
yes
yes
no
no
Jobs compilation

on exec
no
yes

on exec
yes
no

Jobs deployment and execution

job
task
compil
activ
no*
yes
shared*
sys*
Data management

gen (+spec)
noB
yesS
direct*
net chan/
msg service
ephem*
yes
no*
push*

pers*
yes
yes*
hybrid*
State management
–
–
–
noB
yesS
no
–

–
–
–
noB
yesS
no
–

–

–

–
–

–

–

–
–

sys

reg

client*

reg*

sys*

reg*

sys dep

sys dep

sys dep

sys dep

pass

yes

yes

sys dep

sys dep

sys dep

yes*

yes

yes*

yes

yes

yes

cluster

cluster*

cluster

cluster

lib

expl

expl

graph
yes
no*

cont

expl

yes
yes

on exec
yes
no

sys dep
sys dep
sys dep
sys*

sys dep

impl
(+expl)
impl
(+expl)
sys dep
no*
no

lib

sys dep

impl*

sys dep
yes
no*

one-shot

one-shot*

expl

no*
no

on exec*
yes
no

job
compil
no
sys*

expl

yes*
no*

on exec*
yes*
no*

sys dep
sys dep
no*
sys*

lib
(+DSL)
expl
(+impl)

impl

sys dep
no*
no
one-shotB
contS

expl*

yes*
sys dep

on exec*
yes
no

sys dep
sys dep
sys dep
sys

graph

sys dep

sys dep

spec*

no

sys dep
net chan/
mem
sys dep
yes
no
sys dep

graph
mem
user-def

yes

no
–

–

–

–
–

no*

direct

no

direct*

sys dep

direct*

net chan

net chan*

net chan*

ephem
yes
no
push*

sys dep
sys dep
sys dep

no

yes
yes
yes/
backup

sys dep

lead.*
op*

ephem*
yes
no*
push*

gen*
mem
sys dep

no

yes
yes

no

–

–
–

ephem*
yes
no*
push*

spec*
mem*
sys dep

sys dep

yes*
yes*

sys dep

–*

–
–*

Table 4. Survey of systems: functional components; jobs definition, compilation, deployment, and execution;
data management; state management. Legend: sys dep= system dependent; class dep= differences captured
by the specific sub-classes in our taxonomy (see Fig. 3); *= with few system-specific exceptions; SP= in the
case of stored procedures; B= in the case of batch processing; S= in the case of stream processing.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:17

Data management

Data processing

NoSQL

NewSQL

Task depl

Jobs depl

Graph

Comput on
data manag

Other
New prog
models

Aborts
Protocol
Assumptions

Level
Implement
Assumptions

Delivery guar
Nature of ts
Order guar

–*
–*
–

–/coord free*
–/SEQ
–/1P

most/exact*
no/event
–

job+sys*
blocking*
–/DC/1W

blocking*
ts/lock
no/DC/1W

exact*
no
–

Detection

sys dep

mast-work*

Scope

Comput recov
State recov

shared st

–
log (+repl)

shared st

–
log (+repl)

Guar for state

none/conf*

same/conf

Assumptions

STOR/
REPL

STOR/
REPL (+DC)

–
–
–

–
–
–

Group atomicity

–
–
–

Group isolation

–
–
–

Delivery and order

exact/least
no/event
alw/event*
Fault tolerance
mast-work
comp
+task stS
job*
checkp*

exact
noB/eventS
–B/alwS

mast-work
comp
+task stS
task
–B/checkpS
–B
valid/sameS

valid/same

valid/same

REPLAY

REPLAY

–

Dynamic reconfiguration

Goal

Automated
State migr.
Task migr.
Add/rem slots
Restart

avail
+load bal+elast
sys dep
yes
–
yes
no

change schema
+load bal+elast
sys dep
yes
–
yes
no*

–/elast

–/yes
–B/yesS
–B/yesS
–/yes
–/no

–/
load bal+elast
sys dep
yes
yes
–/yes
sys dep

load bal

yes
yes
yes
no
no

–
–
–

–
–
–

exact
no
–

job+sys*
blocking*
no

blocking
ts/lock*
no/1P

exact*
no*
–

–*
–*
–

sys dep
sys dep
no

exact
no
–

Hybrid

job+sys*
blocking*
no/DC *

sys dep
sys dep
no/DC *

exact
sys dep
–/alw

mast-work

comp+task st

job*
checkp

mast-work*
shared st
(+comp)
sys dep
sys dep

mast-work*
shared st
(+comp)
sys dep
checkp*

mast-work*

sys dep

sys dep
log+checkp

same*

STOR

sys dep

yes
yes
sys dep
yes
no

valid

valid/same*

sys dep

sys dep

–/elast

–/yes
–/yes
–/yes
–/yes
–/no

–/
load bal+elast
sys dep
–/yes
–/yes
sys dep
sys dep

Table 5. Survey of systems: group atomicity and isolation; delivery and order; fault tolerance; dynamic
reconfiguration. Legend: sys dep= system dependent; *= with few system-specific exceptions; B= in the case
of batch processing; S= in the case of stream processing; DC = jobs are deterministic; SEQ = jobs are executed
sequentially, with no interleaving; 1W = a single worker handles all writes; 1P = jobs access a single state
portion; STOR = storage layer is durable; REPL = replicated data is durable; REPLAY = sources are replayable.

to data volume, number of simultaneous users, and geographical distribution pointed up the cost
of transactional semantics. This state of things fostered the development of the DMSs presented in
this section. Sec. 4.1 discusses the aspects in our model that are common to all such systems. Then,
following an established terminology, we organize them in two broad classes: NoSQL databases [39]
(Sec. 4.2) emerged since the early 2000s, providing simple and flexible data models such as key-
value pairs, and trading consistency guarantees and strong (transactional) semantics for horizontal
scalability, high availability, and low response time; NewSQL databases [85] (Sec. 4.3) emerged in
the late 2000s and take an opposite approach: they aim to preserve the traditional relational model
and transactional semantics by introducing new design and implementation strategies that reduce
the cost of coordination.

4.1 Overview

Functional model. All DMSs provide a global shared state that applications can simultaneously
access and modify. In the more traditional systems there is a sharp distinction between the appli-
cation logic (the driver, executed client-side on registration) and the queries (the jobs, executed
by the distributed computing infrastructure). More recent systems increasingly allow to move the
part of the application logic that orchestrates jobs execution within the DMS, in the form of stored
procedures that run system-side on start. Stored procedures may bring two advantages: (i) reducing
the interactions with external clients, thus improving latency; (ii) moving part of the overhead for
compiling jobs from driver execution time to driver registration time.

Being conceived for interactive use, all DMSs offer synchronous APIs to invoke jobs from the
driver program. Many also offer asynchronous APIs that allow the driver to invoke multiple jobs

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:18

Margara et al.

and receive notifications of their results when they terminate. A common approach to reduce the
cost of communication when starting jobs from a client-side driver is batching multiple invocations
together, which is offered in some NoSQL systems such as MongoDB [35] and Redis [62].

Several DMSs can interact with active sources and sinks. Active sources push new data into the
system, leading to insertion or modification of state elements. Sinks can register to state elements
of interest (e.g., by specifying a key or a range of keys) and are notified upon modification of such
elements. In some cases, the communication with sources and sinks plays a central role in the
system definition: for instance, the Redis [62] official documentation presents the system as both
a key-value store and an efficient publish-subscribe messaging system that notifies subscribers
(sinks) when the values associated to keys change.

DMSs greatly differ in terms of deployment strategies, which are vastly influenced by the
coordination protocols that govern replication, group atomicity and isolation. NoSQL systems
do not offer group atomicity and isolation. Those designed for cluster deployment typically use
synchronous or semi-synchronous replication protocols. Those that support wide area deployments
either use asynchronous replication protocols that reduce durability and consistency guarantees,
or employ a hybrid strategy, with synchronous replication within a data center and asynchronous
replication across data centers. Many NewSQL systems claim to support wide area deployments
while offering strong consistency replication, group atomicity and isolation. We review their
implementation strategies to achieve this result in Sec. 4.3.

Jobs. All DMSs implement one-shot jobs with explicit state management primitives to read and
modify a global shared state. Jobs definition API greatly differ across systems, ranging from pure
key-value stores that offer CRUD (create, read, update, and delete) primitives for individual state
elements to expressive domain specific libraries (e.g., for graph computation) or languages (e.g., SQL
for relational data). In almost all DMSs, the execution plan and the communication between tasks
are implicit and do not include iterations or dynamic creation of new tasks. A notable exception are
graph databases, which support iterative algorithms where developers explicitly define how tasks
update portions of the state (the graph) and exchange data. The structure of the execution plan also
varies across systems, ranging from single tasks that implement CRUD primitives, to dataflows, to
various types of workflows (e.g., with a central coordination or hierarchical). Jobs are compiled on
driver execution, except for those systems where part of the driver program is registered server-side
(as stored procedures). Job compilation always uses static information about resources, such as
the allocation of shared state portions onto nodes. Some structured and relational NewSQL DMSs
such as Spanner [15] and CockroachDB [89] also exploit dynamic information about resources, for
instance to configure a given task (e.g., select a sort-merge or a hash-based strategy for a join task)
depending on some statistics about resource utilization or state (e.g., cardinality of the tables to
join).

All DMSs perform the deployment at job level, when the job is compiled, with the only exception
of AsterixDB [10] that compiles jobs into a dataflow plan and deploys individual tasks when they
are activated [22]. Deployment is always guided by the location of the state elements to be accessed,
which we consider as a static information. Indeed, under normal execution, shared state portions
do not move, and our model captures dynamic relocation of shared state portions (e.g., for load
balancing) as a distinct aspect (dynamic reconfiguration). Also, we keep saying that the deployment
is based on static information even for those systems that exploit dynamic information (e.g., the load
of workers) but only to deploy the tasks that manage the communication between the system and
external clients. Finally, DMSs are not typically designed to operate in scenarios where the compute
infrastructure is shared with other software applications. This is probably due to the interactive
and short-lived nature of jobs, which would makes it difficult to predict and adapt the demand

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:19

of resources in those scenarios. The only case in which we found explicit mention of a shared
platform is in the description of the Google infrastructure, where DMSs components (BigTable [32],
Percolator [75], Spanner [37]) share the same physical machines and their scheduling, execution,
and monitoring is governed by an external resource management service.

Data and state management. The data model (that is, the structure of data and state elements) is
a key distinguishing characteristic of DMSs, which we use as the classification criterion to organize
our discussion in Sec. 4.2 and Sec. 4.3. Some systems also explicitly consider the temporal dimension
of data elements: for instance, some wide columns stores associate timestamps to elements and
let users store multiple versions of the same element, while time series databases are designed to
efficiently store and query sequences of measurements over time.

DMSs also differ in terms of storage medium and structure. We detail the choices of the different
classes of systems in Sec. 4.2 and Sec. 4.3, but we can identify some common concerns and design
strategies. First, the representation of state on storage is governed by the data model and the
expected access pattern: relational tables are stored by row, whereas time series are stored by
column to facilitate computations on individual measurements over time (e.g., aggregation, trend
analysis). Second, there is a tension between read and write performance: read can be facilitated
by indexed data structures such as B-trees, but they incur higher insertion and update costs.
Hierarchical structures such as log-structured merge (LSM) trees improve write performance by
buffering updates in higher-level logs (frequently in-memory) that are asynchronously merged into
lower-level indexed structured, at the expense of read performance, due to the need to navigate
multiple layers. Third, most DMSs exploit main memory to improve data access latency. For instance,
systems based on LSM-trees store the write buffer in memory. Similarly, most systems that use
disk-bases storage frequently adopt some in-memory caching layer. Finally, some systems adopt
a modular architecture that supports different storage layers. This is common in DMSs offered
as a service in public cloud environments (e.g., Amazon Aurora [94]) or in private data centers
(e.g., Google Spanner [37]), where individual system components (including the storage layer) are
themselves services.

Tasks always communicate using direct, ephemeral connections, which implement a partitioned,
non-replicated data bus. Shared state is always partitioned across workers to enable concurrent
execution of tasks. Most DMSs also adopt state replication to improve read access performance,
with different guarantees in terms of consistency between replicas: NoSQL databases provide weak
(or configurable, in some cases) consistency guarantees to improve availability and reduce response
time. NewSQL databases provide strong consistency using leader-based or distributed consensus
protocols. Group atomicity and isolation are typically absent or optional in NoSQL databases, or
restricted to very specific cases, such as jobs that operate on single data elements. Instead, NewSQL
databases provide strong guarantees for atomicity and isolation, at the cost of blocking coordination.
The transactional semantics of NewSQL systems ensures exactly once delivery. Indeed, transactions
(jobs) either complete successfully (and their effects become durable) or abort, in which case they
are either automatically retried or the client is notified and can decide to retry them until success.
Conversely, NoSQL systems frequently offer at most once semantics, as they do not guarantee
that the results of job execution are safely stored on persistent storage or replicated. In some
cases, users can balance durability and availability by selecting the number of replicas that are
updated synchronously. Finally, systems that support timestamps use event time semantics, where
timestamps are associated to state elements by clients, while none of the systems provides order
guarantees. Even in the presence of timestamps, jobs are not guaranteed to consider all elements
produced before a given timestamps: indeed, DMSs do not implement any mechanisms to account
for elements produced or received out of timestamp order.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:20

Margara et al.

Fault tolerance. Frequently, DMSs offer multiple mechanisms for fault tolerance and durability
that administrators can enable and combine depending on their needs. Fault detection can be
centralized (master-slave) or distributed, depending on the specific system. Fault recovery mostly
targets the durability of shared state. Since jobs are lightweight, DMSs simply abort them in the case
of failure and do not attempt to recover (partial) computations. Transactional systems guarantee
group atomicity: in the case of failure, none of the effects of a job become visible. To enable recovery
of failed jobs, they either notify the clients about an abort, allowing them to restart the failed job, or
restart the job automatically. Almost all DMSs adopt logging mechanisms to ensure that the effects
of jobs execution on shared state are durable. Logging enables changes to be recorded on some
durable append-only storage before being applied to shared state: most systems adopt a write-ahead
log that records changes to individual data elements, while few others adopt a command log that
stores the operations (commands) that perform the change. Individual systems make different
assumptions on what they consider as durable storage: in some cases, logs are stored on a single
disk, but more frequently they are replicated (or saved on third party log services that are internally
replicated). Logging is frequently used in combination with replication of shared state portions
on multiple workers: in these cases, each worker stores its updates on a persistent log to recover
from software crashes, but in the case of hardware crashes or network disconnections, replication
on other workers may avoid unavailability. In addition, most systems also offer geo-replication
for disaster recovery, where the entire shared state is replicated in a different data center and
periodically synchronized with the working copy. Similarly, many systems provide periodic or
manual checkpointing to store a copy of the entire database at a given point in time. Depending on
the specific mechanisms adopted, DMSs provide either no guarantees for state, for instance in the
case of asynchronous replication, or same state guarantees, for instance in the case of persistent
log or consistent replication.

Dynamic reconfiguration. Most DMSs support adding and removing workers at runtime, and
migrating shared state portions across workers, which enable dynamic load balancing and scaling
without restarting the system. All systems support dynamic reconfiguration as a manual adminis-
trative procedure, but many can also automatically migrate state portions for load balancing. A
special case of reconfiguration for NewSQL systems that rely on structured state (in particular,
relational systems) involves changing the state schema: many systems support arbitrary schema
changes as long-running procedures that validate state against the new schema and migrate it
while still serving clients using the previous schema. Instead, systems such as VoltDB [88] rely on
a given state partitioning scheme and prevent changes that violate such scheme.

4.2 NoSQL systems
Using an established terminology, we classify as NoSQL all those DMSs that aim to offer high
availability and low response time by relinquishing features and guarantees that require blocking
coordination between workers. In particular, they typically: (i) avoid expressive job definition API
that may lead to complex execution plans (as in the case of SQL, hence the name) [84]. In fact, the
majority of the systems we analyzed focuses on jobs comprising a single task that operates on an
individual element of the shared state. (ii) Use asynchronous replication protocols: depending on
the specific protocol, this may affect consistency, durability (if replication is used for fault tolerance)
and may generate conflicts (when clients are allowed to write to multiple replicas). (iii) Abandon or
restrict group guarantees (atomicity and isolation), when jobs with multiple tasks are supported. In
our discussion, we classify systems by the data model they offer.

4.2.1 Key-value stores. Key-value stores offer a very basic API for managing shared state: (i) shared
state elements are represented by a key and a value; (ii) elements are schema-less, meaning that

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:21

different elements can have different formats, such as free text or JSON objects with heterogeneous
attributes; (iii) the key-space is partitioned across workers; (iv) jobs consist of a single task that
retrieves the value of an element given a key (get) or insert/update an element given its key (put).
Individual systems differ in the way they physically store elements. For instance, Dynamo [42]
supports various types of physical storage, DynamoDB2 uses B-Trees on disk but buffers incoming
updates in main memory for improved write performance, while Redis [62] stores elements in
memory only. Keys are typically partitioned across workers based on their hash (hash partitioning),
but some systems also support range partitioning, where each worker stores a sequential range of
keys, as in the case of Redis. Given the focus on latency, some systems cache the association of
keys to workers client-side, allowing clients to directly forward requests to workers responsible for
the key they are interested in.

Individual systems may provide richer API to simplify the interaction with the store: (i) Keys
can be organized into tables, mimicking the concept of a table in a relational database, for instance,
DynamoDB and PNUTS [36] let developers split state elements into tables; (ii) Elements may have
an associated structure, for instance, PNUTS let developers optionally define a schema for each table,
DynamoDB specifies a set of attributes but does not constrain their internal structure, Redis provides
built-in datatypes to define values and represent them efficiently in memory; (iii) Most systems
provide functions to iterate (scan) on the keyspace or on individual tables (range-based partitioning
may be used to speedup such range-based iterations), as it happens for DynamoDB, PNUTS, and
Redis; (iv) Finally, some systems provide query operations (select) to retrieve elements by value
and in some cases these operations are accompanied by secondary indexes that are automatically
updated when the value of an element changes, as in DynamoDB.

All key-value stores replicate the shared state to increase availability, but use different replication
protocols. Dynamo, Voldemort3, and Riak KV4 use a quorum approach, where read and write
operations for a given key need to be processed by a given number of replica workers responsible
for that key. After a write quorum is reached, updates are propagated to remaining replicas
asynchronously. A larger number of replicas and a larger write quorum better guarantee durability
and consistency at the cost of latency and availability. However, being designed for availability, these
systems adopt mechanisms to avoid blocking on write when some workers are not responsive: for
instance, other workers can supersede and store written values on their behalf. In some cases, these
mechanisms can lead to conflicting simultaneous updates: Dynamo tracks causal dependencies
between writes to solve conflicts automatically whenever possible, and stores conflicting versions
otherwise, leaving manual reconciliation to the users. DynamoDB, Redis, and Aerospike [83]
use single-leader protocols where all writes are processed by a single worker and propagated
synchronously to some replicas and asynchronously to others, depending on the configuration.
DynamoDB also supports consistent reads that are always processed by the leader at a per-operation
granularity. Redis supports multi-leader replications in the case of wide-area scenarios, using conflict
free replicated datatypes for automated conflict resolution.

In summary, key-value stores represent the core building block of a DMS. They expose a low-
level but flexible API to balance availability, consistency, and durability, and to adapt to different
deployment scenarios. Systems that adopt a modular implementation can use key-value stores as
a storage layer or a caching layer [72] and build richer job definition API, job execution engines,
protocols for group atomicity, group isolation, and consistent replication on top.

2http://aws.amazon.com/dynamodb/
3https://www.project-voldemort.com/
4https://riak.com/products/riak-kv/

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:22

Margara et al.

4.2.2 Wide-column stores. Wide-column stores organize shared state into tables (multi-dimensional
maps), where each row associates a unique key to a fixed number of column families, and each
column family contains a value, possibly organized into more columns (attributes). State is physically
stored per column family and keys need not have a value for each column family (the table is
typically sparse). One could define the wide-column data model as middle ground between the key-
value and the relational model: it is similar to the key-value model, but associates a key to multiple
values (column families); it defines tables as the relational model, but tables are sparse and lack
referential integrity. The main representative systems of this class are Google BigTable [32], with
its open-source implementation HBase5, and Apache Cassandra [56]. As the official documentation
of Cassandra explains6, the typical use of wide-column systems is to compute and store answers to
frequent queries (read-only jobs) for each key, at insertion/update time, within column families.
In contrast, relational databases normalize tables to avoid duplicate columns and compute results
at query time (rather then insertion/update time) by joining data from multiple tables. In fact,
wide-column stores offer rich API to scan, select, and update values by key, but do not offer any join
primitive. To support the above scenario, wide-column systems: (i) aim to provide efficient write
operations to modify several column families, for instance, both BigTable and Cassandra adopt
log-structured merge trees for storage and improve write latency by buffering writes in memory;
(ii) provide isolation for operations that involve the same key. These two design choices allow users
to update all entries for a given key (answers to queries) efficiently and in isolation.

BigTable and Cassandra have different approaches to replication. BigTable uses replication only
for fault tolerance and executes all tasks that involve a single key on the leader worker responsible
for that key. It also supports wide-area deployment by fully replicating the data store in additional
data centers: replicas in these data centers can be used for fault tolerance but also to perform
jobs, in which case they are synchronized with eventual consistency. Cassandra uses quorum
replication as the Dynamo key-value store, and allows users to configure the quorum protocols to
trade consistency and durability for availability.

4.2.3 Document stores. Document stores represent a special type of key-value stores where values
are structured documents, such as XML or JSON objects. Document stores offer an API similar to
key-value stores but they can exploit the structure of documents to update only some of their fields.
Physical storage solutions vary across systems, ranging from disk-based, to memory solutions, to
hybrid approaches and storage-agnostic solutions. In most cases document stores support secondary
indexes to improve retrieval of state elements using criteria different from the primary key. Most
document stores offer group isolation guarantees for jobs that involve a single document. This
is the case of MongoDB [35] and AsterixDB [10]. Recent versions of MongoDB also implement
multi-document atomicity and isolation as an option, using blocking protocols.

MongoDB supports replication for fault tolerance or also to serve read-only jobs. It implements a
single leader protocol with semi-synchronous propagation of changes, where clients can configure
the number of replicas that need to synchronously receive an update, thus trading durability and
consistency for availability and response time. CouchDB [11] offers a quorum-based replication
protocol and allows for conflicts in the case a small write quorum is selected. In this case, conflict
resolution is manual. AsterixDB does not currently support replication.

Interestingly, several document stores support some form of data analytic jobs: for instance,
MongoDB offers jobs in the form of a pipeline of data transformations that can be applied in
parallel to a set of documents. CouchDB focuses on Web applications and can start registered jobs
when documents are added or modified to compute and update some views. AsterixDB provides

5https://hbase.apache.org
6https://cassandra.apache.org/doc/latest/cassandra/data_modeling/index.html

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:23

a declarative language that integrates operators for individual documents as well as for multiple
documents (like joins, group by). Jobs are compiled into a dataflow execution plan.

4.2.4 Time-series stores. Time-series stores are a special form of wide-column stores dedicated
to store sequences of values over time, for instance measurements of a numeric metric such as
the CPU utilization of a computer over time. Given the specific application scenario, this class of
systems stores data by column, which brings several advantages: (i) together with the use of an
in-memory or hybrid storage layer, it improves the performance of write operations, which typically
append new values (measurements) to individual columns; (ii) it offers faster sequential access
to columns, which is common in read-only jobs that perform aggregations or look for temporal
patterns over individual series; (iii) it enables a higher degree of data compression, for instance by
storing only the difference between adjacent numerical values (delta compression), which is small
if measurements change slowly.

Among the time-series stores we analyzed, InfluxDB7 is the most general one. It provides a
declarative language for jobs that supports computations on individual columns (measurements).
Gorilla [74] is used as an in-memory cache to store monitoring metrics at Facebook. Given the
volume and rate at which metrics are produced, Facebook keeps most recent data at a very fine
granularity within the Gorilla cache and stores historical data at a coarser granularity in HBase.
Peregreen [96] follows a similar approach and optimized retrieval of data through indexing. It uses
a three-tier data indexing, where each tier pre-computes aggregated statistics (minimum, maximum,
average, etc.) for the data it references. This allows to quickly identify chunks of data that satisfy
some conditions based on the pre-computed statistics and to minimize the number of interactions
with the storage layer. Monarch [3] is used to store monitoring data at Google. It has a hierarchical
architecture: data is stored in the zone (data center) in which it is generated and sharded (by key
ranges, lexicographically) across nodes called leaves. Jobs are evaluated hierarchically: nodes are
organized in three layers (global, zone level, leaves) and the job plan pushes tasks as close as
possible to the data they need to consume. All time-series stores we analyzed replicate shared state
to improve availability and performance of read operations. To avoid blocking write operations,
they adopt asynchronous or semi-synchronous replication, thus reducing durability guarantees.
This is motivated by the specific application scenarios, where losing individual measurements may
be tolerated.

4.2.5 Graph stores. Graph stores are a special form of key-value stores specialized in graph-shaped
data, meaning that shared state elements represent entities (vertices of a graph) and their relations
(edges of the graph). Despite researchers widely recognized the importance of large scale graph data
structures [76], several graph data stores do not scale horizontally [79]. A prominent examples of
distributed graph store is TAO [23], used at Facebook to manage the social graph that interconnects
users and other entities such as posts, locations, and actions. It builds on top of key-value stores
with hybrid storage (persisted on disk and cached in memory), asynchronously replicated with no
consistency guarantees and without support for group atomicity and group isolation.

A key distinguishing factor in graph stores is the type of queries (read-only jobs) they support.
Indeed, a common use of graph stores is to retrieve sub-graphs that exhibit certain patterns of
relations: for instance, in a social graph, one may want to retrieve people (vertices) that are direct
friends or have friends in common (friendship relation edges) and like the same posts. This problem
is denoted as graph pattern matching and its general form can only be solved by systems that can
express iterative or recursive jobs, as it needs to traverse the graph following its edges. These types
of vertex-centric computations have been first introduced in the Pregel data processing system [64],

7https://www.influxdata.com

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:24

Margara et al.

also discussed in Sec. 5. When discussing hybrid systems in Sec. 6.1.3, we present Trinity [79]: a
system that supports vertex-centric computations on top of a graph data store.

Efficient query of graph stores can also be supported by external systems. For instance, Facebook
developed the Unicorn [38] system to store indexes that allow to quickly navigate and retrieve data
from a large graph. Indexes are updated periodically using an external compute engine. Unicorn
adopts a hierarchical architecture, where indexes (the shared state of the system) are partitioned
across servers and the results of index lookups (read jobs) are aggregated first at the level of
individual racks and then globally to obtain the complete query results. This approach aggregates
results as close as possible to the servers producing them to reduce network traffic. Unicorn supports
graph patterns queries by providing an apply function that can dynamically starting new lookups
based on the results of previous ones: our model captures this feature by saying that jobs can
dynamically start new tasks.

4.3 NewSQL systems
NewSQL systems aim to provide transactional semantics (group atomicity and isolation), dura-
bility (fault tolerance), and strong replication consistency while preserving horizontal scalability.
Following the same approach we adopted in Sec. 4.2, we organize them according to their data
model.

4.3.1 Key-value stores. NewSQL key-value stores are conceived as part of a modular system,
where the store offers transactional guarantees to read and update a group of elements with
atomicity and isolation guarantees, and it is used by a job manager that compiles and optimizes
jobs written in some high-level declarative language. A common design principle of these systems
is to separate the layer that manages the transactional semantics from the actual storage layer, thus
enabling independent scaling based on the application requirements. Deuteronomy [58] implements
transactional semantics using a locking protocol and is storage agnostic. FoundationDB [100] uses
optimistic concurrency control and uses a storage layer based on B-Trees. Solar [101] also uses
optimistic concurrently control with log structured merge trees.

Structured and relational stores. Stores for structured and relational data provide the same
4.3.2
data model, job model, and job execution semantics as classic non-distributed relational databases.
As we clarify in the classification below, they differ in their protocols for implementing group
atomicity, group isolation, and replication consistency, which reflects on their architectures.

Time-based protocols. Some systems exploit physical (wall-clock) time to synchronize nodes. This
approach was pioneered by Google’s Spanner [37]. Spanner adopts standard database techniques:
two-phase commit for atomicity, two-phase locking and multi-version concurrency control for
isolation, and single-leader synchronous replication of state portions. The Paxos consensus protocol
is used to elect a leader for each state portion and to keep replicas consistent. The key distinguishing
characteristic of Spanner is the use TrueTime, a clock abstraction that uses atomic clocks and
GPS to return physical time within a known precision bound. In Spanner, each job is managed
by a transaction coordinator, which assigns jobs with a timestamp at the end of the TrueTime
clock uncertainty range and waits until this timestamp is passed for all nodes in the system. This
ensures that jobs are globally ordered by timestamp, thus offering the illusion of a centralized
system with a single clock (external consistency). Spanner is highly optimized for workloads with
many read-only jobs. Indeed, multi-version concurrency control combined with TrueTime allows
read-only jobs to access a consistent snapshot of the shared state without locking and without
conflicting with in-progress read-write jobs, as they will be certainly be assigned a later timestamp.
More recently, Spanner has been extended with support for distributed SQL query execution [15].

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:25

CockroachDB [89] is similar to Spanner (it also supports distributed execution plans) but uses
an optimistic concurrency control protocol that, in the case of conflicts, attempts to modify the
timestamp of a job to a valid one rather than re-executing the entire job. CockroachDB supports
wide-area deployment and allows users to define how data is partitioned across regions, to promote
locality of data access or to enforce privacy regulations.

Deterministic execution. Calvin [91]8 builds on the assumption that jobs are deterministic and
achieves atomicity, isolation, and consistency by ensuring that jobs are executed in the same order
in all replicas. Determinism ensures that jobs either succeed or fail in any replica (atomicity),
interleave in the same way (global order ensures isolation), leading to the same results (consistency).
Workers are organized into three layers: (i) A sequencing layer receives jobs invocations from
clients, organizes them into batches, and orders them consistently across replicas. Ordering of
jobs is the only operation that requires coordination and takes place before jobs execution. Calvin
provides both synchronous (Paxos) and asynchronous protocols for ordering job that bring different
tradeoff between latency and cost of recovery in the case of failures. (ii) A scheduler layer that
executes tasks onto workers in the defined global order. In cases where it is not possible to statically
determine which shared state portions will be involved in the execution of a job (for instance in the
case of state-dependent control flow), Calvin uses an optimistic protocol and aborts jobs if some of
their tasks are received by workers out of order. (iii) A storage layer that stores the actual data. In
fact, Calvin supports any storage engine providing a key-value interface.

Explicit partitioning and replication strategies. VoltDB [87, 88] lets users control partitioning and
replication of shared state, so they can optimize most frequently executed jobs. For instance, users
can specify that a Customer and a Payment tables are both partitioned by the attribute (column)
customerId. Jobs that are guaranteed to access only a shared state portion within a given worker
are executed sequentially and atomically on that worker. For instance, a job that accesses tables
Customer and Payment to retrieve information for a given customerId can be fully executed on
the worker with the state portion that includes that customer. Every table that is not partitioned is
replicated in every worker, which optimizes read access from any worker at the cost of replicating
state changes. In the case jobs need to access state portions at different workers, VoltDB resorts to
standard two-phase commit and timestamp-based concurrency control protocols. Differently from
Spanner and Calvin, VoltDB provides strong consistency only for cluster deployment: geographical
replication is supported, but only implemented with asynchronous and weakly consistent protocols.

Primary-based protocols. Primary-based protocols are a standard approach to replication used in
traditional transactional databases. They elect one primary worker that handles all read-write jobs
and acts as a coordinator to ensure transactional semantics. Other (secondary) workers only handle
read-only jobs and can be used to fail over if the primary crashes. Recently, the approach has been
revamped by DMSs offered as services on the cloud. These systems adopt a layered architecture
that decouples jobs execution functionalities (e.g., scheduling, managing group atomicity and
isolation) from storage functionalities (durability): the two layers are implemented as services that
can scale independently from each other. The execution layer still consists of one primary worker
and an arbitrary number of secondary workers. However, they access shared state through the
storage service rather than storing shared state portions locally (although they typically implement
a local cache to improve performance). Amazon Aurora [94] implements the storage layer as a
sequential log (replicated for availability and durability), which offers better performance for write
operations. Indexed data structures that improve read performance are materialized asynchronously
without affecting write latency. The storage layer uses a quorum approach to guarantee replication

8The database is available under the name of Fauna: https://fauna.com

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:26

Margara et al.

consistency across workers. Microsoft Socrates [12] adopts a similar approach but further separates
storage into a log layer (that stores write requests with low latency), durable storage layer (that
stores a copy of the shared state), and a backup layer (that periodically copies the entire state store).

4.3.3 Objects stores. Object-oriented stores [18] became popular in the early Nineties, inheriting
the same data model as object-oriented programming languages. We found one recent example of
a distributed DMS that uses the object oriented data model, namely Tango [17]. In Tango, clients
store their view of objects locally, in-memory, and this view is kept up to date with respect to a
distributed (partitioned) and durable (replicated) log of updates. The log represents the primary
replica of the shared state that all clients refer to. All updates to objects are globally ordered on
the log through sequence numbers that are obtained through a centralized sequencer. Total order
guarantees isolation for operations on individual objects: Tango also offers group atomicity and
group isolation across objects using the log to store information for an optimistic concurrency
control protocol.

4.3.4 Graph stores. We found one example of graph store, named A1 [25], that provides strong
consistency, atomicity, and isolation using timestamp-based concurrency control. Its data model is
similar to that of NoSQL distributed graph stores, and jobs can traverse the graph and read and
modify its associated data during execution. The key distinguishing characteristic of A1 is that it
builds on a distributed shared memory abstraction that uses RDMA (remote direct memory access)
implemented within network interface cards [43].

5 DATA PROCESSING SYSTEMS
Data processing systems (DPSs) aim to perform complex computations (long lasting jobs) on large
volumes of data. Most of today’s DPSs inherit from the seminal MapReduce system [41]: to avoid the
hassle of concurrent programming and to simplify scalability, they organize each job into a dataflow
graph where vertices represent functional operators for data transformation and edges indicate
how data flows across operators. Each operator is applied in parallel to independent partitions of
the input data, and the system automatically handles data partitioning and data transfer across
workers. Following an established terminology, we denote as batch processing systems those that
take in input static (finite) datasets, and stream processing systems those that take in input streaming
(potentially unbounded) datasets. In practice, many systems support both types of input and we do
not use the distinction between batch and stream processing as the main factor to organize our
discussion. Instead, after discussing the aspects in our model that are common to all DPSs (Sec. 5.1),
we classify dataflow systems based on the key aspect that impacts their implementation: if they
deploy individual tasks on activation (Sec. 5.2) or entire jobs on registration (Sec. 5.3). Finally, we
present systems designed to support computations on large graph data structures. They evolved in
parallel with respect to dataflow systems, which originally were not suited for the iterative style of
computation that is typical in graph algorithms (Sec. 5.4).

5.1 Overview

Functional model. Most DPSs use a master-workers architecture, where one of the processes
that compose the system (denoted the master) has the special role of coordinating other workers.
Such systems always allow submitting the driver program to the master for system-side execution.
Some of them also allow client-side driver execution, such as Apache Spark [99] and Apache
Flink [27]. Other systems, such as Kafka Streams [20] and Timely Dataflow [70], are implemented
as libraries where client processes also act as workers. Developers start one or more processes
and the library handles the distributed execution of jobs onto them. Stream processing systems
support asynchronous invocation of (continuous) jobs, whereas batch processing systems may

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:27

offer synchronous or asynchronous job invocation API, or both. All DPSs support sources and
sinks. Sources are passive in the case of batch processing systems and active in the case of stream
processing systems. Most batch processing systems are stateless: output data is the result of
functional transformations of input data. Stream processing systems can persist a (task) state across
multiple activations of a continuous job. We model iterative graph algorithms as continuous jobs
where tasks (associated to vertices, edges, or sub-graphs) are activated at each iteration and store
their partial results (values associated to vertices, edges, or sub-graphs) in task state. DPSs assume
a cluster deployment, as job execution typically involves exchanging large volumes of data (input,
intermediate results, and final results) across workers.

Jobs. All dataflow-based systems provide libraries to explicitly define the execution plan of jobs.
Increasingly often, they also offer higher-level abstractions for specific domains, such as relational
data processing [2, 13, 26], graph computations [47], or machine learning [67]. Some of these APIs
are declarative in nature and make the definition of the execution plan implicit. Task communication
is always defined implicitly and controlled by the system’s runtime. With respect to jobs, dataflow
systems differ with respect to the following aspects: (i) Generality. MapReduce [41] and some
early systems derived from it only support two processing stages with fixed operators, while later
systems such as Spark support any number of processing stages and a vast library of operators;
(ii) Support for iterations. Systems such as HaLoop [24] extended MapReduce to efficiently support
iterations by caching data accessed across iterations in workers. Apache Spark inherits the same
approach and, together with Flink, supports some form of iterative computations for streaming
data. Timely Dataflow [70] generalizes the approach to nested iterations; (iii) Dynamic creation
of tasks. To the best of our knowledge, only one system, CIEL [71], enables to dynamically create
tasks depending on the results of processing.

Data parallelism is key to dataflow systems, and all operators in their jobs definition API are
data parallel. Jobs are one-shot in the case of batch processing and continuous in the case of stream
processing. In the latter case, jobs may implicitly define some task state. Jobs cannot control task
placement explicitly. Many systems, however, provide configuration parameters to guide placement
decisions, for instance to force or inhibit the colocation of certain tasks. An exception to the above
rules is represented by graph processing systems, which are based on a programming model where
developers define the behavior of individual vertices [66]: the model provides explicit primitives to
access the state of a vertex and for sending messages between vertices (explicit communication).
All DPSs compile jobs on driver execution. For other characteristics related to jobs compilation,
deployment, and execution, we distinguish between systems that perform task level deployment
(discussed in Sec. 5.2) and systems that perform job level deployment (discussed in Sec. 5.3).

Data and state management. Deployment and execution strategies affect the implementation of
the data bus. In the case of job-level deployment, the data bus is implemented using ephemeral,
push-based communication channels between tasks. In the case of task-level deployment, the data
bus is mediated and implemented by a persistent service (e.g., a distributed filesystem or a persistent
message queuing system) where upstream tasks push the intermediate results of their computation
and downstream tasks pull them when activated. A persistent data bus can be replicated for fault
tolerance (as in the case of Kafka Streams, which builds on replicated Kafka topics) or to make
data available at multiple workers (possible in Spark when caching intermediate results). Notably,
CIEL [71] and Dryad [51] support hybrid bus implementations, where some connections may be
direct while others may be mediated. Data elements may range from arbitrary strings (unstructured
data) to specific schemas (structured data). The latter offer opportunities for optimizations in
the serialization and deserialization process, for instance allowing for better compression or for
selective deserialization of only the fields that are accessed by a given task. In general, DPSs do

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:28

Margara et al.

not provide shared state. Stream processing and graph processing systems include a task state to
persist information across multiple activations of a continuous job. In absence of shared state, DPSs
do not provide group atomicity or isolation properties. Almost all systems provide exactly once
delivery, under the assumption that sources can persist and replay data in the case of failure and
sinks can distinguish duplicates. The concrete approaches to provide such guarantee depend on the
type of deployment (task-level or job-level) and are discussed later in Sec. 5.2 and Sec. 5.3. Order
is relevant for stream processing systems: with the exception of Apache Storm [92], all stream
processing systems support timestamped data (event or ingestion time semantics). Most systems
deliver events in order, under the assumptions that sources either produce data with a predefined
maximum delay or inform the system about the progress of time using special metadata denoted as
watermark. Kafka Streams takes a different approach: it does not wait for out of order elements
and immediately produces results. In the case new elements arrive out of order, it retracts updates
the previous results.

Fault tolerance. All DPSs detect faults using a master-worker architecture and, in absence of
a shared state, they recover from failures through the mechanisms that guarantee exactly once
delivery.

Dynamic reconfiguration. DPSs use dynamic reconfiguration to adapt to the workload by adding
or removing slots. Systems that adopt task-level deployment can decide how to allocate resources to
individual tasks when they are activated, while systems that adopt job-level deployment need to sus-
pend and resume the entire job, which increases the overhead for performing a reconfiguration. The
mechanisms that dynamically modify the resources (slots) available to a DPS can be activated either
manually or by an automated service that monitors the utilization of resources and implements the
allocation and deallocation policies. All commercial systems implement automated reconfiguration,
frequently by relying on external platforms for containerization, such as Kubernetes, or for cluster
resources management, such as YARN. The only exceptions for which we could not find official
support for automated reconfiguration are Storm [92] and Kafka Streams [20].

5.2 Dataflow with task-level deployment
Systems that belong to this class deploy tasks on activation, when their input data becomes available.
Tasks store intermediate results on a persistent data bus, which enables to selectively restart them
individually in the case of failure. This approach is best suited for long running batch jobs. In fact,
it was pioneered in the seminal batch processing MapReduce system [41] and has been widely
adopted in various extensions and generalizations. HaLoop optimized iterative computations by
caching loop-invariant data and by co-locating tasks that reuse the same data across iterations [24].
Dryad [51] generalizes the programming model to express arbitrary dataflow plans and enables
developers to flexibly select the concrete channels (data bus in our model) that implement the
communication between tasks. CIEL [71] extends the dataflow programming model of Dryad by
allowing tasks to create other tasks, which enables jobs whose execution plan is dynamically
defined based on the results of computation. Spark is the most popular system of this class [99]:
it inherits the dataflow model of Dryad and supports iterative execution and data caching like
HaLoop. Spark Streaming [98] implements streaming computations on top of Spark by splitting
the input stream into small batches and by running the same jobs for each batch. Spark Streaming
implements task state using native Spark features: the state of a task after a given invocation is
implicitly stored as a special data item that the task receives as input in the subsequent invocation.
In systems with task-level deployment, job compilation considers dynamic information to create
tasks: for instance, the number of tasks instantiated to perform a data-parallel operation depends
on how the input data is partitioned. Similarly, the deployment phase uses dynamic information

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:29

to submit tasks to workers running as close as possible to the their input data. Hadoop (the open
source implementation of MapReduce) and Spark adopt a delay scheduling [97]. They put jobs (and
their tasks) in a FIFO queue. When slots become available, the first task in the queue is selected:
if the slot is located near to the input data for the task, then the task is immediately deployed,
otherwise each task can be postponed for some time to wait for available slots closer to their input
data. Task-level deployment enables sharing of compute resources with other applications: in fact,
most of the systems that use this approach can be integrated with cluster management systems.

Task-level deployment also influences how systems implement fault tolerance and ensure exactly
once delivery of results. Batch processing systems can simply re-execute the tasks involved in the
failure. In absence of state, the results of a task depend only on input data and can be recomputed
at need. Intermediate results may be persisted on durable storage and retrieved in the case of a
failure or recomputed from the original input data. Spark Streaming [98] adopts the same fault
tolerance mechanism for streaming computations. It segments a continuous stream into a sequence
of so-called micro-batches and executes them in order. Task state is treated as a special form of data,
it is periodically persisted to durable storage and can be retrieved in the case of a failure. Failure
recovery may require activating failed tasks more than once, to recompute the task state from the
last state persisted before the failure.

Dynamic reconfiguration is available in all systems that adopt task-level deployment. Systems
that do not provide any state abstraction can simply exploit new slots to schedule tasks when they
become available, and remove workers when idle. In the presence of task state, migrating a task
involves migrating its state across activations: as in the case of fault tolerance, this is done by
storing task state on some persistent storage.

5.3 Dataflow with job-level deployment
In the case of job-level deployment, all tasks of a job are deployed onto the slots of the computing
infrastructure on job registration. As a result, this class of system is better suited for streaming
computations that require low latency: indeed, no scheduling decisions are taken at runtime and
tasks are always ready to receive and process new data. Storm [92] and its successor Heron [55]
are stream processing systems developed at Twitter. They offer lower-level programming API than
previously discussed dataflow systems, asking developers to fully implement the logic of each
processing step using a standard programming language. Flink [27] is a unified execution engine
for batch and stream processing. In terms of programming model, it strongly resembles Spark, with
a core API to explicitly define job plans and domain specific libraries for structural (relational) data,
graph processing, and machine learning. One notable difference involves iterative computations:
Flink supports them with native operators (within jobs) rather than controlling them from the
driver program. Timely dataflow [70] offers a lower-level and more general dataflow model than
Flink, where jobs are expressed as a graph of (data parallel) operators and data elements carry a
logical timestamp that tracks global progress. Management of timestamps is explicit, and developers
control how operators handle and propagate them, which enables various execution strategies: for
instance, developers may choose to complete a given computation step before letting the subsequent
one start (mimicking a batch processing strategy as implemented in MapReduce or Spark), or they
may allow overlapping of steps (as it happens in Storm or Apache Flink). The flexibility of the
model allows for complex workflows, including streaming computations with nested iterations,
which are hard or even impossible to express in other systems. The above systems rely on direct
and ephemeral channels (typically, TCP connections) to implement the data bus. Kafka Streams [20]
and Samza [73], instead, build a dataflow processing layer on top of Kafka [54] durable channels. In
systems that adopt job-level deployment, job compilation and deployment only depends on static
information about the computing infrastructure: for instance, the number of tasks for data-parallel

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:30

Margara et al.

operations only depends on the total number of slots made available in workers. As a result, this
class of systems does not support sharing resources with other applications: all resources need to
be acquired at job compilation, which prevents scheduling decisions across applications at runtime.
We observed three approaches to implement fault tolerance and delivery guarantees: (1) Systems
such as Flink and MillWheel [8] periodically take a consistent snapshot of the state. The command
to initiate a snapshot starts from sources and completes when it reaches the sinks. In the case
of failure, the last completed snapshot is restored and sources replay data that was produced
after that snapshot, in the original order. If sinks can detect and discard duplicate results, this
approach guarantees exactly once delivery; (2) Storm acknowledges each data element delivered
between two tasks: developers decide whether to use acknowledgements (and retransmission if an
acknowledgement is lost), providing at least once delivery, or not, providing at most once delivery;
(3) Kafka Streams relies on the persistency of the data bus (Kafka): it stores the task state in special
Kafka topics and relies on two-phase commit to ensure that upon activation a task consumes
its input, updates its state, and produces results for downstream tasks atomically. In the case of
failure, a task can resume from the input elements that were not successfully processed, providing
exactly once delivery (unless data elements are received out-of-order, in which case it retracts and
updates previous results leading to at least once delivery). These three mechanisms are also used
for dynamic reconfiguration, as they allow a system to resume processing after a new deployment.

5.4 Graph processing
Early dataflow systems were not well suited for iterative computations, which are common in graph
processing algorithms. To overcome this limitation, a computational model alternative to dataflow
was developed for graph processing, known as vertex-centric [66]. In this model, pioneered by the
Google Pregel system [64], jobs are iterative: developers provide a single function that encodes
the behavior of each vertex 𝑣 at each iteration. The function takes in input the current (local)
state of 𝑣 and the set of messages produced for 𝑣 during the previous iteration; it outputs the new
state of 𝑣 and a set of messages to be delivered to connected vertices, which will be evaluated
during the next iteration. The job terminates when vertices do not produce any message at a
given iteration. Vertices are partitioned across workers and each task is responsible for a given
partition. Jobs are continuous, as tasks are activated multiple times (once for each iteration) and
store the vertex state across activations (in their task state). Tasks only communicate by exchanging
data (messages between vertices) over the data bus, which is implemented as direct channels.
One worker acts as a master and is responsible for coordinating the iterations within the job and
for detecting possible failures of other workers. Workers persist their state (task state and input
messages) at each iteration: in the case of a failure, the computation restarts from the last completed
iteration. Several systems inherit and improve the original Pregel model in various ways: (i) By
using a persistent data bus, where vertices can pull data when executed, to reduce the overhead for
broadcasting state updates to many neighbor vertices [61]; (ii) By decoupling communication and
processing in each superstep, to combine messages and reduce the communication costs [46]; (iii) By
allowing asynchronous execution of supersteps, to reduce synchronization overhead and inactive
time [61]; (iv) By optimizing the allocation of vertices to tasks based on topological information, to
reduce the communication overhead; (v) By dynamically migrating vertices between tasks (dynamic
reconfiguration) across iterations, to keep the load balanced or to place frequently communicating
vertices on the same worker [33]; (vi) By offering sub-graph centric abstractions, suitable to express
graph mining problems that aim to find sub-graphs with given characteristics [90]. For space sake,
we do not discuss all systems that derived from Pregel here, but the interested reader can find a
detailed taxonomy of these systems in the survey by McCune et al [66].

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:31

6 OTHER SYSTEMS
This section includes all systems that do not clearly fall in either of the two classes identified above.
We organize them in three main classes: (i) Systems that support analytical jobs on top of shared
state abstractions; (ii) Systems that propose new programming models; (iii) Systems that integrate
concepts from both DMSs and DPSs in an attempt to provide a unifying solution.

6.1 Computations on data management systems
DMSs are designed to execute lightweight jobs that read and modify a shared state. We identified a
few systems that support some form of heavy-weight job.

Incremental computations. Percolator [75] is a system built at Google on top of the BigTable
6.1.1
column store to incrementally update the shared state. Percolator extends BigTable with observers
and multi-row transactions. Observers are processes that periodically scan BigTable tables: when
they detect changes, they start a computation that may update other tables with its results. This
approach is designed for expensive computations that can be broken down into small updates to
the current shared state, differently from batch computations in DPSs, which are not designed
to be incremental. For instance, Percolator can incrementally update Web search indexes as new
information about Web pages and links become available. Percolator ensures group atomicity
through two-phase commit and group isolation (snapshot isolation) using timestamps.

Long-running jobs. F1 [82] implements a SQL query executor on top of Spanner that supports
6.1.2
long-running jobs. SQL queries are analyzed and converted into a plan that may include sub-plans
executed in a dataflow fashion to support distributed computation of heavy-weight jobs. F1 also
introduces optimistic transactions (jobs), which consist of a read phase to retrieve all the data
needed for the computation and a write phase to store the results. The read phase does not block
other concurrent jobs, so they can run for long time (as in the case of analytical jobs). The write
phase completes only if no conflicting updates from other jobs occurred during the read phase.

6.1.3 Graph processing. In graph data stores, long-running jobs appear in the form of computations
that traverse multiple hops of the graph (such as queries that search for paths or for patterns in the
graph) or iterative analytical jobs (such as vertex-centric computations). Trinity [79] is a graph
data store developed at Microsoft. It inherits the same model of NoSQL graph stores such as TAO,
but implements features designed specifically to support long-running jobs: it lets users define the
communication protocols that govern the exchange of data during over the data but, to optimize
them for the specific computation, it supports checkpointing the intermediate state of a job to
resume it in the case of a failure.

6.2 New programming models

Stateful dataflow. The absence of shared mutable state in the dataflow model forces de-
6.2.1
velopers to encode all information as data that flows between tasks. However, some algorithms
would benefit from the availability of state that can be modified in-place. A notable case is that
of machine learning algorithms that iteratively refine a set of parameters. Thus, several systems
propose extensions to the dataflow programming model that accommodate shared mutable state.
In stateful dataflow graphs (SDG) [45], developers write a driver program using imperative (Java)
code that includes mutable state and methods to access and modify it. Code annotations are used to
specify state access patterns within methods. The resulting jobs are compiled into a dataflow graph
where operators access the shared state. If possible, state elements are partitioned across workers,
otherwise they are replicated in each worker and the programming model supports user-defined

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:32

Margara et al.

functions to merge changes applied to different replicas. SDG adopts a job-level deployment and
execution model [29], similar to that of data processing systems such as Apache Flink.

Tangram [50] implements task-based deployment and allows tasks to access and update an
in-memory key-value store as part of their execution. By analyzing the execution plan, Tangram
can understand which parts of the computation depend on mutable state and which parts do not,
and optimizes fault tolerance for the job at hand.

TensorFlow [1] is a library to define machine learning models. Jobs represent models with
transformations (tasks) and variables (shared state elements). As strong consistency is not required
for the application scenario, tasks can execute and update variables asynchronously, with only
barrier synchronization at each step of an iterative algorithm.

6.2.2 Relational actors. ReactDB [78] extends the actor-based programming model [6] with data
management concepts such as relational tables, declarative queries, and transactional semantics. In
a nutshell, ReactDB builds on logical actors that embed state in the form of relational tables. They
can query their internal state using a declarative language and asynchronously send messages to
other actors. The core idea of ReactDB is to let developers explicitly control how the shared state is
partitioned across actors. Jobs are submitted to a coordinator actor that may access its share state
portion and/or explicitly communicate to other actors, over the data bus, to access remote state.
The system guarantees transactional semantics for the entire duration of the job, both within the
coordinator and within other actors that are directly or indirectly invoked by the coordinator.

6.3 Hybrid systems
Several works aim to integrate data management and processing within a unified solution. S-
Store [30] integrates stream processing capabilities within a transactional database. It uses an
in-memory store to implement the shared state (visible to all tasks), the task state (visible only to
individual tasks of stream processing jobs), and the data bus (where data flowing from task to task of
stream processing jobs is temporarily stored). S-Store uses the same concepts as VoltDB [88] to offer
transactional guarantees with low overhead. Tasks of data management jobs and stream processing
tasks are scheduled on the same engine in an order that preserves transactional semantics and is
consistent with the dataflow. It is interesting to note how S-Store unifies input data (for streaming
jobs) and invocations (of data management jobs, in the form of stored procedures): this is in line
with the conceptual view we provide in Sec. 2 to define unifying model for data intensive systems.
SnappyData [69] has a similar goal to S-Store but a different programming and execution model. It
builds on Spark and Spark Streaming, and augments them with the possibility to write to a mutable
state (a key-value store) during their execution. In the attempt to efficiently support heterogeneous
types of jobs, SnappyData lets developers select how to organize the shared state (for instance, in
terms of format, which can be row-oriented or column-oriented, partitioning, and replication). It
supports group atomicity and isolation using two-phase commit and multi-version concurrency
control, and integrates fault detection and recovery mechanisms for Spark tasks and their effects
on the shared state.

StreamDB [34] and TSpoon [5] take the opposite approach with respect to S-Store by integrating
data management capabilities within a stream processor. StreamDB models database queries as
stream processing jobs that receive updates from external sources and output new results to sinks.
Stream processing tasks can read and modify portions of a shared state: all database queries that
need to access a given portion will include the task responsible for that portion. StreamDB ensures
group atomicity and isolation without explicit locks: invocation of jobs are timestamped when
first received by the system and each worker executes tasks from different jobs in timestamp order.
TSpoon does not provide a shared state, but enriches the dataflow programming and execution

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:33

model with: (i) The possibility to read (query) task state on demand; (ii) Transactional guarantees
in the access to task state. Developers can identify portions of the dataflow graph (denoted as
transactional sub-graphs) that need to be read and modified in a consistent way, meaning that
each change should be reflected in all tasks states or none (atomicity) and the effects of changes
should not overlap in unexpected ways (isolation). TSpoon implements atomicity and isolation
by decorating the dataflow graph with additional operators that act as transaction managers. It
supports different levels of isolation (from read committed to serializable) with different tradeoffs
between guarantees and runtime overhead.

Hologres [52] is used within Alibaba to execute both analytical jobs and interactive jobs. The
system is designed to support high volume ingestion data from external sources and continuous
analysis to compute derived information to be stored in the shared state or to present to external
sinks. The shared state that is partitioned across workers. A worker stores an in-memory represen-
tation of the partition it is responsible for and delegates durability to an external storage service.
The distinctive features of the system are: (i) a structured data model where state is represented
as tables that can be physically stored row-wise or column-wise depending on the access pattern;
(ii) a scheduling mechanism where job tasks are deployed and executed onto workers based on
load-balancing and prioritization of jobs that require low latency.

7 DISCUSSION
The classification criteria that derive from the model we described in Sec. 2 enabled us to organize
data-intensive systems into a taxonomy (see Sec. 3) and to point out the most relevant features
shared by systems within each class in the taxonomy. This section focuses on design principles and
implementation strategies that cross the boundaries of our taxonomy and on works that are related
but outside the scope of this paper.

State and data management in data-intensive architectures. DMSs and DPSs are orthogonal with
respect to state and data management. DMSs target lightweight jobs that read and modify a mutable
shared state, while DPSs target computationally expensive jobs that transform input data into
output data, and do not consider state at all or consider it only at the granularity of individual tasks.
Due to their complementary focus, the two classes of systems are frequently used in conjunction.
A classic architectural pattern distinguishes between two types of jobs: read-write jobs that mutate
the state of the application (e.g., user requests in an e-commerce portal) and read-only analytic jobs
(e.g., analysis of sales segmented by time, product, and region). Jobs of the first type are denoted
as OLTP (on line transaction processing) and are handled by DMSs that support both read and
write queries (e.g., relational databases), while the latter are denoted as OLAP (on line analytical
processing) and are handled by DMSs optimized for read queries (e.g., wide-column stores). The
process of extracting data from OLTP systems and loading it into OLAP systems is denoted ETL
(extract, transform, load). ETL is handled by DPSs that analyze input data to pre-compute and
materialize views to speedup read queries in OLAP systems (e.g., by executing expensive grouping,
joins, aggregates, as well as building secondary indexes).

Traditionally, ETL was executed periodically by batch DPSs, with the downside that analytical
jobs do not always access the latest available data. Recent architectural patterns (e.g., Lambda and
Kappa architectures [59]) advocate the use of stream DPSs for ETL. The survey by Davoudian and
Liu [40] offers a good overview on the architectures for data-intensive applications. In this role,
stream DPSs such as Flink and Kafka started to offer primitives to access their task state with
read-only queries (one-shot jobs), thus avoiding the need of external OLAP systems to store the
results of their transformations. In practice, as their task state is frequently partitioned by key, they
offer the same abstraction of key-value DMSs. This triggered interesting research on declarative

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:34

Margara et al.

APIs that integrate streaming data and state changes into a unifying abstraction [77]. Our model
took inspiration from these works to define the duality of data and invocations, observing that new
data can be seen ad the implicit invocation of jobs that alter the state of the system.

In summary, DPSs are frequently used as an asynchronous (periodic or continuous) channel
between OLTP and OLAP DMSs, and in some cases they substitute the latter by offering one-shot
read-only jobs. Few systems such as S-Store [30] and TSpoon [5] further explore the possibility to
integrate OLTP workloads (with transactional semantics) within stream DPSs.

Coordination avoidance. In distributed scenarios, the coordination between workers may easily
become a bottleneck. Avoiding or reducing coordination is a recurring principle in the design of all
data-intensive systems. Most DPSs circumvent this problem by forcing developers to think in terms
of functional and data-parallel transformations. As state is absent or local to tasks, tasks may freely
proceed in parallel. Coordination, if present, is limited to barrier synchronization in systems that
support iterative jobs (e.g., iterative dataflow systems and graph processing systems). Conversely,
DMSs require coordination to control concurrent access to shared state from multiple jobs. Indeed,
the approach to coordination is the main criterion we used to classify them in Sec. 4. NoSQL systems
partition state by key: they either only support jobs that operate on individual keys or relinquish
group guarantees for jobs that span multiple keys, effectively treating accesses to different keys as
if they came from independent jobs that do not coordinate with each other. NewSQL systems do
not entirely avoid coordination, but try to limit the situations in which it is required or its cost. In
our analysis we identified four main approaches to reach this goal: (1) use of precise clocks [37],
(2) pre-ordering of jobs and deterministic execution [91], (3) explicit partitioning strategies to
maximize jobs executed (sequentially) in a single slot [88], (4) primary-based protocols that delegate
the scheduling of all read-write jobs to a single worker [94]. In addition, all DMSs adopt strategies
that optimize the execution of read-only jobs and minimize their impact on read-write jobs. They
include the use of replicas to serve read-only jobs and multi-version concurrency control to let
read-only jobs access a consistent view of the state without conflicting with read-write jobs.

A deep understanding of the performance implications of different coordination avoidance
strategies under various workloads is an interesting research problem, which may open the room
for dynamic adaptation strategies.

Wide area deployment. All the systems we analyzed are primarily designed for cluster deployment.
In DPSs, tasks exchange large volumes of data over the data bus and bandwidth may easily
become a bottleneck in wide area deployments. Some DMSs support wide-area deployment through
replication. They either drop consistency guarantees or implement mechanisms that reduce the
cost for updating remote replicas: for instance, deterministic databases [91] define an order for
jobs and force all replicas to follow the same order without further synchronization during job
execution. As increasingly many applications work at a geographical scale, the edge computing
paradigm [81] is emerging, which aims to exploit (processing and storage) resources at the edge of
the network, close to the end users. Designing data-intensive systems that embrace this paradigm
and simplify the use of edge resources is an important topic of investigation.

Modular architectures. Several DMSs adopt a modular approach where the components that imple-
ment the functionalities of the system are developed and deployed independently. This is well suited
for cloud environments where individual components are offered as services and can be scaled
independently and depending on the workload. Also, the same service can be used in multiple prod-
ucts: e.g., storage services, log services, lock services, key-value stores offered to users or adopted
as building block for relational databases. This is the case of systems developed at Google [32, 37],
Microsoft [12], and Amazon [94]. Future research could build on these results to derive more general
architectural models. Identifying the abstract components that build data-intensive systems, the

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:35

interfaces they should offer, the assumptions they rely on, and the functionalities they provide
would promote reusability and adaptation to specific application scenarios. Concrete architectures
could be derived from high-level description of the application requirements, as proposed in some
recent work on model-driven development for stream processing applications [48]. Our work can
serve as a conceptual model to guide future research in this direction.

Specialized hardware. The use of specialized hardware in workers is outside the scope of our model,
but it is an active area of research. Recent work study hardware acceleration for DPSs [49] and
DMSs [44, 57] using GPUs or FPGAs. Offloading of tasks to GPUs is also supported in recent
versions DPSs such as Spark and is a key feature for systems that target machine learning problems,
such as TensorFlow [1]. Non-volatile memory has also the potential to drive new solutions for
DMSs, as it offers durability at nearly the same performance as main-memory. The interested reader
can refer to the work by Arulraj and Pavlo [14] that discusses the use of non-volatile memory to
implement a database system.

Dynamic adaptation. Our model captures the ability of some systems to adapt to mutating workload
conditions, i.e., to dynamically scale. Many works use this feature to implement automated control
systems for DPSs that monitor the use of resources and adapt the deployment to meet the quality
of service specified by the users, while using the minimum amount of resources. The interested
reader can refer to recent work on dynamic adaptation for batch [19] and stream [28] DPSs.

8 CONCLUSION
This paper presented a unifying model for distributed data-intensive systems, which defines a
system in terms of abstract components that cooperate to offer the system functionalities. The
model precisely captures the possible design and implementation strategies for each component,
with the assumptions they rely on and the guarantees they provide. From the model, we derive a
list of classification criteria that we use to organize the presentation of state-of-the-art systems,
highlighting their commonalities and distinctive features. Our work can be useful for software
engineers that need to deeply understand the range of possibilities to select the best systems for
their application scenario, but also to researchers and practitioners that work on data-intensive
systems, to acquire a wide yet precise view of the field.

REFERENCES

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.
Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2016. TensorFlow: A System for Large-Scale Machine Learning. In Proc of the Conf on Operating Systems Design and
Impl (OSDI’16). USENIX, 265–283.

[2] Azza Abouzeid, Kamil Bajda-Pawlikowski, Daniel Abadi, Avi Silberschatz, and Alexander Rasin. 2009. HadoopDB:
An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads. Proc VLDB 2, 1 (2009),
922–933.

[3] Colin Adams, Luis Alonso, Benjamin Atkin, John Banning, Sumeer Bhola, Rick Buskens, Ming Chen, Xi Chen, Yoo
Chung, Qin Jia, Nick Sakharov, George Talbot, Adam Tart, and Nick Taylor. 2020. Monarch: Google’s Planet-Scale
in-Memory Time Series Database. Proc of VLDB 13, 12 (2020), 3181–3194.

[4] A. Adya, B. Liskov, and P. O’Neil. 2000. Generalized isolation level definitions. In Proc of the Intl Conf on Data

Engineering (ICDE ’00). IEEE, 67–78.

[5] Lorenzo Affetti, Alessandro Margara, and Gianpaolo Cugola. 2020. TSpoon: Transactions on a stream processor. J.

Parallel and Distrib. Comput. 140 (2020), 65–79.

[6] Gul A. Agha. 1990. ACTORS - a model of concurrent computation in distributed systems. MIT Press.
[7] Phillipe Ajoux, Nathan Bronson, Sanjeev Kumar, Wyatt Lloyd, and Kaushik Veeraraghavan. 2015. Challenges to
Adopting Stronger Consistency at Scale. In Proc of the Conf on Hot Topics in Operating Systems (HOTOS’15). USENIX,
13:1–13:7.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:36

Margara et al.

[8] Tyler Akidau, Alex Balikov, Kaya Bekiroğlu, Slava Chernyak, Josh Haberman, Reuven Lax, Sam McVeety, Daniel
Mills, Paul Nordstrom, and Sam Whittle. 2013. MillWheel: Fault-Tolerant Stream Processing at Internet Scale. Proc of
VLDB 6, 11 (2013), 1033–1044.

[9] Tyler Akidau, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax,
Sam McVeety, Daniel Mills, Frances Perry, Eric Schmidt, and Sam Whittle. 2015. The Dataflow Model: A Practical
Approach to Balancing Correctness, Latency, and Cost in Massive-scale, Unbounded, Out-of-order Data Processing.
Proc of VLDB 8, 12 (2015), 1792–1803.

[10] Sattam Alsubaiee, Yasser Altowim, Hotham Altwaijry, Alexander Behm, Vinayak Borkar, Yingyi Bu, Michael Carey,
Inci Cetindil, Madhusudan Cheelangi, Khurram Faraaz, Eugenia Gabrielova, Raman Grover, Zachary Heilbron, Young-
Seok Kim, Chen Li, Guangqiang Li, Ji Mahn Ok, Nicola Onose, Pouria Pirzadeh, Vassilis Tsotras, Rares Vernica, Jian
Wen, and Till Westmann. 2014. AsterixDB: A Scalable, Open Source BDMS. Proc of VLDB 7, 14 (2014), 1905–1916.

[11] J Chris Anderson, Jan Lehnardt, and Noah Slater. 2010. CouchDB: the definitive guide: time to relax. O’Reilly.
[12] Panagiotis Antonopoulos, Alex Budovski, Cristian Diaconu, Alejandro Hernandez Saenz, Jack Hu, Hanuma Ko-
davalla, Donald Kossmann, Sandeep Lingam, Umar Farooq Minhas, Naveen Prakash, Vijendra Purohit, Hugh Qu,
Chaitanya Sreenivas Ravella, Krystyna Reisteter, Sheetal Shrotri, Dixin Tang, and Vikram Wakade. 2019. Socrates:
The New SQL Server in the Cloud. In Proc of the Intl Conf on Management of Data (SIGMOD ’19). ACM, 1743–1756.
[13] Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer
Kaftan, Michael J. Franklin, Ali Ghodsi, and Matei Zaharia. 2015. Spark SQL: Relational Data Processing in Spark. In
Proc of the Intl Conf on Management of Data (SIGMOD ’15). ACM, 1383–1394.

[14] Joy Arulraj and Andrew Pavlo. 2017. How to Build a Non-Volatile Memory Database Management System. In Proc of

the Intl Conf on Management of Data (SIGMOD ’17). ACM, 1753–1758.

[15] David F. Bacon, Nathan Bales, Nico Bruno, Brian F. Cooper, Adam Dickinson, Andrew Fikes, Campbell Fraser, Andrey
Gubarev, Milind Joshi, Eugene Kogan, Alexander Lloyd, Sergey Melnik, Rajesh Rao, David Shue, Christopher Taylor,
Marcel van der Holst, and Dale Woodford. 2017. Spanner: Becoming a SQL System. In Proc of the Intl Conf on
Management of Data (SIGMOD ’17). ACM, 331–343.

[16] Peter Bailis, Aaron Davidson, Alan Fekete, Ali Ghodsi, Joseph M. Hellerstein, and Ion Stoica. 2013. Highly Available

Transactions: Virtues and Limitations. Proc of VLDB 7, 3 (2013), 181–192.

[17] Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, Ming Wu, Vijayan Prabhakaran, Michael Wei, John D. Davis,
Sriram Rao, Tao Zou, and Aviad Zuck. 2013. Tango: Distributed Data Structures over a Shared Log. In Proc of the
Symposium on Operating Systems Principles (SOSP ’13). ACM, 325–340.

[18] François Banciihon. 1988. Object-Oriented Database Systems. In Proc of the Symposium on Principles of Database

Systems (PODS ’88). ACM, 152–162.

[19] Luciano Baresi, Alberto Leva, and Giovanni Quattrocchi. 2021. Fine-Grained Dynamic Resource Allocation for

Big-Data Applications. IEEE Transactions on Software Engineering 47, 8 (2021), 1668–1682.

[20] Bill Bejeck. 2018. Kafka Streams in Action: Real-time apps and microservices with the Kafka Streams API. Manning.
[21] Philip A. Bernstein and Nathan Goodman. 1981. Concurrency Control in Distributed Database Systems. ACM Comput.

Surv. 13, 2 (1981), 185–221.

[22] Vinayak Borkar, Michael Carey, Raman Grover, Nicola Onose, and Rares Vernica. 2011. Hyracks: A Flexible and
Extensible Foundation for Data-Intensive Computing. In Proc of the Intl Conf on Data Engineering (ICDE ’11). IEEE,
1151–1162.

[23] Nathan Bronson, Zach Amsden, George Cabrera, Prasad Chakka, Peter Dimov, Hui Ding, Jack Ferris, Anthony
Giardullo, Sachin Kulkarni, Harry Li, Mark Marchukov, Dmitri Petrov, Lovro Puzar, Yee Jiun Song, and Venkat
Venkataramani. 2013. TAO: Facebook’s Distributed Data Store for the Social Graph. In Proc of the USENIX Annual
Technical Conf (ATC ’13). USENIX Assoc., 49–60.

[24] Yingyi Bu, Bill Howe, Magdalena Balazinska, and Michael D. Ernst. 2010. HaLoop: Efficient Iterative Data Processing

on Large Clusters. Proceeding of VLDB 3, 1–2 (2010), 285–296.

[25] Chiranjeeb Buragohain, Knut Magne Risvik, Paul Brett, Miguel Castro, Wonhee Cho, Joshua Cowhig, Nikolas Gloy,
Karthik Kalyanaraman, Richendra Khanna, John Pao, Matthew Renzelmann, Alex Shamis, Timothy Tan, and Shuheng
Zheng. 2020. A1: A Distributed In-Memory Graph Database. In Proc of the Intl Conf on Management of Data (SIGMOD
’20). ACM, 329–344.

[26] Jesús Camacho-Rodríguez, Ashutosh Chauhan, Alan Gates, Eugene Koifman, Owen O’Malley, Vineet Garg, Zoltan
Haindrich, Sergey Shelukhin, Prasanth Jayachandran, Siddharth Seth, Deepak Jaiswal, Slim Bouguerra, Nishant Ban-
garwa, Sankar Hariappan, Anishek Agarwal, Jason Dere, Daniel Dai, Thejas Nair, Nita Dembla, Gopal Vijayaraghavan,
and Günther Hagleitner. 2019. Apache Hive: From MapReduce to Enterprise-Grade Big Data Warehousing. In Proc of
the Intl Conf on Management of Data (SIGMOD ’19). ACM, 1773–1786.

[27] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi, and Kostas Tzoumas. 2015. Apache
Flink™: Stream and Batch Processing in a Single Engine. IEEE Data Engineering Bulletin 38, 4 (2015), 28–38.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:37

[28] Valeria Cardellini, Francesco Lo Presti, Matteo Nardelli, and Gabriele Russo Russo. 2022. Run-Time Adaptation of

Data Stream Processing Systems: The State of the Art. Comput. Surveys (2022).

[29] Raul Castro Fernandez, Matteo Migliavacca, Evangelia Kalyvianaki, and Peter Pietzuch. 2013. Integrating Scale out
and Fault Tolerance in Stream Processing Using Operator State Management. In Proc of the Intl Conf on Management
of Data (SIGMOD ’13). ACM, 725–736.

[30] Ugur Cetintemel, Jiang Du, Tim Kraska, Samuel Madden, David Maier, John Meehan, Andrew Pavlo, Michael
Stonebraker, Erik Sutherland, Nesime Tatbul, et al. 2014. S-Store: a streaming NewSQL system for big velocity
applications. Proc of VLDB 7, 13 (2014), 1633–1636.

[31] K. Mani Chandy and Leslie Lamport. 1985. Distributed Snapshots: Determining Global States of Distributed Systems.

Trans on Computer Systems 3, 1 (1985), 63–75.

[32] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Michael Burrows, Tushar Chandra,
Andrew Fikes, and Robert Gruber. 2006. Bigtable: A Distributed Storage System for Structured Data. In Proc of the
Symposium on Operating Systems Design and Implementation (OSDI ’06), Brian N. Bershad and Jeffrey C. Mogul (Eds.).
USENIX Assoc., 205–218.

[33] Hongzhi Chen, Miao Liu, Yunjian Zhao, Xiao Yan, Da Yan, and James Cheng. 2018. G-Miner: An Efficient Task-Oriented

Graph Mining System. In Proc of the EuroSys Conf (EuroSys ’18). ACM, Article 32.

[34] Huankai Chen and Matteo Migliavacca. 2018. StreamDB: A Unified Data Management System for Service-Based

Cloud Application. In Proc of the Intl Conf on Services Computing (SCC ’18). IEEE, 169–176.

[35] Kristina Chodorow. 2013. MongoDB: the definitive guide: powerful and scalable data storage. O’Reilly.
[36] Brian F. Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, Adam Silberstein, Philip Bohannon, Hans-Arno Jacobsen,
Nick Puz, Daniel Weaver, and Ramana Yerneni. 2008. PNUTS: Yahoo!’s Hosted Data Serving Platform. Proc of VLDB
1, 2 (2008), 1277–1288.

[37] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, J. J. Furman, Sanjay Ghemawat,
Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi
Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi
Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, and Dale Woodford. 2013. Spanner: Google’s Globally
Distributed Database. Trans on Computer Systems 31, 3 (2013), 8:1–8:22.

[38] Michael Curtiss, Iain Becker, Tudor Bosman, Sergey Doroshenko, Lucian Grijincu, Tom Jackson, Sandhya Kunnatur,
Soren Lassen, Philip Pronin, Sriram Sankar, Guanghao Shen, Gintaras Woss, Chao Yang, and Ning Zhang. 2013.
Unicorn: A System for Searching the Social Graph. Proc of VLDB 6, 11 (2013), 1150–1161.

[39] Ali Davoudian, Liu Chen, and Mengchi Liu. 2018. A Survey on NoSQL Stores. ACM Comput. Surv. 51, 2, Article 40

(2018).

[40] Ali Davoudian and Mengchi Liu. 2020. Big Data Systems: A Software Engineering Perspective. ACM Comput. Surv.

53, 5, Article 110 (2020), 39 pages.

[41] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: Simplified Data Processing on Large Clusters. Commun. ACM

51, 1 (2008), 107–113.

[42] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin,
Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels. 2007. Dynamo: Amazon’s Highly Available
Key-value Store. In Proc of the Symposium on Operating Systems Principles (SOSP ’07). ACM, 205–220.

[43] Aleksandar Dragojević, Dushyanth Narayanan, Miguel Castro, and Orion Hodson. 2014. FaRM: Fast Remote Memory.
In USENIX Symposium on Networked Systems Design and Implementation (NSDI ’14). USENIX Assoc., 401–414.
[44] Jian Fang, Yvo TB Mulder, Jan Hidders, Jinho Lee, and H Peter Hofstee. 2020. In-memory database acceleration on

FPGAs: a survey. The VLDB Journal 29, 1 (2020), 33–59.

[45] Raul Castro Fernandez, Matteo Migliavacca, Evangelia Kalyvianaki, and Peter Pietzuch. 2014. Making State Explicit
for Imperative Big Data Processing. In Proc of the USENIX Annual Technical Conf (ATC’14). USENIX Assoc., 49–60.
[46] Joseph E. Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, and Carlos Guestrin. 2012. PowerGraph: Distributed
Graph-Parallel Computation on Natural Graphs. In Proc of the Conf on Operating Systems Design and Implementation
(OSDI’12). USENIX, 17–30.

[47] Joseph E. Gonzalez, Reynold S. Xin, Ankur Dave, Daniel Crankshaw, Michael J. Franklin, and Ion Stoica. 2014.
GraphX: Graph Processing in a Distributed Dataflow Framework. In Symposium on Operating Systems Design and
Implementation (OSDI ’14). USENIX Assoc., 599–613.

[48] Michele Guerriero, Damian Andrew Tamburri, and Elisabetta Di Nitto. 2021. StreamGen: Model-Driven Development
of Distributed Streaming Applications. ACM Transactions on Software Engineering and Methodology 30, 1, Article 1
(2021), 30 pages.

[49] Joost Hoozemans, Johan Peltenburg, Fabian Nonnemacher, Akos Hadnagy, Zaid Al-Ars, and H. Peter Hofstee. 2021.
FPGA Acceleration for Big Data Analytics: Challenges and Opportunities. Circuits and Systems Magazine 21, 2 (2021),
30–47.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:38

Margara et al.

[50] Yuzhen Huang, Xiao Yan, Guanxian Jiang, Tatiana Jin, James Cheng, An Xu, Zhanhan Liu, and Shuo Tu. 2019. Tangram:
Bridging Immutable and Mutable Abstractions for Distributed Data Analytics. In Proc of the USENIX Annual Technical
Conf (ATC ’19). USENIX, 191–205.

[51] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. 2007. Dryad: Distributed Data-Parallel
Programs from Sequential Building Blocks. In Proc of the European Conf on Computer Systems (EuroSys ’07). ACM,
59–72.

[52] Xiaowei Jiang, Yuejun Hu, Yu Xiang, Guangran Jiang, Xiaojun Jin, Chen Xia, Weihua Jiang, Jun Yu, Haitao Wang, Yuan
Jiang, Jihong Ma, Li Su, and Kai Zeng. 2020. Alibaba Hologres: A Cloud-Native Service for Hybrid Serving/Analytical
Processing. Proc of VLDB 13, 12 (2020), 3272–3284.

[53] Martin Kleppmann. 2016. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintain-

able Systems. O’Reilly.

[54] Jay Kreps, Neha Narkhede, Jun Rao, et al. 2011. Kafka: A distributed messaging system for log processing. In Proc of

the Intl Workshop on Networking meets Databases (NetDB). USENIX, 1–7.

[55] Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli, Christopher Kellogg, Sailesh Mittal, Jignesh M.
Patel, Karthik Ramasamy, and Siddarth Taneja. 2015. Twitter Heron: Stream Processing at Scale. In Proc of the Intl
Conf on Management of Data (SIGMOD ’15). ACM, 239–250.

[56] Avinash Lakshman and Prashant Malik. 2010. Cassandra: A Decentralized Structured Storage System. SIGOPS

Operating Systems Review 44, 2 (2010), 35–40.

[57] Rubao Lee, Minghong Zhou, Chi Li, Shenggang Hu, Jianping Teng, Dongyang Li, and Xiaodong Zhang. 2021. The Art
of Balance: A RateupDB Experience of Building a CPU/GPU Hybrid Database Product. Proc of VLDB 14, 12 (2021),
2999–3013.

[58] Justin Levandoski, David Lomet, , and Kevin Keliang Zhao. 2011. Deuteronomy: Transaction Support for Cloud Data.

In Proc of the Conf on Innovative Data Systems Research (CIDR ’11). www.crdrdb.org.
[59] Jimmy Lin. 2017. The Lambda and the Kappa. IEEE Internet Computing 21, 5 (2017), 60–66.
[60] Gang Liu, Leying Chen, and Shimin Chen. 2021. Zen: A High-Throughput Log-Free OLTP Engine for Non-Volatile

Main Memory. Proc. VLDB 14, 5 (2021), 835–848.

[61] Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo Kyrola, and Joseph M. Hellerstein. 2012.
Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud. Proc of VLDB 5, 8 (2012),
716–727.

[62] Tiago Macedo and Fred Oliveira. 2011. Redis cookbook: Practical techniques for fast data manipulation. O’Reilly.
[63] Sujaya Maiyya, Faisal Nawab, Divy Agrawal, and Amr El Abbadi. 2019. Unifying Consensus and Atomic Commitment

for Effective Cloud Data Management. Proc of VLDB 12, 5 (2019), 611–623.

[64] Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz
Czajkowski. 2010. Pregel: A System for Large-Scale Graph Processing. In Proc of the Intl Conf on Management of Data
(SIGMOD ’10). ACM, 135–146.

[65] Nirmesh Malviya, Ariel Weisberg, Samuel Madden, and Michael Stonebraker. 2014. Rethinking main memory OLTP

recovery. In Proceeding of the Intl Conf on Data Engineering (ICDE ’14). IEEE, 604–615.

[66] Robert Ryan McCune, Tim Weninger, and Greg Madey. 2015. Thinking Like a Vertex: A Survey of Vertex-Centric

Frameworks for Large-Scale Distributed Graph Processing. ACM Comput. Surv. 48, 2, Article 25 (2015).

[67] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman,
DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, and
Ameet Talwalkar. 2016. MLlib: Machine Learning in Apache Spark. Journal of Machine Learning Research 17, 1 (2016),
1235–1241.

[68] C. Mohan, Don Haderle, Bruce G. Lindsay, Hamid Pirahesh, and Peter M. Schwarz. 1992. ARIES: A Transaction
Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging. Trans on
Database Systems 17, 1 (1992), 94–162.

[69] Barzan Mozafari, Jags Ramnarayan, Sudhir Menon, Yogesh Mahajan, Soubhik Chakraborty, Hemant Bhanawat, and
Kishor Bachhav. 2017. SnappyData: A Unified Cluster for Streaming, Transactions and Interactice Analytics. In Proc
of the Conf on Innovative Data Systems Research (CIDR ’17). www.cidrdb.org.

[70] Derek G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Martín Abadi. 2013. Naiad: A
Timely Dataflow System. In Proc of the Symposium on Operating Systems Principles (SOSP ’13). ACM, 439–455.
[71] Derek G. Murray, Malte Schwarzkopf, Christopher Smowton, Steven Smith, Anil Madhavapeddy, and Steven Hand.
2011. CIEL: A Universal Execution Engine for Distributed Data-Flow Computing. In Proc of the Conf on Networked
Systems Design and Implementation (NSDI’11). USENIX Assoc., 113–126.

[72] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee, Harry C. Li, Ryan McElroy, Mike
Paleczny, Daniel Peek, Paul Saab, David Stafford, Tony Tung, and Venkateshwaran Venkataramani. 2013. Scaling
Memcache at Facebook. In Proc of the Conf on Networked Systems Design and Implementation (NSDI ’13). USENIX

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:39

Assoc., 385–398.

[73] Shadi A. Noghabi, Kartik Paramasivam, Yi Pan, Navina Ramesh, Jon Bringhurst, Indranil Gupta, and Roy H. Campbell.

2017. Samza: Stateful Scalable Stream Processing at LinkedIn. Proc of VLDB 10, 12 (2017), 1634–1645.

[74] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan.

2015. Gorilla: A Fast, Scalable, in-Memory Time Series Database. Proc of VLDB 8, 12 (2015), 1816–1827.

[75] Daniel Peng and Frank Dabek. 2010. Large-Scale Incremental Processing Using Distributed Transactions and
Notifications. In Proc of the Conf on Operating Systems Design and Implementation (OSDI’10). USENIX Assoc., 251–264.
[76] Sherif Sakr, Angela Bonifati, Hannes Voigt, Alexandru Iosup, Khaled Ammar, Renzo Angles, Walid Aref, Marcelo
Arenas, Maciej Besta, Peter A. Boncz, Khuzaima Daudjee, Emanuele Della Valle, Stefania Dumbrava, Olaf Hartig,
Bernhard Haslhofer, Tim Hegeman, Jan Hidders, Katja Hose, Adriana Iamnitchi, Vasiliki Kalavri, Hugo Kapp, Wim
Martens, M. Tamer Özsu, Eric Peukert, Stefan Plantikow, Mohamed Ragab, Matei R. Ripeanu, Semih Salihoglu,
Christian Schulz, Petra Selmer, Juan F. Sequeda, Joshua Shinavier, Gábor Szárnyas, Riccardo Tommasini, Antonino
Tumeo, Alexandru Uta, Ana Lucia Varbanescu, Hsiang-Yun Wu, Nikolay Yakovets, Da Yan, and Eiko Yoneki. 2021.
The Future is Big Graphs: A Community View on Graph Processing Systems. Commun. ACM 64, 9 (2021), 62–71.

[77] Matthias J. Sax, Guozhang Wang, Matthias Weidlich, and Johann-Christoph Freytag. 2018. Streams and Tables: Two
Sides of the Same Coin. In Proc of the Intl Workshop on Real-Time Business Intelligence and Analytics (BIRTE ’18). ACM,
Article 1.

[78] Vivek Shah and Marcos Antonio Vaz Salles. 2018. Reactors: A Case for Predictable, Virtualized Actor Database

Systems. In Proc of the Intl Conf on Management of Data (SIGMOD ’18). ACM, 259–274.

[79] Bin Shao, Haixun Wang, and Yatao Li. 2013. Trinity: A Distributed Graph Engine on a Memory Cloud. In Proc of the

Intl Conf on Management of Data (SIGMOD ’13). ACM, 505–516.

[80] Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski. 2011. Conflict-Free Replicated Data Types. In

Proc of the Intl Conf on Stabilization, Safety, and Security of Distributed Systems (SSS’11). Springer-Verlag, 386–400.

[81] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. 2016. Edge Computing: Vision and Challenges. Internet

of Things Journal 3, 5 (2016), 637–646.

[82] Jeff Shute, Radek Vingralek, Bart Samwel, Ben Handy, Chad Whipkey, Eric Rollins, Mircea Oancea, Kyle Littlefield,
David Menestrina, Stephan Ellner, John Cieslewicz, Ian Rae, Traian Stancescu, and Himani Apte. 2013. F1: A Distributed
SQL Database That Scales. Proc of VLDB 6, 11 (2013), 1068–1079.

[83] V. Srinivasan, Brian Bulkowski, Wei-Ling Chu, Sunil Sayyaparaju, Andrew Gooding, Rajkumar Iyer, Ashish Shinde,
and Thomas Lopatic. 2016. Aerospike: Architecture of a Real-Time Operational DBMS. Proc of VLDB 9, 13 (2016),
1389–1400.

[84] Michael Stonebraker. 2010. SQL Databases V. NoSQL Databases. Commun. ACM 53, 4 (2010), 10–11.
[85] Michael Stonebraker. 2012. New Opportunities for New SQL. Commun. ACM 55, 11 (2012), 10–11.
[86] Michael Stonebraker and Ugur Cetintemel. 2005. "One Size Fits All": An Idea Whose Time Has Come and Gone. In

Proc of the Intl Conf on Data Engineering (ICDE ’05). IEEE, 2–11.

[87] Michael Stonebraker, Samuel Madden, Daniel J Abadi, Stavros Harizopoulos, Nabil Hachem, and Pat Helland. 2007.
The end of an architectural era (It’s time for a complete rewrite). In Proc of VLDB (VLDB ’07). VLDB Endow., 1150–1160.
[88] Michael Stonebraker and Ariel Weisberg. 2013. The VoltDB Main Memory DBMS. IEEE Data Engineering Bulletin 36,

2 (2013), 21–27.

[89] Rebecca Taft, Irfan Sharif, Andrei Matei, Nathan VanBenschoten, Jordan Lewis, Tobias Grieger, Kai Niemi, Andy
Woods, Anne Birzin, Raphael Poss, Paul Bardea, Amruta Ranade, Ben Darnell, Bram Gruneir, Justin Jaffray, Lucy
Zhang, and Peter Mattis. 2020. CockroachDB: The Resilient Geo-Distributed SQL Database. In Proc of the Intl Conf on
Management of Data (SIGMOD ’20). ACM, 1493–1509.

[90] Carlos H. C. Teixeira, Alexandre J. Fonseca, Marco Serafini, Georgos Siganos, Mohammed J. Zaki, and Ashraf
Aboulnaga. 2015. Arabesque: A System for Distributed Graph Mining. In Proc of the Symposium on Operating Systems
Principles (SOSP ’15). ACM, 425–440.

[91] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, Kun Ren, Philip Shao, and Daniel J. Abadi. 2012. Calvin:
Fast Distributed Transactions for Partitioned Database Systems. In Proc of the Intl Conf on Management of Data
(SIGMOD ’12). ACM, 1–12.

[92] Ankit Toshniwal, Siddarth Taneja, Amit Shukla, Karthik Ramasamy, Jignesh M. Patel, Sanjeev Kulkarni, Jason Jackson,
Krishna Gade, Maosong Fu, Jake Donham, Nikunj Bhagat, Sailesh Mittal, and Dmitriy Ryaboy. 2014. Storm@Twitter.
In Proc of the Intl Conf on Management of Data (SIGMOD ’14). ACM, 147–156.

[93] Maarten Van Steen and Andrew S. Tanenbaum. 2017. Distributed Systems. Maarten Van Steen.
[94] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam, Kamal Gupta, Raman Mittal, Sailesh
Krishnamurthy, Sandor Maurice, Tengiz Kharatishvili, and Xiaofeng Bao. 2017. Amazon Aurora: Design Considerations
for High Throughput Cloud-Native Relational Databases. In Proc of the Intl Conf on Management of Data (SIGMOD
’17). ACM, 1041–1052.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:40

Margara et al.

[95] Paolo Viotti and Marko Vukolić. 2016. Consistency in Non-Transactional Distributed Storage Systems. ACM Comput.

Surv. 49, 1, Article 19 (2016).

[96] Alexander Visheratin, Alexey Struckov, Semen Yufa, Alexey Muratov, Denis Nasonov, Nikolay Butakov, Yury
Kuznetsov, and Michael May. 2020. Peregreen – modular database for efficient storage of historical time series
in cloud environments. In Proc of the USENIX Annual Technical Conf (ATC ’20). USENIX, 589–601.

[97] Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma, Khaled Elmeleegy, Scott Shenker, and Ion Stoica. 2010. Delay
Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling. In Proc of the European
Conf on Computer Systems (EuroSys ’10). ACM, 265–278.

[98] Matei Zaharia, Tathagata Das, Haoyuan Li, Timothy Hunter, Scott Shenker, and Ion Stoica. 2013. Discretized Streams:
Fault-tolerant Streaming Computation at Scale. In Proc of the Symposium on Operating Systems Principles (SOSP ’13).
ACM, 423–438.

[99] Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh
Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016.
Apache Spark: A Unified Engine for Big Data Processing. Commun. ACM 59, 11 (2016), 56–65.

[100] Jingyu Zhou, Meng Xu, Alexander Shraer, Bala Namasivayam, Alex Miller, Evan Tschannen, Steve Atherton, Andrew J
Beamon, Rusty Sears, John Leach, et al. 2021. FoundationDB: A Distributed Unbundled Transactional Key Value Store.
In Proc of the Intl Conf on Management of Data (SIGMOD ’21). ACM, 135–146.

[101] Tao Zhu, Zhuoyue Zhao, Feifei Li, Weining Qian, Aoying Zhou, Dong Xie, Ryan Stutsman, Haining Li, and Huiqi Hu.
2019. Solar: Toward a Shared-Everything Database on Distributed Log-Structured Storage. Trans on Storage 15, 2
(2019).

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:41

A DATA MANAGEMENT SYSTEMS

Driver exec

Driver exec time

Invoc of jobs

Sources

Sinks

State

Deployment

NoSQL systems

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

client
client
client+sysSP
client+sysSP
client
client
client
client
client+sysSP
client
client+sysSP
client
client
client+sysSP

client
client
client
client
client
client
sysSP+client
client
client
client
client

reg
reg
reg+startSP
reg+startSP
reg
reg
reg
reg
reg+startSP
reg
reg+startSP
reg
reg
reg

sync
sync
sync
sync
sync+async
sync
sync
sync
sync+async
unknown
unknown
unknown
unknown
sync

NewSQL systems

reg
reg
reg
reg
reg
reg
startSP(+reg)
reg
reg
reg
reg

sync
sync
sync
sync+async
sync
implem dep
sync+async
sync
sync
sync
sync

no
no
no
no
no
active
no
both
active
active
active
no
no
no

no
no
no
no
no
no
no
no
no
no
no

no
yes
yes
no
yes
yes
yes
no
yes
no
yes
no
no
no

no
no
no
no
no
no
yes
no
no
no
no

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

wide
wide
hybrid
hybrid
wide
cluster
hybrid
cluster
cluster
cluster
wide
cluster
wide
cluster

cluster+wide
cluster+wide
cluster+wide
cluster+wide
wide
cluster+wide
hybrid
cluster+wide
cluster
cluster
cluster

Table 6. Data management systems: functional model. Legend: SP= stored procedures.

Jobs def API

Exec plan
def

Task
comm

Exec plan
struct

Iter

Dyn
creat.

Nature
of jobs

State man.

Data par
API

Placem.
-aware API

Dynamo
DynamoDB

Redis

BigTable

Cassandra

MongoDB

CouchDB

AsterixDB

InfluxDB

Gorilla
Monarch
Peregreen
TAO
Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

crud
crud+scan

crud+scan
+ops on vals

crud+scan

crud+scan
+ops on vals

impl
impl

impl

impl

impl

impl
impl

impl

impl

NoSQL systems

1 task (put/get)
1 task (put/get)

1 task (put/get)

1 task (put/get)

impl

1 task (put/get)

no
no

no

no

no

crud+scan
+ops on vals+aggs

impl

impl

1 task (put/get)

no

crud+scan
+ops on vals

SQL-like

crud+scan
+aggs

crud+scan
SQL-like
crud
crud+relations
search

crud+scan
crud+scan
crud+scan
declar DSL
SQL
API agnostic
Java + SQL
SQL
SQL
library
library

impl

impl

impl

expl
impl
impl
impl
impl

impl
impl
impl
impl
impl
impl
impl
impl
impl
impl
expl

impl

impl

impl

impl
impl
impl
impl
impl

impl
impl
impl
impl
impl
impl
impl
impl
impl
impl
expl

1 task (put/get)

dataflow

workflow

1 task (put/get)
workflow (tree)
workflow (tree)
task (put/get)
hierar. workflow

no

no

no

no
no
no
no
yes

NewSQL systems

1 task (put/get)
1 task (put/get)
1 task (put/get)
workflow
dataflows+coord
workflow
workflow
workflow
workflow
workflow
graph

no
no
no
no
no
no
no
no
no
no
yes

no
no
no/yes
no
no
no
no
no
no
no
no

no
no

no

no

no

no

no

no

no

no
no
no
no
yes

one-shot
one-shot

one-shot

one-shot

one-shot

expl
expl

expl

expl

expl

one-shot

expl

one-shot

one-shot

one-shot

one-shot
one-shot
one-shot
one-shot
one-shot

one-shot
one-shot
one-shot
one-shot
one-shot
one-shot
one-shot
one-shot
one-shot
one-shot
one-shot

expl

expl

expl

expl
expl
expl
expl
expl

expl
expl
expl
expl
expl
expl
expl
expl
expl
expl
expl

no
no

no

no

no

no

no

no

no

no
no
no
no
no

no
no
no
no
no
no
no
no
no
no
yes

no
no

no

no

no

no

no

no

no

no
no
no
no
no

no
no
no
no
yes
no
yes
no
no
no
no

Table 7. Data management systems: jobs definition.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:42

Margara et al.

Jobs comp. time

Use static res info

Use dyn res info

Granul of depl

Depl time

Use dyn res info

Manag of res

NoSQL systems

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

exec
exec
exec
exec
exec
exec
exec
exec
reg
exec
reg
exec
exec
exec

exec
exec
exec
exec
exec
exec
reg
exec
exec
exec
exec

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

no
no
no
no
no
no
no
no
no
no
no
no
no
no

NewSQL systems

no
no
no
no
no
no
no
no
no
no
no

job
job
job
job
job
job
job
task-level
job
job
job
job
job
job

job
job
job
job
job
job
job
job
job
job
job

job comp
job comp
job comp
job comp
job comp
job comp
job comp
task activ
job comp
job comp
job comp
job comp
job comp
job comp

job comp
job comp
job comp
job comp
job comp
job comp
job comp
job comp
job comp
job comp
job comp

no
no
no
no
no
no
no
no
no
no
no
no
no
no

no
no
no
yes
yes
no
no
no
no
no
no

sys-only
sys-only
sys-only
shared
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only

sys-only
sys-only
sys-only
shared
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only
sys-only

Table 8. Data management systems: jobs compilation and execution.

Elem struct

Temp elem

Bus conn

Bus impl

Bus persist

Bus partition

Bus repl

Bus inter

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

key-value
key-value
key-value
wide-col
wide-col
document
document
document
time series
time series
time series
time series
typed graph
typed graph

Deuternomy
FoundationDB
SolarBD
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

key-value
key-value
key-value
semirelational
relational
structure agnostic
relational
relational
relational
object
typed graph

no
no
no
yes
no
no
no
no
yes
yes
yes
yes
yes
yes

no
no
no
no
no
no
no
no
no
no
no

NoSQL systems

direct
direct
direct
direct
direct
direct
direct
direct
direct
direct
direct
direct
direct
direct

net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan

NewSQL systems

direct
direct
direct
direct
direct
direct
direct
direct
direct
direct
direct

net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
net chan
remote mem

ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral

ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes

Table 9. Data management systems: data management.

no
no
no
no
no
no
no
no
no
no
no
no
no
no

no
no
no
no
no
no
no
no
no
no
no

pull
pull
pull
pull
pull
pull
pull
pull
pull
pull
pull
pull
pull
pull

pull
pull
pull
pull
pull
push
pull
pull
pull
pull
pull

A.1 NoSQL systems

A.1.1 Key-value stores. Dynamo. Dynamo [42] is a NoSQL key-value store used by Amazon to save
the state of its services. State elements are arbitrary values (typically, binary objects) identified by
a unique key. Jobs consist of operations on individual state elements: retrieve the value associated
to a key (get) or insert/update a value with a given key (put). Dynamo builds a distributed hash
table: workers have an associated unique identifier, which determines the portion of shared state
(set of keys) they are responsible for. Shared state is replicated, so multiple workers are responsible
for the same key. When a worker receives a client request for a key, it forwards the request to one
of the workers responsible for that key. Clients may also be aware of the distribution of shared

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

Elem struct

Stor
medium

Stor
struct

Task st

Shared st

St part

Repl

Repl consist

Repl prot

NoSQL systems

Dynamo

key-value

agnostic

agnostic

DynamoDB

key-value

Redis

key-value

disk

mem

B-trees

user-def

BigTable

wide-col

hybrid

LSM trees

Cassandra

wide-col

hybrid

LSM trees

MongoDB

document

agnostic

agnostic

CouchDB

document

disk

B-trees

AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen

TAO

Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB

document
time series
time series
time series
time series

graph

graph

key-value
key-value
key-value
relational
relational

hybrid
hybrid
mem
mem
agnostic

serv

mem

agnostic
disk
hybrid
disk
hybrid

LSM trees
LSM trees
TSMap
user-def
agnostic

MySQL+
memcache

indexes

agnostic
B-trees
LSM trees
B-trees
LSM trees

Calvin

agnostic

agnostic

agnostic

VoltDB

relational

mem

B-trees

relational

disk

log+
B-trees

Aurora

Socrates
Tango
A1

no

no

no

no

no

no

no

no
no
no
no
no

no

no

no
no
no
no
no

no

no

no

yes

yes

yes

yes

yes

yes

yes

yes
yes
yes
yes
yes

yes

yes
NewSQL systems
yes
yes
yes
yes
yes

yes

yes

yes

yes

yes

yes

yes

yes

yes

yes

yes
yes
yes
yes
yes

yes

yes

yes
yes
yes
yes
yes

yes

yes

yes

yes

yes

yes

conf

conf

weak

quorum
+confl

leader

leader(clus)
confl(wide)

backup(clus)
yes(wide)

weak

confl

yes

yes

yes

no
yes
yes
yes
yes

yes

backup

yes
yes
backup
yes
yes

yes

yes

yes

conf

conf

conf

n.a.
weak
weak
weak
strong

weak

n.a.

strong
strong
n.a.
strong
strong

strong

quorum
+confl

leader

quorum
+confl

n.a.
leader
leader
leader
leader

leader

n.a.

leader
leader
leader
leader
cons

leader
or cons

strong(clus)
weak(wide)

cons(clus)
+confl(wide)

strong

leader

leader
n.a.
n.a.

relational
object
graph

yes
serv+disk
strong
yes
serv
n.a.
yes
mem
n.a.
Table 10. Data management systems: state management.

log
log
user-def

yes
backup
backup

yes
yes
yes

no
no
no

1:43

Update
propag

op

unknown

op

op

op

op

op

n.a.
state
state
state
unknown

op

n.a.

n.a.
state
op
op
op

op

op

op

op
n.a.
n.a.

state portions across workers, and use this information to better route their requests. Dynamo uses
a quorum-based approach where read and write operations on a key need to be processed by a
quorum of the replicas responsible for that key. Users can set the number of replicas for each key,
the read quorum, and the write quorum to trade consistency and durability for performance and
availability. After a write quorum is reached, updates are asynchronously propagated to remaining
replicas. In the case of transient unavailability of one replica, another node can temporarily store
writes on its behalf, which avoids blocking write operations while preserving the desired degree
of replication for durability. When Dynamo is configured to trade consistency for availability,
replicas may diverge due to concurrent writes: to resolve conflicts, Dynamo adopts a versioning
system, where multiple versions of a given key may exist at different replicas. Upon write, clients
specify which version they want to overwrite: Dynamo uses this information to trace causality
between updates and automatically resolves conflicts when it can determine a unique causal order
of updates. In the case of concurrent updates, Dynamo preserves conflicting versions and presents
them to clients (upon a read) for semantic reconciliation. Dynamic reconfiguration is a key feature
of Dynamo. Nodes can be added and removed dynamically, and shared state portions automatically
migrate. Dynamo adopts a distributed (gossip based) failure detection model, while failure recovery
takes place by simply redistributing state portions over remaining nodes.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:44

Margara et al.

Aborts

Protocol

Assumptions

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

n.a.
sys
n.a.
n.a.
n.a.
sys+job
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

sys+job
sys+job
sys+job
sys+job
sys+job
job
sys+job
sys+job
sys+job
sys+job
sys+job

n.a.
unknown
n.a.
n.a.
n.a.
blocking
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

blocking
blocking
blocking
blocking
blocking
blocking
blocking
coord free
coord free
blocking
blocking

n.a.
cluster deployment
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

Impl

Level
NoSQL systems
n.a.
conf
n.a.
coord free
coord free
blocking
n.a.
blocking
n.a.
n.a.
n.a.
coord free
n.a.
n.a.

n.a.
unknown
n.a.
SEQ
SEQ
ts
n.a.
lock
n.a.
n.a.
n.a.
SEQ
n.a.
n.a.

none
none
none
none
none
DC
DC
1W
1W
none
none

NewSQL systems
blocking
blocking
blocking
blocking
blocking
blocking
blocking
coord free
coord free
blocking
blocking

lock+ts
ts (OCC)
ts (OCC)
lock+ts
lock+ts
ts
ts
ts
ts
ts
ts

Assumptions

Delivery

Nature of ts

Order

n.a.
cluster depl
n.a.
1P
1P
none
n.a.
1P
n.a.
n.a.
n.a.
1P
n.a.
n.a.

none
none
none
none
none
DC
DC
1W
1W
none
none

conf (most/exact)
conf (most/exact)
most
exact
conf (most/exact)
conf (most/exact)
conf (most/exact)
exact
exact
most
most
most
most
most

exact
exact
exact
exact
exact
exact
exact
exact
exact
exact
conf (most/exact)

no
no
no
event
no
no
no
no
event
event
event
no
event
event

no
no
no
no
no
no
no
no
no
no
no

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

Table 11. Data management systems: group atomicity, group isolation, delivery, order. Legend: DC = jobs are
deterministic; SEQ = jobs are executed sequentially, with no interleaving; 1W = a single worker handles all
writes; 1P = jobs access a single state portion; OCC = optimistic concurrency control.

Detection

Scope

Comput recov

State recov

Guarantees for state

Assumptions

NoSQL systems

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

p2p
mast-work
p2p
mast-work
p2p
mast-work
manual
n.a.
mast-work
mast-work
unknown
p2p
unknown
unknown

mast-work
mast-work
mast-work
mast-work
mast-work
n.a.
p2p
mast-work
mast-work
mast-work
mast-work

shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st

shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st
shared st

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

repl
log+checkp+repl
log+checkp+repl
log+repl
log+repl
log+repl
log+repl
log+checkp
log+repl
log+repl
log+repl
repl
repl
repl

none
same
conf (none or same)
same
none
conf (none or same)
none
same
none
none
none
same
none
none

NewSQL systems

log+checkp+repl
log+repl
log+checkp+repl
log+checkp+repl
log+repl
log+checkp+repl
log+checkp+repl
log+checkp
log+checkp
log+repl
repl

same
same
same
same
same
same
same(cluster)/none(hybrid)
same
same
same
conf (none or same)

none
REPL
REPL
STOR
none
REPL
none
STOR
none
none
none
REPL
none
none

STOR
STOR
STOR
STOR
STOR
DC
DC
STOR
STOR
REPL
STOR

Table 12. Data management systems: fault tolerance. Legend: STOR = storage layer is durable; REPL =
replicated data is durable.

DynamoDB. DynamoDB9 is an evolution of Dynamo that aims to simplify operational concerns:
it is fully managed and offered as a service. It exposes the same data model as Dynamo, but uses
a single-leader replication protocol, where all writes for a key are handled by the leader for that
key, which propagates them synchronously to one replica (for durability) and asynchronously to
another replica. Read operations can be either strongly or weakly consistent: the former are always

9http://aws.amazon.com/dynamodb/

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:45

Goal

Automated

State migration

Task migration

Add/del slots

Restart

Dynamo
DynamoDB
Redis
BigTable
Cassandra
MongoDB
CouchDB
AsterixDB
InfluxDB
Gorilla
Monarch
Peregreen
TAO
Unicorn

load balan+elast
load balan+elast
load balan
load balan
load balan
load balan
load balan
n.a.
load balan
load balan
load balan
load balan+elast
unknown
unknown

Deuteronomy
FoundationDB
SolarDB
Spanner
CockroachDB
Calvin
VoltDB
Aurora
Socrates
Tango
A1

elast
elast
elast
change schema+load balan
elast
n.a.
n.a
elast
elast
n.a.
unknown

NoSQL systems
yes
yes
yes
yes
yes
yes
yes
n.a.
yes
yes
yes
yes
unknown
unknown

yes
yes
no
yes
yes
yes
no
n.a.
no
yes
yes
unknown
unknown
unknown

NewSQL systems
yes
yes
yes
yes
yes
n.a.
n.a.
yes
yes
n.a.
unknown

no
no
no
yes
yes
n.a.
n.a.
yes
no
n.a.
unknown

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

yes
yes
yes
yes
yes
yes
yes
n.a.
yes
yes
yes
yes
unknown
unknown

yes
yes
yes
yes
yes
n.a.
n.a.
yes
yes
n.a.
unknown

no
no
no
no
no
no
no
n.a.
no
no
no
no
unknown
no

no
no
no
no
no
n.a.
n.a.
no
no
n.a.
unknwon

Table 13. Data management systems: dynamic reconfiguration.

processed by the leader, while the latter may be processed by any replica, even if the replica lags
behind of updates. Replicas can be located in different data centers to support wide area deployments.
DynamoDB stores data on disk using B-trees and buffers incoming write requests in a write-ahead
log for improve write performance. It supports secondary indexes that are asynchronously updated
from the log upon write. Recent versions of DynamoDB enable automated propagation of changes
to external sinks. They also offer an API to group individual operations in transactions that provide
group atomicity and configurable group isolation. Transactions can be configured to be idempotent,
thus offering exactly once delivery. To detect failures, the leader of each partition periodically
sends heartbeats to all replicas. After some heartbeats are lost, the remaining nodes use the Paxos
consensus algorithm to elect a new leader. As an additional fault tolerance mechanism, B-trees
and logs are periodically checkpointed to durable storage. Amazon offers automatic scaling of
DynamoDB as a service: users can select the expected read and write throughput for a given
table (blocks of key-value pairs) and DynamoDB automatically increases the amount of resources
dedicated to that table to meet the requirements.

Redis. Redis [62] is a single-node in-memory key-value store. Since version 3.0, Redis Cluster
provides a distributed implementation of the store. In terms of data model, Redis differs from
other key-value stores in that it provides typed values and optimized operations for those types:
for instance, a value may be declared as a list, which supports appending new elements without
overwriting the entire list. Redis provides a scripting language to express driver programs (stored
procedures) that run system-side. Redis supports data dispatching to sinks in the form of a publish-
subscribe service: clients can subscribe to a given key and be notified about the changes to that
key. Users can configure their desired level of durability (for fault tolerance), with options ranging
from no persistence to using periodic checkpointing to command logging. Redis Cluster partitions
data by key and uses single leader asynchronous replication. Alternative solutions are available
for wide area deployments, where redirecting all writes to a single leader may be unfeasible: for
instance, Redis CRDTs offer multi-leader replication with automated conflict resolution based on
conflict-free replicated data types. Dynamic reconfiguration with migration of shared state portions
is supported but not automated.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:46

Margara et al.

Other key-value stores. Several other stores implement the key-value model with design and im-
plementation strategies that are similar to those presented above. For the sake of space, we only
discuss their key distinguishing factors. Voldemort10 and Riak KV11 follow the same design as
Dynamo. Like Redis, Aerospike [83] adopts single leader replication within a single data center
and multi-leader replication for wide area deployments. It focuses on both horizontal scalability
(with workers on multiple nodes) and vertical scalability (with multiple workers on the same node).
It adopts a hybrid storage model where key indexes are kept in memory but concrete values can
be persisted on disk, and a threading model that reduces locking and contention by assigning
independent shared state portions to threads. PNUTS [36] provides single leader replication in
wide area deployments: it uses an intermediate router component to dispatch jobs invocations to
responsible workers, and relies on an external messaging system to implement the data bus that
propagates updates to replicas. The same service can also notify external sinks. Thanks to their
simple interface, key-value stores are sometimes used as building blocks in distributed software
architectures: Memcached is used as a memory-based distributed cache to reduce data access
latency at Facebook [72]. RocksDB12 is used to persist task state in the Flink stream processing
system (also discussed later). Other data stores offering a key-value model are presented below as
part of NewSQL databases.

A.1.2 Wide-column stores. BigTable. BigTable [32] and its open-source implementation HBase13
use a wide-column data model: shared state is organized in tables, where each row is associated
to a fixed number of column families. A column family typically contains multiple columns that
are frequently accessed together. Tables associate a value (binary object) to a row and a column
(within a column family), they are range-partitioned across workers by row and physically stored
(compressed) per column family. Jobs can read and update individual values and perform table scans.
Rows are units of isolation: accesses to columns of the same row from different jobs are serialized,
and this is the only task grouping guarantee that BigTable offers. BigTable adopts a master-worker
deployment, where a master is responsible for assigning shared state portions to workers. Initially,
each table is associated to a single worker, but it is automatically split when it increases in size.
Clients retrieve and cache information about state distribution and submit their requests (tasks)
involving a given state portion to the worker responsible for that state portion. BigTable can store
multiple versions for each value. Versions are also visible to clients, which can retrieve old versions
and control deletion policies. Writes always append new versions of a value, which improves write
throughput to support frequent updates. Workers store recent versions in memory and use an
external storage service (the GFS distributed filesystem) for durability. Background compaction
procedures prune old versions from memory and from the storage. BigTable uses replication only
for fault tolerance. Specifically, it relies on the replication of the GFS storage layer, where it also
saves a command log. In the case of failure of a worker, the latest snapshot of its shared state portion
is restored from GFS and from the commands in the log that were not part of the snapshot. BigTable
supports wide area deployments by fully replicating the data store in each data center. Replicas
in other data centers may be only used for fault tolerance or they can serve client invocations,
in which case they provide eventual consistency. Similar to key-value stores, BigTable provides
dynamic reconfiguration by migrating state across available workers.
Cassandra. Cassandra [56] combines the wide-column data model of BigTable and the distributed
architecture of Dynamo. Like BigTable, Cassandra uses versioning and background compaction tasks

10https://www.project-voldemort.com/
11https://riak.com/products/riak-kv/
12https://rocksdb.org
13https://hbase.apache.org

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:47

to improve the performance of write-intensive workloads. It offers a richer job definition language
that includes predefined and user-defined types and operations. It supports task grouping only for
compare-and-swap operations within a single partition. Like Dynamo, it uses a distributed hash table
to associate keys to workers, and provides replication both in cluster and wide-area deployments,
using a quorum-based approach for consistency. The quorum protocol can be configured to trade
consistency and durability for performance, setting the number of local replicas (within a data
center) and global replicas (in the case of wide area deployments) that need to receive and approve
a task before the system returns to the client. In the case of weak (eventual) consistency, Cassandra
uses an anti-entropy protocol to periodically and asynchronously keep replicas up-to-date, using
automated conflict resolution (last write wins).

A.1.3 Document stores. MongoDB. MongoDB [35] is representative of document stores, an exten-
sion of key-value stores where values are semistructured documents, such as binary JSON in the
case of MongoDB. MongoDB jobs can insert, update, and delete entire documents, but also scan,
retrieve, and update individual values within documents. Recent versions of MongoDB support
simple data analytic jobs expressed as pipelines of data transformations. External systems can
register as sinks to be notified about changes to documents. Shared state partitioning can be either
hash-based or range-based. Clients are oblivious of the location of state portions and interact with
the data store through a special worker component that acts as a router. Shared state portions can be
replicated for fault tolerance only or also to serve read queries. Replication is implemented using a
single leader protocol with semi-synchronous propagation of changes, where clients can configure
the number of replicas that need to synchronously receive an update, thus trading durability and
consistency for availability and response time. By default, only single-document jobs are atomic.
Recent versions also support distributed transactions using two-phase commit for atomicity and
multi-version concurrency control for snapshot isolation.
CouchDB. CouchDB [11] adopts the same document data model as MongoDB. Early versions only
support complete replication of shared state, allowing clients to read and write from any replica to
improve availability. Replicas are periodically synchronized and conflicts are handled by storing
multiple versions of conflicting documents or fields, delegating resolution to users. Since version
2.0, CouchDB provides a cluster mode with support for shared state partitioning and quorum-
based replication, where users can configure the number of replicas for shared state portions, and
quorum for read and write operations, thus balancing availability and consistency. CouchDB is
conceived for Web applications and provides a synchronous HTTP API for job invocation. It makes
the list of changes (change feed) to a document available for external components, which can
consume it either in pull mode or in push mode (thus representing the sink components in out
model). CouchDB lets users define multiple views for each document. In our model, we can see
views as the results of registered jobs that are triggered by changes to documents. Computations
that create views are restricted to execute on individual documents, but their results can then be
aggregated by key (mimicking the map-reduce programming model). CouchDB supports dynamic
reconfiguration with addition and removal of nodes, and redistribution of shared state portions
across nodes. However, reconfiguration is a manual procedure.
AsterixDB. AsterixDB14 is a semi-structured (document) store born as a research project that
integrates ideas from NoSQL databases and distributed processing platforms. AsterixDB offers a
SQL-like declarative language that integrates operators for individual documents as well as for
multiple document (like joins, group by). Jobs are converted into a dataflow format and run on
the Hyracks data-parallel platform [22]. Interestingly, the platform deploys jobs task-by-task, but

14https://asterixdb.apache.org

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:48

Margara et al.

does not rely on a persistent data bus. Rather, when all input data for a task become ready, it
dynamically establish network connections from upstream tasks that deliver the results of their
computation before terminating. AsterixDB supports querying shared state as well as external
data active and passive sources. Like BigTable and Cassandra, it stores state in LSM trees, which
improve the performance of write operations [10]. It supports partitioning but does not currently
support replication. As part of its shared state, AsterixDB can store indexes to simplify accessing
external data. It offers isolation only for operations on individual values, using locking to update
indexes. Its data structure offer fault tolerance through logging, but it does not currently implement
fault detection nor dynamic reconfiguration mechanisms.

A.1.4 Time-series stores. InfluxDB. InfluxDB15 is a DMS for time series data. Shared state is or-
ganized into measurements, which are sparse tables resembling wide columns in BigTable and
Cassandra. Each row maps a point in time (primary key) to the values of one or more columns.
Developers need to explicitly state which columns are indexed and which are not, thus balancing
read and write latency. InfluxDB uses a storage model (time structured merge – TSM trees) that
derives from LSM trees. It stores measurements column-wide and integrates disk-based storage
and an in-memory write cache to improve write performance. Jobs are defined in the InfluxQL
declarative language and are restricted to single measurements. InfluxDB supports active sources,
and mimics continuous jobs through periodic execution of one-shot jobs, which write their results
inside the database and/or send them to sinks. InfluxDB partitions and replicates shared state across
workers. Write operations are propagated semi-synchronously across replica workers for fault
tolerance. Read operations can access any of the replica workers that contain the requested data,
leading to weak consistency. InfluxDB adopts a master-worker approach, where master nodes store
meta-data about membership, data partitioning, continuous queries, and access rights, and worker
nodes store the actual data. The master nodes are replicated with strong consistency for fault
tolerance, using the Raft consensus protocol. InfluxDB offers dynamic reconfiguration to migrate
data and add new workers, but the process is manual.
Gorilla. Gorilla [74] is an in-memory time series store that Facebook uses as a cache to an HBase
data store. HBase stores historical data compressed with a coarser time granularity, while Gorilla
persists the most recent data (26 hours) in memory. Gorilla uses a simple data model where values
for each measure always consist of a 64-bit timestamp and a 64-bit floating point number. It uses
an encoding scheme based on bit difference that reduces the data size by 12 times on average.
Jobs in Gorilla only perform simple read, write, and scan operations. Few ad-hoc jobs have been
implemented to support correlation between time series and in-memory aggregation, which is
used to compress old data before writing it to HBase. Gorilla supports geo-replication for disaster
recovery, but trades durability for availability. Written data is asynchronously replicated and it can
be lost before it is made persistent. Gorilla supports dynamic adaptation by re-distributing the key
space across workers.
Monarch. Monarch [3] is a geo-distributed in-memory time series store designed for monitoring
large-scale systems and used within Google. Its data model stores time series data as schematized
tables. Each table consists of multiple key columns that form the time series key, and a value column,
one for each point in the history of the time series. Key columns include a target field, which is
the entity that generates the time series, and a metrics field, which represents the aspect being
measured. Monarch has a hierarchical architecture. Data is stored in the zone (data center) in which
it is generated and sharded (by key ranges, lexicographically) across nodes called leaves. Data is
stored in main memory and asynchronously persisted to logs on disk, trading durability to reduce

15https://www.influxdata.com

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:49

write delay. Monarch offers a declarative, SQL-like language to express jobs, which can be either
one-shot or continuous. In the latter case, they are evaluated periodically and store their results
in new derived tables (materialized views). Jobs are evaluated hierarchically: nodes are organized
in three layers (global, zone level, leaves), and the job plan pushes tasks as close as possible to
the data they need to consume. Each level also stores an approximate view (compressed index) of
what the nodes in the lower level store. This enables optimizing communication across levels by
avoiding pushing tasks to nodes that have no data related to that task. Monarch supports dynamic
reconfiguration: it monitors lower-level nodes and re-distributes data (key ranges) across nodes to
adapt to changes in the load.
Peregreen. The design of the Peregreen [96] time series database aims to satisfy the following
requirements. (1) Cloud deployment of large volumes of historical data: as the scale of data is
prohibitive for in-memory solutions, Peregreen relies on storage services such as distributed
filesystems or block storage. It limits raw data footprint by supporting only numeric values and
by representing them in a compressed columnar format: each column is split into chunk and data
in each chunk is represented using differences between adjacent values (delta encoding), further
compressed to form a single binary array. (2) Fast retrieval of data through indexing: Peregreen
uses a three-tier data indexing, where each tier pre-computes aggregated statistics (minimum,
maximum, average, etc.) for the data it references. This allows to quickly identify chunks of data
that satisfy some conditions based on the pre-computed statistics and to minimize the number of
interactions with the storage layer. Peregreen jobs can only insert, update, delete, and retrieve data
elements. Retrieval supports limited conditional search (only based on pre-computed statistics) and
data transformation. Chunks are versioned and a new version is created in the case of modifications.
Peregreen is designed for cluster deployments and uses the Raft algorithm to reach consensus on
the workers available at any point in time and the state portions (indexes to the storage layer) they
are responsible for. This enables dynamic adaptation, with addition and removal of workers for
load-balancing and elasticity. State portions are replicated at multiple workers for fault tolerance.

A.1.5 Graph stores. TAO. TAO [23] is a data store that Facebook developed to manage its social
graph, which contains billions of entities (such as people and locations) and relations between
them (such as friendship). TAO offers a simple data model where entities and relations have a type
and may contain data in the form of key-value pairs. It provides a restricted API for job definition,
to create, delete, and modify entities and relations, and to query relations for a given entity. With
respect to other graph data stores, it does not support queries that search for subgraphs that satisfy
specific constraints (path queries or subgraph pattern queries). TAO is designed to optimize the
latency of read jobs, as it needs to handle a large number of simultaneous user-specific queries. To
do so, TAO implements shared state in two layers: a persistent storage layer based on a relational
database (MySQL) and a key-value in-memory cache based on memcache. TAO is designed for
wide area deployments. Within a single data center, both the persistent layer and the cache are
partitioned. The cache is also replicated using a single-leader approach: clients always interact
with follower cache servers, which reply to read operations in the case of cache hit and propagate
read operations in the case of cache miss as well as write operations to the leader cache server,
which is responsible for interacting with the storage layer and for propagating changes. Across
data centers, the storage layer is fully replicated with a single-leader approach: reads are served
using the data center local cache or storage layer, so they do not incur latency, while all write
operations are propagated to the leader data center for the storage layer. Data is replicated between
the storage layer and the cache as well as across data centers asynchronously, which provides weak
consistency and durability. Dynamic reconfiguration is possible in the case of failures: in the case a
leader cache or storage server fails, replicas automatically elect a new leader.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:50

Margara et al.

Unicorn. Unicorn [38] is an indexing system used at Facebook to search its social graph. The shared
state of Unicorn consists of inverted indexes that enable retrieving graph entities (vertices) based
on their relations (edges) and the data associated to them: for instance, in a social graph, one could
use an index on relations to retrieve all people (vertices) that are in a friend relation with a given
person, or a string prefix index to retrieve all people with a name that starts with a given prefix.
Unicorn is optimized for read-only queries (jobs), in the form of index lookups and set operations
(union, intersection, difference) on lookup results. Jobs are evaluated by exploiting a hierarchical
organization of workers into three layers: (i) index servers store the shared state (the indexes)
partitioned by results, meaning that any index server will store a subset of the results for each
index lookup; (ii) a rack aggregator per rack is responsible for merging the (partial) query results
coming from individual index servers; (iii) a top aggregator is responsible for merging the (partial)
query results coming from each rack. Interestingly, Unicorn jobs can dynamically start new tasks:
this feature is implemented as an apply function that performs new lookups starting from the
results obtained in previous ones. This feature can be used to implement iterative computations:
for instance, to find the friends of friends of a given person, Unicorn can first retrieve the direct
friends and then lookup for all their friends, using result set of the first lookup (direct friends) as
a parameter for the second lookup. Unicorn indexes are kept up-to-date with the content of the
social graph using a periodic procedure that runs on an external computation engine. Unicorn
tolerates failures through replication. However, it is possible for some shared state portions to
remain temporarily unavailable: this may result in incomplete results when searching (if some
index serves do not reply), which is acceptable in its specific application domain.

A.2 NewSQL systems

A.2.1 Key-value stores. Deuteronomy. Deuteronomy [58] is a data store designed for wide area de-
ployments that decouples storage functionalities from transactional execution of jobs. In Deuteron-
omy, the driver program runs client-side and submits jobs (queries) to a transaction component.
The component ensures group atomicity and isolation for all read and write operations performed
on the shared state, using a locking protocol. The actual storage is implemented in a separate layer.
Deuteronomy supports any distributed storage component that offers read and write operations for
individual elements (such as a key-value store), and is oblivious of the actual location of the workers
implementing the storage component, which may be geographically distributed. Deuteronomy
provides fault tolerance through logging and replication, and enables dynamic reconfiguration by
independently scaling both the transactional and the storage component.

FoundationDB. FoundationDB [100] is a transactional key-value store, which aims to offer the
core building blocks (hence the name) to build scalable distributed data management systems
with heterogeneous requirements. The use of key-value abstractions provides flexibility in the
data model, on top of which developers can build various types of abstractions: for instance, each
element may encode a relation indexed by its primary key. Jobs consist of a group of read and
write requests issued by a driver program that runs client-side. Like Deuteronomy, FoundationDB
is organized into layers, each of them offering one of the core functionalities of a transactional
DMS and each implemented within a different set of workers, thus enabling independent scaling. A
storage layer persists data and serves read and write requests. A log layer manages a write ahead
log. A transaction system handles isolation and atomicity for multiple read and write requests.
Transactional semantics is enforced by assigning timestamps to operations and by checking for
conflicts between concurrent transactions after they have been executed: in the case of conflict the
transaction aborts and the driver program is notified. Fault tolerance is implemented by replicating
both the log layer (synchronously) and the storage layer (asynchronously). Replication of the

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:51

storage layer is also used to serve read requests in parallel. Moving data between workers that
implement the storage layer and the log layer is also used when adding or removing workers for
scalability (dynamic reconfiguration).

Solar. Like Deuteronomy and FoundationDB, Solar [101] is a transactional key-value store that de-
couples storage of shared state from transaction processing. The transaction layer uses a timestamp-
based optimistic concurrency control and stores a write ahead log in main memory. The storage
layer persists checkpoints of the shared state. Together, they form a LSM tree. The key distinguish-
ing feature of Solar is that the transaction layer is implemented as a centralized service, replicated
for fault tolerance of the write ahead log.

A.2.2

Structured and relational stores.

Time-based protocols. Spanner. Spanner [37] is a semirelational database: shared state is organized
into tables, where each table has an ordered set of one or more primary key columns and defines a
mapping from these key columns to non key columns. Spanner provides transactional semantics
and replication with strong consistency for cluster and wide area deployments. At its core, Spanner
uses standard database techniques: two-phase locking for isolation, two-phase commit for atomicity,
and synchronous replication of jobs results using Paxos state machine replication. Jobs are globally
ordered using timestamps, and workers store multiple versions of each state element (multi-version
concurrency control). This way, read-only jobs can access a consistent snapshot of the shared state
and do not conflict with read-write jobs. A consistent snapshot preserves causality, meaning that if a
job reads a version of a state element (cause) and subsequently updates another state element (effect),
the snapshot cannot contain the effect without the cause. Traditional databases ensure causality by
acquiring read and write locks to prevent concurrent accesses, but this could be too expensive in a
distributed environment. The key distinguishing idea of Spanner is to serve a consistent snapshot to
read-only jobs without locking. To do so, it uses an abstraction called TrueTime, which returns real
(wall-clock) time within a known precision bound using a combination of GPS and atomic clocks.
Tasks of read-write jobs first obtain the locks for all the data portions they access: a coordinator for
the job assigns that job with a timestamp at the end of its time uncertainty range, then it waits
until this timestamp is passed for all workers in the system, releases the locks and writes the results
(commits). The waiting time ensures that jobs with later timestamps read all writes of jobs with
earlier timestamps without explicit locking. Using TrueTime, Spanner also supports consistent
reconfiguration, for instance to change the database schema or to move data for load balancing.
More recently, Spanner has been extended with support for distributed SQL query execution [15].

CockroachDB. CockroachDB [89] is a relational database for wide area deployments. It shares many
similarities with Spanner and integrates storage and processing capabilities within each node.
As a storage layer, it relies on RocksDB a disk-based key-value store that organizes data in LSM
trees. It replicates data across nodes, ensuring strong consistency through Raft consensus. On
top of this, it partitions data with transactional semantics: it uses an isolation mechanism based
on hybrid physical and logical clocks (similar to Spanner) but integrates it with an optimistic
protocol that, in the case of conflicts, attempts to modify the timestamp of a job to a valid one
rather than re-executing the entire job. CockroachDB compiles SQL queries into a plan of tasks that
can be either fully executed on a single worker or in a distributed dataflow fashion. Interestingly,
CockroachDB also enables users to configure data placement across data centers. For instance, a
table can be partitioned across a Region column to ensure that all data about one region is stored
within a single data center. This may improve access time from local client and enforce privacy
regulations.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:52

Margara et al.

Deterministic execution. Calvin. Calvin [91] is a job scheduling and replication layer to provide
transactional semantics and replication consistency on top of non-transactional distributed datas
tores such as Dynamo, Cassandra, and MongoDB. It is currently implemented within the Fauna
database16. The core Calvin layer is actually agnostic with respect to the specific data and query
model. In fact, Fauna supports document and graph-based models in addition to the relational
model. Calvin builds on the assumption that jobs are deterministic. Its core idea is to avoids as much
as possible expensive coordination during job execution by defining a global order for tasks before
the actual execution. In Calvin, workers are organized in regions: each region contains a single
copy of the entire shared state, and each worker in a region is fully replicated in every other region.
All workers that contain a replica of the same state portion in different regions are referred to as a
replication group. Invocations from clients are organized in batches: all workers in a replication
group receive a copy of the batch and they coordinate to agree on a global order of execution. Under
the assumption of deterministic jobs, this approach ensures consistent state across all replicas in
a replication group. Replicas are used both to improve access for read-only jobs, which can be
executed on any replica, and for fault tolerance, as in the case of a failure remaining replicas can
continue to operate without interruption. Invocations are stored in a durable log, and individual
workers can be resumed from a state snapshot by reapplying all invocations that occurred after
that snapshot. Calvin supports different replication protocols with different tradeoffs between job
response time and complexity in fail over. Deterministic jobs executed in the same order lead to
the same results in all (non failing) replicas. Specifically, they either commit or abort in all replicas,
which provides atomicity without blocking coordination. Global execution order is also used for
isolation: Calvin exploits a locking mechanism where tasks acquire locks on their shared state
portion in the agreed order. This requires knowing upfront the exact state portions accessed within
each job: when they cannot be statically determined, for instance due to state-dependent control
flow, Calvin runs reconnaissance jobs that perform all read accesses to determine the state portions
of interest. However, during concrete execution, shared state may have changed, and concrete
jobs may deviate from reconnaissance jobs and try to access different portions, in which case they
are deterministically restarted. Interestingly, Calvin provides the same strong semantics both for
cluster and for wide area deployments. The initial coordination increases the latency to schedule
a batch of jobs, affecting the response time of individual jobs in wide area deployments, but the
batching mechanisms can preserve throughput.

Explicit partitioning and replication strategies. VoltDB. VoltDB [88] is an in-memory relational
database developed from the HStore research project [87]. In VoltDB, clients register stored proce-
dures, which are driver programs written in Java and executed system side. They include multiple
jobs, are compiled on registration, and executed on invocation. Jobs can also write data to sinks.
The key idea of VoltDB is to let users control database partitioning and replication, so they can
optimize most frequently executed jobs. In particular, VoltDB preserves the same (transactional)
execution semantics as centralized databases while minimizing the overhead of concurrency control.
By default, all tasks that derive from a single driver program represent a transaction and are guar-
anteed to execute with group atomicity and isolation. In the worst case, this is achieved through
blocking coordination (two-phase commit for atomicity and timestamp-based concurrency control
for isolation). However, VoltDB avoids coordination for specific types of queries by exploiting
user-provided data about data partitioning and replication. Users can specify that a relational table
is partitioned based on the value of a column, for instance a Customer table may be partitioned by
region, meaning that all customers that belong to the same region (have the same value for the

16https://fauna.com

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:53

attribute region) are stored in the same shared state portion. Jobs that only refer to a given region
can then be executed by the single worker responsible for that data portion, sequentially, without
incurring expensive concurrency control overhead. Every table that is not partitioned is replicated
in every worker, which optimizes read access from any worker at the cost of replicating state
changes. Users need to select the best partitioning and replication schema to improve performance
for the most frequent jobs. VoltDB also supports replicating tables (including partitioned ones) for
fault tolerance. Replicas are kept up-to-date by propagating and executing tasks to all replicas, under
the assumption that tasks are deterministic. VoltDB is designed for cluster deployment as clock
synchronization and low latency are necessary to guarantee that timestamp-based concurrency
control and replication work well in practice. However, VoltDB also provides a hybrid deployment
model where a database is fully replicated at multiple geographical regions. These replicas can
be used only as an additional form of fault tolerance (with asynchronous propagation of state)
or can serve local clients. In this case, consistency across regions is not guaranteed, and conflicts
get resolved using predefined automatic rules. VoltDB supports manual reconfiguration of both
partitioning and workers, but requires stopping and restarting the system.

Primary-based protocols. Aurora. Aurora [94] is a relational database offered as a service by
Amazon. Aurora builds on two key design choices: (i) decouple the storage layer from the query
processing layer; (ii) store the log of changes (write ahead log) in the storage layer instead of the
actual shared state. Shared state is materialized only to improve read performance, and materi-
alization can be performed asynchronously without increasing write latency. The storage layer
(that is, the write log) is replicated both to improve read performance and for fault tolerance. To
ensure consistency, read and write operations use a quorum-based approach. The processing layer
accepts jobs from clients in the form of SQL queries, which are always executed within a single
worker. Specifically, to guarantee isolation, Aurora assumes that a single worker is responsible
for processing all read-write jobs at any given point in time. Read-only jobs can be executed on
any worker, which can read a consistent snapshot of the state without conflicting with concurrent
writes. In Aurora, the storage and processing layers can scale independently: the processing layer
is stateless, while the storage layer only needs to replicate the log.
Socrates. Socrates [12] is a relational database offered as a service in the Azure cloud platform
(under the name of SQL DB Hyperscale17). Like Aurora, Socrates decomposes the functionality of a
DMS and implements them as independent services. Its design goals include quick recovery from
failure and fast reconfiguration. To do so, it relies on four layers, each implemented as a service
that can scale out when needed. (1) Compute nodes handle jobs, including protocols for group
atomicity and isolation. There is one primary compute node that processes read-write jobs and an
arbitrary number of secondary nodes that handle read-only jobs and may become primary in the
case of failure. Compute nodes cache shared state pages in main memory and on SSD. (2) A log
service logs write requests with low latency. (3) A storage service periodically applies writes from
the log to store the shared state durably. (4) A backup service stores copies of the storage layer for
fault tolerance.

A.2.3 Objects stores. Tango. Tango [17] is a service for storing metadata. Application code (the
driver program) executes client-side and reads and accesses a shared state consisting of Tango
objects. Clients store their view of Tango objects locally in-memory, and this view is kept up
to date with respect to a distributed (partitioned) and durable (replicated) totally ordered log of
updates. Tango objects can contain references to other Tango objects, thus enabling the definition
of complex linked data structures such as trees or graphs. Although the log is physically partitioned

17https://docs.microsoft.com/en-us/azure/azure-sql/database/

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:54

Margara et al.

across multiple computers, all operations are globally ordered through sequence numbers, which
are obtained through a centralized sequencer service: the authors demonstrate that this service
does not become a bottleneck for the system when serving hundreds of thousands of requests per
second. Clients check if views are up-to-date before performing updates, thus ensuring totally
ordered, linearizable updates. Tango also guarantees group atomicity and isolation using the log
for optimistic concurrency control.

A.2.4 Graph stores. A1. A1 [25] is an in-memory database that resembles TAO and Trinity in
terms of data model (typed graphs) and jobs (changes to graph entities and graph pattern matching
queries). The distinguishing characteristic of A1 is that it builds on a distributed shared memory
abstraction that uses RDMA (remote direct memory access) implemented within network interface
cards [43]. A1 stores all the elements of the graph in a key-value store. Jobs may traverse the graph
and read and modify its associated data during execution. A1 provides strong consistency, atomicity,
and isolation using timestamp-based concurrency control. Fault tolerance is implemented using
synchronous replication in memory and asynchronous replication to disk. In the case of failures,
users can decide whether to recover the last available state or only state that is guaranteed to be
transactionally consistent.

B DATA PROCESSING SYSTEMS

Driver exec

Driver exec time

Invoc of jobs

Sources

Sinks

State

Deployment

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

sys
sys
sys
sys
configurable
configurable

sys
configurable
sys
client
client
client

Dataflow task deployment systems

reg
reg
reg
reg
reg
reg

sync
unknown
unknown
async
sync+async
async

passive
passive
passive
passive
passive
both

Dataflow job deployment systems

reg
reg
reg
reg
reg
reg

async
async
async
async
async
sync

both
both
both
both
both
both

Graph processing systems

yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes

no
no
no
no
no
yes

yes
yes
yes
yes
yes
yes

sys
sys
sys
sys
sys

yes
yes
yes
yes
yes
Table 14. Data processing systems: functional model.

unknown
unknown
unknown
unknown
unknown

passive
passive
passive
passive
passive

yes
yes
yes
yes
yes

reg
reg
reg
reg
reg

cluster
cluster
cluster
cluster
cluster
cluster

cluster
cluster
cluster
cluster
cluster
cluster

cluster
cluster
cluster
cluster
cluster

B.1 Task-level deployment

MapReduce. MapReduce [41] is a distributed processing model and system developed at Google
in the early 2000s, and later implemented in open source projects such as Apache Hadoop18. Its
programming and execution models represent a paradigm shift in distributed data processing
that influenced virtually all DPSs discussed in this paper: developers are forced to write jobs as a
sequence of functional transformations, avoiding by design the complexity and cost associated to
state management. In the specific case of MapReduce, jobs are constrained to only two processing
steps: (i) map transforms each input element into a set of key-value pairs, (ii) reduce aggregates
all values associated to a given key. Both functions are data parallel: developers specify the map
function for a single input element and the reduce function for a single key, and the system
automatically applies them in parallel. The data bus is implemented using a distributed filesystem.
The system schedules map tasks as close as possible to the physical location of their input in the

18https://hadoop.apache.org

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:55

Jobs def API

Exec plan
def

Task
comm

Exec plan
struct

Iter

Dyn
creat.

Nature
of jobs

State man.

Data par
API

Placem.
-aware API

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

lib
lib
lib
lib
lib (+DSLs)
lib (+DSLs)

expl
expl
expl
expl
expl (+impl)
expl (+impl)

MillWheel

lib

expl

impl
impl
impl
impl
impl
impl

impl

Dataflow task deployment systems

dataflow
dataflow
cycl. dataflow
dyn. dataflow
dataflow
dataflow

no
no
yes
no
no
no

no
no
no
yes
no
no

Dataflow job deployment systems

dataflow

no

Flink

Heron

lib (+DSLs)

expl (+impl)

impl

dataflow

limited

lib

expl

impl

dataflow

Kafka Streams

lib (+DSL)

expl (+impl)

impl

dataflow

Samza / Liquid

Timely dataflow

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

lib

lib

lib
lib
lib
lib
lib

expl

expl

expl
expl
expl
expl
expl

impl

impl

expl
expl
expl
expl
expl

dataflow

cycl. dataflow
Graph processing systems

yes

graph
graph
graph
graph
graph

yes
yes
yes
yes
yes

one-shot
one-shot
one-shot
one-shot
one-shot
cont

cont
one-shotB
contS

cont
one-shotB
contS

one-shotB
contS

cont

cont
cont
cont
cont
cont

absent
absent
absent
absent
absent
impl

impl

impl

impl

impl

impl

impl

expl
expl
expl
expl
expl

yes
yes
yes
yes
yes
yes

yes

yes

yes

yes

yes

yes

yes
yes
yes
yes
yes

no
no
no
no
no
no

no

no

no

no

no

no

yes
yes
yes
yes
yes

no

no

no

no

no

no

no
no
no
no
yes

no

no

no

Table 15. Data processing systems: jobs definition. Legend: B= in batch processing; S= in stream processing.

Jobs comp. time

Use static res info

Use dyn res info

Granul of depl

Depl time

Use dyn res info

Manag of res

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Heron
Kafka Streams
Samza / Liquid
Timely dataflow

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

reg
reg
reg
reg
reg
reg

reg
reg
reg
reg
reg
reg

no
no
no
no
no
no

yes
yes
yes
yes
yes
yes

yes
yes
yes
yes
yes
yes

Dataflow task deployment systems
task
task
task
task
task
task
Dataflow job deployment systems
job
job
job
job
job
job

no
no
no
no
no
no

Graoh processing systems

task activ
task activ
task activ
task activ
task activ
task activ

job compil
job compil
job compil
job compil
job compil
job compil

yes
yes
yes
yes
yes
yes

no
no
no
no
yes
no

reg
reg
reg
reg
reg

no
no
no
no
no
Table 16. Data processing systems: jobs compilation and execution.

job compil
task activ
task activ
job compil
task activ

job
task
task
job
task

yes
yes
yes
yes
yes

no
no
no
no
no

shared
sys-only
shared
sys-only
shared
shared

sys-only
sys-only
sys-only
sys-only
shared
sys-only

shared
sys-only
sys-only
sys-only
sys-only

filesystem. It then automatically redistributes intermediate results by key before scheduling the
subsequent reduce tasks. Fault detection is implemented using a master-slave approach. Tasks
that did not complete due to a failure are simply rescheduled. The same approach is used for
tasks that take long to complete (stragglers): they are scheduled multiple times if some workers
are available, to increase the probability of successful completion. Dynamic scheduling at the
granularity of tasks simplifies implementation of dynamic reconfiguration mechanisms to promote
elasticity. For instance, Hadoop supports scheduling policies based on user-defined quality of
service requirements, such as expected termination time: the scheduler tunes the use of resources
(possibly shared with other applications) to meet the expectation while minimizing the use of
resources. Several works build on top of MapReduce to offer a declarative language similar to SQL
to express analytical queries that are automatically translated into MapReduce jobs [2, 26].

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

Margara et al.

Bus partition

Bus repl

Bus inter

1:56

Elem struc

Temp elem

MapReduce

Dryad

HaLoop

CIEL

general

general

general

general

Spark
Spark Streaming

general+spec
general+spec

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

general
general+spec
general
general+spec
general
general

no

no

no

no

no
yes

yes
yes
yes
yes
yes
yes

Bus conn
Dataflow task deployment systems
mediated

Bus impl

distr fs

Bus persist

direct
or mediated

configurable

mediated

distr fs

direct
or mediated

configurable

distr fs (+cache)
distr fs (+cache)

mediated
mediated
Dataflow job deployment systems
direct
direct
direct
mediated
mediated
direct

RPC
net chan
net chan
kafka
kafka
net chan

Graph processing systems

persist

persist
or ephem

persist

persist
or ephem

persist
persist

ephem
ephem
ephem
persist
persist
ephem

yes

yes

yes

yes

yes
yes

yes
yes
yes
yes
yes
yes

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

typed graph
typed graph
typed graph
typed graph
typed graph

no
no
no
no
no

yes
yes
yes
yes
yes
Table 17. Data processing systems: data management.

net chan
mem
mem
net chan
net chan

ephem
persist
persist
ephem
ephem

direct
direct
direct
direct
direct

yes

hybrid

yes or no

push
or hybrid

yes

hybrid

yes or no

push
or hybrid

yes
yes

no
no
no
yes
yes
no

no
no
no
no
no

hybrid
hybrid

push
push
push
hybrid
hybrid
push

push
hybrid
hybrid
push
pull

Elem struct

Stor
medium

Stor
struct

Task st

Shared st

St part

Repl

Repl consist

Repl prot

Update
propag

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

Pregel
GraphLab
PowerGraph
Arabesque
GraphMiner

vertex
vertex/edge
vertex/edge
subgraph
subgraph

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

mem
mem
mem
mem
mem

Dataflow task deployment systems

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

no
no
no
no
no
yes

no
no
no
no
no
no

Dataflow job deployment systems

yes
noB/yesS
yes
noB/yesS
noB/yesS
yes

no
no
no
no
no
no

Graph processing systems

user-def
user-def
user-def
user-def
user-def

yes
yes
yes
yes
yes

no
no
no
no
no

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

Table 18. Data processing systems: state management. Legend: B= in batch processing; S= in stream processing.

Dryad. Several systems developed in parallel and after MapReduce inherit and extend the core
concepts in its programming and execution models. Dryad [51] generalizes the programming model,
representing jobs as arbitrary acyclic dataflow graphs. Like MapReduce, it uses a master-slave
approach, where a master schedules individual tasks (operators in the dataflow graph), but it
enables different types of channels (data bus in our model), including shared memory on the same
machine, TCP channels across machines, or distributed filesystems. The master is also responsible
for fault detection. Jobs are assumed to be deterministic, and in the case of failure the failing task
is re-executed: in the case of ephemeral channels, also upstream tasks in the dataflow graph are
re-executed to re-create the input for the failing task.

HaLoop. Systems like MapReduce and Dryad are constrained to acyclic job plans and cannot natively
support iterative algorithms. HaLoop [24] addresses this limitation with a modified version of
MapReduce that: (i) integrates iterative MapReduce jobs as first class programming concepts;
(ii) optimizes task scheduling by co-locating tasks that reuse the same data across iterations;

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:57

Aborts

Protocol

Assumptions

Level

Impl

Assumptions

Delivery

Nature of ts

Order

Dataflow task deployment systems

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
Dataflow job deployment systems
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

Graph processing systems

exact
exact
exact
exact
exact
exact

exact
exact
most/least/exact
least
least
exact

no
no
no
no
no
no or ingest

ingest
no or ingest
no
no or ingest
no or ingest
event

n.a.
n.a.
n.a.
n.a.
n.a.
always

always
always
n.a.
eventually
eventually
always

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
Table 19. Data processing systems: group atomicity, group isolation, delivery, order.

exact
exact
exact
exact
exact

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.

no
no
no
no
no

n.a.
n.a.
n.a.
n.a.
n.a.

Detection

Scope

Comput recov

State recov

Guarantees for state

Assumptions

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

mast-work
mast-work
mast-work
mast-work
mast-work
mast-work

mast-work
mast-work
mast-work
mast-work
mast-work
mast-work

mast-work
mast-work
mast-work
mast-work
mast-work

comput
comput
comput
comput
comput
comput+task st

Dataflow task deployment systems
n.a.
n.a.
n.a.
n.a.
n.a.
log+checkp

task
task
task
task
task
task

n.a.
n.a.
n.a.
n.a.
n.a.
valid or same

Dataflow job deployment systems

comput+task st
comput+task st
comput+task st
comput+task st
comput+task st
comput+task st

comput+task st
comput+task st
comput+task st
comput+task st
comput+task st

job
job
task
job
job
job

log+checkp
log+checkp
ack+checkp
log+checkp
log+checkp
log+checkp

same
valid or same
none or valid or same
valid or same
valid or same
same

Graph processing systems

job
job
job
job
tas

checkp
checkp
checkp
checkp
checkp

same
valid
valid or same
valid
valid

REPLAY
REPLAY
REPLAY
REPLAY
REPLAY
REPLAY

REPLAY
REPLAY
REPLAY
REPLAY
REPLAY
REPLAY

n.a.
n.a.
n.a.
n.a.
n.a.

Table 20. Data processing systems: fault tolerance. Legend: REPLAY = sources are replayable.

Goal

Automated

State migration

Task migration

Add/del slots

Restart

MapReduce
Dryad
HaLoop
CIEL
Spark
Spark Streaming

MillWheel
Flink
Storm
Kafka Streams
Samza / Liquid
Timely dataflow

elast
n.a.
elast
n.a.
elast
elast

load balan
elast
elast
elast
elast
n.a.

n.a.
n.a.
n.a.
n.a.
n.a.
yes

Dataflow task deployment systems
yes
n.a.
yes
n.a.
yes
yes
Dataflow job deployment systems
yes
yes
no
no
no
n.a.

yes
yes
yes
yes
yes
n.a.

Graph processing systems

n.a.
n.a.
n.a.
n.a.
n.a.
yes

yes
yes
yes
yes
yes
n.a.

yes
n.a.
n.a.
n.a.
yes
yes

yes
yes
yes
yes
yes
n.a.

Pregel
GraphLab
PowerGraph
Arabesque
G-Miner

load balan
load balan
load balan
load balan
load balan

yes
yes
yes
yes
yes
Table 21. Data processing systems: dynamic reconfiguration.

yes
yes
yes
yes
yes

yes
yes
yes
yes
yes

no
no
no
no
no

no
n.a.
no
n.a.
no
no

no
yes
yes
no
no
n.a.

no
no
no
no
no

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:58

Margara et al.

(iii) caches and indexes loop-invariant data to optimize access across iterations; (iv) caches and
indexes results across iterations to optimize the evaluation of fixed point conditions for termination.
CIEL. CIEL [71] extends the dataflow programming model of Dryad by allowing tasks to dynamically
create other tasks. This enables defining the job plan dynamically based on the results of data
computation, which can be used to implement iterative algorithms. The programming model
ensures that tasks cannot have cyclic dependencies, thus avoiding deadlocks in the execution. The
execution model and the fault tolerance mechanism remain identical to Dryad, the only exception
being that the list of tasks that the master schedules and tracks can dynamically change during the
execution, thus allowing the execution flow to be defined at runtime based on the actual content of
input data.
Spark. Spark [99] inherits the dataflow programming model of Dryad and supports iterative execu-
tion and data caching like HaLoop. (1) Spark jobs are split in sequences of operations that do not
alter data partitioning, called stages. Multiple tasks are scheduled for each stage, to implement data
parallelism. (2) The driver program can run either client-side or system-dice, and can dynamically
spawn new jobs based on the results collected from previous jobs. This enables data-dependent
control flow under the assumption that control flow conditions are evaluated in the driver program,
and is used to implement iterative algorithms. (3) As in HaLoop, intermediate results that are
re-used by the same or different jobs (as in the case of iterative computations) can be cached in
main memory to improve efficiency. Spark provides domain specific libraries and languages for
structured (relational) data [13], graphs [47], and machine learning computations [67], which often
include ad-hoc job optimizers. Spark also inherits the fault tolerance model of MapReduce: as tasks
are stateless, failing tasks are simply re-executed starting from their input data; if the input data is
not available anymore (for instance, in the case of intermediate results not persisted on any other
node), all tasks necessary to reconstruct the input are also re-executed. Task-level deployment also
enables runtime reconfiguration to provide elasticity.
Spark Streaming. Spark Streaming [98] implements streaming computations on top of Spark by
splitting the input stream into small batches and by running the same jobs for each batch. Spark
Streaming implements task state using native Spark features: the state of a task after a given
invocation is implicitly stored as a special data item that the task receives as input in the subsequent
invocation. This also enables reusing the fault tolerance mechanism of Spark to persist or recompute
state as any other data element. Another benefit of using the Spark system is simple integration of
static and streaming input data. The main drawback of the approach is latency: since input data
needs to be accumulated in batches before processing, Spark Streaming can only provide latency in
the range of seconds.

B.2 Job-level deployment

MillWheel. MillWheel [8] is a framework to build general-purpose and large-scale stream processing
systems. Jobs consist of data-parallel tasks expressed using an imperative language. As part of their
computation, tasks can use the MillWheel API to access (task local) state, logical time information,
and to produce data for downstream tasks. MillWheel implements the communication between
tasks (the data bus) using point-to-point remote procedure calls, and uses an external storage
service to persist task state and metadata about global progress (watermarks). Upstream tasks
are acknowledged when downstream tasks complete, and the storage service is kept consistent
with atomic updates of state and watermarks. In the case of failure, individual tasks can rely on
the external storage to restore a consistent view of their state, and they can discard duplicate
invocations from upstream tasks in the case some acknowledgements are lost. The same approach
is also used for dynamic load distribution and balancing.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:59

Flink. Flink [27] is a unified execution engine for batch and stream processing. In terms of pro-
gramming model, it strongly resembles Spark, with a core API to explicitly define job plans and
domain specific libraries for structural (relational) data, graph processing, and machine learning.
One notable difference involves iterative computations: Flink supports them with native operators
(within jobs) rather than controlling them from the driver program. Flink adopts job-level deploy-
ment and implements the data bus using direct TCP channels. This brings several implications.
(i) Tasks are always active and compete for workers resources. For each processing step in the
execution plan, Flink instantiates one (data parallel) task for each CPU core available on workers. In
practice, each CPU core receives one task for each step and scheduling is delegated to the operating
system that hosts the worker. (ii) In the case of stream processing, tasks can continuously exchange
data, which traverses the graph of computation leading to a pipelined execution. As there is no
need to accumulate input data in batches, this processing model may reduce latency. (iii) As tasks
cannot be deployed independently, fault tolerance requires restarting an entire job. In the case
of stream processing jobs, Flink takes periodic snapshots of task states: in the case of a failure, it
restores the last snapshot and replays all input data that was not part of the snapshot (assuming it
remains available). (iv) Dynamic reconfiguration builds on the same mechanism: when the number
of available slots changes, the system needs to restart and resume jobs from a recent snapshot.

Storm. Storm [92] and its successor Heron [55] are stream processing systems developed at Twitter.
They offer lower-level programming API than previously discussed dataflow systems, where
developers fully implement the logic of each processing step. At the time of writing both Storm
and Heron have experimental higher-level API that mimic the functional approach of Spark and
Flink. Storm and Heron adopt job-level deployment, and implement the data bus as direct network
channels between workers. They implement fault tolerance by acknowledging every message.
If a message is not acknowledged within a given timeout, the sender replays it, which leads
to at least once delivery (messages may be duplicated). The same applies in the case of state:
state is checkpointed, but may be modified more than once in the case of duplication. Dynamic
reconfiguration is possible, but required redeploying and restarting the entire job.

Kafka Streams. Kafka [54] is a distributed communication platform designed to scale in terms of
clients, data volume and production rate. Kafka offers logical communication channels named
topics: producers append immutable data to topics and consumers read data from topics. Topics are
persistent, which decouples data production and consumption times. Topics may be partitioned
to improve scalability: multiple consumers may read in parallel from different partitions, possibly
hosted on different physical nodes. Topics may also be (semi-synchronously) replicated for fault
tolerance. With respect to our model in Sec. 2, Kafka represents the implementation of a persistent
data bus that external clients can use to exchange immutable data. Kafka Streams [20] implements
batch and stream processing functionalities on top of Kafka. Its programming model is similar to
those of Spark and Flink, with core functional API and a higher-level domain specific language for
relational data processing (KSQL). As in Flink, all tasks for a job are instantiated and scheduled
when the job starts and continuously communicate in a pipelined fashion using Kafka as data
bus. Each channel in the job logical plan is implemented as a Kafka topic, and data parallelism
exploits topic partitioning, allowing multiple tasks to simultaneously read from different partitions.
Interestingly, Kafka Streams does not offer resource management functionalities but runs the driver
program and the tasks within clients: each job definition is associated with a unique identifier, and
clients can offer resources for a job (that is, become workers) by referring to the job identifier. The
policy for allocating tasks to slots is similar to that of Flink: each slot represents a physical CPU
core and receives one task for each computation step in the logical plan. Task state for streaming
jobs is stored on Kafka, following the same idea of persisting state as a special element in the data

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:60

Margara et al.

bus that we already found in Spark Streaming. Fault detection relies on Kafka ability to detect when
consumers disconnect. For fault recovery Kafka Streams adopts a two-phase commit protocol to
ensure that upon activation a task consumes its input, updates its state, and produces results for
downstream tasks atomically. In the case of failure, a task can resume from the input elements
that were not successfully processed, providing exactly once delivery of individual elements. We
still classify the system as offering at least once delivery, because, in the case of timestamped
elements, it does not implement any mechanism to process them in timestamp order, but retracts
and updates its results upon receiving elements out-of-order (resulting in visible changes at the
sinks). Storing both data and task state on Kafka enables for dynamic reconfiguration that involve
addition and removal of clients (workers) at runtime. The same approach of Kafka Streams is used
at LinkedIn in the Samza system [73], which is the core for the platform to integrate data from
multiple sources and offer a unique view to the back-end system, updated incrementally as new
data becomes available.

Timely dataflow. Timely dataflow [70] is a unified programming model for batch and stream
processing, which is lower-level and more general than the dataflow model of systems such as Flink.
In timely dataflow, jobs are expressed as a graph of (data parallel) operators and data elements
carry a logical timestamp that tracks global progress. Management of timestamps is explicit, and
developers control how operators handle and propagate them. This enables implementing various
execution strategies: for instance, developers may choose to complete a given computation step
before letting the subsequent one start (mimicking a batch processing strategy as implemented in
MapReduce or Spark), or they may allow overlapping of steps (as it happens in Storm or Flink).
The flexibility of the model allows for complex workflows, including streaming computations with
nested iterations, which are hard or even impossible to express in other systems. Timely dataflow
is currently implemented as a Rust library19: as in Kafka Streams, developers write a program
that defines the graph of computation using the library API, and run multiple instances of the
program, each of them representing a worker in our model. At runtime, the program instantiates
the concrete tasks, which communicate with each other either using shared memory (within one
worker) or TCP channels (across workers). Timely dataflow provides API to checkpoint task state
and to restore the last checkpoint for a job. Dynamic reconfiguration is currently not supported.

B.3 Graph processing

Pregel. Pregel [64] is a programming and execution model for computations on large-scale graph
data structures. Pregel jobs are iterative: developers provide a single function that encodes the
behavior of each vertex 𝑣 at each iteration. The function takes in input the current (local) state of 𝑣
and the set of messages produced for 𝑣 during the previous iteration; it outputs the new state of 𝑣
and a set of messages to be delivered to connected vertices, which will be evaluated during the
next iteration. The job terminates when vertices do not produce any message at a given iteration.
Vertices are partitioned across workers and each task is responsible for a given partition. Jobs are
continuous, as tasks are activated multiple times (once for each iteration) and store the vertex state
across activations (in their task state). Tasks only communicate by exchanging data (messages
between vertices) over the data bus, which is implemented as direct channels. One worker acts
as a master and is responsible for coordinating the iterations within the job and for detecting
possible failures of other workers. Workers persist their state (task state and input messages) at
each iteration: in the case of a failure, the computation restarts from the last completed iteration.
Several systems inherit and improve the original Pregel model in various ways: we discuss some

19https://github.com/TimelyDataflow

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:61

key variants through the systems that introduced them. The interested reader can find more details
and systems in the survey by McCune et al. [66].

GraphLab. GraphLab [61] abandons the synchronous model of Pregel, where all vertices execute an
iteration before any can move to the subsequent one. GraphLab schedules the execution of tasks
that update vertices. During execution, tasks can read the value of neighboring vertices and edges
(rather than receiving update messages, as in Pregel) and can update the value of outgoing edges.
We model this style of communication as a pull-based persistent data bus20. Tasks are scheduled and
execute asynchronously, without barriers between iterations. This paradigm is suitable for machine
learning and data mining computations that do not require synchronous execution for correctness
and can benefit from asynchronous executions for performance. GraphLab still supports some form
of synchronization between tasks: for instance, users can grant exclusive or non-exclusive access
to neighboring edges and vertices. GraphLab implements this synchronization constraints either
with a locking protocol or with scheduling policies that prevent execution of potentially conflicting
tasks.

PowerGraph. PowerGraph [46] observes that vertex-centric execution may lead to unbalanced
work in the (frequent) scenario of skewed graphs. It proposes a solution that splits each iteration
into three sub-steps: gather collects and combines data from adjacent vertices and edges, updates
the state of the local vertex, scatter updates the data on adjacent edges for the next iteration.
This way, the gather and the scatter tasks for all vertices can be distributed over all workers, and
executed in a MapReduce fashion. PowerGraph tasks can be executed synchronously, as in Pregel,
or asynchronously, as in GraphLab, depending on the specific problem at hand.

Sub-graph centric systems. Graph mining problems typically require retrieving sub-graphs with
given characteristics. A class of systems designed to tackle these problems uses a sub-graph centric
approach. We model these systems by considering the input graph as a static data source and by
storing the state of each sub-graph in task state. Arabesque [90] explores the graph in synchronous
rounds, it starts with candidate sub-graphs consisting of a single vertex and at each round it expands
the exploration by adding one neighboring vertex or edge to a candidate. G-Miner [33] spawns a
new task for each candidate sub-graph, allowing tasks to proceeed asynchronously. When scheduled
for execution, a task can update its (task) state. G-Miner supports dynamic load-balancing with
task stealing.

C OTHER SYSTEMS

Percolator
F1
Trinity

SDG
TensorFlow
Tangram
ReactDB

S-Store
SnappyData
StreamDB
Tspoon
Hologres

Driver exec

sys
client
client

sys
client
sys
sys

sys
sys
sys
configurable
client

Sinks

Sources

yes
yes
no

start
reg
reg

Driver exec time

active
both
no

reg
reg
reg
start

Invoc of jobs
Computations on data management systems
sync
sync+async
unknown
New programming models
sync
sync
sync
sync+async
Hybrid systems
sync+async
sync+async
async
sync+async
unknown
Table 22. Other systems: functional model.

active
both
active
active
both

active
passive
passive
no

reg
reg
start
reg
reg

yes
yes
yes
yes
yes

yes
yes
yes
no

State

Deployment

yes
yes
yes

yes
yes
no
yes

yes
yes
yes
yes
yes

cluster
cluster + wide
cluster

cluster
cluster
cluster
cluster (not impl)

cluster (not impl)
cluster
cluster
cluster
cluster

20Another way to model this communication paradigm is by saying that tasks have access to a global shared state representing
vertices and edges. However, this would not capture the strong connection between tasks and the vertices they update.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:62

Margara et al.

Jobs def API

Exec plan
def

Task
comm

Exec plan
struct

Iter

Dyn
creat.

Nature
of jobs

State man.

Data par
API

Placem.
-aware API

Computations on data management systems

Percolator

imperative
+BigTable jobs

F1

SQL

impl

impl

impl

impl

Trinity

crud+lib

impl+expl

impl + expl

SDG
TensorFlow
Tangram
ReactDB

imperative
lib
lib
lib

impl
impl
expl
impl+expl

impl
impl
impl
expl

task (put/get)

datafl+coord

task (put/get)
+graph

no

no

yes

New programming models
stateful datafl
stateful datafl
stateful datafl
workfl

yes
yes
yes
yes

Hybrid systems

no

no

no

no
yes
no
no

S-Store

lib+SQL

impl+expl

impl

workfl+datafl

yes

no

SnappyData

lib+SQL

impl+expl

StreamDB
Tspoon
Hologres

lib
lib
declarative DSL

expl
expl
impl

impl

impl
impl
impl

workfl+datafl

no

datafl
datafl
workfl

no
no
unknown

no

no
no
yes

one-shot

one-shot

one-shot

cont
one-shot
one-shot
one-shot

one-shotB
contS

one-shotB
contS

cont
cont
one-shot

expl

expl

expl

expl
expl
expl
expl

no

no

yes

yes
yes
yes
no

expl+impl

yes

expl

expl
impl
expl

yes

yes
yes
no

no

no

no

no
yes
no
no

no

yes

yes
no
no

Table 23. Other systems: jobs definition. Legend: B= in batch processing; S= in stream processing.

Jobs comp. time

Use static res info

Use dyn res info

Granul of depl

Depl time

Use dyn res info

Manag of res

Computations on data management systems

Percolator
F1
Trinity

SDG
TensorFlow
Tangram
ReactDB

S-Store
SnappyData
StreamDB
Tspoon
Hologres

reg
exec
exec

exec
exec
exec
reg

exec
exec
reg
exec
exec

yes
yes
yes

yes
yes
no
yes

yes
yes
yes
yes
yes

no
no
no

job
job
job

New programming models

no
no
yes
no
Hybrid systems
no
no
no
no
no

job
task
task
job

job
task
job
job
task

job compil
job compil
job compil

job compil
task activ
task activ
job compil

job compil
task activ
job compil
job compil
task activ

no
no
no

no
no
yes
no

no
yes
no
no
yes

shared
sys-only
sys-only

sys-only
sys-only
shared
sys-only

sys-only
sys-only
sys-only
sys-only
sys-only

Table 24. Other systems: jobs compilation and execution.

Elem struc

Temp elem

Bus conn

Bus impl

Bus persist

Bus partition

Bus repl

Bus inter

Percolator
F1
Trinity

wide-column
relational
typed graph

SDG
TensorFlow
Tangram
ReactDB

general
tensors
general
relational

S-Store

relational

SnappyData
StreamDB
Tspoon
Hologres

relational
relational
general
structural

yes
no
no

no
no
no
no

yes

no
no
yes
no

Computations on data management systems

direct
direct
direct

net chan + RPC
net chan
net chan

New programming models

direct
direct
mediated
direct

net chan
net chan
distr fs
net chan
Hybrid systems

ephem
ephem
ephem

ephem
ephem
persist
ephem

direct
+mediatedS

mediated
direct
direct
direct

net chan
+DBS

distr fs (+cache)
net chan
net chan
net chan

ephem
+persistS

persist
ephem
ephem
ephem

yes
yes
yes

yes
yes
yes
yes

yes

yes
yes
yes
yes

no
no
no

no
no
yes
no

pull
push
push

push
push
hybrid
push

yes + no

push
or hybrid

yes
no
no
no

hybrid
push
push
pull

Table 25. Other systems: data management. Legend: S= in stream processing.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:63

Elem struct

Stor
medium

Stor
struct

Task st

Shared st

St part

Repl

Repl consist

Repl prot

n.a.
strong
weak

mem
mem
mem
mem

hybrid
service
hybrid

backup
yes
yes

LSM trees
Spanner
map

wide-col
relational
typed graph

general
general
general
relational

Computations on data management systems
yes
yes
no
yes
yes
no
no
yes
yes
New programming models
yes
yes
no
yes
yes
no
yes
yes
no
yes
yes
no
Hybrid systems
relational
yes
relational
yes
relational
yes
n.a.
no
structural
yes
Table 26. Other systems: state management. Legend: S= in stream processing.

unknown
key-val
unkwnon
n.a.
LSM tree

user-def
user-def
key-val
unknown

n.a.
backup
yes
n.a.
no

mem
mem
mem
n.a.
service

n.a.
n.a.
strong
n.a.
n.a.

n.a.
n.a.
no readable st
n.a.
n.a.

yesS
yesS
no
yes
no

yes
yes
yes
n.a.
yes

n.a.
leader
leader

n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.

no
no
no
no

Update
propag

n.a.
op
unknown

n.a.
n.a.
n.a.
n.a.

n.a.
n.a.
unknown
n.a.
n.a.

Aborts

Protocol

Assumptions

Level

Impl

Assumptions

Delivery

Nature of ts

Order

Computations on data management systems

sys+job
sys+job
n.a.

n.a.
n.a.
n.a.
sys+job

blocking
blocking
n.a.

n.a.
n.a.
n.a.
blocking

n.a.
none
n.a.

n.a.
n.a.
n.a.
none

blocking
blocking
blocking

ts
lock+ts
SEQ
New programming models
n.a.
barrier
n.a.
ts

n.a.
config
n.a.
blocking

sys+job
sys+job
job
sys+job(task)
sys+job

blocking
blocking
coord free
blocking
blocking

DC
none
dataflow
none
none

Hybrid systems

blocking
blocking
coord free
conf
n.a

ts
lock+ts
ts
conf
n.a.

none
none
1P

n.a.
n.a.
n.a.
none

DC
none
dataflow
none
n.a.

exact
exact
most

exact
exact
exact
exact

exact
exact
exact
exact
exact

event
no
no

no
no
no
no

n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
n.a.

ingest
no or ingest
no
no or ingest
no

always
always
n.a.
always
n.a.

Percolator
F1
Trinity

SDG
TensorFlow
Tangram
ReactDB

S-Store
SnappyData
StreamDB
Tspoon
Hologres

Percolator
F1
Trinity

SDG
TensorFlow
Tangram
ReactDB

S-Store
SnappyData
StreamDB
Tspoon
Hologres

Table 27. Other systems: group atomicity, group isolation, delivery, order. Legend: DC = jobs are deterministic;
SEQ = jobs are executed sequentially, with no interleaving; 1P = jobs access a single state portion.

Scope

Detection

State recov

Assumptions

Guarantees for state

Percolator
F1
Trinity

mast-work
mast-work
p2p

SDG
TensorFlow
Tangram
ReactDB

shared st
shared st
comput+shared st

Comput recov
Computations on data management systesms
log+repl
n.a.
log+checkp+repl
n.a.
job
checkp+repl
New programming models
job
n.a.
task
n.a.
Hybrid systems
job
task
n.a.
job
unknown
Table 28. Other systems: fault tolerance. Legend: STOR = storage layer is durable; REPLAY = sources are
replayable.

comput+shared st
comput+task st+shared st
n.a.
comput+task st
shared st

comput+task st
shared st
comput+shared st
n.a.

valid or same
valid or same
n.a.
valid or same
same

log+checkp
checkp+repl
n.a.
log+checkp
log

S-Store
SnappyData
StreamDB
Tspoon
Hologres

mast-work
p2p
n.a.
mast-work
mast-work

log+checkp
checkp
checkp
n.a.

mast-work
n.a.
mast-work
n.a.

REPLAY
REPLAY
n.a.
REPLAY
STOR

REPLAY
STOR
REPLAY
n.a.

valid
valid
valid
n.a.

STOR
STOR
none

same
same
none

C.1 Computations on data management systems

Percolator. Percolator [75] builds on top of BigTable and is used to automatically and incrementally
maintain views when BigTable gets updated. For instance, it is used within Google to incrementally
maintain the indexes of its search engines as Web pages and links change. Percolator enables
developers to register driver programs within the system, which are invoked when a given BigTable
column changes. Each driver program is executed on a single process server-side, and can start
multiple jobs that read and modify BigTable columns. All these jobs are executed as a group ensuring

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:64

Margara et al.

Goal

Automated

State migration

Task migration

Add/del slots

Restart

Computations on data management systems

Percolator
F1
Trinity

load balan
change schema+load balan
avail

yes
yes
yes

yes
yes
yes

SDG
TensorFlow
Tangram
ReactDB

S-Store
SnappyData
StreamDB
Tspoon
Hologres

elast
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
elast
load balan+elast

New programming models

yes
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
no
yes

yes
n.a.
n.a.
n.a.

Hybrid systems

n.a.
n.a.
n.a.
yes
yes

n.a.
no
yes

yes
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
yes
yes

yes
yes
unknown

yes
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
yes
yes

no
no
no

no
n.a.
n.a.
n.a.

n.a.
n.a.
n.a.
yes
no

Table 29. Other systems: dynamic reconfiguration.

group atomicity through two-phase commit and group isolation (snapshot isolation) through a
timestamps. It relies on an external service to obtain valid timestamps to interact with BigTable, and
saves metadata about running transactions on additional BigTable columns. Changes performed
during the execution of a driver program may trigger the execution of other driver programs.
However, these executions are independent, and atomicity and isolation are not guaranteed across
them.

F1. F1 [82] builds a relational database on top of the storage, replication, and transactional features
of Spanner. F1 inherits all features of Spanner and adds distributed SQL query evaluation, support
for external data sources and sinks, and optimistic transactions. F1 converts SQL queries into a plan
that can be either fully executed on a single coordinator worker, or include dataflow sub-plans that
are executed on multiple workers and managed by the coordinator. This execution mode mimics
dataflow DPSs. To optimize read-intensive, analytical jobs, F1 introduces optimistic transactions.
They are split into two phases, the first one reads all data needed for processing, the second one
attempts to write results. The read phase does not block any other transaction, and so it can be
arbitrary long (as in the case of complex data analytics). The subsequent write phase will complete
only if no conflicting updates from other transactions occurred during the read phase.

Trinity. Trinity [79] is a graph data store developed at Microsoft with similar characteristics as
TAO in terms of data model (typed graphs), storage model (in-memory key-value store backed up
in a shared distributed file system), and guarantees (weak consistency without transactions). The
main distinguishing feature of Trinity is the ability to perform more complex computations on
graphs, including those that require traversing multiple hops of the graph (such as graph pattern
matching) and even iterative analytical jobs (such as vertex-centric computations, as introduced
by Pregel [64]). To support these computations, Trinity lets users define different communication
protocols that govern data exchange over the data bus during job execution. For instance, data
may be buffered and aggregated at sender side or at receiver side. For fault tolerance, Trinity stores
the association between shared state portions and workers on the distributed file system, and
updates it in the case of failure. Interestingly, it also uses checkpoints within long lasting iterative
computations to resume them in the case of failure.

C.2 New programming models

Stateful dataflow. SDG. Stateful dataflow graphs (SDG) [45] is a programming model that
C.2.1
extracts a dataflow graph of computation from imperative code (Java programs). In SDG, developers
write driver programs that include mutable state and methods to access and modify it. Code
annotations are used to specify state access patterns within methods. The program executor (a
client-side compiler) analyzes the program to extract state elements and task elements, representing

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:65

shared state and data-parallel tasks in our model. If possible, state elements are partitioned across
workers. Similarly, task elements are converted into multiple concrete tasks, each accessing one
single state element from the shared state portion of the worker it is deployed on. For instance,
consider a program including a matrix of numbers and two methods to update a value in the matrix
and to return the sum of a row. The matrix would be converted into a state element partitioned
by row (since both methods can work on individual rows). Each method would be converted a
data-parallel task, with one instance of the task per matrix partition. State elements that cannot
be partitioned are replicated in each worker, and the programming model supports user-defined
functions to merge changes applied to different replicas. In terms of execution, SDGs are similar
to stream processing systems such as Storm or Flink: jobs are continuous and tasks communicate
through direct TCP channels. SDGs rely on periodic snapshots and re-execution for fault tolerance:
the same mechanism is adopted to dynamically scale in and out individual task elements depending
on the input load they receive [29].

TensorFlow. TensorFlow [1] is a system for large-scale machine learning that extends the dataflow
model with explicit shared mutable state. Jobs represent machine learning models and include
operations (data transformations) and variables (shared mutable state elements representing the
parameters of the machine learning model). Frequently, jobs are iterative and update variables at
each iteration. The specific application scenario does not require strong consistency guarantees for
accessing shared state, so tasks are allowed to execute and read/write variables asynchronously. If
needed, TensorFlow permits some form of barrier synchronization, for instance to guarantee that
all tasks perform an iteration step using a given value of variables before they get updated with the
results of that step. TensorFlow tasks can be executed on heterogeneous devices (e.g., hardware
accelerators) and users can express explicit placement constraints. Part of the dataflow plan may be
defined at runtime, meaning that tasks can dynamically define and spawn downstream tasks based
on the input data. Variables can be periodically checkpointed to durable storage for faul-tolerance.
In the case of failure, workers can be restarted and they restore the latest checkpoing available,
with no further consistency guarantees.

Tangram. Tangram [50] is a data processing framework that extends the dataflow model with explicit
shared mutable state. It implements task-based deployment but allows tasks to access and update
an in-memory key-value store as part of their execution, which enables optimizing algorithms that
benefit from fine-grain updates of intermediate states of computations (e.g., iterative algorithms or
graph processing algorithms). By analyzing the execution plan, Tangram can understand which
parts of the computation depend on mutable state and which parts do not, and optimizes fault
tolerance for the job at hand. Immutable data is recomputed using the same (lineage) approach
of MapReduce and Spark, thus re-executing only tasks that are necessary to rebuild the data.
Mutable state is periodically checkpointed. In general, Tangram does not provide group atomicity
or isolation for state: simultaneous accesses to the same state portions from multiple tasks may be
executed in any order.

C.2.2 Relational actors. ReactDB. ReactDB [78] extends the actor-based programming model [6]
with database concepts such as relational tables, declarative queries, and transactional execution
semantics. ReactDB builds on the abstraction of reactors, which are logical actors that embed state
in the form of relational tables. Each reactor can query its internal state using a declarative language
(SQL) or can explicitly invoke other reactors. Invocations across reactors are asynchronous and
retain transactional semantics: clients invoke a root reactor and all invocations it makes belong
to the same root-level transaction, and are atomic and isolated. The core idea of ReactDB is that
developers control data partitioning across reactors and distributed execution: on one extreme,

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

1:66

Margara et al.

they can place all state in a single reactor and mimic a centralized database; on the other extreme,
they can assign a single table (or a single partition of a table) to each reactor, which maximizes
distributed execution. With this model, the execution plan is partly implicit (within a single reactor)
and partly explicit (calls between reactors). ReactDB is currently a research prototype. As such,
it lacks a distributed implementation, replication, fault tolerance, and dynamic reconfiguration
mechanisms.

C.3 Hybrid systems

S-Store. S-Store [30] integrates stream processing capabilities withing a transactional database
system. It builds on H-Store [87], the research prototype that later evolved into VoltDB [88], and
adopts the same approach to implement transactional guarantees with limited overhead. It extends
H-Store by enabling stream processing jobs, represented as a dataflow graph of tasks that may
access local task state as part of their processing. S-Store exploits the database state to implement
the shared state (visible to all tasks), the task state (visible only to individual tasks of stream
processing jobs), and the data bus (that stream processing tasks use to exchange data streams).
Input data (for streaming jobs) and transaction invocations (for data management jobs) are handled
by the same engine, which schedules task execution ensuring that dataflow order is preserved
for stream processing jobs. S-Store supports two fault tolerance mechanisms, one ensuring same
state recovery through a command log and a periodic snapshot, and one ensuring valid state by
replaying streaming data. While the S-Store prototype is not distributed, we included it in the
survey for its original integration of data management and data stream processing and because its
core concepts can easily lead to a distributed implementation.

SnappyData. SnappyData [69] aims to unify data processing abstractions (both for static and for
streaming data) with mutable shared state. To do so, it builds on Spark and Spark Streaming as
job execution engines, but extends them to enable writing to a distributed key-value store. Users
write jobs as declarative SQL queries. SnappyData analyzes jobs and classifies them as lightweight
(transactional) or heavy (analytical): in the first case, it directly interacts with the underlying key-
value store, while in the latter case it compiles them into an Spark dataflow execution plan. Based on
the application at hand, users can decide how to store shared state (for example, in row or in column
format) and how to partition and replicate it, and how to associate shared state portions to workers,
to maximize co-location of data that is frequently accessed together. Interestingly, SnappyData
also supports probabilistic data and query models that may sacrifice precision to reduce latency.
SnappyData supports group atomicity and group isolation (up to the repeatable-read isolation
model) using two-phase commit and multiversion concurrency control. Fault detection is performed
in a distributed manner, to avoid single points of failures. Fault recovery is based on replication of
the key-value store, which is also used to persist the checkpoints of the data used by Spark during
long-running one-shot and continuous jobs.

StreamDB. StreamDB [34] integrates shared state and transactional semantics within a distributed
stream processing system. From stream processing systems, StreamDB inherits a dataflow execution
plan with job-level deployment. As in data management systems, tasks have access to a shared
state, which represents relational tables that can be replicated and partitioned horizontally and/or
vertically across workers. Each workers are responsible for reading and updating their portion of
the shared state. Input requests represent invocations of jobs: they are timestamped when received
by the system and each worker executes tasks from multiple jobs in timestamp order, thus ensuring
that jobs are executed in a sequential order without using explicit locks (group isolation). The
results of a job are provided to sinks that expressed interest in that job. StreamDB is a research
prototype and currently requires developers to explicitly define how to partition the shared state

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

A Model and Survey of Distributed Data-Intensive Systems

1:67

and to write the dataflow execution plan for all the jobs. It lacks fault tolerance and reconfiguration
mechanisms.
TSpoon. Like StreamDB, TSpoon [5] also integrates data management capabilities within a dis-
tributed stream processing system. Unlike StreamDB, it does not provide a shared state. Instead, it
builds on the programming and execution models of Flink and enriches them with (i) the possibility
to read (query) task state on demand; (ii) transactional guarantees in the access to task state. TSpoon
considers each input data element as a notification of some change occurred in the environment
in which the system operates. Developers can identify portions of the dataflow graph (denoted
as transactional subgraphs) that need to be read and modified in a consistent way, meaning that
each change should be reflected in all task states or none (atomicity) and the effects of changes
should not overlap in unexpected ways (isolation). TSpoon implements atomicity and isolation
by decorating the dataflow graph with additional operators that act as transaction managers. It
supports atomicity under the assumption that jobs abort either due to a system failure or due to
some inconsistency during the update to the state of individual tasks. It supports different levels of
isolation (from read committed to serializable) with different tradeoffs between guarantees and
runtime overhead. It supports different isolation protocols, both based on locks and on timestamps.
Hologres. Hologres [52] is a system developed at Alibaba to integrate analytical (computationally
expensive) and interactive (lightweight) jobs. The system is designed to support high volume data
ingestion from external sources, continuously compute derived information, store it into a shared
state, and make it available to external sinks. Hologres uses a modular approach, where the storage
layer is decoupled from the processing layer and delegated to external services (e.g., a distributed
file system). It adopts a master-worker approach: the shared state is partitioned across workers
and each worker stores a log of updates for the partition it is responsible for and an in-memory
store that is periodically flushed on the durable storage service. Hologres supports a structured
data model, where data is organized into tables that can be stored row-wise or column-wise or
both depending on the access pattern, element by element rather or range scan/aggregation. A
distinctive feature of the system is its scheduling mechanism. Each job is decomposed into tasks
and jobs execution is orchestrated by a coordinator, which assigns tasks to workers based on their
current load and their priority: for instance analytical tasks may be assigned a lower priority to
guarantee low response time for interactive queries. Hologres supports group atomicity through a
two-phase commit protocol but does not support group isolation. Replication of the shared state
is not currently implemented. Fault tolerance relies on logging and checkpointing and assumes
that the storage layer to be durable. Dynamic reconfiguration is a design concern, and includes
migration of shared state portions but also reconfiguration of execution slots and priorities for load
balancing.

ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: March 2022.

