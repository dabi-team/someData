ADAMANT: A Pipeline for Adaptable Manipulation Tasks

Ana Huam´an Quispe∗

Stephen Hart

Seth Gee

Robert R. Burridge

2
2
0
2

p
e
S
4
1

]

O
R
.
s
c
[

1
v
8
8
8
6
0
.
9
0
2
2
:
v
i
X
r
a

Abstract— This paper presents ADAMANT, a set of software
modules that provides grasp planning capabilities to an existing
robot planning and control software framework. Our presented
work allows a user to adapt a manipulation task to be used
under widely different scenarios with minimal user input, thus
reducing the operator’s cognitive load. The developed tools
include (1) plugin-based components that make it easy to extend
default capabilities and to use third-party grasp libraries, (2)
An object-centric way to deﬁne task constraints, (3) A user-
friendly Rviz interface to use the grasp planner utilities, and
(4) Interactive tools to use perception data to program a task.
We tested our framework on a wide variety of robot simulations.

I. INTRODUCTION

Robot manipulation applications continue to push the
boundaries towards more unstructured and unpredictable
environments. Robots nowadays can be found in a wide
range of scenarios, from warehouse ﬂoors, such as Stretch
[1], to hospital corridors, such as Moxi [2]; and for diverse
applications, from humanoids in search and rescue situations
[3], to helpful caretakers for space missions [4].

Depending on the application, input from a human opera-
tor might be needed, requiring cognitive load of different de-
grees. For applications where the robotic system is expected
to operate mostly autonomously, the user’s role is limited
to being supervisory in nature. On the other hand, tasks
that might involve unpredictable variables—such as changing
environments or manipulating novel objects—might require
increased user input. Imagine, for instance, an example such
as the one shown in Figure 1. A Sawyer robot performs a
pick-and-place task to transport a glass. As the goal pose of
the object changes, the robot must choose different grasps;
e.g., for placing the glass inside the box the grasp has to be
from above, whereas to place the glass in the cabinet, the
grasp has to come from the right. Having multiple grasp
options thus provides a robot with ﬂexibility to adapt to
changing scenarios.

The work presented in this paper focuses on the de-
velopment of a grasp planning software framework called
ADAMANT (Adaptable Manipulation for Tasks) that allows
a user to select a grasp from a set of candidates calculated
on the ﬂy by a grasp synthesis module. These candidates
are visually presented via a user-friendly interface, which

All authors are with TRACLabs, Inc. 16969 N. Texas Ave. Suite 300,

Webster, TX. 77598. ∗Corresponding author: ana@traclabs.com.

This work was supported by NASA contracts 80NSSC18P2203 and

80NSSC19C0216.

Preprint. Under review

(a) Grasp from the left

(b) Grasp from above

(c) Grasp from the right

Fig. 1.
with varying environmental constraints.

Sawyer robot using 3 different grasps for 3 pick and place tasks

allows the user to quickly go through all the kinematically-
feasible alternatives. Furthermore, the candidates are ordered
such that the ﬁrst grasps shown are the ones that maximize
a user-deﬁned metric, allowing the user to see the most
promising grasps ﬁrst. All our tools are designed as plugins,
which permits extensibility to use third-party libraries that
address grasp generation and grasp metric implementations.
Finally, our framework uses an object-centric task description
language that greatly facilitates robot- (and object-) agnostic
development.

Our main goal with this work was to reduce the cognitive
load for a human operating a robot performing manipulation

 
 
 
 
 
 
tasks under variable environmental conditions. We present
several results in kinematic simulations that showcase the
software tools developed being utilized with a variety of
robots, objects, and tasks with varying degrees of complexity.
The rest of this paper is organized as follows. Section II
describes related work on currently available grasp planning
frameworks, as well as tools for grasp synthesis and eval-
uation. Section III succinctly deﬁnes the object-centric task
description we use as input to our grasp planning modules.
Section IV presents the architecture of our developed soft-
ware and its integration to our existing software framework
for robot planning and control. Section V presents further
details on the UI interface for grasp generation using Rviz
and a widget for easily capturing RGB-D sensor data and
incorporate it into the grasp planning pipeline. Section VI
shows some of the demonstrations performed in a variety of
robot kinematic simulations. Finally, Section VII summarizes
our conclusions as well as proposed future work.

II. RELATED WORK

A. Grasp Planning

Work on grasp planning is vast and covers a variety of
approaches that have dramatically evolved as robot hardware
and computational resources have become more accessible.
Classical approaches primarily consisted of the generation
of robot hand poses and the use of analytic metrics, such as
the force-closure metric (cid:15) proposed by Ferrari and Canny [5]
to determine whether the grasp was valid or not. Hand and
ﬁnger poses have been generated by a range of strategies,
including optimization approaches [6], surface sampling [7],
primitive shape decomposition [8], [9], [10] and heuristic
approaches [11], [12]. During the last few years, the advances
of deep learning, and particularly, its application on data such
as point clouds, have produced a number of approaches, such
as 6DOF-Graspnet [13] and derived approaches (e.g., [14])
that use the Pointnet++ architecture [15].

Most of the aforementioned approaches focus on generat-
ing a grasp for, primarily, just grasping the object. That is,
picking it up from its original location and guaranteeing that
it can withstand some mild perturbation. While this is useful
for simple transport tasks, it is not sufﬁcient for tasks that
consist of more than one step (e.g., rotating a wheel, opening
a door, pouring a drink, painting on a planar surface).
MoveIt! [16], a very popular robot planning framework,
provides Rviz tools that allow a user to deﬁne tasks by means
of their Task Constructor module, and generate grasps using
third-party libraries such as GPD and DexNet. While our
presented framework offers similar capabilities to those of
MoveIt!, it also presents some features that make it more
extensible (Section IV-A), and allows a more interactive
way for a user to be involved in the grasp planning process
(Section V). Furthermore, the task description language we
use is fairly simple (Section II-C) and is easily editable by
the user either manually (directly modifying a single JSON
ﬁle) or interactively, within the Rviz 3D window.

B. CRAFTSMAN

Over the past 7 years, TRACLabs researchers have de-
veloped a powerful suite of robot planning, control, and
tasking software called CRAFTSMAN (Figure 2). CRAFTS-
MAN is designed to be robot agnostic, providing generic
collision-free path planning, trajectory optimization, action-
sequencing capabilities, and methods for parameterizing,
encoding, and visualizing task descriptions. One innovation
of CRAFTSMAN is that it enables users to create robot-
independent
task descriptions that can be re-used across
deployments and can endow robot systems with a set of core
competencies. This has allowed TRACLabs to successfully
demonstrate CRAFTSMAN components on a variety of
robotic platforms [17], [3].

Fig. 2. A high-level overview of the commercially-deployed CRAFTS-
MAN framework, which has been demonstrated on a variety of industrial
manipulators, research robots, and humanoid platforms.

CRAFTSMAN provides a straightforward tool suite for
both expert and non-expert developers [18]. The current
software implementation uses libraries from the Robot Op-
erating System (ROS) ecosystem,
including inter-process
messaging and Rviz 3D visualization tools [19], providing
easy integration with custom or off-the-shelf perception and
navigation packages for systems that have 3D sensing and/or
mobile capabilities. In particular, CRAFTSMAN contains
an implementation of a graphical task description language
called the Affordance Template framework [20], [21] that
greatly simpliﬁes task design.

C. The AT Task Description Language

The Affordance Template (AT) speciﬁcation is a task
description language that provides deﬁnitions for tasks that
can be used by a variety of robots in a variety of contexts.
Speciﬁcally, ATs allow a programmer to specify sequences
of navigation and end-effector waypoints represented in the
coordinate systems of stationary or manipulable objects in
the environment, including the robot itself when necessary.
Virtual overlays of desired objects and task-speciﬁc end-
effector waypoints can be visualized and adjusted in a
3D GUI, which also contains sensory data and related
environmental models. Waypoints can include conditioning
information, such as the type of path planning to use or
the style of manipulation motion to prefer (e.g., straight-line
Cartesian versus joint motion). Waypoints can also specify
attached objects, so that grasped objects will be accounted for
automatically when checking for collisions during planning.
Figure 3 shows an example where an operator registers
an open-door affordance template to a robot’s 3D sensor

Nav wp

(a)

(b)

(c)

(d)

(e)

Fig. 3. Affordance Template usage example: Performing a mobile manipulation task to approach and open a door. (a) Start conﬁguration. (b) User aligned
AT with sensor data (moved door handle IM marker -highlighted with green- to match cloud). (c) Robot navigated towards the Navigation waypoint IM,
EE waypoints (closeup in top-right corner) are now reachable, (d) Robot opened the door moving its hand through the EE waypoints, (e) View of real
robot after (d).

data. The lower levels of the CRAFTSMAN framework then
use the updated AT waypoints to plan arm and (if required)
navigation trajectories.

Although CRAFTSMAN has been used successfully in
a variety of robot applications through the years, it lacks
autonomous grasp planning capabilities. For instance, for
the example shown, the hand pose that allows the robot to
grasp the door was crafted by the user, with the help of the
Affordance Template Rviz interactive tools. The goal of the
ADAMANT modules was to provide CRAFTSMAN with
those grasping capabilities. The software modules we devel-
oped and present in the rest of this paper allow CRAFTS-
MAN users to use a grasp planning module to generate
grasps automatically,
thus making the task of designing
manipulation tasks easier for a developer.

III. TASK REPRESENTATION

In this section, we describe the low-level task represen-
tation we utilize to represent a robot manipulation problem
within the grasp planning module, speciﬁcally, its input (task
description) and output (candidate grasps):

# Minimal task description
string ee_group
CollisionObjectInfo object

#Steps
geometry_msgs/PoseStamped[] steps
geometry_msgs/Vector3[] tol_pos
geometry_msgs/Vector3[] tol_rot

# Arm start pose
sensor_msgs/JointState start_arm_config

Fig. 4. FullTaskDescription.msg

generated and a task contains multiple steps. Finally, note
that this Task Description representation is used internally
by the grasp planning module. The user only has to deal
with the Affordance Template default representation. Any
required transformation between message types (e.g. AT
Object Waypoints into Task steps) is done in the background.

A. Input: Task Description

(a) Pour task steps

(b) Paint task steps

We represent a manipulation problem with a custom ROS
message named FullTaskDescription, shown in Fig.
4. It contains:

• MinimalTaskDescription: The geometric information of
the object to be manipulated and the name of the end
effector to be used.

• Task steps: Discrete 3D poses (with corresponding
tolerances) that represent the object pose at relevant
points. Examples of those are shown in Fig. 5. The
tolerances are expressed with respect to the object’s
frames of reference.

Note that our task description (for grasp planning pur-
poses) does not include information about path constraints,
if any; this is due to the fact that while grasp planning is
expected to return grasps that are kinematically reachable
is not expected to perform motion
at each task step,
planning for moving the arm between steps, as this might
be too time-consuming, especially if dozens of grasps are

it

Fig. 5. Visualization of object-centric task step deﬁnition (Object Way-
points). These are Rviz IMs that allow a user to edit them interactively
(right-click to make 6D gimbal visible).

B. Output: Candidate grasp set

The output, or solution to a robot manipulation problem
is a set of candidate grasps. A grasp is deﬁned as (1) the

pose of the hand’s TCP (tool center point) with respect to the
object’s frame, (2) the ﬁnger joint conﬁguration that positions
the ﬁngers in contact with the object, and (3) the name of the
end effector. Figure 6 shows some of the candidate grasps
generated for a painting task.

(a) Grasp 1

(b) Grasp 2

(c) Grasp 3

(d) Grasp 4

Fig. 6. Visualization of output (some of the grasp candidates used to update
AT End Effector Waypoints). The magenta markers represent the pose of
the end effector with respect to the target object. The cyan color indicates
that the grasp at this step is only reachable by considering the tolerances

IV. ARCHITECTURE

The ADAMANT structure is shown in Figure 7. The
most important component of these modules is the Grasp
Planner, which we explain in some more detail as follows.

Fig. 7. Diagram of the ADAMANT Grasp Planning components.

hand’s information to generate a set of candidate grasps
that allow the robot hand to close around the object and
have at least two points of contact with it.

2) Grasp Filter: This module receives the candidate
grasps produced by the Grasp Generator and only
keeps the candidates that are kinematically feasible
the manipulation task steps with
to execute at all
tolerances.

3) Grasp Evaluator: This module is fed with the surviv-
ing grasps from the Grasp Filter and applies a function
which calculates a heuristic measure of how adequate
a grasp is for a speciﬁc task. This metric could consider
factors such as kinematic reachability (how easy it is
for an arm to reach the end-effector poses at relevant
steps in the task) and grasp suitability (how robust a
grasp is).

The output of this 3-step process is an ordered list of can-
didate grasps that can now be returned to the operator, who
can visualize them and select one of them, or alternatively,
can rely on the grasp metric ordering and automatically
choose the ﬁrst grasp in the list.

The 3 modules described are implemented using the ROS
C++ pluginlib library, which allows the user ample ﬂex-
ibility to create instances of one or all of the 3 grasp planner
components and seamlessly plug them in the grasp planner.
This approach allows us to experiment with different grasp
generation algorithms as well as diverse grasp metrics, either
by encoding new grasp algorithms in these instances, or if
available, taking advantage of existing third-party libraries
available in the community by creating a new plugin for
them. We developed a number of plugins, including the
following:

• Default Grasp Generator: This plugin generates can-
didate grasps by uniformly sampling the surface of the
object and using the normals as the candidate grasp
approach directions. The ﬁngers are then moved small
steps from their open to their close pose, each link stops
moving if a contact is made.

• GPD Grasp Generatorn: This plugin implements a
wrapper around the popular Grasp Pose Detection li-
brary [22] (with OpenVINO extensions) to generate
grasps for prismatic grippers.

• Capability Index Grasp evaluator: We implemented
the metric proposed in [23], which maximizes the
projection of the task ellipsoid along the trajectory of
the object. This is the metric used for most of the
examples presented in this paper.

• Default Grasp evaluator: This plugin implements the
metric proposed in [24], and it combines grasp and arm
kinematics factors to rank grasps.

A. Grasp Planner

B. Application usage

Under our implementation, the grasp planning process
consists of three sequential steps, each of which is operated
by the modules described below:

We provide a number of utilities built over the core
GraspPlanner class such that they facilitate its usage. These
include:

1) Grasp Generator: This module receives the task
description and uses the object’s geometry and robot

• Grasp Visualization: A class that contains an Interac-
tive Marker server and is used to display visualization

of the candidate grasps. The Grasp Planner contains an
instance of this class.

• Grasp Cache: A class also contained within the Grasp
Planner, which allows a user to store the grasps gen-
erated either on disk or only during the current run.
The Grasp Cache uses the MinimalDescription message
(which contains the combination of hand and object
geometries) as a key; and the grasps produced by the
Grasp Generator step as the value. This means that
during a run, the planner only calls the Grasp Generator
step at most once, for a given object and end effector
combination. This is particularly useful for objects with
complex geometries. The Grasp Filter and Evaluator are
always called, as their ﬁltering effect depends on the
(likely changing) environment.

• Grasp Planner Interface: Wrapper class that contains
an instance of the GraspPlanner and a set of servers pro-
viding ROS services for easy communication between
the planner and external applications.

• Grasp Planner Client: A class that contains functions
that allow a user to easily request grasp planning
information. In the background, these functions use the
services provided by the Interface and send requests and
receive responses to grasp queries.

V. USER INTERFACES

In addition to the tools described in IV-B, which are
useful for interacting with the grasp planning capabilities in
a programmatic way, we also developed a front-end interface
that allows the user to build, modify, and update manipulation
tasks on the ﬂy using interactive Rviz tools. The following
subsections summarize their usage.

A. Grasping Tab

The purpose of this tab is to allow the user an easy way to
calculate candidate grasps for a task deﬁned with Affordance
Templates, our default task description language. The widget
is shown in Fig. 82.

As can be seen in the image, the user can make a grasp
query with the button Get grasp. If candidate grasps
are returned, they can be easily viewable using the View
grasp scroll box (Fig.9). The button Set wp is used to
effectively update the current AT’s End Effector waypoints
with the currently selected grasp candidate.

B. Supporting Object-agnostic tasks

The Affordance Template’s object-updating feature can
easily be exploited to reuse a single AT JSON ﬁle for
multiple objects. Figure 10(a) shows a Cheez-It Affordance
Template, with the end effector waypoints set appropriately
by the grasp planner with a side-grasp. Now imagine that we
want to perform this same task (moving an object through the
same four poses in space) but instead of using the Cheez-It
box, we want to use different objects.

Fig. 8. Grasping Tab after the Get grasps button has been pressed.
Grasping Tab is at the right of the image. The Update AT widget (used to
send grasp queries and update the AT on the ﬂy) is highlighted in blue. The
Interactive Markers for the task steps (Object waypoints) are highlighted in
green.

(a)

(b)

(c)

(d)

Fig. 9.
(a) Compact (arrow) view of all the valid candidate grasps for the
task. (b)-(d) Non-compact view of individual candidate grasps, available by
scrolling through the spin box in the Update AT widget.

We can make use of the update object function to try out
diverse objects. Figures 10(b) and (c) show the visualization
of the same AT trajectory, after calling the service and
updating the object with a wine glass CAD model and a
Spam tin mesh. An important observation to make here is
the fact that the service call updates only the display object,
not directly the Object Waypoints. The Object Waypoints are
nonetheless updated because the whole Affordance Template
is rebuilt after the update object function is called, hence the
Object Waypoints are updated down the line. Furthermore,
observe that the grasps shown in Figure 10 for the new
objects are not the same grasps as the ones used for the
Cheez-It box. The original EE waypoints were no longer
viable to use for the updated objects, as their shape is
obviously different. The waypoints shown were obtained by
making a call to the grasp planner (using the Get grasps
button in the Grasping Tab) and selecting one of the grasps
returned by the planner3.

C. ROI Tab

In many applications, the geometry of the object to be
manipulated might not be known beforehand. Sensor data,
such as point clouds, is used to describe the object’s geom-
etry and as input to the grasp planning pipeline. Such point

2Video of a user utilizing this Tab is available here: https://youtu.

3Video showing the update object feature is available here: https://

be/ECK4io6KWww.

youtu.be/C1axyAKVkZE

update object
service

(a)

update object service

(b)

(c)

Fig. 10.
Interactive updating of the target object for a generic 4-step task.
(a) Original Cheez-It object. (b) After updating the object with a goblet
mesh. (c) After calling the update object with a Spam CAD model.
For (b) and (c) we also sent a grasp query and updated the end effector
waypoint

cloud must be previously segmented such that only the points
corresponding to the object are considered.

A user might have available tools to perform automatic
object segmentation under certain circumstances (e.g., table-
top segmentation), however this is not always the case. To
address these scenarios, we developed the ROI Tab, which
is a widget that offers the user an easy way to interactively
select a region of the point cloud to be segmented and
converted to a mesh form, which is then automatically
used to update the object geometry of the currently used
Affordance Template.

Figure 11(b) shows the ROI Tab being used to grasp a
Pringles container using a Panda arm4. Fig. (a) shows the
scene as seen in Gazebo and (b) shows the point cloud
obtained from a RGB-D sensor located overhead. Fig. (c)
shows the ROI tab (highlighted in blue). It contains a set
of edit boxes that allow a user to deﬁne a bounding box
to extract a desired volume of the point cloud. The box’s
dimensions can also be edited using interactive markers in
the 3D Rviz window (also shown in Fig(c)). After the user
presses the Create mesh button in the ROI tab, two things
happen: (1) the point cloud is ﬁltered, and a mesh is created
with the segmented points; and (2) the newly created mesh

4Video of the ROI Tab used with the Panda arm is available here: https:

//youtu.be/5d5w1vB6wzM

is used to update the target object in the AT currently loaded
(Figure (d)). Finally, the user can utilize the Grasping Tab
to generate grasps using the updated mesh and the task
steps, which are the same regardless of the change of object
geometry (Figure (e)). The last ﬁgure of the group shows
the Panda arm grasping the object with one of the candidate
grasps.

VI. EXAMPLES

In this section, we present some examples of using the
grasp planning tools developed by this project with a number
of different robots. A playlist with all our demonstrations is
also publicly available5.

A. Handing over a wine glass

Figure 12(a) and (b) shows a Jaco and Panda arm per-
forming a simple handover task consisting of transporting
a glass of wine from a utility cart to be presented to a
waiting customer6. In these examples, the task description
used is the same, with the only difference being that the
target object was changed from a wine glass (for Jaco) to
a ﬂute (for Panda); this change was necessary as the Panda
gripper opening was not wide enough to hold the original
glass. This task had an additional path constraint of keeping
the object with a tilt less than 20◦.

(a) Jaco

(b) Panda

Fig. 12.
Jaco (and Panda) manipulators performing a handover task.
Overlay of handover motion. Note how the tilting is very small, with the
glass being kept upright at most times.

B. Painting a square on a table

This demonstration showcases the utility of planning with
path constraints and the use of a grasp evaluator metric
that can be used for tasks with multiple waypoints. In this
example, the task consists of painting a square on a table
(Figure 13). The roller has to keep its surface in contact
with the table (Z tolerance is zero). The roller has to closely
follow a line between waypoints, so the tolerances in X and
Y are very small during motion (1mm). Finally, the handle
can rotate with respect to the roller axis, as long as contact
is kept7.

5Playlist available here: https://www.youtube.com/playlist?

list=PLmQko6gr2EvHqGXtsES6WOjAiPudvLZ16

6Videos of Jaco test: https://youtu.be/bvIVQOjKwS0. Panda

test: https://youtu.be/t4l36rmIY_k

7Video of Yumi painting test: https://youtu.be/YeGa8pRv6wU

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 11. Pipeline of ROI Tab usage: (a) Original scene, (b) Pointcloud acquired from an overhead sensor, (c) Using the ROI Tab and interactive tools to
obtain a segmented volume of the cloud, (d) Using the segmented cloud (converted now into a mesh) to update the current Affordance Template’s object,
(e) Generating grasps with the Grasping Tab; and (f) Executing the ﬁrst motion towards the selected grasp.

(a) Start

(b) Waypoint 1

(c) Waypoint 2

Start: A stance is found

Mid-navigation: Val moves towards cart

(d) Waypoint 3

(e) Waypoint 4

(f) Waypoint 5

Val reached the cart

Val grasps the can

Fig. 13. Yumi robot performing a painting task with 5 object waypoints.
The path constraints require the roller to be in contact with the surface at
all times, following closely a line, while the handle can rotate with respect
to the roller’s axis.

C. Valkyrie walking towards a cart to pick up a beer

In this demonstration (Figure 14), we use the mobile
grasping capabilities developed (combining the grasp plan-
ning features with the existing stance generation module) and
use them to enable Valkyrie to plan both a robot stance and a
set of grasp solutions to pick up a drink can located outside
of its reachable space8.

D. Fetch navigating towards a cart to pour a drink

This demonstration showcases the Fetch robot performing
a drink pouring task. The additional constraint here is that
the service cart where the drink is located is far from its
reachable space. Figure 15 shows some snapshots of the task
9. The pouring task is deﬁned with 3 object waypoints, the
ﬁrst for reaching the object, the second for raising the bottle

8Video of Valkyrie pick: https://youtu.be/PraEhCeglRo
9Video
pouring

https://youtu.be/

Fetch

test:

the

of

gZKIBqDPFUE

Fig. 14. Example of grasp planning integration with stance generation in
CRAFTSMAN. Snapshots show Valkyrie planning simultaneously a robot
placement that allows it to reach a number of candidate grasps to grab the
beverage can.

to be close to the cup to receive the liquid, and the ﬁnal
object waypoint is a rotation of the second one by 90◦, such
that the wine bottle is tilted over the cup. The end effector
waypoints are 5, with 3 matching the 3 object waypoints and
2 additional pre-grasp waypoints at the start of the task.

VII. CONCLUSIONS AND FUTURE WORK

In this paper we have presented ADAMANT, the grasp
software module developed to provide the CRAFTSMAN
framework with grasp planning capabilities. Our imple-
mented modules allow a user to generate a set of candidate
grasps for a manipulation task, ordered in such a way that the
best grasps are presented ﬁrst. The software tools presented
were developed using a plugin-based design, such that they
are easily extensible to use existing third-party libraries for
grasp generation. Our tools also include interactive tools that
allow the user to update the object being manipulated on the
ﬂy, either with an existing CAD model or with sensor data.
We provided multiple examples showcasing the usefulness
of our enhancements.

As future work, we’d like to explore how to better integrate

(a) Fetch navigates towards the cart

(b) Fetch picks the bottle

(c) Fetch completes the pouring task

Fig. 15. Fetch robot performing a (mobile) pouring task on a cart located 3 meters away.

[14] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox, “6-dof
grasping for target-driven object manipulation in clutter,” in 2020 IEEE
International Conference on Robotics and Automation (ICRA).
IEEE,
2020, pp. 6232–6238.

[15] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep
hierarchical feature learning on point sets in a metric space,” Advances
in neural information processing systems, vol. 30, 2017.

[16] I. A. Sucan and S. Chitta, “Moveit!” Online at http://moveit. ros. org,

2013.

[17] J. James, Y. Weng, S. Hart, P. Beeson, and R. Burridge, “Prophetic
goal-space planning for human-in-the-loop mobile manipulation,” in
15th International Conference on Humanoid Robots (Humanoids).
IEEE, 2015, pp. 1185–1192.

[18] P. Beeson, S. Hart, and S. Gee, “Cartesian motion planning & task
programming with craftsman,” in Robotics: Science and Systems
Workshop on Task and Motion Planning, 2016.

[19] D. Gossow, A. Leeper, D. Hershberger, and M. Ciocarlie, “Interactive
markers: 3-D user interfaces for ROS applications,” IEEE Robotics &
Automation Magazine, vol. 18, no. 4, pp. 14–15, 2011.

[20] S. Hart, P. Dinh, and K. Hambuchen, “The affordance template ros
package for robot task programming,” in International Conference on
Robotics and Automation (ICRA).

IEEE, 2015, pp. 6227–6234.

[21] S. Hart, A. Huam´an Quispe, M. Lanighan, and S. Gee, “Generalized
affordance templates for mobile manipulation,” in International Con-
ference on Robotics and Automation (ICRA).

[22] A.

ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose
detection in point clouds,” The International Journal of Robotics
Research, p. 0278364917735594.

[23] N. Mavrakis, M. Kopicki, R. Stolkin, A. Leonardis et al., “Task-
relevant grasp selection: A joint solution to planning grasps and
manipulative motion trajectories,” in 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2016,
pp. 907–914.

[24] A. C. Huam´an Quispe, “Grasp selection strategies for robot ma-
nipulation used a superquadric based object representation,” Ph.D.
dissertation, School of Interactive Computing, Georgia Institute of
Technology, 2016.

perception tools into our interactive tools (e.g., the ROI
Tab). Algorithms for object completion (to address partial
point clouds) would be particularly useful. Additionally, the
integration of algorithms to segment an object into parts
would be helpful to improve the grasp generation process.

REFERENCES

[1] E. Ackerman, “A robot for the worst job in the warehouse: Boston dy-
namics’ stretch can move 800 heavy boxes per hour,” IEEE Spectrum,
vol. 59, no. 1, pp. 50–51, 2022.

[2] ——, “How diligent’s robots are making a difference in texas hospi-

tals,” IEEE Spectrum, 2020.

[3] S. J. Jorgensen, M. W. Lanighan, S. S. Bertrand, A. Watson, J. S.
Altemus, R. S. Askew, L. Bridgwater, B. Domingue, C. Kendrick,
J. Lee et al., “Deploying the nasa valkyrie humanoid for ied response:
An initial approach and evaluation summary,” in 2019 IEEE-RAS 19th
International Conference on Humanoid Robots (Humanoids).
IEEE,
2019, pp. 1–8.

[4] M. A. Diftler, J. Mehling, M. E. Abdallah, N. A. Radford, L. B.
Bridgwater, A. M. Sanders, R. S. Askew, D. M. Linn, J. D. Yamokoski,
F. Permenter et al., “Robonaut 2-the ﬁrst humanoid robot in space,”
in 2011 IEEE international conference on robotics and automation.
IEEE, 2011, pp. 2178–2183.

[5] C. Ferrari and J. Canny, “Planning optimal grasps,” in International
IEEE, 1992, pp.

Conference on Robotics and Automation (ICRA).
2290–2295.

[6] M. Ciocarlie, C. Goldfeder, and P. Allen, “Dimensionality reduction
for hand-independent dexterous robotic grasping,” in IEEE/RSJ Int.
Conf. on Intelligent Robots and Systems (IROS), 2007, pp. 3270–3275.
[7] R. Diankov and J. Kuffner, “Openrave: A planning architecture for
autonomous robotics,” Robotics Institute, Pittsburgh, PA, Tech. Rep.
CMU-RI-TR-08-34, vol. 79, 2008.

[8] K. Huebner and D. Kragic, “Selection of robot pre-grasps using box-
based shape approximation,” in IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2008.

[9] M. Przybylski, T. Asfour, and R. Dillmann, “Unions of balls for shape
approximation in robot grasping,” in IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2010.

[10] C. Goldfeder, P. Allen, C. Lackner, and R. Pelossof, “Grasp planning
via decomposition trees,” in IEEE Int. Conf. on Robotics and Automa-
tion (ICRA), 2007, pp. 4679–4684.

[11] R. Balasubramanian, L. Xu, P. D. Brook, J. Smith, and Y. Matsuoka,
“Physical human interactive guidance: Identifying grasping principles
from human-planned grasps,” in The Human Hand as an Inspiration
for Robot Hand Development. Springer, 2014.

[12] M. Ciocarlie, K. Hsiao, E. G. Jones, S. Chitta, R. B. Rusu, and
I. A. S¸ ucan, “Towards reliable grasping and manipulation in household
environments,” in Experimental Robotics. Springer, 2014, pp. 241–
252.

[13] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Variational
grasp generation for object manipulation,” in Proceedings of
the
IEEE/CVF International Conference on Computer Vision, 2019, pp.
2901–2910.

