2
2
0
2

r
p
A
4
1

]
E
S
.
s
c
[

1
v
5
2
2
7
0
.
4
0
2
2
:
v
i
X
r
a

MP-CodeCheck: Evolving Logical Expression Code
Anomaly Learning with Iterative Self-Supervision

Urs C. Muff
Merly.ai
urs.muff@merly.ai

Paul Gottschlich
Merly.ai
paul.gottschlich@merly.ai

Celine Lee
Merly.ai, Cornell University
celine.lee@merly.ai

Justin Gottschlich
Merly.ai, University of Pennsylvania
justin.gottschlich@merly.ai

Abstract
Machine programming (MP) is concerned with automating
software development. According to studies, software engi-
neers spend upwards of 50% of their development time de-
bugging software. To help accelerate debugging, we present
MP-CodeCheck (MPCC). MPCC is an MP system that at-
tempts to identify anomalous code patterns within logical
program expressions. In designing MPCC, we developed two
novel programming language representations, the forma-
tions of which are critical in its ability to exhaustively and
efficiently process the billions of lines of code that are used
in its self-supervised training.

To quantify MPCC’s performance, we compare it against
ControlFlag, a state-of-the-art self-supervised code anomaly
detection system; we find that MPCC is more spatially and
temporally efficient. We demonstrate MPCC’s anomalous
code detection capabilities by exercising it on a variety of
open-source GitHub repositories and one proprietary code
base. We also provide a brief qualitative study on some of
the different classes of code anomalies that MPCC can detect
to provide an abbreviated insight into its capabilities.1

CCS Concepts: • Computing methodologies → Learn-
ing settings; • Software and its engineering → General
programming languages; Semantics.

ACM Reference Format:
Urs C. Muff, Celine Lee, Paul Gottschlich, and Justin Gottschlich.
2022. MP-CodeCheck: Evolving Logical Expression Code Anomaly
Learning with Iterative Self-Supervision. In Proceedings of ACM
Conference (Conference’17). ACM, New York, NY, USA, 11 pages.

1 Introduction
Software debugging has been found to consume upwards
of 50% of all software development time [14]. Machine pro-
gramming (MP), the field concerned with the automation
of all aspects of software development [21], has seen ad-
vances in software development related tasks such as code
auto-completion [12, 24, 54], code generation and program

1Note: To ensure author anonymity, MPCC’s GitHub repository has been
withheld (and made private). The authors will open-source MPCC upon
completion of the review process.

synthesis [6, 7, 18, 22, 23, 26, 28, 31, 34, 37, 39, 41, 43, 45, 52,
55, 59, 60], program transformation [20, 36] and repair [4,
8, 16, 32, 57], code similarity and recommendation [35, 58],
learned optimizations [10, 38, 44], and performance regres-
sion testing [3, 5, 30, 42, 53], amongst others.

The latter few tasks support the MP goal of software adap-
tation, which focuses on evaluating or transforming higher-
order program representations or legacy software programs
to achieve certain characteristics (e.g., performance, security,
etc.). An open challenge in software adaptation is reasoning
about legacy software. In such code bases, software defects
may arise from a number of issues. These include, but are
not limited to, logical errors, poorly organized code, and
technical debt. Even defects that do not have a clearly neg-
ative impact on a system may leave it susceptible to future
vulnerabilities. Some of these defects manifest in weaknesses
(or brittleness) in logical expressions. In an attempt to help
remedy this, we present MP-CodeCheck (MPCC), a self-
supervised, inventive, and adaptive MP system which detects
anomalous logical expressions at the source code level.

For the purposes of this paper, we trained MPCC on over
two billion lines of semi-trusted source code for common
logical expression programming patterns, which we refer
to as expression blocks. Using the intuition that deviation
from trusted programming techniques and paradigms can
lead to potential erroneous programming, MPCC uses mined
expression blocks to identify anomalous code that make the
program incorrect, prone to future bugs, or contain technical
debt. For example, consider the following C/C++ example
that (incorrectly) checks if a variable x is NULL:

// malformed, but legal double NULL equality check
if (NULL == x == NULL)
{

throw std::runtime_error("x is NULL.");

}

While this code snippet may look suspicious, it compiles
without warning in the default configuration of Visual Studio
2022 and produces only a warning using GCC with compiler
flag -Wpointer-arith. However, the code’s logical expres-
sion – when considered holistically – is erroneous. With x
set to NULL, the execution of the if statement will first check
whether NULL == x, which will evaluate to true. Then the

 
 
 
 
 
 
Figure 1. System Overview of MP-CodeCheck (MPCC).

if statement will continue its evaluation from left to right
and compare true == NULL, which will evaluate to false.
This will (incorrectly) cause the exception code that was
meant to throw an exception when x = NULL to be skipped.
Given this, the system will likely dereference x at a later time
causing an illegal memory access. MP-CodeCheck was de-
signed to identify these issues. In fact, MP-CodeCheck found
this anomaly, and others like it, in a large-scale production-
quality software repository.

In this paper, we make the following contributions:

1. We present MP-CodeCheck (MPCC), a system that
aims to identify anomalous logical expressions in code
by incorporating several novel code representations
and an iteratively-refined heuristic framework that
guides MPCC’s self-supervised engine.

2. We present a spatial and temporal learning and in-
ference performance comparison between MPCC and
ControlFlag [25] across 6,000 C/C++ repositories.
3. We provide an analysis of MPCC’s anomaly detection
capabilities across ten GitHub repositories that are
intentionally varied in size and lifetime.

2 Related Work
There have been many recent works in the field of machine
programming. In this section, we discuss some of the works
that we have found are most relevant to MPCC.

2

2.1 Self-supervised Systems

The emergence of self-supervised MP systems may be promis-
ing for large-scale machine learning, due to their ability to
function on the enormous corpora of unlabeled open-sourced
code training data. In the domain of natural language pro-
cessing, large pre-trained language models [9, 15, 51] have
already shown to be powerful tools for few-shot learning
in language processing tasks [48, 50] where task-specific
or domain-specific labeled datasets may be unavailable. For
code processing tasks, GitHub alone hosts over 46 million
public software repositories, presenting one source of abun-
dant but unlabeled code data. Recent self-supervised code
processing systems such as OpenAI’s Codex [12], Intel’s
ControlFlag [25], Microsoft Research’s BugLab [4], and the
basis for DeepMind’s AlphaCode [31], among others, have
taken advantage of the vast amount of available code data
to achieve impressive results for their respective tasks of
code generation, idiosyncratic code pattern detection, bug
detection and repair, and competitive programming.

Codex [12] and the underlying model for AlphaCode [31]
are both large transformer language models pre-trained on
large amounts of unlabeled GitHub code. This model setup
allows for Codex variants and AlphaCode to then be fine-
tuned on some small dataset of domain-specific examples to
perform a particular task; AlphaCode, for example, is fine-
tuned on a competitive programming dataset to generate

solutions to complex programming tasks. ControlFlag [25]
mined over one billion lines of unlabeled open-source C/C++
code for common and uncommon code patterns to detect id-
iosyncratic programming patterns. Once trained, it performs
inference on user-supplied code and suggest corrections on
anomalies it has found. BugLab [4] takes an adversarial ap-
proach to learning software bug detection and repair by
co-training a bug detection and repair model alongside a bug
injection model such that the bug injection model learns to
generate data from which the bug detection and repair model
is trained. This boot-strapped technique does not require
any external labeled training data.

Other systems such as Snorkel [49] combine weak super-
vision techniques to enable users to train state-of-the-art
models without hand-labeling (much) training data.

2.2 Program Repair / Program Synthesis

Anomaly detection is closely related to other types of auto-
mated program reasoning such as program repair and pro-
gram synthesis. These systems can be broken down into
human-in-the-loop systems and closed loop systems.

Human-in-the-loop Systems. A primary challenge in
automated program reasoning is code semantic understand-
ing; an incorrect implementation with even minor syntactic
differences (e.g., in C/C++ == (equality), = (assignment)) can
produce drastically different software results. One way that
previous systems have attempted to address this issue is to
incorporate humans into the overall system. That is, humans
can guide the MP system’s choice and potentially reinforce
its learning algorithm.

Code recommendation systems are one such family of pro-
gram reasoning systems. A code recommendation system is an
automated system that ingests data (in some form) and then
recommends some code fragment that is meant to satisfy the
supplied input. Microsoft IntelliSense [40], Tabnine [1] and
GitHub Copilot [2] are examples of commercially-deployed
code completion suggestion tools. They can be integrated
into interactive development environments (IDEs) and pro-
vide real-time suggestions for the programmer in the IDE.
IntelliSense [40] uses knowledge of programming language
semantics to suggest possible variables, methods, fields, type
parameters, constants and classes, among other completions,
as the programmer edits code in the IDE. Tabnine [1] and
Copilot [2] train deep neural networks to learn from large
corpora of source code to autocomplete whole lines or whole
functions of code. Tabnine offers the option to fine-tune
learned models on the user’s (or user’s team’s) code to pro-
vide guidance aligned with the user’s own code practices.
Copilot adapts to edits made to its suggestions to match the
user’s coding style. Chaurasia and Mooney incorporate hu-
mans into a natural language-to-code generation system [11]
by using dialog to clarify user intent until it has enough in-
formation to produce correct code.

3

Code similarity systems analyze code fragments and de-
termine if they are semantically (i) similar, (ii) dissimilar,
or (iii) equivalent. Such systems can be used for a variety
of purposes. For example, they can help to identify exist-
ing code intent and suggest alternatives that may be less
brittle and easier to understand. The MISIM neural code sim-
ilarity system uses a context-aware semantics structure to
lift semantics from code syntax. It then scores the semantic
similarity of any two semantics structures via an extensible
learned scoring algorithm (usually in the form of a deep
neural network) [58]. Such a system could be used for many
things, one being language-to-language transpilation. The
Aroma code recommendation system performs code search
using a novel simplified parse tree (SPT), which elides away
many syntactical details of the original code [35]. Using the
SPT, the Aroma system aims to take incomplete code snip-
pets and return (more) complete code snippets.

Closed Loop Systems. Some systems operate in an end-
to-end manner to generate and modify code, without any
human user feedback. Many program synthesis systems
using sketching [52], inductive program synthesis [6, 18,
19, 22, 23, 33, 37, 39, 45, 46], and natural language descrip-
tions [27, 43, 47, 56] operate in this closed loop manner to
generate code satisfying some input specification.

One explored application of program transformation is
transpilation, which is a technique in translating code from
one programming language to another. Kamil et al. [26]
demonstrate a technique to lift low-level Fortran code to a
high-level predicate language summary and lowering it back
down into Halide code to achieve performance speedups.
Chen et al. [13] also develop a tree-to-tree model to translate
programs from one programming language to another.

Automated program repair (APR) [29] is the task of auto-
matically repairing software to reduce the work of human
engineers while maintaining program usability and avoiding
software regression. BugLab [4] is one example APR system
that co-trains a bug detection and repair model alongside
a bug injection model such that as the bug injection model
learns to generate harder-to-find bugs, the bug detection and
repair model learns to find and repair harder to find bugs.
Li et al. [32] present a technique that uses prior bug fixes
and surrounding code context to modify a buggy program’s
abstract syntax tree. Some APR works have also leveraged
natural language to model reasoning about automated repair,
such as Yasunaga and Liang [57] who use a buggy program’s
diagnostic feedback error message to localize then generate a
repaired version of the erroneous line in the software source
code. TFix [8] is another end-to-end text-to-text system that
fixes buggy code without labels by pre-training a model on
natural language then fine-tuning it on generating code fixes.
Hoppity [17] approaches automatic program debugging by
learning a sequence of graph transformations over a buggy
program represented as a graph structure.

Figure 2. Illustration of basic and complex expression blocks used in MP-CodeCheck.

These systems, however, focus on syntactic bugs that
cause incorrect or failed program execution. Semantic pro-
gram repair is the task of fixing non-syntactic bugs that
cause program behavior divergent from what the program-
mer intended. Devlin et al. [16] propose an approach for
automatic semantic program repair without access to the
code’s intended correct behavior at either training or test
time: the system first proposes many potential bug repairs
then scores them using a learned neuro-symbolic network,
outputting the highest-scoring candidate as the solution.

3 MP-CodeCheck System Design
MPCC’s system overview is shown in Figure 1. At the highest
level, MPCC can be thought of as a code anomaly detection
system, similar to ControlFlag [25]. However, MPCC has at
least two fundamental design departures from such prior
works. First, it uses novel code representations that help its
anomalous code detection engine. Second, it uses an iter-
ative, programmatic heuristic to guide its self-supervised
engine. As described in Section 4, we have found that these
design elements can reduce computational overhead (see
Section 4) as well as reducing false positives (see Section 5).
Moreover, by using these design elements together, MPCC
can manually or automatically be augmented to fit differ-
ent programming languages, development environments,
or stylistic constraints. Without these capabilities, it can
be challenging or impossible to achieve similar customiza-
tion (and debugging augmentation) when using a machine
learning-only approach. This is especially true for systems
that do not provide insight into their underlying mechanics
(e.g., ControlFlag’s string pattern matching algorithm). We
describe both of these design elements in this section.

3.1 Novel Code Representations

MPCC combines existing and novel code representations
for its predicate expression classification system. Most of
MPCC’s internal code manipulation uses representations
that are implemented using various graph structures, gen-
erally in the form of a tree (e.g., abstract syntax tree (ASTs),

flattened non-binary tree, etc.). We have designed two new
representations to enhance MPCC’s ability to reason about
the semantic properties of logical expressions: basic expres-
sion blocks and complex expression blocks. We describe them
as follows.

3.1.1 Basic and Complex Expression Blocks. A nov-
elty in MPCC code representation is in its utilization of basic
and complex expression blocks. The purpose of these blocks
is to help the system reason about the semantics of logical
expressions that may be asymmetrically spread across mul-
tiple logical operations in the same control structure. The
formation of semantically rich compound expressions – of-
ten found in complex expression blocks – has helped MPCC
distill semantically complex expressions that are both nom-
inal (i.e., non-anomalies) and anomalous. This distillation
helps MPCC identify more true positives as well as assisting
in reducing false positives. We formally define basic and
complex expression blocks as follows.

A basic expression block is a predicate expression that
contains no logical conjunction operators (i.e., it contains
no logical-ANDs, logical-ORs). Every predicate expression
can be divided into its basic expression blocks as shown
in Figure 2. A complex expression block is a composition
of at least two basic expression blocks, normalized across
variable (identifier) names. The intuition behind a complex
expression block (or complex block) is that program semantics
are often encapsulated across multiple, disjoint predicates
in a control structure. For example, in Figure 2, for each
of the complex blocks, multiple predicates with a single,
shared identifier must be satisfied for the complete logical
expression to be true (e.g., x in the left-most yellow complex
block, y in the blue complex block, and p in the right-most
yellow complex block). These shared identifiers often embed
semantic relationships between the basic expression blocks.
To illustrate this concretely, consider the following common
programming idiom to ensure a variable is within a minimum
and maximum bounds:

if (min < x && x < max) { ... }

4

The identifier x is present in both of the basic expression
blocks (i.e., min < x and x < max). As such, if used with
MPCC a new complex expression block that conjoins both
min < x and x < max would be constructed. When applied
during inference, if MPCC constructed a complex expression
block that had such a signature (after being normalized), it
would flag the block as non-anomalous as it would have
learned this expression is nominal.

Our experience with MPCC is that this use of complex ex-
pression blocks to capture compound programmatic seman-
tics helps identify more complex programming anomalies,
while simultaneously flagging nominal complex code struc-
tures that are common, such as the minimum and maximum
expression discussed here.

3.2 Programmatic Evolution of Self-Supervision

A second novelty of MPCC is in its machine-driven guidance
on the evolution of its heuristic-based rules. This evolution-
ary process is a key aspect in the development of the core
elements of the system as well as in improving the quality
of the results. This step is captured in Step 5 in MPCC’s
Training in Figure 1. In this process, the knowledge that
the self-supervised system learns, the common and uncom-
mon control structure patterns, is used to inform human
programmers in their construction of heuristics, represen-
tations, and rules that further improve the self-supervised
learning. This iteration was done over a dozen times during
the construction of MPCC.

A key reason for why this approach is critical to MPCC
is that in our experience, isolated self-supervision is gen-
erally insufficient to learn complex and nuanced concepts
in programming languages; on its own, self-supervision of-
ten produces a large number of false positives. Instead, a
heuristic-driven and iterative learning approach, that uti-
lizes subject matter experts for reinforcement learning, helps
to properly guide MPCC through nuanced and unknown
software design patterns, whether infrequent but correct, or
frequent and incorrect.

Moreover, although it is not captured in this paper, in
our early experiments, which did not use this human-in-the-
loop iterative feedback loop, MPCC’s false positive rate for
code anomalies exceeded ≈ 90%. When combining humans
and machines in MPCC’s design, it has reached ≈ 31% false
positive rate (see Table 1), a 3× reduction in false positives.

4 Quantitative Results
In this section, we present quantitative results on the MP-
CodeCheck system. These include (i) performance metrics of
MPCC compared to ControlFlag and (ii) MPCC’s inference

2All experiments run on the same system: OS: 64-bit Windows 11 Home;
Processor: 11th Gen Intel(R) Core (TM) i7@ 2.80GHz; Ram: 16.0 GB; Drive:
1TB SSD

5

Figure 3. Performance comparison: MP-CodeCheck vs. Con-
trolFlag in terms of training time, model size, model loading
time, inference time, and reliability (i.e., number of halts).2

accuracy metrics with respect to false positive rates across
several open-source GitHub repositories (see Table 1). 3

4.1 Computational Performance Metrics

Figure 3 details the performance results of our experimental
study comparing MPCC to ControlFlag. We trained both
systems on the open-source code data recommended by the
ControlFlag repository’s README4, which consists of 6,000
repositories containing 1.1 billion lines of C code with some
minor C++-specific code (< 5%). The training metrics we
gathered include training time (in minutes), maximum mem-
ory utilized during training (in MB), number of software halts
during training, and resulting trained model size (in GB). For
inference, we ran both systems on the open-source GitHub
Load Balancer 5 repository and collected metrics on model
loading and inference time across the entire repository.

4.2 MPCC’s Basic and Complex Expression Blocks
for Computational and Spatial Efficiency

MPCC’s basic and complex expression block representations
are core components of its efficient training, inference, and

3We tried to compare MPCC’s inference results to those from ControlFlag,
but were unable to due to computational tractability limitations in Con-
trolFlag’s open-source system.
4https://github.com/IntelLabs/control-flag/blob/master/README.md
5https://github.com/github/glb-director

Repository Size (MB) Est. Year

# of Anoms

# of Expr. Anoms Per Expr. Top 20 False Positive %

git/git

40.07
curl/CURL 15.92
15.14
iovisor/bcc
44.26
netdata/netdata
php/php-src
125.27
proprietary* 196.06
qemu/qemu 116.78

raspberrypi/pico-sdk 7.25
shakevsky/keybuster 2.47
ventoy/Ventoy 112.2
67.4
Averages

2008
2010
2015
2013
2011
1999
2012
2021
2020
2020
N/A

Repository Size (MB) Est. Year

41
16
7
48
76
66
122
22
2
22
42.2
# of Anoms

31,298
13,983
3,607
26,092
47,994
12,948
79,050
1,226
393
17,316
23,390
# of Expr. Anoms Per Expr. Top 20 False Positive %

55%
56.25%
42.86%
30%
10%
20%
35%
27.28%
0%
35.29%
31.16% FP Rate

0.131%
0.114%
0.194%
0.184%
0.158%
0.510%
0.154%
1.79%
0.509%
0.127%
0.387%

Table 1. Results of MPCC’s Inference on 10 Repositories Ranging in Size and Year of Establishment.

model size. For example, ControlFlag uses a syntax-driven
trie (a type of k-ary search tree) for its training and inference.
This syntax-driven trie stores and compares each individual
syntax letter of an expression in trie form to numerous other
expression tries from both anomalous and non-anomalous
clusters. This process is performed iteratively, per syntax
element (i.e., letter), until an identical match, or no match
at all, is found. While mathematically sound, this approach
suffers from search space growth with an upper bound that
is exponentially proportional to the size of the alphabet used
in the associated search grammar.

On the other hand, for training and inference expression
matching, MPCC uses a restricted search that only consists of
basic and complex expression blocks that have already been
learned as potential matches. In total, these blocks constitute
less than 70, 000 unique entries for the one billion lines of
code used for training. Moreover, when compared to a syntax
trie counterpart, the number of unique basic and complex
expression blocks has a relatively small numerical bound.
This is due to two reasons. First, each expression block is
reduced from its original syntax into a minimized normal-
ized form, which reduces its spatial footprint and enables
divergent syntax expressions to converge into semantically
equivalent normalized (and minimized) block form. Second,
the expression blocks are limited to those that are accepted
design patterns found in the semi-trusted training reposito-
ries, thus limiting the total number of unique blocks. The
computational divergence of these two search approaches
leads to notable overall system performance differences be-
tween MPCC and ControlFlag as shown in Figure 3.6

4.3 System Accuracy

In Table 1, we show our experimental results for MPCC
inference accuracy. We tested MPCC on ten repositories
with an intentional variation in repository size and year of
establishment. For each repository, MPCC parses all C and
C++ source files for control expressions then classifies each
as anomalous or non-anomalous according to the scoring
system explained in Section 3. In our experiments, we set the
anomaly threshold to 1000: if a logical expression is assigned
a complexity score greater than 1000, then MPCC flags it
as an anomaly. We then manually inspected 20 anomalies
with the highest anomaly complexity scores per repository
to determine whether the flagged anomaly is not in fact an
anomaly (i.e., it is a false positive).

Across the ten repositories that we inspected, we found
a false positive rate of 31.16%. Anomalies are marked as
false positives if they do not introduce excess technical debt.
That is, most anomalies that are not false positives (i.e., true
positives) possess one or more of the following features:

1. They use improper pointer checking practices.
2. They have unclear arithmetic operations.
3. They have unclear and inconsistent type casting.
4. They have many connected but disjoint predicates (i.e.
each predicate performs a check on a different variable)
for error checking.

5. They have inefficient usage of logical operators.
6. They have over- or under-utilization of parentheses.
7. They are C++ specific operations.
8. They perform arithmetic and Boolean operations on

the same variable.

9. They are potential bugs.

6Note that MPCC’s utilization of ≈ 2× more available memory during train-
ing than ControlFlag results in an additional computational efficiency over
ControlFlag, as MPCC is able to more fully exercise the available hardware.

6

7https://stackoverflow.com/questions/39150884/is-there-a-shorter-
way-to-write-compound-if-conditions/39151002#39151002,
//stackoverflow.com/questions/16644906/how-to-check-if-a-string-is-
a-number/16644949#16644949,
13214506/shorthand-for-checking-for-equality-to-multiple-possibilities

https://stackoverflow.com/questions/

https:

Figure 4. Four Qualitatively Analyzed Examples of Anomalous Logical Code Expressions as Detected by MPCC. 7

7

Proprietary RepositoryClassificationTrue Positivenetdata\collectors\cups.plugin\cups_plugin.c (Line: 434)if (!now_monotonic_sec() - started_t > 14400)Anomalous Predicate!a()-bExpression Block Classificationcomplex_2Complexity Score1003.1ClassificationTrue PositiveQEMU\hw\net\rocker\rocker_of_dpa.c (Line: 325)netdata\exporting\opentsdb\opentsdb.c (Line: 130)Anomalous Predicate(~*a & *b & *c)|(*a & *b & ~*c)Expression Block Classificationcomplex_2Complexity Score2046.5ClassificationTrue PositiveAnomalous Predicatea() || b() || *c==123||*c==123||*c==123||*c==123||d()Expression Block Classificationcomplex_2Complexity Score1032.5ClassificationFalse Positiveif ((~*k & *m & *v) | (*k & *m & ~*v))if (isalpha(*src) || isdigit(*src) ||    *src=='-' || *src=='_' || *src=='.' ||     *src=='/' || IS_UTF8_BYTE(*src))This example converts the value of the now_monotonic_sec() function into a Boolean when the unary NOT  (!)operator is applied to its return value. Yet, the term is subsequently treated as an integer value for subtraction and greater than comparison.Also, it is not likely performing as the programmer intended. The ! operator will return 0 or a 1. In this embodiment, the only way for this conditional to evaluate to true would be for started_t to be <-14399 or <-14400, depending on the result of the !. However, started_t is a time_t type, which is set to the time of invocation in the line prior to the condition and is unlikely to ever be negative. This sequence of bitwise logical operations introduces technical debt. The goal of this conditional is to determine whether at at least one of the bits where *m is active (logical 1), the corresponding bits in *k and *v is are not equal. While logically correct, this conditional conducts excess logical operations that do not contribute to the readability of the code. We reported this anomaly to the QEMU repository maintainers with a proposal on how to improve the efficiency of the code, and together we agreed upon the following way to write the condition, which both conducts fewer logical operations and more clearly conveys the goal of the conditional:((*k & *m) != (*v & *m)) if ( NULL == actor == NULL )Anomalous PredicateNULL == a == NULLExpression Block Classificationcomplex_1Complexity Score1003.25This expression incorrectly checks whether a variable, actor, is NULL. If actor is set to NULL, the expression, moving left-to-right, would first evaluate NULL==actor to true then evaluate true==NULL to false. Likewise, if actor is set to anything but NULL, the expression will first evaluate NULL==actor to false then evaluate false==NULL to true. So, somewhat unintuitively,    NULL == NULL == NULL  → false    NULL ==(not NULL)== NULL → trueThis behavior is inconsistent with the goal of comparing actor to NULL. In fact, this logical expression is a potential crash bug.This expression is flagged as an anomaly by MPCC because of the logical or || operators between many predicates. This is a reasonable flag because usually too many predicates in a single conditional can be difficult for a programmer to cognitively reason about; thus, the mental burden can introduce technical debt. However, in this case, upon consultation with four experienced C programmers and the most viewed StackOverflow threads* on this topic, this method of checking a character as part of a set of possibilities is common practice in C. Though relatively long and embedded with unnamed constant values, this expression is generally agreed to be easy to understand and thus easy to maintain.(Q1)(Q2)(Q3)(Q4)5 Qualitative Results
Figure 4 presents qualitative results on the MP-CodeCheck
system by analyzing four flagged anomalies in detail. Each of
the four flagged anomalies, shown in quadrants (Q1)-(Q4) of
the figure, were flagged by MPCC because the programming
patterns that they exhibit, as represented by MPCC’s novel
code representation structure, were uncommon or unseen
before in the C code on which MPCC was trained. The first
three anecdotal examples (Q1)-(Q3) are true positives, or
true anomalies. The first example (Q1) is the same one as
introduced in the Introduction (Section 1). The last anec-
dotal example (Q4) is a false positive: MPCC flagged it as
an anomaly, but upon manual inspection, it is actually non-
anomalous. We detail our analysis of each anecdotal example
in their corresponding quadrants in Figure 4.

6 Conclusion
In this paper, we introduced MP-CodeCheck (MPCC), a self-
supervised anomaly detection system for logical expressions.
Our early evidence seems to demonstrate that MPCC can
assist in one of the more painstaking aspects of software
development, debugging, by identifying anomalies in even
hardened production-quality code. We also demonstrated
that MPCC is more temporally and spatial efficient than
ControlFlag, a state-of-the-art self-supervised anomaly de-
tection that also identifies anomalies in logical code expres-
sions. Moreover, our early results across ten high-quality
code repositories, rates MPCC with a false positive rate of
≈ 31%. In conducting our experimentation with MPCC on
these repositories, we identified what we believe are not only
anomalies with technical debt, but also more serious ones
such as security vulnerabilities and illegal memory accesses
(e.g., crash bugs).

7 Broader Impact
MP-CodeCheck uses semi-trust to obtain its training data,
which means that MPCC could be susceptible to uninten-
tionally learning from untrustworthy code. For example, if
an adversarial attacker were to provide training code data
with many instances of malicious programming patterns, the
resulting MPCC model would likely exhibit many false nega-
tives and false positives. That is, it may flag non-anomalous
code as anomalous and anomalous code as non-anomalous.
Another broader impact to consider is MPCC’s demand
for computational resources. MPCC obtains its knowledge
by mining billions of lines of code, which demands non-
trivial amounts of computation, albeit much less than any
deep learning based counterpart. This demand for computa-
tion places some amount of strain on both the environment,
which can contribute to climate-related issues, and the global
supply chain, which can contribute to economic issues.

As a machine programming system, MPCC has the objec-
tive of reducing or eliminating some burdens of software

8

development. This may seem to potentially reduce demand
for software engineers. However, we believe that the tasks
that MPCC alleviates are tasks that, despite being critical to
software robustness, are principally only understood deeply
enough to be done by a small minority of existing software
developers. These tasks also take up a sizable chunk of soft-
ware development time, whether in hunting down and fixing
code anomalies or in fixing bugs manifested by unremedied
code anomalies. We believe that by automating anomaly
detection, MPCC would increase productivity of software
developers and subsequently open up time for more creative
tasks such as algorithm design and entrepreneurship, making
robust software engineering more accessible to even more
software developers.

References
[1] 2018. Code Faster with AI Code Completions. https://www.tabnine.

com/

[2] 2021. GitHub Co-Pilot: Your AI Pair Programmer. https://copilot.

github.com

[3] Mejbah Alam, Justin Gottschlich, Nesime Tatbul, Javier Turek, Timo-
thy Mattson, and Abdullah Muzahid. 2019. A Zero-Positive Learning
Approach for Diagnosing Software Performance Regressions. In Pro-
ceedings of the 33rd International Conference on Neural Information
Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Arti-
cle 1043, 13 pages.

[4] Miltiadis Allamanis, Henry Richard Jackson-Flux, and Marc
Self-Supervised Bug Detection and Repair.
Brockschmidt. 2021.
In Advances in Neural Information Processing Systems, A. Beygelz-
imer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (Eds.). https:
//openreview.net/forum?id=EQbyD_KG6w-1h

[5] Mona Attariyan, Michael Chow, and Jason Flinn. 2012. X-Ray: Au-
tomating Root-Cause Diagnosis of Performance Anomalies in Pro-
duction Software. In Proceedings of the 10th USENIX Conference on
Operating Systems Design and Implementation (Hollywood, CA, USA)
(OSDI’12). USENIX Association, USA, 307–320.

[6] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
Nowozin, and Daniel Tarlow. 2017. DeepCoder: Learning to Write
Programs. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceed-
ings.

[7] Kory Becker and Justin Gottschlich. 2021. AI Programmer: Au-
tonomously Creating Software Programs Using Genetic Algorithms.
In Proceedings of the Genetic and Evolutionary Computation Conference
Companion. Association for Computing Machinery, New York, NY,
USA, 1513–1521. https://doi.org/10.1145/3449726.3463125

[8] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021.
TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.
In Proceedings of the 38th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and
Tong Zhang (Eds.). PMLR, 780–791. https://proceedings.mlr.press/
v139/berabi21a.html

[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language Models are Few-Shot Learners.
In Advances in Neural Information Processing Systems, H. Larochelle,

M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran
Associates, Inc., 1877–1901. https://proceedings.neurips.cc/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[10] Lujing Cen, Ryan Marcus, Hongzi Mao, Justin Emile Gottschlich, Mo-
hammad Alizadeh, and Tim Kraska. 2020. Learned Garbage Collection.
In Proceedings of the 4th ACM SIGPLAN International Workshop on
Machine Learning and Programming Languages.

[11] Shobhit Chaurasia and Raymond J. Mooney. 2017. Dialog for Language
to Code. In Proceedings of the Eighth International Joint Conference on
Natural Language Processing (Volume 2: Short Papers). Asian Federation
of Natural Language Processing, Taipei, Taiwan, 175–180.
https:
//www.aclweb.org/anthology/I17-2030

[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde,
Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott
Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,
Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Pet-
roski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,
Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan
Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford,
Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Pe-
ter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language
Models Trained on Code. ArXiv abs/2107.03374 (2021).

[13] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-Tree Neural
Networks for Program Translation. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems (Mon-
tréal, Canada) (NIPS’18). Curran Associates Inc., Red Hook, NY, USA,
2552–2562.

[14] Evans Data Corporation. 2012. Global Development Survey Report.
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2019. BERT: Pre-training of Deep Bidirectional Transformers for Lan-
guage Understanding. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1 (Long and Short Papers).
Association for Computational Linguistics, Minneapolis, Minnesota,
4171–4186. https://doi.org/10.18653/v1/N19-1423

[16] Jacob Devlin, Jonathan Uesato, Rishabh Singh, and Pushmeet Kohli.
2017. Semantic Code Repair using Neuro-Symbolic Transformation
Networks. ArXiv abs/1710.11054 (2017).

[17] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and
Ke Wang. 2020. Hoppity: Learning Graph Transformations To Detect
And Fix Bugs In Programs. In International Conference on Learning
Representations. https://openreview.net/forum?id=SJeqs6EFvB
[18] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lu-
cas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and
Joshua B. Tenenbaum. 2021. DreamCoder: Bootstrapping Inductive
Program Synthesis with Wake-Sleep Library Learning. In Proceedings
of the 42nd ACM SIGPLAN International Conference on Programming
Language Design and Implementation. Association for Computing Ma-
chinery, New York, NY, USA, 835–850. https://doi.org/10.1145/3453483.
3454080

[19] John K. Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesiz-
ing Data Structure Transformations from Input-Output Examples, In
Proceedings of the 36th ACM SIGPLAN International Conference on
Programming Language Design and Implementation. SIGPLAN Not.
50, 6, 229–239. https://doi.org/10.1145/2813885.2737977

[20] Xiang Gao, Shraddha Barke, Arjun Radhakrishna, Gustavo Soares,
Sumit Gulwani, Alan Leung, Nachiappan Nagappan, and Ashish Ti-
wari. 2020. Feedback-Driven Semi-Supervised Synthesis of Program
Transformations. Proc. ACM Program. Lang. 4, OOPSLA, Article 219
(nov 2020), 30 pages. https://doi.org/10.1145/3428287

9

[21] Justin Gottschlich, Armando Solar-Lezama, Nesime Tatbul, Michael
Carbin, Martin Rinard, Regina Barzilay, Saman Amarasinghe, Joshua B.
Tenenbaum, and Tim Mattson. 2018. The Three Pillars of Machine
Programming. In Proceedings of the 2nd ACM SIGPLAN International
Workshop on Machine Learning and Programming Languages (Philadel-
phia, PA, USA) (MAPL 2018). Association for Computing Machinery,
New York, NY, USA, 69–80. https://doi.org/10.1145/3211346.3211355
[22] Sumit Gulwani. 2011. Automating String Processing in Spreadsheets
Using Input-Output Examples. SIGPLAN Not. 46, 1 (Jan. 2011), 317–330.
https://doi.org/10.1145/1925844.1926423

[23] Sumit Gulwani and Prateek Jain. 2017. Programming by Examples: PL
Meets ML. In Programming Languages and Systems - 15th Asian Sym-
posium, APLAS 2017, Suzhou, China, November 27-29, 2017, Proceedings
(Lecture Notes in Computer Science, Vol. 10695), Bor-Yuh Evan Chang
(Ed.). Springer, 3–20. https://doi.org/10.1007/978-3-319-71237-6_1

[24] Daya Guo, Alexey Svyatkovskiy,

Jian Yin, Nan Duan, Marc
Brockschmidt, and Miltiadis Allamanis. 2022. Learning to Complete
Code with Sketches. In International Conference on Learning Represen-
tations. https://openreview.net/forum?id=q79uMSC6ZBT

[25] Niranjan Hasabnis and Justin Gottschlich. 2021. ControlFlag: A Self-
Supervised Idiosyncratic Pattern Detection System for Software Con-
trol Structures. In Proceedings of the 5th ACM SIGPLAN International
Symposium on Machine Programming (Virtual, Canada) (MAPS 2021).
Association for Computing Machinery, New York, NY, USA, 32–42.
https://doi.org/10.1145/3460945.3464954

[26] Shoaib Kamil, Alvin Cheung, Shachar Itzhaky, and Armando Solar-
Lezama. 2016. Verified Lifting of Stencil Computations. In Proceedings
of the 37th ACM SIGPLAN Conference on Programming Language Design
and Implementation (Santa Barbara, CA, USA) (PLDI ’16). Association
for Computing Machinery, New York, NY, USA, 711–726. https://doi.
org/10.1145/2908080.2908117

[27] Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning
to Transform Natural to Formal Languages. In Proceedings of the 20th
National Conference on Artificial Intelligence - Volume 3 (Pittsburgh,
Pennsylvania) (AAAI’05). AAAI Press, 1062–1068.

[28] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded
Padon, Alex Aiken, and Percy S Liang. 2019.
SPoC: Search-
based Pseudocode to Code. In Advances in Neural Information
Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran
Associates, Inc.
https://proceedings.neurips.cc/paper/2019/file/
7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf

[29] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019.
Automated Program Repair. Commun. ACM 62, 12 (Dec 2019),
56–65. https://cacm.acm.org/magazines/2019/12/241055-automated-
program-repair/fulltext?mobile=false

[30] Jiaxin Li, Yuxi Chen, Haopeng Liu, Shan Lu, Yiming Zhang, Haryadi S.
Gunawi, Xiaohui Gu, Xicheng Lu, and Dongsheng Li. 2018. Pcatch: Au-
tomatically Detecting Performance Cascading Bugs in Cloud Systems.
In Proceedings of the Thirteenth EuroSys Conference (Porto, Portugal)
(EuroSys ’18). Association for Computing Machinery, New York, NY,
USA, Article 7, 14 pages. https://doi.org/10.1145/3190508.3190552
[31] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrit-
twieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno,
Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-
son d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Jo-
hannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J.
Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Fre-
itas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-Level
Code Generation with AlphaCode. DeepMind (2022).

[32] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based
Code Transformation Learning for Automated Program Repair. In
Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering (Seoul, South Korea) (ICSE ’20). Association for Computing

Machinery, New York, NY, USA, 602–614.

[33] H. Lieberman. 2001. Your Wish is My Command: Programming by

Example. Morgan Kaufmann.

[34] Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann,
Tomáš Kočiský, Fumin Wang, and Andrew Senior. 2016. Latent Predic-
tor Networks for Code Generation. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, Berlin, Germany,
599–609. https://doi.org/10.18653/v1/P16-1057

[35] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra.
2019. Aroma: Code Recommendation via Structural Code Search. Proc.
ACM Program. Lang. 3, OOPSLA, Article 152 (Oct. 2019), 28 pages.
https://doi.org/10.1145/3360578

[36] Semantic Machines, Jacob Andreas, John Bufe, David Burkett, Charles
Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach,
Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin
Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan
Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christo-
pher H Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich,
Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy,
Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin,
Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela
Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander
Zotov. 2020. Task-Oriented Dialogue as Dataflow Synthesis. Trans-
actions of the Association for Computational Linguistics 8 (September
2020). https://www.microsoft.com/en-us/research/publication/task-
oriented-dialogue-as-dataflow-synthesis/

[37] Shantanu Mandal, Todd Anderson, Javier Turek, Justin Gottschlich,
Shengtian Zhou, and Abdullah Muzahid. 2021. Learning Fitness
Functions for Machine Programming. In Proceedings of Machine
Learning and Systems, A. Smola, A. Dimakis, and I. Stoica (Eds.),
Vol. 3. 139–155.
https://proceedings.mlsys.org/paper/2021/file/
32bb90e8976aab5298d5da10fe66f21d-Paper.pdf

[38] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Moham-
mad Alizadeh, and Tim Kraska. 2020. Bao: Learning to Steer Query
Optimizers. arXiv:2004.03814 [cs.DB]

[39] Aditya Menon, Omer Tamuz, Sumit Gulwani, Butler Lampson, and
Adam Kalai. 2013. A Machine Learning Framework for Programming
by Example. In Proceedings of the 30th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 28),
Sanjoy Dasgupta and David McAllester (Eds.). PMLR, Atlanta, Georgia,
USA, 187–195. http://proceedings.mlr.press/v28/menon13.html

[40] Microsoft. 2021.

Intellisense in Visual Studio Code.

https://code.

visualstudio.com/docs/editor/intellisense

[41] Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jer-
maine. 2018. Neural Sketch Learning for Conditional Program Gener-
ation. In International Conference on Learning Representations. https:
//openreview.net/forum?id=HkfXMz-Ab

[42] Thanh H.D. Nguyen, Bram Adams, Zhen Ming Jiang, Ahmed E. Hassan,
Mohamed Nasser, and Parminder Flora. 2012. Automated Detection
of Performance Regressions Using Statistical Process Control Tech-
niques. In Proceedings of the 3rd ACM/SPEC International Conference
on Performance Engineering (Boston, Massachusetts, USA) (ICPE ’12).
Association for Computing Machinery, New York, NY, USA, 299–310.
https://doi.org/10.1145/2188286.2188344

[43] Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-
Lezama. 2019. Learning to Infer Program Sketches. In Proceedings of
the 36th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan
Salakhutdinov (Eds.). PMLR, 4861–4870. https://proceedings.mlr.press/
v97/nye19a.html

Locality Optimization for Higher-Order Tensor Computations. In Pro-
ceedings of the 5th ACM SIGPLAN International Symposium on Machine
Programming (Virtual, Canada) (MAPS 2021). Association for Comput-
ing Machinery, New York, NY, USA, 43–52. https://doi.org/10.1145/
3460945.3464955

[45] Daniel Perelman, Sumit Gulwani, Dan Grossman, and Peter Provost.
2014. Test-Driven Synthesis. In Proceedings of the 35th ACM SIGPLAN
Conference on Programming Language Design and Implementation (Ed-
inburgh, United Kingdom) (PLDI ’14). Association for Computing Ma-
chinery, New York, NY, USA, 408–418. https://doi.org/10.1145/2594291.
2594297

[46] Yewen Pu, Zachery Miranda, Armando Solar-Lezama, and Leslie Kael-
bling. 2018. Selecting Representative Examples for Program Syn-
thesis. In Proceedings of the 35th International Conference on Ma-
chine Learning (Proceedings of Machine Learning Research, Vol. 80),
Jennifer Dy and Andreas Krause (Eds.). PMLR, 4161–4170.
http:
//proceedings.mlr.press/v80/pu18b.html

[47] Chris Quirk, Raymond Mooney, and Michel Galley. 2015.

Lan-
guage to Code: Learning Semantic Parsers for If-This-Then-That
Recipes. In Proceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers).
Association for Computational Linguistics, Beijing, China, 878–888.
https://doi.org/10.3115/v1/P15-1085

[48] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. 2019. Language Models are Unsupervised Multitask
Learners.

[49] Alexander J. Ratner, Stephen H. Bach, Henry R. Ehrenberg, Jason Alan
Fries, Sen Wu, and C. Ré. 2019. Snorkel: rapid training data creation
with weak supervision. The Vldb Journal 29 (2019), 709 – 730.
[50] Timo Schick and Hinrich Schütze. 2021. Exploiting Cloze-Questions
for Few-Shot Text Classification and Natural Language Inference. In
Proceedings of the 16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Volume. Association for
Computational Linguistics, Online, 255–269. https://doi.org/10.18653/
v1/2021.eacl-main.20

[51] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGres-
ley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prab-
humoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon
Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad
Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan
Catanzaro. 2022. Using DeepSpeed and Megatron to Train Megatron-
Turing NLG 530B, A Large-Scale Generative Language Model. CoRR
abs/2201.11990 (2022). arXiv:2201.11990 https://arxiv.org/abs/2201.
11990

[52] Armando Solar-Lezama. 2008. Program Synthesis by Sketching. Ph. D.

Dissertation. USA. Advisor(s) Bodik, Rastislav.

[53] Linhai Song and Shan Lu. 2014. Statistical Debugging for Real-World
Performance Problems. SIGPLAN Not. 49, 10 (oct 2014), 561–578. https:
//doi.org/10.1145/2714064.2660234

[54] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundare-
san. 2019. Pythia: AI-Assisted Code Completion System. In Proceed-
ings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining (Anchorage, AK, USA) (KDD ’19). Asso-
ciation for Computing Machinery, New York, NY, USA, 2727–2735.
https://doi.org/10.1145/3292500.3330699

[55] Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J Lim. 2021.
Learning to Synthesize Programs as Interpretable and Generaliz-
able Policies. In Advances in Neural Information Processing Systems,
A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (Eds.).
https://openreview.net/forum?id=wP9twkexC3V

[44] Tharindu R. Patabandi, Anand Venkat, Abhishek Kulkarni, Pushkar
Ratnalikar, Mary Hall, and Justin Gottschlich. 2021. Predictive Data

[56] Yuk Wah Wong and Raymond Mooney. 2006. Learning for Semantic
Parsing with Statistical Machine Translation. In Proceedings of the

10

Human Language Technology Conference of the NAACL, Main Confer-
ence. Association for Computational Linguistics, New York City, USA,
439–446. https://www.aclweb.org/anthology/N06-1056

[57] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, Self-
Supervised Program Repair from Diagnostic Feedback. In Proceedings
of the 37th International Conference on Machine Learning (Proceed-
ings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti
Singh (Eds.). PMLR, 10799–10808. https://proceedings.mlr.press/v119/
yasunaga20a.html

[58] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nes-
ime Tatbul, Jesmin Jahan Tithi, Niranjan Hasabnis, Paul Petersen,

Timothy Mattson, Tim Kraska, Pradeep Dubey, Vivek Sarkar, and
Justin Gottschlich. 2020. MISIM: A Novel Code Similarity System.
arXiv:2006.05265 [cs.LG]

[59] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model
for General-Purpose Code Generation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Vancouver,
Canada, 440–450. https://doi.org/10.18653/v1/P17-1041

[60] Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-
based Neural Abstract Syntax Parser for Semantic Parsing and Code
Generation. In EMNLP.

11

