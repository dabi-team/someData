CORNET: A neurosymbolic approach to learning conditional table formatting
rules by example

Mukul Singh1, Jos´e Cambronero2*, Sumit Gulwani2*, Vu Le2*, Carina Negreanu3*,
Mohammad Raza2*, Gust Verbruggen2*
1 Microsoft, India
2 Microsoft, USA
3 Microsoft, UK
{singhmukul, jcambronero, levu, sumitg, cnegreanu, moraza, gverbruggen}@microsoft.com

2
2
0
2

g
u
A
2
2

]
I

A
.
s
c
[

3
v
2
3
0
6
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Spreadsheets are widely used for table manipulation and pre-
sentation. Stylistic formatting of these tables is an important
property for both presentation and analysis. As a result, pop-
ular spreadsheet software, such as Excel, supports automat-
ically formatting tables based on data-dependent rules. Un-
fortunately, writing these formatting rules can be challenging
for users as that requires knowledge of the underlying rule
language and data logic. In this paper, we present CORNET, a
neuro-symbolic system that tackles the novel problem of auto-
matically learning such formatting rules from user examples
of formatted cells. CORNET takes inspiration from inductive
program synthesis and combines symbolic rule enumeration,
based on semi-supervised clustering and iterative decision tree
learning, with a neural ranker to produce conditional format-
ting rules. To motivate and evaluate our approach, we extracted
tables with formatting rules from a corpus of over 40K real
spreadsheets. Using this data, we compared CORNET to a
wide range of symbolic and neural baselines. Our results show
that CORNET can learn rules more accurately, across varying
conditions, compared to these baselines.

Introduction

Tables are a popular form of representing structured data
for understanding and sharing. Spreadsheets are the most
common table manipulation software, with around a billion
monthly active users (N. 2022). Formatting is a fundamental
and frequently used visual aid to better display, highlight
or distinguish between data points in a spreadsheet. In an
analysis of a large public spreadsheet corpus (Barik et al.
2015; Fisher and Rothermel 2005), we ﬁnd that close to 25%
of spreadsheets have some formatting to present data.

Conditional formatting (CF) is a feature that automates
table formatting based on user-deﬁned data-dependent rules.
It is a standard feature in all major spreadsheet manipulation
tools, such as Microsoft Excel, Google Sheets and Apple
Numbers. All these tools support predeﬁned templates for
popular rules, such as value greater than a speciﬁc value.
Excel and Sheets also support custom rules, where a user
can enter a boolean-valued formula to format cells. Figure 1
illustrates adding CF Rule in Excel with a real example.

*Listed in alphabetical order

Figure 1: Adding a CF Rule in Excel: 1 CF Option in Styles
Pane; 2 CF Dropdown Menu to add new rule 3 CF New
Rule Dialog box; 4 Resulting formatted column from rule.
After the user formats two cells, CORNET automatically sug-
gest the intended CF rule for the user.

Unfortunately, creating conditional formatting rules re-
quires users to understand the syntax and logic behind them.
As of June 2022, more than 10,000 conditional formatting re-
lated questions were posted on the Excel tech help community
alone (Excel 2022). By analyzing the posts we discovered
three key struggles that prevent users from using CF to its
full potential. First, many users are unaware of this feature
and manually apply formatting to their spreadsheets. Second,
when a user does enter a CF rule, they sometimes fail to do
so correctly and end up manually formatting the sheet. Third,
even when users write valid rules that matches their desired
formatting, the rules are either unnecessarily complex (can be
rewritten to much simpler rules), or not generalizable (may
produce wrong formatting when the columns change).

Example 1 Figure 2 shows a case from a public spreadsheet
where the user wanted to highlight cells with value < 5, but
wrote an incorrect rule and had to manually format the sheet.

Some of these struggles are reinforced by limitations in the
interface used to apply conditional formatting. For example,
using logical operations like OR and AND requires writing
custom formulas. Additionally, there is no validation for data
types, which can cause surprising results, for example, it is
possible to write a rule that formats cells whose value is less
than 5 for a text column.

 
 
 
 
 
 
Figure 2: Example of incorrectly using CF: 1 a user deﬁned
a custom rule that always evaluates to false (it compares the
cell to the string “5” instead of the number 5); 2 because
the rule does not format any cell, the user had to manually
format the sheet; 3 the correct rule for the formatting.

In this paper we present CORNET (Conditional
ORNamentation by Examples in Tables), a system that al-
lows users to automatically generate a formatting rule from
examples, thus obviating the challenges outlined. CORNET
takes a small number of user formatted cells as input to learn
the most likely formatting rule that generalizes to other cells.
CORNET explores possible predicates for the target column,
hypothesizes cell grouping via semi-supervised clustering
and then learns candidate rules by employing an iterative tree
learning procedure. Since multiple rules match the examples,
CORNET uses a neural ranker to return the most likely CF
rule to the user. For example, in Fig. 1, after the user formats
two cells, CORNET suggests the intended CF rule.

Traditional programming-by-example (PBE) systems (Gul-
wani 2011; Le and Gulwani 2014) can typically derive useful
search constraints by relating properties of the outputs to the
inputs provided (e.g. an output text may share spans with
an input text). This is challenging to do in CF learning as
the user only provides what amounts to a small number of
formatting labels. The predicate generation and clustering
step in CORNET mitigates this by generating and applying
simple predicates, which jointly can help hypothesize such
formatting labels for the entire target column. Once these
hypothesized labels are available, we can apply our iterative
rule enumeration procedure. Much like other PBE problems,
we enumerate multiple possible programs consistent with our
hypothesized outputs. Our enumeration uses tree learning
as we can easily enforce consistency over user-provided ex-
amples. The learning process is iterative to induce diversity
of candidate rules. Finally, we employ a ranker that can dis-
ambiguate between these competing programs by capturing
properties not just of the rule but also of the underlying data
and the execution of the rule on that data.

To evaluate CORNET, we created a benchmark of 968
real user tasks from public Excel spreadsheets. CORNET
outperforms symbolic and neural baselines that were adapted
to our problem, by accurately learning conditional formatting
rules with as few as two or three formatted examples.
This paper makes the following key contributions:

• Based on the observation that users often struggle to for-
mat their tabular data, we introduce the novel problem of
learning conditional formatting rules from examples.
• We propose a neurosymbolic system CORNET to learn
CF rules from examples over tabular data. CORNET is
based on a novel PBE methodology that leverages few

examples to predict more examples and then generalize
them into an intended CF rule. This is done via a novel
combination of symbolic (predicate set) enumeration, ML
methods of semi-supervised clustering (to predict more
examples using predicate set) and iterative tree learning
(to propose candidate rules), and a neural ranker (to pick
an intended rule).

• We create a dataset of 968 real formatting tasks. We re-

lease this dataset to encourage future research.

• We evaluate CORNET extensively and show that it outper-

forms the baselines by 20–25% on our benchmark.

Conditional formatting is often a preliminary step to per-
forming further sophisticated analysis such as conditional
transformations for data cleaning or visualizations. Our work
on automating conditional formatting shall motivate research
in automating such more encompassing workﬂows.

Problem Deﬁnition

Let C = [ci]n
i=1 be a column of n cells with each cell ci
represented by a tuple (vi, ti) of its value vi ∈ V and its
annotated type ti ∈ T . In this paper, we consider string,
number, and date as possible types—these are available in
most spreadsheet software. We associate a format identiﬁer
fi ∈ N0 (or simply format) with each cell, which corresponds
to a unique combination of formatting choices made by the
user. A special identiﬁer f⊥ = 0 is reserved for cells without
any speciﬁc formatting. In this paper, we consider cell ﬁll
color, font color, font size, and cell borders.

A conditional formatting rule (or simply rule) is a function
r : C → N0 that maps a cell to a formatting identiﬁer. Given a
column C and speciﬁcation, the goal of automatic formatting
is to ﬁnd a rule r such that r(ci) = fi for all ci ∈ C.

Let C(cid:63) = {ci | ci ∈ C, fi (cid:54)= f⊥} be the cells with format-
ting applied. The goal of automatic formatting by example is
to ﬁnd r given only a small, observed subset Cobs ⊂ C(cid:63) and
throughout this work we will refer to the elements of Cobs
as formatted examples. Any cell in C \ Cobs is considered
unlabeled, including unformatted cells.

The examples in Cobs do not provide a complete problem
speciﬁcation. To ﬁnd the best r, we cast the problem as a
heuristic search over candidate rules R. Let h : R × Cn → R
be a function that takes a rule r and a column C and returns
an estimate of the quality of r with respect to C—how likely
it is that a user would have wanted this rule. Given a column
C with only a few formatted cells we then want to ﬁnd the
rule r∗ = arg maxr∈R h(r, C) that maximizes this heuristic.
In this regard, learning CF rules shares motivations with
multilabel classiﬁcation with limited and noisy supervision.
For automatic conditional formatting by example we re-
quire a space of possible rules R, a method to enumerate
candidate rules r ∈ R and a method of evaluating the qual-
ity of such rules. In the following section we introduce our
approach, aided by an analysis of real user spreadsheets.

Approach
This section describes how CORNET learns rules from a
small number of examples. We restrict rules to being logical

Table 1: Supported predicates and their arguments for each
data type. The d argument in datetime predicates determines
which part of the date is compared—day, month, year, or
weekday. For example, greater(c, 2, month) matches datetime
cells with a date in March or later for any year.

Numeric

Datetime

Text

greater(c, n)
greaterEquals(c, n)
less(c, n)
lessEquals(c, n)
between(c, n1, n2)

greater(c, n, d)
greaterEquals(c, n, d)
less(c, n, d)
lessEquals(c, n, d)
between(c, n1, n2, d)

equals(c, s)
contains(c, s)
startsWith(c, s)
endsWith(c, s)

combinations of boolean-valued properties of cells. Still, the
space of combinations of predicates is much larger than the
space of possible outputs. Rather than directly learning a
program that generalizes to unlabeled cells, we therefore ﬁrst
predict the expected output on the whole column.

Figure 3 shows a schematic overview of CORNET. Step
1 enumerates cell properties using predicates. Step 2 com-
putes the expected output using semi-supervised clustering.
Once this is known, we enumerate programs that match this
output in step 3 , and rank them in step 4 . The following
sections describe the challenges and solutions for each step.

Predicate Generation
This step enumerates a set of properties that hold for a strict
subset of the cells of the given column. Each property is en-
coded as a predicate—a boolean-valued function that takes a
cell c, zero or more arguments and returns true if the property
that it describes holds for c. To avoid redundancy, all predi-
cates are assigned a type ti and they only match cells of their
type. Supported predicates are shown in Table 1.

Given a column of cells and a predicate, the goal is to
initialize each additional argument to a constant value such
that the predicate returns true for a strict subset of the column.
Table 2 shows an overview of how the additional argument
values are initialized for predicates of each type.

Semi-supervised Clustering
Rather than immediately combine predicates into rules, we
ﬁrst predict the expected output of the rules on the column’s
unobserved (i.e., unformatted) cells. This allows us to lever-
age the relatively small search space of output conﬁgurations
over cell properties to ﬁnd programs that generalize to sim-
ilar cells. CORNET biases the output using a programmatic
vocabulary of predicates instead of relying on arbitrary cell
clustering. (Padhi et al. 2017) uses the same concept but with
the programmatic vocabulary of regular expressions.

More concretely, we assign a (potentially noisy) formatting
label ˆfi to each unobserved cell ci /∈ Cobs by building on
two insights. First, tables are typically annotated by users
from top to bottom, which implies that there is positional
information available. In particular, cells ci /∈ Cobs such that
there exist cj, ck ∈ Cobs for which j < i < k are likely
intended to have no formatting associated with them. We
refer to this set of ci as soft negative examples (Raza and

Table 2: Overview of constants for concretizing predicates
of each type. For example, we generate constants for text
predicates from two token sources.

Type

Arg(s)

Values

numeric
numeric

n
n

numeric
numeric

n
n1 and n2

text

text
date

s

s
n and d

all numbers that occur in the column
summary statistics: mean, min, max,
and percentiles
popular constants such as 0, 1 and 10n
use numeric generators for n and keep
the ones n1 < n2
tokens obtained by splitting on non-
alphanumeric delimiters
tokens from preﬁx trie
for available d, extract numeric value
and use generator for n

Gulwani 2020). Second, user provided examples Cobs should
be treated as hard constraints—we assume that the user has
provided their formatting goals without errors.

We perform iterative clustering over k clusters, where k is
the number of unique format identiﬁers for our column, plus
one cluster for unassigned cells. The distance between two
cells is the size of the symmetric difference between the sets
of predicates that hold for either cell. Let clusterf be the clus-
ter associated with format f . Some supervision is introduced
by initializing each cell ci ∈ Cobs to clusterfi and soft nega-
tive example cells to cluster0. These cells are never assigned
to another cluster. The remaining cells Cu are assigned to
the unknown formatting cluster, labeled clusteru. Taking in-
spiration from k-medoids (Kaufman and Rousseeuw 2009)
we iteratively reassign cu ∈ Cu to a new cluster. Instead
of computing a cluster medoid, however, we combine the
minimal and maximal distance to any element of the cluster.
This is computationally much more efﬁcient (linear instead
of quadratic in the number of distance computations) and
was found to perform well in practice. When clusters become
stable or a maximal number of iterations is reached, each
cell takes the format value of their associated cluster, with
clusteru added to cluster0. If ci ∈ Cobs, we have ˆfi = fi.

Candidate Rule Enumeration
After clustering, we have a target formatting label ˆfi for each
ci in C. We now learn a set of candidate rules R such that
r(ci) = ˆfi for all r ∈ R. We deﬁne the space of rules and a
search procedure in the following two subsections.

Predicates to Rules A rule in CORNET for a column C
consists of a set of tuples (rf , f ) with rf : C → B a function
that takes a cell and returns a boolean and f ∈ N0 a format
identiﬁer. These rf are disjoint with respect to C such that for
each c ∈ C, there is at most one rf such that rf (c) = true and
we say that rf matches c. For a given cell, the rule returns the
associated f if it evaluates to true. The cell is left unformatted
if all rf evaluate to false. CORNET supports rf that can be
built as a propositional formula in disjunctive normal form
over predicates. In other words, every rf is of the form

( p1(c) ∧ p2(c) ∧ . . . ) ∨ ( pj(c) ∧ pj+1(c) ∧ . . . ) ∨ . . .

Figure 3: Proposed system architecture illustrated through the example case from Figure 1: 0 input table with partial formatting,
1 predicate generation for all cells in the table, 2 semi supervised clustering using examples and other cells to address the
challenge of unlabeled cells, 3 enumerating rules based on the clustering using multiple decision trees, 4 neural ranker to
score generated rules, and 5 ﬁnal learned conditional formatting rule.

with pi a generated predicate or its negation. Our goal is to
strike a balance between expressiveness and simplicity.

Enumerating Rules Despite the large search space, we
can generate candidates by learning rules for each format
separately in a one versus all setting. In this setting, the rule
for a format f is expected to consist of predicates that either
hold for many cells with format f and few with other formats,
or vice versa. Taking this one step further, the combination of
predicates in a conjunction in one rf should either only hold
for cells with format f or not hold for any cell with format f .
Based on this intuition, we greedily enumerate promising
candidates by iteratively learning decision trees and using
predicates as features. Each decision tree then corresponds
to a rule in disjunctive normal form (Blockeel and De Raedt
1998). We identify and address three challenges: variety in
rules, simplicity of rules, and coping with noisy labels. To
ensure variety, the root feature is removed from the set of
candidates after each iteration. To ensure simplicity, we only
accept decision trees with λn or fewer nodes. To deal with
noisy labels, we only require decision trees to have perfect
accuracy on observed examples, we consider labeled cells to
be twice as important as unlabeled ones and we stop learning
more rules once the accuracy falls below a threshold. This
learning procedure is shown in Algorithm 1.

Algorithm 1: Rule Enumeration using Decision Trees

Require: Matrix of features P and vector of labels Y
Ensure: A set of candidate rules
1: rules ← {}
2: forbidden ← {}
3: while accuracy ≥ λa and |P \ forbidden| > 0 do
4:
5:
6:
7:
8:
9: return rules

tree ← FIT(P \ forbidden, Y )
accuracy ← EVALUATE(tree, P \ forbidden, Y )
forbidden ← forbidden ∪ ROOT(tree)
if tree satisﬁes user examples ∧ accuracy ≥ λn then

rules ← rules ∪ RULES(tree)

Candidate Rule Ranking

We ﬁrst assign scores to rules for each format separately and
combine and rank them later. Rules for different formats must
be disjoint with respect to the given column. Combined rules
are ranked by the highest total score. We use this section to
describe how to assign scores to rules rf for one format.

Rules can be ranked based on output features (Natarajan
et al. 2019) or rule features (Ellis and Gulwani 2017). The
proposed neural ranker combines information from both. In-
formation about the rule is captured by handpicked features.
Information about the column data is captured by turning it
into a sequence of words and using a pre-trained language
model (Devlin et al. 2019) to obtain cell-level embeddings.
These embeddings are augmented with information about
the execution of the rule through cross-attention (Lee et al.
2018). Both vectors of information are merged by concate-
nating them and learning weights that produce a single score.
This score thus combines both syntactic (rule) and semantic
(data and execution) information. For our task, using simple
heuristics such as “choose the shortest rule that matches user
examples” does not necessarily work. For example, in Fig-
ure 3 the gold rule is longer than other candidates consistent
with the user’s examples. Figure 4 shows an overview of our
ranker, further detailed in the appendix.

We train the model by treating this problem as binary clas-
siﬁcation (i.e., a rule candidate matches the gold CF rule) and
using the output of the ﬁnal activation as the score. Training
data is generated by using CORNET up to rule enumeration
on a held-out dataset of columns with conditional formatting.
We provide 1, 3 or 5 examples and only keep rules that do
not match the gold as negative samples. Additionally, we
apply gold rules on other columns to obtain both positive (by
construction) and negative (by the procedure above) exam-
ples. All gold rules in this dataset are also used. This process
results in approximately 3500 examples for our model.

Evaluation
We perform experiments to answer the following questions:

Q1. How does CORNET compare to baselines?

Figure 4: Ranking model architecture: 1 inputs to the model are the data column and the rule to be scored; 2 the column
encoding model pools BERT token embeddings, passes them through cross attention with the rule’s execution outputs (i.e.
formatted or not), and then through a linear layer; and 3 the resulting embedding is concatenated with manually-engineered
rule features and fed into a ﬁnal linear layer which outputs the score after applying a sigmoid activation.

Table 3: Summary of rules in our benchmark with number of
rules, average number of cells, average number of formatted
cells, and depth of CF rule (by parsing into our grammar).

Type

Rules

# Cells

# Formatted Rule Depth

Text
Numeric
Date
Total

586
329
53
968

54.8
72.6
28.9
59.5

17.1
25.0
10.4
19.4

2.1
1.7
1.8
1.9

Q2. How does the input table conﬁguration affect learning?
Q3. How do design decisions impact our effectiveness?

We ﬁrst describe the components we built to evaluate COR-
NET: our benchmark, evaluation metrics and baselines.

Benchmark To evaluate CORNET, we created a benchmark
by crawling 22,000 publicly available Excel workbooks from
the web. Among these, 2,500 workbooks contain at least one
CF rule added by users, from which we extracted 10,000
CF rules and their corresponding cell values and formatting.
After removing duplicates and restricting to rules that operate
on at least ﬁve cells and format at least two of them, we retain
968 tasks. Table 3 shows a summary of the benchmark. Text
rules are the most popular, followed by numeric then date.

Evaluation Metrics We consider two metrics: exact match
and execution match. Exact match is a syntactic match be-
tween a conditional rule candidate and the gold rule, with tol-
erance for differences arising from white space, alternative ar-
gument order, and a set of basic semantics-preserving rewrite
rules. Execution match consists of executing a rule and com-
paring the produced formatting to the gold formatting—there
is an execution match if the two formattings are identical.
This distinction between exact and execution match is also
made in related areas, such as natural language to code (Poe-
sia et al. 2022; Liguori et al. 2022).

Baselines As existing systems do not target CF directly,
we adapt as baselines a variety of symbolic and neural sys-
tems designed for general classiﬁcation or table tasks. We
implemented six symbolic and three neural baselines:

• Three decision tree variants: We ﬁt a standard decision
tree, treating user (un)formatted cells as (negative) posi-
tive labels, respectively. Numeric and datetime columns
are fed directly into the decision tree, while text columns
are encoded using an ordinal label encoder. We explore
variants of this baseline where we learn a decision tree
over our generated predicates (P ) and add our ranker. We
report the best performance across hyper-parameters.
• Popper (Cropper and Morel 2020): We use the state-of-
the-art inductive logic programming (ILP) tool by casting
CF rules as an ILP problem over the same space of rules
as CORNET. Similar to the decision tree baseline, we
consider raw column values as well as representing these
with our generated predicates.

• COP-KMeans (Wagstaff et al. 2001): Learning CF rules
can be treated as a constrained clustering problem. COP-
KMeans is a k-means based clustering strategy that sup-
ports linkage constraints for clusters.

• TAPAS (Herzig et al. 2020), TUTA (Wang et al. 2020)
and TaBERT (Yin et al. 2020): We use three neural table
encoders that are trained for different objectives. TAPAS
is a BERT model trained to answer questions over tables,
TUTA is a tree-based transformer, pre-trained (as one of
its objectives) for cell type classiﬁcation and TaBERT is
an encoder trained to summarize table information. We
also present a custom neural baseline which uses cell
level embeddings and a cross attention over formatted and
unformatted cells (detailed architecture in the appendix).
Starting from pre-trained versions, we further train the
models on a collection of 50,000 CF rules that are synthet-
ically generated by randomly formatting cells in unfor-
matted columns and using CORNET to learn CF rules. We
ﬁne-tune on a held-out set of 1650 gold user-written CF
rules that were not used for creating benchmarks. Since
these models only predict whether a cell is formatted or
not, we only evaluate them on execution match.

Q1. Comparison against baselines
Table 4 presents an overview of our results. CORNET out-
performs both symbolic and neural baselines using both rule
and execution match metrics. CORNET is also able to learn

Table 4: Comparison of CORNET with both published (Pub.) and new baselines. The performance is evaluated on exact and
execution match for 1, 3 and 5 user formatted examples (ex.). The column Rules denotes if a system generates symbolic rules or
just the output formatting. CORNET outperforms both neural and symbolic baselines in both execution and exact rule match.

System Description

Execution Match

Rule Match

Name

Technique

Rules

Pub.

1 ex.

3 ex.

5 ex.

1 ex.

3 ex.

5 ex.

Decision Tree
Predicates + Decision Tree
Predicates + Decision Tree + Ranking
Popper
Popper + Predicates
Constrained Clustering
TUTA for Cell Type Classiﬁcation
TAPAS + Cell Classiﬁcation
BERT + Cell Classiﬁcation (Custom)
CORNET

Yes
Symbolic
Yes
Symbolic
Yes
Symbolic
Yes
Symbolic
Yes
Symbolic
No
Symbolic
No
Neural
No
Neural
Neural
No
Neuro-symbolic Yes

No
No
No
Yes
Yes
Yes
Yes
Yes
No
No

44.8
54.8
55.2
57.3
57.9
54.4
57.1
40.1
37.8
65.4

56.4
67.9
69.9
62.5
63.6
61.3
63.5
52.2
53.4
77.6

60.1
71.7
74.3
66.4
68.2
65.9
67.8
55.2
57.8
82.5

14.4
42.9
43.2
43.4
43.7
–
–
–
–
46.2

18.9
50.1
51.2
52.1
52.8
–
–
–
–
54.6

19.6
51.5
52.2
54.7
55.2
–
–
–
–
60.7

Figure 5: Examples showing CORNET’s ability to generate
simpler rules equivalent to the user’s rules. The top (red) rule
is user-written and the bottom (black) is CORNET-generated.

CF rules with fewer examples than baselines. We found that
although CORNET consistently learns better rules than the
symbolic baselines, sometimes the neural baselines succeed
while CORNET fails. This happens when CORNET is given
very few examples, and as a result, its learning procedure
does not generalize to a more complex rule. For example, to
learn a rule with OR CORNET requires at least 2 formatted
examples. Furthermore, unlike neural models that can color
cells arbitrarily, CORNET is bound by a grammar. Because we
design our grammar to capture common CF patterns, COR-
NET may not support scenarios that require arbitrary Excel
formulas. However, such cases are rare in practice (11 such
cases in our benchmark). The Appendix presents an extended
analysis and a visual comparison of CORNET with baselines
on selected examples.

Execution versus exact match. Additional analysis revealed
that in 56% of the cases where CORNET’s generated rules
matched in execution, but were not an exact match, the user
written rules could be simpliﬁed. To explore CORNET’s po-
tential to simplify user rules, we created a dataset of 1000 CF
rules, randomly sampled from the held-out data used to ﬁne
tune neural baselines. We found that CORNET can simplify
117 rules by using fewer predicates. Figure 5 presents two
examples and further examples can be found in the Appendix.

Q2. Impact of Input Conﬁguration

Next we study how CORNET’s performance varies with the
number of formatted examples and unformatted cells.

The number of formatted examples. Fig. 6 shows the exe-
cution match accuracy over the number of examples across

Figure 6: Execution match over the number of formatted
examples for different column data types. CORNET has higher
accuracy for Text and DateTime columns. Numeric columns
need more examples to converge to the correct rule, given the
larger search space.

different column types. The convergence rates vary signiﬁ-
cantly per column type. For example, the performance growth
diminishes for Text after 2 examples, but steadily increases
for Numeric even after 14 examples. This is because numeric
rules are harder to learn (the numeric value can be anything
from a cell value, an aggregated value, or a constant). Being
aware of such differences is important as we could proac-
tively suggest text rules after a couple of examples, whereas
for numeric rules we should be more conservative.

The number of unformatted cells. When deploying systems
like CORNET data availability might be restricted (e.g., in
browsers or on mobile devices), which led us to investigate
the impact of the number of unformatted cells on CORNET’s
performance, as in practice, we have signiﬁcantly more un-
formatted cells than formatted examples. Our results show
that performance growth stabilizes after 30 unformatted cells.
Most browsers have at least 30 cells in their view. Refer to
the Appendix for a detailed convergence plot.

Q3. Impact of design decisions on effectiveness
Our ablation studies, summarized in Table 5, show that dif-
ferent components of CORNET (clustering, use of positional

Table 5: Execution match for the top rule for 1, 3 and 5
user formatted examples (ex.). “–” means the corresponding
component is removed, Iter. Enum. stands for iterative rule
enumeration. Full CORNET outperforms all ablations.

Table 6: Execution match within Top-k candidates with 3
formatted examples for different ranking models. Top-All
represents the performance of an oracle ranker. CORNET’s
ranker improves over both ablated rankers.

Model

1 ex.

3 ex.

5 ex.

Ranker

Top-1 Top-3 Top-5 Top-10

Top-All

CORNET – Clustering
CORNET – Iter. Enum.
CORNET – Iter. Enum. – Clustering

CORNET without negatives
CORNET with hard negatives

CORNET

54.5
60.5
52.0

60.3
62.2

65.4

72.4
73.8
70.4

73.7
75.4

77.6

77.5
80.5
75.8

79.6
80.1

82.5

information through soft negative examples, iterative rule
enumeration through repeated tree learning, and neurosym-
bolic ranker) contribute substantially to its performance.

Clustering and Iterative Rule Enumeration. We compare
CORNET with three ablated versions (1) without clustering,
(2) without iterative rule enumeration, and (3) without either.
The approach without iterative rule enumeration ﬁrst enu-
merates trees but only extracts a rule from the tree with the
highest accuracy on the extended labels (see Algorithm 1).
Table 5 shows that clustering and rule enumeration have the
highest impact on performance, demonstrating the value of
our output-oriented synthesis procedure and our use of iter-
ated tree learning to produce multiple viable rule candidates
(which are then disambiguated by the ranker).

Hard versus Soft Examples. We compare the performance
of CORNET with: (a) no negative examples, and (b) treat-
ing all negative examples as hard constraints. As Table 5
shows, performance drops in both cases. Intuitively, negative
examples capture positional information associated with a
top down ﬁlling order in columns which is beneﬁcial infor-
mation for CORNET. However we ﬁnd that treating negative
examples as hard constraints can lead to overﬁtting, and thus
negative examples should be treated as soft constraints.

Neural versus Symbolic Ranker. We compare CORNET’s
neurosymbolic ranker with two ablated versions (1) a pure
symbolic ranker obtained by keeping only the handpicked
features and removing the column encoding module, and (2)
a pure neural ranker obtained by replacing the hand picked
features with a CodeBERT (Feng et al. 2020) encoding of
the CF rule. Table 6 shows that CORNET’s ranker outper-
forms the symbolic and neural ablations across all Top-k
conﬁgurations. Furthermore the execution match accuracy
gap between CORNET’s Top-1 and the oracle ranker (Top-
All) is around 6% suggesting that future work should focus
on improving rule enumeration, rather than rule ranking.

Related Work
Despite the needs for CF in spreadsheet systems, there
have been relatively few formal studies on this feature.
(Abramovich et al. 2004) discusses how CF in Excel can
improve the demonstration of mathematical concepts. (Dong
et al. 2020) describes CellGAN, a conditional Generative Ad-
versarial Network model which focuses on borders and align-
ment of cells to learn hierarchical headers and data groups in

Symbolic
Neural
CORNET

74.3
65.7
77.6

76.1
69.2
79.7

78.4
74.5
80.2

80.6
75.6
83.3

83.7
83.7
83.7

tables. In contrast, CORNET targets value based formatting
of cells and also generates their associated formatting rules.
CORNET uses a program-by-example (PBE) paradigm,
which has been popularized by systems like FlashFill (Gul-
wani 2011) and FlashExtract (Le and Gulwani 2014). These
systems, which are available in Excel, learn string transfor-
mation and data extraction programs from few input-output
examples. (Raza and Gulwani 2017) ﬁnds outputs and pro-
grams together, while (Natarajan et al. 2019) ﬁnds programs
and then ranks them based on output. In contrast, CORNET
ﬁrst hypothesizes the outputs (cell formats) and then learns
the associated rule. CORNET is the ﬁrst system to take an
“output-ﬁrst” synthesis approach motivated by the fact that in
this case output space is much smaller than program space.
In terms of search techniques, (Polozov and Gulwani 2015)
uses goal-driven top-down symbolic backpropagation. This
is not applicable in our setting because the boolean signal
(i.e., is a cell in a particular format group) is too-weak to
derive strong-enough constraints to navigate the search space.
Another popular alternative in PBE is bottom-up enumera-
tion (Odena et al. 2021), but our search space is too large.

Past PBE work has ranked programs using program fea-
tures (Ellis and Gulwani 2017) or output features (Natarajan
et al. 2019). CORNET uses a neural ranking model that com-
bines both the rule (program) and its execution (output).

Neural approaches have previously been applied in vari-
ous table tasks. For example, TaBERT (Yin et al. 2020) and
TAPAS (Herzig et al. 2020) are popular Sequential Question
Answering systems that use a neural model to encode the ta-
ble and query vector. TUTA (Wang et al. 2020) is another sys-
tem for cell and table type classiﬁcation tasks. TabNet (Sun,
Rayudu, and Pujara 2021) uses a neuro-symbolic model to
understand relational structure of data in tables by predicting
cell types. Unlike these systems, CORNET targets the task of
learning table formatting rules from examples.

Conclusion
We introduce the problem of learning conditional formatting
rules for spreadsheet data from user examples. We introduce
CORNET, a neuro-symbolic system that learns such data-
dependent rules from few examples. To evaluate CORNET,
we created a benchmark of 968 CF tasks from real Excel
sheets. We compare CORNET to both symbolic and neural
approaches on this benchmark and ﬁnd that it performs sig-
niﬁcantly better. This paper opens future work such as purely
predictive CF rule learning and combining multiple input
modalities to capture user formatting intent. This paper also

acts as the initial step to performing more complex anal-
ysis such as conditional transformations for data cleaning,
or visualizations. This work shall inspire more research in
automating workﬂows that include such downstream tasks.

Acknowledgements
We would like to thank Almog-Ben Kandi, Sophie Gerzie,
Avital Nevo, and Yoav Hayun for their feedback on this
research.

References
Abramovich, S.; Sugden, S.; Abramovich, S.; and Sugden,
S. J. 2004. Spreadsheet Conditional Formatting: An Un-
tapped Resource for Mathematics Education. Spreadsheets
in Education, 85105.
Barik, T.; Lubick, K.; Smith, J.; Slankas, J.; and Murphy-Hill,
E. 2015. Fuse: a reproducible, extendable, internet-scale cor-
pus of spreadsheets. In 2015 IEEE/ACM 12th Working Con-
ference on Mining Software Repositories, 486–489. IEEE.
Blockeel, H.; and De Raedt, L. 1998. Top-down induction
of ﬁrst-order logical decision trees. Artiﬁcial intelligence,
101(1-2): 285–297.
Cropper, A.; and Morel, R. 2020. Learning programs by
learning from failures. CoRR, abs/2005.02259.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. ArXiv, abs/1810.04805.
Dong, H.; Wang, J.; Fu, Z.; Han, S.; and Zhang, D. 2020.
Neural Formatting for Spreadsheet Tables. In Proceedings
of the 29th ACM International Conference on Information &
Knowledge Management, CIKM ’20, 305–314. New York,
ISBN
NY, USA: Association for Computing Machinery.
9781450368599.
Ellis, K.; and Gulwani, S. 2017. Learning to Learn Programs
from Examples: Going Beyond Program Structure. In IJCAI
2017.
Excel, M. 2022.
https:
//techcommunity.microsoft.com/t5/forums/searchpage/tab/
message?q=conditional\%20formatting. Last Accessed:
2022-06-30.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;
Shou, L.; Qin, B.; Liu, T.; Jiang, D.; and Zhou, M. 2020.
CodeBERT: A Pre-Trained Model for Programming and Nat-
ural Languages. CoRR, abs/2002.08155.
Fisher, M.; and Rothermel, G. 2005. The EUSES spreadsheet
corpus: a shared resource for supporting experimentation
with spreadsheet dependability mechanisms. In Proceedings
of the ﬁrst workshop on End-user software engineering, 1–5.
Gulwani, S. 2011. Automating String Processing in Spread-
sheets using Input-Output Examples. In PoPL’11, January
26-28, 2011, Austin, Texas, USA.
Gulwani, S.; Le, V.; Radhakrishna, A.; Radicek, I.; and Raza,
M. 2020. Structure interpretation of text formats. In Object-
Oriented Programming, Systems, Languages & Applications
(OOPSLA). ACM.

Excel Tech Help Forum.

Herzig, J.; Nowak, P. K.; M¨uller, T.; Piccinno, F.; and Eisen-
schlos, J. M. 2020. Tapas: Weakly Supervised Table Parsing
via Pre-training. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics (Volume 1:
Long Papers). Seattle, Washington, United States.
Hurst, N.; Marriott, K.; and Moulder, P. 2005. Toward tighter
tables. In Proceedings of the 2005 ACM symposium on Doc-
ument engineering, 74–83.
Kaufman, L.; and Rousseeuw, P. J. 2009. Finding groups in
data: an introduction to cluster analysis. John Wiley & Sons.
Le, V.; and Gulwani, S. 2014. FlashExtract: a framework for
data extraction by examples. In 2014 Programming Language
Design and Implementation, 542–553. ACM.
Lee, K.-H.; Chen, X.; Hua, G.; Hu, H.; and He, X. 2018.
Stacked Cross Attention for Image-Text Matching.
Liguori, P.; Al-Hossami, E.; Cotroneo, D.; Natella, R.; Cukic,
B.; and Shaikh, S. 2022. Can We Generate Shellcodes via Nat-
ural Language? An Empirical Study. CoRR, abs/2202.03755.
Lin, X. 2006. Active layout engine: Algorithms and appli-
cations in variable data printing. Computer-Aided Design,
38(5): 444–456.
N., J. 2022. Number of Google Sheets and Excel Users
Worldwide. https://askwonder.com/research/number-google-
sheets-users-worldwide-eoskdoxav. Last Accessed: 2022-
07-30.
Natarajan, N.; Simmons, D.; Datha, N.; Jain, P.; and Gulwani,
S. 2019. Learning Natural Programs from a Few Examples
in Real-Time. In AIStats.
Neuwirth, E.; and Arganbright, D. 2003. The Active Mod-
eler: Mathematical Modeling With Microsoft Excel. Duxbury
Press.
Odena, A.; Shi, K.; Bieber, D.; Singh, R.; and Sutton, C.
2021. BUSTLE: Bottom-up program-Synthesis Through
Learning-guided Exploration. ArXiv, abs/2007.14381.
Padhi, S.; Jain, P.; Perelman, D.; Polozov, O.; Gulwani, S.;
and Millstein, T. D. 2017. FlashProﬁle: Interactive Synthesis
of Syntactic Proﬁles. CoRR, abs/1709.05725.
Poesia, G.; Polozov, O.; Le, V.; Tiwari, A.; Soares, G.;
Meek, C.; and Gulwani, S. 2022. Synchromesh: Reliable
code generation from pre-trained language models. CoRR,
abs/2201.11227.
Polozov, O.; and Gulwani, S. 2015. FlashMeta: A Framework
for Inductive Program Synthesis. SIGPLAN Not., 50(10):
107–126.
Raggett, D.; Hors, A.; and Jacobs, I. 1999. HTML 4.01
speciﬁcation, section ‘autolayout algorithm’.
Raza, M.; and Gulwani, S. 2017. Automated Data Extraction
using Predictive Program Synthesis. In AAAI 2017. Associa-
tion for the Advancement of Artiﬁcial Intelligence.
Raza, M.; and Gulwani, S. 2020. Web data extraction us-
ing hybrid program synthesis: A combination of top-down
and bottom-up inference. In Proceedings of the 2020 ACM
SIGMOD International Conference on Management of Data,
1967–1978.

Sun, K.; Rayudu, H.; and Pujara, J. 2021. A Hybrid Proba-
bilistic Approach for Table Understanding. Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, 35(5): 4366–
4374.
Wagstaff, K.; Cardie, C.; Rogers, S.; and Schr¨odl, S. 2001.
Constrained K-Means Clustering with Background Knowl-
edge. In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, ICML ’01, 577–584. San Fran-
cisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN
1558607781.
Wang, Z.; Dong, H.; Jia, R.; Li, J.; Fu, Z.; Han, S.; and Zhang,
D. 2020. Structure-aware Pre-training for Table Understand-
ing with Tree-based Transformers. CoRR, abs/2010.12537.
Yin, P.; Neubig, G.; Yih, W.-t.; and Riedel, S. 2020. TaBERT:
Pretraining for Joint Understanding of Textual and Tabular
Data. In Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, 8413–8426. Online:
Association for Computational Linguistics.

