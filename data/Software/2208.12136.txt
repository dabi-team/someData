Empirical Software Engineering manuscript No.
(will be inserted by the editor)

A Comparison of Reinforcement Learning
Frameworks for Software Testing Tasks

Paulina Stevia Nouwou Mindom · Amin
Nikanjam · Foutse Khomh

2
2
0
2

g
u
A
0
3

]
E
S
.
s
c
[

2
v
6
3
1
2
1
.
8
0
2
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Software testing activities scrutinize the artifacts and the behav-
ior of a software product to ﬁnd possible defects and ensure that the product
meets its expected requirements. Although various approaches of software test-
ing have shown to be very promising in revealing defects in software, some of
them are lacking automation or are partly automated which increases the test-
ing time, the manpower needed, and overall software testing costs. Recently,
Reinforcement Learning (RL) has been successfully employed in complex test-
ing tasks such as game testing, regression testing, and test case prioritization
to automate the process and provide continuous adaptation. Practitioners can
employ RL by implementing from scratch an RL algorithm or use an RL frame-
work. RL frameworks oﬀer well-maintained pre-implemented state-of-the-art
RL algorithms to facilitate and speed up the development of RL applications.
Developers have widely used these frameworks to solve problems in various
domains including software testing. However, to the best of our knowledge,
there is no study that empirically evaluates the eﬀectiveness and performance
of pre-implemented algorithms in RL frameworks. Moreover, some guidelines
are lacking from the literature that would help practitioners choose one RL
framework over another. In this paper, therefore, we empirically investigate
the applications of carefully selected RL algorithms (based on the characteris-
tics of algorithms and environments) on two important software testing tasks:
test case prioritization in the context of Continuous Integration (CI) and game
testing. For the game testing task, we conduct experiments on a simple game
and use RL algorithms to explore the game to detect bugs. Results show that
some of the selected RL frameworks such as Tensorforce outperform recent
approaches in the literature. To prioritize test cases, we run extensive exper-
iments on a CI environment where RL algorithms from diﬀerent frameworks
are used to rank the test cases. We ﬁnd some cases where our RL conﬁgura-

Paulina Stevia Nouwou Mindom · Amin Nikanjam · Foutse Khomh
Polytechnique Montr´eal, Qu´ebec, Canada
E-mail: {paulina-stevia.nouwou-mindom, amin.nikanjam, foutse.khomh}@polymtl.ca

 
 
 
 
 
 
2

Nouwou Mindom et al.

tions outperform the implementation of the baseline. Our results show that
the performance diﬀerence between pre-implemented algorithms in some cases
is considerable, motivating further investigation. Moreover, empirical evalua-
tions on some benchmark problems are recommended for researchers looking to
select RL frameworks, to make sure that RL algorithms perform as intended.

Keywords Software Testing · Reinforcement Learning · Game Testing · Test
Case Prioritization

1 Introduction

Software bugs and failures are costing trillions of dollars every year to the
global economy according to a recent report by a software testing company
Tricentis1. In 2017 alone, 606 software bugs costed the global economy about
$1.7 trillion dollars, aﬀecting 3.7 billion people. To alleviate this issue, re-
searchers and practitioners have been striving to develop eﬃcient testing tech-
niques and tools, to help improve the reliability of software systems before
they are released to the public. Several strategies, such as random testing
[22], coverage-based testing [58] and search-based testing [23] have been pro-
posed to evaluate that a software product does what it is supposed to do.
More recently, Reinforcement Learning (RL) is being increasingly leveraged
for software testing purposes [5, 31, 35, 57] thanks to the availability of multi-
ple RL frameworks providing pre-implemented RL algorithms, e.g., Advantage
Actor Critic (A2C), Deep Q-Networks (DQN), Proximal Policy Optimization
(PPO). For example, Kim et al. [26] leveraged the Keras-RL framework to
apply RL to test data generation. Similarly, Drozd et al. used the Tensorforce
framework to apply RL to Fuzzing testing [14] and Romdhana et al. used the
Stable-baselines framework for black box testing of android applications [42].
However, given that these pre-implemented RL algorithms often make as-
sumptions that could hold only for certain types of problems and not for others,
it could be challenging for developers and researchers to select the most ad-
equate RL implementation for their problem. The choice of an RL algorithm
depends on the nature of the problem to solve, the available computation bud-
get, and the desired generalizability of the trained models. Moreover, given
the fact that RL algorithms are often implemented diﬀerently in diﬀerent RL
frameworks, it is unclear if the same results can be obtained using diﬀerent
frameworks.

To clear up these interrogations and help researchers and practitioners
make informed decisions when choosing a RL framework for their problem, in
this paper, we examine and compare the applicability of diﬀerent RL frame-
works for software engineering testing tasks. Speciﬁcally, we apply RL algo-
rithms from diﬀerent frameworks to game testing and test case prioritization.
The automation of game testing is critical because of the frequent require-
ments changes that occur during a game development process [43]. Recently

1 https://www.techrepublic.com/article/report-software-failure-caused-1-7-

trillion-in-financial-losses-in-2017/

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

3

diﬀerent RL algorithms have been applied to automate game testing and im-
prove the fault identiﬁcation process [2, 29, 54, 55]. Test case prioritization
improves the testing process by ﬁnding optimal ordering of the test cases and
detecting faults as early as possible. RL has also been shown to be successful
in prioritizing test cases for various conﬁgurations [7, 49].

In this paper, we perform a comprehensive comparison of diﬀerent RL algo-
rithms implemented in three frameworks, i.e., Stable-Baselines3 [40], Keras-rl
[39], and Tensorforce [44]. We investigate which RL algorithm/frameworks
may be more suitable for detecting bugs in games and solving the test case
prioritization problem. Results show that the diversity of hyperparameters
that each framework provides impacts its suitability for each of the studied
software testing tasks. Tensorforce framework tends to be more suitable for
detecting bugs as it provides hyperparameters that allow a deeper exploration
of the states of the environment while the Stable-baselines framework tends
to be more suitable for the test case prioritization problem.

To summarize, our work makes the following contributions:

– To evaluate the usefulness of RL on game testing, we utilized three state-of-
the-art RL frameworks: Stable-baselines, Keras-rl, and Tensorforce. Specif-
ically, we applied them to the Block Maze game for bug detection and
collected the number of bugs, the state coverage and the code coverage.
We have compared a total of seven RL conﬁgurations and some of them
outperform the existing work.

– Based on eight publicly available datasets, we applied state-of-the-art RL
frameworks on three ranking models and collected results to evaluate their
usefulness in prioritizing test cases. As metrics of comparison, we consider
the Normalized Rank Percentile Average (NRPA), the Average Percentage
of Faults Detected (APFD), the sum of training time and the average pre-
diction time for each RL conﬁguration. The results collected are compared
with the baselines and we derive conclusions regarding the most accurate
RL frameworks for test case prioritization. We found out that in most
datasets, the Stable-Baselines framework originally used in [5] performs
better than Tensorforce and Keras-rl.

– We provide some recommendations for researchers looking to select an RL
framework as we noticed diﬀerences in performance when considering the
same algorithm among diﬀerent frameworks. For example, the same DQN
algorithm from diﬀerent frameworks show diﬀerent results.

The rest of this paper is organized as follows. In Section 2, we review
the necessary background knowledge on the game testing problem, the test
case prioritization problem, and RL. The methodology followed is our study is
described in Section 3. We discuss the obtained empirical results in Section 4.
Some recommendations for future work are mentioned in Section 5. We review
related work in Section 6. Threats to validity of our study are discussed in
Section 7. Finally, we conclude the paper in Section 8.

4

Nouwou Mindom et al.

Fig. 1 The interaction between a player and a game environment (inspired from [57]).

2 Background

In this section, we describe the terms and notations used to deﬁne the test
case prioritization and game testing problems. We also introduce the RL frame-
works used to tackle these problems.

2.1 Game Testing

The process of testing a game is an essential activity before its oﬃcial release.
The complexity of game testing has led researchers to investigate ways to auto-
mate it [3, 17]. In the following, we introduce few concepts that are important
to understand automatic game testing.

Deﬁnition 1 : Game. A game G can be deﬁned as a function G : An →
(S × R)n, where A is the set of action that can be performed by the agent, S
is the set of states of the game and R represents the set of rewards that comes
from the game, and n is the number of steps in the game. A player takes a
sequence of actions (n actions) based on the observations it received until the
end of the game. If we consider the game as an environment that the agent
interacts with, each state refers to observations of the environment perceived
by the agent at every time stamp. The action is a set of decisions that can
be made by the agent which can be rewarded positively or negatively by the
environment.

Figure 1 depicts the overall interaction between a player and a game. Given the
state st at time step t the agent selects an action at to interact with the game
environment and receives a reward st from the environment. The environment
moves into a new state st+1, which aﬀects the selection of the next action.

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

5

Deﬁnition 2 : Game state. A state in the game refers to game’s current
status and can be represented as a ﬁxed-length vector (v0, v1, ..., vn). Each el-
ement vi of the vector represents an aspect of the state of the game such as
the position of a player, its speed or the location of the gold trophy in case of
a Block Maze game.

Deﬁnition 3 : Policy. Given an agent, a policy π is deﬁned as a function
π : S → A mapping each state s ∈ S to an action a ∈ A. The policy indicates
the agent’s decision in each state of the game. It can be a strategy from a
human expert or learned from experiences accordingly.

Deﬁnition 4 : Game tester. Given a game G, a set of policies Π to interact
with G, a set of states S of G, and a set of bugs B on G, a game tester T is
deﬁned as a function TG : Π → S × B.

A sequence of actions is a test case for a game. Since G is often a stochastic
function, a test case may lead to multiple distinct states. A game tester aims
to implement diﬀerent strategies in order to explore the diﬀerent states of
the game to ﬁnd bugs. In this paper, the game tester play the role of oracle
to verify the presence or absence of a bug on an output state. Therefore, it
implements diﬀerent strategies in order to explore the diﬀerent states of the
game to ﬁnd bugs. So, a test case generated by a game tester is a series of
valid actions that can reach a state in which a bug might hide.

Deﬁnition 5 : Test adequacy criteria. We consider the state coverage and
line coverage as criteria to discover whether the existing test cases have a good
bug-revealing ability. The state coverage measures the number of visited state
of the player during the play, and the code coverage measures the number of
lines of code related to the function of the game that have been covered during
the play.

Considering a 5 × 5 Block Maze game where bugs are injected and triggered
when the player reached a location on the maze:

– A player has 4 possible actions (LEFT, RIGHT, UP, DOWN), a state is
deﬁned as the vector (P, B) where P is the player position at each step of
the play and B the position of a bug (the position that triggers a bug on
the maze).

– Initially the state of the Block Maze is ((0, 0), (1, 4)).
– A test case that lead to a bug can be

{RIGHT → RIGHT → RIGHT → RIGHT → DOWN},
corresponding to the following states of the game
{((0, 0), (1,4)) → ((0, 1), (1,4)) → ((0, 2), (1,4)) →
((0, 3), (1,4)) → ((0, 4), (1,4)) → ((1,4),(1,4))}.

As in [57], in this work, we consider the testing of large combat games with

one agent.

6

Nouwou Mindom et al.

2.2 Test Case Prioritization

Test Case Prioritization is the process of prioritizing test cases in a test suite. It
allows to execute highly signiﬁcant test cases ﬁrst according to some measures,
in order to detect faults as early as possible. In this paper, similar to [5], we
study test case prioritization in the context of Continuous Integration (CI).

Deﬁnition 6 : CI Cycles. A CI cycle is composed of a logical value and
a set of test cases. The logical value indicates whether or not the cycle has
failed. Failed cycles due to a test case failure are considered in this work, and
we select a test case with at least one failed cycle.

Deﬁnition 7 : Test case feature. Each test case has an execution history
and code-based features. The execution history shows a record of executions of
test cases over the cycles. The execution history includes the execution verdict
of a test case, the execution time, a sequence of verdicts from prior cycles, and
the test age capturing the time the test case was introduced for the ﬁrst time.
The execution verdict indicates if the test case has failed or not. The execution
time of a test case can be computed by averaging its previous execution times.
The code-based features for a test case can indicate the changes that have been
made, the impacted ﬁles with the number of lines of code which are relevant to
predict the execution time and can be leveraged to prioritize test cases.

Deﬁnition 8 : Optimal ranking (Test Case prioritization). The test
case prioritization process in this work is a ranking function that produces
an ordered sequence based on the optimal ranking of test cases. The goal of
prioritization is to get as close to this order as possible. The optimal ranking
of a set of test cases is an order in which all test cases that fail are executed
before test cases that pass. Furthermore, in this optimal ranking, test cases
with a smaller time of execution should be executed sooner.

A detailed explanation of the terms CI Cycles, Test case feature, Test
Case prioritization, and Optimal ranking can be found in [5].

2.3 Reinforcement Learning

RL is a technique in which the agent interacts with the environment to learn
how to optimally behave in it. Agent’s perception of the environment is called
an observation (i.e., state of the environment). Based on the observation, the
agent chooses among available actions. Upon running the action, the environ-
ment moves to its next state and the agent receives a reward. This reward
rates the agent’s performance (usually) on the previous action. RL algorithms
can be classiﬁed based on the following properties [5]:

Model-based and Model-free RL. In model-based RL, the agent knows
the environment [5]. It knows in advance the reaction of the environment to
possible actions and the potential rewards it will get from taking each action.
During training, the agent learns the optimal behavior by taking actions and

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

7

observing the outcomes which include the next state and the immediate re-
ward. On the contrary, in model-free RL, the agent has to learn the dynamics
of the environment by interacting with it. From the interaction with the en-
vironment, the agent learns an optimal policy for selecting an action. In this
work, we are only interested in model-free RL algorithms as some of the test
case features (execution time) are unknown beforehand as well as the location
of faults in a game.

Value-based, policy-based, and actor-critic learning. At every state,
value-based methods estimate the Q-value and select the action with the best
Q-value. A Q-value shows how good an action might be given a state. Re-
garding policy-based methods, an initial policy is parameterized, then during
training, the parameters are updated using gradient-based or gradient-free op-
timization techniques. Regarding actor-critic methods, the agent learns simul-
taneously from value-based and policy-based techniques. The policy function
(actor) selects the action and the value function (critic) estimates the Q-values
based on the action selected by the actor.

Action and observation space. The action space indicates the possible
moves of the agent inside the environment. The observation space indicates
what the agent can know about the environment. The action and observation
space can be discrete or continuous. Speciﬁcally, the observation space can be
a real number or high dimensional. While a discrete action space means that
the agent chooses its action among distinct values, a continuous action space
implies that the agent chooses actions among real values vectors. Not all RL
algorithms support discrete and continuous conﬁgurations for both the action
and observation space, which limits the choice of algorithms to implement.

On-policy vs oﬀ-policy. On-policy methods will collect data that is used
to evaluate and improve a target policy and take actions. On the contrary, oﬀ-
policy methods will evaluate and improve a target policy that is diﬀerent from
the policy used to generate the data. Oﬀ-policy learners generally use a replay
buﬀer to update the policy.

DRL methods use Deep Neural Networks (DNNs) to approximate the value
function, or the model (state transition function and reward function) and tend
to be a more manageable solution space in large complex environments.

2.4 State-of-the-art RL Frameworks

In recent years, multiple model-free RL algorithms have been introduced; ad-
vancing the research around RL [30, 34]. Diﬀerent RL frameworks such as
Stable-baselines [24, 40] and Tensorforce [44] have also been introduced to
ease the implementation of RL-based applications. These frameworks usually
contain pre-implementations of diﬀerent RL algorithms. While the developers
may implement their own algorithm, in this work, we focus on comparing the
pre-implemented algorithms of existing RL frameworks on software testing
tasks. Table 1 provides a list of popular RL frameworks, which are described
below.

8

Nouwou Mindom et al.

Table 1 Popular RL frameworks.

Framework

OpenAI base-
lines [13]

Github stars
# of contributors
Issues

Algorithms

430

open,

12.1k
112
391
closed
A2C,
ACER,
ACKTR, DDPG,
DQN,
GAIL,
HER,PPO2,TRPO

Stable-
baselines3
[40]
2.6k
66
34
closed
DDPG,
A2C,
DQN,
HER,
PPO, SAC, TD3

open,

453

Tensorboard support Yes

Pull requests

Fork
Last update

open

83
closed
4.2k
Jan 31, 2020

Yes
4 open, 199 closed

284

546
Dec 9, 2021

511
Nov 10, 2021

Tensorforce [44] Keras-RL [39]

3.1k
54
4 open, 617 closed

Duel-
DQN,
DQN,
ing
Double
DQN,
DDPG, CDQN,
A3C,
A2C,
TRPO,PPO, TA,
Reinforce
Yes
0 open, 225 closed

Dopamine
[10]

9.7k
10
60 open, 82
closed
DQN, C51,
Rainbow,
IQN, SAC

219

open,

5.2k
39
10
closed
DQN,
Double
DQN, DDPG ,
CDQN, Dueling
DQN, SARSA

120

open,

No
33
closed
1.3k
Nov 11, 2019

Yes
20 open, 20
closed
1.3k
Dec
2021

14,

– OpenAI baselines [13] is the most popular RL framework given its high
GitHub star rating. It provides many state-of-the-art RL algorithms. After
installing the package, training a model only requires specifying the name
of the algorithm as a parameter.

– Stable-baselines [24, 40] is an improved version of OpenAI baselines with
a more comprehensive documentation. In this paper, we used the version
3 of this framework, which is reported to be more reliable because of its
Pytorch [38] backend that captures DNN policies. To train an agent, Stable-
baselines has built-in functions that create a model depending on the RL
algorithm chosen.

– Keras-rl [39] provides the dueling extension of the DQN algorithm and
SARSA algorithm that are not oﬀered by Stable-baselines version 3. How-
ever, Keras-rl oﬀers less algorithms than the previous frameworks. The
training of an agent requires a few steps: the deﬁnition of the DNN that
will be used for the training, the instantiation of the agent, its compilation,
and ﬁnally the call of the training function.

– Tensorforce [44] provides the same algorithms as the Stable-baselines frame-
work with some additions: Trust-Region Policy Optimization (TRPO),
Dueling DQN, Reinforce, and Tensorforce Agent (TA). Tensorforce oﬀers
built-in functions to create and train an agent. Also, it oﬀers the ﬂexibility
to train the agent without using the built-in functions, which allows it to
capture the performance metrics of the agent, such as the reward. Finally,
the training starts in a loop function depending on the number of episodes.
Tensorforce relies on TensorFlow [1] as backend.

– Dopamine [10] is a more recent framework that proposes an improved vari-
ant of the Deep Q-Networks (DQN) and the Soft Actor-Critic (SAC) algo-
rithm. In addition to a TensorFlow backend for creating DNNs, Dopamine
is conﬁgured using the gin 2 framework, to specify and conﬁgure hyper-

2 https://github.com/google/gin-config

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

9

Table 2 Comparison between RL frameworks

RL Frameworks

Stable-baselines

Keras-rl

Tensorforce

Algorithms
DQN [33]
DDPG [30]
A2C [34]
TD3 [18]
SAC [21]
PPO [46]
DQN [33]
Dueling DQN [52]
Double DQN [33]
SARSA [51]
CDQN [20]
DDPG [30]
DQN [33]
Double DQN [33]
CDQN [20]
PPO [46]
DDPG [30]
A2C [34]
A3C [34]
TRPO [45]
TA [45]
Reinforce [45]

Policy

Learn.
Value
Policy

On/Oﬀ
oﬀ-policy
Oﬀ-policy
Actor-Critic On-policy
Oﬀ-policy
Actor-Critic Oﬀ-policy
Actor-Critic On-policy
oﬀ-policy
Oﬀ-policy
oﬀ-policy
On-policy
On-policy
Oﬀ-policy
oﬀ-policy
oﬀ-policy
On-policy
Actor-Critic On-policy
Oﬀ-policy
Actor-Critic On-policy
Actor-Critic On-policy
Actor-Critic On-policy
Actor-Critic On-policy
On-policy

Value
Value
Value
Value
Value
Policy
Value
Value
Value

Policy

Policy

Act.
Dis
Cont
Both
Cont
Cont
Both
Dis
Dis
Dis
Dis
Cont
Cont
Dis
Dis
Cont
Both
Cont
Both
Both
Both
Both
Both

Cont:continuous, Dis:discrete, Both: continuous and discrete

parameters. The training of an agent requires instantiating the model and
then starting the training with built-in functions.

Based on their popularity and ease of implementation, we choose to rely on
Stable-baselines, Tensorforce, and Keras-RL frameworks. Table 2 summarizes
the pre-implemented DRL algorithms available in theses frameworks. Stable-
baselines, Keras-rl, and Tensorforce have respectively 6, 5, and 10 available pre-
implemented DRL algorithms. They all contain the DQN algorithms, which
we apply to the test case prioritization and game testing problems. We also
apply the A2C algorithm from Stable-baselines and Tensorforce to both test
case prioritization and game testing problems. In addition to A2C and DQN,
we applied the DDPG algorithm to the test case prioritization problem. Sim-
ilarly, we applied the PPO algorithm to the game testing problem. Keras-rl
does not have a pre-implemented version of the A2C, nor DDPG algorithms,
which could be applied to the previously mentioned problems. Moreover, these
selected DRL algorithms are suitable for this paper, as we are capable of com-
paring their results with the baselines [5, 57]. In [57], the authors used their
own implementation of the A2C algorithm to detect bugs on three games.
Thus, among the selected DRL strategies, we consider the A2C algorithm
from the DRL frameworks and evaluate and compare our results with the re-
sults reported in [57]. In [5], given that the applicability of RL algorithms is

10

Nouwou Mindom et al.

limited by the type of their action space, the authors chose DRL algorithms
from Stable-baselines that are compatible with the type of action space of the
prioritization techniques they considered (see Section 3.3.1). We do the same,
and evaluate and compare the obtained results.

3 Study design

In this section, we describe the methodology of our study which aims to com-
pare diﬀerent pre-implemented RL algorithms from existing frameworks. We
also introduce the two problems that we selected for this comparison.

3.1 Research questions

The goal of our work is to evaluate and compare pre-implemented algorithms
oﬀered by diﬀerent RL frameworks. In order to achieve this goal, we focus on
answering the following research questions.

– RQ1: Do the choice of DRL framework aﬀects the performance of software

testing tasks?

– RQ2: Which combinations of RL frameworks-algorithms perform better

(i.e., get trained accurately and solve the problem eﬀectively)?

– RQ3: Are the results obtained using DRL frameworks stable across mul-

tiple runs?

3.2 Problem 1: Game testing using RL

We aim to employ several RL algorithms from diﬀerent RL frameworks in a
game testing environment. More speciﬁcally, we use RL to explore more states
of a game where bugs might hide. Our work is based on Wuji [57], an auto-
mated game testing framework that combines Evolutionary Multi-Objective
Optimization (EMOO) and DRL to detect bugs on a game. Wuji randomly
initializes a population of policies (represented by DNNs), adopts EMOO to
diversify the state’s exploration, then uses DRL to improve the capability of
the agent to accomplish its mission. To train Wuji on multiple RL frameworks,
we turn oﬀ EMOO and only consider the DRL part of Wuji. In this way, we
can focus on the eﬀect of diﬀerent DRL algorithms on detecting bugs.

3.2.1 Creation of the RL environment

A game environment can be mapped into a DRL process by deﬁning the state
or observation, reward, action, end of an episode and the information related
to the bug.

Observation space: As mentioned in Deﬁnition 2, an observation is a set
of features describing the state of the game. In our case, the observation of
the agent is its position inside the maze.

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

11

Action space: The action space describes the available moves that can
be made in the game. We consider a game with 4 discrete actions to choose:
north, south, east, west.

Reward function: The reward function is the feedback from the envi-
ronment regarding the agent’s actions. It is designed so that the agent can
accomplish its mission. The agent gets negatively rewarded when it reaches
a non-valid position in the game or any other position that is not the goal
position of the game, in all other cases it receives a positive reward.

3.2.2 Experimental Setup

The Block Maze game has a discrete action space which limits the RL conﬁg-
urations it can be applied to. Therefore, we consider the following algorithms:
DQN-SB, PPO-SB, A2C-SB, DQN-KR, DQN-TF, PPO-TF, and A2C-TF dur-
ing our experiments. We employ the same parameters used in [57] for training,
in our experiments. Mean square error (MSE) is adopted as the loss function
in DRL training, which is minimized by using the Adam [27] optimizer. The
DNN model is three fully-connected linear layers with 256, 128, 128 units as
the hidden layers, connected to the output layer. The output layer consists of
4 neurons, one per each action. The activation function ReLU [36] is adopted
between the two hidden layers. The epoch for DRL training is set to 20 and
the learning rate is set to 0.25 × 103 in the Adam optimizer. We collected
the results of each DRL algorithm for a total of 4 millions steps. To counter
the randomness eﬀect during testing, we repeat each run 10 times to average
the results. The experiments are ran on an ASUS desktop machine running
Windows 10 on a 3.6 GHz Intel core i7 CPU with 16GB main memory, for
approximately 100 days. Authors in [57] studied the detection of bugs by im-
plementing a DRL approach. The game is tested by considering the winning
score. We consider their work as a baseline and compare other RL approaches
with their results.

3.2.3 Training of an RL agent

The DRL part of Wuji randomly initializes DNN policies, then uses the A2C
algorithm to evolve the policies so that the agent can explore more states and
accomplish its mission. We are going to apply DQN, A2C, and PPO algorithms
from Stable-baselines3 (SB), Keras-rl (KR) and Tensorforce (TF), respectively
to detect bugs on the DRL part of Wuji. As in [57], we do not have access to
the games directly, but we are able to train the agent oﬄine with a description
of the game environment as mentioned in Subsection 3.2.1. The game follows
the OpenAI gym [9] interface then the training starts.

3.2.4 Datasets

A Block Maze game from [57], is selected in the evaluation Figure 2. In the
Block Maze game, the player’s objective is to reach the goal coin. It has 4

12

Nouwou Mindom et al.

Fig. 2 Block Maze with bugs (red, green and yellow dots).

possible actions to choose: north, south, east, west. Every action leads to
moving into a neighbor cell in the grid in the corresponding direction, except
that a collision on the block (dark green) results in no movement. To evaluate
the eﬀectiveness of our RL approaches, the Block Maze is injected with 25 bugs
that are randomly distributed within the environment. A bug is triggered if
the robot reaches a speciﬁc location in the map, shown on Figure 2.

3.2.5 Evaluation metrics

We use the following metrics, also considered in [57] (except the average cu-
mulative reward) to assess the accuracy and eﬀectiveness of the game testing
process across diﬀerent RL approaches:

– Number of bugs detected: the average number of bugs detected by our

RL agents after being trained.

– The average cumulative reward obtained by the RL agents after being

trained.

– The line coverage: the lines covered by each RL approach during testing.
– The state coverage: the number of visited state during testing.

3.2.6 Analysis Method

We proceeded as follows to answer our research questions. In RQ1, we at-
tempted to ﬁnd bugs and the cumulative reward obtained by the player in the
Block Maze game by using DRL algorithms from state-of-the-art frameworks.
We relied on the implementation of algorithms provided by Stable-baselines3
[40], Keras-rl [39] and Tensorforce [44] frameworks. Moreover, we computed
the state coverage and line coverage as adequacy criteria to assess the perfor-
mance of the DRL strategies. To determine the best RL strategy in RQ2 we
use the Welch’s ANOVA and Games-Howell post-hoc test [53], [19]. We com-
pare all RL strategies across all runs in terms of bug’s detected. As in [5], The
signiﬁcance level is set to 0.05, diﬀerence with p-value <= 0.05 is considered
signiﬁcant.

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

13

3.3 Problem 2: Test case prioritization using RL

We aim to apply several DRL algorithms from diﬀerent frameworks for test
case prioritization in the context of CI. To do so, we follow a recent work on
using DRL for test case prioritization: [5]. The authors studied diﬀerent ap-
proaches to prioritization techniques that can adapt and continuously improve
while interacting with the CI environment. The interaction between the CI en-
vironment and test case prioritization is modelled as an RL problem. They use
state-of-the-art RL techniques to learn prioritization strategies which are close
to optimal ranking. The RL agent is ﬁrst trained oﬄine on the test case execu-
tion history and code-based features of past cycles to prioritize the next cycles.
At the end of each cycle, if the agent’s accuracy in predicting the next cycles
is less than a speciﬁed threshold, the test case execution history is replayed
to improve the agent’s policy. After oﬄine training, the trained agent can be
applied to rank the available test cases. Similarly, our approach for applying
RL techniques in the context of the CI environment is to train an RL agent,
based on the algorithms designed in [5] that describe the ranking model in the
context of CI and test case prioritization. We train an RL agent using various
DRL algorithms from popular frameworks, as described in subsection 2.4.

3.3.1 Creation of the RL environment

Test case prioritization can be mapped into an RL problem by deﬁning the
details of the interaction of the agent with the environment, meaning obser-
vation, action, reward, and end condition of an episode. We map test case
prioritization as an RL problem by considering three ranking models: List-
wise, Pairwise, and Pointwise that have been employed in [5].

Listwise ranking function: The authors of [5] designed the listwise ranking
model as a class on which the observation space, action space, and reward
function are deﬁned as follows. This class uses the indices of the test case with
the most priority at each time step as the action to compute the reward and
the next observation.

Observation space: The agent’s observation is a set of test cases. Its
length is not ﬁxed and increases with the size of the test cases, which can
increase the training and prediction time.

Action space: The action describes the test case with the most priority

and is a discrete value between 0 and the number of test cases.

Reward function: The reward function takes into account the optimal
ranking and the ranking of the selected action. The closer the ranking is to
the optimal ranking, the higher the reward.

Pointwise ranking function: The authors of [5] designed the pointwise rank-
ing model as a class on which the observation space, action space and reward
function are deﬁned. This class consists of determining scores for each test

14

Nouwou Mindom et al.

case and then storing them in a temporary vector. At the end of the learn-
ing process, the test cases are sorted according to their scores stored in the
temporary vector.

Observation space: The agent’s observation is a record of the character-

istics of a single test case with 4 numerical values.

Action space: The action describes a score associated with each test case.
The agent uses this score to order the test cases. Each action is a real number
between 0 and 1.

Reward function: The reward function is also computed here based on
the gap between the assigned ranking and the optimal ranking. The diﬀerence
with the listwise ranking function is that the ﬁnal ranking of the test cases is
known by the end of an episode.

Pairwise ranking function: The authors of [5] designed the pairwise ranking
model as a class on which the observation space, action space and reward
function are deﬁned. This class uses the selection sorting algorithm [28] to
rank the test cases. All test cases are divided into a pair: the sorted part on
the left and the unsorted part on the right. At each time step, if a test case
with high priority is found, its position is changed in the sorted part. The
process continues until all test cases are sorted.

Observation space: An agent observation is a pair of test case records.
Action space: The action space values are 0 or 1. The ﬁrst value indicates

that the ﬁrst test case in the observation has a higher priority.

Reward function: The reward function takes into account whether or
not the test case with a higher priority fails. If the test case has failed, the
agent receives the maximum reward.

3.3.2 Experimental Setup

We implemented our ranking models using the RL algorithms of the selected
frameworks. We used the OpenAI Gym library [9] to mimic the CI environ-
ment using logs execution and relied on the implementation of the RL algo-
rithms provided by the Stable-baselines2 [25], Keras-rl [39] and Tensorforce
[44] frameworks. Regarding the APFD and NRPA metrics, for each dataset,
we performed several experiments that correspond to the three pairwise, list-
wise and pointwise ranking models. It should be noted that the applicability of
the RL algorithms is restricted by the type of their action space. The pairwise
and listwise ranking models involve ﬁve experiments for each data set, one for
each RL framework with RL algorithms that can support a discrete action
space (i.e., DQN-SB, DQN-KR, DQN-TF, A2C-SB, A2C-TF). Similarly, the
point-ranking model involves ﬁve experiments for each dataset, one for each
RL framework with RL algorithms that can support a continuous action space
(i.e., DDPG-SB, DDPG-KR, DDPG-TF, A2C-SB, A2C-TF). We reused the
results of [5] regarding the Stable-baselines framework without repeating their
experiments. The total number of experiments is thus 96. The training process
begins with training an agent by replaying the execution logs from the ﬁrst

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

15

cycle, followed by evaluating the trained agent on the second cycle. Then the
logs from the second run are replayed to improve the agent, and so on.

As in [5], we trained the agent for a minimum of 200 × n × log2 n episodes
and one million steps for training each cycle, where n refers to the number of
test cases in the cycle. Training stops when the budget of steps per training
instance is exhausted or when the sum of rewards in an episode cannot be
improved for more than 100 consecutive episodes. Experiments are run once
using the default settings provided by the Stable-baselines framework, on a
machine with an Intel core i7, with 16 GB of RAM under the Windows 10
operating system. To answer our questions, we recorded the rank of each test.
Even though the experiments are run once, we can still draw safe conclusions
because the RL agents are evaluated over many CI cycles, allowing us to
account for randomness.

3.3.3 Comparison Baselines

In [5], the authors applied RL using state-of-the-art RL algorithms from the
Stable-baselines framework to solve the test case prioritization problem. We
use this work as a baseline and compare our suggested RL strategies with
their conﬁgurations. The authors also presented the results of three bench-
mark works RL-BS1 [49], RL-BS2 [7], MART [? ] which are similar to their
work in [5]. RL-BS1 applies RL on simple history data sets. RL-BS2 applies
RL Shallow Network, Deep Neural Network and Random Forest implemen-
tations on enriched datasets. MART is a supervised learning technique for
ranking test cases. RL-BS1 and RL-BS2 show results with runs containing
fewer than ﬁve test cases, which can inﬂate APFD and NRPA values when
prioritization is not required. MART, as a deep learning technique, has no
support for incremental learning [56] which is important for dealing with fre-
quently changing CI environments. We will also compare our results with the
mentioned baselines, i.e., RL-BS1, RL-BS2 and MART.

3.3.4 Training of an RL agent.

The applicability of DRL algorithms depends on the action space of the rank-
ing models. Pairwise and Listwise ranking models have discrete action spaces
while the Pointwise ranking model has a continuous action space. For the sake
of comparison of our selected RL frameworks (Table 2), DQN and A2C will be
applied to Pairwise and Listwise ranking models while DDPG will be applied
on the Pointwise ranking model.

Regarding the test case prioritization problem, the agent is trained oﬄine,
which is the case for many systems, especially safety-critical systems. Never-
theless, after the training, the agent can be deployed into a real environment
[15]. We follow the same procedure as in [5]. The agent is trained ﬁrst on the
available execution history. Then at the end of the cycle, the test cases are
ranked and new execution logs are captured. The new logs are used to train
the agent at the beginning of the next cycle.

16

Nouwou Mindom et al.

Table 3 Datasets [5]

Dataset

Type

Cycles

Logs

Fail Rate(%)

Failed Cycles

Paint-Control
IOFROL
Codec
Compress
Imaging
IO
Lang
Math

Simple
Simple
Enriched
Enriched
Enriched
Enriched
Enriched
Enriched

332
209
178
438
147
176
301
57

25,568
32,118
2,207
10,335
4,482
4,985
10,884
3,822

19.36
28.66
0
0.06
0.04
0.06
0.01
0.01

252
203
0
7
2
3
2
7

Avg. Calc. Time (Avg)
Enriched Features
NA
NA
1.78
3.64
5.60
2.88
5.58
9.46

3.3.5 Integration of an RL agent into CI Environments.

To integrate the RL agent into CI environments, the agent must ﬁrst be trained
on the execution history of available test cases and the history of test case-
related code features [5]. Then, the trained agent is deployed to the production
setting where the test case features can be used in each CI cycle to rank the
test cases. During the testing process, if accuracy decreases, execution logs are
captured and passed to the agent so that it can adapt to the changes.

3.3.6 Datasets

We ran our experiments on datasets used in [5]: Simple and enriched historical
data sets. Simple historical datasets represent test situations where source code
is not available and contain the age, average execution time, and verdicts of test
cases. Enriched historical datasets represent test situations where source code
is available but due to time constraints imposed by the CI, complete coverage
analysis is not possible. They are enriched with history data, execution history,
and code characteristics from Apache Commons projects [7]. Table 3 shows
the list of datasets that we employ in this study and their characteristics.

The execution logs contain up to 438 CI cycles, and each CI cycle includes
at least 6 test cases. Less than 6 test cases will not be relevant and can inﬂate
the accuracy of the results [5]. The logs column indicates the number of test
case execution logs which ranges from 2, 207 to 32, 118. Enriched datasets show
a low rate of failed cycles and failure rate while the failure rates and number
of failed cycles in simple datasets are high. The last column shows the average
computation time of enriched features per cycle.

3.3.7 Evaluation metrics

We use two evaluation metrics to assess the accuracy of prioritization tech-
niques across our DRL conﬁgurations. Both metrics are used in [5]. We present
a description of them in the rest of this section.

Normalized Rank Percentile Average (NRPA): NRPA measures
how close a predicted ranking of items is to the optimal ranking indepen-
dently of the context of the problem or ranking criteria. The value can range

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

17

from 0 to 1. The NRPA is deﬁned as follows: N RP A = RP A(se)
RP A(so) . In this equa-
tion se is the ordered sequence generated by a ranking algorithm R that takes
a set of k items, and so is the optimal ranking of the items. RP A is deﬁned
as:

RP A =

(cid:80)

m∈s

(cid:80)k

i=idx(s,m)|s| − idx(so, m) + 1

k2(k + 1)/2

(1)

where idx(s, m) returns the position of m in sequence s.

Average Percentage of Faults Detected (APFD): APFD measures
the weighted average of the faults percentage detected by the execution of test
cases in a certain order. It ranges from 0 to 1. Values close to 1 imply fast
fault detection. It is deﬁned as follow:

AP F D(se) = 1 −

(cid:80)

t∈se

idx(se, t) ∗ t.v
|se| ∗ m

+

1
2 ∗ |se|

(2)

where m is the total number of faults, t is a test case among se and v its
execution verdict, either 0 or 1.

Both APFD and NRPA metrics are suitable to measure the accuracy of a
prioritization technique. However, NRPA can be misleading in the presence of
failures as it treats all test cases the same regardless of their execution verdict.
In [5], the authors show that NRPA values contradict APFD values for some
datasets, therefore recommending the use of APFD metric to measure how
well a certain ranking can reveal faults early.

3.3.8 Analysis Method

To answer RQ1, we conducted experiments and collected the averages and
standard deviations of APFD and NRPA for eight datasets (see Subsection
3.3.6), as well as their training and prediction times using DRL algorithms
from selected frameworks. We relied on the implementation of algorithms pro-
vided by Stable-baselines3 [40], Keras-rl [39], and Tensorforce [44] frameworks.
Furthermore, we collected from [5], the averages and standard deviations of
baselines conﬁgurations in terms of NRPA and APFD values. For each frame-
work, we compare its best conﬁguration with the baselines in terms of NRPA
or APFD. We calculate CLE, between the best conﬁguration of each frame-
work and baselines to assess the eﬀect size of diﬀerences. In RQ2, we use
the Welch’s ANOVA and Games-Howell post-hoc test [53], [19] to indicate
the best RL algorithm. All conﬁgurations across all cycles are compared using
one NRPA or APFD value per cycle. Similar to the game testing problem, a
diﬀerence with p-value <= 0.05 is considered signiﬁcant in our assessments.

18

Nouwou Mindom et al.

Fig. 3 Number of bugs detected by
DQN agents from diﬀerent frameworks.

Fig. 4 Average cumulative reward earned by
DQN agents from diﬀerent frameworks.

3.4 Data Availability

The source code of our implementation and the results of experiments can be
found here3.

4 Experimental results

We now report about the results of our experiments.

4.1 Game Testing

RQ1: Figure 3 and 4 shows respectively the average number of detected bugs
and average cumulative reward obtained by DQN algorithms from Stable-
baselines3 (DQN SB), Keras-rl (DQN KR), and Tensorforce (DQN TF) frame-
works. First, compared with the baselines, which is the A2C implementation
in [57], the DQNs algorithm ﬁnd less bugs. A2C algorithm takes advantage
of all the beneﬁts of value based (like DQN) and policy based RL algorithms.
Also, the A2C algorithm employs multiple workers and environments interact-
ing concurrently whereas DQN is a single agent single environment that needs
a powerful GPU to train faster. The Block Maze has a small search space
and the faster convergence rate of A2C can lead to detection of more bugs
quickly. Among the DQN algorithms, the stable-baselines’s version performs
better in terms of detecting bugs and Tensorforce’s version performs better in
terms of cumulative rewards. To explain these results, our intuition lies into
the diversity of the hyperparameters provided by the DRL frameworks which
aﬀect the performance, as well as the diﬀerence between Tensorﬂow and Py-
torch as backend of the frameworks. Stable-baselines and Tensorforce, in their

3 https://github.com/npaulinastevia/drl_se

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

19

Fig. 5 Number of bugs detected by
A2C agents from diﬀerent frameworks.

Fig. 6 Average cumulative reward earned by
A2C agents from diﬀerent frameworks.

DQN implementation, have hyperparameters diﬀerent from the ones use in [57]
which can impact the performance. For example, Stable-baselines provides a
soft update coeﬃcient to update the target network frequently and optimize
the network eﬃciency. It can also be noted that between [0.25E + 6, 0.75E + 6]
steps DQN KR has a faster convergence. On the oﬃcial implementation of the
algorithm4, DQN KR is brieﬂy trained randomly, which could explain a brief
faster convergence when detecting bugs.

Figure 5 and 6 shows respectively the average number of bugs and aver-
age cumulative reward obtained by the A2C algorithm from Stable-baselines3
(A2C SB), Tensorforce (A2C TF), and Wuji [57] (A2C WUJI). Recall that
in this paper, we only consider the DRL part of Wuji, we then compare our
results with the number of bugs detected by the Wuji framework. Since the
authors of Wuji did not consider the average cumulative reward as a metric
in the original work, we did not report it here. The reason is that we would
not have any baselines to validate the results. A2C SB performs better than
A2C WUJI and A2C TF in terms of detecting bugs. The latter slightly per-
forms better than A2C WUJI. Wuji randomly initialized DNNs policies that
are used by their A2C agent to detect bugs, which explained the faster detec-
tion of bugs between [0.25E + 6, 1.25E + 6] steps in comparison to the others
A2Cs. By employing DRL algorithms from diﬀerent frameworks, the data to
feed the DNNs are directly collected from the environment. This diﬀerence,
we think, might aﬀect the agent eﬀectiveness to detect bugs [37].

Figure 7 and 8 shows respectively the average number of detected bugs
and average cumulative reward obtained by the PPO algorithms from Stable-
baselines3 (PPO SB) and Tensorforce (PPO TF) frameworks. The PPO algo-
rithms detect 1 to 2 more bugs than the A2Cs.

Figure 9 shows the statistical results of the number of bugs discovered
by all the studied RL conﬁgurations. A2C and PPO algorithms from the se-
lected RL frameworks have a faster convergence rate, until the number of bugs

4 https://github.com/keras-rl/keras-rl/blob/216c3145f3dc4d17877be26ca2185ce7db462bad/

rl/agents/dqn.py

20

Nouwou Mindom et al.

Fig. 7 Number of bugs detected by
PPO agents from diﬀerent frameworks.

Fig. 8 Average cumulative reward earned by
PPO agents from diﬀerent frameworks.

Fig. 9 The number of bugs discovered using diﬀerent strategies after 4 millions steps for
Block Maze.

Table 4 State coverage of DRL algorithms on the Block Maze game.

State coverage

182

182

A2C SB A2C TF

PPO SB
182

PPO TF DQN SB DQN TF DQN KR

177.9

37.2

35.3

39.7

detected reaches 19. Figure 9 shows that their bug detection capabilities are
statistically similar. The A2C implementation of Wuji’s paper [57] detects 19%
less bugs than the others A2Cs and PPOs after 4 millions steps during testing.
Among the studied DQN strategies, DQN SB, DQN KR and DQN TF detect
respectively 88%, 92%, 98% less bugs than the A2C implementation of Wuji’s
paper at the same step number.

In addition to the number of bugs, we analyze the coverage obtained by
each DRL strategies, as well as their state coverage on the Block Maze game.
The line coverage is exactly the same for all strategies: 96%. The Block Maze
has a total of 400 potential states to be visited by the RL agent. Table 4 shows
the results of the state coverage obtained by the DRL algorithms from the RL
frameworks we have evaluated. As expected, A2Cs and PPOs have large state

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

21

Fig. 10 Average cumulative reward obtained by diﬀerent DRL algorithms after 4 millions
steps for Block Maze.

coverage as they are able to detect more bugs. The state coverage obtained by
the DQNs algorithms are lower as they lead to fewer bugs detected. The code
is relatively easy to cover, as opposed to the state coverage. Thus, detecting
bugs by maximizing the state coverage could lead to better performance for
game testing.

In terms of winning the game (i.e., reaching the gold position of the Block
Maze as illustrated in Figure 2) none of our strategies are successful. Our
results in Figure 10 show that the RL agents earn negative reward for all steps
during testing. It is easily justiﬁed by the bugs detected, as when detecting
bugs, the RL agents that we implemented are negatively rewarded. As in [57],
at each run, the gold position and the bugs are randomly injected for bias
mitigation. In this way, it can be hard on the agent to ﬁnd a path to the goal
and win the game.

Finding 1: PPOs ﬁnd more bugs in the Block Maze compared
to other DRL algorithms across all examined frameworks, and
Stable-Baselines demonstrates the best performance.

RQ2: Table 5 shows that on average the A2Cs and PPOs algorithms perform
better than the DQNs algorithms with a high bug detection number between
12.23 and 15.28. In bold are the mean of the RL conﬁgurations that perform
better with a p-value <= 0.05. The A2C algorithms have similar performance,
same as the PPO algorithms: while PPOs detect more bugs, they do not have
statistically signiﬁcant results in comparison to A2Cs. The following items
summarise our results per DRL algorithm where > denotes greater perfor-
mance:

22

Nouwou Mindom et al.

Table 5 Results of Welch’s ANOVA and Games-Howell post-hoc test regarding the number
bugs detected by DRL algorithms.

A
A2C SB
A2C SB
A2C SB
A2C SB
A2C SB
A2C SB
A2C SB
A2C TF
A2C TF
A2C TF
A2C TF
A2C TF
A2C TF
A2C WUJI
A2C WUJI
A2C WUJI
A2C WUJI
A2C WUJI
DQN KR
DQN KR
DQN KR
DQN KR
DQN SB
DQN SB
DQN SB
DQN TF
DQN TF
PPO SB

B
A2C TF
A2C WUJI
DQN KR
DQN SB
DQN TF
PPO SB
PPO TF
A2C WUJI
DQN KR
DQN SB
DQN TF
PPO SB
PPO TF
DQN KR
DQN SB
DQN TF
PPO SB
PPO TF
DQN SB
DQN TF
PPO SB
PPO TF
DQN TF
PPO SB
PPO TF
PPO SB
PPO TF
PPO TF

mean(A) mean(B)

14.44
14.44
14.44
14.44
14.44
14.44
14.44
12.43
12.43
12.43
12.43
12.43
12.43
14.66
14.66
14.66
14.66
14.66
0.85
0.85
0.85
0.85
1.25
1.25
1.25
0.3
0.3
15.28

12.43
14.66
0.85
1.25
0.3
15.28
12.23
14.66
0.85
1.25
0.3
15.28
12.23
0.85
1.25
0.3
15.28
12.23
1.25
0.3
15.28
12.23
0.3
15.28
12.23
15.28
12.23
12.23

p-value
0.09
0.99
0
0
4.55E-15
0.94
0.03
5.13E-05
7.08E-14
0
0
0.0005
0.99
0
8.84E-14
7.13E-14
0.91
5.54E-07
0.0006
0
0
0
2.00E-14
0
0
4.44E-14
0
6.02E-05

A2C Algorithms:

– A2C WUJI > A2C TF

PPO Algorithms:

– PPO SB > PPO TF

DQN Algorithms:

– DQN SB > DQN KR > DQN TF

Finding 2: A2Cs and PPOs show statistically signiﬁcant per-
formance compared to DQN algorithms in ﬁnding bugs in the
examined game.

RQ3: Our ﬁndings show that we do not get similar results even though we ran
our experiments with the same settings. We explain this by the fact that each
RL framework that we used in this study do not provide the same hyperpa-
rameters regarding the RL algorithm. For example, the DQN algorithm from

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

23

Stable-baselines has an additional hyperparameter called ”gradient steps” to
perform gradient process as there are steps done during a rollout, instead of
doing it after a complete rollout is done. These other hyperparameters, even
with default values, can slightly improve eﬃciency as we observe on our results.

4.2 Test case prioritization

Table 6 and 7 show the averages and standard deviations of APFD and NRPA
for the eight datasets, using diﬀerent conﬁgurations (i.e., combinations of rank-
ing model, RL framework and algorithm). The ﬁrst column reports diﬀerent
RL algorithms, the second column reports the ranking models followed by
four datasets per table (a total of eight datasets). Each dataset column is
subdivided into the RL frameworks. In the rest of this section, we use [rank-
ing model]-[RL algorithm]-[RL framework] to refer to RL conﬁgurations. For
example, Pairwise-DQN-KR corresponds to a conﬁguration of the pairwise
ranking model and the DQN algorithm from the Keras-rl framework. For each
dataset (column), the relative performance rank of conﬁgurations in terms of
APFD or NRPA are expressed with n , where a lower rank indicates better
performance. Again, we analyze the diﬀerences in the results by using Welch’s
ANOVA and Games-Howell post-hoc test.

Table 8 and 9 show the overall training times for the ﬁrst 10 cycles across
datasets. Similarly, Table 10 and 11 show the averages and standard deviations
of prediction time (ranking) for the ﬁrst 10 cycles across datasets. Because of
our hardware limitations and considering that these experiments take a lot
of time, we only collected the training time and prediction time for Keras-
rl and Tensorforce frameworks. Each cell value represents a conﬁguration as
mentioned before. For each dataset, the relative performance ranks of conﬁg-
urations in terms of training/prediction time are expressed with n , where a
lower rank n indicates better performance.

RQ1: As shown in Table 6, pairwise conﬁgurations perform best across Stable-
baselines’s algorithms. Pairwise-A2C-SB yields to the best averages. Based on
the post-hoc test, Pairwise-A2C-SB performs best across all datasets followed
by Pairwise-A2C-TF and Pairwise-DQN-KR. Similarly, the Stable-baselines
framework performs best regarding the pointwise ranking model. Using the
listwise ranking model, the Stable-baselines framework performs poorly, but
the other frameworks perform well, e.g., Tensorforce yields to better averages
compared to Stable-baselines. While Pairwise-A2C-SB has the best perfor-
mance overall, we found some cases where Tensorforce conﬁgurations outper-
form Stable-baselines (more than 10% improvement) as shown in Table 12.

We noticed that Tensorforce framework performs better when using IOFROL

or PAINT which are simple datasets. To show the importance of selecting the
best RL conﬁguration, we measured the eﬀect size of the diﬀerences between
pairs of conﬁgurations based on the common language eﬀect size (CLE) [32],
[4]. CLE estimates the probability that a randomly sampled score from one

24

Nouwou Mindom et al.

M
A
R
T

R
L
-
B
S
2

R
L
-
B
S
1

O
p
t
i

m
a
l

A
2
C

D
D
P
G

D
Q
N

P
A

P
O

P
O

N
A

L
I

P
O

P
A

P
O

L
I

P
A

.
7
6
±
.
0
5

.
9
1
±
.
0
5

.
9
8
±
.
0
2

.
8
7
±
.
0
9

.
7
6
±
.
0
5

9

2

1

3

9

.
7
8
±
.
0
8

5

.
7
7
±
.
0
4

6

.
8
2
±
.
0
7

5

.
7
3
±
.
0
6

1
1

.
7
7
±
.
0
5

.
9
8
±
0
2

1

.
8
5
±
0
6

4

.
7
7
±
.
0
6

7

.
9
7
±
0
3

.
7
4
±
.
0
7

1
0

.
7
4
±
.
0
7

1
0

.
7
7
±
.
0
5

7

2

.
7
7
±
.
0
5

7

.
7
7
±
.
0
5

7

.
7
7
±
.
0
5

.
8
5
±
0
7

4

.
7
6
±
.
1
1

1
0

.
9
5
±
0
4

.
9
3
±
.
0
2

.
8
4
±
.
1
3

N
A

N
A

N
A

N
A

N
A

.
7
4
±
.
0
8

1
1

.
7
6
±
.
0
4

.
7
7
±
.
0
4

8

6

.
7
6
±
.
0
6

.
9
2
±
.
0
5

9

3

.
9
8
±
.
0
3

1

.
9
6
±
.
0
2

.
9
0
±
.
0
5

N
A

N
A

N
A

N
A

N
A

.
7
8
±
.
0
7

.
7
6
±
.
0
4

.
7
6
±
.
0
6

9

6

8

7

.
7
7
±
.
0
5

.
8
6
±
.
0
7

.
9
5
±
.
0
4

.
8
0
±
.
0
7

8

2

1

5

8

1

.
9
4
±
.
0
2

.
8
9
±
.
0
7

N
A

N
A

N
A

N
A

N
A

.
7
8
±
.
0
4

.
7
7
±
.
0
6

7

9

.
7
0
±
.
2
1

1
2

.
7
5
±
.
0
5

1
0

.
7
8
±
.
0
3

.
8
0
±
.
0
4

4

.
8
0
±
.
0
4

6

4

.
7
6
±
.
0
6

.
9
0
±
.
0
4

.
9
6
±
.
0
4

.
8
6
±
.
0
7

.
7
6
±
.
0
5

9

3

1

5

8

.
9
5
±
.
0
2

.
9
5
±
.
0
2

N
A

N
A

N
A

N
A

N
A

.
7
9
±
.
0
4

6

.
3
8
±
.
3
8

1
4

.
6
5
±
.
0
2

1
1

.
7
7
±
.
0
4

7

.
7
4
±
.
0
0

1
0

.
7
3
±
.
0
2

1
1

.
8
3
±
0
7

3

.
7
1
±
.
2
4

1
1

.
9
4
±
0
5

2

.
8
9
±
0
7

4

.
4
0
±
.
3
7

1
3

o
t
h
e
r
s

f
o
r

e
a
c
h

d
a
t
a
s
e
t

(
c
o
l
u
m
n
)

i

n

t
e
r
m

s

o
f

N
R
P
A
o
r
A
P
F
D

,

b
a
s
e
d

o
n

s
t
a
t
i
s
t
i
c
a
l

t
e
s
t
i
n
g
.

R
L
-
B
S
2
,

a
n
d
M
A
R
T
)

f
o
r

I

O

,

C
O
M
P

,

L
A
N
G

,

a
n
d
M
A
T
H
d
a
t
a
s
e
t
s
.

T
h
e

i

n
d
e
x

i

n

e
a
c
h

c
e
l
l

s
h
o
w
s

t
h
e

p
o
s
i
t
i
o
n

o
f

a

c
o
n
ﬁ
g
u
r
a
t
i
o
n

(
r
o
w
)
w
i
t
h

r
e
s
p
e
c
t

t
o

T
a
b
l
e

7

T
h
e

a
v
e
r
a
g
e

p
e
r
f
o
r
m
a
n
c
e

o
f

i

d
ﬀ
e
r
e
n
t

c
o
n
ﬁ
g
u
r
a
t
i
o
n
s

i

n

t
e
r
m

s

o
f

A
P
F
D
a
n
d
N
R
P
A

,

a
l
o
n
g
w
i
t
h

t
h
e

r
e
s
u
l
t
s

o
f

t
h
e

t
h
r
e
e

b
a
s
e
l
i

n
e
s

(
R
L
-
B
S
1
,

R
M

S
B

K
R

T
F

S
B

K
R

T
F

S
B

K
R

T
F

S
B

K
R

T
F

(
N
R
P
A
)

I

O

(
N
R
P
A
)

C
O
M
P

(
N
R
P
A
)

L
A
N
G

(
N
R
P
A
)

M
A
T
H

M
A
R
T

R
L
-
B
S
2

R
L
-
B
S
1

O
p
t
i

m
a
l

A
2
C

D
D
P
G

D
Q
N

P
A

P
O

P
O

N
A

L
I

P
O

P
A

P
O

L
I

P
A

R
M

.
4
8
±
.
1
8

1
4

.
5
7
±
.
2
3

.
7
2
±
.
2
4

.
5
2
±
.
1
3

.
5
0
±
.
1
9

6

2

1
0

1
2

N
A

N
A

.
6
3
±
.
1
6

.
7
9
±
.
1
4

N
A

N
A

N
A

.
5
6
±
.
2
1

8

.
6
2
±
.
2
2

2

.
6
1
±
.
2
0

3

.
5
5
±
.
3
2

.
5
3
±
1
3

5

.
5
1
±
0
8

1
1

.
5
2
±
.
0
1

.
5
2
±
2
4

1
1

.
5
9
±
.
2
2

.
5
0
±
.
1
0

1
2

.
5
9
±
.
0
9

.
5
5
±
.
0
9

7

5

4

9

.
7
6
±
.
0
3

1

.
5
0
±
.
1
0

1
2

.
5
7
±
.
2
5

.
5
9
±
.
2
6

.
4
9
±
.
2
5

1
3

.
5
2
±
.
1
4

.
5
5
±
.
1
3

.
5
2
±
.
1
3

9

4

8

N
A

N
A

.
7
4
±
.
2
4

.
8
9
±
.
1
4

N
A

N
A

N
A

.
5
2
±
.
0
8

7

2

.
5
4
±
.
0
4

.
7
4
±
.
2
5

4

1

.
5
1
±
.
0
0

1
0

3

6

.
7
8
±
.
0
8

.
8
9
±
.
0
7

.
9
7
±
.
0
4

.
8
8
±
.
0
8

.
7
9
±
.
0
7

9

3

1

5

6

.
9
6
±
.
0
3

.
9
0
±
.
0
5

N
A

N
A

N
A

N
A

N
A

.
7
3
±
.
0
8

.
7
5
±
.
0
7

1
3

1
2

.
7
5
±
.
0
4

.
7
7
±
.
0
7

1
1

1
0

.
7
8
±
.
0
7

8

.
7
7
±
.
0
7

1
0

.
7
8
±
0
8

.
7
7
±
.
0
6

.
9
2
±
.
0
5

.
9
6
±
.
0
5

.
8
2
±
.
0
7

.
7
7
±
.
0
6

9

3

1

5

9

.
9
0
±
.
0
5

.
8
9
±
.
0
9

N
A

N
A

N
A

N
A

N
A

.
7
8
±
.
0
6

.
7
8
±
.
0
6

7

7

.
7
4
±
.
0
5

1
0

.
7
8
±
.
0
6

.
7
9
±
.
0
8

7

6

.
7
2
±
.
0
3

1
1

.
7
7
±
.
0
5

.
9
4
±
0
6

2

.
8
8
±
0
7

4

.
7
8
±
.
0
6

7

.
9
5
±
0
6

2

.
8
6
±
0
7

4

.
7
7
±
.
0
5

8

S
B

K
R

(
A
P
F
D
)

I

P
A
N
T

T
F

S
B

K
R

(
A
P
F
D
)

I

O
F
R
O
L

T
F

S
B

K
R

(
N
R
P
A
)

C
O
D
E
C

T
F

S
B

K
R

(
N
R
P
A
)

I

M
A
G

T
F

r
e
s
p
e
c
t

t
o

o
t
h
e
r
s

f
o
r

e
a
c
h

d
a
t
a
s
e
t

(
c
o
l
u
m
n
)

i

n

t
e
r
m

s

o
f

N
R
P
A
o
r
A
P
F
D

,

b
a
s
e
d

o
n

s
t
a
t
i
s
t
i
c
a
l

t
e
s
t
i
n
g
.

R
L
-
B
S
2
,

a
n
d
M
A
R
T
)

f
o
r

I

P
A
N
T

,

I

O
F
R
O
L

,

C
O
D
E
C

,

a
n
d

I

M
A
G
d
a
t
a
s
e
t
s
.

T
h
e

i

n
d
e
x

i

n

e
a
c
h

c
e
l
l

s
h
o
w
s

t
h
e

p
o
s
i
t
i
o
n

o
f

a

c
o
n
ﬁ
g
u
r
a
t
i
o
n

(
r
o
w
)

w
i
t
h

T
a
b
l
e

6

T
h
e

a
v
e
r
a
g
e

p
e
r
f
o
r
m
a
n
c
e

o
f

i

d
ﬀ
e
r
e
n
t

c
o
n
ﬁ
g
u
r
a
t
i
o
n
s

i

n

t
e
r
m

s

o
f

A
P
F
D
a
n
d
N
R
P
A

,

a
l
o
n
g
w
i
t
h

t
h
e

r
e
s
u
l
t
s

o
f

t
h
e

t
h
r
e
e

b
a
s
e
l
i

n
e
s

(
R
L
-
B
S
1
,

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

25

Table 8 The sum of training time (in minutes) of RL conﬁgurations for the ﬁrst 10 cycles
across PAINT, IOFROL, CODEC, and IMAG datasets.

PAINT

IOFROL

CODEC

IMAG

RM

PA

DQN

KR

TF

KR

74.7 3

255.4 4

438.5 3

DDPG PO 52.0 1

53.5 2

405.9 2

A2C

PA

PO

NA

NA

331.6 6

263.8 5

NA

NA

TF

567.1 4

374.4 1

2466.2 5

2510.0 6

KR

19.1 2

31.3 4

NA

NA

TF

20.6 3

18.9 1

157.7 6

129.3 5

KR

20.2 2

23.0 3

NA

NA

TF

23.6 4

18.3 1

154.9 6

105.3 5

Table 9 The sum of training time (in minutes) of RL conﬁgurations for the ﬁrst 10 cycles
across IO, COMP, LANG, and MATH datasets.

IO

COMP

LANG

MATH

RM

PA

KR

28.7 3

DQN

DDPG PO 27.2 2

A2C

PA

PO

NA

NA

TF

45.3 4

25.5 1

180.8 6

152.3 5

KR

24.7 2

24.42 1

NA

NA

TF

24.8 3

25.1 4

188.0 5

193.9 6

KR

35.5 2

55.5 4

NA

NA

TF

37.8 3

35.2 1

274.0 5

221.1 4

KR

157.9 4

132.9 2

NA

NA

TF

138.6 3

129.4 1

1040.1 6

878.1 5

Table 10 The average of prediction (ranking) time (in seconds) of RL conﬁgurations for
the ﬁrst 10 cycles across PAINT, IOFROL, CODEC, and IMAG datasets.

PAINT

IOFROL

CODEC

IMAG

RM

KR
1.8±.7 1
DQN
DDPG PO 3.2±1.6 3

PA

A2C

PA

PO

NA

NA

TF
4.4±.6 4
3.2±1.4 2
7.4±1.6 6
6.2±.05 5

KR
2.7±1.7 1
3.3±1.5 3

NA

NA

TF
3.9±2.1 4
3.2±1.5 2
7.9±1.0 5
11.1±2.8 6

KR
1.3±.5 1
4.8±2.6 4

NA

NA

TF
1.9±.9 2
3.2±1.6 3
7.0±1.1 6
6.7±1.5 5

KR
1.8 ±1.0 1
4.1±2.0 4

NA

NA

TF
2.8±1.3 2
3.3±1.6 3
7.4±1.1 5
6.1±.06 5

Table 11 The average of prediction (ranking) time (in seconds) of RL conﬁgurations for
the ﬁrst 10 cycles across IO, COMP, LANG, and MATH datasets.

RM

KR
1.5±.5 1
DQN
DDPG PO 3.2±1.5 2

PA

A2C

PA

PO

NA

NA

IO

COMP

LANG

MATH

TF
4.6±4.0 3
3.2±1.5 2
5.8±.3 4
5.9±.06 5

KR
1.3±.5 1
3.2±1.5 3

NA

NA

TF
1.9±.8 2
3.3±1.5 4
6.2±.3 5
7.9±1.0 6

KR
1.3±.6 1
4.7±2.3 4

NA

NA

TF
2.0±.9 2
3.1±1.5 3
7.4±2.5 6
6.1±.06 5

KR
1.8±.9 1
3.3±1.5 4

NA

NA

TF
2.2±1.0 2
3.1±1.5 3
7.1±1.3 5
7.9±.2.3 6

population is greater than a randomly sampled score from the other popula-
tion. As shown in Table 13, The CLE values among one of the worst and best
cases for the six enriched datasets are over 88%, whereas they are 79% and
85% for the simple Paint-Control and IOFROL datasets, respectively. These
results show that, for each dataset, we have, with high probability, an RL
conﬁguration that has adequately learned a ranking strategy.

In terms of training time, as shown in Table 8 and 9, multiple pairwise and
pointwise conﬁgurations perform well for some datasets. Figures 11 and 12
show the statistical results of the training time involving Pairwise-DQN and
Pointwise-DDPG conﬁgurations respectively. The results show that Pointwise-
DDPG-TF performs best followed by Pointwise-DDPG-KR. Training times
involving the listwise ranking model are expected to have the worst values.
Due to limited hardware settings, we could not ﬁnish collecting the results.
As mentioned in [5], the listwise ranking model uses dummy test cases for

26

Nouwou Mindom et al.

Table 12 Improvement gained by RL conﬁgurations.

RL conﬁgurations

Dataset

LISTWISE-DQN-TF
POINTWISE-DDPG-TF
PAIRWISE-A2C-TF

PAINT
PAINT
IOFROL

NRPA or
APFD
.59±.22
.59±.26
.74±.25

Improvement (%)

18%
13%
34%

Table 13 Common Language Eﬀect Size between one of the worst and best conﬁgurations
for each dataset based on accuracy.

Dataset
Paint-Control
IOFROL
Codec
Compress
Imaging
IO
Lang
Math

Best Conf.
PAIRWISE-A2C-SB
PAIRWISE-DQN-TF
PAIRWISE-A2C-SB
PAIRWISE-A2C-SB
PAIRWISE-A2C-SB
PAIRWISE-A2C-SB
PAIRWISE-A2C-SB
PAIRWISE-A2C-SB

Worst Cons.
LISTWISE-A2C-SB
LISTWISE-A2C-SB
POINTWISE-DDPG-KR
POINTWISE-DDPG-KR
POINTWISE-DDPG-TF
LISTWISE-DQN-KR
PAIRWISE-A2C-TF
PAIRWISE-DQN-TF

CLE
.79
.85
.99
.99
.99
.99
.88
.93

Fig. 11 Training time of Pairwise-DQN
conﬁguration accross RL frameworks for
enriched datasets.

Fig. 12 Training time Pointwise-DDPG
conﬁguration accross RL frameworks for
enriched datasets.

padding to prevent test cases from being selected repeatedly. Nevertheless,
dummy test cases can be selected many times when the agent policy is not
suitable enough, which increases training time, and will results in months of
training. It is worth mentioning that, since RL agents are trained oﬄine, the
training time does not add any delay to the CI build process. Also, with good
hardware conﬁgurations these results would be improved as the training will
take less time.

In terms of prediction time, as shown in Table 10 and 11, similar to the
training time, multiple conﬁgurations (pairwise or pointwise) perform well for
some of the datasets. Based on the post-hoc test, Pairwise-DQN-TF performs
best on average followed by Pairwise-DQN-KR. In contrast, the listwise rank-
ing model with all algorithms/frameworks feature the worst prediction time.
The prediction time among pointwise and pairwise conﬁgurations goes up to

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

27

Table 14 Common Language Eﬀect Size between Pairwise-A2C-SB and selected baselines.

RL-BS1 RL-BS2 MART
CLE
.856
.862
.751
.914
.771
.588
NA
NA

CLE
NA
NA
NA
NA
NA
NA
.622
.243

CLE
.961
.579
.801
.710
.411
.588
NA
NA

IO
CODEC
IMAG
COMP
LANG
MATH
PAINT.
IOFROL

Table 15 Common Language Eﬀect Size between Pairwise-DQN-KR and selected baselines.

RL-BS1 RL-BS2 MART
CLE
.527
.408
.396
.280
.272
.204
NA
NA

CLE
.102
.146
.320
.065
.065
.204
NA
NA

CLE
NA
NA
NA
NA
NA
NA
.468
.181

IO
CODEC
IMAG
COMP
LANG
MATH
PAINT.
IOFROL

Table 16 Common Language Eﬀect Size between Pairwise-A2C-TF and selected baselines.

RL-BS1 RL-BS2 MART
CLE
.303
.081
.026
.014
.195
.067
NA
NA

CLE
NA
NA
NA
NA
NA
NA
.419
.500

CLE
.000
.009
.121
.000
.000
.000
NA
NA

IO
CODEC
IMAG
COMP
LANG
MATH
PAINT.
IOFROL

7 seconds, notably for A2C-PA-TF and A2C-PO-TF, which is non negligible
for CI builds.

The last three rows of Table 6 and 7 show the averages and standard
deviations of baselines conﬁgurations in terms of NRPA and APFD values
collected from [5], for the datasets on which they were originally experimented.
Tables 14, 15, and 16 show the results of CLE between the best conﬁguration
of each framework and selected baselines for all datasets, to assess the eﬀect
size of diﬀerences.

The row RL-BS1 in Table 6 and 7 shows the results of an RL-based solu-
tion reported in [5]. As shown in the ﬁrst two columns, for the Paint-Control
dataset, Pairwise-A2C-SB fares slightly better than RL-BS1 however with a
low CLE of 62.2. Also, both solutions (RL-BS1, Pairwise-A2C-SB) are close
to the optimal ranking (the row labeled as “Optimal” in Table 6). For dataset

28

Nouwou Mindom et al.

IOFROL, RL-BS1 performs better than Pairwise-A2C-SB: however both so-
lutions do not perform well as their values are lower than the optimal ranking.
RL-BS1 performs better than Pairwise-A2C-TF and Pairwise-DQN-KR for
both simple datasets. This is justiﬁed with low CLE values reported in Ta-
bles 15 and 16. These results are anyway lower than the optimal ranking. As
pointed out in [5], test execution history provided by simple datasets is not
suﬃcient enough to learn an accurate test prioritization policy.

The row of RL-BS2 in Tables 6 and 7 shows the results of an RL-based
solution reported in [5]. For all datasets except MATH, Pairwise-A2C-SB fares
signiﬁcantly better than RL-BS2 with CLE values between 77.1 and 91.4 as
shown in Table 14. For the MATH dataset, Pairwise-A2C-SB performs almost
the same as RL-BS2. In contrast, RL-BS2 performs better than Pairwise-A2C-
TF and Pairwise-DQN-KR for all datasets: CLE values between Pairwise-A2C-
TF and RL-BS2 ranges between 1.4 and 30.3, and between 20.4 and 52.7 for
Pairwise-DQN-KR and RL-BS2. Thus, according to these results Pairwise-
A2C-SB improves the baselines in the use of RL for test case prioritization.

The row labeled by MART (MART ranking model) in Table 6 and 7 pro-
vides the results of the best ML-based solution reported in [5]. For CODEC
and MATH datasets, Pairwise-A2C-SB performs equivalently as MART. We
observe 57.9 and 58.8 as CLE values respectively for CODEC and MATH
datasets. For other datasets, Pairwise-A2C-SB fares better than MART except
for the LANG dataset where it performs poorly. The CLE of Pairwise-A2C-
SB vs. MART ranges between 0.411 to 0.961 with an average of 0.675, i.e.,
in 67.5% of the cycles, Pairwise-A2C-SB fares better than MART. Then, we
can conclude that Pairwise-A2C-SB advances state-of-the-art compared to the
best ML-based ranking technique (MART). Pairwise-A2C-TF and Pairwise-
DQN-KR solutions perform poorly in comparison to MART with 0.043 and
0.150 CLE averages respectively.

Finding 3: The performance of DQN algorithms is close to
the A2C algorithms when applying to the Pairwise ranking
model.

RQ2: Figure 13 shows the statistical results of APFD and NRPA metrics for
the DQN-Pairwise conﬁguration. The results show that Stable-baselines frame-
work perfoms better for all enriched datasets. Similarly, Figure 14 shows the
statistical results of APFD and NRPA metrics regarding the DDPG-Pointwise
conﬁguration. Moreover, to analyze the accuracy of RL algorithms w.r.t the
relative performance, we perform two sets of Welch ANOVA and Games-Howell
post-hoc tests corresponding to the pairwise and pointwise ranking models,
based on the result of all algorithms across enriched datasets. Table 17 shows
for each conﬁguration the calculated mean and p-value.

To compare training time, we perform a similar analysis based on training

time for the 10 ﬁrst cycles. We summarize the results as follows:

– Pairwise and enriched datasets:

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

29

Fig. 13 APFD (simple datasets) or NRPA (enriched datasets) of DQN-PAIRWISE con-
ﬁguration accross RL frameworks for all datasets: Stable-baselines vs. Keras-RL (left) and
Stable-baselines vs. Tensorforce (right).

Fig. 14 APFD (simple datasets) or NRPA (enriched datasets) of DDPG-POINTWISE
conﬁguration accross RL frameworks for all datasets: Stable-baselines vs. Keras-RL (left)
and Stable-baselines vs. Tensorforce (right).

– DQN-KR > DQN-TF

– Pointwise and enriched datasets:
– DDPG-TF > DDPG-KR

For the prediction time, here is the result based on training time for the

10 ﬁrst cycles:

– Pairwise and enriched datasets:

– DQN-KR > DQN-TF

– Pointwise and enriched datasets:
– DDPG-TF > DDPG-KR

Based on the results presented, we can conclude that listwise conﬁgurations
have the worst training and prediction times regardless of the choice of RL.
Consequently, it can not be recommended for use in test case prioritization.
When using a pairwise or pointwise ranking model, the RL conﬁgurations fare
relatively well in terms of prediction times. Nevertheless, A2C-TF has the
worst performance in terms of training time.

30

Nouwou Mindom et al.

Table 17 Results of Welch ANOVA and Games-Howell post-hoc tests on pairwise and
pointwise ranking models.

Pairwise and
enriched datasets

Pointwise and
enriched datasets

A
DQN TF
DQN TF
DQN TF
DQN TF
A2C SB
A2C SB
A2C TF
A2C TF
DQN KR
A2C SB
A2C SB
A2C SB
A2C SB
A2C TF
A2C TF
A2C TF
DDPG KR
DDPG KR
DDPG SB

B
A2C SB
A2C TF
DQN KR
DQN SB
A2C TF
DQN KR
DQN KR
DQN SB
DQN SB
A2C TF
DDPG KR
DDPG SB
DDPG TF
DDPG KR
DDPG SB
DDPG TF
DDPG SB
DDPG TF
DDPG TF

mean(A) mean(B)

0.80
0.80
0.80
0.80
0.92
0.92
0.71
0.71
0.86
0.87
0.87
0.87
0.87
0.82
0.82
0.82
0.75
0.75
0.87

0.92
0.71
0.86
0.92
0.71
0.86
0.86
0.92
0.92
0.82
0.75
0.87
0.78
0.75
0.87
0.78
0.87
0.78
0.78

p-value
9.85E-14
0.04
0.001
1.37E-12
5.72E-08
9.72E-05
0.0001
1.18E-07
0.0005
0.005
1.50E-11
0.99
7.59E-08
2.27E-05
0.002
0.02
3.74E-12
0.26
2.07E-08

Finding 4: Overall Stable-baselines framework has better per-
formance than Keras-rl and Tensorforce framework, therefore
can be recommended when using RL for test case prioritiza-
tion.

RQ3: Figure 15, 17, 16 show the results of the Pairwise-DQN conﬁgurations
from Tensorforce and Keras-rl frameworks in terms of NRPA, accumulated
reward obtained by agents during training and accumulated reward obtained
by agents during testing on CODEC dataset. The results are collected for the
ﬁrst 10 CI cycles over 10 diﬀerent runs. Regarding the DQN algorithm, Keras-
rl and Tensorforce have the same performance in terms of reward but perform
diﬀerently in terms of NRPA. Similarly as with the others DRL algorithms,
we do not observe stable results across the DRL frameworks.

5 Recommendations about frameworks/algorithms selection

In this section, we discuss our recommendations regarding the selection of
DRL frameworks/algorithms for researchers and practitioners.

The results of our analysis indicate that there are some diﬀerences in using
the same algorithm from diﬀerent DRL frameworks. This is due to the diver-
sity of hyperparameters that are oﬀered by diﬀerent RL frameworks. Among
the studied RL frameworks, the DQN algorithm from Keras-rl has the least
number of hyperparameters (13 in total), leading to less ﬂexibility in improving
the agent’s training process thus explaining its poor performance.

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

31

Fig. 15 Accumulated reward during train-
ing of the Pairwise-DQN conﬁgurations for
the ﬁrst 10 CI cycles over 10 runs on CODEC
dataset.

Fig. 16 Accumulated reward during testing
of the Pairwise-DQN conﬁgurations for the
ﬁrst 10 CI cycles over 10 runs on CODEC
dataset.

Fig. 17 NRPA of the Pairwise-DQN conﬁg-
urations for the ﬁrst 10 CI cycles over 10 runs
on CODEC dataset.

Recommendation 1: When applying a DRL algorithm from a
DRL framework to an SE problem, we recommend choosing
the DRL framework that oﬀers the largest number of hy-
perparameters to have the ﬂexibility to improve the agent
eﬃciency.

Regardless of the studied frameworks, PPO’s and A2C’s algorithms have
shown good performance when applying on the game testing problem. The
PPO algorithm has shown slightly better performance as it has detected 1 to
2 more bugs than the A2C algorithms when used to detect bugs in the Block
Maze game. Nevertheless PPO algorithm from Stable-baselines frameworks
has detected 2 more bugs than PPO algorithm from Tensorforce. Regarding
the studied test case prioritization problem, Pairwise-A2C-SB yields to the
best performance.

32

Nouwou Mindom et al.

Recommendation 2: Given the studied game testing problem,
we recommend choosing PPO algorithm over A2C for better
performance, especially PPO algorithm from Stable-baselines.

Recommendation 3: We recommend choosing A2C from
Stable-baselines for the test case prioritization problem.

Regarding the game testing problem, the DQN algorithm among all the
studied frameworks has poor performance when detecting bugs. It shows that
DQN has a poor exploration capability. For the test case prioritization prob-
lem, Table 17 shows that for the pairwise conﬁguration and some enriched
datasets DQN’s performance is close to the A2C algorithm. It shows that
DQN has a good ranking capability. The DQN algorithm computes the Q-
values of each state-action pair in order to predict the next action to take. It
is suitable for the pairwise ranking model as its action space is discrete (0 or
1), half the size of the action space of the game testing problem (0, 1, 2 or 4).

Recommendation 4: Regardless of the DRL framework, we
recommend choosing the DQN algorithm for simple SE tasks
with discrete action space that are small.

To showcase the diﬀerence between employing a simple DRL algorithm
from two frameworks, we performed some additional analysis of the hyper-
parameters oﬀered by Stable-baselines and Tensorforce regarding the DQN
algorithm and conducted some experiments. Here are our ﬁndings:

– Stable-baselines3 provides a total of 25 hyperparameters while Tensor-

force provides 22 hyperparameters.

– Table 18 describes for Stable-baselines3 and Tensorforce, the hyperparame-
ters that diﬀer from each other. An interesting hyperparameter is the vari-
able noise from Tensorforce, that adds Gaussian noise [50] to all trainable
variables as an exploration strategy. Adding noise to DRL agents during
training has shown to improve their exploration of the environment and
their gains of reward throughout training [16].

– We consider the variable noise=0.5 as an additional hyperparameter for
the DQN algorithm from Tensorforce. Therefore, we collected the number
of detected bugs and average reward of the DQN agent from Tensorforce
for 50, 000 steps of training on the Block Maze game. Figures 18 and 19
show that the DQN agent from Tensorforce is able to detect more bugs
than initially (see Figure 3 and 4), with more gained reward.

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

33

Table 18 DQN algorithm hyperparameters: diﬀerences between Stable-baselines3 and Ten-
sorforce.

Frameworks

Hyperparameters

Soft Update Coeﬃcient

Gradient Step

Stable-baselines3

Exploration Fraction

Gradient Clipping

Tensorboard Log

Evaluate Environment

Seed
Device

Model Setup

Variable Noise

State Preprocessing
Reward Preprocessing
Return of processing
L2 Regularization

Entropy Regularization

Huber Loss

Tensorforce

Deﬁnitions
The target network of the DQN algorithm is
updated frequently by a little amount.
Number of gradient steps to do
after each rollout.
Fraction of the training period over
which the exploration rate is reduced
The maximum value for
the gradient clipping
The log location for tensorboard
Whether to create a second environment that
will be used for evaluating the agent periodically.
Seed for the pseudo random generators
Device on which the code should be run
Whether or not to build the network at
the creation of the instance
Alternative exploration mechanism by
adding Gaussian noise to all trainable variables
State preprocessing as layer or list of layers
Reward preprocessing as layer or list of layers
Return processing as layer or list of layers,
L2 regularization loss weight
To discourage the policy distribution
from being too certain
Threshold of the Huber loss function

Fig. 18 Number of bugs detected by DQN
Stable-baselines and DQN (with Gaussian
noise) Tensorforce.

Fig. 19 Average cumulative reward earned
by DQN Stable-baselines and DQN (with
Gaussian noise) Tensorforce.

Recommendation 5: In the context of game testing, or testing
through exploration, we recommend an DRL framework that
oﬀers more eﬀective techniques as exploration strategy such
as Tensorforce.

34

6 Related work

Nouwou Mindom et al.

Incorporating RL algorithms in software engineering tasks has long been an
active area of research [47], [6], [11].

In the case of SE testing, Wuji [57], is a framework that applies EA, MOO
and DRL to facilitate automatic game testing. EA and MOO are designed to
explore states and DRL ensures the completion of the mission of the game.
Further, the authors use the Block Maze game and two commercial online
games to evaluate Wuji. This work is used as a baseline in this paper. We
compare the DRL part of wuji to state-of-the-art RL algorithms from RL
frameworks. Speciﬁcally, we implement RL algorithms from RL frameworks
to detect bugs on the Block Maze game and assess their performance against
the DRL part of Wuji.

Bagherzadeh et al. [5] leveraged state-of-the-art DRL algorithms from the
Stable-baselines framework in CI regression testing. They investigate point-
wise, pairwise, and listwise ranking models as RL problems to ﬁnd the opti-
mal prioritized test cases. The authors also conducted experiments on eight
datasets and compared their solutions against a small subset of non-standard
RL implementations. Again, we use this work as a baseline and implement RL
algorithms from RL frameworks to rank test cases on a CI environment. As
the authors implement Stable-baselines framework, we leverage 2 others RL
frameworks (Tensorforce and Keras-rl) and compare them to Stable-baselines.
Koroglu et al. [29] proposed QBE, a Q-learning framework to automat-
ically test mobile apps. QBE generates behaviour models and uses them to
train the transition prioritization matrix with two optimization goals: activity
coverage and the number of crashes. Its goal is to improve the code coverage
and the number of detected crashes for Android apps. Bottinger et al. [8] in-
troduced a program fuzzer that uses RL to learn reward seed mutations for
testing software. This technique obtains new inputs that can drive a program
execution towards a predeﬁned goal, e.g., maximizing code coverage. In [26],
RL is leveraged to automatically generate test data from structural coverage.
Particularly, a Double DQN agent is trained in a Search-based Software Test-
ing (SBST) environment to ﬁnd a qualifying solution following the feedback
from the ﬁtness function. Chen et al. [11] proposed RecBi, the ﬁrst compiler
bug approach via structural mutation that uses RL. RecBi uses the A2C al-
gorithm to mutate a given failing test program. Then, it uses that failed test
program to identify the compiler buggy ﬁles. RL have also been used to gen-
erate test cases [2, 12, 41]. Adamo et al. [2], build a DQN based testing tool
that generates test cases for Android applications. The tool is guided by the
code coverage to generate suitable test suites. Reichstaller et al. [41], proposed
a framework to test a Self-Adaptive System (SAS) where the tester is modeled
as a Markov Decision Process (MDP). The MDP is then solved by using both
model free and model based RL algorithms to generate test cases that will
adapt to SAS as they have the ability to take decisions at runtime. Soualhia et
al. [48] leveraged RL algorithms to propose a dynamic and failure-aware frame-
work that adjusts Hadoop’s scheduling decisions based on events occurring in

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

35

a cloud environment. Each of these previous approaches from the literature
either implement an RL algorithm from scratch or use a pre-implemented one
from an RL framework. None of them has evaluated the performance of RL
frameworks on software testing tasks. Moreover, it is not clear as to what mo-
tivates the choice of RL frameworks in the literature, as there are several of
them. In our work, we investigate various state-of-the-art RL algorithms from
popular RL frameworks, to assess RL conﬁgurations on a game and regression
testing environments.

7 Threats to validity

Conclusion validity. Conclusion limitations concern the degree to which
the statistical conclusions about which algorithms/framework perform best
are accurate. We use Welch’s ANOVA and Games-Howell’s post-hoc test as a
statistical test. The signiﬁcance level is set to 0.05 which is standard across
literature [53], [19]. The nondeterministic nature of RL algorithms can threaten
the conclusions made in this work. We address this by collecting results from
10 independent runs in the case of the game testing problem. Regarding the
test case prioritization problem, the results are collected on multiple cycles
(the MATH dataset has 55 cycles which is the least number of cycles among
all datasets).

Internal validity. Regarding the game testing problem, the fact that we
only consider the DRL part of the WUJI framework for comparison with the
DRL strategies we studied, might threaten the validity of this work. Although
we only compare the algorithms on sub-optimal solutions, it is necessary to
make a fair comparison among the DRL algorithms. A potential limitation
is the number of frameworks used and the algorithms chosen among these
frameworks. We have chosen to evaluate some of the available frameworks
and have not evaluated all the algorithms they oﬀer. However, the frameworks
used are among the most popular on GitHub, as well as the algorithms (see
Section 2.3). This ensures a good coverage in terms of the usage of RL in
SE. In the future, we plan to expand our study to cover more algorithms.
The limitations of our hardware may threaten the generality of our evaluation
results because it leads us to make choices about the results presented. As
a matter of fact, in Section 4.2, we often limited ourselves to the ﬁrst 10 CI
cycles when comparing the performance of the RL frameworks. Although this
may seem to be insuﬃcient, we were always able to see statistical signiﬁcant
diﬀerences between the studied frameworks.

Construct validity. A potential threat to validity is related to our eval-
uation metrics, which are standard across the literature. We use these met-
rics to make a fair comparison amongst framework/algorithms under identical
circumstances. We discussed some of their limitations and how they can be
interpreted in Sections 3.2 and 3.3.

External validity. Since our goal is to compare DRL frameworks and
their pre-implemented algorithms for SE testing tasks, a potential limitation

36

Nouwou Mindom et al.

is the choice of the testing tasks for comparing frameworks. We address this
threat by choosing game testing and test case prioritization problems that
are totally diﬀerent in SE testing to achieve enough diversity. While test case
prioritization focuses on optimizing order of test cases, one needs to ﬁnd bugs
in the game testing as early as possible. The results we found on the two studied
problems might mitigate this threat as we consistently found some algorithms
performing similar among the frameworks. For example the A2C algorithm
had good performance whether applying on the game testing problem or test
case prioritization problem.

Reliability validity. To allow other researchers to replicate or build on
our research, we provide a detailed replication package5 including the code
and obtained results.

8 Conclusion

In this paper, we study the application of state-of-the-art pre-implemented
RL algorithms from well-known frameworks on two important software test-
ing tasks: test case prioritization and game testing. We rely on two baselines
studies to apply and evaluate the performance of RL algorithms from several
frameworks (i) in terms of detecting bugs in a game, and (ii) in the context
of a CI environment to rank test cases. Our results show that the same al-
gorithm from diﬀerent RL frameworks can have diﬀerent performance. Each
framework provides hyperparameters unique to its implementation, therefore
depending on the underlying SE tasks, the framework that has most suitable
hyperparameters will lead to better performance. We formulate recommenda-
tions to help SE practitioners to make an informed decision when leveraging
DRL frameworks for the development of SE tasks. In the future, we plan to
expand our study to investigate more RL algorithms/frameworks, and more
SE activities.

9 Conﬂict of interest

The authors declared that they have no conﬂict of interest.

References

1. Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS,
Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving
G, Isard M, Jozefowicz R, Jia Y, Kaiser L, Kudlur M, Levenberg J, Man´e
D, Schuster M, Monga R, Moore S, Murray D, Olah C, Shlens J, Steiner
B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Vi´egas
F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X (2015)

5 https://github.com/npaulinastevia/drl_se

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

37

Tensorﬂow, large-scale machine learning on heterogeneous systems. DOI
10.5281/zenodo.4724125

2. Adamo D, Khan MK, Koppula S, Bryce R (2018) Reinforcement learning
for android gui testing. In: Proceedings of the 9th ACM SIGSOFT In-
ternational Workshop on Automating TEST Case Design, Selection, and
Evaluation, pp 2–8

3. Alshahwan N, Gao X, Harman M, Jia Y, Mao K, Mols A, Tei T, Zorin I
(2018) Deploying search based software engineering with sapienz at face-
book. In: International Symposium on Search Based Software Engineering,
Springer, pp 3–45

4. Arcuri A, Briand L (2014) A hitchhiker’s guide to statistical tests for
assessing randomized algorithms in software engineering. Software Testing,
Veriﬁcation and Reliability 24(3):219–250

5. Bagherzadeh M, Kahani N, Briand L (2021) Reinforcement learning for

test case prioritization. IEEE Transactions on Software Engineering

6. Bahrpeyma F, Haghighi H, Zakerolhosseini A (2015) An adaptive rl based
approach for dynamic resource provisioning in cloud virtualized data cen-
ters. Computing 97(12):1209–1234

7. Bertolino A, Guerriero A, Miranda B, Pietrantuono R, Russo S (2020)
Learning-to-rank vs ranking-to-learn: strategies for regression testing in
continuous integration. In: Proceedings of the ACM/IEEE 42nd Interna-
tional Conference on Software Engineering, pp 1–12

8. B¨ottinger K, Godefroid P, Singh R (2018) Deep reinforcement fuzzing. In:
2018 IEEE Security and Privacy Workshops (SPW), IEEE, pp 116–122
9. Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J,

Zaremba W (2016) Openai gym. arXiv:1606.01540

10. Castro PS, Moitra S, Gelada C, Kumar S, Bellemare MG (2018)
Dopamine: A research framework for deep reinforcement learning. arXiv
preprint arXiv:181206110

11. Chen J, Ma H, Zhang L (2020) Enhanced compiler bug isolation via mem-
oized search. In: Proceedings of the 35th IEEE/ACM International Con-
ference on Automated Software Engineering, pp 78–89

12. Dai H, Li Y, Wang C, Singh R, Huang PS, Kohli P (2019) Learning
transferable graph exploration. Advances in Neural Information Process-
ing Systems 32

13. Dhariwal P, Hesse C, Klimov O, Nichol A, Plappert M, Radford A, Schul-
man J, Sidor S, Wu Y, Zhokhov P (2017) Openai baselines. https:
//github.com/openai/baselines

14. Drozd W, Wagner MD (2018) Fuzzergym: A competitive framework for

fuzzing and learning. arXiv preprint arXiv:180707490

15. Dulac-Arnold G, Mankowitz D, Hester T (2019) Challenges of real-world

reinforcement learning. arXiv preprint arXiv:190412901

16. Fortunato M, Azar MG, Piot B, Menick J, Osband I, Graves A, Mnih
V, Munos R, Hassabis D, Pietquin O, et al. (2017) Noisy networks for
exploration. arXiv preprint arXiv:170610295

38

Nouwou Mindom et al.

17. Fraser G, Arcuri A (2011) Evosuite: automatic test suite generation for
object-oriented software. In: Proceedings of the 19th ACM SIGSOFT sym-
posium and the 13th European conference on Foundations of software en-
gineering, pp 416–419

18. Fujimoto S, Hoof H, Meger D (2018) Addressing function approximation
error in actor-critic methods. In: International Conference on Machine
Learning, PMLR, pp 1587–1596

19. Games PA, Howell JF (1976) Pairwise multiple comparison procedures
with unequal n’s and/or variances: a monte carlo study. Journal of Edu-
cational Statistics 1(2):113–125

20. Gu S, Lillicrap T, Sutskever I, Levine S (2016) Continuous deep q-learning
with model-based acceleration. In: International conference on machine
learning, PMLR, pp 2829–2838

21. Haarnoja T, Zhou A, Abbeel P, Levine S (2018) Soft actor-critic: Oﬀ-policy
maximum entropy deep reinforcement learning with a stochastic actor. In:
International conference on machine learning, PMLR, pp 1861–1870
22. Hamlet R, Maciniak J (1994) Random testing, encyclopedia of software

engineering. Wiley: New York pp 970–978

23. Harman M, Jia Y, Zhang Y (2015) Achievements, open problems and
challenges for search based software testing. In: 2015 IEEE 8th Interna-
tional Conference on Software Testing, Veriﬁcation and Validation (ICST),
IEEE, pp 1–12

24. Hill A, Raﬃn A, Ernestus M, Gleave A, Kanervisto A, Traore R, Dhariwal
P, Hesse C, Klimov O, Nichol A, Plappert M, Radford A, Schulman J,
Sidor S, Wu Y (2018) Stable baselines. https://github.com/hill-a/
stable-baselines

25. Hill A, Raﬃn A, Ernestus M, Gleave A, Kanervisto A, Traore R, Dhariwal
P, Hesse C, Klimov O, Nichol A, et al. (2019) Stable baselines. 2018. URL:
https://github com/hill-a/stable-baselines

26. Kim J, Kwon M, Yoo S (2018) Generating test input with deep rein-
forcement learning. In: 2018 IEEE/ACM 11th International Workshop on
Search-Based Software Testing (SBST), IEEE, pp 51–58

27. Kingma DP, Ba J (2014) Adam: A method for stochastic optimization.

arXiv preprint arXiv:14126980

28. Knuth DE (1997) The art of computer programming, vol 3. Pearson Ed-

ucation

29. Koroglu Y, Sen A, Muslu O, Mete Y, Ulker C, Tanriverdi T, Donmez Y
(2018) Qbe: Qlearning-based exploration of android applications. In: 2018
IEEE 11th International Conference on Software Testing, Veriﬁcation and
Validation (ICST), IEEE, pp 105–115

30. Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Silver D,
Wierstra D (2015) Continuous control with deep reinforcement learning.
arXiv preprint arXiv:150902971

31. Malialis K, Devlin S, Kudenko D (2015) Distributed reinforcement learn-
ing for adaptive and robust network intrusion response. Connection Sci-
ence 27(3):234–252

A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks

39

32. McGraw KO, Wong SP (1992) A common language eﬀect size statistic.

Psychological bulletin 111(2):361

33. Mnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D,
Riedmiller M (2013) Playing atari with deep reinforcement learning. arXiv
preprint arXiv:13125602

34. Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver
D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement
learning. In: International conference on machine learning, PMLR, pp
1928–1937

35. Moghadam MH, Saadatmand M, Borg M, Bohlin M, Lisper B (2021) An
autonomous performance testing framework using self-adaptive fuzzy re-
inforcement learning. Software quality journal pp 1–33

36. Nair V, Hinton GE (2010) Rectiﬁed linear units improve restricted boltz-

mann machines. In: Icml

37. Nguyen NM (2018) Improving model-based rl with adaptive rollout using

uncertainty estimation

38. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen
T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E,
DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai
J, Chintala S (2019) Pytorch: An imperative style, high-performance
deep learning library. In: Wallach H, Larochelle H, Beygelzimer A,
d'Alch´e-Buc F, Fox E, Garnett R (eds) Advances in Neural Information
Processing Systems 32, Curran Associates, Inc., pp 8024–8035, URL
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-
style-high-performance-deep-learning-library.pdf

39. Plappert M (2016) keras-rl. https://github.com/keras-rl/keras-rl
40. Raﬃn A, Hill A, Gleave A, Kanervisto A, Ernestus M, Dormann N (2021)
Stable-baselines3: Reliable reinforcement learning implementations. Jour-
nal of Machine Learning Research 22(268):1–8, URL http://jmlr.org/
papers/v22/20-1364.html

41. Reichstaller A, Knapp A (2018) Risk-based testing of self-adaptive systems
using run-time predictions. In: 2018 IEEE 12th international conference
on self-adaptive and self-organizing systems (SASO), IEEE, pp 80–89
42. Romdhana A, Merlo A, Ceccato M, Tonella P (2022) Deep reinforcement
learning for black-box testing of android apps. ACM Transactions on Soft-
ware Engineering and Methodology

43. Santos RES, Magalh˜aes CVC, Capretz LF, Correia-Neto JS, da Silva
FQB, Saher A (2018) Computer games are serious business and so is
their quality: Particularities of software testing in game development from
the perspective of practitioners. In: Proceedings of the 12th ACM/IEEE
International Symposium on Empirical Software Engineering and Mea-
surement, Association for Computing Machinery, New York, NY, USA,
ESEM ’18, DOI 10.1145/3239235.3268923, URL https://doi.org/10.
1145/3239235.3268923

44. Schaarschmidt M, Kuhnle A, Ellis B, Fricke K, Gessert F, Yoneki E (2018)
Lift: Reinforcement learning in computer systems by learning from demon-

40

Nouwou Mindom et al.

strations. arXiv preprint arXiv:180807903

45. Schulman J, Levine S, Abbeel P, Jordan M, Moritz P (2015) Trust re-
gion policy optimization. In: International conference on machine learning,
PMLR, pp 1889–1897

46. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal

policy optimization algorithms. arXiv preprint arXiv:170706347

47. Singh L, Sharma DK (2013) An architecture for extracting information
from hidden web databases using intelligent agent technology through re-
inforcement learning. In: 2013 IEEE conference on Information & Com-
munication Technologies, IEEE, pp 292–297

48. Soualhia M, Khomh F, Tahar S (2020) A dynamic and failure-aware task
scheduling framework for hadoop. IEEE Transactions on Cloud Comput-
ing 8(2):553–569, DOI 10.1109/TCC.2018.2805812

49. Spieker H, Gotlieb A, Marijan D, Mossige M (2017) Reinforcement learn-
ing for automatic test case prioritization and selection in continuous inte-
gration. In: Proceedings of the 26th ACM SIGSOFT International Sym-
posium on Software Testing and Analysis, pp 12–22

50. Such FP, Madhavan V, Conti E, Lehman J, Stanley KO, Clune J (2017)
Deep neuroevolution: Genetic algorithms are a competitive alternative for
training deep neural networks for reinforcement learning. arXiv preprint
arXiv:171206567

51. Sutton RS, Barto AG, et al. (1998) Introduction to reinforcement learning,

vol 135. MIT press Cambridge

52. Wang Z, Schaul T, Hessel M, Hasselt H, Lanctot M, Freitas N (2016)
Dueling network architectures for deep reinforcement learning. In: Inter-
national conference on machine learning, PMLR, pp 1995–2003

53. Welch BL (1947) The generalization of ‘student’s’problem when several
diﬀerent population varlances are involved. Biometrika 34(1-2):28–35
54. Yang T, Meng Z, Hao J, Zhang C, Zheng Y, Zheng Z (2018) Towards
eﬃcient detection and optimal response against sophisticated opponents.
arXiv preprint arXiv:180904240

55. Yang T, Hao J, Meng Z, Zheng Y, Zhang C, Zheng Z (2019) Bayes-tomop:
A fast detection and best response algorithm towards sophisticated oppo-
nents. In: AAMAS, pp 2282–2284

56. Zhang C, Zhang Y, Shi X, Almpanidis G, Fan G, Shen X (2019) On in-
cremental learning for gradient boosting decision trees. Neural Processing
Letters 50(1):957–987

57. Zheng Y, Xie X, Su T, Ma L, Hao J, Meng Z, Liu Y, Shen R, Chen Y, Fan
C (2019) Wuji: Automatic online combat game testing using evolution-
ary deep reinforcement learning. In: 2019 34th IEEE/ACM International
Conference on Automated Software Engineering (ASE), IEEE, pp 772–784
58. Zhu H, Hall PA, May JH (1997) Software unit test coverage and adequacy.

Acm computing surveys (csur) 29(4):366–427

