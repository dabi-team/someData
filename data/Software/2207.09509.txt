2
2
0
2

l
u
J

9
1

]

O
L
.
s
c
[

1
v
9
0
5
9
0
.
7
0
2
2
:
v
i
X
r
a

TestSelector: Automatic Test Suite Selection for
Student Projects — Extended Version

Filipe Marques1,2, Ant´onio Morgado1,
Jos´e Fragoso Santos1,2, and Mikol´aˇs Janota3

1 INESC-ID, Lisbon, Portugal
2 Instituto Superior T´ecnico, University of Lisbon, Portugal
3 Czech Technical University in Prague, Czechia

Abstract. Computer Science course instructors routinely have to cre-
ate comprehensive test suites to assess programming assignments. The
creation of such test suites is typically not trivial as it involves selecting
a limited number of tests from a set of (semi-)randomly generated ones.
Manual strategies for test selection do not scale when considering large
testing inputs needed, for instance, for the assessment of algorithms exer-
cises. To facilitate this process, we present TestSelector, a new frame-
work for automatic selection of optimal test suites for student projects.
The key advantage of TestSelector over existing approaches is that
it is easily extensible with arbitrarily complex code coverage measures,
not requiring these measures to be encoded into the logic of an exact
constraint solver. We demonstrate the ﬂexibility of TestSelector by
extending it with support for a range of classical code coverage measures
and using it to select test suites for a number of real-world algorithms
projects, further showing that the selected test suites outperform ran-
domly selected ones in ﬁnding bugs in students’ code.

Keywords: Constraint-based test suite selection · runtime monitoring
· code coverage measures

1

Introduction

Computer science course instructors routinely have to create comprehensive test
suites to automatically assess programming assignments. It not uncommon for
these test suites to have to be created before students actually submit their
solutions. This is, for instance, the case when students are allowed to submit their
solutions multiple times with the selected tests being run each time and feedback
given to the student. We further note that in typical algorithms courses, testing
inputs must be large enough to ensure that the students’ solutions have the
required asymptotic complexity. In such scenarios, course instructors typically
resort to semi-random test generation, selecting only a small number of the
generated tests due to the limited computational resources of testing platforms.
Hence, the included tests must be judiciously chosen. Manual strategies for test
selection, however, do not scale for large testing inputs.

 
 
 
 
 
 
2

F. Marques et al.

This paper presents TestSelector, a new framework for optimal test selec-
tion for student projects. With our framework, the instructor provides a canoni-
cal implementation of the project assignment, a set of generated tests T , and the
number n of tests to be selected, and TestSelector determines a subset T (cid:48) ⊆ T
of size n that maximises a given code coverage measure. By maximising coverage
of the canonical solution, TestSelector provides relative assurances that most
of the corner case behaviours of the expected solution are covered by the selected
test suite. Naturally, the better the coverage measure, the better those assur-
ances. Importantly, the best coverage measure is often project-speciﬁc, there
being no silver bullet.

The key advantage of TestSelector over existing approaches [28,11,1,16] is
precisely that it is easily extensible with arbitrarily complex code coverage mea-
sures speciﬁcally designed for the project at hand. Unlike previous approaches
however, TestSelector does not require the targeted coverage measures to be
encoded into the logic of an exact constraint solver. We achieve this by using
as our optimisation algorithm, a specialised version of the recent Seesaw algo-
rithm [13] for exploring the Pareto optimal frontier of a pair of functions. We
demonstrate the ﬂexibility of TestSelector by extending it with support for
a range of classical code coverage measures and using it to select test suites for a
number of real-world algorithms projects, further showing that the selected test
suites outperform randomly selected ones in ﬁnding bugs in students’ code.

The paper is organized as follows. Section 2 overviews the TestSelector
framework presenting each of its modules and how they interact with each other.
Section 3 and Section 4 describe in detail the main modules of TestSelec-
tor. Section 5 presents an experimental evaluation of the framework. Section 6
overviews related work, and Section 7 concludes the paper.

2 TestSelector Overview

We give an overview of our approach for selecting optimal test suites for student
projects. As illustrated in Figure 1, the TestSelector framework receives three
inputs: (1) the instructor’s implementation for the project, which we refer to
as the canonical solution; (2) a JSON conﬁguration ﬁle with a description of
the coverage measure to be used for test selection as well as the number of
tests to be selected; and (3) an initial set of input tests, T . Given these inputs,
TestSelector computes an optimal subset of tests, T (cid:48) ⊆ T , that maximises
the selected coverage measure with a ﬁxed number of tests, n (|T (cid:48)| = n). Due to
the combinatorial nature of the problem and the sheer size of the search space,
it is often the case that TestSelector is not able to ﬁnd the optimal solution
within the given time constraints. In such cases, it returns the best solution found
so far. Our experimental evaluation indicates that this solution is typically not
far from the optimal one.

The TestSelector framework consists of two main building blocks:

– Summary Generation Module: The summary generation module automati-
cally instruments the code of the canonical solution in order for its execution

TestSelector

3

Fig. 1: TestSelector high-level architecture.

to additionally produce a coverage summary of each given input test. Dif-
ferent coverage measures require diﬀerent summaries. For instance, a block
coverage summary (c.f. §2.2) simply includes the identiﬁers of the code blocks
that were executed during the running of the canonical solution.

– MaxTests Module: The MaxTests module receives as input the cover-
age measure to be used, the number n of tests to be selected, and a set of
summaries, and selects the subset of size n of the given summaries that max-
imises the coverage measure. For instance, for the block coverage measure,
MaxTests selects the summaries corresponding to the testing inputs that
maximise the overall number of executed code blocks. Note that if Max-
Tests does not ﬁnd the optimal solution within the speciﬁed time limit, it
simply outputs the best solution found so far.
At the core of MaxTests is an adapted implementation of the Seesaw
algorithm [13], a novel algorithm for exploring the Pareto optimal frontier of
two given functions using the well-known implicit hitting set paradigm [3,4].
The key innovation of Seesaw is that it allows one to treat one of the two
functions to optimise in a black-box manner. In our case, this black-box
function corresponds to the targeted coverage function, meaning that we are
able to select optimal test suites without encoding the targeted coverage
functions into the logic of an exact constraint solver.

2.1 Supporting New Coverage Measures

The key advantage of TestSelector when compared to existing approaches
for constraint-base test suite selection [28,11,26,1,16] is that it is trivial to extend
TestSelector with support for new, arbitrarily complex coverage measures.
In contrast, existing approaches require users to encode the targeted coverage
measures into the logic of an exact constraint solver, typically SMT [5] or In-
teger Linear Programming (ILP) solvers [10]. The manual construction of such
encodings has two main inconveniences when compared to our approach. First,
it requires requires specialist knowledge on the logic and inner workings of the
targeted solver. Note that even simple encodings must be carefully engineered
so that they can be eﬃciently solved. Second, there might be a mismatch be-
tween the expressivity of the existing solvers and the nature of the measure to

TestSelectorSummaryImplementation Summary  Generation Summary APIMaxTestsInstrumentationMeasure APIMeasure Implementation Selected Inputs.inCanonical Sol..cInputs.in(1)(3)(2)Summaries.jsonMeasure conf. .json4

F. Marques et al.

be encoded. In contrast, with TestSelector, if one wants to add support for
a new coverage measure, one simply has to:

1. Implement a Coverage Summary API that dynamically constructs a cover-

age summary during the execution of the canonical solution;

2. Implement a Coverage Evaluation Function that maps a given set of coverage
summaries to a numeric coverage score. Importantly, in order for TestSe-
lector to work properly, the coverage evaluation function must be mono-
tone; meaning that for any two sets of summaries S1 and S2, it must hold
that: S1 ⊆ S2 =⇒ f (S1) ≤ f (S2). Monotonicity is a natural requirement
for coverage scoring functions. Hence, we do not believe that this restriction
constitutes a limitation to the applicability of our framework.

2.2 Natively Supported Coverage Measures

Even though our main goal is to allow for users to easily implement their own
coverage measures, TestSelector comes with built-in support for various stan-
dard code coverage measures. In particular, it implements4:

– Block Coverage (BC) — counts the number of executed code blocks:

fBC(S) = #{i | ∃s ∈ S. s contains an execution of i}

– Array Coverage (AC) — counts the number of programmatic interactions

with distinct array indexes:

fAC(S) = #{(a, i) | ∃s ∈ S. s contains an access to the i-th position of a}

– Loop Coverage (LC) — counts the number of loop executions with a distinct

number of iterations:

fLC(S) = #{(l, i) | ∃s ∈ S. s contains an execution of loop l with i iterations}

– Decision Coverage (DC) — counts the number of conditional guards that

evaluate both to true and to false:

fDC(S) = #

(cid:26) i | ∃s1, s2 ∈ S. s1 evaluates the i-th guard to true

(cid:27)

∧ s2 evaluates the i-th guard to false

– Condition Coverage (CC) — counts the number of conditional guards for

which all subexpressions evaluate both to true and to false:

fCC(S) = #






1, ..., sn, s(cid:48)

i | ∃s1, s(cid:48)
sj evaluates the j-th sub-expr. of the i-th guard to true
s(cid:48)
j evaluates the j-th sub-expr. of the i-th guard to false

n ∈ S. ∀1 ≤ j ≤ n.






We refer the reader to [25] for a detailed account of standard coverage measures
in the software engineering literature.

4 Note that we use #X to refer to the number of elements of X.

TestSelector

5

Linear Combination of Coverage Measures. In addition to the coverage measures
described above, TestSelector allows the user to specify a linear combination
of coverage measures. Observe that, as the linear combination of two monotone
functions is also monotone, the user is free to combine any monotone coverage
measures without compromising the correct behaviour of MaxTests.

3 Summary Generation

This section overviews the Summary Generation Module of TestSelector
which, given a canonical solution and a set of testing inputs, generates the
corresponding set of summaries with the relevant coverage data. The internal
architecture of the module, described in Figure 2, comprises two components:

– the instrumentation component, described in §3.1, which injects into the code
of the canonical solution calls to the coverage summary API before and/or
after each summary-relevant operation;

– the executor component, described in §3.2, which runs the instrumented code
of the canonical solution on the given set of testing inputs using the appro-
priate implementation(s) of the coverage summary API.

Fig. 2: Summary Generation Module: Architecture.

3.1 Program Instrumentation

The main job of the program instrumentation component is to inject calls to
the coverage summary API into the code of the canonical solution. For instance,
given the program:

while ((x--) > 0) { aux = a[0]; a[0] = aux + x; }

The instrumentation component generates the program:

BEGIN("WHILE", 3);
while (GUARD(COND((x--) > 0))) {

BLOCK(2);
aux = a[0]; ARR_READ("main:a", a, 0);
a[0] = aux + x; ARR_WRITE("main:a", a, 0);

}
END("WHILE", 3);

Listing 1: Instrumented program.

Summary GenerationSummary APIExecutorComponent InstrumentationComponent Canonical Sol..cInstrumented Sol. .c*Inputs.in.jsonSummaries6

F. Marques et al.

If
s(cid:48)
i = BLOCK(Id(si)); C (si, Id(si)) |i=1,2
e(cid:48) = Ce(e)

While

s(cid:48) = Id(si); C (s, Id(s))
e(cid:48) = Ce(e)

C (if (e) {s1} else {s2}, i) (cid:44)




BEGIN("IF", i);
if (GUARD(e(cid:48))) {s(cid:48)
END("IF", i);

1} else {s(cid:48)

2}

C (while (e) {s}, i) (cid:44)






BEGIN("WHILE", i);
while (GUARD(e(cid:48))) {s(cid:48)}
END("WHILE", i);

Sequence
C (s1; s2, −) (cid:44) C (s1, Id(s1)) ; C (s2, Id(s2))

Array Write

idx = fresh()

strx = string(x)

C (x [e1] = e2) (cid:44)






int idx = e1;
x [idx] = e2;
ARR WRITE (strx, (void *) x, idx) ;



Malloc

arg = fresh()
C (x = malloc (e)) (cid:44)
int arg = e;
CALL MALLOC(arg);
x = malloc(arg);






Array Read
idx = fresh()

strx = string(x)

τ = Type(e1)

C (x = e1 [e2]) (cid:44)






τ aux = e1; int idx = e2;
x = aux[idx];
ARR READ (strx, (void *) aux, idx) ;

Fig. 3: Instrumentation function C : S × I → S.

The example above showcases the API functions: BEGIN, END, BLOCK, GUARD,
COND, ARR READ, and ARR WRITE. In a nutshell, we inject a call to BEGIN and END
respectively before and after each control-ﬂow statement, providing both the
type of control-ﬂow statement and its unique identiﬁer. We inject a call to BLOCK
at the beginning of each conditional/loop branch. All conditional guards are
wrapped inside a call to the API function GUARD and all Boolean sub-expressions
of a guard are wrapped inside a call to COND. The instrumentation also ensures
that all array-lookup and array-update operations are respectively succeeded by
a call to ARR READ and ARR WRITE, providing the static identiﬁer of the array,
the pointer to the array, and the accessed index. Subsection §3.2 gives a more
comprehensive account of the coverage summary API, while below we discuss
the instrumentation procedure.

Formalisation. We formalise our instrumentation procedure for the fragment
of the C programming language given below. Note that the implementation of
TestSelector supports the entire syntax of C.

s ∈ S ::= if (e) {s1} else {s2} (cid:12)
(cid:12)
(cid:12) x[e1] = e2
(cid:12) (cid:9) e (cid:12)
(cid:12) x (cid:12)

(cid:12) x = e1[e2] (cid:12)
(cid:12)
(cid:12) e1 ⊕ e2

e ∈ E ::= v (cid:12)

(cid:12) while (e) {s} (cid:12)

(cid:12) s1; s2

(cid:12)
(cid:12) x = e

(cid:12) x = malloc(e)

TestSelector

7

Statements s ∈ S include the standard conditional, loop, and sequence state-
ments, variable assignments, and array updates, lookups, and creations. State-
ments are composed of expressions, e ∈ E, which include: constants v ∈ V,
program variables x ∈ X, and unary and binary operators. In the following, we
assume that each statement s is annotated with a unique integer identiﬁer, de-
noted by Id(s). This can be achieved via a simple traversal of the AST of the
program to be instrumented. We formalise the instrumentation as a total compi-
lation function C : S × I → S mapping pairs of statements and integer identiﬁers
to statements and write C(s, i) to denote the instrumentation of the statement
s with identiﬁer i. The compilation rules are given in Figure 3 and are mostly
straightforward. We inject calls to the coverage summary API before and/or
after each summary-relevant operation. For array-manipulating operations, we
have to store the computed index (and pointer) to avoid re-computation and
repetition of side-eﬀects. Finally, our main instrumentation function makes use
of an auxiliary instrumentation function for expressions Ce : E (cid:55)→ E, which sim-
ply wraps all the Boolean sub-expressions of the given expression within a call
to the COND API function. Below, we illustrate the two cases corresponding to
binary operator expressions:

BinOp - COND

isBool(⊕)
Ce(e1 ⊕ e2) = COND(Ce(e1)) ⊕ COND(Ce(e2))

BinOp - Non-COND
¬isBool(⊕)
Ce(e1 ⊕ e2) = Ce(e1) ⊕ Ce(e2)

3.2 Summary Generation API

Figure 4 gives a selection of the functions that form our coverage summary API;
the description of the full API can be found in the appendix. The purpose of
the API is to dynamically compute a coverage summary for each testing input.

BEGIN(char *stmt_t, int id)

Logs the beginning of a control-ﬂow statement with
type stmt t and identiﬁer id.

END(char *stmt_t, int id)

Logs the ending of a control-ﬂow statement with
type stmt t and identiﬁer id.

BLOCK(int id)

GUARD(int expr)

COND(int expr)

Logs the block identiﬁer id.

Logs the value expr of a conditional guard.

Logs the value expr of a boolean sub-expression
of conditional guard.

ARR_WRITE(char *name,

void *ptr,
int i)

Logs an update operation to the index i of the
array pointed to by ptr and associated with
the static identiﬁer name.

ARR_READ(char *name,

void *ptr,
int i)

Logs a lookup operation of the index i of the
array pointed to by ptr and associated with
the static identiﬁer name.

CALL_MALLOC(unsigned size) Logs a call to malloc with argument size.

Fig. 4: Coverage Summary API (selection).

8

F. Marques et al.

Note that diﬀerent coverage measures require diﬀerent summaries and therefore
diﬀerent implementations of the coverage summary API. Instead, we could have
opted to serialise the entire execution trace and then compute the coverage
function directly on the serialised trace. This approach, however, does not scale
to executions with millions of commands required to assess typical algorithms
projects. For this reason, each coverage measure is associated with a particular
implementation of the coverage summary API. For portability, TestSelector
requires coverage summaries to be serialised as JSON documents.

Let us now take a closer look at the implementation of the coverage summary
API. We will speciﬁcally consider the implementation of the API for the block
coverage, array coverage, and loop coverage measures, as the implementations of
the other measures are analogous. We will again use as an example the program:

while ((x--) > 0) { aux = a[0]; a[0] = aux + x; }

whose instrumentation was given in Listing 1, and will assume that it is run with
the testing input x = 4.

Block Coverage. The block coverage summary generated for the example is
{ "2" : true }, simply stating that the code block with identiﬁer 2 was cov-
ered by the testing input x = 4. The API implementation for the block coverage
measure is straightforward: the function BLOCK(int id) has to set the summary
entry corresponding to id to true and all other API functions can be ignored.5

Array Coverage. The array coverage summary generated for the example is:

{ "main_a" : { "read" : [0], "write" : [0] } }

signifying that there was a lookup operation and an update operation at the
index 0 of the array with static identiﬁer a in function main. The API imple-
mentation for the array coverage measure is also straightforward: the functions
ARR READ and ARR WRITE have to add the inspected index to the read and write
lists of the corresponding array.

Loop Coverage. The loop coverage summary generated for the example is:

{ "3" : { "4" : 1 } }

signifying that the loop with identiﬁer 3 had one execution with 4 iterations. The
API implementation for the loop coverage measure is a bit more involved than
the previous ones. We use the BEGIN and GUARD functions to count the number
of consecutive iterations of a given loop and then update the corresponding
summary entry when the corresponding END function is called. To this end, we
have to maintain a stack of counters, each corresponding to an active loop.

5 We provide default implementations to be used when no summary-related behaviour

should be triggered.

TestSelector

9

4 MaxTests: Constraint-based Test Suite Selection

The MaxTests module is responsible for computing the optimal set of testing
inputs according to the chosen measure. This module, whose architecture is
presented in Figure 5, is composed of two main components:

– the Seesaw Component: a specialisation of the Seesaw algorithm [13] for the

test selection problem; and

– the Measure Component: the implementation of the targeted measure func-
tion, which receives as input a set of summaries and generates the corre-
sponding numeric coverage score.

Given a set of testing inputs T , their corresponding summaries S, the number n
of testing inputs to select, and the targeted coverage measure M , MaxTests
determines a subset T ⊆ T of size n that maximises the speciﬁed measure.

Fig. 5: MaxTests Module: Architecture.

As we have seen before, coverage measures are not computed directly on
the testing inputs but rather on their corresponding summaries. Hence, one can
think of a coverage measure, M : 2S → R, as a mathematical function mapping
sets of summaries to their corresponding coverage score. Accordingly, in order
to ﬁnd the optimal set of tests T ⊆ T , MaxTests ﬁrst determines the optimal
subset of summaries S ⊆ S and then outputs their corresponding testing inputs.
If one wants to extend TestSelector with support for a new coverage
measure, one need only provide its corresponding implementation (i.e., the func-
tion M ), with the code of Seesaw not requiring any adaptation. The Seesaw
algorithm requires a back-end ILP solver. Here, we use Gurobi [10]; however,
MaxTests is structured so that it is trivial to replace Gurobi with any other
ILP solver. In the following, we explain our specialisation of Seesaw to the test
selection problem, highlighting domain-speciﬁc choices and design decisions.

Specialised Seesaw Algorithm for Test Selection. The Seesaw algorithm [13]
was designed to explore the Pareto optimal frontier of two functions: (1) a cost
function g, that must be encodable into the logic of an exact solver, such as an
ILP solver (e.g. [10]) or a MaxSAT solver (for e.g. [4]), and (2) an oracle function
f , that is treated in a black-box fashion and only required to be monotone. In
the case of the test selection problem, the oracle function f corresponds to the
coverage measure M , while the cost function g is simply a Boolean function
indicating whether or not the cardinality of the set of selected tests is equal to
the required number of tests. Our goal is to maximise both functions: we want
the maximum coverage with the speciﬁed number of testing inputs.

ILP SolverSeesawSelected Inputs.inMaxTestsMeasure  Implementations Summaries MeasureConfiguration Inputs 10

F. Marques et al.

Algorithm 1: Specialised Seesaw Algorithm.

input
output : subset of S with size n that maximises M

: n, M , S

// M : 2S → R

1 Sbest ← HeuristicM ax(M, S, n)
2 Γ ← ∅
3 while true do
4

S ← argS∈HS(Γ )|S| = n
if S = ⊥ then

// heuristic candidate of size n
// set of collected cores

// find minimal hitting set of size n

return Sbest

if M (S) = M (S) then

return S

if M (S) > M (Sbest) then

// upper bound bound improvement

Sbest ← S

Γ ← Γ ∪ {extractCore(S, M, S)}

// calculate new core

5

6

7

8

9

10

11

Algorithm 1 presents the pseudo-code of Seesaw adapted to the test selec-
tion problem. The algorithm goes through a sequence of sets of size n called
candidates, starting with an initial candidate that is heuristically determined.
At each iteration, the algorithm tries to compute a new candidate with a bet-
ter coverage score. To do so, it enumerates necessary conditions for coverage
improvement, one by one. These conditions take the form of sets of summaries
called cores. In a nutshell, in order to improve the coverage score of the current
best solution, one has to ﬁnd another solution that includes at least one sum-
mary belonging to each computed core. All computed cores are accumulated in
the variable Γ , and the new candidates are chosen to be hitting sets of Γ of size
n,6 meaning that they must include at least one summary belonging to each core
in Γ . If no more candidates exist, that is, S = ⊥ in line 5, then the best candi-
date so far is returned. In line 7, if the coverage score of the current candidate
is equal to the maximum possible coverage (i.e. the coverage of the entire set of
summaries), then the algorithm also outputs the current candidate. In line 9, if
the coverage score of the current candidate improves on the coverage of current
best candidate, then the best candidate Sbest is updated. Finally, in line 11, the
algorithm extends the set of computed cores, Γ , with a new core computed using
the current candidate. Below, we detail this process.

Obtaining New Cores from Candidates. New cores are obtained using the auxil-
iary function extractCore. For a candidate S with a coverage value ν (ν = M (S)),
the computation of core extraction is processed in two steps:

1. Choose a subset-maximal set S(cid:48) such that S ⊆ S(cid:48) and M (S(cid:48)) = ν;
2. Return the core S \ S(cid:48).

6 This step of the algorithm is computed with the help of the underlying ILP solver

(in our case, Gurobi).

TestSelector

11

The intuition is simple: if S(cid:48) is larger than S and does not have a higher cover-
age score; then, in order to improve the coverage score of S, one has to pick a
summary in S \ S(cid:48). Naturally, the smaller the chosen cores, the faster the algo-
rithm converges to an optimal solution. The question is how to eﬃciently ﬁnd the
largest possible S(cid:48) (corresponding to the smallest possible core). Here we consider
two heuristics for core computation: linear search and progression search.

Algorithm 2 presents the pseudo-code of the linear search extractCore func-
tion. We start by ordering all summaries according to their individual coverage
score (in increasing order). Then, we traverse the array of ordered summaries,
checking if the inclusion of each summary that does not belong to the current
candidate maintains the coverage score of the candidate. If it does not, then we
discard that test; otherwise, we add it to the current candidate.

Algorithm 2: Linear Search extractCore

input
output : κ

: S, M , S

// M : 2S → R
// a core

1 Sord ← IncOrder(M, S)
2 S(cid:48) ← S
3 foreach s ∈ Sord and s /∈ S(cid:48) do
4

if M (S(cid:48) ∪ {s}) = M (S(cid:48)) then

// summaries in increasing measure order

5

S(cid:48) ← S(cid:48) ∪ {s}

6 return S \ S(cid:48)

The linear search strategy has one major disadvantage: it requires re-comput-
ing the coverage measure for each summary that does not belong to the current
candidate. The goal of the progression search strategy [12] is precisely to miti-
gate this problem by reducing the number of calls to the measure function. As
summaries are ordered in an array-like fashion (line 1), instead of considering
one summary at a time (as in lines 3 to 4), we consider summaries in exponential
progression, that is, we consider extending the current candidate S(cid:48) ﬁrst with 1
summary, then with 2, then with 4, and so on. If the coverage score of S(cid:48) with
the additional summaries is equal to the coverage score of S, then we add the
summaries to the current candidate S(cid:48); otherwise, we restart the progression
from the last reached point.

5 Evaluation

We evaluate TestSelector with respect to three research questions:

– RQ1: How easy is it to extend TestSelector with new code cover-
age measures? We show that the currently supported coverage measures
are implemented with a small number of lines of code, demonstrating the
practicality of our approach.

12

F. Marques et al.

Project CLoC nproj

TLoC AvgLoC ninpts

P1
P2
P3
P4
P5
P6
P7

256
529
416
208
304
204
108

398 140,349 352.64 1,002
349 176,547 505.86
600
193 26,890 139.32 1,000
166 34,512 207.90 1,000
172 21,114 122.76 1,000
185 24,091 130.22
800
174 24,035 138.13 1,000

Total 2,125 1,637 447,538 273.39 6,402

Fig. 6: Evaluation diagram.

Table 1: Benchmark characterisation.

– RQ2: Do classical code coverage measures improve test suite selec-
tion for bug ﬁnding in student projects? We show that the test suites
selected by TestSelector outperform randomly selected ones in ﬁnding
bugs in students’ code.

– RQ3: Do linear combinations of code coverage measures further
improve test suite selection for bug ﬁnding? We show that by combin-
ing the best code coverage measures, we can ﬁnd more bugs in students’ code.

Experimental Procedure. The experimental procedure is a two-step process, as
illustrated in Figure 6. In the ﬁrst step, TestSelector selects the test suites
for a given canonical solution, set of inputs, and conﬁguration ﬁle specifying the
coverage measures and the size of the computed test suites. This step generates
a set of test suites, each corresponding to one of the speciﬁed measures. In the
second step, an executor will run every student’s project against the selected
test suites. In the end, the executor creates a report detailing the passing/failing
rate for every student’s project on each selected test suite.7

All the experiments were performed on a server with a 12-core Intel Xeon
E5–2620 CPU and 32GB of RAM running Ubuntu 20.04.2 LTS. For the ILP
solver we used the Gurobi Optimizer v9.1.2. For each execution of MaxTests
we set a time limit of 30 minutes.

Benchmarks. We curated a benchmark suite comprising students’ projects from
seven editions of two programming courses organised by the authors. Table 1
presents the benchmark suite characterisation. For each project, we show the
number of lines of code of the canonical solution (CLoC), the number of student
projects (nproj ), the total number of lines of code of the student projects (TLoC),
the average number of lines of code per student project (AvgLoC), and the num-
ber of available input tests (ninpts ). In summary, we tested 1,637 projects, which
totalled 447k lines of code (≈ 273 LoC/project).

7 We consider that a student project fails a test if its output does not coincide with

the output of the canonical solution or if it throws a runtime exception.

TestSelectorSelected Inputs .inExecutorResults for .csvStudent Projects .cStep 1.Step 2.CanonicalSolutionMeasureConfigurationInputsTestSelector

13

5.1 RQ1: TestSelector Extensibility

The table below presents the number of lines of code of the implementation
of each coverage measure: Loop Coverage (LC), Array Coverage (AC), Block
Coverage (BC), Condition Coverage (CC), and Decision Coverage (DC). For
each measure, we give the number of lines of code of both its implementation of
the coverage summary API and evaluation function.

Module

LC AC BC CC DC

Coverage Summary API

90 60 42 120 120
Measure Evaluation Function 54 58 48 74 64

When it comes to the implementation of the coverage summary API, we observe
that the simpler coverage measures, such as LC, AC, and BC require fewer than
100 lines of code to implement and the more complex coverage measures, such
as CC and DC, require 120 lines of code. As expected, the measure evaluation
function is simpler to implement than the coverage summary API, requiring even
fewer lines of code (between 48–74 LoC).

5.2 RQ2: Classical Code Coverage Selection

We investigate the eﬀectiveness of TestSelector when used to select test
suites for ﬁnding bugs in students’ code. In particular, we compare the number
of bugs found by the test suites selected by TestSelector against those found
by test suites obtained through random selection. In all experiments, we ask for
test suites of size 30 out of 900 available randomly generated tests (the number
of tests used to assess the students in the corresponding courses was 30). We
consider the ﬁve coverage measures described in §2.2 and an additional measure
corresponding to the size of the testing input. Furthermore, to determine the
best extractCore search strategy, TestSelector was conﬁgured to run twice:
one time using the linear search (LS) strategy and the other using the progression
search (PS) strategy.

Results. Table 2 presents the results of the experiment. For each project, the
table shows the resulting failure rates for the measures Loop Coverage (LC),
Array Coverage (AC), Block Coverage (BC), Size, Condition Coverage (CC),
and Decision Coverage (DC). We observe that the best measure is project-
dependent, with LC being the best measure in four projects, BC in one, and
Size in two. Importantly, we also observe that the more sophisticated measures,
such as CC and DC, have lower failure rates than simpler measures, such as LC
and AC. This may be explained by the fact that the students’ most common
programming errors are often encoded in loops and array accesses. Additionally,
all coverage measures consistently perform better than the random test suite
selection. Finally, we observe that the progression search strategy yields slightly
better results than the linear search strategy.

14

F. Marques et al.

Project

Search

LC

AC

CC

DC

Rnd

P1

P2

P3

P4

P5

P6

P7

Average

LS
PS

LS
PS

LS
PS

LS
PS

LS
PS

LS
PS

LS
PS

LS
PS

14.67
14.53

18.07
18.07

16.39
16.77

23.68
23.68

3.76
3.76

6.91
6.91

10.46
10.46

13.42
13.45

BC

13.69
13.56

19.47
19.47

7.49
7.95

11.99
11.99

3.56
3.56

8.01
8.01

6.71
6.71

Size

0.20
0.19

6.14
6.14

28.07
28.31

23.59
23.59

3.74
3.74

8.39
8.39

6.39
6.39

13.95
13.82

15.60
15.60

7.49
7.95

17.95
17.93

3.56
3.56

4.72
4.72

7.17
7.17

14.48
14.34

17.15
17.20

20.38
20.70

22.78
22.82

3.23
3.25

8.22
8.22

6.08
6.08

13.19
13.23

10.13
10.18

10.93
10.96

10.06
10.11

14.41
14.27

14.35
14.35

7.49
7.95

17.93
17.93

3.56
3.56

4.68
4.66

7.17
7.17

9.94
9.98

4.81

5.60

7.56

13.52

3.09

6.61

6.28

6.78

Table 2: Results for each measure with linear search (LS) and progression search (PS).

5.3 RQ3: Linear Combinations of Coverage Measures

To investigate whether using linear combinations of code coverage measures can
further improve the bug ﬁnding results, we replay the experiment described in
§5.2 with the following combinations of coverage measures: (1) AC+LC; (2)
BC+LC; (3) AC+BC; and (4) AC+BC+LC.

Results. Figure 7 presents the obtained results for the four linear combinations8
and the ﬁve individual code coverage measures presented in Table 2. For each
measure, we give a blue and a red bar, each corresponding to one of the search
strategies supported by the Seesaw algorithm. It is easy to observe that the
majority of the combinations, i.e., LC+AC, LC+AC+BC, and LC+BC, are able
to ﬁnd more bugs in the students’ code than the overall best-performing single
measure (LC), with only AC+BC obtaining worse results.

6 Related Work

Implicit Hitting Sets for Functions Optimization. In the last decade, implicit hit-
ting sets (IHS) have been used in many problems with great success. MaxHS [3,4]
is a success case of a MaxSAT solver using IHS. The idea stems from the fact that

8 LC+AC, LC+BC, AC+BC, and LC+AC+BC

TestSelector

15

LS
PS
Random

+ B C

+ B C
L C

A C
A C

+

+ B C

L C
A C

A C

Size

B C

C C

C

D

R nd

)

%

(

e
t
a
R
e
r
u
l
i
a
F

15.00

10.00

5.00

0.00

+

L C
L C

Fig. 7: Failure rate (%) for each measure, comparing linear search (LS) with progres-
sion search (PS).

any minimal correction set is a hitting set of all MUSes. Once MUSes are enumer-
ated, then a smallest correction set can be obtained by calculating the minimum
hitting set of them. The number of MUSes may be exponential, as such, MaxHS
enumerates cores (over approximations of MUSes) and tests whether a correction
set is obtained by picking a minimum hitting set of the cores enumerated. Solv-
ing the minimum hitting set problem (MHSP) is as diﬃcult as solving MaxSAT.
However, it has been observed that ILP solvers perform well on MHSP. Moreno-
Centeno and Karp introduce a framework for solving NP-complete problems
based on the IHS approach [19]. Saikko et al. extend this framework and observe
that it is not limited to problems in NP [21]. In both frameworks, the search
considers an oracle predicate. The Seesaw algorithm [13] generalises the above
by considering an oracle function rather than just a predicate. Additionally, [13]
reformulated the precondition of the algorithm by demonstrating that the al-
gorithm is correct as long as the oracle function is anti-monotone.9 Monotone
predicates have been studied extensively in the context of SAT [14].

Test Suit Construction. The software engineering community has dedicated a
considerable eﬀort to the problem of generating eﬀective test suites for com-
plex software systems, exploring topics such as: test suite reduction and test
case selection [2,20,1,29,16,15], combinatorial testing [28,27,26], and a variety of
fuzzying strategies [7,6,9,8,22]. In the following, we focus on the test suite reduc-
tion and test case selection problems, which are immediately close to our own
goal, highlighting constraint-based approaches. Importantly, we are not aware

9 Seesaw considers minimization, thus for maximization the oracle function has to be

monotone.

16

F. Marques et al.

on any works in this ﬁeld speciﬁcally targeted at student projects. The testing of
such projects has, however, its own speciﬁcities when compared to the testing of
large-scale industrial software systems. In particular, the time constraints on the
test generation process are less severe and the code being tested less complex.

The test suite reduction problem [24,2,18,1,29] is the problem of reducing the
size of a given test suite while satisfying a given test criterion. Typical criteria
are the so-called coverage-based criteria, which ensure that the coverage of the
reduced test suite is above a certain minimal threshold. The test case selection
problem [24,2,18,1,29] is the dual problem, in that it tries to determine the mini-
mal number of tests to be added to a given test suite so that a given test criterion
is attained. As most of these algorithms are targeted at the industrial setting,
they assume severe time constraints on the test selection process. Hence, the
vast majority of the proposed approaches for test suite reduction and selection
are based on approximate algorithms, such as similarity-based algorithms [2,18],
which are not guaranteed to ﬁnd the optimal test suite even when given enough
resources. In order to achieve a compromise between precision and scalability,
the authors of [1] proposed a combination of standard ILP encodings and heuris-
tic approaches. Finally, the authors of [16] proposed a SAT-based encoding for
selecting optimal test suites according to the modiﬁed condition decision cov-
erage criterion [25,15]. They argue that, as this criterion is enforced by safety
standards in both the automative and the avionics industries, one is obliged to
resort to exact approaches.

7 Conclusion

We have presented TestSelector, a new framework for the automatic selection
of optimal test suites for student projects. The key innovation of TestSelector
is that it can be extended with support for new code coverage measures without
these measures being encoded into the logic of an exact constraint solver. We
evaluate TestSelector against a benchmark comprised of 1,637 real-world
student projects, demonstrating that: (1) it is trivial to extend TestSelec-
tor with support for new coverage measures and (2) the selected test suites
outperform randomly selected ones in ﬁnding bugs in students’ code.

In the future, we plan to conduct a more thorough investigation on the re-
lation between the characteristics of a project and the code coverage measures
that are appropriate for it. We also plan to integrate TestSelector with an
existing testing platform, such as Mooshak [17] or Pandora [23].

Acknowledgements. The authors were supported by Portuguese national funds through
Funda¸c˜ao para a Ciˆencia e a Tecnologia (UIDB/50021/2020, INESC-ID multi-annual
funding program) and projects INFOCOS (PTDC/CCI-COM/32378/2017) and DIV-
INA (CMU/TIC/0053/2021). The results were also supported by the MEYS within
the dedicated program ERC CZ under the project POSTMAN no. LL1902, and it
is part of the RICAIP project that has received funding from the European Union’s
Horizon 2020 under grant agreement No 857306.

TestSelector

17

References

1. Chen, Z., Zhang, X., Xu, B.: A degraded ILP approach for test suite reduction.
In: Proceedings of the Twentieth International Conference on Software Engineer-
ing & Knowledge Engineering (SEKE). pp. 494–499. Knowledge Systems Institute
Graduate School (2008)

2. Cruciani, E., Miranda, B., Verdecchia, R., Bertolino, A.: Scalable approaches for
test suite reduction. In: Proceedings of the 41st International Conference on Soft-
ware Engineering, ICSE. pp. 419–429. IEEE / ACM (2019)

3. Davies, J., Bacchus, F.: Solving MaxSAT by solving a sequence of simpler SAT

instances. In: Principles and Practice of Constraint Programming (2011)

4. Davies, J., Bacchus, F.: Exploiting the power of MIP solvers in MaxSAT. In: Theory

and Applications of Satisﬁability Testing (2013)

5. De Moura, L., Bjørner, N.: Z3: An Eﬃcient SMT Solver. In: Tools and Algorithms

for the Construction and Analysis of Systems (2008)

6. Godefroid, P.: Compositional dynamic test generation. In: POPL. vol. 42, pp. 47–

54 (2007)

7. Godefroid, P., Klarlund, N., Sen, K.: Dart: Directed automated random testing.

In: ACM Sigplan Notices (2005)

8. Godefroid, P., Levin, M.Y., Molnar, D.A.: Automated whitebox fuzz testing. In:

NDSS (2008)

9. Godefroid, P., Nori, A.V., Rajamani, S.K., Tetali, S.: Compositional may-must

program analysis: Unleashing the power of alternation. In: POPL (2010)

10. Gurobi Optimization, LLC: Gurobi Optimizer Reference Manual (2022), https:

//www.gurobi.com

11. Hnich, B., Prestwich, S.D., Selensky, E., Smith, B.M.: Constraint models for the

covering test problem. Constraints An Int. J. 11(2-3), 199–219 (2006)

12. Ignatiev, A., Morgado, A., Manquinho, V.M., Lynce, I., Marques-Silva, J.: Progres-
sion in Maximum Satisﬁability. In: European Conference on Artiﬁcial Intelligence
(2014)

13. Janota, M., Morgado, A., Fragoso Santos, J., Manquinho, V.: The Seesaw Al-
gorithm: Function Optimization Using Implicit Hitting Sets. In: Principles and
Practice of Constraint Programming (2021)

14. s Janota, M., Marques-Silva, J.: On the query complexity of selecting minimal sets

for monotone predicates. Artiﬁcial Intelligence (2016)

15. Jones, J.A., Harrold, M.J.: Test-suite reduction and prioritization for modiﬁed
condition/decision coverage. IEEE Trans. Software Eng. 29(3), 195–209 (2003)
16. Kitamura, T., Maissonneuve, Q., Choi, E., Artho, C., Gargantini, A.: Optimal test
suite generation for modiﬁed condition decision coverage using SAT solving. In:
Computer Safety, Reliability, and Security - 37th International Conference, SAFE-
COMP. Lecture Notes in Computer Science, vol. 11093, pp. 123–138. Springer
(2018)

17. Leal, J.P., Paiva, J.C., Correia, H.: Mooshak (2022), https://mooshak2.dcc.fc.

up.pt

18. Miranda, B., Cruciani, E., Verdecchia, R., Bertolino, A.: FAST approaches to scal-
able similarity-based test case prioritization. In: Proceedings of the 40th Interna-
tional Conference on Software Engineering, ICSE. pp. 222–232. ACM (2018)
19. Moreno-Centeno, E., Karp, R.M.: The Implicit Hitting Set Approach to Solve Com-
binatorial Optimization Problems with an Application to Multigenome Alignment.
Operations Research (2013)

18

F. Marques et al.

20. Rojas, J.M., Campos, J., Vivanti, M., Fraser, G., Arcuri, A.: Combining multiple
coverage criteria in search-based unit test generation. In: Search-Based Software
Engineering - 7th International Symposium, SSBSE. Lecture Notes in Computer
Science, vol. 9275, pp. 93–108. Springer (2015)

21. Saikko, P., Wallner, J.P., J¨arvisalo, M.: Implicit Hitting Set Algorithms for Rea-
soning Beyond NP. In: Principles of Knowledge Representation and Reasoning
(2016)

22. Sen, K., Agha, G.: Cute and jcute: Concolic unit testing and explicit path model-

checking tools. In: CAV. pp. 419–423 (2006)

23. Serra, P.: Pandora: Automatic Assessment Tool (AAT) (2022), https://saturn.

ulusofona.pt

24. Shi, A., Yung, T., Gyori, A., Marinov, D.: Comparing and combining test-suite
reduction and regression test selection. In: Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering, ESEC/FSE. pp. 237–247. ACM
(2015)

25. Sz˜ugyi, Z., Porkol´ab, Z.: Comparison of DC and MC/DC code coverages. Research

report, Acta Electrotechnica et Informatica (2013)

26. Wu, H., Nie, C., Petke, J., Jia, Y., Harman, M.: A survey of constrained combina-

torial testing. CoRR abs/1908.02480 (2019)

27. Yamada, A., Biere, A., Artho, C., Kitamura, T., Choi, E.: Greedy combinato-
rial test case generation using unsatisﬁable cores. In: Proceedings of the 31st
IEEE/ACM International Conference on Automated Software Engineering, ASE.
pp. 614–624. ACM (2016)

28. Yamada, A., Kitamura, T., Artho, C., Choi, E., Oiwa, Y., Biere, A.: Optimization
of combinatorial testing by incremental SAT solving. In: 8th IEEE International
Conference on Software Testing, Veriﬁcation and Validation, ICST. pp. 1–10. IEEE
Computer Society (2015)

29. Yoo, S., Harman, M.: Pareto eﬃcient multi-objective test case selection. In: Pro-
ceedings of the ACM/SIGSOFT International Symposium on Software Testing and
Analysis, ISSTA. pp. 140–150. ACM (2007)

A Appendix A

TestSelector

19

BEGIN(char *stmt_t, int id)

Logs the beginning of a control-ﬂow statement with
type stmt t and identiﬁer id.

END(char *stmt_t, int id)

Logs the ending of a control-ﬂow statement with
type stmt t and identiﬁer id.

BEGIN_FUN(char *name)

Logs the beginning of the function name.

END_FUN(char *name)

Logs the ending of the function name.

BLOCK(int id)

GUARD(int expr)

COND(int expr)

Logs the block identiﬁer id.

Logs the value expr of a conditional guard.

Logs the value expr of a boolean sub-expression
of conditional guard.

RETURN(char *name)

Logs a return statement in the function name.

BREAK(int id)

CONT(int id)

DEFAULT(int id)

Logs a break statement inside a control-ﬂow construct
with the identiﬁer id.

Logs a continue statement inside a control-ﬂow
construct with the identiﬁer id.

Traces a default statement inside a control-ﬂow
construct with the identiﬁer id.

ARR_DECL(char *name)

Logs the static allocation of an array with the static
identiﬁer name.

ARR_WRITE(char *name,

void *ptr,
int i)

Logs an update operation to the index i of the
array pointed to by ptr and associated with
the static identiﬁer name.

ARR_READ(char *name,

void *ptr,
int i)

Logs a lookup operation of the index i of the
array pointed to by ptr and associated with
the static identiﬁer name.

STRUCT_REF(char *name,
void *base,
void *field)

Logs a pointer dereference operation to the memory
region base with the oﬀset field, and associated
with the static identiﬁer name.

CALL_MALLOC(unsigned size) Logs a call to malloc with argument size.

Fig. 8: Complete Coverage Summary API.

