S¯adhan¯a Vol. , No., , pp.1–
DOI

© Indian Academy of Sciences

2
2
0
2

r
p
A
9
1

]
E
S
.
s
c
[

1
v
5
9
6
8
0
.
4
0
2
2
:
v
i
X
r
a

Automated Application Processing

ESHITA SHARMA, KESHAV GUPTA, LUBAINA MACHINEWALA, SAMAKSH DHIN-
GRA, SHREY TRIPATHI, SHREYAS V S and SUJIT KUMAR CHAKRABARTI

International Institute of Information Technology, Bangalore

Abstract. Recruitment in large organisations often involves interviewing a large number of candidates. The
process is resource intensive and complex. Therefore, it is important to carry it out eﬃciently and eﬀectively.
Planning the selection process consists of several problems, each of which maps to one or the other well-known
computing problem. Research that looks at each of these problems in isolation is rich and mature. However,
research that takes an integrated view of the problem is not common.
In this paper, we take two of the most
important aspects of the application processing problem, namely review/interview panel creation and interview
scheduling. We have implemented our approach as a prototype system and have used it to automatically plan the
interview process of a real-life data set. Our system provides a distinctly better plan than the existing practice,
which is predominantly manual. We have explored various algorithmic options and have customised them to solve
these panel creation and interview scheduling problems. We have evaluated these design options experimentally
on a real data set and have presented our observations. Our prototype and experimental process and results may be
a very good starting point for a full-ﬂedged development project for automating application processing process.

Keywords. Algorithms, meta heuristics, application processing, graph colouring, assignment

1 Introduction

For large organisations, eﬀective talent management critical
for organisational success. Not surprisingly, organisations
invest heavily to ensure that the best talents are hired. It is im-
portant to carry this process smoothly/eﬃciently, otherwise,
the hiring machinery would not only be a collosal waste of
resources, but would lead to sub-optimal talent acquisition in
the organisation, negatively impacting the organisation’s pro-
ductivity and performance. Recruitment drives diﬀer from
each other depending on the recruiting organisation, its size,
and mode of selection (online / face-to-face, written / inter-
view / both, single-step / multi-step etc.). Hence, while talk-
ing about improving the eﬃciency and eﬀectiveness of the
talent hiring process, it is diﬃcult to talk about talent hiring
in general.

In this paper, we address a speciﬁc form of hiring: large
recruitment drives involving testing/interviewing hundreds or
thousands of candidates. This is a common scenario in case
of large IT ﬁrms. Often, requirements stream in correspond-
ing to both technology horizontals (Java, C, web program-
ming, database administrator etc.) and business verticals (in-
surance, banking, aerospace, healthcare etc.). Shortlisting
happens from tens of thousands of applications, and the short-
listed candidates are examined through possibly multiple rounds
involving written tests and/or interviews.

Such recruitment drives cost a lot to organisations be-
cause of the infrastructural resources and because a large
number of subject matter experts (SMEs) have to take time
oﬀ from their regular work to man the selection panels. Given
this, such recruitment drives should be done eﬃciently. Eﬃ-
ciency of such recruitment drives can be measured in many

ways, e.g. as the ratio of resource expenditure to the number
of reviews/interviews happening, or the number of successful
recruitments.

Eﬀectiveness of selection processes is a less tangible, but
more important, parameter. Ideally, eﬀectiveness can only be
measured by tracking the contributions of the selected can-
didates through longitudinal studies. Such an ideal approach
is hard to implement in practice. However, we believe that
one of the elements that would signiﬁcantly inﬂuence the ef-
fectiveness of any selection process is the extent to which
the competence of a candidate is matched with those of the
reviewers/interviewers who examine him/her. The reason be-
hind this is that if a candidate with certain purported skills is
reviewed by an SME in relevant areas, the review is likely to
be deeper and more eﬀective. Contrary to this, if the skills
of a candidate and the reviewer are ill-matched, there will be
mistakes made both ways: bad candidates may get through
while the good ones do not get a fair chance.

In this paper, we investigate two important sub-problems
of a recruitment drive: review/interview panel creation and
interview scheduling, clubbed together as application pro-
cessing problem. In panel creation 1 the goal would be to
maximise the eﬃcacy of the recruitment process by consti-
tuting panels such that the skill match between candidates
and examiners is maximised. Interview scheduling aims to
ensure that interviews are conducted as much in parallel as
possible without causing conﬂicts. This leads to a prompt
completion of the process thus saving time for everyone in-
volved.

As regards the current state-of-practice (which is pre-

1review/interview panel creation

1

 
 
 
 
 
 
2

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

dominantly manual) the cost considerations in application
processing emerge from the following sources:

1. Panel creation and scheduling are very costly and te-

dious processes.

2. The results of manually doing application processing
lead to suboptimal utilisation of resources both in panel
creation and in interview scheduling.

Our approach consists of coming up with multiple algo-
rithms for solving panel creation and interview scheduling
problems automatically and experimentally comparing their
performance based on eﬃciency and eﬃcacy metrics which
we introduce in this paper. We wish to situate our work in the
context of recruitment drives conducted by large corporate
organisations. Due to the paucity of data from such sources,
our experiments have been carried out in the setting of ap-
plication processing in an academic institution. Although
this setup is much smaller than that of corporate recruitment
drives and may diﬀer in some details, it has all the essential
properties of application processing as applied to corporate
recruitment drives. Our experiments show strong evidences
that automating the application processing process not only
relieves its executors of the tedium, it also produces signiﬁ-
cantly more eﬀective panels and eﬃcient schedules than pos-
sible through manual process. The beneﬁts of automation are
expected to get more prominent as the scale increases. Our
experiments, however, do not point towards one clear winner
among the many algorithmic options.

Our work makes the following speciﬁc contributions:

1. mathematical formulation of panel creation and inter-

view scheduling problem

2. algorithms for automatically solving the above two prob-

lems

3. metrics for estimation of eﬃciency and eﬃcacy of ap-

plication processing

4. experimental evaluation of various alternatives.

Both the individual problems map to well-known prob-
lems in computing. To these, a lot of algorithmic solutions
have been proposed in research literature. However, this pa-
per is the ﬁrst work as per our knowledge, which situates
these problems in the application processing context. In that
sense, this research is not a contribution to the ﬁeld of algo-
rithms. Instead, the value of our work is in identifying the
algorithmic aspects of a speciﬁc real-world problem (appli-
cation processing) and providing rigorous experimental eval-
uation of available solution alternatives. Most of these algo-
rithms involve contextualisation of existing algorithms to the
application processing problem. The ﬁndings of this work
may be directly useful to solution providers in this space.

The rest of this paper is organised as follows: we present
the application processing problem in detail in section 2. In
section 3, we present in detail our scheme for automating

the two aspects of application processing, namely panel cre-
ation and interview scheduling. In section 4, we present an
overview of our experimental setup, the criteria by which the
performance of the proposed algorithms is measured and the
experimental results. We discuss related work in section 5
and conclude the paper in section 6.

2 Application Processing Problem

2.1 Illustrative Setup

We introduce the application in the context of an academic
university/institution. Consider an academic institute with
around 50-60 faculty members working in 6-7 research do-
mains (e.g. computer science, software engineering, net-
working and communication etc.). The institution has a stu-
dent population of about 1000, out of which about 100 are
research students. Anywhere between 500-1000 online ap-
plications come in for research programmes (Ph.D. and M.S.
by research) every admission cycle, out of which about 10-
15% are made oﬀers. A subset of all the applications are
shortlisted through a review of applications. Final selection
happens through an interview between the shortlisted candi-
date and a panel of faculty members. Please note that this
example is of an institution of a small size compared to most
institutions of higher learning in India.

Each application is reviewed, and each short-listed candi-
date interviewed, by a panel of 3-4 faculty members. These
faculty members are chosen based on the overlap between
the candidate’s research interests and the faculty’s. These
panels (both for application review and interview) are cre-
ated manually by an oﬃcial who tries to make an informed
guess to this eﬀect. Note that ﬁnding matching research in-
terests manually is far from straight forward. In these days of
multi-disciplinary research, areas hardly ever settle into neat
hierarchies. For example, a person interested in machine-
learning (domain: data-science) may want to apply them to
governance (domain: IT and society). This makes manual
sorting of applications complex and error-prone. The rami-
ﬁcations of such errors are serious: research candidates with
atypical research interests may end up getting reviewed by
faculty members who are not able to judge their interests
with a holistic perspective. In turn, this aﬀects the quality
of review (both at shortlisting and interview level) leading to
errors in both directions: undeserving candidates may get in
while deserving ones are missed.

Assume that panels are created with exclusive attention
paid to overlap in research interests. This leads to another
diﬃcult issue. Panelists may end up in an arbitrary combi-
nation of panels. Pairs of panels with at least one panelist
Interview
in common are said to conﬂict with each other.
schedule creators are under pressure to schedule interviews
as much in parallel as possible so that the timespan of the
entire interview process can be as small as possible. How-
ever, no two panels running in parallel must conﬂict. With
hundreds of interviews to be conducted, detecting conﬂicts
between panels is a very diﬃcult task.

t1, t2

t3

t1, t2

p1

p2

t1, t2, t4

p3

t3

c1

c2

c3

c4

Automated Application Processing

sum of the values of its individual panels.

V(P) =

(cid:88)

P∈P

V(P)

3

(1)

The goal of panel creation is to ﬁnd a P such that V(P) is

the maximised.

2.3 Interview Scheduling

2.3.1 Conﬂicting Panels
Consider a set of interview panels P that have been computed
by solving the problem presented in Section 2.2 using one
of the algorithms presented in Section 3.1. We say that two
panels Pi, P j ∈ P conﬂict with each other if there is at least
one panelist common to them.

con f lict(Pi, P j) = Pi.panel ∩ P j.panel (cid:44) φ

(2)

Example 2 Let P = {p1, p2, p3, p4, p5, p6, p7, p8, p9} be the
set of panelists. Let us assume we have generated six panels
using one of the panel creation algorithms, and each panel
consists of one candidate and a set of panelists. Here, the
panels generated are: P1 = (c1, {p1, p2, p3}), P2 = (c2, {p3, p4, p5}),
P3 = (c3, {p5, p6, p7}), P4 = (c4, {p1, p2, p6}), P5 = (c5, {p7, p8})
and P6 = (c6, {p9, p10}). Here, we can see that P1 and P2
conﬂict (i.e. con f lict(P1, P2) = true) because their panels
have a common panelist p3. Similarly, there are other con-
ﬂicts pairs of panel, e.g. P1 and P4 etc. (cid:4)

Figure 1. Example: Panelists p1, p2, p3, Candidates c1, c2,
c3, c4, Topics t1, t2, t3, t4

It turns out that both the above problems – panel cre-
ation and interview scheduling – map to well-known com-
puting problems. In the remainder of this section, we present
In sec-
a mathematical formulation of both the problems.
tion 3, we present our approach to automatically solve these
problems, both using algorithms designed by us, and using
variants of existing algorithms.

2.2 Panel Creation

2.3.2 Valid Schedule

Consider a set P of panelists, a set C of candidates and a
set T of topics of interest. We deﬁne an assignment G as a
bipartite graph. The left column of G – given by panelists(G)
– consists of nodes, each of which corresponds to one of the
panelists, and the right column – given by candidates(G) –
consists of nodes, each of which corresponds to one of the
candidates. An edge ei j will run between a panelist pi and
candidate c j if there is at least one topic of interest common
between pi and c j. ei j will be annotated with the set of topics
which are common between pi and c j.

An interview is scheduled for each candidate. Assuming that
panel creation step has been completed, each candidate has a
interview panel associated with it. A schedule S maps each
interview to a time-slot or interval, i.e. S : P → interval. An
interval i is a pair (s, e) where i.s is starting time and i.e is
the ending time of i. Two intervals (or time slots) i1 and i2
are said to overlap if either starts before the other ends, i.e.
overlap(i1, i2) = (i2.e > i1.s > i2.s) ∨ (i1.e > i2.s > i1.s). A
schedule is valid or feasible if no two conﬂicting panels are
mapped to overlapping slots, i.e.

Example 1 Consider the example shown in Figure 1. We
have three panelists p1, p2, p3, four candidates c1, c2, c3,
c4. Each edge between a panelist and candidate is annotated
with one or more topics. Here, the complete set of topics is
{t1, t2, t3, t4}. Edges are e1 : (p1, c1, {t1, t2}), e2 : (p2, c1, {t3}),
e3 : (p2, c2, {t1, t2}), e4 : (p2, c3, {t1, t2, t4}), e5 : (p3, c4, {t3}).
(cid:4)

A panel P is a pair (candidate, panel) where candidate
is a candidate, and panel : 2P is a set of panelists. A paneling
P : C (cid:55)→ 2P is a map from one candidate in C to a subset from
P. If, for a panel P, P.candidate ∈ dom(P) and P.panel =
P(P.candidate), then we say, P ∈ P.

We deﬁne the value V of a panel P as a function – cur-
rently not speciﬁed – that maps it to a real value representing
the quality of a panel. The cummulative value V of P is the

valid(S ) =

∀Pi, P j ∈ P, con f lict(Pi, P j) =⇒

¬overlap(S (Pi), S (P j))

(3)

2.3.3 Graph Colouring
Interference graph. We use the data of every panel P ∈ P to
create an interference graph I, where each node ni represents
a panel Pi, and an edge ei j exists between two such nodes
ni and n j if con f lict(Pi, P j) = true. The weight wi j of the
edges signify the number of common panelists.

Since an edge between two nodes indicates a panelist
who is common to both the panels, these two panels cannot
be scheduled simultaneously in a valid schedule. This means
that if we colour a node ni with a speciﬁc colour Ci, which

4

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

c1

2

c2

1

c3

1

1

c4

c6

1

c5

Figure 2. Example: Candidates c1 to c6

denotes a particular time interval in which the interview is to
be scheduled for the panel denoted by ni, we need to ensure
that no two adjacent nodes have the same colour. This leads
us to classifying the interview scheduling process as graph
colouring problem, and we make use of one of the three al-
gorithms mentioned in the subsequent sections to colour the
graph accordingly.

Example 3 Consider the interference graph shown in Fig-
ure 2. The edge e12 with weight w12 = 1 denotes that there is
one common panelist between the panels denoted by c1 and
c2; edge e14 with weight w14 = 2 denotes that there are two
common panelists between the panels denoted by c1 and c4,
and so on for edges e23, e34, and e35.

The goal of the interview scheduling problem is to colour
the graph in k colours, such that k is minimized. The mini-
mum value of k will be determined by one of the graph colour-
ing algorithms. (cid:4)

3 Our Approach

3.1 Panel Creation

3.1.1 Edge sorting approach

The edge sorting approach is presented in Algorithm 1.
Panelists occupy the left column of G, given by panelists(G),
abbreviated as P; the candidates occupy the right column of
G, given by candidates(G) abbreviated as C. The idea is
to try and ensure that a panelist with larger overlap with the
interests of a candidate should preferably be one of the pan-
elists for that candidate. We elaborate this idea as follows:

1. More exclusive matches between a panelist and candi-
date should be given more preference as compared to
generic matches. This means that of two panelists p1
and p2 who are connected to a candidate c through the
same set of topics, if p1 has fewer other matches with
other candidates as compared to p2 (probably due to
p1’s area of expertise being specialised/rarer than that
of p2), then the match between p1 and c should be pre-
ferred over that between p2 and c. This policy applies
the other way too. That is, of two candidates c1 and
c2 who are connected to a panelist p through the same
set of topics, if c1 has fewer other matches with other
panelists as compared to c2, then the match between c1
and p should be preferred over that between c2 and p.

2. More exclusive topics contribute more to the edge weight
as compared to more generic topics. Suppose a candi-
date c is connected to one panelist p1 through a topic t1
and to another panelist p2 through a topic t2. Assume
that the scores of both the panelists are same. Which
panelist should get higher preference? This is deter-
mined on the basis of how exclusive or generic t1 and
t2 are. Suppose that the pair (p1, c) is the only edge
which is annotated by t1 (probably because t1 is a very
specialised topic in the given organisation, e.g. public
policy and IT), while t2 (may be a popular topic like
machine learning or cybersecurity) sits on many other
edges in the graph apart from (p2, c). Then p1 would
get more preference than p2 as a panelist for c.

The above policies are mathematically summarised in the

following equation:

Algorithm 1 Panel creation – Edge sorting approach

S c(p) is deﬁned in equation (4)
Load(p): keeps track of load (number of candidates allot-
ted) for each panelist

S c(p) =





1
S t

(cid:80)
t∈T (p,c)
S p
− 1

1+S p

T (p, c) (cid:44) φ
T (p, c) = φ

(4)

function generatePanels(G)

P, C ← panelists(G), candidates(G)
Load(p) ← initialized to 0 for each panelist
for all c ∈ C do

E ← set of edges connected to candidate c
(cid:46) S: Panel size
n ← S
m ← L
(cid:46) L: Maximum load of a panelist
P[c] ← set of n members of Ec with maximum

S c(p) and Load(p) < m

Load(p) ← increment by 1 for each p in P[c]

return P

In the above, S c(p) denotes the overall match score for
panelist p w.r.t. a candidate c. T (p, c) gives the set of topics
common to p and c, i.e. T (p, c) = T (p) ∩ T (c) where T (p)
and T (c) are the sets of topics of interest of panelist p and
candidate c respectively. Panelist score S p, candidate score
S c and topic score S t are the number of edges panelist p,
candidate c and topic t appear on respectively in the bipar-
tite graph G. The intuition behind the above equation are as
follows:

• The term in the numerator (cid:80)

t∈T (p,c)

1
S t

is the sum of re-

ciprocals of scores of all topics involved between p and
c. This summarises the intuition that with an increase

t1, t2

p1

c1

t
1,

t
2,t

3,t

t1

4

c2

t1
p2

Example 4

Automated Application Processing

5

1. For candidate c1, we have two options for panelists
- {p1, p2} with Load(p) < 1 (as load is 0 for every
panelist initially).

2. Out of these, we will assign 1 panelist to c1 with maxi-

mum score, i.e. p1. Therefore, P[c1] = {p1}.

3. Now we increment the load of p1 by 1. Hence, Load(p1) =

Figure 3. Example: Edge Sorting Algorithm

1.

in the genericness of a topic t (quantiﬁed by its score
S t) its contribution to the strength of the connection
between a panelist and candidate should reduce. How-
ever, with more topics associating two individuals, the
strength should go up. Hence, the summation.

• The score of a panelist w.r.t. a candidate would be
inversely proportional to the genericness of a panelist
(quantiﬁed by his/her score S p). Hence, it appears as
the denominator of the expression.

• A panelist may have to be included in the panel of a
candidate even though they may share no common in-
terest in case the number of panelists who share com-
mon interests with the candidate is not enough to form
In that case, we assign the score − 1
,
the panel.
1+S p
which will help us to assign the more generic panelist
and at the same time panelist which have some com-
mon topics of interest with the candidate still get pri-
ority over the panelist who does not.

• If a panelist shares no common interest with the candi-
date, there is also a possibility where that panelist does
not share common interest with any candidate. S p for
that panelist would be 0. An oﬀset of 1 is included in
denominator to handle this case.

The panel for a candidate c, P(c) is computed by selecting
the set of edges whose scores are the highest ensuring that
|P| = S and Load(p) < L where S is the size of the panel and
L is the maximum allowed load for a panelist.

Panelist weights: W(p1) = 2, W(p2) = 2.
Topic weights: W(t1) = 4, W(t2) = 2, W(t3) = 1, W(t4) =

1.

Taking S = 1 and L = 1
Edge weights:

• W(e1) = S c1 (p1) =

1
W(t1 )

+ 1

W(t2 )

W(p1)

= 1

4

+ 1
2

2

= 0.375

• W(e2) = S c1 (p2) =

1
W(t1 )
W(p2)

= 1
4
2

= 0.125

• W(e3) = S c2 (p1) =

1.375

1
W(t1 )

+ 1

W(t2 )

+ 1

W(t3)

+ 1

W(t4 )

W(p1)

= 1

4

+ 1
2

+ 1
1

+ 1
1

2

=

• W(e4) = S c2 (p2) =
We compute panels for each candidate using Edge Sort-

= 0.125

1
W(t1 )
W(p2)

= 1
4
2

ing Algorithm by following steps:

4. For candidate c2, we have only one option for panelist

- {p2} with Load(p) < 1.

5. So, we assign this panelist to c1. Therefore P[c2] =

{p2}.

Total score of assignment becomes 0.375 + 0.125 = 0.5. (cid:4)

3.1.2 Min-Cost-Max-Flow Approach

Algorithm 2 Panel creation – Min Cost Max Flow Approach

function MinCostMaxFlow(G(cid:48), s,t)
for all Edges E(u, v) ∈ G(cid:48) do

Add edge, e directed from v to u
Capacity[v][u] ← 0
Weight[u][v] ← −Weight[u][v]
Weight[v][u] ← −Weight[u][v]
Flow[u][v] ← Flow[v][u] ← 0

while ∃ path from s to t in G(cid:48) do
p ← Shortest Path from s to t
f ← min(Capacity(∀ Edges ∈ path p) )
for all Edges E(u, v) ∈ path p do

Capacity[u][v] ← Capacity[u][v] − f
Capacity[v][u] ← Capacity[v][u] + f
Flow[u][v] ← Flow[u][v] + f

return Flow

function generatePanels(G)

G(cid:48) ← augmentGraph(G) as follows:
FlowMap ← MinCostMaxFlow(G(cid:48),s,t)
P, C ← panelists(G), candidates(G)
for all c ∈ C do

for all p ∈ P do

if FlowMap[p][c] - FlowMap[c][p] = 1 then

P[c] ← add panelist p

return P

function augmentGraph(G)

Create a graph G(cid:48) isomorphic to G except the follow-

ing:

1. Node s (Source Node) with edges directed from s to
Panelist Nodes with edge capacity L and weights 0.
2. Node t (Sink Node) with edges directed from Can-
didates Nodes to t with edge capacity S and weights
0.

return G(cid:48)

6

P1

...

P2

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

C1

...

C2

⇒

s

P1

...

P2

C1

...

C2

t

0.375

p1

1.
3

7

5

0.125

0.125

p2

c1

c2

Figure 4. Graph augmentation for Min Cost Max Flow
algorithm

Figure 5. Example: Min-Cost-Max-Flow Algorithm

The Minimum Cost Maximum Flow approach is presented

in Algorithm 2.

Input problem instance is modeled as a bipartite graph G
in exactly same way as it is done in edge sorting approach.
The weights of the edges are also deﬁned as before.

In this approach, the concept of Network Flows [1] is
used to solve the given problem. We mapped the given prob-
lem to a well known problem in this ﬁeld, Minimum Cost
Maximum Flow Problem [2]. Various algorithms have been
derived to solve this problem optimally. We will use Min-
Cost-Max-Flow Algorithm [2] as part of our solution to given
problem.

In bipartite graph G, for every candidate in candidates(G),

a panelist from panelists(G) has to be assigned such that:

• every candidate is assigned a panel whose size is S;

• each panelist is assigned at most L number of candi-

dates

• Overall cost of assignment is maximized.

For mapping given problem to Minimum Cost Maximum
Flow Problem, given complete bipartite graph G of original
problem is augmented to a graph G(cid:48) as given in Augment-
Graph(G) (Algorithm 2. Min-Cost-Max-Flow Algorithm is
then run on this augmented graph. The desired solution is
extracted from the optimal ﬂow we get. We elaborate this
idea as follows:

1. Since each panelist can be assigned to at most L num-
ber of candidates, we can think of panelists having L
tickets each that they can give to candidates. That is
why edge capacity of L has been kept from source
node to panelist node.

2. Each panelist can give at most 1 ticket to a candidate.
Giving no ticket or 0 ticket would mean that the pan-
elist is not assigned to the given candidate and giving
1 ticket would mean that the candidate is assigned to
the panelist. That is why edge capacity of 1 has been
kept from panelist node to candidate node.

3. Now, each candidate has to be assigned S panelists.
This can be inferred as each candidate receiving S tick-
ets. Since each panelist can give at most one ticket to

a candidate, receiving S tickets by a candidate would
mean getting S diﬀerent panelists. That is why edge
capacity of S has been kept from candidate node to
sink node.

4. Now, suppose that there are Number Of Panelists × L
tickets present at source node. In the case of maximum
ﬂow, each panelist would be getting at most L tickets
from source node, given the capacity of source to pan-
elist edges.

5. These tickets would be further allotted to the candi-
dates by the respective panelists. Assuming there are
suﬃcient number of panelists and value of L, in case of
maximum ﬂow, all the edges from candidates to sink
node would have ﬂow equal to their capacity. Hence
all candidates will always have exactly S number of
panelists assigned.

6. The cost assigned to edges originating from source node
and edges ending at sink node is 0, because they have
no role to play in ﬁnal cost of assignment we want.

7. As we also have to ﬁnd the optimal cost of assignment,
we have to get maximum ﬂow with the optimal cost. In
our case, the cost has to be maximized. However there
is no direct algorithm that can ﬁnd maximum ﬂow in
a graph with maximum cost. Hence, we change the
signs of our original weights (S c(p) to −S c(p)) so that
when algorithm minimizes cost with negative weights,
the original cost with positive weights get maximized.

8. Finally, when we get optimal ﬂow after performing the
algorithm, we can assign the panelist to those candi-
dates whose net ﬂow is 1.

Note: Having the edge capacity 0 would not allow the
ﬂow from that edge, hence it is equivalent to saying that the
path does not exist from that edge. Since the capacity of edge
is reduced by ﬂow value ( f ) each time ﬂow is augmented
along the shortest path, there will be some point when the
path would not exist from source to sink node.

Example 5 Let us consider the same example from the last
section. The annotations over edges represents the weights
assigned as derived before. We compute panels for each
candidate using Min-Cost-Max-Flow Algorithm by following
steps (shown pictorially in Figure 6):

Automated Application Processing

7

Figure 6. Min Cost Max Flow Example

8

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

1. For representation purposes, we will be annotating the
edges with w representing the weight of the edge and c
representing the capacity of edge. If the edge is having
ﬂow of 1 unit, it is highlighted with green colour.

2. Firstly, the given graph is augmented. (Step 1 in ﬁgure)

3. Min Cost Max Flow algorithm is applied on this graph.
Min Cost Flow algorithm ﬁrst does some preprocess-
ing on the graph on which it is applied.
(Step 2 in
ﬁgure)

4. Then we ﬁnd the shortest path from source node to
sink node in above graph. Highlighted part with red
colour in resulting graph represents the only shortest
path possible. (Step 3 in ﬁgure)

5. We get the following residual graph after augmenting

the ﬂow along this path. (Step 4 in ﬁgure)

6. Again, we ﬁnd the shortest path from source node to
sink node in updated graph. Highlighted part with red
colour in resulting graph represents the only shortest
path possible. (Step 5 in ﬁgure)

7. We get the following residual graph after augmenting

the ﬂow along this path.(Step 6 in ﬁgure)

it is trivial to notice that if we would have started Edge sort-
ing Algorithm from candidate 2 (c2), we would have got total
cost of assignment = 1.5 which is same as latter approach.

Although, following the greedy approach seems easier to
understand and implement, we felt a need to come up with a
deterministic optimal approach for this problem. This ap-
proach is a bit less intuitive to understand and implement
than the previous approach but it guarantees overall optimal
solution always.

3.2 Interview Scheduling

In this subsection, we discuss three approaches to solve the
interview scheduling problem. As mentioned earlier (see
Section 2.3), we model this problem as the graph colouring
problem. All three algorithms presented here really solve
graph colouring problem to create the interview schedule.

3.2.1 Approach 1 – Chaitin’s algorithm

Algorithm 3 Interview scheduling – Chaitin’s algorithm

function chaitin(G, k)
Stack S ← φ
while G is not empty do

if ∃ node n in G such that degree(n) ≤ k then

Remove n from G along with all edges con-

8. We can see that there is no path possible from source
node to sink node in the updated graph. Hence we exit
the while loop.

nected to n.

push(n, S )

else

9. Now, when we ﬁnd panelist to candidate assignments
in original graph for getting the panels created with
help of above network ﬂow, we get ﬁnal assignment
shown highlighted with blue colour. (Step 7 in ﬁgure)

10. Hence, P[c1] = {p2} and P[c2] = {p1}.

Total score of assignment becomes 1.375 + 0.125 = 1.5.

(cid:4)

3.1.3 Proof of Optimality

It has been proven that the given Min-Cost-Max-Flow Al-
gorithm gives an optimal solution for Minimum Cost Max-
imum Flow Problem in Network Flows. As we successfully
mapped this problem to given problem, we can say that we
get an optimal solution for the given problem.

3.1.4 Comparison with Edge Sorting Approach

In the Edge Sorting algorithm, we can intuitively observe that
a greedy approach is followed. For every candidate, we are
choosing some best panelists based on the score between the
candidate and panelist. This is somehow locally optimizing
the problem and does not guarantee an overall optimized so-
lution. To support this statement, we can also see the results
of running both algorithms on same example. When we ran
Edge Sorting Algorithm, the overall cost of assignment we
got was 0.125, whereas, for same example, Min-Cost-Max-
Flow approach gave total cost of assignment = 1.5. However,

Fail or restart with a larger value of k.

while S is not empty do

n ← pop(S )
Restore n to G along with the connected edges.
C[n] ← c, the least colour which not the colour of

any of n(cid:48)s current neighbours.

return C

The ﬁrst approach, Chaitin’s algorithm is shown in Algo-
rithm 3. We will consider the graph shown in Figure 2. In
this approach, all the panels are represented as nodes in the
graph G, and each panel will have one candidate and one or
more panelists.

Our graph I can be represented as a dictionary as I: {c1 :
[c2, c4], c2 : [c1, c3], c3 : [c2,c4,c5], c4 : [c1, c3], c5 : [c3], c6 :
[]}. Here, each candidate is mapped to other candidates who
share a common panelist with the candidate. The various
steps which the graph colouring algorithm goes through are
listed below:

1. The ﬁrst step is to ﬁnd a natural number k such that we
can push the nodes with a degree less than k onto the
stack. We make use of binary search to ﬁnd the value
of k.

2. Chaitin’s algorithm is then run for that particular value
of k that was obtained in the ﬁrst iteration (this value
would be the mid point of the range). Now the nodes

Automated Application Processing

9

with a degree less than k are pushed onto the stack S ,
and the node along with its edges will be removed from
G.
If it reaches a point where G does not have any
nodes with degree ≤ k, we break the Chaitin’s algo-
rithm and run it again with a larger value of k we get
from the next iteration of our binary search.

3. The above process is repeated until G is empty and all
the nodes are pushed onto the stack. Then we pop all
the nodes that were stored in S , and restore them in
G along with its edges. While restoring the node onto
the graph, we also give it a parameter called colour,
which is taken from the k of the Chaitin’s algorithm,
the range of colour varying from 0 to k − 1. The node
that is popped is assigned the least value of colour,
provided the value of its colour is not shared by any of
its adjacent nodes.

4. At this step all the nodes are popped from S , and are
placed back on the graph, and have also been assigned
a colour. The minimum number of colours required to
colour the graph would be the last valid value of k.

5. Regarding our scheduling problem, k acts as the min-
imum number of slots required to conduct all the in-
terviews. And since each node represents a panel, all
the nodes which were coloured with the same value
of colour correspond to interviews that can be held si-
multaneously without any conﬂict with the other inter-
views corresponding to the same colour. For Figure 2,
we can compute its graph colouring by following the
steps above, and the minimum number of colours re-
quired to colour it would be 2. And the graph colour-
ing would be : c1 (cid:55)→ 0, c2 (cid:55)→ 1, c3 (cid:55)→ 0, c4 (cid:55)→ 1,
c5 (cid:55)→ 1,c6 (cid:55)→ 0.

3.2.2 Approach 2 – Genetic algorithm

Algorithm 4 Interview scheduling – Genetic algorithm
List chromosomes ← random colourized graphs
function genetic(G, k)

for i = 0 to noO f Generations do

p1, p2 ← two parents from chromosomes
child ← crossover(p1, p2)
child ← mutate(child)
if colour conﬂicts of child < p1 or p2 then

push(child, chromosomes)
pop(more con f licting parent)

return child, child.con f licts

function compare(G, k)

if colour conﬂicts of child is zero then

genetic(G, k + 1)

else

genetic(G, k − 1)

The second algorithm, Genetic-Chaitin approach is shown
in Algorithm 4. Let us consider the graph shown in Figure 2

as an example again. In this approach, we combine Chaitin’s
and Genetic algorithm. Since Genetic algorithm needs a base
of minimum colours on which it tries to improve, we use the
minimum number of colours obtained from Chaitin’s algo-
rithm and set the Genetic algorithm to work on it to decrease
the minimum number of colours required to colour the graph.
Let us consider each chromosome as a dictionary with
each key denoting a candidate whose value denotes a num-
ber corresponding to the colour that the candidate’s node has
been coloured with. This colour is generated at random ini-
tially for all the nodes.

The ﬁrst step is to select two parents from the pool of
chromosomes, and choose a crossover point in both which
results in a child inheriting some colouring from the ﬁrst par-
ent as well as the second parent. The child is then mutated to
achieve a better colouring, and then the two ﬁttest among the
parents and the child are chosen, the remaining chromosome
is discarded from the population.

Taking the example of the graph shown in Figure 2, we
initially consider chromosomes as a collection of randomly
coloured graphs (50, as an example), where each graph is a
copy of our original graph in Figure 2. For each generation
(which is an iteration of the entire process), we perform the
following steps:

1. We select any two parents from the chromosomes us-

ing a linear combination of two methods:

• The ﬁrst method selects the two parents randomly

from the given population

• The second method selects the two ﬁttest parents
from the entire population (i.e the two parents
having the least conﬂict between them)

2. After selecting the parents, we create a child by crossing-
over the two parents using a random crosspoint. The
ﬁrst part of the child (until the crosspoint) comes from
the ﬁrst parent, and the second part (after the cross-
point) comes from the second parent. This process is
called crossover.

3. After the child has been created, we mutate the child
(colouring the graph), albeit in a sub-optimal way. To
mutate the child, we sort the nodes of the graph in de-
scending order of their degrees, and then use either of
the following two methods to colour the graph:

• The ﬁrst method colours the adjacent nodes of all
the nodes with a valid colour, where a valid colour
is any colour that can be used to colour a node
while preserving the graph colouring property.
• The second method colours the adjacent nodes of

all the nodes with a random colour.

After trying both of the above functions on a collec-
tion of data sets, it is observed that the ﬁrst function
outperformed the second in most the cases.

10

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

4. After the child has been mutated, we replace the more
conﬂicting (less ﬁt) parent from the chromosome list
with the child if and only if the conﬂicts in the child
are less in number than the conﬂicts in both its parents.

5. The above process (steps 1-3) is repeated for all the
generations, after which we get a ﬁttest child from the
entire population. If the conﬂicts of the child is zero, it
implies that the graph is coloured validly. And if there
are more generations left after ﬁnding an optimal child,
the algorithm tries to look for a much more ﬁtter child
(less number of colours required). Larger the number
of generations, more accurate is the minimum number
of colours required to colour the graph.[3]

3.2.3 Approach 3 – Ant colony optimisation

Algorithm 5 Interview scheduling – Ant colony optimisation

function ant colony optimization(G)
while G is not k-colourable do
P ← Pheromone Matrix
while iterations do
generate ants()
for ant in ants do

Place ant on a random vertex initially
Colour the node
Choose next node to travel to using the Pi j

and the DS AT values of the nodes

Assign a valid colour to the next node

end for
pheromone decay()
Update P with the pheromone trail of elite ant

Algorithm 5 shows the Ant Colony Optimization approach.

This is another metaheuristic approach used for solving hard
combinatorial optimization problems. It mimics the forag-
ing behaviour of ants, and uses it to solve the graph colour-
ing problem. The ants can indirectly communicate with each
other through a parameter called the pheromone matrix P.
The multitude of ants work independently but in parallel to
achieve the construction graph (The ﬁnal solution where all
the vertices are coloured). The ants traverse through their
graphs and leave behind a pheromone trail. The pheromone
trail of the best performing ant is considered, which will be
reﬂected in P.

We deﬁne the pheromone matrix Pi j as:

Pi j =



0 if an edge exists between node i and node j

1 otherwise

(5)

7

2

3

1

6

4

5

Figure 7. Example graph: Panelists p1, p2, p3, Candidates
c1, c2, c3, c4

1. alpha: relative importance given to pheromones

2. beta: relative importance given to the heuristic value

(DSAT)

3. the node where the ant is set to be initialized, and a list

of visited and unvisited nodes

4. a dictionary to keep track of the colouring of the graph

Each of these ants work on their own graphs, and try to

colour it.

We consider an example (consider Figure 7) to under-
stand how an ant colours its graph, and updates the pheromone
matrix appropriately. We can take the minimum number of
colours required from the Chaitin’s Algorithm.

We initialize the ant on node 3 (randomly chosen). The
ant makes use of the adjacency matrix to check for its neigh-
bours’ colours, and if any of them are coloured, it makes
a note of it by making it taboo (Tabu search) [4]. Once it
makes note of all the taboo colours, which in our case would
be none as the graph has not been coloured yet, we check for
the available colours, and assign it the lowest possible colour
value (0 in this case). So, after a single iteration, our graph
would look like:

{1: None, 2: None, 3: 0, 4: None, 5: None, 6: None, 7:
None}

After the initial colouring, the ant has to decide where
it should move. This decision is aided by Pi j and the DSAT
value. The ant now iterates through its list of unvisited nodes,
and calculates the heuristic values for all of the nodes. The
heuristic value ν is deﬁned as

ν = Pα

i j × dsatβ

(7)

and an adjacency matrix

Gi j =



1 if an edge exists between node i and node j

0 otherwise

(6)

The ﬁrst step is to create the colony of ants, where each

ant is an object with certain parameters:

The Pi j value is taken from the pheromone matrix cor-
responding to the node the ant is residing in and the node
it would visit. The DSAT value (based on the desaturation
value of all nodes in a graph [5]) is deﬁned as the number of
diﬀerent colours among the neighbours of the node. α and β
here control whether we give more importance to Pheromone
matrix value or the DSAT value. After we experimented with

Automated Application Processing

11

a range of values for α and β, it came out that prioritizing the
DSAT value gives us better results, as it reduces the random-
ness of the pheromone matrix in the initial stages. So a higher
β over α would be preferred.

The DSAT values for the unvisited nodes comes out as

{1: 1, 2: 2, 4: 1, 5: 1, 6: 2, 7: 1}

(Note that None is also counted as a colour). The P value for
the unvisited nodes comes out as

{1: 1, 2: 0, 4: 1, 5: 1, 6: 0, 7: 1}

And the heuristic values according to the formula comes out
as

{1: 1, 2: 0, 4: 1, 5: 1, 6: 0, 7: 1}

The maximum heuristic value is taken as the ant’s next des-
tination, and in our case it could be any of 1, 4, 5 and 7. We
assume that the ant moves to 1. Here, we repeat the process
of ﬁnding taboo colours for this particular node and colour it
with the least valued colour, which would again be 0. After
this iteration the graph would look like

{1: 0, 2: None, 3: 0, 4: None, 5: None, 6: None, 7: None}

This above process is repeated for all the unvisited nodes
by the ant. After it visits all the nodes, the graph colouring
would look like

{1: 0, 2: 1, 3: 0, 4: 0, 5: 1, 6: 1, 7: 0}

Similarly, all the other ants work on their respective graphs
and colour them. After all the ants ﬁnish colouring their re-
spective graphs, an evaluation of which ant performed the
best would be conducted, the factor being the number of
colours used to create the construction graph. The ant which
performed the best is known as the elite ant, and the pheromone
matrix is updated based on the path the ant took to colour it,
indicating to the next iteration of ants to follow it’s trail to get
to achieve an optimal solution. A pheromone decay is per-
formed before the elite ant’s path is updated in P to even out
the repetitiveness of a particular construction graph, which
would be caused by increasing certain pheromone values (the
path taken by elite ant).

This entire process is performed for a certain number of
iterations. If the dataset is small (around 100 nodes), 2 − 3
iterations would be suﬃcient to achieve an optimal solution.

4 Experiments

4.1 Setup

4.1.1 Implementation

We have implemented our interview panel creation algorithm
and interview scheduling algorithm as part of a research ap-
plication processing (RAP) system2.

2https://github.com/sujitkc/rap

4.1.2 Data set

We have applied our method on two real data sets corre-
sponding to research admissions in our institute correspond-
ing to two diﬀerent admission cycles. These two data sets are
referred to as Data set 1 and Data set 2 in our results. Man-
ual panel creation data for data set 1 was unavailable. For
operational reasons, we could not obtain the (manual) panel
creation and interview scheduling data for data set 1. Both
these data were available for data set 2.

4.1.3 Methodology.

We have used our algorithm to generate interview panels au-
tomatically alongside the manual process which is currently
followed in our institute. We have compared the performance
of our algorithm against the manual approach against mul-
tiple measures. The measures used for comparison are ex-
plained in sections 4.3 and 4.4. The research questions we
try to answer through our experiments are as follows:

1. RQ1. How does automated panel creation compare
with manual panel creation in terms of the panel qual-
ity?

2. RQ2. How do the two approaches of automated panel
creation – edge ordering and max ﬂow – compare with
each other in terms of panel quality?

3. RQ3. How does automated scheduling compare with
manual interview scheduling in terms of schedule qual-
ity?

4. RQ4. How do the various approaches of automated
interview scheduling – Chaitin, genetic algorithm and
ant colony optimisation – compare with each other in
terms of the schedule quality?

5. RQ5. Is there a diﬀerence in the schedule quality gen-
erated using the panels generated using the manual pro-
cess and automated process?

RQ1 and RQ2 refer to the panel generation step. RQ3 and
RQ4 are about the interview scheduling step. RQ5 is about
the inﬂuence of panel generation method on the eﬃcacy of
interview scheduling.

4.2 Review/Interview Panel Generation

4.2.1 Manual Generation
It is diﬃcult to precisely measure the eﬀort involved in man-
ual generation of interview panels. However, here is our best
eﬀort estimate: Manual process of panel generation requires
a several professors to sift through all of the applications.
Each application is a dossier of several documents like tran-
scripts, statement of purpose, CV and certiﬁcates. On an av-
erage each application counts up to about 35 pages. A very
cursory glance would take at least 15 minutes. So, for nearly
500 applications (a typical number in our institute currently),
just the sifting process takes approximate 7500 minutes or
125 hours, which is approximately 16 person days of work.

12

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

4.2.2 Automated Generation

In comparison, the algorithm generates the panels nearly in-
stantaneously. The real cost of this approach is in generat-
ing the input data which the algorithm uses. Each candidate
has to ﬁll a survey comprising of a multiple choice question
asking them to click on the options corresponding to their re-
search interests. Each candidate would typically spend about
10 minutes to do this. So, 500 candidates take about 5000
minutes (approximately 2500 minutes for 500 candidates).
The time thus saved may seem modest. But this is very likely
deceptive. Note that:

from the candidate’s point of view. For a candidate c, the
panel quality can be calculated as:

(8)

C(c) = WI
Qe
where WI is the sum of the edge weights connecting the
candidate with the panelists. Superscript e = {R, I} where
R stands for review and I stands for interview. That is, we
compute the quality of panels during both the review process
and the interview process. For example QR
c is the panel qual-
ity during the review process. The average candidate’s panel
quality is:

1. The task of responding to the multiple-choice question
is cognitively far less taxing than mapping research in-
terests of a candidate to a group of professors.

2. The data thus generated is likely to be of much better
quality, as the candidate will be able to speak of her
research interests accurately. Contrary to this, an over-
worked professor will often superimpose her biases
and ignorance in mapping an application to a group
of faculty members by simply glancing through an ap-
plication.

3. Since this is akin to a crowdsourced activity, the cost

gets distributed to willing agents.

4. The research interests of individuals involved is cur-
rently found out through a survey. However, several
documents submitted as a part of the online application
are a rich source of information about the academic in-
terests of the candidates. This information can be ex-
tracted using natural language processing [6]. Once
automated, this component of manual eﬀort also goes
away making our method completely automated. We
are currently working on developing this module for
our RAP system.

To summarise, in our estimate, the algorithm takes far
less time in eﬀect compared to the manual method of panel
generation. Further, the data generated through the survey
is of a higher quality than what a professor can manage by
glancing through an application; and this fact reﬂects in the
improved quality of the panels as discussed in the next sub-
section.

4.3 Panel Quality

We measure the quality of the panels by how well the panel
use the time of the professors and the candidates. Having
panelists who are not experts in the area of interest of the
candidate interview/review the candidate/application aﬀects
the quality of the panel/review. Likewise, from a panelist’s
point of view, being part of a review panel for an irrelevant
application is a waste of time and counts as negative.

4.3.1 Candidate’s Panel Quality (Qe
C)

Thus the sum of weights of the edges connecting a candidate
with her interviewer is a good measure of the panel quality

(cid:80)

c∈Ce Qe
|Ce|
where Ce is the set of candidates being reviewed or inter-

C(c)

¯Qe
C

(9)

=

viewed.

4.3.2 Panelist’s Panel Quality (Qe
P)

An interviewer’s time gets used well when she interviews
a candidate when there is a large overlap between her top-
ics of interest and the candidates’. This gives us the sec-
ond measure of panel quality: interviewer’s Panel Quality.
For an panelist p, panel quality is the ratio between the edge
weights connecting her to the candidates she has interviewed
(Ce
p) and the number of candidates she has interviewed.
(cid:80)

Qe

P(p) =

c∈Ce

p W(p, c)
|Ce
p|

The average score of panelist’s panel quality is:

¯Qe
P

=

(cid:80)

P(i)

i∈P Qe
|P|

(10)

(11)

where P is the set of all panelists.q1
We have computed the panel quality (Qe

P) of both
the manually generated panels and automatically generated
panels. Our observations for two datasets (Dataset 1 and
Dataset 2) each corresponding to the admission cycles of two
distinct academic terms has been shown in Table 1.

C and Qe

We also calculated the time wastage from the panelists’
perspective, by counting the number of panelists who were
in panels where they could not contribute meaningfully as
there was no overlap between the academic interests of the
panelist with the corresponding candidates. Our observations
in this regard are tabulated in Table 2. As can be seen, in the
manually generated panels, panelists end up wasting more
time than in automatically generated panels.

4.4 Schedule Quality

Schedule quality is determined as a function of elapse-time
of interviews: The longer it takes to complete the interviews,
the worse is the quality of the schedule. It is assumed that
schedules created are feasible (i.e. conﬂicting interviews must
not be scheduled in parallel sessions). For our research appli-
cation data, we have measured the elapse time of the sched-
ules generated by all our heuristic algorithms and compared

Automated Application Processing

13

Panel Quality Approach

Dataset1

QR
C

QI
C

QR
P

QI
P

0.0051055
0.0868847

1.4320453
2.4218030

Manual
Edge sorting
Max ﬂow
Manual
Edge sorting
Max ﬂow
Manual
Edge sorting
Max ﬂow
Manual
Edge sorting
Max ﬂow

Dataset2
-0.1072503
0.0134299
0.0973861
-0.0367124
0.0766731
0.2709359
-1.7691448
1.4811494
1.4279264
-0.4768062
4.2805435
4.2067129

Panel Generation

Interview Scheduling Dataset2

Manual

Manual
Chaitin
GE
ACO

(a)

43
29
29
29

Panel Generation

Interview Scheduling Dataset1 Dataset2

Automated (Edge Sorting)

Automated (Max Flow)

Manual
Chaitin
GE
ACO
Manual
Chaitin
GE
ACO

(b)

×
41
41
41
×
41
41
41

×
16
16
16
×
17
17
17

Table 1. Panel Quality

them with the schedule generated manually and with one an-
other. Elapse time is measured as the total number of time
slots (each potentially running multiple panels) required to
complete all the interviews.

4.5 Experiment Workﬂow

For addressing RQ1 and RQ2, we used the manual, edge or-
dering and max ﬂow methods to generate panels for Data set
2. As mentioned above, the manual panel creation and in-
terview scheduling data for data set 1 was not available. To
address RQ2 and RQ3, we measured the quality of the inter-
view schedules generated manually for Data set 2 and com-
pared this with the quality of the interview schedules gener-
ated by all the three automated interview schedule generation
approaches. The input to the automated interview scheduling
algorithms were the panels as generated from the edge order-
ing approach. To answer RQ4, we compared the results of
automated approaches of interview scheduling for both the
data sets. Finally, to address RQ5, we fed the outputs of
the manually generated panels and automatically generated
panels (by edge ordering approach) to ant colony optimisa-
tion approach and compared the interview schedule qualities
generated.

4.6 Observations

We compare the performance of our algorithms against the
manual approach against multiple measures. To answer the
questions raised in Section 4:

Approach
Manual
Edge sorting
Max ﬂow

Review Panels
211
7
10

Interview Panels
36
5
5

Table 2. Time wastage

Table 3. Interview Schedule Quality:
(a) for manually
generated panels from dataset 2 (we could not obtain the
panel creation data for data set 1); (b) for automatically
generated panels from dataset 1 and dataset 2.

1. RQ1: From the results of the experiment, we can see
that unsurprisingly, automated panel creation outper-
formed the manual panel creation in terms of panel
quality from both candidate’s perspective as well as
panelist’s perspective. The time wastage also signiﬁ-
cantly reduced in automated panel creation.

One of the major reasons behind this is that, when
panels are created manually, there is scope for an im-
balanced panel. The factors like popularity of a topic
among the candidate applications, complete knowledge
about faculty’s proﬁle, etc. are often missed at time of
manual panel creation, which causes the imbalance.

2. RQ2: As we saw earlier, edge sorting approach is a non
deterministic approach where as max ﬂow approach
is a deterministic approach and always guarantees an
optimal solution. From the experimental results, we
can observe that in terms of panel quality from candi-
date’s perspective (which is almost same as the total
cost of assignment), max ﬂow approach outperformed
the edge sorting approach. However, in terms of panel
quality from panelist’s perspective and time wastage,
results are nearly similar for both approaches.

3. RQ3: We determine the schedule quality by the num-
ber of slots computed for a particular dataset. Manual
scheduling is often done by a designated group whose
main aim is to not cause any faculty conﬂicts appear-
ing in multiple interviews. So, naturally they are not
optimized to ﬁt in a minimum number of slots, and the
slots are scattered over a longer time. Table 3 further
reinforces the optimization of the scheduling process,
as we see a drastic change in the number of slots from
43 to 16.

4. RQ4: We observe that the number of slots generated

14

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

remained the same across the three scheduling algo-
rithms that we used. The uniformity in the number of
slots across the three scheduling algorithms for a par-
ticular set of generated panels can be attributed to the
small size of the data sets that we used. The sole dif-
ferentiating factor between the algorithms was the run
time, where Algorithm 3 outperformed Algorithm 4
and Algorithm 5 signiﬁcantly (≈ 10 times). However,
it should be noted that if the number of iterations of
Algorithm 5 and number of generations in Algorithm
4 is low, it might lead to ineﬃcient results.

5. RQ5: It can be seen from Table 3 that the results of
automatically generating interview schedules from au-
tomatically generated panels is markedly superior (17
in Table 3(b)) than that when the panels are created
manually (29 in Table 3(a)).

5 Related Work

5.1 Panel Creation

The problem of application processing and automating it has
also been done previously by some researchers in University
of Michigan in their research work [7]. They have discussed
this problem in context of the research applications that are
received by U.S. universities in large number for admissions.
Speciﬁcally, they worked on the step of matching research
applicants with the faculty in universities using the concept
of natural language processing.

Similar work has also been done in IBM India Research
Lab [8]. They designed a system called “PROSPECT” to
screen candidates for recruitment. This system helps in au-
tomating the process of candidate-screening by automating
the decision making.

These works provide good solution for screening the ap-
plicants and ﬁnding common areas of interests of these ap-
plicants with the potential advisors which is the initial phase
of application processing.

Given the large number of applications, it is not possi-
ble for a person to review or interview all applications even
with common areas of interest. There is need to divide the
work which is why panels are created to perform this task ef-
ﬁciently. However, we did not ﬁnd any work that speciﬁcally
solves this part of application processing.

Hence, we felt that this process of creating the panels,
which is equally important step in application processing,
also requires automation.

5.2 Interview Scheduling

We have tackled the problem of interview scheduling by re-
ducing it to the graph colouring problem (GCP). There have
been several attempts by researchers to solve the GCP eﬃ-
ciently. Given that GCP is NP-complete [9], we have made

use of three algorithms (Chaitin’s algorithm [10], genetic al-
gorithm [3], ant colony optimization [11]) to eﬃciently cre-
ate a schedule with zero conﬂicts. Chaitin’s algorithm ex-
tends the traditional register allocation problem into graph
colouring [12]. Previous research on schedule evaluation has
also been done using various other meta-heuristics, one of
which is the simulated annealing method [13]. More re-
search has been done on diﬀerent meta-heuristics for graph
colouring (chaotic ant swarm [14], simulated anealing [15],
quantum heuristics [16], dynamic graph colouring [17], and
a time-eﬃcient demon algorithm [18]).

Researchers have been working on the scheduling prob-
lem for quite a while, as this paper [19] written in 1979 show-
cases an algorithm known as RLF which works on solving
large scheduling problems. However, the algorithm exhibits
higher order time complexity(O(n3)) for certain cases when
the graphs have a high edge density. And the 1981 paper
[20] by Mehta introduces the application of graph colouring
to exam/course scheduling, where they talk about how eco-
nomical and easy it is to equate the scheduling problem to
graph colouring, and also to schedule courses parallelly.

Further research in the ﬁeld of graph colouring for schedul-

ing led to several papers such as [21] which uses a greedy
approach to colour the graph to solve their problem of course
scheduling and minimize the number of conﬂicts. [22] ap-
plies a meta-heuristic approach known as vertex colouring to
solve their class scheduling problem. But the constraint in
these papers as well as the others on exam/course scheduling
is that they have a ﬁxed number of slots to work with, so they
will have to make do with personnel/time conﬂicts. There
has not been much progress made in the interview schedul-
ing domain, where the number of slots are not ﬁxed, but zero
conﬂicts in personnel is given a high priority which we have
explored in this paper. With online interviews attaining more
mainstream acceptance, the idea of a limited number of avail-
able rooms also loses relevance opening doors towards more
aggressive parallel scheduling of interviews.

Standalone research has been done on the ant colony op-
timization method to model insects and other natural phe-
nomena [23]. There has also been further research done on
using a modiﬁed variant of the ant colony optimization meta-
heuristic for graph colouring (modiﬁed ant colony system for
colouring graphs [24]).

6 Conclusion

Application processing is an important and resource-intensive
problem that needs to be solved by a wide variety of organ-
isations. The current followed in practice is predominantly
manual. This leads to high expense and low eﬃciency and
eﬀectiveness. In this paper, we have built a case in favour
of automating application processing. We have presented a
mathematical model of the application processing problem.
This comprises of two steps: panel creation (which maps
to the assignment problem) and interview scheduling (which
maps to the graph colouring problem). We presented two
algorithms for solving panel creation: edge sorting and min-

Automated Application Processing

15

imum capacity maximum ﬂow algorithm. We have presented
three algorithms for solving interview scheduling: Chaitin’s
algorithm, genetic algorithm and ant colony optimisation. Of
these ﬁve algorithms, edge sorting is designed by us, while
all the others are adapted from well known algorithms of the
same name. We have evaluated the eﬀectiveness of our ap-
proach by applying it to real data sets. For our measurement,
we have deﬁned metrics like panel quality and schedule qual-
ity. Our experiments clearly show that automation leads to
signiﬁcant saving of time for the selectors, and increases the
eﬀectiveness and eﬃciency of application processing. We
position our work not as an algorithms paper, but as one pre-
senting a systematic approach towards solving an industrial
problem using mathematical modelling, discovery of new al-
gorithms or adaptation of existing ones to solve the problem
and experimental evaluation of solution approaches based on
novel problem speciﬁc metrics.

In the current stage, the assignment graph is prepared
through an online data collection through participation of
selectors and candidates wherein they provide information
about their areas of research. We believe that this infor-
mation can be automatically extracted: in case of selectors
– from the organisational data; and in case of candidates –
from the wealth of documents submitted by them as a part
of their application. Our future work will focus on preparing
the assignment graph automatically using NLP and knowl-
edge representation techniques.

References

[1] Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Or-
lin. Network Flows: Theory, Algorithms, and Applications.
Prentice-Hall, Inc., USA, 1993.

[2] Zvi Galil and ´Eva Tardos. An o (n2 (m+ n log n) log n) min-
cost ﬂow algorithm. Journal of the ACM (JACM), 35(2):374–
386, 1988.

Information and Knowledge Management, CIKM 2010,
Toronto, Ontario, Canada, October 26-30, 2010, pages
659–668, 2010.

[9] Fabrice Rastello. Florent Bouchez, Alain Darte. Register allo-
cation : what does chaitin’s npcompleteness proof really prove
? Laboratoire de l’informatique du parall´elisme., pages 2–12,
2006.

[10] Gregory J Chaitin. Register allocation & spilling via graph

coloring. ACM Sigplan Notices, 17(6):98–101, 1982.

[11] Karthikeyan R., Dr. T. Geetha, Thamaraiselvan .C, and Sushil
Kumar. Ant colony system for graph coloring problem. In-
ternational Journal of Engineering Science and Computing,
7(7):14120–14125, July 2017.

[12] Shengning Wu and Sikun Li. Extending traditional graph-
coloring register allocation exploiting meta-heuristics for em-
bedded systems. In Third International Conference on Natural
Computation (ICNC 2007), volume 4, pages 324–329, 2007.

[13] Alex Cave, Saeid Nahavandi, and Abbas Kouzani. Sched-
ule evaluation: Simulation optimization for process schedul-
ing through simulated annealing. In Proceedings of the 34th
Conference on Winter Simulation: Exploring New Frontiers,
WSC ’02, page 1909–1913. Winter Simulation Conference,
2002.

[14] Fangzhen Ge, Zhen Wei, Yiming Tian, and Zhenjin Huang.
Chaotic ant swarm for graph coloring. In 2010 IEEE Inter-
national Conference on Intelligent Computing and Intelligent
Systems, volume 1, pages 512–516, 2010.

[15] Xijun Lin, Qiang Lin, and Yanwei Shang. A scheduling op-
timization algorithm based on graph theory and simulated an-
In 2021 6th International Conference on Inventive
nealing.
Computation Technologies (ICICT), pages 492–496, 2021.

[16] Alex Fabrikant and Tad Hogg. Graph coloring with quantum

heuristics. In AAAI/IAAI, pages 22–27, 2002.

[3] Musa Hindi and Roman Yampolskiy. Genetic algorithm ap-
plied to the graph coloring problem. Midwest Artiﬁcial Intel-
ligence and Cognitive Science Conference, page 60, 01 2012.

[17] Long Yuan, Lu Qin, Xuemin Lin, Lijun Chang, and Wenjie
Zhang. Eﬀective and eﬃcient dynamic graph coloring. Proc.
VLDB Endow., 11(3):338–351, nov 2017.

[4] A. Hertz and D. de Werra. Using tabu search techniques for
graph coloring. Computing, Springer, 39:345–351, 1987.

[5] Michael A. Trick Anuj Mehrotra. A column generation ap-
proach for graph coloring. INFORMS Journal on Computing,
8(4):344–354, January 1996.

[6] Christopher D. Manning and Hinrich Sch¨utze. Foundations
of Statistical Natural Language Processing. MIT Press, Cam-
bridge, MA, USA, 1999.

[7] Shibamouli Lahiri, Carmen Banea, and Rada Mihalcea.
In In-
Matching graduate applicants with faculty members.
ternational Conference on Social Informatics, pages 41–55.
Springer, 2017.

[8] Amit Singh, Rose Catherine, Karthik Visweswariah, Vi-
jil Chenthamarakshan,
and Nandakishore Kambhatla.
PROSPECT: a system for screening candidates for recruit-
In Proceedings of the 19th ACM Conference on
ment.

[18] Amani A. Alahmadi, Taghreed M. Alamri, and Manar I.
Hosny. Time eﬃcient demon algorithm for graph coloring
with search cut-oﬀ property. In 2014 Science and Information
Conference, pages 254–259, 2014.

[19] Leighton. A graph coloring algorithm for large schedul-
ing problems. J Res Natl Bur Stand (1977), 84(6):489–506,
November 1979.

[20] Nirbhay K. Mehta. The application of a graph coloring
INFORMS

method to an examination scheduling problem.
Journal on Applied Analytics, 11(5):57–65, October 1981.

[21] Ammar Elhassan. Graph-coloring for course scheduling — a
comparative analysis based on course selection order. In The
Third International Conference on e-Technologies and Net-
works for Development (ICeND2014), pages 83–88, 2014.

16

Sharma, Gupta, Machinewala, Dingra, Tripathi, Shreyas, Chakrabarti

[22] Amal Dandashi and Mayez Al-Mouhamed. Graph coloring
for class scheduling. In ACS/IEEE International Conference
on Computer Systems and Applications - AICCSA 2010, pages
1–4, 2010.

[23] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant
colony optimization. IEEE Computational Intelligence Mag-
azine, 1(4):28–39, 2006.

[24] TaeChoong Chung SangHyuck Ahn, SeungGwan Lee. Mod-
In Fourth In-
iﬁed ant colony system for coloring graphs.
ternational Conference on Information, Communications and
Signal Processing, 2003 and the Fourth Paciﬁc Rim Confer-
ence on Multimedia. Proceedings of the 2003 Joint, volume 3,
pages 1849–1853 vol.3, 2003.

