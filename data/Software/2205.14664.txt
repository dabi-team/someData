Heterogeneous Data-Centric Architectures for Modern Data-Intensive Applications:
Case Studies in Machine Learning and Databases

Geraldo F. Oliveira(cid:5) Amirali Boroumand‡

(cid:5)ETH Zürich

‡Google

Saugata Ghose(cid:12)
(cid:12)University of Illinois Urbana-Champaign

Juan Gómez-Luna(cid:5) Onur Mutlu(cid:5)

1. Motivation & Problem

2. Mensa: Accelerating Google Neural Network

2
2
0
2

y
a
M
9
2

]

R
A
.
s
c
[

1
v
4
6
6
4
1
.
5
0
2
2
:
v
i
X
r
a

Today’s computing systems require moving data back-and-
forth between computing resources (e.g., CPUs, GPUs, accel-
erators) and off-chip main memory so that computation can
take place on the data. Unfortunately, this data movement is a
major bottleneck for system performance and energy consump-
tion [1, 2]. One promising execution paradigm that alleviates
the data movement bottleneck in modern and emerging appli-
cations is processing-in-memory (PIM) [2–12], where the cost
of data movement to/from main memory is reduced by placing
computation capabilities close to memory. In the data-centric
PIM paradigm, the logic close to memory has access to data
with signiﬁcantly higher memory bandwidth, lower latency,
and lower energy consumption than processors/accelerators in
existing processor-centric systems.

Naively employing PIM to accelerate data-intensive work-
loads can lead to sub-optimal performance due to the many
design constraints PIM substrates impose (e.g., limited area
and power budget available inside 3D-stacked memories [6]
or manufacturing limitations of combining memory and logic
elements [6, 13]). Therefore, many recent works co-design
specialized PIM accelerators and algorithms to improve per-
formance and reduce the energy consumption of (i) applica-
tions from various application domains, such as graph pro-
cessing [14–40], machine learning [1, 41–77], bioinformat-
ics [78–100], high-performance computing [95, 101–112],
databases [18, 19, 29, 46, 60, 113–130], security [131–140],
data manipulation [29, 115, 141–146], and mobile work-
loads [1, 46]; and (ii) various computing environments, includ-
ing cloud systems [13, 15, 113, 147–150], mobile systems [1],
and edge devices [151, 152].

We showcase the beneﬁts of co-designing algorithms and
hardware in a way that efﬁciently takes advantage of the PIM
paradigm for two modern data-intensive applications: (1) ma-
chine learning inference models for edge devices and (2) hy-
brid transactional/analytical processing databases for cloud
systems. We follow a two-step approach in our system design.
In the ﬁrst step, we extensively analyze the computation and
memory access patterns of each application to gain insights
into its hardware/software requirements and major sources
of performance and energy bottlenecks in processor-centric
systems. In the second step, we leverage the insights from the
ﬁrst step to co-design algorithms and hardware accelerators
to enable high-performance and energy-efﬁcient data-centric
architectures for each application.

1

Models for Edge Devices

Modern consumer devices make widespread use of machine
learning (ML). The growing complexity of these devices, com-
bined with increasing demand for privacy, connectivity, and
real-time responses, has spurred signiﬁcant interest in push-
ing ML inference computation to the edge (i.e., in or near
consumer devices, instead of the cloud) [153–155]. Due to
their resource-constrained nature, edge computing platforms
now employ specialized energy-efﬁcient accelerators for on-
device inference (e.g., Google Edge Tensor Processing Unit,
TPU [156]; NVIDIA Jetson [157]; Intel Movidius [158]). At
the same time, neural network (NN) algorithms are evolving
rapidly, which has led to many types of NN models.

Despite the wide variety of NN model types, Google’s state-
of-the-art Edge TPU [156] provides an optimized one-size-ﬁts-
all design (i.e., a monolithic accelerator with a ﬁxed, large
number of processing elements (PEs) and a ﬁxed dataﬂow,
which determines how data moves within the accelerator) that
caters to edge device area and energy constraints. Unfortu-
nately, it is very challenging to achieve high energy efﬁciency,
computational throughput, and area efﬁciency for each NN
model with this one-size-ﬁts-all design.

We conduct an in-depth analysis of ML inference execu-
tion on a commercial Edge TPU, across 24 state-of-the-art
Google edge NN models spanning four popular NN model
types: (1) convolutional neural networks (CNNs), (2) long
short-term memories (LSTMs) [159], (3) Transducers [160–
162], and (4) recurrent CNNs (RCNNs) [163, 164]. The key
takeaway from our extensive analysis of Google edge NN
models on the Edge TPU is that all key components of an edge
accelerator (i.e., PE array, dataﬂow, memory system) must be
co-designed and co-customized based on speciﬁc layer char-
acteristics to achieve high utilization and energy efﬁciency.
Therefore, our goal is to revisit the design of edge ML accel-
erators such that they are aware of and can fully exploit the
growing variation within and across edge NN models.

To this end, we propose Mensa, the ﬁrst general HW/SW
composable framework for ML acceleration in edge devices.
The key idea of Mensa is to perform NN layer execution across
multiple on-chip and near-data accelerators, each of which is
small and tailored to the characteristics of a particular subset
(i.e., family) of layers. Our experimental study of the char-
acteristics of different layers in the Google edge NN models
reveals that the layers naturally group into a small number
of clusters that are based on a subset of these characteristics.
This new insight allows us to signiﬁcantly limit the number of

 
 
 
 
 
 
different accelerators required in a Mensa design. We design
a runtime scheduler for Mensa to determine which of these
accelerators should execute which NN layer, using informa-
tion about (1) which accelerator is best suited to the layer’s
characteristics, and (2) inter-layer dependencies.

Using our new insight about layer clustering, we develop
Mensa-G, an example design for Mensa optimized for Google
edge NN models. We ﬁnd that the design of Mensa-G’s un-
derlying accelerators should center around two key layer char-
acteristics (memory boundedness, and activation/parameter
reuse opportunities). This allows us to provide efﬁcient infer-
ence execution for all of the Google edge NN models using
only three accelerators in Mensa-Gthat we call Pascal, Pavlov,
and Jacquard. Pascal, for compute-centric layers, maintains
the high PE utilization that these layers achieve in the Edge
TPU, but does so using an optimized dataﬂow that both re-
duces the size of the on-chip buffer (16× smaller than in the
Edge TPU) and the amount of on-chip network trafﬁc. Pavlov,
for LSTM-like data-centric layers, employs a dataﬂow that
enables the temporal reduction of output activations, and en-
ables the parallel execution of layer operations in a way that
increases parameter reuse, greatly reducing off-chip memory
trafﬁc. Jacquard, for other data-centric layers, signiﬁcantly
reduces the size of the on-chip parameter buffer (by 32×) us-
ing a dataﬂow that exposes reuse opportunities for parameters.
As both Pavlov and Jacquard are optimized for data-centric
layers, which require signiﬁcant memory bandwidth and are
unable to utilize a signiﬁcant fraction of PEs in the Edge TPU,
we place the accelerators in the logic layer of 3D-stacked
memories [165–167] and use signiﬁcantly smaller PE arrays
compared to the PE array in Pascal, unleashing signiﬁcant
performance and energy beneﬁts.

Our evaluation shows that compared to the baseline Edge
TPU, Mensa-G reduces total inference energy by 66.0%, im-
proves energy efﬁciency (TFLOP/J) by 3.0×, and increases
computational throughput (TFLOP/s) by 3.1×, averaged
across all 24 Google edge NN models. Mensa-G improves
inference energy efﬁciency and throughput by 2.4× and 4.3×
over Eyeriss v2, a state-of-the-art accelerator.

We conclude that employing our Mensa framework and tai-
loring ML accelerators to the key characteristics of NN layers
can provide performance, energy, and area beneﬁts to edge
devices. For more information on Mensa, a detailed descrip-
tion of Mensa-G’s accelerators, and our extensive evaluation
results, please refer to the full version of our paper [45, 168].

of data [179, 180]. To enable real-time data analysis, state-
of-the-art database management systems (DBMSs) leverage
hybrid transactional and analytical processing (HTAP) [181–
183]. An HTAP DBMS is a single-DBMS solution that sup-
ports both transactional and analytical workloads [179, 181,
184–186].

Ideally, an HTAP system should have three properties [185]
to guarantee efﬁcient execution of transactional and analytical
workloads. First, it should ensure that both transactional and
analytical workloads beneﬁt from their own workload-speciﬁc
optimizations (e.g., algorithms, data structures). Second, it
should guarantee data freshness and data consistency (i.e.,
access to the most recent version of data) for analytical work-
loads while ensuring that both transactional and analytical
workloads have a consistent view of data across the system.
Third, it should ensure that the latency and throughput of both
the transactional workload and the analytical workload are the
same as if each of them were run in isolation.

We extensively study state-of-the-art HTAP systems and
observe two key problems that prevent them from achieving all
three properties of an ideal HTAP system. First, these systems
experience a drastic reduction in transactional throughput (up
to 74.6%) and analytical throughput (up to 49.8%) compared
to when transactional and analytical workloads run in isolation.
This is because the mechanisms used to provide data freshness
and consistency induce a large amount of data movement
between the CPU cores and main memory. Second, HTAP
systems often fail to provide effective performance isolation.
These systems suffer from severe performance interference
because of the high resource contention between transactional
workloads and analytical workloads. Therefore, our goal is
to develop an HTAP system that overcomes these problems
while achieving all three of the desired HTAP properties.

We propose a novel system for HTAP databases called Poly-
nesia. The key insight behind Polynesia is to partition the
computing resources into two isolated new custom process-
ing islands: transactional islands and analytical islands. An
island is a hardware–software co-designed component spe-
cialized for speciﬁc types of queries. Each island consists of
(1) a replica of data for a speciﬁc workload, (2) an optimized
execution engine, and (3) a set of hardware resources that cater
to the execution engine and its memory access patterns.

3. Polynesia: Accelerating Hybrid Transac-

tional/Analytical Processing using PIM

Many application domains, such as fraud detection [169–171],
business intelligence [172–174], healthcare [175, 176], per-
sonalized recommendation [177, 178], and IoT [177], have a
critical need to perform real-time data analysis, where data
analysis needs to be performed using the most recent version

Figure 1 shows the high-level organization of Polynesia,
which includes one transactional island and one analytical
island. Polynesia is equipped with a 3D-stacked memory
similar to the Hybrid Memory Cube (HMC) [166], where
multiple vertically-stacked DRAM layers are connected with
a logic layer using thousands of through-silicon vias (TSVs).
An HMC chip is split up into multiple vaults, where each vault
corresponds to a vertical slice of the memory and logic layer.

2

Figure 1: High-level organization of Polynesia hardware.

Polynesia meets all desired properties from an HTAP sys-
tem in three ways. First, by employing processing islands,
Polynesia enables workload-speciﬁc optimizations for both
transactional and analytical workloads. Second, we design
new hardware accelerators to add specialized capabilities to
each island, which we exploit to optimize the performance
of several key HTAP algorithms. This includes new accel-
erators and modiﬁed algorithms to propagate transactional
updates to analytical islands and to maintain a consistent view
of data across the system. Such new components ensure data
freshness and data consistency in our HTAP system. Third,
we tailor the design of transactional and analytical islands
to ﬁt the characteristics of transactional and analytical work-
loads. The transactional islands use dedicated CPU hardware
resources (i.e., multicore CPUs and multi-level caches) to exe-
cute transactional workloads since transactional queries have
cache-friendly access patterns [18, 19, 46]. The analytical
islands leverage PIM techniques [6, 7, 187] due to the large
data trafﬁc analytical workloads produce. We equip the ana-
lytical islands with a new PIM-based analytical engine that
includes simple in-order PIM cores added to the logic layer of
a 3D-stacked memory [48, 166, 167], software to handle data
placement, and runtime task scheduling heuristics. Our new
design enables the execution of transactional and analytical
workloads at low latency and high throughput.

In our evaluations, we show the beneﬁts of each component
of Polynesia, and compare its end-to-end performance and
energy usage to three state-of-the-art HTAP systems (modeled
after Hyper [188], AnkerDB [189], and Batch-DB [185]).
Polynesia outperforms all three, with higher transactional
throughput (2.20×/1.15×/1.94×; mean of 1.70×) and analyti-
cal throughput (3.78×/5.04×/2.76×; mean of 3.74×), while
consuming 48% lower energy than the prior lowest-energy
HTAP system.

We conclude that Polynesia efﬁciently provides high-
throughput real-time data analysis, while meeting all three
desired HTAP properties. For more information on Polynesia
design, including our tailored algorithms and hardware units
to maintain data freshness and data consistency, and the design
of our analytical engine, as well as our extensive evaluation
results, please refer to the full version of our paper [117, 190].
We open-source Polynesia and the complete source code of
our evaluation [191].

4. Conclusion & Future Work

This paper summarizes our recent works on accelerat-
ing emerging data-intensive applications with processing-in-

memory (PIM). We showcase the performance and energy
beneﬁts that PIM provides for edge neural network models
and hybrid transactional/analytical processing systems. Our
proposed PIM architectures outperform state-of-the-art solu-
tions by co-designing algorithms and hardware while reducing
area costs. We hope our work inspires the design of novel,
high-performance, and energy-efﬁcient data-centric PIM ar-
chitectures for many other emerging applications.

Acknowledgments

We thank SAFARI Research Group members for valuable
feedback and the stimulating intellectual environment they
provide. We acknowledge the generous gifts provided by
our industrial partners, including ASML, Facebook, Google,
Huawei, Intel, Microsoft, and VMware. We acknowledge
support from the Semiconductor Research Corporation and
the ETH Future Computing Laboratory.

This invited extended abstract is a summary version of our
prior works, Mensa [168, 192] (published at PACT 2021) and
Polynesia [117, 190] (published at ICDE 2022). Presenta-
tions describing Mensa and Polynesia can be found at [193]
and [194], respectively.

References

[1] A. Boroumand et al., “Google Workloads for Consumer Devices: Miti-

gating Data Movement Bottlenecks,” in ASPLOS, 2018.

[2] O. Mutlu, “Memory Scaling: A Systems Architecture Perspective,” in

IMW, 2013.

[3] G. F. Oliveira et al., “DAMOV: A New Methodology and Benchmark
Suite for Evaluating Data Movement Bottlenecks,” arXiv:2105.03725
[cs.AR], 2021.

[4] G. F. Oliveira et al., “DAMOV: A New Methodology and Benchmark
Suite for Evaluating Data Movement Bottlenecks,” IEEE Access, 2021.
[5] S. Ghose et al., “The Processing-in-Memory Paradigm: Mechanisms to
Enable Adoption,” in Beyond-CMOS Technologies for Next Generation
Computer Design, 2019.

[6] O. Mutlu et al., “A Modern Primer on Processing in Memory,”

arXiv:2012.03112 [cs.AR], 2021.

[7] S. Ghose et al., “Processing-in-Memory: A Workload-Driven Perspec-

tive,” IBM JRD, 2019.

[8] O. Mutlu et al., “Enabling Practical Processing in and near Memory for

Data-Intensive Computing,” in DAC, 2019.

[9] O. Mutlu and L. Subramanian, “Research Problems and Opportunities in

Memory Systems,” SUPERFRI, 2014.

[10] O. Mutlu, “Intelligent Architectures for Intelligent Computing Systems,”

in DATE, 2021.

[11] O. Mutlu, “Intelligent Architectures for Intelligent Machines,” in VLSI-

DAT, 2020.

[12] O. Mutlu, “Main Memory Scaling: Challenges and Solution Directions,”
in More Than Moore Technologies for Next Generation Computer De-
sign. Springer-Verlag, 2015.

[13] F. Devaux, “The True Processing in Memory Accelerator,” in HCS, 2019.
[14] J. Ahn et al., “PIM-Enabled Instructions: A Low-Overhead, Locality-

Aware Processing-in-Memory Architecture,” in ISCA, 2015.

[15] J. Ahn et al., “A Scalable Processing-in-Memory Accelerator for Parallel

Graph Processing,” in ISCA, 2015.

[16] L. Nai et al., “GraphPIM: Enabling Instruction-Level PIM Ofﬂoading in

Graph Computing Frameworks,” in HPCA, 2017.

[17] L. Song et al., “GraphR: Accelerating Graph Processing Using ReRAM,”

in HPCA, 2018.

[18] A. Boroumand et al., “LazyPIM: An Efﬁcient Cache Coherence Mecha-

nism for Processing-in-Memory,” IEEE CAL, 2017.

3

VaultAnalytical EnginePIM CorePIM CorePIM CorePIM CoreConsistency Mechanism Copy UnitUpdate Propagation Mechanism Update Gathering and Shipping UnitUpdate Application UnitMemory ControllerTransactional EngineCPUCPUCPUCPUShared Last-Level Cache (LLC)3D-Stacked MemoryTSVProcessorOff-Chip LinkTransactional IslandAnalytical IslandLogic LayerDRAM LayerDRAM Banks[19] A. Boroumand et al., “CoNDA: Efﬁcient Cache Coherence Support for

Near-data Accelerators,” in ISCA, 2019.

[20] M. Zhang et al., “GraphP: Reducing Communication for PIM-Based
Graph Processing with Efﬁcient Data Partition,” in HPCA, 2018.
[21] S. Angizi et al., “GraphS: A Graph Processing Accelerator Leveraging

SOT-MRAM,” in DATE, 2019.

[22] S. Angizi and D. Fan, “GraphiDe: A Graph Processing Accelerator

Leveraging In-DRAM-Computing,” in GLSVLSI, 2019.

[23] Y. Zhuo et al., “GraphQ: Scalable PIM-Based Graph Processing,” in

MICRO, 2019.

[24] G. Dai et al., “GraphH: A Processing-in-Memory Architecture for Large-

Scale Graph Processing,” IEEE TCAD, 2018.

[25] Y. Huang et al., “A Heterogeneous PIM Hardware-Software Co-Design

for Energy-Efﬁcient Graph Processing,” in IPDPS, 2020.

[26] M. Besta et al., “SISA: Set-Centric Instruction Set Architecture for Graph
Mining on Processing-in-Memory Systems,” in MICRO, 2021.
[27] L. Nai and H. Kim, “Instruction Ofﬂoading with HMC 2.0 Standard: A

Case Study for Graph Traversals,” in MEMSYS, 2015.

[28] G. Dai et al., “GraphSAR: A Sparsity-Aware Processing-in-Memory
Architecture for Large-Scale Graph Processing on ReRAMs,” in ASP-
DAC, 2019.

[29] S. Li et al., “Pinatubo: A Processing-in-Memory Architecture for Bulk
Bitwise Operations in Emerging Non-Volatile Memories,” in DAC, 2016.
[30] L. Han et al., “A Novel ReRAM-Based Processing-in-Memory Architec-

ture for Graph Traversal,” TOS, 2018.

[31] Y. Zhuo et al., “Distributed Graph Processing System and Processing-in-
Memory Architecture with Precise Loop-Carried Dependency Guaran-
tee,” TOCS, 2021.

[32] E. Azarkhish et al., “Design and Evaluation of a Processing-in-Memory

Architecture for the Smart Memory Cube,” in ARCS, 2016.

[49] S. Lee et al., “Hardware Architecture and Software Stack for PIM Based
on Commercial DRAM Technology: Industrial Product,” in ISCA, 2021.
[50] L. Ke et al., “Near-Memory Processing in Action: Accelerating Person-

alized Recommendation with AxDIMM,” IEEE Micro, 2021.

[51] D. Niu et al., “184QPS/W 64Mb/mm2 3D Logic-to-DRAM Hybrid Bond-
ing with Process-Near-Memory Engine for Recommendation System,”
in ISSCC, 2022.

[52] Z. He et al., “Sparse BD-Net: A Multiplication-Less DNN with Sparse

Binarized Depth-Wise Separable Convolution,” JETC, 2020.

[53] M. Peemen et al., “Memory-Centric Accelerator Design for Convolu-

tional Neural Networks,” in ICCD, 2013.

[54] S. Angizi et al., “IMCE: Energy-Efﬁcient Bitwise In-Memory Convolu-

tion Engine for Deep Neural Network,” in ASP-DAC, 2018.

[55] Q. Deng et al., “DrAcc: A DRAM Based Accelerator for Accurate CNN

Inference,” in DAC, 2018.

[56] C. Eckert et al., “Neural Cache: Bit-Serial In-Cache Acceleration of

Deep Neural Networks,” in ISCA, 2018.

[57] M. Imani et al., “FloatPIM: In-Memory Acceleration of Deep Neural

Network Training with High Precision,” in ISCA, 2019.

[58] S. Cho et al., “McDRAM v2: In-Dynamic Random Access Memory
Systolic Array Accelerator to Address the Large Model Problem in
Deep Neural Networks on the Edge,” IEEE Access, 2020.

[59] H. Shin et al., “McDRAM: Low Latency and Energy-Efﬁcient Matrix

Computations in DRAM,” IEEE TCADICS, 2018.

[60] N. Hajinazar et al., “SIMDRAM: A Framework for Bit-Serial SIMD

Processing Using DRAM,” in ASPLOS, 2021.

[61] L. Ke et al., “RecNMP: Accelerating Personalized Recommendation

with Near-Memory Processing,” in ISCA, 2020.

[62] J. Park et al., “TRiM: Enhancing Processor-Memory Interfaces with

[33] X. Xie et al., “SpaceA: Sparse Matrix Vector Multiplication on

Scalable Tensor Reduction in Memory,” in MICRO, 2021.

Processing-in-Memory Accelerator,” in HPCA, 2021.

[34] M. Imani et al., “DigitalPIM: Digital-Based Processing In-Memory for

Big Data Acceleration,” in GLSVLSI, 2019.

[63] Y. Wang et al., “REREC: In-ReRAM Acceleration with Access-Aware
Mapping for Personalized Recommendation,” in ICCAD, 2021.
[64] M. Wilkening et al., “RecSSD: Near Data Processing for Solid State

[35] L. Han et al., “A Novel ReRAM-Based Processing-in-Memory Architec-

Drive Based Recommendation Inference,” in ASPLOS, 2021.

ture for Graph Computing,” in NVMSA, 2017.

[36] M. Zhou et al., “GRAM: Graph Processing in a ReRAM-Based Compu-

tational Memory,” in ASP-DAC, 2019.

[37] L. Zheng et al., “Spara: An Energy-Efﬁcient ReRAM-Based Accelerator

for Sparse Graph Analytics Applications,” in IPDPS, 2020.

[38] H. Liu et al., “ReGra: Accelerating Graph Traversal Applications Using
ReRAM With Lower Communication Cost,” IEEE Access, 2020.
[39] G. Li et al., “GraphIA: An In-Situ Accelerator for Large-Scale Graph

Processing,” in MEMSYS, 2018.

[40] E. Kim and H. Kim, “Things to Consider to Enable Dynamic Graphs in

Processing-in-Memory,” in MEMSYS, 2020.

[41] M. Gao et al., “TETRIS: Scalable and Efﬁcient Neural Network Acceler-

ation with 3D Memory,” in ASPLOS, 2017.

[42] D. Kim et al., “NeuroCube: A Programmable Digital Neuromorphic

Architecture with High-Density 3D Memory,” in ISCA, 2016.

[43] A. Shaﬁee et al., “ISAAC: A Convolutional Neural Network Accelerator

with In-Situ Analog Arithmetic in Crossbars,” in ISCA, 2016.

[44] P. Chi et al., “PRIME: A Novel Processing-In-Memory Architecture
for Neural Network Computation In ReRAM-Based Main Memory,” in
ISCA, 2016.

[45] A. Boroumand et al., “Mitigating Edge Machine Learning Inference
Bottlenecks: An Empirical Study on Accelerating Google Edge Models,”
arXiv:2103.00768 [cs.AR], 2021.

[46] A. Boroumand, “Practical Mechanisms for Reducing Processor-Memory
Data Movement in Modern Workloads,” Ph.D. dissertation, Carnegie
Mellon University, 2020.

[47] S. Lee et al., “A 1ynm 1.25V 8Gb, 16Gb/s/Pin GDDR6-Based
Accelerator-in-Memory Supporting 1TFLOPS MAC Operation and Var-
ious Activation Functions for Deep-Learning Applications,” in ISSCC,
2022.

[48] Y.-C. Kwon et al., “25.4 A 20nm 6GB Function-In-Memory DRAM,
Based on HBM2 with a 1.2 TFLOPS Programmable Computing Unit
Using Bank-Level Parallelism, for Machine Learning Applications,” in
ISSCC, 2021.

[65] L. Huang et al., “Practical Near-Data-Processing Architecture for Large-
Scale Distributed Graph Neural Network,” IEEE Access, 2022.
[66] G. Yuan et al., “FORMS: Fine-Grained Polarized ReRAM-Based In-Situ
Computation for Mixed-Signal DNN Accelerator,” in ISCA, 2021.
[67] Y. Huang et al., “Accelerating Graph Convolutional Networks Using
Crossbar-Based Processing-In-Memory Architectures,” in HPCA, 2022.
[68] M. Imani et al., “DUAL: Acceleration of Clustering Algorithms using

Digital-Based Processing In-Memory,” in MICRO, 2020.

[69] S. Resch et al., “MOUSE: Inference in Non-Volatile Memory for Energy

Harvesting Applications,” in MICRO, 2020.

[70] Y. Kwon et al., “TensorDIMM: A Practical Near-Memory Processing
Architecture for Embeddings and Tensor Operations in Deep Learning,”
in MICRO, 2019.

[71] H. Kim et al., “NAND-Net: Minimizing Computational Complexity of
In-Memory Processing for Binary Neural Networks,” in HPCA, 2019.
[72] T. Cao et al., “Performance Analysis of Convolutional Neural Network
using Multi-Level Memristor Crossbar for Edge Computing,” in ICoIAS,
2020.

[73] A. V. Nori et al., “REDUCT: Keep it Close, Keep it Cool!: Efﬁcient Scal-
ing of DNN Inference on Multi-Core CPUs with Near-Cache Compute,”
in ISCA, 2021.

[74] P. Das et al., “Towards Near Data Processing of Convolutional Neural

Networks,” in VLSID, 2018.

[75] Y. Long et al., “ReRAM-Based Processing-in-Memory Architecture for

Recurrent Neural Network Acceleration,” TVLSI, 2018.

[76] A. S. Rakin et al., “PIM-TGAN: A Processing-in-Memory Accelerator
for Ternary Generative Adversarial Networks,” in ICCD, 2018.
[77] Y. Long et al., “Q-PIM: A Genetic Algorithm Based Flexible DNN Quan-
tization Method and Application to Processing-in-Memory Platform,” in
DAC, 2020.

[78] J. S. Kim et al., “GRIM-Filter: Fast Seed Location Filtering in DNA Read
Mapping Using Processing-in-Memory Technologies,” BMC Genomics,
2018.

4

[79] J. S. Kim et al., “GRIM-Filter: Fast Seed Location Filtering in DNA
Read Mapping Using Processing-in-Memory Technologies,” in PSB,
2018.

[80] J. Kim et al., “Genome Read In-Memory (GRIM) Filter: Fast Location
Filtering in DNA Read Mapping using Emerging Memory Technologies,”
in PSB, 2017.

[81] J. Kim et al., “Genome Read In-Memory (GRIM) Filter: Fast Location
Filtering in DNA Read Mapping with Emerging Memory Technologies,”
in RECOMB, 2016.

[82] G. F. Oliveira et al., “NIM: An HMC-Based Machine for Neuron Com-

putation,” in ARC, 2017.

[83] D. S. Cali et al., “GenASM: A High-Performance, Low-Power Approxi-
mate String Matching Acceleration Framework for Genome Sequence
Analysis,” in MICRO, 2020.

[84] N. M. Ghiasi et al., “GenStore: A High-Performance and Energy-
Efﬁcient In-Storage Computing System for Genome Sequence Analysis,”
in ASPLOS, 2022.

[85] D. S. Cali et al., “SeGraM: A Universal Hardware Accelerator for Ge-
nomic Sequence-to-Graph and Sequence-to-Sequence Mapping,” in
ISCA, 2022.

[86] S. Angizi et al., “AlignS: A Processing-in-Memory Accelerator for DNA
Short Read Alignment Leveraging SOT-MRAM,” in DAC, 2019.
[87] S. Gupta et al., “RAPID: A ReRAM Processing In-Memory Architecture

for DNA Sequence Alignment,” in ISLPED, 2019.

[88] X.-Q. Li et al., “PIM-Align: A Processing-in-Memory Architecture for

FM-Index Search Algorithm,” JCST, 2021.

[89] S. Angizi et al., “Exploring DNA Alignment-in-Memory Leveraging

Emerging SOT-MRAM,” in GLSVLSI, 2020.

[90] Z. I. Chowdhury et al., “A DNA Read Alignment Accelerator Based on

Computational RAM,” JXCDC, 2020.

[91] S. Angizi et al., “PIM-Aligner: A Processing-in-MRAM Platform for

Biological Sequence Alignment,” in DATE, 2020.

[92] R. Kaplan et al., “BioSEAL: In-Memory Biological Sequence Alignment
Accelerator for Large-Scale Genomic Data,” in SYSTOR, 2020.
[93] F. Zhang et al., “PIM-Quantiﬁer: A Processing-in-Memory Platform for

mRNA Quantiﬁcation,” in DAC, 2021.

[94] F. Chen et al., “PARC: A Processing-in-CAM Architecture for Genomic

Long Read Pairwise Alignment using ReRAM,” in ASP-DAC, 2020.

[95] G. Singh et al., “FPGA-Based Near-Memory Acceleration of Modern

Data-Intensive Applications,” IEEE Micro, 2021.

[96] M. Alser et al., “Accelerating Genome Analysis: A Primer on an Ongoing

Journey,” IEEE Micro, 2020.

[97] A. Nag et al., “GenCache: Leveraging In-Cache Operators for Efﬁcient

Sequence Alignment,” in MICRO, 2019.

[98] M. Zhou et al., “Ultra Efﬁcient Acceleration for De Novo Genome

Assembly via Near-Memory Computing,” in PACT, 2021.

[99] L. Wu et al., “Sieve: Scalable In-Situ DRAM-Based Accelerator Designs

for Massively Parallel k-Mer Matching,” in ISCA, 2021.

[100] S. Xu et al., “AQUOMAN: An Analytic-Query Ofﬂoading Machine,” in

MICRO, 2020.

[101] G. Singh et al., “NERO: A Near High-Bandwidth Memory Stencil Ac-

celerator for Weather Prediction Modeling,” in FPL, 2020.

[102] A. Denzler et al., “Casper: Accelerating Stencil Computation Using

Near-Cache Processing,” arXiv:2112.14216 [cs.AR], 2021.

[103] E. Vermij et al., “Boosting the Efﬁciency of HPCG and Graph500 with

Near-Data Processing,” in ICPP, 2017.

[104] C. Giannoula et al., “SynCron: Efﬁcient Synchronization Support for

Near-Data-Processing Architectures,” in HPCA, 2021.

[105] I. Fernandez et al., “NATSA: A Near-Data Processing Accelerator for

Time Series Analysis,” in ICCD, 2020.

[110] B. Gu et al., “Biscuit: A Framework for Near-Data Processing of Big

Data Workloads,” in ISCA, 2016.

[111] C. Xie et al., “Processing-in-Memory Enabled Graphics Processors for

3D Rendering,” in HPCA, 2017.

[112] F. Wang et al., “ReRAM-Based Processing-in-Memory Architecture for

Blockchain Platforms,” in ASP-DAC, 2019.

[113] M. Drumond et al., “The Mondrian Data Engine,” in ISCA, 2017.
[114] P. C. Santos et al., “Operand Size Reconﬁguration for Big Data Process-

ing in Memory,” in DATE, 2017.

[115] V. Seshadri et al., “Ambit: In-Memory Accelerator for Bulk Bitwise
Operations Using Commodity DRAM Technology,” in MICRO, 2017.
[116] K. Hsieh et al., “Accelerating Pointer Chasing in 3D-Stacked Memory:

Challenges, Mechanisms, Evaluation,” in ICCD, 2016.

[117] A. Boroumand et al., “Polynesia: Enabling Effective Hybrid Transac-
tional/Analytical Databases with Specialized Hardware/Software Co-
Design,” arXiv:2103.00798 [cs.AR], 2021.

[118] V. Seshadri and O. Mutlu, “In-DRAM Bulk Bitwise Execution Engine,”

arXiv:1905.09822 [cs.AR], 2019.

[119] V. Seshadri and O. Mutlu, “The Processing Using Memory Paradigm:
Initialization, Bitwise AND and OR,”

In-DRAM Bulk Copy,
arXiv:1610.09603 [cs:AR], 2016.

[120] V. Seshadri and O. Mutlu, “Simple Operations in Memory to Reduce
Data Movement,” in Advances in Computers, Volume 106, 2017.
[121] V. Seshadri et al., “Buddy-RAM: Improving the Performance and Efﬁ-
ciency of Bulk Bitwise Operations Using DRAM,” arXiv:1611.09988
[cs:AR], 2016.

[122] M. Gao et al., “Practical Near-Data Processing for In-Memory Analytics

Frameworks,” in PACT, 2015.

[123] S. L. Xi et al., “Beyond the Wall: Near-Data Processing for Databases,”

in DaMoN, 2015.

[124] S. Lu et al., “Agile Query Processing in Statistical Databases: A Process-

In-Memory Approach,” in KSEM, 2019.

[125] D. G. Tomé et al., “HIPE: HMC Instruction Predication Extension Ap-

plied on Database Processing,” in DATE, 2018.

[126] T. R. Kepe et al., “Database Processing-in-Memory: An Experimental

Study,” Proc. VLDB Endow., 2019.

[127] Y. Sun et al., “Bidirectional Database Storage and SQL Query Exploiting

RRAM-Based Process-in-Memory Structure,” TOS, 2018.

[128] D. Lee et al., “Optimizing Data Movement with Near-Memory Accelera-

tion of In-memory DBMS,” in EDBT, 2020.

[129] O. O. Babarinsa and S. Idreos, “JAFAR: Near-Data Processing for

Databases,” in SIGMOD, 2015.

[130] B. Lekshmi and K. Meyer-Wegener, “COPRAO: A Capability Aware
Query Optimizer for Reconﬁgurable Near Data Processors,” in ICDEW,
2021.

[131] P. Gu et al., “Leveraging 3D Technologies for Hardware Security: Op-

portunities and Challenges,” in GLSVLSI, 2016.

[132] J. S. Kim et al., “D-RaNGe: Using Commodity DRAM Devices to Gen-
erate True Random Numbers With Low Latency and High Throughput,”
in HPCA, 2019.

[133] J. S. Kim et al., “The DRAM Latency PUF: Quickly Evaluating Physical
Unclonable Functions by Exploiting the Latency-Reliability Tradeoff in
Modern Commodity DRAM Devices,” in HPCA, 2018.

[134] W. Xiong et al., “SecNDP: Secure Near-Data Processing with Untrusted

Memory,” in HPCA, 2022.

[135] H. Nejatollahi et al., “CryptoPIM: In-Memory Acceleration for Lattice-

Based Cryptographic Hardware,” in DAC, 2020.

[136] D. Reis et al., “Computing-in-Memory for Performance and Energy-

Efﬁcient Homomorphic Encryption,” TVLSI, 2020.

[106] G. Singh et al., “NAPEL: Near-Memory Computing Application Perfor-

[137] W. Li et al., “Leveraging Memory PUFs and PIM-Based Encryption to

mance Prediction via Ensemble Learning,” in DAC, 2019.

[107] Z. Liu et al., “Concurrent Data Structures for Near-Memory Computing,”

in SPAA, 2017.

[108] A. Pattnaik et al., “Scheduling Techniques for GPU Architectures with

Processing-in-Memory Capabilities,” in PACT, 2016.

Secure Edge Deep Learning Systems,” in VTS, 2019.

[138] A. O. Glova et al., “Near-Data Acceleration of Privacy-Preserving
Biomarker Search with 3D-Stacked Memory,” in DATE, 2019.
[139] F. N. Bostancı et al., “DR-STRaNGe: End-to-End System Design for

DRAM-Based True Random Number Generators,” in HPCA, 2022.

[109] K. Hsieh et al., “Transparent Ofﬂoading and Mapping (TOM): Enabling
Programmer-Transparent Near-Data Processing in GPU Systems,” in
ISCA, 2016.

[140] A. Olgun et al., “QUAC-TRNG: High-Throughput True Random Number
Generation using Quadruple Row Activation in Commodity DRAM
Chips,” in ISCA, 2021.

5

[141] S. Li et al., “DRISA: A DRAM-Based Reconﬁgurable In-Situ Accelera-

[170] X. Qiu et al., “Real-Time Constrained Cycle Detection in Large Dynamic

tor,” in MICRO, 2017.

Graphs,” Proc. VLDB Endow., 2018.

[142] V. Seshadri et al., “RowClone: Fast and Energy-Efﬁcient In-DRAM Bulk

Data Copy and Initialization,” in MICRO, 2013.

[143] Y. Wang et al., “FIGARO: Improving System Performance via Fine-

Grained In-DRAM Data Relocation and Caching,” in MICRO, 2020.

[144] K. K. Chang et al., “Low-Cost Inter-Linked Subarrays (LISA): Enabling
Fast Inter-Subarray Data Movement in DRAM,” in HPCA, 2016.
[145] S. H. S. Rezaei et al., “NoM: Network-on-Memory for Inter-Bank Data

Transfer in Highly-Banked Memories,” CAL, 2020.

[146] V. Seshadri et al., “Fast Bulk Bitwise AND and OR in DRAM,” CAL,

2015.

[147] A. J. Awan et al., “Identifying the Potential of Near Data Processing for

Apache Spark,” in MEMSYS, 2017.

[148] J. Gómez-Luna et al., “Benchmarking a New Paradigm: An Ex-
perimental Analysis of a Real Processing-in-Memory Architecture,”
arXiv:2105.03814 [cs.AR], 2021.

[149] J. Gómez-Luna et al., “Benchmarking Memory-Centric Computing Sys-
tems: Analysis of Real Processing-in-Memory Hardware,” in CUT,
2021.

[150] J. Gómez-Luna et al., “Benchmarking a New Paradigm: Experimental
Analysis and Characterization of a Real Processing-in-Memory System,”
IEEE Access, 2022.

[151] W. A. Simon et al., “BLADE: An In-Cache Computing Architecture for

Edge Devices,” IEEE Transactions on Computers, 2020.

[152] X. Si et al., “Circuit Design Challenges in Computing-in-Memory for AI

Edge Devices,” in ASICON, 2019.

[153] C. Wu et al., “Machine Learning at Facebook: Understanding Inference

at the Edge,” in HPCA, 2019.

[154] X. Xu et al., “Scaling for Edge Inference of Deep Neural Networks,”

Nature Electronics, 2018.

[155] Y.-H. Chen et al., “Eyeriss v2: A Flexible Accelerator for Emerging

Deep Neural Networks on Mobile Devices,” JETCAS, 2019.

[156] Google LLC, “Edge TPU,” https://cloud.google.com/edge-tpu/.
[157] NVIDIA Corp., “NVIDIA Jetson Nano,” https://developer.nvidia.com/

embedded/jetson-nano-developer-kit.

[158] Intel

Corp.,

“Intel Movidius Neural
https://software.intel.com/content/www/us/en/develop/articles/
intel-movidius-neural-compute-stick.html.

Compute

Stick,”

[159] H. Sak et al., “Long Short-Term Memory Based Recurrent Neural
Network Architectures for Large Vocabulary Speech Recognition,”
arXiv:1402.1128 [cs.NE], 2014.

[160] Y. He et al., “Streaming End-to-End Speech Recognition for Mobile

Devices,” in ICASSP, 2019.

[161] J. Li et al., “Improving RNN Transducer Modeling for End-to-End

Speech Recognition,” in ASRU, 2019.

[162] K. Rao et al., “Exploring Architectures, Data and Units for
Streaming End-to-End Speech Recognition with RNN-Transducer,”
arXiv:1801.00841 [cs.CL], 2018.

[163] O. Vinyals et al., “Show and Tell: A Neural Image Caption Generator,”

in CVPR, 2015.

[164] J. Donahue et al., “Long-Term Recurrent Convolutional Networks for

Visual Recognition and Description,” in CVPR, 2015.

[165] JEDEC Solid State Technology Assn., “JESD235B: High Bandwidth

Memory (HBM) DRAM,” December 2018.

[166] Hybrid Memory Cube Consortium, “HMC Speciﬁcation 2.0,” 2014.
[167] D. Lee et al., “Simultaneous Multi-Layer Access: Improving 3D-Stacked

Memory Bandwidth at Low Cost,” ACM TACO, 2016.

[168] A. Boroumand et al., “Google Neural Network Models for Edge Devices:
Analyzing and Mitigating Machine Learning Inference Bottlenecks,” in
PACT, 2021.

[169] S. Cao et al., “TitAnt: Online Real-Time Transaction Fraud Detection in

Ant Financial,” arXiv:1906.07407 [cs.LG], 2019.

[171] J. T. Quah and M. Sriganesh, “Real-Time Credit Card Fraud Detection
Using Computational Intelligence,” Expert Systems with Applications,
2008.

[172] P.-Å. Larson et al., “Real-Time Analytical Processing with SQL Server,”

PVLDB, 2015.

[173] J. Ramnarayan et al., “SnappyData: Streaming, Transactions, and Inter-

active Analytics in a Uniﬁed Engine,” in CIDR, 2016.

[174] B. Sahay and J. Ranjan, “Real Time Business Intelligence in Supply
Chain Analytics,” Information Management & Computer Security, 2008.
[175] S. Chisholm, “Adopting Medical Technologies and Diagnostics Rec-
ommended by NICE: The Health Technologies Adoption Programme,”
Annals of the Royal College of Surgeons of England, 2014.

[176] V.-D. Ta et al., “Big Data Stream Computing in Healthcare Real-Time

Analytics,” in ICCCBDA, 2016.

[177] R. Barber et al., “WiSer: A Highly Available HTAP DBMS for IoT

Applications,” arxiv:1908.01908 [cs.DB], 2019.

[178] J. Zhou et al., “Kunpeng: Parameter Server Based Distributed Learning
Systems and Its Applications in Alibaba and Ant Financial,” in SIGKDD,
2017.

[179] P.-A. Larson et al., “Real-time Analytical Processing with SQL Server,”

Proc. VLDB Endow., 2015.

[180] D. Huang et al., “TiDB: A Raft-Based HTAP Database,” Proc. VLDB

Endow., 2020.

[181] Gartner Research, “Hybrid Transaction/Analytical Processing Will
Foster Opportunities for Dramatic Business Innovation,” 2013. [Online].
Available: https://www.gartner.com/en/documents/2657815

[182] V. Sikka et al., “SAP HANA: The Evolution from a Modern Main-
memory Data Platform to an Enterprise Application Platform,” Proc.
VLDB Endow., 2013.

[183] J. Giceva and M. Sadoghi, Hybrid OLTP and OLAP, 2018.
[184] J. Arulraj et al., “Bridging the Archipelago Between Row-Stores and

Column-Stores for Hybrid Workloads,” in SIGMOD, 2016.

[185] D. Makreshanski et al., “BatchDB: Efﬁcient Isolated Execution of Hybrid
OLTP+OLAP Workloads for Interactive Applications,” in SIGMOD
Conference, 2017.

[186] F. Özcan et al., “Hybrid Transactional/Analytical Processing: A Survey,”

in SIGMOD, 2017.

[187] O. Mutlu et al., “Processing Data Where It Makes Sense: Enabling

In-Memory Computation,” MICPRO, 2019.

[188] A. Kemper and T. Neumann, “HyPer: A Hybrid OLTP&OLAP Main
Memory Database System Based on Virtual Memory Snapshots,” in
ICDE, 2011.

[189] A. Sharma et al., “Accelerating Analytical Processing in MVCC us-
ing Fine-Granular High-Frequency Virtual Snapshotting,” in SIGMOD,
2018.

[190] A. Boroumand et al., “Polynesia: Enabling High-Performance and
Energy-Efﬁcient Hybrid Transactional/Analytical Databases with Hard-
ware/Software Co-Design,” in ICDE, 2022.

[191] SAFARI Research Group, “Polynesia — GitHub Repository,” https:

//github.com/CMU-SAFARI/Polynesia/, 2022.

[192] A. Boroumand et al., “Google Neural Network Models for Edge Devices:
Analyzing and Mitigating Machine Learning Inference Bottlenecks,”
arXiv:2109.14320 [cs.AR], 2021.

[193] G. F. Oliveira, “Google Neural Network Models for Edge Devices: Ana-
lyzing and Mitigating ML Inference Bottleneck – Talk at PACT 2021,”
https://www.youtube.com/watch?v=A5gxjDbLRAs.

[194] G. F. Oliveira, “Polynesia: Enabling High-Performance and Energy-
Efﬁcient Hybrid Transactional/Analytical Databases with Hard-
ware/Software Co-Design – Talk at ICDE 2022,” https://www.youtube.
com/watch?v=3IHmaDjtWcE.

6

