2
2
0
2

l
u
J

5

]
F
P
.
s
c
[

1
v
0
7
0
2
0
.
7
0
2
2
:
v
i
X
r
a

FLOPs as a Discriminant for Dense Linear Algebra Algorithms

Francisco López
Umeå Universitet
Umeå, Sweden
flopz@cs.umu.se

Lars Karlsson
Umeå Universitet
Umeå, Sweden
larsk@cs.umu.se

Paolo Bientinesi
Umeå Universitet
Umeå, Sweden
pauldj@cs.umu.se

ABSTRACT
Expressions that involve matrices and vectors, known as linear
algebra expressions, are commonly evaluated through a sequence of
invocations to highly optimised kernels provided in libraries such as
BLAS and LAPACK. A sequence of kernels represents an algorithm,
and in general, because of associativity, algebraic identities, and
multiple kernels, one expression can be evaluated via many different
algorithms. These algorithms are all mathematically equivalent
(i.e., in exact arithmetic, they all compute the same result), but
often differ noticeably in terms of execution time. When faced
with a decision, high-level languages, libraries, and tools such as
Julia, Armadillo, and Linnea choose by selecting the algorithm that
minimises the FLOP count. In this paper, we test the validity of the
FLOP count as a discriminant for dense linear algebra algorithms,
analysing "anomalies": problem instances for which the fastest
algorithm does not perform the least number of FLOPs.

To do so, we focused on relatively simple expressions and anal-
ysed when and why anomalies occurred. We found that anomalies
exist and tend to cluster into large contiguous regions. For one
expression anomalies were rare, whereas for the other they were
abundant. We conclude that FLOPs is not a sufficiently dependable
discriminant even when building algorithms with highly optimised
kernels. Plus, most of the anomalies remained as such even af-
ter filtering out the inter-kernel cache effects. We conjecture that
combining FLOP counts with kernel performance models will sig-
nificantly improve our ability to choose optimal algorithms.

CCS CONCEPTS
• Mathematics of computing → Mathematical software; Math-
ematical software performance; • Computing methodologies
→ Linear algebra algorithms.

KEYWORDS
linear algebra, algorithm selection, scientific computing

ACM Reference Format:
Francisco López, Lars Karlsson, and Paolo Bientinesi. 2022. FLOPs as a
Discriminant for Dense Linear Algebra Algorithms. In 51st International
Conference on Parallel Processing (ICPP ’22), August 29-September 1, 2022,
Bordeaux, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3545008.3545072

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ICPP ’22, August 29-September 1, 2022, Bordeaux, France
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9733-9/22/08.
https://doi.org/10.1145/3545008.3545072

1 INTRODUCTION
Operations that manipulate matrices and vectors are known as lin-
ear algebra expressions. These expressions, such as (𝑋𝑇 𝑋 )−1𝑋𝑇 𝑦,
are the cornerstone of countless applications and programs in both
science and industry. Examples of convoluted linear algebra ex-
pressions can be found in, e.g., image restoration [36], information
theory [2, 22], and signal processing [10, 32]. Oftentimes, the time
to evaluate these expressions can have a substantial impact on the
overall performance of the application [19]. For this reason, the
problem of how to translate linear algebra expressions into efficient
code is central in HPC [1]. The code to evaluate an expression is
not unique. In fact, there might be a myriad of mathematically
equivalent algorithms. This paper is a step towards improving the
selection of the fastest algorithm for a given expression.

The manual translation of an expression into code is a task that
requires thorough knowledge of both numerical linear algebra and
the target computer architecture. The complexity quickly multi-
plies when the code is to be portable to different architectures or
when the sizes of the operands are unknown at the time of trans-
lation. Over the past several decades, the numerical linear algebra
community has invested remarkable effort into identifying and
optimising a relatively small set of basic kernels that can be used
as building blocks when crafting code to evaluate more complex
linear algebra expressions. The BLAS (Basic Linear Algebra Sub-
programs) [11, 12, 26] and LAPACK (Linear Algebra PACKage) [3]
libraries are two prominent results of this effort. The numerous
alternative implementations of the BLAS and LAPACK collectively
provide high performance on a multitude of different computer ar-
chitectures, ranging from single-core to shared-memory multicore
systems, and to accelerators such as GPUs.

The availability of such portable high-performance libraries
shifts the problem of translating an expression into code away
from optimising architecture-specific code into mapping the ex-
pression to a sequence of kernel calls. These sequences of kernel
calls, which might include bits between calls to transform data
structures, is what we will henceforth refer to as algorithms. The
problem of translating an expression to an algorithm while min-
imising a cost function is known as the Linear Algebra Mapping
Problem (LAMP) [30]. However, even the task of enumerating the
set of mathematically equivalent algorithms for a given expres-
sion can be quite complex. As an example, consider the following
expression used in signal processing [10]:

𝑥 :=

(cid:16)
𝐴−𝑇 𝐵𝑇 𝐵𝐴−1 + 𝑅𝑇 𝐿𝑅

(cid:17)−1

𝐴−𝑇 𝐵𝑇 𝐵𝐴−1𝑦.

Tools such as Linnea [4] reveal that there are hundreds of algorithms
that evaluate this expression by a sequence of calls to kernels in
BLAS and LAPACK. These algorithms, while mathematically equiv-
alent, can and often do have different FLOP counts and execution

 
 
 
 
 
 
ICPP ’22, August 29-September 1, 2022, Bordeaux, France

López, Karlsson, and Bientinesi

times. The problem, then, boils down to selecting an algorithm
that is reliably among the fastest ones from this set for a specific
combination of library implementations and computer architecture.
Many widely-used programming languages face this problem and
advances in algorithm selection will benefit their users.

Since the execution time is not known until after the computation
has been performed, and accurate performance prediction remains
a challenging problem [23, 28] (even more so for sequences of
kernel calls [29]) we are forced to make educated guesses based on
proxies related to the execution time. A simple and natural strategy
is to select one of the cheapest algorithms, i.e., an algorithm with
the minimum floating point operation (FLOP) count. In fact, this
strategy is currently used by the tool Linnea [4], and the library
Armadillo [33] as well as compilers for the programming language
Julia are also known to employ this FLOP count strategy for certain
types of linear algebra expressions.

Selecting algorithms based on FLOP count is likely good enough
to avoid very costly algorithms. For example, if 𝐴 is an 𝑛 × 𝑛 ma-
trix and 𝑥, 𝑦 are vectors of length 𝑛, then the algorithm implied
by the parenthesisation (𝑥𝑦𝑇 )𝐴 involves a vector outer product
costing 𝑛2 FLOPs and a matrix multiplication costing 2𝑛3 FLOPs.
The algorithm implied by 𝑥 (𝑦𝑇 𝐴), on the other hand, involves two
matrix-vector multiplications costing 2𝑛2 FLOPs each. However,
when the difference in FLOP count between algorithms is closer to
zero, the performance (FLOP/s) of the various kernels as well as
caching effects between kernel calls progressively gain importance.
We refer to instances for which one of the cheapest algorithms
is not one of the fastest algorithms as anomalies. While it is well-
known that anomalies exist, less is known about how abundant
anomalies are and which key factors explain their presence.

In this paper, we experimentally study the abundance and distri-
bution of anomalies in expressions of the form 𝐴𝐵𝐶𝐷 and 𝐴𝐴𝑇 𝐵
and seek to explain their presence. Being able to identify regions of
the problem space where one can and cannot rely on FLOPs as a
discriminant is especially important when some matrix sizes are
unknown. This work is an initial investigation in this direction. The
expression 𝐴𝐵𝐶𝐷, known as "matrix chain", originates six distinct
algorithms that all rely on only one BLAS kernel (GEMM); this
kernel is unquestionably the most studied and optimised numerical
kernel in any area of computing. We conjecture that if anomalies
can be found in this expression, then they will be even more abun-
dant in more complex expressions whose algorithms use multiple
kernels. The expression 𝐴𝐴𝑇 𝐵 was chosen since it is one of the
simplest operations with algorithms constructed from three differ-
ent BLAS kernels (GEMM, SYRK, SYMM). This paper makes the
following contributions.

• We show that anomalies occur even when selecting algo-
rithms built from optimised libraries for relatively simple
expressions regardless of the combination of library imple-
mentation and computing platform.

• We experimentally investigate both the abundance and sever-
ity of anomalies. In doing so, we present empirical evidence
that anomalies can be both abundant (≈ 10%) and severe
(45% more FLOPs but 40% lower execution time) even for
simple expressions such as 𝐴𝐴𝑇 𝐵.

• We present empirical evidence that the interplay between
the shapes of the performance profiles of the kernels is a
major explanation of anomalies.

• We present empirical evidence that FLOP counts combined
with performance profiles of kernels may be able to predict
a large fraction of anomalies.

• With this study, we make a necessary step towards solving

the LAMP with symbolic sizes.

The remainder of the paper is organised as follows. Related work
is described in Section 2. In Section 3, we describe the experimen-
tal methodology. The results of the experiments are presented in
Section 4. Finally, conclusions and future work are described in
Section 5.

2 RELATED WORK
Before the introduction of CPU caches, the cost of a numerical
algorithm was effectively proportional to the FLOP count — op-
timising execution time meant minimising FLOP count. This is a
major reason why significant effort has been put into developing
algorithms that minimise the FLOP count (e.g., [9, 35, 37, 38]). By
introducing CPU caches, the paradigm to reduce execution time
radically changed. The execution time for an operation not only de-
pends on the FLOP count but also on the location of the operands in
the memory hierarchy and how these are accessed [13, 14]. When
algorithms are tailored to modern computer architectures with
deep memory hierarchies, the importance of also optimising for
caching becomes apparent. Indeed, there are many examples where
an increase in FLOP count results in a decrease in execution time
(e.g., [6–8, 25]).

We stress that the FLOP count may in principle be a good dis-
criminant between algorithms without being an accurate predictor
of execution time. Indeed, being able to select a fastest algorithm
from a finite set of algorithms does not imply an ability to accurately
predict any one algorithm’s execution time [34].

Algorithm selection followed by automatic code generation has
been heavily applied in the area of linear signal transforms (such as
FFT), which are very specific linear algebra expressions. FFTW [17]
is a widely used library for fast transforms that uses this approach.
The library generates many different codelets that it combines into
many different plans (algorithms). For a given instance, the library
estimates the execution time of plans through empirical perfor-
mance testing. SPIRAL [16] is a program generation software for
linear transforms and other mathematical functions. For each trans-
form there is a set of rules for manipulating the input, resulting in
a set of algorithms. The algorithm selection is determined by a cost
function, which can be FLOP count, accuracy, instruction count,
or execution time (default). The execution time is determined by
empirical performance testing.

Algorithm selection and automatic code generation has also, but
to a lesser extent, been used to solve the LAMP [30]. The Trans-
for [18] Maple package is one of the earliest attempts to translate
linear algebra expressions into BLAS kernel calls. The translation
process generates a single algorithm for a given input expression
based on a set of handcrafted rules. Hence, there is no proper algo-
rithm selection phase. The selection is implicit in the design of the
rules and the code generator. The rules were primarily designed to

FLOPs as a Discriminant for Dense Linear Algebra Algorithms

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

limit the amount of auxiliary memory. The work in [27] presents a
source-to-source compiler that converts linear algebra expressions
in Octave into calls to library functions (e.g., BLAS kernels). The
code is first flattened into a sequence of primitive Octave operations.
Then individual statements are, whenever possible, translated into
an equivalent library function call. In cases where the mapping
from operation to library function is not unique, the best choice for
the target platform is made on the basis of empirical performance
testing done ahead of time. Linnea [4] is a more recent attempt
to solve the LAMP. This tool targets both BLAS and LAPACK and
outputs Julia code. Linnea generates algorithms by rewriting the
input expression and identifying parts of it that are computable by
BLAS/LAPACK kernels. The algorithm selection is done by minimis-
ing the FLOP count with a best-first search. High-level languages
and libraries (e.g., [5, 15, 20, 21, 24, 31, 33]) also face the LAMP. Some
have been shown to generate solutions to the LAMP that are worse
than the solution obtained by minimising the FLOP count [30].

3 METHODOLOGY
We aim to demonstrate the existence of anomalies, estimate their
abundance, quantify their severity, and identify major factors that
explain their presence. The experimental study of anomalies is
complicated, since their presence clearly depends on the specific
expression, the set of algorithms considered for the expression, and
the computer system (hardware, software, and configuration) —
each with infinitely many options. Moreover, the set of instances
for a given expression is also infinite.

The algorithms in this paper make use of three BLAS kernels,
which we briefly introduce and give an estimated FLOP count of
in Section 3.1. The expressions we experiment on and the set of
algorithms we consider for each are introduced in Section 3.2. The
procedure for classifying instances as anomalies is described in
Section 3.3 along with two scores we use to measure their severity.
The design of the experiments is described in Section 3.4.

3.1 Kernels and their FLOP Counts
The algorithms are constructed almost exclusively from calls to
these three kernels in the BLAS library:

• The GEMM kernel (as used here) computes matrix products
of the form 𝐴𝐵. If the size of 𝐴 is 𝑚 × 𝑘 and 𝐵 is 𝑘 × 𝑛, then
we take the FLOP count to be 2𝑚𝑛𝑘.

• The SYRK kernel (as used here) computes one triangle of
symmetric matrix products of the form 𝐴𝐴𝑇 . If the size of 𝐴
is 𝑚 × 𝑘, then we take the FLOP count to be (𝑚 + 1)𝑚𝑘.
• The SYMM kernel (as used here) computes matrix products
of the form 𝐴𝐵 where 𝐴 is symmetric. If the size of 𝐴 is 𝑚 ×𝑚
and 𝐵 is 𝑚 × 𝑛, then we take the FLOP count to be 2𝑚2𝑛.

We do not consider algorithms that replace a call to a level 3 BLAS
kernel with an equivalent sequence of calls to lower-level BLAS
kernels, since performance will degrade if the BLAS library is rea-
sonably optimised.

Efficiency (of a kernel or algorithm) is defined as the ratio of
the measured performance to the computer’s theoretical peak per-
formance. In Figure 1, we show the efficiencies of the kernels as
a function of the size of the input operands for the case where

all operands are square matrices. The differences are small but
noticeable.

Figure 1: Efficiency of the BLAS kernels GEMM, SYRK, and
SYMM as the size of the operands (all square matrices)
grows.

3.2 Expressions and their Algorithms

Figure 2: Illustration of the matrix chain 𝐴𝐵𝐶𝐷.

3.2.1 The Matrix Chain Expression. The matrix chain expression
we consider takes the form

𝑋 := 𝐴𝐵𝐶𝐷,
where 𝐴 ∈ R𝑑0×𝑑1 , 𝐵 ∈ R𝑑1×𝑑2 , 𝐶 ∈ R𝑑2×𝑑3 , and 𝐷 ∈ R𝑑3×𝑑4
(see Figure 2). The matrices are assumed to be dense and unstruc-
tured. Hence, only their sizes (not their elements) affect perfor-
mance. An instance of this expression is specified by the tuple
(𝑑0, 𝑑1, 𝑑2, 𝑑3, 𝑑4).

Figure 3: The algorithms for the matrix chain expression.
Each path from the root node (top) to a leaf node represents
an algorithm.

050010001500200025003000Size (m=k=n)0.00.20.40.60.81.0EfficiencygemmsyrksymmBACDX=d0d1d1d3d3d4d2d2d0d4X = A B C DX = AM1DABX = ABM1X = M1CDBM1 AB M1DAM1 CD M1CX = M2DX = M1M2X = M2DX = AM2X = M2M1X = AM2M2 DM1M2M2D AM2M2M1AM2 BCCDComputedRest of expressionICPP ’22, August 29-September 1, 2022, Bordeaux, France

López, Karlsson, and Bientinesi

The set of algorithms for the matrix chain expression is taken to
be all (reasonable) sequences of calls to the BLAS kernel GEMM that
evaluate the expression. The expression has three matrix multipli-
cations and each GEMM performs one of them. Due to associativity,
the multiplications can be done in any order. It follows that there
are 3! = 6 different but mathematically equivalent algorithms, all
illustrated in Figure 3. Specifically, the algorithms and their FLOP
counts are as follows.

• Algorithm 1: 𝑀1 := 𝐴𝐵; 𝑀2 := 𝑀1𝐶; 𝑀2𝐷 with a FLOP

count of 2𝑑0 (𝑑1𝑑2 + 𝑑2𝑑3 + 𝑑3𝑑4).

• Algorithm 2: 𝑀1 := 𝐴𝐵; 𝑀2 := 𝐶𝐷; 𝑀1𝑀2 with a FLOP

count of 2𝑑2 (𝑑0𝑑1 + 𝑑0𝑑4 + 𝑑3𝑑4).

• Algorithm 3: 𝑀1 := 𝐵𝐶; 𝑀2 := 𝐴𝑀1; 𝑀2𝐷 with a FLOP

count of 2𝑑3 (𝑑0𝑑1 + 𝑑0𝑑4 + 𝑑1𝑑2).

• Algorithm 4: 𝑀1 := 𝐵𝐶; 𝑀2 := 𝑀1𝐷; 𝐴𝑀2 with a FLOP

count of 2𝑑1 (𝑑0𝑑4 + 𝑑2𝑑3 + 𝑑3𝑑4).

• Algorithm 5: 𝑀1 := 𝐶𝐷; 𝑀2 := 𝐴𝐵; 𝑀2𝑀1 with a FLOP
count of 2𝑑2 (𝑑0𝑑1 + 𝑑0𝑑4 + 𝑑3𝑑4), same as Algorithm 2.
• Algorithm 6: 𝑀1 := 𝐶𝐷; 𝑀2 := 𝐵𝑀1; 𝐴𝑀2 with a FLOP

count of 2𝑑4 (𝑑0𝑑1 + 𝑑1𝑑2 + 𝑑2𝑑3).

Figure 5: The algorithms for the expression 𝐴𝐴𝑇 𝐵. Each path
from the root node (top) to a leaf node represents an algo-
rithm.

• Algorithm 1: SYRK for 𝑀 := 𝐴𝐴𝑇 followed by SYMM for
𝑀𝐵. The FLOP count is taken to be 𝑑0 ((𝑑0 + 1)𝑑1 + 2𝑑0𝑑2).
• Algorithm 2: SYRK for 𝑀 := 𝐴𝐴𝑇 followed by GEMM for
𝑀𝐵. The triangle computed by SYRK is copied to form a full
matrix before the GEMM. The FLOP count is taken to be
𝑑0 ((𝑑0 + 1)𝑑1 + 2𝑑0𝑑2), the same as for Algorithm 1.

• Algorithm 3: GEMM for 𝑀 := 𝐴𝐴𝑇 followed by SYMM for

𝑀𝐵. The FLOP count is taken to be 2𝑑2

0 (𝑑1 + 𝑑2).

• Algorithm 4: GEMM for 𝑀 := 𝐴𝐴𝑇 followed by GEMM for
0 (𝑑1 + 𝑑2), the same

𝑀𝐵. The FLOP count is taken to be 2𝑑2
as for Algorithm 3.

Figure 4: Illustration of the expression 𝐴𝐴𝑇 𝐵.

3.2.2 The Expression 𝐴𝐴𝑇 𝐵. The second expression takes the form
𝑋 := 𝐴𝐴𝑇 𝐵,
where 𝐴 ∈ R𝑑0×𝑑1 and 𝐵 ∈ R𝑑0×𝑑2 . Since the matrices are as-
sumed dense and unstructured, only the sizes of 𝐴 and 𝐵 (not their
elements) affect performance. An instance of this expression is
specified by the tuple (𝑑0, 𝑑1, 𝑑2).

We take the set of algorithms to be all (reasonable) combinations
of the GEMM, SYRK, and SYMM kernels in the BLAS. Each kernel
performs one of the two matrix multiplications in the expression,
so each algorithm has two kernel calls. There are 2! = 2 ways to
order the multiplications, but, as we will see, the choice of kernels
gives rise to five different algorithms.

Suppose that 𝑀 := 𝐴𝐴𝑇 is done first. There are two kernels
to choose from for each multiplication, resulting in four different
algorithms. Since 𝑀 is symmetric, either GEMM or SYRK can be
used. If SYRK is used, then only one triangle (the upper or lower) of
𝑀 is available. Regardless, the subsequent multiplication 𝑀𝐵 can
be done using either GEMM or SYMM. If GEMM is used, then the
triangle computed by SYRK must first be extended to a full matrix
by copying its elements to fill in the missing triangle.

Now suppose that 𝑀 := 𝐴𝑇 𝐵 is done first. Only GEMM can be
used. The subsequent multiplication with 𝐴 must also use GEMM.
Hence, there is only one algorithm in this case.

All in all, there are five algorithms, all illustrated in Figure 5. The

algorithms and their FLOP counts are as follows.

• Algorithm 5: GEMM for 𝑀 := 𝐴𝑇 𝐵 followed by GEMM for

𝐴𝑀. The FLOP count is taken to be 4𝑑0𝑑1𝑑2.

3.3 Anomaly Classification
The algorithms with the shortest execution time for the instance are
labelled as the fastest algorithms. The algorithms with the lowest
FLOP count for the instance are labelled as the cheapest algorithms.
An instance of an expression is classified as an anomaly if these two
sets are disjoint, i.e., none of the cheapest algorithms are among
the fastest.

To quantify the severity of an anomaly, we calculate two scores
for each anomaly: time score and FLOP score. The time score is
defined as

𝑇cheapest − 𝑇fastest
𝑇cheapest

∈ [0%, 100%],

where 𝑇cheapest is the shortest execution time amongst the set
of cheapest algorithms and 𝑇fastest is the shortest execution time
amongst all algorithms. A time score of 𝑥% means that the fastest
algorithm has an 𝑥% shorter execution time than that of the fastest
amongst the cheapest algorithms. If an instance is an anomaly, then
𝑥 > 0% and otherwise, 𝑥 = 0%. The larger the time score, the more
severe the anomaly is considered to be.

The FLOP score is defined as

𝐹fastest − 𝐹cheapest
𝐹fastest

∈ [0%, 100%],

where 𝐹cheapest is the FLOP count of the cheapest algorithms and
𝐹fastest is the FLOP count of the cheapest amongst the fastest

BAX=d0d1d0d2d0d2 ATd1d0X = A ATBX = A M1SYRK(A AT)GEMM(ATB)GEMM(A AT)X = M1 BComputedRest of expressionSYMM(M1 B)GEMM(M1 B)GEMM(A M1)FLOPs as a Discriminant for Dense Linear Algebra Algorithms

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

algorithms. A FLOP score of 𝑥% means that the cheapest algorithms
perform 𝑥% fewer FLOPs than the fastest algorithm.

In practice, the fastest of the cheapest algorithms could poten-
tially be just barely slower than the fastest algorithm overall and,
thus, the instance would be classified as an anomaly even though
this would hardly be a significant distinction in practice. Therefore,
in all experiments we require a time score above a certain threshold
(usually 10%; see Section 4) before classifying an instance as an
anomaly.

3.4 Experiments
Intuitively, anomalies should be expected to cluster together into
contiguous regions since execution times of nearby instances tend
to be similar due to the following observations. First, the FLOP
counts of the kernels are continuous functions of the sizes of the
matrices (see Section 3.1). Second, a small change in size in any
operand of a kernel call typically induces a small change in the
performance of said kernel (see Figure 1). The execution time for
the instance (𝑑0, 𝑑1, 𝑑2) is thus expected (but not guaranteed) to be
similar to the execution time for nearby instances.

We carried out three experiments on both expressions using dou-
ble precision (64 bit) arithmetic. The first experiment (Section 3.4.1)
is a random search for anomalies to estimate their abundance and
severity. The second experiment (Section 3.4.2) explores the vicinity
of the anomalies from the first experiment to determine to what
extent anomalies cluster into contiguous regions. The third experi-
ment (Section 3.4.3) aims to determine how many of the anomalies
found in the second experiment could have been predicted from
performance profiles of the three kernels obtained via separated
benchmarks.

All experiments were conducted on a Linux-based system with
40 GB of RAM and an Intel Xeon Silver 4210 processor, which con-
tains ten physical cores. Turbo Boost was enabled. All ten physical
cores were used, and threads were pinned to ensure one thread per
physical core. The source code, available on GitHub1, was compiled
with GCC2 and linked against the Intel Math Kernel Library3.

To reduce the impact of measurement noise, each test (of a spe-
cific algorithm on a specific instance) was repeated ten times and
the median was recorded as the execution time. To eliminate cache
effects, the cache was flushed prior to each repetition.

3.4.1 Experiment 1: Random search. The first experiment searches
for anomalies using random search. Instances are repeatedly sam-
pled uniformly at random with replacement from a specified search
space (see Section 4). All algorithms are tested on the instance and
the instance is then classified as an anomaly or not (see Section 3.3).
If the instance is an anomaly, then its time score and FLOP score
are recorded.

3.4.2 Experiment 2: Lines through regions. Mapping entire regions
proved to be computationally infeasible. Instead, in this experiment,
the regions around all the anomalies found in Experiment 1 are
intersected with axis-aligned lines in all dimensions through the
original anomaly. For example, if (𝑑0, 𝑑1, 𝑑2) is an anomaly from

1https://github.com/FranLS7/LAMB
2GCC version 7.5.0
3MKL version 2019.0.5

Experiment 1, then the three axis-aligned lines are: (𝑑0±10𝑥, 𝑑1, 𝑑2)
and (𝑑0, 𝑑1 ± 10𝑥, 𝑑2) and (𝑑0, 𝑑1, 𝑑2 ± 10𝑥) for 𝑥 = 0, 1, 2, . . .. The
line is traversed (in steps of 10 to make the experiment feasible) in
both directions from the original anomaly and stops either when
the boundary of the search space is reached or some distance passed
the end of the region.

Transient changes in performance, system jitter, and measure-
ment noise might cause an isolated instance inside a region to be
classified as not an anomaly. One or two consecutive instances
classified as not anomalies are considered to be a hole in the region
rather than marking the end of it. Thus, the end of a region is char-
acterised by three or more consecutive instances classified as not
anomalies. The first of those three is labelled as the boundary of the
region. If the traversal reaches the boundary of the search space,
then the last instance is labelled as the boundary of region. Let 𝑎
and 𝑏 (with 𝑎 < 𝑏) be the location of the boundary points of the
region along the line. Then 𝑏 − 𝑎 − 1 is referred to as the thickness
of the region in the dimension of the line.

3.4.3 Experiment 3: Prediction from benchmarks. The performance
of an algorithm consisting of a sequence of kernel calls is deter-
mined by the FLOP counts and the observed performance of the
individual calls. Since we flush the cache before each execution,
the performance of the first call is expected to be close to the ideal
(benchmarked) performance of the kernel. However, the state of
the cache when exiting one call may affect the performance of the
next and subsequent calls.

The third experiment determines to what extent anomalies could
have been predicted from benchmarked performance profiles of
the kernels. For each sample in Experiment 2, the algorithms collec-
tively generate a small set of specific calls. In this third experiment,
all of these calls are benchmarked in isolation with a flushed cache.
Thus, for each call made in Experiment 2, we obtain a benchmarked
execution time. By summing over the calls in any given algorithm,
we obtain a prediction of the execution time of the algorithm based
on benchmarked performance of the kernels.

Figure 6: Scatter plot of the time score versus FLOP score
for 100 anomalies of the matrix chain expression found in
Experiment 1.

4 RESULTS
Section 4.1 presents results for the matrix chain expression 𝐴𝐵𝐶𝐷,
and Section 4.2 presents corresponding results for the expression
𝐴𝐴𝑇 𝐵.

0.00.10.20.30.40.5FLOP score0.000.050.100.150.200.250.300.350.40Time scoreICPP ’22, August 29-September 1, 2022, Bordeaux, France

López, Karlsson, and Bientinesi

Figure 7: The distribution of the thicknesses of the regions around the 100 anomalies found in Experiment 1 on the matrix
chain expression in each dimension from 𝑑0 (left) to 𝑑4 (right).

4.1 The Matrix Chain Expression
4.1.1 Abundance. The search space for Experiment 1 was bounded
by 20 ≤ 𝑑𝑖 ≤ 1200 for 𝑖 = 0, 1, . . . , 4. The upper limit was chosen so
that anomalies were plausible since GEMM’s performance usually
plateaus when the dimensions sizes exceed this limit (and therefore
no anomalies could be found there). The search continued until
100 distinct anomalies were found. The time score threshold (see
Section 3.3) was set to 10%.

Figure 6 shows the result of the experiment. Finding 100 anom-
alies required 22,962 samples, which translates to an estimated
abundance of 0.4% in the constrained search space. Conversely, a
cheapest algorithm is either the fastest or within 10% of the fastest
in 99.6% of instances. Most anomalies have a FLOP score below
10% and a time score below 20% and hence most of the (already
rare) anomalies are not particularly severe. On the other hand,
there are cases where an algorithm performing barely more than
the minimum number of FLOPs is 35% faster than the cheapest
algorithms.

4.1.2 Regions. The time score threshold for Experiment 2 was set
at 5%. Figure 7 shows the distribution of the thicknesses of the
anomalous regions in each of the five dimensions 𝑑0 (left) through
𝑑4 (right). The maximum thickness is close to 1181 since that is the
number of instances in a single line from 20 to 1200.

4.1.3 Region Boundaries. The data from Experiment 2 allows for
inspection of what happens near the boundaries of the regions.
Figure 8 shows how the efficiency of the six algorithms varies along
one of the lines for two of the anomalies found in Experiment 1.
When a line crosses a region boundary, either the efficiency of one
or more algorithms abruptly changes or the efficiencies of all algo-
rithms change non-abruptly. Abrupt changes in kernel efficiency
are likely caused by an internal change in the underlying chosen
algorithmic variant. These two types of transitions are complemen-
tary and hence there is no third type.

The two examples in Figure 8 serve to illustrate the two types
of transitions. Each of the six rows corresponds to an algorithm.
In addition to the efficiency of the whole algorithm (solid blue),
we plot the efficiency of the three calls to GEMM (other colours)
involved in each algorithm. The background colour indicates for
each sample along the line i) in red a cheapest algorithm, ii) in green

a fastest algorithm, and iii) in brown a cheapest and fastest algo-
rithm. Therefore, the segments with brown background colour are
not anomalous, whereas those with green and red are anomalous.
The region boundaries are marked with vertical red dashed lines.
The region’s extension in the explored dimension is marked with a
black line on the horizontal axis (coinciding with the anomalous
areas – where no algorithm presents a brown background).

Consider the anomaly in the left column. As 𝑑4 increases along
the line, Algorithm 6 is both the cheapest and the fastest up until
approximately 300. For larger values of 𝑑4, Algorithm 6 is still
the cheapest but Algorithm 4 is the fastest. When 𝑑4 passes 410,
Algorithms 2 and 5 (same FLOP count) become the cheapest, but
Algorithm 4 remains the fastest. When 𝑑4 passes 700, Algorithm 4
becomes both fastest and cheapest, marking the end of the region.
Now consider the anomaly in the right column. For small values
of 𝑑3, Algorithm 4 is both cheapest and fastest, until 𝑑3 reaches 900
and the region starts. For larger values of 𝑑3, Algorithm 4 continues
to be the fastest but Algorithm 6 becomes the cheapest.

The transition at the boundary of an anomalous region is some-
times due to an abrupt change in efficiency in one or more kernels.
The transition near 300 in the left column of Figure 8 is an example
of this type of transition. If there is no abrupt change in efficiency,
then the transition is more gradual. The transition near 700 in the
left column and the transition near 900 in the right column are
both examples of this type of transition.

4.1.4 Prediction from Benchmarks. The time score threshold for
Experiment 3 was set to 5%. For all instances sampled along a line,
the anomaly classification derived from the measured execution
time (Experiment 2) is taken as ground truth and the benchmark
data (Experiment 3) is used to predict anomalies for the same set
of instances. Thus, each instance falls into one of four categories
(actual anomaly yes/no and predicted anomaly yes/no). The result
of Experiment 3 is shown in the form of a confusion matrix in
Table 1. Approximately 92% of the anomalies could have been
predicted from only the benchmark data, and 96% of the anomalies
predicted by the benchmark data were actual anomalies.

4.2 The Expression 𝐴𝐴𝑇 𝐵
4.2.1 Abundance. The time score threshold for Experiment 1 was
set to 10% (see Section 3.3) and the search continued until 1,000

0255075100020040060080010001200d00255075100d10255075100d20255075100d30255075100d40.00.20.40.60.81.0Sorted randomly found anomalies0.00.20.40.60.81.0Region thicknessFLOPs as a Discriminant for Dense Linear Algebra Algorithms

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Predicted

No

7,202

Yes

656

Total

7,858

1,290

15,839

17,129

l No
a
u
t
c
A

Yes

Total

8,492

16,495

24,987

Table 1: Confusion matrix for prediction of anomalies for
the matrix chain expression from benchmark data.

to an abundance of 9.7% in the constrained search space. In total,
39.2% of the anomalies present a time score above 20% or a FLOP
score above 30%. In some extreme instances, performing 45% more
FLOPs reduces the execution time by 40%.

Figure 9: Scatter plot of the time score versus FLOP score for
1,000 anomalies of 𝐴𝐴𝑇 𝐵.

4.2.2 Regions. Figure 10 shows the distribution of the thickness of
the regions in each of the three dimensions derived using data from
Experiment 2. The regions are significantly thinner in dimension
𝑑0 compared to the other dimensions.

Figure 8: Two examples (left and right) of efficiencies in
anomalous regions demonstrating the two types of tran-
sitions. Each row corresponds to one of the six algo-
rithms; the columns correspond to a different anomaly
(and dimension traversed). Left: Efficiency along the line
(331, 279, 338, 854, 427 ± 10𝑥), dimension 𝑑4 being traversed.
Right: Efficiency along the line (320, 172, 293, 919 ± 10𝑥, 284),
dimension 𝑑3 being traversed.

anomalies were found. The search was restricted to the box 20 ≤
𝑑𝑖 ≤ 1200 for 𝑖 = 0, 1, 2. Figure 9 shows the results in the form of
a scatter plot of time scores versus FLOP scores. A total of 10,258
samples were required to find 1,000 anomalies, which translates

Figure 10: Distribution of the thickness of the regions
around 1,000 anomalies of 𝐴𝐴𝑇 𝐵 in each of the three dimen-
sions from 𝑑0 (left) to 𝑑2 (right).

4.2.3 Region Boundaries. Figure 11 uses data from Experiment 2 to
illustrate, for three different anomalies, how the efficiencies of the
five algorithms change along one of the dimensions. Each column is
a separate example with its own anomaly and traversed dimension.
The anomalies were picked at random. The rows from top to bottom
correspond to Algorithms 1 through 5. Besides each algorithm’s

0.40.60.81.0TotalFirstSecondThird0.40.60.81.00.40.60.81.00.40.60.81.00.40.60.81.020040060080010000.40.60.81.09001000110012000.00.20.40.60.81.0Dimension size0.00.20.40.60.81.0Efficiency0.00.10.20.30.40.5FLOP score0.00.10.20.30.40.5Time score02505007501000020040060080010001200d002505007501000d102505007501000d20.00.20.40.60.81.0Sorted randomly found anomalies0.00.20.40.60.81.0Region thicknessICPP ’22, August 29-September 1, 2022, Bordeaux, France

López, Karlsson, and Bientinesi

kernels (see Section 3.2.2). The background colour convention is
the same as in Figure 8.

Algorithms 1 and 2 have the same FLOP counts and are the cheap-
est throughout the anomalous regions in Figure 11. Algorithms 3
and 4 also have identical FLOP counts, but they are always strictly
more expensive than Algorithms 1 and 2 (see Section 3.2.2).

For the anomaly in the leftmost column, the line is in dimension
𝑑0. The anomalous region covers 𝑑0 ≤ 290, and Algorithms 3 and 4
take turns being fastest while Algorithms 1 and 2 are cheapest. For
𝑑0 > 290, Algorithm 1 is cheapest and fastest and, therefore, there
is no anomaly here.

In the middle column, the line is in dimension 𝑑1. For values of
𝑑1 close to zero, Algorithm 5 is both cheapest and fastest (barely
visible), meaning there is no anomaly in this segment. The region
spans 𝑑1 ≥ 40 until the boundary of the search space. Entering
the region near 𝑑1 = 40, Algorithms 1 and 2 become cheapest, but
Algorithm 4 is the fastest throughout.

Finally, for the third anomaly in the rightmost column, the line is
in dimension 𝑑2. The region covers the full segment except for tiny
values of 𝑑2. When 𝑑2 is close to zero, Algorithm 5 is both cheapest
and fastest (barely visible). For larger values of 𝑑2, Algorithm 4 is
fastest while Algorithms 1 and 2 remain cheapest.

Similar to the matrix chain expression, these examples also illus-
trate both types of transitions. In other words, some transitions are
due to abrupt changes in efficiency while others are not.

4.2.4 Prediction from Benchmarks. Data from Experiment 3 allows
us to determine how well the anomalies discovered in Experiment 2
could have been predicted from benchmarked performance profiles
of the kernels used in the algorithms. The time score threshold
was set at 5%. The result of the experiment is summarised in the
form of a confusion matrix in Table 2. For this expression, 75% of
the anomalies could have been predicted from the benchmark data
alone, and 98.5% of the predicted anomalies were actual anomalies.

Predicted

l No
a
u
t
c
A

Yes

No

Yes

Total

36,041

2,434

38,475

53,711

160,867

214,578

Total

89,752

163,301

253,053

Figure 11: Three examples (left to right) of efficiencies in
anomalous regions in the three dimensions of 𝐴𝐴𝑇 𝐵. Each
row corresponds to one of the five algorithms. Each column
is a different anomaly and dimension traversed. Left: Effi-
ciency along the line (227 ± 10𝑥, 260, 549), dimension 𝑑0 be-
ing traversed. Center: Efficiency along the line (80, 514 ±
10𝑥, 768), dimension 𝑑1 being traversed. Right: Efficiency
along the line (110, 301, 938 ± 10𝑥), dimension 𝑑2 being tra-
versed.

efficiency, the figure also plots the efficiency of each of the two
kernel calls per algorithm. Recall that the algorithms use different

Table 2: Confusion matrix for prediction of anomalies for
the 𝐴𝐴𝑇 𝐵 expression from benchmark data.

5 CONCLUSIONS AND FUTURE WORK
The process of mapping a linear algebra expression into a sequence
of calls to numerical libraries such as BLAS and LAPACK can be
seen as consisting of two steps. First, generate a set of mathemati-
cally equivalent algorithms, each of which evaluates the expression.
Second, given a concrete instance and computer system, select from
this set one algorithm that is likely to be the fastest and use that
to evaluate the expression. In the simplest case where the sizes
of all operands are known, then, at least in principle, the fastest

0.00.20.40.60.81.0Total1.syrk2.symm0.00.20.40.60.81.0Total1.syrk2.gemm0.00.20.40.60.81.0Total1.gemm2.symm0.00.20.40.60.81.0Total1.gemm2.gemm01002003000.00.20.40.60.81.00500100005001000Total1.gemm2.gemm0.00.20.40.60.81.0Dimension size0.00.20.40.60.81.0EfficiencyFLOPs as a Discriminant for Dense Linear Algebra Algorithms

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

algorithm can be selected through brute force search. As soon as
one or more of the sizes are symbolic at compile time, algorithm
selection must be delayed until run time. Regardless if it is a matter
of efficiency (fixed sizes) or necessity (symbolic sizes), there is value
in procedures for algorithm selection that do not rely on empirical
testing. Systems that address this problem, such as Transfor and
Linnea, use FLOP counts as a discriminant.

In this paper, we experimentally studied how frequently and
why FLOP count fails to be a reliable discriminant when mapping
dense linear algebra expressions to the BLAS. We ran a set of three
experiments (see Section 3.4) on two expressions: The matrix chain
expression 𝐴𝐵𝐶𝐷 and the expression 𝐴𝐴𝑇 𝐵.

We first estimated the abundance of anomalies through random
search (Experiment 1). Firstly, we searched for anomalies in the
matrix chain expression, whose algorithms only involve the kernel
that is regarded as the most highly optimised in BLAS (GEMM).
Even in this limiting case, we found that anomalies do exist al-
though they appear to be rare (0.4%). Secondly, we did the same for
the expression 𝐴𝐴𝑇 𝐵. In comparison with the matrix chain expres-
sion, this one presents fewer algorithms, but those are built from a
more varied set of BLAS kernels. The results from both expressions
exhibit a sheer contrast, since anomalies appeared to be much more
frequent in the second expression (9.7%). Both expressions are very
simple compared to those typically encountered in the wild. That
such simple expressions can still have an abundance of anomalies
leads us to expect that anomalies will be even more frequent in
more complex expression. This is motivated by the fact that large
expressions have many more mathematically equivalent algorithms
and also involve more kernels. These are two factors that one can
reasonably assume will increase the opportunities for anomalies to
occur.

In this study, we have also confirmed that anomalies tend to
cluster together in contiguous regions, some of them extending
throughout the entire range for one or more dimension sizes. The
boundaries of a region sometimes coincide with abrupt changes in
the efficiency of one or more algorithms. Other times the onset of
a region is due to more gradual changes in the kernels’ efficiencies.
We conjecture that knowledge of the location of abrupt changes in
the performance profiles of the kernels will help to localise regions
of severe anomalies. This information might be particularly useful
to solve the LAMP with symbolic sizes.

Finally, we leveraged the data generated when traversing the
regions to investigate the underlying causes of the anomalies. Al-
though we know that an algorithm’s execution time is determined
by the interplay of its FLOP count, the performance of the invoked
kernels, and the inter-kernel caching effects, we sought to deter-
mine their weights in the occurrence of anomalies. In doing so, we
quantified for both expressions how many of these anomalies could
have been predicted solely from the benchmark data. The high
fraction of actual anomalies that could have been predicted from
benchmark data (92% and 75%, for both expressions, respectively)
leads us to conjecture that the kernels performance profiles have a
greater weight on the anomalies.

In conclusion, this work has shown that FLOP count is not an
adequate discriminant even when selecting amongst algorithms
built from optimised libraries for simple expressions such as 𝐴𝐴𝑇 𝐵.

Although our experiments used a specific computer and implemen-
tation of BLAS, the qualitative conclusions are likely to generalise.
A different setup will affect the performance profiles of the ker-
nels, which, in turn, will translate into the disappearance of some
anomalies and the surge of new ones.

A natural next step is to combine FLOP counts with performance
profiles of kernels to develop a methodology for localising regions
of severe anomalies. A second step is to select algorithms based on
more than the FLOP count; in particular, including performance
profiles of kernels. Combined, these steps may lead to a more robust
algorithm selection methodology suitable for complex expressions
or expressions with symbolic sizes. Given that accurate perfor-
mance modelling for sequences of kernels is known to be expensive
and difficult [29], finding ways to reduce the cost and complexity
are expected to be crucial steps towards a practical algorithm selec-
tion methodology. In doing so, we will take another step towards
selecting optimal algorithms to evaluate linear algebra expressions,
and will be closer to squeeze the most out of our computational
capabilities.

REFERENCES
[1] Ahmad Abdelfattah, Timothy Costa, Jack Dongarra, Mark Gates, Azzam Haidar,
Sven Hammarling, Nicholas J Higham, Jakub Kurzak, Piotr Luszczek, Stanimire
Tomov, et al. 2021. A set of batched basic linear algebra subprograms and LAPACK
routines. ACM Transactions on Mathematical Software (TOMS) 47, 3 (2021), 1–23.
[2] Zaid Albataineh and Fathi M Salem. 2014. A Blind Adaptive CDMA Receiver

Based on State Space Structures. arXiv preprint arXiv:1408.0196 (2014).

[3] Edward Anderson, Zhaojun Bai, Christian Bischof, L Susan Blackford, James
Demmel, Jack Dongarra, Jeremy Du Croz, Anne Greenbaum, Sven Hammarling,
Alan McKenney, et al. 1999. LAPACK Users’ guide. SIAM.

[4] Henrik Barthels, Christos Psarras, and Paolo Bientinesi. 2021. Linnea: Automatic
Generation of Efficient Linear Algebra Programs. ACM Trans. Math. Softw. 47, 3,
Article 22 (June 2021), 26 pages. https://doi.org/10.1145/3446632

[5] Jeff Bezanson, Jiahao Chen, Benjamin Chung, Stefan Karpinski, Viral B Shah, Jan
Vitek, and Lionel Zoubritzky. 2018. Julia: Dynamism and performance reconciled
by design. Proceedings of the ACM on Programming Languages 2, OOPSLA (2018),
1–23.

[6] Christian Bischof, Xiaobai Sun, and Bruno Lang. 1994. Parallel tridiagonalization
through two-step band reduction. In Proceedings of IEEE Scalable High Perfor-
mance Computing Conference. IEEE, 23–27.

[7] Christian Bischof and Charles Van Loan. 1987. The WY representation for
products of Householder matrices. SIAM J. Sci. Statist. Comput. 8, 1 (1987),
s2–s13.

[8] Alfredo Buttari, Julien Langou, Jakub Kurzak, and Jack Dongarra. 2008. Parallel
tiled QR factorization for multicore architectures. Concurrency and Computation:
Practice and Experience 20, 13 (2008), 1573–1590.

[9] Inderjit Singh Dhillon. 1997. A new O (N (2)) algorithm for the symmetric tridiag-

onal eigenvalue/eigenvector problem. University of California, Berkeley.
[10] Yin Ding and Ivan W Selesnick. 2016. Sparsity-based correction of exponential

artifacts. Signal Processing 120 (2016), 236–248.

[11] Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J Hanson. 1985.
A proposal for an extended set of Fortran basic linear algebra subprograms. ACM
Signum Newsletter 20, 1 (1985), 2–18.

[12] Jack J Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain S Duff. 1990. A set
of level 3 basic linear algebra subprograms. ACM Transactions on Mathematical
Software (TOMS) 16, 1 (1990), 1–17.

[13] Jack J Dongarra, Fran Goertzel Gustavson, and A Karp. 1984.

Implementing
linear algebra algorithms for dense matrices on a vector pipeline machine. Siam
Review 26, 1 (1984), 91–112.

[14] Jack J Dongarra, Linda Kaufman, and Sven Hammarling. 1986. Squeezing the
most out of eigenvalue solvers on high-performance computers. Linear Algebra
and Its Applications 77 (1986), 113–136.

[15] John W. Eaton, David Bateman, Søren Hauberg, and Rik Wehbring. 2020. GNU
Octave version 5.2.0 manual: a high-level interactive language for numerical com-
putations. https://www.gnu.org/software/octave/doc/v5.2.0/

[16] Franz Franchetti, Tze Meng Low, Doru Thom Popovici, Richard M Veras,
Daniele G Spampinato, Jeremy R Johnson, Markus Püschel, James C Hoe, and
José MF Moura. 2018. SPIRAL: Extreme performance portability. Proc. IEEE 106,
11 (2018), 1935–1968.

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

López, Karlsson, and Bientinesi

[17] Matteo Frigo and Steven G Johnson. 1998. FFTW: An adaptive software archi-
tecture for the FFT. In Proceedings of the 1998 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181), Vol. 3.
IEEE, 1381–1384.

[18] Claude Gomez and Tony Scott. 1998. Maple programs for generating efficient
FORTRAN code for serial and vectorised machines. Computer Physics Communi-
cations 115, 2-3 (1998), 548–562.

[19] Manuel González, Francisco González, Daniel Dopico, and Alberto Luaces. 2008.
On the effect of linear algebra implementations in real-time multibody system
dynamics. Computational Mechanics 41, 4 (2008), 607–615.

[20] Gaël Guennebaud, Benoît Jacob, et al. 2010. Eigen v3. http://eigen.tuxfamily.org.
[21] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers,
Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,
Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van
Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe,
Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren
Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. 2020.
Array programming with NumPy. Nature 585, 7825 (Sept. 2020), 357–362. https:
//doi.org/10.1038/s41586-020-2649-2

[22] Mohsen Hejazi, Seyed Mohammad Azimi-Abarghouyi, Behrooz Makki, Ma-
soumeh Nasiri-Kenari, and Tommy Svensson. 2015. Robust successive compute-
and-forward over multiuser multirelay networks. IEEE Transactions on Vehicular
Technology 65, 10 (2015), 8112–8129.

[23] Roman Iakymchuk and Paolo Bientinesi. 2012. Modeling performance through
memory-stalls. ACM SIGMETRICS Performance Evaluation Review 40, 2 (2012),
86–91.

[24] The MathWorks Inc. 2022. MATLAB. http://www.mathworks.com/.
[25] Bruno Lang. 1998. Using level 3 BLAS in rotation-based algorithms. SIAM Journal

on Scientific Computing 19, 2 (1998), 626–634.

[26] Chuck L Lawson, Richard J. Hanson, David R Kincaid, and Fred T. Krogh. 1979.
Basic linear algebra subprograms for Fortran usage. ACM Transactions on Mathe-
matical Software (TOMS) 5, 3 (1979), 308–323.

[27] Daniel McFarlin and Arun Chauhan. 2007. Library function selection in compiling
Octave. In 2007 IEEE International Parallel and Distributed Processing Symposium.
IEEE, 1–8.

[28] Elmar Peise and Paolo Bientinesi. 2012. Performance modeling for dense linear
algebra. In 2012 SC Companion: High Performance Computing, Networking Storage
and Analysis. IEEE, 406–416.

[29] Elmar Peise and Paolo Bientinesi. 2014. A study on the influence of caching:
Sequences of dense linear algebra kernels. In International Conference on High
Performance Computing for Computational Science. Springer, 245–258.

[30] Christos Psarras, Henrik Barthels, and Paolo Bientinesi. 2019. The Linear Algebra
Mapping Problem. Current state of linear algebra languages and libraries. arXiv
preprint arXiv:1911.09421 (2019).

[31] R Core Team. 2017. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.
org/

[32] Vishwas Rao, Adrian Sandu, Michael Ng, and Elias D Nino-Ruiz. 2017. Robust
Data Assimilation Using L_1 and Huber Norms. SIAM Journal on Scientific
Computing 39, 3 (2017), B548–B570.

[33] Conrad Sanderson and Ryan Curtin. 2016. Armadillo: a template-based C++
library for linear algebra. Journal of Open Source Software 1, 2 (2016), 26.
[34] Aravind Sankaran and Paolo Bientinesi. 2020. Discriminating Equivalent Algo-
rithms via Relative Performance. arXiv preprint arXiv:2010.07226 (2020).
[35] Volker Strassen et al. 1969. Gaussian elimination is not optimal. Numerische

mathematik 13, 4 (1969), 354–356.

[36] Tom Tirer and Raja Giryes. 2018. Image restoration by iterative denoising and
backward projections. IEEE Transactions on Image Processing 28, 3 (2018), 1220–
1234.

[37] Martin Vetterli, Henri J Nussbaumer, et al. 1984. Simple FFT and DCT algorithms
with reduced number of operations. Signal processing 6, 4 (1984), 267–278.
[38] Wei Xu and Sanzheng Qiao. 2008. A fast symmetric SVD algorithm for square

Hankel matrices. Linear Algebra Appl. 428, 2-3 (2008), 550–563.

