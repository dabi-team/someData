2
2
0
2

y
a
M
8

]

G
L
.
s
c
[

1
v
9
4
7
3
0
.
5
0
2
2
:
v
i
X
r
a

GOCPT: Generalized Online Canonical Polyadic Tensor Factorization
and Completion

Chaoqi Yang1 , Cheng Qian2 and Jimeng Sun1∗
1Department of Computer Science, University of Illinois Urbana-Champaign
2Analytics Center of Excellence, IQVIA
1{chaoqiy2, jimeng}@illinois.edu, 2alextoqc@gmail.com

Abstract
Low-rank tensor factorization or completion is
well-studied and applied in various online settings,
such as online tensor factorization (where the tem-
poral mode grows) and online tensor completion
(where incomplete slices arrive gradually). How-
ever, in many real-world settings, tensors may have
more complex evolving patterns: (i) one or more
modes can grow; (ii) missing entries may be ﬁlled;
(iii) existing tensor elements can change. Existing
methods cannot support such complex scenarios.
To ﬁll the gap, this paper proposes a Generalized
Online Canonical Polyadic (CP) Tensor factoriza-
tion and completion framework (named GOCPT)
for this general setting, where we maintain the CP
structure of such dynamic tensors during the evo-
lution. We show that existing online tensor factor-
ization and completion setups can be uniﬁed un-
der the GOCPT framework. Furthermore, we pro-
pose a variant, named GOCPTE, to deal with cases
where historical tensor elements are unavailable
(e.g., privacy protection), which achieves similar
ﬁtness as GOCPT but with much less computational
cost. Experimental results demonstrate that our
GOCPT can improve ﬁtness by up to 2.8% on the
JHU Covid data and 9.2% on a proprietary patient
claim dataset over baselines. Our variant GOCPTE
shows up to 1.2% and 5.5% ﬁtness improvement on
two datasets with about 20% speedup compared to
the best model.

1 Introduction
Streaming tensor data becomes increasingly popular in ar-
eas such as spatio-temporal outlier detection [Najaﬁ et al.,
2019], social media [Song et al., 2017], sensor monitoring
[Mardani et al., 2015], video analysis [Kasai, 2019] and
hyper-order time series [Cai et al., 2015]. The factoriza-
tion/decomposition of such multidimensional structural data
is challenging since they are usually sparse, delayed, and
sometimes incomplete. There is an increasing need to main-
tain the low-rank Tucker [Sun et al., 2006; Xiao et al., 2018;
Nimishakavi et al., 2018; Fang et al., 2021; Gilman and
Balzano, 2020] or CP [Du et al., 2018; Phipps et al., 2021]
structure of the evolving tensors in such dynamics, consider-
ing model efﬁciency and scalability.

Several online (we also use “streaming” interchangeably)
settings have been discussed before. The most popular two
are online tensor decomposition [Zhou et al., 2016; Song et
al., 2017] and online tensor completion [Kasai, 2019], where
the temporal mode grows with new incoming slices. Some
pioneer works have been proposed for these two particular
settings. For the factorization problem, [Nion and Sidiropou-
los, 2009] incrementally tracked the singular value decompo-
sition (SVD) of the unfolded third-order tensors to maintain
the CP factorization results. Accelerated methods have been
proposed for the evolving dense [Zhou et al., 2016] or sparse
[Zhou et al., 2018] tensors by reusing intermediate quantities,
e.g., matricized tensor times Khatri-Rao product (MTTKRP).
For the completion problem, recursive least squares [Kasai,
2016] and stochastic gradient descent (SGD) [Mardani et al.,
2015] were studied to track the evolving subspace of incom-
plete data.

However, most existing methods are designed for the grow-
ing patterns. They cannot support other possible patterns
(e.g., missing entries in the previous tensor can be reﬁlled
or existing values can be updated) or more complex scenarios
where the tensors evolve with a combination of patterns.

Motivating application: Let us consider a public health
surveillance application where data is modeled as a fourth-
order tensor indexed by location (e.g., zip code), disease
(e.g., diagnosis code), data generation date (GD) (i.e., the
date when the clinical events actually occur) and data load-
ing date (LD) (i.e., the time when the events are reported in
the database) and each tensor element stands for the count
of medical claims with particular location-disease-date tu-
ples. The tensor grows over time by adding new locations,
diseases, GD’s, or LD’s. For every new LD, missing en-
tries before that date may be ﬁlled in, and existing entries
can be revised for data correction purposes [Qian et al.,
2021]. Dealing with such a dynamic tensor is challenging,
and very few works have studied this online tensor factor-
ization/completion problem. The most recent work in [Qian
et al., 2021] can only handle a special case with the GD di-
mension growing but not with data correction or two more
dimensions growing simultaneously.

To ﬁll the gap, we propose GOCPT – a general framework
that deals with the above-mentioned online tensor updating
scenarios. The major contributions of the paper are summa-
rized as follows:

• We propose a uniﬁed framework GOCPT for online tensor

 
 
 
 
 
 
factorization/completion with complex evolving patterns
such as mode growth, missing ﬁlling, and value update,
while previous models cannot handle such general settings.
• We propose GOCPTE, i.e., a more memory and computa-
tional efﬁcient version of GOCPT, to deal with cases where
historical tensor elements are unavailable due to limited
storage or privacy-related issues.

• We experimentally show that both GOCPT and GOCPTE
work well under a combination of online tensor challenges.
The GOCPT improves the ﬁtness by up to 2.8% and 9.2%
on real-world Covid and medical claim datasets, respec-
tively. In comparison, GOCPTE provides comparable ﬁt-
ness scores as GOCPT with 20%+ complexity reduction
compared to the baseline methods.

The ﬁrst version of GOCPT package has been released in
PyPI1 and open-sourced in GitHub2. Supplementary of this
paper can be found in the same GitHub repository.

• (i) Mode growth. New (incomplete) slices are added along

one or more modes. Refer to the blue parts.

• (ii) Missing ﬁlling. Some missing values in the old tensor

is received. Refer to the green entries in the ﬁgure.

• (iii) Value update.

Previously observed entries may

change due to new information. Refer to yellow entries.

To track the evolution process, this paper proposes a general
framework for solving the following problem on the ﬂy.

Figure 1: Illustration of Online Tensor Evolution

2 Problem Formulation
Notations. We use plain letters for scalars, e.g., x or X,
boldface uppercase letters for matrices, e.g., X, boldface low-
ercase letters for vectors, e.g., x, and Euler script letters for
tensors or sets, e.g., X . Tensors are multidimensional arrays
indexed by three or more modes. For example, an Nth-order
tensor X is an N -dimensional array of size I1 ×I2 ×· · ·×IN ,
where xi1i2···iN is the element at the (i1, i2, · · · , iN ) location.
∗ for
For a matrix X, the r-th row is denoted by xr. We use
Hadamard product (element-wise product), (cid:12) for Khatri-Rao
(cid:35)
product, and
for Kruskal product (which inputs matrices
and outputs a tensor).

·
(cid:74)

(cid:75)

For an incomplete observation of tensor X , we use a mask
tensor Ω to indicate the observed entries: if xi1i2···iN is ob-
∗ Ω is the
served, then Ωi1i2···iN = 1, otherwise 0. Thus, X
actual observed data. In this paper, Ω can also be viewed as
(cid:35)
an index set of the observed entries. We deﬁne (cid:107)(·)Ω(cid:107)2
F as
the sum of element-wise squares restricted on Ω, e.g.,

(cid:107)(X − Y)Ω(cid:107)2

F ≡

(cid:88)

(xi1···iN − yi1···iN )2,

(i1,··· ,iN )∈Ω

where X and Y may not necessarily be of the same size, but
Ω must index within the bounds of both tensors. We describe
basic matrix/tensor algebra in appendix A, where we also list
a table to summarize all the notations used in the paper.

2.1 Problem Deﬁnition
Modeling Streaming Tensors. Real-world streaming data
comes with indexing features and quantities, for example, we
may receive a set of disease count tuples on a daily basis, e.g.,
(location, disease, date; count), where the ﬁrst three features
can be used to locate in a third-order tensor and the counts
can be viewed as tensor elements.

Formally, a streaming of index-element tuples, e.g., repre-
sented by (i1, · · · , iN ; xi1···iN ), can be modeled as an evolv-
ing tensor structure. This paper considers three typical types
of evolution, shown in Fig. 1.

1https://pypi.org/project/GOCPT/
2https://github.com/ycq091044/GOCPT

(cid:74)

Problem 1 (Online Tensor Factorization and Completion).
Suppose tensor X t−1 ∈ RI t−1
admits a low-
rank approximation Y t−1 at time (t − 1), parametrized by
rank-R factors {An,t−1 ∈ RI t−1
n=1, i.e., Y t−1 =
A1,t−1, · · · , AN,t−1

. Given the mask Ωt−1, it satisﬁes

1 ×···×I t−1

n ×R}N

N

Ωt−1

(cid:75)
∗ X t−1 ≈ Ωt−1
(cid:35)
1×···×I t

∗ Y t−1.
(cid:35)

Following the evolution patterns, given the new underlying
tensor X t ∈ RI t
N and the mask Ωt at time t, our
target is to ﬁnd N new factor matrices An,t ∈ RI t
n×R,
n ≥ I t−1
I t
, so as
to minimize,

n , ∀n and deﬁne Y t =

A1,t, · · · , AN,t

(cid:74)

(cid:75)

(1)

LΩt(X t; Y t) ≡

(cid:88)

l(xt

i1···iN

; yt

i1···iN

),

(2)

(i1,··· ,iN )∈Ωt

where l(·) is an element-wise loss function and the data and
parameters are separated by semicolon.

2.2 Regularization and Approximation
This section expands the objective deﬁned in Eqn. (2) with
two regularization objectives.

• Regularization on Individual Factors. We add a generic
term Lreg({An,t}) to Eqn. (2), and it can be later cus-
tomized based on application background.

• Regularization on the Reconstruction. Usually, from
time (t − 1) to t, the historical part of the tensor will not
change much, and thus we assume that the new reconstruc-
tion Y t, restricted on the bound of previous tensor, will not
change signiﬁcantly from the previous Y t−1.

Lrec(Y t−1; Y t) =

(cid:88)

l(yt−1

i1···iN

; yt

i1···iN

).

1≤in≤I t−1

n ,∀n

Considering these two regularizations, the generalized objec-
tive can be written as,
L = LΩt(X t; Y t)+αLrec(Y t−1; Y t)+βLreg({An,t}), (3)

where α, β are (time-varying) hyperparameters.

In Eqn.

the current

Approximation.
tensor data
(2),
masked by Ωt consists of two parts: (i) old unchanged data
(indicating dark elements in Fig. 1), we denote it by Ωt,old,
which is a subset of Ωt−1; (ii) newly added data (blue part,
green and yellow entries in Fig. 1), denoted by set subtraction
˜Ωt = Ωt \ Ωt,old.

The old unchanged data can be in large size. Sometime,
this part of data may not be entirely preserved due to (i) lim-
ited memory footprint or (ii) privacy related issues. By re-
placing the old tensor with its reconstruction [Song et al.,
2017], we can avoid the access to the old data. Thus, we
consider an approximation for the ﬁrst term of Eqn. (3),

LΩt (X t; Y t) = L ˜Ωt (X t; Y t) + LΩt,old (X t; Y t)

= L ˜Ωt (X t; Y t) + LΩt,old (X t−1; Y t)
≈ L ˜Ωt (X t; Y t) + LΩt,old (Y t−1; Y t)
; yt
= L ˜Ωt (X t; Y t) +

l(yt−1

(cid:88)

i1···iN

i1···iN ).

(4)

Ωt,old

; yt

i1···iN

Ωt,old l(yt−1

In the above derivation, the rationale of the approximation
is the result in Eqn. (1). In Eqn. (4), we ﬁnd that the term
(cid:80)
) is part of the reconstruction regu-
larization Lrec(Y t−1; Y t) and thus can be absorbed. There-
fore, we can use L ˜Ωt(X t; Y t) to replace the full quantity
LΩt (X t; Y t) in Eqn. (3), which results in a more efﬁcient
objective for streaming data processing,

i1···iN

LE = L ˜Ωt (X t; Y t) + αLrec(Y t−1; Y t) + βLreg({An,t}).

(5)

the unchanged part Ωt,old is not
For this new objective,
counted in the ﬁrst term (only the new elements ˜Ωt counted),
however, it is captured implicitly in the second term.

In sum, L and LE are two objectives in our framework.
They have a similar expression but with different access to
the tensor data (i.e., the former with mask Ωt and the latter
with ˜Ωt). Generally, L is more accurate while LE is more
efﬁcient and can be applied to more challenging scenarios.

2.3 Generalized Optimization Algorithm

Structure of Parameters. For two general objectives de-
ﬁned in Eqn. (3) and (5), the parameters {An,t ∈ RI t
n×R}
are constructed by the upper blocks {Un,t ∈ RI t−1
n ×R} and
the lower blocks {Ln,t ∈ R(I t

n )×R}, i.e.,

n−I t−1

An,t =

(cid:21)

(cid:20)Un,t
Ln,t

, ∀ n,

where Un,t corresponds to the old dimensions along tensor
mode-n and Ln,t is for the newly added dimensions of mode-
n due to mode growth. To reduce clutter, we use “factors” to
refer to An,t, and “blocks” for Un,t, Ln,t.

Parameter Initialization. We use the previous factors at
time (t − 1) to initialize the upper blocks, i.e.,

ˆUn,t init← An,t−1, ∀ n.

(6)

Note that, we use hat ˆ to denote the initialized block/factor.

Each lower block is initialized by solving the following ob-

jective, individually, ∀ n,

ˆLn,t init← arg min
Ln,t

(cid:88)

(i1,··· ,iN )∈Ωt

1≤ik≤It−1
It−1
k <ik≤It

k

, k(cid:54)=n
k, k=n

l(xt

i1···iN ; yt

i1···iN ).

(7)

Here, Y t is constructed from the parameters {An,t}, and ik
is the row index of the mode-k factor Ak,t. The summation is
over the indices bounded by the intersection of Ωt and an N -
dim cube, where other N − 1 modes use the old dimensions
and mode-n uses the new dimensions. Thus, this objective
essentially uses { ˆUk,t}k(cid:54)=n to initialize Ln,t.
Parameter Updating. Generally, for any differentiable loss
l(·), e.g, Frobenius norm [Yang et al., 2021] or KL divergence
[Hong et al., 2020], we can apply gradient based methods,
to update the factor matrices. The choices of loss function,
regularization term, optimization method can be customized
based on the applications.

3 GOCPT Framework
3.1 GOCPT Objectives
To instantiate a speciﬁc instance from the general algorithm
formulation, we present GOCPT with
• Squared residual loss: l(x; y) = (x − y)2;
• L2 regularization: Lreg({An,t}) = (cid:80)N
• Alternating least squares optimizer (ALS): updating factor

n=1 (cid:107)An,t(cid:107)2
F ;

matrices in a sequence by ﬁxing other factors.
Then, the general objectives in Eqn. (3) (5) becomes

LE =

(cid:13)
(cid:13)(X t −
A1,t, · · · , AN,t
(cid:13)
(cid:74)
(cid:13)
(cid:13)Y t−1 −
(cid:13)

U1,t, · · · , UN,t
(cid:74)

(cid:13)
2
(cid:13)
(cid:13)

F

) ˜Ωt
(cid:75)

+ α

(cid:13)
(cid:13)
(cid:13)
(cid:75)

2

F

+ β

N
(cid:88)

n=1

(cid:13)An,t(cid:13)
(cid:13)
(cid:13)

2

F

,

(8)

while the form of L is identical but with a different mask Ωt.
Here, LE has only access to the new data {xt
} ˜Ωt but L
has full access to the entire tensor {xt
}Ωt up to time t.

i1···iN

In this objective,

the second term (regularization on
the reconstruction deﬁned in Sec. 2.2) is restricted on
{(i1, · · · , iN ) : 1 ≤ in ≤ I t−1
n , ∀n}, so only the upper
blocks Un,t ∈ RI t−1
n×R, ∀n, are
involved. Note that, L and LE are optimized following the
same procedures.

n ×R of factors An,t ∈ RI t

i1···iN

3.2 GOCPT Optimization Algorithm
For our objectives in Eqn. (8), we outline the optimization
ﬂow: we ﬁrst initialize the factor blocks; next, we update
the upper or lower blocks (by ﬁxing other blocks) following
this order: U1,t, L1,t, · · · , UN,t, LN,t.
Factor Initialization. As mentioned before,
the upper
blocks, {Un,t}N
n=1, are initialized in Eqn. (6). In this speciﬁc
framework, the minimization problem for initializing lower
blocks {Ln,t}N

n=1 (in Eqn. (7)) can be reformed as

arg min
Ln,t

(cid:13)
(cid:13)(X t −
(cid:13)

· · · , ˆUn−1,t, Ln,t, ˆUn+1,t, · · ·
)Ωn,t
(cid:75)
(cid:74)

(cid:13)
(cid:13)
(cid:13)

2

F

,

(9)

where Ωn,t denotes the intersection space in Eqn. (7). The
initialization problem in Eqn. (9) and the targeted objectives
in Eqn. (8) can be consistently handled by the following.
Factor Updating Strategies. Our optimization strategy de-
pends on the density of tensor mask, e.g., ˜Ωt in Eqn. (8).
For an evolving tensor with sparse new updates (for example,
covid disease count tensor, where the whole tensor is sparse
and new disease data or value correction comes irregularly at
random locations), we operate only on the new elements by
sparse strategy, while for tensors with dense updates (for ex-
ample, multiangle imagery tensors are collected real-time and
update slice-by-slice while each slice may have some missing
or distortion values), we ﬁrst impute the tensor to a full ten-
sor, then apply dense operations by our dense strategy. For
example, we solve Eqn. (8) as follows:
• Sparse strategy. If the ˜Ωt (the index set of newly added
data at time t) is sparse, then we extend the CP completion
alternating least squares (CPC-ALS) [Karlsson et al., 2016]
and solve for each row of the factor.
Let us focus on an,t
∈ R1×R, which is the in-th row of
in
factor An,t. To calculate its derivative, we deﬁne the inter-
mediate variables,

Pn,t
in

=

+ α

qn,t
in

=

(cid:88)

((cid:12)k(cid:54)=nak,t
ik

)(cid:62)((cid:12)k(cid:54)=nak,t
ik

)

(i1,··· ,in−1,in+1,··· ,iN )∈ ˜Ωt,in
(cid:16)

Uk,t(cid:17)

∗ k(cid:54)=nUk,t(cid:62)
(cid:35)
(cid:88)

+ βI, ∀n, ∀in,

xt
i1,··· ,iN

((cid:12)k(cid:54)=nak,t
ik

)

(i1,··· ,in−1,in+1,··· ,iN )∈ ˜Ωt,in

+ αan,t−1
in

(cid:16)

∗ k(cid:54)=nAk,t−1(cid:62)
(cid:35)

Uk,t(cid:17)

, ∀n, ∀in.

≡ a1,t
i1

Here, ˜Ωt,in (we slightly abuse the notation) indicates the
in-th slice of mask ˜Ωt along the n-th mode, and
(cid:12)k(cid:54)=nak,t
ik

(cid:12) an+1,t
in+1
∗ . Here, Pn,t
The same convention works for
in
a positive deﬁnite matrix and qn,t
∈ R1×R.
(cid:35)
in
Then, the derivative w.r.t. each row can be expressed as,

(cid:12) · · · (cid:12) an−1,t
in−1

(cid:12) · · · (cid:12) aN,t
iN

∈ RR×R is

.

∂LE
∂an,t
in

= 2an,t
in

Pn,t
in

− 2qn,t
in

, ∀n, ∀in ∈ [1, · · · , I t−1
n ],

n + 1, · · · , I t

and it applies to ∀ in ∈ [I t−1
Next, given that the objective is a quadratic function w.r.t.
an,t
, we set the above derivative to zero and use the global
in
minimizer to update each row,

n] with α = 0.

an,t
in

update

← qn,t
in

(Pn,t
in

)−1, ∀n, ∀in.

(10)

Note that, this row-wise updating can apply in parallel. If
˜Ωt,in is empty, then we do not update an,t
in

.

• Dense strategy. If ˜Ωt is dense, then we extend the EM-
ALS [Acar et al., 2011] method, which applies standard
ALS algorithm on the imputed tensor. To be more speciﬁc,

we ﬁrst impute the full tensor by interpolating/estimating
the unobserved elements,

ˆX t = ˜Ωt

∗ X t + (1 − ˜Ωt)
(cid:35)

∗
(cid:35)

(cid:74)

ˆA1,t, · · · , ˆAN,t

,

(cid:75)

where we use ˆX t to denote the estimated full tensor, and
{ ˆAn,t}N
n=1 are the initialized factors. Then, the ﬁrst term
of the objective in Eqn. (8) is approximated by a quadratic
form w.r.t. each factor/block,

(cid:13)
ˆX t −
(cid:13)
(cid:13)

(cid:75)
To calculate the derivative, let us deﬁne

A1,t, · · · , AN,t
(cid:74)

(cid:13)
2
(cid:13)
(cid:13)

F

.

Pn,t

Qn,t

(cid:16)

U =

∗ k(cid:54)=nAk,t(cid:62)
(cid:35)
(cid:16)
U =( ˆXt
n)U
∗ k(cid:54)=nAk,t(cid:62)
(cid:35)
(cid:16)
L =( ˆXt
n)L

Ak,t(cid:17)
(cid:12)k(cid:54)=nAk,t(cid:17)
Ak,t(cid:17)
(cid:12)k(cid:54)=nAk,t(cid:17)

L =

(cid:16)

Qn,t

Pn,t

,

(cid:16)

∗ k(cid:54)=nUk,t(cid:62)
+ α
(cid:35)
+ αAn,t−1 (cid:16)

Uk,t(cid:17)

+ βI,

∗ k(cid:54)=nAk,t−1(cid:62)
(cid:35)

Uk,t(cid:17)

,

+ βI,

n

n

n−I t−1

is the ﬁrst I t−1
n )×(cid:81)
n(cid:54)=k I k,t
n. Here, Pn,t

n is the mode-n unfolding of ˆX t, and ( ˆXt
n(cid:54)=k I k,t

where ˆXt
n ×(cid:81)
RI t−1
n)L ∈ R(I t
( ˆXt
n ) rows of ˆXt
I t−1
deﬁnite, Qn,t
We express the derivative of the upper and lower blocks by
the intermediate variables deﬁned above,

rows of ˆXt
n is the the remaining (I t
U , Pn,t
n ×R and Qn,t

n)U ∈
n, while
n −
L ∈ RR×R are positive
L ∈ R(I t
n−I t−1

U ∈ RI t−1

n )×R.

∂LE
∂Un,t = 2Un,tPn,t
∂LE
∂Ln,t = 2Ln,tPn,t

U − 2Qn,t

U , ∀n,

L − 2Qn,t

L , ∀n.

Here, the overall objective LE is a quadratic function w.r.t.
Un,t or Ln,t. By setting the derivative to zero, we can ob-
tain the global minimizer for updating,

Un,t update
Ln,t update

← Qn,t

← Qn,t

U (Pn,t
L (Pn,t

U )−1, ∀n,
L )−1, ∀n.

(11)

To sum up, the optimization ﬂow for LE is summarized in
Algorithm 1. For optimizing L, we simply modify the algo-
rithm by replacing the input {xt
}Ωt.
i1···iN
We analyze the complexity of our GOCPT in appendix B.

} ˜Ωt with {xt

i1···iN

n=1, {xt

i1···iN } ˜Ωt , α, β;;

Algorithm 1: Factor Updates for LE at time t
1 Input: {An,t−1}N
2 Parameters: {Un,t}N
3 Initialize parameters by Eqn. (6) and (9);;
4 for n ∈ [1, · · · , N ] do
5

update Un,t using Eqn. (10) or (11);;
update Ln,t using Eqn. (10) or (11);;

n=1, {Ln,t}N

n=1;;

6
7 end

8 Output: new factors

(cid:26)

An,t =

(cid:20)Un,t
Ln,t

(cid:21)(cid:27)N

.

n=1

4 Unifying Previous Online Settings
Several popular online tensor factorization/completion spe-
cial cases can be uniﬁed in our framework. Among them,
we focus on the online tensor factorization and online tensor
completion. We show that those objectives in literature can
be achieved by GOCPT. Our experiments conﬁrm GOCPT can
obtain comparable or better performance over previous meth-
ods in those special cases.

4.1 Online Tensor Factorization

Figure 2: Online Tensor Factorization

Online tensor factorization tries to maintain the factoriza-
tion results (e.g., CP factors) while the tensor is growing,
shown in Fig. 2. Zhou et al. [Zhou et al., 2016] proposed
an accelerated algorithm for the common setting where only
the temporal mode grows. [Song et al., 2017] considered the
multi-aspect setting where multiple mode grows simultane-
ously. We discuss the multi-aspect setting below.
Problem 2 (Multi-aspect Online Tensor Factorization). Sup-
1 ×I t−1
pose tensor X t−1 ∈ RI t−1
at time (t − 1) admits
a low-rank approximation, induced by Ut−1 ∈ RI t−1
1 ×R,
Vt−1 ∈ RI t−1

2 ×R, Wt−1 ∈ RI t−1

3 ×R, such that

2 ×I t−1

3

X t−1 ≈

Ut−1, Vt−1, Wt−1

.

(cid:75)

(cid:74)

2×R, Wt ∈ RI t
2×I t

RI t
growing new tensor X t ∈ RI t
= xt−1

At time t, we want to learn a new set of factors, Ut ∈
1×R, Vt ∈ RI t
3×R to approximate the
3 , which satisﬁes,
n ], ∀n ∈ [1, 2, 3].

1×I t
, ∀in ∈ [1, · · · , I t−1
Uniﬁcation. To achieve the existing objective in the litera-
ture, we simply reduce our LE by changing the L2 regular-
ization into nuclear norm (i.e., the sum of singular values),

xt
i1i2i3

i1i2i3

Lreg = γ1

(cid:13)Ut(cid:13)
(cid:13)

(cid:13)∗ + γ2

(cid:13)Vt(cid:13)
(cid:13)

(cid:13)∗ + γ3

(cid:13)Wt(cid:13)
(cid:13)

(cid:13)∗ ,

where γn, ∀ n ∈ [1, 2, 3] are the hyperparameters and they
sum up to one. This regularization is the convex relaxation of
CP rank [Song et al., 2017]. Then, the objective becomes

LE = (cid:13)

Ut, Vt, Wt

(cid:13)(X t −
+ α (cid:13)
+ β (cid:0)γ1

(cid:74)
(cid:13)(Y t−1 −
(cid:13)
(cid:13)Ut(cid:13)

(cid:13)
2
) ˜Ωt
(cid:13)
F
(cid:75)
Ut, Vt, Wt
(cid:13)
(cid:13)Vt(cid:13)

(cid:74)
(cid:13)∗ + γ2

(cid:13)
2
)Ωt−1
(cid:13)
F
(cid:75)
(cid:13)
(cid:13)Wt(cid:13)
(cid:13)∗ + γ3
(cid:13)∗

(cid:1) ,

where Ωt and Ωt−1 are the bounds of the new and previous
tensors and ˜Ωt = Ωt \ Ωt−1 (in this case, Ωt,old = Ωt−1)
indicates the newly added elements at time t. This form is
identical to the objective proposed in [Song et al., 2017].

In experiment Sec. 5.3, we evaluate on the temporal mode
growth setting. Our methods use the L2 regularization as
listed in Sec. 3 and follow Algorithm 1.

4.2 Online Tensor Completion

Figure 3: Online Tensor Completion

Online tensor completion studies incomplete tensors, where
new incomplete slices will add to the temporal mode. The
goal is to effectively compute a new tensor completion result
(i.e., CP factors) for the augmented tensor. This problem is
studied in [Mardani et al., 2015; Kasai, 2016; Kasai, 2019].
We show the formulation below.
Problem 3 (Online Tensor Completion). Suppose X t−1 is
the underlying tensor at time (t−1), which admits a low-rank
approximation, Ut−1 ∈ RI1×R, Vt−1 ∈ RI2×R, Wt−1 ∈
RI t−1

3 ×R, such that given the mask Ωt−1,
∗ X t−1 ≈ Ωt−1
(cid:35)

∗
(cid:35)
At the time t, given a new slice with incomplete entries, i.e.,
∆ ∈ RI1×I2 , the new data is concatenated along
Ωt
∆
the temporal mode (the third-mode) by Ωt = [Ωt−1; Ωt
∆] and
X t = [X t−1; Xt
∆]. We want to update the approximation
with Ut ∈ RI1×R, Vt ∈ RI2×R, Wt ∈ RI t
3 =
I t−1
3 + 1, such that

Ut−1, Vt−1, Wt−1

3×R, where I t

∗ Xt
(cid:35)

Ωt−1

(cid:74)

(cid:75)

.

Ωt

∗ X t ≈ Ωt
(cid:35)

∗
(cid:35)
Uniﬁcation. To handle this setting, we remove the recon-
struction regularizer (second term) from our L and the re-
duced version can be transformed into,

Ut, Vt, Wt

(cid:75)

(cid:74)

.

L =

It
3(cid:88)

(cid:20)

k=1

(cid:16)

(cid:13)
(cid:13)
(cid:13)

α

Xt,k − Utdiag(wt

k)Vt(cid:62)(cid:17)

(cid:13)
2
(cid:13)
(cid:13)

F

Ωt,k

(cid:21)

+ β(cid:107)wt

k(cid:107)2
F

+ β (cid:0)(cid:107)Ut(cid:107)2

F + (cid:107)Vt(cid:107)2
F

(cid:1) ,

(12)
where Ωt,k and Xt,k indicate the k-th slices of the tensor
k is the k-th row of Wt.
along the temporal mode, and wt
From Eqn. (12), we could make α exponential decaying w.r.t.
time steps (i.e., αk = γI t
3−k), the β within summation be
time-variant (i.e., βk = λk × γI t
3−k) and the β outside be
, where γ, {λk} are hyperparameters. We then obtain the
λI t
identical objective as proposed in [Mardani et al., 2015]. The
experiments are shown in Sec. 5.4.

3

5 Experiments
In the experiment, we evaluate our model on various settings.
Our models are named GOCPT (with the objective L) and
GOCPTE (with objective LE). Dataset statistics are listed in
Table 1. More details can be found in appendix C.

5.1 Experimental Setups
Metrics. The main metric is percentage of ﬁtness (PoF)
[Acar et al., 2011], which is deﬁned for the factorization or
completion problem respectively by (the higher, the better)
)(cid:107)F
(cid:75)

(cid:107)(1 − Ω)

or 1 −

(cid:107)F
(cid:75)

(cid:107)X −

1 −

,

A1 . . . AN
(cid:74)
(cid:107)X (cid:107)F

∗ (X −
A1 . . . AN
(cid:74)
∗ X (cid:107)F
(cid:35)
(cid:107)(1 − Ω)
(cid:35)

Dataset

Format

Setting

51 × 3 × 8 × 209
JHU Covid
Patient Claim 56 × 22 × 10 × 104

FACE-3D
GCSS
Indian Pines
CovidHT

112 × 99 × 400
50 × 422 × 362
145 × 145 × 200
420 × 189 × 128

General (Sec. 5.2)
General (Sec. 5.2)
Factorization (Sec. 5.3)
Factorization (Sec. 5.3)
Completion (Sec. 5.4)
Completion (Sec. 5.4)

Table 1: Data Statistics

Model

JHU Covid Data

Perturbed JHU Covid Data

Total Time (s)

Avg. PoF

Total Time (s)

Avg. PoF

EM-ALS
CPC-ALS
GOCPTE
GOCPT

1.68 ± 0.001
2.14 ± 0.002
1.32 ± 0.004
2.68 ± 0.002

0.6805 ± 0.024
0.6813 ± 0.028
0.6897 ± 0.016
0.6920 ± 0.022

2.13 ± 0.049
2.50 ± 0.013
1.72 ± 0.034
3.17 ± 0.041

0.6622 ± 0.047
0.6634 ± 0.021
0.6694 ± 0.045
0.6827 ± 0.024

Model

Patient Claim

Perturbed Patient Claim

Total Time (s)

Avg. PoF

Total Time (s)

Avg. PoF

EM-ALS
CPC-ALS
GOCPTE
GOCPT

4.37 ± 0.056
4.74 ± 0.036
2.71 ± 0.033
5.10 ± 0.037

0.4458 ± 0.023
0.5022 ± 0.021
0.5299 ± 0.019
0.5485 ± 0.022

5.35 ± 0.066
5.58 ± 0.009
3.27 ± 0.024
5.91 ± 0.042

0.4626 ± 0.021
0.5169 ± 0.019
0.5454 ± 0.017
0.5577 ± 0.021

Table 2: Results on General Case

where Ω and X are the mask and underlying tensor,
{A1, · · · , AN } are the low-rank CP factors. We also report
the total time consumption as an efﬁciency indicator.

Baselines. We simulate three different practical scenarios
for performance comparison. Since not all existing methods
can deal with the three cases, we select representative state-
of-the-art algorithms in each scenario for comparison:

• For the general case in Sec. 5.2, most previous models can-
not support this setting. We adopt EM-ALS [Acar et al.,
2011; Walczak and Massart, 2001] and CPC-ALS [Karls-
son et al., 2016] as the compared methods, which follow
the similar initialization procedure in Sec. 3.2.

• For the online tensor factorization in Sec. 5.3, we employ
OnlineCPD [Zhou et al., 2016]; MAST [Song et al., 2017]
and CPStream [Smith et al., 2018], which use ADMM and
require multiple iterations; RLST [Nion and Sidiropoulos,
2009], which is designed only for third-order tensors.

• For the online tensor completion in Sec. 5.4, we im-
plement EM-ALS and its variant, called EM-ALS (decay),
which assigns exponential decaying weights for historical
slices; OnlineSGD [Mardani et al., 2015]; OLSTEC [Kasai,
2016; Kasai, 2019] for comparison.

We compare the space and time complexity of each model
in appendix B. All experiments are conduct with 5 random
seeds. The mean and standard deviations are reported.

5.2 General Case with Three Evolving Patterns
Datasets and Settings. We use (i) JHU Covid data [Dong
et al., 2020] and (ii) proprietary Patient Claim data to conduct
the evaluation. The JHU Covid data was collected from Apr.
6, 2020, to Oct. 31, 2020 and the Patient Claim dataset col-
lected weekly data from 2018 to 2019. To mimic tensor value
updates on two datasets, we later add random perturbation to
randomly selected 2% existing data with value changes uni-
formly of [−5%, 5%] at each time step. The leading 50%
slices are used as preparation data to obtain the initial factors
with rank R = 5. The results are in Table 2.

Figure 4: Performance Comparison on Special Cases

Result Analysis. Overall, our models show the top ﬁt-
ness performance compared to the baselines, and the variant
GOCPTE shows 20% efﬁciency improvement over the best
model with comparable ﬁtness on both datasets. CPC-ALS
and EM-ALS performs similarly on Covid tensor and CPC-
ALS works better on the Patient Claim data, while they are
inferior to our models in both ﬁtness and efﬁciency.
5.3 Special Case: Online Tensor Factorization
Dataset and Setups. We present the evaluation on low-rank
synthetic data.
In particular, we generate three CP factors
from uniform [0, 1] distribution and then construct a low-rank
tensor (I1, I2, I3, R) = (50, 50, 500, 5). We use the leading
10% slices along the (third) temporal mode as preparation;
then, we add one slice at each time step to simulate mode
growth. We report the mean values in Figure 4.

Also, we show that our GOCPT can provide better ﬁtness
than all baselines and GOCPTE outputs comparable ﬁtness
with state of the art efﬁciency on two real-world datasets: (i)
ORL Database of Faces (FACE-3D) and (ii) Google Covid
Search Symptom data (GCSS). The result tables are moved
to appendix C.3 due to space limitation.

5.4 Special Case: Online Tensor Completion
Datasets and Setups. Using the same synthetic data de-
scribed in Sec. 5.3, we randomly mask out 98% of the en-
tries and follow the same data preparation and mode growth
settings. The results of mean curves are shown in Fig. 4.

We also evaluate on two real-world datasets: (i) Indian
Pines hyperspectral image dataset and (ii) a proprietary Covid
location by disease by date, we call
disease counts data:
it the health tensor (CovidHT), and the results (refer to ap-
pendix C.4) show that GOCPT has the better ﬁtness and
GOCPTE has decent ﬁtness with good efﬁciency.

6 Conclusion
This paper proposes a generalized online tensor factorization
and completion framework, called GOCPT, which can support
various tensor evolving patterns and uniﬁes existing online
tensor formulations. Experiments conﬁrmed that our GOCPT
can show good performance on the general setting, where
most previous methods cannot support. Also, our GOCPT
provides comparable or better performance in each special
setting. The GOCPT package is currently open-sourced.

Acknowledgements
This work was in part supported by the National Sci-
ence Foundation award SCH-2014438, PPoSS 2028839, IIS-
1838042, the National Institute of Health award NIH R01
1R01NS107291-01 and OSF Healthcare. We thank Navjot
Singh and Prof. Edgar Solomonik for valuable discussions.

0100200300400Time Step t0.910.920.930.940.950.960.970.980.99PoFOnline Tensor Factorization (synthetic)OnlineCPD (time: 6.138s)MAST (time: 52.47s)RLST (time: 5.697s)CPStream (time: 9.06s)GOCPTE (time: 6.805s)GOCPT (time: 8.919s)0100200300400Time Step t0.940.950.960.970.980.991.00PoFStreaming Tensor Completion (synthetic)EM-ALS (12.04s)EM-ALS (decay) (12.05s)OnlineSGD (7.121s)OLSTEC (7.26s)GOCPTE (8.221s)GOCPT (16.85s)References
[Acar et al., 2011] Evrim Acar, Daniel M Dunlavy,
Scalable ten-
Tamara G Kolda, and Morten Mørup.
sor factorizations for incomplete data. Chemometrics and
Intelligent Laboratory Systems, 106(1):41–56, 2011.
[Cai et al., 2015] Yongjie Cai, Hanghang Tong, Wei Fan,
Ping Ji, and Qing He. Facets: Fast comprehensive mining
of coevolving high-order time series. In Proceedings of the
21th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 79–88, 2015.
[Carroll and Chang, 1970] J Douglas Carroll and Jih-Jie
Chang. Analysis of individual differences in multidi-
mensional scaling via an n-way generalization of “eckart-
young” decomposition. Psychometrika, 1970.

[Dong et al., 2020] Ensheng Dong, Hongru Du, and Lauren
Gardner. An interactive web-based dashboard to track
covid-19 in real time. The Lancet infectious diseases,
20(5):533–534, 2020.

[Du et al., 2018] Yishuai Du, Yimin Zheng, Kuang-chih
Lee, and Shandian Zhe. Probabilistic streaming tensor de-
composition. In 2018 IEEE International Conference on
Data Mining (ICDM), pages 99–108. IEEE, 2018.

[Fang et al., 2021] Shikai Fang, Robert M Kirby, and Shan-
dian Zhe. Bayesian streaming sparse tucker decomposi-
In Proceedings of the 37th Conference on Uncer-
tion.
tainty in Artiﬁcial Intelligence (UAI), 2021.
[Gilman and Balzano, 2020] Kyle Gilman

Laura
Balzano. Grassmannian optimization for online tensor
completion and tracking with the t-svd. arXiv preprint
arXiv:2001.11419, 2020.

and

[Hitchcock, 1927] Frank L Hitchcock. The expression of a
tensor or a polyadic as a sum of products. Journal of Math-
ematics and Physics, 6(1-4):164–189, 1927.

[Hong et al., 2020] David Hong, Tamara G Kolda, and Jed A
Duersch. Generalized canonical polyadic tensor decompo-
sition. SIAM Review, 62(1):133–163, 2020.

[Karlsson et al., 2016] Lars Karlsson, Daniel Kressner, and
Andr´e Uschmajew. Parallel algorithms for tensor comple-
tion in the cp format. Parallel Comput., 57(C), 2016.
[Kasai, 2016] Hiroyuki Kasai. Online low-rank tensor sub-
space tracking from incomplete data by cp decomposition
using recursive least squares. In 2016 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 2519–2523. IEEE, 2016.

[Kasai, 2019] Hiroyuki Kasai. Fast online low-rank tensor
subspace tracking by cp decomposition using recursive
least squares from incomplete observations. Neurocom-
puting, 347:177–190, 2019.

[Mardani et al., 2015] Morteza Mardani, Gonzalo Mateos,
and Georgios B Giannakis. Subspace learning and impu-
tation for streaming big data matrices and tensors. IEEE
TSP, 63(10):2663–2677, 2015.

[Najaﬁ et al., 2019] Mehrnaz Najaﬁ, Lifang He, and S Yu
Philip. Outlier-robust multi-aspect streaming tensor com-
pletion and factorization. In IJCAI, 2019.

[Nimishakavi et al., 2018] Madhav Nimishakavi, Bamdev
Mishra, Manish Gupta, and Partha Talukdar.
Inductive
framework for multi-aspect streaming tensor completion
In Proceedings of the 27th ACM
with side information.
International Conference on Information and Knowledge
Management, pages 307–316, 2018.

[Nion and Sidiropoulos, 2009] Dimitri Nion and Nicholas D
Sidiropoulos. Adaptive algorithms to track the parafac de-
composition of a third-order tensor. IEEE Transactions on
Signal Processing, 57(6):2299–2310, 2009.

[Phipps et al., 2021] Eric Phipps, Nick Johnson,

Tamara G Kolda.
cal polyadic tensor decompositions.
arXiv:2110.14514, 2021.

and
Streaming generalized canoni-
arXiv preprint

[Qian et al., 2021] Cheng Qian, Nikos Kargas, Cao Xiao,
Lucas Glass, Nicholas Sidiropoulos, and Jimeng Sun.
Multi-version tensor completion for time-delayed spatio-
temporal data. IJCAI, 2021.

Smith,

[Smith et al., 2018] Shaden

Huang,
Nicholas D Sidiropoulos, and George Karypis. Streaming
tensor factorization for inﬁnite data sources. In Proceed-
ings of the 2018 SIAM International Conference on Data
Mining, pages 81–89. SIAM, 2018.

Kejun

[Song et al., 2017] Qingquan Song, Xiao Huang, Hancheng
Ge, James Caverlee, and Xia Hu. Multi-aspect stream-
ing tensor completion. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, pages 435–443, 2017.

[Sun et al., 2006] Jimeng Sun, Dacheng Tao, and Christos
Faloutsos. Beyond streams and graphs: dynamic tensor
In Proceedings of the 12th ACM SIGKDD in-
analysis.
ternational conference on Knowledge discovery and data
mining, pages 374–383, 2006.

[Walczak and Massart, 2001] Beata Walczak and DL Mas-
sart. Dealing with missing data: Part i. Chemometrics
and Intelligent Laboratory Systems, 58(1):15–27, 2001.
[Xiao et al., 2018] Houping Xiao, Fei Wang, Fenglong Ma,
and Jing Gao. eotd: An efﬁcient online tucker decomposi-
tion for higher order tensors. In 2018 IEEE International
Conference on Data Mining (ICDM). IEEE, 2018.

[Yang et al., 2021] Chaoqi Yang, Navjot Singh, Cao Xiao,
Cheng Qian, Edgar Solomonik, and Jimeng Sun. Mtc:
Multiresolution tensor completion from partial and coarse
observations. KDD, 2021.

[Zhou et al., 2016] Shuo Zhou, Nguyen Xuan Vinh, James
Bailey, Yunzhe Jia, and Ian Davidson. Accelerating on-
In In
line cp decompositions for higher order tensors.
SIGKDD, pages 1375–1384, 2016.

[Zhou et al., 2018] Shuo Zhou, Sarah Erfani, and James Bai-
ley. Online cp decomposition for sparse tensors. In 2018
IEEE ICDM. IEEE, 2018.

A Matrix and Tensor Algebra, Notations

Table 3: Notations used in GOCPT

Symbols

Descriptions

1×···It
N

1×···It
N
1×···It
N

(cid:12)
∗
·
(cid:35)
(cid:75)
(cid:74)
R
Ωt ∈ RIt
˜Ωt, ˜Ωt,old
X t ∈ RIt
Y t ∈ RIt
An,t ∈ RIt
an,t
in
Un,t ∈ RIt−1
Ln,t ∈ R(It
Pn,t
∈ RR×R, qn,t
in
in
U ∈ RR×R, Qn,t
Pn,t
Pn,t
L ∈ RR×R, Qn,t
Note that (i) notation with superscript t means at time t; (ii) no-
tations with ˆ means the initialized value or the estimated value.

tensor Khatri-Rao product
tensor Hadamard product
tensor Kruskal product
tensor CP rank
tensor (observation) mask
mask for new data and old data
where we have ˜Ωt (cid:83) ˜Ωt,old = Ωt
the underlying tensor
low-rank approximation of X t
the factor matrices
the in-th row of the factor matrices
the upper block matrices
the lower block matrices
auxiliary variables
auxiliary variables for Un,t
auxiliary variables for Ln,t

n×R, ∀n
∈ R1×R, ∀n, in
n ×R, ∀n
n−It−1

∈ R1×R
U ∈ RIt−1
L ∈ R(It

n ×R
n−It−1

n )×R, ∀n

n )×R

Kronecker Product. One important operation for matrices
is the Kronecker product. For A ∈ RI×J and B ∈ RK×L,
their Kronecker product is deﬁned by (each block is a scalar
times matrix)

A ⊗ B =







a11B a12B · · ·
a21B a22B · · ·
...
· · ·
aI1B aI2B · · ·

...







a1J B
a2J B
...
aIJ B

∈ RIK×JL.

Khatri–Rao Product. Khatri-Rao product is another im-
portant product for matrices, speciﬁcally, for matrices with
same number of columns. The Khatri-Rao product of A ∈
RI×J and B ∈ RK×J can be viewed as column-wise Kron-
cker product,

A (cid:12) B = (cid:2)a(1) ⊗ b(1), a(2) ⊗ b(2), · · · , a(L) ⊗ b(L)(cid:3) ,
where a(k) and b(k) are the k-th column of A and B, and
A (cid:12) B ∈ RIK×L.
Tensor Unfolding. This operation is to matricize a tensor
along one mode. For tensor X ∈ RI1×I2×I3, we could un-
fold it along the ﬁrst mode into a matrix X1 ∈ RI1×I2I3.
Speciﬁcally, each row of X1 is a vectorization of a slice in
the original tensor; we have

X1(i, j × I3 + k) = X (i, j, k), ∀i, j, k.
Here, to reduce clutter, we use Matlab-style notation to index
the value. Similarly, for the unfolding operation along the
second or third mode, we have

X2(j, i × I3 + k) = X (i, j, k) ∈ RI2×I1I3 , ∀i, j, k,
X3(k, i × I2 + j) = X (i, j, k) ∈ RI3×I1I2 , ∀i, j, k.
is

Hadamard Product. The Hadamard product
the
element-wise product for tensors of the same size. For ex-
ample, the Hadamard product of two 3-mode tensors X , Y ∈
RI1×I2×I3 is

Z = X

∗ Y ∈ RI1×I2×I3 .
(cid:35)

Canonical Polyadic (CP) Factorization. One of the com-
mon compression methods for tensors is CP factorization
[Hitchcock, 1927; Carroll and Chang, 1970], also called
CANDECOMP/PARAFAC (CP) factorization, which repre-
sents a tensor by multiple rank-one components. For exam-
ple, let X ∈ RI1×I2×I3 be an arbitrary 3-order tensor of CP-
rank R, then it can be expressed exactly by factor matrices
U ∈ RI1×R, V ∈ RI2×R, W ∈ RI3×R as

X =

R
(cid:88)

r=1

u(r) ◦ v(r) ◦ w(r) =

U, V, W

,

(cid:75)

(cid:74)

where ◦ is vector outer product, u(r), v(r), w(r) are the r-th
column vectors.

n=1 I t

n=1 I t

B Complexity Analysis
Follow the notations in Section 2 of main paper, we analyze
the space and time complexity of the optimization algorithm.
Assume the CP-rank R satisﬁes: I t
n (cid:29) R, ∀t, ∀n. Let St =
n, P t = (cid:81)N
(cid:80)N
n, and nnz(·) indicates the number
of non-zero entries.
Space Complexity. The space complexity records the stor-
age needed for input at time t. For objective L, we need the
previous factors, St−1R, and the current data, nnz(Ωt) (note
that, nonzero entries of the mask indicates the observed data).
In total, St−1R + nnz(Ωt) is the input size; for the efﬁcient
version LE, the old unchanged data is not needed. Therefore,
the input size is St−1R + nnz( ˜Ωt).
Time Complexity. For time complexity, the analysis is un-
der the sparse or dense strategies in Sec 3.2 of the main paper.
• Sparse strategy.
In Eqn. (10), the overall cost of com-
puting Pn,t
is O(nnz(Ωt)N R(N + R)); In Eqn. (11),
in
the overall cost of computing qn,t
is O(nnz(Ωt)N 2R);
in
the cost of matrix inverse for all Pn,t
is O(StR3) by
in
Cholesky decomposition (which could be omitted since
nnz(Ωt) (cid:29) St). Overall, the time complexity for L is
O(nnz(Ωt)N R(N + R)). The time complexity for LE is
O(nnz( ˜Ωt)N R(N + R)).

U and Qn,t

• Dense strategy. First, the cost of imputing the tensor is
O(P tR). Then, in Eqn. (13), the overall cost of MTTKRP
L ) is O(N P tR); the cost of computing Pn,t
(Qn,t
U
and Pn,t
L is O(StR2) and the cost of calculating their in-
verse is O(N R3) (these two quantity could be omitted
since P t (cid:29) St, P t (cid:29) R2). The dominant complexity are
imputation and MTTKRP, which sum up to O(N P tR) for
L. The cost of LE is O(N P tR/I t
n) with only mode growth
(one new slice at a time), but it can be up to O(N P tR) in
more complex scenarios.

For a comprehensive comparison, we listed the space and
time complexity of all baselines in Table 4, while for different
settings, the baselines are different.

C Additions for Experiments
All experiments use R = 5 and are implemented by Python
3.8.5, Numpy 1.19.1 and the experiments are conducted on a

Model

Space Complexity

Time Complexity

nnz(Ωt) + ( ¯S + IN )R
nnz(Ωt) + ( ¯S + IN )R
nnz( ˜Ωt) + ( ¯S + IN )R

For Sec. 5.2 (the general setting) and Sec. 5.4 (the completion setting):
EM-ALS
CPC-ALS
OnlineSGD
OLSTEC
GOCPTE
GOCPT

O(N R ¯P IN )
O(nnz(Ωt)N R(N + R))
O(nnz( ˜Ωt)N R(N + R))
nnz( ˜Ωt) + ( ¯SR + 2 ¯S + IN )R O(nnz( ˜Ωt)N R(N + R))
O(nnz( ˜Ωt)N R(N + R))
O(nnz(Ωt)N R(N + R))

nnz( ˜Ωt) + ( ¯S + IN )R
nnz(Ωt) + ( ¯S + IN )R

For Sec. 5.3 (the factorization setting):
OnlineCPD
MAST
RLST
CPStream
GOCPTE
GOCPT

¯P + (2 ¯S + IN )R + (N − 1)R2
¯P + 3( ¯S + IN )R
¯P + ( ¯S + 2 ¯P + IN )R + 2R2
¯P + ( ¯S + IN + R)R
¯P + ( ¯S + IN )R
¯P IN + ( ¯S + IN )R

O(N R ¯P )
O(N R ¯P )
O(R2 ¯P )
O(N R ¯P )
O(N R ¯P )
O(N R ¯P IN )

Table 4: Complexity per iteration at Time t, where ¯P = P/IN , ¯S =
S − IN (refer to Sec. B) and we remove the superscript t whenever
there is no ambiguity.

Linux workstation with 256 GB memory, 32 core CPUs (3.70
GHz, 128 MB cache) In our experiments of the main text,
we use the sparse strategy for the general case and the tensor
completion case and use dense strategy for the factorization
case.

Average PoF (Avg. PoF). Note that, in section 5.2 of the
main paper, the Avg. PoF is calculated by averaging the PoF
scores over the time steps (remember that we calculate a PoF
score per time step), and the mean and standard deviation val-
ues are calculated on ﬁve runs with different random seeds.

C.1 Datasets Description

• JHU Covid Dataset. As mentioned in the main text, this
dataset3 [Dong et al., 2020] was collected from Apr. 6,
2020 to Oct. 31, 2020. We model the data as a fourth-order
tensor and treat 3 categories: new cases, deaths, and hos-
pitalization as the feature dimension. The collected data
covers 51 states across the US. This data contains 209 gen-
eration dates (GD) and 8 loading dates (LD) for each data
point. The leading 50% slices (before Jul. 16) are used as
preparation data to obtain the initial factors. The reﬂection
of three evolving patterns: at each timestamp, the GD will
increase by 1 and the LD dimension will be ﬁlled accord to
how previous data is received. Later, we add random per-
turbations to randomly selected 2% eisting data with value
changes uniformly of [−5%, 5%] at each time step.

• Patient Claim Data. This dataset can also be modeled
as a fourth-order tensor, which contains 56 states, 22 dis-
ease categories, 10 LD’s (for the delayed updates), and 104
GD’s (104 weeks over the year 2018 and 2019, there is one
generated service data point for each week). The leading
50% slices (data of year 2018) are used as preparation data
to obtain the initial factors. The reﬂection of three evolv-
ing patterns: at each timestamp, the GD will increase by 1
and the LD dimension will be ﬁlled accordingly. Later, at
each time step, we add random perturbations to randomly
selected 2% eisting data with value changes uniformly of
[−5%, 5%].

3https://github.com/CSSEGISandData/COVID-19

For the above two datasets, the GD mode is the grow-
ing mode (referring to change type (i)), and the LD (de-
layed updates) causes the incompleteness of previous ten-
sor slices, while the incomplete slices will be ﬁlled in later
dates (referring to change type (ii)). By ﬁxing the ﬁrst and
the second modes, we show a diagram (ﬁgure below) of
how the GD and LD evolve. At each time step, a new GD is
generated while the LDs for the previous GD’s can be ﬁlled
(assuming that all GD’s have the same number of LDs in to-
tal). A special setting is discussed in a recent paper [Qian
et al., 2021].

Additionally, to simulate change type (iii) on these two
datasets, we add random perturbations to the existing tensor
elements in the experiments.

• FACE-3D. ORL Database of Faces4 contains 400 shots of
face images with size 112 pixels by 99 pixels. To con-
struct a third-order tensor, we treat each shot as a slice and
concatenate them together. The ﬁrst 10% leading shots are
used as preparation data.

• GCSS. This is a public dataset5 containing google covid-
19 symptom search results during the complete year 2020.
The data can be viewed as a third-order tensor: 50 states,
422 keywords, and 362 days. The ﬁrst 10% leading dates
are used as preparation data.

• Indian Pines. This is also an open dataset6 containing 200
shots of hyperspectral images with size 145 pixels by 145
pixels. The data is also modeled as a third-order tensor.
We manually mask out 98% of the tensor elements for the
online tensor completion problem. Later, the masked out
entries are used as ground truth to evaluate our completion
results. The ﬁrst 10% leading images are used as prepara-
tion data.

• CovidHT. This is a proprietary covid-19 related disease
tensor with 420 zip codes, 189 disease categories, and 128
dates. We model it as a third-order tensor. Similar to the
Indian Pines datasets, we manually mask out 98% of the
tensor elements for later evaluation, and the ﬁrst 10% lead-
ing shots are used as preparation data.

C.2 Hyperparameter settings
Our framework has two hyperparameters, the ﬁrst one is β,
which is the weight for L2 regularization. We set β = 10−5
for all experiments and it does not change w.r.t. time step t.
Another hyperparameter is α, controlling the importance of
historical reconstruction. Let us set I t to be the dimension of
the growing mode. We listed the α used in the experiments

4https://cam-orl.co.uk/facedatabase.html
5https://pair-code.github.io/covid19 symptom dataset/
6https://purr.purdue.edu/publications/1947/1

Model

Indian Pines

CovidHT

Total Time (s)

Avg. PoF

Total Time (s)

Avg. PoF

EM-ALS
EM-ALS (decay)
OnlineSGD
OLSTEC
GOCPTE
GOCPT

17.15 ± 0.022
17.05 ± 0.014
11.33 ± 0.025
11.32 ± 0.014
11.82 ± 0.031
19.89 ± 0.545

0.8839 ± 5.696e-4
0.8845 ± 5.452e-4
0.8576 ± 3.502e-4
0.8864 ± 7.447e-5
0.8923 ± 5.083e-4
0.8970 ± 1.281e-3

25.71 ± 0.148
25.69 ± 0.115
15.99 ± 0.061
16.05 ± 0.141
16.44 ± 0.039
27.92 ± 0.091

0.5432 ± 2.317e-2
0.5463 ± 2.314e-2
0.4422 ± 1.185e-2
0.6503 ± 7.041e-3
0.6612 ± 1.321e-2
0.6792 ± 2.602e-2

Table 7: Results for Online Tensor Completion

C.5 Discussion on Different Evolving Patterns
Here, we summarize some intuitions and conclusions for the
evolving patterns.

• Mode growth may lead to better or poor PoF based on data
quality in new dimensions. With more ﬁlled missing val-
ues, PoF will intuitively increase. These two patterns will
increase the running time because the data size (or dimen-
sion size) increases.

• According to Table 2 of the main paper, with the additional
value update pattern (perturbed version), the running time
of all models increases slightly while the ﬁnal PoF may be
improved or not (based on how good the updated value is).

C.6 Ablation Study on Mask Density
This section evaluates the performance of sparse and dense
strategies in Sec. 3.2. We experiment on Indian Pines dataset
with the full tensors and generate random masks with six den-
sity levels. Both strategies run for 25 iterations. We plot the
running time versus PoF metric in Fig. 5.

Figure 5: Comparison with Different Density on Mask

Result Analysis. We observe that dense strategy works
well and more efﬁciently on masks with high density while
the sparse strategy is more advantageous when the density is
low. Thus, we use the sparse strategy for the general case in
Sec. 5.2 and completion case in Sec. 5.4, and use the dense
strategy (without the imputation step) for common factoriza-
tion setting in Sec. 5.3. Note that, our models run either strat-
egy for one iteration per time step.

in Table 5. There are several advices on choosing the hy-
perparameters: (i) for any settings included with real-world
datasets, the α can be chosen based on the simulation results
of the synthetic data; (ii) we keep a larger α for the real-world
data (in factorization case), since the real data is usually not
smooth and the statistics from old slices to new slices could
change. A larger α would make transition from time (t − 1)
to time t more smooth, which makes the ﬁnal results more
stable; (iii) compared to GOCPT, our GOCPTE does not keep
the historical data, so it relies on the historical reconstruc-
tion more, then we set a larger α, which is 100 times the α
for GOCPT. The details for different settings can refer to: the
general case (Sec. 5.2 of main text), factorization case (Sec.
5.3 of main text), completion cases (Sec. 5.4 of main text).

Settings

general case

factorization case

completion case

GOCPT
α = 0.02/I t
α = 0.02/I t for synthetic data
α = 2/I t for real data
α = 0.005/I t

GOCPTE
α = 2/I t
α = 2/I t for synthetic data
α = min(1, 200/I t) for real data
α = 0.5/I t

Table 5: Hyperparameter α Settings for All Experiments

C.3 Exp. for Online Tensor Factorization

The setting is discussed in the main paper, however, we move
the result table here due to space limitation.

Note that the contribution of our model is to (a) provide a
uniﬁed framework and can handle both online tensor factor-
ization and completion (previous models are designed only
for either); (b) show that our model can provide comparable
or better performance over all baselines in all settings. Here,
section C.3 and C.4 are to empirically show (b).

Model

FACE-3D

GCSS

Total Time (s)

Avg. PoF

Total Time (s)

Avg. PoF

MAST
RLST

OnlineCPD 13.61 ± 0.040
98.52 ± 0.012
13.33 ± 0.033
CPStream 18.35 ± 0.005
GOCPTE
13.77 ± 0.165
GOCPT
17.14 ± 0.071

0.7447 ± 1.129e-3
0.7464 ± 1.405e-3
0.7216 ± 2.342e-3
0.7446 ± 1.281e-3
0.7452 ± 1.208e-3
0.7473 ± 1.315e-3

26.38 ± 0.035
159.21 ± 0.192
26.64 ± 0.004
43.61 ± 0.112
26.75 ± 0.094
34.17 ± 0.338

0.9258 ± 3.053e-4
0.9278 ± 2.437e-4
0.6725 ± 3.653e-2
0.9256 ± 5.192e-4
0.9270 ± 1.022e-4
0.9297 ± 3.203e-4

Table 6: Results for Online Tensor Factorization

Result Analysis. On the real data, OnlineCPD and our
GOCPTE show great ﬁtness and the best efﬁciency. Though
GOCPT presents the best ﬁtness score, it is relatively slower
than the best model. The baselines MAST and CPStream are
expensive since they use ADMM to iteratively update the fac-
tors within each time step, which is time-consuming.

C.4 Exp. for Online Tensor Completion

Due to space limitation, we move the experimental results
here and give result summary in the main text.

Result Analysis. On the real-world data, baseline OLSTEC
and our GOCPTE show decent performance, and they both
outperform other baseline methods in terms of both ﬁtness
and speed, though GOCPT has the best ﬁtness score consis-
tently.

