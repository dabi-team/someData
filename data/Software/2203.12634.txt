Applications of physics informed neural operators

Shawn G. Rosofsky1, 2, 3 and E. A. Huerta3, 4, 2
1NCSA, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
2Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
3Data Science and Learning Division, Argonne National Laboratory, Lemont, Illinois 60439, USA
4Department of Computer Science, University of Chicago, Chicago, Illinois 60637, USA

We present an end-to-end framework to learn partial differential equations that brings together initial data pro-
duction, selection of boundary conditions, and the use of physics-informed neural operators to solve partial
differential equations that are ubiquitous in the study and modeling of physics phenomena. We ﬁrst demonstrate
that our methods reproduce the accuracy and performance of other neural operators published elsewhere in the
literature to learn the 1D wave equation and the 1D Burgers equation. Thereafter, we apply our physics-informed
neural operators to learn new types of equations, including the 2D Burgers equation in the scalar, inviscid and
vector types. Finally, we show that our approach is also applicable to learn the physics of the 2D linear and
nonlinear shallow water equations, which involve three coupled partial differential equations. We release our
artiﬁcial intelligence surrogates and scientiﬁc software to produce initial data and boundary conditions to study
a broad range of physically motivated scenarios. We provide the source code, an interactive website to visualize
the predictions of our physics informed neural operators, and a tutorial for their use at the Data and Learning
Hub for Science.

I.

INTRODUCTION

The description of physical systems has a common set of elements, namely: the use of ﬁelds (electromagnetic, gravitational, etc.)
on a given spacetime manifold, a geometrical interpretation of these ﬁelds in terms of the spacetime manifold, partial differential
equations (PDEs) on these ﬁelds that describe the change of a system over spacetime, and an initial value formulation of these
equations with suitable boundary conditions [1]. Given that the evolution of physical ﬁelds over spacetime may be naturally
expressed in terms of PDEs, a plethora of numerical methods have been developed to accurately and rapidly solve these class of
equations [2].

In time, and even with the advent of extreme scale computing, some physical systems have become increasingly difﬁcult
to model, e.g., multi-scale and multi-physics systems that combine disparate time and spatial scales, and which demand the
use of subgrid-scale precision to accurately resolve the evolution of physical ﬁelds. This is a well known problem in multiple
disciplines, including general relativistic simulations [3–5], weather forecasting [6], ab initio density functional theory simula-
tions [7], among many other computational grand challenges.

The realization that large scale computing resources are ﬁnite and will continue to be oversubscribed [8, 9], has impelled
scientists to explore novel approaches to address computational bottlenecks in scientiﬁc software [10]. Some approaches include
rewriting modules of software stacks to leverage GPUs, leading to signiﬁcant speedups [11–13]. Other contemporary approaches
have harnessed advances in machine learning to accelerate speciﬁc computations in software modules [14], while others have
developed entirely new solutions by combining GPU-accelerated computing and novel signal processing tools that have at their
core machine learning or artiﬁcial intelligence applications [15–21].

The creation of AI surrogates aims not only to enhance the science reach of advanced computing facilities. Most importantly,
this emergent area of research aims to combine AI and extreme scale computing to enable research that would otherwise remain
unfeasible with traditional approaches. Furthermore, AI surrogates aim to capture known knowledge, and ﬁrst principles to stir
AI learning in the right direction, and then reﬁne AI surrogates performance and predictions by using experimental scientiﬁc
data. In time, it is expected that by exposing AI to detailed simulations, ﬁrst principles and experimental data, AI surrogates
will capture the non-linear behaviour of experimental phenomena and guide the planning, automation and execution of new
experiments, leading to breakthroughs in science and engineering.

In this article we contribute to the construction of AI surrogates by demonstrating their application to solve a number of PDEs
that are ubiquitous in physics, and which have not been presented before in the literature. Beyond these original contributions,
we also provide an end-to-end framework that uniﬁes initial data production, construction of boundary conditions and their use to
train, validate and test the performance and reliability of AI surrogates. These activities aim to create FAIR (ﬁndable, accessible,
interoperable and reusable) and AI-ready datasets and models [22, 23]. In the following sections we introduce key concepts and
ideas that will facilitate the understanding and use of physics-inspired neural operators (PINOs) to solve PDEs [24]. This article
is organized as follows. In Section II we introduce the AI tools we use to learn the physics of PDEs. We describe our methods
and approaches to create PINOs in Section III. We summarize the PDEs we consider in this study in Section IV, and present a
direct comparison between numerical solutions of these PDEs and predictions from our PINOs in Section V. We describe future
directions of work in Section VI.

2
2
0
2

r
a

M
3
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

1
v
4
3
6
2
1
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
II. MODELING PHYSICS WITH ARTIFICIAL NEURAL NETWORKS

A. Physics informed neural networks

2

Physics informed neural networks (PINNs) provide a method of using known physical laws to predict the results of various
physical systems at high accuracy [25–29]. These methods estimate the results for a given physical system consisting of a
PDE, initial conditions (ICs), and boundary conditions (BCs) by minimizing constraints in the loss function. PINNs utilize
the automatic differentiation of deep learning frameworks to compute the derivatives of the PDE to compute the residual error.
Despite the success of PINNs, their inability to produce results for different initial conditions prevents them from being useful
surrogate models.

B. Neural operators

Neural operators use neural networks to learn operators rather than single physical systems.

In our case, PDEs are the
operator these networks try to learn. Speciﬁcally, these networks take coordinates, ICs and BCs as their inputs. Neural operators
then output the solutions of that operator at those coordinates. There are various types of neural operators that have been
studied in recent works such as DeepONets, physics informed DeepONets, low-rank neural operators (LNO), graph neural
operators (GNO), multipole graph neural operators (MGNO), Fourier neural operators (FNO), and physics informed neural
operators [24, 30–35]. These networks have all illustrated their ability to reproduce the results of operators much faster than
computing with the operator directly.

C. Physics informed neural operators (PINOs)

PINOs are a variation of neural operators that incorporate knowledge of physical laws into their loss functions [24]. PINOs
have been shown reproduce the results of operators with remarkable accuracy. They employ the FNO architecture which applies
a fast Fourier transform (FFT) to the data and applies its fully connected layers in Fourier space before performing an inverse
FFT back to real space [34]. Moreover, this architecture has been demonstrated the ability to perform ability perform zero-shot
super-resolution, predicting on higher resolution data having only seen low resolution data [34, 35]. Figure 1 illustrates the FNO
architecture that we use in this study.

PINOs improve upon the FNO architecture by adding physics information such as PDEs, ICs, BCs, and other conservation
laws. These are done by adding the violation of these laws into the loss function, the network can learn these laws in addition
to the data. Rather than using automatic differentiation, these networks use Fourier derivatives to compute the derivatives for
the PDE constraints as automatic differentiation is very memory intensive for this type of architecture. This physics knowledge
enables the network to learn operators faster and with less training data.

Here we describe the approach we followed to design, train, validate and test our PINOs, and then how we quantiﬁed their

accuracy by comparing our predictions with actual numerical solutions of PDEs we consider in this study.

III. METHODS

A.

Initial data

The ﬁrst step in this process was to generate initial data that matches the boundary conditions of the problem. This was done
using the Gaussian random ﬁelds (GRF) method in a similar fashion to [24] where the kernel was transformed into Fourier space
to match our periodic boundary conditions. We employed a general Matern kernel, but discovered that the PINOs performed
better when the smoothness parameter was set to inﬁnity. In this limit, this Matern kernel becomes the radial basis radial basis
function kernel (RBF) as used in [30] deﬁned as kl (x1, x2) = exp
, where l is the length scale of typical
spatial deviations in the data. For this work, we set l = 0.1 for all cases to provide features at our desired spatial scale. A number
of these random ﬁelds were produced for each of the test problems described Section IV.

− (cid:107)x1 − x2(cid:107)2 /2l2(cid:17)

(cid:16)

3

FIG. 1. Neural network architecture The top panel shows the architecture of our neural operators, whereas the inset in the bottom panel
shows the structure of one of the ﬁve Fourier layers, {F1, ..., F5}, we use in our PINOs. The top left panel shows the data we feed into our
models, labelled as “Initial Condition u(x, y)”. This input data are initially lifted into a higher dimension representation by the neural network,
P1. Thereafter, we apply a series of updates that consist of non-local integral operators and nonlinear activation functions, σ, shown in the
inset labelled as “Fourier Layer F”. Eventually, the neural network P2 projects back the updates, producing the output shown on the right
panels, which describe the time evolution of the system. In the inset, T1 represents a linear transform, T2 a local linear transform, σ a nonlinear
activation function. F and F −1 stand for Fourier transform and inverse Fourier transform, respectively.

B. Boundary conditions

For our boundary conditions, we used periodic boundary conditions in all cases. This boundary condition is good for problems
with signiﬁcant symmetry over long length scales or for cases where the boundary is sufﬁciently far from the region of interest.
We are particularly focused on the latter case where the boundary condition does not signiﬁcantly impact the problem. Moreover,
the neural network architecture implements periodic boundary conditions by default via FFTs. This periodicity of a certain
dimension can be removed by zero padding said dimension before feeding it to the neural network. We employed this padding
in the time dimension with a length of lpad = 5 for all cases except the 1D wave equation, which is periodic in time for a time
interval of t = 1.

C. Training data generation

To generate training data, we took the random IC ﬁelds that we had previously generated and evolved them in space and time.
Speciﬁcally, we evolve each of these equations in time with RK4 time stepping starting at t = 0 until t = 1 with the timestep
δt varying depending on the problem. To compute spacial derivatives, we employed ﬁnite difference method (FDM) with 4th
order central difference for most cases. The lone exception was the inviscid Burgers equation in 2D which required a shock
capturing method. Therefore, we employed a ﬁnite volume method (FVM) with local Lax-Friedrichs (LLF) ﬂuxes and MP5
reconstruction.

D. Training approach

We set up the problem as follows. We are given some space and time coordinates as well as the initial conditions at those
coordinates. Our objective is to compute the solution at each given space and time coordinate from the initial data. To train the
network to reproduce the simulated results, we employ multiple losses to ensure the network properly reproduces the correct

4

loss. These losses are the data loss Ldata, the physics loss Lphys, and the IC loss LIC. We do not include a loss term for the BC
here because the FNO architecture of the PINO assures the desired periodic BCs as long as those dimensions are not padded.

Ldata attempts to ﬁt the training data directly to the training data. This loss is computed via the relative mean squared error
(MSE) between the training data and the network outputs. This relative MSE is computed by dividing the MSE of the predictions
by the norm of the true values. For cases with multiple equations where the outputs vary in magnitude such as the linear and
nonlinear shallow water equations, care must taken to ensure that each of those outputs contributes equally to Ldata. Therefore,
we compute the relative MSE for each of the output ﬁelds separately before combining them together.

Lphys ensures that the PINO predictions obey known physical laws such as PDEs or more general conservation laws. We
deﬁne the loss as the MSE between the violation of the physical law and the value of said law if it was perfectly satisﬁed which
is 0 in most cases. Again, one must be careful in cases like the linear and nonlinear shallow water equations with multiple
physical laws whose violations differ signiﬁcantly in their magnitude. In such cases, compute the physical violations separately
and multiply each term by some weight before combining them and computing the MSE of the combined term to obtain Lphys.
LIC allows the PINO to learn how the input initial condition given to the network is the value of output that point at t = 0.
Moreover, by minimizing LIC, Lphys more easily converges to the correct solution for cases where the physical law is a PDE.
We deﬁned LIC as the relative MSE between the PINO prediction at t = 0 and the initial condition fed into the network. There
were no cases where the ICs differed signiﬁcantly in magnitude for the PDEs since for the linear and nonlinear shallow water
equations, the velocity ﬁelds were taken to be zero at t = 0 and were not fed into the initial data.

To combine these terms into the total loss Ltot, we perform a weighted sum deﬁned as

Ltotal = wdataLdata + wphysLphys + wICLIC ,

(1)

where wdata is the data weight, wphys is the physics weight, and wIC is the IC weight. We varied these values between different
cases and even during the training. Typically, we would set wdata to be 5 or 10, wphys to be 1 or 2, and wIC to be 5 or 10.

E. Performance quantiﬁcation

To quantify the performance, we ran PINO on the test dataset, then calculated the MSE of their predictions with the test data
and the MSE of the physics loss that accompanied violations of physical laws. The test dataset is composed of approximately
10% of the simulations produced from the random initial data that was separated from the rest of the data to ensure that the
PINO did not train on it.

IV. TEST PROBLEMS

We use PINOs to learn eight different PDEs. We consider the wave equation in 1D and 2D to demonstrate that our methods
produce accurate and reliable results. We also present results for the 1D Burgers equation, which was used in [24] to quantify
the performance of PINOs to learn PDEs. We then put our methods at work to solve a variety of PDEs with different levels of
complexity.

Our ﬁrst test was the wave equation in 1D with periodic boundary conditions. This is a computationally simple PDE that is

second order in time and models a variety of different physics phenomena. The equation takes the form

A. Wave Equation 1D

utt (x, t) + c2uxx (x, t) = 0,

x ∈ [0, 1) , t ∈ [0, 1]

(2)

u (x, 0) = u0 (x) ,

where c = 1 is the speed of the wave.

B. Wave Equation 2D

5

We extend the wave equation in 2D with periodic boundary conditions to explore the requirements for adding the additional

spatial dimension. The equation is given by

utt (x, y, t) + c2 [uxx (x, y, t) + uyy (x, y, t)] = 0,

x, y ∈ [0, 1) , t ∈ [0, 1]

(3)

u (x, y, 0) = u0 (x, y) ,

where as before the speed of the wave is set to c = 1.

C. Burgers Equation 1D

The 1D Burgers equation with periodic boundary conditions serves as a nonlinear test case with for a variety of numerical
methods. This allowed us to verify that our PINOs can learn and reconstruct nonlinear phenomena. The equation is given in
conservative form by

ut(x, t) + ∂x

(cid:2)u2(x, t)/2(cid:3) = νuxx(x, t),
u(x, 0) = u0(x) ,

x ∈ [0, 1), t ∈ [0, 1]

(4)

where the viscosity ν = 0.01.

D. Burgers Equation 2D Scalar

To verify our model can handle nonlinear phenomena in 2D, we extend the Burgers equation into 2D by assuming the ﬁeld u

is a scalar. The equations take the form

ut(x, y, t) + ∂x

(cid:2)u2(x, y, t)/2(cid:3) + ∂y

(cid:2)u2(x, y, t)/2(cid:3) = ν [uxx(x, y, t) + uyy(x, y, t)] ,

x, y ∈ [0, 1), t ∈ [0, 1]

(5)

u(x, y, 0) = u0(x, y) ,

where the viscosity ν = 0.01.

E. Burgers Equation 2D Inviscid

We also looked at cases involving the inviscid Burgers equation in 2D in which we set the viscosity ν = 0. This setup is
known to produce shocks that can result in numerical instabilities if not handled correctly. We used a ﬁnite volume method
(FVM) to generate this data to ensure stability in the presence of shocks. In turn, this allowed us to investigate the network’s
performance when processing shocks. The equations are given by

ut(x, y, t) + ∂x

(cid:2)u2(x, y, t)/2(cid:3) + ∂y

(cid:2)u2(x, y, t)/2(cid:3) = 0,

x, y ∈ [0, 1), t ∈ [0, 1],

(6)

u(x, y, 0) = u0(x, y).

We observed that the presence of the shock prevented our Fourier derivative method from producing accurate residuals when we
included the full equation in our physics loss term. Instead, for our physics loss, we used the conserved quantity,

(cid:90)

Ω

u(x, y, t) dx dy =

(cid:90)

Ω

u(x, y, 0) dx dy = C ,

(7)

where C is a constant and Ω is the domain. In other words, we ensured that the total u at every time instance is equal to the total
u at t = 0.

F. Burgers Equation 2D Vector

6

We then looked at a vectorized form of the 2D Burgers equation with periodic boundary conditions. This allowed us to test

how well the model handles the coupled ﬁelds u and v that parameterize the system. The equations take the form

ut(x, y, t) + u(x, y, t)ux(x, y, t) + v(x, y, t)uy(x, y, t) = ν [uxx(x, y, t) + uyy(x, y, t)] ,
vt(x, y, t) + u(x, y, t)vx(x, y, t) + v(x, y, t)vy(x, y, t) = ν [vxx(x, y, t) + vyy(x, y, t)] ,
u(x, y, 0) = u0(x, y), v(x, y, 0) = v0(x, y), x, y ∈ [0, 1), t ∈ [0, 1]

(8)
(9)

where the viscosity ν = 0.01. We note that this system of equations does not have a conservative form as there is not a continuity
equation in this system.

G. Linear Shallow Water Equations 2D

To examine the properties of PINOs with 3 coupled equations, we examined the ability of the networks to reproduce the linear
shallow water equations with periodic boundary conditions. We assumed that the height of the perturbed surface h(x, y, t) is
initially perturbed, but the initial velocity is initially zero. These equations can be expressed as

∂h
∂t
∂u
∂t
∂v
∂t

+ H

(cid:19)

(cid:18) ∂u
∂x

+

∂v
∂y

= 0 ,

− f v = −g

+ f u = −g

∂h
∂x
∂h
∂y

,

,

(10)

(11)

(12)

with h(x, y, 0) = h0(x, y), u(x, y, 0) = 0, v(x, y, 0) = 0,

x, y ∈ [0, 1), t ∈ [0, 1],

where the gravitational constant g = 1, the mean ﬂuid height H = 100, and we considered two cases for the Coriolis coefﬁcient
f = {0, 1}.

H. Nonlinear Shallow Water Equations 2D

Finally, we examined the network performance on the nonlinear shallow water equations. We assumed a similar setup as in
the linear case where we assumed the total ﬂuid column height η(x, y, t), was given by a mean value of 1 plus some initial
perturbation. We again assumed the initial velocity was zero. These equations are given by

∂(η)
∂t

+

∂(ηu)
∂x
(cid:19)

ηu2 +

gη2

+

+

∂(ηv)
∂y
∂(ηuv)
∂y

= 0 ,

= ν (uxx + uyy) ,

(cid:18)

ηv2 +

(cid:19)

1
2

gη2

= ν (vxx + vyy) ,

∂(ηu)
∂t
∂(ηv)
∂t

+

+

(cid:18)

∂
∂x
∂(ηuv)
∂x

1
2
∂
∂y

+

(13)

(14)

(15)

with η(x, y, 0) = η0(x, y), u(x, y, 0) = 0, v(x, y, 0) = 0,

x, y ∈ [0, 1), t ∈ [0, 1],

where the gravitational coefﬁcient g = 1 and the viscosity coefﬁcient ν = 0.002 to prevent the formation of shocks.
All these different PDEs serve the purpose of establishing the accuracy and reliability of our PINOs, and then explore their
application for more interesting scenarios for the 2D Burgers equation, and 2D linear and non linear shallow waters equations,
which involve several coupled equations.

V. RESULTS

7

We now quantify the ability of our PINOs to learn the physics described by the PDEs described above. In Figures 2-10 we
present qualitative and quantitative results that illustrate the performance of our PINOs. We use two types of quantitative metrics,
namely, absolute error (shown in each ﬁgure) and mean squared error (summarized in Table I) between PINO predictions and
ground truth simulations. While the ﬁgures below provide snapshots of the performance of our PINOs at a given time, t, we also
provide interactive visualizations of these results at this website. We also provide a tutorial to use our PINOs and reproduce our
results in the Data and Learning Hub for Science [36, 37].

A. Wave Equation 1D

Figure 2 shows that our PINO for the 1D wave equation learns and describes the physics of this PDE with remarkable accuracy.
In Figure 2 we present two different initial conditions (left column); exact solutions for the 1D wave equation (center left
column); PINO predictions (center right column); and the discrepancy between PINO and ground truth predictions, upred −utrue
(right column). These results serve the purpose of validating our methods with a simple PDE. We found that the MSE in this
case was 1.22 × 10−3 on the test dataset. We note that for this case, we experimented with using a very high resolution of 4096
grid points to generate the training data, but downsampled by a factor of 32 to a ﬁnal resolution of 128 grid points that were fed
into the network. This simulates having available less data to reproduce a result than was required to compute it.

FIG. 2. PINO for 1D wave equation Test set initial conditions (left column) fed into our neural networks. The center left column displays
the ground truth value of utrue(x, t) as produced by our simulations. The center right column shows the value of upred(x, t) predicted by our
PINO. The right column illustrates the error between the PINO predictions and the ground truth, upred − utrue.

B. Wave Equation 2D

Next we consider the 2D wave equation. Figure 3 summarizes our results. As before, we consider two different initial
conditions (left column) that we selected from our test set and fed them into our PINOs. The center left column shows the
actual ground truth simulations at t = 1. We selected this time because we want to quantify the performance of our PINOs once
the system has evolved sufﬁcient time and thus we get a clear picture of how errors arise and evolve in time in our data-driven
solutions. The center right column shows the PINO-predicted evolution of the system at t = 1, while the fractional difference
between PINO and ground truth predictions is shown in the right column. As in the 1D scenario, our PINOs have learned the
physics of the 2D wave equations with excellent accuracy. Speciﬁcally, we found the MSE on the test dataset for this case to be
7.73 × 10−3.

8

FIG. 3. PINO for 2D wave equation As Figure 2, but now in 2D. The left column represent the test set initial conditions that we feed into our
PINOs. We evolved the systems until t = 1 and present ground truth solutions (center left) and PINO predictions (center right). Even after
evolving these simulations until the end of the time domain under consideration, our PINOs predict with excellent accuracy the evolution of
the system, as shown in the right column which shown at t = 1, upred − utrue.

C. Burgers Equation 1D

We now turn our attention to the Burgers equation, and begin this analysis with a simple and illustrative case, namely the 1D
Burgers equation, shown in Figure 4. These results show that our PINOs have accurately learned the physics described by this
PDE with an excellent level of accuracy. Quantitatively, the MSE on the test daatset was 6.94 × 10−3. These results furnish
evidence that our methods can reproduce results published elsewhere in the literature [24].

FIG. 4. PINO for 1D Burgers equation The left column shows the test set initial conditions fed into our PINOs. The center left column
shows the ground truth value, utrue(x, t), produced by our simulations. The center right column presents the predicted values upred(x, t) by
our PINOs. The right column shows the discrepancy between PINO and ground truth predictions, upred − utrue.

We now turn our attention to several 2D Burgers equations that, to the best of our knowledge, have not been explored previ-

ously in the literature.

D. Burgers Equation 2D Scalar

9

This PDE is given by Equation (5). As shown in Figure 5 our PINOs can learn and describe the physics of this PDE accurately.
Note that we present results for this PDE once the system has been evolved throughout the time domain under consideration,
i.e., t ∈ [0, 1]. By presenting results at t = 1 we gain a good understanding of the actual performance of our PINOs once they
have evolved in time and accumulated numerical errors that may depart from ground truth values. In Figure 5 we present two
different initial conditions from our test set (left column) and the ground truth values of our simulations at t = 1 in the center
left column. We can directly compare these predictions with those produced by our PINOs in the center right column. It is not
straightforward to compare these results by eye. Thus in the right column we present the discrepancy between PINO predictions
and the ground truth, upred − utrue, at t = 1. We found that the MSE for the full test dataset was 3.89 × 10−3. These results
show that our PINOs have learned well the physics of this PDE.

FIG. 5. PINO for 2D scalar Burgers equation The left column shows a sample of test set initial conditions fed into our PINOs. The center
left column displays the ground truth value, utrue(x, y), of our simulations once they have evolved up to t = 1. The center right column
shows PINO predictions for this PDE at t = 1. We have selected this time to quantify the accuracy of our PINOs once the system has
evolved sufﬁcient time to accumulate errors. The right column illustrates the discrepancy between PINO predictions and the ground truth,
upred − utrue, at t = 1.

E. Burgers Equation 2D Inviscid

This PDE is given by Equation 6. As we mentioned before, shocks are a common occurrence for this PDEs, and may lead to
numerical instabilities if we do not use shock capturing schemes. We have quantiﬁed the ability of our PINOs to handle shocks.
We found the PINO to have an MSE of 0.0356 on the test dataset. In Figure 6 we notice that our PINOs are able to describe
the physics of this PDE with excellent accuracy. However, we observe a signiﬁcant discrepancy between ground truth and AI
predictions right at the regions where shocks occur. These ﬁndings indicate that further work is needed to better handle PDEs
that involve shocks. This is a speciﬁc area of work that we will pursue in future work.

F. Burgers Equation 2D Vector

This is the most complex PDE of the Burgers family we consider in this study, see Equations (8) and (9). The novel feature of
this PDE is that we now need to treat two different ﬁelds (u, v). In Figure 7 we present test set initial conditions for these ﬁelds
in the left column. We evolved this system up to t = 1 and present results of the ground truth value of our simulations in the
center left column, and predictions by our PINOs in the center right column. The right column shows the discrepancy between
PINO predictions and ground truth results from our simulations at t = 1. These results demonstrate that our PINOs have learned
the physics of this PDE and describe it with remarkable precision even after we have evolved this system until the end of the
time domain under consideration. We achieved an MSE of 8.49 × 10−3 on the test dataset.

10

FIG. 6. PINO for 2D inviscid Burgers equation As Figure 5 but now in the inviscid case given by Equation 6.

FIG. 7. PINO for 2D vector Burgers equation The top and bottom rows show the u and v ﬁelds, respectively. The left column shows test
set initial conditions for (u, v). The center left column shows ground truth values for the (u, v) ﬁelds once this PDE has been evolved up to
t = 1. The center right column shows PINO results for the (u, v) ﬁelds at t = 1. The right column show the discrepancy between PINO and
ground truth values at t = 1.

These results complete our analysis for a variety of PDE that involve the Burgers equation, and indicate that for various levels
of complexity and initial conditions our PINOs are capable of learning and describing the physics of these PDEs with excellent
accuracy. We have also realized that we need to further develop these methods for PDEs that involve shocks. We are keenly
interested in this particular case, and will explore in future work.

G. Linear Shallow Water Equations 2D

Another original result in this study is the use of PINOs to learn the physics of 3 coupled PDE equations. The ﬁrst case under
consideration is the 2D linear shallow water equation given by Equations (10)-(12). In this case we now consider the height of
the perturbed surface, h, and the ﬁelds (u, v). In Figures 8 and 9 present results assuming two Coriolis coefﬁcient f = {0, 1},
respectively. As before, we show test set initial conditions in the left column, and the state of the system for (h, u, v) at t = 1

in the center left column. We show predictions by our PINOs at t = 1 in the center right column. The right column shows that
the discrepancy between data-driven PINO predictions and ground truth values is very small, which furnishes strong evidence
for the adequacy of PINOs to learn the physics of this PDE. Highlights of these results include:
• f = 0 case We only required 5 training samples to produce PINOs that exhibit optimal performance. In this case, the MSE

on the test dataset was 0.0362. This illustrates the ability of the generalize from very sparse training data.

• f = 1 case We increased the number of training samples to 45, which is comparable to some of the other 2D cases. The MSE

on the test dataset was calculated to be 9.06 × 10−3.

11

FIG. 8. PINO for 2D linear shallow water equation Assuming a system with a Coriolis coefﬁcient f = 0, we show the h, u, and v ﬁelds in
the top, middle, and bottom rows, respectively. The left column shows the initial condition provided to the network. The center left column
displays the ground truth value at t = 1 as produced by the simulation. The center right column shows the value of prediction at t = 1
predicted by our PINO. The right column indicates the discrepancy between PINO predictions and the ground truth at t = 1.

These results provide a glimpse of the data-driven capabilities of PINOs to learn the physics of these linear, coupled PDEs.
We have extensively tested these equations and found that they are robust to a broad range of initial data. These results provided
enough motivation to explore the use of PINOs for a more challenging set of coupled PDEs, namely, the 2D nonlinear shallow
water equations that we discuss next.

H. Nonlinear Shallow Water Equations 2D

The ﬁnal original contribution of this study is the use of PINOs to learn the physics of the 2D nonlinear shallow water equation,
given by Equations (13)- (15). Even though this PDE is signiﬁcantly more complex than its linear counterpart, we notice in
Figure 10 that our PINOs learn the physics described by the ﬁelds (η, u, v) with remarkable accuracy. We found the MSE for
the test dataset of this case to be 0.0344.

Finally, we provide an additional metric to quantify the overall performance of all the PDEs we have used in this study in
Table I. These results indicate that our PINOs provide state-of-the-art results to model simple PDEs (1D wave equation & 1D
Burgers equations), and excellent performance for a variety of PDEs that are solved for the ﬁrst time in the literature with physics
informed neural operators.

12

FIG. 9. PINO for 2D linear shallow water equation As Figure 8 but now with a Coriolis coefﬁcient f = 1.

Model

Spatial Resolution Time Steps Training Samples Testing Samples Relative MSE

Wave Equation 1D
Wave Equation 2D
Burgers Equation 1D
Burgers Equation 2D Scalar
Burgers Equation 2D Inviscid
Burgers Equation 2D Vector
Linear Shallow Water Equations 2D f=0
Linear Shallow Water Equations 2D f=1
Nonlinear Shallow Water Equations 2D

128
128 × 128
128
128 × 128
128 × 128
128 × 128
128 × 128
128 × 128
128 × 128

101
101
101
101
101
101
101
101
101

900
45
90
45
90
475
5
45
45

100
5
10
5
10
25
5
5
5

1.22E-03
7.73E-03
6.94E-03
3.89E-03
3.56E-02
8.49E-03
3.62E-02
9.06E-03
3.44E-02

TABLE I. Summary of PINO results The ﬁrst column describes the modeled equation. The second and third columns describe the spatial
and temporal resolution, respectively. The ﬁfth and sixth columns display the number of training and testing samples used. The ﬁnal column
provides the relative mean squared error (MSE) of our physics informed neural operators on the test set.

VI. CONCLUSIONS

We have introduced an end-to-end framework to learn PDEs that range from simple equations that serve the purpose of testing
our methods (1D wave equation & 1D Burgers equation) to increasingly more complex equations (2D scalar, 2D inviscid and
2D vector Burgers equation), and coupled PDEs that are solved with PINOs for the ﬁrst time in the literature (2D linear &
nonlinear shallow waters equations). The methods we introduce in this study provide the ﬂexibility to produce initial data to
test the robustness and applicability of AI surrogates for a broad range of physically motivated scenarios. We provide scientiﬁc
visualizations of our results through an interactive website. We also release our PINOs and scientiﬁc software through the Data
and Learning Hub for Science so that AI practitioners may download, use and further develop our neural networks. In addition
to this Data and Learning Hub for Science implementation, we release the source code used in this paper.

Future work may focus on the extension of these PINOs to high-dimensional PDEs that are relevant for the modeling of
complex phenomena that demand subgrid scale precision, and which typically lead to computationally expensive simulations.

13

FIG. 10. PINO for 2D nonlinear shallow water equation The (η, u, v) ﬁelds are shown in the top, middle, and bottom rows, respectively.
The left column presents the test set initial condition that is fed into our network. The center left column displays the ground truth value at
t = 1 as produced by our simulations. The center right column shows the value of prediction at t = 1 predicted by the PINO. The right column
illustrates the discrepancy between the PINO predictions and the ground truth at t = 1.

We will also focus on developing methods that handle shocks effectively, since these phenomena are common in ﬂuid dynamics
and relativistic astrophysics, to mention a few. We will also continue our research program combining scientiﬁc visualization
and accelerated computing to gain insights about what PINOs learn from data, and how this information may be used to enhance
their performance when applied to experimental datasets [38–40].

It is our expectation that our AI surrogates may be used to replace computationally demanding numerical methods to learn
PDEs in scientiﬁc software used to model multi-scale and multi-physics phenomena—e.g., numerical relativity simulations
of gravitational wave sources, weather forecasting, etc.,—and eventually provide data-driven solutions that more accurately
describe and identify novel features and patterns in experimental data.

ACKNOWLEDGMENTS

This material is based upon work supported by Laboratory Directed Research and Development (LDRD) funding from Ar-
gonne National Laboratory, provided by the Director, Ofﬁce of Science, of the U.S. Department of Energy under Contract
No. DE-AC02-06CH11357. This research used resources of the Argonne Leadership Computing Facility, which is a DOE
Ofﬁce of Science User Facility supported under Contract DE-AC02-06CH11357. S.R. and E.A.H. gratefully acknowledge Na-
tional Science Foundation award OAC-1931561. This work used the Extreme Science and Engineering Discovery Environment
(XSEDE), which is supported by National Science Foundation grant number ACI-1548562. This work used the Extreme Sci-
ence and Engineering Discovery Environment (XSEDE) Bridges-2 at the Pittsburgh Supercomputing Center through allocation

TG-PHY160053. We thank NVIDIA for their continued support.

14

[1] R. P. Geroch, in 46th Scottish Universities Summer School in Physics: General Relativity (1996) arXiv:gr-qc/9602055
[2] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes 3rd Edition: The Art of Scientiﬁc Computing, 3rd

ed. (Cambridge University Press, USA, 2007)

[3] D. Radice, Symmetry 12, 1249 (2020), arXiv:2005.09002 [astro-ph.HE]
[4] D. Radice, S. Bernuzzi, A. Perego, and R. Haas, arXiv e-prints , arXiv:2111.14858 (2021), arXiv:2111.14858 [astro-ph.HE]
[5] F. Foucart, Frontiers in Astronomy and Space Sciences 7, 10.3389/fspas.2020.00046 (2020)
[6] J. Schalkwijk, H. J. J. Jonker, A. P. Siebesma, and E. V. Meijgaard, Bulletin of the American Meteorological Society 96, 715 (2015)
[7] A. Erba, J. Baima, I. Bush, R. Orlando, and R. Dovesi, Journal of Chemical Theory and Computation 13, 5019 (2017), pMID: 28873313,

https://doi.org/10.1021/acs.jctc.7b00687

[8] M. Asch, T. Moore, R. Badia, M. Beck, P. Beckman, T. Bidot, F. Bodin, F. Cappello, A. Choudhary, B. de Supinski, E. Deelman,
J. Dongarra, A. Dubey, G. Fox, H. Fu, S. Girona, W. Gropp, M. Heroux, Y. Ishikawa, K. Keahey, D. Keyes, W. Kramer, J.-F. Lavignon,
Y. Lu, S. Matsuoka, B. Mohr, D. Reed, S. Requena, J. Saltz, T. Schulthess, R. Stevens, M. Swany, A. Szalay, W. Tang, G. Varoquaux, J.-P.
Vilotte, R. Wisniewski, Z. Xu, and I. Zacharov, The International Journal of High Performance Computing Applications 32, 435 (2018)

[9] W. Gropp, S. Banerjee, and I. Foster, arXiv e-prints , arXiv:2012.09303 (2020), arXiv:2012.09303 [cs.CY]
[10] E. A. Huerta, A. Khan, E. Davis, C. Bushell, W. D. Gropp, D. S. Katz, V. Kindratenko, S. Koric, W. T. C. Kramer, B. McGinty,

K. McHenry, and A. Saxton, Journal of Big Data 7, 88 (2020), arXiv:2003.08394 [physics.comp-ph]

[11] M. Taher, in 2009 4th International Design and Test Workshop (IDT) (2009) pp. 1–6
[12] C. I. Rodrigues, D. J. Hardy, J. E. Stone, K. Schulten, and W.-M. W. Hwu, in Proceedings of the 5th Conference on Computing Frontiers,

CF ’08 (Association for Computing Machinery, New York, NY, USA, 2008) p. 273–282

[13] D. Wysocki, R. O’Shaughnessy, J. Lange, and Y.-L. L. Fang, Phys. Rev. D 99, 084026 (2019), arXiv:1902.04934 [astro-ph.IM]
[14] P. Graff, F. Feroz, M. P. Hobson, and A. Lasenby, MNRAS 421, 169 (2012), arXiv:1110.2997 [astro-ph.IM]
[15] S. G. Rosofsky and E. A. Huerta, Phys. Rev. D 101, 084024 (2020), arXiv:1912.11073 [physics.comp-ph]
[16] E. A. Huerta and Z. Zhao, Advances in machine and deep learning for modeling and real-time detection of multi-messenger sources, in
Handbook of Gravitational Wave Astronomy, edited by C. Bambi, S. Katsanevas, and K. D. Kokkotas (Springer Singapore, Singapore,
2020) pp. 1–27

[17] E. Cuoco, J. Powell, M. Cavagli`a, K. Ackley, M. Bejger, C. Chatterjee, M. Coughlin, S. Coughlin, P. Easter, R. Essick, H. Gabbard,
T. Gebhard, S. Ghosh, L. Haegel, A. Iess, D. Keitel, Z. Marka, S. Marka, F. Morawski, T. Nguyen, R. Ormiston, M. Puerrer, M. Razzano,
K. Staats, G. Vajente, and D. Williams, Mach. Learn. Sci. Tech. 2, 011002 (2021)

[18] E. A. Huerta, A. Khan, X. Huang, M. Tian, M. Levental, R. Chard, W. Wei, M. Heﬂin, D. S. Katz, V. Kindratenko, D. Mu, B. Blaiszik,

and I. Foster, Nature Astronomy 5, 1062 (2021), arXiv:2012.08545 [gr-qc]

[19] E. A. Huerta, G. Allen, I. Andreoni, J. M. Antelis, E. Bachelet, G. B. Berriman, F. B. Bianco, R. Biswas, M. Carrasco Kind, K. Chard,
M. Cho, P. S. Cowperthwaite, Z. B. Etienne, M. Fishbach, F. Forster, D. George, T. Gibbs, M. Graham, W. Gropp, R. Gruendl, A. Gupta,
R. Haas, S. Habib, E. Jennings, M. W. G. Johnson, E. Katsavounidis, D. S. Katz, A. Khan, V. Kindratenko, W. T. C. Kramer, X. Liu,
A. Mahabal, Z. Marka, K. McHenry, J. M. Miller, C. Moreno, M. S. Neubauer, S. Oberlin, A. R. Olivas, D. Petravick, A. Rebei,
S. Rosofsky, M. Ruiz, A. Saxton, B. F. Schutz, A. Schwing, E. Seidel, S. L. Shapiro, H. Shen, Y. Shen, L. P. Singer, B. M. Sipocz, L. Sun,
J. Towns, A. Tsokaros, W. Wei, J. Wells, T. J. Williams, J. Xiong, and Z. Zhao, Nature Reviews Physics 1, 600 (2019), arXiv:1911.11779
[gr-qc]

[20] A. Khan, E. A. Huerta, and H. Zheng, Phys. Rev. D 105, 024024 (2022), arXiv:2110.06968 [gr-qc]
[21] P. Chaturvedi, A. Khan, M. Tian, E. A. Huerta, and H. Zheng, Front. Artif. Intell. 5, 828672 (2022), arXiv:2201.11133 [gr-qc]
[22] M. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, J.-W. Boiten, L. O. Bonino da Silva San-
tos, P. Bourne, J. Bouwman, A. Brookes, T. Clark, M. Crosas, I. Dillo, O. Dumon, S. Edmunds, C. Evelo, R. Finkers, and B. Mons,
Scientiﬁc Data 3 (2016)

[23] Y. Chen, E. A. Huerta, J. Duarte, P. Harris, D. S. Katz, M. S. Neubauer, D. Diaz, F. Mokhtar, R. Kansal, S. Eon Park, V. V. Kindratenko,

Z. Zhao, and R. Rusack, Scientiﬁc Data 9, 31 (2022), arXiv:2108.02214 [hep-ex]

[24] Z. Li, H. Zheng, N. Kovachki, D. Jin, H. Chen, B. Liu, K. Azizzadenesheli, and A. Anandkumar, arXiv e-prints , arXiv:2111.03794

(2021), arXiv:2111.03794 [cs.LG]

[25] M. Raissi, P. Perdikaris, and G. E. Karniadakis, arXiv preprint arXiv:1711.10561 (2017)
[26] M. Raissi, P. Perdikaris, and G. E. Karniadakis, arXiv preprint arXiv:1711.10566 (2017)
[27] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Journal of Computational Physics 378, 686 (2019)
[28] G. Pang, L. Lu, and G. E. Karniadakis, SIAM Journal on Scientiﬁc Computing 41, A2603 (2019), https://doi.org/10.1137/18M1229845
[29] L. Lu, X. Meng, Z. Mao, and G. E. Karniadakis, SIAM Review 63, 208 (2021)
[30] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Nature Machine Intelligence 3, 218 (2021)
[31] S. Wang, H. Wang, and P. Perdikaris, Science Advances 7, eabi8605 (2021)
[32] Z.-Y. Li, N. B. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, ArXiv abs/2003.03485 (2020)
[33] Z. Li, N. B. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. M. Stuart, and A. Anandkumar, CoRR abs/2006.09535 (2020),

2006.09535

[34] Z.-Y. Li, N. B. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, ArXiv abs/2010.08895 (2021)

[35] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar, arXiv e-prints , arXiv:2108.08481

(2021), arXiv:2108.08481 [cs.LG]

[36] R. Chard, Z. Li, K. Chard, L. Ward, Y. Babuji, A. Woodard, S. Tuecke, B. Blaiszik, M. J. Franklin, and I. Foster, in 2019 IEEE

International Parallel and Distributed Processing Symposium (IPDPS) (2019) pp. 283–292

[37] B. Blaiszik, L. Ward, M. Schwarting, J. Gaff, R. Chard, D. Pike, K. Chard, and I. Foster, MRS Communications 9, 1125 (2019)
[38] F. Doshi-Velez and B. Kim, arXiv e-prints , arXiv:1702.08608 (2017), arXiv:1702.08608 [stat.ML]
[39] M. Safarzadeh, A. Khan, E. A. Huerta, and M. Wattenberg, arXiv e-prints , arXiv:2202.07399 (2022), arXiv:2202.07399 [gr-qc]
[40] D. V. Carvalho, E. M. Pereira, and J. S. Cardoso, Electronics 8, 832 (2019)

15

