2
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

2
v
5
3
2
1
1
.
8
0
2
2
:
v
i
X
r
a

Preprocessing Source Code Comments for Linguistic Models

SERGEY MATSKEVICH, Drexel University, USA
COLIN S. GORDON, Drexel University, USA

Comments are an important part of the source code and are a primary source of documentation. This has
driven interest in using large bodies of comments to train or evaluate tools that consume or produce them —
such as generating oracles or even code from comments, or automatically generating code summaries. Most
of this work makes strong assumptions about the structure and quality of comments, such as assuming they
consist mostly of proper English sentences. However, we know little about the actual quality of existing
comments for these use cases. Comments often contain unique structures and elements that are not seen in
other types of text, and filtering or extracting information from them requires some extra care. This paper
explores the contents and quality of Python comments drawn from 840 most popular open source projects
from GitHub and 8422 projects from SriLab dataset, and the impact of naïve vs. in-depth filtering can have on
the use of existing comments for training and evaluation of systems that generate comments.

ACM Reference Format:
Sergey Matskevich and Colin S. Gordon. 2022. Preprocessing Source Code Comments for Linguistic Models.
1, 1 (August 2022), 27 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Comments play a major role in the software development process. They are often considered
to be de-facto documentation which is the closest to the source and play an essential role in
understanding [13] and maintaining [1] software. Due to the explosion of machine learning and the
parallel increase in the number of public online code repositories, much research is now focused on
using existing source code for mining, and for training or evaluating automation tools. This work
spans such topics as generating partial oracles [14], generating specifications [6, 51], predicting or
generating comments [26, 33, 34], and evaluating the quality of the source or comments [22, 47].
All of these lines of work rely on source code comments for training and/or evaluation, and many
use source code as well. However, while the fact that code is designed specifically for machine-
processing (e.g., by type-checkers, compilers, and unit tests) imposes a kind of lower-bound on
code quality, no such baseline is imposed for source code comments.

Many systems that automatically process comments make assumptions about the structure and
the contents of the comments. They generally assume that comments are written in mostly plain
text (usually in English as opposed to another natural language), optionally assuming light use
of highly-standardized structured comments (e.g., Javadoc) or that the full general summary of
the whole comment is in the first sentence of the comment. However, this is simply assumed, not
validated, and most evaluations proceed with small carefully-curated codebases for which these
assumptions hold. If these assumptions do not hold widely, then attempting to train similar tools
on different datasets can produce models that have vastly different performance.

Authors’ addresses: Sergey Matskevich, Drexel University, Philadelphia, Pennsylvania, USA, sm3372@drexel.edu; Colin S.
Gordon, Drexel University, Philadelphia, Pennsylvania, USA, csgordon@drexel.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Association for Computing Machinery.
XXXX-XXXX/2022/8-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: August 2022.

 
 
 
 
 
 
2

Sergey Matskevich and Colin S. Gordon

Overall, there is limited comprehensive research into comment quality: the quality of the language
in comments, what information is contained in the comments, types of comments, and so on. The
comment quality evaluation work that does exist focuses on detailed comment type taxonomies [40],
or a holistic way of evaluating whether a comment is good for software developers on the relevant
project [22, 47]; they do not consider the suitability of comment data for automated processing.
There is very recent work [9] showing that automatic comment generation is sensitive to comment
type, highlighting one way in which unvalidated assumptions of one body of work [20, 24, 27, 35]
likely skew results.

Most published work consuming comments as training data either performs only basic text
sanitization and filtering, or works only with smaller numbers of manually-curated high quality
exemplars of comments. Common data sanitization for NLP tasks is simply removing the punctua-
tion, making all words lower case and removing phrases/sentences that are too short. However,
this filtering assumes what remains afterwards is linguistic in nature, which we demonstrate is
often not true. In this work we set out to test the impact that the data will have on the accuracy of
text generation linguistic models with different stages of data sanitization.

This paper analyzes the source comments from the two datasets. One consists of the top 840 open
source Python projects on GitHub1 (208661 files, 46710511 lines of code), and the other comes from
the existing SRILab Py1502 dataset with 8422 projects containing 149970 files and 24540627 lines of
code. We provide a new taxonomy of comments with respect to relevance for building linguistic
models of source code comments, and investigate the impact different types of comments have on
machine learning algorithms that make use of natural language models. Our focus in this paper
is not the evaluation of the models as a whole, but a specific sub-task required for building any
model: the sanitization of comments with the goal of producing suitable training (and evaluation)
data for machine learning based systems.

In this paper we build two language models that we use for generation of comments and evaluate
the quality of the generated sentences based on different levels of sanitization of input. The
contributions of this paper are:

• We explore what types of comments are in the source code (e.g. copyright, special file headers,
etc) with respect to suitability for building linguistic models, and provide a taxonomy of
relevant types.

• We analyze the frequency of these comment types in two large Python corpora.
• We analyze the impact of different levels of care in filtering out non-linguistic comments

have on machine learning algorithms trained on source code comments.

• We highlight discrepancies between developer-focused comment taxonomies and tool-focused
taxonomies, discussing the usefulness of the comment to a human vs. a machine learning
algorithm, finding they are often in opposition.

In addition, this paper is to the best of our knowledge, the first analysis of Python code comments
(as opposed to prior work, which studies primarily Java and C++), finding that there is a higher
incidence of unambiguously useless comments than prior studies have found.

2 BACKGROUND AND RELATED WORK
Since machine learning models learn on large amounts of data, one major source of model bias is
duplication of data. Allamanis [2] explored the influence of code duplication on machine learning
models trained on large datasets. He points out that raw training data often has many duplicates,
which can result in duplication within and across the training and testing datasets, which both

1https://github.com/
2https://www.sri.inf.ethz.ch/py150

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

3

contradicts a common assumption in mining work, and can heavily skew results by biasing training
towards performing well on duplicated code. He analyzed multiple corpora of different languages
and found that the number of duplicates within one dataset can be as much as 25%, affecting
performance metrics by up to 50%. Although his work is not directly concerned with comments,
the same problem exists for systems trained on comment data. As the dataset contains duplicate
source files, all comments in those files will also be duplicated and thus impact training models. In
our study we found that comments from open source repositories contain significant amount of
duplication.

Blasi et al. [6] and Goffi et al. [14] use comments to create tools and applications for the automated
generation of the specifications and tests. They parse comments, relying on the use of Javadoc
formatting (particularly the @param tag) to extract oracle information for generating tests. The
Javadoc assumption is useful, but also a significant limitation: if the assumed Javadoc formatting
was not used, the tools behave as if the information was missing — even if present in prose —
and will not generate specifications or tests. The assumption of the structure of comments is also
an issue in the context of using the comment for machine learning. We find that comments can
have unexpected elements in them, such as text in a multiple natural languages, Markdown, LATEX,
and even different forms of the source code (Figure 1b). Pandita et al. [36] generate specifications
for an API using the API’s documentation, though without assuming specific language idioms
or comment formatting (i.e., Javadoc). They expect, however, to find similar information such as
method description, argument description, return information, exceptions, and general remarks.
LeClair et al. [25] produced a tool CodeGNN3 that uses comments in conjuction with the ASTs
to generate source code summaries using graph neural networks. This is one of the only works we
have found that actually addresses the filtering process in detail and specifically addressed (filtered)
comments written in a language other than English. However they still assume that the first sentence
of a comment suffices for a general summary, in a corpus of 1.9 million method-comment pairs. In
addition they only use similarity metrics such as BLEU Papineni et al. [38] scores for assessing the
quality of their system (consistent with standard practice for similar work [3, 9, 11, 20, 21, 27, 35]).
They never look at other metrics such as as relevance of the generated summary to the source
code, helpfulness of the summary to the developers, or actual readability of the final summary. We
show later that even if the BLEU score is high for a particular sentence, it does not necessarily mean
that the sentence is actually coherent or useful, consistent with other work showing that high BLEU
scores by themselves can be misleading in general [7, 8, 48] and specifically for software engineering
tasks [44, 46], in addition to being highly sensitive to minor details of preprocessing [41].

There is interesting prior work on comment quality with respect to utility for human developers
(the primary reason comments exist at all). This is an important focus, but this notion of comment
quality does not necessarily coincide with usefulness for comments for natural language-based
techniques [26, 28, 36]. We show that some comments (such as those in Figure 1) are useful for
developers yet are not suited for building linguistic machine learning models, and therefore need
to be pruned during input preparation.

Khamis et al. [22] provide heuristics for the analysis of the comment’s quality, however the
paper focuses on the qualitative analyses instead. The authors point out that comments are crucial
for timely and well-performed software maintenance, however comments are often overlooked
by developers and do not contain enough information for various development purposes. They
proposed several metrics to analyze the quality of a comment with regard to human use: the number
of tokens, nouns and verbs, number of words per Javadoc comment, number of abbreviations,
readability and code/comment consistency.

3https://github.com/acleclair/ICPC2020_GNN

, Vol. 1, No. 1, Article . Publication date: August 2022.

4

Sergey Matskevich and Colin S. Gordon

r"""
Display the homogeneous components of the mixed form.

The output is either text-formatted (console mode) or
LaTeX-formatted (notebook mode).

decoding_table = (

EXAMPLES::

u'\x00' # 0x00 -> NULL
u'\x01' # 0x01 -> START OF HEADING
u'\x02' # 0x02 -> START OF TEXT
u'\x03' # 0x03 -> END OF TEXT
u'\x9c' # 0x04 -> CONTROL
u'\t' # 0x05 -> HORIZONTAL TABULATION
u'\x86' # 0x06 -> CONTROL
u'\x7f' # 0x07 -> DELETE
u'\x97' # 0x08 -> CONTROL
u'\x8d' # 0x09 -> CONTROL
u'\x8e' # 0x0A -> CONTROL

"""

sage: M = Manifold(2, 'M')
sage: f = M.scalar_field(name='f')
sage: omega = M.diff_form(1, name='omega')
sage: eta = M.diff_form(2, name='eta')
sage: F = M.mixed_form([f, omega, eta], name='F'); F
Mixed differential form F on the 2-dimensional
differentiable manifold M
sage: F.display() # display names of homogeneous components
F = f + omega + eta

(a) Comments useful to humans but useless for
natural language processing in Cpython4repo

(b) SageMath5comment for MixedForm.display, con-
taining markup and code

Fig. 1. Comments containing non-linguistic text

They analyze how well the source is documented and explore the correlation of the code doc-
umentation and bugs in the software. The analysis, however, is limited in the size and scope.
The authors only analyzed a small number of related projects and looked only at how well the
comments were written. The authors did not provide any insight into the actual semantic contents
of the comments. In addition since they were focused on Javadoc comments, they did not evaluate
different categories of comments, such as copyrights and other types of comments that exist, but
do not help with the documentation of the code for human readers.

Steidl et al. [47] analyzed comment quality in Java and C++, bucketing comments into categories
such as copyright, header comments, inline comments, method comments, and others. They used
machine learning to classify comments into each category with high precision and recall and
provided a good metrics for overall human-relevant comment quality, such as usefulness, relevance,
completeness and consistency. However, the authors did not explore in detail the contents of all
comments, or comment properties that might affect other uses. During their preprocessing they
removed special characters and other types of symbols from the comments. As we show later, this
is inadequate for extracting high-quality linguistic models from comments, as it leaves significant
non-linguistic data.

Pascarella and Bacchelli [40] provide additional insight into different types of comments and
their classification. Their work is one of the only papers that acknowledge that not all comments
serve the same purpose and there are different types of comments encountered in the real world
code bases. For example, there are documentation-purpose comments targeted at programmers,
such as Javadoc, and there are inline comments which target internal developers — aimed at
different audiences for different purposes. Some comments are reminders, some provide rationale
for implementation choices, and some are merely separators of logical blocks. This distinction is
important because comments used for different purposes will have different structure and will
have different levels of usefulness for machine learning. The authors categorize comments by their

4https://github.com/python/cpython/
5https://github.com/sagemath/sage

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

5

function and provide statistical analysis and breakdown of the categories. However, this work
considers only the frequency of each comment category, not consequences of the frequencies.

This work on comment general quality shares two other important limitations: all of it focuses
on Java (with one exception also treating C++ comments), and the size of the corpora studies are
both small and manually curated to be of high quality.

Pandita et al. [36] evaluate on 2500 sentences. Steidl et al. [47] evaluate on 830 comments drawn
from 12 Java projects, and 500 comments drawn from C++ projects.6 Khamis et al. [22] evaluate
on comments drawn from 3 releases each of 2 large Java projects known to be of reasonably high
quality (ArgoUML and Eclipse), for a total of just over 121K comments — but this is the sum of all
comments in each release, including duplicates.

Focusing on high-quality software as exemplars of good commenting practice is useful for
identifying major kinds of comments, but side-steps the general issue of also dealing with comments
that may be inappropriate for major tasks. These projects also focus exclusively on method-level
comments, discarding information that may reside in comments inside methods.

Work on automatically generating one-sentence summaries of Java methods [24, 26, 28] uses
a larger dataset of 2.1 million pairs of Java method bodies with one-sentence comments, produced
filtering comment-method pairs from Linstead et al. [29] to select only those pairs whose meth-
ods are less than 100 word-tokens long, and whose comments are specifically Javadoc comments
between 3 and 13 words long. We discuss the trade-offs of this approach in comparison to ours
in Section 5.1. This resembles a scaling-up of Howard et al. [19]’s approach, extracting the first
sentence of every method-level Javadoc comment, yielding 20,199 such sentences, of which they
examined a random sample of 150 in depth.

By contrast, we evaluate on two Python comment corpora containing just over 3.7 million
comments, the first of which is a new large sample of publicly-available GitHub repositories
of Python code. Our data and code are available online7 and will be archived with a DOI upon
acceptance.

3 GOALS AND APPROACH
We seek to understand the distribution of code comments suitable for linguistic processing, as
well as the frequency of non-linguistic data, and how removing non-linguistic data affects the
performance of language models trained on large comment corpora. The tasks we are interested
in include suggesting comment completions, generating method summaries, or generating code
from comments or text. Training machine learning systems for any of these purposes relies on the
assumption that code is not only amenable to statistical and neural techniques drawn from natural
language processing, but is specifically (primarily) actually human language.

To this end, we have built two linguistic models for comment completion — one statistical,
one neural — and trained each four times: for baseline filtering common across any recent work
handling code comments on both an existing corpus and a new large corpus, and again on the
same corpora after designing and applying mechanisms to specifically filter identified categories of
non-linguistic data. We focus on comment completion because it isolates the effects of filtering
comments (e.g., compared to interactions filtering pairs of code and comments which may have
non-trivial correlations). We evaluate two kinds of linguistic models on two preparations of two
corpora, to evaluate the effects of comment filtering across different linguistic modeling techniques,
and using both large scale and carefully-curated corpora. This section details our data collection,
characteristics of the new corpus, the details of our analysis process in terms of the research

6Further details on the C++ data are in a MS thesis that is no longer available online.
7https://figshare.com/s/e3cd836401ecadae5b88

, Vol. 1, No. 1, Article . Publication date: August 2022.

6

Sergey Matskevich and Colin S. Gordon

questions answered (in dependency order) for our ultimate investigation. The next section reports
on our results.

3.1 Data Collection
For this project we used comments from publicly available open source projects from GitHub using
GitHub RESTful search API. We processed the 840 most popular Python projects (according to the
GitHub search API as of June 2021). In addition, we couple studies on this new corpus with an
additional Python corpus, the SRILab Python 150K dataset [43]8. While our new corpus selected
as many of the most popular and currently maintained projects as Github’s API would permit
us to retrieve, the Python 150K dataset had more restrictive criteria, restricting to only Python
files (not necessarily whole projects) with at most 30,000 AST nodes (in service to AST-based
program synthesis goals) with permissive licenses. They have files from 8,422 projects in their
dataset. Despite that our GitHub dataset contains smaller number of projects, Figure 3 shows that
our dataset actually contains more lines of code and more comments. This disparity is due to the
process of acquiring the datasets. While we downloaded top 840 popular projects from GitHub,
SRILab had a different selection criteria and represent a size-limit corpus, in addition to a selection
by the license type. The official GitHub API allows downloading only 1000 projects, however there
are some duplicate projects in the list which are forks of the same project and therefore the total
number of unique projects is less. We did not download the forks of the more popular projects as
the original projects had higher rating an were first in the download order, they were the only ones
downloaded. The larger number of projects in the SRILab dataset can only be acquired by crawling
the GitHub web site.

The two datasets overlap on 35 projects, such as several implementations of Python, PyMySql,
nltk and others. However our GitHub dataset is several years newer than the SRILab dataset, and
contains much more code overall (39% more files, 90% more lines of code, and 46% more unique
comments per Tables 1 and 3), so while the corpora are not completely independent, the overlap is
very small.

For the GitHub dataset we were able to open 208,661 files, discarding 90 in an encoding that
was not ASCII or any flavor of Unicode. From those files we extracted 2,436,575 comments prior to
filtering, 592,063 of which were Python docstrings. The SRI Lab dataset contained 149,970 files,
from which we extracted 1,276,558 comments, of which 365,083 are docstrings, and were unable to
open 28 files which were written in a uncommon encoding.

We implemented custom comment extraction logic, which parses out both Python “docstrings”
(multi-line string literals which are deemed comments based on their syntactic location) and
single-line comments (which standard Python parsers discard during parsing).

3.2 Dataset Characteristics
We give some statistics on our datasets, both to provide a general high-level characterization, and
because the statistics suggest the collected body of comments is plausibly representative of general
Python code. Figures 2 and 3 (note the logarithmic scales on both axes) show that the vast majority
of files contain only a small number of comments (less than 10) while a small number of files contain
a large number of comments in them, which can also be seen from Table 1. From the figures we can
observe that that in addition to containing small number of comments, a majority of the source files
are also small on average, containing fewer than 150 lines of code and that in general larger files
tend to have more source comments in them. Figure 2 in addition shows an interesting trend at the
bottom of the graph that is not present in Figure 3: a wider distribution of files that contain very

8https://www.sri.inf.ethz.ch/py150

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

7

Fig. 2. Statistics of how many files contain specific number of comments in Github dataset

Fig. 3. Statistics of how many files contain specific number of comments in SRILab dataset

, Vol. 1, No. 1, Article . Publication date: August 2022.

8

Sergey Matskevich and Colin S. Gordon

Table 1. Breakdown of the number comments per files and lines of code

# Comts
0
1
2
3
4
5
6
7
8
9
10
11-100
100-2000
Total

# Files
43909
17551
12127
8863
9391
7845
9718
7726
5775
4424
4100
68890
8205
208524

GitHub

# LoC LoC Avg
47.40
93.83
86.19
102.08
110.86
111.64
117.45
148.55
124.80
139.68
151.64
508.44
3037.54
4780.09

2081111
1646820
1045279
904753
1041079
875778
1141387
1147669
720700
617946
621704
22559732
12306553
46710511

# Files
22269
18903
17369
8046
7794
7384
6569
4956
4401
3586
3243
40648
4795
149963

SRILab

# LoC LoC Avg
44.00
979823
87.12
1646895
51.35
891869
93.58
752938
88.59
690436
123.23
909928
105.14
690690
116.31
576455
118.88
523181
131.79
472613
134.38
435779
439.63
10980807
2079.67
4989213
3613.67
24540627

large numbers of comments, approaching 10,000. This may be due in part to the SRILab dataset’s
focus on programs of limited size.

Table 1 shows that we separate the files into three groups by the number of comments they
contain. We inspected some of the files that belong to each category, which provided us with some
insights into what types of files belong to each group. While detailed analysis of each category is
subject to further research, we can infer some characteristics of the them.

We observed that the largest group with 10 comments or fewer includes a number of data
initialization scripts, configurations, low-level details (e.g. a set of database queries for a higher-
level API), and similar code. We found that these files are often very small, containing mostly
the object initialization and setup in main functions of programs or scripts. The small amount of
comments in these cases is likely due to the generally limited overall file sizes.

The files we observed containing 11-100 comments are generally class definitions for objects
that appear to work at a high level of abstraction (i.e., collecting functionality of other units, rather
than directly implementing tricky functionality). These examined files commonly contained many
comments that at least superficially appear to contain high-quality predominantly-linguistic data.
The files containing over 100 comments is the smallest group. The files we inspected in this category
largely consisted of library files that are defining large APIs.

The files with many comments, however, do not always contain useful comments. There are
several notable outliers in the dataset that contain more than a 1000 comments and more than
5000 lines of code in a single file, with the most notable file containing more than 80,000 lines of
code. We did not include these especially large files in the table 1 as they are outliers and are not
representative of the overall trend in our data. We did, however use these outliers in our dataset.
Some of these files contain large map definitions for encoding schemes such as font or character
sets, as in Figure 1a. They often contain large numbers of comments, but these comments are not
useful for linguistic purposes. Some other files contain definitions of unit tests with thousands
of tests in them. These files contain a high degree of similar code and comments. Figure 4 shows
one such example where the file contains over 1000 comments for unit tests that differ in only 1
or 2 words. The last category of special-case files that we observed in our GitHub dataset are the

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

9

def test_AAD_1(self):
""" Instruction AAD_1
Groups: not64bitmode
0x80702ff: aad 0xff
"""

Fig. 4. A comment from the Manticore9 project that can’t be used for NLP

source code that contains encoded binary objects. These files can have thousands of lines of code,
but most of it would be actually encoded binary. These files contain few, if any, comments and are
also largely useless for the natural language processing and language-oriented machine learning
algorithms.

This data shows our corpus has coverage of large and small projects and files, heavily commented

and lightly commented, which likely covers a variety of commenting practices.

3.3 Research Questions
Our ultimate goal is to evaluate the impact of non-linguistic data in comments on machine-learning
algorithms processing comments. However, while such an investigation clearly requires removing
such non-linguistic data, we initially had no knowledge of how to do this, or even what kinds of
non-linguistic data might exist, or what kinds might be the most problematic (i.e., recurring, rather
than having a single exceptional example). Thus we set a series of research questions to answer, in
order, to work towards such an evaluation. Each of these research questions depends on the results
of the prior research questions.

Research Question 1. What categories of comments exist that may be worth excluding from

training data for general-purpose linguistic processing for English?

Building a linguistic model that generates coherent English comments requires training data that
is (mostly) valid English prose relevant to source code. Common sense and prior experience indicate
that there are many ways to deviate from that scope. Machine learning algorithms applied to text
will readily process text filled with things a human would immediately recognize as gibberish
or at least not natural language, but will often still produce some reasonable-looking outputs
given enough valid information. However, a linguistic model trained on poor quality data will
generally produce poorer results than one trained on higher-quality data. Prior work applying
machine learning to comments typically either uses a very small hand-groomed data set of high
quality comments [22, 40, 47], or assume that some level of modest filtering is adequate [28], based
on achieving what are considered good results for the intended task [26]. However, the former
assumption is never fully explained (the criteria for the manual filtering are typically stated without
detail10) and the latter assumption is typically not validated on the actual data.

We repeatedly manually reviewed random selections of raw comments extracted via the tool
of Section 3.1 to identify common themes in comments that were clearly not linguistic in nature.
We did fix in advance that we wished to focus on English language comments — widely known
to be the most prevalent natural language in open source software development — so elected to
treat non-English natural language as non-linguistic for the purpose of building English-language
models.

Research Question 2. Can comment categories worth excluding be automatically and effectively

detected and excluded using deterministic techniques?

9https://github.com/trailofbits/manticore
10And in some cases [25] there are clearly multiple natural languages in the remaining comments.

, Vol. 1, No. 1, Article . Publication date: August 2022.

10

Sergey Matskevich and Colin S. Gordon

Identifying meaningfully different categories of comments is important, but does not necessarily
imply the good and bad comments for generating linguistic models can easily be separated, either for
our purposes or in future work seeking to account for non-linguistic data embedded in comments.
Prior work [40] has attempted to categorize comments by purpose using a neural network for a
fixed set of comment categories. However, such an approach is difficult to extend: adding a new
class of comment requires retraining the entire network based on significant additional training
data. We seek human-readable textual filters to classify comments, which may be individually
understood or extended.

For each problematic category of comment, we iteratively developed one or more deterministic,
human-auditable filtering criteria to match human-recognizable patterns in those categories. To en-
sure that each filter only matches comments in the intended category, we ran it on both files where
the relevant category had been observed, as well as randomly selected sets of other files, and then
manually inspected the data removed and left by the filter. For each category, we added, generalized,
or specialized filters for portions of that category until we were unable to (manually) locate addi-
tional examples of that category in filtered data or locate misfiltering examples in the data removed.

Research Question 3. How prevalent are the major categories of comments?

Given an effective filtering method, we can quantify how prevalent certain types of comments
are in raw source code. Beyond being of intrinsic interest, this helps evaluate how critical filtering
for specific comment classes is (rare categories are unlikely to pollute models significantly), and
can contribute to evaluations of confidence in work that does not account for specific kinds of
non-linguistic comments. We quantify how many comments match each criteria we filtered for,
further categorized by whether a comment was a Docstring or not.

Research Question 4. Are there comments which are both high quality but also not useful for

direct linguistic processing?

Literature on comment quality tends to assume comments are binary: either useful or useless.
These positive or negative traits are often assumed to be correlated with factors like size of the
comment, but there is limited empirical evidence of this. Moreover, there has been little exploration
of why comments might be inappropriate for linguistic models yet still desirable to have in source
files. For the classes of comments we filter due to being non-linguistic (or not purely linguistic), we
describe examples of meaningful comments in that category, justifying their existence.

Research Question 5. What is the impact of different levels of filtering on the quality of trained

linguistic models for comment completion?

It is well-known that the quality of data used for machine learning algorithms can have major
impacts on the quality of the learned model, and it is known that this has had impact on some
software engineering work [2, 28]. We evaluate the impact of filtering on two linguistic models for
suggesting comment completions: a 4-gram model and a neural network for sequence prediction.
We evaluated each model on two variants of each corpus:

(1) after applying basic filtering — roughly, common baseline best practices, removing special

characters and punctuation, described in more detail in Section 4

(2) after applying advanced filtering — all filters identified for Research Question 2. Here we
observed that the order of filters matter, as for example, the source code filters need to be
applied prior to removal of punctuation. We discuss is in more detail in the later section.
As noted earlier, focusing on the task of comment completion allows us to attribute all changes
in model performance on filtered data to the content of comments. Alternative evaluation tasks,
such as oracle or formal specification generation from comments [6, 14, 51] or automatic method

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

11

summarization [9, 20, 24, 27, 35] train on pairs of code and comments, leaving open the possibility
that non-linguistic comments may have a statistical relationship with qualities of code they are
associated with, which may confound the impact of comment-focused filtering.

4 ANALYSIS

4.1 Research Question 1: Comment Categories
The inspection of the contents of the comments yielded a range of non-linguistic comments,
including comments containing source code (not always Python, including LATEX, HTML, and
parser-generator syntax), comments written in a language other than English, containing no
dictionary words, or serving other special functions. The special function comments include
copyrights, licenses, encoding declarations11 in the file headers and other service-type comments
that serve purposes other than documenting the source. Our categories are listed together with
their filters (from Research Question 2) in Table 2. Some comments were highly structured, such as
the example in Figure 1b, which includes text along with a structured portion specifying intended
use of the API. We also found that many comments contain markup for LATEXor Markdown that
specify mathematical formulas and in some cases text layout, in addition to the expected non-
English human languages. While LATEXcan be identified by it’s keywords and markup commands,
the Markdown is often difficult to distinguish in automated way from ordinary comments due to
how it is structured. One important property that Markdown contributes to comments, however, is
the fact that it produces more short-length comments that do not necessarily contribute much to
the linguistic richness of a comment.

Code/Math. The most commonly observed class of non-linguistic comments contained source
code and/or mathematical equations, sometimes mixed with regular linguistic comments. In cases
where code or formulas occur embedded within linguistic text, the surrounding text likely does
have some value to linguistic models. However, this text often contains mentions of subformulas
or variables, so feeding those into training for linguistic models is of unclear impact. Comments
containing code were sometimes clearly intended to document behavior, but in many other cases
appeared to be simply commented-out code left in the repository. Mathematical equations likely
contribute useful information for human readers, but are of no value to linguistic models of comment
text.

We decided to group these two large categories together due to their similarity in representing
some technical notation and because many comments contain both, as in Figure 1b. In some cases
a comment might contain LATEX which formats a math equation together with the source code
example for this equation. We have subcategorized this group according to the type of code or
math they contained, highlighted in Table 2.

Non-English. Many comments consist primarily of non-English words/characters. Comments in
other languages have clear value to linguistic models for those languages, but our work is limited
to English. We observed comments in French, Italian, Chinese, Spanish, and other languages.

Copyright. A significant number of comments simply state copyright or license information for
a file. As these comments do not actually describe the source code, we are not interesting in them.
Moreover, they are often identical across most files in a codebase. Feeding this sort of repetition to
a linguistic model would obviously skew the model towards copyright and license statements.

11Hints to editors indicating the correct encoding for the file, common in UTF-8 source files.

, Vol. 1, No. 1, Article . Publication date: August 2022.

12

Sergey Matskevich and Colin S. Gordon

Non-linguistic. Some of the comments consist only of punctuation symbols (e.g. separator com-
ments) or contain ASCII art. Some of these symbols are not part of the standard punctuation
removal (e.g. ˘¶

all appeared in the comments).

Other. There are also many file encoding directives extracted as comments, for example several
forms of # -*- coding: utf-8 -*- are common. These type of comments provide no linguistic
value and Other examples include commented out ASCII hash values, or encodings of text in a
non-readable format.

Duplicates. Even from manual inspection, setting aside copyright and licensing statements, it

was readily apparent that there were many duplicate comments.

4.2 Research Question 2: Effective Filtering

For each filterable category, the set of filters we developed are shown in Table 2. To identify
(typically, Python) code within comments our filters look for patterns such as assignments and
method calls. We also used separate patterns to match a variety of formal languages nested inside
comments: HTML, ANTLR [39] parser-generator comments (indicating generated code), LATEX,
and prefixes for the way SageMath represents example interactive Python sessions in comments.
We also match 32- or 64-character hexadecimal strings, which typically indicate hashes of objects
outside the program that are not useful for linguistic models, like git commit hashes, IPv6 addresses,
or hard-coded smart contract addresses.

To identify a non-English text we used a combination of checking for non-ASCII characters
(though this is not tenable for languages whose writing systems use accents, umlauts, diareses,
etc.), building a classifier with the NLTK [5] toolkit for implementing NLP tasks in Python, and
using the langid.py [30] natural language classifier. Where langid.py was unable to give clear
classification, we complemented its results by checking with the English language dictionary from
NLTK, and checks on character encodings. This filter kept only comments where all 4-grams
(subsequences of 4 words) were classified as English (lang.py is significantly more accurate on
fragments than individual words).

A separate discussion is warranted in regards to the order of the filter application. As filtering is
destructive by nature, applying a complex set of filters also requires special ordering in case they
overlap. For example, removing punctuation (a standard pre-processing step for training models on
text) will remove some of the indication that a comment contains code or technical markup. As we
can see from table 3, source code is one of the largest prevalent category of non-linguistic type
of comment, and thus failing to remove it will have a large impact on the resulting data. Another
example where the order of the filters matter is the encoding directive # -*- coding: utf-8 -*-.
Here, stripping the punctuation first reduces the size of the comment to two words, which will be
removed by the length filter in the next step. Reversing the filtering order in this case will result in
failing to filter this directive.

There was one category of non-linguistic comments that we could not effectively filter: license
clauses. In contrast to copyright statements, which surprisingly can be filtered with high accuracy
by simply looking for the word "copyright," we found no effective means of filtering all (or even
most) and only license clauses. All license clauses we identified include the word “license” but
this alone is too coarse a filtering criteria, removing many legitimate comments (e.g., dealing with
software usage licensing).

4.3 Research Question 3: Category Prevalence
Based on the filters developed for the previous research question, Table 3 gives counts of how many
comments fall into each category. The numbers include duplicates (i.e., the number of Docstrings

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

13

s
g
n

i
r
t
s
c
o
D

l
l

A

s
g
n
i
r
t
s
c
o
D

l
l

A

s
g
n
i
r
t
s
c
o
D

l
l

A

s
g
n
i
r
t
s
c
o
D

l
l

A

s
e
t
a
c
i
l
p
u
D

t
u
o
h
t
i

W

s
e
t
a
c
i
l
p
u
D
h
t
i

W

s
e
t
a
c
i
l
p
u
D

t
u
o
h
t
i

W

s
e
t
a
c
i
l
p
u
d
h
t
i

W

b
a
L
I
R
S

b
u
H

t
i

G

s
e
i
r
o
g
e
t
a
C
t
n
e
m
m
o
C

.

3

e
l
b
a
T

s
e
i
r
o
g
e
t
a
C
t
n
e
m
m
o
C
g
n
i
r
e
t
l
i
F

.

2

e
l
b
a
T

'
)
}
\
}
5
,
0
{
]
w
\
[
{
\
f
b
h
t
a
m
?
\
\
\
\
(

|
)
?
}
\
)
a
d
b
m
a
l
|
a
g
e
m
o
|
a
m
m
a
g
|
a
t
e
b
|
a
h
p
l
a
(
\
\
?
{
\
(
|
)
}
\
+
w
\
{
\
n
i
g
e
b
\
\
(
'

s
e
c
n
e
u
q
e
s
h
t
a
m
X
e
T
a
L
n
o
m
m
o
C

X
e
T
a
L

h
t
a
M
/
e
d
o
C

'
)
\
*
.
(
\
+
]
_
d
\
w
\
[
?
.
\
+
]
_
d
\
w
\
[
*
s
\
)
?
.
\
|
}
2
,
1
{
=
(
*
s
\
+
]
d
\
w
\
[
'

.
c
t
e

.

.

.
)
.

.

.
(
o
o
f
.
j
b
o

.

.

.

s
e
h
c
t
a
M

e
t
i
s

l
l
a
C

:
t
n
e
m
m
o
c

n
i
"
t
h
g
i
r
y
p
o
c
"

f
i

t
h
g
i
r
y
p
o
c

r
o
f
h
c
t
a
m
e
v
i
t
i
s
n
e
s
n
i
-
e
s
a
c

-

t
h
g
i
r
y
p
o
C

n
o
i
t
i
d
n
o
C
R
O
n
o
i
s
s
e
r
p
x
E
r
a
l
u
g
e
R
n
o
h
t
y
P

n
r
e
t
t
a
P

y
r
o
g
e
t
a
c
b
u
S

y
r
o
g
e
t
a
C

'
+
]
d
\
[
*
s
\
}
2
,
1
{
=
*
s
\
+
]
d
\
w
\
[
'

s
n
o
s
i
r
a
p
m
o
c
d
n
a

s
t
n
e
m
n
g
i
s
s
a

s
e
h
c
t
a
M

t
n
e
m
n
g
i
s
s
A

'
}
4
6
,
2
3
{
]
9
-
0
f
-
a
[
'

s
g
n
i
r
t
s

x
e
h
r
a
h
c
-
4
6

r
o
-
2
3
h
c
t
a
M

s
e
u
l
a
V
h
s
a
H

'
?
)
$
}
n
r
e
t
t
a
p
_
\
e
t
i
s
l
l
a
c
{
t
i
h
t
a
m
\
$
(
*
s
\
:
e
g
a
s
'

s
t
p
r
e
c
x
e

e
v
i
t
c
a
r
e
t
n
i
h
t
a
M
e
g
a
S

h
t
a
M
e
g
a
S

w
o
l
e
b
n
o
i
t
p
i
r
c
s
e
d
e
s
o
r
p
e
e
S

'
:
e
p
y
t
|
R
L
T
N
A
$
\
'

'
>
*
]
>
^
[
<
'

s
c
o
d
r
o
t
a
r
e
n
e
g
-
r
e
s
r
a
p
r
l
t
n
A

s
g
a
t
L
M
X
/
L
M
T
H
p
i
r
t
S

r
e
fi
i
s
s
a
l
c
&
y
r
a
n
o
i
t
c
i
D

L
M
T
H

r
l
t
n
A

-

h
s
i
l
g
n
E
-
n
o
N

8
6
1
3

4
6
8

2
1

4
6
3

0

3
2
6
1

1
6
6
9

0
3
0
1

4
0
2

0

9
6
2
0
5
5

9
6
7
6
5
8

1
6
8
5
1

2
9
7
4
1

2
3
4
6

7
6
0
1

2
1

9
7
1
5
3

1
5
2
2

8
9
0
2

8
0
4
7

1
1
9
1

3
2
9
2

5
8
8
8
2
2

3
9
6
5
4
4

0

2
6
8
1
2

8
8
3

2
6
7
1
2

2
4
0
5
2

0
1
2

7
4
0
1

3
9
6
2
3
1

2
3
7
3
3
1

8
2
1
0
7
1

0
7
7
6
7
1

8
5
2
6
2
1

2
3
3
2
2

4
4
5
9
2
2

0

7
2
6
7

9
0
5
9

3
6
6
1

5
6
9
6
4

8
8
0
3

4
9
4
4

9
8
7
5

3
2
7
6
8
1

3
1
3
1

9
4
2
0
4

6
8
9
2
8
6

9
2
9
1

4
9
5
6
7

0
2
0
4
7

8
1
1
6
6

5
5
8
0
6

8
5
2
0
2
3

h
s
i
l
g
n
E
-
n
o
N

h
t
a
M
/
e
d
o
C

t
h
g
i
r
y
p
o
C

c
i
t
s
i
u
g
n
i
L
-
n
o
N

6
7
4
6
1
2
1

s
e
t
a
c
i
l
p
u
D

L
M
T
H

x
e
t
a
L

l
a
t
o
T

5
7
9
1
8
7

2
6
4
2
0
3
1

4
8
6
0
1
8

7
2
9
6
5
2
1

9
6
6
3
9
4
1

3
0
4
3
7
4
2

, Vol. 1, No. 1, Article . Publication date: August 2022.

14

Sergey Matskevich and Colin S. Gordon

containing copyright statements includes duplicates). We have also encountered a total of 118 files
that could not be easily opened because the files were not in a typical latin1 or unicode encoding.
After inspecting these files in our dataset, they did not contain any useful comments, so we chose
to ignore them. However, this is also an important factor because some comments within a code
base might not be encoded in the same encoding as the whole file and will produce gibberish text if
processed as-is: common recommendations for text processing in Python suggest opening the file
with a specified encoding and a directive to ignore bytes that do not conform to that encoding,12
which can result in missing characters that could be important for accurate processing of comments
(or in other work, code).

4.4 Research Question 4: Value of Non-Linguistic Comments
For some categories identified in previous research questions, the need is obvious: license headers
dictate how code may be reused, copyright headers indicate the claimed copyright owner. Non-
English comments presumably have similar uses to English linguistic comments, but for those
reading the appropriate language. Strictly speaking non-English comments deserve further follow-
up, as given the English-centric nature of computing, it is reasonable to expect non-English
comments to at least seek to meet some additional needs like translation or re-explanation [12, 15].
This leaves the code and math category. Discussion in Section 4.1 explained that comments
belonging to these categories frequently overlap. A good example of this is shown in Figure 1b,
which shows a comment from the SageMath13 project, is a large library used for computational
mathematics. Specifically, the figure shows the start of the display method on the MixedForm class used
to represent mixed forms of differentiable manifolds. (The comment text has been slightly re-flowed
to fit in the column, in ways that do not affect significant whitespace.)

This comment shows a number of interacting features of specialized comments. First, this
comment contains both Python code snippets and a mathematical formula. Moreover, both code
and formula are nested within the larger comment whose overall structure is dictated by the Sphinx14
documentation tool, which assumes comments are ReST (ReStructured Text)15 formatted (akin to
Markdown) with sections specific to the SageMath project. The build process for SageMath processes
the comment above into visually-appealing API documentation16 including syntax highlighting for
the embedded code sample. Note that Python, unlike Java, has no language-standardized equivalent
of Javadoc which all comments may be assumed to be written in; Sphinx is an independent project
from Python itself. Other examples are more complex, containing additional documentation sections,
and even embedded LaTeX math markup.

SageMath may be an outlier in this regard: it is one of the largest Python projects in our corpus,
and with a userbase consisting largely of professional mathematicians who also write Python code,
it may not be representative of Python developer practices in general. However, even Python’s
standard library contains examples with some kinds of structure, such as Figure 5’s documentation
of a method in an implementation of LOGO [37] in Python.

In both of these cases, and many more, the docstrings provide good descriptions of the relevant
method, its arguments, and (via examples formatted as embedded code) its usage. This is essential
information for high quality comments [22, 40, 47]. However, these comments are not directly

12e.g., https://stackoverflow.com/a/46781624
13https://www.sagemath.org/
14https://www.sphinx-doc.org/
15https://docutils.sourceforge.io/rst.html
16https://doc.sagemath.org/html/en/reference/manifolds/sage/manifolds/differentiable/mixed_form.html#sage.manifolds.
differentiable.mixed_form.MixedForm.display
17https://github.com/python/cpython/blob/master/Lib/turtle.py

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

15

Fig. 5. Example of a good comment that can’t be used for text generation17

useful for linguistic models; they contain some short snippets of text, but they are intermingled with
more structured non-linguistic markup or ad hoc formatting. Separating mixed natural and formal
language is a distinct, heavily studied, open problem in computational linguistics [49]. Even parsing
mixed structured input languages that can be combined is a difficult problem [4]. This suggests that
making effective use of structured comments like these in training linguistic models, or further,
training systems that can understand these structured comments without being explicitly coded
for the particular structure (e.g., Sphinx comments), represents a significant research challenge.
There are more of these than duplicates in raw data, suggesting this is a challenge worth tackling.

4.5 Research Question 5: Filtering Impact
It is well-known that data quality can significantly impact the results of machine learning algo-
rithms, so in practice any attempt to use machine learning will include some baseline cleanup of
major issues (i.e., ones that prevent even providing input to the algorithm). However, given that
some prior work applying ML to large code samples did not even address issues with duplication
(Allamanis [2] gives examples and a comprehensive study), and the establishment of a general
taxonomy of comment types is an active topic of research [40], it is unclear what baseline filtering
of comments might look like.

For this reason, we implemented the two levels of filtering mentioned in Section 3.3, and evaluated

two machine learning techniques on two filtered datasets:

• Basic Filtering has been filtered with minimal best practices, and removes comments which
would cause problems with some tokenization approaches. The basic filtering corresponds to
standard data cleaning practices such as normalizing text to lowercase, removing punctuation
(including deleting URLs and stripping HTML tags, removing extra blank lines and extra
white space, removing some special symbols (also including comments that contain nothing
but symbols), and removing duplicates. We also removed comments that were fewer than
10 characters in length or fewer than 4 words, and all comments matching the copyright
pattern. Just doing this filtering reduced the number of comments from our Github corpus
to 1,652,974, a 66.5% reduction. However, as our results show, this result is far from being a
properly cleaned dataset. Table 1 shows that in addition to just punctuation, the comments
contain a number of code snippets and non-English comments, which this filtering ignores.
• Advanced Filtering is first normalized to lowercase, then comments matching any of the
patterns in Table 2 are dropped, then URLs and miscellaneous symbols are removed. Applying
filters prior to general removal of punctuation is crucial because many filters make critical use

, Vol. 1, No. 1, Article . Publication date: August 2022.

16

Sergey Matskevich and Colin S. Gordon

of punctuation.18 After this filtering was performed the Github dataset was further reduced
by 22.7% compared to the basic filtering (74.2% compared to the raw data), with 1,277,176
comments remaining.

We do not claim that what is here called “advanced filtering” is optimal for removing all non-
linguistic comments, even for our dataset (see our discussion of limitations in Section 5). A deeper
study of reliable filtering methods for more complete comment type taxonomies [40] could improve
the quality of our data even further. The term advanced refers to the level of sophistication compared
to basic filtering.

To evaluate the impact of this filtering, we evaluated the quality of two kinds of comment comple-
tion models on each level of filtering (basic, advanced) for each corpus (our Github corpus, and the
SRILab Py150 corpus). First, we built linguistic models with Python Natural Language Toolkit [5]
using 4-grams with the Naive Bayesian [31] probability distribution, as n-gram performance is well
understood and the model can be easily explored. Second, we implemented a modestly-sized neural
network tailored for sequence prediction. As mentioned earlier, we chose comment completion as
a task because it depends exclusively on comment filtering (and not the quality of some adjoined
dataset, like corresponding code fragments [28]), and because it depends largely on the linguistic
coherence of the data (as opposed to depending on non-linguistic but otherwise useful comments).
For each of the datasets and each filtering approach we extracted a set of 10,000 sentences at random
prior to the learning process. We extracted prefixes from these sentences to use for evaluating the
comment completion models. The same set of prefixes was used for the corresponding 4-gram
model and neural network model.

4-gram Linguistic Modeling. In the experiment we generated 10,000 sentences from each of
4.5.1
the models. The generation process was the same for each model: for every prefix search for the next
word until either no words are left or the produced sentence is longer than 40 words (as otherwise
the model would generate infinite sequences for some seeds). The prefixes were randomly selected
from each dataset and the sentences they were extracted from were not included in the learning
process. Because of this experiment we only wanted to see only the highest quality sentences, we
only selected matches such that 𝑝 (𝑤4|𝑤1, 𝑤2, 𝑤3) >= 0.9, where 𝑝 is posterior probability and 𝑤𝑖
are words in the order they appear in a sentence. If the model prediction for the next word was less
than that threshold, generation was stopped.

For each model, we classified how many of the 10,000 generation attempts produced sentences of
length 4 (i.e., how many sentences the model generated no extension for), how many contained code
or formula fragments, and how many were grammatically correct according to an English-language
parser. Our results are shown in Table 4.

The categories in the table are the following:

• Length 4: the number of generated sentences of length 4.
• Code Artifacts: the number of sentences, containing elements that came from code or an

equation.

• Non-English: the number of generated sentences with non-English words in them, or an

entirely non-English sentence.

• Unparsed: the number of sentences that the were grammatically incorrect and were not

able to be parsed by a parser for English language.

If the generated sentence is of length 4, it means that the model could not generate anything for a
given prefix. However, it is needed to note that some of the prefixes are grammatically complete

18Note that this means the first pre-processing step applied by most NLP-with-comments work removes information that is
important for filtering non-linguistic data.

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

17

Table 4. Statistics for the generated sentences for each model. The % 1 column shows the percentage of the
Bleu scores of 1 in specific categories

GitHub
Basic % 1 Advanced % 1
Category
81
353
Length 4
612
-
17.18
Average Length 16
23.8
94
804
Code Artifacts
-
0
39
Non-English
-
13
95
Unparsed

90.2
-
19.7
37.5
-

SriLab
Basic % 1 Advanced % 1
79.8
436
353
-
16.65
15.52
18
82
678
-
0
6
-
11
62

87
-
22.5
16
-

phrases and would not necessarily need to have any additional words generated for them. For
example both GitHub models produced a phrase “unpopular query goes viral” while both SriLab
models generated sentences “parameters for removing interfaces” and “call after posting auth.” All
of these are prefixes given to the models and while not being proper English sentences, can both
serve as comments for the source code. On the other hand, there are many cases where the basic
model generates a nonsensical sentence and the model with the advanced-filtered dataset generates
a somewhat readable sentence. For example (prompt in italics):

• Github Basic: it can be used with tfdatadatasetmap to apply class weighting
• Github Advanced: it can be used to read a specific corpus format

The Code Artifacts category is of special interest here and it also has some overlap with the
sentences of length 4. As the models are trained on the filtered datasets, all punctuation and the
structure (formatting of comments that can be code-like with many spaces and tabs in them) are re-
moved. Therefore it is very hard to tell if a particular sentences actually was a part of the source code
or an equation or not. However, we can still see artifacts that appear in many of these sentences that
can show that they came from a code or an equation. Examples of these artifacts would be strings of
the form x 5, which likely came from the assignment x = 5. Another example of an artifact are com-
posite words that could be either variable or function names like classplexapiphotophotoalbum,
which came from the comment Return the photo’s :class:‘ plexapi.photo.Photoalbum‘.
Strictly speaking, this is not a code, but a reference to a named object, therefore we did not explicitly
filter these kinds of comments even during the advanced filtering step. As a result, the linguistic
model with advanced filters applied has some artifacts as well, which came from variable names
and named objects. These items can be processed further with more advanced NLP methods and
add more semantical knowledge to language model. For example, [16] uses named objects and
function names in their generation process.

In addition we can see that having even small numbers of non-English words in the dataset with
the basic filtering applied will have impact on the training model as we have still generated several
sentences with non-English words in them by randomly sampling the dataset.

As a coarse automated measure of text quality, we attempted to parse each generated completion
with a modern English-language parser. We chose the depccg parser [50] that was trained on
CCGBank [18] — a large natural language corpus from the Wall Street Journal, with words hand-
annotated for part of speech [32]. This parser tends to only parse grammatically correct sentences,
unlike statistical parsers like Stanford NLP Stanza [42]. As table 4 shows, the models trained after
advanced filtering produce more grammatical sentences. This does not imply the sentences were
semantically sensible, but that is much more difficult to evalute.

Some sentences in the basic model were parsed completely even though they contain source code
artifacts. The main reason for this is that most modern parsers use probabilistic parsing models

, Vol. 1, No. 1, Article . Publication date: August 2022.

18

Sergey Matskevich and Colin S. Gordon

and have some room for parsing never-seen before words. In case a sentence contains one or two
unknown words to the parser, it can probabilistically assign a role to these words (e.g. a noun or an
adjective). Consider:

• S1: name attrs angle anglea90angleb0rad00 angle3 anglea90angleb0 arc

anglea0angleb0armanonearmbnonerad00 arc3 rad00 bar
arma00armb00fraction03anglenone note that 3 6 2 8 8 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0

• S2: rabstract base class for 2d lines using the estimated model

modelclsresidualsdata all data samples with residuals smaller than the maxlen

S1 is a sentence that was not able to be parsed while S2 is a sentence that was parsed, despite

containing several non-words.

Finally, as another means to assess overall model performance, we computed 4-BLEU scores [38],
which we produced using the NLTK toolkit. This is a metric commonly used for evaluating the
quality of predicted text compared to a baseline, frequently used for evaluating automatic comment
generation [9, 27]. The scores for all models for GitHub and SRILab datasets can be found in Figure 6.
From the figures we can observe that the models produced from the basic filtering datasets generate
many sentences with BLEU score 0, indicating the result was highly textually dissimilar from the
original comment. However the models produced from the advanced filtered datasets have few
with 0 scores, and the average BLEU score of the generated sentences is higher. Note the y axis of
each plot differs. Another observation in the data that the total number of sentences with the BLEU
score 1 went down slightly in the advanced filtered dataset. This, however, does not necessarily
mean that the overall quality of the comments went down. There are several contributing factors
to this phenomenon, some of which can be observed in the Table 4. One of the major contributor to
the BLEU scores of 1 are the sentences of length 4 while the other are sentences with code artifacts
in them. The “% 1” columns in Table 4 give the percentage of results in that category (for counting
categories) that had a 4-BLEU score of 1, meaning it perfectly predicted the original comment.

We can see that in our datasets short generated comments of length 4 tend to match the original
comment. The raw datasets contain 6.7% and 7.2% of comments of length 4 for GitHub and SriLab
respectively. Additionally, the average length of the comments across all datasets converges to 9
words. We can see from table 4 all models generate comments that on aeverage are twice as long
as the average comments in the raw dataset. This also explains why shorter comments will have
higher BLEU scores and why so many of the generated sentences of length 4 have a BLEU score
of 1. As our 4-gram linguistic model tends to generate longer sentences than the raw dataset, the
BLEU scores for it will also drop.

Code artifacts also contribute greatly to the set of sentences with BLEU score 1. Even though
only about 20% of them have a BLEU score 1, because of their number, each contributes several
hundred sentences to the count. As advanced filtering remove majority of the source code and
source code artifacts, the number of sentences with BLEU score 1 will go down as well.

Combining the two facts above we can explain why better filtering can generate lower BLEU
scores: it produces longer sentences and generates fewer sentences with unique words like non-
English words or code artifacts. However, the generated sentences are also of a higher quality, and
upon visual inspection most of them can be read and understood, which is not the case for the
model with only basline filtering.

4.5.2 Deep Learning Evaluation. We also trained a small neural network to predict the next word
based on up to 10 previous words. For each level of filtering (basic and advanced), we trained a
neural network implemented in Keras, encoding the top 10,000 words in each dataset (plus an
‘unknown’ token, and tokens for the start and end of a sentence). Thus unlike the 4-gram model,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

19

Fig. 6. Distribution of the BLEU scores for all datasets.

this model does not perfectly memorize the vocabulary of its training data. The network consisted
of 5 layers:

• A 32-dimensional embedding layer
• A GRU [10]19 layer returning sequences
• A GRU layer not returning sequences
• A dropout layer (20% dropout rate)
• A dense layer of size 10,003, using softmax activation

This particular network ends up with 662,932 tunable parameters. The network was trained with
the Adamax [23] optimizer, using categorical crossentropy as the loss measure, for a maximum of
150 epochs, but set to stop training if three consecutive epochs failed to yield improvement.

For each dataset (corpus subject to a given filtering level), the model was trained on a randomly
sampled 70% of subsequences ending in a word in the dictionary (top 10,000 tokens), another 15%
was used for validation, and the remaining 15% was used for testing.

Table 5 reports statistics for each dataset: the number of productive training epochs, and accuracy

of the model on the final round of training, final round of validation, and on the test set.

19Like the slightly better-known LSTM [17], GRU layers are for long range dependencies. GRUs generally train faster with
similar performance to LSTMs.

, Vol. 1, No. 1, Article . Publication date: August 2022.

20

Sergey Matskevich and Colin S. Gordon

Table 5. DL Training on Prediction

Dataset
Productive Epochs
Final Training Accuracy
Final Validation Accuracy
Test Accuracy

SRI Lab
Basic Advanced
143
22.54%
23.23%
23.34%

105
20.53%
21.35%
21.27%

Github
Basic Advanced

86
23.81%
24.65%
24.56%

112
20.86%
21.73%
21.64%

Table 6. Statistics for the generated sentences for each model

Category
Length 4
Average Length 8.7
179
Code Artifacts
28
Non-English
14
Unparsed

GitHub
Basic % 1 Advanced % 1
14.4
2362
-
7.7
-
-

2142
9
36
0
12

22.8
-
9.4
0
-

SriLab
Basic % 1 Advanced % 1
15.4
2394
-
8.5
10.3
174
-
3
-
9

2439
7.5
26
0
5

24.1
-
16.1
0
-

We repeated the same evaluation on completing 10,000 sentences as for the 4-gram models, using
the same 10,000 prefixes for each model as were used for the corresponding 4-gram model trained
on the same data. Table 6 shows statistics for the neural network model analagous to the 4-gram
results in Table 4. We can see a few notable changes by using different generation model. For one,
there are significantly more short comments of length 4. In addition, the neural network models
on average generates sentences that are closer to the average length of comments in the original
dataset. We also see a large reduction in generated code artifacts from the same datasets.

However, we also see that a total number of BLEU scores 1 went down across all categories.
There are several reasons for this. The neural-network-generated sentences contain more four-word
sentences than the original source comments. For example the basic filtered GitHub sample contains
only 696 sentences of length 4. Which means that the neural network failed to generate longer
sentences for majority of the results. The situation reverses for the source code artifacts - the
original datasets contain more sentences contain more source code artifacts compared to what was
generated by the neural network.

There two most likely reasons for this and both are contributing the behavior we observe. One is
that neural network does not work on probabilities and selects the next word in a different manner,
compared to a 4-gram model. The second reason is that neural network limits it’s vocabulary during
the learning process. Due to the fact that we do not lemmatize words in the learning process for
the neural network model, it treats all forms of the same word as a different word. It is less of a
problem in the 4-gramm model because it utilizes the full lexicon that was provided in the source
dataset. However, for the neural network model the quality of the original source plays a significant
role. We retrained neural network with the lexicon twice as large (20,000 words rather than 10,000)
on one of the datasets and the average generated sentence length only increased by one word, the
number of four-word sentences increased by two hundred and the number of generated code went
down a little.

The difference in the way the lexicon is stored between two models plays a large role in this case.
The 4-gram model is biased towards low-frequency words in two ways. On one hand low-frequency
words will have a lower chance to be generated if they appear as a derivation candidate. However,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

21

Fig. 7. Venn diagram of dictionary overlap for neural networks

if they appear in the prefix, then they will increase the likelihood that a specific sentence will be
generated. This is the reason why the BLEU scores in general are higher for the 4-gram model: the
unique code artifacts and other words in the prefix that are not part of a standard English language
cause the generated sentences to resemble the original more often. In the neural network case,
however, low frequency words are just dropped, causing the model to lose many candidate words.
In return, however, we can see that the number of unparsed sentences went down significantly and
overall the sentences generated by the neural network model are more grammatically correct than
the ones generated by the 4-gram model. Therefore properly filtering the dataset and reducing
the number of the rare words will cause a large increase in the overall quality of the generated
sentences in this model.

Figure 7 shows the 4-way Venn diagram of the four models’ dictionaries. It shows that nearly
there are 7,154 words in common across all models (this includes the start- and end-of-sentence
tokens, as well as the unknown-word token). The advanced-filtered datasets have more unique
words shared with no other models at all (866 for advanced filtering of the Py150 dataset, 942 for
advanced filtering of our dataset). Within each data set, the top 10,000 words (plus 3 special tokens)
have an overlap of 7154+424+902=8480 words for the Py150 dataset, and 7154+59+820=8033 words
for the new Github dataset — i.e., advanced filtering of either dataset drops 19.7% and 15.2% of the
vocabulary for each dataset, showing that a substantial number of common tokens occur heavily in
non-linguistic contexts.

5 DISCUSSION
One fundamental question that can be asked about automatically generating comments is whether
there is a need to distinguish the comments that are read by a human vs. read by a machine
learning algorithm. During our work we found that source comments can have many unpredictable
non-linguistic contents, and these can have noticeable impacts on language generation systems
trained on this data unless these items are specifically accounted for. Great examples of this kind of

, Vol. 1, No. 1, Article . Publication date: August 2022.

22

Sergey Matskevich and Colin S. Gordon

items can be seen in figures 1a and 4. In both of these cases comments have properties that other
researchers identify as useless by their algorithms [47]: they are short, and repeat only what the
code does. However, we can see from the example that in both of these cases the comments are
useful, as one explains a character encoding code value in English while the other explain exactly
what CPU instruction is being tested by an emulator.

These examples on their own do not provide any linguistic utility, as neither contains full
sentences, nor do they contain grammatically correct English phrases. However, even with the
context of the surrounding source code the algorithmic value of these comments is questionable. If
we learn that a u’\x8e’ character corresponds to a CONTROL in this case, it doesn’t mean that it
will be correct in a different scenario. We found that our GitHub dataset contains a total of 163 files
with encoding tables and in some character encodings map this symbol to a different character. In
case of the CPU instruction test example in figure 4, the test and the comment would not be valid
for a different architecture.

Using the data from our experiments we can surmise that it is, in fact, not clear in many cases
if a particular comment will be useful for natural language generation or not. Source comments
consisting of a proper English language without other inclusions will be very useful for machine
learning algorithms (assuming it is consistent with the source code it describes). The more interest-
ing question is whether the comments that include other data, such as formulas can eventually
also be used without detriment to the resulting language generation model.

Our hypothesis is that comments with the source and math formula inclusions are of limited use
in the form they are written. However, they potentially can be transformed into a different form
that can be more useful in building linguistic models. Suppose a comment contains an equation x
= 5. It is possible to translate it into an English phrase that will fit in the context where it’s used.
However, this problem of translating formulas to text is still open and hard to implement, therefore
further research is required and the full discussion of translating formulas to text is outside of
scope of this paper.

In our experiments we have identified several problems that researchers need to be aware of if

they use comments as an input to machine learning algorithms.

More than a half of the comments extracted were docstrings, the rest are single-line comments.
Because Python’s default parser recognizes these multiline comments simply as strings in the
parse tree, extracting them requires implementing a specialized parser (which we have done).
This problem is a Python-specific due to the way it standardized comments, but should not be
overlooked. Having or missing this amount of data can significantly impact corpus quality.

Overall we observe from the data that comments in Python projects contain a significant amount
of material that is inappropriate for use in linguistic processing. As pointed out by Allamanis [2],
a high number of duplicates can have up to 50% impact on the accuracy of algorithms trained
on such skewed data, when observing duplication rates of just 25%. Our GitHub corpus contains
about 40% of duplicated comments and SriLab corpus contains about 35% of duplicated comments.
Deduplication plus our filtering removed 74% of raw comments (advanced filtering). Much of the
duplication came from copyright notices, or editor directives such as # -*- coding: utf-8 -*-
(dropped due to length), as well as comments on standard “boilerplate” functions, such as getters,
setters, methods taking user input, and methods processing and converting data. Many of these
occurred both repeated verbatim, or in slight variation.

The closest point of comparison on gathering a comment corpus is the work of LeClair et al.
[26], LeClair and McMillan [28]. They prune a set of over 50 million method-comment pairs down
to 2.1 million, on the basis of natural language, method size, use of Javadoc, and comment length.
The described filtering process would simultaneously rule out useful data from comments not using

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

23

the /** lead for Javadoc comments and from larger methods, and would permit comments removed
by the filters in Table 2.

And finally, in case many filters/steps are being applied to processing source comments, a special
care needs to be taken in regards to the order in which the filtering is applied. Different order can
make some filtering/processing steps useless or remove candidates that should not be removed
otherwise.

5.1 Limitations
All of the filtering patterns have the potential to discard comments containing valuable natural
language information. The code/math filters specifically target examples including punctuation
or markup language, so identify comments which at a minimum contain a mixture of English and
formal text. We believe this is acceptable given the difficulty of separating mixed English and formal
text (see Section 4.4). Classification of natural language is inherently heuristic, given words that
belong to multiple natural languages (sometimes with unrelated meanings). The character encoding
check in particular runs the risk of, for example, removing English comments containing Unicode
identifiers. In our evaluation, we find relatively few comments are removed by the English filter.
In general, evaluation of the quality of a generated sentence is a difficult problem in itself. The
question also has different answers depending on the context of the generation task. For example a
sentence generated as a part of a story will be evaluated differently compared to a sentences that is
a part of a source code comment. The quality of the source code comments is a widely discussed
topic and there are many criteria used, such as relevance of the comment to the source it describes.
In our case we are only interested in evaluating the contents of the source comments on their
own as English sentences. The widely-used metric for this kind of evaluation is BLEU score, but
it only evaluates if generated sentence matches the "golden standard" which might or might not
be a high-quality sentence itself. BLEU scores have some issues and can be misleading if used by
themselves [7, 8, 41, 48]. We also used parsing as a means to check the level of grammaticallity
of the sentences, however grammatical sentence might not be evaluated as a high quality. As an
example, a short grammatical sentence might be viewed as too simplistic. Overall the definition of a
"high-quality" sentence is a little blurred and the current latest work evaluates generated sentences
by their syntactic or semantic similarity to the original. Our goal was evaluation of the general
quality of a sentence and thus most of the current evaluation methods are not really suited and we
are limited in our analysis by the technology.

Our evaluation considers the largest bodies of general source code comments we know of, which
is a double-edged sword. On one hand, its size and scope balances out the fact that prior work has
evaluated on comment data that is both limited in size and biased towards high-quality (for human
use) comments — these are the reasons we chose the corpora we used. On the other hand, selecting
the 840 most popular Python projects on Github naturally selects for other biases. The projects
studied are open source; as usual for such cases, proprietary software may have different trends.
The projects were selected based on popularity, not any measure of quality, which is generally
unknown, so the corpus may include pathological cases well outside the minimum level of quality
where we might reasonably expect comment processing or generating tools to function well. This
is balanced out to a degree by also evaluating on the SRILab Py150 dataset, which is smaller, but
also sizeable and selected on very different criteria.

LeClair et al. [26], LeClair and McMillan [28] make a point of removing auto-generated code
from their corpus by removing any file containing “generated by”, following Shimonaka et al. [45].
This filter is extremely coarse. There are 20,001 occurrence of this string in the 19,268 Python
files in our GitHub dataset, of which a large number are meaningful comments (e.g., comments
discussing data generated by other systems or the method by which a shape is being generated). For

, Vol. 1, No. 1, Article . Publication date: August 2022.

24

Sergey Matskevich and Colin S. Gordon

those clearly indicating generated source code, the mentioned tools do not appear to add significant
numbers of comments to source code, so we left these in our data. However, this may not be a
generalizable trend, and may in fact be specific to the tools used to generate Python (vs. Java) in
our dataset. At the same time, we observe some auto-generated code (e.g., from ANTLR) which
lacks this phrase, so this was already an imprecise filter.

Our process for answering Research Question 1 (non-linguistic categories) relies on manual
analysis of a randomly selected subset of the corpus, which naturally admits the possibility that
other interesting categories of non-linguistic comments may have been missed. Likewise, our
process for Research Question 2 (filters for those categories) also relies on manual inspection of the
filters’ effects on the subset from Research Question 1 and heuristic manual searches through the
remainder of the corpus, which may have missed comments which were incorrectly filtered (or not
filtered).

Our evaluation of Research Question 5 considers two kinds of techniques, but only one goal task:
comment completion. Evaluation on a different kind of task (e.g., oracle synthesis, or generating
a comment from code) may yield different trends (e.g., a certain kind of filtering becoming more
or less important), or require new task-specific filters (e.g., for some approximation of language
quality). However, these would also require a slightly different sort of corpus, such as one pairing
code with comments [28]. We believe conclusions from the comment completion tasks likely
generalize to other tasks involving comments, even if they require other inputs as well.

6 CONCLUSIONS
For building tools consuming or producing natural language from comments, the quality of the
comments used for training or evaluation can be a significant factor. We found that there are a
number of substantial categories of comments which may be useful in some ways, but are not purely
linguistic in nature, making them undesirable in linguistically-oriented datasets as they are written.
We offer a categorization of non-obvious linguistically-undesirable comments, and practical filters
for removing them. Finally, we have shown that applying these filters to remove non-linguistic
data improves the output quality of two generative models (4-gram and neural network models),
on two large datasets of open source Python comments. We believe this offers compelling evidence
that software engineering tools trained or evaluated on source code comments should be closer
attention to the linguistic qualities of the data than is typically done today, and have provided a set
of reusable filters to remove non-linguistic data to help.

REFERENCES
[1] K. K. Aggarwal, Y. Singh, and J. K. Chhabra. 2002. An integrated measure of software maintainability. In Annual

Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No.02CH37318). 235–241.

[2] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings
of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward’19). 143–153.

[3] Ahmed Awad and Khaled Nagaty. 2019. Commit Message Generation from Code Differences using Hidden Markov
Models. In Proceedings of the 2019 8th International Conference on Software and Information Engineering. ACM, 96–99.
[4] Edd Barrett, Carl Friedrich Bolz, Lukas Diekmann, and Laurence Tratt. 2016. Fine-grained Language Composition: A

Case Study. In 30th European Conference on Object-Oriented Programming.

[5] Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python (1st ed.). O’Reilly Media,

Inc.

[6] Arianna Blasi, Alberto Goffi, Konstantin Kuznetsov, Alessandra Gorla, Michael D. Ernst, Mauro Pezzè, and Sergio Del-
gado Castellanos. 2018. Translating Code Comments to Procedure Specifications. In Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis (Amsterdam, Netherlands) (ISSTA 2018). Association
for Computing Machinery, New York, NY, USA, 242–253. https://doi.org/10.1145/3213846.3213872

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

25

[7] Chris Callison-Burch, Cameron Shaw Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceedings of the third workshop on statistical machine translation. 70–106.
[8] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation

research. In 11th Conference of the European Chapter of the Association for Computational Linguistics.

[9] Qiuyuan Chen, Xin Xia, Han Hu, David Lo, and Shanping Li. 2021. Why My Code Summarization Model Does
Not Work: Code Comment Improvement with Category Prediction. ACM Transactions on Software Engineering and
Methodology (TOSEM) 30, 2 (2021), 1–29.

[10] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, Doha, Qatar, 1724–1734. https://doi.org/10.3115/v1/D14-1179

[11] Luis Fernando Cortés-Coy, Mario Linares-Vásquez, Jairo Aponte, and Denys Poshyvanyk. 2014. On automatically
generating commit messages via summarization of source code changes. In 2014 IEEE 14th International Working
Conference on Source Code Analysis and Manipulation. IEEE, 275–284.

[12] Sayamindu Dasgupta and Benjamin Mako Hill. 2017. Learning to code in localized programming languages. In

Proceedings of the fourth (2017) ACM conference on learning@ scale. 33–39.

[13] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia M. de Oliveira. 2005. A Study of the Documentation Essential
to Software Maintenance. In Proceedings of the 23rd Annual International Conference on Design of Communication: Doc-
umenting & Designing for Pervasive Information (Coventry, United Kingdom) (SIGDOC ’05). Association for Computing
Machinery, New York, NY, USA, 68–75. https://doi.org/10.1145/1085313.1085331

[14] Alberto Goffi, Alessandra Gorla, Michael D. Ernst, and Mauro Pezzè. 2016. Automatic Generation of Oracles for
Exceptional Behaviors. In Proceedings of the 25th International Symposium on Software Testing and Analysis (Saarbrúcken,
Germany) (ISSTA 2016). Association for Computing Machinery, New York, NY, USA, 213–224. https://doi.org/10.1145/
2931037.2931061

[15] Philip J Guo. 2018. Non-native english speakers learning computer programming: Barriers, desires, and design

opportunities. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1–14.

[16] Emily Hill. 2010. Integrating Natural Language and Program Structure Information to Improve Software Search and
Exploration. Ph. D. Dissertation. University of Delaware, Newark, DE, USA. Advisor(s) Pollock, Lori L. and Shanker,
Vijay K. AAI3423409.

[17] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.
[18] Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 505–512.

[19] Matthew J Howard, Samir Gupta, Lori Pollock, and K Vijay-Shanker. 2013. Automatically mining software-based,
semantically-similar words from comment-code mappings. In 2013 10th Working Conference on Mining Software
Repositories (MSR). IEEE, 377–386.

[20] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural
attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 2073–2083.

[21] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating commit messages from diffs using
neural machine translation. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software
Engineering. IEEE Press, 135–146.

[22] Ninus Khamis, René Witte, and Juergen Rilling. 2010. Automatic Quality Assessment of Source Code Comments: The
JavadocMiner. In Natural Language Processing and Information Systems, Christina J. Hopfe, Yacine Rezgui, Elisabeth
Métais, Alun Preece, and Haijiang Li (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 68–79.

[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980

[24] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved code summarization via a graph

neural network. In Proceedings of the 28th International Conference on Program Comprehension. 184–195.

[25] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code Summarization via a Graph

Neural Network. CoRR abs/2004.02843 (2020). arXiv:2004.02843 https://arxiv.org/abs/2004.02843

[26] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries
of program subroutines. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 795–806.
[27] A. LeClair, S. Jiang, and C. McMillan. 2019. A Neural Model for Generating Natural Language Summaries of Program
Subroutines. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). 795–806. https://doi.org/
10.1109/ICSE.2019.00087

, Vol. 1, No. 1, Article . Publication date: August 2022.

26

Sergey Matskevich and Colin S. Gordon

[28] Alexander LeClair and Collin McMillan. 2019. Recommendations for Datasets for Source Code Summarization. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis,
Minnesota, 3931–3937. https://doi.org/10.18653/v1/N19-1394

[29] Erik Linstead, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and Pierre Baldi. 2009. Sourcerer: mining
and searching internet-scale software repositories. Data Mining and Knowledge Discovery 18, 2 (2009), 300–336.
[30] Marco Lui and Timothy Baldwin. 2012. langid.py: An Off-the-shelf Language Identification Tool. In Proceedings of
the ACL 2012 System Demonstrations. Association for Computational Linguistics, Jeju Island, Korea, 25–30. https:
//www.aclweb.org/anthology/P12-3005

[31] Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. The MIT

Press, Cambridge, Massachusetts. http://nlp.stanford.edu/fsnlp/

[32] Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a Large Annotated Corpus of
English: The Penn Treebank. Comput. Linguist. 19, 2 (June 1993), 313–330. http://dl.acm.org/citation.cfm?id=972470.
972475

[33] Sergey Matskevich and Colin S. Gordon. 2018. Generating Comments from Source Code. In ACM Workshop on Natural
Language for Software Engineering (NL4SE). Lake Buena Vista, FL, USA. https://doi.org/10.1145/3283812.3283822
Citations: 1.

[34] Dana Movshovitz-Attias and William W. Cohen. 2013. Natural Language Models for Predicting Programming Comments.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).
Association for Computational Linguistics, Sofia, Bulgaria, 35–40. https://www.aclweb.org/anthology/P13-2007
[35] Dana Movshovitz-Attias and William W Cohen. 2013. Natural language models for predicting programming comments.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Vol. 2.
35–40.

[36] Rahul Pandita, Xusheng Xiao, Hao Zhong, Tao Xie, Stephen Oney, and Amit Paradkar. 2012.

Inferring Method
Specifications from Natural Language API Descriptions. In Proceedings of the 34th International Conference on Software
Engineering (Zurich, Switzerland) (ICSE ’12). IEEE Press, 815–825.

[37] Seymour Papert. 1972. Teaching children thinking. Programmed Learning and Educational Technology 9, 5 (1972),

245–255.

[38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of

Machine Translation. 311–318.

[39] Terence J. Parr and Russell W. Quong. 1995. ANTLR: A predicated-LL (k) parser generator. Software: Practice and

Experience 25, 7 (1995), 789–810.

[40] L. Pascarella and A. Bacchelli. 2017. Classifying Code Comments in Java Open-Source Software Systems. In 2017
IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). 227–237. https://doi.org/10.1109/MSR.
2017.63

[41] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine

Translation: Research Papers. 186–191.

[42] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python Natural
Language Processing Toolkit for Many Human Languages. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics: System Demonstrations.

[43] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic model for code with decision trees. ACM SIGPLAN

Notices 51, 10 (2016), 731–747.

[44] Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing automatic evaluation metrics for code
summarization tasks. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 1105–1116.

[45] Kento Shimonaka, Soichi Sumi, Yoshiki Higo, and Shinji Kusumoto. 2016. Identifying auto-generated code by using
machine learning techniques. In 2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP).
IEEE, 18–23.

[46] Sean Stapleton, Yashmeet Gambhir, Alexander LeClair, Zachary Eberhart, Westley Weimer, Kevin Leach, and Yu Huang.
2020. A Human Study of Comprehension and Code Summarization. In Proceedings of the 28th International Conference
on Program Comprehension. 2–13.

[47] D. Steidl, B. Hummel, and E. Juergens. 2013. Quality analysis of source code comments. In 2013 21st International

Conference on Program Comprehension (ICPC). 83–92. https://doi.org/10.1109/ICPC.2013.6613836

[48] Elior Sulem, Omri Abend, and Ari Rappoport. 2018. BLEU is Not Suitable for the Evaluation of Text Simplification. In

Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 738–744.

[49] Magdalena Wolska and Ivana Kruijff-Korbayová. 2004. Analysis of Mixed Natural and Symbolic Input in Mathematical
Dialogs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). Barcelona,

, Vol. 1, No. 1, Article . Publication date: August 2022.

Preprocessing Source Code Comments for Linguistic Models

27

Spain, 25–32. https://doi.org/10.3115/1218955.1218959

[50] Masashi Yoshikawa, Hiroshi Noji, and Yuji Matsumoto. 2017. A* CCG Parsing with a Supertag and Dependency Factored
Model. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
(Vancouver, Canada). Association for Computational Linguistics, 277–287. https://doi.org/10.18653/v1/P17-1026
[51] Juan Zhai, Yu Shi, Minxue Pan, Guian Zhou, Yongxiang Liu, Chunrong Fang, Shiqing Ma, Lin Tan, and Xiangyu
Zhang. 2020. C2S: Translating Natural Language Comments to Formal Program Specifications. In Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering (Virtual Event, USA) (ESEC/FSE 2020). Association for Computing Machinery, New York, NY, USA, 25–37.
https://doi.org/10.1145/3368089.3409716

, Vol. 1, No. 1, Article . Publication date: August 2022.

