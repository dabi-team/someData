Towards Using Data-Influence Methods to Detect Noisy Samples
in Source Code Corpora

2
2
0
2

t
c
O
3

]
E
S
.
s
c
[

2
v
2
2
0
3
1
.
5
0
2
2
:
v
i
X
r
a

Anh T. V. Dau*
Thang Nguyen-Duc
Hoang Thanh-Tung
{anhdtv7,thangnd34,tunght18}@fsoft.com.vn
FPT Software AI Center, Viet Nam

ABSTRACT
Despite the recent trend of developing and applying neural source
code models to software engineering tasks, the quality of such
models is insuï¬ƒcient for real-world use. This is because there could
be noise in the source code corpora used to train such models. We
adapt data-inï¬‚uence methods to detect such noises in this paper.
Data-inï¬‚uence methods are used in machine learning to evaluate
the similarity of a target sample to the correct samples in order to
determine whether or not the target sample is noisy. Our evalu-
ation results show that data-inï¬‚uence methods can identify noisy
samples from neural code models in classiï¬cation-based tasks. This
approach will contribute to the larger vision of developing better
neural source code models from a data-centric perspective, which
is a key driver for developing useful source code models in prac-
tice.

ACM Reference Format:
Anh T. V. Dau*, Thang Nguyen-Duc, Hoang Thanh-Tung, and Nghi D. Q.
Bui*. 2022. Towards Using Data-Inï¬‚uence Methods to Detect Noisy Sam-
ples in Source Code Corpora. In 37th IEEE/ACM International Conference
on Automated Software Engineering (ASE â€™22), October 10â€“14, 2022, Rochester,
MI, USA. ACM, New York, NY, USA, 3 pages.

1 INTRODUCTION
Research in the area of Deep Learning for Code [1â€“3, 8, 10, 11, 18]
mostly relies on a large code corpus of code that allows deep learn-
ing methods to reason about source code properties. However, the
real-world usage of such code models is still limited due to their
quality. We observe that the source code data used for training
code models is collected using a variety of heuristics, such as com-
mit messages, tags provided by code competition websites [4, 5]
and people tend to assume that the label of such data is accurate,
despite the fact that it may contain a lot of noise [5]. There are
many kinds of noise for classiï¬cation-based tasks, but in our work,
mislabeled examples are referred to as noisy examples.

*Equal Contributions

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9475-8/22/10. . . $15.00

Nghi D. Q. Bui*
dqnbui.2016@smu.edu.sg
School of Information Systems
Singapore Management University

We ï¬nd that the majority of code learning research focuses on
improving performance from a model-centric standpoint. This means
that the dataset will remain constant while new models are being
proposed to improve performance. There are recent eï¬€orts to an-
alyze or propose methods to create high-quality datasets for soft-
ware engineering [6, 9, 16, 17, 19] from the data-centric standpoint.
In the data-centric approach, the model remains ï¬xed while the
quality of the datasets used to train such model gets improved.
[6, 16] used simple rules to ï¬lter noise. Rule-based methods do not
scale well to large and complex datasets. [17] proposed a learning-
based approach for measuring the alignment between code and
text for code search data. In this paper, we approach the problem
by adapting data-inï¬‚uence methods [14] to detect noises and pro-
pose some strategies to enhance the quality of datasets. By identi-
fying noisy samples, we hope to improve the source code modelâ€™s
performance. Results from learning theory also suggest that mod-
els trained on clean datasets converge faster and are more robust
[13]. Data inï¬‚uence methods calculate the inï¬‚uence of training ex-
amples on model predictions. They track the changes in loss at
test data points whenever the training example of interest is used.
Finally, these methods will provide an inï¬‚uence score. This score
indicates whether the sample is noisy or not.

In this work, we concentrate on classiï¬cation-based tasks, in-
cluding code classiï¬cation and defect prediction. For these two
tasks, our evaluation shows that the data-inï¬‚uence methods can
successfully identify a large number of noisy samples in the train-
ing dataset. Furthermore, retraining the models on good training
data improves the modelsâ€™ performance and robustness.

2 TECHNICAL DETAILS
A source code model trained on a training set ğ’ğ’• ğ’“ğ’‚ğ’Šğ’ is denoted
as ğ‘´ (Â·; ğœ½ ) . Assuming ğ’ğ’• ğ’“ğ’‚ğ’Šğ’ contains some noise, our goal is to
identify the set of noisy samples ğ’ğ’ğ’ ğ’Šğ’”ğ’† âŠ‚ ğ’ğ’• ğ’“ğ’‚ğ’Šğ’. By removing
ğ’ğ’ğ’ ğ’Šğ’”ğ’†, we get a new training set ğ’
ğ’„ ğ’ğ’†ğ’‚ğ’
results in a new model ğ‘´ğ‘ğ‘™ğ‘’ğ‘ğ‘›. We use the validation set ğ’ğ’—ğ’‚ğ’ as
the anchor to detect noise and select from ğ’
ğ’—ğ’‚ğ’ a set of correctly
labeled samples by using ğ‘´. A sample is considered correct if the
prediction from the model ğ‘´ match its label with high conï¬dence.
We call this set of correctly labeled samples ğ’ğ’ˆğ’ğ’ğ’….

ğ’„ ğ’ğ’†ğ’‚ğ’. Retraining ğ‘´ on ğ’

Now, we introduce the data-inï¬‚uence methods. We focus on In-
ï¬‚uence Function (IF) [7] and TracIn [15] as they are currently state-
of-the-art techniques. IF 1 [7] estimates the inï¬‚uence of a sample

1TracIn [15] follows the same principle. Readers are encouraged to read the original
paper to check the details of TracInâ€™s formula.

 
 
 
 
 
 
ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

Anh T. V. Dau*, Thang Nguyen-Duc, Hoang Thanh-Tung, and Nghi D. Q. Bui*

Table 1: Results on identifying the mislabeled samples on
Synthetic Noisy dataset.

Table 2: Results after retraining on the Devign dataset.

Method

ğ‘˜ = 1

ğ‘˜ = 5

ğ‘˜ = 10

ASTNN

CodeBERT

IF
TracIn

IF
TracIn

94.20 Â± 3.72
91.09 Â± 5.06

84.88 Â± 2.26
79.96 Â± 1.29

59.71 Â± 0.32
55.97 Â± 1.14

64.15 Â± 1.07
72.36 Â± 2.39

31.50 Â± 1.28
49.30 Â± 1.33

14.92 Â± 1.33
35.36 Â± 1.15

Test ACC

Method

Test ACC after removing

IF

CodeBERT

62.91 Â± 0.08

TracIn

Random

63.31 Â± 0.10
63.40 Â± 0.20
61.73 Â± 0.05

ğ’ (ğ‘–)
ğ’• ğ’“ğ’‚ğ’Šğ’ on the model ğ‘´ by measuring the inï¬‚uence score in ac-
cordance with the change of the loss at ğ’ ( ğ‘—)
ğ’ˆğ’ğ’ğ’… when removing a
training sample ğ’ (ğ‘–)

ğ’• ğ’“ğ’‚ğ’Šğ’ from the training set.

We then present the pipeline for our evaluation with data-inï¬‚uence

ğ’ˆğ’ğ’ğ’….

methods as the key component to detect noisy samples.
(1) Initially, we train the model on training set ğ’ğ’• ğ’“ğ’‚ğ’Šğ’, result in
model ğ‘´. We use ğ‘´ to randomly select ğ‘ correctly predicted
samples from the validation set ğ’

ğ’—ğ’‚ğ’ , resulting in ğ’

ğ‘—=1 ğ‘† (ğ’ (ğ‘–)

, ğ’ğ’ˆğ’ğ’ğ’…) = Ãğ‘

(2) For each sample in ğ’ğ’• ğ’“ğ’‚ğ’Šğ’, we compute the inï¬‚uence score
with all samples in ğ’ğ’ˆğ’ğ’ğ’…. The score for each training sample is:
, ğ’ ( ğ‘—)
ğ‘† (ğ’ (ğ‘–)
). A negative score
ğ’• ğ’“ğ’‚ğ’Šğ’
shows that ğ’ (ğ‘–)
ğ’• ğ’“ğ’‚ğ’Šğ’ has bad inï¬‚uence on the model ğ‘´, which
means that ğ’ (ğ‘–)
ğ’• ğ’“ğ’‚ğ’Šğ’ is likely to be mislabeled.
(3) The top ğ‘˜% samples with the lowest score, denoted as ğ’ğ’ğ’ ğ’Šğ’”ğ’†,
are deemed to be noisy. We then remove ğ’ğ’ğ’ ğ’Šğ’”ğ’† from ğ’ğ’• ğ’“ğ’‚ğ’Šğ’
to create the new training set ğ’ğ’„ ğ’ğ’†ğ’‚ğ’. On ğ’ğ’„ ğ’ğ’†ğ’‚ğ’, the model is
retrained from scratch using the same hyperparameters.

ğ’• ğ’“ğ’‚ğ’Šğ’

ğ’ˆğ’ğ’ğ’…

3 EVALUATION
We introduce the datasets and source code models used in our ex-
periments.

Datasets: Two types of dataset are involved in our evaluation:

(1) Synthetic Noisy Dataset: We inject random noise to a clean
dataset. We chose the POJ-104 [12], a commonly used dataset
for code classiï¬cation. This dataset contains 52,000 C programs
divided into 104 classes of 500 programs each. We randomly
select 10% samples in each class and randomly relabel these
samples. Now we have a training set ğ’ğ’• ğ’“ğ’‚ğ’Šğ’ containing both
clean and noisy data.

(2) Real Noisy Dataset: As shown in [6], the dataset used in de-
fect prediction task might contain a signiï¬cant amount of noise.
We chose Devign [20] as the representative after conï¬rming
that there are noises in the dataset. Devign dataset includes
21,854 potententially vulnerable C functions collected from open
source projects. Each function is manually labeled by software
security experts as vulnerable or not.

Source code Models: We take the public code artifacts from
ASTNN2, CodeBERT3 to reproduce results reported in the original
works.

2https://github.com/zhangj111/astnn
3https://github.com/microsoft/CodeBERT

3.1 Evaluation Results
Firstly, we evaluate our method on the synthetic noisy dataset. Ta-
ble 1 shows the performance of IF and TracIn in identifying the
noisy examples on ğ’ğ’• ğ’“ğ’‚ğ’Šğ’. In practice, we do not know how many
percent of the samples in the dataset are noisy, so we choose dif-
ferent values of ğ‘˜. With ASTNN, in top ğ‘˜ = 10% samples with the
lowest score, both IF and TracIn can detect more than 55% noisy
samples, and when ğ‘˜ = 1%, more than 91% samples in this sub-
set are noise. Next, we evaluate data-inï¬‚uence methods on real
noisy dataset. Column Test ACC in table 2 shows the performance
of models trained on the original dataset. We then calculate the
score for all training instances with the same procedure in our eval-
uation pipeline. The top 1% (best hyperparameter) samples with
the lowest score are selected. We also include Random - a base-
line randomly selecting a 1% sample set as ğ’ğ’ğ’ ğ’Šğ’”ğ’†. The results in
table 2 show that by using data-inï¬‚uence methods, there are im-
provements in terms of ACC after retraining. In general, the results
in Table 1 and Table 2 support our hypothesis that data-inï¬‚uence
methods are eï¬€ective at detecting noise in code corpus; and re-
moving noises and re-training with cleaner datasets improves the
performance of code models

4 DISCUSSION & CONCLUSION
We present a novel data-centric perspective for enhancing the qual-
ity of source code models by using data-inï¬‚uence methods. We per-
formed various analyses on several baselines and obtained poten-
tially promising results for improving the quality of source code
data. There are numerous aspects that we can investigate in the
future. We mostly rely on synthetic noisy datasets to perform the
evaluation. Also, we only concentrate on classiï¬cation-based tasks
while we can do the same for many other tasks.For example, when
performing a generation-based task like code summarization, the
comments and method body are extracted from code snippets col-
lected on Github. However, not all of the developersâ€™ comments
reï¬‚ect the functionality of the given code snippet; this can also be
interpreted as noise and should be carefully examined too. In the
future, we intend to pursue our research in three directions: (1)
Identifying more noisy datasets to analyze and providing insights
on the noises of such datasets; (2) Improving the methods to de-
tect noisy data; and (3) Applying the methods to a broader range
of software engineering tasks, such as code summarization, bug
detection, and code translation.

5 ACKNOWLEDGEMENTS
This work is partly funded by FPT Software AI Center. We also
thank the anonymous reviewers for their insightful comments and
suggestions.

Towards Using Data-Influence Methods to Detect Noisy Samples in Source Code Corpora

ASE â€™22, October 10â€“14, 2022, Rochester, MI, USA

REFERENCES
[1] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn-
ing to Represent Programs with Graphs, In International Conference on Learn-
ing Representations (ICLR). CoRR. https://doi.org/arXiv:1711.00740

[2] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional atten-
tion network for extreme summarization of source code. In International confer-
ence on machine learning. PMLR, 2091â€“2100.

[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29.

[4] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In
2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE).
IEEE, 933â€“944.

[5] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of seman-
tic code search. arXiv preprint arXiv:1909.09436 (2019).

[6] Shihab Shahriar Khan, Nishat Tasnim Niloy, Md Aquib Azmain, and Ahmedul
Impact of Label Noise and Eï¬ƒcacy of Noise Filters in Software

Kabir. 2020.
Defect Prediction.. In SEKE. 347â€“352.

[7] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions
via inï¬‚uence functions. In International conference on machine learning. PMLR,
1885â€“1894.

[8] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated
Graph Sequence Neural Networks, In International Conference on Learning Rep-
resentations (ICLR). arXiv:1511.05493 [cs, stat]. arXiv: 1511.05493.

[9] Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John Grundy. 2021.
Opportunities and challenges in code search tools. ACM Computing Surveys
(CSUR) 54, 9 (2021), 1â€“40.

[10] Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu. 2021. Deep Learning
for Android Malware Defenses: a Systematic Literature Review. arXiv preprint
arXiv:2103.05292 (2021).

[11] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural
Networks over Tree Structures for Programming Language Processing. In Pro-
ceedings of the Thirtieth AAAI Conference on Artiï¬cial Intelligence. 1287â€“1293.

[12] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural
networks over tree structures for programming language processing. In Thirtieth
AAAI conference on artiï¬cial intelligence.

[13] Andrew Ng. 2022.

Andrew Ng "the data-centric AI

approach".

https://www.youtube.com/watch?v=TU6u_T-s68Y

An Empirical Comparison of

[14] Pouya Pezeshkpour, Sarthak Jain, Byron Wallace, and Sameer Singh.
2021.
Instance Attribution Methods for
NLP. In Proceedings of the 2021 Conference of the North American Chap-
ter of
the Association for Computational Linguistics: Human Language
Technologies. Association for Computational Linguistics, Online, 967â€“975.
https://doi.org/10.18653/v1/2021.naacl-main.75

[15] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. Es-
timating training data inï¬‚uence by tracing gradient descent. Advances in Neural
Information Processing Systems 33 (2020), 19920â€“19930.

[16] Arumoy Shome, Luis Cruz, and Arie van Deursen. 2022. Data Smells in Public

Datasets. arXiv preprint arXiv:2203.08007 (2022).

[17] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the importance of
building high-quality training datasets for neural code search. In Proceedings of
the 44th International Conference on Software Engineering. 1609â€“1620.

[18] Cody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, and Denys
Poshyvanyk. 2020. A Systematic Literature Review on the Use of Deep Learning
in Software Engineering Research. arXiv preprint arXiv:2009.06520 (2020).
[19] Yanjie Zhao, Li Li, Haoyu Wang, Haipeng Cai, TegawendÃ© F BissyandÃ©, Jacques
Klein, and John Grundy. 2021. On the impact of sample duplication in machine-
learning-based android malware detection. ACM Transactions on Software Engi-
neering and Methodology (TOSEM) 30, 3 (2021), 1â€“38.

[20] Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and Yang Liu.
2019. Devign: Eï¬€ective Vulnerability Identiï¬cation by Learning Compre-
hensive Program Semantics via Graph Neural Networks. In Advances in
Neural
Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van-
couver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett (Eds.). 10197â€“10207.
https://proceedings.neurips.cc/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html

