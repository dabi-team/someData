Code Comment Inconsistency Detection with BERT and Longformer

Theo Steiner
UNC Chapel Hill
theo2023@live.unc.edu

Rui Zhang
Penn State University
rmz5227@psu.edu

2
2
0
2

l
u
J

9
2

]
L
C
.
s
c
[

1
v
4
4
4
4
1
.
7
0
2
2
:
v
i
X
r
a

Abstract

Comments, or natural language descriptions
of source code, are standard practice among
software developers. By communicating im-
portant aspects of the code such as function-
ality and usage, comments help with software
project maintenance. However, when the code
is modiﬁed without an accompanying correc-
tion to the comment, an inconsistency between
the comment and code can arise, which opens
up the possibility for developer confusion and
bugs.
In this paper, we propose two mod-
els based on BERT (Devlin et al., 2019) and
Longformer (Beltagy et al., 2020) to detect
such inconsistencies in a natural language in-
ference (NLI) context. Through an evalu-
ation on a previously established corpus of
comment-method pairs both during and after
code changes, we demonstrate that our models
outperform multiple baselines and yield com-
parable results to the state-of-the-art models
that exclude linguistic and lexical features. We
further discuss ideas for future research in us-
ing pretrained language models for both incon-
sistency detection and automatic comment up-
dating.

1

Introduction

language comments embedded within
Natural
source code are indispensable to successful soft-
ware projects. Although they are not compiled,
comments serve as a guide to developers by provid-
ing valuable information about the functionality the
source code implements as well as how develop-
ers are meant to utilize the code (Tan et al., 2012).
Comments additionally help reduce ramping-up
time, which is the time required for new developers
to get accustomed to the speciﬁcs of a software
project. Moreover, since developers on average
spend around 82% of their time on program com-
prehension and navigation (Xia et al., 2018), main-
taining consistent comments can increase developer
productivity. As such, it is essential for developers

(a) Inconsistent

(b) Consistent

Figure 1:
Inconsistent and consistent Javadoc com-
ments after each code method is modiﬁed (Panthap-
lackel et al., 2021). Removed tokens are in red and
added tokens are in green.

to keep comments up to date with the code so that
their beneﬁts can be capitalized on. In practice,
however, comments are not always updated along-
side code changes (Wen et al., 2019), which can
lead to them becoming outdated and inconsistent
with the code. Figure 1 shows an example of an
inconsistent comment versus a consistent comment.
If not addressed in a timely manner, these incon-
sistencies can linger in code bases and mislead
developers, becoming sources of confusion, bugs,
and an overall impaired software development cy-
cle (Wen et al., 2019; Jiang and Hassan, 2006; Tan
et al., 2007; Ibrahim et al., 2012). Code comment
inconsistency detection is therefore of immense
practical use to software developers who have a
vested interest in keeping their code bases easily
readable, navigable, and as bug-free as possible.

Prior work focuses on rule-based techniques for
speciﬁc types of comments such as references to
renamed identiﬁers (Ratol and Robillard, 2017),
API directives pertaining to parameter constraints
(Zhou et al., 2017), and null values and excep-
tions (Tan et al., 2012). Others have attempted
text similarity methods (Rabbi and Siddik, 2020;

 
 
 
 
 
 
Corazza et al., 2018). However, these previous ap-
proaches only apply to pre-existing inconsistencies
within a software project, which is not helpful for
developers who would beneﬁt more from a compre-
hensive, real-time comment maintenance system.

To resolve this dilemma, Panthaplackel et al.
(2021) develop a state-of-the-art deep learning ap-
proach that not only detects existing inconsisten-
cies (referred to as post hoc) but also ﬂags incon-
sistencies that appear immediately as a result of
code changes, before they can cause harm to the
code base (referred to as just-in-time). A comment
update model, designed to aid developers in ﬁx-
ing discrepancies by suggesting updates to those
comments that are labeled as inconsistent, is also
presented within an extrinsic evaluation (Panthap-
lackel et al., 2021).

In this paper, we alternatively propose a much
more straightforward architecture and methodol-
ogy. Our approach leverages existing large, pre-
trained models, namely BERT (Devlin et al., 2019)
and Longformer (Beltagy et al., 2020) to address
the problem of code comment inconsistency detec-
tion as a natural language inference (NLI) task.1
It is well known that such models have previously
demonstrated high performance on many down-
stream natural language processing (NLP) tasks,
including NLI. To compare with the state-of-the-
art, we evaluate our models on the corpus provided
by Panthaplackel et al. (2021) and show that in
the post hoc setting, our approach outperforms the
baselines by substantial margins but underperforms
the state-of-the-art in the just-in-time setting.

Our main contributions are that we (1) formulate
code comment inconsistency detection as an NLI
problem via binary classiﬁcation; (2) show that
large, pretrained language models can provide sig-
niﬁcant improvements in performance in the post
hoc setting and comparable results to the state-of-
the-art (without the addition of explicit features)
in the just-in-time setting; and (3) offer propos-
als for future work in inconsistency detection and
comment updating based on our approach.

2 Related Work

Rabbi and Siddik (2020) introduce a siamese recur-
rent neural network architecture that detects incon-
sistencies by measuring the similarity between the
comment and the code. They employ two separate

1Implementation is available at https://github.

com/theo2023/coco-bert-longformer.

LSTM (Hochreiter and Schmidhuber, 1997) mod-
els, one for the comment and the other for the code,
and compute the Euclidean norm distance between
the ﬁnal hidden vectors of each LSTM. If the simi-
larity score is below a certain predeﬁned threshold,
the comment is classiﬁed as inconsistent. While
this technique achieves high recall on a limited set
of Java code-comment pairs, it does not consider
changes between different versions of the code and
thus is incompatible with the just-in-time setting.
Additionally, in general, text similarity is only a
relevant metric for comments that summarize their
code methods, not comments that document more
speciﬁc aspects of the code (e.g., @return and
@param Javadoc comments) (Panthaplackel et al.,
2021).

Liu et al. (2018) develop several random forest
classiﬁers and use 64 features relating to the code
before and after modiﬁcation, the comment, and
their relationship. The features include code refac-
torings, length of the comment, and code-comment
similarity, among others, in order to detect incon-
sistent block and line comments just-in-time. We
avoid such extensive feature engineering in our pro-
cedure and also consider Javadoc comments instead
of the more low-level block/line comments.

Instead of a model based on code-comment si-
miliarity or thorough feature extraction, Panthap-
lackel et al. (2021) learn the relationship between
comments and code modiﬁcations through a more
intricate architecture that features several BiGRU
(Cho et al., 2014) encoders and multi-head atten-
tion (Vaswani et al., 2017) layers. This architecture
is explained in more detail in Section 5.1. Rather
than using external attention modules, we rely on
the built-in attention mechanisms of BERT and
Longformer to correlate the comment and code
tokens.

Automatic comment updating has also been
of interest in the recent literature. This is the
more challenging problem of resolving inconsis-
tencies once they have been detected. Both Pan-
thaplackel et al. (2020) and Zhu et al. (2022) de-
velop an encoder-decoder architecture that exam-
ines changes in the source code and generates an
edit action sequence specifying how the comment
should be updated, which is then parsed into the
gold comment. Although we focus on inconsis-
tency detection and do not provide a comment up-
date model in this work, we encourage further re-
search in this important area in Section 8.

Figure 2: Sequence-based representation of Medit
(Panthaplackel et al., 2021). Removed tokens are in
red and added tokens are in green.

3 Task

The task of code comment inconsistency detection
is presented as follows: given a comment C and its
corresponding code method M , determine whether
C is inconsistent with M . We accomplish this
task in two settings, post hoc and just-in-time, and
formulate it in terms of NLI.

3.1 Post hoc

In the post hoc setting, only the existing version
of the comment-method pair is available such that
inconsistencies are already present in the code base.
Modiﬁcations to the code are not considered.

3.2

Just-in-time

In the just-in-time setting, the idea is that modiﬁ-
cations to the code must be analyzed in order to
determine which code changes caused the comment
to become inconsistent, and to detect this inconsis-
tency before it materializes and is committed to the
repository. Thus, both versions of the code method
Mold and M (before and after modiﬁcation by the
developer) are available, as well as Medit, an edit
action sequence representing the changes between
Mold and M . An example of Medit is shown in
Figure 2.

3.3 Natural Language Inference

We view code comment inconsistency detection
as an NLI task, also known as textual entailment.
Given a premise and hypothesis, the objective of
this common NLP task is to determine whether
the hypothesis entails the premise, contradicts the
premise, or is neutral.2 For this speciﬁc problem,
the premise is the comment C and the hypothesis
is the code method M . If our models predict that
M entails C, then C is labeled as consistent. Oth-
erwise, if our models predict that M contradicts
C, then C is labeled as inconsistent. Note that we
slightly modify NLI and drop the neutral label en-
tirely, simplifying inconsistency detection to binary

Train Valid Test

Total
@return 15,950 1,790 1,840 19,580
@param
932
1,038 10,610
8,640
8,398
Summary
1,034 1,066 10,498
32,988 3,756 3,944 40,688
Full
1,518
Projects

332

357

829

Table 1: Statistics of the full dataset (Panthaplackel
et al., 2021).

classiﬁcation. Also note that in our initial problem
formulation, the premise was M and the hypoth-
esis was C, but for practical reasons described in
Section 5.2, we swap the order of M and C.

4 Data

We use the training, validation, and test data cu-
rated by Panthaplackel et al. (2021).3 This data
consists of @return, @param, and summary
Javadoc comments paired with their correspond-
ing Java methods, totaling 40,688 examples (see
Table 1). They consider comment-method pairs
(C1, M1), (C2, M2) from consecutive commits of
popular open-source Java projects and set C = C1,
Mold = M1, and M = M2. Only examples in
which M1 (cid:54)= M2, i.e., where the developer modi-
ﬁed the code method, are extracted. The data fur-
ther includes a sequence of span diff subtokens for
each method that we use as Medit in the just-in-
time setting.

Collected examples are given a positive label,
i.e., C1 is inconsistent with M2, if C1 (cid:54)= C2. The
assumption behind this decision is that the devel-
oper updated the comment when they modiﬁed the
code because the comment would have become in-
consistent otherwise. On the other hand, collected
examples are given a negative label, i.e., C1 is con-
sistent with M2, if C1 = C2. The assumption
behind this decision is that the developer did not
need to update the comment when they modiﬁed
the code because the comment remained consistent
(Panthaplackel et al., 2021).

However, this data collection procedure intro-
duces noise in the form of mislabeled examples
(Panthaplackel et al., 2021). A positive mislabel-
ing occurs if the developer simply improved the
comment without changing its semantics; hence,
C1 (cid:54)= C2 even though the comment is actually
consistent. A negative mislabeling occurs if the de-

2https://paperswithcode.com/task/

natural-language-inference

3https://github.com/panthap2/
deep-jit-inconsistency-detection

veloper neglected to update the comment alongside
the code changes; hence, C1 = C2 even though the
comment is actually inconsistent. To reduce this
noise, Panthaplackel et al. (2021) only consider the
1,000 best maintained Java projects and curate a
smaller clean test sample separate from the full test
set. As these clean examples were not provided,
however, we do not report results from that data
and only evaluate on the full test set.

Lastly, comment, method, and edit action se-
quences are tokenized via the subword tokenization
algorithms of BERT and Longformer, i.e., Word-
Piece (Schuster and Nakajima, 2012) and byte pair
encoding (Sennrich et al., 2016) respectively.

5 Models

Figure 3: Panthaplackel et al. (2021) architecture.

We ﬁrst discuss the baselines then introduce our
models.

Figure 4: AST-based representation of Medit (Tedit)
(Panthaplackel et al., 2021). Removed nodes are in red
and added nodes are in green.

5.1 Baselines

• Corazza et al. (2018): This is a post hoc
SVM approach with a TF-IDF schema that
learns the decision boundary between consis-
tent and inconsistent comment-method pairs.

• CodeBERT BOW: As implemented by Pan-
thaplackel et al. (2021), this bag-of-words
baseline is the same underlying model for
each setting. It is an application of CodeBERT
(Feng et al., 2020) in which the average em-
bedding vectors of C and M (or Medit) are
concatenated and sent through a feedforward
neural network.

• Liu et al. (2018): Originally involving block
and line comments that accompany code
snippets, this is a just-in-time approach re-
implemented by Panthaplackel et al. (2021). It
uses random forest classiﬁers with code, com-
ment, and relationship features to predict in-
consistent comments based on code changes.

• Panthaplackel et al. (2021): There are nine
total models.
In the architecture, the code
encoder (Figure 3 (3)) is one of two distinct
conﬁgurations. The ﬁrst is a sequence en-
coder, a BiGRU that encodes Medit (or M in
the post hoc setting). The SEQ(C, ∗) models
correspond to the use of this encoder.

The second conﬁguration is an abstract syntax
tree (AST) encoder, a gated graph neural net-
work (GGNN) (Li et al., 2016) that encodes

Tedit (or T in the post hoc setting), which
is an AST representation of Medit. Figures
2 and 4 show examples of Medit and Tedit
respectively. The GRAPH(C, ∗) models cor-
respond to the use of this encoder. The last
three models, denoted HYBRID(C, ∗, ∗), in-
corporate both the GRU and GGNN when
encoding the code method.

Once the code and comment encoders encode
their input as a sequence of tokens, integrating
bidirectional context of other input tokens in
the process, multi-head self-attention (Figure
3 (2)) and multi-head cross-attention (Figure
3 (4)) are each computed. The former updates
the comment representation to include more
context and help capture long-range depen-
dencies, while the latter is employed between
each hidden state of the comment encoder
and the hidden states of the code encoder to
associate the comment with changes in the
code. Then, the contextualized embeddings
from each attention module are combined and
encoded by another BiGRU (Figure 3 (5)),
at which point the comment encoding carries
context from both the code method and other
tokens in the comment. Finally, this output
is fed through a fully connected layer and a
softmax operation (Figure 3 (6)) to predict
whether the comment is inconsistent with the
code method (Figure 3 (7)).

@return @param Summary
32.0
936.8
942.8
1102.6

35.0
753.1
750.1
955.3

34.0
728.0
726.0
895.1

Full
34.0
797.0
790.0
981.0

C
Mold
M
Medit

C
Mold
M
Medit

@return @param Summary
8.4
186.9
187.7
240.9

13.3
137.0
135.4
186.6

9.7
131.1
131.9
179.4

Full
10.3
147.2
147.3
197.3

Table 2: 98th percentile of the comment and code se-
quence lengths.

Table 3: Average lengths of the comment and code
sequences (Panthaplackel et al., 2021).

5.2 Our Models

Our ﬁrst model is BERT (Devlin et al., 2019) ﬁne-
tuned on an NLI task. In other words, it is the
base BERT model with an attached sequence clas-
siﬁcation head. Since BERT allows a maximum
sequence length of 512 tokens, the concatenated
sequences are truncated if they exceed that length.
This technique is not ideal because potentially valu-
able information from tokens in M (or Medit) that
the classiﬁer may need to make a correct prediction
is lost.

To better accommodate longer sequences, we
propose our second model, Longformer (Beltagy
et al., 2020) ﬁnetuned on an NLI task. In other
words, it is the base Longformer model with an at-
tached sequence classiﬁcation head. The eightfold
increase in maximum sequence length (from 512
to 4096) provided by Longformer helps to reduce
the adverse effects of truncation that our BERT
model encounters. However, we believe the ability
to process every single comment-method pair in the
dataset does not justify the time and memory costs
that would be incurred if we had chosen to lever-
age the default Longformer maximum sequence
length of 4096. Therefore, the statistics in Table
2 motivate a choice of 1024 as the custom maxi-
mum sequence length for our Longformer model.
As in the ﬁrst model, the concatenated sequences
are truncated if they exceed this length, and lower
performance is the tradeoff.

The major advantage of Longformer is its linear
attention pattern, addressing the fact that the time
and memory complexity of vanilla self-attention
(Vaswani et al., 2017) scales quadratically with se-
quence length (Beltagy et al., 2020). This improved
attention mechanism makes Longformer especially
efﬁcient for longer sequences.

As input to our models, the order (C, M ) was
chosen because preliminary experiments revealed
that (M, C) did not lead to a signiﬁcant change
in results. We also found that with the (M, C)
ordering, there were cases in which all the tokens

in C were being truncated. Swapping the order of
the inputs aligns with the fact that comments are
shorter than their corresponding code methods on
average (see Table 3), so in the worst case, the end
of M is truncated rather than the entirety of C.

6 Experiments

6.1

Implementation Details

C and M (or Medit) are tokenized separately and
concatenated into one sequence of the form

[CLS] C tokens [SEP] M tokens [SEP],
where [CLS] is the classiﬁcation token and
[SEP] is the separation token. An attention mask
is created as well as a binary mask representing
the token type IDs so that BERT can differentiate
between the part of the sequence that corresponds
to C and the part of the sequence that corresponds
to M (or Medit). For the Longformer model, the
token type IDs are unnecessary due to the use of the
RoBERTa (Liu et al., 2019) tokenizer. As described
in Section 5.2, the full sequence is truncated if it is
longer than the maximum length permitted by the
model. All other conﬁgurations, such as pretrained
comment, code, and code edit embeddings, are the
BERT and Longformer defaults.

6.2 Training

Because we are essentially training a binary classi-
ﬁer, we employ binary cross entropy (BCE) (Zhang
and Sabuncu, 2018) as our loss function. We also
use the Adam optimizer (Kingma and Ba, 2014)
and consider {0.0001, 0.00001} as learning rates.
To address memory issues, gradient accumulation
is employed to achieve effective batch sizes of {16,
32, 64}. Training terminates if the validation F1
score does not improve for 10 epochs.

6.3 Software and Hardware

We implemented all models using PyTorch and
the Hugging Face Transformers library,4 and we

4https://huggingface.co/docs/

transformers/index

Model

Corazza et al. (2018)
CodeBERT BOW(C, M )
SEQ(C, M )
GRAPH(C, T )
HYBRID(C, M, T )
BERT(C, M )
LONGFORMER(C, M )

Precision Recall
47.8
73.2
73.4
72.6
80.8
71.9
81.0

63.7
68.9
60.6
62.6
56.3
72.1
92.7

F1 Accuracy
54.6
70.7
66.3
67.2
66.3
72.0
86.4

60.3
69.8
62.8
64.6
58.9
72.1
87.3

Table 4: Results for post hoc models compared with baselines. Scores are averaged across three different seeds.

Model

CodeBERT BOW(C, Medit)
Liu et al. (2018)
SEQ(C, Medit)
GRAPH(C, Tedit)
HYBRID(C, Medit, Tedit)
SEQ(C, Medit) + features
GRAPH(C, Tedit) + features
HYBRID(C, Medit, Tedit) + features
BERT(C, Medit)
LONGFORMER(C, Medit)

Precision Recall
76.8
63.8
73.8
74.4
74.7
73.2
78.3
72.4
78.3
76.5

67.4
77.5
80.7
79.8
80.9
88.4
83.8
88.6
76.4
80.3

F1 Accuracy
71.6
70.0
77.1
76.9
77.7
80.0
80.9
79.6
77.3
78.3

69.6
72.6
78.0
77.6
78.5
81.8
81.5
81.5
77.1
78.8

Table 5: Results for just-in-time models compared with baselines. Scores are averaged across three different seeds.

used scikit-learn to compute the evaluation metrics
precision, recall, F1 score, and accuracy. Models
were trained in a distributed fashion on 8 NVIDIA
RTX A5000 GPUs (24 GB each).

7 Results and Analysis

We report precision, recall, F1, and accuracy for
our post hoc and just-in-time models in Tables
In the post hoc setting,
4 and 5 respectively.
we ﬁnd that both models outperform the base-
lines with respect to precision, F1, and accuracy,
and LONGFORMER(C, M ) achieves higher scores
across all metrics. In the just-in-time setting, we
ﬁnd that the recall achieved by BERT(C, Medit)
of 78.3 matches the highest (from the baseline
GRAPH(C, Tedit) + features); otherwise, our just-
in-time models overall underperform the Panthap-
lackel et al. (2021) models that have added surface
features like part-of-speech tags, comment/code
overlap, etc. However, these models produce re-
sults comparable to the Panthaplackel et al. (2021)
models without these features.

The results in Table 4 indicate that BERT and
Longformer are clearly advantageous for post hoc
inconsistency detection when ﬁnetuned on an NLI
task. The substantial performance gains attained

by our Longformer model are foreseeable given its
linear attention pattern and ability to adequately
process longer sequences. However, the decrease
in performance from LONGFORMER(C, M ) to
LONGFORMER(C, Medit) in the just-in-time set-
ting (Table 5) is puzzling, especially considering
the increase in performance from BERT(C, M ) to
BERT(C, Medit). The reason for this performance
drop is unclear.

8 Future Work

Provided these results, we leave it to future work
to both improve code comment inconsistency de-
tection as an NLI task and apply our approach to a
comment update model as mentioned in Sections
1 and 2. Accordingly, in addition to the just-in-
time system ﬂagging the inconsistent comment and
notifying the developer that they should manually
correct it, the high-level overview is that the code
changes that caused the inconsistency would be
examined and suggestions would be generated to
help the developer rectify the outdated comment.
Certain variants of these pretrained models also
show promise. For instance, GraphCodeBERT
(Guo et al., 2021) is speciﬁcally designed for pro-
cessing programming language and is pretrained

on a large corpus of comment-code pairs. The
main contribution of GraphCodeBERT is that it
takes into account the semantic structure of code
through its use of data ﬂow. Furthermore, Long-
former Encoder-Decoder (Beltagy et al., 2020) is
a possibility for a comment update model since
it is intended for sequence-to-sequence tasks with
longer input.

Another option as an implementation detail is to
handle longer sequences via chunking, i.e., split-
ting the input sequence into smaller chunks and
processing them separately. We did not pursue this
method due to time constraints, but we believe it is
worthwhile to explore.

9 Conclusion

We formulated code comment inconsistency detec-
tion as a natural language inference task and pro-
posed two models based on BERT and Longformer.
By evaluating on a known corpus of comment-
method pairs in both a post hoc and just-in-time
manner, we demonstrated that our models outper-
form several baselines in the post hoc setting and
yield comparable results to the state-of-the-art in
the just-in-time setting without additional linguis-
tic and lexical features. We further discussed some
ideas for future research in using pretrained lan-
guage models for inconsistency detection and an
automatic comment updating system.

Acknowledgements

We thank the 2022 Machine Learning in Cyber-
security Research Experience for Undergraduates
(REU) program at Penn State University, funded by
National Science Foundation Grant 1950491, for
supporting this work.

References

Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv:2004.05150.

Kyunghyun Cho, Bart van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Anna Corazza, Valerio Maggio, and Giuseppe Scan-
niello. 2018. Coherence of comments and method

implementations: a dataset and an empirical investi-
gation. Software Quality Journal, 26(2):751–777.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A pre-trained model for programming and
In Findings of the Association
natural languages.
for Computational Linguistics: EMNLP 2020, pages
1536–1547, Online. Association for Computational
Linguistics.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. GraphCodeBERT: Pre-training code represen-
tations with data ﬂow. In International Conference
on Learning Representations.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Walid M. Ibrahim, Nicolas Bettenburg, Bram Adams,
and Ahmed E. Hassan. 2012. On the relationship be-
tween comment update practices and software bugs.
J. Syst. Softw., 85(10):2293–2304.

Zhen Ming Jiang and Ahmed E. Hassan. 2006. Ex-
amining the evolution of code comments in post-
In Proceedings of the 2006 International
gresql.
Workshop on Mining Software Repositories, MSR
’06, page 179–180, New York, NY, USA. Associa-
tion for Computing Machinery.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and
Richard S. Zemel. 2016. Gated graph sequence
In 4th International Conference
neural networks.
on Learning Representations, ICLR 2016, San Juan,
Puerto Rico, May 2-4, 2016, Conference Track Pro-
ceedings.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

IEEE/ACM 27th International Conference on Pro-
gram Comprehension (ICPC), pages 53–64.

Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing,
Ahmed E. Hassan, and Shanping Li. 2018. Mea-
suring program comprehension: A large-scale ﬁeld
IEEE Transactions on
study with professionals.
Software Engineering, 44(10):951–976.

Zhilu Zhang and Mert Sabuncu. 2018. Generalized
cross entropy loss for training deep neural networks
with noisy labels. Advances in neural information
processing systems, 31.

Yu Zhou, Ruihang Gu, Taolue Chen, Zhiqiu Huang, Se-
bastiano Panichella, and Harald Gall. 2017. Analyz-
ing apis documentation and code to detect directive
defects. In 2017 IEEE/ACM 39th International Con-
ference on Software Engineering (ICSE), pages 27–
37.

Hongquan Zhu, Xincheng He, and Lei Xu. 2022.
Hatcup: Hybrid analysis and attention based
arXiv preprint
just-in-time comment updating.
arXiv:2205.00600.

Zhiyong Liu, Huanchao Chen, Xiangping Chen, Xiao-
nan Luo, and Fan Zhou. 2018. Automatic detection
of outdated comments during code changes. In 2018
IEEE 42nd Annual Computer Software and Appli-
cations Conference (COMPSAC), volume 01, pages
154–163.

Sheena Panthaplackel, Junyi Jessy Li, Milos Glig-
oric, and Raymond J. Mooney. 2021. Deep just-in-
time inconsistency detection between comments and
source code. In AAAI, page To appear.

Sheena Panthaplackel, Pengyu Nie, Milos Gligoric,
Junyi Jessy Li, and Raymond Mooney. 2020. Learn-
ing to update natural language comments based on
In Proceedings of the 58th Annual
code changes.
Meeting of the Association for Computational Lin-
guistics, pages 1853–1868, Online. Association for
Computational Linguistics.

Fazle Rabbi and Md Saeed Siddik. 2020. Detecting
Code Comment Inconsistency Using Siamese Recur-
rent Network, page 371–375. Association for Com-
puting Machinery, New York, NY, USA.

Inderjot Kaur Ratol and Martin P. Robillard. 2017. De-
tecting fragile comments. In 2017 32nd IEEE/ACM
International Conference on Automated Software
Engineering (ASE), pages 112–122.

Mike Schuster and Kaisuke Nakajima. 2012. Japanese
In 2012 IEEE Interna-
and korean voice search.
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 5149–5152.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Lin Tan, Ding Yuan, Gopal Krishna, and Yuanyuan
/*icomment: Bugs or bad com-
Zhou. 2007.
In Proceedings of Twenty-First ACM
ments?*/.
SIGOPS Symposium on Operating Systems Princi-
ples, SOSP ’07, page 145–158, New York, NY, USA.
Association for Computing Machinery.

Shin Hwei Tan, Darko Marinov, Lin Tan, and Gary T.
Leavens. 2012. @tcomment: Testing javadoc com-
In
ments to detect comment-code inconsistencies.
2012 IEEE Fifth International Conference on Soft-
ware Testing, Veriﬁcation and Validation, pages
260–269.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.

Fengcai Wen, Csaba Nagy, Gabriele Bavota, and
Michele Lanza. 2019. A large-scale empirical
In 2019
study on code-comment inconsistencies.

