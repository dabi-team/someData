Vulnerability Detection in Open Source Software:
An Introduction

Stuart Millar, Rapid7 LLC, stuart millar@rapid7.com

1

2
2
0
2

r
a

M
6

]

R
C
.
s
c
[

1
v
8
2
4
6
1
.
3
0
2
2
:
v
i
X
r
a

Abstract—This paper is an introductory discussion on the cause
of open source software vulnerabilities, their importance in the
cybersecurity ecosystem, and a selection of detection methods. A
recent application security report showed 44% of applications
contain critical vulnerabilities in an open source component, a
concerning proportion. Most companies do not have a reliable
way of being directly and promptly notiﬁed when zero-day vul-
nerabilities are found and then when patches are made available.
This means attack vectors in open source exist longer than
necessary. Conventional approaches to vulnerability detection are
outlined alongside some newer research trends. A conclusion is
made that it may not be possible to entirely replace expert human
inspection of open source software, although it can be effectively
augmented with techniques such as machine learning, IDE plug-
ins and repository linking to make implementation and review
less time intensive. Underpinning any technological advances
should be better knowledge at the human level. Development
teams need trained, coached and improved so they can implement
open source more securely, know what vulnerabilities to look for
and how to handle them. It is the use of this blended approach
to detection which is key.

Index Terms—open source software, cyber security, vulner-
ability detection, static analysis, dynamic analysis, software
assurance, machine learning.

I. INTRODUCTION

Open source software (OSS) is developed collaboratively
in the public domain with a licence granting rights to the
user base that are usually reserved for copyright holders. A
well-known OSS licence is the GNU General Public Licence
that allows free distribution under the condition that further
developments are also free. In a globally connected software
society, a sizeable amount of development work is effectively
crowd-sourced to an international community of OSS develop-
ers, with little awareness of the potential security problems this
creates [1]. OSS libraries increase development speed but there
is a tangible increase in risk also, with the Heartbleed bug in
OpenSSL being a prime example. Research into vulnerability
detection in OSS is crucial given its prevalence with many
companies using vulnerable OSS components and vulnerable
libraries being repackaged in software. This OSS uptake shows
no sign of reversing or slowing, with a recent survey [2]
indicating that 43% of respondents think OSS is superior to
its commercial equivalent.

At the crux of OSS vulnerability is that today’s applications
commonly use thirty or more libraries which in turn can
comprise up to 80% of the code in any such application. These
libraries have the same full privileges of the application that

S. Millar is with Rapid 7 LLC, Boston, MA, USA. This version dated March

26th 2017. For correspondence please e-mail: stuart millar@rapid7.com

use them, letting them access data, write to ﬁles or send data
to the internet. Anything the application can do, the library can
do. Some estimate that custom built Java applications contain
5-10 vulnerabilities per 10,000 lines of code [3]. A library can
have on average 10,000 to 200,000 lines of code, therefore
the chances a library has never had a vulnerability are very
slim, with it perhaps being more likely in fact that it has not
even been examined for vulnerabilities. Hence libraries with no
vulnerabilities should not automatically be considered ‘safe’.
Most vulnerabilities are undiscovered and it might be said the
only way to deal with the risk of unknown vulnerabilities is
to have someone who understands security manually analyse
the source code. Tool support provides hints but is not a
replacement for experts because the lack of context within
libraries makes it very difﬁcult for current tools at time of
writing to conclusively identify vulnerabilities.

This paper contributes a short introductory discussion on
vulnerability detection in OSS, and is organised as follows:
Section II is a background overview and Section III contains
information on conventional and emerging detection methods.
Conclusions are presented with ideas for future work in
Section IV.

II. BACKGROUND

A study carried out

in 2012 [3] found that more than
50% of the Fortune Global 500 companies have downloaded
vulnerable OSS components, security libraries and web frame-
works. This report analysed 113 million Java framework and
security library downloads by more than 60,000 commercial,
government and non-proﬁt organisations from the Central
Repository. Central is the software industries most widely used
repository of OSS with more than 300,000 libraries. It was
found the vast majority of library ﬂaws remain undiscovered,
the presence of a vulnerability (or an absence of one) is not a
security indicator, and that typical Java applications are likely
to include at least one vulnerable library. Furthermore, the
same study showed most organisations did not have a strong
process in place for ensuring the libraries they rely upon are
up-to-date and free from vulnerabilities. The authors of [3]
stress there are no shortcuts and they go as far as saying
the only useful indicator of library security is a thorough
review that ﬁnds minimal vulnerabilities – in other words,
software assurance, or the measure of how safe the software
is to use, needs to be generated internally. One might say this
is surprising, as in many other product or service industries,
this assurance – consider it some kind of warranty or seal
of approval perhaps – is offered up by the supplier without

 
 
 
 
 
 
hesitation to help build trust and sell to the customer. [4] adopt
a similar stance, agreeing that recurring vulnerabilities in soft-
ware are due to reuse. This reuse includes the same code base
with an identical or very similar code structure, method calls
and variables. Interestingly these attributes form the basis of a
proposed method of detecting unreported vulnerabilities in one
system by consulting knowledge of reported vulnerabilities in
other systems that reuse the same code.

Linus’ Law [5] is often quoted in relation to OSS, which is
“given enough eyeballs, all bugs are shallow”, meaning with
a large enough number of developers looking at code, errors
can be found. However, this can questioned from a scientiﬁc
viewpoint, and an empirical study of Linus’ Law appeared to
show more collaboration meant more vulnerabilities, not less.
[6] found that ﬁles with changes from nine or more developers
were sixteen times more likely to have a vulnerability than
ﬁles changed by fewer than nine developers. Thus inherent
collaborative nature of OSS creates potentially unavoidable
vulnerabilities that require addressing.

III. OSS VULNERABILITY DETECTION

A. Conventional Detection Methods

There are not currently an abundance of publications on
vulnerability detection in OSS. However, those that have been
written thus far describe three conventional methods – static
analysis, dynamic analysis and code reviews.

1) Static Analysis: Many black-box static analysis tech-
niques and tools scan source code and detect vulnerabilities
in software after it has been written, which encourages late
detection and produces a lot of false positives . [7] explicitly
referenced the cut and thrust of the software development pro-
cess, saying that external static tools for secure programming
don’t ﬁt into such a workﬂow, since they don’t work with
the IDE and are retrospective. [8] concur that static analysis
produces high levels of false positives, as do [9] and [10].
[11] point out it is hard to know which vulnerabilities a static
analysis tool deals with, and there are difﬁculties in obtaining
and maintaining up-to-date tooling.

[12] speciﬁcally wrote about the capability of static code
analysis to detect vulnerabilities, concluding that tools are
not effective. They tested three widely used commercial tools
and found 27% of C/C++ vulnerabilities and 11% of Java
vulnerabilities in their dataset were missed by all three. In
some cases, they were comparable to or worse than random
guessing. They too make the point about tools being prone
to false positives, and this consolidates the need to ﬁnd
other methods of detection rather than rely solely on static
analysis. That is not to say static analysis is of little use,
as some compliance regulations require inventories of OSS
components so that risks can be addressed. Static tools can
scan open source code and create an inventory, so when a new
vulnerability is disclosed, it is known which applications use
the vulnerable OSS. The OWASP Dependency-Check tool [13]
analyses code and creates reports on associated CVE entries.
2) Dynamic Analysis: White-box dynamic analysis can also
be called run-time analysis. Fuzzing is often used here, where
inputs are changed using random values to detect unwanted

2

behavior [11]. [14] researched the nuances of how vulnera-
bilities were discovered by researchers, and how those same
researchers shared their ﬁndings with the OSS community.
They found running a fuzzer and debugging was the chosen
method for developers exploring binary executables to ﬁnd
buffer overﬂows. Vulnerability researchers tend to make their
own fuzzing tools, seeing it as part of the learning process
and preferring this approach over more systematic exploration
methods. [9] notes the usefulness of fuzzing, and that it needs
only basic knowledge to undertake. However fuzzing does not
allow the control of program execution, large campaigns are
needed for results, and it is time consuming. [8] contends
fuzzing doesn’t scale if dynamic symbolic execution is used,
as it explores code paths simultaneously which could create
large workloads. Symbolic execution uses symbolic values for
variables instead of concrete values to execute all paths in a
program.

3) Manual Code Reviews: These involve manual inspection
of the source code in a white-box manner. Consequently, this
method requires a lot of human effort [10]. Working on source
code manually does however detect vulnerabilities [14], and
recall that [3] argued code reviews, conducted by someone
with appropriate security knowledge, may be the only way to
properly deal with vulnerabilities.

B. Emerging Detection Methods

The issues with the conventional methods in the main are
that static analysis produces too many false positives, dynamic
analysis does not scale, and code reviews are time consuming.
Research into newer methods tries to address these problems
via some interesting and novel approaches.

1) Distributed demand-driven security testing: Proposed by
[8], this involves many clients using OSS and one main testing
server, in a hub and spoke style per Figure 1. When a new path
in a program is about to be exercised by user input, it is sent
to the testing hub for security testing. Symbolic execution is
applied to the execution trace to check potential vulnerabilities
on this new path, and if one is detected then a signature is
generated and updated back to all the clients for protection. If
a path exercised by an input triggers any vulnerability that has

Fig. 1: Hub and spoke layout for distributed demand-driven security
testing

3

TABLE I: Execution complexity metrics deﬁned in [15]

Name
NumCalls

InclusiveExeTime

ExclusiveExeTime

Deﬁnition
The number of calls to the functions deﬁned
in a ﬁle.
Execution time for the set of functions, S,
deﬁned in a ﬁle including all the execution
time spent by the functions called directly or
indirectly by the functions in S.
Execution time for the set of functions, S,
deﬁned in a ﬁle excluding the execution time
spent by the functions called by the functions
in S.

TABLE II: Execution statistics from [15]

Program

Firefox
Wireshark

% of vulnerable
ﬁles
3.8
7.8

% of vulnerable ﬁles in
executed ﬁles
11
19

already been detected, the execution is terminated. This allows
testing to focus on paths being used and helps stop attackers
exploiting unreported vulnerabilities at a client site.

However, questions remain over how to handle large time
and space overheads at client sites, how sensitive data is
transmitted and handled, and actual implementation details are
scarce. That said, the principle of increasing test coverage of
important paths as users exercise them is sound, and [8] offers
an interesting conclusion that machine learning can in future
identify patterns of bugs at the testing server and use them to
predict problematic code.

2) Use of Execution Complexity Metrics: [15] examined
complexity metrics collected during code execution, consid-
ering them potential indicators of vulnerable code locations.
Table I describes these metrics. They measure the frequency
of function calls and duration of execution functions using
Callgrind, a Valgrind tool for proﬁling programs. The collected
data consists of the number of instructions executed on a run,
their relationship to source lines, and call relationship among
functions together with call counts. Firefox and Wireshark
were analysed with Callgrind to gather the metrics and results
showed execution complexity metrics may be better indicators
of vulnerable code than the conventional static complexity
metric of lines of code, or LoC.

Their initial results, shown in Table II, indicate the per-
centage of vulnerable ﬁles in execution is higher than the
percentage of vulnerable ﬁles in total, and hence execution
complexity metrics could be good indicators of vulnerability.
This can reduce the code inspection effort as prioritisation can
take place based on the metrics.

3) IDE Plugins for Early Detection: [7] attempted to detect
vulnerabilities earlier in the development process by using an
Eclipse Java plug-in, arguing developers should be aware of
security vulnerabilities as they are coding. To reduce false
positives, they proposed context-sensitive data ﬂow analysis
which uses a program’s context of variables and methods
when searching for vulnerabilities instead of pattern matching.
[16] presented interactive static analysis, also known as IDE
static analysis. They too developed an Eclipse Java plug-in
for detecting code patterns that gives a two-way interaction
between the IDE and the developer. According to [16], their
tool detected multiple zero day vulnerabilities. Figure 2 shows

Fig. 2: An IDE static analysis tool from [16]

a screenshot where the developer is instructed to annotate
access control logic for a highlighted sensitive method call.

4) Machine Learning: Machine learning is a type of ar-
tiﬁcial intelligence where computers use algorithms to learn
iteratively, teaching themselves to recognise patterns. Most
OSS code is managed using version control systems like Git
or CVS, with vulnerable code inserted via commits from the
developer to the main data repository. But many tools can’t run
on a small code snippet in an individual commit, and checking
the whole project is time consuming. [10] implemented a
type of machine learning algorithm called a Support Vector
Machine (SVM) that used metadata from commits made to
OSS repositories. The SVM used features from the metadata
such as the number of added, deleted or modiﬁed functions
and how often a contributor had contributed to a given project
before. Their results showed that false positives were reduced
by over 99% compared to those generated by a static analysis
tool - to be exact, their SVM driven tool generated 36 false
positives compared to 5,460 generated from the static analysis
tool. The goal of their work was to reduce the chance of
vulnerabilities getting from a vulnerable commit into the fully
deployed software. [9] also developed a machine learning
tool to predict vulnerabilities for large scale software like
operating systems. They took the popular Debian OS as an
example, since it has 30,000 programs and 80,000 bug reports.
Clearly, code ﬂaws can be hard to ﬁnd manually in a code
base of that size, so the application of machine learning is
of interest. Their classiﬁcation results were not conclusive but
nevertheless, as an initial study, they showed promise for large-
scale vulnerability detection only using binary executables,
an approach which does not appear to have been attempted
elsewhere.

5) Further Knowledge Formalisation and Linking Reposi-
tories: [17] discussed formalising knowledge representation
to determine transitive dependencies in software. The idea is
the various vulnerability repositories that exist online like the
NIST National Vulnerability Database (NVD), or the Common
Weakness Enumeration (CWE) database can be linked and
is indirectly
simultaneously used to ﬁnd out
dependent on vulnerable components.

if a project

4

[10] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y. Acar, “Vccﬁnder: Finding potential vulnerabilities in open-source
projects to assist code audits,” in Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security, ser. CCS ’15.
New York, NY, USA: Association for Computing Machinery, 2015, p.
426–437. [Online]. Available: https://doi.org/10.1145/2810103.2813604
[11] N. Shahmehri, A. Mammar, E. Montes De Oca, D. Byers,
A. Cavalli, S. Ardi, and W.
Jimenez, “An advanced approach
for modeling and detecting software vulnerabilities,” Inf. Softw.
Technol., vol. 54, no. 9, p. 997–1013, sep 2012. [Online]. Available:
https://doi.org/10.1016/j.infsof.2012.03.004

[12] K. Goseva-Popstojanova and A. Perhinschi, “On the capability of
static code analysis to detect security vulnerabilities,” Information and
Software Technology, vol. 68, pp. 18–33, 2015. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0950584915001366

[13] OWASP, “OWASP Dependency Check,” https://www.owasp.org/index.
php/OWASP Dependency Check, [Online: last accessed March 2017].
[14] M. Haﬁz and M. Fang, “Game of detections: How are security
vulnerabilities discovered in the wild?” Empirical Softw. Engg.,
[Online]. Available:
vol. 21, no. 5, p. 1920–1959, oct 2016.
https://doi.org/10.1007/s10664-015-9403-7

[15] Y. Shin and L. Williams,

“An initial

study on the use of
execution complexity metrics as indicators of software vulnerabilities,”
in Proceedings of
the 7th International Workshop on Software
Engineering for Secure Systems, ser. SESS ’11. New York, NY,
USA: Association for Computing Machinery, 2011, p. 1–7. [Online].
Available: https://doi.org/10.1145/1988630.1988632

[16] J. Zhu, J. Xie, H. R. Lipford, and B. Chu, “Supporting secure
programming in web applications through interactive static analysis,”
Journal of Advanced Research, vol. 5, no. 4, pp. 449–462, 2014, cyber
[Online]. Available: https://www.sciencedirect.com/science/
Security.
article/pii/S2090123213001422

[17] S. S. Alqahtani, E. E. Eghan, and J. Rilling, “Tracing known
security vulnerabilities in software repositories – a semantic web
enabled modeling approach,” Science of Computer Programming,
vol. 121, pp. 153–175, 2016, special
Issue on Knowledge-based
Software Engineering. [Online]. Available: https://www.sciencedirect.
com/science/article/pii/S0167642316000253

[18] M. English, C. Exton, I. Rigon, and B. Cleary, “Fault detection
and prediction in an open-source software project,” in Proceedings
of
in
the 5th International Conference on Predictor Models
Software Engineering, ser. PROMISE ’09. New York, NY, USA:
Association for Computing Machinery, 2009.
[Online]. Available:
https://doi.org/10.1145/1540438.1540462

IV. CONCLUSIONS & FUTURE WORK

The global use of OSS presents such a huge number of at-
tack vectors that discovering novel techniques of vulnerability
detection is an essential area of research. Of the new methods
mentioned in this paper, machine learning, early detection
IDE plug-ins and linking repositories show much promise for
future work. Machine learning lends itself well to feature-
rich OSS which speeds up classiﬁcation of vulnerable code
and reduces the time burden on development teams. Early
detection IDE plug-ins will help developers implementing
OSS to grow and consolidate their secure coding knowledge.
Linking repositories ensures better value from the separate,
unconnected datastores of vulnerabilities as they presently ex-
ist. It may also be possible to use machine learning to levarage
all these options in a modular system. Improvements in OSS
vulnerability detection might be quicker to realise than one
would think – [18] mention Pareto’s law, where 80% of effects
can be contributed to 20% of causes, and so identifying a small
proportion of problematic OSS code then focusing testing
efforts using a selection of detection methods could improve
code quality and time-to-release, whilst reducing development
and maintenance costs. The exact mix of techniques might
vary from one OSS scenario to another but in the ﬁrst instance
a strategy using a blend of methods that augment each other is
likely to be considerably more performant than one approach
in isolation.

REFERENCES

[1] S. Koussa,

“Techbeacon: 13 tools
open-source

for
dependencies,”

of

risk
13-tools-checking-security-risk-open-source-dependencies-0,
last accessed March 2017].
Software,

[2] Black Duck
Survey,”
2015-future-of-open-source-survey-results,
March 2017].

Source
https://www.slideshare.net/blackducksoftware/
accessed

of Open

[Online:

Future

“2015

last

checking the
security
https://techbeacon.com/
[Online:

[3] J. Williams, A. Dabirsiaghi, “The Unfortunate Reality of Insecure
http://cdn2.hubspot.net/hub/315719/ﬁle-1988689661-pdf/
Libraries,”
download-ﬁles/The Unfortunate Reality of Insecure Libraries.pdf?t=
14901257\24196, [Online: last accessed March 2017].

[4] N. H. Pham, T. T. Nguyen, H. A. Nguyen, and T. N. Nguyen,
“Detection of recurring software vulnerabilities,” in Proceedings of
the IEEE/ACM International Conference on Automated Software
Engineering, ser. ASE ’10. New York, NY, USA: Association
for Computing Machinery, 2010, p. 447–456. [Online]. Available:
https://doi.org/10.1145/1858996.1859089

[5] Wikipedia, “Linus’ Law,” https://en.wikipedia.org/wiki/Linus%27s

Law, [Online: last accessed March 2017].

[6] A. Meneely and L. Williams, “Secure open source collaboration: An
empirical study of linus’ law,” in Proceedings of
the 16th ACM
Conference on Computer and Communications Security, ser. CCS ’09.
New York, NY, USA: Association for Computing Machinery, 2009, p.
453–462. [Online]. Available: https://doi.org/10.1145/1653662.1653717
[7] L. Sampaio and A. Garcia, “Exploring context-sensitive data ﬂow
Journal of Systems
[Online]. Available:

analysis
and Software, vol. 113, pp. 337–361, 2016.
https://www.sciencedirect.com/science/article/pii/S0164121215002873

early vulnerability detection,”

for

[8] D. Zhang, D. Liu, C. Csallner, D. Kung, and Y. Lei, “A distributed
framework for demand-driven software vulnerability detection,” Journal
of Systems and Software, vol. 87, pp. 60–73, 2014. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0164121213002288

[9] G. Grieco, G. L. Grinblat, L. Uzal, S. Rawat, J. Feist, and L. Mounier,
“Toward large-scale vulnerability discovery using machine learning,” in
Proceedings of the Sixth ACM Conference on Data and Application
Security and Privacy, ser. CODASPY ’16. New York, NY, USA:
[Online].
Association for Computing Machinery, 2016, p. 85–96.
Available: https://doi.org/10.1145/2857705.2857720

