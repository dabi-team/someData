2
2
0
2

l
u
J

5
1

]

A
N
.
h
t
a
m

[

1
v
5
1
6
7
0
.
7
0
2
2
:
v
i
X
r
a

PLSS: A PROJECTED LINEAR SYSTEMS SOLVER∗

JOHANNES J. BRUST† AND MICHAEL A. SAUNDERS‡

Abstract. We propose iterative projection methods for solving square or rectangular consistent
linear systems Ax = b. Projection methods use sketching matrices (possibly randomized) to generate
a sequence of small projected subproblems, but even the smaller systems can be costly. We develop
a process that appends one column each iteration to the sketching matrix and that converges in a
ﬁnite number of iterations independent of whether the sketch is random or deterministic. In general,
our process generates orthogonal updates to the approximate solution xk. By choosing the sketch to
be the set of all previous residuals, we obtain a simple recursive update and convergence in at most
rank(A) iterations (in exact arithmetic). By choosing a sequence of identity columns for the sketch,
we develop a generalization of the Kaczmarz method. In experiments on large sparse systems, our
method (PLSS) with residual sketches is competitive with LSQR, and our method with residual and
identity sketches compares favorably to state-of-the-art randomized methods.

Key words. linear systems, iterative solver, randomized numerical linear algebra, projection

method, LSQR, Kaczmarz method, Craig method

AMS subject classiﬁcations. 15A06, 15B52, 65F10, 68W20, 65Y20, 90C20

1. Introduction. Consider a general linear system

(1.1)

Ax = b,

b ∈ range(A),

where A ∈ Rm×n, x ∈ Rn and b ∈ Rm. For computations with large matrices,
randomized methods [6, 7, 16] aim to generate inexpensive (possibly less accurate)
estimates of x by solving smaller projected systems. When the elements of A are
contaminated by errors or noise, which may be the case in data-driven problems, highly
accurate solutions are not always requested. For large systems, randomized solvers
are growing in popularity, although it is not uncommon to observe slow convergence
for naive implementations. Reasons for the widespread interest may be the arguably
intuitive approach of solving a sequence of small projected systems instead of (1.1),
and the fact that randomization has emerged as an enabling technology in data science,
machine learning and modern data intensive ﬁelds. This article develops a family of
projection methods that solve a sequence of smaller systems and can have signiﬁcant
advantages in terms of computation, accuracy and convergence.

1.1. Notation. Integer k ≥ 1 represents the iteration index, and vector ek
denotes the kth column of the identity matrix, with dimension depending on the
k A. For the kth solution estimate xk, the residual
context. The kth row of A is a(cid:62)
vector is rk := b − Axk, with associated vector yk := A(cid:62)rk. Lower-case Greek letters
represent scalars, and the range of integers from 1 to k is written 1 : k. We make use
of the economy SVD A = UΣV(cid:62).

k = e(cid:62)

1.2. Related work. To handle large problems, there has been a growing interest
in sketching techniques. A straightforward approach is random sketching, which
consists of selecting a random matrix S ∈ Rm×r with r (cid:28) m and solving the reduced
linear system

(1.2)

S(cid:62)Ax = S(cid:62)b.

∗Version of July 18, 2022.
†Department of Mathematics, University of California San Diego, La Jolla, CA (jjbrust@ucsd.edu).
‡Department of Management Science and Engineering, Stanford University, Stanford, CA (saun-

ders@stanford.edu).

1

 
 
 
 
 
 
2

J. J. BRUST AND M. A. SAUNDERS

By the Johnson-Lindenstrauss lemma [8], a solution to the reduced system (1.2) is
related to a solution of (1.1). However, unless range(S) = range(A), the solutions
will not be the same. Since (1.2) involves the product S(cid:62)A, previous work in [1, 10]
has focused on the choice of S to reduce the computational cost of this product.
In particular, with certain sketching matrices S, the randomized Kaczmarcz [17],
randomized coordinate descent [11], and stochastic Newton [15] methods can be
deﬁned by (1.2). An overview of randomized iterative methods for solving linear
systems is in [6, 16], which we consider to be state-of-the-art for the purpose of this
article.

1.3. Motivation. Given x0 ∈ Rn, let estimates of the solution to (1.1) be deﬁned

by the iterative process

(1.3)

xk = xk−1 + pk,

k = 1, 2, . . . ,

where pk ∈ Rn is called the update. It is important to compute the update eﬃciently.
Because sketching techniques aim to generate iterates with relatively low computational
complexity, we choose to solve a sequence of sketched systems. In particular, we use a
sequence of full-rank matrices

(1.4)

Sk ∈ Rm×k, k = 1, 2, . . . ,

rank(Sk) = k.

In general, the matrices can be arbitrary as long as they have the speciﬁed dimensions
and rank. We assume that Sk is in the range of A, though this is not strictly necessary.
We show later that the iterates xk for solving (1.1) converge in a ﬁnite number of steps.
We also demonstrate that a particular choice for Sk results in very eﬃcient updates.
Part of our scheme is an additional full-rank parameter matrix B(cid:62)B ∈ Rn×n that
can possibly improve the numerical behavior of the methods. (This matrix is not
essential, and all results hold when B = In×n.) Computations with B(cid:62)B are intended
to be inexpensive, as they would be if it were a diagonal matrix. The sequence of
systems that deﬁne each update are of the form

(1.5)

(cid:20)B(cid:62)B A(cid:62)Sk
S(cid:62)
k A 0k×k

(cid:21) (cid:20)pk
λk

(cid:21)

=

(cid:21)

(cid:20)A(cid:62)
S(cid:62)
k

(b − Axk−1).

Since only pk is used to deﬁne the next iterates, the vector λk ∈ Rn×k is not computed
in our approach. When Sk is in the range of A, solving (1.5) is equivalent to the
constrained least-squares problem

(1.6)

(1.7)

arg min
p∈Rn

subject to

(cid:107)Bp(cid:107)2
2

1
2
k A(xk−1 + p) = S(cid:62)
S(cid:62)

k b.

Details of formulating problem (1.6)–(1.7) from (1.5) are in Appendix A.

Here we summarize that the solution pk to (1.6)–(1.7) is deﬁned by W := (B(cid:62)B)−1

and

(1.8)

pk = WA(cid:62)Sk(S(cid:62)

k AWA(cid:62)Sk)−1S(cid:62)

k (b − Axk−1).

If Sk is not in the range of A, the inverse in (1.8) is replaced by the pseudo-inverse.
For ease of notation we set B(cid:62)B = I in the next sections (and hence W = I). Later
we lift this assumption and describe updates with nontrivial W.

LINEAR PROJECTION SOLVERS

3

Once pk is obtained from (1.8), it can deﬁne the next iterate via (1.3). Note that
popular methods in [6, 16] use an update like (1.8); however, the matrix S ≡ Sk is then
typically randomly generated, and A(cid:62)S, S(cid:62)AA(cid:62)S and (S(cid:62)AA(cid:62)S)−1 are typically
recomputed each iteration. Thus, a small parameter r > 0 must be selected such that
a random S ∈ Rm×r maintains low computational cost. Convergence is characterized
by a rate ρ ∈ [0, 1) that depends on the smallest singular value of a matrix deﬁned
by S and A. Convergence implicitly depends on the choice of r and may result in
prohibitively many iterations when the rate ρ is close to unity.

1.4. Contributions. We prove that iteration (1.3)–(1.8) converges in kmax steps
(1 ≤ kmax ≤ min(m, n)) when the sketch is in the range of A or the system is
underdetermined. Each sketch Sk can be random or deterministic, and can be
augmented by one column each iteration or recomputed from scratch. We show
that the ﬁnite-termination property holds for m ≥ n and also m < n. By selecting
previous residuals to form each Sk, we develop an iteration with orthogonal updates
and residuals. This process is simple to implement and only stores and updates ﬁve
vectors. By selecting columns of the identity matrix for each Sk, we develop an update
that generalizes the Kaczmarz method [9]. These choices are two instances in our
general class of methods characterized by the choice of Sk.

2. Method. Our method solves a sequence of sketched systems S(cid:62)

k b
that exploit information generated at previous iterations. Suppose that one sketching
column sk is generated each iteration and stored in the matrix
Sk := (cid:2)s1

k Axk = S(cid:62)

(cid:3) ∈ Rm×k.

(2.1)

. . .

sk

s2

Throughout this analysis, we assume Sk has full column rank.

2.1. Orthogonality. When constraints (1.7) are satisﬁed, each update has the

property

(2.2)

0 = S(cid:62)

k A(xk−1 + pk) − S(cid:62)

k b = −S(cid:62)

k rk.

Therefore from (2.1), by construction, previous sketching columns are orthogonal to
the next residual (and hence linearly independent): sj ⊥ ri for 1 ≤ j ≤ i, 1 ≤ i ≤ k.
In section 4 we show that the updates pk are also orthogonal.

2.2. Practical computations. We develop general techniques to compute pk
(1.8) eﬃciently. First, we describe a method based on updating a QR factorization.
Second, we deduce an alternative method (based on updating a triangular factorization)
to avoid recomputing S(cid:62)
k AA(cid:62)Sk)−1. However, since both of these methods,
for general sketches, have memory requirements that grow with k, we additionally
show in Section 4 that pk satisﬁes a short recursion deﬁned by pk−1 and another
vector when the sketch is chosen judiciously.

k A and (S(cid:62)

In order to avoid recomputing the potentially expensive product S(cid:62)
A(cid:62)sk and deﬁne Yk−1 := [ y1 y2 . . . yk−1] to collect the previous y’s. Then

k A, let yk :=

(2.3)

(S(cid:62)

k A)(cid:62) = A(cid:62)Sk = (cid:2)A(cid:62)Sk−1 A(cid:62)sk

(cid:3) = (cid:2)Yk−1 yk

(cid:3) = Yk ∈ Rn×k.

2.3. QR factorization. In general it is numerically safer to update factors of a
matrix rather than its inverse (because factors exist even when the matrix is singular).
QR factors of Yk can be used to this eﬀect. Speciﬁcally, let the QR factorization be

(2.4)

Yk = QkTk,

4

J. J. BRUST AND M. A. SAUNDERS

where Qk ∈ Rn×k is orthonormal and Tk ∈ Rk×k is upper triangular. Note that the
update pk from (1.8) with ρk−1 ≡ s(cid:62)

k rk−1 simpliﬁes to

(2.5)

pk = Yk(Y(cid:62)

k Yk)−1S(cid:62)

k rk−1 =

ρk−1
rkk

Qkek =

ρk−1
rkk

qk,

where rkk is the ﬁnal diagonal element in Tk. Householder reﬂectors Hk can be used
to represent Qk = H1 H2 · · · Hk in factored form with essentially the same storage
as Yk [5]. Computing pk in (2.5) requires one product with the factored form of Qk.
In particular, qk and rkk are obtained by

Qkek = qk

and

k (Q(cid:62)
e(cid:62)

k yk) = q(cid:62)

k yk = rkk.

We note that another option to develop the QR factorization (2.4) is to append yk to
Tk−1 and apply a sequence of plane rotations to eliminate the nonzeros in (yk)k+1:m
to deﬁne Qk and Tk. Both QR strategies use storage that grow with k.

2.4. Triangular factorization. In another approach, we update the inverse of
k Yk in (1.8). This is based on storing and updating the previous inverse in order to

Y(cid:62)
obtain the next. Concretely, suppose we store

Nk−1 = (Y(cid:62)

k−1Yk−1)−1 = (S(cid:62)

k−1AA(cid:62)Sk−1)−1

and wish to compute Nk.

Theorem 2.1. Assume Yk has full rank. Then
(cid:35)
(cid:34)

(2.6)

Nk =

(cid:62)
Nk−1 + 1
δk (cid:98)tk(cid:98)t
k
(cid:62)
−1
δk (cid:98)t
k

−1
δk (cid:98)tk
1
δk

= RkDkR(cid:62)
k ,

where Rk := [ t(k)
1

t(k)
2

· · · t(k)

k ] is upper triangular, Dk := diag(1/δj)j=1,...,k, and

δj := y(cid:62)

j yj − y(cid:62)

j Yj(cid:98)tj,

t(k)
j

:=



 ,

with

(cid:98)tj := NjY(cid:62)

j yj.





(cid:98)tj
−1
01:k−j

Proof. Observe that

(2.7) Nk = (Y(cid:62)

k Yk)−1 =

(cid:20)Y(cid:62)
k−1Yk−1 Y(cid:62)
(y(cid:62)
y(cid:62)
k Yk−1

k−1yk
k yk)−1

(cid:21)−1

=

(cid:20) N−1
k−1
y(cid:62)
k Yk−1

Y(cid:62)
k−1yk
k yk)−1
(y(cid:62)

(cid:21)−1

.

Setting (cid:98)tk := Nk−1Y(cid:62)
the block inverse in (2.7), we obtain

k−1yk and δk := y(cid:62)

k yk − y(cid:62)

k Yk−1(cid:98)tk and then explicitly forming

(2.8)

Nk =

(cid:34)

(cid:62)
Nk−1 + 1
δk (cid:98)tk(cid:98)t
k
(cid:62)
−1
δk (cid:98)t
k

(cid:35)

,

−1
δk (cid:98)tk
1
δk

which proves the ﬁrst equality.

Since all terms on the right side of (2.8) depend on Nk−1 only, this recursion can
be used to form Nk once Nk−1 (and Yk−1 and yk) have been stored. The recursive
process can be initialized with N0 := (y(cid:62)

1 y1)−1. Moreover, with the vectors

t(k)
j

:=



 ,

j = 1 : k,





(cid:98)tj
−1
01:k−j

LINEAR PROJECTION SOLVERS

5

the recursion of (2.8) can be expressed as a sum of rank-one updates. In particular,

(cid:20)Nk−1

Nk =

(cid:20)Nk−2



=

(cid:34)N0











=

(cid:21)

0

+

(cid:21)

0

(cid:35)

. . .

k (t(k)
t(k)
k )(cid:62)
δk



 +
0






0

k−1)(cid:62)

k−1(t(k)
t(k)
δk−1

+

k (t(k)
t(k)
k )(cid:62)
δk







0

+

1 (t(k)
t(k)
1 )(cid:62)
δ1

+ · · · +

t(k)
k (t(k)
k )(cid:62)
δk

.

Deﬁning the upper triangular matrix Rk = [ t(k)
1
Dk = diag(1/δj)j=1:k then gives the factorized representation of Nk in (2.6).

k ] and the diagonal matrix

· · · t(k)

t(k)
2

This factorization of Nk already improves computing pk from (1.8). Speciﬁcally,

a formula that does not require any solves is described in the following corollary.
Corollary 2.2. If Nk is generated by the process in Theorem 2.1, then

(2.9)

pk =

=

sk

sk

(cid:62)rk−1
δk
(cid:62)rk−1
δk

(cid:16)

yk − Yk−1(cid:98)tk

(cid:17)

(cid:0)yk − Yk−1Rk−1Dk−1R(cid:62)

k−1Y(cid:62)

k−1yk

(cid:1) .

Proof. Substituting (2.8) into (1.8) with the deﬁnition from (2.3) yields

(2.10)

(cid:18)

pk =

Yk−1Nk−1 +

1
δk

(cid:62)
k −
Yk−1(cid:98)tk(cid:98)t

(cid:19)

(cid:62)
yk(cid:98)t
k

1
δk

S(cid:62)

k−1rk−1

+

(cid:16)

sk

(cid:62)rk−1
δk

yk − Yk−1(cid:98)tk

(cid:17)

,

where S(cid:62)

k−1rk−1 = 0 by the orthogonality property.

Formula (2.9) implies that we do not have to compute Y(cid:62)

k Yk explicitly nor do
solves with it. Instead, the factors Rk−1 and Dk−1 can be updated one column per
iteration by products with triangular matrices only. For instance, Rk−1 is obtained by
computing tk−1 (suppressing superscripts) and appending: Rk−1 = [ Rk−2
tk−1 ].
k−1 −1 0(cid:62)](cid:62), it is obtained from 2 multiplications with triangular
(Since tk−1 = [(cid:98)t(cid:62)
matrices only: (cid:98)tk−1 = Rk−2(Dk−2(R(cid:62)
k−2yk))).) However, (2.9) still needs Yk−1,
Rk−1 and Dk−1, which all grow with k.

k−2(Y(cid:62)

2.5. Orthogonality of pk. It is valuable to note from (2.9) that

sk

pk =

(cid:62)rk−1
δk
k−1pk = 0. Deﬁning Pk−1 := (cid:2) p1

(cid:0)I − Yk−1(Y(cid:62)

so that Y(cid:62)
span(Yi) for i = 1 : k − 1, we see that

(2.11)

P(cid:62)

k−1pk = 0.

k−1Yk−1)−1Y(cid:62)

k−1

(cid:1) yk,

. . . pk−1

(cid:3) and noting that pi ∈

6

J. J. BRUST AND M. A. SAUNDERS

That is, the updates generated by our class of methods are orthogonal. We also deﬁne
the squared lengths θi = (cid:107)pi(cid:107)2

2 (i = 1 : k) and the diagonal matrix

Θk−1 := P(cid:62)

k−1Pk−1 = diag(θ1, . . . , θk−1).

2.6. Linear combination of pk. The methods in Sections 2.4 and 2.3 use
memory that grows with k. By further unwinding recursive relations in (2.9), described
in Appendix B, we can represent the update as a linear combination of previous updates.
This enables us to derive in Section 4 a short recursion deﬁned in terms of pk−1 and
yk only. For some scalars αj, the dependencies become

(2.12)

pk =

sk

(cid:62)rk−1
δk

k−1
(cid:88)

(cid:0)

j=1

αjpj+yk

(cid:1).

Representation (2.12) implies that pk = Pk−1gk−1 + γk−1yk for some vector
gk−1 ∈ Rk−1 and scalar γk−1. This will lead to a computationally eﬃcient formula for
pk.

3. Convergence. We ﬁrst show ﬁnite termination for iterates generated by (1.3)
i=1 in the range of A. Denote by (·)−1 the

and (1.8) using a sequence {Si ∈ Rm×i}k
inverse for square matrices and by (·)† the pseudo-inverse for rectangular matrices.

Theorem 3.1. Assume that m ≥ n and A has full column rank. Given x0 ∈ Rn,
consider the sequence {xk} computed by (1.3) and (1.8). Also assume that after n
iterations, Sn has full rank and is in the range of A. Then, xn solves (1.1).

Proof. After k = n iterations, Sk is by assumption a rank-n matrix in the range
=

k A is a nonsingular square matrix. Hence, (cid:0)S(cid:62)
k A)−1. Let rk−1 := b − Axk−1, so that

of A, so that S(cid:62)
(A(cid:62)Sk)−1W−1(S(cid:62)

k AWA(cid:62)Sk

(cid:1)−1

pk = WA(cid:62)Sk

(cid:0)S(cid:62)

k AWA(cid:62)Sk

(cid:1)−1

k rk−1 = (S(cid:62)
S(cid:62)

k A)−1S(cid:62)

k rk−1.

Let A = UΣV(cid:62) be the economy SVD of A. As Sk is in the range of A, it can be
represented by Sk = UTk for some nonsingular Tk. Therefore,

k A)−1S(cid:62)

k rk−1 = (T(cid:62)

(S(cid:62)
xn = xn−1 + pn = xn−1 + A†(b − Axn−1) = A†b.

k ΣV(cid:62))−1T(cid:62)

k U(cid:62)rk−1 = A†rk−1,

We conclude that xn is the least squares solution of (1.1) when m > n, and the unique
solution when m = n.

When not every member of {Sk} is in the range of A, Corollary 3.2 shows

convergence in at most m iterations.

Corollary 3.2. Suppose we use the same iterative process from Theorem 3.1
except that each matrix in the sequence {Sk}m−1
k=1 has full rank only (and is not
necessarily in the range of A), and Sm is a square nonsingular matrix. Then either
xm or xl, n < l < m, is a solution of (1.1).

Proof. At iteration k = m, the matrix S(cid:62)

rank. Thus the update pk in (1.8) is deﬁned by the pseudo-inverse (S(cid:62)
Since Sk is square and nonsingular,

k AWA(cid:62)Sk ∈ Rm×m does not have full
k AWA(cid:62)Sk)†.

Sk(S(cid:62)

k AWA(cid:62)Sk)†S(cid:62)

k = (AWA(cid:62))†.

LINEAR PROJECTION SOLVERS

7

Let rk−1 := b − Axk−1 so that pk = WA(cid:62)(AWA(cid:62))†rk−1 = A†rk−1. Then

xm = xm−1 + pm = xm−1 + A†(b − Axm−1) = A†b.

At an earlier iteration n < k = l < m, if Sk can be partitioned by a square nonsingular
matrix Tk ∈ Rn×n and a matrix U⊥ ∈ Rm×(k−n) in the nullspace of A(cid:62) (i.e.,
U(cid:62)

⊥U = 0 where A = UΣV(cid:62)) so that

Sk = (cid:2)UTk U⊥

(cid:3) ,

then xl is a solution to (1.1). In particular,

(S(cid:62)

k AWA(cid:62)Sk)† =

The update is then

(cid:20)T(cid:62)

k ΣV(cid:62)WVΣTk 0
0

0

(cid:21)†

.

pk = (cid:2)A† 0(cid:3)

(cid:21)

(cid:20)Im
0

rk−1,

and hence xk = xk−1 + pk will be a least-squares solution of (1.1).

Corollary 3.2 further implies that the process in (1.3) and (1.8), with appropriate

sketching matrices, also ﬁnds a solution when A is underdetermined.

Corollary 3.3. If m < n so that A ∈ Rm×n is underdetermined, and {Sk}m−1
k=1

is a sequence of full-rank matrices with Sm nonsingular, then xm solves (1.1).

Proof. At iteration k = m, matrix Sm is square and nonsingular so that the

update is given by (as in Corollary 3.2)

pk = WA(cid:62)(AWA(cid:62))†rk−1,

rk−1 = b − Axk−1.

Therefore

Axm = A(xm−1 + pm)

= Axm−1 + AWA(cid:62)(AWA(cid:62))†rm−1
= Axm−1 + (b − Axm−1),

and we conclude that Axm = b and that xm solves (1.1).

4. PLSS residuals. By storing a history of residuals in the sketching ma-
trix, we construct an iteration with orthogonal residuals and updates. Recall that
rk := b − Axk is the residual at iteration k (where since b is in the range of A, all
residuals are, too). Suppose the previous residuals have been stored, i.e., si = ri−1
(1 ≤ i ≤ k) so that the history of all residuals is in the matrix

(4.1)

Sk := [ r0

r1

· · ·

rk−1 ] ∈ Rm×k.

Since S(cid:62)
k rk = 0, all residuals are orthogonal with this choice of sketch. Moreover,
since all residuals are in the range of A, by Theorem 3.1, iteration (1.3) converges
in at most min(m, n) iterations in exact arithmetic. Deﬁning the scalar ρi = (cid:107)ri(cid:107)2
2
(0 ≤ i ≤ k − 1) we develop a 1-step recursive update.

8

J. J. BRUST AND M. A. SAUNDERS

Theorem 4.1. The update pk from (1.8) with Sk from (4.1) can be computed by

the 1-step recursive formula

(4.2)

(4.3)

(4.4)

pk = βk−1pk−1 + γk−1yk,
1

βk−1 :=

γk−1 :=

( (cid:107)pk−1(cid:107)(cid:107)yk(cid:107)
(cid:107)rk−1(cid:107)(cid:107)rk−1(cid:107) − 1)( (cid:107)pk−1(cid:107)(cid:107)yk(cid:107)

(cid:107)rk−1(cid:107)(cid:107)rk−1(cid:107) + 1)
1
(cid:107)yk(cid:107)2(1 − (cid:107)rk−1(cid:107)(cid:107)rk−1(cid:107)

(cid:107)pk−1(cid:107)(cid:107)yk(cid:107) )(1 + (cid:107)rk−1(cid:107)(cid:107)rk−1(cid:107)
(cid:107)pk−1(cid:107)(cid:107)yk(cid:107) )

.

,

Proof. From (2.12), pk can be represented as a linear combination of the columns

in Pk−1 and yk. Thus for a vector gk−1 ∈ Rk−1 and scalar γk−1 we have

(4.5)

pk = Pk−1gk−1 + γk−1yk.

k−1pk = 0, by multiplying (4.5) left and right by P(cid:62)

Since P(cid:62)
diagonal Θk−1 we obtain gk−1 = −γk−1Θ−1
that rk−1 = b − Axk−1 and xk−1 = xk−2 + pk−1, so that

k−1P(cid:62)

k−1yk. To simplify P(cid:62)

k−1 and solving with the
k−1yk, recall

(4.6)

rk−1 = rk−2 − Apk−1.

Because residuals are orthogonal, multiplying (4.6) on the left by S(cid:62)

k gives

ρk−1ek = ρk−2ek−1 − Y(cid:62)

k pk−1

and

ρk−1 = −y(cid:62)

k pk−1.

From (4.6) we also have

rk−1 = rk−3 − Apk−2 − Apk−1,

ρk−1ek = ρk−3ek−2 − Y(cid:62)

k pk−2 − Y(cid:62)

k pk−1,

and y(cid:62)
Therefore, P(cid:62)

k pk−1 = −ρk−1 implies that y(cid:62)

k pk−2 = 0. Similarly, y(cid:62)
k−1yk = −ρk−1ek−1 and gk−1 = (γk−1ρk−1/θk−1)ek−1. From (2.12),

k pk−i = 0 for i = 3 : k − 1.

(4.7)

pk =

γk−1ρk−1
θk−1

pk−1 + γk−1yk,

and from (1.8) we have y(cid:62)

k pk = S(cid:62)

k rk−1 = ρk−1. Thus, multiplying (4.7) by y(cid:62)

k gives

ρk−1 = −

γk−1ρ2
θk−1

k−1

+ γk−1(cid:107)yk(cid:107)2.

Solving the previous expression for γk−1 we ﬁnd

γk−1 =

θk−1ρk−1
θk−1(cid:107)yk(cid:107)2 − ρ2

=

βk−1 =

ρk−1
θk−1

γk−1 =

1

,

k−1

(cid:107)rk−1(cid:107)2( (cid:107)yk(cid:107)

(cid:107)rk−1(cid:107) − (cid:107)rk−1(cid:107)
(cid:107)rk−1(cid:107)4
((cid:107)pk−1(cid:107)(cid:107)yk(cid:107) − (cid:107)rk−1(cid:107)2)((cid:107)pk−1(cid:107)(cid:107)yk(cid:107) + (cid:107)rk−1(cid:107)2)

(cid:107)pk−1(cid:107) )( (cid:107)yk(cid:107)

(cid:107)rk−1(cid:107) + (cid:107)rk−1(cid:107)
(cid:107)pk−1(cid:107) )

.

Therefore we specify pk = βk−1pk−1 + γk−1yk.

4.1. W not the identity. Lifting the assumption from Section 3, suppose that
In this case, we may represent it in triangular factorized form

W = (B(cid:62)B)−1.

LINEAR PROJECTION SOLVERS

9

as W = R(cid:62)
transformed quantities

WRW (because W is symmetric and nonsingular). Further, deﬁne the

k−1 := R−(cid:62)
xW

W xk−1, pW

k := R−(cid:62)

W pk,

and AW := AR(cid:62)
W.

Then update (1.8) becomes

k = A(cid:62)
pW

WSk(S(cid:62)

k AWA(cid:62)

WSk)−1S(cid:62)

k rk−1.

As for pk, there is a recursion

(4.8)

with certain scalars βW
we ﬁnd from (4.8), by rewriting variables in terms of pk and yk, that

Wrk−1. Because pW

k−1 and γW

k := A(cid:62)

k−1 = R−(cid:62)

W pk,

k−1 + γW

k−1yW
k

k−1pW
k = βW
pW
k−1, where yW

(4.9)

where

βW
k−1 =

(cid:114)

(cid:0)

pk = βW

k−1pk−1 + γW

k−1Wyk,

(p(cid:62)

k−1W−1pk−1)(y(cid:62)
(cid:107)rk−1(cid:107)2(cid:107)rk−1(cid:107)2

k Wyk)

1

(cid:114)

− 1(cid:1)(cid:0)

(p(cid:62)

k−1W−1pk−1)(y(cid:62)
(cid:107)rk−1(cid:107)2(cid:107)rk−1(cid:107)2

k Wyk)

γW
k−1 =

(cid:114)

(cid:0)1 −

(cid:107)rk−1(cid:107)2(cid:107)rk−1(cid:107)2
k−1W−1pk−1)(y(cid:62)

(p(cid:62)

k Wyk)

(y(cid:62)

k Wyk)−1
(cid:114)
(cid:1)(cid:0)1 +

(cid:107)rk−1(cid:107)2(cid:107)rk−1(cid:107)2
k−1W−1pk−1)(y(cid:62)

(p(cid:62)

k Wyk)

,

.

+ 1(cid:1)

(cid:1)

Note that (4.9) can be evaluated eﬃciently as long as products and solves with W
are inexpensive. Therefore, we typically use the identity or a diagonal matrix that
normalizes the columns of A, namely W = diag(

1

(cid:107)A:,1(cid:107) , . . . ,

1
(cid:107)A:,n(cid:107) ).

5. Algorithm. Because of the short recursive update formula, our method can
be implemented eﬃciently as in Algorithm 5.1. The parameter matrix B, where
B(cid:62)B = W−1 and (B(cid:62)B)−1 = W, is optional.

We emphasize that Algorithm 5.1 updates only four vectors rk, yk, pk and xk (and
uses one intermediate vector WIyk). When the matrix WI is diagonal (which is typically
the case), we always apply it element-wise to a vector (i.e., WI . ∗ pk or WI .\ yk).
The algorithm uses one multiplication with A and one with A(cid:62) per iteration. For
practical implementation one may wish to change the stopping condition in the loop.
For instance, with consistent linear systems (where Ax = b can be solved exactly),
the condition sqrt(ρk)/norm(b) < τ for a tolerance τ > 0 may be used to stop the
iterations.

6. Relation to Craig’s method. The orthogonal residuals in our method are
reminiscent of Craig’s method [12, 13]. Indeed when W = I, the updates pk−1 from
(1.8) or (4.2) with Sk from (4.1) correspond to the updates in Craig’s method. For
background, Craig’s method can be developed using the Golub-Kahan bidiagonalization
procedure as described in [13, Secs. 3 & 7.2]. With β1u1 = b, β1 ≡ (cid:107)b(cid:107), the
bidiagonalization generates orthonormal matrices Uk and Vk in Rn×k and a lower
bidiagonal matrix Lk ∈ Rk×k with diagonals α1, . . . , αk and subdiagonals β2, . . . , βk.
After k iterations, the following relations hold:

(6.1)

(6.2)

AVk = UkLk + βk+1uk+1e(cid:62)
k ,
A(cid:62)Uk = VkL(cid:62)
k .

10

J. J. BRUST AND M. A. SAUNDERS

Algorithm 5.1 PLSS (Projected Linear Systems Solver)

WI = B(cid:62)B;

Ensure: A ∈ (m × n), x0, b, 0 < maxIt, (Optional : B(cid:62)B ∈ (n × n))
1: if B(cid:62)B (cid:54)= Empty then
2:
3: else
4:
5: end if
6: % Initialization

WI = I; % Identity size (n × n)

7:

xk = x0;
ρk = r(cid:62)

rk = b − A ∗ xk; yk = A(cid:62) ∗ rk;

k ∗ rk; WIyk = WI\yk;

φk = y(cid:62)

k ∗ WIyk;

k (WI ∗ pk)

8: pk = (ρk/φk). ∗ WIyk;
9: θk = p(cid:62)
10: xk = xk + pk;
11: while k < maxIt do
k = k + 1;
12:
rk = rk − A ∗ pk;
13:
yk = A(cid:62) ∗ rk;
14:
ρk = r(cid:62)
k ∗ rk;
15:
WIyk = WI\yk;
16:
φk = y(cid:62)
17:
βk = 1/((sqrt(θk ∗ φk)/ρk − 1) ∗ (sqrt(θk ∗ φk)/ρk + 1));
γk = (θk/ρk) ∗ βk;
pk = βk ∗ pk + γk ∗ WIyk;
θk = p(cid:62)
k (WI ∗ pk)
xk = xk + pk;

18:
19:
20:
21:
22:
23: end while

k ∗ Wiyk;

With zk ∈ Rk deﬁned by Lkzk = β1e1 and ζk denoting the kth element of zk, the
iterates in Craig’s method are computed as

(6.3)

(6.4)

xk = Vkzk = xk−1 + ζkvk,
rk = b − Axk = −ζkβk+1uk+1.

We prove that iterates generated by Craig’s method are equivalent to (1.8) with W = I
and using residuals in a sketch.

Theorem 6.1. The updates ζkvk ≡ pC
pk in (1.8) or (4.2) with W = I and Sk in (4.1).

k in Craig’s method are equal to the updates

Proof. First note that the residual in Craig’s method is

rk = b − Axk = b − A(xk−1 + ζkvk) = rk−1 − ζkAvk,

and also rk = −ζkβk+1uk+1 from (6.4). By orthogonality of Uk it holds that

(6.5)

U(cid:62)

k rk = 0

and U(cid:62)

k A(ζkvk) = U(cid:62)

k rk−1.

We deﬁne the update as ζkvk ≡ pC

k . The scalar ζk is deﬁned recursively as ζk =

LINEAR PROJECTION SOLVERS

11

− βk
αk

ζk−1 (with ζ0 ≡ −1, cf. [13, Sec. 7.2]). From (6.1)–(6.2) we deduce

A(cid:62)rk−1 = A(cid:62)(−ζk−1βkuk)

= A(cid:62)(−ζk−1βkUkek)
= −ζk−1βkVkL(cid:62)
k ek


0
βk
αk

= −ζk−1βkVk





= −ζk−1βkαkvk − ζk−1βkVk









0
βk
0

= α2

kpC

k − ζk−1βkVk(L(cid:62)

k − Lk)ek.

Using φk ≡ −α2

k/(ζk−1βk) and a vector λk ∈ Rk, we have

(6.6)

−A(cid:62)rk−1
ζk−1βk

= A(cid:62)uk =

−α2
k
ζk−1βk

k + Vk(L(cid:62)
pC

k − Lk)ek ≡ φkpC

k + A(cid:62)Ukλk.

Combining the second equality in (6.5) with (6.6) gives the system

(cid:20) φkI A(cid:62)Uk
U(cid:62)

0

k A

(cid:21)

(cid:21) (cid:20)pC
k
λk

=

(cid:21)

(cid:20) A(cid:62)uk
U(cid:62)
k rk−1

,

which leads to

(6.7)

A(cid:62)Uk(U(cid:62)

k AA(cid:62)Uk)−1U(cid:62)

k rk−1 = pC
k .

With Sk as deﬁned in (4.1), matrices Uk and Sk are closely related:

Uk = (cid:2)r0

r1

· · ·

rk−1

(cid:3) Dk = SkDk, Dk ≡ −diag(ζ0β1, ζ1β2, . . . , ζk−1βk).

Substituting Uk = SkDk into (6.7), we obtain

(6.8)

A(cid:62)Sk(S(cid:62)

k AA(cid:62)Sk)−1S(cid:62)

k rk−1 = pC
k .

Comparing (6.8) with (1.8) we see that the updates are the same when W = I.

7. PLSS Kaczmarz. Another way to construct the sketching matrix is to con-
catenate columns of the identity matrix ek in the sktech Sk = (cid:2)e1 e2
(cid:3).
When the columns are assembled in a random order, this process generalizes the
randomized Kaczmarz iteration [9, 17]. We denote a random identity column by eik .
th element of
From S(cid:62)
rk−1. Thus the updates with this sketch are (cf. (1.8))

k−1rk−1 = 0 we have S(cid:62)

k rk−1 = r(k−1)

, where r(k−1)

is the ik

e(cid:62)
ik

· · ·

ek

ik

ik

pk = r(k−1)

ik

Yk(Y(cid:62)

k Yk)−1eik .

In the notation of Sections 2.5 and 2.6, the updates are orthogonal and can be
represented by

pk = Pk−1gk−1 + γk−1yk.

12

J. J. BRUST AND M. A. SAUNDERS

Since yk = A(cid:62)eik = aik (the ik
we develop the update

th row of A) and from y(cid:62)

k pk = r(k−1)

ik

= e(cid:62)
ik

(b−Axk−1),

(7.1)

(7.2)

(7.3)

pk = γk−1(aik − Pk−1Θ−1

k−1P(cid:62)
(b − Axk−1)

k−1aik ),

e(cid:62)
ik

γk−1 =

((cid:107)aik (cid:107)2 − (cid:107)dk−1(cid:107)2)((cid:107)aik (cid:107)2 + (cid:107)dk−1(cid:107)2)

,

dk−1 ≡ Θ−1/2

k−1 P(cid:62)

k−1aik .

If the history of previous updates is not used, i.e., Pk−1 = 0, then pk is equal to
the update in the Kaczmarz method. The Kaczmarz method is useful, especially in
situations where the entire matrix is not accessible (possibly because of its size), because
it enables versions that access only one row of A each iteration. Note that update
(7.1) can be implemented with one multiplication of Pk−1 and one A. To compute
k A(cid:62)eik , we compute Apk
= p(cid:62)
the next update using rk = rk−1 − Apk and e(cid:62)
(cid:3)(cid:62)
and append it to an array that stores (cid:2)Ap1
k A(cid:62). With this, pk+1
= P(cid:62)
k A(cid:62) to obtain P(cid:62)
can be computed by selecting the (k + 1)th column of P(cid:62)
then
multiplying Pk(Θ−1/2
− Pk(Θ−1/2
dk)).

dk) and forming pk+1 = γk(a(cid:62)

k P(cid:62)
k a(cid:62)
ik
· · · Apk

k a(cid:62)

ik+1

ik+1

k

k

8. Numerical experiments. Our algorithms are implemented in MATLAB and
PYTHON 3.9. The numerical experiments are carried out in MATLAB 2016a on a
MacBook Pro @2.6 GHz Intel Core i7 with 32 GB of memory. For comparisons, we
use the implementations of [6], a randomized version of (1.8), Algorithm 5.1, LSQR
and CRAIG [13, p.58], [14]. All codes are available in the public domain [2]. The
stopping criterion is either (cid:107)Axk − b(cid:107)2/(cid:107)b(cid:107)2 ≤ (cid:15) or (cid:107)Axk − b(cid:107)2 ≤ (cid:15), depending on the
experiment. Unless otherwise speciﬁed, the iteration limit is n. We label Algorithm
5.1 with W = I and W = diag(
(cid:107)A:,n(cid:107) ) as PLSS and PLSS W respectively.
Our implementation of update (7.1) with random columns of the identity matrix is
called PLSS KZ .

(cid:107)A:,1(cid:107) , · · · ,

1

1

8.1. Experiment I. This experiment uses moderately large sparse matrices
with m > n and 103 ≤ n ≤ 104. All linear systems are consistent and we set
x=ones(n,1); x(1)=10; and b=A*x; with x0 = 0. For reference, we add the method
“Rand. Proj.”, which uses update formula (1.8) (with W = I) yet with Sk ∈ Rm×r
being a random standard normal matrix. The sketching parameter is r = 10. Each
iteration with this method thus recomputes Yk = A(cid:62)Sk ∈ Rn×r, Y(cid:62)
k Yk, and a solve
with the latter matrix. Thus the computational eﬀort with this approach is typically
much larger than with the proposed methods. Relative residuals (cid:107)Axk − b(cid:107)2/(cid:107)b(cid:107) ≤ (cid:15)
are used to stop with (cid:15) = 10−2. Table 1 records detailed outcomes of the experiment.

8.2. Experiment II. In this experiment the matrices are large with m > n
and 104 ≤ n ≤ 107. The right-hand side b and starting vector x0 are initialized as
in Experiment I. Because computing full random normal sketching matrices is not
feasible for these large matrices, we use sprandn instead of randn in a randomized
implementation of (1.8). The parameter r is set as follows: If n > 105 then r = 5 else
r = 50. Convergence is determined if (cid:107)Axk − b(cid:107)2 ≤ (cid:15) with (cid:15) = 10−2. The iteration
limit is 500 and outcomes are reported in Table 2.

8.3. Experiment III. This experiment is on underdetermined systems m < n.
The right-hand side b, starting vector x0 and random sketching matrix are computed
as in Experiment I. Convergence is determined if (cid:107)Axk − b(cid:107)2 ≤ (cid:15) with (cid:15) = 10−4. The
iteration limit is n + 1500 and outcomes are reported in Table 3.

LINEAR PROJECTION SOLVERS

13

Table 1: Experiment I compares 4 solvers on 42 linear systems from the SuiteSparse
Matrix Collection [4]. “Dty” in column 3 is the density of a particular matrix A
calculated as Dty = nnz(A)
m·n . Entries with NC† denote problems for which the solver
did not converge to the speciﬁed tolerance. Bold entries mark the fastest times, while
second fastest times are italicized.

It

Dty

m/n

Problem

Rand. Proj.
Sec
0.8
46†
51†
0.56†
4.8†
5.7
3.9
3.8
3
22
13
13
1.8†

Res
It
0.01
62
0.003 1192
2658/2525
lpi gran
0.02†
0.006 2704†
33
71952/2704
landmark
0.7† 2258
Kemelmacher 28452/9693 0.0004 9693†
0.08†
1034†
97
0.01
1964/1034
Maragal 4
0.08† 181
0.006 3320†
4654/3320
Maragal 5
8
0.01
0.001 3878
6784/5252
Franz4
4
0.01
0.002 2536
7382/2882
Franz5
3
0.01
0.002 2434
7576/3016
Franz6
1
0.01
10164/1740
0.002 1555
Franz7
4
0.01
16728/7176 0.0008 6541
Franz8
7
0.01
0.001 3270
19588/4164
Franz9
7
0.01
0.001 3322
19588/4164
Franz10
0.03†
0.004 1019†
14
8899/1019
GL7d12
47271/8899 0.0008 8899† 1.1e+02† 0.02†
19
GL7d13
0.002 2035
4
0.01
5400/2400
ch6-6-b3
0.001 3531
4
0.01
12600/4200
ch7-6-b3
0.003 1003
3
0.01
11760/1176
ch7-8-b2
0.002 1326
3
0.01
17640/1512
ch7-9-b2
3
0.01
0.002 1377
18816/1568
ch8-8-b2
2
0.01
0.003 1118
5970/1330
cis-n4c6-b3
2
0.01
cis-n4c6-b4 20058/5970 0.0008 4462
6
0.01
4725/3150
0.001 3047
mk10-b3
5
0.01
17325/6930 0.0006 6079
mk11-b3
3
0.01
0.002 1306
13860/1485
mk12-b2
1
0.01
0.004
3003/1365
929
n2c6-b4
1
0.01
0.002 1861
4945/3003
n2c6-b5
4
0.01
0.001 3585
5715/4945
n2c6-b6
1
0.01
0.004
3003/1365
907
n3c6-b4
1
0.01
0.002 1825
5005/3003
n3c6-b5
1
0.01
0.001 2729
6435/5005
n3c6-b6
3
0.01
0.004
2852/1350
953
n4c5-b4
3
0.01
0.002 1925
4340/2852
n4c5-b5
3
0.01
0.002 2417
4735/4340
n4c5-b6
2
0.01
5970/1330
0.003 1114
n4c6-b3
2
0.01
20058/5970 0.0008 4483
n4c6-b4
0
0
21924/1045
rel7
0
0
21924/1045
relat7b
0
0
21924/1045
relat7
mesh deform 234023/9393 0.0004 9393† 4.2e+02† 0.1†
290
0.02†
90
162bit
0.02† 141
176bit
50
specular

2.3
8.6
2.2
4.2
4.7
1.3
17
2.9
20
3.3
0.58
1.9
4.9
0.56
1.9
3.8
0.57
1.8
2.7
1.3
17
0.0047
0.0042
0.004

3.7†
16†
1600† 2.2e+02† 0.02†

0.003 3597†
0.001 7431†
0.01

3606/3597
7441/7431
477976/1600

0.002
0.004
0.004

0
0
0

0

PLSS
Res
Sec
0.01
0.012
0.01
0.083
0.01
0.96
0.009
0.0085
0.052
0.01
0.0017 0.007
0.00081 0.001
0.00069 0.006
0.00042
0.0015 0.005
0.006
0.0034
0.006
0.003
0.008
0.003
0.009
0.033
0.0005 0.008
0.0011 0.004
0.001
0.002
0.001
0.001
0.0011 0.0005
0.00024 2e-17
0.0011 0.005
0.00065 0.002
0.0016 0.001
0.00086 0.002
0.00016 8e-17
0.00022 2e-16
0.0011 0.008
0.00016 8e-17
0.00021 2e-16
0.00035 8e-17
0.0003 0.004
0.00044 0.009
0.00069 0.008
0.00026 2e-17
0.0012 0.005
0.00025
0.00021
0.00022
0.95
0.017
0.055
0.81

0
0
0
0.01
0.01
0.009
0.01

It
26
9
1323
57
141
4
3
4
2
4
4
4
7
7
4
4
3
3
3
2
2
6
5
3
1
1
4
1
1
1
3
3
3
2
2
0
0
0
243
44
46
6

PLSS W
Sec
0.011
0.039
0.63

It
Res
0.01
2525
0.008 1196
839
0.01
362
0.0063 0.009
588
0.009
0.047
10
0.003
0.0019
5
0.005
0.0014
5
0.003
0.0016
1
0.00087 0.005
7
0.0032
0.004
12
0.0031 0.005
12
0.0028 0.005
32
0.0019 0.008
44
0.009
0.017
6
0.00081 0.008
6
0.004
0.0018
5
0.001
0.001
5
0.0014
0.001
4
0.0014 0.0005
2
0.00049 2e-17
4
0.008
0.002
7
0.002
0.0011
6
0.001
0.0023
4
0.0011
0.002
1
0.00031 2e-16
1
0.00049 2e-16
8
0.008
1
0.00029 2e-16
1
0.00048 2e-16
1
0.0013
2e-16
5
0.00045 0.003
5
0.00075 0.005
7
0.007
0.0013
2
0.00043 2e-17
4
0.008
0.0018
0
0
0.00085
0
0
0.00082
0
0.00058
0
551
0.01
0.82
1008
0.01
0.01
0.01
0.021
1688
0.009 1600
0.2

0.002

LSQR
Res
Sec
3e-05
0.42
4e-05
2.8
0.005
0.52
0.0002
0.045
0.0001
0.2
0.0044
7e-05
0.0019 0.0001
5e-05
0.0017
0.00088 3e-15
0.0033 0.0001
2e-06
0.0076
2e-06
0.0072
4e-05
0.0085
1e-05
0.08
0.0016 0.0003
0.0023 0.0002
4e-06
0.0019
6e-06
0.0024
0.0022
7e-16
0.00082 5e-16
7e-05
0.0034
7e-16
0.0017
4e-16
0.0032
0.0019
8e-16
0.00059 4e-14
0.0016
4e-16
0.0037 0.0001
0.00059 4e-14
0.00066 4e-16
0.00098 1e-13
0.0011 0.0001
0.0014 0.0003
8e-05
0.0029
0.00072 5e-16
7e-05
0.0032
0
0.0014
0
0.00043
0.00041
0
0.0001
1.9
1e-05
0.3
7e-06
0.89
3e-05
26

14

J. J. BRUST AND M. A. SAUNDERS

Table 2: Experiment II compares 4 solvers on 51 linear systems from the SuiteSparse
Matrix Collection [4]. “Dty” in column 3 is the density of a particular matrix A
calculated as Dty = nnz(A)
m·n . Entries with NC† denote problems for which the solver
did not converge to the speciﬁed tolerance. Bold entries mark the fastest times, while
second fastest times are italicized.

Problem

graphics
deltaX
NotreDame actors
ESOC
psse0
psse1
psse2
Rucci1
Maragal 6
Maragal 7
Franz11
IG5-16
IG5-17
IG5-18
GL7d14
GL7d15
GL7d16
GL7d17
GL7d18
ch7-6-b4
ch7-8-b3
ch7-8-b4
ch7-9-b3
ch7-9-b4
ch7-9-b5
ch8-8-b3
ch8-8-b4
ch8-8-b5
D6-6
mk12-b3
mk12-b4
n4c6-b5
n4c6-b6
n4c6-b7
n4c6-b8
shar te2-b2
kneser 10 4 1
kneser 8 3 1
wheel 601
rel8
rel9
relat8
relat9
sls
image interp
192bit
208bit
tomographic1
LargeRegFile
JP
Hardesty2

7

7

It

It

It

Dty

m/n

Res
2†

PLSS W

It
500†

Res
0.05†

5.8†
21†
59†
38†

12†
29†
29†
23†
49†
43†

PLSS
Sec
500† 0.18†

Sec
0.2 †
0.08† 500† 0.43† 0.0002† 500† 0.46 †
4.2†
9†

Rand. Proj.
Sec
10†
0.0003 500†
29493/11822
25†
0.0002 500†
68600/21961
3e-05 500† 2.5e+02† 0.1† 500†
392400/127823
0.0005 500† 2.8e+02† 0.09† 500†
327062/37830
9.5†
0.0003 500†
26722/11028
5.8†
0.0004 500†
14318/11028
0.0004 500†
11†
28634/11028
4e-05 500† 1e+03†
1977885/109900
0.002 500†
21255/10152
0.001 500†
46845/26564
0.0002 500†
47104/30144
0.002 500†
18846/18485
0.001 500†
30162/27944
0.0009 500†
47894/41550
0.0002 500† 1.1e+02† 0.7†
171375/47271
8e-05 500† 4.4e+02† 0.9†
460261/171375
1†
3e-05 500† 1.4e+03†
955128/460261
1†
2e-05 500† 2.6e+03†
1548650/955128
1†
1955309/1548650 1e-05 500† 3.6e+03†
0.2†
0.0004 500†
0.3†
0.0003 500†
0.7†
9e-05 500†
0.0002 500†
0.5†
5e-05 500† 1.3e+02† 0.8†
2e-05 500† 2.3e+02† 0.9†
0.0002 500†
0.5†
4e-05 500† 2.1e+02† 0.8†
1†
2e-05 500† 3.7e+02†
0.5†
5e-05 500†
0.4†
0.0003 500†
0.6†
0.0001 500†
0.5†
0.0003 500†
0.7†
0.0001 500†
0.9†
8e-05 500†
0.9†
6e-05 500† 1e+02†
0.4†
0.0002 500†
0.6†
9e-06 500† 2e+02†
0.0002 500†
0.09†
3e-06 500† 5.9e+02† 0.1†
0.0002
9e-06
0.0003
12360060/549336 6e-06

LSQR
Res
Sec
Res
0.003†
0.01†
500† 0.22†
2e-05†
5e-05† 500† 0.51†
0.0001†
4.2 †
1e-05† 500†
0.001† 500†
3.9†
0.003†
500†
0.03†
8.7†
500†
0.05†
8.9 †
0.06†
500† 0.22†
0.6†
500† 0.21 †
0.8†
1†
500† 0.19†
0.07†
500† 0.19†
1†
500† 0.18 †
6†
1†
500† 0.15†
0.08†
500† 0.25†
0.4†
500† 0.25 †
6†
1†
500† 0.22†
0.2†
13†
388†
2†
17†
500†
3†
17 †
500†
1†
0.001†
0.002† 500† 0.47 †
500† 0.51†
0.01†
0.4† 500† 0.46†
0.001†
1.4†
0.003† 500†
1.3†
500†
0.01†
1.3 †
0.6† 500†
0.3†
6e-06
0.024
4e-05
0.029
6e-06
5
0.023
0.0001†
1.5†
1e-05† 500†
1.4 †
0.001† 500†
1.3†
0.02† 500†
0.0001†
1.3†
9e-06† 500†
1.6 †
0.001† 500†
2†
0.03† 500†
0.0001†
2.2 †
1e-05† 500†
2.3†
0.001† 500†
2†
0.04† 500†
6e-06
0.33
41
7e-06
0.29
33
7e-06
0.33
43
4e-06
1.7
57
3e-06
1.4
47
3e-06
1.6
59
2e-06
8.1
57
2e-06
6.2
43
2e-06
7.8
58
2e-06
13
55
2e-06
11
45
1e-06
12
56
2e-06
23
71
1e-06
21
63
2e-06
23
74
8e-05
12 0.0043
0.0073
13
13 0.0034 3e-05
3e-05
0.0052 0.0002
5
0.0049 0.0002
5
0.0037 0.0002
5
1e-05
0.022
9
2e-05
0.031
9
0.019
9
2e-05
0.0001
0.008
5
0.0001
0.013
5
0.0065 0.0001
5
8e-06
0.054
9
8e-06
0.068
9
8e-06
0.046
9
0.0004
0.13
11
0.0002
0.16
12
0.0002
0.1
12
5e-05
0.011
5
5e-05
0.013
5
5e-05
0.008
5
5e-06
0.059
8
5e-06
0.074
8
5e-06
0.05
8
0.0003
0.17
9
0.0003
0.18
9
0.0003
0.12
9
3e-05
0.02
18
3e-05
0.023
18
0.017
18
4e-05
3e-05
0.0064
6
3e-05
0.0058
6
0.0045 3e-05
6
1e-14
0.015
11
3e-16
0.017
11
0.012
11
3e-16
1e-15
0.0028
2
9e-05
0.0065
4
0.0025 2e-16
2
2e-05
0.018
6
2e-05
0.024
6
0.014
6
2e-05
0.0001
0.027
5
8e-05
0.039
5
0.022 0.0001
5
8e-06
0.067
9
7e-06
0.084
9
8e-06
0.052
9
1e-05
0.015
7
1e-05
0.02
7
1e-05
0.015
7
2e-06
0.25
31
2e-06
0.25
31
32
2e-06
0.19
2e-05
26 0.0082
1e-05
0.01
26
28 0.0063 1e-05
7e-07
0.59
42
6e-07
0.78
42
6e-07
42
0
0.0031
0.28
0
0
0.008
0
0
0
0
0.21
9.6
0
0
0.49
0
0
0
0
0.0044
0.32
0
0
0.029
0
0
0
0
0.4
0
0
0.85
0
0
13
0
6e-05 500† 9.6e+02† 0.08† 392
3e-06
334
2e-06
5.2
130
3e-06
16
0.3†
1.5 †
500†
0.3†
2e-05 500† 1e+02†
0.02†
500†
1.3†
500†
5e-06 500† 0.31† 0.0004†
0.007†
0.06† 500† 0.24 †
0.0008 500†
7.5†
0.18
329
3e-06 500† 0.59† 0.0007†
0.007†
0.08† 500† 0.49 †
0.0005 500†
16†
0.33
304
9e-05†
0.0008† 500†
1.1 †
0.001† 500†
0.1† 500† 0.95†
39†
0.0001 500†
0.004†
4e-06 500†
0.4†
18 †
3e-06 500† 1.1e+03† 0.6† 500†
2.1
53
0.0005†
0.002† 500†
17†
0.009† 500†
17 †
0.002 500† 2.3e+02† 0.03† 500†
0.002†
500†
0.02†
7 †
500†
0.03†
6.4†
500†
1e-05 500† 4.4e+02†

15120/12600
58800/11760
141120/58800
105840/17640
317520/105840
423360/317520
117600/18816
376320/117600
564480/376320
120576/23740
51975/13860
62370/51975
51813/20058
104115/51813
163215/104115
198895/163215
200200/17160
349651/330751
15737/15681
902103/723605
345688/12347
9888048/274669
345688/12347

1748122/62729
240000/120000
13691/13682
24430/24421
73159/59498
2111154/801374
87616/67320
929901/303645

0.87
0.0075
0.23
0.0046
0.41
13
1.7†

44†
20†
30†
22†
50†
78†

1.2†
20†
17†
7.3†

0
0
0
0

0
0
0
0

6.3†

73†

47†

1†

1†

LINEAR PROJECTION SOLVERS

15

Table 3: Experiment III compares 4 solvers on 42 linear systems from the SuiteSparse
Matrix Collection [4]. “Dty” in column 3 is the density of a particular matrix A
calculated as Dty = nnz(A)
m·n . Entries with NC† denote problems for which the solver
did not converge to the speciﬁed tolerance. Bold entries mark the fastest times, while
second fastest times are italicized.

It

It

3†

1†

Res

Dty

m/n

2†
4†

Problem

0.003 2660† 0.5† 2e+01†

1148 0.15 0.0001
0.06†
2†

It
1697
398
1129
3109
2897
4871†
84
7331†
324
522
2528†
90
21
15
93
92

346
89
120
114
211
3433
3433
171
2e+03† 3466†
0.0001
0.1†
2†

Rand. Proj.
Sec
0.007 3376† 0.9† 1e+02† 3376†
821/1876
lp 25fv47
0.005 3086† 0.58† 2e+01† 1916
643/1586
lp bnl1
2324/4486 0.001 5986† 3.4† 3e+01† 5878
lp bnl2
3516/7248 0.0007 8748† 6.7† 2e+02† 8748†
lp cre a
3068/6411 0.0008 7911† 5.6† 2e+02† 7911†
lp cre c
1903/3371 0.003 4871† 2.6† 2e+02† 4871†
lp cycle
0.003 5062† 1.8†
929/3562
123
lp czprob
3e+02† 7331†
6†
2171/5831 0.003 7331†
lp d2q06c
7684† 4.8†
240
0.01
415/6184
lp d6cube
1503/2604 0.006 4104† 2.2†
1150
lp degen3
0.01
lp fffff800 524/1028
0.005 2564† 0.43†
497/1064
lp finnis
0.5
24/1049
lp fit1d
0.009 3177† 0.74†
627/1677
lp fit1p
1309/1706 0.003 3206† 1.1†
lp ganges
lp gfrd pnc 616/1160
lp greenbea 2392/5598 0.002 7098† 5.7†
lp greenbeb 2392/5598 0.002 7098† 5.6†
2426/3602 0.001 5102† 3.2†
lp ken 07
1†
0.006 3466†
846/1966
lp maros
14
lp maros r7 3136/9408 0.005 9409
13
0.003 3120† 0.72†
70
687/1620
lp modszk1
2953/7716 0.0007 9216† 7.6†
119
lp pds 02
0.007 3006† 0.68† 2e+03† 3006†
625/1506
lp perold
1441/4860 0.006 6360† 4.4† 3e+01† 3344
lp pilot
2623† 0.44† 1e+03† 2623†
410/1123
lp pilot4
lp pilot87 2030/6680 0.006 8180† 7.6† 5e+02† 7563
0.007 3767† 1.3† 3e+05† 3767†
lp pilot ja 940/2267
0.004 4428† 1.2† 2e+03† 4428†
lp pilot we 722/2928
0.006 3946† 1.3† 1e+05† 3946†
lp pilotnov 975/2446
9
lp qap12
lp qap8
lp scfxm2
lp scfxm3
lp scrs8
lp scsd6
lp scsd8
lp sctap2
lp sctap3
lp shell
lp ship04l
lp ship04s

PLSS
Res
Sec
0.09†
0.1 †
7e-05
0.036
9e-05
0.28
0.1†
0.73†
0.3†
0.5 †
9†
0.28†
8e-05
0.0049
2†
0.94 †
7e-05
0.041
8e-05
0.091
2†
2528† 0.43† 2e+02† 2528† 0.053†
8e-05
0.0059
1e-05
0.0029
7e-05
0.0041
8e-05
0.0033
7e-05
0.0036
0.0001 1604
0.3
0.0001 1604
0.3
1e-05
162
0.0079
6e+01† 3466†
0.11†
4e-05
0.0039
14
0.0016
8e-05
61
0.0087 0.0001
106
3e+01†
0.07 †
844
9e-05
0.38
660
2†
0.05 †
410
8e-05
1.2
685
2e+03† 3767†
0.16†
4e+01† 2711
0.16 †
0.15†
1e+03† 3946†
0.0011
5e-12
0.00021 1e-13
0.02†
0.002†
0.1†
2e-05
6e-05
9e-05
0.0001
8e-05
8e-05
0.0001

0.0001
0.005 2351 0.65 0.0001
0.007 2700† 0.56† 5e+01† 2700†
0.06 †
0.005 3300† 0.97† 8e+01† 3300† 0.097 †
0.005 2775† 0.48† 7e+01† 2775† 0.048 †
0.0011
0.02
0.008 4250† 0.92†
0.0047
0.025
0.036
0.0019
0.0019
0.002

3192/8856 0.001 9114
912/1632
660/1200
990/1800
490/1275
2850† 0.34† 0.006†
147/1350
0.01†
397/2750
1090/2500 0.003 4000† 1.3† 3e+01†
1480/3340 0.002 4840† 2.3† 4e+01†
536/1777
402/2166
402/1506

0.004 3277† 0.65†
0.007 3666† 0.71†
0.007 3006† 0.49†

7
7
1091
1113
747
54
130
37
40
86
63
80

55
124
785
834
85
70
90

8†
9†
3†

1†
1†
1†

0.01

7
7

It

CRAIG
PLSS W
Res
Sec
Res
Sec
0.3†
0.11†
9e-05 3376†
0.065
8e-05
0.037
1741
9e-05
0.013
9e-05
0.3
5417
9e-05
0.069
0.5†
0.72 †
9e-05 8748†
0.33
0.6†
0.57†
0.0001 7911†
0.23
4e+01†
3†
0.29 †
4871†
0.32†
6e-05
9e-05
0.0044
109
0.0043
2e+01†
2†
0.94†
7331†
1.1†
0.0001
9e-05
0.032
191
0.06
0.0001
9e-05
1009
0.082
0.046
9†
0.0006† 2528† 0.057 †
0.062†
9e-05
0.0058
330
0.0019
3e-05
2e-05
0.0021
65
0.00098 3e-05
2e-05
0.0039
103
8e-05
0.0008
9e-05
0.0037
113
7e-05
0.0033
0.0001
0.0039
6e-05
0.0021
203
0.0001
0.3
0.0001 3151
0.18
0.0001
0.3
0.0001 3151
0.17
5e-05
0.0087
167
5e-05
0.0096
1e+03†
0.13 †
3466†
0.2†
0.13†
0.0032 0.0001
13
1e-05
0.0058
8e-05
0.0017
69
6e-05
0.0017
0.0001
0.01
6e-05
0.011
117
5e+02†
0.079†
9e-05 3006†
0.024
8e-05
0.35
9e-05
0.087
3047
9e+01†
0.053†
8e-05 2623†
0.0095
0.0001
1
9e-05
0.13
6880
2e+04†
0.16 †
9e+02† 3767†
0.18†
4e+02†
0.17†
0.0001 4428†
0.13
0.18†
8e+04†
0.17 †
8e+02† 3946†
0.00091 4e-10
1e-12
0.0016
8e-12
0.00022
4e-13
0.00033
0.005†
0.068†
9e-05 2700†
0.03
0.0008†
0.11†
8e-05 3300†
0.039
0.3†
0.055†
0.0001 2775†
0.017
6e-05
0.0011
2e-05
0.0013
7e-05
0.0044
6e-05
0.0062
0.0001
0.028
6e-05
0.0016
9e-05
0.039
5e-05
0.0024
9e-05
0.002
6e-05
0.0023
8e-05
0.002
4e-05
0.0022
9e-05
0.002
4e-05
0.0022

54
123
750
814
82
69
88

6
6

16

J. J. BRUST AND M. A. SAUNDERS

8.4. Experiment IV. In this experiment we compare the proposed solvers with
the implementations from [6]. The same test problems are used (i.e., aloi-scale,
covtype-libsvm, protein, SUSY, and four additional ones). The quantities A and b
are obtained from LIBSVM [3]. Convergence is determined if the norm of residuals is
less than or equal to (cid:15) = 10−4. The outcomes are displayed in Figure 1, with residuals
for our proposed solvers and four methods from [6, Section 7.3].

8.5. Randomized projection. For further comparison of our methods with
randomized projections, we use the unsymmetric ill-conditioned ‘sampling’ matrix
from MATLAB’s matrix gallery with n = 100; A = gallery(‘sampling’,n). The
solution is the vector of all ones, and x0 = 0. For this problem, cond(A) = 2.7859e+17.
In Figure 2 we compare Algorithm 5.1 (with W = I) to updates (1.8) with random
normal sketching matrices and varying dimensions of the sketch (i.e., r varies). As
expected, the convergence behavior of the randomized methods improves when r
increases from 10 to 40. At the same time, PLSS uses a simple recursive update (with
very low computational cost) and converges signiﬁcantly more rapidly than any of the
random sketches.

9. Conclusions. We develop an iterative projection method for solving consistent
rectangular or square systems. Our method is based on appending one column each
iteration to the sketching matrix. For full-rank sketches (common in practice) we
prove that the underlying process terminates, with exact arithmetic, in a ﬁnite number
of iterations. When the sketching matrix stores the history of all previous residuals,
we develop a method with orthogonal residuals and updates. We include a parameter
matrix that can be used to improve computations. Importantly, we derive a short
recursive formula that is simple to implement, and an algorithm that updates only
four vectors. In numerical experiments, including large sparse systems, our methods
compare favorably to widely known methods such as LSQR or CRAIG and randomized
methods.

Appendix A. Optimality.
constrained optimization problem

Solving linear system (1.5) is equivalent to the

(A.1)

(A.2)

arg min
p∈Rn

subject to

(cid:107)Bp(cid:107)2

2 − pA(cid:62)rk−1

1
2
k A(xk−1 + p) = S(cid:62)
S(cid:62)

k b.

When Sk ∈ Rn×k is in the range of A = UΣV(cid:62), it can be represented as Sk = UTk
using a nonsingular matrix T ∈ Rk×k. Thus constraint (A.2) implies

ΣV(cid:62)(xk−1 + p) = U(cid:62)b

or Ap = UU(cid:62)rk−1,

with rk−1 = b − Axk−1. Since r(cid:62)
problem (A.1)–(A.2) is equivalently represented by

k−1Ap = (cid:107)U(cid:62)rk−1(cid:107)2

2 is constant with respect to p,

arg min
p∈Rn

subject to

(cid:107)Bp(cid:107)2
2

1
2
k A(xk−1 + p) = S(cid:62)
S(cid:62)

k b.

Appendix B. Linear combination of updates.

LINEAR PROJECTION SOLVERS

17

We derive a representation of pk as a linear combination of previous updates

pk−1, . . . , p1. Recall from (2.9) that

pk =

(cid:16)

sk

(cid:62)rk−1
δk

yk − Yk−1(cid:98)tk

(cid:17)

,

where

(cid:98)tk = Rk−1Dk−1R(cid:62)

k−1Y(cid:62)

k−1yk =

(cid:18) (cid:20)Rk−2Dk−2R(cid:62)

k−2

(cid:21)

0

+

tk−1t(cid:62)
δk−1

k−1

(cid:19)

Y(cid:62)

k−1yk

(with superscript indices suppressed). From this we deduce

Yk−1(cid:98)tk = Yk−2Rk−2Dk−2R(cid:62)

k−2Y(cid:62)

k−2yk +

= Yk−2Rk−2Dk−2R(cid:62)

k−2Y(cid:62)

k−2yk +

= Yk−2Rk−2Dk−2R(cid:62)

k−2Y(cid:62)

k−2yk −

Yk−1tk−1

(Yk−2(cid:98)tk−1 − yk−1)

k−1yk)

k−1yk)

k−1(Y(cid:62)
t(cid:62)
δk−2
k−1(Y(cid:62)
t(cid:62)
δk−1
k−1(Y(cid:62)
s(cid:62)
k−1rk−2

(t(cid:62)

k−1yk))

pk−1.

Now for j = 1 : k − 1, deﬁne scalars

αj =

j yk)

j (Y(cid:62)
t(cid:62)
s(cid:62)
j rj−1

,

so that writing Rk−2Dk−2Rk−2 in terms of Rk−3Dk−3Rk−3 and tk−2 (and recursively
backwards) gives

Yk−1(cid:98)tk = −

k−1
(cid:88)

j=1

αjpj.

Hence the update pk can be represented by previous updates and the vector yk:

pk =

(cid:16)

sk

(cid:62)rk−1
δk

yk − Yk−1(cid:98)tk

(cid:17)

=

sk

(cid:62)rk−1
δk

(cid:0)yk +

k−1
(cid:88)

j=1

αjpj

(cid:1).

REFERENCES

[1] N. Ailon and B. Chazelle, The fast Johnson–Lindenstrauss transform and approximate

nearest neighbors, SIAM J. Computing, 39 (2009), pp. 302–322.

[2] J. J. Brust, Code for Algorithm PLSS and test programs. https://github.com/johannesbrust/

PLSS, 2022.

[3] C.-C. Chang and C.-J. Lin, LIBSVM: A library for support vector machines, ACM Trans. on

Intelligent Systems and Technology (TIST), 2 (2011), pp. 1–27.

[4] T. A. Davis, Y. Hu, and S. Kolodziej, SuiteSparse matrix collection. https://sparse.tamu.edu/,

2015–present.

[5] G. H. Golub and C. F. Van Loan, Matrix Computations, The Johns Hopkins University Press,

Baltimore, Maryland, third ed., 1996.

[6] R. M. Gower and P. Richt´arik, Randomized iterative methods for linear systems, SIAM
J. Matrix Anal. Appl., 36 (2015), pp. 1660–1690, https://doi.org/10.1137/15M1025487,
https://doi.org/10.1137/15M1025487.

18

J. J. BRUST AND M. A. SAUNDERS

[7] N. Halko, P. Martinsson, and J. Tropp, Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions, SIAM Review, 53 (2011),
pp. 217–288, https://doi.org/10.1137/090771806.

[8] W. Johnson and J. Lindenstrauss, Extensions of Lipschitz maps into a Hilbert space, Contem-
porary Mathematics, 26 (1984), pp. 189–206, https://doi.org/10.1090/conm/026/737400.
[9] S. Kaczmarz, Angenaeherte auﬂoesung von systemen linearer gleichung, Bull. Internat. Acad.

Polon. Sci. Lettres A, (1937), pp. 335–357.

[10] D. M. Kane and J. Nelson, Sparser Johnson–Lindenstrauss transforms, J. Association for

Computing Machinery, 61 (2014), pp. 1–23.

[11] D. Leventhal and A. S. Lewis, Randomized methods for linear constraints: convergence rates
and conditioning, Mathematics of Operations Research, 35 (2010), pp. 641–654.
[12] C. C. Paige, Bidiagonalization of matrices and solution of linear equations, SIAM J. Numer.

Anal., 11 (1974), pp. 197–209.

[13] C. C. Paige and M. A. Saunders, LSQR: An algorithm for sparse linear equations and sparse
least squares, ACM Trans. Math. Softw., 8 (1982), pp. 43–71, https://doi.org/10.1145/
355984.355989, https://doi.org/10.1145/355984.355989.

[14] C. C. Paige and M. A. Saunders, CRAIG: Sparse equations. http://stanford.edu/group/

SOL/software/craig/, 2014–2022.

[15] Z. Qu, P. Richt´arik, M. Tak´ac, and O. Fercoq, SDNA: Stochastic dual Newton ascent
for empirical risk minimization, in International Conference on Machine Learning, 2016,
pp. 1823–1832.

[16] P. Richt´arik and M. Tak´aˇc, Stochastic reformulations of linear systems: Algorithms and
convergence theory, SIAM J. Matrix Anal. Appl., 41 (2020), pp. 487–524, https://doi.org/
10.1137/18M1179249, https://doi.org/10.1137/18M1179249, https://arxiv.org/abs/https:
//doi.org/10.1137/18M1179249.

[17] T. Strohmer and R. Vershynin, A randomized Kaczmarz algorithm with exponential conver-

gence, J. Fourier Anal. Appl., 15 (2009), https://doi.org/10.1007/s00041-008-9030-4.

LINEAR PROJECTION SOLVERS

19

Fig. 1: Comparison with randomized projection methods over a time interval of 1
second. The error represents the 2-norm of residuals. Four implementations of [6]
are included for reference.

00.20.40.60.81time(s)10-1010-5100105krkk2a6aGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-1010-5100105krkk2a7aGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-1010-5100105krkk2a8aGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-1010-5100105krkk2a9aGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-51001051010krkk2aloi-scaleGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-51001051010krkk2covtype-libsvm-binary-scaleGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-1010-5100105krkk2proteinGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ00.20.40.60.81time(s)10-1010-51001051010krkk2SUSYGausspdCDpdBlockCD-pdBlockGaussPLSSWPLSSPLSSKZ20

J. J. BRUST AND M. A. SAUNDERS

Fig. 2: Comparison of PLSS with random normal projected solvers when the size of
the sketch increases: r = 10, 20, 40.

020406080100k02004006008001000krkk2ProjectionSolversPLSSRand.Proj.(r=10)Rand.Proj.(r=20)Rand.Proj.(r=40)