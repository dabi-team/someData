TO APPEAR IN IEEE COMPUTER ARCHITECTURE LETTERS

1

Characterizing and Understanding
HGNNs on GPUs

Mingyu Yan, Mo Zou, Xiaocheng Yang, Wenming Li,
Xiaochun Ye, Dongrui Fan, Senior Member, IEEE, and Yuan Xie, Fellow, IEEE

2
2
0
2

g
u
A
9

]

R
A
.
s
c
[

1
v
8
5
7
4
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Heterogeneous graph neural networks (HGNNs) deliver powerful capacity in heterogeneous graph representation learning.
The execution of HGNNs is usually accelerated by GPUs. Therefore, characterizing and understanding the execution pattern of
HGNNs on GPUs is important for both software and hardware optimizations. Unfortunately, there is no detailed characterization effort
of HGNN workloads on GPUs. In this paper, we characterize HGNN workloads at inference phase and explore the execution of HGNNs
on GPU, to disclose the execution semantic and execution pattern of HGNNs. Given the characterization and exploration, we propose
several useful guidelines for both software and hardware optimizations for the efﬁcient execution of HGNNs on GPUs.

Index Terms—Heterogeneous Graph Neural Networks, GNNs, Characterization, Execution Semantic, Execution Pattern.

(cid:70)

1 INTRODUCTION

I N recent years, heterogeneous graph neural networks

(HGNNs) have attracted much attention in graph rep-
resentation learning, as they deliver powerful capacity to
capture the rich structural and semantic information from
the heterogeneous graph (HG). Unlike graph neural net-
works (GNNs) which learn from the homogeneous graph
constituted by one type of node and edge, HGNNs learn
from the HG consisted of multi-types of nodes and edges.
HGNNs are accelerated by GPUs to achieve a considerable
performance. Therefore, disclosing the execution pattern
and execution semantic of HGNNs on GPUs is important
for both software and hardware optimizations for HGNNs.
The complex structure and semantic of HGs offer a more
realistic application scenario for HGNNs, but also impose
great challenges on the optimization of HGNNs. Compared
to GNNs, HGNNs aggregate not only the structural infor-
mation but also semantic information due to the multi-types
of nodes and edges in HGs. To this end, most of preva-
lent HGNNs usually contain four major execution stages:
Subgraph Build, Feature Projection, Neighbor Aggregation, and
Semantic Aggregation, making the execution patterns differ
from traditional workloads. Therefore, observations and
conclusions in previous characterization efforts [8], [9], [10]
for GNNs cannot be directly inferred in HGNNs.

To disclose the execution pattern and execution semantic
of HGNNs, we characterize the inference phase of three
prevalent HGNNs with three well-known HGs on GPU. The
key observations and insights are summarized below.
• Overall Analysis: 1) Neighbor Aggregation stage dominates
most time of HGNNs; 2) Most time is consumed by the

• M. Yan, M. Zou, X. Yang, W. Li, X. Ye, and D. Fan are with the
Institute of Computing Technology, Chinese Academy of Sciences, Beijing,
China. M. Zou is also with University of Chinese Academy of Sciences,
Beijing, China. E-mail: yanmingyu, zoumo,
liwenming, yexiaochun,
fandr@ict.ac.cn. Y. Xie is with University of California, Santa Barbara,
California, USA. Email: yuanxie@ece.ucsb.edu.

• This work was supported by CAS Project for Young Scientists in Basic
Research (Grant No. YSBR-029), and CAS Project for Youth Innovation
Promotion Association. Corresponding author is Mo Zhou.

kernel with reduction-tree-based computational graph.
• Detail Analysis for Each Stage: 1) Feature Projection stage
is dominated by the execution of dense-dense matrix mul-
tiplication, primarily facing compute bound; 2) Neighbor
Aggregation stage is dominated by the execution of graph-
topology-based and element-wise operations, primarily
facing memory bound and exhibiting irregular memory
access pattern; 3) Semantic Aggregation stage is dominated
by the execution of dense-dense matrix multiplication,
element-wise operation, and data rearrangement oper-
ation, primarily facing memory bound ﬁrst and then
compute bound; 4) The data rearrangement in Semantic
Aggregation stage is expensive.

• Comparison: 1) Except for increasing with the average
number of neighbors as in Aggregation stage in GNNs, the
execution time of Neighbor Aggregation stage in HGNNs
increases further as the number of metapaths increases; 2)
Inter-subgraph parallelism exists in Neighbor Aggregation
stage in HGNNs; 3) A barrier exists between Neighbor
Aggregation and Semantic Aggregation stages of HGNNs.
• Exploration: 1) The sparsity of subgraph decreases as the
length of metapath increases; 2) The total execution time
increases as the number of metapaths increases.

2 BACKGROUND

HGs. HGs consist of multiple types of nodes and/or
multiple types of edges. Each two nodes in a HG can be
connected via different semantic paths, which are called
metapaths. A metapath is deﬁned as a path in the form of
rl−→ t1+1 (abbr. as t1t2 · · · tl+1) with node types
t1
t1, · · · , tl+1 and edge types r1, · · · , rl. Given a node v and a
metapath P in a HG, the metapath-based neighbors of v are
deﬁned as the set of nodes which connect with v via P .

r1−→ t2 · · ·

Fig. 1(a) illustrates an example of a HG (IMDB dataset).
IMDB dataset consists of three types of nodes (Director
(D), Movie (M), and Actor (A)) and two types of edges
corresponding to two relations (Direct relation between
movie and director, Act relation between actor and movie).
Considering a metapath DMD in Fig. 1(b), Bob-Inception-

 
 
 
 
 
 
TO APPEAR IN IEEE COMPUTER ARCHITECTURE LETTERS

2

Fig. 1. Illustrations of (a) HG, (b) metapath, (c) metapath instance, and (d) HGNN.

Tom is a metapath instance and Director Bob is a metapath-
based neighbor of Director Tom, indicating the co-director
relationship in movie Inception, as shown in Fig. 1(c).

HGNNs. To capture structural and semantic informa-
tion, the executions of HGNNs can be generally categorized
into four primary stages as shown in Fig. 1(d): 1 Subgraph
Build splits a HG into multiple subgraphs using relation
walk or metapath walk. 2 Feature Projection (FP) projects
feature vectors of different types of nodes into one latent
vector space. A type-speciﬁc linear transformation for each
type of nodes is performed for the projection. 3 Neighbor
Aggregation (NA) aggregates feature vectors of neighbor
nodes for each node within each subgraph to capture the
structural information. 4 Semantic Aggregation (SA) aggre-
gates semantic information revealed by all metapaths, i.e.,
aggregates the results of Neighbor Aggregation stage across
different subgraphs for each node, with the consideration
of the importance of different metapaths. In this work, we
focus on metapath-based HGNNs and summarize three
prevalent HGNNs brieﬂy in Table 1.

Differences between HGNNs and GNNs. Traditional
GNNs usually consist of two stages: Aggregation stage ag-
gregates the feature vectors from neighbor nodes; Combina-
tion stage updates feature vectors of each node using the
aggregating results. The major differences between HGNNs
and GNNs are as follows:
• Heterogeneous Input versus Homogeneous Input. There exists
only one type of node with the same dimension feature
vectors in the input homogeneous graph of GNNs. But
the multi-types of nodes in HGs have different attributes
which need an extra Feature Projection stage in HGNNs to
project raw feature vectors with various dimensions into
ones with the same dimension in a latent vector space.
• Multiple Semantics versus Single Semantic. There exists
only one type of relation between each pair of nodes
in homogeneous graphs, which implies that only one
type of semantic information can be captured in GNNs.
On the contrary, each two nodes can be connected via
different metapaths in HGs, which reveals that multiple
semantic information can be captured in HGNNs. An
extra Semantic Aggregation stage is used to capture these
semantic information in HGNNs.

• Two-stage Aggregation versus One-stage Aggregation. Un-
like GNNs which only aggregate once for the neighbor
aggregation, HGNNs need both neighbor and semantic
aggregations, while the former aggregates neighbor in-
formation from intra-metapaths and the latter aggregates
semantic information from inter-metapaths.

3 EVALUATION SETUP

Benchmark. Table 1 and 2 provide details of the bench-
mark HGNN models and HG datasets used in our exper-
iments. The HAN, RGCN, and MAGNN are implemented

TABLE 1
Primary operations of four stages across three HGNN models.

1

2

3

4

Relation Walk Linear Transformation Mean

R-GCN [5]
HAN [7] Metapath Walk Linear Transformation GAT Attention Sum
MAGNN [2] Metapath Walk Linear Transformation GAT Attention Sum

Sum

TABLE 2
Information of HG datasets and Reddit dataset.

Dataset

Node

Feature Dimension

Relation

IMDB
(IM)

ACM
(AC)

DBLP
(DB)

movie (M): 4278
director (D): 2081
actor (A): 5257

author (A): 5912
paper (P): 3025
subject (S): 57

author (A): 4057
paper (P): 14328
Term (T): 7723
Venue (V): 20

Reddit (RD)

232965

M: 3066
D: 2081
A: 5257

A: 1902
P: 1902
S: 1902

A: 334
P: 14328
T: 7723
V: 20

602

A-M: 12828
D-M: 4278
M-A: 12828
M-D: 4278

P-A: 9936
P-S: 3025
A-P: 9936
P-S: 3025

P-A: 19645
P-T: 85810
P-V: 14328

114615892

based on the state-of-the-art framework DGL 0.7.2 [6]. See
their source codes in OpenHGNN 0.2.0 1 and MAGNN 2.

Proﬁling Platform. All the workloads are proﬁled on a
NVIDIA GPU T4 using NVIDIA Nsight System and Nsight
Compute command lines.

4 OBSERVATION AND ANALYSIS
4.1 Overview of Proﬁle

Execution Time Breakdown. Fig. 2 shows the execution
time breakdown of inference phase. We omit Subgraph Build
stage since it is executed in CPU before inference phase.

Neighbor Aggregation stage dominates most execution time
of HGNNs. Fig. 2 shows that Feature Projection, Neighbor
Aggregation, and Semantic Aggregation stages take 19%, 74%,
and 7% execution time averaging across different models
and datasets, respectively. Neighbor Aggregation stage takes
most time. This is because for each subgraph, neighboring
feature vectors of each node in the corresponding subgraph
all need to be aggregated, which is time-consuming.
Execution time breakdown on different

types of
CUDA kernels. To further disclose the overall execution of
HGNNs, Fig. 3 shows four major kernel types that occupy
most execution time on the three stages across three models
and three datasets. These kernel types include dense-dense
matrix multiplication (DeMM) kernel (DM-Type), topology-
based matrix operation kernel (TB-Type), element-wise com-
pute kernel (EW-Type), and data rearrangement kernel (DR-
Type). The DM-Type kernels perform DeMM such as the
sgemm, generally exhibiting compute bound due to their reg-
ular execution pattern and high compute-to-memory access
ratio. The TB-Type kernels perform the compute operations
based on the irregular topology of graph such as the SpMM-
Csr and SDDMMCoo (SDDMM, sampled dense-dense matrix

1. https://github.com/BUPT-GAMMA/OpenHGNN
2. https://github.com/cynricfu/MAGNN

(a) HG(b) MetapathDMDDMABobTom(c) MetapathInstanceAliBobInceptionInception(d) IllustrationofHGNNs3NeighborAggregation4SemanticAggregationDirectorActorDirectMovieFeatureVectorActSubgraph2Aggregation1SubgraphBuild2FeatureProjectionSubgraph1Subgraph2Subgraph1TO APPEAR IN IEEE COMPUTER ARCHITECTURE LETTERS

3

Fig. 2. Execution time breakdown.

Fig. 4. Kernels in single-precision ﬂoating point operation Rooﬂine.

TABLE 3
Proﬁling results of major kernels on HAN model with DB dataset.

Kernel
Name

Kernel
Type

Time
(%)

Peak Perf.
(%)

DRAM BW
Utilization

Shared Memory
BW Utilization

L2 Hit
Rate

2 Feature Projection

sgemm DM 97.4% 95.9%

33.6%

24.3%

82.7%

Fig. 3. Execution time breakdown on different types of CUDA kernels.

3 Neighbor Aggregation

multiplication), generally exhibiting memory bound derived
from the irregular execution pattern caused by the irregular
neighbor connection pattern in graph. The EW-Type ker-
nels perform element-wise compute operations on a set of
vectors or a matrix such as the unrolled elementwise kernel
(uEleWise), vectorized elementwise kernel (vEleWise), and re-
duce kernel (Reduce), generally exhibiting memory bound
due to low compute-to-communication ratio. The DR-Type
kernels perform data rearrangement on a matrix such as the
CatArrayBatchedCopy (Concat), generally exhibiting memory
bound due to a large amount of data movement.

Most execution time is consumed by the kernel with reduction-
tree-based computational graph. This is because each result-
ing element of the result is computed with the reduction
tree-based computational graph in all DM-Type, EW-Type,
and TB-Type kernels. Taking HAN model as an example,
the sgemm kernel occupies most execution time of Feature
Projection and Semantic Aggregation stages. For each node,
this kernel reduces the dimension of feature vector in prior
stage and aggregates multiple resulting feature vectors from
Neighbor Aggregation stage into a single feature vector in
later stage. In addition, the SpMMCsr kernel occupies most
execution time of Neighbor Aggregation stage, which aggre-
gates multiple feature vectors into a single vector for each
node. Furthermore, the uEleWise and vEleWise, Reduce ker-
nels reduce multiple feature vectors or matrix into a single
vector. The computational graphs of all these kernels are
represented as a reduction tree. Therefore, most execution
time is consumed by the kernel with reduction-tree-based
computational graph in the execution of HGNNs.

4.2 Analysis of Feature Projection Stage

Feature Projection stage is dominated by the execution of
dense-dense matrix multiplication, primarily facing compute
bound. Fig. 3 shows that the DM-Type kernel (i.e., sgemm)
performing DeMM consumes most execution time of Feature
Projection stage on different HGNNs across datasets. The
sgemm kernel exhibits high performance and high degree of
data locality. For example, the sgemm kernel called in HAN
model on DB dataset costs over 97.4% execution time of
Feature Projection stage, as shown in Table 3. Due to the
intensive computation of projection, this kernel achieves
95.9% Peak Performance. Due to the high reuse ratio of
projection matrix, this kernel reaches 82.7% L2 Cache Hit

SpMMCsr TB 85.9% 3.9%
6.5%
SDDMM TB

8.4%

74.3%
44.0%

0%
0%

31.4%
67.6%

4 Semantic Aggregation

sgemm DM 47.8% 84.2%
0.9%
uEleWise EW 20%
3.1%
EW 11%
Reduce
0%
DR 17.5%
Concat

42.4%
82.4%
88.3%
81.6%

21.4%
0%
0%
0%

83.3%
50.0%
25.2%
50.0%

‘Bandwidth’ and ‘Performance’ are respectively abbreviated to BW and Perf..
The unit of arithmetic intensity is FLOP/Byte. ‘Peak Performance (%)’ represents
the percentage of achieved performance to peak performance (FLOPS).

Rate, 24.3% Shared Memory Bandwidth Utilization, and 33.6%
DRAM Bandwidth Utilization. As a result, Arithmetic Intensity
of this kernel is 26.8 FLOP/Byte and larger than the one
(9.37 FLOP/Byte) in the ridge of Rooﬂine (see Fig. 4), which
reveals that Feature Projection stage faces compute bound.

4.3 Analysis of Neighbor Aggregation Stage

Neighbor Aggregation stage is dominated by the execution
of graph-topology-based and element-wise operations, primarily
facing memory bound and exhibiting irregular memory access
pattern. Fig. 3 shows that the TB-Type and EW-Type kernels
occupy most execution time of Neighbor Aggregation stage on
different HGNNs across datasets. Taking HAN model on DB
dataset as an example, the SpMMCsr kernel consumes over
85.9% execution time of Neighbor Aggregation stage, which
aggregates neighboring feature vectors into a single vector
for each node. Table 3 shows that this kernel achieves high
DRAM Bandwidth Utilization (74.3%) with low L2 Cache Hit
Rate (31.4%). This is mainly because its memory accesses
to neighboring feature vectors heavily rely on the irregular
neighbor connection pattern of graph, exhibiting irregular
memory access pattern. In addition, Fig. 4 shows that this
kernel exhibits low Arithmetic Intensity (0.49 FLOP/Byte)
and Percentage of Peak Performance (3.9%). In summary,
Neighbor Aggregation stage is mainly bounded by memory.

4.4 Analysis of Semantic Aggregation Stage

Semantic Aggregation stage is dominated by the execution of
dense-dense matrix multiplication, element-wise operation, and
data rearrangement operation, primarily facing memory bound
ﬁrst and then compute bound. In this stage, the DM-Type
kernel sgemm ﬁrst calculates attention weights for each
resulting feature vectors of each subgraph from Neighbor
Aggregation stage, and then the EW-type kernel uEleWise
and Reduce aggregate these feature vectors into single one
for each node with attention weights. The sgemm is still

020406080100IMACDBIMACDBIMDBRGCNHANMAGNNAVGExecutionTime(%)Feature ProjectionNeighbor AggregationSemantic Aggregation020406080100IMACDBIMACDBIMACDBIMACDBIMACDBIMACDBIMDBIMDBIMDBFPNASAFPNASAFPNASARGCNHANMAGNNExecutionTime(%)DM-TypeTB-TypeEW-TypeDR-TypeFloatingPointOperationRooflinePerformance(0.1TFLOP/s)ArithmeticIntensity(FLOP/Byte)0.010.010.11030110100(9.37,30)Ridge(26.8,29)sgemm(FP)SpMMCsr(NA)SDDMM(NA)sgemm(SA)uEleWise(SA)Reduce(SA)(29,25.2)10.1(0.49,1.2)(1.4,2.0)(0.1,0.27)(0.34,0.9)Concat(SA)(0,0)TO APPEAR IN IEEE COMPUTER ARCHITECTURE LETTERS

4

Fig. 5. Comparisons: (a) Neighbor aggregation time increases as
dropout rate of edge decreases (i.e., average #neighbor increases)
and (b) increases as #metapath increases in HGNNs; (c) Timeline of
Neighbor and Semantic Aggregation stages on HAN and DB dataset.

compute-bound. On the contrary, as shown in Table 3, the
uEleWise and Reduce kernel achieve high DRAM Bandwidth
Utilization (82.4% and 88.3%) with low L2 Hit Rate (50%
and 25.2%), on HAN model with DB dataset. In addition,
Fig. 4 shows that these two kernels exhibit low Arithmetic
Intensity (0.1 and 0.34 FLOP/Byte) and Percentage of Peak
Performance (0.9% and 3.1% ), exhibiting memory bound.
Note that RGCN, the early-stage HGNN model, directly
performs Reduce kernel, using sum operation to aggregate
the resulting feature vectors without attention weights, so
that its Semantic Aggregation stage only exhibits memory
bound. But attention-based aggregation is prevalent in re-
cent, because it helps achieving better accuracy [7].

The data rearrangement in Semantic Aggregation stage is
expensive. A high overhead is caused by data rearrange-
ment which aims to perform the computations of attention
weights and aggregation in a manner of batch. For example,
as shown in Table 3, the Concat kernel, concatenating a set of
vectors into a matrix, costs 17.5% execution time of Semantic
Aggregation stage on HAN with DB dataset, exhibiting high
DRAM Bandwidth Utilization (81.6%).

4.5 Comparisons between HGNNs with GNNs

We conduct several experiments on HAN model and a
prevalent GNN (i.e., GCN [4]) with RD dataset to experi-
mentally compare HGNNs with GNNs in details.

Except for increasing with the average number of neighbors
as in Aggregation stage in GNNs, the execution time of Neighbor
Aggregation stage in HGNNs increases further as the number
of metapaths increases, as shown in Fig. 5(a) and (b). This is
because one more metapath introduces one more subgraph,
which requires extra time for neighbor aggregation.

A new type of parallelism, inter-subgraph parallelism, exists
in Neighbor Aggregation stage, as shown in Fig.5(c), except for
the inter-vertex, intra-vertex, and inter-edge parallelism [8] as in
Aggregation stage of GNNs. This parallelism is derived from
the independent neighbor aggregation for each subgraph.

A barrier exists between Neighbor Aggregation and Semantic
Aggregation stages of HGNNs as shown in Fig.5(c), however,
one-stage aggregation of GNNs is without it. This is because
most HGNNs use all results of Neighbor Aggregation stage
across different subgraphs to calculate the attention weights
of different semantic for the following semantic aggregation.

4.6 Exploring The Execution of HGNN Model

The sparsity of adjacency matrix for each subgraph decreases
as the length of metapath increases, as shown in Fig. 6(a). This is

Fig. 6. Exploration: (a) Sparsity of subgraph decreases as metapath
length increases; (b) Execution time increases as #metapath increases.

because more number of metapath-based neighbors in the
same subgraph can be found for each node in Subgraph Build
stage as the length of metapath increases.

The total execution time increases as the number of metapaths
increases, as shown in Fig. 6(b). This is because as the number
of metapaths increases, more subgraphs are built, costing
extra time to perform neighbor and semantic aggregation.

5 ARCHITECTURAL GUIDELINES

From software perspective, an execution-bound-aware
kernel mixing technique can be used to overlap the exe-
cution of memory-bound and compute-bound kernels to
simultaneously utilize all available memory and compute
resources, like Graphite [3] proposed for GNN accelera-
tion on CPU. In addition, a subgraph-level kernel fusion
technique can be used to fuse the execution of feature
projection and neighbor aggregation for each subgraph to
endow more opportunities for high-degree parallelism and
high utilization of various resources, like fusedGCN [1]
proposed for GNN acceleration on GPU. From hardware
perspective, a correlation model can be built to quantify the
relation between sparsity and the length of metapath, help-
ing generate accurate conﬁguration parameters for sparsity-
aware optimizations and improve their effects. In addition,
a ﬂexible dataﬂow scheduling equipped with a reduction-
tree-based compute unit can be used to exploit the paral-
lelism and reduce data movements among the reduction-
tree-based computational graph for the efﬁcient inference.

6 CONCLUSIONS

In this work, we characterize and explore an emerging
application HGNNs on GPU. We believe our observations
and conclusions will help related researchers understand the
execution pattern and execution semantic of HGNNs.

REFERENCES

[1] Zhaodong Chen et al. fuseGNN: accelerating graph convolutional
neural network training on GPGPU. In ICCAD. IEEE, 2020.
[2] Xinyu Fu et al. MAGNN: Metapath aggregated graph neural
network for heterogeneous graph embedding. In WWW, 2020.
[3] Zhangxiaowen Gong et al. Graphite: optimizing graph neural
networks on CPUs through cooperative software-hardware tech-
niques. In ISCA, pages 916–931, 2022.

[4] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation

with graph convolutional networks. arXiv preprint, 2016.

[5] Michael Schlichtkrull et al. Modeling relational data with graph
convolutional networks. In ESWC, pages 593–607. Springer, 2018.
[6] Minjie Wang et al. Deep Graph Library: A graph-centric, highly-
performant package for graph neural networks. arXiv preprint,
2019.

[7] Xiao Wang et al. Heterogeneous graph attention network. In The

World Wide Web Conference, pages 2022–2032, 2019.

[8] Mingyu Yan et al. Characterizing and understanding GCNs on

GPU. IEEE Computer Architecture Letters, 19(1):22–25, 2020.

[9] Mingyu Yan et al. HyGCN: A GCN accelerator with hybrid

architecture. In HPCA, pages 15–29, 2020.

(c)TimeNeighborAggregation3SemanticAggregationSubgraph2BarrierSubgraph100.04s0.13s0.16s135790.90.80.70.60.50.4Relative Timeto0.9 DropoutRateGCN-RDHAN-IMHAN-ACHAN-DB(a)DropoutRateofEdge020406080100120123456Relative Timeto One MetapathIMACDB(b)NumberofMetapath00.20.40.60.811.2357911SparsityIMACDB(a)LengthofMetapath0102030405060123456RelativeTimetoOneMetapathIMACDB(b)NumberofMetapathTO APPEAR IN IEEE COMPUTER ARCHITECTURE LETTERS

5

[10] Zhihui Zhang et al. Architectural implications of graph neural
networks. IEEE Computer architecture letters, 19(1):59–62, 2020.

