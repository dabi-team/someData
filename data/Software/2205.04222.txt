2
2
0
2

y
a
M
9

]

V
C
.
s
c
[

1
v
2
2
2
4
0
.
5
0
2
2
:
v
i
X
r
a

Alternative Data Augmentation for Industrial
Monitoring using Adversarial Learning

Silvan Mertes1, Andreas Margraf2, Steffen Geinitz2, and Elisabeth Andr´e1

1 University of Augsburg, Universit¨atsstraße 1, 86159 Augsburg, Germany
{silvan.mertes,elisabeth.andre}@informatik.uni-augsburg.de
2 Fraunhofer IGCV, Am Technologiezentrum 2, 86159 Augsburg, Germany
{andreas.margraf, steffen.geinitz}@igcv.fraunhofer.de

Abstract. Visual inspection software has become a key factor in the manufactur-
ing industry for quality control and process monitoring. Semantic segmentation
models have gained importance since they allow for more precise examination.
These models, however, require large image datasets in order to achieve a fair
accuracy level. In some cases, training data is sparse or lacks of sufﬁcient annota-
tion, a fact that especially applies to highly specialized production environments.
Data augmentation represents a common strategy to extend the dataset. Still, it
only varies the image within a narrow range. In this article, a novel strategy is
proposed to augment small image datasets. The approach is applied to surface
monitoring of carbon ﬁbers, a speciﬁc industry use case. We apply two differ-
ent methods to create binary labels: a problem-tailored trigonometric function
and a WGAN model. Afterwards, the labels are translated into color images us-
ing pix2pix and used to train a U-Net. The results suggest that the trigonometric
function is superior to the WGAN model. However, a precise examination of the
resulting images indicate that WGAN and image-to-image translation achieve
good segmentation results and only deviate to a small degree from traditional
data augmentation. In summary, this study examines an industry application of
data synthesization using generative adversarial networks and explores its poten-
tial for monitoring systems of production environments.

Keywords: Image-to-Image Translation, Carbon Fiber, Data Augmentation, Com-
puter Vision, Industrial Monitoring, Adversarial Learning.

1

Introduction

1.1 Motivation

Visual inspection respresents a wide spread methodology in industrial quality control
which is usually employed in mass production to ensure quality standards. With recent
progress in deep learning research, the focus of computer vision shifted from image
processing ﬁlters to neural network architecture selection and design. The increasing
level of automation and digitalisation in the manufacturing industry has drawn atten-
tion to sensing technology and camera sensors in particular. The ﬁeld of online process
monitoring (OPM) primarily deals with imaging technology to detect changes, faults or

 
 
 
 
 
 
2

Mertes et al.

potential anomalies in continuous production environments. Of course, the actual bene-
ﬁt of measurement technology depends on its level of automation. Therefore, intelligent
image processing is a key feature of monitoring systems.

In recent years, machine learning algorithms have progressively overtaken ﬁlter
based image processing, a fact that has been discussed in relevant publications (Cav-
igelli et al., 2017; McCann et al., 2017). In the context of computer vision, convo-
lutional neural networks (CNNs) have proven to be superior because they generalize
better. Given large training and sample data, they are ﬂexible across domains and there-
fore can be applied to very different kinds of applications (Simonyan and Zisserman,
2014; He et al., 2016).

1.2 Pushing the Limits of Image Segmentation

Highly specialized industries are often confronted with incomplete or insufﬁcient data.
With increasing effort spent on data collection and preparation - tasks that require time
and skilled personel - deep learning models become inefﬁcient, expensive and therefore
unattractive. ML solutions only serve the cause if they add to productivity. This arti-
cle discusses concepts that augment scarce datasets and allow semantic segmentation
models to achieve a high accuracy when trained on this data.

Research work has been conducted in the ﬁeld of data preparation, cleaning and
augmentation using e. g. algorithms for interpolation, smoothing, simple transformation
or ﬁltering (Shorten and Khoshgoftaar, 2019). Although these methods help to create
the precondition for successful model development, they quickly appear constrained in
their variation space. This article takes a closer look to a branch of research explor-
ing the representation of artiﬁcially generated data based on real world blueprints, an
approach also denoted synthetic data generation. In this article we consider semantic
segmentation on carbon ﬁber textiles with unique surface structures and heterogeneous
anomalies. Earlier publications have shown that image processing based on conven-
tional ﬁlters such as edge, contour, threshold or Fourier transformation do not cover
anomalies with the desired accuracy. In this respect, we offer a CNN-based method
using two differnt models for adverarial learning to augment the data. Thus, our DA
methods are based on triginometric functions, Wasserstein Generative Adversarial Net-
works (WGANs) and pix2pix image-to-image translation. As an overall goal, we pursue
this approach to improve the reliability of defect detection and reduce the costs of model
training by automating a considerable portion of the data preparation tasks.

One of the main advantages of CNNs is their ability to classify images, i.e. to pre-
dict which category an image, object within the image or single pixel belong to. This
dependency is usually determined on the basis of the ”euclidian distance”, a widely
used similarity score. The latest generation of CNNs requires large amounts of samples
and data to achieve reasonable results. Manual annotation cannot be fully automated
to this date, but still constitutes a time consuming task and - since it is performed by
people - is prone to error over longer periods of time. Regular DA has been a popu-
lar and obviously simple tool to increase the number of training samples. DA methods
include rotation, scaling, blurring and similiar transformations. These functions adjust
image and label pairs carefully and to a limited extent but do not exploit the full po-

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

3

tential that is offered by Generative Adversarial Networks (GANs) and image-to-image
translation.

GANs are designed to output ‘new’ images which appear to be as realistic as pos-
sible, so that they are indistinguishable from real-world photographs. Image-to-image
translation as performed by pix2pix improves the ability of the original GANs to trans-
fer images from one domain to another. However, input data for GAN training should
be carefully collected or created with the expected application domain in mind. In gen-
eral, there are two possible approaches to provide the input. One very obvious approach
would be to gather data which already contain the desired image-to-image translation
information. This could be accomplished by e. g. acquiring images from different an-
gles using the same camera or by moving the target around. This has been examined for
the fashion industry in recent publications (Liu et al., 2019; Cui et al., 2018).

In the manufacturing industry such intensive interfering with the process is undesir-
able because it may cause time delays or unscheduled interruptions. Therefore, getting
access to the target from more than one angle may not be possible. As an alternative ap-
proach, one can use simluated data or approximate image-to-image translation by using
a speciﬁcally designed algorithm. Of course, the second suggestion can only be applied
to smaller sets of data in order to keep the workﬂow efﬁcient.

In this article, image-to-image translation is used to translate randomly generated
binary labels to images. The approach allows for artiﬁcially creating label and image
pairs that are actually new and serve as training samples for semantic segmentation
models. For the generation of new labels, we propose two distinct concepts: the ﬁrst
approach is based on a function precisely tailored to the application and was already
introduced by Mertes et al. (Mertes. et al., 2020b). The second one is presented the
ﬁrst time in this paper and uses a WGAN model trained on the original binary labels in
order to allow the generation of synthetic labels. In both cases, binary label images are
generated and used for further processing, i.e. for generating image/label pairs by using
an image-to-image translation system. Designing problem-speciﬁc functions requires
good domain understanding and further effort to model an algorithm and tune it to
adjust to the given problem. The resulting function is transparent and human-readible
which allows for better debugging and testing. However, the proposed WGAN-based
approach requires less manual effort and automates a considerable part of the overall
process.

All in all, we present a novel approach, to augment image data for semenatic seg-
mentation networks by applying image-to-image translation with both, a domain-speciﬁc
mathematical model and an approach entirely based on generative models. We test both
approaches based on images containing carbon ﬁber surface defects and discuss the
results.

1.3 Structure

The structure of this article is divided into four main sections: at ﬁrst, we provide an
overview on related work and existing technology in section 2. The subsequent section
present the two approaches (section 3) followed by the experimental setup (section 4).
We then discuss relevant results and critically reﬂect them in respect of related concepts

4

Mertes et al.

in section 5. The ﬁnal section covers concluding statements based on our ﬁndings and
provides an outlook on future work as can be seen in section 6.

2 Related Work

This section lists and discusses previously published papers in related research areas
which include Machine Learning (ML), Artiﬁcial Neural Networks (ANN), Computer
Vision (CV), GANs, OPM and Organic Computing (OC) as well as publications that
contributed to the research presented in this article. The identiﬁcation of anomalies on
carbon ﬁbers, e.g. the misalignment on textile surfaces, has been discussed in various
publications (Geinitz et al., 2016a; Geinitz et al., 2016b). In the same context, Margraf
et al. examined the self-adaptation of image processing ﬁlters using organic computing
paradigms (Margraf et al., 2017). Methodologies for automated algorithm selection and
ﬁlter pipelines have been discussed by Stein et al. (Stein et al., 2018). A partly self-
adaptive algorithm for data analysis based on carbon ﬁber monitoring was introduced
by Margraf et al. (Margraf. et al., 2020).

The domain of artiﬁcal neural networks took large steps forward when AlexNet
(Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2015) and VGGNet (Simonyan
and Zisserman, 2014) were introduced for the classiﬁcation of large image sets. Sev-
eral publications address industrial monitoring applications: Masci et al. used CNN for
classiﬁcation of steel defects, and Soukup et al. used CNN for photometric stereoscopic
images (Masci et al., 2012; Soukup and Huber-M¨ork, 2014). A region proposal network
for real-time object detection was presented by Ren et al., while Ferguson et al. used
CNNs and transfer learning to detect X-ray image defects (Ren et al., 2015; Ferguson
et al., 2018). Furthermore, the use of CNNs for industrial surface anomaly inspection
was explored by Staar et al. (Staar et al., 2019). The ﬁrst ones to introduce pixel-based
segmentation was Long et al. (Long et al., 2015), while Schlegl et al. published a work
in which GANs for marker detection were used for unsupervised anomaly detection
(Schlegl et al., 2017). A survey exploring GANs for anomaly detection was presented
by Di Mattia et al. (Di Mattia et al., 2019). Methods for color translation between differ-
ent photographic contexts were discussed in various related publications (Zhang et al.,
2016; Xie and Tu, 2015). Translation of completely different image domains was ﬁrst
achieved by Isola et al., who presented the pix2pix architecture (Isola et al., 2017). The
pix2pix architecture was the ﬁrst to allow the projection of various image domains such
as edge objects or label images to colored photographs. A huge problem when dealing
with pixel-based segmentation tasks is a big gap between a small foreground and a rel-
atively large background. This is especially reﬂected in the case of carbon ﬁber suface
images. Ronneberger et al. conﬁrmed, that multi-channel feature maps prove to be more
useful in these tasks (Ronneberger et al., 2015). It should be noted that this architecture
makes heavy use of data enhancement, while taking only little input data. However, its
capabilities are limited when dealing with very small training sets. Multiple approaches
to use GANs for the speciﬁc purpose of data augmentation were presented in different
works. Frid-Adar et al. and Mariani et al. were able to apply GANs to classiﬁcation
tasks, while Choi et al. presented an approach that makes use of image-to-image trans-
lation networks for semantic segmentation tasks by transforming labeled data to related

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

5

image domains so that the original label still ﬁts to the artiﬁcially created image (Frid-
Adar et al., 2018; Mariani et al., 2018; Choi et al., 2019). Huang et al. utilized image-to-
image networks for shared segmentation tasks by using multiple image domains for the
training process (Huang et al., 2018). The authors of this paper could not identify any
publication dedicated to the use of GANs for generating entirely new image and label
pairs for enhanced training datasets in the context of semantic image segmentation.

An early application of generative models was presented by Pathak et al. for im-
age inpainting to reconstruct a region within an image that adapts well to its sur-
roundings (Pathak et al., 2016). Shrivastava et al. proposed an approach denoted Simu-
lated+Unsupervised (S+U) learning which allows to improve the output of a simulator
based on a GAN network (Shrivastava et al., 2017). Furthermore, StarGAN was intro-
duced by Choi et al. to improve the quality of translated images by providing a more
scalable method for image-to-image translation to an arbitrary domain (Choi et al.,
2018).

The WGAN architecture constitutes a variant of generative models. It was intro-
duced to increase learning stability and is less prone to mode collapse. Its structure
ressembles the general GAN approach, except for its discriminator that learns on the
basis of the Wasserstein distance by approximating a 1-Lipschitz function (Arjovsky
et al., 2017). Later, Arjosvky et al. made further adjustments to the weight clipping
which improved the WGAN behaviour to better handle hyperparameter tuning (Gul-
rajani et al., 2017). In addition, a comparative study increased the understanding of
GAN behaviour regarding training stability and model saturation (Arjovsky and Bot-
tou, 2017). The authors of this article are aware that metaheuristics for hyperparameter
optimization, e. g. swarm intelligence (Strumberger et al., 2019) exist. However, this
ﬁeld of research is not subject to this research article.

3 Approach

The following sections explain the concepts that are introduced in this paper. The main
idea of our approach is that we are enhancing datasets with augmented data by artiﬁ-
cially modeling label images and after that convert them into real image data. By doing
so, we get image/annotation pairs that are needed for the training of neural networks
for semantic segmentation tasks. To create new label data, we propose two different
methods. The ﬁrst method, that we already introduced in (Mertes. et al., 2020b), is an
algorithm speciﬁcally designed for our particular application at hand, i.e. the defect de-
tection in carbon ﬁber structures. It is based on a randomized label generator that uses a
stochastically parametrized function to build segmentation masks. The second method
is a more generic one. It uses a WGAN that is trained on raw segmentation masks. After
training, the WGAN is capable of generating new label images that appear similar to
the original labels. By applying this concept, we get rid of the engineering overhead
that is necessary when using the ﬁrst method. While the randomized label generator
that is used by the ﬁrst method has to be deﬁned and optimized speciﬁcally for every
new segmentation task, the WGAN should be able to learn the label structure of new
tasks by itself.
The labels that are produced by either of the two methods are fed into a pix2pix network

6

Mertes et al.

that was trained on an image-to-image conversion task, i.e. the network was trained to
convert label mask images to real images that ﬁt to the respective labels, thus resulting
in image/label pairs that can be used to enlarge training datasets.
All in all, our approach can be seen as a three-folded system: ﬁrst, we train a pix2pix
network on an image-to-image translation task, so that it learns to perform a transla-
tion from labels of defect images to their corresponding image data. Second, one of
the aforementioned methods is used to generate synthetic label data. At last, the syn-
thetic label data is fed into the trained pix2pix network, resulting in new training pairs
for further machine learning tasks. The following sections explain these steps in more
detail.

Fig. 1: Training of a pix2pix network to perform image-to-image translation between
labels and defect images (Step 1) (Mertes. et al., 2020b)

3.1 Label-to-Image Model

In order to convert label masks to corresponding images, we trained a pix2pix model.
For the training, a dataset of existing real defect images and manually labeled annotation
masks was used. The basic scheme of the training process is depicted in Fig. 1. We used
the pix2pix network architecture that was introduced by (Isola et al., 2017). We adapted
the size of the input layer to ﬁt the dimensions of our dataset. Other than that, we did
not make any modiﬁcations to the originally proposed architecture.

3.2 Synthetic Label Generation

The idea of our approach is to feed new synthetic label data into the pix2pix network in
order to obtain new pairs of defect images and label masks. As mentioned above, two
different methods were applied for the stage of synthesizing new label data.

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

7

Mathematical Modelling of Defects The ﬁrst method is based on the observation that
in many application scenarios label masks have a common structure as proposed by
Mertes et al. in their related publication (Mertes. et al., 2020b). It is illustrated in Fig.
2. The idea behind this ﬁrst approach to generate fake label masks is to ﬁnd a math-
ematical description of those structures for a speciﬁc case. In the application scenario
that serves as an example for evaluation in this article - the mentioned defect detection
on carbon ﬁber structures - label masks usually appear as mostly straight or curved
lines. Those structures can be seen as a combination of multiple graphs with different
rotations and varying thickness of the plots. Thus, the mathematical description of a sin-
gle defect label could be approximated through a trigonometric function. By adding a
stochastic factor to such a function, we can plot different graphs that can be considered
as new, artiﬁcial label masks. For our speciﬁc task, we conducted several experiments
that showed that the following function f (x) can be used to cover a huge part of carbon
ﬁber defect structures. We denote f (x) as already presented in (Mertes. et al., 2020b):

f (x) = a1 · sin(a2 · x) + a3 · sin(x) + a4 · cos(a5 · x) + a6 · x + a7 · x2

where the parameters an are chosen randomly within certain deﬁned intervals. For our
speciﬁc experiments, we found appropriate intervals by visual expection of carbon ﬁber
defect images. By using those intervals, that are listed in Tab. 1, we ensure to cover a
wide range of different defect structures. To that end, a sine function was tuned with a
rather big amplitude to model the global structure of the label, that is typically shaped
in curves. For the structure on a more microperspective level, we used another sine
with a much smaller amplitude interval. Aperiodic curvings were modeled by the use
of polynomic functions. As described in (Mertes. et al., 2020b), we randomly set the
variables and plotted the resulting graph for every fake label for x ∈ [0, w] where w
represents the width of the sample images. After creating those plots they were rotated
randomly. At last, we took a random number of those graphs, randomized the thickness
of the resulting lines, and overlapped those graphs to create images of labels with a
realistic ﬁber-like appearance.

The authors are aware of the fact that this method is very speciﬁc to the given task
at hand. A lot of engineering time and effort has to be spent to ﬁnd sufﬁcient mathemat-
ical models for the respective task. However, similar approaches for defect-modeling
have been successfully applied to similiar problems in previous work (Haselmann and
Gruber, 2017).

Fig. 2: Heuristic to generate fake labels using the label generator (Step 2) (Mertes. et al.,
2020b)

8

Mertes et al.

Table 1: Parameters for the fake label generator (Mertes. et al., 2020b)
Parameter Lower bound Upper bound

a1
a2
a3
a4
a5
a6
a7

15
0.02
1
-0.5
-0.5
-0.5
0.005

30
0.03
50
0.5
0.5
0.5
0.0095

Fig. 3: Architecture of both the generator and critic of the used WGAN network.

Generative Modelling of Defects The fact that the approach of mathematically mod-
eling label mask structures is coupled to a lot of engineering overhead led to the inves-
tigation of a more generic approach, which is described in this section.
The basic idea of this method is to use the capability of original GANs to transform
random noise vectors into data that looks similar to data of a given training set. While
the pix2pix network that was described earlier in this work performs a transformation
between different image domains, more traditional GANs are designed to generate com-
pletely new data. This property was used for data augmentation tasks in the past, not
only in the image domain (Bowles et al., 2018), but also for audio classiﬁcation tasks
(Mertes. et al., 2020a). However, instead of generating new image data of carbon ﬁber
defect images, our approach uses such a rather traditional GAN to create new label seg-
mentation masks, which then can be transformed to defect image data by feeding it to
our pix2pix network, as will be described in the next section. The beneﬁt of generating
label masks instead of real image data is that, on the one hand, the pixelwise structure
of those label masks appears less complex due to its binary nature. On the other hand,
the cascade of generating label masks in one step and transforming those label masks
further to actual image data in another step makes those image/label pairs available for
direct processing: they ﬁnally serve as a training set for segmentation networks.
To generate those artiﬁcial label mask images, we make use of a convolutional GAN
that operates on the Wasserstein-Loss as introduced by Arjovsky et al. (Arjovsky et al.,
2017). The network architecture of both the generator and the critic, as the discrimina-

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

9

Fig. 4: The generation and preparation of training data for U-Net using a trained pix2pix
model and the fake label generator to create fake training pairs. (Step 3) (Mertes. et al.,
2020b)

tor is called in the context of WGANs, is illustrated in Fig.3.
As a training dataset, we use real label masks. More speciﬁcally, the same label masks
can be used which were already part of the training pairs of the pix2pix network. Details
regarding the training conﬁguration can be found in Section 4.5.

3.3 Finalizing the Training Data

In the last step, the generated label data is used to create new corresponding image
data. Thus, the label data that was produced by either mathematical modelling or by
the WGAN is used as input to the trained pix2pix model. The resulting data pairs of
label/image data can be used to train further networks for the actual segmentation task.
The whole system is shown in Fig. 4. For our experiments, we chose a U-Net architec-
ture to perform this segmentation task. It has to be emphasized that the selection of this
speciﬁc network was done for the purpose of evaluating and comparing our augmenta-
tion approaches, and that the authors don’t claim that architecture to be the best choice
for the respective task. However, U-Net could achieve promising results in related ﬁelds
like biomedical image segmentation (Ronneberger et al., 2015).

4 Experiments and Discussion

4.1 Dataset Speciﬁcs

Our system was evaluated in the context of an industrial application scenario. More
precisely, the domain of carbon ﬁber defect monitoring was chosen for testing and eval-
uating of the proposed approaches. In images of ﬁber structures without recognizable
defects, the single ﬁbers are aligned in parallel and form a carpet of straight lines. Dur-
ing the production process, mechanical stress caused by spools in the transportation
system can lead to damage of the ﬁber material, which usually can be recognized as
misaligned ﬁbers. The shape of those cracked ﬁbers, as well as their position and size

10

Mertes et al.

Fig. 5: Examples of real image data pairs labelled by experts. The misaligned ﬁbers are
visible on top of the ﬁber carpet (Mertes. et al., 2020b).

(a)

(b)

(c)

(d)

Fig. 6: Samples of synthetic labels (top row) and corresponding pix2pix outputs (bottom
row) imitating misaligned ﬁbers (Mertes. et al., 2020b).

(a)

(b)

(c)

(d)

Fig. 7: Samples of synthetic labels generated by WGAN (top row) and corresponding
outputs (bottom row) using the same pix2pix model as for Fig. 6.

vary heavily. Thus, there is no template for single defects. Given the different images
of defective ﬁber material, a huge variety of defect structures can be observed. In this
speciﬁc use case, we aim for the identiﬁcation of defects on a carbon ﬁber carpet. To
achieve this goal, a U-Net architecture is trained to perform a binary segmentation of
pixels that contain defects. Fig. 5 shows examples of defect images with corresponding
binary labels. The environment and the design of the monitoring system that was used

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

11

to acquire the image data for our experiments has been described in earlier publications
(Geinitz et al., 2016b; Geinitz et al., 2016a; Margraf et al., 2017).

4.2 Experimental Setup

For a meaningful evaluation, we ran several experiments to compare the two vari-
ants of our approach with conventional data augmentation methods. Thus, parts of our
datasets that are described below were augmented with traditional data augmentation
techniques. The following simple image transformations were applied to those artiﬁ-
cially extended datasets:

– Randomised crop of squares of different size (RandomSizedCrop)
– Horizontal and vertical ﬂip
– Rotation (for 180 degrees)
– Elastic transformation
– Grid distortion

We arranged the image data in six different sets and performed multiple trainings of a
U-Net architecture. Then, we used the resulting models to make predictions on a test
set. To ensure comparability, the same test set was used for every training set. Every
training pair for the U-Net architecture consists of a real or fake defect image and a
real or fake binary label image. The ﬁrst four datasets are the exact same as used in the
publication by Mertes et al. (Mertes. et al., 2020b). We added two additional sets using
the WGAN model for generating fake labels as listed below:

Dataset 1 contains 300 pairs of real defect data and corresponding binary label images.

Thus, only original data without DA was taken.

Dataset 2 contains the same 300 pairs of defect data and corresponding labels as dataset
1. Additionally, an online form of data augmentation was applied as described
above. For each image some of those aforementioned transformation operations
were performed with a predeﬁned probability.

Dataset 3 contains 3000 pairs of defect data and corresponding labels. 2700 of the
3000 data pairs were generated by applying the pix2pix based data augmentation
approach on dataset 1. For the creation of synthetic label, our mathematical model
was applied. Furthermore, the 300 original data pairs from dataset 1 were taken.
Dataset 4 contains the same 3000 pairs of defect data and corresponding labels as
dataset 3. Additionally, the same conventional stochastic data augmentation as for
dataset 2 was applied, i.e. each image was transformed with a predeﬁned probabil-
ity during training. Thus, dataset 4 combines common data augmentation with our
approach.

Dataset 5 contains 3000 pairs of defect data and corresponding labels. In this dataset,
2700 of the 3000 pairs were generated by the pix2pix network. This time, however,
the input data for image-to-image-translation were not generated from a trigonometic
function, but by training a WGAN model on image pairs of real sample data. The re-
sulting model was then used to create new binary labels. The remaining 300 image
pairs were taken from dataset 1 as performed in the previous datasets.

12

Mertes et al.

Dataset 6 contains the same 3000 pairs of defect data and corresponding labels as
dataset 5. Hereby, though, we altered the training of the U-Net by adding online,
stochastic data augmentation as for dataset 4 and dataset 2. In this dataset, 2700
of the 3000 pairs were generated by the pix2xpix network. This was performed to
examine how regular data augmentation will change the result on top of WGAN
based label generation and image-to-image translation.

Each of the datasets was used to train a separate U-Net model for semantic segmen-
tation. For testing and evaluation, a single, distinct dataset was used, containing real
defect data and annotations that are provided by domain experts.

4.3 Pix2Pix Conﬁguration

The conﬁguration of the pix2pix model is given in Tab. 2. We stopped the training after
3200 epochs, as we could not observe any further improvement of the generated images
by that time. Fig. 6 shows a selection of pairs of labels and images generated through
application of the pix2pix model, where the labels where created by mathematical mod-
eling. Fig. 7 shows label/image pairs where the labels were generated by our WGAN
approach.

Table 2: Pix2Pix Conﬁguration (Mertes. et al., 2020b)

Parameter

Learning rate
Batch Size
Epochs

Value

0.0005
1
3200

Loss Function Mean Squared/Absolute Error

4.4 U-Net Conﬁguration

The U-Net architecture was trained individually for every dataset. As described above,
dataset 2, 4 and 6 were augmented with traditional DA, i.e. conventional image trans-
forms.

All of those conventional data augmentation methods are based on the library pub-
lished by (Rizki et al., 2002). We used the same methods that were already described
in (Mertes. et al., 2020b). A stochastic component was added to the image transforma-
tions, i.e. all operations were performed with a given probability.
The randomized crop was given the probability p = 0.25 and a window size interval of
[400, 512] pixels. Furthermore, the probabilities for ﬂipping, rotation, elastic transform
and grid distortion were set to p = 0.5. For these three methods, only one operation, i.e.
either elastic transform or grid distortion was allowed (OneOf ). The degree of rotation
was set to exactly 180, as the structures of carbon ﬁbres used in our experiments al-
ways have a vertical alignment. Elastic Transform was performed with the parameters

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

13

α = 10, σ = 10, al pha a f f ine = 512 · 0.05 and border mode = 4. Grid Distortion was
given the parameters num steps = 2 and distort limit = 0.4. In the respective datasets,
the operations were applied online using the given parameters on every original input
image. The U-Net model itself was slightly adapted from (Yakubovskiy, 2019) to ﬁt
the dataset. The default size of the training images was 512x512, yet the default U-Net
setting only accepts images of size 28x28. A ResNet-18 model is used as encoder by
the U-Net. The architecture was adapted to ﬁt the input size before applying the model.
The training conﬁguration of the U-Net is shown in Tab. 3.

Table 3: U-Net Conﬁguration (Mertes. et al., 2020b)

Parameter

Learning rate
Batch Size
Epochs

Value

0.0001
10
200

Loss Function Binary Cross Entropy / Dice Loss

4.5 WGAN Conﬁguration

The conﬁguration for the WGAN training is shown in Table 4. As the WGAN produces
non-binary image data as output, we applied a binarization stage to the ﬁnal output
images in order to get binary label masks.

Table 4: WGAN Conﬁguration

Parameter

Value

Learning rate 0.00005

Batch Size
Epochs

64
100,000

5 Evaluation

The metrics accuracy, MCC and Fβ − Score are less dependable for an objective eval-
uation of the classiﬁcation model in our speciﬁc task. The proportion between back-
ground pixels (i.e. the non-defect pixels) and foreground pixels (i.e. the defect pixels)
per image is thoroughly unbalanced, as the defects mostly consist of single ﬁbers and
therefore take much less space in the images. While accuracy returns the proportion of
true results among all data points examined, MCC and Fβ − Score aim to balance out
true and false positives and negatives of the binary classiﬁcation result. In contrast, the
Jaccard index or Intersection over Union (IoU) is used to measure the similarity of two

14

Mertes et al.

Table 5: U-Net results from test runs on the datasets 1 through 6 for batch size 5, based
on (Mertes. et al., 2020b).

PPV

TPR

IoU

ACC

MCC

F1

F2

Dataset 1 0.539169 0.586753 0.390778 0.985035 0.55487 0.561956 0.576576
Dataset 2 0.772803 0.718101 0.592925 0.991935 0.740872 0.744448 0.728413
Dataset 3 0.745926 0.721067 0.578888 0.991419 0.729034 0.733286 0.725905
Dataset 4 0.756767 0.705387 0.57502 0.991471 0.72631 0.730175 0.715098
Dataset 5 0.602418 0.585941 0.422541 0.990628 0.594065 0.543877 0.589383
Dataset 6 0.281570 0.433361 0.205801 0.980426 0.339847 0.341352 0.354881

sets, i. e. the similarity of the ground truth and the prediction. This property makes the
IoU the most appropriate for the task at hand. Thus, we focus on the IoU metric for
our experiments in order to allow an objective and problem related evaluation method-
ology. However, all relevant statisticals scores are reported in Tab. 5 for the sake of
completeness.

5.1 Discussion of Results

For all of the six datasets the training was aborted after 200 epochs since it was clearly
observable that the models had converged. As Fig. 9 suggests, both training accuracy
and loss converge from epoch 100 onwards for the ﬁrst four datasets. Training on dataset
1 was stopped at a loss rate of ∼ 0.7, while for both datasets 3 and 4 the training ended
at a loss rate of ∼ 0.4. For dataset 2, model training reached an IoU of ∼ 0.6 and ∼ 0.5
for validation when the process was aborted. At the same time, the training loss ended
at ∼ 0.4 and reached a value of ∼ 0.2 for validation. Furthermore, training on dataset
1 reached an IoU score of ∼ 0.7 while dataset 3 and 4 achieved an IoU value of ∼ 0.8
after 200 epochs.

Also, Fig. 10 shows that training on dataset 5 and 6 reached an IoU score of ∼ 0.7
and ∼ 0.5 for validation after 200 epochs. In both cases, the loss converged at ∼ 0.2 for
training and ∼ 0.4 for validation. All training results for the four datasets are shown in
Tab. 5.

The trained models were alltogether evaluated on the test dataset. The model trained
on dataset 1 reached an accuracy of 0.985 and IoU of 0.391 on the test set, while
the model trained with dataset 2 reached an accuracy of 0.992 and an IoU of 0.593.
Furthermore, the IoU for the model based on dataset 3 reached an IoU of 0.579 and
an accuracy of 0.991, while training with dataset 4 achieved a value of 0.575 for the
IoU and 0.991 for the accuracy. In addition, dataset 5 resulted in an IoU of 0.423 and
accuracy of 0.991, while training on dataset 6 let the IoU drop to 0.206 and accuracy
decrease to 0.980. All metrics for the test runs were acquired from prediction on 25
randomly selected sample images as presented in Tab. 5.

Fig. 8 shows a sample selection of defect images taken from the test set with red
overlays representing the ROIs predicted by the U-Net model. It should be noted that
training without any DA leads to more false positives which reminds of noise in the
overlays as can be seen in Fig. 8b and 8c.

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

15

(a)

(b)

(c)

(d)

Fig. 8: Real carbon ﬁber defects from the test set with red overlay from U-Net segmen-
tation for dataset 1 (top row), dataset 3 (second row), dataset 5 (third row) and the
ground truth (bottom row) (Mertes. et al., 2020b)

For dataset 1, the loss rate drops heavily for 50 epochs before converging at a value
of around 0.5 for the test set and just over 0.0 for the training set, as can be seen in Fig.
9. The IoU value increases for 50 epochs before slowing down and converging to an
IoU of around 0.85 for the training set and around 0.4 for the test set after 125 epochs.
For dataset 2 the loss rate drops considerably during the ﬁrst 5 epochs, then de-
creases slowly but steady until it converges around 0.2 after 125 epochs during training.
The loss on the test set shows a similar behaviour, except it converges around a value of
0.4. Again, the IoU value increases clearly within less than 5 epochs during the training
process, then only slightly rises before converging around 0.6 after epoch 125. For the
test set, the IoU value increases constantly between epoch 0 and 75, then converges
around 0.4 as illustrated in the second row of Fig. 9.

As can be seen in Fig. 10, the loss rate of dataset 5 drops heavily for 25 epochs,
then quickly converges around 0.2 for the training set. The loss curve behaves the same
way, although it oscillates more. The documented loss on dataset 6 shows the same
characteristics, however its loss curve oscillates even more on the test set.

16

Mertes et al.

Fig. 9: IoU Score and Loss during U-Net training for dataset 1 through 4 from top to
bottom row (Mertes. et al., 2020b).

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

17

Fig. 10: IoU Score and Loss during U-Net training for dataset 5 and 6 from top to bottom
row.

5.2 Assessment of Results and Insights

As can be seen, the U-Net model trained on dataset 3 signiﬁcantly outperformed the
model trained on dataset 1. This shows that our approach of mathematical defect mod-
eling in combination with a pix2pix architecture could substantially improve the quality
and diversity of the raw training set. When comparing the results of dataset 2 and dataset
3, it becomes apparent that the proposed approach is slightly worse, however not sig-
niﬁcantly diverts from conventional data augmentation techniques as applied on dataset
2. The difference comprises within less then 0.02 for the IoU.

The combination of generating synthetic data using that ﬁrst approach with subse-
quent conventional data augmentation as for dataset 4 did not lead to any improvement.
The model trained on dataset 4 outperforms dataset 1, but leads to a slighty lower IoU,
accuracy and MCC than dataset 2 and 3. However, the degradation ranges within less
then 0.02 for the IoU and is therefore not signiﬁcant under the given circumstances.
The last two rows of Tab. 5 show the results from the experiments that apply image-to-
image translation on labels generated with the WGAN model. As can be seen, dataset
5 slightly outperforms dataset 1 since it results in a higher IoU, accuracy and MCC.
However, it clearly proves inferior to datasets 2 through 4 which means regular data
augmentation as well as the problem-tailored label generator clearly performs better
in the given scenario. For the sake of completeness and transparency, we also did an
experiment in which we extended the data of dataset 5 with conventional data augmen-
tation, resulting in dataset 6. The outcome, though, indicates a clear deterioration all

18

Mertes et al.

(a) Ground Truth

(b) No DA

(c) With DA

Fig. 11: Comparison of noticable side effects in the classiﬁcation of ﬁlaments based on
the WGAN based approach with and without data augmentation and the corresponding
ground truth

along the line. Every metric appears lower then for any other dataset (i. e. dataset 1 -
5 perform better). Precision, IoU, MCC, F1 − Score and F2 − Score drop heavily - to
values of 0.28, 0.21, 0.34, 0.34 and 0.35. The only metrics, that show a comparably
‘light’ degradation are accuracy (to 0.98) and recall (to 0.43) which is due to the fact
that they weigh out background and foreground pixels. Since only ∼ 1% of the pixels
are associated with the region of interest, the accuracy and recall cannot provide high
signiﬁcance.

The experiment based on dataset 6 obviously suggests that online data augmentation
does not add any value to WGAN generated labels and image-to-image translation. The
numbers suggest that it even worsens the performance of the semantic segmentation
model by a magnitude.

As the results from Tab. 5 and the samples depicted in Fig. 8 suggest, the pairs of
synthetic images and labels of carbon ﬁber defects were successfully used to replace
traditional data augmentation for semantic segmentation network training. With an IoU
of 0.579, the ﬁrst variant of our approach, i.e. the mathematical modeling of defects,
performs comparably to U-Net training with regular data augmentation. Since the abso-
lute difference between dataset 2 and 3 results in a value of 0.01, it appears negligible.
The results show that synthesized training data helps to improve the detection quality of
a U-Net segmentation model to a great extent. Moreover, the augmented dataset could
be created based on few samples of only 300 images with an image size of 512x512
pixels in which the ROI on average only covers 1 % of an image frame.

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

19

5.3 Visual Comparison and Critical Reﬂection

Since size of a ROI only take up a small portion of each image, even little variance in
the detection results heavily affects the evaluation. This is the case if the classiﬁcation
of single pixels differs slightly from the ground truth. Adding a small margin to each
ﬁlament e. g. largely affects metrics such as IoU or MCC and blurs the results of a
proper analysis in this particular case. As can be seen in Fig. 8, the visual comparison of
test results suggests that WGAN+Pix2Pix based semantic segmentation tends to return
better segmentation than training a U-Net model without any data augmentation.

Actually, the ground truth sometimes reveals some room for misinterpretation by
showing annotations that results in contradicatory objective functions: as can be seen in
Fig. 11, the ground truth differs substantially from all segmentation results; to the user
this may not be obvious and visually the results range at a very similiar level, but little
variations at this level cause the metrics to ﬂuctuate heavily. Although this is intended
to some extent, it must be considered closely upon interpretation. All in all, we consider
this approach as promising and stress that future work should test it on other similiar or
very different applications.

Moreover, the authors suggest to keep in mind that this study has only applied the
workﬂow based on WGAN, Pix2Pix and trigonometric functions on images of a carbon
ﬁber surface with its very special types of defects and anomalies. At this point, it cannot
be ruled out that the workﬂow functions differently under other circumstances that are
considerably distinct to the present use case. In fact, the authors greatly encourage the
research community to test this approach on other datasets and settings.

We consider both our pix2pix based image generation approaches a more realis-
tic and application-oriented form of DA. The experiments were conducted with and
without traditional DA in order to evaluate the effectiveness and expandability of our
approach. Excessive use of traditional DA might superimpose the ‘real’ data within the
training set due to its low level form of manipulation, reproduction and reuse which
raises the risk of overﬁtting during model training. GAN based data generation is also
less prone to repetitive patterns since it tries to project the variation found in the original
data to the synthetic data.

5.4 Summary

In summary, the approach as proposed in this article reveals a potential alternative to
traditional, simple data augmentation. The generated data forms representation of im-
ages that are signiﬁcantly different from the sample data but still resemble the training
distribution. Furthermore, the suggested algorithms support training deep learning mod-
els for semantic segmentation with small sample datasets. This also applies if only few
annotations are available.

In this article, two competing concepts were suggested and evaluated on indus-
try data. While the ﬁrst approach based on a trigonometric function represents a very
problem-speciﬁc, hand-crafted solution, the second concept entirely uses adversarial
and deep learning for model training. In both cases, the sample data needs to be anno-
tated. Still, the number of sample data needed to train the models appears to be compa-
rably low.

20

Mertes et al.

On the one hand, the approach based on WGAN, Pix2Pix and U-Net combines three
different deep learning architectures and only requires parameter tuning to work for the
given dataset. This workﬂow already offers a high level of automation since it reduces
the effort for designing a defect detection system to providing a small sample set of an-
notated data. Of course, only a well-selected and sufﬁciently annotated test set allows
for serious model validation and testing. On the other hand, the trigonometric func-
tion is transparent, human-readable and stable. Its output can be visualized and tested
against tolerance criteria. It also allows to be tuned by setting limiting parameters, such
as window size, orientation or line thickness. Also, auditing and requirements testing
can be easily performed. However, the mathematical function lacks of ﬂexiblity in terms
of domain transfer. In order to design a mathematical model speciﬁc to the problem, do-
main knowledge needs to be collected and translated into abstract dependencies. Thus,
it qualiﬁes for applications with a high need for transparency and stability, e. g. security
in critical environments.

However the authors stress that they cannot evaluate the whole extent of GAN based
DA, but encourage fellow reseachers to explore the application of this approach to other
ﬁelds and use cases. We expect beneﬁts especially in the ﬁeld of deep learning, indus-
trial monitoring and neuroevolution.

6 Conclusion and Outlook

In this article, we presented image-to-image translation as a means for data augmenta-
tion in the context of defect detection on textiles and carbon ﬁber in particular. There-
fore, we discussed related GAN approaches and designed two variations of a novel
concept for generating synthetic defects based on sparse labeled data using a pix2pix
model.

Within our experiments on six different datasets we showed that a pix2pix based
approach could substantially improve the pixel-based classiﬁcation quality of U-Net
models when using a problem-speciﬁc label generator. In general, the synthetic defects
helped to augment the dataset so that segmentation quality improves on sparse data.
However, the approach did not clearly outperform regular DA techniques. Still, domain-
speciﬁc modelling of defect images allows to achieve similiar quality scores. Although
WGAN proved inferior to competing techniques, it still helped to support semantic
segmentation to a certain degree.

Furthermore, the approach can be used to train neural networks for semantic seg-
mentation on comparably sparse data since GANs manage to generate realistic, yet
artiﬁcal labels from few samples. Fuzzballs and misaligned ﬁbers serve as a model
for industrial camera based surface monitoring in the manufacturing process. The sug-
gested approach has been tested for the given setting but is not limited to textiles. The
example was selected in order to demonstrate how the approach behaves for complex
detection tasks. The authors deem further experimentation necessary to evaluate the
whole potential, i. e. explore and demonstrate the proposed workﬂow in other domains
and for defects that are maybe even less complex. It should be mentioned, that by us-
ing the label generator with mathematical models, training data can always be created
for a speciﬁc use case as for carbon ﬁber images. Conventional DA only applies very

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

21

general image transformations without any reference to speciﬁc requirements in the
application scenario. For that reason, we also suggested WGAN for a higher level of
automation. In conclusion, we could reveal great potential for problem-tailored func-
tion models but could not verify the same for the WGAN based apporach. The authors,
however, suggest to conduct hyperparameter tuning on the GAN models for improve-
ments in future work. The study suggests that semi-supervised training exhibits high
relevance for defect detection in industrial applications. Furthermore, it could also be
demonstrated that traditional DA did not substantially improve the pixel-based classi-
ﬁcation. Under the given circumstances the assumption can be made that GAN based
augmentation already provides a well-balanced and diverse dataset so that conventional
image transformation methods do not add any additional value.

Since the current research activities still focus on the adaptation of network models
for semantic segmentation and object detection in different areas of industrial image
data, we are going to explore the potential for a wide application of GAN training with
very little supervision. For this purpose, future research will mainly address the de-
sign of networks which allow to design models that are trained with few or no data
and still manage to effectively apply industrial monitoring solutions. This will result
in problem-speciﬁc algorithm development in order to decrease efforts spent on defect
simulation and shift it to domain independent methods. We deem the ﬁeld of neuroevo-
lution in combination with transfer and few-shot learning as promising for industrial
applications. Future work will strive for a closer look to hyperparameter optimization
in the context of deep learning. Therefore, we plan to extend our concepts with meth-
ods from the ﬁelds of Evolutionary and Organic Computing to equip our approach with
self-conﬁguring and self-learning properties. The application of evolutionary computa-
tion, i.e. genetic algorithms and co-evolution, constitutes another topic of our research
agenda.

ACKNOWLEDGEMENTS

The authors would like to thank the Administration of Swabia and the Bavarian Min-
istry of Economic Affairs and Media, Energy and Technology for funding and support
to conduct this research as part of the program Competence Expansion of Fraunhofer
IGCV formerly Fraunhofer Project Group for ”Functional Lightweight Design” FIL of
ICT.

Bibliography

Arjovsky, M. and Bottou, L. (2017). Towards principled methods for training generative adver-

sarial networks. arXiv preprint arXiv:1701.04862.

Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein gan.

arXiv preprint

arXiv:1701.07875.

Bowles, C., Chen, L., Guerrero, R., Bentley, P., Gunn, R., Hammers, A., Dickie, D. A.,
Hern´andez, M. V., Wardlaw, J., and Rueckert, D. (2018). Gan augmentation: Augment-
ing training data using generative adversarial networks. arXiv preprint arXiv:1810.10863.
Cavigelli, L., Hager, P., and Benini, L. (2017). CAS-CNN: A deep convolutional neural network
In 2017 International Joint Conference on

for image compression artifact suppression.
Neural Networks (IJCNN), pages 752–759.

Choi, J., Kim, T., and Kim, C. (2019). Self-ensembling with gan-based data augmentation for
In Proceedings of the IEEE International

domain adaptation in semantic segmentation.
Conference on Computer Vision, pages 6830–6840.

Choi, Y., Choi, M., Kim, M., Ha, J.-W., Kim, S., and Choo, J. (2018). Stargan: Uniﬁed generative
adversarial networks for multi-domain image-to-image translation. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 8789–8797.

Cui, Y. R., Liu, Q., Gao, C. Y., and Su, Z. (2018). Fashiongan: Display your fashion design using
conditional generative adversarial nets. In Computer Graphics Forum, volume 37, pages
109–119. Wiley Online Library.

Di Mattia, F., Galeone, P., De Simoni, M., and Ghelﬁ, E. (2019). A survey on gans for anomaly

detection. arXiv preprint arXiv:1906.11632.

Ferguson, M. K., Ronay, A., Lee, Y.-T. T., and Law, K. H. (2018). Detection and segmentation
of manufacturing defects with convolutional neural networks and transfer learning. Smart
and sustainable manufacturing systems, 2.

Frid-Adar, M., Klang, E., Amitai, M., Goldberger, J., and Greenspan, H. (2018). Synthetic data
augmentation using gan for improved liver lesion classiﬁcation. In 2018 IEEE 15th inter-
national symposium on biomedical imaging (ISBI 2018), pages 289–293. IEEE.

Geinitz, S., Margraf, A., Wedel, A., Witthus, S., and Drechsler, K. (2016a). Detection of ﬁlament
misalignment in carbon ﬁber production using a stereovision line scan camera system. In
Proc. of 19th World Conference on Non-Destructive Testing.

Geinitz, S., Wedel, A., and Margraf, A. (2016b). Online detection and categorisation of defects
along carbon ﬁber production using a high resolution, high width line scan vision system. In
Proceedings of the 17th European Conference on Composite Materials ECCM17, Munich.
European Society for Composite Materials.

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved
training of wasserstein gans. In Advances in neural information processing systems, pages
5767–5777.

Haselmann, M. and Gruber, D. (2017). Supervised machine learning based surface inspection
by synthetizing artiﬁcial defects. In 2017 16th IEEE international conference on machine
learning and applications (ICMLA), pages 390–395. IEEE.

He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778.

Huang, S.-W., Lin, C.-T., Chen, S.-P., Wu, Y.-Y., Hsu, P.-H., and Lai, S.-H. (2018). Auggan: Cross
In Proceedings of the European

domain adaptation with gan-based data augmentation.
Conference on Computer Vision (ECCV), pages 718–731.

Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning

23

Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017). Image-to-image translation with condi-
tional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1125–1134.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural information processing systems, pages
1097–1105.

Liu, L., Zhang, H., Ji, Y., and Wu, Q. M. J. (2019). Toward ai fashion design: An attribute-gan

model for clothing match. Neurocomputing, 341.

Long, J., Shelhamer, E., and Darrell, T. (2015). Fully convolutional networks for semantic seg-
mentation. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 3431–3440.

Margraf., A., H¨ahner., J., Braml., P., and Geinitz., S. (2020). Towards self-adaptive defect clas-
siﬁcation in industrial monitoring. In Proceedings of the 9th International Conference on
Data Science, Technology and Applications - Volume 1: DATA,, pages 318–327. INSTICC,
SciTePress.

Margraf, A., Stein, A., Engstler, L., Geinitz, S., and H¨ahner, J. (2017). An evolutionary learning
approach to self-conﬁguring image pipelines in the context of carbon ﬁber fault detection. In
2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA).
IEEE.

Mariani, G., Scheidegger, F., Istrate, R., Bekas, C., and Malossi, C. (2018). Bagan: Data aug-

mentation with balancing gan. arXiv preprint arXiv:1803.09655.

Masci, J., Meier, U., Ciresan, D., Schmidhuber, J., and Fricout, G. (2012). Steel defect classi-
ﬁcation with max-pooling convolutional neural networks. In The 2012 International Joint
Conference on Neural Networks (IJCNN), pages 1–6. IEEE.

McCann, M. T., Jin, K. H., and Unser, M. (2017). Convolutional neural networks for inverse

problems in imaging: A review. IEEE Signal Processing Magazine, 34(6):85–95.

Mertes., S., Baird., A., Schiller., D., Schuller., B., and Andr´e., E. (2020a). An evolutionary-based
generative approach for audio data augmentation. In Proceedings of the 22nd International
Workshop on Multimedia Signal Processing (MMSP). IEEE.

Mertes., S., Margraf., A., Kommer., C., Geinitz., S., and Andr´e., E. (2020b). Data augmentation
for semantic segmentation in the context of carbon ﬁber defect detection using adversarial
learning. In Proceedings of the 1st International Conference on Deep Learning Theory and
Applications - Volume 1: DeLTA,, pages 59–67. INSTICC, SciTePress.

Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A. A. (2016). Context encoders:
Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 2536–2544.

Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster r-cnn: Towards real-time object detection
with region proposal networks.
In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama,
M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28, pages
91–99. Curran Associates, Inc.

Rizki, M. M., Zmuda, M. A., and Tamurino, L. A. (2002). Evolving pattern recognition systems.

In IEEE Transactions on Evolutionary Computation, volume 6, pages 594–609.

Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomed-
In International Conference on Medical image computing and

ical image segmentation.
computer-assisted intervention, pages 234–241. Springer.

Schlegl, T., Seeb¨ock, P., Waldstein, S. M., Schmidt-Erfurth, U., and Langs, G. (2017). Unsuper-
vised anomaly detection with generative adversarial networks to guide marker discovery.
In International conference on information processing in medical imaging, pages 146–157.
Springer.

Shorten, C. and Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep

learning. Journal of Big Data, 6(1):60.

24

Mertes et al.

Shrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J., Wang, W., and Webb, R. (2017). Learning
from simulated and unsupervised images through adversarial training. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 2107–2116.
Simonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556.

Soukup, D. and Huber-M¨ork, R. (2014). Convolutional neural networks for steel surface defect
detection from photometric stereo images. In International Symposium on Visual Comput-
ing, pages 668–677. Springer.

Staar, B., L¨utjen, M., and Freitag, M. (2019). Anomaly detection with convolutional neural

networks for industrial surface inspection. Procedia CIRP, 79:484–489.

Stein, A., Margraf, A., Moroskow, J., Geinitz, S., and Haehner, J. (2018). Toward an Organic
Computing Approach to Automated Design of Processing Pipelines. ARCS Workshop 2018;
31th International Conference on Architecture of Computing Systems. VDE.

Strumberger, I., Tuba, E., Bacanin, N., Jovanovic, R., and Tuba, M. (2019). Convolutional neural
network architecture design by the tree growth algorithm framework. In 2019 International
Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,
and Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 1–9.

Xie, S. and Tu, Z. (2015). Holistically-nested edge detection. In Proceedings of the IEEE inter-

national conference on computer vision, pages 1395–1403.

Yakubovskiy, P. (2019). Segmentation models. https://github.com/qubvel/segmentation_

models.

Zhang, R., Isola, P., and Efros, A. A. (2016). Colorful image colorization. In European conference

on computer vision, pages 649–666. Springer.

