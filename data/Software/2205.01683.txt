2
2
0
2

y
a
M
3

]

V

I
.
s
s
e
e
[

1
v
3
8
6
1
0
.
5
0
2
2
:
v
i
X
r
a

SpineNetV2: Automated Detection, Labelling and
Radiological Grading Of Clinical MR Scans

Rhydian Windsor1, Amir Jamaludin1, Timor Kadir1,2, and Andrew
Zisserman1

1Visual Geometry Group, Department of Engineering Science,
University of Oxford
2Plexalis
{rhydian,amirj}@robots.ox.ac.uk

May 5, 2022

Abstract

This technical paper presents SpineNetV2, an automated tool which:
(i) detects and labels vertebral bodies in clinical spinal magnetic res-
onance (MR) scans across a range of commonly used sequences; and
(ii) performs radiological grading of lumbar intervertebral discs in T2-
weighted scans for a range of common degenerative changes. SpineNetV2
improves over the original SpineNet software in two ways: (1) The
vertebral body detection stage is sigiﬁcantly faster, more accurate
and works across a range of ﬁelds-of-view (as opposed to just lum-
bar scans).
(2) Radiological grading adopts a more powerful archi-
tecture, adding several new grading schemes without loss in perfor-
mance. A demo of the software is available at the project website:
http://zeus.robots.ox.ac.uk/spinenet2/.

Figure 1: An overview of SpineNetV2, shown acting on a T2 lumbar MR
scan. Note that the vertebra detection and labelling pipeline works across
a range of diﬀerent MR sequences (T1, T2, STIR, etc.) and ﬁelds-of-view
(e.g. cervical, thoracic, lumbar and whole spine) as illustrated in Figure 8.

1

Raw ClinicalMR ScanDetect Vertebral Landmarksin each sliceLabel vertebral levelsExtract vertebral units using detectionsPerform disc-level radiological gradingfor 11 spinal diseasesGroup landmarks intovertebrae instances1. Detect VBs2. Label3. Perform Radiological Grading 
 
 
 
 
 
Contents

1 Introduction

2 Overview

2.1 Processing Pipeline . . . . . . . . . . . . . . . . . . . . . . . .

3 Methods Used

3.1.1

3.1 VB Detection by Vector Field Regression . . . . . . . . . . .
Splitting Large Scans Into Patches . . . . . . . . . . .
3.2 Convolutional Labelling of Vertebral Levels . . . . . . . . . .
. . . . . . . . . . . . . . .
3.3 Extracting Intervertebral Volumes
3.4 Radiological Grading of Intervertebral Volumes . . . . . . . .

4 Implementation

4.1 Code Implementation . . . . . . . . . . . . . . . . . . . . . .
4.2 Model Architectures . . . . . . . . . . . . . . . . . . . . . . .
4.3 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 OWS . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
4.3.2 Genodisc
4.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4.1 VB Detection . . . . . . . . . . . . . . . . . . . . . . .
4.4.2 VB Labelling . . . . . . . . . . . . . . . . . . . . . . .
4.4.3 Radiological Grading of Degeneration Changes . . . .

5 Results

5.1 VB Detection and Labelling . . . . . . . . . . . . . . . . . . .
5.2 Radiological Grading . . . . . . . . . . . . . . . . . . . . . . .
5.3 Processing Speed . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Example Qualitative Results
. . . . . . . . . . . . . . . . . .
5.5 Online Demo . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Conclusions

7 Acknowledgements

3

4
5

7
7
8
9
12
12

12
13
13
13
13
13
15
16
17
18

18
18
20
20
21
21

22

22

2

1

Introduction

Back pain is the most common cause of long-term disability; it aﬀects around
80% people in the UK during their lifetime [11]. As people live longer, inci-
dence will only increase. To combat this, we need methods to diagnose and
monitor etiology like degenerative changes that are quick, eﬀective and cheap
to perform. This is the motivation behind SpineNet; to oﬀer a completely
automated set of tools for performing common gradings & measurements in
clinical spinal MR scans.

This report describes the second iteration of the SpineNet software with
several improvements over the initial version1. Namely, the method of de-
tecting and labelling vertebral bodies has been completely overhauled and
is now much faster, more robust, and can run across a range of ﬁelds of
views (cervical, whole spine, lumbar, thoracic etc.) as opposed to just lum-
bar scans. The grading network has also been improved to performs several
new grading types; disc herniation, left and right foraminal stenosis and
multiclass central canal stenosis (as opposed to binary classiﬁcation in the
original version). Grading performance across pre-existing tasks is similar
or slightly improved from SpineNetV1.

Crucially, unlike SpineNetV1 which was implemented using MATLAB,
SpineNetV2 is implemented using open-source python libraries. Therefore,
it can be run on a much wider range of hardware without the need for
potentially expensive software licences. The time-consuming, HOG-based
vertebra detection system of V1 has been replaced with a much faster (on
GPUs and CPUs) and more robust deep-learning based approach. Fur-
thermore, the network used to perform radiological grading has now uses a
more powerful ResNet34 backbone to extract visual features as opposed to
VGG-F. A complete summary of these changes can be seen in Table 1.

This technical report builds on several existing publications on the sub-
ject of vertebrae detection and radiological grading, amongst which are [6,
7, 8, 9, 10, 14].

The report is organised as follows; Section 2 describes the operation and
functionality of SpineNetV2 at a high level. Section 3 dicusses the methods
used for vertebral body (VB) detection (§3.1), labelling (§3.2), extracting
intervertebral volumes (IVVs) from the detected VBs (§3.3), and performing
radiological grading of these detected IVVs (§3.4). Section 4 discusses the
implementation of the system, including the libraries (§4.1) and datasets
(§4.3) used and methods for training the consituent neural networks (§4.4).
Section 5 gives experimental results for the detection and labelling pipeline
as well as the radiological grading pipelines, including comparisons with the
original version of SpineNet. Finally, Section 6 concludes the report, giving
future plans for SpineNet as well as information on how SpineNet software

1SpineNetV1 project page: http://zeus.robots.ox.ac.uk/spinenet/

3

Table 1:
SpineNetV2.

Side-by-side comparison of

the original SpineNetV1 and

SpineNetV1

SpineNetV2

- Vertebra detection & labelling on lumbar
sagittal scans across a range of common clin-
ical MR sequences

- Vertebra detection & labelling on any ﬁeld
of view sagittal scan (e.g. cervical, lumbar,
whole spine)

- Implemented in MATLAB

- Implemented in open-source python libraries
only (e.g. PyTorch, PyDicom)

- Complete radiological grading of
scan in approx. 5 minutes.

lumbar

- Complete radiological grading of lumbar

scan in approx. 5 seconds.

- Grades for (No. Classes): Pﬁrrmann (5),
Disc Narrowing (4), Endplate Defects (2),
Marrow Changes (2), Spondylolisthesis (2)
and Central Canal Stenosis (2).

- Grades for (No. Classes): Pﬁrrmann (5),
Disc Narrowing (4), Endplate Defects (2),
Marrow Changes (2), Spondylolisthesis (2)
and Central Canal Stenosis (4), Foraminal
Stenosis (2) and Disc Herniation (2).

- VGG-F Backbone for Grading Network
Backbone.

- ResNet34 Grading Network Backbone.

can be used.

2 Overview

SpineNetV2 provides an entire pipeline to go from raw DICOM ﬁles (as
output by a conventional MRI machine) to 3-D detections localising the
each vertebral body visible in the scan and labels describing the level of
each detection. In the case of T2 lumbar scans, the pipeline can also output
multiple common radiological gradings for each intervertebral disc visible in
the scan.

Inputs: DICOM ﬁles corresponding to slices of a sagittal MRI spinal scan.
This scan can be one of a range of common clinical sequences (e.g. T1, T2,
STIR, FLAIR, TIRM, Dixon-technique, etc.), have an arbitrary ﬁeld-of-view
(e.g. lumbar, cervical, whole spine, etc.) and be of varying resolution and
slice thickness.

Outputs:

• CSV/JSON ﬁle describing the location of vertebral bodies in the scan

and the corresponding level of these detections (from S1 to C2).

• T2 Lumbar Scans Only: CSV/JSON ﬁle with predictions for a range
of common radiological grading schemes (itemized in Section 3.4) for

4

each intervertebral disk visible in the scan.

2.1 Processing Pipeline

SpineNet consists of multiple processing steps, each of which is described in
detail in the following sections. These are:

1. Splitting each slice of the input DICOM ﬁle into a set of smaller patches

(for large scans only).

2. Detecting each visible vertebral landmark (vertebral corners & cen-

troid) in each patch/slice.

3. Grouping together these landmarks into vertebrae instances.

4. Determining the level of each detected vertebra.

For T2 Lumbar scans the following stages are added:

5. Locating IVVs (inter-vertebral volumes) using the VB detections.

6. Performing radiological grading of the extracted IVV.

This entire process is shown in Figure 2.

5

(a) VB Detection - Detecting 3D bounding boxes around each visible vertebral
body. Larger scans are split into multiple patches, as explained in Section 3.1.1.

(b) VB Labelling - Determining the level of each detected vertebra.

(c) VB Grading - Grading each detected IVV using a range of common radiological
grading schemes. Note this step only applies to T2 Lumbar scans.

Figure 2: The processing pipeline for SpineNetV2. Note that the vertebra
detection and labelling section works across a range of sequences and ﬁelds-
of-view, whereas the radiological grading system only operates on T2 lumbar
scans.

6

Input Sagittal Spinal MRI ScansDetect Vertebral Landmarks visible in each sliceCorners + Associated ArrowsVertebrae CentroidsSlicewise DICOM FilesDetection Quadrilaterals in Each SliceGroup Landmarks into Slicewise Vertebra DetectionsGroup Slicewise Detections across slices to get 3D De-tections163264128128643216183D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV46400FC1256FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOIDFlattenFF83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1FC224SIGMOID83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1FC224SIGMOIDFlattenFF1. 3D Bounding Quadrilaterals From Vertebrae Detection Stage2. Predict Level From Local Appearance Alone3. Construct Input Height-Probability Map From Appearance Predictions4. Reﬁne Input Height-Proba-bility Map Using Predictions From Context Network5. Use Beam Seach To Find Most Probable Series of Levels Detected In Image83D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID25683D-CONV1163D-CONV2323D-CONV3643D-CONV41283D-CONV4FC1256FC224SIGMOID256Vertebrae Detections & Labels From Previous StagesIdentify Volumes Corresponding To Intervertebral DiscsExtract Invertebral Disc VolumesPass Each IVD Volumes To Radiological Grading Network Output Radiological Gradings3 Methods Used

This section decribes methods used for the three main stages of the SpineNet
pipeline (VB detection, VB labelling and IVV grading) from an algorithmic
standpoint. These stages are illustrated in Figure 1. Broadly speaking,
whole spine scans and smaller scans (e.g. lumbar, cervical) are processed
identically, with only minor diﬀerences in how the patch splitting is done in
the VB detection stage. These diﬀerences are described fully in Section 3.1.1,
however we will initially assume a single lumbar scan when describing the
processing pipeline.

3.1 VB Detection by Vector Field Regression

The ﬁrst stage in the SpineNetV2 pipeline is to detect vertebral bodies
(VBs) in the raw scans. This is done using a method called Vector Field
Regression (VFR). Operating in each sagittal slice of the scan, a network
detects gaussian responses over the centroids and corners (vertebral land-
marks) of each visible VB. For each corner detected, a corresponding vector
ﬁeld is also output. At the point of each corner, this corresponding vector
ﬁeld should ‘point’ to the centroid of the vertebra to which it belongs. This
allows landmarks from the same vertebra to be grouped together in a simple
manner which is robust to rotations, ﬂips and variable vertebra size.

At this point, each detection consists of a centroid and four corners
grouped together to make a quadrilateral in an individual sagittal slice.
Slicewise 2D quadrilaterals corresponding to the same vertebra must then
be grouped together across slices forming 3D volumes. This is done are by
measuring the intersection-over-union (IOU) of quadrilaterals in neighbour-
ing slices. If the IOU is large, the quadrilaterals are grouped together into
a single vertebral instance. A diagram of this process going from raw scans
to 3D volumes is shown in Figure 3.

Speciﬁcally, inference proceeds as follows:

1. A raw MR scan of dimension S×H×W is split into S sagittal slices.
These slices are in turn split into square patches of size 25 cm2 with an
overlap of 30% between neighbouring patches on each side. This patch-
splitting allows the network to deal with scans of varying dimensions.

2. The patches are resampled by cubic interpolation to resolution of
224×224. The resamples patches are fed as input to a ResNet50-
encoded U-Net. This results in a 13×224×224 output per patch; 5
detection channels and 8 grouping channels (see Figure 3).

3. By resampling & concatenating these patch-level outputs using the
median output for overlapping regions, an slice-level output tensor is
reconstructed of size 13×H×W .

7

Figure 3: An overview of the VFR (Vector Field Regression) method. A
U-Net has 13 output channels; 5 detection channels which output gaussian
heatmaps over the four corners and centroid of each visible vertebral body
and 8 grouping channels; the x and y components of vector ﬁelds corre-
sponding to each corner.

4. The detection channels are then decoded to ﬁnd the location of de-
tected VB centroids and corners. This is done by thresholding each
channel & ﬁnding all connected component in the resulting binary
map. The exact point-of-detection for each landmark is the point of
maximal response in the detection channel for the corresponding con-
nected component.

5. Now that each VB landmark in the slice has been detected, they must
be grouped into quadrilaterals corresponding to individual VBs. This
is done by measuring the value of the corresponding vector ﬁeld at the
point-of-detection for each corner landmark. Looping through each
detected VB centroid, the corner landmark of each type which points
closest is assigned as the corresponding corner for that centroid, form-
ing a quadrilateral. If no arrow is within a distance from the centroid
of 50% of the arrow length, that centroid is discarded as a spurious
detection.

6. Finally the resulting VB polygons are grouped across neighbouring
slices if they have an IOU over 0.25.
If more than one polygon in
a neighbouring slice overlap, then the one with the greatest IOU is
chosen.

3.1.1 Splitting Large Scans Into Patches

Larger non-square scans (such as whole spine scans) are split into patches
before VFR is applied. This is done by splitting the scan into a grid of over-
lapping squares with edge length 50cm (as determined by the pixel spacing
parameter in the DICOM header) and an overlap of 40% between neigh-

8

NN5×N641281282565122561286432135 Detection Channels(centroid & 4 corners)8 Grouping Channels(4 corner vector ﬁelds,top right corner ﬁeld shown)Thresholded Detections &Corresponding VectorsRaw MR scan with N sagittal slicesDetections Grouped into Vertebrae InstancesU-Net N8×NGroup Detections Across Sagittal SlicesFigure 4: Splitting a large scan into patches before applying VFR.

bouring patches. The output from the detection and grouping channels are
then used to ﬁnd landmarks in each patch. These landmarks are then trans-
formed back into the frame of the original scan. At this point, the algorithm
proceeds as before by grrouping landmarks into slicewise polygons and then
across saggital slices. Figure 4 shows the process of patch-splitting for a
whole spine scan.

3.2 Convolutional Labelling of Vertebral Levels

The VFR method described above allows us to detect vertebral bodies in
a sagittally sliced MR scan. The next step of the pipeline is to determine
which vertebral levels the detections correspond to (e.g. S1, L5, L4 etc). This
is made more challenging by the fact that SpineNetV2 is not constrained
to a single ﬁeld of view. As such, ‘counting up’ methods, i.e. those that
rely on an anchor vertebra being visible (such S1/C2 at the bottom/top of
the vertebral column) are unsuitable. Furthermore, such methods are not
robust to missed detections or variations in the number of vertebra in the
vertebral column (e.g. in cases where a transitional vertebra is present).

There are two pieces of information to consider when labelling a vertebra
- its appearance (e.g. intensity pattern, shape, size etc.) and its context (the
vertebra’s position relative to other detections in the scan). For example,
S1 usually has a very distinctive shape which allows it to be labelled from
appearance alone. On the other hand, L5 looks very similar to the other
lumbar vertebrae, however can be easily identiﬁed from it’s context - it is the
next vertebra up from S1. Our method attempts to use both these sources
of information when labelling a vertebra.

Firstly, a 3D volume around each detected VB is fed as input to an ap-
pearance network. This outputs a 23-element (from S1 to C3) probability
vector predicting the level of the VB from appearance alone. To include
information about the spatial conﬁguration of the detections, this is then
used to contruct a probability-height map, P . At the height of each de-

9

Split Large Scan Into PatchesPerform VFR In Each Patch(Detection Channel Shown Only)Threshold To Get Landmarks andTransfer Into Original Scan Frametection, P has value equal to the output from the appearance network for
that detection. Using this as input, a convolutional context network reﬁnes
the height-probability map, taking into account appearance predictions from
spatially nearby VBs to update the probability vectors for each detection.
The result is a reﬁned height-probability map P (cid:48). This process is shown in
Figure 5. Finally, P (cid:48) needs to be decoded into discrete level predictions.
Na¨ıvely, this could be done by taking the maximum probability level at the
height of each detection in P (cid:48). However, this would allow for nonsensical
outputs, such as the same level for two detections. Ideally, we also want to
build in soft contraints such that successive detections are labelled as suc-
cessive labels. For example, we would expect S1 to be the detection below
L5. However, we also wish to remain robust to missed detection.

To build these constraints into our approach, we take inspiration from
language modelling. Using a beam search, we can ﬁnd the most probable
valid sequence of levels for the detections. Penalties are added to the se-
quences probability score in the case of transitional vertebrae or numerical
variations to reﬂect the unlikeliness, yet possibility, of such events. The
exact parameterisation of this search is speciﬁed in the next paragraph.

Figure 5: The language modelling-inspired approach to vertebra labelling
outlined in this section. We begin by constructing a probability-height map
P based on appearance information alone. Using a context-aware CNN this
map is reﬁned (P (cid:48)). Finally P (cid:48) is decoded into a valid sequence of levels
using a penalised beam search.

The exact speciﬁcation of this labelling stage is as follows:

1. Given VB detections from the previous stage, a 2-D bounding cube
is ﬁt tightly around the union each of detection’s slicewise series of
polygons. By expanding this cube by 100% on each side in the axial
and coronal directions and by 50% on each side in the sagittal direction
a volume around each detection and its nearby anatomical features is
created.

2. All extracted volume are resized by cubic interpolation to 224×224×16
(where 16 is the number of sagittal slices). These are then fed to an

10

NS1L4L2T12T10T8T6T4T2C7C5C3Vertebral LevelHeight in ScanS1L4L2T12T10T8T6T4T2C7C5C3Vertebral LevelHeight in Scan161616162242242242242242242242248163264128640025624Flatten8163264128640025624Flatten8163264128640025624Flatten8163264128640025624FlattenContext NetworkAppearanceNetworkBeam Search w.  Language Modelling1632641281286432161Extract volumes surrounding detections and resize to 224×224×16P’Pappearance network which outputs a 23-element probability vector,
attempting to classify the VB as a level from S1-C3.

3. The output probability vectors are re-calibrated [5] using a softmax
temperature T = 10. These recalibrated vectors are then used to
construct an initial probability-height map, P ∈ RH×23. For a detec-
tion polygon with output appearance probability vector pa ∈ R23 and
spanning from height h1 to h2 in the scan, P is equal to this value
between these heights, i.e. P (h) = pa ∀ h1 ≤ h ≤ h2.

4. P is then given as input to a image-to-image context CNN. This out-
puts P (cid:48) ∈ RH×23, a reﬁned probability-height map which considers the
neighbours of each detection to update its probability vector. This re-
ﬁnement step can be seen in Figure 5.

5. The next step is to decode P (cid:48) into a series of discrete level predictions.
Firstly, the probability vector at the centroid of each detection, pi is
extracted. The joint probability of a sequence of levels can then be
calculated as the product of elements of these vectors. For example,
if detected VB i has a probability of being S1 of a and VB j has
probability of being L5 of b, then the joint probability of the level
sequence is a × b. To reduce search space, we use a beam search to
ﬁnd the most likely sequence, starting from the bottom detection and
only storing the 100 most probable level sequences at each step.

6. To impose constraints on sequences of level predictions, penalties can
be added to the sequence scores. For example, sequences which predict
the same level twice are given a probability of 0. Furthermore, if there
is a missed detection (i.e. L4 directly above S1), the joint probability
score is multiplied by a penalty score to reﬂect the rarity of this event.
To allow for cases where S2 or C2 are detected, double detections of
S1 or C3 are allowed without a score penalty and then relabelled in
post-processing.

At this point, each vertebra in the scan from S1 to C3 should have been
detected and assigned a level label. These detections can be used in a range
of applications. For example in whole spine scans, SpineNet can be used to
measure spinal curvature (for scoliosis measurement) or extract regions of
anatomical interest such as the spinal chord or vertebral bodies for lesion
segmentation (e.g. spinal metastases or ankylosing spondylitis).

One area of particular interest is radiological grading of degenerative
changes. The following section describe how this is done in the context of
the SpineNet.

11

3.3 Extracting Intervertebral Volumes

To perform radiological grading in T2 lumbar scans, ﬁrst volumes surround-
ing each intervertebral disc must be extracted. This is done using the VB
detections from the previous stage. The mid-point between the centroids of
two consecutive VB detections is calculated. This deﬁnes the centre of each
extracted IVV. From this the volume is rotated such that the lower endplate
of the upper vertebra is horizontal. The width of the extracted endplate vol-
ume is then deﬁned as double that of the larger VB detection. The height
of the IVV is then chosen such that the aspect ratio of the extracted patch
is 2:1. This is done across all slices in which the vertebrae is detected.

3.4 Radiological Grading of Intervertebral Volumes

Once the IVVs are extracted, they are resampled to have a resolution of
112 × 224 × 9 (height, width and sagittal slices respectively) These volumes
are then fed to a radiological grading network which outputs scores for the
following radiological gradings:

1. Pﬁrrman Grading (5 classes)

2. Disc Narrowing (4 classes)

3. Central Canal Stenosis (4 classes)

4. Upper & Lower Endplate Defects (Binary)

5. Upper & Lower Marrow Changes (Binary)

6. Left & Right Foraminal Stenosis (Binary)

7. Spondylolisthesis (Binary)

8. Disc Herniation (Binary)

The radiological grading network is a standard multi-class classiﬁcation
3D CNN. We experiment with a range of diﬀerent architectures, described
in Section 5. Each of these grading schemes is described in further detail
in Section 4.3.2. Note that the same model is used for all gradings schemes
and vertebral levels.

4 Implementation

This section describes the speciﬁc design choices made while developing
SpineNetV2, as well as details of the datasets and methods used to train the
constituent neural networks.

12

4.1 Code Implementation

All stages of the SpineNetV2 processing pipeline are implemented entirely
using open-source python libraries. Deep learning functionality is provided
by CUDA-enabled PyTorch v1.7 (or later). Input DICOMs are processed
using pydicom.

The software is designed such that detection and grading can be seper-
ated. During inference, models can run on CPUs or GPUs for faster pro-
cessing (the performance and memory constraints of each conﬁguration are
given in Section 5.3).

4.2 Model Architectures

There are 4 constituent neural networks in the SpineNetV2 pipeline; the
VFR regression network, the appearance network, the context network and
the grading network. The VFR regression network is a ResNet18-encoded
UNet. The appearance network is a simple VGG-F network as outlined in
the original paper [2]. The context network is a simple UNet [12]. Finally,
the radiological grading network is a conventional ResNet32 model with 3D
3×3×3 convolutions in the ﬁrst layer and 3×3 convolutions in all other layers.

4.3 Datasets

Two datasets are used to train SpineNetV2; Oxford Whole Spine(OWS) and
Genodisc. OWS is a dataset of whole spine scans across a range of commonly
used sequences, used only for training the vertebra detection and labelling
sections of the SpineNetV2 pipeline. Genodisc is a dataset of sagittally-sliced
lumbar T2 scans used in training all stages of the pipeline.

4.3.1 OWS

OWS consists of 710 sagittally-sliced whole spine scans across 196 patients
extracted from local orthopaedic centre’s PACS (picture archiving and com-
munication system) and anonymised under appropriate ethical clearance.
These scans are across a range of commonly used clinical sequences (mostly
T1, T2, STIR and TIRM). The distribution of sequences can be seen in Fig-
ure 6c. Each vertebral body from S1 to C2 is annotated as a quadrilateral in
the central slice of each scan by a non-specialist. These are used to generate
the ground truths for training, as discussed in section 4.4.

4.3.2 Genodisc

Genodisc is a dataset of sagittaly-sliced lumbar T1 and T2 scans from 6
diﬀerent international clinical spinal imaging centres. This dataset is used
for training the detection (T1 & T2) and the radiological grading (T2 only)

13

(a) Scans per patient in Genodisc.

(b) Scans per patient in OWS.

(c) MR sequence types in OWS.

Figure 6: Breakdown of scan types in both OWS (196 patients, 719 scans)
and Genodisc (2279 patients, 2819 scans). Both datasets are split 80:20:20%
down the patient line.

14

1234# Scans025050075010001250150017502000# Patients1776474235TrainValTest1234# Scans010203040506070# Patients11632452TrainValTestT1T2TIRMSTIRFLAIRT1GADFSMR Sequence050100150200250300350# Scans312154120113821TrainValTeststages of the pipeline. Each T2 scan is annotated by an expert radiologist.
The following degenerative changes are graded:

1. Pﬁrrman Grading (5 classes) - A general grading system proposed in
2007 by Griﬃth et al. to categorise the degree of intervertebral disc
(IVD) degeneration (originally for older patients). Ranges from 1 (no
degeneration) to 5 (severe degeneration)

2. Disc Narrowing (4 classes) - The width of the IVDs. Ranges from 1

(no narrowing) to 4 (extreme narrowing).

3. Central Canal Stenosis (4 classes) - A narrowing of the spinal canal
which can in turn lead to compression of the spinal chord. Ranges from
1 (no compression), 2 (mild compression), 3 (moderate compression)
and 4 (severe compression).

4. Upper & Lower Endplate Defects (Binary) - Abnormalities/damage to

the top or base of the VB’s constituent bodies.

5. Upper & Lower Marrow Changes (Binary) - Lesions/changes in the

intensity of constituent VBs.

6. Left & Right Foraminal Stenosis (Binary) - A narrowing of the in-
tervertebral foramina (openings where spinal nerves leave the central
canal). In severe cases, this can lead to nerve compression similar to
central canal stensosis.

7. Spondylolisthesis (Binary) - A condition when a vertebra slips forward
onto the vertebral disc below. This is often caused by a fracture in the
pars interarticularis, a segment of bone that joins the vertebrae.

8. Disc Herniation (Binary) - A condition where the centre of the IVD
(nucleus pulposus) breaks through its casing (annulus ﬁbrosus). This
can lead to nerve compression.

Examples of these degenerative changes from the training dataset are

shown in Figure 7.

4.4 Training

In total 4 networks are trained for the SpineNetV2 pipeline: 1) a detec-
tion network which detects vertebral bodies agnostic to their level; 2) a
labelling appearance network which aims to label vertebrae based on their
appearance alone; 3) a labelling context network which reﬁnes the predic-
tions of the appearance network based on the appearance of neighbouring
VB detections 4) a radiological grading network which operates on volumes
surrounding IVDs and outputs gradings for the degenerative changes listed
in section 4.3.2. Each network is trained independently on data from either
Genodisc, OWS or a combination of both.

15

Figure 7: Example degenerative changes from the Genodisc training dataset.

4.4.1 VB Detection

The network is trained on patches of sagittal slices from Genodisc and OWS.
These patches cover an area of approximately 25cm2 and are resampled to
224×224 pixels via cubic interpolation.

Ground truths are constructed from VB polygons marked by annota-
tors. The network’s 13 output channels are divided in two types: detection
channels which indicate the location of individual landmarks in the image
and grouping channels that show which vertebra each landmark belongs to
by ‘pointing’ to the corresponding centroid. The ground truth for each of
these two channel types is generated as follows:

• Detection Channels: A gaussian response is added to the correspond-
ing channel for each VB vertex, normalised to have a peak value of 1
and have variance proportional to the VB polygon’s surface area. A
gaussian response is added to the centroid detection channel at the
centre of each annotated polygon.

• Grouping Channels: The grouping channels for each vertex are con-
structed such that, for an area around each vertex proportional to the
VB’s surface area, the two corresponding grouping channels together
point to the VB’s centroid.
Once the target tensor, ˆY ,

is constructed, the detection network is
trained end-to-end using the following composite loss function for output
tensor Y :

L(Y, ˆY ) = Ldetect(Y, ˆY ) + Lgroup(Y, ˆY ).

An L1-regression loss is applied to the detection channels;

Ldetect(Y, ˆY ) =

5
(cid:88)

k=1

16

αijk |yijk − ˆyijk| ,

(1)

(2)

Foraminal StenosisPﬁrrmann Grading(Grade 3)Disc HerniationEndplate DefectsSpondylolisthesisCentral Canal StenosisMarrow ChangesDisc Narrowingwhere k indexes the landmark channel (the four VB corners and centroid),
i and j index the position in the patch and αijk is a weighing factor given
by

αijk =

(cid:40) Nk

Nk+Pk
Pk
Nk+Pk

if ˆyijk ≥ T
if ˆyijk < T

(3)

where Nk and Pk are the number of pixels in the target detection channel
respectively less than or greater than some threshold T (T = 0.01 in this
case).

The vector ﬁeld grouping channels are supervised by an L2-regression

loss,

Lgroup =

4
(cid:88)

(cid:88)

(cid:88)

l=1

b

(i,j)∈Nbl

||vl

ij − rk

ij||2
2.

(4)

Here l indexes each corner type/vector ﬁeld, b indexes the annotated VBs
in the patch and Nbl is a neighbourhood surrounding the lth corner of the
bth VB annotated in that patch. vl
ij is the value of the output vector ﬁeld
corrresponding to corner l at location (i, j) and rb
ij is the ground truth value
of the vector ﬁeld, ie. the displacement vector from the centroid of VB b to
location (i, j).

We use heavy augmentation during training including image rotation,
rescaling and ﬂipping in the coronal plane. The network is trained using an
Adam optimizer with learning rate 10−3 with parameters β = (0.9, 0.999).

4.4.2 VB Labelling

The labelling pipeline requires both the appearance and context networks to
be trained which is done seperately as follows: Firstly a volume is extracted
around each annotated VB. This is done by tightly ﬁtting a bounding cuboid
around each detection and then expanding the box by 50% in each direction
to capture nearby anatomical structures. The resulting volume is the resam-
pled to a size of 224×224×16 voxels (isotropically in the axial and coronal
planes but not in the sagittal plane). These volumes are then given as input
to the appearance network which attempt to classify the vertebra from S1
to C3 in a 23-way classiﬁcation problem. This network is trained on OWS
only using a standard cross-entropy loss.

The context network is also trained on OWS. Input height-probability
maps are contructed such that for a given VB with detected from height
ya to yb, with centroid at yc = ya+yb
, the height-probability map P has
the same value as the temperature-softmaxed (T=0.1) predictions from the
appearance network from height yc −0.5×(yb −ya) to yc +0.5×(yb −ya). The
context network is an image-to-image translation network which takes P as
input and then outputs a reﬁned version of P , denoted P (cid:48). P (cid:48) is then decoded

2

17

into a discrete series of predictions at the height of each VB detection using a
beam search. A visual representation of this process is shown in Figure 5. As
an augmentation during training each vertebra detection is dropped from
P with probability 0.1. A loss function is still applied to the predictions
at height of the missing detection, on the basis that the model should be
able to infer the vertebra’s level from the surrounding detections. Both the
appearance and context networks are trained using an Adam optimizer with
learning rate 10−3 with parameters β = (0.9, 0.999).

4.4.3 Radiological Grading of Degeneration Changes

The grading network is trained on the radiologist-labelled IVDs from the
Genodisc dataset from S1-L5 to L1-T12. The model consists of a feature-
extracting backbone that encodes each IVD as a 512-dimensional vector,
followed by 8 2-layer MLP projection heads, each of which produces predic-
tions for a diﬀerent grading task. Each of grading task is highly imbalanced
in Genodisc and hence a balanced cross entropy loss is used in all cases.
During training, augmentation is applied by jittering, rotating, ﬂipping and
increasing/reducing the brightness and contrast of each IVD by ±10%. In
50% of the training cases a random gaussian noise of 10% is also added. The
entire model is trained end-to-end until 10 consecutive validation epochs do
not yield an improvement in averaged balanced accuracy. This is done using
an Adam optimizer with learning rate 10−4 with parameters β = (0.9, 0.999).
Once this is completed, the feature-extraction backbone is frozen and the
task-speciﬁc projections heads are then trained individually with a learning
rate of 10−5, until 10 consecutive validation epochs do not yield improved
accuracy for that speciﬁc grading task. The results of this training, along
with those of the labelling and context networks, are given in the following
Section.

5 Results

This section details experimental results from validating the output of the
detection, labelling and grading pipelines on withheld data.

5.1 VB Detection and Labelling

The detection and labelling stages of SpineNetV2 are trained on the train-
ing splits of Genodisc and OWS. Here, we present results of the labelling
and grading pipelines on the test splits from that dataset, as well as on a
publically available dataset published by Zuki´c et al. [16], distributed on
SpineWeb2. These results are from our initial paper on the method used to
detect and label vertebrae used in SpineNet [14].

2http://spineweb.digitalimaginggroup.ca/

18

Evaluation: For the detection stage of the pipeline we report the precision
and recall of the VB detector. Following [9], we deﬁne correct detections to
be when the ground truth vertebra centroid is contained entirely within a
single detected bounding quadrilateral. For the labelling stage, we report
the identiﬁcation rate (IDR). To be correctly identiﬁed, a vertebra must be
both detected and assigned the correct level by the labelling stage.

Dataset

OWS
(Whole
Spine)

Genodisc
(Lumbar)

Zuki´c
(Lumbar)

Scans Verts

Method
Windsor† [13]
888 Label Baseline

37

Ours

Lootus [9]

421

2947 Label Baseline

Ours

Zuki´c [16]

17

154 Label Baseline

Ours

Prec. (%) Rec. (%) IDR(%) IDR± 1(%) LE (mm)

99.4
-
99.0

-
-
99.7

98.7
-
99.3

99.4
-
98.1

-
-
99.7

92.9
-
98.7

-
86.9
96.5

86.9
90.1
98.4

-
87.0
90.9

-
93.4
97.3

-
97.4
99.7

-
94.3
98.7

1.0 ± 0.9
-
2.4 ± 1.3

3.5 ± 3.3
-
1.6 ± 1.1

1.6 ± 0.8
-
2.0 ± 1.5

Table 2: Performance of the detection and labelling pipeline on the three
datasets. Our approach is compared with other methods using the same
datasets and also a LSTM labelling baseline , reported on a per-vertebra
level. We also report the percentage of vertebrae within one level of their
ground truth value (IDR±1). Lootus [9] is tested on a subset of 291 scans
from the Genodisc dataset. Note, Windsor† [13] requires manual initializa-
tion by providing the location of the S1 vertebra, so is not directly compa-
rable

Baselines: For each of the three datasets used to assess detection and la-
belling, we compare to pre-existing methods reporting results on the same
dataset. For OWS, we use the method outlined in [13] which detects verte-
brae sequentially moving up the spine, starting from S1. It should be noted
that this algorithm requires the location of S1 to be known, and is therefore
not directly comparable to the our method in that it is only semi-automated.
For Genodisc, we compare to results reported in [9], which detects and la-
bells vertebrae using a HOG-template based method in combination with a
graphical model. Finally, for the Zuki´c dataset, we compare to the results
reported in the initial paper. It should be noted that several other meth-
ods have been proposed to perform vertebra detection and labelling in MRI
scan (e.g. [1, 3, 4, 15]). However, these methods do not have publically avail-
able datasets, and thus direct comparison is not possible. To motivate the
use of a fully-convolutional context network as opposed to a more standard
recurrent network, we also train a bi-directional LSTM labelling baseline.
This baseline takes features extracted from each vertebra by the appearance
network as a baseline, and outputs a label for each vertebra as output.

19

5.2 Radiological Grading

This section evaluates SpineNetV2’s agreement with an expert radiologist
on withheld data from the Genodisc Dataset. Table 3 compares SpineNetV1
and V2 across all V1 tasks (Pﬁrrmann, Disc Narrowing, Endplate Defects,
Marrow Changes, Spondylolisthesis and Binary Central Canal Stenosis). It
also reports results for new grading tasks added in V2, namely 4-class central
canal stenosis grading, foraminal stenosis grading, and disc herniation. In
all cases the balanced accuracy is reported.

Task

Pﬁrrmann

Disc Narrowing

# Classes

SpineNet V1
SpineNet V2

5

71
70.9

4

76.1
76.3

Endplate Defect
Lower
Upper

Marrow Change
Lower
Upper

2

82.9
84.9

2

87.8
89.6

2

89.2
88.9

2

88.4
88.2

Task

Spondylolisthesis Central Canal Stenosis

Foraminal Stenosis Herniation
Left

Right

# Classes

SpineNet V1
SpineNet V2

2

95.4
95

2

95.8
93.2

4

-
64.9

2

-
84.8

2

-
82.4

2

-
80.4

Table 3: Grading results for withheld data from the Genodisc dataset. It
should be noted that SpineNetV2 does not have a seperate head for binary
central canal stenosis, and instead concatenates together predictions from
the multiclass central canal stenosis projection head.

As can be seen from Table 3, performance of SpineNetV2 closely matches
that of SpineNetV1 in all tasks. The major changes are the addition of the
4-class central canal stenosis, foraminal stenosis grading and disc herniation.

5.3 Processing Speed

For application in real-world scenarios, SpineNetV2 needs to be as fast as
possible. Here we report the inference time of SpineNetV2 with and with-
out a GPU on lumbar and whole spine scans. We also report the peak
memory usage during processing. For a lumbar scan, vertebrae detection
and labelling takes approximately 2 seconds and grading taskes 1 second,
whereas for a whole spine scan, detection and labelling takes approximately
5 seconds on a GPU. Peak GPU memory usage is 1.6GB for a lumbar scan,
and 2.5GB for a whole spine scan. On a CPU, lumbar detection and la-
belling takes 25 seconds, with grading taking 4 seconds. For a whole spine
scans VB detection and labelling takes 2 minutes on a CPU. This is because
the scan is split into multiple patches, each of which is ingested seperately
by the VFR model. This could likely be reduced signiﬁcantly by changing
the patch-splitting strategy to use cover larger areas with smaller overlaps.
These results are summarized in Table 4.

20

Task Name

Scan Type

Processing Time (s)

VB Detection & Labelling
VB Detection & Labelling
IVV Radiological Grading

Lumbar
Whole Spine
Lumbar

GPU
3
5
1

CPU
25
120
4

Table 4: Processing speed of SpineNetV2 on lumbar and whole spine scans.
Note that, in all cases, SpineNetV2 is much faster than V1 which takes
several minutes to perform detection and grading on a single lumbar scan
using a GPU.

Figure 8: Example VB detections for a variety of ﬁelds of view.

5.4 Example Qualitative Results

Figure 8 shows example results from the detection pipeline. Example results
from the grading pipeline for healthy and pathological scans can be seen
at http://zeus.robots.ox.ac.uk/spinenet2/demo.html.

5.5 Online Demo

A demo version of the SpineNetV2 software for T2-weighted lumbar MRIs
is available online at http://zeus.robots.ox.ac.uk/spinenet2/. This
demo takes as input zipped sagittal slices as DICOMs and outputs a CSV
or JSON containing the grading results, depending on preference. Cached
results for publically available online samples can be found on this website.
These are shown in Figure 9. If you want try the demo software on your
own scans please use the contact information provided on the website to be

21

given access. Input scans are not stored after processing, however should
be anonymised prior to submission. Furthermore, please note SpineNetV2
is still a research tool and should not be used for clinical purposes.

6 Conclusions

This technical report describes SpineNetV2, a deep learning framework for
detecting and labelling vertebrae in clinical spinal MR scans and to perform
radiological grading in T2-weighted scans. We describe the methods used to
in each step of the pipeline, as well as the software architecture and datasets
for training the constituent neural networks. We also give experimental re-
sults for both VB detection and labelling and radiological grading on unseen
test data.

SpineNetV2 is an ongoing project. As such we are always looking to
validate the existing functionality of SpineNetV2, as well as extend its func-
tionality to new tasks and applications. If you would like to use SpineNetV2
in your own research, or have any questions, please contact us via email:
{rhydian,amir}@robots.ox.ac.uk.

7 Acknowledgements

We would like to thank our clinical collaborators, without whose sunpport
this project would not be possible; Prof. Jeremy Fairbank, Dr. Jill Urban,
Dr. Sarim Ather, Prof. Ian McCall (in no particular order). We are also
grateful to our funders; Cancer Research UK via the EPSRC CDT in Au-
tonomous Intelligent Machines and Systems and EPSRC programme grant
Visual AI (EP/T025872/1).

22

(a) Example healthy spine (radiopedia.org, rID:33543, Courtesy: A.F. Galliard).

(b) An example spine with degenerative changes (radiopedia.org, rID:56636,
Courtesy: H. Knipe)

Figure 9: Outputs from the online web demo for two example lumbar MRIs.
Results such as these can be exported in both CSV and JSON format.

23

References

[1] Cai, Y., Landis, M., Laidley, D.T., Kornecki, A., Lum, A., Li, S.: Multi-
modal vertebrae recognition using Transformed Deep Convolution Net-
work. Computerized Medical Imaging and Graphics 51, 11–19 (2016)

[2] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the
Devil in the Details: Delving Deep into Convolutional Nets. In: British
Machine Vision Conference (2014)

[3] Forsberg, D., Sj¨oblom, E., Sunshine, J.L.: Detection and Labeling of
Vertebrae in MR Images Using Deep Learning with Clinical Anno-
tations as Training Data. Journal of Digital Imaging 30(4), 406–412
(2017)

[4] Glocker, B., Zikic, D., Konukoglu, E., Haynor, D.R., Criminisi, A.:
Vertebrae Localization in Pathological Spine CT via Dense Classiﬁ-
cation from Sparse Annotations. In: Medical Image Computing and
Computer-Assisted Intervention. pp. 262–270. Lecture Notes in Com-
puter Science (2013)

[5] Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On Calibration of
International Conference on Machine

Modern Neural Networks. In:
Learning (2017)

[6] Jamaludin, A., Kadir, T., Zisserman, A.: Spinenet: Automatically pin-
pointing classiﬁcation evidence in spinal mris. In: International Confer-
ence on Medical Image Computing and Computer Assisted Intervention
(2016)

[7] Jamaludin, A., Kadir, T., Zisserman, A.: SpineNet: Automated classi-
ﬁcation and evidence visualization in spinal MRIs. Medical Image Anal-
ysis 41, 63–73 (2017)

[8] Jamaludin, A., Lootus, M., Kadir, T., Zisserman, A., Urban, J., Batti´e,
M.C., Fairbank, J., McCall, I.: Automation of reading of radiological
features from magnetic resonance images (mris) of the lumbar spine
without human intervention is comparable with an expert radiologist.
European Spine Journal (2017)

[9] Lootus, M., Kadir, T., Zisserman, A.: Vertebrae detection and labelling
in lumbar mr images. In: MICCAI Workshop: Computational Methods
and Clinical Applications for Spine Imaging (2013)

[10] Lootus, M., Kadir, T., Zisserman, A.: Radiological grading of spinal
MRI. In: MICCAI Workshop: Computational Methods and Clinical
Applications for Spine Imaging (2014)

24

[11] Palmer, K.T., Walsh, K., Bendall, H., Cooper, C., Coggon, D.: Back
pain in britain: comparison of two prevalence surveys at an interval of
10 years. BMJ 320(7249), 1577–1578 (2000)

[12] Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Net-
works for Biomedical Image Segmentation. In: Navab, N., Hornegger,
J., Wells, W.M., Frangi, A.F. (eds.) Medical Image Computing and
Computer-Assisted Intervention (2015)

[13] Windsor, R., Jamaludin, A.: The ladder algorithm: Finding repeti-
tive structures in medical images by induction. In: IEEE International
Symposium on Biomedical Imaging (2020)

[14] Windsor, R., Jamaludin, A., Kadir, T., Zisserman, A.: A convolutional
approach to vertebrae detection and labelling in whole spine mri (2020)

[15] Yang, D., Xiong, T., Xu, D., Huang, Q., Liu, D., Zhou, S.K., Xu, Z.,
Park, J., Chen, M., Tran, T.D., Chin, S.P., Metaxas, D., Comaniciu, D.:
Automatic Vertebra Labeling in Large-Scale 3D CT Using Deep Image-
to-Image Network with Message Passing and Sparsity Regularization.
In: Information Processing in Medical Imaging. pp. 633–644 (2017)

[16] Zuki´c, D., Vlas´ak, A., Egger, J., Hoˇr´ınek, D., Nimsky, C., Kolb, A.:
Robust Detection and Segmentation for Diagnosis of Vertebral Diseases
Using Routine MR Images. Computer Graphics Forum 33(6), 190–204
(2014)

25

