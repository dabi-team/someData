Accepted at the European Solid-state Devices and Circuits Conference (ESSDERC), September 2022

In-memory Realization of In-situ
Few-shot Continual Learning with
a Dynamically Evolving Explicit Memory

G. Karunaratne∗§, M. Hersche∗§, J. Langenegger∗§, G. Cherubini∗, M. Le Gallo∗, U. Egger∗,
K. Brew†, S. Choi†, I. Ok†, C. Silvestre†, N. Li†, N. Saulnier†, V. Chan†, I. Ahsan†,
V. Narayanan‡, L. Benini§, A. Sebastian∗, A. Rahimi∗
∗IBM Research, Z¨urich, Switzerland †IBM Research, Albany, NY, USA
‡IBM T. J. Watson Research Center, NY, USA §ETH Z¨urich, Z¨urich, Switzerland

2
2
0
2

l
u
J

4
1

]

G
L
.
s
c
[

1
v
0
1
8
6
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Continually learning new classes from few train-
ing examples without forgetting previous old classes demands
a ﬂexible architecture with an inevitably growing portion of
storage, in which new examples and classes can be incrementally
stored and efﬁciently retrieved. One viable architectural solution
is to tightly couple a stationary deep neural network to a
dynamically evolving explicit memory (EM). As the centerpiece
of this architecture, we propose an EM unit that leverages
energy-efﬁcient in-memory compute (IMC) cores during the
course of continual learning operations. We demonstrate for the
ﬁrst time how the EM unit can physically superpose multiple
training examples, expand to accommodate unseen classes, and
perform similarity search during inference, using operations on
an IMC core based on phase-change memory (PCM). Speciﬁcally,
the physical superposition of few encoded training examples is
realized via in-situ progressive crystallization of PCM devices.
The classiﬁcation accuracy achieved on the IMC core remains
within a range of 1.28%–2.5% compared to that of the state-of-
the-art full-precision baseline software model on both the CIFAR-
100 and miniImageNet datasets when continually learning 40
novel classes (from only ﬁve examples per class) on top of 60 old
classes.

Index Terms—In-memory Computing, Continual Learning,
Few-shot Learning, Hyperdimensional Computing, Non-volatile
Memory Devices

I. INTRODUCTION

Few-shot continual learning (FSCL), aka few-shot class-
incremental learning [1], [2], requires a learner to incremen-
tally learn new classes from very few training examples,
without forgetting the previously learned classes. The learner
is exposed to a series of sessions, whereby each session
introduces distinct unseen classes by providing only a few
training examples per class. From these few examples, the
learner should quickly and incrementally learn novel classes
without forgetting the previously learned old classes. After
learning novel classes in each session, the learner is evaluated
on several query samples from all the classes, to which it was
exposed so far (i.e., the union of the classes from the previous
and the current sessions). FSCL is a very challenging research
problem that could impose signiﬁcant additional compute and
memory costs on the learner.

Very recently, a low-cost solution was proposed that avoids
expensive gradient-based computations for learning unseen
classes during the course of FSCL [3]. This solution is inspired
by a robust few-shot learner [4] that brings together deep
neural networks with hyperdimensional computing [5], [6]
to be able to represent raw images with high-dimensional
holographic binary or bipolar vectors. Inspired by this pow-
erful combination, the learner in [3] is composed of a frozen
controller and a dynamically evolving explicit memory (EM)
for FSCL. The controller is a deep convolutional network
(including a ﬁnal fully connected layer) that is interfaced with
the EM unit to dynamically store or retrieve the acquired
knowledge about the classes. The controller interacts with the
EM through write and read operations using d-dimensional
holographic vectors. Although the controller remains station-
ary, the EM dynamically updates its contents by new examples,
and grows its size by storing new classes. Hence, it is desirable
to have an EM unit with dynamically evolving contents that
retains the acquired knowledge about already-seen classes in
both compressed and nonvolatile manner.

II. PROPOSED EXPLICIT MEMORY UNIT FOR FSCL

Fig. 1(a)(b) illustrates the main stages of FSCL involved
in [3]: a pretraining and metalearning stage, and an inference
stage. The ﬁrst stage is all done in software. The goal of
this rather elaborate stage is to train the controller (here,
a ResNet-12) to be able to generate d-dimensional1 quasi-
orthogonal real-valued vectors for different classes. By the end
of this stage, the controller has learned how to assign 256-
dimensional quasi-orthogonal, and thus dissimilar, vectors to
novel classes in the EM, which allows the controller to remain
stationary afterwards. This pretraining and metalearning stage
is based on just
the ﬁrst session’s (S1) training samples.
Based on the datasets used in our experiments, as explained
in Section IV-A, the ﬁrst session contains 60 classes, each
including 500 samples, out of the total 100 classes.

1As seen later in Section III, d is ﬁxed to 256 so that the vectors occupy

an entire column of a PCM crossbar array.

 
 
 
 
 
 
Fig. 1. (a) Stage 1 (Pretraining and Metalearning), the controller weights are
updated over a series of episodes. (b) In Stage 2 (Inference), the pretrained
and metalearned (P&M) controller generates support and query vectors over
a series of sessions. (c) EM operations during inference stage can be divided
learning consists of: [ex]=expansion
into two phases. Phase 1 continual
(allocating a new column in the array to store the ﬁrst support vector from a
previously unseen class); [ps]=physical superposition (few support examples
are accumulated on the relevant class memory). Phase 2 evaluation phase
consists of: [ss]=similarity search between a query vector and class vectors
on the array at that point in time. Black and white slots in EM correspond to
already ﬁlled and empty slots, respectively.

Next, the inference stage is composed of two phases, as
learning phase of novel
shown in Fig. 1(c): a continual
classes from very few training/support examples per class (in
our results, 5 training examples, hence 5-shot), and a query
evaluation phase in which we evaluate the accuracy over a
batch of query examples containing a number of samples
(100 in the datasets we used) per each class encountered so
far. We quantize the controller’s output vector elements to
1-bit, hence the controller generates bipolar support vectors
(of training examples) to be written, or accumulated, in the
EM unit during the continual learning phase. For the query
vectors (of query samples) during the evaluation phase, we
quantize the controller’s output vector elements to 8-bit, so an
analog input corresponding to the 8-bit query vector element
is applied in each row of the crossbar array. This performs a
similarity search between query vector and the class vectors.
The resulting vector received via the columns of the crossbar
array is used to classify the query sample. The class corre-
sponding to the maximum element of the resulting vector is
then taken as the predicted class.

For the inference stage,

the EM is implemented on an
IMC core with a unit-cell array comprising PCM devices
(see Fig. 2). In every continual learning phase: (i) when the
ﬁrst example of a new class appears, the EM is expanded
by choosing a fully reset column of unit-cells and, based on
the sign of the bipolar support vector element, the unit-cell
conductance is increased or decreased by the application of
single SET pulses; ii) when an example from a previously seen
class appears, a similar update is performed on the column of
unit cells corresponding to that particular class.

(a) Schematic illustration of the EM implemented on a cross-bar
Fig. 2.
array of unit-cells. Each unit-cell comprises 4 PCM devices (only 2 used for
this experiment) organized in a differential conﬁguration. During continual
learning, the expansion and physical superposition operations are performed
by applying SET pulses to a column of devices. Similarity search is realized
by applying read voltage pulses with varying pulse width and subsequently
digitizing the accumulated current using ADCs. (b) Illustration of the partial
crystallization of the differential pair of devices under different +1/-1 pulse
sequences during superposition operation.

We exploit the in-situ accumulation via progressive crys-
tallization of PCM to realize physical superposition of few
related support vectors on a single class vector. This means,
as shown in Fig. 2(b), starting from a fully reset pair of PCM
devices, ﬁne grained SET pulses are applied on either the
positive or the negative device depending on whether the type
of accumulation is incremental or decremental based on the
input bipolar support vector element. This creates a multibit
analog EM, whereby the size of the EM is set just by the
number of classes, as opposed to the product of the number
of classes and the number of training examples per class [4].
The nonvolatility of the resulting analog states preserves the
acquired knowledge about already-seen classes. Finally, during
the evaluation phase, the frequent similarity search between
the 8-bit query vector (of a query image) and the analog class
vectors are computed in-memory by exploiting Kirchhoff’s
circuit laws, and the result is directly used for classiﬁcation.
Moreover, given that the controller requires no parameter
updates after the ﬁrst stage of pretraining and metalearning, it
can be treated as a deep neural network with stationary weights
that can also beneﬁt from an IMC-based implementation, as
demonstrated for example in [7].

Fig. 3. Experimental Setup. On the right: the micrograph of the Hermes chip.
The chip is accessed using a sequence of programming and MVM commands
prepared and sent by the host computer via a FPGA-based interface.

Rand episode samplerWeight updatesupportqueryPretrainedControllerS1(base training set) Activation(Tanh)ExplicitMemorysupportquery To predictionMetalearningInferencePretrainingStage 1Stage 2P&MControllerQuantize(1bit)Quantize(8bit)ExplicitMemorysupport vectorquery vectorexplicit memory[ps][ex][ex][ps][ss]P&MControllertimecontinual learning phaseevaluationphaseTo prediction(a)(c)(b)Column addressesPolarityselection vectorsP&MControllerSignTrainingLabelsTrainingsamplesQueryLabelsQuerysamplesQuantizeP&MControllerPWM vectorsprogramming commandsMVMcommandsADC code vectorsSessionaccuracyHost computerHermessingletileMVM result vectors(a) Progressive crystallization of PCM devices in the IMC core.
Fig. 4.
The conductance distribution corresponding to 65,536 devices is shown with
the successive application of SET pulses after an initial RESET. The blue
curve denotes the mean conductance and the red bars indicate one standard
deviation. (b) A 2-D illustration of the conductance distribution for the same
experiment. It can be seen that the dynamic range (number of SET pules
needed to span the conductance range) is fully adequate, because of the very
small number ( 5) of training examples available during FSCL.

III. EXPERIMENTAL SETUP

The experiments are carried out on the Hermes chip with
a 256x256 unit-cell array of PCM devices organized in a
differential conﬁguration [8] (see Fig. 3), accessed by a host
computer via FPGA (Field Programmable Gate Array)-based
interface. The bipolar support vectors are sent as SET pulses
to the corresponding column of 256 unit cells of the initially
fully RESET array. Based on the +1/-1 vector elements,
the conductance of the positive/negative polarity devices are
increased. The evolution of the conductance distribution of
individual PCM devices as a function of the number of applied
SET pulses on initially RESET devices is shown in Fig. 4.

For similarity search in the evaluation phase, a 4-quadrant
matrix-vector multiply (MVM) is performed between the 8-bit
query vector and the set of analog class vectors stored in the
unit-cell array.

IV. EXPERIMENTAL RESULTS

A. Datasets used for evaluation

For the accuracy evaluation, we use the CIFAR-100 and
the miniImageNet dataset, restructured to comply with the
FSCL setting [1]–[3]. Both datasets contain natural images
of 100 classes in total, which are divided into a ﬁrst session
(S1) containing 60 classes with 500 training and 100 query
examples per class, and eight novel sessions (S2–S9) with 5
novel classes introduced in each session containing 5 support
examples and 100 query examples per class. The updated class
vectors occupy 256 rows and 60 columns (classes) on the
array in S1, evolving to 100 columns (classes) in the last S9.
The conductance evolution of unit cells of the crossbar array
corresponding to CIFAR-100 is shown in Fig. 5.

Fig. 5. A 2-D map of the array conductance (of the unit-cells with differential
PCM devices) for the FSCL experiment on CIFAR-100 dataset. With each
new session, more columns of the array are selected corresponding to the
new classes in that session. And with each training example per class, the
corresponding class vectors are updated with the application of SET pulses.
The unit-cells activated for the update at each panel are highlighted in green. A
close up view of the conductance evolution in a 10x10 region of the crossbar
is shown in the bottom row.

Fig. 6. FSCL classiﬁcation accuracy for IMC vs. FP32 software implemen-
tations for (a) CIFAR-100 and (b) miniImageNet. As is expected in FSCL,
the accuracy progressively reduces with increasing number of sessions, as the
learner is progressively exposed to more classes. However, the IMC accuracy
is still within 2.5% of the full-precision software baseline [3] for all sessions.
Notably, our IMC hardware accuracy surpasses other full-precision software
methods [1], [2], for all sessions across the two datasets.

all sessions. This still makes the accuracy of our EM on
the IMC hardware from 2.5% to 11.01% higher than the
other best performing full-precision software methods reported
in [1], [2], considering all sessions across CIFAR-100 and
miniImageNet datasets.

B. Classiﬁcation accuracy

C. Energy estimation

The accuracy obtained for each dataset with our EM on
the IMC hardware and various full-precision software base-
lines are illustrated in Fig. 6. Compared to the full-precision
software baseline in [3], the accuracy degradation with our
EM realization is at worst 2.5% and at best 1.28% across

The energy consumption during incremental class vector
updates is estimated using the programming parameters, which
include peak pulse current of 150 uA, ﬂat pulse duration 5 ns,
trailing edge pulse duration 40 ns and source voltage 2.34 V.
These parameters yield an energy expense of 8.78 pJ per PCM

5ns250uA50ns700uARESET pulsefrequencypulse count(b)(a)Conductance (a.u.)Conductance (a.u.)10310210110010-140ns12345shots (increasing # training examples from the same class)S5S9conductance (a.u.)sessions256256256bitlines80S1wordlines111(a) CIFAR-1006065707580859095100#classessession(b) miniImageNetS1S2S3S4S5S6S7S8S9304050607080Software [1]Software [2]Baseline Software (Mode 1) [3]Ours IMC HardwareS1S2S3S4S5S6S7S8S9(%)6065707580859095100304050607080device during one programming cycle. Considering the vector
dimension of 256, the total programming time and energy
spent during an incremental update of one class vector are
11.5 us and 2.25 nJ, respectively. Given that 25 (5-shots from
5 classes) class vectors are updated during all subsequent
sessions (S2–S9), the time and energy spent on updating the
class vectors in these sessions are estimated to be 57.6 us and
56.2 nJ, respectively.

In comparison, the time and energy spent on similarity
search of a single query (including digital to analog conver-
sion, PCM read and analog to digital conversion) are estimated
to be 520 ns and 7.74 nJ, respectively, during the last session.
This leads to a total similarity search time and energy of 5.2 ms
and 77.3 uJ respectively, as in this session we evaluate 10,000
queries (100 queries per class) in total, compared to just 25
updates of the class vectors. The limited number of updates
and the application of SET pulses with low energy ensure that
the durability of PCM is not signiﬁcantly affected [9], [10].

TABLE I
COMPARISON WITH THE RELATED WORKS

Kazemi
et al. [11]

Li et al.
[12], [13]

Karunaratne
et al. [4]

This work

Few-shot learning
Continual learning
In-situ accumulation
Holographic rep. in EM
Analog multibit EM
Truly O(1) searcha
Query vector dim. (d)
Number of classes
Similarity search energyb
Programming energyc

(cid:88)

(cid:88)

128–192
≤20
-
-

(cid:88)

128
32
17.5 pJ
99 pJ

(cid:88)

(cid:88)

512
≤100
25.6 pJ
6240 pJ

Dataset(s)

Omniglot

Omniglot

Omniglot

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

256
≤100
19.1 pJ
8.78 pJ
miniImageNet
& CIFAR100

a Actual crossbar (no emulation based on individual memory devices) performing all dot

product operations in parallel in-memory

b Core energy normalized to one class vector of length 256 (d = 256)
c per vector element

D. Comparison

We compare features of our work against the related works
learning capability
in Table I. Our work shares few-shot
with [4], [11]–[13], holographic representation of vectors
with [4], and truly O(1) similarity search capability with [12],
[13]. However, our work is the ﬁrst to demonstrate continual
learning capability using the in-situ accumulation property.
This makes our EM unit the ﬁst multibit analog IMC core,
whereas in [12], [13] storage and queries use binary vectors.
Furthermore, our EM unit can handle up to 100 class vectors
for the natural image datasets, making it the largest truly O(1)
similarity search engine with analog states to date.

In Table I, we also compare energy saving we gain by
programming the support vectors using the in-situ accumu-
lation instead of programming them from scratch in the novel
bit lines. Programming with the in-situ accumulation is at
least 4.7× energy efﬁcient
than the normal programming,
because the in-situ accumulation requires a SET pulse of

shorter duration. Our similarity search energy remains on par
with other works.

V. CONCLUSION

We present a hardware based on an in-memory compute
core consisting of PCM devices to implement the EM opera-
tions required in FSCL. We demonstrate for the ﬁrst time how
support vectors are accumulated in-situ using the progressive
crystallization property of PCM. The proposed approach leads
to physically superposed representations and enhanced energy
savings, while retaining the accuracy within 2.5% from the
full-precision software baseline.

ACKNOWLEDGEMENTS

This work is supported by the IBM Research AI Hardware
Center, and the Center for Computational Innovation at Rens-
selaer Polytechnic Institute for computational resources on the
AiMOS Supercomputer.

REFERENCES
[1] X. Tao et al., “Few-shot class-incremental learning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), 2020. 1, 3

[2] G. Shi et al., “Overcoming catastrophic forgetting in incremental few-
shot learning by ﬁnding ﬂat minima,” Advances in Neural Information
Processing Systems, vol. 34, 2021. 1, 3

[3] M. Hersche et al., “Constrained Few-shot Class-incremental Learning,”
in Conference on Computer Vision and Pattern Recognition (CVPR),
2022. 1, 3

[4] G. Karunaratne et al., “Robust high-dimensional memory-augmented
neural networks,” Nature Communications, vol. 12, no. 1, pp. 1–12,
2021. 1, 2, 4
[5] P. Kanerva,

“Hyperdimensional Computing: An Introduction to
Computing in Distributed Representation with High-Dimensional
Random Vectors,” Cognitive Computation, vol. 1, no. 2, pp. 139–159,
2009. 1

[6] R. W. Gayler, “Vector symbolic architectures answer Jackendoff’s
the Joint
challenges for cognitive neuroscience,” in Proceedings of
International Conference on Cognitive Science. ICCS/ASCS, 2003, pp.
133–138. 1

[7] V. Joshi et al., “Accurate deep neural network inference using computa-
tional phase-change memory,” Nature Communications, vol. 11, no. 1,
pp. 1–13, 2020. 2

[8] R. Khaddam-Aljameh et al., “HERMES-core—a 1.59-TOPS/mm2 PCM
on 14-nm CMOS in-memory compute core using 300-ps/LSB linearized
CCO-based ADCs,” IEEE Journal of Solid-State Circuits, vol. 57, no. 4,
pp. 1027–1038, 2022. 3

[9] T. Tuma et al., “Stochastic phase-change neurons,” Nature Nanotech-

nology, vol. 11, no. 8, pp. 693–699, 2016. 4

[10] M. Le Gallo et al., “An overview of phase-change memory device
physics,” Journal of Physics D: Applied Physics, vol. 53, no. 21, p.
213002, 2020. 4

[11] A. Kazemi et al., “In-memory nearest neighbor search with FeFET
multi-bit content-addressable memories,” in 2021 Design, Automation
Test in Europe Conference Exhibition (DATE), 2021, pp. 1084–1089. 4
[12] H. Li et al., “One-shot learning with memory-augmented neural net-
works using a 64-kbit, 118 GOPS/W RRAM-based non-volatile asso-
ciative memory,” in 2021 Symposium on VLSI Technology.
IEEE, 2021,
pp. 1–2. 4

[13] H. Li et al., “SAPIENS: A 64-kb RRAM-based non-volatile associative
memory for one-shot learning and inference at the edge,” IEEE Trans-
actions on Electron Devices, vol. 68, no. 12, pp. 6637–6643, 2021. 4

