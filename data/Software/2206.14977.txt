Multiple Targets Directed Greybox Fuzzing

Hongliang Liang, Xianglin Cheng, Jie Liu, Jin Li

1

2
2
0
2

n
u
J

0
3

]

R
C
.
s
c
[

1
v
7
7
9
4
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Directed greybox fuzzing (DGF) can quickly dis-
cover or reproduce bugs in programs by seeking to reach a
program location or explore some locations in order. However,
due to their static stage division and coarse-grained energy
scheduling, prior DGF tools perform poorly when facing multiple
target locations (targets for short).

In this paper, we present multiple targets directed greybox
fuzzing which aims to reach multiple programs locations in
a fuzzing campaign. Speciﬁcally, we propose a novel strategy
to adaptively coordinate exploration and exploitation stages,
and a novel energy scheduling strategy by considering more
relations between seeds and target locations. We implement
our approaches in a tool called LeoFuzz and evaluate it on
crash reproduction, true positives veriﬁcation, and vulnerability
exposure in real-world programs. Experimental results show that
LeoFuzz outperforms six state-of-the-art fuzzers,
i.e., QYSM,
AFLGo, Lolly, Berry, Beacon and WindRanger in terms of
effectiveness and efﬁciency. Moreover, LeoFuzz has detected 23
new vulnerabilities in real-world programs, and 11 of them have
been assigned CVE IDs.

Index Terms—directed greybox fuzzing, crash reproduction,

true positives veriﬁcation, vulnerability exposure

I. INTRODUCTION

Context. Currently, fuzzing is one of the most effective
and practical techniques to discover bugs or vulnerabilities
automatically. By constantly mutating seeds initially provided,
fuzzers generate lots of new inputs and report those that cause
the program under test (PUT) failure or crash [12]. A greybox
fuzzer such as AFL [13] uses program feedback like branch
coverage to boost the efﬁciency of ﬁnding bugs. However, its
consideration to achieve maximum code coverage may waste
a lot of resources in some bug independent code.

By contrast, directed greybox fuzzers, e.g., AFLGo [14],
Lolly [15], Berry [16], spend most of the time budget in
reaching target program locations (targets for short), e.g., prob-
lematic changes, critical APIs or potential bugs, and thus are
more suitable for patch testing and crash reproduction etc.
For example, AFLGo uses harmonic distance between a seed
and targets to reach the targets fast. Lolly exploits target
statement sequences to trigger bugs which are resulted from
the sequential execution of multiple statements. Berry uses
concolic execution to enhance the directedness when reaching
deep targets along some complex paths.

It is reasonable and meaningful for DGF to seek to reach
multiple targets because there are often multiple bugs in real-
world programs. To demonstrate the situation, we randomly
selected nine widely-used programs in the real world and

H. Liang, X. Cheng, J. Liu are with Trusted Software and Intelligent
System Lab, Beijing University of Posts and Telecommunications, Beijing,
China. E-mail: {hliang, chengxl, liujie ran}@bupt.edu.cn. J. Li is with Nation
Key Laboratory of Science and Technology on Information System Security,
Beijing, China. E-mail: tianyi198012@163.com. This research is partially
supported by CNKLSTISS.

counted the bugs or vulnerabilities in them. As shown in
Table I, at least three CVEs were discovered in each of them.
Moreover, to expose or verify multiple (e.g., n) bugs in a
program via directed greybox fuzzing, one way is to run
in parallel n fuzzing instances, each of which is given a
single target to trigger a single bug; another way is to run
a directed greybox fuzzing instance with n targets to trigger
n bugs, e.g., AFLGo, Lolly and Berry. Though both methods
are complementary, in this paper, our goal is to improve the
effectiveness and efﬁciency of the second way on real-world
programs.

Problems. Although DGF is efﬁcient by spending more
resources to explore the code towards the target locations, prior
DGF tools [14]–[16] perform poorly when facing multiple
target locations due to their coarse-grained energy scheduling
and static stage division.

An energy scheduling strategy is usually designed in the
Fuzzers to control the number of seed mutations. In DGF,
the scheduling strategy gradually adds (reduces) energy to
seeds closer to (far away) target locations, which helps trigger
multiple targets faster. For example, AFLGo gives more energy
to a seed with a smaller harmonic distance to all targets,
though this strategy makes AFLGo ignore the local optima.

Problem 1: To cover multiple targets, pursuing a global
optimal scheduling for all targets would ignore local optimal
scheduling for some targets, as AFLGo does, while seeking an
optimal scheduling for a single target would make other targets
difﬁcult to be reached, as Lolly and Berry do. Designing a
suitable energy scheduling for reaching as many targets as
possible within a time limit is another problem.

DGF works in two stages, i.e., exploration and exploitation.
In the exploration stage,
the fuzzer aims to obtain more
code coverage through seeds mutation and execution, thereby
obtaining more run-time information. In exploitation stage,
the fuzzer mutates and executes seeds to get seeds closer
to the target locations. For instance, AFLGo begins with the
exploration stage ﬁrst and randomly mutates the initial seeds to
generate many new inputs, in order to increase code coverage.
It then enters exploitation stage to generate more new inputs
which are increasingly closer to the targets. However, the time
to enter exploitation stage is speciﬁed statically. For example,
AFLGo speciﬁes 20 hours for exploration and 4 hours for
exploitation. This static switching strategy ignores the dynamic
runtime information and may decrease the performance of
AFLGo.

Problem 2: Less exploration would provide less coverage
information for exploitation, making it difﬁcult to generate
high-quality directed seeds in exploitation stage. However,
overmuch exploration would cost many resources and delay
the exploitation, resulting in loss of directedness. Therefore, it
is a challenge to coordinate exploration and exploitation stages

 
 
 
 
 
 
TABLE I
A REAL-WORLD PROGRAM USUALLY CONTAINS MULTIPLE BUGS.

TABLE II
THE PROGRAM COMPONENTS FOR EACH NODE IN FIG.1

2

Program
cxxﬁlt
httpd
jasper
jasper
libming
objdump
readelf
sqlite
tcpdump
tiff2pdf
tiff2pdf

Version
2.26
2.4.46
2.0.14
2.0.12
0.4.8
2.34
2.28
3.32.0
4.9.3
4.09
4.08

#Bugs
6 [3]
7 [11]
14 [6]
15 [7]
70 [8]
4 [1]
8 [2]
10 [9]
26 [10]
3 [4]
4 [5]

in order to balance the coverage and directedness in DGF.

Proposal. To solve the above problems, we present multiple
targets directed greybox fuzzing to efﬁciently cover multiple
programs locations in a single fuzzing campaign. Speciﬁcally,
we propose a novel energy scheduling strategy that considers
multiple relations between a seed and target sequences (MES
for short) and a novel approach to adaptively coordinate
exploration and exploitation stages (CEE for short) based on
two queues.

As for energy scheduling, unlike AFLGo, which considers
the harmonic distance between a seed and multiple targets in
energy scheduling, and also unlike Lolly, which uses seed’s
target sequence coverage as the feature of energy scheduling,
for a seed s and multiple target sequences, MES ﬁrst selects
the target sequence ts which has the highest coverage over
s’ execution trace, and then considers three relations between
the seed and target sequences for energy scheduling, namely
s’s sequence coverage (seqCov), ts’ priority (priority) and
global maximum coverage (gM axCov). Speciﬁcally, MES as-
signs more energy to seeds with high seqCov, high priority,
low gM axCov, and vice versa. In this way, MES enables our
fuzzer to reach as many targets as possible.

CEE uses a queue to store seeds which help to reach the
targets (directed seeds for short) and another queue to store
seeds that increase the code coverage (coverage seeds for
short). If the proportion of the coverage seeds in the total seeds
is too high (e.g., exceeds a threshold rate) when exploring,
our fuzzer switches to exploitation stage. If code coverage
information is insufﬁcient (e.g., the fuzzer does not generate
new directed seeds during a long period) when exploiting, our
fuzzer turns to the exploration stage. Moreover, CEE adjusts
the threshold rate dynamically by recording the duration
time and the number of generated directed seeds in each
exploitation stage, in order to coordinate the exploration and
exploitation stage adaptively.

Evaluation. We implemented the above techniques in a tool
named LeoFuzz and conducted extensive experiments with
seven real-world programs. Evaluation results demonstrate that
LeoFuzz is effective and efﬁcient on crash reproduction, true
positives veriﬁcation, and vulnerability discovery, compared to
six state-of-the-art fuzzers i.e., QSYM, AFLGo, Lolly, Berry,
Beacon and WindRanger. Contrary to intuition, running a
fuzzer with multiple targets in a single fuzzing campaign
is more efﬁcient than running multiple fuzzer instances in

Node
a
b
c

File
objdump.c
format.c
objdump.c

m
p

objdump.c
libbfd.c

Line
3682
234
3688

2508
271

......

Function
display object bfd
bfd check format matches
display object bfd

load speciﬁc debug section
bfd malloc

parallel with a target per instance. In addition, LeoFuzz found
in three real-world programs 23 new vulnerabilities and 11 of
which are assigned CVE IDs.

Contributions. The main contributions of this paper are as

follows:

• An adaptive stage coordination approach which steers the
fuzzer to switch between exploration stage and exploita-
tion stage dynamically;

• A novel energy scheduling strategy which considers more
relations between a seed and targets and hence enables
to reach multiple targets efﬁciently;

• A tool named LeoFuzz can expose and verify vulnerabil-
ities in real-world programs. We make LeoFuzz publicly
available1 to foster further research in the area;

• Extensive evaluation results show that LeoFuzz outper-
forms six state-of-the-art fuzzers, i.e., QSYM, AFLGo,
Lolly, Berry, Beacon and WindRanger, on crash reproduc-
tion and true positives veriﬁcation. Moreover, LeoFuzz
found in three real-world programs 23 new vulnerabilities
and 11 of which are assigned CVE IDs.

The rest of this article is structured as follows. Our motiva-
tion is described in section II. We present LeoFuzz’s overall
design in section III, static analysis in section IV, dynamic
analysis in section V, and its implementation in section VI.
Section VII presents the evaluation of LeoFuzz. We discuss
the related work in section IX, threats to validity in section
VIII and conclude in section X.

II. MOTIVATION

In this section, we use an example to discuss two limitations

of the existing DGF tools and introduce our approach.

Fig. 1 shows a part of the inter-procedural control ﬂow
graph (ICFG) of objdump program (V2.31). Each node in
the ﬁgure indicates a basic block, whose details are shown in
Table II. Two shadowed nodes, i.e., m and p, refer to an out-
of-memory vulnerability and an integer overﬂow vulnerability
respectively, i.e., CVE-2018-13033 and CVE-2018-20671. We
ran DGF tools, i.e., AFLGo, Lolly and Berry, with the loca-
tions of m and p as targets to trigger these bugs, and found
two problems in these tools.

Problem 1: unsuitable energy scheduling hinders cov-
ering multiple targets. Existing DGF tools use two en-
ergy scheduling strategies. AFLGo adapts Dijkstra algo-
rithm to schedule seeds, and Lolly or Berry exploits tar-
get sequence coverage in energy scheduling. When testing
the objdump program, AFLGo had an execution trace for

1https://github.com/hongliangliang/leofuzz

3

Fig. 1. A part of ICFG of objdump

each of three seeds, i.e., a-c-e-i-q-m, a-b-d-k-o and
a-b-d-f-g-j-k-p, respectively. It calculates the harmonic
distance (d, we label it aside the node in Fig. 1) between each
node in these paths to two targets. For example, the harmonic
distance of node a is da = 2/(1/5 + 1/4) = 40/9, where 5
and 4 is the length of the shortest path from a to target m and
p respectively, and 2 represents two targets that a can reach.
So the global distances of three seeds are daceiqm = (40/9 +
4+3+2+1)/5 = 2.89, dabdko = (40/9+3+2+1)/4 = 2.61,
dabdf gjkp = (40/9+3+2+4+3+2+1)/7 = 2.78, respectively.
AFLGo always selects the seed with the smallest global
distance, i.e., a-b-d-k-o here, though it is not reasonable
as the seed covers none of two targets. In fact, the other two
seeds, i.e., a-c-e-i-q-m and a-b-d-f-g-j-k-p, reach
the target m and p respectively, each of which should be
selected. Therefore, AFLGo would ignore the local optima
when seeking global optimal scheduling, thus reducing the
directedness of fuzzing.

By contrast, Lolly or Berry may fall into an easy local opti-
mum and thus never explore other deep targets. For example,
if the paths going through the target m are more difﬁcult to
explore (e.g., due to complex conditions) than those going
through the target p, the target sequence coverage of a seed
close to m would increase more slowly than that of another
seed close to p. So both tools would generate a large number
of seeds exploring the right branch of Fig. 1, and only a few
seeds covering the left branch of Fig. 1. They would continue
to explore the right branch of Fig. 1 even after reaching the
target p. As a result, it is difﬁcult for them to schedule a seed
close to the target m.

Problem 2: improper exploration-exploitation division.
Existing DGF tools switch between exploration and exploita-
tion stage in three ways. The ﬁrst one uses the seed selec-
tion strategy without considering the exploration-exploitation
switchover, like Lolly [15]. The generated coverage seeds or
directed seeds are placed at the end of a queue for sequential
scheduling. This method is simple but may take a long time to
mutate those directed seeds at (or near) the rear of the queue,
which slows down the reaching of targets. The second is the
static division strategy used by AFLGo [14] and RDFuzz [24],
which gives each stage a ﬁxed period. This strategy is not

Fig. 2. LeoFuzz’s architecture

ﬂexible and does not consider the runtime information at all.
The third is the exploitation-ﬁrst strategy used in Berry [16]. It
divides the seed queue into three priority levels, and directed
seeds have higher priority than coverage seeds. This strategy
doesn’t work well when the fuzzer has insufﬁcient code
coverage information, hence causing a lower quality of the
generated seeds.

Our approach. To solve the above problems, we propose
and implement two techniques in LeoFuzz. 1) a ﬁne-grained
energy scheduling strategy, which considers more relations
between seeds and targets, e.g., target sequence priority, tar-
get sequence coverage and global maximum coverage. Our
strategy can avoid ignoring the local optimum like AFLGo
and avoid falling into an easy local optimum like Lolly
or Berry. For instance, when seeds always have a higher
coverage with the target sequence of p than that of m, we
can know that target m is more difﬁcult to reach than target
p. Thus the global maximum coverage of the target m is
lower. Therefore, more energy is given to seeds with high
sequence coverage of target m (i.e., a-c-e-i-l). In this
way, LeoFuzz has more chances to explore the left branch
of Fig. 1, thus more likely to reach target m. 2) an adaptive
exploration and exploitation coordination approach, which is
based on two queues storing directed seeds and coverage seeds
respectively. LeoFuzz coordinates exploration and exploitation
stage ﬂexibly according to the ratio of seeds in two queues.

LeoFuzz also uses a concolic executor to solve difﬁcult
constraints, such as magic number, which enables LeoFuzz
to reach targets quickly. In addition, LeoFuzz combines call
graph (CG) and control ﬂow graph (CFG) to increase the
length of each target sequence, thus further improving Leo-
Fuzz’s guidance on reaching targets.

III. APPROACH

The architecture of LeoFuzz is shown in Fig. 2, which
includes two phases, i.e., static analysis and dynamic analysis.
In the static analysis phase, the graph extractor extracts a CG
and a set of CFG from the program under test (PUT). Then
the target sequence generator maps the statements in targets
to basic blocks of the graphs and generates for each target a

acbehilqd234340/9kfgjnop12234m1TargetsGraph extractorTarget sequence generatorTarget sequencesInstrumentorInstrumented binaryProgram under testState coordinatorSeed selectorMutatorExecutorConcolic executor   Initial seedsCrash queue Directed seeds queueCoverage seeds queueCGCFGstateseedinputFuzzerSeed queues12312341Algorithm 1 Target Sequence Generation
Input: Program P , Target T
Output: Target Sequence T S
1: TS = ∅;
2: cg = getCG(P);
3: domCg = CG-to-Dom(cg);
4: cfg = getCFG(P, T);
5: domCfg = CFG-to-Dom(cfg);
6: funName = getFunName(P, T);
7: seq1 = getNesNodesByCFG(domCfg, T);
8: seq2 = getNesNodesByCG(domCg, funName);
9: TS = TS ∪ seq2 ∪ seq1;

target sequence, which contains necessary basic blocks along
the paths to the target. Finally, the PUT is instrumented for
collecting runtime information, such as code coverage and
execution traces, and the instrumented binary is sent to the
executor.

In the dynamic analysis phase, the fuzzer takes the initial
seeds and the instrumented binary as inputs. First, the stage
coordinator judges the stage of the fuzzer (exploration stage or
exploitation stage). According to the stage, the seed selector
then obtains a seed from the corresponding seeds queue,
i.e., coverage seeds queue (CQ for short) or directed seeds
queue (DQ for short). After the mutator mutates the seed, the
generated input is fed to the executor. The input is stored
into the crash queue if it crashes the PUT, or into DQ
if it increases the target sequence coverage, or into CQ if
it increases code coverage, otherwise discarded. The fuzzer
communicates with the concolic executor by sharing two seed
queues. The concolic executor helps LeoFuzz focus on the
paths going through the targets and explore more branches thus
obtaining better code coverage. The concolic executor obtains
seeds from two queues and stores its generated directed inputs
or coverage inputs into the corresponding queue.

IV. STATIC ANALYSIS

A. Generating Target Sequences

Given a target, a target sequence is composed of a set of
nodes, and each node is a necessary basic block which exists
on all paths from the entry function (e.g., main) to the target.
To make the fuzzer have better guidance, LeoFuzz combines
CG and CFG of the PUT to enhance the target sequences,
unlike Lolly and Berry, which only use CFG to generate target
sequences.

As shown in Algorithm 1, we generate the target sequence
for a target based on Dominator Tree2 [36]. It takes a program
P and a target T as inputs, and outputs the target sequence T S
which is initially empty. The algorithm ﬁrst obtains CG from
P and converts it to the dominator tree (i.e., domCg) (lines
2-3). Then we get CFG and convert it to the dominator tree
domCf g and get the target function name f unN ame (lines
4-6). Finally, we get the necessary nodes from domCf g and
domCg respectively, and add them to T S (lines 7-9).

2A tree is called a dominating tree if each node in the tree dominates only
itself and its descendants. A node d dominates a node n if and only if each
path that goes through n must go through d ﬁrst. Each node dominates itself
by deﬁnition.

4

Fig. 3. Example: Construct target sequence based on CFG and CG

We use an example to illustrate the algorithm, as shown in
ﬁgure 3. The ﬁgure shows the CG of a program P and the CFG
of function G which contains a target g, as well as the dominat-
ing trees domCg and domCf g. The blue nodes in the ﬁgure
represent the necessary nodes to reach the target g. Using
Algorithm 1, we can know that main1-A1-entry-a-f-g
is the target sequence of the target g, where main1 and A1
represents the entry node of main and A function, respectively.

B. Static Instrumentation

Like Lolly, LeoFuzz instruments the basic blocks each
least a target statement, and uses a
of which contains at
shared memory to sequentially record identiﬁers of the blocks
following the order in which they are executed (i.e., execution
trace). As a result, the fuzzer can collect the code coverage
information and execution traces related to targets during ex-
ecution. These runtime information assists LeoFuzz in energy
scheduling and exploration-exploitation coordination.

V. DYNAMIC ANALYSIS

During the dynamic analysis stage,

the fuzzer and the
concolic executor independently executes the program under
test while they help each other by sharing the coverage seed
queue (CQ) and the directed seed queue (DQ).

A. Fuzzer

The fuzzer in LeoFuzz works like other DGF tools though
we enhance it with two novel techniques. We propose a novel
approach to adaptively coordinate exploration and exploitation
stages (CEE for short) based on two queues and a novel energy
scheduling strategy that considers more relations between
seeds and targets (MES for short).

entrycabedfmain1gexitmainGABDECFA1entryafCFG of Function GCGTarget SequencemainGABDECFdomCgentrycabedfgexitdomCfg of Function GgTABLE III
DESCRIPTION OF SYMBOLS IN ALGORITHM 2 & 3.

Symbol
sof
dsc
csc
ndc

cdsc

rate

epoch
lndc, slndc

Description
stage of Fuzzer (0 for exploration, 1 for exploitation)
number of seeds in directed seed queue
number of seeds in coverage seed queue
number of consecutive executions in current exploitation
stage during each of which no directed seed is generated
number of directed seeds generated in current exploita-
tion stage
control coefﬁcient to switch Fuzzer from exploration to
exploitation stage
frequency that Fuzzer switches to exploitation stage
values of ndc in the last two epochs

Algorithm 2 Multiple Targets Directed Grey-box Fuzzing
Input: Instrumented Binary P , Target Sequence Set T SS, Coverage

seed queue CQ, Directed Seed queue DQ,

else

s = getN extSeed(DQ);

s = getN extSeed(CQ);

sof = stageCoord(sof );
if sof == 0 then

end if
p = assignEnergy(s);
for i from 1 to p do
s(cid:48) = mutate(s);
execute(P, s(cid:48));
if s(cid:48) crashes P then
add s(cid:48) to CS

Output: Crash seeds CS
1: sof = 0; epoch = 0; dsc = 0; csc = 0; ndc = 0; cdsc = 0;
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29: until timeout or abort

tseqCov = isIncreaseT SeqCov(s(cid:48), T SS);
codeCov = isIncreaseCodeCov(s(cid:48));
if tseqCov then
add s(cid:48) to DQ;
dsc++; cdsc++; ndc = 0; continue;

end if
if codeCov then
add s(cid:48) to CQ;
csc++; ndc++; continue;

end if
end for

end if
ndc++;

else

The fuzzer’s workﬂow is shown in Algorithm 2. Its inputs
are the instrumented binary P , coverage seed queue CQ,
directed seed queue DQ and target sequence set T SS, and
the output is a set of crash seeds CS. Table III describes the
meaning of each symbol/variable used in Algorithm 2 and 3.
After initialization, the fuzzer decides its stage using CEE
Algorithm 3 and accordingly selects a seed s from CQ or DQ
respectively (lines 3-8). It then assigns energy p to the seed
using MES strategy. In the energy loop, the fuzzer generates a
new input s(cid:48) via mutation, and executes P with s(cid:48). If s(cid:48) causes
the program crash, increases code coverage or target sequence
coverage, the fuzzer stores it in CS, CQ or DQ respectively
(lines 13-25). Note that if s(cid:48) increase both code coverage and
target sequence coverage, we store it in DQ since directed
seeds are usually less than coverage seeds.

5

end if

// switch to exploitation

if csc/(csc + dsc) > rate then

sof = 1;
ndc = 0;
cdsc = 0;
epoch++;

Algorithm 3 Coordinating Exploration and Exploitation Stage
function stageCoord(sof)
1: if sof == 0 then
2:
3:
4:
5:
6:
7:
8: else
9:
10:
11:
12:
13:
14:
15:
16: end if
return sof

th = 1/2 ∗ (slndc + lndc) ∗
if ndc >= th then
slndc = lndc;
lndc = ndc;
updateRate(rate, cdsc, epoch);
sof = 0;

// update ndc of the second last epoch
// update ndc of the last epoch

// switch to exploration

epoch;

end if

√

1) Exploration-exploitation Coordination: CEE mechanism
is shown in Algorithm 3. The fuzzer starts in the exploration
stage initially and will switch to exploitation stage when the
ratio of coverage seeds (csc) in total seeds is greater than a
dynamic threshold rate, which means that fuzzer has adequate
code coverage information, so we set sof as exploitation stage,
and updates the related data, e.g., ndc, cdsc, epoch (lines 1-
7). Note that we set the initial value of dsc as 10 at the
ﬁrst exploration stage to prevent the fuzzer from immediately
switching to exploitation.

In exploitation stage, we record related runtime information,
e.g., cdsc and ndc, as shown in Algorithm 2. The fuzzer will
switch to exploration stage when ndc exceeds a threshold th,
which indicates that the fuzzer’s exploitation ability is weak
now. Therefore we set sof as exploration stage in order to
explore more code paths, and update the coefﬁcient rate (lines
8-16). The threshold th is calculated by the values of ndc in
last two epochs and epoch (line 9). Because the probability
of ﬁnding a new directed seed decreases gradually during
fuzzing, we increase th at each epoch to keep the fuzzer in
exploitation stage longer.

The rate is used to decide when the fuzzer switches from
the exploration stage to the exploitation stage, so we update its
value by using run-time information in the current exploitation
stage (e.g., cdsc) only before the fuzzer will leave for the
exploration stage. Obviously, the greater the rate is, the longer
the fuzzer stays in exploration, and vise versa. To balance
the fuzzer’s code coverage and directed exploitation, we use
function updateRate to adjust rate according to epoch and
cdsc, as follows:

rate∗ = rate − γ(tanh(

cdsc
√
t

∗ (cid:112)epoch) − δ)

(1)

where rate∗ indicts the new value of rate for use in next
exploration stage, tanh() is a hyperbolic function, t represents
the time duration (seconds) of current exploitation stage.
(Note: the upper and lower bound of rate is set to 1 and
0, respectively.)
Speciﬁcally,

the larger cdsc√
is, which means that more
t
directed seeds are produced in exploitation stage and that
current code coverage is helpful in reaching targets, the less

time the fuzzer would use to reach targets and should switch
to exploitation stage as soon as possible. As such, we reduce
rate to make the fuzzer enter exploitation stage fast. On the
contrary, the smaller cdsc√
is, which indicates that current code
t
coverage does not help reach targets, the more exploration
time is needed to get more coverage information, therefore we
increase rate to keep the fuzzer exploring longer. In addition,
the probability of ﬁnding a new directed seed decreases
gradually during fuzzing, resulting in a smaller cdsc√
and a
t
greater rate over time. Therefore, we use the parameter epoch
to offset this effect so that rate changes reasonably.

2) Seed Energy Scheduling for Multiple Targets: To balance
the energy scheduling for multiple targets, we propose a novel
energy scheduling strategy (MES for short) that considers
more relations between seeds and targets.

Speciﬁcally, for a seed s and multiple targets, LeoFuzz
generates all target sequences (e.g., N in total) at the static
analysis phase (Section IV) and obtains the seed’s execution
trace during fuzzing. We consider three relations between the
seed and target sequences as follows.

• The priority of a target sequence T Si, which indicts T Si’s
similarity with other sequences and is computed at static
analysis phase as follows:

priorityi =

N
(cid:88)

j=1

(cid:16) LCS(T Si, T Sj)
M ax(T Si, T Sj)

(cid:17)

≥ (cid:15) ?1 : 0

, j (cid:54)= i

(2)
where LCS() indicts the length of the longest common
subsequence of two sequences, M ax() returns the max-
imum length of two sequences. The higher the priority,
the greater the chance that the fuzzer can reach multiple
targets by mutating s, so s should be assigned more
energy.

• The global maximum coverage gM axCovi of a target
sequence T Si. It refers to the maximum coverage of any
execution trace in the past over the target sequence, which
approximates T Si’s difﬁculty to cover and is updated
during dynamic analysis. (In fact, the gM axCov used for
the next seed energy scheduling is obtained by calculating
the maximum value of the seqCov of the current seed and
the current gM axCov.) The less gM axCovi is, T Si is
more difﬁcult to be covered, the target corresponding to
T Si is more difﬁcult to reach, and thus the seed should
be assigned more energy.

• The seed’s sequence coverage (seqCov) over a target
sequence (T Si). It measures the similarity between the
seed’s execution trace (ET) and T Si, and is calculated
during dynamic analysis as follows:

seqCovi =

LCS(ET, T Si)
lengh(T Si)

(3)

where LCS() gets the length of the longest common
subsequence between s’ execution trace and T Si. The
greater the seqCovi, the more likely that the fuzzer will
cover T Si by mutating s, so s should be assigned more
energy.

6

For a seed and multiple targets, MES selects the target
sequence3 with the maximum value of seqCov as the seed’s
outstanding target sequence (OT S for short), and considers
OT S’ priority, gM axCov and the seed’s seqCov over OT S
when scheduling energy for the seed. In this way, LeoFuzz can
provide a ﬁne-grained energy scheduling and thus effectively
improve the ability of DGF to cover multiple targets.

Below we show how to calculate these values by using an
example. Given three target sequences, T S1: a-b-c-d-f-g,
T S2: a-b-c-g-h, T S3: a-g-i-k, we ﬁrst calculate the
priority of each target sequence as follows. LCS(T S1,T S2)
and LCS(T S1,T S3) is 3 and 1 respectively, Max(T S1,T S2)
and Max(T S1,T S3) are 6, so the priority of T S1 is 1 according
to equation (2). Similarly the priority of T S2 and T S3 is 1
and 0 respectively. Suppose that seed s’ execution trace (ET)
is a-b-c-g-k-m-d, we can get LCS(ET ,T S1) is 3, and
s’ sequence coverage over T S1 is 3/6 = 0.5 according to
equation (2). Similarly that of s over T S2, T S3 is 0.8 and 0.25
respectively. Therefore, s’ sequence coverage is 0.8, and s’
OT S is T S2. Assuming that three seeds in total were executed
in the past and their coverage with T S2 is 0.3, 0.5 and 0.4
respectively, then the global maximum coverage of T S2 is 0.5.
It is arduous to judge which target is difﬁcult to reach when
the fuzzer has insufﬁcient code coverage, especially in initial
executions. Therefore, we won’t consider global maximum
coverage of the target sequence in energy scheduling until the
fuzzer has sufﬁcient code coverage. For example, when the
number of target sequences whose global maximum coverage
is greater than or equal to a threshold (β) exceeds half of total
sequences, the fuzzer likely has covered shallow (or easy)
targets, it is reasonable to consider those deep (or difﬁcult)
targets. Therefore, those seeds corresponding to these targets
will be allocated less energy and other targets have more
opportunities to be explored.

We use a comprehensive factor (CF) to represent the above

relations:

CF =





1
2 (seqCov + priority/N ),

if (cid:80)N

j=1 (gM axCovj ≥ β ? 1 : 0) < 1

2 N
1
3 (seqCov + priority/N + (1 − gM axCov)),
2 N

j=1 (gM axCovj ≥ β ? 1 : 0) ≥ 1

if (cid:80)N

(4)

where N is the number of total sequences.

AFLGo uses an energy scheduling scheme based on sim-
ulated annealing in the greybox fuzzing. Different from the
traditional random walk algorithm which may be trapped in
a local optimum, the simulated annealing algorithm accepts a
solution which is worse than the current one with a certain
probability, so it can jump out of the local optimum and reach
the global optimum. This probability gradually decreases as
the control parameter temperature decreases.

Like AFLGo, LeoFuzz also applies simulated annealing to
our energy scheduling for a global optimum and uses the same
coefﬁcient values as AFLGo in the following equations. For
multiple targets directed fuzzing, an optimal solution is a test
case that can achieve the maximum CF. In our method, the

3If there are multiple ones, the ﬁrst is used.

temperature T with an initial value T0= 1 is exponentially
cooled.

TABLE IV
SUBJECT PROGRAMS FOR CRASH REPRODUCTION. IFT: INPUT FILE TYPE

7

T = T0 × αk

(5)

Package

where, α is a constant which meets 0.8 ≤ α ≤ 0.99, and
k is the temperature cycle. The threshold of Tk is set to
0.05. The fuzzer will not accept worse solutions when the
temperature is lower than Tk. Speciﬁcally, if Tk > 0.05,
LeoFuzz randomly mutates the existing seeds to generate many
new inputs. Otherwise, it generates more new inputs from
seeds with higher CF. In this case, the simulated annealing
process is similar to the traditional gradient descent algorithm.
Since a common limitation of fuzzing is the time budget,

we use time t to adjust the temperature cycle k:

k
kx

=

t
tx

(6)

where kx and tx are the temperature cycle and the time
respectively when temperature drops to Tk. So we use k to
establish the relationship between time t and temperature T :
Tk = 0.05 = αkx

(7)

T = αk = αt/tx×log(0.05)/ log (α) = 20−t/tx

(8)

Given a seed s, multiple targets, and their comprehensive
factor (CF), we deﬁne the capability of s to cover the given
the multiple targets as:

Cap = CF × (1 − T ) + 0.5 × T

(9)

At the beginning of fuzzing, the initial value of temperature
T is 1, which means that Cap is independent of CF . As
time goes on, the temperature T gradually decreases and CF
becomes increasingly important.

To combine our MES strategy with the existing seed energy
schedule algorithm of a fuzzer (e.g., AFL), LeoFuzz integrates
the capability of covering multiple targets (Cap) into the
energy calculation formula. LeoFuzz calculates the integrated
energy for a seed as:

M energy = energy × 2.0(Cap−0.2)×10

(10)

where energy is the original energy given by AFL and
M energy is the energy given by LeoFuzz which integrates
MES strategy in the original fuzzer.

B. Concolic Executor

The fuzzer leverages a random mutation to generate test
inputs without considering the context of the PUT and thus it
has difﬁculty to reach deep targets and ﬁnd deep errors along
complex paths [25], [26], [32], [33]. Therefore, we combine
the fuzzer with the concolic executor to solve this problem.
The concolic executor continuously obtains seeds from two
seed queues, executes them, and generates new inputs, which
are then put into CQ or DQ if they bring new code coverage
or new target sequence coverage. To generate directed seeds
as quickly as possible so that LeoFuzz can reach the targets
faster, the concolic executor prefers to acquire seeds from DQ
ﬁrst and then seeds from CQ if there are no available directed
seeds.

binutils [38]

libtiff
[39]
libredwg[40]
zziplib [41]
[42]
mjs

Program
cxxﬁlt
objdump

readelf
tiff2pdf
dwg2dxf
unzzipcat-mem
mjs

IFT
ELF
ELF

ELF
TIFF
DWG
ZIP
JS

#CVE
5
4

2
2
10
6
2

VI. IMPLEMENTATION

Arguments

–dwarf-check -C -g
-f -dwarf -x @@
-zR3 @@
@@
@@ -o /dev/null
@@

Static analysis: We wrote an LLVM pass which builds
call graph (CG) for the program under test and control ﬂow
graph (CFG) for each of its functions. The dominator tree
is constructed for each CFG and CG by NetworkX, and the
necessary nodes are calculated to obtain the target sequence.
We modiﬁed the AFL’s instrumentation pass which writes
the IDs of basic blocks in a target sequence into the shared
memory, in order to record the target sequence’s execution
trace.

Dynamic analysis: We implemented our exploration-
exploitation coordination strategy and energy scheduling strat-
egy in AFL. In addition, we modiﬁed the concolic executor
in QSYM to leverage guidance from the directed seeds, and
hence LeoFuzz can reach the targets faster.

VII. EVALUATION

In this section, we ﬁrst evaluate LeoFuzz’s effectiveness
and efﬁciency in terms of crash reproduction, true positive
veriﬁcation and vulnerability exposure in real-world programs,
and compare the performance difference of running a LeoFuzz
instance given multiple targets vs. running in parallel multiple
LeoFuzz instances each of which aims to reach a target. Then
we evaluate the contributions of four main design decisions
in LeoFuzz, i.e., target sequence enhancement, exploration-
exploitation coordination, ﬁne-grained energy scheduling strat-
egy and concolic execution, respectively.

A. Experiment Setup

We conducted all experiments on a virtual machine with an
Intel(R) Xeon(R) Gold 6126 CPU, 128GB RAM and Ubuntu
18.04 (64 bit) as operating system. To evaluate the effec-
tiveness and efﬁciency of LeoFuzz, we compare it with six
state-of-the-art fuzzers, AFLGo, Lolly, Berry, QSYM, Beacon
and WindRanger. We evaluated them with the same programs
under test, initial input corpus, target locations and time budget
(12 hours). Since Beacon cannot support multiple targets, we
ran it and compared it with LeoFuzz in experiments with single
target. Note that Hawkeye [19], RDFuzz [24] and CAFL [31]
are not publicly available, so LeoFuzz doesn’t compare with
them. LeoFuzz was not compared to ParmeSan [22] because
we could not replicate its experiments successfully even after
we asked its authors for help.

We leverage seven real-world programs shown in Table
IV for all experiments except clearly stated, because these

programs are widely used, extensively evaluated by fuzzing
tools in both academia [17], [19], [34] and industry [35], and
found vulnerable due to multiple bugs. For these programs,
we collected 31 vulnerabilities and corresponding arguments
from CVE database or their ofﬁcial sites and took each
vulnerability’s crash site as a target in experiments, i.e., A
unique target location corresponds to a unique vulnerability.
Furthermore, we repeated all experiments 10 times and used
the average values.

In the experiments, we aim to answer the following ques-

tions:
RQ1 How does LeoFuzz perform when it is given a single

target each time?

RQ2 Is LeoFuzz effective and efﬁcient in crash reproduction?
RQ3 How does running LeoFuzz with multiple targets com-
pared to running multiple LeoFuzz instances with one
target per instance?

RQ4 Is LeoFuzz efﬁcient in terms of true positives veriﬁca-

tion?

RQ5 Is LeoFuzz effective to discover vulnerabilities in real-

world software?

RQ6 How do four main design decisions contribute to Leo-

Fuzz?

B. Crash reproduction

Software programs may crash due to potential bugs or
vulnerabilities. A crash report usually contains memory dumps
or call stacks of the program. Based on it, developers need to
generate test cases that trigger the crash, i.e., reproduce the
crash. Directed fuzzing technique is demonstrated effective on
crash reproduction [14]–[16].

Using 31 vulnerabilities from seven real-world programs
shown in Table IV, we evaluate LeoFuzz’s ability on crash
reproduction. We conduct experiments using two settings:
1) we run LeoFuzz with a single target (i.e., RQ1), in this
case, we name it LeoFuzzs for convenience, and 2) we run
LeoFuzz with multiple targets (RQ2), comparing to baseline
tools, respectively. Then we compare the difference between
two settings of LeoFuzz (i.e., RQ3).

1) RQ1: Performance of LeoFuzz when given a single
target: The experimental results for RQ1 are shown in Table
V. The ﬁrst column is the program under test. The second
and third column is the vulnerability’s identiﬁcation and type
respectively. Time-to-Exposure (TTE) measures the length of
the fuzzing campaign until the ﬁrst test input is generated
that exposes a given vulnerability. Factor measures the perfor-
mance gain as the mean TTE of each baseline tool divided
by that of LeoFuzzs. Values of factor greater than one indict
that LeoFuzzs performs better than the corresponding baseline
tool. Note that when a vulnerability is not found in the time
budget (i.e., 12 hours), we use the time budget to calculate
the factor. The fourth column to the thirteenth column are the
mean of TTE and factor of QSYM, AFLGo, Lolly, Berry and
Beacon in 10 experiments, respectively. The last column is the
mean of TTE of LeoFuzzs.

As shown in Table V, LeoFuzzs can reproduce each crash
while Beacon, QSYM, AFLGo, Lolly and Berry fails on 10, 8,

8

3, 2 vulnerabilities and 1 vulnerability, respectively. Moreover,
LeoFuzzers is faster than all baseline tools, i.e., 7.12× than
QSYM, 2.92× than AFLGo, 2.83× than Lolly, 2.21× than
Berry and 5.46× than Beacon, respectively.

2) RQ2: Effectiveness and efﬁciency of LeoFuzz when given
multiple targets: In this setting, we evaluate LeoFuzz’s capa-
bility to deal with multiple targets. The baseline tools are also
run with the same programs and targets as LeoFuzz. Table
VI describes the experimental results. The ﬁrst column is the
program under test and the second column lists the fuzzing
tools. The third column is the number of targets that are
given to the tools, The fourth column is the number of bugs
reproduced by each tool. The ﬁfth column is the number of
additional bugs discovered by each tool. The sixth column is
the mean TTE spent by each tool to trigger all speciﬁed bugs.
The Vargha-Delaney statistic (A12) is a standard measure for
evaluating randomized algorithms [51]. Given a performance
measure M (e.g., TTE ),
the A12 statistic measures the
probability that running LeoFuzz yields higher M values than
running baseline tools, indicating the conﬁdence that LeoFuzz
performs better than the baseline tools.

As shown in Table VI, when given multiple targets, LeoFuzz
can trigger all bugs within 4 hours, while QSYM, AFLGo,
Lolly, Berry and WindRanger failed to trigger 10, 7, 7, 5 and 9
bugs within 12 hours, respectively. Moreover, LeoFuzz is able
to discover more unique bugs in all programs than the baseline
tools. In addition, LeoFuzzer is faster than all baseline tools,
i.e., 7.35× than QSYM, 4.63× than AFLGo, 4.61× than Lolly,
3.45× than Berry and 4.46× than WindRanger, respectively.
Furthermore, LeoFuzz performs better with 92%, 90%, 90%,
86% and 91% conﬁdence on average than QSYM, AFLGo,
Lolly, Berry and WindRanger, respectively.

3) RQ3: One LeoFuzz with multiple targets versus multiple
LeoFuzzs with a target per instance: As shown in the previous
sections, both LeoFuzz and LeoFuzzs are effective in crash re-
production. Then a question arises naturally; running LeoFuzz
with multiple targets or running multiple LeoFuzz instances
with one target per instance, which one is more efﬁcient?
We explore the question in this section and the experimental
results are shown in Table VII. The third and fourth column
is the sum and the longest of TTE spent by each LeoFuzzs
instance to trigger a given vulnerability respectively. The ﬁfth
column measures the vulnerabilities triggered by LeoFuzz, and
the sixth column is the TTE spent by LeoFuzz to trigger all
vulnerabilities in each program.

As shown in Table VII, both one LeoFuzz instance and
multiple LeoFuzzs instances can trigger all vulnerabilities,
however,
the time cost of LeoFuzz is less than the total
time spent by each LeoFuzzs
time and even the longest
instance. For example, LeoFuzz spent 56m11s when triggering
both CVE-2017-7209 and CVE-2019-14444 in readelf
program, while LeoFuzzs took 1h12m when triggering CVE-
2019-14444 only (see Table V). The efﬁciency of LeoFuzz
may beneﬁt from the fact
that real-world programs often
have multiple bugs which are usually dependent or related,
e.g., caused by the same bad programming practices. In fact,
we observed that the call stack in crash dump caused by CVE-
2017-7209 has a large overlap with that caused by CVE-2019-

TABLE V
RESULTS OF LEOFUZZ AND BASELINE TOOLS WHEN GIVEN A SINGLE TARGET. UAF=USE-AFTER-FREE, IO=INTEGER OVERFLOW, BOF=BUFFER
OVERFLOW, SOF=STACK OVERFLOW, NP=NULL POINT EXCEPTION, ML=MEMORY LEAK, AE=ARITHMETIC ERROR, OR=OUT-OF-BOUNDS READ,
IR=INVALID MEMORY READ

9

Program

CVE-ID

Type

QSYM

AFLGo

Lolly

Berry

Beacon

2016-4487
2016-4489
2016-4490
2016-4491
2016-4492
2018-17985
2018-20671
2018-9138
2019-9070
2017-7209
2019-14444
2018-15209
2018-16335
2019-9770
2019-9771
2019-9772
2019-9773
2019-9774
2019-9775
2019-9776
2019-9777
2019-9778
2019-9779
2017-5974
2017-5975
2017-5976
2017-5977
2017-5978
2017-5980
issues-59
issues-136

cxxﬁlt

objdump

readelf

tiff2pdf

dwg2dxf

unzzipcat
-mem

mjs

TTE
2m54s
3m53s
50s
1h23m
2m15s

UAF
IO
IO
BOF
BOF
SOF —
BOF —
SOF —
SOF —
NP
IO
BOF —
BOF —
BOF
NP
NP
BOF
OR
OR
NP
BOF
BOF
NP
BOF —
BOF —
BOF
IR
OR
NP
AE
SOF

6h0m
9h37m

8h7m
3h38m
—
7h57m
32m17s
2h2m
—
41s
33m32s
7h40m

3h57m
3h45m
2h57m
8m17s
3h50m
2h38m

Factor
2.32
2.04
1.56
1.69
1.07
3.43 —
1.89
4.11 —
5.14
4.53
8.01

TTE
2m57s
2m12s
50s
1h46m
4m41s

9h44m

6h2m
2h14m
4h22m

11.80 —

7.06
1.90
17.37
19.73
1.82
6.07
3.25
15.47
1.46
1.22
12.33
4.31
18.32
11.67
10.09
6.44
2.49
15.33
15.05

4h14m
8h24m
31m41s
3h26m
8h35m
10m10s
6h5m
1h53m
49s
42m57s
59m47s
7h36m
1h43m
38m50s
33m26s
1h23m
6m17s
34m1s
10m29s

TTE
3m12s
3m21s
49s
1h43m
5m17s
10h17m

Factor
2.36
1.16
1.56
2.16
2.23
3.43
1.54 —
4.11 —
2.60
2.53
3.64
11.80 —
2.49
1.97
2.53
5.64
1.97
1.91
9.72
2.43
1.75
1.57
1.60
2.73
2.62
1.91
1.34
3.02
1.89
2.24
1.00

6h53m
3h4m
3h51m

5h23m
9h11m
35m11s
2h57m
9h17m
7m26s
5h1m
2h12m
47s
47m31s
53m24s
8h36m
1h16m
41m22s
37m43s
1h8m
6m9s
30m12s
12m14s

TTE
2m19s
2m27s
42s
1h19m
4m52s
7h41m
7h27m

Factor
2.56
1.76
1.53
2.08
2.52
2.95
1.89
4.11 —
2.97
3.50
3.21
11.80
3.17
2.15
2.81
4.85
2.13
1.40
8.02
2.84
1.68
1.73
1.41
3.09
1.93
2.04
1.39
2.47
1.85
1.99
1.18

4h47m
1h56m
3h12m
4h43m
4h13m
8h19m
27m49s
2h19m
8h13m
7m2s
3h32m
1h43m
33s
39m32s
43m44s
7h42m
1h3m
26m49s
31m4s
57m54s
6m11s
31m54s
11m3s

TTE
2m50s
2m51s
34s
1h26m
3m55s
4h26m

2h59m
2h39m

Factor
1.86
1.29
1.31
1.59
2.32
2.21
1.18 —
4.11
2.06
2.21 —
2.67 —
4.64 —
2.48 —
1.95 —
2.22
3.81 —
2.04 —
1.32
5.65
2.21
1.18
1.44
1.17
2.77 —
1.60 —
1.32
1.14
2.11
1.86
2.10
1.06

28m18s

22m37s
5h45m
49m21s
31s
31m9s
41m27s

7h58m
7h24m
29m28s
5m27s
21m13s
11m22s

Factor
2.27
1.50
1.06
1.73
1.87
1.27
1.89
1.01
1.14
13.64
10.00
11.80
7.06
2.81
2.25
19.73
2.75
4.26
9.18
1.06
1.11
1.14
1.10
4.31
18.32
23.55
16.26
1.07
1.64
1.39
1.09

LeoFuzzs
TTE
1m15s
1m54s
32s
49m36s
2m6s
3h29m
6h18m
2h55m
2h19m
52m48s
1h12m
1h1m
1h42m
4h16m
12m33s
36m29s
4h22m
5m19s
37m35s
46m33s
28s
27m22s
37m18s
2h47m
39m16s
20m18s
27m18s
27m31s
3m19s
15m12s
10m27s

14444, in other words, the paths to reach both vulnerabilities
go through much same functions, which helps LeoFuzz trigger
both of them fast.

C. RQ4: Efﬁciency on True Positives Veriﬁcation

Developers and testers usually apply static analysis tools
to discover bugs or vulnerabilities in software before release.
However, static analysis tools often have high false positive,
and thus require a lot of manual efforts to verify their analysis
results. Due to its directed execution feature, the DGF tech-
nique has been used for automatic veriﬁcation of bugs [14]–
[16]. Moreover, Lolly and Berry outperformed over AFLGo
due to their sequence coverage approach [15], [16].

We evaluated LeoFuzz’s ability on true positive veriﬁcation
and compared it with QSYM, AFLGo, Lolly and Berry. We
use the same subject program, i.e., Libming 0.4.8 [43], as
AFLGo, Lolly and Berry. In addition, we run the Clang Static
Analyzer [37] on the subject program and use its analysis
results as targets, i.e., the code locations of potential bugs. In
the experiments, the analysis results of the Clang analyzer are
not intentionally ﬁltered and therefore may contain false pos-
itives and infeasible paths. In order to evaluate the efﬁciency

of LeoFuzz and four baseline tools, we guide them with the
above targets to trigger CVE vulnerabilities of Libming and
compare their time cost. The CVE vulnerabilities are listed in
the ﬁrst column of table VIII.

Table VIII presents the experimental results. The second to
the ninth column is the mean of TTE and factor of QSYM,
AFLGo, Lolly and Berry in ten runs, respectively. In particular,
if a tool fails to trigger a vulnerability in a run within the time
limit, its TTE is uniformly recorded as the time budget (i.e., 12
hours). The last column is the mean TTE of LeoFuzz.

As shown in Table VIII, ﬁve tools successfully generated
inputs that can trigger the vulnerabilities, while LeoFuzz is
7.00× faster than QSYM, 4.45× than AFLGo, 3.33× than
Lolly and 2.91× than Berry, respectively. Experimental results
show that LeoFuzz is effective in true positives veriﬁcation and
more efﬁcient than baseline tools.

D. RQ5: Effectiveness on Vulnerabilities Exposure

To evaluate LeoFuzz’s ability exposing bugs or vulnera-
bilities in real-world programs, we tested three widely used
software with their latest versions, i.e., cxxﬁlt 2.36 , SWFTools
a9d5082 [44] and libredwg 0.12.4.4608 [40]. Cxxﬁlt is a tool

TABLE VI
RESULTS OF LEOFUZZ AND BASELINE TOOLS WHEN GIVEN MULTIPLE
TARGETS

Prog.

cxxﬁlt

Tool
QSYM

AFLGo

Lolly

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

objdump Lolly

readelf

tiff2pdf

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

Lolly

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

Lolly

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

dwg2dxf Lolly

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

unzzipc

atmem

Lolly

Berry

WindRanger

LeoFuzz

QSYM

AFLGo

Lolly

Berry

WindRanger

LeoFuzz

mjs

Tgt.
5
5
5
5
5
5
4
4
4
4
4
4
2
2
2
2
2
2
2
2
2
2
2
2
10
10
10
10
10
10
6
6
6
6
6
6
2
2
2
2
2
2

Rep.
5
5
5
5
5
5
0
3
3
3
1
4
2
2
2
2
2
2
0
0
1
2
1
2
8
7
7
7
7
10
4
5
4
5
4
6
2
2
2
2
2
2

Add.
1
1
1
2
2
3
1
1
2
3
0
5
1
1
1
1
0
1
0
1
1
0
0
1
6
3
3
4
6
6
1
2
2
2
2
2
1
1
2
4
1
4

TTE
1h23m
1h54m
1h37m
1h21m
39m50s
46m22s
—
—
—
—
—
4h28m
9h37m
5h21m
5h49m
3h17m
4h11m
56m11s
—
—
—
4h23m
—
1h17m
—
—
—
—
—
3h51m
—
—
—
—
—
1h43m
3h50m
25m42s
21m52s
24m11s
47m17s
13m27s

Factor A12
0.59
0.54
0.62
0.55
0.41
—
1.00
1.00
1.00
1.00
1.00
—
0.83
0.95
0.94
0.87
0.96
—
1.00
1.00
1.00
0.81
1.00
—
1.00
1.00
1.00
1.00
1.00
—
1.00
1.00
1.00
1.00
1.00
—
1.00
0.81
0.75
0.80
0.98
—

1.79
2.46
2.09
1.75
0.86
—
2.90
2.90
2.90
2.90
2.90
—
10.27
5.71
6.21
3.51
4.47
—
9.35
9.35
9.35
4.06
9.35
—
3.12
3.12
3.12
3.12
3.12
—
6.99
6.99
6.99
6.99
6.99
—
17.10
1.91
1.62
1.80
3.51
—

in Binutils, which decodes low-level names into user-level
names to be human readable. SWFTools is a collection of
utilities for working with Adobe Flash ﬁles. LibreDWG is a
free C library to read and write DWG ﬁles. The targets in this
experiment come from the results of Clang static analyzer [37]
or the patches of the corresponding program under test, and
LeoFuzz aims to explore towards the potentially buggy code.
As a result, LeoFuzz found 23 previously unreported vul-
nerabilities, 11 of which are assigned CVE IDs and others
have been conﬁrmed by the corresponding developers. Table
IX presents the subject program, the buggy method, the type
and CVE/Bug ID of each vulnerability. Seven of eleven CVEs
are assigned high severity score. Below we discuss one of them
in detail to highlight the ability of LeoFuzz.

LeoFuzz found a use-after-free vulnerability in SWFTools

10

TABLE VII
ONE LEOFUZZ VERSUS MULTIPLE LEOFUZZs INSTANCES

Program

Tgt.

cxxﬁlt
objdump
readelf
tiff2pdf
dwg2dxf
unzzipcat-mem
mjs

5
4
2
2
10
6
2

LeoFuzzs
Total
TTE
55m23s
15h1m
2h4m
2h43m
13h1m
4h25m
25m39s

LeoFuzzs
Longest
TTE
49m36s
6h18m
1h12m
1h42m
4h22m
2h47m
15m12s

Rep.

LeoFuzz
TTE

5
4
2
2
10
6
2

46m22s
4h28m
56m11s
1h17m
3h51m
1h43m
13m27s

static int swf FontExtract DeﬁneTextCallback(int id, SWFFONT * f,
TAG * t, int jobs, void (*callback) (void *self, int *chars, int
*xpos, int nr, int fontid, int fontsize, int xstart, int ystart,
RGBA * color), void *self)

for (i = 0; i < num; i++) {

int glyph;
int adv = 0;
advance[i] = xpos;
glyph = swf GetBits(t, gbits);
adv = swf GetBits(t, abits);

435

...

494
495
496
497
498
499

Fig. 4. Code snippet of swftext.c in SWFTools project

i.e., CVE-2021-42203, which involves different
package,
functions in multiple ﬁles. As shown in Fig. 4 and Fig.
5,
the
the program uses a pointer
line swftext.c:498, and frees it at
the line rfxswf.c:1234.
When testing the program, LeoFuzz successfully explored
a path where function swf ReadTag is called before func-
tion swf FontExtract DeﬁneTextCallback, causing the pro-
gram crash.

t of TAG type at

E. RQ6: Contributions of four design decisions

To evaluate the contributions of four main design decisions
in LeoFuzz, i.e., target sequence enhancement, exploration-
exploitation coordination, ﬁne-grained energy scheduling strat-
egy and concolic execution, we disabled each technique and
compiled four variants of LeoFuzz and named them LeoFuzz-
s, LeoFuzz-e, LeoFuzz-f and LeoFuzz-c respectively. We ran
LeoFuzz and its four variants against those programs in Table
IV and the results are shown in Table X. The TTE columns
indict the mean value of TTEs spent by LeoFuzz-s, LeoFuzz-
e, LeoFuzz-f, LeoFuzz-c and LeoFuzz to trigger all CVEs in
each subject program, and the factor columns reﬂect the TTE’s
ratio between four variants and LeoFuzz.

We have two ﬁndings: 1) Each technique contributes to
LeoFuzz as the performance of each variant is weaker than
LeoFuzz; 2) The contribution of MES is better than that of
CEE and that of target sequence enhancement, which are better
than that of concolic execution. It is reasonable because MES
considers both the relations between a seed and targets and
the relations within multiple target sequences.

11

TABLE VIII
RESULTS ON TRUE POSITIVE VERIFICATION.

CVE-ID

2018-9132
2018-9009
2018-8807
2018-7877
2018-7876
2018-7875
2018-7873
2018-7872
2018-7870
2018-7869
2018-7868
2018-7867
2018-6359
2018-6315
2018-20591
2018-20429
2018-20427
2018-11226
2018-11225
2018-11017

QSYM

AFLGo

Lolly

Berry

TTE
7h19m
4h11m
25m43s
5h41m
2h16m
5h12m
5h59m
8h27m
5h38m
6h1m
8h1m
5h37m
41m52s
4m14s
4h6m
7h49m
7h6m
5h27m
9m45s
51m25s

Factor
6.55
10.20
8.60
4.55
3.00
4.33
6.71
7.68
6.30
17.56
10.41
6.54
5.25
2.19
13.92
5.94
5.20
5.03
5.48
2.58

TTE
7h55m
3h23m
9m30s
2h55m
3h54m
2h57m
3h2m
6h47m
3h12m
1h6m
6h13m
1h45m
18m4s
3m17s
3h13m
6h22m
8h2m
1h55m
5m20s
42m44s

Factor
7.09
8.25
3.06
2.33
5.14
2.46
3.40
6.17
3.48
3.22
8.07
2.04
2.36
1.70
10.92
4.84
5.88
1.77
3.24
2.14

TTE
6h11m
3h12m
6m11s
3h9m
2h13m
3h26m
2h31m
1h59m
2h34m
1h8m
1h42m
1h17m
13m43s
5m19s
2h35m
2h29m
6h49m
1h53m
4m12s
41m16s

Factor
5.54
7.78
2.04
2.52
2.92
2.86
2.88
1.80
2.79
3.32
2.21
1.50
1.79
2.75
8.77
1.89
4.99
1.73
2.42
2.07

TTE
5h48m
2h35m
4m18s
2h49m
2h42m
2h53m
1h47m
1h51m
1h54m
49m46s
1h18m
52m16s
13m38s
5m59s
2h15m
2h18m
6h0m
1h29m
3m3s
32m19s

Factor
5.19
6.29
1.42
2.26
3.56
2.40
2.00
1.68
2.07
2.44
1.69
1.02
1.79
3.09
7.64
1.75
4.39
1.37
1.76
1.62

LeoFuzz
TTE
1h7m
24m39s
3m2s
1h15m
45m23s
1h12m
53m21s
1h6m
55m12s
20m25s
46m12s
51m32s
7m39s
1m56s
17m41s
1h19m
1h22m
1h5m
1m44s
19m58s

TABLE IX
UNREPORTED VULNERABILITIES FOUND BY LEOFUZZ. BOF=BUFFER OVERFLOW, SOF=STACK OVERFLOW, NP=NULL POINT EXCEPTION,
MAF=MEMORY ALLOCATION FAILURE, UAF=USE-AFTER-FREE, ML=MEMORY LEAK, BF=BAD FREE, AE=ASSERT ERROR, DF=DOUBLE FREE,
SBOF=STACK BUFFER OVERFLOW

Program
cxxﬁlt

Buggy Func.
demangle path
demangle type
handleEditText
traits parse
rfx alloc
swf GetBits
swf FontExtract DeﬁneTextCallback

SWFTools main

swf GetD64
swf DeleteFilter
swf FontExtract DeﬁneTextCallback
swf GetBits
bit calc CRC
decode preR13
decode preR13 section
decode preR13 section
decode preR13 section hdr
dwg add object
dwg add handleref
dwg read ﬁle
decode preR13 entities
dwg read ﬁle
copy bytes

libredwg

Type
SOF
SOF
BOF
NP
ML
NP
BOF
NP
BOF
NP
UAF
BOF
BOF
NP
NP
UAF
BOF
BOF
UAF
BF
AE
DF
SBOF

Reported as
CVE-2021-3530
ubuntu bug-1927070
CVE-2021-42195
CVE-2021-42196
CVE-2021-42197
CVE-2021-42198
CVE-2021-42199
CVE-2021-42200
CVE-2021-42201
CVE-2021-42202
CVE-2021-42203
CVE-2021-42204
issues-484
issues-485
issues-486
issues-487
issues-488
issues-489
issues-490
issues-491
issues-492
issues-493
issues-494

12

TABLE X
EFFECTIVENESS OF EACH DESIGN DECISION IN LEOFUZZ

Program

cxxﬁlt
objdump
readelf
tiff2pdf
dwg2dxf
unzzipcat-mem
mjs

LeoFuzz-s

LeoFuzz-e

LeoFuzz-f

LeoFuzz-c

TTE
1h15m
—
2h17m
2h53m
4h23m
1h49m
17m52s

Factor
1.61
2.69
2.45
2.25
1.14
1.06
1.35

TTE
1h16m
7h51m
3h7m
2h21m
5h23m
2h16m
25m10s

Factor
1.64
1.76
3.40
1.83
1.40
1.32
1.92

TTE
1h27m
—
3h41m
3h26m
6h19m
2h31m
39m14s

Factor
1.88
2.69
3.95
2.68
1.64
1.47
3.00

TTE
1h13m
5h34m
2h18m
1h32m
4h36m
1h53m
17m41s

Factor
1.57
1.25
2.46
1.32
1.20
1.10
1.31

LeoFuzz
TTE
46m22s
4h28m
56m11s
1h17m
3h51m
1h43m
13m27s

1201

TAG * swf ReadTag(reader t*reader, TAG * prev)

...

1229
1230
1231

1232
1233
1234
1235
1236

if (reader−>read(reader, t−>data, t−>len) != t−>len) {

#ifdef DEBUG RFXSWF
fprintf(stderr, ”rfxswf: Warning: Short read (tagid %d). File

truncated?\n”, t−>id);

#endif
free(t−>data); t−>data=0;
free(t);
return NULL;

}

Fig. 5. Code snippet of rfxswf.c in SWFTools project

VIII. THREATS TO VALIDITY

Internal validity: The main internal threat is the randomness
of fuzzing. We conducted the experiments multiple times for
fairness, and as initial seeds might inﬂuence the outcomes in
the fuzzing experiments, we used the same seeds to LeoFuzz
as inputs to each baseline in all experiments. The second
internal threat comes from the conﬁgurable options in Leo-
Fuzz, e.g., two parameters in Equation 1 and 2, which are
currently set based on our preliminary experiments. Though
the current results are promising, we believe ﬁne-tuning them
may improve the experiment results but it is not the key
technique here. Therefore, we leave it as future work.

External validity: Although our experimental results may
to mitigate this threat, we chose
vary to other programs,
31 vulnerabilities in 7 real-world programs that have been
frequently evaluated in the existing fuzzers. These programs
also have diverse functionalities as well as different program
sizes. Moreover, the vulnerabilities chosen come from different
types (9 in total) and thus have different difﬁculty to trigger
them.

Construct validity: We compare different conﬁgurations of
LeoFuzz according to the main techniques proposed in this
paper, so we can understand that any effect on the results is
due to their differences, and can also verify that the proposed
strategies are all effective.

IX. RELATED WORK

A. Coverage-based Greybox Fuzzing

to generate test cases that can trigger vulnerabilities in pro-
grams. Compared with blackbox fuzzing [29] and whitebox
fuzzing [27], [28], greybox fuzzing has higher efﬁciency and
effectiveness [13], [35].

To improve the exploration ability of greybox fuzzing,
Vuzzer [26] uses dynamic data-ﬂow analysis in greybox
fuzzing to maximize coverage and explore deeper paths. An-
gora [32] solves path constraints by gradient descent algorithm
to improve the coverage of branches. REDQUEEN [18] lever-
ages a lightweight input-to-state correspondence mechanism
as an alternative to data-ﬂow analysis and symbolic execu-
tion. GREYONE [21] exploits a data ﬂow-sensitive fuzzing
scheme since fuzzing based on traditional data ﬂow analysis
is inaccurate and slow. DeepFuzzer [48] ﬁrst uses symbolic
execution to generate qualiﬁed initial seeds that can help
pass complex checks, then applies a statistical seed selection
algorithm to balance mutation frequencies among different
seeds. Its hybrid mutation strategy aims to balance global
exploration and deep search. To reﬁne the seed scheduling
of greybox fuzzing, AFLFast [25] shows that most test cases
execute high-frequency paths, so AFLFast assigns more energy
to those seeds which can pass through the low-frequency
paths. EcoFuzz [30] proposes a variant of the Adversarial
MultiArmed Bandit (VAMAB) model to model scheduling
problems and balances exploration stage and exploitation stage
for reasonable seed selection, while LeoFuzz coordinates two
stages adaptively to balance the fuzzer’s code coverage and
directedness. AFLsmart [47] leverages a high-level structural
representation of the seed ﬁle and new mutation operators
to generate new ﬁles, and introduces a validity-based power
schedule to spend more time generating ﬁles that are more
likely to pass the parsing stage of the program.

Unlike the above efforts that aim to improve the per-
formance of coverage-based greybox fuzzing, LeoFuzz is a
target-oriented directed greybox fuzzer that aims to trigger
multiple target sites in a single instance. PAFL [46] extends
existing fuzzing optimizations of single mode to industrial
parallel mode by dividing tasks and synchronizing guiding
information. It improves the fuzzing efﬁciency by running
more instances of multiple fuzzers and is complementary to
our approach.

Greybox fuzzing is scalable and practical in ﬁnding bugs or
vulnerabilities in software. AFL [13] uses lightweight compile-
time instrumentation, coverage feedback and genetic algorithm

B. Directed Greybox Fuzzing

AFLGo [14] is the ﬁrst directed greybox fuzzer (DGF), its
simulated annealing-based power schedule gradually assigns

more energy to seeds that are closer to the target sites
while reduces energy for seeds that are far away. Based on
AFLGo, Hawkeye [19] supports indirect calls and adjusts its
seed prioritization, power scheduling and mutation strategies
adaptively to reach the target sites rapidly. However, Hawk-
eye has similar problems with AFLGo when dealing with
multiple targets. As discussed in Section II, their unsuitable
energy schedule may ignore local optimal solutions and hinder
covering multiple targets efﬁciently. Moreover, their strategy
of coordinating exploration-exploitation stages is static and
inﬂexible. Also based on AFLGo, RDFuzz [24] prioritizes a
seed by combining its input-distance to the target sites and
its trace’s frequency and uses a static intertwined schedule
to perform exploration and exploitation in turn. By contrast,
LeoFuzz dynamically coordinates exploration and exploitation
stages according to the ratio of directed seeds and coverage
seeds.

Some directed fuzzers exploit a sequence-based guided
approach. Lolly [15] is the ﬁrst sequence directed greybox
fuzzer. For a given set of target statement sequences, Lolly
aims to generate inputs that can reach the statements in each
sequence in order. Berry [16] uses the target program’s CFGs
to extend the given target sequence to improve the directedness
of fuzzing. UAFL [23] focuses on UAF vulnerability and thus
takes use-after-free sequence to guide its fuzzer. Lolly, Berry
and UAFL consider the execution order of targets, while Leo-
Fuzz further takes into account three kinds of relation between
seeds and targets, i.e., seed’s target sequence coverage, priority
and global maximum coverage of each target sequence.

Several directed fuzzers leverage the output of sanitizers
to guide fuzzing. SAVIOR [20] uses the output of UBSan
as target sites, and calculates a seed’s energy according to
the new branches that the seed meets, the target sites on
these branches and the difﬁculty of solving these branches’
constraints. ParmeSan [22] leverages the errors or warnings
reported by multiple sanitizers as target sites and then guides
the fuzzer by the distance from a seed to a target. LeoFuzz
can also use the results from sanitizers as targets though it
does not depend on sanitizers.

Several directed fuzzers use data ﬂow analysis or data con-
ditions. CAFL [31] aims to satisfy a sequence of constraints
and prioritizes the seeds that better satisfy those in order. It
deﬁnes a constraint as a single target site and optionally a
number of data conditions. If multiple constraints are speciﬁed,
they must be satisﬁed in the speciﬁed order. CAFL assumes
that the target sites are dependent to each other while LeoFuzz
support multiple independent target sites. CAFL requires the
additional information sources, i.e., crash dumps from memory
error detectors and changelogs from patches, to generate the
constraints. Moreover, CAFL was evaluated with up to 2
targets and cannot cover bugs that require three or more
constraints, while LeoFuzz triggered ten bugs in dwg2dxf
within four hours in our evaluation. Considering that each
basic block isn’t equally important in seed distance calculation,
WindRanger [49] uses the deviation basic blocks (DBBs)
and their data ﬂow information for seed distance calculation,
mutation, seed prioritization and energy scheduling. It dy-
namically switches between the exploration and exploitation

13

stage according to the execution status of DBBs. Beacon [50]
leverages a provable path pruning method to improve the
efﬁciency of DGF. It identiﬁes infeasible paths via control ﬂow
reachability and path condition satisﬁability, instruments those
related statements, and prunes these paths during fuzzing. By
contrast, LeoFuzz combines concolic execution and fuzzing
by sharing two types of seeds, and hence can solve the path
constraint for a seed and mutate the validated inputs satisfying
the path condition. Overall,
these methods are orthogonal
to LeoFuzz and can be integrated with LeoFuzz for better
performance.

X. CONCLUSION

We present a multiple targets directed greybox fuzzing
approach, which leverages a novel strategy to adaptively
coordinate exploration and exploitation stages, and a novel
energy scheduling strategy that considers more relations be-
tween seeds and targets, i.e., target sequence coverage, target
sequence priority and global maximum coverage of target se-
quence. Our approach also uses concolic execution to help the
fuzzer explore complex branches in programs. We implement
our approach in LeoFuzz and evaluate it on crash reproduction,
true positives veriﬁcation, and vulnerability exposure in seven
real-world programs. Experimental results show that LeoFuzz
outperforms six state-of-the-art tools, i.e., QYSM, AFLGo,
Lolly, Berry, Beacon and WindRanger.

As future work, we will combine parallel programming
with LeoFuzz to improve its performance further. We are
also planning to combine LeoFuzz with QEMU emulator to
discover vulnerabilities in embedded devices.

ACKNOWLEDGMENTS

We would like to thank the anonymous reviewers for their

insightful comments.

REFERENCES

[1] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=objdump+2.34&search type=all

[2] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=readelf+2.28&search type=all

[3] https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=binutils
[4] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=tiff2pdf+4.0.9&search type=all

[5] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=tiff2pdf+4.0.8&search type=all

[6] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=jasper++2.0.14&search type=all

[7] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=jasper++2.0.12&search type=all

[8] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=tiff2pdf+4.0.9&search type=all

[9] https://www.sqlite.org/cves.html
[10] https://nvd.nist.gov/vuln/search/results?form type=Basic&results type=

overview&query=tcpdump+4.9.3&search type=all
[11] https://httpd.apache.org/security/vulnerabilities 24.html
[12] H. Liang, X. Pei, X. Jia, W. Shen, and J. Zhang, “Fuzzing: State of
the Art”. IEEE Transactions on Reliability, vol.67, no.3 pp, 1199-1218,
Sep. 2018

[13] M. Zalewski, “American fuzzy lop,” http://lcamtuf.coredump.cx/aﬂ/.
[14] M. B¨ohme, V.-T. Pham, M.-D. Nguyen, and A. Roychoudhury, “Directed
Greybox Fuzzing”, in Proceedings of the 2017 ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS 2017, Dallas,
TX, USA, October 30 - November 03, 2017, 2017, pp. 2329–2344.

[15] H. Liang, Y. Zhang, Y. Yu, Z. Xie and L. Jiang, “Sequence Coverage
Directed Greybox Fuzzing,” in Proceedings of 2019 IEEE/ACM 27th
International Conference on Program Comprehension (ICPC), 2019, pp.
249-259.

[16] H. Liang, L. Jiang, L. Ai and J. Wei, “Sequence Directed Hybrid
Fuzzing”, in Proceedings of 2020 IEEE 27th International Conference
on Software Analysis, Evolution and Reengineering (SANER), 2020,
pp. 127-137.

[17] I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim, “QSYM:A Practical Con-
colic Execution Engine Tailored for Hybrid Fuzzing”, in Proceedings of
USENIX Security Symposium, 2018, pp. 745–761.

[18] C. Aschermann, S. Schumilo, T. Blazytko, R. Gawlik, and T. Holz,
“REDQUEEN: Fuzzing with Input-to-State Correspondence”, in Pro-
ceedings of 2019 Network and Distributed System Security Symposium,
2019.

[19] H. Chen, Y. Xue, Y. Li, B. Chen, X. Xie, X. Wu, and Y. Liu, “Hawkeye:
Towards a Desired Directed Grey-box Fuzzer”, in Proceedings of the
2018 ACM SIGSAC Conference on Computer and Communications
Security, CCS 2018, pp. 2095–2108.

[20] Y. Chen, P. Li, J. Xu, S. Guo, R. Zhou, Y. Zhang, T. Wei, and L.
Lu, “SAVIOR: Towards Bug-Driven Hybrid Testing”, in Proceedings of
2020 IEEE Symposium on Security and Privacy (SP), pp. 1580–1596.
[21] S. Gan, C. Zhang, P. Chen, B. Zhao, X. Qin, D. Wu and Z. Chen.
”GREYONE: Data Flow Sensitive Fuzzing”, in Proceedings of USENIX
Security Symposium, 2020, pp. 2577-2594.

[22] S. ¨Osterlund, K. Razavi, H. Bos and C. Giuffrida, “ParmeSan: Sanitizer-
guided Greybox Fuzzing.”, in Proceedings of USENIX Security Sym-
posium, 2020, pp. 2289-2306.

[23] H. Wang, X. Xie, Y. Li, C. Wen, Y. Li, Y. Liu, S. Qin, H. Chen and Y.
Sui, “Typestate-Guided Fuzzer for Discovering Use-after-Free Vulnera-
bilities.”, in Proceedings of IEEE/ACM 42nd International Conference
on Software Engineering (ICSE) ,2020, pp. 999-1010.

[24] J. Ye, R. Li and B. Zhang, “RDFuzz: Accelerating Directed Fuzzing
with Intertwined Schedule and Optimized Mutation.”, in Proceedings of
Mathematical Problems in Engineering, 2020, pp. 1-12.

[25] M. B¨ohme, V. Pham and A. Roychoudhury, “Coverage-Based Greybox
Fuzzing as Markov Chain,” in IEEE Transactions on Software Engineer-
ing, vol. 45, no. 5, pp. 489-506.

[26] S. Rawat, V. Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos,
“VUzzer: Application-aware Evolutionary Fuzzing”, In Proceedings
2017 Network and Distributed System Security Symposium, 2017.
[27] C. Cadar, D. Dunbar, and D. R. Engler, “KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems
Programs,” in 8th USENIX Symposium on Operating Systems Design
and Implementation, OSDI 2008, December 8-10, 2008, San Diego,
California, USA, Proceedings, R. Draves and R. van Renesse, Eds.
USENIX Association, 2008, pp. 209–224.

[28] Y. Shoshitaishvili et al., “SOK: (State of) The Art of War: Offensive
Techniques in Binary Analysis,” 2016 IEEE Symposium on Security
and Privacy (SP), 2016, pp. 138-157.

[29] Peach. 2016. Peach Fuzzer: Discover unknown vulnerabilities. Peach

Fuzzer (July 2016). http://www.peachfuzzer.com/

[30] Yue, Tai, Pengfei Wang, Yong Tang, Enze Wang, Bo Yu, Kai Lu and
Xu Zhou. “EcoFuzz: Adaptive Energy-Saving Greybox Fuzzing as a
Variant of the Adversarial Multi-Armed Bandit.” USENIX Security
Symposium,2020, pp.2307-2324.

[39] Libtiff is a library for reading and writing tag image ﬁle format
(abbreviated as TIFF).[Online]. Available:http://www.libtiff.org/

14

[31] L. Gwangmu, W.-J. Shim and B. Lee, “Constraint-guided Directed
Greybox Fuzzing.”, in Proceedings of USENIX Security Symposium,
2021, pp. 3559-3576.

[32] P. Chen, H. Chen, “Angora: Efﬁcient Fuzzing by Principled Search,”
2018 IEEE Symposium on Security and Privacy (SP), 2018, pp. 711-
725.

[33] C. Lemieux and K. Sen, “FairFuzz: Targeting Rare Branches to Rapidly
Increase Greybox Fuzz Testing Coverage”, arXiv:1709.07101 [cs],
September 2017.

[34] libfuzzer, https://llvm.org/docs/LibFuzzer.html, 2015.
[35] Oss-fuzz report, https://security.googleblog.com/2018/11/a-new-chapter-

for-oss-fuzz.html, 2018.

[36] Finding Dominators in Directed Graphs. SIAM Journal on Computing.
Society for Industrial and Applied Mathematics. Retrieved June 24, 2021
from https://epubs.siam.org/doi/10.1137/0203006

[37] Clang Static Analyzer. [Online]. Available: http://clanganalyzer.llvm.org/
Available:
[38] Binutils

[Online].

toolset.

is

a
https://github.com/bminor/binutils-gdb

binary

[40] LibreDWG is a free C library to read and write DWG ﬁles.[Online].

Available: https://github.com/LibreDWG/libredwg

[41] zziplib offers the ability to easily extract data from ﬁles archived in a
single zip ﬁle.[Online]. Available: http://zziplib.sourceforge.net/
[42] mjs is designed for microcontrollers with limited resources. [Online].

Available: http://github.com/cesanta/mjs

[43] Libming is a library for generating macromedia ﬂash ﬁles. [Online].

Available: https://github.com/libming/libming

[44] SWFTools is a collection of utilities for working with Adobe Flash ﬁles

(SWF ﬁles). [Online]. Available: http://www.swftools.org/
infrastructure

[45] ClusterFuzz

scalable
is
security and stability issues
https://github.com/google/clusterfuzz

a

fuzzing
in software.

ﬁnds
that
[Online]. Available:

[46] Jie Liang, Yu Jiang, Yuanliang Chen, Mingzhe Wang, Chijin Zhou and
Jiaguang Sun. “PAFL: extend fuzzing optimizations of single mode to
industrial parallel mode.” Proceedings of the 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2018. pp. 809–814.

[47] Van-Thuan Pham, Marcel B¨ohme, Andrew E. Santosa, Alexandru Raz-
van Caciulescu, Abhik Roychoudhury. “Smart Greybox Fuzzing.” IEEE
Trans. Software Engineering. 47(9): 1980-1997, 2021.

[48] Jie Liang, Yu Jiang, Mingzhe Wang, Xun Jiao, Yuanliang Chen, Houbing
Song, Kim-Kwang Raymond Choo. “DeepFuzzer: Accelerated Deep
Greybox Fuzzing.” IEEE Trans. Dependable Secure Computing. 18(6):
2675-2688, 2021.

[49] Zhengjie Du, Yuekang Li, Yang Liu, and Bing Mao. “WindRanger:
A Directed Greybox Fuzzer driven by Deviation Basic Block.” 44th
International Conference on Software Engineering (ICSE), 2022.
[50] Heqing Huang, Yiyuan Guo, Qingkai Shi, Peisen Yao, Rongxin Wu and
Charles Zhang. “BEACON: Directed Grey-Box Fuzzing with Provable
Path Pruning.”, The 43rd IEEE Symposium on Security and Privacy.
2022.

[51] A. Vargha and H. D. Delaney, “A Critique and Improvement of the
Common Language Effect Size Statistics of McGraw and Wong,”
Journal of Educational & Behavioral Statistics, vol. 25, no. 2, pp.
101–132, 2000.

