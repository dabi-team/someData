CRISPnet: Color Rendition ISP Net

Matheus Souza and Wolfgang Heidrich

King Abdullah University of Science and Technology (KAUST),
Thuwal, Saudi Arabia
{matheus.medeirosdesouza,wolfgang.heidrich}@kaust.edu.sa

Abstract. Image signal processors (ISPs) are historically grown legacy
software systems for reconstructing color images from noisy raw sensor
measurements. They are usually composited of many heuristic blocks
for denoising, demosaicking, and color restoration. Color reproduction in
this context is of particular importance, since the raw colors are often
severely distorted, and each smart phone manufacturer has developed
their own characteristic heuristics for improving the color rendition, for
example of skin tones and other visually important colors.
In recent years there has been strong interest in replacing the historically
grown ISP systems with deep learned pipelines. Much progress has been
made in approximating legacy ISPs with such learned models. However,
so far the focus of these efforts has been on reproducing the structural
features of the images, with less attention paid to color rendition.
Here we present CRISPnet, the first learned ISP model to specifically
target color rendition accuracy relative to a complex, legacy smart phone
ISP. We achieve this by utilizing both image metadata (like a legacy
ISP would), as well as by learning simple global semantics based on
image classification – similar to what a legacy ISP does to determine
the scene type. We also contribute a new ISP image dataset consisting
of both high dynamic range monitor data, as well as real-world data,
both captured with an actual cell phone ISP pipeline under a variety of
lighting conditions, exposure times, and gain settings.

Keywords: image signal processor; image restoration; color rendition.

1

Introduction

The past decade has seen tremendous progress in miniaturizing camera modules
to fit high quality imaging systems into flat mobile devices. As this miniatur-
ization effort is approaching physical limits, such systems increasingly rely on
computational methods to maintain high image quality. Image Signal Processors
(ISPs) are responsible for tasks such as interpolation, demosaicking, denoising,
enhancement on the edges, white balancing and color restoration, exposure cor-
rection, gamma encoding, compression, and so forth. The increasing miniatur-
ization places a higher burden on these tasks, and in particular the reproduction
of accurate colors. For example, the pixel pitch in modern smart phone cameras

2

M. Souza and W. Heidrich

Fig. 1. Examples of faithful color rendition produced by CRISPnet compared to
AWNet [6] for two full images and several image patches. The lower triangle in each
image is the ground truth, the upper triangle is the respective learned model. Within
each pair, the left result is CRISPnet, the right AWNet. Note also the quantitative
improvement in ∆E∗

ab76 (lower is better).

is now as small as 0.7 µm – barely larger than one wavelength of red light1,
leading to significant crosstalk between the pixels in a Bayer pattern image [1].
This distortion is a spectral effect that can not be accurately modeled as a simple
deconvolution or color matrix in RGB space.

Existing ISPs combat such image artifacts with complex, historically grown
pipelines made up of heuristic blocks that are prone to error accumulation
through the pipeline. At the same time, these systems are based on substan-
tial expertise in color theory and human perception, and lead to a brand-specific
“look” that accounts for a lot of the appeal of particular smart phone models.

In recent years there have been a number of proposals for replacing the
historically grown and heuristic ISP pipelines with simpler, more principled ap-
proaches. Initially, these efforts relied on optimization-based approaches [13], but
with increasingly more powerful compute resources [17] the attention has now
shifted to deep learning and convolution neural networks (CNNs) [29,32]. More
recent efforts include PyNET [16] and similar methods [14,20,6] that train ISP
networks to match DSLR data.

We believe this approach is fundamentally limited in that DSLRs can not
be considered the gold standard in color processing: while DSLRs have
superior optics and noise characteristics, they have very minimalistic ISPs, and
effectively offload the problem of color rendition to the end user – typically a pro-
fessional photographer who will manually post-process the images in photoshop
or similar tools.

In this work we therefore propose a learned ISP framework that leverages
in particular the color science expertise that has gone into the development of
legacy smart phone ISPs. Color Rendition ISP Net (CRISPnet) is trained on
pairs of raw and ISP-processed smart phone images. In addition, it also learns

1 e.g. Samsung ISOCELL sensors

Color Rendition ISP Net

3

to leverage the same white balance metadata that also informs the legacy ISP
pipelines. Finally, like many legacy ISPs, CRISPnet performs a rough semantic
image analysis, for example to distinguish between landscape and portrait shots.
We show that each of these architectural improvements significantly increases
reconstruction accuracy, especially with respect to color rendition. In summary,
we make the following contributions:

– a novel deep architecture for ISP processing specifically tailored at matching

the color processing of legacy ISPs.

– a way to inject white balance metadata into the network so that it can learn

to be robust under different illuminations and scenes.

– an attention-based image semantics module that helps the ISP net to make

different decisions for different scene types.

– an extensive new dataset for ISP networks consisting of pairs of raw and ISP
processed smart phone camera images over a wide range of exposure and gain
settings. The dataset consists of two parts – a large monitor dataset captured
from a color calibrated HDR monitor (used for large scene diversity), and a
smaller real-world dataset used for fine tuning.

Code and data will be made available.

2 Related Work

The idea of treating all stages involved in an ISP pipeline as a single integrated
problem was proposed in [13]. Their framework takes the traditional modular
structure applying different heuristics in each stage and substituted it with a
single inverse problem. The proposed approach uses proximal operators and
the primal-dual method for optimization, performing all of that together with
various natural image priors (BM3D [5], TV, and cross-channel [12]). However,
these natural image priors are relatively weak compared to modern deep learned
models, leading to a degradation in image quality.

Later the optimization strategies were substitute by deep learning methods,
building on excellent results reported for in image processing tasks like denoising
[40,30,39,3], deblurring [37,25,4], super-resolution [35,26,7,21], etc. These sys-
tems focused on solving denoising and demosaicking jointly, learning mappings
from raw to sRGB and from sRGB to raw [2,38]. Although these works can
diminish noise, the color fidelity is sometimes left behind.

Other works on deep learned ISPs were focusing on image enhancements.
Schwartz et al. [32] proposed a DeepISP mapping from a low-light mosaicked
image to final sRGB, while Igantov et al. [16] proposed PyNET, targeting trans-
lation of mobile raw images to match the quality DSLR sRGB. After PyNET,
more architecture ideas emerged [18,15], the most competitive ones relying on
encoding-decoding structure with backbone resembling network like ResNET,
UNet, Pix2Pix, etc. [11,31,19] often combined with attention mechanisms [14,20,6],
these models further increase the accuracy, exploiting global and local features,
applying spatial and channel attention similar to [36] proposed.

4

M. Souza and W. Heidrich

However, all these works have adopted the notion of training a mapping
between mobile raw data and DSLR color images. While DSLRs have superior
optics and sensors, they mostly rely on the user to post-process the image to
improve the color rendition, while the ISPs of smart phones and point-and-
shoot cameras are optimized to produce good looking final color images without
manual post-processing. As a result many consumer camera tests now consider
mobile camera images to have better color rendition than DSLRs without post-
processing.2

In this work we therefore aim to leverage the substantial expertise in color
theory and human perception that has flown into legacy ISP pipelines, and train
a network to reproduce the color processing of a legacy smart phone ISP. Since
such ISP pipelines make many non-linear and scene-dependent decisions, we
show that this mapping can not be learned effectively with previous ISP net-
work architectures. We therefore propose two architectural improvements: first,
we utilize image meta data captured with the raw data. Image meta data was pre-
viously used in tone mapping applications [28], but it also is used extensively in
traditional ISP pipelines. In this work, we specifically usilize white balance infor-
mation to learn an ISP network with improved color rendition. We note that the
use of white balance information differs form the previously proposed use of the
color matrix metadata [22], since the color matrix is a scene-independent charac-
teristic of the camera hardware, while the white balance data is scenes-specific.
We also propose a global feature learning approach to feed scene composition
and semantics information into the ISP process. For this purpose, we adopt the
new XCiT transformer architecture [10], which we found to work better for this
purpose than alternatives like Swin transformers [24].

As mentioned above, the most commonly used dataset for ISP networks uses
pairings of mobile and DSLR images [16], and is therefore not of use for our
system. Other existing datasets (e.g. [32]) are too small to really benefit our
method. We therefore also propose a new dataset with a mix between HDR
monitor captures and real world data. The HDR monitor data provides large
scene diversity with excellent dynamic range and color calibration but is limited
by the resolution of the monitor, which is lower than that of the camera. The
real world data is used for fine tuning the recovery of pixel-level details, but
cannot match the scene diversity of the monitor data.

3 Method

The ISP problem can be interpreted as an image translation task. Given a raw
image, we want to learn a mapping to RGB that maintains the characteristics of
the original device. Therefore, during this process, the method must learn low-
level and high-level properties such as denoising, deblurring, sharpening, white
balance, etc. The recent ideas provide end-to-end solutions mainly focusing on
denoising and demosaicking [38,2,32]. However, in modern ISP pipelines, the

2 e.g. https://www.photographyacademy.com/why-phones-take-better-pictures-than-

your-dslr

Color Rendition ISP Net

5

Fig. 2. An illustration of the CRISPnet. It receives as input the image divided into
patches, the full image downsampled to 368 x 480 pixels, and the white balancing
metadata. Here, H and W represent the full image dimension, while H and W is the
dimension of the image patches. W represents the white balance value for each image
channel. The network is divided into three branches. The reconstruction branch is re-
sponsible for overall image reconstruction while aggregating information from the other
branches efficiently. The white balance branch projects the white balance information
to match the layer dimension inside the reconstruction and the global semantics branch
learns global scene semantics from a downsampled version of the full image. a) and b)
correspond to two different strategies explored in this work.

white balance and the global scene classification heavily influence the final result
and will be explored in the following.

CRISPnet follows the architecture shown in Fig.3. The Bayer raw image I ∈
RH×W goes through two branches. The reconstruction branch restructures the
Bayer mosaic I into a 4 channel image IRGBG ∈ R H
2 ×4. Tiles of this image
are processed with a modified UNet, which is also responsible for aggregating
information from the other branches. The output of the reconstruction branch
are ISP processed tiles that are reassembled into a translated RGB image IRGB ∈
RH×W ×3.

2 × W

The second branch operates on a downsampled version of the full image
ID ∈ R480×368 with fixed resolution. This branch is responsible for learning global
semantics when the training images are divided into patches. The output of this
branch F ∈ R512 matches the dimensions of the bottleneck in the reconstruction
branch and the two branches are combined with a channel-wise product.

Finally, we also inject image meta data into the reconstruction branch at
three separate stages, again through a channel-wise product. The meta data
consists of white balance information – three channel weights that are processed

6

M. Souza and W. Heidrich

through a third network branch. We also experimented with additional meta
data such as exposure time and gain, but did not find them useful in improving
the results. On the other hand, a learned injection of the white balance data
does show significant improvements compared to both not using meta data at
all and also compared to simple pre-multiplication of the white balance weights
on the input data (see Section 5).

3.1 Reconstruction Branch

The backbone of CRISPnet has a UNet structure Fig.3, which has proved to
be effective for this task by many of the works proposed in [18,15]. Its fully
convolutional structure enables efficient inference for high resolution images.
Like other learned ISPs, we perform patch-based image processing in order to
control the computational expense for training the network. Our implementation
has three main differences when compared with “vanilla” UNet proposed by [31].
Here we want to aggregate additional information in order to reconstruct faithful
colors.

In legacy ISP pipelines white balance information is taken into account in
several stages. To reproduce this behavior, we aggregate this information in
the early layers using the white balance branch output as scales for the first
three downsample levels in the network. Different ideas were proposed to better
perform feature matching, including concatenation, dot-product, as well as more
complex solutions like [27]. However, as detailed below, since the white balance
branch is based on a small number of inputs, a simple projection works best for
this purpose.

After aggregating the ambient light information exploiting white balance
data, we add residual blocks in the bottleneck. Mixing ideas from ResNET and
UNet is not new; [8,41] explored the power of both architecture together. In
this work, we also exploit this, using the residual blocks only in the network
bottleneck, as Fig.3 illustrates. For our application we observed that residual
blocks in the encoder part lead to overfitting.

Finally we inject global features from the scene learned by the implicit global
semantics branch. We take advantage of the compressed representation after the
residual bottleneck to match the global features F ∈ R512. We experimented
with a number of injection strategies, but finally adopted a channel-wise prod-
uct, which scales the information inside the bottleneck channels, increasing or
diminishing the impact of feature channels in the final reconstruction.

3.2 White Balance Awareness

The raw file contains unprocessed or minimally processed data as well as im-
age metadata that describes image characteristics and parameters chosen by
the device during the shot, based on light conditions or user preferences. Tradi-
tional raw to RGB solutions take advantage of this data but most current Deep
Learning models ignore it.

Color Rendition ISP Net

7

The standardized metadata in a DNG file includes parameters such as a color
matrix, white balance weights, as well as exposure and ISO settings. Of these,
the color matrix describes the camera hardware but not the specific image; as
such it is easy to implicitly pick up for a neural network just based on image pairs
without explicitly using the metadata information. Similarly, we experimentally
found that ISO and exposure settings do not improve the ISP reconstruction
task. The situation however is very different for the white balance data – here
we notice a substantive improvement by injecting this type of metadata into the
learning process.

The white balance meta data comes in the form of one scalar multiplier per
color channel, where the green channel is usually normalized to a value of 1. We
expand this information to W ∈ R4 in RGBG format. Learned convolutional
layers then upsample this information to the same size as the corresponding
layers in the reconstruction branch so that the white balance information can
be injected into the reconstruction branch at three distinct locations with a
channel-wise dot product.

Note that the white balance branch does not contain any activation functions
– since the input is only three scalars, and no scene information is available, we
did not find it helpful to include non-linearities in this branch. Instead, the
information is simply injected into the reconstruction branch, which can then
learn how to best merge the information from the image and the meta data.

In Section 5 we show that this learned utilization of metadata is superior to
the naive approach of just pre-multiplying the image with the three scalars. We
also show in the ablation studies that three injection points are a sweet spot to
balance image quality and network complexity.

3.3 Global Semantics Branch

Unlike the ISPs in DSLRs, the legacy ISPs in mobile phones and point-and-
shoot cameras perform highly scene dependent image processing to produce good
looking images without manual intervention. Different scenes (e.g. indoor vs.
outdoor, portraits vs. landscapes) and conditions (e.g. sunny vs. overcast, snowy
vs. green) may require different color adjustments to produce appealing final
images. Reproducing this behavior is extremely hard because ISPs are closed
software, and is not straightforward to infer how all the high level and low level
image processing blocks work inside.

Deep learning models have the power to learn global semantics, and atten-
tion mechanisms have proven to work particularly well for this purpose. Previous
works [6,14,20] apply attention mechanisms to capture the relation between ob-
jects, however since they work on patches, they have no access to the global scene
semantics. To address this issue, CRISPnet has a specialized branch to deal with
global semantics, exploiting the efficient transformer architecture XCiT [10] to
extract and combine global features We also discuss alternative ideas that we
experimented with.

Different from the reconstruction branch, this branch takes as input the full
raw image I ∈ RH×W ×4, downsampled to ID ∈ R368×480×3. While the main

8

M. Souza and W. Heidrich

reconstruction branch is convolutional and can operate either on patches or full
frame images, this branch therefore always operates on images of the same size,
and has full access to the global semantics of the scene independent of patch
cropping.

This empowers CRISPnet to exploit global scene semantics in the ISP re-
construction task. Specifically, we propose to exploit attention mechanisms is
a different way from previous approaches with spatial and channel attention
[14,20]. Here, we apply a transformer-based approach. Transformers are known
for their large receptive field and hence the ability to learn global representa-
tions. The recent Cross-Covariance Image Transformers (XCiT) [10] has these
benefits while remaining efficient; we therefore chose them as the core of our
global feature branch.

The XCiT transformer, instead of having all tokens attending to all tokens,
applies a “transposed” version of the ViT idea [9], where the features channels
attend to other channels, making the complexity linear in the image resolution.
In XCiT, the attention is computed using the cross-covariance between queries
and key projections of tokens. This is motivated by the relationship between the
Gram matrix and the covariance matrix, in which the eigenvalues of one can be
obtained by decomposition of the other. This cross-covariance self-attention is
followed by a local patch interaction and a feed-forward network. In CRISPnet,
the “tiny” version of XCiT was simplified even more, reducing its depth to 4
– legacy ISP classification strategies are simple, and we did not want to over
parameterize this branch.

The downsampled full image passes through these blocks, and the final rep-
resentation is then aggregated using the final hidden state of the CLS token [34],
which is commonly used for classification tasks. This sequence is projected using
a 1x1 convolutional layer to match the bottleneck dimension of the reconstruc-
tion branch F ∈ R512. Finally, the features are combined using a channel-wise
product.

This pipeline produces consistent results between patch-based and full-frame
inference. As Section 5 shows empirically, learning full image global representa-
tions substantially improves the reconstruction accuracy.

In earlier versions of CRISPnet, we also experimented with alternative ways
to extract global semantics. For example, we tried an architecture resembling a
simple classification network, where the downsampled raw image passes through
strided convolution followed by batch normalization and ReLU, and then a max
pooling layer. This process was repeated twice and lastly, a fully connected layer
was applied to encode the information to F ∈ R512. We matched this compressed
representation with the reconstruction branch bottleneck using a channel-wise
product. Notice that we did not downscale the feature to the dimension of the
label because they are not known. Instead, we implicitly learned the global fea-
tures for the current image through the same loss function as the reconstruction
branch. While this approach worked well overall, we observed more issues with
noisy low light images, and instead adopted the XCiT model described above.

Color Rendition ISP Net

9

4 Dataset and Training

4.1 Dataset

To train CRISPnet, we require a large dataset of pairs of raw and ISP-processed
mobile phone data. Existing datasets are not suitable for this task since they are
either too small for our purposes [32], or use a DSLR as a reference camera [16].
We therefore captured our own dataset. Since global semantic scene infor-
mation is crucial to our approach, we require a large diversity of different scene
types. We therefore resort to a two-part dataset: a large database of monitor-
captured images that that cover a wide range of different environments, including
indoor and outdoor, and different types of landscapes. In addition, we also cap-
ture a smaller real-world dataset that is used for fine-tuning to overcome any
pixel artifacts that may occur due to the monitor data.

All data was captured using an iPhone XR and the ProCam software for
IOS. Every shot using this app generates two images of size 4032 × 3024: raw
(DNG format) and RGB. The raw is a single channel grid (Bayer Pattern) with
16 bit values, which represents the measured light intensity. The RGB images
are generated by passing the same raw images through the iPhone XR pipeline.

Monitor data. The monitor captures were performed in a dark room using
the CG3145 4K HDR monitor. This monitor has a typical contrast ratio of
1,000,000:1, which it achieves with a dual modulation principle [33]. However,
unlike most most dual modulation HDR monitors that use an LCD illuminated
by a low-frequency LED backlight [33], the Eizo CG3145 actually uses two LCD
layers stacked on top of each other on the same glass substrate. This allows the
monitor to achieve not only high global contrast, but also excellent local con-
trast of high frequency features. As such, it is capable of producing high quality
images over a large range of intensities, as well as individual images that exceed
the dynamic range of the mobile phone camera to simulate challenging illumi-
nation conditions. The monitor was color calibrated using off the shelf software
for accurate representation of the source material.

Using a tripod-mounted phone setup and a capture script, we acquired a
total of 2000 raw/ISP image pairs. The source material was taken from [23] as
well as from a new HDR portrait dataset.3

Real world data. One downside of the monitor setup is that the screen resolution
of 4, 096 × 2, 160 is lower than the resolution of the mobile phone camera, which
introduces pixel level artifacts. To combat these, we also capture a real world
dataset for fine-tuning the training. This dataset has accurate pixel-level details,
but much lower scene diversity than the monitor data, since it is not feasible to
travel to remote places to capture images in different sites. All in all, we captured
200 real world image pairs.

3 Details will be provided in the final version to preserve double blindness.

10

M. Souza and W. Heidrich

4.2 Training

From the monitor dataset we select 750 images (600 for training, 75 for testing,
and 75 for validation) and from the real-world dataset 198 images (160 for train-
ing, 19 for testing, and 19 for validation). The idea here is to first train using
only monitor data and then fine-tune with the real-world. A small crop was per-
formed on the monitor data to avoid capturing the bezel of the monitor itself,
reducing the resolution from 4032 × 3024 to 3840 × 2944. Next, each picture was
divided into 64 patches of 368 × 480 to train the reconstruction part. While the
reconstruction receives a patch, the global feature acquisition branch is fed with
the downsampled as discussed above.

4.3 Loss Function

In our experiments we adopted the MSE loss function. Usually in this field,
“perceptual” and SSIM losses are used together with MSE or MAE. However, we
did not observe sufficient improvements from these losses to justify the increased
training overhead. We also note that these alternative losses focus on structural
image features and tend to neglect color reproduction. In the supplementary
material we provide a more detailed analysis.

5 Experiments

Fig. 3. Full frame image reconstruction after fine-tuning CRISPnet to deal with real-
world data, compared against AWNet [6]. CRISPnet is able to reproduce the light
condition in the scene and threfore producing accurate colors.

Color Rendition ISP Net

11

For comparison with the state-of-the-art, we re-train several SOTA methods
on the same dataset described above. The chosen methods we compare with are
AWNet [6], PyNET CA [20] and CSANet [14]. All these strategies are improve-
ments over PyNET [16] and the best reported results in Mobile ISP contests
[18,15]. All three methods were trained using our dataset following the technical
recommendations, parameters, and loss functions described by them. AWNet
and PyNET CA provided code and CSANet was implemented by ourselves. The
strategy of training with monitor data and then using the real-world to fine-tune
was performed for them as well. CRISPnet is also compared against the UNet
with residual bottleneck, which is our network backbone without the global fea-
ture acquisition and white balance branches.

To assess the color rendition accuracy we use the ∆E∗

ab76 metric, which is
the RMS error in the non-linear, perceptually uniform CIELAB space. Perceptual
uniformity means that the distance in the CIELAB space can be directly mapped
to just noticeable differences (JND) between two similar colors. Specifically, one
JND corresponds to ∆E∗
ab76 ≈ 2.3, so that two colors that differ by this value
can just barely be distinguished by a standard human observer. We also adopted
the standard PSNR and SSIM metrics.

Method
CSANET [14]
PYNET CA [20]
AWNET [6]
UNet + Residual Bottleneck
CRISPnet

PSNR ↑ ∆E∗
24.51
25.00
25.70
26.33
27.54

ab76 ↓ SSIM ↑
16.07 0.8631
15.50 0.8561
14.41 0.8947
13.76 0.9130
11.75 0.8911

Table 1. Reconstruction results on Real-World dataset first fine-tuning using Monitor
data.

Table 1 shows quantitively our final result evaluated with real-world data af-
ter fine-tuning. CRISPnet provides a substantial increase in terms of PSNR and
∆E∗
ab76 over our comparisons. We can observe how the other methods suffer to
translate faithful colors and how relying only on PSNR and SSIM does not reflect
the color rendition. In particular the SSIM is too focused on structural similar-
ity and overlooks color discrepancies that are very noticeable in Fig.3. When
only monitor data training is taken into account the metrics are higher overall
(Table 2). For this case, we have a larger dataset, with captures done always in
the same camera positioning settings, and the monitor resolution is lower than
the camera, these facts make the reconstruction task easier. CRISPnet proves to
better exploit these characteristics and outputs better results in all evaluated
metrics. Fig.1 shows qualitatively that our model is almost indistinguishable
from the Ground Truth, while AWNet cannot translate colors precisely.

5.1 Ablation Studies

Tables 1 and 2 show how each proposed improvement enhances the reconstruc-
tion process. Initially, we have a well-known UNet [31] with residual connection

12

M. Souza and W. Heidrich

Method
CSANET [14]
PYNET CA [20]
AWNET [6]
UNet + Residual Bottleneck
CRISPnet

PSNR ↑ ∆E∗
27.05
27.41
27.66
28.34
32.04

ab76 ↓ SSIM ↑
11.43 0.8986
11.06 0.8796
10.84 0.9093
10.25 0.9243
6.36 0.9344

Table 2. Reconstruction results on Monitor dataset. This is considerer in our pipeline
a pre-training step to finally fine-tune for Real-World data.

in the bottleneck. Next, we introduce the white balance (WB) information, Table
3 ablates over not using metadata at all, naively using white balance information
as preprocessing step, and finally using the proposed branch with the UNet with
residual bottleneck. We can observe quantitatively its importance and Fig.4 clar-
ify this even more. Through Table 3 we empirically justify our design choice of
injecting white balancing information at three points. We also show that prepro-
cessing the images with metadata for AWNet [6] is not effective as our proposed
branch – Table 3 makes this clear.

Fig. 4. CRISPnet compared against AWNet, PyNET CA and our backbone
(UNet+Residual Bottleneck). PSNR/∆E∗
ab76.

We also ablate about the importance of training first with monitor data and
then fine-tune with real-world. The additional branches of CRISPnet make it
powerful enough to learn color conditions that can be heavily refined during

Color Rendition ISP Net

13

Method
AWNet [6]
AWNet [6] Preprocess WB
No WB
Preprocess WB
Branch level 1
Branch level 1, 2 and 3
Branch level 1, 2, 3 and 4
Branch level 1, 2, 3, 4 and 5

PSNR ↑ ∆E∗
27.66
28.51
28.34
28.98
29.84
29.86
29.75
29.69

ab76 ↓ SSIM ↑
10.84 0.9093
9.88 0.9113
10.25 0.9243
9.57 0.9236
8.42 0.9261
8.40 0.9281
8.48 0.9264
8.56 0.9256

Table 3. Comparison of different ways to explore the White Balance (WB) information.
AWNet is slightly improved by preprocessing the input with white balance metadata.
However, it still behind our results even before the global semantics branch been added.

fine-tuning. If we train CRISPnet only with monitor data and reconstruct real-
world scene our PSNR, for instance, stays around 23.9 dB, after fine-tuning it
increases beyond 27.5 dB. Table 4 shows what happens when we train straight
using real-world data for both CRISPnet and the UNet with residual bottleneck.
Comparing against Table 1 we can notice that for all metrics the results are
degraded. Therefore, our proposed dataset attached with the fine-tune strategy
enables better output without the requirement of taking thousands of real-world
image pairs.

Method
UNet + Residual
CRISPnet
CRISPnet Fine-tuned

PSNR ↑ ∆E∗
26.03
26.73
27.54

ab76 ↓ SSIM ↑
14.14 0.9038
12.51 0.8746
11.75 0.8911

Table 4. Result on the Real-World data without and with fine-tunning process.

Finally, we observe the impact of global semantics branch and the differ-
ent ways to do it. CRISPnet extracts the global semantics information through
an XCiT transformer model [10], which is a more powerful attention mecha-
nism than traditional spatial or channel attention. Table 5 shows quantitatively
that exploiting global semantics indeed improves the reconstruction quality and
a light transformer-based attention mechanism beats a simple CNN approach
for this purpose. More ablations about different attention mechanisms and how
CRISPnet performs with deeper XCiT models are available in the supplementary
material.

6 Discussion, Limitations, and Future Work

In this work we introduced CRISPnet, the first ISP network designed to im-
prove color rendition by learning from the expertise encoded in legacy ISP

14

M. Souza and W. Heidrich

Monitor

Global Semantics PSNR↑ ∆E∗
Without
29.86
Classification CNN 31.89
32.04
XCiT

Real-World
ab76 ↓ SSIM↑ PSNR↑ ∆E∗

8.40 0.9281
6.55 0.9311
6.36 0.9344

26.80
26.90
27.53

ab76 ↓ SSIM↑
13.01 0.9133
12.62 0.8874
11.75 0.8911

Table 5. Comparison between different global semantics strategies and its contribution.

pipelines. We achieve this by combining a convolutional encoder/decoder ar-
chitecture for the main reconstruction task with both white balancing metadata
and a transformer-based global feature branch. We demonstrate substantial im-
provements in both traditional image metrics (PSNR, SSIM), as well as color
accuracy (∆E∗

ab76).

However, the proposed approach is not without shortcomings. As can be
seen from both the quantitative and the qualitative results, the color rendition is
much improved, but often still not below the noticeable threshold. This indicates
that legacy ISPs are still more complex than the existing network architectures
are capable of reproducing. Furthermore, while our architecture improves color
rendition, it can have sometimes have issues with noise especially in very low
light. As shown in Figure 5, the color improvements usually still outweigh the
poorer noise performance for an overall increased PSNR.

Fig. 5. For very low light images, our method can have more issues with noise than
AWNet. However, the significant improvement in color rendition still results in an
overall increased PSNR for CRISPnet. PSNR/∆E∗

ab76.

Despite the low-light issue we believe that our approach is preferable in many
situations – global color distortions are often significantly more noticeable than
fine scale noise in typical use cases for the images (e.g. when the images are
posted on social media in lower resolution anyways).

In the future it would be interesting to explore how to improve the noise
performance, for example by having both a DSLR reference image to improve
the structural details of the image and a mobile phone ISP image for the color
rendition. Ultimately it would be best if we did not require a reference ISP algo-
rithm at all and could instead learn to reproduce the manual color adjustments
made by skilled professional photographers. We believe the architectural changes
we propose in this paper can also be useful for these types of systems.

Color Rendition ISP Net

15

References

1. Anzagira, L., Fossum, E.R.: Color filter array patterns for small-pixel image sensors

with substantial cross talk. JOSA A 32(1), 28–34 (2015)

2. Cao, Y., Wu, X., Qi, S., Liu, X., Wu, Z., Zuo, W.: Pseudo-isp: Learning pseudo

in-camera signal processing pipeline from a color image denoiser (2021)

3. Chang, M., Li, Q., Feng, H., Xu, Z.: Spatial-adaptive network for single image

denoising (2020)

4. Cho, S.J., Ji, S.W., Hong, J.P., Jung, S.W., Ko, S.J.: Rethinking coarse-to-fine

approach in single image deblurring (2021)

5. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-d
transform-domain collaborative filtering. IEEE Transactions on Image Processing
16(8), 2080–2095 (2007). https://doi.org/10.1109/TIP.2007.901238

6. Dai, L., Liu, X., Li, C., Chen, J.: Awnet: Attentive wavelet network for image isp

(2020)

7. Dai, T., Cai, J., Zhang, Y., Xia, S.T., Zhang, L.: Second-order attention network
for single image super-resolution. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2019)

8. Diakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C.: Resunet-a: A deep
learning framework for semantic segmentation of remotely sensed data. IS-
PRS Journal of Photogrammetry and Remote Sensing 162, 94–114 (Apr 2020).
https://doi.org/10.1016/j.isprsjprs.2020.01.013, http://dx.doi.org/10.1016/j.
isprsjprs.2020.01.013

9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale (2021)
10. El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A.,
Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., Jegou, H.: Xcit: Cross-
covariance image transformers (2021)

11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition

(2015)

12. Heide, F., Rouf, M., Hullin, M.B., Labitzke, B., Heidrich, W., Kolb, A.: High-
quality computational imaging through simple lenses. ACM Trans. Graph. 32(5)
(oct 2013). https://doi.org/10.1145/2516971.2516974, https://doi.org/10.1145/
2516971.2516974

13. Heide, F., Steinberger, M., Tsai, Y.T., Rouf, M., Pająk, D., Reddy, D.,
Gallo, O., Liu, J., Heidrich, W., Egiazarian, K., Kautz, J., Pulli, K.: Flex-
isp: A flexible camera image processing framework. ACM Trans. Graph.
33(6) (Nov 2014). https://doi.org/10.1145/2661229.2661260, https://doi.org/
10.1145/2661229.2661260

14. Hsyu, M.C., Liu, C.W., Chen, C.H., Chen, C.W., Tsai, W.C.: Csanet: High speed
channel spatial attention network for mobile isp. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp.
2486–2493 (June 2021)

15. Ignatov, A., Chiang, C.M., Kuo, H.K., Sycheva, A., Timofte, R., Chen, M.H.,
Lee, M.Y., Xu, Y.S., Tseng, Y., Xu, S., Guo, J., Chen, C.H., Hsyu, M.C., Tsai,
W.C., Chen, C.W., Malivenko, G., Kwon, M., Lee, M., Yoo, J., Kang, C., Wang,
S., Shaolong, Z., Dejun, H., Fen, X., Zhuang, F., Ma, Y., Peng, J., Wang, T.,
Song, F., Hsu, C.C., Chen, K.L., Wu, M.H., Chudasama, V., Prajapati, K., Patel,
H., Sarvaiya, A., Upla, K., Raja, K., Ramachandra, R., Busch, C., de Stoutz,

16

M. Souza and W. Heidrich

E.: Learned smartphone isp on mobile npus with deep learning, mobile ai 2021
challenge: Report (2021)

16. Ignatov, A., Gool, L.V., Timofte, R.: Replacing mobile camera isp with a single

deep learning model (2020)

17. Ignatov, A., Timofte, R., Chou, W., Wang, K., Wu, M., Hartley, T., Gool, L.V.:
Ai benchmark: Running deep neural networks on android smartphones (2018)
18. Ignatov, A., Timofte, R., Zhang, Z., Liu, M., Wang, H., Zuo, W., Zhang, J., Zhang,
R., Peng, Z., Ren, S., Dai, L., Liu, X., Li, C., Chen, J., Ito, Y., Vasudeva, B., Deora,
P., Pal, U., Guo, Z., Zhu, Y., Liang, T., Li, C., Leng, C., Pan, Z., Li, B., Kim,
B.H., Song, J., Ye, J.C., Baek, J., Zhussip, M., Koishekenov, Y., Ye, H.C., Liu, X.,
Hu, X., Jiang, J., Gu, J., Li, K., Tan, P., Hou, B.: Aim 2020 challenge on learned
image signal processing pipeline (2020)

19. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-

tional adversarial networks (2018)

20. Kim, B.H., Song, J., Ye, J.C., Baek, J.: Pynet-ca: Enhanced pynet with channel
attention for end-to-end mobile image signal processing. Lecture Notes in Com-
puter Science p. 202–212 (2020). https://doi.org/10.1007/978-3-030-67070-2_12,
http://dx.doi.org/10.1007/978-3-030-67070-2_12

21. Liang, J., Cao, J., Sun, G., Zhang, K., Gool, L.V., Timofte, R.: Swinir: Image

restoration using swin transformer (2021)

22. Liang, Z., Cai, J., Cao, Z., Zhang, L.: Cameranet: A two-stage framework for

effective camera isp learning (2019)

23. Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M.: Enhanced deep residual networks
for single image super-resolution. In: The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops (July 2017)

24. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin

transformer: Hierarchical vision transformer using shifted windows (2021)

25. Mao, X., Liu, Y., Shen, W., Li, Q., Wang, Y.: Deep residual fourier transformation

for single image deblurring (2021)

26. Niu, B., Wen, W., Ren, W., Zhang, X., Yang, L., Wang, S., Zhang, K., Cao, X.,
Shen, H.: Single image super-resolution via a holistic attention network (2020)
27. Perez, E., Strub, F., de Vries, H., Dumoulin, V., Courville, A.: Film: Visual rea-

soning with a general conditioning layer (2017)

28. Punnappurath, A., Brown, M.S.: Spatially aware metadata for raw reconstruction.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV). pp. 218–226 (January 2021)

29. Ratnasingam, S.: Deep camera: A fully convolutional neural network for image

signal processing (2019)

30. Remez, T., Litany, O., Giryes, R., Bronstein, A.M.: Deep class aware denoising

(2017)

31. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-

ical image segmentation (2015)

32. Schwartz, E., Giryes, R., Bronstein, A.M.: Deepisp: Toward learning an end-to-
end image processing pipeline. IEEE Transactions on Image Processing 28(2),
912–923 (Feb 2019). https://doi.org/10.1109/tip.2018.2872858, http://dx.doi.
org/10.1109/TIP.2018.2872858

33. Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste,
M., Ghosh, A., Vorozcovs, A.: High dynamic range display systems. ACM Trans.
Graphics pp. 760–768 (2004)

34. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., Jégou, H.: Going deeper

with image transformers (2021)

Color Rendition ISP Net

17

35. Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Loy, C.C., Qiao, Y., Tang,
X.: Esrgan: Enhanced super-resolution generative adversarial networks (2018)
36. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block attention

module (2018)

37. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.: Restormer:

Efficient transformer for high-resolution image restoration (2021)

38. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.:

Cycleisp: Real image restoration via improved data synthesis (2020)

39. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.:
Multi-stage progressive image restoration. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR). pp. 14821–14831
(June 2021)

40. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image

restoration (2017)

41. Zhang, Z., Liu, Q., Wang, Y.: Road extraction by deep residual u-
IEEE Geoscience and Remote Sensing Letters 15(5), 749–753 (May
net.
2018). https://doi.org/10.1109/lgrs.2018.2802944, http://dx.doi.org/10.1109/
LGRS.2018.2802944

