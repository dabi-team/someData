Supervised Learning for Coverage-Directed Test
Selection in Simulation-Based Veriﬁcation

Nyasha Masamba
Faculty of Engineering
Trustworthy Systems Laboratory
University of Bristol
United Kingdom
Email: nyasha.masamba@bristol.ac.uk

Kerstin Eder
Faculty of Engineering
Trustworthy Systems Laboratory
University of Bristol
United Kingdom
Email: kerstin.eder@bristol.ac.uk

Tim Blackmore
Inﬁneon Technologies
Stoke Gifford
Bristol
United Kingdom
Email: tim.blackmore@inﬁneon.com

2
2
0
2

t
c
O
6
1

]

R
A
.
s
c
[

3
v
4
2
5
8
0
.
5
0
2
2
:
v
i
X
r
a

Abstract— Constrained random test generation is one of
the most widely adopted methods for generating stimuli for
simulation-based veriﬁcation. Randomness leads to test diversity,
but tests tend to repeatedly exercise the same design logic.
Constraints are written (typically manually) to bias random
tests towards interesting, hard-to-reach, and yet-untested logic.
However, as veriﬁcation progresses, most constrained random
tests yield little to no effect on functional coverage. If stimuli
generation consumes signiﬁcantly less resources than simulation,
then a better approach involves randomly generating a large
number of tests, selecting the most effective subset, and only
simulating that subset. In this paper, we introduce a novel method
for automatic constraint extraction and test selection. This
method, which we call coverage-directed test selection, is based on
supervised learning from coverage feedback. Our method biases
selection towards tests that have a high probability of increasing
functional coverage, and prioritises them for simulation. We
show how coverage-directed test selection can reduce manual
constraint writing, prioritise effective tests, reduce veriﬁcation
resource consumption, and accelerate coverage closure on a large,
real-life industrial hardware design.
Veriﬁcation,

Learning,
Coverage-Directed Test Generation, Test Selection, Machine
Learning for Veriﬁcation, CDG, EDA

Keywords—Design

Supervised

I. INTRODUCTION

Functional veriﬁcation is the process of ensuring the func-
tional correctness of a hardware design with respect to its in-
tended speciﬁcation [1]. A popular method of performing func-
tional veriﬁcation is simulation-based veriﬁcation. Simulation-
based veriﬁcation is a process in which the design under test
(DUT) is stressed by test stimuli within a simulation engine,
with the resulting DUT behaviour being checked against
expected behaviour according to the speciﬁcation. Deviations
between DUT behaviour and the speciﬁcation indicate possible
faults, referred to as ‘bugs’, in the design logic. Exercised DUT
functionality is recorded in a coverage database. Coverage is
an important measure of veriﬁcation progress and test quality.
Based on analysis of the coverage reports, veriﬁcation effort
can focus on uncovered areas.

Constrained random test generation, illustrated in Figure
1,
test generation method.
is currently the state-of-the-art
Random test generation allows for automatic generation of
diverse test stimuli. Constraining random tests ensures they

Fig. 1: Constrained Random Test Generation

are legal and meaningful with respect to the speciﬁcation.
However, many of the generated tests are redundant in that
they repeatedly explore the same DUT functionality while con-
suming signiﬁcant veriﬁcation resources. Constrained random
tests are generally ineffective at exercising hard-to-reach or
yet uncovered DUT functionality.

A logical unit of DUT functionality that has been identiﬁed
for veriﬁcation and remains unexercised is called a coverage
hole. Writing constraints that target coverage holes is tedious
work often carried out manually by senior veriﬁcation engi-
neers with deep knowledge of the DUT on a trial-and-error
basis.

Automatic test biasing based on coverage feedback leads to
more effective testing. Coverage feedback consists of coverage
data derived from coverage reports, and previously simulated
test data. Effective tests are those that are legal, meaningful,
and useful at exercising the DUT functionality required to
plug coverage holes. Effective testing signiﬁcantly increases
functional veriﬁcation productivity by freeing engineers from
manually biasing test generation. Coverage is also closed
faster, resulting in faster project completion.

In this paper, we introduce coverage-directed test selection
(CDS) as a novel, machine learning-based [2] approach to
automatic test biasing during simulation-based veriﬁcation.
Although we largely focus on using the method for test
selection, our approach can also be adapted for test generation
through techniques such as constraint extraction using decision
tree classiﬁcation. The CDS method is applicable to any design

DriversCheckersTest GeneratorCoverageTestbenchDUTMonitorCode CoverageOutput SpaceConstrained Random Test StimuliInput SpaceFunctional CoverageBug FrequencyBug RateSimulationBug DataCoverage DataVerification EngineerConstraints 
 
 
 
 
 
in which labelled coverage feedback data can be presented in
a tabular structure (i.e., with features as columns and rows as
training examples), and where simulation can be decoupled
from coverage collection.

This paper is structured as follows. Section II introduces
related research, and how the limitations of the research mo-
tivated this work. Section III provides a detailed introduction
to CDS. Section IV describes the environment in which the
CDS experiments were conducted. Section V presents and
discusses the results derived from performing CDS using a
variety of supervised learning algorithms. Section VI presents
suggestions for future research directions and concludes the
paper.

II. RELATED WORK

Coverage-Directed Test Generation (CDG) [3] provided an
early attempt at using artiﬁcial intelligence (AI) to automate
test biasing during simulation-based veriﬁcation. CDG meth-
ods, exempliﬁed by [3], [4], use AI techniques to build a model
mapping test generation parameters, to the coverage exercised
by generated stimuli. The model is used to bias test generation
towards plugging coverage holes to achieve faster coverage
closure.

The approach in [3] utilised a Bayesian Network to learn the
relationship between test generation constraints and coverage,
then subsequently querying the Bayesian Network for new
constraints that have a high probability of exercising a given
coverage point. Although the results showed that the CDG
method could indeed accelerate coverage progress, it was also
clear that the method required signiﬁcant domain knowledge
to be encoded into the model.

Encoding domain knowledge into an AI model is largely a
manual endeavour which has to be undertaken by veriﬁcation
engineers who understand the design. Therefore, the problem
of veriﬁcation engineer time and effort being taken by manual
the time and
work still remains. It could be argued that
effort expended during the model building phase is a one-off
cost. But if the design changes signiﬁcantly across projects
– a likely outcome given the ever-expanding complexity and
functionality of modern hardware – then a large part of that
manual work has to be done again.

Building an AI model mapping coverage feedback to test
stimuli is generally known to be difﬁcult in practice. This
can be due to several reasons, such as a lack of positive
training examples; different abstraction levels between test
stimuli and microarchitectural behaviour; lack of a suitable
distance metric; and the lack of suitable or adequate positive
training examples.

In this paper, we adapt CDG for biasing test selection,
while addressing several of its shortcomings. We ﬁrst solve the
problem of lack of labelled training data by converting simu-
lation trace data from an unlabelled into a labelled state. We
proceed to model relationships between simulated test stimuli
and coverage using supervised learning. This generalises the
model building process to be performed by any classiﬁer of
choice and removes the need to extensively encode domain

knowledge into the model structure. Relationships between test
stimuli and coverage are learnt automatically despite different
abstraction levels. We use coverage groups derived from the
veriﬁcation plan to approximate distance between coverage
points, and to mitigate problems that would have arisen due to
the lack of positive training examples when targeting coverage
holes.

III. COVERAGE DIRECTED TEST SELECTION

Coverage-directed test selection (CDS) focuses on selecting
a small subset of pre-generated test stimuli based on the
constraints learnt by the CDS engine. This necessitates that
a large set of test stimuli have already been generated, for
example through a constrained random process.

If test stimuli generation takes a small amount of time and
computation, while simulation takes an exponentially longer
time plus signiﬁcantly more computation, it is beneﬁcial to
generate a large number of test stimuli, then subsequently bias
selection towards the most promising tests. CDS is therefore
ideal for veriﬁcation environments where test generation is
‘cheap’, while simulation is ‘expensive’.

A. Constraint Extraction Theory

At the core of the CDS engine is a constraint extraction
process during which constraint learning and probability esti-
mation are performed for two possible outcomes, namely exer-
cising target coverage, versus not exercising target coverage.
The constraint extraction process takes previously simulated
test stimuli and coverage data as inputs.

There are two main aspects of constraint extraction that
require special attention: labelling based on coverage, and
constructing training sets. We now discuss each of these in
turn.

1) Labelling: Labelled data are required to train the clas-
siﬁcation model. A major issue immediately arises because
the target coverage points have never been exercised before,
meaning that positive training examples are not present in the
data.

To mitigate this lack of positive training examples, we
ﬁrst group the functional coverage space into non-overlapping
coverage groups. Coverage grouping enables us to label the
training set based on the coverage groups, and also build
coverage feedback models at the coverage group level. We
derive coverage groups through the coverage hole analysis
method of partitioning [5]. Partitioning divides the coverage
space into manageable partitions based on information readily
obtained the veriﬁcation plan. After identifying target coverage
groups, each group’s exercising test stimuli can be labelled as
positive examples.

The CDS engine can automatically inspect a coverage report
to identify target coverage groups containing coverage holes.
To ensure adequate training data is obtained, only coverage
groups exercised by a speciﬁed minimum number of test
stimuli are considered as target coverage groups.

conﬁguration ﬁelds, which control the device’s behaviour. The
DUT’s features are:

• input interface, with values {MEM, RDR}
• data size, with values {1, 2, 3, 4}
• output active, with values {0, 1}
• data bin, with values {0, ... , 232 − 1}
Broadly speaking, the multi-dimensional region deﬁned by
the conﬁguration ﬁelds and their values forms the coverage
model. This simple DUT’s coverage model would consist
of 6.87x1010 cross-product coverage points if all possible
combinations needed to be veriﬁed. In real designs, coverage
model sizes can be reduced by, for example, largely focusing
on complex functionality that is known to be bug-prone, or by
excluding combinations that should never occur according the
speciﬁcation.

A test is described as a vector of feature values, an example
being (input interface: MEM, data size: 4, output active:
1, data bin: 298). Each test belongs to one of two mutually
exclusive classes known apriori, which we will denote 1 and
0 for positve and negative classes, respectively. The table in
3a shows an encoded training set comprising of 10 training
examples. A training set is constructed for each target coverage
group.

The target coverage group associated with the training
set in Figure 3a contains coverage points that are exercised
when data inputs have been received from the radar receiver
(input interface = 1). In addition, the coverage points tend to
be exercised when output is required (output active = 1), as
is typically the case when the value of data bin is relatively
high.

Figure 3b shows the decision tree induced from the example
training set. The root and internal nodes display the splitting
condition alongside the Gini impurity of the node (gini), the
proportion of training examples remaining in the node relative
to the complete training set (samples), the class frequencies
of the node’s training examples - one for each class (value),
and the node’s predicted class (class). Class frequencies are
important because they can be interpreted as an empirical
estimation of the model’s ability to generalise to unseen data.
The leaf nodes display the Gini impurity of the node, the
proportion of examples used to train the node, and the node’s
predicted class.

Following the decision tree, we ﬁnd that the example test
(input interface: 0, data size: 4, output active: 1, data bin:
298) is predicted to belong to the negative class with 100%
probability. This tells us that it is unlikely to exercise the cov-
erage points within the target coverage group. The implication
of a negative class prediction for a newly generated test is that
it is not selected for simulation.

IV. EXPERIMENTAL SETUP

A. Design Under Test

We evaluated CDS on Inﬁneon Technologies’ Radar Signal
Processing Unit (RSPU) AURIX TC3XX design. The RSPU
is a large, complex and highly conﬁgurable block crucial to

Fig. 2: Original data (left). Original data augmented with
reference data (right).

2) Training Set Construction: Each target coverage group’s
is constructed by adopting Hastie et al.’s [6]
training set
method of transforming problems from unsupervised to su-
pervised learning. This entails augmenting an ‘original’ data
set with ‘reference’ data. We treat stimuli that exercised the
coverage group as ’original’ data generated from speciﬁc,
known constraint distributions. Conversely, we draw a sample
of ’reference data’ of the same size from tests that did not
exercise the coverage group, assuming they are generated from
unknown constraint distributions. We label reference data as
negative examples and pool the data sets.

The sampling step leads to balanced training sets in which
positive and negative examples are equally represented. Sam-
pling from different constraint distributions is likely to facili-
tate for the positive and negative examples to be separable due
to the high density of positive examples in speciﬁc regions of
the feature space. Figure 2 shows an example of original blue
data points sampled from a normal distribution, augmented
with an equal number of reference orange data points sampled
from a uniform distribution. A binary classiﬁer can then be
trained to classify the original data from the reference data.
It can be seen that the separation of positive and negative
examples is generally made possible by data augmentation, but
it is imperfect and prone to some error based on the noisiness
of the data.

B. Constraint Extraction example using a Decision Tree

To further aid understanding, we now provide an example
of the constraint extraction process using classiﬁcation and
regression tree (CART) [7] methodology. In general, a decision
tree is a function, f , that takes a vector, x, of n feature values
{x0, x1, ..., xn−1} as inputs; the function returns a decision, ˆy
(e.g., a class for classiﬁcation, or a real value for regression)
as the output.

Suppose that the example DUT is a small radar signal
processor that either takes inputs from main memory or from
a radar receiver. During processing, the inputs are written to a
speciﬁc register determined by a data bin value and processed
according to their size. If output is required, the processed
results are stored in main memory.

The features for training the decision tree are the DUT’s

20304050607080X20304050607080Y020406080100X020406080100YB. Test Stimuli and Coverage

Each RSPU test is a feature vector of 300 conﬁguration
ﬁelds. Test stimuli for the DUT are generated in an industry-
standard constrained random environment. The DUT is treated
as a black box over which we can control the input stimuli,
and observe outputs in the form of functional coverage data.
The DUT functionality we aim to verify is detailed in the
veriﬁcation plan as coverage groups. There are almost 200
coverage groups of varying size that collectively form the
RSPU’s functional coverage model. We assume that coverage
points are grouped together because they are judged to be
semantically similar by the veriﬁcation team.

C. Metric

The metric we use to assess the success of these experiments
is functional coverage. The functional coverage model contains
about 6,000 whitebox cross-product coverage points.

D. Experimentation Database

Relying on DUT simulation was impractical because it takes
a long time to obtain results. In a real-world setting, achieving
100% functional coverage closure for the RSPU requires
approximately 2 million constrained random test stimuli. It
takes an average of 2 hours to simulate an RSPU test within
the constrained random testbench. Achieving coverage closure
would require continuous consumption of 1,000 EDA licences
for 6 months, alongside several months of effort spent by
veriﬁcation engineers writing constraints.

Instead of direct DUT simulation, the experiments were
expedited through the creation of an experimentation database.
The experimentation database stored test stimuli and coverage
data that could emulate DUT simulation. A golden regression
of approximately 3,000 tests were found to enable optimal
coverage closure on the RSPU’s coverage model. These golden
regression tests were stored in the experimentation database to
ensure coverage closure was possible.

To resemble real-life RSPU veriﬁcation, where there are
relatively more tests that do not contribute to coverage closure,
a further 83,000 constrained random test stimuli were added to
the database. Because they are constrained random tests, they
are also able to exercise coverage on the DUT. However, the
coverage they exercise will generally be easy to reach, hence
we cannot reasonably expect them to exercise new coverage,
particularly during the advanced stages of veriﬁcation when
mostly hard-to-reach coverage points remain. These extra
83,000 stimuli were estimated to be large enough in number
to make the selection problem more realistic by introducing
diversity and redundancy among the tests, but small enough
to enable most experiments to be completed within a day.

Before experimentation began,

the 86,000 tests in the
database were simulated, and the coverage achieved by each
test was saved to a separate table in the database. Relations
were deﬁned between the test and coverage tables to associate
each test to the coverage points it activated. During experi-
mentation, test selection means sampling without replacement
from the 86,000 stimuli. Simulating a test means querying the

(a) Training set

(b) Decision tree

Fig. 3: Decision tree induced from training set

the function of Advanced Driver Assistance Systems (ADAS)
in Inﬁneon’s AURIX family of microcontrollers. The design
functions as a semi-autonomous accelerator for performing
Fast Fourier Transforms (FFT) on input data. The RSPU has
also been used as the DUT in research that focused on novelty-
directed veriﬁcation [8], [9] as a mechanism for test selection
and ordering.

The behaviour of the RSPU is conﬁgured by inputs received
as a set of conﬁguration instructions and data via two input
interfaces. The largest effort to uncover logical errors in the
RSPU design occurs via simulation-based functional veriﬁca-
tion.

inputinterfacedatasizeoutputactivedatabinclass1103091141402,483,63611211,334,29111418,124,58711411,839,38010313200101,00901312,9830110115,768002119,289,8760input_interface <= 0.5gini = 0.5samples = 100.0%value = [0.5, 0.5]class = 0gini = 0.0samples = 30.0%value = [1.0, 0.0]class = 0Truedata_bin <= 725029.5gini = 0.408samples = 70.0%value = [0.286, 0.714]class = 1Falsedata_bin <= 1646.0gini = 0.444samples = 30.0%value = [0.667, 0.333]class = 0gini = 0.0samples = 40.0%value = [0.0, 1.0]class = 1gini = 0.0samples = 10.0%value = [0.0, 1.0]class = 1gini = 0.0samples = 20.0%value = [1.0, 0.0]class = 0database for the coverage points exercised by the test. The
experimentation database allows for tests to be simulated in
any order, and to quickly query for the coverage points relating
to a particular test, without direct DUT simulation. This
enabled rapid experimentation and prototyping of solutions.

E. CDS Classiﬁers

To understand the trade-offs associated with the different
classiﬁers, we distinguish between tree-based and non-tree-
based classiﬁcation. Tree-based classiﬁers depend on decision
trees as the mechanism for making predictions, while non-tree-
based classiﬁers depend on other mechanisms for their pre-
dictions. CDS experiments were implemented using Python3.
All classiﬁer implementations used in the experiments are
available from Scikit-learn [10].

The following decision tree-based classiﬁers were com-

pared:

• Baseline - a ‘dummy’ classiﬁer that makes predictions
in a uniform random manner regardless of the input fea-
tures. A dummy classiﬁer is useful for gauging baseline
performance when comparing multiple classiﬁers.

• DT - decision tree classiﬁer with standard hyperparame-

ters.

• DCDT - depth-constrained decision tree whose maximum
depth was chosen based on the observation that many
cross-product coverage points in the coverage model
tend to encompass 3 conﬁguration ﬁelds. Therefore, the
maximum depth was set to 3. This is an experiment to
determine the effect of integrating domain knowledge into
the model. One way of integrating domain knowledge
into a machine learning model is through hyperparameter
tuning.

• DCRDT - depth-constrained decision tree with ran-
domised splits. Similar to DCDT, the maximum depth for
the induced trees is restricted to 3. But instead of using
an information gain measure to determine which feature
to split on, the algorithm randomly selects a feature to
split on from the set of available features.

• RF - random forest algorithm consisting of several deci-

sion trees.

• GB - gradient boosting algorithm based on decision tree

estimators.

The following non-tree classiﬁers were also compared:
• LR - logistic regression classiﬁer.
• NN - ﬁve-layer neural network.
• NB - Naive Bayes classiﬁer.

F. CDS Procedure

CDS is performed as part of an iterative simulation-based
veriﬁcation process. In the early stages of veriﬁcation, 1,000
tests are randomly selected and simulated per iteration. When
90% functional coverage is reached, which typically takes
roughly 5,000 tests, CDS is activated.

During each CDS iteration, a classiﬁcation model is trained
for each target coverage group. The trained model is used to
select the most optimal test to simulate in the next iteration,

such that the total number of tests simulated in each iteration
is equal to the number of target coverage groups. Since it
typically takes less than 5 minutes to train the models, we
fully re-train the models during every iteration.

No feature selection is performed for the CDS experiments,
and only basic feature engineering is performed. To avoid
increasing the number of features, an ordinal encoding scheme
is applied to the categorical features. Features that can take on
a large range of values are categorised according to powers
of 2, thereby reducing the cardinality of the features and
facilitating better model generalisation.

V. RESULTS

We evaluate the performance of the underlying CDS classi-
ﬁer based on the reduction in the number of tests required to
reach certain functional coverage levels. The more effective
the underlying CDS classiﬁer is at learning from coverage
feedback, the more effective it is at selecting test stimuli that
target coverage holes, thereby resulting in fewer tests being
simulated.

The number of tests required to reach 95%, 98% and 99%
functional coverage were recorded. Performing CDS after
achieving 99% functional coverage was not signiﬁcantly better
than constrained random testing for this particular coverage
model. Because roughly 60 coverage points remain unexer-
cised at that point, we reasonably expect that a veriﬁcation
engineer can intervene and manually bias testing towards
exercising them.

A. CDS using Tree-Based Classiﬁers

We begin by studying the performance of CDS when deci-
sion tree-based classiﬁers are utilised for constraint extraction.
We are interested in these because the resulting constraints can
be interpreted by human beings when required. Their ability
to reduce simulation resource consumption is compared to
random selection, which is analogous to traditional constrained
random testing.

The results from decision tree-based CDS experiments are
summarised in Table I. The most important measures of CDS
classiﬁer performance in the table are the simulations saved
at 99% functional coverage.

The dummy classiﬁer had the worst performance by actually
adding to the average number of simulations performed by
random selection. This is to be expected because the dummy
classiﬁer is conceptually equivalent to random selection.

The standard decision tree algorithm’s performance was
inconsistent, as it can be seen to add more simulation at 98%
coverage, yet at the same time saving simulation at 95% and
99% coverage. This could be due to a well-known issue with
untuned decision trees: they tend to overﬁt to the training data,
making predictions that generalise poorly to unseen data. This
issue is clearly mitigated when the depth of the decision tree
is constrained (DCDT and DCRDT classiﬁers), which is a
known method of combatting overﬁtting.

The best performer among decision tree-based algorithms is
the gradient boosting classiﬁer, reducing the number of stimuli

Functional Coverage
95%
98%
99%
Savings (vs. Random)
95%
98%
99%

Random Baseline

12866
29300
44200

11287
30374
44582

-12.27%
+3.67%
+0.86%

DT
12561
29396
42334

DCDT
11770
27179
41458

DCRDT
10659
26553
40628

RF
12329
28440
44044

GB
10911
25801
39419

-2.37% -8.52% -17.15% -4.17%
+0.33% -7.24%
-6.2%
-4.22%

-15.2%
-2.94% -11.94%
-0.35% -10.82%

-9.38%
-8.08%

TABLE I: Number of tests required to reach given functional coverage levels: Decision Tree CDS techniques vs Baseline and
Random

Functional Coverage
95%
98%
99%
Savings (vs. Random)
95%
98%
99%

Random
12866
29300
44200

LR
10420
27282
42667

NN
10065
24786
37919

NB
9614
22731
35960

-19.01% -21.77% -25.28%
-15.41% -22.42%
-6.89%
-14.21% -18.64%
-3.47%

TABLE II: Number of tests required to reach given functional
coverage levels: non-tree-based CDS techniques vs Random

required to reach 99% functional coverage by almost 11% and
consistently performing better than random selection at every
functional coverage level.

B. CDS using Non-Tree-Based Classiﬁers

For completeness, we also studied the performance of CDS
using a selection of classiﬁers that are not based on decision
trees. The results are summarised in Table II.

The worst performing non-tree-based classiﬁer is logistic
regression, which utilises a linear estimator that is unlikely
to capture the nonlinear complexities in the coverage data. It
is reasonable to expect that the logistic regression algorithm
would be the worst performing out of the three.

The best performing classiﬁer is Naive Bayes, managing
to save 18.64% of tests being simulated when compared
to random selection. However,
this performance comes at
the expense of constraints that are not transparent to human
beings.

Non-tree-based classiﬁers generally achieved higher sav-
ings than tree-based classiﬁers. However, non-tree-based al-
gorithms also tend to be slower to train, and demand more
computational resources. As an example, the neural network
(NN) took 5 days to complete the CDS experiment, whereas
the standard decision tree (DT) completed the experiment
in 1 day. However, in practice, the extra time required by
the neural network might be justiﬁable because it is more
accurate than the decision tree. We can check this with
some quick calculations. The neural network achieved 99%
functional coverage with 4,415 less tests than the decision tree.
For the RSPU, this translates to approximately 9,000 hours
of saved simulation time. Assuming a maximum simulation
capacity of 1,000 tests a day, the neural network saves 9 days’
worth of simulation resources compared to the decision tree.
Accounting for the extra 4 days of training and inference time

required by the neural network still leaves a net simulation
resource saving of 5 days. If the prime objective is to reduce
simulation, a neural network classiﬁer would be preferable to
a decision tree in this case.

VI. CONCLUSION

In this work, we have proposed coverage-directed test selec-
tion as a method for automatic test biasing during simulation-
based veriﬁcation. Coverage-directed test selection uses ma-
chine learning on coverage feedback to identify and prioritise
test stimuli that target coverage holes. Coverage-directed test
selection adds value to simulation-based veriﬁcation through
gains in testing efﬁcacy (targeted stimuli) and testing ef-
ﬁciency (reduced simulation). These enhancements lead to
faster functional coverage closure, higher quality test stimuli
and reduced manual effort throughout the simulation-based
veriﬁcation process.

According to the experimental results, this particular RSPU
coverage model could be nearly closed by simulating 18% less
tests when using coverage-directed test selection. Scaling to
RSPU veriﬁcation in production, this could mean simulating
1.7 million, instead of 2 million tests, to achieve coverage
closure. Note that the golden regression would still consist of
3,000 tests, hence there is still signiﬁcant redundancy in the
coverage-directed test selection. We are working to improve
feature selection, feature engineering, and dimensionality re-
duction during the constraint extraction process to improve
these results.

Although coverage is a useful measure of veriﬁcation
progress, a functional coverage model is a somewhat sub-
jective approximation of the design behaviour that must be
explored. Because of the subjectivity inherent in the coverage
model, and because bugs can hide anywhere in the design,
there is value in aiming to explore beyond the coverage model.
Future research will focus on further optimising test se-
lection by combining coverage-directed test selection method
with novelty-driven test selection [9]. In contrast to coverage-
directed test selection, novelty-driven test selection has been
found to be useful at exploring more of the design by pri-
oritising relatively novel tests for simulation. We expect that
complementing coverage-directed test selection with novelty-
driven test selection will result in improvements in the test
selection capabilities of both methods.

REFERENCES

[1] B. Wile, J. C. Goss, and W. Roesner, Comprehensive Functional
Veriﬁcation: The Complete Industry Cycle. Morgan Kaufmann, San
Francisco, 2005.

[2] P. Flach, Machine Learning: The Art and Science of Algorithms that

Make Sense of Data. Cambridge University Press, 2012.

[3] S. Fine and A. Ziv, “Coverage directed test generation for functional
veriﬁcation using bayesian networks,” in Proceedings of the 40th Design
Automation Conference, 2003, pp. 286 – 291.

[4] K. Eder, P. Flach, and H. Hsueh, “Towards automating simulation-
based design veriﬁcation using ILP,” in Inductive Logic Programming,
S. Muggleton, R. Otero, and A. Tamaddoni-Nezhad, Eds.
Berlin,
Heidelberg: Springer Berlin Heidelberg, 2007, pp. 154–168.

[5] O. Lachish, E. Marcus, S. Ur, and A. Ziv, “Hole analysis for functional
coverage data,” in Proceedings 2002 Design Automation Conference
(IEEE Cat. No.02CH37324), June 2002, pp. 807–812.

[6] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning, 2nd ed. Springer Science+Business Media, New York, 2009.
[7] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classiﬁcation and

Regression Trees. Wadsworth, 1984.

[8] X. Zheng, “Advanced stimulus for coverage closure,” Master’s thesis,

2019.

[9] T. Blackmore, R. Hodson,

and

S.

close

coverage,”

veriﬁcation: Using machine
learning to identify novel
of
and
and Veriﬁcation Conference (Virtual), 2021.
https://www.dropbox.com/s/iulpk8kba3f7xxn/DVCon%20US%202021
Proceedings-FINAL.zip?dl=0&ﬁle subpath=%2FPapers%2F7060.pdf

in Proceedings

“Novelty-driven
stimuli
2021 Design
the
[Online]. Available:

Schaal,

[10] Scikit-learn. (2021) Machine learning in Python. [Online]. Available:

https://scikit-learn.org/stable/index.html

