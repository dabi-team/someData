Deep API Learning Revisited

James Martin
james.martin3@mail.mcgill.ca
McGill University
Montréal, Canada

Jin L.C. Guo
jguo@cs.mcgill.ca
McGill University
Montréal, Canada

2
2
0
2

y
a
M
3

]
E
S
.
s
c
[

1
v
4
5
2
1
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Understanding the correct API usage sequences is one of the most
important tasks for programmers when they work with unfamiliar
libraries. However, programmers often encounter obstacles to find-
ing the appropriate information due to either poor quality of API
documentation or ineffective query-based searching strategy. To
help solve this issue, researchers have proposed various methods
to suggest the sequence of APIs given natural language queries
representing the information needs from programmers. Among
such efforts, Gu et al. adopted a deep learning method, in particular
an RNN Encoder-Decoder architecture, to perform this task and
obtained promising results on common APIs in Java. In this work,
we aim to reproduce their results and apply the same methods for
APIs in Python. Additionally, we compare the performance with
a more recent Transformer-based method, i.e., CodeBERT, for the
same task. Our experiment reveals a clear drop in performance
measures when careful data cleaning is performed. Owing to the
pretraining from a large number of source code files and effective
encoding technique, CodeBERT outperforms the method by Gu et
al., to a large extent.

CCS CONCEPTS
• Software and its engineering → Reusability.

KEYWORDS
API, deep learning, RNN, RoBERTa, Transformer, API usage

ACM Reference Format:
James Martin and Jin L.C. Guo. 2022. Deep API Learning Revisited. In 30th
International Conference on Program Comprehension (ICPC ’22), May 16–
17, 2022, Virtual Event, USA. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3524610.3527872

1 INTRODUCTION
When working with unfamiliar APIs, programmers frequently re-
sort to learning resources and code examples to understand API
usage sequences [26]. Programmers normally start with forming
queries based on their information need, e.g., “JSON serialize an
object”, and then search the API documentation or websites such as
StackOverflow [1] to identify answers to their queries or to similar

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ’22, May 16–17, 2022, Virtual Event, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3527872

import sys
from os.path import dirname, join as join_path

def sys_path():

""" Add `./third_party` to `sys.path`.
"""

dir = join_path(dirname(__file__), 'third_party')
if not dir in sys.path:

sys.path.insert(1, dir)

(cid:119)(cid:127)

Description: Add `./third_party` to `sys.path`.
os.path.join,
os.path.dirname,
API
sequence:
sys.path.insert

Figure 1: An example of a description and API sequence pair
extracted from a Python function1. The description is con-
sidered as a proxy of programmers’ query while the API se-
quence is the expected output for this learning task.

questions. This process can be long and ineffective when the queries
represent more complex API usage, normally involving a sequence
of API calls from the target library, either due to the incompleteness
of the documentation, or the mismatch between the search queries
and the descriptions or discussions of the relevant APIs.

Meanwhile, numerous previous work has proposed to use learning-

based approaches to support programmers for various tasks. Consid-
ering that code exhibits repetitive patterns such as the use of iden-
tifier names and idioms, and even the appearance of bugs [3, 5, 13],
learning-based approaches have been adopted to support learning,
writing, and fixing programs [9, 10, 12]. In particular, a “deep API
learning” method proposed by Gu et al. [12] represents a recent
major progress on the task of suggesting API sequences based on a
given query and has been broadly cited since its publishing. Adopt-
ing an RNN encoder-decoder architecture, the deep API learning
method maps natural language queries to sequences of Java Plat-
form API calls to help programmers implement certain functions in
Java. Considering the demonstrated performance and substantial
impact of this work, we adapt this technique to APIs from common
libraries in Python, one of the most used programming languages.
Starting from Python source code, we can use a similar technique
to formulate the API sequence learning task as shown in Figure 1.

1An actual example from https://github.com/00/wikihouse/blob/master/patch.py

 
 
 
 
 
 
ICPC ’22, May 16–17, 2022, Virtual Event, USA

James Martin and Jin L.C. Guo

As a reproducibility study, our work investigates if a similar per-
formance in terms of BLEU scores can be obtained for our curated
Python dataset. Our experiment reveals that the performance on
Java and Python datasets is comparable if a duplication removal
step is performed on both datasets.

Furthermore, since the deep API learning work was published,
more advanced techniques have been employed for code and natu-
ral language representation learning. For example, the work Code-
BERT proposed by Feng et al. [9], using a transformer-based model
and a pretraining phase with a large number of code-natural lan-
guage pairs, demonstrated an improved performance on several
downstream tasks that require reasoning across natural language
and programming languages. Its potential on the task of API se-
quence generation, however, is yet unknown. Therefore, we also
include CodeBERT in our investigation to compare with the original
deep API learning approach. We aim to understand if API sequence
learning can also benefit from a transformer-based architecture and
pretraining tasks on a large dataset. We observe that CodeBERT
indeed performs better by a large margin on both the Python and
Java datasets. The improvement over the RNN based model is still
substantial for Python even without the pretraining step.
In summary, we make the following contributions:
(1) Our experiment results have validated the reproducibility of
the work by Gu et al. [12] for the task of API learning on the
original Java dataset and our newly curated Python dataset;
(2) Our study reveals the importance of data inspection and how
the quality of the dataset might impact model performance
and its interpretation;

(3) The comparison with the CodeBERT model shows a clear
potential for adopting transformer-based models for the API
learning task.

(4) The Python dataset we have curated is shared as supplemen-

tary material for future research in this direction.2

2 RELATED WORK
2.1 Machine Learning for Code
Since the early work revealing the naturalness of source code [13],
numerous efforts have been proposed to accumulate and learn from
large datasets of source code and peripheral artifacts for various
programming related tasks. The tasks normally either focus on the
code itself, such as code completion [22], bug detection [16], and
program repair [27], or focus on the relationship between code
and natural language and code written in a different programming
language, such as commit message generation [15], tracing source
code with issues [21], code summarization [25]. We suggest that
interested readers refer to surveys by Allamanis et al. [3] and by
Devanbu et al. [7] to get a more comprehensive picture. Among such
a variety of machine learning for code applications, in particular
adopting deep learning based approaches, we focus on connecting
natural language descriptions of code functions to a subset of source
code, in particular, API calls, an idea first proposed by Gu et al. [12].
Recent work on machine learning for code has a clear emphasis
on some sort of self-supervision to pretrain the models. For example,
the work of CodeBERT was pretrained using two self-supervised

2Datasets are available at https://doi.org/10.5281/zenodo.6388030. The code that was
used to curate the dataset is available at https://github.com/hapsby/deepAPIRevisited.

objectives across natural language and source code [9]. Gu et al.
[11] propose to assemble CodeBERT and other similar “foundation”
models to perform the code summarization task. Allamanis et al.
[4] performs the self-supervision on bug detection and repair by
training two models at the same time, i.e., a selector model that
injects bugs in the code and a detector model that detects and fixes
the bug. In this work, we adopt the pretrained CodeBERT model
for the API learning task. We also explicitly investigate the impact
of the pretraining step on the rate at which the CodeBERT model
converges when trained with the Python API dataset.

2.2 Programmers’ API Learning Support
Previous work on supporting programmers to learn API usage
can be categorized into three groups. One line of work focuses on
augmenting existing API documentation from external resources
such as StackOverflow [23]. Since the official documentation nor-
mally suffers from quality problems such as incompleteness, out-
of-datedness, and correctness [2], external resources can bring ad-
ditional insights to the programmers for the target API. The second
direction is on improving code example search. By comparing the
representation of search queries with code snippets, the objective
of such work is to recommend the most relevant code examples [6].
The third direction is based on generative machine learning meth-
ods using which the search queries from the programmers can be
“translated” into source code or API sequences. The work by Gu
et al. [12] upon which our study is built can be put in the last direc-
tion. While the current machine learning models such as CodeBERT
might be able to generate the entire code sequence, we select the
task with a focus on API sequence considering its clear implication
on programmers’ learning objective.

3 EXPERIMENT DESIGN
3.1 Data Collection
The data set for training and evaluating learning based models
consists of (desc, apiseq) pairs, where desc is a natural language
description of a task, representing the queries that programmers
might use, and apiseq is a sequence of calls to API or library func-
tions. While there are existing datasets of code and natural language
pairs such as the CodeSearchNet [14], there is no existing dataset
for Python that connects the natural language description to iso-
lated API calls, to the best of our knowledge. Obtaining such a
dataset is non-trivial and requires the resolution of import aliases;
therefore we curated from GitHub ourselves. We first downloaded
257,049 open source Python projects from GitHub and extracted
each top-level function and class method from these projects. We
then converted each to a (desc, apiseq) pair. We discuss the detailed
process below.

3.1.1 Projects Selection. We started with identifying all projects
tagged as Python projects on GitHub. We then ranked them based
on stars and selected only the projects that have at least five stars
indicating obtaining certain popularity. We then removed all the
forks from other projects to avoid duplicated projects. We also
filtered projects with a size bigger than 300 MB due to limited
disk space and bandwidth. For the 257,049 projects that remained
after filtering, we cloned the latest revision of the default branch

Deep API Learning Revisited

ICPC ’22, May 16–17, 2022, Virtual Event, USA

without submodules for further processing in the next step. GitHub
did not offer a way to distinguish between Python 2 and Python
3 projects, so projects written in both versions of Python were
collected. However, if a particular source file did not contain valid
Python 3 syntax, the source file was rejected in a later step.

In the original deep API learning experiment, projects were
downloaded with at least one star; we chose to require more stars,
because the number of Python projects available on GitHub greatly
exceeded the number of projects that we were able to download in
our timeframe.

3.1.2 Descriptions Extraction. We used the docstring for a Python
function, if it exists, to extract its description. Each description is
the concatenation of the “primary description” and the “returns
description” – The “primary description” is the beginning con-
tent of the docstring until either an empty line or a line starting
with “param”, “params”, “parameter”, “parameters”, “return”, or
“returns” (with possible decorating colons); The “returns descrip-
tion” is the part of the docstring the first line of which begins with
“return” or “returns” (with possible decorating colons) until a
blank line or a line beginning with “param”, “params”, “parameter”,
or “parameters”. In the latter case, the decorating colons are re-
moved and the word is normalized to “return”.

When the docstring is not available, we extracted the descrip-
tions from the function name. Underscores in the function name
were turned to spaces, and spaces are inserted before uppercase
letters, which are then lowercased. For example, the function names
“do_something” and “doSomething” both become “do something”.
The extraction was performed using the ast module in Python
3.8.8 to parse every file with a .py filename extension found in
each project, in total 9,292,645 such files. During this process, we
removed 345,253 files due to character encoding errors, and 610,892
files due to contained syntax errors, including errors on Python 2
syntax that cannot be recognized by Python 3’s ast module.

In the original deep API learning experiment, the description of
a function was extracted from the first sentence of the function’s
JavaDoc comment, and functions with no JavaDoc were ignored.
Our extraction procedure differs from the original to compensate for
our smaller corpus of projects (257,049 projects instead of 442,928
projects) and the relative sparsity of docstrings in Python code as
compared to JavaDoc comments in Java code.

3.1.3 API Sequence Extraction. To extract the API call sequence
from the function body, we first processed the import statement in
each python source code file. When an import statement is encoun-
tered, we recorded the symbols defined by the import statement
along with the appropriate replacement string, unless:

• it is a relative import, e.g., “from ..x import y”;
• the module being imported is part of the project’s source
code, e.g., “import a.b” when either the file “a/b.py” or
the file “a/b/__init__.py” exists; or

• it is a wildcard import, e.g., “from x import *”.

In the first two cases, the import is ignored because calls to functions
in these modules are not considered to be API calls. We also ignored
the last case considering the complexity of matching API calls; it
could be added in future versions.

Once the module name is extracted, we searched the body of
each function for all function calls to the imported modules. Each
function call consists of one or more identifiers joined by dots, fol-
lowed by a list of arguments: for example, x.y.z(). If the function
call matches a top-level function that was defined earlier in the file
or a local function that was defined earlier in the function, then
the function call was replaced with the list of API calls extracted
from that earlier function. Otherwise, we checked whether the first
identifier (e.g., the identifier x in the function call x.y.z()) was
defined by any target import statements for the file. If so, then the
first identifier was replaced according to the import rule, and the
function call is then considered as an API call.

We identified all the API calls for each function and listed them
in the order in which the close-parentheses of their argument lists
appear in the source code: for example, if the source code reads
f(g(),h()) then the function calls are listed as g, h, f. If no API
calls were extracted from a function, we removed that function
from further analysis. Among 68,048,224 functions, we extracted
28,493,702 instances of API sequences.

3.1.4 Data Filtering. From the initial dataset of (desc, apiseq) pairs,
we further filter the instances to improve its quality and its applica-
bility to the API learning task. In particular,

• we remove all pairs with more than 14 API calls, because the
Deep API project truncates sequences to 28 tokens, and API
calls each require at least two tokens. This leaves 27,296,964
pairs;

• we sort all Python modules in descending order of popularity
(determined by the total number of calls to functions in the
module) and, in that order, we add each module to a list of
accepted modules if the total number of identifiers in the API
calls of this and all previously accepted modules does not
exceed 9,995. Then we remove all pairs that contain at least
one API call to a module that is not in the list of accepted
modules. This guarantees that the output vocabulary, which
consists of the identifiers together with the “.” separator and
four other special tokens, does not exceed 10,000, which is
the size of the RNN network’s output vector and is therefore
the maximum vocabulary size. This step leaves just 7,505,321
pairs;

• we remove all duplicate pairs, which leaves 1,655,645 pairs;
• we remove all pairs with descriptions smaller than 3 words,

which leaves 1,087,673 pairs; and

• we remove all pairs with “test” in the function name or

description.

The final dataset is comprised of 855,107 (desc, apiseq) pairs for
Python programs. We set aside 4% of the pairs to use as the test set
and the remaining 820,967 pairs as the training set for training the
deep learning models on the API learning task.

3.2 Model Selection
Two neural network models were selected in our experiment. We
considered the RNN Encoder-Decoder model because it was pro-
posed by Gu et al. [12] to perform API sequence learning for Java
in their original work. We also investigated the CodeBERT model

ICPC ’22, May 16–17, 2022, Virtual Event, USA

James Martin and Jin L.C. Guo

considering its broad impact and wide availability through GitHub 3
and HuggingFace API 4. We describe how we trained and fine-tuned
the models in detail below.

3.2.1 RNN Encoder-Decoder. We have adapted the implementation
of the Deep API project by Gu et al. [12] to the task of generating
Python API sequences for given queries. This model has an RNN
encoder that accepts embedded descriptions, a linear layer, a tanh
layer, and an RNN decoder that outputs embedded API sequences.
The RNN encoder has an input size of 120 (the size of an embed-
ded word) and one bidirectional hidden layer with 2000 features
in total. The linear layer reduces the 2000 features from the RNN
encoder to 1000 features. The RNN decoder has one hidden layer
of 1000 features, an attention layer, and an output size of 10,000
which is our vocabulary size.

We trained this model in batches of 100 pairs. Porter’s stem-
mer [20] was used to pre-process the input tokens before encoding.
The AdamW algorithm [18] was used to optimize the model pa-
rameters, with a learning rate 𝛼 = 10−4 and 𝜖 = 10−8. The loss
function was the cross-entropy between the actual apiseq and the
apiseq predicted by the model. After every 1000 batches, the model
was evaluated with the test data set: the actual apiseqs from the
test set were compared to the apiseqs predicted by the model. We
adopt the same metric as in the original deep API learning work by
Gu et al. [12], i.e., BLEU-4 score [19] that is the BLEU score taking
into account 𝑛-grams up to 𝑛 = 4.

For performance reasons, only 100 test pairs were used for each
evaluation of the BLEU-4 score shown in Figures 2 and 3. The final
BLEU-4 scores in Table 1 were computed with 10,000 pairs from
the original test set, except for the deduplicated Java API test set
which has only 2,441 pairs. Each RNN model took approximately
six days to train using an Intel® Core™ i5-3570K (3.40GHz) CPU
and a NVIDIA® GeForce GTX™ 1070 GPU with 8 GiB VRAM.

3.2.2 CodeBERT. We have adapted the code of the CodeBERT
project [9] to the task of generating Python and Java API sequences
based on given queries. CodeBERT model reuses the model architec-
ture of RoBERTa [17] and is pretrained with two objectives directly
connecting the natural language and source code modalities:

• A “Masked Language Modeling (MLM)” objective, where
the model is given a natural language sequence and a code
sequence with 15% of the tokens replaced by a special [MASK]
token, and the model infers what the original tokens were;
and

• a “Replaced Token Detection (RTD)” objective, where the
model is given a natural language sequence and a code se-
quence with some of the tokens replaced by random tokens
from the vocabulary, and the model determines which tokens
were replaced.

The code sequences used to pretrain the model were from Code-
SearchNet [14], a large dataset drawn from open source projects
hosted on GitHub in six languages: Go, Java, JavaScript, PHP, Python,
and Ruby.

We fine-tuned the pretrained model with our training set of
Python (desc, apiseq) pairs. The model was trained in batches of

3https://github.com/microsoft/CodeBERT
4https://huggingface.co/microsoft/codebert-base

Figure 2: The cross-entropy loss and the BLEU-4 score
achieved during the training of the RNN Encoder-Decoder
with the Python API training set. Each BLEU-4 score is eval-
uated using a sample of 100 pairs from the testing set.

Figure 3: The cross-entropy loss and the BLEU-4 score met-
rics achieved during the training of the same RNN Encoder-
Decoder with the Java API training set provided by Gu et al.
[12], both with and without duplicates removed. The BLEU-
4 score achieved using the original test set is shown in red
and using deduplicated test set in blue (a sample of 100 pairs
in each case).

96 pairs. To ensure that the words in the task descriptions match
the words that were used in the pretraining, we used the default
RoBERTa tokenizer instead of the Porter stemmer. The loss function
was the cross-entropy function. After every 1000 batches, the model
was evaluated by computing the BLEU-4 score of our test set.

We additionally experimented with CodeBERT model initialized
with random parameters – we directly trained the CodeBERT model
with our Python (desc, apiseq) dataset. By comparing the model
performance in both settings, we aim to understand the impact
of the pretraining step with a larger dataset using self-supervised
objectives.

050000100000150000200000250000Batch #012345678Cross-entropy loss05101520253035BLEU-4 scoreMetrics while training Deep API with Python data050000100000150000200000250000300000Batch #012345678Cross-entropy loss05101520253035BLEU-4 scoreMetrics while training Deep API with Java dataDeep API Learning Revisited

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 1: A summary of the top-1 BLEU-4 scores achieved
with the various datasets and models tested. The last column
lists the outcomes when duplicates of the training pairs are
not removed from the test set.

10,000 pairs from the Python API test data set were used in the
evaluation of each BLEU-4 score. All 2,441 pairs from the dedupli-
cated Java API test data set were used. The Python API CodeBERT
model was trained for approximately 21 hours using an AMD®
EPYC™ (2.50 GHz) CPU and a NVIDIA® RTX™ A6000 GPU with
48 GiB VRAM. The Java API CodeBERT model was trained for
approximately 60 hours on the same machine.

4 EXPERIMENT RESULTS
Our experiment is set up to answer the following research ques-
tions:

- RQ1: To what extent can we reproduce the deep API learning

results from Gu et al. [12] on the Java dataset?

- RQ2: Can the RNN Encoder-Decoder model achieve similar per-
formance on the API learning task for Python compared with for
Java?

- RQ3: To what extent can the CodeBERT model compete with the

RNN Encoder-Decoder model on the API learning task?

- RQ3.1: To what extent does the CodeBERT model benefit from

the self-supervised pretraining step.

In this section, we discuss our findings on each research question

in detail. These results are summarized in Table 1.

4.1 Reproducing the Deep API Learning

Results (RQ1)

As a reproducibility study, our first step is to understand the extent
to which we can reproduce the deep API learning results from Gu
et al. [12] on the Java dataset used in the original work (RQ1). The
Deep API source code provided by Gu et al. [12] is available in two
versions: an older implementation using Theano 5, and a newer
implementation using PyTorch 6. The Theano implementation is
exactly what was used in Gu et al. [12]. We trained the Theano im-
plementation with the data provided by Gu et al. [12] and achieved
a top-1 BLEU-4 score of 54.71%, consistent with that paper.

However, when we created the Python dataset, we noticed a
large number of duplicated instances that cannot be removed on
the project level (see Section 3.1.4). This might be due to the reuse
and clones of source code within and across projects. We then
performed duplicate detection on the Java dataset released by the
previous work and had a similar observation. The Java training
set with 7,475,850 (desc, apiseq) pairs becomes 1,880,472 pairs after

5https://github.com/Theano/
6https://pytorch.org/

removing duplicates. The Java testing set with 10,000 pairs becomes
2,441 after removing pairs that are also found in the training set. The
experiment was repeated, with one million iterations, and the BLEU-
4 scores achieved were 20.97% (top-1), 32.42% (top-5), and 36.99%
(top-10). This dramatic difference is due to the overlap between the
training set and the test set in the previous work. In our remaining
experiment, we decide to compare with the BLEU-4 scores on the
dataset after removing duplicates, as they more accurately reflect
the model’s capacity to generalize on unseen queries.

4.2 RNN Encoder-Decoder Model Performance

(RQ2)

After establishing the baseline performance on the Java dataset, we
used the PyTorch implementation developed by Gu et al. [12] to
train the model with our Python data to answer our RQ2. The BLEU-
4 scores achieved are shown in Figure 2. The model converged
quickly and continued to improve with training with more batches.
The BLEU-4 score achieved on the testing set was 14.14% with the
top-1 output API sequence.

We also used the PyTorch implementation to train the original
Java API dataset (with duplicates removed). The results are shown
in Figure 3. The BLEU-4 score achieved was 12.41%, which is less
than the score of 20.97% reported in Section 4.1; possible reasons
for this are discussed in Section 5.

Figure 3 also shows the result of training the model with the
original data set before the duplicates were removed. The BLEU-4
score achieved was 38.26%.

Our experiments indicate that the same model architecture per-
forms similarly for Python and Java. At the same time, the evalua-
tion of the model’s performance is greatly affected by the quality
of the test set. When it makes inferences on unseen queries, its
performance is much weaker than on a collection of queries with
entries from the training set.

4.3 CodeBERT Performance (RQ3)
Our RQ3 aims to understand the potential of using a more recent ap-
proach, CodeBERT, on this task, compared to the method proposed
by Gu et al. [12]. The performance of fine-tuning the pretrained
CodeBERT model on the task of API sequence generation is shown
in Figure 4 and Figure 5. The maximum BLEU-4 score achieved
on the Python dataset was 41.56%, largely improved compared
with 14.14% achieved by the RNN Encoder-Decoder model. On
the Java dataset, the maximum BLEU-4 score was 24.17%. In com-
parison, the performance by the RNN Encoder-Decoder model on
Java is only 12.41% (using the Pytorch implementation). Such im-
provement might be attributed to the model architecture, i.e., trans-
former [24] versus RNN based models, and/or the pretraining with
self-supervised objectives on a larger and potentially overlapped
dataset. To understand the impact of each factor, we performed two
additional analyses, one on the dataset overlapping and the other
on model performance without pretraining.

The CodeSearchNet dataset, on which the CodeBERT model
is pretrained, was collected from GitHub, the same source as our
Python dataset. It is possible that CodeSearchNet has seen a func-
tion during its pretraining step from which our test instance is
constructed. To understand the overlap, we compare the desc and

DeepAPILearningRevisitedICPC’22,May16–17,2022,VirtualEvent,USAPythonAPIJavaAPIJavaAPI(dups)RNN(Theano)—20.97%54.71%RNN(PyTorch)14.14%12.41%38.26%CodeBERT41.56%24.17%—CodeBERT(nopretraining)35.52%——Table1:Asummaryofthetop-1BLEU-4scoresachievedwiththevariousdatasetsandmodelstested.Thelastcolumnliststheoutcomeswhenduplicatesofthetrainingpairsarenotremovedfromthetestset.10,000pairsfromthePythonAPItestdatasetwereusedintheevaluationofeachBLEU-4score.All2,441pairsfromthededupli-catedJavaAPItestdatasetwereused.ThePythonAPICodeBERTmodelwastrainedforapproximately21hoursusinganAMD®EPYC™(2.50GHz)CPUandaNVIDIA®RTX™A6000GPUwith48GiBVRAM.TheJavaAPICodeBERTmodelwastrainedforapproximately60hoursonthesamemachine.4EXPERIMENTRESULTSOurexperimentissetuptoanswerthefollowingresearchques-tions:-RQ1:TowhatextentcanwereproducethedeepAPIlearningresultsfromGuetal.[12]ontheJavadataset?-RQ2:CantheRNNEncoder-Decodermodelachievesimilarper-formanceontheAPIlearningtaskforPythoncomparedwithforJava?-RQ3:TowhatextentcantheCodeBERTmodelcompetewiththeRNNEncoder-DecodermodelontheAPIlearningtask?-RQ3.1:TowhatextentdoestheCodeBERTmodelbene￿tfromtheself-supervisedpretrainingstep.Inthissection,wediscussour￿ndingsoneachresearchquestionindetail.TheseresultsaresummarizedinTable1.4.1ReproducingtheDeepAPILearningResults(RQ1)Asareproducibilitystudy,our￿rststepistounderstandtheextenttowhichwecanreproducethedeepAPIlearningresultsfromGuetal.[12]ontheJavadatasetusedintheoriginalwork(RQ1).TheDeepAPIsourcecodeprovidedbyGuetal.[12]isavailableintwoversions:anolderimplementationusingTheano5,andanewerimplementationusingPyTorch6.TheTheanoimplementationisexactlywhatwasusedinGuetal.[12].WetrainedtheTheanoim-plementationwiththedataprovidedbyGuetal.[12]andachievedatop-1BLEU-4scoreof54.71%,consistentwiththatpaper.However,whenwecreatedthePythondataset,wenoticedalargenumberofduplicatedinstancesthatcannotberemovedontheprojectlevel(seeSection3.1.4).Thismightbeduetothereuseandclonesofsourcecodewithinandacrossprojects.WethenperformedduplicatedetectionontheJavadatasetreleasedbythepreviousworkandhadasimilarobservation.TheJavatrainingsetwith7,475,850(desc,apiseq)pairsbecomes1,880,472pairsafter5https://github.com/Theano/6https://pytorch.org/removingduplicates.TheJavatestingsetwith10,000pairsbecomes2,441afterremovingpairsthatarealsofoundinthetrainingset.Theexperimentwasrepeated,withonemillioniterations,andtheBLEU-4scoresachievedwere20.97%(top-1),32.42%(top-5),and36.99%(top-10).Thisdramaticdi￿erenceisduetotheoverlapbetweenthetrainingsetandthetestsetinthepreviouswork.Inourremainingexperiment,wedecidetocomparewiththeBLEU-4scoresonthedatasetafterremovingduplicates,astheymoreaccuratelyre￿ectthemodel’scapacitytogeneralizeonunseenqueries.4.2RNNEncoder-DecoderModelPerformance(RQ2)AfterestablishingthebaselineperformanceontheJavadataset,weusedthePyTorchimplementationdevelopedbyGuetal.[12]totrainthemodelwithourPythondatatoanswerourRQ2.TheBLEU-4scoresachievedareshowninFigure2.Themodelconvergedquicklyandcontinuedtoimprovewithtrainingwithmorebatches.TheBLEU-4scoreachievedonthetestingsetwas14.14%withthetop-1outputAPIsequence.WealsousedthePyTorchimplementationtotraintheoriginalJavaAPIdataset(withduplicatesremoved).TheresultsareshowninFigure3.TheBLEU-4scoreachievedwas12.41%,whichislessthanthescoreof20.97%reportedinSection4.1;possiblereasonsforthisarediscussedinSection5.Figure3alsoshowstheresultoftrainingthemodelwiththeoriginaldatasetbeforetheduplicateswereremoved.TheBLEU-4scoreachievedwas38.26%.Ourexperimentsindicatethatthesamemodelarchitectureper-formssimilarlyforPythonandJava.Atthesametime,theevalua-tionofthemodel’sperformanceisgreatlya￿ectedbythequalityofthetestset.Whenitmakesinferencesonunseenqueries,itsperformanceismuchweakerthanonacollectionofquerieswithentriesfromthetrainingset.4.3CodeBERTPerformance(RQ3)OurRQ3aimstounderstandthepotentialofusingamorerecentap-proach,CodeBERT,onthistask,comparedtothemethodproposedbyGuetal.[12].Theperformanceof￿ne-tuningthepretrainedCodeBERTmodelonthetaskofAPIsequencegenerationisshowninFigure4andFigure5.ThemaximumBLEU-4scoreachievedonthePythondatasetwas41.56%,largelyimprovedcomparedwith14.14%achievedbytheRNNEncoder-Decodermodel.OntheJavadataset,themaximumBLEU-4scorewas24.17%.Incom-parison,theperformancebytheRNNEncoder-DecodermodelonJavaisonly12.41%(usingthePytorchimplementation).Suchim-provementmightbeattributedtothemodelarchitecture,i.e.,trans-former[24]versusRNNbasedmodels,and/orthepretrainingwithself-supervisedobjectivesonalargerandpotentiallyoverlappeddataset.Tounderstandtheimpactofeachfactor,weperformedtwoadditionalanalyses,oneonthedatasetoverlappingandtheotheronmodelperformancewithoutpretraining.TheCodeSearchNetdataset,onwhichtheCodeBERTmodelispretrained,wascollectedfromGitHub,thesamesourceasourPythondataset.ItispossiblethatCodeSearchNethasseenafunc-tionduringitspretrainingstepfromwhichourtestinstanceisconstructed.Tounderstandtheoverlap,wecomparethedescandRQ1RQ2RQ3ICPC ’22, May 16–17, 2022, Virtual Event, USA

James Martin and Jin L.C. Guo

Figure 4: The cross-entropy loss and the BLEU-4 score
achieved during the fine-tuning of the CodeBERT model
with the Python API training set.

Figure 6: The cross-entropy loss and the BLEU-4 score
achieved achieved when training the CodeBERT model with
the Python API training set from random initial parameters
(that is, discarding the pretraining).

Table 2: Overlap between the testing dataset used for the API
learning task and the CodeSearchNet pretraining data.

# of Matched
desc
93
2

# of Matched
apiseq
10267
5948

# of Matched
pairs
1
0

Python Dataset
Java Dataset

4.4 CodeBERT Pretraining (RQ3.1)
To eliminate the impact of pretraining, we further trained the Code-
BERT model from scratch using the task of Python API sequence
learning. The model performance is shown in Figure 6. The max-
imum BLEU-4 score was 35.52%. There were fewer iterations in
this experiment (40,000 instead of 50,000), but the same experiment
with pretraining had already achieved a BLEU-4 score of 40.67%
by 40,000 iterations. While the result of training CodeBERT from
scratch was not as good as the pretrained CodeBERT model, it is
still considerably better than the RNN based model, demonstrating
that the transformer based model is more effective for encoding the
natural language queries and connecting them with the relevant
API calls.

5 DISCUSSION
Model Performance Comparison. Our study reveals that the trans-
former based model, in particular CodeBERT, even without self-
supervised pretraining, markedly outperforms the RNN based en-
coder-decoder architecture for the task of API sequence learning.
When CodeBERT model was pretrained, the convergence is much
faster than RNN based models and it further improved the perfor-
mance evaluated using the BLEU-4 metric. However, such improve-
ment comes with a cost. The CodeBERT model is 674 MB in size
when saved to disk, significantly larger than the RNN based model
which is only 160 MB. The pretraining process can also be time and
resource consuming.

Figure 5: The cross-entropy loss and the BLEU-4 score
achieved during the fine-tuning of the CodeBERT model
with the Java API training set.

apiseq with CodeSearchNet. We found 93 docstrings in Code-
SearchNet that exactly match a desc in our Python test set after
both were tokenized with the RoBERTa tokenizer. We found 10,267
instances in CodeSearchNet in which the code matches an apiseq
in our Python test set (after RoBERTa tokenization). Matching for
the apiseq means that every token in the tokenized apiseq is found
in the tokenized CodeSearchNet code and is in the same order;
however, the tokenized CodeSearchNet code may have additional
tokens. When both desc and apiseq from one instance is matched
to a CodeSearchNet instance, we consider this pair to be matched.
We found only one matched pair for the Python dataset. The same
analysis was also performed on the Java dataset and the overlap is
summarized in Table 2.

01000020000300004000050000Batch #012345678Cross-entropy loss0510152025303540BLEU-4 scoreMetrics while fine-tuning the pre-trained CodeBERT model01000020000300004000050000Batch #012345678Cross-entropy loss0510152025303540BLEU-4 scoreMetrics while fine-tuning CodeBERT with Java data0500010000150002000025000300003500040000Batch #012345678Cross-entropy loss0510152025303540BLEU-4 scoreMetrics while training CodeBERT from random parametersDeep API Learning Revisited

ICPC ’22, May 16–17, 2022, Virtual Event, USA

In Table 3 and Table 4, given in Appendix A, we present example
input and output by different methods from both Python and Java
testing sets. We observe that both models still answer many queries
incorrectly, revealing a gap before those models can be used in
practice. The RNN model sometimes gives the preferable answer,
such as its answer to “round a decimal value” in Table 4, while the
CodeBERT answer is preferable for other queries, such as “JSON
serialize an object” in Table 3.

Reproducibility. During our replication of the work by Gu et al.
[12], we haven’t encountered any major challenge thanks to the
remarkable effort of releasing and maintaining the source code
and dataset on GitHub from the original authors. However, there
were a few key differences between our experiment and theirs re-
ported in the paper which might explain the gap of performance
on the Java dataset. As we discussed in Section 4.1, there are a large
number of duplicated pairs in the original Java dataset. After re-
moving the duplication, the size shrinks from 7,475,850 to 1,880,472.
While the evaluation result using the original dataset can reflect the
model performance on common and repeated queries, it might not
accurately reveal the model performance on entirely new queries.
Moreover, we adapted the PyTorch implementation of Deep API
that was developed by Gu et al. [12] after their paper was published.
This implementation has significant differences from the original
version: for example, the new PyTorch implementation does not
weight API calls by their importance, as described in Section 3.2 of
the Deep API paper. When we evaluated the performance using the
original Theano implementation, the performance is 54.71% on the
original Java dataset which is consistent with the paper, and 20.97%
on the dataset with duplicated instances removed. These results
suggest that the significant impact of the dataset quality remains.
Additionally, we did not train the model for as many iterations.
They trained for 1 million iterations (i.e., batches of 100 pairs), while
we only trained for 250,000 iterations; but this is unlikely to account
for the difference, because our BLEU score had already converged
by 250,000 iterations, as shown by Figure 2.

Limitations and Future Directions. Table 3 in Appendix A gives
example inputs and outputs of the models trained on the Python
dataset. From this table we can identify three types of errors. We
discuss each of them using representative examples from the table
below:

(1) Some answers are incorrect even though the correct API
calls exist in the training set. For example, the query “create
socket” has incorrect answers even though the correct API
call, socket.socket, appears in the training dataset. This
error might be fixed by tuning the hyperparameters of the
models, or by collecting a larger training corpus and training
the models for more iterations.

(2) Some answers are incorrect because the correct API calls
were removed from the training set to satisfy the upper limit
of 10,000 possible output tokens. For example, the query
“generate md5 hash code” has incorrect answers because the
correct answer, hashlib.md5, was not included in the train-
ing set, even though it was found in the original corpus of
source code. To fix this error, a model with a larger maximum
vocabulary would be required.

(3) Other answers are incorrect because the correct answer is to
call a method on an instance of a class, and method calls are
not detected by the dataset curation procedure described in
3.1.3. For example, the correct answer to the query “save an
image to a file” is to call the save method of the ImageFile
class. To fix this error, the data curation procedure would
have to deduce the types of variables, at least in some cases,
so that a call of the form image_file.save might be recog-
nized as a call to the ImageFile class.

The general method of API discovery pursued by these experi-
ments requires a substantial body of open source software already
written in a particular programming language, and it can only learn
to recognize APIs that have a history of use in real world software.
Ideally, API discovery should work for new languages and new
APIs for which there is no example code. Future experiments might
augment the training set with pairs that are extracted from the
authoritative API documentation, or the docstrings of the API func-
tions themselves. This would present some difficulties that would
need to be solved. For example, the phrasing of the authoritative
API documentation may not reflect how programmers use docu-
mentation in their code. Furthermore, these pairs would account for
only a tiny fraction of the training set, so weighting the importance
of the training instance depending on its source might be necessary.
Additionally, if any of these pairs are assigned to the testing set
instead of the training set, then the API call will not appear in the
training set at all. To mitigate these issues, and to avoid overfitting
the model with one precise phrasing of the function descriptions,
we can apply data augmentation techniques [8] to generate multi-
ple distinct but equivalent descriptions for each pair that is derived
from the authoritative API documentation.

6 CONCLUSION
In this work, we have trained and compared two neural network
models to take an input of natural language description of a task
and to output a suggested sequence of Python API calls that can per-
form that task. Previously Gu et al. [12] had already demonstrated
that this could be done with the Java Platform API; we showed
that these results can be generalized to at least one other platform
Python. We have repeated our experiment with two different archi-
tectures: the RNN Encoder-Decoder architecture achieved a BLEU-4
score of 14.14% on Python and 12.41% on Java, and the CodeBERT
architecture achieved a BLEU-4 score of 41.56% on Python and
24.17% on Java. We discussed the differences between these two
experiments that could account for the improved score of the Code-
BERT architecture. Our work calls for future research in combining
learning from the wild and the authoritative API documentations.

ACKNOWLEDGMENTS
We acknowledge the support of the Natural Sciences and Engineer-
ing Research Council of Canada (NSERC) Discovery Grant Program
[RGPIN-2019-05403].

REFERENCES
[1] 2022. Stack Overflow. https://stackoverflow.com/
[2] Emad Aghajani, Csaba Nagy, Olga Lucero Vega-Márquez, Mario Linares-Vásquez,
Laura Moreno, Gabriele Bavota, and Michele Lanza. 2019. Software documenta-
tion issues unveiled. In 2019 IEEE/ACM 41st International Conference on Software

ICPC ’22, May 16–17, 2022, Virtual Event, USA

James Martin and Jin L.C. Guo

[25] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S Yu. 2018. Improving automatic source code summarization via deep rein-
forcement learning. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering. 397–407.

[26] Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, and
Zhenchang Xing. 2017. What do developers search for on the web? Empirical
Software Engineering 22, 6 (2017), 3149–3185.

[27] Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program
repair from diagnostic feedback. In International Conference on Machine Learning.
PMLR, 10799–10808.

Engineering (ICSE). IEEE, 1199–1210.

[3] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 1–37.

[4] Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021. Self-
Supervised Bug Detection and Repair. Advances in Neural Information Processing
Systems 34 (2021).

[5] Miltiadis Allamanis and Charles Sutton. 2014. Mining idioms from source code.
In Proceedings of the 22nd acm sigsoft international symposium on foundations of
software engineering. 472–483.

[6] Gina R Bai, Joshua Kayani, and Kathryn T Stolee. 2020. How graduate computing
students search when using an unfamiliar programming language. In Proceedings
of the 28th International Conference on Program Comprehension. 160–171.

[7] Prem Devanbu, Matthew Dwyer, Sebastian Elbaum, Michael Lowry, Kevin Moran,
Denys Poshyvanyk, Baishakhi Ray, Rishabh Singh, and Xiangyu Zhang. 2020.
Deep learning & software engineering: State of research and future directions.
arXiv preprint arXiv:2009.08525 (2020).

[8] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,
Teruko Mitamura, and Eduard Hovy. 2021. A Survey of Data Augmentation
Approaches for NLP. In Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 968–988.
https://doi.org/10.18653/v1/2021.findings-acl.84

[9] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2020. 1536–1547.

[10] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated

program repair. Commun. ACM 62, 12 (2019), 56–65.

[11] Jian Gu, Pasquale Salza, and Harald C Gall. 2022. Assemble Foundation Models
for Automatic Code Summarization. arXiv preprint arXiv:2201.05222 (2022).
[12] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep
API Learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sym-
posium on Foundations of Software Engineering (Seattle, WA, USA) (FSE 2016).
Association for Computing Machinery, New York, NY, USA, 631–642. https:
//doi.org/10.1145/2950290.2950334

[13] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering (Zurich, Switzerland) (ICSE ’12). IEEE Press,
837–847.

[14] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).

[15] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135–146.

[16] Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019. Improving bug
detection via context-based code representation learning and attention-based
neural networks. Proceedings of the ACM on Programming Languages 3, OOPSLA
(2019), 1–30.

[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[18] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.

arXiv:1711.05101 [cs.LG]

[19] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A Method for Automatic Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computational Linguistics (Philadel-
phia, Pennsylvania) (ACL ’02). Association for Computational Linguistics, USA,
311–318. https://doi.org/10.3115/1073083.1073135

[20] Martin F. Porter. 1980. An algorithm for suffix stripping. Program 40 (1980),

211–218.

[21] Michael Rath, Jacob Rendall, Jin LC Guo, Jane Cleland-Huang, and Patrick Mäder.
2018. Traceability in the wild: automatically augmenting incomplete trace links.
In Proceedings of the 40th International Conference on Software Engineering. 834–
845.

[22] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vi-
cente Franco, and Miltiadis Allamanis. 2021. Fast and memory-efficient neural
code completion. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR). IEEE, 329–340.

[23] Christoph Treude and Martin P Robillard. 2016. Augmenting api documentation
with insights from stack overflow. In 2016 IEEE/ACM 38th International Conference
on Software Engineering (ICSE). IEEE, 392–403.

[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. arXiv:1706.03762 [cs.CL]

Deep API Learning Revisited

ICPC ’22, May 16–17, 2022, Virtual Event, USA

A EXAMPLE API SEQUENCES PRODUCED BY THE MODELS

Table 3: Example outputs from the RNN (deep API) model and the CodeBERT model trained with Python API calls. Sequences
that accomplish the task are marked with ✓, ones that don’t are marked with ✗. If both are ✗, we provide an API sequence that
would satisfy the query. A call of the form X::y means that the method y is called on an instance of the class X. The queries in
this table were adapted from Gu et al. [12].

Query

get current time

parse datetime from string

test file exists

list files in folder

match regular expressions

generate md5 hash code

generate random number

round a decimal value

connect to database

copy file

copy a file and save it to your
destination path

delete files and folders in a
directory

create socket

rename a file

download a file from a url

JSON serialize an object

read a binary file

save an image to a file

Python API Sequence
✓ RNN: time.time
✓ CodeBERT: time.localtime, time.strftime
✗ RNN: re.match
✓ CodeBERT: pandas.to_datetime
✓ RNN: os.path.join, os.path.exists
✓ CodeBERT: os.path.exists
✗ RNN: os.listdir, re.search, os.path.join, os.path.join, os.listdir
✓ CodeBERT: os.walk, os.path.join
✗ RNN: re.escape, re.escape
✓ CodeBERT: re.search
✗ RNN: os.path.dirname, os.path.dirname, os.path.abspath, os.path.join, os.path.join,
os.path.join
✗ CodeBERT: os.urandom

Actual: hashlib.md5, hashlib.HASH::update, hashlib.HASH::digest

✗ RNN: numpy.where, scipy.stats.randint, numpy.random.randint
✓ CodeBERT: os.urandom
✓ RNN: numpy.round
✓ CodeBERT: numpy.round
✗ RNN: sys.exit
✗ CodeBERT: sys.exit

Actual: mysql.connector.connect

✗ RNN: os.access, time.sleep
✗ CodeBERT: os.path.join
Actual: shutil.copyfile
✗ RNN: os.path.basename
✗ CodeBERT: os.path.join

Actual: shutil.copy

✗ RNN: os.walk, os.path.join, os.path.dirname, os.path.join, os.rmdir, os.path.join, os.testing,
os.rmdir
✓ CodeBERT: os.walk, os.path.join, os.remove, os.rmdir
✗ RNN: time.time, time.time, time.time
✗ CodeBERT: time.sleep
Actual: socket.socket

✓ RNN: os.rename
✓ CodeBERT: os.rename
✗ RNN: os.path.expanduser, os.path.basename, os.path.join, os.path.exists, os.path.isfile,
os.unlink, os.path.isfile, os.stat, os.path.isdir, os.path.isfile
✗ CodeBERT: os.path.exists

Actual: urllib.request.urlopen, http.client.HTTPResponse::read

sys.stdout.flush,

✗ RNN:
sys.stdout.writelines,
sys.stdout.flush, sys.stdout.write, sys.stdout.flush
✓ CodeBERT: ujson.dumps
✓ RNN: numpy.memmap
✓ CodeBERT: numpy.fromfile
✗ RNN: imread.imsave.toimage, os.path.dirname, os.path.exists, os.path.dirname, os.makedirs
✗ CodeBERT: os.path.splitext

sys.stdout.flush,

sys.stdout.flush,

Actual: PIL.ImageFile.ImageFile::save

ICPC ’22, May 16–17, 2022, Virtual Event, USA

James Martin and Jin L.C. Guo

Table 4: Example outputs from the RNN (deep API) model and the CodeBERT model trained with Java API calls. Sequences
that accomplish the task are marked with ✓, ones that don’t are marked with ✗. If both are ✗, we provide an API sequence that
would satisfy the query. The queries in this table were adapted from Gu et al. [12].

Query

get current time

Java API Sequence
✗ RNN: new SimpleDateFormat, SimpleDateFormat.parse
✗ CodeBERT: new SimpleDateFormat, SimpleDateFormat.format

parse datetime from string

test file exists

list files in folder

match regular expressions

generate md5 hash code

round a decimal value

execute sql statement

connect to database

create file

copy a file and save it to your
destination path

delete files and folders in a
directory

create socket

rename a file

download a file from a url

XML serialize an object

read a binary file

save an image to a file

write an image to a file

Actual: new Date, new SimpleDateFormat, SimpleDateFormat.format

Inte-

String.getBytes, MessageDigest.digest,

✗ RNN: String.indexOf, String.length, String.substring, String.length, String.substring
✓ CodeBERT: new SimpleDateFormat, SimpleDateFormat.parse
✗ RNN: new File, File.isFile, File.getAbsolutePath
✓ CodeBERT: new File, File.exists
✓ RNN: new File, File.listFiles, new ArrayList<File>, File.getName, List<File>.add
✗ CodeBERT: new File, File.listFiles
✓ RNN: Pattern.compile, Pattern.matcher, Matcher.matches
✗ CodeBERT: Pattern.compile, Pattern.matcher
✓ RNN: MessageDigest.getInstance,
ger.toHexString
✓ CodeBERT: MessageDigest.getInstance, String.getBytes, MessageDigest.digest
✓ RNN: Math.round, Math.round
✗ CodeBERT: Math.pow, Math.round
✓ RNN: Connection.prepareStatement, PreparedStatement.execute
✓ CodeBERT: Connection.createStatement, Statement.execute, Statement.close
✓ RNN: Class.forName, DriverManager.getConnection
✓ CodeBERT: Class.forName, DriverManager.getConnection
✓ RNN: new FileOutputStream, FileOutputStream.write, FileOutputStream.close
✓ CodeBERT: new File, File.createNewFile
✓ RNN: new File, new FileInputStream, FileInputStream.getChannel, FileOutput-
Stream.getChannel, FileChannel.size, FileChannel.transferTo, FileChannel.close
✓ CodeBERT: new FileInputStream, new FileOutputStream, InputStream.read, Output-
Stream.write, InputStream.close, OutputStream.close
✓ RNN: new File, File.listFiles, File.delete
✗ CodeBERT: File.listFiles, File.delete
✓ RNN: new Socket, Socket.setSoTimeout
✗ CodeBERT: Socket.getOutputStream, new ObjectOutputStream
✓ RNN: new File, File.renameTo
✓ CodeBERT: new File, File.renameTo
✓ RNN: new URL, URL.openConnection, URLConnection.getInputStream, new BufferedIn-
putStream, new FileOutputStream, new BufferedOutputStream, BufferedInputStream.read,
BufferedOutputStream.write, BufferedInputStream.close, BufferedOutputStream.close
✗ CodeBERT: new URL, URL.openConnection, HttpURLConnection.cast, HttpURLConnec-
tion.setRequestMethod, HttpURLConnection.setDoInput, HttpURLConnection.setDoOutput,
HttpURLConnection.setUseCaches,
HttpURLConnection.setInstanceFollowRedirects,
HttpURLConnection.setRequestProperty, HttpURLConnection.getOutputStream, Output-
Stream.write, OutputStream.close, HttpURLConnection.disconnect
✓ RNN: new XMLSerializer, new XMLSerializer, XMLSerializer.serialize
✗ CodeBERT: new ByteArrayOutputStream, ByteArrayOutputStream.toString
✗ RNN: new FileInputStream, new BufferedInputStream, BufferedInputStream.close
✗ CodeBERT: new FileInputStream, FileInputStream.close

Actual: new File, File.length, new FileInputStream, FileInputStream.read, FileInput-

Stream.close
✓ RNN: new File, ImageIO.write
✓ CodeBERT: new File, ImageIO.write
✓ RNN: new File, ImageIO.write
✓ CodeBERT: new File, ImageIO.write

