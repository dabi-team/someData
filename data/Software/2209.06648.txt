2
2
0
2

p
e
S
4
1

]
L
P
.
s
c
[

1
v
8
4
6
6
0
.
9
0
2
2
:
v
i
X
r
a

Automated Synthesis of Asynchronizations‚ãÜ

Sidi Mohamed Beillahi1, Ahmed Bouajjani2, Constantin Enea3, and Shuvendu
Lahiri4

1 University of Toronto, Canada
sm.beillahi@utoronto.ca
2 Universit¬¥e Paris Cit¬¥e, IRIF, CNRS, Paris, France
abou@irif.fr
3 LIX, Ecole Polytechnique, CNRS and Institut Polytechnique de Paris, France
cenea@irif.fr
4 Microsoft Research Lab - Redmond
shuvendu@microsoft.com

Abstract. Asynchronous programming is widely adopted for building
responsive and efficient software, and modern languages such as C# pro-
vide async/await primitives to simplify the use of asynchrony. In this
paper, we propose an approach for refactoring a sequential program into
an asynchronous program that uses async/await, called asynchroniza-
tion. The refactoring process is parametrized by a set of methods to
replace with asynchronous versions, and it is constrained to avoid intro-
ducing data races. We investigate the delay complexity of enumerating
all data race free asynchronizations, which quantifies the delay between
outputting two consecutive solutions. We show that this is polynomial
time modulo an oracle for solving reachability in sequential programs.
We also describe a pragmatic approach based on an interprocedural data-
flow analysis with polynomial-time delay complexity. The latter approach
has been implemented and evaluated on a number of non-trivial C# pro-
grams extracted from open-source repositories.

1

Introduction

Asynchronous programming is widely adopted for building responsive and eÔ¨É-
cient software. As an alternative to explicitly registering callbacks with asyn-
chronous calls, C# 5.0 [3] introduced the async/await primitives. These prim-
itives allow the programmer to write code in a familiar sequential style without
explicit callbacks. An asynchronous procedure, marked with async, returns a
task object that the caller uses to ‚Äúawait‚Äù it. Awaiting may suspend the exe-
cution of the caller, but does not block the thread it is running on. The code
after await is the continuation called back when the callee result is ready. This
paradigm has become popular across many languages, C++, JavaScript, Python.
The async/await primitives introduce concurrency which is notoriously com-
plex. The code in between a call and a matching await (referring to the same
task) may execute before some part of the awaited task or after the awaited

‚ãÜ This work is supported in part by the European Research Council (ERC) under the
Horizon 2020 research and innovation programme (grant agreement No 678177).

 
 
 
 
 
 
2

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 void Main(string f) {
2 x = 0;
3 int val = RdFile(f);
4 y = 1;

6 int r = x;
7 Debug.Assert(r == val); }

9 int RdFile(string f) {

10 var rd=new StreamReader(f);
11 string s = rd.ReadToEnd();
12 int r1 = x;

14 x = r1 + s.Length;
15 return s.Length;

}

1 async Task Main(string f) {
2 x = 0;
3 Task<int> t1 = RdFile(f);
4 y = 1;
5 int val = await t1;
6 int r = x;
7 Debug.Assert(r == val); }

9 async Task<int> RdFile(string f) {

10 var rd = new StreamReader(f);
11 Task<string> t=rd.ReadToEndAsync();
12 int r1 = x;
13 string s = await t;
14 x = r1 + s.Length;
15 return s.Length;

}

Fig. 1: Synchronous and asynchronous C# programs (x, y are static variables).

task Ô¨Ånished. For instance, on the middle of Fig. 1, the assignment y=1 at line 4
can execute before or after RdFile Ô¨Ånishes. The await for ReadToEndAsync in
RdFile (line 13) may suspend RdFile‚Äôs execution because ReadToEndAsync did
not Ô¨Ånish, and pass the control to Main which executes y=1. If ReadToEndAsync
Ô¨Ånishes before this await executes, then the latter has no eÔ¨Äect and y=1 gets
executed after RdFile Ô¨Ånishes. The resemblance with sequential code can be es-
pecially deceitful since this non-determinism is opaque. It is common that awaits
are placed immediately after the corresponding call which limits the beneÔ¨Åts that
can be obtained from executing steps in the caller and callee concurrently [25].
In this paper, we address the problem of writing eÔ¨Écient asynchronous code
that uses async/await. We propose a procedure for automated synthesis of
asynchronous programs equivalent to a given synchronous (sequential) program
ùëÉ . This can be seen as a way of refactoring synchronous code to asynchronous
code. Solving this problem in its full generality would require checking equiv-
alence between arbitrary programs, which is known to be hard. Therefore, we
consider a restricted space of asynchronous program candidates deÔ¨Åned by sub-
stituting synchronous methods in ùëÉ with asynchronous versions (assumed to
be behaviorally equivalent). The substituted methods are assumed to be leaves
of the call-tree (they do not call any method in ùëÉ ). Such programs are called
asynchronizations of ùëÉ . A practical instantiation is replacing IO synchronous
calls for reading/writing Ô¨Åles or managing http connections with asynchronous
versions.

For instance, the sequential C# program on the left of Fig. 1 contains a Main
that invokes a method RdFile that returns the length of the text in a Ô¨Åle. The
Ô¨Åle name input to RdFile is an input to Main. The program uses a variable x to
aggregate the lengths of all Ô¨Åles accessed by RdFile; this would be more useful
when Main calls RdFile multiple times which we omit for simplicity. Note that
this program passes the assertion at line 7. The time consuming method Read-
ToEnd for reading a Ô¨Åle is an obvious choice for being replaced with an equivalent
asynchronous version whose name is suÔ¨Éxed with Async. Performing such tasks
asynchronously can lead to signiÔ¨Åcant performance boosts. The program on the
middle of Fig. 1 is an example of an asynchronization deÔ¨Åned by this substitu-
tion. The syntax of async/await imposes that every method that transitively

  14  15                     }   RdFile(string f) {   10  11  12                    13Main() {  2   3   4  5           6     7        }Automated Synthesis of Asynchronizations

3

calls one of the substituted methods, i.e., Main and RdFile, must also be de-
clared as asynchronous. Then, every asynchronous call must be followed by an
await that speciÔ¨Åes the control location where that task should have completed.
For instance, the await for ReadToEndAsync is placed at line 13 since the next
instruction (at line 14) uses the computed value. Therefore, synthesizing such
refactoring reduces to Ô¨Ånding a correct placement of awaits (that implies equiv-
alence) for every call of a method that transitively calls a substituted method
(we do not consider ‚Äúdeeper‚Äù refactoring like rewriting conditionals or loops).

We consider an equivalence relation between a synchronous program and an
asynchronization that corresponds to absence of data races in the asynchroniza-
tion. Data race free asynchronizations are called sound. Relying on absence of
data races avoids reasoning about equality of sets of reachable states which is
harder in general, and an established compromise in reasoning about concur-
rency. For instance, the asynchronization in Fig. 1 is sound because the call to
RdFile accessing x Ô¨Ånishes before the read of x in Main (line 6). Therefore,
accesses to x are performed in the same order as in the synchronous program.

The asynchronization on the right of Fig. 1 is not the only sound (data-race
free) asynchronization of the program on the left. The await at line 13 can be
moved one statement up (before the read of x) and the resulting program re-
mains equivalent to the sequential one. In this paper, we investigate the problem
of enumerating all sound asynchronizations of a sequential program ùëÉ w.r.t. sub-
stituting a set of methods with asynchronous versions. This makes it possible to
deal separately with the problem of choosing the best asynchronization in terms
of performance based on some metric (e.g., performance tests).

Identifying the most eÔ¨Écient asynchronization is diÔ¨Écult and can not be done
syntactically. It is tempting to consider that increasing the distance between calls
and matching awaits so that more of the caller code is executed while waiting for
an asynchronous task to Ô¨Ånish increases performance. However, this is not true
in general. We use the programs in Fig. 2 to show that the best await placement
w.r.t. performance depends on execution times of code blocks in between calls
and awaits in a non-trivial manner. Note that estimating these execution times,
especially for IO operations like http connections, can not be done statically.

The programs in Fig. 2 use Thread.Sleep(n) to abstract sequential code
executing in ùëõ milliseconds and Task.Delay(n) to abstract an asynchronous call
executing in ùëõ milliseconds on a diÔ¨Äerent thread. The functions named Foo diÔ¨Äer
only in the position of await t. We show that modifying this position worsens
execution time in each case. For the left program, best performance corresponds
to maximal distance between await t in Foo and the corresponding call. This
allows the IO call to execute in parallel with the caller, as depicted on the bottom-
left of Fig. 2. The executions corresponding to the other two positions of await
t are given just above. For the middle program, placing await t in between the
two code blocks in Foo optimizes performance (note the extra IO call in Main):
the IO call in Foo executes in parallel with the Ô¨Årst code block in Foo and the
IO call in Main executes in parallel with the second one. This is depicted on the
bottom-middle of Fig. 2. The execution above shows that placing await t as on

4

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 async Task Main() {
2 var t1 = Foo();

4 await t1;
5

}

1 async Task Main() {
2 var t1 = Foo();
3 var t2 = IO();
4 await t1;
5 await t2;

}

7 async Task Foo() {
8 var t = IO();

7 async Task Foo() {
8 var t = IO();

10 Thread.Sleep(200);

12 Thread.Sleep(200);
13 await t;

}

10 Thread.Sleep(200);
11 await t;
12 Thread.Sleep(200);
13
}

1 async Task Main() {
2 var t1 = Foo();
3 var t2 = IO();
4 await t1;
5 await t2;

}

7 async Task Foo() {
8 var t = IO();
9 await t;

10 Thread.Sleep(200);

12 Thread.Sleep(200);
13

}

15 async Task IO() {
16 var t0 = Task.Delay(300);

15 async Task IO() {
16 var t0 = Task.Delay(300);

18 await t0;

}

18 await t0;

}

15 async Task IO() {
16 var t0 = Task.Delay(300);
17 Thread.Sleep(150);
18 await t0;

}

Fig. 2: Asynchronous C# programs and executions. On the bottom, time dura-
tions of executing code blocks from the same method are aligned horizontally,
and time goes from left to right. Vertical single-line arrows represent method call
steps, dashed arrows represent awaits passing control to the caller, and double-
line arrows represent a call return. Total execution time is marked time=....

the left (after the two code blocks) leads to worse execution time (placing await
t immediately after the call is also worse). Finally, for the right program, placing
await t immediately after the call is best (note that IO executes another code
block before await). The IO call in Main executes in parallel with Foo as shown
on the bottom-right of Fig. 2. The execution above shows the case where await
t is placed in the middle (the await has no eÔ¨Äect because IO already Ô¨Ånished,
and Foo continues to execute). This leads to worse execution time (placing await
t after the two code blocks is also worse). These diÔ¨Äerences in execution times
have been conÔ¨Årmed by running the programs on a real machine.

As demonstrated by the examples in Fig. 2, the performance of an asynchro-
nization depends on the execution environment, e.g., the overhead of IO opera-
tions like http connections and disk access (in Fig. 2, we use Thread.Sleep(n)
or Task.Delay(n) to model such overheads). Since modeling the behavior of
an execution environment w.r.t. performance is diÔ¨Écult in general, selecting the
most performant asynchronization using static reasoning is also diÔ¨Écult. As a
way of sidestepping this diÔ¨Éculty, we focus on enumerating all sound asynchro-

300 msTask1IO1FooMainIO2Task2200 ms300 ms300 ms300 msawaitTask1IO1FooMainIO2Task2200 ms300 ms300 msTask1IO1FooMainTask1IO1FooMain200 ms300 ms300 msTask1IO1FooMainIO2Task2300 msTask1IO1FooMainIO2Task2time=850 ms200 mstime = 700 mstime = 500 mstime = 700 mstime = 400 ms300 msTask1IO1FooMain200 mstime = 500 mstime = 700 msawait200 ms200 msawaitawait300 ms200 ms200 ms200 mscall returnawait passes controlcall200 ms200 ms150 ms150 msawait150 ms150 ms200 msAutomated Synthesis of Asynchronizations

5

nizations that allows to evaluate performance separately in a dynamic manner
using performance tests for instance (for each sound asynchronization).

In the worst-case, the number of (sound) asynchronizations is exponential
in the number of method calls in the program. Therefore, we focus on the de-
lay complexity of the problem of enumerating sound asynchronizations, i.e., the
complexity of the delay between outputting two consecutive (distinct) solutions,
and show that this is polynomial time modulo an oracle for solving reachabil-
ity (assertion checking) in sequential programs. Note that a trivial enumeration
of all asynchronizations and checking equivalence for each one of them has an
exponential delay complexity modulo an oracle for checking equivalence.

As an intermediate step, we consider the problem of computing maximal
sound asynchronizations that maximize the distance between every call and its
matching await. We show that rather surprisingly, there exists a unique max-
imal sound asynchronization. This is not trivial since asynchronizations can be
incomparable w.r.t. distances between calls and awaits (i.e., better for one await
and worse for another, and vice-versa). This holds even if maximality is relative
to a given asynchronization ùëÉùëé imposing an upper bound on the distance be-
tween awaits and calls. In principle, avoiding data races could reduce to a choice
between moving one await or another closer to the matching call. We show that
this is not necessary because the maximal asynchronization is required to be
equivalent to a sequential program, which executes statements in a Ô¨Åxed order.
As a more pragmatic approach, we deÔ¨Åne a procedure for computing sound
asynchronizations which relies on a bottom-up interprocedural data-Ô¨Çow analy-
sis. The placement of awaits is computed by traversing the call graph bottom up
and using a data-Ô¨Çow analysis that computes read or write accesses made in the
callees. We show that this procedure computes maximal sound asynchroniza-
tions of abstracted programs where every Boolean condition is replaced with
non-deterministic choice. These asynchronizations are sound for the concrete
programs as well. This procedure enables a polynomial-time delay enumeration
of sound asynchronizations of abstracted programs.

We implemented the asynchronization enumeration based on data-Ô¨Çow anal-
ysis in a prototype tool for C# programs. We evaluated this implementation
on a number of non-trivial programs extracted from open source repositories to
show that our techniques have the potential to become the basis of refactoring
tools that allow programmers to improve their usage of async/await primitives.

In summary, this paper makes the following contributions:

‚Äì DeÔ¨Åne the problem of data race-free (sound) asynchronization synthesis for
refactoring sequential code to equivalent asynchronous code (Section 3).
‚Äì Show that the problem of computing a sound asynchronization that maxi-
mizes the distance between calls and awaits has a unique solution (Section 4).

‚Äì The delay complexity of sound asynchronization synthesis (Sections 5‚Äì6).
‚Äì A pragmatic algorithm for computing sound asynchronizations based on a

data-Ô¨Çow analysis (Section 7).

‚Äì A prototype implementation of this algorithm and an evaluation of this

prototype on a benchmark of non-trivial C# programs (Section 8).

Additional formalization and proofs are included in the appendix.

6

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

‚ü®prog‚ü©
‚ü®md ‚ü©
‚ü®inst‚ü©

::= program ‚ü®md ‚ü©
::= method ‚ü®m‚ü© { ‚ü®inst‚ü© } | async method ‚ü®m‚ü© { ‚ü®inst‚ü© } | ‚ü®md ‚ü© ‚ü®md ‚ü©
::= ‚ü®x ‚ü© := ‚ü®le‚ü© | ‚ü®r ‚ü© := ‚ü®x ‚ü© | ‚ü®r ‚ü© := call ‚ü®m‚ü© | return | await ‚ü®r ‚ü©
| await * | if ‚ü®le‚ü© {‚ü®inst‚ü©} else {‚ü®inst‚ü©} | while ‚ü®le‚ü© {‚ü®inst‚ü©} |
‚ü®inst‚ü© ; ‚ü®inst‚ü©

Fig. 3: Syntax. ‚ü®ùëö‚ü©, ‚ü®ùë•‚ü©, and ‚ü®ùëü‚ü© represent method names, program and local
variables, resp. ‚ü®ùëôùëí‚ü© is an expression over local variables, or * which is non-
deterministic choice.

2 Asynchronous Programs

We consider a simple programming language to formalize our approach, shown
in Fig. 3. A program is a set of methods, including a distinguished main, which
are classiÔ¨Åed as synchronous or asynchronous. Synchronous methods run contin-
uously until completion when they are invoked. Asynchronous methods, marked
using the keyword async, can run only partially and be interrupted when exe-
cuting an await. Only asynchronous methods can use await, and all methods
using await must be deÔ¨Åned as asynchronous. We assume that methods are not
(mutually) recursive. A program is called synchronous if it is a set of synchronous
methods.

A method is deÔ¨Åned by a name from a set M and a list of statements over a set
PV of program variables, which can be accessed from diÔ¨Äerent methods (ranged
over using ùë•, ùë¶, ùëß,. . .), and a set LV of method local variables (ranged over using
ùëü, ùëü1, ùëü2,. . .). Input/return parameters are modeled using program variables.
Each method call returns a unique task identifier from a set T, used to record
control dependencies imposed by awaits (for uniformity, synchronous methods
return a task identiÔ¨Åer as well). Our language includes assignments, awaits,
returns, loops, and conditionals. Assignments to a local variable ùëü := ùë•, where
ùë• is a program variable, are called reads of ùë•, and assignments to a program
variable ùë• := ùëôùëí (ùëôùëí is an expression over local variables) are called writes to ùë•.
A base method is a method whose body does not contain method calls.
Asynchronous methods. Asynchronous methods can use awaits to wait for
the completion of a task (invocation) while the control is passed to their caller.
The parameter ùëü of the await speciÔ¨Åes the id of the awaited task. As a sound
abstraction of awaiting the completion of an IO operation (reading or writing a
Ô¨Åle, an http request, etc.), which we do not model explicitly, we use a variation
await *. This has a non-deterministic eÔ¨Äect of either continuing to the next
statement in the same method (as if the IO operation already completed), or
passing the control to the caller (as if the IO operation is still pending).

Fig. 4 lists our modeling of the IO method Read-
ToEndAsync used in Fig. 1. We use program vari-
ables to represent system resources such as the Ô¨Åle
system. The await for the completion of accesses
to such resources is modeled by await *. This en-
ables capturing racing accesses to system resources
in asynchronous executions. Parameters or return

async method ReadToEndAsync() {

await *;
ind = Stream.index;
len = Stream.content.Length;
if (ind >= len)

retVal =

""; return

Stream.index = len;
retVal = Stream.content(ind,len);
return
Fig. 4: An IO method.

}

Automated Synthesis of Asynchronizations

7

values are modeled using program variables. ReadToEndAsync is modeled us-
ing reads/writes of the index/content of the input stream, and await * models
the await for their completion.

We assume that the body of every asynchronous method ùëö satisÔ¨Åes several
well-formedness syntactic constraints, deÔ¨Åned on its control-Ô¨Çow graph (CFG).
We recall that each node of the CFG represents a basic block of code (a maximal-
length sequence of branch-free code), and nodes are connected by directed edges
which represent a possible transfer of control between blocks. Thus,
1. every call ùëü := call ùëö‚Ä≤ uses a distinct variable ùëü (to store task identiÔ¨Åers),
2. every CFG block containing an await ùëü is dominated by the CFG block
containing the call ùëü := call . . . (i.e., every CFG path from the entry to the
await has to pass through the call),

3. every CFG path starting from a block containing a call ùëü := call . . . to the

exit has to pass through an await ùëü statement.

The Ô¨Årst condition simpliÔ¨Åes the technical exposition, while the last two ensure
that ùëü stores a valid task identiÔ¨Åer when executing an await ùëü, and that every
asynchronous invocation is awaited before the caller Ô¨Ånishes. Languages like
C# or Javascript do not enforce the latter constraint, but it is considered bad
practice due to possible exceptions that may arise in the invoked task and are
not caught. We forbid passing task identiÔ¨Åers as method parameters (which is
possible in C#). A statement await ùëü is said to match a statement ùëü := call ùëö‚Ä≤.

}

}

if *

while *

await r;

await r;

await r;

r = call m1;

async method m {

r‚Äô = call m1;
await r‚Äô;

async method m {
r = call m1;

async method m {
r = call m1;
while *

}
Fig. 5: Examples of programs

In Fig. 5, we give three
examples of programs to ex-
plain in more details
the
well-formedness syntactic con-
straints. The program on the
left of Fig. 5 does not satisfy
the second condition since await r can be reached without entering the loop.
The program in the center of Fig. 5 does not satisfy the third condition since
we can reach the end of the method without entering the if branch and thus,
without executing await r. The program on the right of Fig. 5 satisÔ¨Åes both
conditions.
Semantics. A program conÔ¨Åguration is a tuple (g, stack, pend, cmpl, c-by, w-for)
where g is composed of the valuation of the program variables excluding the
program counter, stack is the call stack, pend is the set of asynchronous tasks,
e.g., continuations predicated on the completion of some method call, cmpl is
the set of completed tasks, c-by represents the relation between a method call
and its caller, and w-for represents the control dependencies imposed by await
statements. The activation frames in the call stack and the asynchronous tasks
are represented using triples (ùëñ, ùëö, ‚Ñì) where ùëñ ‚àà T is a task identiÔ¨Åer, ùëö ‚àà M
is a method name, and ‚Ñì is a valuation of local variables, including as usual
a dedicated program counter. The set of completed tasks is represented as a
function cmpl : T ‚Üí {‚ä§, ‚ä•} such that cmpl(ùëñ) = ‚ä§ when ùëñ is completed and
cmpl(ùëñ) =‚ä•, otherwise. We deÔ¨Åne c-by and w-for as partial functions T ‚áÄ T with
the meaning that c-by(ùëñ) = ùëó, resp., w-for(ùëñ) = ùëó, iÔ¨Ä ùëñ is called by ùëó, resp., ùëñ is

8

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

waiting for ùëó. We set w-for(ùëñ) = * if the task ùëñ was interrupted because of an
await * statement.

The semantics of a program ùëÉ is deÔ¨Åned as a labeled transition system (LTS)
[ùëÉ ] = (C, Act, ps0, ‚Üí) where C is the set of program conÔ¨Ågurations, Act is a
set of transition labels called actions, ps0 is the initial conÔ¨Åguration, and ‚Üí‚äÜ
C √ó Act √ó C is the transition relation. Each program statement is interpreted
as a transition in [ùëÉ ]. The set of actions is deÔ¨Åned by (Aid is a set of action
identiÔ¨Åers):
Act ={(aid , ùëñ, ev ) : aid ‚àà Aid, ùëñ ‚àà T, ev ‚àà {rd(ùë•), wr(ùë•), call(ùëó), await(ùëò), return,

cont : ùëó ‚àà T, ùëò ‚àà T ‚à™ {*}, ùë• ‚àà PV}}

The transition relation ‚Üí is deÔ¨Åned in Fig. 6. Transition labels are written

on top of ‚Üí.

Transitions labeled by (aid , ùëñ, rd(ùë•)) and (aid , ùëñ, wr(ùë•)) represent a read and
a write accesses to the program variable ùë•, respectively, executed by the task
(method call) with identiÔ¨Åer ùëñ. A transition labeled by (aid , ùëñ, call(ùëó)) corresponds
to the fact that task ùëñ executes a method call that results in creating a task ùëó.
Task ùëó is added on the top of the stack of currently executing tasks, declared
pending (setting cmpl(ùëó) to ‚ä•), and c-by is updated to track its caller (c-by(ùëó) =
ùëñ). A transition (aid , ùëñ, return) represents the return from task ùëñ. Task ùëñ is removed
from the stack of currently executing tasks, and cmpl(ùëñ) is set to ‚ä§ to record the
fact that task ùëñ is Ô¨Ånished.

A transition (aid , ùëñ, await(ùëó)) relates to task ùëñ waiting asynchronously for
task ùëó. Its eÔ¨Äect depends on whether task ùëó is already completed. If this is
the case (i.e., cmpl[ùëó] = ‚ä§), task ùëñ continues and executes the next statement.
Otherwise, task ùëñ executing the await is removed from the stack and added to
the set of pending tasks, and w-for is updated to track the waiting-for rela-
tionship (w-for(ùëñ) = ùëó). Similarly, a transition (aid , ùëñ, await(*)) corresponds to
task ùëñ waiting asynchronously for the completion of an unspeciÔ¨Åed task. Non-
deterministically, task ùëñ continues to the next statement, or task ùëñ is interrupted
and transferred to the set of pending tasks (w-for(ùëñ) is set to *).

A transition (aid , ùëñ, cont) represents the scheduling of the continuation of
task ùëñ. There are two cases depending on whether ùëñ waited for the completion
of another task ùëó modeled explicitly in the language (i.e., w-for(ùëñ) = ùëó), or an
unspeciÔ¨Åed task (i.e., w-for(ùëñ) = *). In the Ô¨Årst case, the transition is enabled
only when the call stack is empty and ùëó is completed. In the second case, the
transition is always enabled. The latter models the fact that methods implement-
ing IO operations (waiting for unspeciÔ¨Åed tasks in our language) are executed
in background threads and can interleave with the main thread (that executes
the Main method). Although this may seem restricted because we do not allow
arbitrary interleavings between IO methods and Main, this is actually sound
when focusing on the existence of data races as in our approach. As shown later
in Table 1, any two instructions that follow an await * are not happens-before
related and form a race.

By the deÔ¨Ånition of ‚Üí, every action a ‚àà Act ‚àñ {( ,

, cont)} corresponds to

executing some statement in the program, which is denoted by S(ùëé).

Automated Synthesis of Asynchronizations

9

r := x ‚àà inst(‚Ñì(pc)) aid ‚àà Aid fresh

‚Ñì‚Ä≤ = ‚Ñì[ùëü ‚Ü¶‚Üí g(ùë•), pc ‚Ü¶‚Üí next(‚Ñì(pc))]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack,

,

,

,

(aid, ùëñ, rd(ùë•))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëñ, ùëö, ‚Ñì‚Ä≤) ‚àò stack,

)

,

,

,

)

x := le ‚àà inst(‚Ñì(pc)) aid ‚àà Aid fresh

‚Ñì‚Ä≤ = ‚Ñì[pc ‚Ü¶‚Üí next(‚Ñì(pc))]

g‚Ä≤ = g[ùë• ‚Ü¶‚Üí ‚Ñì(le)]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack,

,

,

,

(aid, ùëñ, wr(ùë•))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g‚Ä≤, (ùëñ, ùëö, ‚Ñì‚Ä≤) ‚àò stack,

)

,

,

,

)

ùëü := call ùëö ‚àà inst(‚Ñì(pc)) aid ‚àà Aid fresh

‚Ñì0 = init(g, ùëö)

‚Ñì‚Ä≤ = ‚Ñì[ùëü ‚Ü¶‚Üí ùëó, pc ‚Ü¶‚Üí next(‚Ñì(pc))]

cmpl‚Ä≤ = cmpl[ùëó ‚Ü¶‚Üí‚ä•]

ùëó ‚àà T fresh
c-by‚Ä≤ = c-by[ùëó ‚Ü¶‚Üí ùëñ]

(g, (ùëñ, ùëö‚Ä≤, ‚Ñì) ‚àò stack,

, cmpl, c-by,

(aid, ùëñ, call(ùëó))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëó, ùëö, ‚Ñì0) ‚àò (ùëñ, ùëö‚Ä≤, ‚Ñì‚Ä≤) ‚àò stack,

)

, cmpl‚Ä≤, c-by‚Ä≤,

)

return ‚àà inst(‚Ñì(pc))

aid ‚àà Aid fresh

cmpl‚Ä≤ = cmpl[ùëñ ‚Ü¶‚Üí ‚ä§]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack,

, cmpl,

,

(aid, ùëñ, return)
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, stack,

)

, cmpl‚Ä≤,

,

)

await r ‚àà inst(‚Ñì(pc))

aid ‚àà Aid fresh

cmpl(‚Ñì(ùëü)) = ‚ä§ ‚Ñì‚Ä≤ = ‚Ñì[pc ‚Ü¶‚Üí next(‚Ñì(pc))]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack,

, cmpl,

,

(aid, ùëñ, await(‚Ñì(ùëü)))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëñ, ùëö, ‚Ñì‚Ä≤) ‚àò stack,

)

, cmpl,

,

)

await r ‚àà inst(‚Ñì(pc))

aid ‚àà Aid fresh

cmpl(‚Ñì(ùëü)) =‚ä• w-for‚Ä≤ = w-for[ùëñ ‚Ü¶‚Üí ‚Ñì(ùëü)]

‚Ñì‚Ä≤ = ‚Ñì[pc ‚Ü¶‚Üí next(‚Ñì(pc))]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack, pend, cmpl,

, w-for)

(aid, ùëñ, await(‚Ñì(ùëü)))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, stack, {(ùëñ, ùëö, ‚Ñì‚Ä≤)} ‚äé pend, cmpl,

, w-for‚Ä≤)

await * ‚àà inst(‚Ñì(pc))

aid ‚àà Aid fresh

‚Ñì‚Ä≤ = ‚Ñì[pc ‚Ü¶‚Üí next(‚Ñì(pc))]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack,

,

,

,

(aid, ùëñ, await(*))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëñ, ùëö, ‚Ñì‚Ä≤) ‚àò stack,

)

,

,

,

)

await * ‚àà inst(‚Ñì(pc))

aid ‚àà Aid fresh

w-for‚Ä≤ = w-for[ùëñ ‚Ü¶‚Üí *]

‚Ñì‚Ä≤ = ‚Ñì[pc ‚Ü¶‚Üí next(‚Ñì(pc))]

(g, (ùëñ, ùëö, ‚Ñì) ‚àò stack, pend,

,

, w-for)

(aid, ùëñ, await(*))
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, stack, {(ùëñ, ùëö, ‚Ñì‚Ä≤)} ‚äé pend,

,

, w-for‚Ä≤)

aid ‚àà Aid fresh

w-for(ùëñ) = ùëó

cmpl(ùëó) = ‚ä§

(g, ùúñ, {(ùëñ, ùëö, ‚Ñì)} ‚äé pend, cmpl,

, w-for)

(aid, ùëñ, cont)
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëñ, ùëö, ‚Ñì), pend, cmpl,

, w-for)

(g, stack, {(ùëñ, ùëö, ‚Ñì)} ‚äé pend,

,

, w-for)

(aid, ùëñ, cont)
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí (g, (ùëñ, ùëö, ‚Ñì) ‚àò stack, pend,

,

, w-for)

aid ‚àà Aid fresh

w-for(ùëñ) = *

Fig. 6: Program semantics. For a function ùëì , we use ùëì [ùëé ‚Ü¶‚Üí ùëè] to denote a function
ùëî such that ùëî(ùëê) = ùëì (ùëê) for all ùëê Ã∏= ùëé and ùëî(ùëé) = ùëè. The function inst returns the
instruction at some given control location while next gives the next instruction
to execute. We use ‚àò to denote sequence concatenation and init to denote the
initial state of a method call.

a1‚àí‚Üí ps1

An execution of ùëÉ is a sequence ùúå = ps0

a2‚àí‚Üí . . . of transitions starting
in the initial conÔ¨Åguration ps0 and leading to a conÔ¨Åguration ps where the call
stack and the set of pending tasks are empty. C[ùëÉ ] denotes the set of all program
variable valuations included in conÔ¨Ågurations that are reached in executions of ùëÉ .
Since we are only interested in reasoning about the sequence of actions a1 ¬∑a2 ¬∑. . .
labeling the transitions of an execution, we will call the latter an execution as
well. The set of executions of a program ùëÉ is denoted by Ex(ùëÉ ).
Traces. The trace of an execution ùúå ‚àà Ex(ùëÉ ) is a tuple tr(ùúå) = (ùúå, MO, CO, SO, HB)
of strict partial orders between the actions in ùúå deÔ¨Åned in Table 1. The method
invocation order MO records the order between actions in the same invocation,
and the call order CO is an extension of MO that additionally orders actions be-
fore an invocation with respect to those inside that invocation. The synchronous
happens-before order SO orders the actions in an execution as if all the invoca-
tions were synchronous (even if the execution may contain asynchronous ones).

10

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

It is an extension of CO where additionally, every action inside a callee is or-
dered before the actions following its invocation in the caller. The (asynchronous)
happens-before order HB contains typical control-Ô¨Çow constraints: it is an exten-
sion of CO where every action ùëé inside an asynchronous invocation is ordered
before the corresponding await in the caller, and before the actions following its
invocation in the caller if ùëé precedes the Ô¨Årst5 await in MO (an invocation can
be interrupted only when executing an await) or if the callee does not contain
an await (it is synchronous). Tr(ùëÉ ) is the set of traces of ùëÉ .

Table 1: Strict partial orders included in a trace. CO, SO, and HB are the smallest
satisfying relations.

a1 <ùúå a2
a1 ‚àº a2

(a1, a2) ‚àà MO
(a1, a2) ‚àà CO

(a1, a2) ‚àà SO

a1 occurs before a2 in ùúå and a1 Ã∏= a2

a1 = ( , ùëñ,

) and a2 = ( , ùëñ,

)

a1 ‚àº a2 ‚àß a1 <ùúå a2

(a1, a2) ‚àà MO ‚à® (a1 = ( , ùëñ, call(ùëó)) ‚àß a2 = ( , ùëó,

))

‚à® (‚àÉ a3. (a1, a3) ‚àà CO ‚àß (a3, a2) ‚àà CO)
(a1, a2) ‚àà CO ‚à® (‚àÉ a3. (a1, a3) ‚àà SO ‚àß (a3, a2) ‚àà SO)

‚à® (a1 = ( , ùëó,

) ‚àß a2 = ( , ùëñ,

) ‚àß ‚àÉ a3 = ( , ùëñ, call(ùëó)). a3 <ùúå a2)

(a1, a2) ‚àà HB

(a1, a2) ‚àà CO ‚à® (‚àÉ a3. (a1, a3) ‚àà HB ‚àß (a3, a2) ‚àà HB)

‚à® ( a1 = ( , ùëó,

) ‚àß a2 = ( , ùëñ,

) ‚àß ‚àÉ a3 = ( , ùëñ, await(ùëó)). a3 <ùúå a2 )

‚à® ( a1 = ( , ùëó, await(ùëñ‚Ä≤)) is the first await in ùëó ‚àß
) ‚àß ‚àÉ a3 = ( , ùëñ, call(ùëó)). a3 <ùúå a2 )
a2 = ( , ùëñ,

‚à® ( a1 = ( , ùëó,

) ‚àß Ã∏ ‚àÉ ( , ùëó, await( )) ‚àà ùúå ‚àß

a2 = ( , ùëñ,

) ‚àß ‚àÉ a3 = ( , ùëñ, call(ùëó)). a3 <ùúå a2 )

On the right of Fig. 1, we show a trace where two statements (represented
by the corresponding lines numbers) are linked by a dotted arrow if the corre-
sponding actions are related by MO, a dashed arrow if the corresponding actions
are related by CO but not by MO, and a solid arrow if the corresponding actions
are related by the HB but not by CO.

3 Synthesizing Asynchronous Programs

Given a synchronous program ùëÉ and a subset of base methods ùêø ‚äÜ ùëÉ , our goal
is to synthesize all asynchronous programs ùëÉùëé that are equivalent to ùëÉ and that
are obtained by substituting every method in ùêø with an equivalent asynchronous
version. The base methods are considered to be models of standard library calls
(e.g., IO operations) and asynchronous versions are deÔ¨Åned by inserting await
* statements in their body. We use ùëÉ [ùêø] to emphasize a subset of base methods
ùêø in a program ùëÉ . Also, we call ùêø a library. A library is called (a)synchronous
when all methods are (a)synchronous.
Asynchronizations of a synchronous program. Let ùëÉ [ùêø] be a synchronous
program, and ùêøùëé a set of asynchronous methods obtained from those in ùêø by

5 Code in between two awaits can execute before or after the control is returned to

the caller, depending on whether the first awaited task finished or not.

Automated Synthesis of Asynchronizations

11

inserting at least one await * statement in their body (and adding the key-
word async). Each method in ùêøùëé corresponds to a method in ùêø with the same
name, and vice-versa. ùëÉùëé[ùêøùëé] is called an asynchronization of ùëÉ [ùêø] with re-
spect to ùêøùëé if it is a syntactically correct program obtained by replacing the
methods in ùêø with those in ùêøùëé and adding await statements as necessary.
More precisely, let ùêø* ‚äÜ ùëÉ be
the set of all methods of ùëÉ
that transitively call methods of
ùêø. Formally, ùêø* is the smallest
set of methods that includes ùêø
and satisÔ¨Åes the following: if a
method ùëö calls ùëö‚Ä≤ ‚àà ùêø*, then
ùëö ‚àà ùêø*. Then, ùëÉùëé[ùêøùëé]
is an
asynchronization of ùëÉ [ùêø] w.r.t.
ùêøùëé if it is obtained from ùëÉ as follows:

Fig. 7: A program and its asynchronizations.

async method m {
r1 = call m1;
await r1;
r2 = x;

await *
retVal = x;
x = input;
return;

await *
retVal = x;
x = input;
return;

retVal = x;
x = input;
return; }

async method m {
r1 = call m1;

r2 = x;
await r1;

async method m1 {

async method m1 {

r1 = call m1;

method m1 {

method m {

r2 = x;

}

}

}

}

}

‚Äì Each method in ùêø is replaced with the corresponding method from ùêøùëé.
‚Äì All methods in ùêø* ‚àñ ùêø are declared as asynchronous (because every call to an
asynchronous method is followed by an await and any method using await
must be asynchronous).

‚Äì For each invocation ùëü := call ùëö of ùëö ‚àà ùêø*, add await statements await ùëü
satisfying the well-formedness syntactic constraints described in Section 2.
Fig. 7 lists a synchronous program and its two asynchronizations, where ùêø =
{ùëö1} and ùêø* = {ùëö, ùëö1}. Asynchronizations diÔ¨Äer only in the await placement.
Asy[ùëÉ, ùêø, ùêøùëé] is the set of all asynchronizations of ùëÉ [ùêø] w.r.t. ùêøùëé. The strong
asynchronization strongAsy[ùëÉ, ùêø, ùêøùëé] is an asynchronization where every await
immediately follows the matching call. It reaches exactly the same set of program
variable valuations as ùëÉ .
Problem definition. We investigate the problem of enumerating all asynchro-
nizations of a given program w.r.t. a given asynchronous library, which are sound,
in the sense that they do not admit data races. Two actions a1 and a2 in a trace
ùúè = (ùúå, MO, CO, SO, HB) are concurrent if (a1, a2) Ã∏‚àà HB and (a2, a1) Ã∏‚àà HB.

An ansynchronous program ùëÉùëé admits a data race (a1, a2), where (a1, a2) ‚àà
SO, if a1 and a2 are two concurrent actions of a trace ùúè ‚àà Tr(ùëÉùëé), and a1 and
a2 are read or write accesses to the same program variable ùë•, and at least one
of them is a write. We write data races as ordered pairs w.r.t. SO to simplify
the deÔ¨Ånition of the algorithms in the next sections. Also, note that traces of
synchronous programs can not contain concurrent actions, and therefore they
do not admit data races. strongAsy[ùëÉ, ùêø, ùêøùëé] does not admit data races as well.
ùëÉùëé[ùêøùëé] is called sound when it does not admit data races. The absence of
data races implies equivalence to the original program, in the sense of reaching
the same set of conÔ¨Ågurations (program variable valuations).

Definition 1. For a synchronous program ùëÉ [ùêø] and asynchronous library ùêøùëé,
the asychronization synthesis problem asks to enumerate all sound asynchro-
nizations in Asy[ùëÉ, ùêø, ùêøùëé].

12

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

4 Enumerating Sound Asynchronizations

We present an algorithm for solving asynchronization synthesis, which relies on
a partial order between asynchronizations that guides the enumeration of pos-
sible solutions. The partial order takes into account the distance between calls
and corresponding awaits. Fig. 8 pictures the partial order for asynchroniza-
tions of the program on the left of Fig. 1. Each asynchronization is written as
a vector of distances, the Ô¨Årst (second) element is the number
of statements between await t1 (await t) and the matching call
(we count only statements that appear in the sequential program).
The edges connect comparable elements, smaller elements being
below bigger elements. The asynchronization on the middle of
Fig. 1 corresponds to the vector (1, 1). The highlighted elements
constitute the set of all sound asynchronizations. The strong asyn-
chronization corresponds to the vector (0, 0).

Fig. 8

(1, 1)

(2, 1)

(0, 0)

(0, 1)

(2, 0)

(1, 0)

Formally, an await statement sùë§ in a method ùëö of an asynchronization
ùëÉùëé[ùêøùëé] ‚àà Asy[ùëÉ, ùêø, ùêøùëé] covers a read/write statement s in ùëÉ if there exists a path
in the CFG of ùëö from the call statement matching sùë§ to sùë§ that contains s. The
set of statements covered by an await sùë§ is denoted by Cover(sùë§). We compare
asynchronizations in terms of sets of statements covered by awaits that match
the same call from the synchronous program ùëÉ [ùêø]. Since asynchronizations are
obtained by adding awaits, every call in asynchronization ùëÉùëé[ùêøùëé] ‚àà Asy[ùëÉ, ùêø, ùêøùëé]
corresponds to a fixed call in ùëÉ [ùêø]. Therefore, for two asynchronizations ùëÉùëé, ùëÉ ‚Ä≤
ùëé ‚àà
ùëé, denoted by ùëÉùëé ‚â§ ùëÉ ‚Ä≤
Asy[ùëÉ, ùêø, ùêøùëé], ùëÉùëé is smaller than ùëÉ ‚Ä≤
ùëé, iÔ¨Ä for every await sùë§
in ùëÉùëé, there exists an await s ‚Ä≤
ùë§ in ùëÉ ‚Ä≤
ùëé that matches the same call as sùë§, such that
Cover(sùë§) ‚äÜ Cover(s ‚Ä≤
ùë§). For example, the two asynchronous programs in Fig. 7
are ordered by ‚â§ since Cover(await ùëü1) = {} in the Ô¨Årst and Cover(await ùëü1) =
{r2 = x} in the second. Note that the strong asynchronization is smaller than
every other asynchronization. Also, note that ‚â§ has a unique maximal element
that is called the weakest asynchronization and denoted by wkAsy[ùëÉ, ùêø, ùêøùëé]. In
Fig. 8, the weakest asynchronization corresponds to the vector (2, 1).

In the following, we say moving an await down (resp., up) when moving the
await further away from (resp. closer to) the matching call while preserving well-
formedness conditions in Section 2. Further away or closer to means increasing
or decreasing the set of statements that are covered by the await. For instance,
if an await sùë§ in a program ùëÉùëé is preceded by a while loop, then moving it up
means moving it before the whole loop and not inside the loop body. Otherwise,
the third well-formedness condition would be violated.
Relative Maximality. A crucial property of this partial order is that for every
asynchronization ùëÉùëé, there exists a unique maximal asynchronization that is
smaller than ùëÉùëé and that is sound. Formally, an asynchronization ùëÉ ‚Ä≤
ùëé is called a
maximal asynchronization of ùëÉ relative to ùëÉùëé if (1) ùëÉ ‚Ä≤
ùëé is sound, and
(2) ‚àÄ ùëÉ ‚Ä≤‚Ä≤

ùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé]. ùëÉ ‚Ä≤‚Ä≤

ùëé ‚â§ ùëÉùëé, ùëÉ ‚Ä≤
ùëé ‚â§ ùëÉ ‚Ä≤
ùëé.

ùëé is sound and ùëÉ ‚Ä≤‚Ä≤

ùëé ‚â§ ùëÉùëé ‚áí ùëÉ ‚Ä≤‚Ä≤

Lemma 1. Given an asynchronization ùëÉùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé], there exists a unique
program ùëÉ ‚Ä≤
ùëé that is a maximal asynchronization of ùëÉ relative to ùëÉùëé.

Automated Synthesis of Asynchronizations

13

Algorithm 1 An algorithm for enumerating all sound asynchronizations (these
asynchronizations are obtained as a result of the output instruction). MaxRel
returns the maximal asynchronization of ùëÉ relative to ùëÉùëé
1: procedure AsySyn(ùëÉùëé, sùë§)
2:
3:
4:
5:
6:

ùëÉ ‚Ä≤
ùëé ‚Üê MaxRel(ùëÉùëé);
output ùëÉ ‚Ä≤
ùëé;
ùí´ ‚Üê ImPred(ùëÉ ‚Ä≤
for each (ùëÉ ‚Ä≤‚Ä≤

ùëé , s ‚Ä≤‚Ä≤
AsySyn(ùëÉ ‚Ä≤‚Ä≤

ùëé, sùë§);
ùë§) ‚àà ùí´
ùëé , s ‚Ä≤‚Ä≤
ùë§);

ùëé and ùëÉ 2

The asynchronization ùëÉ ‚Ä≤

ùëé exists because the bottom element of ‚â§ is sound.
To prove uniqueness, assume by contradiction that there exist two incomparable
maximal asynchronizations ùëÉ 1
ùë§ w.r.t. the
control-Ô¨Çow of the sequential program that is placed in diÔ¨Äerent positions in the
two programs. Assume that s 1
ùëé . Then, we
move s 1
ùëé further away from its matching call to the same position as in
ùëÉ 2
ùëé . This modiÔ¨Åcation does not introduce data races since ùëÉ 2
ùëé is data race free.
Thus, the resulting program is data race free, bigger than ùëÉ 1
ùëé , and smaller than
ùëÉùëé w.r.t. ‚â§ contradicting the fact that ùëÉ 1

ùë§ is closer to its matching call in ùëÉ 1

ùëé and select the Ô¨Årst await s 1

ùëé is a maximal asynchronization.

ùë§ in ùëÉ 1

4.1 Enumeration Algorithm

Our algorithm for enumerating all sound asynchronizations is given in Algo-
rithm 1 as a recursive procedure AsySyn that we describe in two phases.

First, ignore the second argument of AsySyn (in blue), which represents an
await statement. For an asynchronization ùëÉùëé, AsySyn outputs all sound asyn-
chronizations that are smaller than ùëÉùëé. It uses MaxRel to compute the maximal
asynchronization ùëÉ ‚Ä≤
ùëé of ùëÉ relative to ùëÉùëé, and then, calls itself recursively for all
ùëé. AsySyn outputs all sound asynchronizations of
immediate predecessors of ùëÉ ‚Ä≤
ùëÉ when given as input the weakest asynchronization of ùëÉ .

Recursive calls on immediate predeces-
sors are necessary because the set of sound
asynchronizations is not downward-closed
w.r.t. ‚â§. For instance, the asynchroniza-
tion on the right of Fig. 9 is an immediate
predecessor of the sound asynchronization
on the left but it has a data race on ùë•.

async method m {
r1 = call m1;
r2 = x;
await r1;

}
async method m1 {
r3 = call m2;
x = x + 1;
await r3;

}
async method m2 {

async method m {
r1 = call m1;
r2 = x;
await r1;

}
async method m1 {
r3 = call m2;
await r3;
x = x + 1;

}
async method m2 {

await *
retVal = input;
}
return;

await *
retVal = input;
}
return;

The delay complexity of this algorithm
remains exponential
in general, since a
sound asynchronization may be outputted
multiple times. Asynchronizations are only partially ordered by ‚â§ and diÔ¨Äerent
chains of recursive calls starting in diÔ¨Äerent immediate predecessors may end up
outputting the same solution. For instance, for the asynchronizations in Fig. 8,
the asynchronization (0, 0) will be outputted twice because it is an immediate
predecessor of both (1, 0) and (0, 1).

Fig. 9: Asynchronizations.

14

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

To avoid this redundancy, we use a reÔ¨Ånement of the above that restricts
the set of immediate predecessors available for a (recursive) call of AsySyn.
This is based on a strict total order ‚â∫ùë§ between awaits in a program ùëÉùëé that
follows a topological ordering of its inter-procedural CFG, i.e., if sùë§ occurs before
s ‚Ä≤
ùë§ in the body of a method ùëö, then sùë§ ‚â∫ùë§ s ‚Ä≤
ùë§, and if sùë§ occurs in a method
ùë§ occurs in a method ùëö‚Ä≤ s.t. ùëö (indirectly) calls ùëö‚Ä≤, then sùë§ ‚â∫ùë§ s ‚Ä≤
ùëö and s ‚Ä≤
ùë§.
Therefore, AsySyn takes an await statement sùë§ as a second parameter, which
is initially the maximal element w.r.t. ‚â∫ùë§, and it calls itself only on immediate
predecessors of a solution obtained by moving up an await s ‚Ä≤‚Ä≤
ùë§ smaller than or
equal to sùë§ w.r.t. ‚â∫ùë§. The recursive call on that predecessor will receive as input
s ‚Ä≤‚Ä≤
ùë§. Formally, this relies on a function ImPred that returns pairs of immediate
predecessors and await statements deÔ¨Åned as follows:

ImPred(ùëÉ ‚Ä≤

ùëé, sùë§) = {(ùëÉ ‚Ä≤‚Ä≤

ùëé , s ‚Ä≤‚Ä≤

ùë§) : ùëÉ ‚Ä≤‚Ä≤

ùëé < ùëÉ ‚Ä≤
and s ‚Ä≤‚Ä≤

ùëé and ‚àÄ ùëÉ ‚Ä≤‚Ä≤‚Ä≤
ùë§ ‚™Øùë§ sùë§ and ùëÉ ‚Ä≤‚Ä≤

ùëé ‚àà ùëÉ ‚Ä≤

ùëé ‚Üë s ‚Ä≤‚Ä≤

ùë§ }

ùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé]. ùëÉ ‚Ä≤‚Ä≤‚Ä≤

ùëé < ùëÉ ‚Ä≤

ùëé =‚áí ùëÉ ‚Ä≤‚Ä≤‚Ä≤

ùëé ‚â§ ùëÉ ‚Ä≤‚Ä≤
ùëé

ùëé ‚Üë s ‚Ä≤‚Ä≤

ùë§, moving it up w.r.t. the position in ùëÉ ‚Ä≤

ùë§ is the set of asynchronizations obtained from ùëÉ ‚Ä≤

(ùëÉ ‚Ä≤
ùëé by changing only the
position of s ‚Ä≤‚Ä≤
ùëé). For instance, looking
at immediate predecessors of (1, 1) in Fig. 8, (0, 1) is obtained by moving the
first await in ‚â∫ùë§. Therefore, the recursive call on (0, 1) computes the maximal
asynchronization relative to (0, 1), which is (0, 1), and stops (ImPred returns ‚àÖ
because the input sùë§ is the minimal element of ‚â∫ùë§, and already immediately
after the call). Its immediate predecessor is explored when recursing on (1, 0).

Algorithm 1 outputs all sound asynchronizations because after having com-
ùëé in a recursive call with parameter sùë§, any

ùëé and ùëÉ 2
ùëé, sùë§) obtained by moving up the awaits s 1

puted a maximal asynchronization ùëÉ ‚Ä≤
smaller sound asynchronization is smaller than some predecessor in ImPred(ùëÉ ‚Ä≤
Also, it can not output the same asynchonization twice. Let ùëÉ 1
predecessors in ImPred(ùëÉ ‚Ä≤
respectively, and assume that s 1
cursive call on ùëÉ 1
in the recursive call on ùëÉ 2
the sets of solutions computed in these two recursion branches are distinct.
Theorem 1. AsySyn(wkAsy[ùëÉ, ùêø, ùêøùëé], sùë§), where sùë§ is maximal in wkAsy[ùëÉ, ùêø, ùêøùëé]
w.r.t. ‚â∫ùë§, outputs all sound asynchronizations of ùëÉ [ùêø] w.r.t. ùêøùëé.

ùëé be two
ùë§ and s 2
ùë§,
ùë§. Then, all solutions computed in the re-
ùëé while all the solutions computed
ùë§ closer to the matching call. Therefore,

ùë§ placed as in ùëÉ ‚Ä≤

ùëé will have s 2

ùëé will have s 2

ùë§‚â∫ùë§s 2

ùëé, sùë§).

The delay complexity of Algorithm 1 is polynomial time modulo an oracle
that returns a maximal asynchronization relative to a given one. In the next
section, we show that the latter problem can be reduced in polynomial time to
the reachability problem in sequential programs.

5 Computing Maximal Asynchronizations

In this section, we present an implementation of the procedure MaxRel that
relies on a reachability oracle. In particular, we Ô¨Årst describe an approach for
computing the maximal asynchronization relative to a given asynchronization
ùëÉùëé, which can be seen as a way of repairing ùëÉùëé so that it becomes data-race

Automated Synthesis of Asynchronizations

15

free. Intuitively, we repeatedly eliminate data races in ùëÉùëé by moving certain
await statements closer to the matching calls. The data races in ùëÉùëé (if any) are
enumerated in a certain order that prioritizes data races between actions that
occur Ô¨Årst in executions of the original synchronous program. This order allows
to avoid superÔ¨Çuous repair steps.

5.1 Data Race Ordering
An action a representing a read/write access in a trace ùúè of an asynchronization
ùëÉùëé of ùëÉ is synchronously reachable if there is an action a ‚Ä≤ in a trace ùúè ‚Ä≤ of ùëÉ
that represents the same statement, i.e., S(a) = S(a ‚Ä≤). It can be proved that
any trace of an asynchronization contains a data race if it contains a data race
between two synchronously reachable actions (see Appendix C). In the following,
we focus on data races between actions that are synchronously reachable.

We deÔ¨Åne an order between such data races based on the order between
actions in executions of the original synchronous program ùëÉ . This order relates
data races in possibly diÔ¨Äerent executions or asynchronizations of ùëÉ , which is
possible because each action in a data race corresponds to a statement in ùëÉ .

For two read/write statements s and s ‚Ä≤, s ‚â∫ s ‚Ä≤ denotes the fact that there
is an execution of ùëÉ in which the first time s is executed occurs before the
first time s ‚Ä≤ is executed. For two actions a and a ‚Ä≤ in an execution/trace of an
asynchronization, generated by two read/write statements s = S(ùëé) and s ‚Ä≤ =
S(ùëé‚Ä≤), a ‚â∫SO a ‚Ä≤ holds if s ‚â∫ s ‚Ä≤ and either s ‚Ä≤ Ã∏‚â∫ s or s ‚Ä≤ is reachable from s in the
interprocedural6 control-Ô¨Çow graph of ùëÉ without taking any back edge7. For a
deterministic synchronous program (admitting a single execution), a ‚â∫SO a ‚Ä≤ iÔ¨Ä
S(ùëé) ‚â∫ S(ùëé‚Ä≤). For non-deterministic programs, when S(ùëé) and S(ùëé‚Ä≤) are contained
in a loop body, it is possible that S(ùëé) ‚â∫ S(ùëé‚Ä≤) and S(ùëé‚Ä≤) ‚â∫ S(ùëé). In this case, we
use the control-Ô¨Çow order to break the tie between a and a ‚Ä≤.

The order between data races corresponds to the colexicographic order in-
duced by ‚â∫SO. This is a partial order since actions may originate from diÔ¨Äerent
control-Ô¨Çow paths and are incomparable w.r.t. ‚â∫SO.
Definition 2 (Data Race Order). Given two races (a1, a2) and (a3, a4) ad-
mitted by (possibly different) asynchronizations of a synchronous program ùëÉ , we
have that (a1, a2) ‚â∫SO (a3, a4) iff a2 ‚â∫SO a4, or a2 = a4 and a1 ‚â∫SO a3.

Repairing a minimal data race (a1, a2) w.r.t. ‚â∫SO removes any other data race
(a1, a4) with (a2, a4) ‚àà HB (note that we cannot have (a4, a2) Ã∏‚àà HB since a2 ‚â∫SO
a4). The repair will enforce that (a1, a2) ‚àà HB which implies that (a1, a4) ‚àà HB.

5.2 Repairing Data Races
Repairing a data race (a1, a2) reduces to modifying the position of a certain
await. We consider only repairs where awaits are moved up (closer to the match-
ing call). The ‚Äúcompleteness‚Äù of this set of repairs follows from the particular
order in which we enumerate data races.
6 The interprocedural graph is the union of the control-flow graphs of each method
along with edges from call sites to entry nodes, and from exit nodes to return sites.
7 A back edge points to a block that has already been met during a depth-first traversal

of the control-flow graph, and corresponds to loops.

16

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

Algorithm 2 The procedure MaxRel to Ô¨Ånd the maximal asynchronization of
ùëÉ relative to ùëÉùëé.
1: procedure MaxRel(ùëÉùëé)
2:
3:
4:
5:
6:
7:

ùëÉ ‚Ä≤
root ‚Üê RCMinDR(ùëÉ ‚Ä≤
ùëé)
while root Ã∏= ‚ä•

ùëÉ ‚Ä≤
ùëé ‚Üê RDR(ùëÉ ‚Ä≤
root ‚Üê RCMinDR(ùëÉ ‚Ä≤
ùëé)

ùëé ‚Üê ùëÉùëé

ùëé, root)

return ùëÉ ‚Ä≤
ùëé

Let s1 and s2 be the statements generating a1
and a2. In general, there exists a method ùëö that
(transitively) calls another asynchronous method
ùëö1 that contains s1 and before awaiting for ùëö1 it
(transitively) calls a method ùëö2 that executes s2.
This is pictured in Fig. 10. It is also possible that
ùëö itself contains s2 (see the program on the right
of Fig. 7). The repair consists in moving the await
for ùëö1 before the call to ùëö2 since this implies that s1 will always execute before
s2 (and the corresponding actions are related by happens-before).

Fig. 10: A data race repair.

Formally, any two racing actions have a common ancestor in the call order
CO which is a call action. The least common ancestor of a1 and a2 in CO among
call actions is denoted by LCACO(a1, a2). In Fig. 10, it corresponds to the call
statement sùëê. More precisely, LCACO(a1, a2) is a call action aùëê = ( , ùëñ, call(ùëó)) s.t.
(aùëê, a1) ‚àà CO, (aùëê, a2) ‚àà CO, and for each other call action a ‚Ä≤
ùëê) ‚àà CO
then (a ‚Ä≤
ùëê, a1) Ã∏‚àà CO. This call action represents an asynchronous call for which
the matching await sùë§ must move to repair the data race. The await should
be moved before the last statement in the same method generating an action
which precedes a2 in the reÔ¨Çexive closure of call order (statement s in Fig. 10).
This way every statement that follows sùëê in call order will be executed before s
and before any statement which succeeds s in call order, including s2. Note that
moving the await sùë§ anywhere after s will not aÔ¨Äect the concurrency between
a1 and a2.

ùëê, if (aùëê, a ‚Ä≤

The pair (sùëê, s) is called the root cause of the data race (ùëé1, ùëé2). Let RDR(ùëÉùëé, sùëê, s)

be the maximal asynchronization ùëÉ ‚Ä≤
ment matching sùëê occurs after s on a CFG path.

ùëé smaller than ùëÉùëé w.r.t. ‚â§, s.t. no await state-

5.3 A Procedure for Computing Maximal Asynchronizations
Given an asynchronization ùëÉùëé, the procedure MaxRel in Algorithm 2 computes
the maximal asynchronization relative to ùëÉùëé by repairing data races iteratively
until the program becomes data race free. The sub-procedure RCMinDR(ùëÉ ‚Ä≤
ùëé)
computes the root cause of a minimal data race (a1, a2) of ùëÉ ‚Ä≤
ùëé w.r.t. ‚â∫SO such that
ùëé is data race free, RCMinDR(ùëÉ ‚Ä≤
the two actions are synchronously reachable. If ùëÉ ‚Ä≤
ùëé)
returns ‚ä•. The following theorem states the correctness of MaxRel.
Theorem 2. Given an asynchronization ùëÉùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé], MaxRel(ùëÉùëé) re-
turns the maximal asynchronization of ùëÉ relative to ùëÉùëé.

async method m2 {     s2: ‚Ä¶ ;}async method m1 {     s1: ‚Ä¶ ;}async method m {  sc: r1 = call m1;   s: r2 = call m2;  sw: await r1;}Automated Synthesis of Asynchronizations

17

MaxRel(ùëÉùëé) repairs a number of data races which is linear in the size of
the input. Indeed, each repair results in moving an await closer to the matching
call and before at least one more statement from the original program ùëÉ .

The problem of computing root causes of minimal data races is reducible to
reachability (assertion checking) in sequential programs. This reduction builds
on a program instrumentation for checking if there exists a data race that in-
volves two given statements (s1, s2) that are reachable in an executions of ùëÉ .This
instrumentation is used in an iterative process where pairs of statements are
enumerated according to the colexicographic order induced by ‚â∫. For lack of
space, we present only the main ideas of the instrumentation (see Appendix D).
The instrumentation simulates executions of an asynchronization ùëÉùëé using non-
deterministic synchronous code where methods may be only partially executed
(modeling await interruptions). Immediately after executing s1, the current in-
vocation ùë°1 is interrupted (by executing a return added by the instrumentation).
The active invocations that transitively called ùë°1 are also interrupted when reach-
ing an await for an invocation in this call chain (the other invocations are ex-
ecuted until completion as in the synchronous semantics). When reaching s2, if
s1 has already been executed and at least one invocation has been interrupted,
which means that s1 is concurrent with s2, then the instrumentation stops with
an assertion violation. The instrumentation also computes the root cause of the
data race using additional variables for tracking call dependencies.

6 Asymptotic Complexity of Asynchronization Synthesis

We state the complexity of the asynchronization synthesis problem. Algorithm 1
shows that the delay complexity of this problem is polynomial-time in the num-
ber of statements in input program modulo the complexity of computing a
maximal asynchronization, which Algorithm 2 shows to be polynomial-time re-
ducible to reachability in sequential programs. Since the reachability problem is
PSPACE-complete for Ô¨Ånite-state sequential programs [16], we get the following:

Theorem 3. The output complexity8 and delay complexity of the asynchroniza-
tion synthesis problem is polynomial time modulo an oracle for reachability in
sequential programs, and PSPACE for finite-state programs.

This result is optimal, i.e., checking whether there exists a sound asynchro-
nization which is diÔ¨Äerent from the trivial strong synchronization is PSPACE-
hard (follows from a reduction from the reachability problem). See Appendices D
and E for the detailed formal proofs.

7 Asynchronization Synthesis Using Data-Flow Analysis

In this section, we present a reÔ¨Ånement of Algorithm 2 that relies on a bottom-
up inter-procedural data Ô¨Çow analysis. The analysis is used to compute maximal
asynchronizations for abstractions of programs where every Boolean condition

8 Note that all asynchronizations can be enumerated with polynomial space.

18

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

(in if-then-else or while statements) is replaced with the non-deterministic choice
*, and used as an implementation of MaxRel in Algorithm 1.

For a program ùëÉ , we deÔ¨Åne an abstraction ùëÉ # where every conditional if
‚ü®ùëôùëí‚ü© {ùëÜ1} else {ùëÜ2} is rewritten to if * {ùëÜ1} else {ùëÜ2}, and every while ‚ü®ùëôùëí‚ü©
{ùëÜ} is rewritten to if * {ùëÜ}. Besides adding the non-deterministic choice *,
loops are unrolled exactly once. Every asynchronization ùëÉùëé of ùëÉ corresponds to
an abstraction ùëÉ #
ùëé obtained by applying exactly the same rewriting. ùëÉ # is a
sound abstraction of ùëÉ in terms of sound asynchronizations it admits. Unrolling
loops once is sound because every asynchronous call in a loop iteration should
be awaited for in the same iteration (see the syntactic constraints in Section 2).

Theorem 4. If ùëÉ #
sound asynchronization of ùëÉ w.r.t. ùêøùëé.

ùëé is a sound asynchronization of ùëÉ # w.r.t. ùêøùëé, then ùëÉùëé is a

The procedure for computing maximal asynchronizations of ùëÉ # relative to
a given asynchronization ùëÉ #
ùëé traverses methods of ùëÉ #
ùëé in a bottom-up fashion,
detects data races using summaries of read/write accesses computed using a
straightforward data-Ô¨Çow analysis, and repairs data races using the schema pre-
sented in Section 5.2. Applying this procedure to a real programming language
requires an alias analysis to detect statements that may access the same memory
location (this is trivial in our language which is used to simplify the exposition).
We consider an enumeration of methods called bottom-up order, which is the
reverse of a topological ordering of the call graph9. For each method ùëö, let ‚Ñõ(ùëö)
be the set of program variables that ùëö can read, which is deÔ¨Åned as the union
of ‚Ñõ(ùëö‚Ä≤) for every method ùëö‚Ä≤ called by ùëö and the set of program variables
read in statements in the body of ùëö. The set of variables ùí≤(ùëö) that ùëö can
write is deÔ¨Åned in a similar manner. We deÔ¨Åne RW-var(ùëö) = (‚Ñõ(ùëö), ùí≤(ùëö)).
We extend the notation RW-var to statements as follows: RW-var(‚ü®ùëü‚ü© := ‚ü®ùë•‚ü©) =
({ùë•}, ‚àÖ), RW-var(‚ü®ùë•‚ü© := ‚ü®ùëôùëí‚ü©) = (‚àÖ, {ùë•}), RW-var(ùëü := call ùëö) = RW-var(ùëö),
and RW-var(s) = (‚àÖ, ‚àÖ), for any other type of statement ùë†. Also, let CRW-var(ùëö)
be the set of read or write accesses that ùëö can do and that can be concurrent
with accesses that a caller of ùëö can do after calling ùëö. These correspond to
read/write statements that follow an await in ùëö, or to accesses in CRW-var(ùëö‚Ä≤)
for a method ùëö‚Ä≤ called by ùëö. These sets of accesses can be computed using the
following data-Ô¨Çow analysis: for all methods ùëö ‚àà ùëÉ #
ùëé in bottom-up order, and
for each statement s in the body of ùëö from begin to end,

‚Äì if s is a call to ùëö‚Ä≤ and s is not reachable from an await in the CFG of ùëö

‚àô CRW-var(ùëö) ‚Üê CRW-var(ùëö) ‚à™ CRW-var(ùëö‚Ä≤)

‚Äì if s is reachable from an await statement in the CFG of ùëö

‚àô CRW-var(ùëö) ‚Üê CRW-var(ùëö) ‚à™ RW-var(s)

We use (‚Ñõ1, ùí≤1) ‚óÅ‚ñ∑ (‚Ñõ2, ùí≤2) to denote the fact that ùí≤1 ‚à© (‚Ñõ2 ‚à™ ùí≤2) Ã∏= ‚àÖ or
ùí≤2 ‚à© (‚Ñõ1 ‚à™ ùí≤1) Ã∏= ‚àÖ (i.e., a conÔ¨Çict between read/write accesses). We deÔ¨Åne the
procedure MaxRel# that given an asynchronization ùëÉ #

ùëé works as follows:

9 The nodes of the call graph are methods and there is an edge from a method ùëö1 to

a method ùëö2 if ùëö1 contains a call statement that calls ùëö2.

Automated Synthesis of Asynchronizations

19

‚Äì for all methods ùëö ‚àà ùëÉ #

ùëé in bottom-up order, and for each statement s in

the body of ùëö from begin to end,

‚àô if s occurs between ùëü := call ùëö‚Ä≤ and await ùëü (for some ùëö‚Ä≤), and

RW-var(s) ‚óÅ‚ñ∑ CRW-var(ùëö‚Ä≤), then ùëÉ #

ùëé ‚Üê RDR(ùëÉ #

ùëé , ùëü := call ùëö‚Ä≤, ùë†)

‚Äì return ùëÉ #
ùëé

Theorem 5. MaxRel#(ùëÉ #
Since MaxRel# is based on a single bottom-up traversal of the call graph of
the input asynchronization ùëÉ #

ùëé ) returns a maximal asynchronization relative to ùëÉ #
ùëé .

ùëé we get the following result.

Theorem 6. The delay complexity of the asynchronization synthesis problem
restricted to abstracted programs ùëÉ # is polynomial time.

8 Experimental Evaluation

We present an empirical evaluation of our asynchronization synthesis approach,
where maximal asynchronizations are computed using the data-Ô¨Çow analysis in
Section 7. Our benchmark consists mostly of asynchronous C# programs from
open-source GitHub projects. We evaluate the eÔ¨Äectiveness in reproducing the
original program as an asynchronization of a program where asynchronous calls
are reverted to synchronous calls, along with other sound asynchronizations.
Implementation. We developed a prototype tool that uses the Roslyn .NET
compiler platform [27] to construct CFGs for methods in a C# program. This
prototype supports C# programs written in static single assignment (SSA) form
that include basic conditional/looping constructs and async/await as concur-
rency primitives. Note that object Ô¨Åelds are interpreted as program variables in
the terminology of ¬ß2 (data races concern accesses to object Ô¨Åelds). It assumes
that alias information is provided apriori; these constraints can be removed in
the future with more engineering eÔ¨Äort. In general, our synthesis procedure is
compatible with any sound alias analysis. The precision of this analysis impacts
only the set (number) of asynchronizations outputted by the procedure (a more
precise analysis may lead to more sound asynchronizations).

The tool takes as input a possibly asynchronous program, and a mapping
between synchronous and asynchronous variations of base methods in this pro-
gram. It reverts every asynchronous call to a synchronous call, and it enumerates
sound asynchronizations of the obtained program (using Algorithm 1).
Benchmark. Our evaluation uses a benchmark listed in Table 2, which con-
tains 5 synthetic examples (variations of the program in Fig. 1), 9 programs
extracted from open-source C# GitHub projects (their name is a preÔ¨Åx of the
repository name), and 2 programs inspired by questions on stackoverflow.com
about async/await in C# (their name ends in StackoverÔ¨Çow). Overall, there
are 13 base methods involved in computing asynchronizations of these pro-
grams (having both synchronous and asynchronous versions), coming from 5 C#
libraries (System.IO, System.Net, Windows.Storage, Microsoft.WindowsAzure.-
Storage, and Microsoft.Azure.Devices). They are modeled as described in Sec-
tion 2.

20

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

Table 2: Empirical results. Syntactic characteristics of input programs: lines
of code (loc), number of methods (m), number of method calls (c), number of
asynchronous calls (ac), number of awaits that could be placed at least one state-
ment away from the matching call (await#). Data concerning the enumeration
of asynchronizations: number of awaits that were placed at least one statement
away from the matching call (await), number of races discovered and repaired
(races), number of statements that the awaits in the maximal asynchronization
are covering more than in the input program (cover), number of computed asyn-
chronizations (async), and running time (t).

Program

loc m c

SyntheticBenchmark-1
SyntheticBenchmark-2
SyntheticBenchmark-3
SyntheticBenchmark-4
SyntheticBenchmark-5
Azure-Remote
Azure-Webjobs
FritzDectCore
MultiPlatform
NetRpc
TestAZureBoards
VBForums-Viewer
Voat
WordpressRESTClient
ReadFile-Stackoverflow 47
50
UI-Stackoverflow

77
115
168
171
170
520
190
141
53
887
43
275
178
133

3
4
6
6
6
10
6
7
2
13
3
7
3
3
2
3

6
12
16
17
17
14
14
11
6
18
3
10
5
10
3
4

ac

5
10
13
14
14
5
6
8
4
11
3
7
5
8
3
4

await#

await

races

cover

async

t(s)

4
6
9
10
10
0
1
1
2
4
0
3
2
4
1
3

4
3
7
8
8
0
1
1
2
1
0
2
1
2
0
3

5
3
4
5
9
0
0
0
0
3
0
1
1
1
1
3

0
0
0
0
0
0
1
1
2
0
0
1
1
0
0
0

9
8
128
256
272
1
3
2
4
3
1
6
3
4
1
12

1.4
1.4
1.5
1.9
2
2.2
1.6
1.6
1.1
2
1.5
1.8
1.2
1.7
1.5
1.5

Evaluation. The last Ô¨Åve columns of Table 2 list data concerning the application
of our tool. The column async lists the number of outputted sound asynchro-
nizations. In general, the number of asynchronizations depends on the number
of invocations (column ac) and the size of the code blocks between an invocation
and the instruction using its return value (column await# gives the number of
non-empty blocks). The number of sound asynchronizations depends roughly, on
how many of these code blocks are racing with the method body. These asyn-
chronizations contain awaits that are at a non-zero distance from the matching
call (non-zero values in column await) and for many Github programs, this dis-
tance is bigger than in the original program (non-zero values in column cover).
This shows that we are able to increase the distances between awaits and their
matching calls for those programs. The distance between awaits and matching
calls in maximal asynchronizations of non synthetic benchmarks is 1.27 state-
ments on average. A statement representing a method call is counted as one
independently of the method‚Äôs body size. With a single level of inlining, the
number of statements becomes 2.82 on average. However, these statements are
again, mostly IO calls (access to network or disk) or library calls (string/bytes
formatting methods) whose execution time is not negligible. The running times
for the last three synthetic benchmarks show that our procedure is scalable when
programs have a large number of sound asynchronizations.

With few exceptions, each program admits multiple sound asynchronizations
(values in column async bigger than one), which makes the focus on the delay
complexity relevant. This leaves the possibility of making a choice based on

Automated Synthesis of Asynchronizations

21

other criteria, e.g., performance metrics. As shown by the examples in Fig. 2,
their performance can be derived only dynamically (by executing them). These
results show that our techniques have the potential of becoming the basis of a
refactoring tool allowing programmers to improve their usage of the async/await
primitives. The artifacts are available in a GitHub repository [2].

9 Related Work

There are many works on synthesizing or repairing concurrent programs in
the standard multi-threading model, e.g., automatic parallelization in compil-
ers [1,6,19], or synchronization synthesis [10,11,12,24,31,30,5,9,18]. We focus on
the use of async/await which poses speciÔ¨Åc challenges that are not covered in
these works.

Our semantics without await * instructions is equivalent to the semantics
deÔ¨Åned in [3,28]. But, to simplify the exposition, we consider a more restricted
programming language. For the modeling of asynchronous IO operations, we
follow [3] with the restriction that the code following an await * is executed
atomically. This is sound when focusing on data-race freedom because even if
executed atomically, any two instructions from diÔ¨Äerent asynchronous IO oper-
ations (following await *) are not happens-before related.
Program Refactoring. Program refactoring tools have been proposed for con-
verting C# programs using explicit callbacks into async/await programs [25] or
Android programs using AsyncTask into programs that use IntentService [22].
The C# tool [25], which is the closest to our work, makes it possible to repair
misusage of async/await that might result in deadlocks. This tool cannot mod-
ify procedure calls to be asynchronous as in our work. A static analysis based
technique for refactoring JavaScript programs is proposed in [17]. As opposed
to our work, this refactoring technique is unsound in general. It requires that
programmers review the refactoring for correctness, which is error-prone. Also,
in comparison to [17], we carry a formal study of the more general problem of
Ô¨Ånding all sound asynchronizations and investigate its complexity.
Data Race Detection. Many works study dynamic data race detection using
happens-before and lock-set analysis, or timing-based detection [21,20,29,26,14].
They could be used to approximate our reduction from data race checking to
reachability in sequential programs. Some works [4,23,13] propose static analyses
for Ô¨Ånding data races. [4] designs a compositional data race detector for multi-
threaded Java programs, based on an inter-procedural analysis assuming that
any two public methods can execute in parallel. Similar to [28], they precom-
pute method summaries to extract potential racy accesses. These approaches are
similar to the analysis in Section 7, but they concern a diÔ¨Äerent programming
model.
Analyzing Asynchronous Programs. Several works propose program analy-
ses for various classes of asynchronous programs. [7,15] give complexity results for
the reachability problem, and [28] proposes a static analysis for deadlock detec-
tion in C# programs that use both asynchronous and synchronous wait prim-
itives. [8] investigates the problem of checking whether Java UI asynchronous

22

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

programs have the same set of behaviors as sequential programs where roughly,
asynchronous tasks are executed synchronously.

10 Conclusion

We proposed a framework for refactoring sequential programs to equivalent asyn-
chronous programs based on async/await. We determined precise complexity
bounds for the problem of computing all sound asynchronizations. This problem
makes it possible to compute a sound asynchronization that maximizes perfor-
mance by separating concerns ‚Äì enumerate sound asynchronizations and evaluate
performance separately. On the practical side, we have introduced an approxi-
mated synthesis procedure based on data-Ô¨Çow analysis that we implemented and
evaluated on a benchmark of non-trivial C# programs.

The asynchronous programs rely exclusively on async/await and are deadlock-
free by deÔ¨Ånition. Deadlocks can occur in a mix of async/await with ‚Äúexplicit‚Äù
multi-threading that includes blocking wait primitives. Extending our approach
for such programs is an interesting direction for future work.

References

1. Bacon, D.F., Graham, S.L., Sharp, O.J.: Compiler

for
high-performance computing. ACM Comput. Surv. 26(4), 345‚Äì420 (1994).
https://doi.org/10.1145/197405.197406,
https://doi.org/10.1145/197405.
197406

transformations

2. Beillahi, S.M., Bouajjani, A., Enea, C., Lahiri, S.: Artifact for the SAS
2022).
https://doi.org/10.5281/zenodo.

2022
https://doi.org/10.5281/zenodo.7055422,
7055422

of Asynchronizations

paper: Automated

Synthesis

(May

3. Bierman, G.M., Russo, C.V., Mainland, G., Meijer, E., Torgersen, M.: Pause ‚Äôn‚Äô
play: Formalizing asynchronous c#. In: Noble, J. (ed.) ECOOP 2012 - Object-
Oriented Programming - 26th European Conference, Beijing, China, June 11-
16, 2012. Proceedings. Lecture Notes in Computer Science, vol. 7313, pp. 233‚Äì
257. Springer (2012). https://doi.org/10.1007/978-3-642-31057-7 12, https://
doi.org/10.1007/978-3-642-31057-7_12

4. Blackshear, S., Gorogiannis, N., O‚ÄôHearn, P.W., Sergey, I.: Racerd: composi-
tional static race detection. Proc. ACM Program. Lang. 2(OOPSLA), 144:1‚Äì144:28
(2018). https://doi.org/10.1145/3276514, https://doi.org/10.1145/3276514
5. Bloem, R., Hofferek, G., K¬®onighofer, B., K¬®onighofer, R., Ausserlechner, S., Spork,
R.: Synthesis of synchronization using uninterpreted functions. In: Formal Methods
in Computer-Aided Design, FMCAD 2014, Lausanne, Switzerland, October 21-
24, 2014. pp. 35‚Äì42. IEEE (2014). https://doi.org/10.1109/FMCAD.2014.6987593,
https://doi.org/10.1109/FMCAD.2014.6987593

6. Blume, W., Doallo, R., Eigenmann, R., Grout, J., Hoeflinger, J.P., Lawrence,
T., Lee, J., Padua, D.A., Paek, Y., Pottenger, W.M., Rauchwerger, L.,
Tu, P.: Parallel programming with polaris. Computer 29(12), 87‚Äì81 (1996).
https://doi.org/10.1109/2.546612, https://doi.org/10.1109/2.546612

Automated Synthesis of Asynchronizations

23

7. Bouajjani, A., Emmi, M.: Analysis of recursively parallel programs. In: Field,
J., Hicks, M. (eds.) Proceedings of the 39th ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages, POPL 2012, Philadel-
phia, Pennsylvania, USA, January 22-28, 2012. pp. 203‚Äì214. ACM (2012).
https://doi.org/10.1145/2103656.2103681, https://doi.org/10.1145/2103656.
2103681

8. Bouajjani, A., Emmi, M., Enea, C., Ozkan, B.K., Tasiran, S.: Verifying robust-
ness of event-driven asynchronous programs against concurrency. In: Yang, H.
(ed.) Programming Languages and Systems - 26th European Symposium on Pro-
gramming, ESOP 2017, Held as Part of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-29,
2017, Proceedings. Lecture Notes in Computer Science, vol. 10201, pp. 170‚Äì
200. Springer (2017). https://doi.org/10.1007/978-3-662-54434-1 7, https://doi.
org/10.1007/978-3-662-54434-1_7

9. Cern¬¥y, P., Clarke, E.M., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Samanta,
R., Tarrach, T.: From non-preemptive to preemptive scheduling using synchro-
nization synthesis. In: Kroening, D., Pasareanu, C.S. (eds.) Computer Aided Ver-
ification - 27th International Conference, CAV 2015, San Francisco, CA, USA,
July 18-24, 2015, Proceedings, Part II. Lecture Notes in Computer Science,
vol. 9207, pp. 180‚Äì197. Springer (2015). https://doi.org/10.1007/978-3-319-21668-
3 11, https://doi.org/10.1007/978-3-319-21668-3_11

10. Cern¬¥y, P., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Tarrach, T.: Ef-
ficient synthesis for concurrency by semantics-preserving transformations. In:
Sharygina, N., Veith, H. (eds.) Computer Aided Verification - 25th Inter-
national Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.
Proceedings. Lecture Notes in Computer Science, vol. 8044, pp. 951‚Äì967.
Springer (2013). https://doi.org/10.1007/978-3-642-39799-8 68, https://doi.
org/10.1007/978-3-642-39799-8_68

11. Cern¬¥y, P., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Tarrach, T.: Regression-
free synthesis for concurrency.
In: Biere, A., Bloem, R. (eds.) Computer
Aided Verification - 26th International Conference, CAV 2014, Held as Part
of the Vienna Summer of Logic, VSL 2014, Vienna, Austria, July 18-22,
2014. Proceedings. Lecture Notes in Computer Science, vol. 8559, pp. 568‚Äì
584. Springer (2014). https://doi.org/10.1007/978-3-319-08867-9 38, https://
doi.org/10.1007/978-3-319-08867-9_38

12. Clarke, E.M., Emerson, E.A.: Design and synthesis of synchronization skeletons
using branching time temporal logic. In: Grumberg, O., Veith, H. (eds.) 25 Years of
Model Checking - History, Achievements, Perspectives. Lecture Notes in Computer
Science, vol. 5000, pp. 196‚Äì215. Springer (2008). https://doi.org/10.1007/978-3-
540-69850-0 12, https://doi.org/10.1007/978-3-540-69850-0_12

13. Engler, D.R., Ashcraft, K.: Racerx: effective, static detection of race con-
ditions and deadlocks.
(eds.) Proceed-
ings of the 19th ACM Symposium on Operating Systems Principles 2003,
SOSP 2003, Bolton Landing, NY, USA, October 19-22, 2003. pp. 237‚Äì
252. ACM (2003). https://doi.org/10.1145/945445.945468, https://doi.org/10.
1145/945445.945468

In: Scott, M.L., Peterson, L.L.

14. Flanagan, C., Freund, S.N.: Fasttrack: efficient and precise dynamic race de-
tection. In: Hind, M., Diwan, A. (eds.) Proceedings of the 2009 ACM SIG-
PLAN Conference on Programming Language Design and Implementation,
PLDI 2009, Dublin, Ireland, June 15-21, 2009. pp. 121‚Äì133. ACM (2009).

24

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

https://doi.org/10.1145/1542476.1542490, https://doi.org/10.1145/1542476.
1542490

15. Ganty, P., Majumdar, R.: Algorithmic

asynchronous
programs. ACM Trans. Program. Lang. Syst. 34(1),
(2012).
https://doi.org/10.1145/2160910.2160915, https://doi.org/10.1145/2160910.
2160915

of
6:1‚Äì6:48

verification

16. Godefroid, P., Yannakakis, M.: Analysis of boolean programs. In: Piterman, N.,
Smolka, S.A. (eds.) Tools and Algorithms for the Construction and Analysis
of Systems - 19th International Conference, TACAS 2013, Held as Part of the
European Joint Conferences on Theory and Practice of Software, ETAPS 2013,
Rome, Italy, March 16-24, 2013. Proceedings. Lecture Notes in Computer Science,
vol. 7795, pp. 214‚Äì229. Springer (2013). https://doi.org/10.1007/978-3-642-36742-
7 16, https://doi.org/10.1007/978-3-642-36742-7_16

17. Gokhale, S., Turcotte, A., Tip, F.: Automatic migration from synchronous to asyn-
chronous javascript apis. Proc. ACM Program. Lang. 5(OOPSLA), 1‚Äì27 (2021).
https://doi.org/10.1145/3485537, https://doi.org/10.1145/3485537

18. Gupta, A., Henzinger, T.A., Radhakrishna, A., Samanta, R., Tarrach, T.: Suc-
cinct representation of concurrent trace sets. In: Rajamani, S.K., Walker, D.
(eds.) Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL 2015, Mumbai, India, January
15-17, 2015. pp. 433‚Äì444. ACM (2015). https://doi.org/10.1145/2676726.2677008,
https://doi.org/10.1145/2676726.2677008

19. Han, H., Tseng, C.: A comparison of parallelization techniques for irregular reduc-
tions. In: Proceedings of the 15th International Parallel & Distributed Processing
Symposium (IPDPS-01), San Francisco, CA, USA, April 23-27, 2001. p. 27. IEEE
Computer Society (2001). https://doi.org/10.1109/IPDPS.2001.924963, https:
//doi.org/10.1109/IPDPS.2001.924963

time.

In: Cohen, A., Vechev, M.T.

20. Kini, D., Mathur, U., Viswanathan, M.: Dynamic race prediction in lin-
the 38th
ear
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2017, Barcelona, Spain, June 18-23, 2017. pp. 157‚Äì170.
ACM (2017). https://doi.org/10.1145/3062341.3062374, https://doi.org/10.
1145/3062341.3062374

(eds.) Proceedings of

21. Li, G., Lu, S., Musuvathi, M., Nath, S., Padhye, R.: Efficient scalable thread-
safety-violation detection: finding thousands of concurrency bugs during testing.
In: Brecht, T., Williamson, C. (eds.) Proceedings of the 27th ACM Symposium
on Operating Systems Principles, SOSP 2019, Huntsville, ON, Canada, October
27-30, 2019. pp. 162‚Äì180. ACM (2019). https://doi.org/10.1145/3341301.3359638,
https://doi.org/10.1145/3341301.3359638

22. Lin, Y., Okur, S., Dig, D.: Study and refactoring of android asynchronous pro-
gramming (T). In: Cohen, M.B., Grunske, L., Whalen, M. (eds.) 30th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2015, Lincoln,
NE, USA, November 9-13, 2015. pp. 224‚Äì235. IEEE Computer Society (2015).
https://doi.org/10.1109/ASE.2015.50, https://doi.org/10.1109/ASE.2015.50

23. Liu, B., Huang, J.: D4:

fast concurrency debugging with parallel differen-
tial analysis. In: Foster, J.S., Grossman, D. (eds.) Proceedings of the 39th
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018. pp. 359‚Äì
373. ACM (2018). https://doi.org/10.1145/3192366.3192390, https://doi.org/
10.1145/3192366.3192390

Automated Synthesis of Asynchronizations

25

24. Manna, Z., Wolper, P.: Synthesis of communicating processes

from tem-
logic specifications. ACM Trans. Program. Lang. Syst. 6(1), 68‚Äì
poral
93 (1984). https://doi.org/10.1145/357233.357237, https://doi.org/10.1145/
357233.357237

25. Okur, S., Hartveld, D.L., Dig, D., van Deursen, A.: A study and toolkit
for asynchronous programming in c#. In: Jalote, P., Briand, L.C., van der
(eds.) 36th International Conference on Software Engineering,
Hoek, A.
ICSE ‚Äô14, Hyderabad,
India - May 31 - June 07, 2014. pp. 1117‚Äì1127.
ACM (2014). https://doi.org/10.1145/2568225.2568309, https://doi.org/10.
1145/2568225.2568309

26. Raman, R., Zhao, J., Sarkar, V., Vechev, M.T., Yahav, E.: Efficient data race
detection for async-finish parallelism. In: Barringer, H., Falcone, Y., Finkbeiner,
B., Havelund, K., Lee, I., Pace, G.J., Rosu, G., Sokolsky, O., Tillmann, N.
(eds.) Runtime Verification - First International Conference, RV 2010, St. Ju-
lians, Malta, November 1-4, 2010. Proceedings. Lecture Notes in Computer Science,
vol. 6418, pp. 368‚Äì383. Springer (2010). https://doi.org/10.1007/978-3-642-16612-
9 28, https://doi.org/10.1007/978-3-642-16612-9_28

In: Cohen, A., Vechev, M.T.

27. Roslyn: (2021), https://github.com/dotnet/roslyn
28. Santhiar, A., Kanade, A.: Static deadlock detection for asynchronous c#
the 38th
programs.
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2017, Barcelona, Spain, June 18-23, 2017. pp. 292‚Äì305.
ACM (2017). https://doi.org/10.1145/3062341.3062361, https://doi.org/10.
1145/3062341.3062361

(eds.) Proceedings of

29. Smaragdakis, Y., Evans, J., Sadowski, C., Yi, J., Flanagan, C.: Sound predic-
tive race detection in polynomial time. In: Field, J., Hicks, M. (eds.) Proceed-
ings of the 39th ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January
22-28, 2012. pp. 387‚Äì400. ACM (2012). https://doi.org/10.1145/2103656.2103702,
https://doi.org/10.1145/2103656.2103702

30. Vechev, M.T., Yahav, E., Yorsh, G.:

Inferring synchronization under lim-
ited observability. In: Kowalewski, S., Philippou, A. (eds.) Tools and Algo-
rithms for the Construction and Analysis of Systems, 15th International Con-
ference, TACAS 2009, Held as Part of the Joint European Conferences on
Theory and Practice of Software, ETAPS 2009, York, UK, March 22-29,
2009. Proceedings. Lecture Notes in Computer Science, vol. 5505, pp. 139‚Äì
154. Springer (2009). https://doi.org/10.1007/978-3-642-00768-2 13, https://
doi.org/10.1007/978-3-642-00768-2_13

31. Vechev, M.T., Yahav, E., Yorsh, G.: Abstraction-guided synthesis of syn-
chronization. In: Hermenegildo, M.V., Palsberg, J. (eds.) Proceedings of the
37th ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, POPL 2010, Madrid, Spain, January 17-23, 2010. pp. 327‚Äì338.
ACM (2010). https://doi.org/10.1145/1706299.1706338, https://doi.org/10.
1145/1706299.1706338

26

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

A Formalization and Proofs of Section 3

The following lemma shows that the absence of data races implies equivalence
to the original program, in the sense of reaching the same set of conÔ¨Ågurations
(program variable valuations).
Lemma 2. ùëÉùëé[ùêøùëé] is sound implies C[ùëÉ [ùêø]] = C[ùëÉùëé[ùêøùëé]], for every ùëÉùëé[ùêøùëé] ‚àà
Asy[ùëÉ, ùêø, ùêøùëé]

) (i.e., the task ùëó is executed synchronously). If an action ( , ùëñ,

Proof (Proof of Lemma 2). Let ùúå be an execution of ùëÉùëé that reaches a conÔ¨Åg-
uration ps ‚àà C[ùëÉùëé]. We show that actions in ùúå can be reordered such that any
action that occurs in ùúå between ( , ùëñ, call(ùëó)) and ( , ùëó, return) is not of the form
( , ùëñ,
) occurs
in ùúå between ( , ùëñ, call(ùëó)) and ( , ùëó, return), then it must be concurrent with
(ùëó, return). Since ùëÉùëé does not admit data races, an execution ùúå‚Ä≤ resulting from ùúå
by reordering any two concurrent actions reaches the same conÔ¨Åguration ps as
ùúå. Therefore, there exists an execution ùúå‚Ä≤‚Ä≤ where the actions that occur between
any ( , ùëñ, call(ùëó)) and ( , ùëó, return) are not of the form ( , ùëñ,
). This is also an
execution of ùëÉ (modulo removing the awaits which have no eÔ¨Äect), which implies
ps ‚àà C[ùëÉ ].

B Formalization and Proofs of Section 4

ùëé and ùëÉ 2

The following lemma shows that for a given ùëÉùëé there exists a unique ùëÉ ‚Ä≤
ùëé that is
a maximal asynchronization of ùëÉ relative to ùëÉùëé. The existence is implied by the
fact that strongAsy[ùëÉ, ùêø, ùêøùëé] is the bottom element of ‚â§. To prove uniqueness,
we assume by contradiction that there exist two incomparable maximal asyn-
chronizations ùëÉ 1
ùë§, according to
the control-Ô¨Çow of the sequential program, that is placed in diÔ¨Äerent positions
in the two programs. Assume that s 1
ùëé . Then,
we move s 1
ùëé further away from its matching call to the same position as in
ùëÉ 2
ùëé . This modiÔ¨Åcation does not introduce data races since ùëÉ 2
ùëé is data race free.
Thus, the resulting program is data race free, bigger than ùëÉ 1
ùëé , and smaller than
ùëÉùëé w.r.t. ‚â§ contradicting the fact that ùëÉ 1

ùëé and select the Ô¨Årst await statement s 1

ùë§ is closer to its matching call in ùëÉ 1

ùëé is a maximal asynchronization.

ùë§ in ùëÉ 1

Lemma 3. Given an asynchronization ùëÉùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé], there exists a unique
program ùëÉ ‚Ä≤
ùëé that is a maximal asynchronization of ùëÉ relative to ùëÉùëé.

Proof (Proof of Lemma 3). Since strongAsy[ùëÉ, ùêø, ùêøùëé] is the bottom element of
‚â§, then there always exists a sound asynchronization smaller than ùëÉùëé. Assume
by contradiction that there exist two distinct programs ùëÉ 1
ùëé that are
both maximal asynchronizations of ùëÉ relative to ùëÉùëé. Let ùúå1 (resp., ùúå2) be an
execution of ùëÉ 1
ùëé ) where every await * does not suspend the execution
of the current task, i.e., ùúå1 and ùúå2 simulate the synchronous execution of ùëÉ . Let
ùë§ be the statement corresponding to the Ô¨Årst await action in ùúå1 such that (1)
s 1
there exists an await action in ùúå2 with the corresponding await statement s 2
ùë§,
ùë§ match the same call in ùëÉ , and Cover(s 1
such that s 1
ùë§) (this

ùë§) ‚äÇ Cover(s 2

ùëé (resp., ùëÉ 2

ùëé and ùëÉ 2

ùë§ and s 2

Automated Synthesis of Asynchronizations

27

Let ùëÉ 3

ùë§ in ùúå1, there exists an await statement s 4

ùëé are distinct asynchronizations of the same synchronous
ùë§) and Cover(s 2
ùë§) must be comparable), and (2) for every
ùë§ in ùëÉ 1
ùëé that generates an await action which occurs
ùë§ in ùëÉ 2
ùëé

holds because ùëÉ 1
ùëé and ùëÉ 2
program, thus Cover(s 1
other await statement s 3
before the await action of s 1
matching the same call in ùëÉ , such that Cover(s 3
ùëé be the program obtained from ùëÉ 1
ùë§ down
(further away from the matching call) such that Cover(s 1
ùë§). Moving
an await down can only create data races between actions that occur after the
execution of the matching call. Then, ùëÉ 3
ùëé contains a data race iÔ¨Ä there exists
an execution ùúå of ùëÉ 3
ùëé and two concurrent actions a1 and a2 that occur between
the action ( , ùëñ, await(ùëó)) generated by s 1
ùë§ and the action ( , ùëñ, call(ùëó)) of the call
matching s 1

ùë§) = Cover(s 4
ùëé by moving the await s 1

ùë§) = Cover(s 2

ùë§).

ùë§, such that:

(( , ùëñ, call(ùëó)), a1) ‚àà CO, (a1, aùë§) Ã∏‚àà HB, (( , ùëñ, call(ùëó)), a2) ‚àà CO and (a2, ( , ùëñ, await(ùëó))) ‚àà HB

ùëé and ùëÉ 2

where the action aùë§ corresponds to the Ô¨Årst await action in the task ùëó. Let
sùë§ be the statement corresponding to the action aùë§. Since the only diÔ¨Äerence
between ùëÉ 3
ùëé is the placement of awaits then (( , ùëñ, call(ùëó)), a1) ‚àà CO and
(( , ùëñ, call(ùëó)), a2) ‚àà CO hold in any execution ùúå‚Ä≤ of ùëÉ 2
ùëé that contains the actions
a1 and a2. Also, note that since aùë§ occurs in the task ùëó that the action of s 1
ùë§
is waiting for. This implies that in ùúå1 the action of sùë§ occurs before the action
of s 1
ùë§ in ùúå1. Therefore, by the deÔ¨Ånition of s 1
ùëé covers the
same set of statements as the corresponding s ‚Ä≤
ùëé that matches the same
call as sùë§. Consequently, (a1, a ‚Ä≤
ùë§) Ã∏‚àà HB and (a2, ( , ùëñ, await(ùëó))) ‚àà HB hold in
any execution ùúå‚Ä≤ of ùëÉ 2
ùë§ is the action of
s ‚Ä≤
ùë§). Thus, there exists an execution ùúå‚Ä≤ of ùëÉ 2
ùëé such that the actions a1 and a2 are
concurrent. This implies that if ùëÉ 3
ùëé admits a data
race between actions generated by the same statements. As ùëÉ 2
ùëé is data race free,
we get that ùëÉ 3
ùëé is data race free as well. Since ùëÉ 1
ùëé is not
maximal, which contradicts the hypothesis.

ùëé that contains the actions a1 and a2 (a ‚Ä≤

ùëé admits a data race, then ùëÉ 2

ùë§ we have that sùë§ in ùëÉ 1

ùëé , we get that ùëÉ 1

ùë§ in ùëÉ 2

ùëé < ùëÉ 3

The complexity analysis also relies on a property of the maximal asynchro-
nization relative to an immediate predecessor: if the predecessor is deÔ¨Åned by
moving an await s ‚Ä≤‚Ä≤
ùë§, then the maximal asynchronization is obtained by moving
only awaits smaller than s ‚Ä≤‚Ä≤

ùë§ w.r.t. ‚â∫ùë§.

Lemma 4. If ùëÉ ‚Ä≤‚Ä≤
which is defined by moving an await s ‚Ä≤‚Ä≤
chronization relative to ùëÉ ‚Ä≤‚Ä≤
w.r.t. ‚â∫ùë§.

ùëé is an immediate predecessor of a sound asynchronization ùëÉ ‚Ä≤
ùëé,
ùëé up, then the maximal sound asyn-
ùëé is obtained by moving only awaits smaller than s ‚Ä≤‚Ä≤
ùë§

ùë§ in ùëÉ ‚Ä≤

Proof (Proof of Lemma 4). Moving an await up in ùëÉ ‚Ä≤
ùëé can only create data
races between actions that occur after the execution of this await (because the
invocation is suspended earlier). The only possible repairs of these data races
ùë§ down which results in ùëÉ ‚Ä≤
consists in either moving s ‚Ä≤‚Ä≤
ùëé or moving up some other
awaits that occur in methods that (indirectly) call the method in which s ‚Ä≤‚Ä≤
ùë§
occurs. The Ô¨Årst case is not applicable because it gives a program that is not

28

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

ùëé . In the second case, every await s ‚Ä≤

smaller than ùëÉ ‚Ä≤‚Ä≤
a method that (indirectly) calls the method in which s ‚Ä≤‚Ä≤
ùë§ is smaller than s ‚Ä≤‚Ä≤
s ‚Ä≤

ùë§ w.r.t. ‚â∫ùë§.

ùë§ that is moved up occurs in
ùë§ occurs, and therefore,

Before giving the proof of Theorem 1, we note that the total order relation ‚â∫ùë§
between awaits is Ô¨Åxed throughout the recursion of AsySyn and it corresponds to
the order of the awaits in the weakest asynchronization of ùëÉ , i.e., wkAsy[ùëÉ, ùêø, ùêøùëé].
This is because the order between awaits in the same method might change from
one asynchronization to another in Asy[ùëÉ, ùêø, ùêøùëé]. If the control-Ô¨Çow graph of a
method contains branches, it is possible to replace all await statements matching
sùëê that are reachable in the CFG from s with a single await statement sùë§, in
this case sùë§ is ordered before any other await that one of the awaits that sùë§
replaces is ordered before and is ordered after any await that all the awaits
that sùë§ replaces are ordered before. Also, it is possible to add additional awaits
statements in branches, in this case derive a total order between these awaits
and order the awaits before or after any other await that the original await was
ordered before or after, respectively.

Proof (Proof of Theorem 1). Let ùëÉùëé be the weakest asynchronization of ùëÉ , then
the set of all sound asynchronizations of ùëÉ is ùíú = {ùëÉ ‚Ä≤‚Ä≤
ùëé ‚â§
ùëÉ ‚Ä≤
ùëé}, where ùëÉ ‚Ä≤
ùëé is the maximal asynchronization of ùëÉ relative to ùëÉùëé. It is clear
that every asynchronization outputted by AsySyn(ùëÉùëé, s 0

ùëé ] = C[ùëÉ ] and ùëÉ ‚Ä≤‚Ä≤

ùë§) is in the set ùíú.

ùëé : C[ùëÉ ‚Ä≤‚Ä≤

Let ùëÉ 0

ùëé = ùëÉ ‚Ä≤

ùëé = ùëÉ 1

ùëé or ùëÉ 0

ùëé = ùëÉ 1‚Ä≤

ùë§). Since ùëÉ 1

ùë§). Then, let (ùëÉ 1

ùëé . We have that either ùëÉ 0

ùë§ ) ‚äÇ Cover(s 1
ùëé or ùëÉ 0

ùëé be a sound asynchronization of ùëÉ [ùêø] w.r.t. ùêøùëé, i.e., ùëÉ 0

ùë§ ) ‚äÇ Cover(s 2
ùë§, then Lemma 4 implies ùëÉ 1‚Ä≤

ùëé is in AsySyn(ùëÉùëé, sùë§). For the second case: let s 1
ùëé w.r.t. ‚â∫ùë§ that matches the same call as s 1‚Ä≤
ùë§) ‚àà ImPred(ùëÉ ‚Ä≤

ùëé , s 1
ùëé . The Ô¨Åst case implies that ùëÉ 0
ùëé = MaxRel(ùëÉ 1
ùëé ) then either ùëÉ 0
ùëé is in AsySyn(ùëÉùëé, sùë§). For the second case: let s 2
ùëé w.r.t. ‚â∫ùë§ that matches the same call as s 2‚Ä≤
ùëé is an immediate successor of ùëÉ ‚Ä≤

ùëé ‚àà ùíú. We will
ùëé < ùëÉ ‚Ä≤
ùëé. The
ùë§ be
ùë§ in ùëÉ 0
ùëé
ùëé, sùë§). We obtain that
ùëé is in AsySyn(ùëÉùëé, sùë§).
ùëé < ùëÉ 1‚Ä≤
ùëé or ùëÉ 0
ùëé .
ùë§ be
ùë§ in ùëÉ 0
ùëé
ùëé by moving
is obtained by moving only awaits
ùë§ or s 2
ùë§ ‚â∫ùë§ s 1
ùë§. Thus,
ùëé or ùëÉ 0
ùëé = ùëÉ 2
ùëé < ùëÉ 2
ùëé .
ùëé . Thus, Asy-
ùëé = ùëÉ 0

show that AsySyn outputs ùëÉ 0
Ô¨Årst case implies that ùëÉ 0
the maximum element in ùëÉ ‚Ä≤
s.t. Cover(s 1‚Ä≤
either ùëÉ 0
ùëé < ùëÉ 1
For the second case: let ùëÉ 1‚Ä≤
The Ô¨Åst case implies that ùëÉ 0
the maximum element in ùëÉ 1‚Ä≤
s.t. Cover(s 2‚Ä≤
the await s 1
ùë§ = s 1
smaller than s 1
(ùëÉ 2
ùë§). We then obtain that either ùëÉ 0
Then, we repeat the above proof process until we obtain ùëÉ ùëõ
Syn outputs ùëÉ 0
ùëé .
ùë§ and s 2
(ùëÉ 1
ùë§), (ùëÉ 2
ùëé , s 2
MaxRel(ùëÉ 2
ùëé ) is obtained by moving only awaits smaller than s 2
Thus, in ùëÉ 2‚Ä≤
ùëé
MaxRel(ùëÉ 1
ùëÉ 1‚Ä≤‚Ä≤
ùëé
we have that the two programs are distinct since in ùëÉ 2‚Ä≤‚Ä≤
ùëé

ùë§ and
ùëé =
ùë§ w.r.t. ‚â∫ùë§.
ùëé. Then, ùëÉ 1‚Ä≤
ùëé =
and ùëÉ 2‚Ä≤‚Ä≤
s.t.
ùëé
ùëé , s 2
ùë§)),
ùë§ is in the

ùë§ be two distinct await statements in ùëÉ ‚Ä≤
ùë§) ‚àà ImPred(ùëÉ ‚Ä≤

ùë§ is in the same position as in ùëÉ ‚Ä≤
ùëé

ùëé . For any two programs ùëÉ 1‚Ä≤‚Ä≤
ùëé , s 1

ùëé, sùë§). Similar to before then we have that ùëÉ 2‚Ä≤

ùë§ w.r.t. ‚â∫ùë§. Then, we either have s 2

ùëé ) is outputted by AsySyn(ùëÉ 1‚Ä≤

ùëé ) is diÔ¨Äerent than ùëÉ 2‚Ä≤

ùë§) (resp., AsySyn(ùëÉ 2‚Ä≤

ùë§) ‚àà ImPred(ùëÉ 1‚Ä≤

Let s 1
ùëé , s 1

the await s 1

(resp., ùëÉ 2‚Ä≤‚Ä≤

the await s 1

ùë§ ‚â∫ùë§ s 1

ùëé s.t. s 2

ùëé , s 1

ùëé , s 2

ùëé

Automated Synthesis of Asynchronizations

29

same position as in ùëÉ ‚Ä≤
only once.

ùëé. Thus, we get that AsySyn outputs every element of ùíú

C Formalization and Proofs of Section 5

The following lemma proves that for any unsound asyn-
chronization, any trace with a data race contains at least one
data race that involves two actions that are synchronously
reachable. For instance, the program in Fig. 11 has two data
races, one between ùë• = 1 and ùëü4 = ùë• and the other be-
tween ùë¶ = 2 and ùëü5 = ùë¶. However, the statement ùë¶ = 2
is not reachable in the corresponding synchronous program.
It is reachable in this asynchronization because of the data
race between ùë• = 1 and ùëü4 = ùë•, which are both reachable
in the synchronous program. Eliminating the latter data race
by moving the statement await ùëü1 before ùë• = 1, makes ùë¶ = 2
unreachable and the data race between ùë¶ = 2 and ùëü5 = ùë¶ is
also eliminated.

async method Main {

r1 = call m;
x = 1;
await r1;

}
async method m {
r2 = call m1;
r3 = call m1;
await r2;
r4 = x;
if r4 == 1
y = 2;
await r3;

}
async method m1 {

await *;
r5 = y;
return;

}
Fig. 11

Lemma 5. An asynchronization ùëÉùëé[ùêøùëé] is sound iff it does
not admit data races between actions that are synchronously reachable.

Proof (Proof of Lemma 5). Assume by contradiction that ùëÉùëé[ùêøùëé] is sound and
it admits a data race (a1, a2) in a trace ùúè ‚àà Tr(ùëÉùëé[ùêøùëé]) where one of the actions,
say a1, is not synchronously reachable. We assume w.l.o.g that the data race
(a1, a2) is the Ô¨Årst that occurs in ùúè with at least one synchronously unreachable
action. Then, there must exist a read access aùëü that enabled a1, and therefore,
aùëü reads a value that was not read in any synchronous execution. Thus, the read
value must the result of another data race that occurs earlier in the trace ùúè ,
which is a contradiction.

async method Main {

r1 = call m;
r2 = x;
x = r2 + 1;
await r1;

In Fig. 12, we explain how the repairing data races based
on the partial order relating data races allows to avoid su-
perÔ¨Çuous repair steps. For instance, in Fig. 12, the Ô¨Årst data
race to repair involves the read of x from Main and the write
to x in m, because these statements are the Ô¨Årst to execute in
the original sequential program among the other statements
involved in data races. Repairing this data race consists in
moving await r1 before the read of x from Main, which im-
plies that m completes before the read of x. This repair is
deÔ¨Åned from a notion of root cause of a data race, that in
this case, contains the call to m and the read of x from Main. Interestingly, this
repair step removes the write-write data race between the write to x in Main
and the write to x in m as well. If we would have repaired these data races in the
opposite order, we would have moved await t1 Ô¨Årst before the write to x, and
then, before the read of x.

}
async method m {

await *;
x = 2;
return;

}
Fig. 12

30

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

In Fig. 13, we give a non-deterministic program where two
statements of the program can be executed in diÔ¨Äerent orders in
diÔ¨Äerent executions. In particular, the statements r1 = x and r2
= y of the program can be executed in diÔ¨Äerent orders depending
on the number of loop iterations and whether the if branch is
entered during the Ô¨Årst loop iteration.

method Main {
while *
if *

r1 = x;

r2 = y;

}

Fig. 13

For the program in Fig. 14, we have the following order
between data races: (x = input, r2 = x) ‚â∫SO(retVal = x,
x = r2 + 1) because r2 = x is executed before the write x
= r2 + 1 in the original synchronous program (for simplic-
ity we use statements instead of actions). However, the data
races (x = input, r2 = x) and (x = input, r3 = x) are in-
comparable.

async method Main {

r1 = call m;
if *

r2 = x;
x = r2 + 1;

else

r3 = x;
await r1;

}
async method m {

The following lemma identiÔ¨Åes a suÔ¨Écient transformation
for repairing a data race (a1, a2): moving the await sùë§ gener-
ating the action aùë§ just before the statement s generating a.
This is suÔ¨Écient because it ensures that every statement that
follows LCACO(a1, a2)10 in call order will be executed before a
and before any statement which succeeds a in call order, including a2. Note that
moving the await aùë§ anywhere after a will not aÔ¨Äect the concurrency between
a1 and a2.

await *
retVal = x;
x = input;
return;

}
Fig. 14

Lemma 6. Let (a1, a2) be a data race in a trace ùúè of an asynchronization ùëÉùëé,
and aùëê = (ùëñ, call(ùëó)) = LCACO(a1, a2). Then, ùúè contains a unique action aùë§ =
(ùëñ, await(ùëó)) and a unique action a such that:

‚Äì (a, aùë§) ‚àà MO, and a is the latest action in the method order MO such that
(aùëê, a) ‚àà MO and (a, a2) ‚àà CO* (CO* denotes the reflexive closure of CO).
Proof (Proof of Lemma 6). Let ùúå be the execution of the trace ùúè . By deÔ¨Ånition,
ùúå ends with a conÔ¨Åguration where the call stack and the set of pending tasks are
empty. Therefore, ùúå contains an action aùë§ = ( , ùëñ, await(ùëó)) matching ùëéùëê which is
unique by the deÔ¨Ånition of the semantics. Since (aùëê, a1) ‚àà CO and (aùëê, a2) ‚àà CO
then either aùëê and a2 occur in the same method, or there exists a call action a ‚Ä≤ in
the same task as aùëê such that (a ‚Ä≤, a2) ‚àà CO. Then, we deÔ¨Åne a = a2 in the Ô¨Årst
case, and a as the latest action in the same task as aùëê such that (a, a2) ‚àà CO in
the second case. We have that (a, aùë§) ‚àà MO because otherwise, (aùë§, a) ‚àà MO
and (a, a2) ‚àà CO* implies that (a1, a2) ‚àà HB (because (a1, aùë§) ‚àà HB, and MO
and CO are included in HB), and this contradicts a1 and a2 being concurrent.

When the control-Ô¨Çow graph of the method contains branches, the construc-
tion of RDR(ùëÉùëé, sùëê, s) involves (1) replacing all await statements matching sùëê
that are reachable in the CFG from s with a single await statement placed just
before s, and (2) adding additional await statements in branches that ‚ÄúconÔ¨Çict‚Äù
with the branch containing s. This is to ensure the syntactic constraints described
in Section 2. These additional await statements are at maximal distance from
the corresponding call statement because of the maximality requirement.

10 We abuse the terminology and make no distinction between statements and actions.

Automated Synthesis of Asynchronizations

31

async method Main {

async method Main {

For instance, to repair the data race
between r2 = x and x = input in the
program on the left of Fig. 15, the
statement await r1 must be moved be-
fore r2 = x in the if branch, which
implies that another await must be
added on the else branch. The result
is given on the right of Fig. 15.

r1 = call m;
if *

r2 = x;

else

r3 = y;

await r1;

}
async method m {

The following lemma shows that
repairing a minimal data race can-
not introduce smaller data races (w.r.t.
‚â∫SO), which ensures some form of
monotonicity when repairing minimal data races iteratively.

await *
retVal = x;
x = input;
return;

r1 = call m;
if *

await r1;
r2 = x;

else

r3 = y;
await r1;

}
async method m {

await *
retVal = x;
x = input;
return;

}
}
Fig. 15: Examples of asynchronizations.

Lemma 7. Let ùëÉùëé be an asynchronization, (a1, a2) a data race in ùëÉùëé that is
minimal w.r.t. ‚â∫SO, and (sùëê, s) the root cause of (a1, a2). Then, RDR(ùëÉùëé, sùëê, s)
does not admit a data race that is smaller than (a1, a2) w.r.t. ‚â∫SO.

Proof of Lemma 7. The only
modiÔ¨Åcation in the program ùëÉ ‚Ä≤
ùëé =
RDR(ùëÉùëé, sùëê, s) compared to ùëÉùëé is the
movement of the await sùë§ matching the
call sùëê to be before the statement s in a
method ùëö. The concurrency added in ùëÉ ‚Ä≤
ùëé
that was not possible in ùëÉùëé is between ac-
tions (a ‚Ä≤, a ‚Ä≤‚Ä≤) generated by statements s ‚Ä≤
and s ‚Ä≤‚Ä≤, respectively, as shown in Fig. 16.
W.l.o.g., we assume that (a ‚Ä≤, a ‚Ä≤‚Ä≤) ‚àà SO.
The statements s1 and s2 are those gen-
erating a1 and a2, respectively. The state-
ment s ‚Ä≤ is related by CO* to some statement in ùëö that follows s, and s ‚Ä≤‚Ä≤ is
related by CO* to some statement that follows the call to ùëö in the caller of ùëö.
Note that s ‚Ä≤ is ordered by ‚â∫ after s2. Since (a1, a2) ‚àà SO and (a ‚Ä≤, a ‚Ä≤‚Ä≤) ‚àà SO
then s2 ‚â∫ s ‚Ä≤‚Ä≤ and s1 ‚â∫ s ‚Ä≤. Thus, any new data race (a ‚Ä≤, a ‚Ä≤‚Ä≤) in ùëÉ ‚Ä≤
ùëé that was not
(cid:3)
reachable in ùëÉùëé is bigger than (a1, a2).

Fig. 16: An excerpt of an asyn-
chronous program.

Theorem 7. Given an asynchronization ùëÉùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé], MaxRel(ùëÉùëé) re-
turns the optimal asynchronization of ùëÉ relative to ùëÉùëé.

Proof (Proof of Theorem 7). Since the recursive calls RCMinDR Ô¨Ånd all data
races between synchronously reachable actions then the output ùëÉ ‚Ä≤
is sound and therefore it is equivalent to ùëÉ (Lemma 2 and Lemma 5). Now we
need to show that any successor ùëÉ 1
ùëé of ùëÉ ‚Ä≤
ùëé that is also smaller than ùëÉùëé (w.r.t.
‚â§) admits data races. Let sùë§ be the biggest await statement w.r.t. ‚â∫ùë§ whose
position in ùëÉ 1
ùëé (moved down). Since
ùëé ‚â§ ùëÉùëé, then sùë§ was also moved up by the procedure MaxRel with respect
ùëÉ 1
to its position in ùëÉùëé to Ô¨Åx some data race (a1, a2). Let ùëö be the method ùëö that

ùëé is changed with respect to its position in ùëÉ ‚Ä≤

ùëé = MaxRel(ùëÉùëé)

        async method m‚Äô {                              r‚Äô = call m;                                                                                            s‚Äô‚Äô               await r‚Äô;           }        async method m {                            sc: r = call _ ;                  s1                   s: ...                     s2                                                                                     s‚Äô              sw: await r;           }COCO*CO*CO*32

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

ùëé as well. ùëÉ 1

contains sùë§ and sùëê be the matching call. We will now show that (a1, a2) forms a
data race in ùëÉ 1
ùëé has an execution ùúå that reaches both a1 and a2 (since
Ex(ùëÉ 1
ùëé ) includes the synchronous execution where all await * are interpreted as
skip which reaches a1 and a2). Since every other await s ‚Ä≤
ùëé that occurs in a
method ùëö‚Ä≤ (in)directly called by ùëö (including the method associated with the
call sùëê) is in the same position as in ùëÉ ‚Ä≤
ùëé, then the two actions a1 and a2 are not
related by HB and are concurrent. Thus, (a1, a2) forms a data race in ùëÉ 1
ùëé , which
concludes the proof.

ùë§ in ùëÉ 1

The fact that data races are enumerated in the order deÔ¨Åned by ‚â∫SO guaran-
tees a bound on the number of times an await matching the same call is moved
during the execution of MaxRel(ùëÉùëé). In general, this bound is the number of
statements covered by all the awaits matching the call in the input program
ùëÉùëé. Actually, this is a rather coarse bound. A more reÔ¨Åned analysis has to take
into account the number of branches in the CFGs. For programs without con-
ditionals or loops, every await is moved at most once during the execution of
MaxRel(ùëÉùëé). In the presence of branches, a call to an asynchronous method
may match multiple await statements (one for each CFG path starting from
the call), and the data races that these await statements may create may be
incomparable w.r.t. ‚â∫SO. Therefore, for a call statement sùëê, let |sùëê| be the sum
of |Cover(sùë§)| for every await sùë§ matching sùëê in ùëÉùëé.
Lemma 8. For any asynchronization ùëÉùëé ‚àà Asy[ùëÉ, ùêø, ùêøùëé] and call statement sùëê
in ùëÉùëé, the while loop in MaxRel(ùëÉùëé) does at most |sùëê| iterations that result in
moving an await matching sùëê.
Proof (Proof of Lemma 8). We consider Ô¨Årst the case without conditionals or
loops, and we show by contradiction that every await statement sùë§ is moved
at most once during the execution of MaxRel(ùëÉùëé), i.e., there exists at most
one iteration of the while loop which changes the position of sùë§. Suppose that
the contrary holds for an await sùë§. Let (a1, a2), and (a3, a4) be the data races
repaired by the Ô¨Årst and second moves of sùë§, respectively. By Lemma 6, there
exist two actions a and a ‚Ä≤ such that
(aùëê, a) ‚àà MO, (a, a2) ‚àà CO*, (a, aùë§) ‚àà MO and (aùëê, a ‚Ä≤) ‚àà MO, (a ‚Ä≤, a4) ‚àà CO*, (a ‚Ä≤, aùë§) ‚àà MO
where aùë§ = ( , ùëñ, await(ùëó)) and aùëê = ( , ùëñ, call(ùëó)) are the asynchronous call ac-
tion and the matching await action. Let s2 and s4 be the statements generating
the two actions a2 and a4, respectively. Then, we have either s2 ‚â∫ s4 or s2 = s4,
and both cases imply that (a, a ‚Ä≤) ‚àà MO*. Thus, moving the await statement gen-
erating aùë§ before the statement generating a implies that it is also placed before
the statement generating a ‚Ä≤ (that occurs after a in the same method). Thus, the
Ô¨Årst move of the await sùë§ repaired both data races, which is contradiction.

In the presence of conditionals or loops, moving an await up in one branch
may correspond to adding multiple awaits in the other conÔ¨Çicting branches. Also,
one call in the program may correspond to multiple awaits on diÔ¨Äerent branches.
However, every repair of a data race consists in moving one await closer to the
matching call sùëê and before one more statement covered by some await matching
sùëê in the input ùëÉùëé.

Automated Synthesis of Asynchronizations

33

1
2
3
4
5

7
8
9
10

Add before s1:
if ( lastTaskDelayed == ‚ä• && * )
lastTaskDelayed := myTaskId();
DescendantDidAwait := thisHasDoneAwait;
return

Add before s2:

if ( task_sùëê == myTaskId() )

s := s2;

assert (lastTaskDelayed == ‚ä• ||

!DescendantDidAwait);

13
14
15
16

17
18
19
20

22
23
24

26
27
28
29

Replace every statement ‚Äò‚Äòawait r‚Äô‚Äô with:

if( r == lastTaskDelayed ) then

if ( !DescendantDidAwait )
DescendantDidAwait :=
thisHasDoneAwait;

lastTaskDelayed := myTaskId();
return

else

thisHasDoneAwait := true

Add before every statement ‚Äò‚Äòr := call m‚Äô‚Äô:

if ( task_sùëê == myTaskId() ) then

s := this statement;

Add after every statement ‚Äò‚Äòr := call m‚Äô‚Äô:

if ( r == lastTaskDelayed )

sùëê := this statement;
task_sùëê := myTaskId();

Fig. 17: A program instrumentation for computing the root cause of a minimal
data race between the statements s1 and s2 (if any). All variables except for
thisHasDoneAwait are program (global) variables. thisHasDoneAwait is a local
variable. The value ‚ä• represents an initial value of a variable. The variables
sùëê and s store the (program counters of the) statements representing the root
cause. The method myTaskId returns the id of the current task.

D Computing Root Causes of Minimal Data Races

We present a reduction from the problem of computing root causes of mini-
mal data races to reachability (assertion checking) in sequential programs. This
reduction builds on a program instrumentation for checking if there exists a
minimal data race that involves two given statements (s1, s2) that are reachable
in an execution of the original synchronous program, whose correctness relies on
the assumption that another pair of statements cannot produce a smaller data
race. This instrumentation is used in an iterative process where pairs of state-
ments are enumerated according to the colexicographic order induced by ‚â∫. This
speciÔ¨Åc enumeration ensures that the assumption made for the correctness of the
instrumentation is satisÔ¨Åed.

Given an asynchronization ùëÉùëé, the instrumentation described in Fig. 17 rep-
resents a synchronous program where all await statements are replaced with
synchronous code (lines 14‚Äì20). This instrumentation simulates asynchronous
executions of ùëÉùëé where methods may be only partially executed, modeling await
interruptions. It reaches an error state (see the assert at line 10) when an action
generated by s1 is concurrent with an action generated by s2, which represents
a data race, provided that s1 and s2 access a common program variable (these
statements are assumed to be given as input). Also, the values of sùëê and s when
reaching the assertion violation represent the root-cause of this data race.

The instrumentation simulates an execution of ùëÉùëé to search for a data race

as follows (we discuss the identiÔ¨Åcation of the root-cause afterwards):

‚Äì It executes under the synchronous semantics until an instance of s1 is non-
deterministically chosen as a candidate for the Ô¨Årst action in the data race
(s1 can execute multiple times if it is included in a loop for instance). The

34

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

current invocation is interrupted when it is about to execute this instance
of s1 and its task id ùë°0 is stored into lastTaskDelayed (see lines 2‚Äì5).

‚Äì Every invocation that transitively called ùë°0 is interrupted when an await for
an invocation in this call chain (whose task id is stored into lastTaskDelayed)
would have been executed in the asynchronization ùëÉùëé (see line 18).

‚Äì Every other method invocation is executed until completion as in the syn-

chronous semantics.

‚Äì When reaching s2, if s1 has already been executed (lastTaskDelayed is not
‚ä•) and at least one invocation has only partially been executed, which is
recorded in the boolean Ô¨Çag DescendantDidAwait and which means that
s1 is concurrent with s2, then the instrumentation stops with an assertion
violation.

A subtle point is that the instrumentation may execute code that follows an
await ùëü even if the task ùëü has been executed only partially, which would not
happen in an execution of the original ùëÉùëé. Here, we rely on the assumption that
there exist no data race between that code and the rest of the task ùëü. Such
data races would necessarily involve two statements which are before s2 w.r.t.
‚â∫. Therefore, the instrumentation is correct only if it is applied by enumerating
pairs of statements (s1, s2) w.r.t. the colexicographic order induced by ‚â∫.

Next, we describe the computation of the root-cause, i.e., the updates on the
variables sùëê and s. By deÔ¨Ånition, the statement sùëê in the root-cause should be a
call that makes an invocation that is in the call stack when s1 is reached. This
can be checked using the variable lastTaskDelayed that stores the id of the
last such invocation popped from the call stack (see the test at line 27). The
statement s in the root-cause can be any call statement that has been executed
in the same task as sùëê (see the test at line 23), or s2 itself (see line 9).

Let [[ùëÉùëé, s1, s2]] denote the instrumentation in Fig. 17. We say that the values
of sùëê and s when reaching the assertion violation are the root cause computed
by this instrumentation. The following theorem states its correctness.

Theorem 8. If [[ùëÉùëé, s1, s2]] reaches an assertion violation, then it computes the
root cause of a minimal data race, or there exists (s3, s4) such that [[ùëÉùëé, s3, s4]]
reaches an assertion violation and (s3, s4) is before (s1, s2) in colexicographic
order w.r.t. ‚â∫.

Based on Theorem 8, we deÔ¨Åne an implementation of the procedure RCMinDR(ùëÉùëé)

used in computing maximal asynchronizations (Algorithm 2) as follows:

‚Äì For all pairs of read or write statements (s1, s2) in colexicographic order w.r.t.
‚â∫ that are reachable in an execution of the original synchronous program
ùëÉ .

‚àô If [[ùëÉùëé, s1, s2]] reaches an assertion violation, then
‚àó return the root cause computed by [[ùëÉùëé, s1, s2]]

‚Äì return ‚ä•

Checking whether read or write statements are reachable can be determined
using a linear number of reachability queries in the synchronous program ùëÉ . Also,

Automated Synthesis of Asynchronizations

35

the order ‚â∫ between read or write statements can be computed using a quadratic
number of reachability queries in the synchronous program ùëÉ . Therefore, s ‚â∫ s ‚Ä≤
iÔ¨Ä an instrumentation of ùëÉ that sets a Ô¨Çag when executing s and asserts that this
Ô¨Çag is not set when executing s ‚Ä≤ reaches an assertion violation. The following
theorem states the correctness of the procedure above.

Theorem 9. RCMinDR(ùëÉùëé) returns the root cause of a minimal data race of
ùëÉùëé w.r.t. ‚â∫SO, or ‚ä• if ùëÉ ‚Ä≤

ùëé is data race free.

E Formalization and Proofs of Section 6

Theorem 10. Checking whether there exists a sound asynchronization different
from the strong asynchronization is PSPACE-complete.

Proof (Proof of Theorem 10). (1) deÔ¨Åne a new method ùëö that writes to a new
program variable ùë•, and insert a call to ùëö followed by a write to ùë• at location ‚Ñì,
and (2) insert a write to ùë• after every call statement that calls a method in {ùëö‚Ä≤}*,
where ùëö‚Ä≤ is the method containing ‚Ñì. Let ùëöùëé be an asynchronous version of ùëö
obtained by inserting an await * at the beginning. Then, ‚Ñì is reachable in ùëÉ iÔ¨Ä
the only sound asynchronization of ùëÉ ‚Ä≤ w.r.t. {ùëöùëé} is the strong asynchronization.

F Formalization and Proofs of Section 7

The MaxRel# procedure repairs data races in an order which is ‚â∫SO with
some exceptions that do not aÔ¨Äect optimality, i.e., the number of times an await
matching the same call can be moved. For instance, if a method ùëö calls two other
methods ùëö1 and ùëö2 in this order, the procedure above may handle ùëö2 before ùëö1,
i.e., repair data races between actions that originate from ùëö2 before data races
that originate from ùëö1, although the former are bigger than the latter in ‚â∫SO.
This does not aÔ¨Äect optimality because those repairs are ‚Äúindependent‚Äù, i.e., any
repair in ùëö2 cannot inÔ¨Çuence a repair in ùëö1, and vice-versa. The crucial point
is that this procedure repairs data races between actions that originate from a
method ùëö before data races that involve actions in methods preceding ùëö in the
call graph, which are bigger in ‚â∫SO than the former.

Note that MaxRel# procedure which is based on the bottom-up inter-
procedural data-Ô¨Çow analysis compromises precision to reduce the complexity of
the problem from undecidable in general or PSPACE-complete with Ô¨Ånite data
to polynomial time. However, because of this imprecision, certain await state-
ments may be moved closer to the matching call unnecessarily. For instance, in
Fig. 11, the precise algorithm (using the procedure MaxRel in Algorithm 2)
will only repair the data race on ùë• because doing so, the potential data race on
ùë¶ will become unreachable. On the other hand, the polynomial-time algorithm
(using the MaxRel# procedure) will also repair the data race on ùë¶, moving
another await closer to the matching call, since it cannot reason about data (one
statement of this data race is only reachable if the variable ùëü4 is 1).

36

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 void Main() {
2 F();

1 async Task MainAsync() {
2 Task t1 = F();

4 x = 2;
5 await t1;
6 }

1 void Main() {
2 Thread thr1 = new Thread(F);
3 thr1.Start();
4 x = 2;
5 thr1.Join();
6 }

8 async Task F() {
9 Task t2 = IOAsync();

8 void F() {
9 Thread thr2 = new Thread(IO);

11 x = 1;
12 await t2;
13 }

10 thr2.Start();
11 x = 1;
12 thr2.Join();
13 }

4 x = 2;

6 }

8 void F() {
9 IO();

11 x = 1;

13 }

Fig. 18: A synchronous C# program, an asynchronization, and a multi-threaded
refactoring.

G Multi-threaded Refactorings

We discuss an extension of our framework to multi-threaded refactorings that
rewrite a sequential program into a multi-threaded program where every method
invocation is executed on a diÔ¨Äerent thread. A caller can wait for a callee to
complete using a join primitive. A start primitive for spawning a new thread
is the counterpart of an asynchronous call while join is the counterpart of await.
For instance, Fig. 18 lists a sequential program, a possible asynchonization, and
a multi-thread refactoring (both refactorings place the awaits/joins as far away
as possible from the calls).

An important diÔ¨Äerence between start/join and async/await is the happens-
before order relation. For instance, the asynchronization on the center of Fig. 18
assigns 1 to x (line 11) before it assigns 2 to x (line 4), as in the original sequential
program. However, the multi-thread program on the right of Fig. 18 may execute
these two assignments in any order, and admits a behavior that is not possible
in the sequential program (assigning 2 before assigning 1). Repairing this data-
race consists in moving the join at line 5 to occur before assigning 2 to x at
line 4. In general, the happens-before order is weaker compared to an analogous
asynchronization, where awaits are placed as the joins, which implies that any
multi-threaded refactoring can be rewritten to an asynchronization. The vice-
versa may not be possible as shown in this example.

Despite this diÔ¨Äerence, it can still be proved that there exists a unique multi-
threaded refactoring that is sound, i.e., does not admit data races, and max-
imal, i.e., maximizes the distance between start and join, a result similar to
Lemma 3. Assuming by contradiction the existence of two incomparable maxi-
mal and sound refactorings, one can show that moving a join in one refactoring
further away from the matching call as in the other refactoring does not in-
troduce data races (contradicting optimality). To compute maximal and sound
multi-threaded refactorings, one can apply the same iterative process of repairing
data-races (the happens-before reÔ¨Çects multi-threading instead of async/await),
prioritizing data races involving statements that would execute Ô¨Årst in the se-
quential program. The repairing of a data-race is similar and consists in moving
a join up.

Automated Synthesis of Asynchronizations

37

In contrast to async/await, moving a join up does not introduce new data
races (since no new parallelism is introduced). This implies that all the predeces-
sors of a sound multi-threaded refactoring are also sound, i.e., the set of sound
multi-threaded refactorings is downward closed.

