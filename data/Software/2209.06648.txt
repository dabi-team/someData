2
2
0
2

p
e
S
4
1

]
L
P
.
s
c
[

1
v
8
4
6
6
0
.
9
0
2
2
:
v
i
X
r
a

Automated Synthesis of Asynchronizationsâ‹†

Sidi Mohamed Beillahi1, Ahmed Bouajjani2, Constantin Enea3, and Shuvendu
Lahiri4

1 University of Toronto, Canada
sm.beillahi@utoronto.ca
2 UniversitÂ´e Paris CitÂ´e, IRIF, CNRS, Paris, France
abou@irif.fr
3 LIX, Ecole Polytechnique, CNRS and Institut Polytechnique de Paris, France
cenea@irif.fr
4 Microsoft Research Lab - Redmond
shuvendu@microsoft.com

Abstract. Asynchronous programming is widely adopted for building
responsive and efficient software, and modern languages such as C# pro-
vide async/await primitives to simplify the use of asynchrony. In this
paper, we propose an approach for refactoring a sequential program into
an asynchronous program that uses async/await, called asynchroniza-
tion. The refactoring process is parametrized by a set of methods to
replace with asynchronous versions, and it is constrained to avoid intro-
ducing data races. We investigate the delay complexity of enumerating
all data race free asynchronizations, which quantifies the delay between
outputting two consecutive solutions. We show that this is polynomial
time modulo an oracle for solving reachability in sequential programs.
We also describe a pragmatic approach based on an interprocedural data-
flow analysis with polynomial-time delay complexity. The latter approach
has been implemented and evaluated on a number of non-trivial C# pro-
grams extracted from open-source repositories.

1

Introduction

Asynchronous programming is widely adopted for building responsive and eï¬ƒ-
cient software. As an alternative to explicitly registering callbacks with asyn-
chronous calls, C# 5.0 [3] introduced the async/await primitives. These prim-
itives allow the programmer to write code in a familiar sequential style without
explicit callbacks. An asynchronous procedure, marked with async, returns a
task object that the caller uses to â€œawaitâ€ it. Awaiting may suspend the exe-
cution of the caller, but does not block the thread it is running on. The code
after await is the continuation called back when the callee result is ready. This
paradigm has become popular across many languages, C++, JavaScript, Python.
The async/await primitives introduce concurrency which is notoriously com-
plex. The code in between a call and a matching await (referring to the same
task) may execute before some part of the awaited task or after the awaited

â‹† This work is supported in part by the European Research Council (ERC) under the
Horizon 2020 research and innovation programme (grant agreement No 678177).

 
 
 
 
 
 
2

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 void Main(string f) {
2 x = 0;
3 int val = RdFile(f);
4 y = 1;

6 int r = x;
7 Debug.Assert(r == val); }

9 int RdFile(string f) {

10 var rd=new StreamReader(f);
11 string s = rd.ReadToEnd();
12 int r1 = x;

14 x = r1 + s.Length;
15 return s.Length;

}

1 async Task Main(string f) {
2 x = 0;
3 Task<int> t1 = RdFile(f);
4 y = 1;
5 int val = await t1;
6 int r = x;
7 Debug.Assert(r == val); }

9 async Task<int> RdFile(string f) {

10 var rd = new StreamReader(f);
11 Task<string> t=rd.ReadToEndAsync();
12 int r1 = x;
13 string s = await t;
14 x = r1 + s.Length;
15 return s.Length;

}

Fig. 1: Synchronous and asynchronous C# programs (x, y are static variables).

task ï¬nished. For instance, on the middle of Fig. 1, the assignment y=1 at line 4
can execute before or after RdFile ï¬nishes. The await for ReadToEndAsync in
RdFile (line 13) may suspend RdFileâ€™s execution because ReadToEndAsync did
not ï¬nish, and pass the control to Main which executes y=1. If ReadToEndAsync
ï¬nishes before this await executes, then the latter has no eï¬€ect and y=1 gets
executed after RdFile ï¬nishes. The resemblance with sequential code can be es-
pecially deceitful since this non-determinism is opaque. It is common that awaits
are placed immediately after the corresponding call which limits the beneï¬ts that
can be obtained from executing steps in the caller and callee concurrently [25].
In this paper, we address the problem of writing eï¬ƒcient asynchronous code
that uses async/await. We propose a procedure for automated synthesis of
asynchronous programs equivalent to a given synchronous (sequential) program
ğ‘ƒ . This can be seen as a way of refactoring synchronous code to asynchronous
code. Solving this problem in its full generality would require checking equiv-
alence between arbitrary programs, which is known to be hard. Therefore, we
consider a restricted space of asynchronous program candidates deï¬ned by sub-
stituting synchronous methods in ğ‘ƒ with asynchronous versions (assumed to
be behaviorally equivalent). The substituted methods are assumed to be leaves
of the call-tree (they do not call any method in ğ‘ƒ ). Such programs are called
asynchronizations of ğ‘ƒ . A practical instantiation is replacing IO synchronous
calls for reading/writing ï¬les or managing http connections with asynchronous
versions.

For instance, the sequential C# program on the left of Fig. 1 contains a Main
that invokes a method RdFile that returns the length of the text in a ï¬le. The
ï¬le name input to RdFile is an input to Main. The program uses a variable x to
aggregate the lengths of all ï¬les accessed by RdFile; this would be more useful
when Main calls RdFile multiple times which we omit for simplicity. Note that
this program passes the assertion at line 7. The time consuming method Read-
ToEnd for reading a ï¬le is an obvious choice for being replaced with an equivalent
asynchronous version whose name is suï¬ƒxed with Async. Performing such tasks
asynchronously can lead to signiï¬cant performance boosts. The program on the
middle of Fig. 1 is an example of an asynchronization deï¬ned by this substitu-
tion. The syntax of async/await imposes that every method that transitively

  14  15                     }   RdFile(string f) {   10  11  12                    13Main() {  2   3   4  5           6     7        }Automated Synthesis of Asynchronizations

3

calls one of the substituted methods, i.e., Main and RdFile, must also be de-
clared as asynchronous. Then, every asynchronous call must be followed by an
await that speciï¬es the control location where that task should have completed.
For instance, the await for ReadToEndAsync is placed at line 13 since the next
instruction (at line 14) uses the computed value. Therefore, synthesizing such
refactoring reduces to ï¬nding a correct placement of awaits (that implies equiv-
alence) for every call of a method that transitively calls a substituted method
(we do not consider â€œdeeperâ€ refactoring like rewriting conditionals or loops).

We consider an equivalence relation between a synchronous program and an
asynchronization that corresponds to absence of data races in the asynchroniza-
tion. Data race free asynchronizations are called sound. Relying on absence of
data races avoids reasoning about equality of sets of reachable states which is
harder in general, and an established compromise in reasoning about concur-
rency. For instance, the asynchronization in Fig. 1 is sound because the call to
RdFile accessing x ï¬nishes before the read of x in Main (line 6). Therefore,
accesses to x are performed in the same order as in the synchronous program.

The asynchronization on the right of Fig. 1 is not the only sound (data-race
free) asynchronization of the program on the left. The await at line 13 can be
moved one statement up (before the read of x) and the resulting program re-
mains equivalent to the sequential one. In this paper, we investigate the problem
of enumerating all sound asynchronizations of a sequential program ğ‘ƒ w.r.t. sub-
stituting a set of methods with asynchronous versions. This makes it possible to
deal separately with the problem of choosing the best asynchronization in terms
of performance based on some metric (e.g., performance tests).

Identifying the most eï¬ƒcient asynchronization is diï¬ƒcult and can not be done
syntactically. It is tempting to consider that increasing the distance between calls
and matching awaits so that more of the caller code is executed while waiting for
an asynchronous task to ï¬nish increases performance. However, this is not true
in general. We use the programs in Fig. 2 to show that the best await placement
w.r.t. performance depends on execution times of code blocks in between calls
and awaits in a non-trivial manner. Note that estimating these execution times,
especially for IO operations like http connections, can not be done statically.

The programs in Fig. 2 use Thread.Sleep(n) to abstract sequential code
executing in ğ‘› milliseconds and Task.Delay(n) to abstract an asynchronous call
executing in ğ‘› milliseconds on a diï¬€erent thread. The functions named Foo diï¬€er
only in the position of await t. We show that modifying this position worsens
execution time in each case. For the left program, best performance corresponds
to maximal distance between await t in Foo and the corresponding call. This
allows the IO call to execute in parallel with the caller, as depicted on the bottom-
left of Fig. 2. The executions corresponding to the other two positions of await
t are given just above. For the middle program, placing await t in between the
two code blocks in Foo optimizes performance (note the extra IO call in Main):
the IO call in Foo executes in parallel with the ï¬rst code block in Foo and the
IO call in Main executes in parallel with the second one. This is depicted on the
bottom-middle of Fig. 2. The execution above shows that placing await t as on

4

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 async Task Main() {
2 var t1 = Foo();

4 await t1;
5

}

1 async Task Main() {
2 var t1 = Foo();
3 var t2 = IO();
4 await t1;
5 await t2;

}

7 async Task Foo() {
8 var t = IO();

7 async Task Foo() {
8 var t = IO();

10 Thread.Sleep(200);

12 Thread.Sleep(200);
13 await t;

}

10 Thread.Sleep(200);
11 await t;
12 Thread.Sleep(200);
13
}

1 async Task Main() {
2 var t1 = Foo();
3 var t2 = IO();
4 await t1;
5 await t2;

}

7 async Task Foo() {
8 var t = IO();
9 await t;

10 Thread.Sleep(200);

12 Thread.Sleep(200);
13

}

15 async Task IO() {
16 var t0 = Task.Delay(300);

15 async Task IO() {
16 var t0 = Task.Delay(300);

18 await t0;

}

18 await t0;

}

15 async Task IO() {
16 var t0 = Task.Delay(300);
17 Thread.Sleep(150);
18 await t0;

}

Fig. 2: Asynchronous C# programs and executions. On the bottom, time dura-
tions of executing code blocks from the same method are aligned horizontally,
and time goes from left to right. Vertical single-line arrows represent method call
steps, dashed arrows represent awaits passing control to the caller, and double-
line arrows represent a call return. Total execution time is marked time=....

the left (after the two code blocks) leads to worse execution time (placing await
t immediately after the call is also worse). Finally, for the right program, placing
await t immediately after the call is best (note that IO executes another code
block before await). The IO call in Main executes in parallel with Foo as shown
on the bottom-right of Fig. 2. The execution above shows the case where await
t is placed in the middle (the await has no eï¬€ect because IO already ï¬nished,
and Foo continues to execute). This leads to worse execution time (placing await
t after the two code blocks is also worse). These diï¬€erences in execution times
have been conï¬rmed by running the programs on a real machine.

As demonstrated by the examples in Fig. 2, the performance of an asynchro-
nization depends on the execution environment, e.g., the overhead of IO opera-
tions like http connections and disk access (in Fig. 2, we use Thread.Sleep(n)
or Task.Delay(n) to model such overheads). Since modeling the behavior of
an execution environment w.r.t. performance is diï¬ƒcult in general, selecting the
most performant asynchronization using static reasoning is also diï¬ƒcult. As a
way of sidestepping this diï¬ƒculty, we focus on enumerating all sound asynchro-

300 msTask1IO1FooMainIO2Task2200 ms300 ms300 ms300 msawaitTask1IO1FooMainIO2Task2200 ms300 ms300 msTask1IO1FooMainTask1IO1FooMain200 ms300 ms300 msTask1IO1FooMainIO2Task2300 msTask1IO1FooMainIO2Task2time=850 ms200 mstime = 700 mstime = 500 mstime = 700 mstime = 400 ms300 msTask1IO1FooMain200 mstime = 500 mstime = 700 msawait200 ms200 msawaitawait300 ms200 ms200 ms200 mscall returnawait passes controlcall200 ms200 ms150 ms150 msawait150 ms150 ms200 msAutomated Synthesis of Asynchronizations

5

nizations that allows to evaluate performance separately in a dynamic manner
using performance tests for instance (for each sound asynchronization).

In the worst-case, the number of (sound) asynchronizations is exponential
in the number of method calls in the program. Therefore, we focus on the de-
lay complexity of the problem of enumerating sound asynchronizations, i.e., the
complexity of the delay between outputting two consecutive (distinct) solutions,
and show that this is polynomial time modulo an oracle for solving reachabil-
ity (assertion checking) in sequential programs. Note that a trivial enumeration
of all asynchronizations and checking equivalence for each one of them has an
exponential delay complexity modulo an oracle for checking equivalence.

As an intermediate step, we consider the problem of computing maximal
sound asynchronizations that maximize the distance between every call and its
matching await. We show that rather surprisingly, there exists a unique max-
imal sound asynchronization. This is not trivial since asynchronizations can be
incomparable w.r.t. distances between calls and awaits (i.e., better for one await
and worse for another, and vice-versa). This holds even if maximality is relative
to a given asynchronization ğ‘ƒğ‘ imposing an upper bound on the distance be-
tween awaits and calls. In principle, avoiding data races could reduce to a choice
between moving one await or another closer to the matching call. We show that
this is not necessary because the maximal asynchronization is required to be
equivalent to a sequential program, which executes statements in a ï¬xed order.
As a more pragmatic approach, we deï¬ne a procedure for computing sound
asynchronizations which relies on a bottom-up interprocedural data-ï¬‚ow analy-
sis. The placement of awaits is computed by traversing the call graph bottom up
and using a data-ï¬‚ow analysis that computes read or write accesses made in the
callees. We show that this procedure computes maximal sound asynchroniza-
tions of abstracted programs where every Boolean condition is replaced with
non-deterministic choice. These asynchronizations are sound for the concrete
programs as well. This procedure enables a polynomial-time delay enumeration
of sound asynchronizations of abstracted programs.

We implemented the asynchronization enumeration based on data-ï¬‚ow anal-
ysis in a prototype tool for C# programs. We evaluated this implementation
on a number of non-trivial programs extracted from open source repositories to
show that our techniques have the potential to become the basis of refactoring
tools that allow programmers to improve their usage of async/await primitives.

In summary, this paper makes the following contributions:

â€“ Deï¬ne the problem of data race-free (sound) asynchronization synthesis for
refactoring sequential code to equivalent asynchronous code (Section 3).
â€“ Show that the problem of computing a sound asynchronization that maxi-
mizes the distance between calls and awaits has a unique solution (Section 4).

â€“ The delay complexity of sound asynchronization synthesis (Sections 5â€“6).
â€“ A pragmatic algorithm for computing sound asynchronizations based on a

data-ï¬‚ow analysis (Section 7).

â€“ A prototype implementation of this algorithm and an evaluation of this

prototype on a benchmark of non-trivial C# programs (Section 8).

Additional formalization and proofs are included in the appendix.

6

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

âŸ¨progâŸ©
âŸ¨md âŸ©
âŸ¨instâŸ©

::= program âŸ¨md âŸ©
::= method âŸ¨mâŸ© { âŸ¨instâŸ© } | async method âŸ¨mâŸ© { âŸ¨instâŸ© } | âŸ¨md âŸ© âŸ¨md âŸ©
::= âŸ¨x âŸ© := âŸ¨leâŸ© | âŸ¨r âŸ© := âŸ¨x âŸ© | âŸ¨r âŸ© := call âŸ¨mâŸ© | return | await âŸ¨r âŸ©
| await * | if âŸ¨leâŸ© {âŸ¨instâŸ©} else {âŸ¨instâŸ©} | while âŸ¨leâŸ© {âŸ¨instâŸ©} |
âŸ¨instâŸ© ; âŸ¨instâŸ©

Fig. 3: Syntax. âŸ¨ğ‘šâŸ©, âŸ¨ğ‘¥âŸ©, and âŸ¨ğ‘ŸâŸ© represent method names, program and local
variables, resp. âŸ¨ğ‘™ğ‘’âŸ© is an expression over local variables, or * which is non-
deterministic choice.

2 Asynchronous Programs

We consider a simple programming language to formalize our approach, shown
in Fig. 3. A program is a set of methods, including a distinguished main, which
are classiï¬ed as synchronous or asynchronous. Synchronous methods run contin-
uously until completion when they are invoked. Asynchronous methods, marked
using the keyword async, can run only partially and be interrupted when exe-
cuting an await. Only asynchronous methods can use await, and all methods
using await must be deï¬ned as asynchronous. We assume that methods are not
(mutually) recursive. A program is called synchronous if it is a set of synchronous
methods.

A method is deï¬ned by a name from a set M and a list of statements over a set
PV of program variables, which can be accessed from diï¬€erent methods (ranged
over using ğ‘¥, ğ‘¦, ğ‘§,. . .), and a set LV of method local variables (ranged over using
ğ‘Ÿ, ğ‘Ÿ1, ğ‘Ÿ2,. . .). Input/return parameters are modeled using program variables.
Each method call returns a unique task identifier from a set T, used to record
control dependencies imposed by awaits (for uniformity, synchronous methods
return a task identiï¬er as well). Our language includes assignments, awaits,
returns, loops, and conditionals. Assignments to a local variable ğ‘Ÿ := ğ‘¥, where
ğ‘¥ is a program variable, are called reads of ğ‘¥, and assignments to a program
variable ğ‘¥ := ğ‘™ğ‘’ (ğ‘™ğ‘’ is an expression over local variables) are called writes to ğ‘¥.
A base method is a method whose body does not contain method calls.
Asynchronous methods. Asynchronous methods can use awaits to wait for
the completion of a task (invocation) while the control is passed to their caller.
The parameter ğ‘Ÿ of the await speciï¬es the id of the awaited task. As a sound
abstraction of awaiting the completion of an IO operation (reading or writing a
ï¬le, an http request, etc.), which we do not model explicitly, we use a variation
await *. This has a non-deterministic eï¬€ect of either continuing to the next
statement in the same method (as if the IO operation already completed), or
passing the control to the caller (as if the IO operation is still pending).

Fig. 4 lists our modeling of the IO method Read-
ToEndAsync used in Fig. 1. We use program vari-
ables to represent system resources such as the ï¬le
system. The await for the completion of accesses
to such resources is modeled by await *. This en-
ables capturing racing accesses to system resources
in asynchronous executions. Parameters or return

async method ReadToEndAsync() {

await *;
ind = Stream.index;
len = Stream.content.Length;
if (ind >= len)

retVal =

""; return

Stream.index = len;
retVal = Stream.content(ind,len);
return
Fig. 4: An IO method.

}

Automated Synthesis of Asynchronizations

7

values are modeled using program variables. ReadToEndAsync is modeled us-
ing reads/writes of the index/content of the input stream, and await * models
the await for their completion.

We assume that the body of every asynchronous method ğ‘š satisï¬es several
well-formedness syntactic constraints, deï¬ned on its control-ï¬‚ow graph (CFG).
We recall that each node of the CFG represents a basic block of code (a maximal-
length sequence of branch-free code), and nodes are connected by directed edges
which represent a possible transfer of control between blocks. Thus,
1. every call ğ‘Ÿ := call ğ‘šâ€² uses a distinct variable ğ‘Ÿ (to store task identiï¬ers),
2. every CFG block containing an await ğ‘Ÿ is dominated by the CFG block
containing the call ğ‘Ÿ := call . . . (i.e., every CFG path from the entry to the
await has to pass through the call),

3. every CFG path starting from a block containing a call ğ‘Ÿ := call . . . to the

exit has to pass through an await ğ‘Ÿ statement.

The ï¬rst condition simpliï¬es the technical exposition, while the last two ensure
that ğ‘Ÿ stores a valid task identiï¬er when executing an await ğ‘Ÿ, and that every
asynchronous invocation is awaited before the caller ï¬nishes. Languages like
C# or Javascript do not enforce the latter constraint, but it is considered bad
practice due to possible exceptions that may arise in the invoked task and are
not caught. We forbid passing task identiï¬ers as method parameters (which is
possible in C#). A statement await ğ‘Ÿ is said to match a statement ğ‘Ÿ := call ğ‘šâ€².

}

}

if *

while *

await r;

await r;

await r;

r = call m1;

async method m {

râ€™ = call m1;
await râ€™;

async method m {
r = call m1;

async method m {
r = call m1;
while *

}
Fig. 5: Examples of programs

In Fig. 5, we give three
examples of programs to ex-
plain in more details
the
well-formedness syntactic con-
straints. The program on the
left of Fig. 5 does not satisfy
the second condition since await r can be reached without entering the loop.
The program in the center of Fig. 5 does not satisfy the third condition since
we can reach the end of the method without entering the if branch and thus,
without executing await r. The program on the right of Fig. 5 satisï¬es both
conditions.
Semantics. A program conï¬guration is a tuple (g, stack, pend, cmpl, c-by, w-for)
where g is composed of the valuation of the program variables excluding the
program counter, stack is the call stack, pend is the set of asynchronous tasks,
e.g., continuations predicated on the completion of some method call, cmpl is
the set of completed tasks, c-by represents the relation between a method call
and its caller, and w-for represents the control dependencies imposed by await
statements. The activation frames in the call stack and the asynchronous tasks
are represented using triples (ğ‘–, ğ‘š, â„“) where ğ‘– âˆˆ T is a task identiï¬er, ğ‘š âˆˆ M
is a method name, and â„“ is a valuation of local variables, including as usual
a dedicated program counter. The set of completed tasks is represented as a
function cmpl : T â†’ {âŠ¤, âŠ¥} such that cmpl(ğ‘–) = âŠ¤ when ğ‘– is completed and
cmpl(ğ‘–) =âŠ¥, otherwise. We deï¬ne c-by and w-for as partial functions T â‡€ T with
the meaning that c-by(ğ‘–) = ğ‘—, resp., w-for(ğ‘–) = ğ‘—, iï¬€ ğ‘– is called by ğ‘—, resp., ğ‘– is

8

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

waiting for ğ‘—. We set w-for(ğ‘–) = * if the task ğ‘– was interrupted because of an
await * statement.

The semantics of a program ğ‘ƒ is deï¬ned as a labeled transition system (LTS)
[ğ‘ƒ ] = (C, Act, ps0, â†’) where C is the set of program conï¬gurations, Act is a
set of transition labels called actions, ps0 is the initial conï¬guration, and â†’âŠ†
C Ã— Act Ã— C is the transition relation. Each program statement is interpreted
as a transition in [ğ‘ƒ ]. The set of actions is deï¬ned by (Aid is a set of action
identiï¬ers):
Act ={(aid , ğ‘–, ev ) : aid âˆˆ Aid, ğ‘– âˆˆ T, ev âˆˆ {rd(ğ‘¥), wr(ğ‘¥), call(ğ‘—), await(ğ‘˜), return,

cont : ğ‘— âˆˆ T, ğ‘˜ âˆˆ T âˆª {*}, ğ‘¥ âˆˆ PV}}

The transition relation â†’ is deï¬ned in Fig. 6. Transition labels are written

on top of â†’.

Transitions labeled by (aid , ğ‘–, rd(ğ‘¥)) and (aid , ğ‘–, wr(ğ‘¥)) represent a read and
a write accesses to the program variable ğ‘¥, respectively, executed by the task
(method call) with identiï¬er ğ‘–. A transition labeled by (aid , ğ‘–, call(ğ‘—)) corresponds
to the fact that task ğ‘– executes a method call that results in creating a task ğ‘—.
Task ğ‘— is added on the top of the stack of currently executing tasks, declared
pending (setting cmpl(ğ‘—) to âŠ¥), and c-by is updated to track its caller (c-by(ğ‘—) =
ğ‘–). A transition (aid , ğ‘–, return) represents the return from task ğ‘–. Task ğ‘– is removed
from the stack of currently executing tasks, and cmpl(ğ‘–) is set to âŠ¤ to record the
fact that task ğ‘– is ï¬nished.

A transition (aid , ğ‘–, await(ğ‘—)) relates to task ğ‘– waiting asynchronously for
task ğ‘—. Its eï¬€ect depends on whether task ğ‘— is already completed. If this is
the case (i.e., cmpl[ğ‘—] = âŠ¤), task ğ‘– continues and executes the next statement.
Otherwise, task ğ‘– executing the await is removed from the stack and added to
the set of pending tasks, and w-for is updated to track the waiting-for rela-
tionship (w-for(ğ‘–) = ğ‘—). Similarly, a transition (aid , ğ‘–, await(*)) corresponds to
task ğ‘– waiting asynchronously for the completion of an unspeciï¬ed task. Non-
deterministically, task ğ‘– continues to the next statement, or task ğ‘– is interrupted
and transferred to the set of pending tasks (w-for(ğ‘–) is set to *).

A transition (aid , ğ‘–, cont) represents the scheduling of the continuation of
task ğ‘–. There are two cases depending on whether ğ‘– waited for the completion
of another task ğ‘— modeled explicitly in the language (i.e., w-for(ğ‘–) = ğ‘—), or an
unspeciï¬ed task (i.e., w-for(ğ‘–) = *). In the ï¬rst case, the transition is enabled
only when the call stack is empty and ğ‘— is completed. In the second case, the
transition is always enabled. The latter models the fact that methods implement-
ing IO operations (waiting for unspeciï¬ed tasks in our language) are executed
in background threads and can interleave with the main thread (that executes
the Main method). Although this may seem restricted because we do not allow
arbitrary interleavings between IO methods and Main, this is actually sound
when focusing on the existence of data races as in our approach. As shown later
in Table 1, any two instructions that follow an await * are not happens-before
related and form a race.

By the deï¬nition of â†’, every action a âˆˆ Act âˆ– {( ,

, cont)} corresponds to

executing some statement in the program, which is denoted by S(ğ‘).

Automated Synthesis of Asynchronizations

9

r := x âˆˆ inst(â„“(pc)) aid âˆˆ Aid fresh

â„“â€² = â„“[ğ‘Ÿ â†¦â†’ g(ğ‘¥), pc â†¦â†’ next(â„“(pc))]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack,

,

,

,

(aid, ğ‘–, rd(ğ‘¥))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘–, ğ‘š, â„“â€²) âˆ˜ stack,

)

,

,

,

)

x := le âˆˆ inst(â„“(pc)) aid âˆˆ Aid fresh

â„“â€² = â„“[pc â†¦â†’ next(â„“(pc))]

gâ€² = g[ğ‘¥ â†¦â†’ â„“(le)]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack,

,

,

,

(aid, ğ‘–, wr(ğ‘¥))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (gâ€², (ğ‘–, ğ‘š, â„“â€²) âˆ˜ stack,

)

,

,

,

)

ğ‘Ÿ := call ğ‘š âˆˆ inst(â„“(pc)) aid âˆˆ Aid fresh

â„“0 = init(g, ğ‘š)

â„“â€² = â„“[ğ‘Ÿ â†¦â†’ ğ‘—, pc â†¦â†’ next(â„“(pc))]

cmplâ€² = cmpl[ğ‘— â†¦â†’âŠ¥]

ğ‘— âˆˆ T fresh
c-byâ€² = c-by[ğ‘— â†¦â†’ ğ‘–]

(g, (ğ‘–, ğ‘šâ€², â„“) âˆ˜ stack,

, cmpl, c-by,

(aid, ğ‘–, call(ğ‘—))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘—, ğ‘š, â„“0) âˆ˜ (ğ‘–, ğ‘šâ€², â„“â€²) âˆ˜ stack,

)

, cmplâ€², c-byâ€²,

)

return âˆˆ inst(â„“(pc))

aid âˆˆ Aid fresh

cmplâ€² = cmpl[ğ‘– â†¦â†’ âŠ¤]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack,

, cmpl,

,

(aid, ğ‘–, return)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, stack,

)

, cmplâ€²,

,

)

await r âˆˆ inst(â„“(pc))

aid âˆˆ Aid fresh

cmpl(â„“(ğ‘Ÿ)) = âŠ¤ â„“â€² = â„“[pc â†¦â†’ next(â„“(pc))]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack,

, cmpl,

,

(aid, ğ‘–, await(â„“(ğ‘Ÿ)))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘–, ğ‘š, â„“â€²) âˆ˜ stack,

)

, cmpl,

,

)

await r âˆˆ inst(â„“(pc))

aid âˆˆ Aid fresh

cmpl(â„“(ğ‘Ÿ)) =âŠ¥ w-forâ€² = w-for[ğ‘– â†¦â†’ â„“(ğ‘Ÿ)]

â„“â€² = â„“[pc â†¦â†’ next(â„“(pc))]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack, pend, cmpl,

, w-for)

(aid, ğ‘–, await(â„“(ğ‘Ÿ)))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, stack, {(ğ‘–, ğ‘š, â„“â€²)} âŠ pend, cmpl,

, w-forâ€²)

await * âˆˆ inst(â„“(pc))

aid âˆˆ Aid fresh

â„“â€² = â„“[pc â†¦â†’ next(â„“(pc))]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack,

,

,

,

(aid, ğ‘–, await(*))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘–, ğ‘š, â„“â€²) âˆ˜ stack,

)

,

,

,

)

await * âˆˆ inst(â„“(pc))

aid âˆˆ Aid fresh

w-forâ€² = w-for[ğ‘– â†¦â†’ *]

â„“â€² = â„“[pc â†¦â†’ next(â„“(pc))]

(g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack, pend,

,

, w-for)

(aid, ğ‘–, await(*))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, stack, {(ğ‘–, ğ‘š, â„“â€²)} âŠ pend,

,

, w-forâ€²)

aid âˆˆ Aid fresh

w-for(ğ‘–) = ğ‘—

cmpl(ğ‘—) = âŠ¤

(g, ğœ–, {(ğ‘–, ğ‘š, â„“)} âŠ pend, cmpl,

, w-for)

(aid, ğ‘–, cont)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘–, ğ‘š, â„“), pend, cmpl,

, w-for)

(g, stack, {(ğ‘–, ğ‘š, â„“)} âŠ pend,

,

, w-for)

(aid, ğ‘–, cont)
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ (g, (ğ‘–, ğ‘š, â„“) âˆ˜ stack, pend,

,

, w-for)

aid âˆˆ Aid fresh

w-for(ğ‘–) = *

Fig. 6: Program semantics. For a function ğ‘“ , we use ğ‘“ [ğ‘ â†¦â†’ ğ‘] to denote a function
ğ‘” such that ğ‘”(ğ‘) = ğ‘“ (ğ‘) for all ğ‘ Ì¸= ğ‘ and ğ‘”(ğ‘) = ğ‘. The function inst returns the
instruction at some given control location while next gives the next instruction
to execute. We use âˆ˜ to denote sequence concatenation and init to denote the
initial state of a method call.

a1âˆ’â†’ ps1

An execution of ğ‘ƒ is a sequence ğœŒ = ps0

a2âˆ’â†’ . . . of transitions starting
in the initial conï¬guration ps0 and leading to a conï¬guration ps where the call
stack and the set of pending tasks are empty. C[ğ‘ƒ ] denotes the set of all program
variable valuations included in conï¬gurations that are reached in executions of ğ‘ƒ .
Since we are only interested in reasoning about the sequence of actions a1 Â·a2 Â·. . .
labeling the transitions of an execution, we will call the latter an execution as
well. The set of executions of a program ğ‘ƒ is denoted by Ex(ğ‘ƒ ).
Traces. The trace of an execution ğœŒ âˆˆ Ex(ğ‘ƒ ) is a tuple tr(ğœŒ) = (ğœŒ, MO, CO, SO, HB)
of strict partial orders between the actions in ğœŒ deï¬ned in Table 1. The method
invocation order MO records the order between actions in the same invocation,
and the call order CO is an extension of MO that additionally orders actions be-
fore an invocation with respect to those inside that invocation. The synchronous
happens-before order SO orders the actions in an execution as if all the invoca-
tions were synchronous (even if the execution may contain asynchronous ones).

10

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

It is an extension of CO where additionally, every action inside a callee is or-
dered before the actions following its invocation in the caller. The (asynchronous)
happens-before order HB contains typical control-ï¬‚ow constraints: it is an exten-
sion of CO where every action ğ‘ inside an asynchronous invocation is ordered
before the corresponding await in the caller, and before the actions following its
invocation in the caller if ğ‘ precedes the ï¬rst5 await in MO (an invocation can
be interrupted only when executing an await) or if the callee does not contain
an await (it is synchronous). Tr(ğ‘ƒ ) is the set of traces of ğ‘ƒ .

Table 1: Strict partial orders included in a trace. CO, SO, and HB are the smallest
satisfying relations.

a1 <ğœŒ a2
a1 âˆ¼ a2

(a1, a2) âˆˆ MO
(a1, a2) âˆˆ CO

(a1, a2) âˆˆ SO

a1 occurs before a2 in ğœŒ and a1 Ì¸= a2

a1 = ( , ğ‘–,

) and a2 = ( , ğ‘–,

)

a1 âˆ¼ a2 âˆ§ a1 <ğœŒ a2

(a1, a2) âˆˆ MO âˆ¨ (a1 = ( , ğ‘–, call(ğ‘—)) âˆ§ a2 = ( , ğ‘—,

))

âˆ¨ (âˆƒ a3. (a1, a3) âˆˆ CO âˆ§ (a3, a2) âˆˆ CO)
(a1, a2) âˆˆ CO âˆ¨ (âˆƒ a3. (a1, a3) âˆˆ SO âˆ§ (a3, a2) âˆˆ SO)

âˆ¨ (a1 = ( , ğ‘—,

) âˆ§ a2 = ( , ğ‘–,

) âˆ§ âˆƒ a3 = ( , ğ‘–, call(ğ‘—)). a3 <ğœŒ a2)

(a1, a2) âˆˆ HB

(a1, a2) âˆˆ CO âˆ¨ (âˆƒ a3. (a1, a3) âˆˆ HB âˆ§ (a3, a2) âˆˆ HB)

âˆ¨ ( a1 = ( , ğ‘—,

) âˆ§ a2 = ( , ğ‘–,

) âˆ§ âˆƒ a3 = ( , ğ‘–, await(ğ‘—)). a3 <ğœŒ a2 )

âˆ¨ ( a1 = ( , ğ‘—, await(ğ‘–â€²)) is the first await in ğ‘— âˆ§
) âˆ§ âˆƒ a3 = ( , ğ‘–, call(ğ‘—)). a3 <ğœŒ a2 )
a2 = ( , ğ‘–,

âˆ¨ ( a1 = ( , ğ‘—,

) âˆ§ Ì¸ âˆƒ ( , ğ‘—, await( )) âˆˆ ğœŒ âˆ§

a2 = ( , ğ‘–,

) âˆ§ âˆƒ a3 = ( , ğ‘–, call(ğ‘—)). a3 <ğœŒ a2 )

On the right of Fig. 1, we show a trace where two statements (represented
by the corresponding lines numbers) are linked by a dotted arrow if the corre-
sponding actions are related by MO, a dashed arrow if the corresponding actions
are related by CO but not by MO, and a solid arrow if the corresponding actions
are related by the HB but not by CO.

3 Synthesizing Asynchronous Programs

Given a synchronous program ğ‘ƒ and a subset of base methods ğ¿ âŠ† ğ‘ƒ , our goal
is to synthesize all asynchronous programs ğ‘ƒğ‘ that are equivalent to ğ‘ƒ and that
are obtained by substituting every method in ğ¿ with an equivalent asynchronous
version. The base methods are considered to be models of standard library calls
(e.g., IO operations) and asynchronous versions are deï¬ned by inserting await
* statements in their body. We use ğ‘ƒ [ğ¿] to emphasize a subset of base methods
ğ¿ in a program ğ‘ƒ . Also, we call ğ¿ a library. A library is called (a)synchronous
when all methods are (a)synchronous.
Asynchronizations of a synchronous program. Let ğ‘ƒ [ğ¿] be a synchronous
program, and ğ¿ğ‘ a set of asynchronous methods obtained from those in ğ¿ by

5 Code in between two awaits can execute before or after the control is returned to

the caller, depending on whether the first awaited task finished or not.

Automated Synthesis of Asynchronizations

11

inserting at least one await * statement in their body (and adding the key-
word async). Each method in ğ¿ğ‘ corresponds to a method in ğ¿ with the same
name, and vice-versa. ğ‘ƒğ‘[ğ¿ğ‘] is called an asynchronization of ğ‘ƒ [ğ¿] with re-
spect to ğ¿ğ‘ if it is a syntactically correct program obtained by replacing the
methods in ğ¿ with those in ğ¿ğ‘ and adding await statements as necessary.
More precisely, let ğ¿* âŠ† ğ‘ƒ be
the set of all methods of ğ‘ƒ
that transitively call methods of
ğ¿. Formally, ğ¿* is the smallest
set of methods that includes ğ¿
and satisï¬es the following: if a
method ğ‘š calls ğ‘šâ€² âˆˆ ğ¿*, then
ğ‘š âˆˆ ğ¿*. Then, ğ‘ƒğ‘[ğ¿ğ‘]
is an
asynchronization of ğ‘ƒ [ğ¿] w.r.t.
ğ¿ğ‘ if it is obtained from ğ‘ƒ as follows:

Fig. 7: A program and its asynchronizations.

async method m {
r1 = call m1;
await r1;
r2 = x;

await *
retVal = x;
x = input;
return;

await *
retVal = x;
x = input;
return;

retVal = x;
x = input;
return; }

async method m {
r1 = call m1;

r2 = x;
await r1;

async method m1 {

async method m1 {

r1 = call m1;

method m1 {

method m {

r2 = x;

}

}

}

}

}

â€“ Each method in ğ¿ is replaced with the corresponding method from ğ¿ğ‘.
â€“ All methods in ğ¿* âˆ– ğ¿ are declared as asynchronous (because every call to an
asynchronous method is followed by an await and any method using await
must be asynchronous).

â€“ For each invocation ğ‘Ÿ := call ğ‘š of ğ‘š âˆˆ ğ¿*, add await statements await ğ‘Ÿ
satisfying the well-formedness syntactic constraints described in Section 2.
Fig. 7 lists a synchronous program and its two asynchronizations, where ğ¿ =
{ğ‘š1} and ğ¿* = {ğ‘š, ğ‘š1}. Asynchronizations diï¬€er only in the await placement.
Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘] is the set of all asynchronizations of ğ‘ƒ [ğ¿] w.r.t. ğ¿ğ‘. The strong
asynchronization strongAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘] is an asynchronization where every await
immediately follows the matching call. It reaches exactly the same set of program
variable valuations as ğ‘ƒ .
Problem definition. We investigate the problem of enumerating all asynchro-
nizations of a given program w.r.t. a given asynchronous library, which are sound,
in the sense that they do not admit data races. Two actions a1 and a2 in a trace
ğœ = (ğœŒ, MO, CO, SO, HB) are concurrent if (a1, a2) Ì¸âˆˆ HB and (a2, a1) Ì¸âˆˆ HB.

An ansynchronous program ğ‘ƒğ‘ admits a data race (a1, a2), where (a1, a2) âˆˆ
SO, if a1 and a2 are two concurrent actions of a trace ğœ âˆˆ Tr(ğ‘ƒğ‘), and a1 and
a2 are read or write accesses to the same program variable ğ‘¥, and at least one
of them is a write. We write data races as ordered pairs w.r.t. SO to simplify
the deï¬nition of the algorithms in the next sections. Also, note that traces of
synchronous programs can not contain concurrent actions, and therefore they
do not admit data races. strongAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘] does not admit data races as well.
ğ‘ƒğ‘[ğ¿ğ‘] is called sound when it does not admit data races. The absence of
data races implies equivalence to the original program, in the sense of reaching
the same set of conï¬gurations (program variable valuations).

Definition 1. For a synchronous program ğ‘ƒ [ğ¿] and asynchronous library ğ¿ğ‘,
the asychronization synthesis problem asks to enumerate all sound asynchro-
nizations in Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘].

12

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

4 Enumerating Sound Asynchronizations

We present an algorithm for solving asynchronization synthesis, which relies on
a partial order between asynchronizations that guides the enumeration of pos-
sible solutions. The partial order takes into account the distance between calls
and corresponding awaits. Fig. 8 pictures the partial order for asynchroniza-
tions of the program on the left of Fig. 1. Each asynchronization is written as
a vector of distances, the ï¬rst (second) element is the number
of statements between await t1 (await t) and the matching call
(we count only statements that appear in the sequential program).
The edges connect comparable elements, smaller elements being
below bigger elements. The asynchronization on the middle of
Fig. 1 corresponds to the vector (1, 1). The highlighted elements
constitute the set of all sound asynchronizations. The strong asyn-
chronization corresponds to the vector (0, 0).

Fig. 8

(1, 1)

(2, 1)

(0, 0)

(0, 1)

(2, 0)

(1, 0)

Formally, an await statement sğ‘¤ in a method ğ‘š of an asynchronization
ğ‘ƒğ‘[ğ¿ğ‘] âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘] covers a read/write statement s in ğ‘ƒ if there exists a path
in the CFG of ğ‘š from the call statement matching sğ‘¤ to sğ‘¤ that contains s. The
set of statements covered by an await sğ‘¤ is denoted by Cover(sğ‘¤). We compare
asynchronizations in terms of sets of statements covered by awaits that match
the same call from the synchronous program ğ‘ƒ [ğ¿]. Since asynchronizations are
obtained by adding awaits, every call in asynchronization ğ‘ƒğ‘[ğ¿ğ‘] âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘]
corresponds to a fixed call in ğ‘ƒ [ğ¿]. Therefore, for two asynchronizations ğ‘ƒğ‘, ğ‘ƒ â€²
ğ‘ âˆˆ
ğ‘, denoted by ğ‘ƒğ‘ â‰¤ ğ‘ƒ â€²
Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘], ğ‘ƒğ‘ is smaller than ğ‘ƒ â€²
ğ‘, iï¬€ for every await sğ‘¤
in ğ‘ƒğ‘, there exists an await s â€²
ğ‘¤ in ğ‘ƒ â€²
ğ‘ that matches the same call as sğ‘¤, such that
Cover(sğ‘¤) âŠ† Cover(s â€²
ğ‘¤). For example, the two asynchronous programs in Fig. 7
are ordered by â‰¤ since Cover(await ğ‘Ÿ1) = {} in the ï¬rst and Cover(await ğ‘Ÿ1) =
{r2 = x} in the second. Note that the strong asynchronization is smaller than
every other asynchronization. Also, note that â‰¤ has a unique maximal element
that is called the weakest asynchronization and denoted by wkAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘]. In
Fig. 8, the weakest asynchronization corresponds to the vector (2, 1).

In the following, we say moving an await down (resp., up) when moving the
await further away from (resp. closer to) the matching call while preserving well-
formedness conditions in Section 2. Further away or closer to means increasing
or decreasing the set of statements that are covered by the await. For instance,
if an await sğ‘¤ in a program ğ‘ƒğ‘ is preceded by a while loop, then moving it up
means moving it before the whole loop and not inside the loop body. Otherwise,
the third well-formedness condition would be violated.
Relative Maximality. A crucial property of this partial order is that for every
asynchronization ğ‘ƒğ‘, there exists a unique maximal asynchronization that is
smaller than ğ‘ƒğ‘ and that is sound. Formally, an asynchronization ğ‘ƒ â€²
ğ‘ is called a
maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘ if (1) ğ‘ƒ â€²
ğ‘ is sound, and
(2) âˆ€ ğ‘ƒ â€²â€²

ğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘]. ğ‘ƒ â€²â€²

ğ‘ â‰¤ ğ‘ƒğ‘, ğ‘ƒ â€²
ğ‘ â‰¤ ğ‘ƒ â€²
ğ‘.

ğ‘ is sound and ğ‘ƒ â€²â€²

ğ‘ â‰¤ ğ‘ƒğ‘ â‡’ ğ‘ƒ â€²â€²

Lemma 1. Given an asynchronization ğ‘ƒğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘], there exists a unique
program ğ‘ƒ â€²
ğ‘ that is a maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘.

Automated Synthesis of Asynchronizations

13

Algorithm 1 An algorithm for enumerating all sound asynchronizations (these
asynchronizations are obtained as a result of the output instruction). MaxRel
returns the maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘
1: procedure AsySyn(ğ‘ƒğ‘, sğ‘¤)
2:
3:
4:
5:
6:

ğ‘ƒ â€²
ğ‘ â† MaxRel(ğ‘ƒğ‘);
output ğ‘ƒ â€²
ğ‘;
ğ’« â† ImPred(ğ‘ƒ â€²
for each (ğ‘ƒ â€²â€²

ğ‘ , s â€²â€²
AsySyn(ğ‘ƒ â€²â€²

ğ‘, sğ‘¤);
ğ‘¤) âˆˆ ğ’«
ğ‘ , s â€²â€²
ğ‘¤);

ğ‘ and ğ‘ƒ 2

The asynchronization ğ‘ƒ â€²

ğ‘ exists because the bottom element of â‰¤ is sound.
To prove uniqueness, assume by contradiction that there exist two incomparable
maximal asynchronizations ğ‘ƒ 1
ğ‘¤ w.r.t. the
control-ï¬‚ow of the sequential program that is placed in diï¬€erent positions in the
two programs. Assume that s 1
ğ‘ . Then, we
move s 1
ğ‘ further away from its matching call to the same position as in
ğ‘ƒ 2
ğ‘ . This modiï¬cation does not introduce data races since ğ‘ƒ 2
ğ‘ is data race free.
Thus, the resulting program is data race free, bigger than ğ‘ƒ 1
ğ‘ , and smaller than
ğ‘ƒğ‘ w.r.t. â‰¤ contradicting the fact that ğ‘ƒ 1

ğ‘¤ is closer to its matching call in ğ‘ƒ 1

ğ‘ and select the ï¬rst await s 1

ğ‘ is a maximal asynchronization.

ğ‘¤ in ğ‘ƒ 1

4.1 Enumeration Algorithm

Our algorithm for enumerating all sound asynchronizations is given in Algo-
rithm 1 as a recursive procedure AsySyn that we describe in two phases.

First, ignore the second argument of AsySyn (in blue), which represents an
await statement. For an asynchronization ğ‘ƒğ‘, AsySyn outputs all sound asyn-
chronizations that are smaller than ğ‘ƒğ‘. It uses MaxRel to compute the maximal
asynchronization ğ‘ƒ â€²
ğ‘ of ğ‘ƒ relative to ğ‘ƒğ‘, and then, calls itself recursively for all
ğ‘. AsySyn outputs all sound asynchronizations of
immediate predecessors of ğ‘ƒ â€²
ğ‘ƒ when given as input the weakest asynchronization of ğ‘ƒ .

Recursive calls on immediate predeces-
sors are necessary because the set of sound
asynchronizations is not downward-closed
w.r.t. â‰¤. For instance, the asynchroniza-
tion on the right of Fig. 9 is an immediate
predecessor of the sound asynchronization
on the left but it has a data race on ğ‘¥.

async method m {
r1 = call m1;
r2 = x;
await r1;

}
async method m1 {
r3 = call m2;
x = x + 1;
await r3;

}
async method m2 {

async method m {
r1 = call m1;
r2 = x;
await r1;

}
async method m1 {
r3 = call m2;
await r3;
x = x + 1;

}
async method m2 {

await *
retVal = input;
}
return;

await *
retVal = input;
}
return;

The delay complexity of this algorithm
remains exponential
in general, since a
sound asynchronization may be outputted
multiple times. Asynchronizations are only partially ordered by â‰¤ and diï¬€erent
chains of recursive calls starting in diï¬€erent immediate predecessors may end up
outputting the same solution. For instance, for the asynchronizations in Fig. 8,
the asynchronization (0, 0) will be outputted twice because it is an immediate
predecessor of both (1, 0) and (0, 1).

Fig. 9: Asynchronizations.

14

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

To avoid this redundancy, we use a reï¬nement of the above that restricts
the set of immediate predecessors available for a (recursive) call of AsySyn.
This is based on a strict total order â‰ºğ‘¤ between awaits in a program ğ‘ƒğ‘ that
follows a topological ordering of its inter-procedural CFG, i.e., if sğ‘¤ occurs before
s â€²
ğ‘¤ in the body of a method ğ‘š, then sğ‘¤ â‰ºğ‘¤ s â€²
ğ‘¤, and if sğ‘¤ occurs in a method
ğ‘¤ occurs in a method ğ‘šâ€² s.t. ğ‘š (indirectly) calls ğ‘šâ€², then sğ‘¤ â‰ºğ‘¤ s â€²
ğ‘š and s â€²
ğ‘¤.
Therefore, AsySyn takes an await statement sğ‘¤ as a second parameter, which
is initially the maximal element w.r.t. â‰ºğ‘¤, and it calls itself only on immediate
predecessors of a solution obtained by moving up an await s â€²â€²
ğ‘¤ smaller than or
equal to sğ‘¤ w.r.t. â‰ºğ‘¤. The recursive call on that predecessor will receive as input
s â€²â€²
ğ‘¤. Formally, this relies on a function ImPred that returns pairs of immediate
predecessors and await statements deï¬ned as follows:

ImPred(ğ‘ƒ â€²

ğ‘, sğ‘¤) = {(ğ‘ƒ â€²â€²

ğ‘ , s â€²â€²

ğ‘¤) : ğ‘ƒ â€²â€²

ğ‘ < ğ‘ƒ â€²
and s â€²â€²

ğ‘ and âˆ€ ğ‘ƒ â€²â€²â€²
ğ‘¤ âª¯ğ‘¤ sğ‘¤ and ğ‘ƒ â€²â€²

ğ‘ âˆˆ ğ‘ƒ â€²

ğ‘ â†‘ s â€²â€²

ğ‘¤ }

ğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘]. ğ‘ƒ â€²â€²â€²

ğ‘ < ğ‘ƒ â€²

ğ‘ =â‡’ ğ‘ƒ â€²â€²â€²

ğ‘ â‰¤ ğ‘ƒ â€²â€²
ğ‘

ğ‘ â†‘ s â€²â€²

ğ‘¤, moving it up w.r.t. the position in ğ‘ƒ â€²

ğ‘¤ is the set of asynchronizations obtained from ğ‘ƒ â€²

(ğ‘ƒ â€²
ğ‘ by changing only the
position of s â€²â€²
ğ‘). For instance, looking
at immediate predecessors of (1, 1) in Fig. 8, (0, 1) is obtained by moving the
first await in â‰ºğ‘¤. Therefore, the recursive call on (0, 1) computes the maximal
asynchronization relative to (0, 1), which is (0, 1), and stops (ImPred returns âˆ…
because the input sğ‘¤ is the minimal element of â‰ºğ‘¤, and already immediately
after the call). Its immediate predecessor is explored when recursing on (1, 0).

Algorithm 1 outputs all sound asynchronizations because after having com-
ğ‘ in a recursive call with parameter sğ‘¤, any

ğ‘ and ğ‘ƒ 2
ğ‘, sğ‘¤) obtained by moving up the awaits s 1

puted a maximal asynchronization ğ‘ƒ â€²
smaller sound asynchronization is smaller than some predecessor in ImPred(ğ‘ƒ â€²
Also, it can not output the same asynchonization twice. Let ğ‘ƒ 1
predecessors in ImPred(ğ‘ƒ â€²
respectively, and assume that s 1
cursive call on ğ‘ƒ 1
in the recursive call on ğ‘ƒ 2
the sets of solutions computed in these two recursion branches are distinct.
Theorem 1. AsySyn(wkAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘], sğ‘¤), where sğ‘¤ is maximal in wkAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘]
w.r.t. â‰ºğ‘¤, outputs all sound asynchronizations of ğ‘ƒ [ğ¿] w.r.t. ğ¿ğ‘.

ğ‘ be two
ğ‘¤ and s 2
ğ‘¤,
ğ‘¤. Then, all solutions computed in the re-
ğ‘ while all the solutions computed
ğ‘¤ closer to the matching call. Therefore,

ğ‘¤ placed as in ğ‘ƒ â€²

ğ‘ will have s 2

ğ‘ will have s 2

ğ‘¤â‰ºğ‘¤s 2

ğ‘, sğ‘¤).

The delay complexity of Algorithm 1 is polynomial time modulo an oracle
that returns a maximal asynchronization relative to a given one. In the next
section, we show that the latter problem can be reduced in polynomial time to
the reachability problem in sequential programs.

5 Computing Maximal Asynchronizations

In this section, we present an implementation of the procedure MaxRel that
relies on a reachability oracle. In particular, we ï¬rst describe an approach for
computing the maximal asynchronization relative to a given asynchronization
ğ‘ƒğ‘, which can be seen as a way of repairing ğ‘ƒğ‘ so that it becomes data-race

Automated Synthesis of Asynchronizations

15

free. Intuitively, we repeatedly eliminate data races in ğ‘ƒğ‘ by moving certain
await statements closer to the matching calls. The data races in ğ‘ƒğ‘ (if any) are
enumerated in a certain order that prioritizes data races between actions that
occur ï¬rst in executions of the original synchronous program. This order allows
to avoid superï¬‚uous repair steps.

5.1 Data Race Ordering
An action a representing a read/write access in a trace ğœ of an asynchronization
ğ‘ƒğ‘ of ğ‘ƒ is synchronously reachable if there is an action a â€² in a trace ğœ â€² of ğ‘ƒ
that represents the same statement, i.e., S(a) = S(a â€²). It can be proved that
any trace of an asynchronization contains a data race if it contains a data race
between two synchronously reachable actions (see Appendix C). In the following,
we focus on data races between actions that are synchronously reachable.

We deï¬ne an order between such data races based on the order between
actions in executions of the original synchronous program ğ‘ƒ . This order relates
data races in possibly diï¬€erent executions or asynchronizations of ğ‘ƒ , which is
possible because each action in a data race corresponds to a statement in ğ‘ƒ .

For two read/write statements s and s â€², s â‰º s â€² denotes the fact that there
is an execution of ğ‘ƒ in which the first time s is executed occurs before the
first time s â€² is executed. For two actions a and a â€² in an execution/trace of an
asynchronization, generated by two read/write statements s = S(ğ‘) and s â€² =
S(ğ‘â€²), a â‰ºSO a â€² holds if s â‰º s â€² and either s â€² Ì¸â‰º s or s â€² is reachable from s in the
interprocedural6 control-ï¬‚ow graph of ğ‘ƒ without taking any back edge7. For a
deterministic synchronous program (admitting a single execution), a â‰ºSO a â€² iï¬€
S(ğ‘) â‰º S(ğ‘â€²). For non-deterministic programs, when S(ğ‘) and S(ğ‘â€²) are contained
in a loop body, it is possible that S(ğ‘) â‰º S(ğ‘â€²) and S(ğ‘â€²) â‰º S(ğ‘). In this case, we
use the control-ï¬‚ow order to break the tie between a and a â€².

The order between data races corresponds to the colexicographic order in-
duced by â‰ºSO. This is a partial order since actions may originate from diï¬€erent
control-ï¬‚ow paths and are incomparable w.r.t. â‰ºSO.
Definition 2 (Data Race Order). Given two races (a1, a2) and (a3, a4) ad-
mitted by (possibly different) asynchronizations of a synchronous program ğ‘ƒ , we
have that (a1, a2) â‰ºSO (a3, a4) iff a2 â‰ºSO a4, or a2 = a4 and a1 â‰ºSO a3.

Repairing a minimal data race (a1, a2) w.r.t. â‰ºSO removes any other data race
(a1, a4) with (a2, a4) âˆˆ HB (note that we cannot have (a4, a2) Ì¸âˆˆ HB since a2 â‰ºSO
a4). The repair will enforce that (a1, a2) âˆˆ HB which implies that (a1, a4) âˆˆ HB.

5.2 Repairing Data Races
Repairing a data race (a1, a2) reduces to modifying the position of a certain
await. We consider only repairs where awaits are moved up (closer to the match-
ing call). The â€œcompletenessâ€ of this set of repairs follows from the particular
order in which we enumerate data races.
6 The interprocedural graph is the union of the control-flow graphs of each method
along with edges from call sites to entry nodes, and from exit nodes to return sites.
7 A back edge points to a block that has already been met during a depth-first traversal

of the control-flow graph, and corresponds to loops.

16

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

Algorithm 2 The procedure MaxRel to ï¬nd the maximal asynchronization of
ğ‘ƒ relative to ğ‘ƒğ‘.
1: procedure MaxRel(ğ‘ƒğ‘)
2:
3:
4:
5:
6:
7:

ğ‘ƒ â€²
root â† RCMinDR(ğ‘ƒ â€²
ğ‘)
while root Ì¸= âŠ¥

ğ‘ƒ â€²
ğ‘ â† RDR(ğ‘ƒ â€²
root â† RCMinDR(ğ‘ƒ â€²
ğ‘)

ğ‘ â† ğ‘ƒğ‘

ğ‘, root)

return ğ‘ƒ â€²
ğ‘

Let s1 and s2 be the statements generating a1
and a2. In general, there exists a method ğ‘š that
(transitively) calls another asynchronous method
ğ‘š1 that contains s1 and before awaiting for ğ‘š1 it
(transitively) calls a method ğ‘š2 that executes s2.
This is pictured in Fig. 10. It is also possible that
ğ‘š itself contains s2 (see the program on the right
of Fig. 7). The repair consists in moving the await
for ğ‘š1 before the call to ğ‘š2 since this implies that s1 will always execute before
s2 (and the corresponding actions are related by happens-before).

Fig. 10: A data race repair.

Formally, any two racing actions have a common ancestor in the call order
CO which is a call action. The least common ancestor of a1 and a2 in CO among
call actions is denoted by LCACO(a1, a2). In Fig. 10, it corresponds to the call
statement sğ‘. More precisely, LCACO(a1, a2) is a call action ağ‘ = ( , ğ‘–, call(ğ‘—)) s.t.
(ağ‘, a1) âˆˆ CO, (ağ‘, a2) âˆˆ CO, and for each other call action a â€²
ğ‘) âˆˆ CO
then (a â€²
ğ‘, a1) Ì¸âˆˆ CO. This call action represents an asynchronous call for which
the matching await sğ‘¤ must move to repair the data race. The await should
be moved before the last statement in the same method generating an action
which precedes a2 in the reï¬‚exive closure of call order (statement s in Fig. 10).
This way every statement that follows sğ‘ in call order will be executed before s
and before any statement which succeeds s in call order, including s2. Note that
moving the await sğ‘¤ anywhere after s will not aï¬€ect the concurrency between
a1 and a2.

ğ‘, if (ağ‘, a â€²

The pair (sğ‘, s) is called the root cause of the data race (ğ‘1, ğ‘2). Let RDR(ğ‘ƒğ‘, sğ‘, s)

be the maximal asynchronization ğ‘ƒ â€²
ment matching sğ‘ occurs after s on a CFG path.

ğ‘ smaller than ğ‘ƒğ‘ w.r.t. â‰¤, s.t. no await state-

5.3 A Procedure for Computing Maximal Asynchronizations
Given an asynchronization ğ‘ƒğ‘, the procedure MaxRel in Algorithm 2 computes
the maximal asynchronization relative to ğ‘ƒğ‘ by repairing data races iteratively
until the program becomes data race free. The sub-procedure RCMinDR(ğ‘ƒ â€²
ğ‘)
computes the root cause of a minimal data race (a1, a2) of ğ‘ƒ â€²
ğ‘ w.r.t. â‰ºSO such that
ğ‘ is data race free, RCMinDR(ğ‘ƒ â€²
the two actions are synchronously reachable. If ğ‘ƒ â€²
ğ‘)
returns âŠ¥. The following theorem states the correctness of MaxRel.
Theorem 2. Given an asynchronization ğ‘ƒğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘], MaxRel(ğ‘ƒğ‘) re-
turns the maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘.

async method m2 {     s2: â€¦ ;}async method m1 {     s1: â€¦ ;}async method m {  sc: r1 = call m1;   s: r2 = call m2;  sw: await r1;}Automated Synthesis of Asynchronizations

17

MaxRel(ğ‘ƒğ‘) repairs a number of data races which is linear in the size of
the input. Indeed, each repair results in moving an await closer to the matching
call and before at least one more statement from the original program ğ‘ƒ .

The problem of computing root causes of minimal data races is reducible to
reachability (assertion checking) in sequential programs. This reduction builds
on a program instrumentation for checking if there exists a data race that in-
volves two given statements (s1, s2) that are reachable in an executions of ğ‘ƒ .This
instrumentation is used in an iterative process where pairs of statements are
enumerated according to the colexicographic order induced by â‰º. For lack of
space, we present only the main ideas of the instrumentation (see Appendix D).
The instrumentation simulates executions of an asynchronization ğ‘ƒğ‘ using non-
deterministic synchronous code where methods may be only partially executed
(modeling await interruptions). Immediately after executing s1, the current in-
vocation ğ‘¡1 is interrupted (by executing a return added by the instrumentation).
The active invocations that transitively called ğ‘¡1 are also interrupted when reach-
ing an await for an invocation in this call chain (the other invocations are ex-
ecuted until completion as in the synchronous semantics). When reaching s2, if
s1 has already been executed and at least one invocation has been interrupted,
which means that s1 is concurrent with s2, then the instrumentation stops with
an assertion violation. The instrumentation also computes the root cause of the
data race using additional variables for tracking call dependencies.

6 Asymptotic Complexity of Asynchronization Synthesis

We state the complexity of the asynchronization synthesis problem. Algorithm 1
shows that the delay complexity of this problem is polynomial-time in the num-
ber of statements in input program modulo the complexity of computing a
maximal asynchronization, which Algorithm 2 shows to be polynomial-time re-
ducible to reachability in sequential programs. Since the reachability problem is
PSPACE-complete for ï¬nite-state sequential programs [16], we get the following:

Theorem 3. The output complexity8 and delay complexity of the asynchroniza-
tion synthesis problem is polynomial time modulo an oracle for reachability in
sequential programs, and PSPACE for finite-state programs.

This result is optimal, i.e., checking whether there exists a sound asynchro-
nization which is diï¬€erent from the trivial strong synchronization is PSPACE-
hard (follows from a reduction from the reachability problem). See Appendices D
and E for the detailed formal proofs.

7 Asynchronization Synthesis Using Data-Flow Analysis

In this section, we present a reï¬nement of Algorithm 2 that relies on a bottom-
up inter-procedural data ï¬‚ow analysis. The analysis is used to compute maximal
asynchronizations for abstractions of programs where every Boolean condition

8 Note that all asynchronizations can be enumerated with polynomial space.

18

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

(in if-then-else or while statements) is replaced with the non-deterministic choice
*, and used as an implementation of MaxRel in Algorithm 1.

For a program ğ‘ƒ , we deï¬ne an abstraction ğ‘ƒ # where every conditional if
âŸ¨ğ‘™ğ‘’âŸ© {ğ‘†1} else {ğ‘†2} is rewritten to if * {ğ‘†1} else {ğ‘†2}, and every while âŸ¨ğ‘™ğ‘’âŸ©
{ğ‘†} is rewritten to if * {ğ‘†}. Besides adding the non-deterministic choice *,
loops are unrolled exactly once. Every asynchronization ğ‘ƒğ‘ of ğ‘ƒ corresponds to
an abstraction ğ‘ƒ #
ğ‘ obtained by applying exactly the same rewriting. ğ‘ƒ # is a
sound abstraction of ğ‘ƒ in terms of sound asynchronizations it admits. Unrolling
loops once is sound because every asynchronous call in a loop iteration should
be awaited for in the same iteration (see the syntactic constraints in Section 2).

Theorem 4. If ğ‘ƒ #
sound asynchronization of ğ‘ƒ w.r.t. ğ¿ğ‘.

ğ‘ is a sound asynchronization of ğ‘ƒ # w.r.t. ğ¿ğ‘, then ğ‘ƒğ‘ is a

The procedure for computing maximal asynchronizations of ğ‘ƒ # relative to
a given asynchronization ğ‘ƒ #
ğ‘ traverses methods of ğ‘ƒ #
ğ‘ in a bottom-up fashion,
detects data races using summaries of read/write accesses computed using a
straightforward data-ï¬‚ow analysis, and repairs data races using the schema pre-
sented in Section 5.2. Applying this procedure to a real programming language
requires an alias analysis to detect statements that may access the same memory
location (this is trivial in our language which is used to simplify the exposition).
We consider an enumeration of methods called bottom-up order, which is the
reverse of a topological ordering of the call graph9. For each method ğ‘š, let â„›(ğ‘š)
be the set of program variables that ğ‘š can read, which is deï¬ned as the union
of â„›(ğ‘šâ€²) for every method ğ‘šâ€² called by ğ‘š and the set of program variables
read in statements in the body of ğ‘š. The set of variables ğ’²(ğ‘š) that ğ‘š can
write is deï¬ned in a similar manner. We deï¬ne RW-var(ğ‘š) = (â„›(ğ‘š), ğ’²(ğ‘š)).
We extend the notation RW-var to statements as follows: RW-var(âŸ¨ğ‘ŸâŸ© := âŸ¨ğ‘¥âŸ©) =
({ğ‘¥}, âˆ…), RW-var(âŸ¨ğ‘¥âŸ© := âŸ¨ğ‘™ğ‘’âŸ©) = (âˆ…, {ğ‘¥}), RW-var(ğ‘Ÿ := call ğ‘š) = RW-var(ğ‘š),
and RW-var(s) = (âˆ…, âˆ…), for any other type of statement ğ‘ . Also, let CRW-var(ğ‘š)
be the set of read or write accesses that ğ‘š can do and that can be concurrent
with accesses that a caller of ğ‘š can do after calling ğ‘š. These correspond to
read/write statements that follow an await in ğ‘š, or to accesses in CRW-var(ğ‘šâ€²)
for a method ğ‘šâ€² called by ğ‘š. These sets of accesses can be computed using the
following data-ï¬‚ow analysis: for all methods ğ‘š âˆˆ ğ‘ƒ #
ğ‘ in bottom-up order, and
for each statement s in the body of ğ‘š from begin to end,

â€“ if s is a call to ğ‘šâ€² and s is not reachable from an await in the CFG of ğ‘š

âˆ™ CRW-var(ğ‘š) â† CRW-var(ğ‘š) âˆª CRW-var(ğ‘šâ€²)

â€“ if s is reachable from an await statement in the CFG of ğ‘š

âˆ™ CRW-var(ğ‘š) â† CRW-var(ğ‘š) âˆª RW-var(s)

We use (â„›1, ğ’²1) â—â–· (â„›2, ğ’²2) to denote the fact that ğ’²1 âˆ© (â„›2 âˆª ğ’²2) Ì¸= âˆ… or
ğ’²2 âˆ© (â„›1 âˆª ğ’²1) Ì¸= âˆ… (i.e., a conï¬‚ict between read/write accesses). We deï¬ne the
procedure MaxRel# that given an asynchronization ğ‘ƒ #

ğ‘ works as follows:

9 The nodes of the call graph are methods and there is an edge from a method ğ‘š1 to

a method ğ‘š2 if ğ‘š1 contains a call statement that calls ğ‘š2.

Automated Synthesis of Asynchronizations

19

â€“ for all methods ğ‘š âˆˆ ğ‘ƒ #

ğ‘ in bottom-up order, and for each statement s in

the body of ğ‘š from begin to end,

âˆ™ if s occurs between ğ‘Ÿ := call ğ‘šâ€² and await ğ‘Ÿ (for some ğ‘šâ€²), and

RW-var(s) â—â–· CRW-var(ğ‘šâ€²), then ğ‘ƒ #

ğ‘ â† RDR(ğ‘ƒ #

ğ‘ , ğ‘Ÿ := call ğ‘šâ€², ğ‘ )

â€“ return ğ‘ƒ #
ğ‘

Theorem 5. MaxRel#(ğ‘ƒ #
Since MaxRel# is based on a single bottom-up traversal of the call graph of
the input asynchronization ğ‘ƒ #

ğ‘ ) returns a maximal asynchronization relative to ğ‘ƒ #
ğ‘ .

ğ‘ we get the following result.

Theorem 6. The delay complexity of the asynchronization synthesis problem
restricted to abstracted programs ğ‘ƒ # is polynomial time.

8 Experimental Evaluation

We present an empirical evaluation of our asynchronization synthesis approach,
where maximal asynchronizations are computed using the data-ï¬‚ow analysis in
Section 7. Our benchmark consists mostly of asynchronous C# programs from
open-source GitHub projects. We evaluate the eï¬€ectiveness in reproducing the
original program as an asynchronization of a program where asynchronous calls
are reverted to synchronous calls, along with other sound asynchronizations.
Implementation. We developed a prototype tool that uses the Roslyn .NET
compiler platform [27] to construct CFGs for methods in a C# program. This
prototype supports C# programs written in static single assignment (SSA) form
that include basic conditional/looping constructs and async/await as concur-
rency primitives. Note that object ï¬elds are interpreted as program variables in
the terminology of Â§2 (data races concern accesses to object ï¬elds). It assumes
that alias information is provided apriori; these constraints can be removed in
the future with more engineering eï¬€ort. In general, our synthesis procedure is
compatible with any sound alias analysis. The precision of this analysis impacts
only the set (number) of asynchronizations outputted by the procedure (a more
precise analysis may lead to more sound asynchronizations).

The tool takes as input a possibly asynchronous program, and a mapping
between synchronous and asynchronous variations of base methods in this pro-
gram. It reverts every asynchronous call to a synchronous call, and it enumerates
sound asynchronizations of the obtained program (using Algorithm 1).
Benchmark. Our evaluation uses a benchmark listed in Table 2, which con-
tains 5 synthetic examples (variations of the program in Fig. 1), 9 programs
extracted from open-source C# GitHub projects (their name is a preï¬x of the
repository name), and 2 programs inspired by questions on stackoverflow.com
about async/await in C# (their name ends in Stackoverï¬‚ow). Overall, there
are 13 base methods involved in computing asynchronizations of these pro-
grams (having both synchronous and asynchronous versions), coming from 5 C#
libraries (System.IO, System.Net, Windows.Storage, Microsoft.WindowsAzure.-
Storage, and Microsoft.Azure.Devices). They are modeled as described in Sec-
tion 2.

20

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

Table 2: Empirical results. Syntactic characteristics of input programs: lines
of code (loc), number of methods (m), number of method calls (c), number of
asynchronous calls (ac), number of awaits that could be placed at least one state-
ment away from the matching call (await#). Data concerning the enumeration
of asynchronizations: number of awaits that were placed at least one statement
away from the matching call (await), number of races discovered and repaired
(races), number of statements that the awaits in the maximal asynchronization
are covering more than in the input program (cover), number of computed asyn-
chronizations (async), and running time (t).

Program

loc m c

SyntheticBenchmark-1
SyntheticBenchmark-2
SyntheticBenchmark-3
SyntheticBenchmark-4
SyntheticBenchmark-5
Azure-Remote
Azure-Webjobs
FritzDectCore
MultiPlatform
NetRpc
TestAZureBoards
VBForums-Viewer
Voat
WordpressRESTClient
ReadFile-Stackoverflow 47
50
UI-Stackoverflow

77
115
168
171
170
520
190
141
53
887
43
275
178
133

3
4
6
6
6
10
6
7
2
13
3
7
3
3
2
3

6
12
16
17
17
14
14
11
6
18
3
10
5
10
3
4

ac

5
10
13
14
14
5
6
8
4
11
3
7
5
8
3
4

await#

await

races

cover

async

t(s)

4
6
9
10
10
0
1
1
2
4
0
3
2
4
1
3

4
3
7
8
8
0
1
1
2
1
0
2
1
2
0
3

5
3
4
5
9
0
0
0
0
3
0
1
1
1
1
3

0
0
0
0
0
0
1
1
2
0
0
1
1
0
0
0

9
8
128
256
272
1
3
2
4
3
1
6
3
4
1
12

1.4
1.4
1.5
1.9
2
2.2
1.6
1.6
1.1
2
1.5
1.8
1.2
1.7
1.5
1.5

Evaluation. The last ï¬ve columns of Table 2 list data concerning the application
of our tool. The column async lists the number of outputted sound asynchro-
nizations. In general, the number of asynchronizations depends on the number
of invocations (column ac) and the size of the code blocks between an invocation
and the instruction using its return value (column await# gives the number of
non-empty blocks). The number of sound asynchronizations depends roughly, on
how many of these code blocks are racing with the method body. These asyn-
chronizations contain awaits that are at a non-zero distance from the matching
call (non-zero values in column await) and for many Github programs, this dis-
tance is bigger than in the original program (non-zero values in column cover).
This shows that we are able to increase the distances between awaits and their
matching calls for those programs. The distance between awaits and matching
calls in maximal asynchronizations of non synthetic benchmarks is 1.27 state-
ments on average. A statement representing a method call is counted as one
independently of the methodâ€™s body size. With a single level of inlining, the
number of statements becomes 2.82 on average. However, these statements are
again, mostly IO calls (access to network or disk) or library calls (string/bytes
formatting methods) whose execution time is not negligible. The running times
for the last three synthetic benchmarks show that our procedure is scalable when
programs have a large number of sound asynchronizations.

With few exceptions, each program admits multiple sound asynchronizations
(values in column async bigger than one), which makes the focus on the delay
complexity relevant. This leaves the possibility of making a choice based on

Automated Synthesis of Asynchronizations

21

other criteria, e.g., performance metrics. As shown by the examples in Fig. 2,
their performance can be derived only dynamically (by executing them). These
results show that our techniques have the potential of becoming the basis of a
refactoring tool allowing programmers to improve their usage of the async/await
primitives. The artifacts are available in a GitHub repository [2].

9 Related Work

There are many works on synthesizing or repairing concurrent programs in
the standard multi-threading model, e.g., automatic parallelization in compil-
ers [1,6,19], or synchronization synthesis [10,11,12,24,31,30,5,9,18]. We focus on
the use of async/await which poses speciï¬c challenges that are not covered in
these works.

Our semantics without await * instructions is equivalent to the semantics
deï¬ned in [3,28]. But, to simplify the exposition, we consider a more restricted
programming language. For the modeling of asynchronous IO operations, we
follow [3] with the restriction that the code following an await * is executed
atomically. This is sound when focusing on data-race freedom because even if
executed atomically, any two instructions from diï¬€erent asynchronous IO oper-
ations (following await *) are not happens-before related.
Program Refactoring. Program refactoring tools have been proposed for con-
verting C# programs using explicit callbacks into async/await programs [25] or
Android programs using AsyncTask into programs that use IntentService [22].
The C# tool [25], which is the closest to our work, makes it possible to repair
misusage of async/await that might result in deadlocks. This tool cannot mod-
ify procedure calls to be asynchronous as in our work. A static analysis based
technique for refactoring JavaScript programs is proposed in [17]. As opposed
to our work, this refactoring technique is unsound in general. It requires that
programmers review the refactoring for correctness, which is error-prone. Also,
in comparison to [17], we carry a formal study of the more general problem of
ï¬nding all sound asynchronizations and investigate its complexity.
Data Race Detection. Many works study dynamic data race detection using
happens-before and lock-set analysis, or timing-based detection [21,20,29,26,14].
They could be used to approximate our reduction from data race checking to
reachability in sequential programs. Some works [4,23,13] propose static analyses
for ï¬nding data races. [4] designs a compositional data race detector for multi-
threaded Java programs, based on an inter-procedural analysis assuming that
any two public methods can execute in parallel. Similar to [28], they precom-
pute method summaries to extract potential racy accesses. These approaches are
similar to the analysis in Section 7, but they concern a diï¬€erent programming
model.
Analyzing Asynchronous Programs. Several works propose program analy-
ses for various classes of asynchronous programs. [7,15] give complexity results for
the reachability problem, and [28] proposes a static analysis for deadlock detec-
tion in C# programs that use both asynchronous and synchronous wait prim-
itives. [8] investigates the problem of checking whether Java UI asynchronous

22

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

programs have the same set of behaviors as sequential programs where roughly,
asynchronous tasks are executed synchronously.

10 Conclusion

We proposed a framework for refactoring sequential programs to equivalent asyn-
chronous programs based on async/await. We determined precise complexity
bounds for the problem of computing all sound asynchronizations. This problem
makes it possible to compute a sound asynchronization that maximizes perfor-
mance by separating concerns â€“ enumerate sound asynchronizations and evaluate
performance separately. On the practical side, we have introduced an approxi-
mated synthesis procedure based on data-ï¬‚ow analysis that we implemented and
evaluated on a benchmark of non-trivial C# programs.

The asynchronous programs rely exclusively on async/await and are deadlock-
free by deï¬nition. Deadlocks can occur in a mix of async/await with â€œexplicitâ€
multi-threading that includes blocking wait primitives. Extending our approach
for such programs is an interesting direction for future work.

References

1. Bacon, D.F., Graham, S.L., Sharp, O.J.: Compiler

for
high-performance computing. ACM Comput. Surv. 26(4), 345â€“420 (1994).
https://doi.org/10.1145/197405.197406,
https://doi.org/10.1145/197405.
197406

transformations

2. Beillahi, S.M., Bouajjani, A., Enea, C., Lahiri, S.: Artifact for the SAS
2022).
https://doi.org/10.5281/zenodo.

2022
https://doi.org/10.5281/zenodo.7055422,
7055422

of Asynchronizations

paper: Automated

Synthesis

(May

3. Bierman, G.M., Russo, C.V., Mainland, G., Meijer, E., Torgersen, M.: Pause â€™nâ€™
play: Formalizing asynchronous c#. In: Noble, J. (ed.) ECOOP 2012 - Object-
Oriented Programming - 26th European Conference, Beijing, China, June 11-
16, 2012. Proceedings. Lecture Notes in Computer Science, vol. 7313, pp. 233â€“
257. Springer (2012). https://doi.org/10.1007/978-3-642-31057-7 12, https://
doi.org/10.1007/978-3-642-31057-7_12

4. Blackshear, S., Gorogiannis, N., Oâ€™Hearn, P.W., Sergey, I.: Racerd: composi-
tional static race detection. Proc. ACM Program. Lang. 2(OOPSLA), 144:1â€“144:28
(2018). https://doi.org/10.1145/3276514, https://doi.org/10.1145/3276514
5. Bloem, R., Hofferek, G., KÂ¨onighofer, B., KÂ¨onighofer, R., Ausserlechner, S., Spork,
R.: Synthesis of synchronization using uninterpreted functions. In: Formal Methods
in Computer-Aided Design, FMCAD 2014, Lausanne, Switzerland, October 21-
24, 2014. pp. 35â€“42. IEEE (2014). https://doi.org/10.1109/FMCAD.2014.6987593,
https://doi.org/10.1109/FMCAD.2014.6987593

6. Blume, W., Doallo, R., Eigenmann, R., Grout, J., Hoeflinger, J.P., Lawrence,
T., Lee, J., Padua, D.A., Paek, Y., Pottenger, W.M., Rauchwerger, L.,
Tu, P.: Parallel programming with polaris. Computer 29(12), 87â€“81 (1996).
https://doi.org/10.1109/2.546612, https://doi.org/10.1109/2.546612

Automated Synthesis of Asynchronizations

23

7. Bouajjani, A., Emmi, M.: Analysis of recursively parallel programs. In: Field,
J., Hicks, M. (eds.) Proceedings of the 39th ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages, POPL 2012, Philadel-
phia, Pennsylvania, USA, January 22-28, 2012. pp. 203â€“214. ACM (2012).
https://doi.org/10.1145/2103656.2103681, https://doi.org/10.1145/2103656.
2103681

8. Bouajjani, A., Emmi, M., Enea, C., Ozkan, B.K., Tasiran, S.: Verifying robust-
ness of event-driven asynchronous programs against concurrency. In: Yang, H.
(ed.) Programming Languages and Systems - 26th European Symposium on Pro-
gramming, ESOP 2017, Held as Part of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden, April 22-29,
2017, Proceedings. Lecture Notes in Computer Science, vol. 10201, pp. 170â€“
200. Springer (2017). https://doi.org/10.1007/978-3-662-54434-1 7, https://doi.
org/10.1007/978-3-662-54434-1_7

9. CernÂ´y, P., Clarke, E.M., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Samanta,
R., Tarrach, T.: From non-preemptive to preemptive scheduling using synchro-
nization synthesis. In: Kroening, D., Pasareanu, C.S. (eds.) Computer Aided Ver-
ification - 27th International Conference, CAV 2015, San Francisco, CA, USA,
July 18-24, 2015, Proceedings, Part II. Lecture Notes in Computer Science,
vol. 9207, pp. 180â€“197. Springer (2015). https://doi.org/10.1007/978-3-319-21668-
3 11, https://doi.org/10.1007/978-3-319-21668-3_11

10. CernÂ´y, P., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Tarrach, T.: Ef-
ficient synthesis for concurrency by semantics-preserving transformations. In:
Sharygina, N., Veith, H. (eds.) Computer Aided Verification - 25th Inter-
national Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.
Proceedings. Lecture Notes in Computer Science, vol. 8044, pp. 951â€“967.
Springer (2013). https://doi.org/10.1007/978-3-642-39799-8 68, https://doi.
org/10.1007/978-3-642-39799-8_68

11. CernÂ´y, P., Henzinger, T.A., Radhakrishna, A., Ryzhyk, L., Tarrach, T.: Regression-
free synthesis for concurrency.
In: Biere, A., Bloem, R. (eds.) Computer
Aided Verification - 26th International Conference, CAV 2014, Held as Part
of the Vienna Summer of Logic, VSL 2014, Vienna, Austria, July 18-22,
2014. Proceedings. Lecture Notes in Computer Science, vol. 8559, pp. 568â€“
584. Springer (2014). https://doi.org/10.1007/978-3-319-08867-9 38, https://
doi.org/10.1007/978-3-319-08867-9_38

12. Clarke, E.M., Emerson, E.A.: Design and synthesis of synchronization skeletons
using branching time temporal logic. In: Grumberg, O., Veith, H. (eds.) 25 Years of
Model Checking - History, Achievements, Perspectives. Lecture Notes in Computer
Science, vol. 5000, pp. 196â€“215. Springer (2008). https://doi.org/10.1007/978-3-
540-69850-0 12, https://doi.org/10.1007/978-3-540-69850-0_12

13. Engler, D.R., Ashcraft, K.: Racerx: effective, static detection of race con-
ditions and deadlocks.
(eds.) Proceed-
ings of the 19th ACM Symposium on Operating Systems Principles 2003,
SOSP 2003, Bolton Landing, NY, USA, October 19-22, 2003. pp. 237â€“
252. ACM (2003). https://doi.org/10.1145/945445.945468, https://doi.org/10.
1145/945445.945468

In: Scott, M.L., Peterson, L.L.

14. Flanagan, C., Freund, S.N.: Fasttrack: efficient and precise dynamic race de-
tection. In: Hind, M., Diwan, A. (eds.) Proceedings of the 2009 ACM SIG-
PLAN Conference on Programming Language Design and Implementation,
PLDI 2009, Dublin, Ireland, June 15-21, 2009. pp. 121â€“133. ACM (2009).

24

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

https://doi.org/10.1145/1542476.1542490, https://doi.org/10.1145/1542476.
1542490

15. Ganty, P., Majumdar, R.: Algorithmic

asynchronous
programs. ACM Trans. Program. Lang. Syst. 34(1),
(2012).
https://doi.org/10.1145/2160910.2160915, https://doi.org/10.1145/2160910.
2160915

of
6:1â€“6:48

verification

16. Godefroid, P., Yannakakis, M.: Analysis of boolean programs. In: Piterman, N.,
Smolka, S.A. (eds.) Tools and Algorithms for the Construction and Analysis
of Systems - 19th International Conference, TACAS 2013, Held as Part of the
European Joint Conferences on Theory and Practice of Software, ETAPS 2013,
Rome, Italy, March 16-24, 2013. Proceedings. Lecture Notes in Computer Science,
vol. 7795, pp. 214â€“229. Springer (2013). https://doi.org/10.1007/978-3-642-36742-
7 16, https://doi.org/10.1007/978-3-642-36742-7_16

17. Gokhale, S., Turcotte, A., Tip, F.: Automatic migration from synchronous to asyn-
chronous javascript apis. Proc. ACM Program. Lang. 5(OOPSLA), 1â€“27 (2021).
https://doi.org/10.1145/3485537, https://doi.org/10.1145/3485537

18. Gupta, A., Henzinger, T.A., Radhakrishna, A., Samanta, R., Tarrach, T.: Suc-
cinct representation of concurrent trace sets. In: Rajamani, S.K., Walker, D.
(eds.) Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL 2015, Mumbai, India, January
15-17, 2015. pp. 433â€“444. ACM (2015). https://doi.org/10.1145/2676726.2677008,
https://doi.org/10.1145/2676726.2677008

19. Han, H., Tseng, C.: A comparison of parallelization techniques for irregular reduc-
tions. In: Proceedings of the 15th International Parallel & Distributed Processing
Symposium (IPDPS-01), San Francisco, CA, USA, April 23-27, 2001. p. 27. IEEE
Computer Society (2001). https://doi.org/10.1109/IPDPS.2001.924963, https:
//doi.org/10.1109/IPDPS.2001.924963

time.

In: Cohen, A., Vechev, M.T.

20. Kini, D., Mathur, U., Viswanathan, M.: Dynamic race prediction in lin-
the 38th
ear
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2017, Barcelona, Spain, June 18-23, 2017. pp. 157â€“170.
ACM (2017). https://doi.org/10.1145/3062341.3062374, https://doi.org/10.
1145/3062341.3062374

(eds.) Proceedings of

21. Li, G., Lu, S., Musuvathi, M., Nath, S., Padhye, R.: Efficient scalable thread-
safety-violation detection: finding thousands of concurrency bugs during testing.
In: Brecht, T., Williamson, C. (eds.) Proceedings of the 27th ACM Symposium
on Operating Systems Principles, SOSP 2019, Huntsville, ON, Canada, October
27-30, 2019. pp. 162â€“180. ACM (2019). https://doi.org/10.1145/3341301.3359638,
https://doi.org/10.1145/3341301.3359638

22. Lin, Y., Okur, S., Dig, D.: Study and refactoring of android asynchronous pro-
gramming (T). In: Cohen, M.B., Grunske, L., Whalen, M. (eds.) 30th IEEE/ACM
International Conference on Automated Software Engineering, ASE 2015, Lincoln,
NE, USA, November 9-13, 2015. pp. 224â€“235. IEEE Computer Society (2015).
https://doi.org/10.1109/ASE.2015.50, https://doi.org/10.1109/ASE.2015.50

23. Liu, B., Huang, J.: D4:

fast concurrency debugging with parallel differen-
tial analysis. In: Foster, J.S., Grossman, D. (eds.) Proceedings of the 39th
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018. pp. 359â€“
373. ACM (2018). https://doi.org/10.1145/3192366.3192390, https://doi.org/
10.1145/3192366.3192390

Automated Synthesis of Asynchronizations

25

24. Manna, Z., Wolper, P.: Synthesis of communicating processes

from tem-
logic specifications. ACM Trans. Program. Lang. Syst. 6(1), 68â€“
poral
93 (1984). https://doi.org/10.1145/357233.357237, https://doi.org/10.1145/
357233.357237

25. Okur, S., Hartveld, D.L., Dig, D., van Deursen, A.: A study and toolkit
for asynchronous programming in c#. In: Jalote, P., Briand, L.C., van der
(eds.) 36th International Conference on Software Engineering,
Hoek, A.
ICSE â€™14, Hyderabad,
India - May 31 - June 07, 2014. pp. 1117â€“1127.
ACM (2014). https://doi.org/10.1145/2568225.2568309, https://doi.org/10.
1145/2568225.2568309

26. Raman, R., Zhao, J., Sarkar, V., Vechev, M.T., Yahav, E.: Efficient data race
detection for async-finish parallelism. In: Barringer, H., Falcone, Y., Finkbeiner,
B., Havelund, K., Lee, I., Pace, G.J., Rosu, G., Sokolsky, O., Tillmann, N.
(eds.) Runtime Verification - First International Conference, RV 2010, St. Ju-
lians, Malta, November 1-4, 2010. Proceedings. Lecture Notes in Computer Science,
vol. 6418, pp. 368â€“383. Springer (2010). https://doi.org/10.1007/978-3-642-16612-
9 28, https://doi.org/10.1007/978-3-642-16612-9_28

In: Cohen, A., Vechev, M.T.

27. Roslyn: (2021), https://github.com/dotnet/roslyn
28. Santhiar, A., Kanade, A.: Static deadlock detection for asynchronous c#
the 38th
programs.
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, PLDI 2017, Barcelona, Spain, June 18-23, 2017. pp. 292â€“305.
ACM (2017). https://doi.org/10.1145/3062341.3062361, https://doi.org/10.
1145/3062341.3062361

(eds.) Proceedings of

29. Smaragdakis, Y., Evans, J., Sadowski, C., Yi, J., Flanagan, C.: Sound predic-
tive race detection in polynomial time. In: Field, J., Hicks, M. (eds.) Proceed-
ings of the 39th ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January
22-28, 2012. pp. 387â€“400. ACM (2012). https://doi.org/10.1145/2103656.2103702,
https://doi.org/10.1145/2103656.2103702

30. Vechev, M.T., Yahav, E., Yorsh, G.:

Inferring synchronization under lim-
ited observability. In: Kowalewski, S., Philippou, A. (eds.) Tools and Algo-
rithms for the Construction and Analysis of Systems, 15th International Con-
ference, TACAS 2009, Held as Part of the Joint European Conferences on
Theory and Practice of Software, ETAPS 2009, York, UK, March 22-29,
2009. Proceedings. Lecture Notes in Computer Science, vol. 5505, pp. 139â€“
154. Springer (2009). https://doi.org/10.1007/978-3-642-00768-2 13, https://
doi.org/10.1007/978-3-642-00768-2_13

31. Vechev, M.T., Yahav, E., Yorsh, G.: Abstraction-guided synthesis of syn-
chronization. In: Hermenegildo, M.V., Palsberg, J. (eds.) Proceedings of the
37th ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, POPL 2010, Madrid, Spain, January 17-23, 2010. pp. 327â€“338.
ACM (2010). https://doi.org/10.1145/1706299.1706338, https://doi.org/10.
1145/1706299.1706338

26

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

A Formalization and Proofs of Section 3

The following lemma shows that the absence of data races implies equivalence
to the original program, in the sense of reaching the same set of conï¬gurations
(program variable valuations).
Lemma 2. ğ‘ƒğ‘[ğ¿ğ‘] is sound implies C[ğ‘ƒ [ğ¿]] = C[ğ‘ƒğ‘[ğ¿ğ‘]], for every ğ‘ƒğ‘[ğ¿ğ‘] âˆˆ
Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘]

) (i.e., the task ğ‘— is executed synchronously). If an action ( , ğ‘–,

Proof (Proof of Lemma 2). Let ğœŒ be an execution of ğ‘ƒğ‘ that reaches a conï¬g-
uration ps âˆˆ C[ğ‘ƒğ‘]. We show that actions in ğœŒ can be reordered such that any
action that occurs in ğœŒ between ( , ğ‘–, call(ğ‘—)) and ( , ğ‘—, return) is not of the form
( , ğ‘–,
) occurs
in ğœŒ between ( , ğ‘–, call(ğ‘—)) and ( , ğ‘—, return), then it must be concurrent with
(ğ‘—, return). Since ğ‘ƒğ‘ does not admit data races, an execution ğœŒâ€² resulting from ğœŒ
by reordering any two concurrent actions reaches the same conï¬guration ps as
ğœŒ. Therefore, there exists an execution ğœŒâ€²â€² where the actions that occur between
any ( , ğ‘–, call(ğ‘—)) and ( , ğ‘—, return) are not of the form ( , ğ‘–,
). This is also an
execution of ğ‘ƒ (modulo removing the awaits which have no eï¬€ect), which implies
ps âˆˆ C[ğ‘ƒ ].

B Formalization and Proofs of Section 4

ğ‘ and ğ‘ƒ 2

The following lemma shows that for a given ğ‘ƒğ‘ there exists a unique ğ‘ƒ â€²
ğ‘ that is
a maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘. The existence is implied by the
fact that strongAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘] is the bottom element of â‰¤. To prove uniqueness,
we assume by contradiction that there exist two incomparable maximal asyn-
chronizations ğ‘ƒ 1
ğ‘¤, according to
the control-ï¬‚ow of the sequential program, that is placed in diï¬€erent positions
in the two programs. Assume that s 1
ğ‘ . Then,
we move s 1
ğ‘ further away from its matching call to the same position as in
ğ‘ƒ 2
ğ‘ . This modiï¬cation does not introduce data races since ğ‘ƒ 2
ğ‘ is data race free.
Thus, the resulting program is data race free, bigger than ğ‘ƒ 1
ğ‘ , and smaller than
ğ‘ƒğ‘ w.r.t. â‰¤ contradicting the fact that ğ‘ƒ 1

ğ‘ and select the ï¬rst await statement s 1

ğ‘¤ is closer to its matching call in ğ‘ƒ 1

ğ‘ is a maximal asynchronization.

ğ‘¤ in ğ‘ƒ 1

Lemma 3. Given an asynchronization ğ‘ƒğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘], there exists a unique
program ğ‘ƒ â€²
ğ‘ that is a maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘.

Proof (Proof of Lemma 3). Since strongAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘] is the bottom element of
â‰¤, then there always exists a sound asynchronization smaller than ğ‘ƒğ‘. Assume
by contradiction that there exist two distinct programs ğ‘ƒ 1
ğ‘ that are
both maximal asynchronizations of ğ‘ƒ relative to ğ‘ƒğ‘. Let ğœŒ1 (resp., ğœŒ2) be an
execution of ğ‘ƒ 1
ğ‘ ) where every await * does not suspend the execution
of the current task, i.e., ğœŒ1 and ğœŒ2 simulate the synchronous execution of ğ‘ƒ . Let
ğ‘¤ be the statement corresponding to the ï¬rst await action in ğœŒ1 such that (1)
s 1
there exists an await action in ğœŒ2 with the corresponding await statement s 2
ğ‘¤,
ğ‘¤ match the same call in ğ‘ƒ , and Cover(s 1
such that s 1
ğ‘¤) (this

ğ‘¤) âŠ‚ Cover(s 2

ğ‘ (resp., ğ‘ƒ 2

ğ‘ and ğ‘ƒ 2

ğ‘¤ and s 2

Automated Synthesis of Asynchronizations

27

Let ğ‘ƒ 3

ğ‘¤ in ğœŒ1, there exists an await statement s 4

ğ‘ are distinct asynchronizations of the same synchronous
ğ‘¤) and Cover(s 2
ğ‘¤) must be comparable), and (2) for every
ğ‘¤ in ğ‘ƒ 1
ğ‘ that generates an await action which occurs
ğ‘¤ in ğ‘ƒ 2
ğ‘

holds because ğ‘ƒ 1
ğ‘ and ğ‘ƒ 2
program, thus Cover(s 1
other await statement s 3
before the await action of s 1
matching the same call in ğ‘ƒ , such that Cover(s 3
ğ‘ be the program obtained from ğ‘ƒ 1
ğ‘¤ down
(further away from the matching call) such that Cover(s 1
ğ‘¤). Moving
an await down can only create data races between actions that occur after the
execution of the matching call. Then, ğ‘ƒ 3
ğ‘ contains a data race iï¬€ there exists
an execution ğœŒ of ğ‘ƒ 3
ğ‘ and two concurrent actions a1 and a2 that occur between
the action ( , ğ‘–, await(ğ‘—)) generated by s 1
ğ‘¤ and the action ( , ğ‘–, call(ğ‘—)) of the call
matching s 1

ğ‘¤) = Cover(s 4
ğ‘ by moving the await s 1

ğ‘¤) = Cover(s 2

ğ‘¤).

ğ‘¤, such that:

(( , ğ‘–, call(ğ‘—)), a1) âˆˆ CO, (a1, ağ‘¤) Ì¸âˆˆ HB, (( , ğ‘–, call(ğ‘—)), a2) âˆˆ CO and (a2, ( , ğ‘–, await(ğ‘—))) âˆˆ HB

ğ‘ and ğ‘ƒ 2

where the action ağ‘¤ corresponds to the ï¬rst await action in the task ğ‘—. Let
sğ‘¤ be the statement corresponding to the action ağ‘¤. Since the only diï¬€erence
between ğ‘ƒ 3
ğ‘ is the placement of awaits then (( , ğ‘–, call(ğ‘—)), a1) âˆˆ CO and
(( , ğ‘–, call(ğ‘—)), a2) âˆˆ CO hold in any execution ğœŒâ€² of ğ‘ƒ 2
ğ‘ that contains the actions
a1 and a2. Also, note that since ağ‘¤ occurs in the task ğ‘— that the action of s 1
ğ‘¤
is waiting for. This implies that in ğœŒ1 the action of sğ‘¤ occurs before the action
of s 1
ğ‘¤ in ğœŒ1. Therefore, by the deï¬nition of s 1
ğ‘ covers the
same set of statements as the corresponding s â€²
ğ‘ that matches the same
call as sğ‘¤. Consequently, (a1, a â€²
ğ‘¤) Ì¸âˆˆ HB and (a2, ( , ğ‘–, await(ğ‘—))) âˆˆ HB hold in
any execution ğœŒâ€² of ğ‘ƒ 2
ğ‘¤ is the action of
s â€²
ğ‘¤). Thus, there exists an execution ğœŒâ€² of ğ‘ƒ 2
ğ‘ such that the actions a1 and a2 are
concurrent. This implies that if ğ‘ƒ 3
ğ‘ admits a data
race between actions generated by the same statements. As ğ‘ƒ 2
ğ‘ is data race free,
we get that ğ‘ƒ 3
ğ‘ is data race free as well. Since ğ‘ƒ 1
ğ‘ is not
maximal, which contradicts the hypothesis.

ğ‘ that contains the actions a1 and a2 (a â€²

ğ‘ admits a data race, then ğ‘ƒ 2

ğ‘¤ we have that sğ‘¤ in ğ‘ƒ 1

ğ‘ , we get that ğ‘ƒ 1

ğ‘¤ in ğ‘ƒ 2

ğ‘ < ğ‘ƒ 3

The complexity analysis also relies on a property of the maximal asynchro-
nization relative to an immediate predecessor: if the predecessor is deï¬ned by
moving an await s â€²â€²
ğ‘¤, then the maximal asynchronization is obtained by moving
only awaits smaller than s â€²â€²

ğ‘¤ w.r.t. â‰ºğ‘¤.

Lemma 4. If ğ‘ƒ â€²â€²
which is defined by moving an await s â€²â€²
chronization relative to ğ‘ƒ â€²â€²
w.r.t. â‰ºğ‘¤.

ğ‘ is an immediate predecessor of a sound asynchronization ğ‘ƒ â€²
ğ‘,
ğ‘ up, then the maximal sound asyn-
ğ‘ is obtained by moving only awaits smaller than s â€²â€²
ğ‘¤

ğ‘¤ in ğ‘ƒ â€²

Proof (Proof of Lemma 4). Moving an await up in ğ‘ƒ â€²
ğ‘ can only create data
races between actions that occur after the execution of this await (because the
invocation is suspended earlier). The only possible repairs of these data races
ğ‘¤ down which results in ğ‘ƒ â€²
consists in either moving s â€²â€²
ğ‘ or moving up some other
awaits that occur in methods that (indirectly) call the method in which s â€²â€²
ğ‘¤
occurs. The ï¬rst case is not applicable because it gives a program that is not

28

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

ğ‘ . In the second case, every await s â€²

smaller than ğ‘ƒ â€²â€²
a method that (indirectly) calls the method in which s â€²â€²
ğ‘¤ is smaller than s â€²â€²
s â€²

ğ‘¤ w.r.t. â‰ºğ‘¤.

ğ‘¤ that is moved up occurs in
ğ‘¤ occurs, and therefore,

Before giving the proof of Theorem 1, we note that the total order relation â‰ºğ‘¤
between awaits is ï¬xed throughout the recursion of AsySyn and it corresponds to
the order of the awaits in the weakest asynchronization of ğ‘ƒ , i.e., wkAsy[ğ‘ƒ, ğ¿, ğ¿ğ‘].
This is because the order between awaits in the same method might change from
one asynchronization to another in Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘]. If the control-ï¬‚ow graph of a
method contains branches, it is possible to replace all await statements matching
sğ‘ that are reachable in the CFG from s with a single await statement sğ‘¤, in
this case sğ‘¤ is ordered before any other await that one of the awaits that sğ‘¤
replaces is ordered before and is ordered after any await that all the awaits
that sğ‘¤ replaces are ordered before. Also, it is possible to add additional awaits
statements in branches, in this case derive a total order between these awaits
and order the awaits before or after any other await that the original await was
ordered before or after, respectively.

Proof (Proof of Theorem 1). Let ğ‘ƒğ‘ be the weakest asynchronization of ğ‘ƒ , then
the set of all sound asynchronizations of ğ‘ƒ is ğ’œ = {ğ‘ƒ â€²â€²
ğ‘ â‰¤
ğ‘ƒ â€²
ğ‘}, where ğ‘ƒ â€²
ğ‘ is the maximal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘. It is clear
that every asynchronization outputted by AsySyn(ğ‘ƒğ‘, s 0

ğ‘ ] = C[ğ‘ƒ ] and ğ‘ƒ â€²â€²

ğ‘¤) is in the set ğ’œ.

ğ‘ : C[ğ‘ƒ â€²â€²

Let ğ‘ƒ 0

ğ‘ = ğ‘ƒ â€²

ğ‘ = ğ‘ƒ 1

ğ‘ or ğ‘ƒ 0

ğ‘ = ğ‘ƒ 1â€²

ğ‘¤). Since ğ‘ƒ 1

ğ‘¤). Then, let (ğ‘ƒ 1

ğ‘ . We have that either ğ‘ƒ 0

ğ‘¤ ) âŠ‚ Cover(s 1
ğ‘ or ğ‘ƒ 0

ğ‘ be a sound asynchronization of ğ‘ƒ [ğ¿] w.r.t. ğ¿ğ‘, i.e., ğ‘ƒ 0

ğ‘¤ ) âŠ‚ Cover(s 2
ğ‘¤, then Lemma 4 implies ğ‘ƒ 1â€²

ğ‘ is in AsySyn(ğ‘ƒğ‘, sğ‘¤). For the second case: let s 1
ğ‘ w.r.t. â‰ºğ‘¤ that matches the same call as s 1â€²
ğ‘¤) âˆˆ ImPred(ğ‘ƒ â€²

ğ‘ , s 1
ğ‘ . The ï¬st case implies that ğ‘ƒ 0
ğ‘ = MaxRel(ğ‘ƒ 1
ğ‘ ) then either ğ‘ƒ 0
ğ‘ is in AsySyn(ğ‘ƒğ‘, sğ‘¤). For the second case: let s 2
ğ‘ w.r.t. â‰ºğ‘¤ that matches the same call as s 2â€²
ğ‘ is an immediate successor of ğ‘ƒ â€²

ğ‘ âˆˆ ğ’œ. We will
ğ‘ < ğ‘ƒ â€²
ğ‘. The
ğ‘¤ be
ğ‘¤ in ğ‘ƒ 0
ğ‘
ğ‘, sğ‘¤). We obtain that
ğ‘ is in AsySyn(ğ‘ƒğ‘, sğ‘¤).
ğ‘ < ğ‘ƒ 1â€²
ğ‘ or ğ‘ƒ 0
ğ‘ .
ğ‘¤ be
ğ‘¤ in ğ‘ƒ 0
ğ‘
ğ‘ by moving
is obtained by moving only awaits
ğ‘¤ or s 2
ğ‘¤ â‰ºğ‘¤ s 1
ğ‘¤. Thus,
ğ‘ or ğ‘ƒ 0
ğ‘ = ğ‘ƒ 2
ğ‘ < ğ‘ƒ 2
ğ‘ .
ğ‘ . Thus, Asy-
ğ‘ = ğ‘ƒ 0

show that AsySyn outputs ğ‘ƒ 0
ï¬rst case implies that ğ‘ƒ 0
the maximum element in ğ‘ƒ â€²
s.t. Cover(s 1â€²
either ğ‘ƒ 0
ğ‘ < ğ‘ƒ 1
For the second case: let ğ‘ƒ 1â€²
The ï¬st case implies that ğ‘ƒ 0
the maximum element in ğ‘ƒ 1â€²
s.t. Cover(s 2â€²
the await s 1
ğ‘¤ = s 1
smaller than s 1
(ğ‘ƒ 2
ğ‘¤). We then obtain that either ğ‘ƒ 0
Then, we repeat the above proof process until we obtain ğ‘ƒ ğ‘›
Syn outputs ğ‘ƒ 0
ğ‘ .
ğ‘¤ and s 2
(ğ‘ƒ 1
ğ‘¤), (ğ‘ƒ 2
ğ‘ , s 2
MaxRel(ğ‘ƒ 2
ğ‘ ) is obtained by moving only awaits smaller than s 2
Thus, in ğ‘ƒ 2â€²
ğ‘
MaxRel(ğ‘ƒ 1
ğ‘ƒ 1â€²â€²
ğ‘
we have that the two programs are distinct since in ğ‘ƒ 2â€²â€²
ğ‘

ğ‘¤ and
ğ‘ =
ğ‘¤ w.r.t. â‰ºğ‘¤.
ğ‘. Then, ğ‘ƒ 1â€²
ğ‘ =
and ğ‘ƒ 2â€²â€²
s.t.
ğ‘
ğ‘ , s 2
ğ‘¤)),
ğ‘¤ is in the

ğ‘¤ be two distinct await statements in ğ‘ƒ â€²
ğ‘¤) âˆˆ ImPred(ğ‘ƒ â€²

ğ‘¤ is in the same position as in ğ‘ƒ â€²
ğ‘

ğ‘ . For any two programs ğ‘ƒ 1â€²â€²
ğ‘ , s 1

ğ‘, sğ‘¤). Similar to before then we have that ğ‘ƒ 2â€²

ğ‘¤ w.r.t. â‰ºğ‘¤. Then, we either have s 2

ğ‘ ) is outputted by AsySyn(ğ‘ƒ 1â€²

ğ‘ ) is diï¬€erent than ğ‘ƒ 2â€²

ğ‘¤) (resp., AsySyn(ğ‘ƒ 2â€²

ğ‘¤) âˆˆ ImPred(ğ‘ƒ 1â€²

Let s 1
ğ‘ , s 1

the await s 1

(resp., ğ‘ƒ 2â€²â€²

the await s 1

ğ‘¤ â‰ºğ‘¤ s 1

ğ‘ s.t. s 2

ğ‘ , s 1

ğ‘ , s 2

ğ‘

Automated Synthesis of Asynchronizations

29

same position as in ğ‘ƒ â€²
only once.

ğ‘. Thus, we get that AsySyn outputs every element of ğ’œ

C Formalization and Proofs of Section 5

The following lemma proves that for any unsound asyn-
chronization, any trace with a data race contains at least one
data race that involves two actions that are synchronously
reachable. For instance, the program in Fig. 11 has two data
races, one between ğ‘¥ = 1 and ğ‘Ÿ4 = ğ‘¥ and the other be-
tween ğ‘¦ = 2 and ğ‘Ÿ5 = ğ‘¦. However, the statement ğ‘¦ = 2
is not reachable in the corresponding synchronous program.
It is reachable in this asynchronization because of the data
race between ğ‘¥ = 1 and ğ‘Ÿ4 = ğ‘¥, which are both reachable
in the synchronous program. Eliminating the latter data race
by moving the statement await ğ‘Ÿ1 before ğ‘¥ = 1, makes ğ‘¦ = 2
unreachable and the data race between ğ‘¦ = 2 and ğ‘Ÿ5 = ğ‘¦ is
also eliminated.

async method Main {

r1 = call m;
x = 1;
await r1;

}
async method m {
r2 = call m1;
r3 = call m1;
await r2;
r4 = x;
if r4 == 1
y = 2;
await r3;

}
async method m1 {

await *;
r5 = y;
return;

}
Fig. 11

Lemma 5. An asynchronization ğ‘ƒğ‘[ğ¿ğ‘] is sound iff it does
not admit data races between actions that are synchronously reachable.

Proof (Proof of Lemma 5). Assume by contradiction that ğ‘ƒğ‘[ğ¿ğ‘] is sound and
it admits a data race (a1, a2) in a trace ğœ âˆˆ Tr(ğ‘ƒğ‘[ğ¿ğ‘]) where one of the actions,
say a1, is not synchronously reachable. We assume w.l.o.g that the data race
(a1, a2) is the ï¬rst that occurs in ğœ with at least one synchronously unreachable
action. Then, there must exist a read access ağ‘Ÿ that enabled a1, and therefore,
ağ‘Ÿ reads a value that was not read in any synchronous execution. Thus, the read
value must the result of another data race that occurs earlier in the trace ğœ ,
which is a contradiction.

async method Main {

r1 = call m;
r2 = x;
x = r2 + 1;
await r1;

In Fig. 12, we explain how the repairing data races based
on the partial order relating data races allows to avoid su-
perï¬‚uous repair steps. For instance, in Fig. 12, the ï¬rst data
race to repair involves the read of x from Main and the write
to x in m, because these statements are the ï¬rst to execute in
the original sequential program among the other statements
involved in data races. Repairing this data race consists in
moving await r1 before the read of x from Main, which im-
plies that m completes before the read of x. This repair is
deï¬ned from a notion of root cause of a data race, that in
this case, contains the call to m and the read of x from Main. Interestingly, this
repair step removes the write-write data race between the write to x in Main
and the write to x in m as well. If we would have repaired these data races in the
opposite order, we would have moved await t1 ï¬rst before the write to x, and
then, before the read of x.

}
async method m {

await *;
x = 2;
return;

}
Fig. 12

30

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

In Fig. 13, we give a non-deterministic program where two
statements of the program can be executed in diï¬€erent orders in
diï¬€erent executions. In particular, the statements r1 = x and r2
= y of the program can be executed in diï¬€erent orders depending
on the number of loop iterations and whether the if branch is
entered during the ï¬rst loop iteration.

method Main {
while *
if *

r1 = x;

r2 = y;

}

Fig. 13

For the program in Fig. 14, we have the following order
between data races: (x = input, r2 = x) â‰ºSO(retVal = x,
x = r2 + 1) because r2 = x is executed before the write x
= r2 + 1 in the original synchronous program (for simplic-
ity we use statements instead of actions). However, the data
races (x = input, r2 = x) and (x = input, r3 = x) are in-
comparable.

async method Main {

r1 = call m;
if *

r2 = x;
x = r2 + 1;

else

r3 = x;
await r1;

}
async method m {

The following lemma identiï¬es a suï¬ƒcient transformation
for repairing a data race (a1, a2): moving the await sğ‘¤ gener-
ating the action ağ‘¤ just before the statement s generating a.
This is suï¬ƒcient because it ensures that every statement that
follows LCACO(a1, a2)10 in call order will be executed before a
and before any statement which succeeds a in call order, including a2. Note that
moving the await ağ‘¤ anywhere after a will not aï¬€ect the concurrency between
a1 and a2.

await *
retVal = x;
x = input;
return;

}
Fig. 14

Lemma 6. Let (a1, a2) be a data race in a trace ğœ of an asynchronization ğ‘ƒğ‘,
and ağ‘ = (ğ‘–, call(ğ‘—)) = LCACO(a1, a2). Then, ğœ contains a unique action ağ‘¤ =
(ğ‘–, await(ğ‘—)) and a unique action a such that:

â€“ (a, ağ‘¤) âˆˆ MO, and a is the latest action in the method order MO such that
(ağ‘, a) âˆˆ MO and (a, a2) âˆˆ CO* (CO* denotes the reflexive closure of CO).
Proof (Proof of Lemma 6). Let ğœŒ be the execution of the trace ğœ . By deï¬nition,
ğœŒ ends with a conï¬guration where the call stack and the set of pending tasks are
empty. Therefore, ğœŒ contains an action ağ‘¤ = ( , ğ‘–, await(ğ‘—)) matching ğ‘ğ‘ which is
unique by the deï¬nition of the semantics. Since (ağ‘, a1) âˆˆ CO and (ağ‘, a2) âˆˆ CO
then either ağ‘ and a2 occur in the same method, or there exists a call action a â€² in
the same task as ağ‘ such that (a â€², a2) âˆˆ CO. Then, we deï¬ne a = a2 in the ï¬rst
case, and a as the latest action in the same task as ağ‘ such that (a, a2) âˆˆ CO in
the second case. We have that (a, ağ‘¤) âˆˆ MO because otherwise, (ağ‘¤, a) âˆˆ MO
and (a, a2) âˆˆ CO* implies that (a1, a2) âˆˆ HB (because (a1, ağ‘¤) âˆˆ HB, and MO
and CO are included in HB), and this contradicts a1 and a2 being concurrent.

When the control-ï¬‚ow graph of the method contains branches, the construc-
tion of RDR(ğ‘ƒğ‘, sğ‘, s) involves (1) replacing all await statements matching sğ‘
that are reachable in the CFG from s with a single await statement placed just
before s, and (2) adding additional await statements in branches that â€œconï¬‚ictâ€
with the branch containing s. This is to ensure the syntactic constraints described
in Section 2. These additional await statements are at maximal distance from
the corresponding call statement because of the maximality requirement.

10 We abuse the terminology and make no distinction between statements and actions.

Automated Synthesis of Asynchronizations

31

async method Main {

async method Main {

For instance, to repair the data race
between r2 = x and x = input in the
program on the left of Fig. 15, the
statement await r1 must be moved be-
fore r2 = x in the if branch, which
implies that another await must be
added on the else branch. The result
is given on the right of Fig. 15.

r1 = call m;
if *

r2 = x;

else

r3 = y;

await r1;

}
async method m {

The following lemma shows that
repairing a minimal data race can-
not introduce smaller data races (w.r.t.
â‰ºSO), which ensures some form of
monotonicity when repairing minimal data races iteratively.

await *
retVal = x;
x = input;
return;

r1 = call m;
if *

await r1;
r2 = x;

else

r3 = y;
await r1;

}
async method m {

await *
retVal = x;
x = input;
return;

}
}
Fig. 15: Examples of asynchronizations.

Lemma 7. Let ğ‘ƒğ‘ be an asynchronization, (a1, a2) a data race in ğ‘ƒğ‘ that is
minimal w.r.t. â‰ºSO, and (sğ‘, s) the root cause of (a1, a2). Then, RDR(ğ‘ƒğ‘, sğ‘, s)
does not admit a data race that is smaller than (a1, a2) w.r.t. â‰ºSO.

Proof of Lemma 7. The only
modiï¬cation in the program ğ‘ƒ â€²
ğ‘ =
RDR(ğ‘ƒğ‘, sğ‘, s) compared to ğ‘ƒğ‘ is the
movement of the await sğ‘¤ matching the
call sğ‘ to be before the statement s in a
method ğ‘š. The concurrency added in ğ‘ƒ â€²
ğ‘
that was not possible in ğ‘ƒğ‘ is between ac-
tions (a â€², a â€²â€²) generated by statements s â€²
and s â€²â€², respectively, as shown in Fig. 16.
W.l.o.g., we assume that (a â€², a â€²â€²) âˆˆ SO.
The statements s1 and s2 are those gen-
erating a1 and a2, respectively. The state-
ment s â€² is related by CO* to some statement in ğ‘š that follows s, and s â€²â€² is
related by CO* to some statement that follows the call to ğ‘š in the caller of ğ‘š.
Note that s â€² is ordered by â‰º after s2. Since (a1, a2) âˆˆ SO and (a â€², a â€²â€²) âˆˆ SO
then s2 â‰º s â€²â€² and s1 â‰º s â€². Thus, any new data race (a â€², a â€²â€²) in ğ‘ƒ â€²
ğ‘ that was not
(cid:3)
reachable in ğ‘ƒğ‘ is bigger than (a1, a2).

Fig. 16: An excerpt of an asyn-
chronous program.

Theorem 7. Given an asynchronization ğ‘ƒğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘], MaxRel(ğ‘ƒğ‘) re-
turns the optimal asynchronization of ğ‘ƒ relative to ğ‘ƒğ‘.

Proof (Proof of Theorem 7). Since the recursive calls RCMinDR ï¬nd all data
races between synchronously reachable actions then the output ğ‘ƒ â€²
is sound and therefore it is equivalent to ğ‘ƒ (Lemma 2 and Lemma 5). Now we
need to show that any successor ğ‘ƒ 1
ğ‘ of ğ‘ƒ â€²
ğ‘ that is also smaller than ğ‘ƒğ‘ (w.r.t.
â‰¤) admits data races. Let sğ‘¤ be the biggest await statement w.r.t. â‰ºğ‘¤ whose
position in ğ‘ƒ 1
ğ‘ (moved down). Since
ğ‘ â‰¤ ğ‘ƒğ‘, then sğ‘¤ was also moved up by the procedure MaxRel with respect
ğ‘ƒ 1
to its position in ğ‘ƒğ‘ to ï¬x some data race (a1, a2). Let ğ‘š be the method ğ‘š that

ğ‘ is changed with respect to its position in ğ‘ƒ â€²

ğ‘ = MaxRel(ğ‘ƒğ‘)

        async method mâ€™ {                              râ€™ = call m;                                                                                            sâ€™â€™               await râ€™;           }        async method m {                            sc: r = call _ ;                  s1                   s: ...                     s2                                                                                     sâ€™              sw: await r;           }COCO*CO*CO*32

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

ğ‘ as well. ğ‘ƒ 1

contains sğ‘¤ and sğ‘ be the matching call. We will now show that (a1, a2) forms a
data race in ğ‘ƒ 1
ğ‘ has an execution ğœŒ that reaches both a1 and a2 (since
Ex(ğ‘ƒ 1
ğ‘ ) includes the synchronous execution where all await * are interpreted as
skip which reaches a1 and a2). Since every other await s â€²
ğ‘ that occurs in a
method ğ‘šâ€² (in)directly called by ğ‘š (including the method associated with the
call sğ‘) is in the same position as in ğ‘ƒ â€²
ğ‘, then the two actions a1 and a2 are not
related by HB and are concurrent. Thus, (a1, a2) forms a data race in ğ‘ƒ 1
ğ‘ , which
concludes the proof.

ğ‘¤ in ğ‘ƒ 1

The fact that data races are enumerated in the order deï¬ned by â‰ºSO guaran-
tees a bound on the number of times an await matching the same call is moved
during the execution of MaxRel(ğ‘ƒğ‘). In general, this bound is the number of
statements covered by all the awaits matching the call in the input program
ğ‘ƒğ‘. Actually, this is a rather coarse bound. A more reï¬ned analysis has to take
into account the number of branches in the CFGs. For programs without con-
ditionals or loops, every await is moved at most once during the execution of
MaxRel(ğ‘ƒğ‘). In the presence of branches, a call to an asynchronous method
may match multiple await statements (one for each CFG path starting from
the call), and the data races that these await statements may create may be
incomparable w.r.t. â‰ºSO. Therefore, for a call statement sğ‘, let |sğ‘| be the sum
of |Cover(sğ‘¤)| for every await sğ‘¤ matching sğ‘ in ğ‘ƒğ‘.
Lemma 8. For any asynchronization ğ‘ƒğ‘ âˆˆ Asy[ğ‘ƒ, ğ¿, ğ¿ğ‘] and call statement sğ‘
in ğ‘ƒğ‘, the while loop in MaxRel(ğ‘ƒğ‘) does at most |sğ‘| iterations that result in
moving an await matching sğ‘.
Proof (Proof of Lemma 8). We consider ï¬rst the case without conditionals or
loops, and we show by contradiction that every await statement sğ‘¤ is moved
at most once during the execution of MaxRel(ğ‘ƒğ‘), i.e., there exists at most
one iteration of the while loop which changes the position of sğ‘¤. Suppose that
the contrary holds for an await sğ‘¤. Let (a1, a2), and (a3, a4) be the data races
repaired by the ï¬rst and second moves of sğ‘¤, respectively. By Lemma 6, there
exist two actions a and a â€² such that
(ağ‘, a) âˆˆ MO, (a, a2) âˆˆ CO*, (a, ağ‘¤) âˆˆ MO and (ağ‘, a â€²) âˆˆ MO, (a â€², a4) âˆˆ CO*, (a â€², ağ‘¤) âˆˆ MO
where ağ‘¤ = ( , ğ‘–, await(ğ‘—)) and ağ‘ = ( , ğ‘–, call(ğ‘—)) are the asynchronous call ac-
tion and the matching await action. Let s2 and s4 be the statements generating
the two actions a2 and a4, respectively. Then, we have either s2 â‰º s4 or s2 = s4,
and both cases imply that (a, a â€²) âˆˆ MO*. Thus, moving the await statement gen-
erating ağ‘¤ before the statement generating a implies that it is also placed before
the statement generating a â€² (that occurs after a in the same method). Thus, the
ï¬rst move of the await sğ‘¤ repaired both data races, which is contradiction.

In the presence of conditionals or loops, moving an await up in one branch
may correspond to adding multiple awaits in the other conï¬‚icting branches. Also,
one call in the program may correspond to multiple awaits on diï¬€erent branches.
However, every repair of a data race consists in moving one await closer to the
matching call sğ‘ and before one more statement covered by some await matching
sğ‘ in the input ğ‘ƒğ‘.

Automated Synthesis of Asynchronizations

33

1
2
3
4
5

7
8
9
10

Add before s1:
if ( lastTaskDelayed == âŠ¥ && * )
lastTaskDelayed := myTaskId();
DescendantDidAwait := thisHasDoneAwait;
return

Add before s2:

if ( task_sğ‘ == myTaskId() )

s := s2;

assert (lastTaskDelayed == âŠ¥ ||

!DescendantDidAwait);

13
14
15
16

17
18
19
20

22
23
24

26
27
28
29

Replace every statement â€˜â€˜await râ€™â€™ with:

if( r == lastTaskDelayed ) then

if ( !DescendantDidAwait )
DescendantDidAwait :=
thisHasDoneAwait;

lastTaskDelayed := myTaskId();
return

else

thisHasDoneAwait := true

Add before every statement â€˜â€˜r := call mâ€™â€™:

if ( task_sğ‘ == myTaskId() ) then

s := this statement;

Add after every statement â€˜â€˜r := call mâ€™â€™:

if ( r == lastTaskDelayed )

sğ‘ := this statement;
task_sğ‘ := myTaskId();

Fig. 17: A program instrumentation for computing the root cause of a minimal
data race between the statements s1 and s2 (if any). All variables except for
thisHasDoneAwait are program (global) variables. thisHasDoneAwait is a local
variable. The value âŠ¥ represents an initial value of a variable. The variables
sğ‘ and s store the (program counters of the) statements representing the root
cause. The method myTaskId returns the id of the current task.

D Computing Root Causes of Minimal Data Races

We present a reduction from the problem of computing root causes of mini-
mal data races to reachability (assertion checking) in sequential programs. This
reduction builds on a program instrumentation for checking if there exists a
minimal data race that involves two given statements (s1, s2) that are reachable
in an execution of the original synchronous program, whose correctness relies on
the assumption that another pair of statements cannot produce a smaller data
race. This instrumentation is used in an iterative process where pairs of state-
ments are enumerated according to the colexicographic order induced by â‰º. This
speciï¬c enumeration ensures that the assumption made for the correctness of the
instrumentation is satisï¬ed.

Given an asynchronization ğ‘ƒğ‘, the instrumentation described in Fig. 17 rep-
resents a synchronous program where all await statements are replaced with
synchronous code (lines 14â€“20). This instrumentation simulates asynchronous
executions of ğ‘ƒğ‘ where methods may be only partially executed, modeling await
interruptions. It reaches an error state (see the assert at line 10) when an action
generated by s1 is concurrent with an action generated by s2, which represents
a data race, provided that s1 and s2 access a common program variable (these
statements are assumed to be given as input). Also, the values of sğ‘ and s when
reaching the assertion violation represent the root-cause of this data race.

The instrumentation simulates an execution of ğ‘ƒğ‘ to search for a data race

as follows (we discuss the identiï¬cation of the root-cause afterwards):

â€“ It executes under the synchronous semantics until an instance of s1 is non-
deterministically chosen as a candidate for the ï¬rst action in the data race
(s1 can execute multiple times if it is included in a loop for instance). The

34

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

current invocation is interrupted when it is about to execute this instance
of s1 and its task id ğ‘¡0 is stored into lastTaskDelayed (see lines 2â€“5).

â€“ Every invocation that transitively called ğ‘¡0 is interrupted when an await for
an invocation in this call chain (whose task id is stored into lastTaskDelayed)
would have been executed in the asynchronization ğ‘ƒğ‘ (see line 18).

â€“ Every other method invocation is executed until completion as in the syn-

chronous semantics.

â€“ When reaching s2, if s1 has already been executed (lastTaskDelayed is not
âŠ¥) and at least one invocation has only partially been executed, which is
recorded in the boolean ï¬‚ag DescendantDidAwait and which means that
s1 is concurrent with s2, then the instrumentation stops with an assertion
violation.

A subtle point is that the instrumentation may execute code that follows an
await ğ‘Ÿ even if the task ğ‘Ÿ has been executed only partially, which would not
happen in an execution of the original ğ‘ƒğ‘. Here, we rely on the assumption that
there exist no data race between that code and the rest of the task ğ‘Ÿ. Such
data races would necessarily involve two statements which are before s2 w.r.t.
â‰º. Therefore, the instrumentation is correct only if it is applied by enumerating
pairs of statements (s1, s2) w.r.t. the colexicographic order induced by â‰º.

Next, we describe the computation of the root-cause, i.e., the updates on the
variables sğ‘ and s. By deï¬nition, the statement sğ‘ in the root-cause should be a
call that makes an invocation that is in the call stack when s1 is reached. This
can be checked using the variable lastTaskDelayed that stores the id of the
last such invocation popped from the call stack (see the test at line 27). The
statement s in the root-cause can be any call statement that has been executed
in the same task as sğ‘ (see the test at line 23), or s2 itself (see line 9).

Let [[ğ‘ƒğ‘, s1, s2]] denote the instrumentation in Fig. 17. We say that the values
of sğ‘ and s when reaching the assertion violation are the root cause computed
by this instrumentation. The following theorem states its correctness.

Theorem 8. If [[ğ‘ƒğ‘, s1, s2]] reaches an assertion violation, then it computes the
root cause of a minimal data race, or there exists (s3, s4) such that [[ğ‘ƒğ‘, s3, s4]]
reaches an assertion violation and (s3, s4) is before (s1, s2) in colexicographic
order w.r.t. â‰º.

Based on Theorem 8, we deï¬ne an implementation of the procedure RCMinDR(ğ‘ƒğ‘)

used in computing maximal asynchronizations (Algorithm 2) as follows:

â€“ For all pairs of read or write statements (s1, s2) in colexicographic order w.r.t.
â‰º that are reachable in an execution of the original synchronous program
ğ‘ƒ .

âˆ™ If [[ğ‘ƒğ‘, s1, s2]] reaches an assertion violation, then
âˆ— return the root cause computed by [[ğ‘ƒğ‘, s1, s2]]

â€“ return âŠ¥

Checking whether read or write statements are reachable can be determined
using a linear number of reachability queries in the synchronous program ğ‘ƒ . Also,

Automated Synthesis of Asynchronizations

35

the order â‰º between read or write statements can be computed using a quadratic
number of reachability queries in the synchronous program ğ‘ƒ . Therefore, s â‰º s â€²
iï¬€ an instrumentation of ğ‘ƒ that sets a ï¬‚ag when executing s and asserts that this
ï¬‚ag is not set when executing s â€² reaches an assertion violation. The following
theorem states the correctness of the procedure above.

Theorem 9. RCMinDR(ğ‘ƒğ‘) returns the root cause of a minimal data race of
ğ‘ƒğ‘ w.r.t. â‰ºSO, or âŠ¥ if ğ‘ƒ â€²

ğ‘ is data race free.

E Formalization and Proofs of Section 6

Theorem 10. Checking whether there exists a sound asynchronization different
from the strong asynchronization is PSPACE-complete.

Proof (Proof of Theorem 10). (1) deï¬ne a new method ğ‘š that writes to a new
program variable ğ‘¥, and insert a call to ğ‘š followed by a write to ğ‘¥ at location â„“,
and (2) insert a write to ğ‘¥ after every call statement that calls a method in {ğ‘šâ€²}*,
where ğ‘šâ€² is the method containing â„“. Let ğ‘šğ‘ be an asynchronous version of ğ‘š
obtained by inserting an await * at the beginning. Then, â„“ is reachable in ğ‘ƒ iï¬€
the only sound asynchronization of ğ‘ƒ â€² w.r.t. {ğ‘šğ‘} is the strong asynchronization.

F Formalization and Proofs of Section 7

The MaxRel# procedure repairs data races in an order which is â‰ºSO with
some exceptions that do not aï¬€ect optimality, i.e., the number of times an await
matching the same call can be moved. For instance, if a method ğ‘š calls two other
methods ğ‘š1 and ğ‘š2 in this order, the procedure above may handle ğ‘š2 before ğ‘š1,
i.e., repair data races between actions that originate from ğ‘š2 before data races
that originate from ğ‘š1, although the former are bigger than the latter in â‰ºSO.
This does not aï¬€ect optimality because those repairs are â€œindependentâ€, i.e., any
repair in ğ‘š2 cannot inï¬‚uence a repair in ğ‘š1, and vice-versa. The crucial point
is that this procedure repairs data races between actions that originate from a
method ğ‘š before data races that involve actions in methods preceding ğ‘š in the
call graph, which are bigger in â‰ºSO than the former.

Note that MaxRel# procedure which is based on the bottom-up inter-
procedural data-ï¬‚ow analysis compromises precision to reduce the complexity of
the problem from undecidable in general or PSPACE-complete with ï¬nite data
to polynomial time. However, because of this imprecision, certain await state-
ments may be moved closer to the matching call unnecessarily. For instance, in
Fig. 11, the precise algorithm (using the procedure MaxRel in Algorithm 2)
will only repair the data race on ğ‘¥ because doing so, the potential data race on
ğ‘¦ will become unreachable. On the other hand, the polynomial-time algorithm
(using the MaxRel# procedure) will also repair the data race on ğ‘¦, moving
another await closer to the matching call, since it cannot reason about data (one
statement of this data race is only reachable if the variable ğ‘Ÿ4 is 1).

36

S.M. Beillahi, A. Bouajjani, C. Enea, and S. Lahiri.

1 void Main() {
2 F();

1 async Task MainAsync() {
2 Task t1 = F();

4 x = 2;
5 await t1;
6 }

1 void Main() {
2 Thread thr1 = new Thread(F);
3 thr1.Start();
4 x = 2;
5 thr1.Join();
6 }

8 async Task F() {
9 Task t2 = IOAsync();

8 void F() {
9 Thread thr2 = new Thread(IO);

11 x = 1;
12 await t2;
13 }

10 thr2.Start();
11 x = 1;
12 thr2.Join();
13 }

4 x = 2;

6 }

8 void F() {
9 IO();

11 x = 1;

13 }

Fig. 18: A synchronous C# program, an asynchronization, and a multi-threaded
refactoring.

G Multi-threaded Refactorings

We discuss an extension of our framework to multi-threaded refactorings that
rewrite a sequential program into a multi-threaded program where every method
invocation is executed on a diï¬€erent thread. A caller can wait for a callee to
complete using a join primitive. A start primitive for spawning a new thread
is the counterpart of an asynchronous call while join is the counterpart of await.
For instance, Fig. 18 lists a sequential program, a possible asynchonization, and
a multi-thread refactoring (both refactorings place the awaits/joins as far away
as possible from the calls).

An important diï¬€erence between start/join and async/await is the happens-
before order relation. For instance, the asynchronization on the center of Fig. 18
assigns 1 to x (line 11) before it assigns 2 to x (line 4), as in the original sequential
program. However, the multi-thread program on the right of Fig. 18 may execute
these two assignments in any order, and admits a behavior that is not possible
in the sequential program (assigning 2 before assigning 1). Repairing this data-
race consists in moving the join at line 5 to occur before assigning 2 to x at
line 4. In general, the happens-before order is weaker compared to an analogous
asynchronization, where awaits are placed as the joins, which implies that any
multi-threaded refactoring can be rewritten to an asynchronization. The vice-
versa may not be possible as shown in this example.

Despite this diï¬€erence, it can still be proved that there exists a unique multi-
threaded refactoring that is sound, i.e., does not admit data races, and max-
imal, i.e., maximizes the distance between start and join, a result similar to
Lemma 3. Assuming by contradiction the existence of two incomparable maxi-
mal and sound refactorings, one can show that moving a join in one refactoring
further away from the matching call as in the other refactoring does not in-
troduce data races (contradicting optimality). To compute maximal and sound
multi-threaded refactorings, one can apply the same iterative process of repairing
data-races (the happens-before reï¬‚ects multi-threading instead of async/await),
prioritizing data races involving statements that would execute ï¬rst in the se-
quential program. The repairing of a data-race is similar and consists in moving
a join up.

Automated Synthesis of Asynchronizations

37

In contrast to async/await, moving a join up does not introduce new data
races (since no new parallelism is introduced). This implies that all the predeces-
sors of a sound multi-threaded refactoring are also sound, i.e., the set of sound
multi-threaded refactorings is downward closed.

