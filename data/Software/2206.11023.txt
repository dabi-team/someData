2
2
0
2

n
u
J

0
3

]
E
S
.
s
c
[

2
v
3
2
0
1
1
.
6
0
2
2
:
v
i
X
r
a

Heterogeneous Graph Neural Networks for Software Effort
Estimation

Hung Phan
Iowa State University
Ames, Iowa, USA
hungphd@iastate.edu

Ali Jannesari
Iowa State University
Ames, Iowa, USA
jannesar@iastate.edu

ABSTRACT
Background. Software effort can be measured by story point [35].
Story point estimation is important in software projects’ planning.
Current approaches for automatically estimating story points focus
on applying pre-trained embedding models and deep learning for
text regression to solve this problem. These approaches require
expensive embedding models and confront challenges that the se-
quence of text might not be an efficient representation for software
issues which can be the combination of text and code.

Aims. We propose HeteroSP, a tool for estimating story points
from textual input of Agile software project issues. We select GPT2SP
[12] and Deep-SE [8] as the baselines for comparison.

Method. First, from the analysis of the story point dataset [8],
we conclude that software issues are actually a mixture of natural
language sentences with quoted code snippets and have problems
related to large-size vocabulary. Second, we provide a module to
normalize the input text including words and code tokens of the
software issues. Third, we design an algorithm to convert an input
software issue to a graph with different types of nodes and edges.
Fourth, we construct a heterogeneous graph neural networks model
with the support of fastText [6] for constructing initial node em-
bedding to learn and predict the story points of new issues.

Results. We did the comparison over three scenarios of estima-
tion, including within project, cross-project within the repository,
and cross-project cross repository with our baseline approaches.
We achieve the average Mean Absolute Error (MAE) as 2.38, 2.61,
and 2.63 for three scenarios. We outperform GPT2SP in 2/3 of the
scenarios while outperforming Deep-SE in the most challenging
scenario with significantly less amount of running time. We also
compare our approaches with different homogeneous graph neu-
ral network models and the results show that the heterogeneous
graph neural networks model outperforms the homogeneous mod-
els in story point estimation. For time performance, we achieve
about 570 seconds as the time performance in both three processes:
node embedding initialization, model construction, and story point
estimation. HeterpSP’s artifacts are available at [22].

Conclusion. HeteroSP, a heterogeneous graph neural networks
model for story point estimation, achieved good accuracy and run-
ning time.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ESEM ’22, September 19–23, 2022, Helsinki, Finland
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9427-7/22/09.
https://doi.org/10.1145/3544902.3546248

CCS CONCEPTS
• Software and its engineering;

KEYWORDS
software effort estimation, heterogeneous graph transformer, graph
neural networks

ACM Reference Format:
Hung Phan and Ali Jannesari. 2022. Heterogeneous Graph Neural Networks
for Software Effort Estimation. In ACM / IEEE International Symposium
on Empirical Software Engineering and Measurement (ESEM) (ESEM ’22),
September 19–23, 2022, Helsinki, Finland. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3544902.3546248

1 INTRODUCTION
Agile software development (ASD) is the class of software devel-
opment approaches that can provide the flexibility in delivering
products by multiple iterations of development [2]. In ASD, require-
ments along with solutions are achieved thanks to the collaboration
between self-organizing cross-functional teams. A core component
of producing the following ASD correctly is the step of planning to
make the effort estimation. Software effort estimation is a software
engineering (SE) activity that is performed under a development
process called planning. There are three common levels of planning
in ASD, including release planning, iteration planning, and current-
day planning. All of these levels of planning require good effort
estimation [35]. There are several techniques for effort estimation,
including Planning poker, Expert Opinion, Analogy, and Disag-
gregation [9]. From a study of [35], Planning poker is considered
the best Agile planning technique. This approach starts when the
product owners or customers went through an Agile user story and
described a feature to a group of estimators. Next, each estimator
holds planning poker cards with a specific set of integer values such
as [0, 1, 2, 3, 5, 8.., 100] that are represented for the measure of how
much effort is needed to complete a story. Estimators continue
asking questions to product owners and discussing with each other
until all estimators unite the assigned effort value for each story.
In ASD, we call that value the story point of a software story [35].
In Planning poker, story points are assigned as Fibonacci numbers
[30].

Story Points (SP) require a lot of effort from the estimators. To
have the information on SPs, estimators need to make multiple pan-
els for discussion to solve the conflict about the assignment of SP
for a specific work item. Besides, estimators from specific research
domains can have a bias in the decision of SP. That fact leads to
the risk of incorrect story point estimation [35]. These inaccurate
SPs cause the project to confront the challenges of harmful sprint
planning. For example, assigning a story point that is too small

 
 
 
 
 
 
ESEM ’22, September 19–23, 2022, Helsinki, Finland

Phan et al.

compared to the actual effort for completing a work item will cause
a reduction in the productivity of sprint development. Besides, the
developer time of a story might be much longer than expected,
which brings the risk of project failure. Both overestimating sto-
ries and underestimating stories cause problems for ASD. Thus,
automatically predicting SP given the work item is an important
and helpful research problem in SE. It will help to reduce a large
amount of effort in the planning process for ASD.

Automatically predicting story points from software stories (is-
sues) in ASD can be formulated as a text regression problem. Given
the input as software issues, the output of software effort estimation
is the number of efforts estimated. There are several research works
that have appeared since 2016 for automatically inferring story
points in software engineering [8, 12, 28]. The key idea of these
works is to suggest the SP in three steps. First, embedding models
are used to learn the vector representation of software issues. Each
issue contains a title and a description. Thus, the first task is to learn
the embedding of a document with sentences in titles and sentences
in descriptions. Second, a training model will be provided by differ-
ent regression machine learning and deep learning algorithms to
learn the SP from the sequence of input vectors from the first step.
Third, the model constructed from training will be used to predict
story points for new issues. In prior works related to story point esti-
mation, Porru et al. [28] used the traditional bag-of-words approach
for vectorization and multiple machine learning approaches such
as Support Vector Machine, Naive Bayes, and K Nearest Neighbour-
hood for model construction. Choetkiertikul et al. [8] designed a
deep learning based approach to solve the problem, called Deep-SE.
They use Long Short Term Memory (LSTM) for vectorization and
Recurrent Highway Network (RHN) for model construction. Fu et
al. [12] proposed a story point estimation tool GPT2SP, which relied
on a pre-trained language model GPT-2 and Byte Pair Encoding
(BPE) tokenization for issue representation. Next, in this approach,
a transformer decoder and Multi-Layer Perceptron (MLP) are used
for constructing the training model. In general, these works solved
the problem by taking advantage of techniques of machine learning
and deep learning approaches that are used in Natural Language
Processing (NLP).

Prior research works showed that applying NLP techniques in SE
problems can be problematic in various ways and cause unexpected
low accuracy [16, 23]. A possible reason is that SE corpora have
different characteristics [23]. Similarly, in story point estimation,
all of Porru’s approach [28], Deep-SE [8], and GPT2SP [12] have
several drawbacks. First, they rely on a sequence-based learning
approach. In other words, they consider the input of the machine
learning model as a sequence of text that has a large number of
words. This design choice has a disadvantage in that they need to
build or rely on a very expensive language model for getting the
vector representation of each issue. Deep-SE required 2-8 hours for
pre-training to get the vector representation of issues for each single
software project in their dataset consisting of 16 projects. However,
the replication study conducted by Tawosi et al. [32] showed that
the Deep-SE didn’t statistically outperform the classical non-neural
networks approach TFIDF-SE which is much more faster in model
construction. GPT2SP relied on the expensive language model GPT-
2 which was trained by the content of 8 million websites in different
domains compared to SE. Secondly, another problem related to the

running time is that the software issues are actually a combination
of source code messages/ source code snippets along with natural
language sentences. There has not been any mechanism to differen-
tiate information from code and information from natural language
in Deep-SE and GPT2SP due to the limitation of a sequence of
text as representation. Another data structure that can substitute
sequence-based learning is using graph neural networks (GNN).
Phan et al. [24] applied Text Level GNN to predict the range of
story points. However, this work also confronted the challenges of
the inefficiency of training time, which is 6 times longer than using
the traditional Random Forest regression approach.

We attempt to overcome these challenges in this project. We
propose HeteroSP, a tool for effectively estimating story points
from issues’ titles and descriptions. First, to make sure that our
approach takes into consideration the characteristic of the software
engineering corpus, we do a study of the characteristics of the
evaluated dataset of story points proposed by Choetkiertikul et al.
[8]. Next, we design an algorithm for constructing the graph from
input as software issues’ title and description. Third, we construct
a heterogeneous graph neural networks (HeteroGNNs) training
model to learn information about different types of nodes extracted
from software issues. Forth, we use the training model for predicting
story points of issues. We propose the following contributions:

(1) A technique for preprocessing textual information of soft-
ware issues that highlights the characteristic of software
engineering issues.

(2) An algorithm of graph construction from software issues

with efficient running time.

(3) A heterogeneous graph neural networks model for story

point estimation.

The rest of the paper is provided as follows. In section 2, we intro-
duce the our baseline approaches, GPT2SP [8] and Deep-SE [8]. In
section 3, we show an example of the input and output of a soft-
ware issue. Section 4 describes our study of the dataset to highlight
the characteristics of this software engineering corpus. Section 5
illustrates our approach overview and important components. Sec-
tion 6 mentions the list of research questions. Section 7 shows the
results of our approach for each research question. Other sections
are Related work, Threats to validity, and Conclusion.

2 BACKGROUND
Story Point Estimation (SPE). SPE is the main effort estimation
approach used in Agile Methodology [9]. In this methodolody, user
stories with title and description are provided as an input. Instead
of using time for cost estimation, the metric for representing story
point is man-hour, which is defined the amount of work completed
by an average IT engineer during one hour working continuously
[3]. The man-hours assigned to each story points are decided by mul-
tiple panels of discussion between software project’s participants.
To our knowledge, GPT2SP [12] is the latest tool for automatic SPE
while Deep-SE is the most successful approach in the last 5 years
until May 2022.

GPT2SP. This tool has four modules for estimating the SP. First,
the Subword Tokenization module will tokenize words in each issue
to a smaller unit called subword with the use of Byte Pair Encod-
ing (BPE) and the output of GPT-2 language model [29]. Second,

Heterogeneous Graph Neural Networks for Software Effort Estimation

ESEM ’22, September 19–23, 2022, Helsinki, Finland

the Word and Positional Encoding module will init a vector rep-
resentation with each subword of the input. Third, the Stack of
12 Decoder-only Transformer use a modified version of GPT-2 to
generate the representation for software issues. Forth, the Multi-
layer Perceptron will be used for regression learning to estimate
the story points as real numbers.

Deep-SE. There are two modules of Deep-SE. First, the Docu-
ment Representation was done by Long Short Term Memory (LSTM)
to construct a model for text-to-vector conversion. Second, a Recur-
rent Highway Network model was constructed for SP estimation
[8]. The first module required 2-8 hours for each software project,
which can be considered an expensive process [12].

3 MOTIVATION EXAMPLE
An example of a software issue in the Deep-SE dataset is shown
in Figure 1. From the issue DM-21571 in the Data Management
project, we can see that there are two parts of input for story point
estimation (highlighted in the red blocks). First, the title part will
tell developers about the summarized problem of the issue in one
or two sentences. In this example, this sentence mentions the crash
of the object called 𝐷𝑎𝑡𝑎𝐿𝑜𝑎𝑑𝑒𝑟 . Second, the description part will
tell the readers about the details of the issue. In this example, the
description part contains three components. The first sentence of
the description is about a discussion of the issue’s creator and one
tester. The second part is the information of the compiler returned
by the task of the tester. The second sentence is the opinion of the
issue’s author and some predictions about the reason for the bug.
The story point for this issue is 1 (in the blue box), which is the
target object for learning and prediction. Title and description are
the two inputs provided by the Deep-SE dataset. Other information,
such as Fix version, Components, and Sprint id will be omitted from
this research problem.

4 ANALYSIS ON DATASET
Example in Figure 1 shows that the description of a specific software
issue can be a mixture of natural language and non-natural language
parts. In this paper, we call the non-natural language part the code
part. However, there is a possibility that the example is just an
exceptional case in which the issue’s creator embeds the code part
onto the content of the issue. To test the hypothesis of whether there
are a lot of issues containing code parts, we perform an analysis on
23313 issues of the Deep-SE dataset.

Before the analysis, we summarize key points about the Deep-SE
dataset. This dataset is collected from 16 software projects available
in the Jira system2. These projects were collected from 8 different
repositories. In total there are 23313 issues are collected. The num-
ber of issues per project ranges from 384 for the Clover project
to 4667 for the Data Management project. The summary of the
Deep-SE dataset along with the abbreviation of each project we use
in this paper can be seen in Table 1.

Analysis of the size of the vocabulary. We count the number
of unique words per project. In this configuration, we use the raw
text of the title and description for each issue as the input for our
analysis.

1https://jira.lsstcorp.org/browse/DM-2157
2https://jira.lsstcorp.org/secure/Dashboard.jspa

Figure 1: Motivation Example: Issue DM-2157 of the Data
Management project

Table 1: Dataset proposed by Choetkiertikul et al. [8]

Repo.

Apache

Appeclerator

DuraSpace

Atlassian

Moodle
Lsstcorp

Mulesoft

Spring

Talendforge

Total

Project

Abb.
Memos
ME
UG
Usergrid
Appecelerator Studio AS
AP
Aptana Studio
TI
Titanium
DC
DuraCloud
BB
Bamboo
CV
Clover
JI
JIRA Software
MD
Moodle
DM
Data Management
MU
Mule
MS
Mule Studio
XD
Spring XD
TD
Talend Data Quality
TE
Talend ESB

# issues
1680
482
2919
829
2251
666
521
384
352
1166
4667
889
732
3526
1381
868
23313

Analysis of average appearance in issue (Avg. App.). We
consider another metric to evaluate the sparsity of the vocabulary
in the Deep-SE dataset along with the vocabulary as the average
appearance of a word in issues.

Analysis of the number of code parts. We count the number
of code parts that appeared in the issues. We detect the code part
by the regular expression on each word in the combination of title
and description.

The results of analyzing the size of vocabulary and Avg. App. are
shown in the preprocess column of Table 3. Most of the software
projects have a vocabulary size greater than 5000. Besides, the Avg.
App for projects is relatively low. They are from 2.77 for Bamboo
project to 6.59 for Appcelerator Studio (AS) project. It means that

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Phan et al.

Table 2: Statistic on Special Tags of Deep-SE dataset [8]

Special Tag

{code}
{noformat}
{quote}
{code:java}
{code:javascript}
<redacted>
{code:xml}

# of issues
6371
1069
333
182
178
118
107

Table 3: Impact of pre-processing input description of issues

Proj. Before Preprocess After Preprocess
Vocab Avg. App.
7.64
23580
4.50
12152
3.31
7688
3.26
7396
7.18
25516
6.02
4915
5.86
3337
3.64
34123
5.36
11034
5.00
8386
4.87
6430
6.78
23105
5.55
12532
4.53
11913
5.20
36038
3.84
5254

Vocab Avg. App.
6.59
26301
3.79
13424
2.77
7946
2.85
7906
5.63
32617
4.60
6699
4.63
4471
3.34
37242
4.35
14236
4.14
9769
4.17
7101
5.43
26544
4.50
13257
3.77
13394
4.15
41839
3.09
5628

AS
AP
BB
CV
DM
DC
JI
ME
MD
MU
MS
XD
TD
TE
TI
UG

for a general word that appeared in the Bamboo dataset, there are
only three issues that contained that word.

The summary of popular special tags along with a number of
issues is shown in Table 2. We can see that there are a lot of issues
that have special tags in the content. The tag {𝑛𝑜 𝑓 𝑜𝑟𝑚𝑎𝑡 } is used
to split the content of compilation messages. About one-third of
23313 issues have special tags in their content. It proves that the
combination of code parts and natural language parts is common
in the story point dataset Deep-SE.

A good story point estimation approach should handle
the problem of large size vocabulary and preserve the infor-
mation of software issues as the combination of natural lan-
guage parts and code parts in its model.

5 APPROACH
In this section, we present HeteroSP, a heterogeneous graph neural
networks model for story point estimation.

Overview: Given the software issue as the input, a graph with
different types of nodes and edges for each issue is constructed.
Second, a heterogeneous graph generated by the combination of
multiple graphs for software issues is created. Third, each node in
the heterogeneous graph has the initial embedding based on the
textual content of their leaf nodes. Fourth, the model building pro-
cess is done by multiple layers of heterogeneous graph transformer
(HGT) training. Fifth, the trained model is used for prediction from

the graph extracted from the testing issue to generate the story
point for Agile users. The heart of HeteroSP is the engine of hetero-
geneous graph transformer (HGT) which we apply and optimize
from the work of Fey et al. [11]. Compared to other traditional
GNN models such as TextLevelGNN [24], HGT has been consid-
ered advantageous since this model allows different types of nodes
and edges for model construction. We describe components of the
HeteroSP below.

5.1 Graph Generation for Issue
5.1.1 Types of nodes in Graph. From the analysis in the previous
section, we design the list of types for nodes for our constructed
graph from the textual description as in Figure 3. Each issue will
have a root node of type Document. The children of the Docu-
ment are Title and Description nodes, which are the corresponding
summarized nodes for the title and description of the issue. The
Title node can be the parent node of multiple Sentence nodes. Each
sentence will contain a sequence of words. Compared to the Title
node, the Description node can accept a child as Code Part, which
is the root node of all code tokens that appeared inside special tags
shown in Table 2.

Algorithm 1: Graph Construction from Software Issue
: title,description: string
Input
Output : nodeDocument: Node

1 𝑛𝑜𝑑𝑒𝑇 𝑖𝑡𝑙𝑒 ← 𝑛𝑒𝑤 𝑁 𝑜𝑑𝑒 ()
2 𝑛𝑜𝑑𝑒𝐷𝑒𝑠𝑐 ← 𝑛𝑒𝑤 𝑁 𝑜𝑑𝑒 ()
3 𝑙𝑖𝑠𝑡𝑃𝑎𝑟𝑡𝑠𝑇 𝑖𝑡𝑙𝑒 ← splitToParts(𝑡𝑖𝑡𝑙𝑒)
4 𝑙𝑖𝑠𝑡𝑃𝑎𝑟𝑡𝑠𝐷𝑒𝑠𝑐 ← splitToParts(𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛)
5 createGraphFromParts(𝑛𝑜𝑑𝑒𝑇 𝑖𝑡𝑙𝑒, 𝑙𝑖𝑠𝑡𝑃𝑎𝑟𝑡𝑠𝑇 𝑖𝑡𝑙𝑒)
6 createGraphFromParts(𝑛𝑜𝑑𝑒𝐷𝑒𝑠𝑐, 𝑙𝑖𝑠𝑡𝑃𝑎𝑟𝑡𝑠𝐷𝑒𝑠𝑐)
7 𝑛𝑜𝑑𝑒𝐷𝑜𝑐𝑢𝑚𝑒𝑛𝑡 ← 𝑛𝑒𝑤 𝑁 𝑜𝑑𝑒 ()
8 𝑛𝑜𝑑𝑒𝐷𝑜𝑐𝑢𝑚𝑒𝑛𝑡 .𝑎𝑝𝑝𝑝𝑒𝑛𝑑𝐶ℎ𝑖𝑙𝑑 (𝑛𝑜𝑑𝑒𝑇 𝑖𝑡𝑙𝑒)
9 𝑛𝑜𝑑𝑒𝐷𝑜𝑐𝑢𝑚𝑒𝑛𝑡 .𝑎𝑝𝑝𝑝𝑒𝑛𝑑𝐶ℎ𝑖𝑙𝑑 (𝑛𝑜𝑑𝑒𝐷𝑒𝑠𝑐)
10 return 𝑛𝑜𝑑𝑒𝐷𝑜𝑐𝑢𝑚𝑒𝑛𝑡

5.1.2 Algorithm for Graph Construction. The algorithm for gener-
ating a graph for each software issue is shown in Algorithm 1. The
content of the title and description will be analyzed and split into
smaller parts in lines 3 and 4. The construction of graphs for the
Title node and Description node is done by lines 5 and 6. Finally,
the Document node will be created as the root of the Title and
Document node. We explain the details of each function below.

Purpose of 𝑠𝑝𝑙𝑖𝑡𝑇𝑜𝑃𝑎𝑟𝑡𝑠 function. This function will split the
content of the title by stop words in English and split the content
of the description by stop words along with the special tags defined
in Table 2. The output of this function is multiple parts that can be
sentences or code parts.

Purpose of 𝑐𝑟𝑒𝑎𝑡𝑒𝐺𝑟𝑎𝑝ℎ𝐹𝑟𝑜𝑚𝑃𝑎𝑟𝑡𝑠 function. This function
goes over each part as a loop. In each iteration, it creates the graph
for each part by traversing each token inside the part and doing
the preprocessing for each token. The token after preprocessing is
added as Word nodes.

Token Normalization. The preprocessing of the token has the
following steps. First, the token is split by camel cases into multiple

Heterogeneous Graph Neural Networks for Software Effort Estimation

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Figure 2: An Overview Architecture of HeteroSP

issue and there are nodes that are generalized to shared nodes be-
tween issues. The shared nodes (red nodes in Figure 1) helped the
model to avoid having disjoint sub-graphs in the heterogeneous
graph, which can badly impact the training [11]. In HeteroSP, we
design to split the independent nodes and shared nodes between
issues by types of nodes. We select nodes of ancestor types in Figure
3 as the independent nodes and nodes of type Word or CodeToken
are shared nodes. In a heterogeneous graph, the independent nodes
will be assigned their identities by concatenating the issue identi-
fication and value of nodes in the individual graphs. The shared
nodes will have the identities as the value of nodes in graphs for
each issue. An example of graph in Figure 4, non terminal nodes
such as T-Sent-1 will be unique in the heterogeneous graph. The
terminal nodes like "data" will be generalized and can be connected
with nodes of other issues that have "data" in their content.

5.3 Assign Initial Embedding
Each node will have an initial vector representation before model
construction. Due to the characteristic of the SE corpus, we have
to use an initial embedding model that supports embedding for
subwords and is also efficient in terms of running time. We apply
the fastest [6] as the embedding model for this task. We choose the
cbow model with 100 dimensions as the configuration for assigning
initial embedding. The embedding of non-terminal nodes like Sen-
tence nodes and Document nodes will be the vector representation
of a sequence of text that they covered. The embedding of terminal
nodes such as Word nodes will be the vector represented for their
nodes’ values.

5.4 Model Construction
We inherit the Heterogeneous GNN model proposed by Fey et al
[11] for training the model for story point estimation. The idea of
this engine is to create different layers for learning in different types
of nodes. In Figure 2, there are three sub-layers related to three
types of nodes: Document, Sentence, and Word. A heterogeneous
graph transformer will provide the learning process with L layers.
In our work, we choose the number of layers L as 2. Information
between each sub-layers will be connected by the residual connec-
tion module, which has three components. The first component
is heterogeneous mutual information, which is used to assign a

Figure 3: Types of nodes in HeteroSP

Figure 4: Sub-graph extracted of Issue DM-2157 in Figure 1

subtokens. Second, for each subtokens, we perform stemming and
lemmatization to have the final tokens. We use StanfordCoreNLP
toolkit [18] for text normalization. The text normalization reduces
the size of vocabulary, along with increasing the average appear-
ance of a token in the dataset as shown in the After Preprocess
column in Table 3.

5.1.3 Example of Graph Representation for Issues. Part of the graph
constructed for issue DM-2157 in Figure 1 can be shown in Figure 4.
In this example, 4 parts of the issues, including one sentence from
the title and 2 sentences plus one code part in the implementation
will be represented on the graph. The terminal nodes will be Word
nodes which are normalized.

5.2 Heterogeneous Graph Construction
Multiple graphs for each issue will be combined into a heteroge-
neous graph in this step. There are nodes that are unique for each

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Phan et al.

weight for more important neighbor nodes in different distribu-
tions. The second component, heterogeneous message passing, is
used to pass the information between neighbor nodes. The third
component, target-specific aggregation, is used to combine the cal-
culation based on the message passing and mutual information
to pass the information to the next layer. The implementation of
residual connection for each layer of HGT is implemented by Hu
et al. [13].

Since Fey et al. [11] designed their model for the classification of
the category of authors and papers in a dataset of research papers
in NLP, we adjust the library for estimating story points. First, we
formulate the types of nodes for prediction as Document nodes.
The reason for this design selection is that Document nodes are
considered the root nodes for each issue. Second, we substitute the
final layer of the HGT engine from Fey et al. [11] to a linear trans-
formation layer, which outputs a numeric value instead of a vector
with 𝑛 dimensions (𝑛 is the number of classes for classification).
Third, while Fey et al. [11] used cross-entropy as a loss function,
we substitute this loss function to 𝐿1 loss function [1] to make our
model suitable for calculating the Mean Absolute Error per each
step of training.

6 RESEARCH QUESTIONS
We attempt to answer the following research questions (RQs):

(1) RQ1: How well can HeteroSP perform in within project effort

estimation?

(2) RQ2: How well can HeteroSP perform in cross-project within

repository effort estimation?

(3) RQ3: How well can HeteroSP perform in cross-project cross

repository effort estimation?

(4) RQ4: Can the proposed graph structure support homoge-

neous graph neural networks effort estimation?

(5) RQ5: How well can HeteroSP perform in issues without title

or without description?

(6) RQ6: How well can HeteroSP perform in story point classifi-

cation?

(7) RQ7: How well can statistical NLP parse trees help for effort

estimation?

We describe the purposes of each RQs along with how we set up
the experiments below.

6.1 Setup
Configuration. We train our HGT model with 128 hidden chan-
nels. We use four attention heads for mutual attention learning. We
construct two HGT convolutional layers for each type of node in
our heterogeneous graph. For each RQs, we run our model 10 times
and take the average MAE as the final results. The summary of our
configuration can be shown in Table 4.

State-of-the-art Approaches. We compare HeteroSP with the
tools GPT2SP [12] and Deep-SE [8]. We provide the head-to-head
comparison with GPT2SP and Deep-SE from RQ1 to RQ3 per each
project. We report the MAE retrieved from the result of the GPT2SP
replication package with the last commit at March 10th, 2022. The
raw prediction results for each issues of GPT2SP are available at

Table 4: Configuration of HeteroSP

Config

Attention Head
Epoch
Convolutional Layer
Hidden Channels

Num.
4
500
2
128

this site3. We observe that for RQ1, the average MAE provided by
their testing_results folder is 2.57, which is higher than what they
reported from their diagram4 as 1.44. We contacted the author and
they suggest to use the results in their diagram for RQ1. We use
the results reported in the testing_results folder for RQ2 and RQ3.
Similarly, we use the results from 3 RQs reported from Deep-SE [8]
for comparison.

6.2 RQ1: How well can HeteroSP perform in

within project effort estimation?

In this RQ, we want to test the ability of HeteroSP to predict the
story point based on previous planning and estimation of software
issues in the past. In this scenario, for each project, we use a set of
software issues that appeared earlier in the dataset as training data.
The validation and testing issues are the most recent issues that
appeared later compared to issues in the training dataset. As this
is within project configuration, we set up 16 models for training
and evaluation for 16 software projects. We use 60% of data for
training, 20% of data for validation, and 20% of data for testing. Our
configuration is the same as the experiment provided in GPT2SP.
Metric for evaluation. Similar to Deep-SE and GPT2SP, we
use Mean Absolute Error (MAE) to calculate the accuracy of effort
estimation. The MAE calculated the average distance between the
expected story point and the predicted story point for each issue.
MAE is a traditional metric used for regression problems. It has
been used as the evaluation metric for all research works related to
story point estimation in Software Engineering [8, 12].

6.3 RQ2: How well can HeteroSP perform in
cross-project within repository effort
estimation?

In this RQ, we want to set up the scenario that a group of software
developers in a software organization want to develop a new soft-
ware project. Can they rely on the software issues that were made
in the development of previous projects developed by their coun-
terparts in the same organization? From this scenario, developers
will save time for requirements and planning thanks to the material
of the organization in the past.

We follow the same configuration with GPT2SP to set up this
experiment. We provide 8 training and testing processes. For each
process, we use all the story points from one project for training.
The constructed model will be used to predict all the story points of
the target projects. There are 8 pairs of source and target projects.

3https://tinyurl.com/mu7jpbur
4https://tinyurl.com/mwhmmdnw

Heterogeneous Graph Neural Networks for Software Effort Estimation

ESEM ’22, September 19–23, 2022, Helsinki, Finland

6.4 RQ3: How well can HeteroSP perform in
cross-project cross repository effort
estimation?

The purpose of this research question is to simulate a practical
challenge in Jira development [8]. In some projects, there are not
enough issues for training the model for estimation. Even other
projects from the same organization might have a lack issues to
produce an efficient training model. In this case, we need to use the
story issues from other projects that are in different repositories to
enrich the data for story point estimation.

We select the same source and target projects for evaluation with
GPT2SP [12] in our experiment. They are AS-MU, AS-MS, CV-UG,
MS-TI, MU-TI, AD-AS, TD-AP and TE-ME. Since this configuration
is considered the most challenging task in three RQs from the work
of Deep-SE [8] and GPT2SP [12], we also use this configuration for
experiments of our next RQs, which are more related to intrinsic
evaluation about the efficiency of our constructed model.

6.5 RQ4: Can HeteroSP’s graphs support

homogeneous GNN in effort estimation?
Phan et al. [24] is the first work that has attempted to apply Graph
Neural Networks classification for predicting the range of story
points of the Deep-SE dataset. They rely on the graph construction
and training process of the TextLevelGNN engine [14] to solve the
problem of estimation. Although TextLevelGNN is a well-known
homogeneous GNN model in NLP research, Phan et al. [24] show
that the original GNN models can cause negative impacts on esti-
mating story points due to the large size of vocabulary and number
of edges for graph construction.

In this RQ, we want to test whether our algorithm 1 for graph
construction can achieve better performance for not only heteroge-
neous GNN but also homogeneous GNN models. The main differ-
ence between heterogeneous GNN models and homogeneous GNN
models is that the homogeneous GNN models accept only a unique
type of nodes and edges as the input.

We did not use TextLevelGNN [14] approach since they use the
n-grams strategy in NLP for graph construction which is ineffi-
cient in story point estimation. Instead, we select three well-known
homogeneous GNN models that allow customizing the graph con-
struction process. They are Graph Convolutional Networks (GCN)
[17], Graph Attention Networks (GAT) [36] and Attention-based
Graph Neural Networks (AGNN) [34]. We use the same configura-
tion defined in Table 4 to set up the experiment for this RQ. We run
these three GNN models with the cross projects cross repositories
estimation.

6.6 RQ5: How well can HeteroSP perform in

issues without title or without description?
In this RQ, we want to test the ability of HeteroSP on incomplete
issues. We simulate the practical case where the issue’s creator
has already named the issue but hasn’t written the description
yet. Besides, there is another case where developers forgot to set
the title of the issue. To set up the experiment, we performed the
training processes of cross-project cross repository effort estimation

Figure 5: The sub-graph of parse tree of issue DM-2157

from two types of input: issues with only title and issues with only
description.

6.7 RQ6: How well can HeteroSP perform in

story point classification?

The output of the story point can range from 1 to 100 in the Deep-
SE dataset. However, story points are usually assigned as Fibonacci
numbers [8]. The specific set of Fibonacci numbers from 0 to 100 is
smaller than the set of integers from 0 to 100. This fact prompted
us to think about the hypothesis that a regression model can be
replaced by a classification model. In this model, the value of the
story point will be assigned as the label for multi-label classification.
To conduct the experiment for this RQ, we changed the HGT
model to a node classification model. We changed the L1 loss func-
tion in regression training to a cross-entropy loss function. We
performed the experiment on cross-project cross repository esti-
mation.

6.8 RQ7: How well can statistical NLP parse

trees help for Story Point Estimation?
There has been some recent research on generating graphs from
natural language. In these works, StanfordCoreNLP [18] provided
an approach to generate a parse tree (or Part-of-Speech POS tree)
along with universal dependencies edges. Since the parse tree is
well-known in NLP research, we wonder if it can make the impact of
using the parse tree to replace our graph representation. An example
of a parse tree with universal dependencies edges is shown in Figure
5. From our knowledge, there are advantages and disadvantages
of replacing our graph construction mechanism with parse tree
construction.

Advantages. The parse tree generated from StanfordCoreNLP
provides useful information related to the parse tags for each sen-
tence. Besides, there is extra information between nodes that were
represented by universal dependencies edges.

Disadvantages. We observe two disadvantages of using a parse
tree for constructing a model for HeteroSP. First, the cost of parse
tree generation is high. There are eight hours of running the Stan-
fordCoreNLP model on a server with 64 GB of RAM memory and a
core-i9 processor to generate a parse tree for 23313 software issues
of the Deep-SE dataset. Second, the StanfordCoreNLP parse tree
might not be suitable for SE corpora.

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Phan et al.

We tested our hypothesis of the pros and cons of using the parse
tree by this experiment. We changed the graph constructed by our
algorithm to the information of the parse tree to construct a new
HGT model. Since embedding the whole parse tree can cause ex-
ponential increase of nodes and edges, we embedded information
of terminal nodes and their directly ancestor nodes for graph con-
struction. Next, we ran the effort estimation on cross-project cross
repositories configuration.

7 RESULTS
7.1 RQ1: Within project effort estimation
We summarize the result of within project estimation in Table 5. We
achieved better MAE than Deep-SE on the AS project and Moodle
project, while GPT2SP achieve the best MAE over 16 projects. A
possible reason is due to the fact that the GNN models are usually
hungry for data [15] while within repository estimation learned the
model from the small number of the training set. We achieved the
best MAE for project Bamboo with 0.75 MAE. A possible reason for
the lower MAE of Bamboo compared to the other projects is that
the estimated story points are consistent over the history of project
planning in this project. Besides, the range of the minimum story
point to maximum story point is from 1 to 20, which means that
the space of numbers assigned for issues of the Bamboo project is
more continuous than other projects such as Moodle.

We get the highest MAE with the Data Management project. This
project caused challenges to machine learning models in predicting
story points. A possible reason is that the training model predicts
incorrectly for issues with high story points assigned. The Moodle
project is another challenging project with MAE achieved by Het-
eroSP as 5.34. We outperformed Deep-SE for this project. HeteroSP
and GPT2SP achieved about the same accuracy as the Bamboo
project. On average, we achieved 2.38 MAE over 16 projects, while
GPT2SP achieved 1.44 and Deep-SE achieved 2.09.

We also analyze the performance of HeteroSP in terms of run-
ning time. We achieve the total running time as 18.47 seconds over
4671 issues for this configuration. Since other deep learning models
such as Deep-SE required 2-8 hours of training per project, our ap-
proach shows the benefits of learning with more representative data
structures such as graphs. The data management project required
most of the time for running with over 21 seconds. All projects
have sufficient testing time from 0.01 seconds to 0.23 seconds.

HeteroSP achieves the average MAE as 2.38, which has a
higher average MAE than GPT2SP and Deep-SE for within
project effort estimation.

7.2 RQ2: Cross Project Within Repository

effort estimation

The result of predicting story points for cross projects within the
repository is shown in Table 6. In this evaluation, we predict the
data of issues for 7 software projects based on the issues provided.
by another project from the same repository. For this evaluation, we
outperformed GPT2SP for most of the target projects. We achieved
about the same MAE as 1.5 for the prediction of the Mesos project.
The best project that our tool HeteroSP outperformed GPT2SP can
perform in this setting is the Usergrid project. A possible reason
for the good MAE is thanks to the ratio of training and testing

Table 5: RQ1: Comparison of MAE between HeteroSP and
baseline approaches for Within Project Estimation

Project

Appcelerator Studio AS
AP
Aptana Studio
BB
bamboo
CV
Clover
DM
Data Management
DC
Duracloud.csv
JI
Jirasoftware
ME
Mesos
MD
Moodle
MU
Mule
MS
Mule Studio
XD
Spring XD
TD
Talend Data Quality
TE
Talend ESB
TI
Titanium
UG
Usergrid

Abbrev HeteroSP GPT2SP Deep-SE
1.36
2.71
0.74
2.11
3.77
0.68
1.38
1.02
5.97
2.18
3.23
1.63
2.97
0.64
1.97
1.03
2.09

1.30
3.24
0.75
3.64
6.19
0.78
1.62
1.21
5.34
2.47
3.58
1.72
2.20
0.91
2.04
1.16
2.38

0.84
1.93
0.44
1.98
3.10
0.48
0.92
0.66
4.09
1.43
2.04
0.96
1.58
0.50
1.36
0.68
1.44

Average

Table 6: RQ2: Comparison of MAE Between HeteroSP and
baselines for Cross Project Within Repository Estimation

Source Target HeteroSP GPT2SP Deep-SE
2.78
AS
2.06
AS
3.45
AP
1.16
ME
2.31
MS
3.14
MU
3.22
TI
1.13
UG
2.41

4.45
3.20
3.71
0.98
2.66
3.36
2.29
1.50
2.77

4.17
3.17
3.18
0.89
2.60
3.26
2.10
1.50
2.61

AP
TI
TI
UG
MU
MS
AS
ME

Average

as 4:1, which can be significant to avoid a lack of training data.
The project that caused challenges for story point prediction is
the Aptana Studio project, which has the highest MAE returned
by HeteroSP. We achieved consistently the MAE of prediction for
the Titanium project at 3.17 and 3.18. Compared to Deep-SE, we
achieved better accuracy with 3 configurations including AP-TI,
ME-UG, and TI-AS.

For the training time, HeteroSP required 238 seconds to provide
the training for 8 projects. Though this is a larger number compared
to the running time in RQ1, it is reasonable since for RQ2 we need
to run on the whole dataset of story points for several projects.
HeterroSP took 0.23 seconds to get the testing result for 8 projects.
HeteroSP outperforms GPT2SP in predicting the SPs with
lower MAE by 0.16 but got higher MAE than Deep-SE by 0.2
for cross-project within repository estimation.

7.3 RQ3: Cross Project Cross Repository effort

estimation

For this scenario, the projects for training and projects for test-
ing are from different repositories. The result is shown in Table
7. Compared to Deep-SE, HeteroSP achieved a significantly better

Heterogeneous Graph Neural Networks for Software Effort Estimation

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Table 7: Comparison Between HeteroSP and models from RQ3, RQ4, RQ5, RQ6 and RQ7

Cross Proj. Cross Repo.
Source Target
AS
AS
CV
MS
MU
TD
TD
TE

MU
MS
UG
TI
TI
AS
AP
ME
Average

RQ3

RQ4

RQ5

HeteroSP GPT2SP Deep-SE AGNN GAT GCN Title Desc
2.60
3.26

2.60
3.26

2.72

2.60
3.26
0.90
3.17
3.17
2.10
4.28
1.54
2.63

2.63
3.32
1.06
3.25
3.27
2.59
4.96
1.72

2.85

2.70
4.24
1.57
6.36

2.83
3.26
1.83
3.17

3.25
1.79
3.17

2.67
3.11
5.37
2.08

3.51

3.15
2.10

3.74
2.09

2.77

3.12
2.40

3.72
2.01

2.77

3.03
3.26
1.69
3.18
3.17
3.00

3.75
1.47
2.82

0.89
3.17
3.17
2.10
4.30
1.51

2.62

0.89
3.17
3.17
2.10
4.29
1.51

2.62

RQ6
Acc. MAE
2.60
26.21%
3.26
30.46%
1.85
18.05%
3.17
31.27%
3.17
31.27%
3.17
26.17%
34.62%
29.64%

3.76
1.50

28.46%

2.81

RQ7
Parse Tree
2.61
3.26
0.91
3.17
3.17
2.10
4.30

1.51
2.63

MAE of 2.63 over 3.51. We also achieved better MAE than GPT2SP
in 8 target projects. With the Titanium project, in this setting, we
used Mule and Mule Studio projects as the source projects for train-
ing. We got almost the same MAE with different source projects
training for predicting issues of Titanium. This fact means training
projects from the same repository can construct models that have
the same performance for effort estimation of unseen issues. We
achieved the best MAE with Clover to Usergrid prediction as 0.9.
Similarly, GPT2SP achieved the best MAE in this project. While
both HeteroSP and GPT2SP achieved high MAE for prediction from
the Talend-Data Quality project to the Aptanta-Studio project, Het-
eroSP achieved better MAE with over 0.6 as the lower distance
compared to GPT2SP.

We require the running time from 17.6 seconds to 32 seconds to
complete the training for each project. The total testing time to run
on all projects is 0.2 seconds.

HeteroSP outperforms GPT2SP by 0.22 and Deep-SE by
0.88 in MAE for cross-project cross repository story point
estimation.

compared to the previous GNN approach is that in this project we
design a better graph representation for the prediction. In [24], the
graph generation is provided at windows of n-grams of words as
input, which caused the exponential of vocabulary for training due
to the characteristics of the story point dataset.

Heterogeneous GNN achieves better MAE than homoge-

neous GNN approaches but got worse time performance.

7.5 RQ5: Estimation of Issues lacking of title or

description

The result of prediction on issues that are incomplete is shown in
Table 7. It shows that the MAE on the configuration of predicting
only title or description of software issues provides a slightly better
MAE. Projects that got better MAE with lack of text are Mesos
and Usergrid. In project Appcelerator, the MAE goes high with
incomplete issue description but not significantly.

HeteroSP can solve the prediction for incomplete issues

with comparable accuracy.

7.4 RQ4: Comparison between Heterogeneous

7.6 RQ6: Accuracy of Estimation’s

Graph and Homogeneous Graph

Classification

While prior work on estimating story points using Text Level Graph
Neural Networks [24] confronts challenges in estimating story
points, we want to test the hypothesis that newer homogeneous
GNN can work with this problem. By the result shown in Table 7,
we see that. the HeteroSP outperformed three homogeneous GNN
models by 0.14 to 0.19 in the average MAE of cross-project cross
repository effort estimation. HeteroSP achieved significantly better
MAE in the prediction of Usergrid from Clover. There are projects
in that HeteroSP got higher MAE compared to other homogeneous
approaches. They are Titanium and Mule Studio projects. However,
overall HeteroSP achieved 2.63 of average MAE while homogeneous
GNN models achieved from 2.77 to 2.82 of MAE.

For the running time, GCN required 1.52 seconds for training
and 0.001 seconds for testing, while AGNN required 4.13 seconds
and 0.0026 seconds. GAT required 6.26 seconds for training and
0.003 seconds for testing. In general, they have better training time
performance compared to HeteroSP, which is reasonable since they
don’t need to be trained on multiple types of nodes and edges. Both
GCN, AGNN, and GAT have better running time than Text-Level
GNN [24]. A possible reason for the improvement of performance

From Table 7, we can see that the accuracy score for software effort
classification is from 26% to over 34% while the MAE ranges from
2.6 to 3.76. We got the average accuracy of classification as 28.446%
while the average MAE is 2.81.

With HeteroSP, the SP classification model achieves lower

accuracy than the SP regression model.

7.7 RQ7: Effects of statistical NLP parse tree to

SP estimation

From Table 7, we observe that the average MAE if we include the
parse tree of StanfordNLP in the graph is the same as the MAE
when we train on our graph. The parse tree can improve the MAE
for Mesos project estimation, while it had worse performance on
predicting issues of the Aptana Studio project. Moreover, the time
for generating the parse tree of 23313 issues in this studied dataset
is over 8 hours.

Compared to the NLP parse tree, HeteroSP achieves a com-
parable MAE but our algorithm requires significantly less
time for graph construction.

ESEM ’22, September 19–23, 2022, Helsinki, Finland

Phan et al.

8 RELATED WORK
Graph Neural Networks in Software Engineering. Machine
Learning techniques on sequential textual input have been widely
applied on many research problems in SE such as specification
inference [25], type inference [26] and natural language to code
translation [20, 27]. Compare to these works, GNN-based models
have advantages on allow graph representation of the input. In
SE, research that applies GNN models mainly focuses on the rep-
resentation of the Abstract Syntax Tree (AST). Allamanis et al. [5]
optimized the Gate Graph Neural Networks to solve node-level
classification problems using AST for predicting the name of the
variables. Brockschmidt et al. [7] proposed a graph extension pro-
cedure to synthesize expression from program location. Aravind
et al. [19] propose a GNN model to detect the similarity between
two input programs. Tehrani et al. [33] applied Relational Graph
Convolutional Networks (RGCN) model for predicting optimum
NUMA/prefetcher configurations of C/C++ programs. In future, ap-
plying techniques on optimization of GNN models [38, 39] can help
increasing the performance and accuracy in solving SE problems.
Heterogeneous Graph Transformer Models. The idea of the
HGT model is proposed by the work of Hu et al. [13]. Their HGT
model was originally built to support web-scale graph datasets. Fey
et al. [11] implement a library that included multiple GNN models
including the HGT transformer developed by Hu et al. [13]. These
works formulate the research problem as node-level classification.
In SE, Wang et al. [37] design a model for method name generation
using heterogeneous graph. They adjust the default HGT model for
classification to an encoder-decoder layer for text summarization.
Other Approaches on SPE. In this paper, we already men-
tioned Porru’s approach [28], Deep-SE [8] and GPT2SP [12] are
three well-known works for story point estimation. There are some
other works related to this area that have appeared recently. Phan
et al. [24] studied problems of the GNN model for this SE problem.
Morais et al. [10] apply multiple deep learning algorithms on the
Deep-SE dataset. They also do the preprocessing on input text but
achieved worse accuracy compared to Deep-SE [8] due to the inef-
fective of their deep learning models. To the best of our knowledge,
HeteroSP the first work that fully implements and optimizes GNN
models with competitive accuracy to achieve this task.

A recent study on Deep-SE and future directions for SPE.
In June 2022 (after the time our paper was submitted to ESEM 2022),
Tawosi et al. [32] conducted a replication study on Deep-SE and
compared Deep-SE over the traditional text regression approach
using Term Frequency Inverse Document Frequency (TFIDF) for
input feature extraction, called TFIDF-SE. They had some findings
and conclusions. First, the replicated implementation of Deep-SE
provided by Tawosi et al received lower accuracy than the results
reported in the Deep-SE paper [8] in both three configurations (RQs
1-3). They concluded that possible reasons for these discrepancies
were the incorrectness of MAE computation in the original work
of Deep-SE proposed by Choetkiertikul et al. [8] along with the
private ways that Deep-SE used for random guessing and splitting
the train/valid/test data. Second, Tawosi et al. showed that the re-
implemented version of Deep-SE didn’t statistically outperform the
classical TFIDF-SE approach in most cases. This fact suggests future
works on SP estimation should compare their works carefully with

traditional approaches and use other metrics which can be more
meaningful than MAE. Third, they provide some new directions for
researching in this area. Similar to our RQ6, they suggest solving
the problem by multi-label classification models can be a more
suitable direction compared to regression techniques. Moreover,
they conducted a new dataset of software issues story points [31],
which contains over 39000 entities and can be a good substitution
for the Deep-SE dataset in future works.

9 THREATS TO VALIDITY
There are several threats to validity that we want to discuss. First,
the dataset of story points proposed by Deep-SE [8] might not be
representative. We mitigate this threat by a comprehensive litera-
ture review and see that most current-trend tools for story point
estimation used this dataset for evaluation. Second, there can be a
risk that the task of estimating story points is a trivial task that does
not require models for automatic inference. GPT2SP [12] conducted
a study on a group of software developers and the study shows
that story point estimation is a challenging task even with senior
developers. Third, there can be an alternative graph generation
tool from natural language description for RQ7 such as NLTK Bllip
parser [4] and POSIT [21]. We already checked these tools. NLTK
Bllip parser hasn’t been updated for more than 7 years and POSIT
didn’t support tree and dependency edges generation. We conclude
that StanfordCoreNLP [18] is the best tool for our experiment.

10 CONCLUSION
In this work, we propose HeteroSP, a framework for predicting story
points from the software issues dataset proposed by Choetkiertikul
et al [8]. To overcome the problems of large size vocabulary and
the nature of issues’ descriptions which are mixed of text and code,
we provide an approach to representing issues’ descriptions by a
heterogeneous graph. By the evaluation, we show that HeteroSP
performs best with cross-project cross repository estimation over
GPT2SP and Deep-SE. Moreover, our approach requires less than
600 seconds for three steps: initial node embedding, graph genera-
tion, and model construction, which is much more efficient than
deep learning-based approaches like Deep-SE [8]. In future research,
we will work on improving the performance of our model by other
strategies of graph representation and text normalization. Another
direction for this research problem is to integrate other informa-
tion along with textual information on software issues, such as the
source code of software repositories.

ACKNOWLEDMENTS
We acknowledge the authors of GPT2SP and Deep-SE for provid-
ing replication packages. We thank Instructor Mouly Kumar from
the Department of Electrical and Computer Engineering and Dr.
Theresa Windus from the Department of Chemistry of Iowa State
University (ISU) for hiring and providing funding support for the
first author. We thank Ms. Nicole Lewis and the Department of
Computer Science of ISU for providing a working environment for
this project. We are grateful to Dr. Frank Proschan, an alumnus of
the University of Texas at Austin, Dr. Susan Bayly from the Univer-
sity of Cambridge, and some other friends of the first author for
carefully proofreading and revising the writing of this paper.

Heterogeneous Graph Neural Networks for Software Effort Estimation

ESEM ’22, September 19–23, 2022, Helsinki, Finland

[25] Hung Phan, Hoan Anh Nguyen, Tien N. Nguyen, and Hridesh Rajan. 2017.
Statistical Learning for Inference between Implementations and Documenta-
tion. In 2017 IEEE/ACM 39th International Conference on Software Engineer-
ing: New Ideas and Emerging Technologies Results Track (ICSE-NIER). 27–30.
https://doi.org/10.1109/ICSE-NIER.2017.9

[26] Hung Phan, Hoan Anh Nguyen, Ngoc M. Tran, Linh H. Truong, Anh Tuan
Nguyen, and Tien N. Nguyen. 2018. Statistical Learning of API Fully Qualified
Names in Code Snippets of Online Forums. In Proceedings of the 40th International
Conference on Software Engineering (Gothenburg, Sweden) (ICSE ’18). Association
for Computing Machinery, New York, NY, USA, 632–642. https://doi.org/10.
1145/3180155.3180230

[27] Hung Phan, Arushi Sharma, and Ali Jannesari. 2021. Generating Context-Aware
API Calls from Natural Language Description Using Neural Embeddings and
Machine Translation. In 36th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2021 - Workshops, Melbourne, Australia, November
15-19, 2021. IEEE, 219–226. https://doi.org/10.1109/ASEW52652.2021.00050
[28] Simone Porru, Alessandro Murgia, Serge Demeyer, Michele Marchesi, and
Roberto Tonelli. 2016. Estimating Story Points from Issue Reports. In Pro-
ceedings of the The 12th International Conference on Predictive Models and Data
Analytics in Software Engineering (Ciudad Real, Spain) (PROMISE 2016). Asso-
ciation for Computing Machinery, New York, NY, USA, Article 2, 10 pages.
https://doi.org/10.1145/2972958.2972959

[29] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-

proving language understanding by generative pre-training. (2018).

[30] Ritesh Tamrakar and Magne Jørgensen. 2012. Does the use of Fibonacci numbers
in planning poker affect effort estimates?. In 16th International Conference on
Evaluation Assessment in Software Engineering (EASE 2012). 228–232. https:
//doi.org/10.1049/ic.2012.0030

[31] Vali Tawosi, Afnan A. Al-Subaihin, Rebecca Moussa, and Federica Sarro. 2022. A
Versatile Dataset of Agile Open Source Software Projects. CoRR abs/2202.00979
(2022). arXiv:2202.00979 https://arxiv.org/abs/2202.00979

[32] Vali Tawosi, Rebecca Moussa, and Federica Sarro. 2022. Deep Learning for Agile
Effort Estimation Have We Solved the Problem Yet? CoRR abs/2201.05401 (2022).
arXiv:2201.05401 https://arxiv.org/abs/2201.05401

[33] Ali TehraniJamsaz, Mihail Popov, Akash Dutta, Emmanuelle Saillard, and Ali
Jannesari. 2022. Learning Intermediate Representations using Graph Neural
Networks for NUMA and Prefetchers Optimization. https://doi.org/10.48550/
ARXIV.2203.00611

[34] Kiran K. Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li.
2018. Attention-based Graph Neural Network for Semi-supervised Learning.
arXiv:1803.03735 [stat.ML]

[35] Muhammad Usman, Emilia Mendes, Francila Weidt, and Ricardo Britto. 2014.
Effort Estimation in Agile Software Development: A Systematic Literature Review.
In Proceedings of the 10th International Conference on Predictive Models in Software
Engineering (Turin, Italy) (PROMISE ’14). Association for Computing Machinery,
New York, NY, USA, 82–91. https://doi.org/10.1145/2639490.2639503

[36] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Graph Attention Networks.

Pietro Liò, and Yoshua Bengio. 2018.
arXiv:1710.10903 [stat.ML]

[37] Wenhan Wang, Kechi Zhang, Ge Li, and Zhi Jin. 2020. Learning to Represent Pro-
grams with Heterogeneous Graphs. CoRR abs/2012.04188 (2020). arXiv:2012.04188
https://arxiv.org/abs/2012.04188

[38] Sixing Yu, Arya Mazaheri, and Ali Jannesari. 2021. Auto Graph Encoder-Decoder

for Neural Network Pruning. arXiv:2011.12641 [cs.CV]

[39] Sixing Yu, Arya Mazaheri, and Ali Jannesari. 2021. GNN-RL Compression:
Topology-Aware Network Pruning using Multi-stage Graph Embedding and
Reinforcement Learning. arXiv:2102.03214 [cs.CV]

REFERENCES
[1] [n.d.]. Article on L1 Loss Function. https://tinyurl.com/yr9tzuuj. Accessed:

2022-03-23.

[2] [n.d.]. Definition of Agile development process. https://tinyurl.com/y6fp2mby.

Accessed: 2022-03-23.

[3] [n.d.]. Definition of Man-Hour metric. https://tinyurl.com/ydkracue. Accessed:

2022-6-20.

[4] [n.d.]. NLTK Bllip Parser. https://tinyurl.com/2syp938y. Accessed: 2021-10-21.
[5] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2017. Learn-
CoRR abs/1711.00740 (2017).

ing to Represent Programs with Graphs.
arXiv:1711.00740 http://arxiv.org/abs/1711.00740

[6] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

Enriching Word Vectors with Subword Information. arXiv:1607.04606 [cs.CL]

[7] Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr
Polozov. 2018. Generative Code Modeling with Graphs. CoRR abs/1805.08490
(2018). arXiv:1805.08490 http://arxiv.org/abs/1805.08490

[8] Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya
Ghose, and Tim Menzies. 2016. A deep learning model for estimating story points.
CoRR abs/1609.00489 (2016). arXiv:1609.00489 http://arxiv.org/abs/1609.00489

[9] Mike Cohn. 2005. Agile estimating and planning. Pearson Education.
[10] Rene Avalloni de Morais. 2021. Deep learning based models for software effort

estimation using story points in agile environments. (2021).

[11] Matthias Fey and Jan Eric Lenssen. 2019. Fast Graph Representation Learning

with PyTorch Geometric. arXiv:1903.02428 [cs.LG]

[12] Michael Fu and Chakkrit Tantithamthavorn. 2022. GPT2SP: A Transformer-
Based Agile Story Point Estimation Approach. IEEE Transactions on Software
Engineering (2022), 1–1. https://doi.org/10.1109/TSE.2022.3158252

[13] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous
Graph Transformer. CoRR abs/2003.01332 (2020). arXiv:2003.01332 https://arxiv.
org/abs/2003.01332

[14] Lianzhe Huang, Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2019.
Text Level Graph Neural Network for Text Classification. CoRR abs/1910.02356
(2019). arXiv:1910.02356 http://arxiv.org/abs/1910.02356

[15] Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe
Wang, Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou. 2021. Could
graph neural networks learn better molecular representation for drug discovery?
A comparison study of descriptor-based and graph-based models. Journal of
Cheminformatics 13, 1 (2021), 12. https://doi.org/10.1186/s13321-020-00479-8
[16] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary Models for
Source Code. CoRR abs/2003.07914 (2020). arXiv:2003.07914 https://arxiv.org/
abs/2003.07914

[17] Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. CoRR abs/1609.02907 (2016). arXiv:1609.02907
http://arxiv.org/abs/1609.02907

[18] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven
Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language
processing toolkit. In Proceedings of 52nd annual meeting of the association for
computational linguistics: system demonstrations. 55–60.

[19] Aravind Nair, Avijit Roy, and Karl Meinke. 2020. FuncGNN: A Graph Neural
Network Approach to Program Similarity. In Proceedings of the 14th ACM / IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM) (Bari, Italy) (ESEM ’20). Association for Computing Machinery, New York,
NY, USA, Article 10, 11 pages. https://doi.org/10.1145/3382494.3410675
[20] Anh Tuan Nguyen, Peter C. Rigby, Thanh Van Nguyen, Mark Karanfil, and
Tien N. Nguyen. 2017. Statistical Translation of English Texts to API Code
Templates. In Proceedings of the 39th International Conference on Software Engi-
neering Companion (Buenos Aires, Argentina) (ICSE-C ’17). IEEE Press, 331–333.
https://doi.org/10.1109/ICSE-C.2017.81

[21] Profir-Petru Pârtachi, Santanu Kumar Dash, Christoph Treude, and Earl T. Barr.
2020. POSIT: Simultaneously Tagging Natural and Programming Languages. In
Proceedings of the ACM/IEEE 42nd International Conference on Software Engineer-
ing (Seoul, South Korea) (ICSE ’20). Association for Computing Machinery, New
York, NY, USA, 1348–1358. https://doi.org/10.1145/3377811.3380440

[22] Hung Phan and Jannesari Ali. 2022. Replication Package of HeteroSP. https:

//pdhung3012.github.io/heterosp.github.io/. Accessed: 2022-6-20.

[23] Hung Phan and Ali Jannesari. 2020. Statistical Machine Translation Outper-
forms Neural Machine Translation in Software Engineering: Why and How.
In Proceedings of the 1st ACM SIGSOFT International Workshop on Representa-
tion Learning for Software Engineering and Program Languages (Virtual, USA).
Association for Computing Machinery, New York, NY, USA, 3–12.
https:
//doi.org/10.1145/3416506.3423576

[24] Hung Phan and Ali Jannesari. 2022. Story Point Effort Estimation by Text Level

Graph Neural Network. https://doi.org/10.48550/ARXIV.2203.03062

