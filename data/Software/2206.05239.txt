2
2
0
2

n
u
J

0
1

]

G
L
.
s
c
[

1
v
9
3
2
5
0
.
6
0
2
2
:
v
i
X
r
a

StructCoder: Structure-Aware Transformer for Code
Generation

Sindhu Tipirneni, Ming Zhu, Chandan K. Reddy
Department of Computer Science, Virginia Tech, Arlington, VA
sindhut@vt.edu, mingzhu@vt.edu, reddy@cs.vt.edu

Abstract

There has been a recent surge of interest in automating software engineering tasks
using deep learning. This work addresses the problem of code generation where
the goal is to generate target code given source code in a different language or a
natural language description. Most of the state-of-the-art deep learning models
for code generation use training strategies that are primarily designed for natural
language. However, understanding and generating code requires a more rigorous
comprehension of the code syntax and semantics. With this motivation, we de-
velop an encoder-decoder Transformer model where both the encoder and decoder
are trained to recognize the syntax and data ﬂow in the source and target codes,
respectively. We not only make the encoder structure-aware by leveraging the
source code’s syntax tree and data ﬂow graph, but we also ensure that our decoder
preserves the syntax and data ﬂow of the target code by introducing two auxiliary
tasks: AST (Abstract Syntax Tree) paths prediction and data ﬂow prediction. To
the best of our knowledge, this is the ﬁrst work to introduce a structure-aware
Transformer decoder to enhance the quality of generated code by modeling target
syntax and data ﬂow. The proposed StructCoder model achieves state-of-the-art per-
formance on code translation and text-to-code generation tasks in the CodeXGLUE
benchmark.1

1

Introduction

Code generation is the problem of generating code given input code that is either imperfect or in a
different language, or generating code from a natural language description. In this paper, we consider
the problem of generating target code given source code in a different language (code translation)
or a natural language description (text-to-code generation). Code translation has applications in
migrating legacy codebases to contemporary programming languages and porting existing software to
various other platforms. A good text-to-code generation model can help in speeding up the software
development process. Traditional code generation tools have been designed using hand-crafted rules
based on the Abstract Syntax Tree (AST). The design of such tools demands a lot of time and effort
as it requires proﬁciency in both source and target languages. Moreover, such tools are speciﬁc to the
particular language pair that they are designed for.

Since natural language generation using deep learning has achieved great success in recent years, it
is natural to exploit similar deep learning based approaches for code generation as well. However,
the code domain faces a unique set of challenges. Since the generated code is to be understood
by a machine as opposed to a human, it is even more important for the generated code (compared
to natural language) to adhere to a speciﬁc syntax. Moreover, since a minor change in code could
alter its function, it is also critical to preserve the semantic information from the source code
during translation. To generate syntactically correct code, some of the existing approaches for code

1The codes are available at https://github.com/reddy-lab-code-research/StructCoder

Preprint. Under review.

 
 
 
 
 
 
generation leveraged the AST structure by learning to generate inorder traversal of AST [10], learning
to generate production rules for AST based on a grammar, encoding AST paths using RNNs [2], and
using AST-based attention [10, 9] in sequence models. Guo et al. [6] hypothesize that Data Flow
Graph (DFG), which contains more semantic information and is less complex than AST, is a more
useful structure to learn code representations. They incorporate DFG into the Transformer encoder
by appropriately masking the attention matrix. In this work, we introduce a Transformer-based
encoder-decoder model which incorporates structure more comprehensively by modeling both the
AST and DFG information in both the source and target codes.

Our model, StructCoder, consists of a Transformer encoder with a structure-aware self-attention
framework to incorporate source syntax and data ﬂow when given a code input. Code generation also
heavily relies on the decoder to generate code that is syntactically correct and preserves the semantics
of the source code. However, none of the existing state-of-the-art Transformer-based methods utilize
the target code structure in the decoder. Structcoder is the ﬁrst to incorporate a structure-aware
Transformer decoder that models both the target syntax and data ﬂow. We not only incorporate source
AST and DFG into the encoder, but also “teach" the decoder to learn the target syntax and data ﬂow
by introducing novel AST and DFG related tasks. Particularly, we train the decoder to predict node
types on all the root-leaf paths in the target AST and also to predict the DFG edges.

Similar to pretrained language models [4, 11, 17, 25], pretrained code models using Transformer
[5, 1, 21, 28, 24] have resulted in signiﬁcant performance gains on code-related tasks. While some
pretext tasks like Masked Language Modeling (MLM) and Replaced Token Detection (RTD) only
pretrain the encoder, other pretext tasks like Denoising Autoencoding (DAE) and Back Translation
(BT) jointly pretrain both the encoder and decoder. StructCoder falls in the latter category and is
pretrained using a structure-based DAE task. The main contributions of this work are listed below:

1. We develop a Transformer-based encoder-decoder model called StructCoder for code gener-

ation where both encoder and decoder are structure-aware.
(a) The encoder incorporates a structure-aware self-attention framework to model source

code structure.

(b) The decoder is trained to recognize target syntax and data ﬂow via two novel auxiliary

tasks: AST paths prediction and data ﬂow prediction.

2. We pretrain StructCoder using a structure-based DAE task where the input code as well as
its AST and DFG are partially corrupted and the model is trained to generate the original
input code and also perform the auxiliary tasks.

3. Our experiments demonstrate that the proposed model achieves state-of-the-art perfor-
mance on the code translation and text-to-code generation tasks in the CodeXGLUE [12]
benchmark.

2 Related Work

Leveraging structure to generate code To leverage code structure in deep models, many approaches
have utilized AST structure. Some approaches modeled code completion as a language modeling
task by ordering the code tokens using a depth-ﬁrst traversal of the AST. Li et al. [10] used an LSTM
appended with parent-child attention while Alon et al. [2] encoded each root-to-leaf path with an
LSTM. Kim et al. [9] used the Transformer to encode the sequenced AST by encoding AST paths
into self-attention. For text-to-code generation, Rabinovich et al. [16] proposed a modular decoder
to recursively generate target AST. Yin and Neubig [26], Brockschmidt et al. [3], Sun et al. [22]
construct ASTs by generating production rules based on a grammar.

Self-supervised models for code The recent state-of-the-art results on most natural language gener-
ation tasks are obtained by pretraining huge deep learning models on large datasets with carefully
designed pretext tasks. Since code generation is very similar to text generation and there is abundant
unsupervised code data available through open source code repositories, pretraining code generation
models using similar pretext tasks has been successful. Most recent state-of-the-art pretrained models
for code utilize the Transformer [23] architecture and are discussed below.

CodeBERT [5] performs encoder-only pretraining using Masked Language Modeling and Replaced
Token Detection as pretext tasks on the CodeSearchNet dataset. Transcoder [20] is an unsupervised
translation model which pretrains both encoder and decoder using Denoising Autoencoding and

2

Back-Translation with only monolingual datasets. PLBART [1] is pretrained with DAE objective
using 680M Java and Python functions. DOBF [21] attempts to understand code structure with
a deobfuscation pretext task where every occurrence of a sampled identiﬁer is replaced by an
uninformative token. Code Transformer [28] modiﬁes the attention computations in the encoder
according to AST-based distances. CodeT5 [24] pretrains T5 model [18] with code data in 8
programming languages and includes identiﬁer-aware objective in the training, which helps maintain
the correctness of the code. Zhu et al. [27] improve code translation performance by introducing
a ﬁne-grained snippet-level translation task during pretraining. GraphCodeBERT [6] utilizes code
structure in the form of Data Flow Graph (DFG) which contains semantic information as opposed to
the syntatic information in AST. However, the decoder is completely unaware of the code structure in
all of the above methods. Our model advances the domain of code generation by being the ﬁrst one
to train a structure-aware Transformer encoder and decoder by modeling both syntax and data ﬂow.
A tabular summary of the pretext tasks and code structures used by the above Transformer-based
methods is provided in Table 1.

Table 1: A summary of the recent pre-trained models for code generation. (Abbreviations: DFG: Data
Flow Graph, MLM: Masked Language Modeling, DAE: Denoising Autoencoding, RTD: Replaced
Token Detection, BT: Back Translation, EP: DFG Edge Prediction, NA: Alignment prediction
between code tokens and DFG nodes, DOBF: Deobfuscation, IT: Identiﬁer Tagging, MSP: Masked
Span Prediction, MIP: Masked Identiﬁer Prediction, MuST: Multilingual Snippet Translation.)

Model

Encoder-only
pretraining
MLM, RTD

CodeBERT[5]
GraphCodeBERT[6] MLM, EP, NA
Transcoder[20]
PLBART[1]
DOBF[21]
CodeT5[24]

MLM
-
-
IT

MuST[27]
StructCoder (ours)

-

3 StructCoder

Encoder-Decoder
pretraining
-
-
DAE, BT
DAE
DOBF
MSP, MIP, NL-PL
dual generation
DAE, MuST
structure-based
DAE, NL-PL dual
generation

Encoder structure-
awareness
-
DFG
-
-
-
Identiﬁers

Decoder structure-
awareness
-
-
-
-
-
Identiﬁers

-
AST, DFG

-
AST, DFG

StructCoder is a Transformer based encoder-decoder model where both encoder and decoder are
structure-aware. We build our model using T5 architecture and add the relevant components for
modeling code structure. For code inputs, the encoder (see Section 3.2) inputs the tokenized source
code sequence along with its AST and DFG and employs structure-aware self-attention. The structure-
aware decoder (see Section 3.3) simultaneously learns to generate the target code sequence as well as
to perform target AST and DFG related tasks.

3.1 Preliminaries

A Code can be a function or a program, and is represented as a sequence of tokens S = (s1, ..., s|S|).
A code S has a corresponding AST represented as T = (N, Nleaf , r, p(.), Last), where N is the
set of nodes in the AST, Nleaf = {l1, ..., l|Nleaf |} ⊂ N is the subset of leaf nodes, r ∈ N is
the root node, p : N − r −→ N is a mapping such that p(n) denotes the parent of node n, and
Last ∈ {0, 1}|S|×|Nleaf | is a linking matrix such that Last
ij = 1 if and only if token si is part of leaf
lj. Each node n ∈ N has a type denoted by n.type.

A code S also has a corresponding DFG represented as G = (V, D, Ldf g), where V =
{v1, v2, ..., v|V |} is the set of variables extracted from code S, and D ∈ {0, 1}|V |×|V | is the adjacency
matrix where Dij = 1 if and only if value of vi is obtained from vj, and Ldf g ∈ {0, 1}|S|×|V | is a
linking matrix such that Ldf g

ij = 1 if and only if variable vj is derived from token si.

3

Figure 1: Structure-aware encoder: The input sequence to the encoder consists of source code
concatenated with the AST leaves and DFG variables, where the AST leaves are embedded using the
root-leaf paths in the AST. The modiﬁed structure-aware self-attention mechanism of this Transformer
encoder utilizes code-AST/DFG linking information, leaf-leaf similarities in the AST, and the
(asymmetric) DFG adjacency matrix to compute the attention matrix.

The goal of code translation is to transform a code S = (s1, ..., s|S|) in a source language to code
T = (t1, ..., t|T |) in a different target language such that the translated code T solves exactly the
same problem as input code S but in a different (target) language. In text-to-code generation, the
goal is to generate target code T from a natural language description.

3.2 Encoder

Given source code S, its corresponding AST T , and DFG G, the input sequence to the encoder is
< CLS >, s1, .., s|S|, < SEP >, l1, ..., l|Nleaf |, v1, ..., v|V |
which consists of the code tokens, special tokens < CLS > and < SEP >, AST leaves, and DFG
variables. For text input, the leaves and variables are simply ignored in the input. The encoder
architecture is illustrated in Figure 1 and is described in detail below.

Input embedding: As StructCoder consists of a Transformer encoder, each token in the input
sequence has to be embedded in Rd. We embed the code tokens along with special tokens by using a
lookup table, and use a default embedding for all DFG variables. The DFG information will be used
by the encoder in structure-aware self-attention. We compute the embedding of a leaf l in an AST as
a function of the path from the root to the leaf l in the AST.

Let (r1, r2, ..., r|l|) be the nodes on the path from root r = r1 to leaf l = r|l|. We utilize node-type
embedding Etype(·) ∈ Rd to encode a node’s semantics and a node-height embedding Eheight(·) ∈
Rd to encode the order of nodes on this path. The leaf embedding E(l) is computed as

E(l) =

|l|
(cid:88)

i=1

Etype(ri.type) (cid:12) Eheight(|l| − i) ∈ Rd

(1)

where (cid:12) denotes element-wise multiplication.

Structure-aware self-attention: Since the input contains DFG and AST which consist of structural
information, the traditional attention computation using (relative) positional embeddings which
capture sequential ordering information is not sufﬁcient. Hence, we propose structure-aware self-
attention which computes attention scores between tokens based on the structural relations between
them.

Code-code: Following T5, we compute attention scores (before softmax) between code tokens
by adding the query-key dot product with weights Wq, Wk ∈ Rdk×d and a lookup embedding

4

Figure 2: Structure-aware decoder generates the next token in the target code as well as predicts the
node types on the root-leaf path to the leaf containing this token in the target AST and also the DFG
edges incident on this token.

φ : Z≥0 −→ R for relative position. Denoting embedding of x by Ex, we have

A(si, sj) = ET
si

W T

q WkEsj + φ(|i − j|)

(2)

Leaf-leaf: To calculate attention scores between leaves, we introduce a similarity-based transformation
to replace the relative positional embedding in equation 2. Let (ri
1, ..., ri
|li|) be the nodes on the path
from root to leaf li. We deﬁne similarity between two leaves li and lj as

sim(li, lj) = log








(cid:18) min(|li|,|lj |)
(cid:80)
k=1

(cid:19)2

1(ri

k = rj
k)

|li||lj|








(cid:18) min(|li|,|lj |)
(cid:88)

= 2 log

k=1

(cid:19)

1(ri

k = rj
k)

− log |li| − log |lj|

(3)

(4)

which is based on the number of common nodes on the paths from root to leaves l1 and l2. The
attention scores between the leaves are then computed as

A(li, lj) = ET
li

W T

q WkElj + (wa sim(li, lj) + wb) where wa, wb ∈ R

(5)

Variable-variable: Following Guo et al. [6], the attention scores between DFG nodes are computed
using only the query-key dot product and are set to −∞ if corresponding edges are absent in the
DFG.

A(vi, vj) =

W T

q WkEvj

(cid:26)ET
vi
−∞

if Dij = 1
else

(6)

Code-leaf/variable: For interaction between code tokens and AST leaves (or DFG variables), we only
compute the query-key dot product and do not use any positional information. We set the attention
score to −∞ for cases where the leaf (or variable) is not linked to the code token. We show the
equations only for interactions between code tokens and leaves as those for interactions between code
tokens and variables are similar.
(cid:26)ET
si
−∞

; A(lj, si) =

if Last
else

if Last
else

A(si, lj) =

ET
lj
−∞

q WkEsi

q WkElj

ij = 1

ij = 1

W T

W T

(7)

(cid:40)

The special tokens < CLS > and < SEP > are treated just like code tokens and are assumed to be
linked to all leaves and variables.

3.3 Decoder

We include additional layers at the end of T5 decoder for the auxiliary tasks of AST paths prediction
and data ﬂow prediction that are introduced in this section. Figure 2 illustrates the structure-aware

5

decoder which predicts the next target code token along with the node types on the root-leaf path to
this token in the target AST and also the data ﬂow relations between this token and all past tokens.

Let h1, h2, ..., h|T | be the hidden states generated by the Transformer decoder. Decoders of existing
transformer models including T5 employ a linear layer with weights W ∈ R|V|×d followed by
softmax transformation to extract a probability distribution pi on the token vocabulary space V for
the ith position.

And the sequence generation task is trained using language modeling loss as shown below for one
sample.

pi = sof tmax (W hi)

(8)

Llm = −

|T |
(cid:88)

i=1

log pi(ti)

(9)

where pi(ti) refers to the predicted probability for true target token ti at the ith position.
In addition to sequence generation, StructCoder also learns target syntax using AST paths prediction
task, and learns to match target DFG using a data ﬂow prediction task.

AST paths prediction (APP): In this task, the goal is to enable the decoder to be aware of all
root-leaf paths in the target AST. Since the type attribute of a node captures important syntactic
information, we predict the type of each ancestor on each root-leaf path.
Let li be the leaf node containing the ith target token ti and let (ri
|li|) be the nodes on the
root-li path. To predict type of node ri
k (which is at height |li| − k in the tree), we use a linear layer
with weights Wast (|li|−k) ∈ R|Y|×d followed by a softmax transformation to predict a probability
distribution on the set of node types Y.

1, ..., ri

The APP loss for a sample is given by

past
ik = sof tmax(Wast (|li|−k)hi)

Lapp = −

|T |
(cid:88)

|li|
(cid:88)

i=1

k=1

log past

ik (ri

k.type)

(10)

(11)

Data ﬂow prediction (DFP): In this task, the decoder learns to predict all the data ﬂow edges in
target code. The probability pdf g
ij of data ﬂow from jth to ith position in target code sequence is
computed using an asymmetric transformation since data ﬂow is directed.

pdf g
ij = σ (hT

i U T

df gVdf ghj + uT

df ghi + vT

df ghj + wdf g)

(12)

Suppose G = (V, D, L) is the true target DFG. There is a data ﬂow from jth to ith position in target
sequence if and only if “target DFG contains variables vj(cid:48), vi(cid:48) such that variable vj(cid:48) is derived from
tj, variable vi(cid:48) is derived from ti, and value of variable vi(cid:48) is derived from vj(cid:48)". Thus, the DFP loss
for a sample can be written as

|T |
(cid:88)

|T |
(cid:88)

Ldf p = −

(cid:8) 1(cond) log pdf g

ij + 1(¬ cond) log (1 − pdf g

ij ) (cid:9)

i=1

j=1
cond = (∃ vi(cid:48), vj(cid:48) ∈ V such that Di(cid:48)j(cid:48) = Ldf g

ii(cid:48) = Ldf g

jj(cid:48) = 1)

(13)

where

3.4 Pretraining

We pretrain StructCoder on a structure-based DAE task along with NL-PL bimodal dual generation
to generate code from text and vice-versa. For the denoising task, we ﬁrst corrupt random spans in a
code sequence by replacing with <MASK> or a random token, or deleting it. The span lengths are
sampled from a Poisson distribution of mean 3.5 and we corrupt 35% of the code tokens in total,
similar to Ahmad et al. [1]. To improve the understanding of code structure, we also randomly drop
35% of the DFG variables and AST leaves, and 35% of the ancestors for each leaf from the input to

6

Table 2: Results on code translation tasks from CodeXGLUE benchmark. (*Since CodeT5 is a
competitive baseline and did not report CodeBLEU in their paper, we tested this model using their
ﬁnetuned checkpoint and provided the results.)

Java-C#

C#-Java

Naive Copy
Transformer
RoBERTa (code)
CodeBERT
GraphCodeBERT
PLBART
CodeT5*
StructCoder

BLEU xMatch CodeBLEU BLEU xMatch CodeBLEU
18.54
55.84
77.46
79.92
80.58
83.02
83.88
85.03

34.94
61.59
80.18
79.41
-
85.27
85.51
86.10

0.00
37.90
57.90
58.80
58.80
65.00
67.50
67.70

42.20
63.74
83.07
85.10
-
87.92
87.38
88.41

18.69
50.47
71.99
72.14
72.64
78.35
79.71
80.73

0.00
33.00
56.10
59.00
59.40
64.60
64.70
66.60

Table 3: Results on text-to-code generation task from CodeXGLUE benchmark.

GPT-2
CodeGPT
CodeGPT-adapted
PLBART
CoTexT
CodeT5
StructCoder

BLEU xMatch CodeBLEU
17.35
28.69
32.79
36.69
37.40
40.73
40.91

29.69
32.71
35.98
38.52
40.14
43.20
44.77

25.37
18.25
20.10
18.75
20.10
22.30
22.35

StructCoder. The model is then trained to predict the uncorrupted code along with the AST root-leaf
paths and data ﬂow edges. We pretrain StructCoder on a randomly chosen subset of 300K samples
from CodeSearchNet 2 [7] belonging to four languages: Python, PHP, JavaScript, and Java. We
initialize our model for pretraining with CodeT5’s weights except for the AST and DFG related
weights which are randomly initialized.

4 Experiments

We evaluate StructCoder on the code translation and text-to-code generation tasks from the
CodeXGLUE 3 [12] benchmark and compare it with state-of-the-art models. For each task, we
use the metrics from the CodeXGLUE leaderboard which include BLEU [13] score which measures
n-gram overlap, exact match which checks if the prediction is the same as ground truth, and Code-
BLEU [19] which combines BLEU score with keywords-based weighted n-gram match as well as
syntax and semantic matches based on AST and DFG. The implementation details of our model are
provided in the appendix.

4.1 Code Translation

This dataset consists of two tasks for translating between Java and C# functions in either direction
and contains 10K training samples, 500 validation samples, and 1000 test samples. Table 2 presents
the results of StructCoder alongside the baselines on the two code translation tasks. The Naive Copy
baseline simply copies source code to target and the Transformer model involves no pretraining.
RoBERTa (code) [12], CodeBERT, and GraphCodeBERT involve encoder-only pretraining while
PLBART and CodeT5 incorportate encoder-decoder pretraining like StructCoder. StructCoder
achieves the best results on the two translation tasks which can be attributed to the structure-aware
encoder-decoder design of our model. From Table 2, we observe that the encoder-decoder pretraining
of PLBART, CodeT5, and StructCoder is very beneﬁcial to code translation. Also, the encoder-only
pretrained models improve over Transformer by a huge margin. GraphCodeBERT which utilizes DFG
offers minor improvements over CodeBERT and we also observed in our ablation study (provided in
the appendix) that DFG-related components contribute little to the performance gains of StructCoder
compared to AST-related components.

2Available at https://github.com/github/CodeSearchNet under the MIT license.
3The datasets are available at https://github.com/microsoft/CodeXGLUE under the MIT license.

7

Figure 3: Case study: An example from Java-C# translation task where StructCoder is able to
accurately predict the target code while CodeT5 fails. (Red text indicates errors made by CodeT5 and
blue text indicates correctly predicted code by StructCoder where baseline generates errors. The blue
arrows show some of the correctly predicted (probability > 97th percentile) data ﬂow edges relevant
to the colored text.)

4.2 Text-to-code Generation

The text-to-code generation task uses the CONCODE [8] dataset and the goal here is to generate
a Java function given a natural language description. This dataset contains 100K training samples,
2K validation samples, and 2K test samples. Table 3 presents the results of our model alongside
the baselines on the text-to-code generation task. Among the baselines, GPT-2 [17] is pretrained
on natural language to predict next token, CodeGPT [12] is pretrained from scratch like GPT-2 but
using code data, CodeGPT-adapted [12] is pretrained from GPT-2 initialization using code data, and
CoTexT [15] pretrains the T5 model further on code data using MSP objective. The decoder-only
baselines which include GPT-2 based models are outperformed by the rest which are all encoder-
decoder models. StructCoder again achieves the best performance on all metrics for the text-to-code
generation task.

4.3 Model Analysis

APP and DFP We measure the performance of StructCoder on the auxiliary tasks of APP and DFP
as follows. When predicting the next target token, we use the ground truth for target sequence until
the previous step as input to the decoder. The decoder then predicts the next token as well as the DFG
edges incident on this token and the types of nodes on the path from root to the leaf node containing
this token in the AST. On Java-C# translation, StructCoder achieves 76.6% accuracy on APP task
and 74.8% average precision on DFP task where positive class prevalence is just 0.8%. On C#-Java
translation, StructCoder achieves 76.8% accuracy on APP task and 33.6% average precision on DFP
task where positive class prevalence is just 0.5%. For both the translation tasks, there are 291 classes
for node type in APP task.

Case Study Figure 3 shows an example from Java-C# translation task with predictions from Struct-
Coder and the best baseline CodeT5. We observe that our structure-aware encoder-decoder archi-
tecture is able to correctly generate the target code where CodeT5 fails, which can be explained by
inspecting the predictions of the two auxiliary tasks. Referring to Figure 3, CodeT5 generates both
the ‘for’ loops with index ‘i’, leaving variable ‘c’ undeﬁned. It also misses the ﬁrst ‘if’ statement and
creates a syntax error from unbalanced braces. On the other hand, StructCoder correctly generates
the for loops by deﬁning variable ‘c’ and the model predicts (with probability greater than 97th
percentile) most of the DFG edges incident on the variable ‘c’ inside these for loops and also in the
ﬁrst ‘if’ statement. Also, for token ‘[]’ in args, the correct parent node type ‘array rank speciﬁer’ is in
the top two predicted node types.

8

Limitations Though StructCoder achieves state-of-the-art results on CodeXGLUE leaderboard, our
model suffers from the following limitations. (i) The additional tokens from AST and DFG in the
encoder input approximately doubles the input sequence length. Since the complexity of attention
layers in the Transformer grows quadratically in input length, our model may not be usable with
longer codes given limited resources. Thus, new techniques need to be developed to adapt StructCoder
to longer codes. (ii) Our model generates syntactically correct codes in most cases but a few of the
generated codes cannot be parsed. A post-processing module to ﬁx remaining syntax errors may be
useful. (iii) For code generation tasks, there may be multiple correct answers but the datasets used
contain a single ground truth. Thus, a better way to evaluate code generation models is to check if
the target code parses and follows the desired logic, which needs the development of new evaluation
measures.

5 Conclusion

This work proposes a structure-aware Transformer encoder-decoder model called StructCoder for
code generation. Our encoder employs a structure-aware self attention mechanism to model AST
and DFG relations in source code and our decoder is trained to recognize target syntax and data ﬂow
using two novel auxiliary tasks to predict node types on all root-leaf AST paths and data ﬂow edges
in target code. We also pretrained our model using a structure-based DAE task. Experiments on
code translation and text-to-code generation tasks from the CodeXGLUE benchmark demonstrate the
performance gains of StructCoder over state-of-the-art baselines. We believe that this work would
encourage future research in this ﬁeld to give careful consideration to code structure while building
models for code generation.

Broader Impact

Although automated code generation can potentially beneﬁt the development and migration of
software, there are risks associated with it. First of all, the model is not capable of taking into
consideration constraints like security, efﬁciency, and modularization when generating code. Thus,
deploying model-generated code can introduce vulnerabilities in complex systems, and increase the
energy cost and emissions. Furthermore, the maintenance of the generated code can be challenging if
it is less modularized.

Second, the performance improvements of the code generation models largely rely on scaling-up
of both the model and the training, which require signiﬁcant amount of computational resources.
Individuals, small organizations, and academic institutes usually cannot afford the large-scale training
of such models, while big companies have a natural advantage in this aspect. Therefore, the advances
in this domain might beneﬁt the big businesses more than general audience, which limits the societal
value of it.

Third, the deployment of automated code generation tools requires reforming the current skill sets for
software engineering jobs. But once the users are well-educated about the usage and maintenance of
such systems and the security risks associated with them, the software development process should
become more efﬁcient.

References

[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Uniﬁed pre-
training for program understanding and generation. In Kristina Toutanova, Anna Rumshisky,
Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tan-
moy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2655–2668. As-
sociation for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.211. URL
https://doi.org/10.18653/v1/2021.naacl-main.211.

[2] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In

International Conference on Machine Learning, pages 245–256. PMLR, 2020.

9

[3] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polozov. Genera-

tive code modeling with graphs. arXiv preprint arXiv:1805.08490, 2018.

[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[5] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained model for
programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.

[6] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan
Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement,
Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. Graphcodebert: Pre-
training code representations with data ﬂow. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
URL https://openreview.net/forum?id=jLoC4ez43PZ.

[7] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436, 2019.

[8] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to

code in programmatic context. arXiv preprint arXiv:1808.09588, 2018.

[9] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees
to transformers. In 2021 IEEE/ACM 43rd International Conference on Software Engineering
(ICSE), pages 150–162. IEEE, 2021.

[10] Jian Li, Yue Wang, Michael R Lyu, and Irwin King. Code completion with neural attention and

pointer networks. arXiv preprint arXiv:1711.09573, 2017.

[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.

[12] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
understanding and generation. arXiv preprint arXiv:2102.04664, 2021.

[13] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pages 311–318, 2002.

[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library.
In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances
in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
bdbca288fee7f92f2bfa9f7012727740-Abstract.html.

[15] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal, Alec Peltekian, and Yanfang Ye.
Cotext: Multi-task learning with code-text transformer. arXiv preprint arXiv:2105.08645, 2021.
[16] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code genera-

tion and semantic parsing. arXiv preprint arXiv:1704.07535, 2017.

[17] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language

models are unsupervised multitask learners. 2019.

[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.

10

[19] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming
Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code
synthesis. arXiv preprint arXiv:2009.10297, 2020.

[20] Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsuper-
vised translation of programming languages. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.
cc/paper/2020/hash/ed23fbf18c2cd35f8c7f8de44f85c08d-Abstract.html.

[21] Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. Dobf: A deob-
fuscation pre-training objective for programming languages. arXiv preprint arXiv:2102.07492,
2021.

[22] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. Treegen: A tree-
based transformer architecture for code generation. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pages 8984–8991, 2020.

[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

[24] Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code understanding and generation. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696–8708,
2021.

[25] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in
neural information processing systems, 32, 2019.

[26] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code

generation. arXiv preprint arXiv:1704.01696, 2017.

[27] Ming Zhu, Karthik Suresh, and Chandan K Reddy. Multilingual code snippets training for

program translation. 2022.

[28] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann.
Language-agnostic representation learning of source code from structure and context. arXiv
preprint arXiv:2103.11318, 2021.

APPENDIX

A Implementation Details

We use the CodeT5 tokenizer with a vocabulary size of 32,100. As we build upon CodeT5 architecture,
both the encoder and decoder of StructCoder contain 12 T5 blocks with hidden dimension 768, and
12 attention heads in each block. During implementation, we only used ﬁrst 16 bits of the last hidden
representation from the decoder to predict DFG links and the next 128 bits for AST paths prediction.
This is done because the model learns DPP task more easily than APP task and using few bits for
these auxiliary tasks prevents overﬁtting on these tasks. We trained our model with a loss function
that combines the language modeling objective, and the APP and DFP losses with learnable weights
α1 and α2.

L = (3 − α1 − α2)Llm + α1Lapp + α2Ldf p

(14)

We initialize a1 = a2 = −4, and compute α1 = σ(a1), α2 = σ(a2). For pretraining, we initialized
the non-structure-based weights with CodeT5 pretrained model, and ran our pretraining with a batch
size to 32 for 12k steps, using AdamW optimizer with a learning rate of 5e-5. For code translation,
we ran ﬁnetuning with a batch size of 25 for 22k steps, using AdamW optimizer with a learning rate
of 5e-5. For text-to-code generation, we ran ﬁnetuning with a batch size of 32 for 100k steps, using
AdamW optimizer with a learning rate of 5e-5. For new AST node types seen during ﬁnetuning, we
initialized the weights corresponding to these new node types randomly. Also, we clipped a1, a2 to

11

have a max value of -4 during ﬁnetuning. We used beam search with beam size 10 for decoding for
all ﬁnetuning tasks. We ran validation every 500 steps and chose the checkpoint with the best BLEU
score on the validation set for testing. To facilitate minibatch training with available resources, we set
max no. of DFG variables in input to 75, max no. of AST leaves to 250, and max root-leaf path length
to 12 (by trimming paths from the root’s side). We set the max source length (no. of code tokens
when input is code) to 400 for pretraining, 320 for translation, 325 for text-to-code generation. We set
max target length to 400 for pretraining, 320 for Java-C# translation, 256 for C#-Java translation, 155
for text-to-code generation. All the hyperparameters discussed above were set either from CodeT5’s
implementation or choosing the ones with best validation performance after a few trials. We used
Pytorch [14] and Huggingface4 libraries to implement our model. The code for generating ASTs
and DFGs is built using tree-sitter 5 and is also adapted from https://github.com/microsoft/
CodeBERT/tree/master/GraphCodeBERT. The code for ﬁnetuning StructCoder is made available
at https://github.com/reddy-lab-code-research/StructCoder. We ran our experiments
using 4 RTX 8000 GPUs with 48GB memory on each GPU.

B Ablation Study

To understand the importance of the different components of StructCoder, we conducted an ablation
study on the Java-C# translation task from CodeXGLUE [12] by ignoring the structure from either
AST or DFG in either the input or the output. As pretraining each model is quite expensive, we
use CodeT5 [24] initialization for non-structure-based weights during ﬁnetuning for the ablation
study. The results are provided in Table B4. We observe that ablating any component results in
a performance drop but removing the AST from input or output results in a greater performance
drop than removing the DFG. This could be because the DFG constitutes relations between just the
variables while the AST includes root-leaf paths to all kinds of tokens including keywords, operators,
identiﬁers, punctuation, etc. (For C# (Java) codes in the CodeXGLUE translation test set, we observed
that 32% (26%) of the AST leaves are variables/identiﬁers.)

Table B4: Ablation Study: Performance on Java-C# translation by ablating different components of
the proposed model. All models are initialized using CodeT5 weights with random initialization for
structure-related weights. (Abbreviations: APP: AST Paths Prediction, DFP: Data Flow Prediction)

StructCoder
-w/o APP
-w/o AST input
-w/o DFP
-w/o DFG input

BLEU xMatch CodeBLEU
84.96
84.29
84.08
84.59
84.74

66.70
65.70
65.40
66.30
66.50

88.22
87.86
87.70
87.93
88.16

C Examples

In this section, we illustrate a few examples of text-to-code generation along with the predicted DFG
links and AST paths. See Figures C4-C6. The DFG predictions are visualized as a matrix where the
ijth cell denotes the probability of data ﬂow from jth to ith token. To visualize predicted AST paths,
for each predicted token, we indicate the predicted node types on the path starting from the root (top)
to the leaf (bottom) containing this token, vertically using colored discs.

4https://huggingface.co/transformers/
5https://github.com/tree-sitter/py-tree-sitter

12

Figure C4: An example from the concode dataset with BLEU=78.85.

Figure C5: An example from the concode dataset with BLEU=100.

13

GENERATED CODE:void function ( ) { File loc0 = new File ( mTestFolder. getRoot( ) , "srini_string" ) ;FileUtils. delete ( loc0 . getAbsolutePath( ) ) ; Assert . fail ( "srini_string" ) ; }Predicted DFG linksSOURCE:tests the fileutils#delete string method to throw an exception when trying to delete anon-existent file . concode_field_sepExpectedExceptionmExceptionconcode_elem_sepTemporaryFolder…ACTUAL CODE:void function ( ) { File loc0 = new File ( mTestFolder. getRoot( ) , "srini_string" ) ; mException. expect ( IOException. class ) ; FileUtils. delete ( loc0 . getAbsolutePath( ) ) ; Assert . fail ( "srini_string" ) ; } Actual DFG linksActual ASTPredicted node types on root-leaf pathsGENERATED CODE:void function ( ParametersMetaDataarg0 ) {this . parametersMetaData= arg0 ; }Predicted DFG linksSOURCE:sets the value of the parametersmetadataproperty . concode_field_sepMetaData.Templatetemplate concode_elem_sepMetaData.WorkflowImplworkflowImplconcode_elem_sepString workflowImplId…ACTUAL CODE:void function ( ParametersMetaDataarg0 ) {this . parametersMetaData= arg0 ; }Actual DFG linksActual ASTPredicted node types on root-leaf pathsFigure C6: An example from the concode dataset with BLEU=87.25.

14

GENERATED CODE:void function ( MapNodearg0 ) { nodes . put ( arg0 . getId( ) , arg0 ) ; }Predicted DFG linksSOURCE:adds a node to the central node hashtable. concode_field_sepBoundingBoxboundingBoxconcode_elem_sepHashtable<Long,MapNode> nodes concode_elem_sepHashtable<Long,MapWay> …ACTUAL CODE:void function ( DefaultMapNodearg0 ) { nodes . put ( arg0 . getId( ) , arg0 ) ; }Actual DFG linksActual ASTPredicted node types on root-leaf paths