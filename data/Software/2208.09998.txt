2
2
0
2

g
u
A
2
2

]
E
S
.
s
c
[

1
v
8
9
9
9
0
.
8
0
2
2
:
v
i
X
r
a

Antecedent Predictions Are Dominant for Tree-Based
Code Generation

Yihong Dong1, Ge Li1∗, Zhi Jin1∗,
1Key Laboratory of High Conﬁdence Software Technologies (Peking University),
Ministry of Education; Institute of Software, EECS, Peking University, Beijing, China

dongyh@stu.pku.edu.cn

{lige, zhijin}@pku.edu.cn

Abstract

Code generation focuses on the automatic conversion of natural language (NL)
utterances into code snippets. The sequence-to-tree (Seq2Tree) methods, e.g.,
TRANX, are proposed for code generation, with the guarantee of the compilability
of the generated code, which generate the subsequent Abstract Syntax Tree (AST)
node relying on antecedent predictions of AST nodes. Existing Seq2Tree meth-
ods tend to treat both antecedent predictions and subsequent predictions equally.
However, under the AST constraints, it is difﬁcult for Seq2Tree models to produce
the correct subsequent prediction based on incorrect antecedent predictions. Thus,
antecedent predictions ought to receive more attention than subsequent predictions.
To this end, in this paper, we propose an effective method, named APTRANX
(Antecedent Prioritized TRANX), on the basis of TRANX. APTRANX contains
an Antecedent Prioritized (AP) Loss, which helps the model attach importance
to antecedent predictions by exploiting the position information of the generated
AST nodes. With better antecedent predictions and accompanying subsequent
predictions, APTRANX signiﬁcantly improves the performance. We conduct ex-
tensive experiments on several benchmark datasets, and the experimental results
demonstrate the superiority and generality of our proposed method compared with
the state-of-the-art methods.

1

Introduction

Code generation is an essential text generation task in the community of natural language processing
(NLP), which deals with automatically generating a piece of executable code from NL utterances. In
recent years, a series of Seq2Tree models have made remarkable achievements for code generation
[Dong and Lapata, 2016, 2018, Rabinovich et al., 2017, Yin and Neubig, 2017, 2018, 2019, Shin
et al., 2019, Sun et al., 2019, 2020, Xie et al., 2021, Jiang et al., 2021, 2022]. Speciﬁcally, given an
NL utterance input, instead of outputting a sequence of code tokens directly, the Seq2Tree model
outputs a sequence of AST actions, which correspond to the nodes in AST, in a traversal method,
and converts them into code via certain deterministic functions eventually. Depending on the AST
structure, the Seq2Tree model ensures the compilability of the generated code. Furthermore, the AST
structure also helps the Seq2Tree model shrink the search space, i.e., the antecedent predictions will
constrain the subsequent predictions, to generate code better.

Despite the success of the above models, they still neglect that the antecedent predictions play an
important role for Tree-based code generation. The AST corresponding to the code is a structure of
progressive derivation from the root node to the leaf nodes, and the child nodes are derivations of the
parent nodes. Under the constraints of the AST, parent nodes are able to determine the type, order,
and even number of child nodes with ease. When the output node sequence of the Seq2Tree model

∗Corresponding author

Preprint. Under review.

 
 
 
 
 
 
Figure 1: An example of an input NL utterance and the corresponding python code snippet.

is based on the pre-order traversal of the AST2, the parent node is always ahead of the child node.
Consequently, when the antecedent parent node is wrong generated, the Seq2Tree model can barely
predict the subsequent child nodes in a right way, which dramatically impacts the effectiveness of the
Seq2Tree model.

In this paper, we propose an effective APTRANX method with AP Loss, which helps the model
prioritize antecedent predictions during training, so that generating more accurate antecedent predic-
tions and accompanying subsequent predictions during inference. AP Loss is a dynamically scaled
cross entropy loss, which contains a scaling factor and a magnitude factor. The scaling factor is
related to the position of the traversal order of AST nodes, and the magnitude factor controls the
magnitude of the total loss function. Intuitively, AP Loss automatically reduces the contribution of
the susceptible subsequent predictions and focuses on the more important antecedent predictions
during training. To demonstrate the effectiveness of APTRANX, we conduct extensive experiments
on three benchmark datasets. Experimental results show that our proposed method outperforms the
previous state-of-the-art methods. Furthermore, we note that APTRANX is not only suitable for the
pre-order traversal but also achieves signiﬁcant improvement on the breadth-ﬁrst traversal, which
indicates its generality.

The main contribution of this paper can be summarized as follows:

• We propose that the antecedent predictions are more essential than the subsequent predictions

in the Tree-based code generation method.

• We propose a novel method, namely APTRANX, with AP Loss that assists the model to
concentrate on antecedent predictions by leveraging the position information of the generated
AST nodes.

• Our proposed method outperforms the previous state-of-the-art methods on several bench-
mark datasets. Extensive experimental results also demonstrate the effectiveness and versa-
tility of our proposed method.

2 Motivation

It is well known that code is a type of specialized text with strict grammatical rules and speciﬁc
structural patterns. A number of statements in the code can effectively inﬂuence the structure of
subsequent code, such as for, while, and if. Fig. 1 demonstrates an example for code generation. As
shown in the right part of Fig. 1, the beginning tokens marked in orange lead the structure of the
following code sentences, respectively, which indicates the importance of the antecedent token in the
corresponding code sentence.

The antecedent node is dominant, when the code is converted into the corresponding AST based
on the pre-order traversal. Fig. 2 shows the procedure of converting an input NL utterance into the
corresponding python code snippet by the Seq2Tree model. As shown in Fig. 2, the AST nodes have
some of the following properties:

2Note that the breadth-ﬁrst traversal has the same conclusion, and we use the pre-order traversal as examples

in this paper.

2

Figure 2: The procedure of code generation by the Seq2Tree model. The order of nodes is based on
the pre-order traversal of the AST.

1. The parent node can determine the left side of the production rule of the child node, e.g.,
since the right side of the production rule of node 0 is Module(stmt* body), the left side of
the production rule of node 1 must be stmt.

2. The parent node is able to control the number and order of the child node. For instance, the
right side of the production rule of node 3 is Attribute(expr value, identiﬁer attr), therefore
node 3 has two child nodes, the ﬁrst is expr and the second is identiﬁer.

3. The parent node allows for a decision of the type of the child node. For example, because
the right side of the production rule of node 4 is Name(identiﬁer id), the type of node 5
has to be the id of the identiﬁer. Similarly, the type of node 6 has to be the attribute of the
identiﬁer.

In pre-order traversal of the AST node, the parent node precedes the child node. As a result, the more
anterior AST node in pre-order traversal is more crucial.

For effective training, the Seq2Tree model at each step generate the next prediction based on the
right label during training stage, which leads to an unnegligible inconsistency between training and
inference stage, i.e., the right label during training is inconsistent with the real previous prediction
during inference, because the right label is unknowable during inference. It is extremely difﬁcult to
generate the correct subsequent prediction based on incorrect antecedent predictions under the AST

3

constraints. Thus, the cost of producing incorrect predictions is quite expensive during inference. To
this end, in this paper, we propose an effective method to alleviate the negative effect of the training
inconsistency.

3 Background

In this section, we describe the principle and architecture of TRANX [Yin and Neubig, 2018], which
is adopted as our basic model. Because of its extensive applications and competitive performance,
TRANX has been widely employed in code generation. Please note that our proposed AP Loss is
suitable for other Seq2Tree models as well.

In Fig. 2, based on the abstract syntax description language (ASDL) grammer and the input NL
utterance, TRANX ﬁrst outputs a sequence of actions used to construct the AST in pre-order traversal.
Then the user speciﬁed function AST_to_Code(*) is called to map the generated intermediate AST
into the code. Concretely, three types of ASDL grammar-based actions are available in TRANX:

APPLYRULE[c] actions apply a constructor c to the composite ﬁeld of a node with the same type

as c, generating the child node using the ﬁelds in c.

REDUCE actions complete the generation of the child node for a ﬁeld which has optional(?) or

multiple((cid:63)) cardinalities.

GENTOKEN[v] actions expand the ﬁeld of a node to generate a token v.

TRANX uses an attentional encoder-decoder framework with augmented recurrent connections to
interpret the topology of the AST, where the encoder is a Bidirectional Long Short-term Memory
(BiLSTM) network and the decoder is an LSTM network as well. Given the input NL utterance x of
n tokens, i.e., {xi}n
i=1, the BiLSTM encoder learns vectorial representations {h}n
i=1. Then, at each
step t, the LSTM decoder generate hidden state st as:

st = fLSTM([at−1 : ˜st−1 : pt], st−1),

(1)

where fLSTM is the LSTM decoder transition function, [:] denotes the concatenation of vectors,
at−1 and pt denote the embedding of previous action and parent action, respectively. We deﬁne the
attentional vector ˜st as:

˜st = tanh(Wc[ct : st]),
(2)
where tanh is the hyperbolic tangent function, Wc is a parameter matrix, ct denotes the context
vector weighted by encoding representations {h}n

i=1 using attention.

Finally, the probability of an APPLYRULE[c] action with embedding ac at each step t is:

p(at = APPLYRULE[c] | a<t, x) = softmax(aT

c W˜st),

(3)

where softmax is the softmax function, aT
c denotes the transpose of embedding ac, W is another
parameter matrix. Note that REDUCE is considered as a special APPLYRULE action. For the
GENTOKEN action, its probability is deﬁned as:

p(at = GENTOKEN[v] | a<t, x)

= p(gen | at, x)p(v | gen, at, x) + p(copy | at, x)p(v | copy, at, x),

(4)
(5)

where

p(gen | at, x) = softmax(W˜st),
p(copy | at, x) = 1 − p(gen | at, x),
p(v | gen, at, x) = softmax(aT
v W˜st),
p(xi | copy, at, x) = softmax(hT

i W˜st).

Similar to other Seq2Tree models, given a corpus (x, a), TRANX is trained to minimize the cross
enropy loss:

Lce(x, a; θ) = −

T
(cid:88)

t=1

log p (at | a<t, x; θ) ,

(6)

where T is the total number of actions and θ denotes the parameters of TRANX.

4

Figure 3: The effect of varying γ on the scaling factor t−γ.

4 Methodology

In this section, we ﬁrst introduce AP Loss in detail, and then a magnitude factor is given to regulate
the magnitude of the total loss function.

4.1 Antecedent Prioritized Loss

We extend the standard cross entropy loss (6) to AP Loss by exploiting the position information of
the AST traversal order. Speciﬁcally, a scaling factor t−γ is added with respect to the action index t,
and the AP Loss is deﬁned as:

Lapl(x, a; θ) = −

T
(cid:88)

t=1

t−γ log p (at | a<t, x; θ) .

(7)

where the tunable hyperparameter γ ≥ 0. The scaling factor t−γ decreases as the action index t
increases, which helps the model down-weight the action with large t and thus focus training on the
action with small t, i.e., the antecedent action. The scaling factor is visualized for several values of
γ ∈ [0, 2] in Fig. 3. When γ = 0, Lapl is equivalent to Lce. Intuitively, we would prefer not to choose
γ > 1, because the scaling factor t−γ varies dramatically under this circumstance. For example, with
γ = 2, an action with index 10 would have 100× lower loss compared with Lce. This may cause the
model to abandon the training of subsequent predictions completely, which is undesirable. A desired
scaling factor has to balance the loss of both antecedent predictions and subsequent predictions,
therefore we recommend picking γ from 0.1 to 0.5. Note that the difference between Lapl and Lce is
only the scaling factor t−γ, so it is quite easy to implement AP Loss in code.

4.2 Magnitude Adjustment

In practice, we insert an α-adjusted magnitude factor in AP Loss:

Lapl(x, a; θ) = −α

T
(cid:88)

t=1

t−γ log p (at | a<t, x; θ) .

(8)

We adopt the above form in our experiments because it yields improvements over the non-α-adjusted
form (7). Generally, α can be chosen from the positive constants. Furthermore, in order to reduce the
choice of hyperparameters, we keep the magnitude of Lapl equal to the magnitude of original Lce

5

using the integral, which relates α to γ.






α =

T
ln (T + 1)
α = (1 − γ)T γ,

,

if γ = 1,

otherwise.

(9)

See supplementary material for a detailed derivation of α. With the help of (9), we only need to tune
γ, and the appropriate α will be calculated automatically.

5 Experiment Setup

5.1 Datasets

We conduct experiments on three benchmark datasets as follows:

GEO [Zelle and Mooney, 1996] consists of 880 US geographical NL utterances and their corre-

sponding code deﬁned in lambda logical forms.

ATIS [Hemphill et al., 1990] is a collection of 5,410 paired ﬂight data, which contains the NL

description of ﬂight information and their corresponding lambda calculus code like GEO.

CONALA [Yin et al., 2018] contains 2,879 real-world data of manually annotated NL questions
and their Python3 solutions on STACK OVERFLOW, which is more difﬁcult because of its complex
and extensive code composition.

The detailed statistics of the three datasets are shown in Table 1.

Table 1: Statistics of datasets.

Dataset

Examples Num

Avg Length

Train

Dev

Test NL

Code AST

600 (480)
GEO
ATIS
4,473
CONALA 2,175

- (120)
491
200

280
448
500

7.4
10.5
10.2

19.1
28.1
14.9

19.3
31.5
23.2

5.2 Baselines

To exhibit the effectiveness of our proposed method, we compared it with several competitive and the
state-of-the-art methods as follows:

SEQ2TREE [Dong and Lapata, 2016] uses a general attention-enhanced encoder-decoder model

to generate logical forms by conditioning output trees.

ASN [Rabinovich et al., 2017] outputs the AST constructed by a decoder whose modular structure

is dynamically determined, parallel to the structure of the output tree.

COARSE2FINE [Dong and Lapata, 2018] ﬁrst generates a rough meaning sketches abstracted

abstracted from low-level information, and then ﬁlls in missing detail.

TRANX [Yin and Neubig, 2019] is our basic model described detailed in Section 3. TRANX
(our) and TRANX’ (our) are the versions of the pre-order and breadth-ﬁrst traversals of TRANX
reimplemented by us, respectively.

TREEGEN [Yin and Neubig, 2019] uses the Transformers instead of LSTM to alleviate the
long-dependency problem and the AST reader to incorporate both grammar rules and AST structures.

ML-TRANX [Xie et al., 2021] adopts a mutual learning framework to jointly train models for

different traversals based decodings.

TRANX-RL [Jiang et al., 2021] uses a context-based branch selector to dynamically determine

optimal branch expansion orders for multi-branch nodes.

ASED [Jiang et al., 2022] generates the current prediction with both the history and future

information using an AST structure enhanced decoder.

6

Table 2: The performance of AP Loss based on the pre-order and breadth-ﬁrst traversals of TRANX,
i.e., APT and APT’, compared with various baselines.

Model

SEQ2TREE [Dong and Lapata, 2016]
ASN [Rabinovich et al., 2017]
COARSE2FINE [Dong and Lapata, 2018]
TREEGEN [Sun et al., 2020]
TRANX [Yin and Neubig, 2019]
ML-TRANX [Xie et al., 2021]
TRANX-RL [Jiang et al., 2021]
ASED [Jiang et al., 2022]

TRANX (our)
APT

TRANX’ (our)
APT’

5.3 Metrics

GEO
ACC.

86.1
87.1
88.2
89.6
88.8±1.0
89.2±0.6
89.5±1.2
89.8±1.1
88.8±0.6
90.4±0.8
86.2±0.9
88.5±0.5

ATIS
ACC.

84.6
85.9
87.7
89.1
87.6±0.1
89.3±0.3
89.1±0.5
88.9±0.7
87.6±0.4
89.8±0.7
86.4±0.5
87.3±0.7

DJANGO
ACC.

-
-
74.1
-
77.3±0.4
79.6±0.3
77.9±0.5
79.2±0.5
77.2±0.4
79.3±0.5
76.9±0.4
78.0±0.6

CONALA

BLEU

ACC.

-
-
-
-
24.35±0.4
24.42±0.8
25.47±0.7
25.56±0.6
25.13±0.8
27.56±0.7
24.79±0.6
26.92±0.8

-
-
-
-
2.5±0.7
2.2±0.4
2.6±0.4
2.8±0.7
2.2±0.6
2.9±0.7
1.4±0.4
1.6±0.4

To evaluate the effectiveness of different methods, we use two widely-used evaluation metrics: the
exact matching accuracy (ACC.) for all datasets and the corpus-level BLEU for CONALA as a
complement.

5.4 Settings

We train our model for a maximum epoch of 80 with Adam [Kingma and Ba, 2015] optimizer on a
single GPU of Tesla V100-SXM2-32G. To ensure fair comparisons, we adopt the same experimental
setup as TRANX [Yin and Neubig, 2019]. To be speciﬁc, we set the sizes of word embedding, action
embedding, and hidden states as 128, 128, and 256, respectively. The beam size is set to 5 for GEO
and ATIS, while 15 for CONALA. For additional hyperparameter γ and α, we pick γ ∈ [0.1, 0.5]
and α from the set of 1, 2, 4, 8, and ‘auto’ 3 using validation set. To mitigate the instability of the
model training, we exhibit the average performance of the model running ﬁve times.

6 Experimental Results

In this section, we ﬁrst report the main experimental results to demonstrate the remarkable effec-
tiveness of our proposed method, and then we analyze the effects of γ and α, ﬁnally, we ﬁnd some
examples to verify the beneﬁts of AP Loss in practice.

6.1 Main Results

Table 2 lists the overall experimental results on three benchmark datasets. We can ﬁnd that the
performance of our reimplemented TRANX is comparable to the original paper [Yin and Neubig,
2019]. With the help of AP Loss, APTRANX outperforms baselines across all datasets, which
validates the superiority of AP Loss. It also indicates that letting the model concentrate on the
predictions of the antecedent AST nodes during training helps code generation in inference. In
addition, the performance of TRANX’ is inferior to that of TRANX. A possible reason is that the
breadth-ﬁrst traversal has some drawbacks. For examples, back to Fig. 2, the breadth-ﬁrst traversal,
compared to the pre-order traversal, prioritizes the output of the Reduce actions and delays the output
of GenToken actions, which is actually evaluated. Moreover, in Fig. 2, the attribute of the variable
is output before the variable in breadth-ﬁrst traversal, which increases the difﬁculty of prediction.
Nevertheless, both the pre-order and breadth-ﬁrst traversals of TRANX are improved using AP Loss.
It implies AP Loss can be applied to other reasonable orders.

3‘auto’ denotes α is calculated by (9)

7

Figure 4: Effect of γ with optimal α.

7 Effects of γ and α

The coefﬁcient γ and α are important hyperparameters to regulate AP Loss. Therefore, we ﬁrst
investigate the effect of γ on AP Loss with optimal α in Fig. 4. The best γ is 0.4 for GEO, 0.1 for
ATIS, 0.1 and 0.2 for CONALA (γ with high BLEU is preferred in this paper). We can ﬁnd the best γ
is inversely correlated with the Avg Length of the AST in Table 1. This is because, in training stage,
AP Loss need to increase the scaling factor t−γ appropriately by reducing γ for long AST sequences,
thus avoiding ignoring the later nodes.

Table 3: Varying α for AP Loss with optimal γ.
CONALA
BLEU ACC.

GEO ATIS DJANGO
ACC. ACC.

ACC.

α

1
2
4
8
‘auto’

89.6
90.4
88.8
88.9
89.7

87.9
88.6
87.7
87.3
89.8

78.8
79.1
78.5
78.3
79.3

26.75
27.56
25.64
24.93
27.12

2.4
2.9
2.6
2.3
3.4

In Table 3, we vary α for AP Loss with optimal γ. As α increases, the general trend of the performance
of APTRANX rises and then falls on all datasets. When α is set to ‘auto’, APTRANX achieves the
best performance in terms of accuracy for ATIS and CONALA, and impressive results regarding
accuracy for GEO and BLEU for CONALA. When we substitute the optimal γ and the corresponding
average AST length for each dataset into Eq. (9), α is 1.96 for GEO, 1.27 for ATIS, 1.23 and 1.50 for
CONALA, which are all in the range of 1 to 2. Therefore, it is expected that the model works well
when α is equal to 1 and 2. These results demonstrate the advantage of using Eq. (9) to calculate α
automatically.

7.1 Case Study

We compare the top-1 code generated from TRANX and APTRANX on different datatsets in Table 4.
We ﬁnd that TRANX tends to produce the incorrect antecedent prediction, which makes it difﬁcult to
generate subsequent predictions with the constraints of the AST. By contrast, our model APTRANX
succeeds in overcoming the above problem with the help of AP Loss to generate more accurate code.

8

Table 4: Case study examples. Incorrect codes are marked in blue while the ﬁrst token of counterparts
is marked in red.

Dataset

Model

NL

What length is the r0?

Code

r0 (cid:37)

( len:i r0 ) (cid:34)

GEO

ATIS

CONALA

TRANX

APTRANX

TRANX

APTRANX

TRANX

APTRANX

TRANX

APTRANX

TRANX

APTRANX

TRANX

APTRANX

What is the highest elev of c0?

( argmax $0 ( and ( place:t $0 ) ( loc:t $0 c0 ) ) c0 ) (cid:37)

( argmax $0 ( and ( place:t $0 ) ( loc:t $0 c0 ) ) ( elevation:i $0 ) ) (cid:34)

Tell me about the type of aircraft
call an ac0.

Is there ground transport from the
ap0 into ci0 town?

( lambda $0 e ( exists $1 ( and ( ﬂight $1 ) ( = ( aircraft_code $1 ) $0 ) ( aircraft_code $1
ac0 ) ) ) ) (cid:37)
ac0 (cid:34)

( lambda $0 e ( exists $1 ( and ( = ( arrival_time $1 ) $0 ) ( approx_arrival_time $1 ti0 )
( ground_transport $1 ) ( to_city $1 ci0 ) ) ) ) (cid:37)
( lambda $0 e ( and ( from_airport $0 ap0 ) ( ground_transport $0 ) ( to_city $0 ci0 ) ) )
(cid:34)

Running bash script str_0.

Get attribute str_0 from object
var_0.

exec() (cid:37)

subprocess.call(str_0, shell=True) (cid:34)

print(var_0.rstrip(str_0)) (cid:37)

getattr(var_0, str_0) (cid:34)

8 Related Work

Recently, a numbers of code generation models have achieved great success [Alvarez-Melis and
Jaakkola, 2017, Hayati et al., 2018, Sun et al., 2019, Wei et al., 2019, Xu et al., 2020]. The work [Ling
et al., 2016] treated code generation as conditional text generation and solved it with the sequence-to-
sequence model. In order to utilize the syntactic information of the code, the Seq2Tree models are
resorted, which transforms NL utterances into a sequence of AST based grammar actions. Typically,
[Dong and Lapata, 2016] ﬁrst explored a Seq2Tree model for code generation. [Rabinovich et al.,
2017] explored a modular encoder-decoder architecture with structured AST output spaces. [Yin and
Neubig, 2018] proposed a Seq2Tree model to generate the AST as the intermediate representations
of codes. Moreover, [Sun et al., 2020] used the Transformer [Vaswani et al., 2017] architecture and
attention mechanisms to address the long dependency problem for code generation. [Jiang et al.,
2022] explored the use of the future nodes information generated by the AST for prediction.

Some researchers have also noticed that the importance of each AST node for code generation is
different. [Xie et al., 2021] proposed a mutual learning framework for Seq2Tree models to learn
the information of AST nodes in different traversals simultaneously. [Jiang et al., 2021] select the
generation path of the node in AST using reinforcement learning. Signiﬁcantly different from the
above work, we explore the impact of the position information of AST nodes during inference and
propose an effective method to focus on the more important antecedent nodes during training.

9 Conclusion and Discussion

In this paper, we have identiﬁed the prediction of antecedent nodes as an important factor inﬂuencing
the accuracy of the Seq2Tree model. To address this, we have proposed an effective APTRANX
method with AP Loss, which scales the standard cross entropy loss in order to focus learning
on antecedent nodes and down-weight the susceptible subsequent nodes. With the help of AP
Loss, APTRANX achieves the state-of-the-art performance on three benchmark datasets. Extensive
experimental result and analysis also verify its effectiveness and generality.

The judgment of AP Loss on critical nodes is limited by the traversal method, while the position of
the node in the pre-order or breadth-ﬁrst traversal of the AST only reﬂects its importance partially. In
the future, we plan to design a method that is able to extract the importance of nodes directly from
the AST structure, because it has more information than the traversal of the AST.

9

References

Li Dong and Mirella Lapata. Language to logical form with neural attention. In ACL, pages 33–43,

2016.

Li Dong and Mirella Lapata. Coarse-to-ﬁne decoding for neural semantic parsing. In ACL, pages

731–742, 2018.

Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code generation

and semantic parsing. In ACL, pages 1139–1149, 2017.

Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation.

In ACL, pages 440–450, 2017.

Pengcheng Yin and Graham Neubig. TRANX: A transition-based neural abstract syntax parser for

semantic parsing and code generation. In EMNLP, pages 7–12, 2018.

Pengcheng Yin and Graham Neubig. Reranking for neural semantic parsing. In ACL, pages 4553–

4559, 2019.

Eui Chul Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Alex Polozov. Program
synthesis and semantic parsing with learned code idioms. In NeurIPS, pages 10824–10834, 2019.

Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. A grammar-based structural

CNN decoder for code generation. In AAAI, pages 7055–7062, 2019.

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. Treegen: A tree-based

transformer architecture for code generation. In AAAI, pages 8984–8991, 2020.

Binbin Xie, Jinsong Su, Yubin Ge, Xiang Li, Jianwei Cui, Junfeng Yao, and Bin Wang. Improving
tree-structured decoder training for code generation via mutual learning. In AAAI, pages 14121–
14128, 2021.

Hui Jiang, Chulun Zhou, Fandong Meng, Biao Zhang, Jie Zhou, Degen Huang, Qingqiang Wu, and
Jinsong Su. Exploring dynamic selection of branch expansion orders for code generation. In
ACL/IJCNLP, pages 5076–5085, 2021.

Hui Jiang, Linfeng Song, Yubin Ge, Fandong Meng, Junfeng Yao, and Jinsong Su. An AST structure
IEEE ACM Trans. Audio Speech Lang. Process., 30:

enhanced decoder for code generation.
468–476, 2022.

John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic

programming. In AAAI, pages 1050–1055, 1996.

Charles T. Hemphill, John J. Godfrey, and George R. Doddington. The ATIS spoken language

systems pilot corpus. In NAACL, 1990.

Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to
mine aligned code and natural language pairs from stack overﬂow. In MSR, pages 476–486, 2018.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

David Alvarez-Melis and Tommi S. Jaakkola. Tree-structured decoding with doubly-recurrent neural

networks. In ICLR, 2017.

Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and

Graham Neubig. Retrieval-based neural code generation. In EMNLP, 2018.

Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summariza-

tion. In NeurIPS, pages 6559–6569, 2019.

Frank F Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and Graham Neubig. Incorporating
external knowledge through pre-training for natural language to code generation. In ACL, pages
6045–6052, 2020.

10

Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, Fumin
Wang, and Andrew W. Senior. Latent predictor networks for code generation. In ACL, pages
599–609, 2016.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008, 2017.

11

A Supplementary Material

A.1 Derivation of α

We keep the magnitude of Lapl equal to the magnitude of original Lce:

α

T
(cid:88)

t=1

t−γ =

T
(cid:88)

t=1

1.

For the right side of (10),

T
(cid:88)

t=1

1 =

(cid:90) T +1

1

1dt = t|T +1

1 = (T + 1) − 1 = T.

Similarly, using the integral, the left side of (10) approximates to:

t−γ ≈ α

(cid:90) T +1

t−γdt

α

T
(cid:88)

t=1

1
α ln t|T +1
1
1 − γ

α

1

,
t1−γ(cid:12)
T +1
(cid:12)
1

if γ = 1,

,

otherwise.

α(ln (T + 1) − ln 1),

if γ = 1,

((T + 1)1−γ − 11−γ),

otherwise.

α
1 − γ
α ln (T + 1),

if γ = 1,

α
1 − γ

((T + 1)1−γ − 1),

otherwise.














=

=

=

According to (10), (11), and (12), we have

Thus,

When γ (cid:54)= 1,






α ln (T + 1) = T,

if γ = 1,

α
1 − γ

((T + 1)1−γ − 1) = T,

otherwise.

α =






T
ln (T + 1)

,

if γ = 1,

(1 − γ)

T
(T + 1)1−γ − 1

,

otherwise.

α = (1 − γ)

T
(T + 1)1−γ − 1

= (1 − γ)

= (1 − γ)

(a)
≈ (1 − γ)

= (1 − γ)

1
(T +1)1−γ −1
T

− 1
T

1
(T +1)1−γ
T
1
(T )1−γ
T − 0
1
(T )−γ

= (1 − γ)T γ,

where (a) is true because when T is large, T + 1 approximates to T and 1

T approximates to 0.

12

(10)

(11)

(12)

(13)

(14)

(15)

In conclusion, we have






α =

T
ln (T + 1)
α = (1 − γ)T γ,

,

if γ = 1,

otherwise.

13

