2
2
0
2

r
a

M
8
2

]
x
e
-
p
e
h
[

2
v
5
4
6
7
0
.
3
0
2
2
:
v
i
X
r
a

Software and Computing for Small HEP Experiments1

Dave Casper (editor),1,a,b Maria Elena Monzani (editor),2,3,4,c,d Benjamin Nachman
(editor),5,b,e Costas Andreopoulos,6,7,f,g Stephen Bailey,5,h Deborah Bard,5,8 Wahid
Bhimji,5,8 Giuseppe Cerati,9,i Grigorios Chachamis,10 Jacob Daughhetee,11,j Miriam
Diamond,12,k V. Daniel Elvira,9,l Alden Fan,2,3,c Krzysztof Genser,9,m,n,o Paolo
Girotti,13,p Scott Kravitz,5,c Robert Kutschke,9,m Vincent R. Pascuzzi,14 Gabriel N.
Perdue,9,q Erica Snider,9,i Elizabeth Sexton-Kennedy,9,n Graeme Andrew Stewart,15,b,n
Matthew Szydagis,16,c Eric Torrence,17,a,b Christopher Tunnell18,r

1University of California, Irvine
2SLAC National Accelerator Laboratory
3Kavli Institute for Paritcle Astrophysics and Cosmology
4Vatican Observatory
5Lawrence Berkeley National Laboratory
6University of Liverpool
7STFC Rutherford Appleton Laboratory
8National Energy Research Scientiﬁc Computing Center (NERSC)
9Fermi National Accelerator Laboratory
10Laborat´orio de Instrumenta¸c˜ao e F´ısica Experimental de Part´ıculas (LIP)
11Oakridge National Laboratory
12University of Toronto
13INFN Pisa
14Brookhaven National Laboratory
15CERN
16SUNY Albany
17University of Oregon
18Rice University

aFASER Collaboration
bATLAS Collaboration
cLZ Collaboration
dFermi-LAT Collaboration
eH1 Collaboration
f T2K Collaboration
gSBND Collaboration

1Authors have listed their experimental aﬃliations to illustrate the diversity of experiments represented
here, but the opinions, ﬁndings, and conclusion or recommendations expressed in the paper are those of the
authors and do not necessarily reﬂect the view of their collaborations.

 
 
 
 
 
 
hDESI Collaboration
iMicroBooNE Collaboration
jCOHERENT Collaboration
kSuperCDMS Collaboration
lCMS Collaboration
mMu2e Collaboration
nHEP Software Foundation
oGeant4 Collaboration
pMuon g-2 Collaboration
qMINERvA Collaboration
rXenon Collaboration

E-mail: dcasper@uci.edu, monzani@stanford.edu, bpnachman@lbl.gov

Abstract: This white paper brieﬂy summarized key conclusions of the recent US Com-
munity Study on the Future of Particle Physics (Snowmass 2021) workshop on Software
and Computing for Small High Energy Physics Experiments.

Submitted to the Proceedings of the US Community Study
on the Future of Particle Physics (Snowmass 2021)

Contents

1 Introduction

2 Key Observations and Recommendations
2.1 High Performance Computing Centers

2.1.1 Key challenge: Porting workﬂows to new architectures
2.1.2 Key challenge: Policy optimization
2.1.3 Key challenge: Scaling user support

2.2 Requirements beyond FLOPS, bytes, and bandwidth
2.3 Software Challenges
2.4 Support for Common Tools
2.5 Data Storage and Preservation
2.6 Computing Personnel

1

2
2
2
3
3
4
4
5
6
7

1 Introduction

This white paper summarizes the discussion from a dedicated Snowmass workshop on
Software and Computing for Small High Energy Physics (HEP) Experiments (https://
indico.physics.lbl.gov/event/1756). The mandate for the workshop was:

• Identify unique computational challenges of the ‘small’ experiment community.

• Gather input about what is needed in terms of computation for these experiments to

be successful.

• Connect members of the ‘small’ experiment community to the computational frontier

in Snowmass and encourage participation in topical groups.

• Foster the development and re-use of open-source software, building on the work
of the HEP Software Foundation (HSF) and other collaborative eﬀorts within the
community.

In order to be inclusive, we did not impose a deﬁnition of ‘small’ and have asked experiments
to self-select. The workshop was organized into two days: a ﬁrst day for perspectives from
representative experiments and a second day about computational tools.
In particular,
on the ﬁrst day, we heard from CERN-based experiment FASER [1], Fermilab-based ex-
periments MicroBooNE [2] and g-2 [3], and non-CERN/FNAL experiments LUX-ZEPLIN
(LZ) [4], COHERENT [5], and DESI [6]. These experiments covered all of the HEP fron-
tiers. On the second day, there were speakers covering the HSF, High Performance Com-
puting (HPC), event processing frameworks, reconstruction frameworks, event generators,

– 1 –

detector simulations, and machine learning. The slides contain a wealth of information
about each of these topics and are preserved at the indico link above. The goal of this
white paper is to describe some of the key takeaway points from the presentations and
discussions. If the entire workshop had to be summarized in one phrase, it would be1

small experiment (cid:54)= small data volume or small computing problem!

2 Key Observations and Recommendations

Below is a summary of important points discussed at the workshop; key recommendations
are oﬀset in bold. The four subsections below are in no particular order.

2.1 High Performance Computing Centers

Increasingly, small experiments need to rely on HPC resources for a signiﬁcant fraction of
their computing budget. At the workshop, we heard about challenges by experiments in
porting to HPC resources and about challenges from HPC center staﬀ about accommodat-
ing small experiment users.

From the perspective of an HPC center, ‘small’ experiments look very similar to ‘large’
experiments. They have similar scales in terms of compute hours, storage used and number
of users, and they all need peripheral services such as databases and workﬂow controls, as
well as allocations on the big systems. For an HPC center, this means both ‘small’ and
‘large’ experiments present the same demands on resources. What distinguishes ‘small’
experiments is that many of the common issues faced by all experiment teams working at
HPC centers are exacerbated by the lack of human resources available to work on these
issues. It is vital that science teams and HPC centers partner to address these challenges.
These issues fall into roughly three areas:

• Getting codes running on new architectures

• Adapting to HPC center policy

• Tools that are easy to stand up and maintain

2.1.1 Key challenge: Porting workﬂows to new architectures

The DOE Oﬃce of Advanced Scientiﬁc Computing Research (ASCR) operates three com-
puting and one networking user facilities. All have a joint mission to advance both science
and the state of the art in computing. ASCR facilities engage with vendors years in advance
of a system’s delivery, to ensure that production systems push the limits of existing tech-
nology. There are three key drivers in this advancement that pose a challenge to scientiﬁc
workﬂow design:

1Quoted from E. Snider’s presentation.

– 2 –

• Energy eﬃciency. Scientiﬁc computing needs are ever-increasing, but there is a limit
to the amount of power and cooling we can safely bring into a building. Energy
eﬃcient architectures are here to stay, including specialized devices that perform
speciﬁc calculations at ultra-high eﬃciency (e.g., for AI/ML).

• New storage technologies. Object stores are increasingly prevalent, and massively

parallel ﬁle systems do not always respond well to data-intensive workloads.

• Peripheral services. We are seeing an increasing use and reliance on containers,
scalable databases, and interfaces to other computing sites (e.g., local clusters, the
cloud).

A pressing question for both experiment teams and HPC center staﬀ is how to optimize
the entire end-to-end workﬂow, not just individual applications. The NESAP program at
NERSC (the National Energy Science Research Computing center) partners with applica-
tion development teams and vendors to port and optimize codes to new architectures and
platforms. Lessons learned from this process are shared with the wider NERSC community
via documentation and training. NESAP is evolving with the types of systems we deploy,
and will need to support workﬂow optimisation in the future.

Programs like NESAP are extremely valuable to small experiments and we
advocate for a continuation and/or expansion of this program. However, we also
heard about diﬃculties in recruiting NESAP postdocs. This is discussed in more detail in
Sec. 2.6.

2.1.2 Key challenge: Policy optimization

Some of the biggest challenges faced by users of HPC centers are due to security and policy
concerns, rather than technical barriers. Supporting the needs of experiment workﬂows has
pushed NERSC to change policies around (for example) near-realtime access to systems,
and support for federated ID.

In addition, NERSC has recognised that some workﬂows highly value continued access
to NERSC services - for example, if an experiment is operating 24/7. Through careful
planning and investment, we are now able to keep most infrastructure up during outages
using generators and new system software capabilities for rolling upgrades. We are also
supporting research in several projects to support more portable cross-facility workﬂows [7].

2.1.3 Key challenge: Scaling user support

NERSC is increasingly supporting more users and projects from experiment facilities (in-
cluding other DOE user facilities). HPC centers are facing a sea change in the number and
type of users they support - without any change in the number of staﬀ available to support
them. This challenge requires a new approach to user support, to make it easier for users
to develop and adapt their workﬂows without direct support from NERSC staﬀ. If done
successfully, this will also make it easier for ‘small’ experiments to adapt to HPC centers.
The LBNL Superfacility Project [8] was designed to leverage and integrate work being
done across NERSC, ESnet and research divisions at LBNL to provide a coordinated and

– 3 –

coherent approach to supporting experimental science at DOE facilities. The principles
behind the project were to provide an integrated, scalable and sustainable framework for
experiment science, working closely with a range of science teams to get the design right
and using existing, industry standard and open source tools wherever possible. This 3-year
Project was completed at the end of 2021, having achieved its aim, although work continues
very actively on Superfacility topics. NERSC is now able to support automated pipelines
that analyze data from remote facilities at large scale, without routine human intervention,
using capabilities such as near-realtime computing, dynamic networking, API-driven au-
tomation, HPC-scale notebooks with Jupyter [9], state of the art data management tools,
federated ID and container-based peripheral services.

2.2 Requirements beyond FLOPS, bytes, and bandwidth

Classic high performance computing (HPC) centers have traditionally focused on maxi-
mizing the number of ﬂoating point operations per second (FLOPS) delivered to massively
parallel programs, coupled with large bandwidth to disk and tape storage. Small ex-
periments often have requirements that go beyond just FLOPS, bytes, and bandwidth,
e.g. databases, data access portals, I/O robustness, complex workﬂows with many inter-
dependent smaller jobs, collaboration accounts accessible by multiple users to coordinate
data processing, queue waittime, and jupyter notebook servers. High uptime, low mean
time to failure (MTTF), and access to realtime and interactive computing resources are
particularly important for the eﬀective use of computing centers by smaller experiments.
Supporting these requirements takes tangible speciﬁc eﬀort from the computing centers,
and tangible speciﬁc eﬀort from the experiments to adapt. As such, HPC centers need
dedicated support for non-HPC workloads with appropriate metrics for evaluating success,
e.g. minimizing MTTF and providing interactive access, and not just FLOPS delivered.
Similarly, experiments need dedicated support to adapt and maintain their workﬂows to
take full advantage of the opportunities at HPC centers.

2.3 Software Challenges

High Energy Physics has a vast investment in software estimated to be greater than 50M
lines of C++ code. It is a critical part of our physics production pipeline, from triggering
and simulation all the way to analysis and ﬁnal plots. This legacy represents a huge in-
vestment over the years. It requires ongoing support in human eﬀort to adapt to changing
computing technologies. As shown in ﬁgure 1, improved single-threaded performance will
not come from the hardware. Computing gains come from increasing concurrency in the
code, especially at HPC centers. Concurrent programming skills must be learned and not
all experimenters want to invest in becoming proﬁcient in concurrent programming. There
is no longer a large supply of graduate students and postdocs, the historical sources of
experiment software, that can write or even maintain codes for these complicated architec-
tures. This hits smaller experiments especially hard, because there may not be a critical
mass of people with the critical skills needed. See Sec. 2.6 for suggestions on how to ad-
dress personnel needs. Eﬀorts to share code developed by the large experiments should
be funded to have broader impact for the community, and thereby maximize return on

– 4 –

Figure 1. Microprocessors reach the power wall limit circa 2010

those investments. This means adding time for documentation, training, and outreach.
Whenever possible the code should be open sourced.

2.4 Support for Common Tools

A reoccurring theme in nearly all of the experiment vignettes was the need for common
tools. A distinguishing feature of small experiments compared to large ones is that they
often do not have many, many or, in some cases, any dedicated software and computing
experts. This means that there is limited/no resources to develop new data acquisition,
simulation and reconstruction tools from scratch as large experiments are often able to do.
Some small experiments beneﬁt from shared expertise with large experiments, especially
those located at CERN and FNAL. Yet many small experiments do not have such natural
overlap, and they are left with few avenues to obtain the resources necessary to develop
their required suite of tools.

In some cases, larger experiments have (at least partially) abstracted and open-sourced
their software in a way that it can be re-used by small experiments. Two great examples
of this are the software packages ACTS [10] and LArSoft [11] (https://larsoft.org)
as well as the event processing frameworks Gaudi [12], Art [13] and Stitched (https://
github.com/cms-sw/Stitched), an abstraction of the CMSSW framework[14–18]). ACTS
(A Common Tracking Software) is a package for tracking at collider experiments, initially
based on ATLAS code. It provides experiment-independent implementations of track and

– 5 –

vertex reconstruction algorithms which are available for usage by any experiment once their
geometry and material description is implemented in ACTS. Successful examples include
ATLAS, FASER, sPHENIX, Belle-II. LArSoft is a shared base of simulation, reconstruc-
tion, and analysis software across liquid argon time projection chamber (LArTPC) neutrino
experiments. LArSoft is currently used by all Fermilab-based LArTPC experiments, in-
cluding DUNE and smaller-scale experiments (SBN, MicroBooNE, LArIAT, ArgoNeuT).
Both ACTS and LArSoft provide ready-to-use software solutions as well as a platform for
R&D work spanning usage of AI methods, algorithm parallelization, and deployment at
HPC, which can signiﬁcantly reduce the required workload for smaller experiments.

However, some open source tools still require signiﬁcant adaptation or development
and thus require experiment-independent support. The best example of this is the widely-
used detector simulation software Geant4 [19–21]. Even for large experiments, signiﬁcant
resources are required to integrate Geant4 into the detector-speciﬁc setup. Many small
experiments have physics needs beyond those covered by large experiments and additional
development is necessary. Some of this is happening organically (e.g. NEST [22] for direct
dark matter detection), but this is not always the case. To strengthen the partnership
between the Geant4 Collaboration and the experiments, Geant4 has recently introduced a
“Geant4 contributor” status, which should facilitate a faster inclusion of codes developed
by the experiments, beneﬁting the entire community.

In the US, the HEP support for Geant4 comes from the computing operations budgets
of large experiments and intensity frontier operations at Fermilab. This leads to a stove-
piping of Geant4 support such that issues speciﬁc to those experiments are addressed
because they are needed for the operation of the experiment. Other experiments that use
Geant4 are left out in the cold. In addition, Geant4’s common software, such as physics
models, no longer receives any US maintenance funding. The physics Generators used in
the ﬁeld (eg. Pythia, GENIE, Madgraph, Sherpa) also suﬀer from lack of stable funding
in a similar way. This is not sustainable for the long term viability of small experiments.
Long term, experiment-agnostic Geant4 support is critical for the success of
small experiments. At the workshop, we also heard that many small experiments do not
update Geant4 versions frequently (or ever!), and so they are unable to take advantage of
new developments, whereas maintaining many old versions by the Geant4 collaboration is
unsustainable. This means that small experiments often use no longer supported Geant4
versions and miss out on recent physics developments and computational innovations, such
as multi-threading or the ongoing integration with Graphical Processing Units (GPUs).

2.5 Data Storage and Preservation

Data preservation is necessary for scientiﬁc repeatability, while helping maximize the “re-
turn on investment” of our experiments. Re-analysis of data is of particular interest, as
advances in analysis methods and physics models become available such as more detailed
simulations and new applications of machine learning. Well-understood datasets are often
the best source of training for young physicists, and open-access datasets support out-
reach eﬀorts by targeting both data literacy and science skills. In addition, standardized,
trusted data repositories are critical for developing and testing new machine-learning algo-

– 6 –

rithms [23]. Encouraging the experiments to make their data publicly available will lead to
a richer ecosystem of available datasets, and help with long-term preservation of data and
experimental knowledge. Most important is the publication of data following the end of
an experiment. One way of ensuring data are usable for post-experiment use is to periodi-
cally release data throughout the lifetime of the experiment. Another model is to internally
future-proof data formats and analysis software to always be able to analyze older datasets.
In both cases, person-power is required for an eﬀective implementation.

Computing clusters do not typically guarantee hosting for signiﬁcant lengths of time,
nor do they have mechanisms to easily share these data with the public. However, ser-
vices for long-term data and software preservation (https://heasarc.gsfc.nasa.gov and
https://lambda.gsfc.nasa.gov) have been used for decades in astronomy, and the HEP
community has become quite familiar with them. Ensuring data usability long after the
end of an experiment is not quite as simple as “dumping everything” in a publicly available
repository. Detailed detector information is needed, as well as access to metadata such as
slow-control and calibration databases. Moreover, community standards specifying data
format and interfaces must be agreed upon. Finally, prototype analysis codes must be pub-
lished with the datasets, further lowering the barrier to data access and ensuring that the
data is analyzed correctly. It has become more commonplace in our community to take ad-
vantage of general purpose data and code services like Zenodo (https://www.zenodo.org),
which provide most of the infrastructure required for eﬀective data preservation.

2.6 Computing Personnel

The general challenge of maintaining software and computing talent is exacerbated in small
experiments by the lack of long term, permanent positions within the experiments. Many
of the software tasks are carried about by young collaborators and there is signiﬁcant
turnover from year to year. A special case is an experiment that is both small and far from
data, for which it is common that the community of junior people working on software is
not large enough to be self-sustaining. This problem can be partly mitigated by increased
use of common tools but that does not address the task of developing experiment speciﬁc
conﬁgurations, customizations and glueware.

It is therefore essential that there is funding for permanent software and com-
puting experts. The careers of these researchers should be evaluated appropriately with
rewards to eﬃciency, stability, and robustness. A more transparent approach to software
development and data sharing would go a long way towards improving the career prospects
of software and computing experts, as it would allow individuals to claim credit for their
work and be evaluated appropriately. There is little funding for software maintenance, even
though this is critical. One possibility is the creation of research software engineer (RSE)
positions that have long term funding independent of experiments, but for the support of
existing and planned experiments. Another area of opportunity is the increase of joint par-
ticle physics and data science appointments at universities, which have become marginally
more common over the last decade. We also need to increase the community investment
in general software literacy of physicists, through (continuing) education initiatives and
collaborations with industry, national labs and academia.

– 7 –

The diversity issues pervasive in HEP are exacerbated in the computing domain due
to the (perceived) technical nature of the work, and we must ensure that faculty, staﬀ, and
trainee positions are viable career options for a diverse group of people. Our workforce
development eﬀorts should explicitly include equity, and recognize that diversity is foun-
dational to our success, as it demonstrably increases the creativity of solutions and variety
of approaches. Diversifying the computational workforce will require eﬀorts to diversify
physics in general, and to leverage our partnership with the astronomy and industry com-
munities, who have made substantial eﬀorts in this direction over the last decade. This
remains a challenging problem, but not insurmountable, as STEM identity formation for
underrepresented minorities has become a priority in a variety of educational settings, from
early childhood to higher education [24].

Acknowledgments

We thank the participants of the Software and Computing for Small HEP Experiments
workshop https://indico.physics.lbl.gov/event/1756 for many useful discussions.

This work was supported by the US Department of Energy, Oﬃce of Science, under
contract numbers DE-AC02-76SF00515 (SLAC) and DE-AC02-05CH11231 (LBNL), and
by the Fermi National Accelerator Laboratory, managed and operated by Fermi Research
Alliance, LLC under Contract DE-AC02-07CH11359 with the US Department of Energy.
GC acknowledges support by the Funda¸c˜ao para a Ciˆencia e a Tecnologia (Portugal) under
project CERN/FIS-PAR/0024/2019 and contract ’Investigador auxiliar FCT - Individual
Call/03216/2017’ and from the European Union’s Horizon 2020 research and innovation
programme under grant agreement No. 824093.

References

[1] J. L. Feng, I. Galon, F. Kling and S. Trojanowski, ForwArd Search ExpeRiment at the LHC,

Phys. Rev. D 97 (2018) 035001, [1708.09389].

[2] MicroBooNE collaboration, R. Acciarri et al., Design and Construction of the

MicroBooNE Detector, JINST 12 (2017) P02017, [1612.05824].

[3] Muon g-2 collaboration, J. Grange et al., Muon (g-2) Technical Design Report, 1501.06858.
[4] LZ collaboration, D. S. Akerib et al., The LUX-ZEPLIN (LZ) Experiment, Nucl. Instrum.

Meth. A 953 (2020) 163047, [1910.09124].

[5] COHERENT collaboration, D. Akimov et al., COHERENT 2018 at the Spallation Neutron

Source, 1803.09183.

[6] DESI collaboration, M. E. Levi et al., The Dark Energy Spectroscopic Instrument (DESI),

1907.10688.

[7] K. B. Antypas, D. J. Bard, J. P. Blaschke, R. Shane Canon, B. Enders, M. A. Shankar et al.,
Enabling discovery data science through cross-facility workﬂows, in 2021 IEEE International
Conference on Big Data (Big Data), pp. 3671–3680, 2021, DOI.

– 8 –

[8] B. Enders, D. Bard, C. Snavely, L. Gerhardt, J. Lee, B. Totzke et al., Cross-facility science

with the superfacility project at lbnl, in 2020 IEEE/ACM 2nd Annual Workshop on
Extreme-scale Experiment-in-the-Loop Computing (XLOOP), pp. 1–7, 2020, DOI.

[9] R. Thomas and S. Cholia, Interactive supercomputing with jupyter, Computing in Science

Engineering 23 (2021) 93–98.

[10] X. Ai et al., A Common Tracking Software Project, 2106.13593.

[11] E. L. Snider and G. Petrillo, LArSoft: Toolkit for Simulation, Reconstruction and Analysis of

Liquid Argon TPC Neutrino Detectors, J. Phys. Conf. Ser. 898 (2017) 042057.

[12] G. Barrand et al., GAUDI - A software architecture and framework for building HEP data

processing applications, Comput. Phys. Commun. 140 (2001) 45–55.

[13] C. Green, J. Kowalkowski, M. Paterno, M. Fischler, L. Garren and Q. Lu, The Art

Framework, J. Phys. Conf. Ser. 396 (2012) 022020.

[14] C. Jones, M. Paterno, J. Kowalkowski, L. Sexton-Kennedy and W. Tanenbaum, The new
cms event data model and framework, Proceedings for Computing in High-Energy Physics
(CHEP’06), Mumbai, India 13 (2006) .

[15] C. D. Jones and E. Sexton-Kennedy, Stitched Together: Transitioning CMS to a Hierarchical

Threaded Framework, J. Phys. Conf. Ser. 513 (2014) 022034.

[16] CMS collaboration, C. D. Jones, L. Contreras, P. Gartung, D. Hufnagel and

L. Sexton-Kennedy, Using the CMS Threaded Framework In A Production Environment, J.
Phys. Conf. Ser. 664 (2015) 072026.

[17] CMS collaboration, C. D. Jones, CMS event processing multi-core eﬃciency status, J. Phys.

Conf. Ser. 898 (2017) 042008.

[18] A. Bocci, D. Dagenhart, V. Innocente, C. Jones, M. Kortelainen, F. Pantaleo et al., Bringing

heterogeneity to the CMS software framework, EPJ Web Conf. 245 (2020) 05009,
[2004.04334].

[19] GEANT4 collaboration, S. Agostinelli et al., GEANT4–a simulation toolkit, Nucl. Instrum.

Meth. A 506 (2003) 250–303.

[20] GEANT4 collaboration, J. Allison et al., Geant4 developments and applications, IEEE

Trans. Nucl. Sci. 53 (2006) 270.

[21] GEANT4 collaboration, J. Allison et al., Recent developments in Geant4, Nucl. Instrum.

Meth. A 835 (2016) 186–225.

[22] M. Szydagis, N. Barry, K. Kazkaz, J. Mock, D. Stolp, M. Sweany et al., Nest: a

comprehensive model for scintillation yield in liquid xenon, Journal of Instrumentation 6
(Oct, 2011) P10002–P10002.

[23] D. Dua and C. Graﬀ, UCI machine learning repository, 2017.

[24] A. Singer, G. Montgomery and S. Schmoll, How to foster the formation of stem identity:
studying diversity in an authentic learning environment, International Journal of STEM
Education 7 (12, 2020) .

– 9 –

