2
2
0
2

n
u
J

2

]
E
S
.
s
c
[

1
v
4
0
8
0
0
.
6
0
2
2
:
v
i
X
r
a

Learning code summarization from a small and local dataset

Toufique Ahmed
University of California, Davis
Davis, California, USA
tfahmed@ucdavis.edu

Premkumar Devanbu
University of California, Davis
Davis, California, USA
ptdevanbu@ucdavis.edu

ABSTRACT
Foundation models (e.g.,CodeBERT, GraphCodeBERT, CodeT5)
work well for many software engineering tasks. These models are
pre-trained (using self-supervision) with billions of code tokens, and
then fine-tuned with hundreds of thousands of labeled examples,
typically drawn from many projects. However, software phenomena
can be very project-specific. Vocabulary, and other phenomena vary
substantially with each project. Thus, training on project-specific
data, and testing on the same project, is a promising idea. This
hypothesis has to be evaluated carefully, e.g., in a time-series setting,
to prevent training-test leakage. We compare several models and
training approaches, including same-project training, cross-project
training, training a model especially designed to be sample efficient
(and thus prima facie well-suited for learning in a limited-sample
same-project setting) and a maximalist hybrid approach, fine-tuning
first on many projects in many languages and then training on the
same-project. We find that the maximalist hybrid setting provides
consistent, substantial gains over the state-of-the-art, on many
different projects in both Java and Python.

KEYWORDS
deep learning, same-project training, code summarization, transfer
learning

ACM Reference Format:
Toufique Ahmed and Premkumar Devanbu. 2018. Learning code summa-
rization from a small and local dataset. In Proceedings of Make sure to en-
ter the correct conference title from your rights confirmation emai (Confer-
ence acronym ‚ÄôXX). ACM, New York, NY, USA, 11 pages. https://doi.org/
XXXXXXX.XXXXXXX

1 INTRODUCTION
Machine learning applications in software engineering have been
very successful in practice (e.g., Microsoft‚Äôs CoPilot) and also on a
wide range of more advanced applications [23]. Recently, there has
been a great deal of interest in foundation models [1, 9, 11, 17, 34]
which subjects a very highly parametrized, high-capacity neural
model to a two-phase training regime. The first unsupervised ‚Äúpre-
training" phase is done with an enormous corpus, using a simple
fill-in-the-blanks or predict-the-next-token/sentence regime. This
phase can be carried out on essentially any (unlabeled) code data

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

harvested from the web. There is no task-specific goal here; the
model simply learns the statistics of the input data. The second
phase, fine-tuning, trains on-task, and requires carefully curated,
consistently labeled data, consisting typically of input-output pairs
reflecting good, consistent, on-task performance.

The challenges of creating well-curated, de-duplicated, and yet
relevant training datasets have been described by several authors [4,
10]. Recent paper by Ahmed et al [3] and Chen et al [7] explore
some of the issues that arise with finding sufficient quantities of
high-quality find-tuning data. Data availability may be limited be-
cause: first, some languages (e.g., Ruby) are relatively less popular
than other languages, and available high-quality data may be lim-
ited. Second, the projects in a language may be skewed towards one
application domain (e.g., Javascript for the web) and thus the perfor-
mance of the trained model maybe somewhat uneven. Finally, and
most interestingly, when curating software engineering datasets,
for on-task fine-tuning, yet another strange, unique, wrinkle arises:
project specificity.

It‚Äôs well known that developers in different projects do behave
somewhat differently; they use different terminology, different algo-
rithms, and even different coding practices. As far back as 2009 [37]
it was observed that cross-project models don‚Äôt perform as well
as in-project models on defect prediction tasks. These difficulties
cross-over into language models; even the earliest paper on lan-
guage modeling for code [13] noted application-specific effects.
Subsequent work by Tu et al [32] and Hellendoorn [12] noted the
highly local, project- and even file-specific vocabularies of source
code, and proposed ways to handle them.

This phenomenon offers an entirely new opportunity: Can project-
specific training data improve performance? On the plus side, since
vocabulary, coding styles etc are notoriously project-specific, train-
ing and testing on the same project should give better performance.
This seems like an easy, low-hanging fruit. However, there are a
couple of traps. First, when working within project, one has to be
careful in partitioning training and test data, so that we only use
data that would be realistically available in practice. Second, within-
project data may be quite substantially limited in comparison to
cross-project data. For this reason, within-project training regimes
would require models that learn well from fewer samples.

For this reason, we believe that it would be useful to investi-
gate approaches that would improve sample efficiency for the fine-
tuning phase of foundation model training. By ‚Äúimproving sample
efficiency" we mean the general goal of increasing the ability of
machine-learning models to learn to perform better, with fewer
training samples. For example, a model ùê¥ that reliably performs
as well as model ùêµ with much fewer training samples is a more
‚Äúsample efficient‚Äù model. Sample efficient model ùê¥ both requires
less data and potentially trains much faster: thus saving human
effort, time, and energy usage. Most of all, in settings where high

 
 
 
 
 
 
Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Toufique Ahmed and Premkumar Devanbu

quality training data is not as abundant, model ùê¥ would be more
attractive.

Finally, as a sort of ‚Äústress-testing‚Äù of the same-project tuning
idea, we applied this to an extremely well-tuned code summariza-
tion model, to see if same-project training provided any improve-
ment at all. For this we used the multilingual ‚ÄúPolyGlot‚Äù model,
by Ahmed & Devanbu [3]. They found that cross-project, multi-
lingual training, using a very large fine-tuning set in many lan-
guages provided best-in-class performance (this ‚ÄúPolyGlot" model
was the chart-topper on the CodeXGlue Leaderboard1 for a while,
although CodeT5 has since reported better performance). We won-
dered whether even this extensively well-tuned model could be
further improved on a specific project by further fine-tuning on the
same project. One might expect that it wouldn‚Äôt, since it is already
so well trained. . . but actually, it worked!

In this paper we consider same-project fine-tuning, for the task

of code summarization, and make the following contributions

(1) We investigate the benefits of within-project training, using
a time-series scenario: we train only on ‚Äúpast" data, and
evaluate on ‚Äúfuture‚Äù data. In the code summarization setting,
this reflects a realistic setting where a developer asks for the
summary of a piece of code, and we train only data already
available that specific time point in the history of the project.
We find that within-project training offers some advantages.
(2) Second, we adapt the GraphCodeBERT foundation model
specifically to improve its sample efficiency for code sum-
marization; the resulting GCBhybrid model, achieves high
levels of sample-efficiency and can outperform the state of
the art in some project-specific settings.

(3) We also found that the ‚Äúmaximalist stress test‚Äù, adding project-
specific fine-tuning to the already extensively fine-tuned
‚ÄúPolyGlot‚Äù model, actually provides further benefits, and
yields the best performance, comfortably beating the state of
the art CodeT5 model overall, with statistical and practical
significance (pairwise Wilcoxon test, with a difference in
means, over all test samples, of about 3.7 BLEU-4; this is
above the 2.0 BLEU-4 threshold difference that humans are
experimentally [28] known to notice).

(4) Finally, while ‚ÄúPolyGlot‚Äù+same-project setting is most per-
formant, we do find that same-project training is remarkably
efficient; even the largest projects use less than 2.5% of the
time taken for cross-project training, while attaining compa-
rable performance to current state-of-the-art

The paper begins in the next section with some motivating explo-
rations of project-specific phenomena relevant to the foundation
model setting. Following that we present our methodology, fol-
lowed by results, discussion, and related work. The paper ends with
a brief speculation on future directions.

2 BACKGROUND & MOTIVATION
We begin with a brief overview of Foundation models [5], which are
currently widely used in NLP and SE. Foundation models are trained
in two stages (i.e., pre-training and fine-tuning). In the pre-training
stage, we train the models with billions of unsupervised tokens to
teach the models the statistics of the language in a self-supervised

1See https://microsoft.github.io/CodeXGLUE/.

way, using simple tasks like auto-regressively predicting the next
token, filling in a blank in context, completing the next sentence,
denoising an input, etc. These tasks are performed in a multi-layer
deep network; the intermediate layers thus learn a representation
(‚Äúembedding‚Äù) of the salient patterns of input token sequences in the
code. Later we take the embeddings of the input sequence learned in
the pre-training stage and further train it with supervised data in the
fine-tuning stage. This pre-training+fine-tuning paradigm was first
introduced in NLP by Devlin et al. [8]. They proposed an encoder-
only model BERT, pre-trained with two training objectives: Mask
Language Modeling (MLM) and Next Sentence Prediction (NSP).
MLM is the most effective pre-training objective for encoder-only
models where the model randomly masks out a certain percentage
of tokens and unmasks them. Liu et al. showed that RoBERTA
outperforms BERT using only MLM as a pre-training objective with
some new training strategies (e.g., dynamic masking instead of static
masking of a sequence) and hyperparameters tuning [22]. BERT-
style encoder-only models have inherent limitations for seq2seq
generative tasks like Neural Machine Translations (NMT) because
of the missing trained decoder. Two models, BART [20] and T5 [27]
have well-trained decoders and perform well on seq2seq generative
tasks.

These models are not designed for code; subsequent research
has refined these models (retaining the same basic scheme) for
code and code-related natural language description. CodeBERT [9]
and GraphCodeBERT [11] are similar to the BERT model pre-
trained with MLM and some code-specific pre-training objectives.
PLBART [1] and CodeT5 [34] are replications of BART [20] and
T5 [27] specially designed for SE tasks. Code-specific pre-trained
models perform quite well on several SE tasks, including code sum-
marization. The standard benchmark dataset CodeXGLUE [23] is
used to evaluate these models. The CodeXGLUE includes a de-
duplicated code summarization dataset prepared by modifying the
CodeSearchNet [15] dataset. CodeXGLUE is a multilingual dataset
(consisting of data from six languages) and has between 25K and
252K cross-project training samples for each language. Though
some languages have relatively smaller samples (i.e., Ruby and
JavaScript), other have very large training datasets (i.e., Java and
Python).

Ahmed and Devanbu [3] have recently shown that multilingual
training is beneficial for code summarization. Identifiers play a sig-
nificant role in ML-based summarization, and they are mostly pre-
served across languages; this phenomenon enables cross-language
training to work well. This prior work suggests that if methods
from the same projects share similar identifiers, then same-project
training can benefit the model. However, there are some issues
when using same-project data. First, to be realistic, we can only
use data as it becomes available; thus at any point time, only past
data in the same project is available; we cannot use data on classes,
methods, etc that haven‚Äôt been created yet. Thus we perform all
our evaluations below in a ‚Äútime-series‚Äù or ‚Äútime-partioned" setting.
Second, and following from this time-partitioned train-and-test
approach, sample sizes get limited. Some times there is no more
than a few hundred samples for each project, which differs greatly
from the cross-project, cross-language setting where hundreds of
thousands of instances can be used to fine-tune the models. If the
pre-trained models are sample-efficient, then same-project training

Learning code summarization from a small and local dataset

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

can be proven effective for code summarization task. We will briefly
look into the two preliminary, motivating questions (PMQs) :

PMQ 1 Do the different samples in the same project share more
identifiers than samples drawn from random projects? If this
were the case, one might hope that training with same project
data would be especially advantageous.

PMQ 2 Are the high capacity pre-trained models sample effi-
cient? If these models were not especially sample-efficient,
then (because same-project data might be as abundant) we
might have difficulty exploiting any same-project data syn-
ergies.

PMQ 1: Are identifiers preserved across same-project samples? We con-
jecture that the same-project samples have higher identifier simi-
larities because of domain and API usage similarity. Same-project
samples also will use overlapping sets of user-defined class objects
and identifiers. To evaluate this question, we perform a small ex-
periment using time partioned data. We take five projects from the
Java CodeXGLUE code summarization dataset, each with at least
200 samples and sort them according to their creation date. We
perform the following steps.

(1) Divide the first 200 samples of each project into two groups

(i.e., 1-100 and 101-200).

(2) Take each group and find the unique case-insensitive identi-

fiers of the group. We repeat it for all five projects.

(3) Take group I of project A and calculate the Jaccard index

with group II of the same project.

(4) Now pair group I of project A with group II of all other

projects and calculate the Jaccard index2.

(5) Repeat steps 3 and 4 for all projects and observe the Jaccard

Index difference.

Table 2 shows that we get the highest Jaccard indices (always 2-5
times higher than other positions) in the diagonal position where
the data of both groups are coming from the same projects. Hence,
same-project samples have higher identifier similarities.

Observation 1. Same-projects samples likely to exhibit more iden-
tifier similarity than the cross-project samples.

PMQ 2: What is the fine-tuning sample efficiency of foundation models?
To observe the sample efficiency of foundations models, we con-
sider two best performing models from each family of models
(i.e.,, GraphCodeBERT from BERT-type encoder-only models and
CodeT5 from seq2seq generative models). We also introduce a hy-
brid model GCBhybrid in this paper (described in more detail
below), where we cascade the GraphCodeBERT encoder with a pre-
trained decoder. For this experiment, we use the java CodeXGLUE
code summarization dataset. Note that CodeT5 is the best perform-
ing model for this task achieving 20.32 BLEU-4, where GraphCode-
BERT reaches 19.22 BLEU-4 We sample datasets of different sizes
(10-300 examples) and observe the cross-project performance of the
three models. Table 1 presents that with 300 code-project samples,
CodeT5 achieves 18.23 BLEU-4 which is only about 2 BLEU-4 lower
than what it performs with the complete dataset of 165k samples.
This, with about 550 times as much data! Therefore, we can con-
clude that models like CodeT5 are fairly sample-efficient and can

2Jaccard Index is calculated as

|ùëã ‚à©ùëå |
|ùëã ‚à™ùëå |

perform well with a few data samples. However, CodeT5 struggles
to summarize code well, when less than 150 training samples are
available.

In the same-project fine-tuning scenario, this situation is quite
common, in many projects, as we argue later. Happily, our GCB-
hybrid model is highly sample efficient, and attains two-digit BLEU-
4 even with ten examples. This is because GCBhybrid‚Äôs pre-trained
decoder is especially trained to generate (denoised) comments. GCB-
hybrid dominates the CodeT5 until 150 samples become available.

#of samples

GraphCodeBERT GCBhybrid Codet5

10
50
100
150
200
250
300
Complete (‚àº165k)

4.88
9.29
10.02
10.33
10.57
10.73
10.58

19.22

11.37
13.7
14.73
14.98
15.51
15.63
15.71

19.97

1.38
1.84
2.32
14.93
18.64
18.81
18.23

20.32

Table 1: Fine-tuning Sample-efficiency of Foundation mod-
els

Observation 2. Pre-trained models can be adapted to be fine-
tuning sample-efficient; such models are competitive with State-
of-the-art for the code summarization task when samples are lim-
ited

The following sections will discuss same-project training for
code summarization using time series data and observe whether it
can outperform the cross-project training performance with a few
examples.

3 METHODOLOGY
This section briefly describes the dataset preparation and founda-
tion models we used for the evaluation.

3.1 Dataset Preparation
To evaluate the potential of same-project, sample-efficient training,
we prepare a new dataset from CodeXGLUE benchmark dataset.
There are three reasons for choosing CodeXGLUE

(1) The dataset is known to be appropriately de-duplicated, thus

avoiding issues raised in prior work [4, 29]

(2) We can more easily baseline our approach; most foundation

models have been evaluated on this dataset.

(3) This dataset provides the complete path to all the functions
with commit ID and line number. Using this information, we
can find out the creation date of that particular function, and
perform time-series partitioning.

Preparing a same-project dataset, and then partitioning for train-
ing and test, has to be done carefully, to avoid the risk of possible
data leakage from future to past instances during evaluation. There-
fore, we perform time-series partitioning: we sort the samples from
each project according to the creation date and perform an 80:20
split. 80% of data are used for training, and later 20% are randomly

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Toufique Ahmed and Premkumar Devanbu

Projects

oblac/jodd wildfly/wildfly

orientechnologies/orientdb Unidata/thredds

ngageoint/geopackage-android

Group II

Group I

oblac/jodd
wildfly/wildfly
orientechnologies/orientdb
Unidata/thredds
ngageoint/geopackage-android

0.16
0.06
0.07
0.07
0.05

0.08
0.16
0.07
0.06
0.04

0.08
0.06
0.17
0.06
0.05

0.06
0.05
0.05
0.10
0.05

0.05
0.03
0.05
0.04
0.19

Table 2: Intra and inter project identifier overlap

divided into test and validation sets. Note that we could not get the
exact 80:20 splits for all projects because several functions share the
same creation date at the point of splits. Now we will briefly discuss
the process followed for assigning creation date to the examples
and number of project used for the evaluation.
Assigning creation date to functions As already mentioned, Code-
XGLUE provides the commit ID and path to the original function
with specific line numbers. We use "git blame ‚Äìignore-rev" to ex-
tract the first commit date of a specific line. Note that most of the
functions are multi-line, and we use start and end line numbers in
the command to get the creation dates for all the lines. We consider
the earliest date as the creation date of the complete function. We
follow such a strategy because creation dates differ by line. Consider
the following code snippet.

p u b l i c s t a t i c BeanCopy from ( f i n a l O b j e c t

s o u r c e )

{

BeanCopy beanCopy = new BeanCopy ( s o u r c e ) ;
beanCopy . i s S o u r c e M a p = s o u r c e i n s t a n c e o f Map ;
return beanCopy ;

}

Figure 1: Example for assigning creation date

For the example presented above (Fig. 1,) we get the same time-
stamp (2015-08-26 12:57:28) for all the lines except the first one
(2018-01-13 01:41:10). The approach we are following to extract
the creation date has some limitations. ‚Äúgit blame ‚Äìignore rev‚Äù
reports the earliest commit that changes a specific line. The first line
was rewritten/edited from ‚Äúpublic static BeanCopy fromMap(Map
source) {‚Äù to ‚Äúpublic static BeanCopy from(final Object source)
{‚Äù on ‚Äú2018-01-13 01:41:10‚Äù, almost 2.5 years later than original
function creation date (2015-08-26 12:57:28). We are interested in
the original creation date instead of the last commit that changed
a single program line. Considering the change made to the first
line doesn‚Äôt introduce any major change into the program. It does
not even introduce or remove any identifier from the code. If we
consider the times-tamps of all the function lines, that may help
us get the original creation date. We consider the earliest commit
time-stamp (2015-08-26 12:57:28) as the function creation date, for
example presented in Fig. 1.

There is one case when this approach will fail. If all the program
lines have been modified over time at least once, we will fail to
predict the actual creation date of the program because ‚Äúgit blame
‚Äìignore rev‚Äù will not be able to report the original creation date
for any line of the program. However, this is very unlikely to hap-
pen. Another challenge is that we can only track down the history
recorded on GitHub. Our approach will fail if the programs are cre-
ated and edited on a local machine and then dumped into GitHub.

However, we believe our system will still give a fair amount of time-
sorted instances and can be used to evaluate the same-project data.
Note that we are following a very conservative rule while splitting
the project. Multiple functions with the same creation time-stamp
will not appear on the training set and validation/test sets.
Prepared datasets for different instance ranges While creating the
same-project dataset, we could only use the test data from Code-
XGLUE dataset. Pre-trained models (i.e.,, GraphCodeBERT, GCB-
hybrid and CodeT5) are trained using the CodeSearchNet dataset,
and there is a possibility of data leakage if we use the training set.
We avoided the validation set because pre-trained models were
evaluated on the validation set, and those models are likely to do
well if the projects are taken from the validation set. We consider
two popular programming languages (i.e., Python and Java) for
dataset generation.

Observing our experiments regarding sample efficiency, we di-

vided our projects into three different training instance ranges.

(1) Category I: Projects with 150+(more than 150) training sam-

ples (CodeT5 outperforms GCBhybrid).

(2) Category II: Projects with 100-150 training samples (GCB-

hybrid performs well in that range).

(3) Category III: Projects with 100-(less than 100) training sam-
ples (none of the models shows impressive performance).

Table 3 presents the number of projects from each category. We

selected 34 projects from 2 programming languages.

Category

Category I
Category II
Category III

Total

Language

Java

Python

10
6
2

18

7
7
2

16

Table 3: Number of projects from each category

3.2 Foundation Models
In this section, we briefly describe the foundation models that we
use to compare the performance of cross-project and same-project
training.
GraphCodeBERT CodeBERT is one of the first BERT-type encoder-
only pre-trained models specially designed for Software Engineer-
ing tasks. CodeBERT pre-trained with two objectives: i) MLM and ii)
Replaced Token Detection (RTD). Though CodeBERT is successful

Learning code summarization from a small and local dataset

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

in many downstream tasks, it does not use any code-related prop-
erty. Guo et al. [11] describe an encoder-only foundation model,
GraphCodeBERT, which uses two additional pre-training objec-
tives (i.e., edge prediction and node alignment) to MLM. The first
additional task is to predict code structure edges, and the second
aligns representations between source code and code structure.
These two objectives incorporate data flow in the pre-training
stage, a semantic-level code structure that encodes the relation
of ‚Äúwhere- the-value-comes-from‚Äù between variables. We evalu-
ate GraphCodeBERT in this paper to evaluate the effectiveness
of same-project training because GraphCodeBERT outperforms
CodeBERT in all downstream tasks, including code summarization.
CodeT5 CodeT5 [34] is a unified pre-trained encoder-decoder Trans-
former model well-suited for the seq2seq generative task. This
model is pre-trained with three objectives: i) Masked Span Predic-
tion (MSP), ii) Identifier Tagging (IT), and iii) Masked Identifier Pre-
diction (MIP). CodeT5 learns improved embedding by leveraging the
code semantics conveyed from the developer-assigned identifiers. It
also achieves the state-of-the-art performance in CodeXGLUE code
summarization task. Both GraphCodeBERT and CodeT5 are pri-
marily pre-trained with CodeSearchNet dataset. However, CodeT5
is pre-trained with some additional C and C# datasets.
GCBhybrid

Figure 2: Steps for preparing GCBhybrid

In the sample-efficiency experiment, presented earlier (Table 1),
GraphCodeBERT reached only 10.58 BLEU-4 even after fine-tuning
with 300 samples. Even the current SOTA, CodeT5, underperforms
until we fine-tune with atleast 200 samples. However, both models
do relatively well on the complete Java dataset, reaching 19.22 and
20.32 BLEU-4, respectively. Why do the pre-trained models under-
perform, with smaller fine-tuning datasets, even after training with
billions of unsupervised tokens? GraphCodeBERT does not have
a pre-trained decoder; it learns to generate comments only during
fine-tuning. Thus, it cannot produce good comments until it‚Äôs seen a
large number of samples. On the other hand, CodeT5‚Äôs pre-training
also trains the decoder; however, it‚Äôs trained to ‚Äúrefill‚Äô masked-out

span of code, rather than a complete natural language description.
The PLBART model is trained to denoise code and natural lan-
guage description; but it failed to outperform GraphCodeBERT,
even with its trained decoder, on Java code summarization (18.45
BLEU-4). We propose a hybrid model GCBhybrid where we cas-
cade the pre-trained GraphCodeBERT model with a specialized
decoder pre-trained to denoise natural language description. Such
a decoder help the model to do well on code summarization task
by incorporating prior knowledge for generating natural language
description.

Like GraphCodeBERT and CodeT5, We use CodeSearchNet
dataset for training the decoder. We use only the given training
partition of the CodeSearchNet to prevent any data leakage in the
fine-tuning stage because our final test dataset is taken from the
test partition of CodeSearchNet. We got approximately 2M natural
language descriptions to proceed. Following BART, we implement
five noising modes. Note that we apply two different types of noise
modes to each sample to enhance the dataset. In the next segment
of the paper, we will briefly explain the noise modes.
Comment permutation With this noise mode, we take one code-
related natural language description/comment at a time and shuffle
the tokens in random order.
Comment rotation We randomly choose one token at a time and
rotate the comment to bring that token to the first position of the
statement. Repairing such noise helps to model to pick the starting
of the comments.
Token deletion We randomly choose 15% of the tokens and drop
them from the comment. The model‚Äôs task is to recover those
dropped tokens and generate natural comments.
Token masking Like token deletion, we randomly mask out 15%
of the tokens and ask the model to recover them and generate the
comment using the decoder. Token masking is a comparatively
easier task than token deletion. In token deletion, the model needs
to learn both position and content of the missing token.
Token infilling We select a random span with span lengths drawn
from a Poisson distribution (ùúÜ = 3). We replace the span with a
single token <mask>. The model will recover the complete missing
span and learn about predicting the number of missing tokens in
the span.

Sequence Type

Sequence

Original
Return next line with tag masked with whitespace .
Comment permutation with line . masked whitespace next tag with Return
masked with whitespace . Return next line with tag
Comment rotation
Return line tag masked with whitespace .
Token deletion
Return next <mask> with <mask> masked with whitespace .
Token masking
Return <mask> tag masked with whitespace .
Token infilling

Table 4: Denoising natural language description

Table 4 illustrates a comment mutated with the 5 noise modes.
For training the decoder, we cascade a RoBERTa encoder with a
newly created, 12 layers transformer decoder model. To make this
hybrid model work, we need to ensure both encoder and decoder use
the same vocabulary. Therefore, we use the original GraphCode-
BERT vocabulary for this denoising task. To accelerate the training
process, GraphCodeBERT was initialized with CodeBERT, and
CodeBERT was loaded with the weights from the natural language

Pre-trained GraphCodeBERT EncoderUntrained 12 layers decoder Pre-trained GraphCodeBERT EncoderUntrained 12 layers decoder Cascade encoder-decoderFine-tuned GraphCodeBERT Encoderpre-trained 12 layers decoder Train to denoise commentspre-trained 12 layers decoder Drop the fine-tuned decoderpre-trained 12 layers decoder Cascade encoder-decoder again (GCBHYBRID)Pre-trained GraphCodeBERT Encoder(a)(b)(c)(d)(e)(f)Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Toufique Ahmed and Premkumar Devanbu

RoBERTa. We also initialized our encoder with GraphCodeBERT
and continued the denoising task for three epochs. Figure 2 depicts
the steps involved in preparing GCBhybrid. We startwith with (a)
the pretrained GraphCodeBERT model and (b) untrained 12 layers
transformer decoder. These are adjoined (c) together and trained
together (d) for the de-noising task described above. After sufficient
de-noising performance is achieved, the decoder has become pretty
good at generating comments. Now we detach just the decoder
(e) and adjoin it with the pre-trained original GraphCodeBERT
encoder, to create the (f) GCBhybrid model. We drop the fine-
tuned encoder because it is subject to ‚Äúcatastrophic forgetting‚Äù of
the knowledge learned in the base model [18].
Stress test: the ‚ÄúPolyGlot‚Äù Model Finally, as a stress-test of the same-
project training approach, we wondered if even a very extensively
fine-tuned model such as Pùëúùëôùë¶ùëîùëôùëúùë°GraphCodeBERT [3], which
fine-tuned on enormous (more than 900K) sample dataset, incorpo-
rate a diverse sample of project in many languages, could actually
benefit from fine-tuning on relatively small number of same-project
samples. For this, we took the published Pùëúùëôùë¶ùëîùëôùëúùë°GraphCodeBERT
model, and ran a few epochs of fine-tuning on same project data,
and evaluated in on same-project test data. We could do this with-
out fear of training-test data overlap, because the data partitions
provided by CodeXGLUE for pre-training and fine-tuning guaran-
teed that this ‚ÄúPolyGlot‚Äù model had not seen these projects during
its previous pre-training and fine-tuning.
Complete pipeline and baselines We are primarily investigating per-
formance relative to the cross-project models, that are fine-tuned
with hundreds of thousands of instances, with same-project mod-
els fine-tuned only a few hundred samples. Figure 3 presents the
complete pipeline of our approach. In the pre-processing stage, we
separate the same project data using a ‚Äúsegmenter‚Äù and convert it
to time-series data following the approach described in Section 3.1
in ‚Äúcreation date retriever‚Äù stage. After preparing data, we fine-tune
four foundation models (i.e.,GraphCodeBERT, GCBhybrid, ‚ÄúPoly-
Glot‚Äù, and CodeT5) for the code summarization task and compare
them with the performance achieved by those three models in the
cross-project setup (where there is no shortage of data).

4 RESULTS
In this section, we evaluate same-project training for the code
summarization task, in different settings.
Fine-tuning cross-project baselines & same-project models As men-
tioned earlier ( section 3), we compare our approach with the
models fine-tuned with abundant of cross-project data. We need
to fine-tune the baseline models because we look for the BLEU-
4 of a subset of test data while evaluating same-project training.
The repository of the baseline models (e.g.,GraphCodeBERT3 and
CodeT54) only provide cumulative (corpus) BLEU-4, which we can-
not map into results on the (test) subset within the same project.
For GraphCodeBERT, we fine-tune the model with 32 batch size, as
recommended by CodeXGLUE repository. However, we fine-tune
the CodeT5 model with 24 batch size instead of 48 to fit the model
into our Nvidia Titan RTX GPUs. We keep the other parameters
unchanged. We also train our proposed GCBhybrid model with the

3https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text
4https://github.com/salesforce/CodeT5

cross-project data for Java (‚âà 165ùëò training samples) and Python
(‚âà 251ùëò training samples). For same-project training, we replace
the cross-project samples with same-project data and fine-tune the
models using the same codebases used for fine-tuning the baselines.
During fine-tuning, the cross-project models stop improving
after 10 epochs, but same-project models continue improving even
after 20 epochs, because of the smaller training set. Therefore, we
fine-tune the same-project models for 30 epochs. Following all the
code relevant foundation models [1, 9, 34], we use smooth BLEU-
4 [21] as the evaluation metric.

4.1 Effectiveness of same-project training on

Category I projects

Table 5 presents the results of the Category I projects (with 150+
training samples) for Java. We achieve 18.65, 18.83, 19.52, and 19.72
on average with GraphCodeBERT, GCBhybrid, CodeT5 and the
"PolyGlot" models on the Java dataset in the cross-project fine-
tuning setup. PolyGlot very slightly outperforms the other two
models. In the same-project setup, we can see the encoder-only
GraphCodeBERT lags, because the untrained decoder has too few
samples from which to learn. However, same-project CodeT5 and
GCBhybrid perform really well, achieving 22.71 and 22.69 BLEU-4;
the "PolyGlot" model excels, refinings it‚Äôs already extensive multilin-
gual fine-tuning to reach 25.87 BLEU-4. All models significantly im-
proves over their cross-project counterpart (20.6% for GCBhybrid
and 16.2% for CodeT5). Roy et al. [28] reported that less than 2
points do not guarantee systematic improvements in summariza-
tion quality and are not trustworthy as proxies of human evaluation.
In this category, our best model shows over 6 BLEU-4 improvement
over CodeT5 and the "PolyGlot" model, which is 300% the Roy et
al. threshold. Hence, same-project training introduces systematic
improvement in code summarization task.

Same-project training also works for Category I python projects.
As per Table 6, same-project "PolyGlot", on average, beats the
next best prior work (CodeT5) by a solid 5.7 BLEU-4. Note that
CodeT5 and GCBhybrid also improve on same-project training,
while GraphCodeBERT does not.

Observation 3. Further same-project fine-tuning with 150+ sam-
ples helps the ‚ÄúPolyGlot‚Äù model beat all other prior conventionally
fine-tuned models by a substantial margin, on average.

4.2 Effectiveness of same-project training on

Category II projects

Table 7 presents the results of the Category II projects (with 100-150
training samples) for Java. We measure 17.72, 17.83, 19.18, 18.93
BLEU-4 on average with GraphCodeBERT, GCBhybrid, CodeT5,
and "PolyGlot" models on the Java dataset in the cross-project set-
ting. The performance of the cross-project setting is consistent with
the results we observe with Category I projects. However, CodeT5
underperforms with same-project training and scores only 5.33
BLEU-4 on average. In section 2, we found that CodeT5 generally
performs worse with less than 150 training samples. On the other
hand, our decoder-enhanced model GCBhybrid outperforms all the
cross-project models and achieves 21.34 BLEU -4, which is 2.16 higher

Learning code summarization from a small and local dataset

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Figure 3: Complete pipeline for data generation and model training.

Projects

Number of Samples

Cross-project

Same-project

Training Validation Test GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

oblac/jodd
wildfly/wildfly
orientechnologies/orientdb
Unidata/thredds
ngageoint/geopackage-android
RestComm/jain-slee
OpenEstate/OpenEstate-IO
tiefaces/TieFaces
jboss/jboss-common-core
rupertlssmith/lojix

Average

913
356
346
1341
239
184
196
281
209
336

114
44
43
167
16
25
10
14
17
41

114
45
44
167
16
25
10
15
17
42

17.98
12.53
15.21
14.18
24.19
14.24
21.89
36.74
17.16
12.36

18.65

17.93
13.57
14.21
15.89
22.91
16.21
15.39
37.17
22.26
12.8

18.83

16.98
16.16
16.22
16.36
32.24
17.33
18.7
32.37
17.33
11.54

19.52

17.63
15.32
14.51
15.11
21.42
12.87
17.99
38.99
29.06
14.32

19.72

14.26
10.41
11.16
11.82
15.92
5.35
12.44
25.53
11.01
9.72

12.76

20.54
13.93
16.05
16.68
40.72
13.20
13.28
61.74
17.42
13.5

22.71

20.71
14.67
15.8
16.07
38.77
17.33
3.01
64.02
21.83
14.65

22.69

20.21
14.92
16.52
17.26
34.95
16.85
22.23
72.12
29.04
14.62

25.87

Table 5: Effectiveness of same-project fine-tuning for code summarization task on category I Java projects

Projects

Number of Samples

Cross-project

Same-project

Training Validation Test GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

apache/airflow
tensorflow/probability
h2oai/h2o-3
Qiskit/qiskit-terra
chaoss/grimoirelab-perceval
PyCQA/pylint
SmokinCaterpillar/pypet

Average

435
425
215
376
188
271
277

53
53
26
43
24
33
35

54
53
27
43
24
34
35

16.16
17.9
13.72
23.13
14.59
17.91
19.1

17.50

16.41
18.86
15.03
22.81
11.69
17.89
18.36

17.29

17.11
22.18
16.29
24.27
14.79
18.91
16.61

18.59

18.14
22.39
14.17
19.93
11.12
19.27
15.72

17.25

10.33
13.49
10.69
17.65
26.26
12.89
8.63

14.28

16.39
18.19
14.31
21.24
38.7
17.23
16.16

20.32

17.96
21.33
16.25
24.13
36.82
20.12
19.37

22.28

19.46
20.76
14.59
25.65
50.27
18.36
20.85

24.28

Table 6: Effectiveness of same-project fine-tuning for code summarization task on category I Python projects

than the best performing cross-models. The "PolyGlot" model sur-
passes even this model, reaching 23.39 BLEU, over 4.2 BLEU-4 better
than the best conventionally fine-tuned (cross-project) model. We
find similar performance with Python also (Table 8)

Observation 4. With 100-150 same-projects samples, GCBhybrid
outperforms all the cross-project models. Again, "PolyGlot" does
best overall. However, CodeT5 underperforms in this sample-
range.

4.3 Effectiveness of same-project training on

Category III projects

Foundation models are pre-trained with billions of tokens. How-
ever, they are not trained to do code summarization task. They need
enough samples to fine-tune the objective of the models. Though
GCBhybrid scored 11.37 BLEU-4 (Table 1) with only 10 samples, it
is much lower than we usually achieve with Cross-project models.
Therefore, same-project training has some limitations. It requires a
certain number of samples to achieve comparable results to cross-
project models. We found that the models need at least 100 samples
to compete with cross-project models from Category II and Cate-
gory III projects. Table 9 has 4 projects in total (the first two from

Pre-trained modelCreation date retrieverSegmenterTest set from CodeXGLUEProject nameSame-project dataTime series dataTime series dataFine-tuningFine-tuned modelTest setDev setTraining setPre-processing dataFine-tuning foundation modelsConference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Toufique Ahmed and Premkumar Devanbu

Projects

Number of Samples

Cross-project

Same-project

Training Validation Test GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

real-logic/aeron
boonproject/boon
Koekiebox-PTY-LTD/Fluid
lessthanoptimal/GeoRegression
tony19/logback-android
spring-projects/spring-security

Average

128
123
100
120
118
132

16
14
13
15
16
12

16
15
13
16
16
13

20.54
20.14
24.46
16.91
12.15
12.12

17.72

19.58
21.83
21.83
13.18
15.87
14.73

17.84

18.54
25.16
22.51
16.35
18.88
13.64

19.18

17.84
22.00
21.37
17.72
17.58
17.04

18.93

11.4
16.94
20.1
8.75
6.61
5.53

11.56

23.22
20.7
39.23
14.17
14.71
16.01

21.34

13.54
1.87
10.89
3.64
1.5
0.54

5.33

27.07
25.72
31.6
19.00
20.84
16.12

23.39

Table 7: Effectiveness of same-project fine-tuning for code summarization task on category II Java projects

Projects

Number of Samples

Cross-project

Same-project

Training Validation Test GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

Nic30/hwt
vaexio/vaex
assemblerflow/flowcraft
funilrys/PyFunceble
pyca/pyopenssl
LionelAuroux/pyrser
OpenKMIP/PyKMIP

Average

124
124
113
104
100
102
150

15
15
14
13
13
11
18

16
16
15
13
13
12
18

9.64
15.9
14.01
16.63
19.81
16.33
14.57

15.27

15.81
17.23
14.03
25.82
27.47
16.91
15.66

18.99

14.18
15.62
14.39
22.97
23.69
17.53
17.03

17.92

16.93
17.08
18.56
27.05
23.1
13.75
18.8

19.32

5.79
9.2
9.04
19.11
15.22
7.23
31.38

13.85

12.8
13.6
14.24
31.06
24.87
13.98
39.11

21.38

4.36
2.2
1.1
5.67
17.24
3.22
41.17

10.71

17.05
18.19
25.42
38.65
25.4
20.79
42.59

26.87

Table 8: Effectiveness of same-project fine-tuning for code summarization task on category II Python projects

Python and later ones from Java). It shows that the same-project
models are underperforming with less than 100 samples and achieve
14.99 with GCBhybrid models. CodeT5 only achieves 2.84 BLEU-
4, whereas all the cross-project models achieve more than 15.36
BLEU-4. However, even in this case, ‚ÄúPolyGlot" dominates after
same-project fine-tuning, reaching 19.16; however, in this setting,
the improvement falls slightly shot of the 2 BLEU-4 threshold; how-
ever, it is important to note that this approach could be used with
other approaches to gain stronger improvements.

Observation 5. Same-project fine-tuning does help ‚ÄúPolyGlot‚Äù
model dominate again; however, by itself it provides only modest
gains with less than 100 samples, and may have to be used together
with other refinements

Figure 4: Complete lifespan and time needed to generate re-
quired number of samples.

5 DISCUSSION
In this section, we will discuss the feasibility of applying same-
project training and the computational cost needed for such training.
We will also present one motivating example at the end of this
section.

5.1 Feasibility of same-project training
So far, we have discussed the possible benefits of same-project
training; higher BLEU-4 scores can be attained with just 100 samples.
Our experiments suggest that an already extensively fine-tuned
model like Pùëúùëôùë¶ùëîùëôùëúùë°GraphCodeBERT can still benefit from a few
epochs same-project fine-tuning.

However, how long must we wait, after a project starts, to get
100 samples? This matters, because if it takes a long time to get
enough samples, the benefit of same-project training is perhaps
lower; one might just use cross-project training from older/existing
projects. We look into the project-wise lifespans, and time it takes

to net 100 fine-tuning samples for our Category I and II projects.
We track this data for both Python and Java, for each project in the
CodeSearchNet data, from project inception to the date the data
was collected. Many of these projects are still active

Table 4 shows the distribution of the project time spans, in days,
for both Java and Python projects. We show both the Complete
lifespan of the project (until last activity, or the current time, if
still active and the time elapsed until 100 fine-tuning samples are
available. It‚Äôs evident that Java projects ‚Äúlive‚Äù longer than Python
projects in our dataset. The median lifespan for java projects 2872
days (almost 8 years); however, the time required to generate 100
fine-tuning samples is 335 days (less than 1 year). That means for
nearly 7 years (85% of the total lifespan), these projects could benefit
from same-project fine-tuning. We have observed a similar situation
for Python also. The median lifespan for Python projects 1365 days
(almost 4 years), and the time required to generate 100 comments
is 496 days (less than 1.5 years). Python projects could also benefit

Learning code summarization from a small and local dataset

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Projects

Number of Samples

Cross-project

Same-project

Training Validation Test GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

GraphCodeBERT GCBhybrid CodeT5

PolyGlot
GraphCodeBERT

singularityhub/sregistry-cli
deepmipt/DeepPavlov
apache/parquet-mr
wro4j/wro4j

98
89
84
94

Average

10
14
10
13

11
14
11
14

10.98
15.27
18.01
17.12

15.35

12.64
16.8
17.51
15.92

15.72

14.6
16.72
19.5
17.99

17.20

12.68
18.39
21.83
16.42

17.33

9.35
8.4
10.16
7.76

8.92

11.36
20.79
14.15
13.67

14.99

2.51
5.69
1.2
1.96

2.84

11.2
26.5
21.57
17.35

19.16

Table 9: Effectiveness of same-project fine-tuning for code summarization task on category III projects

from same-project training for 64% of their lifespan. Therefore,
we can assume that the sample sizes sufficient for same-project
training become available reasonably early and can be used for the
remaining lifespan of the project.

5.2 Computational cost of same-project

training

Same-project training is computationally much cheaper! In fact, in
our experiments same-project training run is persisted for three
times as many epochs as the cross-project run; we do this get better
convergence with fewer samples; even so, we see significant gains
in fine-tuning costs. In the cross-project setting, we have 164,923
and 251,820 samples for Java and Python, respectively. To compare
with the same-project setting, we choose the projects with the
largest in-project fine-tuning datasets: 1341 for Java, and 435 for
Python. Even these big projects, same-project training takes just
2.43% (Java) and 0.52% (Python) of the time of cross-project training.
Considering all the Java and Python Category I and II projects, the
total training samples will be 5,122 and 3,004, across all projects..
Cumulatively, for all projects, same-project training takes just 9.31%
and 3.57% of the cross-project training time for Java and Python,
respectively. Note that we compare the computational efficiency
with respect to sample counts instead of time because the time
required for training a certain number of samples varies due to
server load.

5.3 Motivating Example
Now we present one illustrative example where our same-project
polyglot GraphCodeBERT model outperforms all the cross-project
and same-project models. Table 10 presents the results from dif-
ferent models for the examples presented in Figure 5. We can see
cross-project GraphCodeBERT, GCBhybrid, and polyglot Graph-
CodeBERT produce meaningful summaries for example I. Same-
project GCBhybrid also generates a complete sentence (0.36 BLEU-
4). However, our polyglot GraphCodeBERT fine-tuned with the
same project data gives the closest summary achieving 0.49 BLEU-4.

6 RELATED WORK
Code summarization Code summarization is a widely studied prob-
lem in software engineering. Developers spend around 59% of
their time on activities somewhat relevant to program comprehen-
sion [35], and good comments can ease the development and main-
tenance process by helping developers more quickly understand the
meaning of code under maintenance [30]. However, misaligned and
outdated comments are prevalent in SE projects. Automatic code

@Override
p u b l i c void b e g i n ( I n t e r p r e t a t i o n C o n t e x t
A t t r i b u t e s

a t t r i b u t e s )

throws A c t i o n E x c e p t i o n {

i c , S t r i n g name ,

hook = n u l l ;
i n E r r o r = f a l s e ;
S t r i n g c l a s s N a m e = a t t r i b u t e s . g e t V a l u e ( CLASS_ATTRIBUTE ) ;
i f

( O p t i o n H e l p e r . i s E m p t y ( c l a s s N a m e ) )

{

c l a s s N a m e = DefaultShutdownHook . c l a s s . getName ( ) ;
a d d I n f o ( " Assuming ‚ê£ c l a s s N a m e ‚ê£ [ " + c l a s s N a m e + " ] " ) ;

}
t r y {

a d d I n f o ( " About ‚ê£ t o ‚ê£ i n s t a n t i a t e ‚ê£ shutdown ‚ê£ hook ‚ê£ o f ‚ê£ t y p e ‚ê£ [ " +

c l a s s N a m e + " ] " ) ;

hook = ( ShutdownHookBase ) O p t i o n H e l p e r . i n s t a n t i a t e B y C l a s s N a m e

( className , ShutdownHookBase . c l a s s ,

c o n t e x t ) ;

hook . s e t C o n t e x t ( c o n t e x t ) ;
i c . p u s h O b j e c t ( hook ) ;

} c a t ch ( E x c e p t i o n e )

{

i n E r r o r = t r u e ;
a d d E r r o r ( " Could ‚ê£ n o t ‚ê£ c r e a t e ‚ê£ a ‚ê£ shutdown ‚ê£ hook ‚ê£ o f ‚ê£ t y p e ‚ê£ [ " +

c l a s s N a m e + " ] . " , e ) ;
throw new A c t i o n E x c e p t i o n ( e ) ;

}

}

Figure 5: Code for motivating example

summarization can help provide more faithful & current comments.
Code summarization can also help write new comments.

We can closely relate the code summarization task to Neural
Machine Translation (NMT) (e.g., translating English to German).
In NMT, an encoder-decoder-based framework is used to do the
translation task. Reachers in the SE domain have also adopted
such a framework for code summarization tasks. Systems like Co-
deNN [16], DeepCom [14], Astattgru [19], Rencos [36], NCS [2] and
many more applied different kinds of deep learning architecture
(e.g., LSTM [31] and Transformers [33]) on encoder-decoder frame-
work and show good performance on code summarization task.
Prior work [10, 28, 29] discuss the evaluation metrics and datasets
that have been used for code summarization task.
Foundation models for code summarization Foundation models [1,
9, 22, 24‚Äì26, 34] are currently state-of-the-art for the code sum-
marization task. Pre-training is key for foundation models, and
helps them learn the language‚Äôs statistical properties well. Since
the model already knows about the language, a few examples are
enough to train the model for a downstream task like code sum-
marization. In this paper, we show that the pre-trained models are
indeed sample-efficient and can outperform the models trained
with cross-project data. Note that there are more than 30 papers
that have been published in the last five years that follow some
form of encoder-decoder architecture for code summarization [28].

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Toufique Ahmed and Premkumar Devanbu

Original

Cross-project or
same-project?

Model name

Prediction

BLEU-4

Cross-project

Instantiates a shutdown
hook of the given class
and sets its name .

GraphCodeBERT Initialize the shutdown hook .
Initialize the shutdown hook .
GCBhybrid
This method is called at the
beginning of the action .

CodeT5

PolyGlot
GraphCodeBERT

Parses a shutdown hook .

GraphCodeBERT Get the the the .

Same-project

GCBhybrid

CodeT5

PolyGlot
GraphCodeBERT

Instantiate a hook of
the given class .
) { addInfo ( "Aboutto instantiate
shutdown hook oftype [" + ...
more random tokens
Instantiates a new shutdown
hook of the given class .

Table 10: Predictions for the example presented in Fig. 5

0.11
0.11

0.13

0.14

0.07

0.36

0.02

0.49

Comparing each model is beyond the scope of this paper. We pri-
marily emphasize same-project, sample efficient fine-tuning that
can be applicable to a wide range of models. Ahmed and Devanbu
discuss augmenting the dataset using multilingual training and
help models perform better. In contrast, we propose to reduce the
sample count and perform well using same-project data. Autore-
gressive generative language models, such as GPT-3 [6] have shown
strong on-task performance, even after very limited fine-tuning;
however, in our setting, without custom pre-training, as was done
for GraphCodeBERT, and GCBhybrid, it‚Äôs difficult to ensure that
the (enormously sized) pre-training data used in these models was
not already pre-polluted with the data we use for same-project
testing; these enormously sized models are too costly to pre-train,
except for the wealthiest organizations, so we omit these from our
evaluation.

7 THREATS
The main threats to our results arise from our evaluation approach.
We explore some of them below

Data Pollution. For external validity, and stability of results, it is
important ensure that we never test on data we used for pre-training
or fine-tuning. The CodeXglue dataset for code summarization is
split very carefully to avoid risk of data pollution; the pre-training
data is separate from the fine-tuning data, and the test data is
distinct from both of these. Our evaluation of the GraphCodeBERT
and GCBhybrid models adheres to this protocol. The ‚Äúpolyglot"
model is first fine-tuned on a large, multilingual dataset, of around
a million samples, from CodeXGlue, before project-specific fine-
tuning and same-project evaluation, on held-out set of projects.

Data Duplication. As described by Allamanis [4], duplication
lead to poor estimates of performance that don‚Äôt generalize. Fortu-
nately CodeXglue [23] is very carefully de-duplicated, and thus the
performance numbers we report here can be expected to be fairly
good.

External Validity. External validity threats may arise from size
of samples used to estimate performance, as well as whether the
representativeness of the samples. First, our results have statistical
significance: we have compare the performance of our best model
(‚Äúpolyglot‚Äù model) with the SOTA (CodeT5) using a paired, non-
parametric 2-sample test, and can reject the null hypothesis that
SOTA is the same or better than our best model. Second, our average
improvement on the BLEU-4 score is well above the 2 BLEU-4
threshold reported to be the barrier for humans to detect.

On the other hand, we have tested on a total of 18 Java projects
and 16 Python projects. In almost every setting our best model beats
CodeT5; but in some cases, it does not. Therefore, some caution is
warranted in assessing the external validity of our results.

8 CONCLUSION
The existence, and impact, of Project-specific phenomena in soft-
ware projects has been known for quite a while. The advent of
foundation models, which can be fine-tuned on-task, offers a pos-
sible direction to exploit project-specificity for better on-task per-
formance. We explore this direction, for the code summarization
task, with several popular foundation models. We find that same-
project training helps models exhibit competitive performance in
several settings. In particular, we develop a new kind of Graph-
CodeBERT model, named GCBhybrid, which combines Graph-
CodeBERT with a specially trained decoder. GCBhybrid exhibits
very high sample-efficiency, which further enables exploitation
of project specificity; except ‚Äúpolyglot‚Äù, GCBhybrid does achieve
state-of-the-art in some realistic same-project time-series settings.
We also find that same-project training offers substantial savings
in computational cost. In addition to code summarization, project-
specific fine-tuning is a general idea that could well prove an useful
adjunct for other tasks, such as defect prediction, fault localization,
de-obfuscation, or automated patching. Finally, the same-project
code summarization dataset and GCBhybrid source code are made
available anonymously at https://doi.org/10.5281/zenodo.6523229.

Learning code summarization from a small and local dataset

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-
fied Pre-training for Program Understanding and Generation. In Proceedings of
the 2021 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies. Association for Computational
Linguistics, Online, 2655‚Äì2668. https://www.aclweb.org/anthology/2021.naacl-
main.211

[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).
[3] Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for

Software Engineering. In Proceedings, ICSE.

[4] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Proceedings of the 2019 ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software. 143‚Äì153.

[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al. 2021. On the Opportunities and Risks of Foundation Models.
arXiv preprint arXiv:2108.07258 (2021).

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877‚Äì1901.

[7] Fuxiang Chen, Fatemeh Fard, David Lo, and Timofey Bryksin. 2022. On the
Transferability of Pre-trained Language Models for Low-Resource Programming
Languages. arXiv preprint arXiv:2204.09653 (2022).

[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[9] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: Findings.
1536‚Äì1547.

[10] David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment ?Translation?: Data, Metrics, Baselining & Evaluation. In 2020 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 746‚Äì757.

[11] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. GraphCodeBERT:
Pre-training Code Representations with Data Flow. In International Conference
on Learning Representations.

[12] Vincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks
the best choice for modeling source code?. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering. 763‚Äì773.

[13] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the naturalness of software. In2012 34th International Conference on
Software Engineering (ICSE).

[14] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation. In 2018 IEEE/ACM 26th International Conference on Program Comprehension
(ICPC). IEEE, 200‚Äì20010.

[15] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).

[16] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing source code using a neural attention model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). 2073‚Äì2083.

[17] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning and evaluating contextual embedding of source code. In International
Conference on Machine Learning. PMLR, 5110‚Äì5121.

[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural
networks. Proceedings of the national academy of sciences 114, 13 (2017), 3521‚Äì
3526.

[19] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In 2019

IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795‚Äì806.

[20] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising
sequence-to-sequence pre-training for natural language generation, translation,
and comprehension. arXiv preprint arXiv:1910.13461 (2019).

[21] Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating auto-
matic evaluation metrics for machine translation. In COLING 2004: Proceedings
of the 20th International Conference on Computational Linguistics. 501‚Äì507.
[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[23] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. CoRR abs/2102.04664 (2021).

[24] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
336‚Äì347.

[25] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal, Alec Peltekian,
and Yanfang Ye. 2021. CoTexT: Multi-task Learning with Code-Text Transformer.
arXiv preprint arXiv:2105.08645 (2021).

[26] Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou, Biao Cheng,
Daxin Jiang, Jiusheng Chen, Ruofei Zhang, et al. 2021. ProphetNet-X: Large-
Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code
Generation. arXiv preprint arXiv:2104.08006 (2021).

[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-
its of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683 (2019).

[28] Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing auto-
matic evaluation metrics for code summarization tasks. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 1105‚Äì1116.

[29] Ensheng Shia, Yanlin Wangb, Lun Dub, Junjie Chenc, Shi Hanb, Hongyu Zhangd,
Dongmei Zhangb, and Hongbin Suna. 2022. On the Evaluation of Neural Code
Summarization. ICSE.

[30] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-
Shanker. 2010. Towards automatically generating summary comments for java
methods. In Proceedings of the IEEE/ACM international conference on Automated
software engineering. 43‚Äì52.

[31] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems. 3104‚Äì
3112.

[32] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the localness
of software. In Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering. 269‚Äì280.

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998‚Äì6008.
[34] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-
aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and
Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing. 8696‚Äì8708.

[35] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan-
ping Li. 2017. Measuring program comprehension: A large-scale field study with
professionals. IEEE Transactions on Software Engineering 44, 10 (2017), 951‚Äì976.
[36] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd Inter-
national Conference on Software Engineering (ICSE). IEEE, 1385‚Äì1397.

[37] Thomas Zimmermann, Nachiappan Nagappan, Harald Gall, Emanuel Giger, and
Brendan Murphy. 2009. Cross-project defect prediction: a large scale experiment
on data vs. domain vs. process. In Proceedings of the 7th joint meeting of the
European software engineering conference and the ACM SIGSOFT symposium on
The foundations of software engineering. 91‚Äì100.

