Noname manuscript No.
(will be inserted by the editor)

What makes Ethereum blockchain transactions be
processed fast or slow? An empirical study

Michael Pacheco · Gustavo A. Oliva ·
Gopi Krishnan Rajbahadur ·
Ahmed E. Hassan

Received: date / Accepted: date

Abstract The Ethereum platform allows developers to implement and deploy
applications called ÐApps onto the blockchain for public use through the use
of smart contracts. To execute code within a smart contract, a paid trans-
action must be issued towards one of the functions that are exposed in the
interface of a contract. However, such a transaction is only processed once one
of the miners in the peer-to-peer network selects it, adds it to a block, and
appends that block to the blockchain This creates a delay between transaction
submission and code execution. It is crucial for ÐApp developers to be able to
precisely estimate when transactions will be processed, since this allows them
to deﬁne and provide a certain Quality of Service (QoS) level (e.g., 95% of the
transactions processed within 1 minute). However, the impact that diﬀerent
factors have on these times have not yet been studied. Processing time esti-
mation services are used by ÐApp developers to achieve predeﬁned QoS. Yet,
these services oﬀer minimal insights into what factors impact processing times.
Considering the vast amount of data that surrounds the Ethereum blockchain,
changes in processing times are hard for ÐApp developers to predict, making it
diﬃcult to maintain said QoS. In our study, we build random forest models to
understand the factors that are associated with transaction processing times.
We engineer several features that capture blockchain internal factors, as well
as gas pricing behaviors of transaction issuers. By interpreting our models, we
conclude that features surrounding gas pricing behaviors are very strongly as-
sociated with transaction processing times. Based on our empirical results, we

(cid:0) Michael Pacheco, Gustavo A. Oliva, and Ahmed E. Hassan
Software Analysis and Intelligence Lab (SAIL), School of Computing
Queen’s University, Kingston, Ontario, Canada
E-mail: {gustavo,ahmed}@cs.queensu.ca

Gopi Krishnan Rajbahadur
Centre for Software Excellence (CSE), Huawei, Canada
E-mail: gopi.krishnan.rajbahadur1@huawei.com

2
2
0
2

n
u
J

7
1

]
E
S
.
s
c
[

1
v
5
0
9
8
0
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Michael Pacheco et al.

provide ÐApp developers with concrete insights that can help them provide
and maintain high levels of QoS.

Keywords Transaction Processing Time, Smart Contracts, Ethereum,
Blockchain, Machine Learning, Regression Model, Model Interpretation

1 Introduction

A blockchain provides a secure and decentralized infrastructure for the exe-
cution and record-keeping of digital transactions. A programmable blockchain
platform supports smart contracts, which are stateful, general-purpose com-
puter programs that enable the development of blockchain-powered applica-
tions. Ethereum is currently the most popular programmable blockchain plat-
form. In Ethereum, these blockchain-powered applications are known as de-
centralized applications (ÐApps).

Transactions are the means through which one interacts with Ethereum.
Ethereum deﬁnes two types of transactions: regular transactions and contract
transactions. Regular transactions enable transfers of cryptocurrency (Ether,
or simply ETH) between end-user accounts. In turn, contract transactions
invoke functions deﬁned in the interface of a smart contract. When an end-
user triggers a certain functionality on the frontend of a ÐApp (e.g., checkout
the items in the shopping cart of an online shopping application), such a
request is translated into one or more blockchain contract transactions. ÐApps
themselves facilitate these transactions by submitting them to the blockchain
(e.g., Ethereum). That is, end-users are completely unaware that a blockchain
is in use, as transactions are handled entirely by the ÐApp.

In Ethereum, all transactions must be paid for, including contract trans-
actions. However, transaction prices are not prespeciﬁed. Transaction issuers
need to decide how much they wish to pay for each transaction by the assign-
ment of the gas price parameter, paid in Ether (ETH). The higher one pays
for a transaction, the faster it is generally processed. This is because there is
a ﬁnancial incentive that mining nodes (i.e., those who choose, prioritize, and
process all transactions on Ethereum) receive for transactions that they pro-
cess and verify. This incentive is a function of the gas prices that are associated
with the transactions they choose to process. As a result, the execution of code
on programmable blockchains such as Ethereum takes time, as the correspond-
ing transaction(s) must ﬁrst be chosen by the mining node that will append
the next block to the blockchain. Consistently setting up transactions with
high gas prices is not an option, since it would render the ÐApp economically
unviable. Hence, for a ÐApp to be proﬁtable, it needs to issue transactions
with the lowest gas price that fulﬁlls a predeﬁned Quality of Service (QoS)
(e.g., transactions processed within 1 minute in 95% of the cases). To achieve
such a goal, ÐApp developers make use of processing time estimation services.
Although existing processing time estimation services are useful, they oﬀer
little to no insight into what drives transaction processing times in Ethereum
(except for gas prices). This happens either because the underlying machine

Title Suppressed Due to Excessive Length

3

learning model that is used by these services is a complete black-box (e.g.,
Etherscan’s Gas Tracker1) or because the service does not clearly indicate the
features that drive the model’s decisions (e.g., EthGasStation2). The process-
ing time estimations made by these services impact the gas prices of trans-
actions sent by ÐApp developers. We thus argue that these services should
be interpretable. Without explanations or insights into the underlying mod-
els, the estimations provided by these services lack reliability, robustness, and
trustworthiness.

The very limited insights brought by current estimation services make
changes in processing times virtually unpredictable, since ÐApp developers
do not know exactly what metric(s) to track. As a consequence, ÐApp devel-
opers struggle to deﬁne proper gas prices and manage transaction submission
workloads. For instance, if overall gas prices are high, but those are likely to
lower soon, then ÐApp developers might be better oﬀ holding their transac-
tions for a while and submitting them later on in batches. Since no processing
time estimation service oﬀers such information, ÐApp developers are forced
to blindly rely on these services, and monitor and analyze information on the
Ethereum blockchain to discover impacting factors themselves in order to pre-
dict transaction processing times in the future. This issue calls for a uniﬁed,
free, and open-source alternative to the existing services, which would provide
DApp developers with explainable processing time models and predictions.

Building an explainable transaction processing time model entails discov-
ering and understanding the factors that inﬂuence (or are at least correlated
with) processing times. The resulting information surrounding the model will
allow ÐApp developers to make more informed decisions about what metrics
they choose to monitor in real-time in order to set adequate gas prices. As a
ﬁrst attempt at bridging this gap, in this study we set out to determine and
analyze the factors that are associated with transaction processing times in
Ethereum. In the following, we list our research questions and the key results
that we obtained:

RQ1: How well do blockchain internal factors characterize transac-
tion processing times? To answer this RQ, we ﬁrst build a random forest
model with an extensive set of features which capture internal factors of the
blockchain. (e.g., the diﬃculty to mine a block, or the characteristics of the
contracts and transactions) of the Ethereum blockchain. These features are
derived directly from the contextual, behavioral, and historical characteris-
tics associated with 1.8M transactions. Next, we interpret the model using
adjusted R2 and SHAP values.

Our model achieves an adjusted R2 of 0.16, indicating that blockchain internal
factors explain little of the variance in processing times.

RQ2: How well does gas pricing behavior characterize transaction
processing times? Similar to the stock market, the Ethereum blockchain

1 https://etherscan.io/gastracker
2 https://ethgasstation.info

4

Michael Pacheco et al.

ecosystem is impacted by a plethora of external factors, including specula-
tion, the global geopolitical situation, and cryptocurrency regulations. Many
of these factors are largely unpredictable and diﬃcult to be captured in the
form of engineered features. Yet, all these factors are known to impact the gas
pricing behavior of a large portion of transaction issuers. Hence, in RQ2 we
build a model that aims to account for the gas pricing behavior of transaction
issuers (i.e., the mass behavior). We do so by engineering additional features
that indicate how the price of a given transaction compare to that of recently
submitted transactions.
Our model achieves an adjusted R2 score of 0.53 (2.3x higher than the baseline
model), indicating that it explains a substantially larger amount of the variance
in processing times. The most important feature of our model is the median per-
centage of transactions processed per block, in the previous 120 blocks (~30min
ago), with a gas price below that of the current transaction.

Paper organization. The remainder of this paper is organized as follows.
Section 2 describes our study methodology, including the data collection, our
engineered feature-set, and the model construction. Sections 3 and 4 show
the motivation, approach, and ﬁndings associated with the research questions
addressed in this paper. Section 6 discusses related work. Section 7 discusses
the threats to the validity of our study. Finally, Section 8 states our concluding
remarks. Appendix A provides a summary of the key blockchain concepts that
we employ throughout this paper.

2 Methodology

We build a machine learning model with a comprehensive feature-set in or-
der to determine the factors that are strongly associated with the processing
times of transactions in Ethereum. In this section, we explain our approaches
to data collection (Section 2.1), feature engineering (Section 2.2), and model
construction (Section 2.3).

2.1 Data Collection

In this section, we describe the data sources that we used (Section 2.1.1) and
the approach that we followed to collect the data (Section 2.1.2).

2.1.1 Data Sources

Our study relies on two data sources, namely Etherscan and Google BigQuery:
Etherscan. Etherscan3 is a popular real-time dashboard for the Ethereum
blockchain platform. Etherscan has its own nodes in the blockchain, which are
used to actively monitor the activity of the network in real time. This includes

3 https://etherscan.io/

Title Suppressed Due to Excessive Length

5

monitoring various details regarding transactions, blocks, and smart contracts.
We use the data from this website to obtain transaction processing times (our
dependent variable) and to engineer several features used in our model (e.g.,
pending pool size and network utilization level). Data from Etherscan has been
used in several blockchain empirical studies ([32], [33], [? ])).
Google BigQuery. Google BigQuery4 is an online data warehouse that sup-
ports querying through SQL. In particular, Google also actively maintains
several public datasets, including an Ethereum dataset5. This dataset is up-
dated daily and contains metadata for several blockchain elements, including
transactions, blocks, and smart contracts. Most of the features that we engi-
neer rely on data collected from this dataset.

2.1.2 Approach

Figure 1 provides an overview of our data collection approach. In the following,
we discuss each of the data collection steps.

(Step-1) Draw a statistically representative sample of blocks for each
day within our data collection period. To bring data size to more man-
ageable levels, we draw a statistically representative sample of mined blocks
for each day within a one-month period. We use a 95% conﬁdence level and
± 5 conﬁdence interval, similarly to several prior studies [8, 42, 53]. Following
this sampling approach, we obtain a total of 10,865 blocks (362 blocks per day
on average). A replication package containing our studied data is available
online6

(Step-2) Retrieve the hashes of each transaction included in the
sampled blocks. We proceed by collecting and saving the transaction hashes
of all transactions that have been included within the sampled blocks. The
transaction hash is a unique identiﬁer used to diﬀerentiate between transac-
tions. We use these transaction identiﬁers in several of the data collection
steps to collect various associated metadata with each of these transactions.
We collect the transaction hash for all 1,825,042 (1.8M) transactions included
in the 10,865 sampled blocks based on the previous step.

(Step-3) Collect the processing time for each transaction. Next we
begin to collect the processing time provided by the Etherscan platform. For a
given transaction t, the processing time is the estimated diﬀerence between the
time t has been included in a block that has been appended to the blockchain,
and the time the user executed the submission of t onto the blockchain. We
retrieve this data by using the transaction hash of each transaction in our
set to access the corresponding processing time details provided by Etherscan
(deﬁned as conﬁrmation time by Etherscan).

4 https://cloud.google.com/bigquery
5 The dataset is bigquery-public-data.crypto_ethereum
6 https://drive.google.com/drive/folders/1nyPY3w8TOjne8RItv2TqfvJrfnnfbtI4?usp=

sharing

6

Michael Pacheco et al.

Fig. 1: An overview of our data collection approach. The dashed lines represent
a connection to a data source.

(Step-4) Retrieve additional metadata for transactions, blocks, and
contracts. We then leverage Google BigQuery to retrieve additional infor-
mation regarding transactions, including information about the blocks and
smart contracts they are associated with (when applicable). Examples of the
types of the collected metadata from this source includes details such as gas
price, block number, and network congestion. These metadata are obtained by
querying the transactions, blocks, and contracts tables.

(Step-5) Retrieve historical data regarding contextual aspects of the
Ethereum blockchain. In addition to the processing time, the Etherscan
platform provides historical data that describes some of the contextual in-
formation of the Ethereum blockchain. In particular, we collect data which
describes: 1) the amount of transactions in the pending pool7, and 2) the

7 https://etherscan.io/chart/pendingtx

FeatureEngineering(Section 2.2)Pending Pool Page Network Utilization Page (Step-1) Draw a statisticallyrepresentative sample ofblocks for each day withinour data collection period.(Step-2) Retrieve the hashesof each transaction includedin the sampled blocks.Transaction HashesBlockNumbersTx. Page(Step-3) Collect theprocessing time for eachtransaction(Step-4) Retrieve additionalmetadata for transactions,blocks, and contracts.Tx. processingtime(Step-5) Retrieve historicaldata regarding contextualaspects of the EthereumblockchainNetwork utilization and pending pool size over timeTx., Contract,and Block Metadata Title Suppressed Due to Excessive Length

7

network utilization8. We then map this set of data to each transaction, associ-
ating the relevant context of the blockchain to the time each transaction was
executed.

2.2 Feature Engineering

To understand the features that impact the transaction processing times, we
collect and engineer a variety of features. We explain the rationale behind
focusing on these dimensions, as well as the rationale for designing each of the
features in our set. Features were carefully crafted based on our scientiﬁc [19,
31, 32, 52] and practical knowledge of Ethereum (e.g., buying/selling/trading
ETH and watching the cryptocurrency market ﬂuctuations). We list each of
the features that we use for building our models in Table 1 and Table 2. The
deﬁnitions of these features touch on several key blockchain concepts. To assist
in the comprehension of the description and rationale of our features, we refer
the reader to Appendix A (background on blockchain key concepts).
1. Features that capture internal factors We ﬁrst engineer features that
aim to capture internal factors of the Ethereum blockchain (RQ1), which are
summarized in Table 1. These features span across a few dimensions, including
contextual, behavioral, and historical dimensions. We explain these dimensions
below.
– Contextual. Contextual data describes the circumstances in which a given
transaction is being processed. This information can be associated with the
entire blockchain network, and also the transactions themselves. For example,
the network utilization at the time a transaction was executed is a contex-
tual feature. The network utilization should help provide indicators of changes
to transaction processing times, such as increase in network traﬃc causing
processing times to be delayed. Similarly, a feature which deﬁnes the smart
contract a transaction had interacted with in its lifetime also provides contex-
tual insights.
– Behavioral. Behavioral features include information which relate to the user
and the particular values that were chosen for each of the transaction param-
eters. Depending on the experience of the user who is sending the transaction,
the values set for these parameters might impact transaction processing times.
Most notably, miners have the freedom to prioritize transactions to process
using their own algorithms. However, it is common for miners to prioritize
processing transactions with higher gas price values, as they receive a reward
for processing a particular transaction based on this value. An inexperienced
user who is not familiar with gas price values might set too low of a value,
resulting in an extremely long processing time.
– Historical. Historical features relate to features based on historical data from
transactions that have already been processed in the blockchain. Such metrics
are widely used in machine learning for predictive modelling tasks, as data

8 https://etherscan.io/chart/networkutilization

8

Michael Pacheco et al.

of the past is used to predict outcomes of the future. For example, a feature
which describes the average processing time for processed transactions in the
previous x blocks can be used by models to understand and predict processing
times for similar transactions in the future.

2. Features that capture changes in gas pricing behavior. We also
engineer features that aim to capture pricing behavior and changes in thereof
(RQ2). Our features primarily involve comparing the gas prices of transactions
processed in recent blocks with that of the current transaction at hand. These
features and their associated rationale are summarized in Table 2.

2.3 Model Construction

In this section, we describe our model construction steps. The goal of our
constructed model is to explain how features related to internal factors and
features related to gas pricing behavior inﬂuence the transaction processing
times respectively. We also aim to answer which of the studied features in-
ﬂuence the transaction processing times the most. To do this, we design and
follow a generalizable and extensible model construction pipeline to generate
interpretable models, as outlined in prior studied [20, 26, 47]. Rather than fo-
cusing on generating models that are meant to be used to predict transaction
processing times with high to perfect levels of accuracy, we instead leverage
machine learning as a tool to understand the impact of these features. Our
model construction pipeline in summarized Figure 2. We explain each step in
more detail below.

Fig. 2: A summary of our model construction approach.

(Step-1) LognormalizationTransaction Data(Step-2) Correlationanalysis(Step-4) ModelconstructionReduce noiseand biasaffecting modelinterpretationfrom correlatedfeatures Log transformtransactionprocessing time using  log(x+1) Reduce biascaused byoutliers found inboth independentand dependentfeaturesLog transformindependent featuresto reduce bias causedby outliers by usinglog(x+1)Use thevarclus function in Rto identify collinearfeatures (Spearman’srank correlation)(Step-3) RedundancyanalysisUse the redun functionin R to identifyredundant featuresBreak pairs ofcollinear features with |ρ| > 0.7, removing thefeature that is easierto obtainRemove redundantfeatures that can beexplained by modelsthat achieve  R2 ≥ 0.9Reduce noiseand biasaffecting modelinterpretationfrom redundantfeatures Use scikit-learn inPython to buildRandom ForestmodelsTitle Suppressed Due to Excessive Length

9

Table 1: The list of features used to capture internal factors of the Ethereum
blockchain, along with their rationale. These features are used to build our
models in 3 in order to predict transaction (tx) processing times. The • symbol
represents data retrieved from Etherscan, while the (cid:63) symbol represents data
retrieved from Google BigQuery. Braces ({}) group related features.

Type

Name

Description

Rationale

Contextual

contract_block_number(cid:63)

The block number of the smart contract that
the tx has interacted with.

contract_bytecode_length(cid:63)

The bytecode length of the smart contract the
tx has interacted with.

Behavioral

gas_price_gwei(cid:63)

The gas price value in GWEI set by the user of
the tx.

The day of the week the tx was executed on.

The hour of the day the tx was executed on.

Contextual patterns of the network, such as
increased usage, might revolve around certain
days of the week or hours in the day.

is_{erc721, erc20}(cid:63)

to_contract(cid:63)

pending_pool•

net_util•

day(cid:63)

hour(cid:63)

tx_nonce(cid:63)

value(cid:63)

A ﬂag indicating whether the smart contract the
tx has interacted with is an ERC721 or ERC20
(token-related) contract.

A ﬂag indicating whether the tx is interacting
with a contract.

The total amount of txs. in the pending pool
that have yet to be processed, at the time closest
to the time of sending the tx.

The network utilization of the network, as cal-
culated by Etherscan.

The total number of txs. sent by the user up
until the current tx.

Contracts that have been deployed onto the
Ethereum blockchain long ago might impact
processing times by containing ineﬃcient code,
security vulnerabilities, or bugs, causing them
to be avoided by miners.

Larger smart contract sizes might result in an
increase in execution time, which might also im-
pact processing times.

Contract txs. might be prioritized by miners,
and might also have longer execution times,
compared to user txs.

The amount of txs. in the pending pool, along
with network utilization, reﬂects the network
traﬃc at a given point in time. We predict an
increase in these metrics should also result in
increased tx processing times.

The gas price is directly related to tx processing
times - miners receive a monetary reward based
on the gas prices of the txs. inside the blocks
that they append to the chain. As a result, txs.
with higher gas prices should have fast process-
ing times.

User inexperience might be associated with set-
ting inadequate tx parameters, most impor-
tantly the gas price. Also, txs. with diﬀerent
nonce values might also be prioritized diﬀer-
ently by various miners.

The value of ETH the user is sending along with
their tx.

Diﬀerences in the ETH value might be priori-
tized diﬀerently by various miners.

tx_gas_limit(cid:63)

The gas limit value set by the user of the tx.

input_length(cid:63)

The length of the input data associated with the
tx.

Diﬀerence in gas limits might be prioritized dif-
ferently by various miners. For example, miners
might avoid speciﬁc txs. if they think it might
result in out of gas error.

Longer inputs might take longer to process in
smart contracts,
leading to longer execution
time, and also longer processing time.

Historical

{total, avg, med, std}_txs_
120(cid:63)

The total, average, median, and standard devia-
tion of all processed txs. across all previous 120
blocks, and the previous block.

Increases in the number of txs. being processed
in the recent past is likely to increase processing
times in the near future.

total_txs_1(cid:63)

The total number of txs. processed in the pre-
vious block.

avg_gas_price_gwei_prev_day•

The average gas price of txs. processed the pre-
vious day.

{avg, med, std}_difficulty_
120(cid:63)

The average, median, and standard deviation of
the diﬃculty of the previous 120 blocks.

difficulty_1(cid:63)

The diﬃculty of the previous block.

The diﬃculty is directly related to the time
taken to process blocks. This value is altered
based on the time taken by miners to append
blocks to the Ethereum blockchain.

{avg, med, std}_func_gas_
usage_120(cid:63)

The average, median, and standard deviation of
gas usage of the processed txs. in the previous
120 blocks, targeting the same smart contract
function as the current tx.

The gas usage from the smart contract txs. are
constantly targeting may indicate the popular-
ity and utilization of such contract, which may
be prioritized by miners in a particular way.

{avg, med, std}_pending_pool_
120•

{avg, med, std}_pend_prices (cid:63)

num_pending(cid:63)

The average, median, and standard deviation
of the number of txs. in the pending pool per
minute, during the appending of the previous
120 blocks.

The average, median, standard deviation of gas
prices (GWEI) of txs from the same user that
are currently in the pending pool and not pro-
cessed at the same time of the recorded tx For
the ﬁrst tx, the value of these features is zero.

The number of txs from the same user that are
currently in the pending pool and not processed
at the same time of the recorded tx.

As the amount of txs. in the pending pool in-
creases, miners have more txs. to prioritize,
which should increase processing times.

The more txs a user has that are simultaneously
waiting to be processed, the longer it could take
for a single tx of that user to be processed due
to the gas prices and nonce of the txs.

10

Michael Pacheco et al.

Table 2: The list of features used to capture gas pricing behavior, along with
their rationale. These features are used to build our models in Section 4 in
order to predict transaction (tx) processing times. The • symbol represents
data retrieved from Etherscan, while the (cid:63) symbol represents data retrieved
from Google BigQuery. Braces ({}) group a set of related features.

Name

Description

Rationale

{avg, med, std}_num_
{above, same, below}_
120(cid:63)

{avg, med, std}_pct_
{above, same, below}_
120(cid:63)

num_{above, same, below}_
{1, 120}(cid:63)

pct_{above, same, below}_
{1, 120}(cid:63)

gas_price_cat_enc(cid:63)

The average, median, and standard deviation
of txs. processed per block, in the previous
120 blocks, with a gas price above, equal to,
and below the current tx.

The average, median, and standard deviation
of the percentage of txs. processed per block,
in the previous 120 blocks, with a gas price
above, equal to, and below the current tx.

The total number of txs. processed per block,
in the previous 120 blocks and previous
block, with a gas price above, equal to, and
below the current tx.

The percentage of the total number of txs.
processed per block,
in the previous 120
blocks and previous block, with a gas price
above, equal to, and below the current tx.

The label encoded gas price category of the
tx calculated using the percentiles [0.2, 0.4,
0.6, 0.8, 1.0] of gas prices from all txs. from
the previous 120 blocks.

{avg, med, std}_gas_price_
{1, 120}(cid:63)

The average, median, and standard deviation
of tx gas prices per block, in the previous 120
blocks and previous block

past_{avg, med, std}_time•

closest_tx_pr_time•

The average, median, and standard deviation
of the processing times for the most recent
100 txs. in the previous 120 blocks, with the
most similar gas prices as the current tx.

The processing time of the most recent tx
processed in the previous 120 blocks with the
most similar gas price as the current tx.

The various aggregations of txs. sent in
previous blocks, combined with the
aggregations of the associated gas price
value, should hold stronger explanatory
power than these gas price and processing
time features alone. As a result, these
features should be powerful in explaining
variations in processing times.

The gas price is an incentive for miners to
prioritize one tx over another. This feature
aims to determine whether a certain gas
price is very low, low, medium, high, or very
high by comparing it to the prices of recent
txs.

Such aggregations capture current gas prices
being paid, and thus should help in predict-
ing future trends in gas prices and its rela-
tionship to processing times.

The aggregations of processing times
recorded for the most similar txs. within the
previous blocks will give the model a sense
of how long the current tx should be taking
to be processed.

(Step-1) Log normalization. We observe that the distribution of transac-
tion processing time is right skewed. Therefore, we log transform the data with
the function log(x + 1) to reduce the skew. Furthermore, we also log transform
all the independent features of the type numeric with an identical function to
reduce the bias caused by outliers, similar to prior studies [20, 26, 28, 44].

(Step-2) Correlation analysis. Several prior studies show that presence of
correlated features in the used data to construct a machine learning model

Title Suppressed Due to Excessive Length

11

leads to unreliable and spurious insights [20, 26, 29, 43]. Therefore, in this
step we outline our process to remove correlated and redundant features in
our data. To remove correlated features, we use the varclus function from
the Hmisc package in R9, which generates a hierarchical clustering of the pro-
vided features, based on the correlation between them. We choose Spearman’s
rank correlation (ρ) to determine the level of correlation between pairs, as
rank correlation can assess both monotonic and non-monotonic relationships.
Similar to prior studies [20, 26, 43], we use a threshold of |ρ| > 0.7 to deem a
pair of features as correlated. For each pair of correlated features, we choose
the feature that is easier to collect or calculate in practice. We summarize our
correlation analysis in Table 3 (Appendix B). As a result of such an analysis,
we removed a total of 41 correlated features.

(Step-3) Redundancy analysis In addition to correlated features, redun-
dant features may also negatively impact the generated explanations of a ma-
chine learning model [20, 26, 29, 43]. Hence, we choose to remove redundant
features. To do so, we use the redun function from the Hmisc package in R10.
This function builds several linear regression models, each of which selects one
independent feature to use as the dependent feature. It then calculates how
well the selected dependent features can be explained by the independent ones,
through the R2. It then drops the most well-explained features in a stepwise,
descending order. This feature removal process is repeated until either: 1) none
of the features can be predicted by the models resulting in an R2 greater than
a set threshold, or 2) removing the feature causes another previously removed
feature to be explained at a level less than the threshold. In our study, we use
the default R2 threshold of 0.9. As a result of such an analysis, we removed a
total of 1 redundant feature, namely std_pct_below_120.

(Step-4) Model construction At this step, we ﬁt our random forest model
using the RandomForestRegressor function from the scikit-learn package
in Python11. We choose a random forest model as it provides an optimal bal-
ance between predictive power and explainability. Such models are equipped
to ﬁt data better than simple regression models, potentially leading to more
robust and reliable insights [25]. Although they are not inherently explainable,
several feature attribution methods can and have been used to derive explana-
tions using these models [9, 24, 46]. Random forests have also been widely used
in several empirical software engineering research to derive insights about the
data [36, 45, 46]. Historically these models have shown good performance re-
sults for Software Engineering tasks [1, 4, 13, 51]. Finally, the speciﬁc random
forest implementation that we choose is natively compatible with the shap12
python package. The latter implements the SHAP interpretation technique,
which we employ to address our research questions (Section 3 and Section 4).

9 https://www.rdocumentation.org/packages/Hmisc/versions/4.5-0/topics/varclus
10 https://www.rdocumentation.org/packages/Hmisc/versions/4.5-0/topics/redun
11 https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.
RandomForestRegressor.html
12 https://shap.readthedocs.io/en/latest/index.html

12

Michael Pacheco et al.

2.4 Model validation

We use the adjusted R2 to evaluate how well our models can predict transac-
tion processing times. To compute the adjusted R2 of our constructed random
forest model, we set up a model validation pipeline that splits data into train,
test, and validation sets (Figure 3). Our train dataset needs to be as large
as possible, otherwise our model cannot identify possible seasonalities in the
data (e.g., evaluate the inﬂuence of the day of the week in transaction pro-
cessing times) nor changes in the blockchain context (e.g., network conditions
are unlikely to change in small periods of time). We thus deﬁne the training
set as the ﬁrst 28 days of data, the validation set as the 29th day, and the test
set as the ﬁnal day. We split the data along its temporal nature to avoid data
leakage [25]. We then use the training set to generate 100 bootstrap samples
(i.e., samples with replacement and of the same length as the training set).
For each bootstrap sample, we ﬁrst ﬁt a random forest model on that sample.
Next, we use the validation set to hyperparameter tune that model as recom-
mended by prior studies to ensure that our constructed model ﬁts the data
optimally. To do this we utilize a Random Search algorithm, and choose the
RandomizedSearchCV [5] from scikit-learn13. We choose to hyperparameter
tune the parameters: max_depth, max_features, and n_estimators. Once we
discover the optimal hyperparameters, we retrain the model using the chosen
parameters (on the same bootstrap sample) and evaluate it on the test set by
adjusted R2 measure. While prior studies typically measure the explanatory
power of a constructed model by measuring its R2 score on the training set,
we instead measure the performance of our hyperparameter-tuned model on a
hold-out test set. Our reasoning is that random forest models are by default
generally constructed in a way which causes all training data points to be
found in at least one terminal node of the forest. As a consequence, it is com-
mon for the random forest to predict correctly for the majority of instances
of the train set, resulting in an R2 that is very close to 1 (typically in the
0.95 to 1.0 range) [21]. In addition, we use adjusted version of the R2 measure,
which mitigates the bias introduced by the high number of features used in
our model [14]. As we generate 100 bootstrap samples, we obtain 100 adjusted
R2 values. We report summary statistics for this distribution of adjusted R2
values in the research questions below.

13 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.
RandomizedSearchCV.html

Title Suppressed Due to Excessive Length

13

Fig. 3: Our model validation approach. RF stands for Random Forest and HP
stands for hyperparameter.

Summary

• Total collected transactions: 1.8M transactions, spanning over 10.8k blocks.

• Data sources: Etherscan and Google BigQuery.

• Pieces of collected data: Metadata for blocks (e.g., number of included transactions),
transactions (e.g., gas price parameters, processing times), contracts (e.g., whether it
implements the ERC20 interface), network utilization over time, pending pool size over
time.

• Raw feature-set (pre-ﬁltering): 75 features (9 contextual, 5 behavioral, 20 historical,
and 41 for gas pricing behavior)

• Final feature-set (post-ﬁltering): 30 features (6 contextual, 4 behavioral, 9 historical,
and 11 for gas pricing behavior).

• Machine learning algorithm: Random forest regressor.

3 RQ1: How well do blockchain internal factors characterize
transaction processing times?

Motivation. It is currently unknown which factors from the internal workings
of the Ethereum blockchain best characterize transaction processing times.
There exists an exhaustive set of information recorded on the blockchain at
any given moment in time. For ÐApp developers, it is unclear which of these
factors are useful in predicting processing times, and thus which should be
monitored and analyzed.

Approach. We analyze the model’s explanatory power and feature impor-
tances. The details are shown below:

Generate 100bootstrapsamplesBootstrap  Sample 1HP Tune RF 1Validation Set(29th day)Test Set  (30th day)Train Set (28 days)Train Set (28 days)Evaluate RF 1Bootstrap  Sample 2HP Tune RF 2Evaluate RF 2Bootstrap  Sample 100HP Tune RF 100Evaluate RF 10014

Michael Pacheco et al.

– Analysis of a model’s explanatory power. To determine the explanatory
power of blockchain factors, we engineer features based on the internal data
recorded and available in the Ethereum blockchain (Table 1). Next, we use
these feature to construct (Section 2.3) and validate (Section 2.4) our random
forest model. As a result of applying our model validation approach, we obtain
a distribution of 100 adjusted R2 scores. This score distribution determines
how much of the total variability in processing times is explained by our ran-
dom forest model, thus serving as a good proxy for the explanatory power of
our model. We judge the explanatory power of our model by analyzing the
mean and median adjusted R2 scores.

– Analysis of feature importance. To determine what individual features are
most strongly associated with processing times, we compute and analyze fea-
ture importance using SHAP Lundberg and Lee [23], Molnar [30]. SHAP is
a model-agnostic interpretation technique based on the game theoretically
optimal Shapley values. SHAP ﬁrst determines an average prediction value
known as the base rate. Next, for each dataset instance, SHAP determines the
Shapley value of each feature. The Shapley value informs the degree to which
each feature moves the base rate (positively or negatively), in the same scale
as the dependent variable. The ﬁnal prediction is explained as: base rate +
(cid:80)n
i=1 Shapley(fi), where fi is one of the n features of the model. The impor-
tance of a feature fi corresponds to its distribution of Shapley values across
all dataset instances. More details about SHAP can be found in Appendix C.
To compute Shapley values, we ﬁrst train a single random forest model
on the entire train set (28 consecutive days) using the procedures that are
outlined in Section 2.3. Next, we use the TreeExplainer API from the shap
python package to compute the Shapley value of each feature for each dataset
instance. As a result, each feature will have an associated Shapley value dis-
tribution of size N , where N is the total number of rows in our train dataset.
Next, we convert these Shapley value distributions into absolute Shapley value
distributions. The rationale of such a conversion is that a feature f1 with a
Shapley value of x is as important as a feature f2 with a Shapley value of
−x. Finally, we employ Scott Knott algorithm to rank the distributions of
absolute Shapley values. The feature ranked ﬁrst is the one with the highest
Shapley values. We note that there can be a rank tie between two or more
features, which indicates that their underlying shapley distributions are indis-
tinguishable from an eﬀect-size standpoint. We also note that we use Menzies’
implementation [27] of the original SK algorithm [39], which (i) replaces the
original parametric statistical tests with non-parametric ones and (ii) takes
eﬀect size into consideration (Cliﬀ’s Delta). Menzies’ changes make the SK
algorithm suitable to be used in the context of large datasets, where p-values
are close to meaningless without an accompanying eﬀect size measure and few
or even no assumptions can be made in terms of the shape of the distributions
under evaluation.

Findings. Observation 1) Internal factors provide little explanatory
power in the characterization of transaction processing times. Our

Title Suppressed Due to Excessive Length

15

random forest models achieve identical mean and median adjusted R2 scores:
0.16. These scores indicate that internal factors can only explain approximately
16% of the variance in processing times.

Observation 2) Gas price, median price of pending transactions, and
nonce are the top-3 most important features of the model. Feature
importance ranks are summarized in Figure 4. It is unsurprising that the
transaction’s gas price (gas_price_gwei) is the most important feature, since
the incentive that miners receive to process transactions is a function of the gas
price of the transactions that they choose to process. Nevertheless, we reiterate
that our model achieves a median adjusted R2 of only 0.16 (Observation 1).
That is, despite its inherent importance, the gas price of a transaction explains
less than 0.16 of the variability in transaction processing times. We conjecture
that the explanation lies in the relative value of a given gas price. In a way, gas
prices can be interpreted as bids in an auction system. The actual, practical
value of a bid will always depend on the bids oﬀered by others. Hence, it is
entirely possible that the same gas price of x GWEI could be regarded as
high in one day (i.e., when other transactions typically have a gas price that
is lower than x) and low in some other day (i.e., when the opposite scenario
takes place). We explore this conjecture as part of RQ2, when we evaluate the
explanatory power of pricing behavior.

The second most important feature is the median gas price of recent pend-
ing transactions from the same transaction issuer (med_pend_prices). In
Ethereum, a transaction t from a given transaction issuer I can only be pro-
cessed once all prior transactions from I have been processed (i.e., those trans-
actions with nonce lower than that of t). For instance, a transaction can stay in
a pending state for a long time when a prior transaction from the same issuer
has a very low gas price. Hence, pending transactions penalize the process-
ing time of the current transaction. In fact, if the value of med_pend_prices
is higher than zero, then we know by construction that there exists at least
one pending transaction (since gas price cannot be zero). We assume that our
model has learned this rule. Interestingly, the third most important feature
is the transaction’s nonce (tx_nonce). Hence, we believe that our model is
combining the second and third most important features to estimate the time
penalty induced by pending transactions.

There is also a complementary facet to tx_nonce, which is bound to the
transaction issuer. A transaction with a high nonce indicates that its transac-
tion issuer has submitted many transactions in the past. Experienced trans-
action issuers are likely to be more careful in setting up their gas prices in
comparison to novice issuers (e.g., they might be more mindful about the
competitiveness of their chosen gas prices compared to novices) [31].

Observation 3) Contextual features have little importance. The most
important contextual feature is ranked only 7th. Its median Shapley value is
0.03 minutes (1.8 seconds). In other words, in at least 50% of the cases, this
features either adds or removes only 1.8 seconds to the ﬁnal prediction.

16

Michael Pacheco et al.

Fig. 4: The distribution of absolute Shapley values per feature (minutes). Fea-
tures are grouped according to their importance rank (lower rank, higher im-
portance). Due to the high skewness of the data, we apply a log(x+1) trans-
formation to the y-axis.

Summary

How well do blockchain internal factors characterize transaction processing
times?

Internal factors provide little explanatory power in the characterization of transaction
processing times. In particular:

• Our random forest models only achieve both mean and median adjusted R2 scores
of 0.16.

• Gas price, median price of recent pending transactions from the same transaction
issuer, and nonce are the top-3 most important features of our model.

• Contextual features have little importance.

123456791011121416171819gas_price_gweimed_pend_pricestx_noncedifficulty_med_120med_txs_120std_txs_120total_txs_1net_utilhourmed_pending_pool_120std_pending_pool_120daydifficulty_std_120tx_gas_limitcontract_block_numberstd_func_gas_usage_120valueis_erc20is_erc7210.00.10.20.30.40.51.02.03.0Absolute Shapley Values (minutes)Title Suppressed Due to Excessive Length

17

4 RQ2: How well does gas pricing behavior characterize
transaction processing times?

Motivation. The results of our previous research question indicates that in-
ternal factors of the Ethereum blockchain only explain a small amount of the
variability in transaction processing times.

Similar to the stock market, the Ethereum blockchain ecosystem is im-
pacted by a plethora of external factors, including speculation (e.g., NFTs,
Elon Musk Tweets), the global geopolitical situation (e.g., wars, the USA-
China trade war), cryptocurrency regulations (e.g., FED interventions), and
even unforeseen events (e.g., the COVID-19 pandemic). Many of these factors
are largely unpredictable and diﬃcult to be captured in the form of engineered
features. Yet, these factors are known to impact the gas pricing behavior of
a large portion of transaction issuers [2, 6]. For instance, a cryptocurrency
ban in a country can cause a massive amount of people to sell their ETH and
tokens, causing many ÐApps to send transactions to facilitate those actions,
thus increasing the average gas price being paid for all transactions in the
platform. Since the practical value of a gas price depends on the gas price
of all other transactions, we conjecture that pricing behavior (and changes in
thereof) inﬂuences the processing times of transactions.
Approach. The additional features that we engineer in order to capture gas
pricing behaviors are described in Table 2 along with their rationale. In sum-
mary, we introduce 42 additional features to our internal feature set. We pri-
marily involve aggregations of the number of previously processed transactions
in recent blocks and their processing times (historical features), in conjunction
with the gas price of these transactions (a behavioral feature). For instance,
one of our features indicates whether a given gas price is very low/low/medi-
an/high/very high by comparing it to the gas prices of all transactions included
in the prior 120 mined blocks. In a way, our model from RQ2 provides a relative
perspective on gas prices instead of an absolute one (RQ1).

Our approach is similar to that of RQ1. First, we reuse the model validation
approach from Section 3 to determine the explanatory power of our new model
(adjusted R2). Next, we compare the diﬀerence in adjusted R2 between the
models from RQ1 and RQ2. The rationale is to determine the importance of
the additional features (pricing behavior). We operationalize the comparison
by means of a two-tailed Mann-Whitney test (α = 0.05) followed by a Cliﬀ’s
Delta (d) calculation of eﬀect size. We evaluate Cliﬀ’s Delta using the follow-
ing thresholds [38]: negligible for |d| ≤ 0.147, small for 0.147 ≤ |d| ≤ 0.33,
medium for 0.33 ≤ |d| ≤ 0.474, and large otherwise. Finally, to understand
the importance of individual features, we plot their distribution of absolute
Shapley values and compute SK.

To further understand the role of individual features, we use Partial De-
pendence Plots (PDP). A PDP reﬂects the expected output of a model if
we were to intervene and modify exactly one of the features. This is diﬀer-
ent from SHAP, where the Shapley value of a feature represents the extent
to which that feature impacts the prediction of single dataset instance while

18

Michael Pacheco et al.

accounting for interaction eﬀects. We also compute pair-wise feature interac-
tions to understand the interplay between speciﬁc pairs of features. We use the
explainer.shap_interaction_values() function from the same shap pack-
age. This computes the Shapely interaction index after computing individual
eﬀects, and does so by subtracting the main eﬀect of the features to result in
the pure interaction eﬀects [30].

Findings. Observation 4) Our new model explains more than half of
the variance in processing times. With the new feature set, our random
forest models achieve identical mean and median adjusted R2 scores: 0.53. In
RQ1, the mean and median scores were also identical at 0.16. As the numbers
indicate, the pricing behavior dimension leads to an absolute increase of 0.37
in the median/mean adjusted R2. Indeed, the diﬀerence between the adjusted
R2 distributions is statistically signiﬁcant (p-value ≤ 0.05) with a large (d =
1) eﬀect size. We ﬁnd these scores conﬁrm the robust explanatory power of
our features, demonstrating the stability of our models. In particular, they
are compatible (and even sometimes outperform) adjusted R2 scores reported
in software engineering studies that built regression models with the purpose
of understanding a certain phenomenon [7, 16, 26]. For instance, Mcintosh
et al. [26] built sophisticated regression models using restricted cubic splines
to determine whether there is a relationship between (i) code review coverage
and post-release defects, (ii) code review participation and post-release defects,
and (iii) reviewer expertise and post-release defects. In total, the authors built
10 models and obtained adjusted R2 scores that ranged from 0.20 to 0.67
(mean = 0.46, median = 0.41, sd = 0.17). In comparison, the median adjusted
R2 score of our model is 0.53, which sits higher than the median score achieved
by their models. Most importantly, our score was computed on a hold-out test
set, while theirs were computed on the train set. As the hold-out test set is
unseen by the model during training, it is generally harder to achieve a higher
R2 on compared to the train set.

Observation 5) Gas price competitiveness is a major aspect in de-
termining processing times. Figure 5 depicts the distribution of absolute
Shapley values associated with each feature. The most important feature is the
median of the percentage of transactions processed per block, in the previous
120 blocks, with a gas price below that of the current transaction (med_pct_
below_120). The gas price per se (gas_price_gwei) only ranks 8th, with a
small median absolute Shapley value of 0.01.

Observation 6) There is a gas price at which transactions are pro-
cessed closest to the fastest speed possible (and thus increasing such
a price is *not* likely to further reduce the processing time.) The
PDP depicted in Figure 6 leads to several insights. First, the range of the
y-axis indicates that changes in med_pct_below_120 have a considerable in-
ﬂuence on the predicted processing times. Second, we note an inverse relation-
ship between med_pct_below_120 and the predicted processing time up until
med_pct_below_120 = 50%. That is, let p(B) be the lowest gas price used by
a transaction inside a mined block B. Let p(t) be the gas price of a transaction

Title Suppressed Due to Excessive Length

19

Fig. 5: The distribution of absolute Shapley values per feature in our model
with additional features. Features are grouped according to their importance
rank. Due to the high skewness of the data, we apply a log(x+1) transformation
to the y-axis.

t. Setting p(t) such that p(t) > p(B) is true for at least half of the 120 most
recently mined blocks already provides a speed boost in transaction process-
ing time. From med_pct_below_120 = 50% onwards, however, the processing
time does not decrease anymore.

In summary, (i) increasing the gas price beyond a certain value x does not
tend to reduce processing times any further and (ii) the value of x is dependent

123456781015171819202122232527282930med_pct_below_120med_pend_pricesstd_num_below_120tx_noncepct_same_1closest_tx_pr_timemed_txs_120gas_price_gweistd_txs_120std_pct_same_120std_pending_pool_120total_txs_1std_gas_price_1med_pending_pool_120past_std_timenet_utildifficulty_med_120std_gas_price_120hourdifficulty_std_120past_med_timetx_gas_limitcontract_block_numberdayvaluestd_func_gas_usage_120med_pct_same_120gas_price_cat_encis_erc20is_erc7210.00.10.20.30.40.51.02.03.0Absolute Shapley Values (minutes)20

Michael Pacheco et al.

on the prices of the transaction in the prior 120 blocks (otherwise the gas_
price_gwei would be more important than med_pct_below_120).

Fig. 6: A partial dependence plot for the med_pct_below_120 feature and
transaction processing time.

Observation 7) Information about the median price of pending trans-
actions and nonce remain important factors. Looking at Figure 5 again,
we note that the feature median price of pending transactions from the same
issuer (med_pend_prices) and transaction nonce (tx_nonce) are ranked sec-
ond and fourth. In RQ1, our baseline model suggests that these same two
features were ranked second (the same) and third (one rank lower) respec-
tively. In other words, these two features remain important factors.

We plotted the PDP for median price of pending transactions from the
same issuer (med_pend_prices) and observed that the y-axis range is very

0%25%50%75%100%0.40.50.60.70.80.90102030405060708090100med_pct_below_120 (%)E[f(x) | med_pct_below_120] (minutes)Title Suppressed Due to Excessive Length

21

constrained (range of 0.01 for 90% of the values of med_pend_prices). In
other words, the predicted processing time changes very little when med_pend_
prices is changed and everything else kept constant. Nevertheless, such a
feature ranks second according to SHAP (Figure 5). Hence, we conjecture
that med_pend_prices has strong interactions with some other feature. In
fact, in RQ1 we conjectured that the model could be using both med_pend_
prices and tx_nonce to penalize processing time due to pending transactions.
To investigate our conjecture, we calculated feature interactions between med_
pend_prices and every other feature. The results indicate that the strongest
interaction is indeed with tx_nonce. We then computed all pairwise feature
interactions and noted that the feature pair <med_pend_prices, tx_nonce> is
the second-strongest feature interaction in our model, only behind <med_pct_
below_120, std_num_below_120> (i.e., the ﬁrst and third most important
features).

Summary

RQ2: How well does gas pricing behavior characterize transaction processing
times?

Gas pricing behavior characterize transaction processing times much better than
blockchain internal factors alone. In particular:

• Our random forest models achieve a median adjusted R2 scores of 0.53.

• Gas price competitiveness is a major aspect in determining processing times.

• Setting a gas price above a certain value is unlikely to further reduce processing time.

• Number and gas price of pending transactions have a large inﬂuence on the processing
time of the current transaction.

5 Discussion

In the following section, we discuss the implication drawn from our ﬁndings in
the previous sections.
Implication 1) ÐApp developers should focus on quantifying how
competitive their gas prices are. Price competitiveness is the most im-
portant indicator for transaction processing times among our entire set of
engineered features. In particular, it is signiﬁcantly more important than in-
ternal blockchain factors that practitioners typically associate with transaction
processing times, including: block diﬃculty (ranks 17 and 20), the number of
transactions in the pending pool (rank 10), and network utilization (rank 15).
In practice, ÐApp developers should ensure that they execute transactions
with gas competitive gas prices and the competitiveness of a given speciﬁc
price is likely to change over time. Given our empirical results, we believe
that our metric med_pct_below_120 can serve as a starting point for ÐApp
developers to quantify price competitiveness. We warn developers, however,
that such a price should not be “too high”, since there is a point at which higher
prices do not result in lower processing times. More generally, future research

22

Michael Pacheco et al.

in explainable models for transaction processing times should investigate the
optimal number of past blocks to consider and how frequently such a number
needs to be ﬁne-tuned over time to avoid concept drift.

Implication 2) The architecture of ÐApps should take into account
the fact that processing time is only weakly associated with gas usage
and gas limit. The contribution of work lies in not only identifying those
feature that are most important, but also in identifying those that are not.

Figure 5 indicates that features related to gas usage, such as tx_gas_
limit and std_func_gas_usage_120 (which is correlated with med_func_gas_
usage_120), have little importance. This suggests that transactions with com-
petitive gas prices are prioritized by miners over the amount of gas that they
will potentially consume. Hence, if a ÐApp invokes a heavy computation func-
tion that consume lots of gas, then such a transaction will still require a com-
petitive gas price to be processed in a timely fashion. Such a ÐApp might be
costly and potentially unfeasible to maintain. A more frugal approach would
be to design a ÐApp architecture in which heavy computations are accom-
plished by means of multiple transactions that consume little gas each. We
acknowledge, however, that such an architecture might be unfeasible in spe-
ciﬁc scenarios or use cases (e.g., when the computation needs to be performed
as soon as possible). Most importantly, our work highlights an important and
yet not so obvious interplay between ÐApp architecture design and transaction
processing times.

Implication 3) We did not observe a strong relationship between
contextual features and processing time. However, future research
should not discard this type of feature. We observed that features in the
contextual dimensions generally contribute very little to the resulting predic-
tion. This may occur either because (i) contextual really do not matter that
much compared to pricing behavior or (ii) because contextual factors are not
strongly present in our dataset (e.g., seasonalities and network congestion). We
thus invite future research to further explore contextual features with other
datasets and timeframes, as well as engineer additional contextual features.

6 Related Work

We note that the analysis we conduct in our study on the explanatory power
of an exhaustive set of features with respect to transaction processing times
has not yet been investigated. We discover several insights using this proposed
approach, the majority of which suggest the importance of historical features
related to gas prices. We hope our contributions prove useful for ÐApp devel-
opers, and invite future research to build upon our study.

Pierro and Rocha [34] investigate how several factors related to transac-
tion processing times cause or correlate with variations in gas price predic-
tions made by Etherchain’s Gas Price oracle14. The authors conclude there

14 https://etherchain.org/tools/gasPriceOracle

Title Suppressed Due to Excessive Length

23

is non-directional causality between oracle gas price predictions and the time
to mine blocks, a unidirectional causality (inverse correlation) between oracle
predictions and number transactions in the pending pool, and a unidirectional
causality (inverse correlation) between oracle predictions and the number of
active miners.

In a later study the same authors Pierro et al. [35] evaluate the processing
time and corresponding gas price predictions made by EthGasStation’s Gas
Price API. To do this, the authors use processed transactions by retrieving
those with a gas price greater or equal to the gas prices in all processing time
predictions made by EthGasStation. They then verify whether those transac-
tions were processed in the following j blocks, as predicted by EthGasStation.
The authors conclude that EthGasStation holds a higher margin of error com-
pared to what they claim. Using a Poisson model, the authors achieve more
accurate predictions than EthGasStation by considering data within only the
previous 4 blocks. In turn, the authors suggest that such models should be
constructed using data within the most recent blocks, as opposed to the 100
previous blocks considered by EthGasStation, to improve prediction accuracy.
de Azevedo Sousa et al. [3] investigate the correlation between transaction
fees and transaction pending time in the Ethereum blockchain. The authors
calculate Pearson correlation matrices between several features across 1) the
full set of transactions, 2) distinct sets of transactions based on their gas usage
and gas price, and 3) clusters of transactions resulting from DBSCAN. The
authors conclude there is a negligible correlation between all features related
to transaction fees and the pending time of transactions.

Singh and Haﬁd [41] focus on the prediction of transaction conﬁrmation
time within the Ethereum blockchain. The study includes 1 million transac-
tions with non-engineered features from the blockchain. The authors discretize
transactions by their conﬁrmation times (15s, 30s, 1m, 2m, 5m, 10m, 15m,
and ≥ 30m). The majority of constructed classiﬁers (Naive Bayes, Random
Forests, and Multi Layer Perceptrons) achieved higher than 90% accuracy,
with the Multi Layer Perceptron classiﬁer regularly outperforming the others.
The impact of the transaction fee mechanism (TFM) EIP-1559 on process-
ing times is investigated by Liu et al. [22] in the Ethereum blockchain since its
relatively recent implementation. The authors discover that EIP-1559 makes
fee estimation easier for users, ultimately reducing both transaction processing
times and gas price paid of transactions within the same block. However, when
the price of Ether is volatile, processing times become much longer. We ﬁnd
that features related to historical gas prices are most important in estimating
processing time, and considering EIP-1559 makes fee estimations and thus as-
signing gas prices easier for users, similar TFMs may help ÐApp developers
avoid slow processing times altogether,

The study conducted by Kasahara and Kawahara [18] uses queuing theory
to investigate the impact of transaction fees on transaction processing time
within the Bitcoin blockchain. The authors conclude that transactions associ-
ated with high transaction fees are processed quicker than those that are not.
The authors also ﬁnd network congestion strongly impacts the processing time

24

Michael Pacheco et al.

of transactions, by increasing the time, regardless if they are associated with
either low or high fees, and regardless of block size. Conversely, our study ﬁnds
network utilization to hold minimal impact on transaction processing time.

Oliva et al. [32] analyze how smart contracts deployed on the Ethereum
blockchain are commonly used. The authors conclude that 0.05% of total smart
contracts were the target of 80% of all transactions executed during the time
period of their study. In addition, 41.3% of these contracts were found to be
deployed for token-related purposes. Our paper includes information regarding
the smart contract targeted by a transaction if applicable, though we do not
ﬁnd that these features have strong impacts on transaction processing time.

The work of Zarir et al. [52] investigates how ÐApp developers can develop
cost eﬀective ÐApps. Similar to our study, the authors conclude transactions
are prioritized based on their gas price, rather than their gas usage. In addi-
tion, the authors ﬁnd the gas usage of a function can be easily predicted. As
a result, the authors suggest that ÐApp developers provide gas usage infor-
mation in their smart contracts, and platforms such as Etherscan and wallets
should provide users with historical gas usage information for smart contract
functions.

7 Threats to Validity

Construct Validity. The pending timestamp of a transaction is not publicly
available information within the Ethereum blockchain. As a result, we rely on
Etherscan to provide an accurate measurement of when a transaction was ac-
tually executed. Because of the size, reputability, and popularity of Etherscan,
we continue to rely on it and reaﬃrm the accuracy of their provided data.

This paper is a ﬁrst attempt toward using predictive models in order to
derive and explain insights from data within the context of the Ethereum
blockchain. We choose to leverage Random Forest models as they hold an
optimal balance between performance and explainability. We encourage fu-
ture work to build upon the approach demonstrated in our study in order
to achieve models which achieve higher performance, and thus further con-
tribute to predicting transaction processing times and the generalizability of
the model explanations in our study.

Our Random Forest models are hyperparameter tuned in each bootstrap
to control states of the model as well as to achieve greater performance on the
hold out test set. We only consider a few hyperparameters to control, and there
exist multiple hyperparameters that can be tuned which were not considered
in our study. We invite future researchers to consider a more exhaustive set of
hyperparameters to tune in similar Random Forest models, and to investigate
their impact on performance.

Although we opt to leverage the adjusted R2 metric as a robust approach
to determine the goodness of ﬁt of the model, other performance metrics exist
that can, in certain contexts, contribute toward a complete assessment of the

Title Suppressed Due to Excessive Length

25

performance of the model. As such, future work should assess similar models
using additional performance metrics.

We fully rely on SHAP values to generate explanations for the predictions
of our Random Forest models. It is possible that diﬀerent feature importance
methods would generate diﬀerent results. We thus encourage future work to
explore other methods for explaining models (e.g., LIME [37]).

Internal Validity. In our paper, we choose to study the explanatory power
of a set of both internal and external features spanning across various internal
dimensions. The majority of features in our set include those which primarily
reﬂect technical aspects of the Ethereum blockchain. However, other types of
features, such as those that are based upon and or track market speculation
(e.g., Elon Musk tweets [2]), exist which might play a big role in describing
the behavior of the Ethereum blockchain. As a result, we invite future work
to design and investigate an even more exhaustive set of features which were
not included in our study.

The collected data and concluding results naturally depend on the deﬁned
time frame of our study. We also use sampling methods to construct our results,
though we draw a representative sample of blocks appended to the Ethereum
blockchain per day, in attempt to capture the complete and natural behavior
of the network during this period. Seasonal trends are known to impact the
blockchain [17, 49] and therefore may have aﬀected the insights and conclusions
derived in our study. As a result, we suggest that future studies should validate
our ﬁndings by replicating it using diﬀerent and longer time frames. Finally, we
emphasize that in this paper we demonstrate an approach that is extensible,
and therefore can be followed in similar contexts with similar goals.

In addition to the dependence on the chosen time frame, our study also
relies on actively collecting data from Etherscan. As Etherscan does not oﬀer
a robust API which allows us to collect all data points used in our study,
we developed other methods of data collection. Our data collection methods
respect Etherscan’s platform by adhering to standards they have set so as to
not disrupt the availability of their normal services. Because of this, we are
unable to collect every transaction executed within the Ethereum blockchain,
which might result in imbalanced data, such as collecting a low amount of
transactions that are processed in extremely fast or slow times.

External Validity. Our study focuses solely on the Ethereum blockchain dur-
ing a speciﬁc time frame. As it is likely for blockchains to be unique in purpose
and design, the consistency of the conclusions resulting from our study may
not hold in other blockchain environments. We encourage future research to
explore ideas and conduct replication experiments similar to ours in other
blockchain environments, as well as diﬀerent time frames, which may be fruit-
ful.

26

8 Conclusion

Michael Pacheco et al.

ÐApps on the blockchain require code to be executed through the use of trans-
actions, which need to be paid for. For ÐApps to be proﬁtable, developers need
to balance paying high amounts of Ether to have their application transactions
processed timely, and high-end user experience. Existing processing time esti-
mation services aim to solve this problem, however they oﬀer minimal insight
into what features truly impact processing times, as the platforms to not oﬀer
any interpretable information regarding their models.

With this as motivation, we collect data from Etherscan, and Google Big-
Query to engineer and investigate features which capture internal factors and
gas pricing behaviors. We use these features to build interpretable models us-
ing a generalizable and extensible model construction approach, in order to
then discover what features best characterize transaction processing times in
Ethereum, and to what extent. In particular, we discover that metrics regard-
ing the gas price information of transactions processed in the recent past to
hold the most explanatory power of all features. In comparison, the studied
features which capture gas pricing behaviors hold more explanatory power
than any feature dimension found in internal factors alone and combined. As
a result, ÐApp developers should focus on monitoring recent gas pricing be-
haviors when choosing gas prices for the transactions in their applications,
which will ultimately help in achieving a desired level of QoS. ÐApp develop-
ers should also avoid setting high gas prices that deviate too much from the
recent past to avoid diminishing returns. Finally, as gas price is much more
important than gas limit, ÐApp developers should avoid designing contract
functions which consume large amounts of gas by dividing such functions into
multiple ones which consume lower amounts.

Our most robust models achieve an adjusted R2 of 0.53, meaning almost
half the variance is not covered by the many features we collect and engineer.
As a result, we invite future work to investigate explainable models for trans-
action processing times which include additional features and reﬁne the ones
we propose.

We encourage future research to use the results of our experiments to
motivate the continuation of empirical studies within the area of blockchain.
In relation to our study speciﬁcally, possible topics for future research include:
i) the construction and interpretation of other types of models which can
more accurately predict processing times, ii) similar experiments conducted in
blockchain environments other than Ethereum, and iii) a similar or new set
of experiments conducted for comparison which use alternative parameters to
those in our study, such as time frames, statistical methods and techniques,
and additional features.

Title Suppressed Due to Excessive Length

27

Disclaimer

Any opinions, ﬁndings, and conclusions, or recommendations expressed in this
material are those of the author(s) and do not reﬂect the views of Huawei.

Conﬂict of Interest

The authors declared that they have no conﬂict of interest.

References

1. Aniche M, Maziero E, Durelli R, Durelli V (2020) The eﬀectiveness of supervised ma-
chine learning algorithms in predicting software refactoring. IEEE Transactions on Soft-
ware Engineering

2. Ante L (2021) How elon musk’s twitter activity moves cryptocurrency markets. Adver-

tising & Marketing Law eJournal

3. de Azevedo Sousa JE, Oliveira V, Valadares J, Dias Gonçalves G, Moraes Villela S,
Soares Bernardino H, Borges Vieira A (2021) An analysis of the fees and pending
time correlation in ethereum. International Journal of Network Management 31(3),
DOI https://doi.org/10.1002/nem.2113, https://onlinelibrary.wiley.com/doi/pdf/
10.1002/nem.2113

4. Bao L, Xia X, Lo D, Murphy GC (2021) A large scale study of long-time contributor
prediction for github projects. IEEE Transactions on Software Engineering 47(6):1277–
1298, DOI 10.1109/TSE.2019.2918536

5. Bergstra J, Bengio Y (2012) Random search for hyper-parameter optimization. Journal

of machine learning research 13(2):281–305

6. Binder M (2022) Bored ape yacht

to
astronomical levels. https://mashable.com/article/ethereum-gas-fees-skyrocket-
bored-ape-yacht-club-otherside-nft-launch, [Online; accessed 10-May-2022]

club caused ethereum fees

to soar

7. Bird C, Nagappan N, Murphy B, Gall H, Devanbu P (2011) Don’t touch my code!
examining the eﬀects of ownership on software quality. In: Proceedings of the 19th ACM
SIGSOFT symposium and the 13th European conference on Foundations of software
engineering, pp 4–14

8. Boslaugh S, Watters PA (2008) Statistics in a nutshell - a desktop quick reference
9. Breiman L (2001) Random forests. Machine learning 45(1):5–32

10. Buterin V (2014) Ethereum: A next-generation smart contract and decentralized appli-
cation platform. https://github.com/ethereum/wiki/wiki/White-Paper, [Online; ac-
cessed 20-November-2019]

11. Comben C (2018) What are blockchain conﬁrmations and why do they matter? https:
//coincentral.com/blockchain-confirmations, [Online; accessed 04-December-2019]
12. Esteves G, Figueiredo E, Veloso A, Viggiato M, Ziviani N (2020) Understanding machine
learning software defect predictions. Automated Software Engineering 27(3):369–392
13. Fan Y, Xia X, Lo D, Hassan AE (2020) Chaﬀ from the wheat: Characterizing and
determining valid bug reports. IEEE Transactions on Software Engineering 46(05):495–
525, DOI 10.1109/TSE.2018.2864217

14. Harrell F (2015) Regression Modeling Strategies with Applications to Linear Models,

Logistic and Ordinal Regression, and Survival Analysis, 2nd edn. Springer

15. Jakobsson M, Juels A (1999) Proofs of work and bread pudding protocols. In: Proceed-
ings of the IFIP TC6/TC11 Joint Working Conference on Secure Information Networks:
Communications and Multimedia Security, Kluwer, B.V., Deventer, The Netherlands,
The Netherlands, CMS ’99, pp 258–272

16. Jiarpakdee J, Tantithamthavorn CK, Grundy J (2021) Practitioners’ perceptions of the
goals and visual explanations of defect prediction models. In: 2021 IEEE/ACM 18th
International Conference on Mining Software Repositories (MSR), IEEE, pp 432–443

28

Michael Pacheco et al.

17. Kaiser L (2019) Seasonality in cryptocurrencies. Finance Research Letters 31
18. Kasahara S, Kawahara J (2019) Eﬀect of bitcoin fee on transaction-conﬁrmation process.
Journal of Industrial & Management Optimization 15(1547-5816_2019_1_365):365,
DOI 10.3934/jimo.2018047

19. Kondo M, Oliva GA, Jiang ZMJ, Hassan AE, Mizuno O (2020) Code cloning in smart
contracts: a case study on veriﬁed contracts from the ethereum blockchain platform.
Empirical Software Engineering 25(6):4617–4675

20. Lee D, Rajbahadur GK, Lin D, Sayagh M, Bezemer CP, Hassan AE (2020) An empirical
study of the characteristics of popular minecraft mods. Empirical Software Engineering
25(5):3396–3429

21. Liaw A (2010) [r] random forest auc. https://stat.ethz.ch/pipermail/r-help/2010-

October/257208.html, [Online; accessed 17-November-2021]

22. Liu Y, Lu Y, Nayak K, Zhang F, Zhang L, Zhao Y (2022) Empirical analysis of eip-1559:
Transaction fees, waiting time, and consensus security. arXiv preprint arXiv:220105574
23. Lundberg SM, Lee SI (2017) A uniﬁed approach to interpreting model predictions.
In: Proceedings of the 31st International Conference on Neural Information Processing
Systems, Curran Associates Inc., Red Hook, NY, USA, NIPS’17, p 4768–4777

24. Lundberg SM, Erion GG, Lee SI (2018) Consistent individualized feature attribution

for tree ensembles. arXiv preprint arXiv:180203888

25. Lyu Y, Rajbahadur GK, Lin D, Chen B, Jiang ZM (2021) Towards a consistent interpre-
tation of aiops models. ACM Transactions on Software Engineering and Methodology
(TOSEM) 31(1):1–38

26. Mcintosh S, Kamei Y, Adams B, Hassan AE (2016) An empirical study of the impact
of modern code review practices on software quality. Empirical Software Engineering
21(5):2146–2189, DOI 10.1007/s10664-015-9381-9

27. Menzies T (2020) Scott Knot with nonparametric eﬀect size and signiﬁcance test. https:
[Online; accessed 04-

//gist.github.com/timm/41b3a8790c1adce26d63c5874fbea393,
May-2021]

28. Menzies T, Greenwald J, Frank A (2007) Data mining static code attributes to learn
defect predictors. IEEE Transactions on Software Engineering 33(1):2–13, DOI 10.1109/
TSE.2007.256941

29. Midi H, Sarkar S, Rana S (2013) Collinearity diagnostics of binary logistic regression
model. Journal of Interdisciplinary Mathematics 13:253–267, DOI 10.1080/09720502.
2010.10700699

30. Molnar C (2020) Interpretable machine learning. Lulu
31. Oliva GA, Hassan AE (2021) The gas triangle and its challenges to the development
of blockchain-powered applications. In: Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, pp 1463–1466

32. Oliva GA, Hassan AE, Jiang ZMJ (2020) An exploratory study of smart contracts in
the ethereum blockchain platform. Empirical Software Engineering 25(3):1864–1904
33. Oliveira VC, Almeida Valadares J, A Sousa JE, Borges Vieira A, Bernardino
HS, Moraes Villela S, Dias Goncalves G (2021) Analyzing transaction conﬁrmation
in ethereum using machine learning techniques. SIGMETRICS Perform Eval Rev
48(4):12–15, DOI 10.1145/3466826.3466832

34. Pierro GA, Rocha H (2019) The inﬂuence factors on ethereum transaction fees. In: 2019
IEEE/ACM 2nd International Workshop on Emerging Trends in Software Engineering
for Blockchain (WETSEB), IEEE, pp 24–31

35. Pierro GA, Rocha H, Ducasse S, Marchesi M, Tonelli R (2022) A user-oriented model
for oracles’ gas price prediction. Future Generation Computer Systems 128:142–157
36. Rajbahadur GK, Wang S, Ansaldi G, Kamei Y, Hassan AE (2021) The impact of feature
importance methods on the interpretation of defect classiﬁers. IEEE Transactions on
Software Engineering

37. Ribeiro MT, Singh S, Guestrin C (2016) "why should i trust you?": Explaining the
predictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Association for Computing Ma-
chinery, New York, NY, USA, KDD ’16, p 1135–1144, DOI 10.1145/2939672.2939778

Title Suppressed Due to Excessive Length

29

38. Romano J, Kromrey J, Coraggio J, Skowronek J (2006) Appropriate statistics for ordinal
level data: Should we really be using t-test and Cohen’sd for evaluating group diﬀerences
on the NSSE and other surveys? In: Annual meeting of the Florida Association of
Institutional Research, pp 1–3

39. Scott AJ, Knott M (1974) A cluster analysis method for grouping means in the analysis

of variance. Biometrics 30(3):507–512

40. Signer C (2018) Gas cost analysis for ethereum smart contracts. Master’s thesis, ETH

Zurich, Department of Computer Science

41. Singh HJ, Haﬁd AS (2020) Prediction of transaction conﬁrmation time in ethereum
blockchain using machine learning. In: Prieto J, Das AK, Ferretti S, Pinto A, Corchado
JM (eds) Blockchain and Applications, Springer International Publishing, Cham, pp
126–133

42. Tagra A, Zhang H, Rajbahadur GK, Hassan AE (2022) Revisiting reopened bugs in

open source software systems. Empirical Software Engineering 27(4):1–34

43. Tantithamthavorn C, Hassan AE (2018) An experience report on defect modelling in
practice: Pitfalls and challenges. In: Proceedings of the 40th International conference
on software engineering: Software engineering in practice, pp 286–295

44. Tantithamthavorn C, McIntosh S, Hassan AE, Matsumoto K (2016) An empirical com-
parison of model validation techniques for defect prediction models. IEEE Transactions
on Software Engineering 43(1):1–18

45. Tantithamthavorn C, Hassan AE, Matsumoto K (2020) The impact of class rebal-
ancing techniques on the performance and interpretation of defect prediction models.
IEEE Transactions on Software Engineering 46(11):1200–1219, DOI 10.1109/TSE.2018.
2876537

46. Tantithamthavorn CK, Jiarpakdee J (2021) Explainable ai for software engineering. In:
2021 36th IEEE/ACM International Conference on Automated Software Engineering
(ASE), IEEE, pp 1–2

47. Thongtanunam P, Hassan AE (2020) Review dynamics and their impact on software

quality. IEEE Transactions on Software Engineering 47(12):2698–2712

48. Viggiato M, Bezemer CP (2020) Trouncing in dota 2: An investigation of blowout
matches. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence and In-
teractive Digital Entertainment, vol 16, pp 294–300

49. Werner SM, Pritz PJ, Perez D (2020) Step on the gas? a better approach for recom-
mending the ethereum gas price. In: Mathematical Research for Blockchain Economy,
Springer, pp 161–177

50. Wood G (2019) Ethereum: A secure decentralised generalised transaction ledger byzan-
tium version 7e819ec - 2019-10-20. URL https://ethereum.github.io/yellowpaper/
paper.pdf

51. Yatish S, Jiarpakdee J, Thongtanunam P, Tantithamthavorn C (2019) Mining software
defects: Should we consider aﬀected releases? In: Proceedings of the 41st International
Conference on Software Engineering, IEEE Press, ICSE ’19, p 654–665, DOI 10.1109/
ICSE.2019.00075

52. Zarir AA, Oliva GA, Jiang ZM, Hassan AE (2021) Developing cost-eﬀective blockchain-
powered applications: A case study of the gas usage of smart contract transactions
in the ethereum blockchain platform. ACM Transactions on Software Engineering and
Methodology (TOSEM) 30(3):1–38

53. Zhang H, Wang S, Chen TH, Zou Y, Hassan AE (2019) An empirical study of obsolete
answers on stack overﬂow. IEEE Transactions on Software Engineering 47(4):850–862

30

Appendix

A Background

Michael Pacheco et al.

In this section, we deﬁne the key concepts that are employed throughout our
study.
Blockchain. A blockchain is commonly conceptualized as a ledger which
records all transaction-related activity that occurs between peers on the net-
work. This activity data is stored through the usage of blocks, which is a data
structure that holds a set of transactions. Each of these transactions can be
identiﬁed by an identiﬁer called the transaciton hash. Blocks are appended
to one another to form a linked list type structure. A block stored in the
blockchain becomes immutable once a speciﬁc number of blocks has been ap-
pended to that block. This is because for one to change the information within
a speciﬁc block, they must also change all blocks appended after it as well.
Transactions. Transactions allow peers to interact with each other in the con-
text of a blockchain. There are two types of transactions used in the Ethereum
blockchain: user transactions and contract transactions. User transactions are
employed to transfer cryptocurrency (known as Ether in Ethereum) to an-
other user, who is identiﬁed by a unique address. Contract transactions allow
users (e.g., ÐApp developers) to execute functions deﬁned in smart contracts,
which are immutable, general-purpose programs deployed onto the Ethereum
blockchain.
Gas and transaction fees. Transactions sent within the Ethereum blockchain
require a transaction fee to be paid before being sent. This fee is calculated by
gas usage × gas price. The gas usage term refers to the amount of comput-
ing power consumed to process a transaction. This value is constant for user
transactions, costing 21 GWEI, equivalent to 2.1E-8 ETH. For contract trans-
actions, this value is calculated depending on the bytecode executed within a
smart contract, as each instruction has a speciﬁc amount of gas units required
for it to execute [50]. The gas price term deﬁnes the amount of Ether the
sender is prepared to pay per unit of gas consumed to process their trans-
action. This parameter is set by the sender, and allows users to incentivize
the processing of their transaction at a higher priority [40], as miners which
process the transaction are rewarded based on the transaction fee value.
Transaction processing lifecycle. The lifecycle of a transaction transitions
through various states, which are illustrated in Figure 7. The ﬁrst state of
the transaction processing lifecycle is when a user ﬁrst submits their transac-
tion t to the Ethereum blockchain (submitted state). Next, this transaction t
is broadcasted through the network by a peer-to-peer based communication
method. Once the ﬁrst node discovers transaction t, the transaction enters the
pending state as it enters the pending pool of transactions. As time passes
this transaction t is continuously discovered by an increasing amount of nodes
deployed on the network, which track and record all transaction related activ-
ity, including those that are pending. Mining nodes will compete to win the

Title Suppressed Due to Excessive Length

31

Proof-of-Work (PoW) [15], which will allow the winner to append their new
block b to the blockchain. If transaction t has been processed and recorded in
block b, transaction t then transitions into the processed state (i.e., at the block
timestamp of b). Transaction t will transition into the next state, conﬁrmed,
once a speciﬁc number of blocks n has been appended after b. This number
of blocks n has not been oﬃcially deﬁned, and many diﬀerent values exist in
various sources. For instance, n = 7 is stated in the Ethereum whitepaper [10].
Considering that blocks are appended every 15s on average, n = 7 translates
to approximately 1m 45s. Alternatively, some cryptocurrency exchanges deﬁne
n as a higher value - including Coin central, where n is deﬁned as n = 250,
translating to approximately 1h 2m 30s [11].

Fig. 7: Transaction lifecycle in the Ethereum blockchain.

Proof-of-work. Proof-of-Work (PoW) is a consensus protocol that requires
nodes to solve a diﬃcult mathematical puzzle. This protocol ensures that the
strategy used to solve the puzzle is optimal (i.e. no solution for the puzzle
exists other than a brute force solution). Fortunately the veriﬁcation of such
a solution is much cheaper in terms of computation. Ultimately this allows
the nodes, or entities within the network who do not know the authority of
one another, to trust each other and build a valid transaction ledger without
requiring a centralized third-party authority (e.g., a bank).

Next-generation ÐApps. Commonly, ÐApps expose the internal blockchain
complexities to end-users, ultimately requiring that they install a wallet (e.g.,
Metamask). A wallet is a computer program (commonly a web browser exten-
sion) that enables one to submit transactions in a blockchain without having
to set up their own node. However, submitting transactions with a wallet still
requires end-users to set up transaction parameters. Such a task is error-prone
by construction [31], as not all end-users are familiar with blockchain concepts
and their intricacies. Other problems include end-users getting confused with

2. PendingTransaction is executed and broadcasted onto the blockchain networkTransaction issuer submits a transactionMiner obtains PoW & the block with the transaction is appended to the chainN blocks are appended to the block containing the transaction3. Processed4. ConﬁrmedTransactionProcessingTime1. Submitted32

Michael Pacheco et al.

the user interface of wallets or even making typos while inputting the trans-
action parameter values. As an illustrative example, an Ethereum user called
Proudbitcoiner spent 9,500 USD in transaction fees to send just 120 USD to a
smart contract15. The user said the following (gas limit and gas price are two
transaction parameters and 1 GWEI = 1E-9 ETH):

“Metamask didn’t populate the ’gas limit’ ﬁeld with the correct amount in my previous
transaction and that transaction failed, so I decided to change it manually in the next
transaction (this one), but instead of typing 200000 in “gas limit” input ﬁeld, I wrote it
on the “Gas Price” input ﬁeld, so I paid 200000 Gwei for this transaction and destroyed
my life.”

Due to incidents such as the one above, alongside end-user onboarding
barriers, more recently ÐApp developers have started to engineer their appli-
cations using a new paradigm, which hides all blockchain complexities from
end-users. In this new paradigm, the burden of setting up transaction parame-
ters is on the ÐApp developers. ÐApp developers, in turn, can charge end-users
with a ﬂat fee (e.g., payable via credit card), such that they do not need to
hold Ether or use wallets, ultimately easing their onboarding process. We refer
to these ÐApps as the next-generation ÐApps. Next-generation ÐApps are a
reality and include ÐApps such as MyEtherWallet16.

transaction

15 The
can
0x5641c38bf30fd19ad3332100762017573ee676f35250dcf4a976bb0cfe31ac2f
16 https://www.myetherwallet.com/

seen

logs

be

at

https://etherscan.io/tx/

Title Suppressed Due to Excessive Length

33

B Model Construction: Correlation and Redundancy Analysis

Table 3: The independent features removed resulting from conducting the cor-
relation analysis in Section 2.3

Feature 2
avg_num_same_120
diﬃculty_med_120
num_above_120
avg_num_below_120
med_num_above_120
med_num_below_120
avg_txs_120
diﬃculty_med_120
med_func_gas_usage_120
med_txs_120
std_num_same_120
med_pending_pool_120
med_gas_price_1
med_gas_price_120
med_num_below_120
med_gas_price_1
med_pending_pool_120
gas_price_gwei
to_contract
contract_bytecode_length

Index Feature 1 (Removed)
num_same_120
1
diﬃculty_avg_120
2
med_num_above_120
3
num_below_120
4
avg_num_above_120
5
avg_num_below_120
6
total_txs_120
7
diﬃculty_1
8
avg_func_gas_usage_120
9
avg_txs_120
10
avg_num_same_120
11
avg_pending_pool_120
12
avg_gas_price_1
13
avg_gas_price_120
14
med_num_above_120
15
med_gas_price_120
16
pending_pool
17
med_gas_price_120
18
input_length
19
to_contract
20
avg_gas_price_gwei_prev_day med_gas_price_120
21
past_std_mins_120
past_avg_mins_120
22
std_func_gas_usage_120
med_func_gas_usage_120
23
std_func_gas_usage_120
contract_bytecode_length
24
pct_same_1
num_same_1
25
med_pct_above_120
pct_above_120
26
med_pct_above_120
avg_pct_above_120
27
avg_pct_below_120
pct_below_120
28
med_pct_below_120
avg_pct_below_120
29
med_pct_same_120
med_num_same_120
30
avg_pct_same_120
pct_same_120
31
med_pct_below_120
med_pct_above_120
32
std_pct_same_120
std_num_same_120
33
med_pct_below_120
med_num_below_120
34
std_pct_below_120
std_pct_above_120
35
std_pct_same_120
avg_pct_same_120
36
pct_above_1
num_above_1
37
pct_below_1
num_below_1
38
std_pct_below_120
std_num_above_120
39
pct_below_1
pct_above_1
40
med_pct_below_120
pct_below_1
41
med_pend_prices
num_pending
42
med_pend_prices
avg_pend_prices
43
med_pend_prices
std_pend_prices
44

34

C SHAP

Michael Pacheco et al.

To interpret our random forest models at the feature level, we use a model-
agnostic interpretation technique called SHAP [23]. The intuition behind SHAP
is shown in Figure 8. Assume that we are using a black-box model to predict
the chances of someone eventually having a heart attack (HT). The model’s
features are Age, Sex, BP (blood pressure), and BMI (body mass index). By
taking the mean of the HT column, we have an expected value for HT. We
refer to this expected value as the base rate. In the example shown in Fig-
ure 8 (left-hand side), this base rate is 0.1 (10%). For a particular instance
(row) in the dataset (Age = 75, Sex = F, BP = 180, and BMI = 40), the
black-box model outputs a HT of 0.4. The question now is: how much does
each feature contribute to moving the output from 0.1 (base rate) to 0.4 (the
actual prediction)? In an additive model such as a linear model, the eﬀect
of each feature is simply the weight of the feature times the feature value.
SHAP relies on game theory to extract the features contributions from any
machine learning model. As shown in the right-hand side of Figure 8, SHAP
indicates that BMI = 180 contributed with +0.1 to the base rate (positive
impact, shown in red), BP = 180 contributed with an additional +0.1 to the
base rate, Sex = F contributed with -0.3 to the base rate (negative impact,
shown in blue), and Age = 65 contributed the most with +0.4 to the base
rate. If we sum up all these individual feature contributions, we obtain the
ﬁnal prediction of HT = 0.4. Hence, SHAP is additive in nature and can easily
explain how each feature contributed to the prediction of a given instance in
the dataset. SHAP has been increasingly adopted in the Software Engineering
community [12, 36, 48].

Fig. 8: Intuition behind the SHAP model interpretation technique (adapted
from https://shap.readthedocs.io).

