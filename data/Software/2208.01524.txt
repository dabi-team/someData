2
2
0
2

g
u
A
2

]

R
C
.
s
c
[

1
v
4
2
5
1
0
.
8
0
2
2
:
v
i
X
r
a

A replication of a controlled experiment with two STRIDE
variants

Winnie Mbaka
w.mbaka@vu.nl
Vrije Universiteit Amsterdam
The Netherlands

Katja Tuma
k.tuma@vu.nl
Vrije Universiteit Amsterdam
The Netherlands

ABSTRACT
To avoid costly security patching after software deployment, security-
by-design techniques (e.g., STRIDE threat analysis) are adopted in
organizations to root out security issues before the system is ever
implemented. Despite the global gap in cybersecurity workforce
and the high manual effort required for performing threat analysis,
organizations are ramping up threat analysis activities. However,
past experimental results were inconclusive regarding some perfor-
mance indicators of threat analysis techniques thus practitioners
have little evidence for choosing the technique to adopt. To address
this issue, we replicated a controlled experiment with STRIDE. Our
study was aimed at measuring and comparing the performance
indicators (productivity and precision) of two STRIDE variants (el-
ement and interaction). We conclude the paper by comparing our
results to the original study.

CCS CONCEPTS
• Software and its engineering; • General and reference →
Empirical studies;

KEYWORDS
STRIDE, Threat Analysis, Controlled Experiment, Empirical Soft-
ware Engineering, Replication

ACM Reference Format:
Winnie Mbaka and Katja Tuma. 2022. A replication of a controlled experi-
ment with two STRIDE variants. In Proceedings of ACM Conference (Con-
ference’17). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

use of external (possibly vulnerable) libraries and to minimize the
attack surface of the software system.

Indeed, organizations are ramping up their investments in per-
forming architectural threat and risk analysis (latest BSIMM study
gathered data from 128 organizations and reports an increase by
more than 65% [16]). However, previous research has shown that
architectural threat analysis requires a high manual effort [38] and
demands the involvement of security and domain experts [7]. But
this is hampered by a significant shortage of the security workforce.
This trend has been reported across sectors and globally. According
to CyberSeek, the United States faced a shortage of about 314,000
(44% of the total, 716,000, employed cybersecurity workforce at the
time) security professionals in January 2019 [8]. Similar gap was
observed for the EU security workforce [6].

Pre-print

Empirical evidence of threat analysis performance indicators is a
crucial piece of the puzzle. First, such evidence supports security ex-
perts in understanding the trade-offs between the myriad of existing
threat analysis techniques (e.g., STRIDE [42], CORAS [28], attack
trees [35], threat patterns [1], PASTA [52], etc.). Second, favourable
performance indicators would result in cost saving for organiza-
tions where security experts are scarce. For instance, STRIDE can be
performed using two techniques, per-element and per-interaction.
The documentation of STRIDE-per-interaction includes a larger
threat mapping table is considered to be more suitable for expert use
and is “too complex to use without a reference chart handy” [42].
Yet, past empirical studies were inconclusive about some perfor-
mance indicators. For instance, past controlled experiments have
neither found significant difference between STRIDE variants [48]
for precision and productivity (whereas a significant difference
was measured for recall) nor conducted a statistical analysis of
equivalence.

1 INTRODUCTION
Security-by-design techniques [9, 37], aim to forefront some secu-
rity effort (to the design phase) and prevent costly security fixes to
software in production. For instance, threat analysis [27, 47] is used
to scrutinize the software architecture for potential security issues.
STRIDE [42] is a popular threat analysis technique developed by
Microsoft. The outcomes of such analyses include adopting miti-
gations early-on or planning architectural refactoring to limit the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

To address the above problems we conducted a replication of
a past controlled experiment comparing STRIDE-per-element to
STRIDE-per-interaction with 45 student participants. We have repli-
cated the analysis protocol from [48] and adopted the same mea-
sures of success (i.e., precision and productivity). Similar to the
previous study, we found no significant difference (or equivalence)
between the treatment groups in precision and productivity. In
addition, we also observe similar variations in the distribution of
identified threats per category (more spoofing, tampering, infor-
mation disclosure and denial of service compared to repudiation
and elevation of privilege threats) for both treatment groups. In
contrast to the original study, we found that per interaction teams
performed better (albeit, not significantly) which negates the claim
that this technique is more suitable for expert analysts. Finally,
we analyze and discuss the practice of study replication (with hu-
man participants) in software engineering. The statistical tests used

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Winnie Mbaka and Katja Tuma

to answer to our hypotheses can be observed in the replication
package 1.

The rest of this paper is organized as follows. Section 2 discusses
the related works and replication best practices. Section 3 describes
the research questions and methodology of the replication. In Sec-
tion 4 we present the results and discuss them in Section 5. Section 6
lists the threats to the validity of this study. Finally, Section 7 gives
the concluding remarks.

2 RELATED WORK
We positioned our contributions with respect to existing literature
on empirical studies of STRIDE and replication studies in the ex-
isting literature on requirements engineering or software design
analysis.

follow replication procedures however, their experimental analy-
sis is performed on the original data with the aim of getting the
same results. To this end, reproduction deals with re-analysing raw
data obtained from baseline experiments while replications leads
to generation of new data that can be combined with other studies
to provide joint conclusions [36].

Labunets et al [25] conducted a controlled experiment that was
replicated in [24] using student participants to compare two risk as-
sessment methods, a visual method (CORAS) and a textual method
(SREP). The first study found that the visual method was more
effective for identifying threats than the textual one. However, the
replicated experiment showed that the two techniques were (sta-
tistically) equivalent in terms of the quality of identified threats
and security controls. Similar to our work, the authors conduct
a TOST analysis to determine statistical equivalence between the
treatments.

Jung et al. [19] performed a within-subject experiment to com-
pare two safety analysis techniques, Component Integrated Fault
Trees (CFT) and Fault Trees (FT), with 7 phd candidates and repli-
cated the experiment with 11 practitioners. Concretely, the authors
compare the two techniques in terms of quality of results and per-
ceived method consistence, clarity, and maintainability. Similar to
our work, the authors find no major differences in terms of result
quality, and some differences were observed in the perceived mea-
sures. In contrast to our work, Jung et al. [19] compared safety
techniques and replicated the experiment with a different popula-
tion sample (i.e., practitioners).

Several studies have empirically compared [20, 21, 31] and con-
ducted replications [19, 33, 34] requirement engineering techniques
(e.g., requirements elicitation). For brevity, we direct the interested
reader a comprehensive review by Ambreen et al. [4].

Replication best practice. In order to get a more complete
overview of the current status of empirical research in software
engineering, we conducted a literature search on some of the ma-
jor (empirical) software-engineering venues. Our sample consisted
of papers form ESEM, IST, TSE and other relevant venues (e.g.,
RE, JSS, ESEJ, EASE, WICSA). We extracted the literature based
on a keyword search on Scopus which leaned 159 results in the
domain of computer science. We limited the search to papers inves-
tigating requirements engineering or software architecture (design)
approaches. We manually examined each paper, with regards to
the use of empirical research methods, recruitment of participants
(from academia or industry), subject sample size and background,
experimental processes (data collection, analysis and processing),
statistical tests and results (conclusion).

The resulting analysis was compared to the relevant baseline
study and populated in Table 1. In total, we analysed 15 studies
(which included replication and family of experiments) and limited
our analysis to papers produced in the last 10 years. From the
analysis we noticed a few trends.

We could observe two type of replications: close replications
and differentiated replications [26]. Close replications recreate the
same conditions of the original study with the aim of determin-
ing to what extent (if at all) the original study is replicable. On
the other hand, differentiated replications include deliberate vari-
ations of the original conditions or major effects, which allows
researchers to observe the impact of treatment variation on the

2.1 Empirical studies of STRIDE
In addition to the replicated study [48], several works have investi-
gated STRIDE empirically.

Scandariato et al. [38] performed a descriptive analysis measur-
ing the productivity, precision, and recall of STRIDE in an academic
setting. Their study reports similar values for a version of STRIDE-
per-element, however the conditions of the descriptive study were
different compared to our controlled experiment, therefore the re-
sults can not be directly compared. In comparison to [38], this study
is replicating a controlled experiment of two STRIDE techniques.
Two studies [5, 7] conducted case studies investigating the chal-
lenges of STRIDE. Bernsmed et al. [5] conducted semi-structured
interviews (with transcription code analysis) with agile organiza-
tions and recorded the perceived challenges. The authors report
that practitioners see value in performing STRIDE despite the high
manual effort it requires. Other discovered challenges were related
to the lack of expertise by developers conducting the analysis, and
uncompatibility of systematic approaches with the Agile workflow.
Stevens et al. [45] conducted a qualitative case study to investi-
gate the efficacy the Center of Gravity (CoG) technique in an indus-
trial setting. The CoG, originally conceived as a military strategy,
is a risk-first threat analysis technique but has not been extensively
used to analyze software security. The authors designed surveys
and classroom sessions and involved 25 practitioners in the study.
Similar to other studies conducted with experts, they report a very
high accuracy of the participant results.

Pre-print

2.2 Replications
Due to the nature of most software engineering studies, drawing re-
liable conclusions from a single study might be prone to errors [43].
Hence, researchers have resolved to using replication methods as a
means of tackling these limitations. According to [30] replication
would allow the experiment to amass enough “evidence” to be con-
vincing (if successful). Generally, SE replication studies are directly
related to the concept of applying similar experimental procedures
as in the original study, on a different participation pool. According
to [36], this process is aimed at generating new/raw data. However,
it is crucial to differentiate between two important concepts, repli-
cation and reproduction. In the latter, reproduction, researchers

1https://doi.org/10.5281/zenodo.6513366

2

A replication of a controlled experiment with two STRIDE variants

Conference’17, July 2017, Washington, DC, USA

Table 1: Analysis of Replicated Studies. Keyword ‘same’ refers to the replication maintaining the exact measures as the original
study, ‘similar’ to slight variations in the measures, ‘Different’ to major variations in the measures, ‘extra steps’ to additional
measures and processes being used, and ‘complements’ to support the results and introduce relevant additional information.

Paper

Research
Methods
(EX, QEX)
EX
[51]
EX
[2]
EX
[10]
QEX
[33]
EX
[15]
EX
[22]
EX
[50]
EX
[19]
EX
[32]
EX
[13]
EX
[14]
EX
[39]
EX
[40]
[17]
EX
This work EX

RQs

Subjects

Population size

Population
Background

Data Collection Data Cleaning Data Processing

Statistical Test Conclusion

Provides Repli-
cation Package

Similar

Academia
Academia

Similar
Similar
Similar
Same
Different Both
Different
Different Academia Different
Academia
Same
Academia Different
Same
Academia Different
Similar
Similar
Industry
Same
Industry
Similar
Different
Academia Different
Same
Different
Both
Same
Academia
Same
Same
Academia Different
Same
Academia Different
Same
Academia Different
Similar

Same
Different
Same
Similar
same
Same
Same
Different
Same
Same
Same
Same
Same
Similar
Similar

Same
Similar
Same
Similar
Similar
Same
Same
Same
Same
Same
Same
Extra Steps
Extra Steps
Same
Similar

Same
Same
Same
Extra Steps
-
Same
Same
Same
Same
Same
Same
Same
Same
Same
Same

Same
Same
Same
Extra Steps
Same
Similar
Same
Same
Same
Same
Same
Same
Same
Same
Extra Steps

Additional
Same
Different
Same
Additional
Different
Different
Same
-
Additional
Same
Different
Same
Same
Different

Yes
Partly Supports
Yes
Partly Supports
Yes
Supports
No
Partly Supports
No
Partly Supports
Upon Request
Supports
No
Complements
No
Supports
No
Partly Supports
Yes
Non conclusive
No
Supports
Yes
Supports
No
Non conclusive
Supports
No
Partly contradicts Yes

study outcomes. First, close replications [51] [2] [33] [15] [32] can
only partially support/contradict the original study. Indeed, the
issue with close replication studies in software engineering was al-
ready observed by Lung et al. [29]. Instead, replications that go one
step further [10] [22] [50] [19] [14] [39] [17], by either changing the
way they analyse (using statistical tests) or analyze from different
perspective tend to either support or contradict completely.

Pre-print

3.1 Research questions
Our main motivation for conducting this study is to provide a refer-
ence direction for researchers interested in undertaking controlled
experiments in the field of threat modeling using STRIDE by measur-
ing two dependent variables, productivity and precision. According
to [3] productivity is the quintessential indicator of efficiency in
any production system. Similarly, in this study, productivity is de-
fined as the measure of correctly identified output (True positive)
produced within a specified time-frame. In practice, the STRIDE
threat modeling process is time-consuming [38]. Any organization
or team that is new to threat modeling may be overwhelmed with
information on STRIDE application and potentially miss out on
important aspects to consider. Even experienced threat modelers
may find this method more time consuming and thus adversely
affecting productivity. However, [23] claims that providing a struc-
tured approach to threat modeling, similar to the one presented in
this paper, can be used to mitigate time constraints. On the other
hand, measuring precision, defined as correctness of analysis, is
crucial in maintaining high productivity. To this end, there is a
direct correlation between high productivity and high precision
and vice versa. For instance, It would be impractical for a team
to have high precision if they have a high number of false posi-
tives (TP< FP). From a scientific perspective, both productivity and
precision obtained in the previous study will be compared to this
replication in an attempt at understanding how a different partici-
pation pool perceives and handles similar tasks. The difference, or
lack thereof, of statistical significance in productivity and precision
between the two studies will be used inform the practicability of us-
ing non-experts in practical implementation of STRIDE. To achieve
the aforementioned objectives, this study addresses the following
research questions (RQs):

Second, we noticed that, on average, less studies provide replica-
tion packages. Availability of replication packages for experiments
encourages better replications and complementary studies [43].
However, availability of information packages and transfer are still
an open question in SE [44]. Particularly for studies involving hu-
man participants, researchers are often additionally constrained to
share the complete data set due to privacy concerns.

Third, most studies (73% of our analysed papers) use only aca-
demic participants. Although [25], [46] agree that students are
well suited to perform empirical studies, [11] argues that the low
proportion of professionals used in SE experiments may reduce
experiments realism by inhibiting the understanding of industrial
software processes. Fourth, many studies do not report important
information (such as participant reward, target population, hypoth-
esis, and conclusion and construct validity), as also observed in [12].
While [18] outlines the guidelines for reporting controlled experi-
ments studies, the results shown in this research is a clear indication
that some aspects of reporting need improvement. However, we
understand that some limitations (space, number of pages) are of-
ten a determining factor to the amount of information included
in reports. None-the-less, it is important to include necessary in-
formation to facilitate better understanding and future replication
efforts (either internal or external).

3 RESEARCH METHODS
This work is a replication study of the controlled experiment in [48].
This replication follows the experimental procedure of the original
study. The primary goal of the present study is to examine whether
the hypotheses in the previous study are upheld given a different
participation pool. As a secondary goal we also aim to observe
how experiments with threat analysis techniques using human
participants are replicated.

3

RQ1:Productivity. Are the means of productivity between treat-

ment groups equivalent?

The following null and alternative hypotheses were formulated:
𝐻𝑝𝑟𝑜𝑑0: The productivity difference between the two group means

is outside the equivalence interval

𝐻𝑝𝑟𝑜𝑑1: The productivity difference between the two group means

is within the equivalence interval

Conference’17, July 2017, Washington, DC, USA

Winnie Mbaka and Katja Tuma

RQ2:Precision. Are the means of precision between treatment

Table 2: CSV Threat Template

groups equivalent?

The following null and alternative hypotheses were formulated:
𝐻𝑝𝑟𝑒𝑐0: The precision difference between the two group means is

outside the equivalence interval

𝐻𝑝𝑟𝑒𝑐1: The precision difference between the two group means is

within the equivalence interval

3.2 Design of the replication
The following sub-sections will describe how the replication study
was conducted. We followed best practices for replicating experi-
ments with human participants (discussed in Section 2) and tried
to stay as close as possible to the original design. For instance, we
have adapted a similar training and used the resources published
in the original study companion web-site2.

3.2.1 Compared techniques. Threat modeling is a systematic ap-
proach performed by security analysts (and domain experts) to
identify and mitigate security threats early-on in the design phase
of the Software Development Life-cycle (SDL). It includes elicit-
ing the threats, finding and discussing the security attacks, and
prioritizing the identified threats to plan for their mitigations. In
this work, we investigate Microsoft’s threat modeling methodol-
ogy, STRIDE [42]. STRIDE is an acronym representing six different
threat categories which are used to help the analysts in findings
the security threats. The threat categories, spoofing, tampering,
repudiation, information disclosure, denial of service and elevation
of privilege have been described further in the documentation of
STRIDE.

STRIDE is a model-based approach and uses the Data-Flow Di-
agram (DFD) to represent the software system under analysis. A
DFD is a graphical model representing the sources of data, the data
flows through the software system, and where it is consumed (e.g.,
at rest). Thus, the DFD notation consists of data flows, external
entities, processes, data stores, and trust boundaries. The latter are
used to separate DFD elements depending on the level of privilege.
For instance, Figure 1 is a DFD representation of how informa-
tion moves around in a software-based system (in this case, the
home monitoring system). Analysts can perform the analysis with
STRIDE using two variants, per-element and per-interaction [42].
The main difference between the variants is the strategy of visiting
the diagram (see figure 1). For more details see Appendix 7.

Pre-print

ThreadID ElementName
1
2

External Entity X External Entity
Process X

Process

ElementType Category

Spoofing
Repudiation

Description Assumption
......
.....

.....
.....

HomeSys cloud services (depicted as process ‘HomeSys Cloud’ in
Figure 1) via a browser or a mobile application. Using their user
profiles, customers can manage their HomeSys installation and gain
access to historical data on the dashboard. The Gateway component
(depicted as process ‘Gateway’ in Figure 1) is a hardware device
that enables automation of smart homes.

To properly understand the Home monitoring system, a detailed

documentation was made available to the participants.

3.2.3 Participants. All participants are Computer Science Master
students enrolled in a course taught by the experimenters. At the
beginning of the course an entry survey was conducted to help
the experimenters understand participants’ background and areas
of expertise relevant to the study. Majority of the students had a
background in computer science with specific areas of expertise
including artificial intelligence, internet and web technologies, soft-
ware engineering, and computer systems. The remainder (20%) of
the participants had a cyber-security background. Most of them
stated that they were new to secure design techniques (e.g STRIDE,
threat modeling, Data Flow Diagrams, misuse cases, attack trees
etc). Lastly, a large number of the participants rated their familiarity
with specific software design models (e.g sequence, component and
deployment diagrams) as being very limited. However, the course
material also included training lectures on relevant technical skills
like STRIDE as a threat modeling tool and components of a DFD.

3.2.4 Treatment. First, the students were randomly divided into
two groups (A and B) representing the two treatment groups. Group
A was tasked with analysing the HomeSys app using STRIDE Per
Element while Group B was tasked to use STRIDE per Interaction.
Next, participants within each treatment group were randomly
assigned a partner.

3.2.5 Task. The experiment started after the participants watched
all the necessary training videos assigned to each treatment group.
The participants were then asked to complete three tasks:

(1) Create a detailed DFD (Figure 1 is an example of a high level

DFD of the HomeSys application)
(2) Identify all the possible security threats
(3) Map the threats to their element type

In the first task, participants were instructed to refer to the
architectural description of the HomeSys monitoring system and
use UML diagrams to create DFDs (similar to Figure 1 ). When
participants were satisfied with their DFDs, they began analysing
it using the different STRIDE variant depending on the treatment
group. Using STRIDE per-element, participants analysed every
element independently (e.g., starting in the upper-left corner of
Figure 1 ). To facilitate this analysis, mapping element types to
threat categories, STRIDE also provides a mapping chart (see Figure
1). On the other hand, STRIDE per-interaction group would visit
every interaction in their DFD to elicit a possible threat scenario. An
example of an interaction using Figure 1 is when an actuator sends

3.2.2 Description of the Case. The home monitoring system (Home-
Sys) is an automated surveillance system designed for residential
places. Its main objective is to enable the home-owner to remotely
monitor their property. One of the key features of HomeSys is its
ability to send notifications when a critical event occurs (eg. sudden
spike in temperature, or smoke). The architectural documentation
of a HomeSys consists of three component diagrams:

• Context diagram
• Gateway component diagram
• Cloud component diagram

The context diagram consists of three external entities, the mote,
sensor, and actuator (as also depicted on the DFD in Figure 1),
that communicates with the system. The customer, can access the

2https://sites.google.com/site/empiricalstudythreatanalysis/home?authuser=0

4

A replication of a controlled experiment with two STRIDE variants

Conference’17, July 2017, Washington, DC, USA

Figure 1: High Level DFD (Home monitoring System)

Table 3: Criteria for Threat Classification

Table 4: List of all Available Materials

Interaction
Training video:
STRIDE-per-interaction
STRIDE book:
Chapter three (interaction)

Both Groups
Training Video:
HomeSys monitoring system
HomeSys
Documentation

Element
Training video:
STRIDE-per-element
STRIDE book:
Chapter three (element)

is defined below.

Pre-print

𝑃𝑟𝑜𝑑 =

𝑇 𝑃
𝐻𝑜𝑢𝑟

𝑃𝑟𝑒𝑐 =

𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑃

True Positive
Correct element name,
STRIDE category
Description matching threat category Mismatch of description and threat cat-

False Positive
Correct element name, type, but wrong
STRIDE category

type, and

egory

Assumptions matching the description Mismatch of assumption and descrip-

Clearly stated description and assump-
tions

tion
Lack of either description or assump-
tion or both

a command to the gateway. For the third task, the participants
received a threat template (see Table 2) where they categorised
each threat based on its element name, type, threat category and a
description of how the threat might occur. Optionally, the students
could state their assumptions. After completion, the participants
were required to upload a copy of their DFD accompanied by a
filled-in template with the identified security threats.

Time taken to complete the task was captured using an online sur-
vey tool. The start time was entered when the participants started
the survey and ended after the submission of the exit questionnaire.

3.2.6 Measures. To analyse the participants submissions, a ground
truth was developed. The process of creating a ground truth was
divided into two sub-tasks. In the first one, one experimenter anal-
ysed the ground truth from the previous experiment (each unique
DFD had a different ground truth in the previous experiment) to
consolidate a single list of threats that are applicable in the Home-
Sys monitoring system. Secondly, the ground truth was discussed
between the authors to come up with a final draft. The final ground
truth was used as a guide to mark correct and incorrect threats.
Correctly marked threats were categorised as True positives while
incorrect threats were considered false positives. Unlike the previ-
ous experiment, we do not identify False negatives and therefore
recall was not analysed. Table 3 is a visual representation of how
threat classification was done. In the first row, the participants re-
ported threats were compared to the ground truth. The other rows
checked the consistency within the participants reported threats.
Two measures of success were used to determine the outcome

of the experiment, team productivity and precision.
Productivity- in measuring the productivity of each STRIDE variant,
the sum of each teams’ TP were was divided by the time taken (in
hours) to complete tasks (see tasks). The formulae for productivity

5

Precision- to determine the precision, the rates of True Positives
and False Positives were considered. Using both metrics, precision
was calculated using the formulae below.

3.2.7 Execution. Training- Prior to the start of the experiment,the
participants were given access to comprehensive training videos
the day before via a learning management system and instructed
to watch them. The training materials included a description of the
HomeSys monitoring system, a STRIDE training video which was
different depending on the group (group A received STRIDE per
Element). Additional documentation was also provided, Table 4 is
a detailed list of all materials provided to each treatment group.

Physical Labs- The experiment was conducted during a two hour
class where the different teams were allocated different labs depend-
ing on their treatment group. Due to the large number of teams
(22 pairs), the experimenters separated the groups into four labs
with each lab being supervised by either a teaching assistant or the
course lecturers. The participants were allowed to ask the supervi-
sors questions but team interactions were limited to partners.

Reports- All necessary metrics, such as duration taken to com-
plete the task, the group member who submitted the report, and
the uploaded DFDs and CSV templates were captured on Qualtrics.
The last step in the experiment consisted of an exit questionnaire
which consisted of questions aimed at gauging the participants ex-
perience, familiarity, confidence on their performance, and general
knowledge on the subject. The responses were captured on a Likert
scale.

Conference’17, July 2017, Washington, DC, USA

Winnie Mbaka and Katja Tuma

Table 5: Summary Observations in Per-Element Group

Element Group (ID) Time Taken (hours)

0
1
2
3
4
5
6
7
8
9
10
11
Mean
Standard Deviation

1.20
1.65
1.67
1.56
1.70
1.90
1.72
2.40
2.49
3.16
5.48
2.27
2.27
1.13

TP
3
4
2
7
7
7
0
10
5
9
3
7
5.33
2.99

FP
10
6
6
13
1
3
7
5
5
6
10
63
11.25
16.61

Productivity
2.49
2.41
1.19
4.47
4.09
3.66
0.00
4.15
2.00
2.84
0.54
3.08
2.5
1.19

Precision
0.23
0.40
0.25
0.35
0.87
0.70
0.00
0.66
0.50
0.60
0.23
0.10
0.40
0.26

Table 6: Summary Observations in Per-interaction Group

positives regardless of whether or not they were correctly matched
to the element name and element type. The authors concluded that
the members of these two teams lacked an understanding of what
the task required them to do. While the teams performed relatively
better in identifying spoofing, information disclosure, and denial of
service threats, the high frequency of FPs were attributed to the mis-
take mentioned above. However, a significant amount of tampering
threats identified had been matched with the wrong element type,
a common occurrence in threat analysis using non-experts [41].
Generally, repudiation and elevation of privilege attacks were less
identified with an average of two attacks per team. For teams that
listed more than two repudiation or elevation of privilege attacks,
they either lacked a threat description, an assumption or both.

4.2 RQ1: Productivity
The study defined productivity as the number of correctly identi-
fied threats (True Positives) per time taken to complete the task (in
hours). True positives were also used to represent the amount of
work (effort) performed by a team. The average time spent by the
teams performing STRIDE per element is 2.27 h (STDEV=1.13), while
STRIDE per interaction teams spent an average of 1.9 h (STDEV=
0.37) (not statistically significant).The overall productivity (calcu-
lated as TP/Hrs) was dependant on the amount of correctly iden-
tified threats. The higher the TP the higher the average produc-
tivity, and vice versa. As depicted in Table 5, the average produc-
tivity in the per-element group is 2.5 TP/h (STDEV= 1.19), and
per-interaction teams is 3.57 TP/h (STDEV=3.17). The equivalence
test on productivity produced a 0.7159 p-value, supporting the
null hypothesis (𝐻𝑝𝑟𝑜𝑑0). Therefore, we conclude that the groups
productivity is not equivalent.

FP
5
30
5
4
2
4
27
11
5
2
9.50
10.34

Productivity
4.65
10.81
0.42
4.10
4.71
4.17
2.07
0.00
4.30
0.48
3.57
3.17

Precision
0.64
0.25
0.16
0.66
0.81
0.66
0.12
0.00
0.64
0.33
0.43
0.28

Pre-print

4.3 RQ2: Precision
The study defined precision as the correctness of the analysis cal-
culated as TP/TP+FP. Overall, the average precision for each treat-
ment group was 0.43 (STDEV= 0.28) for per-interaction and 0.40
(STDEV=0.26) for per-element. Similar to the original study, there is
no statistically significant difference between the precision of both
teams. However, we conducted an equivalence test (p-value=0.3986),
since the p-value is outside the equivalence interval, supporting
the null hypothesis (𝐻𝑝𝑟𝑒𝑐0), we concluded that the location shift
between the two sample means of precision from both treatment
groups is statistically not equivalent. Figure 3 and 4 display pre-
cision for each threat category (median-yellow line, mean-green
triangle).

Interaction Group (ID) Time Taken (hours)

0
1
2
3
4
5
6
7
8
9
Mean
Standard Deviation

1.93
0.92
2.35
1.94
1.90
1.91
1.92
2.02
2.08
2.07
1.91
0.37

TP
9
10
1
8
9
8
4
0
9
1
5.90
3.95

4 RESULTS
In addition to the analysis performed in [48], if we can not find
significant difference, we also check whether the location shift be-
tween the two sample means in precision is statistically equivalent.
For both measures (productivity and precision) we performed a
T-test equivalence test to check whether the location shift between
the two sample means in the treatment groups is statistically equiv-
alent. The delta was estimated by the authors due to lack of larger
samples from other similar studies that could be used to empirically
derive the delta estimate. For productivity, we consider a delta of
0.5 correct threats per hour as equivalent, while for precision we
consider a delta of 5% as equivalent.

The outcome of the study will be presented in this section by

answering the research questions.

4.1 True Positives and False Positives
Table 5 and 6 show a summary of all necessary observations in both
groups (Identified TP, FP, overall productivity, and precision for
each team). The average number of True positives identified by the
per-Element group was 5.33 (STDEV= 2.99) with a corresponding
average False positive of 11.25 (STDEV=16.61). The per-Interaction
group reported an average of 5.90 (STDEV= 3.95) True Positives
and 9.50 (STDEV=10.34) false positives. Figure 2 is a representa-
tion of the average TP and FP of each threat category across the
treatment groups. To understand the high occurrence of FP in both
treatment groups, we further analysed the submitted threat tem-
plates. We discovered that at least one team in each treatment group
listed a huge number of threat categories without a corresponding
description, assumption, or both despite repeated warning during
the training to avoid making this mistake. According to the criteria
for threat classification presented on Table 3, such mistakes auto-
matically led to the classification of some threat categories as false

4.4 Exit Questionnaire
The final step in the experiment was conducting an exit question-
naire, meant to gauge the students perceived complexity in per-
forming the task. Their responses were recorded using a predefined
likert scale. The training materials made available to the partic-
ipants was instrumental in improving their familiarity with the
home monitoring system (HomeSys), evident from the exit ques-
tionnaire. Both treatment groups found the process of building
a DFD neither challenging nor easy, which also reflected on the
quality of the DFD they uploaded. Analyzing the DFD and threat
mapping using the different variant charts was considered to be

6

A replication of a controlled experiment with two STRIDE variants

Conference’17, July 2017, Washington, DC, USA

Figure 2: Precision Per Threat Category (per-element)

Pre-print

Figure 3: Precision Per Threat Category (per-element)

Figure 4: Precision Per Threat Category (per-interaction)

easy by both teams. However, participants strongly agreed that
the process was time consuming and tedious. There was a slight
difference in how much each treatment group liked their technique.
Majority of the teams in Per-element chose "I do not like it" while
Per-interaction teams stated that they "quite liked it", which can be
used to explain their slightly better productivity.

5 DISCUSSION
This section reports analysis from comparing our results to the
original study.

7

Comparison of True Positives and False Positives. The orig-
inal study [48] reported slightly better averages of TP from Per-
element group (14.3) compared to per-interaction (11.6). However,
the average FPs recorded were relatively lower for per-element
(14.2) while per-interaction had 15 FP. In comparison, our study re-
ported lower averages across both teams (per-element: 5.33 TP and
11.25 FP vs per-interaction: 5.90 TP and 9.50 FP). Overall, the origi-
nal study produced higher averages across both treatment groups.
We conclude this was a result of the original study having relatively
higher participants per team (4-5 students) while the replication
had 2 students per team. However, analysis done on both studies

Conference’17, July 2017, Washington, DC, USA

Winnie Mbaka and Katja Tuma

show that there were no significant differences between the average
number of TP and FP across treatment groups.

Comparison of productivity. As discussed in Section 4, the
average time spent performing the tasks in the replication was
not statistically different, similar to the original study from [48].
Replication (per-element: 2.27 h, per-interaction: 1.9 h), original
(per-element: 3.5 h, per-interaction: 3.95 h). The resulting average
productivity across both studies were also not statistically differ-
ent. Replication productivity (per-element: 2.5 TP/h, per-interaction:
3.57 TP/h), original productivity (per-element: 4.35 TP/h, per-interaction:
3.27 TP/h). It is clear that per-interaction group in the replication
study performed better than in the original. Productivity is driven
by the number of TP, compared to the original study, the produc-
tivity was much lower in the replication. In contrast to the past
study, in the replication, per-interaction was slightly better (not
significant) in terms of productivity. A possible explanation for this
is the tendency of per-interaction teams to liking the technique
more (as described already in 4.4)

the task of the experiment. This was an important decision because
alternatively, students tend to submit more security threats (which
results in many false positives) to maximize their grade. Second,
the students actually used the data they produced as part of an
assignment in the course. Hence the students were motivated to
undertake the study seriously.

Group Composition and Execution. Compared to the previ-
ous study whereby teams were made up of 3-5 participants, our
replication consisted of two members per team with an exception
of one team (3 members). The reduced team composition might
present an unequal distribution of knowledge of task or techniques
used in the experiment. However, we provided access to all training
materials 24 hours prior to the experiment. Participants could access
these materials at anytime, even during the experiment. Addition-
ally, there was at-least one lecturer or teaching assistant present
in the labs to answer any questions that might arise. We remark
that the time given to participants to perform the task in future
replications should be proportional to the system under analysis
(in our replication we allotted about 2h for the analysis, whereas
the original study allotted about 4h).

Comparison of precision. We also compared the precision of
teams from [48] and our treatments groups. Similar to productivity,
the resulting precision measure by our replication was similar to
the precision measured in the past study. Replication precision
(per-element: 0.40, per-interaction: 0.43), original precision (per-
element: 0.62, per-interaction: 0.58). Although the difference is not
significant, per-interaction teams in the replication study were
slightly more precise than per-element (again, the reverse is true
for the original study).

Comparison of threat categories. To complete our analysis,
we compared the average number of identified threats per category
across each treatment group in the original and replication studies.
On average, both treatment groups identified more (TPs and FPs)
spoofing, tampering, information disclosure and denial of service
compared to repudiation and elevation of privilege threats. This
trend was also observed in other studies [48][38]. We concluded
that such threats are easily identifiable by participants regardless of
the treatment group. On the other hand, repudiation and elevation
of privilege threats were less identified (averagely lower TP). A
possible explanation for this might be that students lack a clear
understanding of how vulnerabilities can be exploited to result in
repudiation or elevation of privilege.

Pre-print

7 CONCLUSION
This study focuses on threat modelling techniques by investigating
performance indicators (productivity and precision) of two STRIDE
variants (Element and Interaction). We conducted a replication of
the original study [48] in a controlled experiment. Our participants
are enrolled Master’s students with a background in computer
science and cyber-security. The contribution of this work is as
follows:

(1) A quantitative comparison of performance indicators of
STRIDE by testing the hypothesis of the original study on a
different participation pool. From the results, study found
that per-interaction teams had better productivity and pre-
cision compared to per-element while in the original study
per-element performed better in both measures.

(2) A comparative analysis of SE replicated studies with the aim
of providing suggestions for future replication designs. For
the best practices of replication, we analysed 15 technical pa-
pers obtained from some of the major software-engineering
venues, and documented the trends identified. We also ex-
plained how these trends may be a hindrance to performing
complete (and successful) replications.

6 THREAT TO VALIDITY
This section will discuss the various limitations with the experiment
that might pose a threat to validity to the results.

Participant selection and sample size. The use of student par-
ticipants as opposed to experts might be considered a treat to valid-
ity due to their limited knowledge of implementing industrial-level
techniques. However, [25] states that student participants are well-
suited for empirical experiments. The sample size, 45 students, was
relatively smaller than in the original study (110 students). Since
the experiment was conducted in-person, the authors were satis-
fied with this sample size because it was easy to manage given the
Covid-19 mandates on social distancing in public gatherings.

Participants’ Motivation. Participation in the experiment was
part of the learning objectives of the course, however only partici-
pation was graded (as a pass or fail) and not their performance in

As future work, we plan to conduct similar studies with industry
professionals with (possibly) prior experience in threat analysis.
In addition it would be interesting to observe performance (and
perceived performance) differences between expert to non-expert
populations. Finally, the target population of our study (and related
studies investigating STRIDE) was computer science students (or
professionals in other studies), however, such a sample is not very
diverse e.g., in terms of gender. We are interested to observe whether
diversity factors plays a role in security threat analysis and to what
extent biased decision making (if any) is more pronounced in the
computer science domain when compared to populations with
different educational backgrounds (e.g., with a primary background
in social sciences).

8

A replication of a controlled experiment with two STRIDE variants

Conference’17, July 2017, Washington, DC, USA

ACKNOWLEDGEMENTS
Witheld for anonymous submission

REFERENCES
[1] Tatsuya Abe, Shinpei Hayashi, and Motoshi Saeki. 2013. Modeling security threat
patterns to derive negative scenarios. In Proceedings of the Asia-Pacific Software
Engineering Conference (APSEC), Vol. 1. IEEE, 58–66.

[2] S. Abrahão and E. Insfran. 2017. Evaluating software architecture evaluation
methods: An internal replication. ACM International Conference Proceeding Series
Part F128635 (2017), 144–153. https://doi.org/10.1145/3084226.3084253

[3] Giovanni Abramo and Ciriaco Andrea D’Angelo. 2014. How do you define and

measure research productivity? Scientometrics 101, 2 (2014), 1129–1144.

[4] Talat Ambreen, Naveed Ikram, Muhammad Usman, and Mahmood Niazi. 2018.
Empirical research in requirements engineering: trends and opportunities. Re-
quirements Engineering 23, 1 (2018), 63–95.

[5] Karin Bernsmed and Martin Gilje Jaatun. 2019. Threat modelling and agile
software development: Identified practice in four Norwegian organisations. In
2019 International Conference on Cyber Security and Protection of Digital Services
(Cyber Security). IEEE, 1–8.

[22] Tomaž Kosar, Marjan Mernik, and Jeffrey C Carver. 2012. Program comprehension
of domain-specific and general-purpose languages: comparison using a family of
experiments. Empirical software engineering 17, 3 (2012), 276–304.

[23] Sriram Krishnan. 2017. A hybrid approach to threat modelling. URL https://blogs.
sans. org/appsecstreetfighter/files/2017/03/A-Hybrid-Approach-to-Threat-Modelling.
pdf,[Accessed on 10-Jul-2018] (2017).

[24] Katsiaryna Labunets, Fabio Massacci, and Federica Paci. 2017. On the equivalence
between graphical and tabular representations for security risk assessment. In
International Working Conference on Requirements Engineering: Foundation for
Software Quality. Springer, 191–208.

[25] Katsiaryna Labunets, Fabio Massacci, Federica Paci, et al. 2013. An experimental
comparison of two risk-based security methods. In 2013 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement. IEEE, 163–172.
[26] R Murray Lindsay and Andrew SC Ehrenberg. 1993. The design of replicated

studies. The American Statistician 47, 3 (1993), 217–228.

[27] Engla Ling, Robert Lagerström, and Mathias Ekstedt. 2020. A systematic liter-
ature review of information sources for threat modeling in the power systems
domain. In International Conference on Critical Information Infrastructures Security.
Springer, 47–58.

[28] Mass Soldal Lund, Bjørnar Solhaug, and Ketil Stølen. 2011. Model-driven Risk

Analysis: The Coras Approach. Springer.

2022 from https://www.cyberseek.org/heatmap.html

[6] Borka Jerman Blažič. 2021. Cybersecurity Skills in EU: New Educational Concept
In Cybersecurity Threats with New

for Closing the Missing Workforce Gap.
Perspectives. IntechOpen.

[9] Chad Dougherty, Kirk Sayre, Robert C Seacord, David Svoboda, and Kazuya To-
gashi. 2009. Secure Design Patterns. Technical Report. Carnegie-Mellon University
Pittsburgh, Software Engineering Institute.

[7] Daniela Soares Cruzes, Martin Gilje Jaatun, Karin Bernsmed, and Inger Anne
Tøndel. 2018. Challenges and experiences with applying microsoft threat model-
ing in agile development projects. In 2018 25th Australasian Software Engineering
Conference (ASWEC). IEEE, 111–120.

[8] CyberSeek. 2019. Cybersecurity Supply/Demand Heat Map. Retrieved April 21,

Pre-print

[10] F. Fagerholm, C. Becker, A. Chatzigeorgiou, S. Betz, L. Duboc, B. Penzenstadler,
R. Mohanani, and C.C. Venters. 2019. Temporal Discounting in Software Engi-
neering: A Replication Study. International Symposium on Empirical Software
Engineering and Measurement 2019-Septemer (2019). https://doi.org/10.1109/
ESEM.2019.8870161

[11] L. Falcão, W. Ferreira, A. Borges, V. Nepomuceno, S. Soares, and M.T. Baldassare.
2015. An Analysis of Software Engineering Experiments Using Human Subjects.
International Symposium on Empirical Software Engineering and Measurement
2015-November (2015), 128–131. https://doi.org/10.1109/ESEM.2015.7321185

[12] L. Falcão and S. Soares. 2021. Human-Oriented Software Engineering Experi-
ments: The Large Gap in Experiment Reports. ACM International Conference
Proceeding Series (2021), 330–334. https://doi.org/10.1145/3474624.3474649
[13] A.M. Fernández-Sáez, M. Genero, and M.R.V. Chaudron. 2012. Does the level of
detail of UML models affect the maintainability of source code? Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial Intelligence and
Lecture Notes in Bioinformatics) 7167 LNCS (2012), 134–148. https://doi.org/10.
1007/978-3-642-29645-1_15

[14] J.M. Ferreira, F. Rodriguez, A. Santos, O. Dieste, S.T. Acuna, and N. Juristo. 2022.
Impact of Usability Mechanisms: A Family of Experiments on Efficiency, Effec-
tiveness and User Satisfaction. IEEE Transactions on Software Engineering (2022).
https://doi.org/10.1109/TSE.2022.3149586

[15] Marta N Gómez and Silvia T Acuña. 2014. A replicated quasi-experimental
study on the influence of personality and team climate in software development.
Empirical software engineering 19, 2 (2014), 343–377.

[29] J. Lung, J. Aranda, S. Easterbrook, and G. Wilson. 2008. On the difficulty
of replicating human subjects studies in software engineering. Proceedings
- International Conference on Software Engineering (2008), 191–200.
https:
//doi.org/10.1145/1368088.1368115

[30] J. Miller. 2007. Creating real value in software engineering experiments. Lecture
Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence
and Lecture Notes in Bioinformatics) 4336 LNCS (2007), 38. https://doi.org/10.
1007/978-3-540-71301-2_11

[31] Andreas L Opdahl and Guttorm Sindre. 2009. Experimental comparison of attack
trees and misuse cases for security threat identification. Information and Software
Technology 51, 5 (2009), 916–932.

[32] H.G. Perez-Gonzalez, A.S. Nunez-Varela, F.E. Martinez-Perez, F.E. Hernandez-
Castro, F. Torres-Reyes, R. Juarez-Ramirez, K. Bauer, and C. Guerra-Garcia.
2019. Exploring Software Design Skills of Students in different Stages of
their Curriculum. Proceedings - 2019 7th International Conference in Software
Engineering Research and Innovation, CONISOFT 2019 (2019), 65–71.
https:
//doi.org/10.1109/CONISOFT.2019.00019

[33] M. Riaz, J. King, J. Slankas, L. Williams, F. Massacci, C. Quesada-López, and
M. Jenkins. 2017. Identifying the implied: Findings from three differentiated
replications on the use of security requirements templates. Empirical Software
Engineering 22, 4 (2017), 2127–2178. https://doi.org/10.1007/s10664-016-9481-1
[34] S. Rueda, J.I. Panach, and D. Distante. 2020. Requirements elicitation methods
based on interviews in comparison: A family of experiments. Information and
Software Technology 126 (2020). https://doi.org/10.1016/j.infsof.2020.106361
[35] Vineet Saini, Qiang Duan, and Vamsi Paruchuri. 2008. Threat Modeling Using
Attack Trees. Consortium for Computing Sciences in Colleges 23, 4 (2008), 124–131.
[36] Adrian Santos, Sira Vegas, Markku Oivo, and Natalia Juristo. 2019. A procedure
and guidelines for analyzing groups of software engineering replications. IEEE
Transactions on Software Engineering 47, 9 (2019), 1742–1763.

[37] Joanna C. S. Santos, Katy Tarrit, and Mehdi Mirakhorli. 2017. A Catalog of
Security Architecture Weaknesses. In Proceedings of the International Conference
on Software Architecture Workshops (ICSAW). IEEE Computer Society, 220–223.
https://doi.org/10.1109/ICSAW.2017.25

[38] Riccardo Scandariato, Kim Wuyts, and Wouter Joosen. 2015. A descriptive study
of Microsoft’s threat modeling technique. Requirements Engineering 20, 2 (2015),
163–180.

[16] Synopsys Software Integrity Group. 2021. Building Security In Maturity Model

(BSIMM12). Retrieved April 20, 2022 from https://www.bsimm.com

[17] M.A. Javed and U. Zdun. 2014. The supportive effect of traceability links in
architecture-level software understanding: Two controlled experiments. Proceed-
ings - Working IEEE/IFIP Conference on Software Architecture 2014, WICSA 2014
(2014), 215–224. https://doi.org/10.1109/WICSA.2014.43

[18] Andreas Jedlitschka, Marcus Ciolkowski, and Dietmar Pfahl. 2008. Reporting
experiments in software engineering. In Guide to advanced empirical software
engineering. Springer, 201–228.

[19] J. Jung, K. Hoefig, D. Domis, A. Jedlitschka, and M. Hiller. 2013. Experimental
comparison of two safety analysis methods and its replication.
International
Symposium on Empirical Software Engineering and Measurement (2013), 223–232.
https://doi.org/10.1109/ESEM.2013.59

[20] Peter Karpati, Andreas L Opdahl, and Guttorm Sindre. 2011. Experimental
comparison of misuse case maps with misuse cases and system architecture
diagrams for eliciting security vulnerabilities and mitigations. In Availability,
Reliability and Security (ARES), 2011 Sixth International Conference on. IEEE,
507–514.

[21] Peter Karpati, Guttorm Sindre, and Raimundas Matulevicius. 2012. Comparing
misuse case and mal-activity diagrams for modelling social engineering attacks.
International Journal of Secure Software Engineering (IJSSE) 3, 2 (2012), 54–73.

[39] G. Scanniello and U. Erra. 2014. Distributed modeling of use case diagrams with
a method based on think-pair-square: Results from two controlled experiments.
Journal of Visual Languages and Computing 25, 4 (2014), 494–517. https://doi.
org/10.1016/j.jvlc.2014.03.002

[40] G. Scanniello, M. Staron, H. Burden, and R. Heldal. 2014. On the effect of using
SysML requirement diagrams to comprehend requirements: Results from two
controlled experiments. ACM International Conference Proceeding Series (2014).
https://doi.org/10.1145/2601248.2601259

[41] Adam Shostack. 2008. Experiences Threat Modeling at Microsoft. http://ftp.
informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-413/paper12.pdf
[42] Adam Shostack. 2014. Threat modeling: Designing for security. John Wiley &

Sons.

[43] Forrest Shull, Manoel G Mendoncça, Victor Basili, Jeffrey Carver, José C Mal-
donado, Sandra Fabbri, Guilherme Horta Travassos, and Maria Cristina Ferreira.
2004. Knowledge-sharing issues in experimental software engineering. Empirical
Software Engineering 9, 1 (2004), 111–137.

[44] Martín Solari, Sira Vegas, and Natalia Juristo. 2018. Content and structure of
Information and

laboratory packages for software engineering experiments.
Software Technology 97 (2018), 64–79.

[45] R Stevens, D Votipka, and E.M. Redmiles. 2018. The Battle for New York: A Case
Study of Applied Digital Threat Modeling at the Enterprise Level. In SEC’18:

9

Conference’17, July 2017, Washington, DC, USA

Winnie Mbaka and Katja Tuma

sensitive information from the Sensor. In this case, [42] suggests
investigating repudiation threats. Again, to limit the number of
threats that the analysts need to consider, a threat to interaction
mapping table is used [42]. STRIDE-per-interaction is considered to
be more suitable for expert use and is “too complex to use without
a reference chart handy” [42].

Proceedings of the 27th USENIX Conference on Security Symposium. USENIX Asso-
ciation, 621–637.

[46] Mikael Svahnberg, Aybüke Aurum, and Claes Wohlin. 2008. Using students as
subjects-an empirical evaluation. In Proceedings of the Second ACM-IEEE interna-
tional symposium on Empirical software engineering and measurement. 288–290.
[47] Matt Tatam, Bharanidharan Shanmugam, Sami Azam, and Krishnan Kannoorpatti.
2021. A review of threat modelling approaches for APT-style attacks. Heliyon 7,
1 (2021), e05969.

[48] Katja Tuma and Riccardo Scandariato. 2018. Two architectural threat analysis
techniques compared. In European Conference on Software Architecture. Springer,
347–363.

[49] Dimitri Van Landuyt and Wouter Joosen. 2021. A descriptive study of assumptions
in STRIDE security threat modeling. Software and Systems Modeling (2021), 1–18.
[50] S. Wagner, D.M. Fernández, M. Felderer, A. Vetrò, M. Kalinowski, R. Wieringa,
D. Pfahl, T. Conte, M.-T. Christiansson, D. Greer, C. Lassenius, T. Männistö, M.
Nayebi, M. Oivo, B. Penzenstadler, R. Prikladnicki, G. Ruhe, A. Schekelmann,
S. Sen, R. Spínola, A. Tuzcu, J.L. De La Vara, and D. Winkler. 2019. Status quo
in requirements engineering: A theory and a global family of surveys. ACM
Transactions on Software Engineering and Methodology 28, 2 (2019). https://doi.
org/10.1145/3306607

[51] Y. Wang, D.R. Degutis, and S. Wagner. 2018. Speed up BDD for safety ver-
ification in agile development: A partially replicated controlled experiment.
ACM International Conference Proceeding Series Part F147763 (2018).
https:
//doi.org/10.1145/3234152.3234181

[52] Andreas Wolf, Dimitrios Simopoulos, Luca D’Avino, and Patrick Schwaiger. 2021.
The PASTA threat model implementation in the IoT development life cycle.
INFORMATIK 2020 (2021).

APPENDIX A: STRIDE-PER-ELEMENT VS
PER-INTERACTION
In addition to DFD creation, analysts often make assumptions about
the system under analysis. Assumptions are statements (which may
or may not be true) about the domain, functionality and security of
its’ components. They can be identified when the diagram is built
and they can arise during the analysis. Landuyt [49] defines threat
assumptions as information used to hypothesise certain system
properties that are relevant to the attack identified. Threat assump-
tions are important in threat analysis as they can be used to justify
threat existence and prioritise mitigation efforts.

After the threats are elicited, analysts discuss attack scenarios
and document them with threat descriptions. A threat description
contains concrete steps of the identified security threats and how
they may impact system components associated to it. Finally, the
threat can be prioritized according to estimation of risk (this step
is not investigated in our work).

Pre-print

STRIDE-per-Element. This technique is performed by analysing
each component in a DFD individually. To limit effects of threat ex-
plosion, the documentation of STRIDE provides a threat to element
mapping table. For each element type a different subset of threat
categories are suggested. For instance, for external entities (such as
“Sensor” in Figure 1), [42] suggests investigating spoofing and re-
pudiation threats. STRIDE-per-element is described as a simplified
approach to identifying threats that can be easily understood by
the beginner [42].

STRIDE-per-Interaction. However, threats can not be always
discussed in a vacuum as they are a result of some interaction in
the system. An interaction is a tuple of origin, destination, and
the interaction itself (e.g., the “Customer” sending “credentials”
to the “HomeSys Cloud” in Figure 1). In contrast to STRIDE-per-
element, this technique is performed by systematically visiting each
interaction in a DFD. For instance, when an external entity, such as
“Sensor”, is passing information to a process, such as “Gateway” with
no logging in place, the Gateway is able to deny having received

10

