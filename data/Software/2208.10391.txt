MOM: Matrix Operations in MLIR
Towards Compiler Support for Linear Algebra Computations in MLIR

Lorenzo Chelini
Huawei Technologies
Switzerland
l.chelini@icloud.com

Marcin Copik
ETH Zurich
Switzerland
marcin.copik@inf.ethz.ch

Henrik Barthels
RelationalAI, Inc.
Germany
henrik.barthels@relational.ai

Tobias Grosser
University of Edinburgh
UK
tobias.grosser@ed.ac.uk

Paolo Bientinesi
Umea University
Sweden
paolo.bientinesi@umu.se

Daniele G. Spampinato
Huawei Technologies
Switzerland

daniele.giuseppe.spampinato@huawei.com

Abstract
Modern research in code generators for dense linear algebra
computations has shown the ability to produce optimized
code with a performance which compares and often exceeds
the one of state-of-the-art implementations by domain ex-
perts. However, the underlying infrastructure is often de-
veloped in isolation making the interconnection of logically
combinable systems complicated if not impossible. In this
paper, we propose to leverage MLIR as a unifying compiler
infrastructure for the optimization of dense linear algebra
operations. We propose a new MLIR dialect for expressing
linear algebraic computations including matrix properties
to enable high-level algorithmic transformations. The in-
tegration of this new dialect in MLIR enables end-to-end
compilation of matrix computations via conversion to exist-
ing lower-level dialects already provided by the framework.

1 Introduction
A signiﬁcant part of processor time is spent on mathemati-
cal algorithms used in simulations, machine learning, com-
munication, signal processing, computer vision, and other
domains. Of course, the mathematics used in these domains
may diﬀer widely. Still, the actual computations often fall
into the realm of linear algebra, meaning sequences of com-
putations on matrices and vectors. Research in the area of lin-
ear algebraic domain-speciﬁc languages (DSLs) has demon-
strated that expert-level optimizations can be carried out
automatically when taking the mathematical semantics of
the computation into account (e.g., [2, 7, 9]). Over time, how-
ever, the code generation infrastructure supporting such
research has grown into a fragmented software ecosystem,
where diﬀerent components that could logically cooperate
towards a shared performance goal are in practice unable to
interoperate with one another. Thanks to the new MLIR [8]
compiler infrastructure is now possible to de-fragmentize
our software ecosystem by enabling domain-speciﬁc com-
pilation directly in a general-purpose compiler. However,
after careful evaluation of the available dialects in MLIR,

✕✖✗

(cid:0)✁✂✄✁ ☎✆✂✁✝✞ ✟☎✠✁✡✞✝

✓✔

☛✆✂✂✁✝

✌✝✚✞✆✛✜✢✣✝✆✂

✤✥

✦

✧

✤

✚✆

✆

✝✚✆

✂✄

☛✆✂✝☎✠

☛☛☞✌ ✍✎

☛✆✂✝☎✠ ✝✂★

✠✁✂✁✞✆✢

✤✥

✦

✧

✤

✚✆

✆

✝✚✆

✂✄

✘✙

✏✑✒

Figure 1. The MOM compiler for dense linear algebra.

none was able to capture linear-algebra semantics, including
Linalg [10]; therefore, we developed our dialect.

This paper proposes an implementation of a subset of the
state-of-the-art linear algebra code generator Linnea [2] in
MLIR. Speciﬁcally, our contributions are:

• An IR representation to model linear algebra opera-

tions, types and properties.

• A lowering from such representation to LLVM IR.

2 The Linnea Dialect
We introduce a new linear algebraic dialect in MLIR called
Linnea after the homonymous code generator [2]. Linnea
provides MLIR attributes, types, operations, and transfor-
mations required to make dense computations as ﬁrst-class
citizens in the compiler IR. In Fig. 1 we show an overview of
the Matrix Operations in MLIR (MOM 1) compiler framework.
Linnea bridges dense linear algebra computations expressed
in our Python DSL and the Linalg dialect, as shown in Fig. 1.
From Linalg, we lower to LLVM IR and then machine code.
Linnea comes with custom attributes, types and operations
described next.

1https://github.com/Algebraic-Programming/MLIR-Linnea

Lorenzo Chelini, Henrik Barthels, Paolo Bientinesi, Marcin Copik, Tobias Grosser, and Daniele G. Spampinato

Matrix Attribute. Inspired by the Sparse Tensor Dialect [3],

we introduce Matrix attributes to encode non-conﬂicting,
compile-time properties of a matrix. For example, the prop-
erty that a matrix is lower triangular is expressed using the
following attribute:

#linnea.property<['lowerTri']>

During buﬀerization 2, the underlying algebraic object is ma-
terialized using a straightforward memref where attributes
dictate how the memory is ﬁlled (i.e., set to zeros the ele-
ments below the diagonal for an upper triangular matrix).
In the future, we aim to depart from memref and use more
appropriate containers.

Types: Matrix, Term and Identity. Linnea provides three
built-in types: Matrix, Term, and Identity. Matrix represents
a 2-dimensional tensor type which accepts an attribute that
describes a set of non-conﬂicting properties, a static dimen-
sion, and an MLIR built-in type that describes each stored
element’s type (e.g. f32). For example, the following is the
type declaration of a 5 × 5 lower-triangular matrix:

1
2
3
4
5
6
7

!linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>

A term type represents the result of Linnea equationOp,
which in turn models a mathematical equation, composable
by the available operations (see next paragraph). A term is is
a placeholder type, meaning that it will be replaced by a con-
crete one (i.e., MatrixType). In fact, only after optimization
and simpliﬁcation the result of an equationOp is known and
thus the placeholder types can be swapped with a concrete
one. Finally, an Identity type represents the identity matrix –
a square matrix where all the elements on the diagonal are
1. The Identity type enables the expression of mathematical
identities such as 𝐴 · 𝐼 = 𝐴 and 𝐼 · 𝐼 = 𝐼 .

Operations. Table 1 shows the operations exposed by the
Linnea dialect. Init and Fill initialize and ﬁll an algebraic
object, respectively. Mul and Add are variadic operations that
take as input an arbitrary numbers of MLIR Values of Lin-
nea types [6]. The Equation represents a “bag” of Linnea
operations that together represent a given mathematical ex-
pression. We build a symbolic representation of the program
by walking each equation starting from the yield operation.
An equation is simpliﬁed and rematerialized to new low-
level Linnea IR (not shown). For example, variadic multi-
plications are rewritten as binary multiplications with an
optimal parenthesization.

Transformations. Currently, we limit ourselves to three
transformations: matrix-chain reordering, identity simpliﬁ-
cation and properties propagation. Matrix chain reordering
is an algorithmic improvement that minimizes the number of
scalar multiplications when multiplying a chain of matrices.
We implement the algorithm described in [5] and generalize

2buﬀerization materializes memref (i.e., malloc) from tensors.

Op name

Description

Init
Fill
Equation

Add
Yield

Materialize the algebraic object using a memref
Initialize the algebraic object according to its properties
Represents a linear algebra equation

Variadic addition of diﬀerent algebraic objects
Return the result of an Equation

Mul
Transpose

Variadic multiplication of diﬀerent algebraic objects
Transpose the algebraic object

Table 1. Operations exposed by the Linnea dialect.

n = 5
m = 5
Matrix A(n, m) <LowerTriangular>
Matrix B(n, m) <LowerTrinagular>
Matrix C(n, m) <>
C = A * B
print(C)

Listing 1. MOM speciﬁcation for a triangular matrix multi-
plication.

it to account for matrix properties as in [1, 2]. Identity simpli-
ﬁcation consists of exploiting the identity matrix to simplify
the computation. For example, 𝐴 · 𝐼 → 𝐴. Finally, Linnea’s IR
makes it possible to annotate matrices with properties. But,
it is also essential to understand the properties of intermedi-
ate results as the computation unfold, thus we introduce a
symbolic engine and encode a set of inference rules such as
𝑙𝑜𝑤𝑒𝑟𝑇 𝑟𝑖𝑎𝑛𝑔𝑢𝑙𝑎𝑟 (𝐴) → 𝑢𝑝𝑝𝑒𝑟𝑇 𝑟𝑖𝑎𝑛𝑔𝑢𝑙𝑎𝑟 (𝐴𝑇 ). We use the
symbolic engine to replace a TermType with a concrete type.

3 MOM Compiler Usage
MOM provides users with a convenient Python DSL lan-
guage to express their computation. It does not require any
knowledge about the internal intermediate representation or
the diﬀerent compiler passes involved in lowering a Python
speciﬁcation to binary. For example, Listing 1 shows how a
user can specify a multiplication between two lower trian-
gular matrices.

Behind the scene, MOM lowers the Python speciﬁcation
to the Linnea dialect (see Listing 2). Line 3 and 4 map to
the initialization for the A and B matrix. Line 6 maps to the
linnea.equation operation.

Lowering to Linalg. The conversion to Linalg is rela-
tively straightforward; each operation in the Linnea dialect
maps one-to-one with available operations in Linalg. For
example, Listing 3 shows the Linalg operations materialized
after the conversion of Listing 2. Matrix types are converted
to two-dimensional tensors with the same element type and
sizes. The attribute that speciﬁes the type of the matrix is
preserved.

References
[1] Henrik Barthels, Marcin Copik, and Paolo Bientinesi. 2018. The Gen-
eralized Matrix Chain Algorithm. In Proceedings of the 2018 Interna-
tional Symposium on Code Generation and Optimization (CGO 2018).
Association for Computing Machinery, New York, NY, USA, 138–148.
https://doi.org/10.1145/3168804

[2] Henrik Barthels, Christos Psarras, and Paolo Bientinesi. 2021. Linnea:
Automatic Generation of Eﬃcient Linear Algebra Programs. ACM
Transactions on Mathematical Software (TOMS) 47, 3 (June 2021), 1–26.
https://arxiv.org/pdf/1912.12924.pdf

[3] Aart JC Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasi-
lache, Bixia Zheng, and Fredrik Kjolstad. 2022. Compiler Support for
Sparse Tensor Computations in MLIR. arXiv preprint arXiv:2202.04305
(2022).

[4] Lorenzo Chelini, Andi Drebes, Alex Zinenko, Albert Cohen, Nicolas
Vasilache, Tobias Grosser, and Henk Corporaal. 2021. Progressive
Raising in Multi-level IR. In International Conference on Code Genera-
tion and Optimization (CGO). February 27th - March 3rd, 2021, Virtual
Conference.

[5] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord

Stein. 2009. Introduction to algorithms. MIT press.

[6] MLIR Developers. [n. d.]. MLIR Value. ([n. d.]). https://mlir.llvm.org/

doxygen/classmlir_1_1Value.html

[7] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and
Saman Amarasinghe. 2017. The Tensor Algebra Compiler. Proc. ACM
Program. Lang. 1, OOPSLA, Article 77 (Oct. 2017), 29 pages. https:
//doi.org/10.1145/3133901

[8] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy
Davis, Jacques Arnaud Pienaar, River Riddle, Tatiana Shpeisman, Nico-
las Vasilache, and Oleksandr Zinenko. 2021. MLIR: Scaling Compiler
Infrastructure for Domain Speciﬁc Computation. In CGO 2021.
[9] Daniele G. Spampinato, Diego Fabregat-Traver, Paolo Bientinesi, and
Markus Püschel. 2018. Program Generation for Small-Scale Linear
Algebra Applications. In International Symposium on Code Generation
and Optimization (CGO). 327–339.

[10] Nicolas Vasilache, Oleksandr Zinenko, Aart JC Bik, Mahesh Ravis-
hankar, Thomas Raoux, Alexander Belyaev, Matthias Springer, To-
bias Gysi, Diego Caballero, Stephan Herhut, et al. 2022. Compos-
able and Modular Code Generation in MLIR: A Structured and Retar-
getable Approach to Tensor Compiler Construction. arXiv preprint
arXiv:2202.03293 (2022).

MOM: Matrix Operations in MLIR

// Initialize matrix A. Default initialize to 1 (%fc = 1).
%A = linnea.init [5, 5] :

!linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>

%Af = linnea.fill(%fc, %A) :

f32, !linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>

// Initialize matrix B.
%B = linnea.init [5, 5] :

!linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>

%Bf = linnea.fill(%fc, %B) :

f32, !linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>

// Multiply the two and print the result.
%0 = linnea.equation {

%1 = linnea.mul %Af, %Bf :

!linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>
!linnea.matrix#linnea.property<['lowerTri', [5, 5], f32]>
-> !linnea.term

linnea.yield %1 : !linnea.term

}
linnea.print %0 : !linnea.term

Listing 2. A Linnea IR representation for a multiplication
between two lower triangular matrices.

// Initialize matrix A. Default initialize to 1 (%fc = 1).
%A = linalg.init-tensor [5, 5]

: tensor<5x5xf32, #linnea.property<['lowerTri']>>

%Af = linalg.fill ins(%fc : f32)

outs(%A : tensor<5x5xf32, #linnea.property<['lowerTri']>>)

// Initialize matrix B.
%B = linalg.init-tensor [5, 5]

: tensor<5x5xf32, #linnea.property<['lowerTri']>>

%Bf = linalg.fill ins(%fc : f32)

outs(%B : tensor<5x5xf32, #linnea.property<['lowerTri']>>)

%C = linalg.init-tensor [5, 5]

: tensor<5x5xf32, #linnea.property<['lowerTri']>>

linalg.matmul ins(%Af : tensor<5x5xf32, #linnea.property<['lowerTri']>>
%Bf : tensor<5x5xf32, #linnea.property<['lowerTri']>>)

outs(%C : tensor<5x5xf32, #linnea.property<['lowerTri']>>)

linnea.print %C : tensor<5x5xf32, #linnea.property<['lowerTri']>>

Listing 3. A Lianlg IR representation for Listing 2.

4 Results
The experiments have been conducted on an Intel Core i7-
10750H. All results were obtained considering the minimal
execution time of ﬁve independent runs for single-precision
(f32) operands as in [4]. For a chain of 4 matrices 800×1100×
900 × 1200 × 100 with initial parenthesization (((𝐴1 × 𝐴2) ×
𝐴3) × 𝐴4) we obtain a 1.24𝑋 speedup by re-parenthesizing
as (𝐴1 × (𝐴2 × (𝐴3 × 𝐴4))).

5 Conclusion
We presented MOM, an end-to-end compiler for dense linear
algebra operations based on MLIR. Due to the missing alge-
braic semantics in Linalg, we proposed a new linear algebraic
dialect for capturing essential matrix properties and enabling
the expression of domain-expert optimization strategies. For
example, it is eﬀortless to detect matrix-chain multiplications
thanks to Linnea and the introduction of variadic multiplica-
tions and matrix types. Future work includes the integration
of more properties and algebraic transformations, as well
as the investigation of suitable extensions of the existing in-
frastructure to support them (e.g., the multiplication of two
triangular matrices could map to a non-rectangular iteration
space currently not supported by Linalg).

