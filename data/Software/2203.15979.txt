A First Look at Duplicate and Near-duplicate Self-admitted
Technical Debt Comments
Jerin Yasmin
Mohammad Sadegh Sheikhaei
Yuan Tian
{jerin.yasmin,sadegh.sheikhaei,y.tian}@queensu.ca
Queen’s University
Canada

2
2
0
2

r
a

M
0
3

]
E
S
.
s
c
[

1
v
9
7
9
5
1
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT
Self-admitted technical debt (SATD) refers to technical debt that is
intentionally introduced by developers and explicitly documented
in code comments or other software artifacts (e.g., issue reports) to
annotate sub-optimal decisions made by developers in the software
development process.

In this work, we take the first look at the existence and char-
acteristics of duplicate and near-duplicate SATD comments in
five popular Apache OSS projects, i.e., JSPWiki, Helix, Jackrab-
bit, Archiva, and SystemML. We design a method to automatically
identify groups of duplicate and near-duplicate SATD comments
and track their evolution in the software system by mining the com-
mit history of a software project. Leveraging the proposed method,
we identified 3,520 duplicate and near-duplicate SATD comments
from the target projects, which belong to 1,141 groups. We man-
ually analyze the content and context of a sample of 1,505 SATD
comments (by sampling 100 groups for each project) and identify if
they annotate the same root cause. We also investigate whether du-
plicate SATD comments exist in code clones, whether they co-exist
in the same file, and whether they are introduced and removed
simultaneously. Our preliminary study reveals several surprising
findings that would shed light on future studies aiming to improve
the management of duplicate SATD comments. For instance, only
48.5% duplicate SATD comment groups with the same root cause
exist in regular code clones, and only 33.9% of the duplicate SATD
comment pairs are introduced in the same commit.

KEYWORDS
Self-admitted technical debt (SATD), Apache, OSS, duplicate com-
ments, documentation.

ACM Reference Format:
Jerin Yasmin, Mohammad Sadegh Sheikhaei, and Yuan Tian. 2022. A First
Look at Duplicate and Near-duplicate Self-admitted Technical Debt Com-
ments. In 30th International Conference on Program Comprehension (ICPC
’22), May 16–17, 2022, Virtual Event, USA. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/3524610.3528387

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICPC ’22, May 16–17, 2022, Virtual Event, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9298-3/22/05. . . $15.00
https://doi.org/10.1145/3524610.3528387

1 INTRODUCTION
Technical debt (TD) is a concept used to describe the trade-off
between the “quick and dirty” design and implementation choices
in modern software development and the long-term quality and
maintainability of a software system [7]. Being aware of TD is
crucial because it would introduce extra cost as TD accumulates
over time due to sub-optimal implementation. Moreover, it would
also increase the risk of performance issues and functionality bugs.
In recent years, researchers have intensively studied self-admitted
technical debt (SATD), a concept proposed by Potdar and Shi-
hab [14], referring to the sub-optimal design and implementation
decisions that developers explicitly acknowledge. SATD instances
mainly appear as code comments 1, which we refer to as SATD
comments. Like general code comments, duplicate SATD comments
may exist in a software system due to copy-pasting code snippets or
annotating the same technical debt. Figure 1 shows an example pair
of duplicate SATD comments in which the developer reports that
the hard-coded assignment of a specific variable must be modified
in the future to prevent bugs. However, most existing studies on
SATD focus on identifying and tracking SATD instances. None of
them have discussed the appearance of duplicate SATD comments.

Figure 1: Example of duplicate SATD comments

This paper conducts the first empirical study on the duplicate and
near-duplicate SATD comments in open-source software projects.
We define a group of duplicate and near-duplicate SATD comments
as a set of SATD comments with identical textual content or highly
similar and semantically identical textual content. Each comment
in the group is a duplicate or near-duplicate SATD comment, which
we refer to as a duplicate SATD comment in the rest of this paper.
We aim to explore why duplicate SATD comments appear, where
they appear and how they evolve in software systems. Answers
to the above questions would help developers and researchers un-
derstand whether duplicate SATD comments would bring risks
to the maintenance of projects and pose challenges in managing

1SATD instances may also appear in issue report [21].

 
 
 
 
 
 
ICPC ’22, May 16–17, 2022, Virtual Event, USA

Jerin Yasmin, Mohammad Sadegh Sheikhaei, and Yuan Tian

SATD instances. For instance, duplicate SATD comments might
annotate the same bug and should be fixed together. If only one
SATD instance is fixed, the remaining one might introduce a bug
in the future. To achieve our goal, we develop a method to identify
SATD comments from SmartSHARK [18], a dataset contains rich
evolution information of 77 Apache projects. Among all the 77
projects, we picked five projects containing the most number of
SATD comments in their commit history, i.e., JSPWiki 2, Helix 3,
Jackrabbit 4, Archiva 5, and SystemML 6. Next, we applied a state-
of-the-art natural language processing (NLP) model to compute
semantic similarity scores between pairs of identified SATD com-
ments from each target project and use them to identify groups of
duplicate SATD comments. We use the collected data to answer the
following three research questions:
RQ1: Are duplicate SATD comments introduced by the same
root cause? Do they exist in code clones? Intuitively, duplicate
SATD comments would be introduced for the same root cause,
e.g., indicating areas in the code that are affected by the same
(specific) poor implementation choice or the same bug. Also, many
of duplicate SATD comments might appear when developers copy
and paste code that contains a SATD comment. In RQ1, we test
the above two hypotheses by manually examining a sample set of
duplicate SATD comment groups.
RQ2: Are duplicate SATD comments exist in the same or dif-
ferent files? For duplicate SATD comments in groups with the
same root cause (identified in RQ1), we further check if they ap-
pear in the same file or different files. Intuitively, when developers
resolve one SATD instance, they would be more likely to ignore
the opportunities to change other code containing duplicate SATD
comments if they appear in different files. Moreover, bugs could be
introduced due to inconsistent changes on SATD instances anno-
tated by duplicate SATD comments.
RQ3: Are duplicate SATD comments introduced and removed
at the same time? In RQ3, we investigate the evolution of dupli-
cate SATD comment pairs with the same root cause (identified
in RQ1). Answering this question would shed light on develop-
ers’ current practice in introducing and evolving duplicate SATD
comments.
Contribution. We introduce a new concept, i.e., duplicate SATD
comment. We gain the first preliminary understanding of duplicate
SATD comments by analyzing thousands of SATD comments auto-
matically mined from commit history of five Apache OSS projects.
Our work would shed light on future analysis on the practice and
quality of duplicate/general SATD documentation.
Data availability. Our annotated duplicate and near-duplicate SATD
comments are available at https://doi.org/10.6084/m9.figshare.19426766.
v1.

2 METHODOLOGY
2.1 Data Collection and Target Projects
We extract the SATDs from SmartSHARK (version 2.1) [18], a
dataset that contains detailed information about the evolution of 77

2https://jspwiki.apache.org/
3https://helix.apache.org/
4https://jackrabbit.apache.org/jcr/index.html
5https://archiva.apache.org/
6https://systemds.apache.org/docs/1.2.0/

Apache projects that have Java as the main programming language.
Given each project X in the dataset, we perform the following four
steps to extract SATD comments by mining the whole commits in
the master branch of project X:

• Step 1: Extract the sequence of commits that are in the

master branch of the project X.

• Step 2: Extract the ancestor of each file in project X by
tracking the file renames in file_action collection, and make
the sequence of files for each final file (i.e. the files that have
not been renamed anymore).

• Step 3: According to commits’ order (step 1) and files’ order
(step 2), extract the ordered hunks from the hunk collection
for each final file.

• Step 4: Scan the hunks of each final file in X to extract
SATD. Following Trautsch et al. [18], we use task tags, i.e.,
“TODO/FIXME/XXX” to identify SATD from Java code com-
ments. For each found SATD, we store its file_id, file path, cre-
ated_in_commit, created_in_line. Then, we track the changes
in later hunks to find the final state of each found SATD. If
a SATD comment gets deleted in later hunks, we also store
the corresponding commit_id.

After extracting SATD comments from the 77 projects in Smark-
SHARK, we select the five projects that have the largest number
of SATD comments (shown in Table 1). Our target projects con-
tain 7,861 SATD comments mined from the commit history of all
five projects. Note that there might be SATD comments ignored
by our approach, as they might use less explicit text to admit TD.
However, as the first preliminary study on duplicate SATD com-
ments, our goal is to understand their existence and characteristics,
rather than reporting the precise ratio of duplicate SATD com-
ments among all SATD comments. Moreover, Guo et al. found that
detecting SATD comments by searching the appearance of prede-
fined task tags ( “TODO,” “FIXME,” and “XXX”) in code comments
can achieve competitive performance when compared to complex
machine learning-based approaches [5].

2.2 Data Cleaning and Filtering
Since task tags such as “TODO” do not carry meaningful infor-
mation, we further clean extracted SATD comments by removing
task tags. Then, we filter out the SATD comments that have the
following properties:

• Auto-generated SATD comments: They are not written by
the developers (e.g., “auto-generated method stub”, “auto-
generated catch block”).

• Single-word or numbers-only SATD comments: They are

not informative to analyze the SATD they annotate.

2.3 Identifying Duplicate SATD comments
First, we leverage a pre-trained BERT (Bidirectional Encoder Rep-
resentations from Transformers) model to generate a sentence em-
bedding for each SATD comment. Specifically, we use the functions
provided by SentenceTransformer7. Next, we calculate the pairwise
cosine similarity between each pair of SATD comment’s embed-
ding. We rank all pairs by their similarity scores and remove SATD
comments that do not exist in any pair with a similarity score ≥ 0.8.

7https://www.sbert.net/

A First Look at Duplicate and Near-duplicate Self-admitted Technical Debt Comments

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Table 1: Basic dataset statistics. Filter 1 removes auto-generated SATD. Filter 2 removes single-word/number-only SATD.

Project

Domain

# SATD

Content, Wiki Engine
Build Management

JSPWiki
Archiva
Jackrabbit Database
SystemML Machine Learning
Helix

Big-data, Cloud

1,325
1,508
1,721
1,559
1,748

# Removed by
Filter 1
243
327
407
281
1,056

# Removed by
Filter 2
15
61
119
51
15

# SATD after
filtering
1,067
1,120
1,195
1,227
677

# Dup. SATD

865
757
725
737
436

# Groups of
Dup. SATD
274
241
237
232
157

The rational behind this threshold is that for the most pairs with a
similarity score lower than 0.8, they are not duplicate SATD com-
ments. For the remaining SATD comments, we generate a matrix
𝑋 where each row and each column represents a selected SATD
comment, and the value of a cell 𝑋𝑖 𝑗 refers to the similarity score
between SATD comment 𝑖 and 𝑗. We then fit the matrix 𝑋 as in-
put to DBSCAN clustering algorithm [2] by calling the function
fi_predict provided by package scikit-learn 8. We set the parameter
“eps" as 0.4 and “min_samples" as 2.

In the end, we get a total of 1,141 groups containing 3,520 du-
plicate SATD comments (ref. Table 1). The ratio of the duplicate
SATD comments that are found from the total number of
SATDs are 65%, 50.2%, 50.3%, 47.3%, 41.3% for project JSPWiki,
Archiva, Jackrabbit, SystemML, and Helix respectively.

3 PRELIMINARY RESULTS
3.1 RQ1: Are duplicate SATD comments

introduced by the same root cause? Do they
exist in clones?

Approach: To answer RQ1, we create a set that contains 500 dupli-
cate SATD groups identified in Section 2. Specifically, we randomly
sampled 100 groups from all identified groups for each target project.
We choose to set the same number per project because we want
to compare statistics across projects, and we do not want to bias
a specific project. In total, these 500 groups involve 1,505 SATD
comments. For each sampled group, for each duplicate SATD com-
ment in the group, we carefully read the commit that introduced
the SATD comment and identified the root cause of the TD this
comment annotates. We label a group as a duplicate SATD comment
group with identical root cause (i.e., identical group) if all SATD
comments in the group are introduced for the same reason and
developers can potentially resolve them together.

To investigate the relationship between code clones and duplicate
SATD comments, we first need to identify clones. We download the
file associated with each target SATD comment and run NiCad [15]
to identify block-level clones. Note that one SATD comment may
exist in different versions of the same file, we consider the first
version when the SATD was committed to code base. NiCad is a
famous clone detection tool, as it has high precision and recall in
identifying the code clones that are very similar but not an exact
match, i.e., near-miss clones [16]. We set the similarity threshold
of NiCad as 70% (i.e., the default value) and the minimum length
of a block at five lines, the best-reported value for clone detection
using NiCad in Java source code [19]. However, duplicate SATD
comments might also exist in micro-clones, which refers to the code
clones having a size (LOC-line of code) smaller than the minimum

8https://scikit-learn.org/stable/

size of regular code clones [12]. The size of a micro-clone ranges
from 1 to 4 LOC. As micro-clones are difficult to detect due to the
small size [13], we manually read the code surrounding (five lines
of code above and below the line containing the SATD) of the SATD
comment and determine if all SATDs in each group exist in the one
micro-clones.

Two authors participate in the manual study. They first inde-
pendently annotated 50 duplicate SATD comment groups. This
process reported 4 groups with different and uncertain labels. The
two authors discussed and resolved the conflict and one author
then labeled the remaining 450 groups.

Results: 484 out of 500 sampled duplicate SATD comment
groups are groups with the identical root cause. Among the
500 manually examined groups, we only find 16 groups containing
duplicate SATD pairs that do not refer to the same problem, i.e., irrel-
evant. One potential reason is that most of our identified duplicate
SATD comments are very specific in describing the corresponding
TD. We manually checked the 16 non-identical groups and observed
that most SATD comments in these groups are less informative, e.g.,
“implement this", “add more tests". That means SATD comments
with general textual description would likely generate duplicate
SATD comments referring to different specific problems and poten-
tially introduce challenges to track and maintain SATD instances
that should be or better be resolved together.

48.5% and 42% of the duplicate SATD comment groups with
the identical root cause belong to regular code clones and
micro-clones. The ratio in project JSPWiki, Archiva, Jackrabbit,
SystemML, and Helix is 35%, 53.6%, 49%, 60%, and 45.3% respec-
tively. Our manual analysis finds that another 42% (204 out of 484
groups) groups are within micro-clones. The ratio is 58%, 35%, 27.7%,
38%, 51% for project JSPWiki, Archiva, Jackrabbit, SystemML, and
Helix respectively. The results indicate the need in building micro-
clone detection tool that can automatically detect and track the
micro-clones containing duplicate SATD comments.

3.2 RQ2: Are duplicate SATD comments exist

in the same or different files?

Approach: For each identical group of duplicate SATD comments
𝑐𝑘 that contains 𝑛 duplicate SATD comments, we first generate all
unique duplicate SATD comment pairs < 𝑠𝑖, 𝑠 𝑗 >, where 𝑠𝑖 and 𝑠 𝑗
are SATD comments in group 𝑐𝑘 . A group with 𝑛 SATD comments
would have 𝑛(𝑛 − 1)/2 duplicate pairs. Next, for each < 𝑠𝑖, 𝑠 𝑗 >
pair, we identify if the two SATD comments exist in the same file
or different files. If the group 𝑐𝑘 has 𝑚 duplicate pairs exist in the
same file, we then calculate a ratio for group 𝑐𝑘 ,
𝑛 (𝑛−1)/2 , denotes
the prevalence of duplicate SATD comment pairs that exist in the
same file in the group.

𝑚

ICPC ’22, May 16–17, 2022, Virtual Event, USA

Jerin Yasmin, Mohammad Sadegh Sheikhaei, and Yuan Tian

Table 2: Distribution of duplicate SATD comment pairs based on their introduction and removal status. Js/A/Ja/S/H refers to
project JSPWiki/Archiva/Jackrabbit/SystemML/Helix.

Introduction
Status

Introduced in
the same commit

Introduced in
different commits

Js
61

Js
267

A
170

A
272

#pairs

848
Ja
251
1,657
Ja
362

S
271

S
354

H
85

H
402

Removed in
the same commits
or both remain
651
Ja
201
276
Ja
50

S
229

S
39

136

Js A
17

Js A
70
41

Removed Status

Removed in
different commits

One is removed
and the other remains

H Js
4
68

A
28

H Js
76

100

A
138

122
Ja
40
930
Ja
261

S
46

H
4

Js
40

S
274

H
157

Js
126

75
Ja
10
451
Ja
51

A
6

A
64

S
6

S
41

H
13

H
169

Results: On average, 12.9% to 72.1% of the pairs in identical
groups appear in the same file. The 484 identical groups contain
a total of 1,437 duplicate SATD comments, forming 2,505 duplicate
SATD comment pairs. We observe that the ratio of duplicate SATD
comment pairs in the same file varies among groups and across
projects. For instance, the number of groups that have all SATD
comments in the same file is 62, 25, 27, 57, 9, for project JSPWiki,
Archiva, Jackrabbit, SystemML, and Helix respectively. We further
perform four Mann-Whitney test [20] comparing the group ratios
between Helix and the other four projects. The test result shows
that the groups from Helix project contain a significantly lower
ratio of pairs in the same file (𝑝 < 0.05). We then perform a man-
ual examination on groups in Helix that have a low ratio of pairs
appearing in the same file. One potential explanation is that many
such pairs (in different files) are introduced because developers fre-
quently change the project’s file structure by removing one file and
creating another file at a different location. This is understandable
because Helix is a less mature project than the other four projects,
with the latest version being v0.8.4 (in our dataset).

3.3 RQ3: Are duplicate SATD comments

introduced and removed at the same time?

Approach: For each identical group of duplicate SATD comments
(484 groups/1,437 SATD comments/2,505 pairs), we categorize the
relationship of the two SATD comments in each pair of duplicate
SATD comments by checking the commits that introduce the two
comments and determine if both comments are introduced in the
same commit. Next, we check if the two duplicate SATD comments
have same removal status, i.e., removed in the same commit or
both remain, remove in different commits, one is removed and the
other remain in the system. The later two cases show inconsistent
evolution of duplicate SATD comments. We perform the above
analysis for pairs from each project and investigate if statistics vary
across different projects.

Results: 848 (33.9%) of the 2,505 studied duplicate SATD com-
ment pairs are introduced at the same time. 77% of the 848
pairs have a consistent removal status. Table 2 shows the dis-
tribution of different types of duplicate SATD comment pairs deter-
mined by their introduce and removal status. Surprisingly, across
all 5 projects, less than 50% of the considered pairs are introduced
at the same time. This number varies when we break the numbers
down to each project. For instance, the ratio of pairs introduced in
the same commit ranges from 17.5% to 43.4%, with the highest ratio
in project SystemML and the lowest ratio in project JSPWiki. The

ratio of pairs having consistent removal status are 27.8%, 80%, 80%,
84.5%, 80%, for project JSPWiki, Archiva, Jackrabbit, SystemML, and
Helix respectively. For pairs that are introduced at the same commit
but removed at different commit (122 pairs), we also measure the
difference of their removal time and observe that the overall mean
and median of the removal time difference is 8.3 days and 1.4 days
respectively. This indicate potential time saving if we can improve
the awareness of duplicate SATD comments.

4 RELATED WORK
SATD was coined only a few years ago by Potdar and Shihab [14],
but many studies have been conducted, indicating the importance
of this field of research. Sierra et al. [17] provided a survey of self-
admitted technical debt and classified SATD research work into
three categories: SATD detection [5, 6, 8, 9, 14], SATD comprehen-
sion [1, 3, 7, 21, 22], and SATD repayment [10, 11]. In this regard,
our study took place in SATD comprehension category. One close
related work is from Gao et al. [4]. They are concerned about the
quality of SATD comments, focusing on obsolete SATD comments,
and they proposed an approach that can automatically detect obso-
lete TODO comments.

5 CONCLUSION AND FUTURE WORK
This paper introduces a new concept, i.e., duplicate SATD comment,
referring to a SATD comment that is identical or highly similar
(semantically identical) to one or more other SATD comments. We
conduct the first empirical study by automatically identifying gen-
eral and duplicate SATD comments from five Apache projects. We
find that 41.3%-65% of our identified SATD comments are duplicate
SATD comments. We manually analyzed 500 groups of duplicate
SATD comments and found that the majority of them are identi-
cal groups, i.e., all SATD comments are introduced by the same
root cause. We also find that 12.9%-72.1% of the pairs in identi-
cal groups appear in the same file, but only 33.9% of the duplicate
SATD comment pairs in identical groups are introduced in the same
commit. In the future, we would like to expand the scope of our
study and further investigate who introduced/removed/modified
duplicate SATD comments, why they have different evolution in
the project history, and what are the impacts of those duplicate
SATD comments on the quality and evolution of software projects.

ACKNOWLEDGEMENT
We acknowledge the support of the Natural Sciences and Engi-
neering Research Council of Canada (NSERC), [funding reference
number: RGPIN-2019-05071].

A First Look at Duplicate and Near-duplicate Self-admitted Technical Debt Comments

ICPC ’22, May 16–17, 2022, Virtual Event, USA

REFERENCES
[1] Gabriele Bavota and Barbara Russo. 2016. A Large-Scale Empirical Study on
Self-Admitted Technical Debt. In Proceedings of the 13th International Confer-
ence on Mining Software Repositories (Austin, Texas) (MSR ’16). Association for
Computing Machinery, New York, NY, USA, 315–326. https://doi.org/10.1145/
2901739.2901742

[2] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-
based algorithm for discovering clusters in large spatial databases with noise.. In
kdd, Vol. 96. 226–231.

[3] Gianmarco Fucci, Nathan Cassee, Fiorella Zampetti, Nicole Novielli, Alexander
Serebrenik, and Massimiliano Di Penta. 2021. Waiting around or job half-done?
Sentiment in self-admitted technical debt. In 2021 IEEE/ACM 18th International
Conference on Mining Software Repositories (MSR). IEEE, 403–414.

[4] Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann. 2021.
Automating the removal of obsolete TODO comments. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 218–229.

[5] Zhaoqiang Guo, Shiran Liu, Jinping Liu, Yanhui Li, Lin Chen, Hongmin Lu, and
Yuming Zhou. 2021. How far have we progressed in identifying self-admitted
technical debts? A comprehensive empirical study. ACM Transactions on Software
Engineering and Methodology (TOSEM) 30, 4 (2021), 1–56.

[6] Qiao Huang, Emad Shihab, Xin Xia, David Lo, and Shanping Li. 2018. Identify-
ing Self-Admitted Technical Debt in Open Source Projects Using Text Mining.
Empirical Softw. Engg. 23, 1 (feb 2018), 418–451. https://doi.org/10.1007/s10664-
017-9522-4

[7] Zengyang Li, Paris Avgeriou, and Peng Liang. 2015. A systematic mapping study
on technical debt and its management. Journal of Systems and Software 101 (2015),
193–220. https://doi.org/10.1016/j.jss.2014.12.027

[8] Zhongxin Liu, Qiao Huang, Xin Xia, Emad Shihab, David Lo, and Shanping
Li. 2018. SATD Detector: A Text-Mining-Based Self-Admitted Technical Debt
Detection Tool. In Proceedings of the 40th International Conference on Soft-
ware Engineering: Companion Proceeedings (Gothenburg, Sweden) (ICSE ’18).
Association for Computing Machinery, New York, NY, USA, 9–12.
https:
//doi.org/10.1145/3183440.3183478

[9] Rungroj Maipradit, Bin Lin, Csaba Nagy, Gabriele Bavota, Michele Lanza, Hideaki
Hata, and Kenichi Matsumoto. 2020. Automated Identification of On-hold Self-
admitted Technical Debt. CoRR abs/2009.13113 (2020). arXiv:2009.13113 https:
//arxiv.org/abs/2009.13113

[10] Solomon Mensah, Jacky Keung, Michael Franklin Bosu, and Kwabena Ebo Bennin.
2016. Rework effort estimation of self-admitted technical debt. CEUR Workshop

Proceedings 1771 (2016), 72–75. http://ceur-ws.org/Vol-1771/ Joint of the 4th In-
ternational Workshop on Quantitative Approaches to Software Quality, QuASoQ
2016 and 1st International Workshop on Technical Debt Analytics, TDA 2016 ;
Conference date: 06-12-2016.

[11] Solomon Mensah, Jacky Keung, Jeffery Svajlenko, Kwabena Ebo Bennin, and Qing
Mi. 2018. On the Value of a Prioritization Scheme for Resolving Self-Admitted
Technical Debt. J. Syst. Softw. 135, C (jan 2018), 37–54. https://doi.org/10.1016/j.
jss.2017.09.026

[12] Manishankar Mondai, Chanchal K Roy, and Kevin A Schneider. 2018. Micro-
clones in evolving software. In 2018 IEEE 25th International Conference on Software
Analysis, Evolution and Reengineering (SANER). IEEE, 50–60.

[13] Manishankar Mondal, Banani Roy, Chanchal K Roy, and Kevin A Schneider. 2020.
Investigating near-miss micro-clones in evolving software. In Proceedings of the
28th International Conference on Program Comprehension. 208–218.

[14] Aniket Potdar and Emad Shihab. 2014. An Exploratory Study on Self-Admitted
Technical Debt. In 2014 IEEE International Conference on Software Maintenance
and Evolution. 91–100. https://doi.org/10.1109/ICSME.2014.31

[15] Chanchal K Roy and James R Cordy. 2008. NICAD: Accurate detection of near-
miss intentional clones using flexible pretty-printing and code normalization. In
2008 16th iEEE international conference on program comprehension. IEEE, 172–181.
[16] Chanchal K Roy and James R Cordy. 2009. A mutation/injection-based automatic
framework for evaluating code clone detection tools. In 2009 international confer-
ence on software testing, verification, and validation workshops. IEEE, 157–166.

[17] Giancarlo Sierra, Emad Shihab, and Yasutaka Kamei. 2019. A survey of self-
admitted technical debt. Journal of Systems and Software 152 (2019), 70–82.
https://doi.org/10.1016/j.jss.2019.02.056

[18] Alexander Trautsch, Fabian Trautsch, and Steffen Herbold. 2021. MSR Min-
ing Challenge: The SmartSHARK Repository Mining Data. arXiv preprint
arXiv:2102.11540 (2021).

[19] Tiantian Wang, Mark Harman, Yue Jia, and Jens Krinke. 2013. Searching for
better configurations: a rigorous approach to clone evaluation. In Proceedings of
the 2013 9th Joint Meeting on Foundations of Software Engineering. 455–465.
[20] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-

throughs in statistics. Springer, 196–202.

[21] Laerte Xavier, Fabio Ferreira, Rodrigo Brito, and Marco Tulio Valente. 2020.
Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems.
Association for Computing Machinery, New York, NY, USA, 137–146. https:
//doi.org/10.1145/3379597.3387459

[22] Fiorella Zampetti, Gianmarco Fucci, Alexander Serebrenik, and Massimiliano Di
Penta. 2021. Self-Admitted Technical Debt Practices: A Comparison Between
Industry and Open-Source. Empirical Software Engineering 26, 6 (Nov. 2021).
https://doi.org/10.1007/s10664-021-10031-3

