2
2
0
2

n
u
J

0
2

]

C
D
.
s
c
[

2
v
0
0
1
1
1
.
3
0
2
2
:
v
i
X
r
a

Migrating CUDA to oneAPI: A Smith-Waterman
Case Study

Manuel Costanzo1, Enzo Rucci1, Carlos Garcia Sanchez2, Marcelo
Naiouf1, and Manuel Prieto-Matias2

1III-LIDI, Facultad de Inform´atica, Universidad Nacional de La
Plata - CIC, La Plata, Buenos Aires, Argentina,
{mcostanzo,erucci,mnaiouf}@lidi.info.unlp.edu.ar
2Dpto. Arquitectura de Computadores y Autom´atica, Universidad
Complutense de Madrid. Madrid (28040), Espa˜na,
{garsanca,mpmatias}@dacya.ucm.es

March 1, 2022

Abstract

To face the programming challenges related to heterogeneous comput-
ing, Intel recently introduced oneAPI, a new programming environment
that allows code developed in Data Parallel C++ (DPC++) language to
be run on diﬀerent devices such as CPUs, GPUs, FPGAs, among oth-
ers. To tackle CUDA-based legacy codes, oneAPI provides a compati-
bility tool (dpct) that facilitates the migration to DPC++. Due to the
large amount of existing CUDA-based software in the bioinformatics con-
text, this paper presents our experiences porting SW#db, a well-known
sequence alignment tool, to DPC++ using dpct. From the experimental
work, it was possible to prove the usefulness of dpct for SW#db code
migration and the cross-GPU vendor, cross-architecture portability of the
migrated DPC++ code. In addition, the performance results showed that
the migrated DPC++ code reports similar eﬃciency rates to its CUDA-
native counterpart or even better in some tests (approximately +5%).

Keywords— oneAPI SYCL GPU CUDA Bioinformatics

This version of the contribution has been accepted for publication, after
peer review (when applicable) but is not the Version of Record and does

not reflect post-acceptance improvements, or any corrections.

The

Version of Record is available online at:
https://doi.org/10.1007/978-3-031-07802-6_9. Use of this Accepted
Version is subject to the publisher’s Accepted Manuscript terms of use

https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms\OT1\textquotedblright

1

 
 
 
 
 
 
1

Introduction

At present, heterogeneous computing and massively parallel architectures have proven
to be an eﬀective strategy for maximizing the performance and energy eﬃciency of
computing systems [20]. That is the main reason why the programmers typically
rely on a variety of hardware, like CPU, GPU, FPGAs, and other kinds of acceler-
ators. This raises the need for specialized libraries, tools, and APIs that increase
the programming cost and complexity, and complicate future code maintenance and
extension.
On the one hand, Khronos Group has proposed SYCL 1, an open standard, to face some
of the programming issues related to heterogeneous computing. Although SYCL shares
some characteristics with OpenCL (such as being royalty-free and cross-platform), it
can actually be seen as an improved, high-level version of the latter. SYCL is an
abstraction layer that enables code for heterogeneous systems to be written using
standard, single-source C++ host code including accelerated code expressed as func-
tions or kernels. SYCL implementations are often based on OpenCL, but also have the
ﬂexibility to use other backends like CUDA or OpenMP. Furthermore, SYCL features
asynchronous task graphs, buﬀers deﬁning location-independent storage, interoperabil-
ity with OpenCL, among other characteristics oriented to increase productivity [18, 6].

On the other hand, Intel recently introduced the oneAPI programming ecosystem that
provides a uniﬁed programming model for a wide range of hardware architectures.
The core of the oneAPI environment is a simpliﬁed language to express parallelism in
heterogeneous platforms, named Data Parallel C++ (DPC++), which can be sum-
marized as C++ with SYCL. In addition, oneAPI also comprises a runtime, a set of
domain-focused libraries and supporting tools [1].

In this scenario, GPUs can be considered the dominant accelerator and CUDA is
the most popular programming language for them nowadays [14]. Bioinformatics and
Computational Biology are some of the communities that have been exploiting GPUs
for more than two decades [12]. Lots of GPU implementations can be found in se-
quence alignment [3], molecular dynamics [9], molecular docking [13], prediction and
searching of molecular structures [11], among other application areas. Even though
some applications achieve a better performance with CUDA, their portability to other
architectures is strongly restricted due to their proprietary nature.

To tackle CUDA-based legacy codes, oneAPI provides a compatibility tool (dpct)
that facilitates the migration to the SYCL-based DPC++ programming language.
A few preliminary studies assessing dpct usefulness can be found in simulation [1],
math [19, 2], and cryptography [10]; however, to the best of our knowledge, no study
assess their utility in Bioinformatics arena. In this paper, we present our experiences
porting a biological software tool to DPC++ using dpct.
In particular, we have
selected SW#db [8]: a CUDA-based, memory-eﬃcient implementation of the Smith-
Waterman (SW) algorithm, that can be used either as a stand-alone application or a
library. Our contributions are:

• An analysis of the dpct eﬀectiveness for the CUDA-based SW#db migration,
including a detailed summary of the porting steps that required manual modi-
ﬁcations.

• An analysis of the DPC++ code’s portability, considering diﬀerent target plat-

forms and vendors (Intel CPUs and GPUs; NVIDIA GPUs).

• A comparison of the performance on diﬀerent hardware architectures (Intel

CPUs and GPUs; NVIDIA GPUs).

1https://www.khronos.org/registry/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf

2

This work can be considered the starting point for a more exhaustive evaluation explo-
ration of CUDA-based biological tool migration to oneAPI. The remaining sections of
this article are organized as follows: in Section 2, the background is presented. Next, in
Section 3, the migration process is described, and in Section 4, the experimental work
carried out is detailed and the results obtained are analyzed. Finally, in Section 5, the
conclusions and possible lines of future work are presented.

2 Background

2.1 The oneAPI Programming Ecosystem
oneAPI 2 is a uniﬁed programming model for application development that can be
used on diﬀerent architectures, such as CPUs, GPUs, and even FPGAs. oneAPI seeks
to facilitate the hard task of developing applications on a diﬀerent set of hardware.
Using oneAPI, the coding task can be performed at various levels: (1) invoking one of
the multiple optimized libraries (oneMKL, oneDAL, oneVPL, etc) that takes advan-
tage of oﬄoading technology in a transparent way to the programmer, or (2) direct
programming using the SYCL heterogeneous programming language supported by the
Data Parallel C++ (DPC++) language. The DPC++ programming language (sup-
ported by Intel’s dpcpp compiler) combines the C++ language with SYCL, allowing
the same source code to be compiled and executed across diﬀerent accelerators.

oneAPI comprises several programming tools and one of the most interesting con-
sidering code migration is a compatibility one named as dpct. This tool converts
applications written in the proprietary CUDA language to SYCL. According to Intel,
this tool automatically migrates 80%-90% of the original CUDA code to SYCL. In ad-
dition, regarding non-ported code, dpct inlines comments (through warning messages)
that help the programmer to migrate and tune the ﬁnal DPC++ code. [4].

The migration process consists of 3 stages:

1. Run the dpct tool that performs the automatic code migration.
2. Modify the migrated code attending all dpct warnings to reach a ﬁrst, executable

version following the diagnostics reference3.

3. Verify the correctness and eﬃciency of the resulting oneAPI program and make

the necessary modiﬁcations accordingly.

2.2 Smith-Waterman Algorithm

This algorithm was proposed by Smith and Waterman [17] to obtain the optimal local
alignment between two biological sequences. SW employs a dynamic programming
approach and presents quadratic time and space complexities. Furthermore, it has
been used as the basis for many subsequent algorithms and is often employed as a
benchmark when comparing diﬀerent alignment techniques [5].

The SW algorithm can be used to compute (a) pairwise alignments (one-to-one) or
(b) database similarity searches (one-to-many). Both cases have been parallelized
in the literature.
In case (a), a single SW matrix is calculated and all Processing
Elements (PEs) work collaboratively (intra-task parallelism). Due to inherent data
dependencies, neighbour PEs communicate in order to exchange border elements. In
case (b), multiple SW matrices are calculated simultaneously without communication
between the PEs (inter-task parallelism) [3].

2https://www.oneapi.com/
3Diagnostics Reference

of

Intel® DPC++ Compatibility Tool

available

at:

https://software.intel.com/content/www/us/en/develop/documentation/intel-dpcpp-compatibility-tool-user-guide/top/diagnostics-reference.html

3

2.3 SW#

SW# is a tool to compute biological sequence alignments that can be used as an
API-based library or as a standalone command-line executable [7]. It is considered
a versatile tool since it works with both protein and DNA sequences, being able to
compute pairwise alignments as well as database similarity searches.

SW#db is the package for fast exact similarity searches, which works by simultaneously
utilizing CPU and GPU(s). The GPU part is based on CUDA and follows both
inter-task and intra-task parallelism approaches (depending on the sequence lenght).
On its behalf, CPU just exploits inter-task paralleism through multithreading and
SIMD instructions 4. Through dynamic work distribution and dynamic communication
between the CPU-GPU, SW#db signiﬁcantly reduces execution time.

3

Implementation

3.1 Diﬀerences between CUDA and DPC++

Before migrating a code from CUDA to oneAPI, some diﬀerences should be considered.

3.1.1 Memory model:

on the one hand, CUDA provides two diﬀerent types of memory model:

1. Conventional model: it is mandatory to specify all memory operations between

the CPU and GPU.

2. Uniﬁed Memory (UM): introduced in CUDA 6, this model creates a shared
memory pool between the CPU and GPU, where both access the data through
pointers in a transparent manner to the programmer.

On the other hand, oneAPI oﬀers three abstractions for managing memory:

1. Buﬀers: they are data abstractions that represent one or more objects of a
given C++ language type. Buﬀers represent data objects rather than speciﬁc
memory addresses, so the same buﬀer can be allocated to several diﬀerent mem-
ory locations on diﬀerent devices, or even on the same device, for performance
reasons.

2. Images: they are a special type of buﬀer created especially for image processing.
They include support for special image formats, image reading through sampling
objects, among others.

3. Uniﬁed Shared Memory (USM): it consists of creating a uniﬁed virtual memory
space where pointers are shared between the CPU and the device (similar to the
CUDA UM).

3.1.2 Verbosity:

in DPC++, all variables used within a kernel must be declared and explicitly sent
to the functions, as well as other aspects that in CUDA are not mandatory. On the
contrary, in CUDA it is possible to indicate the variables that you want to send to
the device and implicitly use them in the kernels. These issues may cause the oneAPI
code to be longer than its CUDA counterpart.

4In

particular,

it makes

use

of

the OPAL

library

for

the CPU

part https://github.com/Martinsos/opal

4

1

2

3

4

5

6

7

8

9

size_t valuesSize =

databaseLen * sizeof(double);

double* valuesGpu;

CUDA_SAFE_CALL(
cudaMalloc(

&valuesGpu, valuesSize

)

);

(a) CUDA

1

2

3

4

5

6

7

8

size_t valuesSize =

databaseLen * sizeof(double);

double* valuesGpu;

CUDA_SAFE_CALL((

valuesGpu = (double *)sycl::malloc_device(
valuesSize, dpct::get_default_queue()),

0));

(b) oneAPI

Figure 1: CUDA SAFE CALL example

3.2 Migrating CUDA Codes to DPC++

As a general, dpct is not able to generate a fully functional DPC++ code. Thus, it is
required to perform hand-tunned adaptations. However, the dpct tool reports list of
warnings which facilitates successful refactoring.

3.2.1 Warnings generated by dpct:

these warnings vary among simple recommendations (i.e to improve the performance)
to more complex issues, such as fragments code not successfully migrated.

This section will detail the messages reported by the migration dpct tool when porting
the SW# and the manual adaptation made to obtain the ﬁnal DPC++ code.

DPCT1003 : Migrated API d o e s not

r e t u r n e r r o r code .

( ∗ , 0 )

i s

i n s e r t e d . You may need t o r e w r i t e

t h i s code

DPCT1009 : SYCL u s e s e x c e p t i o n s

e r r o r s and d o e s not
u s e t h e e r r o r c o d e s . The o r i g i n a l code was commented out
and a warning s t r i n g was
t h i s code .

i n s e r t e d . You need t o r e w r i t e

t o r e p o r t

Both warnings occur when using native CUDA functions, such as CUDA error codes
(Fig. 1a). Since dpct cannot translate them, it modiﬁes the code to still keep it
functional (Fig. 1b). Generally, this technique is used when exchanging data with the
device. Figs. 1a) and 1b) show memory allocations on the GPU using CUDA and
oneAPI, respectively. By default, dpct tries to use the USM model because it produces
less volume of code and allows dpct to support more memory-related APIs.

DPCT1005 : The SYCL d e v i c e v e r s i o n i s d i f f e r e n t

from CUDA

Compute C o m p a t i b i l i t y . You may need t o r e w r i t e

t h i s code .

This problem is related to the previous one and appears when querying for intrinsic
CUDA attributes. While dpct can obtain information from the GPU, such as the
number of registers, maximum memory size, among others 5, some CUDA-proprietary
attributes (e.g. CUDA driver information) are not translatable. Fig. 2a shows that,
in the original code, the number of CUDA blocks and threads depends on the driver
version. Fig. 2b presents the migrated code, showing that it is possible to obtain
information about GPU properties, with the exception of those speciﬁc to CUDA.

5https://docs.oneapi.io/versions/latest/dpcpp/iface/device.html

5

1

2

3

4

5

6

7

8

9

10

11

12

13

14

cudaDeviceProp properties;
cudaGetDeviceProperties(
&properties, card

);

int threads;
int blocks;
if (properties.major < 2) {
threads = THREADS_SM1;
blocks = BLOCKS_SM1;

} else {

threads = THREADS_SM2;
blocks = BLOCKS_SM2;

}

(a) CUDA

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

dpct::device_info properties;

dpct::dev_mgr::instance()
.get_device(card)
.get_device_info(properties);

int threads;
int blocks;

if (false) {

threads = THREADS_SM1;
blocks = BLOCKS_SM1;

} else {

threads = THREADS_SM2;
blocks = BLOCKS_SM2;

}

(b) oneAPI

Figure 2: Querying device properties

1

solveShort<<<blocks, threads>>>(...);

(a) CUDA

1

2

3

4

5

6

7

8

9

10

11

12

dpct::get_default_queue()

.submit([&](sycl::handler &cgh) {

...

cgh.parallel_for(

sycl::nd_range<3>

(sycl::range<3>(1, 1, blocks) *
sycl::range<3>(1, 1, threads),
sycl::range<3>(1, 1, threads)),

[=](sycl::nd_item<3> item_ct1) {

solveShort(...);

});

});

(b) oneAPI

Figure 3: Kernel launch with dynamic work-group size

DPCT1049 : The workgroup s i z e p a s s e d t o t h e SYCL k e r n e l may
t h e d e v i c e l i m i t , q uery i n f o : :

e x c e e d t h e l i m i t . To g e t
d e v i c e : : m a x w o r k g r o u p s i z e . Adjust
needed

t h e workgroup s i z e

i f

To run the CUDA kernel, both block and thread sizes must be conﬁgured; however,
each device have a diﬀerent size limit. dpct alerts the programmer that the migrated
code may exceed the maximum work-group limit that the underlying architecture
supports. In addition, it recommends adjusting the code if necessary. Fig. 3a shows
how to run the kernel in CUDA, while Fig. 3b shows the DPC++ counterpart.

DPCT1065 : C o n s i d e r

r e p l a c i n g s y c l : : n d i t e m : : b a r r i e r ( ) with

s y c l : : n d i t e m : : b a r r i e r ( s y c l : : a c c e s s : : f e n c e s p a c e : :
l o c a l s p a c e )
t o g l o b a l memory

f o r b e t t e r p e r f o r m a n c e

t h e r e

i f

i s no a c c e s s

On this situation, dpct recommends the programmer to use an additional parameter
when synchronizing threads within the kernel as long as no global memory is used. By
default, the tool does not automatically optimize this issue because it cannot discern
whether this memory is being used. An example of the CUDA thread synchronization
and the migrated oneAPI code can be seen in the Figs. 4a and 4b, respectively.

DPCT1084 : The f u n c t i o n c a l l has m u l t i p l e m i g r a t i o n r e s u l t s
i n s t a n t i a t i o n s

t h a t c o u l d not be

t e m p l a t e

d i f f e r e n t
u n i f i e d . You may need t o a d j u s t

t h e code .

i n

6

1

2

3

...
__syncthreads();
...

(a) CUDA

1

2

3

4

5

6

7

8

...
item_ct1.barrier();

//DPCT recommends adding the following parameter.
/*item_ct1.barrier(

sycl::access::fence_space::local_space

);*/
...

(b) oneAPI

Figure 4: Thread synchronization

In CUDA, generic functions are a common way to reduce code size, since they permit
code reusing for data of diﬀerent type. Although oneAPI supports this programming
feature, it cannot automatically port this kind of code due to the multiplicity of possible
migration options. Fig. 5a shows a CUDA example where instructions depend on
the type of parameter sent to the kernel function. Fig. 5b shows the corresponding
migrated code.

DPCT1059 : SYCL o n l y s u p p o r t s 4−c h a n n e l

image format . Adjust

t h e code .

In SYCL,
In CUDA, texture memory variables can contain from 1 to 4 channels.
texture memory is accessed through images. As it was reported by the dpct warning,
SYCL only supports the use of 4-channel images, so the programmer must adapt the
parts of the code where images of diﬀerent sizes are used. In Fig. 6a a 1-channel texture
variable is declared in CUDA (placed in the device) and ﬁnally a data is read from
it. Fig. 6b presents a possible adjustment to corresponding code to convert a texture
1-channel variable to an equivalent 4-channel one. As it can be seen, this conversion
requires to modify the indexes through which the memory is accessed to obtain the
correct data. In that sense, a 2-bit right shift (equivalent to DIV 4) combined with a
logical AND 3 operation (equivalent to MOD 4) must be performed in the corresponding
read operation.

3.2.2 Runtime and results check:

once the code compiles correctly, it must be veriﬁed that there are no execution errors
and that the results obtained are correct. In this case, although the oneAPI program
compiled correctly, the following runtime error appeared:

For a 1D/2D image / image a r r a y ,

t h e width must be a Value >= 1

and <= CL DEVICE IMAGE2D MAX WIDTH

This error appears because SYCL images have a limited size, being the maximum size
of the 1D images (vectors) smaller than their 2D counterparts (matrices). To solve this
issue, this image object must be converted to another DPC++ memory abstraction:
buﬀers or USM. We have chosen USM because the required modiﬁcations were simpler
compared to the other option.

Fig. 7a shows how a 2-level texture memory is allocated on the GPU, while Fig. 7b
illustrates how to use USM to send a the array to the device. In that sense, the read
mechanism also changes, both in CUDA (Fig. 8a) and in DPC++.

7

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

class SubScalarRev {
public:

__device__ int operator()(...) {

...

}

};

class SubVector {
public:

__device__ int operator()(...) {

...

}

};

template <class Sub>
__global__ static void solveLong(
..., Sub sub) {

sub(...);

}

solveLong<<<blocks, threads>>>(

..., SubScalarRev()

);

solveLong<<<blocks, threads>>>(

..., SubVector());

(a) CUDA

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

class SubScalarRev {
public:

int operator()(...) {

...

}

};

class SubVector {
public:

int operator()(...) {

...

}

};

template <class Sub>
static void solveLong(..., Sub sub) {

sub(...);

}

cgh.parallel_for(

sycl::nd_range<3>

(sycl::range<3>(1, 1, blocks) *
sycl::range<3>(1, 1, threads),
sycl::range<3>(1, 1, threads)),

[=](sycl::nd_item<3> item_ct1) {

solveLong(..., SubScalarRev());

});

});

cgh.parallel_for(

sycl::nd_range<3>

(sycl::range<3>(1, 1, blocks) *
sycl::range<3>(1, 1, threads),
sycl::range<3>(1, 1, threads)),

[=](sycl::nd_item<3> item_ct1) {
solveLong(..., SubVector());

});

});

(b) oneAPI

Figure 5: Generic functions

After the DPC++ program executed successfully, diﬀerent tests were performed and
their results were veriﬁed to ensure that they were equivalent to those of the original
CUDA code.

4 Experimental Results

4.1 Experimental design

All tests were carried out using the platforms described in Table 1. oneAPI and CUDA
versions are 2022.0 and 11.5, respectively, and to run DPC++ codes on NVIDIA
GPUs, we build a DPC++ toolchain with support for NVIDIA CUDA, as it is not
supported by default on oneAPI 6. The performance was evaluated by carrying out
similar experiments to those in previous works [15, 16], searching 20 query protein
sequences against the well-known UniProtKB/Swiss-Prot database (release 2021 04)7:

• The input queries range in length from 144 to 5478, and they were extracted
from the Swiss-Prot database (accession numbers: P02232, P05013, P14942,
P07327, P01008, P03435, P42357, P21177, Q38941, P27895, P07756, P04775,
P19096, P28167, P0C6B8, P20930, P08519, Q7TMA5, P33450, and Q9UKN1).

6https://intel.github.io/llvm-docs/GetStartedGuide.html
7Swiss-Prot: https://www.uniprot.org/downloads

8

1

2

3

4

5

6

7

8

9

10

11

1

2

3

4

5

6

7

8

9

10

11

12

texture<char> colTexture;

int colSize = colsGpu * sizeof(char);
char *colGpu;
cudaMalloc(&colGpu, colSize);
cudaMemcpy(colGpu, colCpu,
colSize, TO_GPU);

cudaBindTexture(NULL, colTexture,

colGpu, colSize);

char v = tex1Dfetch(colTexture, 10);

(a) CUDA

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

// Migraci´on DPCT
//dpct::image_wrapper<char, 1> colTexture;

dpct::image_wrapper<sycl::char4, 1> colTexture;

int colSize = colsGpu * sizeof(char);
char* colGpu;

colGpu = (char *)sycl::malloc_device(

colSize, dpct::get_default_queue());

dpct::get_default_queue()

.memcpy(colGpu, colCpu, colSize).wait();

colTexture.attach(colGpu, colSize);

// DIV 4 y MOD 3
char v = colTexture.read(10 >> 2)[10 & 3];

(b) oneAPI

Figure 6: 4-channel texture memory

texture<int, 2,

cudaReadModeElementType> seqsTexture;

cudaArray *sequencesGpu;
cudaChannelFormatDesc channel =

seqsTexture.channelDesc;

cudaMallocArray(&sequencesGpu,

static int *seqsGpu;

seqsGpu = (int *)sycl::malloc_device(

sequencesCols * sequencesRows * sizeof(int),
dpct::get_default_queue());

1

2

3

4

5

6

&channel, sequencesCols, sequencesRows);

7

dpct::get_default_queue()

cudaMemcpyToArray(sequencesGpu, 0, 0,

sequences, sequencesSize, TO_GPU);

cudaBindTextureToArray(

seqsTexture, sequencesGpu);

8

9

10

(a) CUDA

.memcpy(seqsGpu, sequences, sequencesCols *
sequencesRows * sizeof(int))

.wait();

(b) oneAPI

Figure 7: CUDA 2-D texture memory adaptation using DPC++ USM

1

2

int columnCodes = tex2D(

seqsTexture, colOff, j + rowOff);

1

2

int columnCodes =

seqsGpu[(j + rowOff) * sequencesCols + colOff];

(a) CUDA

(b) oneAPI

Figure 8: Data accessing in 2D array

9

Table 1: Experimental platforms used in the tests

CPU

ID

Processor

RAM
Memory

Core-i5

Core-i3

Core-i9

Xeon

Intel Core
i5-7400
Intel Core
i3-4160
Intel Core
i9-10920X
Intel Xeon
E-2176G

16 GB

8 GB

32 GB

65 GB

ID

Titan

RTX

Vendor
(Type)

NVIDIA
(Discrete)
NVIDIA
(Discrete)

Iris XE Intel

P630

(Discrete)
Intel
(Integrated)

GPU

Model
(Architecture)

GFLOPS
peak (SP)

Titan X
(Pascal)
RTX 2070
(Turing)
Iris Xe MAX Graphics
(Gen 12.1)
UHD Graphics P630
(Gen 9.5)

10970

7465

2534

441.6

• The database contains 204173280 amino acid residues in 565928 sequences with

a maximum length of 35213.

• BLOSUM62 and 10(2) were set as the scoring matrix and gap insertion (exten-

sion) penalty, respectively.

As SW#db is a hybrid CPU-GPU software, just a single thread was conﬁgured at CPU
level (ﬂag T=1) to minimize its impact on the overall performance. Besides, diﬀerent
work-group 8 sizes were conﬁgured for kernel execution. Finally, each particular test
was run twenty times and the performance was calculated as their average to avoid
variability.

4.2 Performance Results

GCUPS (billion cell updates per second) is commonly used as the performance metric
in the context of SW [15]. Fig. 9 presents the performance of both CUDA and DPC++
versions on two NVIDIA GPUs when varying work-group size. First, it can be noted
that both codes are sensitive to the work-group size. In fact, the best performances
are reached using work-group sizes diﬀerent to the ones that SW#db set as default.
Regarding the performance on each NVIDIA GPU, there are no signiﬁcant diﬀerences
between both codes on the Titan. However, on the RTX, this situation gets reversed;
the DPC++ version outperforms its CUDA counterpart (approximately 5%).

Fig. 10 deepens the above analysis presenting the performance of both CUDA and
DPC++ versions on the NVIDIA GPUs when varying the query length (optimal work-
group size is used for each case). It can be noted that all versions beneﬁt from larger
workloads. As expected, the CUDA code achieves the same GCUPS as the DPC++
one for all query lengths on the Titan. Both DPC++ and CUDA versions present
practically the same performance on the RTX, outperforming the latter to the former
on the largest sequences.

To verify cross-GPU vendor portability, the DPC++ code was executed on two dif-
ferent Intel GPUs varying the query lenght (see Fig. 11). Due to the absence of an
optimized version for both Intel devices, little can be said about its performance.
However, it is important to remark that only two minor changes were necessary to
carry out these tests: (1) setting the appropriate work-group size and (2) setting the
corresponding backend. As the ported code was compiled and executed with minimal
tuning, there is probably room for further improvement.

Finally, Fig. 12 presents the performance of the DPC++ code on 4 diﬀerent In-
tel CPUs, demonstrating its cross-architecture portability. Considering performance,

8A DPC++ work-group is equivalent to a CUDA block

10

Figure 9: Performance of both CUDA
and DPC++ versions on the NVIDIA
GPUs when varying work-group size.

Figure 10:
Performance of both
CUDA and DPC++ versions on the
NVDIA GPUs when varying the query
lenght.

Figure 11: Performance of DPC++
code on the diﬀerent Intel GPUs when
varying the query lenght.

Figure 12: Performance of DPC++
code on the diﬀerent Intel CPUs when
varying the query lenght.

more GCUPS are reached as the query length increases. Once again, running the mi-
grated code just required minimal intervention and its performance could be improved
through ﬁne tuning.

5 Conclusions and Future Work

The recently introduced Intel oneAPI ecosystem aims to respond to the programming
challenge related to heterogeneous computing. In this paper, we present our experi-
ences migrating a CUDA-based, biological software tool to DPC++ using the oneAPI
framework. Among the main contributions of this research we can summarize:

• dpct proved to be an eﬀective tool for SW#db code migration to DPC++.
While it was not able to translate the complete code, dpct did most of the work
and gave hints to the programmer on the pending parts.

• The migrated code could be successfully executed on CPUs and also GPUs
from diﬀerent vendors, demonstrating its cross-architecture, cross-GPU vendor
portability.

• Performance results showed that the migrated DPC++ code is comparable to
In fact, DPC++ can be even faster in some cases.
the original CUDA one.
As the ported code was compiled and executed with minimal tuning, there is
probably room for further improvement.

Future work will focus on:

11

• Understanding the gap in performance between DPC++ and CUDA codes, and

optimizing DPC++ codes to reach their maximum performance.

• Carrying out more exhaustive experimental work.

In particular, considering
other alignment operations, larger workloads, multi-GPU execution, among oth-
ers, to increase the representativeness of this study.

• Running the DPC++ code on other architectures like FPGAs, to verify its

cross-architecture portability.

Acknowledgements

This paper has been supported by the EU (FEDER) and the Spanish MINECO and
CM under grants S2018/TCS-4423, RTI2018-093684-B-I00 and PID2021-126576NB-
I00.

References

[1] Christgau,
Code
to
https://doi.org/10.1109/IPDPSW50202.2020.00070

S.,
oneAPI.

Steinke,
In:

T.:
2020

Porting

a

IEEE IPDPSW. pp.

Legacy CUDA Stencil
(2020).
359–367

[2] Costanzo, M., Rucci, E., Sanchez, C.G., Naiouf, M.: Early experiences mi-
grating cuda codes to oneapi. In: Short papers of the 9th Conference on
Cloud Computing Conference, Big Data & Emerging Topics. pp. 14–18 (2021),
http://sedici.unlp.edu.ar/handle/10915/125138

[3] De Oilveira Sandes, E.F., Boukerche, A., De Melo, A.C.M.A.: Parallel optimal
pairwise biological sequence comparison: Algorithms, platforms, and classiﬁca-
tion. ACM Comput. Surv. 48(4) (2016). https://doi.org/10.1145/2893488

[4] Hariharan, N., Mallady, R.K., Kapoor, A., O’Leary, K.: Heterogeneous program-

ming using oneapi. Parallel Universe 39, 5 – 18 (2020)

[5] Hasan, L., Al-Ars, Z.: Computational Biology and Applied Bioinformatics,

chap. 9, pp. 187–202. InTech (September 2011)

[6] Keryell, R., Yu, L.Y.: Early Experiments Using SYCL Single-Source Modern
C++ on Xilinx FPGA. In: Proceedings of the IWOCL ’18. ACM, New York,
NY, USA (2018). https://doi.org/10.1145/3204919.3204937

[7] Korpar, M.,

Sikic, M.:

SW# - GPU-enabled

ments
scale. Bioinformatics
https://doi.org/10.1093/bioinformatics/btt410

genome

on

29(19),

exact
2494–2495

align-
(2013).

[8] Korpar, M., Sosic, M., Blazeka, D., Sikic, M.: SWdb: GPU-Accelerated Ex-
act Sequence Similarity Database Search. PLOS ONE 10(12), 1–11 (12 2016).
https://doi.org/10.1371/journal.pone.0145857

[9] Loukatou, S., Papageorgiou, L., Fakourelis, P., Filntisi, A., Polychronidou, E.,
Bassis, I., Megalooikonomou, V., Maka lowski, W., Vlachakis, D., Kossida, S.:
Molecular dynamics simulations through gpu video games technologies. Journal
of molecular biochemistry 3(2), 64 (2014)

[10] Marinelli, E., Appuswamy, R.: XJoin: Portable, Parallel Hash Join across
Diverse XPU Architectures with OneAPI, chap. 11, p. 5. ACM (2021).
https://doi.org/10.1145/3465998.3466012

[11] Mrozek, D., Bro˙zek, M., Ma lysiak-Mrozek, B.: Parallel implementation of 3d pro-
tein structure similarity searches using a gpu and the cuda. Journal of molecular
modeling 20(2), 1–17 (2014)

12

[12] Nobile, M.S., Cazzaniga, P., Tangherloni, A., Besozzi, D.: Graphics processing
units in bioinformatics, computational biology and systems biology. Brieﬁngs in
Bioinformatics 18(5), 870–885 (07 2016). https://doi.org/10.1093/bib/bbw058

[13] Ohue, M., Shimoda, T., Suzuki, S., Matsuzaki, Y., Ishida, T., Akiyama, Y.:
Megadock 4.0: an ultra–high-performance protein–protein docking software for
heterogeneous supercomputers. Bioinformatics 30(22), 3281–3283 (2014)

[14] Robert Dow:

GPU shipments

increase year-over-year

in Q3 (2021),

https://www.jonpeddie.com/press-releases/gpu-shipments-increase-year-over-
year-in-q3

[15] Rucci, E., Garcia, C., Botella, G., Giusti, A.E.D., Naiouf, M., Prieto-Matias, M.:
Oswald: Opencl smith–waterman on altera’s fpga for large protein databases.
The International Journal of High Performance Computing Applications 32(3),
337–350 (2018). https://doi.org/10.1177/1094342016654215

[16] Rucci, E., Sanchez, C.G., Juan, G.B., De Giusti, A., Naiouf, M., Prieto-Matias,
M.: Swimm 2.0: enhanced smith–waterman on intel’s multicore and manycore
architectures based on avx-512 vector extensions. International Journal of Parallel
Programming 47(2), 296–316 (2019)

[17] Smith, T.F., Waterman, M.S.: Identiﬁcation of common molecular subsequences.

Journal of Molecular Biology 147(1), 195–197 (March 1981)

[18] The Khronos

SYCL Working Group:

SYCL Speciﬁcation

(2020),

https://www.khronos.org/registry/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf

[19] Tsai, Y.M., Cojean, T., Anzt, H.: Porting a sparse linear algebra math library to

intel gpus (2021)

[20] Zahran, M.: Heterogeneous computing: Here to stay. Communications of the

ACM 60(3), 42–45 (2017)

13

