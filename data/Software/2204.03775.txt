Massively scalable stencil algorithm

Mathias Jacquelin§
Cerebras Systems Inc.
Sunnyvale, California, USA
mathias.jacquelin@cerebras.net

Mauricio Araya-Polo§ and Jie Meng
TotalEnergies EP Research & Technology US, LLC.
Houston, Texas, USA
mauricio.araya@totalenergies.com

2
2
0
2

r
p
A
7

]
S
M

.
s
c
[

1
v
5
7
7
3
0
.
4
0
2
2
:
v
i
X
r
a

Abstract—Stencil computations lie at the heart of
many scientiﬁc and industrial applications. Unfortu-
nately, stencil algorithms perform poorly on machines
with cache based memory hierarchy, due to low re-
use of memory accesses. This work shows that for
stencil computation a novel algorithm that leverages
a localized communication strategy eﬀectively exploits
the Cerebras WSE-2, which has no cache hierarchy.
This study focuses on a 25-point stencil ﬁnite-diﬀerence
method for the 3D wave equation, a kernel frequently
used in earth modeling as numerical simulation. In
essence, the algorithm trades memory accesses for data
communication and takes advantage of the fast commu-
nication fabric provided by the architecture. The algo-
rithm —historically memory bound— becomes com-
pute bound. This allows the implementation to achieve
near perfect weak scaling, reaching up to 503 TFLOPs
on WSE-2, a ﬁgure that only full clusters can eventually
yield.

Index Terms—Stencil computation, high perfor-
mance computing, energy, wafer-scale, distributed
memory, multi-processor architecture and micro-
architecture

I. Introduction

Stencil computations are central to many scientiﬁc prob-
lems and industrial applications, from weather forecast (
[32]) to earthquake modeling ( [19]). The memory access
pattern of this kind of algorithm, in which all values in
memory are accessed but used in only very few arith-
metic operations, is particularly unfriendly to hierarchical
memory systems of traditional architectures. Optimizing
these memory operations is the main focus of performance
improvement research on the topic.

Subsurface characterization is another area where sten-
cils are widely used. The objective is to identify major
structures in the subsurface that can either hold hydrocar-
bon or be used for CO2 sequestration. One step towards
that end is called seismic modeling, where artiﬁcial per-
turbations of the subsurface are modeled solving the wave
equation for given initial and boundary conditions. Solv-
ing seismic modeling eﬃciently is crucial for subsurface
characterization, since many perturbation sources need to
be modeled as the subsurface model iteratively improves.
The numerical simulations required by seismic algorithms
for ﬁeld data are extremely demanding, falling naturally
in the HPC category and requiring practical evaluation

§Equal contribution.

Traditional architecture
L1
L2 & L3
DRAM
Oﬀ-node interconnect

WSE
Memory
∅
∅
Fabric & routers

TABLE I: Equivalences between traditional architectures
and the WSE

of technologies and advanced hardware architectures to
speed up computations.

Advances in hardware architectures have motivated al-
gorithmic changes and optimizations to stencil applica-
tions for at least 20 years ( [23]). Unfortunately, the
hierarchical memory systems of most current architectures
is not well-suited to stencil applications, therefore limiting
performance. This applies to multi-core machines, clusters
of multi-cores, and accelerator-based platforms such as
GPGPUs, FPGAs, etc. ( [2],
[5]). Alternatively, non-
hierarchical architectures were explored in this context,
such as the IBM Cell BE ( [3]), yielding high computa-
tional eﬃciency but with limited impact.

A key element for large scale simulations is the potential
of deploying substantial number of processing units con-
nected by an eﬃcient fabric. The Cell BE lacked the former
and it had limited connectivity. Another example of non-
hierarchical memory system is the Connection Machine (
[12]), which excelled on scaling but at the cost of a very
complex connectivity. In this work, a novel stencil algo-
rithm based on localized communications that does not
depend on memory hierarchy optimizations is introduced.
This algorithm can take advantage of architectures such
as the WSE from Cerebras ( [4]) and potentially Anton
3-like systems ( [28]). These are examples of architectures
addressing both limitations described above.

Another angle to be considered is the availability of
hardware-based solutions in the market. Literature re-
view yields no generally available hardware architecture
addressing the speciﬁc bottlenecks of stencil applications.
Only a few custom designs examples are available ( [10],
[14]).

In this work, an implementation of such seismic mod-
eling method on a novel architecture is presented. The
proposed mapping requires a complete redesign of the
basic stencil algorithm. The contribution of this work is
multi-fold:

 
 
 
 
 
 
• An eﬃcient distributed implementation of Finite Dif-
ferences for seismic modeling on a fully-distributed
memory architecture with 850,000 processing ele-
ments.

• A stencil algorithm that is performance bound to the
capacity of individual processing element rather than
bound by memory or communication bandwidth.
• The target application ported relies on an industry-

validated stencil order.

The paper is organized as follows: Section II reviews
relevant contributions in the literature. Section III de-
scribes the target application. Section IV provides details
of how the target application was redesigned to eﬃciently
use the novel processor architecture. Sections V and VI
discusses experimental results and proﬁling data. Section
VII provides discussions and conclusions.

II. Related work

A. Stencil Computation

Not all stencil computations are the same, and the
structure and order of the stencil set the limits of the
attainable performance. The higher the order (neighbors
to be accounted for) and the closer to a pure star shape
is, the harder to compute the stencil is. Traditional hi-
erarchical memory subsystems will be overwhelmed by
the memory access pattern which displays very little data
reuse. Considerable amount of research eﬀort has been
devoted to optimizing stencil computations, and to ﬁnding
ways around these issues. Spurred by emerging hardware
technologies, studies on how stencil algorithms can be
tailored to fully exploit unique hardware characteristics
have covered many aspects, from DSLs, performance mod-
eling, to pure algorithmic optimizations, targeting low-
level architectural features in some cases.

Domain-speciﬁc languages (DSLs), domain-speciﬁc par-
allel programming models, and compiler optimizations for
stencils have been proposed (e.g., [9], [11], [16], [22]). Per-
formance models have been developed for this computing
pattern (see [6], [30]), and the kernel has been ported to
a variety of platforms ( [2], [3], [5], [35]) including speciﬁc
techniques to beneﬁt from unique hardware features.

Stencil computations have also been the subject of
multiple algorithmic optimizations. Spatial and temporal
blocking has been proposed [8], [13], [34]. A further exam-
ple is the semi-stencil algorithm proposed by De la Cruz
et al. [7], which oﬀers an improved memory access pattern
and a higher level of data reuse. Promising results are also
achieved using a higher dimension cache optimization, as
introduced by Nguyen et al. [20] , accommodating both
thread-level and data-level parallelism. Most recently, Sai
et al.
[26] studied high-order stencils with a manually
crafted collection of implementations of a 25-point seismic
modeling stencil in CUDA and HIP for the latest GPGPU
hardware. Along this line of hardware-oriented stencil
optimizations, Matsumura et al. [17] proposed a frame-

work (AN5D) for GPU stencil optimization, obtaining
remarkable results.

Wafer-scale computations have ﬁrst been explored in
Rocki et al. [24], in which the authors explore a BiCGStab
implementation to solve a linear system arising from a
7-point ﬁnite diﬀerence stencil on the ﬁrst generation of
Cerebras Wafer-Scale Engine. Albeit computing a much
simpler stencil and having a higher arithmetic intensity,
this study paved the way to the work presented in this
study. A notable diﬀerence between the current work and
this study is that ﬂoating point operations were performed
in mixed precision: stencil and AXPY operations being
computed using 16 bit ﬂoating point operations and global
reductions using 32 bit arithmetic. In the present study,
only 32 bit ﬂoating point arithmetic is used, and neither
AXPY operation nor global reductions are involved. This
makes the performance achieved by these two applications
not directly comparable.

III. Finite difference for seismic modeling

Minimod is a proxy application that simulates the prop-
agation of waves through the Earth models, by solving a
Finite Diﬀerence (FD) which is discretized form of the
wave equation. It is designed and developed by Total-
Energies EP Research & Technologies [18]. Minimod is
self-contained and designed to be portable across multi-
ple compilers. The application suite provides both non-
optimized and optimized versions of computational kernels
for targeted platforms. The main purpose is benchmarking
of emerging new hardware and programming technologies.
Non-optimized versions are provided to allow analysis of
pure compiler-based optimizations.

In this work, one of the kernels contained in Minimod is
used as target for redesign: the acoustic isotropic kernel in
a constant-density domain [21]. For this kernel, the wave
equation PDE has the following form:

1
V2

∂2u
∂t2 − ∇2u = f ,
where u = u(x, y, z) is the waveﬁeld, V is the Earth
model (with velocity as the main property), and f is the
source perturbation. The equation is discretized in time
using a 2nd order centered stencil, resulting in the semi-
discritized equation:

(1)

un+1 − Qun + un−1 = (cid:0)∆t2(cid:1) V2f n,
with Q = 2 + ∆t2V2∇2.

(2)

Finally, the equation is discretized in space using a 25-
point stencil in 3D (8th order in space), with four points
in each direction as well as the centre point:

∇2u(x, y, z) ≈

4
X

m=0

cxm [u(i + m, j, k) + u(i − m, j, k)] +

cym [u(i, j + m, k) + u(i, j − m, k)] +
czm [u(i, j, k + m) + u(i, j, k − m)]

where cxm, cym, czm are discretization parameters, solved
in step 4 in Algorithm 1. In the remainder of the document
we refer to this operator as the Laplacian.

A simulation in Minimod consists of solving the wave
equation at each timestep for thousands of timesteps.
Pseudocode of the algorithm is shown in Algorithm 1.

Data: f : source
Result: un: waveﬁeld at timestep n, for n ← 1 to

T

1 u0 := 0;
2 for n ← 1 to T do
3

for each point in waveﬁeld un do

Solve Eq. 2 (left hand side) for waveﬁeld un;

end
un = un + f n (Eq. 2 right hand side);

4

5

6
7 end

Algorithm 1: Minimod high-level description

We note that a full simulation includes additional ker-
nels, such as I/O and boundary conditions. These addi-
tional kernels are not evaluated in this study but will
be added in the future. The kernel has been ported and
optimized for GPGPUs,
full
report can be found in [25], this implementation is used
as baseline to compare the results with the proposed
implementation.

including NVIDIA A100,

IV. Finite-differences on the WSE

This section introduces general architectural details of
the Cerebras Wafer-Scale Engine (WSE) and discusses
hardware features allowing the target application to reach
the highest level of performance. The mapping of the
target algorithm onto the system is then discussed. The
implementation is referred to as Finite Diﬀerences in the
remainder of the study. Communication strategy and core
computational parts involved in Finite Diﬀerences are also
reviewed.

The implementation of Finite Diﬀerences on the WSE
is written in Cerebras Software Language (CSL) using the
Cerebras SDK [27], which allows software developers to
write custom programs for Cerebras systems. CSL is a
C-like language based on Zig [31], a reinterpretation of
C which provides a simpler syntax and allows to declare
compile-time blocks/optimizations explicitly (rather than
relying on macros and the C preprocessor). CSL provides
direct access to key hardware features, while allowing the
use of higher-level constructs such as functions and while
loops. The language allows to express computations and
communications across multiple cores. Excerpts provided
in the following will use the CSL syntax.

A. The WSE architecture

The WSE is an unprecedented-scale manycore proces-
sor. It is the ﬁrst wafer-scale system [4], embedding all

compute and memory resources within a single silicon
wafer, together with a high performance communication
interconnect. An overview of the architecture is given in
Figure 2. In its latest version, the WSE-2 provides a
total of 850,000 processing elements, each with 48 KB
of dedicated SRAM memory; up to eight 16-bit ﬂoating
point operations per cycle; 16 bytes of read and 8 bytes
of write bandwidth to the memory per cycle; and a 2D
mesh interconnection fabric that can handle 4 bytes of
bandwidth per PE per cycle in steady state [15].

The WSE can be seen as an on-wafer distributed-
memory machine with a 2D-mesh interconnection fabric.
This on-wafer network connects processing elements or
PEs. Each PE has a very fast local memory and is
connected to a router. The routers link to the routers
of the four neighboring PEs. There is no shared memory.
The WSE contains a 7 × 12 array of identical “dies”, each
holding thousands of PEs. Other chips are made by cutting
the wafer into individual die. In the WSE, however, the
interconnect is extended between dies. This results in a
wafer-scale processor tens of times larger than the largest
processors on the market at the time of its release.

The instruction set of the WSE is designed to operate
on vectors or higher dimensionality objects. This is done
by using data structure descriptors, which contain informa-
tion regarding how a particular object should be accessed
and operated on (such as address, length, stride, etc.).

As mentioned above, given the distributed memory
nature of the WSE, the interconnect plays a crucial role in
delivering performance. It is convenient to think of the 2D
mesh interconnect in terms of cardinal directions. Each PE
has 5 full-duplex links managed by its local router: East,
West, North, and South links allow data to reach other
routers and PEs, while the ramp link allows data to ﬂow
between the router and the PE, on which computations
can take place. Each link is able to move a 32 bit packet in
each direction per cycle. Each unidirectional link operates
in an independent fashion, allowing concurrent ﬂow of data
in multiple directions.

Every 32 bit packet has a color (contained in additional

bits of metadata). The role of colors is twofold:

1) Colors are used in the routing of communications. A
color determines the packet’s progress at each router it en-
counters from source to destination(s). A router controls,
for each color, where — to what subset of the ﬁve links —
to send a packet of that color. Moreover, colors are akin
to virtual channels in which ordering is guaranteed.

2) Colors can also be used to indicate the type of a
message: a color can be associated to a handler triggered
when a packet of that particular color arrives.

The WSE is an unconventional parallel computing ma-
chine in the sense that the entire distributed memory
machine lies within the same wafer. There is no cache
hierarchy nor shared memory. Equivalences between hard-
ware features of the WSE and what they correspond to

Z

X

Y

Fig. 1: The 25-point stencil used in Finite Diﬀerences. Cells
in white (along Z) reside in the local memory of a PE, blue
cells are communicated along the X dimension, and green
cells are communicated along the Y dimension.

on traditional architectures (such as distributed memory
supercomputers) are summarized in Table I.

B. Target Algorithm/application mapping

The sheer scale of the WSE calls for a novel map-
ping of the target algorithm onto the processor. The 3D
nx × ny × nz grid on which the stencil computation is
performed is mapped onto the WSE in two diﬀerent ways:
the X and Y dimensions are mapped onto the fabric while
the Z dimension is mapped into the local memory of a PE.
This follows the approach that was explored in Rocki et
al. [24], and has the beneﬁt of expressing the highest pos-
sible level of concurrency for this particular application.
Figure 3a depicts how the domain is distributed over the
WSE. Each PE owns a subset of nz cells of the original
grid, as depicted in Figure 3b. In order to simplify the
implementation, this local subset is extended by 8 extra
cells: 4 cells below and 4 cells above the actual grid. This
ensures that any cell in the original grid always has 4
neighbors below and 4 neighbors above. A PE stores the
waveﬁeld at two time steps (see Equation 2). In order to
lower overheads, computations and communications are
performed on blocks of size b. The block size is chosen
to be the largest such that the 2 × (nz + 8) cells and all
buﬀers depending on b can ﬁt in memory.

C. Stencil computation

Computing the spatial component (referred to as the
Laplacian) of the governing PDE lies at the heart of
the target application, and it is traditionally the most
demanding component. In addition to requiring a signiﬁ-
cant amount of ﬂoating point operations, computing the
Laplacian also involves data movement, which is known to
be very expensive on distributed memory platforms.

In the context of this paper, a 25-point stencil (depicted
in Figure 1) is used. The stencil spans over all three
dimensions of the grid. In order to compute a particular
cell, data from neighboring cells is needed in all three

dimensions. More precisely, a cellx,y,z requires data from
neighboring cells:

cellx−4≤i<x,y,z,
celli,y−4≤j<y,z,
celli,j,z−4≤k<z,

cellx<i≤x+4,y,z,
celli,y<j≤y+4,z,
celli,j,z<k≤z+4.

1) Localized broadcast patterns: Dimensions X and Y
from the grid are mapped onto the fabric of the WSE. To
compute the stencil, a PE has therefore to communicate
with 4 of its neighbors along each cardinal direction of the
PE grid. A communication strategy similar to Rocki et
al. [24], in which a single color is used per neighboring
PE, would have resulted in an excessive color use for
the stencil of interest to this application. In this work,
localized broadcast patterns along every PE grid directions
(Eastbound and Westbound for the X dimension, and
Northbound and Southbound for the Y dimension) are
used instead. Each broadcast pattern uses two dedicated
colors (one for receiving data, one for sending data) and
can happen concurrently with others broadcast patterns
using separate links to communicate between PEs. Given
the stencil size used in the application and the number of
colors available on the hardware, the limited color usage
per broadcast pattern is critical to the feasibility of the
implementation.

In each broadcast pattern, multiple Root PEs send their
local block of data of length b to their respective neigh-
boring 4 processing elements. This pattern is depicted in
Figure 4b for the Eastward direction.

The router of each PE is conﬁgured to control how
packets are received and transmitted. Each router deter-
mines, for each color, the incoming links from which that
color can be received and the subset of the ﬁve outgoing
links to which that color will be sent. The routing can
be changed at run-time by special commands which can
be sent just as other packets are sent. This capability
lies at the heart of the communication strategy proposed
here. In Figure 4a, the diﬀerent router conﬁgurations
used by Finite Diﬀerences are given. All Root PEs are in
conﬁguration 0. Intermediate PEs in each broadcast are
in conﬁguration 1, while Last PEs are in conﬁguration 2.
Ideally, a Root PE should broadcast its data only to other
PEs. However, due to hardware constraints, a Root PE is
obliged to receive its own data as well.

After sending its local data, a Root PE sends a com-
mand to its local router and the following 4 routers.
In eﬀect, this routing update shifts the communication
pattern by one position: the ﬁrst neighbor now becomes
a Root in the next step of the broadcast pattern. After 5
steps (and 5 shifts), a PE has sent its data out and has
received data from its 4 neighbors. In Figure 4b, the target
PE receives data from the West during the ﬁrst 4 steps,
and sends its data to the East at step 5.

One of the very important aspects of this is that chang-
ing the routing on a remote router does not require any

Fig. 2: An overview of the Wafer Scale Engine (WSE). The WSE (to the right) occupies an entire wafer, and is a
2D array of dies. Each die is itself a grid of tiles (in the middle), which contains a router, a processing element and
single-cycle access memory (to the left). In total, the WSE-2 embeds 2.6 trillion transistors in a silicon area of 46,225
mm2 .

M
E
M
O
R
Y

nz

F

A

B

ny

R

I

C

FABRIC

nx

(a) 3D grid of size nx × ny × nz. X
and Y dimensions are mapped onto
the PE grid of the WSE.

4

b

4

nz

(b) Column of cells
stored in local mem-
ory, extended by 8
cells. Operations are
carried out by blocks
of size b.

Fig. 3: Computing pattern mapping

action from the local PE. It is therefore uninterrupted
and can perform computations simultaneously. Another
advantage is that all the control
logic is encapsulated
in this routing update. A particular PE has to do two
things only: sending b cells out, and receiving 5 × b cells
(from 4 neighbors and itself). The router conﬁguration
will determine when the data ﬂows in or out of a given
router. Once a PE is notiﬁed that its data has been sent
out, it sends a router command to update the routing and
transition to the next step of the broadcast pattern. There
is no bookkeeping required to determine whether a PE is
in a given position in a broadcast.

2) Stencil computation over the X and Y dimensions:
In order to compute the stencil over the X and Y axes,
communications between PEs are required. As the stencil
involved in this application is a 25-point stencil, data from

16 neighboring PEs along the X and Y directions must
be exchanged. This means that at each time step, a PE is
involved in 4 localized broadcast patterns (one per cardinal
direction). In each broadcast pattern, a PE sends its data
and receives data from 4 neighbors.

Using a FMUL instruction, incoming cells from a given
direction are multiplied “on the ﬂy” with coeﬃcients
depending on their respective distance to the local cell.
There are 4 FMUL operations happening concurrently
(one per cardinal direction). This is depicted as step 1 in
Figure 5 for the data coming from the West. Each FMUL
instruction operates on 5 × b incoming cells coming from
a particular cardinal direction, and the coeﬃcients (corre-
sponding to cxm and cym, ∀m ∈ {1 . . . 4} in Section III).
A given coeﬃcient is applied to b consecutive cells are they
are coming from the same distance neighbor.

Since a PE is receiving data from itself, it is advan-
tageous to compute the contribution from the center
cellx,y,z during this step. This is done during the FMUL
operation that processes the cells coming from the West,
by multiplying the cells coming from the same PE with
cx0 + cy0 + cz0. FMULs operating on cells coming from all
other directions use a coeﬃcient of 0 for the data coming
from the same PE.

Once this distributed computation phase is complete,
the data of size 4 × 5 × b is reduced into a single buﬀer of b
cells (which is referred to as accumulator) using a FADD
instruction (step 2 in Figure 5). The dimension of size 4
corresponds to the number of localized broadcast patterns
a PE participates in, 5 corresponds to the number of PE it
is receiving from, and b is the number of elements coming
from each PE. All contributions of neighboring cells along
the X and Y dimensions are contained in the accumulator
buﬀer after the reduction.

WaferScale EngineSingle dieSingle tileRouterControlSchedulerxi[i]wyyDSRfleFMACMemoryNSEWNSEWCore tiles tiles7 dies12 dies(a) WSE-2 router conﬁgurations used
by Finite Diﬀerences. Conﬁguration 0
corresponds to the conﬁguration of the
Root of a broadcast, conﬁguration 1 is
used by PEs in the middle, conﬁgura-
tion 2 is used by the Last PE.

(b) 5 communication steps required to fetch all the data required by a target PE
from the West (steps 1 through 4) and to send its data to the East (step 5).
Corresponding router conﬁgurations are given in the circled numbers. At each step, a
router command is sent through the broadcast pattern, changing the conﬁgurations
of each set of 5 routers.

Fig. 4: Eastward localized broadcast operation used in Finite Diﬀerences to exchange cells along the X dimension.

z

z − 4

z − 2

z − 3

z − 1

(a)

(b)

(c)

(d)

Fig. 6: Applying the stencil over the Z dimension

(corresponding to discretization parameters czm, ∀m ∈
1 . . . 4). The result of each FMAC is placed into the
accumulator buﬀer (which also contains the contributions
from the X and Y dimensions). Given a target block of
size b starting at coordinate zb, each FMAC takes an input
block starting at index zb + oﬀset and multiplies it by a
coeﬃcient. The oﬀset values are {0, 1, 2, 3} and {5, 6, 7, 8},
and corresponding coeﬃcients are {cz4, cz3, cz2, cz1} and
{cz1, cz2, cz3, cz4}. The CSL code is provided in Figure 7
and the ﬁrst 4 steps of this process are illustrated in
Figure 6. As can be seen, this step skips oﬀset 4, which
would correspond to the multiplication by cz0, since that
particular computation has already been done as discussed
earlier. At the end of this step, the Laplacian is contained
in the accumulator buﬀer.

D. Time integration

Fig. 5: A summary of main operations: computing the
stencil over the X and Y dimensions (for each cardinal
direction), reducing the accumulator buﬀer, and subtract-
ing the accumulator from the waveﬁeld.

3) Stencil computation over the Z dimension: After
remote contributions to the Laplacian from the X and
Y axes of the grid have been accumulated, contributions
from Z can be computed. Given the problem mapping
over the WSE, this means that, at each time step, the
computation over the Z dimension can be performed in
an embarrassingly parallel fashion since this dimension
resides entirely in the memory of a PE.

Each PE executes 8 FMACs instructions of length b,
multiplying the waveﬁeld by one of the 8 coeﬃcients

Once the Laplacian has been computed, the time itera-
tion step given in Equation 2 can happen. The waveﬁeld

012102110130031021120130031021120130031021120130031021120000111111111111Step 1Step 2Step 3Step 4Step 5PE indexCommandCommandCommandCommandTargetPEAccumulatorNORTHFMULCells comingfrom fabric   (WEST)EASTCoefficientsBufferFSUB3FADD21Processingdatafrom WestSOUTHaccumulatorzb≤i<zb+b = accumulatorzb≤i<zb+b
+zWFzb−4≤i<zb+b−4 × cz4
+zWFzb−3≤i<zb+b−3 × cz3
+zWFzb−2≤i<zb+b−2 × cz2
+zWFzb−1≤i<zb+b−1 × cz1
+zWFzb+1≤i<zb+b+1 × cz1
+zWFzb+2≤i<zb+b+2 × cz2
+zWFzb+3≤i<zb+b+3 × cz3
+zWFzb+4≤i<zb+b+4 × cz4

(a) Operations performed along the Z dimension.
zWF is the waveﬁeld stored in the local memory of
a PE

const accumDsd = @get_dsd ( mem1d_dsd , .{

. tensor_access = | i |{ nz } -> accumulator [ i ]}) ;

const srcZ0 = @get_dsd ( mem1d_dsd , .{

. tensor_access = | i |{ nz } -> zWF [0 , i ]}) ;

@fmacs ( accumDsd , accumDsd , srcZ0 , coefficients [0]) ;
const srcZ1 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 1 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ1 , coefficients [1]) ;
const srcZ2 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 2 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ2 , coefficients [2]) ;
const srcZ3 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 3 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ3 , coefficients [3]) ;
// srcZ4 not used : update is done in Eastwards broadcast
const srcZ5 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 5 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ5 , coefficients [5]) ;
const srcZ6 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 6 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ6 , coefficients [6]) ;
const srcZ7 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 7 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ7 , coefficients [7]) ;
const srcZ8 = @ i n c r e m e n t _ d s d _ o f f s e t ( srcZ0 , 8 , f32 ) ;
@fmacs ( accumDsd , accumDsd , srcZ8 , coefficients [8]) ;

(b) Equivalent CSL code. Each @fmacs instruction takes an output
argument and three input arguments. @get_dsd returns a descriptor,
corresponding to a view of an array. @increment_dsd_offset allows to
oﬀset the array pointed by an existing descriptor.

Fig. 7: Applying the stencil along the Z dimension.

from the previous time step is added to the accumulator
buﬀer. In reality, this is also done during the stencil
computation: as mentioned earlier, a PE receives its own
data. Doing so allows to use cycles which would have
otherwise been wasted.

The next step is to update the waveﬁeld (per Equa-
tion 2), by subtracting the waveﬁeld to the accumulator
buﬀer (step 3 in Figure 5).

Next, a stimulus, called source, needs to be added to
a particular cell (with coordinates (srcX, srcY, srcZ)) at
each time step. The source value at the current time step
is added to the waveﬁeld at oﬀset srcZ on the PE with
coordinates (srcX, srcY).

V. Experimental Evaluation

In this section, experimental results of Finite Diﬀerences
running on a Wafer-Scale Engine are presented. The scala-
bility and energy eﬃciency achieved by Finite Diﬀerences
on this massively parallel platform are discussed.

A. Experimental Conﬁguration

The experiments are conducted on two platforms: a
Cerebras CS-2 equipped with a WSE-2 chip, and a GPU-
based platform used as a reference. The CS-2 is Cerebras’
second generation chassis, which uses the 7nm WSE-2
second generation Wafer-Scale Engine. The WSE-2 oﬀers
2.2× more processing elements than the original WSE.
The experiments used a fabric of size 755 × 994 out of
the total 850,000 processing elements of the WSE-2. The
CS-2 is driven by a Linux server on which no computations
take place in the context of this work. The WSE-2 platform
uses Cerebras SDK 0.3.0 [27].

and 256 GB of main memory. The GPU platform is using
CUDA 11.2 and GCC 8.3.1.

Numerical results produced by Finite Diﬀerences on
WSE-2 are compared to the results produced by Minimod.

B. Weak scaling Experiments

This section discusses scalability results of Finite Dif-
ferences on a WSE-2 system. In order to characterize
the scalability of Finite Diﬀerences, the grid dimension
is modiﬁed along the X and Y dimensions, while the
Z dimension (residing in memory) is kept constant to a
relevant value for this type of application. The X and Y
dimensions are grown up to a size of 755 × 994. Results
presented in Table II show the throughput achieved on
WSE-2 in Gigacell/s, the wall-clock time required to com-
pute 1,000 time steps on WSE-2, as well as timings on
a GPGPU provided as baseline. Timing reported in this
section correspond to computations taking place on the
device only, be it on GPU or WSE-2.

As can be seen in the table, for all problem sizes, the
wall-clock time required on WSE-2 is constant, meaning
that Finite Diﬀerences scales nearly perfectly on this plat-
form. It is crucial to observe that such a reduction in wall-
clock time has a signiﬁcant impact in practice since this
type of computations is repeated hundreds of thousands of
times in an industrial context. Finite Diﬀerences reaches
a throughput of 9872.78 Gcell/s on the largest problem
size, which is rarely seen at single system level. This type
of throughput is diﬃcult to achieve without using a large
number of nodes on distributed-memory supercomputers
due to limited strong scalability.

The GPU-based platform is Cypress from TotalEner-
gies. It has 4 NVIDIA A100 GPUs, each oﬀering 40 GB
of on-device RAM, a 16-core AMD EPYC 7F52 CPU,

Figure 8 depicts the ratio between the elapsed time
achieved by the A100-tuned kernel compared to Finite
Diﬀerences on WSE-2. As can be seen, when the largest

problem is solved (grid size of 755 × 994 × 1000), a
speedup of 228x is achieved. While this number shows
great potential, it is understood that using multiple GPUs
will likely narrow the gap. However, it is unlikely that such
a performance gap can be closed entirely, given the strong
scalability issues encountered by this kind of algorithm
when using a large number of multi-GPU nodes in HPC
clusters ( [1], [29]).

Finite Diﬀerences shows close to perfect weak scaling
on WSE-2. No matter what the grid size is, the run time
stays fairly stable. Taking the 200x200 case as a reference,
percentages of the ideal weak scaling for various grid sizes
are depicted in Figure 9. As can be seen in the plot,
Finite Diﬀerences systematically reaches over 98% of weak
scaling eﬃciency. This demonstrates how extremely low
latency interconnect coupled with local fast memories can
be eﬃciently leveraged by stencil applications relying on
a localized communication pattern.

In the next experiment, the sizes of the X and Y
dimensions of the grid are ﬁxed to nx = 755 and ny = 994
while the size of the Z dimension nz is varied from
100 to 1, 000. Results presented in Table III show that
the throughput increases slightly with nz. This indicates
that the implementation gains in eﬃciency due to larger
block sizes b and therefore lower relative overheads. More
importantly, it conﬁrms that memory accesses do not limit
the performance of the implementation, conﬁrming that it
is compute-bound.

nx

ny

nz

Throughput WSE-2
time [s]

Gcell/s

A100
time [s]

200
400
600

755
755
755
755
755

200
400
600

500
600
900
990
994

1000
1000
1000

1000
1000
1000
1000
1000

533.64
2097.60
4731.53

4956.17
5945.40
8922.08
9782.14
9862.78

0.0750
0.0763
0.0761

0.0762
0.0762
0.0762
0.0764
0.0761

0.7892
3.5828
8.0000

8.5499
10.1362
15.5070
17.4991
17.4186

TABLE II: Experimental results for 1,000 time steps for
various grid sizes with ﬁxed nz.

nz

b

Throughput WSE-2
time [s]

Gcell/s

Scaling

100
200
300
400
500
700
1000

100
200
300
400
250
350
334

8688.76
9303.26
9492.89
9614.15
9786.51
9885.04
9936.79

0.8637
1.6133
2.3716
3.1223
3.8342
5.3143
7.5524

1.0000
1.8679
2.7458
3.6151
4.4392
6.1531
8.7442

TABLE III: Experimental results, ﬁxed nx × ny grid
dimensions of 755 × 994. 100,000 time steps.

Fig. 8: Comparisons between implementation on WSE-2
and A100 using elapsed time describe in Table II. Fixed
nz = 1000.

Fig. 9: Weak scaling under assumption that PE memory
is fully utilized (nz = 1000).

C. Proﬁling data

In the following, various proﬁling results are provided
for Finite Diﬀerences, with the objective to provide general
insights on WSE-2-based computations.

Using Cerebras’ hardware proﬁling tool on a 600 ×
600 × 1000 grid, the execution of Finite Diﬀerences results
in an average of 69.6% busy cycles. The 4 PEs at the
corners of the grid have 0 busy cycles since they are not
doing any computation. There is an average of 11.6%
idle cycles caused by memory accesses. As expected, the
load is extremely balanced, with a standard deviation of
0.8%. This shows that the hardware is kept busy during
the experiment, further conﬁrming the eﬃciency of the

approach proposed in this work.

The power consumption of the CS-2 during a Finite
Diﬀerences run on a 755 × 994 × 1000 grid is reported
in Figure 10. In order to record a suﬃcient number of
samples, the run time is extended by setting the number
of time steps to 10,000,000, leading to a total run time of
754 seconds. The average power consumption during the
execution is 22.8 kW, which corresponds to 22 GFLOP/W.
Such an energy eﬃciency is hard to ﬁnd in the literature
for a stencil of this order. In addition to power consump-
tion, Figure 10 also depicts the coolant temperature of
the CS-2, which uses a closed-loop water-cooling system.
During the execution, the coolant temperature rises very
moderately from 23.6◦C to a peak of 25.6◦C.

Fig. 10: Power consumption during experiment with grid
755 × 994 × 1000 for 10,000,000 time steps, which amounts
to 754 seconds. In the plot, time is subsampled by 3.5
seconds. The power baseline is 16.1 kW and peak is 22.9
kW. Coolant temperature is also reported, with a baseline
of 23.6◦C and a peak of 25.6◦C.

Altogether, experiments show that the Finite Diﬀer-
ences algorithm presented in this study is able to exploit
low-latency distributed memory architectures such as the
WSE-2 with very high hardware utilization. The applica-
tion has near-perfect weak scalability and provides signif-
icant speedups over a reference GPGPU implementation
.

VI. Roofline model

A rooﬂine model [33] is a synthetic view of how many
ﬂoating point instructions per cycles can be done. This
is the peak compute capacity of the platform. However,
no computation can be done without loading data: the
number of 32 bit words that can be accessed per cycle
will also impact the peak compute rate. In the case of
a bandwidth-bound application, the memory will actually

Fig. 11: Rooﬂine models for WSE-2 and GPGPU-based
implementations (in log-log scale) for a 755 × 994 × 1000
grid. Dots represent Finite Diﬀerences implementations.
WSE-2 (top) has two distinct resources: memory and
fabric: the leftmost blue dot corresponds to memory ac-
cesses, while the red dot to the right corresponds to fabric
accesses. Rooﬂines are given using the same colors. The
kernel
is clearly in the compute-bound zone for both
memory and fabric accesses.
On GPGPU (bottom), red dots and lines correspond to
DRAM accesses. L1 cache accesses are depicted in blue.
The kernel is clearly in the bandwidth-bound zone.

102101100101102FLOP/Byte101102103104105106107GFLOPsMemory20.0 PB/sOff/onramp to fabric3.3 PB/s1.785 PFLOPsCerebras WSE-2NVIDIA A10019.5 TFLOPsL1 - 2573.8 GB/sDRAM - 1224.2 GB/sOperation
17 FMUL
17 FADD
8 FMA
1 FSUB

FLOP
1
1
2
1

Mem. traﬃc
1/b load, 1 store
2 loads, 1 store
2 + 1/(b) loads, 1 store
2 loads, 1 store

Fabric traﬃc
1 load
0
0
0

TABLE IV: Instruction and memory access counts of the
Finite Diﬀerences implementation on WSE-2

determine the performance of the overall application. In
a similar fashion, accesses to the interconnect can be
taken into account when the architecture is a distributed
memory platform. This ratio between ﬂoating point op-
erations (i.e. the “useful” operations) and the volume of
data coming from/going to a given resource, for instance
memory, is referred to as arithmetic intensity (measured
in FLOP/byte).

The WSE-2 is a SIMD-centric multiprocessor providing
up to 4 simultaneous 16 bit ﬂoating point instructions per
cycle. In the context of this work, only 32 bit ﬂoating
point operations are used. The peak number of ﬂoating
point operations per second (FLOPs) is represented by
the horizontal line at the top of Figure 11.

The WSE-2 does not have a complex cache memory
hierarchy: each PE has a single local memory accessed
directly. Four 32 bit packets can be accessed from memory
per cycle, and up to two packets can be stored to memory
per cycle. In memory intensive application, memory limits
the peak achievable performance. This corresponds to the
slanted blue line at the top of Figure 11.

Each PE is connected to its router by a bi-directional
link able to move 32 bits per cycle in each direction
(referred to as “oﬀ/onramp bandwidth” in the plot). The
router is connected to other routers by 4 bi-directional
links, each moving 32 bits per cycle. This corresponds to
the slanted red line at the top of Figure 11.

For each cell, the stencil computation in Finite Dif-
ferences involves 25 multiplies and 25 adds. In addition
to that, Finite Diﬀerences requires a subtraction between
the previous time step and the current time step. This
corresponds to a total of 51 ﬂoating point instructions
per cell. As explained in Section IV, due to architecture
constraints, only the computation of the stencil along
the Z dimension can be done using FMAs. For the X
and Y dimensions, the implementation relies on separate
FMUL and FADD instructions. The ﬁnal value of the
current time step is computed using a FSUB operation.
A summary of the instructions used in Finite Diﬀerences,
ﬂoating point operations per cell, memory traﬃc, fabric
traﬃc, and instruction count per cell is given in Table IV.
On WSE-2, Finite Diﬀerences computes 57 ﬂoating
point operations per cell, 51 of which are required by
the algorithm. Extra operations are due to hardware
constraints. The number of operations strictly required by
the algorithm is used in all performance numbers. These
51 ﬂoating point operations require 112 load and store of
32 bit words from/to memory, and 17 loads from fabric.

This leads to an arithmetic intensity of 0.11 with respect to
memory accesses, and 0.75 with respect to fabric transfers.
On WSE-2, a 755 × 994 × 1000 grid is computed in
0.0761s (see Table II). This leads to a ﬂop rate of 670.3
MFLOPs per PE, and an aggregated performance of
503 TFLOPs for the entire grid of PEs used by this
problem size. The rooﬂine model of the WSE-2, depicted
in Figure 11(top),
indicates that Finite Diﬀerences is
compute bound thanks to the extremely fast local memory.
This is quite remarkable, and conﬁrms the weak scaling
results given in Section V. The application is communi-
cation/memory bound on most architecture, such as the
GPU platform used in this study (rooﬂine model depicted
in Figure 11(bottom)). Note that diﬀerent optimizations
lead to diﬀerent arithmetic intensities.
VII. Conclusion

In this work, a Finite Diﬀerences algorithm taking
advantage of low-latency localized communications and
ﬂat memory architecture has been introduced. Localized
broadcast patterns are introduced to exchange data be-
tween processing elements and fully utilize the intercon-
nect. Experiments show that it is possible to reach near
perfect weak scalability on distributed memory architec-
ture such as the WSE-2. On this platform, the implemen-
tation of Finite Diﬀerences reaches 503 TFLOPs. This
is a remarkable throughput for this stencil order on a
single node machine. The rooﬂine model introduced in this
work conﬁrms that Finite Diﬀerences becomes compute-
bound on the WSE-2. This demonstrates the validity
and potential of the approach presented in this work,
and demonstrate how diﬀerent hardware architectures like
the WSE-2 can be exploited eﬃciently by stencil-based
applications.

Future eﬀorts include the integration of the ported
kernel at the center of more ambitions applications, such
as the ones regularly used by seismic modeling experts
when taking real-life decisions. Further, given the well
established capacity of this hardware architecture for ML-
based applications, a hybrid HPC-ML approach will also
be investigated.

One interesting consequence of having a relatively com-
pact machine delivering such a high performance level for
this type of application is that seismic data processing can
happen at the same time it is acquired on the ﬁeld, which
is key when constant monitoring is required. Furthermore,
under this scenario, processing capacity can move from
data centers closer to where sensors are, namely target
edge-HPC.

Acknowledgements

Authors would like to thank Cerebras Systems and
TotalEnergies for allowing to share the material. Also,
authors would like to acknowledge Grace Ho, Ashay Rane,
and Natalia Vassilieva from Cerebras for the contributions,
and Ruychi Sai from Rice U. for fruitful discussions about
GPGPU optimized kernels.

References

[1] O. Anjum, M. Almasri, S. de Gonzalo, and W. Hwu, “An
eﬃcient gpu implementation and scaling for higher-order 3d
stencils,” Information Sciences, vol. 586, pp. 326–343, Mar.
2022.

[2] M. Araya-Polo, J. Cabezas, M. Hanzich, M. Pericas, F. Rubio,
I. Gelado, M. Shaﬁq, E. Morancho, N. Navarro, E. Ayguade,
J. M. Cela, and M. Valero, “Assessing accelerator-based hpc
reverse time migration,” IEEE Transactions on Parallel and
Distributed Systems, vol. 22, no. 1, pp. 147–162, 2011.

[3] M. Araya-Polo, F. Rubio, R. de la Cruz, M. Hanzich, J. M.
Cela, and D. P. Scarpazza, “3d seismic imaging through reverse-
time migration on homogeneous and heterogeneous multi-core
processors,” Scientiﬁc Programming, vol. 17, no. 1-2, pp. 185–
198, 2009.

[4] Cerebras, “Wafer-scale deep learning,” in 2019 IEEE Hot
Chips 31 Symposium (HCS). Los Alamitos, CA, USA: IEEE
Computer Society, aug 2019, pp. 1–31.
[Online]. Available:
https://doi.org/10.1109/HOTCHIPS.2019.8875628

[5] K. Datta, S. Kamil, S. Williams, L. Oliker, J. Shalf,
and K. Yelick, “Optimization and performance modeling
of stencil computations on modern microprocessors,” SIAM
Review, vol. 51, no. 1, pp. 129–159, 2009. [Online]. Available:
https://doi.org/10.1137/070693199

[6] R. de la Cruz and M. Araya-Polo, “Towards a multi-level cache
performance model for 3d stencil computation,” Procedia Com-
puter Science, vol. 4, pp. 2146 – 2155, 2011, proceedings of the
International Conference on Computational Science, ICCS 2011.
[7] R. de la Cruz and M. Araya-Polo, “Algorithm 942: Semi-

stencil,” ACM Trans. Math. Softw., vol. 40, no. 3, 2014.

[8] M. Frigo and V. Strumpen, “Cache oblivious stencil computa-
tions,” in Proceedings of the 19th Annual International Confer-
ence on Supercomputing, ser. ICS ’05. New York, NY, USA:
Association for Computing Machinery, 2005, p. 361–366.

[9] S. Ghosh, T. Liao, H. Calandra, and B. M. Chapman, “Experi-
ences with OpenMP, PGI, HMPP and OpenACC directives on
ISO/TTI kernels,” in 2012 SC Companion: High Performance
Computing, Networking Storage and Analysis, Nov 2012, pp.
691–700.

[10] F. G¨urkaynak and J. Kr¨uger, “Stx – stencil/tensor accelerator
factsheet,” https://www.european-processor-initiative.eu/wp-
content/uploads/2019/12/EPI-Technology-FS-STX.pdf, 2019.
[11] T. Gysi, C. M¨uller, O. Zinenko, S. Herhut, E. Davis, T. Wicky,
O. Fuhrer, T. Hoeﬂer, and T. Grosser, “Domain-speciﬁc multi-
level ir rewriting for gpu,” arXiv:2005.13014, 2020.

[12] B. Kahle

and W. Hillis, The Connection Machine
Model CM-1 Architecture, ser. Technical report (Thinking
Machines Corporation). Thinking Machines Corporation,
1989. [Online]. Available: https://books.google.com/books?id=
PCq7uAAACAAJ

[13] S. Kronawitter and C. Lengauer, “Polyhedral search space
exploration in the exastencils code generator,” ACM Trans.
[Online].
Archit. Code Optim., vol. 15, no. 4, oct 2018.
Available: https://doi.org/10.1145/3274653

[14] J. Krueger, D. Donofrio, J. Shalf, M. Mohiyuddin, S. Williams,
L. Oliker, and F.-J. Pfreund, “Hardware/software co-design
for energy-eﬃcient seismic modeling,” in Proceedings of 2011
International Conference for High Performance Computing,
Networking, Storage and Analysis, ser. SC ’11. New York, NY,
USA: Association for Computing Machinery, 2011. [Online].
Available: https://doi.org/10.1145/2063384.2063482

[15] S. Lie, “Multi-million core, multi-wafer ai cluster,” in 2021 IEEE
IEEE Computer Society,

Hot Chips 33 Symposium (HCS).
2021, pp. 1–41.

[16] M. Louboutin, M. Lange, F. Luporini, N. Kukreja, P. A. Witte,
F. J. Herrmann, P. Velesko, and G. J. Gorman, “Devito (v3.1.0):
an embedded domain-speciﬁc language for ﬁnite diﬀerences
and geophysical exploration,” Geoscientiﬁc Model Development,
vol. 12, no. 3, pp. 1165–1187, 2019.

[17] K. Matsumura, H. R. Zohouri, M. Wahib, T. Endo, and
S. Matsuoka, “An5d: Automated stencil framework for high-
degree temporal blocking on gpus,” in Proceedings of the 18th
ACM/IEEE International Symposium on Code Generation and

Optimization, ser. CGO 2020. New York, NY, USA: Associa-
tion for Computing Machinery, 2020, p. 199–211.

[18] J. Meng, A. Atle, H. Calandra,

and M. Araya-
for Seismic
Polo, “Minimod: A Finite Diﬀerence solver
Modeling,” arXiv:2007.06048v1, Jul. 2020. [Online]. Available:
https://arxiv.org/abs/2007.06048v1

[19] P. Moczo, J. Kristek, and M. G´alis, The Finite-Diﬀerence Mod-
elling of Earthquake Motions: Waves and Ruptures. Cambridge
University Press, 2014.

[20] A. Nguyen, N. Satish, J. Chhugani, C. Kim, and P. Dubey, “3.5-
d blocking optimization for stencil computations on modern
cpus and gpus,” in SC ’10: Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Computing,
Networking, Storage and Analysis, 2010, pp. 1–13.

[21] A. Qawasmeh, M. R. Hugues, H. Calandra, and B. M. Chapman,
“Performance portability in reverse time migration and seismic
modelling via openacc,” The International Journal of High
Performance Computing Applications, vol. 31, no. 5, pp. 422–
440, 2017.

[22] P. S. Rawat, M. Vaidya, A. Sukumaran-Rajam, A. Rountev,
L. Pouchet, and P. Sadayappan, “On optimizing complex sten-
cils on GPUs,” in 2019 IEEE International Parallel and Dis-
tributed Processing Symposium (IPDPS), 2019, pp. 641–652.

[23] G. Rivera and C.-W. Tseng, “Tiling optimizations for 3d sci-
entiﬁc computations,” in Proceedings of the 2000 ACM/IEEE
USA: IEEE
Conference on Supercomputing, ser. SC ’00.
Computer Society, 2000, p. 32–es.

[24] K. Rocki, D. Van Essendelft, I. Sharapov, R. Schreiber, M. Mor-
rison, V. Kibardin, A. Portnoy, J. F. Dietiker, M. Syamlal,
and M. James, “Fast stencil-code computation on a wafer-
scale processor,” in Proceedings of the International Conference
for High Performance Computing, Networking, Storage and
Analysis, 2020, pp. 1–14.

[25] R. Sai, J. Mellor-Crummey, X. Meng, M. Araya-Polo, and
J. Meng, “Accelerating high-order stencils on GPUs,” in 2020
IEEE/ACM Performance Modeling, Benchmarking and Simu-
lation of High Performance Computer Systems (PMBS). Los
Alamitos, CA, USA: IEEE Computer Society, nov 2020, pp. 86–
108.

[26] R. Sai, J. Mellor-Crummey, X. Meng, K. Zhou, M. Araya-
Polo, and J. Meng, “Accelerating high-order stencils on gpus,”
Concurrency and Computation: Practice and Experience, vol.
e6467, 2021.

[27] J. Selig, “The cerebras software development kit: A technical
overview,” https://f.hubspotusercontent30.net/hubfs/8968533/
Cerebras%20SDK%20Technical%20Overview%20White%
20Paper.pdf, 2022.

[28] D. E. Shaw, P. J. Adams, A. Azaria, J. A. Bank, B. Batson,
A. Bell, M. Bergdorf, J. Bhatt, J. A. Butts, T. Correia,
R. M. Dirks, R. O. Dror, M. P. Eastwood, B. Edwards,
A. Even, P. Feldmann, M. Fenn, C. H. Fenton, A. Forte,
J. Gagliardo, G. Gill, M. Gorlatova, B. Greskamp, J. Grossman,
J. Gullingsrud, A. Harper, W. Hasenplaugh, M. Heily, B. C.
Heshmat, J. Hunt, D. J. Ierardi, L. Iserovich, B. L. Jackson,
N. P. Johnson, M. M. Kirk, J. L. Klepeis, J. S. Kuskin, K. M.
Mackenzie, R. J. Mader, R. McGowen, A. McLaughlin, M. A.
Moraes, M. H. Nasr, L. J. Nociolo, L. O’Donnell, A. Parker,
J. L. Peticolas, G. Pocina, C. Predescu, T. Quan, J. K. Salmon,
C. Schwink, K. S. Shim, N. Siddique, J. Spengler, T. Szalay,
R. Tabladillo, R. Tartler, A. G. Taube, M. Theobald, B. Towles,
W. Vick, S. C. Wang, M. Wazlowski, M. J. Weingarten, J. M.
Williams, and K. A. Yuh, “Anton 3: Twenty microseconds of
molecular dynamics simulation before lunch,” in Proceedings of
the International Conference for High Performance Computing,
Networking, Storage and Analysis, ser. SC ’21. New York, NY,
USA: Association for Computing Machinery, 2021. [Online].
Available: https://doi.org/10.1145/3458817.3487397

[29] T. Shimokawabe, T. Endo, N. Onodera, and T. Aoki, “A stencil
framework to realize large-scale computations beyond device
memory capacity on gpu supercomputers,” in 2017 IEEE Inter-
national Conference on Cluster Computing (CLUSTER), 2017,
pp. 525–529.

[30] H. Stengel, J. Treibig, G. Hager, and G. Wellein, “Quantifying
performance bottlenecks of stencil computations using the

execution-cache-memory model,” in Proceedings of the 29th
ACM on International Conference on Supercomputing, ser.
ICS ’15. New York, NY, USA: Association for Computing
[Online]. Available: https:
Machinery, 2015, p. 207–216.
//doi.org/10.1145/2751205.2751240

[31] Z. team, “Zig programming language,” https://ziglang.org/,

2018, (Accessed on 03/18/2022).

[32] F. Thaler, S. Moosbrugger, C. Osuna, M. Bianco, H. Vogt,
A. Afanasyev, L. Mosimann, O. Fuhrer, T. C. Schulthess, and
T. Hoeﬂer, “Porting the cosmo weather model to manycore
cpus,” in Proceedings of the Platform for Advanced Scientiﬁc
Computing Conference, ser. PASC ’19. New York, NY,
USA: Association for Computing Machinery, 2019. [Online].
Available: https://doi.org/10.1145/3324989.3325723

[33] S. Williams, A. Waterman, and D. Patterson, “Rooﬂine: An in-
sightful visual performance model for multicore architectures,”
Commun. ACM, vol. 52, no. 4, p. 65–76, apr 2009. [Online].
Available: https://doi.org/10.1145/1498765.1498785

[34] D. Wonnacott, “Using time skewing to eliminate idle time due
to memory bandwidth and network limitations,” in Proceedings
14th International Parallel and Distributed Processing Sympo-
sium. IPDPS 2000.

IEEE, 2000, pp. 171–180.

[35] K. Zhang, H. Su, P. Zhang, and Y. Dou, “Data layout trans-
formation for stencil computations using arm neon extension,”
in 2020 IEEE 22nd International Conference on High Perfor-
mance Computing and Communications; IEEE 18th Interna-
tional Conference on Smart City; IEEE 6th International Con-
ference on Data Science and Systems (HPCC/SmartCity/DSS),
2020, pp. 180–188.

