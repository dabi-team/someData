Software Fairness: An Analysis and Survey

EZEKIEL SOREMEKUN, MIKE PAPADAKIS, MAXIME CORDY, and YVES LE TRAON, SnT, Univer-

sity of Luxembourg, Luxembourg

ABSTRACT In the last decade, researchers have studied fairness as a software property. In particular, how to engineer fair software

systems? This includes specifying, designing, and validating fairness properties. However, the landscape of works addressing bias as a

software engineering concern is unclear, i.e., techniques and studies that analyze the fairness properties of learning-based software. In

this work, we provide a clear view of the state-of-the-art in software fairness analysis. To this end, we collect, categorize and conduct

in-depth analysis of 164 publications investigating the fairness of learning-based software systems. Speciﬁcally, we study the evaluated

fairness measure, the studied tasks, the type of fairness analysis, the main idea of the proposed approaches and the access level (e.g.,

black, white or grey box). Our ﬁndings include the following: (1) Fairness concerns (such as fairness speciﬁcation and requirements

engineering) are under-studied; (2) Fairness measures such as conditional, sequential and intersectional fairness are under-explored; (3)

Unstructured datasets (e.g., audio, image and text) are barely studied for fairness analysis; and (4) Software fairness analysis techniques

hardly employ white-box, in-processing machine learning (ML) analysis methods. In summary, we observed several open challenges

including the need to study intersectional/sequential bias, policy-based bias handling and human-in-the-loop, socio-technical bias

mitigation.

Additional Key Words and Phrases: Software Fairness, Software Analysis, Bias, Discrimination, Artiﬁcial intelligence, Machine learning

ACM Reference Format:

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon. 2022. Software Fairness: An Analysis and Survey. 1, 1

(May 2022), 34 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION

Software fairness is a property of learning-based systems which aims to ensure that the software does not exhibit

biases [131]. Given a set of inputs, a fair software should not result in discriminatory outputs or behaviors for inputs

relating to certain groups or individuals. In essence, the goal is to ensure that software systems exhibit fair behavior for

all inputs that are similar for the task-at-hand. For instance, discriminatory inputs, inputs that are similar for a task but

only diﬀer in sensitive attributes (e.g., gender, race or age), should produce similar outputs or induce similar program

behaviors [56].

Speciﬁcally, the goal of software fairness analysis is to ensure a given system produces the same results or exhibit

similar behaviors for a number of discriminatory inputs. This is important to detect, expose, diagnose and mitigate

bias (i.e., discrimination) in software systems. As an example, consider a sentiment analyzer software that determines

the emotional state or situation in a text, which outputs either a positive emotion (e.g., text describing excitement) or

negative emotion (e.g., text portraying sadness or anger). Figure 1 shows an example of a bias in such a system, where

Authors’ address: Ezekiel Soremekun, ezekiel.soremekun@uni.lu; Mike Papadakis, michail.papadakis@uni.lu; Maxime Cordy, maxime.cordy@uni.lu; Yves
Le Traon, Yves.LeTraon@uni.lu, SnT, University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, Luxembourg, Luxembourg, Luxembourg, L-1359.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org.

© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

Manuscript submitted to ACM

1

2

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

the output (sentiment) is diﬀerent when the gender of the noun in the text is a “man” compared to a “woman”. This is

an illustrative example of (gender) bias found in real-world natural language processing (NLP) systems [11, 95, 121].

Several researchers have studied software fairness analysis, with the aim to address a fundamental question: How to

engineer fair software systems? Such works consider fairness as a non-functional software property and a software

engineering (SE) concern. This includes work studying how to specify [14, 117], test [56], and mitigate [34] fairness

properties in software systems. However, despite several works on software fairness analysis, it is diﬃcult to understand

the state of research practice: Speciﬁcally, what fairness concerns have been addressed? How have they been addressed?

What are the open problems and challenges?

In this paper, we aim to provide a clear view of the state-of-the-art in software fairness analysis, i.e., techniques

and studies that analyse the fairness properties of learning-based software. This paper aims to analyze the trends in

software fairness analysis, the available techniques, the focus of the research community, the problems that have been

addressed and the open research problems. To this end, we perform a systematic analysis of the literature, where we

studied 164 papers studying fairness as a software property. These papers are mostly published in fairness-related or

software engineering (SE) venues, as well as venues focused on machine learning (ML), artiﬁcial intelligence (AI),

security, computer vision (CV) and natural language processing (NLP). Particularly, we conduct an in-depth study of

the set of publications that explore fairness with the lens of software engineering, e.g., in terms of software quality

control, requirement engineering, design and development. We then characterize several factors encapsulated in these

research papers, including the evaluated fairness measure (e.g., individual, group, causal or conditional fairness), the

studied tasks (e.g., credit rating, CV, NLP), the type of fairness analysis (e.g., testing or mitigation), the main idea of the

proposed approach and the level of access (e.g., black, white or grey box).

Overall, we observe that the research community is facing several open challenges in addressing the following

fairness concerns: compounding or intersectional bias, veriﬁcation of fairness properties, sequential bias, equity-based

bias handling and socio-technical solutions to bias. The key ﬁndings of this work includes the following:

● Software Fairness analysis is mostly performed to validate or mitigate biases, i.e., the focus of the community has
been to test and reduce unfair program behaviors, other fairness concerns (such as requirements engineering

and veriﬁcation) are under-studied;

● The most studied fairness measure include individual, group and causal fairness, measures such as conditional,

sequential and intersectional fairness remain under-explored;

● The most examined tasks involve structured datasets (such as the adult income dataset), while unstructured

datasets (e.g., audio, image and text) are barely studied in the SE community;

● The most employed techniques in the SE community are mutation (e.g., input perturbation) and speciﬁcation
(e.g., using input templates, schemas or grammars) -based techniques, other approaches such as in-processing

machine learning (ML) analysis methods are hardly employed for software fairness analysis;

● Most approaches support the analysis of an atomic attribute (e.g. race, or gender), very few approaches study the
combination or sequence of attributes (e.g., race × gender), the compounding eﬀect of multiple instances of an
attribute, or complex attributes (such as non-binary gender);

● We found little or very few works tackling the fairness concerns like fairness test metrics/adequacy, automatic

repair of biased classiﬁers or time-based fairness concerns (e.g., sequential or regression fairness bugs).

The rest of this paper is structured as follows: We provide background on software fairness in section 2. We discuss

the process of collecting/analyzing publications in section 3, and section 4 highlights our research questions. In section 5,

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

5

properties in learning-based systems. Besides, this paper (provides a comprehensive view of the literature w.r.t. to SE,

involving signiﬁcantly more (SE-related) publications than the aforementioned specialized surveys. In the following we

discuss the closely related work to this paper, in particular the published surveys in this area.

There are a few domain-speciﬁc surveys of fairness where researchers have surveyed fairness issues in a speciﬁc

sub-domain of learning-based software, e.g., recommendation systems [45], ranking-based systems [139], sequential

decision systems [146], NLP [23], CV [46], or ﬁnancial services [106]. Blodgett et al. [23] comprehensively analyzed 146

papers addressing fairness of NLP systems, especially quantitative techniques for measuring or mitigating bias. The

authors found that almost all surveyed papers are poorly matched to their motivations and do not engage with the

relevant literature outside of NLP. The authors recommend that fairness analysis in NLP systems should be based on

characterizing system behaviors that are harmful, by centering bias mitigation around people and their experiences.

Fabbrizzi et al. [46] provides a survey on bias in visual datasets, i.e., for CV applications. The authors describe diﬀerent

biases in visual datasets, the proposed methods for detecting and measuring these biases and existing bias-aware

datasets. The authors concluded that there is no bias-free dataset and detecting biases in visual datasets is an open

problem and recommend a checklist to help practitioners spot diﬀerent biases in their dataset, in order to make

bias explicit. Meanwhile, Zehlike et al. [139] studied the application of fairness-enhancing interventions in ranking

algorithms, especially examining the literature on incorporating fairness requirements into algorithmic rankers. The

authors surveyed papers from several venues, including data management, algorithms, information retrieval, and

recommendation systems. The goal of the survey is to provide a new framework that uniﬁes fairness mitigation

objectives and ranking requirements, such that it allows to examine the trade-oﬀ between both goals. Ekstrand et al.

[45] studied how algorithmic fairness issues applies to information retrieval and recommendation systems, especially

addressing how to translate algorithmic fairness from classiﬁcation, scoring, and ranking settings into recommendation
and information retrieval settings.1 In addition, Zhang and Liu [146] provides a literature review of fairness in sequential
learning-based decision-making systems, i.e., systems where decision-making are not a one time event, but rather occur

in a sequential nature, such that decisions made in the past may have an impact on future data. Unlike these papers, this

work is not domain-speciﬁc, instead, we study the problem of fairness analysis across several (sub-)domains, including

all of the aforementioned domains.

Moreover, other surveys on fairness are bias-speciﬁc or metric-speciﬁc, they either focus on a speciﬁc sensitive

attribute (e.g., race) [51], or a speciﬁc fairness metric (e.g., causal fairness) [97], respectively. For instance, Field et al. [51]

provides a survey of race-related bias in the stages of NLP model development. The authors surveyed 79 papers with

the goal of understanding the gaps between race-related bias analysis in NLP and other related ﬁelds. The authors found

that race has been siloed as a niche topic in the NLP community and often ignored in many NLP tasks. The authors

also emphasize the need for racial inclusion in NLP research. In addition, Makhlouf et al. [97] study the application

of causality to address the problem of fairness, by studying papers that examine causal fairness properties and their

applicability in real-world scenarios. The authors employed identiﬁability theory to determine the criteria for the

real-world applicability of causal fairness. However, in this work, we examine papers examining several sensitive

attributes, biases and fairness metrics. In particular, unlike these metric or bias -speciﬁc surveys, we do not focus on

papers examining a single type of bias or fairness metric. Instead, we evaluate the literature across diﬀerent biases and

fairness metrics, including race and causal fairness, respectively.

1https://fair-ia.ekstrandom.net/

Manuscript submitted to ACM

6

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

A few researchers have conducted surveys of the literature on software fairness, albeit mostly targeting fair prediction

in machine learning [31, 55, 98, 101], general ML testing [142] or fairness notions across domains [68] or in speciﬁc

domains, e.g., concurrent systems [85]. For instance, Gajane and Pechenizkiy [55] studied the formalization of fairness in

the machine learning literature for prediction tasks, especially examining how this relates to the notions of distributive

justice in the social science literature. In their study, the authors proposed that two notions of distributive justice be

formalised for fairness in ML, namely equality of resources and equality of capability of functioning. Likewise, Ntoutsi

et al. [101] provides an introductory survey on the technical challenges and available solutions to bias in data-driven

AI, with a focus on the legal grounds for bias challenges and the societal implications of these solutions. Mehrabi

et al. [98] examined the fairness issues in real-world applications, speciﬁcally investigating the diﬀerent sources of

bias in AI systems and providing a taxonomy of fairness deﬁnitions in the ML community. Similarly, Caton and Haas

[31] provide an overview of fairness mitigation approaches for ML, by categorising mitigation techniques into several

stages in the ML model development pipeline. The authors highlight 11 mitigation methods categorised into three

areas, namely pre-processing, in-processing, and post-processing methods. In addition, Zhang et al. [142] surveyed

the testing of several properties in machine learning (ML), including fairness properties. In contrast to Zhang et al.

[142], we investigate fairness improvement beyond automated testing approaches, for instance, we also examine papers

examining its formalization, empirical evaluation, detection and improvement. Meanwhile Hutchinson and Mitchell [68]

and [85] studied the notions of fairness properties across domains, and for concurrent systems, respectively. Notably,

Hutchinson and Mitchell [68] surveyed the history of the deﬁnitions of fairness properties over the last 50 years across

multiple disciplines, including education, hiring, and machine learning. The authors compared past and current notions

of fairness along several dimensions, including the fairness criteria, the purpose of the criteria (e.g., testing) and how

it relates to the mathematical method for measuring fairness (e.g., classiﬁcation, regression) and people (individuals,

groups, and subgroups). Unlike the aforementioned works, our survey of fairness is more general, we study software

fairness beyond advances in specialised ML communities. In this work, we additionally examine several papers from

security, CHI, PL, CV and NLP venues.

Overall, we provide a systematic literature review of the analysis of software fairness properties for learning-based

systems. To the best of our knowledge, this work is the only systematic literature review concerned with the analysis of

fairness a (non-functional) software property of learning-based systems. We are not aware of any other survey that focuses

on this research area, i.e., software fairness analysis, especially providing a comprehensive survey of its formalization,

testing, diagnosis and mitigation across several fairness metrics, biases and domains.

3 METHODOLOGY

The research methodology employed in this work is based on the methodology detailed in Kitchenham [83]. In the

following, we provide the details of our research protocol:

(1) Aim and Scope: First, we deﬁne the goals of this work and the scope of works to be examined. Then, we deﬁne

the scientiﬁc questions, the analysis protocol and the information relevant for our data analysis. To this end, we

deﬁne the research questions (see section 4).

(2) Detailed Information: To analyse each paper in-depth, we identify the information necessary to answer all

research questions, this includes information that allow us to categorise, understand and describe the problem

addressed by each approach, and the technique or results provided by each paper. This informed the publication

search (e.g., our focus venues,), as well as the keywords employed in the ﬁltering process used in identifying

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

7

relevant papers. Among several details, our interests include the studied fairness metrics and biases, the form of

fairness analysis (e.g., fairness testing) and the level of access (e.g., white, grey or black -box).

(3) Publication Search: We curated fairness-related publication via three means, (1) we searched the top venues in

SE (such as ICSE, TSE and FSE), Programming Languages (PL) (e.g., PLDI and POPL), Security (e.g., CCS and

Euro S & P), Artiﬁcial Intelligence (AI) (e.g., AAAI), ML (e.g., ICML, NeurIPS), CV (e.g., CVPR) and NLP (e.g.,

ACL, EMNLP), (2) we collected papers from fairness focused conferences and workshops such as FairWare, FAT,

FATML, and FaaCT; and (3) we conducted a keyword guided publication search of paper respositories (such as
ACM Digital library2, IEEE Xplore Digital Library3 and Google Scholar4) using popular fairness-related terms
such as “bias”, “fairness”, “discrimination”, etc. In total we merged all collected papers which amounted to 420

papers from 64 diﬀerent venues.

(4) Filtering: To identify relevant publications we ﬁlter out publications that are not relevant to our goals and

analysis. Speciﬁcally, we ﬁlter out papers that (1) do not conduct fairness analysis as a part of the software

engineering process, i.e., papers that are not relevant to the requirements, design and quality control of AI or ML

-based software systems; (2) we exclude papers that are not written in English language, duplicate papers, as well

as short papers or extended abstracts; (3) we also exclude papers that analyze fairness for other systems, i.e.,

fairness for non-learning-based software systems (e.g., fairness of network systems); (4) we also exclude papers

that are not focused on software fairness properties, e.g., papers studying other properties such as robustness,

consistency, security or accuracy; (5) ﬁnally, we read the abstract of each paper and excluded papers that are not

research papers studying software fairness, for instance, we excluded surveys, literature reviews and invited

lectures. In total, we ﬁltered out 256 papers and were left with 164 papers for our analysis.

Given the large number of collected papers, we designed a research protocol to analyse each paper and extract

certain information from the papers. For each paper, we extract both the metadata of the paper, as well as the detailed

research information. In terms of metadata, we extracted the the author details, the paper title, the year and venue of

publication and the domain of the publication (e.g., SE, security or PL). For detailed research-relevant information,

we studied the following details about the techniques and evaluation of the paper: the employed datasets, the studied

fairness metrics, the studied biases (i.e., sensitive attributes, e.g., race), the form of analysis (e.g., fairness testing), the

access level, the speciﬁc problem addressed, the main idea of the paper, the proposed solution, the resulting ﬁndings, as

well as the strengths and weaknesses of proposed analysis/solution.

4 RESEARCH QUESTIONS

In this work, we examine several publications (164) that analyse software fairness in learning-based systems. Table 1

provides details of some of the collected publications. We analyse publications from diﬀerent domains and venues,

including SE, PL, security, AI//ML, CV and NLP (see Figure 4). Firstly, we examine the research advances and trends, we

investigate the volume and categories of publications from 2010 till date (i.e., early 2022) (RQ1). Secondly, we investigate

the purpose of these publications including the main idea of the proposed techniques as well as how SE researchers

study software fairness as a software engineering task (RQ2). For instance, we examine if the aim of the proposed

method or analysis is to formalize, test, mitigate or diagnose fairness issues in learning-based software systems. Next,

we analyse the fairness measure studied in the papers, such as individual, group, causal or intersectional fairness (RQ3).

2https://dl.acm.org/
3https://ieeexplore.ieee.org/Xplore/home.jsp
4https://scholar.google.com/

Manuscript submitted to ACM

14

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

Table 3. Excerpt of works performing Software Fairness Analysis (“Acc.” means “level of software access”)

.
c
c
A

Approach

Goal

Problem

LTDD [88]

Debugging

identifying biased
features in training data

Main
Idea
debugging biased features
to build fair ML software.

Multiacc.
Boost [82]

Cito et
al. [35]

ASTR-
AEA [121]

Fair-
Way [34]

FairSM-
OTE [33]

Fair-
Vis [28]
Flip-
test [22]
Aequ-
itas [130]
Themis
[10, 26, 56]

Aeque-
Vox [105]

x
o
b
-
k
c
a
l
B

Auditing,
Analysis,
Mitigation
Debugging,
Mitigation
Debugging,
Testing,
Mitigation
Debugging,
Testing,
Mitigation
Debugging,
Testing,
Mitigation
Auditing,
Analysis
Testing,
Mitigation
Testing,
Mitigation
Formaliz.,
Testing

audit/mitigate multiaccuracy, i.e.,
group fairness for all subgroups

perform multiaccuracy audit and
post-process models to achieve it

debug and isolate the cause of
mispredictions in ML models

performing fairness testing
without existing datasets

detect and explain how ML model
acquires bias from training data

characterize the data on which
the model performs poorly

discover and diagnose fairness
violations in NLP software

identify how ground truth bias
aﬀects ML fairness

ﬁnding biased labels in
training data generation

auditing and analysing
group fairness in ML model
testing individual fairness – similar
treatment of protected statuses
validation of fairness
for arbitrary ML models?
formalize software fairness testing for
casual discovering of discrimination

remove biased labels and balance
data using sensitive attribute

visual analytics for the discovery
and audit of (sub)group fairness
discover individual (un)fairness
and its associated features
generating discriminatory inputs
to uncover fairness violations
measure causal discrimination in
software to direct fairness testing

Debugging,
Testing

testing group fairness for Automatic
Speech Recognition (ASR) systems

group fairness testing by simulating
diﬀerent environments

ExpGA [47]

Testing

CGFT [100]

Testing

SG [5]

Testing

current individual fairness testing
methods suﬀer poor eﬃciency,
eﬀectiveness, and model speciﬁcity
Uneven distribution of fairness tests
and variations in execution results
detecting the presence of individual
discrimination in ML models.

Bias-
Finder [11]

Biswas and
Rajan [20]

Testing

Mitigation

Fairea [65]

Mitigation

Bias testers for SA systems rely
on small, short, predeﬁned templates

understanding fairness characteristics
in ML models from practice
what is the SE trade-oﬀ
between accuracy and fairness?

ADF [144, 145] Testing

searching individual
discriminatory instances
detecting confusion and bias
errors at class-level
how to detect and improve
individual fairness of a model
interpretability, performance, and
generalizability in bias testing

balancing accuracy-fairness
trade-oﬀ without additional
model(s)
explaining fairness impact
of hyper-parameters

Testing

Testing,
Mitigation
Testing,
Mitigation,
Analysis

Mitigation,
Analysis

Debugging,
Testing,
Mitigation

x
o
b
-
e
t
i
h
W

y
e
r
G

Deep-
Inspect [127]

EIDIG [143]

Neuron-
Fair [148]

Fair-
Neuron [57]

Tizpaz-
Niari [128]

CAT/
TransRepair
[125, 126]

fairness testing by modifying
feature values using explanation
results and genetic algorithm (GA)
leverage combinatorial testing to
generate evenly-distributed test suites
auto-generation of test inputs for
detecting individual discrimination.
discover biased predictions
in SA systems via
metamorphic testing.
empirical evaluation of fairness and
mitigations on real-world ML models
benchmarking and quantifying the
fairness-accuracy trade-oﬀ
achieved by bias mitigation methods
generating discriminatory inputs
violating individual fairness via ML
expose confusion and bias errors in
image classiﬁers
generating test cases that
violate individual fairness

identifying biased neurons, i.e.,
neurons that cause discrimination

neuron activation,
adversarial attacks

detect neurons with contradictory
optimization directions, and achieve
trade-oﬀ via selective dropout
identify the eﬀect of parameters
on software fairness

joint-optimization,
adversarial game

search based testing,
statistical debugging

Testing,
Mitigation

detecting inconsistency
in machine translation (MT)

detect inconsistency bugs without
access to human oracles

mutation testing,
metamorphic testing,
language model (BERT)

proposed approaches are more specialised approaches proposed for analysing applications in speciﬁc domains, e.g., CV

Manuscript submitted to ACM

Core
Technique
data debugging,
linear-regression
multiaccuracy
auditing,
post-processing

rule induction

grammar-
based testing

multi-objective
optimization,
pre/in -processing

situation testing,
data balancing

visual analytics,
domain knowledge
optimal transport,
ﬂipset, distr. sampling
directed testing,
probabilistic search
input schema,
causal relationships
ML robustness,
test simulation,
fault localization
genetic algorithm,
feature mutation,
search based testing
combinatorial testing,
input coverage
symbolic execution,
local explainability
template curation,
NLP techniques,
metamorphic testing

empirical study

model behaviour
mutation

gradient computation
and clustering
class property violations,
robustness
gradient descent,
global/local search

Software Fairness: An Analysis and Survey

15

or NLP venues. Overall, we found that most approaches are focused on fairness testing, some methods are focused on

the mitigation of unfairness, while very few techniques are focused on debugging (diagnosing and understanding the

root causes of) unfairness. In the following, we shed more light on the main idea of available approaches and the gaps

in techniques.

Fairness testing techniques employ a plethora of techniques for test generation, including ML, search and program

analysis techniques. Table 3 provides details on the fairness testing methods proposed in the literature. These test

generation methods include several black box testing approaches (especially, input-based approaches), and few white-box

techniques. Notably, there are even fewer grey-box fairness testing approach. Thus, most proposed approaches drive the

generation of discriminatory inputs either by analysing the model under test (MUT) or the input space. However, there are

very few techniques that leverage both the input space and the model analysis, besides, there are few studies investigating

the relationships between both dimensions for testing purposes (e.g., Tizpaz-Niari et al. [128]). Notably, white box

approaches (e.g., ADF [144, 145] and EIDIG [143]) mostly employ ML techniques (e.g., gradient computation, and

clustering) to drive the generation of discriminatory test cases. Meanwhile, black-box approaches focus on leveraging the

knowledge of the input space, program analysis and/or search algorithms to generate discriminatory inputs. They mostly

employ templates, schemas, grammar, mutation or search algorithms to drive fairness test generation [121, 125, 130, 137].

Other approaches employ program analysis, e.g., symbolic execution [5] and combinatorial testing [100] to drive the

generation of discriminatory test inputs. Notably, we found few grey-box testing techniques that leverage both the input

space and internal model attributes/properties to drive the generation of discriminatory inputs. Moreover, there is little

work that studies the link between the properties of the input space or discriminatory inputs to other internal model

attributes/properties for fairness testing (e.g., Tizpaz-Niari et al. [128])).

Fairness testing approaches are mostly black or white box test generation methods: There are few grey-box approaches

that leverage (or study) the relationship between the input space and internal model properties.

Fairness Veriﬁcation, Certiﬁcation and Proof Guarantees: We examine the literature on the veriﬁcation, certiﬁ-

cation and proving of fairness properties in learning-based software systems. Speciﬁcally, we examine the categories

of veriﬁers, the main ideas of proposed veriﬁcation approaches and the gaps in this research area. To this end, we

identify three major kinds of fairness veriﬁcation approaches, namely distributional veriﬁers (e.g., FairSquare [7] and

VeriFair [14]), specialised veriﬁers designed for a particular domain, metric or task (e.g., [32, 49, 70, 91, 95]) and sample-

based veriﬁers (e.g., AIF360 [15] and Themis [56]). In our study, most approaches are distributional-based veriﬁers or

specialised veriﬁcation techniques, and there are fewer sample-based veriﬁers. Distributional veriﬁers typically encode

fairness metrics as probabilistic properties then verify such properties with respect to the underlying data distribution

of the learning-based system. On the other hand, sample-based approaches (e.g, AIF360) generate tests to verify fairness

metrics based on a ﬁxed test dataset, otherwise they generate counter-examples to refute the satisfaction of the fairness

property. Other veriﬁcation approaches target speciﬁc domains (e.g. NLP [95] ), speciﬁc fairness metrics (e.g., individual

fairness [70] and disparate impact [49]), or tasks (fair training [32] and data debiasing [49]).

Some veriﬁcation approaches encode fairness metrics as probabilistic properties, then provide guarantees over the

system’s data distribution. These approaches take as input the probability distribution of the attributes in the dataset and

the MUT, then verify the fairness of the system with respect to the distribution and MUT. For instance, FairSquare [7]

presents a technique for verifying and certifying fairness properties by encoding fairness deﬁnitions as probabilistic

properties. Moreover, Albarghouthi et al. [8] also proposed an approach that applies distribution-guided inductive

synthesis to verify (and repair) unfair ML classiﬁers. Likewise, Bastani et al. [14] developed an algorithm for verifying

Manuscript submitted to ACM

16

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

fairness speciﬁcations (called VeriFair), the algorithm provides probabilistic guarantees for fairness properties and

allows users to verify that the probability of fairness errors is small. Ghosh et al. [59] presents a stochastic satisﬁability

(SSAT) framework (called Justicia) to formally verify fairness measures of supervised learning algorithms with respect

to the underlying data distribution. Justicia is applicable to diﬀerent fairness metrics including disparate impact,

statistical parity, and equalized odds. Compared to previous distribution-based veriﬁcation approaches (FairSquare [7]

and VeriFair [14]), Justicia supports non-Boolean and compound sensitive attribute. It also provides theoretical bound

for the ﬁnite-sample error of the veriﬁed fairness measure, and it is more robust than sample-based veriﬁers (e.g.,

AIF360).

There are several veriﬁcation approaches that are focused on a speciﬁc domain, metric, or task (e.g., debiasing

datasets [49] or fair training [32]). For domain- or task-speciﬁc approaches, Ma et al. [95] provides a black-box technique

to enforce fairness guarantee for NLP systems by leveraging advances in certiﬁed robustness of machine learning.

Their approach employs a neutral phase to piggyback the NLP model to smooth its outputs such that they are certiﬁed

to preserve individual fairness. Similarly, Liu et al. [91] presents a (semi-)automated veriﬁcation framework (called

FairCon) to ascertain the fairness of smart contracts, their approach can refute false claims with concrete examples, or

certify that contract implementation fulﬁl desired fairness properties.

For metric-speciﬁc approaches, John et al. [70] proposes sound (but incomplete) veriﬁers for proving individual

fairness of models by employing appropriate relaxations of the problem, speciﬁcally for linear classiﬁers and kernelized

polynomial/radial basis function classiﬁers. Likewise, Feldman et al. [49] presents a veriﬁcation technique for certifying

the (im)possibility of disparate impact on a data set by employing a regression algorithm that minimizes the balanced

error rate (BER) of the dataset. The goal is to verify that a protected or sensitive attribute can not be predicted from the

other attributes in the dataset by ascertaining if there is suﬃcient information about the dataset to detect sensitive

attributes from the data. In terms of verifying fair training, Celis et al. [32] propose a technique to train fair classiﬁers

with theoretical guarantees, using a meta-algorithm for classiﬁcation that can take as input a general class of fairness

constraints with respect to multiple non-disjoint and multi-valued sensitive attributes. Notably, this approach can

handle non-convex fairness constraints such as predictive parity.

A diﬀerent line of fairness veriﬁcation techniques are sample-based veriﬁers such as AIF360 [15] and Themis [56].

Typically, these approaches leverage software testing techniques to verify fairness properties on ﬁxed data sample.

AIF360 [15] is an extensible open source toolkit for testing and verifying fairness properties, particularly for a ﬁxed

data sample. It provides an array of testing methods to generate discriminatory test suites, and report several fairness

performance metrics for any fairness algorithm. It also allows to perform both unit and integration testing for bias

mitigation algorithms. Meanwhile, Themis [56] allows developers to verify that a ﬁxed sample do not discriminate

against speciﬁc sensitive attributes (e.g., race and gender) by automatically generating discriminatory that veriﬁes that

changing the instance of the attribute does not cause a change in the output of the learning-based system. Overall,

these approaches leverage advances in software testing to measure and verify that a ﬁxed sample data fulﬁlls speciﬁc

fairness properties.

There are other general veriﬁcation approaches that verify multiple (user-deﬁned) fairness constraints or other

properties beyond, but including, fairness properties. As an example, Metevier et al. [99] addressed the problem of

verifying multiple fairness deﬁnitions as well as user-deﬁned fairness metrics for learning-based systems. The authors

proposed a veriﬁcation approach (called RobinHood) which employs an oﬄine contextual bandit algorithm determine

the satisﬁability of several fairness constraints. Morevoer, RobinHood provides a probabilisitic guarantee of fairness by

ensuring that it does not return a solution with a probability greater than a user-deﬁned threshold. Besides, Sharma

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

17

et al. [117] is a more general veriﬁcation approach which is also applicable to fairness properties. The authors propose

a (white-box) veriﬁcation approach that employs the knowledge of the internal structure of the model to verify that ML

models fulﬁl several properties (including fairness), in particular by training a shadow model that approximates the

MUT by using the prediction of the original model as training data. It employs a property speciﬁcation language to test

and verify model properties of learning-based software and provides counter-examples (i.e., test cases violating the

property) if the property is not fulﬁlled.

Most fairness veriﬁers are distribution-based, sample-based or specialised for a speciﬁc fairness measure, domain or task.

There are very few veriﬁers that support multiple or user-deﬁned fairness constraints.

Others: Our investigation showed that the SE research community has mostly focused on analysing software fairness

as a fairness validation (i.e., testing and debugging) problem [56, 130] and as a fair system design problem, especially to

mitigate biases [27, 34, 129]. However, other aspects of SE concerns are under-studied, such as the formalization of

fairness as a software requirement [26, 52], the veriﬁcation of fairness properties [7], and empirical evaluation of software

fairness properties [20, 65]. Furthermore, some aspects of SE concerns are hardly studied by the community, namely,

empirical evaluation of fairness properties (especially human factors in software fairness [61]) and the maintenance

of fairness properties as the software evolves (e.g., because of model re-training, model compression [64] or software

regression).

In 2017, Brun and Meliou [26] formalised software fairness as a software engineering problem that needs to be

tackled by all aspects of software engineering. These aspects include steps in the software development life cycle

such as requirements engineering, design, testing, veriﬁcation and maintenance. Since their publication, the bulk of

the published papers have focused on the design, testing and mitigation of software fairness, these areas have been

well explored and investigated by these communities. For instance, several papers have explored the problem of

fairness testing by employing random test generation (Themis) [10], local search algorithms (Aequitas) [130], gradient

computation (ADF and EIDIG) [143–145], mutation testing (TransRepair) [125], grammar-based testing (Astraea) [121],

symbolic execution [5], property-driven testing [117] and schema or template based testing (BiasRV) [137]. In addition,

the problem of fair system design to mitigate biases has been studied by a few researchers via several techniques

including behavior mutation [65], and feature or dataset manipulation [141].

Very few researchers have conducted empirical evaluation of software fairness properties in real-world applications.

Notably, Biswas and Rajan [20] conducted an empirical study to study several fairness mitigation techniques including

their impact on performance. Likewise, Hort et al. [65] conducted a large scale empirical study to test the eﬀectiveness

of 12 widely-studied bias mitigation methods. Meanwhile, Zhang and Harman [141] empirically evaluated how feature

set and training data aﬀect fairness. The focus of most studies has been on fairness mitigation, except Zhang and

Harman [141] that empirically studied the impact of features and datasets on fairness properties. Besides these two

concerns, we have found very few studies in these ﬁelds empirically evaluating other SE concerns (such as human

factors [38, 63]) or concerns relevant to steps of the model/software development pipeline.

However, some aspects of software fairness analysis remains under-investigated. Firstly, there is little work addressing

concerns about the maintenance of software fairness, for instance, as the software or model evolves over time (i.e.,

software regression analysis) or is optimized (e.g., via model compression for edge devices). For instance, although

there is a recent paper investigating the impact of model compression on software fairness [64], we found no work to

support the software engineering activities around such changes, such as testing for such changes, e.g., in a regression

scenario. Likewise, there are very few empirical studies on software fairness properties, particularly, there has been few

Manuscript submitted to ACM

18

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

empirical evaluation in this area involving humans, which is vital to determine the harm caused by unfair software

behavior [38, 63]. Secondly, the formalization and deﬁnition of fairness as a software requirement, metric or measure

has only been performed by a few researchers, we found few (about three) papers in this area, namely Brun and

Meliou [26], Verma and Rubin [131] and Finkelstein et al. [52]. We aslo found only few papers (e.g., FairSquare [7]) in

the area of fairness veriﬁcation. Other veriﬁcation approaches, e.g., Finkelstein et al. [52], tackled the veriﬁcation of

fairness properties by encoding fairness deﬁnitions as probabilistic program properties, then automatically verifying

and certifying that a program meets a given fairness property. We encourage that more eﬀort has to be invested in

verifying, certifying and providing proof guarantees of fairness properties in learning-based software systems.

The SE research community mostly study software fairness properties as mitigation, design and testing problems. Very

few works have studied fairness as a requirements engineering or veriﬁcation problem, and fewer works empirically

studying human factors of fairness properties and how to maintain fairness as software changes/evolves.

5.3 RQ3 Fairness measure.

In this research question (RQ3), we examine the fairness metrics analyzed by the studies and methods proposed for

software fairness analysis. We investigate the number of studies examining the diﬀerent classes of fairness metrics (e.g.,

statistical measures, similarity-based measures and time-based measures), as well as the distribution of speciﬁc metrics,

such as individual fairness, group fairness and causal fairness. Figure 11 highlights our analysis of the distribution of

these metrics across proposed methods and conducted studies.

Generally, we observed that most studies examine statistical or similarity based measures (such as individual, group or

causal fairness), while time-based metrics (e.g., sequential or long-term fairness) or measures based on causal reasoning

(e.g., fair inference) are not (yet) studied in the SE community as software fairness metrics. Statistical and similarity

based fairness metrics (such as group, individual, and intersectional fairness) are the most examined metrics in software

engineering (see Figure 11(a)). For instance, individual fairness and group fairness account for more than three in four

(76.5%) of studies and methods in software fairness analysis. Causal reasoning based fairness metrics (such as causal

fairness) are also commonly studied. However, time-based metrics were not found in the SE literature [146].

Statistical and similarity -based fairness metrics are popularly studied (86.5%) in fairness analysis, but metrics hinged on

causal reasoning are under-studied (about 10%) and we could not a ﬁnd a single work studying time-based metrics.

In addition, we observed that fairness metrics such as individual and group fairness metrics are well studied in the SE

community. Concretely, about 42.5% and 35% of publications in software engineering venues study individual fairness

and group fairness, respectively. The focus of most of these studies is in terms of testing, validating and mitigating

software to fulﬁl these metrics. Several of these techniques are tailored towards testing, discovering and mitigating one

or both of these metrics. Concretely, 27% of examined studies study only individual discrimination [130, 143, 144]. For

instance, Zhang et al. [143] proposed EIDIG (Eﬃcient Individual Discriminatory Instances Generator), a scalable and

eﬃcient approach to systematically generate test cases that violate the individual fairness for DNN models. Likewise,

Zhang et al. [144] proposed approaches to search for individual discriminatory instances of DNN, using lightweight

procedures like gradient computation and clustering. Overall, we found that 23% of examined papers in the community

study both individual and group fairness, 17% examine only group fairness and 27% study only individual fairness.

In our analysis, causal fairness is also a commonly studied fairness measure in the community, it accounts for one in

ten publications. Notably, Galhotra et al. [56] and Biswas and Rajan [21] have studied the testing and debugging of

Manuscript submitted to ACM

26

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

Table 6. Excerpt of Fairness Analysis Tools

Goal

Addressed Problem

Approach

Access

Tool
(Paper)
Fairkit-
learn [71]

AIF360 [15]

POF [18]

Fair learning,
Analysis
Fair learning,
Analysis

Fair learning,
Analysis

AITEST [6]

Testing

2AFC [89]

Testing

Pc-fair-
ness [135]

Formalization,
Analysis

BiasRV [137]

Testing

Themis [10]

Fair-
Square [7]

FAT Fore-
nsics [120]

VeriFair [14]

Formalization,
Testing
Formalization,
Veriﬁcation,
Certiﬁcation
Analysis,
Auditing,
certifying
Veriﬁcation,
Speciﬁcation

Justicia [59]

Veriﬁcation

FairML [2]

MT-NLP [95]

ASTRAEA [121]

Aequitas [110]

Auditing,
Analysis
Testing,
Mitigation
Debugging, Testing,
Mitigation
Auditing,
Analysis

FairTest [129]

Testing,
Debugging

Themis-ml [13]

Fair Learning,
Mitigation, Analysis,
Auditing

how to reason about and determine the trade-oﬀ
between model quality (accuracy) and fairness
understanding how, when and why to use diﬀerent
bias handling algorithms in the model life-cycle
how to compute the “Pareto curve” of the trade-oﬀ
between accuracy and fairness in the regression
settings (continuous prediction/targeted values)
how to detect the presence of individual
discrimination in ML models
how to relate unobservable phenomena deep
inside models with observable, outside quantities
that we can measure from inputs and outputs
how to bound path-speciﬁc counterfactual fairness,
address their identiﬁability, i.e., whether they can be
uniquely measured from observational data
how to monitor and
uncover biased predictions at runtime
how to formally deﬁne and test software fairness
using a causality-based measure of discrimination

how to verify or certify that a program meets
a given fairness property

Process.
Stage
pre, &
post
pre, in,
& post

pre

post

post

post

post

post

post

model search,
visualisation
extensible architecture for
analysing fairness metrics
fairness regularizers,
Price of Fairness (PoF)
metric
symbolic execution and
local explainability
Test Experiments, Experimental
Psychology, Psycophysics, two-
alternative forced choice (2AFC)
parameterized causal modelling,
linear programming, response-
function variables, constraints
automatic template generation,
mutation, metamorphic relations
causal inference, schema
-based test generation
probabilistic reasoning, SMT
solving, symbolic weighted
-volume-computation algorithm

how to inspect datasets (features), models
and their prediction for fairness metrics

pre, in,
& post

an inter-operable Python framework
for fairness (FAT) algorithms

how to verify fairness speciﬁcations,
i.e., fairness properties of ML programs
how to formally verify the fairness metrics are
satisﬁed by diﬀerent algorithms on diﬀerent datasets

how to determine the signiﬁcance of inputs
in assessing the fairness of black-box models
how to determine if NLP models are free of
unfair bias toward certain sub-populations/groups
how to perform fairness testing without an
existing dataset, i.e., no training data access
how to audit for bias and fairness when developing
and deploying algorithmic decision making systems
how to detect unwarranted associations (UA) (disparate
impact, oﬀensive labels, and uneven error rates)
between model outcomes and data attributes

post

pre

post

post

post

post

post

post

adaptive concentration
inequalities

stochastic satisﬁability (SSAT)

behavioral testing, template
-based test generation
model compression, input
ranking algorithms

metamorphic testing

grammar-based testing,
metamorphic relations
bias audit toolkit to
support many bias metrics
unwarranted associations (UA)
framework to determine UA
between outcomes and attributes

how to measure, understand, and mitigate the
implicit historical biases in socially sensitive data

pre, in
& post

API for Fair ML for
simple binary classiﬁer

Grey

Grey

Grey

Black

Black

Black

Black

Black

Black

White,
Black,
& Grey

Black

Black

Black

Black

Black

Black

Black

Black

White,
Black,
& Grey

Checklist [107]

Testing

how to test (fairness) behaviors of NLP systems

fairness metrics while auditing or analyzing fairness properties (e.g., Fairkit-learn [71]), and others enable fairness test

experimentation. Notably, 2AFC [89] tests for (un)fairness via psycho-physical experimentation.

Fairness Analysis tools are mostly focused on the post validation of AI-based software, with little (grey) or no access

(black) to the AI model. There is a need for tools that support white-box, in-processing stages of fairness analysis,

especially for speciﬁcation, formalization and veriﬁcation.

6 LIMITATIONS AND THREATS TO VALIDITY

Collection, Filtering and Analysis of Publications: In this work, we have focused on in-depth analysis of publica-

tions exploring fairness as a software property or conducting fairness analysis via the lens of software engineering (SE).

This scope means that we may have missed or ﬁltered out papers that study fairness in other aspects, e.g., as a legal,

ethical or transparency concern. Hence, this work is limited to the analysis of fairness property as an SE concern.

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

27

Table 7. Details of Open Problems and Future Research Opportunities

Open Problems
Fairness Test Metrics
and Adequacy
Automatic Repair of
Biased Classiﬁers
Tooling for Fairness
Property Speciﬁcation
Unexplored or Poorly
Understood Biases
Sequential and Long-term
Fairness concerns
Human factors in
fairness analysis
Non-Speciﬁc/Holistic
mitigation approaches
Fair Policy, Legalisation,
and Compliance

Problem Description
Measuring when fairness testing
is suﬃcient/enough
How to automatically repair biased
classiﬁers to be (less or ) un-biased?
Specifying and engineering fairness
properties for learning-based systems
Analyzing rare biases (e.g., age), complex or
intersectional biases (e.g., age × gender)?
How to analyse/maintain fairness
as the AI system evolves over time?
Evaluating the harm induced by
fairness violations to humans/society
Designing bias mitigation methods that
are agnostic of tasks, domains or datasets
How to design fairness analysis tools for
policy makers and compliance oﬃcers?

Potential Solutions
Design of fairness test metrics
and adequacy criteria
Automatic Program Repair
for fairness property
Requirement Engineering tool
support for Fairness properties
Fairness Analysis Support for rare,
complex or intersectional Biases
Techniques to support analysis of
sequential and long-term fairness
Empirical studies of Human Factors
in Fairness Analysis
General (i.e., task, domain and dataset
-agnostic) bias analysis techniques
Fairness Analysis Tool Support for
Policy and Compliance Analysis

Sample Related Work
[43, 81, 94, 103]
[50, 96, 119]

[8, 112, 125]

[14, 117]

[25, 28]

[146]

[38, 63, 86]

[145]

[89, 102]

Manual Publication Analysis/Interpretation: The analysis of the publications studied in this paper are potentially

open to human bias since they were manually coded and analyzed. However, to mitigate this threat we ensure that the

in-depth analysis and categorization of each paper is validated by at least one other researcher.

7 CONCLUSION AND FUTURE OUTLOOK

This paper presents a comprehensive analysis and survey of publications on software fairness. In particular, we study

publications that study fairness as a software property, works that examine fairness with a software engineering lens, or

study how to engineer fair learning-based software systems. We identiﬁed several open problems and gaps. Table 7

highlights the details of the open problems identiﬁed in this work. Speciﬁcally, we highlight open problems in the

engineering of fair software systems, including concerns in the areas of fairness testing, veriﬁcation and empirical

evaluation of fairness properties. In the following we discuss the gaps and open problems we elicited from our analysis.

In our study, we found no work exploring the test adequacy of fairness testing approaches. Even though there are several

papers exploring fairness testing concerns [5, 130, 143] as well as works investigating test metrics for learning-based

systems [66, 93, 124, 136], there is still the problem of measuring test adequacy for fairness testing. We found no work

investigating test adequacy for fairness concerns. Invariably, several questions concerning the adequacy of fairness testing

remains unanswered and unexplored: When is fairness testing (in)suﬃcient? How can fairness testing be guided to

reduce redundant test cases? Beyond fairness violations, are there other test metrics that indicate (in)suﬃcient testing

has been performed? These are open problems in the area of fairness testing.

Despite the advances in test metrics for traditional and learning-based software, determining the appropriate test

metrics or adequacy criteria for fairness evaluation remains an open problem. Researchers have proposed several test

adequacy criteria for learning-based software, such as neuron coverage [103] and surprise adequacy [81]. Neuron coverage

(from DeepXplore [103]) measures the parts of a learning-based systems that is exercised by a test input, it employs

the ratio of neurons whose activation values were above a predeﬁned threshold to measure the diversity of neuron

behaviour and guide test generation. Likewise, surprise adequacy evaluates the behaviour of learning-based system

with respect to their training data by measuring the surprise of an input as the diﬀerence in the system’s behaviour

between the input and the training data, such that a good test input should be suﬃciently but not overtly surprising

compared to the training data. Other test metrics for learning-based systems include DeepGini [50], Multiple-Boundary

Clustering and Prioritization (MCP) [119] and Maximum Probability (MaxP) [96]. Despite the availability of several test

Manuscript submitted to ACM

28

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

metrics and adequacy criteria for functional testing of learning-based systems, there is no indication or evaluation that

demonstrates these test metrics are applicable for the fairness testing of learning-based systems. The test metric typically

employed by the fairness testing literature remains unfair behavior/outputs characterizing fairness violations. We

encourage researchers to further investigate this vital challenge of fairness testing.

Although there are several fairness mitigation approaches, repairing unfair models to fulﬁll software fairness properties

(i.e., to be fair or unbiased) has been hardly explored, except for few works (e.g., Albarghouthi et al. [8]). Albarghouthi

et al. [8] proposed an approach that applies distribution-guided inductive synthesis to repair unfair ML classiﬁers with

the goal of making them fair, it also veriﬁes that the repaired classiﬁers is semantically close to the original (unfair)

classiﬁer. Their approach formulates the problem as a probability distribution problem, such that the repaired classiﬁer

needs to satisfy a probabilistic Boolean expression. Most importantly, this work is one of the few approaches we have

found that aims to directly repair ML classiﬁers to fulﬁll fairness properties, without model re-training. Similarly, there

is low support for veriﬁcation of fairness properties (see RQ6). We had observed that verifying, certifying and providing

guarantees for software fairness in learning-based software is under-explored.

Additional open problems include the low number of empirical evaluation of fairness properties, especially as they relate

to human factors and societal policies. There are limited empirical studies studying or measuring the harm caused by

unfair software behavior (to humans) and the impact of fairness mitigation on marginalized individuals and communities.

Blodgett et al. [23] emphasizes the importance of evaluating the harms induced by unfair (NLP) systems on humans

and societies.

Furthermore, there is a need to develop approaches and tool support that are task, and dataset agnostic. For instance,

we observed that there is a lack of general, non-speciﬁc fairness mitigation approaches, most works target a speciﬁc

domain/task (e.g., NLP, CV) or a speciﬁc type of dataset (e.g., structured datasets), with little work that is generally or

demonstrably applicable across domains, tasks or datasets (except for few works like ADF [145]).

Besides, there is a need to provide tools to support the automatic speciﬁcation and requirements engineering of fairness

properties. Closely related works in this area includes MLCheck [117] and VeriFair [14] which allows to specify and

check fairness speciﬁcations. However, these tools do not provide support for non-technical experts, e.g., compliance

oﬃcers and policy makers. Indeed, such tools should support the deﬁnition and formalization of speciﬁc fairness

properties as a socio-technical issue, not just a technical issue. For instance, these tools should allow not only allow

to test fairness, but enable compliance oﬃcers and policy makers to audit and derive bias-preserving policies (e.g.,

equity-based solutions like aﬃrmative action [102]), respectively.

In RQ4, we demonstrate that there are some poorly explored and understood biases, especially in terms of protected

attributes. Generally, we observed that certain tasks, datasets and biases are poorly studied. For instance, protected

attributes relating to marital status, sexuality, non-binary gender, and education are poorly explored. Relating to this

issue is the fact that the interactions of protected attributes is under-explored, especially in terms of the compounding

or intersectional eﬀect of multiple protected attributes, e.g., biases triggered by a combination of attributes (e.g., age
× gender). While works like FairVis [28] allow to visualize and analyze intersectional bias, there is little support for
specifying, testing and mitigating such biases.

Sequential and long-term fairness concerns remain an open problem [146]. For instance, how do we mitigate fairness

properties as the software system evolves? Do the proposed mitigation approaches for one-time fairness analysis scale to

sequential or long-term concerns. In addition, there is the socio-technical and ethical concern of fairness analysis: How

do we ensure that our fairness mitigation approaches do not induce new and unintended biases? How do our proposed

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

29

mitigation and analysis approaches translate to real-world intervention for fairness, e.g., in terms of equity-based

mitigation approaches typically employed in public policy (e.g., aﬃrmative action [102])?

In summary, we posit that there is a need for socio-technical, human-in-the-loop bias analysis approaches that

translate to the mitigation of real-world harms to humans and society. There is a need to provide bias analysis methods

to support developers, policy makers and compliance oﬃcers. For easy scrutiny, reuse and replication, we provide

details of collected papers, our in-depth analysis for each research question and ﬁndings:

https://github.com/ezekiel-soremekun/Software-Fairness-Analysis

REFERENCES

[1] Serge Abiteboul and Julia Stoyanovich. 2019. Transparency, fairness, data protection, neutrality: Data management challenges in the face of new

regulation. Journal of Data and Information Quality (JDIQ) 11, 3 (2019), 1–9.

[2] Julius A Adebayo et al. 2016. FairML: ToolBox for diagnosing bias in predictive modeling. Ph.D. Dissertation. Massachusetts Institute of Technology.
[3] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. 2018. A reductions approach to fair classiﬁcation. In

International Conference on Machine Learning. PMLR, 60–69.

[4] Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uniﬁed framework for fair and stable graph representation learning.

In Uncertainty in Artiﬁcial Intelligence. PMLR, 2114–2124.

[5] Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha. 2019. Black box fairness testing of machine learning models.
In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 625–635.

[6] Aniya Aggarwal, Samiulla Shaikh, Sandeep Hans, Swastik Haldar, Rema Ananthanarayanan, and Diptikalyan Saha. 2021. Testing framework for
black-box AI models. In 2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). IEEE,
81–84.

[7] Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and Aditya V Nori. 2017. Fairsquare: probabilistic veriﬁcation of program fairness. Proceedings

of the ACM on Programming Languages 1, OOPSLA (2017), 1–30.

[8] Aws Albarghouthi, Loris D’Antoni, and Samuel Drews. 2017. Repairing decision-making programs under uncertainty. In International Conference

on Computer Aided Veriﬁcation. Springer, 181–200.

[9] Aws Albarghouthi and Samuel Vinitsky. 2019. Fairness-aware programming. In Proceedings of the Conference on Fairness, Accountability, and

Transparency. 211–219.

[10] Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2018. Themis: Automatically testing software for discrimination. In Proceedings of
the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 871–875.
[11] Muhammad Hilmi Asyroﬁ, Zhou Yang, Imam Nur Bani Yusuf, Hong Jin Kang, Ferdian Thung, and David Lo. 2021. Biasﬁnder: Metamorphic test

generation to uncover bias for sentiment analysis systems. IEEE Transactions on Software Engineering (2021).

[12] Fatma Basak Aydemir and Fabiano Dalpiaz. 2018. A roadmap for ethics-aware software engineering. In 2018 IEEE/ACM International Workshop on

Software Fairness (FairWare). IEEE, 15–21.

[13] Niels Bantilan. 2018. Themis-ml: A fairness-aware machine learning interface for end-to-end discrimination discovery and mitigation. Journal of

Technology in Human Services 36, 1 (2018), 15–30.

[14] Osbert Bastani, Xin Zhang, and Armando Solar-Lezama. 2019. Probabilistic veriﬁcation of fairness properties via concentration. Proceedings of the

ACM on Programming Languages 3, OOPSLA (2019), 1–27.

[15] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoﬀman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep
Mehta, Aleksandra Mojsilović, et al. 2019. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of
Research and Development 63, 4/5 (2019), 4–1.

[16] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoﬀman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Sameep Mehta, Aleksandra

Mojsilovic, Seema Nagar, et al. 2019. Think your artiﬁcial intelligence software is fair? Think again. IEEE Software 36, 4 (2019), 76–80.

[17] Nelly Bencomo, Jin LC Guo, Rachel Harrison, Hans-Martin Heyn, and Tim Menzies. 2021. The Secret to Better AI and Better Software (Is

Requirements Engineering). IEEE Software 39, 1 (2021), 105–110.

[18] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. 2017. A Convex

Framework for Fair Regression. Fairness, Accountability, and Transparency in Machine Learning (2017).

[19] Sarah Bird, Miro Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker.

2020. Fairlearn: A toolkit for assessing and improving fairness in AI. Microsoft, Tech. Rep. MSR-TR-2020-32 (2020).

[20] Sumon Biswas and Hridesh Rajan. 2020. Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model
fairness. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 642–653.

Manuscript submitted to ACM

30

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

[21] Sumon Biswas and Hridesh Rajan. 2021. Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine

Learning Pipeline. arXiv preprint arXiv:2106.06054 (2021).

[22] Emily Black, Samuel Yeom, and Matt Fredrikson. 2020. Fliptest: fairness testing via optimal transport. In Proceedings of the 2020 Conference on

Fairness, Accountability, and Transparency. 111–121.

[23] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical Survey of “Bias” in NLP. In

Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 5454–5476.

[24] Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian salmon: an inventory of
pitfalls in fairness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 1004–1015.

[25] Martim Brandao. 2019. Age and gender bias in pedestrian detection algorithms. In Proceedings of the Workshop on Fairness Accountability

Transparency and Ethics in Computer Vision at IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.

[26] Yuriy Brun and Alexandra Meliou. 2018. Software fairness. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering

Conference and Symposium on the Foundations of Software Engineering. 754–759.

[27] Margaret Burnett, Simone Stumpf, Jamie Macbeth, Stephann Makri, Laura Beckwith, Irwin Kwan, Anicia Peters, and William Jernigan. 2016.

GenderMag: A method for evaluating software’s gender inclusiveness. Interacting with Computers 28, 6 (2016), 760–787.

[28] Ángel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, and Duen Horng Chau. 2019. FairVis: Visual analytics
for discovering intersectional bias in machine learning. In 2019 IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE, 46–56.

[29] Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for discrimination-free classiﬁcation. Data mining and knowledge discovery 21,

2 (2010), 277–292.

[30] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. 2017. Optimized pre-processing for

discrimination prevention. Advances in neural information processing systems 30 (2017).

[31] Simon Caton and Christian Haas. 2020. Fairness in machine learning: A survey. arXiv preprint arXiv:2010.04053 (2020).
[32] L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. 2019. Classiﬁcation with fairness constraints: A meta-algorithm with

provable guarantees. In Proceedings of the conference on fairness, accountability, and transparency. 319–328.

[33] Joymallya Chakraborty, Suvodeep Majumder, and Tim Menzies. 2021. Bias in machine learning software: why? how? what to do?. In Proceedings of

the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 429–440.

[34] Joymallya Chakraborty, Suvodeep Majumder, Zhe Yu, and Tim Menzies. 2020. Fairway: A way to build fair ml software. In Proceedings of the 28th

ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 654–665.

[35] Jürgen Cito, Isil Dillig, Seohyun Kim, Vijayaraghavan Murali, and Satish Chandra. 2021. Explaining mispredictions of machine learning models
using rule induction. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. 716–727.

[36] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness. In

Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining. 797–806.

[37] Kate Crawford. 2017. The trouble with bias. In Conference on Neural Information Processing Systems, Invited Speaker. https://www.youtube.com/

watch?v=fMym_BKWQzk

[38] Jenna Cryan, Shiliang Tang, Xinyi Zhang, Miriam Metzger, Haitao Zheng, and Ben Y Zhao. 2020. Detecting gender stereotypes: lexicon vs.

supervised learning methods. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–11.

[39] Anubrata Das and Matthew Lease. 2019. A Conceptual Framework for Evaluating Fairness in Search. arXiv preprint arXiv:1907.09328 (2019).
[40] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. 2019. Does object recognition work for everyone?. In Proceedings
of the Workshop on Fairness Accountability Transparency and Ethics in Computer Vision at IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops. 52–59.

[41] Emily Denton, Ben Hutchinson, Margaret Mitchell, and Timnit Gebru. 2019. Detecting bias with generative counterfactual face attribute

augmentation. (2019).

[42] Marina Drosou, HV Jagadish, Evaggelia Pitoura, and Julia Stoyanovich. 2017. Diversity in big data: A review. Big data 5, 2 (2017), 73–84.
[43] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. DeepStellar: Model-Based Quantitative Analysis of Stateful Deep
Learning Systems. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (Tallinn, Estonia) (ESEC/FSE 2019). Association for Computing Machinery, New York, NY, USA, 477–487.
https://doi.org/10.1145/3338906.3338954

[44] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd

innovations in theoretical computer science conference. 214–226.

[45] Michael D Ekstrand, Robin Burke, and Fernando Diaz. 2019. Fairness and discrimination in recommendation and retrieval. In Proceedings of the

13th ACM Conference on Recommender Systems. 576–577.

[46] Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, and Ioannis Kompatsiaris. 2021. A Survey on Bias in Visual Datasets. arXiv preprint

arXiv:2107.07919 (2021).

[47] Ming Fan, Wenying Wei, Wuxia Jin, Zijiang Yang, and Ting Liu. 2022. Explanation-Guided Fairness Testing through Genetic Algorithm. In 2022

IEEE/ACM 44th International Conference on Software Engineering (ICSE). IEEE.

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

31

[48] Golnoosh Farnad, Behrouz Babaki, and Michel Gendreau. 2020. A unifying framework for fairness-aware inﬂuence maximization. In Companion

Proceedings of the Web Conference 2020. 714–722.

[49] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying and removing disparate

impact. In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 259–268.

[50] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu Chen. 2020. DeepGini: Prioritizing Massive Tests to Enhance the
Robustness of Deep Neural Networks. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis (Virtual
Event, USA) (ISSTA 2020). Association for Computing Machinery, New York, NY, USA, 177–188. https://doi.org/10.1145/3395363.3397357
[51] Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A Survey of Race, Racism, and Anti-Racism in NLP. arXiv preprint

arXiv:2106.11410 (2021).

[52] Anthony Finkelstein, Mark Harman, S Afshin Mansouri, Jian Ren, and Yuanyuan Zhang. 2009. A search based approach to fairness analysis in

requirement assignments to aid negotiation, mediation and decision making. Requirements engineering 14, 4 (2009), 231–245.

[53] Jessie Finocchiaro, Roland Maio, Faidra Monachou, Gourab K Patro, Manish Raghavan, Ana-Andreea Stoica, and Stratis Tsirtsis. 2021. Bridging
machine learning and mechanism design towards algorithmic fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency. 489–503.

[54] Batya Friedman and Helen Nissenbaum. 1996. Bias in computer systems. ACM Transactions on Information Systems (TOIS) 14, 3 (1996), 330–347.
[55] Pratik Gajane and Mykola Pechenizkiy. 2017. On formalizing fairness in prediction with machine learning. arXiv preprint arXiv:1710.03184 (2017).
[56] Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing: testing software for discrimination. In Proceedings of the 2017 11th

Joint Meeting on Foundations of Software Engineering. 498–510.

[57] Xuanqi Gao, Juan Zhai, Shiqing Ma, Chao Shen, Yufei Chen, and Qian Wang. 2022. FairNeuron: Improving Deep Neural Network Fairness with

Adversary Games on Selective Neurons. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). IEEE.

[58] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021.

Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.

[59] Bishwamittra Ghosh, Debabrota Basu, and Kuldeep S Meel. 2021. Justicia: A Stochastic SAT Approach to Formally Verify Fairness. In Proceedings of

the AAAI Conference on Artiﬁcial Intelligence, Vol. 35. 7554–7563.

[60] CV González Zelaya, P Missier, and D Prangle. 2019. Parametrised data sampling for fairness optimisation. In 2019 XAI Workshop at SIGKDD,

Anchorage, AK, USA.

[61] Nina Grgic-Hlaca, Elissa M Redmiles, Krishna P Gummadi, and Adrian Weller. 2018. Human perceptions of fairness in algorithmic decision making:

A case study of criminal risk prediction. In Proceedings of the 2018 world wide web conference. 903–912.

[62] Galen Harrison, Julia Hanson, Christine Jacinto, Julio Ramirez, and Blase Ur. 2020. An empirical study on the perceived fairness of realistic,

imperfect machine learning models. In Proceedings of the 2020 conference on fairness, accountability, and transparency. 392–402.

[63] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach. 2019. Improving fairness in machine learning

systems: What do industry practitioners need?. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1–16.

[64] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint

arXiv:2010.03058 (2020).

[65] Max Hort, Jie M Zhang, Federica Sarro, and Mark Harman. 2021. Fairea: a model behaviour mutation approach to benchmarking bias mitigation
methods. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 994–1006.

[66] Chao Huang, Junbo Zhang, Yu Zheng, and Nitesh V. Chawla. 2018. DeepCrime: Attentive Hierarchical Recurrent Networks for Crime Prediction.
In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (Torino, Italy) (CIKM ’18). Association for
Computing Machinery, New York, NY, USA, 1423–1432. https://doi.org/10.1145/3269206.3271793

[67] Waqar Hussain, Davoud Mougouei, and Jon Whittle. 2018. Integrating social values into software design patterns. In 2018 IEEE/ACM International

Workshop on Software Fairness (FairWare). IEEE, 8–14.

[68] Ben Hutchinson and Margaret Mitchell. 2019. 50 years of test (un) fairness: Lessons for machine learning. In Proceedings of the Conference on

Fairness, Accountability, and Transparency. 49–58.

[69] HV Jagadish, Julia Stoyanovich, and Bill Howe. 2021. Covid-19 brings data equity challenges to the fore. Digital Government: Research and Practice

2, 2 (2021), 1–7.

[70] Philips George John, Deepak Vijaykeerthy, and Diptikalyan Saha. 2020. Verifying individual fairness in machine learning models. In Conference on

Uncertainty in Artiﬁcial Intelligence. PMLR, 749–758.

[71] Brittany Johnson, Jesse Bartola, Rico Angell, Katherine Keith, Sam Witty, Stephen J Giguere, and Yuriy Brun. 2020. Fairkit, Fairkit, on the Wall,

Who’s the Fairest of Them All? Supporting Data Scientists in Training Fair Models. arXiv e-prints (2020), arXiv–2012.

[72] Faisal Kamiran and Toon Calders. 2009. Classifying without discriminating. In 2009 2nd international conference on computer, control and

communication. IEEE, 1–6.

[73] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for classiﬁcation without discrimination. Knowledge and information

systems 33, 1 (2012), 1–33.

[74] Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. 2010. Discrimination aware decision tree learning. In 2010 IEEE International Conference on

Data Mining. IEEE, 869–874.

Manuscript submitted to ACM

32

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

[75] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. 2012. Decision theory for discrimination-aware classiﬁcation. In 2012 IEEE 12th International

Conference on Data Mining. IEEE, 924–929.

[76] Faisal Kamiran, Sameen Mansha, Asim Karim, and Xiangliang Zhang. 2018. Exploiting reject option in classiﬁcation for social discrimination

control. Information Sciences 425 (2018), 18–33.

[77] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classiﬁer with prejudice remover regularizer. In Joint

European conference on machine learning and knowledge discovery in databases. Springer, 35–50.

[78] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing fairness gerrymandering: Auditing and learning for subgroup

fairness. In International Conference on Machine Learning. PMLR, 2564–2572.

[79] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2019. An empirical study of rich subgroup fairness for machine learning. In

Proceedings of the conference on fairness, accountability, and transparency. 100–109.

[80] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. 2019. Learning not to learn: Training deep neural networks with biased

data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9012–9020.

[81] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system testing using surprise adequacy. In 2019 IEEE/ACM 41st International

Conference on Software Engineering (ICSE). IEEE, 1039–1049.

[82] Michael P Kim, Amirata Ghorbani, and James Zou. 2019. Multiaccuracy: Black-box post-processing for fairness in classiﬁcation. In Proceedings of

the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 247–254.

[83] Barbara Kitchenham. 2004. Procedures for performing systematic reviews. Keele, UK, Keele University 33, 2004 (2004), 1–26.
[84] Caitlin Kuhlman, MaryAnn VanValkenburg, and Elke Rundensteiner. 2019. Fare: Diagnostics for fair ranking using pairwise error metrics. In The

World Wide Web Conference. 2936–2942.

[85] Marta Z Kwiatkowska. 1989. Survey of fairness notions. Information and Software Technology 31, 7 (1989), 371–386.
[86] Michelle Seng Ah Lee and Jat Singh. 2021. The landscape and gaps in open source fairness toolkits. In Proceedings of the 2021 CHI conference on

human factors in computing systems. 1–13.

[87] Chenglu Li, Wanli Xing, and Walter Leite. 2021. Yet Another Predictive Model? Fair Predictions of Students’ Learning Outcomes in an Online Math

Learning Platform. In LAK21: 11th International Learning Analytics and Knowledge Conference. 572–578.

[88] Yanhui Li, Linghan Meng, Lin Chen, Li Yu, Di Wu, Yuming Zhou, and Baowen Xu. 2022. Training Data Debugging for the Fairness of Machine

Learning Software. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). IEEE.

[89] Lizhen Liang and Daniel E Acuna. 2020. Artiﬁcial mental phenomena: Psychophysics as a framework to detect perception biases in AI models. In

Proceedings of the 2020 conference on fairness, accountability, and transparency. 403–412.

[90] Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. 2018. Does mitigating ML’s impact disparity require treatment disparity? Advances

in neural information processing systems 31 (2018).

[91] Ye Liu, Yi Li, Shang-Wei Lin, and Rong Zhao. 2020. Towards automated veriﬁcation of smart contract fairness. In Proceedings of the 28th ACM Joint

Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 666–677.

[92] Kristian Lum and Tarak Shah. 2019. Measures of fairness for New York City’s Supervised Release Risk Assessment Tool. Human Rights Data

Analytics Group (2019), 21.

[93] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong
Wang. 2018. DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering (Montpellier, France) (ASE 2018). Association for Computing Machinery, New York, NY, USA,
120–131. https://doi.org/10.1145/3238147.3238202

[94] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al. 2018. Deepmutation: Mutation

testing of deep learning systems. In 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 100–111.

[95] Pingchuan Ma, Shuai Wang, and Jin Liu. 2020. Metamorphic Testing and Certiﬁed Mitigation of Fairness Violations in NLP Models.. In IJCAI.

458–465.

[96] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon. 2021. Test Selection for Deep Learning Systems. ACM Trans. Softw.

Eng. Methodol. 30, 2, Article 13 (Jan. 2021), 22 pages. https://doi.org/10.1145/3417330

[97] Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. 2020. Survey on Causal-based Machine Learning Fairness Notions. arXiv preprint

arXiv:2010.09553 (2020).

[98] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning.

ACM Computing Surveys (CSUR) 54, 6 (2021), 1–35.

[99] Blossom Metevier, Stephen Giguere, Sarah Brockman, Ari Kobren, Yuriy Brun, Emma Brunskill, and Philip Thomas. 2019. Oﬄine contextual

bandits with high probability fairness guarantees. Advances in neural information processing systems 32 (2019).

[100] Daniel Perez Morales, Takashi Kitamura, and Shingo Takada. 2021. Coverage-Guided Fairness Testing. In International Conference on Intelligence

Science. Springer, 183–199.

[101] Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosiﬁdis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon
Papadopoulos, Emmanouil Krasanakis, et al. 2020. Bias in data-driven artiﬁcial intelligence systems—An introductory survey. Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery 10, 3 (2020), e1356.

[102] U.S. Department of Labor. Accessed: 25.04.2022. Aﬃrmative Action. https://www.dol.gov/general/topic/hiring/aﬃrmativeact

Manuscript submitted to ACM

Software Fairness: An Analysis and Survey

33

[103] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Automated whitebox testing of deep learning systems. In proceedings of

the 26th Symposium on Operating Systems Principles. 1–18.

[104] Geoﬀ Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. 2017. On fairness and calibration. Advances in neural information

processing systems 30 (2017).

[105] Sai Sathiesh Rajan, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. AequeVox: Automated Fairness Testing of Speech Recognition Systems.

(2022).

[106] Stephen Rea. 2020. A Survey of Fair and Responsible Machine Learning and Artiﬁcial Intelligence: Implications of Consumer Financial Services.

Available at SSRN 3527034 (2020).

[107] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond Accuracy: Behavioral Testing of NLP Models with

CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4902–4912.

[108] Luke Rodriguez, Babak Salimi, Haoyue Ping, Julia Stoyanovich, and Bill Howe. 2018. MobilityMirror: Bias-adjusted transportation datasets. In

Workshop on Big Social Data and Urban Computing. Springer, 18–39.

[109] Debjani Saha, Candice Schumann, Duncan C McElfresh, John P Dickerson, Michelle L Mazurek, and Michael Carl Tschantz. 2020. Human

comprehension of fairness in machine learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 152–152.

[110] Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. Aequitas: A bias

and fairness audit toolkit. arXiv preprint arXiv:1811.05577 (2018).

[111] Babak Salimi, Bill Howe, and Dan Suciu. 2019. Data management for causal algorithmic fairness. arXiv preprint arXiv:1908.07924 (2019).
[112] Babak Salimi, Bill Howe, and Dan Suciu. 2020. Database repair meets algorithmic fairness. ACM SIGMOD Record 49, 1 (2020), 34–41.
[113] Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, and Yejin Choi. 2020. Social Bias Frames: Reasoning about Social and

Power Implications of Language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 5477–5490.

[114] Prasanna Sattigeri, Samuel C Hoﬀman, Vijil Chenthamarakshan, and Kush R Varshney. 2019. Fairness GAN: Generating datasets with fairness

properties using a generative adversarial network. IBM Journal of Research and Development 63, 4/5 (2019), 3–1.

[115] Daniel Schwarcz. 2021. Health-Based Proxy Discrimination, Artiﬁcial Intelligence, and Big Data. Artiﬁcial Intelligence, and Big Data (March 3,

2021). Houston Journal of Health Law and Policy (2021).

[116] Shahar Segal, Yossi Adi, Benny Pinkas, Carsten Baum, Chaya Ganesh, and Joseph Keshet. 2021. Fairness in the eyes of the data: Certifying

machine-learning models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 926–935.

[117] Arnab Sharma, Caglar Demir, Axel-Cyrille Ngonga Ngomo, and Heike Wehrheim. 2021. MLCheck-Property-Driven Testing of Machine Learning

Models. arXiv preprint arXiv:2105.00741 (2021).

[118] Shubham Sharma, Yunfeng Zhang, Jesús M Ríos Aliaga, Djallel Bouneﬀouf, Vinod Muthusamy, and Kush R Varshney. 2020. Data augmentation for

discrimination prevention and bias disambiguation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 358–364.

[119] Weijun Shen, Yanhui Li, Lin Chen, Yuanlei Han, Yuming Zhou, and Baowen Xu. 2020. Multiple-Boundary Clustering and Prioritization to Promote

Neural Network Retraining. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). 410–422.

[120] Kacper Sokol, Alexander Hepburn, Rafael Poyiadzi, Matthew Cliﬀord, Raul Santos-Rodriguez, and Peter Flach. 2020. FAT forensics: a python
toolbox for implementing and deploying fairness, accountability and transparency algorithms in predictive systems. Journal of Open Source
Software 5, 49 (2020), 1904.

[121] Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. 2022. Astraea: Grammar-based fairness testing. IEEE Transactions on Software

Engineering (2022).

[122] Julia Stoyanovich. 2019. TransFAT: Translating fairness, accountability and transparency into data science practice. In CEUR Workshop Proceedings,

Vol. 2417. CEUR-WS.

[123] Julia Stoyanovich, Serge Abiteboul, and Gerome Miklau. 2016. Data, responsibly: Fairness, neutrality and transparency in data analysis. In

International Conference on Extending Database Technology.

[124] Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, and Rob Ashmore. 2019. DeepConcolic: Testing and Debugging Deep
Neural Networks. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). 111–114.
https://doi.org/10.1109/ICSE-Companion.2019.00051

[125] Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020. Automatic testing and improvement of machine translation. In

Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 974–985.

[126] Zeyu Sun, Jie M Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis, and Lu Zhang. 2022. Improving Machine Translation Systems via Isotopic

Replacement. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). IEEE.

[127] Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, and Baishakhi Ray. 2020. Testing DNN image classiﬁers for confusion & bias errors. In

Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1122–1134.

[128] Saeid Tizpaz-Niari, Ashish Kumar, Gang Tan, and Ashutosh Trivedi. 2022. Fairness-aware Conﬁguration of Machine Learning Libraries. In 2022

IEEE/ACM 44th International Conference on Software Engineering (ICSE). IEEE.

[129] Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. 2017. Fairtest:
Discovering unwarranted associations in data-driven applications. In 2017 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE,
401–416.

Manuscript submitted to ACM

34

Ezekiel Soremekun, Mike Papadakis, Maxime Cordy, and Yves Le Traon

[130] Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Automated directed fairness testing. In Proceedings of the 33rd ACM/IEEE

International Conference on Automated Software Engineering. 98–108.

[131] Sahil Verma and Julia Rubin. 2018. Fairness deﬁnitions explained. In 2018 ieee/acm international workshop on software fairness (fairware). IEEE, 1–7.
[132] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. 2019. Balanced datasets are not enough: Estimating and mitigating

gender bias in deep image representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5310–5319.

[133] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. 2020. Towards fairness in visual
recognition: Eﬀective strategies for bias mitigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 8919–8928.

[134] Benjamin Wilson, Judy Hoﬀman, and Jamie Morgenstern. 2019. Predictive inequity in object detection. (2019).
[135] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. 2019. Pc-fairness: A uniﬁed framework for measuring causality-based fairness. Advances

in Neural Information Processing Systems 32 (2019).

[136] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter:
A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks. In Proceedings of the 28th ACM SIGSOFT International Symposium
on Software Testing and Analysis (Beijing, China) (ISSTA 2019). Association for Computing Machinery, New York, NY, USA, 146–157. https:
//doi.org/10.1145/3293882.3330579

[137] Zhou Yang, Muhammad Hilmi Asyroﬁ, and David Lo. 2021. BiasRV: Uncovering biased sentiment predictions at runtime. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1540–1544.
[138] Jun Yu, Xinlong Hao, Haonian Xie, and Ye Yu. 2020. Fair face recognition using data balancing, enhancement and fusion. In European Conference on

Computer Vision. Springer, 492–505.

[139] Meike Zehlike, Ke Yang, and Julia Stoyanovich. 2021. Fairness in Ranking: A Survey. arXiv preprint arXiv:2103.14000 (2021).
[140] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning fair representations. In International conference on machine

learning. PMLR, 325–333.

[141] Jie M Zhang and Mark Harman. 2021. “Ignorance and Prejudice” in Software Fairness. In 2021 IEEE/ACM 43rd International Conference on Software

Engineering (ICSE). IEEE, 1436–1447.

[142] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing: Survey, landscapes and horizons. IEEE Transactions on Software

Engineering (2020).

[143] Lingfeng Zhang, Yueling Zhang, and Min Zhang. 2021. Eﬃcient white-box fairness testing through gradient search. In Proceedings of the 30th ACM

SIGSOFT International Symposium on Software Testing and Analysis. 103–114.

[144] Peixin Zhang, Jingyi Wang, Jun Sun, Guoliang Dong, Xinyu Wang, Xingen Wang, Jin Song Dong, and Ting Dai. 2020. White-box fairness testing

through adversarial sampling. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 949–960.

[145] Peixin Zhang, Jingyi Wang, Jun Sun, Xinyu Wang, Guoliang Dong, Xingen Wang, Ting Dai, and Jin Song Dong. 2021. Automatic Fairness Testing

of Neural Classiﬁers through Adversarial Sampling. IEEE Transactions on Software Engineering (2021).

[146] Xueru Zhang and Mingyan Liu. 2021. Fairness in learning-based sequential decision algorithms: A survey. In Handbook of Reinforcement Learning

and Control. Springer, 525–555.

[147] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias ampliﬁcation

using corpus-level constraints. arXiv preprint arXiv:1707.09457 (2017).

[148] Haibin Zheng, Zhiqing Chen, Tianyu Du, Xuhong Zhang, Yao Cheng, Shouling Ji, Jingyi Wang, Yue Yu, and Jinyin Chen. 2022. NeuronFair:
Interpretable White-Box Fairness Testing through Biased Neuron Identiﬁcation. In 2022 IEEE/ACM 44th International Conference on Software
Engineering (ICSE). IEEE.

[149] Indre Žliobaite, Faisal Kamiran, and Toon Calders. 2011. Handling conditional discrimination. In 2011 IEEE 11th International Conference on Data

Mining. IEEE, 992–1001.

Manuscript submitted to ACM

