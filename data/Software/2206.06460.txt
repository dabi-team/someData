2
2
0
2

n
u
J

3
1

]
E
S
.
s
c
[

1
v
0
6
4
6
0
.
6
0
2
2
:
v
i
X
r
a

MetaTPTrans: A Meta Learning Approach for
Multilingual Code Representation Learning

Weiguo Pian1, Hanyu Peng2, Xunzhu Tang1, Tiezhu Sun1, Haoye Tian1∗,
Andrew Habib1, Jacques Klein1, Tegawendé F. Bissyandé1
1SnT, University of Luxembourg, Luxembourg
2Cognitive Computing Lab, Baidu Research, Beijing, China
{weiguo.pian, xunzhu.tang, tiezhu.sun, haoye.tian,
andrew.habib, jacques.klein, tegawende.bissyande}@uni.lu
hanyupeng0510@gmail.com

Abstract

Representation learning of source code is essential for applying machine learn-
ing to software engineering tasks. Learning code representation across different
programming languages has been shown to be more effective than learning from
single-language datasets, since more training data from multi-language datasets
improves the model’s ability to extract language-agnostic information from source
code. However, existing multi-language models overlook the language-speciﬁc in-
formation which is crucial for downstream tasks that is training on multi-language
datasets, while only focusing on learning shared parameters among the different
languages. To address this problem, we propose MetaTPTrans, a meta learning
approach for multilingual code representation learning. MetaTPTrans generates
different parameters for the feature extractor according to the speciﬁc program-
ming language of the input source code snippet, enabling the model to learn both
language-agnostics and language-speciﬁc information. Experimental results show
that MetaTPTrans improves the F1 score of state-of-the-art approaches signiﬁcantly
by up to 2.40 percentage points for code summarization, a language-agnostic task;
and the prediction accuracy of Top-1 (Top-5) by up to 7.32 (13.15) percentage
points for code completion, a language-speciﬁc task.

1

Introduction

Modeling source code aims at capturing both its syntax and semantics to enable applying machine
learning to software engineering tasks such as code summarization (Zügner et al., 2021; Peng et al.,
2021), code completion (Liu et al., 2020b,a), and bug ﬁnding (Wang et al., 2016; Pradel & Sen, 2018).
Inspired by the success of deep learning in the ﬁeld of natural language processing (NLP) (Hochreiter
& Schmidhuber, 1997; Vaswani et al., 2017), modeling source code with deep learning techniques
has attracted increasing attention from researchers in recent years (Alon et al., 2019a,b; Hellendoorn
et al., 2020; Kim et al., 2021). Although programs are repetitive and predictable (i.e. ‘natural’ (Hindle
et al., 2012)), unlike natural language text, they also contain rich structural information, e.g., ASTs,
and data- and control-ﬂow information. Therefore, a direct adoption of NLP models to source code
struggles to capture structural information from the source code context (sequence of tokens) directly.

To address the above issue, Alon et al. (2019a,b) proposed to incorporate the structural information by
encoding pairwise paths on AST to represent the source code via LSTMs (Hochreiter & Schmidhuber,
1997). Inspired by the success of Graph Neural Networks (GNN) in the modeling of structural

∗Corresponding author

Preprint. Under review.

 
 
 
 
 
 
Figure 1: The same code snipped and its associated AST in (a) Python and (b) JavaScript.

data (Kipf & Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018; Gasteiger et al., 2020),
several works leverage GNNs on ASTs to capture the program structure. For instance, Allamanis
et al. (2018) used Gated Graph Neural Networks (GGNN) to learn program representation over ASTs
and data ﬂow graphs. Based on GGNN and programs graphs, Fernandes et al. (2019) proposed
Sequence GNN for code summarization and Zhou et al. (2019) proposed a GNN-based approach to
detect software vulnerabilities. However, GNN-based methods struggle to extract global structural
information since GNNs focus on local message passing when aggregating information across
nodes. Besides, the aforementioned approaches focus on structural information and ignore the
contextual information when extracting features from source code. To model both structural and
contextual information, Hellendoorn et al. (2020) combined the two kinds of information through
Transformers (Vaswani et al., 2017) by biasing the self-attention process with relation information
extracted from graph edge types. Zügner et al. (2021) proposed to bias the computation of self-
attention with multiple kinds of pairwise distances between AST nodes and integrate it into the
XLNet method (Yang et al., 2019). Lastly, Peng et al. (2021) introduced TPTrans, which biases the
attention processes in Transformers with the encoding of relative and absolute AST paths.

At a different direction, Zügner et al. (2021) introduced their novel insight that multilingual training
improves the performance of language models compared to single-language models. In particular,
training language models with cross-language data enhances the model ability to learn language-
agnostic information, e.g., similar AST structures. As an example, Figure 1 shows a code snippet of
the same function in (a) Python and (b) JavaScript. The two ASTs of the code snippets are shown on
the right hand side of the respective ﬁgures. We see that both ASTs share similar structure information,
e.g., the highlighted AST paths in red, orange, and green. Because of such recurring structural patterns
across different programming languages, multi-language code models show superior performance to
single-language models, especially, for languages with smaller number of examples (Zügner et al.,
2021). That said, we note that because such approaches learn a uniﬁed language model with shared
parameters among different programming languages, i.e., the models are oblivious to the underlying
programming language, they struggle to learn language-speciﬁc features. Consider the same example
shown in Figure 1. We see that the context of the code snippets (the sequence of source code tokens)
carry distinctive features, e.g., the API names for the standard libraries: lower() and print() in
Python vs. toLowerCase() and console.log() in JavaScript. Such features, which differ across
different languages, are vital for several downstream tasks such as code completion or bug detection
but are usually overlooked by current multi-language source code models.

Therefore, in this paper, we propose MetaTPTrans, a meta learning-based approach for multilingual
code representation learning. Based on meta learning, our approach generates different parameters for
Transformer models when extracting features from different programming languages. This enables

2

def  findTwo(s):l=s.lower()iflin("two","double","2"):returnlprint("Notfound")functionfindTwo(s) {s=  s.toLowerCase();if (s== 'two' || s== 'double' || s== '2') {returns;}console.log('Not found');}(a) Python code snippet and its AST(b) JavaScript code snippet and its ASTMetaTPTrans to not only extract language-agnostic information from multilingual source code
data, but also capture the language-speciﬁc information, which is overlooked by previous methods.
Speciﬁcally, our approach consists of a language-aware Meta Learner and a feature extractor (Base
Learner). The language-aware Meta Learner takes the programming language kind (e.g.: "Python",
"Go") as input and generates different groups of parameters for the feature extractor (Base Learner)
based on the kind of the programming language. We use TPTrans (Peng et al., 2021), the state-of-
the-art model, as the architecture of our Base Learner. To the best of our knowledge, we are the ﬁrst
to leverage meta learning to learn both language-speciﬁc and language-agnostic information from
multilingual source code datasets. We evaluate MetaTPTrans on two common software engineering
tasks: code summarization, following (Zügner et al., 2021; Peng et al., 2021), and code completion.
The former is a language-agnostic task while the latter is language-speciﬁc. Our results show that
MetaTPTrans outperforms the state-of-the-art models signiﬁcantly on both tasks.

In summary, this paper contributes the following: (1) MetaTPTrans, a meta learning-based approach
for multilingual code representation learning, which learns language-agnostic and language-speciﬁc
information from multilingual source code data, (2) Three different schemes for generating parameters
for different kinds of weights of the Base Learner in MetaTPTrans, and (3) Experimental evaluation on
two important and widely-used software engineering tasks: code summarization and code completion;
and achieving state-of-the-art results.

2 Technical Preliminaries

In this section, we introduce the foundations of absolute and relative position embedding in self-
attention and the TPTrans model (Peng et al., 2021), upon which we build our model.

2.1 Self-Attention with Absolute and Relative Position Embedding

Self-attention (SA) is the basic module in Transformers (Vaswani et al., 2017). It maintains three
projected matrices Q ∈ Rdq×dq , K ∈ Rdk×dk , and V ∈ Rdv×dv to compute an output that is the
weighted sum of the input by attention score:

SA(Q, K, V ) = sof tmax(

s.t.

(cid:35)

(cid:34)Q
K
V

= X





W Q
W K
W V





QKT
√
d

)V

(1)

where X = (x1, x2, ..., xn) is the input sequence of the self-attention module, xi ∈ Rd, d is the
dimension of the hidden state, and W Q ∈ Rd×dq , W K ∈ Rd×dk , W V ∈ Rd×dv are the learnable
parameters matrices of the self-attention component. Here, we follow the previous works (Vaswani
et al., 2017; Zügner et al., 2021; Peng et al., 2021) and set dq = dk = dv = d. More speciﬁcally, the
above equation can be reformulated as:

zi =

n
(cid:88)

j=1

(cid:80)n

s.t. αij =

(xjW V )

exp(αij)
k=1 exp(αik)
(xiW Q)(xjW K)T
√
d

(2)

where zi is the output of xi calculated by self-attention operation. In Vanilla Transformer, Vaswani
et al. (2017) used a non-parameteric absolute position encoding, which is added to the word vectors
directly. Ke et al. (2021) proposed a learnable projection for absolute position for computing the
attention score among words:

αij =

(xiW Q)(xjW K)T
√
2d

+

(piU Q)(pjU K)T
√
2d

(3)

where pi denotes the learnable real-valued vector of position i, and U Q, U K ∈ Rd×d are the
projection matrices of the position vectors pi and pj respectively. To capture the relative position

3

relationship between words, Shaw et al. (2018) proposed to use the relative position embedding
between each two words:

zi =

n
(cid:88)

j=1

(cid:80)n

s.t. αij =

(xjW V + rV
ij )

exp(αij)
k=1 exp(αik)
(xiW Q)(xjW K + rK
√

ij )T

d

(4)

where rK

ij , rV

ij denote the learnable relative position embedding between positions i and j.

2.2 TPTrans

We brieﬂy introduced how absolute and relative position embeddings are integrated into the self-
attention module of Transformers. In this section, we describe the TPTrans (Peng et al., 2021) model
which is based on the aforementioned position embedding concepts.

TPTrans modiﬁes the relative and absolute position embedding in self-attention with AST paths
encodings so that AST paths could be integrated into Transformers. Speciﬁcally, they ﬁrst encode the
relative and absolute path via a bi-directional GRU (Cho et al., 2014):

rij = GRU (P athxi→xj )
ai = GRU (P athroot→xi)
where P athxi→xj denotes the AST path from node xi to node xj, rij is the relative path encoding
between positions i and j, and ai is the absolute path (from the root node to node xi) encoding of
position i. Then, the two types of path encodings are integrated into Eq. (3) - (4) by replacing the
absolute and relative position embeddings:

(5)

zi =

n
(cid:88)

j=1

exp(αij)
k=1 exp(αik)

(cid:80)n

(xjW V + rijW r,V )

s.t. αij =

(xiW Q)(xjW K + rijW r,K)T + (aiW a,Q)(ajW a,K)T
√
d

(6)

where W r,K, W r,V ∈ Rd×d are the key and value projection matrices of the relative path encoding
and W a,Q, W a,K denote the query and key projection matrices of the absolute path encoding.

3 Approach

We introduce MetaTPTrans which consists of two components: a Meta Learner and a Base Learner.
Speciﬁcally, the Meta Learner takes the language kind as input and generates language-speciﬁc
parameters for the Base Learner and the Base Learner, TPTrans in our case, takes the source code as
input, and uses the language-speciﬁc parameters generated from the Meta Learner to calculate the
representation of the input source code snippet. The overview of our approach is shown in Figure 2.

3.1 Meta Learner

The Meta Learner generates learnable parameters for the Base Learner according to the language
kind. Speciﬁcally, for a given source code snippet Xi and its corresponding language kind ti, the
Language Embedding Layer T embeds the language kind ti into an embedding Ti ∈ RdT . Then, a
projection layer scales the dimension of Ti. The overall process can be presented as:

Ti = T (ti)
Pi =P rojection(Ti)
where P rojection(·) and Pi ∈ RdP denote the projection layer and the projected language kind
embedding, respectively. After producing the projected language kind embedding Pi, we now present
the parameter generation scheme for the Base Learner. For a weight matrix W λ ∈ Rd×d in the
parameters of Base Learner, we conduct it via a generator Gλ, which can be denoted as:

(7)

W λ = Gλ(Pi)

4

(8)

Figure 2: Architecture of MetaTPTrans.

As noted in (Bertinetto et al., 2016; Wang et al., 2019), using a linear projection layer to scale a dP -
dimension vector to a matrix of dimension d×d exhibits a large memory footprint, especially that there
are many such weight matrices in the parameters of the Base Learner. To reduce the computational
and memory cost, we adopt the factorized scheme for the weight generation procedure by factorizing
the representation of the weights, which is analogous to the Singular Value Decomposition (Bertinetto
et al., 2016). In this way, the original vector can be projected into the target matrix dimension space
with fewer parameters in the generator. More speciﬁcally, we ﬁrst apply the diagonal operation
to transform the vector Pi to a diagonal matrix, then two projection matrices Mλ ∈ RdP ×d and
λ ∈ Rd×dP are deﬁned to project the diagonal projected language kind embedding matrix into the
M (cid:48)
space of the target weight matrix. This operation can be formulated as:

where diag(·) denotes the non-parametric diagonal operation, and Mλ and M (cid:48)
parameters in generator Gλ.

W λ = Gλ(Pi) = M (cid:48)

λdiag(Pi)Mλ

(9)
λ are the learnable

3.2 MetaTPTrans: The Base Learner

The Base Learner is the module that actually learns the representation of code snippets, the parameters
of which are generated from the Meta Learner. In our approach, we apply the TPTrans model as
the architecture of the Base Learner. Recall from Eq. (6), the learnable parameters of TPTrans
consist of the projection matrices of tokens (W Q, W K, W V ) and the projection matrices of path
encodings (W r,K, W r,V , W a,Q, W a,K). For the feed-forward layers, we share their parameters
across different languages so that their parameters are updated through back-propagation directly and
not generated by the Meta Learner.

The Meta Learner generates the parameters of the Base Learner. First, we consider that the most
obvious language-speciﬁc information is the the context of a certain source code snippet. Such
contextual information is extracted from the input token sequences. Therefore, we ﬁrst generate the
projection matrices for token sequences. This procedure can be formulated as:

W λ
ti

= Gλ(Pi) = M (cid:48)

λdiag(Pi)Mλ

s.t. λ ∈ {Q, K, V }

(10)

where ti denotes the corresponding language kind of code snippet Xi, and W Q
, W V
ti
are the learnable weights of W Q, W K, W V associated with language kind ti, and Pi =
P rojection(T (ti)) denotes the projected language kind embedding of ti. After that, the gener-
ated weights matrices are assigned to the related parameters by replacing the uniﬁed related weights
matrices in Eq. (6):

ti , W K
ti

zi =

n
(cid:88)

j=1

exp(αij)
k=1 exp(αik)

(cid:80)n

(xjGV (Pi) + rijW r,V )

s.t. αij =

(xiGQ(Pi))(xjGK(Pi) + rijW r,K)T + (aiW a,Q)(ajW a,K)T
√
d

(11)

5

Base LearnerEmbed &ProjectionParse &EmbedPythonJavascriptRubyGoLanguage KindCode SnippetsCode Context & ASTEncoderLayer1EncoderLayer2EncoderLayerKGeneratorsMeta Learner……EmbeddingsParameters…Code Representation*GeneratorSharedWeightsSharedWeightsLanguage-Specific WeightsencoderdecoderCodesummarizationsummarycodeCodecompletionAES.MODE_?Model??CBC…TasksdiagFurther, the weights matrices for path encoding projection can also be generated by the Meta Learner
for different language kinds. This process aims to integrate the structural information (path encodings)
into the language-speciﬁc contextual information dynamically, which we believe is meaningful to
investigate. This enables MetaTPTrans to associate source code structure with its language kind.
Similar to Eq. (10), this procedure can be expressed as:
= Gλ(Pi) = M (cid:48)

λdiag(Pi)Mλ

W λ
ti

, W r,V
ti

where W r,K
are the generated weights of W r,K, W r,V , W a,Q, W a,K asso-
ciated with language kind ti. After that, the generated weights matrices in Eq. (12) can be integrated
into Eq. (6) by replacing the related weights matrices:

, W a,Q
ti

ti

s.t. λ ∈ {{r, K}, {r, V }, {a, Q}, {a, K}}
, W a,K
ti

(12)

zi =

n
(cid:88)

j=1

exp(αij)
k=1 exp(αik)

(cid:80)n

(xjW V + rijGr,V (Pi))

s.t. αij =

(xiW Q)(xjW K + rijGr,K(Pi))T + (aiGa,Q(Pi))(ajGa,K(Pi))T
√
d

(13)

Finally, we combine the above two kinds of weights generation schemes in which both context token
projection and path encoding projection are generated by the Meta Learner:
exp(αij)
k=1 exp(αik)

(xjGV (Pi) + rijGr,V (Pi))

zi =

n
(cid:88)

(cid:80)n

j=1

(14)

s.t. αij =

(xiGQ(Pi))(xjGK(Pi) + rijGr,K(Pi))T + (aiGa,Q(Pi))(ajGa,K(Pi))T
√
d

In the above, we generate weights matrices from three perspectives: (i) For context token projection
(Eq. (11)), (ii) For path encoding projection (Eq. (13)), and (iii) For both context token projection and
path encoding projection (Eq. (14)). We name those three schemes of weights matrices generation:
MetaTPTrans-α, MetaTPTrans-β, and MetaTPTrans-γ, respectively.

4 Experimental Setup and Results

In this section, we present our experimental setup and results. We evaluate MetaTPTrans on two
common and challenging software engineering tasks: code summarization and code completion.

Code Summarization Code summarization aims at describing the functionality of a piece of code
in natural language and it demonstrates the capability of language models in capturing the semantics
of source code (Alon et al., 2019b; Zügner et al., 2021; Peng et al., 2021). A high level semantic
task, code summarization is a language-agnostic task where the prediction targets do not depend on
the speciﬁcs of the underlying language, like its syntax and speciﬁc API names. Similar to previous
work (Zügner et al., 2021; Peng et al., 2021), we consider a complete method body as the source
code input and the method name as the target prediction (i.e. the NL summary) while predicting the
method name as a sequence of subtokens. We evaluate the performance of MetaTPTrans on the code
summarization task using precision, recall, and F1 scores over the target sequence.

Code Completion Code completion is another
challenging downstream task in source code mod-
eling (Liu et al., 2020a,b). A language model for
code completion predicts a missing token from its
context (sequence of tokens). Code completion
would beneﬁt from language-speciﬁc informa-
tion, e.g., the syntax and API names, which differ
across programming languages. An example of
code completion in realistic scenario is shown
in Figure 3, in which the IDE predicts a list of
possible completions for a missing token in an
incomplete code snippet. To construct the dataset for code completion task, we randomly replace a
token with a special token <MASK> in a given code snippet, and then parse the code snippet into AST.
The AST and code context are jointly used to model the code snippet to predict the missing token. We
report Top-1 and Top-5 prediction accuracy as the evaluation metrics for the code completion task.

Figure 3: An example of code completion

6

Dataset We conduct our experiments on the CodeSearch-
Net (Husain et al., 2019) dataset. We consider four program-
ming languages in the dataset: Python, Ruby, JavaScript, and
Go. For a summary of the dataset, please see Table 1. For the
pre-processing of the data, and the subset of data used for the
code completion task, please see Appendices A.1 and A.4.

Table 1: Dataset statistics

Language

Samples per partition

Train

Valid

Test

Python
Ruby
JavaScript
Go

412,178
48,791
123,889
317,832

23,107
2,209
8,253
14,242

22,176
2,279
6,483
14,291

Baselines For code summarization, we compare MetaTPTrans
with code2seq (Alon et al., 2019a), GREAT (Hellendoorn
et al., 2020), codeTransformer (Zügner et al., 2021) and TP-
Trans (Peng et al., 2021). For code completion, we compare MetaTPTrans with Transformer (Vaswani
et al., 2017) and TPTrans (Peng et al., 2021). See Appendix A.2 for the description of the baselines.

902,690

47,811

45,229

Total

Implementation Details For both tasks, following (Peng et al., 2021), we set the embedding sizes
of the word, path node, and hidden size of the Transformer to 512, 64, and 1024, respectively. A
linear layer projects the word embedding into the size of the hidden layer of the Transformer. We use
one bidirectional-GRU (Cho et al., 2014) layer of size 64 to encode the paths, and concatenate the
ﬁnal states of both directions as output. We use the Adam (Kingma & Ba, 2015) optimizer with a
learning rate of 1e−4. We train our models for 10 and 40 epochs for the code summarization and
code completion tasks, respectively on 4 Tesla V100 GPUs with batch size of 128 and dropout of 0.2.
For the Base Learner in the code summarization task, we use the same hyperparameters setting of
TPTrans (Peng et al., 2021) for a fair comparison. Speciﬁcally, we set the number of encoder and
decoder layers to 3 and 8, the number of attention heads to 3, and the dimension of the feed-forward
layer to 4096. In the Meta Learner, the dimension of the language kind embedding (dT ) and its
projection (dP ) are set to 1024 and 2048, respectively. We follow (Zügner et al., 2021; Peng et al.,
2021) and add a pointer network (Vinyals et al., 2015) to the decoder. For the code completion task,
we also keep the same hyperparameters setting of TPTrans in the Base Learner. In particular, we set
the number of encoder layers, number of heads, and the dimension of feed-forward layers to 5, 8 and
2048, respectively. Code completion is not a sequence-to-sequence task (Liu et al., 2020a,b), so we
apply a fully connected layer after the encoder instead of a decoder. In the Meta Learner, we set both
the dimension of the language kind embedding (dT ) and its projection (dP ) to 512.

Table 2: Precision, recall, and F1 for the code summarization task. Underlined, bold, and ± values
denote the best results in the baselines, the best results overall, and error bars, respectively.

Model

Single-lang. models
code2seq
GREAT
CodeTransformer
TPTrans

Multi-lang. models
code2seq
GREAT
CodeTransformer
TPTrans

MetaTPTrans-α

MetaTPTrans-β

MetaTPTrans-γ

Python

Ruby

JavaScript

Go

Prec.

Rec.

F1

Prec.

Rec.

F1

Prec.

Rec.

F1

Prec.

Rec.

F1

35.79
35.07
36.40
38.39

34.49
36.75
38.89
39.71

24.85
31.59
33.66
34.70

25.49
31.54
33.82
34.66

29.34
33.24
34.97
36.45

29.32
33.94
36.18
37.01

23.23
24.64
31.42
33.07

23.97
30.05
33.93
39.51

10.31
22.23
24.46
28.34

17.06
24.33
28.94
32.31

14.28
23.38
27.50
30.52

19.93
26.89
31.24
35.55

30.18
31.20
35.06
33.68

31.62
33.58
36.95
34.92

19.88
26.84
29.61
28.95

22.16
27.78
29.98
30.01

23.97
28.86
32.11
31.14

26.06
30.41
33.10
32.33

52.30
50.01
55.10
55.67

52.70
52.65
56.00
56.48

43.43
46.51
48.05
51.31

44.36
48.30
50.44
52.02

47.45
48.20
51.34
53.39

48.17
50.38
53.07
54.16

36.22

40.62

34.01

38.12

40.22
55.89
±0.22 ±0.37 ±0.11 ±0.03 ±1.56 ±0.96 ±0.04 ±0.95 ±0.59 ±0.01 ±0.17 ±0.10
56.45
39.97
±0.45 ±0.09 ±0.15 ±0.85 ±0.04 ±0.33 ±0.45 ±0.44 ±0.45 ±0.63 ±0.07 ±0.33
40.47
55.68
±0.55 ±0.37 ±0.03 ±0.20 ±1.24 ±0.68 ±0.43 ±0.73 ±0.27 ±0.44 ±0.16 ±0.11

35.50

54.24

58.86

38.87

32.66

35.81

33.56

36.76

34.64

37.94

53.38

58.20

37.90

40.58

32.04

37.65

30.11

31.92

40.44

33.69

37.87

53.82

36.12

58.12

35.19

37.02

4.1 Code Summarization

Table 2 shows the results of the code summarization task. The top part of the table shows the results
for single-language models, i.e., models trained and applied to the same language dataset. The middle
and and bottom parts of the table show the results of the multi-language baseline models and our
MetaTPTrans, respectively. MetaTPTrans-α (Eq. 11), MetaTPTrans-β (Eq. 13) and MetaTPTrans-γ
(Eq. 14) outperform all the baseline methods signiﬁcantly. Speciﬁcally, compared with the state-of-
the-art results on the Python, Ruby, JavaScript, and Go datasets, our approach improves the F1 score
by 1.11, 1.47, 2.40, and 2.29, respectively. Overall, MetaTPTrans improves precision by 0.76–2.38,
recall by 1.52–2.65, and F1 by 1.11–2.40 across the four programming languages.

7

Table 3: Top-1 and Top-1 accuracy for the code completion task. Underlined, bold, and ± values
denote the best results in the baselines, the best results overall, and error bars, respectively.

Model

Single-lang. models
Transformer
TPTrans

Multi-lang. models
Transformer
TPTrans

MetaTPTrans-α

MetaTPTrans-β

MetaTPTrans-γ

Python

Ruby

JavaScript

Go

Top-1 Top-5

Top-1 Top-5

Top-1 Top-5

Top-1 Top-5

47.57
63.71

69.86
77.99

44.39
64.42

62.24
72.50

37.57
64.67

53.15
73.42

40.21
57.15

59.65
67.81

47.02
69.81

78.82
84.10

47.16
72.14

77.32
82.27

38.77
67.45

70.84
81.17

42.01
60.45

72.95
79.03

95.42

94.28

78.05

73.52

77.13
91.15
±0.35 ±0.52 ±0.36 ±0.12 ±0.10 ±0.05 ±0.49 ±0.42
71.75
85.21
±0.23 ±0.53 ±0.37 ±0.36 ±0.33 ±0.09 ±0.56 ±0.63
67.12
88.99
±0.04 ±0.07 ±0.36 ±0.12 ±0.19 ±0.24 ±0.03 ±0.17

61.60

86.85

69.55

86.90

71.89

90.72

93.61

72.71

90.97

73.82

66.74

86.26

67.47

92.88

Figure 4: t-SNE visualization of the the validation set for the code summarization task learned by
MetaTPTrans-α (left) and TPTrans (right).

4.2 Code Completion

Table 3 shows the results for the code completion task where the top, middle, and bottom parts of
the table correspond to the results of single-language models, multi-language baseline models, and
MetaTPTrans, respectively. MetaTPTrans-α improves the Top-1 (Top-5) prediction accuracy over
the best baseline, TPTrans, by 7.32 (10.18), 5.91 (13.15), 6.07 (11.71) and 7.02 (12.12) for Python,
Ruby, JavaScript, and Go, respectively. Although the three variants of MetaTPTrans outperform the
baselines, MetaTPTrans-α consistently achieves the best results over the four programming languages.
This is because in MetaTPTrans-α, the language-speciﬁc weights matrices generated by the Base
Learner are only assigned to the parameters of the code context token projection (Eq. 11) while
MetaTPTrans-β and MetaTPTrans-γ assign the generated weights to AST path encodings. This means
that for language-speciﬁc tasks like code completion, compared with generating language-speciﬁc
weights for the projection of structural information, generating weights for context token projection
is more conductive for the extraction of language-speciﬁc information, which yields a signiﬁcant
performance improvement. Achieving code completion Top-5 accuracy of 91.15%–95.42% for the
four programming languages altogether is a signiﬁcant improvement, underscoring MetaTPTrans
ability to learn from multi-language datasets even for language-speciﬁc tasks like code completion.

4.3 Visualization of Learned Representation

To assess the quality of the MetaTPTrans language model, Figure 4 shows the t-SNE (Van der
Maaten & Hinton, 2008) visualization of the learned representations of all the code snippets from
the validation set of the code summarization task. The left and right parts of Figure 4 show the code
representation learned by our model, MetaTPTrans-α, and by the best baseline, TPTrans, respectively.

8

GoPythonJavascriptRubyName starts with subtoken‘new’For MetaTPTrans-α, the representation is generated by the Base Learner and the Meta Learner jointly
(the “code representation” box in Figure 2), and for TPTrans, the representation is generated by
the encoder of the model. We see that MetaTPTrans learns a distributed code representation that
also respects the kind of the programming language of the code snippet as data points from the
same language group together. This demonstrates that our model learns languages-speciﬁc features
much better than the baseline model, where code snippets from the same language do not necessarily
group together. Moreover, as an example, we mark all the code snippets whose names start with the
subtoken ‘new’ by the symbol (cid:56). We see that MetaTPTrans achieves a much better grouping of
those code snippets compared to the baseline model emphasizing our model ability to learn a better
semantic representation of source code, while also respecting the language-speciﬁc features.

5 Related Work

Learning Representation of Source Code Source code representation learning has seen many de-
velopments. Early works mainly focus on learning language models from raw token sequences (Wang
et al., 2016; Dam et al., 2016; Allamanis et al., 2016; Iyer et al., 2016). More recent works explore
the effectiveness of leveraging structural information to model source code. Mou et al. (2016) apply
the convolutional operation on ASTs to extract structure features to represent source code. Alon et al.
(2019b,a) extract paths from ASTs and use RNNs to encode them to represent the source code. Alla-
manis et al. (2018); Fernandes et al. (2019); Zhou et al. (2019) use Graph Neural Networks to capture
the structural information from carefully designed code graphs. Hellendoorn et al. (2020); Zügner
et al. (2021); Peng et al. (2021) use Transformer-based models to represent source code by capturing
both context and structural information, in which the structural information is integrated into the
self-attention module by replacing the position embedding with encoding from AST. Speciﬁcally,
Hellendoorn et al. (2020) bias the self-attention process with different types of correlations between
nodes in code graph. Zügner et al. (2021) use several pair-wise distances on ASTs to represent the
pair-wise relationships between tokens in code context sequence and ﬁnd that multilingual training
improves the performance of language models compared to single-language models. Peng et al.
(2021) encode AST paths and integrate them into self-attention to learn both context and structural
information. Compared to these works, ours is the ﬁrst to use meta learning to learn multilingual
source code models that are capable of learning language-speciﬁc in addition to language-agnostic
information and improves on several of the aforementioned models yielding state-of-the-art results.

Meta Learning for Parameters Generation Meta learning is a novel learning paradigm with the
concept of learning to learn. There are several types of meta learning such as learning to initialize (Finn
et al., 2017), learning an optimizer (Andrychowicz et al., 2016), and learning hyperparameters (Li
et al., 2021). Bertinetto et al. (2016) propose a method called learnet to learn to generate the
parameters of the pupil network. Ha et al. (2017) propose Hypernetworks to generate parameters for
large models through layer-wise weight sharing scheme. Chen et al. (2018) propose a meta learning-
based multi-task learning framework in which a meta network generates task-speciﬁc weights for
different tasks to extract task-speciﬁc semantic features. Wang et al. (2019) use a task-aware meta
learner to generate parameters for classiﬁcation models for different tasks in few-shot learning. Pan
et al. (2019, 2022) apply meta learning to generate parameters for models in spatial-temporal data
mining to capture spatial and temporal dynamics in urban trafﬁc. Our approach is a form of meta
learning to generate model parameters where MetaTPTrans learns to generate different weights for
different programming languages.

6 Conclusion

We propose MetaTPTrans, a meta learning-based approach for multilingual code representation
learning. Instead of keeping the feature extractor with a ﬁxed set of parameters, we adopt meta
learning to generate different sets of parameters for the feature extractor according to the kind of the
programming language. This enables MetaTPTrans to not only extract language-agnostic information,
but to also capture language-speciﬁc features of source code. Experimental results show that our
approach signiﬁcantly improves over the state-of-the-art techniques for two downstream tasks. Our
work provides a novel direction for multilingual source code representation learning.

9

References

Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme
summarization of source code. In Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48, pp. 2091–2100,
2016.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs
with graphs. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.

Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from
structured representations of code. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019a.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: learning distributed representa-

tions of code. Proc. ACM Program. Lang., 3(POPL):40:1–40:29, 2019b.

Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Matthew W. Hoffman, David Pfau,
Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In
Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 3981–3989, 2016.

Luca Bertinetto, João F. Henriques, Jack Valmadre, Philip H. S. Torr, and Andrea Vedaldi. Learning
feed-forward one-shot learners. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona,
Spain, pp. 523–531, 2016.

Junkun Chen, Xipeng Qiu, Pengfei Liu, and Xuanjing Huang. Meta multi-task learning for sequence
modeling. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-
18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 5070–5077. AAAI Press, 2018.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.

Hoa Khanh Dam, Truyen Tran, and Trang Pham. A deep language model for software code. arXiv

preprint arXiv:1608.02715, 2016.

Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization.
In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019, 2019.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning
Research, pp. 1126–1135. PMLR, 2017.

Johannes Gasteiger, Janek Groß, and Stephan Günnemann. Directional message passing for molecular
graphs. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020, 2020.

David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings, 2017.

William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034,
2017.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global
relational models of source code. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.

10

Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. Devanbu. On the natural-
ness of software. In Martin Glinz, Gail C. Murphy, and Mauro Pezzè (eds.), 34th International
Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, pp. 837–847.
IEEE Computer Society, 2012.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Code-
searchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436,
2019.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code
using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics, 2016.

Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In 9th
International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021, 2021.

Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. Code prediction by feeding trees to
transformers. In 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021,
Madrid, Spain, 22-30 May 2021, pp. 150–162. IEEE, 2021.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, 2017.

Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug:
Meta semantic augmentation for long-tailed visual recognition. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 5212–5221,
2021.

Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin. A self-attentional neural architecture
for code completion with multi-task learning. In ICPC ’20: 28th International Conference on
Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020, pp. 37–47. ACM, 2020a.

Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning based pre-trained language model for
code completion. In 35th IEEE/ACM International Conference on Automated Software Engineering,
ASE 2020, Melbourne, Australia, September 21-25, 2020, pp. 473–485. IEEE, 2020b.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree
structures for programming language processing. In Proceedings of the Thirtieth AAAI Conference
on Artiﬁcial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp. 1287–1293. AAAI
Press, 2016.

Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang. Urban trafﬁc
In Proceedings of the 25th
prediction from spatio-temporal data using deep meta learning.
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019,
Anchorage, AK, USA, August 4-8, 2019, pp. 1720–1730. ACM, 2019.

Zheyi Pan, Wentao Zhang, Yuxuan Liang, Weinan Zhang, Yong Yu, Junbo Zhang, and Yu Zheng.
Spatio-temporal meta learning for urban trafﬁc prediction. IEEE Transactions on Knowledge and
Data Engineering, 34(3):1462–1476, 2022.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. Integrating tree path in transformer for

code representation. Advances in Neural Information Processing Systems, 34, 2021.

11

Michael Pradel and Koushik Sen. Deepbugs: A learning approach to name-based bug detection.

Proceedings of the ACM on Programming Languages, 2(OOPSLA):1–25, 2018.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.
In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short
Papers), pp. 464–468, 2018.

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research, 9(11), 2008.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, NeurIPS 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008,
2017.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Corinna Cortes, Neil D.
Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2692–2700, 2015.

Song Wang, Devin Chollak, Dana Movshovitz-Attias, and Lin Tan. Bugram: bug detection with
n-gram language models. In David Lo, Sven Apel, and Sarfraz Khurshid (eds.), Proceedings of
the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016,
Singapore, September 3-7, 2016, pp. 708–719. ACM, 2016.

Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and Joseph E. Gonzalez. Tafe-net: Task-aware
feature embeddings for low shot learning. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 1831–1840.
Computer Vision Foundation / IEEE, 2019.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 5754–5764,
2019.

Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnera-
bility identiﬁcation by learning comprehensive program semantics via graph neural networks. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
10197–10207, 2019.

Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann.
Language-agnostic representation learning of source code from structure and context. In 9th
International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021, 2021.

12

A Appendix

A.1 Preprocessing

We parse code snippets using Tree-Sitter2, an open-source parser that can parse multiple programming
languages into AST. Besides, we follow the token splitting rule in (Alon et al., 2019a; Zügner et al.,
2021; Peng et al., 2021) that splits each code token into sub-tokens regarding to the code name
convention. For instance, the code token sendDirectOperateCommandSet is split into [send,
direct, operate, command, set]. For the vocabulary of sub-tokens, we limit it with the least
occurrence number of 100 in the training set, and we also restrict the max length of the token sequence
to 512 after removing all punctuation. For code summarization task, we follow (Zügner et al., 2021;
Peng et al., 2021) and remove all the anonymous functions in the javaScript dataset which can not be
used for this task. For the code completion task, we randomly select a subset from the whole CSN
dataset with 144,175, 19,974 and 19,680 code snippets for training, validation and testing respectively,
please see Appendix A.4 for details of the dataset used in the code completion task. For the paths, we
set the max length to 32, and we make padding for the path shorter than max length while sampling
nodes with equal intervals to maintain max length following (Peng et al., 2021).

A.2 Baselines

In our experiments, we set the following methods as baselines:

• Transformer: Transformer (Vaswani et al., 2017) is a language model based on multi-head
attention, which can only model contextual information. Transformer is the basic backbone
of GREAT, CodeTransformer and TPTrans.

• code2seq: Code2seq (Alon et al., 2019a) is an LSTM-based method that utilizes the pairwise

path information in AST to model code snippets.

• GREAT: GREAT (Hellendoorn et al., 2020) is a Transformer-based approach that utilizes
manually designed edges, such as dataﬂow, ‘computed from’, ‘next lexical use’ edges.
• CodeTransformer: CodeTransformer (Zügner et al., 2021) is a Transformer-based model
that combines multiple pairwise distance relation between nodes in AST to integrate the
structure information into the context sequence of code

• TPTrans: TPTrans (Peng et al., 2021) is a recent state-of-the-art approach for code repre-
sentation learning based on Transformer and tree paths in AST, which integrate the encoding
of tree paths into self-attention module by replacing the relative and absolute position
embedding.

A.3 Code Summarization Results w/o Pointer Network

In our experiments on code summarization task, we apply a pointer network (Vinyals et al., 2015) in
the decoder following (Zügner et al., 2021; Peng et al., 2021). Here, we conduct an ablation study in
which we remove the pointer network and the experiment results are shown in Table 4. We can see
that, our approaches still have better performance compared with the baselines.

A.4 Summary of the CSN Subset in Code Completion Task

For the code completion task, we randomly select a subset from the whole CSN dataset with 144,175,
19,974 and 19,680 code snippets for training, validation and testing respectively. We show the
summary of the CSN subset In Table 5.

A.5 Number of Parameters

We show the number of parameters of our approach and TPTrans in Table 6. In code summariza-
tion task, compared with TPTrans, our MetaTPTrans-α, MetaTPTrans-β and MetaTPTrans-γ have
57.29%, 4.57% and 60.01% more parameters respectively. For code completion task, MetaTPTrans-α,
MetaTPTrans-β and MetaTPTrans-γ have 7.99%, 0.97% and 8.70% more parameters than TPTrans
respectively.

2https://github.com/tree-sitter/

13

Table 4: Experimental results of code summarization task w/o pointer network. The bold part denotes
the best results, the underlined part denotes the best results of baselines, and the ± values denote the
error bars.
Model

JavaScript

Python

Ruby

Go

Prec.

Rec.

F1

Prec.

Rec.

F1

Prec.

Rec.

F1

Prec.

Rec.

F1

CodeTransformer (Multi.)
TPTrans (Multilingual)

38.91
38.78

33.12
34.72

35.78
36.64

34.52
38.05

27.31
32.35

30.50
34.97

37.21
36.35

29.75
30.06

33.07
32.90

56.07
56.49

50.76
51.99

53.28
54.15

MetaTPTrans-α

MetaTPTrans-β

MetaTPTrans-γ

39.22

34.55

37.87

36.57

39.26
55.78
±0.16 ±0.01 ±0.07 ±0.39 ±0.26 ±0.03 ±0.30 ±0.07 ±0.17 ±0.27 ±0.20 ±0.03
55.84
38.34
±0.47 ±0.25 ±0.36 ±0.04 ±0.28 ±0.17 ±0.13 ±0.23 ±0.07 ±0.38 ±0.11 ±0.13
38.50
55.38
±0.08 ±0.50 ±0.30 ±0.14 ±0.29 ±0.10 ±0.32 ±0.61 ±0.49 ±0.04 ±0.41 ±0.19

37.72

36.05

56.49

37.71

36.96

37.42

37.82

54.30

37.29

56.56

57.14

38.87

55.14

38.38

54.48

36.07

32.50

32.62

35.63

34.06

34.98

34.73

33.99

37.35

37.32

36.74

Table 5: Dataset Statistics

Language

Samples per partition

Python
Ruby
JavaScript
Go

Train

Valid

Test

69,527
11,524
26,626
36,948

11,112
1,075
4,353
3,434

10,565
1,048
3,504
4,563

Total

144,175

19,974

19,680

Table 6: Number of Parameters of TPTrans and MetaTPTrans
Model

Task

Code summarization Code completion

TPTrans
MetaTPTrans-α
MetaTPTrans-β
MetaTPTrans-γ

113.47 × 106
178.48 × 106
118.65 × 106
181.56 × 106

101.74 × 106
109.87 × 106
102.73 × 106
110.59 × 106

14

