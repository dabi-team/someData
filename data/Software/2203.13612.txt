2
2
0
2

r
p
A
4

]

G
L
.
s
c
[

2
v
2
1
6
3
1
.
3
0
2
2
:
v
i
X
r
a

Repairing Group-Level Errors for DNNs Using Weighted
Regularization

ZIYUAN ZHONGâˆ— and YUCHI TIANâˆ—, Columbia University, USA
CONOR J.SWEENEY, Columbia University, USA
VICENTE ORDONEZ, Rice University, USA
BAISHAKHI RAY, Columbia University, USA

Deep Neural Networks (DNNs) have been widely used in software making decisions impacting peopleâ€™s lives.
However, they have been found to exhibit severe erroneous behaviors that may lead to unfortunate outcomes.
Previous work shows that such misbehaviors often occur due to class property violations rather than errors
on a single image. Although methods for detecting such errors have been proposed, fixing them has not been
studied so far. Here, we propose a generic method called Weighted Regularization (WR) consisting of five
concrete methods targeting the error-producing classes to fix the DNNs. In particular, it can repair confusion
error and bias error of DNN models for both single-label and multi-label image classifications. A confusion
error happens when a given DNN model tends to confuse between two classes. Each method in WR assigns
more weights at a stage of DNN retraining or inference to mitigate the confusion between target pair. A bias
error can be fixed similarly. We evaluate and compare the proposed methods along with baselines on six
widely-used datasets and architecture combinations. The results suggest that WR methods have different
trade-offs but under each setting at least one WR method can greatly reduce confusion/bias errors at a very
limited cost of the overall performance.

Additional Key Words and Phrases: deep neural network, software repair, robustness, fairness

ACM Reference Format:
Ziyuan Zhong, Yuchi Tian, Conor J.Sweeney, Vicente Ordonez, and Baishakhi Ray. 2022. Repairing Group-
Level Errors for DNNs Using Weighted Regularization. J. ACM 0, 0, Article 000 ( 2022), 30 pages. https:
//doi.org/10.1145/1122445.1122456

1 INTRODUCTION
Deep Neural Networks are widely used nowadays as components in many critical applications like
self-driving cars, face-recognition, medical diagnosis, etc. "Although DNN models do not contain
any code logic like more traditional forms of software engineering, these models can still suffer from
a different form of "serious bugs"[28, 68]. For example, it has been found that Google photo-tagging
app tagged pictures of two dark-skinned people as â€œgorillasâ€ [21]. Analogous to traditional software
bugs, previous work in Software Engineering (SE) has denoted classification errors like this as
model bugs [45] that arise from either biased training data, problematic model architecture, training
procedure error or the combination of them.

âˆ—Both authors contributed equally to this research.

Authorsâ€™ addresses: Ziyuan Zhong, ziyuan.zhong@columbia.edu; Yuchi Tian, yt2667@columbia.edu, Columbia University,
116th and Broadway, New York, NY, USA, 10027; Conor J.Sweeney, cjs2201@columbia.edu, Columbia University, 116th and
Broadway, New York, NY, USA, 10027; Vicente Ordonez, vicenteor@rice.edu, Rice University, 6100 Main St, Houston, TX,
USA, 77005; Baishakhi Ray, rayb@cs.columbia.edu, Columbia University, 116th and Broadway, New York, NY, USA, 10027.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2022 Association for Computing Machinery.
0004-5411/2022/0-ART000 $15.00
https://doi.org/10.1145/1122445.1122456

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

 
 
 
 
 
 
000:2

Zhong and Tian, et al.

DNN classification errors fall into three main categories, instance-wise, group-wise, and dataset-
wise. Instance-wise error has been well studied in the previous literature. In essence, an instance-
wise error happens when a DNN model misclassifies different semantic-preserving transformations
of a given input [47, 57, 65, 86]. Over the years, researchers have found numerous such transforma-
tions such as norm-bounded perturbation[47], natural transformation[14], or physical attack[16]
to fool a well-trained DNN classifier. The fixing strategies such as adversarial training, data aug-
mentation are also widely studied [14, 19, 47]. Dataset-wise error is essentially the modelâ€™s overall
accuracy being worse than expected. Previous work have shown promising fixing results using
strategies like data augmentation [58], weight adaptation [83] and input selection using differential
heat maps [45].

Different from the above two, group-wise error is about the DNN modelâ€™s weak performance on
differentiating among certain classes or has inconsistent performance across classes[68]. There are
very few work on repairing group-wise errors and it only receives attentions recently [68]. This
type of bugs is very concerning since it has been found to relate to many real-world notorious
errors. Some work have proposed techniques to detect this kind of errors, however, until now, no
fixing methods have been proposed for repairing them. To bridge this gap, in this work, we propose
a generic fixing method for repairing such errors of any given DNN models.

The group-wise errors definition proposed in [68] consists of two main types with different root
causes: (i) Confusion: The model cannot differentiate one class from another. For example, Google
Photos confuses skier and mountain [48]. (ii) Bias: The model shows disparate outcomes between two
related groups. For example, Zhao et al. [84] find classification bias in favor of women on activities like
shopping, cooking, washing, etc.. Figure 1 presents two concrete examples of both types of errors
from COCO and Image-Net reported in [68]. Note that unlike an instance-wise error which affects
a particular misclassified image or a dataset-wise error which affects all the misclassified images in
the dataset, such group-wise error affects all the images falling into the groups.

(a) given laptop, a mouse is predicted
to be present

(b) a surfing woman is misclassied as
man

Fig. 1. Examples of confusion and bias errors found in [68]

The causes of a group-wise error can be that certain classes are harder to be differentiated from
each other. For example, in CIFAR-10 , dog and cat tend to confuse even a state-of-the-art DNN
model . Figure 2 shows the confusion matrix for a well-trained VGG11-BN model on CIFAR-10 . The
(i,j)-th entry is the probability of the class i being predicted to class j by mistake. It can be seen that
the model is much more likely to confuse between dog and cat with a confusion of 0.081 meaning
given a uniformly randomly sampled dog or cat image there is a 8.1% chance for the model to
make a mistake by predicting the image to belong to the other class. This is much larger than 0.025
which is the second largest pairwise confusion. One potential cause of the modelâ€™s high confusion
between dog and cat is that these two classes share many common semantic features. As a result,

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:3

Fig. 2. Confusion matrix of VGG11-BN on CIFAR-10 .

the two classes tend to be very close to each other in the representation space and the decision
boundary between them might not be "fine-grained" enough for correct classification on some dog
and cat images. We denote the error-inducing classes as target classes. To fix the errors of the target
classes, the model needs to take more effort to learn from them. Note that this is similar to solving
the problem of class imbalance[7, 29] in which case a given model is supposed to take more effort
on some minority classes in order to achieve improved class-balanced error. The key differences
are two-fold. First, group-wise errors are not necessarily caused by class imbalance. As shown in
the example of CIFAR-10 in Figure 2, although each class in the training data has the same number
of samples, the model still suffer from much higher confusion between dog and cat compared with
other pairs. Second, the two share different optimization objectives. The objective in our setting is
mitigating confusion/bias while maintaining overall accuracy rather than improving class-balanced
accuracy. It is possible for a model to have similar class-wise accuracy for each class but still suffer
from high pair-wise confusion between some pairs of classes.

For large and complex DNN models, complete training from scratch may not be possible. Some-
times no extra data can be collected, either. In these cases, fine-tuning with potential data aug-
mentation can be applied to enforce the model to learn to better classify the target classes. When
fine-tuning is not possible or training data cannot be accessed, (for example, the user does not have
right to access the data) the output can be modified to fix the errors while sacrificing the overall
performance to some extent.

Fig. 3. Overview of Weighted Regularization for Target Fixing

Based on these observations as well as inspiration from existing works on the problem of class
imbalance, we propose a generic method called weighted regularization (WR). WR consists of
multiple concrete methods including weighted augmentation (w-aug), weighted batch normalization
(w-bn), weighted output smoothing (w-os), weighted loss (w-loss), and weighted distance-based
regularization (w-dbr). These methods function at different stages of a given DNNâ€™s training or

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0060.0130.0050.0070.0020.0020.0040.0250.0070.006000.0010.0010.0010.00100.0060.0240.0150.00100.0210.0250.0170.0230.0060.0030.0030.0060.0020.02900.020.0820.0260.0130.0040.0060.0030.0010.0130.01400.0170.0080.01100.0010.00500.010.080.01800.0070.010.0010.0020.00400.0110.0220.0050.0040000.0010.00500.0040.0110.0110.0130.001000.0030.0150.0080.0030.0030.00100000.0070.0070.0250.0010.0050.0010.0010.0020.0030.01200.000.020.040.060.080.100.120.14InputModelOutputLossweightedaugmentationweightedlossweightedbatchnormweighted outputsmoothingweighteddistance-based 000:4

Zhong and Tian, et al.

inference time Figure 3. In particular, if retraining is allowed and training data are provided, w-aug
assigns more weights to the target classes during the retraining, w-bn shifts the distribution of
the activation values induced by the input at every batchnorm layer(assuming the model has
batchnorm layers), and w-loss and w-dbr modify the loss function by assigning more weights to the
erroneous instances and regularizing the class centroids in the representation space, respectively.
Such fine-tuning strategies enable the model to emphasize more on the instances of the target
classes and thus more likely to avoid the errors involving the target classes. If fine-tuning and
training data are not provided, w-os multiplies the modelâ€™s prediction on target classes by a smaller
user-specified constant. In other words, making the model predict less the target class. In this way,
the group-wise errors on those unsure data points can be avoided.

We illustrate an ideal fixing result and potential fixing results after applying these methods in
Figure 4 with an example consists of three classes (square, circle and diamond). The colors represent
the modelâ€™s prediction while the dashed lines denote the modelâ€™s decision boundary. Figure 4(a)
shows that the original model tends to confuse between square and circle since these two classes are
very close to each other. Ideally, a fixing method wants to fine-tune the model such that the decision
boundary becomes that in Figure 4(d). w-os tends to solve the confusion issue by contracting the
decision boundary of the target classes as illustrated in Figure 4(c). w-aug, w-loss, and w-dbr try to
reduce confusion by shifting the decision boundary. They may be able to achieve Figure 4(d) but
may also sacrifice the decision boundary for other classes sometimes and get the decision boundary
in Figure 4(b) instead. w-bn comes in between: on the one hand, it tends to contract the decision
boundary as w-os; on the other hand, it tends to shift the decision boundary through fine-tuning.

Fig. 4. Illustration of different potential decision boundary before and after applying WR.

To check what the effectiveness of the proposed methods and what are their fixing performance
in practice, we evaluate them on fixing confusion error and bias error for both single-label and
multi-label image classification in six different settings involving four datasets and five DNN
architectures. Our experiments show that in every setting, a subset of our proposed methods can
significantly reduce the error significantly and most of time at least one method can have similar
accuracy (or mean average precision for multi-label image classification) and lower confusion/bias
error than the original model at the same time. We also provide some analysis of the proposed
methodsâ€™ performance and applicability. In summary, we make the following contributions:

â€¢ We propose a generic method for targeted group-wise error fixing of DNN models, WR,

which consists of five specific methods.

â€¢ We compare the five proposed methods performance and show their effectiveness on fixing

two types of group-wise errors.

Our code is available at https://github.com/yuchi1989/deeprepair .

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

(a)Original(b)Shifted(c)Contracted(d)IdealRepairing Group-Level Errors for DNNs Using Weighted Regularization

000:5

2 BACKGROUND

2.1 Image Classification
This work focus on two types of image classification problems. In a single-label classification
problem, each datum is associated with a single label l from a set of disjoint labels ğ¿ where |ğ¿| > 1.
Typical datasets include CIFAR-10/CIFAR-100 [33] where an image can be categorized into one
of 10/100 classes. In a multi-label classification problem, each datum is associated with a set of
labels Y where ğ‘Œ âŠ† ğ¿. MS-COCO[38] is a commonly used dataset where an image can be labeled
as car, person, traffic light and bicycle.

Given any single- or multi-label classification task, a DNN classifier software tries to learn
decision boundaries that can separate the classes. In particular, all members of one class, say ğ¶ğ‘– ,
should be categorized identically irrespective of their individual features, while all members of any
other class, say ğ¶ ğ‘— , should not be categorized to ğ¶ğ‘– [5]. A DNN represents a given input image in an
embedded space with the feature vector at a certain intermediate layer and uses the layers after as a
classifier to classify these representations. The class separation between two classes estimates how
well the DNN has learned to separate each class from the other. If the embedded distance between
two classes is too small compared to other classes, or lower than some pre-defined threshold, the
DNN is likely to not be able to separate them from each other.

2.2 Group-wise Error
The proposed method aims to address the two types of group-wise errors proposed in [68]. We
provide the definitions of them in the following.

2.2.1 Confusion Error. A confusion error occurs when a DNN frequently makes mistakes in
disambiguating members of two different classes.

Type1 confusions: In single-label classification, Type1 confusion occurs when an object of class ğ´
(e.g.,dog) is misclassified to another class ğµ (e.g.,cat). We call class ğ´ and class ğµ as target classes
since they are the classes inducing the error. For all the objects of class ğ´ and ğµ, it can be quantified
as:

type1conf (ğ´, ğµ) = mean(P(ğ´|ğµ), P(ğµ|ğ´))
In other words, it is the DNNâ€™s probability to misclassify class ğµ as ğ´ and vice-versa, and takes the
average value between the two. For example, given two classes cat and dog, type1conf estimates
the mean probability of dog misclassified to cat and vice versa. Note that, this is a bi-directional
score, i.e. misclassification of ğµ as ğ´ is the same as misclassification of ğ´ as ğµ.

Type2 confusions: In multi-label classification, Type2 confusion occurs when an input image
contains an object of class ğ´ (e.g.,person) and no object of class ğµ (e.g.,bus), but the model predicts
both classes. For a pair of classes, this can be quantified as:

type2conf (ğ´, ğµ) = mean(P((ğ´, ğµ)|ğ´), P((ğµ, ğ´)|ğµ))

In other words, it is the probability to detect two objects in the presence of only one. For example,
given two classes bus and person, type2conf estimates the mean probability of person being
predicted while predicting bus and vice versa. This is also a bi-directional score.

2.2.2 Bias Error. A DNN model is biased if it associates two classes differently with a third class.
For example, consider three classes: man, woman, and skis. An unbiased model should not have
different error rates while classifying man or woman in the presence of skis. To measure such bias
formally, confusion disparity (cd) is defined to measure differences in error rate between classes
ğ´ and ğ¶ and between ğµ and ğ¶:

cd(ğ´, ğµ, ğ¶) = |ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (ğ´, ğ¶) âˆ’ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (ğµ, ğ¶)|,

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:6

Zhong and Tian, et al.

where the ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ measure can be either type1conf or type2conf as defined earlier. cd essentially
estimates the disparity of the modelâ€™s error between classes ğ´, ğµ (e.g., man, woman) w.r.t. a third
class ğ¶ (e.g., skis).

Without loss of generality, we assume ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (ğ´, ğ¶) â‰¥ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (ğµ, ğ¶). We call class ğ´ positive target
class, class ğµ negative target class, and class ğ¶ anchor target class. The three classes ğ´, ğµ, and ğ¶ are
also collectively called target classes.

2.2.3 Repairing Group-wise Error. We define if a group-wise confusion/bias error (type1conf,
type2conf, or cd) of a given DNN model is repaired if that error is reduced more than a desired
user-specified threshold ğœ†1 after repairing. Note that it is sometimes acceptable to sacrifice the
modelâ€™s overall accuracy to some extent as long as the overall accuracy loss is smaller than a
user-specified threshold ğœ†2. In this work, if not stated otherwise, we consider the error reduction to
be 25% of the original error and accuracy loss to be 1% of the original accuracy. We next provide
some usage scenarios.

Usage scenarios for repairing confusion errors: one usage scenario is fixing the erroneous
model used in google photo which was once reported to tag black people to gorillas by mistakes
[21]. This is likely caused by the modelâ€™s confusion between black people and gorillas. Given the
model was trained using lots of resources, the developer might want to fix this confusion error
through fine-tuning or other strategies rather than retrain a new model from scratch. Meanwhile,
it might be tolerable for the model to make some less irritating confusions (e.g. confusion between
two very similar genres of dogs) as a trade-off. Another usage scenario is when one wants to have
the pair-wise confusions among multiple pairs roughly similar. This relates to group-level fairness
definitions (e.g., equalized odds, equal opportunity, etc.) which seek for group-level treatment to be
roughly similar [50].

Usage scenarios for repairing bias errors: one usage scenario is fixing the well-trained but
biased multi-label classification models which have been shown to be much more likely to wrongly
classify a woman to a man in the context of doing sports like surfing or skiing [68]. Such biased
behaviors involving gender are undesirable and can also be regarded as a violation of a kind of
group-level fairness property. In fair ML, it is sometimes considered acceptable to slightly sacrifice
the overall accuracy to avoid certain fairness propertyâ€™s violation. In fact, such trade-offs between
group-level fairness and accuracy are unavoidable (except in extreme cases) and the methods
trying to achieve the best trade-offs w.r.t. some group-level fairness properties have been studied
in previous work [2, 24].

2.3 Regularization
Based on the no free lunch theorem[75], a specific machine learning algorithm needs to be designed
in order to perform well on a specific task. One approach is to leverage regularization to give an
algorithm a preference for one solution over another solution. As proposed by Goodfellow et al.
[20], regularization are all different approaches expressing preference for different solutions. One
well-known technique of regularization is by imposing a penalty into an optimization process to
prevent models from overfitting. For example, the norm penalty of a DNN modelâ€™s weight can be
added into the loss function to encourage smaller weight and to prevent overfitting. In this work,
we apply the idea of regularization in the context of fixing group-wise errors. In the current work,
the proposed five concrete weighted regularization methods, which are respectively applied in
input phase, model layer phase, output phase and loss phase of training or inference, all try to fix a
given target confusing pair or bias triplet by forcing the DNN models to take more effort for the
target classes.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:7

3 METHODOLOGY
Our goal is to reduce a DNN modelâ€™s confusion error on a targeted pair or bias error on a targeted
triplet (defined in Section 2.2) while maintaining its overall accuracy. The targeted pair or triplet are
user-specified e.g. the laptop,mouse pair and the surfing, man, woman triplet as shown in Figure 1b.
To achieve our goal, an intuitive approach is to let the DNN model focus more on the targeted
pair/triplet. All of our proposed methods follow this intuitive approach but differ in at which stage
of the DNN retraining/inference they assign more weights to the targeted pair/triplet as shown in
Figure 3. There are two reasons of designing five methods. First, there are underlying intuitions for
each method and it is not clear which method will perform the best under each setting. Second,
since these methods function at different stages, they have different applicable scenarios.

In the following subsections, we will first introduce the loss function of a generic DNN image
classifier to provide notations and serve as a baseline. Next, we introduce each method by showing
the underlying motivations and connections with existing literature, how they are developed
to fix the confusion error and the bias error for both single-label classification and multi-label
classification, and finally their applicable scenarios. For simplicity, we only explain our methods
in fixing confusion error for one pair of classes and bias error for one triplet of classes. However,
our method can be easily extended to fix multiple pairs or triplets by treating every target pair
/ triplet the same way as the one demonstrated and in Section 5. To demonstrate the methodâ€™s
generalizability to multiple pairs, we show the effectiveness of applying our methods to fix confusion
errors involving multiple pairs.

3.1 Baseline: Original Model (orig) and Fine-tuned Original Model (orig-ft)
The original DNN model is trained using a standard objective function:

ğ¿ğ‘œğ‘ ğ‘ ğ‘œğ‘Ÿğ‘–ğ‘” = E(ğ‘¥,ğ‘¦)âˆ¼DL(ğ‘“ (ğ‘¥), ğ‘¦)

where D is the underlying data distribution of input ğ‘¥ and label ğ‘¦, and L is a loss function. Two
widely used classification loss functions are cross-entropy loss and L2 loss. We additionally consider
a fine-tuned model which applies the same fine-tuning procedures as w-aug, w-bn, w-loss, and
w-dbr discussed in the following but using ğ¿ğ‘œğ‘ ğ‘ ğ‘œğ‘Ÿğ‘–ğ‘” as the objective function during the fine-tuning
process.

3.2 Regularize Input: Weighted Augmentation (w-aug)
The weighted augmentation method fine-tunes a given DNN model with reweighted sampling
probability for images according to their classes. In particular, it samples more from the target
classes and less from the non-target classes. With more sampled from the target classes, the DNN
model is expected to be able to better identify these target classes. This method is similar to the
over-sampling used to addressing class imbalance[7, 29]. The difference is that in our setting the
augmented classes are the target classes while in the class imbalance setting the augmented classes
are the minority classes. However, target classes are not necessarily the minority classes. For
example, in CIFAR-10 , although all the classes have the same number of samples, large confusion
error exist for the dog and cat pair. The loss function for w-aug is defined as:

where the probability density function for the weighted distribution Dâ€² is

ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘¢ğ‘” = E(ğ‘¥,ğ‘¦)âˆ¼Dâ€²L(ğ‘“ (ğ‘¥), ğ‘¦)

ğ‘ğ‘‘ ğ‘“ â€²(ğ‘‹, ğ‘Œ ) =

(cid:40)ğ‘ğ‘‘ ğ‘“ (ğ‘‹, ğ‘Œ ),
ğœŒÃ—ğ‘ğ‘‘ ğ‘“ (ğ‘‹, ğ‘Œ ), otherwise

if ğ‘¦ âˆˆ ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:8

Zhong and Tian, et al.

where ğ‘ğ‘‘ ğ‘“ is the probability density function of the original data distribution ğ·. In essence, the
images that have labels belonging to the target classes are oversampled by scaling the weights of
the non-target classes by a user specified constant ğœŒ âˆˆ [0, 1] during fine-tuning. The smaller ğœŒ
is, the less effort the DNN model will spend on the non-target classes compared with the target
classes during the fine-tuning.

Fixing confusion error. The target classes are the confused pair of classes ğ´ and ğµ so the

3.2.1
DNN should be able to better distinguish one from the other.

Fixing bias error. The target classes are the biased triplet of classes ğ´, ğµ and ğ¶ so the DNN

3.2.2
should be able to better differentiate the three.

3.2.3 Applicable Scenario. Fine-tuning is allowed.

3.2.4 Extension to multiple pairs/triplets. We include all the involved classes in the pairs/triplets
into ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ .

3.3 Regularize Model: Weighted Batch Normalization (w-bn)
The weighted batch normalization method assumes the existence of batch norm layers of a given
DNN model. Batch normalization is usually applied after convolutional layers for more stable
training and faster convergence[27] by making the loss landscape smoother[61]. It re-centers and
re-scales the input data using the estimated mean and variance during training into Gaussian
distribution with mean ğ›½ and variance ğ›¾[1], where ğ›½ and ğ›¾ are learned during back propagation.
w-bn redistributes each batch normalization layer by increasing the weights of the targeted classes
when estimating the mean and variance of the input data for each batch norm layer. This method
comes from a finding that when doing so, the decision boundaries between the target classes and
non-target classes can be shifted towards the target classes. To demonstrate this phenomenon,
Figure 5(a) shows a toy 2D dataset composed of three classes. Figure 5(b) shows the decision
boundary of a well-trained simple ResNet model and Figure 5(c) shows the decision boundary of
the model retrained via w-bn. It is noticeable that the decision boundary of class 2 expand over
class 0 and class 1.

With the decision boundaries shifting towards the target classes, there are four consequences.
First, the DNNâ€™s correct prediction on non-target classes will not be mispredicted to the non-
target classes. Second, the DNNâ€™s wrong prediction on non-target classes to target classes can
potentially be assigned correctly. Third, the DNNâ€™s correct prediction on the target classes might
be mispredicted to non-target classes. Fourth, the DNNâ€™s wrong prediction on the target classes to
other target classes might be mispredicted to non-target classes.

Consequence four will reduce the confusion/bias error. Consequence two benefit while con-
sequence three and four will hurt the overall accuracy. The overall consequence will reduce
confusion/bias error and have an uncertain influence on the overall accuracy. Empirically, we find
the later depends on specific problem and hyper-parameter tuning.

Next, we formally introduce w-bn in Figure 6. We first denote ğ‘¥ to be a regular batch of images
and ğ‘¥ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ to be a batch of images sampled only from the target classes. (a) shows a traditional BN
layer and (b) shows a reweighted BN layer. The main differences are that the weighted BN layer
passes an extra batch of the target classes and assigns more weights to those data (controlled using a
hyper-parameter ğœŒ âˆˆ [0, 1]) when estimating the BN statistics (i.e. batch mean ğ¸ and batch variance
ğ‘‰ ğ‘ğ‘Ÿ ) in the DNNâ€™s forward pass. It should be noted that during the back-propagation, only the loss
coming from the regular batch (highlighted in dashed red box) is considered. This is because if the
extra sampled input data from the target classes are considered in the optimization, this part will
be similar to w-aug but we want to evaluate the influence of w-aug and w-bn separately.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:9

(a) toy 2D dataset

(b) original model

(c) weighted BN

Fig. 5. Shift of decision boundary using weighted BN

Fig. 6. Illustration of the traditional BN layer and the proposed weighted BN layer.

Fixing confusion error. the pair of confused classes ğ´ and ğµ are the target classes. As we
3.3.1
discussed earlier, many instances confused between ğ´ and ğµ will be predicted to non-target classes.
Consequently, the confusion between class ğ´ and ğµ drops.

Fixing bias error. the biased triplet of classes ğ´, ğµ, and ğ¶ will be included in the target class.
3.3.2
The decision boundaries of all the three classes will contract and a subset of images mispredicted
from one target class to another is likely to be predicted to other non-target classes. As a result,
both confusions between A and C, and B and C tend to drop. It follows that the bias will be reduced
as long as the two pairs of confusion drop at relatively similar rate.

3.3.3 Applicable scenario. Fine-tuning is allowed and the DNN model has batch norm layers.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

6420246x132101234x2012432101234x1432101234x2012432101234x1432101234x2012ReLUBNconvReLUBNconv(a) Traditional BN(b) proposed reweighted BN000:10

Zhong and Tian, et al.

3.3.4 Extension to multiple pairs/triplets. We consider all the involved classes in the pairs/triplets
as the target classes.

3.4 Regularize Output: Weighted Output Smoothing (w-os)
The weighted output smoothing method tries to reduce a DNNâ€™s confusion/bias error when fine-
tuning the model is not possible. Since the model itself cannot be updated, only its output can be
changed. The key observation is that misclassified images are more likely to have lower confidence
in the last layer. Thus, to reduce the confusion/bias error of the target classes, we can make those low
confident DNNâ€™s prediction on the target classes to be predicted to the most confident non-target
classes instead. The consequences are essentially similar to w-bn which contracts the decision
boundaries of the target classes. Based on similar reasoning, the confusion/bias errors are likely to
be reduced while the impact on the overall accuracy depends on the specific case.

Note that similar prediction scaling methods have been used for other purposes like calibrating
a DNNâ€™s prediction to be consistent with its true correctness likelihood[22] or post-processing
a DNNâ€™s prediction to improve a DNNâ€™s group-level fairness[24]. The main difference is that
the concrete scaling strategies are developed differently for different purposes. For example, the
post-processing method proposed in [24] assigns different thresholds to each sensitive group for a
binary classifierâ€™s to mitigate equalized odds/opportunity. The thresholds are set to make trade-off
between prediction accuracy and fairness criteria. In contrast, we apply different levels of smoothing
for target classes to make trade-off between accuracy and confusion/bias errors.

We denote the last layerâ€™s output of a given input x to be ğ‘ (ğ‘¥), which is a ğ‘š (i.e. the number of
classes) dimensional vector. Each field of ğ‘ (ğ‘¥) is positively correlated with the prediction probability
of the class corresponding to that field.

Fixing confusion error. For single-label classification, w-os multiplies the target class predic-
3.4.1
tion probability ğ‘ (ğ‘¥)ğ‘¡ by a specified parameter ğœŒ âˆˆ [0, 1] for images classified into any of the target
classes ğ´ and ğµ; for multi-label classification, w-os multiplies the target class prediction probability
ğ‘ (ğ‘¥)ğ‘¡ by ğœŒ for images predicted to have both target classes ğ´ and ğµ.

Fixing bias error. For single-label classification, w-os multiplies the target classes prediction
3.4.2
probability ğ‘ (ğ‘¥)ğ‘¡ by ğœŒ for images predicted to be any of the target classes ğ´, ğµ, and ğ¶; for multi-
label classification, w-os multiplies the target classes prediction probability ğ‘ (ğ‘¥)ğ‘¡ by ğœŒ for images
predicted to have the classes ğ´ and ğ¶ or the classes ğµ and ğ¶.

3.4.3 Applicable Scenario. We only have access to a modelâ€™s last layer output.

3.4.4 Extension to multiple pairs/triplets. We consider all the involved classes in the pairs/triplets
as the target classes.

3.5 Regularize Loss: Weighted Loss (w-loss)
The weighted loss method fine-tunes the given DNN model with more weights in the loss function
assigned to images contributing to the confusion/bias error. Intuitively, since the model takes more
loss when making mistakes on those contributing to the confusion/bias error, it is likely to correctly
predict those even at the sacrifice of the overall accuracy to some extent. This method is similar
to the proposed class-balanced loss in [10] which increase the weight of the minority classes to
address the class imbalance problem. The key difference is that the class-balanced loss increases
the weights of the loss for those minority classes while w-loss increases the weights of confused
samples / bias samples since the former aims to maximize the class-balanced accuracy while the
latter aims to mitigate confusion/bias errors of the target classes. We next provide the new loss
functions for fixing confusion and bias error respectively.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:11

3.5.1

Fixing confusion error. Denote ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ = {ğ´, ğµ}. The loss function is defined as:
ğ¿ğ‘œğ‘ ğ‘ ğ‘Ÿğ‘™ = ğœŒE(ğ‘¥,ğ‘¦)âˆ¼DL(ğ‘“ (ğ‘¥), ğ‘¦)
+ (1 âˆ’ ğœŒ)E(ğ‘¥,ğ‘¦)âˆ¼D(ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ ) L(ğ‘“ (ğ‘¥), ğ‘¦)

where the probability density function for the distribution D(ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ ) is

ğ‘ğ‘‘ ğ‘“ â€²(ğ‘‹, ğ‘Œ ) =

ğ‘ğ‘‘ ğ‘“ (ğ‘‹, ğ‘Œ ),

0,

if (ğ‘¥, ğ‘¦) âˆ¼ D s.t. ğ‘¦ âˆˆ ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡
and ğ‘“ (ğ‘¥) â‰  ğ‘¦ and ğ‘“ (ğ‘¥) âˆˆ ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ .
otherwise

ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³

and ğœŒ âˆˆ [0, 1] is the hyper-parameter balacing the two objectives. Intuitively, the DNN model is
encouraged to better differentiate between ğ´ and ğµ compared with differentiating among other
classes in general.

3.5.2

Fixing bias error. The loss function is defined as:
ğ¿ğ‘œğ‘ ğ‘ ğ‘Ÿğ‘™ = ğœŒE(ğ‘¥,ğ‘¦)âˆ¼DL(ğ‘“ (ğ‘¥), ğ‘¦)

+ (1 âˆ’ ğœŒ)

(cid:16)E(ğ‘¥,ğ‘¦)âˆ¼Dâ€² (ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ + ) L(ğ‘“ (ğ‘¥), ğ‘¦)

+ E(ğ‘¥,ğ‘¦)âˆ¼D(ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ âˆ’ ) L(ğ‘“ (ğ‘¥), ğ‘¦)

(cid:17)

where ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ + = {ğ´, ğ¶} and ğ‘Œğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ âˆ’ = {ğµ, ğ¶}. This loss function encourages the DNN model to
better differentiate between ğ´ and ğ¶ as well as ğµ and ğ¶ compared with differentiating among other
classes in general.

3.5.3 Applicable Scenario. Fine-tuning is allowed.

3.5.4 Extension to multiple pairs/triplets. For each pair(triplet), we add one(two) extra term(terms)
into the second half of the loss function and divide this part by the number of pairs(triplets).

3.6 Regularize Loss: Weighted Distance-Based Regularization (w-dbr)
The distance-based regularization method leverages the class-level representation and adds an
extra regularization term in the loss function to balance the distance among the target classes in
the representation space under a defined metric during the fine-tuning. The insight here is that
the closer the two classes representations are, the more confused the model is between the two
classes[68].

Given a class A, we define its class-level representation

ğ‘ƒğ‘›ğ‘’ğ‘¤ (ğ´) =

[ğ‘† (ğ‘›1), ğ‘† (ğ‘›2), ..., ğ‘† (ğ‘›ğ‘¡ )]
ğ‘

where ğ‘† (ğ‘›ğ‘– ) is the sum of each output of neuron ğ‘›ğ‘– , given ğ‘ input images. Then, we define the
distance metric between two classes A and B as:

ğ·ğ‘›ğ‘’ğ‘¤ (ğ´, ğµ) = ||(ğ‘ƒğ‘›ğ‘’ğ‘¤ (ğ´), ğ‘ƒğ‘›ğ‘’ğ‘¤ (ğµ))||2

3.6.1

Fixing confusion error. We define a new loss:

ğ¿ğ‘œğ‘ ğ‘ ğ‘‘ğ‘ğ‘Ÿ âˆ’ğ‘ğ‘œğ‘›ğ‘“ = ğœŒğ¿ğ‘œğ‘ ğ‘ ğ‘œğ‘Ÿğ‘–ğ‘” âˆ’ (1 âˆ’ ğœŒ)ğ·ğ‘›ğ‘’ğ‘¤ (ğ´, ğµ)
where ğœŒâˆˆ [0, 1] trades off the original loss and the new distance-based regularization. In essence,
the regularization term encourages a larger separation of the centroids of the two classes ğ´ and ğµ
in the representation space.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:12

Zhong and Tian, et al.

3.6.2

Fixing bias error. Similarly, we define a new loss for reducing bias:

ğ¿ğ‘œğ‘ ğ‘ ğ‘‘ğ‘ğ‘Ÿ âˆ’ğ‘ğ‘–ğ‘ğ‘  = ğœŒğ¿ğ‘œğ‘ ğ‘ ğ‘œğ‘Ÿğ‘–ğ‘” + (1 âˆ’ ğœŒ) ğ‘ğ‘ğ‘  (ğ·ğ‘›ğ‘’ğ‘¤ (ğ´, ğ¶) âˆ’ ğ·ğ‘›ğ‘’ğ‘¤ (ğµ, ğ¶)).
The regularization term balances the difference between the centroid distance between class ğ´ and
ğ¶, and centroid distance between class ğµ and ğ¶ in the representation space such that the relative
distances from ğ´ to ğµ and ğ¶ are similar.

3.6.3 Applicable Scenario. Fine-tuning is allowed.

3.6.4 Extension to multiple pairs/triplets. For each pair(triplet), we add one(two) extra term(terms)
into the second half of the loss function and divide this part by the number of pairs(triplets).

4 EXPERIMENTAL DESIGN

4.1 Study Subjects
We evaluate the proposed method for single-label and multi-label DNN-based classifications in-
cluding six combinations of five DNN architectures and four datasets. For each combination, we
choose highly confused pair/biased triplet , those potentially have greater ethical implications, or
randomly chosen pair/triplet having non-zero confusion/bias as the target using DeepInspect[68].
Datasets: We conduct our experiments on two single-label image classification datasets, CIFAR-
10, CIFAR-100 and two multi-label image classification datasets, MS-COCO[38] and MS-COCO
gender[84].

â€¢ CIFAR-10: consists of 50,000 training and 10,000 testing 32x32 color images. It has 10 classes

and 6,000 images per class.

â€¢ CIFAR-100: consists of 50,000 training and 10,000 testing 32x32 color images. It has 100

classes and 600 images per class.

â€¢ MS-COCO: consists of 80783 training images and 40504 validation images. Each image

labeled a subset of 80 objects.

â€¢ MS-COCO gender: same as MS-COCO but with the person class split into man and woman

classes[84].

Architectures: We evaluate our repairing performance on five different convolutional neural
networks[26, 63].

â€¢ ResNet-18: ResNet-18 is trained on CIFAR-10 dataset. The model is trained using the training
scripts from CutMix[79]. The training takes 300 epochs and the repairing takes 60 epochs for
methods requireing fine-tuning. The initial learning rate is 0.1 and is multiplied by 0.1 after
50% and 75% training epochs respectively.

â€¢ VGG11_BN: VGG11_BN is a variant of VGG11 model with batch normalization layers [63].

We train a VGG11_BN model on CIFAR-10 dataset in the same way as above.

â€¢ MobileNetv2: MobileNetv2 is a popular model tailored for mobile and resource constrained

environments [60]. We train it on CIFAR-10 dataset in the same way as above.

â€¢ ResNet-34: ResNet-34 is trained on CIFAR-100 dataset. The model is trained in the same

way as above.

â€¢ ResNet-50: Following Zhao et al[84], we train ResNet-50 models for both MS-COCO and MS-
COCO gender datasets. Both models are trained for 12 epochs and are repaired by retraining
of another 6 epochs for methods requiring fine-tuning.

Table 1 summarizes our study subjects including the details of all the datasets and models used.
Pairs/Triplets Selection: In this work, we mainly focus on mitigating the user selected confusion
pairs / biased triplets. We next discuss how we choose the pairs/triplets used for the experiments.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:13

Table 1. Study Subjects

Dataset

Model

Classification
Task

Name

#classes

Models

Reported
#Params #Layers Accuracy

Multi-label
classification COCO gender[84]

COCO [38]

80
81

ResNet-50[26] 23,671,952
ResNet-50[26] 23,674,001

Single-label

CIFAR-100[33]

100

ResNet-34 [79]

336,244

classification CIFAR-10[33]

10

ResNet-18[79]
127,642
VGG11-BN[63] 9,756,426
MobileNetv2[60] 2,296,922

174
174

101

41
36
115

0.6603*
0.6691*

0.6961â€ 

0.8747â€ 
0.9175â€ 
0.9420â€ 

* reported in mean average precision, â€ reported in mean accuracy

First, for the selection of confused pairs:

â€¢ COCO, CIFAR-100, CIFAR-10: we choose the most confused pairs in terms of type1conf/type2conf

(defined in Section 2.2).

â€¢ COCO gender: since the major difference between COCO gender and COCO is that the
person class is categorized into man and woman and the confusion related to gender are of
high interest in the study of fairness, we choose pairs with respect to gender. In particular,
we choose the â€œwoman-handbagâ€ pair which is a relatively (but not the most) confused pair.
â€¢ COCO two pairs and CIFAR-10 two pairs: we keep the original pair and choose another one

randomly among the pairs with non-zero confusions with each other.

Second, for the selection of biased triplets:

â€¢ COCO, CIFAR-100, CIFAR-10: we keep the original two classes in the confused pair and
randomly choose a third class such that the bias in terms of cd (defined in Section 2.2.1) is
larger than 0.

â€¢ COCO gender: with the same motivation as in choosing the confused pairs, we choose the
â€œman-woman-skisâ€ triplet which involves both man and woman as well as being a highly
biased triplet.

4.2 Baseline
The baselines we use are the original models and the fine-tuned original model which have been
properly trained as discussed in Section 4.1. We did not compare with methods for instance-wise
fixing or dataset-wise fixing because they target different problems. We also did not compare with
methods developed for class imbalance directly since our methods w-aug, w-os, w-loss can already
be regarded as the adaptations of those methods for the current problem setting.

4.3 Evaluations Metrics
For either fixing the confusion error or bias error, the goal is to reduce error while maintaining the
modelâ€™s overall accuracy.

4.3.1 Rank Sum. Since there are two goals i.e. high accuracy and low confusion/bias a model tries
to achieve, for comparison purpose, we rank each fixed model (including the original model) by
accuracy and confusion respectively. Next, we sum up the two ranks for each model and compare
the rank sums. The model with the smallest rank sum is considered the one that achieves the best
trade-off between accuracy and confusion/bias. Note that we choose this simple metric to give the
most intuitive performance comparisons. A more complicated method like weighted combination
of accuracy and confusion/bias can also be used.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:14

Zhong and Tian, et al.

Statistical Tests. To validate the statistical significance of the results, we additionally conduct
4.3.2
Wilcoxon rank-sum test [9] and Vargha-Delaney effect size test [3, 70] between the proposed
methods and the baseline orig-ft in terms of both accuracy difference and confusion difference.
To highlight the results, we only apply the tests for confusion difference on those already having
smaller average confusion than orig-ft. Next, we only further apply the tests for accuracy difference
for those methods that have statistically significantly (at least at the 0.10 level) lower confusion than
orig-ft. Ideally, when compared with orig-ft, a fixed model should have statistically significantly
smaller confusion and no significant lower accuracy at the same time. If not stated otherwise, we
use 0.05 as the default significance level.

4.4 Hyper-parameter Selection
To show the influence of hyper-parameter choice on each methodâ€™s performance as well as selecting
the best one, under each setting, we apply a grid search on {0.1, 0.3, 0.5, 0.7, 0.9} by running each
method with each hyper-parameter one times and compare the results. Since w-os is a very
fast post-processing method but tends to have very different scales across datasets, for datasets
(CIFAR-10, CIFAR-100) where w-os(0.1) does not reduce confusion/bias much, we further try 0.01,
0.001, and 0.0001 in order until we have found one that can reduce confusion/bias by at least 25%
when compared with that of orig-ft. We present the detailed results of hyper-parameter search in
Appendix A. Since there are two objectives to optimize: mitigating confusion for the target classes
and keeping the overall accuracy of all classes, for each method, we apply the following procedures
to select the best performing one:

â€¢ If there are hyper-parameters that result in the models having confusion at least 25% (one
can change this number according to oneâ€™s need) smaller than the original model, among
them, select the one with the highest overall accuracy.

â€¢ Otherwise, if there are hyper-parameters that result in confusion decreases or stays the same,

among them, select the one with the highest overall accuracy.

â€¢ Otherwise, among all the hyper-parameters, select the one with the highest overall accuracy.
If the selected hyper-parameter does not result in higher accuracy and lower confusion when
compared with the original model, and such a hyper-parameter exists for the current method, this
initially not selected hyper-parameter is additionally selected.

4.5 Research Questions.
We investigate two research questions to evaluate WR for target bug fixing of DNNs:

â€¢ RQ1. Can WR fix confusion errors of DNN models for both single-label classification and

multi-label classification effectively?

â€¢ RQ2. Can WR fix bias errors of DNN models for both single-label classification and multi-label

classification effectively?

5 RESULTS
RQ1. Fixing Confusion Error We first explore if the proposed methods can reduce confusion
errors effectively. In particular, we evaluate them on two settings for the task of multi-label
classification and four settings for the task of single-label classification. We run each method with
the selected hyper-parameter(s) (based on the procedure in Section 4.4) for five times to compare
with each other as well as the baselines, and apply statistical tests to check significance.

Table 2 shows the main results for reducing confusion following the previously mentioned
procedures, where we highlight the top2 (or top3 if tied) methods having the smallest rank sums. In
summary, under every setting, at least two methods can achieve lower confusion while preserving

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:15

decent overall accuracy (or mean average precision). For example, w-os can almost always decrease
the confusion more than 25% under every setting while maintaining accuracy at a reasonable level
(no more than 1%).

Table 2. Results on Confusion

Dataset

Model

Target Method Accuracy
Classes

Confusion

Acc Conf Rank Acc Acc
Rank Rank Sum W VD

Conf Conf
W VD

COCO *

ResNet-50

COCO gender* ResNet-50

CIFAR-100

ResNet-34

person, orig
bus

handbag, orig
woman orig-ft

girl,
woman orig-ft

orig

CIFAR-10

ResNet-18

cat,
dog

VGG-11
with BN

cat,
dog

MobileNetv2 cat,
dog

0.2381

0.0044
0.155

0.1159
0.203
0.0402

0.6604
5
0.6614 Â± 0.0003 0.2329 Â± 0.0084 2
orig-ft
w-aug(0.9) 0.6611 Â± 0.0004 0.2622 Â± 0.0097 4
0.6617 Â± 0.0005 0.1875 Â± 0.0121 1
w-bn(0.9)
w-loss(0.9) 0.6613 Â± 0.0002 0.1244 Â± 0.0094 3
w-dbr(0.9) 0.6604 Â± 0.0004 0.0077 Â± 0.0012 5
8
w-os(0.7)
5
w-os(0.9)

0.6602
0.6604
0.6701
5
0.6710 Â± 0.0002 0.0394 Â± 0.0029 1
w-aug(0.9) 0.6707 Â± 0.0004 0.0442 Â± 0.0069 2
0.6707 Â± 0.0003 0.0225 Â± 0.0059 2
w-bn(0.9)
w-loss(0.7) 0.6707 Â± 0.0002 0.0225 Â± 0.0059 2
w-dbr(0.9) 0.6700 Â± 0.0004 0.0725 Â± 0.0079 6
7
w-os(0.7)

0.6698
0.6998
4
0.7051 Â± 0.0025 0.1430 Â± 0.0179 1
w-aug(0.3) 0.7051 Â± 0.0011 0.1520 Â± 0.0311 1
0.7034 Â± 0.0013 0.1240 Â± 0.0192 3
w-bn(0.9)
w-loss(0.9) 0.6956 Â± 0.0108 0.1440 Â± 0.0074 6
w-dbr(0.9) 0.6818 Â± 0.0046 0.1210 Â± 0.0082 7
5
w-os(0.1)

0.6976
0.8747
5
orig
0.8779 Â± 0.0009 0.0993 Â± 0.0041 1
orig-ft
w-aug(0.7) 0.8778 Â± 0.0016 0.0966 Â± 0.0017 3
0.8489 Â± 0.0007 0.0671 Â± 0.0007 8
w-bn(0.7)
0.8746 Â± 0.0010 0.866 Â± 0.0024 6
w-bn(0.9)
w-loss(0.9) 0.8761 Â± 0.0005 0.1003 Â± 0.0013 4
w-dbr(0.9) 0.8779 Â± 0.0007 0.0891 Â± 0.0024 1
7
w-os(0.1)

0.8654
0.9197
1
orig
0.9180 Â± 0.0020 0.0757 Â± 0.0070 4
orig-ft
w-aug(0.9) 0.9187 Â± 0.0011 0.0724 Â± 0.0040 3
0.9070 Â± 0.0011 0.0539 Â± 0.0032 6
w-bn(0.7)
w-loss(0.9) 0.9129 Â± 0.0019 0.0866 Â± 0.0026 5
w-dbr(0.1) 0.7402 Â± 0.0400 0.0908 Â± 0.0288 8
w-os(0.001) 0.9059
7
0.9197
1
w-os(0.9)
0.9420
orig
1
0.9384 Â± 0.0012 0.0632 Â± 0.0041 5
orig-ft
w-aug(0.5) 0.9388 Â± 0.0015 0.0591 Â± 0.0033 4
0.9159 Â± 0.0018 0.0357 Â± 0.0030 8
w-bn(0.7)
w-loss(0.9) 0.9341 Â± 0.0012 0.0667 Â± 0.0064 6
w-dbr(0.9) 0.9397 Â± 0.0014 0.0571 Â± 0.0033 3
w-os(0.001) 0.9226
7
0.9420
1
w-os(0.3)

0.0525
0.0805
0.0565

0.105
0.0960

0.071
0.081

0.0360
0.0415

7
6
8
4
3
1
2
5

5
4
6
2
3
7
1

7
4
6
3
5
2
1

5
7
6
1
3
8
4
2

6
4
3
2
7
8
1
5

4
7
6
1
8
5
2
3

12
8
12
5
6
6
10
10

10
5
8
4
5
13
8

11
5
7
6
11
9
6

10
8
9
9
9
12
5
9

7
8
6
8
12
16
8
6

5
12
10
9
14
8
9
4

0.347 0.680(m) 0.009 0.0(l)
0.676 0.420(s) 0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)

0.251 0.280(m) 0.009 0.0(l)
0.095 0.180(l) 0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.175 0.240(l)

0.009 0.0(l)
0.009 0.0(l)

0.022 0.060(l)
0.009 0.0(l)

0.009 0.0(l)
0.009 0.0(l)

0.009 0.0(l)
0.009 0.0(l)

0.917 0.48(n)
0.009 0.0(l)

0.009 0.0(l)
0.009 0.0(l)

0.347 0.68(m) 0.296 0.3(m)
0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 1.0(l)

0.009 0.0(l)

* reported in mean average precision; Acc: Accuracy; Conf: Confusion; VD: Vargha-Delaney effect size test;
W: Wilcoxon rank-sum test; n: negligible; s: small; m:medium; l: large

For the multi-label classification task, both w-loss and w-bn achieve good trade-off between mean
average precision and confusion in terms of the rank sum. On both the COCO and COCO gender
datasets, compared with orig-ft, at the 0.05 significance level, both w-loss and w-bn reduce confusion
significantly with large effect size and do not significantly decrease the overall mean average
precision at the same time. For example, on COCO , compared with orig-ft, w-loss(0.9) has much

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:16

Zhong and Tian, et al.

smaller average confusion between person and bus (0.1244 VS 0.2329) and its mean average precision
is only smaller by 0.0001 which is not statistically significant.

For the single-label classification task, w-os usually achieves good trade-offs between accuracy
and confusion. It is among the top2 in terms of the rank sum in three out of the four settings. On
CIFAR-100 , orig-ft is the best in terms of rank sum. In order to mitigate confusion, accuracy has to
be sacrificed. This can be seen from w-os(0.1) where the average confusion decreases from 0.1430
to 0.105 but the average accuracy drops from 0.7051 to 0.6976. Both differences are statistically
significant at the 0.01 level. In the setting of CIFAR-10 and ResNet-18, and CIFAR-10 and VGG-11
with BN, w-dbr(0.9) and w-aug(0.9) are ranked the top respectively. They both achieve significantly
smaller confusion while maintaining the accuracy with no significant drop. On CIFAR-10 with
MobileNetV2, w-os(0.3) is ranked the top and has both significant smaller confusion and higher
accuracy when compared with orig-ft.

Under all the settings, both w-os and w-bn can always decrease confusion significantly, and
are ranked among the top ones in many settings. Although w-loss works very well on COCO and
COCO gender, it works poorly on CIFAR-10 and CIFAR-100 . A deeper exploration of the retraining
process reveals that its retraining processes on CIFAR-10 and CIFAR-100 tend to be very unstable.
For example, on CIFAR-100 , it tends to misclassify dog to cat much more frequently at one epoch
and the reverse at another. w-aug does not perform well in general. This is potentially because
the cause of confusion is not due to the lack of instances belonging to the target classes. This
is particularly true for CIFAR-100 and CIFAR-10 where all the classes have the same number of
instances for each class. w-dbr can reduce confusion for COCO , CIFAR-100 and CIFAR-10 but fails
to do so on COCO gender. One possibility is that the confusion between handbag and woman is
already relatively small and the class centroids between woman and handbag are far away from
each other so the extra loss regularization term does not help much to reduce the confusion further.
Figure 7 shows some examples of the fixed confusion instances. On the CIFAR-10 dataset and
ResNet-18 combination, Figure 7(a)-(b) show two cat images that were classified to dog by the
original model. After applying w-dbr, they are correctly predicted to cat. Figure 7(c)-(d) show two
dog images that were classified to cat by the original model. After applying w-dbr, they are correctly
predicted to dog. Similarly, on the CIFAR-100 dataset and ResNet-34 combination, Figure 7(e)-(f)
show two girl images that were classified to woman by the original model. After applying w-dbr,
they are correctly predicted to girl. Figure 7(g)-(h) show two woman images that were classified
to girl by the original model. After applying w-dbr, they are correctly predicted to woman. On
the COCO dataset and ResNet-50 combination, Figure 7(i)-(j) show two images that contain only
person but the original model mispredicts the presence of bus. After applying w-loss, the model
correctly predicts the presence of person without false positively predicting the presence of bus.
Similarly, Figure 7(k)-(l) show two images with only bus in them but the original model false
positively predicts the presence of person as well. After applying w-loss, the model can correctly
predict the presence of bus while not falsely predicting the presence of person.

Next, we evaluate if the proposed methods can be applied to fix confusion errors among multiple
pairs at the same time by applying the proposed methods to fix confusions of two pairs (one top
confused pair and one randomly picked confused pair) on CIFAR-10 and COCO . Table 3 shows
the results. On COCO , w-loss(0.9) achieves the best trade-off between accuracy and confusion in
terms of rank sum. Compared with orig-ft, it has smaller confusion at 0.01 significance level and no
significant accuracy drop. On CIFAR-10 , w-bn(0.9) achieves the best trade-off. However, compared
with orig-ft, it does not have significantly smaller confusion. In contrast, w-dbr(0.9) has smaller
confusion than orig-ft at the 0.1 significance level and no significant accuracy drop.

Next, besides the overall accuracy and the confusion of the target pair as shown in Table 2 and
Table 3, we explore more fine-grained impact of applying the proposed methods. We showed the

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:17

(a) cat

(b) cat

(c) dog

(d) dog

(e) girl

(f) girl

(g) woman

(h) woman

(i) person

(j) person

(k) bus

(l) bus

Fig. 7. Fixed confusion errors on CIFAR-10 ((a)-(d)), CIFAR-100 ((e)-(h)), and COCO ((i)-(l)) respec-
tively.

Table 3. Results on Confusion (Two Pairs)

Dataset Model

Target
Classes

Method Accuracy

Confusion

Acc Conf Rank Acc Acc
Rank Rank Sum W VD

Conf Conf
W VD

0.2013

COCO * ResNet-50 (person,

0.6604
5
orig
0.6614 Â± 0.0003 0.2266 Â± 0.0088 1
orig-ft
bus),
w-aug(0.9) 0.6609 Â± 0.0004 0.2305 Â± 0.0140 3
(mouse,
keyboard) w-bn(0.7) 0.6606 Â± 0.0002 0.1234 Â± 0.0081 4
w-loss(0.9) 0.6611 Â± 0.0003 0.1307 Â± 0.0158 2
w-dbr(0.9) 0.6589 Â± 0.0002 0.1629 Â± 0.0068 7
w-os(0.7) 0.6595
6
0.8747
7
CIFAR-10 ResNet-18 (cat,
0.8774 Â± 0.0010 0.0680 Â± 0.0025 1
dog),
(automobile, w-aug(0.7) 0.8764 Â± 0.0012 0.0678 Â± 0.0011 4
w-bn(0.5) 0.8471 Â± 0.0004 0.0495 Â± 0.0007 8
truck)
w-bn(0.9) 0.8772 Â± 0.0008 0.0649 Â± 0.0028 2
w-loss(0.9) 0.8755 Â± 0.0024 0.0670 Â± 0.0034 5
w-dbr(0.9) 0.8770 Â± 0.0018 0.0650 Â± 0.0034 3
w-os(0.9) 0.8749
6

0.0968
0.0670

orig
orig-ft

0.0660

5
6
7
2
3
4
1

5
8
7
1
2
5
3
4

10
7
10
6
5
11
7

12
9
11
9
4
10
6
10

0.009 0.0(l)
0.009 0.0(l)
0.251 0.280(m) 0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)
0.175 0.240(l)

0.465 0.360(s) 0.076 0.160(l)
0.602 0.400(s)
0.009 0.0(l)

* reported in mean average precision; Acc: Accuracy; Conf: Confusion; VD: Vargha-Delaney effect size test;
W: Wilcoxon rank-sum test; n: negligible; s: small; m:medium; l: large

confusion matrices for all the classes under different methods under the setting of CIFAR-10 and
VGG-11 with BN in Figure 8. In Figure 9, we additionally visualize the confusion from the target
classes to other classes for all the methods. In both the original model and the fine-tuned original
model, dog(label 5) is highly confused with cat(label 3) than any other classes. Both w-os and w-bn
reduce the confusion between dog and cat from both directions. As the trade-off, the confusion

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:18

Zhong and Tian, et al.

between dog/cat and other classes increase. This is consistent with the intuition that both methods
contract the decision boundaries of the target classes. It also worth noting that w-bn provides a
relatively uniform distribution of the confusion among all the pairs after the fixing. This might be
a desirable property if a user does not want to overburden one particular non-target class in terms
of the confusion distribution. The result also suggests a future exploration direction of our method:
by adjusting hyper-parameters, one can optimize the model such that the maximum pair-wise
confusion is the lowest. w-aug similarly decreases the confusion between dog and cat but to a
smaller extent. At the same time, it also increases less of the confusion for other pairs. Although w-
dbr reduces the confusion between dog and cat, the main reduction comes from the zero confusion
from dog to cat after w-dbr. The confusion from cat to dog actually increases. Besides, it increases
the modelâ€™s confusion from many classes to dog and frog (as shown in Figure 8f). w-loss reduces
some confused instances from cat to dog, it fails to do so for instances from dog to cat. The overall
observation is that there is no free lunch for reducing the confusion for the target classes, the
confusion for others has to be sacrificed to some extent.

Since w-os can be applied when no training data is available or retraining is allowed and
can reduce confusion by a significant amount (>25%) while only slightly sacrificing the overall
performance (<1%) under every setting, we conduct a more comprehensive ablation study on its
hyper-parameter ğœŒ to explore its trade-off between confusion and accuracy. Figure 10 shows the
results. Note that by decreasing ğœŒ, the confusion decreases and accuracy decrease at the same
time. Thus, a user can decide what parameter to use depending on the significance of accuracy
and confusion. The influence of the hyper-parameter for other methods can also be found in
Appendix A.

Result 1: The proposed method WR can reduce confusion errors for both single-label and
multi-label image classification. Under every setting, at the 0.05 significance level, compared with
the fine-tuned original model baseline, at least one fixing method can achieve significant lower
confusion. Besides, under four out of six settings, the methods also do not have significant accuracy
drop at the same time. Under the rest two settings, the sacrificed accuracy is less than 1%. The
proposed methodâ€™s also generalize to reduce confusion errors for multiple pairs.

RQ2. Fixing Bias Error In this RQ, we explore if the proposed methods can fix bias errors. The
settings and procedures are similar to those for evaluating confusion error fixing. Table 4 shows
the results under different settings. In summary, the general trend is similar to fixing the confusion
error. Under every setting, at least two of the proposed methods can achieve lower bias while
preserving decent overall accuracy (or mean average precision).

For the multi-label classification task, w-loss strikes the best trade-off between mean average
precision and bias in terms of the rank sum. On both the COCO and COCO gender datasets, at the
0.05 significance level, compared with orig-ft, w-loss has lower bias with large effect size and does
not have significant mean average precision drop. For example, on COCO , compared with orig-ft,
w-loss(0.9) has much lower average bias between person and clock with respect to bus (0.1240 VS
0.2314) while only has 0.0001 smaller mean average precision on average.

For the single-label classification task, w-os is ranked among the top2 under every setting.
However, compared with orig-ft, at the 0.05 significance level, although w-os can always reduce
bias, it has to sacrifice the accuracy as well. Under the setting of CIFAR-100 and ResNet-34, and
CIFAR-10 and MobileNetv2, in order to reduce bias, any of the proposed method has to sacrifice
accuracy. Under the setting of CIFAR-10 and ResNet-18, and CIFAR-10 and VGG-11 with BN, at the
0.05 significance level, w-bn(0.9) and w-aug(0.3) can reduce bias while still maintain the accuracy.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:19

(a) orig-ft(fine-tuned original model)

(b) w-aug(0.9)

(c) w-bn(0.7)

(d) w-loss(0.9)

(e) w-os(0.001)

(f) w-dbr(0.1)

Fig. 8. Confusion matrices of the VGG11-BN after applying different methods on it.

For example, on CIFAR-10 and ResNet-18, compared with orig-ft, w-bn(0.9) has lower bias (0.0619
VS 0.0806) with large effect size and has only accuracy drop by 0.0002 which is not statistically
significant.

Similar to fixing confusion errors, under all the settings, w-os and w-bn work reasonably well
and give decent trade-off between accuracy and bias. w-bn can reduce bias significantly in most
settings but tends to be slightly worse than w-os overall. w-loss works very well for the multi-label

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0040.0170.0080.00500.0040.0060.0170.010.004000.00100.001000.0070.0270.0190.00100.0270.0210.0140.0230.0060.0030.0020.0060.0010.02200.020.0830.0190.0040.0030.0030.00300.0120.02200.0150.010.0130.0010.0010.00700.0140.0850.01800.0080.0110.0020.0020.00400.0120.020.0030.00600.00100.0010.0030.0010.0050.0140.0140.0130.00300.0010.0020.0210.0040.0020.0050.0010.00100.00100.0080.0090.0260.0020.00200.0010.0020.0010.01200.000.020.040.060.080.100.120.14airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0050.0140.0070.00400.0010.0030.0190.0060.005000.00100.00100.0010.0060.0210.012000.0260.0250.0130.0140.0030.0030.0040.0060.0010.0300.0210.0690.0230.0120.0040.0070.0030.0010.020.01600.0120.010.0130.00100.0030.0010.0120.0740.01600.0080.0110.0030.0020.00200.0170.0170.0070.00500.0010.0010.0010.00800.0040.0170.0130.0130000.0030.0340.0060.0050.001000.0010.00100.0110.0120.0310.0020.00300.0010.0030.0010.01200.000.020.040.060.080.100.120.14airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0050.0140.0010.00900.0010.0050.0260.0120.0050000.0010.001000.0060.0230.018000.0050.0260.0060.0210.0050.0020.0050.0130.0070.05600.0580.060.0460.0220.0150.0160.0020.0010.020.00200.0040.0120.01400.0010.010.0040.0340.0550.04400.0210.0280.0030.010.00500.0130.0040.009000.00200.0030.0040.0010.0040.0010.0160.0050.00100.0020.0050.0170.0090.0040000.0020.00300.0150.0050.0310.0010.00100.0010.0010.0010.01100.000.020.040.060.080.100.120.14airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0030.0120.0090.00300.0020.0040.020.010.004000.00200.001000.010.0290.0210.00100.0320.030.020.0130.0090.0030.0020.00900.01600.0290.0650.0180.0090.0050.0040.0050.0010.0150.02400.0150.0040.0180.0010.0020.0080.0020.0070.110.01600.0050.0120.0010.0010.00300.0180.0370.0080.00400.00200.0020.00700.0080.0160.0090.0120.001000.0020.0240.0020.0030.0060000.00200.0130.0070.030.0020.00400.002000.01300.000.020.040.060.080.100.120.14airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0060.0150.0010.00700.0020.0050.0250.0070.0060000.00100.00100.0060.0240.0150.00100.0040.0280.0050.0250.0060.0030.0030.0120.0050.07600.0480.0560.0610.030.0130.0130.0030.0010.0150.00500.0050.0120.01200.0010.01100.0370.0490.04900.0250.040.0030.0050.00400.0150.0040.0050.0020000.0010.00500.0070.0040.0140.0030.001000.0030.0150.0080.00500.00100000.0070.0080.0260.0010.0010.00100.0020.0030.01200.000.020.040.060.080.100.120.14airplaneautomobilebirdcatdeerdogfroghorseshiptruckairplaneautomobilebirdcatdeerdogfroghorseshiptruck00.0110.02300.0020.0070.0640.0050.0420.0220.0100.00100.0010.0060.0120.0010.0120.0470.0690.002000.0370.110.230.0080.0060.0070.01200.00900.0120.120.830.0090.0030.0050.0140.0020.03000.130.210.0310.0010.0010.00700.00500.00400.110.0070.00200.0050.0020.02700.0110.04800.0020.0050.0020.0140.0020.0090.0010.0250.130.09700.0020.0080.0550.0140.00800.0010.0040.037000.0210.0250.0560.00400.0020.0070.0250.0040.01800.000.020.040.060.080.100.120.14000:20

Zhong and Tian, et al.

(a) confusion between dog and other classes

(b) confusion between cat and other classes

Fig. 9. Confusion between target classes and non-target classes for VGG-11 with BN on CIFAR-
10 after applying each method.

Fig. 10. Accuracy and Confusion trade-off of different parameters for w-os.

classification task but not for the single-label classification task. w-aug performs much better on
CIFAR-100 and CIFAR-10 than on COCO and COCO gender. w-dbr can reduce bias for COCO ,
COCO gender and CIFAR-100 but fails to do so for CIFAR-10 .

Figure 7 shows two examples of the fixed bias instances under the setting of COCO gender and
ResNet-50. Figure 11(a) shows an image containing a woman and a skis but the original model
classifies the woman to man. After applying w-loss, the model correctly predicts the presence of
woman and skis. Figure 11(b) shows an image containing several woman, man and skis while the
original model only predicts the presence of only man and skis while missing woman. After fixing
the model using w-loss, the model successfully predicts the presence woman, man and skis.

Result 2: The proposed method WR can effectively reduce bias errors for both single-label and
multi-label image classification. Under every setting, compared with the fine-tuned original model
baseline, at least one fixing method can achieve significant lower bias. Besides, under four out of
six settings, the method does not have significant accuracy drop at the same time. Under the rest
two settings, significant lower bias can also be achieved but accuracy has to be sacrificed for 1%
and 5% respectively.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

airplaneautomobilebirdcatdeerdogfroghorseshiptrucklabels0.000.020.040.060.080.100.120.14ConfusionMisclassification dogs(5) -> othersorigfinetunew-aug(0.9)w-bn(0.7)w-loss(0.9)w-os(0.001)w-dbr(0.1)airplaneautomobilebirdcatdeerdogfroghorseshiptrucklabels0.000.020.040.060.080.100.120.14ConfusionMisclassification cats(3) -> othersorigfinetunew-aug(0.9)w-bn(0.7)w-loss(0.9)w-os(0.001)w-dbr(0.1)81828384858687accuracy(%)2345678910confusion(%)cifar10w-os 0.001w-os 0.01w-os 0.1w-os 0.3w-os 0.5orig65.9865.9966.0066.0166.0266.0366.04accuracy(%)0510152025confusion(%)cocow-os 0.5w-os 0.6w-os 0.7w-os 0.8w-os 0.9origRepairing Group-Level Errors for DNNs Using Weighted Regularization

000:21

Table 4. Results on Bias

Accuracy

Bias

Bias Bias
Acc Bias Rank Acc Acc
Rank Rank Sum W VD W VD

Dataset

Model

COCO *

ResNet-50

COCO gender* ResNet-50

Target Method
Classes

orig

bus,
person, orig-ft
clock w-aug(0.9)
w-bn(0.3)
w-loss(0.9)
w-dbr(0.1)
w-os(0.7)
w-os(0.9)

orig

skis,
woman, orig-ft
man

w-aug(0.9)
w-bn(0.9)
w-loss(0.9)
w-dbr(0.3)
w-os(0.5)

CIFAR-100

ResNet-34 woman, orig

CIFAR-10

ResNet-18

girl,
boy

dog,
cat,
bird

orig-ft
w-aug(0.7)
w-bn(0.7)
w-bn(0.9)
w-loss(0.9)
w-dbr(0.1)
w-os(0.01)

orig
orig-ft
w-aug(0.7)
w-bn(0.7)
w-bn(0.9)
w-loss(0.9)
w-dbr(0.9)
w-os(0.01)
w-os(0.9)

0.2366

0.1151
0.2015
0.2521

0.6604
4
0.6614 Â± 0.0003 0.2314 Â± 0.0084 1
0.6607 Â± 0.0005 0.2483 Â± 0.0209 3
0.6575 Â± 0.0005 0.1722 Â± 0.0176 7
0.6613 Â± 0.0002 0.1240 Â± 0.0098 2
0.6537 Â± 0.0007 0.1949 Â± 0.0179 8
0.6602
6
0.6604
4
0.6701
4
0.6709 Â± 0.0002 0.2501 Â± 0.0153 1
0.6708 Â± 0.0002 0.2437 Â± 0.0106 2
0.6684 Â± 0.0002 0.1771 Â± 0.0054 6
0.6708 Â± 0.0001 0.1370 Â± 0.0207 2
0.6677 Â± 0.0003 0.1311 Â± 0.0564 7
0.6693
0
5
0.07
0.6988
4
0.7042 Â± 0.0010 0.0680 Â± 0.0164 2
0.7051 Â± 0.0011 0.0710 Â± 0.0108 1
0.6779 Â± 0.0009 0.0450 Â± 0.0079 7
0.7041 Â± 0.0007 0.0700 Â± 0.0190 3
0.6939 Â± 0.0128 0.0780 Â± 0.0175 5
0.0472 Â± 0.0111 0.0590 Â± 0.0277 8
0.6938
6
0.8747
5
0.8776 Â± 0.0011 0.0806 Â± 0.0044 1
0.8760 Â± 0.0008 0.0651 Â± 0.0031 3
0.8617 Â± 0.0015 0.0553 Â± 0.0023 8
0.8774 Â± 0.0020 0.0619 Â± 0.0030 2
0.8734 Â± 0.0011 0.0654 Â± 0.0058 6
0.8712 Â± 0.0009 0.0925 Â± 0.0028 7
0.8181
9
0.8751
4
0.9197
2
0.9181 Â± 0.0023 0.0603 Â± 0.0074 4
0.9171 Â± 0.0015 0.0497 Â± 0.0055 5
0.9183 Â± 0.0017 0.0530 Â± 0.0074 3
0.9122 Â± 0.0009 0.0602 Â± 0.0039 7
0.9170 Â± 0.0015 0.0634 Â± 0.0072 6
8
1

0.0425
0.063
0.0675

0.045
0.074

7
6
8
3
2
4
1
5

7
6
5
4
3
2
1

5
4
7
1
5
8
3
1

7
8
5
2
3
6
9
1
4

8
6
2
3
5
7
1
4

3
5
4
2
7
6
1

11
7
11
10
4
12
7
9

11
7
7
10
5
9
6

9
6
8
8
8
13
11
7

12
9
8
10
5
12
16
10
8

10
10
7
6
12
13
9
5

4
7
7
7
14
9
7

0.009 0.0(l)
0.009 0.0(l)
0.531 0.380(s) 0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)

0.465 0.360(s)
0.009 0.0(l)
0.009 0.0(l)
0.296 0.3(m) 0.009 0.0(l)
0.016 0.04(l)
0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)

0.009 0.0(l)

0.028 0.080(l)

0.009 0.0(l)

0.403 0.340(s)
0.009 0.0(l)

0.028 0.08(l) 0.009 0.0(l)
0.009 0.0(l)
0.009 0.0(l)
0.465 0.36(s) 0.009 0.0(l)
0.009 0.0(l)

0.012 0.020(l)

0.009 0.0(l)
0.009 0.0(l)

0.009 0.0(l)
0.009 0.0(l)

0.251 0.28(m) 0.047 0.120(l)

0.251 0.280(m)
0.917 0.480(n)

0.009 0.0(l)

0.009 0.0(l)
0.117 0.2(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

0.009 0.0(l)

VGG-11
with BN

dog,
cat,
bird

MobileNetv2 dog,
cat,
bird

orig
orig-ft
w-aug(0.3)
w-bn(0.9)
w-loss(0.9)
w-dbr(0.9)
w-os(0.001) 0.9023
0.9198
w-os(0.7)
0.942
1
0.9384 Â± 0.0012 0.0468 Â± 0.0047 2
0.9383 Â± 0.0010 0.0418 Â± 0.0018 3
0.8981 Â± 0.0017 0.0272 Â± 0.0025 5
0.3494 Â± 0.0420 0.0916 Â± 0.0482 7
0.9383 Â± 0.0004 0.0615 Â± 0.0035 3
6

orig
orig-ft
w-aug(0.5)
w-bn(0.5)
w-loss(0.1)
w-dbr(0.9)
w-os(0.0001) 0.8943

0.041
0.056
0.0415

0.0240

* reported in mean average precision; Acc: Accuracy; VD: Vargha-Delaney effect size test; W: Wilcoxon
rank-sum test; n: negligible; s: small; m:medium; l: large

6 RELATED WORK

6.1 Software Repairing
Automatic software repairing is very challenging and most of existing work focus on traditional
software.[53]. Traditional automatic repairing techniques include random or guided mutation
of AST(Abstract Syntax Tree)[11, 17, 36, 37, 56, 62, 73, 74], static program analysis or symbolic
execution/concrete execution[18, 39, 40, 54, 55, 67]. The most recent techniques involve language
models training and program synthesis[23, 78]. All these techniques proposed to repair traditional

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:22

Zhong and Tian, et al.

(a) woman, skis

(b) woman, man, skis

Fig. 11. Fixed bias errors on COCO gender.

programs such as C, C++, Java or Python, cannot work on DNN based software because there is no
program logic or AST in DNN models. In this paper, we propose a generic method for automatic
target repairing group-wise errors in DNN based software.

6.2 DNN Testing and Repairing
An increasing number of works in SE for AI area focus on DNN testing and repairing. The testing
techniques usually leverage metamorphic relation as oracle and coverage guided image trans-
formation or perturbation for generating test cases[31, 42â€“44, 57, 66, 77, 86]. Data augmentation
and retraining techniques are usually proposed for repairing DNN models in improving overall
accuracy[45, 59, 64]. There are also works in improving robustness of models against adversar-
ial instances[15, 19, 30, 46, 47, 49, 52, 69, 71, 72, 76, 85]. All of these papers focus on repairing
instance-wise or dataset-wise errors. In contrast, our paper focuses on fixing group-wise errors.

6.3 Fairness
Fairness is an important problem from both a theoretical and a practical perspective [6, 41, 81, 82].
Related works in fairness usually define a fairness criteria and optimize the original objective while
satisfying the fairness criteria [4, 12, 13, 25, 35, 51]. These properties are defined at individual [13,
32, 34] or group levels [8, 25, 80]. Our paper focuses on fixing errors based on a group-level fairness
definition called bias error proposed in [68]. To the best of our knowledge, methods on repairing
this bias error have not been proposed before.

7 THREATS TO VALIDITY AND DISCUSSION
There are many potential ways to fix group-level errors of DNNs. To mitigate this threat, we
propose and compare the performance of five different methods along with two baselines. For each
method, we set a parameter to make trade-off between accuracy and confusion/bias and show
results of each method when using at least five different hyper-parameters.

There are many datasets and models, which can be used for the evaluation purpose. We choose
six combinations of four widely used datasets and five for image classification. Besides, DNN
training is stochastic so the results may have some fluctuations. We repeat each method with top
performing hyper-parameter(s) for five times and apply statistical tests to check the significance
of the results. Lastly, our method can be potentially applied to DNN models used in applications
beyond image classifications such as object detection and recommendation systems. We leave that
for future work.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:23

8 CONCLUSION
In this work, we propose a generic method called Weighted Regularization(WR) consists of five
concrete methods that can fix group-level errors for DNNs with different trade-offs. To the best of
our knowledge, this is the first work proposing, exploring and comparing target fixing methods,
which can be applied in different stages of DNN retraining or inference, on repairing group-level
DNN model errors. Our experimental results show that WR can effectively fix confusion and bias
errors and these methods all have their pros, cons and applicable scenarios.

REFERENCES
[1] 2018. Pytorch BATCHNORM2D. https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html.
[2] Alekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, John Langford, and Hanna Wallach. 2018. A Reductions Approach
to Fair Classification. In Proceedings of the 35th International Conference on Machine Learning, Jennifer G. Dy and
Andreas Krause (Eds.), Vol. 80. JMLR, Stanford, CA, 60â€“69.

[3] Andrea Arcuri and Lionel Briand. 2014. A Hitchhikerâ€™s guide to statistical tests for assessing randomized algorithms
in software engineering. Software Testing, Verification and Reliability 24, 3 (2014), 219â€“250. https://doi.org/10.1002/
stvr.1486

[4] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2018. Fairness and Machine Learning. fairmlbook.org. http:

//www.fairmlbook.org.

[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives.

IEEE transactions on pattern analysis and machine intelligence 35, 8 (2013), 1798â€“1828.

[6] Yuriy Brun and Alexandra Meliou. 2018. Software Fairness. In Proceedings of the 2018 26th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Lake Buena
Vista, FL, USA) (ESEC/FSE 2018). ACM, New York, NY, USA, 754â€“759. https://doi.org/10.1145/3236024.3264838
[7] Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. 2018. A systematic study of the class imbalance problem in
convolutional neural networks. Neural Networks 106 (2018), 249â€“259. https://doi.org/10.1016/j.neunet.2018.07.011
[8] T. Calders, F. Kamiran, and M. Pechenizkiy. 2009. Building Classifiers with Independency Constraints. In 2009 IEEE

International Conference on Data Mining Workshops. 13â€“18.

[9] J. Anthony Capon. 1991. Elementary Statistics for the Social Sciences: Study Guide. (1991).
[10] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-Balanced Loss Based on Effective

Number of Samples. In CVPR.

[11] Vidroha Debroy and W Eric Wong. 2010. Using mutation to automatically suggest fixes for faulty programs. In 2010

Third International Conference on Software Testing, Verification and Validation. IEEE, 65â€“74.

[12] Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. 2018. Empirical Risk
Minimization Under Fairness Constraints. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, MontrÃ©al, Canada. 2796â€“2806. http:
//papers.nips.cc/paper/7544-empirical-risk-minimization-under-fairness-constraints

[13] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. 2012. Fairness Through
Awareness. In Proceedings of the Innovations in Theoretical Computer Science Conference abs/1104.3913 (2012), 214â€“226.
[14] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2019. Exploring the

landscape of spatial robustness. In International Conference on Machine Learning. 1802â€“1811.

[15] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2019. A Rotation and a
Translation Suffice: Fooling CNNs with Simple Transformations. In Proceedings of the 36th international conference on
machine learning (ICML).

[16] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn
Song. 2017. Robust Physical-World Attacks on Machine Learning Models. arXiv preprint arXiv:1707.08945 (2017).
[17] Stephanie Forrest, ThanhVu Nguyen, Westley Weimer, and Claire Le Goues. 2009. A genetic programming approach
to automated software repair. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation.
947â€“954.

[18] Qing Gao, Yingfei Xiong, Yaqing Mi, Lu Zhang, Weikun Yang, Zhaoping Zhou, Bing Xie, and Hong Mei. 2015. Safe
memory-leak fixing for c programs. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,
Vol. 1. IEEE, 459â€“470.

[19] Xiang Gao, Ripon K Saha, Mukul R Prasad, and Abhik Roychoudhury. 2020. Fuzz testing based data augmentation to
improve robustness of deep neural networks. In 2020 IEEE/ACM 42nd International Conference on Software Engineering
(ICSE). IEEE, 1147â€“1158.

[20] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning. Vol. 1. MIT press Cambridge.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:24

Zhong and Tian, et al.

[21] Loren Grush. 2015. Google engineer apologizes after Photos app tags two black people as gorillas. (2015). https:

//www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-black-people-gorillas

[22] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibration of Modern Neural Networks. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia) (ICMLâ€™17).
JMLR.org, 1321â€“1330.

[23] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fixing common c language errors by

deep learning. In Proceedings of the aaai conference on artificial intelligence, Vol. 31.

[24] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised Learning. In Proceedings
of the 30th International Conference on Neural Information Processing Systems (Barcelona, Spain) (NIPSâ€™16). Curran
Associates Inc., Red Hook, NY, USA, 3323â€“3331.

[25] Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised Learning. In Proceedings of the
30th International Conference on Neural Information Processing Systems (Barcelona, Spain) (NIPSâ€™16). USA, 3323â€“3331.
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition. 770â€“778.

[27] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In International conference on machine learning. PMLR, 448â€“456.

[28] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repairing Deep Neural Networks: Fix Patterns

and Challenges. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). 1135â€“1146.

[29] J.M. Johnson and T.M. Khoshgoftaar. 2019. Survey on deep learning with class imbalance. J Big Data 6 (2019).

https://doi.org/10.1186/s40537-019-0192-5

[30] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efficient SMT Solver

for Verifying Deep Neural Networks. Springer International Publishing, Cham, 97â€“117.

[31] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system testing using surprise adequacy. In

Proceedings of the 41st International Conference on Software Engineering. IEEE Press, 1039â€“1049.

[32] Michael P. Kim, Omer Reingold, and Guy N. Rothblum. 2018. Fairness Through Computationally-Bounded Awareness.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018) (2018).

[33] Alex Krizhevsky. 2012. Learning Multiple Layers of Features from Tiny Images. University of Toronto (05 2012).
[34] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In Advances in Neural

Information Processing Systems 30. 4066â€“4076.

[35] Alexandre Louis Lamy, Ziyuan Zhong, Aditya Krishna Menon, and Nakul Verma. 2019. Noise-tolerant fair classification.

CoRR abs/1901.10837 (2019). arXiv:1901.10837 http://arxiv.org/abs/1901.10837

[36] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer. 2012. A systematic study of automated
program repair: Fixing 55 out of 105 bugs for $8 each. In 2012 34th International Conference on Software Engineering
(ICSE). IEEE, 3â€“13.

[37] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2011. Genprog: A generic method for

automatic software repair. Ieee transactions on software engineering 38, 1 (2011), 54â€“72.

[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence
Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer,
740â€“755.

[39] Francesco Logozzo and Thomas Ball. 2012. Modular and Verified Automatic Program Repair. In Proceedings of the
ACM International Conference on Object Oriented Programming Systems Languages and Applications (Tucson, Arizona,
USA) (OOPSLA â€™12). ACM, New York, NY, USA, 133â€“146. https://doi.org/10.1145/2384616.2384626

[40] Francesco Logozzo and Matthieu Martel. 2013. Automatic repair of overflowing expressions with abstract interpretation.

arXiv preprint arXiv:1309.5148 (2013).

[41] Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. 2011. k-NN as an implementation of situation testing for
discrimination discovery and prevention. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge
discovery and data mining. ACM, 502â€“510.

[42] Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao. 2019. Deepct: Tomographic combinatorial
testing for deep learning systems. In 2019 IEEE 26th International Conference on Software Analysis, Evolution and
Reengineering (SANER). IEEE, 614â€“618.

[43] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu,
Jianjun Zhao, and Yadong Wang. 2018. DeepGauge: Multi-granularity Testing Criteria for Deep Learning Systems.
(2018), 120â€“131. https://doi.org/10.1145/3238147.3238202

[44] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al.
2018. Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE 29th International Symposium on Software
Reliability Engineering (ISSRE). IEEE, 100â€“111.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:25

[45] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama. 2018. MODE: Automated Neural
Network Model Debugging via State Differential Analysis and Input Selection. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Lake
Buena Vista, FL, USA) (ESEC/FSE 2018). ACM, New York, NY, USA, 175â€“186. https://doi.org/10.1145/3236024.3236082
[46] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E
Houle, and James Bailey. 2018. Characterizing adversarial subspaces using local intrinsic dimensionality. In International
Conference on Learning Representations (ICLR).

[47] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. 2018. Towards Deep Learning Models Resistant to

Adversarial Attacks. In International Conference on Learning Representations (ICLR).

[48] MalletsDarker. 2018. I took a few shots at Lake Louise today and Google offered me this panorama. (2018). https:

//www.reddit.com/r/funny/comments/7r9ptc/i_took_a_few_shots_at_lake_louise_today_and/dsvv1nw/

[49] Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. 2019. Metric Learning for Adversarial
Robustness. In Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 478â€“489. http://papers.nips.cc/paper/8339-metric-
learning-for-adversarial-robustness.pdf

[50] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A Survey on Bias and
Fairness in Machine Learning. ACM Comput. Surv. 54, 6, Article 115 (jul 2021), 35 pages. https://doi.org/10.1145/3457607
[51] Aditya Krishna Menon and Robert C. Williamson. 2018. The cost of fairness in binary classification. In Conference
on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA. 107â€“118. http:
//proceedings.mlr.press/v81/menon18a.html

[52] Matthew Mirman, Timon Gehr, and Martin Vechev. 2018. Differentiable abstract interpretation for provably robust

neural networks. In International Conference on Machine Learning. 3575â€“3583.

[53] Martin Monperrus. 2018. Automatic software repair: a bibliography. ACM Computing Surveys (CSUR) 51, 1 (2018),

1â€“24.

[54] Paul Muntean, Vasantha Kommanapalli, Andreas Ibing, and Claudia Eckert. 2014. Automated generation of buffer
overflow quick fixes using symbolic execution and SMT. In International Conference on Computer Safety, Reliability,
and Security. Springer, 441â€“456.

[55] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. Semfix: Program repair via

semantic analysis. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 772â€“781.

[56] Mihai Nica, Simona Nica, and Franz Wotawa. 2013. On the use of mutations and testing for debugging. Software:

practice and experience 43, 9 (2013), 1121â€“1142.

[57] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Automated whitebox testing of deep learning

systems. In proceedings of the 26th Symposium on Operating Systems Principles. 1â€“18.

[58] Xuhong Ren, Bing Yu, Hua Qi, Felix Juefei-Xu, Zhuo Li, Wanli Xue, Lei Ma, and Jianjun Zhao. 2020. Few-Shot Guided
Mix for DNN Repairing. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). 717â€“721.
https://doi.org/10.1109/ICSME46990.2020.00079

[59] Xuhong Ren, Bing Yu, Hua Qi, Felix Juefei-Xu, Zhuo Li, Wanli Xue, Lei Ma, and Jianjun Zhao. 2020. Few-Shot Guided
Mix for DNN Repairing. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE,
717â€“721.

[60] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2:
Inverted Residuals and Linear Bottlenecks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.
4510â€“4520. https://doi.org/10.1109/CVPR.2018.00474

[61] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2018. How Does Batch Normalization Help
Optimization?. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/
905056c1ac1dad141560467e0a99e1cf-Paper.pdf

[62] Eric Schulte, Stephanie Forrest, and Westley Weimer. 2010. Automated program repair through the evolution of
assembly code. In Proceedings of the IEEE/ACM international conference on Automated software engineering. 313â€“316.
[63] Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks for large-scale image recognition.

In International Conference on Learning Representations (ICLR).

[64] Jeongju Sohn, Sungmin Kang, and Shin Yoo. 2019. Search based repair of deep neural networks. arXiv preprint

arXiv:1912.12463 (2019).

[65] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: Automated testing of deep-neural-network-
driven autonomous cars. In International Conference of Software Engineering (ICSE), 2018 IEEE conference on. IEEE.
[66] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated testing of deep-neural-network-

driven autonomous cars. In Proceedings of the 40th international conference on software engineering. 303â€“314.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:26

Zhong and Tian, et al.

[67] Yuchi Tian and Baishakhi Ray. 2017. Automatically diagnosing and repairing error handling bugs in c. In Proceedings

of the 2017 11th Joint Meeting on Foundations of Software Engineering. 752â€“762.

[68] Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, and Baishakhi Ray. 2020. Testing DNN Image Classifier for

Confusion & Bias Errors. In International Conference of Software Engineering (ICSE).

[69] Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. Ensemble

adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204 (2017).

[70] AndrÃ¡s Vargha and Harold D. Delaney. 2000. A Critique and Improvement of the CL Common Language Effect
Size Statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics 25, 2 (2000), 101â€“132. https:
//doi.org/10.3102/10769986025002101 arXiv:https://doi.org/10.3102/10769986025002101

[71] Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. 2018. Mixtrain: Scalable training of formally robust neural

networks. arXiv preprint arXiv:1811.02625 (2018).

[72] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal Security Analysis of Neural

Networks using Symbolic Intervals. (2018).

[73] Westley Weimer, Stephanie Forrest, Claire Le Goues, and ThanhVu Nguyen. 2010. Automatic program repair with

evolutionary computation. Commun. ACM 53, 5 (2010), 109â€“116.

[74] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest. 2009. Automatically finding patches using genetic programming.

In ICSE. 364â€“374.

[75] D.H. Wolpert and W.G. Macready. 1997. No free lunch theorems for optimization. IEEE Transactions on Evolutionary

Computation 1, 1 (1997), 67â€“82. https://doi.org/10.1109/4235.585893

[76] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. 2018. Scaling provable adversarial defenses. In

Advances in Neural Information Processing Systems. 8400â€“8409.

[77] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and
Simon See. 2019. DeepHunter: a coverage-guided fuzz testing framework for deep neural networks. In Proceedings of
the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. 146â€“157.

[78] Geunseok Yang, Kyeongsic Min, and Byungjeong Lee. 2020. Applying deep learning algorithm to automatic bug
localization and repair. In Proceedings of the 35th Annual ACM Symposium on Applied Computing. 1634â€“1641.

[79] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. 2019. CutMix:
Regularization Strategy to Train Strong Classifiers with Localizable Features. In International Conference on Computer
Vision (ICCV).

[80] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. 2017. Fairness Beyond
Disparate Treatment & Disparate Impact: Learning Classification Without Disparate Mistreatment. In Proceedings of
the 26th International Conference on World Wide Web (Perth, Australia). 1171â€“1180.

[81] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. 2017. Fairness constraints:
Mechanisms for fair classification. In Proceedings of the 20th International Conference on Artificial Intelligence and
Statistics ((AISTATS) 2017, Vol. 54). JMLR.

[82] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. 2013. Learning Fair Representations. In

Proceedings of the 30th International Conference on Machine Learning. 325â€“333.

[83] Hao Zhang and W.K. Chan. 2019. Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models. In 2019
34th IEEE/ACM International Conference on Automated Software Engineering (ASE). 376â€“387. https://doi.org/10.1109/
ASE.2019.00043

[84] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing
Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods
in Natural Language Processing. 2941â€“2951.

[85] Ziyuan Zhong, Yuchi Tian, and Baishakhi Ray. 2021. Understanding Local Robustness of Deep Neural Networks under

Natural Variations. Fundamental Approaches to Software Engineering 12649 (2021), 313.

[86] Ray B. Zhong Z, Tian Y. 2021. Understanding Local Robustness of Deep Neural Networks under Natural Variations. In

Fundamental Approaches to Software Engineering.

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:27

A ADDITIONAL RESULTS ON HYPER-PARAMETER SELECTION
In this section, we provide the detailed results of our hyper-parameter search process. The numbers
colored in blue represent being selected based on the first criteria and those colored in yellow
are those additionally selected based on our second criteria. The details of the selection have
been discussed in Section 4.4. We then run all the selected settings for five runs with different
random seeds and report the results in RQ1 and RQ2. The results presented here also reflect the
influence of the choice of the hyper-parameters on each method. In particular, the smaller ğœ‚ is,
w-os and w-bn have lower confusion/bias and accuracy (or mean average precision). The influence
of the hyper-parameters for other methods are also usually monotonic for accuracy. However, the
influence on confusion/bias usually varies case by case.

Table 5. Results on CIFAR-10 and ResNet-18

Table 6. Results on CIFAR-10 and VGG-11 w/ BN

confusion
model

bias
model

model

acc

conf

acc

bias

0.8747 0.096 0.8747 0.074
orig
orig-ft
0.8775 0.0978 0.8763 0.084
w-aug 0.1 0.8456 0.096 0.8335 0.0655
w-aug 0.3 0.8731 0.0985 0.8683 0.0675
w-aug 0.5 0.8757 0.093 0.8731 0.0655
w-aug 0.7 0.8777 0.095 0.8765 0.063
w-aug 0.9 0.877 0.0975 0.8765 0.0675

w-bn 0.1
w-bn 0.3
w-bn 0.5
w-bn 0.7
w-bn 0.9

0.7361 0.041 0.7472 0.032
0.7916 0.1035 0.7942 0.1005
0.8099 0.097 0.8317 0.0505
0.8485 0.068 0.864 0.0545
0.8761 0.083 0.8806 0.066

w-loss 0.1 0.4078 0.3325 0.51
0.1135
w-loss 0.3 0.6848 0.2165 0.6775 0.116
w-loss 0.5 0.7921 0.158 0.7619 0.102
w-loss 0.7 0.8663 0.1035 0.8214 0.072
w-loss 0.9 0.8767 0.1005 0.8716 0.06

w-os 0.001 0.8056 0.0195 0.7675 0.022
w-os 0.01 0.839 0.043 0.8181 0.0425
0.8654 0.071 0.8591 0.0565
w-os 0.1
0.873 0.083 0.8711 0.057
w-os 0.3
0.8741 0.088 0.8731 0.058
w-os 0.5
0.8745 0.093 0.8741 0.0615
w-os 0.7
0.8745 0.093 0.8751 0.063
w-os 0.9

w-dbr 0.1 0.7036 0.12
0.5327 0.463
w-dbr 0.3 0.8238 0.0915 0.7518 0.427
w-dbr 0.5 0.8588 0.084 0.7845 0.4285
w-dbr 0.7 0.8696 0.0935 0.8288 0.2665
w-dbr 0.9 0.8781 0.089 0.8719 0.0935

confusion
model

bias
model

model

acc

conf

acc

bias

orig
orig-ft

0.9197 0.081 0.9197 0.0675
0.9204 0.076 0.9213 0.059

w-aug 0.1 0.9129 0.063 0.9156 0.0535
w-aug 0.3 0.9161 0.071 0.9186 0.0495
w-aug 0.5 0.9165 0.0735 0.917 0.05
w-aug 0.7 0.9162 0.077 0.9171 0.0575
w-aug 0.9 0.9193 0.0725 0.916 0.0545

w-bn 0.1
w-bn 0.3
w-bn 0.5
w-bn 0.7
w-bn 0.9

0.8168 0.0015 0.8314 0.016
0.8554 0.0125 0.8717 0.022
0.8868 0.0305 0.9004 0.0335
0.9068 0.0575 0.9098 0.0395
0.9157 0.072 0.9186 0.044

0

0.1

w-loss 0.1 0.1
0.5
w-loss 0.3 0.1903 0.4835 0.285 0.1645
0.1465
w-loss 0.5 0.5521 0.36
w-loss 0.7 0.836 0.2455 0.838 0.1395
w-loss 0.9 0.9153 0.088 0.9122 0.0615

0.75

w-os 0.001 0.9059 0.0525 0.9023 0.041
w-os 0.01 0.9148 0.071 0.9144 0.0535
0.9179 0.0765 0.9179 0.0565
w-os 0.1
0.919 0.0795 0.9192 0.0565
w-os 0.3
0.9193 0.08
w-os 0.5
0.9196 0.056
0.9196 0.0805 0.9198 0.056
w-os 0.7
0.9197 0.0805 0.9197 0.0555
w-os 0.9

w-dbr 0.1 0.7035 0.0585 0.6136 0.443
w-dbr 0.3 0.8954 0.0865 0.8145 0.4405
w-dbr 0.5 0.9109 0.075 0.8323 0.4415
w-dbr 0.7 0.9147 0.0745 0.8717 0.2105
w-dbr 0.9 0.9159 0.081 0.9171 0.0535

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:28

Zhong and Tian, et al.

Table 7. Results on CIFAR-10 and MobileNetv2

Table 8. Results on CIFAR-100 and ResNet34

confusion
model

bias
model

model

acc

conf

acc

bias

orig
orig-ft
w-aug 0.1
w-aug 0.3
w-aug 0.5
w-aug 0.7
w-aug 0.9

w-bn 0.1
w-bn 0.3
w-bn 0.5
w-bn 0.7
w-bn 0.9

0.942 0.0565 0.942 0.0415
0.9398 0.057 0.9398 0.0395
0.9302 0.057 0.929 0.04
0.9366 0.055 0.9373 0.04
0.9384 0.0545 0.9394 0.04
0.9397 0.0595 0.9363 0.046
0.0485
0.9383 0.054 0.94

0.7825 0.001 0.7906 0.0008
0.8319 0.0065 0.8598 0.017
0.8804 0.0175 0.8976 0.0285
0.9139 0.039 0.9305 0.0345
0.939 0.053 0.9385 0.0395

w-loss 0.1
w-loss 0.3
w-loss 0.5
w-loss 0.7
w-loss 0.9

0.1764 0.433 0.345 0.032
0.6606 0.2835 0.7258 0.124
0.8411 0.196 0.8295 0.105
0.8899 0.132 0.8791 0.101
0.932 0.0755 0.9316 0.051

w-os 0.0001 0.9073 0.027 0.8943 0.024
w-os 0.001 0.9226 0.036 0.9171 0.033
0.9339 0.0435 0.9317 0.034
w-os 0.01
0.9412 0.052 0.9401 0.039
w-os 0.1
0.942 0.053 0.9414 0.039
w-os 0.3
0.9416 0.0545 0.9413 0.04
w-os 0.5
0.9421 0.056 0.9415 0.041
w-os 0.7
0.9421 0.0565 0.9417 0.0415
w-os 0.9

w-dbr 0.1
w-dbr 0.3
w-dbr 0.5
w-dbr 0.7
w-dbr 0.9

0.6594 0.06
0.4402 0.4675
0.9003 0.0665 0.7939 0.392
0.9248 0.0625 0.8477 0.465
0.9335 0.0615 0.8667 0.4005
0.9422 0.0535 0.9381 0.0625

confusion
model

bias
model

model

acc

conf acc

bias

orig
0.6988 0.155 0.6988 0.07
0.7092 0.13 0.7047 0.05
orig-ft
w-aug 0.1 0.7021 0.17 0.701 0.095
w-aug 0.3 0.707 0.13 0.705 0.1
w-aug 0.5 0.7097 0.17 0.7081 0.075
w-aug 0.7 0.7108 0.17 0.7061 0.07
w-aug 0.9 0.7055 0.17 0.7054 0.085

0.5513 0
0.5993 0

w-bn 0.1 0.5438 0
w-bn 0.3 0.596 0
w-bn 0.5 0.6342 0.01 0.6385 0.015
w-bn 0.7 0.6736 0.035 0.6784 0.035
w-bn 0.9 0.7039 0.11 0.705 0.045

w-loss 0.1 0.0175 0.39 0.0227 0.06
w-loss 0.3 0.144 0.375 0.1399 0.175
w-loss 0.5 0.5747 0.15 0.5756 0.09
w-loss 0.7 0.6161 0.16 0.6209 0.105
w-loss 0.9 0.6767 0.14 0.671 0.07

w-os 0.01 0.6957 0.04 0.6938 0.045
0.6976 0.105 0.6968 0.095
w-os 0.1
0.6984 0.12 0.6988 0.085
w-os 0.3
0.6984 0.15 0.6985 0.095
w-os 0.5
0.6987 0.155 0.6991 0.085
w-os 0.7
0.6987 0.155 0.699 0.075
w-os 0.9

w-dbr 0.1 0.0208 0.37 0.0643 0.035
w-dbr 0.3 0.5157 0.125 0.5459 0.175
w-dbr 0.5 0.5677 0.22 0.6347 0.14
w-dbr 0.7 0.6695 0.21 0.6661 0.195
w-dbr 0.9 0.6752 0.11 0.6758 0.125

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

Repairing Group-Level Errors for DNNs Using Weighted Regularization

000:29

Table 9. Results on COCO and ResNet-50

Table 10. Results on COCO gender and ResNet-
50

confusion
model

bias
model

model

acc

conf

acc

bias

orig
orig-ft

0.6604 0.2381 0.6604 0.2366
0.6613 0.2298 0.6613 0.2283

w-aug 0.1 0.6536 0.2983 0.653 0.3483
w-aug 0.3 0.6567 0.3126 0.6563 0.3019
w-aug 0.5 0.6581 0.2793 0.6588 0.2759
w-aug 0.7 0.6601 0.2896 0.6605 0.2749
w-aug 0.9 0.6609 0.2687 0.6607 0.2456

w-bn 0.1 0.6559 0.0119 0.6569 0.121
w-bn 0.3 0.658 0.0266 0.658 0.1453
w-bn 0.5 0.6597 0.0549 0.6588 0.1933
w-bn 0.7 0.6607 0.1266 0.6593 0.2244
w-bn 0.9 0.6612 0.1694 0.6594 0.2692

w-loss 0.1 0.661 0.0201 0.661 0.0194
w-loss 0.3 0.661 0.0261 0.661 0.0253
w-loss 0.5 0.661 0.0357 0.6611 0.0347
w-loss 0.7 0.6611 0.048 0.6611 0.0474
w-loss 0.9 0.6613 0.1246 0.6613 0.1238

w-os 0.1
w-os 0.3
w-os 0.5
w-os 0.7
w-os 0.9

0.6557 0
0.6582 0
0.6599 0

0.6557 0
0.6582 0
0.6599 0
0.6602 0.1159 0.6602 0.1151
0.6604 0.203 0.6604 0.2015

w-dbr 0.1 0.6548 0.4108 0.6542 0.2171
w-dbr 0.3 0.6567 0.0978 0.6563 0.2729
w-dbr 0.5 0.6575 0.0476 0.6586 0.3047
w-dbr 0.7 0.6597 0.0126 0.6594 0.3369
0.3347
w-dbr 0.9 0.6607 0.0077 0.66

confusion
model

bias
model

model

acc

conf

acc

bias

orig
orig-ft

0.6701 0.0402 0.6701 0.2521
0.6709 0.0382 0.6708 0.2614

w-aug 0.1 0.6614 0.1435 0.6589 0.3185
w-aug 0.3 0.6676 0.0656 0.666 0.3045
w-aug 0.5 0.67
0.0629 0.6686 0.2574
w-aug 0.7 0.6696 0.0594 0.6694 0.2631
w-aug 0.9 0.6711 0.0422 0.6712 0.2541

0.6641 0.063
w-bn 0.1 0.6662 0
w-bn 0.3 0.6679 0
0.6658 0.0781
w-bn 0.5 0.6691 0.0025 0.6671 0.1031
w-bn 0.7 0.6699 0.0039 0.6681 0.1419
w-bn 0.9 0.6705 0.0092 0.6686 0.1768

w-loss 0.1 0.6695 0.0349 0.6698 0.0944
w-loss 0.3 0.6704 0.019 0.67
0.0875
w-loss 0.5 0.6705 0.0179 0.6702 0.1037
w-loss 0.7 0.6707 0.0203 0.6704 0.1223
w-loss 0.9 0.6708 0.0331 0.6707 0.1501

w-os 0.1
w-os 0.3
w-os 0.5
w-os 0.7
w-os 0.9

0.6657 0
0.668 0
0.6693 0

0.6687 0
0.6692 0
0.6695 0
0.6698 0.0044 0.6699 0.2122
0.2368
0.0314 0.67
0.67

w-dbr 0.1 0.6663 0.0476 0.6673 0.0277
w-dbr 0.3 0.668 0.0654 0.6678 0.1019
w-dbr 0.5 0.6686 0.0642 0.6682 0.2284
w-dbr 0.7 0.6692 0.0686 0.6688 0.3267
w-dbr 0.9 0.6704 0.077 0.6696 0.3926

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

000:30

Zhong and Tian, et al.

Table 11. Results on CIFAR-10 and ResNet-18 on
two pairs

Table 12. Results on COCO and ResNet-50 on
two pairs

confusion
model

model

acc

conf

orig
orig-ft

0.8747 0.067
0.8771 0.0698

w-aug 0.1 0.8307 0.0683
w-aug 0.3 0.8656 0.0648
w-aug 0.5 0.8713 0.071
w-aug 0.7 0.8769 0.0668
w-aug 0.9 0.8783 0.0683

w-bn 0.1
w-bn 0.3
w-bn 0.5
w-bn 0.7
w-bn 0.9

0.7839 0.03
0.8192 0.041
0.8466 0.0498
0.8654 0.0605
0.8765 0.0663

w-loss 0.1 0.5628 0.185
w-loss 0.3 0.7045 0.1402
w-loss 0.5 0.8144 0.093
w-loss 0.7 0.8591 0.0735
w-loss 0.9 0.8739 0.0698

w-os 0.001 0.7788 0.02125
w-os 0.01 0.8289 0.037
w-os 0.1
w-os 0.3
w-os 0.5
w-os 0.7
w-os 0.9

0.8628 0.05275
0.8722 0.05975
0.8735 0.06275
0.8743 0.065
0.8749 0.066

w-dbr 0.1 0.7727 0.0728
w-dbr 0.3 0.8498 0.0602
w-dbr 0.5 0.8672 0.0592
w-dbr 0.7 0.8739 0.0633
w-dbr 0.9 0.8792 0.0622

confusion
model

model

acc

conf

orig
orig-ft

0.6604 0.2013
0.6613 0.2298

w-aug 0.1 0.6521 0.2937
w-aug 0.3 0.6564 0.2628
w-aug 0.5 0.6586 0.2697
0.2484
w-aug 0.7 0.66
w-aug 0.9 0.6607 0.2141

w-bn 0.1 0.6548 0.0593
w-bn 0.3 0.6572 0.0835
w-bn 0.5 0.6592 0.0961
w-bn 0.7 0.6607 0.1249
w-bn 0.9 0.6614 0.175

w-loss 0.1 0.6605 0.0388
w-loss 0.3 0.6605 0.0442
w-loss 0.5 0.6608 0.0512
w-loss 0.7 0.6611 0.0605
w-loss 0.9 0.6615 0.1189

w-os 0.1
w-os 0.3
w-os 0.5
w-os 0.7
w-os 0.9

0.6477 0
0.6535 0
0.6575 0
0.6595 0.0968
0.6602 0.1725

w-dbr 0.1 0.6552 0.2957
w-dbr 0.3 0.6571 0.2019
w-dbr 0.5 0.6578 0.1828
w-dbr 0.7 0.6583 0.1624
w-dbr 0.9 0.6588 0.1607

J. ACM, Vol. 0, No. 0, Article 000. Publication date: 2022.

