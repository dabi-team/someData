On the Practical Power of Automata in Pattern Matching

Ora Amir∗

David Sarne§
Bar-Ilan University Bar-Ilan University Weizmann Institute of Science Bar-Ilan University

Aviezri Fraenkel‡

Amihood Amir†

2
2
0
2

l
u
J

7
1

]
S
D
.
s
c
[

1
v
0
2
1
8
0
.
7
0
2
2
:
v
i
X
r
a

and

NVIDIA

and

Georgia Tech

Abstract

The classical pattern matching paradigm is that of seeking occurrences of one string - the pattern, in
another - the text, where both strings are drawn from an alphabet set Σ. Assuming the text length is n
and the pattern length is m, this problem can naively be solved in time O(nm). In Knuth, Morris and
Pratt’s seminal paper of 1977, an automaton, was developed that allows solving this problem in time
O(n) for any alphabet.

This automaton, which we will refer to as the KMP-automaton, has proven useful in solving many
other problems. A notable example is the parameterized pattern matching model.
In this model, a
consistent renaming of symbols from Σ is allowed in a match. The parameterized matching paradigm
has proven useful in problems in software engineering, computer vision, and other applications.

It has long been suspected that for texts where the symbols are uniformly random, the naive algorithm
will perform as well as the KMP algorithm. In this paper we examine the practical eﬃciency of the KMP
algorithm vs. the naive algorithm on a randomly generated text. We analyse the time under various
parameters, such as alphabet size, pattern length, and the distribution of pattern occurrences in the
text. We do this for both the original exact matching problem and parameterized matching. While
the folklore wisdom is vindicated by these ﬁndings for the exact matching case, surprisingly, the KMP
algorithm works signiﬁcantly faster than the naive in the parameterized matching case.

We check this hypothesis for DNA texts, and observe a similar behaviour as in the random text. We

also show a very structured case where the automaton is much more eﬃcient.

1 Introduction

One of the most well-known data structures in Computer science is the Knuth-Morris-Pratt automaton, or
the KMP automaton [20]. It allows solving the exact string matching problem in linear time. The exact
string matching problem has input text T of length n and pattern P of length m, where the strings are
composed of symbols from a given alphabet Σ. The output is all text locations where the pattern occurrs
in the text. The naive way of solving the exact string matching problem takes time O(nm). This can be
achieved by sliding the pattern to start at every text location, and comparing each of its elements to the
corresponding text symbol. Using the KMP automaton, this problem can be solved in time O(n). In fact,
analysis of the algorithm shows that at most 2n comparisons need to be done.

It has long been known in the folklore 1 that if the text is composed of uniformly random alphabet symbols,
the naive algorithm’s time is also linear. This belief is bolstered by the fact that the naive algorithm’s mean

∗Department of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israel,; oramir70@gmail.com.
†Department of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israel, +972 3 531-8770; amir@cs.biu.ac.il;
and College of Computing, Georgia Tech, Atlanta, GA 30332. Partly supported by ISF grant 1475/18 and BSF grant 2018141.
‡Dept of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 76100, Israel, +972 8 934-

3545; aviezri.fraenkel@weizmann.ac.il.

§Department of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israel, +972 3 531-8052; sarned@cs.biu.ac.il.
1The second author heard this for the ﬁrst time from Uzi Vishkin in 1985. Since then this belief has been mentioned, in

many occasions, by various researchers in the community.

 
 
 
 
 
 
number of comparisons for text and pattern over a binary alphabet is bounded by

n

m
(cid:88)

i=1

i
2i which is bounded by 2n comparisons.

The number of comparisons in the KMP algorithm is also bounded by 2n. However, because control in the
naive algorithm is much simpler, then it may be practically faster than the KMP algorithm.

The last few decades have prompted the evolution of pattern matching from a combinatorial solution of the
exact string matching problem [16, 20] to an area concerned with approximate matching of various rela-
tionships motivated by computational molecular biology, computer vision, and complex searches in digitized
and distributed multimedia libraries [15, 7]. An important type of non-exact matching is the parameterized
matching problem which was introduced by Baker [10, 11]. Her main motivation lay in software maintenance,
where program fragments are to be considered “identical” even if variable names are diﬀerent. Therefore,
strings under this model are comprised of symbols from two disjoint sets Σ and Π containing ﬁxed symbols
and parameter symbols respectively. In this paradigm, one seeks parameterized occurrences, i.e., exact occur-
rences up to renaming of the parameter symbols of the pattern string in the respective text location. This
renaming is a bijection b : Π → Π. An optimal algorithm for exact parameterized matching appeared in [5].
It makes use of the KMP automaton for a linear-time solution over ﬁxed ﬁnite alphabet Σ. Approximate
parameterized matching was investigated in [10, 17, 8]. Idury and Sch¨aﬀer [18] considered multiple matching
of parameterized patterns.

Parameterized matching has proven useful in other contexts as well. An interesting problem is searching for
color images (e.g. [24, 9, 3]). Assume, for example, that we are seeking a given icon in any possible color map.
If the colors were ﬁxed, then this is exact two-dimensional pattern matching [2]. However, if the color map
is diﬀerent the exact matching algorithm would not ﬁnd the pattern. Parameterized two dimensional search
is precisely what is needed. If, in addition, one is also willing to lose resolution, then a two dimensional
function matching search should be used, where the renaming function is not necessarily a bijection [1, 6].

Parameterized matching can also be naively done in time O(nm). Based on our intuition for exact matching,
it is expected that here, too, the naive algorithm is competitive with the KMP automaton-based algorithm
of [5] in a randomly generated text.

In this paper we investigate the practical eﬃciency of the automaton-based algorithm vs. the naive algorithm
both in exact and parameterized matching. We consider the following parameters: pattern length, alphabet
size, and distribution of pattern occurrences in the text. Our ﬁndings are that, indeed, the naive algorithm
is faster than the automaton algorithm in practically all settings of the exact matching problem. However,
it was interesting to see that the automaton algorithm is always more eﬀective than the naive algorithm for
parameterized matching over randomly generated texts. We analyse the reason for this diﬀerence.

We established that the randomness of the text is what made the naive algorithm so eﬃcient for exact
matching. We, therefore, ran the comparison in a very structured artiﬁcial text, and the automaton algorithm
was a clear winner.

Having understood the practical behavior of the naive vs. automaton algorithm over randomly generated
texts, we were curious if there were “real” texts with a similar phenomenom. We ran the same experiments
over DNA texts and observed a similar behavior as that of a randomly generated text.

2 Problem Deﬁnition

We begin with basic deﬁnitions and notation generally following [13].

Let S = s1s2 . . . sn be a string of length |S| = n over an ordered alphabet Σ. By ε we denote an empty
string. For two positions i and j on S, we denote by S[i..j] = si..sj the factor (sometimes called substring)
of S that begins at position i and ends at position j (it equals ε if j < i). A preﬁx of S is a factor that
begins at position 1 (S[1..j]) and a suﬃx is a factor that ends at position n (S[i..n]).

The exact string matching problem is deﬁned as follows:

2

Deﬁnition 1 (Exact String Matching) Let Σ be an alphabet set, T = t1 · · · tn the text and P = p1 · · · pm
the pattern, ti, pj ∈ Σ,
input: text T and pattern P .
output: All indices j,

i = 1, . . . , n; j = 1, . . . , m. The exact string matching problem is:

j ∈ {1, ..., n − m + 1} such that

We simplify Baker’s deﬁnition of parameterized pattern matching.

tj+i−1 = pi,

for i = 1, ..., m

Deﬁnition 2 (Parameterized-Matching) Let Σ, T and P be as in Deﬁnition 1. We say that P parameterize-
∼= tj+i−1,
matches or simply p-matches T in location j if pi
following condition holds:

∼= tj if and only if the

i = 1, . . . , m, where pi

for every k = 1, . . . , i − 1,

pi = pi−k if and only if tj = tj−k.

The p-matching problem is to determine all p-matches of P in T .

∼= s2i for all i ∈ {1, ..., m}.

It two strings S1 and S2 have the same length m then they are said to parametrize-match or simply p-match
if s1i
Intuitively, the matching relation ∼= captures the notion of one-to-one mapping between the alphabet symbols.
Speciﬁcally, the condition in the deﬁnition of ∼= ensures that there exists a bijection between the symbols
from Σ in the pattern and those in the overlapping text, when they p-match. The relation ∼= has been deﬁned
by [5] in a manner suitable for computing the bijection.

Example: The string ABABCCBA parameterize matches the string XY XY ZZY X. The reason is that
β
if we consider the bijection β : {A, B, C} → {X, Y, Z} deﬁned by A
−→ Z, then we get
β(ABABCCBA) = XY XY ZZY X. This explains the requirement in Def. 2, where two sumbols match iﬀ
they also match in all their previous occurrences.

β
−→ X, B

β
−→ Y, C

Of course, the alphabet bijection need not be as extreme as bijection β above. String ABABCCBA also
γ
−→
parameterize matches BABACCAB, because of bijection γ : {A, B, C} → {A, B, C} deﬁned as: A
B, B

γ
−→ A, C

γ
−→ C.

For completeness, we deﬁne the KMP automaton.

Deﬁnition 3 Let P = p1 . . . pm be a string over alphabet Σ. The KMP automaton of P is a 5-tuple
(Q, Σ, δs, δf , q0, qa), where Q = {0, ..., m} is the set of states, Σ is the alphabet, δs : Q → Q is the success
function, δf : Q → Q is the failure function, q0 = 0 is the start state and qa = m is the accepting state.

The success function is deﬁned as follows:
δs(i) = i + 1, i = 0, ..., m − 1 and
δs(0) = 0

The failure function is deﬁned as follows:
Denote by (cid:96)(S) the length of the longest proper preﬁx of string S (i.e. excluding the entire string S) which
is also a suﬃx of S.
δf (i) = (cid:96)(P [1..i]),

for i = 1, ..m.

For an example of the KMP automaton see Fig. 1.

Theorem 1 [20] The KMP automaton can be constructed in time O(m).

3 The Exact String Matching Problem

The Knuth-Morris-Pratt (KMP) search algorithm uses the KMP automaton in the following manner:

Variables:
pointert points to indices in the text. pointerp points to indices in the pattern.

3

Figure 1: Automaton example

Initialization:
set pointer pointert to 1. set pointer pointerp to 0.

Main Loop:
While pointert ≤ n − m + 1 do:

If tpointert = δs(pointerp) then do:
pointert ← pointert + 1
pointerp ← δf (pointerp)
If pointerp = m − 1 then do:

output “pattern occurrence ends in text location pointert”.
pointerp ← δf (m)

enddo

enddo

else (tpointert (cid:54)= δs(pointerp)) do:

if pointerp = 0 then pointert ← pointert + 1
else pointerp ← δf (pointerp)

enddo
go to beginning of while loop

endwhile

Theorem 2 [20] The time for the KMP search algorithm is O(n). In fact, it does not exceed 2n compar-
isons.

4 The Parameterized Matching Problem

Amir, Farach, and Muthukrishnan [5] achieved an optimal time algorithm for parameterized string matching
by a modiﬁcation of the KMP algorithm. In fact, the algorithm is exactly the KMP algorithm, however,
every equality comparison “x = y” is replaced by “x ∼= y” as deﬁned below.
Implementation of “x ∼= y”

4

0 1 2 3 4 P=AABB |P|=4 ∑={A,B} δs = all arrows with a symbol above them. δf = all other arrows.  ≠A  Construct table A[1], . . . , A[m] where A[i] = the largest k,
then A[i] = i.

1 ≤ k < i, such that pk = pi. If no such k exists

The following subroutines compute “pi

∼= tj” for j ≥ i, and “pi

∼= pj” for j ≤ i.

Compare(pi,tj)

if A[i] = i and tj (cid:54)= tj−1, . . . , tj−i+1 then return equal
if A[i] (cid:54)= i and tj = tj−i+A[i] then return equal
return not equal

end

Compare(pi,pj)

if (A[i] = i or i − A[i] ≥ j) and pj (cid:54)= p1, . . . , pj−1 then return equal
if i − A[i] < j and pj = pj−i+A[i] then return equal
return not equal

end

Theorem 3 [5] The p-matching problem can be solved in O(n log σ) time, where σ = min(m, |Σ|).

Proof:

The table A can be constructed in O(m log σ) time as follows: scan the pattern left to right keeping track of
the distinct symbols from Σ in the pattern in a balanced tree, along with the last occurrence of each such
symbol in the portion of the pattern scanned thus far. When the symbol at location i is scanned, look up
this symbol in the tree for the immediately preceding occurrence; that gives A[i].

Compare can clearly be implemented in time O(log σ). For the case A[i] (cid:54)= i, the comparison can be done in
time O(1). When scanning the text from left to right, keep the last m symbols in a balanced tree. The check
tj (cid:54)= tj−1, . . . , tj−i+1 in Compare(pi,tj) can be performed in O(log σ) time using this information. Similarly,
Compare(pi,pj) can be performed using A[i]. Therefore, the automaton construction in KMP algorithm with
every equality comparison “x = y” replaced by “x ∼= y” takes time O(m log σ) and the text scanning takes
time O(n log σ), giving a total of O(n log σ) time.

As for the algorithm’s correctness, Amir, Farach and Muthukrishnan showed that the failure link in automa-
ton node i produces the largest preﬁx of p1 · · · pi that p-matches the suﬃx of p1 · · · pi.

5 Our Experiments

Our implementation was written in C + +. The platform was Dell latitude 7490 with intel core i7 - 8650U,
32 GB RAM, with 8 MB cache. The running time was computed using the chrono high resolution clock.
The random strings were generated using the random Python package.

We implemented the naive algorithm for exact string matching and for parameterized matching. The same
code was used for both, except for the implementation of the equivalence relation for parameterized matching,
as described above. This required implementing the A array. We also implemented the KMP algorithm for
exact string matching, and used the same algorithm for parameterized matching. The only diﬀerence was
the implementation of the equivalence parameterized matching relation.

The text length n was 1,000,000 symbols. Theoretically, since both the automaton and naive algorithm
are sequential and only consider a window of the pattern length, it would have been suﬃcient to run the
experiment on a text of size twice the pattern [4]. However, for the sake of measurement resolution we opted
for a large text. Yet the size of 1,000,000 comfortably ﬁts in the cache, and thus we avoid the caching issue. In
general, any searching algorithm for patterns of length less than 4MB would ﬁt in the cache if appropriately
constructed in the manner of [4]. Therefore our decision gives as accurate a solution as possible.

We ran patterns of lengths m = 32, 64, 128, 256, 512, and 1024. The alphabet sizes tested were |Σ| =
2, 4, 6, 8, 10, 20, 40, 80, 160, 320. For each size, 10 tests were run, for a total of 600 tests.

5

Methodology: We generated a uniformly random text of length 1, 000, 000. If the pattern would also be
randomly generated, then it would be unlikely to appear in the text. However, when seeking a pattern in
the text, one assumes that the pattern occurs in the text. An example would be searching for a sequence
in the DNA. When seeking a sequence, one expects to ﬁnd it but just does not know where. Additionally,
we considered the common case where one does not expect many occurrences of the pattern in the text.
Consequently, we planted 100 occurrences of the pattern in the text at uniformly random locations. The
ﬁnal text length was always 1, 000, 000. The reason for inserting 100 pattern occurrences is the following. We
do not expect manny occurrences, and a 100 occurrences in a million-length text means that less than 0.1%
of the text has pattern occurrences. On the other hand, it is suﬃcient to introduce the option of actually
following all elements of the pattern 100 times. This would make a diﬀerence in both algorithms. They
would both work faster if there were no occurrences at all.

We also implemented a variation where half of the pattern occurrences were in the last quarter of the text.
For each alphabet size and pattern length we generated 10 tests and considered the average result of all 10
tests. It should be noted that from a theoretical point of view, the location of the pattern should not make
a diﬀerence. We tested the diﬀerent options in order to verify that this is, indeed, the case.

5.1 Exact Matching

5.1.1 Results

Tables 1 and 2 in the Appendix show the alphabet size, the pattern length, the average of the running times
of the naive algorithm for the 10 tests, the average of the running time of the KMP algorithm for the 10
tests, and the ratio of the naive algorithm running time over the KMP algorithm running time. Any ratio
value below 1 means that the naive algorithm is faster. A small value indicates a better performance of the
naive algorithm. Any value above 1 indicates that the KMP algorithm is faster than the naive algorithm.
The larger the number, the better the performance.

To enable a clearer understanding of the results, we present them below in graph form. The following graphs
show the results of our tests for the diﬀerent pattern lengths. In Figs. 2 and 3, the x-axis is the pattern
size. The y-axis is the ratio of the naive algorithm running time to the KMP algorithm running time. The
diﬀerent colors depict alphabet sizes. In Fig. 2, the patterns were inserted at random, whereas in Fig. 4 the
patterns appear at the last half of the text.

To better see the eﬀect of the pattern distribution in the text, Fig. 4 maps, on the same graph, both cases.
In this graph, the x-axis is the average running time of all pattern lengths per alphabet size, and the y-axis
is the ratio of the naive algorithm running time to the KMP algorithm running time. The results of the
uniformly random distribution are mapped in one color, and the results of all pattern occurrences in the last
half of the text are mapped in another.

We note the following phenomena:

1. The naive algorithm performs better than the automaton algorithm. Of the 600 tests we ran,
there were only 3 instances where the KMP algorithm performed better than the naive, and all were
subsumed by the average. In the vast majority of cases the naive algorithm was superior by far.

2. The naive algorithm performs relatively better for larger alphabets.

3. For a ﬁxed alphabet size, there is a slight increase in the naive/KMP ratio, as the pattern length

increases.

4. The distribution of the pattern occurrences in the text does not seem to make a change in performance.

An analysis of these implementation behaviors appears in the next subsection.

6

Figure 2: Performance in the Exact Matching case, pattern oc-
currences distributed uniformly random.

5.1.2 Analysis

We analyse all four results noted above.

Better Performance of the Naive Algorithm
We have seen that the mean number of comparisons of the naive algorithm for binary alphabets is bounded
by

n

m
(cid:88)

i=1

i
2i which is bounded by 2n comparisons.

The running time of the KMP algorithm is also bounded by O(2n). However, the control of the KMP
algorithm is more complex than that of the naive algorithm, which would indicate a constant ratio in favor
of the naive algorithm. However, when the KMP algorithm encounters a mismatch it follows the failure link,
which avoids the need to re-check a larger substring. Thus, for longer length patterns, where there are more
possibilities of following the failure links for longer distances, there is a lessening advantage of the naive
algorithm.

Better Performance of the Naive Algorithm for Larger Alphabets
This is fairly clear when we realize that the mean performance of the naive algorithm for alphabet of size k
is:

n

m
(cid:88)

i=1

i
ki = n

k

(k − 1)2 comparisons.

This is clearly decreasing the larger the alphabet size. However, the repetitive traversal of the failure link,
even in cases where there is no equality in the comparison check, will still raise the relative running time of
the KMP algorithm. Here too, the longer the pattern length, the more failure link traversals of the KMP,
and thus less overall comparisons, which slightly decreases the advantage of the naive algorithm.

The Distribution of Pattern Occurrences in the Text
If the pattern is not periodic, and if the patterns are not too frequent in the text, then there will be at
most one pattern in a text substring of length 2m. In these circumstances, there is really no eﬀect to the
distribution of the pattern in the text. We would expect a diﬀerence if the pattern is long with a small

7

Figure 3: Performance in the Exact Matching case, pattern oc-
currences congregated at end of text.

period. Indeed, an extreme such case is tested in Subsection 5.1.3.

5.1.3 A Very Structured Example

All previous analyses point to the conviction that the more times a preﬁx of the pattern appears in the text,
and the more periodic the pattern, the better will be the performance of the KMP algorithm. The most
extreme case would be of text An (A concatenated n times), and pattern Am−1B. Indeed the results of this
case appear in Fig. 5.

Theoretical analysis of the naive algorithm predicts that we will have nm comparisons, where n is the text
length and m is the pattern length. The KMP algorithm will have 2n comparisons, for any pattern length.
Thus the ratio q of naive to KMP will be O( m
q we get twice the cost of the control
of the KMP algorithm. This can be seen in Fig. 5 to be 5.

2 ). In fact, when we plot m

5.2 Parameterized Matching

5.2.1 Results

The exact matching results behaved roughly in the manner we expected. The surprise came in the param-
eterized matching case. Below are the results of our tests. As in the exact matching case, the tables show
the alphabet size, the pattern length, the average of the running times of the naive algorithm for the 10
tests, the average of the running time of the automaton-based algorithm for the 10 tests, and the ratio q of
the naive algorithm running time over the automaton-based algorithm running time. Any ratio value above
1 means that the automaton-based algorithm is faster. A large value indicates a better performance of the
automaton-based algorithm.

The following graphs show the results of our tests for the diﬀerent pattern lengths. The x-axis is the pattern
size. The y-axis is the ratio of the naive algorithm running time to the automaton-based algorithm running
time. The diﬀerent colors depict alphabet sizes. To better see the eﬀect of the pattern distribution in the
text, we also map, on the same graph, both cases. In this graph, the x-axis is the average running time of

8

Figure 4: Comparison of average performance of uniform pattern
distribution vs. pattern occurrences congregated at end of text.

Figure 5: Performance in the Exact Matching case, periodic text.

all pattern lengths per alphabet size, and the y-axis is the ratio of the naive algorithm running time to the
automaton-based algorithm running time. The results of the uniformly random distribution are mapped in
one color, and the results of all pattern occurrences in the last half of the text are mapped in another.

The parameterized matching results appear in tables 3 and 4 in the appendix. Figs. 6 and 7 map the results
of the parameterized matching comparisons for the case where the patterns were inserted at random vs. the
case where the patterns appear at the last half of the text. In Fig. 8 we map at the same graph the average
results of both the cases where the patterns appear at the text uniformly at random, and where the patterns
appear at the last half of the text.

The results are very diﬀerent from the exact matching case. We note the following phenomena:

1. The automaton-based algorithm always performs signiﬁcantly better than the naive algorithm.

2. The automaton-based algorithm performs relatively better for larger alphabets.

3. For a ﬁxed alphabet size, the pattern length does not seem to make much diﬀerence.

9

Figure 6: Performance in the Parameterized Matching case, pat-
tern occurrences distributed uniformly random.

4. The distribution of the pattern occurrences in the text does not seem to make a change in performance.

An analysis of these implementation behaviors and an explanation of the seemingly opposite results from
the exact matching case appear in the next subsection.

5.2.2 Analysis

We analyse all four results noted above.

Better Performance of the Automaton-based Algorithm
We have established that the mean number of comparisons for the naive algorithm in size k alphabet is

n

m
(cid:88)

i=1

i
ki = n

k

(k − 1)2 comparisons.

However, when it comes to parameterized matching, any order of the alphabet symbols is a match, thus the
mean number of comparisons is to be multiplied by k!. Therefore, for size 2 alphabet we get 4n comparisons,
and the number rises exponentially with the alphabet size. Also, the automaton-based algorithms is constant
at 2n comparisons. Even for a size 2 alphabet, there is twice the number of comparisons in the naive algorithm
than in the automaton-based algorithm. Note, also, that because of the need to ﬁnd the last parameterized
match, the control mechanism even of the naive algorithm, is more complex. This results in a superior
performance of the automaton-based algorithm even for small alphabets. Of course, the larger the alphabet,
the better the performance of the automaton-based algorithm.

Pattern Length

The pattern length does not play a role in the automaton-based algorithm, where the number of comparisons
is always bounded by 2n. In the naive case, the multiplication of the factorial of the alphabet size is so
overwhelming that it dominates the pattern length. For example, note that for an extremely large alphabet,
there would be a leading preﬁx of diﬀerent alphabet symbols. That preﬁx will always be traversed by the
naive algorithm. The larger the alphabet, the longer will be the mean length of that preﬁx.

10

Figure 7: Performance in the Parameterized Matching case, pat-
tern occurrences congregated at end of text.

Pattern Distribution

As in the exact matching case, for a non-periodic pattern that does not appear too many times, the distri-
bution of occurrences will have no eﬀect on the complexity.

6 DNA Data

Having understood the behavior of the naive algorithm and the automaton-based algorithm in randomly
generated texts, the natural question is are there any “real” texts for which the naive algorithm performs
better than the automaton-based algorithm.

We performed the same experiments on DNA data. The experimental setting was identical to that of the
randomly generated texts with the following diﬀerences:

1. The DNA of the fruit ﬂy, Drosophila melanogaster is 143.7 MB long. We extracted 60 subsequences
of length 1,000,000 each, as FASTA data from the NIH National Library of Medicine, National Center
for Biotechnology Information. We ran 10 tests on each of the six pattern lengths 32, 64, 128, 256,
512, 1024.

2. The alphabet size is 4, due to the four bases in DNA sequence.

Figs. 9 and 10 below show the ratio between the average running time of the naive algorithm and the
automaton based algorithm. As in the uniformly random text we see that for the exact matching case the
ratio is less than 1, i.e., the naive algorithm is faster, whereas in the parameterized matching case, the ratio
is more than 1, indicating that the automaton based algorithm is faster.

11

Figure 8: Comparison of average performance of uniform pattern
distribution vs. pattern occurrences congregated at end of text.

7 Conclusions

The folk wisdom has always been that the naive string matching algorithm will outperform the automaton-
based algorithm for uniformly random texts. Indeed this turns out to be the case for exact matching. This
study shows that this is not the case for parameterized matching, where the automaton-based algorithm
always outperforms the naive algorithm. This advantage is clear and is impressively better the larger the
alphabets. The same result is true for searches over DNA data.

The conclusion to take away from this study is that one should not automatically assume that the naive
string matching algorithm is better. The matching relation should be analysed. There are various matchings
for which an automaton-based algorithm exists. We considered here parameterized matching, but other
matchings, such as ordered matching [12, 14, 19], or Cartesian tree matching [21, 22, 23], can also be solved
by automaton-based methods. In a practical application it is worthwhile spending some time considering
the type of matching one is using. It may turn out to be that the automaton-based algorithm will perform
signiﬁcantly better than the naive, even for uniformly random texts. Alternately, even non-uniformly random
data may be such that the naive algorithm performs better than the automaton based algorithm for exact
matching.

An open problem is to compare the search time in DNA data to the search time in uniformly random data.
While it is clear that DNA data is not uniformly random, it would be interesting to devise an experimental
setting to compare search eﬃciency in both types of strings.

References

[1] A. Amir, A. Aumann, M. Lewenstein, and E. Porat. Function matching. SIAM Journal on Computing,

35(5):1007–1022, 2006.

[2] A. Amir, G. Benson, and M. Farach. An alphabet independent approach to two dimensional pattern

matching. SIAM J. Comp., 23(2):313–323, 1994.

[3] A. Amir, K. W. Church, and E. Dar. Separable attributes: a technique for solving the submatrices
In Proc. 13th ACM-SIAM Symp. on Discrete Algorithms (SODA), pages

character count problem.
400–401, 2002.

12

Figure 9: Performance in the Exact Matching case on DNA se-
quences.

[4] A. Amir and M. Farach. Eﬃcient 2-dimensional approximate matching of half-rectangular ﬁgures.

Information and Computation, 118(1):1–11, April 1995.

[5] A. Amir, M. Farach, and S. Muthukrishnan. Alphabet dependence in parameterized matching. Infor-

mation Processing Letters, 49:111–115, 1994.

[6] A. Amir and I. Nor. Generalized function matching. J. of Discrete Algorithms, 5(3):514–523, 2007.

[7] A. Apostolico and Z. Galil (editors). Pattern Matching Algorithms. Oxford University Press, 1997.

[8] A. Apostolico, M. Lewenstein, and P. Erd¨os. Parameterized matching with mismatches. Journal of

Discrete Algorithms, 5(1):135–140, 2007.

[9] G.P. Babu, B.M. Mehtre, and M.S. Kankanhalli. Color indexing for eﬃcient image retrieval. Multimedia

Tools and Applications, 1(4):327–348, Nov. 1995.

[10] B. S. Baker. Parameterized pattern matching: Algorithms and applications. Journal of Computer and

System Sciences, 52(1):28–42, 1996.

[11] B. S. Baker. Parameterized duplication in strings: Algorithms and an application to software mainte-

nance. SIAM Journal on Computing, 26(5):1343–1362, 1997.

[12] S. Cho, J. C. Na, K. Park, and J. S. Sim. Fast order-preserving pattern matching. In Proc. 7th conf.
Combinatorial Optimization and Applications COCOA, volume 8287 of Lecture Notes in Computer
Science, pages 295–305. Springer, 2013.

[13] M. Crochemore, C. Hancart, and T. Lecroq. Algorithms on Strings. Cambridge University Press, 2007.

[14] M. Crochemore, C.S. Iliopoulos, T. Kociumaka, M. Kubica, A. Langiu, S.P. Pissis, J. Radoszewski,
W. Rytter, and T. Walen. Order-preserving incomplete suﬃx trees and order-preserving indexes. In
Proc. 20th International Symposium on String Processing and Information Retrieval (SPIRE), volume
8214 of LNCS, pages 84–95. Springer, 2013.

[15] M. Crochemore and W. Rytter. Text Algorithms. Oxford University Press, 1994.

[16] M.J. Fischer and M.S. Paterson. String matching and other products. Complexity of Computation,

R.M. Karp (editor), SIAM-AMS Proceedings, 7:113–125, 1974.

13

Figure 10: Performance in the Parameterized Matching case on
DNA Sequences.

[17] C. Hazay, M. Lewenstein, and D. Sokol. Approximate parameterized matching. In Proc. 12th Annual

European Symposium on Algorithms (ESA 2004), pages 414–425, 2004.

[18] R.M. Idury and A.A Sch¨aﬀer. Multiple matching of parameterized patterns. In Proc. 5th Combinatorial

Pattern Matching (CPM), volume 807 of LNCS, pages 226–239. Springer-Verlag, 1994.

[19] J. Kim, A. Amir, J.C. Na, K. Park, and J.S. Sim. On representations of ternary order relations in
numeric strings. In Proc. 2nd International Conference on Algorithms for Big Data (ICABD), volume
1146 of CEUR Workshop Proceedings, pages 46–52, 2014.

[20] D.E. Knuth, J.H. Morris, and V.R. Pratt. Fast pattern matching in strings. SIAM J. Comp., 6:323–350,

1977.

[21] S. G. Park, A. Amir, G. M. Landau, and K. Park. Cartesian Tree Matching and Indexing. In Nadia
Pisanti and Solon P. Pissis, editors, Proc. 30th Symposium on Combinatorial Pattern Matching (CPM),
volume 128 of Leibniz International Proceedings in Informatics (LIPIcs), pages 16:1–16:14, 2019.

[22] S. G. Park, M. Bataa, A. Amir, G. M. Landau, and K. Park. Finding patterns and periods in cartesian

tree matching. Theor. Comput. Sci., 845:181–197, 2020.

[23] S. Song, G. Gu, C. Ryu, S. Faro, T. Lecroq, and K. Park. Fast algorithms for single and multiple

pattern cartesian tree matching. Theor. Comput. Sci., 849:47–63, 2021.

[24] M. Swain and D. Ballard. Color indexing. International Journal of Computer Vision, 7(1):11–32, 1991.

14

8 Appendix

|Σ|

2

6

10

40

160

patt. length Naive KMP N aive
KM P
0.6729
4514.1
0.6623
4449.3
0.693
4697.3
0.6666
4522.9
0.7051
4764.7
0.7304
4521.4
0.5139
2225.1
0.5157
2199.2
0.5108
2180.9
0.5163
2169.2
0.5314
2193.2
0.5455
2238.7
0.4427
1593.1
0.44
1578.3
0.4391
1564.8
0.4516
1594.5
0.4317
1554.3
0.5619
1892.5
0.3316
943.9
0.3358
964.3
0.3401
972.5
0.3363
952.6
0.3523
975.4
0.3655
970.5
0.302
810.9
0.2918
794.0
0.3281
922.2
0.3285
899.2
0.3374
897.8
0.3399
861.6

6712.5
6727.8
6764.3
6814.2
6734.7
6188.7
4331.2
4263.2
4270.6
4201.4
4128.4
4110.1
3598.9
3586.4
3563.9
3531.6
3626.0
3380.0
2846.7
2869.3
2852.5
2835.3
2769.0
2655.4
2686.1
2733.1
2771.1
2700.6
2635.6
2534.9

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

|Σ|

4

8

20

80

320

patt. length Naive KMP N aive
KM P
0.5879
3174.2
0.5818
3167.8
0.5917
3136.8
0.5942
3109.7
0.608
3108.8
0.6368
3141.1
0.4553
1771.8
0.4659
1794.5
0.4654
1764.0
0.4652
1766.5
0.4827
1771.9
0.5085
1827.3
0.396
1312.0
0.4269
1428.6
0.3817
1252.7
0.375
1187.4
0.4
1281.7
0.4347
1274.6
0.3242
898.1
0.335
938.4
0.3336
946.7
0.323
875.9
0.3302
875.8
0.346
899.6
0.2916
790.3
0.3074
833.4
0.3005
803.3
0.2877
785.2
0.3269
878.5
0.3427
883.8

5409.9
5428.3
5293.0
5228.2
5110.5
4928.7
3903.4
3852.4
3789.7
3798.4
3670.6
3596.6
3309.2
3297.7
3264.9
3161.3
3166.8
2923.1
2758.7
2777.9
2824.5
2709.0
2653.9
2605.0
2712.0
2711.1
2676.3
2743.0
2690.4
2563.6

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

Table 1: Implementation Results - Exact Matching, patterns uniformly distributed.

15

|Σ|

2

6

10

40

160

patt. length Naive KMP N aive
KM P
0.6649
4613.3
0.6824
4570.1
0.667
4462.8
0.6692
4441.5
0.744
4786.4
0.7105
4493.8
0.509
2374.7
0.5093
2336.6
0.534
2467.1
0.5274
2350.4
0.5243
2306.2
0.5597
2411.2
0.4608
1741.8
0.4528
1719.8
0.4264
1616.5
0.4424
1685.1
0.4737
1774.0
0.4922
1727.8
0.3606
1108.6
0.3283
1014.5
0.3533
1142.9
0.3413
1026.3
0.5205
1503.7
0.3951
1170.1
0.3393
981.8
0.3061
863.6
0.3178
908.6
0.3047
851.2
0.313
909.6
0.4093
1174.9

6931.1
6695.7
6702.2
6644.9
6441.1
6360.6
4638.6
4586.8
4597.0
4453.1
4447.2
4302.9
3762.0
3772.8
3800.2
3814.7
3724.7
3484.3
3048.3
3084.3
3210.4
3005.2
2930.9
2926.9
2855.0
2818.4
2842.8
2796.4
2917.1
2815.9

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

|Σ|

4

8

20

80

320

patt. length Naive KMP N aive
KM P
0.5759
3091.7
0.5819
3203.2
0.5933
3190.4
0.5924
3200.3
0.6176
3305.2
0.6469
3322.4
0.4616
1836.3
0.4589
1804.2
0.465
1816.9
0.4655
1802.8
0.4684
1792.0
0.5183
1889.1
0.3916
1242.4
0.3615
1173.5
0.3847
1286.4
0.411
1334.3
0.399
1231.7
0.4168
1263.8
0.2988
867.4
0.3248
941.2
0.3546
1023.5
0.3397
1005.4
0.3355
956.0
0.3532
954.3
0.2894
769.6
0.2882
771.8
0.304
799.5
0.3345
917.9
0.3455
967.3
0.3604
951.2

5362.9
5499.5
5373.6
5413.1
5340.0
5125.8
3978.1
3930.2
3908.6
3875.2
3832.8
3640.7
3173.7
3251.9
3302.4
3234.5
3090.4
3031.5
2912.6
2912.8
2872.7
2949.3
2852.1
2701.8
2662.8
2681.5
2627.0
2722.0
2757.1
2601.3

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

Table 2: Implementation Results - Exact Matching, patterns at end.

16

|Σ|

2

6

10

40

160

patt. length Naive KMP
6871.8
6761.4
6780.8
6688.6
6440.3
6277.9
6818.3
7022.8
6879.7
6778.1
6460.7
6312.7
7629.8
7787.6
7664.8
7478.5
7406.5
7074.3
5708.6
6076.9
5994.7
5544.9
5411.8
5353.9
4789.375
5222.7
5212.2
4953.4
4793.2
4913.2

25738.0
25996.5
26080.5
26269.7
26004.0
26456.0
26213.2
26244.3
26130.3
26141.2
26212.3
26171.5
28663.6
28787.8
28629.8
28647.0
28843.4
28516.9
33994.8
33826.0
33971.3
33740.9
34501.6
34172.0
54881.0
56750.0
57775.6
56719.3
58276.6
57331.2

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

|Σ|

4

8

20

80

320

N aive
KM P
3.7655
3.8593
3.8571
3.934
4.0456
4.2167
3.96
3.8621
3.9429
3.987
4.1752
4.2986
3.8967
3.8351
3.8775
3.99
4.0576
4.1282
6.0731
5.6046
5.7342
6.2016
6.5045
6.496
11.5167
10.8806
11.2048
11.5
12.1498
11.7029

patt. length Naive KMP N aive
KM P
3.5351
26104.6
3.5998
26734.4
3.6136
26281.4
3.6062
26204.3
3.71
26169.6
3.924
26570.9
3.9411
26863.5
3.9394
27010.3
4.0336
26965.3
4.0304
26918.8
4.1592
27211.8
4.3042
27406.5
5.1463
28539.6
4.6772
28543.3
4.8694
28254.3
5.2725
28526.7
5.3728
28326.8
5.5292
28457.7
8.0792
42524.1
7.8236
41425.9
7.8057
41547.1
8.0644
41489.1
8.165
41615.2
8.478
42184.8
17.9046
70360.0
17.1093
75533.8
17.4987
75098.4
18.328
77763.7
18.1751
75922.3
17.989
76831.3

7489.6
7538.6
7370.8
7361.0
7123.6
6863.1
7229.3
7258.5
7067.4
7099.7
6888.9
6698.6
5832.4
6329.9
6041.4
5733.2
5546.4
5433.1
5292.8
5340.1
5387.7
5269.7
5189.5
5067.8
3919.7
4456.5
4284.8
4238.4
4181.3
4366.4

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

Table 3: Implementation Results - Parameterized Matching, patterns uniformly distributed.

17

|Σ|

2

6

10

40

160

patt. length Naive
26063.4
26285.3
26053.8
26612.5
26501.7
26397.8
26312.4
26733.2
26470.1
26346.3
26610.6
26632.3
29612.8
28948.9
29305.1
29457.3
29650.7
30742.0
34179.7
34385.3
34951.8
36703.8
37417.4
35190.1
52173.125
54173.0
56313.9
54897.4
55123.0
55603.0

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

KMP
6801.4
6878.0
7047.4
6671.7
6764.8
6506.4
7071.4
6924.6
7067.1
6701.1
6682.2
6399.8
7759.5
7748.2
7925.5
7619.7
7836.9
7421.5
5577.9
5723.2
5758.1
6033.8
5682.4
5488.2
4773.875
5176.9
5032.4
5257.0
5025.2
4915.0

N aive
KM P
3.8439
3.828
3.7047
3.996
3.9329
4.0685
3.828
3.9976
3.8636
4.0218
4.117
4.2563
3.9578
3.8873
3.829
4.0189
3.9754
4.3099
6.2968
6.2199
6.1685
6.216
6.7656
6.5708
10.9192
10.4658
11.1442
10.433
11.0268
11.26

|Σ|

4

8

20

80

320

patt. length Naive KMP N aive
KM P
3.61
26616.5
3.6226
26571.7
3.4449
26385.6
3.4807
26236.1
3.6748
26660.5
3.8591
26667.6
3.8491
27246.5
3.9748
27046.1
4.009
27117.6
4.04
27154.8
4.0791
26901.8
4.2963
27227.5
5.0995
29588.6
5.1754
29393.4
4.8688
29498.7
5.1945
29659.5
5.226
29067.8
5.5624
28922.4
7.6963
41534.5
7.8299
41907.7
7.6894
41709.3
8.1372
41900.3
8.1023
41753.4
8.5567
43312.9
16.8561
67440.5
16.7108
71874.8
16.725
72359.4
17.2654
72268.3
17.1366
72729.5
17.7372
73777.8

7505.3
7443.4
7829.9
7649.5
7356.9
7038.6
7421.6
7185.0
7170.1
7089.7
6998.0
6667.8
6153.9
6010.3
6312.8
5966.3
5802.3
5455.1
5441.2
5373.0
5474.4
5211.0
5196.7
5074.3
3981.8
4294.4
4315.2
4179.1
4234.7
4152.8

32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024
32
64
128
256
512
1024

Table 4: Implementation Results - Parameterized Matching, patterns at end.

18

