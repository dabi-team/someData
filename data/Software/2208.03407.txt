Springer Nature 2021 LATEX template

2
2
0
2

g
u
A
5

]
E
S
.
s
c
[

1
v
7
0
4
3
0
.
8
0
2
2
:
v
i
X
r
a

An Overview of Structural Coverage Metrics
for Testing Neural Networks

Muhammad Usman1, Youcheng Sun2, Divya Gopinath3, Rishi
Dange4, Luca Manolache5 and Corina S. P˘as˘areanu6*

1University of Texas, Austin.
2University of Manchester, UK.
3NASA Ames, KBR.
4Princeton University.
5Palo Alto High School.
6NASA Ames, KBR, CMU Cylab.

*Corresponding author(s). E-mail(s): pcorina@cmu.edu;

Abstract

Deep neural network (DNN) models, including those used in safety-
critical domains, need to be thoroughly tested to ensure that they
can reliably perform well
in diﬀerent scenarios. In this article, we
provide an overview of structural coverage metrics for testing DNN
models, including neuron coverage (NC), k-multisection neuron cover-
age (kMNC), top-k neuron coverage (TKNC), neuron boundary coverage
(NBC), strong neuron activation coverage (SNAC) and modiﬁed condi-
tion/decision coverage (MC/DC). We evaluate the metrics on realistic
DNN models used for perception tasks (including LeNet-1, LeNet-4,
LeNet-5, and ResNet20) as well as on networks used in autonomy
(TaxiNet). We also provide a tool, DNNCov, which can measure the
testing coverage for all these metrics. DNNCov outputs an informa-
tive coverage report to enable researchers and practitioners to assess
the adequacy of DNN testing, compare diﬀerent coverage measures,
and to more conveniently inspect the model’s internals during testing.

Keywords: Coverage, Neural Networks, Testing

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

An Overview ...

1 Introduction

Today’s world has seen a signiﬁcant rise in deep learning models that are used
for solving complex tasks, such as medical diagnosis, image and video recogni-
tion, text processing, program understanding, and so on. Since such models are
increasingly used for safety-critical applications, including the perception and
control of self-driving vehicles, ensuring that deep learning models perform as
expected is of extreme importance. Typically, statistical accuracy on a set of
tests is used as a measure of model performance. However, it is unclear if the
tests cover all the possible behaviors of the model, including corner cases. Cov-
erage metrics can be used to measure the adequacy of testing. In this article we
review and evaluate recently proposed structural coverage metrics for testing
neural networks. In addition, we describe a tool, DNNCov, which incorpo-
rates into a common framework these diﬀerent coverage metrics, to allow for
easy experimentation and comparison.

Speciﬁcally, we evaluate neuron coverage and its variants, as well as a
more complex MC/DC (modiﬁed condition/decision coverage) criterion for
neural networks. Neuron coverage was ﬁrst proposed in DeepXplore [1] where
it was developed as a DNN counterpart for statement coverage. This was later
extended to a family of more ﬁne-grained criteria [2], such as k-multisection
neuron coverage (KMNC), top-k neuron coverage (TKNC), top-k neuron pat-
terns (TKNP), neuron boundary coverage (NBC) and strong neuron activation
coverage (SNAC) [2]. In [3], four MC/DC variants were deﬁned for DNN test-
ing. Subsequently, there has been a boom in coverage-guided DNN testing
techniques. ADAPT [4] uses an adaptive learning algorithm to generate new
images for testing neural networks with an aim of increasing e.g., neuron cover-
age. DeepHunter [5] is a fuzzing framework which uses metamorphic mutations
to generate tests following the metrics in [2] while the concolic testing in [6]
targets MC/DC.

Despite the proliferation of the aforementioned approaches, it is still not
well understood which technique or criteria can adequately determine if a neu-
ral network model has been well tested with respect to properties such as
functional diversity or vulnerability to attacks (e.g., adversarial input pertur-
bations) so on. The problem is challenging due to the opaque nature of the
networks. The coverage criterion that is the most eﬀective may vary from one
model to the other, and from one task to another. In this work, we present
DNNCov, a uniﬁed framework for evaluating, comparing and visualizing
diﬀerent structural coverage metrics for a given model and test suite.

We summarize our contributions as follows:

• We provide an overview of diﬀerent structural coverage metrics (NC, KMNC,

TKNC, NBC, SNAC, and MC/DC) for testing neural networks.

• We evaluate the criteria with respect to functional diversity and defect detec-
tion. We consider both adversarial robustness and data poisoning scenarios
(the latter has not been considered in previous evaluations). We consider
state-of-the art models trained for classiﬁcation and regression tasks.

Springer Nature 2021 LATEX template

An Overview ...

3

• We describe a tool, DNNCov, that incorporates multiple structural test-
ing criteria, including the complex MC/DC coverage proposed in [3] (not
considered in previous evaluations) to enable easy comparison between
diﬀerent testing metrics. DNNCov computes multiple coverage criteria
simultaneously giving a 2.5x time improvement over the baseline sequential
implementation.

• DNNCov has a visualization component that displays the neural network
architecture along with the coverage achieved (neurons covered or not), and
quantitative information (measuring the number of tests that achieve the
coverage). We believe that this nuanced view of the coverage, can enable
developers to better understand and debug the behavior of the neural
networks.

1.1 Applications
DNNCov can be used to compare diﬀerent coverage metrics on the same
model and reason about the eﬀectiveness of a metric in determining test
set adequacy. Moreover, one can compare the coverage metrics over diﬀerent
models (for the same task) to assess how diﬀerent architectures impact test-
ing adequacy. One can also compare diﬀerent test sets with varying sizes to
determine how well they test a given model. Our evaluation (Section 4) has
experiments highlighting these applications. If a smaller test suite achieves
same coverage as that achieved by the full test suite, the test set can also
be reduced. Furthermore, DNNCov can enable coverage-driven test genera-
tion for diﬀerent criteria and can be used for optimizing neural networks, for
instance by pruning the neurons that are never covered (or have low coverage).

1.2 Comparison with Related Studies

There are many approaches and metrics that have been proposed recently,
aiming to measure the testing adequacy of DNN models [1–3]. There are also
frameworks proposed to facilitate such measurements, such as [7]. In contrast
to previous studies, we provide the following contributions. We evaluate dif-
ferent structural metrics with respect to both functional diversity and defect
detection. The latter is evaluated not only with respect to adversarial robust-
ness (which is overwhelmingly the only one studied in the related literature),
but also with respect to poisoning attacks, which provide additional inter-
esting insights on diﬀerent coverage metrics. We consider models trained for
both classiﬁcations and regression tasks. Furthermore, we provide a tool that
incorporates these criteria, including the complex MC/DC criterion not con-
sidered in previous studies. DNNCov provides ﬁner grained results, as it
computes not only qualitative results (as was done also in previous work) but
also quantitative information about how many inputs satisfy the criteria.

Springer Nature 2021 LATEX template

4

An Overview ...

2 Structural Coverage Criteria for Neural

Networks

2.1 Neural Networks.

Neural networks (NNs) are machine learning algorithms that can be trained
to perform diﬀerent tasks such as classiﬁcation and regression. NNs consist of
multiple layers, starting from the input layer, followed by one or more hid-
den layers (such as convolutional, dense, activation, and pooling), and a ﬁnal
decision layer. Each layer consists of a number of computational units, called
neurons. Each neuron applies an activation function on a weighted sum of its
inputs (coming from the previous layer). N (X) = σ((cid:80)
i wi · Ni(X) + b) where
Ni denotes the value of the ith neuron in the previous layer of the network
and the coeﬃcients wi and the constant b are referred to as weights and bias,
respectively; σ represents the activation function. For instance, the ReLU (rec-
tiﬁed linear unit) activation function returns its input as is if it is positive, and
returns 0 otherwise, i.e., σ(X) = max(0, X). The ﬁnal decision layer (logits)
typically uses a specialized function (e.g., max or softmax ) to determine the
decision or the output of the network.

2.2 DNN Structural Coverage Metrics

In this study we evaluate neuron coverage [1] and its extensions [2] and the
MC/DC variants for DNNs [3]. We describe each of these criteria by formu-
lating their test conditions. Given a test suite and a DNN model, the coverage
for each criterion is thus computed as the portion of test conditions that are
satisﬁed.

In the following, we use al,i to denote a neuron’s activation value, where l

is the layer index and i is the neuron index at that layer.
Neuron Coverage (NC). NC can be seen as a statement coverage variant
for DNNs. A neuron nl,i is said to be covered, if its neuron activation value
(al,i) is larger than 0 (or some speciﬁed threshold) at least by one of the test
inputs. Thus, the set of test conditions to be met for NC can be formulated as
follows, where L is the total number of layers in the DNN.

{al,i > 0, 1 < l < L}

Neuron Boundary Coverage (NBC). NBC extends NC by considering the
neuron activations at the maximum and minimum boundary cases. Assuming
highl,i and lowl,i are respectively the estimated lower and upper bounds on the
neuron activation al,i’s value, then we can formulate the set of test conditions
for NBC as follows.

{al,i > highl,i, al,i < lowl,i, 1 < l < L}

The estimation of lower/upper bounds is typically done via proﬁling the
training dataset.

Springer Nature 2021 LATEX template

An Overview ...

5

Strong Neuron Activation Coverage (SNAC). SNAC focuses on test
conditions on corner cases with respect to the upper boundary value.

{al,i > highl,i, 1 < l < L}

K-Multisection Neuron Coverage (KMNC). KMNC divides a neuron’s
activation range between highl,i and lowl,i into K equivalent sections, each
denoted by rangel,i,k, and test conditions in KMNC are deﬁned as the coverage
of these activation sections.

{al,i ∈ rangel,i,k, 1 < l < L, 1 ≤ k ≤ K}

Top-K Neuron Coverage (TKNC). Given a test input x, a neuron is
TKNC covered if its neuron activation value occurs to be one of the most active
K neurons at its layer, denoted by al,i ∈ topK(l, x). Thus, the test conditions
are as follows.

{al,i ∈ topK(l, x), 1 < l < L}
Modiﬁed Condition/Decision Coverage (MC/DC). MC/DC was origi-
nally developed by NASA and has been widely used for testing high integrity
software. It was recently adapted for testing DNNs. Diﬀerent from the cov-
erage criteria above, MC/DC takes into account the relation between neuron
activations at two adjacent layers, such that its test conditions require that
any neuron activation at layer l +1 (decision) must be independently impacted
by each neuron at layer l (condition).

{∀i, j, h, change(al,i) ∧ change(al+1,j) ∧ ¬change(al,h), 1 < l < L − 1}

A Sign (S) change function and a Value (V) change function are deﬁned in
[3] for depicting how a neuron activation changes when the input changes from
a test to another. As a result, there is a family of four variants of MC/DC for
DNNs, including SS coverage (SS), SV coverage (SV), VS coverage (VS) and
VV coverage (VV). To ease the use of MC/DC in large DNN models, in the
later evaluation, we generalize its test conditions from single neurons to sets
of neurons (feature maps) between two adjacent layers.

In this study we focus on structural coverage criteria for DNNs, due to their
popularity, wide use, and similarity to established metrics for general-purpose
software. These criteria are thus more likely to be integrated in certiﬁcation
procedures for safety critical systems that contain neural networks. There are
also other, non-structural proposals for measuring the adequacy of testing
DNNs e.g., the safety coverage [8] and the surprise adequacy [9].

Springer Nature 2021 LATEX template

6

An Overview ...

Fig. 1: The DNNCov Tool.

3 The DNNCov Tool

DNNCov (Figure 1) provides an integrated framework for testing neural net-
work models. The framework takes in an input test set and optionally a training
set and computes the diﬀerent coverage results achieved by the set. The user
also needs to specify a conﬁguration (# of training/test inputs to be used and
criteria to calculate). DNNCov proﬁles the neural network model to calculate
threshold values based on the training set. These threshold values are used
as input to the coverage calculation module. The coverage calculation mod-
ule computes coverage for the diﬀerent metrics. DNNCov then produces a
summarized coverage report and a visualization of the results.

3.1 Implementation, Proﬁling and Coverage Calculation
To implement DNNCov, we leveraged the DeepHunter [5] codebase, which
already provided support for the neuron-based coverage metrics. We extended
it with the four MC/DC variants i.e., Sign-Sign (SS), Sign-Value (SV), Value-
Sign (VS) and Value-Value (VV). We proﬁle the neural network model to
estimate the threshold values needed in some coverage criteria. We wrote new
functions to measure coverage for a given test suite, replacing the original
test-generation process. We also implemented simultaneous coverage computa-
tions to improve eﬃciency (achieving 2.5x runtime improvement). DNNCov
outputs an informative coverage report to the user as illustrated in Figure 2.

3.2 Reporting Quantitative Information
DNNCov computes not only which coverage obligations are fulﬁlled, but also
records the number of tests that achieve the coverage. This quantitative infor-
mation gives us an idea of which parts of the network have been exercised
lesser than others, thus enabling us to direct testing / debugging eﬀorts in

Springer Nature 2021 LATEX template

An Overview ...

7

Fig. 2: Tool Progress on MNIST (LeNet-1)

Fig. 3: Visualization of Neuron Coverage

those parts. This is speciﬁcally important for neural networks since unlike tra-
ditional programs, every test exercises almost every neuron of the network and
the changes in behavior is due to the diﬀerent output values of the neurons.
Therefore executing a neuron only once or just few times may not suﬃce to
exercise diﬀerent behaviors or expose vulnerabilities. Diﬀerent test suites may
have similar total coverage values but diﬀerent coverage distributions. This
cannot be highlighted by existing coverage metrics. We calculate minimum,
maximum, average, standard deviation and variance of the number of inputs
that cover each of the coverage obligations for each coverage criterion.

Springer Nature 2021 LATEX template

8

An Overview ...

3.3 Visualization
The coverage results computed by DNNCov are saved in a ﬁle and are viewed
by the visualization component of the tool (which is built based on Tensor-
ﬂow Playground). This component has a web-based interface that shows the
architecture of the model and quantitative coverage information. The user can
navigate through the model, and visualize speciﬁc information related to the
coverage. Neurons are given a color from white (least covered) to red (most
covered) based on how many times they are covered (see Figure 3). Neuron
pairs covered according to MC/DC criteria are shown as connected with each
other using connection lines. The displayed information can be used to identify
poorly tested areas in the neural network.

3.4 Tool Conﬁguration
Users can load their own trained neural network model using the −−model
option. Users can select the coverage criteria by using the −−criteria option.
There are three possible settings; all calculates all six coverage criteria,
no−mcdc excludes MC/DC and mcdc only calculates MC/DC.

The reason MC/DC is separate is that it works on input pairs whereas
the rest of the metrics are computed for each input. Therefore, calculations of
MC/DC and the other NC variants are implemented with two seperate mod-
ules in DNNCov. All output results (including the intermediate ﬁles used for
visualization) are stored in an output folder. Users can specify the directory
of the output folder using the −−outputs option. Three common datasets can
be automatically loaded in DNNCov:
−−mnist−dataset for MNIST,
−−cifar10−dataset for CIFAR-10 and
−−tinytaxinet−dataset for TaxiNet.
Users’ own datasets can be used by specifying the −−input−tests and
−−input−train folders. The tool also supports calculation over a subset of
training and test dataset. Users can specify the number of training/test inputs
to be used by using the −−train and −−test options. For example,
python dnncov.py −−criteria all −−model lenet1.h5 −−outputs outs1
−−mnist−dataset −−train 60000 −−test 10000
calculates coverage for all six criteria for the lenet1 .h5 model using the default
training and test sets for MNIST.

4 Evaluation

We structure our evaluation along the following thrusts.
• Comparison of coverage metrics on standard test sets.
• Evaluation of coverage information with respect to sensitivity to functional

diversity.

• Evaluation of coverage information with respect to sensitivity to defect

detection; we consider both adversarial and poisoned inputs.

Springer Nature 2021 LATEX template

An Overview ...

9

Table 1: Details of the Datasets and Neural Network Models.

Model

Benchmark

LeNet-1
LeNet-4
LeNet-5
MNIST-Funct.
MNIST-Pois.
MNIST-Adv.

MNIST
MNIST
MNIST
MNIST
MNIST
MNIST

#Dataset
(training,test)
(60k,10k)
(60k,10k)
(60k,10k)
(60k,10k)
(60k,10k)
(60k,10k)

Test
Accuracy
90.6%
89.9%
92.8%
96.34%
Test:98.63, Pois.:10.38%

Model
Architecture
2 conv/2 maxpool
2 conv/2 maxpool/1 dense
2 conv/2 maxpool/2 dense
2 conv/4 act/1 maxpool/2 dense/1 ﬂatten
2 conv/4 act/1 maxpool/2 dense/1 ﬂatten
Test: 97.87%, Adv.: 28.37% 2 conv/4 act/1 maxpool/2 dense/1 ﬂatten

ResNet20

CIFAR-10

(50k,10k)

68.7%

TinyTaxiNet

TaxiNet

(51462,7386)

MAE (CTE:1.44, HE:2.75)

21 conv/19 batchnorm/19 act/9 add
/1 globalavgpool
3 dense

• Evaluation of quantitative and visualization information.

In turn, this information can be used by developers to select among multiple
test sets, favoring the ones for which multiple metrics yield high coverage. Fur-
thermore, developers can use the coverage information to even choose between
models (trained for the same task), favoring the model that again achieves
high coverage for the majority of metrics.
Benchmarks. Table 1 shows the details of the datasets and models used
in our study. We used a set of image classiﬁcation models on benchmark
datasets. MNIST (Modiﬁed National Institute of Standards and Technology
database) [10] is a collection of handwritten digits from '0' to '9'. It has a train-
ing set (MNIST-Train) with 60k inputs and a test set (MNIST-Test) of 10k
inputs, which are 28×28 grey scale images. We used the popular LeNet convo-
lutional neural network model trained for image classiﬁcation on the MNIST
dataset [11, 12]. CIFAR-10 (Canadian Institute For Advanced Research) [13]
is a collection of color images classiﬁed to one of 10 classes which include vehi-
cles such as 'airplane', 'truck' so on and animals such as bird, cat so on. The
training set (CIFAR-Train) contains 50k 32×32 color images and a test set
(CIFAR-Test) of 10k images. We used the state-of-the-art ResNet20 model for
image classiﬁcation on this dataset [14].

We also applied DNNCov on a regression model from the autonomy
domain, namely TaxiNet, which is a perception model for center-line track-
ing in airport runways [15]. The model takes images of the runway as input
and produces two outputs, cross-track (CTE) and heading angle (HE) errors
which indicate the lateral and angular distance respectively of the nose of the
plane from the center-line of the runway. We use a model called the Tiny Tax-
inet [15] which takes in a down-sampled 8×16 image of the runway. It has a
training set and test set with 51462 and 7386 inputs respectively.

The tool, neural network models along with the datasets and Appendix
are publicly available at the GitHub repository1. All experiments were run on
a machine with an Intel Core i9-9980HK processor and 64GB RAM running
Windows 10. All experiments were repeated 5 times and average results are
reported.

1https://github.com/DNNCov/DNNCov

Springer Nature 2021 LATEX template

10

An Overview ...

Table 2: Coverage of the Neural Network Models for the standard Test Sets.
LeNet-4
85.29
54.33
81.59
3.27
0.66
1.05
90.58
76.09
63.77
59.42
44.17
35.68
58.20
67.21

LeNet-5 ResNet20 TinyTaxiNet
62.35
98.75
43.54
71.16
52.06
65.09
0.59
3.90
2.01
5.55
3.21
6.46
67.65
100.00
67.65
100.00
61.76
99.16
52.94
13.99
27.60
24.66
28.65
51.58
41.67
52.20
46.88
99.63

Coverage Metrics (%)
KMNC (K: 10)
KMNC (K: 1000)
TKNC (K: 10)
TKNC (K: 1000)
NBC
SNAC
NC (Threshold: 0.00)
NC (Threshold: 0.20)
NC (Threshold: 0.50)
NC (Threshold: 0.75)
MC/DC (Sign-Sign)
MC/DC (Sign-Value)
MC/DC (Value-Sign)
MC/DC (Value-Value)

LeNet-1
95.0
60.23
88.57
1.0
0.87
0.87
100.0
61.9
30.95
23.81
5.77
63.46
23.08
100.00

90.7
59.10
82.40
4.93
0.58
1.16
96.12
85.66
74.03
67.05
10.62
8.40
13.71
15.23

4.1 Comparison of coverage metrics

Table 2 depicts the results obtained for diﬀerent coverage metrics, in terms of
percentage of covered obligations, when testing the models using the default
test data from MNIST, CIFAR-10 and Tiny TaxiNet. The respective training
sets were used to obtain threshold values for measures such as KMNC, NBC,
SNAC and MC/DC metrics. As expected, NC (Threshold: 0.00) seems the
easiest to achieve. The NC value is greater than 67% for all the models, with
100% for the LeNet-1 and even the most complex ResNet20 model. Setting
the threshold to 0.75 reduces the values for all of the models.

The KMNC (K: 10) assesses if the tests cover diﬀerent ranges of neuron
values. We can observe that the value for this metric is greater than 85% for
the LeNet and ResNet20 models whereas it is only 62% for Tiny TaxiNet. This
appears reasonable considering that the test sets for LeNet and ResNet20 are
the standard test sets for the widely studied MNIST and CIFAR-10 respec-
tively, which are expected to test the respective models adequately. On the
other hand, the test set for Tiny TaxiNet is generated by simulation which
may not have a good coverage of diﬀerent neuron values. A similar trend can
be observed for TKNC (K: 10).

The NBC and SNAC metrics determine if there are any tests that exer-
cise neurons beyond the boundaries observed for the training sets. Looking at
the results we can understand that the test set for the MNIST, CIFAR and
Tiny TaxiNet benchmarks are very close in their distribution to the respective
training sets which results in low values for these two metrics.

The MC/DC coverage appears to be more diﬃcult to achieve than the other
structural metrics. This is expected considering that it involves satisfaction of
more constraints. The Value-Value variant has generally the highest coverage
values and seems easier to achieve than the Sign-Sign counterparts. The test set
for MNIST has better MC/DC (VV) coverage values for LeNet-1 and LeNet-
4, whereas the LeNet-5 model with a more complex architecture has the least
MC/DC (VV) coverage values.

Springer Nature 2021 LATEX template

An Overview ...

11

Fig. 4: Comparison of Coverage Metrics for LeNet-5.

Figure 4 shows in more detail how the metrics compare for the same test
set on the same model (LeNet-5). The graphs show how the coverage values
change as the number of tests increase. As can be seen the KMNC, NBC and
SNAC metrics display a more gradual increase in comparison with NC and
TKNC. This indicates that the KMNC, NBC and SNAC may be better for
indicating the quality of the test set better than the other metrics. For instance,
the maximum values for TKNC and NC can be achieved by executing just
10% of the test set. The additional tests which are redundant with respect to
these metrics are actually useful since they cover diﬀerent neuron value ranges.
Also note that accuracy on the test sets is typically used to judge the
quality of a trained model. Table 1 provides the statistical test set accuracy
of the models. For instance, LeNet-5 has 92.8% accuracy while Tiny TaxiNet
has Mean Absolute Error for “CTE”: 1.44 and for “HE”: 2.75. A user may
consider these models to have good generalization and accuracy on unseen
inputs. However, the corresponding coverage measures highlight how much this
test set accuracy could be trusted. For instance, the Tiny TaxiNet model has
low coverage scores indicating that test set may not have suﬃciently tested
the behaviors of the model.

4.2 Evaluating coverage metrics with respect to

functional diversity

We use each DNN output class as a proxy for the functionality of the model. A
dataset is considered to be functionally diverse if it contains inputs belonging to
a comprehensive range of output classes. For example, a dataset which consists
of inputs belonging to ten output classes has higher functional diversity than
a dataset which consists of inputs belonging to only one output class (even if
the structural coverage is the same). Intuitively, a functionally diverse test set
is better in testing and debugging neural network models, as it covers more

Springer Nature 2021 LATEX template

12

An Overview ...

(a) Accuracy

(b) Functional Diversity

(c) Defect Detection (Adversarial(cid:15)=0.30)

(d) Defect Detection (Poisoned)

Fig. 5: Summary of Results

behaviors. If a test coverage metric is sensitive to functional diversity, it should
obtain higher coverage when the dataset contains inputs belonging to multiple
output classes, as opposed to fewer classes.

To evaluate the sensitivity of each metric towards functional diversity, we
create six diﬀerent datasets from the clean MNIST test set (MNIST-Test). We
name the datasets as Fα where α is the number of randomly selected output
classes whose inputs are included in the respective dataset. For example, F1
represents the dataset which consists of inputs belonging to one randomly
selected output class and F10 is the dataset which consists of inputs belonging
to ten output classes. Similarly, we have F2, F3, F5 and F7. The size of each
dataset is ﬁxed to 800.

To better compare the diﬀerent coverage metrics, we normalize the results
as follows. For each dataset, we ﬁrst calculate ∆α which is essentially the
diﬀerence (Eq. 1) between the coverage value of the metric on the baseline
dataset (F1) and the respective dataset (Fα). Max(∆α) is the maximum ∆α
across datasets F1 to F10. Min(∆α) is the minimum ∆α across datasets F1
to F10. We then calculate normalized diﬀerence (NCoverage(Fα)) in coverage
(Eq. 2).

∆α = CoverageFα − CoverageF1

N Coverage(Fα) =

CoverageFα − CoverageF1
M ax(∆α) − M in(∆α)

(1)

(2)

Springer Nature 2021 LATEX template

An Overview ...

13

Table 3: Accuracy of Model on Datasets. Column ”Dataset” shows the
name of the Datasets. Column ”MNIST (Functional)” shows the Accuracy for
Datasets F1 to F10. Column ”MNIST (Adversarial)” shows the Accuracy for
Datasets A0 to A10. Column ”MNIST (Poisoned)” shows the Accuracy for
Datasets P0 to P10.

Dataset

F0/A0/P0
F1/A1/P1
F2/A2/P2
F3/A3/P3
F5/A5/P5
F7/A7/P7
F10/A10/P10

MNIST
(Functional)
N/A
97.00
97.48
95.93
96.30
96.65
96.60

MNIST
(Adversarial)
98.22
97.26
96.26
95.32
93.42
91.40
88.50

MNIST
(Poisoned)
98.52
97.70
96.74
95.76
93.96
92.32
89.60

Figure 5b summarizes the results. Detailed results are available in the
Appendix. We consider the coverage obtained on dataset F1 as the baseline.
We can see that as the value of α is increased from 1 to 10, the metrics
report higher coverage (shown by the high normalized diﬀerence value). With
the exception of NC (Threshold: 0.00), MC/DC (SS) and MC/DC (SV), all
of the remaining 11 metrics/conﬁgurations shows highest coverage on dataset
F10. NC (Threshold: 0.00) achieves 100% coverage on all datasets (even when
the dataset consists of inputs belonging to only one output class) whereas
MCDC(SS) and MC/DC (SV) obtains 0% coverage on all datasets (even when
the dataset has inputs belonging to all output classes). Due to this issue, we
cannot observe any diﬀerence in these three coverage metrics across all the
datasets.

Figure 5a and Table 3 shows that the accuracy of the neural network model
remains between 95% and 98%. Thus, the accuracy metric fails to diﬀerentiate
between these datasets.

4.3 Evaluating coverage metrics with respect to defect

detection.

We evaluate the diﬀerent metrics by testing the defect detection ability, with
respect to adversarial robustness and data poisoning scenarios.

4.3.1 Adversarial Scenario.

In an adversarial attack [16], a neural network mis-classiﬁes the adversarial
input that is obtained by adding adversarial perturbations on the original
input. Testing the neural network model with adversarial inputs can help one
identify if the neural network is robust towards adversarial attacks.

Springer Nature 2021 LATEX template

14

An Overview ...

For this scenario, we apply perturbations to MNIST images in the training
(MNIST-Train) and test sets (MNIST-Test) using the FGSM (Fast Gradi-
ent Sign Method) attack [16], with (cid:15) ∈[0.01, 0.05, 0.10, 0.20, 0.30] being used
for controling the perturbation level. This gives us MNIST-Adversarial-Train
and MNIST-Adversarial-Test respectively. Due to space limitations, we report
results for (cid:15) = 0.30 in this paper. Results for other values of (cid:15) are available in
the Appendix. Intuitively, if a coverage metric has the capability to check the
defect detection ability of a dataset, it should obtain higher coverage when the
dataset contains inputs that can reveal defects in the neural network model.
We create dataset A0 by randomly selecting 1000 inputs from the clean MNIST
test set (MNIST-Test). We consider the coverage obtained on this dataset
as the baseline. From this baseline dataset (A0), we further create six dif-
ferent datasets by replacing β% of clean inputs with adversarial inputs. We
name these datasets as Aβ. For example, A1 represents the dataset in which
we replaced 1% (10) of the clean inputs with the same number of randomly
selected adversarial inputs and A10 is the dataset in which we replaced 10%
(100) of the clean inputs with the same number of randomly selected adver-
sarial inputs. Similarly, we have A2, A3, A5 and A7. The size of each dataset
is 1000.

For each dataset, we ﬁrst calculate ∆β which is essentially the diﬀerence
(Eq. 3) between the coverage value of the metric on the baseline dataset (A0)
and the respective dataset (Aβ). Max(∆β) is the maximum ∆β across datasets
A0 to A10. Min(∆β) is the minimum ∆β across datasets A0 to A10. We then
calculate normalized diﬀerence (NCoverage(Aβ)) in coverage (Eq. 4).

∆β = CoverageAβ − CoverageA0

N Coverage(Aβ) =

CoverageAβ − CoverageA0
M ax(∆β) − M in(∆β)

(3)

(4)

Figure 5c summarizes the results. Detailed results are available in the
Appendix. We can see that as the value of β is increased from 0 to 10, ten met-
rics (with the exception of NC (Threshold: 0.00), NC (Threshold: 0.20), NC
(Threshold: 0.75) and MC/DC (VV)) report higher coverage (shown by the
high normalized diﬀerence value). NC (Threshold: 0.00), NC (Threshold: 0.20),
NC (Threshold: 0.75) and MC/DC (VV) report similar coverage across all
datasets. Due to this issue, we cannot observe any diﬀerence in these coverage
metrics across all the datasets. Figure 5a and Table 3 shows that the accuracy
of the model is highest on clean dataset (A0) and it gradually decreases as the
value of β is increased. This is expected because as the number of adversar-
ial inputs increases in the dataset, there are more inputs on which the neural
network fails to predict the correct class.

4.3.2 Poisoned Scenario.

In a data poisoning scenario [17], an attacker has ’poisoned’ the training data
such that the model has high accuracy on clean data but when it is presented

Springer Nature 2021 LATEX template

An Overview ...

15

Fig. 6: Example poisoned data for MNIST (left) and CIFAR-10 (right). The
backdoor trigger is embedded as the white square at the bottom right corner
of each image. When the backdoor appears, the poisoned MNIST model will
classify the input as '7' and the poisoned CIFAR-10 model will classify it as
'horse'.

with an input that contains a poisoned ’trigger’, the model mis-classiﬁes the
respective input to a target output class. Testing the neural network model
with poisoned inputs can help one identify if the neural network contains a
backdoor which can be exploited by a malicious actor.

For the poisoned scenario, we apply the backdoor attack from [17]. Figure
6 shows some examples of poisoned attacks. We applied the backdoor trigger
to the ﬁrst 600 inputs in the MNIST training set (MNIST-Train); rest of the
59400 inputs are not changed. This results in the MNIST-Poisoned-Train set.
For evaluation, we created a poisoned test set (MNIST-Poisoned-Test) from
the MNIST test set (MNIST-Test).

We create dataset P0 by randomly selecting 1000 inputs from the clean
MNIST test set (MNIST-Test). We consider the coverage obtained on this
dataset as the baseline. From this baseline dataset (P0), we further create six
diﬀerent datasets by replacing β% of clean inputs with poisoned inputs. We
name these datasets as Pβ. For example, P1 represents the dataset in which
we replaced 1% (10) of the clean inputs with the same number of randomly
selected poisoned inputs and P10 is the dataset in which we replaced 10%
(100) of the clean inputs with the same number of randomly selected poisoned
inputs. Similarly, we have P2, P3, P5 and P7. The size of each dataset is 1000.
For each dataset, we ﬁrst calculate ∆β which is essentially the diﬀerence
(similar to Eq. 3) between the coverage value of the metric on the baseline
dataset (P0) and the respective dataset (Pβ). Max(∆β) is the maximum ∆β
across datasets P0 to P10. Min(∆β) is the minimum ∆β across datasets P0
to P10. We then calculate normalized diﬀerence (NCoverage(Pβ)) in coverage
(similar to Eq. 4).

We repeated similar experiments using poisoned data. Figure 5d summa-
rizes the results. Detailed results are available in the Appendix. We can see
that as the value of β is increased from 0 to 10, ﬁve metrics (KMNC (K: 10),
KMNC (K: 1000), TKNC (K: 1000), NBC and SNAC ) report higher cover-
age (shown by the high normalized diﬀerence value). All variants of NC and
MC/DC fail to diﬀerentiate between the datasets because they report almost
similar coverage across all datasets. Four of the metrics (TKNC (K: 10), NC
(Threshold: 0.50), MC/DC (SV) and MC/DC (VS)) even report lesser cover-
age on poisoned datasets. NC (Threshold: 0.00) and MC/DC (VV) achieves
100% coverage on all datasets (even when the dataset does not contain any
poisoned inputs) and MC/DC (SS) obtain 0% coverage on all datasets (even
when the dataset consists 10% of poisoned inputs). Due to this, we cannot

Springer Nature 2021 LATEX template

16

An Overview ...

Table 4: Quantitative Information for Datasets.

Scenario

Dataset Metric

Functional

Adversarial(cid:15)=0.30

Poisoned

F1

F10

A0

A10

P0

P10

Min
Max
Avg
Std.
Var.

Min
Max
Avg
Std.
Var.
Min
Max
Avg
Std.
Var.

Min
Max
Avg
Std.
Var.
Min
Max
Avg
Std.
Var.

Min
Max
Avg
Std.
Var.

KMNC
(10)
0.00
254.60
44.26
67.43
4,546.93

KMNC
(1000)
0.00
16.00
0.80
1.67
2.81

0.00
254.80
72.01
76.94
5,919.58
0.00
255.00
74.26
78.73
6,198.64

0.00
255.00
80.15
78.65
6,184.81
0.00
255.00
65.99
74.18
5,503.32

0.00
255.00
68.33
75.30
5,669.85

0.00
10.80
0.80
1.25
1.55
0.00
12.60
0.99
1.42
2.03

0.00
11.60
0.98
1.37
1.88
0.00
12.60
1.00
1.49
2.21

0.00
12.00
1.00
1.46
2.15

TKNC
(10)
0.00
245.60
9.96
28.09
789.82

0.00
228.80
11.23
24.34
592.63
0.00
243.80
14.98
33.36
1,112.89

0.00
233.00
14.91
32.04
1,026.34
0.00
247.00
15.84
33.46
1,120.00

0.00
247.40
15.74
32.92
1,084.01

TKNC
(1000)
0.00
245.60
0.74
4.80
23.07

0.00
228.80
0.75
4.01
16.11
0.00
246.40
0.94
6.42
41.19

0.00
247.20
0.94
6.25
39.04
0.00
250.20
0.95
5.05
25.46

0.00
247.40
0.95
4.98
24.84

NBC SNAC NC (0) NC (0.2) NC (0.5) NC (0.75) MC/DC (SS) MC/DC (SV) MC/DC (VS)

MC/DC (VV)

0.00
0.80
0.00
0.02
0.00

0.00
0.80
0.00
0.02
0.00
0.00
0.80
0.00
0.02
0.00

0.00
69.20
0.34
3.17
10.10
0.00
1.00
0.00
0.03
0.00

0.00
64.60
0.03
1.15
1.33

0.00
0.80
0.00
0.03
0.00

0.00
0.80
0.00
0.03
0.00
0.00
0.80
0.00
0.03
0.00

0.00
69.20
0.68
4.46
19.96
0.00
1.00
0.00
0.03
0.00

0.00
64.60
0.07
1.63
2.66

2.80
251.00
40.91
41.07
1,701.20

3.20
246.80
39.53
39.64
1,573.37
0.20
232.00
218.81
40.92
1,674.34

0.00
232.00
218.81
39.96
1,596.77
30.80
232.60
221.60
32.65
1,071.57

35.40
234.20
221.95
31.16
971.28

0.00
251.40
64.62
67.64
4,622.98

0.00
254.40
80.42
83.46
6,967.05
0.00
246.00
196.66
61.28
3,755.88

0.00
251.60
194.41
60.96
3,717.62
0.00
247.40
199.85
56.13
3,151.56

0.00
246.80
199.37
54.54
2,975.74

0.00
253.40
85.99
82.06
6,738.52

0.00
253.80
110.70
81.18
6,590.26
0.00
252.20
94.57
73.75
5,440.95

0.00
253.40
93.05
73.37
5,384.96
0.00
253.80
122.49
77.95
6,076.93

0.00
254.40
118.89
78.35
6,139.18

0.00
250.40
42.92
63.53
4,056.32

0.00
252.60
80.85
75.04
5,630.99
0.00
244.80
55.21
71.70
5,143.82

0.00
246.60
56.35
73.38
5,386.06
0.00
248.20
99.68
73.52
5,405.70

0.00
249.80
95.75
73.83
5,452.29

0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
2,363.60
6.08
114.16
13,825.67
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.00
0.00

0.00
47,428.40
14,994.11
14,461.88
217,047,915.68

0.00
0.00
0.00
0.00
0.00
0.00
98,840.40
749.13
7,946.83
63,197,042.79

0.00
95,831.60
797.94
7,496.02
56,250,911.82
0.00
156,211.40
882.18
10,372.68
107,599,179.64

0.00
155,748.20
878.16
10,327.47
106,676,969.06

0.00
90,890.80
44,167.25
28,187.31
795,678,292.52
0.00
125,897.80
20,325.60
40,408.32
1,633,247,596.37

0.00
146,695.20
22,522.68
44,955.50
2,021,423,808.66
0.00
166,161.00
66,915.34
38,601.23
1,490,070,791.53

0.00
162,128.00
67,644.00
38,962.82
1,518,177,548.27

216.20
80,757.40
15,958.12
18,205.57
343,441,740.25

7,972.00
142,748.00
40,287.12
32,664.23
1,069,718,510.41
0.00
199,146.00
35,183.88
44,938.10
2,019,902,359.70

0.00
242,320.40
44,255.64
58,645.70
3,439,790,761.82
12,986.40
219,296.00
43,847.31
31,083.70
966,466,131.37

13,404.20
217,657.80
45,336.55
30,707.17
943,247,524.91

observe any diﬀerence in these coverage metrics across all the datasets. There-
fore, these metrics are not good for checking the defect detection ability of the
dataset.

4.4 Quantitative and Visualization information.

We also discuss brieﬂy the quantitative information that can be computed with
DNNCov. We calculate minimum, maximum, average, standard deviation and
variance of the number of inputs that cover each of the coverage obligations
for each coverage criterion. The summarized results are shown in Table 4.
Detailed results are available in the Appendix. Diﬀerent test suites may have
similar total coverage values but diﬀerent coverage distributions. This cannot
be highlighted by existing coverage metrics. For example, in Section 4.3.1, we
mentioned that MC/DC (VV) remained constant across all datasets (A0 to
A10). However, if we analyze the quantitative information (see A0 and A10 rows
in Table 4), we can see that even though the MC/DC (VV) remained 100%
across A0 and A10, on average more input pairs (44255 vs 35183) covered each
neuron pair on A10 as compared to A0. This shows that dataset A10 is better
than dataset A0 and it may reveal more defects in the neural network model.
Figure 3 shows the visualisation report generated by DNNCov. We can
see that neurons in layer 5 are covered fewer times as compared to neurons in
the layer 1. Moreover, we can observe that neuron 4 in layer 2 is never covered.
Either this neuron is not required and it can be pruned from the neural network
or the test set has not properly tested this neuron.

5 Conclusion

In this paper we presented and evaluated several recently proposed structural
coverage criteria for testing neural networks. We also described DNNCov,
an integrated tool for helping developers with testing neural networks, by

Springer Nature 2021 LATEX template

An Overview ...

17

providing the means to measure, visualize and compare diﬀerent, state-of-
the-art testing criteria for neural networks. The results indicate that many
of the existing structural metrics are not sensitive to functional diversity and
defect detection abilities in test suites. To address these limitations, we plan
to develop new coverage metrics that are still in terms of the structure of the
DNN, but also have semantic meaning, such as activation patterns [18]. We
also plan to add more structural coverage criteria to the tool, as they appear in
the related research. We are also working on a method for measuring KMNC,
NBC, and SNAC coverage in the absence of the training set. One idea is to
classify statistical outliers as boundary cases that would be part of NBC/SNAC
and determine KMNC based on the non-boundary cases. Finally, we plan to
improve the visualization component in DNNCov to display more information
about the network architecture.

Springer Nature 2021 LATEX template

18

An Overview ...

References

[1] Pei, K., Cao, Y., Yang, J., Jana, S.: DeepXplore: Automated whitebox

testing of deep learning systems. In: SOSP (2017)

[2] Ma, L., Juefei-Xu, F., Zhang, F., Sun, J., Xue, M., Li, B., Chen, C., Su,
T., Li, L., Liu, Y., et al.: DeepGauge: Multi-granularity testing criteria
for deep learning systems. In: ASE (2018)

[3] Sun, Y., Huang, X., Kroening, D., Sharp, J., Hill, M., Ashmore, R.: Struc-
tural test coverage criteria for deep neural networks. ACM Transactions
on Embedded Computing Systems (TECS) (2019)

[4] Lee, S., Cha, S., Lee, D., Oh, H.: Eﬀective white-box testing of deep neural
networks with adaptive neuron-selection strategy. In: ISSTA (2020)

[5] Xie, X., Ma, L., Juefei-Xu, F., Xue, M., Chen, H., Liu, Y., Zhao, J., Li, B.,
Yin, J., See, S.: DeepHunter: A coverage-guided fuzz testing framework
for deep neural networks. In: ISSTA (2019)

[6] Sun, Y., Wu, M., Ruan, W., Huang, X., Kwiatkowska, M., Kroening, D.:

Concolic testing for deep neural networks. In: ASE (2018)

[7] Tian, Y., Zeng, Z., Wen, M., Liu, Y., Kuo, T.-y., Cheung, S.-C.:
Evaldnn: A toolbox for evaluating deep neural network models. In: 2020
IEEE/ACM 42nd International Conference on Software Engineering:
Companion Proceedings (ICSE-Companion), pp. 45–48 (2020)

[8] Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety veriﬁcation of

deep neural networks. In: CAV (2017)

[9] Kim, J., Feldt, R., Yoo, S.: Guiding deep learning system testing using

surprise adequacy. In: ICSE (2019)

[10] Deng, L.: The MNIST database of handwritten digit images for machine

learning research. IEEE Signal Processing Magazine (2012)

[11] LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning

applied to document recognition. Proceedings of the IEEE (1998)

[12] LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E.,
Hubbard, W.E., Jackel, L.D.: Handwritten digit recognition with a
back-propagation network. In: NIPS (1989)

[13] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features

from tiny images (2009)

[14] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image

Springer Nature 2021 LATEX template

An Overview ...

19

recognition. In: CVPR (2016)

[15] Julian, K.D., Lee, R., Kochenderfer, M.J.: Validation of image-based
neural network controllers through adaptive stress testing. In: ITSC
(2020)

[16] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow,
I., Fergus, R.: Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199 (2013)

[17] Gu, T., Liu, K., Dolan-Gavitt, B., Garg, S.: Badnets: Evaluating back-
dooring attacks on deep neural networks. IEEE Access 7, 47230–47244
(2019). https://doi.org/10.1109/ACCESS.2019.2909068

[18] Gopinath, D., Converse, H., Pasareanu, C.S., Taly, A.: Property inference
for deep neural networks. In: 34th IEEE/ACM International Conference
on Automated Software Engineering, ASE 2019, San Diego, CA, USA,
November 11-15, 2019, pp. 797–809. IEEE, ??? (2019). https://doi.org/
10.1109/ASE.2019.00079. https://doi.org/10.1109/ASE.2019.00079

