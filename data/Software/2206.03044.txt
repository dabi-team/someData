CAISAR: A platform for Characterizing Artificial
Intelligence Safety and Robustness

Julien Girard-Satabin1,‚Ä†, Michele Alberti1,‚Ä†, Fran√ßois Bobot1, Zakaria Chihani1 and
Augustin Lemesle1

1 Universit√© Paris-Saclay, CEA, List, F-91120, Palaiseau, France

Abstract
We present CAISAR, an open-source platform under active development for the characterization of AI systems‚Äô robustness and
safety. CAISAR provides a unified entry point for defining verification problems by using WhyML, the mature and expressive
language of the Why3 verification platform. Moreover, CAISAR orchestrates and composes state-of-the-art machine learning
verification tools which, individually, are not able to efficiently handle all problems but, collectively, can cover a growing number
of properties. Our aim is to assist, on the one hand, the V&V process by reducing the burden of choosing the methodology
tailored to a given verification problem, and on the other hand the tools developers by factorizing useful features ‚Äì visualization,
report generation, property description ‚Äì in one platform. CAISAR will soon be available at https://git.frama-c.com/pub/caisar.

Keywords
formal verification, platform, formal methods, testing, metamorphic testing, reachability analysis, satisfaction modulo theory,
artificial intelligence,

1. Introduction

The integration of machine learning programs as compo-
nents of critical systems is said to be bound to happen;
initiatives from various private and govermental actors
(e.g., US‚Äô NSF funding for Trustworthy AI1, France‚Äôs
Grand D√©fi IA de confiance2) are a consequence of that
fact. Trusting such programs is thus becoming a crucial
issue, both on technical and ethical sides.

A possible approach to trust is formal test and verifica-
tion, a broad set of techniques and tools that have been
applied to software safety for several decades. These for-
mal methods build on sound mathematical foundations
to assess the behaviour of programs in a principled way,
be it for generating tests or providing proven guarantees.
For the last few years, several independent works have
started to investigate the possible applications of formal
verification to machine learning program verification and
its limitations. This led to what could be characterized as
a Cambrian explosion of tools aiming to solve a particular
subset of the machine learning verification field. In less
than five years, more than 20 tools were produced, up-

The IJCAI-ECAI-22 Workshop on Artificial Intelligence Safety
(AISafety 2022), July 24-25, 2022, Vienna, Austria
‚Ä†

These authors contributed equally.
" julien.girard2@cea.fr (J. Girard-Satabin); michele.alberti@cea.fr
(M. Alberti); francois.bobot@cea.fr (F. Bobot);
zakaria.chihani@cea.fr (Z. Chihani); augustin.lemesle@cea.fr
(A. Lemesle)
(cid:18) 0000-0001-6374-3694 (J. Girard-Satabin)

¬© 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License
Attribution 4.0 International (CC BY 4.0).
CEUR Workshop Proceedings (CEUR-WS.org)
1https://www.nsf.gov/pubs/2022/nsf22502/nsf22502.htm
2https://www.gouvernement.fr/grand-defi-securiser-certifier-et-
fiabiliser-les-systemes-fondes-sur-l-intelligence-artificielle

graded or abandoned. These tools use different techniques,
different input formats, handle different ML artefacts and,
most importantly, have varying performances depending
on the problems. Our goal of orchestrating multiple tools
aims at maximizing the property coverage in the context
of a validation and verification process.

To this end, we present a platform dedicated to Char-
acterizing Artificial Intelligence Safety and Robustness
(CAISAR) that aims to unify several formal methods and
tools, at the input, through the use of a mature and expres-
sive property description and proof orchestration language,
at the output, through the factorization of features such
as visualization and report generation, and at the usage,
through shared heuristics and interconnections between
tools.

The answer to the question ‚ÄúTo what extent can I trust
my machine learning program?‚Äù has many components,
ranging from data analysis to decision explainability. One
such important components is dealing with verification
and validation, and we wish to make CAISAR an im-
portant element in the safety toolbelt by covering these
applications.

In the following, we will first present the design princi-
ples of CAISAR and state its main goals. We will follow
by a description of its most prominent features, as well
as its limitations. We will then explain the position of
CAISAR regarding other tools for formal verification of
machine learning programs, and conclude by presenting
some future work and possible research problems.

2
2
0
2

n
u
J

1
2

]
I

A
.
s
c
[

2
v
4
4
0
3
0
.
6
0
2
2
:
v
i
X
r
a

CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 
 
 
 
 
 
2. Core principles of CAISAR

The aim for CAISAR is to provide a verification envi-
ronment for Artificial Intelligence (AI) based systems
tailored to different needs. The profusion of tools for AI-
programs certification offers numerous possibilities, from
the choice of technology (formal methods, test generation)
to the scope of properties to check (coverage, suitability to
a given distribution, robustness). However, with increased
possibilites comes the burden of choice. Which method
better suits a given use case? Are the results provided
by this particular method trustworthy enough? How to
bring trust in the process of selecting, tailoring and com-
puting results of a given tool? How to evaluate a given
tool against others? Those are the questions that we aim
to answer with CAISAR.

2.1. Compatibility with existing and

future methods

The first principle of CAISAR is to ease this burden of
choice by automating parts of it. CAISAR aims to provide
a unique interface for vastly different tools, with a single
entry point to specify verification goals. Choosing which
tool to use is an informed decision that may not be relevant
for the user; the goal is to provide an actionable answer on
the safety of the system, by using whatever tool is suitable
for the problem. Ideally, the user should not be bothered
with deciding which tool is suitable for their use case:
CAISAR will automatically figure out how to express the
given property to suit verifiers. As AI systems pipelines
are becoming more and more complex, it is crucial for
CAISAR to handle this complexity. Currently, CAISAR
supports neural networks and support vector machines,
and an industrial benchmark of an ensemble model (NN-
SVM), which we are unable to further discuss, is being
used as a concrete real-word use-case.

2.2. Common modelling and answers

Existing verifiers rely on different decision procedures,
e.g., Mixed Integer Linear Programming (MILP), abstract
interpretation, or Satisfaction Modulo Theory (SMT) cal-
culus. Modelling a verification problem using these frame-
works require different skills and is time-consuming; for
instance, some modelling choices made for MILP may not
be applied under SMT. Moreover, even if one succeeds in
phrasing a verification problem under multiple decision
procedures, the different results may not be immediately
comparable.

CAISAR aims to provide a common ground for inputs
and outputs, which will lead to an easier comparison,
lower time consumption and an informed decision. Fur-
thermore, collecting and presenting the user with multiple
answers from different techniques can provide additional

confidence on the studied system. In order for users to
trust CAISAR as well, it needs to rely on well-known and
approved principles and technologies. It is developed in
OCaml, a strongly typed programming language, used
to develop tools for program verification and validation.
Such tools include Coq [1], a proof assistant that for in-
stance used to develop CompCert[2], a C compiler that
is guaranteed to output C-ANSI compliant source code,
Frama-C [3], a platform for the static analysis of C code,
and Why3 [4], a platform for deductive verification of
programs.

2.3. Tools composition

Some works are starting to combine multiple tech-
niques [5] for their analysis, using an exact MILP solver
to refine bounds obtained by abstract interpretation. Our
goal with CAISAR is to bring tool composition to an-
other level. For instance, metamorphic transformations
could generate different input space partitions for formal
verifiers. A reachability analysis tool could be called nu-
merous times with tighter bounds until reaching a precise
enough answer. Coverage testing objectives could be ex-
tracted from reachability analysis tools and fed to test
generators. CAISAR will be more than the sum of its part,
allowing communication between vastly different tools to
provide faster and more accurate answers.

2.4. Automatic proposal of verification

strategies

A long-term goal for CAISAR is to provide a reason-
ing engine where past verification problems processed
by CAISAR can inform next ones, gradually building a
knowledge base that is suitable for the specific needs of
the user. CAISAR will also implement its own built-in
heuristics to supplement specialized programs that do not
implement them.

3. Architecture and features

CAISAR‚Äôs architecture can be divided into the following
functional blocks:

1. A Common specification Language (CL)
2. A Proof Obligation Generator (POG), associated

with a Dispatcher (DISP)

3. An Intermediate Representation (IR)
4. A visualization module (VIZ)

See fig. 1 for a visual depiction of dependencies between
blocks.

Ada programs [6]. This global expressiveness and safety
allows to write ùí± once and for all, independently of the
verifier. For instance, Figure 2 shows the definition of
robustness against a perturbation of amplitude ùúÄ using the
ùëô‚àû distance within CAISAR‚Äôs standard library, and fig. 3
the WhyML file the user need to write in order to verify
the robustness of a given TestSVM against a perturbation
of amplitude 0.5. Note that all the necessary element to
define ùí±, namely ùëì , ùí≥ , ùí´ and ùí¨, are defined in those
files: ùëì is the function TestSVM, and ùí¨ is the predicate
itself. Note that WhyML is not limited to the robustness
against a given perturbation property, often met in the
literature. For instance, asserting that a neural network
respect the properties of being differentialy private [7]
or respecting causal fairness [8] is something that could
be phrased as WhyML programs, since those properties
have a mathematical characterization. Finally, WhyML
does not constrain the form of ùí´ nor ùí¨. In particular, it is
possible to define multiple verification goals in the same
ùí±, opening the way to subdivide it into subproblems, and
providing answers at each step.

CAISAR then automatically translates ùí± into a format
supported by the selected verifiers, through a succession
of encoding transformations and simplifications. For in-
stance, some verifiers are best used when trying to falsify
the property: instead of checking

‚àÄùë• ‚àà ùí≥ , ùí´(ùë•) ‚áí ùí¨(ùëì (ùë•), ùë•)

they instead try to satisfy the negation

‚àÉùë• ‚àà ùí≥ , ùí´(ùë•) ‚àß ¬¨ùí¨(ùëì (ùë•), ùë•)

This transformation is embedded in CAISAR, when call-
ing Marabou: a verification problem ùí± can be trans-
formed into an equivalent one ùí±
that can be dispatched
to Marabou.

‚Ä≤

3.2. Proof Obligations Generations for

various tools

CAISAR currently supports a variety of tools and tech-
niques: metamorphic testing, reachability analysis based
on abstract interpretation and constraint-based propaga-
tions. CAISAR can analyze neural networks and support
vector machines. This versatility allows for CAISAR to
verify system components using different machine learn-
ing architectures.

Marabou

Marabou [9] is a deep neural network verification com-
plete verifier. Its core routine relies on a modified sim-
plex algorithm that lazily relaxes constraints on piecewise
linear activation functions. Marabou also makes use of
several heuristics that help speeding up the verification

Figure 1: CAISAR overall architecture

3.1. Specification language and
verification predicates

A typical task for program verification involves to solve
a verification problem. A verification problem consists
on checking that a program with a given set of inputs is
meeting certain expectations on its outputs. More for-
mally, let ùí≥ be an input space, ùí´(¬∑) be a property on the
input space, ùëì be a program, and ùí¨(¬∑, ¬∑) be a property
on the output space. By property, we mean a statement
that describes a desirable behaviour for the program. Let
ùí± = (ùí≥ , ùëì, ùí´, ùí¨) be a verification problem. The goal is
to verify the following property:

‚àÄùë• ‚àà ùí≥ , ùí´(ùë•) ‚áí ùí¨(ùëì (ùë•), ùë•)

To write ùí±, one needs to be able to express concisely and
without ambiguity each component: the program to verify
ùëì , the properties ùí´ and ùí¨, and the dataset ùí≥ . To this end,
CAISAR provides full support for the WhyML specifi-
cation and programming language [4]. WhyML is a lan-
guage with static strong typing, pattern matching, types in-
variants and inductive predicates. This gives WhyML pro-
grams a sound semantic as logical propositions. WhyML
is at the core of the Why3 verification platform, and has
been used as an intermediate language for verification of

procedure, like relying on tight convex overapproxima-
tions [10] or sound overapproximations [11]. It can
answer reachability and conjunction of linear constraint
queries. Marabou ranked fifth at the VNN-COMP 2021.
It is currently in active development.

SAVer

The Support Vector Machine reachability analysis tool
SAVer [12] is specialized in the verification of support
vector machines (SVM), a popular machine learning algo-
rithm used alongside neural networks for classification or
regression tasks. SAVer can answer reachability queries,
and supports a variety of SVM configurations. This tool
was selected for support as, to the best of our knowledge,
it is the first one to deal with verification of SVM.

Alt-Ergo and SMTLIB compliant solvers

Existing general purpose SMT solvers for program verifi-
cation like Alt-Ergo [13] or Z3 [14] all support a standard
input language, SMTLIB [15]. CAISAR leverages Why3
existing support for SMT solvers and can translate neural
network control flows directly into SMTLIB compliant
strings using its intermediate representation, which allows
the support of a variety of off-the-shelf solvers. Note that

type i n p u t _ t y p e = i n t ‚àí> t
type o u t p u t _ t y p e = i n t
type model = {

app :
i n p u t _ t y p e ‚àí> o u t p u t _ t y p e ;
num_input :
i n t ;
num_classes :

i n t

}

p r e d i c a t e d i s t _ l i n f

i n p u t _ t y p e )
i n p u t _ t y p e )

( a :
( b :
( eps : t )
( n :
f o r a l l

i n t ) =

i . 0 <= i < n ‚àí>

. ‚àí eps . < a i

. ‚àí b i

. < eps

i n p u t _ t y p e )

p r e d i c a t e r o b u s t _ t o
( model : model )
( a :
( eps :
f o r a l l b . d i s t _ l i n f a b eps
model . num_input ‚àí>
model . app a = model . app b

t ) =

Figure 2: An example of a predicate in CAISAR‚Äôs stan-
dard library: being ‚Äúrobust to‚Äù against a perturbation of
amplitude ùúÄ. Here, the predicate defines ùí¨(ùí¥).

use TestSVM . SVMasArray
use i e e e _ f l o a t . F l o a t 6 4
use c a i s a r .SVM

g o a l G:
r o b u s t _ t o svm_apply a ( 0 . 5 : t )

f o r a l l a :

i n p u t _ t y p e .

Figure 3: Example verification problem specified to
CAISAR. The program to verify is TestSVM, the input
space is defined by the elements in a, the output space is
the result of the application of the function svm_apply.

the VNNLIB standard, used in the VNN-COMP, uses a
subset of SMTLIB2, which paves the way for the support
of future tools in CAISAR.

Python Reachability Assessment Tool

The Python Reachability Assessment Tool (PyRAT) is a
static analyzer targeting specifically neural networks. It
builds upon the framework of abstract interpretation [16]
using abstract domains adapted for the approximation
of the reachable space in a neural network. Three main
domains are used: intervals with symbolic relations as
described in [17, 18], zonotopes [19] and Deep poly do-
main [5].

For low dimensional inputs, PyRAT use input parti-
tioning as described in [18], with heuristics tailored to
relational domains: the zonotope domain and the deep-
poly domain with backsubstitution. Those heuristics allow
the computation of a non-trivial (e.g., not just widest in-
terval first) score ranking the inputs by their estimated
influence on the outputs. PyRAT has comparable results
to state-of-the-art analyzers on the widely used ACAS-
Xu [20] benchmark, and outperforms the similar domains
of ERAN on S-shape activations functions such as the
sigmoid or hyperbolic tangent functions with specific ap-
proximations.

AIMOS: a Metamorphic testing utility

AI Metamorphism Observing Software (AIMOS) is a
software developped at the same time as CAISAR, aiming
to provide metamorphic properties testing or perturbations
on a dataset for a given AI model. Metamorphic testing
is a testing technique relying on properties symmetries
and invariance on the operating domain. See [21] for a
comprehensive survey on this approach. AIMOS offers
tools to derive properties from a set of transformations on
the inputs: given ùí´, ùí¨, ùí≥ and a transformation function
‚Ä≤
ùë°ùúÉ : ùë• ‚àà ùí≥ ‚Ü¶‚Üí ùí≥ , it generates a set of new properies ùí¨
that are coherent with the transformation. As an example,
a symmetry on the inputs of a classification model could

result on a symmetry on the outputs; AIMOS would then
automatically modify the property to check against the
symmetrical labels.

AIMOS can generate test cases scenarios from the
most common input transformations (geometrical rota-
tions, noise addition); others can be added if necessary.
AIMOS was evaluated on a metamorphic property on the
ACAS-Xu benchmark. The aim of the property was to
evaluate the ability of neural networks trained on ACAS
to generalize with symmetric inputs. Given a symmetry
on inputs, AIMOS generates the expected symmetrical
output, and tests models against the base and symmetrical
outputs. See table 1 for results. AIMOS was able to show
that neural networks trained on ACAS have a low, but
noticeable sensitivity to symmetry on one input.

3.4. Answer composition

CAISAR currently offer two ways to compose verifiers.
First, CAISAR can launch several solvers on the same task
and compose their answer: it can then provide a summary
stating which solver succeeded and which one failed. Sec-
ond, CAISAR has the ability to verify pipelines that are
composed of several machine learning programs: for in-
stance, a pipeline composed of several neural networks, or
a neural network which outputs are processed by a SVM.
CAISAR can be used to state an overall verification goal,
and to model that the outputs of a block of the pipeline
are the inputs of another block. More advanced methods
of composition, such as automatic subgoals generation
or refinement by multiple analysis constitute a promising
research venue.

Table 1
Average number of same answer for all 45 models of the
ACAS-Xu benchmark, computed by AIMOS. First column
denotes values presented in the benchmark. Second
column denotes the percentage of identical answer given.

Previous answer
COC
WL
WR
L
R

Identical answer
89.7%
95.9%
99.6%
95.3%
99.8%

3.3. Supported formats

CAISAR supports all input formats used by its inte-
grated verifiers. Most verifiers require either a framework-
specific binary (Pytorch‚Äôs pth, Tensorflow tf), a custom
description language (NNet), or an Open Neural Net-
work eXchange (ONNX) 3 file. CAISAR is able to parse
any of these input formats and extract useful metadata
for the building of the verification strategy. It can also
output a verification problem into the SMTLIB [15] for-
mat, supported by all general purpose solvers, as well
as in the ONNX format. The VNN-Lib initiative 4 pro-
vides a standard format for verification problems that
relies on SMTLIB; thus CAISAR also supports VNN-Lib.
CAISAR aims for maximum interoperability, and can be
used as a hub to write and convert verification queries
adapted to different verifiers. Additionally, verifiers some-
times require datasets to verify properties against, espe-
cially reachability analysis tools. As such, CAISAR cur-
rently supports datasets as flattened features under a csv
file, and RGB images.

3https://onnx.ai/
4http://www.vnnlib.org/

4. Background & related works

In less than five years, a profusion of tools and techniques
leveraging formal verification to provide trust on neural
network sprouted [22, 9, 18, 23, 19, 5, 24, 25, 26, 27,
28, 8, 29]. See [30, 31] for more comprehensive surveys
on the verification and validation of machine learning
programs.

As for general purpose verification platforms, exam-
ples include the Why3 deductive verification platform and
the Frama-C [3] C static analysis platform. We leverage
multiple existing features of Why3, such as the WhyML
language support, transformation and rewriting engine.
Why3 and Frama-C both lack the interfaces and tooling to
handle neural network. Experiments we conducted involv-
ing the EVA Frama-C plugin applied on simple reachabil-
ity analysis properties showed a lack of scalability that a
naive python reachability analysis tool, specialized in neu-
ral networks, was able to overcome quickly. Conversion of
neural networks in C programs that were scalable for EVA
presented difficult challenges. The differing structure be-
tween C programs and neural networks implies differing
verification problems. Thus, it seems more fruitful to
investigate a specialized platform for machine learning
programs.

The ProDeep platform [32] aims to regroup several ver-
ifiers under a single user interface. It provides a single
entry point, supports input formats and offers numerous
visualization tools. It does not aim to provide other proper-
ties than those that are natively supported by its embedded
verifiers. It also supports a fixed set of datasets. They
make use of DeepG [33] to generate constraints for ver-
ifiers, effectively combining tools. Their scope seems
limited to neural networks, whereas CAISAR currently
supports neural networks and support vector machines,
and aims to support a wider set of machine learning mod-
els.

The most similar work to CAISAR is the DNNV plat-
form [34]. As CAISAR, DNNV provides support to var-
ious state-of-the-art verifiers. It similarly aims to be a
hub for neural network verification by supporting a wide
range of input and output formats, and by providing a mod-
elling language for properties specification and discharge
to capable provers. Their Domain Specific Language,
DNNP, is built on Python; while CAISAR‚Äôs specifica-
tion language, WhyML, is already used in several formal
verification platforms and provide additional theoretical
guarantees, which is a key component to provide trust. As
stated before, WhyML allows specifying multiple verifi-
cation goals in the same verification problem, which helps
modelling more complex use cases.

The main difference between CAISAR and DNNV is
that the latter does not combine verifiers answers, that
is to say there is (at the time of writing) no feature that
aims to interoperate verifiers: from the DNNV documen-
tation5: "DNNV standardizes the network and property
input formats to enable multiple verification tools to run
on a single network and property. This facilitates both
verifier comparison, and artifact re-use." As verifiers are
becoming more and more sophisticated and specialized,
combination of methods will become even more fruitful,
and we expect this to be a key difference with DNNV.

5. Conclusion & future works

As the field of machine learning verification is blooming,
choosing the right tool for the right verification problem
becomes more and more tedious. We presented CAISAR,
a platform aimed to alleviate this difficulty by presenting
a single, extensible entry point to machine learning veri-
fication problem modelling and solving. Plenty of work
still needs to be done, however.

Altough CAISAR already integrates some state-of-the-
art tools, other verifiers that ranked high in the VNN-
COMP are on the way of integration. Such verifiers in-
clude ùõº, ùõΩ-CROWN [23, 35], who scored first on said
competition.

Another research venue would be the integration of
neural network reparation techniques such as [36]. Cor-
rective techniques would contribute to provide a feedback
loop composed of problem specification, verification, fault
identification and correction proposal.

Various problem splitting heuristics based, for instance,
on [37, 38] could be integrated into CAISAR to leverage
parallelism for verifiers that do not support them

Data is the cornerstone of modern machine learning
systems, and it is necessary to give tools to handle its com-
plexity. Support for more various data kinds, such as time
series, is a first step towards this direction. Integration of
tools for analyzing data in relation with a program, for

5https://docs.dnnv.org/en/stable/

instance out-of-distribution detection, is another future
work.

Finally, to further help the user to select the optimal set
of tools for its verification problem, a long-term goal of
CAISAR is to provide a verification helper to design opti-
mal queries for verification problems based on previous
runs.

References

[1] G. Huet, G. Kahn, C. Paulin-Mohring, The Coq
Proof Assistant : A Tutorial : Version 7.2, Research
Report RT-0256, INRIA, 2002. URL: https://hal.
inria.fr/inria-00069918, projet COQ.

[2] X. Leroy, S. Blazy, D. K√§stner, B. Schommer,
M. Pister, C. Ferdinand, Compcert-a formally veri-
fied optimizing compiler, in: ERTS 2016: Embed-
ded Real Time Software and Systems, 8th European
Congress, 2016.

[3] P. Baudin, F. Bobot, D. B√ºhler, L. Correnson,
F. Kirchner, N. Kosmatov, A. Maroneze, V. Perrelle,
V. Prevosto, J. Signoles, N. Williams, The dogged
pursuit of bug-free C programs: The Frama-C soft-
ware analysis platform, Communications of the
ACM 64 (2021) 56‚Äì68. doi:10.1145/3470569.
[4] J.-C. Filli√¢tre, A. Paskevich, Why3 - Where Pro-
grams Meet Provers, in: M. Felleisen, P. Gardner
(Eds.), Programming Languages and Systems, Lec-
ture Notes in Computer Science, Springer, Berlin,
Heidelberg, 2013, pp. 125‚Äì128. doi:10.1007/
978-3-642-37036-6_8.

[5] G. Singh, T. Gehr, M. P√ºschel, M. Vechev, An
abstract domain for certifying neural networks, Pro-
ceedings of the ACM on Programming Languages
3 (2019) 1‚Äì30.

[6] J. Guitton, J. Kanig, Y. Moy, why hi-lite ada, Rustan,

et al.[32] (2011) 27‚Äì39.

[7] M. Abadi, A. Chu, I. Goodfellow, B. McMahan,
I. Mironov, K. Talwar, L. Zhang, Deep Learning
with Differential Privacy, in: 23rd ACM Conference
on Computer and Communications Security (ACM
CCS), 2016, pp. 308‚Äì318.

[8] C. Urban, M. Christakis, V. W√ºstholz, F. Zhang,
Perfectly Parallel Fairness Certification of Neu-
ral Networks,
arXiv:1912.02499 [cs] (2019).
arXiv:1912.02499.

[9] G. Katz, D. A. Huang, D. Ibeling, K. Julian,
C. Lazarus, R. Lim, P. Shah, S. Thakoor, H. Wu,
A. Zelji¬¥c, D. L. Dill, M. J. Kochenderfer, C. Barrett,
The Marabou Framework for Verification and Analy-
sis of Deep Neural Networks, in: I. Dillig, S. Tasiran
(Eds.), Computer Aided Verification, volume 11561,
Springer International Publishing, Cham, 2019, pp.
443‚Äì452.

[10] H. Wu, A. Zelji¬¥c, G. Katz, C. Barrett, Efficient Neu-
ral Network Analysis with Sum-of-Infeasibilities,
in: D. Fisman, G. Rosu (Eds.), Tools and Algo-
rithms for the Construction and Analysis of Sys-
tems, Lecture Notes in Computer Science, Springer
International Publishing, Cham, 2022, pp. 143‚Äì163.
doi:10.1007/978-3-030-99524-9_8.

[11] M. Ostrovsky, C. W. Barrett, G. Katz,

An
abstraction-refinement approach to verifying convo-
lutional neural networks, CoRR abs/2201.01978
(2022). URL: https://arxiv.org/abs/2201.01978.
arXiv:2201.01978.

[12] F. Ranzato, M. Zanella, Robustness verification of
in: International Static
support vector machines,
Analysis Symposium, Springer, 2019, pp. 271‚Äì295.
[13] S. Conchon, A. Coquereau, M. Iguernlala, A. Meb-
sout, Alt-Ergo 2.2,
in: SMT Workshop: Inter-
national Workshop on Satisfiability Modulo Theo-
ries, Oxford, United Kingdom, 2018. URL: https:
//hal.inria.fr/hal-01960203.

[14] L. de Moura, N. Bj√∏rner, Z3: An Efficient SMT
Solver,
in: C. R. Ramakrishnan, J. Rehof (Eds.),
Tools and Algorithms for the Construction and Anal-
ysis of Systems, Lecture Notes in Computer Science,
Springer, Berlin, Heidelberg, 2008, pp. 337‚Äì340.
doi:10.1007/978-3-540-78800-3_24.
[15] C. Barrett, P. Fontaine, C. Tinelli, The Sat-
isfiability Modulo Theories Library (SMT-LIB),
www.SMT-LIB.org, 2016.

[16] P. Cousot, R. Cousot, Abstract interpretation: A
unified lattice model for static analysis of programs
by construction or approximation of fixpoints, in:
POPL, 1977, pp. 238‚Äì252.

[17] J. Li, J. Liu, P. Yang, L. Chen, X. Huang, L. Zhang,
Analyzing deep neural networks with symbolic prop-
agation: Towards higher precision and faster ver-
in: B. E. Chang (Ed.), Static Analy-
ification,
sis - 26th International Symposium, SAS 2019,
Porto, Portugal, October 8-11, 2019, Proceedings,
volume 11822 of Lecture Notes in Computer Sci-
ence, Springer, 2019, pp. 296‚Äì319. URL: https:
//doi.org/10.1007/978-3-030-32304-2_15. doi:10.
1007/978-3-030-32304-2\_15.

[18] S. Wang, K. Pei, J. Whitehouse, J. Yang, S. Jana,
Efficient Formal Safety Analysis of Neural Net-
works, in: S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, R. Garnett (Eds.),
Information Processing
Advances
in Neural
Systems 31, Curran Associates,
Inc., 2018,
pp. 6367‚Äì6377. URL: http://papers.nips.cc/paper/
7873-efficient-formal-safety-analysis-of-neural-networks.
pdf.

[19] G. Singh, T. Gehr, M. Mirman, M. P√ºschel,
M. Vechev, Fast and Effective Robustness Certifi-
cation, in: S. Bengio, H. Wallach, H. Larochelle,

in Neural

K. Grauman, N. Cesa-Bianchi, R. Garnett (Eds.),
Advances
Information Processing
Systems 31, Curran Associates, Inc., 2018, pp.
10802‚Äì10813. URL: http://papers.nips.cc/paper/
8278-fast-and-effective-robustness-certification.
pdf.

[20] G. Manfredi, Y. Jestin, An introduction to ACAS
Xu and the challenges ahead, in: IEEE/AIAA Dig-
ital Avionics Systems Conference (DASC), Sacra-
mento, CA, USA, 2016. doi:10.1109/DASC.
2016.7778055.

[21] T. Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey,
T. Tse, Z. Q. Zhou, Metamorphic testing: A review
of challenges and opportunities, ACM Computing
Surveys (CSUR) 51 (2018) 1‚Äì27.

[22] G. Katz, C. Barrett, D. Dill, K. Julian, M. Kochen-
derfer, Reluplex: An efficient smt solver for
verifying deep neural networks,
arXiv preprint
arXiv:1702.01135 (2017).

[23] S. Wang, H. Zhang, K. Xu, X. Lin, S. Jana, C.-J.
Hsieh, J. Z. Kolter, Beta-CROWN: Efficient Bound
Propagation with Per-neuron Split Constraints for
Complete and Incomplete Neural Network Robust-
ness Verification, ???? URL: http://arxiv.org/abs/
2103.06624. arXiv:2103.06624.

[24] Z. Shi, H. Zhang, K.-W. Chang, M. Huang, C.-J.
Hsieh, Robustness Verification for Transformers, in:
International Conference on Learning Representa-
tions, 2020. URL: https://openreview.net/forum?id=
BJxwPJHFwS.

[25] S. Bak, Nnenum: Verification of ReLU Neural
Networks with Optimized Abstraction Refinement,
in: A. Dutle, M. M. Moscato, L. Titolo, C. A.
Mu√±oz, I. Perez (Eds.), NASA Formal Methods,
Lecture Notes in Computer Science, Springer In-
ternational Publishing, Cham, 2021, pp. 19‚Äì36.
doi:10.1007/978-3-030-76384-8_2.
[26] P. Henriksen, A. Lomuscio, Efficient Neural Net-
work Verification via Adaptive Refinement and Ad-
versarial Search,
in: 24th European Conference
on Artificial Intelligence - ECAI 2020, Santiago de
Compostela, Spain, 2020, p. 8.

[27] S. Dutta, S. Jha, S. Sanakaranarayanan, A. Ti-
wari, Output Range Analysis for Deep Neural
Networks,
arXiv:1709.09130 [cs, stat] (2017).
arXiv:1709.09130.

[28] A. D. Palma, R. Bunel, A. Desmaison, K. Dvi-
jotham, P. Kohli, P. H. S. Torr, M. P. Kumar,
Improved branch and bound for neural network
verification via lagrangian decomposition (????).
arXiv:2104.06718v1.

[29] R. Ehlers, Formal Verification of Piece-Wise Linear
Feed-Forward Neural Networks, arXiv:1705.01320
[cs] (2017). arXiv:1705.01320.

[30] C. Urban, A. Min√©, A Review of Formal Methods

applied to Machine Learning, arXiv:2104.02466
[cs] (2021). arXiv:2104.02466.

[31] C. Liu, T. Arnon, C. Lazarus, C. Barrett, M. J.
Kochenderfer, Algorithms for Verifying Deep Neu-
ral Networks, arXiv:1903.06758 [cs, stat] (2019).
arXiv:1903.06758.

[32] R. Li, J. Li, C.-C. Huang, P. Yang, X. Huang,
L. Zhang, B. Xue, H. Hermanns, PRODeep: a plat-
form for robustness verification of deep neural net-
works, in: Proceedings of the 28th ACM Joint Meet-
ing on European Software Engineering Conference
and Symposium on the Foundations of Software En-
gineering, ESEC/FSE 2020, Association for Com-
puting Machinery, New York, NY, USA, 2020, pp.
1630‚Äì1634. URL: https://doi.org/10.1145/3368089.
3417918. doi:10.1145/3368089.3417918.

[33] M. Balunovic, M. Baader, G. Singh, T. Gehr,
Certifying Geometric Robust-
in: Advances
Information Processing Systems,
Inc., 2019.
https://papers.nips.cc/paper/2019/hash/

M. Vechev,
ness of Neural Networks,
in Neural
volume 32, Curran Associates,
URL:
f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.
html.

[34] D. Shriver, S. Elbaum, M. B. Dwyer, DNNV:
A Framework for Deep Neural Network Verifi-
cation,
in: A. Silva, K. R. M. Leino (Eds.),
Computer Aided Verification, Lecture Notes in
Computer Science, Springer International Publish-
ing, Cham, 2021, pp. 137‚Äì150. doi:10.1007/
978-3-030-81685-8_6.

[35] K. Xu, H. Zhang, S. Wang, Y. Wang, S. Jana,
Fast and Complete: En-
X. Lin, C.-J. Hsieh,
abling Complete Neural Network Verification with
Rapid and Massively Parallel Incomplete Verifiers,
arXiv:2011.13824 [cs] (2021). URL: http://arxiv.
org/abs/2011.13824, arXiv: 2011.13824.

[36] B. Goldberger, G. Katz, Y. Adi, J. Keshet, Minimal
modifications of deep neural networks using veri-
fication, in: E. Albert, L. Kovacs (Eds.), LPAR23.
LPAR-23: 23rd International Conference on Logic
for Programming, Artificial Intelligence and Reason-
ing, volume 73 of EPiC Series in Computing, Easy-
Chair, 2020, pp. 260‚Äì278. doi:10.29007/699q.
[37] J. Girard-Satabin, A. Varasse, M. Schoenauer,
G. Charpiat, Z. Chihani, DISCO: Division of in-
put space into convex polytopes for neural network
verification, JFLA (2021).

[38] R. Bunel, J. Lu, I. Turkaslan, P. H. Torr, P. Kohli,
M. P. Kumar, Branch and bound for piecewise linear
neural network verification, Journal of Machine
Learning Research 21 (2020) 1‚Äì39.

