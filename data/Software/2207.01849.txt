Learnings from an Under the Hood Analysis 
of an Object Storage Node IO Stack 

Pratik Mishra, Rekha Pitchumani, and Yang-Suk Kee, Samsung Semiconductor Inc., USA.   

1 

Abstract—Conventional  object-stores  are  built  on  top  of  traditional  OS  storage  stack,  where  I/O  requests  typically  transfers 
through  multiple  hefty  and  redundant  layers.  The  complexity  of  object  management  has  grown  dramatically  with  the  ever 
increasing requirements of performance, consistency and fault-tolerance from storage subsystems. Simply stated, more number 
of intermediate layers are encountered in the I/O data path, with each passing layer adding its own syntax and semantics. Thereby 
increasing  the  overheads  of  request  processing.  In  this  paper,  through  comprehensive  under-the-hood  analysis  of  an  object-
storage node, we characterize the impact of object-store (and user-application) workloads on the OS I/O stack and its subsequent 
rippling  effect  on  the  underlying  object-storage  devices  (OSD). We  observe  that  the  legacy  architecture  of  the  OS  based  I/O 
storage stack coupled with complex data management policies leads to a performance mismatch between what an end-storage 
device  is  capable  of  delivering  and  what  it  actually  delivers  in  a  production  environment.  Therefore,  the  gains  derived  from 
developing faster storage devices is often nullified. These issues get more pronounced in highly concurrent and multiplexed cloud 
environments. Owing to the associated issues of object-management and the vulnerabilities of the OS I/O software stacks, we 
discuss the potential of a new class of storage devices, known as Object-Drives. Samsung Key-Value SSD (KV-SSD) [1] and 
Seagate Kinetic Drive [2]  are classic industrial implementations of object-drives, where host data management functionalities can 
be offloaded to the storage device. This leads towards the simplification of the over-all storage stack. Based on our analysis, we 
believe object-drives can alleviate object-stores from highly taxing overheads of data management with 20-38% time-savings over 
traditional Operating Systems (OS) stack. 

Index Terms— Object-storage, Distributed systems, File Systems, Operating System, Linux, Storage devices, Key-Value SSD 
(KV-SSD), Solid State Drives (SSDs), Storage I/O Stack.  

.——————————      —————————— 

1 

 INTRODUCTION 

O 

bject-storage,  Distributed  systems,  File  Systems,  Op-
erating  System,  Linux,  Storage  devices,  Key-Value 
SSD  (KV-SSD),  Solid  State  Drives  (SSDs),  Storage  I/O 
Stack [3]–[6] . Nearly all major cloud providers offer object-
storage services for cloud remote storage with some of the 
most popular choices being AWS  S3, Google Cloud Stor-
age, Azure Blobstore, Ceph, Lustre, MinIO, Swift, Alibaba 
Cloud OSS, IBM Cloud, etc.c[3], [4], [7]–[9]. 

A highly simplistic representation of the software 
stacks in a conventional object-store is shown  (a). Objects 
and their associated metadata is accessible to applications 

———————————————— 
  Pratik Mishra, Rekha Pitchumani, and Yang-Suk Kee. Samsung Semicon-

ductor, Inc. San Jose, CA.  
E-mail: {mishra.p, r.pitchumani, yangseok.ki@samsung.com}. 

Terms of Use: This work is licensed under a Creative Commons Attribution 4.0 

License.  It  is  attributed  to Samsung  Semiconductor,  Inc.    This  document  and all 

information discussed herein remain the sole and exclusive property of Samsung. All 

brand names, trademarks and registered trademarks belong to their respective own-

ers. Unless specifically identified as such, Samsung's use of third-party trademarks 

does not indicate any relationship, sponsorship, or endorsement between Samsung 

and the owners of these trademarks. Any references by Samsung to third-party trade-

marks is to identify the corresponding third party goods and/or services and shall be 

considered nominative fair use under the trademark law. Please find the legal infor-

mation and terms of use in the paper.  

IEEE Transaction on Computers format has been used to write this paper. 

Figure 1 Object-storage I/O stack based on (a) Conventional 
(OS) block devices; (b) Object-Drive. More the intermediate 
layers in the I/O path, higher is the delay. 

via Object-Storage (REST) API. Object-stores receive appli-
cation requests over network, which are processed by Ob-
ject-Store Management services (OSMs). OSMs provide an 
interface  between  applications  and  storage,  thereby  re-
sponsible for data management. However the actual IO is 
processed by backend storage servers which have locally 
attached storage devices known as Object-Storage Devices 

 
 
 
 
   
2 

(OSDs)  for  persisting  objects.  Storage  nodes  are  built  on 
top of legacy operating system (OS) stacks due to the ma-
turity  and  convenience  in  production  environments.  The 
OS  acts  as  a  middleware  between  distributed  OSM  layer 
and the OSDs. Therefore, the performance of the object-store 
at the distributed level is significantly impacted by the local node 
operating system. 

Consider  Figure  1(a),  we  also  observe  that  along 
the  odyssey  of  data  access  i.e.  from  user-applications  to 
OSDs, IO requests transfers through multiple hefty and re-
dundant layers. Each layer has its own syntax and seman-
tics  which  consumes  host  resources  such  as  compute, 
memory, caches, etc. for both data and metadata. The com-
pounding effect of all the intermediate layers and their de-
pendencies shapes the final IO workloads observed at the 
OSD  interface.  For  e.g.:  multiple  address  translations  are 
required  to  serve  an  IO  request,  i.e.  Object-IO  →  File-IO 
→Block-IO→Physical Page Address (PPA). Simply stated, 
more  the  intermediate  layers  encountered  in  the  IO  data  path, 
higher are the delays in request processing. 

In this paper, through comprehensive theoretical 
and empirical analysis we characterize the impact of com-
plex  data-management  policies  and  user-application 
workloads on an in-node traditional OS backed object-stor-
age server (see Section 2). Particularly in the OS- the Vir-
tual File System (VFS), File-System, and the Block IO layer 
are the most critical components which enable data access 
and persistence. It is the orchestration and design of these 
layers which influence the over-all performance of the stor-
age  system,  i.e.  how  efficiently  objects  are  managed. 
Though object stores were initially designed for Hard Disk 
Drives (HDDs) but with changing customer requirements 
most  cloud  providers  also  offer  Solid  State  Drive  (SSD) 
backends as well [9], [10] . Our goal is to demonstrate and 
quantify the impact of the  OS overheads in object-stores, 
more specifically what, why, and, how leads to the underuti-
lization of the OSDs deployed, i.e. HDDs and SSDs both. 
We observe that the gains derived from storage devices is 
typically  nullified  in  highly  concurrent,  and  multiplexed 
cloud object-store environments due to the design princi-
ples of traditional OS stacks. Therefore,  object-store suffers 
from sub-optimal performance and high host resource footprint 
for its entire life-time. 

In Section 3, we identify the major challenges and 
bottlenecks  in  object-storage  servers,  such  as  (1)  multi-
layer  translations  and  index  management;  (2)  resource 
contentions.  Through  our  findings,  we  observe  that  the 
afore-mentioned  issues  leads  to  fragmentation  or  aging1  of 
the  physical  file-system.  This  leads  towards  sub-optimal 
data placement, i.e. non-contiguous address space or Log-
ical Block Addresses (LBA), for current and future data ac-
cesses and persistence. For the host (and user-applications) 
this effect translates into higher resource consumption and 
delays  due  to  more  translations,  cache  and  disk  activity, 
and book-keeping overheads.  

Considering  the  complexity  of  building  storage 

systems from scratch, a natural progression is the develop-
ment of holistic storage device technologies which can ex-
ecute  host  data  management  functionalities.  We  discuss 
the potential of such a class of storage devices, defined as 
Object-Drives  as  solution  to  alleviate  object-stores  from 
the associated issues of the redundant and bulky IO stack.  
Samsung  KV-SSD  [1]  and  Seagate  Kinetic  Drive  [2]  are 
classic  industrial  use-cases  implementations  of  Object-
Drives. Enabling object-drives in data-centric environments 
can simplify and streamline the overall data management 
software  stack  while  balancing  storage  capabilities  and 
their limitations. We briefly evaluate the concept of object-
drives  and  its  applicability  as  an  OSD  to  reduce  the  ad-
verse impact for object management. Based on our analysis 
in Section 5, we believe that leaner stack solutions such as 
Object-Drives  can  lead  towards  20-38%  end-to-end  time-
savings  over  traditional  OS  backed  object-stores.  Object-
Drives could also  lead to reduction in host-resource  con-
sumption  along-with  increasing  the  in-node  scalability 
while achieving higher device bandwidth  [1], [11], [12]. 

2  BACKGROUND AND MOTIVATION 

The  backend  storage  servers  of  popular  object-stores 
such  as  Lustre  [3],  Openstack  Swift  [13],  Ceph  [4],  and 
MinIO[8]  are  built  on  top  of  traditional  OS.  Fundamen-
tally, the object-storage software stack can be broken into 
(1) User-space, (2) OS Kernel, and, (3) Storage as shown 
in Figure 1(a). 

(1) User-space: User-applications and OSMs form the user-
space  as  they  run  as  host  processes  on  the  storage  node. 
OSMs manage distributed services such as application in-
terfaces,  load  balancing,  consistency,  and  fault-tolerance 
[3],  [4],  [6].  OSM  translates  user  requests  (Object-IO)  re-
ceived over network into locations/files in storage servers. 
It  maintains  data and  metadata  indexes  for  objects  using 
key-value  stores  while  orchestrating  object-store  opera-
tions.  Similar  to  other  distributed  systems,  they  can  also 
employ  inefficient  complex  journaling  on  top  of  con-
sistency provided by file-systems [4], [6]. 

(2) OS Kernel: In object-stores the OS acts as a middleware 
between userspace and storage. The OS is responsible for 
the  persistence  and  data  (and  metadata)  management  in 
storage devices. It provides mechanisms for most transla-
tions,  consistency,  allocation  and  scheduling  of  SW  and 
HW resources for physical data management. Briefly, the 
OS  receives  File-IO  requests  from  OSM  daemons  where 
the VFS provides uniform interface to file-systems. The file 
system re-indexes the requests creating its own metadata 
such as inodes and journals. Further the request is trans-
lated into multiple Block-IOs containing the LBAs, which 
is scheduled by the block IO layer to the storage device via 
device drivers. 

1 Fragmentation or aging is an adverse phenomenon in which   logically 
related blocks are allocated non-contiguous address space in a storage de-
vice. This can be inter-file, intra-file, or free-space fragmentation. It implies 

loss  of  sequentiality.  The  sub-optimal  placement  results  into  slower  read 
operation due to disk seeks as well as further fragment data placement. 

 
 
 
 
 
 
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

3 

(3) Storage: Storage devices (here, OSD) are responsible for 
actual physical storage and provide retrieval mechanisms 
of the stored data. The storage device receives Block-IO re-
quests  (LBAs)  from  the  OS  device  driver  and  internally 
translates  it  into  LBA→Physical  Page  Addresses  (PPAs) 
managed via in-device index management. In modern data 
centers  there  are  a  plethora  of  storage  devices  from  me-
chanical HDDs to flash based SSDs to emerging byte-ad-
dressable NVMs. Each have their own characteristics and 
role in the storage topology. However, a common observa-
tion in production systems has been that their performance 
is largely influenced by the above SW layers which is ag-
nostic to the storage device type [6], [14], [15]. We limit our 
scope to HDDs and SSDs which form the bulk of OSDs in 
production. 

2.1 

Impact of OS I/O Software Stack 

Owing to the importance of the OS storage stack in the 
overall performance of the object-store, we discuss the im-
pact  of  the  IO  layers  followed  by  role  and  layer-by-layer 
breakdown. 

Takeaway 1: For the same size IO request, larger number 
of threads are required to achieve high throughput for file-
system (more IO layers) compared to raw device. Indicat-
ing more the layers in the IO stack, lower is device bandwidth 
and more host resource are consumed for increasing the device 
bandwidth utilization. 
Takeaway 2: The maximum bandwidth achieved by file-
system based IO is capped and significantly lower to raw 
device performance. This suggests, with increasing IO layers 
there are higher request processing overheads such as higher re-
source and lock contentions, translations, etc. which forms a bot-
tleneck. 

Takeaway 3: Larger IO requests (here 128K) are more desirable 
as they can saturate device bandwidth with fewer number 
of  threads  and  requests  as  they  require  lesser  processing 
overheads  such  as  fewer  data  packing  queuing,  etc., 
though the average latency increases [1], [15]. 

Findings  in  [14]  also  suggest  that  higher  device  band-
width  is  achieved  by  large  IOs  (contiguous/sequential 
LBAs,  e.g.128K)  than  small  IOs  (4K),  for  both  HDDs  and 
SSDs. Therefore, larger sequential IOs are highly desirable for 
harnessing the device potential2. 

2.1.1  Device Utilization & IO layer Correlation 

(DUiC): 

2.1.2  Layer-by-Layer (LbL) Analysis: 

To understand the impact of IO layers, we measure the 
throughput of block-based  NVMe SSD.  Using  flexible IO 
tester (fio), we performed sequential write IO benchmark 
varying the request sizes and no. of concurrent threads on 
the  raw-device  (RAW)  and  with  XFS  file-system  (FS)  as 
shown in Figure 2(a). We compare the throughput for small 
(4K) and large (128K) request size with and without file-
system while varying the number of concurrent threads to 
understand  the  requirements  to  saturate  device  band-
width. From Figure 2(a), based on our observation we have 
the following take-aways. 

Figure  2  Impact  of  IO-layers:  (a)  Sequential  write  IO  (fio) 
throughput  on  block  NVMe  device  with  (FS)  and  without 
(RAW)  file-system.;  (b)  Normalized  time-spent  by  an  IO  re-
quest in an IO-layer. 

Based on our above analysis, we understand the impact of 
IO layers on device utilization, we further briefly discuss 
first theoretically and then empirically the role of the major 
performance critical components of the OS Kernel IO lay-
ers. 

(1) Virtual File System: The VFS provides the software ab-
straction  and  uniform  interface  to  applications  through 
system calls for accessing physical file-systems (fs) across 
storage devices. Major functionalities include concurrency 
control, permission checks, buffer & metadata caching [16], 
[17].  For  performance,  VFS  manages  3  types  of  metadata 
caches, i.e.,  

  The  file-system  (fs)  superblock  is  stored  in 

memory during mount. 

  The dentry-cache (dcache) stores the pathname (or 
filename)  and  inode  information,  which  aids  in 
fast  look-up  to  translate  pathnames  to  dentries 
and validating the runtime dynamic state of files 
(or objects) during concurrent access. 

  The  inode-cache  (icache)  stores  the  file  attributes 

[16]. 

In an object-store node, due to the large number of look-
ups  associated  to  object  operations  typically  results  into 
large  no.  of  small,  random  and  sporadic  metadata  IOs. 
Therefore, effective VFS caching plays a vital role in reducing 
disk  I/O  traffic  for  fs  metadata  operations  (IO  amplification  as 
well). 

2 Larger IOs increases bandwidth utilization for HDDs & SSDs [14].  
HDDs:  Less  disk-head  seeks,  lower  command  processing  over-

heads→lower delay.  
SSDs: May be attributed to utilization of internal flash-channel parallelism, 

fewer FTL translations, lower traffic, etc [14], [35]. Though the large (se-
quential) & small (random) IO performance difference for SSDs is not as 
huge compared to HDDs. 

 
 
 
 
 
 
 
 
 
 
 
4 

concurrency 

(2) File System (FS): Filesystems manages the organization 
of data, metadata, and extended attributes (inodes, xattr). 
It provides POSIX-compatible common interfaces to appli-
cations for data access on storage devices. Briefly, file-sys-
tem's  major  responsibilities  include  file/block  allocation, 
metadata  organization, 
control,  File-
IOs→Block-IOs  translations  for  scheduling  by  block  IO 
layer.  These  are  achieved  by  employing  efficient  data-
structures such as B-trees, log-structured, etc., for on-disk 
representation of data and metadata. Modern file-systems 
guarantee crash consistency through journaling, with ad-
vanced file-systems such as  Btrfs [18] also support trans-
actions and checksums. However, on observation the most 
common design principle of popular file-systems such as 
xfs, ext4, zfs, Btrfs [18]–[21] is to keep related data together. 
Thereby  reducing  the  impact  of  fragmentation  or  aging. 
Aging  effects  nearly  all  filesystems  and  storage  device 
types,  resulting  into  loss  of  sequentiality  for  data  place-
ment  and  retrieval  of  related  data.  Most  filesystems  em-
ploy multiple fragmentation mitigating strategies such as 
extents3,  group  allocations4,  delayed  allocation5  (please  refer 
[14], [22] for details). Despite such advanced optimizations 
due to workload characteristics filesystems are highly vul-
nerable  to  aging.  This  results  into  sub-optimal  file  place-
ment,  performance  degradation,  poor  data  and  metadata 
layout which also inhibits the ability to cache or prefetch 
data  efficiently  [6],  [14],  [18],  [22].  Thereby,  consuming 
higher compute, memory & disk (more IOs) resources with in-
creased request processing and book-keeping overheads. 

To understand the contribution of an OS layer to the over-
all IO request latency, we conduct LbL analysis using ker-
nel probing with eBPF.  The  normalized layer-wise IO la-
tency  breakdown  for  the  time-spent  in  each  OS  layer  for 
large 128K IO request with xfs file-system (64 threads) is 
shown in Figure 2(b). We observe that on average, the time 
spent by an IO request in Kernel IO layers is roughly 35-
40% of the total time. While the majority of the time in the 
kernel is spent in the block-layer and the file-system, sug-
gesting  file-block  allocation,  resource  (lock)  contentions, 
queue scheduling, multiplexing (interleavings) and serial-
ization delays of BIO requests[15] to device with increased 
average latency. 

Takeaway  4:  Nearly  40%  of  the  device  capability  is  un-
derutilized due to the OS kernel request processing delays.  

Based on our discussions, the file-system is the most critical 
OS  layer  as  it  governs  device  performance,  host  resource 
consumption  and  majorly  influences  working  of  the  VFS 
and  the  block  layer.  In  Filesystems,  the  block  allocation, 
metadata (inode) management, fragmentation mitigation, 
concurrency  control  mechanisms  influence  the  caching 
workload  of  VFS,  and  the  number,  size  and  spatial  IO 
workload on the block-IO layer. Therefore, we focus our at-
tention towards the impact of file-systems deployed in produc-
tion object-stores. 

2.2  Object-store and Choice of File-systems 

(3) Block IO Layer: The block-IO layer is the final OS opti-
mization layer before dispatching IO requests to the stor-
age device via the device driver. It receives IO requests af-
ter  file-to-block  mapping/allocation  by  the  file-system, 
which  are  then  translated  into  Block-IO  (BIO)  request 
structure which is an in-kernel data-structure representing 
discrete  IO  events,  i.e.  linked-list  of  contiguous  LBAs 
(please refer [23] for details). It employs a block IO-sched-
uler which is responsible for dividing the ``request-queue’’ 
bandwidth  amongst  contending  applications,  where  BIO 
requests are staged, thereby providing opportunities to be 
coalesced,  merged  or  ordered  to  form  larger  IO  requests 
and maintain sequentiality. This shapes the final workload 
observed at the disk interface, depending on the IO sched-
uling  policy.  Vanilla  Linux  comes  with  CFQ,  noop,  and 
Deadline scheduling policies which prove to deliver sub-
optimal  performance  in  production  [15],  [23]–[25].  Solu-
tions such as Blk-mq [24] and BID [15] optimizes the block-
layer for IO scheduling to cater cloud environments.  

Object-store  user-applications  and  OSM  workloads  em-
ploy highly taxing operations (ops) which have tightly-cou-
pled  consistency  and  performance  requirements.The  ma-
jor  functionalities  include  from  basic  IO,  read-modify-
write, transactions, cloning, to critical reliability ops such 
as  enumeration,  fast  point  queries,  recovery,  and  scrub-
bing,  etc.  Object-stores  use  key-value  stores  to  build  in-
dexes.  All  these  operations  are  extensively  data  and 
metadata heavy (object and local filesystem, both) [1], [6]. 
Apart from object persistence/access there would still be a 
large number of small metadata IOs.  

Object-stores such as Openstack Swift [13] use filesystems 
extended attributes (xattr) for storing object-metadata and 
data  as  binary  objects.  To  specifically  manage  object-
metadata,  metastores  are  built  on  over  DBs  such  as 
LevelDB, RocksDB[26], etc., which consume huge host re-
sources  and  limit  scalability  [1],  [6].  Therefore,  in  object-
stores  the  role  of  file-systems  is  highly  prominent  which 

3  Extents  are  physically  contiguous  blocks  allocated  by  file  systems, 
which  try  to  increase  locality,  reduce  metadata  and  book-keeping  over-
heads. Extents allocation help in maintaining sequentiality for larger files, 
as well as for small files bin-packing heuristics such as packing small files 
and metadata together into fewer blocks or extents, etc can lead to preserv-
ing locality. 

4 Allocation groups (AGs) are best-effort approaches to maintain direc-
tory  locality  or  co-locating  files  in  same  directory  till  allocation  group 
space is exhausted. Each allocation group consists of data-structures about 
its inodes and bitmap of blocks, with every new directory placed in an al-
location group with higher number of free inodes, while inodes and data 

in  a  directory  are  placed  in  same  group  till  possible.  Allocation  groups 
(AGs) or bands concept realized by xfs (default: 4) allows superior concur-
rency support with the intention of minimal interference with each other. 

5 Delayed allocation  are data placement strategies in which data blocks 
are  buffered  in-memory  to  batch  writes  and  updates,  and  allocation  to 
blocks occur on flushing till the consistency and durability requirements 
of file system or applications (fsync or flush) are not violated. Thereby in-
creasing chances for contiguous allocation, reduction of amplification, co-
alescing requests together 

 
 
 
 
 
 
 
 
 
 
 
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

5 

necessitates  reliability,  performant  data  access  with  effi-
cient index management. Therefore,  an object-store is as 
good as the local file system.  

Similar to other distributed file systems, most popular ob-
ject-stores such as Ceph, Openstack Swift, MinIO support 
and recommend ext4 and xfs file-systems which are most 
widely  used  due  to    their  deployment  popularity  and 
proven persistence. Some salient features are as follows: 

 

 

ext4 is de-facto file-system in most linux distribu-
tions.  ext4  is  update  in-place  filesystem  which 
manages data in 128 MiB extents (runs of contigu-
ous space) - reduces metadata book-keeping over-
heads. Uses tree-based index structures to repre-
sent files, and, directories for efficient block allo-
cation tracking. ext4 uses a write-ahead journal to 
ensure atomicity [18], [20]. 

xfs is also update in-place filesystem designed to 
support high scalability, concurrency, and paral-
lelism. The inodes and free space information are 
managed  by  respective  B+  trees,  where  inodes 
keep track of their own allocated extents [19] with 
the goal of reducing the amount of metadata.  

  Both,  ext4  and  xfs  employ  anti-aging  strategies 
such as extents and delayed-allocation, while xfs 
also tries to place sub-directories in different allo-
cation groups (or bands) for concurrency and fu-
ture  expansion  of  files.  Analysis  of  both  have 
shown  significantly  lower  IO  amplification  cost 
for data and metadata operations. 

 Owing  to  the  semantic  gap  between  between  Object-IO 
and  Block-IO,  object  storage  systems  employ  expensive 
consistency mechanisms such as maintaining write-ahead-
logs (WAL) on top file-system metadata journals. This in-
creases  IO  amplification  and  synchronization  dependen-
cies  between  IOs  resulting  into  high  host  resource  con-
sumption (and contentions). Further, it has a crippling ef-
fect on the over-all storage subsystem by fragmenting the 
OSD, ineffective caching, and increased page-faults for the 
lifetime of the OSD. Thereby, request processing delays can 
cause  stallness  or  slowdown  of  computation  waiting  for 
the data [1], [6], [11], [15].  

 Advanced file-systems such as Btrfs [18] exposes internal 
transaction  mechanisms  to  applications,  while  providing 
deduplication, checksum, and compression support which 
are not exposed by native filesystems such as ext4 and xfs. 
These functionalities of Btrfs's would highly benefit object-
stores  by  providing  fast  and  efficient  transactions  with 
consistency  guarantees.  However  from  practical  experi-
ence of Ceph team [6], Btrfs suffers from severe data and 
metadata  fragmentation,  costly  checksums  with  lack  of 

rollback transaction mechanisms and poor interfaces often 
plagued by data loss and file corruption. In the next section 
through  empirical  analysis  we  understand  what,  why,  & 
how  the  architectural  decisions  at  local  in-node  OS  influ-
ences the over-all lifetime performance of  objectstores. 

3  UNDER THE HOOD ANALYSIS 

Through cloud application and remote cloud storage liter-
ature [5], [7], [9], [10], [27], [28],  the workloads experienced 
by end object storage nodes is highly skewed with multiple 
concurrent  application  submitting  requests  at  the  same 
time in conjunction to OSM workloads. This leads to con-
tentions at all SW and HW resources imposing great strain 
on the legacy components of the traditional IO stack with 
repercussions on the performance and health of the object-
storage system for its lifetime. In this section, through com-
prehensive  characterization  of  an  in-node  OS  object-stor-
age  stack  we  establish  the  relationship  between  what  and 
how  in  the  OS  kernel  leads  to  the  underutilization  of  the 
OSDs  deployed.  Whereas  the  why  was  discussed  in  the 
previous sections. We broadly classify workloads into two 
categories:  
1) Object-store core-functionality (OSM) workloads;  
2) User-application workloads.  
The former are services required for th object-store to func-
tion,  while  the  later  are  workloads  run  on  top  of  object-
stores by user-applications.  

First  we  describe  the  evaluation  environment  fol-
lowed by empirical Layer-by-Layer (LbL) analysis as dis-
cussed below. 

3.1  Evaluation Environment Set-up 

We evaluate the in-node OS storage backend of our dis-
tributed  MinIO6  object-storage  cluster.  The  cluster  com-
prises of 4x storage servers with Ubuntu 16.04 LTS (xenial) 
OS using kernel 4.4.0-142-generic. Each storage node con-
sists of 2x Xeon E5 2.30GHz with 48 CPUs, 128GB DRAM, 
4x 15K SAS HDDs and 4x block-based SSDs. The erasure 
code  is  set  to  Reed-Solomon  (12,4)  based  on  cloud  litera-
ture  [29].  To  emulate  user-applications  3  clients  compute 
nodes are connected to the cluster via 1Gbps network. For 
analyzing  system  level  performance,  we  collect  system 
event metrics and traces across the different layers of the 
IO stack for the node and OSDs using kernel spoofing tools 
such as BCC eBPF iovisor [30], ftrace, blktrace, strace, sar, 
and iostat, etc. while running representative workloads for 
both ext4 and xfs file systems. It is important to note, that 
our  goal  is  not  to  compare  different  file-systems  but  to 
evaluate  compounding  impact  of  different  architectural 
design decisions (pros and cons) which leads to underutili-
zation of local OSDs. This also serves as a guiding stepping 
stone  for  critical  design  considerations  for  Object-Drive 
based object-stores. 

6 MinIO  [8]  is  a popular  open-source  high-performance  s3-compatible 
object-store, written in GO and assembly language with features such as 
inline erasure code, bitrot protection, compression, encryption, encryption 

with strict consistency. It only supports xfs and ext4 file system mounted 
OSDs. 

 
 
 
 
 
 
 
  
 
 
 
6 

3.2  Evaluation Metrics 

From the discussions in Section 2, we believe the follow-
ing  metrics  can  be  used  to  quantify  and  characterize  the 
impact of the OS layer in shaping IO workload on OSDs.  

  Overall performance by the workload's total ex-

ecution time. 

  VFS  by  metadata  caching  through  dcache 

 

hits/misses. 
Filesystem by no. of page-cache evictions & no. 
of disk IOs for index management;  and block-
IO  size  distribution,  size  of  sequential  LBA 
chains in submission order and LBA disk-lay-
out for block-allocation strategy. 

  Block IO layer coalesces, packs and forms BIO 
structures by sequential LBA chains and block-
IO submission request size. 

For sequential chain analysis, i.e. runs  of  request with 
adjacent  (numerically  consecutive)  LBAs  for  an  object  is 
considered. For e.g.: During object writes (allocation), if ob-
ject size (O) is 128MB on an objectstore with 16 disks (Di) 
set-up with Erasure-code RS(12,4), i.e. 12 data (D) + 4 par-
ity  (P).  There  are  16  chunks  of  an  object  (O)  with  each 
chunk (C) is stored in a single OSD. In an ideal scenario, 
the  filesystem  should  allocate  10MB  of  contiguous  LBA 
space for object-chunk (C) inside an OSD. i.e.,  

     (a) 

           (b) 

              (c) 

                (d) 

However to quantify fragmentation or aging, we also use 
another metric, Natural Transfer Size (NTS) proposed by 
Convay  et.al  [14],  [22].  NTS  corresponds  to  the  length  of 
sequential LBA chains submitted to the device for achiev-
ing  maximal  throughput  performance.  Any  LBA  (or  IO) 
chains  greater  than  NTS  achieves  full  device  bandwidth. 
Based  on  their  analysis,  NTS  is  4MB  for  both  HDDs  and 
SSDs. 

3.3  OSM Workload: Lexicographic Listing 

Lexicographic listing or enumeration of objects is a clas-
sic  object-store  core  functionality  used  extensively  by 
OSMs and user-applications [3], [4], [6]. OSMs use it for re-
covery, indexing, rebalancing and scrubbing for reliability. 
While  user-applications  use  it  for  building  their  own  in-
dexes.  Enumeration  involves  access  to  storage  and  then 
computation (listing + sorting) while building indexes in 
cache. It is metadata heavy which involves reading object 
metadata (OM) or corresponding filesystem metadata. Each 
OM read involves path-lookups, sys calls, dcache and inode 
reads. This workload stresses the metadata (inode) manage-
ment and VFS. 

We write 1 million 128KB objects to the cluster with 3 
clients using YCSB key-value generator [31], then enumer-
ate all objects with cold cache with xfs and ext4 filesystem. 
Figure 3  shows the metrics to evaluate the workload on an 
OSD during the enumeration workload. The performance 

                (e) 

Figure  3  Object  Store  services  Management  Workload: 
Lexicographic  Enumeration  (a)  total  execution  time;  (b) 
dcache misses per server; (c) number of IOs incurred; (d) 
page  cache  miss  ratio;  (e)  Cummulative  Distribution 
(CDF) IO block-size (KB) for xfs and ext4. 

of both filesystem backed objectstores take hours for com-
pletion but xfs is 3x faster than ext4 as shown in Figure 3. 
While ext4 incurs 60% lesser dcache misses than xfs (please 
refer Figure 3 (b)) with denser page-cache evictions for ext4 
(Figure 3 (d)). This is attributed to the large number of page 
faults  or  expensive  disk  IOs  (15x)  for  serving  the  same 
workload on ext4 compared to xfs OSD as shown in Figure 
3(c)  rendering  the  filesystem  OS  page-caching  and  VFS 
caching  inefficient.  On  deeper  inspection  of  LBA  access 
pattern  or  heatmap  during  enumeration  shown  in  Figure 
4we visualize the superior block allocation and metadata 
access of xfs over ext4. We observe that xfs places metadata 
on specific bands or allocation groups while ext4 has poor 
metadata on-disk layout. Further, on observing the cumu-
lative distribution (CDF) of IO size in Figure 3(e), most ext4 
IOs are 4KB and for xfs 40% of all IO requests are 16KB. As 
this workload is metadata heavy, most IO requests are 4KB 
and  16KB  in  size  which  is  also  coherent  to  the  default 
metadata  B-tree  node  size  for  ext4  and  xfs,  respectively. 

 
 
 
 
 
 
 
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

7 

W-O  workload:  W-O  workload  persists  objects  ensuring 
proper  organization  of  data  for  efficient  storage  and  re-
trieval. An object write follows read-modify-write protocol, 
i.e,  first  the  object  metadata  (OM)  is  read  from  caches  or 
OSDs for validity of the object and then store subsequent 
object data(+metadata). For an OSD, the object chunk (C) 
for  erasure-coded  systems  is  stored.  Each  require  valida-
tion checks of the associated file in the OS filesystem such 
as path-lookups, stat, read/write system calls, etc. The IO 
path is redundant and depends on reading metadata and 
then start issuing write IOs for object data.  

Figure 5 [a-e] quantifies data placement in a write inten-
sive workload. Both xfs and ext4 objectstore backends have 
marginal (~5%) difference in execution time (please refer 
Figure  5(a)),  attributed  to  the  synchronous  dependency 
along  with  the  OS  kernel  overheads.  The  filesystem 
metadata grows enormously due to large no. of files cre-
ated  for  the  objects.  As  shown  in  Figure  5(b),  the  VFS 
metadata caching is not effective in both cases (xfs is better) 
resulting in large number of disk seeks for corresponding 
metadata  depending  on  filesystem  index  management 
[16], coherent with the page-fault density in Figure 5(c) . On 
analyzing block IO size CDF (Figure 5(d)) submitted to the 
OSD, predominantly all read IOs are metadata reads and 
40% of write IOs (≤16K) are updates to object metadata and 
filesystem, with xfs (B-tree metadata node size = 16K) in-
curring 2x lesser metadata IOs than ext4 (node size = 4K). 
This is coherent with lower page-cache and dcache misses 
in xfs due to larger and fewer metadata. 

Filesystems typically rely on OS pagecache for buffering 
writes and prefetching. In a lightly loaded system, writes 
are staged in-memory and provide the opportunity to allo-
cate contiguous LBAs before synchronizing at regular in-
tervals. On analysis of the write IO block size for both cases 
in Figure 5(d), we observe that roughly 50% of the write IO 
traffic is 256K (Object-IO) with 65% for xfs, 256K is the de-
fault max. BIO size. The 10% of the IOs are between 64K-
256K indicating contentions and breakage of 256K requests 
into smaller IOs. This is attributed to sporadic cache evic-
tions  (write-back)  which  are  dependent  on  various  com-
plex  set  of  policies,  which  also  lead  to  sub-optimal  block 
allocation, and an object can be fragmented (non-contigu-
ous LBAs).  

On  investigating  the  filesystem’s  and  block  IO  layer's 
ability to retain Object chunk (C=10MB) in an OSD or sub-
mission  order,  we  analyze  the  sequential  chain  length  in 

Figure 4 LBA heatmap over time during enumeration. 

For xfs, this in addition to band-based allocation is the rea-
son  for  better  cache  performance  and  lower  page-cache 
misses.  Therefore,  this  clearly  shows  superior  file-system 
metadata management results into better performance, lower re-
source consumption (cache, memory, disk IOs) and on-disk lay-
out beneficial for the entire life-cycle of the object-store. 

3.4  Application Workload Analysis 

Literature  on  large  scale  datacenters  and  cloud  analytics 
services over objectstores [5], [7], [9], [10] broadly classifies 
workloads  into  three  categories,  namely,1)  Write-Only  
(W-O), 2) Read-Only  (R-O), and, Read-Write  (R-W), with 
characteristics as shown in Table 1.  

Table 1 Object-Store User Workload Characteristics 

For  simulating  near  realistic  cloud  representative  work-
loads, we use YCSB[31] MinIO workload engine for gener-
ating object key-values with 3 clients and 128 threads con-
currently issuing Object-IO  requests  over s3a interface to 
our  distributed  MinIO  object-store  cluster7.  The  size  of 
each object (O) is selected to 128MB, which is common ob-
ject  size  for  data-warehousing  and  analytics  objectstore 
workloads  such  as  HDFS  over  S3  (HDFS  most  popular 
block size = 128MB). W-O is a typical bulk data ingestion 
workload  with  uniform  distribution  to  generate  similar 
patten  as  observed  in  [10].  Similarly,  for  R-O  and  R-W, 
YCSB-B and YCSB-A with zipfian distribution to emulate 
highly  concurrent  read  heavy  data-warehousing  and  ex-
tract-transform-load (ETL) workloads, respectively. These 
workloads are designed to simulate similar workloads on 
in-node  OS  and  OSDs  as  experienced  in  highly  complex 
and parallel cloud deployments. 

7 Benchmarks such as CosBench [45], s3benchmark, and WARP had sim-
ilar  findings,  but  we  preferred  YCSB  over  others  due  to  its  widespread 

popularity, reproducibility and applicability to represent real-world cloud 
workloads 

 
 
 
 
 
 
 
 
 
 
8 

Figure 5 Application Object-storage Workload: [(a) - (e)] W-only; [(f) - (j)] R-only; [(k) - (o)] R-W as per Table 1 with ext4 and xfs 
backends  on  a  MinIO  distributed  object-storage  node.  Here  (a),(f),(k)  total  execution  time;  (b),(g),(l)  no.  of  dcache  misses; 
(c),(h),(m) page cache miss ratio; (d),(i),(n) Block-IO size CDF (KB) Write IOs:xfs,ext4, Read IOs:xfs,ext4; (e),(j),(o) Sequential 
chain length CDF (KB): xfs,ext4; OSD Object chunk (C) = 10MB; NTS = 4MB [Convay et. al]. 

Figure 5(e).  It clearly shows that the chains are both filesys-
tems are not able to sustain the contiguous chains and the 
submission order is fragmented with atleast 3 contiguous 
LBA fragments. For quantifying it with NTS, xfs has 80% 
of the chains greater than 4MB (NTS) while ext4 sustains 
only 30% with most of the chains (~70%) being less than 
128K  indicating  poor  concurrency  control  and  higher  re-
source contentions at block layer. Therefore, longer IO path, 
dependent IOs, poor metadata management leads to sub-optimal 
placement,  fragmentation  and  higher  resource  consumption. 
The impact of fragmentation or poor data layout is more 
prevalent during reads. 

R-O  workload:  Serving  reads  faster  is  a  crucial  issue  in 
computing  systems  as  applications  need  to  wait  for  the 
data. Read-only workloads scan the objects from the OSDs 
to the compute nodes. The basic principle is to efficiently 
place  the  data  in  OSD  such  that  retrieval  performance  is 
maximized. Figure 5 [f-j] shows the characterization of R-O 
workload on already persisted objects. The overall execu-
tion time of xfs is 20% lower than ext4, coherent to superior 
metadata  management  and  data  placement  being  ex-
tremely  vital  for  future  retrieval  during  object-store  life-
time.  OS  page-caching  and  VFS  caching  (Figure  5  (f)  and 
(g),  respectively)  are  ineffective,  which  may  be  due  to 
translations from Object IO → Block IO. However it is still 
10x lower than W-O workload as an object read involves 
finding the relevant object file and reading it from OSD di-
rectly. CDF of block IO size in Figure 5(i) shows that xfs has 
much  lower  metadata  footprint  than  ext4,  which  consti-
tutes  20%  (≤16K)  and  80%  (≤4K)  of  the  IO  traffic  respec-
tively, rest is object data (128K). Sequential chain analysis 

in  Figure  5(j),  shows  that  object  sequentiality  is  not  re-
tained,  with  xfs  having  superior  index  organization  and 
concurrency control with 70% of chains being 256K in size 
compared to 40% for ext4. In best case, 20% of the chains 
are 1MB, much lower than NTS. The intense contention at 
block  layer  makes  coalescing  ineffective  with  max.  8  IOs 
for xfs (1MB/128K) and 2 IOs for ext4 (256K/128K) being 
merged. This clearly indicates read-aging and underutiliza-
tion of the OSD. 

R-W workload: In R-W workloads (similar to ETL), objects 
are  read,  computed  and,  if  required  new  objects  are  per-
sisted. Figure 5 [k-o] characterizes the R-W workload, we 
observe that the execution time xfs is 1.4x faster than ext4 
with 2x lower dcache misses. While page-cache is ineffec-
tive  with  spikes  of  nearly  100%  misses  due  to  reads  and 
density of ext4 misses much higher than xfs. This is coher-
ent to the CDF block IO size findings (Figure 5(n)) with 90% 
and 50% of read IO traffic for ext4 and xfs respectively, is 
due to metadata lookups. On analyzing sequential chains 
in Figure 5(o), even in best case i.e. xfs 40% of the chains are 
shorter than NTS (4MB) while during data-placement (W-
O)  its  80%  (for  ext4  its  15%).  Therefore,  similar  to  R-O 
workloads,  R-W  are  also  impacted  by  initial  data  layout 
due  to  read-aging  during  reads  for  already  persisted  data 
and suggesting free-space fragmentation due to unavailabil-
ity  of  runs  of  free  space  for  new  objects  due  to  already 
poorly organized data in OSDs.  

Through our comprehensive workload characterization 
it  is  clearly  evident  that  complex  and  redundant  object 
management  policies  coupled  with  application  require-
ments  puts  intense  strain  on  the  already  vulnerable  OS 

 
 
  
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

9 

stack. This leads to the underutilization of the OSD (object-
storage device) deployed for the lifetime of the objectstore. 
Though,  both  filesystems  in  consideration  (ext4  and  xfs) 
suffer from read-aging, however due to superior metadata 
organization and data allocation strategies xfs outperforms 
ext4  while  consuming  lower  host  and  disk  resources. 
Therefore,  a  conventional  OS  based  object-store  is  as 
good as the underlying OS IO stack.  

4  RELATED WORKS 

There  have  been a  plethora of  solutions  proposed  for  re-
ducing the impact of long and iterative OS IO path. From 
developing  new  file-systems  such  as  BtrFS  [18],  BetrFS 
[32], F2FS [21] for efficient index and consistency manage-
ment to storage device specific optimizations [32]–[35]. To 
reduce  index  management  overheads,  multiple  solutions 
are  proposed  from  use  of  hash-based  operatives  such  as 
DLFS [36]  to bypassing some OS layers such as ByVFS [16] 
which tend to avoid VFS for metadata overheads. File sys-
tems have been discussed in detail in the paper. While ad-
vanced optimizations such as atomicity, transaction, and, 
snapshot support to local file systems as well as use of us-
erspace filesystems such as FUSE [37], FSaP [17] have been 
futile  due  to  unstability  in  production  environments  [6]. 
The adoption of data-centric specific emerging devices re-
quires extensive revamping of the host and data manage-
ment storage stacks to overcome OS data path limitations. 
For instance, recent interests in the development of device 
specific optimizations such as [38] for high capacity SMR 
HDDs,  and  Open-channel  SSDs  and  Zoned  Namespace 
SSD (ZNS) [39] to eliminate long FTL IO tail latency. Local 
production file systems such as ext4, and xfs work on up-
date-in-place  design,  and  the  zone  management  requires 
log-structured COW, making the interfaces backward in-
compatible.  While  their  host-managed  solutions  [38]    is 
highly unstable with unpredictable performance with con-
trol  plane  shifting  towards  host  resulting  in  higher  re-
source consumption [6]. There have been also been efforts 
to build complete storage subsystems without kernel file-
system and caches such as Aerospike [40]. Initial Ceph [4] 
object-store- FileStore is built on top of file system, prefer-
ably  xfs.  Due  to  the  afore-mentioned  issues  with  redun-
dant OS IO stacks lead to the development of Ceph Blue-
Store [6]. BlueStore is designed to manage indexes or object 
metadata using ordered key-value store RocksDB [26] and 
runs of custom built userspace filesystem BlueFS, while the 
binary object data is written asynchronous to raw block de-
vices  for  maximizing  sequential  allocations.  KV  stores 
such as RocksDB can improve metadata performance but 
consume  huge  amount  of  host  resources  limiting  the  in-
node scalability, suffer from severe compaction and high 
write-amplification limiting the lifetime of the OSD [1]. All 
these solutions are highly specific limiting their adaptabil-
ity  in  production  storage  systems,  most  of  them  being 
piece-meal  solutions  which  have  high  tradeoffs  between 
host resource consumption, stability, consistency and high 
performance [1], [6]. 

5  FUTURE DIRECTIONS: OBJECT DRIVES 

Considering the complexity of building storage systems 
from scratch, a natural progression is the development of 
holistic storage device technologies which can execute host 
data management functionalities. In production eliminat-
ing the OS kernel completely is not feasible but offloading 
some  key  performance  critical  functionalities  with  mini-
mal infrastructural changes can provide seamless benefits 
(please refer Section 4). Here, we briefly discuss the poten-
tial of such a new class of storage devices, defined as Ob-
ject-Drives  as  a  solution  to  alleviate  object-stores  from 
afore-mentioned hefty and redundant host (OS + applica-
tion) IO-path discussed in the previous sections. In the case 
of object-drives, host data management functionalities can 
be  offloaded  and  execute  inside  the  storage  device  itself, 
thereby  simplifying  the  overall  SW  storage  stack  and 
streamlining the data processing. 

Samsung KV-SSD [1] and Seagate Kinetic Drive [2] are 
classic  industrial  use-case  implementations  of  Object-
Drives  which use a key-value (KV) interface instead of tra-
ditional block based (LBA) interface. Seagate Kinetic plat-
form [2]  provides a KV interface over Ethernet while exe-
cuting LevelDB KV-store inside a HDD. Samsung KV-SSD 
[1] is the first Key-Value SSD (KVSSD) industrial prototype 
with  SNIA  standardized  API,  which  executes  key-value 
management inside the device itself. Host kv-stores can di-
rectly  interact  with  underlying  devices  using  KV-pairs 
through a thin host library KV-API, thereby allowing users 
to switch from block SSDs to KVSSD with just a firmware 
change.  

Key-value stores such as RocksDB [26] are extensively 
employed by storage backends, especially objectstores for 
basic  object  (data  (OD)  +  metadata  (OM))  load/store, 
caches,  consistency  and  indexing.  We  briefly  discuss  the 
potential  of  Object-Drives  deployed  in  objectstores  as 
OSDs.  We  limit  our  discussion  to  alleviation  of  OS  over-
heads  while  being  storage  media  agnostic  as  we  want  to 
evaluate  the  concept  of  Object-Drive.  However,  we  use 
KVSSD [1] as an use-case which has recently gained atten-
tion in industrial and academic research [1], [11], [12], [41]. 
Consider  Figure  1(b)  which  shows  our  proposed  Object-
Drive based object-storage architecture that is also broadly 
divided into three major components: 

  Userspace  consists  of  user  applications  and  ob-
jectstore  manager  (OSM)  services  for  managing 
user object-namespace. 

  OS kernel consists of light weighted Object-Drive 
library  which  acts  as  a  glue  between  userspace 
and the OSD, providing an interface to persist Ob-
ject KV pairs with protocols similar to KV-API in 
KVSSD  [1].  Therfore,  shifting  the  data  control 
plane to the OSD to be managed internally. 
  Object-Drive as OSD, which can support variable 

 
 
 
 
10 

(a) 

                               (b) 

Figure 6  Normalized latency breakdown for user-applications comparing conventional OS-based with Object-Drive OSD 
with (a) HDD; and (b) SSD, storage media backend. IO type: Object Data (OD), Object Metadata (OM) and Filesystem 
Metadata (FSM). 

sized KV-pairs internally managing Object-IO → 
Physical Page addressing via multi-level hash via 
flat-namespace such as defined in DLFS [36]. The 
evident advantage of using Object-Drives is that it 
makes the object-storage stack much leaner com-
pared to traditional OS based stack shown in Fig-
ure 1. 

Object-Drive solutions are capable of alleviating object-
stores from the adverse issues due to the hefty operating 
system (OS) IO stack, as discussed in Sections 2 and 3 due 
to the following: 

1.  Light  weight  address  translation:  Expensive  multi-layer 
translation mapping is not required any more. Application 
characteristics can be retained with variable size KV pair 
support contrary to fixed sized (LBA) blocks in block de-
vices,  The  IO  path  is  reduced  to  Object-IO  →  Object-ID 
(OID) to identify KV-pairs by the device. Clearly, host re-
sources of compute, memory (VFS, caches) and expensive 
disk  IOs  are  saved  (discussed  in  Section  3)  for  data  and 
metadata. OIDs as key can directly be used by applications 
via the Object library and type of request in the command 
header.  

2. Simplified persistence: Data in our use-case is large to in-
crease the chances of placing related data together (or re-
duce aging) inside the device, the driver can keep track of 
object-parts  initiating  multiple  fixed  size  DMAs  larger 
than device Natural transfer size (NTS) asynchronously for 
performance  with  hints  from  driver.  Allocation  of  large 
data similar to DLFS [36] using hashes for supporting large 
sequential allocation with efficient bin-packing algorithms.  

core-functionalities such as lexicographic listing discussed in 
Section 3.3. This reduces host resources and time for listing 
related objects while also frees up device bandwidth due 
to multiple IOs for every object. 

5. Reduced contentions: As discussed previously, with fewer 
layers and simplified device internal object management, 
the contention for resources is significantly less. KV-SiPC 
[11]  analyzes  time  spent  on  locks  is  ~35%  lower  for  KV-
SSD stacks than traditional OS IO stacks. This is due to the 
reduction  in  number  and  synchronous  dependence  of 
metadata ops such as on-demand indoes, caches, transla-
tions, WAL and journals compared to block-devices. Such 
overheads can also account for upto 70-90% of the IO traf-
fic (Section 3.4 and [42]). Therefore, coherent to our analy-
sis in the previous sections, i.e., leaner the IO path lesser the 
overheads in request processing. 

5.1  Object-Drive Analysis 

Based  on  our  analysis  we  understand  that  on  average 
atleast ~40% of the total IO time is consumed by kernel re-
sources  (please  refer  Figure  2(b)),  while  in  complex  ob-
jectstore environments it could be much higher.  We focus 
on the fundamental root issue, i.e. need for the shift from 
hefty and redundant OS IO stack in an objectstorage node. 
As per our knowledge other works on such devices such 
as  [1],  [11],  [12]  focus  on  individual  device  performance 
and application. Therefore, our goal is to briefly evaluate the 
concept of Object-Drive agnostic of the storage media type (i.e. 
HDDs and SSDs) in reducing the OS overheads when deployed 
as an OSD. 

3. Consistency without journaling: Similar to KV-SSD [1] bat-
tery-backed DRAM and in-device transaction support [41] 
provides consistency gaurantees and in-device index man-
agers eliminate the need for additional logging and jour-
naling mechanisms while reducing read/write IO amplifi-
cation. 

4.  Advanced  functionalities:  Similar  to  KV-SSDs  [1],  ad-
vanced feature-set such as in-device support for grouping 
keys (objects) or iterators can be extremely useful for OSM 

It is outside the scope of this paper to implement an ob-
jectstore on top of Object-Drives which requires complete 
control and IO path re-implementations with consistency 
gaurantees, leaving aside the most important factor of lim-
ited  public  availability  of  Object-Drives.  Hence,  we  con-
duct trace-driven simulation by collecting block IO traces, 
system metrics from an OSD and analysis framework us-
ing the tool-chains  described in Section 3.1 during actual 
application workloads runs ( please refer Table 1). The pa-
rameters for modeling storage device characteristics is cho-

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

11 

sen from the data-sheet specifications on which these de-
vices  have  been  implemented,  i.e.  Seagate  Kinetic  HDD 
model ST4000NK001 [43] for HDDs and Samsung PM983 
NVMe SSD [44] (KVSSD [1]) for SSDs, respectively. Based 
on the eBPF tool chains [30] and our analysis framework as 
discussed in Section 3, we identify the source and type of 
IO, i.e. Object Data (OD), Metadata (OM) and Filesystem 
Metadata (FSM) by tags, respectively.  Figure 6 shows the 
normalized  total  layer-wise  latency  breakdown  incurred 
during a workload for access to OD, OM and  FSM in an 
OSD deployed in conventional OS (xfs) and Object-Drive 
objectstore with HDD and SSD as OSD, Figure 6(a) and (b) 
respectively. As mentioned earlier, our goal is not to com-
pare HDDs or SSDs but evaluate the benefits of reducing 
OS overheads by Object-Drives for a particular workload.  
 Comparing  to  its  OS  objectstore  counterparts,  for  all 
workloads the total overhead reduction using ObjectDrive 
OSD is 20-38% in total time-savings. The gains are higher 
in write or update heavy workloads, particularly for R-W 
workload  as  its  metadata  heavy  and  the  major  benefits 
comes  from  reduction  of  FSM  IO  traffic.  These  are  small 
dependent  IOs  (read-modify-write)  which  increases  the  re-
quest processing overheads as well as introduces random-
ness in IO affecting all layers of the IO hierarchy as well as 
HDDs  and  SSDs  both.  The  impact  on  HDDs  is  higher  as 
compared to SSDs (please refer the green bar in Figure 6(a) 
and (b) W or R-W workload) due to mechanical disk seeks. 
Therefore, direct Object access without multi-layer transla-
tions  can  significantly  reduce  delays  due  to  FSM.  The 
streamlined light-weighted object translations also reduces 
the  object  metadata  (OM)  book-keeping  overheads  in  all 
workloads which results in decrease in OM IO traffic. We 
also observe that for servicing the actual object data (OD) 
in  ObjectDrive  solutions  the  time  taken  is  relatively  low. 
This  is  attributed  to  the  in-device  OID  translations,  de-
creased metadata dependencies, suggesting larger sequen-
tial  chains  or  larger  IOs  (more  opportunities  to  coalesce) 
during storage and retrieval in OSDs due to also removal 
of randomness causing FSM IOs with reduced contentions.  

 However, our model is not fully accurate as we do not 
account for latencies for key handling and packing inside 
the device, however at high queue depths and large no. of 
threads the effect is amortized [42]. We purposely do not 
model  complete  contiguous  object  placement  inside  the 
device as our analysis is based on OS overhead removal as 
actual  existing  devices  have  limitations  for  value  size 
(~2MB for KVSSDs [1]). Studies such as [1], [42] show that 
individual  IOs  for  KVSSDs  can  be  lower  than  block  the 
end-to-end  (E2E)  performance  benefits  due  to  overheads 
reduction  is  much  higher  with  reduction  in  CPU  utiliza-
tion,  i.e.  ~13x  compared  to  RocksDB  on  block-SSD.  Our 
study quantifies the impact of the OS overheads in object-
stores,  more  specifically  what,  why,  and,  how  leads  to  the 
underutilization of the OSDs. 

6  CONCLUSION 

In  this  paper,  through  comprehensive  under-the-hood 
analysis of an object-storage node, we characterize the im-
pact  of  object-store  workloads  on  the  Operating  Systems 
IO stack in a storage server and its subsequent rippling ef-
fect on the underlying Object-Storage Device (OSD).  We 
observe that the legacy architecture of the OS storage stack 
coupled with complex data management policies leads to 
a performance mismatch between what an end-storage de-
vice is capable of delivering and what it actually delivers 
in a production environment. Owing to the associated is-
sues of object-management and the vulnerabilities of exist-
ing IO software stacks, we discuss the potential of a new 
class  of  storage  devices,  known  as  Object-Drives,  where 
host data management functionalities can be offloaded to 
the  storage  device.  Thereby,  making  the  IO  stack  leaner. 
Based on our analysis, we believe object-drives can allevi-
ate  object-stores  from  highly  taxing  overheads  of  data 
management with 20-38% time-savings over traditional OS 
storage stack. 

7 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 
[9] 

[10] 

REFERENCES 

Y. Kang et al., “Towards building a high-performance, scale-
in key-value storage system,” in Proceedings of the 12th ACM 
International Conference on Systems and Storage, 2019, pp. 144–
154. 
J.  Hughes,  “Seagate  kinetic  open  storage  platform,”  in 
Proceedings of the 30th IEEE International Conference on Massive 
Storage Systems and Technology (MSST’14), 2014, pp. 2–6. 
P.  Braam,  “The  Lustre  storage  architecture,”  arXiv  Prepr. 
arXiv1903.01955, 2019. 
S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, and C. 
Maltzahn, “Ceph: A scalable, high-performance distributed 
file system,” in Proceedings of the 7th symposium on Operating 
systems design and implementation, 2006, pp. 307–320. 
A.  Anwar,  Y.  Cheng,  A.  Gupta,  and  A.  R.  Butt,  “Mos: 
Workload-aware  elasticity  for  cloud  object  stores,”  in 
Proceedings of the 25th ACM International Symposium on High-
Performance Parallel and Distributed Computing, 2016, pp. 177–
188. 
A. Aghayev, S. Weil, M. Kuchnik, M. Nelson, G. R. Ganger, 
and  G.  Amvrosiadis,  “File  systems  unfit  as  distributed 
storage backends: lessons from 10 years of Ceph evolution,” 
in  Proceedings  of  the  27th  ACM  Symposium  on  Operating 
Systems Principles, 2019, pp. 353–369. 
J.  Tan  et  al.,  “Choosing  a  cloud  DBMS:  architectures  and 
tradeoffs,” Proc. VLDB Endow., vol. 12, no. 12, pp. 2170–2182, 
2019. 
“MinIO.” . 
Y. Cheng, Z. Chai, and A. Anwar, “Characterizing co-located 
datacenter workloads: An alibaba case study,” in Proceedings 
of the 9th Asia-Pacific Workshop on Systems, 2018, pp. 1–3. 
M.  Vuppalapati,  J.  Miron,  R.  Agarwal,  D.  Truong,  A. 
Motivala,  and  T.  Cruanes,  “Building  An  Elastic  Query 
in  17th  USENIX 
Engine  on  Disaggregated  Storage,” 
Symposium  on  Networked  Systems  Design  and  Implementation 
(NSDI 20), 2020, pp. 449–462. 

 
 
 
 
 
12 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

[22] 

[23] 

[24] 

[25] 

[26] 

[27] 

[28] 

J. Bhimani, J. Yang, N. Mi, C. Choi, M. Saha, and A. Maruf, 
“Fine-grained  control  of  concurrency  within  KV-SSDs,”  in 
Proceedings  of  the  14th  ACM  International  Conference  on 
Systems and Storage, 2021, pp. 1–12. 
M. P. Saha, A. Maruf, B. S. Kim, and J. Bhimani, “KV-SSD: 
What  Is  It  Good  For?,”  in  2021  58th  ACM/IEEE  Design 
Automation Conference (DAC), 2021, pp. 1105–1110. 
P.  Biswas,  F.  Patwa,  and  R.  Sandhu,  “Content  level  access 
control for openstack swift storage,” in Proceedings of the 5th 
ACM Conference on Data and Application Security and Privacy, 
2015, pp. 123–126. 
A.  Conway  et  al.,  “File  systems  fated  for  senescence? 
nonsense, says science!,” in 15th USENIX Conference on File 
and Storage Technologies (FAST 17), 2017, pp. 45–58. 
P. Mishra, M. Mishra, and A. K. Somani, “Bulk i/o storage 
management  for  big  data  applications,”  in  2016  IEEE  24th 
International Symposium on Modeling, Analysis and Simulation 
of  Computer  and  Telecommunication  Systems  (MASCOTS), 
2016, pp. 412–417. 
Y. Wang, D. Jiang, and J. Xiong, “Caching or not: Rethinking 
virtual file system for non-volatile main memory,” 2018. 
J. Liu, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, and S. 
Kannan, “File systems as processes,” 2019. 
O. Rodeh, J. Bacik, and C. Mason, “BTRFS: The Linux B-tree 
filesystem,” ACM Trans. Storage, vol. 9, no. 3, pp. 1–32, 2013. 
A.  Sweeney,  D.  Doucette,  W.  Hu,  C.  Anderson,  M. 
Nishimoto,  and  G.  Peck,  “Scalability  in  the  XFS  File 
System.,” in USENIX Annual Technical Conference, 1996, vol. 
15. 
A.  Mathur,  M.  Cao,  S.  Bhattacharya,  A.  Dilger,  A.  Tomas, 
and L. Vivier, “The new ext4 filesystem: current status and 
future plans,” in Proceedings of the Linux symposium, 2007, vol. 
2, pp. 21–33. 
C.  Lee,  D.  Sim,  J.  Hwang,  and  S.  Cho,  “F2FS:  A  new  file 
system for flash storage,” in 13th USENIX Conference on File 
and Storage Technologies (FAST 15), 2015, pp. 273–286. 
A.  Conway  et  al.,  “Filesystem  aging:  It’s  more  usage  than 
fullness,” 2019. 
P.  Mishra  and  A.  K.  Somani,  “Host  managed  contention 
avoidance storage solutions for Big Data,” J. Big Data, vol. 4, 
no. 1, p. 18, 2017. 
M.  Bjørling,  J.  Axboe,  D.  Nellans,  and  P.  Bonnet,  “Linux 
block IO: introducing multi-queue SSD access on multi-core 
systems,”  in  Proceedings  of  the  6th  international  systems  and 
storage conference, 2013, pp. 1–10. 
S. Ibrahim, H. Jin, L. Lu, B. He, and S. Wu, “Adaptive disk 
i/o scheduling for mapreduce in virtualized environment,” 
in 2011 international conference on parallel processing, 2011, pp. 
335–344. 
S.  Dong,  A.  Kryczka,  Y.  Jin,  and  M.  Stumm,  “Evolution  of 
Development  Priorities  in Key-value  Stores  Serving  Large-
scale  Applications:  The  RocksDB  Experience,”  in  19th 
USENIX Conference on File and Storage Technologies (FAST 21), 
2021, pp. 33–49. 
M. Shahrad et al., “Serverless in the Wild: Characterizing and 
Optimizing  the  Serverless  Workload  at  a  Large  Cloud 
Provider,” arXiv Prepr. arXiv2003.03423, 2020. 
Z.  Ren,  X.  Xu,  J.  Wan,  W.  Shi,  and  M.  Zhou,  “Workload 

[29] 

[30] 
[31] 

[32] 

[33] 

[34] 

[35] 

[36] 

[37] 

[38] 

[39] 

[40] 

[41] 

[42] 
[43] 
[44] 
[45] 

pp. 

143–154, 

[Online]. 

characterization  on  a  production  Hadoop  cluster:  A  case 
study on Taobao,” in 2012 IEEE International Symposium on 
Workload Characterization (IISWC), 2012, pp. 3–13. 
S.  Muralidhar  et  al.,  “f4:  Facebook’s  Warm  BLOB  Storage 
System,”  in  11th  USENIX  Symposium  on  Operating  Systems 
Design and Implementation (OSDI}$ 14), 2014, pp. 383–398. 
“BPF Compiler Collection (BCC).” . 
B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. 
Sears, “Benchmarking cloud serving systems with YCSB,” in 
Proceedings  of  the  1st  ACM  symposium  on  Cloud  computing, 
2010, 
Available: 
https://github.com/pingcap/go-ycsb. 
W. Jannen et al., “BetrFS: A right-optimized write-optimized 
file  system,”  in  13th  USENIX  Conference  on  File  and  Storage 
Technologies (FAST 15), 2015, pp. 301–315. 
J.  Xu  and  S.  Swanson,  “NOVA}$:  A  log-structured  file 
system for hybrid volatile/non-volatile main memories,” in 
14th  USENIX  Conference  on  File  and  Storage  Technologies 
(FAST 16), 2016, pp. 323–338. 
C.  Min,  K.  Kim,  H.  Cho,  S.-W.  Lee,  and  Y.  I.  Eom,  “SFS: 
random write considered harmful in solid state drives.,” in 
FAST, 2012, vol. 12, pp. 1–16. 
M.  Jung  and  M.  Kandemir,  “Revisiting  widely  held  SSD 
expectations  and  rethinking  system-level  implications,” 
ACM SIGMETRICS Perform. Eval. Rev., vol. 41, no. 1, pp. 203–
216, 2013. 
P. H. Lensing, T. Cortes, and A. Brinkmann, “Direct lookup 
and hash-based metadata placement for local file systems,” 
in  Proceedings  of  the  6th  International  Systems  and  Storage 
Conference, 2013, pp. 1–11. 
B. K. R. Vangoor, V. Tarasov, and E. Zadok, “To FUSE or Not 
to  FUSE:  Performance  of  User-Space  File  Systems,”  in 15th 
USENIX Conference on File and Storage Technologies (FAST 17), 
2017, pp. 59–72. 
F. Wu, M.-C. Yang, Z. Fan, B. Zhang, X. Ge, and D. H. C. Du, 
“Evaluating host aware SMR drives,” 2016. 
M.  Bjørling,  “From  open-channel  SSDs 
to  zoned 
namespaces,”  in  Linux  Storage  and  Filesystems  Conference 
(Vault 19), 2019, p. 1. 
V. Srinivasan  et al., “Aerospike: Architecture of a real-time 
operational  dbms,”  Proc.  VLDB  Endow.,  vol.  9,  no.  13,  pp. 
1389–1400, 2016. 
Y. Kang, P. Mishra, and Y. S. KI, “Interactive continuous in-
device  transaction  processing  using  key-value  (kv)  solid 
state drives (ssds).” Google Patents, 2021. 
“SNIA Samsung Key Value SSD explained.” . 
“Seagate Kinetic HDD Datasheet.” . 
“Samsung PM983 NGSFF NVMe SSD Datasheet.” . 
Q.  Zheng,  H.  Chen,  Y.  Wang,  J.  Zhang,  and  J.  Duan, 
“Cosbench: Cloud object storage benchmark,” in Proceedings 
of the 4th ACM/SPEC International Conference on Performance 
Engineering, 2013, pp. 199–210. 

 
 
 
 
 
MISHRA ET AL.:  LEARNINGS FROM AN UNDER THE HOOD ANALYSIS OF AN OBJECT-STORE NODE IO STACK 

13 

8  LEGAL INFORMATION 

THIS DOCUMENTATION IS PROVIDED FOR REFER-
ENCE  PURPOSES  ONLY,  AND  ALL  INFORMATION 
DISCUSSED HEREIN IS PROVIDED ON AN "AS-IS" BA-
SIS,  WITHOUT  WARRANTIES  OF  ANY  KIND.  SAM-
SUNG  SEMICONDUCTOR,  INC.  ($''SAMSUNG''$)  AS-
SUMES  NO  LIABILITY  WHATSOEVER,  INCLUDING 
WITHOUT  LIMITATION  CONSEQUENTIAL  OR  INCI-
DENTAL  DAMAGES,  AND  SAMSUNG  DISCLAIMS 
ANY  EXPRESS  OR  IMPLIED  WARRANTY,  ARISING 
OUT  OF  OR  RELATED  TO  THE  USE  OF  THIS  DOCU-
MENT, INCLUDING BUT NOT LIMITED TO, LIABILITY 
OR WARRANTIES RELATED TO FITNESS FOR A PAR-
TICULAR  PURPOSE,  MERCHANTABILITY,  OR  IN-
FRINGEMENT  OF  ANY  PATENT,  COPYRIGHT,  OR 
OTHER  INTELLECTUAL  PROPERTY  RIGHT.  SAM-
SUNG  RESERVES  THE  RIGHT  TO  CHANGE  THE  IN-
FORMATION,  DOCUMENTATION,  AND  SPECIFICA-
TIONS WITHOUT NOTICE. THIS INCLUDES MAKING 
CHANGES TO THIS DOCUMENTATION AT ANY TIME 
WITHOUT  PRIOR  NOTICE.  SAMSUNG  ASSUMES  NO 
RESPONSIBILITY  FOR  POSSIBLE  ERRORS  OR  OMIS-
SIONS, OR FOR ANY CONSEQUENCES FROM THE USE 
OF THE DOCUMENTATION CONTAINED HEREIN.  

Term of Use: This work is licensed under a Creative Com-
mons Attribution 4.0 License. It is attributed to Samsung 
Semiconductor,  Inc.  This  document  and  all  information 
discussed herein remain the sole and exclusive property of 
Samsung.  All  brand  names,  trademarks  and  registered 
trademarks belong to their respective owners. Unless spe-
cifically  identified  as  such,  Samsung's  use  of  third-party 
trademarks  does  not  indicate  any  relationship,  sponsor-
ship, or endorsement between Samsung and the owners of 
these  trademarks.  Any  references  by  Samsung  to  third-
party  trademarks  is  to  identify  the  corresponding  third 
party goods and/or services and shall be considered nom-
inative fair use under the trademark law. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
