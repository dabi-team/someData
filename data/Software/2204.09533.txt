2
2
0
2

r
p
A
0
2

]
E
S
.
s
c
[

1
v
3
3
5
9
0
.
4
0
2
2
:
v
i
X
r
a

Evaluating Commit Message Generation: To BLEU Or Not To
BLEU?

Samanta Dey
Venkatesh Vinayakarao
samanta@cmi.ac.in
venkateshv@cmi.ac.in
Chennai Mathematical Institute
Chennai, India

Monika Gupta
Sampath Dechu
mongup20@in.ibm.com
sampath.dechu@in.ibm.com
IBM Research
Delhi, India

ABSTRACT
Commit messages play an important role in several software en-
gineering tasks such as program comprehension and understand-
ing program evolution. However, programmers neglect to write
good commit messages. Hence, several Commit Message Genera-
tion (CMG) tools have been proposed. We observe that the recent
state of the art CMG tools use simple and easy to compute auto-
mated evaluation metrics such as BLEU4 or its variants. The ad-
vances in the ï¬eld of Machine Translation (MT) indicate several
weaknesses of BLEU4 and its variants. They also propose several
other metrics for evaluating Natural Language Generation (NLG)
tools. In this work, we discuss the suitability of various MT met-
rics for the CMG task. Based on the insights from our experiments,
we propose a new variant speciï¬cally for evaluating the CMG task.
We re-evaluate the state of the art CMG tools on our new metric.
We believe that our work ï¬xes an important gap that exists in the
understanding of evaluation metrics for CMG research.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software veriï¬cation and
validation.

KEYWORDS
BLEU, METEOR, Commit Message Generation

ACM Reference Format:
Samanta Dey, Venkatesh Vinayakarao, Monika Gupta, and Sampath Dechu.
2022. Evaluating Commit Message Generation: To BLEU Or Not To BLEU?.
In New Ideas and Emerging Results (ICSE-NIERâ€™22), May 21â€“29, 2022, Pitts-
burgh, PA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3510455.3512790

1 INTRODUCTION
Commit messages are Natural Language (NL) descriptions of code
changes. Developers submit such textual description along with
their code changes. These messages, called as commit messages or
log messages, are important to the overall software development
process and reduce the time taken to understand the code change.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9224-2/22/05. . . $15.00
https://doi.org/10.1145/3510455.3512790

Often, developers provide low quality commit messages or even no
message. Dyer et al. [16] analyzed 23,000 Java projects on Source-
Forge1 and reported that around 14% of commit messages were
empty. Therefore, CMG tools are required. Commit Message Gen-
eration (CMG) tools take code changes as input and produce com-
mit messages as output.

Automatically generated commit messages are evaluated against
the ground-truth. Human evaluation would provide the best assess-
ments, but it is expensive and time-consuming. Automated evalu-
ation of the CMG task so far has been limited to reusing metrics
popularly used for various Machine Translation (MT) and Natural
Language Generation (NLG) tasks.

The ï¬rst known automatic evaluation metric BLEU [15] relied
on n-gram precision. BLEU computes word overlap between the
predicted and the reference gold-standard written by the project
members. METEOR [2] is another metric which is based on uni-
gram matches on the words and also their stems with additional
synonyms database. In addition to these, numerous other metrics [18]
have evolved for automatically evaluating Machine Translation (MT)
systems. In addition to semantic scoring through word overlap,
these metrics propose diï¬€erent ways to penalize the length, and
score word order alignment.

BLEU and its variants may not be suitable for assessing commit
messages. Reiter [17] claims that the assumption of word overlap
correlating with real-world utility needs to be validated through
user studies or task performance. It is unclear, which metric is most
appropriate for evaluating the CMG task.

The fundamental factors behind the widely used MT based met-
rics are semantic scoring, length penalty and word order alignment.
In addition, Tao et al. [20] observe that case sensitivity and smooth-
ing could be potential factors aï¬€ecting CMG evaluation. The rele-
vance of these factors with respect to commit messages have not
been studied. Understanding their eï¬€ect on commit message qual-
ity as per expert perception i.e., as per human evaluation will help
us in deciding the metric to use for evaluating the commit mes-
sages. Hence, we ask the following research questions.

RQ1 What factors aï¬€ect commit message quality as per expert

perception?

RQ2 Which metric is best suited to evaluate commit messages?
RQ3 How do the CMG tools perform on the new metric?

To the best of our knowledge, this is the ï¬rst work to evaluate
the inï¬‚uence of various factors on existing metrics, and also pro-
pose a metric speciï¬cally to evaluate CMG tools.

1https://sourceforge.net/

 
 
 
 
 
 
ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Samanta et al.

2 BACKGROUND
2.1 Commit Message Generation Tools
Several CMG tools have been proposed.Jiang et al. [8] proposed
CommitGen which uses an attentional Recurrent Neural Network
(RNN) encoder-decoder based Neural Machine Translation (NMT)
model Nematus to translate code change diï¬€s into commit mes-
sages. The CommitGen model is trained using a corpus of code
changes and human-written commit messages from the top 1k Github
projects. NMT proposed by Loyola [12] in 2017, is similar to Com-
mitGen, but it is guided by a global attention model proposed by
Luong et al. [13] instead of the Bahdanau attention model in NMT.
This model is trained using code changes and human-written com-
mit messages from popular Github projects across multiple pro-
gramming languages. NNGen proposed by Liu et al. [11] in 2018,
is a retrieval-based model which is based on the Nearest Neighbour
and Bag-of-Words (BOW) approach. The NNGen model is trained
on a ï¬ltered version of the CommitGen dataset.

2.2 Automated Evaluation in Machine

Translation

Several automated metrics have been proposed [4]. The promi-
nent MT evaluation metrics used in evaluating CMG tools can
be broadly categorised as a) Precision based, b) Recall based, c) F-
Score based, and d) Edit Distance based metrics.

Precision based Metrics. During word matches, precision based
metrics score higher if there are more matches for words in the
predicted text. BLEU4 [16] is a classic example of a precision based
metric. BLEU4 is calculated as shown in Equation 1 where BP is the
brevity penalty, ğ‘¤ğ‘˜ refers to the empirically chosen weight of each
k-gram and ğ‘ğ‘˜ is the match score for the k-gram.

BLEU4 = BP Â· ğ‘’ğ‘¥ğ‘

4

Ã•ğ‘˜=1

ğ‘¤ğ‘˜ Â· ğ‘™ğ‘œğ‘”(ğ‘ğ‘˜ )

!

(1)

= ğ‘šğ‘˜ +1

There are several variants of BLEU such as BLEUMoses [9], BLE-
UNorm [12] and BLEUCC [5]. BLEUNorm applies smoothing to
the match score as ğ‘ğ‘˜
ğ‘™ğ‘˜ +1 where ğ‘šğ‘˜ is number of matched
k-grams between the reference and the predicted text and ğ‘™ğ‘˜ is the
the total number of k-grams in the predicted text. BLEUCC makes
further improvisation on this by applying the assumption that a
k-gram match score can be smoothed by k-1 and k+1 match scores
i.e., ğ‘šğ‘˜

= ğ‘šğ‘˜âˆ’1+ğ‘šğ‘˜ +ğ‘šğ‘˜+1
3

.

Recall based Metrics. During word matches, recall based metrics
score higher if more words are retrieved considering each word in
the reference text. ROUGE [10] is an example of a recall based met-
ric, which is calculated as shown in Equation 2, where Countmatch
is the number of n-gram matches between predicted and reference
(Ref) text while Count(n-gram) refers to the number of n-grams.

ROUGE-n =

n-gramsâˆˆRef Countmatch(n-gram)
n-gramsâˆˆRef Count(n-gram)

Ã

(2)

F-Score based Metrics. METEOR is an F-Score based metric. It
considers a fragmentation penalty, calculated as Frag Penalty =

Ã

where chunks are contiguous predicted

0.5 âˆ—

# chunks
# unigrams matched

h

i

3

unigrams mapped to contiguous unigrams in the reference. The
ï¬nal METEOR score is thus calculated using Equation 3.

METEOR = F-score âˆ— (1 âˆ’ Frag Penalty)

(3)

METEOR-NEXT [6] is a hyper parametrized version of METEOR.

Edit Distance based Metrics. In Edit Distance based metrics, num-
ber of word operations such as insertions, deletions and substitu-
tions are used to compute the word edit distance between the given
and reference texts. TER [19] is an example. The TER metric score
is calculated as shown in Equation 4.

TER = #substitutions + #deletions + #insertions

#words in reference text

(4)

3 EXPERIMENTAL SETUP
3.1 Selection of Factors
Commit messages although deï¬ned as NL descriptions of code changes,
are diï¬€erent from normal NL text. To address RQ1, we gather sev-
eral factors from the various evaluation metrics and related litera-
ture.

Papineni et al. [16] propose that a good evaluation metric should
ensure that the predicted text be neither too long nor too short.
Hence precision based metrics such as BLEU [16] and its variants
use length penalizers to penalize shorter predictions. â€œThe gun-
man killed the copâ€ and â€œThe cop killed the gunmanâ€ are not the
same sentences. In order to handle ordering, a simple bag of words
representation is not suitable. Metrics such as METEOR [2] and
METEOR-NEXT [6] account for the alignment of matched words
in the predicted and reference sentences. The pair â€œUpdate change"
and â€œUpdated changes" essentially convey the same meaning, al-
though exact-word matchers in BLEU and ROUGE would give a
score of zero instead of one. Hence, in addition to exact matches,
stemmed and paraphrased matchings are also considered by ME-
TEOR and METEOR-NEXT. Banerjee and Lavie [2] convert text
to lower case as a part of preprocessing in the implementation of
METEOR. Punctuations are often treated as separate words by the
default parsers and matchers in various programming languages
which heavily aï¬€ects the scores. BLEU4 geometrically averages the
n-gram matches, thereby, giving zero score for pairs like "added
chain of responsibility class diagram" and "added chain diagram"
due to missing n-gram matches of order three or above. BLEUCC
and BLEUNorm are smoothed versions of BLEU. Therefore, we con-
sider the factors namely length, word order, semantics, case, punc-
tuation and smoothing as shown in Table 2.

3.2 Comparison with Human Annotations
We use a dataset of 100 human evaluated reference and predicted
commit message pairs shared by Tao et al. [20]. The data is manu-
ally labelled between 0 to 4 by three domain experts and validated
for reliability. We took the arithmetic mean to obtain a single av-
eraged score of human evaluation for each pair of reference and
predicted commit messages. In our experiments, we compare the
scores produced using the MT metrics discussed in Section 2.2 with
these human annotation scores using Spearmanâ€™s correlation [21].
We do it once in the presence of the factor and once in its absence.
Table 2 lists the modiï¬cations in the metric formulae due to inclu-
sion and exclusion of the various factors used in our study.

 
Evaluating Commit Message Generation: To BLEU Or Not To BLEU?

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Table 1: Spearmanâ€™s correlations of metrics with human judgments w.r.t. potential factors. â€œCleanâ€ refers to the correlation of
metric in its original formulation without any changes.

Metric

BLEU4
BLEUNorm
BLEUCC
METEOR
METEOR-NEXT
ROUGE1
ROUGE2
ROUGEL
TER

Length

Word Order

Semantics

Case Folding

Punctuation

Smoothing

Without With Without With Without With Without With Without With Without
0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

0.707
0.699
0.693
0.807
0.822
0.781
0.485
0.781
0.54

0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

0.717
0.703
0.691
0.748
0.761
0.796
0.485
0.799
0.583

0.705
0.691
0.681
0.74
0.736
0.723
0.443
0.728
0.568

0.705
0.691
0.681
0.707
0.722
0.723
0.443
0.728
0.568

0.705
0.691
0.681
0.725
0.756
0.723
0.443
0.728
0.568

0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

0.69
0.683
0.683
0.748
0.761
0.723
0.443
0.728
0.568

With
0.691, 0.681
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

Clean

0.705
0.691
0.681
0.748
0.761
0.723
0.443
0.728
0.568

Table 2: Computation after addition/removal of factors in
metrics. Here, B4 is BLEU4, BN is BLEUNorm, BCC is
BLEUCC, M is METEOR and MN is METEORNEXT, FP is
Fragmentation Penalty.

Factor

Metrics

Factor not included

Factor included

Formula

Table 3: Performances of CMG models.

Model

CommitGen
NMT
NNGen

C++
11.94
11.69
13.82

C#
18.35
17.96
16.89

MCMDdata
Java
9.63
10.7
7.4

JS
18.11
13.58
18.25

Py
8.27
9.34
14.27

Avg
13.26
12.65
14.13

Length

Word Order

Semantics

B4, BN,
BCC
Others
M, MN

Others
M, MN

Others
Case
All
Punctuation All
B4
Others

Smoothing

4

ğ‘’ğ‘¥ğ‘

ğ‘¤ğ‘˜ğ‘™ğ‘œğ‘” (ğ‘ğ‘˜ )

4

BP âˆ—ğ‘’ğ‘¥ğ‘

ğ‘¤ğ‘˜ğ‘™ğ‘œğ‘” (ğ‘ğ‘˜ )

ğ‘˜=1
Ã•

!

No Change.

ğ‘˜=1
Ã•

F-scoreno alignment âˆ—
(1 âˆ’ FP)

F-score âˆ— (1 âˆ’ FP)

No Change.

F-scoreexact matches âˆ—
(1 âˆ’ FP)

F-score âˆ— (1 âˆ’ FP)

No Change.

No case folding.
No Change.
No Change.

Use lower case text.
Remove punctuations.
BN, BCC

No Change.

3.3 CMG Tools, Metrics and the Commit

Dataset

To answer RQ3, we consider the CMG tools discussed in Section 2.1.
Diï¬€erent CMG tools use diï¬€erent datasets for their evaluation. In
order to compare them, we need a uniï¬ed dataset. NMT and NNGen
use datasets consisting of only Java code changes that are rela-
tively small (i.e., not exceeding 50K). We follow Tao et al. [20] and
use their Multi-Language Commit Message (MCMD) dataset. This
dataset has 3.6M commit messages in ï¬ve popular Programming
Languages (PLs) from the top 100 starred projects on GitHub.

4 RESULTS
4.1 RQ1: What factors aï¬€ect commit message
generation as per expert perception?

Table 1 shows the Spearmanâ€™s correlation values with and with-
out the six factors applied to the various metrics. A factor is as-
sumed to aï¬€ect the evaluation of CMG if its presence in a met-
ric increases the Spearmanâ€™s correlation with human evaluation.
The observations from the Table 1 indicate that the factors Length,
Word Alignment, Semantic Scoring, Lower-Casing and Punctuation

!

Removal, when incorporated in the corresponding metrics, improve
correlation with human judgements. While inclusion of the factor
Smoothing reduces the correlation with human evaluation.

Commit message quality is inï¬‚uenced by Length, Word-
Alignment, Semantic Scoring, Lower-Casing and
Punctuation-Removal. These factors should be consid-
ered in a metric designed for the purpose of CMG evalua-
tion.

.

4.2 RQ2: Which metric is best suited to

evaluate commit messages?

None of the standard MT metrics discussed in Section 2.2 have
all the aï¬€ecting factors incorporated in them. This motivates the
construction of a new metric. We propose a modiï¬ed version of
the METEOR-NEXT metric, called Log-MNEXT. Section 5 discusses
the construction of Log-MNEXT. To validate the goodness of Log-
MNEXT to the existing MT metrics, its Spearmanâ€™s correlation with
human evaluation score is calculated and compared with that of
the other metrics in Table 1.

Log-MNEXT has the highest correlation with human eval-
uation as compared to the standard MT evaluation metrics
with a correlation value of 0.831.

.

4.3 RQ3: How do the CMG tools perform on

the new metric?

The CMG models discussed in Section 2.1 are evaluated using vari-
ants of BLEU. In a similar experimental setup as that of Tao et

 
 
ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Samanta et al.

al. [20], we compare the performances of CommitGen, NMT and
NNGen using Log-MNEXT metric.

The MCMD dataset is split across various programming lan-
guages (PLs). The average Log-MNEXT scores in percentages, across
datasets of diï¬€erent PLs for each of the models, is given in Table 3.
Overall, the retrieval-based model NNGen outperforms Commit-
Gen and NMT, with an average Log-MNEXT score of 14.13.

NNGen performs the best when evaluated on the MCMD
dataset using the Log-MNEXT metric.

5 LOG-MNEXT METRIC
METEOR-NEXT incorporates most of the relevant factors. Hence,
we base Log-MNEXT on METEOR-NEXT. In addition to exact word
matchings, Log-MNEXT also performs stemmed and paraphrased
matchings as in METEOR and METEOR-NEXT metrics. Log-MNEXT
performs string lower-casing and removes punctuation as a prepro-
cessing step, before looking for word matches or performing word
alignment between the predicted and reference texts.

Consider the case where we have identical reference and pre-
dicted messages. Any human annotator will not penalize this. How-
ever, the denominator of the fragmentation penalty factor Frag
Penalty of METEOR-NEXT given by Equation 5 depends on the
number of unigram matches. Hence, it penalizes this case. Log-
MNEXT improvises on the Frag Penalty by assigning no penalty
score in such cases.

Frag Penalty =

0

ğ›½ âˆ—

(

# chunks
# unigrams matched

ğ›¾

if Ref â‰¡ Pred

otherwise

The Log-MNEXT score is ï¬nally obtained using equation 6.

h

i

Log-MNEXT = F-score * (1-Frag Penalty)

(5)

(6)

To get the F-Score, Precision (P) and recall (R) are calculated by

assigning weights to the various unigram matches.

ğ‘ƒ =

ğ‘¤ğ‘–ğ‘šğ‘–
ğ¿pred
Ã

and ğ‘… =

ğ‘¤ğ‘–ğ‘šğ‘–
ğ¿ref

Ã

(7)

where for a matcher type i âˆˆ {exact match, stemmed match, syn-
onym match}, ğ‘¤ğ‘– is the weight and ğ‘šğ‘– is the number of matched
unigrams. ğ¿pred is the length of predicted text and ğ¿ref is the length
of reference text. The weighted F-score with 0 < ğ›¼ < 1 is calculated
as
ğ›¼ğ‘ƒ +(1âˆ’ğ›¼ )ğ‘… . For the optimum values of ğ›¼, ğ›½ and ğ›¾, we use the val-
ues suggested by Denkowski and Lavie [6].

ğ‘ƒğ‘…

6 RELATED WORK
There are works comparing [3, 7, 14, 18] automated evaluation
metrics for machine translation. Our work is diï¬€erent from them
because we discuss the suitability of metrics used in evaluating
commit message generation tools. Tao et al.â€™s [20] work is the clos-
est to our work. However, they limit their study to BLEU variants.
They report that the models rank inconsistently across the diï¬€er-
ent metrics. They ï¬nd that BLEUNorm is most correlated to human
annotations. They suggest smoothing and case sensitivity as two
potential reasons for it to perform well. Finally, they report exis-
tence of heavy sensitivity of these metrics on the dataset. These

are related and relevant to our work. However, we consider other
popular metrics from MT and NLG for evaluation. We ï¬nd that a
new metric is necessary. Further, we propose the new metric and
evaluate the performance of the existing models on the new met-
ric.

7 FUTURE WORK
The results of our experiments emphasize on the use of the novel
metric Log-MNEXT for evaluating commit messages. We now en-
list key future directions.

Building larger human annotated dataset. For the purpose of this
work, we have used existing human annotated dataset. The size of
the dataset (100) leads to internal validity. We plan to build our own
larger dataset, containing more number of reference and predicted
commit message pairs. Claims made on the basis of such a dataset
are expected to be more sound and strong.

Working on learning based metrics. This work limits to non-learning

based i.e., simple and fast metrics such as BLEU. This may lead to
external validity. This is mitigated to a large extent since the exist-
ing CMG tools use the fast metrics for evaluation for various rea-
sons including the fact that even the learning based metrics have
not been established to correlate signiï¬cantly with human annota-
tions better than the fast metrics. We plan to study learning based
metrics in detail as future work.

Revising the Log-MNEXT metric. This work presents the most
basic form of the metric Log-MNEXT. A major portion in the con-
struction of the metric has been adapted from the existing METEOR-
NEXT metric. Improvisations in the form of better synonym and
paraphrase matching, cautious removal of punctuation from sen-
tence pairs and careful case-folding before evaluation, are some
important scopes of future work.

Building an exhaustive commit dataset. To compare the perfor-
mances of the CMG tools on the Log-MNEXT metric, the MCMD
dataset has been used. Although the language diversity in MCMD
has been expanded to 5 popular PLs, it is still not exhaustive. Build-
ing an exhaustive dataset by including more PLs, is a direction of
future work. However, appropriate caution is needed when gener-
alizing our ï¬ndings across PLs.

8 CONCLUSION
In spite of various drawbacks, BLEU continues to be the most pop-
ular metric to be used in the evaluation of CMG models. We iden-
tify various potential factors aï¬€ecting CMG evaluation. The re-
sults of our experiments show the low correlation of BLEU with
human evaluated scores. With an aim of using an evaluation met-
ric which incorporates all valid factors, a new and novel metric
Log-MNEXT is proposed. This metric has the highest correlation
with human evaluations. Finally, we use this metric to compare
the existing CMG models on the MCMD dataset and show that
the NNGen model performs the best overall. We suggest that Log-
MNEXT should be considered as the evaluation metric for CMG
evaluation in future research, instead of BLEU or its variants.

We have shared the implementation [1] of Log-MNEXT for fu-

ture research.

Evaluating Commit Message Generation: To BLEU Or Not To BLEU?

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

REFERENCES
[1] 2021.

Log-MNEXT: A Metric

https://github.com/CMGeval/Evaluating-CMG.
2021].

for Evaluating Commit Messages.
[Online; accessed 15-Oct-

[2] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation with improved correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65â€“72.

[3] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text

generation: A survey. arXiv preprint arXiv:2006.14799 (2020).

[4] Shweta Chauhan, Philemon Daniel, Archita Mishra, and Abhay Kumar. 2021.
AdaBLEU: A Modiï¬ed BLEU Score for Morphologically Rich Languages. IETE
Journal of Research (2021), 1â€“12.

[5] Boxing Chen and Colin Cherry. 2014. A systematic comparison of smoothing
techniques for sentence-level bleu. In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation. 362â€“367.

[6] Michael Denkowski and Alon Lavie. 2010. METEOR-NEXT and the METEOR
Paraphrase Tables: Improved Evaluation Support For Five Target Languages. In
Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation
and Metrics MATR.

[7] Lifeng Han. 2016. Machine translation evaluation resources and methods: A

survey. arXiv preprint arXiv:1605.04515 (2016).

[8] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diï¬€s using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135â€“146.

[9] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation.
In Proceedings of the 45th annual meeting of the association for computational
linguistics companion volume proceedings of the demo and poster sessions. 177â€“
180.

[10] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.

In Text summarization branches out. 74â€“81.

[11] Zhongxin Liu, Xin Xia, Ahmed E. Hassan, David Lo, Zhenchang Xing, and
Xinyu Wang. 2018. Neural-Machine-Translation-Based Commit Message Gen-
eration: How Far Are We?. In Proceedings of the 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineering (Montpellier, France) (ASE

2018). Association for Computing Machinery, New York, NY, USA, 373â€“384.
https://doi.org/10.1145/3238147.3238190

[12] Pablo Loyola, Edison Marrese-Taylor, and Yutaka Matsuo. 2017. A neural archi-
tecture for generating natural language descriptions from source code changes.
arXiv preprint arXiv:1704.04856 (2017).

[13] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Eï¬€ec-
tive approaches to attention-based neural machine translation. arXiv preprint
arXiv:1508.04025 (2015).

[14] Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU:
Reevaluating the Evaluation of Automatic Machine Translation Evaluation Met-
rics. In Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational Linguistics, Online, 4984â€“4997.
https://doi.org/10.18653/v1/2020.acl-main.448

[15] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A Method for Automatic Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computational Linguistics (Philadel-
phia, Pennsylvania) (ACL â€™02). Association for Computational Linguistics, USA,
311â€“318. https://doi.org/10.3115/1073083.1073135

[16] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computational Linguistics. 311â€“318.

[17] Ehud Reiter. 2018. A Structured Review of the Validity of BLEU. Computa-
tional Linguistics 44, 3 (09 2018), 393â€“401. https://doi.org/10.1162/coli_a_00322
arXiv:https://direct.mit.edu/coli/article-pdf/44/3/393/1809172/coli_a_00322.pdf
[18] Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. 2020. A survey
of evaluation metrics used for NLG systems. arXiv preprint arXiv:2008.12009
(2020).

[19] Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Association for Machine Translation in
the Americas: Technical Papers. 223â€“231.

[20] Wei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Hongyu Zhang, Dongmei Zhang,
and Wenqiang Zhang. 2021. On the Evaluation of Commit Message Generation
Models: An Experimental Study. arXiv preprint arXiv:2107.05373 (2021).

[21] Jerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics 7

(2005).

