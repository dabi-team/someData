2
2
0
2

r
a

M
8
2

]

G
L
.
s
c
[

1
v
1
3
3
6
1
.
3
0
2
2
:
v
i
X
r
a

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING
PROBABILISTIC AUTOMATA

SICCO VERWER AND CHRISTIAN HAMMERSCHMIDT

e-mail address: s.e.verwer@tudelft.nl, chris@apta.tech

Intelligent Systems Department, Delft University of Technology, the Netherlands

Abstract. We present the eﬃcient implementations of probabilistic deterministic ﬁnite
automaton learning methods available in FlexFringe. These implement well-known strategies
for state-merging including several modiﬁcations to improve their performance in practice.
We show experimentally that these algorithms obtain competitive results and signiﬁcant
improvements over a default implementation. We also demonstrate how to use FlexFringe to
learn interpretable models from software logs and use these for anomaly detection. Although
less interpretable, we show that learning smaller more convoluted models improves the
performance of FlexFringe on anomaly detection, outperforming an existing solution based
on neural nets.

1. Introduction

We introduce the probabilistic deterministic ﬁnite state automaton (PDFA) learning learning
methods implemented in the FlexFringe automaton learning package. FlexFringe orig-
inated from the DFASAT [HV10] algorithm for learning non-probabilistic deterministic
ﬁnite automata (DFA) and is based on the well-known red-blue state merging frame-
work [LPP98]. Learning automata from trace data [CW98] has been used for analyzing
diﬀerent types of complex software systems such as web-services [BIPT09, ISBF07], X11 pro-
grams [ABL02], communication protocols [CWKK09, ANV11, FBLP+17, FBJM+20], Java
programs [CBP+11], and malicious software [CKW07, CKW07]. A great beneﬁt that state
machines provide over more traditional machine learning models is that software systems
are in essence state machines. Hence, these models give unparalleled insight into the inner
workings of software and can even be used as input to software testing techniques [LY96].

Learning state machines from traces can be seen as a grammatical inference [DlH10] prob-
lem where traces are modeled as the words of a language, and the goal is to ﬁnd a model for
this language, e.g., a (probabilistic) deterministic ﬁnite state automaton [HMU01]. Although
the problem of learning a (P)DFA is NP-hard [Gol78] and hard to approximate [PW93],
state merging is an eﬀective heuristic method for solving this problem [LPP98]. This method
starts with a large tree-shaped model, called the preﬁx tree, that directly encodes the input
traces. It then iteratively combines states by testing the similarity of their future behaviours
using a Markov property [NN98] or a Myhill-Nerode congruence [HMU01]. This process

Key words and phrases: Probabilistic Automata, State Merging, Machine Learning, Software Modeling.

Preprint submitted to
Logical Methods in Computer Science

© S.Verwer and C.Hammerschmidt
CC(cid:13) Creative Commons

 
 
 
 
 
 
2

S.VERWER AND C.HAMMERSCHMIDT

Figure 1: A preﬁx tree printed by FlexFringe and displayed using Graphviz dot. Each state
contains occurrence counters for the total number of traces, as well as how many
of them end (ﬁn counts) or pass through (path counts) for each trace type. You
can use these to infer what traces occurred in the training data, e.g., a-a-a-b-b
occurred 3 times, following transitions from state 0-1-2-4-9 and ending in state 17.
The transitions are labeled by symbols and occurrence counts. The initial state
has a solid edge, indicating it has already been learned as an automaton state.
The dotted states can still be merged with both solid and dotted states.

continues until no similar states can be found. The result is a small model displaying the
system states and transition structure hidden in the data. Figure 1 shows the preﬁx tree for
a small example data set consisting of 20 sequences starting with an “a” event and ending
in a “b” event. Running FlexFringe results in the PDFA shown in Figure 2.

root0:#20ﬁn:  path: 1:20 , 20 01:#20ﬁn:  path: 1:20 , 20 0a20 2:#10ﬁn:  path: 1:10 , 10 0a10 3:#10ﬁn:  path: 1:10 , 10 0b10 4:#5ﬁn:  path: 1:5 , 5 0a5 5:#5ﬁn:  path: 1:5 , 5 0b5 6:#4ﬁn:  path: 1:4 , 4 0a4 7:#6ﬁn:  path: 1:6 , 6 0b6 8:#2ﬁn:  path: 1:2 , 2 0a2 9:#3ﬁn:  path: 1:3 , 3 0b3 10:#3ﬁn:  path: 1:3 , 3 0a3 11:#2ﬁn:  path: 1:2 , 2 0b2 12:#3ﬁn:  path: 1:3 , 3 0a3 13:#1ﬁn:  path: 1:1 , 1 0b1 14:#3ﬁn:  path: 1:3 , 3 0a3 15:#3ﬁn:  path: 1:3 , 3 0b3 16:#2ﬁn: 1:2 ,  path: 0 2b2 17:#3ﬁn: 1:3 ,  path: 0 3b3 18:#3ﬁn: 1:3 ,  path: 0 3b3 19:#2ﬁn: 1:2 ,  path: 0 2b2 20:#3ﬁn: 1:3 ,  path: 0 3b3 21:#1ﬁn: 1:1 ,  path: 0 1b1 22:#3ﬁn: 1:3 ,  path: 0 3b3 23:#3ﬁn: 1:3 ,  path: 0 3b3 FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 3

Figure 2: A PDFA printed after running FlexFringe. It contains the same type of counts as
the preﬁx tree. To obtain a PDFA from these counts, one simply needs to normalize
them. Traces only end in the third state, as indicated by the ﬁn counts. The
learned PDFA correctly represents the set of traces starting with “a” and ending
in “b”, i.e., a(a|b)∗b. A learned PDFA can be used for assigning probabilities to
new sequences by following transitions and multiplying their probabilities. It can
also be used for anomaly detection, for instance by checking whether a new trace
ends in a state with ﬁn counts greater than 0 (a ﬁnal state).

The key design principle of FlexFringe is that this state merging approach can be used
to learn models for a wide range of systems, including Mealy and i/o machines [SG09,
AV10], probabilistic deterministic automata [VTDLH+05, VEDLH14], timed/extended au-
tomata [Ver10, NSV+12, NSV+12, SK14, WTD16], and regression automata [LHPV16]. All
that needs to be modiﬁed is the similarity test, implemented as an evaluation function that
tests merge consistency and computes a score function to decide between possible consistent
merges. In FlexFringe, these functions can be implemented and modiﬁed by adding only a
single ﬁle (the evaluation function) to the code base. This ﬁle needs to implement:

• methods for processing evaluation function speciﬁc inputs such as time values,
• data structures that contain information such as frequency counts,
• methods that compute merge consistency and merge score from these structures,
• and updating routines for modifying their content due to a performed merge.

These functions can be derived and overloaded from existing evaluation functions. FlexFringe
then provides eﬃcient state-merging routines that determine which merge to perform, how
this inﬂuences the automaton structure, and diﬀerent ways to prioritize and search through
the possible merges. This process can be ﬁne-tuned using several parameters.

With FlexFringe, we aim to make eﬃcient state-merging algorithms accessible to a wide
range of users. Currently, most users are from the software engineering domain because they
frequently deal with data generated by deterministic systems. In practice, data is usually
unlabeled since one can only observe what a (software) system does and not what is does not
do. Most applications of state merging therefore make use of some form of PDFA learning.
We have implemented several evaluation functions in FlexFringe for well-known PDFA
learning algorithms such as Alergia [CO94], MDI [Tho00], Likelihood-ratio [VWW10], and
AIC minimization [Ver10]. In this paper, we describe these methods and improvements we
developed to boost their performance in practice. FlexFringe also contains several learning
algorithms for probabilistic and non-probabilistic automata, including mealy machines and
real-time variants that can be applied without modiﬁcation to many diﬀerent problems. In

0:#20ﬁn:  path: 1:20 , 20 01:#50ﬁn:  path: 1:50 , 50 0a20 Ia20 2:#50ﬁn: 1:20 ,  path: 1:30 , 30 20b30 a10 b20 4

S.VERWER AND C.HAMMERSCHMIDT

this work, our focus is on the PDFA learning capabilities implemented in FlexFringe. We
show the following contributions:
• Eﬃcient implementations of existing state-merging algorithms.
• Improvements over the traditional implementations that increase performance in practice.
• Experiments on the PAutomaC data set [VEDLH14] displaying competitive results.
• A use-case on software logs from the HDFS data set [XHF+09], where we demonstrate
how to learn insightful models and outperform existing solutions based on neural nets.

This paper is organized as follows. We start with an overview of PDFAs and the
state-merging algorithm in Sections 2 and 3, including a description of the eﬃcient data
structures used in FlexFringe. We then describe the implemented PDFA evaluation functions
in Section 4 and the developed improvements in Section 5. The results obtained using the
diﬀerent diﬀerent evaluation functions on the PAutomaC competition data sets in Section 6.
For software log data, we show the performance on HDFS, both from an obtained insight and
performance perspective in Section 7. We provide an overview of closely related algorithms
and tools in Section 8, and end with some concluding remarks in Section 9.

2. Probabilistic Automata

Automata or state machines are models for sequential behavior. Like hidden Markov models,
they are models with hidden/latent state information. Given a string (trace) of observed
symbols (events) a1, . . . , an, this means that the corresponding system states q0, q1, . . . , qn
are unknown/unobserved. In deterministic automata, we assume that there exists a unique
start state q0 and that there are no unobserved events (no (cid:15)-transitions). Furthermore,
given the current system state qi and the next event ai+1 there is a unique next state qi+1.
This implies that deterministic automata can be in exactly one state at every time step.
Although hidden Markov models do not have this restriction, it is well-known that automata
can be transformed into hidden Markov models and vice versa [DDE05]. Determinism
can be important when modeling sequential behavior because the resulting models are
frequently easier to interpret and computation of state sequences and probabilities is much
more eﬃcient.

Deﬁnition 2.1. A probabilistic deterministic ﬁnite state automaton (PDFA) is a tuple
A = {Σ, Q, q0, δ, S, F }, where Σ is a ﬁnite alphabet, Q is a ﬁnite set of states, q0 ∈ Q is a
unique start state, δ : Q × Σ → Q ∪ {0} is the transition function, S : Q × Σ → [0, 1] is the
symbol probability function, and F : Q → [0, 1] is the ﬁnal probability function, such that
F (q) + (cid:80)

a∈Σ S(q, a) = 1 for all q ∈ Q.

Given a sequence of observed symbols s = a1, . . . , an, a PDFA A can be used as a
function to assign/compute probabilities A(s) = S(q0, a1) · S(q1, a2) . . . S(qn−1, an)F (qn),
where δ(qi, ai+1) = qi+1 for all 0 ≤ i ≤ n − 1. A PDFA is called probabilistic because it
assigns probabilities based on the symbol S and ﬁnal F probability functions. It is called
deterministic because the transition function δ (and hence its structure) is deterministic. The
transition function is extended with a null symbol, representing that the transition does not
exist, thus a PDFA model can be incomplete. A PDFA computes a probability distribution
over Σ∗, i.e., (cid:80)
s∈Σ∗ A(s) = 1. A PDFA can also be deﬁned without ﬁnal probability function
F , in that case it computes probability functions over Σn, i.e., (cid:80)
s∈Σn A(s) = 1 for all 1 ≤ n.
PDFAs are excellent models for systems that display deterministic behavior such as
software. PDFAs and their extensions have frequently been used to model tasks in both

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 5

software and cyber-physical systems, see, e.g., [NVMY21, KABP14, WRSL21, HMS+16]
and [LVD20, SK14, LAVM18, Mai14, PLHV17]. In addition to their ability to compute
probabilities and predict future symbols, their deterministic nature provides insight into
a system’s inner working and can even be used to fully reverse engineer a system from
observations [ANV11].

3. State merging in FlexFringe

Given a ﬁnite data set of example sequences D called the input sample, the goal of PDFA
learning (or identiﬁcation) is to ﬁnd a (non-unique) small PDFA A that is consistent with
D. We call such sequences positive or unlabeled. In contrast, DFAs are commonly learned
from labeled data containing both positive and negative sequences. PDFA size is typically
measures by the number of states (|Q|) or transitions (|{(q, a) ∈ Q × Σ : δ(q, a) (cid:54)= 0}|).

Consistency is tricky to deﬁne in when dealing with only positive data and most methods
deﬁne restrictions on merging steps performed in the algorithm. Intuitively, a merge step
concludes that for two states q and q(cid:48), the future sequences in D that occur after reaching
them are similarly distributed. Since q and q(cid:48) can be reached by diﬀerent past sequences,
this boils down to a type of test for a Markov property: the future is independent from the
A(sq(cid:48) z)
past given the current state, i.e., A(sqz)
q are
A(sq(cid:48) )
preﬁxes ending in state q and q(cid:48), respectively. States in a PDFA can thus be thought of
as clusters of preﬁxes with similarly distributed suﬃxes. The restriction to a deterministic
model in addition requires that if sq and sq(cid:48) are clustered, then sqz and sq(cid:48)z are clustered as
well for all z ∈ Σ∗, called the determinization constraint.

for all suﬃxes z ∈ Σ∗, where sq and s(cid:48)

A(sq) ≈

Finding a small and consistent PDFA is a well-known hard problem and an active
research topic in the grammatical inference community, see, e.g. [DlH10]. One of the most
successful state merging approaches for ﬁnding a small and consistent (P)DFA is evidence-
driven state-merging in the red-blue framework (EDSM) [LPP98]. FlexFringe implements
this framework making use of ﬁnd/union structures to keep track of and undo the performed
merges, see Figure 3. Like most state merging methods, FlexFringe ﬁrst constructs a
tree-shaped PDFA A known as a preﬁx tree from the input sample D, see Figure 1. and
then iteratively merges the states of A. Initially, since every preﬁx leads to a unique state,
A is consistent with D. A merge (see Algorithm 1 and Figures 2 and 4 to compare models
before and after merging state 2 with state 1) of two states q and q(cid:48) combines the states into
one by setting the representative variable form the ﬁnd/union structure of q(cid:48) to q. After
this merge, whenever a computation of a sequence returns q(cid:48), it returns the representative
q instead of q(cid:48). A merge is only allowed if the states are consistent, determined using a
statistical test or distance computation based on their future sequences. A unique feature of
FlexFringe is that users of the algorithm are able to implement their own test or distance by
adding a single ﬁle, called the evaluation function, to the source code. It is even possible to
add additional attributes such as continuous sensor reading to input symbols and use these
in a statistical test to learn regression automata [LHPV16].

When a merge introduces a non-deterministic choice, i.e., both δ(q, a) and δ(q(cid:48), a) are
non-zero, the target states of these transitions are merged as well. This is called the
determinization process (the for loop in Algorithm 1). Although a user can inﬂuence the
type of model by deﬁning their own evaluation function and type of input data, determinism
is required and used at several places to speed up the computations, i.e., FlexFringe cannot

6

S.VERWER AND C.HAMMERSCHMIDT

Figure 3: The union/ﬁnd data structure in FlexFringe after performing the ﬁrst merge
operation (state 2, reached by a-a, is merged with state 1, creating a self-loop. We
remove the ﬁn and path counts for clarity. The unmerged part of the PDFA is
solid, the merged parts are dotted. The arc labeled “rep” are the representative
pointers for the union/ﬁnd structure. Whenever a state with a representative is
queried by the algorithm, the structure follows the “rep” pointers until it ﬁnds a
state without a representative and returns this one instead. Thus, when looking
for the target of the transition with label “a” from state 1, it will return state 1
(the representative of state 2). States with representatives cannot be merged, but
it is possible to merge states that are the representative of others, such as state 3.

be used to learn non-deterministic automata. The result of a single merge is a PDFA that is
smaller than before (by following the representative values), and still consistent with the
input sample S as speciﬁed by the evaluation function. FlexFringe continuously applies this
state merging process until no more consistent merges are possible. The order in which
these merges are performed can be inﬂuenced by setting parameter values.

Algorithm 1 Merging two states: merge (A, q, q(cid:48))
Require: PDFA A and two states q, q(cid:48) from A
Ensure: merge q and q(cid:48) if consistent and return true, return false otherwise

if consistent(q, q(cid:48)) is false return false
set representative(q(cid:48)) = q, update score(q, q(cid:48)), add counts of q(cid:48) to q
for all a ∈ Σ do

if δ(q(cid:48), a) (cid:54)= 0 then

if δ(q, a) (cid:54)= 0 call merge(A(cid:48), δ(q, a), δ(q(cid:48), a))
else set δ(q, a) = δ(q(cid:48), a)

end if
end for
if any merge returned false: return false
else return true

3.1. The red-blue framework. The successful red-blue framework [LPP98] follows the
state-merging algorithm just described, which adds colors (red and blue) to the states to

root0:#201:#34a20 2:#10a17 3:#20b20 rep4:#5a5 5:#5b5 6:#7a7 7:#11b11 rep8:#2a2 9:#3b3 rep10:#3a3 11:#2b2 12:#3a3 13:#4b4 14:#3a3 15:#5b5 rep16:#2b2 rep17:#3b3 rep18:#3b3 rep19:#2b2 20:#3b3 21:#1b1 22:#3b3 23:#3b3 reprepreprepFLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 7

Figure 4: The red-blue framework corresponding to the ﬁnd/union sets from Figure 4. The
red states are the identiﬁed parts of the automaton. The blue states are the
current candidates for merging. The uncolored states are pieces of the preﬁx tree
that can only be merged during determinization. Currently only state 3 is a merge
candidate. The merges of state 3 with 0 and state 3 with 1 will be tested by
FlexFringe. If consistent, the highest scoring merge will be performed. If both
inconsistent, state 3 will be colored red, state 6 and 7 will be colored blue.

guide the merge process. The framework maintains a core of red states with a fringe of blue
states (see Figure 4 and Algorithm 2). A red-blue algorithm performs merges only between
blue and red states (although FlexFringe has a parameter for allowing blue-blue merges).
When there exists a blue state for which no consistent merge is possible, the algorithm
changes the color of this blue state into red, eﬀectively identifying a new state in the (P)DFA
model. The red core of the (P)DFA can be viewed as a part of the (P)DFA that is assumed
to be correctly identiﬁed. Any non-red state q(cid:48) for which there exists a transition δ(q, a) = q(cid:48)
from a red state q, is colored blue. The blue states are merge candidates. A red-blue
state-merging algorithm is complete since it is capable of producing any (P)DFA that is
consistent with the input sample and smaller than the original preﬁx tree. Furthermore, it
is more eﬃcient than standard state-merging since it considers a lot less merges. Note that
undo merge (c.f. Algorithm 2) is highly eﬃcient when using ﬁnd/union structures because
only the representative variables (pointers) need to be reset to their original values and
counts updated.

In the grammatical inference community, there has been much research into improving
merging in the red-blue framework. Initially state merging algorithms [OG92, CO94] used
an order that colors shallow states ﬁrst, i.e., those closer to the start state or root of the
preﬁx tree. In [Lan99], it was instead suggested to follow a more constrained state ﬁrst
order. In FlexFringe, we typically use a largest ﬁrst order, which colors the most frequent
states ﬁrst. Although other orders are implemented. Also several search strategies have been
studied such as dependency directed backtracking [OS98], using mutually (in)compatible
merges [ACS04], iterative deepening [Lan99], beam-search [BO05], and sat-solving [HV10].
Most of these have been studied when learning DFA classiﬁers, i.e., when both positive and
negative data is available. When learning PDFAs, search strategies have been less studied.
Most PDFA learning algorithm rely on greedy procedures, some with PAC-bounds that

root0:#201:#34a20 a17 3:#20b20 6:#7a7 7:#11b11 12:#3a3 13:#4b4 14:#3a3 15:#5b5 20:#3b3 21:#1b1 22:#3b3 23:#3b3 8

S.VERWER AND C.HAMMERSCHMIDT

Algorithm 2 State-merging in the red-blue framework
Require: a data set D
Ensure: A is a small PDFA that is consistent with D

construct the preﬁx tree A from D
color the start state of A red and all of its children blue
while A contains blue states do
for every blue state q(cid:48) do
for every red state q do

call consistent = merge(A, q, q(cid:48)), compute the merge score
call undo merge(A, q, q(cid:48))

end for

end for
if one of the merges is consistent then

perform the consistent merge(A, q, q(cid:48)) with highest score

else

color the ﬁrst blue state in a given order red

end if
color all children of red states in A blue

end while
return A

guarantee performance when suﬃcient data is available [CG08, CT04, BCG13]. There exist
some works that minimize Akaike’s Information Criterion (AIC) [Ver10], or a Minimum
In FlexFringe, currently a best-ﬁrst beam-search
Description Length measure [AV07].
strategy is implemented that minimizes the AIC.

4. Implemented evaluation functions

Given a potential merge state pair (q, q(cid:48)), an evaluation function has to implement a
consistency check and a score. As mentioned above, the consistency check is used to
determine whether a merge is feasible. The score evaluation should provide the means to
determine from a set of potential merges, which one to perform ﬁrst. In FlexFringe, both of
these functions are user deﬁned. For learning PDFAs, FlexFringe includes several well-known
consistency checks, which we describe below along with their score computation.

4.1. Alergia. Alergia [CO94] is one of the ﬁrst and still a very successful algorithm for
learning PDFAs. It relies on a test derived from the Hoeﬀding bound to determine merge
consistency checks. For each potential merge pair (q, q(cid:48)), it tests for all a ∈ Σ whether

(cid:12)S(q, a) − S(q(cid:48), a)(cid:12)
(cid:12)

(cid:12) <

(cid:32)

(cid:114) 1
2

ln

2
α

1
(cid:112)C(q)

+

1
(cid:112)C(q(cid:48))

(cid:33)

where S is the symbol probability function, C(q) is a function returning the frequency
count of state q, and α is a user-deﬁned parameter. When using ﬁnal probabilities, the ﬁnal
probability function F is used in the same way as the symbol probability function S. This
also holds for the other evaluation functions. The Alergia check guarantees that for every
pair of merged states, the outgoing symbol distributions are not signiﬁcantly diﬀerent. In

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 9

the original Alergia paper, the state merging algorithm does not implement a score. Instead,
it deﬁnes a search order and iteratively performs the ﬁrst consistent merge in this order. In
FlexFringe, Alergia is implemented with the default score of summing up the diﬀerences
between left-hand and right-hand sides of the above equation over all pairs of tested states.
The larger these diﬀerences, the more similar are the distributions. In addition, this score
prefers merges that merge many states during determinization. The intuition is that for
every performed merge a consistency check is performed. Hence, we are more certain of
merges that merge more states.

4.2. Likelihoodratio. A likelihood-ratio test is introduced in [VWW10] to overcome a
possible weakness of Alergia. In Alergia, each pair of merged states is tested independently.
When the determinization merges hundreds of states, then we should not be surprised that
a small number of these tests fail. This prevents states from merging, resulting in a larger
PDFA. The likelihood-ratio test aims to overcome this by computing a single test for the
entire merge procedure, including determinization. It compares the PDFA before the merge
0, δ(cid:48), S(cid:48), F (cid:48)}, computing
A = {Σ, Q, q0, δ, S, F } to the PDFA after the merge A(cid:48) = {Σ, Q(cid:48), q(cid:48)
their likelihood and number of parameters, i.e., the number of transitions. Because the two
models are nested (A(cid:48) is a restriction/grouping of A), we can compute a likelihood-ratio
test to determine whether parameter reduction outweighs the decrease in likelihood. When
it does, the merge is considered consistent. The function it computes is:



χ2

2

(cid:88)

q∈Q,a∈Σ

C(q, a) log(S(q, a)) − 2

(cid:88)

q(cid:48)∈Q(cid:48),a∈Σ

C(q(cid:48), a) log(S(cid:48)(q(cid:48), a)), |A| − |A(cid:48)|

 > α



where α is a user-deﬁned parameter (conﬁdence threshold), χ2(v, n) is the value of v
in the chi-squared distribution with n degrees of freedom, C(q, a) is the frequency count of
symbol a in state q, and |A| is the number of transitions in A. In the case that C(q, a) equals
0, C(q, a) log(S(q, a)) is set to 0. In FlexFringe, this function is computed incrementally by
tracing which parameters get removed during a merge and its eﬀect on the loglikelihood.
As score, likelihood-ratio uses 1 minus the p-value obtained from the χ2 function. A larger
score indicates that the decrease in likelihood is less signiﬁcant, i.e., that the distributions
models by A and A(cid:48) are more similar.

4.3. MDI. The MDI algorithm [Tho00] is an earlier approach to overcome possible weak-
nesses of Alergia, mainly that there is no way to bound the distance of the learned PDFA
from the data sample. Like likelihood-ratio, MDI computes the likelihood and the num-
ber of parameters. Instead of comparing these directly using a test, MDI uses them to
compute the Kullback-Leibler divergence from the models before merging A and after
merging A(cid:48) to the distribution in the original data sample D, i.e., that of the preﬁx tree
Ap = {Σ, Qp, qp,0, δp, Sp, Fp}. When a merge makes this distance too large, it is considered
inconsistent:

(cid:16)(cid:80)

(cid:17)
q∈Qp,a∈Σ Cp(q, a)Sp(q, a) · (log(S(ˆq, a)) − log(S(cid:48)(ˆq(cid:48), a)))
|A| − |A(cid:48)|

< α

10

S.VERWER AND C.HAMMERSCHMIDT

where Cp(q, a) is the frequency count of symbol a in state q in Ap, ˆq and ˆq(cid:48) are the
states that q in merged with in A and A(cid:48) respectively. As before, and α is a user-deﬁned
parameter. The rest is identical to the other evaluation functions, and hence is inherited from
the likelihoodratio implementation. For eﬃciency reasons, our implementation is slightly
diﬀerent from the original formulation in [Tho00]. We use the counts from D to compute the
Kullback-Leibler divergence instead of computing it directly between the diﬀerent models.

4.4. AIC. The AIC or Akaike’s Information Criterion is a commonly used measure for
evaluating probabilistic models. It is a simple yet eﬀective model selection method that like
the two above evaluation functions makes a trade-oﬀ between the number of parameters
and likelihood in a model. It is very similar to the likelihood-ratio function but does not
rely on the χ2 function. It simply aims to minimize the number of parameters minus the
loglikelihood, all merges that decrease the AIC are consistent:

2 (cid:0)|A| − |A(cid:48)|(cid:1) − 2





(cid:88)

q∈Q,a∈Σ

C(q, a) log(S(q, a)) −



C(q(cid:48), a) log(S(cid:48)(q(cid:48), a))

 > 0

(cid:88)

q(cid:48)∈Q(cid:48),a∈Σ

Intuitively, this measures whether the reduction in parameters when going from A
to A(cid:48) is greater than the decrease in loglikelihood. Like all other evaluation functions,
FlexFringe computes this incrementally and only for the states that get merged during the
determinization procedure and the merge itself. In addition to its eﬃcient implementation
and ﬂexibility, FlexFringe introduces several techniques that improve both run-time and
performance in state merging.

5. Improvements in Speed and Performance

When implemented exactly as stated above, the above evaluation functions work for states
that are suﬃciently frequent. When merging infrequent states, however, they can give
bad performance. For instance, Alergia will nearly always merge infrequent states as for
transitions that occur only a handful of times will never provide suﬃcient evidence to
create an inconsistency. As a result, these merges are somewhat arbitrary and can hurt
both performance and the insight you can get the learned models. FlexFringe therefore
implements several techniques that deal with low frequency states and transitions.

5.1. Sinks. Sinks are states with user-deﬁned conditions that are ignored by the merging
algorithm. The idea of using sinks originated from DFASAT in the Stamina challenge
[WLD+13, HV13]. In the competition, data was labeled and a garbage state was needed for
states that are only reached by negative sequences. Merging such states with the rest of the
automaton and combining negative and positive sequences can only lower performance. In
PDFAs, the default condition deﬁnes sinks as states that are reached less than sink count (a
user-deﬁned parameter) times by sequences from the input data D. In FlexFringe’s merging
routines, sinks are never considered as states in a merge pair, i.e., blue states that are sinks
are ignored, but they are merged normally during determinization. Due to determinization,
sinks can become more frequent and thus still be merged in a subsequent iteration. Using
sinks, the merging routines continues until all remaining merge candidate states are sinks.
By default, these sinks and their future states are not output to the automaton model.

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 11

There are options to add these to the model, or to continue merging them in diﬀerent ways
(e.g., with red states, or only with other sinks).

5.2. Pooling. Frequency pooling is a common technique to improve the reliability of
statistical tests when faced with infrequent symbols/events/bins. The idea is to combine the
frequency of infrequent symbols to avoid small frequencies and thus gain conﬁdence in the
outcome of statistical tests. When learning PDFAs, frequency pooling is very important as
the majority of states that are merged during determinization are infrequent. Every blue
state that is considered for merging is the root of a preﬁx tree with frequent states near the
root, and infrequent states in all of its branches. Pooling can be quite straightforward, simply
combine the counts of symbols that occur less that a user-deﬁned threshold in either states
of the merge pair. We noticed, however, that this strategy can miss obvious diﬀerences:

c
10 0
5
5
Although states q and q(cid:48) are diﬀerent, the pooled counts (with threshold t = 6) show that
they are identical. Similar examples exist for combining the counts of symbols that occur
less than a threshold in both merge pairs:

b
a
q
5
5
q(cid:48) 0 10

d pool
20
20

a b c d e
f
5 5 5 5 0 0
q
q(cid:48) 0 0 5 5 5 5

pool
20
20

This creates problems for state merging as the algorithm will consider merges consistent,
which by merging can aﬀect the frequency counts in other states. In FlexFringe, we therefore
opted to perform a diﬀerent pooling strategy, with the aim of not hiding these diﬀerences.
We build two pools:

b
a
5
5
q
q(cid:48) 0 10

c
10 0
5
5

d pool1 pool2

10
15

15
10

The ﬁrst pool contains all frequency counts that occur less the threshold in q (counts
for a, b, and d), the second those in q(cid:48) (counts for a, c, and d). Thus, the counts that occur
infrequently in both states are added to both pools. The parameter in FlexFringe used for
this threshold setting is symbol count. In addition, the statistical tests in FlexFringe
ignore states that have an occurrence frequency lower than state count. This means that
during determinization, whenever one of the two states that are being merged is infrequent, it
still performs the merge but does not compute a statistical test. This is especially important
for the likelihood ratio tests, which can be inﬂuenced by a large amount of infrequent states
when merged during determinization. Using the state count parameter, these counts are
not added to the likelihood value, and the size reduction (number of parameters) is also
not taken into account. FlexFringe also uses a Laplace smoothing by adding correction
counts to every frequency count after pooling.

5.3. Counting parameters. Many evaluation functions require the counting of statistical
parameters before and after a merge. Since the models before and after a merge are nested,
we can compute powerful statistical tests such as the likelihood ratio. A problem is however
how to compute the amount of parameters reduced by performing a merge. Because a merge

12

S.VERWER AND C.HAMMERSCHMIDT

can combine many states during determinization, counting one parameter more or less for
each state greatly inﬂuences the resulting automaton model.

Each state contains statistical parameters for estimating the symbol probability distri-
butions. Essentially one for every possible symbol. Since automata are frequently sparse in
practice, it makes little sense to include parameters for symbols that do not occur. Counting
parameters for these would give a huge preference to merging as every pair of merged states
reduces this amount by the size of the alphabet |Σ|. Instead, we opted to count an additional
parameter only for symbols that have non-zero counts in both states before merging. In
other words, we count every transition as a parameter. This implies that we measure the size
of a PDFA by counting the number of transitions instead of the more commonly measure of
counting the number of states.

5.4. Merge constraints. In addition to ways to deal with low frequency counts and symbol
sparsity, FlexFringe contains several parameters that inﬂuence which merges are considered.
For PDFAs, one of the the most important parameters is largestblue. When set to true,
FlexFringe only considers merges with the most frequently occurring blue state. This greatly
reduces run-time because instead of trying all possible red-blue merge pairs (quadratic),
it only considers merges between all red states with a single blue state (linear). In our
experience, it also improves performance, as merging the most frequent states ﬁrst simply
makes sense when testing consistency using statistical tests. Also important is finalprob.
When set to true, it causes FlexFringe to model ﬁnal probabilities (learning distributions
over Σ∗ instead of Σn). This setting should always be used when the ending of a sequences
contains information, e.g., not when learning from sliding windows.

When learning PDFAs, there are several other parameters that can be useful to try.
Firstly, finalred makes sure that merges cannot add new transitions to red states, when
they do they are considered inconsistent. The key idea is that the red states are already
learned/identiﬁed, and we should therefore not modify their structure. Secondly, blueblue
allows merges between pairs of blue states in addition to red-blue merges. Although state
merging in the red-blue framework is complete in the sense that it can return any possible
automaton, sometimes it can force a barely consistent merge. Allowing blue-blue merge pairs
can avoid such merges. Thirdly, markovian creates a Markov-like constraint. It disallows
merges between states with diﬀerent incoming transition labels when set to 1. When set to
2 (or 3, ...), it also requires their parents (and their parents, ...) to have the same incoming
label. When running likelihood-ratio with a very low statistical test threshold (or negative),
and markovian set to 1, it creates a Markov chain. With a larger setting, it creates an
N-Gram model. Combined with a statistical consistency check, it creates a deterministic
version of a labeled Markov chain. Finally, FlexFringe also implements the well-known
kTails algorithm for learning automata often used in software engineering [BF72], i.e., only
taking futures sequences up to length k into account, which can be accessed using the ktail
parameter.

5.5. Searching. Much of the eﬃciency in FlexFringe is achieved by making use of a
ﬁnd/union data structure, which allows to quickly perform and undo merges. The major-
ity of the time is typically spend on reading, writing, and updating the data structures
maintained by the evaluation function. This allows for search routines that try diﬀerent
merge paths in order to ﬁnd one that minimizes a global objective. We have implemented

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 13

a simple best-ﬁrst beam-search strategy similar to ed-beam [BO05]. Since it is based on
DFASAT [HV10], FlexFringe also contains the translation of state-merging to satisﬁability,
including the speedup managed by including best-ﬁrst search constraints [UZS15]. Currently,
the translation only works for classiﬁers, i.e., when there is both positive and negative data.

6. Results on PAutomaC

To demonstrate the value of the improvements made to general state merging algorithms
in FlexFringe, we run each of the evaluation functions on the PAutomaC problem set.
PAutomaC was a competition on learning probability distributions over sequences held in
2012 [VEDLH14]. In the competition data there are 48 data sets with varying properties
such as the type of automaton/model that was used to generate the sequences, the size of
the alphabet, and the sparsity/density of transitions. For evaluation, a test set is provided
of unique traces. The task was to assign probabilities to these traces. For evaluation, the
assigned probabilities were compared to the ground truth (probabilities assigned by the
model that generated the data) using a perplexity metric:

Score(C, T ) = 2− (cid:80)

x∈T PT (x)∗log(PC (x))

where PT (x) is the normalized probability of x in the target and PC(x) is the normalized
candidate probability for x submitted by the participant. The perplexity score measured
how well the diﬀerences in the assigned probabilities matched with the target probabilities
assigned by the ground truth model.

To avoid 0 probabilities in PT (X), we use Laplace smoothing with an correction of 1.
We compare the performance of FlexFringe using diﬀerent heuristics and parameters to
the PAutomaC winner (a Gibbs sampler by Shibata-Yoshinaka), and the best performing
state merging method by team Llorens). We ﬁrst demonstrate the eﬀectiveness of sinks, low
frequency counts, and other improvements using Alergia.

6.1. Alergia improvements. The results are given in Table 1. We ﬁrst run Alergia as
written in the 1994 seminal paper [CO94]. Out-of-the-box (column Alergia94), this performs
not very well and a key reason for this is the eﬀect of low frequency counts on the consistency
test and the resulting bad merges. When we change the shallow-ﬁrst merge order into
largest-blue, the performance improves. Adding sinks also improves the performance, and
also the run-time. We use a sink count of 25, which causes FlexFringe to complete the full
set of PAutomaC training ﬁles in 20 minutes on a single thread at 2.6 GHz. We did not
tune the threshold parameter and kept it at its default value of 0.01.

The results become competitive when running Alergia with our new pooling strategy
(Column Alergia+). We use a state count of 15, and a symbol count of 10. Note that state
count has to be lower than sink count, otherwise FlexFringe starts to merge states without
any evidence at all.

Overall, we see competitive performance overall. In particular when compared with
the best performing state merging approach at the time of the competition (Llorens). The
competition winner’s Gibbs sampling approach is hard to beat on all problems, in particular
those where the ground truth model is non-deterministic. For the PDFA ground truth
models, the performance is close to optimal. We emphasize, however, that we did not tune
any parameters or run FlexFringe’s search procedure to obtain these results.

14

S.VERWER AND C.HAMMERSCHMIDT

Nr Model Solution Shibata Llorens Alergia94 Alergia+ Likelihood MDI
HMM 29.90
31.20
1
HMM 168.33
168.96
2
49.96
PNFA
51.21
3
PNFA
80.82
80.89
4
HMM 33.24
33.31
5
66.99
PDFA
67.54
6
51.22
PDFA
51.46
7
82.05
81.38
PNFA
8
20.99
20.84
9
PDFA
35.04
33.30
10 PNFA
33.56
31.81
11 PDFA
22.49
21.66
12 PNFA
62.87
13 PDFA
62.81
117.13
14 HMM 116.79
46.80
44.24
15 PNFA
30.78
30.71
16 PDFA
51.13
47.31
17 PNFA
57.39
18 PDFA
57.33
17.92
19 HMM 17.88
98.61
20 HMM 90.97
37.31
21 HMM 30.52
26.61
22 PNFA
25.98
18.47
23 HMM 18.41
38.91
24 PDFA
38.73
66.83
25 HMM 65.74
83.52
80.74
26 PDFA
43.49
27 PDFA
42.43
53.55
28 HMM 52.74
24.58
24.03
29 PNFA
23.33
22.93
30 PNFA
42.27
41.21
31 PNFA
32.65
32.61
32 PDFA
32.64
33 HMM 31.87
26.50
19.96
34 PNFA
36.81
35 PDFA
33.78
38.29
36 HMM 37.99
21.11
37 PNFA
20.98
21.49
38 HMM 21.45
10.05
10.00
39 PNFA
8.52
40 PDFA
8.20
13.98
41 HMM 13.91
16.05
16.00
42 PDFA
32.85
43 PNFA
32.64
12.04
44 HMM 11.71
24.24
45 HMM 24.04
12.89
11.98
46 PNFA
4.13
4.119
47 PDFA
8.24
8.04
48 PDFA

30.40
29.99
168.42
168.43
50.68
50.04
80.84
80.83
33.24
33.24
67.00
67.01
51.26
51.25
81.71
81.40
20.85
20.86
34.04
33.33
32.55
31.85
21.77
21.66
62.82
62.82
116.84 116.84
44.70
44.27
30.72
30.72
47.92
47.35
57.33
57.33
17.92
17.88
93.50
91.00
32.22
30.57
26.08
25.99
18.45
18.41
38.73
38.73
67.27
65.78
80.84
80.83
42.46
42.46
53.20
52.84
24.11
24.04
23.21
22.93
41.62
41.23
32.62
32.62
32.03
31.87
20.54
19.97
34.30
33.80
38.41
38.02
21.02
21.00
21.60
21.46
10.00
10.00
8.21
8.21
13.94
13.92
16.01
16.01
32.78
32.72
12.04
11.76
24.05
24.05
12.10
11.99
4.12
4.12
8.19
8.04

31.98
168.43
51.35
80.95
33.24
67.01
51.24
83.01
20.85
33.65
31.84
21.68
64.76
116.84
45.10
30.72
48.03
57.33
17.97
92.36
35.25
26.56
18.49
38.73
67.26
80.89
42.46
53.77
24.20
23.47
42.08
32.62
31.96
25.99
33.80
38.87
21.19
21.84
10.00
8.26
14.02
16.01
33.14
12.70
24.04
12.50
4.12
8.04

31.58
168.43
50.65
80.93
33.24
67.00
51.24
84.83
20.85
35.62
31.85
21.68
64.86
116.85
48.69
30.72
47.95
57.33
17.98
91.86
35.47
27.26
18.44
38.73
68.24
80.91
42.46
53.05
24.64
23.25
41.51
32.62
31.95
43.01
33.80
38.25
21.07
21.49
10.00
8.67
13.98
16.01
32.97
12.01
24.04
13.02
4.12
8.04

34.01
171.21
52.27
82.30
34.65
74.05
82.92
91.23
22.22
49.51
76.53
23.78
65.01
117.88
52.29
33.49
60.60
67.04
18.60
149.44
83.40
39.25
18.84
39.63
101.97
112.01
80.52
60.83
27.80
26.05
43.00
33.28
32.21
36.27
72.29
40.88
21.11
24.02
10.34
9.66
14.06
16.14
33.30
12.62
24.05
15.55
4.65
11.73

AIC
31.19
168.43
50.65
81.02
33.24
67.00
51.24
82.73
20.85
33.47
31.84
21.68
62.82
116.85
44.66
30.72
48.11
57.33
17.92
91.68
33.52
26.37
18.45
38.73
66.96
80.98
42.47
53.02
24.15
23.22
41.60
32.62
31.97
22.63
33.81
38.32
21.13
21.49
10.00
8.23
14.02
16.01
33.05
12.04
24.04
12.43
4.12
8.04

Table 1: PAutomaC problems, model types (HMM = hidden Markov model, PNFA =
probabilistic non-deterministic automaton), and perplexity scores of top two teams
in PAutomaC and FlexFringe with diﬀerent evaluation functions. Best scores are
bold, large deviations in FlexFringe variants are underlined.

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 15

6.2. Other evaluation functions. We also evaluate the likelihood-ratio, MDI, and AIC
evaluation functions to demonstrate that the choice of function can have a large aﬀect on
the obtained performance. In fact, one of the main reasons we developed FlexFringe is to be
able to design a new evaluation function quickly. We believe that diﬀerent problems not
only require diﬀerent parameter setting, but often require diﬀerent evaluation functions,
similar to the use of diﬀerent loss-functions when learning neural networks.

The results from likelihood-ratio seem slightly worse than the results we obtain from our
modiﬁed Alergia. Although it achieves competitive scores no many problems. On several
problems, the obtained perplexity scores are much larger.

Out-of-the-box, MDI also seems to perform worse than Alergia, though it shows smaller
deviations than out-of-the-box likelihood-ratio, in particular on problem 21. Although we
did not tune any parameters for MDI, the results show competitive performance on many
PAutomaC problems. Interestingly, and unexpectedly, AIC performs best out-of-the-box,
and even outperforms Alergia for which we did test several parameter values. Ignoring empty
lines, the code for AIC is about 20 lines long (it inherits its update routines from likelihood).
This result shows the key strength of FlexFringe: the ability to quickly implement new
evaluation functions. We did not expect AIC to work so well based on earlier results [Ver10].
It indicates that our pooling and parameter counting strategies have a positive eﬀect on
model selection criteria.

7. Results on HDFS

The HDFS data set [XHF+09] is a well-known data set for anomaly detection and has for
instance been used to evaluate the DeepLog anomaly detection framework based on neural
networks [DLZS17]. The ﬁrst few lines of the training ﬁle given to FlexFringe is shown in
Figure 5. As can be seen, this data contains patterns that are quite typical in software
systems such as parallelism, repetitions, and sub-processes. Although FlexFringe does not
speciﬁcally look for such patterns (yet), the deterministic nature of the models learned by
FlexFringe does oﬀer advantages over the use of neural networks. Firstly, since software
is usually deterministic, automaton models provide insight into the software process that
generated the data when visualized. Secondly, again due to software’s deterministic nature,
learned automata provide excellent performance on problems such as sequence prediction
and anomaly detection. Thirdly, learning automata is much faster. FlexFringe requires less
than a second of training time to returns good performing and insightful models from the
HDFS training data.

We obtained the data from the DeepLog GitHub repository. The training data consists
of 4855 training traces (all normal), 16838 abnormal testing traces, and 553366 normal
testing traces. We thus see only a small fraction of the normal data at training time. Despite
of this restriction, DeepLog shows quite good performance on detecting anomalies [DLZS17]:
833 false positives (normal labeled as abnormal) and 619 false negatives (abnormal labeled
as normal). We now present the results of FlexFringe on this data, ﬁrst in terms of insight
then in terms of performance.

7.1. Software process insight. For getting initial insight into the data, we run the aic
evaluation function out-of-the-box on the training data. The result is shown in Figure 6. We
can clearly distinguish subprocesses and parallelism. The initial processes form a diamond-
like shape indicative of parallel executions consisting of ﬁrst the values 22 and 5, followed by

16

S.VERWER AND C.HAMMERSCHMIDT

4855 50
1 19 5 5 5 22 11 9 11 9 11 9 26 26 26 23 23 23 21 21 21
1 13 22 5 5 5 11 9 11 9 11 9 26 26 26
1 21 22 5 5 5 26 26 26 11 9 11 9 11 9 2 3 23 23 23 21 21 21
1 13 22 5 5 5 11 9 11 9 11 9 26 26 26
1 31 22 5 5 5 26 26 26 11 9 11 9 11 9 4 3 3 3 4 3 4 3 3 4 3 3 23 23 23 21 21 21

Figure 5: The ﬁrst six lines of the HDFS training data provided to FlexFringe in Abbadingo
format [LPP98]. The ﬁrst line gives the number of sequences and the alphabet
size. Then each line presents a sequence by specifying the sequence type, length,
and the sequence itself as a list of symbols. All traces have type 1, meaning they
are all valid system occurrences. When multiple classes or sequences types are
available, you can specify the type here in order to learn a classiﬁer. In this use
case, we learn a probabilistic model and do not care about sequence types. From
these few sequences, we already see several subprocesses with symbols: 5s-22s at
the start, 11s-9s in the middle, and 23s-21s at the end. Optionally 2s-3s-4s appears
before the 23s-21s. FlexFringe will capture such structures and more hidden ones.

three 26s and three pairs of 11s and 9s. The ﬁeld of process mining is focused on methods
that explicitly model such behavior using Petri Nets. Automata can model parallel behavior,
but at a great cost in model size. Since the bias of automaton learning is to minimize
this size, it is nice to see that FlexFringe is able to discover this behavior from only a few
thousand traces. In future work, we aim to extend FlexFringe to actively search for such
behavior and potentially complete the obtained models.

After the initial two processes (forming the diamond), there are two possible subprocesses:
an infrequent long chain of executions 25-18-5-6-26-26-21, which can be repeated, and a
frequent process with many repetitions of 2s, 3s, and 4s. These processes can also be skipped
and the repetitions can end at diﬀerent points. This can be seen by the many transitions
going to the ﬁnal process consisting of three optional repetitions of 23s and 21s.

Overall, the learned model provides a lot of insight into the structure of the process that
generated the logs. We could reach similar conclusions simply by looking at the log ﬁles but
we cannot look at 4855 log lines in one view, the learned automaton provides such a view.
Moreover, it can show patterns that would be hard to ﬁnd via manual investigation.

For instance after the parallel executions of the 9s, 11s, and 26s, there are two possible
futures depending on whether the ﬁnal symbol is a 9 or 26. In the latter case, starting the
23s and 21s ending sequence is much more likely. When the parallel execution ends with a 9,
only 361 out of 1106 traces start this ending. When ending with a 26, these sequences occur
2519 out of 4375 times. This diﬀerence causes the learning algorithm to infer there are two
states that signify the end of the 9-11-26 parallel execution: state 98 and state 100. These
are the frequent (thick edged) states in the middle left and middle right of the automaton
model. Another observation is that this 23-21 ending sequence can be started from many
diﬀerent places in the system, but after the initial parallel executions. This can be seen by
the many input transitions to state 102, the frequent state in the right bottom part of the
model.

The model also shows some strange bypasses of this behavior, for instance the rightmost
infrequent path that skips the frequent states after the 9-11-26 parallel executions (rightmost

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 17

Figure 6: The result of FlexFringe’s AIC heuristic without tuning on the HDFS data set.
We see clear parallelism in the top, chains of events at the left bottom, repetitions
(loops) in the right bottom, and a short sequence of ending events. The thickness
of states are an indication of their frequency.

0:#4855ﬁn:  path: 1:4855 , 4855 01:#3595ﬁn:  path: 1:3595 , 3595 053595 2:#1258ﬁn:  path: 1:1258 , 1258 0221258 3:#4ﬁn:  path: 1:4 , 4 0262 I4:#2593ﬁn:  path: 1:2593 , 2593 052593 5:#999ﬁn:  path: 1:999 , 999 022999 6:#3ﬁn:  path: 1:3 , 3 0113 7:#1257ﬁn:  path: 1:1257 , 1257 051257 13:#21ﬁn: 1:1 ,  path: 1:20 , 20 1261 8:#6ﬁn:  path: 1:6 , 6 0264 9:#1637ﬁn:  path: 1:1637 , 1637 051637 10:#1955ﬁn:  path: 1:1955 , 1955 022956 5999 11:#3ﬁn:  path: 1:3 , 3 093 12:#1257ﬁn:  path: 1:1257 , 1257 051257 266 19:#10ﬁn:  path: 1:10 , 10 059 115:#281ﬁn:  path: 1:281 , 281 021 151:#79ﬁn:  path: 1:79 , 79 034 224 262 14:#3589ﬁn:  path: 1:3589 , 3589 0221637 51952 15:#3ﬁn:  path: 1:3 , 3 0113 16:#6ﬁn:  path: 1:6 , 6 053 17:#1255ﬁn:  path: 1:1255 , 1255 051255 18:#5ﬁn:  path: 1:5 , 5 0262 20:#3812ﬁn:  path: 1:3812 , 3812 0113434 21:#93ﬁn:  path: 1:93 , 93 0980 24:#939ﬁn:  path: 1:939 , 939 02675 93 22:#3ﬁn:  path: 1:3 , 3 053 23:#3ﬁn:  path: 1:3 , 3 0113 11378 913 26864 263 25:#2ﬁn:  path: 1:2 , 2 0112 26:#10ﬁn:  path: 1:10 , 10 01110 122:#604ﬁn:  path: 1:604 , 604 03228 123:#77ﬁn:  path: 1:77 , 77 0453 22 102:#3820ﬁn:  path: 1:3820 , 3820 02377 27:#23ﬁn:  path: 1:23 , 23 01123 28:#3789ﬁn:  path: 1:3789 , 3789 093789 29:#56ﬁn:  path: 1:56 , 56 01156 30:#9ﬁn:  path: 1:9 , 9 095 31:#39ﬁn:  path: 1:39 , 39 02632 97 34:#106ﬁn:  path: 1:106 , 106 011106 35:#826ﬁn:  path: 1:826 , 826 026826 32:#3ﬁn:  path: 1:3 , 3 0113 33:#3ﬁn:  path: 1:3 , 3 093 36:#2ﬁn:  path: 1:2 , 2 092 910 37:#3513ﬁn:  path: 1:3513 , 3513 0923 113483 39:#87ﬁn:  path: 1:87 , 87 0938 38:#400ﬁn:  path: 1:400 , 400 026268 117 949 40:#7ﬁn:  path: 1:7 , 7 0115 41:#4ﬁn:  path: 1:4 , 4 0264 92 1126 42:#11ﬁn:  path: 1:11 , 11 02611 51:#56ﬁn:  path: 1:56 , 56 093 43:#3ﬁn:  path: 1:3 , 3 053 9106 44:#133ﬁn:  path: 1:133 , 133 011133 45:#8ﬁn:  path: 1:8 , 8 098 46:#685ﬁn:  path: 1:685 , 685 026685 47:#2ﬁn:  path: 1:2 , 2 052 48:#11ﬁn:  path: 1:11 , 11 01111 49:#3503ﬁn:  path: 1:3503 , 3503 093502 92 1153 52:#45ﬁn:  path: 1:45 , 45 02632 50:#272ﬁn:  path: 1:272 , 272 011272 913 59:#246ﬁn:  path: 1:246 , 246 026115 53:#2ﬁn:  path: 1:2 , 2 0112 54:#5ﬁn:  path: 1:5 , 5 095 112 55:#2ﬁn:  path: 1:2 , 2 0262 56:#11ﬁn:  path: 1:11 , 11 02611 221 64:#2987ﬁn:  path: 1:2987 , 2987 01110 68:#58ﬁn:  path: 1:58 , 58 0944 65:#525ﬁn:  path: 1:525 , 525 0261 57:#2ﬁn:  path: 1:2 , 2 0112 67:#272ﬁn:  path: 1:272 , 272 0261 9131 58:#2ﬁn:  path: 1:2 , 2 0112 60:#8ﬁn:  path: 1:8 , 8 0118 61:#680ﬁn:  path: 1:680 , 680 011680 62:#5ﬁn:  path: 1:5 , 5 095 63:#2ﬁn:  path: 1:2 , 2 0112 911 112965 914 26524 9271 66:#1ﬁn:  path: 1:1 , 1 0111 69:#20ﬁn:  path: 1:20 , 20 01120 70:#36ﬁn:  path: 1:36 , 36 02625 93 75:#775ﬁn:  path: 1:775 , 775 02690 73:#155ﬁn:  path: 1:155 , 155 011153 111 87:#3371ﬁn:  path: 1:3371 , 3371 0261 81:#63ﬁn:  path: 1:63 , 63 0113 71:#2ﬁn:  path: 1:2 , 2 0262 72:#2ﬁn:  path: 1:2 , 2 0262 1111 92 80:#354ﬁn:  path: 1:354 , 354 011245 79:#634ﬁn:  path: 1:634 , 634 02627 92 98 9674 74:#6ﬁn:  path: 1:6 , 6 0116 76:#5ﬁn:  path: 1:5 , 5 0115 77:#2ﬁn:  path: 1:2 , 2 092 78:#2997ﬁn:  path: 1:2997 , 2997 092987 2648 1110 11100 26425 91 912 118 1127 82:#14ﬁn:  path: 1:14 , 14 0269 94:#3853ﬁn:  path: 1:3853 , 3853 0263371 1119 89:#62ﬁn:  path: 1:62 , 62 02644 83:#2ﬁn:  path: 1:2 , 2 0262 84:#2ﬁn:  path: 1:2 , 2 0112 95 85:#776ﬁn:  path: 1:776 , 776 011770 9155 96 86:#5ﬁn:  path: 1:5 , 5 095 261 97:#947ﬁn:  path: 1:947 , 947 0111 262997 9354 88:#454ﬁn:  path: 1:454 , 454 011454 918 92:#948ﬁn:  path: 1:948 , 948 026162 1114 90:#2ﬁn:  path: 1:2 , 2 0112 153:#31ﬁn:  path: 1:31 , 31 0112 9772 91:#4ﬁn:  path: 1:4 , 4 0114 93:#5ﬁn:  path: 1:5 , 5 0115 100:#1106ﬁn: 1:8 ,  path: 1:1098 , 1098 89947 9454 1127 95:#47ﬁn:  path: 1:47 , 47 02635 11941 97 96:#2ﬁn:  path: 1:2 , 2 0112 111 2320 146:#29ﬁn:  path: 1:29 , 29 0310 94 95 21 98:#4375ﬁn: 1:868 ,  path: 1:3507 , 3507 868263850 106:#26ﬁn:  path: 1:26 , 26 0251 99:#1ﬁn:  path: 1:1 , 1 0181 1147 127:#246ﬁn:  path: 1:246 , 246 0261 132:#15ﬁn:  path: 1:15 , 15 021 2272 2513 101:#1ﬁn:  path: 1:1 , 1 051 232519 103:#2ﬁn:  path: 1:2 , 2 0212 104:#460ﬁn:  path: 1:460 , 460 03354 105:#686ﬁn:  path: 1:686 , 686 04344 107:#2ﬁn:  path: 1:2 , 2 0182 116:#28ﬁn:  path: 1:28 , 28 01826 108:#1ﬁn:  path: 1:1 , 1 0251 25 2338 109:#446ﬁn:  path: 1:446 , 446 03186 112:#320ﬁn:  path: 1:320 , 320 0417 415 2154 23361 4323 3260 147:#3959ﬁn: 1:3959 ,  path: 0 3959221 110:#3820ﬁn:  path: 1:3820 , 3820 0233820 111:#2ﬁn:  path: 1:2 , 2 0232 2315 210 3238 113:#356ﬁn:  path: 1:356 , 356 04197 4264 24 114:#425ﬁn:  path: 1:425 , 425 03418 252 117:#1ﬁn:  path: 1:1 , 1 051 237 4159 216 118:#264ﬁn:  path: 1:264 , 264 03264 119:#3821ﬁn:  path: 1:3821 , 3821 0233820 120:#2ﬁn:  path: 1:2 , 2 0232 234 142:#367ﬁn:  path: 1:367 , 367 0392 121:#663ﬁn:  path: 1:663 , 663 04224 231 3355 2313 23 4360 349 124:#28ﬁn:  path: 1:28 , 28 0528 125:#1ﬁn:  path: 1:1 , 1 061 314 23265 44 131:#321ﬁn:  path: 1:321 , 321 02321 4245 137:#19ﬁn:  path: 1:19 , 19 021 126:#18ﬁn:  path: 1:18 , 18 0318 21 128:#3951ﬁn:  path: 1:3951 , 3951 0213820 232 2329 150:#338ﬁn:  path: 1:338 , 338 03338 465 235 129:#130ﬁn:  path: 1:130 , 130 023130 130:#434ﬁn:  path: 1:434 , 434 03433 32 25 2311 133:#230ﬁn:  path: 1:230 , 230 0459 134:#9ﬁn:  path: 1:9 , 9 069 135:#19ﬁn:  path: 1:19 , 19 01619 136:#1ﬁn:  path: 1:1 , 1 0161 319 318 138:#3951ﬁn:  path: 1:3951 , 3951 0213951 139:#130ﬁn:  path: 1:130 , 130 023130 4219 140:#8ﬁn:  path: 1:8 , 8 0235 141:#210ﬁn:  path: 1:210 , 210 03210 23166 143:#171ﬁn:  path: 1:171 , 171 02155 314 231 144:#215ﬁn:  path: 1:215 , 215 04215 145:#29ﬁn:  path: 1:29 , 29 0169 619 261 213951 148:#132ﬁn:  path: 1:132 , 132 023130 149:#8ﬁn:  path: 1:8 , 8 0238 31 4206 233 266 23104 31 238 324 4171 212 152:#29ﬁn:  path: 1:29 , 29 02629 329 21129 157:#9ﬁn:  path: 1:9 , 9 032 154:#9ﬁn:  path: 1:9 , 9 041 238 23181 396 24 155:#57ﬁn:  path: 1:57 , 57 0457 156:#29ﬁn: 1:1 ,  path: 1:28 , 28 12629 159:#9ﬁn:  path: 1:9 , 9 0217 42 42 217 41 356 158:#28ﬁn: 1:15 ,  path: 1:13 , 13 152128 2512 21 160:#19ﬁn: 1:3 ,  path: 1:16 , 16 3219 214 32 410 18

S.VERWER AND C.HAMMERSCHMIDT

Figure 7: A subplot of the PDFA from Figure 6 showing paths taken by the mentioned
anomalous traces and several parallel states (ﬁn and path counts removed for
clarity). State 7 occurs 1257 times in the training data (of 4855 traces in total)
and the subsequent event is always 5 (never 11 as in the diﬀerent start trace).
The infrequent trace reaches state 69 (thick path), there is no outgoing transition
with label 26 from state 69. However, we could infer from the parallel paths that
the target state should be state 79, i.e., the state reached by swapping the last 11
and 26 events, since it seems these can be executed in parallel.

path, middle of the automaton). This path occurs only twice in the entire training data.
Consequently, the statistics used to infer this path are not well estimated. It seems likely
that the learning algorithm made an incorrect inference, i.e., these frequent states should not
be bypassed. We are currently working on techniques to change the bias of FlexFringe to
avoid making such mistakes. Note that the only way to identify such issues is by visualizing
and reasoning about the obtained models, something that is prohibitively hard for many
other machine learning models such as neural nets. This is an important reason why the
recent research line of extracting automaton models from complex neural networks is very
relevant [WGY18, AEG19, MAP+21].

7.2. Anomaly detection performance. Out-of-the-box, the aic model seems to capture
the underlying process behavior and it can therefore be used for anomaly detection. The
most straightforward approach, which does not involve setting a decision threshold, is simply
to run the test set through the model and raise an alarm either when a trace ends in a
state without any ﬁnal occurrences, or when it tries to trigger a transition that does not
exist. This strategy gives 4132 false positives but only 1 false negative. Using the commonly
used F1 score as metric, this gives a score of 0.89, which is worse than the 0.96 obtained by
DeepLog on the same data.

We can of course improve this performance by tuning several parameters. Before we do
so, it is insightful to understand the cause for the somewhat large number of false positives.
FlexFringe learns (merges states) by testing whether the future process is independent from
the past process. By merging more, it will generalize more, and hence cause less false
positives. But should this be our aim?

One of the key strengths of learning a deterministic automaton model is that one can
easily follow a trace’s execution path [HVLS16]. The simplicity of our anomaly detection
setup then allows us to reason on the logic of the detection. This kind of explainable machine
learning is unheard of in the neural network literature. Investigating the raised false positives
provides us with four frequent types of anomalous traces in the normal test set:
(1) Diﬀerent starts, e.g., 22-5-11-9-5-11-9-5-11-9-26-26-26.

0:#48551:#359553595 2:#1258221258 I4:#259352593 5:#99922999 7:#125751257 9:#163751637 10:#195522956 5999 12:#125751257 14:#3589221637 51952 17:#125551255 20:#3812113434 21:#93980 24:#9392675 11378 913 26864 28:#378993789 29:#561156 31:#392632 97 34:#10611106 39:#87938 38:#40026268 949 1126 9106 52:#452632 50:#27211272 913 67:#2729271 69:#201120 70:#362625 80:#35411245 79:#6342627 118 81:#63912 1127 FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 19

(2) Infrequent paths, e.g., 22-5-5-5-9-26-11-9-11-26...
(3) New symbols, e.g., 22-5-5-5-...-3-4-23-23-23-21-21-20-21.
(4) Diﬀerent repetitions, e.g., 5-5-...-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-2-2-...

To facilitate the analysis of these behaviors, we plot a subgraph from Figure 6 in Figure 7.
The diﬀerent start traces quickly reach a state without a transition for the next symbol.
The listed trace ends after the 22 and 5 symbols, the reached state occurred 1257 times by
traces from the training data, and all of these traces had 5 as their next symbol. We would
argue that this is an anomaly that should be raised.

The infrequent paths end in, or traverse, states that occur infrequently. The listed preﬁx
ends after the second 11 symbol in a state that occurs only 20 times and always had a 9
or 11 as next symbol in the training data. This seems no anomaly and diﬀerent parameter
settings would likely cause a merge of this state, and thus possibly provide a transition with
label 11. The sink parameters in FlexFringe can be used to prevent learning models with
infrequent occurrences and thus avoid raising such false positives. We argue however that
this is bad practice as learning such an infrequent state is no mistake. Many states are
required to model the parallelism present in the data, and several of these will be infrequent.
Given this parallelism, we actually know what state to target, the one reached by the preﬁx
22, 5, 5, 5, 9, 26, 11, 9, 26, 11. This state occurs much more frequently (634 times) and we
could simply add this transition to the model. In future work, we aim to either extend a
learned automaton with such 0-occurrence transitions or check for them at test time.

The traces with new symbols are clearly abnormal and should be counted as true
negatives rather than false positives. The HDFS data is somewhat strange in that events
occur in the test set that never occurred at train time. Also many of the true positive traces
contain such symbols.

Traces with diﬀerent repetitions do show mistakes made by the learning algorithm.
The repetition subprocess contains many possible repetitions, but apparently still more are
possible. Performing more or diﬀerent merges will change these and potentially remove these
false positives. Learning which repetitions are possible and which are not requires more data
or a diﬀerent learning strategy/parameter settings.

7.3. A diﬀerent learning strategy. One way to raise less false alarms is to perform more
merges and thus obtain fewer states that have more outgoing transitions. The aic evaluation
function does not have a signiﬁcance parameter. Instead, we learn another model using the
likelihoodratio evaluation function and a very low conﬁdence threshold of 1E-15. Other than
that, we keep the default settings. The resulting model is displayed in Figure 8. The model
is much less insightful than Figure 6 and likely overgeneralizes due to all the added loops.
It certainly models impossible system behavior such as inﬁnite loops of 21s. In terms of
performance, however, this model achieves 330 false positives and 624 false negatives, i.e.,
an F1-score of 0.97 outperforming the score achieved by DeepLog.

This demonstrates automaton learning methods can outperform neural network ap-
proaches with little ﬁne-tuning on software log data. We believe the main reason for this
to be that software data is highly structured and often deterministic. On the experiments
on the PAutomaC data, we also demonstrated that deterministic automata learned using
FlexFringe perform excellent when the ground truth model is deterministic. Automata are
simply good at capturing the type of patterns that occur in deterministic systems.

20

S.VERWER AND C.HAMMERSCHMIDT

Figure 8: The result of FlexFringe’s Likelihoodratio heuristic with a very low conﬁdence
threshold. It performs many more merges compared to Figure 6. As a consequence,
it is much harder to interpret. We still see some parallelism at the top, but the
many self loops are long arcs hides the other properties and likely overgeneralizes.
In terms of F1-score, it performs better than the model from Figure 6, and better
than the DeepLog baseline [DLZS17]

0:#4855ﬁn:  path: 1:4855 , 4855 01:#3595ﬁn:  path: 1:3595 , 3595 053595 2:#1258ﬁn:  path: 1:1258 , 1258 0221258 68:#69ﬁn: 1:1 ,  path: 1:68 , 68 1262 I3:#2593ﬁn:  path: 1:2593 , 2593 052593 4:#2256ﬁn:  path: 1:2256 , 2256 022999 18:#102ﬁn:  path: 1:102 , 102 0113 51257 82:#91ﬁn: 1:1 ,  path: 1:90 , 90 1261 269 35 74:#130ﬁn:  path: 1:130 , 130 0534 13:#56ﬁn: 1:2 ,  path: 1:54 , 54 2225 25:#292ﬁn:  path: 1:292 , 292 0234 43:#71ﬁn:  path: 1:71 , 71 025 49:#87ﬁn:  path: 1:87 , 87 042 83:#85ﬁn: 1:1 ,  path: 1:84 , 84 1114 5:#1637ﬁn:  path: 1:1637 , 1637 051637 6:#3212ﬁn:  path: 1:3212 , 3212 022956 52256 17:#242ﬁn:  path: 1:242 , 242 01140 94 29:#63ﬁn:  path: 1:63 , 63 02658 2323 317 221 21:#137ﬁn:  path: 1:137 , 137 0411 36:#153ﬁn:  path: 1:153 , 153 0118 28:#135ﬁn:  path: 1:135 , 135 026 30:#137ﬁn:  path: 1:137 , 137 0267 75:#3942ﬁn:  path: 1:3942 , 3942 02117 7:#1637ﬁn:  path: 1:1637 , 1637 0221637 8:#3207ﬁn:  path: 1:3207 , 3207 053207 113 262 9:#3812ﬁn:  path: 1:3812 , 3812 0111575 11:#247ﬁn:  path: 1:247 , 247 0935 10:#95ﬁn:  path: 1:95 , 95 02627 112237 958 12:#912ﬁn:  path: 1:912 , 912 026912 43 53 94 629 1629 79:#189ﬁn:  path: 1:189 , 189 02631 237 34 1120 267 52 43 23:#357ﬁn:  path: 1:357 , 357 0113 924 235 211 76:#57ﬁn:  path: 1:57 , 57 024 35 1123 14:#3789ﬁn:  path: 1:3789 , 3789 093789 2699 11141 97 15:#143ﬁn:  path: 1:143 , 143 01171 16:#192ﬁn: 1:2 ,  path: 1:190 , 190 22624 19:#86ﬁn:  path: 1:86 , 86 01186 84:#143ﬁn: 1:3 ,  path: 1:140 , 140 397 20:#819ﬁn:  path: 1:819 , 819 026819 938 26268 22:#3483ﬁn:  path: 1:3483 , 3483 0113483 418 317 2316 1116 24:#88ﬁn:  path: 1:88 , 88 0976 212 925 1125 2636 450 58:#3840ﬁn:  path: 1:3840 , 3840 02317 35:#86ﬁn:  path: 1:86 , 86 027 26:#74ﬁn: 1:1 ,  path: 1:73 , 73 1328 9116 27:#60ﬁn:  path: 1:60 , 60 01160 2666 221 2647 2115 314 425 1137 2321 99 986 1112 2311 911 2611 26 86:#95ﬁn:  path: 1:95 , 95 0335 41:#199ﬁn: 1:50 ,  path: 1:149 , 149 502144 410 98 11130 31:#681ﬁn:  path: 1:681 , 681 026681 94 33:#240ﬁn:  path: 1:240 , 240 011238 34:#115ﬁn:  path: 1:115 , 115 026115 264 952 213 336 417 23 2313 119 29 313 23173 49 2188 48 261 2114 2335 26 35 52 236 36 440 25 910 268 234 22 37 44 42:#118ﬁn:  path: 1:118 , 118 011118 1111 32:#3472ﬁn:  path: 1:3472 , 3472 093472 960 119 234 22 2610 85:#3940ﬁn: 1:3936 ,  path: 1:4 , 4 393642 1129 33 2654 71:#1178ﬁn:  path: 1:1178 , 1178 042 63:#3840ﬁn:  path: 1:3840 , 3840 0233840 24 411 2316 269 310 1134 212 312 2325 427 81:#396ﬁn:  path: 1:396 , 396 029 960 32 235 26 77:#250ﬁn: 1:17 ,  path: 1:233 , 233 1726118 70:#127ﬁn:  path: 1:127 , 127 044 914 53 115 28 264 239 317 424 52 2628 91 48:#105ﬁn:  path: 1:105 , 105 01132 114 264 9129 95 37:#676ﬁn:  path: 1:676 , 676 011676 265 1111 432 919 316 78:#104ﬁn:  path: 1:104 , 104 02312 914 38:#2936ﬁn:  path: 1:2936 , 2936 0112936 39:#759ﬁn:  path: 1:759 , 759 026522 9237 44:#755ﬁn:  path: 1:755 , 755 0113 2682 40:#73ﬁn:  path: 1:73 , 73 01133 23392 242 59:#935ﬁn:  path: 1:935 , 935 03652 492 42 80:#3940ﬁn: 1:2 ,  path: 1:3938 , 3938 2213940 211 2382 3186 4117 116 25 2379 349 435 2659 47 2330 390 2655 1140 910 116 9670 45:#2936ﬁn:  path: 1:2936 , 2936 092936 46:#310ﬁn:  path: 1:310 , 310 011310 47:#567ﬁn:  path: 1:567 , 567 026449 94 50:#751ﬁn:  path: 1:751 , 751 011751 973 415 314 1116 99 2358 215 216 61:#29ﬁn:  path: 1:29 , 29 02516 9118 51:#3246ﬁn:  path: 1:3246 , 3246 0262936 9310 94 52:#409ﬁn:  path: 1:409 , 409 011409 53:#901ﬁn:  path: 1:901 , 901 026154 1826 53 114 9747 54:#3655ﬁn:  path: 1:3655 , 3655 0263246 9409 93 55:#898ﬁn:  path: 1:898 , 898 011898 43 31 21 181 251 56:#3898ﬁn: 1:831 ,  path: 1:3067 , 3067 831263652 57:#1049ﬁn: 1:8 ,  path: 1:1041 , 1041 89898 51 212 232227 182 2512 2246 3283 60:#590ﬁn:  path: 1:590 , 590 04294 23351 2151 4296 62:#243ﬁn:  path: 1:243 , 243 03243 2311 25 64:#642ﬁn:  path: 1:642 , 642 03512 65:#587ﬁn:  path: 1:587 , 587 04407 24 66:#359ﬁn:  path: 1:359 , 359 03359 67:#227ﬁn:  path: 1:227 , 227 04227 237 215 3130 491 69:#3840ﬁn:  path: 1:3840 , 3840 0233840 236 4607 329 2318 3569 23 234 335 72:#966ﬁn:  path: 1:966 , 966 04317 3195 73:#429ﬁn:  path: 1:429 , 429 0432 41 31 21 213837 311 23497 44 2454 213 2319 335 4362 2313 42 389 213938 FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 21

A key question and challenge for future work is how to treat infrequent states during
learning. Is it better to keep them intact to obtain a more interpretable model or should we
merge them and get improved performance at cost of interpretability? In order to prevent
this trade-oﬀ, we are currently extending FlexFringe with methods that look for software
speciﬁc patterns such as parallelism and subprocesses. We believe that such extensions to
be crucial for obtaining high performing interpretable models.

8. Related works

There exist a lot of diﬀerent algorithms for learning (P)DFAs. Like FlexFringe, most of
these use some form of state consistency based on their future behavior, i.e., a test for a
Markov property or Myhill-Nerode congruence. Many algorithms are active, these learn
by interacting with a black-box system-under-test (SUT) by providing input and learning
from the produced output. Starting from the seminal L* work in [Ang87], and its successful
implementation in the LearnLib tool [RSB05], many works have applied and extended this
algorithm, e.g., to analyze and reverse engineer protocols [FBJV16, FBJM+20] and learn
register automata [IHS14a, AFBKV15]. Although closely related to learning from a data
set [LZ04], since FlexFringe does not learn actively, we will not elaborate more on these
approaches and refer to [Vaa17] for an overview of active learning algorithms and their
application. Below, we present related algorithms that learn from a data set as input.

8.1. Algorithms. We described the main state-merging algorithms FlexFringe builds upon
in Section 3.
In the literature, there exist several other approaches. A closely related
research line is consists of diﬀerent versions of the k-Tails algorithm [BF72], essentially
a state-merging method that limits consistency until depth k for computational reasons.
Moreover, this allows to infer models from unlabeled data without using probabilities:
simply require identical suﬃxes up to depth k. In the original work, the authors propose
to solve this problem using mathematical optimization. Afterwards, many greedy versions
of this algorithm have been developed and applied to a variety of software logs [CW98].
Notable extensions of state merging methods are the declarative speciﬁcations [BBA+13],
learning from concurrent/parallel traces [BBEK14], and learning guarded, extended, and
timed automata [MPS16, WTD16, PMM17, HW16]. Several ways to speedup state-merging
algorithms have also been proposed by through divide and conquer and parallel process-
ing [LHG17, ABDLHE10, SBB21]. There have also been several proposals to use diﬀerent
search strategies such as beam search [BO05], genetic algorithms [LR05, TE11], satisﬁability
solving [HV13, ZSU17], and ant-colony optimization [CU13].

Another closely related line of work focuses on spectral learning methods. Spectral
learning formulates the PDFA (or weighted automaton) learning problem as ﬁnding the
spectral decomposition of a Hankel matrix [BCLQ14, GEP15]. Every row in this matrix
represents a preﬁx, every column a suﬃx, each cell contains the string probability of the
corresponding row and column preﬁx and suﬃx. The rows of this matrix correspond to
states of the preﬁx tree. If one row is the multiple of another, it means that the future suﬃx
distribution of the corresponding states are similar, i.e., that they can be merged. Instead of
searching for such similarities and forcing determinization, spectral methods approximate
this using principal component analysis, returning a probabilistic non-deterministic ﬁnite
state automaton (PNFA). These are less interpretable (although typically smaller) than
their deterministic counterparts, but can be computed more eﬃciently.

22

S.VERWER AND C.HAMMERSCHMIDT

Due to their close relationship with hidden Markov models (HMMs) [DDE05], sev-
eral approaches exist that infer HMMs instead of PDFAs from trace data. HMMs are
typically learned using a form of expectation-maximization known as the Baum-Welch
algorithm [RJ86]. However, special state merging [SO92] or state splitting [TS92] algorithms
have also been proposed. A notable recent approach [EM18] learns accurate probabilistic
extended automata using HMMs combined with reinforcement learning.

8.2. Tools. There exist several implementations of state-merging algorithms that can be
found on the internet. We list the most popular ones and highlight diﬀerences with FlexFringe.

8.2.1. MINT.
[WTD16] is a tool for learning extended DFAs. These contain guards on
values in addition to symbols. In MINT, these guards are inferred using a classiﬁer from
standard machine learning tools which aim to predict the next event from features of the
current event. When triggering a transition, the guard is used together with the symbol
to determine the next state. FlexFringe also contains such functionality, but instead of
using a classiﬁer, it uses a decision-tree like construction to determine guards. Moreover,
FlexFringe uses the RTI procedure for this construction, which requires consistency for the
entire future instead of only the next event. Finally, in MINT the learning of these guards
is performed as preprocessing. In FlexFringe it is computed on the ﬂy for every blue state
(merge candidate). MINT contains several algorithms including GK-Tails [LMP08], which
uses the Daikon invariant inference system [EPG+07] to learn guards.

8.2.2. Synoptic and CSight.
[BABE11, BBEK14] are tools based on k-Tails style state-
merging of non-probabilistic automata. They are focused on learning models for concurrent
and distributed systems, contain methods to infer invariants, and can combined with model
checkers to verify these invariants against the learned models. When a model fails to
satisfy an invariant, it is updated used counter-example guided abstraction reﬁnement
(CEGAR) [CGJ+00]. Although CEGAR is a common way to implement active learning
algorithms, Synoptic and CSight both learn from data sets. From the same lab comes
also InvariMint [BBA+13], a framework for declaratively specifying automaton learning
algorithms using properties speciﬁed by LTL formulas. Similar speciﬁcations in other ﬁrst
order logic have also been proposed [BBB+15]. Such speciﬁcations are very powerful and
allow for a lot of ﬂexibility in designing learning algorithms, as a new algorithm requires
just a few lines of code/formulas. Some properties, such as statistical tests, are quite hard
to specify. This is why FlexFringe allows speciﬁcations of new evaluation functions by
writing code instead of formulas. Currently, FlexFringe does not contain functionality for
CEGAR-like reﬁnement, or methods to mine invariants.

8.2.3. GI-learning.
[COP16] is an eﬃcient toolbox for DFA learning algorithms written in
C++, including signiﬁcant speedups due to parallel computation of merge tests. It contains
implementations of basic approaches for both active algorithms and algorithms that learn
from a data set. It is possible to extend to include more algorithms and diﬀerent types
of automata by extending the classes of these basic approaches. FlexFringe makes this
easier by only requiring new implementations of the consistency check and score methods.
FlexFringe currently contains no methods for parallel processing, but the use of the ﬁnd/union
datastructures (see Section 3) makes FlexFringe already very eﬃcient.

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 23

[BKK+10] is a well-known extensive library for automaton learning algorithms,
8.2.4. LibAlf.
both active and for learning from a data set. It includes many standard but also specialized
algorithms for instance for learning visibly one-counter automata, and also non-deterministic
automata. Like FlexFringe, it is easily extensible but does not include algorithms for learning
guards or probabilistic automata.

[MAP+21] is recent a light-weight active automata learning library written
8.2.5. AALpy.
in pure Python. In addition to many active algorithms and optimizations, it also contains
basic algorithms for learning from a data set. A key feature of AALpy is its easy of use
and the many diﬀerent kinds of models that can be learned, including non-deterministic
ones. It is extensible by deﬁning new types of automata and algorithms. It has a diﬀerent
design from FlexFringe in that a new algorithm requires new implementations of the all
merge routines, instead of only the evaluation functions. AALpy currently has no support
for inferring guards.

8.2.6. LearnLib.
[IHS15] is a popular toolkit for active learning of automata, in particular
Mealy machines. It has methods to connect to a software system under test by mapping
discrete symbols from the automata’s alphabet to concrete inputs for the software system,
such as network packets. In addition, it contains diﬀerent model-based testing methods [LY96]
that are used to ﬁnd counterexamples to an hypothesized automaton and optimized active
learning algorithms such as TTT [IHS14b]. As such, it is frequently used in real-world
use-cases, see, e.g., [FBJM+20]. There also exist extensions for LearnLib such as the ability
to learn extended automata [CHJ15].

[ABDE17] is a library for spectral learning of weighted or probabilistic
8.2.7. Sp2Learn.
automata from a data set written in Python. It learns non-deterministic automata, which
are typically harder to interpret than deterministic ones, but can model distributions from
non-determinstic systems more eﬃciently. Spectral learning can be very eﬀective, as it
solves the learning problem using a polynomial time decomposition algorithm. In contrast,
FlexFringe’s state-merging methods also run in polynomial time but likely results in a local
minimum. Search procedures that aim to ﬁnd the global optimum are very expensive to run.

[SLTIM20] is a recent mixed integer linear programming method for learn-
8.2.8. DISC.
ing non-probabilistic automata from a data set. Using mathematical optimization is a
promising recent approach for solving machine learning problems such as decision tree
learning [CMRRM21, VZ17, BD17]. FlexFringe contains one such approach, but based on
satisﬁability solvers instead of integer programming. An advantage of DISC is that it can
handle noisy data due to the use of integer programming, which uses continuous relaxations
during its solving procedure. Due to the explicit modeling of noise, it can handle some
types of non-determinism without requiring additional states. FlexFringe does not explicitly
model noise, but does allow for more robust evaluation functions such as impurity metrics
used in decision tree learning.

9. Conclusion

FlexFringe ﬁlls a gap in automaton learning tools by providing eﬃcient implementations of
key state-merging algorithms including optimizations for getting improved results in practice.
We presented how to use FlexFringe in order to learn probabilistic ﬁnite state automata

24

S.VERWER AND C.HAMMERSCHMIDT

(PDFAs). It can be used to learn many more types of machines due it ﬂexibility in specifying
evaluation functions. Currently, it contains methods to learn DFA, PDFA, deterministic
real-time automata (DRTA), regression automata (RA), and Mealy/Moore machines. It
can learn these using a variety of methods such as EDSM, Alergia, RPNI, RTI, and with
diﬀerent search strategies. The kinds of automata and/or the used evaluation function can
be changed by adding a single ﬁle to the code base. All that is needed is to specify when a
merge is inconsistent and what score to assign to a possible merge. The main restriction
compared to existing tools is that the learned models have to be deterministic. This is an
invariant we use to speed-up the state-merging algorithm.

FlexFringe obtains excellent results on prediction and anomaly tasks thanks to our
optimizations. Moreover, the learned models provide clear insight into the inner workings
of black-box software systems. On trace prediction, our results show FlexFringe performs
especially well when the data are generated from a deterministic system. On anomaly
detection, the model produced by FlexFringe outperforms an existing method based on
neural networks while requiring only seconds of run-time to learn. We believe its excellent
performance is due to properties of software data such as little noise and determinism
favoring automaton models.

We demonstrate that there exists a clear trade-oﬀ between the obtained insight and
(predictive) performance of models. Sometimes, it is best to keep data intact, e.g., when
there is too few data to determine what learning (merging) step to take. FlexFringe
provides techniques such as sinks to prevent the state-merging algorithm from performing
incorrect merges, which can be detrimental for insight as these often lead to incorrect
conclusions. Also, merges with little evidence often lead to convoluted models. For making
predictions, however, such convoluted and likely incorrect models perform better due to
their increased generalization. This trade-oﬀ deserves further study. We expect there exist
better generalization methods for software systems that do lead to both improved insight
and improved performance.

References

[ABDE17]

Denis Arrivault, Dominique Benielli, Fran¸cois Denis, and R´emi Eyraud. Sp2learn: A toolbox
for the spectral learning of weighted automata. In International conference on grammatical
inference, pages 105–119. PMLR, 2017.

[ABL02]

[ABDLHE10] Hasan Ibne Akram, Alban Batard, Colin De La Higuera, and Claudia Eckert. Psma: A parallel
algorithm for learning regular languages. In NIPS workshop on learning on cores, clusters and
clouds. Citeseer, 2010.
Glenn Ammons, Rastislav Bodik, and James R Larus. Mining speciﬁcations. ACM Sigplan
Notices, 37(1):4–16, 2002.
John Abela, Fran¸cois Coste, and Sandro Spina. Mutually compatible and incompatible merges
for the search of the smallest consistent dfa. In International Colloquium on Grammatical
Inference, pages 28–39. Springer, 2004.
St´ephane Ayache, R´emi Eyraud, and No´e Goudian. Explaining black boxes on sequential
data using weighted automata. In International Conference on Grammatical Inference, pages
81–103. PMLR, 2019.

[AEG19]

[ACS04]

[AFBKV15] Fides Aarts, Paul Fiterau-Brostean, Harco Kuppens, and Frits Vaandrager. Learning register
automata with fresh value generation. In International Colloquium on Theoretical Aspects of
Computing, pages 165–183. Springer, 2015.
Dana Angluin. Learning regular sets from queries and counterexamples. Information and
computation, 75(2):87–106, 1987.

[Ang87]

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 25

[AV07]

[AV10]

[BD17]

[BCG13]

[ANV11]

[BCLQ14]

[BABE11]

[BBEK14]

[BBB+15]

[BBA+13]

Joao Antunes, Nuno Neves, and Paulo Verissimo. Reverse engineering of protocols from network
traces. In 2011 18th Working Conference on Reverse Engineering, pages 169–178. IEEE, 2011.
Pieter Adriaans and Paul Vitanyi. The power and perils of mdl. In 2007 IEEE International
Symposium on Information Theory, pages 2216–2220. IEEE, 2007.
Fides Aarts and Frits Vaandrager. Learning i/o automata. In International Conference on
Concurrency Theory, pages 71–85. Springer, 2010.
Ivan Beschastnikh, Jenny Abrahamson, Yuriy Brun, and Michael D Ernst. Synoptic: Studying
logged behavior with inferred models. In Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of software engineering, pages 448–451,
2011.
Ivan Beschastnikh, Yuriy Brun, Jenny Abrahamson, Michael D Ernst, and Arvind Krishna-
murthy. Unifying fsm-inference algorithms through declarative speciﬁcation. In 2013 35th
International Conference on Software Engineering (ICSE), pages 252–261. IEEE, 2013.
Maurice Bruynooghe, Hendrik Blockeel, Bart Bogaerts, Broes De Cat, Stef De Pooter, Joachim
Jansen, Anthony Labarre, Jan Ramon, Marc Denecker, and Sicco Verwer. Predicate logic as a
modeling language: modeling and solving some machine learning and data mining problems
with idp3. Theory and Practice of Logic Programming, 15(6):783–817, 2015.
Ivan Beschastnikh, Yuriy Brun, Michael D Ernst, and Arvind Krishnamurthy. Inferring models
of concurrent systems from logs of their behavior with csight. In Proceedings of the 36th
International Conference on Software Engineering, pages 468–479, 2014.
Borja Balle, Jorge Castro, and Ricard Gavald`a. Learning probabilistic automata: A study in
state distinguishability. Theoretical Computer Science, 473:46–60, 2013.
Borja Balle, Xavier Carreras, Franco M. Luque, and Ariadna Quattoni. Spectral learning of
weighted automata. Machine learning, 96(1):33–63, 2014.
Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–
1082, 2017.
Alan W Biermann and Jerome A Feldman. On the synthesis of ﬁnite-state machines from
samples of their behavior. IEEE transactions on Computers, 100(6):592–597, 1972.
Antonia Bertolino, Paola Inverardi, Patrizio Pelliccione, and Massimo Tivoli. Automatic
synthesis of behavior protocols for composable web-services. In Proceedings of the 7th joint
meeting of the European software engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering, pages 141–150, 2009.
Benedikt Bollig, Joost-Pieter Katoen, Carsten Kern, Martin Leucker, Daniel Neider, and
David R Piegdon. libalf: The automata learning framework. In International Conference on
Computer Aided Veriﬁcation, pages 360–364. Springer, 2010.
Miguel Bugalho and Arlindo L Oliveira. Inference of regular languages using state merging
algorithms with search. Pattern Recognition, 38(9):1457–1467, 2005.
Chia Yuan Cho, Domagoj Babi´c, Pongsin Poosankam, Kevin Zhijie Chen, Edward XueJun Wu,
and Dawn Song. {MACE}:{Model-inference-Assisted} concolic exploration for protocol and
vulnerability discovery. In 20th USENIX Security Symposium (USENIX Security 11), 2011.
Jorge Castro and Ricard Gavalda. Towards feasible pac-learning of probabilistic determinis-
tic ﬁnite automata. In International Colloquium on Grammatical Inference, pages 163–174.
Springer, 2008.
Edmund Clarke, Orna Grumberg, Somesh Jha, Yuan Lu, and Helmut Veith. Counterexample-
guided abstraction reﬁnement. In International Conference on Computer Aided Veriﬁcation,
pages 154–169. Springer, 2000.
Soﬁa Cassel, Falk Howar, and Bengt Jonsson. Ralib: A learnlib extension for inferring efsms.
DIFTS. hp://www. faculty. ece. vt. edu/chaowang/di s2015/papers/paper, 5, 2015.
Weidong Cui, Jayanthkumar Kannan, and Helen J Wang. Discoverer: Automatic protocol
reverse engineering from network traces. In USENIX Security Symposium, pages 1–14, 2007.
[CMRRM21] Emilio Carrizosa, Cristina Molero-R´ıo, and Dolores Romero Morales. Mathematical optimiza-

[BKK+10]

[CBP+11]

[CGJ+00]

[CKW07]

[BIPT09]

[CHJ15]

[CG08]

[BO05]

[BF72]

[CO94]

tion in classiﬁcation and regression trees. Top, 29(1):5–33, 2021.
Rafael C Carrasco and Jose Oncina. Learning stochastic regular grammars by means of a
state merging method. In International Colloquium on Grammatical Inference, pages 139–152.
Springer, 1994.

26

S.VERWER AND C.HAMMERSCHMIDT

[COP16]

[CT04]

[CU13]

[CW98]

[CWKK09]

[DDE05]

[DlH10]

[DLZS17]

[EM18]

[EPG+07]

Pietro Cottone, Marco Ortolani, and Gabriele Pergola. Gl-learning: an optimized framework
for grammatical inference. In Proceedings of the 17th International Conference on Computer
Systems and Technologies 2016, pages 339–346, 2016. URL: https://github.com/piecot/
GI-learning.
Alexander Clark and Franck Thollard. Pac-learnability of probabilistic deterministic ﬁnite
state automata. Journal of Machine Learning Research, 5(May):473–497, 2004.
Daniil Chivilikhin and Vladimir Ulyantsev. Muacosm: a new mutation-based ant colony
optimization algorithm for learning ﬁnite-state machines. In Proceedings of the 15th annual
conference on Genetic and evolutionary computation, pages 511–518, 2013.
Jonathan E Cook and Alexander L Wolf. Discovering models of software processes from
event-based data. ACM Transactions on Software Engineering and Methodology (TOSEM),
7(3):215–249, 1998.
Paolo Milani Comparetti, Gilbert Wondracek, Christopher Kruegel, and Engin Kirda. Prospex:
Protocol speciﬁcation extraction. In 2009 30th IEEE Symposium on Security and Privacy,
pages 110–125. IEEE, 2009.
Pierre Dupont, Fran¸cois Denis, and Yann Esposito. Links between probabilistic automata and
hidden markov models: probability distributions, learning models and induction algorithms.
Pattern recognition, 38(9):1349–1371, 2005.
Colin De la Higuera. Grammatical inference: learning automata and grammars. Cambridge
University Press, 2010.
Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. Deeplog: Anomaly detection and
diagnosis from system logs through deep learning. In Proceedings of the 2017 ACM SIGSAC
conference on computer and communications security, pages 1285–1298, 2017.
Seyedeh Sepideh Emam and James Miller. Inferring extended probabilistic ﬁnite-state au-
tomaton models from software executions. ACM Transactions on Software Engineering and
Methodology (TOSEM), 27(1):1–39, 2018.
Michael D Ernst, Jeﬀ H Perkins, Philip J Guo, Stephen McCamant, Carlos Pacheco, Matthew S
Tschantz, and Chen Xiao. The daikon system for dynamic detection of likely invariants. Science
of computer programming, 69(1-3):35–45, 2007.

[GEP15]

[FBJV16]

[FBLP+17]

[FBJM+20] Paul Fiterau-Brostean, Bengt Jonsson, Robert Merget, Joeri De Ruiter, Konstantinos Sagonas,
and Juraj Somorovsky. Analysis of {DTLS} implementations using protocol state fuzzing. In
29th USENIX Security Symposium (USENIX Security 20), pages 2523–2540, 2020.
Paul Fiter˘au-Bro¸stean, Ramon Janssen, and Frits Vaandrager. Combining model learning and
model checking to analyze tcp implementations. In International Conference on Computer
Aided Veriﬁcation, pages 454–471. Springer, 2016.
Paul Fiter˘au-Bro¸stean, Toon Lenaerts, Erik Poll, Joeri de Ruiter, Frits Vaandrager, and
Patrick Verleg. Model learning and model checking of ssh implementations. In Proceedings
of the 24th ACM SIGSOFT International SPIN Symposium on Model Checking of Software,
pages 142–151, 2017.
Hadrien Glaude, Cyrille Enderli, and Olivier Pietquin. Spectral learning with proper probabili-
ties for ﬁnite state automation. In ASRU 2015-Automatic Speech Recognition and Understanding
Workshop. IEEE, 2015.
E Mark Gold. Complexity of automaton identiﬁcation from given data. Information and control,
37(3):302–320, 1978.
Christian Hammerschmidt, Samuel Marchal, Radu State, Gaetano Pellegrino, and Sicco Verwer.
Eﬃcient learning of communication proﬁles from ip ﬂow records. In 2016 IEEE 41st Conference
on Local Computer Networks (LCN), pages 559–562. IEEE, 2016.
John E Hopcroft, Rajeev Motwani, and Jeﬀrey D Ullman. Introduction to automata theory,
languages, and computation. Acm Sigact News, 32(1):60–65, 2001.
Marijn JH Heule and Sicco Verwer. Exact dfa identiﬁcation using sat solvers. In International
Colloquium on Grammatical Inference, pages 66–79. Springer, 2010.
Marijn JH Heule and Sicco Verwer. Software model synthesis using satisﬁability solvers.
Empirical Software Engineering, 18(4):825–856, 2013.
Christian Albert Hammerschmidt, Sicco Verwer, Qin Lin, and Radu State. Interpreting ﬁnite
automata for sequential data. arXiv preprint arXiv:1611.07100, 2016.

[HMS+16]

[HVLS16]

[HMU01]

[Gol78]

[HV13]

[HV10]

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 27

[HW16]

[IHS14a]

[IHS14b]

[IHS15]

[ISBF07]

[KABP14]

[Lan99]

[LAVM18]

[LHG17]

[LHPV16]

[LMP08]

[LPP98]

[LR05]

[LVD20]

[LY96]

[LZ04]

[Mai14]

[MAP+21]

[MPS16]

[NN98]

[NSV+12]

Mathew Hall and Neil Walkinshaw. Data and analysis code for gp efsm inference. In IEEE
International Conference on Software Maintenance and Evolution (ICSME), pages 611–611.
IEEE, 2016.
Malte Isberner, Falk Howar, and Bernhard Steﬀen. Learning register automata: from languages
to program structures. Machine Learning, 96(1):65–98, 2014.
Malte Isberner, Falk Howar, and Bernhard Steﬀen. The ttt algorithm: a redundancy-free
approach to active automata learning. In International Conference on Runtime Veriﬁcation,
pages 307–322. Springer, 2014.
Malte Isberner, Falk Howar, and Bernhard Steﬀen. The open-source learnlib. In International
Conference on Computer Aided Veriﬁcation, pages 487–495. Springer, 2015.
Kenneth L Ingham, Anil Somayaji, John Burge, and Stephanie Forrest. Learning dfa rep-
resentations of http for protecting web applications. Computer Networks, 51(5):1239–1255,
2007.
Timo Klerx, Maik Anderka, Hans Kleine B¨uning, and Steﬀen Priesterjahn. Model-based
anomaly detection for discrete event systems. In 2014 IEEE 26th International Conference on
Tools with Artiﬁcial Intelligence, pages 665–672. IEEE, 2014.
Kevin J Lang. Faster algorithms for ﬁnding minimal consistent dfas. NEC Research Institute,
Tech. Rep, 1999.
Qin Lin, Sridha Adepu, Sicco Verwer, and Aditya Mathur. Tabor: A graphical model-based
approach for anomaly detection in industrial control systems. In Proceedings of the 2018 on
asia conference on computer and communications security, pages 525–536, 2018.
Chen Luo, Fei He, and Carlo Ghezzi. Inferring software behavioral models with mapreduce.
Science of Computer Programming, 145:13–36, 2017.
Qin Lin, Christian Hammerschmidt, Gaetano Pellegrino, and Sicco Verwer. Short-term time
series forecasting with regression automata. 2016.
Davide Lorenzoli, Leonardo Mariani, and Mauro Pezz`e. Automatic generation of software
behavioral models. In Proceedings of the 30th international conference on Software engineering,
pages 501–510, 2008.
Kevin J Lang, Barak A Pearlmutter, and Rodney A Price. Results of the abbadingo one dfa
learning competition and a new evidence-driven state merging algorithm. In International
Colloquium on Grammatical Inference, pages 1–12. Springer, 1998.
Simon M Lucas and T Jeﬀ Reynolds. Learning deterministic ﬁnite automata with a smart
state labeling evolutionary algorithm. IEEE transactions on pattern analysis and machine
intelligence, 27(7):1063–1074, 2005.
Qin Lin, Sicco Verwer, and John Dolan. Safety veriﬁcation of a data-driven adaptive cruise
controller. In 2020 IEEE Intelligent Vehicles Symposium (IV), pages 2146–2151. IEEE, 2020.
David Lee and Mihalis Yannakakis. Principles and methods of testing ﬁnite state machines-a
survey. Proceedings of the IEEE, 84(8):1090–1123, 1996.
Steﬀen Lange and Sandra Zilles. Formal language identiﬁcation: Query learning vs. gold-style
learning. Information Processing Letters, 91(6):285–292, 2004.
Alexander Maier. Online passive learning of timed automata for cyber-physical production
systems. In 2014 12th IEEE International Conference on Industrial Informatics (INDIN),
pages 60–66. IEEE, 2014.
Edi Muˇskardin, Bernhard K. Aichernig, Ingo Pill, Andrea Pferscher, and Martin Tappler.
AALpy: An active automata learning library. In Automated Technology for Veriﬁcation and
Analysis - 19th International Symposium, ATVA 2021, Gold Coast, Australia, October 18-
22, 2021, Proceedings, Lecture Notes in Computer Science. Springer, 2021. URL: https:
//github.com/DES-Lab/AALpy.
Leonardo Mariani, Mauro Pezz`e, and Mauro Santoro. Gk-tail+ an eﬃcient approach to learn
software models. IEEE Transactions on Software Engineering, 43(8):715–738, 2016.
James R Norris and James Robert Norris. Markov chains. Number 2. Cambridge university
press, 1998.
Oliver Niggemann, Benno Stein, Asmir Vodencarevic, Alexander Maier, and Hans Kleine
B¨uning. Learning behavior models for hybrid timed systems. In Twenty-Sixth AAAI Conference
on Artiﬁcial Intelligence, 2012.

28

S.VERWER AND C.HAMMERSCHMIDT

[NVMY21]

[OG92]

[OS98]

[PLHV17]

[PMM17]

[PW93]

[RJ86]

[RSB05]

[SBB21]

[SG09]

[SK14]

Azqa Nadeem, Sicco Verwer, Stephen Moskal, and Shanchieh Jay Yang. Alert-driven attack
graph generation using s-pdfa. IEEE Transactions on Dependable and Secure Computing, 2021.
Jos´e Oncina and Pedro Garcia. Inferring regular languages in polynomial updated time. In
Pattern recognition and image analysis: selected papers from the IVth Spanish Symposium,
pages 49–61. World Scientiﬁc, 1992.
Arlindo L Oliveira and Joao P Marques Silva. Eﬃcient search techniques for the inference of
minimum size ﬁnite automata. In Proceedings. String Processing and Information Retrieval: A
South American Symposium (Cat. No. 98EX207), pages 81–89. IEEE, 1998.
Gaetano Pellegrino, Qin Lin, Christian Hammerschmidt, and Sicco Verwer. Learning behavioral
ﬁngerprints from netﬂows using timed automata. In 2017 IFIP/IEEE Symposium on Integrated
Network and Service Management (IM), pages 308–316. IEEE, 2017.
Fabrizio Pastore, Daniela Micucci, and Leonardo Mariani. Timed k-tail: Automatic inference
of timed automata. In 2017 IEEE International conference on software testing, veriﬁcation
and validation (ICST), pages 401–411. IEEE, 2017.
Leonard Pitt and Manfred K Warmuth. The minimum consistent dfa problem cannot be
approximated within any polynomial. Journal of the ACM (JACM), 40(1):95–142, 1993.
Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models. ieee
assp magazine, 3(1):4–16, 1986.
Harald Raﬀelt, Bernhard Steﬀen, and Therese Berg. Learnlib: A library for automata learning
and experimentation. In Proceedings of the 10th international workshop on Formal methods
for industrial critical systems, pages 62–71, 2005.
Donghwan Shin, Domenico Bianculli, and Lionel Briand. Prins: Scalable model inference for
component-based system logs. arXiv preprint arXiv:2106.01987, 2021.
Muzammil Shahbaz and Roland Groz. Inferring mealy machines. In International Symposium
on Formal Methods, pages 207–222. Springer, 2009.
Jana Schmidt and Stefan Kramer. Online induction of probabilistic real-time automata. Journal
of Computer Science and Technology, 29(3):345–360, 2014.

[SLTIM20] Maayan Shvo, Andrew C Li, Rodrigo Toro Icarte, and Sheila A McIlraith. Interpretable

[SO92]

[TE11]

[Tho00]

[TS92]

[UZS15]

[Vaa17]
[VEDLH14]

sequence classiﬁcation via discrete optimization. arXiv preprint arXiv:2010.02819, 2020.
Andreas Stolcke and Stephen Omohundro. Hidden markov model induction by bayesian model
merging. Advances in neural information processing systems, 5, 1992.
Fedor Tsarev and Kirill Egorov. Finite state machine induction using genetic algorithm based
on testing and model checking. In Proceedings of the 13th annual conference companion on
Genetic and evolutionary computation, pages 759–762, 2011.
Franck Thollard Thollard. Probabilistic dfa inference using kullback-leibler divergence and
minimality. In In Seventeenth International Conference on Machine Learning. Citeseer, 2000.
Jun-ichi Takami and Shigeki Sagayama. A successive state splitting algorithm for eﬃcient allo-
phone modeling. In Acoustics, Speech, and Signal Processing, IEEE International Conference
on, volume 1, pages 573–576. IEEE Computer Society, 1992.
Vladimir Ulyantsev, Ilya Zakirzyanov, and Anatoly Shalyto. Bfs-based symmetry breaking
predicates for dfa identiﬁcation. In International Conference on Language and Automata
Theory and Applications, pages 611–622. Springer, 2015.
Frits Vaandrager. Model learning. Communications of the ACM, 60(2):86–95, 2017.
Sicco Verwer, R´emi Eyraud, and Colin De La Higuera. Pautomac: a probabilistic automata
and hidden markov models learning competition. Machine learning, 96(1):129–154, 2014.
Sicco Verwer. Eﬃcient identiﬁcation of timed automata: Theory and practice. 2010.

[VWW10]

[Ver10]
[VTDLH+05] Enrique Vidal, Franck Thollard, Colin De La Higuera, Francisco Casacuberta, and Rafael C
Carrasco. Probabilistic ﬁnite-state machines-part i. IEEE transactions on pattern analysis and
machine intelligence, 27(7):1013–1025, 2005.
Sicco Verwer, Mathijs de Weerdt, and Cees Witteveen. A likelihood-ratio test for identifying
probabilistic deterministic real-time automata from positive data. In International Colloquium
on Grammatical Inference, pages 203–216. Springer, 2010.
Sicco Verwer and Yingqian Zhang. Learning decision trees with ﬂexible constraints and
objectives using integer optimization. In International Conference on AI and OR Techniques

[VZ17]

FLEXFRINGE: MODELING SOFTWARE BEHAVIOR BY LEARNING PROBABILISTIC AUTOMATA 29

[WGY18]

[WLD+13]

[WRSL21]

[WTD16]

in Constraint Programming for Combinatorial Optimization Problems, pages 94–103. Springer,
2017.
Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural
networks using queries and counterexamples. In International Conference on Machine Learning,
pages 5247–5256. PMLR, 2018.
Neil Walkinshaw, Bernard Lambeau, Christophe Damas, Kirill Bogdanov, and Pierre Dupont.
Stamina: a competition to encourage the development and assessment of software model
inference techniques. Empirical Software Engineering, 18(4):791–824, 2013.
Kandai Watanabe, Nicholas Renninger, Sriram Sankaranarayanan, and Morteza Lahijanian.
Probabilistic speciﬁcation learning for planning with safety constraints. In 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pages 6558–6565. IEEE,
2021.
Neil Walkinshaw, Ramsay Taylor, and John Derrick. Inferring extended ﬁnite state machine
models from software executions. Empirical Software Engineering, 21(3):811–853, 2016. URL:
https://github.com/neilwalkinshaw/mintframework.

[XHF+09] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I Jordan. Detecting
large-scale system problems by mining console logs. In Proceedings of the ACM SIGOPS 22nd
symposium on Operating systems principles, pages 117–132, 2009.
Ilya Zakirzyanov, Anatoly Shalyto, and Vladimir Ulyantsev. Finding all minimum-size dfa
consistent with given examples: Sat-based approach. In International Conference on Software
Engineering and Formal Methods, pages 117–131. Springer, 2017.

[ZSU17]

This work is licensed under the Creative Commons Attribution License. To view a copy of this
license, visit https://creativecommons.org/licenses/by/4.0/ or send a letter to Creative
Commons, 171 Second St, Suite 300, San Francisco, CA 94105, USA, or Eisenacher Strasse
2, 10777 Berlin, Germany

