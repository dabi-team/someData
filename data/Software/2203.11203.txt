2
2
0
2

t
c
O
2
1

]

G
L
.
s
c
[

2
v
3
0
2
1
1
.
3
0
2
2
:
v
i
X
r
a

Reinforcement learning for automatic quadrilateral
mesh generation: a soft actor-critic approach

Jie Pana, Jingwei Huangb, Gengdong Chengc, Yong Zenga,∗

aConcordia Institute for Information Systems Engineering, Concordia
University, Montreal, H3G 1M8, Quebec, Canada
bDepartment of Engineering Management & Systems Engineering, Old Dominion
University, Norfolk, 23529, Virginia, United States
cDepartment of Engineering Mechanics, Dalian University of
Technology, Dalian, 116023, Liaoning, China

Abstract

This paper proposes, implements, and evaluates a reinforcement learning
(RL)-based computational framework for automatic mesh generation. Mesh
generation plays a fundamental role in numerical simulations in the area of
computer aided design and engineering (CAD/E). It is identiﬁed as one of
the critical issues in the NASA CFD Vision 2030 Study. Existing mesh gener-
ation methods suﬀer from high computational complexity, low mesh quality
in complex geometries, and speed limitations. These methods and tools, in-
cluding commercial software packages, are typically semiautomatic and they
need inputs or help from human experts. By formulating the mesh genera-
tion as a Markov decision process (MDP) problem, we are able to use a state-
of-the-art reinforcement learning (RL) algorithm called “soft actor-critic” to
automatically learn from trials the policy of actions for mesh generation. The
implementation of this RL algorithm for mesh generation allows us to build a
fully automatic mesh generation system without human intervention and any
extra clean-up operations, which ﬁlls the gap in the existing mesh generation
tools.
In the experiments to compare with two representative commercial
software packages, our system demonstrates promising performance with re-
spect to scalability, generalizability, and eﬀectiveness.

Keywords: Reinforcement learning, mesh generation, soft actor-critic,

∗Corresponding author
Email address: yong.zeng@concordia.ca (Yong Zeng )

Preprint submitted to Neural Networks

October 13, 2022

 
 
 
 
 
 
neural networks, computational geometry, quadrilateral mesh

1. Introduction

Reinforcement learning (RL) has been researched and applied in many
ﬁelds, such as games (Silver et al., 2016), health care (Gottesman et al.,
2019), natural language processing (Wang et al., 2018b), and combinatorial
optimization, particularly for NP-hard problems (Mazyavkina et al., 2021).
However, it has rarely been applied to the area of computational geometry,
especially in the ﬁeld of mesh generation (Pan et al., 2021). Mesh generation
is a fundamental step in conducting numerical simulations in the area of
Computer-Aided Design and Engineering (CAD/E) such as ﬁnite element
analysis (FEA), computational ﬂuid dynamics (CFD), and graphic model
rendering (Gordon and Hall, 1973; Yao and Stillman, 2019). Mesh generation
techniques have been identiﬁed as one of the six basic capacities to build in
NASA’s Vision 2030 CFD Study (Slotnick et al., 2014).

Mesh generation discretizes a complex geometric domain (see Fig. 1 (a))
into a ﬁnite set of (geometrically simple and bounded) elements, such as 2D
triangles or quadrilaterals (see Fig. 1 (b) in 2D geometries) and tetrahedra or
hexahedra (in 3D geometries). Since the reliable automation and high qual-
ity of mesh representation matter signiﬁcantly to the numerical simulation
results, mesh generation has continued to be a signiﬁcant bottleneck in those
ﬁelds due to algorithm complexities, inadequate error estimation capabilities,
and complex geometries (Slotnick et al., 2014).

Strongly favored by engineering analysis, quadrilateral mesh generation
has been a critical research problem for decades. However, existing quadri-
lateral mesh generation methods require heuristic knowledge in algorithm
development and severely depend on preprocessing or postprocessing oper-
ations to maintain high mesh quality. Preprocessing includes decomposing
complex domains into multiple regular components (Liu et al., 2017) and gen-
erating favorable vertex locations (Remacle et al., 2013). Postprocessing (i.e.,
clean-up operations) is used to handle where the mesh consists of elements
other than quadrilaterals (e.g., triangular elements), ﬂat or inverted elements,
and irregularly connected elements. The operations include reducing the sin-
gularity (Verma and Suresh, 2017), performing iterative topological changes
(e.g., splitting, swapping, and collapsing elements) (Docampo-Sanchez and
Haimes, 2019), and mesh adaptation (Verma and Suresh, 2018).

2

Figure 1: Meshing problem. (a) The initial geometric boundary, B, is composed of piece-
wise linear segments, represented as a sequence of vertices [V1, V2, ..., VN ]; (b) The ﬁnal
mesh, Ω, is composed of a set of quadrilateral elements [Q1, Q2, ..., QM ].

Improving mesh quality with extra operations increases computational
complexity and slows the meshing speed. Many machine learning-based
methods are proposed to support mesh optimization and reconstruction (Zhang
et al., 2020; Yang et al., 2021; Gupta, 2020; Chen et al., 2018; Wang et al.,
2018a; Deﬀerrard et al., 2016). Nechaeva (2006) proposed an adaptive mesh
generation algorithm based on self-organizing maps (SOMs), which adapts a
given uniform mesh onto a target physical domain through mapping. Pointer
networks can generate triangular meshes, but the result is not robust and
contains intersecting edges (Vinyals et al., 2015). Papagiannopoulos et al.
(2021) proposed a triangular mesh generation method with three neural net-
works. The training data came from the constrained Delaunay triangulation
algorithm (Chew, 1989). The trained model could predict the number of
candidate inner vertices (to form a triangular element), the coordinates of
those vertices, and their connective relations with existing segments on the
boundary. Adapting to arbitrary and complex geometries is a shortcoming
because of the ﬁxed input scale and constrained diversity of training samples.
To achieve generalizability for arbitrarily shaped geometries, Zeng and
Cheng (1993) used a recursive algorithm to generate one element at a time by
three primitive rules to avoid taking the whole contour as the input. Yao et al.
(2005) improved the recursive method by using an artiﬁcial neural network
(ANN) to automate the rule learning. However, the quality and diversity of

3

Figure 2: A sequence of actions taken by the mesh generator to complete a mesh. At each
time step ti, an element (in red) is extracted from the current boundary (in blue). The
boundary is then updated by cutting oﬀ the element and serves as the meshing boundary
in the next time step ti+1. This process continues until the updated boundary becomes
an element.

training data cannot be achieved and limit the model performance.

To resolve this issue, Pan et al. (2021) formulated the mesh generation
algorithm by Zeng and Cheng (1993) into a Markov decision problem (MDP).
The formalized problem framework was addressed by reinforcement learning
(Sutton and Barto, 1998, 2018), as illustrated in Fig. 2. At each time
step, a quadrilateral element is generated from the domain boundary. The
boundary is inwardly updated by cutting oﬀ the generated element. In each
iteration, the meshing boundary evolves into a new boundary to generate
new elements, which is naturally a sequential decision-making process. In
the previous work (Pan et al., 2021), the present authors only used the RL
method to sample training data for a feedforward neural network model,
which did not fully exploit the potential of the RL models and resulted in an
extra sampling selection phase. This heuristic selection can easily cause an
imbalanced dataset and compromise model performance.

The present article resolves the abovementioned problem and provides an
RL-based computational framework, FreeMesh-RL, for quadrilateral mesh
generation that automatically collects balanced samples through trial-and-
error learning, as illustrated in Fig. 3. It aims to automatically provide high-
quality meshes for various complex geometries without human intervention.
A few challenges must be addressed: 1) the RL method needs to be robust and
less hyperparameter-sensitive; 2) the trade-oﬀ between the current element
quality and the remaining boundary quality shall be made to maintain overall

4

Figure 3: The RL-based computational framework for automatic mesh generation. The
agent acts as the mesh generator by implementing various RL techniques. It generates an
element after a state is perceived and improves the action by observing the rewards. The
environment models the meshing boundary and updates the boundary by cutting oﬀ the
element generated via the action.

high mesh quality; and 3) the meshing process should be ﬁnished in ﬁnite
steps. Compared with other RL algorithms (Lillicrap et al., 2015; Mnih et al.,
2016; Schulman et al., 2017; Fujimoto et al., 2018), soft actor-critic (SAC)
(Haarnoja et al., 2018a,b) is selected to address the mesh generation problem,
because of its capability allowing reuse of previous experience, good balance
between exploration and exploitation, stable learning eﬃciency, and being
less hyperparameter-sensitive.

The rest of the present paper is organized as follows. Section 2 introduces
the detailed formulation of mesh generation as an RL problem, including ac-
tion, state, and reward, and proposes an RL-based meshing architecture with
SAC. Section 3 explores the implementation details of each key concepts in
applying SAC to mesh generation, and evaluates the performance of the pro-
posed method in comparison with two state-of-the-art meshing approaches
that share the similar algorithmic strategy yet with the rules and knowledge
developed based on expert experience. Section 4 discusses the main ﬁndings
based on the experiment results and explains how the method can be applied
to a broader area both in RL and mesh generation. Section 5 concludes this
article.

5

2. RL based mesh generation

This section proposes an RL-based computational framework for auto-
matic mesh generation. The action formulation and state representation are
detailed ﬁrst. A reward function is then designed to meet mesh quality re-
quirements. Finally, the architecture of the proposed framework is explained.

2.1. Action formulation

In the mesh generation process (see Fig. 2), two important decisions
should be made: 1) how to choose a vertex (called the reference vertex in
this paper) from the boundary; and 2) how to decide on the other three
vertices to form a quadrilateral element.

The ﬁrst decision is to select a local reference region that has the least
boundary angle from the boundary. It reduces the formation of narrow re-
gions in the remaining boundary, and thus increases high-quality elements
generated by subsequent actions. The reference vertex V ∗
is selected using
i
the following equation:

V ∗
i = arg min

Vi

1
nrv

nrv(cid:88)

j=1

∠Vl,jViVr,j, i ∈ NB,

(1)

where NB is the number of vertices contained in the present boundary B;
Vl,j and Vr,j denote the j-th vertices at the left and right side (considering
clockwise orientation) of the reference vertex Vi along the boundary, respec-
tively; nrv represents how many surrounding vertices should be included; the
Vi is the i-th vertex on the boundary. An example of this selection is shown
in Fig. 4. The detailed selection is discussed in the experiment section.

The second decision is to choose an correct action to form a quadrilateral
element from three basic action types: adding zero, one, or two more vertices
(Zeng and Cheng, 1993). As shown in Fig. 5, the action is represented
by [type, V1, V2], where type ∈ {0, 1, 2}, corresponding to the three possible
actions; V1 and V2 are the coordinates of the newly added vertices. The
coordinate space for the vertices is constrained to a fan-shaped area (in light
blue) with radius r, which is calculated as follows:

r = α ∗ L,
n
(cid:88)

L =

1
2n

j=0

6

|Vl,jVl,j+1| + |Vr,jVr,j+1|, 0 < n < N/2

(2)

Figure 4: The angle calculation in reference vertex selection. For example, two surrounding
vertices of V0, nrv = 2, are used to calculate the angles ϑ1 and ϑ2, where ϑ1 = ∠Vl,1V0Vr,1,
ϑ2 = ∠Vl,2V0Vr,2. This calculation iterates along the boundary. The vertex with the least
averaged angle by ϑ1 and ϑ2 is selected as the reference vertex.

where α is a factor to amplify the base length L; Vl,j and Vr,j denote the j-th
vertex at the left and right side of the reference vertex along the boundary;
Vl,0 = Vi = Vr,0 is the reference vertex; and |VaVb| is the Euclidean distance
between two vertices. Usually, the action type = 2 is needed only on special
occasions (e.g., circular domains) at limited times. It is implemented in the
environment if these situations occur. The action [type, V1] is eventually used
in this article.

2.2. State representation

A state is an observation of the environment by the agent. The environ-
ment here is the meshing boundary (see Fig. 2). The full information for the
state of the environment at time t consists of all the vertices along the bound-
ary. However, not all the vertices are relevant and necessary to deciding what
action to take. Therefore, a partial observation of the boundary environment
is designed, which consists of a reference vertex determining where the agent
should start generating an element, and its surrounding vertices providing a
local environment.

A state at time t is denoted as st, as shown in Fig. 6, and is composed of
ﬁve components: (1) a reference vertex, V0, which is used as the relative origin
to generate the new element with an action at, and is calculated by Equation
1; (2) n neighboring vertices on the right side of the reference vertex; (3)
n neighboring vertices on the left side; (4) g neighboring points Vζ1, ..., Vζg
,
the closest vertices in the fan-shaped area ζ1, ..., ζg within a radius, which is

7

Figure 5: Action space for each rule type. Subﬁgures (a)-(c) correspond to three types
of actions, respectively. The blue area is the space to select the candidate vertices with
a radius r and the origin vertex V0 (reference vertex). The candidate vertices are V2 (in
type 1), and V2 and V3 (in type 2), respectively.

8

Figure 6: Partial observation of the meshing boundary. For example, the partial boundary,
where β = 4, n = 2, g = 3, is represented as the state. First, two vertices on the left and
right sides of the reference vertex V0 are selected. Second, the angle ∠Vl,1V0Vr,1 is evenly
split into three angles ζ1, ζ2, and ζ3; three fan-shaped areas are hence formed with these
angles and a radius Lr = 4 ∗ L. Then, the closest vertex in each area is selected. Vζ3 is an
intersection vertex between the bisector and the boundary segment.

calculated as:

Lr = β ∗ L,
where β is a factor to amplify the base length and L is calculated by Equation
2. When g = 3, the fan-shaped area is evenly divided into three parts,
ζ1 = ζ2 = ζ3, as shown in Fig. 6. If there are no vertices in a sliced fan-shaped
area (e.g., ζ3), the furthest bisector vertex in the slice or the intersection
vertex between the bisector of the slice and the boundary edge is selected,
in Fig. 6; and (5) ρt, the area ratio between the updated domain
such as Vζ3
and the original domain, which is used to indicate the meshing progress.

(3)

The current state St, representing the partial boundary around the refer-

ence point and meshing progress, is denoted as follows:

St = {Vl,n, ..., Vl,1, V0, Vr,1, ..., Vr,n, Vζ1, ..., Vζg , ρt}.

(4)

All the vertices are represented by a polar coordinate system with V0 as
the origin and −−−→
V0Vr,1 as the reference direction. There are a few geometri-
cal operations (i.e., rotation, scaling, and transit) to keep only the relative
information of the contained vertices (Yao et al., 2005).

2.3. Reward function

The criteria for high-quality meshing are as follows: 1) each element is a
quadrilateral; (2) each element should be as close to a square as possible, and

9

minimally, the inner corners of each element should be between 45° and 135°;
(3) the aspect ratio (the ratio of opposite edges) and taper ratio (the ratio of
neighboring edges) of each quadrilateral should be within a predeﬁned range;
and (4) the transition between a dense mesh and a coarse mesh should be
smooth (Zeng and Cheng, 1993; Zeng and Yao, 2009).

The reward function shall guarantee the quality requirements and step-
wisely measure the performance of each action. The action can cause three
situations: 1) if it forms an invalid element or intersects with the boundary,
the reward is set to -0.1; 2) if it generates the last element, the reward is set to
10; and 3) if it constructs a valid element, the reward is a joint measurement
of element quality, the quality of the remaining boundary, and the density.
Consequently, the reward function is represented as follows:

rt(st, at) =






−0.1,
10,
mt,

invalid element;
the element is the last element;
otherwise.

The measurement mt is calculated by the following equation:

mt = ηe

t + ηb

t + µt.

(5)

(6)

The element quality ηe
t

is calculated as follows:

is measured by its edges and internal angles, and

ηe
t =

qedge =

qangle =

(cid:112)
√

qedgeqangle,

2minj∈{0,1,2,3}{lj}
Dmax
minj∈{0,1,2,3}{anglej}
maxj∈{0,1,2,3}{anglej}

,

(7)

,

where qedge refers to the quality of edges of this element; lj is the length of
the jth edge of the element; Dmax is the length of the longest diagonal of
the tth element; qangle refers to the quality of the angles of the element; and
anglej is the degree of the jth inner angle of the element. The quality ηe
t
will range from 0 to 1, which is the greater the better. Examples of various
element qualities are shown in Fig. 7.

The quality of the remaining boundary ηb
t

is measured by both the quality
of the angles formed between the newly generated element and the boundary,

10

Figure 7: Element quality varies with diﬀerent shapes. The quality value ranges from 0
to 1. The element with the best quality 1 is a square.

and the shortest distance of the generated vertex to its surrounding edges,
and is denoted as follows:

(cid:115)

ηb
t =

qdist =

mink∈{1,2}{min(ςk, Mangle)}
Mangle
if dmin < (d1 + d2)/2;
otherwise.

(d1+d2)/2,
1,

(cid:40) dmin

qdist − 1,

(8)

where ςk refers to the degrees of the kth generated angle and dmin is the
distance of V2 to its closest edge. The details are shown in Fig. 8. The quality
It
ranges from -1 to 0, with a larger value representing better quality.
ηb
t
serves as a penalty term to decrease the reward if the quality of the remaining
boundary worsens. We set Mangle = 60◦. When the formed new angles are
It penalizes the generation of
less than Mangle, the quality will decrease.
sharp angles that are harmful to the overall mesh quality and may even fail
the meshing process.

These two qualities together represent the trade-oﬀ between the generated
element and the remaining boundary, ensuring the overall mesh quality. The
last term, density µt, secures the completion of the meshing process within
ﬁnite steps by controlling mesh density, which is calculated as follows:

µt =






−1,

At−Amin
Amax−Amin
0,

,

if At < Amin;
if Amin ≤ At < Amax;
otherwise.

(9)

where At is the area of the element generated at time t; Amin is the es-
timated minimum area that an element should meet, and is calculated by

11

Figure 8: The quality of the remaining boundary for two action types. Q1 is the newly
generated element. Once it is removed, it forms two angles ς1 and ς2 with existing boundary
segments. (a) When action type = 0, the boundary quality only hinges on these two angles,
assuming qdist = 1. (b) When action type = 1, the boundary quality is jointly measured
by the two angles, and the closest Euclidean distance dmin of the newly added vertex V2
to the existing segments. d1 and d2 are the Euclidean lengths of segments V2V3 and V1V2.

min

; Amax is the estimated maximum area of the element, and
Amin = υ · e2
(cid:1)2; emax and emin are the lengths
is calculated by Amax = υ (cid:0) emax−emin
of the longest and shortest edges in the boundary, respectively; κ adjusts
the estimated maximum area and is independent of the unit of edges (κ = 4
in our experiments); and υ is a weight and values in (0, 10], for which a
smaller value means a greater density, and vice versa. We set υ = 1 in our
experiments for the medium density.

+ emin

κ

2.4. Meshing scheme via SAC

The formulated RL-based meshing architecture, FreeMesh-RL, is shown
in Fig. 3. We formulate meshing process as an MDP process, consisting
of a set of boundary environment states S, a set of possible actions A(s),
a set of rewards R, and a state transition probability P (St+1, Rt+1|St, At).
The agent, at each time step t, observes a state St from the environment,
and conducts an action At applied to the environment. The environment
responds to the action and transitions into a new state St+1. It then reveals
the new state and provides a reward Rt to the agent. This process iterates
until a given condition is satisﬁed (i.e., the RL problem is solved). The
extraction process will produce a sequence [S0, A0, S1, R1, A1, ...]. The goal of
the meshing agent is to complete the meshing process for any given geometric
object while maintaining high mesh quality.

12

The RL algorithm deployed in this computational framework is the SAC
method. SAC is one of the state-of-the-art RL algorithms for continuous
action control problems (Haarnoja et al., 2018a,b). To overcome the sam-
ple complexity and hyperparameter-sensitivity, it adds an entropy term in
addition to the reward in the objective function, and maximizes the reward
return while maximizing the randomness of the policy.

Following (Haarnoja et al., 2018a), the objective function of the policy is

correspondingly denoted as

J(π) =

T
(cid:88)

t=0

E(st,at)∼ρπθ

[r(st, at) + αH(πθ(.|st))],

(10)

where H(.) is the entropy measure (Ziebart, 2010); ρπθ
is the state-action
marginal distribution of policy π parameterized by θ; and α indicates the
signiﬁcance of the entropy term, known as the temperature parameter. En-
tropy maximization allows the learned policy to act as randomly as possible
while guaranteeing task completion, which gains a trade-oﬀ between explo-
ration and exploitation and thus accelerates learning. This randomness is
especially important for a partially observable environment.

SAC combines Q-learning with stochastic policy gradient learning. Q-

learning is based on the Bellman equation

Q(st, at) = r(st, at) + γEst+1∼P (max
a(cid:48)

Q(st+1, a(cid:48))).

(11)

As we already know, in SAC, policy entropy maximization is introduced in
the RL objective of maximizing the expected return. Correspondingly, we
have the following form of the soft Bellman equation.

Q(st, at) = r(st, at) + γEst+1∼ρπ(s)[V (st+1)],

(12)

and

V (st) = Eat∼π[Q(st, at) − αlogπ(at|st)],
(13)
where ρπ(s) is the state marginals of the trajectory distribution induced by
a policy π(at|st).

SAC uses this soft Bellman equation to estimate the target soft Q-values,
and soft Q-learning in SAC aims to minimize the diﬀerence between the Q-
value estimated by a Q-function approximator with parameters θ and the
target soft Q-value as follows.

JQ(θ) = Est,at∼D

(Qθ(st, at) − ˆQ(st, at))2

(cid:21)

.

(cid:20)1
2

(14)

13

In training, all samples are taken from the replay buﬀer D, which is the
collection of previous experience, i.e.
(s, a, r, s(cid:48)) tuples. SAC uses neural
networks to approximate the soft Q-function and the policy.

To train Q-function neural network with parameters θ by using gradient

descendent, we can estimate the gradient as follows,

ˆ∇θJQ(θ) = ∇θQθ(st, at)(Qθ(st, at)−

(r(st, at) + γ(Q¯θ(st+1, at+1) − αlog(πφ(at+1|st+1)))),

(15)

where πφ is the current policy parameterized by a neural network with param-
eters φ. To simplify gradient descent, the target Q-value is calculated with a
diﬀerent neural network with parameters ¯θ, which is simply the correspond-
ing soft Q-function network with delayed parameter updates. Practically, ¯θi
is obtained as an exponential moving average of θi.

As in the TD3 (Twin Delayed DDPG) model (Fujimoto et al., 2018), SAC
has a neural network with parameters φ to approximate policy, two neural
networks, each with parameters θi (i = 1, 2), working together to approx-
imate the soft Q-function, and two neural networks, each with parameters
¯θi (i = 1, 2), to estimate target Q-values. The latter two neural networks
are simply the former two networks with the delayed updates of parameters
θi. In order to overcome the overestimation bias (Fujimoto et al., 2018), the
minimum of the two values estimated by the twin networks is used as the
estimated Q-value, i.e., Qθ(st, at) = min{Qθ1(st, at), Qθ2(st, at)}.

In the soft policy improvement stage, following (Haarnoja et al., 2018a),
the policy parameter can be updated by minimizing the expected Kullback-
Leibler (KL)-divergence. The learning objective can be expressed as follows,

J(φ) = Est∼D[Eat∼πφ[αlog(πφ(at|st)) − Qθ(st, at)]].

(16)

The policy is reparamerterized with a neural network transformation at =
fφ((cid:15)t; st), where (cid:15)t is an input noise vector and can be sampled from a ﬁxed
distribution.

Finally, the temperature α is updated by minimizing the objective

J(α) = Eat∼π[−αlogπφ(at|st) − α ¯H].

(17)

The details of the algorithm are shown in Algorithm 1. The parameters used
are detailed in the experiment section.

14

Algorithm 1 SAC for mesh generation.
1: Initialize parameters for Q networks and policy network, θ1, θ2, φ; initial-

ize replay buﬀer D; initialize environment

Select action at ∼ πφ(·|st)

2: Set target Q networks ¯θ1 ← θ1, ¯θ2 ← θ2
3: for time step t in range (1, NT ), do
4:
5: Observe reward rt+1 and new state st+1
Store (st, at, rt+1, st+1) in replay buﬀer D
6:
if st+1 is terminal, reset environment state
7:
if the size of replay buﬀer D > m then
8:
9:
10:

for gradient step j in range (1, NG), do

Sample a batch from replay buﬀer D with size m and calculate
ˆ∇θiJQ(θi), ˆ∇φJπ(φ)
Update soft Q-function θi ← θi − λQ
Update policy network φ ← φ − λπ
Update target network ¯θi ← τ θi + (1 − τ )¯θi for i ∈ {1, 2}
Adjust temperature α ← α − λ∇αJ(α)

ˆ∇θiJQ(θi) for i ∈ {1, 2}
ˆ∇φJπ(φ)

11:

12:
13:
14:
15:
end if
16:
17: end for

end for

3. Experimental results

In this section, we conducted two categories of experiments: 1) to identify
the optimal parameter settings for the proposed method, FreeMesh-RL and
2) to demonstrate the framework performance in scalability, generalizability,
and mesh quality.

3.1. Implementation settings

This section examines the optimal settings of RL algorithm selection,
training domain selection, SAC implementation, state representation, action
space, and reward function design. All the experiments are conducted on a
computer with an i7-8700 CPU and an Nvidia GTX 1080 Ti GPU with 32
GB of RAM. To evaluate the learning performance, we calculate the average
return for ten evaluation episodes at every 10k time steps.

15

Figure 9: Learning eﬃciency and performance comparison of various RL algorithms. (a)
Four types of RL algorithms, PPO, DDPG, TD3, and SAC, are chosen to compare the
learning eﬃciency over the same training domain. (b) The performance of the policies
learned by the four methods is examined on a domain. Their element quality in each mesh
is computed. DDPG and TD3 cannot complete the mesh because of the early convergence
of their meshing policy.

3.1.1. RL method selection

To be applied to real engineering problems, mesh generation requires a
robust and stable policy to be learned without complicated hyperparameter
tuning. We compared the learning eﬃciency of four types of RL algorithms
(i.e., PPO, DDPG, TD3, and SAC) over the same training domain. With
same hyperparameter settings in their original literature (Raﬃn et al., 2021),
the results are compared in Fig. 9 (a). In the early stage of training (< 5e4),
DDPG, TD3, and SAC achieve faster learning performance than the PPO
method. Then the learning speeds of DDPG and TD3 decrease, while SAC
maintains high speed and converges to a stable meshing policy. Although
PPO gradually achieves suboptimal performance, its policy severely oscil-
lates.

We also tested all the learned policies on a domain, as illustrated in
Fig. 9 (b). The policies learned by DDPG and TD3 do not converge and
cannot successfully mesh the domain. PPO can easily generate irregular
elements, causing poor mesh quality, whereas SAC has successfully meshed
the domain. Without further hyperparameter adjustment, the SAC method

16

achieves the fastest learning performance and the most stable meshing policy.
SAC appears to be an optimal method for mesh generation.

3.1.2. Training domain selection

In a real engineering environment, the meshing domains have diverse
shapes and topological structures. The basic domain features include sharp
angles, bottleneck regions, and unevenly distributed boundary segments for a
single connected 2D geometry. Therefore, the training domain should match
those criteria to ensure the richness of the samples. The meshing boundary
changes constantly during element generation, as shown in Fig. 2. The total
number of intermediate boundaries is equivalent to the number of elements
generated. This process will also increase the sample diversity.

To ﬁnd the appropriate training domain, we designed three candidate
domains, T1, T2, and T3, based on the above criteria, as shown in Fig. 10
(a-c). The training results using SAC are shown in Fig. 10(d). The agents
in both domains T1 and T2 achieve fast learning speed and converge to a
stable meshing policy, but T1 has less learning ﬂuctuation. The agent spends
more learning time in domain T3 because of the diﬃculties of having more
sharp angles and bottlenecks. We choose domain T1 as the training domain
because of its optimal steadiness and learning eﬃciency. As shown in the
rest of experiments, the model trained with T1 has strong generalizability.

3.1.3. SAC implementation

To identify the optimal NN hidden layer setting (i.e., the number of hid-
den layers and neurons in each layer) in SAC, we compared four diﬀerent
conﬁgurations, S1-S4, from the perspective of learning eﬃciency. The results
are compared in Fig. 11 (a). Conﬁguration S1 has the poorest performance
in learning the policy. The network structure S2 with three hidden layers,
[128, 128, 128], achieves the best learning performance while having fewer
parameters than S3 and S4. It is used in the article to approximate the soft
Q-function, policy, and target networks.

A random seed is a common parameter in RL, which often aﬀects learning
performance. We selected three random seeds and tested their impact on
policy learning, as shown in Fig. 11 (b). It turns out that the SAC learning
process is not sensitive to those seeds, which is one of its advantages compared
with other RL methods. The other hyperparameters used for SAC are listed
in Table 1.

17

Figure 10: Learning diﬃculty comparison over diﬀerent training domains. (a)-(c) Three
types of training domains, T1, T2, and T3, are designed based on the identiﬁed criteria.
(d) The training results over three domains using SAC. Domain T1 achieves a fast and
stable learning eﬃciency which has an easier meshing policy to learn than the other two
domains, whereas domain T3 is the hardest one to converge to a stable policy because
multiple sharp angles exist.

Figure 11: Comparisons of hidden layer conﬁgurations and random seeds for the SAC
algorithm. (a) Comparison of four types of hidden layer conﬁgurations: S1-S4 represent
four diﬀerent neural network structures in the hidden layers, including [32, 32], [128, 128,
128], [64, 64, 64, 64, 64], and [32, 128, 128, 128, 64, 32], respectively. Conﬁguration S2
achieves a fast and stable learning result. (b) Random seed comparison: three diﬀerent
random seeds, A (356), B (567), and C (999), are chosen to examine the learning diﬀerence.
Certainly, SAC is not sensitive to those random seeds and could achieve stable learning
results.

18

Table 1: Training hyperparameters for the SAC algorithm.

Parameter
ND
m
γ
λQ, λπ
NT
NG
τ

Description
Experience pool size
Minibatch size
Discount factor
Learning rate
Total time steps
Gradient steps
Soft update factor

Value
1e6
256
0.99
3e-4
1.2e6
1
5e-3

3.1.4. Agent’s view of environmental state

The state is the agent’s observation of the environment and provides
a decision basis for the agent. As partial observation is adopted in this
method, it is necessary to decide the observation range for the agent to learn
the meshing policy eﬀectively. The range of the observation is controlled
by three parameters, β (in Equation 3), n and g (in Equation 4). The
parameter β controls how far in the fan-shaped area the agent can observe
from the selected reference vertex while the other two parameters determine
how many vertices the agent perceives around the reference vertex. The
learning performance is compared with three types of settings, O1 (β =
4, n = 2, g = 3), O2 (β = 6, n = 2, g = 3), and O3 (β = 6, n = 3, g = 4).
The results are compared in Fig. 12. By comparing O1 and O2, it can be
found that further observation contributes to more return. This is because
the agent could adjust the position of the candidate vertex in advance to
avoid a conﬂict with the remaining boundary. On the other hand, the more
vertices in the fan-shaped areas are considered, the more information will
be embedded in the state. Therefore, increasing g could slow the learning
speed and delay policy convergence. This phenomenon can be observed when
comparing O2 and O3. The more information the agent observes, the more
time is needed to build the correlation.

We also evaluated the inﬂuence of the observation ranges on the mesh-
ing performance from the aspects of element quality (ηe in Equation 7), the
number of elements generated, and time cost. Table 2 presents those perfor-
mance comparisons. The policies learned by the observations of O1 and O2
have a small diﬀerence in element quality, whereas the one learned by O3 is

19

Figure 12: Learning eﬃciency comparison of diﬀerent agent’s observation ranges. The
observation range is formed by β_n_g, which represents the radius of the fan shape in
the state, the number of neighboring vertices on the left and right sides of the reference
vertex, and the number of vertices in the fan-shaped area. This range determines how far
and how much information the agent will observe in the meshing environment.

20

relatively poorer because of the slow learning speed. With the increase in
observed information, the number of generated elements also increases. This
is reasonable because the agent could make prudent decisions to avoid colli-
sion with the sensed boundary in a fan shape when it observes farther and
broader. As illustrated by the sample meshes, the interior area has smaller
and more regular elements by O2 and O3 as the boundary is updated in-
wardly. The meshing speed hinges on the number of generated elements,
which is indirectly inﬂuenced by the observation ranges because the agent
tends to make smaller elements if it observes a potential collision with the
sensed boundary. Ideally, the more information the agent observes, the more
accurate the decision it can make and the more learning time it requires to
converge. To achieve a trade-oﬀ between the computational cost and mesh
quality, the observation range O2 is used to represent the state in this article.

Table 2: Meshing results comparison of diﬀerent agents’ observation ranges. The observation ranges are
O1, O2, and O3. The performance is evaluated from three indicators, including element quality, the
number of generated elements, and time cost. A sample mesh (without smoothing) for each range is also
represented. All the experiments are repeated ten times over the same domain for each observation.

Observation

Sample mesh

Element quality

#elements

Time cost (s)

O1: 4_2_3

0.722 ± 0.14

198.8 ± 22.11

0.6 ± 0.09

O2: 6_2_3

0.712 ± 0.13

212.8 ± 24

0.64 ± 0.08

O3: 6_3_4

0.689 ± 0.137

217.1 ± 11.16

0.7 ± 0.13

#elements - the number of generated elements.

21

3.1.5. Action space

The agent’s action space deﬁnes the shape and size scale of each element
to be generated, which inﬂuences the mesh quality and policy’s learning ef-
ﬁciency. This section will examine the appropriate size of the search space,
i.e., the coordinate space to select the candidate vertex in forming a quadri-
lateral element. The coordinate space, speciﬁed by the radius in Equation
2, is deﬁned by weight α and a base length L. The number of vertices in
calculating the base length is equivalent to the number of neighboring ver-
tices of the reference vertex in the state (see the parameter n in Equation 4).
The ideal observation range (i.e., state) is determined as O2 in the previous
section. The parameter n is hence set to 2.

The size of the search space is deﬁned by the radius of the viewing fan
shape, determined by the weight α. To ﬁnd the appropriate radius, three
types of settings, R1 (α = 1), R2 (α = 2), and R3 (α = 3), are examined
during meshing policy learning over the same domain (see Fig. 10 (a)). Their
learning results are shown in Fig. 13. The radius R1 achieves the fastest
learning speed and converges earlier than the other two settings. Because
its search space is the smallest, action exploration is less needed. Although
the radius R2 is slightly slower in convergence than R1, it facilitates the
highest reward return indicating optimal mesh quality, due to the larger
search space. Unfortunately, the agent fails to learn an eﬃcient policy by
radius R3. The large search space contains sparse eﬃcient actions, and makes
agent’s exploration harder. Therefore, the radius R2 is used in the present
article.

3.1.6. Reward function

There are three terms in the reward function: element quality ηe
t

, the
quality of the remaining boundary ηb
, and density µt. The ﬁrst two terms
t
guarantee the element quality and the ease of continuous meshing. The last
term controls the meshing density by the parameter υ, which adjusts the
minimum element size tolerated, and ensures the mesh termination within
ﬁnite steps. The ﬁrst two terms are necessary and not changeable once the
quality requirement is determined, whereas the third term can be adjusted
according to diﬀerent requirements for mesh density.

To achieve the optimal mesh density, three diﬀerent parameters are com-
pared, including sparse (υ = 1.5), medium (υ = 1), and dense (υ = 0.5)
settings. The results are compared in Fig. 14 (a)-(c). We also examine the
number of elements generated by each density, as shown in Fig. 14 (d). The

22

Figure 13: Learning eﬃciency comparisons of three types of radius settings in the action.
The appropriate radius is necessary for optimal mesh quality and learning eﬃciency. These
settings are R1 (α = 1), R2 (α = 2), and R3 (α = 3). A larger α indicates a larger
coordinate space for searching a candidate vertex used to form a quadrilateral element.

23

Figure 14: Diﬀerent mesh densities controlled by the reward function. Three diﬀerent
densities from sparse to dense, (a)-(c), are controlled by the parameters υ = 1.5, υ = 1,
and υ = 0.5, respectively. Subﬁgure (d) shows the results of the number of generated
elements by three types of densities averaged over ten episodes.

results are averaged over 10 episodes. The diﬀerence in the number of ele-
ments between sparse and medium is approximately 20, while the diﬀerence
between medium and dense density is approximately 50 elements in the test-
ing domain. The medium density is ideal and used across all the remaining
experiments.

3.2. Eﬀectiveness evaluation

The eﬀectiveness of FreeMesh-RL is evaluated from the perspectives of
scalability, generalizability, and mesh quality. The model used in the follow-
ing experiments is trained on the same domain (i.e., domain T1 in Fig. 10
(a)) without any adjustment.

24

3.2.1. Scalability veriﬁcation

To validate the scalability of the learned meshing policy, we constructed
three geometry domains with the same shape but diﬀerent vertex densities
(i.e., 6.8: 9.9: 20.1, as shown in Table 4) on the boundaries. Three domains
are meshed by the same RL model, and the results are shown in Table 3. It
can be seen that all the domains have successfully meshed; the elements on
the boundaries are denser than in the interior area, which is beneﬁcial for
reducing computational burden; and the transitions between dense and coarse
meshes are smooth. The meshing speed over three domains is approximately
237 elements per second on average. Consequently, the results show that
the learned mesh policy achieves good scalability to diﬀerent scales of the
meshing problem and is not constrained to the density requirements of the
training domain. This scalability is a special manifestation of the broader
generalizability to be discussed next.

Table 3: Meshing the same domain with diﬀerent boundary segment densities by FreeMesh-
RL. The boundary shapes of domains 1-3 are the same, but the number of vertices on the
boundary is from low to high. The scalability is examined by meshing all the domains
with the same trained model.

Domain 1

Domain 2

Domain 3

3.2.2. Generalizability veriﬁcation

To verify the generalizability of the obtained meshing model, we meshed
four diﬀerent domains, domains 4-7, using the same model, trained with
domain T1 as shown in Fig. 10. The shapes of the domains are from simple
to complex. The meshing process and results are shown in Fig. 15. The
general meshing process starts from the boundary and advances inwardly to
the central area of the domain. Regardless of the complexity of the initial
domains, the boundary will evolve into various intermediate shapes with the

25

Table 4: Geometrical details of three domain boundaries and their generated meshes. The
perimeter, the number of vertices and vertices per unit length on the initial boundaries of
three domains are compared as well as their number of generated elements and meshing
time.

#vertices
Perimeter
#vertices per unit length
#elements
Execution time (s)

Domain 1 Domain 2 Domain 3
150
15.1
9.9
665
2.6

304
15.1
20.1
2157
15.3

102
15.1
6.8
289
0.9

#vertices - the number of vertices on the boundary.
#elements - the number of generated elements.

meshing process guided by the learned meshing policy. The dynamic changes
of the domain boundary also reﬂects the generalizability, i.e., how good the
policy to handle diﬀerent shapes.

The number of actions executed in meshing each of the four domains is
shown in Fig. 16. The meshing policy mainly consists of two types of actions
(see type 0 and type 1 in Fig. 5), and the execution number of type 1 is
approximately 80% of the total number. Each action, a single execution of
the policy, will produce an element. The number of intermediate boundaries
equals the number of elements generated. For example, in domain 7, there
are 1489 (the sum of execution times of type 0 and type 1) intermediate
boundaries in total. The meshing policy succeeds in meshing 1489 diﬀerent
domain boundaries. It can be noted that all the intermediate boundaries of
the four domains gradually become an oval shape in Fig. 15. This demon-
strates that the meshing policy can solve complex boundary situations with
sharp angles and create smooth and easy-to-handle situations for future ele-
ment generation.

Although the geometry shape can be diverse and inﬁnite in real engi-
neering applications, our state representation with a partial boundary and
the use of a reference vertex as the origin make our model general and able
to handle various boundary shapes. As demonstrated above, the obtained
policy is able to mesh hard situations while maintaining the high quality
of the remaining boundaries. Therefore, the most challenging part of the
whole meshing process is to mesh the initial boundary situations of a given
domain. However, it can be easily solved by providing training domains with

26

suﬃcient diﬃculties (in Fig. 10 (a-c)). To conclude, the proposed method
can be applied to arbitrary domains and is general to various geometries.

3.2.3. Comparison to conventional methods

To evaluate the meshing performance, we compare the quality of meshes
by FreeMesh-RL with two other representative meshing approaches over three
predeﬁned 2D domains (i.e., domains D7-9). These domains possess diﬀer-
ent features, including sharp angles, bottleneck regions, unevenly distributed
edges, and holes, to increase the testing diversity. The two conventional
meshing approaches are Blossom-Quad (Remacle et al., 2012) and Pave
(Blacker and Stephenson, 1991; White and Kinney, 1997). Blossom-Quad
is an indirect method that generates quadrilateral elements by ﬁnding the
perfect matching of a pair of triangles generated in advance. The method
is implemented by an open source generator Gmsh (Geuzaine and Remacle,
2009). Pave is another state-of-the-art meshing method for directly gener-
ating quadrilateral elements, implemented by the CUBIT software (Blacker
et al., 2016).

The meshing results are shown in Table 5. Although all the methods
can complete the meshes for the three domains, there are some subtle diﬀer-
ences. Only FreeMesh-RL generates fully quadrilateral meshes. The other
two methods have diﬃculties in discretizing the domains into full quadrilater-
als, containing triangles in domains (marked in yellow color in each domain).
Speciﬁcally, Blossom-Quad has a problem in handling domains with sharp
angles along the boundary, while Pave has issues in the interior of the domain.
Extra operations (e.g., clean-ups) are thus usually required to eliminate those
triangles or bad elements. Another advantage of FreeMesh-RL is that the
generated mesh can smoothly transition from very small to large elements
over a short distance, as shown in Table 5 (domain 9), which is beneﬁcial in
reducing the computational burden during simulations (Shewchuk, 2012).

To quantitatively analyze the meshing results of the three methods, we
selected eight common quality metrics, including 1) singularity, the number
of irregular nodes whose number of incident edges in the interior of a mesh is
not equal to four; 2) element quality, ηe in Equation 7; 3) |M inAngle − 90◦|;
4) |M axAngle − 90◦|; 5) scaled Jacobian, the minimum Jacobian (Knupp,
2000) at each corner of an element divided by the lengths of the two edge
vectors; 6) stretch, the degree of deformation; 7) taper, the maximum ab-
solute diﬀerence between the value one and the ratio of two triangles’ areas
are separated by a diagonal within a quadrilateral element; and 8) the num-

27

Figure 15: Model generalizability veriﬁcation. Four diﬀerent domains with shapes from
simple to complex are selected; their meshing procedures are presented to exhibit the
boundary evolution process and general meshing pattern by the learned policy. No matter
how simple an initial shape was, the intermediate shapes can be complex; no matter how
complex the shape of the initial boundary was, it gradually evolves into a smooth oval
shape in the center of the domain.

28

Figure 16: The execution times of the meshing policy over the four domains. The meshing
policy mainly consists of two types of actions: types 0 and 1. The number over each bar
indicates the execution number of each action type.

29

Table 5: Meshing results comparison. Three domains are designed to examine the meshing performance
based on the four identiﬁed criteria: a. sharp angle, b. narrow region, c. unevenly distributed boundary
segments, and d. having a hole inside. Both domains 7 and 8 possess features a and b, whereas domain 9
has features c and d. Two representative methods, Blossom-Quad and Pave, are chosen to compare the
meshing performance with the proposed method, FreeMesh-RL.
Domain 7

Algorithms

Domain 8

Domain 9

Blossom-
Quad

Pave

FreeMesh-
RL

The elements in yellow represent existing triangles in the meshes.

ber of triangles (#triangles) (Pan et al., 2021; Knupp et al., 2006). Smaller
singularity and taper mean better regularity, which can provide more accu-
rate results for numerical simulations. A larger scaled Jacobian indicates
higher convexity of the mesh, containing less inverted and ﬂat quadrilateral
elements. The existence of triangles indicates that the domain is not fully
meshed by quadrilaterals and usually require extra treatments to eliminate.
All the measurement results are averaged over three domains and are
shown in Table 6. The proposed method, FreeMesh-RL, outperforms other
methods in the indices of singularity, taper, scaled Jacobian, and number of
triangles, while being comparable with other best performing indices by Pave.
Pave achieves the best performance in the remaining indices (i.e., element
quality, min and max angles, and stretch), whereas Blossom-Quad has the
lowest quality and is only slightly better than Pave at having fewer triangles
that are not expected.
It is common for indirect methods (i.e., Blossom-
Quad) to have suboptimal performance because of the dependence on prior

30

triangulation. The computational complexities for the three methods are all
O(n2) (Pan et al., 2021).

Table 6: Quantitative measurement of the meshing performance of the three methods.
All the quality metrics are averaged over the three domains (i.e., domains 7-9).

Metrics
Singularity (L)
Element quality (H)
|M inAngle − 90◦| (L)
|M axAngle − 90◦| (L)
Scaled jacobian (H)
Stretch (H)
Taper (L)
#Triangle (L)

Blossom-Quad
388 ± 209.50
0.72 ± 0.12
6.55 ± 6.91
22.16 ± 11.14
0.91 ± 0.08
0.79 ± 0.08
0.15 ± 0.11
2.70 ± 2.50

Pave
FreeMesh-RL
146.70 ± 51.50
132 ± 50
0.79 ± 0.13
0.79 ± 0.12
4.02 ± 5.10
3.69 ± 4.60
15.69 ± 14.71 15.73 ± 12.48
0.94 ± 0.10
0.83 ± 0.11
0.11 ± 0.11
0 ± 0

0.94 ± 0.13
0.84 ± 0.10
0.12 ± 0.14
8 ± 2.80

L and H indicate whether the lower value or higher value is preferred, respec-
tively. The value in bold means the best among other approaches in that
speciﬁc metric.

The boxplots of the quality metrics are compared in Fig. 17, which shows
the steadiness and extremely poor situations in each quality measure. Pave
has the most outliers in all the quality metrics except singularity and number
of triangles, indicating that more elements exist with extremely low quality
than others, although it maintains the optimal averaged performance in some
metrics. Although Blossom-Quad has the lowest average performance, its
generated meshes have the steadiest quality. FreeMesh-RL is in the middle
regarding quality steadiness, generating less extreme low quality elements
than Pave and maintaining high averaged performance. The more unstable
and poor situations require more postprocessing operations.

We also analyzed the knowledge dependence during the algorithm de-
velopment, as shown in Table 7. The development of the meshing method
can be brieﬂy divided into three stages: preprocessing, element generation,
and postprocessing. Since Blossom-Quad is an indirect quadrilateral mesh
generation method, it requires the preprocessing of triangulation, which is
unnecessary for the other two methods. To generate elements, the devel-
opment of Blossom-Quad requires more complex geometric knowledge than
Pave and FreeMesh-RL (see details in (Pan et al., 2021)). Pave, however,
relies on considerable heuristic knowledge, which is time-consuming and inef-
ﬁcient for designers. FreeMesh-RL is the simplest method and only demands

31

Figure 17: Meshing performance comparison results over eight types of quality indices.
BQ represents the Blossom-Quad method; F-RL represents the FreeMesh-RL method. L,
H indicate if the lower value or higher value is preferred, respectively.

32

basic geometry concepts. Extra treatments such as clean-ups are necessary
for Blossom-Quad and Pave, whereas FreeMesh-RL is a clean-up free method.

Table 7: Comparison of knowledge dependence of the three methods during algorithm development. The algorithm
development procedure is divided into three stages: preprocessing, element generation, and postprocessing.

Steps
Preprocessing

Blossom-Quad
Delaunay triangulation

Pave
—

FreeMesh-RL
—

Element generation Complex geometry knowledge Heuristic knowledge Basic geometry concepts

Postprocessing

Clean-ups

Clean-ups

—

4. Discussion

Herein, we present a fully automatic quadrilateral mesh generation algo-
rithm, FreeMesh-RL, by using SAC reinforcement learning. With minimal
knowledge inputs, this algorithm automatically learns mesh generation policy
and performs with comparable quality to commercial systems that conduct
quadrilateral mesh generation. This section will discuss the factors behind
the performance of the proposed algorithm.

What are the big challenges in mesh generation?

The big challenges in mesh generation come from two aspects.

(1) In
real engineering problems, the geometric domains are very complex, having
various shapes, diverse scales, and diﬀerent mesh requirements. (2) In mesh
generation tools, the rules and knowledge manually acquired are insuﬃcient
to tackle all of the situations in a mesh generation problem, especially for
large and complex geometries. For example, postprocessing is mandatory
to guarantee good quality mesh in Pave and Blossom-Quad, the two repre-
sentative state-of-the-art systems that we discussed in the last section. The
fundamental problem is how to make a mesh generation algorithm general
enough to handle various geometric domains with diﬀerent engineering re-
quirements.

How does FreeMesh-RL tackle the challenges?

To tackle the challenges in mesh generation, particularly to achieve gen-
eralizability for arbitrarily shaped geometries, Zeng and Cheng (1993) ﬁrst
proposed an element-wise approach by using a recursive algorithm to gener-
ate one element at a time with three primitive rules shown in Figure 5. This
element-wise approach exhibits a similar problem structure in reinforcement

33

learning. As such, we formulated the mesh generation problem as an RL
problem (Pan et al., 2021), as illustrated in Figure 3. Generally, RL is a ma-
chine learning paradigm whose success essentially depends on two aspects:
(1) the domain-dependent problem formulation, that is, how to represent the
environment, states, actions, and rewards; (2) a suitable RL method. This
article presents our work on (1) how to optimally design the representation
of states, actions, and rewards; (2) how to apply SAC RL to solve the mesh
generation problem eﬀectively.

The problem formulation needs insightful abstraction. The three primi-
tive element extraction rules proposed in Zeng and Cheng (1993) shown in
Figure 5 lays a foundation for the problem formulation. The use of reference
vertex makes it possible to have a general representation of the problem,
avoiding to deal diﬀerent shapes and dynamically changing boundaries. We
use a partial observation of the environment as the state, thus focusing on
the most relevant information of the environment for actions. The reward
function design plays a critical role in mesh quality and optimal trade-oﬀ of
the current reward (the quality of the element to generate) and long-term re-
turn (the overall mesh quality). A general observation is that it is critical to
make the information represented by states well cover the factors of rewards.
With respect to RL methods, we select SAC mainly for two reasons. First,
its oﬀ-policy mechanism allows us to reuse previous experience, thus reducing
sampling complexity and making learning more eﬃcient. Secondly and im-
portantly, its distinguishing feature of maximizing policy randomness while
maintaining policy performance oﬀers a natural mechanism for an eﬀective
trade-oﬀ between exploration and exploitation in policy search. This fea-
ture of the stochastic policy is particularly beneﬁcial for learning in partially
observable environments.

Our problem formulation, together with the SAC RL algorithm, has
demonstrated strong generalizability. As presented in last section, the model
trained with a single domain (see Fig. 10 (a)) can mesh various other unseen
domains, as shown in Table 3, Table 5, and Fig. 15.

The major advantages of the proposed FreeMesh-RL algorithm lie in two
aspects: (1) it can automatically acquire and reﬁne knowledge for mesh gen-
eration, which is generally labor-intensive and time-consuming, and (2) the
automatic knowledge acquisition process does not need predeﬁned samples
and labeled data. These advantages eﬀectively tackle some practical chal-
lenges in commercial mesh generation software systems.

34

What is the distinguishing feature of this work?

The presented research demonstrates how we achieve the integrated intel-
ligence exhibited by the strong generalizability in mesh generation by combin-
ing SAC reinforcement learning with the rule-based knowledge representation
for states, actions, and rewards.

In Zeng and Cheng (1993), the authors developed a rule-based knowledge
system for mesh generation. They created three primitive element extraction
rules, which provide a framework for mesh generation problem representa-
tion. However, to deal with various boundaries, many rules are needed,
but knowledge acquisition is a bottleneck. To address this issue, Yao et al.
(2005) attempted to acquire knowledge for element extraction with MLP neu-
ral networks. They introduced the use of reference vertex, which makes the
mesh generation problem able to be represented in the same relative space,
thus being a MIMO mapping and suitable for using MLP neural networks.
Since MLP is a supervised machine learning model, the availability of a large
number of samples became a new bottleneck. To solve this problem, we for-
mulated mesh generation as an RL problem in (Pan et al., 2021), took RL
as a mechanism to generate meshing samples, selected good quality samples
with rules, then fed the selected samples to MLP for training.

In the present article, we further integrate the rule-based knowledge for
representing states, actions, and rewards into the framework of SAC rein-
forcement learning and realize a fully automatic mesh generation system. In
this system, rules govern the location of reference vertex, the construction
of states, and the quality evaluation of the generated element and the rest
of the shape. SAC does the job of exploring possible actions to generate
a new element in the current shape, learning to evaluate the quality of the
actions, and learning the optimal policy to generate a new element based on
the current shape of the geometric domain to mesh.

What’s next?

As observed in the last section, the performance of the SAC RL algorithm
varies with not only its hyperparameters but also the problem formulation
and representation and the associated parameter settings. On the other hand,
2D mesh generation is a visible and easily understood problem. We believe
this study of automatic 2D mesh generation with RL can be developed into
an RL testbed for RL research.

FreeMesh-RL is currently limited to 2D domains. We will apply the
proposed framework to 3D mesh generation by reformulating the state rep-

35

resentation and a few primitive actions for hexahedron elements as well as
the reward function.

In the fourth industrial revolution, engineering is transforming into dig-
ital engineering (Huang et al., 2021; Huang, 2022), where digital data and
models will be shared across the engineering lifecycle. This will make it pos-
sible to collect a large volume of meshing examples. Currently, our model is
trained with just a single simple geometric domain and shows the capabil-
ity to mesh in comparable quality to representative commercial software. In
real-world engineering for 3D meshing, there will be various challenging geo-
metric domains and meshing requirements for diﬀerent purposes. We intend
to leverage transfer learning (Bozinovski and Fulgosi, 1976; Zhuang et al.,
2021) and the available big data of real-world mesh samples to develop deep
neural networks, which will be pretrained with excellent samples and can be
repurposed by further training with a few samples for a speciﬁc task with
speciﬁc meshing requirements.

5. Conclusion

This article presented our research on applying soft actor-critic reinforce-
ment learning for automatic mesh generation. We designed and implemented
FreeMesh-RL, a fully automatic quadrilateral mesh generation algorithm
with SAC reinforcement learning. With minimal knowledge inputs, this
algorithm automatically learns mesh generation policy and performs with
comparable quality to commercial systems that conduct quadrilateral mesh
generation. Further research can go in several directions. We will extend
our current 2D automatic mesh generation algorithm with RL for 3D mesh
generation. Also, based on this study of automatic 2D mesh generation with
SAC RL, we will develop it into an RL testbed for RL research.

Acknowledgment

The support of the NSERC Discovery Grant (RGPIN-2019-07048) is

gratefully acknowledged.

References

Blacker, T.D., Owen, S.J., Staten, M.L., Quadros, W.R., Hanks, B., Clark,
B.W., Meyers, R.J., Ernst, C., Merkley, K., Morris, R., et al., 2016. CUBIT

36

geometry and mesh generation toolkit 15.2 user documentation. Techni-
cal Report SAND2016-1649 R. Sandia National Laboratory. Albuquerque,
New Mexico.

Blacker, T.D., Stephenson, M.B., 1991. Paving: a new approach to auto-
mated quadrilateral mesh generation. International Journal for Numerical
Methods in Engineering 32, 811–847.

Bozinovski, S., Fulgosi, A., 1976. The inﬂuence of pattern similarity and
transfer learning upon training of a base perceptron b2, in: Proceedings
of Symposium Informatica, pp. 121–126.

Chen, R.T., Rubanova, Y., Bettencourt, J., Duvenaud, D., 2018. Neural

ordinary diﬀerential equations. arXiv:1806.07366 .

Chew, L.P., 1989. Constrained delaunay triangulations. Algorithmica 4,

97–108.

Deﬀerrard, M., Bresson, X., Vandergheynst, P., 2016. Convolutional neu-
ral networks on graphs with fast localized spectral ﬁltering. Advances in
Neural Information Processing Systems 29, 3844–3852.

Docampo-Sanchez, J., Haimes, R., 2019. Towards fully regular quad mesh

generation, in: AIAA Scitech 2019 Forum, p. 1988.

Fujimoto, S., Hoof, H., Meger, D., 2018. Addressing function approximation
International Conference on Machine

error in actor-critic methods, in:
Learning, PMLR. pp. 1587–1596.

Geuzaine, C., Remacle, J.F., 2009. Gmsh: a 3-D ﬁnite element mesh gener-
ator with built-in pre-and post-processing facilities. International Journal
for Numerical Methods in Engineering 79, 1309–1331.

Gordon, W.J., Hall, C.A., 1973. Construction of curvilinear co-ordinate
International Journal for

systems and applications to mesh generation.
Numerical Methods in Engineering 7, 461–477.

Gottesman, O., Johansson, F., Komorowski, M., Faisal, A., Sontag, D.,
Doshi-Velez, F., Celi, L.A., 2019. Guidelines for reinforcement learning
in healthcare. Nature Medicine 25, 16–18.

37

Gupta, K., 2020. Neural mesh ﬂow: 3D manifold mesh generation via diﬀeo-

morphic ﬂows. Master’s thesis. University of California, San Diego.

Haarnoja, T., Zhou, A., Abbeel, P., Levine, S., 2018a. Soft actor-critic:
oﬀ-policy maximum entropy deep reinforcement learning with a stochas-
tic actor, in: International Conference on Machine Learning, PMLR. pp.
1861–1870.

Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Ku-
mar, V., Zhu, H., Gupta, A., Abbeel, P., et al., 2018b. Soft actor-critic
algorithms and applications. arXiv:1812.05905 .

Huang, J., 2022. Digital engineering transformation with trustworthy AI for
industry 4.0. Journal of Integrated Design and Process Science 26, 1–20,
(in print).

Huang, J., Beling, P., Freeman, L., Zeng, Y., 2021. Trustworthy AI for digi-
tal engineering transformation. Journal of Integrated Design and Process
Science 25, 1–7.

Knupp, P.M., 2000. Achieving ﬁnite element mesh quality via optimization of
the jacobian matrix norm and associated quantities. Part II—a framework
for volume mesh optimization and the condition number of the jacobian
matrix. International Journal for Numerical Methods in Engineering 48,
1165–1185.

Knupp, P.M., Ernst, C., Thompson, D.C., Stimpson, C., Pebay, P.P., 2006.
The verdict geometric quality library. Technical Report. Sandia National
Laboratories.

Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Sil-
ver, D., Wierstra, D., 2015. Continuous control with deep reinforcement
learning. arXiv:1509.02971 .

Liu, C., Yu, W., Chen, Z., Li, X., 2017. Distributed poly-square mapping
for large-scale semi-structured quad mesh generation. Computer-Aided
Design 90, 5–17.

Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., 2021. Reinforcement
learning for combinatorial optimization: a survey. Computers & Opera-
tions Research 134, 105400.

38

Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver,
D., Kavukcuoglu, K., 2016. Asynchronous methods for deep reinforcement
learning, in: International Conference on Machine Learning, PMLR. pp.
1928–1937.

Nechaeva, O., 2006. Composite algorithm for adaptive mesh construction
based on self-organizing maps, in: International Conference on Artiﬁcial
Neural Networks, Springer. pp. 445–454.

Pan, J., Huang, J., Wang, Y., Cheng, G., Zeng, Y., 2021. A self-learning
ﬁnite element extraction system based on reinforcement learning. Artiﬁ-
cial Intelligence for Engineering Design, Analysis and Manufacturing 35,
180–208.

Papagiannopoulos, A., Clausen, P., Avellan, F., 2021. How to teach neural
networks to mesh: application on 2-D simplicial contours. Neural Networks
136, 152–179.

Raﬃn, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N.,
2021. Stable-baselines3: reliable reinforcement learning implementations.
Journal of Machine Learning Research 22, 1–8.

Remacle, J.F., Henrotte, F., Carrier-Baudouin, T., Béchet, E., Marchandise,
E., Geuzaine, C., Mouton, T., 2013. A frontal delaunay quad mesh gener-
ator using the l∞ norm. International Journal for Numerical Methods in
Engineering 94, 494–512.

Remacle, J.F., Lambrechts, J., Seny, B., Marchandise, E., Johnen, A.,
Geuzainet, C., 2012. Blossom-quad: a non-uniform quadrilateral mesh
generator using a minimum-cost perfect-matching algorithm. International
Journal for Numerical Methods in Engineering 89, 1102–1119.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., 2017. Prox-

imal policy optimization algorithms. arXiv:1707.06347 .

Shewchuk, J.R., 2012. Unstructured mesh generation. Combinatorial Scien-

tiﬁc Computing 12, 2.

Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche,
G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,

39

et al., 2016. Mastering the game of go with deep neural networks and tree
search. Nature 529, 484–489.

Slotnick, J., Khodadoust, A., Alonso, J., Darmofal, D., Gropp, W., Lurie,
E., Mavriplis, D., 2014. CFD vision 2030 study: a path to revolutionary
computational aerosciences. Technical Report CR-2014-218178. National
Aeronautics and Space Administration, Langley Research Center.

Sutton, R.S., Barto, A.G., 1998. Reinforcement learning: an introduction.

MIT press.

Sutton, R.S., Barto, A.G., 2018. Reinforcement learning: an introduction,

second edition. MIT press.

Verma, C.S., Suresh, K., 2017. A robust combinatorial approach to reduce
singularities in quadrilateral meshes. Computer-Aided Design 85, 99–110.

Verma, C.S., Suresh, K., 2018. αmst: a robust uniﬁed algorithm for quadri-
lateral mesh adaptation in 2D and 3D. Computer-Aided Design 103, 47–60.

Vinyals, O., Fortunato, M., Jaitly, N., 2015. Pointer networks, in: Advances

in Neural Information Processing Systems, pp. 2692–2700.

Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G., 2018a. Pixel2mesh:
generating 3D mesh models from single rgb images, in: Proceedings of the
European Conference on Computer Vision (ECCV), pp. 52–67.

Wang, W.Y., Li, J., He, X., 2018b. Deep reinforcement learning for nlp, in:
Proceedings of the 56th Annual Meeting of the Association for Computa-
tional Linguistics: tutorial abstracts, pp. 19–21.

White, D.R., Kinney, P., 1997. Redesign of the paving algorithm: robustness
enhancements through element by element meshing, in: 6th International
Meshing Roundtable, Citeseer. p. 830.

Yang, J., Dzanic, T., Petersen, B., Kudo, J., Mittal, K., Tomov, V., Camier,
J.S., Zhao, T., Zha, H., Kolev, T., et al., 2021. Reinforcement learning for
adaptive mesh reﬁnement. arXiv:2103.01342 .

Yao, J., Stillman, D., 2019. An angular method with position control for
block mesh squareness improvement. Springer International Publishing.
pp. 129–147.

40

Yao, S., Yan, B., Chen, B., Zeng, Y., 2005. An ann-based element extraction
method for automatic mesh generation. Expert Systems with Applications
29, 193–206.

Zeng, Y., Cheng, G., 1993. Knowledge-based free mesh generation of quadri-
lateral elements in two-dimensional domains. Computer-Aided Civil and
Infrastructure Engineering 8, 259–270.

Zeng, Y., Yao, S., 2009. Understanding design activities through computer

simulation. Advanced Engineering Informatics 23, 294–308.

Zhang, Z., Wang, Y., Jimack, P.K., Wang, H., 2020. Meshingnet: a new mesh
generation method based on deep learning, in: International Conference
on Computational Science, Springer. pp. 186–198.

Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q.,
2021. A comprehensive survey on transfer learning. Proceedings of the
IEEE 109, 43–76. doi:10.1109/JPROC.2020.3004555.

Ziebart, B.D., 2010. Modeling purposeful adaptive behavior with the princi-
ple of maximum causal entropy. Ph.D. thesis. Carnegie Mellon University.

41

