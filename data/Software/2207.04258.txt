A NOVEL EVALUATION METHODOLOGY FOR SUPERVISED
FEATURE RANKING ALGORITHMS

TECHNICAL REPORT

Jeroen G. S. Overschie∗

Faculty of Science and Engineering
University of Groningen
Groningen, 9747 AG
j.g.s.overschie@student.rug.nl

July 1, 2021

ABSTRACT

Both in the domains of Feature Selection and Interpretable AI, there exists a desire to ‘rank’ features
based on their importance. Such feature importance rankings can then be used to either: (1) reduce
the dataset size or (2) interpret the Machine Learning model. In the literature, however, such Feature
Rankers are not evaluated in a systematic, consistent way. Many papers have a different way of
arguing which feature importance ranker works best. This paper ﬁlls this gap, by proposing a new
evaluation methodology. By making use of synthetic datasets, feature importance scores can be
known beforehand, allowing more systematic evaluation. To facilitate large-scale experimentation
using the new methodology, a benchmarking framework was built in Python, called fseval. The
framework allows running experiments in parallel and distributed over machines on HPC systems. By
integrating with an online platform called Weights and Biases, charts can be interactively explored on
a live dashboard. The software was released as open-source software, and is published as a package
on the PyPi platform. The research concludes by exploring one such large-scale experiment, to ﬁnd
the strengths and weaknesses of the participating algorithms, on many fronts.

Keywords Feature Ranking, Feature Selection, Evaluation Metrics, Benchmark, Machine Learning

2
2
0
2

l
u
J

9

]

G
L
.
s
c
[

1
v
8
5
2
4
0
.
7
0
2
2
:
v
i
X
r
a

∗Acknowledgements go to George Azzopardi and Ahmad Alsahaf.

 
 
 
 
 
 
A novel evaluation methodology

TECHNICAL REPORT

Contents

1 Introduction

2 Motivations

2.1 Motivations for Feature Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1

Feature Selection .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2

Interpretable Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Motivations for Evaluating Feature Ranking methods . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

3 Related work

3.1 Evaluation metrics

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Synthetic datasets and apriori information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

3.3 The gap in the current literature .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Methods for Feature Ranking

4.1 Terminology .

.

.

.

.

.

.

.

.

4.2 Types of Feature Rankings .

4.2.1

Feature importance .

4.2.2

Feature support .

4.2.3

Feature rankings

.

.

.

.

.

.

.

.

.

4.3 Feature Selection Taxonomy .

4.3.1

Filter methods

.

.

4.3.2 Wrapper methods .

.

.

4.3.3 Embedded methods .

4.3.4 Hybrid methods .

4.4 Types of features

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4.5 Constructing Feature Rankings .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.5.1 Regularized Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

4.5.2 Relief-Based Feature Selection algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Evaluating Feature Rankings

5.1 Cross-Validation and Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.1 Cross-Validation .

5.1.2 Bootstrapping .

5.2 Validation estimators .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Apriori knowledge on relevant features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 Stability .

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

5.4.1

Stability of feature importance vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4.2

Stability of feature support vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4.3

Stability of feature ranking vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

4

6

6

6

6

8

9

9

10

10

12

12

12

12

13

14

14

14

15

15

15

15

16

16

17

19

19

19

19

20

21

22

22

23

23

A novel evaluation methodology

TECHNICAL REPORT

5.5 Time complexity .

.

5.6 Statistical integrity .

.

.

.

.

.

.

.

.

.

.

6 Building a benchmarking pipeline

6.1 Architecture .

.

.

.

.

.

.

6.1.1 The main script .

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.1.2 The ‘Rank and Validate’ pipeline

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

6.2 Components .

.

.

6.2.1 Datasets .

.

.

.

.

.

.

.

.

.

.

6.2.2 Cross-Validation .

6.2.3 Resampling .

6.2.4 Estimators

.

.

.

.

.

.

.

6.2.5

Storage providers .

6.2.6 Callbacks .

6.3 Execution .

.

.

.

.

.

.

.

.

.

.

.

.

6.3.1 Multiprocessing .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.3.2 Distributed computing .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4 Data- collection and visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Experiments

7.1 Experiment setup .

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2 Experimental results for the ‘Synclf hard’ dataset

. . . . . . . . . . . . . . . . . . . . . . . . . . .

.

7.2.1 ReliefF performance .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.2

Performance of multiple rankers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

7.3 Experimental result for all datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.4 Learning curves and time complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.5 Online dashboard .

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Discussion

9 Conclusion

9.1 Concluding note .

.

.

.

.

.

.

.

9.2 Limitations and Future work .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Appendices

A Experiment line-up

B Additional results

Glossary

3

24

24

25

25

25

26

27

28

29

29

29

30

30

30

31

31

32

34

34

34

34

36

39

41

42

43

45

45

45

51

51

53

56

A novel evaluation methodology

TECHNICAL REPORT

1

Introduction

In this day and age, more data is available than ever before in many domains [Sagiroglu and Sinanc, 2013]. In the
biomedical domain sensory devices such as MRI or PET scanners are getting ever more accurate - requiring more
storage space to store higher resolution data. In the ﬁnancial domain, markets are operating at increasingly low time
intervals - requiring storage and analysis of data at a higher time resolution than before. The internet too, sees increasing
amounts of trafﬁc world-wide and is producing immense amounts of data every day. Applications of Machine Learning
are common in all these domains: training predictive models by learning from example data is able to give us interesting
insights that can improve both economy and the quality of human lives. By the nature of models that learn from
examples, performance is often better when a larger amount of examples is available. But it shall get clear that larger
quantities of data presents itself not as solely beneﬁcial; but rather- a mixed blessing.

The ﬁeld of Machine Learning has seen vast increases in dataset sizes: both in terms of sample size and amount of
dimensions. Although the availability of more data presents practitioners with opportunities to create better performing
models, more data will have to be processed - causing an increased computational burden in the learning process.
This increase is nonlinear: due to the curse of dimensionality, the computational burden can get large, quickly. Even
though the ﬁeld has for long relied on computer processing speed steadily increasing, obeying Moore’s law, the rate of
advancement will inevitably start declining - and in fact already has [Theis and Wong, 2017]. Self-evidently, many
technological advancements can still be realised, either in the silicon world or in a post-silicon world, in which perhaps
forms of quantum computing might become predominant [Britt and Humble, 2017]. But what is certain, is that besides
the technological opportunities the chip-making industry still has, there also exist strict physical limitations as to how
fast computer processing can get. So, besides leveraging faster hardware, we are also going to have to make our software
smarter. Instead of increasing just our computational power, methods for reducing the computational burden in the ﬁrst
place are desired. Thus, the need for preprocessing techniques and data reduction algorithms is instigated.

Feature selection is such a domain that focuses on reducing the overall learning processing burden [Guyon and
Elisseeff, 2003]. By ﬁguring out what dimensions are relevant to the learning task, which ones are redundant, and
which ones are completely noisy with respect to the learning task, a smaller dataset can be obtained by means of a
preprocessing step. This is contrary to the domain of dimensionality reduction, in which data is projected onto a space
of smaller dimensionality - but losing the exact representation of the data distribution. In Feature Selection, we aim
to obtain a binary decision about which dimensions to keep, in the original data space. Aside from reducing dataset
size by cutting off dimensions, in some cases the generalization ability of a learning model can even be improved: the
learning model can better learn the data distribution by using less but more meaningful dimensions with less noise. This
makes the beneﬁt of learning on a subset of the available features two-fold: model ﬁtting and prediction can be both
faster and more accurate. To select relevant dataset features, a wide variety of strategies exist. One is to assign a scoring
to each dimension and keep only the most relevant ones - such a ranking is called a feature ranking.

Feature ranking is a broader domain in contrast to feature selection, in which the sole purpose is not to only reduce
the dataset size, but to construct a hierarchical order on the importance of features given a speciﬁc learning task [Duch
et al., 2004]. Many techniques can be employed to create such feature rankings, of which some are substantially faster
than others, but might yield sub-optimal results: the choice of a suitable feature ranking algorithm is not trivial in most
scenarios. Once such a feature ranking has been constructed, it can be used for various applications. First, a feature
selection can be made by removing features that rank below a certain feature importance score threshold; one such naive
method would be to cut off any feature that ranks below the average feature importance score. Secondly, the feature
ranking can also be used for better Machine Learning model interpretation; in which the feature importance scores help
humans better understand the predictions and reasoning of the models - by knowing which features the model found
important, it can better be understood how the model made the decision that it has. This second application is part of
the bigger domain of Interpretable Artiﬁcial Intelligence, or more commonly Interpretable Artiﬁcial Intelligence (AI),
which has in recent times become ever more relevant [Ghosh and Kandasamy, 2020]. Interpretable AI aims to explain
models that were before considered ‘black box’ models, making them more transparent to the user.

Evaluation on the performance of feature ranking algorithms has been conducted in many different ways. Most
authors used a ‘validation’ estimator, which was trained on a chosen subset of the dataset to then see how the estimator
performed given this feature subset. A ranker is desired, then, that ranks the most predictive features highest, preferably
in a reasonable amount of time. In this way, we get a feature subset that is as small as possible, that gives us the
highest possible predictive power. This evaluation technique, however, might not be systematic enough. Across papers,
many different validation estimators are used, which make the results across papers subsequently incomparable to one
another. Researchers might also beneﬁt from more extensive and systematic evaluation by use of synthetic datasets. By
manually controlling many aspects of the dataset, such as noise levels, the complexity of the data distribution to be
learned and the amount of informative features, a comprehensive evaluation on the feature ranking algorithm behavior
and characteristics can be made. In this way, by employing synthetic datasets, the exact informative features to be

4

A novel evaluation methodology

TECHNICAL REPORT

ranked as relevant can be known apriori, i.e. before conducting the feature ranking operation. Such new and possibly
useful metrics can be employed to evaluate feature rankings independent of any validation estimator.

In this paper, a comprehensive comparative experiment on feature rankers is conducted using both real-world and
synthetic datasets, employing new evaluation metrics on the synthetic datasets by knowing the relevant feature apriori.
Both classical and more recent feature ranking algorithms are included, using methods that originally reside in both the
statistical and feature selection domains. By systematically generating synthetic datasets that are speciﬁcally designed
to vary in various relevant data properties, various characteristics of the feature ranking algorithms can be assessed and
estimated. To employ such a large-scale benchmark, a software framework was built to facilitate such testing. The
framework was released as open-source, freely available software.

The research question which is to be answered is as follows. “How do we evaluate Feature Ranking algorithms in a
way that is systematic, comprehensive, and emphasizes the differences between the algorithms in a meaningful way?”
Currently, Feature Ranking and Feature Selection algorithms are evaluated in many ways. This research aims to ﬁnd a
methodology that is both meaningful and applicable to many algorithms. The goal is to arrive at a scientiﬁcally sound
benchmarking method for feature ranking methods, taking advantage of apriori information wherever possible. Up to
our knowledge, no current literature exists that is aimed at the evaluation of Feature- Ranking and Selection algorithms,
that takes such a comprehensive approach and proposes novel evaluation metrics. An important sub-goal in constructing
such a benchmarking method is the development of a pipeline implementing such a new benchmarking method and
analysing its results.

The scope of this research can be deﬁned as follows. Feature Ranking methods are evaluated that work on tabular
datasets and require example data including prediction targets, i.e. only supervised algorithms are considered. Fur-
thermore, all considered datasets are tabular, that is, no underlying data structures such as linked- or streaming data is
assumed. The considered dataset prediction tasks are regression and classiﬁcation - which limits the considered ranking
methods to these tasks as well. Eight out of fourteen classiﬁcation datasets perfectly balanced classes, the other six have
varying levels of imbalance (see Appendix). All considered Feature Ranking methods are global rankers. Meaning
that all methods construct rankings for the full dataset, i.e., no instance-based methods are included. Lastly, all three
ranking types are supported and are considered in the research, i.e. the research considers feature importance-, feature
support- and feature ranking vectors.

The contribution of this paper is multiple-fold. Firstly, the inclusion of many feature ranking algorithms and many
datasets makes it possible to make more meaningful comparisons between feature ranking algorithms, which would
not have been possible across existing papers due to the lack of a single evaluation standard. Secondly, the proposal
of a new evaluation standard also makes it possible for other authors to conduct experiments in reproducible manner.
This subsequently enables readers to compare results across papers - saving time but also allowing more thorough
comparative analysis on which feature ranker is best suited for a given dataset. Third, the new evaluation standard
was implemented and packaged in an easy-to-use open-source software package, distributed as a Python pip package
manager distribution on the PyPi platform2.

Chapters in this paper are structured as follows. First, motivations for both Feature Ranking and the evaluation thereof
are given in Chapter 2. Second, an analysis on previous work in the literature is conducted, in Chapter 3. Third, various
methods for creating feature rankings are presented, such that a grasp can be obtained on the overall mechanics of the
methods. This is done is Chapter 4. Fourth, an insight into how feature rankings are evaluated is gained; and a new
method is presented afterwards, in Chapter 5. Next, comments are made on the construction of a benchmarking pipeline
in Chapter 6. Then, this new standard is applied in an experiment, which setup and results are explained in Chapter 7.
Lastly, the paper is concluded by a discussion in Chapter 8 and a conclusion in Chapter 9.

2https://pypi.org/project/fseval/

5

A novel evaluation methodology

TECHNICAL REPORT

2 Motivations

2.1 Motivations for Feature Ranking

The desire to create a feature ranking from some features in a dataset stems from two main applications: the domain
of feature selection and the domain of interpretable AI. To better understand why one would want to create a feature
ranking, a brief exploration is made on both domains, motivating the concept throughout.

2.1.1 Feature Selection

The ﬁeld of Machine Learning is plagued by the curse of dimensionality [Köppen, 2009]: as the dimensionality of
datasets grows larger, the computational burden for learning algorithms gets exponentially larger. This is due to the fact
that when data gets of increasingly higher dimensionality, the volume of the data shape grows faster than the amount of
samples grows along, causing the data volume to become sparse. There are many real-world domains that naturally
deal with datasets of such shape. In the biomedical world, for example, collecting data samples might require arduous
amounts of human effort [Hu et al., 2018]. There might be the case where one data sample represents one human and
any such sample is very laborious to collect: but the samples that are collected are of very high dimensionality. In this
case, we deal with a scenario where the amount of dimensions far exceeds the amount of samples (n (cid:29) p where n is
the amount of samples and p the amount of dimensions).

To avoid such a curse of dimensionality, feature selection can be used. The goal is to reduce as much features as possible
while retaining the most relevant information in the dataset. The beneﬁts are a decreased computational burden; though
in some cases the learning algorithm generalization performance might actually be improved over a scenario when no
feature selection is used. This is most often due to the removal of noise that would have interfered with the learning
process.

One such example process of feature selection by using a feature ranking can be seen in Figure 1. As can be seen in
the ﬁgure, a ranking can be used to reduce the overall dataset size by using a threshold operation. In such a threshold
operation, any feature that ranks below the given threshold (cid:15) is removed. Many, if not most, feature selection methods
rely on feature ranking methods under the hood to create a feature subset of reduced size.

Figure 1: An example process of feature selection by thresholding a feature ranking at some point (cid:15). The resulting
feature subset X presumably contains the strongest predictors given the target variable. Feature names come from a
real dataset on antarctic penguins [Horst et al., 2020].

For this reason, the more general problem of constructing feature rankings is omni-relevant in the feature selection
domain. To go from feature ranking to feature selection, the only extra step necessary is a sufﬁcient threshold point,
which many algorithms arrive at using different methods. A straightforward way to cut off low-ranking features is
to exclude any feature that ranks below the mean score - or, alternatively, exclude only the highest ranking quartile,
etcetera. More sophisticated schemes exist, however. One might also choose to iteratively run a feature ranking process
and remove the lowest ranking score in each iteration. Such a process is called Recursive Feature Elimination (RFE).
That said, it is clear that feature ranking and feature selection go in unison.

2.1.2 Interpretable Artiﬁcial Intelligence

The domain of Interpretable AI has arisen due to the need to better understand and reason about the decisions that any
Machine Learning model makes. Traditionally, the sole goal in building a learning algorithm would be to best predict
some target variable, given some set of samples to learn from. With the advent of sophisticated learning algorithms such
as Neural Networks and especially Deep Neural Networks, however, the many computational layers that separate inputs
from answers tend to obfuscate the decision process [Rai, 2020]. Whereas in classical statistical models practitioners
sought to unveil a clear relationship between the independent- and dependent variables, some models in the ﬁeld of

6

Feature nameFeature score Flipper length0.60Bill length0.25Bill depth0.10Sex0.05Subset selectionFeature ranking ŵFeature subset 𝐗Threshold:ε = 0.2 score ≥ εBill lengthBill depthFlipper lengthSex46.213.5211M48.114.1203M42.313.2208FInput data ZFeature ranking methodFlipper lengthBill length21146.220348.120842.3A novel evaluation methodology

TECHNICAL REPORT

Machine Learning have grown so complicated that no human can reason on its output. Because such algorithms tend to
compare metaphorically to a black box one cannot possibly see through, one also refers to such algorithms as black
boxes.

Whilst at the same time models have gotten progressively more sophisticated and thus less transparent, the market for
deploying Machine Learning has been steadily getting bigger. Many institutions seek to beneﬁt from the possibilities
of recent developments in AI and Machine Learning (ML) - including many companies, schools or the government.
In all such applications, for every decision made by a Machine Learning model, an argumentation as how the model
came to such a conclusion is desired. Even, in some scenarios the decision to be made at hand weighs so heavily
that a ML model that cannot explain itself might not be usable at all. For example, one might deploy a ML model
that scores employees based on their performance. The goal is then to ﬁre the lowest ranking employees and keep
highest performing ones. But fully trusting such a model might be a dangerous practice. If a model provides only
bare explanations on how it came to such a decision, employers might have a hard time interpreting the model and
employees might have a hard time understanding its decisions. As a matter of fact, faulty models of such kind have
already been deployed out in the open - with considerable complications with respect to model transparency as a result
[O’Neil, 2016].

For this reason, decisions made by computers are required to be explainable - allowing practitioners to assess not only
ML model decisions, but also assess the overall usefulness of the model in general. After all, many learning models
are designed such to always generate a decision, no matter how noisy the input. That said, the black box of AI can
generally be opened up in two ways. First, one could opt for a less sophisticated in the ﬁrst place - one that reveals its
inner workings and supports its decisions by an elaborate scheme, much in line with classical statistics. Although a
simpler model could sometimes sufﬁce in places where practitioners currently opt for more complicated ones, a solution
for models of any kind of ﬂexibility is desired. Therefore, a second option is to delve into any such model to reveal a
reasoning about its decision-making process - which is the option that the ﬁeld of Interpretable AI bothers itself with.

Feature rankings are one of the facets which can be used to facilitate a better understanding of a Machine Learning
model [Hansen and Rieger, 2019]: by unveiling which variables the ML model ﬁnds important, a better understanding
can be gained on its decision-making process. In some scenarios, it might, for example, be the case that the Machine
Learning model in question unexpectedly weighs a feature as very important, even though a human expert can know
apriori that the feature is not of value to the predictive task at hand. In this way, faulty models can be detected and
prevented from being used in critical settings. In the Interpretable AI jargon, the process of scoring and explaining
feature importance goes under different names. In the literature, feature rankings are referred to as feature impact
scores, feature importance scores, or feature effects, which are synonymous in this case. All terms indicate the process
of quantifying feature relevance with respect to the learning task at hand. The general process of an interpretable AI
algorithm can be seen as in Figure 2.

Figure 2: An example process of Interpretable AI. In this case, the input to the interpretable AI algorithm is a trained
black box model, which has an otherwise hard to understand decision making process. The interpretable AI algorithm
then extracts feature importance scores for better interpretability.

As could be seen in Figure 2, one way of making a black box model more interpretable is to extract a feature ranking
from the trained model - an interpretable AI algorithm reads in the black box model and tries to explain it [Lundberg
and Lee, 2017]. Other interpretable AI methods take a more direct approach [Arik and Pﬁster, 2020], in which the
interpretable AI algorithm functions as a prediction method itself. The latter approach is employed only in more recent
years; in which learning algorithms are designed from the start to be interpretable. However, independent of whether
a direct- or indirect approach to explaining the decision process is taken, the desire to rank features is applicable to
both. That said, we can therefore conclude that the construction of truthful feature rankings are an important aspect in
interpretable AI.

7

Feature nameFeature score Flipper length0.60Bill length0.25Bill depth0.10Sex0.05HumaninterpretationFeature ranking ŵBlack box modelInterpretable AI methodA novel evaluation methodology

TECHNICAL REPORT

2.2 Motivations for Evaluating Feature Ranking methods

Given the need for Feature Ranking methods, the desire to evaluate such methods systematically is clear. Practitioners
throughout the ﬁeld of Machine Learning desire to know the behavior of different methods under a large range of
varying conditions, such as the sample size-, dimensionality-, or domain of the dataset. One also needs to take into
account the available computational resources and the desired degree of accuracy of the solution. Finding out what
evaluation methods and metrics best represent the optimality of the solution is essential, for else the presentation of
scientiﬁc ﬁndings might be misleading compared to real-world performance.

In this way, individuals and institutions are motivated to actively research Feature Ranking methods and evaluate them
such to determine which method works best, under what conditions. As is more often the case in the comparison of
Machine Learning algorithms, however, is the fact that methods are not always easily analytically compared. That is,
solely a theoretical analysis does not sufﬁce to create a meaningful comparison. Although one can reason about the
performance of many methods before conducting concrete data analyses, a real-world experiment is essential to any
reputable publication.

8

A novel evaluation methodology

TECHNICAL REPORT

3 Related work

The domain of feature ranking and selection has a large availability of literature, spread out over many subtopics. What
is more rare, however, is to ﬁnd papers that explicitly research and reason about the usage of certain evaluation metrics.
In general, papers tend to stick to a certain evaluation method when the majority employs the given technique - but a
chance for conducting a more thorough analysis might be missed nonetheless.

In the feature selection domain, the evaluation and comparison of feature selection algorithms is a non-trivial problem.
Among a wide range of metrics, no consensus exists among researchers, leaving many papers to present outcomes in
different ways [Guyon and Elisseeff, 2003]. In absence of a single consistent evaluation pipeline across the ﬁeld, many
scholars adhere to methods that are ‘widely used’ [Solorio-Fernández et al., 2020] [Li et al., 2017].

Recommendations for metrics have been given in previous papers, most often when discussing future work. Arguments
are made for relevant aspects to evaluate, such as in Chandrashekar (2014) [Chandrashekar and Sahin, 2014]:

"a feature selection algorithm can be selected based on the following considerations: simplicity, sta-
bility, number of reduced features, classiﬁcation accuracy, storage and computational requirements"

Of these aspects most proposals focus mainly on number of reduced features, classiﬁcation accuracy and computational
requirements. In the regression case, the classiﬁcation accuracy would be replaced by a commonly used regression
counterpart, the R2-score. Let us explore what validation estimators and corresponding metrics are used in papers
across the ﬁeld. Afterwards, evaluation aspects are covered that are not present in the aforementioned set.

3.1 Evaluation metrics

A validation estimator is often used to evaluate supervised feature selection methods; assessing the quality of a
feature subset by running some predictor over the feature subset selected by the feature selection algorithm, obtaining
the easily interpretable prediction accuracy metric in the classiﬁcation case. Predictors often used in the literature
include k- Nearest Neighbors (k-NN) [Al-Tashi et al., 2020] [Mafarja et al., 2020], Support Vector Machines (SVMs)
[Chandrashekar and Sahin, 2014], Decision Trees (DTs) [Li et al., 2017] and Naïve Bayes (NB) [Koller and Sahami,
1996]. Metrics used are often classiﬁcation accuracy or in some cases average error rate [Khurma et al., 2020], validated
using some n-fold cross validation, commonly 5- or 10-fold. See Table 1.

Validation
estimators
DT
DT
PCC
DT, SVM
k-NN
NB

Acc-
uracy
(cid:88)
(cid:88)

Stab-
ility

Time
t(s)

(cid:88)

Time
Ω(n)
(cid:88)
(cid:88)

Apriori
info

(cid:88)

Name method
FOCUS [Almuallim and Dietterich, 1991]
Relief [Kira and Rendell, 1992]
Relief-F [Kononenko, 1994]
INTERACT [Zhao and Liu, 2007]
Fisher [Gu et al., 2012]
MutInf [Zaffalon and Hutter, 2014]
Joint MutInf Maximization [Bennasar et al., 2015] NB, k-NN
Interaction Weight-based FS [Zeng et al., 2015]
Inﬁnite FS [Roffo et al., 2015]
MultiSURF [Urbanowicz et al., 2018a]

(cid:88)
(cid:88)
(cid:88)
(cid:88)
DT, IB1, PART (cid:88)
(cid:88)
SVM
-
Table 1: A comparison table of evaluation metrics used in Feature- Ranking or Selection paper proposals. Many
different evaluation metrics are used, illustrating there exist no consensus on deﬁnite meaningful evaluation metrics in
the ﬁeld.

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Stability, on the other hand, is not widely used in theoretical or quantitative argumentation. The stability is deﬁned as
the ability of an algorithm to produce consistent results given small changes in the sample data. In this context, this
can be phrased as the sensitivity of the feature selector to data perturbations. Even though stability was recommended
as a relevant evaluation metric by Chandrashekar (2014), not many papers explicitly argue for the stability of their
method; the metric is called an “overlooked problem” in Chandrashekar (2014). In many papers this metric is still
regarded as a future work for solidifying any experimental results - the development of algorithms that achieve both
high classiﬁcation accuracy and high stability is still seen as a ‘challenging’ problem by Tang et al (2015) [Tang et al.,
2014].

The trend seems to be turning though, with more authors becoming aware of the importance of stability. Our reliance on
machine learning is ever-increasing, and so does the demand for interpretability and reliability of the algorithms. Take

9

A novel evaluation methodology

TECHNICAL REPORT

for example a biomedical application, in which feature selection is used to select genes in a gene sequencing analysis.
Any expert in this domain ﬁeld will feel more conﬁdent if an algorithm produces stable results given a varying sample
population. In this way, algorithm stability is of much relevance to real-world applications of ML. Stability has been
long taken into account into prediction tasks, but not so much in feature selection - see for example Table 1.

Scalability is another point of interest that only recently caught more attention. The extra demand of algorithms to
allow for parallel execution has been imminent as data grew tremendously large. Even, multi-core processing can lack
in terms of performance, hence introducing the need for algorithms that can run in distributed fashion. Distributing
a dataset over multiple machines poses challenges to some existing methods, though. Some current methods require
the full dimensionality of the dataset to be available in-memory [Tang et al., 2014]. Yet, other methods require each
sample to be visited multiple times, e.g., to apply a sample re-weighting strategy in order to converge. For these reasons,
distributing any dataset workload on to multiple workers is a non-trivial problem; no generalized solution exists for
cutting the dataset into chunks.

It is up to individual algorithms to ﬁnd suitable ways of supporting parallel solutions and more importantly, support
cases where data is too large to ﬁt in-memory, i.e., apply distributed computing. Recent strategies overcome the issue
of working with fewer samples by retaining only those samples that are most representative of the data - eliminating
the need for working with a full sample population. Although few in number, there exist proposals for distributed
dimensionality reduction methods [Li et al., 2020], using divide-and-conquer techniques. Aggregating disjoint results
would make for a performance similar to that of a centralized solution.

3.2 Synthetic datasets and apriori information

Synthetic datasets are employed in many papers in the literature. Whereas some datasets are injected by synthetically
generated probe variables, others use completed generated benchmark datasets, such that benchmarks can be conducted
in an even more controlled environment. About such partially synthetic datasets has been spoken of in literature since
long, using datasets that are real but altered by injecting more data. In [Guyon and Elisseeff, 2003] the authors speak of
‘probe variables’, which are used to discard any variable that scores lower than any of the probes. If the probe variables
are set to be random variables, a simple way is obtained to apply a threshold to cut off features from the selected feature
subset, i.e., by introducing known noise into the dataset we can construct more thoughtful cut-off thresholds.

Completely synthetic datasets, on the other hand, can allow for more sophisticated metrics to be used. Possibilities
for new evaluation metrics are, for example described in [Solorio-Fernández et al., 2020]: “Evaluation in terms of the
redundancy of the selected features” and “Evaluation in terms of the correctness of the selected features”, the latter of
which requires us to know what features are informative a priori - which is accomplished with synthetic generation.
Controlling all facets relevant to the quantitative analysis manually makes for a Simulation study, which is argued for in
[Urbanowicz et al., 2018b] as follows:

“Simulation studies such as these facilitate proper evaluation and comparison of methodologies
because a simulation study can be designed by systematically varying key experimental conditions,
and the ground truth of the dataset is known i.e. we know which features are relevant vs. irrelevant,
we know the pattern of association between relevant features and endpoint, and we know how much
signal is in the dataset.”

Indeed, there seems to be a trend toward including synthetically generated datasets in experiments. In a review paper
[Bolón-Canedo et al., 2013] the authors argue that synthetically generated datasets can yield statistically sound results
because of the fact no inherent noise or redundancy will obstruct the experiment process. In other papers simulation
studies are conducted as well [Cai et al., 2020] [Tang et al., 2020] [Li et al., 2020], concluded by a small section
depicting a ‘real data’ analysis to conclude the point. For these reasons, a recommendation is made to include simulation
studies in any comprehensive benchmark on feature ranking methods.

3.3 The gap in the current literature

With respect to the above summarized works, there are some aspects missing in the evaluation process. Because this
research is set out to ﬁll in the highlighted missing parts, it is important to get a clear idea of the entire set of missing
aspects. Therefore, the concerned literature ‘gap’ is summarized as follows.

Many papers in the literature evaluate Feature Ranking and Feature Selection algorithms in different ways. Many
validation estimators are used across papers, causing the results to become incomparable. Moreover, not every paper
evaluates the stability or scalability of the algorithm, like shown in Table 1. Whereas the stability is a quantiﬁcation of
the algorithm’s robustness against data permutations, the scalability means both storage- and time complexity. Lastly,

10

A novel evaluation methodology

TECHNICAL REPORT

there exist opportunities for systematic evaluation using apriori information on the feature importances. Few authors
currently utilise this opportunity.

Therefore, the above problems are to be addressed in this paper. A concrete quantiﬁcation of the algorithm’s stability,
scalability and performance is desired. Thereby, also synthetic data is considered, in which the relevant features
are known apriori. This paper also ﬁlls in the gaps left by some papers which do only theoretically describe their
experimental setup: the new methodology was implemented in a pipeline, available as open-source software. However,
ﬁrst, a look is taken at how to make Feature Rankings at all.

11

A novel evaluation methodology

TECHNICAL REPORT

4 Methods for Feature Ranking

In the following, general theory related to the construction of feature rankings is discussed. The theory is required to be
discussed because in order to best understand the evaluation process, an understanding of the construction process is a
must.

4.1 Terminology

Among the subject of reducing dataset dimensions, there exists a common terminology that is used among the literature.
Whilst some terminology is synonymous, other seemingly related terms mean different concepts entirely.

Feature Selection and Feature Ranking are two terms often used interchangeably. Since often feature selection is done
by ﬁrst constructing a feature ranking and then cutting off features ranked lower than some threshold (cid:15), the construction
of a feature ranking is often times an integral part of feature selection. Although feature selection methods exist that do
not construct feature rankings, the two are synchronous in many ways, and are in this paper thus related to one another.

The term Feature Selection is used to indicate the general process of obtaining a feature subset with reduced size without
transforming the data. Note that any feature selection method might transform the data in the algorithm however it likes
internally - the stated terminology is only concerned with the eventual output of the feature selection algorithm. In this
paper, a broad perspective is taken and not only feature selection methods but feature ranking methods generally are
considered.

Feature Ranking is a broader term compared to Feature Selection, mapping onto more domains than just Feature
Selection. Because in the construction of a Feature Ranking no assumptions are made on the desired data subset, a
score is assigned to each of the dataset features, giving each dimension an ‘importance’ score. Such scores can also be
used to interpret and clarify a Machine Learning model: thus making such ranking processes useful to the interpretable
AI domain.

Feature projection and Feature Selection are both processes relating to the concept of dimensionality reduction
[Cunningham, 2007], however, there exists an important distinction between them. Whilst in the process of feature
selection, relevant dimensions are sought and selected without altering their input values, in the process of feature
projection (also called feature extraction) data transformations are applied, mapping the original data onto a lower-
dimensional space. Common methods of feature projection are Principle Component Analysis (PCA) for the supervised
case and Linear Discriminant Analysis (LDA) for the unsupervised case. Both PCA and LDA take a statistical approach
to detecting feature interactions, which not always results in an optimal feature set for prediction. Rather, machine
learning techniques can be used to select a more optimal subset. Although the two methods are different, the two aim at
achieving the same goal and are thus encountered in similar contexts.

4.2 Types of Feature Rankings

To better understand in what form a Feature Ranking might be deﬁned and how it relates to the process of Feature
Selection, exact mathematical deﬁnitions of various types of feature rankings are given ﬁrst. At all times, the prediction-
or estimation of any quantity is denoted with a hat notation, i.e. ‘^’, whilst the ‘true’ value of the quantity has no such
hat.

4.2.1 Feature importance

Feature importance scores are deﬁned as a vector of p dimensions, containing real-valued numbers. Let us deﬁne the
vector like so:

where ˆw ∈ Rp. Such a ranking is obtained, for example, by running a feature ranker on the dataset and having it
assigned as score to each dimension. The vector is assumed to be normalized, i.e. each vector element is divided by the
vector sum:

ˆw = ( ˆw1, ˆw2, . . . , ˆwp−1, ˆwp),

(1)

given a scoring vector ˆr, which indicates feature importance on an arbitrary scale. It is self-evident that ˆw has the
property that (cid:80)p

i=1 ˆwi = 1, i.e. is a probability vector. An example such vector can be:

ˆw =

ˆr
(cid:80)p
i=1 ˆri

,

(2)

12

A novel evaluation methodology

TECHNICAL REPORT

in which it is clear that the ranking algorithm found the second feature to be the most important. In the case where
multiple feature importance vectors are considered, e.g., in the case where B bootstraps (Section 5.1.2) are considered,
the vectors are stacked in a matrix, i.e.:

ˆw = (0.20, 0.8, 0.0),

(3)

which denotes B feature importance p-dimensional vectors arranged in a matrix of reals. The ground-truth w will
remain a vector, since there will still be only one such vector available per dataset.

ˆW ∈ RB×p,

(4)

4.2.2 Feature support

Feature support indicates whether certain dimensions are chosen to be included a feature subset; i.e. the vector marks
elements as being chosen by the feature selection process. In this deﬁnition, the feature support vector is synonymous
with the deﬁnition of a feature subset. Although some feature- ranking and selection processes approximate a suitable
feature support vector directly, an algorithm can also make use of a threshold (cid:15) to generate a feature support vector
from a feature importance vector. The feature support vector is a boolean-valued vector of p dimensions.

where ˆs ∈ Bp. Note B is the boolean-valued vector space, i.e. its elements lie in the set {0, 1}. An example such vector
can be:

ˆs = (ˆs1, ˆs2, . . . , ˆsp−1, ˆsp),

(5)

which is the feature support mask obtained from thresholding the feature importance vector ˆw in the above example
(Equation 3) using the threshold (cid:15) > 0.0, causing one feature to be dropped from the feature subset. Just like for the
feature importance vector, the feature support vectors can be arranged in a matrix in the boolean space, like so:

ˆs = (1, 1, 0),

(6)

ˆS ∈ BB×p,

(7)

given B feature support vectors.

A sparse representation can also be constructed, given a feature support vector. A set is created containing only the
indices of the selected features, causing a more sparse representation of the feature subset in case of high dimensionality
and relatively few selected features. The support vector s and its prediction ˆs can be readily converted back- and forth
into such a sparse representation. We deﬁne the sparse representation as the set ˆS:

ˆS = {i | i ∈ Z ∧ ˆsi = 1},

(8)

where |ˆS| = d, i.e. d dimensions were selected in the feature subset. Note, that the sparse feature subset is represented
as a set instead of a vector, meaning that the sequence is no longer considered ordered. To show a concrete example, the
vector from Equation 6 is converted to the following set:

containing the indices of the selected features as deﬁned in Eq 6, in no particular ordering whatsoever. A sparse set ˆS
can be converted back to a feature support vector ˆs like so:

ˆS = {2, 1} where |ˆS| = 2,

(9)

Denote B such sparse feature support sets as ˆSboot:

ˆsi =

(cid:26)1
0

if i ∈ ˆS
otherwise

ˆSboot = {ˆS1, ˆS2, . . . , ˆSB−1, ˆSB},

13

(10)

(11)

A novel evaluation methodology

TECHNICAL REPORT

meaning B sparse support sets were arranged in the superset ˆSboot.

4.2.3 Feature rankings

Feature rankings are similar to feature importance scores, but with less precision. Whereas in a feature importance
vector each element is approximated with a real-valued score, in an ordinary feature ranking the only considered facet
is the order of the features in terms of importance. Although in most cases a feature importance vector is constructed
ﬁrst, after which a support or ranking vector can be constructed, in some cases only a ranking is available - e.g. in
the case of RFE. A feature ranking is constructed by assigning each dimension a rank, anywhere in the integer set
{1, 2, . . . , p − 1, p}. Such, the vector can be expressed as:

where ˆr ∈ Zp, the integer-space. An example such vector can be:

ˆr = (r1, r2, . . . , rp−1, rp),

ˆr = (2, 3, 1),

(12)

(13)

which is the feature ranking obtained from converting the feature importance vector ˆw in the above example (Eq 3) to
a ranking. Such rankings are easily converted to importance vectors using Equation 2, allowing one to use the same
statistical machinery as for feature importance vectors. A higher rank number means a greater importance.

Such an integer-valued ranking can be utilised to select some k best features to use in a subsequent learning task, i.e. to
perform a feature subset selection. This is similar to selecting features using a feature importance vector. Now, however,
a subset is constructed not based on a threshold value (cid:15), but by including a certain number of best features in the subset.
Even though the feature- importance and ranking vectors both have the ability to select feature subsets, the feature
importance vector carries more meaning, because it more precisely quantiﬁes the relative importance of each feature.
This extra meaning can be made to good use during the evaluation process of the feature rankings, especially when
using synthetically generated datasets, in which the ground-truth feature relevance is available.

4.3 Feature Selection Taxonomy

To better understand what type of feature selectors exist, and how they relate to the types of feature rankings that can be
constructed, a taxonomy is considered. Feature Selection can be constructed by running a separate statistical operation
on the dataset, before running any learning algorithms, or as part of a learning algorithm itself. In some cases, a learning
algorithm that is itself very sophisticated and time-consuming might still be worthwhile to use as a feature selection
pre-processing step. This is because if time is saved by having the prediction estimator process less features, one might
still have to spend less time in his learning process. In this way, one might enjoy gains in processing efﬁciency by using
a feature selector.

Due to this distinction in the approach used in a feature ranking algorithm, a subdivision can be made to separate
methods into more speciﬁc categories. As such, a common taxonomy in the ﬁeld is created: subdividing feature ranking
methods into the categories of ﬁlter-, wrapper- and embedded methods [Chandrashekar and Sahin, 2014].

4.3.1 Filter methods

Filter methods use some scoring mechanism to compute ‘usefulness’ for each feature, without applying a learning
algorithm. Having applied some ranking criterion, often a feature ranking is produced, after which some thresholding
operation can be applied to select features. Although ﬁlter methods are often computationally light and do not overﬁt
due to the absence of a learning algorithm, ﬁlter methods might miss out on more complex feature interactions, causing
a non-optimal subset as a result. Also, choosing a suitable threshold to use can be difﬁcult.

Examples of ﬁlter methods include the Fast Correlation Based Filter [Yu and Liu, 2003], which uses a quick and
easy-to-compute statistical measure to select features according to some predeﬁned threshold (cid:15) (in Yu et al. denoted
as δ). To illustrate the type of statistical quantities generally used in ﬁlter methods, the statistical quantity in Yu et al
(2003) can be denoted like so:

SU (X, Y ) = 2

(cid:20) IG(X | Y )
H(X) + H(Y )

(cid:21)

,

(14)

where IG(X|Y ) is the information gain between two random variables X and Y and H(X) is the entropy of a
variable: which are both metrics coming from the information-theoretical domain. The measure SU (X, Y ), then,

14

A novel evaluation methodology

TECHNICAL REPORT

is the symmetrical uncertainty of two features X and Y , where a feature Y is more correlated to X than to Z if
IG(X, Y ) > IG(Z, Y ). In this way, a ranking can be constructed considering the measure SU (X, Y ), where feature
are sought with high SU scores. In a second phase of the algorithm, the scoring table is traversed yet again, to eliminate
possible redundant features included in the selected feature subset.

4.3.2 Wrapper methods

Wrapper methods, on the other hand, use some learning algorithm to determine a suitable subset of features. A search is
conducted over the space of possible feature subsets, eventually selecting the subset that has the highest validation score
in the test set using a chosen learner as a predictor. Characteristics that deﬁne wrapper methods lend themselves similar
characteristics to traditional optimisation problems; although an exhaustive search might yield an optimal solution, such
a solution might not always be feasible due to its great time complexity. For this reason, in some applications a ﬁlter is
applied ﬁrst, before running a wrapper method.

Examples of such methods are numerous. Straight-forward methods include the range of sequential feature selection
methods, which aim to start (1) either with the full subset of dataset features or (2) with an empty set of features.
The two approaches are called Forward Feature Selection and Backward Feature Elimination, respectively. To then
facilitate a forward- or backward iteration step it is customary to use an estimator of some kind to retrieve a scoring on
the features, selecting either the best- or eliminating the worst scored feature. RFE is one such backward-elimination
method, which might, for example, use a SVM to construct estimation scores [Maldonado and Weber, 2009]. Another
option is to perform an exhaustive search, in which every feature combination is tried such to obtain the optimal feature
subset, given the learning task and the estimator used. To obtain such a ranking, the chosen estimator might use an
arbitrary method to compute it, be it a relatively simple learning step or a sophisticated model evaluation. Although
such methods can perform reasonably well, such methods tend to be more time-consuming than ﬁlter methods.

4.3.3 Embedded methods

Embedded methods seek to combine the training task and feature selection. Given some suitable learner, features are
weighted during the training process, producing either a feature ranking or a feature subset afterwards. e.g. some
learners compute feature importance scores as part of their training process, which can then be used in combination
with some threshold to select relevant features. Having already trained the model, subsequent prediction tasks can
beneﬁt from increased prediction speed by using less data.

Examples of embedded methods are Least Absolute Shrinkage and Selection Operator (LASSO) and Ridge Regression,
which perform feature selection alongside the process of ﬁnding optimal regression coefﬁcients. For further explanations,
see Section 4.5.1. Another notable embedded method is a DT, which constructs, inherent during its ﬁtting stage, a
measure of importance on each variable. It does so by computing the probability of reaching a certain node - and
determining the decrease in node impurity caused by weighting this probability value. To facilitate this computation,
the probability of reaching a certain node is computed by considering the number of samples that reach the node during
its decision-phase, divided by the total number of samples. Such, the leaves and depth obtained during the construction
phase of the DT can be used to determine a measure of feature importance, ‘embedded’ into its learning phase.

4.3.4 Hybrid methods

A hybrid method is any method that is not classiﬁable by a single category, but rather lends from multiple categories.
Hybrid methods can, for example, combine ﬁlter and wrapper methods [Hsu et al., 2011], by ﬁrst applying a compu-
tationally efﬁcient ﬁlter and reﬁning the result by using a wrapper method. Another paper [Das, 2001] describes its
approach as hybrid due to both adding- and removing features in the feature selection process - exhibiting both forward-
and backward selection at the same time. Lately, research was also put into examining Ensemble feature selection
methods [Bolón-Canedo and Alonso-Betanzos, 2019], which combines the outputs of multiple selectors and decides
useful features accordingly using some voting committee. Ensemble methods can be seen as hybrids or are seen as a
category on its own.

4.4 Types of features

An important consideration in choosing a suitable feature selection method for any task is what kind of structure the
concerning data has, if any at all. Data might exhibit tree, graph, or grouped structures, which is essential information
when detecting feature interactions and determining useful features. Support for structured data is relatively new in the
ﬁeld and has not been an extensive point of concern for many feature selection algorithms in the past. Many traditional
feature selection algorithms focused primarily ‘conventional ﬂat’ features [Li et al., 2017], in which the assumption is
made that the data is independent and identically distributed (i.i.d.). However, this assumption is widespread among

15

A novel evaluation methodology

TECHNICAL REPORT

many machine learning algorithms, since in many applications datasets are normalized to ﬁt the i.i.d. condition before
they are used.

Conventional data are opposed to more complex data structures, i.e., datasets with ‘structured features’, as coined by
the proposed taxonomy from Li et al (2017) Li et al. [2017], but also to linked data and streaming data. In streaming
data, the quality of an initially selected feature subset can be improved as more data comes in, and a feature selection
algorithm is able to beneﬁt from a larger distribution of samples. Adapting existing feature selection algorithms to ﬁt the
demands of streaming data proved to be a nontrivial problem. Nowadays, many companies and institutions have to deal
with data volumes that easily exceed the boundaries of in-memory storage capacity - limiting the data scientist to train
on only subsets of the entire datasets. Smart sampling is therefore needed to retain a representative sample distribution.

The scope of this research is limited to only the most common type of features, conventional-, ﬂat features (i.i.d.), or
also tabular data.

4.5 Constructing Feature Rankings

Although the number of existing feature ranking methods are numerous, a small set of example methods are explored to
get a better understanding of the range of methods that do exist. First of all, a way of constructing a feature ranking that
originates from classical statistics is examined. Next, a more algorithmic-type of approach is examined, which was
speciﬁcally designed for the feature selection domain.

4.5.1 Regularized Linear Regression

One of the most fundamental methods in statistics is linear regression. It can be solved both analytically and numerically:
in which the optimal approach is dependent on the amount of dataset dimensions at hand - where the amount of dataset
dimensions p gets very large, the analytic solution gets slower compared to an approximate method like Stochastic
Gradient Descent. Recall that we can analytically solve Linear Regression by minimizing the Residual Sum-of-Squares
cost function [Hastie et al., 2009]:

R(β) = (Y − Zβ)(cid:124)(Y − Zβ),

(15)

in which Z is our design matrix. Regression using this loss function is also referred to as ‘Ordinary Least Squares’.
The mean of the cost function R over all samples is called Mean Squared Error, or MSE. Our design matrix is built
by appending each data row with a bias constant of 1 - an alternative would be to ﬁrst center our data to get rid of the
intercept entirely. To now minimize our cost function we differentiate R with respect to β, giving us the following
unique minimum:

ˆβ = (Z(cid:124)Z)−1Z(cid:124)Y,

(16)

which results in the estimated least-squares coefﬁcients given the training data, also called the normal equation. We
can classify by simply multiplying our input data with the found coefﬁcient matrix: ˆY = Zˆβ. Now, in the case where
our model is ﬁt using multiple explanatory variables, we are at risk of suffering from multicolinearity - the situation
where multiple explanatory variables are highly linearly related to each other causing non-optimal ﬁtting of the model
coefﬁcients.

In Ridge regression, we aim to tamper the least squares tendency to get as ‘ﬂexible’ as possible to ﬁt the data best
it can. This might, however, cause parameters to get very large. We therefore like to add a penalty on the regression
parameters β; we penalise the loss function with a square of the parameter vector β scaled by new hyperparameter λ.
This is called a shrinkage method, or also: regularization. This causes the squared loss function to become:

R(β) = (Y − Zβ)(cid:124)(Y − Zβ) + λβ(cid:124)β,

(17)

where we can see that β(cid:124)β denotes the square of the parameter vector, thus supplementing the loss function with an
extra penalty. This is called regularization with an L2 norm; which generalization is called Tikhonov regularization,
which allows for the case where not every parameter scalar is regularized equally. If we were to now derive the solutions
of β given this new cost function by differentiation w.r.t. β:

ridge

ˆβ

= (cid:0)ZT Z + λI(cid:1)−1

ZT Y,

(18)

16

A novel evaluation methodology

TECHNICAL REPORT

in which λ will be a scaling constant that controls the amount of regularization that is applied. Note I is the p × p
identity matrix - in which p are the amount of data dimensions used. An important intuition to be known about Ridge
Regression, is that directions in the column space of Z with small variance will be shrunk the most; this behavior can be
easily shown be deconstructing the least-squares ﬁtted vector using a Singular Value Decomposition (SVD).
LASSO regression is a slightly modiﬁed variant of Ridge Regression, where instead of an L2 norm an L1 norm is used
instead. This problem can be denoted as the following minimization problem:

lasso

ˆβ

= argmin

β






1
2

N
(cid:88)

i=1



yi − β0 −

p
(cid:88)

j=1


2

xijβj



+ λ

p
(cid:88)

j=1

|βj|






,

(19)

1 |βj|.

in which the similarities between Ridge Regression are easily seen. The squared (cid:80)p
the L1 norm penalty of (cid:80)p
Feature selection can be employed using both LASSO- and Ridge regression. Because the dimensions whose
coefﬁcients are shrunk the most are presumably the least relevant to the prediction task, non-contributing features can
be cut off from the design matrix using a threshold point. In fact, because LASSO does not square the weights vector
but takes the absolute value, some coefﬁcients might even be shrunk to near-zero values: removing the need for deﬁning
a threshold at all, since the coefﬁcients have zero contribution already. In line with the feature subset deﬁnition given in
Section 4.2.2, such a feature support set can be constructed by thresholding the LASSO coefﬁcients like so:

j ridge penalty is replaced by by

1 β2

ˆSlasso = {j | |βj| ≥ (cid:15)},

(20)

which will result in a feature support set containing only the indices of features where the coefﬁcients were at least
larger than (cid:15).

Although the former methods are suited for regression tasks only, regularization schemes are employed widely and
have similar mechanics in these applications. Examples are numerous and include regularized Logistic Regression,
regularized SVMs and regularized Neural Networks: employing regularization in any Machine Learning model is
standard practice. In this way, we gained insight into a fundamental tool to estimate feature importance and shrinking
the model coefﬁcients using a regularization term - and more importantly, the fact that it can be employed for feature
selection.

4.5.2 Relief-Based Feature Selection algorithms

The Relief-family of feature selection algorithms originates from a seminal paper by Kira et al. [Kira and Rendell,
1992], introducing the original version of the Relief algorithm. Over time, many variations on the Relief algorithm
have been made, most notably ReliefF [Kononenko, 1994]. In fact, so many variations on the algorithm were made
that one can speak of Relief-Based Feature Selection Algorithmss (RBAs) in the literature [Urbanowicz et al., 2018a].
Due to the overall architectural design of the algorithm, RBAs manage to rank features and optionally select a feature
subset within a reasonable time-complexity domain and is often competitive when it comes to validation estimator
performance.

The algorithm works by usage of an instance-based learning method, considering one data sample at a time and
updating statistics on feature importance. Given a training dataset Z, n samples, p dimensions and a relevancy threshold
(cid:15), Relief can compute a measure of feature importance in a ﬁnite amount of time. The essential concept is to compute
the p-dimensional Euclidean distance for a randomly picked instance, iterating over all dataset samples and computing
the nearest ‘miss’ and nearest ‘hit’ for every instance. Such a miss- or hit is deﬁned to be either one of the positive- or
negative dataset instances. To now compute how much ‘relevance’ should be added for each feature given the closest
hit- and miss, a subroutine is used, in which for every feature the DIFF between the randomly picked instance and
nearest- hit and miss is computed and squared.
Once the iterations are complete, the summed weights for each feature are stored in a vector wrelief . Once all n
samples have been traversed, the feature weights vector wrelief is normalized by dividing by the amount of samples n.
In this way, a ranking is constructed - which can be converted into a feature subset by applying the threshold (cid:15) to the
ranking. i.e., using the notation introduced in 4.2, a feature subset can be constructed like so:

ˆSrelief = {i | wrelief

i

≥ (cid:15)},

(21)

17

A novel evaluation methodology

TECHNICAL REPORT

given some threshold (cid:15).

18

A novel evaluation methodology

TECHNICAL REPORT

5 Evaluating Feature Rankings

Like seen in Chapter 3, the evaluation of feature- ranking and selection methods is a nontrivial process, that has been
conducted in many ways in the literature. Because of this reason, it is desired to acquire a general evaluation method
that is applicable to all feature- ranking and selection methods. The algorithms should be evaluated comprehensively:
highlighting all aspects relevant to the performance of the method. In this chapter, a reasoning is given on which
evaluation metrics are sensible to use, and which evaluation methods are recommended to use for different scenarios.

5.1 Cross-Validation and Bootstrapping

Several steps can be undertaken to provide more reliable estimates of the feature ranker performance. The goal is to
prevent the practitioner to get promising but misleading results, whilst keeping the complexity of the benchmarking
process at a reasonable level.

5.1.1 Cross-Validation

For almost any Machine Learning experiment, it is recommended to use some form of Cross-Validation (CV). Because
a model has the possibility to get ‘learn’ the training data, evaluating the performance of an estimator on just the set of
training data is dangerous practice. Evaluation metrics can be misleading, suggesting better model performance than is
actually the case. For example, an improperly cross-validated model that is employed outside the realm of the training
data might turn out to have very poor generalization performance. Thus, also in the context of benchmarking feature
ranking and selection algorithms, validation must be performed by holding out a set of the available data samples at
hand.

The most simple form of CV is a training/testing split, in which the practitioner holds out one set of the example data
for use in the testing phase - it is to be remain unseen by the model during the training phase. More robust methods
include training the model multiple times on various training datasets and evaluating on the held out testing data. This
can be done using 5-fold or 10-fold CV, where, for example, in the case of 5-fold CV 4⁄5th of the data is reserved for
training and 1⁄5th for testing, repeated for each fold - so 5 times.

What is especially important in the process of conducting CV, is that no operations are performed before having split
the data, that can inﬂuence the experimental results. For example, in the scenario where one want to select variables
to be included in some feature subset and run some prediction estimators afterward, a practitioner must be careful to
split the data before the variable selection process. This is because otherwise, the predictive estimators might gain an
unfair advantage due to having already seen the left out samples, i.e. they have already gained an advantage from the
variable selection that was performed on the full dataset. This might cause skewed and misleading estimations of the
error rate of the estimators - causing faulty models. Therefore, it is at all times important to keep in mind the ‘right’ way
of performing CV: split ﬁrst before running any operations that regard the dataset samples [Ambroise and McLachlan,
2002].

5.1.2 Bootstrapping

Bootstrapping, on the other hand, is a similar but different process. Performing a bootstrap is a classical method for
estimating statistical quantities regarding the learning process, such as variance, prediction error, or bias. The process
works by resampling the dataset with replacement B times, such to create B different permutations of the dataset.
Then, the learning process is repeated for each of the B bootstrap datasets, i.e., reﬁtting the estimators for each of
the permuted datasets. If, for example, the designated dataset is denoted like Z = (z1, z2, . . . , zb) with each sample
zi = (xi, yi), then the b-th bootstrap permutation of the dataset can be denoted as Z∗b. Such, estimates can be made of,
for example, the variance of some statistical quantity S computed over the dataset Z:

(cid:100)Var[S(Z)] =

1
B − 1

B
(cid:88)

b=1

(cid:0)S (cid:0)Z∗b(cid:1) − ¯S∗(cid:1)2

,

(22)

which can be seen to be the unbiased average of the statistical quantity S over the B bootstrap permutations of Z. Note
that the average value of the statistic S is computed like so:

¯S∗ =

(cid:88)

S (cid:0)Z∗b(cid:1) /B,

b

19

(23)

A novel evaluation methodology

TECHNICAL REPORT

i.e. by averaging S over the B bootstraps. Like such, a reasonable estimate can be made of any statistical quantity
computed over the dataset, as long as the quantity can be computed for any permutation on the dataset and enough
computational resources are available to run B repeated experiments on each of the permutations.

The practice of bootstrapping comes useful to the evaluation of feature- ranking and selection algorithms, allowing the
practitioner to better estimate quantities that would previously be less reliable estimates. Examples of such quantities
are the feature ranking algorithm stability, variance, or ﬁtting time. Especially in assessing the algorithm stability,
there exists an interest to know how the designated algorithm functions under conditions of varying samples. For this,
bootstrapping is especially useful, since the exact data generating distribution at hand is often not available- meaning no
more samples can be drawn from the distribution to generate a larger data population. In the lack of a data generating
distribution, resampling with replacement offers a solution.

5.2 Validation estimators

A straight-forward way to evaluate the performance of a feature selection algorithm is to run the ranking algorithm, and
subsequently run a ‘validation’ estimator on the selected feature subset. The quality of the selected feature subset is
then quantiﬁed through the performance of the validation estimator: when more informative and relevant features are
selected, validation estimator performance presumably goes up. The validation estimator can be conﬁgured to be any
classiﬁer or regressor, dependent on the prediction task at hand, though often choices are made from a common set of
estimators as can be seen in Table 1.

This idea can be extended to feature rankings. Many feature selection algorithms are in fact feature ranking algorithms
and do not provide a built-in mechanism to perform feature subset selection - the algorithms allow a user to deﬁne the
desired amount of features to be selected using a hyper-parameter, which then uses its feature ranking internally to
construct a feature subset, like explained in Section 4.2. It is therefore desired to also benchmark such algorithms in a
systematic way: allowing more ﬂexibility with respect to the evaluation process.

Figure 3: The process of running a validation estimator on a feature subset. In the case of a feature ranking, a validation
estimator is ﬁt for some various amount of feature subsets. Often, feature subsets with some number of the highest
ranked features are evaluated.

As can be seen in Figure 3, the process of running a validation estimator is straight-forward. Once a feature subset has
been determined by thresholding a feature ranking or using the feature subset computed by a feature selection algorithm,
a validation estimator is trained and evaluated on the held-out test set. Afterwards, a suitable evaluation metric is used
depending on the learning task at hand: classiﬁcation or regression. Note - the research scope was conﬁned to just
classiﬁcation and regression, not clustering. Exactly how many validation estimators are ﬁt to evaluate a feature ranking
depends on the ranking type (Section 4.2). In the case of a feature support vector, only the feature subset might have to
be evaluated, whilst for feature importance or ranking vectors one might want to evaluate the ﬁrst k best feature subsets.
Generally, each of the vectors are evaluated like so:

• Feature importance (Section 4.2.1). Fit k validation estimators, for the ﬁrst k best feature subsets, starting with
a feature subset including only the highest ranked feature and subsequently including lower-ranked features. k
might be chosen depending on the dataset size at hand.

• Feature support (Section 4.2.2). A feature subset was already selected by the feature selection algorithm and

such the feature subset can be directly evaluated by the validation estimator.

• Feature ranking (Section 4.2.3). A feature ranking can ﬁt k validation estimators; similarly to the feature

importance vector.

Notably, in the case where a feature ranking algorithm supports computing both feature importance or ranking vectors
and the feature support vector, simply the k best feature subsets are evaluated by the validation estimator. It is thereby
assumed that the selected feature subset is included in one of the k best feature subsets. If for some reason the selected
feature subset ˆS is not in any of the k best feature subsets, the feature subset will be evaluated by the validation estimator
separately.

20

Validation estimatorFeature subset Xfit(X_train, y_train)Fit estimatorFit estimatorscore(X_test, y_test)ClassificationRegressionR² scoreAccuracyA novel evaluation methodology

TECHNICAL REPORT

A bootstrap can be used to generate more accurate estimations of the validation process. Especially for validation
estimators that are sensible to varying permutations of the training data this process is useful - since otherwise validation
metrics could be particularly misleading. In this way, given either an accuracy- or R2 score for classiﬁcation and
regression, respectively, an averaged bootstrapped score can be computed. This is done simply by plugging in either the
R2 score or accuracy (or any other metric suitable for regression- or classiﬁcation) into Equation 23.

5.3 Apriori knowledge on relevant features

In the scenario where we know apriori which features are relevant, more direct evaluation can be applied on the
constructed feature ranking. Such apriori ground-truth information can be gained in a couple of ways: (1) by utilising
human domain expert knowledge, (2) by augmenting real-world datasets with noisy dimensions and assuming all other
dimension to be uniformly relevant or lastly (3) by generating synthetic datasets with known feature importance levels
(see Section 6.2.1. In the context of this section, the assumption is made that the feature importance scores are simply
‘known’ apriori: no restriction is made on exactly how feature relevance ground truth was retrieved.

Knowing the relevant dataset features apriori can be useful information to the feature ranking evaluation process. This
is because even though the goal of feature- ranking and selection algorithms is to separate the useful from the irrelevant
features, the performance of such a ranking is nowadays most commonly evaluated by training yet another estimator;
the ‘validation’ estimator (Section 3 and Section 5.2). This makes the evaluation scores dependent on the validation
estimator, requiring publications to have run the same validation estimator to make the results comparable between
papers. Also, for some datasets, the chosen validation estimator might be sophisticated enough to make up for any
noisy dimensions that were added - reducing the differences between the feature rankings, which is instead desired to
be ampliﬁed to determine which feature ranker is the best for which dataset. Lastly, a sophisticated validation estimator
might also perform a form of feature selection on its own, which might also skew the performance of faulty feature
rankings that included lots of noisy dimensions in its feature subset. For these reasons, it is interesting to investigate the
possibilities to ﬁnd robust and reliable evaluation methods that do not require a validation estimator.

Using the ground-truth relevant features, an accompanying evaluation metric can be constructed. Taking into considera-
tion how different feature ranking algorithms construct various types of feature rankings as described in Section 4.2, a
suitable metric can be created for each.

Feature importance (Section 4.2.1) scores are deﬁned to be a real-valued p dimensional vector, ˆw. In order to evaluate
the closeness of the vector ˆw to the dataset ground truth w, a range of metrics can be used. A ﬁrst approach would be
to regard the estimated vector as an estimation of a continuous target vector, i.e., to regard the problem as a regression
type of task. In such a perspective, each predicted feature importance would pose as a data sample. In this way, usual
metrics related to the regression task can be used.
The R2-score is one such metric that can be used in this context. This can be formalized like so:

R2 = 1 −

RSS
TSS

where

RSS =

TSS =

n
(cid:88)

i=1
n
(cid:88)

i=1

(yi − f (xi))2 =

p
(cid:88)

i=1

(wi − ˆwi)2 , and

(yi − ¯y)2 =

p
(cid:88)

i=1

(wi − ¯w)2 ,

(24)

which can be used to evaluate the closeness of the predicted feature importance vector ˆw to the ground-truth feature
importance vector w.

The logistic-loss, or cross-entropy score is another metric which can be employed to quantify the quality of a feature
importance vector ˆw. In this scenario, however, the target variables are regarded to be binary labels, which therefore
means the predicted feature importance vector ˆw is to be compared against s instead of w. Remember, from
Section 4.2.2, that s is the p-dimensional vector indicating with a boolean whether or not the feature is relevant, i.e.
s ∈ Bp. Such, a zero indicates a feature is not relevant; and a one indicates the feature is informative or relevant. In this
way, a feature ranker approximates the true binary relevance labels s with probability values arranged in the probability
vector ˆw. The logistic loss can be formalized like so:

21

A novel evaluation methodology

TECHNICAL REPORT

Llog(y, p) = − log Pr(y | p) = −(y log(p) + (1 − y) log(1 − p))

substituting for si and ˆwi

Llog(si, ˆwi) = − log Pr(si | ˆwi) = −( ˆwi log( ˆwi) + (1 − si) log(1 − ˆwi))

averaged over all vector components

(25)

Llog(s, ˆw) = −

1
p

p
(cid:88)

i=1

( ˆwi log( ˆwi) + (1 − si) log(1 − ˆwi))

Feature support (Section 4.2.2) is evaluated differently. Just like in the logistic-loss metric, the ground truth relevant
labels are encoded as binary labels, i.e. the vector s is used as the ground-truth. Now, however, the predicted targets are
also encoded as binary labels, i.e. the vector ˆs is used. This means that the problem is to be viewed in the supervised
classiﬁcation perspective and all metrics accompanying such task are available.

Classiﬁcation accuracy is such a metric, measuring simply the average number of correct predictions were made
between the predicted- and true target labels. It can be deﬁned as such:

accuracy(y, ˆy) =

1
n

n
(cid:88)

i=1

1 (ˆyi = yi)

substituting for si and ˆsi

(26)

accuracy(s, ˆs) =

1
p

p
(cid:88)

i=1

1 (ˆsi = si) ,

where 1(x) is the indicator function [Davis, 2004]. Using this metric, the amount of useful features included in a feature
subset is rewarded with higher accuracy scores accordingly.

Feature rankings (Section 4.2.3) can be evaluated similarly to feature importance scores - thereby also requiring
normalization. Presume that besides the relevance of the features is known as a binary value, also the order of relevance
is known, i.e. which features are more relevant than others. This is very similar to the feature importance scores: though
the difference is that the feature importance scores must ﬁrst be converted to a ranking such to allow for meaningful
comparison. In this reasoning, both the predicted- and the ground truth feature ranking vectors, which are ˆr and r
respectively, can be normalized using Equation 2 to obtain ˆw and w, respectively.

In this way, the normalized vectors ˆw and w can again be considered probability vectors, such that metrics like the
R2-score can be used: just like for the feature importance scores.

One has to keep in mind, however, not to intermix the feature importance and feature ranking scores with each other:
even though the same metric is used to convert to summarize the predicted feature- importance and ranking vectors
into a single scalar, the scorings are built from different vectors to begin with. The R2-score coming from the feature
importance vectors might have an unfair advantage due to the fact that they have ﬂoating-point precision on their
approximations, whilst the feature ranking vectors r are converted from the integer domain to be normalized into
ﬂoating-point numbers.

5.4 Stability

The stability of any algorithm is an important facet of the total method performance - which must not be overlooked or
forgotten. This is for in many applications, only robust algorithms can be systematically relied on. Although it is only
natural for algorithms to vary in ﬁtting behavior under different permutations of the sample set, it is at all times desired
to get an algorithm that is as stable as possible. To evaluate the stability of feature ranking algorithms, an easy method
is to take several bootstrap permutations of the dataset (Section 5.1.2), to simulate drawing new samples from the data
generating distribution at hand. Assuming such a bootstrapping procedure to be in place, several metrics can be used to
quantify stability.

5.4.1 Stability of feature importance vectors

Feature importance scores are deﬁned to be p dimensional vectors in the domain of real numbers R (Section 4.2.1).
Accordingly, the matrix containing B such vectors is the B × p dimensional matrix in R, denoted as ˆW. A straight-
forward way to assess the stability of the feature importance matrix is to compute the variance for each of the p

22

A novel evaluation methodology

TECHNICAL REPORT

dimensions over B bootstraps, i.e. Eq 22 is used by plugging in a column of the feature importance matrix ˆW as the
statistical quantity S:

(cid:100)Var[ ˆW:,i] =

1
B − 1

B
(cid:88)

b=1

(cid:16) ˆWb,i −

¯ˆW∗

:,i

(cid:17)2

,

(27)

¯ˆW∗

where
each dimension can be computed for a single feature ranker.

:,i is the average feature importance score for the i-th feature over B bootstraps. In this way, the variance over

To summarize the variances over all dimensions, their summation might be taken, i.e., the variances are summarized as
the scalar (cid:80)p
i=1 (cid:100)Var[ ˆW:,i]. Alternatively, one might want to weight such summation, in order to reﬂect the desire to
penalize instabilities in the ranking of relevant features more so than instabilities in the ranking of irrelevant features. In
the case where the ground-truth feature importance scores are given, i.e. w is known, this vector might be used to apply
a weighting scheme to the summation of the variances. One can do so by taking the inverse normalized ground truth
wi (cid:100)Var[ ˆW:,i].
feature importance score, i.e. 1
wi

for the i-th feature. Such, the summarization of ˆW now becomes (cid:80)p

i=1

1

5.4.2 Stability of feature support vectors

This time around, a way to quantify the stability of a feature subset is desired, i.e., the feature support vector ˆs. A
measure for the quantifying stability of set permutations that comes easily to mind might be the Hamming distance
between multiple pairs of algorithm runs, i.e. ran on the same dataset with varied sample populations. Indeed, in [Dunne
et al., 2002] a measure based on Hamming distance is proposed. Reports also exist on numerous other approaches, like
an entropy based measure [Kˇrížek et al., 2007], a measure based on the cardinality of intersection measure [Kuncheva,
2007] and a measure based on correlation coefﬁcients [Kalousis et al., 2007].

Desired properties of any measure are important to deﬁne in concrete manner. To compare the strength of any of these
measures, the general objective for exactly what information we want to convey in a stability metric should be verbalized
ﬁrst. In [Mohana Chelvan and Perumal, 2016], three desired stability metric properties are expressed: (1) Monotonicity,
(2) Limits and (3) Correction for chance. Given the proposed measures in the literature, only few measures satisﬁed all
properties. In another paper [Nogueira et al., 2018], however, the authors extend the set of desired properties to include
another two: (4) the stability estimator must be Fully deﬁned and (5) Maximum Stability ↔ Deterministic Selection -
meaning that a maximum stability value should be achieved if-and-only if all feature sets are exactly identical.

A measure for quantifying feature subset stability was proposed in [Nogueira et al., 2018], using the set of newly
proposed desired properties. Having explored a statistically sound method to deﬁne a metric satisfying all ﬁve properties,
the authors proposed a novel stability estimator, adjusted to this paper’s terminology:

ˆΦ(ˆSboot) = 1 −

E

(cid:80)p

1
p
(cid:104) 1
(cid:80)p
p

i=1 σ2
i
i=1 σ2

i |H0

(cid:105) = 1 −

1
p
¯k
p

(cid:80)p

i=1 σ2
i
(cid:17) ,
1 − ¯k
p

(cid:16)

(28)

where ˆΦ resembles the stability estimate of the feature subsets arranged in ˆSboot (Eq 11). Remember, that each set in
ˆSboot is a feature subset containing the indices of the selected features. Each of the sets in ˆSboot is created by running
the feature ranker on a permutation of the datasets, i.e. a resampling of a dataset equal probability distribution and
i as the unbiased sample variance of the selection of the ith feature
dataset properties. Furthermore, the authors deﬁne σ2
and ¯k as the average number of features selected over the B feature sets. Given this paper’s goal, there is no necessity
for going into further mathematical details - for this we refer to the paper itself.

What is important is that this new measure satisﬁes all desirable properties for quantifying stability, as was proven in
the paper. Accompanying the novel deﬁnition are instructions for computing conﬁdence intervals and for performing a
hypothesis testing for comparing various method stabilities. Exploring feature selection stability values given a set of
parameter choices not only allows for choosing better hyperparameters, but also allows for comparing stabilities over
various feature selection methods.

5.4.3 Stability of feature ranking vectors

Lastly, the stability of feature ranking vectors is also to be quantiﬁed. Since the stability measure for feature support
was all about working with sets instead of vectors, the best option is to again normalize to a feature importance vector

23

A novel evaluation methodology

TECHNICAL REPORT

and compute the variance thereof. The summarization of the variances might again be weighted using the ground-truth
feature importance vector w, if it is available. See Section 5.4.1.

5.5 Time complexity

Another metric to be taken into account is algorithm complexity, which manifests itself in three interlinked aspects:
time-, storage- and algorithm simplicity. Like the classical principle Occam’s razor implies - there at all times exists
a preference for simpler models over more complex ones, especially in the case both accomplish the same feat. So,
an understandable model is preferred that performs limited computational steps in order to restrain time- and storage
complexity from rising too high.

Although measuring algorithm time- and storage requirements does provide some understanding, more insightful would
be a theoretical description of the complexity in Big O notation. Theoretical complexities are harder to obtain though -
leaving many authors to resort solely to measurements. Nonetheless, complexity analysis is recommended to be part of
any feature selection method comparison, for it is a critical aspect to consider.

5.6 Statistical integrity

Given that the above discussed evaluation methods are computed, a statistical test is ought to be applied to provide
convincing evidence for any algorithm’s superiority. Like explained in [Demšar, 2006], many papers make implicit
hypotheses acclaiming improved performance over existing methods. Although the chosen metrics might have been
appropriate to statistically show performance gains as signiﬁcant, the results are less reliable when left not validated by
a statistical test. Therefore, a statistical veriﬁcation step is required in the feature selection evaluation process.

Comparing multiple feature selectors is a non-trivial problem, which can be seen as the problem of comparing multiple
classiﬁers. To compare multiple classiﬁers, [Demšar, 2006] recommends the Wilcoxon signed ranks test for comparison
of two classiﬁers and the Friedman test for comparison of more classiﬁers given multiple datasets. Accompanying
the Friedman test, it is recommended to perform corresponding post-hoc tests, such as the Nemenyi test [Nemenyi,
1963]. The usually popular ANOVA test was advised against because, given the context of machine learning, ANOVA
assumptions are violated, e.g. ANOVA assumes samples are drawn from normal distributions and the requirement of
random variables having equal variance.

A Wilcoxon signed ranks test and the Friedman test, are for these reasons recommended for two- or more methods
respectively, to statistically verify signiﬁcant differences in performance of feature selection algorithms over the others
using some summarizing scalar per feature selection method per dataset. The Friedman test is recommended to be
accompanied with a post-hoc test, like Nemeyi’s post-hoc test.

24

A novel evaluation methodology

TECHNICAL REPORT

6 Building a benchmarking pipeline

To facilitate conducting a comprehensive experiment, an accompanying software pipeline is to be built. Such a pipeline
must be able to run feature ranking methods, select feature subsets, and validate the quality of the feature subsets using
some validation estimator. After both the feature ranking- and validation step, evaluation metrics have to be computed -
assessing the performance of the feature ranking both directly- and indirectly. After the metrics have been computed,
results should be stored either on-disk or be uploaded to the cloud, after which they should be made interpretable by
visualizing them. Furthermore, because the ‘direct’ evaluation of feature rankings is only possible when a ground-truth
reference of the relevant features exists (Section 5.3), it is also desired to be able to generate synthetic datasets built
in to the framework. In this way, a practitioner using the pipeline can easily design synthetic datasets suited for the
purpose at hand.

Challenges lie mainly in the scalability of the system. To conduct an experiment of moderate size, a single machine can
be used for all stages of the pipeline: run the algorithms, collect and visualize the data. In such a small-scale context,
the data- collection and visualization stage could as well be manual- no huge amounts of data have to be processed, so
such a collection- and visualization stage could be run from a single script. When a larger experiment is desired to be
run, however, a more sophisticated system is desired. Practitioners must not be restricted in running the pipeline on a
single CPU or single machine, i.e., the system should support both horizontal- and vertical scaling. To design a system
that can run experiments in such scalable and distributed ways, the system must be modular to facilitate distributing
jobs over multiple machines, although it is also necessary to aggregate results of individual jobs as part of a pipeline
step itself - requiring a piece of code to be aware of every job having ﬁnished.

An implementation of such a pipeline was made in Python, for the purposes of this paper. Many of the challenges
described above were addressed and overcome by using an architecture that was built from the ground up to be scalable.
Both the data- collection and aggregation processes were automated - after having run the pipeline for a collection of
feature ranking methods and datasets, results are automatically stored, aggregated, and visualized. In this chapter, the
reader is explained the general architecture of the system and the components that make the system work - after which
the usage of the pipeline is shown along with its capabilities for scaling. The chapter is meant to be explanatory at ﬁrst
and provide concrete instructions and examples afterward.

6.1 Architecture

To ﬁrst get an idea of the structure of the system, comments are made on its architecture. The pipeline has two main
components: an encapsulating pipeline ‘run’ script and the pipeline itself. The pipeline run script is to be called the
main script: allowing us to differentiate easily between the two. Whilst the main script is designed to be generic and
work for any ML related benchmarking task, the pipeline itself is to be tailor made for the speciﬁc task at hand. Ideally,
one could swap out the pipeline for another (custom built) one, but still using the main script mechanics: allowing a
practitioner to create various but related tasks using a single framework.

6.1.1 The main script

The sole purpose of the main script is to do three things: (1) load a dataset, (2) split the dataset using a CV method and
lastly (3) run the pipeline. In this procedure, any dataset is loaded into the system using an adapter - allowing data
to come in from any source by use of a plug-able architecture (Section 6.2.1). Immediately after the dataset has been
loaded a CV split is conducted. In this way, because the cross-validation step is separated from all the pipeline steps
itself, it is less likely that a pipeline can go faulty on the CV process - since the data splitting operation was already
conducted once a pipeline implementer gets their hands on the data. An illustration of the main script, with as part of it
the pipeline, can be seen in Figure 4.

An important facet facilitating the operations in the main script is the conﬁg object (as seen at the top-left in Figure 4).
The conﬁg object contains all conﬁguration and parameters relevant to all pipeline steps, essentially storing all
instructions required to perform a single pipeline run. It is thereby important to store all conﬁguration and not to leave
any parameters set at random or use temporary variables - the pipeline execution is desired to be deterministic and
therefore making its results reproducible. In this way, experiments conducted using the pipeline are better suited for
scientiﬁc experiments - a situation in which a user desires to be able to store exactly the state that produces a certain
result and explain its ﬁndings in minute detail.

To facilitate conﬁguring the system in such a way, several options exist in the Python ecosystem. One could use
the built-in optparse module, or its Python 3 counterpart argparse. The built-in packages leave much room for
improvement, however. The modules do not allow loading conﬁg from a ﬁle, do not allow nesting, has only basic input
validation and provides no tools for more complex conﬁguration scenarios like the pipeline at hand. A solution for this

25

A novel evaluation methodology

TECHNICAL REPORT

Figure 4: A general architecture schematic of the built benchmarking pipeline’s main script: providing data and
instructions to the pipeline. Once the designated dataset is loaded using a conﬁgured adapter, the data is fed to the
pipeline after a CV split. The pipeline is ﬁrst ﬁt using the training data and then scored using the testing data, after
which the results are collected and stored.

is the Hydra library [Yadan, 2019], a framework providing powerful utilities for conﬁguring a Python application. The
package allows one to create hierarchical and composable conﬁgurations, to be set using either the command-line or
using yaml ﬁles. A simple yaml deﬁnition conﬁguring the main script to use the ‘Iris Flowers’ dataset and perform a
K-fold CV can be seen in Listing 1.

Listing 1: A simple conﬁguration for the main script, using the ‘Iris Flowers’ dataset and K-fold CV.

d e f a u l t s :

− b a s e _ c o n f i g
− b a s e _ r a n k _ a n d _ v a l i d a t e
− d a t a s e t :
− cv : k f o l d
− c a l l b a c k s :
− wandb

i r i s

− s t o r a g e _ p r o v i d e r : wandb

It can also be observed, in Listing 1, that there exist conﬁguration possibilities for ‘callbacks’ and a ‘storage provider’.
These allow customizing a back-end for sending the result data to and an adapter for storing- or caching pipeline steps,
respectively. Callbacks and the storage provider will be elaborated upon in Section 6.2.6 and Section 6.2.5, respectively.
First, a look is taken at the pipeline implementation for feature ranking and subset validation, which is called Rank and
Validate.

6.1.2 The ‘Rank and Validate’ pipeline

With the main script now deﬁned, a better look can be taken at the pipeline implementation itself. Whereas the main
script was still a general-purpose ML benchmark runner, an implementation of the system by means of a pipeline is
task-speciﬁc. i.e., one such pipeline is meant to execute a speciﬁc ML task, such as performing evaluation on a feature
ranker. The pipeline does so in a couple steps:

1. Resample the dataset. Using the conﬁgured settings, the dataset is resampled. Options that are ought to be
supported are shufﬂing, and more importantly; bootstrapping. It is thereby important to ﬁxate a random ‘seed’,
which allows one to reproduce the resampling exactly.

2. Rank features. Next, the feature ranker is set to work. The resampled data is passed to the ranker’s fit

function, after which the stored ranker might be cached.

3. Validate feature subset. Finally, after the ranker has been ﬁt, its ranking is used to validate its feature subsets,
using some validation estimator. Again, the ﬁt validation estimator might be cached. The ﬁnal results are
stored to disk and/or uploaded to the cloud.

This process is illustrated in Figure 5.

As can be seen illustrated by rectangles in Figure 5, the above steps are executed in a speciﬁc sequence of loops. First
of all, all steps (1-3) are passed through twice: once for the training data using the fit function and once using the
testing data using the score function. The reasoning behind the division into these two steps is to force keeping
application logic separate, i.e., to enforce a separation of concerns. Because the ﬁtting step is essentially different
than the scoring step, it makes sense to separate them. This separation also allows better implementations of caching,

26

DatasetLoaderDatasetPipelineCrossValidatorTrain setTest setResultsAdapterConfig { }K-FoldTrain / testOpenMLSynthetic....A novel evaluation methodology

TECHNICAL REPORT

Figure 5: The ‘Rank and validate’ pipeline. The pipeline performs feature ranking, subset selection and subset validation,
in a sequence of loops, such to facilitate bootstrapping. The pipeline is ﬁrst ﬁt and then scored.

because a practitioner is enforced to store all estimator related state inside the instance itself. This distinction is on par
with the sci-kit learn [Pedregosa et al., 2011] API.

For each of the two passes, the entire process is run B separate times: once for each bootstrap. The random resampling
seed is changed with each bootstrap iteration accordingly: making sure each bootstrap dataset is a different random
resampling of the dataset, but yet keep the pipeline to be reproducible. The results from each bootstrap are then
stored with each bootstrap random state attached, allowing a practitioner to differentiate between results and possibly
aggregate- and group them.

Finally, each of the bootstrap iterations performs a number of feature subset validations. Ideally, in the case where
a dataset has p dimensions and a ranker has computed a feature importance score for each of them, it would be
desired to validate p feature subsets - the ﬁrst subset starting with only the best ranked feature and subsequently
including lesser-ranked features. However, this would fast become computationally intractable: in the case where p is of
considerable size, the amount of feature subsets to validate becomes vast, fast. A compromise between the thoroughness
and computational tractability is to only validate some number of the best feature subsets up to a certain dimension. The
choice that is made in this paper is to validate at most 50 feature subsets and possibly less if p < 50. i.e., the min(p, 50)
best feature subsets are evaluated. This means for every k ∈ {1, . . . , min(p, 50)} the k best features are used for subset
validation.

The scoring process happens only after all estimators in the pipeline have been ﬁt. Once the ﬁtting phase is complete,
the testing sets will be passed to the pipeline’s score function. Each estimator gets the opportunity to generate some
score given the testing dataset. Because both the rankers and estimators are considered estimators in the system, a
ranker will also get such scoring opportunity. More speciﬁcally so, rankers and validators are scored in the following
ways:

• Rankers are scored using the dataset ground-truth relevant features, if available. Thereby metrics as described

in Section 5.3 are used: the R2 score and the logistic loss.

• Validators are scored like described in Section 5.2. i.e., the scoring depends on the dataset task: the R2 score

is used in the case of regression, and the accuracy score is used in case of classiﬁcation.

That said, a general picture of the pipeline architecture and data ﬂow is now obtained. The pipeline is, however, more
complex. The pipeline supports multiprocessing, distributed computing, caching, and data visualization. Moreover,
each pipeline component has very different functionalities and can be conﬁgured separately from the rest of the pipeline.
Therefore, a look is taken into the separate pipeline components ﬁrst in Section 6.2, to then take an in-depth view of the
pipeline execution afterwards, in Section 6.3. Finally, an elaboration is made on data visualization in Section 6.4.

6.2 Components

Each pipeline component can be conﬁgured using their own yaml ﬁles and has a reasonable level of isolation with
respect to its module structure. The main components are discussed brieﬂy.

27

ValidationScoreRankerEstimatorReliefFBoruta..ValidatorEstimatorDTk-NN..Select Feature SubsetRanking scoreResampleBootstrapShuffle..Data X, yPipelineRank and validateBootstrap for every b ∈ {1, …, B}Validate for every k ∈ {1, …, min(p, 50)}:Use k best featuresrandom_state = bA novel evaluation methodology

TECHNICAL REPORT

6.2.1 Datasets

The system allows using datasets from numerous sources, using adapters. Adapters are designed to be generic interfaces
for fetching data from any source, as long as it can be expressed in two matrices X and y. Although some adapters are
available built-in to the framework, new adapters can easily be conﬁgured. Any such adapter must only implement a
get_data function, which returns the two matrices X and y, given a conﬁguration object facilitated by Hydra (see
Section 6.1.1). Such adapters can implement any logic that retrieves the data themselves, be it logic for loading a dataset
from local disk or from an internet platform.

OpenML is one such platform. OpenML [Vanschoren et al., 2014] is a platform for sharing and organizing data,
completely open to all. Due to the ﬂexible adapter structure of the system, and thanks to the availability of a Python
package providing interfacing support with the platform, an integration with OpenML was made possible. Using the
interface, datasets are fetched from a remote and cached to disk once downloaded. In this way, a cached version of
the dataset is used in subsequent requests - making sure a dataset is not unnecessarily downloaded again. Integration
with the platform allows users of the pipeline to access a large library of datasets, varying in type and domain. The
dataset library features both regression and classiﬁcation datasets, and of various target types, i.e., both univariate and
multivariate. Also, the platform has multiple existing benchmark suites that can be used, such as the OpenML-CC18
[Bischl et al., 2019]. An example deﬁnition of a conﬁguration ﬁle for loading an OpenML dataset is to be found in
Listing 2.

Listing 2: A dataset conﬁg for loading the ‘Iris Flowers’ dataset from OpenML.

name :
t a s k :
a d a p t e r :

I r i s F l o w e r s
c l a s s i f i c a t i o n

f s e v a l . a d a p t e r s . OpenML

_ t a r g e t _ :
d a t a s e t _ i d : 61
t a r g e t _ c o l u m n :

c l a s s

It can be seen, in Listing 2, that a ‘target’ class can be deﬁned. This is passed to Hydra, which in turn instantiates the
class with the given conﬁguration. This is one of the core features that makes possible the modular adapter architecture
that is in place.

Synthetic datasets can also be generated using the modular adapter-structure in place. By viewing a synthetic data
generator as yet another adapter, providing data given a conﬁguration object, both real-world and synthetic datasets
can be used and generated using a single interface. A straight-forward way to generate synthetic datasets is to use the
sci-kit learn datasets module, which provides functions for drawing samples from a wide variety of distributions.
Aside from generic generators, there also exists support for drawing from more speciﬁc distributions, such as ‘two
interleaving half circles’, an ‘S curve’ or a ‘swiss roll’ distribution.

Most interesting for the current purposes, however, are two functions for generating classiﬁcation- and regression
datasets, make_classification and make_regression, respectively. Besides allowing one to conﬁgure the amount
of samples and dimensions to generate, one can also deﬁne the desired informative-, redundant-, repeated- and irrelevant
features to generate. In this way, a ground-truth for the relevant features can be constructed. In the case of regression,
one can also retrieve the coefﬁcients, or weighting, for each feature which are to be approximated, i.e., the desired
feature importance for each feature. This means that whilst in the case of classiﬁcation one has access to a binary
ground-truth, an exact measure of the desired feature importance can be retrieved in the case of regression. An example
deﬁnition of a synthetic classiﬁcation dataset can be found in Listing 3.

Listing 3: A dataset conﬁg generating a synthetic dataset using the sci-kit learn make_classification function.

name : S y n c l f h a r d
t a s k :
a d a p t e r :

c l a s s i f i c a t i o n

s k l e a r n . d a t a s e t s . m a k e _ c l a s s i f i c a t i o n

_ t a r g e t _ :
c l a s s _ s e p : 0 . 8
n _ c l a s s e s : 3
n _ c l u s t e r s _ p e r _ c l a s s : 3
n _ f e a t u r e s : 50
n _ i n f o r m a t i v e : 4
n _ r e d u n d a n t : 0
n _ r e p e a t e d : 0
n _ s a m p l e s : 10000

28

A novel evaluation methodology

TECHNICAL REPORT

r a n d o m _ s t a t e : 0
s h u f f l e :

f a l s e

f e a t u r e _ i m p o r t a n c e s :

X [ : , 0 : 4 ] : 1 . 0

Worth noting in Listing 3 is the presence of a ‘feature_importances’ attribute. This is the exact deﬁnition of the
ground-truth relevant features, used by the pipeline. The system allows one to deﬁne the feature importances as
numpy-indexed selectors, operating on some matrix the same size as X. Every instance and every accompanying
dimension can have a speciﬁc ground-truth weighting, although for the current purposes only global feature- ranking
and selection are considered. In the example above, it can be seen that the ﬁrst 5 features are deﬁned to be equally
relevant: and therefore all have a uniform weighting of 1.0.

6.2.2 Cross-Validation

The Cross-Validator can be conﬁgured to be any class that has a split function, taking in the matrices X and y and
returning a generator. The generator must then be an iterator over the number of conﬁgured folds, with each split
returning the designated training- and testing data. An example conﬁguration for a 5-Fold CV split can be seen in
Listing 4.

Listing 4: A conﬁg for 5-Fold CV, with shufﬂing. The split is reproducible due to the ﬁxed random seed.

name : K− F o l d
s p l i t t e r :

s k l e a r n . m o d e l _ s e l e c t i o n . KFold

_ t a r g e t _ :
n _ s p l i t s : 5
s h u f f l e : T r u e
r a n d o m _ s t a t e : 0
f o l d : 0

As is seen in Listing 4, a CV technique can be conﬁgured to be a sci-kit learn module. Another important thing to note
is the ﬁxed ‘fold’ attribute, set in this case to 0. Because the pipeline always executes exactly one fold, performing a
K-Fold CV is done by executing the pipeline multiple times using different values of fold.

6.2.3 Resampling

Built in, there are two options for resampling. There exist options for (1) bootstrap sampling and (2) shufﬂe resampling.
Whilst the latter only changes the permutation of the dataset, the former performs resampling with replacement, therefore
actually changing the dataset distribution. Bootstrap resampling is conﬁgured like as can be seen in Listing 5.

Listing 5: A conﬁg for bootstrap sampling using a sample size of 1.0, i.e, using the full dataset.

name : B o o t s t r a p
r e p l a c e :
s a m p l e _ s i z e : 1 . 0 0

t r u e

Noteworthy, in Listing 5, is the ‘sample size’ parameter. If needed, the dataset can be down- or up-sampled. Although
up-sampling generally does not make much sense, down-sampling can increase the stochasticity of the bootstrapped
dataset permutation. This means that, if desired, one can create more ‘random’ distributions by setting sample_size
to a lower number. Lastly, a random seed can be ﬁxed, like as can be seen in the pipeline schematic, Figure 5: the
resampling random_state is set to the bootstrap number. In this way, the resampling is exactly reproducible.

6.2.4 Estimators

In the context of the current system, an ‘estimator’ can mean two things. An estimator is either: (1) a feature ranker or
(2) a validation estimator. Still, the two are combined into a single estimator interface: which is because both can share
much of the same API for interacting with them. Both implement a ﬁtting- and scoring step, and can be of regressor- or
classiﬁer type. Also, to support these two learning tasks in a better way, any estimator is encapsulated in a generalized
estimator interface. An example conﬁguration for a ranking estimator can be seen in Listing 6.

Listing 6: An estimator conﬁg for ReliefF [Kononenko, 1994].

name : R e l i e f F
c l a s s i f i e r :

29

A novel evaluation methodology

TECHNICAL REPORT

e s t i m a t o r :

_ t a r g e t _ :

s k r e b a t e . R e l i e f F

r e g r e s s o r :

e s t i m a t o r :

_ t a r g e t _ :

s k r e b a t e . R e l i e f F
e s t i m a t e s _ f e a t u r e _ i m p o r t a n c e s :
e s t i m a t e s _ f e a t u r e _ s u p p o r t :

f a l s e

t r u e

Both a regressor and classiﬁer were deﬁned in a single interface, as can be observed in Listing 6. Although the two
were in this case deﬁned to be the same, they might have as well pointed to different modules. Under the hood, the
system detects the conﬁgured dataset task at initialization and instantiates either one of the two modules. Furthermore,
the system has to be told the capabilities of the estimator, e.g., whether it estimates targets, feature importance or feature
support. This is done using the boolean attributes starting with ‘estimates_’: where in this case the ReliefF estimator
estimates just the feature importance, but no feature subset.

Validation estimators are conﬁgured in much the same way - and can also have both a regressor and classiﬁer deﬁned.
The interface also allows indicating an estimator’s support for multioutput learning or the estimator requiring a strictly
positive dataset to work.

6.2.5 Storage providers

A storage provider is deﬁned to be a bridge between the pipeline and a ﬁle system, independent of whether this ﬁle
system is on the local disk or in the cloud. Because in the pipeline it is desired that the ﬁt estimators can be cached, a
storage system must be built accordingly. Independent of exactly where the ﬁles are stored, a storage provider can be
constructed by implementing routines for saving and restoring ﬁles. As long as the storage provider returns a usable
ﬁle object given a ﬁle path, the system is ambivalent about how the ﬁle is loaded. A schematic of how this storage
provider works is to be seen in Figure 6.

Figure 6: An illustration of the Storage Provider functionality. The storage provider can be used to save- and restore
ﬁles, using both a local- or remote ﬁle system.

In the example in Figure 6, it can be seen that the storage provider is used by an estimator to save- and restore data.
Such data can, for example, be a pickle ﬁle used to serialize- and deserialize a ﬁt estimator object.

Caching is supported in this way. By storing a reference to the ﬁle-path right in the job conﬁguration itself, cached ﬁles
can be reused via either the local- or remote ﬁle system. The pipeline allows conﬁguring exactly which parts of the
pipeline allow using cached versions of their ﬁt estimators, such that in any subsequent run of a job, some estimators
can be reused and others overwritten. This functionality facilitates running multiple validation estimators given a single
ﬁt feature ranker, for example.

6.2.6 Callbacks

The ﬁnal component to discuss is the Callback. A callback can be used to send data to a back end during the execution of
the pipeline. Such data can be conﬁgurations, objects containing metrics or entire tables with results. The requirements
for implementing such a callback are low, since any such data can be stored as an opt-in. If a pipeline user desires so,
the usage of a callback can be completely ignored, storing only data to the disk using the storage provider. Sending
metrics to a callback can, however, be a powerful way to send data to a database or cloud back-end right in the pipeline.
In this way, the usual data collection phase can be automatized. About an implementation of such a callback can be
read in Section 6.4. First, however, a closer look is taken at the pipeline execution and its scalability.

6.3 Execution

The pipeline execution happens in a predeﬁned number of steps. Because the pipeline is desired to be run in parallel and
in a distributed way, the execution of such a system is nontrivial. Support for scaling in both the horizontal- and vertical
directions are required, i.e., the pipeline must be able to scale over more machines but must also scale over more system

30

EstimatorStorageProviderLocalStorageProviderWandbStorageProvidersaveDataDatarestoreA novel evaluation methodology

TECHNICAL REPORT

resources such as CPU or RAM. Systems that fulﬁl such desires are High-Performance Computing (HPC) systems,
which are designed to support scalable computing [Ristov et al., 2016]. An elaboration is made on an implementation
of such a scalable architecture.

6.3.1 Multiprocessing

To facilitate vertical scaling, a multiprocessing implementation is built. It is thereby the goal to best utilize the system
resources at hand: maximizing the usage of the available processors. To do so, certain iterative parts of the pipeline
were wrapped in an encapsulating ‘Experiment’ module. The task of the module is, then, to pick up wherever the
pipeline would iterate over a number of steps, and add multiprocessing support. As could be read in Section 6.1.2
and seen in Figure 5, the pipeline has two main loops: the ﬁrst (1) runs B bootstraps and the second (2) validates
min(p, 50) feature subsets. In this implementation, the choice was made to distribute the former loop, i.e., to distribute
the bootstraps over multiple CPU’s. An illustration making clear the functionality of such a multiprocessing approach
can be seen in Figure 7.

Figure 7: The pipeline multiprocessing distribution process. In the pipeline ﬁt step the set of bootstraps is distributed
over the available CPU’s.

As could be seen in Figure 7, every bootstrap resampling job is assigned to a processor. In case more bootstraps
are to be run than the number of available processors, i.e., B > # CPU’s, some CPU’s are assigned more than one
bootstrap resampling job. In HPC environments, it is common to have a dozen CPU’s available in a single job. Given
that a reasonable but still feasible number of bootstraps to run is also a few dozen, it can be straight-forward option to
conﬁgure the number of bootstraps to run as the number of CPU’s available in a job on the designated HPC environment.

Important to note in such a multiprocessing job, is the available scope of variables accessible inside any subprocess.
Because each subprocess runs isolated on a CPU, it cannot access variables exclusively available to the main thread
process. In the case that such a subprocess requires access to variables from the main thread, either one of two options
can be considered. One option is to pass down a copy of the variables in question to the subprocess: this, however, does
not apply when such variables change during the program execution. In that case, the second option is to communicate
between the subprocess and the main thread. In this way, functionality such as storing cache ﬁles (Section 6.2.5) can
still be done in the main thread, if it is deemed necessary.

6.3.2 Distributed computing

To make the pipeline horizontally scalable, an approach must be constructed for distributing the processes over multiple
machines. Although running multiple jobs in a HPC environment can be a relatively straight-forward practice, a
choice must be made on the amount of processing to put in a single job. For example, if a system were in place that
automatically distributes jobs over a number of processors and machines by itself, the horizontal- and vertical scaling
approaches could be tackled in a single solution. In this scenario, an automatic job scheduler would assign very small
jobs to nodes and processors by itself; the smallest experimental unit could be running one bootstrap on a single feature
ranker. However, such an approach is rather complex and often unsupported by existing HPC systems. Therefore, the
choice is to go with a hybrid approach - a single HPC job is assigned multiple experiments. In this way, long queue
waiting times as a consequence of running many small jobs are prevented.

Such an approach can be accomplished by the use of a second job queue, next to the HPC job queue. The queue of
choice is RQ, a library for keeping track of an arbitrary number of job queues in Redis which then can be executed by
workers. Workers are run on the HPC system itself and are built solely to execute work coming from the queues. The
library is built to be fault-tolerant, scalable, and easy to use. The basic idea is that during the enqueueing phase, a piece
of Python code is serialized into a binary format and stored in Redis into a First-In-First-Out queue. Then, once the
time has come that the job can be executed, the job execution code is deserialized from Redis back into Python code,
and executed. This functionality can be seen illustrated in Figure 8.

It can be seen in Figure 8, that the Redis database acts as an intermediary between the user and the HPC system. Whilst
the queues in Redis may contain hundreds or thousands of jobs, the amount of jobs on the HPC system only need to be

31

PipelineBootstrap for every b ∈ {1, …, B}CPU’sb1Experimentb2b3b4b5b6b7b8b9b10b11b12b13b14b15b16b17b18b19b20A novel evaluation methodology

TECHNICAL REPORT

Figure 8: The RQ enqueueing- and dequeueing process. Jobs are serialized on the client and stored in Redis FIFO
queue. Workers on the HPC system process the jobs on the designated queues.

several: one job for each worker. Each worker may run on its own node, or share a node by means of a virtual machine.
In overall, the RQ-based approach has a number of beneﬁts.

• First of all, the HPC system is not overloaded by small jobs. Instead, RQ is better designed to store and process
a multitude of small jobs - preventing overhead and long queuing times in the HPC system. This is because the
only jobs that are launched on the HPC system are the workers - which can be conﬁgured to last an arbitrary
amount of time, or to terminate once its queue is empty.

• A second beneﬁt is that RQ has good support for handling job failures. RQ allows automatically retrying failed
jobs, with optionally set time intervals between tries, such that in the case of a network time-out or system
error the job is be restarted without further human input. Furthermore, if the job still fails execution after its
pre-conﬁgured number of retries were completed, the job is not just removed from all queues. Instead, the job
is moved to a separate ‘failed job’ queue. The error logs for jobs in this queue are easily inspected using a
dashboard interface for RQ.

• A last beneﬁt is the serialization- and deserialization process in RQ. Because Redis does not just store
instructions to execute the code, but the actual execution code itself, code on the HPC system might be changed
during the time a job is in queue. This gives a practitioner more ﬂexibility to change program code and run the
pipeline at the same time.

As a side beneﬁt, the Hydra library used for conﬁguring the system has built-in support for working with RQ. By use of
a plugin, the pipeline can be conﬁgured to enqueue its jobs to RQ instead of executing right away. An important note is
that due to the serialization process the Python version during enqueueing and dequeueing have to be exactly equal.

6.4 Data- collection and visualization

Finally, all results coming out of the pipeline should be collected and visualized. Since the pipeline has support for
plugging in any arbitrary ‘Callback’ (Section 6.2.6), primitives are in place for sending the data to a data- collection and
visualization back-end. Any such callback can implement functions for processing the pipeline conﬁguration, individual
metric objects, and result tables. Because the pipeline is built to be executed at scale, a clear distinction is made between
the pipeline execution phase and the data collection phase: the two are separated by means of the Callback interface.

One callback implementation is the ‘Wandb’ callback, providing integration with the Weights and Biases platform
[Biewald, 2020]. Weights and Biases is a platform for real-time experiment tracking and visualization, with built-in
primitives in Python. The platform allows you to upload tables, metrics, or conﬁguration objects, and subsequently
visualize them in the front-end interface. The platform also allows one to upload- and download raw ﬁles, allowing a
user to integrate remote caching functionality (Section 6.2.5). A screenshot of the platform front-end can be seen in
Figure 9.

An important feature on the Weights and Biases platform is the ability to collect- and aggregate data interactively in
the front-end. This is done by means of ‘Vega-Lite’ [Satyanarayan et al., 2017], a visualization grammar for creating
interactive visualizations in a front-end environment. This allows a user to send ‘raw’ data to the dashboard, and
aggregate it on the front-end itself. Any aggregation operation can be applied, be it taking averages, sums or computing
the variances. The front-end then allows you to visualize the aggregated data using a simple grammar. In this way, the
pipeline requires much less small adjustments and modiﬁcations for the user to be satisﬁed with the data aggregation
process - the data can be summarized in a later stage after all results are already in.

32

Redisqueue-1queue-2HPC systemWorkerUser PCWorkerWorkerWorkerenqueue jobsdequeue jobsPython programserializationA novel evaluation methodology

TECHNICAL REPORT

Figure 9: The Weights and Biases dashboard front-end. The platform allows tracking experiments, aggregating data,
and visualizing results.

33

A novel evaluation methodology

TECHNICAL REPORT

7 Experiments

Accompanying the proposal of a new feature- ranking and selection methodology, there is an experiment. The aim of
the experiment is to show an example of what is possible with both the pipeline, and the newly proposed evaluation
methodology. In this way, ML practitioners and authors of new methods are able to conduct experiments themselves
with the same setup and conﬁguration: therefore allowing comparison between the experiments.

7.1 Experiment setup

The experiment closely follows the outlines of Section 5 and Section 6. The setup can be summarized like so:

• The dataset at hand is split into a 80% training- and 20% testing dataset. Whereas the training set is reserved

for ﬁtting the feature ranker and validator, the testing set is reserved for scoring the validator.

• For each Feature Ranker and dataset, B = 25 bootstrap resampling iterations are run.
• In each bootstrap resampling iteration, min(p, 50) feature subsets are evaluated, with each including the k

best features.

• Validation estimators are evaluated with the R2 score in case of regression and with accuracy in the case of

classiﬁcation. The validation estimators used are k-NN with k = 5 and a DT at default sklearn settings.

• In the aggregation process, in case several experimental runs are found with the same conﬁguration, i.e., for
one Feature Ranker executed on one dataset, only the ‘best’ run is considered. The best run is considered to be
the run with the highest average mean score over all features, considering all bootstraps. This aggregation step
will be elaborated upon in Section 7.2.

All experiments were run on a SLURM HPC environment. Speciﬁcally, the University of Groningen has a ‘Peregrine’
compute cluster, with machines of various types, of which the most common one is a 24 core machine powered by two
Intel Xeon E5 2680v3 CPUs running at 2.5 GHz. Per node, 128 GB memory and 1 TB internal disk space is available,
but 10 GB was requested per CPU instead. This accounts for a total of 100 GB memory for 10 CPU processes. In total,
the experimentation facilitating these results took 7682 hours of processing time on the above-mentioned machines.

A line-up of feature rankers and datasets was constructed to conduct benchmarking on. The full list of both is available
in the Appendix Section A. See Table 3 for the Feature Ranking line-up, and Table 4 for the datasets line-up. The
distribution of classes can be found in Appendix Figure 24. Most datasets are balanced, though also some imbalanced
ones are included. Imbalanced datasets are not treated specially, it is up to the ranking- and validation estimators to
correct for this. It can also be seen that rankers of various types are included: both regressors and classiﬁers, and
rankers that support both learning tasks. Furthermore, some rankers were included that support multioutput datasets,
i.e., datasets with multivariate targets.

7.2 Experimental results for the ‘Synclf hard’ dataset

To best understand the format of the experimental results and the accompanying metrics, a look is taken at the results
for one dataset. In this way, a better understanding of the charts is gained ﬁrst.

First of all, the considered dataset is the ‘Synclf hard’ dataset. It is a synthetically generated dataset, created using the
sklearn make_classification function (Section 6.2.1). Its full conﬁguration speciﬁcation is deﬁned in Listing 3.
The dataset is deﬁned to have n = 10000 samples and p = 50 dimensions. After the CV step was conducted, 8000
samples are left for training. The dataset has 4 relevant features and 3 target classes, which are perfectly balanced. That
said, observations are ﬁrst made on running one feature ranker on the dataset. For this, ReliefF [Kononenko, 1994] is
chosen.

7.2.1 ReliefF performance

To start, a look is taken at a plot that explicitly plots all bootstraps. See Figure 10.

As can be seen in Figure 10, the various bootstraps have had an effect on the validation performance per subset. Due
to the random resampling with replacement taking place in the bootstrap phase, the feature ranker has to deal with
permutations of the dataset each run. Indeed, this randomness is reﬂected in the validation performance.

One clear pattern is the fact that the validation performance ﬁrst goes up, peaks at 4 features, and goes gradually down
again. The fact that the peak happens at 4 features is clariﬁed by the fact that the dataset had 4 informative features
deﬁned, meaning that the feature ranker correctly ranked the four informative features in its top-4 in most bootstraps.

34

A novel evaluation methodology

TECHNICAL REPORT

Figure 10: Validation performance for ReliefF on the ‘Synclf hard’ dataset, for all 25 bootstraps.

An intuition for the performance degradation is the fact that adding noisy dimensions can actually cause the estimator
performance to degrade.

Next, a better look is taken at the estimated feature importances. Since the validation performance suggests that the
ranker correctly identiﬁes the importance of the informative feature, it is expected that this is reﬂected in the feature
importance estimates. Indeed, this is the case. See Figure 11.

Figure 11: Estimated feature importances for ReliefF on ‘Synclf hard’.

It can be seen in Figure 11 that the ﬁrst four features were assigned a larger importance than the others. Indeed, these
are the ground-truth relevant features, as is known apriori because the dataset was synthetically generated. Besides the
‘estimated’ feature importances, also the Ground-Truth (GT) feature importance is visible. At all times, the GT gets the
same treatment as the estimated feature importances: it is also normalized to a probability vector. In this way, the two
vectors can be compared fairly.
Using the information shown in Figure 11, the R2 score and Log loss can now be computed between the estimated- and
GT feature importances. The computation is elaborated upon in Section 5.3. The scores are shown in Table 2.

Metric
R2 score
Log loss

Stdev

Mean
0.6962 ±0.0151
0.1749 ±0.002517

Table 2: The R2- and log loss metrics for ReliefF on ‘Synclf hard’. Scores were aggregated over 25 bootstraps, see
Figure 11. Scores were computed like explained in Section 5.3.

35

6810121416182022242628303234363840424446485042n features to select0.00.10.20.30.40.50.60.70.8accuracy12345678910111213141516171819202122232425boot-strapDecision Tree classiﬁcation accuracy vs. Subset sizeUsing ReliefF on `Synclf hard`dataset (n=10000, p=50)02468101214161820222426283032343638404244464850feature index0.000.050.100.150.200.25feature importanceestimatedground_truthfeatureimportance25totalbootstrapsFeature importance vs. Feature index→ Using ReliefF on `Synclf hard`dataset (n=10000, p=50)A novel evaluation methodology

TECHNICAL REPORT

The metrics in Table 2 are to be interpreted as follows. In the case of the R2 score, a higher score means a better result,
i.e., means the ranker estimated the feature importances closer to the ground truth. The log loss, on the other hand, is
desired to have a low value; meaning the difference between the estimated- and the GT importances were smaller. Next,
lets see how to quantify the stability of the run.

To quantify the stability of estimated feature importance scores, the standard deviations of the feature importances are
taken. First, the stdev score is computed for each feature over the B bootstraps, and then the mean is taken over over
the stdev scores. This is like it was explained in Section 5.4.1. The score can be plotted like it can be seen in Figure 12.

Figure 12: Feature importance scores with stdev error bars and the total mean score visible. A higher stdev score means
less stability. Shown for ReliefF on the ‘Synclf hard’ dataset.

It can be seen in Figure 12, that the instabilities take place mainly in the relevant features. This means the algorithm is
conﬁdent on which features not to choose, but it is not sure about the weighting of the relevant ones. The mean stdev
over all features is illustrated as text on the plot. Thanks to the red error bar lines, it can be quickly observed where the
algorithm is instable.

7.2.2 Performance of multiple rankers

Next, multiple rankers are considered. Again, the dataset at hand is the ‘Synclf hard’ dataset. The validation performance
of all rankers can now be compared in a single plot, see Figure 13.

Figure 13: DT validation performance of several rankers on the ‘Synclf hard’ dataset.

In this case (Figure 13), the validation curve is aggregated over the bootstraps. That is, for each point on the curve, the
average score is taken over the B bootstraps. Looking at the plot, indeed, the various rankers show similar behavior like
before in the validation performance. For most rankers, performance goes up, peaks at 4 features, and goes down again.
Some rankers, however, do not manage to pick out the most useful features right away, and see their performance peak
a bit later. It can also be the case that some rankers selected the most relevant features earlier in their feature subsets,
causing higher values at the start of the curve.

36

feature indexfeature importance0.0027625totalbootstrapsFeature importance & Stability→ A smaller stdev means more stability02468101214161820222426283032343638404244464850n features to select0.00.20.40.60.8accuracyFeatBoostInﬁnite SelectionMultiSURFReliefFStability SelectiorankerAccuracymetricDecision Tree classiﬁcation accuracy vs. Subset sizeVarious rankers on `Synclf hard`dataset (n=10000, p=50)A novel evaluation methodology

TECHNICAL REPORT

Another way to plot the validation curves is to put the curves side by side, instead of stacked upon each other. This
allows illustrating an important aggregation metric: the mean validation score. As could be seen before, the validation
curves are already assumed to be aggregated over the bootstraps. This means, the results for each ranker are now
reduced to min(p, 50) values. To further reduce this value and be able to summarize the performance with a single
scalar, the mean of the aggregated validation curve is taken. See Figure 14.

Figure 14: Aggregated DT validation curves with their mean validation score shown. The mean score is the average
curve value, allowing summarizing the curve with a single value. Curves are the same as in Figure 13.

Looking at the curves (Figure 14), it can be seen that the mean validation score scores higher where the curves scored a
higher total score over all feature subsets. In this regard, the mean validation score could also be expressed as the sum
of scores, or even the ‘area under the validation curve’. Because the tick steps on the x-axis are in this case always
one, however, it is chosen to take the mean score instead. The detail lost by computing the interpolated curve pieces
between the curve points is negligible. Taking the mean validation score is also a much easier metric to compute than
the AUC, which in this case would require lots of interpolation. The mean validation score can be seen to represent the
performance of the feature ranker reasonably well. When the ranker scores the relevant features as highly informative,
the validation score appropriately reﬂects this. Feature rankers that have a low area under the curve are also seen to
have a lower score.

Next, the previously plotted mean validation scores are presented in yet another format. This time, it is desired to
emphasize the relative performance of a ranker, compared to the others. This can be done by using adaptive cell
background colors, see Figure 15.

Figure 15: Mean DT validation scores plotted with emphasized relative performance. The relative performance is a
ranker’s mean validation score as a fraction of the highest achieved score by any ranker for some dataset. Same curves
as in Figure 13 and Figure 14.

This time (Figure 15), the summarized performance of each feature ranker is visible in an instant. The coloring of the
chart adjusts according to the relative performance of the ranker, with respect to the best performing ranker. This is
computed simply by taking the ranker’s mean validation score as a fraction of the highest score achieved by any ranker.
This means that a relative performance of 1.00 resembles the highest performing ranker, and a relative performance
of 0.50 would mean a ranker gets only half of the highest performing ranker’s score. An accompanying color scale
illustrates this fact, where blue resembles a higher score and red a lower score. Furthermore, the differences are
further exacerbated by use of an exponential color scale. In this case, an exponent of x5 is chosen, to be able to better
differentiate between the top performers. The best score also gets its font weight displayed as bold, such to emphasize
the best performing ranker. It is thereby important to note that in case multiple datasets are considered, each dataset
gets its own relative performance scale. This is because the scores across multiple datasets ought not to be compared
directly to each other.
Again the R2- and log loss scores are investigated, which are computed using the ground-truth relevant features. The
hypothesis is that the scores are able to ‘predict’ the validation estimator’s performance. This is useful because, by

37

datasetrankerSynclf hardFeatBoostInﬁniteSelectionMultiSURFReliefFStabilitySelection0.6350.6380.6390.6400.6360.6360.6390.6380.6350.6400.9911.000relativeperformancecolor scale:(x⁵)Mean ranker validation scoresFeatBoostInﬁnite SelectionMultiSURFReliefFStability SelectionrankerSynclfharddatasetA novel evaluation methodology

TECHNICAL REPORT

evaluating the quality of the feature ranking directly, one could already make meaningful conclusions, just by having
run the ranker alone. A comparison chart for these metrics is plotted in Figure 16.

(a) The average R2 score over all bootstraps. A higher score is
better. Computed like explained in Section 4.2.1.

(b) The average log-loss score over all bootstraps. Lower is better.
Computed like explained in Section 4.2.1.

Figure 16: Log loss and R2 scores, computed by comparing the estimated feature importance scores against the feature
importance ground-truth. Such a ground-truth is available when datasets are synthetically generated. Scores shown for
‘Synclf hard’ dataset on multiple rankers.

For Figure 16 it is, ﬁrst of all, important to note that the R2- and log loss scores were computed as an average score over
all bootstraps. This means that after running a single ranker on a single dataset, B such scores are obtained - which are
then aggregated into a single score by taking its mean. In the ﬁgure it can be seen, that in the case of the R2 score the
scorings represent the scores presented in Figure 14 and Figure 15 very well. The order of best performance w.r.t. mean
validation score was predicted by the R2 score: wherever the R2 score is higher, the mean validation performance is
also higher. The same applies to the log loss scores: more loss means worse mean validation performance. Although in
this example the differences are very small, still a measure of proportionality between the scores exists, for this dataset.

To assess the stability of the rankers, the feature importances of the various rankers can be put side-by-side. Then, the
standard deviations of each feature estimated over B bootstraps can be shown. This can be seen illustrated in Figure 17.

Figure 17: The estimated feature importances and their stabilities, quantiﬁed using the mean standard deviation over all
feature importances. Plots shown for ‘Synclf hard’ dataset.

As can be seen in Figure 17, the stability of the ranking algorithms varies. As illustrated by the red standard deviation
error bars, the algorithms have varying amounts of deviation in their feature importance estimations. Whereas Inﬁnite
Selection is very stable, and shows hardly any variance over the bootstraps at all, Stability Selection is very unstable,
despite its name. MultiSURF can be seen to be varying mainly in the relevant features, similarly to ReliefF. Looking at
the summarized stability value, indeed the expected instabilities are captured in the scalar. Stability Selection has the
highest mean standard deviation and Inﬁnite Selection has the lowest. This indicates the metric might be successful in
capturing the instability of the algorithms.

Now that all relevant plots are clariﬁed and understood, a broader view can be taken. In the next section, the experiment
in its entirety will be considered.

38

0.00.10.20.30.40.50.60.7R2 scoreFeatBoostInﬁnite SelectionMultiSURFReliefFStability SelectionrankerFeature importances R2 score w.r.t. GT`Synclf hard` dataset (n=10000, p=50)0.000.100.200.050.150.25Log lossFeatBoostInﬁnite SelectionMultiSURFReliefFStability SelectiorankerFeature importance Log loss w.r.t. GT`Synclf hard` dataset (n=10000, p=50)A novel evaluation methodology

TECHNICAL REPORT

7.3 Experimental result for all datasets

Now, a look is taken at the results of all datasets. See Figure 18, which is explained below.

Figure 18: Mean classiﬁcation accuracy validation scores for all datasets and all rankers. Color scale is conﬁgured
with relative performance, like explained in Section 7.2. Validation scores shown are for classiﬁcation datasets using a
DT as a validator.

To illustrate the performance of the complete experiment, different ways of visualization are required. With this many
combinations of feature rankers and datasets, no longer can the line curves showing the mean validation scores be used
to explain the ﬁndings. Instead, it is chosen to display this data in a heatmap, exactly similar to Figure 15, but with
more data. Various aspects of performance are discussed, starting with the mean validation performance.

In Figure 18 it can be seen that the table coloring reveals much about the performance of the feature ranker. The more
the coloring is toward dark blue, the better the ranker performed. On the other hand, red indicates bad performance: the
worst case in this plot is Boruta, which only achieves about half the score of the best performer for the ‘Multifeat Pixel’
dataset. Boruta’s bad performance is almost certainly due to a conﬁguration anomaly, since the performance is worse
than random in some cases. A notable performer is the DT, used as a Feature Ranker by making use of its computed
feature importance scores. The algorithm scores high on nearly every dataset, making it a good contender for the best
performer. It might seem odd, however, that a DT is run for Feature Selection ﬁrst, and then also for validation. For this
reason, the experiment was also run with another validation estimator, k-NN. For this, see Figure 26 in the Appendix.

Next is the algorithm stability. The algorithm stability scores were computed like illustrated in the previous section,
but now for more datasets. The scores were represented in a table and have a custom color scheme applied to it to better
emphasize the differences. See Figure 19.

Like can be seen in Figure 19, the stability scores differ per ranker. Especially TabNet, XGBoost and Decision Tree can
be seen to be relatively unstable. Inﬁnite Selection, Mutual Info and ReliefF, on the other hand, have a better stability.
Next, a look is taken at the computed R2 scores. The scores were computed using the apriori known relevant features,
i.e., the datasets GT feature importances. A comparison heatmap plot can be seen in Figure 20.
As could be seen in Figure 20, the differences in R2 scores are notable, and correlate slightly with the mean validation
scores for the designated datasets. Both DT and ReliefF have high R2 scores for the chosen datasets. FeatBoost,
however, had a high overall mean validation score but cannot be seen to have a high R2 score. This might indicate that
the scoring does not always foretell validation performance.

Especially in the case of FeatBoost and Inﬁnite Selection, there exist big discrepancies between the mean validation
score and the computed R2 scores. Whereas for both rankers the mean validation scores are high for the ‘Synclf’
datasets, the R2 scores did not follow. A reason for this might be the fact that these rankers do often rank features as
having an importance of above zero. Whereas an algorithm might still get the feature importance order correct, the
algorithm does a worse job in the R2 scoring due to always giving features scores of well above zero. Other rankers,

39

0.9270.9640.8590.4910.8050.6320.8550.7800.9120.9340.7410.5410.9540.9640.8960.5180.5170.5190.4850.5180.5160.5160.4960.5180.4980.6390.6360.6240.6390.6380.6350.6370.6400.6370.6400.8180.7720.8210.7280.8210.8180.8160.6600.8170.6550.8570.8490.7790.8570.8560.8330.8560.8190.8450.8020.9340.9290.7740.7750.7820.7740.7790.7750.7890.7830.7790.7840.9160.9150.9110.9100.9070.9130.9140.9110.9000.9100.9150.9290.9300.9390.9310.7770.9300.7150.5410.7020.7050.6880.7490.6940.7280.4970.6840.9390.8690.6480.9540.9390.9060.7550.6490.9720.8660.9140.9670.9390.9330.9710.9690.8580.6230.9670.9690.8390.9380.5150.9540.9390.9150.8670.9680.9750.9150.8640.7970.3680.8130.8640.8690.9330.8660.8620.8630.8550.7540.6480.6050.6490.9090.9100.9090.9100.9140.9700.9700.9720.9650.9590.9650.9690.9690.9650.9590.5200.6400.8210.8580.7900.9170.9390.7520.8710.7870.9170.9720.9760.9760.50.70.9relativeperformancecolorscale:(x⁵)Feature ranker performance - classiﬁcation→ Mean accuracy validation score over all bootstrapsANOVA F-valueBorutaChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectionTabNetXGBoostrankerClimate Model SimulationCylinder bandsIris FlowersMadelonMultifeat PixelNomaoOzone LevelsPhonemeSynclf easySynclf hardSynclf mediumSynclf very hardTextureWall Robot NavigationdatasetA novel evaluation methodology

TECHNICAL REPORT

Figure 19: Algorithm stability scores. Obtained by computing the standard deviation over the feature importance
vectors, like illustrated in Section 7.2 and explained in Section 5.4.1. A custom color scheme was chosen to better
emphasize the differences. A darker shade of red means worse stability. A darker shade of green means better stability.
The empty cells are due to an experiment failure.

Figure 20: Algorithm R2 scores, computed using the GT feature importanes. The computation was explained in
Section 5.3 and illustrated in Section 7.2. A darker shade of green means a better score.

that assign more importance to some presumably relevant features but less to the others, will be better off with the R2
scoring. The metric might therefore pose an unfair advantage to how rankers behave in this regard.

In a similar fashion, the log loss scores are plotted. See Figure 21.

In the case of the log loss metric, a different pattern is visible (Figure 21). Indeed, well performing rankers are DT and
ReliefF. The scoring did miss FeatBoost and Inﬁnite Selection, however, as being overall good performers. Whereas
according to the mean validation scores FeatBoost and Inﬁnite Selection are among the top performers, this is not
reﬂected in the log loss score. This might, again, be due to an unfair advantage given to rankers that tend to score
irrelevant features closer to zero. This is similar to the R2 score, though arguably the effect is exaggerated in the case of
the R2 score. The metrics might therefore not be ideal for direct algorithm comparison: it is best the mean validation
scores are included at all times. The metrics can, however, reveal interesting information in speciﬁc scenarios. This
might be the case, for example, when no feature selection is applied at all in the evaluation process, and all one has is
the feature importance ground truth and estimations.

Additionally, some plots can be seen in the Appendix. A plot showing the validation scores using k-NN as the validation
estimator can be found in the Appendix Figure 26. To see the R2 scores and Log loss scores for the regression datasets,
see Figure 28 and Figure 29, respectively. The regression dataset mean validation scores can be seen in Figure 27.

40

0.0010.0280.0270.0150.0280.0130.0270.0440.0280.0200.0090.0080.1210.0280.0490.0010.0110.0030.0010.0040.0010.0040.0040.0030.0020.0130.0030.0010.0040.0050.0030.0030.0020.0030.0170.0050.0010.0090.0060.0050.0050.0060.0030.0150.0030.0020.0110.0010.0210.0040.0010.0010.0000.0140.0230.0090.0290.0300.0130.0190.0250.0200.0100.0020.0010.0020.0150.0020.0180.0020.0020.0040.0000.0000.0060.0000.0020.0010.0010.0010.0020.0000.0010.0030.0020.0050.0100.0010.0000.0120.0110.0230.1090.0070.0010.0430.0040.0010.0000.0140.0040.0020.0070.0010.0070.0240.0020.0510.0020.0010.0100.0010.0020.0010.0040.0000.0000.0060.0000.0290.0100.0180.3420.0430.0230.0010.0150.0010.0120.0040.0010.0020.0040.0120.1630.0270.0010.0020.0000.0000.0010.0000.0080.0010.0000.0000.0000.0000.0000.0020.0070.0000.20.61.0relativeperformancecolorscale:(x^0.2)Algorithm Stability→ Mean stdev of feature importances. Lower is better.ANOVA F-valueChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectionTabNetXGBoostrankerClimate Model SimulationCylinder bandsIris FlowersMadelonMultifeat PixelNomaoOzone LevelsPhonemeSynclf easySynclf hardSynclf mediumSynclf very hardTextureWall Robot Navigationdataset−0.097−0.0880.8490.3370.3310.1070.5950.712−0.678−0.7450.2760.3050.0110.8190.1670.6230.4100.4990.2240.3280.0290.6960.5730.4090.4480.4170.1650.3640.0160.5820.4220.2970.9540.8300.7770.5840.110.370.640.89relativeperformancecolor scale:(quantiles)R2 score→ Computed using GT feature importances. Higher is better.ANOVA F-valueChi-SquaredDecision TreeFeatBoostInfinite SelectionMutual InfoReliefFStability SelectionXGBoostrankerSynclf easySynclf hardSynclf mediumSynclf very harddatasetA novel evaluation methodology

TECHNICAL REPORT

Figure 21: Algorithm Log loss scores, computed using the GT feature importanes. The computation was explained in
Section 5.3 and illustrated in Section 7.2. A darker shade of blue means a better score.

7.4 Learning curves and time complexity

To ﬁrst get an idea of the algorithm time complexity, an overview plot is taken into consideration. In this plot, multiple
datasets and multiple rankers are considered. For each benchmark, the ﬁtting time in seconds is plotted. See Figure 22.

Figure 22: Mean ﬁtting times from running the experiment B = 25 times. Fitting times are represented in seconds. A
custom color scheme and scaling factor was chosen to emphasize the differences in ﬁtting times.

In Figure 22 it can ﬁrst of all be seen that the algorithms differ immensely in ﬁtting times. TabNet is (by far) the most
complex and time-consuming estimator to ﬁt. This is much due to the fact that it ﬁts a sophisticated Neural Network
and uses PyTorch to do so. The two algorithms that are next in time complexity are MultiSURF and FeatBoost. Some
statistical estimators can be seen to have very low ﬁtting times.

To investigate algorithm performance under varying conditions of sample size, a separate experiment was run. In this
experiment, only the ‘Synclf hard’ dataset is considered. Then, the sample size for this dataset was varied over a number
of intervals, set to a ﬁxed number each time. The sample size was set to range from 100 to 2,100 with intervals of 200.
Note, that the total sample size for this dataset is 10,000. For every sample size, 10 bootstraps were run.

In this way, two things can be investigated. (1) the time complexity and (2) the learning curve behavior of the various
algorithms. Both were plotted. First, a look is taken at the time complexity. To investigate the time complexity of the
algorithms, the ﬁtting time was recorded. The ﬁtting time was measured to entail only exactly the ranker ﬁtting step - no
supplementary preprocessing steps whatsoever inﬂuenced the time recording. The time was stored in a high-accuracy
ﬂoat but represented as seconds. See Figure 23.

As can be seen in Figure 23, most algorithms scale about slightly worse than linear when increasing the number of
samples. It can be seen that when 1,000 samples are used versus 2,000, the ﬁtting time is about double. The only
outliers are XGBoost and TabNet, which seem to need a bit longer than its competitors when the sample size exceeds
900.

41

0.4770.4110.1120.2510.2480.3100.1220.1570.4700.4720.1550.2840.2780.3660.3050.2090.1440.1640.2620.2390.3190.1750.1880.2240.2610.2610.3520.3020.4020.2600.2790.3170.0900.1410.1390.2591.00.2relativeperformancecolor scale:(linear)Log loss→ Computed using GT feature importances. Lower is better.ANOVA F-valueChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMutual InfoReliefFStability SelectionXGBoostrankerSynclf easySynclf hardSynclf mediumSynclf very harddataset844.53424.5390.0300.192523.6531.2870.915114.208236.411161.17336.5840.0520.290930.5601.9911237.0341.360146.814202.936181.67863.6590.0620.5111725.4965.1582700.7192.362465.192805.083587.63862.5040.0480.5042119.1644.1232696.3082.342444.9721013.507573.02413832.86210638.61413719.75110443.7695451.5983585.1453556.2395346.001860.1710.55717.24523.0850.15079.0330.113113.2010.0280.0052.5121.003105.66162.5591.024205.2330.270935.3030.1050.00688.6147.841202.437430.2477.0601329.8994.0699339.5930.765895.1420.0200.4670.0143961.8811424.6030.0030.013372.69512.941125.15767.24710.2337.0203.816559.1100.0660.1032.5240.0952.2580.0060.0040.0050.0060.0070.0030.0030.0200.0050.0011.00.0relativeperformancecolor scale:(x⁰⋅¹)Fitting time (seconds)→ As mean over all bootstrapsANOVA F-valueBorutaChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectionTabNetXGBoostrankerClimate Model SimulationMadelonMultifeat PixelOzone LevelsPhonemeSynclf easySynclf hardSynclf mediumSynclf very harddatasetA novel evaluation methodology

TECHNICAL REPORT

Figure 23: Time complexity plot for ‘Synclf hard’ dataset. Sample sizes from 100 to 2,100 were used, with intervals of
200. The plot shows the average ﬁtting time over 10 bootstraps.

The learning curve was omitted from the main text and can be seen in the Appendix. See Figure 25. It can be seen
that the algorithms learn very similarly given an increasing number of available samples. Only stability selection and
mutual info are having a slightly harder time learning the useful features with little data.

7.5 Online dashboard

In case the reader is interested in exploring the data interactively him- or herself, the plots above can also be accessed
on the online dashboard3. The dashboard lets the user see the data in more detail by the use of zooming and tooltips.
Moreover, the visualizations contain hyperlinks, taking the user directly to a detailed page of a single benchmark run.

3https://wandb.ai/dunnkers/fseval/reports/Final-results-MSc-Thesis–Vmlldzo3ODEyODE

42

02004006008001,0001,2001,4001,6001,8002,000sample size0.000.010.020.030.040.050.060.070.080.09ﬁtting timeANOVA F-valueChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectioTabNetXGBoostrankerTime complexity`Synclf hard` dataset w/ Decision Tree validationA novel evaluation methodology

TECHNICAL REPORT

8 Discussion

In the following section, a brief discussion is held on the project result. Instead of interpreting the results directly, a
broad view will be taken on the achieved results. Because the contribution of the paper consists of multiple elements,
each is discussed separately.

The newly proposed evaluation methodology consists out of two main aspects. On the one hand, new evaluation metrics
were created and proposed, making use of apriori knowledge on relevant features. On the other hand, recommendations
were made for the inclusion of existing metrics in the evaluation process of feature- rankers and selectors. Like it was
discussed in the related work- and motivation sections, there is an interest in creating better evaluation metrics. There
indeed exists a gap in the literature which is to be ﬁlled: a discussion on the usefulness of new evaluation metrics w.r.t.
the feature- ranking and selection benchmarking process. Seemingly, the new metrics are of some use, and can be used
to foretell the validation performance. It must be said, however, that the new evaluation metrics must probably always
be accompanied by real-world data analysis with the use of validation estimators, in order to conﬁrm the found results
using the apriori knowledge.

An important caveat in the newly proposed evaluation metrics is the assumption of uniform feature importance. In the
synthetically generated classiﬁcation datasets, grouped as the ‘Synclf’ datasets, the informative features are known
apriori, thanks to the make_classification function in sklearn. It is assumed, however, that all informative features
do weight equally, i.e., have a uniform distribution w.r.t. feature importance. All rankers are scored accordingly to
this uniformly distributed feature importance scores. This assumption might unjustly rank feature rankers as being
less performant. This happens, for example, when a ranker assigned many small scores to irrelevant features and thus
also a smaller score to the relevant features - but still getting the feature ranking (order) correct. When combining the
apriori scores with the validation scores, however, a clearer picture be obtained. Nonetheless, for the reason stated
above, caution should be taken into interpreting the R2- and log loss scores for the synthetically generated classiﬁcation
datasets. What is certainly still useful to practitioners, however, are the charts obtained by plotting the estimated feature
importances against the ground-truth feature importances. Such charts can help in qualitative interpretation of the
algorithm’s performance. Also, the mean validation scores have been shown to be of good use in the evaluation process
- which is also a newly proposed metric.

In the case of the synthetically generated regression datasets, this caveat does not hold. This is because in the case of
synthetically generated regression datasets, the actual ground-truth desired feature importance coefﬁcients are known.
This is due to the make_regression function. This is a big plus for the ‘Synreg’ datasets, giving them an extra amount
of trustworthiness.

Next, the created pipeline is discussed. Arguably, this is the biggest contribution of the project. The initial goals before
building the pipeline were several. The pipeline had to be ﬂexible, to support many different feature rankers, datasets,
and validation estimators. It was also desired to have the data- collection and visualization steps largely automatized,
such that new experiments could be added easily. And all this was desired to be scalable, allowing the pipeline to be run
on many processors and on many machines simultaneously. In the pipeline implementation that was built, all above
features were addressed. When the pipeline is run, a practitioner has to complete only little steps to visualize and
interpret the data, due to integration with the online dashboard. The pipeline is open-source and released as a PyPi
package, such that a practitioner can also easily extend the pipeline to their own needs.

A noteworthy remark about the pipeline is to be made, however. The pipeline might have tried to tackle too many
problems in a single codebase. Although the pipeline is, in the end very ﬂexible, lots of code exists to support both
regression- and classiﬁcation datasets. This does make the pipeline more useful for a wider audience, but also forces the
programmer to give in to simplicity. Evaluation metrics have to be implemented twice and validation estimators have to
account for both learning tasks. All result data also has to be kept apart such not to intermingle the two. An upside of
having included both learning tasks, however, is that it is now relatively easy to add support for another learning task.
Since the codebase already deals with multiple learning tasks, the difference between choosing between two and more
is not big. Learning tasks such as clustering could be supported in this way.

Lastly, the experiment is discussed. The experiment is to be an implementation of the proposed methodology, and
an application of the built pipeline. Although at ﬁrst outset, the goal in this paper was to benchmark as many as
possible feature rankers, at a speciﬁc point in time the scope was reduced to contain a more ﬁnite set of feature rankers.
This is mainly due to the fact that even though the literature on feature- rankers and selectors is very extensive, the
implementations are scarce. Only in rare cases, an up-to-date software package for the designated feature- ranker or
selector exists. In most cases, software packages are out dated and work with older versions of sklearn. In other cases,
only an R implementation can be found.

43

A novel evaluation methodology

TECHNICAL REPORT

Nonetheless, the experiment does allow a practitioner to draw useful conclusions. Still, the experiment is more extensive
than what is seen in many algorithm proposals in the literature. Many datasets are included, both of the real-world and
synthetic type. But most importantly, using the newly proposed evaluation methodology and pipeline implementation
that is freely available, new experiments can easily be added. Even, direct comparison is possible when data is uploaded
to the same dashboard - allowing this experiment to be a starting place for further experimentation. In other words, with
a proof-of-concept experiment in place, the ﬁeld is open for more comprehensive experiments.

44

A novel evaluation methodology

TECHNICAL REPORT

9 Conclusion

First, a concluding note will be given. Lastly, possible points of improvement for future work will be elaborated upon.

9.1 Concluding note

In this paper, it was investigated how best to evaluate feature- rankers and selectors. A selection of literature was
considered, and their evaluation methods were compared. Traditionally, papers paid little attention to using apriori
knowledge nor performing stability analysis. Given this lack, one can speak of a gap in the literature - there is an
interesting opportunity for investigating new methods. After this opportunity was identiﬁed, new evaluation methods
were proposed. In this way, a new methodology for evaluating feature- rankers and selectors was compiled in this
paper. The main opportunities lie in the use of apriori knowledge, performing stability analysis, and summarizing the
validation performance in a novel way using a scalar metric.

An extensive benchmarking pipeline was constructed, implementing this new evaluation methodology. The pipeline
is scalable, modular, and easy to conﬁgure. The pipeline is available as open-source free software, called ‘fseval’. A
comprehensive experiment was also run using the pipeline, illustrating the pipeline capability’s in a concrete benchmark.
Using an online dashboard, the benchmark can easily be extended in the future. The results can also be interactively
explored. Strong overall performers for classiﬁcation datasets are Decision Tree, XGBoost, FeatBoost, and Inﬁnite
Selection.

Feature ranking is an ever more relevant problem in a world where data and machine learning pose a prevalent role,
useful for both feature selection and interpretable AI. With many feature ranking algorithms available, a comprehensive
analysis is required to pick a suitable method given the context. But, when all relevant facets are highlighted and
analyzed in a comprehensive feature ranking evaluation pipeline, authors and users can be more deliberate in arguing
for any algorithm’s superiority.

9.2 Limitations and Future work

In the future, authors might build upon the work done in this paper. The following outlines the limitations of this paper
and ideas for extending the work done in this project.

Limitations of the paper were the following.

• The proposed methodology, including new evaluation metrics, could have enjoyed more mathematical argu-
mentative support. Currently, the evaluation metrics are outlined mathematically, but no theoretical predictions
about their behavior are made. Instead, their usefulness is proven only in the experiment section, through
empirical observation. Quite possibly, the usefulness of certain evaluation metrics could have been foretold
mathematically.

• Another important caveat is the fact that the considered feature rankers were not hyper-parameter optimized
for the experiment datasets. All feature rankers were run at their default settings. Fair to say, this completely
invalidates some rankers. Since some rankers do not function properly without hyper-parameter optimization,
their performance can be misleadingly bad. In this experiment, this is the case with Boruta, for example. One
could also argue, however, that the algorithms should be able to function well at their defaults. A practitioner
does not always have the luxury to perform a computationally expensive hyper-parameter optimization process.
• The feature support and feature rankings were less extensively evaluated than feature importance estimations.
For example, the feature support estimations could have also had a more sophisticated metric devised for its
evaluation. Currently, only the mean validation scores of the feature subsets are taken into account, but ideally,
also the amount of selected features would have been taken into account in the metric.

• Lastly, the experiment could have been more extensive. Currently, only three datasets are multioutput, meaning
the rankers supporting multioutput get little comparison material. All these datasets were also of regression
type. Given enough processing power, ideally the entire OpenML-CC18 benchmark suite would be run.
Moreover, more synthetic datasets would be tested using a range of parameter settings.

Ideas for future work are several.

• The pipeline can be extended in several straight-forward ways. (1) Firstly, more dataset adapters could be
added. An interesting platform to support is Kaggle, although adapters for loading local- or remote CSV or
JSON would also be useful. (2) Secondly, the pipeline could integrate with more back-ends aside from WandB.
OpenML can also be used, for example, to upload metrics and experimental results to. Alternatively, data

45

A novel evaluation methodology

TECHNICAL REPORT

could be uploaded directly to a database of some kind, like MySQL. In this way, by adding more integrations
to the existing benchmarking pipeline, the framework could become a truly versatile benchmarking tool for
any feature ranking algorithm. This also the last pipeline improvement idea. (3) Lastly, one might even extract
the general pipeline and benchmarking capabilities of the framework and use them for general-purpose ML
benchmarking. This would turn the framework from being just a feature ranking evaluation framework into a
generic framework for testing ML algorithms.

• The evaluation process with regards to the feature importances ground-truth can enjoy more research. One
idea is to normalize the feature importances vectors w and ˆw by a softmax operation, instead of normalizing
only by the sum of the vector. Using the current normalization method, negative values are not allowed in
the feature importance vector. Although this is rare, it might occur. Furthermore, authors might also want to
try to weight the apriori scorings such as the R2 and log loss scores. A weighting could make sure, then, that
the relevant features are assigned more weight in the scoring process, i.e., algorithms are rewarded more for
getting those features right rather than the irrelevant features.

• Another idea is to apply the new evaluation methodology to interpretable AI algorithms in a more extensive
way. Currently, only one method related to interpretable AI was included, TabNet. No special considerations
were made for such rankers, but many interpretable AI methods support ranking features on a per-instance basis.
Instance-based feature importance scoring is not considered in this paper’s context. Because, however, the
goal of ranking feature importances is similar in the domain of interpretable AI, the benchmarking framework
can be made compatible relatively easily with instance-based methods.

46

A novel evaluation methodology

TECHNICAL REPORT

References

Seref Sagiroglu and Duygu Sinanc. Big data: A review. In 2013 International Conference on Collaboration Technologies

and Systems (CTS), pages 42–47, May 2013. doi:10.1109/CTS.2013.6567202.

Thomas N. Theis and H.-S. Philip Wong. The End of Moore’s Law: A New Beginning for Information Technology.
Computing in Science Engineering, 19(2):41–50, March 2017. ISSN 1558-366X. doi:10.1109/MCSE.2017.29.
Conference Name: Computing in Science Engineering.

Keith A. Britt and Travis S. Humble. High-Performance Computing with Quantum Processing Units. ACM
ISSN 1550-4832.

Journal on Emerging Technologies in Computing Systems, 13(3):39:1–39:13, March 2017.
doi:10.1145/3007651. URL https://doi.org/10.1145/3007651.

Isabelle Guyon and André Elisseeff. An introduction to variable and feature selection. The Journal of Machine Learning

Research, 3(null):1157–1182, March 2003. ISSN 1532-4435.

W. Duch, T. Wieczorek, J. Biesiada, and M. Blachnik. Comparison of feature ranking methods based on information
entropy. In 2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541), volume 2,
pages 1415–1419 vol.2, July 2004. doi:10.1109/IJCNN.2004.1380157. ISSN: 1098-7576.

Adarsh Ghosh and Devasenathipathy Kandasamy.

Interpretable Artiﬁcial Intelligence: Why and When. Ameri-
can Journal of Roentgenology, 214(5):1137–1138, March 2020. ISSN 0361-803X. doi:10.2214/AJR.19.22145.
URL https://www.ajronline.org/doi/full/10.2214/AJR.19.22145. Publisher: American Roentgen Ray
Society.

Mario Köppen. The Curse of Dimensionality. Encyclopedia of Measurement and Statistics, April 2009. doi:10.1007/978-

0-387-39940-9_133.

B. Hu, Yongqiang Dai, Y. Su, P. Moore, Xiaowei Zhang, Chengsheng Mao, J. Chen, and L. Xu. Feature Selec-
tion for Optimized High-Dimensional Biomedical Data Using an Improved Shufﬂed Frog Leaping Algorithm.
undeﬁned, 2018. URL /paper/Feature-Selection-for-Optimized-High-Dimensional-an-Hu-Dai/
9d495734dead7b1f982183b08e3ef070e2dd18b1.

Allison Marie Horst, Alison Presmanes Hill, and Kristen B. Gorman. palmerpenguins: Palmer Archipelago (Antarctica)
penguin data. Self-Published, 2020. doi:10.5281/zenodo.3960218. URL https://allisonhorst.github.io/
palmerpenguins/.

Arun Rai. Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48(1):
137–141, January 2020. ISSN 1552-7824. doi:10.1007/s11747-019-00710-5. URL https://doi.org/10.1007/
s11747-019-00710-5.

Cathy O’Neil. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown,

September 2016. ISBN 978-0-553-41882-8. Google-Books-ID: NgEwCwAAQBAJ.

Lars Kai Hansen and Laura Rieger. Interpretability in Intelligent Systems – A New Concept? In Wojciech Samek,
Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert Müller, editors, Explainable AI: Interpreting,
Explaining and Visualizing Deep Learning, Lecture Notes in Computer Science, pages 41–49. Springer International
Publishing, Cham, 2019. ISBN 978-3-030-28954-6. doi:10.1007/978-3-030-28954-6_3. URL https://doi.org/
10.1007/978-3-030-28954-6_3.

Scott Lundberg and Su-In Lee. A Uniﬁed Approach to Interpreting Model Predictions. arXiv:1705.07874 [cs, stat],

November 2017. URL http://arxiv.org/abs/1705.07874. arXiv: 1705.07874.

Sercan O. Arik and Tomas Pﬁster. TabNet: Attentive Interpretable Tabular Learning. arXiv:1908.07442 [cs, stat],

February 2020. URL http://arxiv.org/abs/1908.07442. arXiv: 1908.07442.

Saúl Solorio-Fernández, J Ariel Carrasco-Ochoa, and José Fco Martínez-Trinidad. A review of unsupervised feature

selection methods. Artiﬁcial Intelligence Review, 53(2):907–948, 2020. Publisher: Springer.

Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and Huan Liu. Feature
selection: A data perspective. ACM Computing Surveys (CSUR), 50(6):1–45, 2017. Publisher: ACM New York, NY,
USA.

Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers & Electrical Engineering,

40(1):16–28, 2014. Publisher: Elsevier.

Qasem Al-Tashi, Helmi Md Rais, Said Jadid Abdulkadir, Seyedali Mirjalili, and Hitham Alhussian. A Review of Grey
Wolf Optimizer-Based Feature Selection Methods for Classiﬁcation. In Evolutionary Machine Learning Techniques,
pages 273–286. Springer, 2020.

Majdi Mafarja, Ali Asghar Heidari, Hossam Faris, Seyedali Mirjalili, and Ibrahim Aljarah. Dragonﬂy algorithm: theory,
literature review, and application in feature selection. In Nature-Inspired Optimizers, pages 47–67. Springer, 2020.

47

A novel evaluation methodology

TECHNICAL REPORT

Daphne Koller and Mehran Sahami. Toward optimal feature selection. Technical report, Stanford InfoLab, 1996.

Ruba Abu Khurma, Ibrahim Aljarah, Ahmad Sharieh, and Seyedali Mirjalili. EvoloPy-FS: An Open-Source Nature-
Inspired Optimization Framework in Python for Feature Selection. In Evolutionary Machine Learning Techniques,
pages 131–173. Springer, 2020.

Hussein Almuallim and Thomas G. Dietterich. Learning With Many Irrelevant Features. In In Proceedings of the Ninth

National Conference on Artiﬁcial Intelligence, pages 547–552. AAAI Press, 1991.

K. Kira and L. Rendell. The Feature Selection Problem: Traditional Methods and a New Algorithm. In AAAI, 1992.

Igor Kononenko. Estimating attributes: Analysis and extensions of RELIEF. In Francesco Bergadano and Luc De Raedt,
editors, Machine Learning: ECML-94, Lecture Notes in Computer Science, pages 171–182, Berlin, Heidelberg,
1994. Springer. ISBN 978-3-540-48365-6. doi:10.1007/3-540-57868-4_57.

Zheng Zhao and Huan Liu.

Searching for interacting features.

the 20th interna-
tional joint conference on Artiﬁcal intelligence, IJCAI’07, pages 1156–1161, San Francisco, CA, USA,
URL https://www.semanticscholar.org/paper/
January 2007. Morgan Kaufmann Publishers Inc.
Searching-for-Interacting-Features-Zhao-Liu/d2debe138a9b67d838b11d622651383322934aee.
Quanquan Gu, Zhenhui Li, and Jiawei Han. Generalized Fisher Score for Feature Selection. arXiv:1202.3725 [cs, stat],

In Proceedings of

February 2012. URL http://arxiv.org/abs/1202.3725. arXiv: 1202.3725.

Marco Zaffalon and Marcus Hutter. Robust Feature Selection by Mutual Information Distributions. arXiv:1408.1487

[cs], August 2014. URL http://arxiv.org/abs/1408.1487. arXiv: 1408.1487.

Mohamed Bennasar, Yulia Hicks, and Rossitza Setchi.

Feature selection using Joint Mutual Informa-
ISSN 0957-
doi:10.1016/j.eswa.2015.07.007. URL http://www.sciencedirect.com/science/article/pii/

tion Maximisation. Expert Systems with Applications, 42(22):8520–8532, December 2015.
4174.
S0957417415004674.

Zilin Zeng, Hongjun Zhang, Rui Zhang, and Chengxiang Yin. A novel feature selection method considering feature
interaction. Pattern Recognition, 48(8):2656–2666, August 2015. ISSN 0031-3203. doi:10.1016/j.patcog.2015.02.025.
URL https://doi.org/10.1016/j.patcog.2015.02.025.

Giorgio Roffo, Simone Melzi, and Marco Cristani. Inﬁnite Feature Selection. In 2015 IEEE International Conference
on Computer Vision (ICCV), pages 4202–4210, Santiago, Chile, December 2015. IEEE. ISBN 978-1-4673-8391-2.
doi:10.1109/ICCV.2015.478. URL http://ieeexplore.ieee.org/document/7410835/.

Ryan J. Urbanowicz, Melissa Meeker, William La Cava, Randal S. Olson, and Jason H. Moore. Relief-based feature
selection: Introduction and review. Journal of Biomedical Informatics, 85:189–203, September 2018a. ISSN
1532-0464. doi:10.1016/j.jbi.2018.07.014. URL http://www.sciencedirect.com/science/article/pii/
S1532046418301400.

Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classiﬁcation: A review. Data classiﬁcation:

Algorithms and applications, page 37, 2014. Publisher: CRC press.

Xingxiang Li, Runze Li, Zhiming Xia, and Chen Xu. Distributed Feature Screening via Componentwise Debiasing.
Journal of Machine Learning Research, 21(24):1–32, 2020. URL http://jmlr.org/papers/v21/19-537.html.

Ryan J. Urbanowicz, Randal S. Olson, Peter Schmitt, Melissa Meeker, and Jason H. Moore. Benchmarking relief-based
feature selection methods for bioinformatics data mining. Journal of Biomedical Informatics, 85:168–188, September
2018b. ISSN 1532-0464. doi:10.1016/j.jbi.2018.07.015. URL http://www.sciencedirect.com/science/
article/pii/S1532046418301412.

Verónica Bolón-Canedo, Noelia Sánchez-Maroño, and Amparo Alonso-Betanzos. A review of feature selection
methods on synthetic data. Knowledge and Information Systems, 34(3):483–519, March 2013. ISSN 0219-3116.
doi:10.1007/s10115-012-0487-8. URL https://doi.org/10.1007/s10115-012-0487-8.

Zhanrui Cai, Runze Li, and Liping Zhu. Online Sufﬁcient Dimension Reduction Through Sliced Inverse Regression.
Journal of Machine Learning Research, 21(10):1–25, 2020. URL http://jmlr.org/papers/v21/18-567.html.

Cheng Yong Tang, Ethan X. Fang, and Yuexiao Dong. High-Dimensional Interactions Detection with Sparse Principal
Hessian Matrix. Journal of Machine Learning Research, 21(19):1–25, 2020. URL http://jmlr.org/papers/
v21/19-071.html.

P Cunningham. Dimension Reduction, Technical report UCD-CSI-2007-7. University College Dublin, 2007.
Lei Yu and Huan Liu. Feature selection for high-dimensional data: a fast correlation-based ﬁlter solution. In Proceedings
of the Twentieth International Conference on International Conference on Machine Learning, ICML’03, pages
856–863, Washington, DC, USA, August 2003. AAAI Press. ISBN 978-1-57735-189-4.

48

A novel evaluation methodology

TECHNICAL REPORT

Sebastián Maldonado and Richard Weber. Weber, R.: A wrapper method for feature selection using support vector

machines. Inf. Sci. 179(13), 2208-2217. Inf. Sci., 179:2208–2217, June 2009. doi:10.1016/j.ins.2009.02.014.

Hui-Huang Hsu, Cheng-Wei Hsieh, and Ming-Da Lu. Hybrid feature selection by combining ﬁlters and wrappers.

Expert Systems with Applications, 38(7):8144–8150, 2011. Publisher: Elsevier.

Sanmay Das. Filters, wrappers and a boosting-based hybrid for feature selection. In Icml, volume 1, pages 74–81, 2001.
Verónica Bolón-Canedo and Amparo Alonso-Betanzos. Ensembles for feature selection: A review and future trends.

Information Fusion, 52:1–12, 2019. Publisher: Elsevier.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference,
and Prediction, Second Edition. Springer Series in Statistics. Springer-Verlag, New York, 2 edition, 2009. ISBN 978-0-
387-84857-0. doi:10.1007/978-0-387-84858-7. URL https://www.springer.com/gp/book/9780387848570.
Christophe Ambroise and Geoffrey J. McLachlan. Selection bias in gene extraction on the basis of microarray gene-
expression data. Proceedings of the National Academy of Sciences, 99(10):6562–6566, May 2002. ISSN 0027-8424,
1091-6490. doi:10.1073/pnas.102102699. URL https://www.pnas.org/content/99/10/6562. Publisher:
National Academy of Sciences Section: Physical Sciences.

Martin Davis. The Undecidable: Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable
Functions. Courier Corporation, January 2004. ISBN 978-0-486-43228-1. Google-Books-ID: qW8x7sQ4JXgC.
Kevin Dunne, Padraig Cunningham, and Francisco Azuaje. Solutions to instability problems with sequential wrapper-

based approaches to feature selection. Journal of Machine Learning Research, pages 1–22, 2002.

Pavel Kˇrížek, Josef Kittler, and Václav Hlaváˇc. Improving stability of feature selection methods. In International

Conference on Computer Analysis of Images and Patterns, pages 929–936. Springer, 2007.

Ludmila I Kuncheva. A stability index for feature selection. In Artiﬁcial intelligence and applications, pages 421–427,

2007.

Alexandros Kalousis, Julien Prados, and Melanie Hilario. Stability of feature selection algorithms: a study on

high-dimensional spaces. Knowledge and information systems, 12(1):95–116, 2007. Publisher: Springer.

P Mohana Chelvan and Karuppasamy Perumal. A survey on feature selection stability measures. International Journal

of Computer and Information Technology, 5(1):98–103, 2016.

Sarah Nogueira, Konstantinos Sechidis, and Gavin Brown. On the Stability of Feature Selection Algorithms. Journal
of Machine Learning Research, 18(174):1–54, 2018. ISSN 1533-7928. URL http://jmlr.org/papers/v18/
17-514.html.

Janez Demšar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine learning research, 7

(Jan):1–30, 2006.

PB Nemenyi. Distribution-free multiple comparisons (doctoral dissertation, princeton university, 1963). Dissertation

Abstracts International, 25(2):1233, 1963.

Omry Yadan. Hydra - A framework for elegantly conﬁguring complex applications. Self-Published, 2019. URL

https://github.com/facebookresearch/hydra.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. Scikit-learn: Machine learning in Python. the
Journal of machine Learning research, 12:2825–2830, 2011. Publisher: JMLR. org.

Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: networked science in machine learning.
ACM SIGKDD Explorations Newsletter, 15(2):49–60, June 2014. ISSN 1931-0145. doi:10.1145/2641190.2641198.
URL https://doi.org/10.1145/2641190.2641198.

Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G. Mantovani, Jan N. van
Rijn, and Joaquin Vanschoren. OpenML Benchmarking Suites. arXiv:1708.03731 [cs, stat], September 2019. URL
http://arxiv.org/abs/1708.03731. arXiv: 1708.03731 version: 2.

Sasko Ristov, Radu Prodan, Marjan Gusev, and Karolj Skala. Superlinear speedup in HPC systems: Why and when? In
2016 Federated Conference on Computer Science and Information Systems (FedCSIS), pages 889–898, September
2016.

Lukas Biewald. Experiment Tracking with Weights and Biases. Self-Published, 2020. URL https://www.wandb.

com/.

Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. Vega-Lite: A Grammar of Interactive
Graphics. IEEE Transactions on Visualization and Computer Graphics, 23(1):341–350, January 2017. ISSN 1941-
0506. doi:10.1109/TVCG.2016.2599030. Conference Name: IEEE Transactions on Visualization and Computer
Graphics.

49

A novel evaluation methodology

TECHNICAL REPORT

Miron B. Kursa and Witold R. Rudnicki. Feature Selection with the Boruta Package. Journal of Statistical Software, 36
(1):1–13, September 2010. ISSN 1548-7660. doi:10.18637/jss.v036.i11. URL https://www.jstatsoft.org/
index.php/jss/article/view/v036i11. Number: 1.

Nicolai Meinshausen and Peter Buehlmann. Stability Selection. arXiv:0809.2932 [stat], May 2009. URL http:

//arxiv.org/abs/0809.2932. arXiv: 0809.2932.

Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System.

In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pages 785–
794, New York, NY, USA, August 2016. Association for Computing Machinery.
ISBN 978-1-4503-4232-2.
doi:10.1145/2939672.2939785. URL https://doi.org/10.1145/2939672.2939785.

Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. Learning to Explain: An Information-Theoretic
Perspective on Model Interpretation. arXiv:1802.07814 [cs, stat], June 2018. URL http://arxiv.org/abs/1802.
07814. arXiv: 1802.07814.

Isabelle Guyon.

Design

of

experiments

for

the NIPS

2003

variable

The Journal of Machine Learning Research,

mark.
semanticscholar.org/paper/Design-of-experiments-for-the-NIPS-2003-variable-Guyon/
b979fa88ca448fb08633f961131f45214b1cf109.

page 30,

2003.

selection

bench-
URL https://www.

David W. Aha, Dennis Kibler, and Marc K. Albert. Instance-based learning algorithms. Machine Learning, 6(1):37–66,
January 1991. ISSN 1573-0565. doi:10.1007/BF00153759. URL https://doi.org/10.1007/BF00153759.

50

A novel evaluation methodology

TECHNICAL REPORT

Appendices

A Experiment line-up

Feature
ranking

Multi-
output

Feature
support

Regr-
ession
(cid:88)

Feature
importance
(cid:88)

Classif-
ication
Name method
(cid:88)
ANOVA F-value (sklearn)
(cid:88)
Boruta [Kursa and Rudnicki, 2010]
(cid:88)
Chi-Squared (sklearn)
(cid:88)
Decision Tree (sklearn)
(cid:88)
FeatBoost (github)
(cid:88)
Inﬁnite Selection [Roffo et al., 2015]
(cid:88)
MultiSURF [Urbanowicz et al., 2018b]
(cid:88)
Mutual Info [Zaffalon and Hutter, 2014]
(cid:88)
ReliefF [Urbanowicz et al., 2018b]
Stability Selection [Meinshausen and Buehlmann, 2009] (cid:88)
(cid:88)
TabNet [Arik and Pﬁster, 2020]
(cid:88)
XGBoost [Chen and Guestrin, 2016]
Table 3: Feature Ranker line-up. Both classiﬁers and regressors are considered, as well as multioutput estimators.

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

n
506

10000
10000
10000
540

5456
150
2600
2000
34465
2534
5404
10000
10000
10000
10000
10000
10000
10000
10000
5500

Name dataset
Boston house
prices
Additive
Orange
XOR
Climate Model
Simulation
Cylinder bands
Iris Flowers
Madelon
Multifeat Pixel
Nomao
Ozone Levels
Phoneme
Synclf easy
Synclf hard
Synclf medium
Synclf very hard
Synreg easy
Synreg hard
Synreg medium
Synreg very hard
Texture

Wall Robot
Navigation

Task
Regression

Multi-
output Domain
Finance
No

Group
-

Yes
Regression
Yes
Regression
Regression
Yes
Classiﬁcation No

Synthetic
Synthetic
Synthetic
Nature

Chen et al. [Chen et al., 2018]
Chen et al. [Chen et al., 2018]
Chen et al. [Chen et al., 2018]
OpenML-CC18 [Bischl et al., 2019]

p
11

10
10
10
18

Classiﬁcation No
24
4
Classiﬁcation No
500 Classiﬁcation No
240 Classiﬁcation No
Classiﬁcation No
89
Classiﬁcation No
72
Classiﬁcation No
5
Classiﬁcation No
20
Classiﬁcation No
50
Classiﬁcation No
30
Classiﬁcation No
50
No
Regression
10
No
Regression
20
No
Regression
10
Regression
20
No
Classiﬁcation No
40

Mechanics
Nature
Synthetic
OCR
Geodata
Nature
Biomedical
Synthetic
Synthetic
Synthetic
Synthetic
Synthetic
Synthetic
Synthetic
Synthetic
Pattern
Recognition
Mechanics

OpenML-CC18 [Bischl et al., 2019]
-
Guyon [Guyon, 2003]
OpenML-CC18 [Bischl et al., 2019]
OpenML-CC18 [Bischl et al., 2019]
OpenML-CC18 [Bischl et al., 2019]
OpenML-CC18 [Bischl et al., 2019]
Synclf
Synclf
Synclf
Synclf
Synreg
Synreg
Synreg
Synreg
OpenML-CC18 [Bischl et al., 2019]

OpenML-CC18 [Bischl et al., 2019]

5456

24

Classiﬁcation No

Table 4: All datasets considered in the experiment. Both real-world and synthetic datasets are considered. Whereas
synthetic datasets are generated in the pipeline itself, real-world datasets are fetched from OpenML.

51

A novel evaluation methodology

TECHNICAL REPORT

Figure 24: Class-distributions of the classiﬁcation datasets described in Table 4. The x-axis shows the target classiﬁcation
classes and the y-axis shows the amount of samples with that target class. No binning was applied, the distributions are
like shown.

52

010100200300400500CountClimate Model Simulation01230500100015002000Cylinder bands01201020304050Iris Flowers01020040060080010001200Madelon0123456789050100150200CountMultifeat Pixel010500010000150002000025000Nomao010500100015002000Ozone Levels0101000200030004000Phoneme01010002000300040005000CountSynclf easy0120500100015002000250030003500Synclf hard01Class010002000300040005000Synclf medium012Class0500100015002000250030003500Synclf very hard012345678910Class0100200300400500CountTexture0123Class0500100015002000Wall Robot NavigationA novel evaluation methodology

TECHNICAL REPORT

B Additional results

Figure 25: Learning curve for ‘Synclf hard’ dataset. Sample sizes from 100 to 2,100 were used, with intervals of 200.
The average accuracy over 10 bootstraps is shown.

53

02004006008001,0001,2001,4001,6001,8002,000sample size0.00.10.20.30.40.50.6accuracyANOVA F-valueChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectioTabNetXGBoostrankerLearning curve`Synclf hard` dataset w/ Decision Tree validationA novel evaluation methodology

TECHNICAL REPORT

Figure 26: k-NN mean validation scores for all classiﬁcation datasets. Color scale is conﬁgured with relative
performance, like explained in Section 7.2.

Figure 27: DT mean validation scores for all regression datasets. Color scale is conﬁgured with relative performance,
like explained in Section 7.2.

54

0.9310.8600.9250.4830.4650.4850.6960.5540.4820.4850.4480.4810.4840.4600.4830.4620.5630.7900.5590.5630.5440.5580.5510.5610.5620.5600.7870.5620.6990.9290.6610.7030.9350.6360.7020.6920.6990.5910.6970.5880.8040.5380.9590.8600.9270.7850.7920.7590.7930.7920.7880.7930.7740.7800.7650.9280.9330.7320.9340.9430.9310.9180.9320.9370.8670.8540.9740.9730.9750.8660.8670.8540.8660.8670.8650.9760.8650.9380.9340.9370.9290.9320.9350.6830.6640.6780.8220.9200.9220.9300.8330.9180.7810.8030.3890.8710.7810.9370.9750.9290.8390.9570.5430.8380.9000.8670.9380.9750.6550.9230.9760.8330.9280.8380.8670.6830.8050.9350.8350.9600.6770.8350.7740.4990.8000.7840.7920.7680.7580.7820.5550.7670.9320.9260.9340.9320.9330.9330.9310.9320.9330.9350.7960.7910.7960.7890.7920.7830.7910.7870.7870.7990.4870.5640.7030.7940.9430.8560.9310.8740.9390.8670.9770.8260.9390.8000.50.70.9relativeperformancecolor scale:(x⁵)Feature ranker performance - classiﬁcation→ Mean accuracy validation score over all bootstrapsANOVA F-valueBorutaChi-SquaredDecision TreeFeatBoostInﬁnite SelectionMultiSURFMutual InfoReliefFStability SelectionTabNetXGBoostrankerClimate Model SimulationCylinder bandsIris FlowersMadelonMultifeat PixelNomaoOzone LevelsPhonemeSynclf easySynclf hardSynclf mediumSynclf very hardTextureWall Robot Navigationdataset0.2060.9030.0240.9080.6940.955−0.5120.5380.5390.4910.4490.5380.4540.4540.4270.4350.4400.9530.9530.9530.9530.9530.9530.9530.9530.9530.953−0.450−0.4470.609−0.602−0.420−0.5830.6090.5390.4640.9530.953−0.4150.7810.9690.9481801.000000relativeperformancecolor scale:(quantiles)Feature ranker performance - regression→ Mean R2 validation score over all bootstrapsANOVA F-valueDecision TreeMultiSURFMutual InfoReliefFTabNetXGBoostrankerAdditive (Chen et al.)Boston house pricesOrange (Chen et al.)Synreg easySynreg hardSynreg mediumSynreg very hardXOR (Chen et al.)datasetA novel evaluation methodology

TECHNICAL REPORT

Figure 28: Algorithm Log loss scores for regression datasets, computed using the GT feature importanes. The
computation was explained in Section 5.3 and illustrated in Section 7.2. A lighter shade of blue means a better score.

Figure 29: Algorithm R2 scores for regression datasets, computed using the GT feature importanes. The computation
was explained in Section 5.3 and illustrated in Section 7.2. A darker shade of green means a better score.

55

7.0920.2469.6800.2530.2240.4931.0051.4221.4391.5136.8281.5221.6161.3761.4091.5098.8951.5311.6410.1800.1980.4880.0810.1880.3000.1810.1980.4880.0810.1880.3000.1670.3690.5040.973relativeperformanccolorscale:(quantiles)Log loss→ Computed using GT relevant features. Lower is better.ANOVA F-valueDecision TreeMultiSURFMutual InfoReliefFTabNetXGBoostrankerditive (Chen et al.)range (Chen et al.)Synreg easySynreg hardSynreg mediumSynreg very hardXOR (Chen et al.)−0.9380.701−2.3640.6900.2130.497−0.1720.6570.7830.067−12.6020.5470.3010.5440.6490.072−10.823−0.129−0.3020.9380.9520.176−6.5080.9130.6640.9360.9520.176−6.5070.9130.664−0.4661.000relativeperformanccolorscale:(quantiles)R2 score→ Computed using GT relevant features. Higher is better.ANOVA F-valueDecision TreeMultiSURFMutual InfoReliefFTabNetXGBoostrankerditive (Chen et al.)range (Chen et al.)Synreg easySynreg hardSynreg mediumSynreg very hardXOR (Chen et al.)A novel evaluation methodology

TECHNICAL REPORT

Glossary

AI Artiﬁcial Intelligence. 4, 7, 56
apriori Apriori information refers to any possible information about the dataset feature importances one has before
running any learning algorithms. Common ways to obtain such information are human domain experts or the
usage of synthetic datasets. 5, 11, 21, 35, 39, 43, 45, 56

CV Cross-Validation. 19, 25, 26, 29, 34, 56

DT Decision Tree. 9, 15, 34, 36, 37, 39, 40, 54, 56

GT Ground-Truth. 35, 36, 39, 40, 41, 55, 56

HPC High-Performance Computing. 31, 32, 34, 56

IB1 Instance-based classiﬁcation algorithm originally proposed by Aha et al. (1991) [Aha et al., 1991]. 9, 56

k-NN k- Nearest Neighbors. 9, 34, 39, 40, 54, 56

LASSO Least Absolute Shrinkage and Selection Operator. 15, 17, 56
LDA Linear Discriminant Analysis. 12, 56

ML Machine Learning. 7, 10, 25, 26, 34, 46, 56

NB Naïve Bayes. 9, 56

PART Partial Decision Tree algorithm. 9, 56
PCA Principle Component Analysis. 12, 56
PCC Pearson product-moment Correlation Coefﬁcient. 9, 56

RBA Relief-Based Feature Selection Algorithms. 17, 56
RFE Recursive Feature Elimination. 6, 14, 15, 56
RQ “RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with
workers. It is backed by Redis and it is designed to have a low barrier to entry.” - https://python-rq.org/. 31, 32,
56

SVD Singular Value Decomposition. 17, 56
SVM Support Vector Machine. 9, 15, 17, 56

56

