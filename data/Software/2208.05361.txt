2
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

2
v
1
6
3
5
0
.
8
0
2
2
:
v
i
X
r
a

Prompt-tuned Code Language Model as a Neural Knowledge
Base for Type Inference in Statically-Typed Partial Code

Qing Huang
qh@whu.edu.cn
Jiangxi Normal University
China

Xiwei Xu
Xiwei.Xu@data61.csiro.au
CSIRO’s Data61
Australia

Zhiqiang Yuan
yuanzhiq@jxnu.edu.cn
Jiangxi Normal University
China

Liming Zhu
Liming.Zhu@data61.csiro.au
CSIRO’s Data61 & School of CSE,
UNSW
Australia

Zhenchang Xing
zhenchang.xing@data61.csiro.au
CSIRO’s Data61 & Australian
National University
Australia

Qinghua Lu
Qinghua.lu@data61.csiro.au
CSIRO’s Data61
Australia

ABSTRACT
Partial code usually involves non-fully-qualified type names (non-
FQNs) and undeclared receiving objects. Resolving the FQNs of
these non-FQN types and undeclared receiving objects (referred
to as type inference) is the prerequisite to effective search and
reuse of partial code. Existing dictionary-lookup based methods
build a symbolic knowledge base of API names and code contexts,
which involve significant compilation overhead and are sensitive
to unseen API names and code context variations. In this paper, we
formulate type inference as a cloze-style fill-in-blank language task.
Built on source code naturalness, our approach fine-tunes a code
masked language model (MLM) as a neural knowledge base of code
elements with a novel “pre-train, prompt and predict” paradigm
from raw source code. Our approach is lightweight and has mini-
mum requirements on code compilation. Unlike existing symbolic
name and context matching for type inference, our prompt-tuned
code MLM packs FQN syntax and usage in its parameters and sup-
ports fuzzy neural type inference. We systematically evaluate our
approach on a large amount of source code from GitHub and Stack
Overflow. Our results confirm the effectiveness of our approach
design and the practicality for partial code type inference. As the
first of its kind, our neural type inference method opens the door
to many innovative ways of using partial code.

CCS CONCEPTS
• Software and its engineering → Software organization and
properties;

ACM Reference Format:
Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu,
and Qinghua Lu. 2022. Prompt-tuned Code Language Model as a Neural
Knowledge Base for Type Inference in Statically-Typed Partial Code. In

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE ’22, October 10–14, 2022, Rochester, MI, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9475-8/22/10. . . $15.00
https://doi.org/10.1145/3551349.3556912

37th IEEE/ACM International Conference on Automated Software Engineering
(ASE ’22), October 10–14, 2022, Rochester, MI, USA. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3551349.3556912

1 INTRODUCTION
Partial code is prevalent in API documentation and online blogs.
Figure 1 shows a typical example of partial code in statically-typed
programming language like Java. Partial code is usually syntacti-
cally incomplete. For example, it may miss the enclosing class and
method declaration. Adding the class and method declaration can
easily fix the issue [1]. The more challenging issue in partial code
is that it may invoke methods or access fields on some undeclared
receiving objects (e.g., reader) or use types by non-fully-qualified
names (non-FQNs) (e.g., List<String>, new File(), URL). Undeclared
receiving objects and non-fully-qualified type names will result in
the compilation error “symbol cannot be resolved”.

Effectively parsing partial code and leveraging its knowledge
is important for software engineering [2, 3]. Without resolving
undeclared receiving objects and non-FQN types, partial code can
only be used as text. This greatly limits the application of program
analysis techniques to partial code, and affects the effective use of
partial code. For example, URL in Figure 1 should be java.net.URL,
not com.google.gwt.http.client.URL. Without knowing the precise
type, a code search engine may recommend irrelevant code exam-
ples [3, 4]. Furthermore, several studies show online code snip-
pets often contain API misuses or even malicious behaviors [5, 6].
However, existing code vulnerability analysis tools [7–9] cannot
effectively work without accurate type information.

To infer fully-qualified type names (FQNs) for undeclared re-
ceiving objects and non-FQN types in partial code, all existing
techniques [1, 10, 11] adopt a dictionary-lookup strategy, which
uses a symbolic knowledge base that maps simple names and code
contexts to FQNs. The mappings between simple names and FQNs
of API elements can be built by parsing library source code or
documentation. However, building the mappings between code
contexts and FQNs requires to successfully compile code contexts
in which API elements are used. This compilation overhead limits
the amount of collected code contexts and used APIs. This limita-
tion, together with symbolic name and context matching, results

 
 
 
 
 
 
ASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

type inference task. Although FQN prompts contain only local code
contexts, the model learns long-range dependencies among code
elements through FQN prompts with overlapping code lines.

Our type inference task aligns perfectly with the pre-training
and prompt learning of code MLM. Given an unseen partial code,
we form code prompts with masks for non-FQN types and unde-
clared receiving objects in the same form as the FQN prompts used
for prompt learning, and use the prompt-tuned code MLM to fill
in the masks with the missing FQNs. In NLP tasks, prompts use a
fixed-length mask span. However, FQNs vary in length. We develop
a variable-length mask prediction method to infer the variable-
length FQNs by searching for the FQN length with maximum prob-
ability. Unlike symbolic name and context matching in existing
methods [1, 11, 33], our inference uses code prompt to activate the
FQN knowledge packed in the parameters of the code MLM.

Our evaluation focuses on two aspects, i.e., effectiveness and
practicality. For effectiveness, we evaluate how intrinsic factors
(prompt learning data size and FQN masking strategy) affect our
model performance and find that prompt learning with full span
masking strategy on 10% of library source code can significantly
boost the type inference accuracy, surpassing the vanilla pre-trained
code MLM by an increase of 0.539. Then we evaluate how extrinsic
factors (code context similarity and API cardinality) affect our model
and find that our model makes accurate type inference in face of
large context variations and high API cardinalities.

For practicality, we evaluate the model performance on partial
code snippets from Stack Overflow. Our model outperforms the
state-of-the-art tools COSTER [1] and SnR [11] (on average accu-
racy 0.91 versus 0.71 and 0.87). Furthermore, for the FQNs and code
contexts unseen during prompt learning, our model still achieves
the accuracy around 0.7.

Our approach is the first successful attempt to adopt the “pre-
train, prompt and predict” paradigm [17, 18, 34, 35] to support
program analysis in software engineering. The outstanding perfor-
mance of our model is rooted in the perfect alignment of large code
pre-training, effective FQN prompt learning, and type inference as
a fill-in-blank task. This alignment not only allows FQNs and code
contexts to be effectively embedded in a neural code knowledge
base, but also allows FQNs (even unseen ones) to be accurately
inferred from variant code contexts.

In this paper, we make the following contributions:

• We are the first to formulate the type inference problem as a cloze-
style fill-in-blank language task, and propose a novel “pre-train,
prompt and predict” paradigm to solve the task.

• We design the first neural partial code type inference approach
that does not rely on symbolic knowledge base of API names and
code contexts. In contrast, our approach uses a prompt-tuned
code MLM as a neural knowledge base which makes it realistic
to overcome the OOV failures in symbolic type inference.

• We design the first prompt learning method to stimulate task-
agnostic pre-trained code MLM to recognize fine-grained FQN
syntax and usage, and are the first to apply prompt-tuned code
MLM for the partial code type inference task.

• Driven by code-specific task needs, we design automatic and
contextual FQN prompt and full-span mask strategy for prompt
tuning, and develop variable-length mask prediction method.

Figure 1: A Partial Code Snippet in Java

Figure 2: Type Inference as a Fill-in-blank Task

in out-of-vocabulary (OOV) failures when looking up the dictio-
nary with unseen API names or different code contexts. Although
we can theoretically overcome the OOV failures by building more
comprehensive knowledge base from more software projects, the
compilation overhead makes this solution unrealistic.

As illustrated in Figure 2, we innovatively formulate type infer-
ence as a cloze-style fill-in-blank language task. Instead of relying
on compilable software projects to construct symbolic knowledge
base for type inference, we fine-tune a masked language model
(MLM) pre-trained on source code [12, 13] as the neural knowl-
edge base. Based on source code naturalness [14, 15], language
models (including recent large pre-trained language models [16–
18]) have been successfully adopted on source code in software
engineering tasks [12, 19–23]. Some recent NLP works show that
pre-trained language models can serve as a neural knowledge base
of real-world entities and relations [24], as opposed to symbolic
knowledge bases [25, 26]. Our work shows for the first time that
pre-trained code model has this knowledge base capability for code
elements and relations. Unlike discrete representation of API names
and code contexts in symbolic knowledge base [1, 10, 11], the MLM-
based neural knowledge base packs complex code information in a
continuous space in the parameters of neural network.

However, the vanilla pre-trained code MLM cannot support the
type inference task, because it is trained purely on source code
without deep understanding of code syntax and semantics [27]. We
design a novel prompt learning method to stimulate pre-trained
code MLM to recognize the form of FQNs and the relationships be-
tween API names and code contexts in source code. Prompt learning
supports “self-upgrade” of pre-trained MLM, as pre-training and
prompt learning are homogeneous at the MLM core. We automat-
ically annotate a small amount of compilable library source code
with FQNs. Different from manually defined, uniform prompts in
NLP tasks [28–32], our approach automatically extract code lines
surrounding a focused code line with FQNs as its contextual FQN
prompt. We mask the FQNs in the focused code line and train the
MLM to predict the masked FQNs in the FQN prompts. Instead
of commonly used random masking [12, 16], we design full span
masking strategy that fits with the FQN characteristics and our

1  List<String> Urlcontent = new ArrayList<String>(); 2  File root = new File(path);  3  File[] subFiles = root.listFiles(); 4  for (File filename : subFiles) { 5      while ((content = reader.readLine()) != null) {  6          URL url = new URL(content); 7          URLConnection connection = url.openConnection(); 8          Urlcontent.add(connection.toString().toLowerCase());} 9     reader.close();}                  reader.readLine()) != null) {               .URL url = new....Undeclared receiving objectNon-FQNjava.netjava.io.BufferedReader....Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Figure 3: Approach Overview

• We systematically evaluate our approach design. Our approach
achieves superior performance even with only a small amount of
prompt learning code, and is the first approach with the promis-
ing capability of few-shot type inference. Our data package can
be found here1. The code will be released upon paper acceptance.

2 PROBLEM DEFINITION
Different from existing dictionary-lookup based type inference
methods [1, 11, 33], we formulate the type inference in statically-
typed partial code as a cloze-style fill-in-blank language task. Given
an input partial code snippet, we make type inference at three types
of places (those highlighted in yellow in Figure 1: 1) data type of
variable declaration; 2) type name of class instantiation and array
creation; and 3) the object or the type on which a method is invoked
or a field is accessed. Type can be simple type including annotation
and enum) (e.g., URL), array type (e.g., File[]), and generic type
(e.g., List<String>). We do not consider other variable usage (e.g.,
variable assignment, method argument, array access), for example
content in the lines 5 and 6 in Figure 1, because the relevant data
type can be derived from assignment expression and method signa-
ture after resolving the missing types in partial code [36]. We do
not make inference for chained method calls and field accesses, for
example, connection.toString().toLowerCase() in the line 8 in Figure 1
Once connection is resolved, the receiving type of toLowerCase()
can be inferred from the return type of connection.toString(). We
refer to non-FQN type names (e.g., File, URL) and undeclared re-
ceiving objects (e.g., reader) as type inference points. As illustrated
in Figure 2, our goal is to fill in the missing type information at
type inference points in the same way as a person completes a
fill-in-blank language task. Although type inference provides the
basis for making a partial code compilable, this work does not aim
to produce a syntactically correct code.

3 APPROACH
We propose a novel masked language model (MLM) based “pre-
train, prompt and predict” paradigm to solve the fill-in-blank task
for type inference. As shown in Figure 3, in offline learning phase,
we choose a MLM pre-trained on a large corpus of source code,
without the need of code compilation. Then, we design code-specific
prompt learning method which uses a small amount of compiled
library source code to stimulate pre-trained code MLM to recognize
FQN syntactic and usage patterns. The offline learning produces a
prompt-tuned code MLM whose learning objective aligns perfectly
with the fill-in-blank task for type inference. At the inference time,
the type inference point in the partial code is converted into a code
prompt with masks, fed into the prompt-tuned code MLM, serving
as a neural knowledge base to predict the tokens at the masks.

1https://anonymous.4open.science/r/Experiment-Datas-93AE/README.md

3.1 Pre-training MLM of Source Code
Exisitng type inference methods [1, 11, 33] suffer from the OOV
failures caused by the reliance on compilable code contexts to build
symbolic knowledge base of API usage. To remove this reliance, our
approach works on raw source code. Studies on source code natu-
ralness [14, 15] show that code can be understood and manipulated
in the same way as natural language text. In this work, we adopt
a Transformer-based MLM. Transformer [37] is the backbone of
the state-of-the-art language models. A MLM, such as BERT [38] is
trained to predict the masked word in the input text. This training
objective aligns perfectly with our formulation of type inference as
a fill-in-blank language task.

In our current implementation, we use the pre-trained CodeBert
model [12] which was trained on a large code corpus provide by
Husain et al. [39]. We choose CodeBert because many studies [40–
42] confirm that the model captures rich code information and
demonstrate its effectiveness in many downstream tasks [7, 43–45].
However, our approach is not limited to CodeBert, but can use
any MLMs. The training code corpus can be easily expanded with
some code crawling effort, as model training does not require any
compiled code. A technical challenge to apply language models to
a large code corpus is the rare words of program element names.
Using a word tokenizer will result in severe out-of-vocabulary
tokens and poor representation for rare tokens. To overcome this
issue, we use subword tokenizer WordPiece [46] which is a standard
practice to deal with the token OOV issues.

Figure 4: An Example of FQN Prompt Generation

3.2 Prompt-based FQN Learning
The vanilla pre-trained code MLM does not have deep understand-
ing of code syntax and semantics [9, 27], because it is trained on
raw source code as text. We propose a prompt tuning method which
uses a small amount of FQN stimuli (i.e., prompts) to upgrade the
pre-trained MLM with FQN syntax and usage. Contextual FQN
prompts are automatically derived from compiled library source
code. Different from existing work [47, 48] that uses heterogeneous
downstream tasks to fine-tune the pre-trained MLM, our model

Randomsamplesource codefrom libraryFQN-annotatedsource codeCode BlockAutomatic, Contextual FQN promptAny partialcodeIdentified typeinferencepointsFill-in-blankcode promptLarge scalecode corpusCode tokensMaskedtokensPre-trainedCode MLMFQNs for themaskPrompt  tuning code MLMtokenizationtoken  random maskingpre-trainingOffline Prompt tuningOffline Pre-trainingOnline InferenceFQN  annotationcontext  collectiontokenization  and full span  maskingtype inference  points  identificationCode Blockpredictcontext  collectiontokenization  and  span maskingmitigatemitigatePrompt-tuned  Code  MLM as neural  knoweldge base OOV Problem in type inferenceSource code X 1  LocalTime base = new LocalTime(); 2  DateTime dt = new DateTime(); 3  DateTime test = base.toDateTime(dt); 4  Calendar cal = new GregorianCalendar(); 5  cal.setTime(new Date()); 6  LocalDate date = new LocalDate();FQN-annotated source code XFQN 1  org.joda.time.LocalTime base = new org.joda.time.LocalTime(); 2  org.joda.time.DateTime dt = new org.joda.time.DateTime(); 3  org.joda.time.DateTime test = org.joda.time.LocalTime.toDateTime(dt); 4  java.util.Calendar cal = new java.util.GregorianCalendar(); 5  java.util.Calendar.setTime(new java.util.Date()); 6  org.joda.time.LocalDate date = new org.joda.time.LocalDate();Code Block CBL3 for the 3rd code line L3 1  org.joda.time.LocalTime base = new org.joda.time.LocalTime(); 2  org.joda.time.DateTime dt = new org.joda.time.DateTime(); 3  org.joda.time.DateTime test = org.joda.time.LocalTime.toDateTime(dt); 4  java.util.Calendar cal = new java.util.GregorianCalendar(); 5  java.util.Calendar.setTime(new java.util.Date());  ['org','.','j','oda','time','.','Local',...,');', '<m>','<m>','<m>','<m>','<m>','<m>','.','Date','Time',...,'<m>','<m>','<m>','<m>','<m>,'<m>','<m>','<m>','<m>','.','to','Date','Time',...,');', 'java','.','util','.','.','Cal','endar',...,');']  ['org','.','j','oda','time','.','Local',...,');', 'org','.','j','oda','.','time','.','Date','Time',...,'org','.','j','oda','.','time','.','Local','Time','.','to','Date','Time',...,');', 'java','.','util','.','.','Cal','endar',...,');'] Token sequence TCBL3FQN prompt X'L3  for the 3rd code line L3FQN annotationContext collectionSubword tokenizationFQN maskingASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

pre-training and prompt learning are homogeneous. Both are MLM
learning and align with our type inference task formulation.

it forces the model to learn to make inference from limited context
information which is often the case for (short) partial code.

3.2.1 Automatic, Contextual FQN Prompt Generation. We generate
FQN prompts from some compiled source code files of the libraries
whose APIs need to be inferred. Compiling a library is much easier
than compiling a large number of software projects using the library.
We only need a small amount of randomly sampled library source
code (e.g., 10%) to achieve highly effective prompt tuning. This is
because prompt tuning stimulates pre-trained MLM to recognize
FQN syntactic and usage patterns in the large code corpus on which
it is trained, rather than simply memorizing some FQNs and limited
usage contexts used for model fine tuning.

Given the code file 𝑋 = {𝐿𝑖 } (𝐿𝑖 (1 ≤ 𝑖 ≤ 𝑛) are 𝑛 code lines in
the file), FQN prompt generation 𝐺 (𝑋 ) includes a sequence of au-
tomatic code transformations: FQN annotation, context collection,
subword tokenization, and FQN masking, and outputs 𝑘 (𝑘 ≤ 𝑛)
distinct FQN prompts 𝑋 ′
𝐿𝑖 for the code lines 𝐿𝑖 with FQNs. Here,
the code lines 𝐿𝑖 are separated by the semicolon (i.e., “;”), not the
newline. This design breaks down the syntactic boundaries between
code elements, which treats the code more similar to the inputs
of model pre-training and results in long prompts. Furthermore,
splitting code lines by “;” brings adjacent more code elements in the
same lines, which makes the resulting code lines contain as much
FQN information as possible so that MLM learns richer FQN syntax
and usage information from an FQN prompt during prompt learn-
ing. Figure 4 illustrates the process. Except for the FQN information,
𝐺 (𝑋 ) does not use any other code analysis (e.g., AST or data flow).
We feed the FQN prompt 𝑋 ′
𝐿𝑖 to the pre-trained MLM to predict
the masked FQN tokens. The original code token sequence is the
ground truth to tune the MLM by minimizing the cross-entropy
loss between the ground-truth toke and the prediction (the same
learning objective as the MLM pre-training [16]).

FQN annotation. We perform FQN annotations at the three
types of type inference points defined in Section 2. As the code
𝑋 is compilable, annotating it with FQNs is straightforward. For
example, simple name LocalTime in line-1 is replaced by its FQN
org.joda.time.LocalTime. For the receiving object of method invo-
cation or field access, we replace the variable name with the type
FQN of the variable. For example, base.toDateTime() is annotated
as org.joda.time.LocalTime.toDateTime(). This abstracts away vari-
able name variations but keeps only the receiving type of method
invocation or field access. Although the code is from a particular
library, it may use APIs of other libraries, such as “Calendar” in
line-4 from the Java SDK, which are also annotated with FQNs. We
denote FQN-annotated code as 𝑋𝐹𝑄𝑁 .

Context collection. Next, we scan 𝑋𝐹𝑄𝑁 from top down to
collect a code block 𝐶𝐵𝐿𝑖 as the local context of each code line 𝐿𝑖 .
If 𝐿𝑖 does not contain FQNs, we ignore this code line. The code
block consists of up to 𝑡 adjacent code lines before and after 𝐿𝑖 ,
denoted as 𝐿𝑡
𝑠 respectively, i.e., 𝐶𝐵𝐿𝑖 =𝐿𝑡
𝑠 . In Figure 4,
the code block 𝐶𝐵𝐿3 for the 3rd code line contains five lines of code:
line-1&2 (𝐿𝑡
𝑠 ). In this work, we set
𝑡 = 2, that is, a code block may have up to 5 lines of code, mainly
limited by the window size of the MLM (512 tokens in our current
implementation). The benefit of using a small local context is that

𝑝 ), line-3 (𝐿2) and line-4&5 (𝐿𝑡

𝑝 and 𝐿𝑡

𝑝𝐿𝑖𝐿𝑡

𝑝

⊕ 𝑇𝐿𝑖 ⊕ 𝑇𝐿𝑡

𝑝 , 𝑇𝐿𝑖 and 𝑇𝐿𝑡

𝑝 are the token sequence for 𝐿𝑡

Subword tokenization. We tokenize the code block by the
same WordPiece [46] tokenizer used for pre-training MLM, and
obtain a token sequence 𝑇𝐶𝐵𝐿𝑖 = 𝑇𝐿𝑡
𝑝 for each code
block, where 𝑇𝐿𝑡
𝑝 , 𝐿𝑖 and
𝐿𝑡
𝑠 , respectively, and ⊕ is sequence concatenation.
FQN masking. Finally, given the token sequence 𝑇𝐶𝐵𝐿𝑖 of the
code block 𝐶𝐵𝐿𝑖 , we mask the FQN tokens in 𝑇𝐿𝑖 , and leave the
𝑠 unchanged. As a result, we obtain 𝑘 (𝑘 ≤ 𝑛)
tokens in 𝑇𝐿𝑡
𝑝 and 𝑇𝐿𝑡
FQN prompts 𝑋 ′
𝑝 for the 𝑘 code lines
= 𝑇𝐿𝑡
𝐿𝑖
𝐿𝑖 with FQNs. As each prompt has a different focused code line
with FQN masks, none of the 𝑘 prompts are identical. Our FQN
prompts have overlapping code lines, which allows the model to
learn long-range dependencies beyond local contexts.

⊕ 𝑚𝑎𝑠𝑘 (𝑇𝐿𝑖 ) ⊕ 𝑇𝐿𝑡

𝑝

𝑚𝑎𝑠𝑘 (𝑇𝐿𝑖 ) masks all FQNs in 𝐿𝑖 , and keep all other tokens un-
changed. The FQN tokens that do not appear in the original code 𝑋
will be masked. For example, for the FQN org.joda.time.DataTime in
the 𝐿3 in Figure 4, we mask the package name prefix org.joda.time
before DataTime. For the FQN of the receiving object of the method
call base.toDateTime(), we mask the whole class FQN org.joda.time.-
LocalTime before toDateTime(). Our mask method aligns with our
fill-in-blank type inference task, and produces FQN prompts that
stimulates the MLM to learn to fill in the missing type information.
We mask not only name tokens but also name separators (e.g., “.” for
Java). This lets the model learn to generate FQNs in the proper form.
We consider two masking strategies: random mask or full span mask.
Random mask is the commonly used strategy in NLP [12, 16]. In our
application, we follow the CodeBert setting, i.e., randomly mask
15% tokens in the FQN parts to be masked. Considering the fact
that parts of a FQN are integral as a whole, we design the full span
mask which masks all the tokens in the FQN parts to be masked.
Studies [49] show span masking makes the mask prediction more
challenging and forces the model to learn better embeddings.

3.3 Type Inference as a Fill-in-blank Task
Given a partial code 𝑋𝑝𝑐 , we use the prompt-tuned code MLM as a
neural knowledge base to infer the FQNs of non-FQN type names
and undeclared receiving objects in 𝑋𝑝𝑐 . As partial code cannot be
compiled in general [50], we develop regular expressions similar to
those in existing type inference methods [11, 33] to identify type
inference points based on code lexical patterns.

We perform type inference for one type inference point at a time.
Let 𝐼 be a type inference point in line 𝐿𝑗 . We collect a code block
𝐶𝐵𝐼
𝐿𝑗 in the same way as described in Section 3.2.1. We use the
WordPiece tokenizer to tokenize the code block. As illustrated in
Figure 2, for a non-FQN name (e.g., URL in Figure 1), we prepend
mask tokens to the non-FQN type name. For an undeclared receiv-
ing object (e.g., reader in Figure 1), we replace the variable name
with mask tokens. The resulting token sequence 𝑋 𝐼
𝐿𝑗 forms a fill-
in-blank code prompt, in the same form as the FQN prompts used
for tuning the MLM. 𝑋 𝐼
𝐿𝑗 is fed into the prompt-tuned MLM to
predict the masked tokens, i.e., fill in the missing type names in
𝑋𝑝𝑐 . The predicted tokens are concatenated to form the FQNs. In
NLP, prompts contain fixed-length mask span. However, the length

Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

of FQNs vary. We produce multiple prompts 𝑋 𝐼
𝐿𝑗 with different
lengths of masks, and compare the average probabilities of the
mask predictions at different lengths to select the best prediction.
We use the minimum and maximum mask span 𝑚𝑖𝑛 and 𝑚𝑎𝑥, i.e.,
the shortest and longest subtoken length of the FQNs observed in
the libraries. 𝑚𝑖𝑛=3 and 𝑚𝑎𝑥=69 in our current implementation. It’s
worth noting that the known FQNs in the generated code prompts
have upper and lower bounds. The upper bound of known FQNs
in a code prompt means only the type inference point to be pre-
dicted is simple name while the FQNs of all other simple names are
known. This is the easiest case for FQN inference (referred to as
leave-one-out). In contrast, the lower bound means the FQNs of
all simple names are unknown. This is the most difficult case for
FQN inference (referred to all-unknown). In fact, the code prompt
generated by the real partial code lies in between the upper and
lower bounds, where, in addition to the type inference point to be
predicted, the FQNs of some simple names would be known while
others may be unknown.

4 EVALUATION
We evaluate our approach from the two following aspects:
• Effectiveness: we investigate how intrinsic factors (RQ1) (i.e.,
prompt-tuning data size and FQN masking strategy) and extrin-
sic factors (RQ2) (i.e., prompt similarity and API cardinalities
by simple names) affect our approach.

• Practicality, we investigate (RQ3) how accurate is our ap-
proach for inferring types in code snippets collected from Stack
Overflow, (RQ4) how well our approach performs for the FQNs
and code contexts seen or unseen during prompt tuning, and
(RQ5) what are typical failure patterns and causes.

Hardware Configuration. These experiments are conducted with
CUDA on NVIDIA GeForce RTX 3090. The operating system is
Ubuntu 20.04.3 LTS.

4.1 Effectiveness Evaluation
4.1.1 Motivation. Our approach is the first type inference method
based purely on a neural knowledge base encoded in the code MLM.
The FQN prompt learning is affected by two intrinsic factors: 1)
the amount of code used for prompt learning; 2) the FQN mask
strategy (random mask versus full span mask). Furthermore, two
extrinsic factors also affect the type inference performance: 1) the
code similarity between the code prompt for type inference and the
FQN prompts for prompt learning; 2) the number of APIs whose
simple names are the same (referred to as cardinality or ambiguity
for type inference [1]). It is important to study and understand how
these intrinsic and extrinsic factors affect our approach in particular
and neural type inference method in general.

4.1.2 Dataset. To obtain reliable results, we need a large amount
of code snippets with ground-truth FQNs. We consider six libraries,
including Android, GWT, Hibernate, Joda Time, Xstream, and JDK
used in previous type inference work [1, 11, 33]. Different from
previous work that collects a small amount of code using these
library APIs, we use the library source code itself for large-scale
effectiveness evaluation. We download the source code from the
library’s Github repositories, and compile the code successfully.

In total, we collect 39,255 source code files, including 3,190, 7,096,
10,670, 17,404, 330 and 565 source code file from Android, GWT,
Hibernate, JDK, Joda Time and Xstream, respectively. As the focus
of this evaluation is on the capability of FQN prompt learning and
inference, we collect all methods, not just public API methods in
these code files.

We randomly split these 39,255 files into two portions: 40% as
the prompt learning dataset and 60% as the type inference dataset.
We further randomly split the prompt-learning data into four equal
portions. We use x-pl (x=1, 2, 3, or 4) to denote the number of these
four portions used for prompt learning. Note that each prompt
tuning involves data from six libraries. We use the FQN annotation
method described in Section 3.2.1 to annotate these source code files
for prompt learning and as ground-truth to evaluate type inference
results. From the prompt learning methods, we obtain 361,048 FQN
prompts which involve 133,739 distinct FQNs. At the inference time,
we use as input the code prompt to simulate partial code. As the
input code does not include import and field declarations, the types
used in the method become non-FQN types, and the fields used
become undeclared receiving objects. Furthermore, we randomly
remove 0-2 variable declarations to create more undeclared receiv-
ing objects. From the type inference methods, we obtain 541,020
code prompts which involve 183,298 distinct FQNs to be inferred.
71.34%, 57.18%, 46.82% and 38.78% of these to-be-inferred FQNs
do not appear in the 1-pl, 2-pl, 3-pl, 4-pl prompt learning methods,
respectively. None of the code blocks used for prompt learning and
type inference are identical. Here, we generate the code prompts by
the leave-one-out setting (see section 3.3), in order to understand
the performance upper bound of our approach.

4.1.3 Metrics. We use accuracy and BLEU score [51] to measure
the performance of our approach. The accuracy is the percentage
of correctly inferred FQNs. Unlike existing methods [1, 11], we do
not use recall and F1-score, because existing methods may fail to
make the inference due to API or context OOV failures, while our
approach can always make an inference. As such, accuracy, recall
and F1-score are the same for our approach. The BLEU score [51]
is commonly used for evaluating text generation tasks. It is based
on n-gram matches between the generated text and the reference
text. Here, we calculate the BLEU score to compare the similarity
between the predicted FQN and the ground-truth FQN. The BLEU
score is 0.00 -1.00, and the higher the better. As the shortest FQNs
to be inferred contain only three tokens, we compute BLEU-2 (i.e.,
2-gram). We compute BLEU on the subword tokens so that we can
understand the matching at a finer-grained level.

4.1.4 RQ1: Intrinsic Factors. This RQ includes the experiments on
prompt-learning data size and FQN mask strategy respectively.

Prompt-Learning Data Size. We use four sizes of prompt-learning
data (1-pl, 2-pl, 3-pl, 4-pl) to fine-tune the MLM and test it on the
same type inference dataset. Here we use full span mask strategy.
We refer to the prompt-tuned MLM as 𝑀𝐹 − 𝑥 for the x-pl portion.
In addition, we use a zero-shot setting to contrast the effectiveness
of prompt learning. In the zero-shot setting, the vanilla code MLM
is used directly to infer the FQNs in the test data.

Result and analysis. Table 1 shows the results. The vanilla code
MLM (without prompt learning) achieves an average accuracy 0.31

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

Libraries

Android
Gwt
Hibernate
JDK
Joda time
Xstream
Average score

Table 1: The Effects of Prompt Learning and Full Span Masking Strategy
𝑀𝐹 − 4
𝑀𝐹 − 2

𝑀𝐹 − 3

𝑀𝐹 − 1

Zero-shot

𝑀𝑅 − 4

Acc
0.355
0.254
0.215
0.341
0.402
0.274
0.307

BLEU-2
0.523
0.403
0.341
0.505
0.633
0.465
0.478

Acc
0.822
0.866
0.866
0.787
0.872
0.863
0.846

BLEU-2
0.892
0.952
0.956
0.906
0.960
0.939
0.934

Acc
0.829
0.870
0.885
0.808
0.894
0.839
0.854

BLEU-2
0.897
0.960
0.965
0.909
0.965
0.939
0.939

Acc
0.849
0.902
0.918
0.843
0.911
0.904
0.888

BLEU-2
0.902
0.965
0.975
0.923
0.968
0.958
0.949

Acc
0.859
0.915
0.923
0.858
0.911
0.900
0.894

BLEU-2
0.911
0.970
0.977
0.926
0.970
0.954
0.951

Acc
0.415
0.292
0.232
0.387
0.413
0.430
0.362

BLEU-2
0.527
0.386
0.342
0.502
0.607
0.555
0.486

and the BLEU score 0.48 in the zero-shot setting. This shows that
even the vanilla code MLM captures some useful FQN information,
the information is not sufficient to accurately predict the FQNs. We
find that many correct predictions are short and frequently used
FQNs, such as java.io.File. But the vanilla MLM generally fails for
relatively complex FQNs. For example, it predicts the wrong FQN
.animationation.Animator.ObjectAnimator for ObjectAnimator, not
the correct android.animation.Animator.ObjectAnimator.

When we use 10% library source code files for prompt learning
(i.e., 𝑀𝐹 − 1), the accuracy rises sharply from 0.31 to 0.85, with
an increase of 0.54. It shows that prompt learning is effective to
stimulate the vanilla code MLM to learn FQN syntactic and usage
patterns, even with a small portion of library code. For example, 𝑀𝐹 -
1 correctly predicts android.animation.Animator.ObjectAnimator
for ObjectAnimator. In addition, the average BLEU-2 is also much
higher (i.e., 0.93 vs. 0.48). This shows that the predicted FQN is
very similar to the ground-truth FQN even they are not exactly
the same. With additional tool support (e.g., search engine), one
may find the correct FQNs from the almost correct predictions. For
example, searching android.raphics.Canvas.clipRect() (missing “g”
for graphics) on Google will find at the top-1 rank the correct API
name. As 71.34% FQNs do not appear in the 1-pl prompt learning
data, our results show 𝑀𝐹 -1 obtains excellent zero-shot inference
capabilty at the FQN level after prompt learning.

With the gradual increase of prompt learning data, the average
accuracy and BELU-2 score increases more incrementally, compared
with the sharp increase from the zero-shot setting to 𝑀𝐹 − 1. The
model performance becomes stable at 𝑀𝐹 -3 (average accuracy 88.8
and average BLEU-2 score 0.95), with only marginal increase at 𝑀𝐹 -
4 (average accuracy 0.89 and average BLEU-2 0.95.). This suggests
that prompt learning or not is much more fundamental to upgrade
the vanilla code MLM than the amount of prompt learning data.

Compared with the other four libraries, JDK and Android achieve
relatively low accuracy and BLEU-2 score. This is because JDK and
Android are software development kits which are essentially col-
lections of many libraries. These SDK libraries have variants FQNs.
Especially for JDK, its FQNs can vary greatly, such as javax.xml,
org.w3c.dom, org.omg.CORBA, org.xml.sax, etc. This increases the
difficulty in accurately inferring the diverse forms of FQNs in JDK,
especially when prompt learning data is small (e.g., 𝑀𝐹 − 1).

Masking Strategy. We use the four sizes of prompt-learning data
to fine-tune the MLM but we use random masking strategy. We
refer to the prompt-tuned MLM as 𝑀𝑅 − 𝑥 for the x-pl data portion,
which are tested on the same inference dataset as 𝑀𝐹 − 𝑥.

Result and analysis Table 1 shows the results of 𝑀𝑅 − 4. Due
to the space limitation, we omit 𝑀𝑅 − 1, 𝑀𝑅 − 2 and 𝑀𝑅 − 3 as 𝑀𝑅 − 4
achieves the best results. Unlike the sharp increase by 𝑀𝐹 −𝑥, 𝑀𝑅 −4
achieves only slightly better accuracy than the zero-shot setting
(0.36 versus 0.31), and 𝑀𝑅 − 4 and the zero-shot setting achieve
the close BLEU-2 scores (0.49 vs. 0.48). This suggests that prompt
learning requires an appropriate masking strategy to achieve its
effectiveness. As the commonly used random masking strategy
does not fit with FQN data characteristics and type inference task,
it performs poorly. In contrast, full span mask specially designed
for FQN inference performs significantly better.

Prompt learning with full span masking produces an effective code
MLM for solving type inference as a fill-in-blank language task,
which significantly outperforms the vanilla pre-trained code MLM.
The amount of prompt learning data does not significantly affect
the model performance, but the masking strategy does if it does not
fit with the data characteristics and task nature.

4.1.5 RQ2: Extrinsic Factors. This RQ includes the experiments on
prompt similarity and API cardinality. In this RQ, we use the model
𝑀𝐹 − 3 on the inference dataset. We choose 𝑀𝐹 − 3 because the
model performance becomes stable at 𝑀𝐹 -3. Furthermore, 𝑀𝐹 -3
is tuned with 30% of source code files which give us more FQN
prompts to obtain reliable results for prompt similarity analysis.

Prompt Similarity. Our FQN prompts and code prompts are to-
ken sequences. We remove mask tokens from the token sequences
and convert them into bags of tokens. We measure the similarity
between a FQN prompt 𝑃𝐹𝑄𝑁 and a code prompt 𝑃𝑐 as |𝑃𝐹𝑄𝑁 ∩
𝑃𝑐 |/|𝑃𝑐 |. This token similarity measures how many FQN prompt
tokens appears in the code prompt. We compute token overlapping
because names in code are meaningful for type inference. Given a
code prompt 𝑃𝑐 from the inference dataset, we compute its highest
similarity with the FQN prompts in the prompt learning dataset.
We bin 𝑃𝑐 by similarity ranges and compute the type inference
accuracy and BLEU-2 for these ranges.

Result and analysis. Table 2 shows the results. The similarities
between FQN prompts and code prompts fall mainly in between 0.15
and 0.45, accounting for 72.5% of code prompts. 4.6% code prompts
have (0, 0.15] similarities and 23.1% code prompts have above 0.45
similarities with FQN prompts. The model performance becomes
stable as long as the code prompts have 0.35 similarity with FQN
prompts. Higher similarity increases the inference accuracy slightly
(from 0.92 accuracy at (0.35, 0.45] to 0.92 accuracy at (0.65, 0.88]).
Even when code prompts have low similarity with FQN prompts
(0.15 or below), the model still performs reasonably well (accuracy

Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Table 2: Performance in Different Prompt Similarity Ranges

Similarity Range (%) Code Prompt (%) Accuracy BLEU-2
4.6
21.8
25.1
25.6
14.6
6.3
2.2

(0, 15]
(15, 25]
(25, 35]
(35, 45]
(45, 55]
(55, 65]
(65, 88]

0.755
0.842
0.891
0.919
0.920
0.918
0.924

0.899
0.940
0.957
0.961
0.945
0.937
0.956

Table 3: Performance for Different API Cardinalities

Cardinality Code Prompt (%) Accuracy BLEU-2

1
2
3
4
5
(5, 10]
(10, 20]
(20, 50]
(50, 100]
(100, 500]
(500, 1000]
(1000, +∞)

34.7
10.6
4.6
3.9
10.6
12.1
5.4
6.5
2.9
6.4
1.7
1.7

0.925
0.910
0.903
0.900
0.897
0.893
0.915
0.901
0.892
0.858
0.842
0.826

0.972
0.961
0.957
0.946
0.955
0.950
0.936
0.910
0.959
0.931
0.902
0.874

0.76 and BLEU 0.90). When the prompt similarity is >0.15, BLEU-2
scores fluctuate slightly around 0.94.

API Cardinality. When multiple APIs have the same simple name
but different FQNs, the model must distinguish such APIs during
type inference. We bin the APIs by the number of APIs having
the same simple name in different ranges, and compute the type
inference accuracy and BLEU-2 for APIs in these ranges.

Result and analysis. Table 3 shows the results. Only 34.7%
of the APIs in the inference dataset have unique simple name
(i.e., cardinality=1). Our model does not achieve 100% accuracy
for these unique-simple-name APIs because it sometimes gener-
ates inaccurate tokens. For example, for the simple name BitString,
the correct FQN is org.apache.harmony.security.asn1.BitString. Our
model predicts an almost correct FQN org.apache.harm..security-
.asn1.BitString with just one wrong token (e.g., “harm.”).

For the 65.3% APIs with cardinality > 1, our model has very sta-
ble performance (accuracy 0.89-0.92 and BLEU-2 0.95-0.97) for the
API cardinality up to 100. The small fluctuation indicates that API
cardinalities have only minor impact on model prediction. For the
extremely high API cardinality (500 and above), the mode perfor-
mance degrades but is still acceptable (accuracy 0.82-0.84 and BLEU
above 0.87-0.90). Our inspection shows that many APIs with such
extremely high API cardinalities are the Java APIs (e.g., toString(),
equals()) or get/set methods (e.g., get()). Such APIs can appear in
any classes, which makes it challenging to determine which specific
class they come from based on only local context.

The prompt-tuned code MLM performs accurately and stably for
a wide range of prompt similarities and API cardinalities, and
performs reasonably well even for very low prompt similarities
(<0.15) or very high API cardinalities (>100). This indicates that
the model does not simply memorize what it sees during prompt
learning, but acquire good inference capability for dealing with
unseen FQNs, variant code contexts, and API ambiguity.

4.2 Practicality Evaluation
4.2.1 Motivation. We investigates the practicality of our approach
from three aspects. First, we want to confirm how well our neural
type inference approach performs on real partial code using certain
library APIs, compared with existing dictionary-lookup based meth-
ods [1, 11, 33] Second, in NLP studies [17, 52], prompt-tuned models
have shown strong inference capability for new tasks unseen dur-
ing prompt learning. We want to investigate if prompt-tuned code
MLM has similar capability for APIs and code contexts unseen
during FQN prompt learning. Third, we want to summarize failure
patterns and plausible causes of our current approach to shed the
light on improving neural type inference methods.

4.2.2 Dataset and Metrics. In this study, we use two datasets of par-
tial code collected from Stack Overflow. The first dataset StatType-
SO [33] has been used in the experiments of previous type inference
work [1, 11, 33]. This dataset contains 268 partial code snippets.
Each code snippet is primarily about the use of APIs of one of
the six libraries, Android, GWT, Hibernate, Joda Time, Xstream
or JDK. But many code snippets concerning the APIs of the first
five libraries also use JDK APIs. These code snippets contain 6-223
lines of code, with an average 28. They involve 1838 and 2248 type
inference points for non-FQN types and undeclared receiving ob-
jects, respectively, and include 1454 unique types to be inferred.
Furthermore, we build a new partial code dataset Short-SO from
the Stack Overflow posts about the six libraries. Furthermore, we
build a new partial code dataset Short-SO from the Stack Overflow
posts based on two criteria: 1) each code snippet must be related to
some of the six libraries, and 2) contain no more than three lines
of code. We collect 20 code snippets for each library, i.e., 120 in
total. We use Short-SO to evaluate the capability of our model in
face of little context information. The code snippets in Short-SO
involves 269 and 256 type inference points for non-FQN types and
undeclared receiving objects, respectively, and include 286 unique
types to be inferred.

We need to resolve the FQNs for non-FQN types and undeclared
receiving objects in Short-SO for evaluation. Based on the informa-
tion in the posts (e.g., mentioned APIs, API links), two annotators
(both are computer science graduate students) tried to make the
code snippets in Short-SO compilable by manually fixing unre-
solved symbols in the code snippets. One annotator performed the
fixing and the other annotator validated the fixed code. They dis-
cussed to resolve the disagreements. We use accuracy (i.e., the strict
ground-truth match) to measure the performance of our approach.
In all the experiments, we generate the code prompts with the all-
unknown setting (see Section 3.3), in order to understand the lower
bound performance of our approach in the worst-case scenario in
the real world. In practice, developers would likely know the FQNs
of some simple names in a code snippet, which very likely helps

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

Table 4: Accuracy on StatType-SO and Short-SO

Libraries

Android
GWT
Hibernate
JDK
Joda Time
Xstream
Average accuracy

StatType-SO

COSTER
0.433
0.908
0.904
0.562
0.571
0.884
0.710

SnR 𝑀𝐹 −3
0.906
0.936
0.952
0.758
0.759
0.948
0.989
0.711
0.975
0.895
0.880
100.0
0.910
0.875

Short-SO
𝑀𝐹 −3
0.819
0.882
0.765
0.973
0.971
0.913
0.887

to improve the prediction performance. And we use 𝑀𝐹 -3 as our
model because the model performance becomes stable at 𝑀𝐹 -3 (see
Section 4.1.4).

4.2.3 RQ3: Performance on Partial Code from Stack Overflow. We
run 𝑀𝐹 -3 on both StatType-SO and Short-SO dataset. We compare
our approach with the state-of-the-art type inference tools for Java
(i.e., COSTER [1] and SnR [11]) as our baselines.

Result and analysis. Table 4 shows the results. For StatType-
SO, we compare the accuracy of our approach with the accuracy
reported in [11]. On average, our model 𝑀𝐹 -3 performs better than
both SnR and COSTER on StatType-SO (0.91 versus 0.87 and 0.71).
𝑀𝐹 -3 achieves much higher accuracy on GWT, JDK and Joda Time
than both COSTER and SnR. On Android, 𝑀𝐹 -3 is close to SnR
(0.91 versus 0.94), and is much better than COSTER (0.91 versus
0.43). Our 𝑀𝐹 -3 performs worse than COSTER and SnR on Hiber-
nate and achieves only accuracy 0.76. However, many errors are
caused by the ground truth FQNs using the old package name (e.g.,
javax.persistence) while the predicted FQNs using the new package
name (e.g., jakarta.persistence). For the Xstream, our 𝑀𝐹 -3 achieves
the same performance as COSTER but is worse than SnR (0.88 vs.
1.00). Most errors in the Xstream are caused by the FQNs in the
package com.cloudbees.api.config. Accidentally, none of the types
in this package have been used in prompt learning, which means
the model has zero knowledge of this package and its types. We
further analyze this failure in Section 4.2.5.

𝑀𝐹 -3 achieves 0.99 accuracy on JDK partial code in StatType-
SO and 0.97 accuracy on JDK partial code in Short-SO. This is
much higher than its performance on the JDK source code (0.84
accuracy). This is because our effectiveness evaluation on JDK
source code involve all kinds of JDK APIs with diverse forms of
FQNs. In contrast, the JDK APIs used in the partial code from Stack
Overflow are mostly APIs from common packages (e.g., java.util,
java.io) which are much easier to predict.

𝑀𝐹 -3 achieves high accuracy 0.89 on Short-SO. Even though the
code snippets in Short-SO is much shorter than those in StatType-
SO, the performance of our 𝑀𝐹 -3 does not differ much on the two
datasets (0.91 versus 0.89). For the four libraries Hibernate, JDK,
Joda Time and Xstream, 𝑀𝐹 -3 achieves very close accuracy on
StatType-SO and Short-SO. 𝑀𝐹 -3 has relative worse accuracy for
Android and GWT in Short-SO than StatType-SO. The main errors
come from the wrong predictions for high cardinality methods,
for example, getString(), add(). In fact, our model relies on only
limited local context (up to 5 code lines) for making inference. Thus,
the length of a code snippet does not matter. Furthermore, our
model makes inference over a neural knowledge base which is

Figure 5: Example for Seen-FQN-Unseen-context and
Unseen-FQN-Unseen-Context

Table 5: Accuracy for Seen/Unseen APIs and Code Contexts

Code-Context

API

Seen Unseen Overall

Seen
Unseen
Overall

0.983
0.976
0.980

0.753
0.671
0.712

0.922
0.874
0.902

more powerful and flexible than the heuristic name and context
matching over a symbolic knowledge base used in existing methods.
Saifullah et al. [1] declared that “the COSTER tool finds very few
to no context in code snippets with only 1 or 2 lines of code and
thus would fail to resolve FQNs of API elements”.

4.2.4 RQ4: Prediction Capability for Unseen APIs and Code Con-
texts. An API in the SO datasets is seen if its FQN appears in the
prompt learning data. Otherwise, the API in the SO datasets is un-
seen. We have 3024 seen APIs and 1371 unseen APIs in the two
SO datasets whose FQNs need to be predicted. Otherwise, the code
prompt is unseen. We compute the prompt similarity as describe
in Section 4.1.5. We use the threshold 0.35 based on the impact
analysis of prompt similarity in Table 2. We have 2626 seen code
prompts and 1985 unseen code prompts. We obtain four combina-
tions of seen/unseen API and seen/unseen context, and compute
the inference accuracy by 𝑀𝐹 -3 for each combination on the two
SO datasets.

Result and analysis. Table 5 shows the results of the four com-
binations. 𝑀𝐹 -3 achieves high accuracy 0.98 for Seen-FQN no matter
code context is seen or unseen. When the FQNs to be predicted are
seen during prompt learning, the accuracy in unseen context is only
marginally lower than that in seen context. When an FQN is seen
during prompt learning, inferring it in a code snippet can be thought
of as a dictionary lookup in our neural knowledge base. However,
compared with the dictionary lookup in a symbolic knowledge
base [1, 11, 33], our neural model can better handle context vari-
ations. For example, the code block (b) in Figure. 5 for inferring
“BufferedImage” is an unseen code context. Even the most similar
code block (a) used in prompt learning is not that similar to the
code block (b). However, the model manages to pick up the most
important code tokens (e.g., those with underlines) and correctly
infer the FQN of BufferedImage. Note that our model does not use

java.awt.image.BufferedImage img = new java.awt.image.BufferedImage(w, h,                                             java.awt.image.BufferedImage.TYPE_INT_ARGB);  java.awt.Graphics2D gfx = java.awt.image.BufferedImage.createGraphics();  BufferedImage bufImage = new BufferedImage(prefWidth, prefHeight,                                                                      BufferedImage.TYPE_INT_RGB); container.paint(bufImage.createGraphics());DefaultTableModel model = new DefaultTableModel(data, header); JTable table = new JTable(model); javax.swing.JTable.DefaultTableCellRenderer tcr = new                                               javax.swing.JTable.DefaultTableCellRenderer();  re=javax.swing.table.DefaultTableCellRenderer.getTableCellRendererComponent();   Seen-FQN-Unseen-ContextUnseen-FQN-Unseen-Context(a)code blockfor prompt-learning (b)code block for infer BufferedImage type(d)code block for infer DefaultTableModel type(c)code block for prompt-learning Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Table 6: Frequent Wrong FQN Predictions

Library

Joda Time

Top-1
Package
Name (PN)
java.sql

JDK

javax.swing

Android

Xstream

GWT

Hibernate

android.os
com.cloudbees.
api.config
com.extjs.gxt.
ui. client.widget
javax.persistence

#DistGTFQNs
(𝐶𝑃𝐾 /𝐶𝐿𝑖𝑏𝑟𝑎𝑟 𝑦)

1 (6/13)

1 (3/9)

1 (26/91)

2 (17/81)

3 (11/50)

17 (111/216)

The Ground
Truth FQNs
to be Inferred
PN.Date
PN.Swing-
Worker
PN.Bundle
PN.Enviro-
nment, etc.
PN.Vertical-
Panel, etc.
PN.Id, etc.

𝐶𝑃𝐾 and 𝐶𝐿𝑖𝑏𝑟𝑎𝑟 𝑦 : the number of wrong predictions in a package and a library.

any explicit AST and data flow information, but is purely based on
neural embedding of code tokens.

When the FQNs to be predicted are unseen during prompt learn-
ing, 𝑀𝐹 -3 still achieves 0.75 accuracy and 0.67 accuracy for Unseen-
FQN-Seen-Context and Unseen-FQN-Unseen-Context, respectively.
This zero-shot FQN prediction capability is impossible for existing
methods, as unseen FQNs or code contexts will simply result in
the OOV failures when looking up the symbolic knowledge base.
Our model’s capability for unseen APIs and unseen contexts comes
from the code naturalness that the model captures from large pre-
training code corpus and subsequent prompt learning. By code nat-
uralness, developers name code elements with meaningful names
and package related types in the same name space. Figure 5(c) and
(d) shows an example of Unseen-FQN-Unseen-Context. The API
javax.swing.JTable.DefaultTableModel to be inferred is unseen in
prompt learning, but the other API javax.swing.JTable.DefaultTable-
CellRender is seen in prompt learning. Although the code block (d)
for inference is not very similar to the code block (c) for prompt
learning, the model could still see that both of them are somehow
about JTable and DefaultTable, and make an intelligent guess of the
FQN of DefaultTableModel. This guess relies on not only the tokens
visible in the prompts, but also the broader knowledge related to
JTable and DefaultTable learned from the large code corpus.

4.2.5 RQ5: Failure analysis. We analyze common failure cases in
the 𝑀𝐹 -3’s prediction results. We count the number of errors 𝑀𝐹 -3
makes for the FQNs in each package. We analyze the top ranked
packages with the most errors for each library.For each such pack-
age, we collect distinct ground-truth FQNs with wrong predictions
and count the times of wrong predictions for them.

Result and analysis. Table 6 shows the results. Due to space
limitation, we show only the top-1 package with the most wrong
predictions (top-3 results in replication package). Take Joda Time
as an example. The top-1 package is java.sql which has one ground-
truth FQN (#DistGTFQN) (i.e., java.sql.Date) been wrongly pre-
dicted as java.util.Date for 6 times (𝐶𝑃𝐾 ). These 6 wrong predic-
tions accounts for 46.2% of all errors (𝐶𝐿𝑖𝑏𝑟𝑎𝑟 𝑦=13) for Joda Time.
Overall, the number of the distinct ground-truth FQNs with wrong
predictions in a package is (much) smaller than the total number of
errors in the package. This mean there are many repeated errors
for the same FQN to be inferred. Our analysis identifies three major
causes.

First, the errors are caused by similar usage contexts between
the to-be-inferred FQN and the inferred FQN and the two FQNs are
similar at the token level. For example, 𝑀𝐹 -3 repeatedly predicts
java.util.Date for Date in the Joda Time code, but the ground truth
is java.sql.Date. In fact, java.sql.Date is the subclass of java.util.Date,
and the two classes have similar usage context. We find that if the
code context for Date contains the SQL-related information (e.g.,
SQLException), 𝑀𝐹 -3 can correctly predict java.sql.Date. Otherwise,
it prefers java.util.Date which is applicable in generic contexts.

Second, the package name of the to-be-inferred FQNs does not
appear in the prompt learning data. That is, none of the types in this
package is used for prompt learning. We refer to this situation as
zero-shot at package level, which is a very challenging task for the
model. For example, 𝑀𝐹 -3 makes 17 wrong inferences for the two
FQNs in the package com.cloudbees.api.config of Xsteam, accouting
for 21% of all errors for Xsteam. All these 17 errors are zero-shot
failures. The same situation for the 11 wrong inferences for the
three FQNs in the package com.extjs.gxt.ui.client.widge of GWT,
accounting for 22% of all errors for GWT. These errors could be
reduced by using some types in the package to prompt the model.
Third, the inferred FQNs are actually correct but are from a
different library version from that of the to-be-inferred FQNs. That
is, the inferred FQNs are false negatives. In Hibernate, 111 errors
(54% of all errors for Hibernate and 24% for all six libraries) are
for the 17 to-be-inferred FQNs in javax.persistence. We obtain the
ground truth FQNs in the Stack Overflow code snippets based
primarily on the information in the post. However, some of the
FQNs in the code snippet can be replaced by the newer versions of
the FQNs. For example, the code snippet2 is selected for Hibernate.
We obtain the package name javax.persistence of the API types (e.g.,
ManyToOne, Column). However, 𝑀𝐹 -3 predicts jakarta.persistence
for these API types. In fact, they are the same APIs in different
version of, i.e., javax.persistence was renamed to jakarta.persistence
in JPA 3.0. As we use the JPA 3.1.0 to prompt-tune the model, the
model makes inference by the newer version of APIs.

Our model achieves higher type inference accuracy than the state-
of-the-art dictionary-lookup methods [1, 11, 33], and it achieves
unprecedented results for short code snippets and unseen FQNs
and code contexts. Our model may make mistakes for APIs with
similar functionalities and in zero-shot package setting. 24% wrong
inferences are false negatives as the inferred FQNs and the ground-
truth FQNs are the same APIs from different library versions.

5 DISCUSSION
This section summarizes our experience, envision future directions,
and discuss threats to validity.
5.1 Model Enhancements and Extensions
Neural type inference is a completely new approach. From our
experience in this study, we highlight research points that are worth
further investigation for enhancing neural type inference methods.

5.1.1 Pre-training Code MLM. We currently use the pre-trained
CodeBert [12]. This allows us to study the prompt learning method
for type inference on top of a well studied, mature code MLM.
CodeBert has only 1.25 millions parameters, which is much smaller

2https://stackoverflow.com/questions/3325387

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

than the latest pre-trained natural language model and code model,
for example, GPT3[17] and its code variant CoPilot [53] which have
175 billions parameters. We are investigating if larger pre-trained
model may lead to better performance. As code MLM is pre-trained
on raw source code, the only technical limitation to train larger
code MLM is computing resources. An important hyperparameter
of MLM is the context window size. Our model uses 512 tokens. This
fits well with short partial code and reduces irrelevant contextual
noise, but it limits the number of contextual code lines in each code
prompt. Through the design of FQN prompts with overlapping code
lines, our model can propagate long-range contextual information.
The state-of-the-art MLM uses much larger context window (e.g.,
2048) [54]. It is worth investigating if larger context window (i.e.,
more contextual code lines in a code prompt) would allow the
model to better capture long-range contextual information or it
may introduce more noise which may interfering the inference.

5.1.2 Prompt Learning. Due to random sampling, we accidentally
encounter the cases in which none of the types in a package are sam-
pled for prompt learning (e.g., com.cloudbees.api.config in Xstream).
As the model has zero knowledge of this package, it fails to infer the
FQNs of the types in this package. This result suggests our model
have good few-short learning capability, but zero-shot at package
level is still very challenging to it. As such, a better prompt learn-
ing strategy should expose the model to as many package prefixes
as possible and let the model see some FQNs from each package
which would maximize the model performance. The FQN prompts
in this work mark only the FQNs in the focused code line but keep
the contextual code lines unchanged. We were wondering if we
could make the learning more challenging by masking some FQNs
and/or other tokens in the contextual code lines. We hypothesize
this could force the model to better learn the correlations between
the focused code and the contextual code. This leads to another
interesting question: how many contextual tokens should we mark?
The original BERT marks 15% of tokens. Later studies [55] show
higher mark ratio (e.g., 75%) may results in better results. We need
to identify a good trade-off point at which there are sufficient masks
to challenge the model to learn harder, but masks are not too many
so that no sufficient information for the model to learn.

5.1.3 Type Inference. We make type inference at each inference
point independently. The benefit is that it removes the reliance on
code analysis. However, type inference points may be correlated
(e.g., the same type of different variables, the same variable used at
different places). When the correlated places are inferred simulta-
neously, the generated code prompt will contain some FQNs, which
helps the model subsequently infer other FQNs. Such code prompt
can be considered to lie in between the leave-out-one setting and
the all-unknown setting in our experiments. It is worth investigat-
ing how this type of code prompts affects the performance of the
model on type inference. However, this would require the design
of corresponding FQN prompt and masking strategy. A concern is
simultaneous multiple type inference would increase the reliance
on code analysis. In addition, in the effectiveness and practicality
evaluation, we generated the code prompts of the upper bound on
Github datasets, and the ones of the lower bound on Short-SO. In
the future, we will generate the code prompts of the lower bound on
the GitHub datasets, to investigate the performance of the model.

Our model generates a small number of incorrect FQN formats
(e.g., two consecutive “.”). Such format errors can be avoided by
post-processing the language model type inference results with
code syntax [56]. More interestingly, this inspire us whether rein-
forcement learning (RL) could be useful in neural type inference
methods. We could encode some FQN format and syntactic rules as
the RL rewards and let RL guide the MLM to avoid incorrect FQN
formats or other FQN-related syntactic errors.

5.1.4 Type Inference for Other Programming Languages (PLs). In
this work, we focus on statically typed PL and evaluate our approach
on Java. As our approach has little reliance on code compilation,
there are no major barriers to extend our model to other statically
typed code, such as C#. In fact, CodeBert is trained with a code
corpus mixing six programming languages. Many PLs use the same
FQN form (a.b.c), while some PLs use other forms (e.g., a::b::c).
Furthermore, different PLs may have different naming conventions.
An interesting question is whether a PL-hybrid type inference
model can provide an one-for-all solution, or we need PL-specific
model because different PLs may interfere with each other. More
challengingly, how can neural type inference methods be applied
to dynamically-typed PLs? From our experience, we first need to
expose the MLM to as much code information as possible during
pre-training. Then, we need to prepare some FQN-annotated code
examples that have a good coverage of typical code use for prompt
learning. As prompt learning has few shot learning capability, this
preparation step (even manually) could still be feasible.
5.2 Downstream Applications
Making effective use of online code examples has been a long-
term interest in software engineering [57–62]. Resolving types
in partial code is the first step towards this goal. Different from
existing type inference methods [1, 11, 33], our approach uses a
much more lightweight method to construct an easily-extensible
neural knowledge base of APIs and usage contexts. This opens the
door to many useful downstream applications. For example, we
could use our model to parse code examples in API documentation
to enrich the code examples in software knowledge graph [63–
65] with API type information. As another example, existing code
search is still mainly based on keyword matching, because advanced
code search methods [66–70] all more or less rely on code analysis
which makes them hard to deploy. Our model could lower the
barrier to deploy these advanced code search engine by annotating
large code base with API types to support API-centric code search.
Last but not least, our model could be integrated in Stack Overflow
or IDE as a neural compiler, which can provide just-in-time type
fixing assistance when developers read code snippets on Stack
Overflow or copy code snippets to the IDE from the Web. For
example, IntelliJ IDE marks “util” in “util.List” as an unresolved
symbol but cannot offer any fixing suggestion. Our model can easily
fix this unresolved symbol as “java.util.List”.
5.3 Threats to Validity
As a deep learning based method, data errors is a major threat. We
validate our data processing modules, and examine many inputs,
intermediate results and outputs to ensure the data correctness. The
FQNs in the StatType-SO dataset come from previous studies [33],
and we double-checked their validity. For our own Short-SO dataset,

Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

two authors labeled the ground-truth FQNs. We also test our tool
chain to ensure its correctness. Note that our approach treats source
code as text and use mature NLP tools (e.g., WordPiece [46] for
tokenization). Our approach does not require any sophisticated
program analysis, except for obtaining FQNs from library source
code using mature Java parser [71]. The major external threat is
that our approach was evaluated on only six Java libraries. We
use these six libraries because they were used in all recent type
inference studies [1, 11, 33]. The six libraries cover two major SDKs
(JDK and Android) and other specific application domains including
XML parsing (Xstream), web tookit (GWT), date and time (Joda
Time), database (Hibernate). These SDKs and libraries have differ-
ent naming conventions and FQN characteristics. We will apply
our approach to more Java libraries to further confirm its generaliz-
ability. We will also extend our approach to other statically-typed
program languages (e.g., C#, Rust, Swift). As our approach treats
code as text, the extension should be straightforward.

6 RELATED WORK
Recently, the great success of large pre-trained language models
(PLM) (e.g., BERT [72, 73], RoBERTa [74], GPT-3 [17], T5 [18]) in a
variety of NLP tasks, spawns a surge of code PLMs (e.g., CuBert [13],
CodeBERT [12], CodeT5 [75], CoPilot [53]). The application of
language models on code is built on source code naturalness [14,
15]. As the pre-training tasks (e.g., mask prediction) are agnostic
of downstream tasks, studies [76] show the vanilla output (e.g,
MLM head) of code PLMs does not encode much code syntax and
semantics. Through external probing, recent works [41, 42, 77]
show code PLM contain information about code syntax, the notions
of identifiers and namespaces, and natural language naming.

To make use of the rich code information in code PLMs in down-
stream software engineering tasks, all existing work follows a “pre-
train, finetune” paradigm which takes the PLM as a feature extractor
and connect it to a downstream neural network (e.g., a Multi-Layer
Perceptron [37]). Then, the PLM, together with this neural network,
can be trained on a small and specialized data for downstream
tasks (e.g., code search [39], AST tagging [78], vulnerability predic-
tion [79], clone detection [44], code summarization [80]) [40, 42, 81].
In this paradigm, the PLM parameters can be fixed or fine-tuned
(usually the last few layers [82]), and the task-specific learning
occurs mainly in the downstream heterogeneous neural network.
We refer to this paradigm as outsourcing upgrade. In contrast, we
adopt a completely new paradigm “pre-train, prompt and predict”
which is a self-upgrade of pre-trained code MLM, in the sense that
model pre-training and prompt learning are homogeneous at the
same MLM core. Due to this alignment, prompt learning efficiently
stimulates task-agnostic pre-trained code MLM to recognize FQN
syntax and usage with some FQN prompts from library code.

Studies [17, 18, 34, 35] show the meta-learning capability of
large PMLs: the model conditioned on a task description and some
examples of the task can learn to complete new instances of the task.
Early methods focused on hand-crafted prompts [18, 34, 35, 83],
and the effects are sensitive to prompt variants [84]. Subsequent
work proposes to automate the search of prompts, including both
discrete prompts [84, 85] and continuous prompts [86–88]. Natural
language prompts cannot be directly applied in our task, because
they are uniform for all inputs. In light of the variable-length form

of FQNs and variant API usage context, we design automatic and
contextual FQN prompts which differ conceptually from existing
NLP prompt designs. Furthermore, our FQN prompts use full span
masks, instead of random masking commonly used in NLP tasks.
Several NLP studies [24, 89–91] show that PLMs can serve as neu-
ral knowledge bases of real-word entities and relations. Compared
with symbolic knowledge bases, neural knowledge bases do not
require schema engineering and human annotations, and support
an open set of queries. Our prompt-tuned code MLM is a neural
knowledge base that packs API names and code contexts in large
code corpus into model parameters. This is completely different
from the symbolic knowledge bases that explicitly maps API names
and code contexts in existing type inference methods [1, 11, 33],
require to compile and parse a large number of software projects
using libraries APIs. Furthermore, we formulate type inference
as a fill-in-blank language task, which aligns perfectly with the
learning objective of prompt-tuned code MLM. As such, our type
inference retrieves the FQN knowledge from the model through
neuron activation, as opposed to symbolic name and code-context
matching. This neural type inference supports fuzzy reasoning of
the correlations between API names and code contexts, which may
not be explicit in the partial code under analysis.

7 CONCLUSION AND FUTURE WORK
This paper presents the first neural type inference method built on
a novel “pre-train, prompt and predict” paradigm. Our approach
solves type inference as a fill-in-blank language task using a prompt-
tuned code MLM, thus removing the reliance on compiling a large
number of software projects to construct symbolic API knowledge
base. Our approach perfectly aligns model pre-training, prompt
learning and inference with the same mask learning objective. We
design novel automatic and contextual FQN prompt, full-span mask
strategy for prompt learning, and variable-length mask prediction
method for FQN inference. Our experiments show prompt learning
takes effect with as minimum as 10% of library source code files,
and our model is robust in handling unseen FQNs, code context
variations and API ambiguities. Our model achieves excellent per-
formance for short partial code with little context information, and
the achieves promising capability of few-shot type inference. In the
future, we will explore ways to enhance and extend our approach
and develop novel applications for partial code search and reuse
enabled by our model’s unprecedented type inference capability.

8 ACKNOWLEDGEMENTS
The work is partly supported by the National Nature Science Foun-
dation of China under Grant (Nos. 61902162, 61862033), the Nature
Science Foundation of Jiangxi Province (20202BAB202015), Post-
graduate Innovation Fund Project of Jiangxi Province(YC2021-S308),
and the Science and technology Key project of Education Depart-
ment of Jiangxi Province (GJJ210307).

REFERENCES
[1] C. M. Khaled Saifullah, Muhammad Asaduzzaman, and Chanchal Kumar Roy.
Learning from examples to find fully qualified names of api elements in code
snippets. 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE), pages 243–254, 2019.

ASE ’22, October 10–14, 2022, Rochester, MI, USA

Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu

[2] Piyush Kumar Gupta, Nikita Mehrotra, and Rahul Purandare. Jcoffee: Using com-
piler feedback to make partial code snippets compilable. 2020 IEEE International
Conference on Software Maintenance and Evolution (ICSME), pages 810–813, 2020.
[3] Suresh Thummalapenta and Tao Xie. Parseweb: a programmer assistant for
reusing open source code on the web.
In Proceedings of the twenty-second
IEEE/ACM international conference on Automated software engineering, pages
204–213, 2007.

[4] Subhadip Maji, Swapna Sourav Rout, and Sudeep Choudhary. Dcom: A deep
column mapper for semantic data type detection. ArXiv, abs/2106.12871, 2021.
[5] Tianyi Zhang, Ganesha Upadhyaya, Anastasia Reinhardt, Hridesh Rajan, and
Miryung Kim. Are code examples on an online q&a forum reliable?: A study of
api misuse on stack overflow. 2018 IEEE/ACM 40th International Conference on
Software Engineering (ICSE), pages 886–896, 2018.

[6] Luca Piccolboni, Giuseppe Di Guglielmo, Luca P. Carloni, and Simha Sethumad-
havan. Crylogger: Detecting crypto misuses dynamically. 2021 IEEE Symposium
on Security and Privacy (SP), pages 1972–1989, 2021.

[7] Yaqin Zhou, Shangqing Liu, J. Siow, Xiaoning Du, and Yang Liu. Devign: Effective
vulnerability identification by learning comprehensive program semantics via
graph neural networks. ArXiv, abs/1909.03496, 2019.

[8] Xiaoxue Ren, Xinyuan Ye, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu,
and Jianling Sun. Api-misuse detection driven by fine-grained api-constraint
knowledge graph. 2020 35th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pages 461–472, 2020.

[9] Leandro T. C. Melo, Rodrigo G. Ribeiro, Breno C. F. Guimarães, and Fernando
Magno Quintão Pereira. Type inference for c: Applications to the static analysis
of incomplete programs. ACM Trans. Program. Lang. Syst., 2020.

[10] Siddharth Subramanian, Laura Inozemtseva, and Reid Holmes. Live api docu-
mentation. International Conference on Software Engineering (ICSE), 2014.
[11] Yiwen Dong, Tianxiao Gu, Yongqiang Tian, and Chengnian Sun. Snr: Constraint
based type inference for incomplete java code snippets. International Conference
on Software Engineering (ICSE), 2022.

[12] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-
trained model for programming and natural languages. ArXiv, abs/2002.08155,
2020.

[13] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning
and evaluating contextual embedding of source code. In International Conference
on Machine Learning, pages 5110–5121. PMLR, 2020.

[27] Anonymous. Analyzing codebert’s performance on natural language code search.

2022.

[28] Yi Sun, Yu Zheng, Chao Hao, and Hangping Qiu. Nsp-bert: A prompt-based
zero-shot learner through an original pre-training task-next sentence prediction.
ArXiv, abs/2109.03564, 2021.

[29] Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. Ptr: Prompt

tuning with rules for text classification. ArXiv, abs/2105.11259, 2021.

[30] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt

tuning for few-shot learning. ArXiv, abs/2109.04332, 2021.

[31] Ning Ding, Yulin Chen, Xu Han, Guangwei Xu, Pengjun Xie, Haitao Zheng,
Zhiyuan Liu, Juan-Zi Li, and Hong-Gee Kim. Prompt-learning for fine-grained
entity typing. ArXiv, abs/2108.10604, 2021.

[32] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-
tuning v2: Prompt tuning can be comparable to fine-tuning universally across
scales and tasks. ArXiv, abs/2110.07602, 2021.

[33] Hung Dang Phan, Hoan Anh Nguyen, Ngoc M. Tran, Linh-Huyen Truong,
Anh Tuan Nguyen, and Tien Nhut Nguyen. Statistical learning of api fully
qualified names in code snippets of online forums. 2018 IEEE/ACM 40th Interna-
tional Conference on Software Engineering (ICSE), pages 632–642, 2018.

[34] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language

models are also few-shot learners. ArXiv, abs/2009.07118, 2021.

[35] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text

classification and natural language inference. In EACL, 2021.

[36] Barthélémy Dagenais and Laurie Hendren. Enabling static analysis for partial java
programs. In Proceedings of the 23rd ACM SIGPLAN conference on Object-oriented
programming systems languages and applications, pages 313–328, 2008.

[37] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In NIPS, 2017.

[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. ArXiv,
abs/1810.04805, 2019.

[39] Hamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code
search. ArXiv, abs/1909.09436, 2019.

[40] Anjan Karmakar and Romain Robbes. What do pre-trained code models know
about code? 2021 36th IEEE/ACM International Conference on Automated Software
Engineering (ASE), pages 1332–1336, 2021.

[14] Premkumar T. Devanbu. On the naturalness of software. 2012 34th International

[41] Sergey Troshin and Nadezhda Chirkova. Probing pretrained models of source

Conference on Software Engineering (ICSE), pages 837–847, 2012.

code. ArXiv, abs/2202.08975, 2022.

[15] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR), 51:1 – 37, 2018.

[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.

[17] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165,
2020.

[18] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683,
2020.

[19] Noah Liebman, Michael Nagara, Jacek Spiewla, and Erin Zolkosky. Cuebert: A

new mixing board concept for musical theatre. In NIME, 2010.

[20] Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gordon, and Yi Wei. Bimodal

modelling of source code and natural language. In ICML, 2015.

[21] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien Nhut Nguyen. Lexical statis-
tical machine translation for language migration. In ESEC/FSE 2013, 2013.
[22] Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. On the use of
automated text summarization techniques for summarizing source code. 2010
17th Working Conference on Reverse Engineering, pages 35–44, 2010.

[23] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and

David Bieber. Global relational models of source code. In ICLR, 2020.

[24] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases?
arXiv preprint arXiv:1909.01066, 2019.

[25] Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie
Taylor. Freebase: a collaboratively created graph database for structuring human
knowledge. In SIGMOD Conference, 2008.

[26] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You
only look once: Unified, real-time object detection. 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 779–788, 2016.

[42] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hairong Jin.
What do they capture? - a structural analysis of pre-trained language models for
source code. ArXiv, abs/2202.06840, 2022.

[43] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue:
A machine learning benchmark dataset for code understanding and generation.
ArXiv, abs/2102.04664, 2021.

[44] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with
graph neural network and flow-augmented abstract syntax tree. In 2020 IEEE
27th International Conference on Software Analysis, Evolution and Reengineering
(SANER), pages 261–271. IEEE, 2020.

[45] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. An empirical study on learning bug-fixing
patches in the wild via neural machine translation. ACM Transactions on Software
Engineering and Methodology (TOSEM), 28(4):1–29, 2019.

[46] Yonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nis-
hant Patil, Wei Wang, Cliff Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol
Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine
translation. ArXiv, abs/1609.08144, 2016.

[47] Jian Gu, Pasquale Salza, and Harald C. Gall. Assemble foundation models for

automatic code summarization. 2022.

[48] Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and
Xiangke Liao. Bridging pre-trained models and downstream tasks for source
code understanding. ArXiv, abs/2112.02268, 2021.

[49] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
Realm: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909,
2020.

[50] Raja Naeem Akram and Konstantinos Markantonakis. Challenges of security
and trust of mobile devices as digital avionics component. In 2016 Integrated
Communications Navigation and Surveillance (ICNS), pages 1C4–1–1C4–11, 2016.
[51] Chin-Yew Lin and Franz Josef Och. Orange: a method for evaluating automatic

evaluation metrics for machine translation. In COLING, 2004.

Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code

ASE ’22, October 10–14, 2022, Rochester, MI, USA

[52] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting
methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.
[53] Hammond A. Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and
Ramesh Karri. An empirical cybersecurity evaluation of github copilot’s code
contributions. ArXiv, abs/2108.09293, 2021.

[54] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The
pile: An 800gb dataset of diverse text for language modeling. arXiv preprint
arXiv:2101.00027, 2020.

[55] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll’ar, and Ross B. Gir-
shick. Masked autoencoders are scalable vision learners. ArXiv, abs/2111.06377,
2021.

[56] Yanlin Wang and Hui Li. Code completion by modeling flattened abstract syntax

trees as graphs. Proceedings of AAAIConference on Artificial Intellegence, 2021.

[57] Tianyi Zhang, Di Yang, Crista Lopes, and Miryung Kim. Analyzing and support-
ing adaptation of online code examples. In 2019 IEEE/ACM 41st International
Conference on Software Engineering (ICSE), pages 316–327. IEEE, 2019.

[58] Medha Umarji, Susan Elliott Sim, and Crista Lopes. Archetypal internet-scale
source code searching. In IFIP International Conference on Open Source Systems,
pages 257–263. Springer, 2008.

[59] Rosalva Gallardo-Valencia and Susan Sim. Internet-scale code search. pages

49–52, 05 2009.

[60] Joel Brandt, Philip J Guo, Joel Lewenstein, Mira Dontcheva, and Scott R Klemmer.
Two studies of opportunistic programming: interleaving web foraging, learning,
and writing code. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pages 1589–1598, 2009.

[61] Sebastian Baltes and Stephan Diehl. Usage and attribution of stack overflow code
snippets in github projects. Empirical Software Engineering, 24(3):1259–1295,
2019.

[62] Yuhao Wu, Shaowei Wang, Cor-Paul Bezemer, and Katsuro Inoue. How do devel-
opers utilize source code from stack overflow? Empirical Software Engineering,
24(2):637–673, 2019.

[63] Hongwei Li, Sirui Li, Jiamou Sun, Zhenchang Xing, Xin Peng, Mingwei Liu,
and Xuejiao Zhao. Improving api caveats accessibility by mining api caveats
knowledge graph. 2018 IEEE International Conference on Software Maintenance
and Evolution (ICSME), pages 183–193, 2018.

[64] Jiamou Sun, Zhenchang Xing, Rui Chu, Heilai Bai, Jinshui Wang, and Xin Peng.
Know-how in programming tasks: From textual tutorials to task-oriented knowl-
edge graph. 2019 IEEE International Conference on Software Maintenance and
Evolution (ICSME), pages 257–268, 2019.

[65] Mingwei Liu, Xin Peng, Andrian Marcus, Zhenchang Xing, Wenkai Xie, Shuang-
shuang Xing, and Yang Liu. Generating query-specific class api summaries.
Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering, 2019.
[66] Erik Linstead, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and
Pierre Baldi. Sourcerer: mining and searching internet-scale software repositories.
Data Mining and Knowledge Discovery, 18(2):300–336, 2009.

[67] Kisub Kim, Dongsun Kim, Tegawendé F. Bissyandé, Eunjong Choi, Li Li, Jacques
Klein, and Yves Le Traon. Facoy – a code-to-code search engine. In 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE), pages 946–957, 2018.
[68] Qing Huang, An Qiu, Maosheng Zhong, and Yuan Wang. A code-description
representation learning model based on attention. In 2020 IEEE 27th International
Conference on Software Analysis, Evolution and Reengineering (SANER), pages
447–455, 2020.

[69] Qing Huang and Guoqing Wu. Enhance code search via reformulating queries

with evolving contexts. Automated Software Engineering, 26(4):705–732, 2019.

[70] Qing Huang and Huaiguang Wu. Qe-integrating framework based on github
knowledge and svm ranking. Science China Information Sciences, 62(5):1–16, 2019.

[71] Renaud Pawlak, Monperrus Martin, Nicolas Petitprez, Carlos Noguera, and Lionel
Seinturier. Spoon: A library for implementing analyses and transformations of
java source code. Software: Practice and Experience, 46:1155 – 1179, 2016.
[72] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising
sequence-to-sequence pre-training for natural language generation, translation,
and comprehension. In ACL, 2020.

[73] Benjamin Heinzerling and Kentaro Inui. Language models as knowledge bases:
On entity representations, storage capacity, and paraphrased queries. ArXiv,
abs/2008.09036, 2021.

[74] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[75] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-
aware unified pre-trained encoder-decoder models for code understanding and
generation. arXiv preprint arXiv:2109.00859, 2021.

[76] Anonymous. A new search paradigm for natural language code search. 2021.
[77] Luca Buratti, Saurabh Pujar, Mihaela A. Bornea, Scott McCarley, Yunhui Zheng,
Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang,
and Giacomo Domeniconi. Exploring software naturalness through neural lan-
guage models. ArXiv, abs/2006.12641, 2020.

[78] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. What
do they capture?–a structural analysis of pre-trained language models for source
code. arXiv preprint arXiv:2202.06840, 2022.

[79] Patrick Morrison, Kim Herzig, Brendan Murphy, and Laurie Williams. Chal-
lenges with applying vulnerability prediction models. In Proceedings of the 2015
Symposium and Bootcamp on the Science of Security, pages 1–9, 2015.

[80] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
A transformer-based approach for source code summarization. arXiv preprint
arXiv:2005.00653, 2020.

[81] Sergey Troshin and Nadezhda Chirkova. Probing pretrained models of source

code. arXiv preprint arXiv:2202.08975, 2022.
[82] Wenxuan Zhou, Junyi Du, and Xiang Ren.

Improving bert fine-tuning with

embedding normalization. arXiv preprint arXiv:1911.03918, 2019.

[83] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
2019.

[84] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh.
Autoprompt: Eliciting knowledge from language models with automatically
generated prompts. arXiv preprint arXiv:2010.15980, 2020.

[85] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models

better few-shot learners. ArXiv, abs/2012.15723, 2021.

[86] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-

efficient prompt tuning. ArXiv, abs/2104.08691, 2021.

[87] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
for generation. Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), abs/2101.00190, 2021.

[88] Tianyi Tang, Junyi Li, and Wayne Xin Zhao. Context-tuning: Learning con-
textualized prompts for natural language generation. ArXiv, abs/2201.08670,
2022.

[89] Adam Roberts, Colin Raffel, and Noam M. Shazeer. How much knowledge can
you pack into the parameters of a language model? ArXiv, abs/2002.08910, 2020.
[90] Zhengbao Jiang, Frank F. Xu, J. Araki, and Graham Neubig. How can we know
what language models know? Transactions of the Association for Computational
Linguistics, 8:423–438, 2020.

[91] Benjamin Heinzerling and Kentaro Inui. Language models as knowledge bases:
On entity representations, storage capacity, and paraphrased queries. arXiv
preprint arXiv:2008.09036, 2020.

