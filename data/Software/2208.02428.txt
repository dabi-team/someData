2
2
0
2

g
u
A
4

]
E
S
.
s
c
[

1
v
8
2
4
2
0
.
8
0
2
2
:
v
i
X
r
a

Designing and developing tools to
automatically identify parallelism

Fabian Mora Cordero

May 31, 2021

Prelim committee:

Advisor: Dr. Sunita Chandrasekaran

Dr. Stephen Siegel

Department of Computer & Information Sciences

University of Delaware

Abstract

In this work we present a dynamic analysis tool for analyzing re-

gions of code and how those regions depend between each other via

data dependencies encountered during the execution of the program.

We also present an abstract method to analyze and study parallelism

in a directed graph, by studying a Quotient Graph of the execution

graph of a program, and give a simple algorithm for searching paral-

lelism in execution graphs with a high degree of symmetry. Finally,

we evaluate our approach selecting four dwarfs out of 13 Berkeley’s

computational dwarfs or otherwise known as parallel patterns.

1

Introduction

In the last couple of decades parallel computing has garnered mainstream in-
terest from computer scientists and in general computer programmers alike.
This happened because computer processor manufacturers changed chip de-
signing practices opting for pipelined or superscalar architectures and chip
parallelism instead of increasing clock frequencies like they did until 2005,

1

 
 
 
 
 
 
which happened for a variety of reasons, among them power and heat dissi-
pation issues [1].

There has been a paradigm shift from homogeneous systems to heteroge-
neous systems over the past decade or two. The heterogeneous system era
has seen the introduction of massively parallel architectures such as GPG-
PUs from vendors such as NVIDIA and AMD, vector architectures such as
the A64fx from Fujitsu/arm and also other types of devices such as FPGAs,
ASICs, neural engines, many core processors and so on. Architectures are
evolving continuously leading to a perpetual disruption in software. Devel-
opers are facing a non-trivial challenge porting existing algorithms, creating
newer parallel algorithms where need be, parallel friendly data structures
and new programming models that oﬀers the features to express parallelism
in a thorough yet easy manner [2].

In this work we present a dynamic analysis tool for C/C++ sequential
programs for exploring available parallelism within the execution of a pro-
gram. The workﬂow of the proposed method is presented in Figure 1, start-
ing with an annotated C/C++ source code and returning an analysis of the
execution graph of the program.

The proposed method is based on a task programming model. In this
model data dependencies create interrelations between executed tasks and
tasks to be executed during a program execution, producing what we call
the execution graph of a program. This graph will then be used to explore
parallelism in the program.

The proposed method in its current state has the limitation that the re-
sults provided by the analysis of the execution graph are directly tied to a
speciﬁc set of input parameters, thus they can not be automatically gen-
eralized to any input parameters, however we believe that they provide an
insightful view of the parallelization opportunities that the programmer could
generalize for an speciﬁc code.

The rest of this work is organized as follows: section 2 dwells into some of

2

Annotated serial
C/C++ source

Instrumentation tool

Execute instru-
mented program

Build execution graph

Analyze the ex-
ecution graph

Figure 1: Workﬂow of the proposed method.

our motivations for proposing this method, in section 3 we present the related
work, section 4 presents the instrumentation tool, section 5 introduces the
tracing library for collecting the necessary information for building the exe-
cution graph, section 6 presents what is and how to generate the execution
graph from the collected trace, section 7 introduces concepts for exploring
parallelism in directed graphs and presents an algorithm for exploring par-
allelism in the execution graph, 8 presents some results obtained from the
proposed approach, we conclude on section 9.

2 Motivation

The need for porting serial codes into parallel ones creates a natural need for
tools capable of aiding and facilitating the job of programmers in the quest of
parallelization. There exist two main categories for code analysis methods,
namely: static and dynamic analysis. The ﬁrst one happens at compile
time, while the second happens on runtime and requires program execution.

3

Both techniques present drawbacks and strengths, for example there exists
important information that will never be available at compile time as well
that a program execution might not generate generalizable results.

Examples of dynamic and static analysis tools are Intel® Inspector[3],
CppDepend [4], Parallelware Analyzer [5] and many others, however almost
all code analysis tools are of proprietary nature, thus cannot be used or
integrated into open source compiler pipelines like Clang [6] or GCC [7] due
to their closed nature. This creates an important vacuum in the area of
code analysis for porting serial codes to parallel programming models, thus
the need to create open, modern and extensible tools like what the LLVM
project initially created for the compiler community.

3 Related Work

The concept of visualizing tasks and their dependencies is a well established
idea, as it has a straightforward meaning, however it remains an open re-
search area. With papers tackling diﬀerent aspects of the area, for example
in [8] they explore how to relate the scheduler and the dependencies and
visualize them in an eﬀective way. In [9] they present an interactive analysis
visualization tool for execution graphs, with their goal not being automated
analysis rather an interactive one performed by the user and aided by the
tool.

Exploring and analyzing the execution graph of a sequential program
for discovering parallelism is an active research area, with new publications
published each year. One of the most prominent works in the area is the Dis-
coPoP proﬁler ﬁrst introduced in [10] and expanded in [11]–[17]. DiscoPoP is
a tool for discovering parallelism based on the idea of Computational Units
(CU)[10], analyzing dependencies between the CU and pointing into likely
parallelization opportunities.

The main diﬀerence between our approach and DiscoPoP is that we re-

4

quire the user to input the regions to study and then obtain the parallelization
opportunities based on abstract approach rather than matching to known
patterns [14].

4

Instrumentalization tool

In this section we present a tool using the Clang-LLVM compiler infrastruc-
ture [6], responsible for adding instrumentation to an annotated input source
code written in C/C++ and producing an instrumented version of the pro-
gram, ready for trace collection using the trace collection library presented
in section 5. For a small introduction to Clang-LLVM internals see [18].

The input source code needs to be annotated with pragmas indicating
the tracing and tasking regions of interest to be analyzed, the syntax needed
for the annotations is presented in Figure 2a. Alternatively we provide a
C/C++ API for deﬁning the tracing and tasking regions, with its corre-
spondent syntax presented in Figure 2b. We provide this second annotation
method because in order to use the ﬁrst one the user needs to compile a mod-
iﬁed version of the Clang compiler. An example of an annotated function is
presented in Figure 12.

#pragma exg trace // Trace block
{
#pragma exg task // Task block
{
<...>
}
<...>
}

// Block of code

// Block of code

mt_btrace(); // Begin trace
<...>
mt_btask();
<...>
mt_etask();
<...>
<...>
mt_etrace(); // End trace

// Block of code
// Begin task
// Block of code
// End task
// Block of code
// Block of code

(a) Pragma syntax used to annotate the
input source code for deﬁning the tracing
and tasking regions.

(b) C/C++ API syntax used to anno-
tate the input source code for deﬁning the
tracing and tasking regions.

Figure 2: C/C++ syntax for deﬁning the tracing and tasking regions.

5

The instrumentation of the code is performed by an LLVM pass schedule
to perform after the LLVM optimization passes. The pass works by inserting
into the LLVM-IR, calls to the functions of the tracing library presented in
Figure 4. Speciﬁcally it adds calls to the __mt_trace_ir function every time
a store or load instruction is detected, passing the address of the memory
position being accessed and an unique constant numeric identiﬁer of the
speciﬁc instrumentation, to the function.

Additionally it will also generate unique numeric identiﬁers for each of
the tasks and tracing regions, calling __mt_btrace when a trace region begins
and __mt_etrace when a trace region ends, __mt_btask when a task region
begins and __mt_etask when a task region ends.

The pass detects that a region began or ended by either encountering
a call to the functions in Figure 2b or encountering a particular metadata
node associated to one of the pragmas in 2a and generated by the CodeGen
phase of the Clang compiler. This later option requires a modiﬁed version
of the Clang compiler, were the modiﬁcations extend the behavior of the
preprocessor, parser, semantic analyzer and the code generation phase to
accept the pragma as a valid C/C++ construct.

Once the source code is processed by the instrumentalization tool, it
needs to be linked against the trace collection library described in section
5. The commands needed for obtaining the executable from a source ﬁle
are presented in Figure 3. Observe that the code gets compiled with debug
symbols as this enables to map the LLVM-IR to the high-level source code.

clang -I<trace library include path> -g -O1 -emit-llvm -S -o

example.bc -Xclang -load -Xclang libinstrumentalization.so
example.c

clang example.bc -o example.exe -lmemory_tracer -lstdc++

Figure 3: Clang commands to instrument the source ﬁle: example.c.

6

5 Trace collection library

The trace collection library handles metric collection upon execution of the
instrumented program. The collection process does not interfere with the
normal runtime behavior of the program, it only observes the interactions of
the tasks and the instrumented memory positions, records them and ﬁnally
produces at program exit a binary ﬁle trace.out with the program trace.
The public API of the library is presented in Figure 4.

/// Begin a trace region
/// @param id an unique identifier for the trace region
void __mt_btrace(uint32_t id);
/// End a trace region
void __mt_etrace();
/// Begin a task region
/// @param id an unique identifier for the task region
void __mt_btask(uint32_t id);
/// End a task region
void __mt_etask();
/// Trace a memory access
/// @param address to trace
/// @param id an unique identifier for the instruction accessing

the address

void __mt_trace_ir(const void* address, uint32_t id);

Figure 4: API of the tracing library.

Internally the trace collection library works as follows:

• Each time that __mt_btrace gets invoked, create a new trace id.

• Every time __mt_btask is called, create a new execution id such that
it is higher than any of the already given and push it into a stack
maintaining the current execution id.

• When __mt_etask is called, pop an element from the stack maintaining

the current execution id.

7

• Every time __mt_trace_ir is invoked, retrieve the current execution
id and the current trace region id and save the trace id, the execution
id, the accessed address, the id of the instruction into the back of list
keeping all the traces.

• On program exit, save all the lists with the traces into a ﬁle.

6 Execution graph

Once the program trace has been collected it is possible to build an execution
graph for the trace, thus we begin by deﬁning the notion of the execution
graph.

Deﬁnition 1. The execution graph of a program trace, is a directed graph
G = (V, E) in which the vertices V represent task instances and the edges E
are execution dependencies between the task instances. A task instance is a
task region in the source code together with an execution id.

In particular, we focus this work to the case where the dependencies
are data dependencies. An example of annotated source code using the an-
notation syntax described in 4 and the resulting execution graph for some
program trace and built with the algorithms presented later in this section,
is presented in Figure 5b. The vertex labels in the execution graph have the
following meaning, the ﬁrst number is the execution id, the second number
is the identiﬁer for that region in the source code. The edges represent the
data dependencies between the tasks.

The procedure for building the execution graph is presented in Algorithm
1; this algorithm builds the graph one instruction trace at a time. The Valid-
ID function simply checks if a task instance is valid. The data structure for
holding the directed graph G must be capable of performing the Insert-
Edge operation for inserting an edge between two task instances and storing

8

void sw(M m, char* s1, char* s2)

{

#pragma exg trace

for(int i = 1; i < M.n(); ++i)

for(int j = 1; j < M.m(); ++j)

#pragma exg task // Region 1

{

int sc = (s1[i-1] = s2[j-1])

?

match : miss;

m(i, j) = max(m(i, j) + sc,
m(i, j - 1) +

gap,

m(i - 1, j) +

gap,

0);

}

}

(a) Tracing and tasking annotations on
a basic version of the Smith-Waterman
algorithm.

(b) Execution graph obtained by tracing
the code presented in LHS (Figure 5a)
with sequences of length 4.

Figure 5: Annotations on a basic version of Smith-Waterman and the gener-
ated execution graph produced by those annotations.

the dependency kind between them, this operation insert the vertices in the
graph if they are not already in the graph.

The data structure for holding the address table AT must be capable of

performing the operations:

• Find-Address: for ﬁnding an entry based on a memory address.

• Update-Entry: for updating a table entry with new data.

The procedure for updating the address table is presented in Algorithm
2. The purpose of the address table AT is to store the recent history of
every memory address accessed by the program, so that we can determine
who was the last task writing to a certain memory position and establish
dependencies. There are three kind of dependencies, RAW read after write,

9

0 : 11 : 14 : 15 : 12 : 16 : 13 : 17 : 18 : 19 : 110 : 111 : 112 : 113 : 114 : 115 : 1Algorithm 1 Algorithm for building the Execution Graph.

Inputs
Outputs

ptrace: program trace

G: execution graph

(cid:46) Execution graph

(cid:46) Go through the trace list in order
(cid:46) Accessed memory position
(cid:46) Task instance accessing a
(cid:46) Either read or write

1: procedure Build-EG(ptrace)
G ← empty directed graph.
2:
AT ← empty address table.
3:
for all trace ∈ ptrace do
4:
a ← trace.address
5:
t ← trace.task instance
6:
rw ← trace.access kind
7:
(t(cid:48), k) ← Update-Table(AT , a, t, rw)
8:
if Valid-ID(t(cid:48)) then
9:
10:
11:
12:
13:
14: end procedure

Add-Dep(G, (t(cid:48), t), k)

end for
return G

end if

WAR write after read and WAW write after write, also known respectively as
true dependency, anti-dependency and output dependency [19]. We often will
ignore WAR and WAW as they can be safely removed by variable renaming
or address duplication [19].

Finally we present two procedures for adding dependencies to the execu-
tion graph. The ﬁrst one calls directly the Insert-Edge function and inserts
the edge irregardless of anything, while the second, Algorithm 3, prevents the
creation of cycles in the graph by inserting an extension dependency EXT.
The second algorithm uses the function Renew-ID, which assigns a new ex-
ecution id to the task instance t higher than all executions ids in the program
trace and the function ID which returns the execution id of a task instance.

Proposition 2.

1. If the program has no nested tasks regions, then the
execution graph produced by the Build-EG algorithm in conjunction
with the Insert-Edge algorithm is a directed acyclic graph, abbrevi-
ated DAG, with the execution ids being a topological order.

10

Algorithm 2 Algorithm for updating the address table.

Inputs AT : the address table

a: accessed memory address
t: the task instance accessing a

rw: one of write or read

Output

t(cid:48): the previous task instance that modiﬁed the status of a in AT
k: the kind of dependency generated by the access to a

else if rw(cid:48) = write ∧ rw = read then

return (t(cid:48), WAR)

if rw(cid:48) = read ∧ rw = write then

1: procedure Update-Table(AT, a, t, rw)
(t(cid:48), rw(cid:48)) ← Find-Address(AT , a)
2:
Update-Entry(AT , a, (t, rw))
3:
if Valid-ID(t(cid:48)) then
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
end if
14:
15: end procedure

return (NULL, NULL)

return (t(cid:48), WAW)

return (t(cid:48), RAW)

end if

else

else if rw(cid:48) = write ∧ rw = write then

2. The execution graph produced by the Build-EG algorithm in conjunc-
tion with the Add-Dep-Ext algorithm is a DAG with the execution
ids being a topological order.

Proof.
Claim. Let G = (V, E) be the execution graph and observe that if ID(t(cid:48)) <
ID(t), ∀(t(cid:48), t) ∈ E then G must not contain any cycles and the ids must be a
topological order, as a cycle would imply ID(t(cid:48)) > ID(t) for some (t(cid:48), t) ∈ E,
and the ids happen in an increasing order.

1. Lets consider a task instance t and observe that all the memory accesses
that happened in the trace from the moment t began execution, to the mo-

11

Algorithm 3 Algorithm for adding dependencies between tasks using ex-
tension dependencies.

Inputs G: the execution graph

t: the task instance accessing an address a
t(cid:48): the previous task instance accessing the address a
k: the kind of dependency between t(cid:48) and t

if ID(t(cid:48)) ≥ ID(t) then
t(cid:48)(cid:48) ← Renew-ID(t)
Insert-Edge(G, (t, t(cid:48)(cid:48)), EXT)
Insert-Edge(G, (t(cid:48), t(cid:48)(cid:48)), k)

1: procedure Add-Dep-Ext(G, (t(cid:48), t), k)
2:
3:
4:
5:
6:
7:
end if
8:
9: end procedure

Insert-Edge(G, (t(cid:48), t), k)

else

ment it ﬁnished, must have been associated with the task instance t as there
were no other task instances on the stack of the tracing library in section
5, and since that is true for any other tasks, we must have that t can only
depend on tasks with a lower execution id, thus the generated graph can not
have cycles and the ids form an topological order.

2. Let (t(cid:48), t) ∈ E and observe that the Add-Dep-Ext algorithm always
forces ID(t(cid:48)) < ID(t), thus the claim proofs the result.

(cid:4)

7 Graph Analysis

In this section we explore properties of directed graphs and provide an algo-
rithm for how to parallelize a certain class of DAGs.

Throughout this section we will use the following notation:

G = (V, E) A graph G, where V is the set of vertices and E the set of edges

of the graph.

12

u → v

u (cid:32) v

A directed edge in between the vertices u and v in a directed
graph G = (V, E).

The existence of a directed path between u and v in a directed
graph G = (V, E).

Equiv(X)

The set of all equivalence relations over the elements of a set
X.

[x]R

|X|

The equivalence class of x ∈ X under R ∈ Equiv(X) for some
set X.

The cardinality of a set X.

7.1 Vertex independence

Deﬁnition 3. Let G = (V, E) be a directed graph. Two vertices v, u ∈ G
are said to be independent, denoted by v ⊥ u, if u (cid:54)(cid:32) v and v (cid:54)(cid:32) u.

• We deﬁne the set [v]⊥ as {u ∈ V : u ⊥ v} ∪ {v}.

• A set I ⊆ V is said to be independent if v ⊥ u, ∀v, u ∈ I such that

v (cid:54)= u.

• Let I ⊆ V be an independent set, we say that I is maximally indepen-
dent if ∀H ⊆ V such that I ⊆ H and H independent then H = I.

Proposition 4. Let G be a directed graph and I ⊆ V , then:

[x]⊥ if and only if I is independent.

[x]⊥ if and only if I is maximally independent.

1. I ⊆ (cid:84)

x∈I

2. I = (cid:84)

x∈I

Proof.

13

1. “⇒”. Suppose that I ⊆ (cid:84)

[x]⊥ and let v, u ∈ I. Observe that we must

have u ∈ [v]⊥ and v ∈ [u]⊥, thus either v = u or v ⊥ u, which in turn proves
that I is independent.

x∈I

“⇐”. Suppose I ⊆ V is independent and let v ∈ I. Observe that for all
u ∈ I we have that v ∈ [u]⊥, as v ⊥ u or v = u, thus I ⊆ (cid:84)
[u]⊥, proving
the claim.

u∈I

2. “⇒”. Suppose I = (cid:84)

[x]⊥. We know that I is independent by 1.. Let

u ∈ V and observe that if ˜I = I ∪ {u} is an independent set then we have:

x∈I

(cid:92)

˜I ⊆

[x]⊥ ⊆

x∈ ˜I

(cid:92)

x∈I

[x]⊥ = I,

thus ˜I = I, hence I is maximally independent.

“⇐”. Suppose I ⊆ V is maximally independent. By 1. we know that
I ⊆ (cid:84)
[v]⊥ and observe that u ⊥ v, ∀v ∈ I or u ∈ I, if

[v]⊥. Let u ∈ (cid:84)

v∈I

v∈I

u ⊥ v, ∀v ∈ I then by the maximality of I we must have that u ∈ I, in
consequence I = (cid:84)

[v]⊥.

v∈I

Deﬁnition 5. Let G = (V, E) be a ﬁnite DAG, with V = {v1, . . . , vn}.

• We say that G is completely serial if there exists a permutation i1, . . . , in
of the set {1, . . . , n}, such that (vik, vik+1) ∈ E for all k = 1, . . . , n − 1.
See Figure 6 for an example of a completely serial graph.

(cid:4)

14

v1

v2

...

vn

Figure 6: Completely serial ﬁnite DAG.

• We say that G is completely parallel if V is independent. See Figure 7

for an example of a completely parallel graph.

v1

v2

· · ·

vn

Figure 7: Completely parallel ﬁnite DAG.

Proposition 6. Let G = (V, E) be a ﬁnite DAG with V = {v1, . . . , vn}, then:

1. G is completely serial, if and only if:

|[v]⊥| = 1, ∀v ∈ V.

2. G is completely parallel, if and only if:

|[v]⊥| = |V |, ∀v ∈ V.

Proof.

1. “⇒”. Suppose that G is completely serial and let i1, . . . , in be a permu-
tation of the set {1, . . . , n}, such that (vik, vik+1) ∈ E for all k = 1, . . . , n − 1.
Let v ∈ V and observe that ∀u ∈ V we have that either v (cid:32) u, u (cid:32) v or
v = u, as v = vij and u = vil for some 1 ≤ j, l ≤ n and either l ≤ k or
k ≤ l.

15

“⇐”. Suppose that |[v]⊥| = 1, ∀v ∈ V , which translates to v (cid:54)⊥ u, ∀v, u ∈ V
such that v (cid:54)= u or equivalently that ∀v, u ∈ V either v (cid:32) u, u (cid:32) v or v = u.
Observe that there exists an unique v ∈ V such that it has no incoming
edges, as G is ﬁnite and directed acyclic, uniqueness follows from the fact
that if v(cid:48) ∈ V is such that it has no incoming edges then either v(cid:48) (cid:32) v or
v = v(cid:48) thus v(cid:48) = v.

Set x1 = v and deﬁne for 1 < k ≤ n, xk ∈ V as the unique vertex such
that xj (cid:32) xk for j = 1, . . . , k − 1 and u (cid:54)(cid:32) xk, ∀u ∈ V − {x1, . . . , xk−1},
existence and uniqueness follows from the acyclicity of the graph and the
property ∀x, u ∈ V either x (cid:32) u, u (cid:32) x or x = u. Thus (xk, xk+1) ∈ E for
k = 1, . . . , n and {x1, . . . , xn} = V , hence G is completely serial.

2. “⇒”. Suppose that G is completely parallel, then by deﬁnition [v]⊥ =
V, ∀v ∈ V proving the result.

“⇐”. Suppose that |[v]⊥| = |V |, ∀v ∈ V , then [v]⊥ = V, ∀v ∈ V as there is
only one subset of V with |V | elements.

(cid:4)

Deﬁnition 7. Let G = (V, E) be a directed graph and R ∈ Equiv(V ).

• The quotient graph is deﬁned by:

G/R = (V /R, E/R)

V /R = {[v]R : v ∈ V }

E/R = {([v]R, [u]R) : (v, u) ∈ E ∧ [v]R (cid:54)= [u]R}

• If G is also acyclic we say R is a DAG-preserving if G/R is also a DAG.

Proposition 8. Let G be a directed graph and R ∈ Equiv(V ). Let ˆu, ˆv ∈
V /R such that ˆu (cid:54)= ˆv then ˆu ⊥ ˆv =⇒ u ⊥ v, ∀u ∈ ˆu, v ∈ ˆv.

16

Proof. Let ˆu, ˆv ∈ V /R such that ˆu ⊥ ˆv and ˆu (cid:54)= ˆv, and let u ∈ ˆu, v ∈ ˆv.
Suppose there is a path between u and v in G given by u = x1 → . . . →
xk = v, then ([xi]R, [xi+1]R)i=1,...,k−1 is also a path in G/R connecting ˆu and
ˆv, which is a contradiction as we assumed ˆu ⊥ ˆv. Similarly we observe that
(cid:4)
there is no path between v and u. Thus we have proved that u ⊥ v.

Deﬁnition 9. Let G = (V, E) be a directed graph, we say that G is connected
if G is connected as an undirected graph.

Lemma 10. Let G = (V, E) be a ﬁnite connected DAG and R Equiv(V )
a DAG-preserving relation such that the equivalent classes are independent
sets. Suppose G/R is a chain in which every vertex has at most one incom-
ing or one outgoing edge, then there exists a path γ of length |V /R| in G,
furthermore γ has maximal path length in G.

Proof. Let ˆG = G/R be the quotient graph, ˆV = { ˆV1, . . . , ˆVn} the set of
quotient vertices and ˆE = {( ˆV1, ˆV2), . . . , ( ˆVn−1, ˆVn)} the set of quotient edges,
we suppose that n > 1 as the case n = 1 is immediate.
Claim. Let u1 →, . . . , → uk be a path in G, then uj ∈ ˆVj+m−1 for j = 1, . . . , k
and k ≤ n, where m is such that [u1]R = ˆVm.

Let i be such that [u2]R = ˆVi and observe that i (cid:54)= m as u2 (cid:54)∈ ˆVm because
ˆVm is an independent set of vertices in G. Next observe that ( ˆVm, ˆVi) ∈ ˆE
as (u1, u2) ∈ E, thus i = m + 1.
If we continue this process we obtain
uj ∈ ˆVj+m−1 for j = 1, . . . , k, furthermore since ˆG has n vertices we have
that k ≤ n, thus proving the claim.

Let ˜vn be a representative of ˆVn and observe that for all k < n exists
u ∈ ˆVk such that u and ˜vn are connected through a path in G. Otherwise G
would have at least two connected components, which is a contradiction as
we assumed G is connected.

Let γ be a path in G between a member of ˆV1 and ˜vn. Using the claim
we obtain that the length of γ in G is n, furthermore the claim shows that γ
(cid:4)
has maximal length.

17

Deﬁnition 11. Let G = (V, E) be a ﬁnite DAG and R ∈ Equiv(V ).

• We deﬁne the execution time of ˜v ∈ V /R, denoted by ET(˜v), as:




1

ET(˜v) =

if ˜v independent as a subset of V



|˜v|

otherwise

• We deﬁne the execution time of G/R, denoted by ET(G/R), as:

ET(G/R) =

(cid:88)

˜v∈V /R

ET(˜v)

Corollary 12. Let G and R be as in Lemma 10 then ET(G/R) is minimal
over Equiv(V ).

Proof. By Lemma 10, there is a path γ of length n. It is easy to observe that
n ≤ ET(G/R(cid:48)), ∀R(cid:48) ∈ Equiv(V ) and that n = ET(G/R), thus proving the
(cid:4)
result.

Remark 13. Let G = (V, E) be a graph and consider k equivalence relations
Ri ∈ Equiv( ˆVi) for i = 1, . . . , k, where ˆV1 = V and ˆVi+1 = ˆVi/Ri for i =
1 . . . , k, then Rk induces an equivalence relation in V given by:

v ˜Rk u ⇐⇒ [[v]R1 . . .]Rk = [[u]R1 . . .]Rk.

Thus we can also study the graph G by studying quotients of quotients.

Deﬁnition 14. Let G = (V, E) be a graph, the automorphism group of G
[20] is deﬁned by:

Aut(G) = {π ∈ BIJ(V ) | (v, u) ∈ E ⇐⇒ (π(v), π(u)) ∈ E},

where BIJ(V ) is the set of bijective functions from V to V .

18

Remark 15. It should be noted that Aut(G) deﬁnes a natural equivalence
relation R given by:

v R u ⇐⇒ ∃g ∈ Aut(G) such that g(v) = u,

(1)

Proposition 16. Let G = (V, E) be a ﬁnite DAG then:

1. All the elements in V / Aut(G) are independent sets.

2. Aut(G) is DAG-preserving.

Proof.

1. Let ˆv ∈ V / Aut(G) and let u, w ∈ ˆv. By deﬁnition there exists an element
π ∈ Aut(G) such that π(u) = w and let πj+1(x) = π(πj(x)), with π0(x) = x
for x ∈ G. Suppose there exists a path between u and w given by u = x1 →
. . . → xk = w. Observe that:

u = x1 → . . . → xk = w = π(x1) → . . . → π(xk) = π2(x1)
π2(x1) → . . . →= π2(xk) → . . . → πj(x1) → . . . →= πj(xk),

is also a path on G for all j ≥ 3, thus G contains an arbitrarily long path
which is a contradiction, as G is ﬁnite and has no cycles. Hence ˆv is an
independent set, thus proving 1.

2. Suppose that ˆG = G/ Aut(G) is not a acyclic, let ˆV = V / Aut G, then
there exists ˆx1, . . . , ˆxk ∈ ˆV such that ˆx1 = ˆxk and (ˆxj, ˆxj+1) ∈ E/ Aut(G), ∀j =
1, . . . , k − 1.

j]Aut(G) = ˆxj, [ye

Observe that for each j = 1, . . . , k − 1 there exists yb
j ]Aut(G) = ˆxj+1 and (yb

[yb
exists π ∈ Aut(G) such that π(yb
(v1, v2) ∈ E. We can continue this process to obtain vj+1 = πj(ye
(vj, vj+1) ∈ E with πj ∈ Aut(G) such that vj = πj(yb
If vk (cid:54) v1 then we can ﬁnd σ ∈ Aut(g) such that σ(yb

j, ye
j ∈ V such that
j ) ∈ E. Let v1 ∈ ˆx1, then there
1) and observe that
j ) and
j) for j = 2, . . . , k − 1.
1) = vk and start the

1) = v1, set v2 = π(ye

j, ye

19

process again, by continuing this scheme either we get a cycle or an inﬁnite
path, thus a contradiction, hence ˆG = G/ Aut(G) is acyclic.

(cid:4)

Remark 17. When working with execution graphs we can identify the notion
of independence with the notion of parallelism, as independent vertices are
tasks instances with no dependencies between each other, thus capable of
parallel execution.

Finally in Algorithm 4 we present a procedure for exploring parallelism

in codes with highly symmetrical execution graphs.

Algorithm 4 Algorithm for computing a symmetrical parallelization.

Input G: A DAG
Output Q: A quotient DAG
1: procedure SYM-Explore(G)
2:
3:
4:
5:
6:
7:
8:
9: end procedure

Q ← G
A ← Automorphism-Group(G)
while |A| (cid:54)= 1 do
Q ← G/A
A ← Automorphism-Group(Q)

end while
return Q

(cid:46) Computes Aut(G)

8 Results

In this section we present the execution graphs generated by our tool, to-
gether with the results produced by Algorithm 4. We choose 5 code repre-
sentatives from 4 of 13 Berkeley’s Dwarfs [21], namely, Dense Linear Algebra
that entails matrix addition and multiplication, Structured Grids that en-
tails explicit scheme for the one dimensional heat equation, Spectral Methods

20

that entails Fast Fourier transform, and Dynamic Programming that entails
Smith-Waterman.

For each of the test codes our results are presented using 1 ﬁgure con-

taining four subﬁgures. Their organization is as follows:

• The ﬁrst subFigure is the source code of the function to analyze, in
this ﬁgure there will be two labels, FG: and CG:, indicating placements
for the #pragma exg task construct, providing ﬁne and coarse grain
tasking regions.

• The second and third subﬁgures correspond to the execution graphs
generated by the tool for the ﬁne and coarse grain placements respec-
tively, where the ﬁrst number in the vertex label is the execution id of
the task instance and the second one is the region id.

• The fourth and ﬁnal subﬁgures is the quotient graph generated by
applying Algorithm 4 to the ﬁne grained execution graph in the second
ﬁgure, where the vertex labels are the equivalent classes generated by
the algorithm, identifying elements by the graph execution ids.

8.1 Matrix addition

The ﬁrst thing we need to observe from the execution graphs for the matrix
addition code, presented in Figure 8, is that they are completely parallel. In
particular we observe that when we apply Algorithm 4, we obtain a graph
with a single node as the quotient graph, meaning that all tasks can be
executed at the same time. This result can be veriﬁed using Proposition 6,
as the number of independent tasks instances equals the number of vertices
in the graph.

21

void madd(M &C, const M &A,

const M &B) {
#pragma exg trace

for (size_t i = 0; i < C.n;

++i)

CG: for (size_t j = 0; j < C.

m; ++j)

FG:

C(i, j) = A(i, j) + B(i

, j);

}

(a) C++ code for adding two matri-
ces.

(b) Fine grained execution graph
generated by the tool.

(c) Coarse grained execution graph gen-
erated by the tool.

(d) Quotient graph, indicating a paral-
lelization of the program.

Figure 8: Results produced by the tool for the matrix addition function, with
matrix sizes of 2 × 2.

8.2 Matrix multiplication

The execution graphs for the matrix multiplication code, presented in Figure
9, shows that the ﬁned grained tasks suﬀer from a serialization eﬀect due to
the fact that adding the numbers in a single entry depend on other additions
to the same entry, we can observe that this eﬀect vanishes in the coarse
grained version of execution graph. With respect to the quotient graph we
observe that the Algorithm 4 speciﬁes that we need to steps of completely
parallel tasks to obtain the ﬁnal result.

8.3 Heat equation

In this example we are solving the one dimensional heat equation given by:

ut = uxx,

22

0 : 11 : 12 : 13 : 10 : 11 : 1{0, 1, 2, 3}void mmult(M &C, const M &A,

const M &B) {
#pragma exg trace

for (size_t i = 0; i < C.n;

++i)

for (size_t j = 0; j < C.m;

++j)

CG: for (size_t k = 0; k < A.

m; ++k)

FG: C(i, j) += A(i, k) * B(k

, j);

}

(a) C++ code for multiplying two
matrices.

(b) Fine grained execution graph
generated by the tool.

(c) Coarse grained execution graph gen-
erated by the tool.

(d) Quotient graph, indicating a paral-
lelization of the program.

Figure 9: Results produced by the tool for the matrix multiplication function,
with matrix sizes of 2 × 2.

where x is the spatial dimension and t the temporal one. Speciﬁcally the
C++ code in Figure 10a corresponds to an explicit ﬁnite diﬀerence scheme
to compute the equation, see [22] for more information about the scheme.

From the execution graph in Figure 10, we can see the stencil dependen-
cies through time, with time going from top to bottom. In particular the
quotient graph shows the evident, all grid points within a ﬁxed time step can
be executed in parallel.

8.4 Fast Fourier Transform (Cooley-Tuckey)

In this example we target the problem of ﬁnding the Fast Fourier Transform,
for a sequence of 2n numbers using the iterative algorithm presented in [23].

23

1 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 10 : 11 : 12 : 13 : 1{1, 3, 5, 7}{2, 4, 6, 8}From the quotient graph in Figure 11 is possible to observe what it is obvious
from the ﬁned grain graph, that the tasks in each of the levels of the tree in
the coarse execution graph is independent and can be executed in parallel.

24

void fft(V &X, const V &x) {

reverse(X, x);
int l2n = __builtin_ctz(x.

size());
#pragma exg trace

for (int s = 1; s <= l2n; ++

s) {

int m = 1 << s;
int mh = m >> 1;
for (int k = 0; k < x.size

(); k += m)

CG:for (int j = 0; j < mh; ++

j) FG: {
T a = (((-2 * j) * M_PI)

/ m);

T w = std::exp(a * 1.i);
T t = w * X[k + j + mh],

u = X[k + j];
X[k + j] = u + t;
X[k + j + mh] = u - t;

}

}

}

(a) C++ code for the Cooley-Tuckey
algorithm.

(b) Fine grained execution graph
generated by the tool.

(c) Coarse grained execution graph gen-
erated by the tool.

(d) Quotient graph, indicating a paral-
lelization of the program.

Figure 11: Results produced by the tool for ﬀt function, for a vector of length
8.

25

1 : 15 : 16 : 12 : 13 : 17 : 18 : 14 : 19 : 111 : 110 : 112 : 11 : 15 : 12 : 13 : 16 : 14 : 17 : 1{1, 2, 3, 4}{5, 6, 7, 8}{9, 10, 11, 12}8.5 Smith-Waterman

In this example we analyze a simple version of the Smith-Waterman algo-
rithm introduced in [24] for computing local sequence alignment. There are
two important things to notice from Figure 12, that the coarse graph version
serializes execution and that the Algorithm 4 is not able to reduce the graph
to a chain, due to the fact that the execution pattern is not that symmetric.

26

void heat(M &u, real h, real

k) {

size_t nt = u.m - 1;
size_t nx = u.n - 2;
T r = k / (h * h);

#pragma exg trace

for (size_t t = 1; t <= nt;

++t)

CG:for (size_t x = 1; x <= nx

; ++x)

FG: u(x, t) = (1 - 2 * r) *

u(x, t - 1)

+
r * u(x + 1, t -

1) +

r * u(x - 1, t -

1);

}

(a) C++ code for computing the so-
lution to 1D heat equation.

(b) Fine grained execution graph
generated by the tool.

(c) Coarse grained execution graph gen-
erated by the tool.

(d) Quotient graph, indicating a paral-
lelization of the program.

Figure 10: Results produced by the tool for the heat equation function, with
4 grid points and 4 time steps.

27

0 : 14 : 15 : 11 : 16 : 12 : 17 : 13 : 18 : 19 : 110 : 111 : 112 : 113 : 114 : 115 : 10 : 11 : 12 : 13 : 1{0, 1, 2, 3}{4, 5, 6, 7}{8, 9, 10, 11}{12, 13, 14, 15}void sw(M &m, const S &s1,

const S &s2) {
#pragma exg trace

for(int i = 1; i < M.n(); ++

i)

CG:for(int j = 1; j < M.m();

++j) FG: {
int sc = (s1[i - 1] == s2[

j - 1]) ?

match : miss;

m(i, j) = max(m(i, j) + sc

,

m(i, j - 1)
+ gap,
m(i - 1, j)
+ gap,
0);

}

}

(a) C++ code for a basic version of
the Smith-Waterman algorithm.

(b) Fine grained execution graph
generated by the tool.

(c) Coarse grained execution graph gen-
erated by the tool.

(d) Quotient graph, indicating a paral-
lelization of the program.

Figure 12: Results produced by the tool for the Smith-Waterman function,
for two sequences of length 4.

28

0 : 11 : 14 : 15 : 12 : 16 : 13 : 17 : 18 : 19 : 110 : 111 : 112 : 113 : 114 : 115 : 10 : 11 : 12 : 13 : 1{0}{1, 4}{5}{2, 8}{6, 9}{3, 12}{7, 13}{10}{11, 14}{15}9 Conclusions & Next Steps

For next steps, we need to explore how to extend the execution graph and
the algorithm for building it, to incorporate notions like atomic operations,
i.e. operations that could be performed in any order without serialization,
thus avoiding task serialization like in the case of the matrix multiply algo-
rithm. Furthermore we need to extend its relationship to the static program
structure and eventually to the high-level source code structure, so that the
any parallelization detected in the execution graph can be transformed into
a potential parallelization of the source code. We also need to extend the
analysis techniques and parallelization discovery algorithms to incorporate
elements like task duration.

29

References

[1] P. E. Ross. (2008). “Why CPU Frequency Stalled,” [Online]. Available:
https://spectrum.ieee.org/computing/hardware/why-
cpu-frequency-stalled.

[2] R. Gerber, J. Hack, K. Riley, K. Antypas, R. Coﬀey, E. Dart, T.
Straatsma, J. Wells, D. Bard, S. Dosanjh, I. Monga, M. E. Papka,
and L. Rotman, “Crosscut report: Exascale Requirements Reviews,
March 9–10, 2017 – Tysons Corner, Virginia. An Oﬃce of Science re-
view sponsored by: Advanced Scientiﬁc Computing Research, Basic
Energy Sciences, Biological and Environmental Research, Fusion En-
ergy Sciences, High Energy Physics, Nuclear Physics,” Jan. 2018. doi:
10 . 2172 / 1417653. [Online]. Available: https : / / www . osti .
gov/biblio/1417653.

[3]

Intel. (2021). “Intel® Inspector,” [Online]. Available: https : / /
software . intel . com / content / www / us / en / develop /
tools/oneapi/components/inspector.html.

[4] CppDepend. (2021). “CppDepend :: C/C++ Static Analysis and Code
Quality Tool,” [Online]. Available: https : / / www . cppdepend .
com/.

[5] apprenta. (2021). “Parallelware Analyzer,” [Online]. Available: https:
//www.appentra.com/products/parallelware-analyzer/.

[6] C. Lattner and V. Adve, “LLVM: A Compilation Framework for Life-
long Program Analysis & Transformation,” ser. CGO ’04, Palo Alto,
California: IEEE Computer Society, 2004, p. 75, isbn: 0769521029.

[7] Free Software Foundation, Inc. (2020). “Using the GNU Compiler Col-
lection (GCC),” Free Software Foundation, Inc., [Online]. Available:
https://gcc.gnu.org/onlinedocs/gcc-10.2.0/gcc/.

30

[8] B. Haugen, S. Richmond, J. Kurzak, C. A. Steed, and J. Dongarra,
“Visualizing Execution Traces with Task Dependencies,” in Proceed-
ings of the 2nd Workshop on Visual Performance Analysis, ser. VPA
’15, Austin, Texas: Association for Computing Machinery, 2015, isbn:
9781450340137. doi: 10.1145/2835238.2835240. [Online]. Avail-
able: https://doi.org/10.1145/2835238.2835240.

[9] K. Williams, A. Bigelow, and K. Isaacs, “Visualizing a Moving Target:
A Design Study on Task Parallel Programs in the Presence of Evolving
Data and Concerns,” IEEE Transactions on Visualization and Com-
puter Graphics, vol. 26, no. 1, pp. 1118–1128, 2020. doi: 10.1109/
TVCG.2019.2934285.

[10] Z. Li, A. Jannesari, and F. Wolf, “Discovery of Potential Parallelism in
Sequential Programs,” in 2013 42nd International Conference on Par-
allel Processing, 2013, pp. 1004–1013. doi: 10.1109/ICPP.2013.
119.

[11] Z. Li, B. Zhao, A. Jannesari, and F. Wolf, “Beyond Data Parallelism:
Identifying Parallel Tasks in Sequential Programs,” in Algorithms and
Architectures for Parallel Processing, G. Wang, A. Zomaya, G. Mar-
tinez, and K. Li, Eds., Cham: Springer International Publishing, 2015,
pp. 569–582, isbn: 978-3-319-27140-8.

[12] B. Zhao, Z. Li, A. Jannesari, F. Wolf, and W. Wu, “Dependence-Based
Code Transformation for Coarse-Grained Parallelism,” in Proceedings
of the 2015 International Workshop on Code Optimisation for Multi
and Many Cores, ser. COSMIC ’15, San Francisco Bay Area, CA, USA:
Association for Computing Machinery, 2015, isbn: 9781450333160. doi:
10.1145/2723772.2723777. [Online]. Available: https://doi.
org/10.1145/2723772.2723777.

[13] Z. Li, “Discovery of potential parallelism in sequential programs,” Ph.D.
dissertation, Technische Universit¨at Darmstadt, Darmstadt, Sep. 2016.

31

[Online]. Available: http://tuprints.ulb.tu-darmstadt.de/
5741/.

[14] M. Norouzi, F. Wolf, and A. Jannesari, “Automatic Construct Selec-
tion and Variable Classiﬁcation in OpenMP,” in Proc. of the Interna-
tional Conference on Supercomputing (ICS), Phoenix, AZ, USA, ACM,
Jun. 2019, pp. 330–341, isbn: 978-1-4503-6079-1. doi: 10 . 1145 /
3330345.3330375.

[15] R. Atre, Z. Ul-Huda, F. Wolf, and A. Jannesari, “Dissecting sequen-
tial programs for parallelization—An approach based on computational
units,” Concurrency and Computation: Practice and Experience, vol. 31,
no. 5, e4770, 2019, e4770 cpe.4770. doi: https://doi.org/10.
1002 / cpe . 4770. eprint: https : / / onlinelibrary . wiley .
com/doi/pdf/10.1002/cpe.4770. [Online]. Available: https:
/ / onlinelibrary . wiley . com / doi / abs / 10 . 1002 / cpe .
4770.

[16] R. Atre, A. Jannesari, and F. Wolf, “Brief Announcement: Meeting the
Challenges of Parallelizing Sequential Programs,” in Proceedings of the
29th ACM Symposium on Parallelism in Algorithms and Architectures,
ser. SPAA ’17, Washington, DC, USA: Association for Computing Ma-
chinery, 2017, pp. 363–365, isbn: 9781450345934. doi: 10 . 1145 /
3087556.3087592. [Online]. Available: https://doi.org/10.
1145/3087556.3087592.

[17] Z. U. Huda, R. Atre, A. Jannesari, and F. Wolf, “Automatic par-
allel pattern detection in the algorithm structure design space,” in
2016 IEEE International Parallel and Distributed Processing Sympo-
sium (IPDPS), 2016, pp. 43–52. doi: 10.1109/IPDPS.2016.60.

[18] A. Brown and G. Wilson, The Architecture of Open Source Applica-
tions, English. Mountain View, CA, USA: Creative Commons, 2011.
[Online]. Available: http://aosabook.org/en/index.html.

32

[19] Y. Solihin, Fundamentals of Parallel Multicore Architecture, ser. Chap-
man & Hall/CRC Computational Science. CRC Press, 2015, isbn:
9781482211191. [Online]. Available: https : / / books . google .
com/books?id=G2fmCgAAQBAJ.

[20] D. S. Dummit and R. M. Foote, Abstract algebra, 3rd ed. New York,

NY, USA: Wiley, 2004.

[21] K. Asanovic, R. Bodik, B. C. Catanzaro, J. J. Gebis, P. Husbands, K.
Keutzer, D. A. Patterson, W. L. Plishker, J. Shalf, S. W. Williams,
et al., “The landscape of parallel computing research: A view from
berkeley,” 2006.

[22] A. Iserles, U. Iserles, D. Crighton, M. Ablowitz, S. Davis, E. Hinch, J.
Ockendon, C. Crighton, and P. Olver, A First Course in the Numerical
Analysis of Diﬀerential Equations, ser. Cambridge Texts in Applied
Mathematics. Cambridge University Press, 1996, isbn: 9780521556552.
[Online]. Available: https://books.google.com/books?id=
7Zofw3SFTWIC.

[23] T. Cormen, T. Cormen, C. Leiserson, I. Books24x7, M. Press, M. I.
of Technology, R. Rivest, M.-H. P. Company, and C. Stein, Intro-
duction To Algorithms, ser. Introduction to Algorithms. MIT Press,
2001, isbn: 9780262032933. [Online]. Available: https: // books.
google.com/books?id=NLngYyWFl%5C_YC.

[24] T. Smith and M. Waterman, “Identiﬁcation of common molecular sub-
sequences,” Journal of Molecular Biology, vol. 147, no. 1, pp. 195–
197, 1981, issn: 0022-2836. doi: https://doi.org/10.1016/
0022 - 2836(81 ) 90087 - 5. [Online]. Available: https : / / www .
sciencedirect.com/science/article/pii/0022283681900875.

33

