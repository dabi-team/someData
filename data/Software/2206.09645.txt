2
2
0
2

n
u
J

0
2

]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[

1
v
5
4
6
9
0
.
6
0
2
2
:
v
i
X
r
a

Boosted decision trees

Yann Coadou

Centre de physique des particules de Marseille (CPPM),
Aix Marseille Universit´e, CNRS/IN2P3, Marseille, France
coadou@cppm.in2p3.fr

Boosted decision trees are a very powerful machine learning technique.
After introducing speciﬁc concepts of machine learning in the high-
energy physics context and describing ways to quantify the performance
and training quality of classiﬁers, decision trees are described. Some
of their shortcomings are then mitigated with ensemble learning, using
boosting algorithms, in particular AdaBoost and gradient boosting. Ex-
amples from high-energy physics and software used are also presented.

To appear in Artiﬁcial Intelligence for High Energy Physics,
P. Calaﬁura, D. Rousseau and K. Terao, eds. (World Scientiﬁc Pub-
lishing, 2022)

1

 
 
 
 
 
 
2

Y. Coadou

Contents

3. Decision trees

Boosted decision trees
1.
2.

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
Speciﬁcity of high-energy physics
2.1. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2. Splitting samples for training . . . . . . . . . . . . . . . . . . . . . . . .
2.3. Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4. Using machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5. Figures of merit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1. ROC curve and area under the curve . . . . . . . . . . . . .
2.5.2. Signiﬁcance . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
3
3
4
4
5
6
6
7
8
2.6. Controlling overtraining . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.1. Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.2. Tree hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.3. Splitting a node . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.4. Variable selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.4.1. Manipulating variables . . . . . . . . . . . . . . . . . . . . . 19
3.4.2. Mean decrease impurity . . . . . . . . . . . . . . . . . . . . 20
3.4.3. Permutation importance . . . . . . . . . . . . . . . . . . . . 20
3.4.4. Choosing variables . . . . . . . . . . . . . . . . . . . . . . . 21
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.5.1. Training sample composition . . . . . . . . . . . . . . . . . 22
3.5.2. Pruning a tree . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.5.3. Ensemble learning . . . . . . . . . . . . . . . . . . . . . . . 23
4. Boosted decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.2. Boosting algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.3. AdaBoost
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.4. Gradient boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.5. Boosting examples
4.5.1. The XOR problem . . . . . . . . . . . . . . . . . . . . . . . 32
4.5.2. Number of trees and overtraining . . . . . . . . . . . . . . . 32
4.6. Other boosting algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 34
. . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.7. Boosted regression trees
. . . . . . . . . . . . . . . 37
4.8. Boosted decision trees in high-energy physics
4.8.1. Use cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.8.2. Systematic uncertainties . . . . . . . . . . . . . . . . . . . . 40
5. Other averaging techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
6.
7. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.5. Limitations

3

1. Introduction

Decision trees are a machine learning technique that appeared in the mid-
1980’s and are still the subject of advanced studies in the ﬁeld. Because it is
a sophisticated supervised multivariate technique, learning from examples,
it is important to remember that before applying it to real data (e.g. col-
lisions from a high-energy physics experiment), it is crucial to have a good
understanding of the data and of the physics model used to describe them
(simulated samples, reconstruction and identiﬁcation eﬃciencies, etc.). Any
discrepancy between the real data and physics model (that is, features in
the data that are not reproduced by the physics model because the simula-
tion is incorrect or because the real data were not properly groomed) will
provide an artiﬁcial separation that the decision trees will use, misleading
the analyser. The hard (and interesting) part of the analysis is in build-
ing the proper physics model, not in ‘just’ extracting the signal. But once
this is properly done, decision trees (and especially their boosted versions)
provide a very powerful tool to increase the signiﬁcance of any analysis.

Ever since their ﬁrst use by the MiniBooNe collaboration for analysis
and particle identiﬁcation [1, 2] and by the D0 experiment for the ﬁrst evi-
dence of single top quark production [3, 4], boosted decision trees have been
a primary tool in high energy physics to increase the discovery potential
and measurement precision of experiments, in particular at the Tevatron
and at the LHC. They are still highly relevant (and highly performing) in
2021, even though deep neural networks are becoming a serious contender.
As this is the ﬁrst chapter of this book, some of the basic concepts
useful in the context of high-energy physics when using most techniques
presented in this and other chapters are summarised in Sec. 2. Section 3
explains how a decision tree is constructed, what parameters can inﬂuence
its development and what its intrinsic limitations are. One possible exten-
sion of decision trees, boosting, is described in detail in Sec. 4, and other
techniques trying to reach the same goal as boosting are presented in Sec. 5.
Popular software implementations are introduced in Sec. 6, before reaching
conclusions in Sec. 7.

2. Speciﬁcity of high-energy physics

All techniques presented in this book need to learn from examples. After
a short list of deﬁnitions to have a common language between the physi-
cist and the computer scientist in Sec. 2.1, several training strategies are

4

Y. Coadou

presented in Sec. 2.2, as well as how to deal with the samples to minimise
training bias and maximise statistical power. In order to properly assess
the performance of a classiﬁer, cross-validation is introduced in Sec. 2.3.
Section 2.4 describes typical usage of machine learning algorithms in high-
energy physics. Several ﬁgures of merit are described in Sec. 2.5 and over-
training is addressed in Sec. 2.6.

2.1. Terminology

Here are a few terms that take on diﬀerent meanings in a high-energy
physics or machine learning context.

Event All information collected during a collision inside a detector, or re-
produced from a Monte Carlo simulation of such collisions (equiv-
alent to a ‘sample’ in machine learning literature).

Sample A collection of events, a dataset.
Variable A property of the event or of one of its constituents (‘feature’ in

machine learning)

Cut To cut on a variable is to apply a threshold on this variable and
keep only events satisfying this condition. A cut-based analysis is
applying such thresholds on several variables to select events.

Event weight In high-energy physics events usually have an associated
weight, which depends on how many events were generated (relat-
ing to the process cross section and collected luminosity) and var-
ious corrections applied to simulations to account for diﬀerences
between data and Monte Carlo predictions (jet energy scale or ob-
ject identiﬁcation eﬃciency are such weights). When using machine
learning techniques all events are often treated equal by default. It
is therefore important for the physicist to make sure to give the
proper initial weight to all its input events. Then machine learn-
ing algorithms may internally reweight the events for their own
purpose, but the starting point will correspond to the physical dis-
tributions. The concept is similar to importance weighting in ma-
chine learning, where events are given a larger weight to account,
for instance, for their scarcity in the training sample.

2.2. Splitting samples for training

Decision trees, as many of the techniques presented in this book, belong
to the class of algorithms using supervised learning: during training, the

5

classiﬁer is presented only with events for which it knows features (discrim-
inating variables) and class label (for instance in the binary case, whether
the event is signal or background).

In order to not introduce bias, it is important to use an independent set
of events during training, events that are then not used when performing a
measurement. The usual approach is to split the dataset in three parts: a
training sample from which to learn the task, a validation sample to evalu-
ate performance and possibly optimise the classiﬁer hyperparameters, and
a testing sample for the actual measurement. In high-energy physics, sim-
ulated Monte Carlo events are often used for these three samples, and the
performance on the testing sample is compared to that on data collected
from the detector (never seen during training). Discrepancies between test-
ing sample and data introduce a potential pitfall, that can be addressed
with transfer learning and domain adaptation [5].

In general labelled data are ‘expensive’ to produce: hiring people to
label images or translate speech, collecting X-ray images and medical diag-
nosis, etc. In high-energy physics very accurate, though not perfect, event
generators and detector simulators are available. Models can be trained on
the samples they provide, which are however quite costly in resources so
that they should be used with parsimony. At the same time an increasing
training set size is often associated with improved classiﬁer performance.
Monte Carlo samples can be split in half, one half for training (holding out
part of this dataset for validation) and one for testing. By doing this half
of the sample is ‘wasted’, not used for either training or testing, decreasing
the quality of the training and of the measurement. The use of the sample
can be maximised by performing two trainings: train the same classiﬁer on
the two halves (say, one on events with an even event number and one on
events with an odd event number), and when testing, apply the classiﬁer
which did not see the event during training (so, the one trained on odd
events is applied on even ones, and vice versa). The concept can be gener-
alised to any number of splits, increasing the number of trained classiﬁers,
each of them using a larger fraction of the available dataset for training.

2.3. Cross-validation

Training machine learning algorithms is usually a stochastic problem, the
randomness coming from the training sample content, the optimisation pro-
cess or the technique itself. This means that when training only once, there
is a possibility to obtain an ‘abnormal’ result by chance, too good or too

6

Y. Coadou

In high-energy physics some
bad compared to what could be expected.
training samples may be limited in size and become very sensitive to this
issue. To get a proper estimate of the mean performance and associated
uncertainty (the variability of the algorithm output, originating from the
training procedure), it may be better to perform several trainings. This
principle was introduced with the so-called K-fold cross-validation, orig-
inally for decision trees [6]. After dividing a training sample L into K
subsets of equal size, L = (cid:83)
k=1..K Lk, a classiﬁer Tk is trained on the
L − Lk sample and tested on Lk. This produces K classiﬁers, from which
the mean performance and associated uncertainty is extracted. It helps in
choosing the best model (each being tested with cross-validation), rather
than relying on a single training for each model (which may or may not
have an upward or downward performance ﬂuctuation). Once the model
is chosen, it can be retrained on the full (larger) training set, assuming its
performance should approach the observed mean performance.

2.4. Using machine learning

This book describes various ways of using machine learning in high energy
physics to accomplish many diﬀerent tasks. Boosted decision trees are
mostly used to separate a rare signal from a large background in physics
analyses or to identify physics objects in the detector (see several use cases
in Sec. 4.8.1).
In practice these results are obtained in two ways. By
applying a threshold on the boosted decision tree output a region or working
point can be deﬁned, as shown in Fig. 1(a): cutting at 0.83 deﬁnes a b-
tagging working point with 70% eﬃciency on b-jets and a rejection factor
(deﬁned as the inverse of eﬃciency) of 313 (8) against light-ﬂavoured jets
(c-jets) [7]. The second approach consists in using the shape of the boosted
decision tree output as the discriminating variable for the ﬁnal analysis. As
an example, in Fig. 1(b) the bins of ‘BDT score’ for all components of the
physics model are included in a binned likelihood ﬁt to the data. The low
score values help constrain the background, and the high score bins reveal
the need of the signal contribution to match the data, leading to the ﬁrst
evidence for the production of t¯tt¯t in ATLAS [8].

2.5. Figures of merit

It is nowadays very easy, in just a few lines, to write the code to train and
apply various machine learning algorithms, with several software options
on the market (see Sec. 6). The lengthy part is more in the design and

7

(a)

(b)

Fig. 1.
(a) Output of the boosted decision tree used to identify jets originating from
b-quarks in ATLAS [7]. (b) Boosted decision tree output used in a ﬁt between data and
physics model to extract the t¯tt¯t signal [8].

optimisation of the model itself (that is, what algorithm, structure, hyper-
parameters to put in these few lines), and how to pick the best one. Several
measures that are commonly used, in particular in high-energy physics, are
presented below.

2.5.1. ROC curve and area under the curve

The receiver operating characteristic curve, or ROC curve, is a represen-
tation of the capacity of a binary classiﬁer to separate the two classes, as
its discrimination threshold is varied. It is plotting the true positive rate
(or recall, a measure of the proportion of actual positives that are cor-
rectly identiﬁed as such) against the false positive rate (or fall-out, actual
negatives improperly identiﬁed as positive), obtained when scanning the
classiﬁer output. In the context of signal and background, it shows signal
eﬃciency versus background eﬃciency (or background rejection, deﬁned as
1 − eﬃciency). An example is shown in Fig. 2.
In this convention, the
better the classiﬁer, the closer the curve is to the top right corner. The
dashed line in the middle represents the performance of a classiﬁer that is
randomly guessing, rejecting or accepting 50% of signal and background in
all cases. This is the worst achievable performance.

To compare ROC curves between classiﬁers, the area under the curve,
AUC (hatched area in Fig. 2) can be computed. Perfect separation gives

1−0.8−0.6−0.4−0.2−00.20.40.60.81MV2D4−103−102−101−10110Fraction of jets / 0.05t = 13 TeV, tsATLAS Simulationb jetsc jetsLight flavour jets0.8-0.6-0.4-0.2-00.20.40.60.81BDT score00.511.5 Data / Pred.1-10110210310410Events / 0.1ATLAS-1 = 13 TeV, 139 fbsSRPost-FitDatattttWttZttHttQ mis-idMat. Conv.HF e*gLow mmHF OtherstttUncertainty8

Y. Coadou

Fig. 2. Example ROC curve. The hatched area is the area under the curve. The dashed
line corresponds to random guessing.

an AUC of one, while random guessing corresponds to an AUC of 0.5.

A single number summary is of course practical, but hides details of the
If one ROC curve is systematically above
ROC curves being compared.
the other, its AUC is larger and reﬂects better performance across the
board. But if two ROC curves cross each other, then the interpretation
of the AUC is more tricky: depending on the usage of the classiﬁer, a
higher curve at high background rejection may be more interesting than
one at high eﬃciency for instance, so how to interpret the AUC is up to the
analyser. To partially account for this eﬀect it is also possible to compute
the AUC only above a certain threshold.

2.5.2. Signiﬁcance

s√

In a physics analysis, the AUC is rarely the number of interest to optimise.
or
It is more typical to aim for the best cross-section signiﬁcance
excess signiﬁcance s√
, where s (b) is the sum of weights (see Sec. 2.1) of
b
signal (background) events. With n events in data, the observed signiﬁcance
is obtained by replacing s by n − b. Given a machine learning algorithm
output, typically in the range [0, 1] or [−1, 1], as is done when producing the
ROC curve, s and b are computed above a threshold on the discriminant
output, scanning its full range. It usually goes through a maximum towards
high output values, before decreasing when statistics become too small.
This maximum signiﬁcance corresponds to the optimal value on which to
cut on the discriminant to get the best possible analysis.

s+b

This simple-minded formula is very popular in high-energy physics but
has shortcomings, and a reﬁned version (counting experiment supposing a

background rejectionsignal eciency10019

single Poisson distributed value, with known background) gives the approx-
imate median signiﬁcance [9]:

AMS =

(cid:114)
2

(cid:16)

(s + b) ln

(cid:16)

1 +

(cid:17)

− s

.

(cid:17)

s
b

Expanding the logarithm in s/b leads back to the previous formula,

qualifying the validity of the approximation (requires s (cid:28) b):

AMS =

s
√
b

(1 + O(s/b)) .

Optimising the AMS corresponds to optimising the ROC curve, focusing
on the region with very high background rejection. This is the typical
regime of a physics analysis.

There is usually an uncertainty on the background, which aﬀects the sig-
niﬁcance. To extract their ﬁnal results, modern analyses rely on advanced
statistical models with a complex machinery (usually based on the RooStat
framework [10]) accounting for all possible systematic eﬀects. Running this
whole infrastructure during machine learning training optimisation is usu-
ally prohibitive (complexity, CPU cost), so a simpler proxy to the analysis
performance measure is necessary.

The simplest way to account partially for background uncertainty (σb ≡
b by the quadratic sum of

||b − bsyst||) is to replace

b and σb:

√

√

s
(cid:112)b + σ2

b

.

A reﬁned version of the AMS can also take into account the background

uncertainty [11]:

(cid:115)

(cid:18)

AMS1 =

2

(s + b) ln

(cid:19)

− s − b + b0

+

s + b
b0

(b − b0)2
σ2
b

,

with b0 =

1
2

(cid:18)

b − σ2

b +

(cid:113)

(b − σ2

b )2 + 4(s + b)σ2
b

(cid:19)

.

Expanding in powers of s/b and σ2

b /b gives back the simpler formula:

s
(cid:112)b + σ2

b

(cid:0)1 + O(s/b) + O(σ2

b /b)(cid:1) .

Finally, to account for the shape of the discriminant rather than only
choosing the best cut in a counting experiment, it is possible to replace

10

Y. Coadou

the global counts s, b and σb by their counts in each bin, summing up
contributions of N bins of discriminant output:

AMSsum

1 =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N
(cid:88)

i

(cid:18)

(cid:18)

2

(si + bi) ln

si + bi
b0i

(cid:19)

− si − bi + b0i

+

(bi − b0i)2
σ2
bi

(cid:19)
,

b0i =

1
2

(cid:18)

bi − σ2

bi +

(cid:113)

(bi − σ2

bi)2 + 4(si + bi)σ2
bi

(cid:19)

.

2.6. Controlling overtraining

Overtraining is what happens when a classiﬁer learns too much about the
speciﬁc details of the training sample, while these features are not repre-
sentative of the underlying distributions. It may then be targeting noise,
or misrepresent regions with too little statistics to train on. When apply-
ing such a classiﬁer on the testing sample, its performance will be worse
than that of a classiﬁer immune to this issue, because it does not generalise
It should be noted that what is often called overtraining here and
well.
in the following, in accordance with high-energy physics usage, is usually
referred to as overﬁtting in the machine learning community. This is the
so-called bias–variance trade-oﬀ [12]: it is diﬃcult to minimise both the bias
(the diﬀerence between the prediction of the model and the correct value
it tries to predict) and variance (the variability of the model prediction for
a given event, when considering multiple realisations of this same model).
Increasing model complexity lowers the bias while increasing variance.

A particular type of overtraining is very easy to avoid, by following good
practices from Sec. 2.2: never use training events when making the ﬁnal
measurement, which has to be performed on an independent set of events,
never seen during training. Otherwise the performance will be artiﬁcially
enhanced on the ‘testing’ sample and comparisons with the application to
data will be impossible (or worse if not noticed).

Several techniques exist to mitigate overtraining, generically referred to
as regularisation. They typically add a penalty for complexity to the loss
function that is minimised during training (the function that maps each
event to a real number quantifying the diﬀerence between the predicted and
true classes or values). With classiﬁer f and loss function L, a regularisation
term R(f ) is added to the loss function, which becomes L(f )+λR(f ), where
λ is a parameter controlling the importance of the regularisation term. This
will favour simpler models (increasingly simpler with larger values of λ,
with the risk of underﬁtting with too much regularisation), less susceptible

11

to overtraining. R(f ) can take various forms, like L1 (L2) regularisation
based on the sum of weights (sum of squared weights) used to describe
neural networks, or the number and depth of trees (see Sec. 3.5.2). Sparsity
(setting many weights to zero [13]) and dropout (randomly dropping out
nodes during training [14]) are more recent very eﬀective approaches for
neural networks. Ensemble learning (see Sec. 3.5.3 and Sec. 5) is another
approach.

It is important to check whether the model suﬀers from overtraining.
As shown in Fig. 3 this can be achieved by monitoring the error rate (or
the loss function) during training, as a function of the number of trees
with boosted decision trees or training epoch with neural networks, on the
training and validation samples. Figure 3(a) is the canonical example of
such curves. The training error tends towards zero, while the testing curve
ﬁrst follows the training curve, reaches a minimum and increases again.
The best classiﬁer is the one at the minimum, training further will reduce
performance and cause overﬁtting: the classiﬁer has too much capacity
(complexity) with respect to the training sample. Selecting the model at
the minimum means early stopping [12].

In many cases though, the situation is similar to Fig. 3(b): the training
and testing curves follow each other but start diverging while still both
improving. The classiﬁer is therefore already learning speciﬁcities of the
training set, but still learning properties that generalise well and improve
performance on the validation set. The testing curve goes through a min-
imum, corresponding to the best model, and increases again, this time
showing detrimental overtraining as the performance decreases on the val-
idation set (overﬁtting regime). This is the typical U-shaped curve arising
from the bias–variance trade-oﬀ.

The curves could also look like Fig. 3(c), where the testing curve never
goes through a minimum and instead ﬂattens. Once in the plateau, all
classiﬁers are equivalent in terms of performance on the validation set,
while the training error keeps improving (and could reach zero, this is the
so-called interpolation regime [15])). This is a typical curve for boosted
decision trees.

Finally the situation could correspond to Fig. 3(d). At the interpolation
threshold the training error reaches zero, but continued training of high
capacity classiﬁers leads to a double descent curve: the testing performance
keeps increasing while the training error stays at zero [16].

12

Y. Coadou

(a)

(b)

(c)

(d)

Fig. 3. Overtraining estimation using the error rate as a function of the number of trees
(for boosted decision trees) or epochs (for neural networks). Black curves are measured
on the training sample and red curves on the validation sample. The optimal classiﬁer
corresponds to the ‘best’ label. The hatched areas represent overtraining: beneﬁcial in
blue (but underﬁtting), detrimental in orange (overﬁtting). (a) Typical curves, with the
best model at the minimum of the testing curve, and overﬁtting beyond with decrease
of performance. (b) The best model is overtrained but still improves performance. (c)
Typical curves for boosted decision trees with ﬂattening testing error rate: all models
in the ﬂat area perform equally well despite increasing overtraining. (d) Interpolation
regime: the best classiﬁer is obtained after the training error has reached zero.

3. Decision trees

Decision trees are a machine learning technique ﬁrst developed in the con-
text of data mining and pattern recognition [6], which then gained momen-
tum in various ﬁelds, including medical diagnosis [17, 18], insurance and
loan screening, or optical character recognition of handwritten text [6].

It was developed and formalised by Breiman et al. [6] who proposed the
CART algorithm (Classiﬁcation And Regression Trees) with a complete

error ratenumber of trees/epochstraintestbesterror ratenumber of trees/epochstraintestbesterror ratenumber of trees/epochstraintestbesterror ratenumber of trees/epochstraintestbestinterpolation threshold13

and functional implementation of decision trees.

The basic principle is rather simple:

it consists in extending a simple
cut-based analysis into a multivariate technique by continuing to analyse
events that fail a particular criterion. Many, if not most, events do not have
all characteristics of either signal or background (for a two-class problem).
The concept of a decision tree is therefore to not reject right away events
that fail a criterion, and instead to check whether other criteria may help
to classify these events properly.

In principle a decision tree can deal with multiple output classes, each
branch splitting in many subbranches. In this chapter almost only binary
trees will be considered, with only two possible classes: signal and back-
ground. The same concepts generalise to non-binary trees, possibly with
multiple outputs.

Section 3.1 describes the decision tree building algorithm, controlled by
hyperparameters presented in Sec. 3.2. The way to split nodes is explained
in Sec. 3.3, while Sec. 3.4 describes how decision trees can advantageously
deal with input variables and how to optimise their list. Finally Sec. 3.5
reports several shortcomings of decision trees, with suggestions to address
them.

3.1. Algorithm

Mathematically, decision trees are rooted binary trees (as only trees with
two classes, signal and background, are considered). An example is shown
in Fig. 4. A decision tree starts from an initial node, the root node. Each
node can be recursively split into two daughters or branches, until some
stopping condition is reached. The diﬀerent aspects of the process leading
to a full tree, indiﬀerently referred to as growing, training, building or
learning, are described in the following sections.

Consider a sample of signal (si) and background (bj) events, each with
j, respectively, described by a set (cid:126)xi of variables. This

weights ws
sample constitutes the root node of a new decision tree.

i and wb

Starting from this root node, the algorithm proceeds as follows:

(1) If the node satisﬁes any stopping criterion, declare it as terminal (that

is, a leaf) and exit the algorithm.

(2) Sort all events according to each variable in (cid:126)x.
(3) For each variable, ﬁnd the splitting value that gives the best separation
between two children, one with mostly signal events, the other with
mostly background events (see Sec. 3.3 for details). If the separation

14

Y. Coadou

cannot be improved by any splitting, turn the node into a leaf and exit
the algorithm.

(4) Select the variable and splitting value leading to the best separation
and split the node in two new nodes (branches), one containing events
that fail the criterion and one with events that satisfy it.

(5) Apply recursively from step 1 on each node.

This is a greedy algorithm, not guaranteed to ﬁnd the optimal solution. At
each node, all variables can be considered, even if they have been used in
a previous iteration: this allows to ﬁnd intervals of interest in a particular
variable, instead of limiting oneself to using each variable only once.

It should be noted that a decision tree is human readable: exactly which
criteria an event satisﬁed in order to reach a particular leaf can be traced.
It is therefore possible to interpret a tree in terms of, e.g., physics, deﬁning
selection rules, rather than only as a mathematical object.

In order to make the whole procedure clearer, let us take the tree in
Fig. 4 as an example. Consider that all events are described by three
variables: x, y and z. All signal and background events make up the root
node.

Fig. 4. Graphical representation of a decision tree. Blue rectangles are internal nodes
with their associated splitting criterion; leaves are terminal nodes with their purity.

x < 1.53failpassfailpassy < 0.004y < 0.1failpass0.910.13failpass0.29z < 30x < 1.8passfail15

All events are ﬁrst sorted according to each variable:

xs1 ≤ xb34 ≤ · · · ≤ xb2 ≤ xs12 ,
yb5 ≤ yb3 ≤ · · · ≤ ys67 ≤ ys43 ,
zb6 ≤ zs8 ≤ · · · ≤ zs12 ≤ zb9,

where superscript si (bj) represents signal (background) event i (j). Using
some measure of separation between classes (see below) the best splitting
for each variable may be (arbitrary unit):

x < 1.53 separation = 5,
y < 0.01 separation = 3,
z < 25

separation = 0.7.

The best split is x < 1.53, and two new nodes are created, the left one
with events failing this criterion and the right one with events satisfying
it. The same algorithm is applied recursively to each of these new nodes.
As an example consider the right-hand-side node with events that satisﬁed
x < 1.53. After sorting again all events in this node according to each of the
three variables, it was found that the best criterion was x < 1.8, and events
were split accordingly into two new nodes. This time the right-hand-side
node satisﬁed one of the stopping conditions and was turned into a leaf.
From signal and background training events in this leaf, the purity was
computed as p = 0.91. The left-hand-side node keeps splitting further.

The decision tree output for a particular event i is deﬁned by how its

(cid:126)xi variables behave in the tree:

(1) Starting from the root node, apply the ﬁrst criterion on (cid:126)xi.
(2) Move to the passing or failing branch depending on the result of the

test.

(3) Apply the test associated to this node and move left or right in the tree

depending on the result of the test.

(4) Repeat step 3 until the event ends up in a leaf.
(5) The decision tree output for event i is the value associated with this

leaf.

There are several conventions used for the value attached to a leaf. It
can be the purity p = s
s+b where s (b) is the sum of weights of signal
(background) events that ended up in this leaf during training. It is then
bound to [0, 1], close to 1 for signal and close to 0 for background.

It can also be a binary answer, signal or background (mathematically
typically +1 for signal and 0 or −1 for background) depending on whether

16

Y. Coadou

the purity is above or below a speciﬁed critical value (e.g. +1 if p > 1
−1 otherwise).

2 and

Looking again at the tree in Fig. 4, the leaf with purity p = 0.91 would
give an output of 0.91, or +1 as signal if choosing a binary answer with a
critical purity of 0.5.

3.2. Tree hyperparameters

The number of hyperparameters of a decision tree is relatively limited.
The ﬁrst one is not speciﬁc to decision trees and applies to most techniques
requiring training: how to normalise signal and background with respect to
each other before starting the training? Conventionally the sums of weights
of signal and background events are chosen to be equal (balanced classes),
giving the root node a purity of 0.5, that is, an equal mix of signal and
background. Decision trees are not particularly sensitive to this original
normalisation as in practice, a few early splits will produce nodes with
more balanced categories, therefore only leading to a limited ineﬃciency in
the training process which only impacts marginally the ﬁnal discriminating
power.

Other hyperparameters concern the selection of splits. A list of dis-
criminating variables is needed, and a way to evaluate the best separation
between signal and background events (the goodness of the split). Both
aspects are described in more detail in Sec. 3.3 and Sec. 3.4.

The splitting has to stop at some point, declaring such nodes as terminal

leaves. Conditions to satisfy can include:

• a minimum leaf size. A simple way is to require at least Nmin training
events in each node after splitting, to ensure the statistical signiﬁcance
Nmin. It
of the purity measurement, with a statistical uncertainty
becomes a little bit more complicated with weighted events, as is nor-
mally the case in high-energy physics applications. Using the eﬀective
number of events instead may be considered:
(cid:1)2

√

(cid:0) (cid:80)N

Neﬀ =

i=1 wi
i=1 w2
i

(cid:80)N

,

for a node with N events associated to weights wi (Neﬀ = N for un-
weighted events).

• having reached perfect separation (all events in the node belong to the

same class).

• an insuﬃcient improvement with further splitting.

17

• a maximum tree depth, if the tree cannot have more than a certain
number of layers (for purely computational reasons or to have like-size
trees).

Finally a terminal leaf has to be assigned to a class. This is classically

done by labelling the leaf as signal if p > 0.5 and background otherwise.

3.3. Splitting a node

The core of a decision tree algorithm resides in how a node is split into
two. Consider an impurity measure i(t) for node t, which describes to what
extent the node is a mix of signal and background. Desirable features of
such a function are that it should be:

• maximal for an equal mix of signal and background (no separation).
• minimal for nodes with either only signal or only background events

(perfect separation).

• symmetric in signal and background purities, as isolating background

is as valuable as isolating signal.

• strictly concave in order to reward purer nodes. This tends to favour

asymmetric end cuts with one smaller node and one larger node.

A ﬁgure of merit can be constructed with this impurity function, as the
decrease of impurity for a split S of node t into two children tP (pass) and
tF (fail):

∆i(S, t) = i(t) − pP · i(tP ) − pF · i(tF ),

where pP (pF ) is the fraction of events that passed (failed) split S.

The goal is to ﬁnd the split S∗ that maximises the decrease of impurity:

∆i(S∗, t) = max

S∈{splits}

∆i(S, t).

It will result in the smallest residual impurity, which minimises the overall
tree impurity.

A stopping condition can be deﬁned using the decrease of impurity, not
splitting a node if ∆i(S∗, t) is less than some predeﬁned value. Such early-
stopping criterion requires care, as sometimes a seemingly very weak split
may allow child nodes to be powerfully split further (see Sec. 3.5.2 about
pruning).

Common impurity functions (exhibiting most of the desired features

mentioned previously) are illustrated in Fig. 5:

18

Y. Coadou

• the misclassiﬁcation error: 1 − max(p, 1 − p),
• the (cross) entropy [6]: − (cid:80)
• the Gini index of diversity [19].

i=s,b pi log pi, with pb = 1 − ps and ps = p,

The Gini index is the most popular in decision tree implementations. It
typically leads to similar performance to entropy.

Fig. 5.

Impurity measures as a function of signal purity.

Other measures are also used sometimes, which do not satisfy all criteria
listed previously but attempt at optimising signal signiﬁcance, a typical
ﬁnal goal in high-energy physics applications (see Sec. 2.5.2):

• cross section signiﬁcance (optimising
• excess signiﬁcance (optimising s√
b

s√
s+b
): − s2
b .

): − s2

s+b ,

3.4. Variable selection

Overall decision trees are very resilient to most factors aﬀecting variables.
They are not too much aﬀected by the ‘curse of dimensionality’, which
forbids the use of too many variables in most multivariate techniques. For
decision trees the CPU consumption scales as nN log N with n variables
and N training events.
It is not uncommon to encounter decision trees
using tens [4] or hundreds [2] of variables, although this is usually frowned
upon in high-energy physics: more variables means more distributions and
correlations to check, more complex interplay with systematic uncertainties,
more dependence on the Monte Carlo event properties that are usually used
during training and may not match real data so well, so physicists tend to
reduce the list of discriminating variables to typically 10–15. On the other
hand adding variables tends to always improve the performance of decision
trees (see Sec. 4.8.1 for an example).

signal purity00.20.40.60.81arbitrary unit00.050.10.150.20.25Split criterionMisclas. errorEntropyGini3.4.1. Manipulating variables

19

With most machine learning algorithms, a careful preparation of inputs
is necessary to achieve good performance. Although not detrimental to
decision trees, such manipulations are not really compulsory as decision
trees tend to be very stable under such transforms.

A decision tree is immune to duplicate variables: the sorting of events
according to each of them would be identical, leading to the exact same
tree. The order in which variables are presented is completely irrelevant:
all variables are treated equal. The order of events in the training samples
is also irrelevant.

If variables are not very discriminating, they will simply be ignored and
will not add any noise to the decision tree. The ﬁnal performance will not
be aﬀected, it will only come with some CPU overhead during both training
and evaluation.

Decision trees can deal easily with both continuous and discrete vari-

ables, simultaneously.

Another typical task before training a multivariate technique is to trans-
form input variables by for instance making them ﬁt in the same range (nor-
malisation), having unit variance (standardisation) or taking the logarithm
to regularise the variable. This is totally unnecessary with decision trees,
which are completely insensitive to the replacement of any subset of input
variables by (possibly diﬀerent) arbitrary strictly monotone functions of
them (e.g. converting MeV to GeV), as the same ordering of events would
induce the same splits on the dataset, producing the same decision tree.
This means that decision trees have some immunity against outliers. The
above is strictly true only if testing all possible cut values while evaluating
the optimal split. If there is some computational optimisation (e.g., check
only 20 possible cuts on each variable), it may not work anymore and some
transformation of inputs may be beneﬁcial, at the very least to speed up
convergence (numerical precision could also be a factor).

If linear correlations exist between variables, ﬁrst decorrelating the input
variables and then feeding them to the decision tree may help. If not doing
this decorrelation, a decision tree will anyway ﬁnd the correlations but in a
very suboptimal way, by successive approximations, adding complexity to
the tree structure without performance gain.

20

Y. Coadou

3.4.2. Mean decrease impurity

It is possible to rank variables in a decision tree, adding up the decrease
of impurity (see Sec. 3.3) for each node where the variable was used to
split, hence computing the mean decrease impurity (MDI). The variable
with the largest decrease of impurity is the best variable. A shortcoming
of this approach is that it is computed on the training set only, and may
be exaggerating the importance of some variables because of overﬁtting.

There is another shortcoming with variable ranking in a decision tree:
variable masking. Variable xj may be just a little worse than variable xi
and would end up never being picked in the decision tree growing process.
Variable xj would then be ranked as irrelevant. But if xi were removed,
then xj would become very relevant. Note that this is not important in
terms of pure performance of the tree: it did ﬁnd the optimal way to use
both variables in this particular training.
If trying to learn something
from the tree structure on the other hand, like deriving selection rules, this
phenomenon will interfere with the potential understanding.

There is a solution to this feature, called surrogate splits [6]. For each
split, a comparison is made between training events that pass or fail the
optimal split and events that pass or fail a split on another variable. The
split that mimics best the optimal split is called the surrogate split. This
can be taken into consideration when ranking variables. It has applications
in case of missing data: the optimal split can be replaced by the surrogate
split.

All in all, variable rankings should never be taken at face value. They

do provide valuable information but should not be over-interpreted.

3.4.3. Permutation importance

The shortcomings of MDI discussed above are partially addressed with a
diﬀerent technique called permutation importance or mean decrease ac-
curacy (MDA). While MDI mostly works for decision trees, permutation
importance is suited for all models using tabular data. It is deﬁned as the
decrease of performance of an already trained model when applying it on a
sample after randomly shuﬄing a single discriminating variable [20]. If the
variable is of any use, the performance should decrease when submitted to
this noisy input, and more so if the tree relies heavily on this feature for its
prediction. Repeating this for all input variables, the importance of each of
them can be ranked. The operation can be done multiple times, shuﬄing
each variable diﬀerently, in order to get a mean value and uncertainty on

21

variable importance. As with MDI, the measured importance is not telling
anything about the intrinsic merit of a single variable (in terms of physics
meaning for instance), but is rather a measure of its importance for this
particular training.

Another advantage of this approach is that it can be applied on the
validation set as well. Variables that are important on the training set but
not on the validation set may be a source of overﬁtting.

As with MDI however, correlations may hide the intrinsic performance
of a variable. If two variables are correlated and only one is shuﬄed, the
proper information is still accessible, giving a lower importance to both.
Once again, interpreting variable rankings must be done with care.

3.4.4. Choosing variables

It may sound obvious that only well discriminating variables should be used
as input features to the decision tree training. It is nevertheless not trivial
to achieve: variables are often correlated, they come in large numbers, and
can be more or less discriminating in various regions of the input-feature
phase space. The decision tree will isolate sub-regions, whose properties
are not readily available when measuring any kind of discrimination in the
full training set.

Brute force is a possibility: with a limited number of N features, train
all possible combinations of N , N − 1, etc., variables, and pick the best
one according to some metric (see Sec. 2.5). In reality this becomes quickly
impractical.

Instead, a commonly used approach in high-energy physics is backward
elimination [21], which starts from the full list of N variables used to train
a tree (TN ). Then train all decision trees with N − 1 variables and keep
the best performing one on the validation set (TN −1). Starting from these
N − 1 variables, train all decision trees with N − 2 variables to build TN −2,
and so on. Usually the performance of tree Tk will decrease with k, and it
is up to the analyser to decide how much performance to lose compared to
getting a simpler (possibly more robust) tree. This is the usual trade-oﬀ of
cost and complexity.

The selection can also be done in reverse, starting from k = 1 vari-
able, training all trees with k + 1 variables, keeping the best one on the
validation set and moving to k + 2 variables, until k = N (forward greedy
selection [21]). The advantage is that one can stop adding variables once the
performance curve seems to saturate. It is on the other hand not equivalent

22

Y. Coadou

to backward elimination, as it may miss powerful variable combinations.

It can be tempting to train a tree with many variables and then remove
the lowest ranked. Although quicker, it will most certainly be suboptimal
because of the shortcomings of such rankings, as described in Sec. 3.4.2 and
Sec. 3.4.3. The ranking is only relevant to the corresponding tree, and as
soon as one of the variables is removed the others may be reshuﬄed.

3.5. Limitations

Despite all the nice features presented above, decision trees are known to be
relatively unstable. If trees are too optimised for the training sample, they
may not generalise very well to unknown events, as they would depend on
the training sample (see Sec. 3.5.1). This can be mitigated with pruning,
described in Sec. 3.5.2. Combining several classiﬁers can also improve the
overall performance, as shown in Sec. 3.5.3.

3.5.1. Training sample composition

A small change in the training sample can lead to drastically diﬀerent tree
structures (high variance), rendering the physics interpretation a bit less
straightforward. As such, a decision tree is not stable, where stability means
that a slight change of the inputs does not change much the output [21].
For suﬃciently large training samples, the performance of these diﬀerent
trees will be equivalent, but on small training samples variations can be
very large. This does not give too much conﬁdence in the result.

Moreover a decision tree output is by nature discrete, limited by the
purities of all leaves in the tree. To decrease the discontinuities the tree
size and complexity has to increase, which may not be desirable or even
possible. Then the tendency is to have spikes in the output distribution at
speciﬁc purity values, or even two delta functions at ±1 if using a binary
answer rather than the purity output.

3.5.2. Pruning a tree

When growing a tree, each node contains fewer and fewer events, leading
to an increase of the statistical uncertainty on each new split. The tree will
tend to become more and more specialised, focusing on properties of the
training sample that may not reﬂect the expected result, had there been
inﬁnite statistics to train on. Its variance increases.

23

A ﬁrst approach to mitigate this eﬀect and keep the variance under
control, sometimes referred to as pre-pruning, has already been described
in Sec. 3, using stopping conditions. The limitation is that requiring too big
a minimum leaf size or too much of an improvement may prevent further
splitting that could be very beneﬁcial later on.

Another approach consists in building a very large tree and then cutting
irrelevant branches (which target too closely the training sample and would
not generalise well) by turning an internal node and all its descendants into
a leaf, removing the corresponding subtree. This is post-pruning, or simply
pruning.

There are many diﬀerent pruning algorithms available. Expected error
pruning [22] starts from a fully grown tree and compares the expected error
of a node to the weighted sum of expected errors from its children. If the
expected error of the node is less than that of the children, then the node
is pruned. This does not require a separate pruning sample. With reduced
error pruning [22] the misclassiﬁcation rate on a pruning sample for the
full tree is compared to the misclassiﬁcation rate when a node is turned
into a leaf.
If the simpliﬁed tree has better performance, the subtree is
pruned. Finally cost–complexity pruning is part of the CART algorithm [6]
and the most used. Starting from a fully grown tree, the cost–complexity
is computed as the sum of misclassiﬁcation rate and a term proportional
to the number of nodes in the tree (the complexity part, penalising larger
trees). A sequence of decreasing cost–complexity subtrees is generated, and
their misclassiﬁcation rate on the pruning sample is computed. It will ﬁrst
decrease, and then go through a minimum before increasing again. The
optimally pruned tree is the one corresponding to the minimum.

It should be noted that the best pruned tree may not be optimal or
necessary when part of a forest of trees, such as those introduced in the
next Sections.

3.5.3. Ensemble learning

Pruning is helpful in maximising the generalisation potential of a single de-
cision tree. It nevertheless does not address other shortcomings of trees like
the discrete output or lack of stability. A way out is to proceed with aver-
aging several trees, with the added potential bonus that the discriminating
power may increase. Such approaches belong to the general theoretical
framework of ensemble learning [23]. Many averaging techniques have been
developed. Bagging, boosting and random forests are such techniques and

24

Y. Coadou

will be described in the following Sections.

The power of ensemble learning resides in the much richer description
of the input patterns when using several classiﬁers simultaneously.
It is
applicable to other machine learning techniques than decision trees. As
shown in the example of Fig. 6(a) in a simple 2D case, a classiﬁer may
split the space in two (partitions 1/2/3), but three classiﬁers each doing
this can possibly give more complete information about seven regions, each
region being represented by three numbers (C1/C2/C3). When all three
classiﬁers give the same answer, the conﬁdence increases. Using decision
trees as in Fig. 6(b), three simple decision trees give a crude separation of
classes 1 and 2, while averaging them produces a decision contour that is
much closer to the actual class separation.

(a)

(b)

Fig. 6.
decision trees and their combination [24].

(a) Description of 2D space combining three discriminants. (b) Three separate

4. Boosted decision trees

As will be shown in this section, the boosting algorithm has turned into
a very successful way of improving the performance of any type of classi-
ﬁer, not only decision trees. After a short history of boosting in Sec. 4.1,
the generic algorithm is presented in Sec. 4.2 and speciﬁc implementations
(AdaBoost and gradient boosting) are described in Secs. 4.3 and 4.4. Boost-
ing is illustrated with a few examples in Sec. 4.5. Other boosting implemen-
tations are shown in Sec. 4.6. The use of boosting for regression rather than
classiﬁcation is presented in Sec. 4.7. Finally the application of boosted de-
cision trees in high-energy physics, where it is so far the machine learning
algorithm of choice, is illustrated in Sec. 4.8.

Partition 1C3=0C1=1C2=1C3=0C1=0C2=0C3=0C1=0C2=1C3=0C1=1C2=1C3=1C1=1C2=0C3=1C1=1C2=1C3=1C1=0Partition 3Partition 2C2=025

4.1. Introduction

The ﬁrst provable algorithm of boosting was proposed in 1990 [25].
worked in the following way:

It

• train a classiﬁer T1 on a sample of N events;
• train T2 on a new sample with N events, half of which were misclassiﬁed

by T1;

• build T3 on events where T1 and T2 disagree.

The boosted classiﬁer was deﬁned as a majority vote on the outputs of T1,
T2 and T3.

Following up on this idea boosting by majority [26] was introduced in
It consisted in combining many learners with a ﬁxed error rate.
1995.
This was an impractical prerequisite for a viable automated algorithm,
but was a stepping stone to the ﬁrst functional boosting algorithm, called
AdaBoost [27].

Boosting, and in particular boosted decision trees, have become increas-
ingly popular in high-energy physics and are extensively used in physics
analyses and object identiﬁcation at the Tevatron and the LHC (see Sec. 4.8
for a few examples).

4.2. Boosting algorithm

It is hard to make a very good discriminant, but relatively easy to make
simple ones which are certainly more error-prone (high bias) but are still
performing at least marginally better than random guessing. Such discrim-
inants are called weak classiﬁers. The goal of boosting is to combine such
weak classiﬁers into a new, more stable one, with a smaller error rate (with
lower bias than the individual classiﬁers) and better performance.

Consider a training sample Tk containing Nk events. The ith event is
associated with a weight wk
i , a vector of discriminating variables (cid:126)xi and a
class label yi = +1 for signal, −1 for background. The pseudocode for a
generic boosting algorithm is:
Initialise T1
for k in 1..Ntree

train classifier Tk on Tk
assign weight αk to Tk
modify Tk into Tk+1

The boosted output is some function F (T1, .., TNtree ), typically a

26

Y. Coadou

weighted average:

F (i) =

Ntree(cid:88)

k=1

αkTk((cid:126)xi).

Thanks to this averaging, the output becomes quasi-continuous, miti-

gating one of the limitations of single decision trees (see Sec. 3.5.1).

Note that in this process, once a particular tree is trained it is never
modiﬁed, but just added to the mix. This is a diﬀerent approach from,
e.g., neural networks, in which the same weights are repeatedly updated
over epochs to converge towards the ﬁnal classiﬁer.

4.3. AdaBoost

One particularly successful implementation of the boosting algorithm is
AdaBoost [27]. AdaBoost stands for adaptive boosting, referring to the fact
that the learning procedure adjusts itself to the training data in order to
classify it better. There are many variations for the actual implementation,
and it is the most common boosting algorithm. It typically leads to better
results than without boosting, up to the Bayes limit as will be seen later.
An actual implementation of the AdaBoost algorithm works as follows.
After having built tree Tk, events in the training sample Tk that are mis-
classiﬁed by Tk should be checked, hence deﬁning the misclassiﬁcation rate
R(Tk). In order to ease the math, let us introduce some notations. Deﬁne
I : X → I(X) such that I(X) = 1 if statement X is true, and 0 otherwise.
A function can now be deﬁned that tells whether an event is misclassiﬁed
by Tk. In the decision tree output convention of returning only {±1} it
gives:

isMisclassiﬁedk(i) = I(cid:0)yi × Tk(i) ≤ 0(cid:1),
while in the purity output convention (with a critical purity of 0.5) it leads
to:

isMisclassiﬁedk(i) = I(cid:0)yi × (Tk(i) − 0.5) ≤ 0(cid:1).

The misclassiﬁcation rate is now:
(cid:80)Nk

R(Tk) = εk =

i=1 wk

i × isMisclassiﬁedk(i)

.

(cid:80)Nk

i=1 wk
i

This misclassiﬁcation rate can be used to derive a weight associated to tree
Tk:

αk = β × ln

1 − εk
εk

,

27

where β is a free parameter to adjust the strength of boosting (set to one
in the original algorithm). Similarly to the naming convention of other
machine learning algorithms, it can be seen as a learning rate or shrinkage
coeﬃcient and drives how aggressive boosting should be.

The core of the AdaBoost algorithm resides in the following step: each
event in Tk has its weight changed in order to create a new sample Tk+1
such that:

wk

i → wk+1

i = wk

i × eαk·isMisclassiﬁedk(i).

This means that properly classiﬁed events are unchanged from Tk to
Tk+1, while misclassiﬁed events see their weight increased by a factor eαk .
The next tree Tk+1 is then trained on the Tk+1 sample. This next tree will
therefore see a diﬀerent sample composition with more weight on previously
misclassiﬁed events, and will therefore try harder to classify properly diﬃ-
cult events that tree Tk failed to identify correctly, while leaving alone those
events that previous iterations can handle properly. The ﬁnal AdaBoost
result for event i is:

T (i) =

1
(cid:80)Ntree
k=1 αk

Ntree(cid:88)

k=1

αkTk(i).

As an example, assume for simplicity the case β = 1. A not-so-good
classiﬁer, with a misclassiﬁcation rate ε = 40% would have a corresponding
α = ln 1−0.4
0.4 = 0.4. All misclassiﬁed events would therefore get their weight
multiplied by e0.4 = 1.5, and the next tree will have to work a bit harder
on these events. Now consider a good classiﬁer with an error rate ε = 5%
and α = ln 1−0.05
0.05 = 2.9. Misclassiﬁed events get a boost of e2.9 = 19 and
will contribute decisively to the structure of the next tree! This shows that
being failed by a good classiﬁer brings a big penalty.

It can be shown [28] that the misclassiﬁcation rate ε of the boosted

result on the training sample is bounded from above:

ε ≤

Ntree(cid:89)

k=1

2(cid:112)εk(1 − εk).

If each tree has εk (cid:54)= 0.5, that is to say, if it does better than random
guessing, then the conclusion is quite remarkable: the error rate falls to
zero for a suﬃciently large Ntree. A corollary is that the training data is
overﬁt.

Overtraining is usually regarded as a negative feature. Does this mean
that boosted decision trees are doomed because they are too powerful on the

28

Y. Coadou

training sample? Not really. As shown in Sec. 2.6 what matters most is not
the error rate on the training sample, but rather the error rate on the testing
sample. In the case of Fig. 3(a) or Fig. 3(b) boosting should stop when
the minimum is reached (early stopping). It has however been routinely
observed [29–31] that boosted decision trees often do not go through such a
minimum, but rather tend towards a plateau in testing error (see Fig. 3(c)).
Boosting could be stopped after having reached this plateau.

In a typical high-energy physics problem, the error rate may not even
be what should be optimised. A good ﬁgure of merit on the testing sam-
ple would rather be the signiﬁcance. Figure 7(a) illustrates this behaviour,
showing how the signiﬁcance saturates with an increasing number of boost-
ing cycles. Arguably one could stop before the end and save resources, but
at least the performance does not deteriorate with increasing boosting.

(a)

(b)

(c)

(d)

Fig. 7. Behaviour of boosting. (a) Signiﬁcance as a function of the number of boosted
trees.
(b) Signal eﬃciency vs. background eﬃciency for single and boosted decision
trees, on the training and testing samples. (c) Misclassiﬁcation rate of each tree as a
function of the number of boosted trees. (d) Weight of each tree as a function of the
number of boosted trees.

Number of treesSignificance33.544.555.5Cross section significanceBackground efficiency00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91Signal efficiency00.10.20.30.40.50.60.70.80.91Signal efficiency vs. background efficiency Single tree on testing sampleBoosted trees on testing sampleSingle tree on training sampleBoosted trees on training sampletestingtrainingNumber of treesMisclassification rate00.10.20.30.40.5Misclassification rate for each treeNumber of treeska00.050.10.150.20.25kaTree weight 29

Another typical curve to optimise is the signal eﬃciency vs. the back-
ground eﬃciency (the ROC curve, see Sec. 2.5.1). Figure 7(b) clearly exem-
pliﬁes this interesting property of boosted decision trees. The performance
is clearly better on the training sample than on the testing sample (the
training curves are getting very close to the upper left corner of perfect
separation), with a single tree or with boosting, a clear sign of overtrain-
ing. But the boosted tree is still performing better than the single tree on
the testing sample, proof that it does learn something more than memoris-
ing the training sample.

No clear explanation has emerged as to why boosting leads to such
features, with typically no loss of generalisation performance due to over-
training, but some ideas have come up. It may have to do with the fact that
during the boosting sequence, the ﬁrst tree is the best while the others are
successive minor corrections, which are given smaller weights. This is shown
in Fig. 7(c) and Fig. 7(d), where the misclassiﬁcation rate of each new tree
separately is actually increasing, while the corresponding tree weight is de-
creasing. This is not surprising: during boosting the successive trees are
specialising on speciﬁc event categories, and can therefore not perform as
well on other events. So the trees that lead to a perfect ﬁt of the training
data are contributing very little to the ﬁnal boosted decision tree output
on the testing sample. When boosting decision trees, the last tree is not an
evolution of the ﬁrst one that performs better, quite the contrary. The ﬁrst
tree is typically the best, while others bring dedicated help for misclassiﬁed
events. The power of boosting does not rely in the last tree in the sequence,
but rather in combining a suite of trees that focus on diﬀerent events.

A probabilistic interpretation of AdaBoost was proposed [31] which
gives some insight into the performance of boosted decision trees. It can
be shown that for a boosted output T ﬂexible enough:

eT (i) =

p(S|i)
p(B|i)

.

This means that the AdaBoost algorithm will tend towards the Bayes clas-
siﬁer, the maximum reachable separation.

Finally AdaBoost performance and its tendency to generalise well de-
spite matching very closely the training data (to the extent that in many
documented cases, to keep boosting even after the training error has reached
zero still improves the performance on the testing sample [29], in the inter-
polation regime [15]) have been qualitatively understood with the margins
explanation [29, 32]. A classiﬁer can be more sure of some predictions than

30

Y. Coadou

of others (recall Fig. 6(a)), and could then generalise better. By boost-
ing, AdaBoost tends to increase the margins on the training set, even after
reaching zero training error. For each event, the margin accounts for the
separability between classes, measured by the proportion of trees that mis-
classify each event. For event x with truth label y the margin y × T (x) for
boosted decision tree T is:

y × T (x) =

y
(cid:80)Ntree
k=1 αk

=

1
(cid:80)Ntree
k=1 αk

Ntree(cid:88)

k=1




αkTk(x)

(cid:88)

αk −

(cid:88)



αk

 ,

k:y=Tk(x)

k:y(cid:54)=Tk(x)

that is, the diﬀerence between the weights of single trees that classify x
correctly and the weights of trees that misclassify x. Boosting more means
adding small corrections that tend to increase the margin for each event.
This increases the conﬁdence in the prediction, more likely to be correct.
It makes a link with support vector machines [33], although this did not
bring great insights to improve AdaBoost in the end.

This shortcoming suggests that there may be other explanations, as dis-
cussed in Ref. [15], focusing on the interpolation regime when the training
error has already reached zero but boosting further still leads to testing
error improvement (better generalisation). The combination of large trees
focusing on extremely local neighbourhoods of the training dataset and av-
eraging over a large number of trees seems to prevent overﬁtting eﬃciently.
This has been interpreted in the more general framework of double de-
scent risk curve [16]. With boosting, the interpolating regime behaviour
(see Fig. 3(d)) may kick in even before the interpolating threshold, possi-
bly explaining why typical boosted decision tree training curves look like
Fig. 3(c).

4.4. Gradient boosting

While trying to understand how AdaBoost and other boosting algorithms
work, they were originally recast in the statistical framework of arcing al-
gorithms (an acronym for adaptive reweighting and combining) [34, 35].
At each step, a weighted minimisation is performed followed by a recom-
putation of the classiﬁer and weighted input. This was further developed
to become gradient boosting [30]. Boosting is formulated as a numerical
optimisation problem, trying to minimise the loss function by adding trees

31

using a gradient descent procedure rather than giving a higher weight to
misclassiﬁed events.

Formally, consider a model F built iteratively, its imperfect instance
at step k being Fk. Fk is therefore an approximation of the best possible
model (in some cases Fk(x) (cid:54)= y), which is to be improved at the next
iteration. This is achieved by adding a new component hk such that:

or equivalently:

Fk+1(x) = Fk(x) + hk(x) = y,

hk(x) = y − Fk(x).

Rather than training Fk+1 a new classiﬁer can be trained to ﬁt the residual
y − Fk(x), which corresponds to the part that the current model Fk cannot
treat correctly. If Fk+1(x) is still not satisfactory, new iterations can be
ﬁtted.

The link with gradient descent is explicit when considering the partic-
ular case of the mean squared error (MSE) loss function (a typical case for
regression problems, see Sec. 4.7):

LMSE(x, y) =

1
2

(y − Fk(x))2 .

Minimising the loss J = (cid:80)

i LMSE(xi, yi) by adjusting all Fk(xi) leads to:

∂J
∂Fk(xi)

=

∂LMSE(xi, yi)
∂Fk(xi)

= Fk(xi) − yi.

Residuals can therefore be interpreted as negative gradients:

hk(xi) = yi − Fk(xi) = −

∂J
∂Fk(xi)

.

The concept can be generalised to any diﬀerentiable loss function instead of
MSE. For instance AdaBoost corresponds to an exponential loss e−Fk(x)y.
There are several variants of gradient boosting algorithms on the mar-
ket. Techniques presented in Sec. 5 with subsampling of the training set
and tree parameters can be used (in particular a bagging-like approach
without replacement), leading to stochastic gradient boosting [36]. These
regularisation techniques help prevent overﬁtting.

4.5. Boosting examples

The examples of this section illustrate typical behaviours of boosted deci-
sion trees.

32

Y. Coadou

4.5.1. The XOR problem

The XOR problem is a small version of the checkerboard, illustrated in
Fig. 8. With enough statistics (Fig. 8(a) and Fig. 8(c)), even a single tree
is already able to ﬁnd more or less the optimal separation, so boosting
cannot actually do much better.

The exercise can be repeated, this time with limited statistics (Fig. 8(b)
and Fig. 8(d)). Now a single tree is not doing such a good job anymore.
Boosted decision trees, on the other hand, are doing almost as well as with
full statistics, separating almost perfectly signal and background. This illus-
trates very clearly how the combination of weak classiﬁers (see for instance
the lousy performance of the ﬁrst tree) can generate a high performance
discriminant with a boosting algorithm.

4.5.2. Number of trees and overtraining

This example uses a highly correlated dataset, shown in Fig. 9(a).

Figure 9(b) compares the performance of a single decision tree and
boosted decision trees with an increasing number of trees (from 5 to 400).
All other parameters are kept to their default value in the TMVA pack-
age [37]. The performance of the single tree is not so good, as expected
since the default parameters make it very small, with a depth of 3 (it should
be noted that a single bigger tree could solve this problem easily). Increas-
ing the number of trees improves the performance until it saturates in the
high background rejection and high signal eﬃciency corner. Adding more
trees does not seem to degrade the performance, the curve stays in the
optimal corner. Looking at the contours in Fig. 9(a) it wiggles a little for
larger boosted decision trees, as they tend to pick up features of the training
sample. This is overtraining.

Another sign of overtraining also appears in Fig. 10, showing the output
of the various boosted decision trees for signal and background, both on
the training and testing samples:
larger boosted decision trees tend to
show diﬀerences between the two samples (as quantiﬁed by a Kolmogorov–
Smirnov (KS) test in the ﬁgures, especially Fig. 10(f)), as they adjust to
peculiarities of the training sample that are not found in an independent
testing sample. The output acquires a ‘better’ shape with more trees, really
becoming quasi-continuous, which would allow to cut at a precise eﬃciency
or rejection.

Both ﬁgures do exhibit clear signs of overtraining, but is it really an
issue? As mentioned before (see Sec. 2.6) what really matters in the end

33

(a)

(b)

(c)

(d)

Fig. 8. The XOR problem. Signal is in blue, background in red. The left column (a
and c) uses suﬃcient statistics, while the right column has a limited number of training
events. The top plots (a and b) show the signal and background distributions as well as
the criteria used by the ﬁrst decision tree. Bottom plots (c, d) illustrate the background
rejection vs. signal eﬃciency curves for the ﬁrst decision tree (red) and for the boosted
decision trees (black), all run on the same testing events.

√

is the performance in data analysis and on the testing sample. One way
s + b (see
to evaluate this is to compute the maximum signiﬁcance s/
Sec. 2.5.2). It is shown in Fig. 11(a) for the same boosted decision trees as
shown in Fig. 10, with increasing number of trees. The best signiﬁcance is
actually obtained with the 400-tree boosted decision tree, following what
was described at the end of Sec. 4.3. To be fair, the performance is very
similar already with 10 trees. Now, comparing the outputs in Fig. 10,
if interested in a smoother result, 10 trees might not be enough, but 50
would probably do, without the overhead of eight times more trees. Such
a choice should in any case not be made based on overtraining statements
comparing performance on the training and testing samples (as some are

x00.20.40.60.81y00.20.40.60.81y:xx00.20.40.60.81y00.20.40.60.81y:xSignal efficiency00.10.20.30.40.50.60.70.80.91Background rejection0.20.30.40.50.60.70.80.91Signal efficiency00.10.20.30.40.50.60.70.80.91Background rejection0.20.30.40.50.60.70.80.91MVA Method:BDTDTBackground rejection versus Signal efficiencySignal efficiency00.10.20.30.40.50.60.70.80.91Background rejection0.20.30.40.50.60.70.80.91Signal efficiency00.10.20.30.40.50.60.70.80.91Background rejection0.20.30.40.50.60.70.80.91MVA Method:BDTDTBackground rejection versus Signal efficiency34

Y. Coadou

(a)

(b)

(a) 2D dataset and decision contour corresponding to several discriminants. (b)
Fig. 9.
Background rejection vs. signal eﬃciency curves for a single decision tree (dark green)
and boosted decision trees with an increasing number of trees (5 to 400).

tempted to do, seeing an increasing disagreement, quantiﬁed by the KS
test, between outputs on the training and testing samples), but rather on
ﬁnal expected physics performance (the ﬁnal number of the analysis, for
instance the signiﬁcance from the complete statistical analysis, possibly
including systematic uncertainties). Boosted decision trees are often in
the situation described in Fig. 3(c), meaning that their performance is not
decreasing when boosting longer, even as the discrepancy in performance
between train and test keeps increasing.

This example also illustrates the performance of each tree in a boosting
sequence. Figure 11(b) shows the rapid decrease of the weight αk of each
tree, while at the same time the corresponding misclassiﬁcation rate εk
of each individual tree increases rapidly towards just below 50%, that is,
random guessing (Fig. 11(c)). It conﬁrms that the best trees are the ﬁrst
ones, while the others are only minor corrections.

4.6. Other boosting algorithms

AdaBoost is but one of many boosting algorithms. It is also referred to
as discrete AdaBoost to distinguish it from other AdaBoost ﬂavours. The
Real AdaBoost algorithm [31] deﬁnes each decision tree output as:

Tk(i) = 0.5 × ln

pk(i)
1 − pk(i)

,

var0-0.8     -0.4  0        0.4      0.8    1.2var1-1-0.500.51DTBDT5BDT10BDT50BDT100BDT400Signal efficiency00.10.20.30.40.50.60.70.80.91Background rejection0.20.30.40.50.60.70.80.91MVA Method:BDT100BDT50BDT400BDT10BDT5DTBackground rejection versus Signal efficiency35

(a) Single decision tree

(b) 5 trees

(c) 10 trees

(d) 50 trees

(e) 100 trees

(f) 400 trees

Fig. 10. Comparison of the output on training (markers) and testing (histograms) signal
(blue) and background (red) samples for boosted decision trees with 1, 5, 10, 50, 100
and 400 trees (from top left to bottom right). The Kolmogorov–Smirnov test quantiﬁes
the (dis)agreement between training and testing outputs.

(a)

(b)

(c)

Fig. 11.
each tree. (c) Error fraction of each tree (0.5 means random guessing).

(a) Maximum signiﬁcance of all boosted decision trees. (b) Boost weight of

where pk(i) is the purity of the leaf on which event i falls. Events are
reweighted as:

i = wk
and the boosted result is T (i) = (cid:80)Ntree
itBoost (with a logistic function) [31] are other variations.

i × e−yiTk(i)

i → wk+1

wk

k=1 Tk(i). Gentle AdaBoost and Log-

DT response 1 0.500.51dx / (1/N) dN0246810121416182022Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability =     1 (    1)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: DTBDT5 response 1 0.500.51dx / (1/N) dN0123456789Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability = 0.837 (    1)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: BDT5BDT10 response 0.8 0.6 0.4 0.200.20.40.60.81dx / (1/N) dN0123456Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability = 0.999 (0.388)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: BDT10BDT50 response 0.6 0.4 0.200.20.40.60.8dx / (1/N) dN00.511.522.53Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability = 0.974 (0.608)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: BDT50BDT100 response 0.6 0.4 0.200.20.40.6dx / (1/N) dN00.511.522.533.54Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability = 0.952 ( 0.95)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: BDT100BDT400 response 0.4 0.200.20.4dx / (1/N) dN012345Signal (test sample)Background (test sample)Signal (training sample)Background (training sample)Kolmogorov Smirnov test: signal (background) probability = 0.491 (0.639)U/O flow (S,B): (0.0, 0.0)% / (0.0, 0.0)%TMVA overtraining check for classifier: BDT400110100Number of trees2828.52929.53030.5SignificanceSignificance vs number of treesboost weight051015202530050100150200250300350400450AdaBooost weight distribution#tree050100150200250300350400boost weight0246810Boost weights vs tree#tree050100150200250300350400error fraction00.10.20.30.40.50.6error fraction vs tree number#tree050100150200250300350400#tree nodes024681012141618Nodes before/after pruningboost weight051015202530050100150200250300350400450AdaBooost weight distribution#tree050100150200250300350400boost weight0246810Boost weights vs tree#tree050100150200250300350400error fraction00.10.20.30.40.50.6error fraction vs tree number#tree050100150200250300350400#tree nodes024681012141618Nodes before/after pruning36

Y. Coadou

ε-Boost, also called shrinkage [30], consists in reweighting misclassiﬁed
events by a ﬁxed factor e2ε rather than the tree-dependent αk factor of
AdaBoost. ε-LogitBoost [31] is reweighting them with a logistic function
e−yiTk (i)
1+e−yiTk (i) . ε-HingeBoost [2] is only dealing with misclassiﬁed events:

wk

i → wk+1

i = I(yi × Tk(i) ≤ 0).

Finally the adaptive version of the ‘boost by majority’ [26] algorithm is
called BrownBoost [38]. It works in the limit where each boosting iteration
makes an inﬁnitesimally small contribution to the total result, modelling
this limit with the diﬀerential equations that govern Brownian motion.

4.7. Boosted regression trees

From their very introduction [6], trees have been considered for classiﬁ-
cation (decision trees) and for regression (regression trees), where instead
of identifying ‘signal-like’ or ‘background-like’ regions of phase space, tree
leaves each contain a single real value supposed to approach the target
function.

During tree building for regression, the maximisation of the decrease
of impurity in decision trees is replaced by the reduction of the standard
deviation or of the mean squared error:

d(t) =

1
Nt

(cid:88)

(y − ˆyt)2,

Nt

for a node t with Nt events, regression target y of each event in the node
and mean value ˆyt of regression targets of all events in the node. Another
typical choice for d is the mean absolute error:

1
Nt

(cid:88)

Nt

|y − median(y)t| .

Constructing a regression tree is about ﬁnding the attribute that return
the highest reduction in d (i.e., the most homogeneous nodes) when going
from node t to nodes tP and tF (see Sec. 3.3 for notations):

∆d(S, t) = d(t) − pP · d(tP ) − pF · d(tF ).

The splitting stops when nodes become too small or when their internal
variation is suﬃciently small. The regression tree output is the mean (or
median if using the mean absolute error) value of the training events in the
corresponding (leaf) node. So a regression tree partitions the feature space

37

of input variables into hyperrectangles and then ﬁts a constant inside each
box.

When boosting regression trees, there are no longer properly and
wrongly classiﬁed events, so the misclassiﬁcation rate cannot be com-
Instead the average loss (cid:104)Lk(cid:105) after the kth
puted to reweight events.
tree is computed over the training sample, and the boosting quantity
βk = (cid:104)Lk(cid:105)/ (cid:0)1 − (cid:104)Lk(cid:105)(cid:1) is derived. The reweighting of events is then com-
puted based on their individual loss Lk(i):

wk

i → wk+1

i = wk

i × β1−Lk(i)

k

.

The training process is then similar to that of boosted decision trees,
and the ﬁnal prediction of the ﬁtted value is the weighted average of all
tree outputs.

4.8. Boosted decision trees in high-energy physics

Boosted decision trees have become very popular in high-energy physics.
A few usage examples are presented in Sec. 4.8.1. Their proper usage also
means addressing issues linked to systematic uncertainties, as reported in
Sec. 4.8.2.

4.8.1. Use cases

The MiniBooNe experiment at Fermilab, searching for neutrino oscilla-
tions, was the ﬁrst in the ﬁeld to compare the performance of diﬀerent
boosting algorithms and artiﬁcial neural networks for analysis and particle
identiﬁcation [1, 2], on Monte Carlo samples. Trees with up to 120 vari-
ables were tested, with diﬀerent boosting algorithms and up to thousands
of trees. These studies introduced boosted decision trees in the particle
physics world.

The concept of boosted decision trees was picked up by the D0 exper-
iment at Fermilab, leading to the ﬁrst evidence (and then observation) of
single top quark production in Tevatron data [3, 4]. Among the 49 variables
used, some had very similar deﬁnitions (like the scalar sum of transverse
momentum of various jets), which was beneﬁcial as not all of them suﬀer
from the same mismeasurements on an event-by-event basis. Boosted de-
cision trees happened to perform slightly better than two other techniques
used: the matrix element calculation and Bayesian neural networks. With-
out such advanced techniques, the signal could not have been seen with the

38

Y. Coadou

dataset available at the time: the total uncertainty on the model predic-
tion was much larger than the expected signal, as illustrated in Fig. 12(a).
This also means that no single distribution (apart from the boosted deci-
sion tree output shown in Fig. 12(b)) could really show the new observed
process, leading to scepticism in the community (‘I want to see a mass
peak!’ is a common argument, reﬂecting on the fact that people are more
conﬁdent in the result if they can see the signal in a physical distribution).
Various cross-checks were performed to increase the degree of belief in the
ﬁnal outcome (removing top-quark-mass-related variables during training,
validating the description of the boosted decision tree output in regions de-
pleted in signal, analysing the shape of other variables after selecting low or
high boosted decision tree output events enriched in background or signal
events as shown in Fig. 12(c) and Fig. 12(d), respectively, etc.).

Since then, boosted decision trees have become a bread and butter tech-
nique in high energy physics and are extensively used in physics analyses (to
extract their tiny signal from large backgrounds or distinguish between dif-
ferent signals) and object identiﬁcation at the Tevatron or the LHC. In the
ATLAS experiment τ -lepton identiﬁcation [39] and ﬂavour tagging [7] used
boosted decision trees in Run 2, and the τ -lepton energy is estimated with
boosted regression trees. The LHCb trigger was reoptimised, comparing
the performance of several tree-based algorithms to neural networks [40],
while their muon identiﬁcation performance for the Run 3 of the LHC will
proﬁt from improvements thanks to gradient boosting [41].

The latest result has just been published at the time of writing, report-
ing the ﬁrst evidence for t¯tt¯t production in ATLAS [8], shown in Fig. 1(b).
The one analysis using the most boosted decision trees is probably the
observation of the diphoton decay of the Higgs boson by the CMS exper-
iment [42]. The diphoton vertex is selected with a boosted decision tree,
while another one estimates, event-by-event, the probability for the vertex
assignment to be within 10 mm of the diphoton interaction point. Photons
are identiﬁed with a boosted decision tree, and their energy is corrected
with a boosted regression tree that provides the energy and its associated
uncertainty. Finally several boosted decision trees are used to select the
various signal regions and extract signal from these diﬀerent categories.

Beyond object identiﬁcation and calibration, and ﬁnal discriminant in
physics analyses, boosted decision trees can also be used to reduce the
number of potential object combinations in order to ﬁnd the correct match
between the observed objects in the detector and their probable source of
production. Such a ‘reconstruction BDT’ was used to look for the associated

39

(a)

(b)

(c)

(d)

Fig. 12. Usage of boosted decision trees in physics analysis [4]. (a) A discriminating
variable, with uncertainty larger than the expected signal (in blue). (b) The boosted
decision tree output with much smaller uncertainty. (c, d) Discriminating variable when
selecting only events with low (high) boosted decision tree output, showing background
(signal)-like shape.

production of a Higgs boson and a pair of top quarks, t¯tH(b¯b) [43].

Lately there is a tendency towards deep neural networks and their many
ﬂavours to replace boosted decision trees in the various stages of anal-
ysis [44, 45]. Boosted decision trees nevertheless remain a favourite in
high-energy physics, for their ease of use, high performance out-of-the-box,
limited required tuning of hyperparameters and resilience against overtrain-
ing.

(W) [GeV]TM050100150Yield [Events/10GeV]0100200300 channelμe+1-2 b-tags2-4 jets(a)(W) [GeV]TM050100150Yield [Events/10GeV]0100200300-1DØ 0.9 fbs-channel t-channel  ttW+jets Multijet tb+tqb Decision Tree Output0.60.70.80.91Yield [Events/0.04]-110110210 channelμe+1-2 b-tags2-4 jets(a)tb+tqb Decision Tree Output0.60.70.80.91Yield [Events/0.04]-110110210-1DØ 0.9 fbs-channel t-channel  ttW+jets Multijet (untag1)ηQ(l) x -4-2024Yield [Events/0.3]0204060 channelμe+1-2 b-tags2-4 jetsDT < 0.3(g)(untag1)ηQ(l) x -4-2024Yield [Events/0.3]0204060(untag1)ηQ(l) x -4-2024Yield [Events/0.3]051015(i) channelμe+1-2 b-tags2-4 jetsDT > 0.65(untag1)ηQ(l) x -4-2024Yield [Events/0.3]05101540

Y. Coadou

4.8.2. Systematic uncertainties

There is an a priori, especially among physicists not very familiar with ma-
chine learning techniques, to distrust their output because they are not a
measurable quantity with a physical meaning like an invariant mass. They
are indeed complex variables, but so are for instance energy quantities for
reconstructed particles in the detector. Uncertainties on such ‘basic’ vari-
ables are typically evaluated by varying the value of a requirement, changing
the calibration of objects that go into the variable, etc. The boosted de-
cision tree output (or of any such multivariate technique) is no diﬀerent:
its inputs can be varied according to their know uncertainties (for instance
varying the jet energy scale will have a correlated impact on all discrimi-
nating variables that depend on jets) and their eﬀect propagated through
the boosted decision tree (the shifted inputs will lead to a diﬀerent boosted
decision tree output), to see how much these changes impact the analy-
sis. This gives the size of the uncertainty on the multivariate discriminant
output.

That being said, the Peter Parker principle applies: ‘With great power
comes great responsibility’. Boosted decision trees are very powerful, and
will target small areas of phase space where potentially not all known sys-
tematic uncertainties are strictly valid. Then extra uncertainties may be
needed, not so much on the technique itself but rather due to the fact that
it extracts information from less well-known regions.

Usually boosted decision trees are trained on the nominal Monte Carlo
samples and are therefore completely oblivious to the eﬀect of systematic
uncertainties. This could lead to bad results once they are introduced, if
the boosted decision trees are sensitive to them, and when applied on real
data. One way to possibly mitigate this eﬀect is with one form of data
augmentation, training the boosted decision trees on a mixture of nominal
and systematically shifted events, hence increasing the training statistics
and allowing the boosted decision trees to see other events than the nominal
ones during training to learn their features. The nominal performance
should decrease, but with the hope that systematic uncertainties will have
less of an impact on the ﬁnal measurement. Experience with this approach
is inconclusive. If the physics model is not properly describing the real data,
then the performance will also be aﬀected. It can be partially addressed
with domain adaptation [5] (as described elsewhere in this book).

41

5. Other averaging techniques

As mentioned in Sec. 3.5.3 the key to improving a single decision tree per-
formance and stability is averaging. Other techniques than boosting exist,
some of which are brieﬂy described below. As with boosting, statistical
perturbations are introduced to randomise the training sample, hence in-
creasing the predictive power of the ensemble of trees.

Bagging (Bootstrap AGGregatING) was proposed in Ref. [46]. It consists
in training trees on diﬀerent bootstrap samples drawn randomly with
replacement from the training sample. Events that are not picked for
the bootstrap sample form an ‘out of bag’ validation sample. The
bagged output is the simple average of all such trees, with a reduced
variance compared to individual trees.

Random forests is bagging with an extra level of randomisation [20]. Be-
fore splitting a node, only a random subset of discriminating variables
is considered. The fraction can vary for each split for yet another level
of randomisation.

Trimming is not exactly an averaging technique per se but can be used
in conjunction with another technique, in particular boosting, to speed
up the training process. After some boosting cycles, it is possible that
very few events with very high weight are making up most of the to-
tal training sample weight. Events with very small weights may be
ignored, hence introducing again some minor statistical perturbations
and speeding up the training. ε-HingeBoost is such an algorithm (see
Sec. 4.6).

6. Software

Many implementations of decision trees exist on the market. Some of them,
all open source, are brieﬂy presented below.

The most popular in high-energy physics is TMVA [37], integrated into
ROOT. It includes single decision trees, boosted trees with AdaBoost and
gradient boost, bagging and random forests. Being part of ROOT it is very
straightforward to use within usual analysis frameworks, both in C++ and
It includes tools for data preparation and makes it simple to
Python.
compare performance between many algorithms, not only tree-based ones.
Already mentioned Refs. [7, 8, 39, 42, 43] are but a few examples of TMVA
usage in the ﬁeld.

42

Y. Coadou

Another implementation has gained visibility in high-energy physics:
XGBoost [47]. It entered the ﬁeld after receiving to special HEP meets ML
award during the Higgs boson machine learning challenge (HiggsML) hosted
by Kaggle [11] (described in Chapter 20). It features a high-performing,
scalable gradient boosting implementation, capable of using GPU and large
cluster parallelisation. Instead of the greedy algorithm described in Sec. 3.1,
the authors developed an approximate algorithm that proposes candidate
splitting points according to percentiles of the input variables, and then
maps the variables into buckets according to these splits to ﬁnd the best
solution. Many analyses at the LHC are now using it (see for instance
Ref. [48]).

Other implementations have lower usage in high-energy physics so far
while being used in other ﬁelds. LightGBM (light gradient boosting ma-
chine [49]), originally developed by Microsoft, is competing with XGBoost
in speed, scalability and performance. It builds trees in a very diﬀerent way
from what was presented in this chapter, with a histogram-based decision
tree learning algorithm. Scikit-learn [50] is a very popular machine learning
framework with several tree-related implementations and utilities for data
preparation. Finally CatBoost [51] is a new gradient boosting implemen-
tation from Yandex used in commercial services as well as in high-energy
physics, for instance in LHCb [41].

7. Conclusion

This chapter introduced what decision trees are and how to construct them,
as a powerful multivariate extension of a cut-based analysis. Advantages
are numerous: their training is fast, they lead to human-readable results
(not black boxes) with possible interpretation by a physicist, can deal easily
with all sorts of variables and with many of them, with in the end relatively
few parameters.

Decision trees are, however, not perfect and suﬀer from the piecewise
nature of their output and a high sensitivity to the content of the training
sample. These shortcoming are for a large part addressed by averaging the
results of several trees, each built after introducing some statistical pertur-
bation in the training sample. Among the most popular such techniques,
boosting (and its AdaBoost and gradient boost incarnations) was described
in detail, providing ideas as to why it seems to be performing so well while
being very resilient against overtraining. Other averaging techniques were
brieﬂy presented.

43

Boosted decision trees have now become quite fashionable in high energy
physics. Following the steps of MiniBooNe for analysis and particle iden-
tiﬁcation and D0 for the ﬁrst evidence and observation of single top quark
production, other experiments and analyses are now using them routinely,
in particular at the LHC.

Boosted decision trees are still a very active ﬁeld of development, with
academic groups and private companies testing their limits, providing new
software [47, 49, 51] and using them to target recent issues like resistance
to adversarial attacks (see e.g. [52, 53]).

References

[1] B. P. Roe et al., Boosted decision trees as an alternative to artiﬁcial neural
networks for particle identiﬁcation, Nucl. Instr. Meth. A 543 (2005) 577.

[2] H.-J. Yang, B. P. Roe, and J. Zhu, Studies of boosted decision trees for

MiniBooNE particle identiﬁcation, Nucl. Instr. Meth. A 555 (2005) 370.
[3] D0 Collaboration, Evidence for Production of Single Top Quarks and First

Direct Measurement of |Vtb|, Phys. Rev. Lett. 98 (2007) 181802.

[4] D0 Collaboration, Evidence for production of single top quarks, Phys. Rev.

D 78 (2008) 012005.

[5] S. Ben-David et al., A theory of learning from diﬀerent domains, Mach.

Learn. 79 (2009) 151.

[6] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classiﬁcation

and Regression Trees. Wadsworth, 1984.

[7] ATLAS Collaboration, ATLAS b-jet identiﬁcation performance and

eﬃciency measurement with t¯t events in pp collisions at
Phys. J. C 79 (2019) 970, arXiv:1907.05120 [hep-ex].

√

s = 13 TeV, Eur.

[8] ATLAS Collaboration, Evidence for t¯tt¯t production in the multilepton ﬁnal
√
s = 13 TeV with the ATLAS detector,

state in proton–proton collisions at
Eur. Phys. J. C 80 (2020) 1085, arXiv:2007.14858 [hep-ex].

[9] G. Cowan et al., Asymptotic formulae for likelihood-based tests of new

physics, Eur. Phys. J. C 71 (2011) 1554, arXiv:1007.1727 [physics.data-an].
Erratum: Eur. Phys. J. C 73 (2013) 2501.

[10] L. Moneta et al., The RooStats project, PoS ACAT2010 (2011) 057,

arXiv:1009.1003 [physics.data-an].

[11] C. Adam-Bourdarios et al., The Higgs boson machine learning challenge, in
Proceedings of the NIPS 2014 Workshop on High-energy Physics and
Machine Learning. 2015.

[12] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical

Learning: Data Mining, Inference, and Prediction (2nd edition). Springer
Series in Statistics. Springer, 2009.
https://web.stanford.edu/~hastie/ElemStatLearn/.

[13] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical Learning with
Sparsity: The Lasso and Generalizations. Chapman & Hall/CRC

44

Y. Coadou

Monographs on Statistics and Applied Probability, 2015.
https://web.stanford.edu/~hastie/StatLearnSparsity/.

[14] N. Srivastava et al., Dropout: A Simple Way to Prevent Neural Networks

from Overﬁtting, J. Mach. Learn. Res. 15 (2014) 1929–1958.

[15] A. J. Wyner, M. Olson, J. Bleich, and D. Mease, Explaining the Success of
AdaBoost and Random Forests as Interpolating Classiﬁers, J. Mach. Learn.
Res. 18 (2017) 1.

[16] M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling modern

machine-learning practice and the classical bias–variance trade-oﬀ, PNAS
116 (2019) 15849, arXiv:1812.11118 [stat.ML].

[17] I. Kononenko, Machine learning for medical diagnosis: history, state of the

art and perspective, Artif. Intell. Med. 23 (2001) 89.

[18] V. Podgorelec, P. Kokol, B. Stiglic, and I. Rozman, Decision Trees: An
Overview and Their Use in Medicine, J. Med. Syst. 26 (2002) 445.
[19] C. Gini, Variabilit`a e mutabilit`a, (reprinted in Memorie di Metodologica

Statistica, eds. E. Pizetti and T. Salvemini, Libreria Eredi Virgilio Veschi,
Rome, 1955), 1912.

[20] L. Breiman, Random forests, Mach. Learn. 45 (2001) 5.
[21] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning:

From Theory to Algorithms. Cambridge University Press, 2014.
https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning.

[22] J. R. Quinlan, Simplifying decision trees, Int. J. Man-Mach. Stud. 27

(1987) 221.

[23] J. H. Friedman and B. E. Popescu, Predictive learning via rule ensembles,

Ann. Appl. Stat. 2 (2008) 916, arXiv:0811.1679 [stat.AP].

[24] T. G. Dietterich, Machine learning research: Four current directions, AI

Magazine 18 (1997) 97.

[25] R. E. Schapire, The strength of weak learnability, Mach. Learn. 5 (1990)

197.

[26] Y. Freund, Boosting a Weak Learning Algorithm by Majority, Inf. Comput.

121 (1995) 256.

[27] Y. Freund and R. E. Schapire, Experiments with a New Boosting

Algorithm, in Proceedings of the Thirteenth International Conference on
Machine Learning, ICML’96. 1996.

[28] Y. Freund and R. E. Schapire, A Decision-Theoretic Generalization of

On-Line Learning and an Application to Boosting, J. Comput. Syst. Sci.
55 (1997) 119.

[29] R. E. Schapire and Y. Freund, Boosting: Foundations and Algorithms.

MIT Press, 2012.

[30] J. H. Friedman, Greedy function approximation: A gradient boosting

machine., Ann. Statist. 29 (2001) 1189.

[31] J. H. Friedman, T. Hastie, and R. Tibshirani, Additive logistic regression:

a statistical view of boosting, Ann. Statist. 28 (2000) 337.

[32] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, Boosting the

margin: a new explanation for the eﬀectiveness of voting methods, Ann.
Statist. 26 (1998) 1651.

45

[33] V. N. Vapnik, The Nature of Statistical Learning Theory. Springer, 2000.
[34] L. Breiman, Arcing the Edge, Ann. Prob. 26 (1998) 1683.
[35] L. Breiman, Prediction Games and Arcing Algorithms, Neural Comput. 11

(1999) 1493.

[36] J. H. Friedman, Stochastic gradient boosting, Comput. Stat. Data Anal. 38

(2002) 367.

[37] A. Hoecker et al., TMVA — Toolkit for Multivariate Data Analysis,

arXiv:physics/0703039 [physics.data-an].

[38] Y. Freund, An Adaptive Version of the Boost by Majority Algorithm,

Mach. Learn. 43 (2001) 293–318.

[39] ATLAS Collaboration, Measurement of the tau lepton reconstruction and

identiﬁcation performance in the ATLAS experiment using pp collisions at
√

s = 13 TeV, ATLAS-CONF-2017-029, 2017.

[40] T. Likhomanenko et al., LHCb Topological Trigger Reoptimization, J. Phys.

Conf. Ser. 664 (2015) 082025, arXiv:1510.00572 [physics.ins-det].

[41] L. Anderlini et al., Muon identiﬁcation for LHCb Run 3, arXiv:2008.01579

[hep-ex].

[42] CMS Collaboration, Observation of the diphoton decay of the Higgs boson
and measurement of its properties, Eur. Phys. J. C 74 (2014) 3076,
arXiv:1407.0558 [hep-ex].

[43] ATLAS Collaboration, Search for the standard model Higgs boson produced

√

in association with top quarks and decaying into a b¯b pair in pp collisions
at
arXiv:1712.08895 [hep-ex].

s = 13 TeV with the ATLAS detector, Phys. Rev. D 97 (2018) 072016,

[44] ATLAS Collaboration, Identiﬁcation of hadronic tau lepton decays using

neural networks in the ATLAS experiment, ATL-PHYS-PUB-2019-033,
2019.

[45] ATLAS Collaboration, Deep Sets based Neural Networks for Impact

Parameter Flav our Tagging in ATLAS, ATL-PHYS-PUB-2020-014, 2020.

[46] L. Breiman, Bagging predictors, Mach. Learn. 24 (1996) 123.
[47] T. Chen and C. Guestrin, XGBoost: A Scalable Tree Boosting System,

in

Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’16. Association for
Computing Machinery, 2016. arXiv:1603.02754 [cs.LG].

[48] ATLAS Collaboration, Observation of Higgs boson production in

association with a top quark pair at the LHC with the ATLAS detector,
Phys. Lett. B 784 (2018) 173, arXiv:1806.00425 [hep-ex].

[49] G. Ke et al., LightGBM: A Highly Eﬃcient Gradient Boosting Decision
Tree, in Advances in Neural Information Processing Systems. 2017.
[50] F. Pedregosa et al., Scikit-learn: Machine learning in Python, J. Mach.

Learn. Res. 12 (2011) 2825.

[51] A. V. Dorogush, V. Ershov, and A. Gulin, CatBoost: gradient boosting with

categorical features support, arXiv:1810.11363 [cs.LG].

[52] H. Chen et al., Robustness Veriﬁcation of Tree-based Models, in Advances

in Neural Information Processing Systems. 2019.

[53] M. Andriushchenko and M. Hein, Provably robust boosted decision stumps

46

Y. Coadou

and trees against adversarial attacks, in Advances in Neural Information
Processing Systems. 2019.

