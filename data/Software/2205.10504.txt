2
2
0
2

y
a
M
1
2

]
E
S
.
s
c
[

1
v
4
0
5
0
1
.
5
0
2
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

1

How to Find Actionable Static Analysis Warnings

Rahul Yedida, Hong Jin Kang, Huy Tu, Xueqi Yang, David Lo Fellow, IEEE, Tim Menzies, Fellow, IEEE

Abstract—Automatically generated static code warnings suffer from a large number of false alarms. Hence, developers only take
action on a small percent of those warnings. To better predict which static code warnings should not be ignored, we suggest that
analysts need to look deeper into their algorithms to ﬁnd choices that better improve the particulars of their speciﬁc problem.
Speciﬁcally, we show here that effective predictors of such warnings can be created by methods that locally adjust the decision
boundary (between actionable warnings and others). These methods yield a new high water-mark for recognizing actionable static
code warnings. For eight open-source Java projects (CASSANDRA, JMETER, COMMONS, LUCENE-SOLR, ANT, TOMCAT, DERBY)
we achieve perfect test results on 4/8 datasets and, overall, a median AUC (area under the true negatives, true positives curve) of 92%.

Index Terms—software analytics, static analysis; false alarms; locality, hyperparameter optimization

(cid:70)

1 INTRODUCTION

Static analysis (SA) tools report errors in source code,
without needing to execute that code. This makes them
very popular in industry. For example, the FindBugs tool [4]
of Figure 1 has been downloaded over a million times.
Unfortunately, due to the imprecision of static analysis and
the different contexts where bugs appear, SA tools often
suffer from a large number of false alarms that are deemed
to be not actionable [44]. Hence, developers never act on
most of their warnings [20, 21, 27]. Previous research work
shows that 35% to 91% of SA warnings reported as bugs by
SA tools are routinely ignored by developers [21, 20, 26].

Those false alarms produced by SA tools are a signiﬁcant
barrier to the wide-scale adoption of these SA tools [23, 13].
Accordingly, in 2018 [50], 2020 [54] and 2021 [55], Wang et al.
and Yang et al. proposed data miners that found the subset
of static code warnings that developers found “actionable”
(i.e. those that motivate developers to change the code).
But in 2022, Kang et al. [25] showed that (a) of the 31,000+
records used by Wang et al. and Yang et al., they could only
generate 768 error-free records– which meant all the prior
Wang and Yang et al. results need to be revisited.

When Kang et al. tried to build predictors from the
768 good records, they found that their best-performing
predictors were not effective (e.g., very low median AUCs
of 41%), for details see Table 2. Hence the following remains
an open research question:

RQ1: For detecting actionable static code warnings, what
data mining methods should we recommend?

This paper conjectures that prior work failed to ﬁnd good
predictors because of a locality problem. In the learners used

• R. Yedida, X. Yang and T. Menzies are with the Department of Computer

Science, North Carolina State University, Raleigh, USA.
E-mail: ryedida@ncsu.edu, xyang37@ncsu.edu, timm@ieee.org

• H.J. Kang and D. Lo are with the School of Computing and Information

Systems, Singapore Management University, Singapore.
E-mail: hjkang.2018@smu.edu.sg, davidlo@smu.edu.sg

• H. Tu is with Meta Platforms, Inc. E-mail: huyqtu7@gmail.com

in that prior work, the decision boundary between between
actionable warnings and other was determined by a single
global policy. More speciﬁcally, we conjecture that:

For complex data, global treatments perform worse
than localized treatments which adjust different parts
of the landscape in different ways.

To test this, we use local treatments to adjust the decision
boundary in different ways in different parts of the data.

1) Boundary engineering: adjust the decision boundary near

our data points;

2) Label engineering: control outliers in a local region by

using just a small fraction of those local labels;

3) Instance engineering: addressing class imbalance in local

regions of the data
treatments

4) These

are
engineering to control how we build models.

combined with parameter

We call this combination of treatments GHOST2 (GHOST2
these
extends GHOST [56] which just used one of
treatments). When researchers propose
an intricate
combination of ideas, it is prudent to ask several questions:

RQ2: Does GHOST2’s combination of
boundary and parameter
complexity of the decision boundary?

engineering,

instance,

label,
the

reduce

Later in this paper, we will show evidence that our
proposed methods simpliﬁes the “error landscape” of a data
set (a concept which we will discuss, in detail in §4).

RQ3: Does GHOST2’s use of instance, label, boundary
and parameter treatments improve predictions?

Using data from Kang et al. (768 records from eight
open-source Java projects), we show that GHOST2 was able
to generate excellent predictors for actionable static code
warnings.

RQ4: Are all parts of GHOST2 necessary;
something simpler also achieve the overall goal?

i.e. would

 
 
 
 
 
 
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

2

Fig. 1: Example of a static analysis warning, generated via the FindBugs tool [4].

To answer RQ4, this paper reports an ablation study
that removes one treatment at a time from our four
recommended treatments. For the purposes of recognizing
and avoiding static code analysis false alarms, it will be
shown that, ignoring any part of our proposed solution
leads to worse predictors. Hence, while we do not know
if changes to our design might lead to better predictors, the
ablations study does show that removing anything from that
design makes matters worse.

This work has six key contributions:

1) As a way to address,

in part, the methodological
problems raised by Kang et al. GHOST2 makes its
conclusions using a small percentage of the raw data
(10%). That is, to address the issues of corrupt data
found by Kang et al., we say “use less data” and, for
the data that is used, “reﬂect more on that data”.

2) A case study of successful open collaboration by
software analytics researchers. This paper is joint work
between the Yang et al. and Kang et al. teams from the
United States and Singapore. By recognizing a shared
problem, then sharing data and tools, in eight weeks
these two teams produced a new state-of-the-art result
that improves on all of the past papers by these two
teams (within this research arena). This illustrates the
value of open and collaborative science, where groups
with different initial ﬁndings come together to help
each other in improving the state-of-the-art for the
beneﬁt of science and industry.

3) Motivation for changing the way we train software
analytics newcomers. It may not be enough to just
reﬂect on the different properties of off-the-shelf
learners. Analysts may need to be skilled in boundary,
label, parameter and instance engineering.

4) GHOST2’s design, implementation, and evaluation.
5) A new high-water mark in software analytics for

learning actionable static code warnings.

6) A reproduction package that other researchers can use

to repeat/refute/improve on our results1.

1. https://github.com/yrahul3910/static-code-warnings/

The rest of this paper is structured as follows. The next
section offers some background notes. §3 discusses the
locality problem for complex data sets and §4 offers details
on our treatments. §5 describes our experimental methods
after which, in §6, we show that GHOST2 outperforms (by
a large margin) prior results from Kang et al. We discuss
threats to validity for our study in §7, before a discussion in
§8 and concluding in §9.

Before all that, we digress to stress the following:
• A learned model must be tested on the kinds of data

expected in practice.

• Hence, any treatments to the data (e.g. via instance,
label, boundary engineering) are restricted to the training
data, and do not affect the test data.

2 BACKGROUND

2.1 Static Code Analysis

Automatic static analysis (SA) tools, such as Findbugs (see
Figure 1), are tools for detecting bugs in source code,
without having to execute that code. As they can ﬁnd
real bugs at low cost [43, 18], they have been adopted in
open source projects and in industry [4, 41, 7, 58, 38, 48].
However, as they do not guarantee that all warnings are real
bugs, these tools produce false alarms. The large number of
false alarms produced is a barrier to adoption [23, 13]; it
is easy to imagine how developers will be frustrated by
using tools that require them to inspect numerous false
alarms before ﬁnding a real bug. While false alarms include
spurious warnings caused by the over-approximation of
possible program behaviors during program analysis, false
alarms also refer to warnings that developers do not
act on. For example, developers may not think that the
warning represents a bug (e.g. due to “style” warnings that
developers perceive to be of little beneﬁt) or may not wish
to modify obsolete code.

The problem of addressing false alarms from static
analysis tools has been widely studied. There have been
many recent attempts to address the problem. Some
researchers have proposed new SA tools that use more

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

3

2.3 Further Result: Yang et al., 2021

Yang et al. [54] further analyzed the features using the data
collected by Wang et al. [50]. They found that all machine
learning techniques were effective and performed similarly
to one another. Their analysis revealed that the intrinsic
dimensionality of the problem was low; the features used
in the experiments were more verbose than the actual
attributes required for classifying actionable warnings.
This motivates the use of simpler machine learners over
more complex learners. From their analysis, SVMs were
recommended for use in this problem, as they were both
effective and can be trained at a low cost. In contrast, deep
learners were effective but more costly to train.

For each project

in their experiments, one revision
(training revision) was selected for extracting warnings for
training the learner, and another revision (testing revision)
set chronologically in the future of the training revision is
selected for extracting warnings for evaluating the learner.
This simulates a realistic usage scenario of the tool, where
the learner is trained using past data before developers
apply it to another revision of the source code.

2.4 Issues in Prior Results: Kang et al., 2022

Subsequently, Kang et al. [25] replicated the Yang et al. [54]
study to ﬁnd subtle methodological issues in the Wang et al.
data [50] which led to overoptimistic results.

Firstly, Kang et al.

found data leakage where the
information regarding the warning in the future, used
to determine the ground-truth labels, leaked into several
features. Five features (warning context in method, ﬁle,
for warning type, defect likelihood, discretization of defect
likelihood) measure the ratio of actionable warnings within
a subset of warnings (e.g. warnings in a method, ﬁle, of
a warning type). To determine if a warning is actionable,
the ground-truth label was used to compute these features,
leading to data leakage. Kang et al.
reimplemented
they are computed using only
the features such that
historical information, without reference to the ground truth
determined from the future state of the projects. As only the
features were reimplemented, the total number of training
and testing instances remained unchanged.

Secondly, they found many warnings appearing in both
the training and testing dataset. As some warnings remain
in the project at the time of both the training and testing
dataset, the model has access to the ground-truth label for
the warning at training time. Kang et al. addressed this issue
by removing warnings that were already present during the
training revision from the testing dataset, ensuring that the
learner does not see the same warning in both datasets.
After removing these warnings, the number of warnings in
the testing revision decreased from 15,695 to 2,615.

Next, Kang et al. analyzed the warning oracle, based on
the heuristic comparing warnings at one revision to another
revision in the future, used to automatically produce labels
for the warnings in the dataset. After manual labelling of
the actionable warnings, Kang et al. found that only 47% of
warnings automatically labelled actionable were considered
by the human annotators to be actionable. This indicates
that the heuristic employed as the warning oracle is not
sufﬁciently reliable for automatically labelling the dataset.

Fig. 2: To detect actionable warnings, a learner is trained
on warnings from a training revision. Each warning is
annotated with a label. When deployed on the latest
revision, only warnings classiﬁed as actionable warnings
by the machine learner are presented to the developers.

sophisticated, but costly, static analysis techniques (e.g.
Infer [10], NullAway [5]). Despite their improvements, these
tools still produce many false alarms [44]. Other attempts to
prune false alarms include the use of test case generation to
validate the presence of a bug at the source code location
indicated by the warning [24]. As generating test cases is
expensive, these techniques may face issues when scaling
up to larger projects, limiting their practicality.

2.2 Early Results: Wang et al., 2018

learning techniques

By framing the problem as a binary classiﬁcation problem,
machine
can identify actionable
warnings (allowing us to prune false alarms) [19, 20, 34,
40, 50, 54, 55]. These techniques use features extracted
from code analysis and metrics computed over the code
and warning’s history in the project. Figure 2 illustrates
this process. A static analyzer is ran on a training revision
and the warnings produced are labelled. When applied to
the latest revision, only warnings classiﬁed as actionable
warnings by the machine learner are presented to the
developers.

To assess proposed machine learners, datasets of
warnings produced by Findbugs have been created. As
the ground-truth label of each warning is not known, a
heuristic was applied to infer them. This heuristic compares
the warnings reported at a particular revision of
the
project against a revision set in the future. If a warning
is no longer present, but the ﬁle is still present, then the
heuristic determines that the warning was ﬁxed. As such,
the warning is actionable. Otherwise, if the warning is still
present, then the warning is a false alarm.

Wang et al. [50] ran a systematic literature review to
collect and analyze 100+ features proposed in the literature,
categorizing them into 8 categories. To remove ineffective
features,
they performed a greedy backward selection
algorithm. From the features, they identiﬁed a set of features
that offered effective performance.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

TABLE 1: Evaluation metrics based on TP (true positives);
TN (true negatives); TP (true positives) and FP (false
positives)

Evaluation Metric Description

Precision
AUC

TP
TP+FP
area under
receiver operating
characteristics curve (the true positive
rate against the false positive rate)

the

False alarm rate
Recall

FP
FP+TN
TP
TP+FN

TABLE 2: The Kang et al. predictors did not perform well
on the repaired data. In this table,lower false alarms are
better while higher precisions, AUC, and recall are better.

Dataset

Precision AUC

False alarm rate

Recall

cassandra
commons
lucene-solr
median
jmeter
tomcat
derby
ant

0.67
0.67
0.56
0.52
0.50
0.52
0.20
0.00

0.33
0.52
0.70
0.41
0.36
0.41
0.64
0.00

0.25
0.57
0.36
0.19
0.14
0.19
0.12
0.00

0.67
0.62
0.71
0.32
0.17
0.32
0.08
0.00

Kang et al. manually labelled 1,357 warnings. After
ﬁltering out duplicates and uncertain labels, a dataset of
768 warnings remained. On this dataset, Kang et al. again
applied off-the-shelf SVM models, assessing them with the
evaluation metrics listed in Table 1.

For their reasoning, Kang et al. used the learners
recommended by prior work; i.e. radial bias SVMs. The
results of the SVM are shown in Table 2. Those results are
hardly impressive:

• Median precisions barely more than 50%;
• Very low median AUCs of 41%;
• Extremely low median recalls of 32%.

That is to say, while Kang et al. were certainly correct in
their criticisms of the data used in prior work, based on
their paper, it is still an open issue about how to generate
good predictors for static code false alarms.

3 RETHINKING THE PROBLEM

This section suggests that detecting actionable static code
warnings is a “bumpy” problem (deﬁned below) and that
such problems can not be understood by learners that use
simplistic boundaries between classes.

The core task of any classiﬁcation problem is the creation
of a hyperspace boundary that let us isolate what is most
desired or most interesting. Different learners build their
boundaries in different ways:

• Simple decision tree learners can only build straight-

line boundaries.

• Neural networks can produce very complex and

convoluted boundaries.

• And internal to Kang et al.’s support vector machine
was a “radial basis function” that allowed those
algorithms to build circular hyperspace boundaries.
Boundaries can be changed by adjusting the parameters that
control the learner. For example, in Kang et al.’s radial basis

4

C = 0.1
acc = 90%

C = 0.1
acc = 64%

C = 0.2
acc = 81%

Fig. 3: The C parameter of a radial basis function alters the
shape of the hyperspace boundary. Acc is accuracy which
is the ratio of true positives plus true negatives divided by
a SVM making predictions across that boundary. Example
from [30].

functions, the C regularization parameter is used to set the
tolerance of the model to (some) classiﬁcations. By adjusting
C, an analyst can change the generalization error; i.e. the
error when the model is applied to as-yet-unseen test data.
Figure 3 show how changes to C can alter the decision
boundary between some red examples and blue examples.
Note that each setting to C changes the accuracy of the
predictor; i.e. for good predictions, it is important to ﬁt the
shape of the decision boundary to the shape of the data.

(Technical aside: while this example was based on SVM
technology, the same line of argument applies to any other
classiﬁer; i.e. changing the control parameters of the learner
also changes the hyperspace boundary found by that learner
and, hence, the predictive prowess of that learner.)

We have tried applying hyperparameter optimization
to C in a failed attempt to improve that performance (see
the C1 results of Table 7). From that failed experiment, we
conclude that however C work for radial bias functions,
they do not work well enough to ﬁx the unimpressive
predictive performances – see Table 2.

Why do radial bias SVMs fail in this domain? Our
conjecture is that the hyperspace boundary dividing the
static code examples (into false positives and others) is so
“bumpy”2 that the kinds of shape changes seen in Figure 3
can never adequately model those examples.

To test that conjecture, we ﬁrst checked for “bumpiness”
using a technique from Li et al.
technique
visualizes the “error landspace” (i.e. how fast small changes
in the independent variables altered the error estimation).
For our TOMCAT data, Li et al.’s methods resulted in Figure
4. There, we see a “bumpy” landscape with several multiple
local minima.

[32]. That

Having conﬁrmed that our data is “bumpy”, our second
step was to we look for ways to reduce that bumpiness.
Initially, we attempted to use neural nets since that kind of
learner is meant to be able to handle complex hyperspace
boundaries [53]. As discussed in §6, that attempt failed
even after trying several different architectures such as
feedforward networks, CNN, and CodeBERT [39, 18, 49]
(with and without tuning learner control parameters).

Since standard neural net technology failed, we tried
several manipulation techniques for the training process,
described in the next section.

2. “Bumpy” data contain complexities such as many local minima,
saddle points, very ﬂat regions, and/or widely varying curvatures. For
example, see Figure 4.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

5

4.2 Label Engineering (via SMOOTHing)

SMOTE has seen much success in recent SE papers as a
way to improve predication efﬁcacy [1]. But this technique
makes a linearity assumption that all the data around X is
correctly labelled (in our case, as examples of actionable or
unactionable static code warnings). This may not be true.
Cordeiro and Carneiro [15] and recent SE researchers [45,
46, 47, 57] note that noisy labels can occur when human
annotators are present [36] or those humans have divergent
opinions about the labels [6, 35]. Although our labels were
re-checked by the authors of Kang et al. [25], our ablation
study (below) reports that it is best to apply some mitigation
method for poorly labelled examples. For example,
in
this work we applied the following SMOOTHing operator
where data is assigned labels using multiple near neighbors.
This has the effect of removing outliers in the data. Our
SMOOTH operator works as follows:

• Given n training samples (and therefore, n labels), we

√

keep

n at random and discard the rest.

• Next, we use a KD-tree to recursively sub-divide
n nearest
the remaining data into leaf clusters of
neighbors. Within each leaf, all examples are assigned a
label that is the mode of the labels in that leaf.

√

4

One interesting and beneﬁcial side-effect of SMOOTHing is
that we make conclusions on our test data using just 10%
of the training data. By reducing the labelling required to
make conclusions, SMOOTHing offers a way to help future
studies avoid the problems reported by Kang et al. [25]:

• One of the major ﬁnding of the Kang et al. study
was that earlier work [54] had mislabelled much of its
data. From that study, we assert that it is important for
analysts to spend more time checking their labels. We
note that there many other ways to reduce the labels
required for supervised learning.

• SMOOTHing reduces the effort required for that

checking process (by a factor of ten).

As an aside, we note that SMOOTHing belongs to a class
of algorithms called semi-supervised learning [45, 46] that try
to make conclusions using as few labels as possible. The
literature on semi-supervised learning is voluminous [9, 11,
28, 59, 60] and so, in the theory, there could be many other
better ways to perform label engineering. This would be a
productive area for future research. But for now, the ablation
study (reported below) shows that SMOOTHing is useful
(since removing it degrades predictive performance).

4.3 Boundary Engineering (via GHOSTing)

As deﬁned above, instance and label engineering do not
reﬂect on the quality of data in the local region.

To counter that, this study employs a boundary method
called “GHOSTing”, recently developed and applied to
software defect prediction by Yedida and Menzies [56].
Boundary engineering is different to label and instance
engineering since it adjusts the frequency of different classes
in the local region (while the above typically end up
repeating the same label for a particular locality). Hence,
in that region, it changes the decision boundary.

GHOSTing addresses class imbalance issues in the data.
When an example with one label is surrounded by too
many examples of another label, then the signal associated

Fig. 4: Error landscape in the TOMCAT data before
applying the methods of this paper. In the plot, the larger
the vertical axes, the greater the loss value. Later in this
paper, we will show this plot again, after it has been
smoothed via the methods of §4 (see Figure 5 and Table 9).

TREATMENTS

4
This section discusses a framework that holds operators for
treating the data in order to adjust the decision boundary
(in different ways for different parts of the data). For
the purposes of illustration and experimentation, we offer
operational examples for each part of the framework:

• SMOTE for instance engineering;
• SMOOTH for label engineering;
• GHOST for boundary engineering;
• DODGE for parameter engineering.

Before presenting those parts we note here that
the
framework is more than just those four treatments. As
SE research matures, we foresee that our framework will
become a workbench within which researchers replace
some/all of these treatments with more advanced options.
That said, we have some evidence that SMOTE,

SMOOTH, GHOST, DODGE are useful:

• The ablation study of §5.2 shows that removing any one

of these treatments leads to worse performance.

• All these treatments are very fast: sub-linear time for
SMOTE and SMOOTH, linear time for GHOST, and
DODGE is known to be orders of magnitude faster than
other hyperparameter optimizers [2].

4.1 Instance Engineering (via SMOTEing)

To remove the “bumpiness” in data like Figure 4, we need
to pull and push the decision boundaries between different
classes into a smoother shape. But also, unlike simplistic C
tuning available in radial SVMs, we want that process to
perform differently in different parts of the data.

One way to adjust the decision boundary in different
parts of the data is to add (or delete) artiﬁcial examples
around each example X. This builds a little “hill” (or valley)
in the local region. As a result, in that local region, it
becomes more (or less) certain that all predictions which
reach the same conclusion as X. In effect, adding/deleting
examples pushes the decision boundary away (or, in the
case of deletions, pulls it closer). SMOTE [12] is one instance
engineering technique that:

• Finds ﬁve nearest neighbors to X with the same label;
• Selects one at random;
• Creates a new example R, with the same label as X at

some random point between X and R.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

TABLE 3: List of hyper-parameters tuned in our study.
CodeBERT is not shown in that table since, as mentioned
in the text, this analysis lacked the resources required to
tune such a large model.

Learner
Feedforward network

Logistic regression

Random forest

Decision Tree

SVM

CNN

Hyper-parameter
#layers
#units per layer
Penalty
C
Criterion
n estimators
Criterion
Splitter
C
Kernel

#convolutional blocks
#convolutional ﬁlters
Dropout probability
Kernel size

Range
[2, 6]
[3, 20]
{l1, l2}
{0.1, 1, 10, 100}
{ gini, entropy }
[10, 100]
{ gini, entropy }
{ best, random }
{0.1, 1, 10, 100}
{sigmoid,
polynomial }
[1, 4]
{4, 8, 16, 32, 64}
(0.05, 0.5)
{16, 32, 64}

rbf,

with example can be drowned out by its neighbors To
ﬁx this, for a two-class dataset D with class c0 being
the minority, GHOSTing oversamples the class by adding
concentric boxes of points around each minority sample.
The number of concentric boxes is directly related to the
class imbalance: higher the imbalance, more the number
of boxes. Speciﬁcally, if n is the fraction of samples in the
minority class, then (cid:98)log2(1/n)(cid:99) boxes are added. While
the trivial effect of this is to oversample the class (indeed,
as pointed out by Yedida and Menzies [56], this reverses
the class imbalance), we note that the algorithm effectively
builds a wall of points around minority samples. This
pushes the decision boundary away from the training
samples, which is preferred since a test sample that is
close to a training sample has a lesser chance of being
misclassiﬁed due to the decision boundary being in between
them.

Our pre-experimental

intuition was that boundary
engineering would replace the need to use instance
engineering. However, as shown by our ablation study,
for recognizing actionable static code warnings, we needed
both tools. On reﬂection, we realized both may be necessary
since while (a) boundary engineering can help make local
adjustments to the decision boundary, it can (b) only work in
regions where samples exist; instance engineering can help
ﬁll in gaps in sparser regions of the dataset.

4.4 Parameter Engineering (via DODGEing)

We noted above that different learners generate different
hyperspace boundaries (e.g. decision learners generate
straight-line borders while SVMs with radial bias functions
generate circular borders). Further, once a learner is selected,
then as seen in Figure 4, it is possible to further adjust a
border by altering the control parameters of that learner (e.g.
see Figure 3). We call this adjustment parameter engineering.
Parameter engineering is like a scientist probing some
phenomenon. After the data is divided into training and
some separate test cases, parameter engineering algorithms
conduct experiments on the training data looking for
parameter settings that improve the performance of a model
learned and assessed on the training data. Once some

6

conclusions are reached about what parameters are best,
then these are applied to the test data. Importantly, the
parameter engineering should only use the training data for
its investigations (since otherwise, that would be a threat to
the external validity of the conclusions).

Parameter engineering executes within the space of
control parameters of selected learners. These learners have
the internal parameter space shown in Table 3. We selected
this range of learners using the following rationale:

• In order to compare our new results to prior work by
Kang et al. [25], we use the Kang et al. SVMs with the
radial basis kernel and balanced class weights.
• In order to compare our work to Kang et al.

[54], we
used a range of traditional learners (logistic regression,
random forests, and single decision tree learners);

• Also, we explored the various neural net algorithms
shown in Table 4 since these algorithms have a
reputation of being able to handle complex decision
boundaries [53]. In this textbook on Empirical Methods
for AI, Cohen [14] advises that supposedly more
complex solutions should be compared to a range
of alternatives,
including very simple methods.
Accordingly, for neural nets, we used (a) feedforward

TABLE 4: Neural net architectures used in this study.

Feedforward networks These are artiﬁcial neural networks,
comprising an acyclic graph of nodes that process input and
produce an output. These dates back to the 1980s, and the
parameters of these models are learned via backpropagation
[39]. These networks have O(103) − O(104) parameters.
For these networks, we used the ReLU (rectiﬁed linear
activation) function (f (x) = max(0, x)). This is a piecewise
linear function that will output the input directly if it is
positive, otherwise, it will output zero.
A convolutional neural net (CNN) is a structured neural net
where the ﬁrst several layers are sparsely connected in order
to process information (usually visual). CNN is an example
of an deep learner and are much larger than feedforward
networks (these may span O(105) − O(107) parameters).
Optimizing an CNN is a very complex task (so many
parameters) so following advice from the literature [22, 42],
we used the following architecture. Our CNNs had multiple
“convolutional blocks” deﬁned as follows:
1) ReLU activation
2) Conv (with “same” padding)
3) Batch norm [22]
4) Dropout [42]
We note that this style of building convolutional networks,
by building multiple “convolutional blocks” is very popular
in the CNN literature [29, 31]. Our speciﬁc design of the
convolutional blocks was based on a highly voted answer
on Stack Overﬂow 3.
Note that with that architecture there is still room to adjust
the ordering of the blocks– which is what we adjust when
we tune our CNNs.
that been
CodeBERT [17]
pre-trained model using millions of examples
from
contemporary programming languages such as Python, Java,
JavaScript, PHP, Ruby, and Go. Such transformer models are
those based on the “self-attention” mechanism proposed by
Vaswani et al. [49]. CodeBERT is even large than CNN and
can contain O(108) − O(109) parameters. One advantage of
such large models is that can learn intricacies that are missed
by smaller models.

is a transformer-based model

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

7

networks from the 1980s; (b) the CNN deep learner
used in much of contemporary SE analytics; and (c) the
state-of-the-art CodeBERT model.

TABLE 5: Summary of the data distribution

Project

# train

# labels

imbalance% # test

currently available

are many algorithms

for
There
automatically tuning these learning control parameters. As
recommended by a prior study [3], we use Agrawal et al.’s
DODGE algorithm [2]. DODGE is based on early work by
Deb et al. in 2005 that proposed a “E-domination rule” [16];
i.e.

If one setting to an optimizer yield results within E
or another, then declare the region ± E as “tabu” and
search elsewhere.

A surprising result from Agrawal et al.’s research was that
E can be very large. Agrawal et al. noted that if learners
were run 10 times, each time using 90% of the training data
(selected at random), then they often exhibited a standard
deviation of 0.05 (or more) in their performance scores.
Assuming that performance differences less than ±2µ, are
statistically insigniﬁcantly different, then Agrawal reasoned
that E could be as large as 4 ∗ .05 = 0.2. This is an important
point. Suppose we are trying to optimize for two goals
(e.g. recall and false alarm). Since those measures have the
range zero to one, then E = 0.2 divides the output space
of those two goals divides into just a 5×5 = 25 regions.
Hence, in theory, DODGE could ﬁnd good optimizations
after just a few dozen random samples to the space of
possible conﬁgurations.

this

When

theoretical

prediction was

checked
experimentally of SE data, Agrawal [3] found that DODGE
with E = 0.2 defeated traditional single-point cross-over
genetic algorithms as well as state-of-the-art optimizers
(e.g. Bergstra and Bengio’s HYPEROPT algorithm [8]4).
Accordingly, this study used DODGE for its parameter
engineering.

Our pre-experimental

intuition was that DODGEing
would be fast enough to tune even the largest neural net
model. This turned out not to be the case. The resources
required to adjust the CodeBERT model are so large that,
for this study, we had to use the “off-the-shelf” CodeBERT.

5 EXPERIMENTAL METHODS

5.1 Data

This paper tested the efﬁcacy of instance, label, boundary
and parameter engineering using the revised and repaired
data from Kang et al. paper [25].

Recall that Kang et al. manually labelled warnings from
the same projects studied by Yang et al. [54] to assess
the level of agreement between human annotators and
the heuristic. The manual labelling was performed by two
human annotators. When the annotators disagreed on the
label of a warning, they discussed the disagreement to reach
a consensus. While they achieved a high level of agreement,
achieving a Cohen’s Kappa of above 0.8, manual labelling
is costly, requiring human analysis of both the source code
and the commit history of the code. That said, this label is
essential since it removed closed warnings which are not

maven
cassandra
jmeter
commons
lucene-solr
ant
tomcat
derby

total

2
9
10
12
19
22
134
346

554

1
4
4
5
5
6
13
20

58

33
38
43
59
38
36
41
37

1
4
4
5
6
7
37
92

156

actionable (e.g., the warnings may have been removed for
reasons unrelated to the Findbugs warning).

Two other ﬁlters employed by Kang et al. where:
• Unconﬁrmed actionable warnings were removed;
• False alarms were randomly sampled to ensure a
balance of labels (40% of the data were actionable)
consistent with the rest of the experiments.

One of the complaints of the Kang et al. paper [25] against
earlier work [54] was that, for data that comes with some
time stamp, it is inappropriate to use future data to predict
past labels. To avoid that problem, in this study, we sorted
the Kang et al. data by time stamps, then used 80% of the
past data to predict the remaining 20% future labels.

The Kang et al. data comes from eight projects and we
analyzed each project’s data separately. The 80:20 train:test
splits resulted in the train:test sets shown in Table 5
(exception: for MAVEN, we split 50:50, since there are only
4 samples in total).

Pre-experimentally, we were concerned that learning
from the smaller data sets of Table 5 would complicate our
ability to make any conclusions from this data. That is, we
needed to know:

RQ5: Are larger training sets necessary (for the task of
recognizing actionable static code warnings)?

This turned out not to be a critical issue. As shown
below, the performance patterns in our experiments were
stable across all the six smaller data sets used in this study.
Technical aside: In other papers, we have run repeated
trials with multiple 80:20 splits for training:test data. This
was not here since some of our data sets are too small (see
the ﬁrst few rows of Table 5) that any reduction in the
training set size might disadvantage the learning process.
Hence, the external validity claims of this paper come from
patterns seen in eight different software projects.

5.2 Experimental Rig

This study explores:

• N = 4 pre-processors (boundary,

label, parameter,

instance) that could be mixed in 24 = 16 ways.

• Six traditional

learners:

logistic regression, decision

trees, random forests, SVMs (with 3 basis functions);
• Three neural net architectures: CNN, CodeBERT,

feedforward networks;

4. At the time of this writing (April 2022), the paper proposing

HYPEROPT has 7,557 citations in Google Scholar.

To clarify the reporting of these 16×(6+3) = 144 treatments,
we made the following decisions. Firstly, when reporting the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

8

TABLE 6: Design of our ablation study. In the learner choice column, F = feedforward networks, T = traditional
learners, C = CNN, B = CodeBERT.

y
r
a
d
n
u
o
B

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

Treatment

A1
A2
A3
A4
A5
A6

A7
B1

B2
C1

C2

D1
CodeBERT

Engineering decisions
r
ete
m
a
r
a
P

r
e
n
r
a
e
L

el
b
a
L

(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

F
F
F
F
F
T

T
T

C
T

C

T
B

e
c
n
sta
In

(cid:51)

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)

(cid:51)

(cid:51)

% Labels Description

10 Our recommended method
10 A1 without instance engineering (no SMOTE)
10 A1 without hyper-parameter engineering (no DODGE)
10 A1 without boundary engineering (no GHOST)
100 A1 without label engineering (no SMOOTH). From TSE’21 [56]
100 A1 without label engineering, replacing feedforward with traditional

learners

10 A1 replacing feedforward with traditional learners
10 A1 without boundary engineering, replacing feedforward with traditional

learners

10 A1 without boundary engineering, replacing feedforward with CNN
100 A1 without boundary engineering or label engineering, replacing

feedforward with traditional learners

100 A1 without boundary engineering or label engineering, replacing

feedforward with CNN
Setup used by the Yang et al. [54] and Kang et al. [25] studies.
CodeBERT without modiﬁcations

100
100

results of the traditional learner, just show the results of the
one that beat the other traditional learners (which, in our
case, was typically random forest or logistic regression).

Secondly, we do not apply pre-processing or parameter
engineering on CodeBERT. This decision was required, for
pragmatic reasons. Due to the computational cost of training
that model, we could only run off-the-shelf CodeBERT.

Thirdly, rather than explore all 16 combinations of
use/avoid different pre-processing, we ran the ablation
study recommended in Cohen’s Empirical Methods
for
AI textbook [14]. Ablation studies let us explore some
combination of N parts can be assessed in time O(N ), not
O(2N ). Such ablation studies work as follows:

• Commit to a preferred approach, with N parts;
• If removing any part ni ∈ N degrades performance,

then conclude that all N parts are useful.

With these decisions, instead of having to report on 144
treatments, we need only show the 13 treatments in the
ablation study of Table 6. In that table, for treatments
that use any of boundary or label or parameter or
instance engineering, we apply those treatments in the
order recommended by the original GHOST paper [56]. That
paper found that it could improve recall by 30% (or more)
by multiple rounds of SMOTE + GHOST. As per that advice,
A1 executes our pre-processors in the order:

smote → ghost → ghost → smote → smooth → dodge

This paper does not explore the effect of different
orderings; rather, our core idea is that
the different
engineering techniques work together to produce strong
results. We leave the exploration of the effect of ordering
to future work.

All the treatments labelled “A” (A1,A2,A3,A4,A5) in
Table 6, use the order shown above, perhaps (as part of the
ablation study) skipping over one or more the steps. We
acknowledge that there are many possible ways to order the
applications of our treatments, which is a matter we will

for future work. For the moment,the ordering shown above
seems useful (evidence: see next section).

As to the speciﬁcs of the other treatments:
• Treatment A5 is the treatments from the TSE’21 paper

that proposed GHOSTing [56].

• Treatment D1 contains the treatments applied in prior

papers by Yang et al. [54] and Kang et al. [25].

• Anytime we applied parameter engineering, this meant
that some automatic algorithm (DODGE) selected the
control parameters for the learners (otherwise, we just
used the default off-the-shelf settings).

• Anytime we apply label engineering, we are only used

10% of the labels in the training data.

• The last

line,

showing CodeBERT, has no pre-
processing or tuning. As said above, CodeBERT is so
complex that we must run it “off-the-shelf”.

6 RESULTS

The results of the Table 6 treatments are shown in Table 7
(and another brief summary is offered in Table 8). These
results are somewhat extensive so, by way of an overview,
we offer the following summary tool. The cells shown in
pink are those that are worse than the A1 results (and A1
is our recommended GHOST2 method). Looking over those
pink cells we can see that across our data sets and across
our different measures, our recommend method (A1) does
as well (or better) than anything else.

(Technical aside:

looking at this pink cells,

it could
be said that A5 comes close to A1, but A5 loses a
little on recalls). Nevertheless, we have strong reasons for
recommending A1 over A5 since, recalling Table 6, A5
requires a labelling for 100% of the data. On the other
hand A1, that uses label engineering, achieves its results
using 10% of the labels. This is important since, as said
in our introduction, one way to address,
the
methodological problems raised by Kang et al. GHOST2
makes its conclusions using a small percentage of the raw

in part,

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

9

TABLE 7: Our results across eight datasets on four metrics.

Treatment

maven

cassandra

jmeter

commons

lucene-solr

ant

tomcat

derby median

A1
A2
A3
A4
A5
A6
A7
B1 (DODGE)
B2 (CNN)
C1 (DODGE)
C2 (CNN)
D1
CodeBERT

A1
A2
A3
A4
A5
A6
A7
B1 (DODGE)
B2 (CNN)
C1 (DODGE)
C2 (CNN)
D1
CodeBERT

A1
A2
A3
A4
A5
A6
A7
B1 (DODGE)
B2 (CNN)
C1 (DODGE)
C2 (CNN)
D1
CodeBERT

A1
A2
A3
A4
A5
A6
A7
B1 (DODGE)
B2 (CNN)
C1 (DODGE)
C2 (CNN)
D1
CodeBERT

1
0
0.5
1
1
1
1
1
0.5
1
0.5
0
0.5

1
0
0.5
1
1
1
1
1
0.5
1
0.5
0.5
0.5

0
0
1
0
0
0
0
0
1
0
1
0
1

1
0.5
1
1
1
1
1
1
1
1
1
0
1

PRECISION (better results are larger)

1
1
0.33
1
1
0.5
0.5
0
0
1
0.5
0
0.8

1
0.67
0.2
1
1
0.33
1
0.5
0.6
0.33
0.6
0.6
0.63

0.8
1
0.25
0.75
0.8
0.67
1
0
0
0.67
0.83
0
0.6

AUC: TP vs. TN (better results are larger)

0.83
0.75
0.67
1
1
0.83
0.83
0.5
0.5
1
0.17
0.5
0.68

1
0.83
0.5
0.75
0.88
0.75
1
0.88
0.5
0.75
0.5
0.5
0.53

0.75
0.63
0.38
0.63
0.75
0.88
0.75
0.5
0.5
0.88
0.5
0
0.63

1
1
0.33
1
1
1
0
0
0.29
0.67
0.43
0
0

1
0.75
0.55
0.8
0.9
1
0.5
0.5
0.5
0.9
0.5
0.38
0.48

FALSE ALARM RATE (better results are smaller)

0
0
0.67
0
0
0.33
0.33
0
0
0
1
0
0.2

0
0.33
1
0
0
0.5
0
0.25
1
0.5
1
1
1

0.5
0
0.75
0.5
0.5
0.25
0
0
0
0.25
1
1
0.25

RECALL (better results are larger)

0.67
0.5
1
1
1
1
1
0
0
1
0.33
0
0.67

1
1
1
0.5
0.75
1
1
1
1
1
1
1
1

1
0.25
0.5
0.75
1
1
0.5
0
0
1
1
0
0.5

0
0
0.4
0
0
0
0
0
1
0.2
1
0.25
0

1
0.5
0.5
0.6
0.8
1
0
0
1
1
1
0
0

1
0.25
0.25
0.75
1
1
0.5
1
0
1
0.5
0.5
1

1
0.5
0.5
0.5
0.67
1
0.83
1
0.5
1
0.5
0.17
0.56

0
1
1
1
0
0
0.33
0
0
0
1
1
0

1
1
1
1
0.33
1
1
1
0
1
1
0.33
0.33

0.79
0.68
0.33
1
0.72
0.85
0.55
0.47
0.51
0.67
0.4
0.39
0.41

0.68
0.6
0.51
0.54
0.67
0.85
0.59
0.58
0.5
0.8
0.63
0.48
0.44

0.29
0.4
0.71
0
0.57
0.09
0.17
0.35
1
0.26
0.46
0.77
0.28

0.65
0.59
0.75
0.09
0.91
0.79
0.36
0.5
1
0.86
0.73
0.73
0.26

0.72
0.73
0.4
0.75
0.84
0.89
0.42
0.59
0.61
0.81
0.73
0.39
0.25

0.57
0.7
0.51
0.59
0.78
0.76
0.62
0.62
0.62
0.76
0.82
0.47
0.63

0.79
0.38
0.05
0.48
0.41
0.03
0.44
0.11
0.25
0.06
0.17
0.67
0.17

0.94
0.77
0.07
0.67
0.97
0.55
0.69
0.34
0.49
0.59
0.82
0.61
0.25

1
0.71
0.33
1
1
0.87
0.53
0.49
0.4
0.74
0.5
0.2
0.55

0.92
0.67
0.51
0.69
0.83
0.87
0.79
0.6
0.5
0.89
0.5
0.48
0.54

0
0.17
0.73
0
0
0.06
0.09
0
0.63
0.13
1
0.72
0.23

1
0.54
0.88
0.71
0.94
1
0.85
0.42
0.75
1
1
0.17
0.42

TABLE 8: Median performance improvements seen after
applying all the treatments A1 (deﬁned in §4); i.e. all of
instance, label, boundary and parameter engineering.

higher is better

lower is better

precision
AUC
recall
false alarm

From
Table 2
50
41
19
32

From right-
hand-side
of Table 7
100
90
100
0

Improvement
50
59
89
32

data (10%). That is, to address to issues of corrupt data
found by Kang et al., we say “use less data” and, for the
data that is used, “reﬂect more on that data”.)

Using these results, we can answer our

research

questions as follows.

RQ1: For detecting actionable static code warnings,

what data mining methods should we recommend?

Regarding feedforward networks versus, say, traditional
learners (decision trees, random forests, logistic regression

and SVMs), the traditional learners all performed worse
than the feedforward networks used in treatment A1
(evidence: compare treatments A1 with A7 which use
feedforward or traditional learners, respectively; there are
four perfect AUCs for feedforward networks in A1, i.e
AUC=100%, but only two for the A7 results).

As to why the 1980s style feedforward networks worked
better than newer neural net technology, we note that
feedforward networks run so fast
is easier to
extensively tune them. Perhaps (a) faster learning plus
(b) more tuning might lead to better results that then non-
linear modeling of an off-the-shelf learner. This could be an
interesting avenue for future work.

than it

As to the value of boundary, label, instance and parameter
engineering, in the ablation study, removing any of these
led to worse results. For example, with boundary engineering,
A1 (that uses boundary engineering) generates more perfect
scores (e.g. AUC=100%) than A4 (that does not use it). Also,
for recall, A1 always performed as good or better than A4
in 6/8 data sets. Similarly, A4 suffers from a drop in AUC
score across the board.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

10

RQ2: Does GHOST2’s combination of instance, label,
boundary and parameter engineering,
reduce the
complexity of the decision boundary?

Previously, this paper argued that reason for the poor
performance seen in prior was due to the complexity of the
data (speciﬁcally, the bumpy shape seen in Figure 4). Our
treatments of §4 were designed to simplify that landscape.
Did we succeed?

Fig. 5: Error landscape in the TOMCAT after applying
the treatments of §4. To understand the simpliﬁcations
achieved via our methods,
the reader might ﬁnd it
insightful to compare this ﬁgure against Figure 4.

As for label engineering, from A1 to A5, specializing our
data to just 10% of the labels (in A1) yields nearly the same
precisions which using 100% of the data (in A5) in nearly all
the AUC results. Moreover, the AUC score for A1 is perfect
in 4/8 cases, while for A5, it is rarely the case.

As to instance engineering, without it the precision can
crash to zero (compare A1 to A2, particularly the smaller
data sets) while often leading to lower recalls. The smaller
datasets also see a decrease in AUC for A2.

Measured in terms of false alarm, these results strongly
recommend parameter
engineering. Without parameter
engineering, some of those treatments could ﬁnd too many
static code warnings and hence suffer from excessive false
alarms (evidence: see the A3 false alarm results in nearly
every data set). A1 (which used all the treatments of §4)
had lower false alarm rates than anything else (evidence: we
rarely see the dark blue A1 spike in the false alarm results).
The only exception to the observation that “parameter
engineering leads to lower false alarm results” are seen
in the DERBY data set. That data set turns out to be
particularly tricky in that, nearly always, modeling methods
that achieved low false alarm rates on that data set also had
to be satisﬁed with much lower recalls.

One ﬁnal point is that these results do not recommend
the use of certain widely used neural network technologies
such as CNN or CodeBERT for ﬁnding actionable static code
warnings. CNN-based treatments (B2 and C2) suffer from
low precision and AUC scores (see Table 7). Similarly, as
shown Table 7, CodeBERT often suffers from low precision
and poor false alarms and (in the case of CodeBERT) some
very low recalls indeed.

In summary:

recognize

actionable

Answer 1: To
code
the treatments of §4. Also,
warnings, apply all
spend most
tuning faster feedforward neural nets
rather than trusting (a) traditional learners or (b) more
recent “bleeding edge” neural net methods.

static

TABLE 9: Percent changes in
Li et al. [32]’s smoothness
metric, seem after applying
the methods of this paper.

Figure 5 shows the landscape in TOMCAT after the
treatments of §4 were applied. By comparing this ﬁgure with
Figure 4, we can see that our treatments achieved the desired
goal of removing the “bumps”.
As to the other data
sets, Li et al. [32] propose
a “smoothness” equation
to measure a data set’s
“bumps”. Table 9 shows
the percentage change in
that smoothness measure
applying the
seen after
methods of this paper. All
these changes are positive,
indicating that the resulting
much
landscapes
smoother. For an intuition
of what
these numbers
mean, the TOMCAT change
of 36.35% results in Figure
4 changing to Figure 5.
Hence we say:

maven
cassandra
jmeter
tomcat
derby
commons
ant
lucene-solr

158.87
73.09
55.53
36.34
31.35
29.61
24.78
16.46

% change

median

Dataset

33.85

are

Answer 2: Label, parameter, instance and boundary
engineering can simplify the internal structure of
training data.

RQ3: Does GHOST2’s combination of

instance,
improve predictive

label, boundary and parameter
performance?

Table 8 shows
the performance improvements after
smoothing out our training data from (e.g.) Figure 4 to
Figure 5. On 4/8 datasets, we achieve perfect scores.
Moreover, we showed through an ablation study that each
of the components of GHOST2 is necessary. For example,
row A3 in Table 7 is another piece of evidence that hyper-
parameter optimization is necessary. The feedforward
networks of our approach outperformed more complex
learners (CNNs and CodeBERT)–we refer the reader to
rows B2, C2, and CodeBERT in Table 7. On the other
hand, going too simple for traditional learners leads to A7,
which suffers from poor precision scores. Given those large
improvements, we say:

Answer 3: Detectors of actionable static code warnings
work much better when learned from smoothed
training data.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

11

RQ4: Are all parts of GHOST2 necessary; i.e. would
something simpler also achieve the overall goal?

We presented an ablation study that showed that each part
of GHOST2 was necessary. Among the 13 treatments that we
tested, GHOST2 was the only one that consistently scored
highly in precision, AUC, and recall, while also generally
having low false alarm rates. The crux of our ablation study
was that each component of GHOST2 works with the others
to produce a strong performer.

Based on the above ablation study results, we say:

Answer 4: Ignoring any of part of instance,
label,
boundary or parameter engineering leads to worse
results than using all parts (at least for the purpose of
recognizing actionable static code warnings).

RQ5: Are larger training sets necessary (for the task of
recognizing actionable static code warnings)?

In the above discussion, when we presented Table 5, it
was noted that several of the train/tests used in this study
were very small. At that time, we expressed a concern that,
possibly, our data sets explored were too small for effective
learning.

This turned out not to be the case. Recall that in Table 7,
the data set were sorted left-to-right from smallest to largest
training set size. There is no pattern there that smaller
data sets perform worse than large ones. In fact– quite the
opposite: the smaller data sets were always associated with
better performance than those seen on right-left-side. Hence
we say:

Answer 5: The methods of this paper are effective, even
for very small data sets.

This is a surprising result since one of the truisms
of data mining is “the more data the better”. Large
data sets are often cited as the key to success for data
mining applications. For example, in his famous talk, “The
Unreasonable Effectiveness of Data”, Google’s former Chief
Scientist Peter Norvig argues that “billions of trivial data
points can lead to understanding” [37] (a claim he supports
with numerous examples from vision research).

7 THREATS TO VALIDITY
As with any empirical study, biases can affect the ﬁnal
results. Therefore, any conclusions made from this work
must be considered with the following issues in mind:

1. Sampling bias threatens any classiﬁcation experiment;
i.e., what matters there may not be true here. For example,
the data sets used here comes prior work and, possibly, if we
explored other data sets we might reach other conclusions.
On the other hand, repeatability is an important part of
science so we argue that our decision to use the Kang et al.
data is appropriate and respectful to both that prior work
and the scientiﬁc method.

2. Learner bias: Machine learning is a large and active
ﬁeld and any single study can only use a small subset of
the known algorithms. Our choice of “local learning” tools

was explained in §4. That said, it is important to repeat the
comments made there that our SMOTEing, SMOOTHing,
GHOSTing and DODGEing operators are but one set of
choice within a larger framework of possible approaches
to instance, label, boundary, and parameter engineering
(respectively). As SE research matures, we foresee that
our framework will become a workbench within which
researchers replace some/all of these treatments with better
options. That said, in defence of the current options, we note
that our ablation study showed that removing any of them
can lead to worse results.

those parameters are changed. Accordingly,

3. Parameter bias: Learners are controlled by parameters
and the resulting performance can change dramatically
if
in this
paper, our recommended methods (from Table 4) includes
parameter engineering methods to ﬁnd good parameter
settings for our different data sets.

4. Evaluation bias: This paper use four evaluation criteria
(precision, AUC, false alarm rate, and recall) and it is
certainly true that by other measures, our results might not
work be seen to work as as well. In defence of our current
selection, we note that we use these measures since they let
us compare our new results to prior work (who reported
their results using the same measures).

Also,

to repeat a remark made previously, another
evaluation bias was how we separated data into train/test.
In other papers, we have run repeated trials with multiple
80:20 splits for training:test data. This was not here since
some of our data sets are too small (see the ﬁrst few
rows of Table 5) that any reduction in the training set
size might disadvantage the learning process. Hence, the
external validity claims of this paper come from patterns
seen in eight different software projects.

8 DISCUSSION

This discussion section steps back from the above to make
some more general points.

We suggest that this paper should lead to a new way of

training newcomers in software analytics:

• Our results show that there is much value in decades-
old learning technology (feedforward networks).
Hence, we say that when we train newcomers to
the ﬁeld of software analytics, we should certainly
train them in the latest techniques (deep learning,
CodeBERT, etc).

• That said, we should also ensure that they know of
prior work since (as shown above), sometimes those
older methods still have currency. For example, if some
learner is faster to run, then it is easier to tune. Hence,
as shown above, it can be possible for old techniques to
do better than new ones, just by tuning.

For future work, it would be useful to check what other
SE domains simpler, faster, learners (plus some tuning) out-
perform more complex learning methods.

That said, we offer the following cautionary note about
tuning. Hyper-parameter optimization (HPO, which we
have call “parameter engineering” in this paper) has
received much recent attention in the SE literature [2,
56, 3] We have shown here that reliance on just HPO
can be foolhardy since better results can be obtained by

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

12

the judicious use of HPO combined with more nuanced
approaches that actually reﬂect the particulars of the current
problem (e.g. our treatments that adjusted different parts
of the data in different ways). As to how much to study
the internals of a learner, we showed above that there
are many choices deep within a learner than can greatly
improve predictive performance. Hence we say that it is
very important to know the internals of a learner and how to
adjust them. In our opinion, all too often, software engineers
use AI tools as “black boxes” with little understanding of
their internal structure.

Our results also doubt some of the truisms of our ﬁeld.

For example:

• There is much recent work on big data research in SE,
the premise being that “the more data, the better”. We
certainly do not dispute that but our results do show
that it is possible to achieve good results with very
small data sets.

• There is much work in software analytics suggesting
that deep learning is a superior method for analyzing
data [56, 51, 33, 52]. Yet when we tried that here, we
found that a decades-old neural net architecture (feed-
forward networks, discussed in Table 4) signiﬁcantly
out-performed deep learners.

For newcomers to the ﬁeld of software analytics, truisms
might be useful. But better results might be obtained
when teams of data scientists combine to suggest multiple
techniques– some of which ignore supposedly tried-and-
true truisms.

9 CONCLUSION
Static analysis tools often suffer from a large number of
false alarms that are deemed to be unactionable [44]. Hence,
developers often ignore many of their warnings. Prior
work by Yang et al. [54] attempted to build predictors for
actionable warnings but, as shown by Kang et al. [25], that
study used poorly labelled data.

This paper extends the Kang et al. result as follows.
Table 2 shows that building models for this domain is a
challenging task. The discussion section of §3 conjectured
that for the purposes of detecting actionable static code
warnings, standard data miners can not handle the
complexities of the decision boundary. More speciﬁcally, we
argued that:

For complex data, global treatments perform worse
than localized treatments which adjust different parts
of the landscape in different ways.

§4 proposed four such localized treatments, which we called
instance, parameter, label and boundary engineering.

These treatments were tested on the data generated by
Kang et al. (which in turn, was generated by ﬁxing the
prior missteps of Yang et al.). On experimentation, it was
shown that the combination of all our treatments (in the
“A1” results of Table 6) performed much better than than
the prior results seen in Table 2. As to why these treatments
before so well, the analysis of Table 9 showed that instance,
parameter,
label and boundary engineering did in fact
remove complex shapes in our decision boundaries. As to
the relative merits of instance versus parameter versus label
versus boundary engineering, an ablation study showed

that using all these treatments produces better predictions
that alternative treatments that ignored any part.

Finally, we comment here on the value of different teams
working together. The speciﬁc result reported in this paper
is about how to recognize and avoid static code analysis
false alarms. That said, there is a more general takeaway.
Science is meant to be about a community critiquing and
improving each other’s ideas. We offer here a successful
example of such a community interaction where teams
from Singapore and the US successfully worked together.
Initially, in a 2022 paper [25], the Singapore team identiﬁed
issues with the data that result in substantially lower
performance of the previously-reported best predictor of
actionable warnings [50, 54, 55]. Subsequently, in this paper,
both teams combined to produce new results that clariﬁed
and improved the old work. That teamwork leads us to
trying methods which, according to the truisms of our ﬁeld,
should not have worked. The teamwork that generated this
paper should be routine, and not some rare exceptional case.

ACKNOWLEDGMENTS
This work was partially supported by an NSF Grant
#1908762.

REFERENCES
[1] Amritanshu Agrawal and Tim Menzies.

Is” better data” better
than” better data miners”? In 2018 IEEE/ACM 40th International
Conference on Software Engineering (ICSE), pages 1050–1061. IEEE,
2018.

[2] Amritanshu Agrawal, Wei Fu, Di Chen, Xipeng Shen, and Tim
IEEE
Menzies. How to “dodge” complex software analytics.
Transactions on Software Engineering, 47(10):2182–2194, 2019.
[3] Amritanshu Agrawal, Xueqi Yang, Rishabh Agrawal, Rahul
Yedida, Xipeng Shen, and Tim Menzies. Simpler hyperparameter
IEEE
optimization for software analytics: why, how, when.
Transactions on Software Engineering, 2021.

[5]

[4] Nathaniel Ayewah and William Pugh. The google ﬁndbugs ﬁxit.
In Proceedings of the 19th international symposium on Software testing
and analysis, pages 241–252, 2010.
Subarno Banerjee, Lazaro Clapp, and Manu Sridharan. Nullaway:
Practical type-based null safety for java. In Proceedings of the 2019
27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, pages
740–750, 2019.

[6] Ella Barkan, Alon Hazan, and Vadim Ratner. Reduce discrepancy
of human annotators in medical imaging by automatic visual
US Patent
comparison to similar cases, February 9 2021.
10,916,343.

[7] Moritz Beller, Radjino Bholanath, Shane McIntosh, and Andy
Zaidman. Analyzing the state of static analysis: A large-scale
evaluation in open source software. In 2016 IEEE 23rd International
Conference on Software Analysis, Evolution, and Reengineering
(SANER), volume 1, pages 470–481. IEEE, 2016.
James Bergstra and Yoshua Bengio. Random search for hyper-
parameter optimization. J. Mach. Learn. Res., 13(null):281–305, feb
2012. ISSN 1532-4435.

[8]

[9] David Berthelot, Nicholas Carlini,

Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A
holistic approach to semi-supervised learning. Advances in Neural
Information Processing Systems, 32, 2019.

[10] Cristiano Calcagno, Dino Distefano, J´er´emy Dubreil, Dominik
Gabi, Pieter Hooimeijer, Martino Luca, Peter O’Hearn, Irene
Papakonstantinou, Jim Purbrick, and Dulma Rodriguez. Moving
In NASA Formal Methods
fast with software veriﬁcation.
Symposium, pages 3–11. Springer, 2015.

[11] J. Chakraborty, S. Majumder, and H. Tu. Can we achieve fairness

using semi-supervised learning? Fairware, 2022.

[12] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and
W Philip Kegelmeyer. Smote: synthetic minority over-sampling
technique. Journal of artiﬁcial intelligence research, 16:321–357, 2002.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

13

[13] Maria Christakis and Christian Bird. What developers want and
need from program analysis: an empirical study. In Proceedings of
the 31st IEEE/ACM International Conference on Automated Software
Engineering, ASE 2016, Singapore, September 3-7, 2016, pages 332–
343. ACM, 2016.

[14] Paul R Cohen. Empirical methods for artiﬁcial intelligence, volume

139. MIT press Cambridge, 1995.

[15] Filipe R Cordeiro and Gustavo Carneiro. A survey on deep
learning with noisy labels: How to train your model when you
cannot trust on the annotations? In 2020 33rd SIBGRAPI conference
on graphics, patterns and images (SIBGRAPI), pages 9–16. IEEE, 2020.
[16] Kalyan Deb, Manikanth Mohan, and Shikhar Mishra. Evaluating
the (cid:15)-dominance based multi-objective evolutionary algorithm for
a quick computation of pareto-optimal solutions. Evolutionary
computation, 13:501–25, 02 2005. doi: 10.1162/106365605774666895.
[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang,
et al. Codebert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155, 2020.

[18] Andrew Habib and Michael Pradel. How many of all bugs do
we ﬁnd? a study of static bug detectors. In 2018 33rd IEEE/ACM
International Conference on Automated Software Engineering (ASE),
pages 317–328. IEEE, 2018.

[19] Quinn Hanam, Lin Tan, Reid Holmes, and Patrick Lam. Finding
improving actionable alert
In Proceedings of the 11th working conference on mining

patterns in static analysis alerts:
ranking.
software repositories, pages 152–161, 2014.

[20] Sarah Heckman and Laurie Williams.

On establishing a
benchmark for evaluating static analysis alert prioritization and
In Proceedings of the Second ACM-
classiﬁcation techniques.
IEEE international symposium on Empirical software engineering and
measurement, pages 41–50, 2008.

[21] Sarah Heckman and Laurie Williams. A model building process
for identifying actionable static analysis alerts. In 2009 International
conference on software testing veriﬁcation and validation, pages 161–
170. IEEE, 2009.

[22] Sergey Ioffe and Christian Szegedy.

Batch normalization:
Accelerating deep network training by reducing internal covariate
shift. In International conference on machine learning, pages 448–456.
PMLR, 2015.

[23] Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert
Bowdidge. Why don’t software developers use static analysis
tools to ﬁnd bugs? In 2013 35th International Conference on Software
Engineering (ICSE), pages 672–681. IEEE, 2013.

[24] Ashwin Kallingal Joshy, Xueyuan Chen, Benjamin Steenhoek, and
Wei Le. Validating static warnings via testing code fragments. In
Proceedings of the 30th ACM SIGSOFT International Symposium on
Software Testing and Analysis, pages 540–552, 2021.

[25] Hong Jin Kang, Khai Loong Aw, and David Lo. Detecting false
alarms from automatic static analysis tools: How far are we? arXiv
preprint arXiv:2202.05982, 2022.

[26] Sunghun Kim and Michael D. Ernst. Which warnings should i
ﬁx ﬁrst? In Proceedings of the the 6th Joint Meeting of the European
Software Engineering Conference and the ACM SIGSOFT Symposium
on The Foundations of Software Engineering, ESEC-FSE ’07, page
45–54, New York, NY, USA, 2007. Association for Computing
Machinery.
ISBN 9781595938114. doi: 10.1145/1287624.1287633.
URL https://doi.org/10.1145/1287624.1287633.

[27] Sunghun Kim and Michael D Ernst. Which warnings should i
ﬁx ﬁrst? In Proceedings of the the 6th joint meeting of the European
software engineering conference and the ACM SIGSOFT symposium on
The foundations of software engineering, pages 45–54, 2007.

[28] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and
Max Welling.
Semi-supervised learning with deep generative
models. Advances in neural information processing systems, 27, 2014.
[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. Advances
in neural information processing systems, 25, 2012.

[30] Ajitesh Kumar.
examples.
using-jsonb-in-postgresql-how-to-effectively-store-1.

Svm rbf kernel parameters with code
Available on-line at https://dzone.com/articles/

[31] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
Richard E Howard, Wayne Hubbard, and Lawrence D Jackel.
Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541–551, 1989.

[32] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom
Goldstein. Visualizing the loss landscape of neural nets. Advances
in neural information processing systems, 31, 2018.

[33] Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara
Ryder. Cclearner: A deep learning-based clone detection approach.
In 2017 IEEE International Conference on Software Maintenance and
Evolution (ICSME), pages 249–260. IEEE, 2017.

[34] Guangtai Liang, Ling Wu, Qian Wu, Qianxiang Wang, Tao Xie,
and Hong Mei. Automatic construction of an effective training
In Proceedings
set
of the IEEE/ACM international conference on Automated software
engineering, pages 93–102, 2010.

for prioritizing static analysis warnings.

[35] Kede Ma, Xuelin Liu, Yuming Fang, and Eero P Simoncelli. Blind
image quality assessment by learning from multiple annotators. In
2019 IEEE International Conference on Image Processing (ICIP), pages
2344–2348. IEEE, 2019.

[36] Don McNicol. A primer of signal detection theory. Psychology Press,

2005.

[37] Peter Norvig. The Unreasonable Effectiveness of Data, 2011. URL

https://www.youtube.com/watch?v=yvDCzhbjYWs.

[38] Sebastiano Panichella, Venera Arnaoudova, Massimiliano
Di Penta, and Giuliano Antoniol. Would static analysis tools help
developers with code reviews? In 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and Reengineering
(SANER), pages 161–170. IEEE, 2015.

[39] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
Learning representations by back-propagating errors. nature, 323
(6088):533–536, 1986.

[40] Joseph Ruthruff, John Penix, J Morgenthaler, Sebastian Elbaum,
and Gregg Rothermel. Predicting accurate and actionable static
analysis warnings. In 2008 ACM/IEEE 30th International Conference
on Software Engineering, pages 341–350. IEEE, 2008.

[41] Caitlin Sadowski, Edward Aftandilian, Alex Eagle, Liam Miller-
Cushon, and Ciera Jaspan. Lessons from building static analysis
tools at google. Communications of the ACM, 61(4):58–66, 2018.

[42] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,

Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to
prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014.

[43] Ferdian Thung, David Lo, Lingxiao Jiang, Foyzur Rahman,
Premkumar T Devanbu, et al. To what extent could we detect
ﬁeld defects? an empirical study of false negatives in static bug
ﬁnding tools. In 2012 Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering, pages 50–59. IEEE,
2012.

[44] David A Tomassi and Cindy Rubio-Gonz´alez. On the real-
world effectiveness of static bug detectors at ﬁnding null pointer
In 2021 36th IEEE/ACM International Conference on
exceptions.
Automated Software Engineering (ASE), pages 292–303. IEEE, 2021.
[45] H. Tu and T. Menzies. FRUGAL: unlocking SSL for software

analytics. ASE, 2021.

[46] H. Tu and T. Menzies. DebtFree: minimizing labeling cost in
self-admitted technical debt identiﬁcation using semi-supervised
learning. EMSE, 2022.

[47] H. Tu, Z. Yu, and T. Menzies. Better data labelling with emblem

(and how that impacts defect prediction). IEEE TSE, 2022.

[48] Carmine Vassallo,

Sebastiano Panichella, Fabio Palomba,
Sebastian Proksch, Harald C Gall, and Andy Zaidman. How
developers engage with static analysis tools in different contexts.
Empirical Software Engineering, 25(2):1419–1457, 2020.

[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
Attention is all you need. Advances in neural information processing
systems, 30, 2017.

[50] Junjie Wang, Song Wang, and Qing Wang.

Is there a” golden”
feature set for static warning identiﬁcation? an experimental
In Proceedings of the 12th ACM/IEEE international
evaluation.
symposium on empirical software engineering and measurement, pages
1–10, 2018.

[51] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. Deep
IEEE
semantic feature learning for software defect prediction.
Transactions on Software Engineering, 46(12):1267–1293, 2018.
[52] Martin White. Deep representations for software engineering.
In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, volume 2, pages 781–783. IEEE, 2015.

[53] Ian H. Witten, Eibe Frank, and Mark A. Hall. Data mining:
practical machine learning tools and techniques, 3rd Edition. Morgan
Kaufmann, Elsevier, 2011.
ISBN 9780123748560. URL https:
//www.worldcat.org/oclc/262433473.

[54] Xueqi Yang, Jianfeng Chen, Rahul Yedida, Zhe Yu, and Tim
Menzies. Learning to recognize actionable static code warnings

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

14

(is intrinsically easy). Empirical Software Engineering, 26(3):1–24,
2021.

[55] Xueqi Yang, Zhe Yu,

and Tim Menzies.
Understanding static code warnings: An incremental ai approach.
Expert Systems with Applications, 167:114134, 2021.

Junjie Wang,

[56] Rahul Yedida and Tim Menzies. On the value of oversampling for
deep learning in software defect prediction. IEEE Transactions on
Software Engineering, 2021.

[57] Zhe Yu, Fahmid Morshed Fahid, Huy Tu, and Tim Menzies.
Identifying self-admitted technical debts with jitterbug: A two-
step approach. IEEE Transactions on Software Engineering, 2020.
[58] Fiorella Zampetti, Simone Scalabrino, Rocco Oliveto, Gerardo
Canfora, and Massimiliano Di Penta. How open source projects
use static code analysis tools in continuous integration pipelines.
In 2017 IEEE/ACM 14th International Conference on Mining Software
Repositories (MSR), pages 334–344. IEEE, 2017.

[59] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas
Beyer.
In
Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 1476–1485, 2019.

S4l: Self-supervised semi-supervised learning.

[60] Xiaojin Jerry Zhu. Semi-supervised learning literature survey.

2005.

is a Ph.D. student at
Hong Jin Kang
Singapore Management
His
University.
research interests include machine learning
for software engineering, and mining rules and
speciﬁcations. https://kanghj.github.io/.

Huy Tu holds a Ph.D. in Computer Science from
North Carolina State University, Raleigh, NC.
They explored frugal
labeling processes while
improving the data quality for software analytics.
Now, they works for Meta Platforms, Inc.
https://kentu.us.

research

Xueqi Yang is a Ph.D. student
in Computer
Science at North Carolina State University.
Her
automatic
static analysis and applying human-assisted
AI
engineering.
https://xueqiyang.github.io/.

algorithms

interests

software

include

in

Rahul Yedida is a PhD student in Computer
Science at NC State University. His research
interests
software
engineering and machine learning for software
engineering. https://ryedida.me.

automated

include

David Lo is a Professor in Computer Science at
Singapore Management University. His research
interests include software analytics, empirical
software engineering, cybersecurity, and SE4AI.
http://www.mysmu.edu/faculty/davidlo/.

Tim Menzies (IEEE Fellow, Ph.D. UNSW, 1995)
is a Professor in computer science at NC State
University, USA. His research interests include
software engineering (SE), data mining, artiﬁcial
intelligence, and search-based SE, open access
science. http://menzies.us.

