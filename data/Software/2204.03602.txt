2
2
0
2

r
p
A
7

]
E
M

.
t
a
t
s
[

1
v
2
0
6
3
0
.
4
0
2
2
:
v
i
X
r
a

A General Class of Trimodal Distributions: Properties
and Inference

Roberto Vila ∗1, Victor Serra † 1, Mehmet N. Çankaya ‡2,3, and Felipe Quintino §4

1Department of Statistics, University of Brasília, Brasília, Brazil
2 Department of International Trading and Finance, Faculty of Applied Sciences, Uşak University, Uşak, Turkey;
3 Department of Statistics, Faculty of Art and Sciences, Uşak University, Uşak, Turkey
4Department of Mathematics and Statistics, Federal University of Rondônia, Paraná, Brazil

April 8, 2022

Abstract

The modality is important topic for modelling. Using parametric models is an eﬃcient way when
real data set shows trimodality. In this paper we propose a new class of trimodal probability distribu-
tions, that is, probability distributions that have up to three modes. Trimodality itself is achieved by
applying a proper transformation to density function of certain continuous probability distributions.
At ﬁrst, we obtain preliminary results for an arbitratry density function g(x) and, next, we focus
on the Gaussian case, studying trimodal Gaussian model more deeply. The Gaussian distribution is
applied to produce the trimodal form of Gaussian known as normal distribution. The tractability of
analytical expression of normal distribution, and properties of the trimodal normal distribution are
important reasons why we choose normal distribution. Furthermore, the existing distributions should
be improved to be capable of modelling eﬃciently when there exists a trimodal form in a data set.
After new density function is proposed, estimating its parameters is important. Since Mathematica
12.0 software has optimization tools and important modelling techniques, computational steps are
performed by using this software. The bootstrapped form of real data sets are applied to show the
modelling ability of the proposed distribution when real data sets show trimodality.

Keywords. Class of distributions · unimodality · bimodality · trimodality · inference.
Mathematics Subject Classiﬁcation (2010). MSC 60E05 · MSC 62Exx · MSC 62Fxx.

1

Introduction

The modality is an important topic when the nature of phenomena can be modelled by using the function
which can be capable to have diﬀerent forms of peaks. The modality can occur when there exists an
irregularity in the output of an experiment.
In the statistical view point, the random variables are
In other words, there can be a mixing of some populations even if same
nonidentically distributed.

∗rovig161@gmail.com
†victorserra92@gmail.com
‡mehmet.cankaya@usak.edu.tr
§felipe.quintino@unir.br

1

 
 
 
 
 
 
experiment is conducted while getting outputs of the corresponding experiment [1, 4, 5, 6].
In such
a situation, location and scale parameters of the mixed populations are important to get the central
tendency and disperson (statistics) of the mixed form of the population in the experiment after shape,
scale and bimodality parameters of function in the statistical theory are necessary components of a
function which is used for conducting an eﬃcient modelling [2, 3, 4, 8, 9, 12, 17].

There are many well-known distributions used at the statistical inference in which regression and its
counter parts such as time series, design of experiments, structural equation modelling in the applications
from social science. There are diﬀerent techniques to produce a probability density function [7]. The an-
alytical tractability and properties of the proposed distribution are important when the new distribution
is used for modelling. For example, the existence of moments and entropy function are important when
the modelling on the real data sets are performed [24, 34]. The well-known normal distribution which
is also known as Gaussian distribution should be transformed into a trimodal form when data sets show
trimodality. The advantage of using a trimodal distribution is that a real data set can be a combination
of two, three or more normal distributions with diﬀerent parameter values for location and scale. The
apperance of such mixed distributions can be in a trimodal form. Especially, since the working principle
of a phenomena depends on many factors, it is reasonable to expect that a trimodal form for a real
data set can occur. In other words, it is assumed that the random variables are identically distributed.
However, the identicality is a restrictive assumptation for modelling a data set. The parametric models
are necessary to perform an eﬃcient modelling when trimodal representation in a data set exists due to
the structure of non-identicality (hetero data) or the mixing form of distributions. On the other side, if
observations x1, x2, · · · , xn are distributed as a parametric model such as trimodal form, then they are
identical, that is, it depends on where and how you look at the results of experiments, because we do
already have ﬁnite sample size. While managing an eﬃcient modelling on real data sets, it is reasonable
to consult parametric models which can be capable of dealing with diﬀerent forms of modality. The
bimodality parameters ρ and δ with shape parameter α of Maxwell distribution in [18] are tools for us to
generate diﬀerent forms of modality. There are diﬀerent degrees of mixing via Maxwell distribution and
the expression which can help us to have diﬀerent forms of modality of function for generating modality
via compounding distributions [9, 12]. The bimodal form on the positive part of the real line produces a
trimodal form when it is reﬂected to the negative part of the real line via mirror imaging [1, 13, 14, 15].
The diﬀerent degree of bimodality, i.e. the length of periodicity of modality, on the positive part of the
real line can be constructed by producing a new objective function based on the deformation [9, 36],
because deformation which can be regarded as a kind of rescaling can make the diﬀerent length for
periodicity [2, 10, 11].

The smooth kernel distribution will be used to ﬁt data sets; because, the strict and soft forms of trimodal
normal distribution should be compared with smooth kernel technique to perform a comparison among
them. Three normal distributions, represented by functions g(x; µ1, σ1), g(x; µ2, σ2) and g(x; µ3, σ3), can
be mixed to get the trimodal form of normal distribution g. Thus, the modelling performance of the
trimodal distribution constructed for the normal distribution g can be tested for the mixed data sets.
In fact, the mixed form of two or three function with one mode and symmetric can show the symmetric
form with trimodal representation. In other words, the groups around location parameter can be divided
into two forms [1, 4, 5, 6].

The main aim in this paper is to propose a distribution with trimodal form on the real line via using
a technique, as is given by [9, 5, 6, 16]. We keep to follow the symmetric case and our aim is to focus
on the trimodal form on the real line. When there exists a trimodal form in a data set, the location
and scale parameters should be estimated eﬃciently. In other words, each group coming from groups
g(x; µ1, σ1), g(x; µ2, σ2) and g(x; µ3, σ3) has its values for location and scale parameters. In our case, we

2

try to estimate one location parameter and one scale parameter when the mixed data sets for two or
three groups are used. It is important to note that the apperance of trimodality can occur via mixing the
diﬀerent values of parameters of functions. Since the true model for the mixed three normal distributions
is chosen to estimate the location and scale parameters precisely, the performance of modelling will
be increased greatly when the probability density functions having modality property are taken into
account. For example, the mixed distribution has parameters which are the mixing proportion w1, w2
and w3 = 1−w1−w2 for three groups which are necessary to estimate. For the mixed normal distributions,
we have µ1, σ1, µ2, σ2, µ3, σ3, w1 and w2 (see Section 5). In totally, there are 8 parameters which have to
be estimated. However, in our case we have three main parameters and also µ and σ of the distribution.
Thus, there will be 5 parameters which will be estimated. The optimization of the log-likelihood function
according to these 5 parameters of a function can be easy to reach the global point of the log(f ) when
compared with a function including 8 parameters.
In addition, we have only one location and scale
parameter which can be free from the mixing proportion w1, w2 and w3 = 1 − w1 − w2. The numerical
computation while conducting the optimization of log-likelihood according to parameters can include the
less numerical errors. Note that the numerical errors in the function with 8 parameters can be bigger
than that of 5 parameters. Thus, the more precise evaluation can be achieved for the numerically precise
evaluation of estimating the parameters µ and σ, which is why we prefer to consider proposing such a
trimodality while conducting an eﬃcient ﬁtting on the real data sets. On the other side of the modelling
perspective, the structure of grouping cannot be determined only for the used mixing proportions w1, w2
and w3 = 1 − w1 − w2. There can be irrational proportions such as 1/6, 1/9, etc. for the mixing in a
data set. The precise evaluation in computation for the true value of irrational proportion cannot be
performed accurately, which makes a disadvantage for us when we use the estimated values of these kinds
of the proportions such as irrational ones.

This paper is organized as follows. In Section 2, we deﬁne the class of trimodal probabilistic models.
In Section 3, some structural properties of the proposed model are examined. We provide a formal proof
for the trimodality of a class of symmetric kernel densities, present a stochastic representation, and we
provide the closed formulas for the moments, entropies and stochastic representation. The existence of
these expressions is important to use the proposed distrbution for ﬁtting the data sets. Section 4 is
divided to describe the proposed model when the normal (Gaussian) is applied (see Table 1). For this
case, some properties such as modality, moments, Shannon entropy among others also are discussed.
In Sections 5, we introduce the mixing form. Section 6, represents the log
likelihood function used
In Section 7, the real data sets are applied. Section 8 is for
as a method for parameter estimation.
conclusion and future works. In Appendices as a supplementary material, we provide proofs and codes
of Mathematica software.

q

2 A class of continuous probability distributions

Let g : D ⊂ R → [0, ∞), D = supp(g) 6= ∅, be a kernel density with corresponding cumulative distribution
function (CDF) denoted by G. The function g can be associated (or not) with an additional parameter
ξ (or vector ξ). For a random variable X we deﬁne the following probability density function (PDF)

f (x; θ) = σ
Zθ

(cid:20)
ρ + δT

(cid:18) x − µ
σ

; α, p

(cid:19)(cid:21)

(cid:18) x − µ
σ

g

(cid:19)

,

x − µ
σ

∈ D,

(2.1)

where θ = (µ, σ, α, ρ, δ) is a parameter vector such that µ ∈ R is a location parameter, σ > 0 is a scale
parameter, α > 0 is a shape parameter and ρ (cid:62) 0 and δ (cid:62) 0 are parameters that control diﬀerent forms

3

of modality of distribution. Note that ρ and δ cannot be zero simultaneously. The function Zθ appearing
in the deﬁnition of f is a normalizing factor and the function T : D ⊂ R → (0, 1) is given by

T (x; α; p) = γ(p, x2/α2)

Γ(p)

with p > 0 known.

(2.2)

Here γ(p, u) = R u
0 wp−1e−wdw is the incomplete gamma function and Γ(p) is the gamma function. For
p = 3/2 and D = (0, ∞), the function T (x; α, p) on D deﬁnes the Maxwell distribution with scale
parameter α.

Hereafter, we will denote X ∼ TD(θ) for a random variable X that follows the trimodal distribution

(2.1).

When δ = 0 and ρ 6= 0 ﬁxed in (2.1), the original density g is recovered.
Since 0 < T (x; α, p) < 1 for almost all x ∈ D, we have 0 < Zθ (cid:54) (ρ + δ)σ. A simple calculation shows

that (see Corollary 3.6)

Zθ = (ρ + δ)σ + δσ

nE(cid:2)G(−α

√

Y )(cid:3) − E(cid:2)G(α

√

Y )(cid:3)o

,

(2.3)

where Y ∼ Gamma(p, 1) and G is the corresponding CDF of g. Furthermore, the CDF of X ∼ TD(θ),
denoted by F (x; θ), is written as
(cid:18) x − µ
F (x; θ) = ρσ
σ
Zθ

(cid:18) x − µ
σ

(cid:18) x − µ
σ

+ δσ
Zθ

E(cid:2)G(−α

Y )(cid:3) + T

; α, p

(cid:19)(cid:27)

√

G

G

(cid:26)

(cid:19)

(cid:19)

−

δσ
Zθ

nEh1

{Y (cid:54)( x−µ

σα )2}G(−α

√

Y )i1{x<µ} + Eh1

{Y (cid:54)( x−µ

σα )2}G(α

√

Y )i1{x(cid:62)µ}

o

,

(2.4)

for each x ∈ R. For more details, see Corollary 3.6.

√

Taking x = µ in (2.4), we get F (µ; θ) = σ{ρG(0) + δE[G(−α

Y )]}/Zθ. Let W be a random variable
If the distribution G of W is symmetric about zero, then Zθ = ρσ +
with corresponding CDF G.
2δσE[G(−α
Y )], G(0) = 1/2, and then F (µ; θ) = 1/2. So, in this case, µ is location parameter of X.
Moreover, by letting x → ∞ in (2.4), a simple observation shows that F (x; θ) tends to 1, showing that
the parametric function in (2.1) is in fact a PDF.

√

Some natural examples of kernel densities g to be plugged into (2.1), with p given, where trimodality

shape is observed, are presented in Table 1.

Table 1: Some kernel densities (g) that generate multimodality in the model (2.1).

Distribution

g

Trimodal Gumbel

e−x−e−x

Trimodal Laplace

Trimodal Logistic

Trimodal Cauchy

e−|x|

1
2

e−x
(1+e−x)2

1
π(1+x2)

Trimodal Student-t

Trimodal Normal

(1 + x2
ν

Γ( ν+1
2 )
√
νπΓ( ν
2 )
φ(x) = 1√
2π

)−(ν+1)/2

e−x2/2

4

G

e−e−x

ξ

−

1 + 1
2

e−x[1{x(cid:54)0} − 1{x(cid:62)0}] −

−

−

1
1+e−x

1
π

arctan(x) + 1
2
2 , ν+1
) 2F1( 1
√

1
2
2
Φ(x) = R x
−∞ φ(t)dt

+ Γ( ν+1

2 ;− x2
2 ; 3
ν )
νπΓ( ν
2 )

ν > 0 R

−

R

D

R

R

R

R

Here 2F1 is the hypergeometric function and erf(x) = 2 R x
0
the Gauss error function).

√

e−t2dt/

π is the error function (also called

3 Structural properties

In this section, some basic properties such as trimodality for symmetric kernels, moments and truncated
moments, and entropies for X ∼ TD(θ) are discussed in detail.

3.1 Trimodality for a class of symmetric kernel densities

In this subsection, we suposse that the kernel density g in (2.1) has the following form

g(x) = g(0) e

− R x

0 th(t2) dt

,

x ∈ R,

(3.1)

for some positive real function h such that the integral R x
to

0 th(t2) dt exists. Notice that (3.1) is equivalent

g0(x) = −xh(x2)g(x),

x ∈ R.

(3.2)

It is immediate to verify that g, as deﬁned in (3.1), is symmetric about zero, that is, g(x) = g(−x) on the
real line D = R. For example, in the Laplace, Cauchy, Student-t and Normal kernel densities (see Table
1) we have h(y) = 1/

y, h(y) = 2/(1 + y), h(y) = (ν + 1)/(ν + y) and h(y) = 1, ∀y > 0, respectively.

√

Moreover, we assume that h has the following form

h(y) =

C
(α2A + y)β−p

for C (cid:62) 1, A (cid:62) 0 and β > p.

(3.3)

That is, h(y), y > 0, decays polynomially. The function h(y) = 1 corresponding to the Normal distribution
is not of the form (3.3), then the next result cannot be applied and a separate study must be carried out.
In this paper, the Gaussian case will be studied in detail in Section 4.
For a formal proof of following lemma, see Section A of Appendix.

Lemma 3.1. Let h be as in (3.3). For some ρ > 0 the function R, deﬁned by

R(y) = 2δ

(cid:20) (1/α2)p

Γ(p) yp−1e−y/α2(cid:21)

(cid:20)
ρ + δ

−

γ(p, y/α2)
Γ(p)

(cid:21)
h(y),

y > 0,

has at most two real roots.

Proposition 3.2. Let g be a kernel density as in (3.1), with h as in (3.3). A point x ∈ R is a critical
point of density (2.1) if x = µ or R[(x − µ)2/σ2] = 0, where R is as in Lemma 3.1.

Proof. The proof is immediate since, by using Equation (3.2), the ﬁrst-order derivative of f (x; θ), with
(cid:4)
respect to x, is given by f 0(x; θ) = [(x − µ)/σ]g((x − µ)/σ)R[(x − µ)2/σ2]/(σZθ).

Theorem 3.3 (Uni- bi- or trimodality). If X ∼ TD(θ) then the following hold:

(1) If R has no real roots then f (x; θ) is unimodal with mode x = µ.

(2) If R has one real root then f (x; θ) is bimodal with minimum point x = µ.

5

(3) If R has two distinct real roots then f (x; θ) is trimodal where x = µ is one of the modes.

Proof. It is clear that if R has no real roots, by Proposition 3.2, x = µ is the only critical point of the
density f . Since limx→±∞ f (x; θ) = 0, the point x = µ is a mode. This proves Item (1).

In order to prove Item (2), we suppose that R has one real root, denoted by a. By Proposition 3.2, it
a are three critical points of f . Since limx→±∞ f (x; θ) = 0, the point
√

follows that x = µ and x = µ ± σ
x = µ is a minimum and x = µ ± σ

a are two symmetrical modes. This proves the second item.

√

Now, we assume that R has two distinct real roots, denoted by a and b. Without loss of generality, we
b are
√
b
(cid:4)

can assume that a < b. Again, by Proposition 3.2 we have that x = µ, x = µ ± σ
ﬁve critical points of f . Since limx→±∞ f (x; θ) = 0 and a < b, the critical points x = µ and x = µ ± σ
are modes and x = µ ± σ

a are minimum points. Hence, the proof of Item (3) follows.

a and x = µ ± σ

√

√

√

3.2 Moments

Theorem 3.4. Let X ∼ TD(θ) and L : R → R be a Borel-measurable function. Then, the expectation
of random variable L(X) with X (cid:54) b and b ∈ R, is given by
E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρσ
Zθ

σ }L(Wµ,σ)(cid:3)

{W (cid:54) b−µ

E(cid:2)1

+ δσ
Zθ

+ δσ
Zθ

(cid:26)

E

(cid:20)
1

{Y (cid:54)( b−µ

ασ )2, W (cid:54) b−µ

σ }L(Wµ,σ)

(cid:21)

(cid:20)
1

+ E

{Y (cid:62)( b−µ

ασ )2, W (cid:54)−α

√

Y }L(Wµ,σ)

(cid:21)(cid:27)

1{b<µ}

(cid:26)

E h1

√

Y }L(Wµ,σ)i + E

{W (cid:54)−α

(cid:20)
1

{Y (cid:54)( b−µ

ασ )2, α

√

Y (cid:54)W (cid:54) b−µ

σ }L(Wµ,σ)

(cid:21)(cid:27)

1{b(cid:62)µ},

where Wµ,σ = σW + µ, W is a continuous random variable with CDF G (that for brevity we write
W d= G), Y ∼ Gamma(p, 1), and W and Y are independent.

Proof. By using the deﬁnition of expectation and by taking the change of variables w = (x − µ)/σ and
dx = σdw, we have

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρ
Zθ

Z

σD+µ

1{x(cid:54)b}L(x)g

(cid:19)

(cid:18) x − µ
σ

dx

+ δ
Zθ

Z

σD+µ

1{x(cid:54)b}L(x)T

(cid:18) x − µ
σ

; α, p

(cid:19)

(cid:19)

(cid:18) x − µ
σ

g

dx

= ρσ
Zθ

Z

D

1

{w(cid:54) b−µ

σ }L(σw + µ)g(w)dw + δσ

Zθ

1D(ω)τ (w, y) dydw,

(3.4)

ZZ

w(cid:54)(b−µ)/σ
0<y(cid:54)w2/α2

where, for notational simplicity, we denote

τ (w, y) = L(σw + µ)g(w)

(cid:20) yp−1e−y
Γ(p)

(cid:21)
.

There are two cases to consider according to whether ξ := (b − µ)/σ < 0 or ξ := (b − µ)/σ (cid:62) 0; see Figure
1 (a) and (b).

6

Figure 1: (a) {w (cid:54) ξ, 0 < y (cid:54) w2/α2, ξ < 0}; (b) {w (cid:54) ξ, 0 < y (cid:54) w2/α2, ξ (cid:62) 0}.

In the former case,

1D(ω)τ (w, y) dydw =

Z ( b−µ

ασ )2

 Z b−µ

σ

0

−∞

1D(ω)τ (w, y) dw

dy

!

ZZ

w(cid:54)(b−µ)/σ
0<y(cid:54)w2/α2

Z ∞

+

 Z −α

√

y

( b−µ

ασ )2

−∞

1D(ω)τ (w, y) dw

dy

!

and in the latter case,

1D(ω)τ (w, y) dydw =

Z ∞

 Z −α

√

y

0

−∞

1D(ω)τ (w, y) dw

dy

!

ZZ

w(cid:54)(b−µ)/σ
0<y(cid:54)w2/α2

Hence, by combining the last two integral identities with (3.4), when b < µ, we have

Z ( b−µ

ασ )2

+

0

 Z b−µ
σ
√

α

y

1D(ω)τ (w, y) dw

dy.

!

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρσ
Zθ

Z

D

1

{w(cid:54) b−µ

σ }L(σw + µ)g(w)dw + δσ

Zθ

Z ( b−µ

ασ )2

 Z b−µ

σ

0

−∞

1D(ω)τ (w, y) dw

dy

!

+ δσ
Zθ

Z ∞

 Z −α

√

y

( b−µ

ασ )2

−∞

1D(ω)τ (w, y) dw

dy

!

and, when b (cid:62) µ,

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρσ
Zθ

Z

D

1

{w(cid:54) b−µ

σ }L(σw + µ)g(w)dw + δσ

Zθ

Z ∞

 Z −α

√

y

0

−∞

1D(ω)τ (w, y) dw

dy

!

+ δσ
Zθ

Z ( b−µ

ασ )2

0

 Z b−µ
σ
√

α

y

1D(ω)τ (w, y) dw

dy.

!

7

Then there are W d= G and Y ∼ Gamma(p, 1) independent so that, for b < µ,

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρσ
Zθ

E(cid:2)1

{W (cid:54) b−µ

σ }L(Wµ,σ)(cid:3)

(cid:26)

(cid:20)
1

E

+ δσ
Zθ

{Y (cid:54)( b−µ

ασ )2, W (cid:54) b−µ

(cid:21)
σ }L(Wµ,σ)

+ E

(cid:20)
1

{Y (cid:62)( b−µ

ασ )2, W (cid:54)−α

(cid:21)(cid:27)

√

Y }L(Wµ,σ)

and, for b (cid:62) µ,

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = ρσ
Zθ

E(cid:2)1

{W (cid:54) b−µ

σ }L(Wµ,σ)(cid:3)

+ δσ
Zθ

(cid:26)

E h1

√

Y }L(Wµ,σ)i + E

{W (cid:54)−α

(cid:20)
1

{Y (cid:54)( b−µ

ασ )2, α

√

This completes the proof.

By taking b → ∞ in Theorem 3.4, with (X − µ)/σ instead of X, we get

Corollary 3.5. Under the hypotheses of Theorem 3.4,

Y (cid:54)W (cid:54) b−µ

σ }L(Wµ,σ)

(cid:21)(cid:27)

.

(cid:4)

(cid:20)
E
L

(cid:18) X − µ
σ

(cid:19)(cid:21)

=

(ρ + δ)σ
Zθ

E[L(W )] + δσ
Zθ

nE h1

√

Y }L(W )i

− E h1

√

Y }L(W )io

.

{W (cid:54)α

{W (cid:54)−α

Corollary 3.6. Under the hypotheses of Theorem 3.4, if

(a) b → ∞ and L(x) = 1, ∀x ∈ σD +µ, then the formula (2.3) for the normalizing factor Zθ is obtained.

(b) b = x ∈ R ﬁxed and L(x) = 1, ∀x ∈ σD + µ, then the formula (2.4) of the CDF F (x; θ) is obtained.

By taking b → ∞ in Theorem 3.4, with L(x) = xn, ∀x ∈ σD + µ and n (cid:62) 1 integer, by a binomial

expansion, we have the next formula for the moments.

Corollary 3.7. The n-th moment of X ∼ TD(θ) is written as

E(X n) =

!

µn−kσk

n
X

k=0

n
k

(cid:26) (ρ + δ)σ
Zθ

E(W k) + δσ
Zθ

hE (cid:16)1

√

Y }W k(cid:17)

− E (cid:16)1

{W (cid:54)α

√

Y }W k(cid:17)i(cid:27)

.

{W (cid:54)−α

The above formula informs that the moments of X (whenever they exist) depends on the existence of
moments of 1

Y }W , with W d= G and Y ∼ Gamma(p, 1) independent.

{W (cid:54)±α

√

3.3 Entropies

The Tsallis [31] entropy associated with the random variable X ∼ TD(θ) is deﬁned as

Sq(X) =






Z

−

σD+µ

Z

−

σD+µ

f q(x; θ) log

q f (x; θ) dx,

if q 6= 1,

f (x; θ) log f (x; θ) dx,

if q = 1,

8

 
where, for x > 0,

log

q

(x) =






x1−q − 1
1 − q
log(x),

,

if q 6= 1,

if q = 1,

(3.5)

represents a Box-Cox transformation in statistics (often called deformed logarithm [20, 32]). Since
log
(x) → log(x) when q → 1, we have Sq(X) → S1(X) when q → 1. That is, when q → 1 the
usual deﬁnition of Shannon’s entropy S1(X) [30] is recovered.

q

Proposition 3.8. Under the hypotheses of Theorem 3.4,

E(cid:2)f q−1(X; θ)(cid:3) =

(ρ + δ)σ
Zθ

E(cid:2)f q−1(σW + µ; θ)(cid:3)

+ δσ
Zθ

nE h1

{W (cid:54)−α

√

Y }f q−1(σW + µ; θ)i

− E h1

√

Y }f q−1(σW + µ; θ)io

,

{W (cid:54)α

where W d= G, Y ∼ Gamma(p, 1), and W and Y are independent. Consequently, the Tsallis entropy of
X (whenever it exists) depends on the existence of truncated moments of f q−1(W ; θ).

Proof. By taking b → ∞ in Theorem 3.4, with L(x) = f q−1(x; θ), ∀x ∈ σD + µ, the proof follows.

(cid:4)

For a formal proof of next result, see Section A of Appendix.

Proposition 3.9. If Sq(W ) exists and q > 0, then Sq(X), with X ∼ TD(θ), also exists.

Remark 3.10. As consequence of Proposition 3.9, by letting q → 1, the Shannon entropy S1(X) exists
whenever S1(W ) also exists.

For a rigorous proof of next result, see Section A of Appendix.

Proposition 3.11. Under the hypotheses of Theorem 3.4, the Shannon entropy of X ∼ TD(θ) is written
as

S1(X) = log(Zθ) −

(ρ + δ)σ
Zθ

E(cid:2) log(ρ + δT (W ; α, p))(cid:3)

−

+

nE h1

δσ
Zθ
(ρ + δ)σ
Zθ

{W (cid:54)−α

√

Y }

log(ρ + δT (W ; α, p))i

− E h1

{W (cid:54)α

√

Y }

log(ρ + δT (W ; α, p))io

S1(W ) + δσ
Zθ

nE h

(cid:0)1

S1

√

Y }W (cid:1)i

− E h

(cid:0)1

S1

√

Y }W (cid:1)io
,

{W (cid:54)α

{W (cid:54)−α

where T was deﬁned in (2.2).

3.4 Stochastic representation

Let h(u), 0 < u < 1, be a PDF with corresponding CDF H. Let S : D → (0, 1) be an injective and
increasing transformation, where D is a non-empty set of R. We consider the following CDF:

F (z) =

Z S(z)

0

h(u)du = H(S(z)),

z ∈ D.

(3.6)

9

We also deﬁne by f to the corresponding PDF of F . That is, F 0(z) = f (z) = h(S(z))S 0(z) for almost all
z ∈ D.

We deﬁne the PDF h as follows

h(u) = σ
Zθ

(cid:2)ρ + δT (cid:0)G−1(u); α, p(cid:1)(cid:3),

0 < u < 1, ρ (cid:62) 0, δ (cid:62) 0, α > 0, σ > 0,

(3.7)

where G and G−1, respectively, are the CDF deﬁned in Section 2 and its inverse function, and T is as in
(2.2). When δ = 0, h reduces to the continuous uniform distribution on the interval (0, 1). The CDF H
of h is given by

H(u) = ρ
Zθ

u + δ
Zθ

nE(cid:2)G(−α

√

o
Y )(cid:3) + S(cid:0)G−1(u); α, p(cid:1)u

(cid:26)

−

δ
Zθ

Eh1

{Y (cid:54)[ G−1(u)

α

√

G(−α

]2}

Y )i1{G−1(u)<0} + Eh1

{Y (cid:54)[ G−1(u)

α

√

Y )i1{G−1(u)(cid:62)0}

(cid:27)

,

G(α

]2}

where Y ∼ Gamma(p, 1).

If S : D → (0, 1) is deﬁned by S(z) = G(z), ∀z ∈ D, by (3.6), the family of trimodal distributions in

(2.1) is obtained. That is,

F (x; θ) = F

(cid:19)

(cid:18) x − µ
σ

(cid:18)

G

= H

(cid:17)(cid:19)

(cid:16) x − µ
σ

(3.8)

and f (x; θ) = 1

σ f ((x − µ)/σ).

If U is distributed according to (3.7) and X ∼ TD(θ), then, by (3.8), the random variable X admits

the following stochastic representation:

X = µ + σG−1(U ).

4 The Gaussian case

In this section, the standard normal kernel density g(x) = φ(x), x ∈ R, is plugged into (2.1). Some
structural properties as modality, moments, entropies and rate of the distribution are discussed.

The CDF corresponding to the normal kernel density g is G(x) = Φ(x), with

Φ(x) =

Z x

−∞

φ(t) dt =

(cid:20)

1
2

1 ± erf

(cid:18)

±

x
√
2

(cid:19)(cid:21)

,

where erf(x) is the error function. So, in this section, we consider the following PDF

f (x; θ) =

1

Zθ

(cid:20)
ρ + δT

(cid:18) x − µ
σ

; α, p

(cid:19)(cid:21)

φ

(cid:18) x − µ
σ

(cid:19)

,

x ∈ R,

where T is as in (2.2) and Zθ is the normalizing factor (2.3), which is given by
Y )(cid:3)o

Y )(cid:3) − E(cid:2)Φ(α

Zθ = (ρ + δ)σ + δσ

nE(cid:2)Φ(−α

√

√

.

(4.1)

(4.2)

(4.3)

We will denote X ∼ TDΦ(θ) for a random variable X that follows (4.2). Plots of the TDΦ density, where
trimodality is observed, are given in Figures 2a and 2b.

10

(a) The strict form of modality for trimodal normal dis-
tribution.

(b) The soft form of modality for trimodal normal distri-
bution.

Figure 2: PDFs of trimodal normal distribution.

4.1 A study on the modality

For a mathematical proof of next result, see Section A of Appendix.

Lemma 4.1. The function R, deﬁned by

R(y) = 2δ

(1/α2)p−1e−y/α2
Γ(p)

(cid:20)
ρ + δ

−

γ(p, y/α2)
Γ(p)

(cid:21)
,

y > 0,

has at most two real roots.

Remark 4.2. Notice that when ρ is suﬃciently large, the function R has no roots. For suﬃciently small
ρ, R have one, or two roots depending on whether p (cid:54) 1 or p > 1. Furthermore, when δ = 0 (with ρ > 0)
or α → ∞, the function R has no roots.

Proposition 4.3. A point x ∈ R is a critical point of density (4.2) if x = µ or R[(x − µ)2/σ2] = 0, where
R is as in Lemma 4.1.

Proof. The proof follows from identity f 0(x; θ) = [(x − µ)/σ]g((x − µ)/σ)R[(x − µ)2/σ2]/(σZθ).

(cid:4)

Theorem 4.4 (Uni- bi- or trimodality). Let X ∼ TDΦ(θ) and R be the function deﬁned in Lemma 4.1.
The following hold:

(1) If R has no real roots then f (x; θ) is unimodal with mode x = µ.

(2) If R has one real root then f (x; θ) is bimodal with minimum point x = µ.

(3) If R has two distinct real roots then f (x; θ) is trimodal where x = µ is one of the modes.

Proof. The proof follows the same steps of proof of Theorem 3.3 by taking R instead R, and by using
(cid:4)
Proposition 4.3 instead of Proposition 3.2.

11

α=1,ρ=1,δ=1.5,μ=0,σ=4α=0.9,ρ=0.95,δ=1.5,μ=0,σ=4α=0.9,ρ=0.7,δ=1.5,μ=0,σ=4α=0.9,ρ=0.7,δ=3.5,μ=0,σ=4-15-10-551015x0.020.040.060.08f(x)α=1.9,ρ=0.3,δ=1.8,μ=0,σ=4α=1.5,ρ=0.7,δ=1.5,μ=0,σ=4α=1.9,ρ=0.7,δ=3.5,μ=0,σ=4α=2.1,ρ=0.7,δ=3.5,μ=0,σ=4-15-10-551015x0.020.040.06f(x)4.2 The normalizing factor

In what follows we ﬁnd a closed expression for the normalizing factor Zθ in (4.2). By (4.3) it is necessary
Y )], where Y ∼ Gamma(p, 1). Indeed, using the formula (4.1) and then taking the
to calculate E[Φ(±α
change of variables z = √

y and dy = 2zdz, we obtain

√

E(cid:2)Φ(±α

√

Y )(cid:3) =

Z ∞

Φ(±α

0

√

y) yp−1e−y
Γ(p)

dy =

=

1
2

1
2

+

1
2

Z ∞

(cid:18)

±

erf

0

√
α
√

y
2

(cid:19) yp−1e−y
Γ(p)

dy

Z ∞

(cid:18)

±

erf

+

0

αz
√
2

(cid:19) z2p−1e−z2
Γ(p)

dz.

(4.4)

(4.5)

By using the formula (see Item 8 in Subsection 4.3 of reference [25]):

Z ∞

0

erf(ax)xpe−b2x2 dx = a
√
π

b−p−2Γ(cid:16) p
2

+ 1(cid:17)

2F1

(cid:18) 1
2 ,

p
2

+ 1;

3
2

; −

(cid:19)
,

a2
b2

b2 > 0, p > −2,

where 2F1(a1, a2; b1; x) is the generalized Hypergeometric function, we have

Z ∞

(cid:18)

±

erf

0

αz
√
2

(cid:19) z2p−1e−z2
Γ(p)

dz = ±

αΓ(p + 1
2
√

)
2π Γ(p) 2F1

(cid:18) 1
2 , p +

1
2

;

3
2

; −

(cid:19)

.

α2
2

Therefore,

E(cid:2)Φ(±α

√

Y )(cid:3) =

1
2 ±

)
αΓ(p + 1
2
√
2π Γ(p) 2F1

(cid:18) 1
2 , p +

1
2

;

3
2

; −

(cid:19)

.

α2
2

Replacing (4.6) in (4.3), we obtain the following closed expression for the normalizing factor Zθ:

Zθ = (ρ + δ)σ −

2δσαΓ(p + 1
2
2π Γ(p)

√

)

2F1

(cid:18) 1
2 , p +

1
2

;

3
2

; −

(cid:19)
.

α2
2

(4.6)

(4.7)

The ﬂexibility of the TD model is shown. Note that the TD model can be unimodal, bimodal or
trimodal. Figures 2a and 2b show how the TD density function is inﬂuenced by parameters α, ρ and δ.

4.3 Cumulative distribution function

To determine the CDF of X ∼ TDΦ(θ) we use formula (2.4). So, by (2.4), it is essential to calculate
the expectation E[1
Y )], x ∈ R, where Y ∼ Gamma(p, 1). Indeed, similarly to the one
done in (4.5),

{Y (cid:54)( x−µ

Φ(±α

σα )2}

√

Eh1

{Y (cid:54)( x−µ

ασ )2}

Φ(±α

√

Y )i =

1
2 T

(cid:18) x − µ
σ

(cid:19)

+

; α, p

1
Γ(p) I

(cid:18) x − µ
ασ

; ±

α
√
2

, 2p − 1, 1

(cid:19)
,

(4.8)

where I(u; a, b, c) = R u
erf(ax)xbe−c2x2 dx, u > 0, a ∈ R, b > 0, c > 0. Since |erf(x)| (cid:54) 1, we get
0
|I(u; a, b, c)| (cid:54) I(u; 0, b, c) < ∞, and therefore, I(u; a, b, c) always exists. In general, closed form solutions
for the deﬁnite integral I(u; a, b, c) are not available in terms of commonly used functions.

12

X ∼ TDΦ(θ):


F (x; θ) =




Replacing the formulas (4.6) and (4.8) in (2.4), we obtain the following closed formula for the CDF of

σ
Zθ

(cid:20)
ρ + δT

(cid:18) x − µ
σ

; α, p

(cid:19)(cid:21)

Φ

(cid:18) x − µ
σ

(cid:19)

(cid:20) 1
2 −

+ δσ
Zθ
(cid:18) x − µ
σ

(cid:19)

; α, p

αΓ(p + 1
2
√

)
2π Γ(p) 2F1
1
Γ(p) I

−

−

δσ
Zθ

(cid:20) 1
2 T

(cid:18) 1

2 , p +

1
2

;

3
2

; −

α2
2

(cid:19)(cid:21)

(cid:18) x − µ
ασ

(cid:19)(cid:21)

, 2p − 1, 1

; α
√
2

,

if x < µ,

σ
Zθ

(cid:20)
ρ + δT

(cid:18) x − µ
σ

; α, p

(cid:19)(cid:21)

Φ

(cid:18) x − µ
σ

(cid:19)

(cid:20) 1
2 −

+ δσ
Zθ
(cid:18) x − µ
σ

(cid:19)

; α, p

αΓ(p + 1
2
√

)
2π Γ(p) 2F1
1
Γ(p) I

+

−

δσ
Zθ

(cid:20) 1
2 T

(cid:18) 1

2 , p +

1
2

;

3
2

; −

α2
2

(cid:19)(cid:21)

(cid:18) x − µ
ασ

(cid:19)(cid:21)

, 2p − 1, 1

; α
√
2

,

if x (cid:62) µ,

where Zθ and T are as in (4.7) and (2.2), respectively.
Remark 4.5. As expected, since Φ has a symmetric distribution around 0, F (µ; θ) = 1/2 and then,
Q2 = µ is the median and the mean for X ∼ TDΦ(θ).
Remark 4.6. Some examples where I( x−µ
ασ
√

, 2p − 1, 1) in (4.8) admits a closed form are:

2 and by using the following formula (see Item 4 in Subsection 1.5.3, pp. 31, of

; ± α√
2

1. By taking α =

[29]):

Z u

0

erf(ax)xλe−a2x2 dx =

√

2a
π(λ + 2) uλ+2

2F2

(cid:18)

1,

λ
2

+ 1;

3
2 ,

λ
2

+ 2; a2u2

(cid:19)

,

λ > −2,

where 2F2(a1, a2; b1, b2; x) is the generalized Hypergeometric, we have

(cid:18) x − µ
ασ

I

; ±

α
√
2

(cid:19)

, 2p − 1, 1

= ±

√

2
π(2p + 1)

(cid:18) x − µ
ασ

(cid:19)2p+1

(cid:18)

1, p +

2F2

1
2

;

3
2 , p +

3
2

; (cid:16) µ − x
ασ

(cid:17)2(cid:19)

.

2. By taking α =

√

2 and p = 3/2,

(cid:18) x − µ
ασ

I

; ±

α
√
2

(cid:19)

, 2p − 1, 1

= ∓

1
2

(cid:18) x − µ
√
2
σ

(cid:19)

erf

(cid:19)

(cid:18) x − µ
√
2
σ

(cid:20)
exp
−

(cid:18) x − µ
√
2
σ

(cid:19)2(cid:21)

1
√

π

±

4

(cid:26)

(cid:20)
−2
1 − exp

(cid:18) x − µ
√
2
σ

(cid:19)2(cid:21)(cid:27)

√

π
8

±

(cid:20)
erf

(cid:18) x − µ
√
2
σ

(cid:19)(cid:21)2

.

4.4 Moments and Shannon entropy

A routine calculation shows that the moments of X ∼ TDΦ(θ) can be written as (for more details, see
Section B in the Appendix)

E(X n) =

(ρ + δ)σ
Zθ

n
X

k=0

n
k

!

µn−kσk

2−k/2k!
(k/2)!

1{k even}

+

√

δσ
2πΓ(p)

Zθ

n
X

k=0

n
k

!

µn−kσk(cid:2)1 − (−1)k−1(cid:3)

bk/2c
X

bkm/2c
X

m=0

j=0

ck,mdkm,j αkm−2j
(1 + α2
2 +p
2

) km−2j

(cid:18) km − 2j
Γ
2

(cid:19)

+ p

,

13

 
 
where p (cid:62) 1 is an integer, km = k − 2m − 1, Zθ is as in (4.7), cn,m and dnm,j are as in (B.2). In particular,
E(X) = µ and Var(X) = σ2. As a consequence, from Corollary 3.5 the closed expressions for skewness
and kurtosis of random variable X ∼ TDΦ(θ) are easily obtained.

On the other hand, a formula for the Shannon entropy is given by (for more details, see Section C in

the Appendix)

S1(X) = log(Zθ) −

2(ρ + δ)σ
Zθ

∞
X

k=0

1
(2k + 1)

(cid:20)
1 +

1
(ρ + 1)2k+1

+

2δσ
Zθ

∞
X

k=0

(cid:26)

1
(2k + 1)

√

2E(cid:2)Φ(α

Y )(cid:3) − 1 +

1
(ρ + 1)2k+1

∞
X

i=p

∞
X

i=p

eci,k

(cid:21)

2−i(2i)!
i!

eci,kE(cid:0)1

{|Z|(cid:54)α

√

(cid:27)

Y }Z2i(cid:1)

+

(ρ + δ)σ
Zθ

(cid:20)
log(

√

2π) +

(cid:21)

1
2

+ δσ
Zθ

( αΓ(p + 1
)
2
2πΓ(p)

√

1

(1 + α2
2

)p+ 1

2

(cid:20)
log(

√

−

2π) +

1
2

(cid:21)
(cid:0)2E(cid:2)Φ(α

√

Y )(cid:3) − 1(cid:1)

)

,

where Zθ is as in (4.7), the coeﬃcients
in (4.6), and E(1
Y }Z2i) = E(1
{|Z|(cid:54)α
Appendix.

√

eci,k’s are determined by relation (C.6), E(cid:2)Φ(α

Y )(cid:3) is given
Y }Z2i) is determined by (B.3) in the

Y }Z2i) − E(1

{Z(cid:54)−α

√

√

{Z(cid:54)α

√

4.5 Rate of the random variable X ∼ TDΦ(θ)
Following the reference [23], the rate of a continuous random variable X is given by

τX = − lim
x→∞

d log fX (x)
dx

,

where fX (x) denotes its respective PDF.

A simple computation shows that, the rate of X ∼ TDΦ(θ) is

τTDΦ(θ) =





1

σ

lim
x→∞

x − µ
σ

−

2
αΓ(p)

( x−µ
ασ
ρ + T ( x−µ
σ

)2p−1e−( x−µ
ασ )2
; α, p)


 = ∞.

Then, far enough out in the tail, the distribution of X ∼ TDΦ(θ) looks like a Normal distribution, as
expected. In addition, we have some comparisons between the rate of X ∼ TDΦ(θ) with the rates of some
random variables with known distributions in the literature: Inverse-gamma, Log-normal, Generalized-
Pareto, BGumbel [26], BWeibull [9], BGamma [8], exponential and Normal;

τInvGamma(α,β) = τLogNorm(µ,σ2) = τGenPareto(α,β,ξ) = τBWeibull(α<1,β,δ) = 0

< τBGumbel(µ,β,δ) = τBWeibull(α=1,β,δ) = τBGamma(α,1/β,δ) = τexp(1/β) = 1/β
< τBWeibull(α>1,β,δ) = τTDΦ(θ) = τNormal(µ,σ2) = ∞.

In other words, the tail of the normal distribution, of X ∼ TDΦ(θ) and of BWeibull(α > 1, β, δ) are
lighter than the tail of the other distributions speciﬁed above.

14

5 The Model arising from a Generalized Mixture

The weighted distributions of random variable (W − µ)/σ, with weight function wk, have its PDF deﬁned
by

fk(x; µ, σ) =

1
R
D wk(y)g(y)dy

wk

(cid:18) x − µ
σ

(cid:19)

(cid:18) x − µ
σ

g

(cid:19)
,

x − µ
σ

∈ D, k = 0, 1, . . . .

By using the power series expansion of the incomplete gamma function in (C.4), note that the PDF
f (x; θ) in (2.1) interprets as an inﬁnite (generalized) mixture of weighted distributions of (W − µ)/σ with
the weight functions wk(y) = y2k, k = 0, 1, . . ., and with same parameter vector (µ, σ). That is,

f (x; θ) = c0f0(x; µ, σ) +

∞
X

k=p

ckfk(x; µ, σ),

(5.1)

where the constants c0, ck, with k = p, p + 1, . . ., that depends only on θ, respectively, are given by

c0 = ρσ
Zθ

;

ck = δσ

Γ(p)Zθ

(−1)k−pE(W 2k)
k(k − p)!α2k

,

k = p, p + 1, . . . .

It is straightforward to verify that c0 + P∞
values.

k=p ck = 1. Note that the weights ck can take on negative

If X ∼ TDΦ(θ) and L : R → R is a Borel-measurable function, then, by representation (5.1), the

expectation of truncated random variable 1{X(cid:54)b}L(X), with b ∈ R, is given by

E(cid:2)1{X(cid:54)b}L(X)(cid:3) = c0E0

(cid:2)1{X(cid:54)b}L(X)(cid:3) +

∞
X

k=p

ckEk

(cid:2)1{X(cid:54)b}L(X)(cid:3).

But this representation as inﬁnite sum is not as informative as that representation of Theorem 3.4. For
this reason in the paper, we had been concerned with providing mathematical representations that allow
us to ﬁnd the closed expressions for some characteristics of the distribution (such as normalizing factor,
CDF, moments and entropies) as a function of the known mathematical functions.

6

Inference

The maximum logq-likelihood estimation (MLqE) and Fisher information matrix for this estimation
method are introduced. MLqE drops to the maximum likelihood estimation (MLE) if q → 1 [19, 20, 21,
32].

6.1 The Logq-Likelihood Function
For a random sample X1, . . . , Xn from the random variable X ∼ TD(θ), with parameter vector θ =
(µ, σ, α, ρ, δ), let us suppose that x1, . . . , xn are the observed values of X1, . . . , Xn. The MLqE of θ is
deﬁned as

lq(θ; x) =

n
X

i=1

log

q f (xi; θ), x = (x1, . . . , xn) ∈ Rn,

(6.1)

15

where log

(x) is the deformed logarithm deﬁned in (3.5) [20, 32].

q

By taking partial derivatives in (6.1), with respect to θ ∈ {µ, σ, α, ρ, δ}, we have

∂lq(θ; x)
∂θ

=

n
X

i=1

f 1−q(xi; θ) ∂ log f (xi; θ)

∂θ

,

with

log f (x; θ) = − log(Zθ) + log

(cid:20)
ρ + δT

(cid:18) x − µ
σ

(cid:19)(cid:21)

; α, p

+ log g

(cid:19)

(cid:18) x − µ
σ

(6.2)

and Zθ, T are as in (2.3) and (2.2), respectively. In general, the estimating equations for the parameters
have the form

n
X

i=1

f 1−q(xi; θ) ∂ log f (xi; θ)

∂θ

= 0,

θ ∈ {µ, σ, α, ρ, δ},

(6.3)

where the ﬁrst-order partial derivatives of the function log f (x; θ) are given in Section D of the Appendix.
A solution of the system of equations in (6.3) is called MLqE estimator.
It is not possible to derive
analytical solution for the MLqE bθ.

6.2 Fisher Information based on logq
The deﬁnition of Fisher information (FI) based on log
are deﬁned by

q

is given in [19, 28]. The elements of FI matrix

[I(θ)]j,k =

n
X

(cid:20)
E

i=1

f 1−q(Xi; θ) ∂ log f (Xi; θ)

∂θj

∂ log f (Xi; θ)
∂θk

(cid:21)
,

θj, θk ∈ {µ, σ, α, ρ, δ},

where log f (x; θ) is given in (6.2) and f (x; θ) is the parametric model in (2.1). When the inverse of
FI matrix exists, it is well-known that the diagonal elements of inverse of FI give Var( bθ) based on logq
[10, 19]. In general, there is no closed form expression for the FI matrix (see Appendices D and F.6).
It is clear that when q = 1 in the above identity, under standard regularity conditions, we obtain the
classical FI matrix [21, 24].

7 Application on real data sets

We present applications to illustrate the performance of the trimodal normal model compared with
smooth kernel distribution as semiparametric distribution (SmoothKernelDistribution) and estimation
of distribution which is performed by using a function named as FindDistribution embedded into
Mathematica 12.0 software to ﬁnd an appropriate distribution for the data set. Since Mathematica
software are capable for performing the optimization, bootstrap and also includes the numerical evaluation
of hypergeometric function (2F1) while conducting the modelling the data sets, practitioners can use
these codes for their aims in researches. The supplementary metarials provide the building codes for
practitioners [37, 38, 39].

The Examples 1 and 2 represent the real data called as heterodatatrain$V5 and heterodata$V4, re-
spectively in the "Rmixmod" package at R software with version 4.1.3. The numbers of sample size n
are 300 and 200 for Examples 1 and 2, respectively. The package "multilevel" includes data called as

16

PM

qTDΦ
SK

EstD

TDΦ

PM

qTDΦ
SK

EstD

TDΦ

PM

qTDΦ
SK

EstD

TDΦ

PM

qTDΦ
SK

"bh1996". The columns 11 and 13 of bh1996 data represent the modality. The Examples 3 and 4 are for
data sets with sample sizes n = 7382. The q values for Examples 1-4 are chosen as 0.98, 0.95, 0.98, 0.99,
respectively, according to the GOFTs.

Table 2: The models, the estimates of parameters, statistics and information criteria for assesment of
models

Estimates

Goodness of ﬁt tests (GOFTs)

log(L) & Information criteria

Example 1

ˆµ
-2.25117

-2.24486

-2.22717

ˆσ
1.32990

1.62363

1.61703

KS

CVM

AD

log(L)

AIC

BIC

0.0778466

0.551431

620.933

-570.632

1155.25

1172.67

-2.25803

1.38998

0.0764954

0.559258

615.978

0.0680419 0.402510 593.949
597.038
0.0706750

0.410858

-563.837

-563.777
-570.669

1130.25 1137.49
1141.45
1134.51

1147.46

1166.25

ˆµ
-1.94546

-1.98323

-1.96097

ˆσ
1.29762

1.63448

1.65964

Example 2

KS

CVM

AD

log(L)

AIC

BIC

0.120948

0.988454

471.754

-391.988

794.657

814.271

0.0862193 0.420766 400.882 -365.779 740.391 744.768
752.477
0.0873551

-370.828

401.620

0.43558

743.939

-1.91852

1.47189

0.120597

0.927997

430.148

-377.753

772.283

787.288

Example 3

ˆµ
2.77324

2.78055

2.78047

2.77505

ˆσ
0.856429

KS

CVM

AD

log(L)

AIC

BIC

0.0350506

1.66183

14510.4

-9762.83

19535.7

19570.2

0.915343 0.0150676 0.293907
1.31802
0.0326443
0.909687

14661.0

14397.0

-9659.05 19322.1 19335.9
19546.8
19532.9
-9764.47

0.882003

0.0333040

1.52912

14414.5

-9761.59

19533.2

19567.7

ˆµ
-0.001341870

ˆσ
0.865658

-0.000056456

0.889187

EstD 0.000581656

0.884030

Example 4

KS

CVM

AD

log(L)

AIC

BIC

0.989796

43.8267

677.270

-9550.79

19111.6

19146.1

0.981952
0.988856

43.0978
43.7301

600.388 -9472.98 18950.0 18963.8
19124.0
663.308

-9553.08

19110.2

TDΦ

-0.002632980

0.869382

0.989468

43.7947

672.560

-9550.61

19111.2

19145.8

PM: (Semi)-parametric models
q TDΦ: Objective function logq from trimodal normal distribution.
SK: The smooth kernel distribution used Gaussian (normal) distribution (semiparametric model).
EstD: The automatically chosen function by "FindDistribution" in Mathematica software.
TDΦ: Objective function log from trimodal normal distribution.
Italic represents closeness to the values produced by SK and EstD or almost best ones

The location (µ) and scale (σ) are important parameters to summarize the data set. The eﬃcient
estimations of these parameters depend on the chosen function used for modelling. Table 2 provides

17

them and other statistics for testing the modelling competence of the used functions. When all of PM
(qTDΦ, SK, EstD and TDΦ ) in Table 2 are compared, SK and EstD are rival ones among models. On
the other side, the statistics and information criteria of qTDΦ and TDΦ can be near to SK and EstD as
alternative models (see also discussion in Appendix F.3).

Table 3: Standard errors of estimators and estimates of shape and bimodality parameters in qTDΦ and
TDΦ

Example 1

pVar(ˆµ)
0.152203

0.167649

pVar(ˆσ)
0.124745

ˆα (pVar(ˆα))
1.22607(0.253034)

ˆρ (pVar(ˆρ))
1.06657(7.41619)

ˆδ (

q

Var(ˆδ))

0.233824(1.63256)

0.143923

1.21113(0.257971)

1.08073(7.41113)

0.230591(1.58755)

Example 2

pVar(ˆµ)
0.071482

pVar(ˆσ)
0.103308

ˆα (pVar(ˆα))
37.1884(205.348)

ˆρ (pVar(ˆρ))
5.53004(3048.8)

ˆδ (

q

Var(ˆδ))

50.6147(27963.2)

0.0985512

0.140166

31.9374(217.858)

11.6716(5270.27)

48.0121(21714.5)

Example 3

pVar(ˆµ)
0.000889860

pVar(ˆσ)
0.000992885

ˆα (pVar(ˆα))
1.161980(0.00549161)

ˆρ (pVar(ˆρ))
1.39357(0.0514929)

ˆδ (

q

Var(ˆδ))

0.400877(0.0145018)

0.000932255

0.000732616

0.888664(0.00562133)

1.17315(0.0706782)

0.182165(0.0109832)

Example 4

pVar(ˆµ)
0.0002215930

pVar(ˆσ)
0.000690205

ˆα (pVar(ˆα))
0.499788(0.00379787)

ˆρ (pVar(ˆρ))
1.405910(0.00379787)

ˆδ (

q

Var(ˆδ))

0.12482400(0.00240774)

0.0000789721

0.000723072

0.509288(0.00275330)

0.346905(0.00799665)

0.0312284(0.000700832)

PM

qTDΦ
TDΦ

PM

qTDΦ
TDΦ

PM

qTDΦ
TDΦ

PM

qTDΦ
TDΦ

Table 3 shows the square root of variance of estimators and also the estimates of ˆα, ˆρ and ˆδ for the

examples analyzed.

Table 4: The estimates for ˆµ and ˆσ from normal distribution and robust form

Example 1 Example 2 Example 3

Example 4

-2.24483

-1.96589

2.77989

1.24627 × 10−12

1.58461

1.58535

0.908344

0.882701

-2.34344

-2.23693

2.83333

0.0309632

ˆµ
ˆσ
Median

MAD

1.08095

0.981103

0.611111

0.607600

N

R

N: Normal distribution
R: Robust statistics
M: Median, MAD: Median absolute deviation (Median(|x-Median(x)|))

Table 4 introduces the basic statistics to see the role of distribution with one mode property and

trimodality.

18

The estimates of ˆµ and ˆσ from diﬀerent PM show that we can have a clue to imply that the existence of
modality can be observed, because the estimates of ˆµ and ˆσ from SK as a smooth kernel technique based
on working on the data-adaptive approach (which is capable to ﬁt the modality whether or not it exists
in the reality–see also discussions on Appendix E.2) instead of parametric approach for modelling can be
close to the estimates of ˆµ and ˆσ from qTDΦ and TDΦ. The kernel estimation method as a smoothing
tehnique is the best one generally. Even if 1000 replication for the diﬀerent design of samplings constructed
by use of bootstrap technique is applied, the numerical error(s) in computation for optimization can be
tricker to consider and make an accurate judgement among the modelling performance of the used four
models. For example, CDF and PDF of TDΦ depend on the Hypergeometric2F1 in Mathematica. Soft
forms of PDF of TDΦ in Figure 2b and smooth kernel technique in Ref. [40] can be alternative to each
other when the estimates of ˆµ and ˆσ, the statistics from GOFTs, the values of log(L) and IC are taken
into account. On the other side, it is very diﬃcult to know which function will be the best one for
modelling when the data sets in the ﬁnite sample size are tried to be ﬁtted by the functions. Even if
the sample sizes of Examples 3 and 4 are 7382, eventually we have ﬁnite sample size whatever it is. The
population in reality will not known exactly. Consequently, an alternative function can be necessary for
driving modality via parameters ρ and δ. This is the reason why we make a comparison between the
SK and other parametric models to observe what and how the estimates of ˆµ and ˆσ will be changed if
PDF are changed [33, 2]. Note that the modelling and numerical error(s) are topics which can aﬀect each
other.

8 Conclusions

Since recent times show that an increasing popularity has been observed in the modelling for data sets
having modality, producing the trimodal form of any PDF has been proposed. The trimodal form is
constructed by using the technique which includes the cumulative function of Maxwell distribution, the
existing unimodal distribution and the corresponding normalizing constant of the proposed distribution.
The properties of trimodality have been examined. The application of producing the trimodality has been
conducted for the normal distribution which is symmetric and unimodal form with two parameters which
are location and scale. The properties of trimodal normal distribution have been examined. Thus, the
applicability of this distribution have been tested. The trimodal normal distribution can have diﬀerent
forms such as strict and soft modalities to perform a precise ﬁtting when there exists three modes in the
empirical distribution of the data sets.

q

A comparison among trimodal normal, the kernel type estimation method, the probable parametric
distribution driven by Mathematica software has been performed in order to make applications for nu-
-likelihood estimation method and its special form with q → 1 have
merical evaluation of TDΦ. The log
been used to estimate of parameters of trimodal normal distribution. The proofs, properties of TDΦ
distribution and codes used for application have been given by appendices if the researchers perform to
model the data sets by use of TDΦ distribution.

The future will be an application on the diﬀerent areas of statistics such as regression modelling, the
tools in the multivariate statistics and other tools based on the distribution theory. The order statistic
form of TDΦ in the least informative distribution will be studied for the trimodal forms of the existing
distributions in the applied ﬁeld of science. Additionally, the precise modelling for inliers into data sets
can also be performed by use of trimodality, the generalized logarithms, entropy functions, order statistic
and diﬀerent estimation methods all together [35, 36]. A package in R software will be prepared for
practitioners after the special function in R software are improved.

19

Acknowledgements We acknowledge the anonymous referees for their helpful comments, suggestions
and references provided in their reports. R. V. thanks A. V. Medino, J. Roldan and E. M. M. Ortega for
partial discussions of Theorem 3.4 and for general paper questions.

Disclosure statement There are no conﬂicts of interest to disclose.

Funding This study was ﬁnanced in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível
Superior - Brasil (CAPES) (Finance Code 001).

ORCID Roberto Vila https://orcid.org/0000-0003-1073-0114
Victor Serra https://orcid.org/0000-0003-3061-2094
Mehmet N. Çankaya https://orcid.org/0000-0002-2933-857X
Felipe Quintino https://orcid.org/0000-0003-0286-0541

References

[1] M. N. Çankaya, Y. M. Bulut, F. Z. Doğru, O. Arslan, A bimodal extension of the generalized gamma

distribution, Revista Colombiana de Estadística (2015), 38:371–384.

[2] Çankaya, M. N., On the estimating equations and objective functions for parameters of exponential

power distribution: Application for disorder, arXiv e-prints, (2021), arXiv-2102.

[3] F. Z. Doğru, Y. M. Bulut, O. Arslan, Doubly reweighted estimators for the parameters of the multi-
variate t-distribution, Communications in Statistics-Theory and Methods, 47(19), (2018), pp. 4751-
4771.

[4] B. Everitt, (2013). Finite mixture distributions. Springer Science & Business Media.

[5] E. Gómez-Déniz, J. M. Sarabia, E. Calderín-Ojeda, Bimodal normal distribution: Extensions and

applications, Journal of Computational and Applied Mathematics, 388, (2021), 113292.

[6] R. Vila, H. Saulo, J. Roldan, On some properties of the bimodal normal distribution and its bivariate

version, Chilean Journal of Statistics, 12(2), (2021), pp. 125-144.

[7] C. Lee, F. Famoye, A. Y. Alzaatreh Methods for generating families of univariate continuous dis-
tributions in the recent decades. Wiley Interdisciplinary Reviews: Computational Statistics, 5(3),
(2013), pp. 219-238.

[8] R. Vila, L. Ferreira, H. Saulo, F. Prataviera, E. M. M. Ortega, A bimodal gamma distribution:

Properties, regression model and applications, Statistics, 54 (2020), pp. 469-493.

[9] R. Vila, M. N. Çankaya, A Bimodal Weibull Distribution: Properties and Inference, Journal of

Applied Statistics, (2021), pp. 1-19.

[10] J. F. Bercher, Some properties of generalized Fisher information in the context of nonextensive
thermostatistics, Physica A: Statistical Mechanics and its Applications, 392(15), (2013), pp. 3140-
3154.

20

[11] J. F. Bercher, A simple probabilistic construction yielding generalized entropies and divergences,
escort distributions and q-Gaussians, Physica A: Statistical Mechanics and its Applications, 391(19),
(2012), pp. 4460-4469.

[12] F. Domma, B.V. Popović, and S. Nadarajah, An extension of Azzalini’s method, J. Comput. Appl.

Math. 278 (2015), pp. 37-47.

[13] M.N. Çankaya, Asymmetric bimodal exponential power distribution on the real line, Entropy, 20(1),

(2018), 23.

[14] F. Domma, F. Condino, and B.V. Popović, A new generalized weighted Weibull distribution with
decreasing, increasing, upside-down bathtub, N-shape and M-shape hazard rate, J. Appl. Stat. 44
(2017), pp. 2978-2993.

[15] M. Rahman, B. Al-Zahrani, M.Q. Shahbaz, Cubic transmuted Pareto distribution. Annals of Data

Science, 7(1), (2020), pp. 91-108.

[16] D. Elal-Olivero, Alpha-skew-normal distribution, Proyecciones (Antofagasta) 29 (2010), pp. 224-240.

[17] R. Vila, L. Alfaia, A. F. Menezes, M.N. Çankaya, M. Bourguignon, A Model for Bimodal Rates and

Proportions, (2021), arXiv preprint arXiv:2108.07934.

[18] R.C. Dunbar, Deriving the Maxwell distribution, Journal of Chemical Education, 59(1), (1982), 22.

[19] M.N. Çankaya, J. Korbel, Least informative distributions in maximum q-log-likelihood estimation,

Physica A 509 (2018), pp. 140-150.

[20] D. Ferrari, Y. Yang, Maximum Lq-likelihood estimation, Ann. Stat. 38 (2010), pp. 753-783.

[21] R.A. Fisher, Theory of Statistical Estimation. In Mathematical Proceedings of the Cambridge Philo-

sophical Society, (Vol. 22, No. 5, pp. 700-725). Cambridge University Press, 1925.

[22] I.S. Gradshteyn, I.M. Ryzhik, Table of Integrals, Series and Products, Academic Press, San Diego,

2000.

[23] S. Klugman, H. Panjer, G. Willmot, Loss models: From data to decisions, Wiley, New York, 1998.

[24] E. L. Lehmann, G. Casella, Theory of Point Estimation, Wadsworth & Brooks/Cole, Paciﬁc Grove,

CA, 589, USA, 1998.

[25] E. W. Ng, M. Geller, A table of integrals of the Error functions, Journal of Research of the National

Bureau of Standards Section B Mathematical Sciences, 73B(1), 1 - January 1969.

[26] C. E. G. Otiniano, R. Vila, P. C. Brom, M. Bourguignon, On the bimodal Gumbel model with

application to environmental data, To appear in Austrian Journal of Statistics, (2021).

[27] J. Pender, The truncated normal distribution: Applications to queues with impatient customers,

Operations Research Letters, 43 (2015), pp. 40-45.

[28] A. Plastino, A. R. Plastino, H.G. Miller, Tsallis nonextensive thermostatistics and Fisher’s infor-

mation measure, Physica A 235 (1997), pp. 577-588.

21

[29] A. P. Prudnikov, IU. A. Brychkov, O. I. Marichev, Integrals and series. Vol 2, Special functions,

London: Taylor & Francis, 2002.

[30] C. E. Shannon, A mathematical theory of communication, Bell Labs. Tech. J. 27 (1948), pp. 623-656.

[31] C. Tsallis, Possible generalization of Boltzmann-Gibbs statistics, J. Stat. Phys. 52 (1988), pp. 479-

487.

[32] C. Tsallis, Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World,

Springer, New York, 2009.

[33] P.J. Huber, Robust estimation of a location parameter, Ann. Math. Stat. (1964), 35.

[34] Çankaya, M.N., Arslan, O., On the robustness properties for maximum likelihood estimators of pa-
rameters in exponential power and generalized T distributions, Communications in Statistics-Theory
and Methods, 49(3), (2020), pp. 607-630.

[35] M.N. Çankaya, M-estimations of shape and scale parameters by order statistics in least informative
distributions on q-deformed logarithm, Journal of the Institute of Science and Technology, (2020),
10(3), 1984-1996.

[36] M.N. Çankaya, Derivatives by ratio principle for q-sets on the time scale calculus , Fractals, 29(8)

(2021), 2140040, DOI: 10.1142/S0218348X21400405.

[37] León, J. G. S. (2017). Mathematica(cid:114) Beyond Mathematics: The Wolfram Language in the Real

World. Chapman and Hall/CRC.

[38] Baglivo, J. A. (2005). Mathematica laboratories for mathematical statistics: Emphasizing simulation

and computer intensive methods. Society for industrial and applied mathematics.

[39] Feagin, J. F., Quantum methods with Mathematica(cid:114), (2002), Springer Science & Business Media.

[40] Härdle, W., Müller, M., Sperlich, S., Werwatz, A. (2004). Nonparametric and semiparametric models

(Vol. 1). Berlin: Springer.

[41] Härdle, W. K. (1991). Smoothing techniques: with implementation in S. Springer Science & Business

Media.

[42] Somani, A. K., Deka, G. C. (Eds.). Big data analytics: Tools and technology for eﬀective planning.

(2017), CRC Press.

22

Appendices

A Proof of some results of Sections 3 and 4

Proof of Lemma 3.1. Let Y be a random variable with Gamma(p, 1/α2) distribution. The corresponding
PDF and CDF of Y , respectively, are given by f (y; α, p) = (1/α2)p yp−1e−y/α2/Γ(p) and F (y; α, p) =
γ(p, y/α2)/Γ(p). Therefore,

R(y) = 2δf (y; α, p) − [ρ + δF (y; α, p)]h(y),

y > 0.

(A.1)

Suppose y (cid:62) α2(1 − A). Then

e(α2A+y)/α2 =

∞
X

k=0

(α2A + y)k/α2k
k!

(cid:62)

(α2A + y)n+1/α2(n+1)
(n + 1)!

,

for each n ∈ N, such that

Hence,

e−y/α2 (cid:54)

(n + 1)!α2(n+1)eA
(α2A + y)n+1

.

2δf (y; α, p) (cid:54) 2δ

(cid:20) (1/α2)p

Γ(p) yp−1

(n + 1)!α2(n+1)eA
(α2A + y)n+1

(cid:21)

(cid:54) 2δ

(cid:20) (1/α2)p
Γ(p)

(n + 1)!α2(n+1)eA
(α2A + y)n+1−p

(cid:21)

(cid:54)

ρC
(α2A + y)β−p

= ρh(y),

for β < n + 1, C (cid:62) 1 and some ρ > 0, so that

ρ (cid:62) 2δ

(cid:20) (1/α2)p
Γ(p)

(n + 1)!α2(n+1)eA

(cid:21)

.

(A.2)

(A.3)

Since F (y; α, p) is a CDF, from (A.2) it follows that

2δf (y; α, p) (cid:54) [ρ + δF (y; α, p)]h(y) (cid:54) (ρ + δ)h(y),

∀y (cid:62) α2(1 − A).

(A.4)

In other words, for every y (cid:62) α2(1 − A) and ρ large enough, the tail of Gamma distribution 2δf (y; α, p)
is lighter than the tail of [ρ + δF (y; α, p)]h(y) which decays polynomially.

On the other hand, it is well-known that: when p < 1, the Gamma distribution is exponentially shaped
and asymptotic to both the vertical and horizontal axes; the Gamma distribution with shape parameter
p = 1 and scale parameter 1/α2 is the same as an exponential distribution of scale parameter (or mean) 1;
when p is greater than one, the Gamma distribution assumes a maximum value (unimodal), but skewed
shape. The skewness reduces as the value of p increases.

Based on the shapes of the Gamma distribution and on the inequality (A.4), in Figure 3, we graphically
sketch the functions 2δf (y; α, p) and [ρ + δF (y; α, p)]h(y), and consider all possible cases, by varying the
parameters α, ρ as in (A.3), δ and p (known), in which the graphs of 2δf (y; α, p) and [ρ + δF (y; α, p)]h(y)
intersect (or not).

23

Figure 3: The graphs of 2δf (y; α, p) and [ρ + δF (y; α, p)]h(y) have at most two common points.

Then, in both cases p (cid:54) 1 or p > 1, the function R in (A.1) has at most two real zeros. This completes
(cid:4)
the proof.

Proof of Proposition 3.9. A simple algebric manipulation shows that

Sq(X) =

1
q − 1

Z

(cid:20)
1 −

f q(x; θ) dx

(cid:21)
,

q 6= 1.

σD+µ

Since 0 < T (x; α, p) < 1 for almost all x ∈ D, we have
(ρ + δ)
Zθ

0 (cid:54) f (x; θ) (cid:54)

(cid:18) x − µ
σ

g

(cid:19)
.

Then, by using that the function x 7→ xq is increasing, for x > 0 and q > 0, we have

for all x ∈ σD + µ. Consequently,

0 (cid:54) f q(x; θ) (cid:54)

(ρ + δ)q
Zq
θ

gq

(cid:18) x − µ
σ

(cid:19)
,

Z

(cid:18) x − µ
σ
Hence, if Sq(W ) exists then of Sq(X) also exists.

(ρ + δ)q
Zq
θ

f q(x; θ) dx (cid:54)

σD+µ

gq

D

Z

(cid:19)

dx =

(ρ + δ)qσ
Zq
θ

(cid:2)1 − (q − 1)Sq(W )(cid:3),

q 6= 1.

(cid:4)

Proof of Proposition 3.11. By taking the logarithm of each side of (2.1), by deﬁnition of Shannon entropy,
we have

(cid:20)
− log
S1(X) = log(Zθ) + E

(cid:18) X − µ
σ
By taking L(x) = log(ρ+δT (x; α, p)) and L(x) = − log g(x), ∀x ∈ σD+µ, in Corollary 3.5, respectively,

(cid:18) X − µ
σ

(cid:20)
+ E

− log g

ρ + δT

; α, p

(A.5)

(cid:19)(cid:19)(cid:21)

(cid:19)(cid:21)

(cid:18)

.

we have

(cid:20)
− log
E

(cid:18)

ρ + δT

(cid:18) X − µ
σ

; α, p

(cid:19)(cid:19)(cid:21)

= −

(ρ + δ)σ
Zθ

E(cid:2) log(ρ + δT (W ; α, p))(cid:3)

−

δσ
Zθ

nE h1

{W (cid:54)−α

√

Y }

log(ρ + δT (W ; α, p))i

− E h1

{W (cid:54)α

√

Y }

log(ρ + δT (W ; α, p))io

(A.6)

24

and

(cid:20)
− log g

E

(cid:18) X − µ
σ

(cid:19)(cid:21)

=

(ρ + δ)σ
Zθ

S1(W ) + δσ
Zθ

nE h

(cid:0)1

S1

√

Y }W (cid:1)i

− E h

(cid:0)1

S1

√

Y }W (cid:1)io
.

{W (cid:54)α

{W (cid:54)−α

(A.7)

By replacing the identities (A.6) and (A.7) in (A.5), the proof follows.

(cid:4)

Proof of Lemma 4.1. The PDF and CDF of Y ∼ Gamma(p, 1/α2), respectively, are given by f (y; α, p) =
(1/α2)p−1e−y/α2/Γ(p) and F (y; α, p) = γ(p, y/α2)/Γ(p). Then, R takes on the following form

R(y) = 2δf (y; α, p) − [ρ + δF (y; α, p)].

(A.8)

Figure 4: The graphs of 2δf (y; α, p) and ρ + δF (y; α, p) have at most two points of intersection.

Based on the shapes of the Gamma law, in Figure 4, we graphically sketch the functions 2δf (y; α, p)
and ρ + δF (y; α, p), and consider all possible cases, by varying the parameters α, ρ, δ and p (known), in
which the graphs of 2δf (y; α, p) and ρ + δF (y; α, p) intersect (or not).

So, for the cases when p (cid:54) 1 or p > 1, the function R in (A.8) has at most two real zeros. This
(cid:4)

completes the proof.

B Moments in the Gaussian case

In this section, for simplicity, we consider p (cid:62) 1 an integer. To get the moments of X ∼ TDΦ(θ),
by Corollary 3.7, it is necessary to determine E(1
Y }Zn), n (cid:62) 1, where Z ∼ N (0, 1) and Y ∼
Gamma(p, 1) are independent. Indeed, since Z and Y are independent, by deﬁnition of expectation,

{Z(cid:54)±α

√

E(cid:0)1

{Z(cid:54)±α

√

Y }Zn(cid:1) =

Z ∞

0

E(cid:0)1{Z(cid:54)±α

√

y}Zn(cid:1) yp−1e−y
Γ(p)

dy.

Let us deﬁne nm = n − 2m − 1,

cn,m = n!2−mnm!
m!(nm + 1)!

and dnm,j =

(−1)j
j!(nm − 2j)!2j .

25

(B.1)

(B.2)

By using the following known formula (see Theorem 2.3. of [27]): for each real numbers a, b such that
a < b,

E(cid:0)1{a(cid:54)Z(cid:54)b}Zn(cid:1) =

bn/2c
X

bnm/2c
X

m=0

j=0

cn,mdnm,j

(cid:2)anm−2jφ(a) − bnm−2jφ(b)(cid:3),

where bxc is the greatest integer less than or equal to x, we have

E(cid:0)1{Z(cid:54)±α

√

y}Zn(cid:1) = −

bn/2c
X

bnm/2c
X

m=0

j=0

cn,mdnm,j (±α

√

y)nm−2jφ(±α

√

y).

Replacing the above identity in (B.1),

E(cid:0)1

{Z(cid:54)±α

√

Y }Zn(cid:1) = −

bn/2c
X

bnm/2c
X

m=0

j=0

cn,mdnm,j

Z ∞

√

(±α

0

y)nm−2jφ(±α

y) yp−1e−y
√
Γ(p)

dy.

But, by deﬁnition of gamma distribution,

Z ∞

0

(±α

√

y)nm−2jφ(±α

√

y) yp−1e−y
Γ(p)

dy =

√

(±α)nm−2j

2πΓ(p)(1 + α2
2

) nm−2j

2 +p

(cid:18) nm − 2j
Γ
2

(cid:19)

.

+ p

Hence

E(cid:0)1

{Z(cid:54)±α

√

Y }Zn(cid:1) = −

√

1
2πΓ(p)

bn/2c
X

bnm/2c
X

cn,mdnm,j (±α)nm−2j

m=0

j=0

(1 + α2
2

) nm−2j

2 +p

(cid:18) nm − 2j
Γ
2

(cid:19)

+ p

.

(B.3)

By using (B.3) and the known formula

E(Zn) =

2−n/2n!
(n/2)!

1{n even},

in Corollary 3.7, we get the formula of moments for X ∼ TDΦ(θ) given in Subsection 4.4.

C Entropy in the Gaussian case

√

By Remark 3.10, the Shannon entropy S1(X) of X ∼ TDΦ(θ) exists whenever S1(Z) of Z ∼ N (0, 1) also
exists. Since S1(Z) = log(
2π) + 1/2, the existence of S1(X) is guaranteed. So it makes sense to ﬁnd
a closed expression for S1(X). To ﬁnd this expression, by Proposition 3.11, it is enough to determine
Y }Z)], where T is as in (2.2) and
the expectations: E[1
Z ∼ N (0, 1) and Y ∼ Gamma(p, 1) are independent. As in the previous subsection, for simplicity, in this
subsection we consider p (cid:62) 1 an integer.

log(ρ + δT (Z; α, p))] and E[S1(1

{Z(cid:54)±α

{Z(cid:54)±α

Y }

√

√

A simple calculation shows that, for each real numbers a, b such that a < b,

S1(1{a(cid:54)Z(cid:54)b}Z) = −

Z b

a

φ(z) log φ(z) dz

=

1
2

(cid:2)aφ(a) − bφ(b)(cid:3) +

(cid:20)
log(

√

2π) +

1
2

(cid:21) (cid:20)

erf

1
2

(cid:19)

(cid:18) b
√
2

− erf

(cid:18) a
√
2

(cid:19)(cid:21)

.

26

Then

But,

E(cid:2)S1(1

{Z(cid:54)±α

√

Y }Z)(cid:3) = ∓

1
2

Z ∞

0

√

α

yφ(±α

√

y) yp−1e−y
Γ(p)

dy

+

1
2

(cid:20)
log(

√

2π) +

1
2

(cid:21) Z ∞

(cid:20)
erf

0

(cid:18) ±α
√

√

2

(cid:19)

y

+ 1

(cid:21) yp−1e−y
Γ(p)

dy.

• by deﬁnition of Gamma distribution,

Z ∞

0

√

α

yφ(±α

√

y) yp−1e−y
Γ(p)

dy =

αΓ(p + 1
)
2
√
2πΓ(p)

1

(1 + α2
2

)p+ 1

2

,

• and by using (4.4),

Z ∞

(cid:20)

erf

0

Hence

(cid:18) ±α
√

√

2

(cid:19)

y

+ 1

(cid:21) yp−1e−y
Γ(p)

dy = 2E(cid:2)Φ(±α

√

Y )(cid:3).

E(cid:2)S1(1

{Z(cid:54)±α

√

Y }Z)(cid:3) = ∓

)
αΓ(p + 1
2
√
2πΓ(p)
2

1

(1 + α2
2

)p+ 1

2

(cid:20)
log(

√

+

2π) +

(cid:21)
E(cid:2)Φ(±α

√

Y )(cid:3).

1
2

(C.1)

The proof of the following result is technical.

Proposition C.1. If Z ∼ N (0, 1) the following hold

E(cid:2) log(ρ + δT (Z; α, p))(cid:3) = 2

∞
X

k=0

1
(2k + 1)

(cid:20)
1 +

1
(ρ + 1)2k+1

∞
X

i=p

eci,k

2−i(2i)!
i!

(cid:21)
;

(C.2)

E(cid:2)1

{Z(cid:54)±α

√

Y }

log(ρ + δT (Z; α, p))(cid:3)

= 2

∞
X

k=0

(cid:26)

1
(2k + 1)

E(cid:2)Φ(±α

√

Y )(cid:3) +

1
(ρ + 1)2k+1

∞
X

i=p

eci,kE(cid:0)1

{Z(cid:54)±α

√

Y }Z2i(cid:1)

(cid:27)

.

(C.3)

Proof. We consider the power series expansions of log(z) and of the incomplete gamma function:

log(z) = 2

∞
X

k=0

1
2k + 1

(cid:18) z − 1
z + 1

(cid:19)2k+1

,

γ(p, z) =

∞
X

k=0

(−1)k
k!

zp+k
p + k

=

∞
X

k=p

(−1)k−p
(k − p)!

zk
k

,

(C.4)

respectively. Using these expansions, we have

log(ρ + δT (z; α, p)) = 2

= 2

∞
X

k=0

∞
X

k=0

1
2k + 1

(cid:20) ρ − 1 + δT (z; α, p)
ρ + 1 + δT (z; α, p)

(cid:21)2k+1

1
2k + 1

" ρ − 1 + δ
Γ(p)

P∞

i=p

ρ + 1 + δ
Γ(p)

P∞

j=p

(−1)i−p
2i(i−p)!α2i z2i
(−1)j−p
2j(j−p)!α2j z2j

#2k+1

.

27

Letting b0 = ρ − 1 and bi = [δ(−1)i−p/2i(i − p)!α2iΓ(p)]1{i(cid:62)p}; and similarly, deﬁning a0 = ρ + 1 and
aj = [δ(−1)j−p/2j(j − p)!α2jΓ(p)]1{j(cid:62)p}; the last series can be expressed as

= 2

∞
X

k=0

1
2k + 1

  P∞
P∞

i=0 biz2i
j=0 ajz2j

!2k+1

.

By application of the equation in Section 0.313 of [22] for division of power series, the above series is
written as

= 2

∞
X

k=0

1
2k + 1

  1

ρ + 1

∞
X

i=0

!2k+1

ciz2i

,

(C.5)

where the coeﬃcients ci’s are determined from the recurrence equation

cn =

(cid:18)

bn −

1
ρ + 1

n
X

i=p

cn−iai

(cid:19)

1{n(cid:62)p}.

By application of the equation in Section 0.314 of [22] for power series raised to powers, the series (C.5)
is equal to

∞
X

2

k=0

1
(2k + 1)(ρ + 1)2k+1

∞
X

i=0

eci,kz2i = 2

∞
X

k=0

1
(2k + 1)

(cid:20)
1 +

1
(ρ + 1)2k+1

(cid:21)
,

eci,kz2i

∞
X

i=p

where the coeﬃcients

eci,k’s are determined by

ec0,k = (ρ + 1)2k+1 and from the recurrence relation

ecm,k =

1
m(ρ + 1)

m
X

i=p

(cid:2)2i(k + 1) − m(cid:3)aiecm−i,k1{m(cid:62)p}.

(C.6)

In short, we have

log(ρ + δT (z; α, p)) = 2

∞
X

k=0

1
(2k + 1)

(cid:20)
1 +

1
(ρ + 1)2k+1

(cid:21)
.

eci,kz2i

∞
X

i=p

By using the above expansion and assuming we can change the series with the integral sign, we have

E(cid:2)1{a(cid:54)Z(cid:54)b} log(ρ + δT (Z; α, p))(cid:3) =

Z b

a

log(ρ + δT (z; α, p))φ(z) dz

= 2

∞
X

k=0

1
(2k + 1)

(cid:20)
Φ(b) − Φ(a) +

1
(ρ + 1)2k+1

∞
X

i=p

eci,kE(cid:0)1{a(cid:54)Z(cid:54)b}Z2i(cid:1)

From the above identity the identities in (C.2) and (C.3) follow.

(cid:21)

.

(cid:4)

Since S1(Z) = log(

2π) + 1/2, by substituting identities (C.2), (C.3) and (C.1) in Proposition 3.11,

we get the formula of the Shannon entropy for X ∼ TDΦ(θ) given in Subsection 4.4.

√

28

D Derivatives of the function log f (x; θ)

Let Y ∼ Gamma(p, 1) and let T be as in (2.2). Then

∂ log f (x; θ)
∂µ

2( x−µ
σα

= −

)2(p−1) e−( x−µ
σ2α2Γ(p)

ασ )2

log

(cid:20)
ρ + δT

(cid:18) x − µ
σ

; α, p

(cid:19)(cid:21)

−

1

σ

∂ log f (x; θ)
∂σ

= −

ρ + δ
Zθ

−

δ
Zθ

√

(cid:8)E[G(−α

Y )] − E[G(α

√

Y )](cid:9)

(cid:17)

g0 (cid:16) x−µ
σ
(cid:17)
(cid:16) x−µ
σ

g

;

−

2δ( x−µ
σα
ρ + δT ( x−µ
σ

)2p−1e−( x−µ

ασ )2
; α; p)i Γ(p)

σ3 h

−

(x − µ)
σ2

(cid:17)

g0 (cid:16) x−µ
σ
(cid:17)
(cid:16) x−µ
σ

g

;

∂ log f (x; θ)
∂α

= −

2σδΓ(p + 1/2)
√
2πZθΓ(p)

2F1

(cid:18) 1
2

; p +

(cid:19)

1
2

;

3
2

; −α2
2

−

2σΓ(p + 1/2)
√
2πZθΓ(p)

(cid:20)(cid:18) α2
2

(cid:19)−(p+ 1
2 )
− 2F1

+ 1

(cid:18) 1
2

; p +

1
2

;

3
2

; −α2
2

(cid:19)(cid:21)

−

)2p e−( x−µ

2δ( x−µ
σα
h
ρ + δT ( x−µ
σ

α

ασ )2
; α, p)i

;

∂ log f (x; θ)
∂ρ

= −

∂ log f (x; θ)
∂δ

= −

σ
Zθ

δ
Zθ

+

1

ρ + δT ( x−µ
σ

; α; p)

;

(cid:8)1 + E[G(−α

√

Y )] − E[G(α

√

Y )](cid:9) +

T

(cid:17)

; α, p

(cid:16) x−µ
σ
(cid:16) x−µ
σ

; α, p

(cid:17) .

ρ + δT

E Inference, Optimization, Bootstrap and Applications

E.1 Rao-Cramer lower bounds of estimators from MLqE and MLE
Since we provide the square roots of Rao-Cramer lower bounds of estimators from MLqE and MLE, the
conﬁdence interval of estimators can be constructed as the following form:

ˆθ ∓ zτ /2

q

Varq(ˆθ),

where ˆθ is chosen as (ˆµ, ˆσ, ˆα, ˆρ, ˆδ). zτ /2 is the critical value from the standard normal distribution when
the signiﬁcance level τ is chosen by researcher [33, 24, 20]. Varq(ˆθ) is the theoretical variance of estimator
evaluated numerically by the codes in Section F.6. Thus, one can observe the limit values of estimates of
parameters to generate the diﬀerent kind of artiﬁcial data sets if it is necessary to do so at an experiment.
The lower limit values of conﬁdence intervals for the estimates must be as follows: ˆσL > 0, ˆαL > 0,
ˆρL (cid:62) 0 and ˆδL (cid:62) 0. In other words, these values should be greater than zero due to the deﬁned values of
these parameters.

29

E.2 Beneﬁt of bootstrapped data when semiparametric and parametric models are

used for ﬁtting data sets

The general tendency for modelling is performed by the existing techniques which are the estimated
distribution including parametric models, kernel smoothing. As is expected, the kernel smoothing tech-
nique as a semiparametric approach for modelling can show better performance for ﬁnite sample size
when compared with parametric models [40]. However, we can need to assume that a data set is member
of a parametric model. In this case, if an artiﬁcial data set is necessary to model the probable results in
the future or if conducting an experiment is expensive for researchers, the necessity of parametric models
such as bimodal [5] and trimodal normal distribution can be inevitable; because, the smooth kernel dis-
tribution used for ﬁtting on the data set is data-adaptive or semiparametric approach [41]. In this case,
the parametric model puts a restriction in where we have situations which give the results mimicing the
parametric model perfectly because of the probabilities coming from PDF. If we have such results, then
we need to make a comparison between kernel smoothing and the trimodal forms of a function which can
be chosen as normal, Student t, etc. [38].

Since the real data set includes results taken at the moment or sometimes experiments have to be
conducted at only one time due to the cost of running of experiment, diﬀerent collections of the same
data should be generated by use of the bootstrap technique. Any mixed models as a hetero form can be
modelled by using TDΦ distribution if the trimodality exists. Since hetero-mixing forms a non-identically
distributed data set, trimodal distribution can be necessary to perform an eﬃcient ﬁtting on the data
sets observed from an experiment.

Since bootstrap technique performs a random choice from a data set of experiment, we need to make
a comparison among the semiparametric and parametric models whether or not which one will be per-
formable in ﬁtting on the data set [38, 41]. The soft and strict forms of trimodal normal distributions
are capable to perform an eﬃcient ﬁtting on data set and is an alternative approach when compared
with smoothing technique if trimodality exists. Because, it is concluded that a parametric model must
be necessary to generate the random numbers from corresponding probabilities of parametric model, i.e
PDF, for research instead of doing an another experiment again. If we generate artiﬁcial numbers by
use of smoothing technique, these numbers will be generated according to the used smoothing technique
which has its probabilities corresponding empirical probability function representing the probabilities
coming from each event in the ﬁnite sample size (see Subsection F.1) [41]. When we come across big data
analytics, the population starts to evolve and take what its real form is as. In this case, a parametric
model can be necessary [42].

E.3 Driving the bootstrap and the optimization of logq likelihood function for esti-

mations of parameters

GOFTs which perform a testing for assesment of the used (semi)-parametric model can show diﬀerent
performance for each model, which is why the diﬀerent GOFTs have been applied to test the ﬁtting
performance of models. The bootstrap technique in Mathematica is applied to the real data set (see
Appendix F.3). The replication number of bootstrap is 1000. Thus, the probable mistakings in conver-
gence of the optimization and the diﬀerent scenarios of real data sets are tried to be clariﬁed. It is also
noted that the data observed after the experiment is conducted can include the measurement error and
the measurements can depend on many factors which are known as the randomness, mistakes, unhidden
factors, etc. in the experiment [38, 39].

The bootstrapped form of the real data is not only beneﬁcial to jump on these kind of problems in
the process of measurement but also the replicated optimization should be useful for us to make the

30

convergence around global point (GP) or reach the real GP where the optimization can reach if the
replication is performed. Further, SK technique depends on the chosen function for kernel and it is
a semiparametric technique [40, 41]. As is shown by Table 2, SK has performance on the modelling.
However, the performance of trimodal normal distribution (TDΦ) can have superior performance when
the closeness to statistics generated by SK and the estimated distribution (EstD) which is parametric
method is taken into account. EstD integrated into Mathematica software tries to ﬁnd the best function
(mixed form of the parametric models or other parametric models) while conducting the ﬁtting on the
real data set. For example, a data set can be a mixing form of two or three normal distributions with the
corresponding parameters from populations 1 2 or 3, respectively. If we have a mixed form for the real
data set, it can also be member of trimodal distribution which is eventually not known by researchers,
which is an important gap in conducting a research. For this reason, TDΦ has been proposed and also
we make a comparison among them to observe the performance of parametric model (see also discussion
in introduction Section 1).

The initial values for starting the optimization performed by FindMaximum with constraints in the
parameters α, ρ and δ searched at the interval (0, 100] are generated by uniform distribution with [0, 1]
[38, 39].

F Mathematica 12.0 codes

F.1 Generating procedure for random numbers if SmoothKernelDistribution is used

The codes for generating random numbers from the smooth kernel distribution in Mathematica is given
by the following lines:

SK = SmoothKernelDistribution[x, {"Adaptive", Automatic, .1},PerformanceGoal
-> "Quality"];
F[m_, s_] := CDF[SK, x];
m := Moment[SK, 1]; s := Sqrt[Moment[SK, 2] - Moment[SK, 1]^2];
For[i = 0, i < n, i++,
z = Table[
x /. FindRoot[F[m, s] - RandomReal[], {x, Lower point, Upper point}], {i, n}]
]

One can get the random number from trimodal normal distribution by use of subsection 3.4 or CDF

of TDΦ in Subsection 4.3.

F.2 Optimization and statistics

iv := RandomReal[];
FM = FindMaximum[{Total[Log[f]],
a< \[Alpha] <= b && a <= \[Rho] <= b &&
a <= \[Delta] <= b}, {{\[Alpha], iv}, {\[Rho] ,
iv}, {\[Delta], iv}, {\[Mu], Median[x]}, {\[Sigma],
Median[Abs[x - Median[x]]]}}];
EstD = FindDistribution[x];
SK = SmoothKernelDistribution[x, {"Adaptive", Automatic, .1},PerformanceGoal

-> "Quality"];

31

statistics =
{

Moment[EstD, 1], Moment[SK, 1],
Sqrt[Moment[EstD, 2] - Moment[EstD, 1]^2],
Sqrt[Moment[SK, 2] - Moment[SK, 1]^2],
StandardDeviation[x], Median[Abs[x - Median[x]]]

Mean[x], Median[x],

};

F.3 The codes for bootstrap

A part for bootstrap is given by the following form [38]:

For[i = 1, i <= replication, i++,

x := Table[RandomChoice[data, Length[data]], {i,1,Length[data]}]

]

F.4 CDF of TNΦ

p := 3/2; ns := n;
x = Sort[x];
nsn = 0; nsp = 0;
For[i = 1, i <= ns, i++,
If[x[[i]] < \[Mu], nsn = nsn + 1]
]
For[i = 1, i <= nsn, i++,
If[x[[i]] < \[Mu], xn[i] := x[[i]]]
]
For[i = nsn + 1, i <= ns, i++,
If[x[[i]] >= \[Mu], xp[i] := x[[i]]]
]
For[i = 1, i <= nsn, i++,
FN[i] = (\[Sigma]/
Z)*(\[Rho] + \[Delta]*
T[xn[i]/\[Sigma] - \[Mu]/\[Sigma], \[Alpha], p])*
CDF[NormalDistribution[\[Mu], \[Sigma]],
xn[i]] + ((\[Delta]*\[Sigma])/
Z)*(1/2 - (\[Alpha]*Gamma[p + 1/2]*
Hypergeometric2F1[1/2, p + 1/2, 3/2, -\[Alpha]^2/2])/(Sqrt[
2*Pi]*Gamma[p])) - ((\[Delta]*\[Sigma])/
Z)*((1/2)*T[xn[i]/\[Sigma] - \[Mu]/\[Sigma], \[Alpha], p] -
NIntegrate[
Erf[(\[Alpha]/Sqrt[2])*z]*z^(2*p - 1)*Exp[-1^2*z^2], {z,
0, (xn[i] - \[Mu])/(\[Alpha]*\[Sigma])},
Method -> {"LobattoKronrodRule"}]/Gamma[p]);
]
For[i = nsn + 1, i <= ns, i++,

32

FP[i] = (\[Sigma]/
Z)*(\[Rho] + \[Delta]*
T[xp[i]/\[Sigma] - \[Mu]/\[Sigma], \[Alpha], p])*
CDF[NormalDistribution[\[Mu], \[Sigma]],
xp[i]] + ((\[Delta]*\[Sigma])/
Z)*(1/2 - (\[Alpha]*Gamma[p + 1/2]*
Hypergeometric2F1[1/2, p + 1/2, 3/2, -\[Alpha]^2/2])/(Sqrt[
2*Pi]*Gamma[p])) - ((\[Delta]*\[Sigma])/
Z)*((1/2)*T[xp[i]/\[Sigma] - \[Mu]/\[Sigma], \[Alpha], p] +
NIntegrate[
Erf[(\[Alpha]/Sqrt[2])*z]*z^(2*p - 1)*Exp[-1^2*z^2], {z,
0, (xp[i] - \[Mu])/(\[Alpha]*\[Sigma])},
Method -> {"LobattoKronrodRule"}]/Gamma[p]);
]
FNFP := Join[{Table[FN[j], {j, nsn}], Table[FP[j], {j, nsn + 1, ns}]}];
JoinFNP := Join[FNFP[[1]], FNFP[[2]]];

F.5 Goodness of ﬁt tests for the proposed and used distributions

The goodness of ﬁt tests such as Kolmogorov-Smirnov (KS), Cramér–von Mises and Anderson-Darling
are evaluated by using the the following codes for TNΦ, EstD and SK.

For[i = 1, i <= ns, i++,
CVM[i] = (JoinFNP[[i]] - (2*i - 1)/(2*ns))^2 + 1/(12*ns);
AD[i] = (-1/
ns)*((2*i - 1)*(Log[JoinFNP[[i]]] + Log[1 - JoinFNP[[i]]]));
DP[i] = i/ns - JoinFNP[[i]]; DN[i] = JoinFNP[[i]] - (i - 1)/ns;
CVMED[i] = (CDF[estimated\[ScriptCapitalD], x][[
i]] - (2*i - 1)/(2*ns))^2 + 1/(12*ns);
ADED[i] = (-1/
ns)*((2*i - 1)*(Log[CDF[EstD], x][[i]]] +
Log[1 - CDF[EstD], x][[i]]]));
DPED[i] = i/ns - CDF[EstD], x][[i]];
DNED[i] = CDF[estimated\[ScriptCapitalD], x][[i]] - (i - 1)/ns;
CVMSK[i] = (CDF[SK, x][[i]] - (2*i - 1)/(2*ns))^2 + 1/(12*ns);
ADSK[i] = (-1/ns)*((2*i - 1)*(Log[CDF[SK, x][[i]]] + Log[1 - CDF[SK, x][[i]]]));
DPSK[i] = i/ns - CDF[SK, x][[i]];
DNSK[i] = CDF[SK, x][[i]] - (i - 1)/ns;
]
DPmax = Max[Table[DP[j], {j, ns}]]; DNmax = Max[Table[DN[j], {j, ns}]];
KS = Max[DPmax, DNmax];StaCVM = Total[Table[CVM[j], {j, ns}]];
StaAD = Total[Table[AD[j], {j, ns}]];{KS, StaCVM, StaAD}

In order to get information criteria (IC) such as Akaike and Bayesian, CDF of Mathematica must be

replaced with its corresponding PDF and the formulae of IC are used. For example,

-2*Total[Log[PDF[SK, x]]] + 2*p

33

F.6 The codes for variances of estimators

The PDF of TDΦ is given by

f[\[Mu]_, \[Sigma]_, \[Alpha]_, \[Rho]_, \[Delta]_] := \
\[Sigma]*((1/((\[Rho] + \[Delta])*\[Sigma] - ((Sqrt[
2]*\[Delta]*\[Sigma]*\[Alpha]*
Gamma[p + 1/2])/(Sqrt[Pi]*Gamma[p]))*
Hypergeometric2F1[1/2, p + 1/2,
3/2, -\[Alpha]^2/2]))*(\[Rho] + \[Delta]*(1 -
Gamma[p, (x - \[Mu])^2/(\[Sigma]^2* \[Alpha]^2)]/
Gamma[p]))*(exp^(-((x - \[Mu])^2/(2 \[Sigma]^2)))/(
Sqrt[2 \[Pi]] \[Sigma])));
g[\[Mu]_, \[Sigma]_] := exp^(-((x - \[Mu])^2/(2 \[Sigma]^2)))/(
Sqrt[2 \[Pi]] \[Sigma]);
T[p_, \[Mu]_, \[Sigma]_, \[Alpha]_] :=
1 - Gamma[p, (x - \[Mu])^2/(\[Sigma]^2* \[Alpha]^2)]/Gamma[p];

Dgm2[\[Mu], \[Sigma]] := D[g[\[Mu], \[Sigma]], \[Mu]];

Z := (\[Rho] + \[Delta])*\[Sigma] - ((Sqrt[
2]*\[Delta]*\[Sigma]*\[Alpha]*Gamma[p + 1/2])/(Sqrt[Pi]*
Gamma[p]))*
Hypergeometric2F1[1/2, p + 1/2, 3/2, -\[Alpha]^2/2];

The score ∂ log f (x;θ)

∂µ

for µ is given by

em := ((-2*((x - \[Mu])/(\[Sigma]*\[Alpha]))^(2*p - 2)*
Exp[-((x - \[Mu])/(\[Sigma]*\[Alpha]))^2])/(\[Sigma]^2*\
\[Alpha]^2*Gamma[p]))*
Log[\[Rho] + \[Delta]*
T[p, \[Mu], \[Sigma], \[Alpha]]] - (1/\[Sigma])*
Dgm2[\[Mu], \[Sigma]];

The score ∂ log f (x;θ)

∂σ

for σ is given by

es := -\[Rho]/Z - \[Delta]/
Z - (\[Delta]/
Z)*(NIntegrate[
CDF[NormalDistribution[\[Mu], \[Sigma]], -\[Alpha]*y^0.5]*
PDF[GammaDistribution[p, 1], y], {y, 0, Infinity},
Method -> {"LobattoKronrodRule"}] -
NIntegrate[
CDF[NormalDistribution[\[Mu], \[Sigma]], \[Alpha]*y^0.5]*
PDF[GammaDistribution[p, 1], y], {y, 0, Infinity},
Method -> {"LobattoKronrodRule"}]) - ((2*\[Delta]*((x - \
\[Mu])/(\[Sigma]*\[Alpha]))^(2*p - 1)*
Exp[-((x - \[Mu])/(\[Sigma]*\[Alpha]))^2])/(\[Sigma]^3*(\[Rho] \

34

+ \[Delta]*T[p, \[Mu], \[Sigma], \[Alpha]])*
Gamma[p])) - (((x - \[Mu])/\[Sigma]^2)*Dgs2[\[Mu], \[Sigma]]);

The score ∂ log f (x;θ)

∂α

for α is given by

ea := -(2*\[Sigma]*\[Delta]*Gamma[p + 0.5])/(Sqrt[2*Pi]*Z*Gamma[p])*
Hypergeometric2F1[1/2, p + 1/2,
3/2, -\[Alpha]^2/
2] - (2*\[Sigma]*Gamma[p + 1/2])/(Sqrt[2*Pi]*Z*
Gamma[p])*((\[Alpha]^2/2 + 1)^(-p - 1/2) -
Hypergeometric2F1[1/2, p + 1/2,
3/2, -\[Alpha]^2/
2]) - (2*\[Delta]*((x - \[Mu])/(\[Sigma]*\[Alpha]))^(2*p)*
Exp[-((x - \[Mu])/(\[Sigma]*\[Alpha]))^2])/(\[Alpha]*(\[Rho] + \
\[Delta]*T[p, \[Mu], \[Sigma], \[Alpha]]));
NIntegrate[
em^2*f[\[Mu], \[Sigma], \[Alpha], \[Rho], \[Delta]]^(2 -
q), {x, -Infinity, Infinity}, Method -> {"LobattoKronrodRule"}];

The score ∂ log f (x;θ)

∂ρ

for ρ is given by

er := -\[Sigma]/Z + 1/(\[Rho] + \[Delta]*T[p, \[Mu], \[Sigma], \[Alpha]]);

The score ∂ log f (x;θ)

∂δ

for δ is given by

ed := -\[Delta]/Z*(1 + NIntegrate[CDF[NormalDistribution[\[Mu], \[Sigma]],
-\[Alpha]*y^0.5]*PDF[GammaDistribution[p, 1], y], {y, 0, Infinity},
Method -> {"LobattoKronrodRule"}] -
NIntegrate[CDF[NormalDistribution[\[Mu], \[Sigma]], \[Alpha]*y^0.5]*
PDF[GammaDistribution[p, 1], y], {y, 0, Infinity},
Method -> {"LobattoKronrodRule"}]) +
T[p, \[Mu], \[Sigma], \[Alpha]] /
(\[Rho] + \[Delta]*T[p, \[Mu], \[Sigma], \[Alpha]]);

An element for ∂ log f (x;θ)
∂µ
its corresponding elements of matrix [19]:

of Fisher information matrix based on log

is computed by the expression and

q

Imm := NIntegrate[em^2*f[\[Mu], \[Sigma], \[Alpha], \[Rho], \[Delta]]^(2 - q),
{x, -Infinity, Infinity}, Method -> {"LobattoKronrodRule"}]

The Fisher matrix M and inverse of M are given by

M = n*{{Imm, Ims, Ima, Imr, Imd}, {Ims, Iss, Isa, Isr, Isd}, {Ima,
Isa, Iaa, Iar, Iad}, {Imr, Isr, Iar, Irr, Ird}, {Imd, Isd, Iad,
Ird, Idd}};IM := Inverse[M]/n

35

