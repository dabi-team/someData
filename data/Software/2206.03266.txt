2
2
0
2

n
u
J

7

]

G
L
.
s
c
[

1
v
6
6
2
3
0
.
6
0
2
2
:
v
i
X
r
a

MACHINE LEARNING SENSORS

Pete Warden 1 Matthew Stewart 2 Brian Plancher 2 Colby Banbury 2 Shvetank Prakash 2 Emma Chen 2
Zain Asgar 1 Sachin Katti 1 Vijay Janapa Reddi 2

1Stanford University 2Harvard University

ABSTRACT
Machine learning sensors represent a paradigm shift for the future of embedded machine learning applications.
Current instantiations of embedded machine learning (ML) suffer from complex integration, lack of modularity,
and privacy and security concerns from data movement. This article proposes a more data-centric paradigm for
embedding sensor intelligence on edge devices to combat these challenges. Our vision for “sensor 2.0” entails
segregating sensor input data and ML processing from the wider system at the hardware level and providing a
thin interface that mimics traditional sensors in functionality. This separation leads to a modular and easy-to-use
ML sensor device. We discuss challenges presented by the standard approach of building ML processing into the
software stack of the controlling microprocessor on an embedded system and how the modularity of ML sensors
alleviates these problems. ML sensors increase privacy and accuracy while making it easier for system builders to
integrate ML into their products as a simple component. We provide examples of prospective ML sensors and an
illustrative datasheet as a demonstration and hope that this will build a dialogue to progress us towards sensor 2.0.

1

INTRODUCTION

Since the advent of AlexNet [43], deep neural networks have
proven to be robust solutions to many challenges that involve
making sense of data from the physical world. Machine
learning (ML) models can now run on low-cost, low-power
hardware capable of deployment as part of an embedded
device. Processing data close to the sensor on an embedded
device allows for an expansive new variety of always-on
ML use-cases that preserve bandwidth, latency, and energy
while improving responsiveness and maintaining data pri-
vacy. This emerging ﬁeld, commonly referred to as embed-
ded ML or tiny machine learning (TinyML) [73, 18, 39, 59],
is paving the way for a prosperous new array of use-cases,
from personalized health initiatives to improving manufac-
turing productivity and everything in-between.

However, the current practice for combining inference and
sensing is cumbersome and raises the barrier of entry to
embedded ML. At present, the general design practice is to
design or leverage a board with decoupled sensors and com-
pute (in the form of a microcontroller or DSP), and for the
developer to ﬁgure out how to run ML on these embedded
platforms. The developer is expected to train and optimize
ML models and ﬁt them within the resource constraints of
the embedded device. Once an acceptable prototype imple-
mentation is developed, the model is integrated with the rest
of the software on the device. Finally, the widget is tethered
to the device under test to run inference. The current ap-
proach is slow, manual, energy-inefﬁcient, and error-prone.

Figure 1. The Sensor 1.0 paradigm tightly couples the ML model
with the application processor and logic, making it difﬁcult to
provide hard guarantees about the ML sensor’s ultimate behavior.

Figure 2. Our proposed Sensor 2.0 paradigm. The ML model is
tightly coupled with the physical sensor, separate from the applica-
tion processor, and comes with an ML sensor datasheet that makes
its behavior transparent to the system integrators and developers.

It requires a sophisticated understanding of ML and the in-
tricacies of ML model implementations to optimize and ﬁt
a model within the constraints of the embedded device.

PhysicalSensorProcessorCloudMCUMachine Learning (ML) SensorProcessorCloudPhysicalSensorProcessorCloudMCUMachine Learning (ML) SensorProcessorCloud 
 
 
 
 
 
Machine Learning Sensors

To unlock the full potential of ML sensors, we must usher
in a new way of thinking about ML and sensing by raising
the level of abstraction for ML sensors from the current
approach (Figure 1) to a self-contained (hardware and soft-
ware) system that utilizes on-device ML to extract useful
information from some complex set of phenomena in the
physical world and reporting it through a simple interface
to a wider system (Figure 2). For example, consider an ML
sensor designed for person detection [26]. Such a device
is often used to wake up a user interface and automatically
adjust environmental controls like lighting, air conditioning,
and heating. In our “Sensor 2.0” paradigm, such a sensor
would have a very minimal interface with an opening that
points outward at the environment and only three external
pins: two for power and ground, and one signal pin that
is driven high when a person is detected and held high for
as long as people are present. As this information should
be all the knowledge that a future ML sensor system engi-
neer needs to know, such a system would be ready to use
out-of-the-box, easing practical deployment.

The above approach has the added beneﬁt of providing more
granular control on data and how it is consumed. By con-
straining the interface between the ML sensor and the rest
of the software to be around inference outcomes and not al-
lowing access to raw sensor data, we avoid the potential for
privacy breaches, whether they are due to malicious intent
or errors. We can also reason about the data ﬂow in a prin-
cipled manner since the ML sensor can now have a ”data”
datasheet, specifying what kind of data is exposed. This
approach enables independent veriﬁcation without trusting
software from the developer. It opens new paths to build-
ing modular ML sensor-based systems for smart homes,
enterprise IoT, and other domains. This method should also
help inform discussions about the privacy and ethical issues
of the technology, similar to how concerns were raised for
traditional internet of things (IoT) devices [3, 76, 21].

ML sensors that adopt
the sensor 2.0 paradigm are
slowly starting to emerge, such as Qualcomm’s Always-
on Computer Vision Module (CVM) [50, 57] and Bosch’s
BHI260AP smart sensor [9]. However, there is still a long
way before the industry adopts a principled approach to ML
sensor design. The current solutions are bespoke architec-
tures that are proprietary and less than transparent. So we
need to rethink how we systematically embed intelligence
into sensor 2.0 devices. To this end, we present ﬁve tentative
design principles to foster discussion within the community:

1. We need to raise the right level of abstraction that
will enable ease of use for scalable deployment of
ML sensors; not everyone should be required to be an
embedded systems developer or an engineer to use or
leverage ML sensors into their ecosystem.

2. The ML sensor’s design should be inherently data-
centric and deﬁned by its input-output behavior in-
stead of exposing the underlying hardware and soft-
ware mechanisms that support ML model execution.

3. An ML sensor’s implementation must be clean and
complexity-free. Features such as reusability, software
updates, and networking must be thought through to
ensure data privacy and secure execution.

4. ML sensors must be transparent, indicating in a pub-
licly and freely accessible ML sensor datasheet all the
relevant information such as fact sheets, model cards,
and dataset nutrition labels to supplement the tradi-
tional information available for hardware sensors.

5. We as a community should aim to foster an open ML
sensors ecosystem by maximizing data, model, and
hardware transparency where possible, without neces-
sarily relinquishing any claim to intellectual property.

In summary, the overarching goal of our work is to focus
new research and development efforts in a way that helps
the ﬁeld progress faster in delivering the potential beneﬁts
of the recent advances in ML and embedded hardware. To
this end, Section 2 outlines the challenges facing existing ap-
proaches for embedding ML intelligence into devices. Sec-
tion 3 describes our proposed approach. Section 4 focuses
on transparency of ML sensors via datasheets. Section 5
talks about the ecosystem development around ML sensors.
Section 6 calls for ethical considerations around the pitfalls
and dangers that we must address as ML sensors become
ubiquitous. Section 7 presents how ML sensors related to
prior work. Finally, Section 8 concludes the paper.

2 SENSOR 1.0 CHALLENGES

This section discusses the challenges conventional ap-
proaches face in integrating and deploying sensor-based
intelligence into embedded devices. While it is possible
to launch successful embedded ML applications using a
traditional low-level embedded systems approach, many
challenges will likely occur during development. The result-
ing systems often have undesirable properties. We discuss
these signiﬁcant challenges in the context of the end-to-end
major ML lifecycle stages, as illustrated in Figure 3.

2.1 Domain Skew Between Training and Sensor Data

Data makes or breaks ML. The data set used for training an
ML model is usually captured using different hardware and
software than is used for a deployed system. These sorts
of changes between the data between when you train and
when you run inference is known as domain skew. Many
properties can vary between sensors. For example, in the

Machine Learning Sensors

Table 1. Varying Sensor Characteristics

Microphones
Sampling rate
Digital or analog capturing
Echo cancellation
Noise Reduction
Gain
Frequency Sensitivity

Cameras
Field of View
Shutter duration
White Balance
CCD Demosaicing
Auto-exposure
Resolution

Accelerometers
Sampling Rate
Noise
Bias
Drift

2.2 Hardware & Software Development Complexities

Traditional software development is inherently different
from software development for machine learning systems.
These differences lead to a substantial “AI Tax” [62, 11]
that can lead to overhead and complexity that is typically
not present in traditional non-machine-learning systems.
Managing these overheads and complexity implies we need
ﬁrst to understand these differences to address them.

Typical embedded software applications usually focus on
signal processing, business logic, and hardware control.
These areas are implemented using traditional procedural
languages that typically involve data-conditional branching,
scattering, and gathering and are memory-bound. Unlike
these traditional workloads, machine-learning workloads
typically involve hundreds of thousands of multiply-adds
for each model inference, with highly-predictable access
patterns, a low ratio of memory accesses to arithmetic oper-
ations, and very few data-dependent branches.

These differences require different trade-offs from hard-
ware and system software designers to run the workloads
effectively. For example, a data cache using a predictive
algorithm will likely help speed up signal processing and
business logic code. However, ML memory accesses can
be accurately predicted at the compilation stage. So, con-
trolling the schedule of loading memory into faster areas
explicitly produces better results for those algorithms.

Another example is cooperative multitasking. Traditional
embedded modules are unlikely to take over tens of thou-
sands of cycles to complete an interaction since they execute
hand-written logic or trigger hardware features and return,
with the work often continued when an interrupt is signaled.
So it is relatively easy to add cooperative multitasking han-
dover calls at natural points in the code and have conﬁdence
that they will occur in a short and bounded interval. Be-
cause individual ML layers may take hundreds of thousands
of cycles to complete and model invocations might take
millions, it is not easy to hand over control at the expected
frequency from the application logic. Instead, handover
calls would need to be made from the inner loops of the
layer implementations, which is much more invasive and
requires preserving more states between yields.

Figure 3. The machine learning lifecycle involves multiple stages
of the pipeline, and several of these stages make the integration,
development, and deployment of ML sensors extremely difﬁcult.

case of a sensor for person detection [14], different imple-
mentations may have different camera modules, each with
different input resolutions and sensitivity.

Table 1 shows how sensor properties can vary across three
commonly used embedded ML sensors. In general, if the
properties are not the same between the sensors used to
capture the training data and those in the deployed system,
the model results are likely to be less accurate in production
than an evaluation on the test set would indicate. This
mismatch can contribute to compounding errors, known as
“data cascades,” in the ﬁnal application [64], problems that
can be both hard to identify and costly to ﬁx.

It is rare to capture all training data using the exact hard-
ware and software processing pipeline present in the ﬁnal
system. Often the data used for training has been gathered
by someone else and may not even include information on
the characteristics relevant during live operations.

To compensate for these differences, it is ﬁrst necessary to
measure model accuracy when run with the ﬁnal hardware
and software conﬁguration, feeding in representative cap-
tured data. This approach should give a reliable metric of
the model’s accuracy in practice. If the accuracy is below
what is required and expected from the training environment,
steps need to be taken to reduce the differences between the
training and production data. Such an approach might entail
retraining with more representative data captured from the
production environment, adjusting the sensor and process-
ing to match the training data more closely, or combining the
two approaches using synthetic data methods [52, 49, 20].

The domain skew becomes a barrier to deployment because
particular sensors are often chosen for availability or cost
reasons on a per-project basis. Hence, the compensation
process has to be performed for each project too. Thus,
skew can increase the cost of reusing models over time or
unpredictably reduce model accuracy.

Stage 4: Live Operations●Model drift and skew●Ethical challenges●Sustainability●Security and privacyStage 3: Model Deployment●Model robustness●Scalable deployment●Embedded MLOps●System IntegrationStage 2: Model Development●ML model architecture●Resource constraints●Model quality/accuracy●End-to-end performanceStage 1: Sensor Data●Heterogeneous devices●Multi-modal data●Sensor drift●Varying data frequencyML Sensors 2.0 LifecycleMachine Learning Sensors

There are many more differences in the workloads, which
has led to the development of specialized accelerator ASICs
for ML in the embedded world and beyond. Even if such
specialized accelerators are present, the embedded applica-
tion still has to deal with extra complexity to orchestrate the
model setup, invocations, and result handling, because these
stages will require something more like a remote invocation
over the bus than a regular branch to a function in the same
address space. Any caches will need to be invalidated, for
example, if shared memory has been altered.

Managing the ML computations imposes additional com-
plexity on the embedded system’s development, whatever
the underlying implementation. This becomes a tax on
development, as the hardware and operating system code de-
signed for traditional embedded software struggle to handle
the requirements of these new ML workloads.

2.3 Testing and Debugging Difﬁculties for

Deployment

It is challenging to debug an embedded system when some-
thing goes wrong. The constraints on memory size, power,
latency, and form factor make enforcing abstractions and
modularity in the code difﬁcult, and exposing state for ob-
servability is difﬁcult. Their compute capabilities are also
limited. These matters are complicated by adding ML to the
mix since ML frameworks have large resource requirements.

Moreover, embedded applications are usually deployed as
monolithic ﬁrmware binaries. Everything from the oper-
ating system and network stacks to the business logic is
compiled into the same executable. More often than not,
there are few accessible external boundaries where infor-
mation can be gained about how the system works. Such a
build environment makes testing individual modules hard
because they often depend on other system parts. After all,
the API abstractions are so leaky that extracting any single
part and running it on a board or simulator will at least
require compiling in the operating system and often other
dependencies, too. Hence, it becomes impossible to test a
module in isolation and have conﬁdence in the results.

In practice, testing and debugging challenges arise due to
the lack of unit tests. Tracking down the root cause of
bugs takes much effort because they cannot be localized
straightforwardly. From the authors’ experience, even if a
wake word model for keyword detection [80, 15] is shipped
as a library with minimal endpoints, a third party integrating
the model may experience problems they believe are caused
by the library code. The only feasible process to resolve the
question of where the issues are is to reproduce the client’s
full executable and environment on a hardware simulator.
Because the library and application share many resources, it
is often unclear if a problem such as failing to process audio
in the expected time is the fault of the framework code for

running too slowly or the client for not allowing enough
free compute cycles for the library to run and execute.

Similarly, given the limited memory protection, especially
on ultra-low-power and low-cost embedded systems, cor-
ruption or illegal access faults can be hard to attribute to
the right owners. There is a lack of enforcement mecha-
nisms for module boundaries when running as part of a
monolithic application, and this means that the contracts
between libraries and clients are unclear. This is a challenge
for ML frameworks because their functions tend to do more
computation and resource allocation than other libraries typ-
ically used for embedded systems, so their requirements and
constraints are also more complex.

2.4 Sensor Data Auditability During Live Operation

One of the signiﬁcant concerns with deploying embedded
devices is concern over the privacy of the users’ data. Hav-
ing an assurance that the raw data cannot be accessed and
shared is vital for users to have conﬁdence that they can
safely deploy embedded devices into private spaces.

Cameras and microphones are ubiquitous input devices to
ML models. These devices are often added to embedded
systems to help answer questions like “Is an object or person
present?” or “Has a particular word been uttered or a sound
heard?.” The answers to these questions are usually not
privacy-sensitive. They are stripped of unnecessary infor-
mation such as an individual’s identity or sounds and words
that are not in the small subset within focus. In contrast,
the raw camera and microphone outputs fed into the models
have much more potential to be used maliciously.

The challenge is that the standard approach to deploying
embedded models is integrating them as part of the same
software system that handles all other aspects of the ap-
plication (Figure 1). Consequently, raw sensor data is in
an address space accessible by the rest of the code since
embedded operating systems usually do not or cannot re-
strict access easily. To ensure that there are no deliberate or
accidental back doors that enable cameras or microphones
to be used as spying devices, it is necessary to audit all of
the software on the system. Such an audit might include
third-party network stacks that expose a large surface to
remote attackers on a networked device. Even if there are
no bugs, software updates may invalidate past audits and
could be vectors for in-house attacks.

We need to understand how we isolate the raw data from
leaking out to address this issue. Speciﬁcally, we need to
architect the ML sensor from the hardware and software.
Figure 2 illustrates one possible way of ensuring separation.
We elaborate on our proposal in greater detail in Section 3.

Machine Learning Sensors

2.5 Full-Stack Skills for the Complete ML Lifecycle

All of the above challenges are exacerbated by the software
development approach to integrating ML models requiring
a broad range of engineering skills beyond just understand-
ing traditional embedded system development. This aspect
makes deploying ML sensors extremely challenging. The
skills gap must be bridged to address this concern as there
is a dire need for infrastructure, frameworks, and tools that
can help automate and streamline the processes.

Model authoring, data collection, and labeling techniques
are new areas where the state-of-the-art is changing rapidly,
experienced practitioners are scarce, and training materials
are not abundant. An engineer needs to know a program-
ming language like Python and be comfortable with accel-
erator architectures and server environments to get started
with training.

Furthermore, to design an ML model that runs on an embed-
ded device, it is also necessary to understand the constraints.
These constraints include items like which model layer types
the chosen deployment framework supports, how compres-
sion techniques like quantization are supported, the read-
only/ﬂash and RAM budgets, and what latency is acceptable
for inference. The model author will also need extensive
knowledge of the sensor’s characteristics (Table 1) such
as capture frequencies, image resolutions, camera lens dis-
tortions, and audio preprocessing techniques such as noise
reduction, in addition to the particularities of the deployment
environment such as the presence of background noise.

The engineer responsible for integrating the model and in-
ference framework into the broader software system needs
to understand the usual details of embedded design but also
be able to investigate and debug issues around the ML mod-
ule. If the model is not producing the expected results, is
it because the input data is incorrect, is it that feature engi-
neering is failing, is there a bug in the ML framework, or is
the output from the model being processed incorrectly?

It is possible to split these responsibilities across multiple
engineers so that an embedded systems expert can inform
a model author of the constraints and requirements of the
target platform. An integration engineer can get debugging
help from an ML specialist, but the amount of continuous
information transfer needed and the need to have a good
enough understanding of both domains to communicate
effectively makes this arrangement onerous and risky. In
practice, successful deployments have required individual
engineers to have or gain proﬁciency in both ML and em-
bedded software development. People willing and able to
embrace this kind of “full-stack” of domain knowledge are
rare and hard to train. So lack of suitable engineers has
become a critical barrier to the broader deployment of em-
bedded ML applications.

To address these issues, better tooling [66] that streamlines
the development process and makes communicating the
required information more accessible will help lower the
amount of training required. There is also a need for bet-
ter educational resources to increase the available pool of
talent. Despite numerous existing education and training
resources [73, 59, 54, 39, 53, 60, 37], these will take time to
have an effect and be counterbalanced by growing demand.

3 SENSOR 2.0 PARADIGM

In this section, we introduce our deﬁnition of an ML sensor.
We follow through with a few widely used examples of ML
sensor implementations to help compare and contrast how
such sensors could be realized in the sensor 2.0 paradigm.
Next, we present our proposed approach as an initial set of
design principles for realizing ML sensors in practice.

3.1 Deﬁnition

We believe many of the sensor 1.0 paradigm’s problems
can be solved or reduced by encapsulating ML sensing
capabilities in separate hardware components outside the
central embedded processor and application code. Like
existing primitive types such as pressure, temperature, or
accelerometer sensors, ML sensors should expose only a
minimal interface through a handful of pins, with no access
to the primary address space. We believe this will provide
an intuitive way for embedded engineers to easily integrate
the newly-emerging advanced capabilities offered by ML
without the headaches of the current approach.

We propose the following deﬁnition of an ML sensor in
simple terms:

“An ML sensor is a self-contained system that
utilizes on-device machine learning to extract use-
ful information by observing some complex set of
phenomena in the physical world and reports it
through a simple interface to a wider system.”

This deﬁnition is somewhat ambiguous since “complex” and
“simple” are not well-deﬁned. However, it distinguishes
these devices from traditional sensors and the current ap-
proach to ML integration on embedded systems. Therefore,
we believe this deﬁnition will still be helpful.

3.2 ML Sensor Examples

Before we dive deeply into the proposed technical approach,
it is helpful to sketch some concrete examples of ML sen-
sors. To this end, we include four examples here, covering
different sensor modalities (audio, visual, and telemetry) to
show how the proposed concept generalizes.

3.2.1 Person Detector

3.2.3 Voice Command

Machine Learning Sensors

A common application of computer vision for embedded
devices is detecting when a person is nearby [15]. This
output is typically used to wake up a user interface and
automatically adjust environmental controls like lights, air
conditioning, heating, or doors. A person detector ML
sensor would ideally have three external pins, two for power
and ground and one signal pin driven high when a person is
detected and held high for as long as people are present. It
would also have an opening that would be pointed outward
at the environment. This information should be all the
knowledge an end-user needs to use the sensor.

Internally, the hardware for the person detector ML sensor
would be a small camera connected to a microcontroller
or digital signal processor. This processor would be run-
ning an ML model periodically to determine whether a
person was detected in the latest captured frame. If so, it
would trigger the single output pin. There may also need to
be a near-infrared LED that can illuminate nearby objects
when ambient lighting is not bright enough, with a camera
capable of capturing those frequencies. If the mounting
orientation cannot be guaranteed, an accelerometer could
also be present to detect which way the images are up.

3.2.2 Finger Tap Recognition

It is possible to use an ML model on accelerometer and
gyroscope data to detect ﬁnger taps on the casing of a de-
vice [36]. Advanced models can infer the location and even
the ﬁrmness of the contacts, but this capability depends on
knowledge of the shape and material of the case. In many
cases, just knowing that a tap or double tap has been de-
tected anywhere would be helpful enough and should be
robustly detectable across a wide range of system coverings.
An ML sensor using this approach would have three pins,
power, ground, and a signal pin driven high for a short pe-
riod (for example, 200 milliseconds) when a tap is detected.
Functionally, this can be used much like a traditional hard-
ware mechanical button, but without requiring any external
openings in the case and able to respond to virtual clicks
anywhere on the physical device.

Such a sensor could be constructed using an accelerometer,
gyroscope, and simple microcontroller. The memory, data
rate, and compute requirements of the model are low enough
that a comparatively low-cost processor can be used. Be-
cause no magnetometer or sensor fusion is needed, a basic
inertial measurement unit is sufﬁcient. There may be some
complications in designing a model that can work robustly
across all the possible variations in cases and mountings, and
there are likely to be false triggers. However, these are fa-
miliar problems to interface designers who have historically
worked with traditional buttons and switches. Therefore, we
do not think it will impair its usefulness signiﬁcantly.

There are a lot of possible approaches to voice interfaces.
If one searches the web or dictates text messages, the de-
vice needs to understand a wide variety of sentences as
faithfully as possible. However, some applications do not
require this level of understanding, where an interface only
needs to understand a few spoken commands. For exam-
ple, a light switch might only respond to “on”, “off”, and
maybe “dim” and “brighten.” Perhaps surprisingly, this is
not a signiﬁcantly easier problem to solve than the general
speech recognition required by more complex interfaces, be-
cause there will be many complete words and fragments that
sound similar to any chosen target words. These homonyms
must be excluded to avoid copious false positives during
regular conversation. However, commercial organizations
have solved this for particular wake words such as “Alexa”
or “Siri,” though the words themselves are usually chosen
carefully to avoid similar sounds used in everyday speech.

An intelligent ML-based voice command sensor could be
set up to recognize a small number of words in a particu-
lar language, either some collection of broadly-applicable
commands such as “on”, “off”, “stop”, “play”, and so on,
or sets that are more speciﬁc to a particular class of use
cases. If the choices are “on” and “off”, the sensor can
follow other examples of ground, power, and a single sig-
nal set high when “on” is heard and driven low for “off”.
For more complex word collections, a serial protocol such
as I2C will be required so that the sensor can send small
packets that indicate what command was recognized to the
central processor or system-on-chip in the system.

The implementation for this ML sensor in its simplest form
will consist of a microphone and a microcontroller, running
an ML model to detect the chosen words. A camera could
also be used to increase the overall accuracy, both by in-
tegrating gaze detection to ensure the command is aimed
at the device and by using video to improve recognition in
noisy environments [23].

3.2.4 Text Reader

Seven-segment numerical displays have been common on
equipment since the 1970’s. They were originally designed
for human readability, but ML models can now interpret
them with high accuracy. Teams involved in areas such
as agriculture and water distribution [78, 48] have been
working on applications that apply such models to retroﬁt
existing hardware systems that use these displays to output
information to nearby people, to also feed into secondary
systems for further processing. For example, an initially-
ofﬂine soil moisture analyzer can be upgraded to transmit
values periodically over the internet for remote monitoring,
using a small IoT board connected to a camera.

Machine Learning Sensors

Since this is expected to be a common use case, an inte-
grated sensor component that handles reading digital dis-
plays and can pass the numerical value to another module
could be helpful. The best interface design is not as evident
as some other examples. However, it could consist of a se-
rial protocol like I2C that transmits two thirty-two-bit values
periodically, the ﬁrst being the signed whole number portion
of the read value, and the second being the fractional part.
These should be transmitted in binary-coded decimal format
to avoid any conversion loss from the original displayed
values. This would accommodate up to eight digits on either
side of the decimal point.

The implementation for this ML sensor would rely on a
small camera connected to a microcontroller running a digit
recognition model. The goal would be to make it as general
and easy to use as possible, so the model would ideally be
able to ﬁnd the string of digits no matter the orientation
or position of the display within the camera frame. It may
be necessary to include a small display on the back of the
sensor to help installers position it correctly if there are
limitations on the model. There may also need to be some
kind of lighting for segment display types that do not self-
illuminate. If mechanical numerics, gauges, dials [16], or
even arbitrary text on a screen can be read, it may be possible
to extend the sensor concept to handle these as inputs.

3.3 Design Principles

The examples of ML sensors we described support various
use cases and have many common properties. Therefore, we
propose that a valuable and feasible ML sensor is deﬁned
by the following externally visible attributes.

3.3.1 Slim Interface

The ML sensors’ hardware and software must have simple
and slim interfaces. The internal implementations of most
ML sensors will involve a general-purpose processor or a
digital signal processor (DSP). It may be tempting to expose
the programmability available through a complex API to
enable features like model updating or other customizations
on the software front. We believe this would be a mistake.
Minimizing conﬁgurability allows testing, security, and pri-
vacy evaluations to be performed with higher conﬁdence
in their results. It is also helpful for application builders to
reduce the knowledge required to integrate the sensors into
their systems. As we discussed in Section 3.2, most ML
sensors should have a straightforward hardware interface
with three pins on the hardware front. If additional function-
ality or ﬂexibility is needed, serial protocols like I2C or SPI
could be readily adopted.

3.3.2 Reusability

All ML sensors should be usable for a variety of differ-
ent applications. It is more effort to build a self-contained
component than it would be to integrate ML software and
models into a single embedded system using the current
approach. Thus, the investment must provide beneﬁts over
multiple applications to make commercial sense. It is much
like the trade-off between writing a software library or just
writing ad-hoc code to solve a problem. The former en-
courages code reuse and robust testing and is just a better
software engineering practice, while the latter gets the job
done in the short term but proves harder in the long term for
maintenance. There should be evidence that any solution
will be used by more than one product before a decision is
made to build it more generically.

3.3.3 Composability

ML sensors ought to be designed such that they are compos-
able. One should be able to treat individual ML sensors as
building blocks that can be pieced together with other ML
sensors to realize more sophisticated ML sensors.

Consider, for instance, the voice command sensor we dis-
cussed (Section 3.2.3). Wake words such as “Siri,” “Alexa,”
or “Hey Google” are becoming ubiquitous interfaces for
everyday appliances. As the number of voice-controlled
devices in an environment grows, it becomes cumbersome
to know or specify which device a user is talking to as part
of a command such as “Siri, living room lights off.”

We believe that gaze can become a functional building block
for more intuitive interfaces in such a scenario. Eye contact
is a vital communication cue for both people and animals.
It gives us conﬁdence that any speech or non-verbal signals
are directed at us rather than anyone else. However, it is
still not common to use this cue for our interactions with
machines, though there are some signs of adoption, such
as phones hiding or showing their screens depending on
whether they detect a user looking at them. We believe that
using gaze to indicate which device the user is talking to
when providing the command, such as looking at a light
switch and saying “off” is a better method of realizing a
voice command sensor so that all devices do not turn on.

To support the more intelligent version of the basic voice
command sensor, we propose a gaze sensor with power,
ground, and a signal pin driven high when any person looks
directly at the component. It will require an opening for the
camera, and its internal implementation will be similar to
the person detector sensor. When the gaze detector signal
pin goes high, it can be coupled with the voice command
sensor’s signal pin to determine if the user was giving a
command to the device under consideration.

Machine Learning Sensors

This example shows that it is possible to build ML sensors
that can sense more complex phenomena using more basic
ML sensors. Enabling and supporting such composability
by design will allow each ML sensor to be robustly tested
and audited before being integrated into a wider system.

seem perverse since the capability is likely to be supported
by the sensor’s processor. However, ensuring the behavior
of a sensor cannot be changed after it has been deployed is
vital if users are going to have conﬁdence in the results of
any privacy guarantees, which we believe take precedence.

3.3.4 Calibration

Thus far, we have focused on widely-applicable problems
where a single ML sensor model can be reused across many
different products. However, there is another class of prob-
lems where the ML sensor requirements are similar to those
solved by a well-known model, but the recognition targets
themselves are domain-speciﬁc.

For example, a farm might want to recognize rodents instead
of people. ML can usually produce reasonable solutions to
these shifts in domains with comparatively small amounts
of data using transfer learning [74, 82, 79]. This technique
entails taking a model trained on a classic problem with an
extensive data set and ﬁne-tuning the parameters by running
training on the new classes, possibly only updating a few of
the later layers in the network using transfer learning.

Fine-tuning the top layers of an ML sensor model to its
domain-speciﬁc task is akin to calibrating a traditional hard-
ware sensor to its environmental characteristics. Transfer
learning forms the basis of many ML cloud services. We
believe it would be a valuable extension to preset ML sen-
sors in many cases. Transfer learning could be supported by
uploading customized models to an ML sensor, either by the
sensor provider or by the system integrator. If the system
integrator needs access, this may involve widening the thin
API we have discussed, so an approach that occurs during
sensor manufacturing might be preferable. The risk should
be manageable since this can be implemented by uploading
only model parameters with no executable code modiﬁed.

3.3.5 Connectivity

WiFi, cellular, LPWAN, and other network connections
are standard on the microcontrollers likely to be used for
ML sensor implementations. We believe that including
networking capability within these ML sensors themselves
is a bad idea. Any network connection can be seen as a pretty
wide API and dramatically increases the attack surface for
security and privacy exploits. It should be possible for third
parties to audit ML sensors to ensure with a high level of
conﬁdence that they cannot be misused to spy on people,
and networking makes this very hard to do.

3.3.6 Updatability

Another particular case of keeping the interface minimal is
removing the ability to update the software used by the in-
ternal implementation of the ML sensor. This approach may

However, the inability to update the ML model within the
sensor once deployed means that it may also be challenging
to resolve issues that arise due to the lack of connectivity,
as discussed in Section 3.3.5, precluding the possibility of
addressing concept drift [75]. Concept drift occurs when
the statistical properties of the target variable, which the
model is attempting to predict, alter in unanticipated ways
over time. This drift is problematic because the accuracy of
predictions decreases as time passes and can occur suddenly,
gradually, incrementally, and it may even recur over time.
Therefore, it is generally essential to detect.

3.4

Implementation Requirements

If ML sensors gain widespread adoption, they will need
to be attractive to embedded system designers. So their
implementations will need to conform to constraints.

3.4.1

Standardized Interchangeable Parts

Providing ML capabilities behind a simple abstract interface
will only simplify system building if the abstraction does
not leak. Different implementations of a given class of ML
sensor will naturally have different performance characteris-
tics depending on the hardware, software, and models used
internally. However, we should aim for the interface itself
to remain constant. For example, person detector sensors
(Section 3.2.1) from different sources might differ in their
sensitivity, performance in different lighting conditions, and
update latency, but there is no reason that they should expose
their signals in different ways since they are fundamentally
trying to report on the same basic information. Suppose the
ML sensor architects can agree on interface standards for
common types. In that case, it will increase the attractive-
ness of these components for system developers and avoid
unnecessary lock-in to particular suppliers. We discuss this
standardization further in ML sensor datasheets (Section 4).

3.4.2 Conformance Testing

One of the challenges for embedded product developers is
that they have no good way to predict how well an ML model
will work in a deployed system ahead of time because of all
the previously discussed challenges. Having well-deﬁned
testing protocols for types of ML sensors would ease this
problem. For example, a person detector sensor might be
placed in a room with a calibrated monitor at a set distance
displaying a series of labeled test images and the percentage
of true and false positives measured. Deﬁning these tests
may be an involved process; for example, this scenario

Machine Learning Sensors

does not consider lighting or near-infrared capabilities, but
even imperfect tests would help system builders compare
alternative components and plan for realistic performance.

well suited to our suggested design approach. With the
software development costs amortized over many different
deployed systems, we hope the economics of scale will favor
picking ML sensors in many cases.

3.4.3 Auditability

The most concerning aspect of the status quo for ML in em-
bedded systems is how it encourages the broad deployment
of cameras and microphones with no way for end-users or
even system integrators to check that they are not compro-
mising their users’ privacy by enabling recording. Central-
izing the risk within a single component and minimizing
the interface to the rest of the system is necessary to enable
the kind of third-party privacy audits that would offer more
conﬁdence for users, but it is not sufﬁcient.

We should also expect ML sensor providers to design their
implementations to make it easier for third parties to evalu-
ate important properties. For example, using an off-the-shelf
microcontroller with no WiFi, Bluetooth, or cellular capabil-
ity should make it straightforward for someone to verify that
networking is impossible. Source code inspection may also
be necessary to achieve high conﬁdence. However, because
the principles have minimized the attack surface we covered
earlier, and the software is a self-contained module running
in a separate address space from the rest of the system, it
ought to take less time than would be required for a regular
software library integrated into a device.

3.5 Potential Pitfalls and Challenges

Thus far, we have primarily focused on the advantages of
the proposed approach. To provide a balanced point of view,
we also outline the potential pitfalls and challenges of our
proposed approach to realize ML sensors’ capabilities.

3.5.1 Cost

Embedded systems often have very tight cost constraints,
and essential design decisions can be made based on differ-
ences of just a few cents. In this context, expecting system
builders to pay for the extra microcontroller within an ML
sensor could seem unrealistic. We believe that this will
not prove insurmountable for several reasons. First, there
are applications where the added user value and reduced
software development costs (or non-recurring engineering
(NRE)) will outweigh the extra expense, so we expect initial
customers even for early versions of the sensors.

Second, we expect the cost of sensors with integrated com-
pute capability to drop rapidly over time. Increasingly, we
already see camera modules, such as Sony’s Spresense [67]
or Himax’s Wise-eye [32] devices emerging, and we expect
that trend to continue. These compute-enabled sensors are
designed to run data processing in a separate module from
the processor controlling the primary system, so they are

3.5.2 Power

The TinyML movement targets devices capable of running
ML with less than one milliwatt of average power usage to
enable battery-powered and energy harvesting applications.
By adding a separate processor to deal with sensor data
analysis rather than integrating it into software running on an
existing microcontroller, the ML sensor approach may seem
like a step backward in power efﬁciency, and we do indeed
expect that such systems will use more energy to operate in
the near term. Early adopters are likely to be cases where
reducing power usage to single-digit milliwatt levels is not
the highest priority, such as devices connected to mains
electricity. However, we believe that reducing the power
usage signiﬁcantly over time should be possible, especially
as the sensor processor hardware can be specialized for a
single workload. In many cases, the signal from the model
run on the sensor data can wake up the general-purpose
processor, allowing it to hibernate in a shallow power mode
the rest of the time. If the ML processing is run as part of a
general software stack on the main microprocessor, it often
requires that all processor components are powered up, even
if the ML does not use them. There is often little choice
about waking different modules like the memory that can
take much energy even if unused, and sensor data processing
typically requires a cadence of running multiple times a
second, more frequently than most other tasks. Breaking the
sensor work out into another physical component makes it
easier to separate just the parts needed into their own power
domain, hopefully reducing overall energy consumption.

3.5.3 Universality

One of the underlying assumptions of the ML sensors con-
cept is that there are problems that occur across many dif-
ferent applications that can be solved with the same model.
The examples listed earlier in Section 3.2 assume that tasks
such as detecting when a person is present are helpful to
many different products and can be adequately addressed
by a single model. There is some evidence for this in cloud
ML APIs, which offer capabilities like speech or face detec-
tion [27] designed to work without knowledge of the end-use
case. However, these models typically require more time
and resources to train than application-speciﬁc networks.
They may also fail to perform as well as more specialized
models in situations not present in their training data. For
example, a person detector may have been trained with the
implicit assumption that the input images are shot horizon-
tally, and images from a sensor mounted on the ceiling
looking downwards showing the tops of people’s heads may

Machine Learning Sensors

not be detected accurately. Similar problems may occur if
the backgrounds, environmental conditions like smoke or
fog, or clothing vary from those present in training data.

We believe it is possible to develop models capable of work-
ing across a wide-enough domain of problems to be usable
for enough applications. Unfortunately, this question of uni-
versality can only be honestly answered through engineering
and then observing the results.

4 DATASHEETS FOR ML SENSORS

A datasheet is a document that speciﬁes the features and
characteristics of a product. Traditional commercially avail-
able sensors are sold with datasheets that outline their hard-
ware and operating characteristics. These characteristics
include parameters such as power consumption, operating
temperature, and the speciﬁcations of the sensing applica-
tion (e.g., detection limit, measurement frequency).

Datasheets are necessary as they allow developers to de-
termine whether the speciﬁcation of the sensor is suitable
for their particular application. Additionally, they act as a
performance reference that can be independently assessed
for quality assurance purposes. Some sensors may be used
in performance-critical workﬂows and, as such, require rig-
orous evaluation before being implemented into workﬂows.

ML sensors will require something analogous to the
datasheet of traditional sensors. However, this new datasheet
will need to document not only the information included
on a traditional sensor datasheet but will also need to cap-
ture the characteristics of the machine learning model, the
dataset(s) used to train and test the model, and end-to-end
metrics that capture the impact of environmental character-
istics on the accuracy of the device. An informed user can
determine whether the device is suitable for their particular
application.

There has been a growing focus on producing new
datasheets for ML in recent years. These range from
datasheets for datasets [24] to nutrition labels [33] which
provides a standard label for understanding a dataset’s char-
acteristics. Even specialized application areas of ML, such
as medical prognosis and diagnosis, are starting to build
their own custom datasheets out of the need for better trans-
parency [65].

4.1 Example Datasheet

An illustrative example of a datasheet for an ML sensor is
shown in Figure 4. It focuses on a person detector sensor
(Section 3.2.1), which we envisage as a broadly applicable
device. This section breaks down the proposed structure
of the datasheet in more detail. As stated above, our pro-
posed datasheet summarizes not only aspects of the sen-

sor hardware but also of the machine learning model, the
datasets used to train and test the model, the security, privacy,
and environmental impacts of the model, and end-to-end
application-speciﬁc performance characteristics; this type
of information is unique to the ML-based sensors.

4.1.1 Description, Features, and Use Cases

This section is essentially unchanged from that seen in a
traditional sensor datasheet. A high-level description of the
device is provided with some of its most essential charac-
teristics, such as the underlying processor, communication
protocols, and sensors utilized in the platform, summarized
in one to two paragraphs. Features are a bullet-pointed list
of the most salient aspects of the device. For a person detec-
tor, this might include the operating range and luminosity,
information about the embedded camera, the device’s size,
the form of output, and other characteristics. Use cases de-
scribe typical applications where the device might be used.
For a person detector, everyday use cases might include in-
telligent lighting or security systems or as part of a cascade
system that activates additional processes such as person
identiﬁcation or counting individuals in a room.

4.1.2 Compliance

The compliance section will often only include a few state-
ments or marks that demonstrate compliance with some
form of government or inter-governmental regulation or an
industry-speciﬁc expectation. Since ML sensors dovetail
ML with traditional embedded hardware, compliance fac-
tors relevant to both industries will need to be highlighted.
The presence or absence of speciﬁc compliance marks may
allow or prohibit the device from being sold or used in spe-
ciﬁc countries or regions. Examples of this might include
being lead-free [70, 17] or GDPR-compliant [71].

4.1.3 Model Characteristics

This section focuses on the characteristics of the machine
learning model integrated within the device. Similar to a
model card [46] or an ML-model fact-sheet [1], an ML sen-
sor’s characteristics may include information regarding the
training and test data set, such as its size and whether it is
open-source or not, the data modality, as well as information
regarding the model architecture. Additional information
on the datasets used to train and test the model may also be
provided, allowing users to understand better the device’s
potential biases and, therefore, better judge its applicability
to their application. Information could also be provided de-
tailing whether some form of third-party entity has validated
this performance.

Machine Learning Sensors

Figure 4. Illustrative example datasheet highlighting the various sections described in Section 4. On the top, we have the items currently
found in standard datasheets: the description, features, use cases, diagrams and form factor, hardware characteristics, and communication
speciﬁcation and pinout. On the bottom, we have the new items that need to be included in an ML sensor datasheet: the ML model
characteristics, dataset nutrition label, environmental impact analysis, and end-to-end performance analysis. While we compressed this
datasheet into a one-page illustrative example by combining features and data from a mixture of sources, on a real datasheet, we assume each
of these sections would be longer and include additional explanatory text to increase the transparency of the device to end-users. Interested
users can ﬁnd the most up-to-date version of the datasheet online at https://github.com/harvard-edge/ML-Sensors.

Dataset Nutrition LabelComplianceDiagrams andForm FactorDescription, Features, and Use CasesSource: docs.luxonis.comDescription: The PA1 Person Detection Module enables you to quickly and easily add smarts to your IoT deployment to monitor and detect for humans. You can use this module indoors and outdoors to understand where and when humans arrive at your deployment site.Features:•Real-time Person Detection with On-Device ML•Indoor and Outdoor use•Finds a person at a maximum distance of 10 meters to a minimum distance of 5 centimeters•Operates in low and high light environments (1-20000 Lux) across a wide temperature range (0 to 50 OC)•Features Color and Black-and-White Detection ModulesUse Cases:•Smart business and home security systems•Multi-modal key word spotting for virtual assistants•Occupancy sensors and other infrastructure sensorsPA1 Person Detection ModuleModel CharacteristicsHardware CharacteristicsSource: docs.luxonis.comPerformance AnalysisSources: fabacademy.org, electroschematics.com, and nxp.com/docsCommunication Specification and PinoutModel performance:Measured withPrecision-Recall (PR) andArea Under the PR Curve (PR-AUC). Download raw performance results datahere. Disaggregated performance measured withRecall, which captures how often the model misses faces with specific characteristics. Equal recall across subgroups corresponds to the“Equality of Opportunity”fairness criterion.Performance evaluated on:•A subset ofOpen Images•Face Detection Data Set and Benchmark•Labeled Faces in the WildSource: modelcards.withgoogle.com0%20%40%60%80%100%1101001000Test AccuracyDistance from Sensor (cm)Detection Accuracy vs Distance to Nearest Dynamic Object - 0.20 0.40 0.601101001000False Positives per HourDistance from Sensor (cm)False Positive Rate vs Distance from Nearest Static ObjectIndoorOutdoorLow lightSYMBOLRATINGMINMAXUNITVBUSRecommended Input Supply Voltage4.755.25VVBUS_MAXMaximum Input Supply Voltage3.55.5VIVBUS_MAXMaximum Input Supply Current01.5APPower Required46WPIDLEIdle Power Draw2.42.6WTAAmbient Operating Temperature4555OCSource: iotsecurityprivacy.orgEnvironmental Impact: Full reportcan be found here. 390g CO2-eq23L WaterEnvironmental ImpactSource: st.com Source: datanutrition.orgIoT Security & Privacy LabelMachine Learning Sensors

4.1.4 Dataset Nutrition Labels

Building on the concept of nutrition labels [33, 13] and
datasheets for datasets [24], the ML sensor datasheet will
need to include information on broader impacts of the ML
sensors and the ML model upon which it is based. Data
nutrition focuses on issues associated with the ML sensor’s
model training data. It helps us understand, for instance,
whether the underlying training data meets the licensing
agreements that permit the use of the model and whether
that data was subject to ethical reviews.

4.1.5 IoT Security & Privacy Labels

As we develop more consumer-facing devices that rely on
machine learning (ML) sensors, there is a need to assist
consumers and system implementers in making informed
purchasing decisions. To this end, it is crucial to ensure
that ML sensor manufacturers disclose their privacy and
security policies. In this regard, we suggest building upon
CMU’s IoT Security & Privacy Label [22, 45] efforts, which
include information on the privacy and security practices of
the smart device, such as the type of data the device collects
and whether or not it receives automatic security updates.

This section is where we can highlight what data is available
to the rest of the system and what is not, along with the
certiﬁcation of any external auditing process performed.
For example, we could mention that the module uses a
camera but only outputs whether a person is present or
not. This information could also be shared on consumer-
facing product descriptions to help end-users understand
what privacy safeguards are included.

4.1.6 Environmental Impact

The ML sensor datasheet should also include information
on the environmental impacts of training and running the
machine learning model used in the sensor. Including the
ML sensor’s footprint in datasheets helps bring awareness
to the large environmental impact that can be imposed by
the machine learning lifecycle [77]. Moreover, while the
footprint of a single low-cost, low-power microcontroller is
much smaller than that of cloud ML solutions [68], it is not
negligible. Thus, given ML sensors’ projected large-scale
deployment, the environmental impact must be considered.

4.1.7 End-to-end Performance Analysis

While the information on the model and data nutrition char-
acteristics are necessary for an ML sensor, it is not sufﬁcient
to inform potential consumers. Some type of information
must be available to the consumer to assess how the de-
vice’s performance might differ based on its speciﬁc ap-
plication, speciﬁcally in real-world characteristics. For a
person detector sensor, many aspects could be relevant to

its performance, such as the lighting environment and op-
erating distance. Thus, this section aims to provide that
information to potential purchasers by outlining how envi-
ronmental factors that might naturally be anticipated in the
data could be incorporated into the end-to-end performance
of the device. This way, there is some characterization of
how a difference in the data distribution compared to the
training data might alter model performance and impact the
device’s applicability for a user’s target application.

4.1.8 Diagrams and Form Factor

In addition to the unique characteristics that ML brings
to sensor 2.0, when buying any sensor, it is often helpful
to understand its size and shape for design and integra-
tion purposes. This information might inform a purchaser
whether the device is suitable for their application, or they
may choose to build around the device speciﬁcation. Often,
CAD models of sensors are available online so that product
designers can integrate these into their 3D models.

4.1.9 Hardware Characteristics

Hardware characteristics focus on the traditional aspects
of the sensor, such as its operating temperature, power
consumption, input voltage, and electrostatic discharge rat-
ing. These electrical characteristics highlight the low-level,
hardware-relevant aspects of the system, separating it from
the ML-relevant aspects of the system, which are examined
at a higher level of abstraction in the model characteristics
section of the datasheet as described in Section 4.1.3.

4.1.10 Communication Speciﬁcation and Pinout

Purchasers will use the datasheet as reference documenta-
tion for the sensor during both design and implementation,
meaning that information regarding its pin layout and com-
munication protocols is necessary to outline. This section
will show the user how to interact and use the sensor at
a hardware level to power the sensor, provide data to the
sensor, and extract and interpret the output.

5 ECOSYSTEM DEVELOPMENT

The ecosystem surrounding ML sensors can be imagined
as either closed source, open-source, or hybrid approach.
There are pros and cons related to each of these approaches.
This section presents our view of why an open ecosystem
around ML sensors is crucial to develop. Similar to how
open-source development for both ML and other software
has radically transformed the software ecosystem, we be-
lieve an open-source ecosystem around ML sensors can
radically accelerate the pace and innovation of ML sensors.

Machine Learning Sensors

5.1 Closed versus Open Source

Fostering an open ecosystem will allow the development of
more ﬂexible and innovative solutions than a closed ecosys-
tem, helping to maximize the impact of ML sensors from
both commercial and research perspectives. The open na-
ture of collaboration will also stimulate community-built
solutions that will likely be superior to any created by a
sole developer. The transparency of the solution will build
trust among users. Open source means it is faster to identify
issues and ﬁle bug reports, which results in quicker ﬁxes.
Given that one of the major hurdles facing embedded ML is
the need for full-stack expertise (Section 2), an open-source
community will be able to build ML sensors collaboratively.

However, achieving the vision of an open ecosystem while
still allowing sensor developers to maintain their intellectual
property, such as the trained model weights, will likely prove
challenging. Closed source solutions will protect intellectual
property but will potentially suffer from monopolies that
naturally lend themselves to a lack of interoperability caused
by ML sensor heterogeneity and potentially reduced trust
due to the black-box nature of the ML sensors.

To address this issue, a hybrid approach may prove optimal.
Allowing an ML sensor’s architecture to be open source
while keeping the trained model proprietary could be a fair
compromise. Also, a validation set could be provided to
users to allow probing of the model output and performance
while keeping the training data used for the algorithm and
explicit model architecture undisclosed. This approach ben-
eﬁts partial transparency while still providing an economic
incentive for companies to develop innovative solutions. In
the case of ML sensors that are applicable across many dif-
ferent use cases, the community may create entirely open
source solutions. At the same time, more specialized appli-
cations remain in the wheelhouse of bespoke developers.

5.2 Large, Public Sensor Datasets

One of the critical limitations of growing the ML sensor
ecosystem is the lack of public sensor datasets. Since ML
systems require large and high-quality datasets to develop ro-
bust models, an equivalent set of datasets is required for ML
sensors. Computer vision specialists have ImageNet [63]
at their disposal, while natural language researchers have
SQuAD [58]. A big question for the ML sensor community
will be “What is the ImageNet of ML sensors?”

Large public datasets are needed to accelerate ML sensors’
design, implementation, and deployment. Preliminary anal-
ysis of major technical organizations shows the widespread
use and adoption of open data sets to establish baselines [61].
Open data sets are vital to accelerating ML innovation for
everyone, but such resources remain scarce across the ML
ecosystem. Thus, a key question facing the ML sensors

community is if dataset creation can be accelerated akin to
the rapid development of open-source software.

To this end, we recommend creating a “Data Commons”
aimed at making sensor data of all forms publicly accessible
to the community under the appropriate licensing terms. A
crucial requirement here is to follow a standard sensor data-
ﬁle format, perhaps similar to the Open Data Format (O-DF)
standard designed for IoT devices [31].

5.3 Benchmarks and Toolchains

To foster a rich and open ML sensor ecosystem, we need
full-stack frameworks that are publicly accessible to ev-
eryone. The full-stack includes software frameworks for
training and running ML models efﬁciently, as well as hav-
ing a ﬂexible hardware infrastructure for accelerating ML
operations inside the ML sensor’s custom processor.

On the hardware front, given that ML sensors will be task-
speciﬁc, it makes sense to develop application-speciﬁc pro-
cessors (ASIPs) [38]. Building a tiny, custom ASIP for the
ML sensor itself (see Figure 2) with an instruction set archi-
tecture (ISA) tailored for the speciﬁc ML model provides
greater energy efﬁciency than an off-the-shelf processor.
The RISC-V [2] hardware ecosystem is well poised for en-
abling custom hardware acceleration for ML sensors.

Software frameworks such as Gemini [25] and CFU Play-
ground [56] are needed to enable the development of open-
source hardware. As these are full-stack software frame-
works capable of running the software and hardware, they
naturally lend themselves to hardware and software co-
design. As ML sensors are bespoke, co-design can offer a
signiﬁcant improvements for low-power, always-on ML.

Such an open hardware and software ecosystem will lead to
the proliferation of ML sensor solutions. We will need new
benchmarks and associated metrics to assess the various
emerging solutions. Benchmarks such as MLPerf Tiny [4]
can help streamline apples-to-apples comparisons of differ-
ent sensors. However, these existing benchmarks will need
to evolve to meet the needs of ML sensors. For instance, for
a person detector sensor, in addition to capturing traditional
metrics such as detection accuracy, we will need to cap-
ture additional metrics such as the minimum and maximum
distance at which the ML sensor will work reliably.

5.4 Sensor Libraries

As the ecosystem matures, the self-contained nature of ML
sensors will result in modular devices built for a singular
purpose. Naturally, this approach lends itself well to the po-
tential for composability (Section 3.3.3), allowing multiple
ML sensor devices, as well as traditional sensor devices, to
be combined as part of wider system architectures that can
achieve more specialized goals.

Machine Learning Sensors

Analogous to how open source libraries are created, the
ecosystem may work together to determine what devices
are most relevant to create, as well as an approach to mix-
and-match devices and communication protocols between
them that can be used to augment individual outputs. The
composability of ML sensors alongside traditional sensors
will provide a new and powerful toolkit for developing more
specialized ML-enabled hardware devices.

6 ETHICAL CONSIDERATIONS

While we anticipate the impact of ML sensors will be signiﬁ-
cant and net-positive, inevitably, there may be some adverse
impacts. The associated risks of these negative effects can
be minimized by limiting, for example, connectivity and up-
datability (Section 3). However, there is no fool-proof way
to guarantee that such methods can eliminate potentially
harmful applications. Consequently, care and considera-
tion must be shown to discern how potential harm can be
mitigated in the context of sensor 2.0.

6.1 Concerns

The growing deployment of online devices running au-
tonomously for extended periods opens up many oppor-
tunities for malicious actors [30]. Our proposal aims to
ensure that as many devices as possible have built-in safe-
guards against accessing personal data like recordings in
their default state. However, there are plenty of other pos-
sible harms that the proliferation of ML sensors is likely
to enable, and some that may be made worse by advances
in on-device ML. Existing ethical challenges in the realm
of traditional ML remain in the sensor 2.0 paradigm, but
additional considerations are necessary to determine how
running ML locally may cause speciﬁc problems.

There is a growing realization that AI has the potential to
revolutionize warfare. ML is already being used to develop
automatic target recognition systems [34]. On-device ML
sensing may push these dangerous capabilities one step fur-
ther. An example use case of on-device ML is in learning-
based autonomous weapons systems (AWS) [35]. A missile
could guide itself towards a target using ML sensors. Al-
ternatively, it is also possible to engineer a landmine or
improvised explosive device (IED) that triggers when a
person is nearby, or worse, when a person of a particular
demographic comes close.

Even outside military applications, organizations could de-
velop generalized ML sensors that distinguish between eth-
nicities or gender presentations and use these to discriminate
on a massive scale. Another potentially harmful application
could be an audio-based sensor that triggers when particular
words like “democracy,” “abortion,” or “human rights” are
spoken nearby, alerting repressive authorities to potential

dissidents. Applications like these are possible even with-
out our proposed approach but could be deployed far more
efﬁciently and widely thanks to their properties.

Last but not least, it is also possible that malicious or care-
less manufacturers take advantage of any increased public
trust in devices offering privacy through on-device process-
ing to promote hardware that is not as secure. For example,
their microphone might not be genuinely segregated from
the main microcontroller and be vulnerable to access.

6.2 Possible Solutions

Because many of the issues with our approach are scaled-up
versions of general problems created by ML being deployed,
many of the same tactics can be used to reduce likely harms.
All ML sensors should have a publicly available datasheet
(Section 4) that discusses the essential properties of the sys-
tem so that product integrators and end-users can be aware
of limitations and quirks. We also believe that the possibility
of having a third-party service audit the privacy claims is an
advantage of our approach, and recognized standards and a
certiﬁcation process should be developed in collaboration
with organizations like Underwriters Laboratories [44] or
Consumers’ Union [69].

Dealing with determined malicious actors is a lot more
challenging. Public availability makes the prospect of, for
example, terrorists using person detectors to trigger impro-
vised explosive devices very concerning and hard to com-
bat. The hardware modules will be available to buy freely
and cheaply from self-service component distributors like
DigiKey or Mouser, so screening prospective customers
will not be possible. Even if we identify and block prob-
lematic initial purchasers, they will be built into everyday
devices that are commonly available and modular. Thus,
malicious actors extracting them from existing products is
a genuine concern. Given this likely proliferation, we will
need to rely on similarly limited approaches for other elec-
tronic components with dual uses, primarily investigation
and enforcement.

7 RELATED WORK

The landscape of ML sensors is rapidly growing, with novel
applications emerging regularly, and, to make them run
efﬁciently, the hardware and software ecosystem is also
booming. We summarize several compelling use cases and
novel system designs and discuss how these innovations are
still applicable in the context of our sensor 2.0 paradigm.

7.1 Application Areas

Systems that use sensors and machine learning are becom-
ing rapidly adopted into the ecosystem. Examples range
from uses in the home or ofﬁce for keyword spotting [81]

Machine Learning Sensors

using audio sensors and person detection [6] using vision
sensors to industrial settings for predictive maintenance
using audio, motor bearing, or IMU sensor data [6]. In
addition to these classic tinyML examples, there are also ap-
plications deployed in wildlife settings using hydrophones
(underwater microphones) to prevent collisions with whales
and ship [40]. More forward-looking applications also exist.
For example, tiny unmanned aerial vehicles with embed-
ded intelligence have been deployed to improve pesticide
application in agriculture [42].

ML sensors will be helpful across all these application areas
and more. We believe that ML sensors will accelerate the
adoption of TinyML into these and many other application
areas because the sensor 2.0 paradigm lowers the barrier to
adoption. Since ML sensors are the physical embodiment of
ML with the addition of sense, it reduces the friction com-
monly found in the ecosystem, which is that developers and
users struggle to cope with understanding ML technology.

7.2 Hardware Technologies

Several hardware advancements on the horizon promise to
increase the efﬁciency of ML sensors dramatically. Hard-
ware such as ﬂexible electronics [7], compute-in-sensor [47],
compute-in-memory [12], analog computing [19], and neu-
romorphic computing hardware [10] can dramatically in-
crease the energy efﬁciency of ML at the edge. Due to
substantially different memory access and synchronicity ap-
proaches, they can be complex to implement in conjunction
with a traditional application processor. We believe these
will be resolved as use cases become obvious.

The sensor 2.0 paradigm abstracts the hardware differences
behind a simple asynchronous interface, thereby enabling
the hardware design space of ML sensors to be uncon-
strained by the application. We can leverage the hardware
technologies to build highly specialized hardware for partic-
ular ML sensor tasks. The investment required to build such
an application-speciﬁc integrated circuit will undoubtedly
be more signiﬁcant than that needed to develop software.
Nevertheless, if the potential market is large enough, this
approach could provide a good return on investment.

7.3 Model Optimization

Recent advancements in efﬁcient model design [5], quan-
tization [51], pruning [8], knowledge distillation [28], and
compression [41] have made it possible to deploy modern
ML models to severely resource-constrained devices, en-
abling the sensor 2.0 paradigm. However, many of these
optimization techniques are typically ignored in traditional
edge and tiny ML applications due to the technical expertise
needed to take them into production robustly.

Under the new paradigm, more aggressive model optimiza-
tions are possible since the additional complexity is borne by
the ML sensor designer and not each application developer,
therefore making the approach more scalable. Furthermore,
the model can be co-designed explicitly with the physical
sensor(s), increasing efﬁciency. This space is largely unex-
plored, leaving room for future work and new research.

7.4 Security

Previous work has aimed to protect user privacy in sensor
applications by encrypting the data in place or in transit [72].
This approach assumes the application developer is trusted
and is prone to man-in-the-middle attacks [29]. Embedded
ML aims to preserve privacy by keeping user data on the
device, but this assumes the application-level code is trusted.
Previous work has proposed running ML models in secure
enclaves to prevent untrusted users from accessing raw sen-
sor data [55]. However, privacy is difﬁcult to ensure without
hardware isolation, even with secure enclaves.

As discussed in sections 3.3.5 and 3.4.3, the sensor 2.0
paradigm isolates the user’s sensor data and exposes only
high-level information to the application. This approach pre-
serves the user’s privacy from hackers and potential misuse
by an otherwise trusted party. In ML sensors, the users in-
terpret the exposed high-level information, making it easier
to audit what information their devices expose.

8 CONCLUSION

We believe the modular sensor 2.0 approach to system de-
sign proposed herein for ML plus sensor capabilities can
offer many advantages to both manufacturers and end-users.
Product builders beneﬁt from having the complexity of an
ML implementation hidden behind a simple interface. Con-
sumers gain new assurances about the privacy of sensitive
information. Unanswered questions remain regarding the
economics, power consumption, and the generality of singu-
lar solutions to multiple contexts, but these can be addressed
through future prototyping and research. We think a new
approach is needed based on our experience deploying em-
bedded ML solutions. We hope that our proposal forms the
basis for a dialogue on how this can be achieved.

We encourage interested parties to visit mlsensors.org
where we hope to foster a community around ML sensors. In
this article, we have outlined several challenges and put forth
many suggestions and recommendations for how one might
go about addressing the challenges. But we believe that it
is challenging for any single organization to answer all the
questions. Nor is it possible for an organization to tackle all
the concerns in a fair and representative manner that meets
the embedded ecosystem’s scale and broad needs. To this
end, there is a clear need for community-level engagement.

Machine Learning Sensors

REFERENCES

[1] Arnold, M., Bellamy, R. K. E., Hind, M., Houde,
S., Mehta, S., Mojsilovi´c, A., Nair, R., Ramamurthy,
K. N., Olteanu, A., Piorkowski, D., Reimer, D.,
Richards, J., Tsay, J., and Varshney, K. R. Fact-
sheets: Increasing trust in ai services through sup-
IBM Journal of
plier’s declarations of conformity.
Research and Development, 63(4/5):6:1–6:13, 2019.
doi: 10.1147/JRD.2019.2942288.

[2] Asanovi´c, K. and Patterson, D. A. Instruction sets
should be free: The case for risc-v. EECS Depart-
ment, University of California, Berkeley, Tech. Rep.
UCB/EECS-2014-146, 2014.

[3] Atlam, H. F. and Wills, G. B. Iot security, privacy,
safety and ethics. In Digital twin technologies and
smart cities, pp. 123–149. Springer, 2020.

[4] Banbury, C., Reddi, V. J., Torelli, P., Holleman, J., Jef-
fries, N., Kiraly, C., Montino, P., Kanter, D., Ahmed,
S., Pau, D., et al. Mlperf tiny benchmark. arXiv
preprint arXiv:2106.07597, 2021.

[5] Banbury, C., Zhou, C., Fedorov, I., Matas, R., Thakker,
U., Gope, D., Janapa Reddi, V., Mattina, M., and What-
mough, P. Micronets: Neural network architectures
for deploying tinyml applications on commodity mi-
crocontrollers. Proceedings of Machine Learning and
Systems, 3:517–532, 2021.

[6] Banbury, C. R., Reddi, V. J., Lam, M., Fu, W., Fazel,
A., Holleman, J., Huang, X., Hurtado, R., Kanter,
D., Lokhmotov, A., et al. Benchmarking tinyml
systems: Challenges and direction. arXiv preprint
arXiv:2003.04821, 2020.

[7] Biggs, J., Myers, J., Kufel, J., Ozer, E., Craske, S.,
Sou, A., Ramsdale, C., Williamson, K., Price, R., and
White, S. A natively ﬂexible 32-bit arm microproces-
sor. Nature, 595(7868):532–536, 2021.

[8] Blalock, D., Gonzalez Ortiz, J. J., Frankle, J., and
Guttag, J. What is the state of neural network pruning?
Proceedings of machine learning and systems, 2:129–
146, 2020.

[9] Bosch. Smart sensor bhi260ap — bosch sensortec.

https://www.bosch-sensortec.com/
products/smart-sensors/bhi260ap/,
2022. (Accessed on 05/26/2022).

[10] Bouvier, M., Valentian, A., Mesquida, T., Rummens,
F., Reyboz, M., Vianello, E., and Beigne, E. Spik-
ing neural networks hardware implementations and
challenges: A survey. ACM Journal on Emerging
Technologies in Computing Systems (JETC), 15(2):
1–35, 2019.

[11] Buch, M., Azad, Z., Joshi, A., and Reddi, V. J. Ai
tax in mobile socs: End-to-end performance analysis
of machine learning in smartphones. In 2021 IEEE
International Symposium on Performance Analysis of
Systems and Software (ISPASS), pp. 96–106. IEEE,
2021.

[12] Chang, L., Li, C., Zhang, Z., Xiao, J., Liu, Q., Zhu, Z.,
Li, W., Zhu, Z., Yang, S., and Zhou, J. Energy-efﬁcient
computing-in-memory architecture for ai processor:
device, circuit, architecture perspective. Science China
Information Sciences, 64(6):1–15, 2021.

[13] Chmielinski, K. The data nutrition project. https:
(Accessed on

//datanutrition.org/, 2021.
05/24/2022).

[14] Chowdhery, A., Warden, P., Shlens, J., Howard, A.,
and Rhodes, R. Visual wake words dataset. arXiv
preprint arXiv:1906.05721, 2019.

[15] Chowdhery, A., Warden, P., Shlens, J., Howard, A.,
and Rhodes, R. Visual wake words dataset, 2019. URL
https://arxiv.org/abs/1906.05721.

[16] Coldewey, D. LiLz uses computer vision to read
to
gauges and dials where humans prefer not
https://techcrunch.com/2022/
tread.
01/04/lilz-uses-computer-vision-to-
read-gauges-and-dials-where-humans-
prefer-not-to-tread/, 2022.
(Accessed on
05/27/2022).

[17] Commission, E.

Eur-lex - 32011l0065 - en -
https://eur-lex.europa.eu/

eur-lex.
legal-content/EN/TXT/?uri=CELEX:
32011L0065, 2011. (Accessed on 05/24/2022).

[18] David, R., Duke, J., Jain, A., Janapa Reddi, V., Jeffries,
N., Li, J., Kreeger, N., Nappier, I., Natraj, M., Wang,
T., et al. Tensorﬂow lite micro: Embedded machine
learning for tinyml systems. Proceedings of Machine
Learning and Systems, 3:800–811, 2021.

[19] Draghici, S.

Neural networks in analog hard-
Interna-
ware—design and implementation issues.
tional journal of neural systems, 10(01):19–42, 2000.

[20] Emam, K. E. Accelerating AI with Synthetic Data.

O’Reilly Media, Inc., 2020.

[21] Emami-Naeini, P., Agarwal, Y., Cranor, L. F., and
Hibshi, H. Ask the experts: What should be on an iot
privacy and security label? In 2020 IEEE Symposium
on Security and Privacy (SP), pp. 447–464. IEEE,
2020.

Machine Learning Sensors

[22] Emami-Naeini, P., Agarwal, Y., Cranor, L. F., and
Hibshi, H. Ask the experts: What should be on an
iot privacy and security label?, 2020. URL https:
//arxiv.org/abs/2002.04631.

[23] Ephrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson,
K., Hassidim, A., Freeman, W. T., and Rubinstein,
M. Looking to listen at the cocktail party: A speaker-
independent audio-visual model for speech separation.
arXiv preprint arXiv:1804.03619, 2018.

[24] Gebru, T., Morgenstern, J., Vecchione, B., Vaughan,
J. W., Wallach, H., Daum´e III, H., and Crawford,
K. Datasheets for datasets (2018). arXiv preprint
arXiv:1803.09010, 2018.

[25] Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V.,
Prakash, P., Zhao, J., Grubb, D., Liew, H., Mao, H.,
et al. Gemmini: Enabling systematic deep-learning
architecture evaluation via full-stack integration. In
2021 58th ACM/IEEE Design Automation Conference
(DAC), pp. 769–774. IEEE, 2021.

[26] Google.

tﬂite-micro/readme.md at main ·
tensorﬂow/tﬂite-micro. https://github.com/
tensorflow/tflite-micro/blob/main/
tensorflow/lite/micro/examples/
person detection/README.md#running-
on-sparkfun-edge, 2021.
05/24/2022).

(Accessed on

[27] Google. Detect faces — cloud vision api — google
cloud. https://cloud.google.com/vision/
docs/detecting-faces, 2022.
(Accessed on
05/24/2022).

[28] Gou, J., Yu, B., Maybank, S. J., and Tao, D. Knowl-
edge distillation: A survey. International Journal of
Computer Vision, 129(6):1789–1819, 2021.

[29] Gou, Q., Yan, L., Liu, Y., and Li, Y. Construction
and strategies in iot security system. In 2013 IEEE
international conference on green computing and com-
munications and IEEE internet of things and IEEE
cyber, physical and social computing, pp. 1129–1132.
IEEE, 2013.

[30] Gray, S.

Always on: privacy implications of
In Future of privacy

microphone-enabled devices.
forum, pp. 1–10, 2016.

[31] Group, T. O. Open data format (o-df), an open
http:
group internet of things (iot) standard.
//www.opengroup.org/iot/odf/, 2014. (Ac-
cessed on 05/24/2022).

[32] HiMax. Himx-announces-ai-embedded-wiseeye-we-
https://www.himax.com.tw/

i-plus-asic.pdf.
wp-content/uploads/2019/09/HIMX-
Announces-AI-Embedded-WiseEye-WE-I-
Plus-ASIC.pdf, 2019. (Accessed on 05/24/2022).

[33] Holland, S., Hosny, A., Newman, S., Joseph, J., and
Chmielinski, K. The dataset nutrition label: A frame-
work to drive higher data quality standards. arXiv
preprint arXiv:1805.03677, 2018.

[34] Host,

learning

P.
darpa

deep machine

Deep learning analytics devel-
proto-
ops
https://www.defensedaily.com/
type.
hub updates/deep-learning-analytics-
develops-darpa-deep-machine-
learning-prototype/, 2016.
on 05/31/2022).

(Accessed

[35] Hua, S.-S. Machine learning weapons and interna-
tional humanitarian law: Rethinking meaningful hu-
man control. Geo. J. Int’l L., 51:117, 2019.

[36] Huang, M. X., Li, Y., Nazneen, N., Chao, A., and Zhai,
S. Tapnet: The design, training, implementation, and
applications of a multi-task learning cnn for off-screen
mobile input, 2021. URL https://arxiv.org/
abs/2102.09087.

[37] Impulse, E. Introduction to embedded machine learn-
ing — coursera. https://www.coursera.org/
learn/introduction-to-embedded-
machine-learning, 2021.
05/24/2022).

(Accessed on

[38] Jain, M. K., Balakrishnan, M., and Kumar, A. Asip
In VLSI
design methodologies: survey and issues.
Design 2001. Fourteenth International Conference on
VLSI Design, pp. 76–81. IEEE, 2001.

[39] Janapa Reddi, V., Plancher, B., Kennedy, S., Moroney,
L., Warden, P., Suzuki, L., Agarwal, A., Banbury, C.,
Banzi, M., Bennett, M., Brown, B., Chitlangia, S.,
Ghosal, R., Grafman, S., Jaeger, R., Krishnan, S.,
Lam, M., Leiker, D., Mann, C., Mazumder, M., Pajak,
D., Ramaprasad, D., Smith, J. E., Stewart, M., and
Tingley, D. Widening access to applied machine learn-
ing with tinyml. Harvard Data Science Review, 2022.
doi: 10.1162/99608f92.762d171a. URL https:
//hdsr.mitpress.mit.edu/pub/0gbwdele.
https://hdsr.mitpress.mit.edu/pub/0gbwdele.

[40] Johnson, K.

Google’s ai powers

real-time
https:

tracking in vancouver bay.

orca
//venturebeat.com/2020/01/28/
googles-ai-powers-real-time-orca-
tracking-in-vancouver-bay/, 2020.
cessed on 05/24/2022).

(Ac-

Machine Learning Sensors

[41] Kim, H., Khan, M. U. K., and Kyung, C.-M. Efﬁcient
neural network compression. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pp. 12569–12577, 2019.

[52] Patki, N., Wedge, R., and Veeramachaneni, K. The syn-
thetic data vault. In 2016 IEEE International Confer-
ence on Data Science and Advanced Analytics (DSAA),
pp. 399–410, 2016. doi: 10.1109/DSAA.2016.49.

[42] King, A. Technology: The future of agriculture. Na-

ture, 544(7651):S21–S23, 2017.

[43] Krizhevsky, A., Sutskever, I., and Hinton, G. E.
Imagenet classiﬁcation with deep convolutional
In Pereira, F., Burges, C., Bot-
neural networks.
(eds.), Advances
tou, L., and Weinberger, K.
in Neural Information Processing Systems, vol-
ume 25. Curran Associates,
URL
Inc., 2012.
https://proceedings.neurips.cc/
paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-
Paper.pdf.

[44] Laboratories, U. Ul empowering trust. https://
www.ul.com/, 2022. (Accessed on 05/31/2022).

[45] Megas, K., Cuthill, B., and Gupta, S. Establishing
conﬁdence in iot device security: How do we get
there?(draft). Technical report, National Institute of
Standards and Technology, 2021.

[46] Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasser-
man, L., Hutchinson, B., Spitzer, E., Raji, I. D., and
Gebru, T. Model cards for model reporting. In Pro-
ceedings of the conference on fairness, accountability,
and transparency, pp. 220–229, 2019.

[47] Moin, A., Zhou, A., Rahimi, A., Menon, A., Benatti,
S., Alexandrov, G., Tamakloe, S., Ting, J., Yamamoto,
N., Khan, Y., et al. A wearable biosensing system with
in-sensor adaptive machine learning for hand gesture
recognition. Nature Electronics, 4(1):54–63, 2021.

[48] Mueller, J. Digitizer - AI on the edge: An ESP32 all-
inclusive neural network recognition system for meter
digitalization. https://github.com/jomjol/
AI-on-the-edge-device, 2021. (Accessed on
05/27/2022).

[49] Nikolenko, S. I. et al. Synthetic data for deep learning.

Springer, 2021.

[50] page, S. H.

Qualcomm wants your smart-
https:

phone to have energy-efﬁcient eyes.
//www.technologyreview.com/2017/03/
29/243161/, 2017. (Accessed on 05/26/2022).

[51] Park, E., Yoo, S., and Vajda, P. Value-aware quantiza-
tion for training and inference of neural networks. In
Proceedings of the European Conference on Computer
Vision (ECCV), pp. 580–595, 2018.

[53] Plancher, B.

The tiny machine learning open
https://
(Accessed

education initiative (tinymledu).
tinyml.seas.harvard.edu/, 2022.
on 05/24/2022).

[54] Plancher, B. and Janapa Reddi, V. Tinymledu: The
tiny machine learning open education initiative. In
Proceedings of the 53rd ACM Technical Symposium
on Computer Science Education V. 2, pp. 1159–1159,
2022.

[55] Prabhu, K., Jun, B., Hu, P., Asgar, Z., Katti, S., and
Warden, P. Privacy-preserving inference on the edge:
Mitigating a new threat model. In Research symposium
on tiny machine learning, 2020.

[56] Prakash, S., Callahan, T., Bushagour, J., Banbury, C.,
Green, A. V., Warden, P., Ansell, T., and Reddi, V. J.
Cfu playground: Full-stack open-source framework
for tiny machine learning (tinyml) acceleration on fp-
gas. arXiv preprint arXiv:2201.01863, 2022.

[57] Qualcomm. Always-on Vision Becomes a Real-
https://www.edge-ai-vision.com/

ity.
2017/08/always-on-vision-becomes-
a-reality-a-presentation-from-
qualcomm-research/,
on 05/26/2022).

2017.

(Accessed

[58] Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
Squad: 100,000+ questions for machine comprehen-
sion of text, 2016. URL https://arxiv.org/
abs/1606.05250.

[59] Reddi, V. J. Tiny machine learning (tinyml) pro-
https://www.edx.org/

fessional certiﬁcate.
professional-certificate/harvardx-
tiny-machine-learning, 2021. (Accessed on
05/24/2022).

[60] Reddi, V. J.

Mlops for scaling tinyml —
edx. https://www.edx.org/course/mlops-
for-scaling-tinyml, 2022.
(Accessed on
05/31/2022).

[61] Reddi, V. J., Diamos, G., Warden, P., Mattson, P., and
Kanter, D. Data engineering for everyone, 2021. URL
https://arxiv.org/abs/2102.11447.

[62] Richins, D., Doshi, D., Blackmore, M., Nair, A. T.,
Pathapati, N., Patel, A., Daguman, B., Dobrijalowski,
D., Illikkal, R., Long, K., et al. Ai tax: The hidden
cost of ai data center applications. ACM Transactions
on Computer Systems (TOCS), 37(1-4):1–32, 2021.

Machine Learning Sensors

[74] Weiss, K., Khoshgoftaar, T. M., and Wang, D. A
survey of transfer learning. Journal of Big data, 3(1):
1–40, 2016.

[75] Widmer, G. and Kubat, M. Learning in the presence of
concept drift and hidden contexts. Machine learning,
23(1):69–101, 1996.

[76] Wolf, M. and Serpanos, D. Safety and security of
cyber-physical and internet of things systems [point
of view]. Proceedings of the IEEE, 105(6):983–984,
2017.

[77] Wu, C.-J., Raghavendra, R., Gupta, U., Acun, B.,
Ardalani, N., Maeng, K., Chang, G., Behram, F. A.,
Huang, J., Bai, C., et al. Sustainable ai: Environmen-
tal implications, challenges and opportunities. arXiv
preprint arXiv:2111.00364, 2021.

[78] Yan, S. Deep learning in deep wells: MCU+AI
makes the ”impossible” smart meter reading in rural
wells possible! https://mp.weixin.qq.com/s/
BF94p22bUlyri3yYqBYCsQ, 2021. (Accessed on
05/27/2022).

[79] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
How transferable are features in deep neural networks?
Advances in neural information processing systems,
27, 2014.

[80] Zhang, Y., Suda, N., Lai, L., and Chandra, V. Hello
edge: Keyword spotting on microcontrollers, 2017.
URL https://arxiv.org/abs/1711.07128.

[81] Zhang, Y., Suda, N., Lai, L., and Chandra, V. Hello
edge: Keyword spotting on microcontrollers. arXiv
preprint arXiv:1711.07128, 2017.

[82] Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H.,
Xiong, H., and He, Q. A comprehensive survey on
transfer learning. Proceedings of the IEEE, 109(1):
43–76, 2020.

[63] Russakovsky, O., Deng, J., Su, H., Krause, J.,
Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., Berg, A. C., and Fei-Fei, L. Ima-
genet large scale visual recognition challenge, 2014.
URL https://arxiv.org/abs/1409.0575.

[64] Sambasivan, N., Kapania, S., Highﬁll, H., Akrong, D.,
Paritosh, P., and Aroyo, L. M. “everyone wants to do
the model work, not the data work”: Data cascades
In proceedings of the 2021 CHI
in high-stakes ai.
Conference on Human Factors in Computing Systems,
pp. 1–15, 2021.

[65] Sendak, M. P., Gao, M., Brajer, N., and Balu, S. Pre-
senting machine learning model information to clin-
ical end users with model facts labels. NPJ digital
medicine, 3(1):1–4, 2020.

[66] Shelby, Z. and Jongboom, J. Edge Impulse: Develop-
ment platform for machine learning on edge devices.
https://edgeimpulse.com/, 2019. (Accessed
on 05/27/2022).

[67] Sony.

Sony Spresense Overview.

https:

//developer.sony.com/develop/
spresense/, 2018. (Accessed on 05/24/2022).

[68] STMicroelectronics. Footprint of a microcontroller.
https://www.st.com/content/st com/en/
about/st approach to sustainability/
sustainability-priorities/
sustainable-technology/eco-design/
footprint-of-a-microcontroller.html,
2022. (Accessed on 05/24/2022).

[69] Union, C. Product reviews and ratings - consumer re-
ports. https://www.consumerreports.org/,
2022. (Accessed on 05/31/2022).

[70] Union, E.

2022 rohs compliance guide: Reg-
https:
(Accessed on

ulations, 10 substances, exemptions.
//www.rohsguide.com/, 2022.
05/24/2022).

[71] Voigt, P. and Von dem Bussche, A. The eu general
data protection regulation (gdpr). A Practical Guide,
1st Ed., Cham: Springer International Publishing, 10
(3152676):10–5555, 2017.

[72] Wang, X., Zhang, J., Schooler, E. M., and Ion, M.
Performance evaluation of attribute-based encryption:
Toward data privacy in the iot. In 2014 IEEE Inter-
national Conference on Communications (ICC), pp.
725–730. IEEE, 2014.

[73] Warden, P. and Situnayake, D. Tinyml: Machine
learning with tensorﬂow lite on arduino and ultra-
low-power microcontrollers. O’Reilly Media, 2019.

