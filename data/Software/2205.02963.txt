Privacy-from-Birth: Protecting Sensed Data
from Malicious Sensors with VERSA

Ivan De Oliveira Nunes
Rochester Institute of Technology
ivanoliv@mail.rit.edu

Seoyeon Hwang
UC Irvine
seoyh1@uci.edu

Sashidhar Jakkamsetti
UC Irvine
sjakkams@uci.edu

Gene Tsudik
UC Irvine
gene.tsudik@uci.edu

2
2
0
2

y
a
M
5

]

R
C
.
s
c
[

1
v
3
6
9
2
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—With the growing popularity of the Internet-of-
Things (IoT), massive numbers of specialized devices are de-
ployed worldwide, in many everyday settings, including homes,
ofﬁces, vehicles, public spaces, and factories. Such devices usually
perform sensing and/or actuation. Many of them handle sensitive
and personal data. If left unprotected, ambient sensing (e.g.,
of temperature, motion, audio, or video) can leak very private
information. At the same time, some IoT devices use low-end
computing platforms with few (or no) security features.

There are many well-known techniques to secure sensed data,
e.g., by authenticating communication end-points, encrypting
data before transmission, and obfuscating trafﬁc patterns. Such
techniques protect sensed data from external adversaries while
assuming that the sensing device itself is secure. Meanwhile, both
the scale and frequency of IoT-focused attacks are growing. This
prompts a natural question: how to protect sensed data even if
all software on the device is compromised? Ideally, in order to
achieve this, sensed data must be protected from its genesis, i.e.,
from the time when a physical analog quantity is converted into
its digital counterpart and becomes accessible to software. We
refer to this property as PfB: Privacy-from-Birth.

In this work, we formalize PfB and design Veriﬁed Remote
Sensing Authorization (VERSA) – a provably secure and formally
veriﬁed architecture guaranteeing that only correct execution
of expected and explicitly authorized software can access and
manipulate sensing interfaces, speciﬁcally, General Purpose In-
put/Output (GPIO), which is the usual boundary between analog
and digital worlds on IoT devices. This guarantee is obtained with
minimal hardware support and holds even if all device software
is compromised. VERSA ensures that malware can neither gain
access to sensed data on the GPIO-mapped memory nor obtain
any trace thereof. VERSA is formally veriﬁed and its open-sourced
implementation targets resource-constrained IoT edge devices,
commonly used for sensing. Experimental results show that PfB
is both achievable and affordable for such devices.

I. INTRODUCTION

The impact and importance of embedded (aka IoT or
“smart”) devices is hard to overestimate. They are increasingly
popular and becoming pervasive in many settings: from homes
and ofﬁces to public spaces and industrial facilities. Not
surprisingly, they also represent increasingly attractive attack
targets for exploits and malware. In particular, low-end (cheap,
small, and simple) micro-controller units (MCUs) are designed
with strict cost, size, and energy limitations. Thus, it is hard
to offer any concrete guarantees for tasks performed by these
MCUs, due to their lack of sophisticated security and privacy
features, compared to higher-end computing devices, such as
smartphones or general-purpose IoT controllers, e.g., Amazon

Echo or Google Nest. As MCUs increasingly permeate private
spaces, exploits that abuse their sensing capabilities to obtain
sensitive data represent a signiﬁcant privacy threat.

Over the past decade, the IoT privacy issues have been
recognized and explored by the research community [1], [2],
[3], [4], [5]. Many techniques (e.g., [6], [7]) were developed
to secure sensor data from active attacks that impersonate
users, IoT back-ends, or servers. Another research direction
focused on protecting private data from passive in-network
observers that intercept trafﬁc [8], [9], [10], [11] or perform
trafﬁc analysis based on unprotected packet headers and other
metadata, e.g., sizes, timings, and frequencies. However, secu-
rity of sensor data on the device which originates that data has
not been investigated. We consider this to be a crucial issue,
since all software on the device can be compromised and leak
(exﬁltrate) sensed data. Whereas, aforementioned techniques
assume that sensing device runs the expected benign software.
We claim that in order to solve this problem, privacy of
sensed data must be ensured “from birth”. This corresponds
to two requirements: (1) access to sensing interfaces must be
strictly controlled, such that only authorized code is allowed
to read data, (2) sensed data must be protected as soon as it is
converted to digital form. Even the simplest devices (e.g., mo-
tion sensors, thermostats, and smart plugs) should be protected
since prior work [12], [13], [14], [15] amply demonstrates that
private – and even safety-critical – information can be inferred
from sensed data. It is also well-known that even simple low-
end IoT devices are subject to malware attacks. This prompts a
natural question: Can privacy of sensed data be guaranteed if
the device software is compromised? We refer to this guarantee
as Privacy-from-Birth (PfB).

Some previous results considered potential software com-
promise in low-end devices and proposed methods to enable
security services, such as remote veriﬁcation of device soft-
ware state (remote attestation) [16], [17], [18], [19], [20], [21],
proofs of remote software execution [22], control- & data-
ﬂow attestation [23], [24], [25], [26], [27], [28], [29], as well
as proofs of remote software updates, memory erasure, and
system reset [30], [31], [32].

Regardless of their speciﬁcs, such techniques only detect
of violations or compromises after the fact. In the context
of PfB, that is too late since leakage of private sensed data
likely already occurred. Notably, SANCUS [17] speciﬁcally

 
 
 
 
 
 
typically running at 1-16MHz clock frequencies, with ≈ 64KB
of addressable memory.

Figure 1 illustrates a generic architecture representing such
MCUs. The CPU core and the Direct-Memory Access (DMA)
controller access memory through a bus.1 Memory can be
divided into 5 logical regions: (1) Read-only memory (ROM),
if present, stores critical software such as a bootloader,
burnt into the device at manufacture time and not modiﬁable
thereafter; (2) program memory (PMEM), usually realized
as ﬂash, is non-volatile and stores program instructions; (3)
interrupt vector table (also in ﬂash and often considered as part
of PMEM), stores interrupt conﬁgurations; (4) data memory
(DMEM), usually implemented with DRAM, is volatile and
used to store program execution state, i.e., its stack and heap;
and, (5) peripheral memory region (also in DRAM and often
considered as a part of DMEM), contains memory-mapped
I/O interfaces, i.e., addresses in the memory layout that are
mapped to hardware components, e.g., timers, UART, and
GPIO. In particular, GPIO are peripheral memory addresses
hardwired to physical ports that interface with external circuits,
e.g., analog sensors/circuits.

We note that small MCUs usually come in one of two
memory architectures: Harvard and von Neumann. The former
isolates PMEM and DMEM by maintaining two different
buses and address spaces, while the latter keeps both PMEM
and DMEM in the same address space and accessible via a
single bus.

Low-end MCUs execute instructions in place, i.e., directly
from ﬂash memory. They have neither memory management
units (MMUs) to support virtualization/isolation, nor memory
protection units (MPUs). Therefore, privilege levels and isola-
tion used in higher-end devices and generic enclaved execution
systems (e.g., Intel SGX [34] or MIT SANCTUM [35]) are
not applicable.

We believe that a PfB-agile architecture that is sufﬁciently
inexpensive and efﬁcient for such low-end devices can be
later adapted to more powerful devices. Whereas, going in the
opposite direction is more challenging. Furthermore, simpler
devices are easier to model and reason about formally. Thus,
we believe that they represent a natural starting point for the
design and veriﬁcation of a PfB-agile architecture. To this end,
our prototype implementation of VERSA is integrated with
MSP430, due in part to public availability of an open-source
MSP430 hardware design from OpenCores [36].2

B. GPIO & MCU Sensing

A GPIO port is a set of GPIO pins arranged and controlled
together, as a group. The MCU-addressable memory for a
GPIO port is physically mapped (hard-wired) to physical ports
that can be connected to a variety of external circuits, such as
analog sensors and actuators, as shown in Figure 1. Each GPIO

1DMA is a hardware controller that can read/write to memory in parallel

with the CPU.

2Nevertheless, the generic machine model and methodology of VERSA
are applicable to other low-end MCUs of the same class, e.g., Atmel AVR
ATmega.

Fig. 1. System Architecture of an MCU-based IoT Device

discusses the problem of access control to sensor peripherals
(e.g., GPIO) and proposes attestation of software accessing
(or controlling access to) these peripherals. However,
this
only allows detection of compromised peripheral-accessing
software and does not prevent illegal peripheral access.

To bridge this gap and obtain PfB, we construct the Veriﬁed
Remote Sensing Authorization (VERSA) architecture. It prov-
ably prevents leakage of private sensor data even when the
underlying device is software-compromised. At a high level,
VERSA combines three key features: (1) Mandatory Sensing
Operation Authorization, (2) Atomic Sensing Operation Execu-
tion, and (3) Data Erasure on Boot (see Section III). To attain
these features, VERSA implements a minimal and formally
veriﬁed hardware monitor that runs independently from (and
in parallel with) the main CPU, without modifying the CPU
core. We show that VERSA is an efﬁcient and inexpensive
means of guaranteeing PfB.

This work makes the following contributions:
• Formulates PfB with a high-level speciﬁcation of require-
ments, followed by a game-based formal deﬁnition of the
PfB goal.

• Constructs VERSA, an architecture that guarantees PfB.
• Implements and deploys VERSA on a commodity low-
end MCU, which demonstrates its cost-effectiveness and
practicality.

• Formally veriﬁes VERSA implementation and proves se-
curity of the overall construction, hence obtaining prov-
able security at both architectural and implementation
levels. VERSA implementation and its computer proofs
are publicly available in [33].

II. PRELIMINARIES

A. Scope & MCU-based devices

This work focuses on low-end CPS/IoT/smart devices with
low computing power and meager resources. These are some
of the smallest and weakest devices based on low-power
single-core MCUs with only a few kilobytes (KB) of program
and data memory. Two prominent examples are Atmel AVR
ATmega and TI MSP430: 8- and 16-bit CPUs, respectively,

2

pin can be set to function as either an input or output, hence
called "general purpose". Input signals produced by external
circuits can be obtained by the MCU software by reading from
GPIO-mapped memory. Similarly, egress electric signals (high
or low voltage) can be generated by the MCU software by
writing (logical 1 or 0) to GPIO-mapped memory.

Remark: “GPIO-mapped memory” includes the set of all
software-readable memory regions connected to external sen-
sors. In some cases, this set may even include multiple physical
memory regions for a single physical pin. For instance, if a
given GPIO pin is also equipped with an Analog-to-Digital
Converter (ADC), a GPIO input could be reﬂected on different
memory regions depending on whether the ADC is active
or inactive. All such regions are considered “GPIO-mapped
memory” and we refer to it simply as GPIO. Using this
deﬁnition, in order to access sensor data, software running
on the MCU must read from GPIO.

We also note that various applications require different
sensor regimes [37]: event-driven, periodic, and on-demand.
Event-driven sensors report sensed data when a trigger event
occurs, while periodic sensors report sensor data at ﬁxed
time intervals. On-demand (or query-driven) sensors report
sensor data whenever requested by an external entity. Although
we initially consider on-demand sensing, as discussed in
Section III, the proposed design is applicable to other regimes.

C. Remote Attestation & VRASED

Remote Attestation (RA) allows a trusted entity (veriﬁer
= Vrf) to remotely measure current memory contents (e.g.,
software binaries) of an untrusted embedded device (prover =
Prv). RA is usually realized as a challenge-response protocol:
1) Vrf sends an attestation request containing a challenge

(Chal) to Prv.

2) Prv receives the request and computes an authenticated
integrity check over its memory and Chal. The memory
region can be either pre-deﬁned or explicitly speciﬁed in
the attestation request.

3) Prv returns the result to Vrf.
4) Vrf veriﬁes the result and decides if it corresponds to a

valid Prv state.

VRASED [18] is a veriﬁed hybrid (hardware/software) RA
architecture for for low-end MCUs. It comprises a set of
(individually) veriﬁed hardware and software sub-modules;
their composition provably satisﬁes formal deﬁnitions of RA
soundness and security. VRASED software component imple-
ments the authenticated integrity function computed over a
given “Attested Region” (AR) of Prv’s memory. VRASED
its software counterpart
hardware component assures that
executes securely and that no function of the secret key is ever
leaked. In short, RA soundness states that the integrity mea-
surement must accurately reﬂect a snapshot of Prv’s memory
in AR, disallowing any modiﬁcations to AR during the actual
measurement. RA security deﬁnes that the measurement must
be unforgeable, implying protection of secret key K used for
the measurement.

In order to prevent DoS attacks on Prv, the RA protocol
may involve authentication of the attestation request, before
Prv performs attestation. If this feature is used, an authen-
tication token must accompany every attestation request.3
For example, in VRASED, Vrf computes this token as an
HMAC over Chal, using K. Since K is only known to Prv
and Vrf, this token is unforgeable. To prevent replays, Chal
is a monotonically increasing counter, and the latest Chal
used to successfully authenticate Vrf
is stored by Prv in
persistent and protected memory. In each attestation request,
incoming Chal must be greater that the stored value. Once
an attestation request is successfully authenticated, the stored
value is updated accordingly.

VRASED software component is stored in ROM and realized
with a formally veriﬁed HMAC implementation from the
HACL* cryptographic library [38], which is used to compute:
H = HM AC(KDF (K, Chal), AR), where KDF (K, Chal)
is a one-time key derived from the received Chal and K using
a key derivation function.

As discussed later in Section VI, in VERSA, VRASED is

used as a means of authorizing a binary to access GPIO.

D. LTL, Model Checking, & Veriﬁcation

Our veriﬁcation and proof methodologies are in-line with
prior work on the design and veriﬁcation of security architec-
tures proving code integrity and execution properties for the
same class of MCUs [18], [22], [39], [40]. However, to the
best of our knowledge, no prior work tackled formal models
and deﬁnitions, or designed services, for guaranteed sensed
data privacy. This section overviews our veriﬁcation and proof
methodologies that allow us to later show that VERSA achieves
required PfB properties and end-goals.

Computer-aided formal veriﬁcation typically involves three
steps. First, the system of interest (e.g., hardware, software, or
communication protocol) is described using a formal model,
e.g., a Finite State Machine (FSM). Second, properties that
the model should satisfy are formally speciﬁed. Third, the
system model is checked against formally speciﬁed properties
to guarantee that the system retains them. This can be done
via Theorem Proving [41] or Model Checking [42]. We use
the latter to verify the implementation of system sub-modules,
and the former to prove new properties derived from the
combination (conjunction) of machine model axioms and
sub-properties that were proved for the implementation of
individual sub-modules.

In one instantiation of model checking, properties are spec-
iﬁed as formulae using Linear Temporal Logic (LTL) and
system models are represented as FSMs. Hence, a system is
represented by a triple: (σ, σ0, T ), where σ is the ﬁnite set of
states, σ0 ⊆ σ is the set of possible initial states, and T ⊆ σ×σ
is the transition relation set, which describes the set of states
that can be reached in a single step from each state. Such usage
of LTL allows representing a system behavior over time.

3By saying “this feature is used”, we mean that its usage (or lack thereof) is
ﬁxed at the granularity of a Vrf-Prv setting, and not per single RA instance.

3

Our veriﬁcation strategy beneﬁts from the popular model
checker NuSMV [43], which can verify generic hardware or
software models. For digital hardware described at Register
Transfer Level (RTL) – which is the case in this work – conver-
sion from Hardware Description Language (HDL) to NuSMV
models is simple. Furthermore, it can be automated [44] as the
standard RTL design already relies on describing hardware as
FSMs. LTL speciﬁcations are particularly useful for verifying
sequential systems. In addition to propositional connectives,
such as conjunction (∧), disjunction (∨), negation (¬), and
implication (→), LTL extends propositional logic with tem-
poral quantiﬁers, thus enabling sequential reasoning. In this
paper, we are interested in the following LTL quantiﬁers:

• Xφ – neXt φ: holds if φ is true at the next system state.
• Gφ – Globally φ: holds if for all future states φ is true.
• φ U ψ – φ Until ψ: holds if there is a future state where ψ holds
and φ holds for all states prior to that.
• φ W ψ – φ Weak until ψ: holds if, assuming a future state where
ψ holds, φ holds for all states prior to that. If ψ never becomes true,
φ must hold forever. Or, more formally: φ W ψ ≡ (φ U ψ)∨ G(φ).
• φ B ψ – φ Before ψ: holds if the existence of state where ψ
holds implies the existence of at least one earlier state where φ holds.
Equivalently: φ B ψ ≡ ¬(¬φ U ψ).

NuSMV works by exhaustively enumerating all possible
states of a given system FSM and by checking each state
against LTL speciﬁcations. If any desired speciﬁcation is found
not to hold for speciﬁc states (or transitions between states),
the model checker provides a trace that leads to the erroneous
state, which helps correct the implementation accordingly. As
a consequence of exhaustive enumeration, proofs for complex
systems that involve complex properties often do not scale
due to the so-called “state explosion” problem. To cope with
it, our veriﬁcation approach is to specify smaller LTL sub-
properties separately and verify each respective hardware
sub-module for compliance. In this process, our veriﬁcation
pipeline automatically converts digital hardware, described at
RTL using Verilog, to Symbolic Model Veriﬁer (SMV) [45]
FSMs using Verilog2SMV [44]. The SMV representation is
then fed to NuSMV for veriﬁcation. Then, the composition
of LTL sub-properties (veriﬁed in the model-checking phase)
is proven to achieve a desired end-to-end implementation
goal, also speciﬁed in LTL. This step uses an LTL theorem
prover [46].

In our case, we show that the end-to-end goal of VERSA,
in composition with VRASED, is sufﬁcient to achieve PfB via
cryptographic reduction from the formal security deﬁnition of
VRASED. These steps are discussed in detail in Section VII.

III. VERSA OVERVIEW

VERSA involves two entities: a trusted remote controller
(Ctrl) and a device (Dev). We expect Ctrl to be a relatively
powerful computing entity, e.g., a home gateway, a backend
server or even a smartphone. VERSA protects sensed data on
Dev by keeping it (and any function thereof) conﬁdential. This
implies: (1) controlling GPIO access by blocking attempted
reads by unauthorized software, and (2) keeping execution
traces (i.e., data allocated by GPIO-authorized software) conﬁ-

dential. Therefore, access to GPIO is barred by default. GPIO
is unlocked only for benign binaries that are pre-authorized
by Ctrl. Whenever a binary is deemed to be authorized on
Dev, VERSA creates for it an ephemeral isolated execution
environment and permits its one-time execution. This isolated
environment lasts until execution ends, which corresponds to
reaching the legal exit point of the authorized binary. There-
fore, by including a clean-up routine immediately before the
legal exit, we can assure that all execution traces, including all
sensitive information, are erased. Any attempt to interrupt, or
tamper with, isolated execution causes an immediate system-
wide reset, which erases all data traces.

We use the term “Sensing Operation”, denoted by S, to
refer to a self-contained and logically independent binary (e.g.,
a function) that is responsible for processing data obtained
through one or more reads from GPIO.

VERSA achieves PfB via three key features:

[A] Mandatory Sensing Operation Authorization requires ex-
plicit authorization issued by Ctrl before any Dev software
reads from GPIO. Recall that access to GPIO is blocked
by default. Each authorization token (ATok) coming from
Ctrl allows one execution of a speciﬁc sensing operation S,
although a single execution of S can implement several GPIO
reads. ATok has the following properties:

1) It can be authenticated by Dev as having been issued by

Ctrl; this includes freshness;

2) It grants privileges only to a speciﬁc S to access GPIO

during its execution; and
3) It can only be used once.

Ctrl can authorize multiple executions of S by issuing a batch
of tokens, i.e., ATok1, ..., ATokn, for up to n executions of S.
Although supporting multiple tokens is unnecessary for on-
demand sensing, it might be useful for periodic or event-driven
sensing regimes discussed in Section II-B.
[B] Atomic Sensing Operation Execution ensures that, once
authorized by Ctrl, S is executed with the following require-
ments:

1) S execution starts from its legal entry point (ﬁrst instruc-
tion) and runs until its legal exit point (last instruction).
This assumes a single pair of entry-exit points;

2) S execution can not be interrupted and its intermediate
results cannot be accessed by external means, e.g., via
DMA controllers; and

3) An immediate MCU reset is triggered if either (1) or (2)

above is violated.

[C] Data Erasure on Reset/Boot works with [B] to guarantee
that, sensed data (or any function thereof) obtained during S
execution is not leaked due to errors or violations of security
properties, which cause MCU reset per item (3) above. This
feature must guarantee that all values that remain in RAM after
a hard reset and the subsequent boot process, are erased before
any unprivileged software can run. While some architectures
already provide memory erasure on boot, for those MCUs that
do not do so, it can be obtained by calling a secure RAM
erasure function at boot time, e.g., as a part of a ROM-resident

4

corresponding to one clock cycle – the smallest unit of time in
the system. We say that the subsequent MCU state is deﬁned
based on the current MCU state (which includes current values
in memory/registers, as well as any hardware signals and
effects, such as external inputs, actions by DMA controller(s),
and interrupts) and the current instruction being executed by
the CPU core. Similarly, the instruction to be executed in the
next state is determined by the current state and the current
instruction being executed.

For example, an arithmetic instruction (e.g., add or mult)
causes the program counter (P C) to point to the subsequent
address in physical memory. However, an interrupt (which
is a consequence of the current MCU state) may occur and
deviate the normal execution ﬂow. Alternatively, a branching
instruction may be executed and cause P C to jump to some
the
arbitrary instruction that
subsequent position in the MCU ﬂash memory.

is not necessarily located at

In order to reason about events during the MCU operation,
we say that each MCU state can belong to one or more sets.
Belonging to a given set implies that the state has a given
property of interest. Deﬁnition 1 introduces six sets of interest,
representing states in which memory is read/written by CPU
or DMA, as well as states in which an interrupt or reset occurs.

B. Hardware Signals

We now formalize the effects of execution, modeled in
Deﬁnition 1, to the values of concrete hardware signals that
can be monitored by VERSA hardware in order to attain PfB
guarantees. Informally, we model the following simple axioms:
[A1] PC: contains the memory address containing the instruc-

tion being executed at a given cycle.

[A2] CPU Memory Access: Whenever memory is read or
written, a data-address signal (Daddr) contains the ad-
dress of the corresponding memory location. A data read-
enable bit (Ren) must be set for a read access and a data
write-enable bit (Wen) must be set for a write access.
[A3] DMA: Whenever a DMA controller attempts to access
the main memory, a DMA-address signal (DM Aaddr)
contains the address of the accessed memory location and
a DMA-enable bit (DM Aen) must be set.

[A4] Interrupts: When hardware interrupts or software inter-

rupts happen, the irq signal is set.

[A5] MCU reset: At the end of a successful reset routine, all
registers (including P C) are set to zero before restarting
software execution. The reset handling routine cannot be
modiﬁed, as resets are handled by MCU in hardware.
When a reset happens, the corresponding reset signal is
set. The same signal is also set when the MCU initializes
for the ﬁrst time.

This model strictly adheres to MCU speciﬁcations, assumed
to be correctly implemented by the underlying MCU core.

Deﬁnition 2 presents formal speciﬁcations for aforemen-
tioned axioms in LTL. Instead of explicitly quantifying time,
LTL embeds time within the logic by using temporal quanti-
ﬁers (see Section II). Hence, rather than referring to execution

Fig. 2. MCU execution workﬂow with VERSA.

bootloader code. Appendix B discusses this further.

At a high level, correct implementation of aforementioned

three features sufﬁces to obtain PfB, because:

• Any compromised/modiﬁed binary can not access GPIO

since it has no authorization from Ctrl.

• Any authorized binary S must be invoked properly and
run atomically, from its ﬁrst, and until its last, instruction.
• Since S is invoked properly, intended behavior of S is
preserved. Code reuse attacks are not possible, unless
they occure as a result of bugs in S implementation
itself. Ctrl can always check for such bugs in S prior
to authorization; see Section V-B.

• S runs uninterrupted, meaning that it can erase all traces
of its own execution from the stack before passing control
to unprivileged applications. This guarantees that no
sensor data remains in memory when S terminates.

• VERSA assures that any violation of aforementioned
requirements causes an MCU reset, triggering erasure
of all data memory. Therefore, malware that attempts to
interrupt S before completion, or tamper with S execution
integrity, will cause all data used by S to be erased.
Support for Output Encryption: S might process and use
sensor data locally as part of its own execution, or generate
some output that needs to be returned to Ctrl. In the latter
case, encryption of S output is necessary. For this reason,
VERSA supports the generation of a fresh key derived from
ATok (thus implicitly shared between Ctrl and S). This key
is only accessible to S during authorized execution. Hence, S
can encrypt any data to be exported with this key and ensure
that encrypted results can only be decrypted by Ctrl.

Since we assume that the encryption function is part of S, it
cannot be interrupted (or tampered with) by any unprivileged
software or external means. Importantly, the encryption key
is only accessible to S (similar to GPIO) and shielded from
all other software. Furthermore, the choice of the encryption
algorithm is left up to the speciﬁc S implementation.

Figure 2 illustrates MCU execution workﬂow discussed in

this section.

IV. MCU MACHINE MODEL

A. Execution Model

To enable formal speciﬁcation of PfB guarantees, we for-
mulate the MCU execution model in Deﬁnition 1. It represents
MCU operation as a discrete sequence of MCU states, each

5

Deﬁnition 1 (MCU Execution Model).
1 – Execution is modeled as a sequence of MCU states S := {s0, ..., sm} and a sequence of instructions I := {i0, ..., in}. Since the next MCU state
and the next instruction to be executed are determined by the current MCU state and the current instruction being executed, these discrete transitions
are denoted as shown in the following example:

(s1, ij ) ← EXEC(s0, i0);

(s2, ik) ← EXEC(s1, ij );

...

(sm, ⊥) ← EXEC(sm−1, il)

The sequence I represents the physical order of instructions in memory, which is not necessarily the order of their execution. The next instruction
and state are also affected by current external inputs, current data-memory values, and current hardware events, e.g., interrupts or resets, which are
modeled as properties of each execution state in S. The MCU always starts execution (at boot or after a reset) from state s0 and initial instruction i0.
EXEC produces ⊥ as the next instruction if there is no instructions left to execute.

2 – State Properties as Sets: sets are used to model relevant execution properties and characterize effects/actions occurring within a given state st.
We are particularly interested in the behaviors corresponding to the following sets:

1) READ: all states produced by the execution of an instruction i that reads the value from memory to a register.
2) WRITE: all states produced by the execution of an instruction i that writes the value from a register to memory.
3) DMAR: all states produced as a result of DMA reading from memory.
4) DMAW : all states st produced as a result of DMA writing to memory.
5) IRQ: all states st where an interrupt is triggered.
6) RESET: all states st wherein an MCU reset is triggered.

Note that these sets are not disjoint, i.e., st can belong to multiple sets. Also, the aforementioned sets do not aim to model all possible MCU behaviors,
but only the ones relevant to PfB. Finally, we further subdivide sets that model memory access into subsets relating to memory regions of interest. For
example, considering a contiguous memory region M = [Mmin, Mmax], READM is a subset of READ containing only the states produced through
M, and DMAW
EXEC of instructions that read from the memory region M. We use the same notation to refer to other subsets, e.g., WRITEM, DMAR
M.

Deﬁnition 2 (Hardware Model).
M denotes a contiguous memory region within addresses Mmin and Mmax in physical memory of Dev, i.e., M := [Mmin, Mmax].
s represents the system execution state at a given CPU cycle.

Program counter & instruction execution:

Memory Reads/Writes:

G :{[X(s) ← EXEC(s, ik) ∧ ik ∈ M)] → (P C ∈ M)}

G :{X(s) ∈ READM → (Ren ∧ Daddr ∈ M)}
G :{X(s) ∈ WRITEM → (Wen ∧ Daddr ∈ M)}

G :{(X(s) ∈ DMAR

M ∨ X(s) ∈ DMAW

M) → (DM Aen ∧ DM Aaddr ∈ M)}

Interrupts (irq) and Resets:

G :{s ∈ IRQ ↔ irq}

G :{s ∈ RESET ↔ reset}

(1)

(2)

(3)

(4)

(5)

(6)

states using temporal variables (i.e., state t, state t + 1, state
t + 2), a single variable (s) and LTL quantiﬁers sufﬁce to
specify, e.g., “current”, “next”, “future” system states (s).
For this part of the model, we are mostly interested in: (1)
describing MCU state at the next CPU cycle (X(s)) as a
function of the MCU state at the current CPU cycle (s), and (2)
describing which particular MCU signals must be triggered in
order for X(s) to be in each of the sets deﬁned in Deﬁnition 1.
LTL statements in Deﬁnition 2 formally model axioms [A1]-
[A5], i.e., the subset of MCU behavior that is relevant to,
and sufﬁcient for formally verifying, VERSA. LTL (1) models
[A1], (2) and (3) model [A2], and each (4), (5), and (6) models
[A3], [A4], and [A5], respectively.

V. PfB DEFINITIONS

Based on the speciﬁed machine model, we now proceed

with the formal deﬁnition of PfB.

A. PfB Syntax

A PfB scheme involves two parties: Ctrl and Dev. Ctrl
authorizes Dev to execute some software S which accesses
GPIO. It should be impossible for any software different
from S to access GPIO data, or any function thereof (see

Deﬁnition 4). Ctrl is trusted to only authorize functionally
correct code. The goal of a PfB scheme is to facilitate sensing-
dependent execution while keeping all sensed data private from
all other software.

Deﬁnition 3 speciﬁes a syntax for PfB scheme composed
of three functionalities: Authorize, Verify, and XSensing.
Authorize is invoked by Ctrl to produce an authorization token,
ATok, to be sent to Dev, enabling S to access GPIO. Verify
is executed at Dev with ATok as input, and it checks whether
ATok is a valid authorization for the software on Dev. If and
only if this check succeeds, Verify returns (cid:62). Otherwise, it
returns ⊥. The veriﬁcation success indicates one execution
of S granted on Dev via XSensing. XSensing is considered
successful (returns (cid:62)), if there is at least one MCU state pro-
duced by XSensing where a GPIO read occurs without causing
an MCU reset, i.e., (s ∈ READGP IO) ∧ ¬(s ∈ RESET).
Otherwise, XSensing returns ⊥. That is, XSensing models
execution of any software in the MCU and its return symbol
indicates whether a GPIO read occurred during its execution.
Therefore, invocation of XSensing on any input software that
does not read from GPIO returns ⊥. Figure 3 illustrates a
benign PfB interaction between Ctrl and Dev.

6

Deﬁnition 3 (Syntax: PfB scheme).

A Privacy-from-Birth (PfB) scheme is a tuple of algorithms [Authorize, Verify, XSensing]:

1) AuthorizeCtrl(S, · · · ): an algorithm executed by Ctrl taking as input at least one executable S and producing at least one authorization token

ATok which can be sent to Dev to authorize one execution of S with access to GPIO.

2) VerifyDev(S, ATok, · · · ): an algorithm (with possible hardware-support), executed by Dev, that takes as input S and ATok. It uses ATok to

check whether S is pre-authorized by Ctrl and outputs (cid:62) if veriﬁcation succeeds, and ⊥ otherwise.

3) XSensingDev(S, · · · ): an algorithm (with possible hardware-support) that executes S in Dev, producing a sequence of states E := {s0, ..., sm}.
It returns (cid:62), if sensing successfully occurs during S execution, i.e., ∃s ∈ E such that (s ∈ READGPIO) ∧ (s /∈ RESET)); it returns ⊥, otherwise.

Remark: In the parameter list, (· · · ) means that additional/optional parameters might be included depending on the speciﬁc PfB construction.

Deﬁnition 4 (PfB Game-based Deﬁnition).

4.1 Auxiliary Notation & Predicate(s):

• Let K be a secret string of bit-size |K|; and λ be the security parameter, determined by |K|, i.e., λ = Θ(|K|);
• Let atomicExec be a predicate evaluated on some sequence of states S and some software – i.e., some sequence of instructions I.
– atomicExec(S := {s1, ..., sm}, I := {i0, ..., in}) ≡ (cid:62) if and only if the following hold; otherwise, atomicExec(S, I) ≡⊥.
1) Legal Entry Instruction: The ﬁrst execution state s1 in S is produced by the execution of the ﬁrst instruction i0 in I.

i.e., (s1 ← EXEC(i0, s∗)) ∨ (s1 ∈ RESET), where s∗ is any state prior to s1.

2) Legal Exit Instruction: The last execution state sm in S is produced by the execution of the last instruction in in I.

i.e., (sm ← EXEC(in, sm−1)) ∨ (sm ∈ RESET).

3) Self-Contained Execution: For all sj in S, sj is produced by the execution of an instruction ik in I, for some k.

i.e., (sj ← EXEC(ik, sj−1)) ∨ (sj ∈ RESET), for some ik ∈ I.

4) No Interrupts, No DMA: For all sj in S, sj is neither in the IRQ or DMA. i.e., [(sj /∈ IRQ) ∧ (sj /∈ DMA)] ∨ (sj ∈ RESET).

4.2 PfB-Game: The challenger plays the following game with Adv:

1) Adv is given full control over Dev software state, implying Adv can execute any (polynomially sized) sequence of arbitrary instructions

{iAdv
0

, ..., iAdv

n }, inducing the associated changes in Dev’s sequence of execution states;

2) Adv has oracle access to polynomially many calls to Verify. Adv also has access to the set of software executables, SW := {S1, ..., Sl}, and
the set of all corresponding authorization “tokens”, T := {ATok1, ..., ATokl}, ever produced by any prior Ctrl calls to Authorize up until
time t. i.e., ATokj ← Authorize(Sj , ...), for all j.

3) Let U ⊂ T be the set of all “used” authorization tokens up until time t, i.e., ATokj ∈ U, if a call to XSensing(Sj , ...) returned (cid:62) up until

time t; Let P be the set of “pending” (issued but not used) authorization tokens, i.e., P := T \ U.

4) At any arbitrary time t, Adv wins if it can perform an unauthorized or tampered sensing execution, i.e.:

– Adv triggers an XSensing(SAdv, ...) operation that returns (cid:62), for ∀SAdv /∈ SW, or
– Adv triggers (S, (cid:62)) ← XSensing(Sj , ...) such that atomicExec(S, Sj ) ≡⊥, for some Sj ∈ SW and ATokj ∈ U .

4.3 PfB-Security: A scheme is considered PfB-Secure iff, for all PPT adversaries Adv, there exists a negligible function negl such that:

P r[Adv, PfB-Game] ≤ negl(l)

Ctrl

Dev

(1) Authorization

( 4 ) R e s u l t

( O p t i o n a l )

(2) Verify

(3) Execute

Fig. 3. PfB interaction between Ctrl and Dev

B. Assumptions & Adversarial Model

We consider an adversary, Adv, that controls the entire
software state of Dev, including PMEM (ﬂash) and DMEM
(DRAM). It can attempt
to modify any writable memory
(including PMEM) or read any memory, including peripheral
regions, such as GPIO, unless explicitly protected by veriﬁed
hardware. It can launch code injection attacks to execute
arbitrary instructions from PMEM or even DMEM (if the
MCU architecture supports execution from DMEM). It also
has full control over any DMA controllers on Dev that can
directly read/write to any part of the memory independently
of the CPU. It can induce interrupts to pause any software
execution and leak information from its stack, or change its
control-ﬂow. We consider Denial-of-Service (DoS) attacks,

7

whereby Adv abuses PfB functionality in order to render
Dev unavailable, to be out-of-scope. These are attacks on Dev
availability and not on sensed data privacy.

Executable Correctness: we stress that VERSA aims to
guarantee that S, as speciﬁed by Ctrl, is the only software
that can access and process GPIO data. Similar to other
trusted hardware architectures, PfB does not check for lack
of implementation bugs within S; thus it is not concerned
with run-time (e.g., control-ﬂow and data-only) attacks. As
a relatively powerful and trusted entity Ctrl can use various
well-known vulnerability detection methods, e.g., fuzzing [47],
static analysis [48], and even formal veriﬁcation, to scrutinize
S before authorizing it.

Physical Attacks: physical and hardware-focused attacks
are considered out of scope. We assume that Adv cannot
modify code in ROM,
induce hardware faults, or retrieve
Dev’s secrets via side-channels that require Adv’s physical
presence. Protection against such attacks can be obtained via
standard physical security techniques [49]. This assumption is
in line with related work on trusted hardware architectures for
embedded systems [16], [18], [21], [20].

TABLE I
NOTATION SUMMARY

Notation

Description

Current program counter value
1-bit signal that indicates if MCU is reading from memory
1-bit signal that indicates if MCU is writing to memory
Memory address of an MCU memory access
1-bit signal that indicates if DMA is active

P C
Ren
Wen
Daddr
DM Aen
DM Aaddr Memory address being accessed by DMA, when active
irq
reset
ER

1-bit signal that indicates if an interrupt is happening
Signal that reboots the MCU when set to logic ‘1’
A conﬁgurable memory region where the sensing opera-
tion S is stored, ER = [ERmin, ERmax]
METADATA Metadata memory region; contains ERmin and ERmax
ATok
Fixed memory region from which Verify reads the autho-
rization token when called
Memory region that is mapped to GPIO port
Memory region storing Verify code which instantiates
VRASED software and its hardware protection
A ﬁxed address in ROM , only be reachable (i.e., P C =
iAuth) by a successful Verify call (i.e., Verify returns (cid:62))
(Optional) memory region for the encryption key Kenc
necessary to encrypt the S output (relevant to sensed data)

GPIO
V R

iAuth

eKR

representing read/write from/to a particular memory address
i by either CPU or DMA. For reads/writes from/to some
continuous memory region (composed of multiple addresses)
M = [Mmin, Mmax], we instead say Daddr ∈ M to denote
that Daddr = i ∧ (i ≥ Mmin) ∧ (i ≤ Mmax). The same
holds for notation DM Aaddr ∈ M.

A. VERSA: Construction

Recall the key features of VERSA from Section III. To
guarantee Mandatory Sensing Operation Authorization and
Atomic Sensing Operation Execution, VERSA constructs PfB
= (Authorize, Verify, XSensing) algorithms as in Construc-
tion 1. We describe each algorithm below.

Authorize:
To authorize S, Ctrl picks a monotonically increasing Chal and
generates ATok := HM AC(KDF (Chal, K), S) (this follows
VRASED authentication algorithm – see VERSA Verify speci-
ﬁcation below). ATok is computed over S with a one-time key
derived from K and Chal, where K is the master secret key
shared between Ctrl and Dev.

Verify:
To securely verify that an executable S’, installed in ER,
matches authorized S, Dev invokes VRASED4 to compute σ :=
HM AC(KDF (Chal, K), S (cid:48)). Verify outputs (cid:62), iff σ = ATok.
In this case, P C reaches a ﬁxed address, called iAuth. Other-
wise, Verify outputs ⊥.

In the rest of this section, we use “authorized software" to
refer to software located in ER, for which Verify(ER, ATok)
outputs (cid:62). Whereas, “unauthorized software" refers to any
software for which Verify(ER, ATok) outputs ⊥.

XSensing:

4Dev and Ctrl act as Prv and Vrf in VRASED respectively.

8

Fig. 4. VERSA Architecture

C. PfB Game-based Deﬁnition

Deﬁnition 4 starts by introducing an auxiliary predicate
atomicExec. It deﬁnes whether a particular sequence of ex-
ecution states (produced by the execution of some software
S) adheres to all necessary execution properties for Atomic
Sensing Operation Execution discussed in Section III.

In atomicExec (in Deﬁnition 4.1), conditions 1-3 guarantee
that a given S is executed as a whole and no external
instruction is executed between its ﬁrst and last instructions.
Condition 4 assures that DMA is inactive during execution,
hence protecting intermediate variables in DMEM against
DMA tampering. Additionally, malicious interrupts could be
leveraged to illegally change the control-ﬂow of S during its
execution. Therefore, condition 4 stipulates that both cases
cause atomicExec to return ⊥.

PfB-Game in Deﬁnition 4.2 models Adv’s capabilities by
allowing it to execute any sequence of (polynomially many)
instructions. This models Adv’s full control over software
executed on the MCU, as well as its ability to use software to
modify memory at will. It can also call Verify any (polynomial)
number of times in an attempt to gain an advantage (e.g., learn
something) from Verify executions.

To win the game, Adv must succeed in executing some
software that does not cause an MCU reset, and either: (1) is
unauthorized, yet reads from GPIO, or (2) is authorized, yet
violates atomicExec predicate conditions during its execution.

VI. VERSA: REALIZING PfB

VERSA runs in parallel with the MCU core and monitors
a set of MCU signals: P C, Daddr, Ren, Wen, DM Aen,
DM Aaddr, and irq. It also monitors ERmin and ERmax,
the boundary memory addresses of ER where S is stored;
these are collectively referred to as “METADATA”. VERSA
hardware module detects privacy violations in real-time, based
on aforementioned signals and METADATA values, causing an
immediate MCU reset. Figure 4 shows the VERSA architec-
ture. For quick reference, MCU signals and memory regions
relevant to VERSA are summarized in Table I. To facilitate
speciﬁcation of VERSA properties, we introduce the following
two macros:
Read_M em(i) ≡ (Ren ∧ Daddr = i) ∨ (DM Aen ∧ DM Aaddr = i)
W rite_M em(i) ≡ (Wen ∧ Daddr = i) ∨ (DM Aen ∧ DM Aaddr = i)

Construction 1. VERSA instantiates a PfB = [Authorize, Verify, XSensing] scheme as follows:
– K is a symmetric key pre-shared between Ctrl and VRASED secure architecture in Dev;

1) AuthorizeCtrl(S): Ctrl produces an authorization message M := (S, Chal, ATok), where S is a software, i.e., a sequence of instructions
{i1, ..., in}, that Ctrl wants to execute on Dev; Chal is a monotonically increasing challenge; and ATok is an authentication token computed
as below. Ctrl sends M to Dev. Upon receiving M, Dev is expected to parse M, ﬁnd the memory region for S, and execute Verify (see below).

2) VerifyDev(ER, ATok, Chal): calls VRASED functionality [18] on memory region ER := [ERmin, ERmax] to securely compute:

ATok := HM AC(KDF (Chal, K), S)

If σ = ATok, output (cid:62); Otherwise, output ⊥.

σ := HM AC(KDF (Chal, K), ER)

(7)

(8)

3) XSensingDev(ER): starts execution of software in ER by jumping to ERmin (i.e., setting P C = ERmin). A benign call to XSensing with
input ER is expected to occur after one successful computation of Verify for the same ER region and contents therein. Otherwise, VERSA
hardware support (see below) will cause the MCU to reset when GPIO is read. XSensing produces E := {s0, ..., sm}, the set of states produced
by executing ER, and outputs (cid:62) or ⊥ as follows:
(cid:40)

XSensing(ER) =

(E, (cid:62)),
(E, ⊥),

if ∃s ∈ E such that (s ∈ READGPIO) ∧ (s /∈ RESET)
otherwise

4) HardwareMonitor: At all times, VERSA veriﬁed hardware enforces all following LTL properties :

A – Read-Access Control to GPIO:

G : {(Read_M em(GPIO) ∧ ¬(P C ∈ ER)) → reset}

G : {[(P C = ERmax) ∨ reset] → (¬Read_M em(GPIO) ∨ reset) W (P C = iAuth)}

B – Ephemeral Immutability of ER and METADATA

G : {(P C = iAuth) ∧ (W rite_M em(ER) ∨ W rite_M em(METADATA)) → reset}

G : {((W rite_M em(ER) ∨ W rite_M em(METADATA) → (¬Read_M em(GPIO) ∨ reset) W (P C = iAuth))}

[Optional] G : {((W rite_M em(ER) ∨ W rite_M em(METADATA) → (¬Read_M em(eKR) ∨ reset) W (P C = iAuth))}

C – Atomicity and Controlled Invocation of ER:

G : {¬reset ∧ (P C ∈ ER) ∧ ¬X(P C ∈ ER) → (P C = ERmax) ∨ X(reset)}

G : {¬reset ∧ ¬(P C ∈ ER) ∧ X(P C ∈ ER) → X(P C = ERmin) ∨ X(reset)}

G : {(P C ∈ ER) ∧ (irq ∨ DM Aen) → reset}

[Optional] Read/Write-Access Control to Encryption Key (Kenc) in eKR:

G : {(Read_M em(eKR) ∧ ¬(P C ∈ ER)) → reset}

G : {[(P C = ERmax) ∨ reset] → (¬Read_M em(eKR) ∨ reset) W (P C = iAuth)}

G : {[W rite_M em(eKR) ∧ ¬(P C ∈ V R)] → reset}

Remark: [Optional] properties are needed only if support for encryption of outputs is desired.

Fig. 5. Veriﬁed Remote Sensing Authorization (VERSA) Scheme

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

When XSensing (ER) is invoked, P C jumps to ERmin, and
starts executing the code in ER. It produces a set E of states
by executing ER, and outputs (cid:62), if there is at least one state
that reads GPIO without triggering an MCU reset. Otherwise,
it outputs ⊥.

HardwareMonitor:
VERSA HardwareMonitor is veriﬁed to enforce LTL speci-

ﬁcations (10)–(20) in Construction 1.

A – Read-Access Control to GPIO is jointly speciﬁed by
LTLs (10) and (11). LTL (10) states that GPIO can only be
read during execution of ER (P C ∈ ER), requiring an MCU
reset otherwise. LTL (11) forbids all GPIO reads (even those
within ER execution) before successful computation of Verify
on ER binary using a valid ATok. Successful Verify computa-
tion is captured by condition P C = iAuth. A new successful

computation of Verify(ER, ATok) is necessary whenever ER
execution completes (P C = ERmax) or after reset/boot.
Hence, each legitimate ATok can be used to authorize ER
execution once.

B – Ephemeral Immutability of ER and METADATA is
speciﬁed by LTLs (12)-(14). From the time when ER binary is
authorized until it starts executing, no modiﬁcations to ER or
METADATA are allowed. LTL (12) speciﬁes that no such mod-
iﬁcation is allowed at the moment when veriﬁcation succeeds
(P C = iAuth); LTL (13) requires ER to be re-authorized
from scratch if ER or METADATA are ever modiﬁed. When-
ever these modiﬁcations are detected (W rite_M em(ER) ∨
W rite_M em(METADATA)) further reads to GPIO are im-
mediately blocked (¬Read_M em(GPIO) ∨ reset) until sub-
sequent re-authorization of ER is completed (... W (P C =
iAuth)). LTL (14) speciﬁes the same requirement in order to

9

read VERSA-provided encryption key (Kenc) which is stored
in memory region eKR. This property is only required when
support for encryption of outputs is desired.

C – Atomicity & Controlled Invocation of ER are
enforced by LTLs (15), (16), and (17). They specify that
ER execution must start at ERmin and end at ERmax.
Speciﬁcally, they use the relation between current and next
P C values. The only legal P C transition from currently
outside of ER to next inside ER is via P C = ERmin.
Similarly, the only legal P C transition from currently inside
ER to next outside ER is via P C = ERmax. All other
cases trigger an MCU reset. In addition, LTL (17) requires an
MCU reset whenever interrupts or DMA activity is detected
during ER execution. This is done by simply checking irq
and DM Aen signals.

We note that XSensing relies on the HardwareMonitor to
reset the MCU when violations to ER atomic execution are
detected. Upon reset all data is erased from memory. However,
when execution of S completes successfully VERSA does not
trigger resets. In this case, S is responsible for erasing its own
stack before completion (reaching of ER its last instruction).
We discuss how this self-clean-up routine can be implemented
as a part of S behavior in Appendix A.

B. Encryption & Integrity of ER Output

As mentioned in Section III, after reading and processing
GPIO inputs, S might need to encrypt and send the result
to Ctrl. VERSA supports encryption of this output, regardless
of the underlying encryption scheme. For that purpose, Verify
implementation derives a fresh one-time encryption key (Kenc)
from K and Chal. To assure conﬁdentiality of Kenc, the fol-
lowing properties are required for the memory region (eKR)
reserved to store Kenc:

1) eKR is writable only by Verify (i.e., P C ∈ V R); and
2) eKR is readable only by ER after authorization.

LTLs (18)-(20) and (14) specify the conﬁdentiality require-
ments of Kenc. In sum, these properties establish the same read
access-control policy for eKR and GPIO regions. Therefore,
only authorized S is able to retrieve Kenc.

VII. VERIFIED IMPLEMENTATION & SECURITY ANALYSIS

A. Sub-module Implementation & Veriﬁcation

VERSA sub-modules are represented as FSMs and individ-
ually veriﬁed to hold for LTL properties from Construction 1.
They are implemented in Verilog HDL as Mealy machines,
i.e., their output is determined by both their current state and
current inputs. Each FSM has a single output: a local reset.
VERSA global output reset is given by the disjunction (logic
OR) of all local reset-s. For simplicity, instead of explicitly
representing the output reset value for each state, we use the
following convention:

1) reset is 1 whenever an FSM transitions to RESET state;
2) reset remains 1 while on RESET state;
3) reset is 0 otherwise.

Fig. 6. Veriﬁed FSM for GPIO and eKR Read-Access Control (LTL (10)-
(14) & LTL (18)-(19))

Fig. 7. Veriﬁed FSM for eKR Write-Access Control (LTL (20))

Note that all FSMs remain in RESET state until P C = 0
which indicates that the MCU reset routine ﬁnished.

Fig. 6 illustrates the VERSA sub-module that implements
read-access control to GPIO and eKR (when applicable).
It guarantees that such reads are only possible when they
emanate from execution of authorized software S contained
in ER. It also assures that no modiﬁcations to ER or META-
DATA occur between authorization of S and its subsequent
execution. The Verilog implementation of this FSM is formally
veriﬁed to adhere to LTLs (10)-(14) and (18)-(19). It has 3
states: (1) rLOCK, when reads to GPIO (and possibly eKR)
are disallowed; (2) rU N LOCK, when such reads are allowed
to ER; and (3) RESET . The initial state (after reset or boot)
is RESET , and it switches to rLOCK state when P C = 0.
It switches to rU N LOCK when P C = iAuth (with no reads
to GPIO and eKR), indicating that Verify was successful.
Note that rU N LOCK transitions to RESET when reads
are attempted from outside ER, thus preventing reads by any
unauthorized software. Once P C reaches ERmax, indicating
that ER execution has ﬁnished, the FSM transitions back to
rLOCK. Also, any attempted modiﬁcations to METADATA
or ER in rU N LOCK state bring the FSM back to rLOCK.
Note that rU N LOCK is only reachable after authorization
of ER, i.e., P C = iAuth.

The FSM in Figure 7 enforces LTL (20) to protect eKR
from external writes. It has two states: (1) wU N LOCK, when
writes to eKR are allowed; and (2) RESET . At boot/after
reset (P C = 0),
this FSM transitions from RESET to
wU N LOCK. It transitions back to RESET state whenever

10

Hence, breaking VERSA requires either violating VERSA
veriﬁed guarantees or breaking VRASED veriﬁed guarantees,
which should be infeasible to any PPT Adv.

Appendix C includes proofs for Theorems 1, 2, and 3,
in accordance to this proof strategy. The rest of this section
focuses on VERSA end-to-end implementation goals captured
by LTLs in Deﬁnitions 5 and 6 as well as their relation to
VERSA high-level features discussed in Section III.

Theorem 1. Deﬁnition 2 ∧ LTL 15, 16, 17 → Deﬁnition 5
Theorem 2. Deﬁnition 2 ∧ LTL 10, 11, 12, 13, 16, 20 → Deﬁnition 6

Theorem 3. VERSA is secure according to the PfB-game in Deﬁni-
tion 4, as long as VRASED is a secure RA architecture according to
VRASED security game from [18].

[Deﬁnition 5] states that it globally (always) holds that ER
is atomically executed with controlled invocation. That is,
whenever an instruction in ER executes (P C ∈ ER), it
keeps executing instructions within ER (P C ∈ ER), with
no interrupts and no DMA enabled, until P C reaches the last
instruction in ERmax or an MCU reset occurs. Also, if an
instruction in ER starts to execute, it always begins with the
ﬁrst instruction in ERmin. This formally speciﬁes the Atomic
Sensing Operation Execution feature discussed in Section III.

[Deﬁnition 6] globally requires that whenever GPIO is suc-
cessfully read (i.e., without a reset), this read must come
from the CPU while ER is being executed. In addition, before
this read operation, the following must have happened at least
once:
(1) Verify succeeded (i.e., P C = iAuth);
(2) From the time when P C = iAuth until ER starts
executing (i.e., P C = ERmin), no modiﬁcation to ER
and METADATA occurred; and

(3) If there was any write to eKR from the time when
P C = iAuth, until P C = ERmin, it must have been
from Verify, i.e., while P C ∈ V R.

This formally speciﬁes the intended behavior of the Mandatory
Sensing Operation Authorization feature, discussed in Sec-
tion III.

VIII. EVALUATION & DISCUSSION

In this section, we discuss VERSA implementation details
and evaluation. VERSA source code and veriﬁcation/proofs are
publicly available at [33]. Evaluation of veriﬁcation costs and
discussion of VERSA limitations are deferred to Appendix D.

A. Toolchain & Prototype Details

VERSA is built atop OpenMSP430 [36]: an open source
implementation of TI-MSP430 [50]. We use Xilinx Vivado to
synthesize an RTL description of HardwareMonitor and de-
ploy it on Diligent Basys3 prototyping board for Artix7 FPGA.
For the software part (mostly to implement Verify), VERSA
extends VRASED software (which computes HM AC over
Dev memory) to include a comparison with the received ATok
(See Section VIII-C for extension details). We use the NuSMV

Fig. 8. ER Atomicity and Controlled Invocation FSM (LTL (15)-(17))

writes to eKR are attempted, unless these writes come from
Verify execution (P C ∈ V R).

Figure 8 shows the FSM veriﬁed to enforce ER atomicity
and controlled invocation: LTLs (15)-(17). It has ﬁve states;
notER and midER correspond to P C being outside and
within ER (not including ERmin and ERmax), respectively.
f irstER and lastER are states in which P C points to
ERmin and ERmax, respectively. The only path from notER
to midER is via f irstER. Likewise, the only path from
midER to notER is via lastER. The FSM transitions to
RESET whenever P C transitions do not follow aforemen-
tioned paths. It also transitions to RESET (from any state
other than notER) if irq or DM Aen signals are set.

B. Sub-module Composition and VERSA End-To-End Security

To demonstrate security of VERSA according to Deﬁni-

tion 4, our strategy is two-pronged:
A) We show that LTL properties from Construction 1 are
sufﬁcient to imply that GPIO (and eKR) is only readable
by S and any XSensing operation that returns (cid:62) (i.e.,
performs sensing) is executed atomically. The former
is formally speciﬁed in Deﬁnition 6, and the latter in
Deﬁnition 5. For this part, we write an LTL computer
proof using SPOT LTL proof assistant [46].

B) We use a cryptographic reduction to show that, as long
as item A holds, VRASED security can be reduced to
VERSA security according to Deﬁnition 4.

The intuition for this strategy is that, to win PfB-game in
Deﬁnition 4, Adv must either break the atomicity of XSensing
(which is in direct conﬂict with Deﬁnition 5) or execute
XSensing with unauthorized software and read GPIO without
causing an MCU reset. Deﬁnition 6 guarantees that the latter
is not possible without a prior successful call to Verify. On
the other hand, Verify is implemented using VRASED veriﬁed
architecture, which guarantees the unforgeability of ATok.

11

Deﬁnition 5. Atomic Sensing Operation Execution:

G{ (P C ∈ ER) → [(P C ∈ ER) ∧ ¬irq ∧ ¬DM Aen] W [(P C = ERmax) ∨ reset] }

∧ G{ ¬reset ∧ ¬(P C ∈ ER) ∧ X(P C ∈ ER) → X(P C = ERmin) ∨ X(reset) }

Deﬁnition 6. Mandatory Sensing Operation Authorization:

G{ (Read_M em(GPIO) ∧ ¬reset) → (P C ∈ ER) } ∧
(cid:110)

(P C = iAuth) ∧ (cid:8)(P C = iAuth) → [¬W rite_M em(ER) ∧ ¬W rite_M em(METADATA) ∧ (W rite_M em(eKR) → (P C ∈ V R))] U (P C = ERmin)(cid:9)
B (cid:8)Read_M em(GPIO) ∧ ¬reset(cid:9)

(cid:111)

due to Verify computation which instantiates VRASED. This
runtime includes: (1) time to compute σ from equation 8; (2)
time to check if σ = ATok; and (3) time to write Kenc to eKR,
when applicable. Naturally the runtime overhead is dominated
by the computation in (1) which is proportional to the size of
ER.

We measure Verify cost on three sample applications: (1)
Simple Application, which reads 32-bytes of GPIO input and
encrypts it using One-Time-Pad (OTP) with Kenc; (2) Motion
Sensor – available at [51] – which continuously reads GPIO
input to detect movements and actuates a light source when
movement is detected; and (3) Temperature Sensor – adapted
code from [52] to support encryption of its outputs – which
reads ambient temperature via GPIO and encrypts this reading
using OTP. We prototype using OTP for encryption for the
sake of simplicity noting that VERSA does not mandate a
particular encryption scheme. All of these sample applications
also include a self-clean-up code executed immediately before
reaching their exit point to erase their stack traces once their
execution is over.

Figure 10 shows Verify runtimes on these applications.
Assuming a clock frequency of 10MHz (a common frequency
for low-end MCUs), Verify runtime ranges from 100 − 200
milliseconds for these applications. The overhead is linear on
the binary size.

D. Comparison with Other Low-End Architectures:

To the best of our knowledge, VERSA is the ﬁrst architecture
related to PfB. However, to provide a point of reference
in terms of performance and overhead, we compare VERSA
with other low-end trusted hardware architectures, such as
SMART [16], VRASED [18], APEX [22], and SANCUS [17].
All these architectures provide RA-related services to attest
integrity of software on Dev either statically or at runtime.
Since PfB also checks software integrity before granting
access to GPIO, we consider these architectures to be related
to VERSA.

Figure 9 compares VERSA hardware overhead with the
aforementioned architectures in terms of additional LUTs and
registers. Percentages are relative to the plain MSP430 core
total cost.

VERSA builds on top of VRASED. As such,

is nat-
urally more expensive than hybrid RA architectures such
as SMART and VRASED. Similar to VERSA, APEX also
monitors execution properties and also builds on top of RA
(in APEX case, with the goal of producing proofs of remote

it

(a) Additional HW overhead (%) in
Number of Look-Up Tables
Fig. 9. Hardware overhead comparisons with other low-end security archi-
tectures.

(b) Additional HW overhead (%) in
Number of Registers

Fig. 10. Runtime overhead of VERSA due to Verify

model checker to formally verify that HardwareMonitor im-
plementation adheres to LTL speciﬁcations (10)-(20). See
Appendix D for details on the veriﬁcation setup and costs.

B. Hardware Overhead

Table II reports on VERSA hardware overhead, as compared
to unmodiﬁed OpenMSP430 and VRASED. Similar to other
schemes [18], [22], [17], [16], we consider hardware overhead
in terms of additional Look-Up Tables (LUTs) and registers.
Extra hardware in terms of LUTs gives an estimate of ad-
ditional chip cost and size required for combinatorial logic,
while extra hardware in terms of registers gives an estimate
of memory overhead required by sequential logic in VERSA
FSMs. Compared to VRASED, VERSA requires 10% additional
LUTs and 2% additional registers. In actual numbers, it adds
255 LUTs and 50 registers to the underlying MCU as shown
in Table II.

TABLE II
HARDWARE OVERHEAD AND VERIFICATION COST

Architecture

OpenMSP430
VRASED
VERSA + VRASED

Hardware

LUTs
1854
1891
2109

Regs
692
724
742

Reserved
RAM (bytes)
0
2332
2336

LoC
-
481
1118

Veriﬁcation
Time (s)
-
0.4
13956.4

#(LTLs)
-
10
21

RAM (MB)
-
13.6
1059.1

C. Runtime Overhead

VERSA requires any software piece seeking to access GPIO
(and Kenc) to be veriﬁed. Consequently, runtime overhead is

12

VERSASMARTVRASEDAPEXSANCUS01020304050607080% Additional LUTsVERSASMARTVRASEDAPEXSANCUS020406080100120% Additional Regs150200250300350400450500Binary size (in bytes)1.11.21.31.41.5Runtime (#cycles in millions)Simple App.Motion SensorTemperatureSensorsoftware execution). Therefore, VERSA and APEX exhibit
similar overheads. SANCUS presents a higher cost because
it implements RA and isolation features in hardware.

IX. RELATED WORK

There is a considerable body of work (overviewed in
Section I) on IoT/CPS privacy. However, to the best of our
knowledge, this paper is the ﬁrst effort speciﬁcally targeting
PfB, i.e., sensor data privacy on potentially compromised
MCUs. Nonetheless, prior work has proposed trusted hard-
ware/software co-designs – such as VERSA – offering other
security services. We overview them in this section.

Trusted components, commonly referred to as Roots of
Trust (RoTs), are categorized as software-based, hardware-
based, or hybrid (i.e., based on hardware/software co-designs).
Their usual purpose is to verify software integrity on a given
device. Software-based RoTs [53], [54], [55], [56], [57], [58],
[59] usually do not rely on any hardware modiﬁcations.
However, they are insecure against compromises to the entire
software state of a device (e.g., in cases where Adv can
physically re-program Dev). In addition,
their inability to
securely store cryptographic secrets imposes reliance on strong
assumptions about precise timing and constant communication
delays to enable device authentication. These assumptions can
be unrealistic in the IoT ecosystem. Nonetheless, software-
based RoTs are the only viable choice for legacy devices that
have no security-relevant hardware support. Hardware-based
methods [60], [61], [62], [63], [64], [65], [17] rely on security
provided by dedicated hardware components (e.g., TPM [61]
or ARM TrustZone [66]). However, the cost of such hardware
is normally prohibitive for low-end MCUs. Hybrid RoTs [16],
[22], [18], [20], [21] aim to achieve security equivalent to
hardware-based mechanisms, yet with lower hardware costs.
They leverage minimal hardware support while relying on
software to reduce additional hardware complexity.

Other architectures, such as SANCTUM [35] and Notary
[67], provide strong memory isolation and peripheral isolation
guarantees, respectively. These guarantees are achieved via
hardware support or external hardware agents. They are also
hybrid architectures where trusted hardware works in tandem
with trusted software. However, we note that such schemes
are designed for application computers that support MMUs
and are therefore unsuitable for simple MCUs.

In terms of functionality, such embedded RoTs focus on
integrity. Upon receiving a request from an external trusted
Veriﬁer, they can generate unforgeable proofs for the state
of the MCU or that certain actions were performed by the
MCU. Security services implemented by them include: (1)
memory integrity veriﬁcation, i.e., remote attestation [16],
[17], [18], [19], [20], [21]; (2) veriﬁcation of runtime prop-
erties, including control-ﬂow and data-ﬂow attestation [22],
[23], [24], [25], [26], [27], [28], [29], [68]; and (3) proofs of
remote software updates, memory erasure, and system-wide
resets [30], [31], [32]. As brieﬂy mentioned in Section I, due
to their reactive nature, they can be used to detect whether

Dev has been compromised after the fact, but cannot prevent
the compromised entity from exﬁltrating private sensor data.
VERSA, on the other hand, enforces mandatory authorization
before any sensor data access and thus prevents leakage even
when a compromise has already happened.

Formalization and formal veriﬁcation of RoTs for MCUs
have gained attention due to the beneﬁts discussed in Sec-
tions I and II. VRASED [18] implemented a veriﬁed hybrid
RA scheme. APEX [22] built atop VRASED to implement and
formally verify an architecture for proofs of remote execution
of attested software. PURE [30] implemented provably secure
services for software update, memory erasure, and system-
wide reset. Another recent result [69] formalized and proved
the security of a hardware-assisted mechanism to prevent
leakage of secrets through timing side-channels due to MCU
interrupts. Inline with the aforementioned work, VERSA also
formalizes its assumptions along with its goals and implements
the ﬁrst formally veriﬁed design assuring PfB.

X. CONCLUSIONS

We formulated the notion of Privacy-from-Birth
(PfB) and proposed VERSA: a formally veriﬁed architecture
realizing PfB. VERSA ensures that only duly authorized soft-
ware can access sensed data even if the entire software state
of the sensor is compromised. To attain this, VERSA enhances
the underlying MCU with a small hardware monitor, which is
shown sufﬁcient to achieve PfB. The experimental evaluation
of VERSA publicly available prototype [33] demonstrates its
affordability on a typical low-end IoT MCU: TI MSP430.

ACKNOWLEDGMENTS

The authors sincerely thank S&P’22 reviewers. This work
was supported by funding from NSF Awards SATC-1956393
and CICI-1840197, as well as a subcontract from Peraton
Labs. Part of this work was conducted while the ﬁrst author
was at the University of California, Irvine.

REFERENCES

[1] R. H. Weber, “Internet of things–new security and privacy challenges,”
Computer law & security review, vol. 26, no. 1, pp. 23–30, 2010.
[2] A. Ukil, S. Bandyopadhyay, and A. Pal, “Iot-privacy: To be private or not
to be private,” in 2014 IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS).

IEEE, 2014, pp. 123–124.

[3] H. Lin and N. W. Bergmann, “Iot privacy and security challenges for
smart home environments,” Information, vol. 7, no. 3, p. 44, 2016.
[4] S. Zheng, N. Apthorpe, M. Chetty, and N. Feamster, “User perceptions of
smart home iot privacy,” Proceedings of the ACM on Human-Computer
Interaction, vol. 2, no. CSCW, pp. 1–20, 2018.

[5] P. Porambage, M. Ylianttila, C. Schmitt, P. Kumar, A. Gurtov, and A. V.
Vasilakos, “The quest for privacy in the internet of things,” IEEE Cloud
Computing, vol. 3, no. 2, pp. 36–45, 2016.

[6] A. L. M. Neto, A. L. Souza, I. Cunha, M. Nogueira, I. O. Nunes,
L. Cotta, N. Gentille, A. A. Loureiro, D. F. Aranha, H. K. Patil et al.,
“Aot: Authentication and access control for the entire iot device life-
cycle,” in Proceedings of
the 14th ACM Conference on Embedded
Network Sensor Systems CD-ROM, 2016, pp. 1–15.

[7] S. Kumar, Y. Hu, M. P. Andersen, R. A. Popa, and D. E. Culler, “JEDI:
Many-to-many end-to-end encryption and key delegation for iot,” in 28th
USENIX Security Symposium (USENIX Security 19), 2019, pp. 1519–
1536.

13

[8] R. Trimananda, J. Varmarken, A. Markopoulou, and B. Demsky,
“Packet-level signatures for smart home devices,” in Network and
Distributed Systems Security (NDSS) Symposium, vol. 2020, 2020.
[9] N. Apthorpe, D. Y. Huang, D. Reisman, A. Narayanan, and N. Feamster,
“Keeping the smart home private with smart (er) iot trafﬁc shaping,”
arXiv preprint arXiv:1812.00955, 2018.

[10] N. Apthorpe, D. Reisman, and N. Feamster, “Closing the blinds: Four
strategies for protecting smart home privacy from network observers,”
arXiv preprint arXiv:1705.06809, 2017.

[11] N. J. Apthorpe, D. Reisman, and N. Feamster, “A smart home is
no castle: Privacy vulnerabilities of encrypted iot trafﬁc,” CoRR, vol.
abs/1705.06805, 2017. [Online]. Available: http://arxiv.org/abs/1705.
06805

[12] Y. Cheng, X. Ji, X. Zhou, and W. Xu, “Homespy: Inferring user presence
via encrypted trafﬁc of home surveillance camera.” in ICPADS, 2017,
pp. 779–782.

[13] S. Narain, T. D. Vo-Huu, K. Block, and G. Noubir, “Inferring user
routes and locations using zero-permission mobile sensors,” in 2016
IEEE Symposium on Security and Privacy (SP).
IEEE, 2016, pp. 397–
413.

[14] A. S. A. Sukor, A. Zakaria, N. A. Rahim, L. M. Kamarudin, R. Setchi,
and H. Nishizaki, “A hybrid approach of knowledge-driven and data-
driven reasoning for activity recognition in smart homes,” Journal of
Intelligent & Fuzzy Systems, vol. 36, no. 5, pp. 4177–4188, 2019.
[15] M. Jin, R. Jia, and C. J. Spanos, “Virtual occupancy sensing: Using
smart meters to indicate your presence,” IEEE Transactions on Mobile
Computing, vol. 16, no. 11, pp. 3264–3277, 2017.

[16] K. Eldefrawy, G. Tsudik, A. Francillon, and D. Perito, “SMART: Secure
and minimal architecture for (establishing dynamic) root of trust,” in
NDSS, 2012.

[17] J. Noorman, J. V. Bulck, J. T. Mühlberg, F. Piessens, P. Maene,
B. Preneel, I. Verbauwhede, J. Götzfried, T. Müller, and F. C. Freiling,
“Sancus 2.0: A low-cost security architecture for iot devices,” ACM
Trans. Priv. Secur., vol. 20, no. 3, pp. 7:1–7:33, 2017. [Online].
Available: https://doi.org/10.1145/3079763

[18] I. De Oliveira Nunes, K. Eldefrawy, N. Rattanavipanon, M. Steiner,
and G. Tsudik, “VRASED: A veriﬁed hardware/software co-design for
remote attestation,” in USENIX Security, 2019.

[19] M. Ammar, B. Crispo, and G. Tsudik, “Simple: A remote attestation
approach for resource-constrained iot devices,” in 2020 ACM/IEEE 11th
International Conference on Cyber-Physical Systems (ICCPS).
IEEE,
2020, pp. 247–258.

tiny trust anchor for tiny devices,” in Proceedings of

[20] F. Brasser, B. E. Mahjoub, A. Sadeghi, C. Wachsmann, and P. Koeberl,
the
“Tytan:
52nd Annual Design Automation Conference, San Francisco, CA, USA,
June 7-11, 2015. ACM, 2015, pp. 34:1–34:6. [Online]. Available:
https://doi.org/10.1145/2744769.2744922

[22] I. De Oliveira Nunes, K. Eldefrawy, N. Rattanavipanon,

[21] P. Koeberl, S. Schulz, A.-R. Sadeghi, and V. Varadharajan, “TrustLite:
A security architecture for tiny embedded devices,” in EuroSys, 2014.
and
G. Tsudik, “APEX: A veriﬁed architecture for proofs of execution
in 29th
on remote devices under
compromise,”
USENIX Security Symposium (USENIX Security 20).
Boston,
MA: USENIX Association, Aug. 2020. [Online]. Available: https:
//www.usenix.org/conference/usenixsecurity20/presentation/nunes
[23] G. Dessouky, T. Abera, A. Ibrahim, and A.-R. Sadeghi, “Litehax:
lightweight hardware-assisted attestation of program execution,” in 2018
IEEE/ACM International Conference on Computer-Aided Design (IC-
CAD).

IEEE, 2018, pp. 1–8.

software

full

[24] T. Abera, N. Asokan, L. Davi, J. Ekberg, T. Nyman, A. Paverd,
A. Sadeghi, and G. Tsudik, “C-FLAT: control-ﬂow attestation for
embedded systems software,” in Proceedings of
the 2016 ACM
SIGSAC Conference on Computer and Communications Security,
Vienna, Austria, October 24-28, 2016, E. R. Weippl, S. Katzenbeisser,
C. Kruegel, A. C. Myers, and S. Halevi, Eds. ACM, 2016, pp.
743–754. [Online]. Available: https://doi.org/10.1145/2976749.2978358
[25] G. Dessouky, S. Zeitouni, T. Nyman, A. Paverd, L. Davi, P. Koeberl,
N. Asokan, and A.-R. Sadeghi, “Lo-fat: Low-overhead control ﬂow
attestation in hardware,” in Proceedings of the 54th Annual Design
Automation Conference 2017. ACM, 2017, p. 24.

[26] S. Zeitouni, G. Dessouky, O. Arias, D. Sullivan, A. Ibrahim, Y. Jin,
and A.-R. Sadeghi, “Atrium: Runtime attestation resilient under mem-
ory attacks,” in Proceedings of the 36th International Conference on
Computer-Aided Design.

IEEE Press, 2017, pp. 384–391.

[27] Z. Sun, B. Feng, L. Lu, and S. Jha, “Oat: Attesting operation integrity of
embedded devices,” in 2020 IEEE Symposium on Security and Privacy
(SP).

IEEE, 2020, pp. 1433–1449.

[28] I. De Oliveria Nunes, S. Jakkamsetti, and G. Tsudik, “Tiny-CFA:
Minimalistic control-ﬂow attestation using veriﬁed proofs of execution,”
in Design, Automation and Test in Europe Conference (DATE), 2021.

[29] I. De Oliveira Nunes, S. Jakkamsetti, and G. Tsudik, “Dialed: Data

integrity attestation for low-end embedded devices,” 2021.

[30] I. De Oliveira Nunes, K. Eldefrawy, N. Rattanavipanon, and G. Tsudik,
“Pure: Using veriﬁed remote attestation to obtain proofs of update, reset
and erasure in low-end embedded systems,” 2019.

[31] M. Ammar and B. Crispo, “Verify&revive: Secure detection and recov-
ery of compromised low-end embedded devices,” in Annual Computer
Security Applications Conference, 2020, pp. 717–732.

[32] N. Asokan, T. Nyman, N. Rattanavipanon, A.-R. Sadeghi, and G. Tsudik,
“ASSURED: Architecture for secure software update of realistic em-
bedded devices,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 37, no. 11, 2018.

[33] “VERSA source code,” https://github.com/sprout-uci/pfb, 2022.
[34] Intel, “Intel Software Guard Extensions (Intel SGX),” https://software.

intel.com/en-us/sgx.

[35] V. Costan, I. Lebedev, and S. Devadas, “Sanctum: Minimal hardware
extensions for strong software isolation,” in 25th USENIX Security
Symposium (USENIX Security 16), 2016.

[36] O. Girard, “openMSP430,” 2009. [Online]. Available: https://opencores.

org/projects/openmsp430

[37]

John Wiley & Sons, Ltd, 2010, pp. 295–300. [Online]. Available:

https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470570517

[38] J.-K. Zinzindohoué, K. Bhargavan, J. Protzenko, and B. Beurdouche,
“Hacl*: A veriﬁed modern cryptographic library,” in CCS, 2017.
[39] I. De Oliveira Nunes, S. Jakkamsetti, N. Rattanavipanon, and G. Tsudik,

“On the toctou problem in remote attestation,” CCS, 2021.

[40] E. Aliaj, I. D. O. Nunes, and G. Tsudik, “GAROTA: generalized active
root-of-trust architecture,” CoRR, vol. abs/2102.07014, 2021. [Online].
Available: https://arxiv.org/abs/2102.07014

[41] D. W. Loveland, Automated Theorem Proving: a logical basis. Elsevier,

2016.

[42] E. M. Clarke Jr, O. Grumberg, D. Kroening, D. Peled, and H. Veith,

Model checking. MIT press, 2018.

[43] A. Cimatti, E. Clarke, E. Giunchiglia, F. Giunchiglia, M. Pistore,
M. Roveri, R. Sebastiani, and A. Tacchella, “Nusmv 2: An opensource
tool for symbolic model checking,” in CAV, 2002.

[44] A. Irfan, A. Cimatti, A. Griggio, M. Roveri, and R. Sebastiani, “Ver-
ilog2SMV: A tool for word-level veriﬁcation,” in Design, Automation
& Test in Europe Conference & Exhibition (DATE), 2016, 2016.
[45] K. L. McMillan, “The smv system,” in Symbolic Model Checking.

Springer, 1993, pp. 61–85.

[46] A. Duret-Lutz, A. Lewkowicz, A. Fauchille, T. Michaud, E. Renault, and
L. Xu, “Spot 2.0—a framework for ltl and ω-automata manipulation,”
in International Symposium on Automated Technology for Veriﬁcation
and Analysis, 2016.

[47] J. Chen, W. Diao, Q. Zhao, C. Zuo, Z. Lin, X. Wang, W. C. Lau, M. Sun,
R. Yang, and K. Zhang, “Iotfuzzer: Discovering memory corruptions in
iot through app-based fuzzing.” in NDSS, 2018.

[48] A. Costin, J. Zaddach, A. Francillon, and D. Balzarotti, “A large-scale
analysis of the security of embedded ﬁrmwares,” in 23rd USENIX
Security Symposium (USENIX Security 14), 2014, pp. 95–110.

[49] S. Ravi, A. Raghunathan, and S. Chakradhar, “Tamper resistance mech-

anisms for secure embedded systems,” in VLSI Design, 2004.

[50] T. Instruments. Msp430 ultra-low-power sensing & measurement mcus.
http://www.ti.com/microcontrollers/msp430-ultra-low-power-mcus/
overview.html.

[51] “Motion sensor code,” https://github.com/Seeed-Studio/LaunchPad_Kit/

tree/master/Grove_Modules/pir_motion_sensor.

[52] “Temperature

code,”
LaunchPad_Kit/tree/master/Grove_Modules/temp_humi_sensor.

https://github.com/Seeed-Studio/

sensor

[53] R. Kennell and L. H. Jamieson, “Establishing the genuinity of remote

computer systems,” in USENIX Security Symposium, 2003.

[54] A. Seshadri, A. Perrig, L. Van Doorn, and P. Khosla, “SWATT: Software-
based attestation for embedded devices,” in IEEE Symposium on Re-
search in Security and Privacy (S&P).
Oakland, California, USA:
IEEE, 2004, pp. 272–282.

14

[55] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn, and P. Khosla,
“Pioneer: Verifying code integrity and enforcing untampered code exe-
cution on legacy systems,” in ACM SOSP, 2005.

[56] A. Seshadri, M. Luk, and A. Perrig, “SAKE: Software attestation for

key establishment in sensor networks,” in DCOSS, 2008.

[57] R. W. Gardner, S. Garera, and A. D. Rubin, “Detecting code alteration
by creating a temporary memory bottleneck,” IEEE TIFS, 2009.
[58] Y. Li, J. M. McCune, and A. Perrig, “VIPER: Verifying the integrity of

peripherals’ ﬁrmware,” in ACM CCS, 2011.

[59] V. D. Gligor and S. L. M. Woo, “Establishing software root of trust

unconditionally.” in NDSS, 2019.

[60] N. L. Petroni Jr, T. Fraser, J. Molina, and W. A. Arbaugh, “Copilot
— A coprocessor-based kernel runtime integrity monitor,” in USENIX
Security Symposium, 2004.
[61] Trusted Computing Group.,
2017.
[Online]. Available:
work-groups/trusted-platform-module/

(tpm),”
“Trusted platform module
http://www.trustedcomputinggroup.org/

[62] X. Kovah, C. Kallenberg, C. Weathers, A. Herzog, M. Albin, and J. But-
terworth, “New results for timing-based attestation,” in Proceedings of
the IEEE Symposium on Research in Security and Privacy.
IEEE
Computer Society Press, 2012.

[63] D. Schellekens, B. Wyseur, and B. Preneel, “Remote attestation on
legacy operating systems with trusted platform modules,” Science of
Computer Programming, vol. 74, no. 1, pp. 13 – 22, 2008.

[64] J. M. McCune, B. J. Parno, A. Perrig, M. K. Reiter, and H. Isozaki,
“Flicker: An execution infrastructure for tcb minimization,” in Pro-
ceedings of the 3rd ACM SIGOPS/EuroSys European Conference on
Computer Systems 2008, 2008, pp. 315–328.

[65] J. M. McCune, Y. Li, N. Qu, Z. Zhou, A. Datta, V. Gligor, and A. Perrig,
“TrustVisor: Efﬁcient TCB reduction and attestation,” in IEEE S&P ’10,
2010.
[66] Arm Ltd.,

https://www.arm.com/products/

“Arm TrustZone,”

security-on-arm/trustzone, 2018.

[67] A. Athalye, A. Belay, M. F. Kaashoek, R. Morris, and N. Zeldovich,
“Notary: A device for secure transaction approval,” in Proceedings of the
27th ACM Symposium on Operating Systems Principles, ser. SOSP ’19.
New York, NY, USA: Association for Computing Machinery, 2019, p.
97–113. [Online]. Available: https://doi.org/10.1145/3341301.3359661

[68] M. Geden and K. Rasmussen, “Hardware-assisted remote runtime at-
testation for critical embedded systems,” in 2019 17th International
Conference on Privacy, Security and Trust (PST).
IEEE, 2019, pp.
1–10.

[69] M. Busi, J. Noorman, J. Van Bulck, L. Galletta, P. Degano, J. T.
Mühlberg, and F. Piessens, “Provably secure isolation for interruptible
enclaved execution on small microprocessors,” in 2020 IEEE 33rd
Computer Security Foundations Symposium (CSF).
IEEE, 2020, pp.
262–276.

[70] “Msp430 ﬂash memory characteristics,” https://www.ti.com/lit/an/

slaa334b/slaa334b.pdf?ts=1638460551489, 2018.

[71] “Avr atmega 1284p 8-bit microcontroller,” http://ww1.microchip.com/

downloads/en/DeviceDoc/doc8059.pdf, 2009.

[72] E. Aras, M. Ammar, F. Yang, W. Joosen, and D. Hughes, “Microvault:
Reliable storage unit for iot devices,” in 2020 16th International Con-
ference on Distributed Computing in Sensor Systems (DCOSS), 2020,
pp. 132–140.

A. Clean-up after Program Termination

APPENDIX

While VERSA guarantees the conﬁdentiality of sensing
operations, it requires the authorized executable S to erase
its own stack/heap before its termination. This ensures that
unauthorized software can not extract and leak sensitive in-
formation from S execution and allocated data. Erasure in
this case can be achieved via a single call to libc’s memset
function with start address matching the base of S stack and
size equal to the maximum size reached by S execution.

The maximum stack size can be determined manually by
counting the allocated local variables in small and simple S
implementations. To automatically determine this size in more

15

Fig. 11. Sample sensing operation that reads GPIO input, encrypts it, and
cleans up its stack after execution.

complex S implementations, all functions called within S must
update the highest point reached by their respective stacks.
Figure 11 shows a sample application that reads 32 bytes
of sensor data, encrypts this data using VERSA one-time key
Kenc, and cleans-up the stack thereafter. Line 12 is S entry
point (ERmin). S ﬁrst saves the stack pointer to STACK_MIN
address. Then, the application function is called, in line
17. application implements S intended behavior. After
the application is done, the clean up code (lines 51-53)
is called with STACK_MIN as the start pointer and size of 32
+ 4 bytes (32 bytes for data variable (in line 39) and 4 bytes
for stack metadata).

B. Data Erasure on Reset/Boot

Violations to VERSA properties trigger an MCU reset. A
reset immediately stops execution and prepares the MCU core
to reboot by clearing all registers and pointing the program
counter (P C) to the ﬁrst instruction of PMEM. However, some
MCUs may not guarantee erasure of DMEM as a part of this
process. Therefore, traces of data allocated by S (including
sensor data) could persist across resets.

In MCUs that do not offer DMEM erasure on reset, a
software-base Data Erasure (DE) can be implemented and
invoked it as soon as the MCU starts, i.e., as a part of the
bootloader code. In particular, DE can be implemented using
memset (similar to lines 51-53) with constant arguments
matching the entirety of the MCU’s DMEM.DE should be
immutable (e.g., stored in ROM) which is often the case for
bootloader binaries. Upon reset, P C must always point to the

ﬁrst instruction of DE. The normal MCU start-up proceeds
normally after DE execution is completed.

C. VERSA Composition Proof

In this section, we show that VERSA is a secure PfB
architecture according to Deﬁnition 4, as long as A) the
sub-properties in Construction 1 hold (Theorem 1, 2) and
B) VRASED is a secure remote attestation (RA) architecture
according to the VRASED security deﬁnition in [18] (Theo-
rem 3). Informally, part A) shows that if the machine model
and all LTLs in Construction 1 hold, then the end-to-end
goals for secure PfB architecture are met, while this does not
include the goal of prevention of forging authorization tokens.
Part B) handles the latter using a cryptographic reduction,
i.e., it shows that an adversary able to forge the authorization
token (with more than negligible probability) can also break
VRASED according to the RA-game, which is a contradiction
assuming the security of VRASED. Therefore, Theorems 1-
3 prove that VERSA is a secure PfB architecture as long as
VRASED is a secure RA architecture.

For part A), computer-checked LTL proofs are performed
using SPOT LTL proof assistant [46]. These proofs are
available at [33]. We present the intuition behind them below.

Proof of Theorem 1 (Intuition). LTL (16) states the legal
entry instruction requirement, while LTL (15) states the legal
exit instruction requirement in atomicExec. Also, since LTL
(15) states that ERmax is the only possible exit of the ER
without a reset, it implies self-contained execution of ER.
Lastly, LTL (17) enforces MCU reset
if any interrupt or
DMA occurs, which naturally prevents interrupts and DMA
actions, as required by atomicExec. These imply the LTL
in Deﬁnition 5 which stipulates that execution of ER must
start with ERmin and stays within ER with no interrupts
nor DMA actions, until P C reaches ERmax (causing a reset
otherwise).

Proof of Theorem 2 (Intuition). Deﬁnition 6 (i) requires at least
one successful veriﬁcation of ER before GPIO can be read
successfully (without triggering a reset); and (ii) disallows
modiﬁcations to ER, METADATA, and Kenc (other than by
V R) in between ER veriﬁcation subsequent ER execution.
LTLs (10) and (11) state that P C must be within ER to read
GPIO and disallow GPIO reads by default (including when
MCU reset occurs) and after the execution of ER is over
(P C = ERmax). Also, LTL (11) requires (re-)authorization
(P C = iAuth) of ER after the execution of ER is over
(P C = ERmax). LTL (13) disallows GPIO reads until the
(re-)veriﬁcation whenever ER or METADATA are written.
LTL (12) disallows changes to ER and METADATA at the
exact time when veriﬁcation succeeds. LTL (16) guarantees
that the execution of ER starts with ERmin and LTL (20)
guarantees that only the V R code can modify the value in
eKR. Thus, these are sufﬁcient imply Deﬁnition 6.

For part B), we construct a reduction from the security
game of VRASED in [18] to the security game of VERSA
i.e., The ability to break
according to the Deﬁnition 4.
the PfB-game of VERSA allows to break the RA-game of
VRASED, and therefore, as long as VRASED is a secure RA
architecture according to the RA-game, VERSA is secure
according to the PfB-game.

Proof of Theorem 3. Denote by AdvPfB, an adversary who
can win the security game in Deﬁnition 4 against VERSA with
more than negligible probability. We show that if such AdvPfB
exists, then it can be used to construct AdvRA that wins the
RA-game with more than negligible probability.

Recall that, to win the PfB-game, AdvPfB must trigger (cid:62)
as a result of XSensing, which means it reads the sensed data
without MCU reset. From the PfB-game step 4 in Deﬁnition 4,
it can be done in either of the following two ways:
Case1. AdvPfB executes a new, unauthorized software SAdv
which causes XSensing(SAdv) → (cid:62); or
Case2. AdvPfB breaks the atomic execution of an autho-
rized, but not yet executed software, Sj, so that it causes
XSensing(Sj) → (E, (cid:62)) such that atomicExec(E, Sj) ≡⊥.

Recall that for the instruction set Ij of Sj and a set Ej of
execution states , to have atomicExec(Ij, Ej) ≡⊥, at least one
of four requirements in Deﬁnition 4.1 must be false. Note that
the atomic sensing operation execution goal in Deﬁnition 5
rules out the probability of Case2. Speciﬁcally, LTL (16)
enforces 1), while 2) and 3) are guaranteed by LTL (15).
Lastly, 4) is covered by LTL (17).

For Case1, i.e., to trigger (cid:62) by running XSensing, AdvPfB
needs to read GPIO without causing an MCU reset. Recall
that the Mandatory Sensing Operation Authorization in Deﬁni-
tion 6 requires Verify (with input executable in ER) to succeed
at
least once before reading GPIO. According to VERSA
construction, AdvPfB causes Verify(ER, ATok∗, Chal∗) to out-
put (cid:62), where ER contains SAdv which is an unauthorized
software, ATok∗ is a valid issued (but never used) token, and
Chal∗ is its corresponding challenge. Since Verify function is
implemented using VRASED to compute HMAC of Chal and
ER, such AdvPfB can be directly used as AdvRA to win the
RA-game of VRASED. Thus, assuming secure RA architecture
VRASED, this is a contradiction, which implies the security of
VERSA according to the PfB-game.

D. Extended Evaluation & Discussion

Veriﬁcation Costs: Formal veriﬁcation costs are reported in
Table II. We use a Ubuntu 18.04 desktop machine running
at 3.4GHz with 32GB of RAM for formal veriﬁcation. Our
veriﬁcation pipeline converts Verilog HDL to SMV speciﬁca-
tion language and then veriﬁes it against the LTL properties
listed in Construction 1 using the NuSMV model checker (per
Section II). VERSA veriﬁcation requires checking 11 extra
invariants – LTLs (10) to (20) – in addition to VRASED LTL
invariants. It also incurs higher run-time and memory usage
than VRASED veriﬁcation. This is due to two additional 16-bit

16

discussion of this matter, see [72].

VERSA Alternative Use-Case:
VERSA can be viewed as a general technique to control access
to memory regions based on software authorization tokens.
We apply this framework to GPIO in low-end MCUs. Other
use-cases are possible. For example, a VERSA-like architecture
could be used to mark a secure storage region and grant access
only to explicitly authorized software. This could be useful if
Dev runs multiple (mutually distrusted) applications and data
must be securely shared between subsets thereof.

hardware signals (ERmin, ERmax) which increase the space
of possible input combinations and thus the complexity of
model checking process. However, veriﬁcation is still man-
ageable in a commodity desktop – it takes around 5 minutes
and consumes 340MB of memory.

VERSA Limitations:

1) Shared Libraries: to verify S, Ctrl must ensure that S
spans one contiguous memory region (ER) on Dev. If any
code dependencies exist outside of ER, VERSA will reset the
MCU according to LTL (17). To preclude this situation, S
must be made self-contained by statically linking all of its
dependencies within ER.

2) Atomic Execution & Interrupts: per Deﬁnition 4, VERSA
forbids interrupts during execution of XSensing. This can
be problematic, especially on a Dev with strict real-time
constraints. In this case, Dev must be reset in order to allow
servicing the interrupt after DMEM erasure. This can cause a
delay that could be harmful to real-time settings. Trade-offs
between privacy and real-time constraints should be carefully
considered when using VERSA. One possibility to remedy this
issue is to allow interrupts as long as all interrupt handlers are:
(1) themselves immutable and uninterruptible from the start of
XSensing until its end; and (2) included in ER memory range
and are thus checked by Verify.

3) Possible Side-channel Attacks: MSP430 and similar
MCU-s allow conﬁguring some GPIO ports to trigger inter-
rupts. If one of such ports is used for triggering an interrupt,
Adv could possibly look at
the state of Dev and learn
information about GPIO data. For example, suppose that a
button press mapped to a GPIO port triggers execution of a
program that sends some ﬁxed number of packets over the
network. Then, Adv can learn that the GPIO port was activated
by observing network trafﬁc. To prevent such attacks, privacy-
sensitive quantities should always be physically connected to
GPIO ports that are not interrupt sources (these are usually
the vast majority of available GPIO ports). Other popular
timing attacks related to cache side-channels and speculative
execution, are not applicable to this class of devices, as these
features are not present in low-end MCUs.

4) Flash Wear-Out: VERSA implements Verify using
VRASED. As discussed in Section II-C, the authentication pro-
tocol suggested by VRASED requires persistent storage of the
highest value of a monotonically increasing challenge/counter
in ﬂash. We note that ﬂash memory has a limited number of
write cycles (typically at least 10,000 cycles [70], [71]). Hence,
a large number of successive counter updates may wear-
out ﬂash causing malfunction. In VRASED authentication, the
persistent counter stored in ﬂash is only updated following
successful authentication of Ctrl. Therefore, only legitimate
requests from Ctrl cause these ﬂash writes, limiting the capa-
bility of an attacker to exploit this issue. Nonetheless, if the
number of expected legitimate calls to Verify is high, one must
select the persistent storage type or (alternatively) use different
ﬂash blocks once a given ﬂash block storing the counter
reaches its write cycles’ limit. For a more comprehensive

17

