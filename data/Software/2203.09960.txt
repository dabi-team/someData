Improving Semantic Consistency of Variable Names
with Use-Flow Graph Analysis

1st Yusuke Shinyama
dept. name of organization (of Aff.)
Tokyo Institute of Technology
Meguro-ku, Tokyo, Japan
euske@sde.cs.titech.ac.jp

2nd Yoshitaka Arahori
dept. name of organization (of Aff.)
Tokyo Institute of Technology
Meguro-ku, Tokyo, Japan
arahori@c.titech.ac.jp

3rd Katsuhiko Gondow
dept. name of organization (of Aff.)
Tokyo Institute of Technology
Meguro-ku, Tokyo, Japan
gondow@cs.titech.ac.jp

2
2
0
2

r
a

M
8
1

]
E
S
.
s
c
[

1
v
0
6
9
9
0
.
3
0
2
2
:
v
i
X
r
a

Abstract—Consistency is one of the keys to maintainable source
code and hence a successful software project. We propose a novel
method of extracting the intent of programmers from source code
of a large project (∼ 300 kLOC) and checking the semantic
consistency of its variable names. Our system learns a project-
speciﬁc naming convention for variables based on its role solely
from source code, and suggest alternatives when it violates its
internal consistency. The system can also show the reasoning why
a certain variable should be named in a speciﬁc way. The system
does not rely on any external knowledge. We applied our method
to 12 open-source projects and evaluated its results with human
reviewers. Our system proposed alternative variable names for
416 out of 1080 (39%) instances that are considered better than
ones originally used by the developers. Based on the results, we
created patches to correct the inconsistent names and sent them
to its developers. Three open-source projects adopted it.

Index Terms—Program comprehension, Source code analysis,
Dataﬂow analysis, Software maintenance, Naming, Semantic
consistency

I. BACKGROUNDS

A large software project is typically developed and main-
tained by a number of people. Even if the project is relatively
small in its size, it might eventually grow into a large project
over time. It is therefore desirable to keep its code looks
and feels consistent across the different components, so that
each member can communicate smoothly. Many software
projects adopt “style guides”, a set of coding standards to
maintain a certain level of superﬁcial consistency. While these
guidelines help the project to achieve a certain aspect of the
code consistency, they mostly address the stylistic aspects
such as indentation or word capitalization. The other types of
consistency, or “semantic consistency” such as word choice
for concepts used in the code, is often left to each developer’s
discretion.

One of the reasons why such a consistency is difﬁcult
to achieve is that they are highly subjective. It is relatively
easy to prescribe the stylistic aspect of source code in a way
that it can be checked automatically. On the other hand, it
is hard to regulate all the names used in source code in
advance. Programmers often encounter situations where they
have to name a certain concept that is not so well-deﬁned,
yet necessary to be named. Some of these concepts might
not be immediately comparable to any real-life objects, so the
programmers have to be inventive. In general, naming is much
more difﬁcult to regulate than superﬁcial coding styles.

One way to cope with such a problem is to maintain a
list of words that are used for names and share them among
the developers 1. However, this is impractical because, unlike
style guides, the concepts used in a program are often project
speciﬁc, and the same word can mean different things in
different projects. For example, the word “view” can mean a
portion of a table when it is used in a database engine, while
it can mean a visible window when it is used in a graphical
application. Or “rate” can mean different things in a ﬁnancial
application and a network application, etc. To make it worse,
different projects use different abbreviation for the same words
(e.g. “att” versus “attr” for attributes). To date, there are
still relatively few naming guidelines for a large project.

Yet, the importance of names in a program code has been
emphasized by many researchers and practitioners [3], [4].
Programmers tend to heavily rely on meaningful identiﬁer
names to understand source code [5], and they generally prefer
a long descriptive name than single-letter variables [6]. It is
also reported that poor naming can lead to misunderstanding
or confusion among programmers, which eventually result in
poor code quality [7]. In many software projects, inconsistent
naming is often considered as a bad smell. Sometimes they
are actually treated as bugs (naming bugs [8]). While naming
inconsistency does not immediately lead to a malfunction, it
decays the code quality over time and developers become more
prone to introduce serious bugs. A quick search over GitHub
reveals that programmers keep being confused by wrongly
named variables and methods. 2. Also, it is natural to assume
that the problem of inconsistent names is exacerbated as the
size of codebase grows. In large software projects, multiple
programmers are involved in changing different parts of the
code, sometimes with not enough communication to each
other. Without the holistic view of the entire codebase, the
inconsistent names can be often overlooked or neglected for
a long time, which further degrade the code quality.

Naming issues have been an active topic of research in
the software engineering community. Overall, two approaches
exist; one is to provide accurate names based on source code,
and the other is to detect and correct the naming inconsistency

1News organizations took a similar approach to this by developing their

in-house style guides [1], [2].

2A search over GitHub issues with “wrong name” turns out over 1 million

results.

 
 
 
 
 
 
in existing code. For the ﬁrst approach, Allamanis et al. used
machine learning algorithms to suggest method and class
names from code [9]. Alon et al. converted source code
into word embeddings [10] that correspond to a certain word
in natural language [11], which can be used for identiﬁers.
Raychev et al. recovered variable names from obfuscated
JavaScript code [12]. For the second approach, Høst et al.
[8] used manually crafted rules to detect naming bugs in a
number of open source Java projects. Liu et al. used machine
learning to capture the relationships between a method name
and its body (code) to discover bad method names that do not
properly describe its function [13]. Allamanis et al. proposed
a method to automatically capture the stylistic conventions
from source code, some of them are naming-related [14]. To
our knowldege, our work is the ﬁrst attempt to detect semantic
inconsistency of variable names in a large software project.

As for the naming guidelines and code readability, [15]
offers an early work for the code documentation. Lawrie et
al. [16] was one of the early attempts to give an insight to
naming bugs. According to Lawrie et al., naming bugs are
divided into two categories: homonym and synonym. If two
different concepts are mapped into the same name (homonym),
or a single concept is called by multiple names (synonym), it
often causes confusion to the programmers [17]. Apart from
naming, detecting semantic inconsistency in source code can
be a powerful tool for ﬁnding potential bugs [18].

A. Importance of Variable Names

So far, most of the existing works focus on the method
names. This is understandable because a method or function
is often a meaningful chunk of code that is supposed to have a
coherent name. However, we argue that it is a variable name,
rather than a method name, that plays the crucial role for
understanding the high-level meaning of the code because it
reveals the type of information that the program handles. We
have found that nouns often play a signiﬁcant role in naming
variables, and in turn help the overall understanding of the
program. We also have found that each project has a fairly
speciﬁc set of nouns that are related to its target domain.

Informally, this can be shown in the two steps: First, method
names often rely on variable names. Table I shows the number
of method names that are related to the variables it uses. By
“related”, we mean that the method name contains a word that
is also used by one of the variables it uses. From this table,
we can say that about one third (or more) of method names
rely on the variable names in its meaning, i.e. we ﬁrst need to
understand the meaning of each variable in order to understand
the meaning of a method.

Next, many variable names are made up with nouns that
refer to domain speciﬁc concepts. Note that many method
names consist of both verbs and nouns. Again, this is under-
standable as a method is often an actor or agent of the objects
while a variable usually contains a reference to certain objects.
However, we observed that the verbs used in method names are
fairly limited in its variety. Table II shows the popular words
used in method names. One can see that the most popular

TABLE I
NUMBER OF METHOD NAMES
Related
5,507
1,046
1,113
995
3,068
2,171
396
3,978
10,519
11,040
3,820
224

Unrelated
5,905
1,922
2,243
1,366
4,630
5,127
943
8,026
11,189
11,428
4,272
464

Project
ant
antlr4
bcel
compress
jedit
jhotdraw
junit4
lucene
tomcat
weka
xerces
xz

TABLE II
POPULAR WORDS USED IN METHOD NAMES

Project
ant

antlr

bcel

Top Verbs
set, get, add, create,
is
get, set, add, remove,
visit
visit, get, accept,
set, dump

compress get, set, read, write,

jedit

close
get, set, add, is, run

jhotdraw get, set, create, is,

junit4

lucene

tomcat

weka

xerces

xz

is,

set,

add
get, assert, run, test,
validate
get, set, compare, add,
read
get,
remove
get,
create
get, set, is, create,
add
get,
close, set

write,

read,

set,

add,

add,

is,

line,

class,

entry,

Top Nouns
file, name, function,
class, output
string, rule, token,
code, name
constant,
string, type, value
stream,
archive, data, input
jj,
action,
string, buffer
action, figure, color,
name, property
test, class, method,
failure, runner
doc,
next,
value, bytes
name, string, session,
max, class
text, tip, options,
action, string
element, name, decl,
type, impl
stream, input, size,
output, memory

string,

verbs are get, set, add, remove, and create in nearly all
projects, whereas the nouns are more diverse across different
projects. One can also see that the nouns are often speciﬁc to
each project topic, whereas the verbs are mostly generic.

From the above observations, we can conclude the follow-

ing:

1) Nouns often play a signiﬁcant role in naming identiﬁers,
and in turn understanding the high-level meaning of the
code.

2) Each project has a fairly speciﬁc set of nouns that are

related to its target domain.

Relatively fewer attempts have been made for predicting
ﬁeld or variable names, possibly because of its variety and sub-
jectivity. Typically, nouns are used for variable names whereas
verbs are used for method names. WordNet [19] has 115k
nouns while it has only 11k verbs. The software industry has a
long history of using existing common nouns for representing
abstract concepts, such as “tree”, “view” or “stream”. The
meaning of these words, however, are only loosely deﬁned
in each project. Raychev et al. obtained the characteristics of
variable names from a large JavaScript codebase [12], which
can be applied to general functions but not project speciﬁc

a.

b.

void update(float income) {

balance += income;

}
void update(float velocity) {

position += velocity;

}

Fig. 1. Functionally Identical But Semantically Different Functions

ones.

There is another reason why we think variable names are
important: ultimately, what a computer program handles is
just a collection of bits;
they are typically interpreted as
numbers, vectors and strings. However, that is not the end
– real applications need to handle concepts such as “counter”,
“position” or “balance”. Fig. 1 shows two functions that
are functionally identical (adding a value of one variable
to another) but semantically different; one is to update the
balance, and the other is to update the position. Similarly, a
string can be used as “username”, “pathname” or “address”.
In other words, they need to assign the meaning to these bits
by naming them, and it is the primary function of variables.
Variable names are particularly important to give programmers
high-level views. They represent a fundamental building block
of application domain.

In reality, maintaining consistency is not always a project’s
top goal. Real world software faces constant challenge to
be modiﬁed or improved. Sometimes one needs to break
consistency in order to upgrade a part of the code to adapt
for a newer requirement. Therefore, the naming rules are often
a set of conventions rather than a strict dogma. Our goal is
to capture these conventions and reuse them effectively in an
automated manner to improve the code quality. In this paper,
we ﬁrst present our general framework of testing consistency,
and then introduce an actual mechanism to apply it to source
code.

II. WHAT IS CONSISTENCY?

In this section, we present a general framework of testing
the consistency between two sets of inputs over a certain
invariant. Suppose we have two sentences, Sa and Sb, where
each sentence has a pair of features (Fa1, Fa2) and (Fb1, Fb2),
respectively. Furthermore, assume that Fa1 in general conveys
a sufﬁcient context to reliably predict Fa2, i.e. there is a
function K such that K(Fa1) = Fa2. Now, if we also ﬁnd
that K(Fb1) = Fb2, i.e. Fb1 has the same context to predict
Fb2, we can say that Sb is consistent with Sa in regard to K.
To illustrate this framework, take a look at the following

example:
(a) “It is rainy today so you should take

an umbrella.”

(b) “It is [X] today so John should take

an umbrella.”

Suppose that we obtained the knowledge (K) that there
is a strong relationship between “rainy” and “umbrella”
from sentence (a). It is fairly easy then to predict the word
[X] according to our knowledge. If the word [X] is indeed
“rainy”, sentence (b) is consistent with sentence (a) in regard
to our knowledge, K. While this formulation is similar to a

Snippet A:
out = open(...);
write(out, ...);

Snippet B:
X = open(...);
write(X, ...);

Fig. 2. Comparable Code Snippets

typical machine learning framework, the focus is different:
instead of predicting the unknown value of [X], we are
interested in measuring the consistency of K over various
inputs.

In reality, however, this kind of categorical knowledge is
hard to obtain. Therefore, we extend our deﬁnition to include
Bayesian inference, i.e. statements that have varying degrees of
certainty. Suppose we have two statements, Sa and Sb, pairs
of their features (Fa1, Fa2) and (Fb1, Fb2). Now, if we ﬁnd
that Fa2 is likely to be predicted by Fa1, i.e. P (Fa2|Fa1) is
high, and we also ﬁnd that P (Fb2|Fb1) is high, we can say
that Sb is likely to be consistent with Sa in regard to P . In
other words, the consistency in our framework is equivalent
to the predictability of answers.

A. Measuring Consistency of Program

Now,

let us apply the above framework to a program
code. Suppose we have two comparable code snippets A and
B (Fig. 2). In snippet A, the name “out” is used for a
return value of open(...) function and also for the ﬁrst
argument of write(...) function. Assume that we learned
the relationship between the variable name “out” and these
two statements. More formally put, we ﬁnd that the snippet A
has three features:

• FA1: the variable is assigned with the return value of

“open()”.

• FA2:

the variable is passed as the ﬁrst argument of

“write()”.

• FA3: the variable has name “out”.
We might say that this is the “knowledge” K that we learned
about the use of this variable. Furthermore, we have another
code snippet B where a certain variable X is used in the exactly
same manner; it has three features and we ﬁnd FA1 = FB1
and FA2 = FB2. If we ﬁnd FA3 is also equal to FB3, i.e. the
name of the variable X is indeed “out”, we can say that the
variable name is consistent in regard to our knowledge K. The
key idea here is that most variables exist with relationship with
other variables, and the relationship deﬁnes the role (name) of
each variable 3 which can be inferred from various aspects of
source code.
So far,

in
that we did not put any assumption on how each feature
should look like or what their relationship can be. We later
create a more concrete mechanism to express a usage of a
variable, and a probabilistic model (knowledge) to measure its
semantic consistency in a similar process described above. If
the program is not consistent with our model, we can suggest a
better name for variables that aligns with our understanding of
the program. However, the general framework can be applied
to any kind of elements in source code. For example, it is

the framework we presented here is general

3We assume that variables with a similar role tend to have a similar name.

f(a, b, c)
{

x = (a+b) * c;
y = -a;
return x + y;

}

Fig. 3. Simple Use-Flow Graph (UFG)

Fig. 4. Typical Program Dependence Graph (PDG)

possible to measure the consistency between method names
and its calling convention, if such features are available. In
the rest of this paper, however, we focus on improving the
consistency of variable names to improve the code readability
using the above framework.

III. PROPOSED METHOD
In this section, we describe how to apply the above frame-
work of naming consistency to program variables. First, we
need to capture the usage of each variable in a systematic
way. For this purpose, we introduce a graph structure called
“Use-Flow Graph” (UFG). The idea of UFG is similar to
a dataﬂow diagram and program dependence graph (PDG).
A typical dataﬂow diagram describes how data is processed
and transmitted from one part of a system to another. In
most settings, the parts involved in a dataﬂow diagram are
processors or storage devices. UFG involves with a more
granular kind of storage: variables and ﬁelds in a program. In
this sense, UFG is similar to a PDG. However, while a typical
PDG only shows the data dependence of each statement, UFG
shows the data dependence between each variable. The idea
of using a graph for representing the dataﬂow among variables
was disseminated by [20]. We added operators and function
(method) arguments as a location. Fig. 3 shows a sample
UFG. Note that the graph not only shows how the value is
transmitted from each variable (a, b, c, x and y), but also
shows how various operators (+, - and *) are applied in the
process. This way, we can see how the value of each variable
is treated in a series of processing 4. A comparable PDG for
the same program could be written as in Fig. 4.

We further added a way to express conditional branches and
loops to UFG, which is explained later. In short, UFG can

4One of the major differences between our graph and the work by Raychev
et al. [12] is that our graph is directional; we only consider a relationship that
reﬂects an actual execution order. For example, there is no direct relationship
between variable x and y in this graph.

present how values (variables, ﬁelds or constants) are treated
at each operation in a precise manner without depending on
a language syntax. A path in UFG can show how a particular
value is given as an input, processed and tested, and passed
to other variables. We then deﬁne the “usage pattern” of a
variable as a UFG path that is originating from that variable.
By traversing the edges in Fig. 3, we obtain the following
usage patterns:

• a →L + →L * → x →R + → return
• a → - → y →L + → return
• b →R + →L * → x →R + → return
• c →R * → x →R + → return
After obtaining such patterns, we construct and use a
probabilistic model that we explained in Section II to test if
each variable name is consistent with its usage pattern. In the
next subsection, we ﬁrst explain how to construct UFGs from
source code.

A. Constructing Use-Flow Graph

We now illustrate how to construct a UFG from typical
language constructs in Java 5. UFG can be constructed in a
linear time for a given program size, allowing to analyze a
large project in a reasonable time.

Let us revisit the UFG shown in Fig. 3. In this graph, every
operator is represented as a separate node, and the transmission
of each value is shown as directed edges. The label of each
edge shows at which side of the binary operator that a value
is used (either L or R). Note that the order of execution can
be recovered by following the edges at each node and each
variable is still distinguished as a different path in a graph. So
it is still possible to reconstruct the equivalent program from
a given graph 6. We expect that the overall structure of UFG
is generally preserved across different programming styles 7,
because all the basic operations still have to be applied in the
same order to have the same effect. The UFG of a program
shows how each piece of data at various locations in a program
is interacted with each other. If one takes a look at the UFG
around a certain variable, its subgraph is likely to show how
the variable is used at the other parts of the program; namely,
they are showing its usage.

B. Tracking Multiple Variables

When a program is purely functional,

is
solely determined by its inputs and there is no side effect,
the program can be represented by a single connected UFG
with one sink node. When multiple independent variables are
modiﬁed, however, there will be multiple sinks or disjointed

its output

i.e.

5Our current UFG generator fully supports Java 8 syntax. In future, we
plan to extend this to other popular procedural languages such as C# or C++.
6Exact reconstruction of the original code is not always guaranteed, because
not all the side effects and indirect access are preserved. We assume that the
lack of these properties do not cause a signiﬁcant loss of accuracy for our
purposes in this paper.

7Note that we do not intend to identify the functional equivalence. Two
mathematically equivalent expressions (e.g. a+b and b+a) does not neces-
sarily result in the same UFG. Our goal here is to preserve the intent of
programmers as much as possible while removing stylistic differences.

a+L-bRc*RLxy+LRreturn(start)x = (a+b) * c;abcy = -a;areturn x + y;xy{

}

x = x + y;
z = y;
a = 42;

do {
S;

// modify x

} while (p);

Fig. 5. Use-Flow Graph of Multiple Variables

if (x) {
y = 1;
} else {
y = 2;

}

x = f(
y,2,3

)

Fig. 8. Use-Flow Graph of Loop. S denotes an inner subgraph that modiﬁes
variable x.

Fig. 6. Use-Flow Graph of Condi-
tional Statement

x = f(y,2,3)

Fig. 7. Use-Flow Graph of Embed-
ded Function Call. (Edge labels are
removed for readability.)

graphs, as shown in Fig. 5. A different graph concerns a
different set of data that are unrelated to each other. Note that
the original order of execution is not preserved because there
is no dependence between statements, and these unrelated
statements could be executed in parallel.

C. Conditional Statement

To represent conditional statements such as if, we intro-
duce special nodes. When a value of a certain variable is
determined conditionally, all the possible ﬂows are connected
to a single Join node (Fig. 6). The idea is to interpret a Join
node as something like a railroad switch, or a conditional
operator. When the conditional statement is executed, only
one of these edges (true or false in this example) is used.
Each edge is labeled with its condition so that they can still
be distinguished. When multiple variables are modiﬁed in the
if statement, a similar structure is created for every variable
that changed. Note that each statement is converted to nodes
in UFG whether or not the statement is actually executed.

D. Loop

We introduce another set of special nodes, Begin and
End, to represent a loop. (Fig. 8). For each variable that is
modiﬁed in the loop, its Use-Flow subgraph is sandwiched
with Begin and End nodes. In the case of do loop, as
shown in Fig. 8, the conditional test is performed at the end
of the loop, and the End node behaves like the Join node
in the previous example. The interpretation of this graph is
that the subgraph between the Begin and End nodes are
repeated by an unknown number of times, and for every time
the conditional value p is reevaluated. Note that the purpose
of this graph is to show in which context each variable is
modiﬁed, but not to show how the loop actually runs. A UFG
is not suitable for inferring the loop invariant or comparing
different loop structures.

Fig. 9. Use-Flow Graph of Referenced Function Call

E. Function/Method Call

A function or method call in a UFG is simply treated as
yet another operator node (Fig. 9). The callee function is
referenced. Each function call node has all possible references
to the functions that has the same signature, including virtual
functions. Each edge for the function arguments is labeled
as #arg0, #arg1 and #arg2 and the return value as
#return. When we want to obtain a relationship of nodes
across multiple functions, however, we can internally treat
each call node as if there was another UFG embedded within
the node, in a similar manner to code inlining, i.e. the callee
function is embedded (Fig. 7). This allows us to consider the
relationship of value operations in an interprocedural context.

F. Collecting Usage Patterns

Realistic software usually contains thousands of functions.
Since we want to collect a usage pattern of a value in a long
context, we want to track how the value is handled and passed
across multiple functions. This is done in the following steps.
The key idea here is to start from a node of interest (a variable
in this case) and gradually incorporate other nodes to multiple
functions that are being called:

1) Start from every variable node (a node referring to a
variable). This is an initial pattern for this variable usage
8.

2) Pick the next node by tracing its outgoing edges. Incor-

porate it as a part of the pattern.

3) If the node is a function call, push the current node to the
stack and connect a value node of the caller function to
the corresponding argument node of the callee function 9.
This is done by connecting the UFGs of both functions.
Incorporate this connection to the pattern.

8The initial variable node itself is not included in a usage pattern.
9In this paper, we limit the number of possible virtual functions that
are potentially referenced to 5. When there are 6 or more possible virtual
functions, we picked the virtual methods for the ﬁve most speciﬁc class.

x+LyRz42ax1Jointrue2falsexcondyf()ya2b3c+-*+xxBeginenterpEndcondSxexitrepeatyf#arg02#arg13#arg2x#return1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

private BufferedReader fp;
public String getName() {

String line = fp.readLine();
int i = line.indexOf(’ ’);
return line.substring(0, i);

}
public void show() {

String name = getName();
System.out.println(name+"!!");

}
public String getField() {

String buf = fp.readLine();
return buf.substring(0, buf.indexOf(’:’));

}
public String getColumn() {

String buf = fp.readLine();
return buf.substring(0, buf.indexOf(’,’));

}

Fig. 10. Sample Java Code

4) If the node is a return node and the stack is not empty,
pop the previous node from the stack. Connect the return
node of the callee function with the receiving node of the
caller function.

5) If the node is a return node and the stack is empty,
Connect the return node of the callee function with the
receiving node of a function call node for every function
that it potentially calls that function. Multiple patterns are
generated.

6) Repeat this process until a pattern grows to a maximum

predeﬁned length 10

There are actually two kinds of usage patterns: forward and
backward. The process described above is one for obtaining
forward usage patterns. For backward patterns,
the same
process is used for the opposite direction of the edges. From
now on, we just use the term “usage patterns” for forward and
backward patterns, combined.

Let us illustrate the above algorithm with a more realistic
example (Fig. 10). Suppose that we are interested in taking a
usage pattern of the variable “line” at Line 4. We start from
the assignment expression, incorporate the .indexOf() and
.substring() node, and reach the end of the getName()
function at the return statement. Then we further extend the
pattern by incorporating the nodes that are receiving the value
of getName() function. In this example, the name node
is added to the pattern. We repeat the same process for the
backward pattern. At the end, we obtain the following usage
pattern for the variable “line”.

• fp.readLine() → line →this indexOf() →arg1
substring() → name →L + →arg0 println()

G. Detecting Inconsistent Names

In the snippet shown in Fig. 10, there are other functions
named getField() and getColumn(). We obtain the
usage pattern of their variables “buf” as follows:

• fp.readLine() → buf →this indexOf() →arg1

substring()

10Note that an exponential number of patterns can be generated when there
is a branch in the graph. We limited the number of maximum nodes to be
traced to ﬁve.

The above pattern is similar to the one obtained for the vari-
able “line”. However, this pattern appears more frequently
the program than the previous one, hence the
throughout
pattern is more strongly associated with the name “buf” rather
than “line”. This way, the system can learn that “line”
should be better named as “buf” to achieve more consistency.
Note that this sort of knowledge is acquired entirely from the
project source code. The system does not use any external
knowledge. Naturally, this allows the system to tune to a
speciﬁc project.

To recapitulate,

the overall algorithm of our proposed

method is the following:

1) Extract UFGs from source code and collect the usage

patterns for each variable.

2) Construct a probabilistic model to test if each variable

name is consistent with its usage pattern.

3) If a variable name is found not to be consistent, suggest
an alternative name that is more strongly associated with
its usage pattern.

H. Constructing and Using Probabilistic Model

After extracting UFGs from source code and obtaining its
usage patterns for each variable, we construct a probabilistic
model. We try to learn a model that predicts a variable name
from a given usage pattern as explained in Section II. In this
paper, we used a simple Bayesian inference, i.e. we assume
that every node in a usage pattern independently affects the
choice of its variable name. In the case of the previous Java
example, the features that inﬂuence the prediction include: the
origin of the value, the way it is used, and its destination (the
variable name it is assigned), and so on. A usage pattern is
converted to a set of features by encoding the sequence of its
adjacent node pairs.

For example, a usage pattern like this
• indexOf() →arg1 substring() → name →L +

→arg0 println()

is converted into the following features:
• indexOf():arg1:substring()
• substring():name
• name:L:+
• +:arg0:println()
To give the model more ﬂexibility, names are not treated
as a single feature but a set of features based on its tokens.
For example, “outputBufferName” is tokenized into three
distinct features: “output”, “buffer” and “name”. 11.
Other than tokenization, the system does not have any prior
knowledge about the natural language used in variable names.
The list of features for each node of a usage pattern is shown
in Table III.

After learning the model using all

the usage patterns
throughout the program, the system re-applies them to every
variable and see if its prediction matches its original name. If
it does, its name is consistent with its usage. If it does not, the

11Our tokenizer assumes that variable names are either the form of

camelCase or snake_case.

Fig. 11. Probabilistic Model for Variable “line”

TABLE III
FEATURES INCLUDED IN USAGE PATTERNS

1) If the value is a constant.
2) If the value is used within a loop.
3) If the value is used in a branch condition.
4) If the value is used as an array index.
5) If the value is used as an object instance.
6) Operator the value is used against.
7) Function argument the value is passed to.
8) Type of the value.
9) Name of another variable the value is assigned to.

name is “outvoted” by other variables which have the same
usage pattern. In this case, the system suggests the most likely
name for that usage pattern.

Note that the acquired model predicts the usage patterns
(and the associated names) of just one variable. This means
that we need to create a different probabilistic model for every
single variable that has different sets of usage patterns. We
cannot use the same model for all the variables in the program
since it is contaminated with their original names. We must
exclude the hints of the original name when we are trying to
predict it.

The overall procedure of our system is as follows:

• Let V as a set of all variables used in a target program.
• For each v1 ∈ V :

1) For all v ∈ V (v (cid:54)= v1), obtain P (v.name|v.pattern).

This is a model that predicts v1.

2) Find a name n = argmax P (n|v1.pattern).
3) If n = v1, the name is consistent with the model.
Otherwise, suggest n as a more appropriate name for
v1.

4) A conﬁdence score is calculated for each sugges-
tion by adding the weights of the usage patterns:
(cid:80)
p∈v.pattern TF (p)×IDF (p)×exp(p.dist) 12 where p
is a usage pattern used for the suggestion and p.dist is
the distance between the pattern and its target variable.
In this paper, we use Na¨ıve Bayes classiﬁer for a probabilis-
tic model. In Na¨ıve Bayes classiﬁcation, learning a model is
simply counting the number of occurrences of v.name and
v.pattern, and therefore it is easy to construct a different
model (P ) for each variable 13.

IV. EXPERIMENTS

We applied to our method to the projects listed in Table IV.
We conducted three experiments for the following questions:

TABLE IV
SOURCE CODE AND USE-FLOW GRAPH SIZES

Project
ant 1.10.6
(build tool)
antlr4 4.7.2
(parser generator)
bcel 6.3.1
(Java bytecode library)
compress 1.18
(compression library)
jedit 5.5.0
(text editor)
jhotdraw 5.3
(drawing tool)
junit4 4.12
(unit testing)
lucene 7.7.2
(document indexing)
tomcat 8.5.43
(application server)
weka 3.8
(machine learning)
xerces 2.12.0
(XML parser)
xz 1.18
(compression library)

kLOC Variables Nodes
350k
112k

24k

Edges
5,211k

31k

31k

24k

7k

7k

6k

74k

1,103k

80k

1,190k

69k

929k

115k

22k

294k

6,106k

80k

9k

109k

238k

324k

114k

17k

235k

2,351k

2k

21k

280k

30k

414k

7,146k

49k

649k 11,799k

59k

943k 13,224k

22k

314k

7,017k

7k

2k

23k

299k

• RQ1. Are usage patterns an effective representation for a

variable usage?

• RQ2. Did the system predict a correct variable name?
• RQ3. Did the system provide convincing evidences to

support the suggested alternatives?

Our results were evaluated by nine reviewers 14. None
of the reviewers were familiar with the source code of a
target project. Reviewers were asked not to talk about speciﬁc
results during the experiments. In the following subsections,
we conducted experiments to answer the above questions.

Our experimental setup was a standard desktop PC 15
running Arch Linux. Extracting UFGs and collecting usage
patterns from source code took from a few minutes to several
hours, depending on the project size. Building a probabilistic
model and generating suggestions took several minutes. All
the tools and datasets that we used for this experiment are
publicly available 16.

A. Variable Equivalence Test (RQ1)

In the ﬁrst experiment, we tested if two variables with
similar usage patterns have indeed a similar role. This was
done by collecting pairs of variables whose usage patterns are
similar to each other (the similarity > 0.90), and check if
the two variables have a similar name (role). The similarity is
computed by taking the cosine distance of two usage patterns
as TF-IDF vectors, as in

Sim(p1, p2) =

V1 · V2
|V1||V2|

,

(cid:88)

Vi =

TF (ni) × IDF (ni).

where pi and ni is a pattern and its nodes, respectively.

12TF is the frequency of the pattern that is associated with the variable.

IDF is the inverse frequency of the pattern over all the outputs.

13We ﬁrst learn a model using all the usage patterns. Then for each variable
v1 we unlearn the patterns by subtracting the counts for v1.name and
v1.pattern.

14Three are the authors of this paper. The other six are graduate students

who has a basic experience of Java programming.

15Intel i5, 1.8GHz, 32GB memory.
16https://github.com/euske/fgyama

fp.readline()line#this:indexOf()#arg1:substring()assign:nameTABLE V
VARIABLE EQUIVALENCE TEST

Project
ant
antlr4
bcel
compress
jedit
jhotdraw
junit4
lucene
tomcat
weka
xerces
xz
Total

Must-Eq. Can-Eq. Must-Neq. Unk. Total
45
45
45
45
45
45
45
45
45
45
45
45
540

17
22
11
9
25
17
21
21
27
28
22
15
235

23
13
3
10
7
15
14
18
11
4
12
4
134

5
9
31
24
9
12
10
6
6
12
9
26
159

0
1
0
2
4
1
0
0
1
1
2
0
12

TABLE VI
GENERATED SUGGESTIONS AND CONFIDENCE SCORE DISTRIBUTION
(BY PERCENTILE)

Project
2
ant
7
antlr4
3
bcel
4
compress
7
jedit
jhotdraw 12
6
junit4
9
lucene
13
tomcat
9
weka
5
xerces
3
xz

10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Total
61 144 302 637 1031 1985 3061 5784 13022
15
74 144 217 344 453 658
41
965 2912
9
55 103 203 294 501 644 1042 2862
12
5
16
98 214 276 504 822 1012 2997
45
6
58 199 345 606 965 1621 2349 3837 10001
14
68 150 251 441 648 895 1284 1695 5466
22
27
818
12
18
48 137 292 567 945 1735 2913 4546 11210
42 112 264 544 1165 1729 3038 4897 8422 20226
63 200 407 970 2054 3468 6905 10672 24770
22
49 131 289 435 756 1263 1924 3371 8250
27
519
8
9

89 149 182

86 157

111

135

14

74

33

40

89

89

24

Then we presented the pairs of variables to the reviewers
while hiding the actual variable names by replacing them with
“xxx”. The reviewers were asked to look at each variable
pair with its surrounding code snippets and choose one of the
following options:

(a) Variables must have the same name. (Must-Eq.)
(b) Variables can have the same name. (Can-Eq.)
(c) Variables must have a different name. (Must-Neq.)
(d) Undecidable. (Unk.)

The nine reviewers are presented with randomly selected
ﬁve variable pairs for 12 projects each. Table V shows the
responses. Out of 540 answers, 369 (68%) was either Must-
Eq. or Can-Eq.. This suggests that usage patterns are a
strong indicator of the role of a variable. The average cosine
similarity of Must-Eq. or Can-Eq. pairs was 0.980, whereas
the similarity of Must-Neq. was 0.976.

B. Name Suggestion Test (RQ2)

In the second experiment, we tested if our proposed method
can actually check the usage of variable names and suggest a
better alternative for those which are found inconsistent with
the other parts of the program. This experiment is twofold;
ﬁrst, we presented candidates of a variable name to the
reviewers and let them choose the best name among them,
where one of the candidates is produced by our system. We
also manually created a patch for correcting some of the
prominent suggestions by our system and sent it to the original
developers of the projects.

1) Evaluating System Outputs by Reviewers: In the ﬁrst
part of the experiment, we presented code snippets to the nine
reviewers. For each question, a reviewer is presented with one
code snippet with one variable highlighted, and another snippet
with a different variable whose usage is similar to the ﬁrst
one, but whose name is hidden. The reviewers were asked to
compare the two snippets and infer an appropriate variable
name for the hidden one. They can choose from the following
candidates, or choose undecidable (Unk.):

(a) A name suggested by our system. (System)
(b) A name suggested by a baseline system. (Base.)
(c) The original name (chosen by the developer). (Orig.)

Fig. 12. Evaluation Tool Screenshot

The baseline system here was only to suggest the most
common name for each data type. 17 The system produces
a number of suggestions for each project. We ranked them by
its conﬁdence score and present the top 10 suggestions to the
reviewers. The total number of generated suggestions (includ-
ing ones that were not reviewed) and its score distribution are
shown in VI. The screenshot of the evaluation tool is shown
in Fig. 12.

Each reviewer answers 120 questions in total. Table VII
shows the reviewers’ responses. Out of 1080 (= 9 × 120)
answers, 416 (39%) was System. This means that for about
40% of the cases, our system can discover inconsistent variable
names and suggest alternatives which are considered better
than the ones from the original developers.

Since there is no gold standard for a variable name, we
calculated the Fleiss’ Kappa [21] for measuring the inter-
reviewers agreement. Fleiss’ Kappa is commonly used for
measuring agreement between N people where N ≥ 3. In case
of N = 2, Cohen’s Kappa is typically used. The Fleiss’ Kappa
for our experiment was K = 0.45 (moderate agreement).

For exploring different ways of generating usage patterns,
we changed some parameters for feature generation and mea-
sured its accuracy against our best output. Table VIII shows
how different parameters can affect the system performance.

17For example, an int variable is always considered to be named i.

TABLE VII
NAME SUGGESTION TEST
System Base.
3
3
10
0
4
8
1
3
4
1
1
1
39

Orig.
39
46
28
35
42
78
48
38
44
44
49
37
528

39
34
48
53
24
4
34
40
34
29
31
46
416

Unk.
9
7
4
2
20
0
7
9
8
16
9
6
97

Total
90
90
90
90
90
90
90
90
90
90
90
90
1080

Project
ant
antlr4
bcel
compress
jedit
jhotdraw
junit
lucene
tomcat
weka
xerces
xz
Total

TABLE VIII
PERFORMANCE BY DIFFERENT FEATURE GENERATION PARAMETERS

+
+
+
+
+
+
-
+

+
+
+
-
+
-
+
+

Methods
5
5
5
5
5
5
5
1

Interproc? Name? Type? Length? Correct %
39%
14%
7%
6%
10%
8%
10%
16%

+
+
+
-
-
+
+
+
Maximum number of virtual methods for each function call.
If a usage pattern spans multiple functions.
If a feature about variable names are included.
If a feature about variable types are included.
Maximum length of usage patterns (number of nodes).

Methods
Interproc?
Name?
Type?
Length?
Correct % Ratio that the system suggestion was chosen.

5
3
1
5
5
5
5
5

2) Sending Patches to Developers: In the second part of the
experiment, we manually created a patch for correcting some
of the prominent suggestions by the system. The patches were
sent to the original developers of the projects. We have gotten
6 responses so far. Three projects adopted our patch, two are
still discussing it, and one is rejected because “this is not a
high priority” according to the developer.

C. Evidence Persuasiveness Test (RQ3)

Our system can provide the evidences for suggested variable
names. An “evidence” is a code snippet where the prominent
usage patterns for the target variable were obtained from. This
way, a user can review the system outputs and decide if they
can accept its result. Since each usage pattern has a weight and
is associated with its original location, the system can retrieve
top N 18 usage patterns and its originating source code. In
this experiment, the reviewers are asked to review a variable
name suggestion with its original name and decide if the
suggestion is sensible based on the accompanying evidences
(snippets). The reviewers are asked to choose the following
options regarding the evidences.
(a) It provides strong support for the name. (Strong)
(b) It provides reasonable support for the name. (Weak)
(c) It provides little support for the name. (Poor)
(d) Undecidable. (Unk.)

Table IX shows the reviewers’ responses. Out of 540
answers, 162 (30%) of them are considered as somewhat

18In this experiment, we set N = 3.

TABLE IX
EVIDENCE PERSUASIVENESS TEST
Unk.
0
0
0
1
0
0
0
1
0
0
1
2
5

Strong Weak
8
8
15
5
6
5
7
14
7
10
10
7
102

Poor
30
29
20
34
33
36
36
27
31
34
28
35
373

7
8
10
5
6
4
2
3
7
1
6
1
60

Project
ant
antlr4
bcel
compress
jedit
jhotdraw
junit4
lucene
tomcat
weka
xerces
xz
Total

Total
45
45
45
45
45
45
45
45
45
45
45
45
540

TABLE X
PERFORMANCE BY CONFIDENCE SCORE

% Score
> 80%
> 60%
> 40%
> 20%
> 0%

Total
207
108
108
108
9

Strong + Weak
71 (34%)
40 (37%)
20 (19%)
24 (22%)
7 (78%)

Poor
133 (64%)
66 (61%)
88 (81%)
84 (78%)
2 (22%)

supportive to the suggested names. The relationship between
the reviewers’ ratings and the system-generated conﬁdence
score is shown in Table X. It is observable that suggestions
with a lower conﬁdence score tend to be considered as a poor
evidence.

D. Anecdotal Examples

Here are a couple of anecdotal results (suggestions) that our

system produced:

• Make the name more task oriented.

final int index) {

- public static short getNoOfOperands(
-
-
return NO_OF_OPERANDS[index];
+ public static short getNoOfOperands(
+
+

return NO_OF_OPERANDS[opcode];

final int opcode) {

• Use the conventional abbreviation of the project.
- void errorWhileMapping( String s ) {
+ void errorWhileMapping( String msg ) {

- String pkgName = className.substring(...);
+ String packageName = className.substring(...);

• Use a synonym which aligns better with the other parts of the code.

- ReferencePosition(int n, int pos) {
+ ReferencePosition(int n, int offset) {

• Correct typo.

- void normalize(int normalizeOffset) {
+ void normalize(int normalizationOffset) {

V. DISCUSSIONS

Our experiments showed that UFGs and usage patterns can
be effectively used for discerning the use of variables. The
system does not rely on any predeﬁned knowledge other than
the language syntax, and can be applied to a realistic project
with modest computational resource.

As for the relevance of the experiment, note that our review-
ers were not familiar with the codebase used for the evaluation.
We made sure that all the reviewers (including the authors) be

not familiar with a target project in advance, and they do not
discuss a speciﬁc example during the experiment. However,
the reviewers had an advantage of seeing the different parts of
the code side-by-side and being presented a direct evidence of
inconsistency, whereas the original developers worked on only
one part of the code. This way, we argue that it is possible that
the reviewers made a better decision for variable naming than
the original developers who are obviously more knowledgeable
about the code.

A. Threats to Validity

There are a couple of threats to internal validity of our
experiments. First, our evaluation is subjective; it is affected by
the number of reviewers and their programming knowledge.
One could argue that nine reviewers are not enough. However,
our Fleiss’ Kappa K = 0.45, which is far from random,
suggests that our reviewers had some common standard about
variable naming. Another concern is the fairness of the re-
views. To address the reviewers’ bias, we randomized the order
of the system output that is presented to a reviewer so that
they cannot know which name was produced by the system.
However, in the Variable Equivalence Test (RQ1) we used
variables that already have a certain similarity (Sim > 0.90).
As a result, this test was not completely blinded.

When generalizing our results to a wider use, the threats to
its external validity are the following: There are not enough
projects tested. The target language for now is limited to Java.
There is a limitation of our UFG extraction program that
cannot track dynamic dispatch and variable aliasing, which
could be problematic for expanding our method to pointer-
rich programming languages like C++. A bigger concern is
that our experiment only measured the accuracy (precision)
of the system output, but not the overall coverage (recall).
There are concerns related to the performance of the machine
learning algorithm. In this paper, we tried to focus on our
overall framework and keep the learning part lean. However,
we expect that better algorithms (such as Recurrent Neural
Network or Conditional Random Field) can produce a better
result. One could use a better metrics for comparing usage
patterns than ours (Cosine similarity, TF-IDF).

VI. CONCLUSION

In this paper, we presented a novel way of testing the
semantic consistency of variable names. We presented a graph
structure called Use-Flow Graph which is intended to capture
the intent of programmers from source code. Our system
learned a naming convention of variables by collecting usage
patterns that are represented as a path of UFG nodes. We built
a probabilistic model to infer a variable name from its usage
patterns. We applied our method to 12 open-source projects
and evaluated its results with human reviewers. The experi-
ment showed that our system identiﬁed inconsistent variable
names and suggest better names reasonably well compared to
human developers. We plan to extend the concept of UFGs to
apply other program comprehension tasks in future.

REFERENCES

[1] A. Editors, The Associated Press Stylebook and Brieﬁng on Media Law.

Lorenz Press, 2004.

[2] A. M. Siegal and W. G. Connolly, The New York Times Manual of Style

and Usage. Three Rivers Press, 1999.

[3] B. W. Kernighan and R. Pike, The Practice of Programming. Boston,
MA, USA: Addison-Wesley Longman Publishing Co., Inc., 1999.
[4] S. McConnell, Code Complete, Second Edition. Redmond, WA, USA:

Microsoft Press, 2004.

[5] D. Lawrie, C. Morrell, H. Feild, and D. Binkley, “What’s in a name?
a study of identiﬁers,” in Proceedings of the 14th IEEE International
Conference on Program Comprehension, ser. ICPC ’06. Washington,
DC, USA: IEEE Computer Society, 2006, pp. 3–12. [Online]. Available:
https://doi.org/10.1109/ICPC.2006.51

[6] G. Beniamini, S. Gingichashvili, A. K. Orbach,

Feitelson, “Meaningful
variables,” in Proceedings of
on Program Comprehension,
USA:
//doi.org/10.1109/ICPC.2017.18

IEEE Press, 2017, pp. 45–54.

and D. G.
identiﬁer names: The case of single-letter
the 25th International Conference
Piscataway, NJ,
ser.
[Online]. Available: https:

ICPC ’17.

[7] E. Avidan and D. G. Feitelson, “Effects of variable names on
comprehension an empirical study,” in Proceedings of
the 25th
International Conference on Program Comprehension, ser. ICPC ’17.
Piscataway, NJ, USA: IEEE Press, 2017, pp. 55–65. [Online]. Available:
https://doi.org/10.1109/ICPC.2017.27

[8] E. W. Høst and B. M. Ostvold, “Debugging method names,” in
Proceedings of
the 23rd European Conference on ECOOP 2009
— Object-Oriented Programming, ser. Genoa. Berlin, Heidelberg:
Springer-Verlag, 2009, pp. 294–317. [Online]. Available: http://dx.doi.
org/10.1007/978-3-642-03013-0 14

[9] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, “Suggesting accurate
method and class names,” in Proceedings of
the 2015 10th Joint
Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2015.
New York, NY, USA: ACM, 2015, pp. 38–49. [Online]. Available:
http://doi.acm.org/10.1145/2786805.2786849

[10] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” CoRR,
vol. abs/1310.4546, 2013. [Online]. Available: http://arxiv.org/abs/1310.
4546

[11] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “Code2vec: Learning
distributed representations of code,” Proc. ACM Program. Lang.,
vol. 3, no. POPL, pp. 40:1–40:29, Jan. 2019. [Online]. Available:
http://doi.acm.org/10.1145/3290353

[12] V. Raychev, M. Vechev, and A. Krause, “Predicting program properties
from ”big code”,” SIGPLAN Not., vol. 50, no. 1, pp. 111–124, Jan.
2015. [Online]. Available: http://doi.acm.org/10.1145/2775051.2677009
[13] K. Liu, D. Kim, T. F. Bissyand´e, T. Kim, K. Kim, A. Koyuncu,
S. Kim, and Y. L. Traon, “Learning to spot and refactor inconsistent
method names,” in Proceedings of the 41st International Conference
on Software Engineering, ser. ICSE ’19.
IEEE Press, 2019, p. 1–12.
[Online]. Available: https://doi.org/10.1109/ICSE.2019.00019

[14] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, “Learning
the 22nd ACM
natural coding conventions,” in Proceedings of
SIGSOFT International Symposium on Foundations of Software
Engineering, ser. FSE 2014. New York, NY, USA: Association
for Computing Machinery, 2014, p. 281–293. [Online]. Available:
https://doi.org/10.1145/2635868.2635883

[15] V. R. Basili and S. K. Abd-El-Haﬁz, “A method for documenting code
components,” J. Syst. Softw., vol. 34, no. 2, pp. 89–104, Aug. 1996.
[Online]. Available: http://dx.doi.org/10.1016/0164-1212(95)00070-4

[16] D. Lawrie, H. Feild, and D. Binkley, “Syntactic identiﬁer conciseness
and consistency,” in 2006 Sixth IEEE International Workshop on Source
Code Analysis and Manipulation, Sep. 2006, pp. 139–148.

[17] F. Deissenboeck and M. Pizka, “Concise and consistent naming,”
Software Quality Journal, vol. 14, no. 3, pp. 261–282, Sep. 2006.
[Online]. Available: http://dx.doi.org/10.1007/s11219-006-9219-1
[18] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf, “Bugs as
deviant behavior: A general approach to inferring errors in systems
code,” SIGOPS Oper. Syst. Rev., vol. 35, no. 5, p. 57–72, Oct. 2001.
[Online]. Available: https://doi.org/10.1145/502059.502041

[19] G. A. Miller, “Wordnet: A lexical database for english,” Commun.
ACM, vol. 38, no. 11, pp. 39–41, Nov. 1995. [Online]. Available:
http://doi.acm.org/10.1145/219717.219748

[20] T. Reps, “Program analysis via graph reachability,” in Proceedings of
the 1997 International Symposium on Logic Programming, ser. ILPS
’97. Cambridge, MA, USA: MIT Press, 1997, pp. 5–19. [Online].
Available: http://dl.acm.org/citation.cfm?id=271338.271343

[21] J. L. Fleiss, “Measuring nominal scale agreement among many raters,”
Psychological Bulletin, vol. 76, no. 5, p. 378–382, 1971. [Online].
Available: https://doi.org/10.1037/h0031619

