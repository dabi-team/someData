Towards Hybrid-Optimization Video Coding

Shuai Huo, Dong Liu, Li Li, Siwei Ma, Feng Wu, and Wen Gao

1

2
2
0
2

l
u
J

2
1

]

V

I
.
s
s
e
e
[

1
v
5
6
5
5
0
.
7
0
2
2
:
v
i
X
r
a

Abstract‚ÄîVideo coding is a mathematical optimization prob-
lem of rate and distortion essentially. To solve this complex
optimization problem in practice, two popular video coding
frameworks have been developed: block-based hybrid video
coding and end-to-end learned video coding. If we rethink video
coding from the perspective of optimization, we Ô¨Ånd that the
existing two frameworks represent two directions of optimization
solutions. Block-based hybrid video coding represents the discrete
optimization solution because those irrelevant coding modes are
discrete in mathematics. The discrete solution provides multiple
starting points (i.e. modes) in global optimization space and then
searches for the best one among them. However, the search-
based optimization algorithm is not efÔ¨Åcient enough. On the other
hand, end-to-end learned video coding represents the continuous
optimization solution because the optimization algorithm of deep
learning, gradient descent, is based on a continuous function.
The continuous solution optimizes a group of model parameters
efÔ¨Åciently by such a numerical algorithm. However, limited by
only one starting point, it is easy to fall into the local optimum.
To better solve the optimization problem, we propose a hybrid
of discrete and continuous optimization video coding. We regard
video coding as a hybrid of the discrete and continuous optimiza-
tion problem, and use both search and numerical algorithm to
solve it. Our idea is to provide multiple discrete starting points
in the global space and optimize the local optimum around each
point by numerical algorithm efÔ¨Åciently. Finally, we search for
the global optimum among those local optimums. Guided by the
hybrid optimization idea, we design a hybrid optimization video
coding framework, which is built on continuous deep networks
entirely and also contains some discrete modes. We conduct a
comprehensive set of experiments to verify the efÔ¨Åciency of our
hybrid optimization. Compared to the continuous optimization
framework, our method outperforms pure learned video coding
methods. Meanwhile, compared to the discrete optimization
framework, our method achieves comparable performance to
HEVC reference software HM16.10 in PSNR.

Keywords‚ÄîDeep neural network, end-to-end optimization, hybrid
optimization, learned video compression, optimization theory, rate-
distortion optimization, video coding.

I.

INTRODUCTION

Video coding is the enabling fundamental for most video
technologies, including computer vision,
image processing,
visual communication, etc. Since the early days of digital
video technology in the 1960s [1], academia and industry have
started to study video coding and developed an outstanding

Date of current version July 13, 2022. (Corresponding author: Dong Liu.)
S. Huo, D. Liu, Li Li, and F. Wu are with the CAS Key Laboratory
of Technology in Geo-Spatial Information Processing and Application Sys-
tem, University of Science and Technology of China, Hefei 230027, China
(e-mail: huoshuai@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; lil1@ustc.edu.cn;
fengwu@ustc.edu.cn).

S. Ma and W. Gao are with the Institute of Digital Media, Peking University,

Beijing 100871, China (email: swma@pku.edu.cn; wgao@pku.edu.cn).

framework called block-based hybrid video coding framework.
The name of block-based hybrid video coding is because it‚Äôs a
hybrid of two coding choices or modes (i.e. motion-handling
and picture-coding techniques) at the block level. Under the
two main choices, the framework allows to add various detailed
modes, such as sub-pixel motion mode and merge mode in the
motion-handling choice, skip and DCT mode in the picture-
coding choice. Even if many modes are not very related to
each other, this framework can still integrate them well and
then searches for the best coding mode among them for each
sequence. It is precisely because of the mode expansibility
that an important development direction of video coding is
to add more advanced modes based on the latest framework.
Accordingly, a series of coding standards have been developed
in the past forty years, including H.261, H.263, H.264, H.265,
H.266 [2], EVC [3], LCEVC [4], AV1 [5], AVS3 [6], etc. Now,
H.266/VVC published by ITU-T and ISO/IEC in July 2020 is
the state-of-the-art video coding standard. However, with the
increase of modes, the rate-distortion optimization by mode
search becomes more complex.

Deep learning [7] has brought about great breakthroughs and
revolutionized the paradigm of image/video processing since
2012 [8]. Deep learning uses deep neural networks to convert
the data into a representation at a more abstract level, and
those representations are not hand-designed but learned from
massive data. With the help of the gradient descent algorithm,
the deep networks can be optimized efÔ¨Åciently. Inspired by that
success, deep learning for image/video coding has also been
explored since 2015 [9], [10]. Those works can be divided
into two categories: deep tools and end-to-end methods. The
deep tool is to add a deep network-based coding tool into the
traditional coding framework. Those deep tools have improved
the coding efÔ¨Åciency signiÔ¨Åcantly, which conÔ¨Årmed the feasi-
bility and potential of deep learning-based video coding. On
the other hand, the end-to-end method is built entirely upon
deep networks, and the whole framework optimizes a group
of model parameters by gradient descent in an end-to-end
manner. Although end-to-end works have more potential, due
to the complexity of video coding tasks, the existing end-to-end
works still have a little gap from the best conventional coding.
Thus, the other key development direction of video coding is
to design more effective network modules under the current
end-to-end learned coding framework to realize its potential.
Video coding is to optimize the encoder-decoder parameters
to achieve the best tradeoff performance of bit rate and distor-
tion. Therefore, video coding is a mathematical optimization
problem of rate and distortion essentially. If we look at video
coding from the perspective of optimization, we Ô¨Ånd that the
existing two frameworks represent
two solution directions.
The block-based hybrid video coding represents the discrete
optimization solution. The various coding modes usually are

 
 
 
 
 
 
2

not relevant to each other, so they are discrete in mathematics.
Thus,
this framework can be seen as providing multiple
discrete points (i.e. coding modes) in the coding optimization
space and then globally searching for the best mode for each
coding sequence. Accordingly, the more discrete starting points
will lead to better optimization results. However, the search
usually is not efÔ¨Åcient enough for so many modes. On the
other hand, deep learning-based video coding represents the
continuous optimization solution. In mathematics, gradient
descent is based on continuous function. Thus, deep learning-
based video coding uses deep networks to formulate coding
as a continuous optimization problem. Through the numerical
algorithm (e.g. gradient descent),
it optimizes a group of
encoder-decoder parameters efÔ¨Åciently in continuous space.
However, limited by the numerical algorithm and only one
starting point, it is easy to fall into the local optimum.

We would like to ask whether discrete and continuous opti-
mization can be combined together. The continuous method
can optimize more efÔ¨Åciently by numerical algorithms, but
continuous optimization usually provides only one starting
point. On the other hand, discrete optimization can provide
multiple starting points in the global space. Therefore, we are
going a step further to ask whether we can provide multiple
discrete starting points for continuous optimization to solve
the video coding optimization problem. Based on this idea,
we propose a hybrid of discrete and continuous optimization
video coding. By means of discrete optimization, we provide
various starting points distributed in the global optimization
space. Through continuous optimization, we can optimize the
local optimum around each starting point efÔ¨Åciently. Then we
use the discrete search among those local optimums to obtain
the global optimum. In theory, hybrid optimization is not only
efÔ¨Åcient but also more possible to achieve the globally optimal
solution.

In this paper, we Ô¨Årst analyze and summarize existing
video coding works from the perspective of optimization. We
conclude that existing video coding frameworks adopt discrete
or continuous optimization essentially, and we analyze why
they will lead to the coding gain theoretically. Inspired by
the theoretical analysis, we propose a hybrid optimization
video coding theory, and then design a hybrid optimization
video coding framework. Our framework is built on deep
networks entirely to achieve continuous optimization, and we
add multiple starting points (i.e. multiple modes) to achieve
discrete optimization. We conduct a comprehensive set of
experiments to verify the efÔ¨Åciency of our hybrid optimization.
Experimental results show that compared to the continuous
optimization framework, our proposed method outperforms the
pure end-to-end learned video coding methods. Meanwhile,
compared to discrete optimization framework, our proposed
method achieves comparable performance to HEVC reference
software HM16.10 in PSNR.

The remainder of this paper is organized as follows. In
Section II, we review the existing video coding works from
the perspective of optimization. In Section III, we theoretically
analyze the optimization in video coding and then proposes a
hybrid-optimization theory. Section IV presents our designed
hybrid-optimization coding framework guided by that theory.

Section V presents the implementation details and Section VI
presents the hybrid optimization procedure. In Section VII,
we show the experimental results. Section VIII discuss the
beneÔ¨Åts and potential of proposed framework, and Section IX
concludes this paper.

II. REVIEW OF OPTIMIZATION IN VIDEO CODING

In this section, we review the development of rate-distortion
optimization in video compression. Theoretically, the opti-
mization objective in video coding is: minimize distortion D,
subject to a constraint Rc, on the number of bits used R.
To solve the optimization of video coding, it‚Äôs necessary to
give a coding framework or model (i.e mathematical function
expression) Ô¨Årstly. Based on the model, the solving process
can generally be divided into two steps [11]: (i) ofÔ¨Çine step:
optimize initial parameters of the model from a large number
of training sequences; (ii) online step: striving to further
optimize parameters of the model for each test sequence to
obtain the best coding performance. Therefore, ofÔ¨Çine step
focuses on how to get the general model parameters as initial
optimization points, while online step concentrates on how
to get the special model parameters for each coding sample
as the Ô¨Ånal optimization solution. Although all video coding
optimization algorithms follow them, those algorithms have
different emphasis on the two steps and then adopt different
optimization ideas correspondingly. According to those differ-
ences, the rate-distortion optimization in video compression
can be divided into four categories.

The Ô¨Årst is the optimization in classic block-based hybrid
video coding, which is the earliest and most widely used
optimization scheme. On the ofÔ¨Çine step, it directly sets the
parameters of modes in the framework without optimization.
Then it searches for the optimal mode online from some
designed modes for a given coding sequence. When the
number of modes is small,
is an effective optimization
it
scheme. However, with the increase of modes, the searching
cost becomes higher and higher, making the coding framework
hard to be optimized.

To address this problem, the second kind of optimization
scheme has appeared, which utilizes some numerical algo-
rithms to online optimize some parts of block-based hybrid
video coding more efÔ¨Åciently. The Ô¨Årst and second classes
of optimization concentrate on the online step, so they can
achieve adaptive coding for each sequence. Unfortunately, the
encoders with too high calculation cost at the online step will
seriously affect the practical application of coding. However,
the computational cost at the ofÔ¨Çine step has less effect on the
coding.

Accordingly,

the third kind of optimization scheme has
emerged in block-based hybrid video coding, which adopts
the numerical optimization algorithms at the ofÔ¨Çine step to
achieve more efÔ¨Åcient optimization for some parts. Those
algorithms usually train a deep network to replace a hand-
optimized module of block-based hybrid video coding, so they
are also called deep tools. Numerical optimization can not only
speed up the entire optimization process but also obtain Ô¨Åner
optimization results, so it shows great potential. However, the

3

TABLE I.

REVIEW OF OPTIMIZATION IN VIDEO CODING

Optimization
Space Formulation

Optimization
Method
(OfÔ¨Çine)

Optimization
Method
(Online)

Optimization
Objective

Representative Works

Discrete

Discrete &

Locally Continuous

Discrete &

N/A

N/A

N/A

N/A

Search

Approximate

H.120, H.261, MPEG-1, MPEG-2,
H.263, H.264, H.265

Approximate

H.266 (AfÔ¨Åne ME, ALF, BIO,
CCLM), AIF, LM, LIC

Search

Numerical

Search

Locally Continuous

Numerical

N/A

Approximate

Deep Tools, H.266 (MIP)

Globally Continuous

Numerical

N/A

End-to-End

End-to-End Learned Video Coding

Category

Classic Block-based Hybrid
Video Coding
(1984-now)

Block-based Hybrid Video Coding
with
Online Numerical Optimization
(1996-now)

Block-based Hybrid Video Coding
with
Deep Tools
(2015-now)

End-to-End Learned
Image/Video Coding
(2015-now)

deep tools only use numerical optimization on some parts of
the coding framework. In order to fully explore the potential
of numerical optimization, it is natural to think whether all the
search-based modules in the coding framework can be replaced
with numerical optimization ones.

Then the fourth kind of optimization scheme was proposed,
in which the whole coding framework adopts the numerical
optimization algorithms at the ofÔ¨Çine step. Those frameworks
are usually on top of deep networks. What‚Äôs more,
those
frameworks are end-to-end trained at
the ofÔ¨Çine step and
directly infer the coding results for each sequence at the online
step, so they are also called end-to-end learned video coding.
In the following, we analyze the four categories of rate-
distortion optimization in video compression and correspond-
ing coding technologies in detail. The characteristic of each
category is summarized in Table I, and the representative works
of each category are analyzed in detail in Table II.

A. Classic Block-based Hybrid Video Coding

Motion video data consists essentially of a time-ordered
sequence of pictures. The most straightforward method of
compressing video content is to compress each picture simply
using an image-coding syntax. This method uses only one
image-coding mode to deal with all video content. However,
much of the depicted scene is essentially just repeated in
picture after picture without any signiÔ¨Åcant change. Image
coding for video data doesn‚Äôt
take advantage of temporal
redundancy.

A simple solution is coding only the changes in a video
scene. Based on this idea, the Ô¨Årst international digital video
coding standard H.120 [12] was developed by the ITU-T
organization and received Ô¨Ånal approval in 1984. H.120 ofÔ¨Çine
designs a conditional replenishment (CR) method to selectively
code the change. CR consists of two coding modes: SKIP and
INTRA mode. The SKIP mode is sending signals to indicate
which areas are repeated, and the INTRA mode is sending new
coded information to replace the changed areas. Accordingly,
the INTRA mode is used to handle the large scene changes,

while the SKIP mode can effectively deal with the repeated
scenes. CR allows searching the best choice between two
modes of representation for each area online. Although CR
is very simple in the algorithm, its optimization ideas lay
the foundation for later video coding standards. That is, CR
introduces multi-mode optimization, which designs various
modes for different situations and searches for the best mode
among those discrete modes. At this time, the optimization
space in CR is still small because it only has two modes.

CR coding only allows exact repetition or complete replace-
ment, but the content of a prior picture often can be a good
approximation. Based on this idea, the Ô¨Årst practical success
video coding standard H.261 [13] is approved by the ITU-T
in early 1991, which can be capable of operation at affordable
telecom bit rates. SpeciÔ¨Åcally, the motion estimation (ME) in
the encoder searches for the best integer spatial displacement.
The motion-compensated prediction (MCP) uses the spatial
displacement of the prior picture to form an approximation
prediction. Next, the displaced frame difference (DFD) coding
method codes the resulting difference to reÔ¨Åne the MCP
signal. Meanwhile, the processing unit is based on the block
(16 √ó 16 macroblock in MCP and 8 √ó 8 block in DCT).
Accordingly, the basic coding structure in H.261 is called
block-based hybrid video coding due to its construction as
a hybrid of motion-handling and picture-coding techniques at
the block level. H.261 was the Ô¨Årst standard to use the basic
typical structure we Ô¨Ånd still predominant today. Compared to
H.120, H.261 still uses multi-mode optimization but greatly
enlarges the optimization space. Because in addition to CR
mode choices, the ME and MCP modes provide a large amount
of discrete motion vector (MV) choices. With the help of more
mode choices, the representative ability of the ofÔ¨Çine designed
model improves signiÔ¨Åcantly. Then the better coding choice
is searched for online among those choices, so the coding
performance is improved.

Based on the H.261 hybrid coding framework and multi-
mode optimization, a series of standards were approved sub-
sequently, including MPEG-1 [14] in 1993, MPEG-2 [15] in

4

TABLE II.

OVERVIEW OF REPRESENTATIVE WORKS IN VIDEO CODING OPTIMIZATION

Method

Year

Optimization
Objective

Bisic idea

Classic Block-based Hybrid Video Coding

H.120

H.261

1984

1991

MPEG-1, MPEG-2,
H.263

1993, 1994,
1996

Rate-Distortion
Optimization

1998

H.264, H.265

2003, 2012

D

D

D

RD

RD

Propose the CR scheme with 2-mode searching, and
introduce the multi-modes optimization idea for the Ô¨Årst time

Introduce the MCP mode with multiply MV choices, and build
the block-based hybrid video coding framework for the Ô¨Årst time

Introduce more modes, such as half-pixel motion, variable block sizes, etc,
but still consider only distortion in the optimization objective

Consider both rate and distortion in the optimization objective
for the Ô¨Årst time

Introduce more modes, such as quarter-pixel motion, merge, etc,
and adopt RD optimization but use approximate RD cost in ME

Block-based Hybrid Video Coding with Online Numerical Optimization

Accelerate Classic ME

Accelerate AfÔ¨Åne ME

Linear Model
(Linear Prediction,
LIC, CCLM)

Adaptive
Interpolation Filter

Adaptive
Loop Filter

Bi-Directional
Optical Flow

Filtering Tools

Prediction Tools

Transform Tools

Entropy Coding Tools

Down- and Up-
Sampling Tools

MIP

Accelerate Block
Partition/Mode Decision

Variable-Rate RNN-
based Image Coding

RD Optimization-
based Image Coding

Deep Video
Compression (DVC)

Hierarchical
Learned Video
Compression (HLVC)

Feature Video
Compression (FVC)

Online Encoder
Updating

Resolution-adaptive
Flow Coding

1996

2018

1998,
2015,
2018

2003

2008

2010

2015

2018

2018

2017

2018

2018

2016

2015

2017

2019

2020

2021

2020

2020

RD

RD

RD

RD

RD

RD

Search for MV towards the direction of gradient descent

Optimize afÔ¨Åne parameters towards the direction of
gradient descent in a high dimensional continuous space

Optimize the linear model by least squares method to predict current pixel
from the neighboring pixels, or the reference pixels, or another component pixels

Optimize the Wiener Ô¨Ålter by least squares method to perform adaptive
sub-pixel interpolation on the reference frame

Optimize the Wiener Ô¨Ålter by least squares method to perform adaptive
loop Ô¨Åltering on the current reconstructed frame

Calculate the analytic solution of optical Ô¨Çow gradient to reÔ¨Åne the bi-prediction

Block-based Hybrid Video Coding with Deep Tools

D

RD

RD

R

RD

RD

Optimize DNN to reÔ¨Åne the reconstructed frame

Optimize DNN to perform or reÔ¨Åne the intra/inter prediction

Optimize DNN to perform the transform

Optimize DNN to predict the probability distribution based on the context

Optimize DNN to achieve the adaptive resolution coding

Optimize simpliÔ¨Åed DNN in H.266 to predict the current block from reference pixels.
This is the Ô¨Årst deep tool adopted by video coding standards.

Accuracy

Optimized DNN to decide the CU partition result directly
or to reduce the mode searching range

End-to-End Learned Image/Video Coding

D

RD

RD

RD

RD

RD

RD

End-to-end optimize DNN for image compression for the
Ô¨Årst time, but use only distortion in the optimization objective

End-to-end optimize DNN for image compression by RD cost

End-to-end optimize DNN for video compression for the Ô¨Årst time

End-to-end optimize DNN for B-frame compression with hierarchical quality layers

End-to-end optimize DNN for video compression in the feature space

Online end-to-end optimize the DNN-based encoder in deep video compression
for each test sequence

Online search from multi-resolution modes in deep video compression

1994, and H.263 [16] in early 1996. They add more advanced
modes, such as half-pixel motion mode and bi-prediction mode
in MPEG-1, variable block-size motion compensation mode
(16 √ó 16 and 8 √ó 8) in H.263, and so on. More modes expand
the optimization space further, so the coding performance is
improved further. However, in all those standards, the multi-
mode search criterion just considers distortion but not rate, so
the optimization objective is approximate.

To address this problem, the rate-distortion optimization
method [11], [17] was proposed in 1998, which considers both
the rate and distortion in the optimization objective. These
works theoretically analyze that the video coding needs to
optimize both the rate and distortion, and then they introduce
the Lagrangian method to solve the optimization problem ef-
fectively. The Lagrangian optimization changes the constrained
optimization problem into an unconstrained problem so that
the optimization objective becomes minimizing the Lagrangian
rate-distortion cost D + ŒªR. This rate-distortion optimization
has a signiÔ¨Åcant impact on video coding. All later video coding
standards adopt this RD optimization, such as H.264 [18],
H.265 [19], and so on. Those standards contain more modes
and use RD optimization to search for the best one effectively.
In the speciÔ¨Åc implementation, the optimization objective in
most modes is accurate, where R and D are based on the Ô¨Ånal
stream and reconstructed frame, respectively. However, in ME,
due to the limitation of computational power, the optimization
objective is approximated, i.e., R and D are based on the MV
bits and prediction frame. Therefore, the optimization objective
in those standards is still approximate.

From the above review, we can Ô¨Ånd that the mentioned
block-based hybrid video coding standards have some same
characteristics in optimization. First, all the ofÔ¨Çine designed
models in those standards consist of many discrete modes [20],
[21], including INTRA mode, SKIP mode, partition mode,
the
MCP mode with lots of MV choices, etc. Therefore,
optimization parameter space in those standards is discrete.
Second, at the ofÔ¨Çine step, the model parameters are almost
not optimized but hand-crafted. It makes efforts to online
search for the optimal mode from the discrete modes for each
sequence. Thus, the optimization method is online searching.
Third, the optimization objective is approximate rather than
actual RD cost.

Although this multi-mode search optimization has achieved
great success, there are still some problems. First, with the
increase of modes, the cost of searching has increased signif-
icantly, making the framework hard to be optimized. Second,
search optimization can only conduct on a Ô¨Ånite number of
discrete modes. Considering the searching complexity, it can‚Äôt
perform more reÔ¨Åned optimization on the parameters.

B. Block-based Hybrid Video Coding with Online Numerical
Optimization

To address the inefÔ¨Åcient problem of search optimization,
some online numerical optimization methods appeared, which
replace part of the search optimization in block-based hybrid
video coding. Compared to search optimization, numerical
optimization [22] not only is more efÔ¨Åcient but also makes

5

the optimization parameters more reÔ¨Åned. According to the
two advantages, the existing numerical optimization methods
can be divided into two categories. The Ô¨Årst is for improving
optimization speed, and the second is to increase compression
efÔ¨Åciency.

The Ô¨Årst is mainly to use gradient descent-based method to
accelerate the multi-modes search. In the classical block-based
hybrid coding, it is difÔ¨Åcult to establish a simple mathematical
model
to associate different characteristic modes, such as
INTRA mode, SKIP mode, and MCP mode. However, the
different MV choices within MCP mode and RD cost can be
easily modeled with the mathematical function. Based on the
characteristic, in 1996, an online gradient descent optimization
algorithm for ME was proposed [23]. It calculates the gradient
of the objective function to MV, and the search for MV always
moves in the direction of optimal gradient descent. Compared
to the exhaustive search on discrete points, gradient-based
optimization can fully use the continuous characteristics and
exclude many unnecessary search points. Accordingly, it can
conduct ME more efÔ¨Åciently. Later on, a series of gradient
descent-based optimization for ME was developed [24]‚Äì[26].
However, because the search points of classical MVs are
not many enough, search-based optimization can still handle
ME well. Thus, gradient descent-based optimization has not
attracted widespread attention.

Until recent years, it‚Äôs hard for the classical translational
motion model to improve performance further, so the more
advanced afÔ¨Åne motion model [27] has been adopted in the
latest video coding standards H.266/VVC [28], [29]. The
afÔ¨Åne motion parameters are no longer simple MVs, but more
complex, larger range and higher precision control parameters,
which is more difÔ¨Åcult for the classical search-based ME.
To efÔ¨Åciently optimize the afÔ¨Åne motion parameters, online
gradient descent-based ME plays a key role. The gradient-
based method can not only handle the parameters in a high
dimensional continuous space [30], such as afÔ¨Åne parameters,
but also quickly optimize the best parameters with only a
few iterations [27]. However, the optimization objective is still
approximate like the classical ME.

The second category is to calculate the analytic solution by
some numerical methods to perform more reÔ¨Åned optimization,
including least-squares optimization and others. The least-
squares optimization is to Ô¨Åt the mapping relationship with the
smallest error for each group of signals. It is mainly applied
in adaptive prediction and Ô¨Ålter in video coding. As for the
adaptive prediction, those works use least squares to online
optimize the linear model (LM) on pixels within or across
the component. Those works build the linear model to predict
the current pixel from the neighboring pixels (called linear
prediction) [31]‚Äì[33], or the reference pixels (called local
illumination compensation (LIC)) [34], or another component
pixels (called cross-component linear model (CCLM)) [35].

As for the Ô¨Ålter, the work is to use least squares to optimize
the Wiener Ô¨Ålter to conduct adaptive Ô¨Åltering. But the Ô¨Ålter
are operated on different frames, which is either on the
reference frame [36]‚Äì[38] (called adaptive interpolation Ô¨Ålter
(AIF)) or in the current reconstructed frame [39], [40] (called
adaptive loop Ô¨Ålter (ALF)). AIF uses some Wiener Ô¨Ålters

6

to interpolate multiply sub-pixel images adaptively from the
reference frames, and then those frames are used by MC to
generate a more accurate prediction. ALF uses the Wiener
Ô¨Ålters to enhance the current reconstructed frame adaptively.
By efÔ¨Åciently and Ô¨Ånely online optimizing the Wiener Ô¨Ålter
with the least squares method, AIF and ALF signiÔ¨Åcantly
improve coding performance. Meanwhile, ALF also is adopted
by H.266 standard as a core tool.

In addition to those mentioned technologies, numerical
methods also are applied in other technologies in H.266, such
as the bi-directional optical Ô¨Çow (BDOF) [41]. BDOF uses
the optical Ô¨Çow concept to reÔ¨Åne the bi-prediction signal.
It uses numerical methods to online calculate the analytic
solution of the optical Ô¨Çow gradient and then derives the
reÔ¨Ånement. Compared to search optimization on some discrete
points, numerical methods can online optimize the parameters
of models (e.g. the linear model, adaptive Ô¨Ålters, and optical
Ô¨Çow model) in the continuous space to achieve more reÔ¨Åned
optimization.

From those works, we can Ô¨Ånd that the mentioned methods
have some same characteristics in optimization. First, these
ofÔ¨Çine designed models are still based on discrete modes,
that is, following block-based hybrid video coding, but they
choose some highly correlated choices within a mode to make
them locally continuous. Second, the model optimization still
focuses on the online step. It still uses the search-based method
for discrete modes, while it uses the numerical method for
continuous parts to achieve efÔ¨Åcient and reÔ¨Åned optimization.
Third, the optimization objective is approximate because those
tools only considers the RD cost of their local parts rather than
the whole RD cost, such as the prediction RD cost for gradient
descent-based ME and prediction RD cost for AIF.

Although online numerical optimization is more efÔ¨Åcient
than search optimization, the optimization still focuses on the
online step. The high computational cost of online optimization
will limit the wide application of the codec.

C. Block-based Hybrid Video Coding with Deep Tools

Compared to online numerical optimization, the compu-
tational cost of ofÔ¨Çine optimization has less effect on the
coding. Correspondingly, some ofÔ¨Çine numerical optimization
methods appeared. It not only retains the two advantages of
efÔ¨Åcient and reÔ¨Åned optimization in numerical optimization,
but also changes the optimization stage from online to ofÔ¨Çine.
Those algorithms usually take ofÔ¨Çine trained deep networks
as tools of block-based hybrid video coding, and then use the
gradient descent-based numerical method to optimize the net-
work. Similarly, according to the two advantages of numerical
optimization, the existing deep tools can also be categorized
into two groups [42]. The Ô¨Årst group is to increase compression
efÔ¨Åciency, and the second is to improve encoding speed.

The Ô¨Årst group of deep tools may either replace the cor-
responding hand-optimized module with a deep network, or
newly add a deep network into the scheme. It involves vari-
ous tools, including prediction tools, transform tools, entropy
coding tools, Ô¨Åltering tools, down- and up-sampling tools, and
so on. Since some online numerically optimized tools have

been developed, the straightforward idea is to replace those
online tools with ofÔ¨Çine optimized networks. Among those
tools, the Ô¨Åltering tool improves compression efÔ¨Åciency most
signiÔ¨Åcantly. Correspondingly, the deep network with ofÔ¨Çine
numerical optimization was Ô¨Årst applied in the reconstruction
Ô¨Åltering module in video coding. In 2015, Dong et al. [9]
propose a CNN for compression artifacts reduction on JPEG,
namely ARCNN. They ofÔ¨Çine designed the network with 4
convocation layers and optimized the parameters with the
gradient descent method through lots of training samples. In
the test, ARCNN doesn‚Äôt need online optimization, which
directly infers the reÔ¨Åned image. Experimental results show
that ARCNN achieves incredible performance, i.e. more than
1dB improvement in PSNR than JPEG. ARCNN proved the
feasibility of deep tools with ofÔ¨Çine numerical optimization
and demonstrated its great potential. Subsequently, a series of
deep Ô¨Åltering tools are proposed [43]‚Äì[47], which occupies the
majority of the deep tools. Although the works have different
network structures, their optimization ideas are the same as
that of ARCNN.

Inspired by the great success of deep Ô¨Åltering tools, other
deep tools have been proposed [48]‚Äì[62]. Some of them
replace other online numerically optimized tools with ofÔ¨Çine
optimized networks, and some develop new tools. For example,
for the online linear prediction tool, Li et al. [53] propose an
ofÔ¨Çine optimized fully connected network for intra prediction,
which also predicts the current pixel from neighboring pixels.
Similar to the idea of LIC, Huo et al. [54] propose an ofÔ¨Çine
optimized CNN to reÔ¨Åne the inter prediction signal. Analogous
to CCLM, Li et al. [55] propose an ofÔ¨Çine optimized CNN to
perform cross-channel prediction. Like AIF, Yan et al. [56],
[57] propose to take ofÔ¨Çine optimized CNNs as Ô¨Ålters to
perform sub-pixel interpolation on the reference frames. In
addition to these existing tools with numerical optimization,
there are also many newly developed ofÔ¨Çine optimized deep
tools. For example, for the prediction module, Huo et al.
[58] propose a CNN-based new inter prediction mechanism
by extrapolating the current frame from the multiple reference
frames and taking it as another reference frame. For the trans-
form module, Liu et al. [59] propose a CNN-based method
to achieve a DCT-like transform for image coding. For the
entropy coding module, Song et al. [60] optimize a CNN to
predict the probability distribution of the intra prediction mode
based on the context. For the down- and up-sampling coding
module, Li et al. [61], [62] propose a CNN-based down- and
up-sampling method to achieve the adaptive resolution coding.
Due to the high compression efÔ¨Åciency, H.266 standard
has adopted an ofÔ¨Çine numerical optimization-based tool, i.e.
matrix-based intra-picture prediction (MIP) [48]. MIP uses the
neural network to predict the current block from the reference
samples. The neural network has been simpliÔ¨Åed to a matrix-
vector multiplication, with the matrix being selected from a set
of pre-trained matrices and the vector being constructed from
the reference samples. In order to ofÔ¨Çine optimize the matrix
coefÔ¨Åcients from lots of training samples, a gradient descent-
based numerical algorithm (Adam [63]) has been used.

The second group of deep tools is intended for improving
encoding speed [64]‚Äì[68]. Based on the analysis in Section

II-B, the choice within a speciÔ¨Åc mode and its RD cost can
be easily modeled with the mathematical function. Among the
core coding modes, the MCP mode and the block partition
mode have the most internal choices, i.e. multiply MV choices
in MCP mode and various block combination choices in
partition mode. However, due to the complexity of motion and
the massive MV choices, it is difÔ¨Åcult for the ofÔ¨Çine optimized
network to predict MV accurately. Therefore, only a few
ofÔ¨Çine numerical optimization methods accelerate MCP mode
decision [69], and most methods focus on accelerating partition
choice selection. In 2016, Liu et al. [64] presented an ofÔ¨Çine
optimized CNN to help decide CU partition mode for HEVC
intra encoder for the Ô¨Årst
time. Without searching among
multiply partition choices, the trained CNN will directly decide
whether to split CU or not based on the content and the
speciÔ¨Åed QP.

From those works, we can Ô¨Ånd that the mentioned methods
have some same characteristics in optimization. First, these
ofÔ¨Çine designed models are still based on discrete modes, i.e.
following block-based hybrid video coding, but they make
part modes or tools locally continuous. Second, the model
optimization is not only on the online step but also on
the ofÔ¨Çine step. The continuous parts are optimized ofÔ¨Çine
by the numerical method, while the discrete parts are opti-
mized online by searching. Third, the optimization objective
is approximate, because the optimization objective of those
deep tools is based on either the accuracy (e.g. fast partition
decision), or only D (e.g. ARCNN), or only R (e.g. CNN-
based entropy coding), or part RD (e.g. CNN-based residual
transform), rather than the actual RD cost.

Numerical optimization can not only speed up the opti-
mization process but also perform more reÔ¨Åned optimization.
However, the deep tools only use numerical optimization on
some parts of the coding framework. We would like to ask
whether the whole coding framework can be optimized with
numerical methods.

D. End-to-End Learned Image/Video Coding

Based on the idea of the whole coding framework with nu-
merical optimization, end-to-end learned image/video coding
methods were proposed. The coding model is built on top
of deep networks entirely. At the ofÔ¨Çine step, one group of
average optimal model parameters are end-to-end optimized
by numerical algorithms through lots of training samples. At
the online step, the ofÔ¨Çine optimal model usually directly
infers the coding results for each sequence, which is not
optimized further. Compared to deep tools with local numerical
optimization, end-to-end learned coding can perform more ef-
Ô¨Åcient and more reÔ¨Åned optimization for the whole framework.
Meanwhile, the whole framework takes the actual RD cost as
the optimization objective and unites all numerical modules to
optimize in an end-to-end manner. Next, we will review the
end-to-end learned image and video coding in turn.

Since image coding is simpler than video coding, the end-
to-end learned method was initially tried in image coding.
In 2015, Toderici et al. [10] proposed an ofÔ¨Çine end-to-
end optimized RNN-based framework for variable rate image

7

compression. They use binary quantization to generate codes,
and do not consider the rate during ofÔ¨Çine training, i.e. the
optimization objective is only end-to-end distortion. This work
builds a coding framework with all numerical optimization
modules for the Ô¨Årst time and initially veriÔ¨Åes the feasibility.
However, the optimization objective only based on distortion
limits the improvement of efÔ¨Åciency.

To address this problem, many rate-distortion optimized
end-to-end image compression frameworks are proposed [70]‚Äì
[77]. The most representative works of CNN-based methods
are from Ball¬¥e et al. [70]‚Äì[72]. Ball¬¥e et al. proposed various
end-to-end RD optimized image coding frameworks, using
the factorized-prior [70], hyper-prior [71] and autoregressive
prior [72] models to estimate the entropy of the latent feature
efÔ¨Åciently. What‚Äôs more, Ma et al. [75] introduce a special end-
to-end framework for both lossy and lossless image compres-
sion, which adopts a trained wavelet-like transform to generate
coefÔ¨Åcients and a context-based entropy model to code those
coefÔ¨Åcients effectively. Those works take the RD cost as
the objective to end-to-end optimize the whole image coding
frameworks with numerical methods. Experiment results show
that the latest end-to-end image coding method has achieved
competitive performance with H.266, which fully veriÔ¨Åed
the feasibility of end-to-end RD optimization with numerical
methods.

Inspired by the successes in end-to-end image coding,
some end-to-end learned video coding methods have been
proposed [78]‚Äì[91]. Different from image coding that removes
the spatial redundant, video coding should also remove the
temporal redundant. For this purpose, inter-picture prediction is
a critical issue in video coding. Accordingly, end-to-end video
coding is more complex than image coding. In 2018, Wu et al.
[79] proposed a recurrent neural network (RNN) based video
compression approach. To code the B frame, it performs ME
and then takes the obtained motion information to interpolate
from two reference frames. However, the ME in this work is
not carried out by learned networks, but by classical search
methods. Therefore, this work is not end-to-end optimized.

Later on, a real end-to-end deep video compression frame-
work (DVC) [81], [82] was proposed by Lu et al. for the
Ô¨Årst time in 2019. All modules in the framework are based
on deep networks, including ME, MC, motion information
compression, and residual compression modules. The entire
network is jointly ofÔ¨Çine optimized by numerical methods with
a single loss function, i.e., the joint rate-distortion cost. Based
on this optimization idea, a series of end-to-end optimized
video compression methods were proposed [83]‚Äì[92]. For
example, Yang et al. [87] proposed a hierarchical end-to-
end learned video compression (HLVC) for B-frame coding,
which introduces hierarchical quality layers and a recurrent
enhancement network to make full use of the temporal corre-
lation. Hu et al. [91] proposed an end-to-end optimized feature-
space video coding framework (FVC). It performs all major
operations (i.e. ME, MC, motion compression, and residual
compression) in the feature space.

From those works, we can Ô¨Ånd that the mentioned methods
have some same characteristics in optimization. First, these
ofÔ¨Çine designed models are based on continuous function

8

Fig. 1.

Illustration of encoder-decoder in video coding.

entirely, so the optimization parameter space is continuous.
Second, the models are optimized only on the ofÔ¨Çine step,
and they directly infer the coding results on the online step.
They use the numerical method to optimize the continuous
model ofÔ¨Çine. Third, the optimization objective is the actual
RD cost, so the optimization is end-to-end.

Based on all the above analyses in this section, we found that
different kinds of coding frameworks use different optimiza-
tion methods, and different optimization methods also have
different advantages. First, ofÔ¨Çine optimization can reduce the
computational cost, and online optimization can achieve adap-
tive coding for each video. Second, numerical optimization in
continuous space is more efÔ¨Åcient and reÔ¨Åned in local parts,
and search optimization in discrete space can Ô¨Ånd better coding
parameters globally among multiple modes. Third, it is more
efÔ¨Åcient to end-to-end optimize with the actual RD cost. We
can Ô¨Ånd that using only one optimization method can not
achieve all the advantages. Therefore, we would like to ask
whether those optimization methods can be combined together
to address this problem.

Based on this idea, a few coding methods have emerged
[93], [94]. To make end-to-end learned coding adaptive for
each video, Lu et al. [93] conduct online end-to-end numerical
optimization on the ofÔ¨Çine end-to-end trained DVC model.
It optimizes the motion information (i.e. optical Ô¨Çow) with
gradient-descent methods at the encoder side online without
changing the decoder. However, online gradient-descent opti-
mization starting from only one group of ofÔ¨Çine trained model
parameters may fail into the local minima. Hu et al. [94] add
multi-resolution modes search optimization into ofÔ¨Çine end-
to-end trained DVC to improve Ô¨Çow coding efÔ¨Åciency. Based
on the ofÔ¨Çine numerically optimized model, this work online
searches for the mode with the lowest RD cost from a few
resolution Ô¨Çow modes. The optimization in this method is
end-to-end, i.e. with the actual RD cost. However, this work
performs numerical optimization only at the ofÔ¨Çine step, but
doesn‚Äôt conduct online to achieve more reÔ¨Åned optimization.
Meanwhile, the number of modes is too few, making it hard
to search for the globally optimal parameters.

III. THEORETICAL ANALYSIS OF OPTIMIZATION

A. Optimization Problem Formulation

The theoretical foundations of lossy compression are rooted
in Shannon‚Äôs seminal work on rate-distortion theory [95],
[96]. Rate-distortion theory analyzes the fundamental tradeoff

between the rate used for representing samples from a data
source variable X, and the expected distortion incurred in
decoding those samples from their compressed representations.
Formally, the relation between the input variable X (original
signal) and output variable ÀÜX (compressed signal) is a (pos-
sibly stochastic) mapping deÔ¨Åned by conditional distribution
P ( ÀÜX|X). The stochastic mapping P ( ÀÜX|X) is also called a
test channel. Based on those, Shannon‚Äôs theory deÔ¨Åned the
classic optimization problem of rate-distortion. That is, for a
data source variable X, if the expected distortion is restricted
to be bounded by Dc, then it optimizes to obtain the lowest
rate

I(X, ÀÜX) s.t. E[D(X, ÀÜX)] ‚â§ Dc

(1)

min
P ( ÀÜX|X)

where I denotes mutual information. For the video coding
is implemented by an encoder-
problem,
decoder pair F , i.e. ÀÜX = F (X). The encoder-decoder pair F is
a deterministic mapping (a special case of stochastic mapping)

the test channel

P ( ÀÜX|X) =

(cid:26)

1, if ÀÜX = F(X)
0, otherwise

(2)

The input X must be mapped to output ÀÜX and cannot
be mapped to other outputs, so the conditional probability
P ( ÀÜX|X) is either 0 or 1. Because the encoder-decoder space
F is huge, it is usually necessary to set the hyper-parameters
of the encoder-decoder Ô¨Årst, i.e. designing model structure
f , to determine the optimized parameter space. The hyper-
parameters are not optimized, while the parameters Œò in the
model structure need to be optimized in space ‚Ñ¶. Thus, the F
can be converted to a parametrical expression f (Œò)

F (X) (cid:44) f (X|Œò), Œò ‚àà ‚Ñ¶

(3)

The model‚Äôs parameters Œò consists of the encoder‚Äôs parameters
Œòenc and the decoder‚Äôs parameters Œòdec

Œò = Œòenc and Œòdec

(4)

Accordingly, the optimization problem Eq.(1) can be expressed
as

I(X, ÀÜX) s.t. E[D(X, ÀÜX)] ‚â§ Dc

(5)

min
Œò‚àà‚Ñ¶

the mutual

in-
According to Shannon‚Äô information theory,
formation I(X, ÀÜX) is equal
to the difference between the
entropy H( ÀÜX) with the conditional entropy H( ÀÜX|X). Due
to the deÔ¨Ånition of entropy and the deterministic mapping in
Eq.(2), the conditional entropy H( ÀÜX|X) must be equal to 0,
i.e. H( ÀÜX|X) = 0. Thus, the the mutual information I(X, ÀÜX)
is equal to the entropy H( ÀÜX)

I(X, ÀÜX) = H( ÀÜX) ‚àí H( ÀÜX|X)
= H( ÀÜX)

(6)

In the practical coding, it uses the syntax to express the
compressed signal. SpeciÔ¨Åcally, the original signal X is input
into the encoder to generate the syntax Z, and then the syntax
Z is input into the decoder to obtain the compressed signal
ÀÜX, as shown in Fig.1.

Z = fenc(X; Œòenc); ÀÜX = fdec(Z; Œòdec)

(7)

ùëìùëíùëõùëê(Œòùëíùëõùëê)ùëìùëëùëíùëê(Œòùëëùëíùëê)ùëãùëç‡∑†ùëãùëì(Œò)Specially, for the determined decoder fdec(Œòdec), one syntax
Z products only one decided output ÀÜX, but one ÀÜX may come
from multiple syntax Z. Thus, fdec(Œòdec) may be not one-to-
one mapping (not injective function). Accordingly, H(Z) is
the upper bound of H( ÀÜX)

H( ÀÜX) ‚â§ H(Z)

(8)

Since it is hard to measure H( ÀÜX), to minimize H( ÀÜX), we
can change to minimize its upper bound H(Z). We denote the
probability distribution of the variable Z as Q, i.e. Z (cid:118) Q,
and the entropy H(Z) is

H(Z) = ‚àí

(cid:88)

j

Q(zj)logQ(zj)

(9)

However, the true probability distribution Q usually is un-
known, so it uses an estimated probability distribution B to
approximate Q to code Z.

H(Q, B) = ‚àí

(cid:88)

j

‚â• H(Z)

Q(zj)logB(zj)

(10)

H(Q, B) is called cross entropy of between two probability
distributions Q and B. We can Ô¨Ånd that H(Q, B) is the upper
bound of H(Z), so H(Z) can be minimized by minimizing
its upper bound H(Q, B). Accordingly, we convert optimizing
mutual information into optimizing entropy. On the other hand,
the entropy represents the average bit rate. Thus, in the coding
problem, optimizing mutual information is equivalent to op-
timizing the bit rate. We donate the calculation of bit rate as
R(), and R(Z) presents the bit rate of coding syntax Z, i.e. the
cross entropy H(Q, B). Based on above formula derivation,
the RD optimization problem Eq.(5) can be expressed as

R(Z) s.t. E[D(X, ÀÜX)] ‚â§ Dc

(11)

min
Œò‚àà‚Ñ¶

For a data source variable X, if the expected distortion is
restricted to be bounded by Dc, then the encoder-decoder
model parameters Œò are optimized to obtain the lowest bit
rate of coding syntax Z, where syntax Z is the output of the
encoder and used for decoding the compressed signal ÀÜX.

In the above derivation of Shannon‚Äôs theory, the source is
regarded as a random variable, which describes the source as a
probability distribution. However, in practical video coding, the
source is the samples of the distribution. Due to this difference,
to adapt to the practical samples‚Äô coding, some limited condi-
tions are usually added to the theoretical formula. Accordingly,
the optimization problem will be changed. According to the
amount of coding samples, the practical optimization problem
can be categorized into two groups.

The Ô¨Årst is to optimize the encoder-decoder parameters Œò
for lots of samples. The set of samples can be considered to
represent the distribution of X. To minimize the loss on the set
of samples, an average optimal encoder-decoder parameters Œò
is optimized. SpeciÔ¨Åcally, a total of K samples x1, ¬∑ ¬∑ ¬∑ , xK
generate K groups of syntax z1, ¬∑ ¬∑ ¬∑ , zK through the encoder,

9

and then the syntax generate K corresponding compressed
samples ÀÜx1, ¬∑ ¬∑ ¬∑ , ÀÜxK, respectively

zk = fenc(xk; Œòenc); ÀÜxk = fdec(zk; Œòdec); k = 1, ¬∑ ¬∑ ¬∑ , K

(12)
For the set of samples, the rate part becomes the upper bound
of bit rate for coding all samples

R(zk) = ‚àílogB(zk); k = 1, ¬∑ ¬∑ ¬∑ , K

(13)

The distortion part becomes

E[D(X, ÀÜX)]|X=x1,¬∑¬∑¬∑ ,xK ; ÀÜX=ÀÜx1,¬∑¬∑¬∑ ,ÀÜxK

=

1
K

K
(cid:88)

D(xk, ÀÜxk)

k=1

(14)
the RD optimization problem

Based on those equations,
Eq.(11) can be expressed as

min
Œò‚àà‚Ñ¶

K
(cid:88)

k=1

R(zk) s.t.

1
K

K
(cid:88)

k=1

D(xk, ÀÜxk) ‚â§ Dc

(15)

For lots of source samples, if the average distortion is restricted
to be bounded by Dc, then it optimizes an average optimal
encoder-decoder model parameter group Œò to obtain the lowest
sum of bits of coding all samples‚Äô syntax. The average optimal
parameters can be directly deployed on the encoder/decoder
side for encoding/decoding all the samples.

The second is to optimize for only one given sample. Dif-
ferent from the average optimum for lots of samples, for each
given sample, we can optimize a group of optimal encoder-
decoder parameters Œò. However, as for the optimal decoder
parameters Œòdec for each sample, deploying parameters Œòdec
for all samples on the decoder side will take up lots of storage,
while transmitting them to the decoder side will pay lots of
bits. Therefore, for the optimization of one given sample, it
usually needs to Ô¨Åx the decoder parameters Œòdec. It only opti-
mizes the encoder parameters Œòenc for each sample to match
decoder parameters Œòdec to achieve the best performance.
SpeciÔ¨Åcally, a given sample x, the syntax z, compressed signal
ÀÜx, rate R(z), and distortion can be calculated by

z = fenc(x; Œòenc); ÀÜx = fdec(z; Œòdec)
R(z) = ‚àílogB(z)
E[D(X, ÀÜX)]|X=x, ÀÜX=ÀÜx = D(x, ÀÜx)

(16)

Based on those equations,
Eq.(11) can be expressed as

the RD optimization problem

min
Œòenc‚àà‚Ñ¶enc

R(z) s.t. D(x, ÀÜx) ‚â§ Dc

(17)

where the ‚Ñ¶enc is the optimization space of Œòenc. For a given
sample, if the distortion is restricted to be bounded by Dc, the
decoder parameters Œòdec is Ô¨Åxed and the encoder parameters
Œòenc are optimized to obtain the lowest bit rate of coding this
sample‚Äôs syntax.

To solve this kind of constrained optimization problem, the
discrete version of Lagrange multipliers [97] can be used to
convert it into an unconstrained problem. The optimized goal

10

is converted to minimizing the rate-distortion cost J of the
optimized parameters Œò. Thus,

At the online step, it searches the best encoder parameters
Œò‚àó

enc from the set Œ® for the coded sample x

Œò‚àó = arg min

J, J = D + ŒªR

(18)

Œò‚àà‚Ñ¶

where Œò‚àó donates the theoretically optimal parameters.

Although there are two coding optimization problems (i.e.
for lots of samples or a single sample), the ultimate goal is to
optimize the best parameters for each coding sequence. The
optimization for lots of samples also serves for the optimiza-
tion of a single sample. Due to the complexity of coding
problems, although the hyper-model is given, the parameter
space for optimization is by far too large to be evaluated. Thus,
the optimization is non-convex and difÔ¨Åcult to be solved. Over
the years, a large number of studies have strived to solve the
video coding optimization problem approximately.

B. Optimization Problem Solution

For given a video sequence x, the solution to the opti-
mization problem of this samples is generally divided into
two steps: ofÔ¨Çine determining the optimization space and
optimizing the initial parameters through lots of samples, and
online optimizing the parameters based on the ofÔ¨Çine model
for the coded sample. In the speciÔ¨Åc implementation, there are
mainly two popular optimization solutions.

The one is still regarding the optimization of the test
channel as a discrete optimization problem, and mainly using
search-based optimization to solve it. The typical example
is block-based hybrid coding. To explain the optimization
for a given data x to be coded more clearly, we take the
model‚Äôs parameters Œò as the abscissa and the corresponding
rate-distortion cost J as the ordinate, and draw the simpliÔ¨Åed
optimization diagram as shown in Fig.2 (a).

At the ofÔ¨Çine step, it designs a model structure to determine
the whole discrete optimization space Œì (Œì ‚àà ‚Ñ¶) Ô¨Årstly.
Discrete optimization space Œì consists of dense parameter
points by discretizing the continuous optimization space ‚Ñ¶.
The discrete optimization space is so large that it is difÔ¨Åcult to
search from such a large number of parameter choices directly.
To address this problem, the discrete space usually needs to
be further discretized (or quantized). SpeciÔ¨Åcally, the whole
discrete optimization space Œì is usually divided into N non-
overlapping subspaces Œì1, ¬∑ ¬∑ ¬∑ , ŒìN

Œì = Œì1 ‚à™ Œì2 ‚à™ ¬∑ ¬∑ ¬∑ ‚à™ ŒìN

(19)

Each subspace Œìi is represented by a discrete optimal point
Œòi that searches through lots of training samples x1, ¬∑ ¬∑ ¬∑ , xK,

Œòi = arg min

Œò‚ààŒìi

K
(cid:88)

k=1

J(xk; Œò)

(20)

dec.

We take the union of all decoder parameters as the optimal
decoder parameters Œò‚àó
Œò‚àó

dec = Œò1 dec ‚à™ Œò2 dec ‚à™ ¬∑ ¬∑ ¬∑ ‚à™ ŒòN dec
Then the whole discrete optimization space is expressed by a
set Œ® with N discrete starting points Œò1 enc, ¬∑ ¬∑ ¬∑ , ŒòN enc

(21)

Œ® = {Œòi enc |i = 1, 2, ¬∑ ¬∑ ¬∑ , N }

(22)

Œò‚àó

enc = arg min
Œòenc‚ààŒ®

J(x; Œò)

(23)

In the speciÔ¨Åc implementation of block-based hybrid coding,
the two optimization steps are realized by ofÔ¨Çine designing
multiple coding modes and setting mode parameters, and
online searching. The discrete optimized space Œì is the Ô¨Ånite
range of all the model‚Äôs parameters, and is divided into N
subspaces by set N modes. There are still many options for
parameters within subspace, and then it chooses the parameter
Œòi as the model‚Äôs parameters in i-th subspace (e.g. for a
quarter MV x-dimension value 0.25,
its discrete subspace
contains any discrete value in the interval [0.125, 0.375], and
it Ô¨Ånally chooses the value 0.25 to represent this subspace).
The search-based optimization searches from multiple starting
points globally so that it‚Äôs more likely to optimize the best
model parameters. However, the computational efÔ¨Åciency of
search optimization is low. In addition, limited by the discrete-
ness of the model‚Äôs parameters and the search complexity, the
search point is not dense enough to perform Ô¨Åner optimization,
so it may not achieve better optimization results in local parts.
The other one is formulating the optimization of model
parameters as a continuous optimization problem, and solving
it with the numerical optimization method. End-to-end learned
coding is a typical example. The simpliÔ¨Åed optimization dia-
gram is shown in Fig.2 (b). At the ofÔ¨Çine step, it designs
continuous functions to modeling the optimization space ‚Ñ¶
and then establishes the continuous relationship between the
optimized parameters Œò and RD cost J. Through minimizing
the cost of lots of samples with the numerical optimization, a
group of average optimal model‚Äôs parameters is obtained

Œò1 = arg min

Œò‚àà‚Ñ¶

K
(cid:88)

k=1

J(xk; Œò)

(24)

where it only optimizes only one group of average optimal
model parameters Œò1. We take the decoder parameters Œò1 dec
as the optimal decoder parameters Œò‚àó
dec. At the online step, for
the coded sample x, it directly applies the initial parameters
Œò1 enc or further optimizes a group of better parameters
around Œò1 enc with the numerical optimization

Œò‚àó

enc = arg min

Œòenc‚àà‚Ñ¶Œò1 enc

J(x; Œò)

(25)

where ‚Ñ¶Œò1 enc represents the convex space where Œò1 enc is.
In the speciÔ¨Åc implementation of end-to-end learned coding,
the two optimization steps are achieved by ofÔ¨Çine designing
a network model and training it with the gradient descent
algorithm, and online direct inference or further Ô¨Åne-tuning
around the ofÔ¨Çine trained model. The continuous optimized
space ‚Ñ¶ is the inÔ¨Ånite range of all network parameters, Œò
is the all network parameters, and Œò1 is the ofÔ¨Çine optimized
network parameters. The numerical optimization in the contin-
uous space can not only solve the optimization problem more
efÔ¨Åciently but also perform Ô¨Åner optimization to make it more
likely to achieve the optimum in the local part. However, most

11

(a)

(b)

(c)

Fig. 2.
hybrid optimization.

Illustration of the optimization solution. (a) Search-based optimization in discrete space. (b) Numerical optimization in continuous space. (c) Proposed

of the numerical optimization are greedy algorithms, such as
the gradient descent algorithm. Those schemes rely heavily on
the starting point since they conduct optimization around it.
Only one starting point will fall into a local optimum.

Based on the two optimization solutions, we propose a
hybrid of discrete and continuous optimization to solve this
problem efÔ¨Åciently. We provide multiple discrete start points
in global continuous space and adopt numerical optimization in
the local part around each starting point. Then we use search-
based optimization among the multiple discrete local optimums
to obtain the global optimum. The hybrid optimization diagram
is shown in Fig.2 (c). At the ofÔ¨Çine step, we Ô¨Årst formulate
the problem as continuous optimization and adopt numerical
optimization on the continuous space ‚Ñ¶ of the designed model.
We divide the whole continuous optimization space into N
non-overlapping continuous subspaces ‚Ñ¶1, ¬∑ ¬∑ ¬∑ , ‚Ñ¶N

‚Ñ¶ = ‚Ñ¶1 ‚à™ ‚Ñ¶2 ‚à™ ¬∑ ¬∑ ¬∑ ‚à™ ‚Ñ¶N

(26)

We optimize an optimal point Œòi for each subspace ‚Ñ¶i through
lots of training samples

Œòi = arg min

Œò‚àà‚Ñ¶i

K
(cid:88)

k=1

J(xk; Œò)

(27)

dec

We take the union of all decoder parameters as the optimal
decoder parameters Œò‚àó
Œò‚àó

dec = Œò1 dec ‚à™ Œò2 dec ‚à™ ¬∑ ¬∑ ¬∑ ‚à™ ŒòN dec
At the online step, for the coded sample x, we adopt numerical
optimization to Ô¨Ånd each group of local optimal model‚Äôs
parameters Œò‚àó

(28)

i around each starting point Œòi
J(x; Œò)

i enc = arg min

Œò‚àó

Œòenc‚àà‚Ñ¶Œòi enc
where ‚Ñ¶Œòi represents the convex space where Œòi is. Then all
N discrete local optimums Œò‚àó

N form a set Œ®

1, ¬∑ ¬∑ ¬∑ , Œò‚àó
i enc |i = 1, 2, ¬∑ ¬∑ ¬∑ , N }

Œ® = {Œò‚àó

(30)

(29)

and search-based optimization is used to search the global
optimal one among all local optimums
Œò‚àó

J(x; Œò)

(31)

enc = arg min
Œòenc‚ààŒ®

Optimization in the continuous space can optimize the local
optimal parameters more efÔ¨Åciently. Meanwhile, the optimiza-
tion in the discrete space can search for the best one from
multiple discrete points globally, which can avoid the Ô¨Ånal
solution falling into the local optimal as much as possible.
Thus, proposed hybrid optimization is a better way to solve
this problem.

IV. PROPOSED METHOD
Guided by the proposed hybrid-optimization theory, we
propose a hybrid-optimization video coding method, as show
in Fig.3 (a).

A. OfÔ¨Çine Hybrid Optimization

As above analysis, Ô¨Årstly, we ofÔ¨Çine set some hyper-
parameters to determine an encoder-decoder model structure.
SpeciÔ¨Åcally, the hyper-parameters mainly consist of two parts.
is to determine the structure of the continuous
The Ô¨Årst
part. We adopt
the hybrid framework of motion-handling
and picture-coding techniques, which has proven very ef-
fective. It consists of some modules, including motion esti-
mation (ME), motion compensation (MC), transform/inverse
transform, quantization/inverse quantization, in-loop Ô¨Ålter, and
entropy coding. Then we set the hyper-parameters for each
module to determine its internal structure. We build those
modules with continuously optimized tools (e.g. deep neural
they can be optimized with numerical
networks) so that
methods. The second is to determine the structure of the
discrete part. We design various discrete modes, which can
divide the whole continuous optimization space into multiply
sub-spaces. Due to the hybrid framework of motion-handling
and residual picture-coding, the designed modes focus on the
motion and transform coding accordingly. Through different
motion models and residual coding methods, different content
in the video can be handled efÔ¨Åciently.

Next, we ofÔ¨Çine optimize the model parameters through
lots of training samples. SpeciÔ¨Åcally, the optimization can be
divided into two steps. First, we optimize the parameters of
the whole continuous framework. We use numerical methods
to optimize all the modules in an end-to-end manner. Sec-
ond, based on the optimized parameters of the continuous

Œò‚àóùêΩŒòŒò1ŒòùëÅŒòiŒò1Œò‚àóùêΩŒòŒòùëÅŒòùëÅ‚àóŒò1ŒòùëñŒò1‚àóùêΩŒòŒò‚àó12

Fig. 3.

(a) Overview of our hybrid-optimization video coding framework. (b) Implementation of our hybrid-optimization video coding framework.

(a)

(b)

framework, we optimize the initial parameters of each discrete
mode. We convert the parameter format of the continuously
optimized model to the format of each discrete mode, and use
the converted parameters to initialize each mode.

Our proposed method is consistent with the proposed
hybrid-optimization theory. The continuous optimization space
‚Ñ¶ is the range of all module and mode parameters. The modes
divide the continuous optimization space ‚Ñ¶ into multiply
sub-spaces ‚Ñ¶1, ¬∑ ¬∑ ¬∑ , ‚Ñ¶N . Through ofÔ¨Çine optimization for all
modes, we obtain multiply groups of initial mode parameters
as the discrete starting points Œò1, ¬∑ ¬∑ ¬∑ , ŒòN .

B. Online Hybrid Optimization

Based on our ofÔ¨Çine optimized framework, we conduct on-
line hybrid optimization for each frame to be coded. Guided by
the proposed hybrid optimization theory, the online optimiza-
tion process is divided into two steps: continuous optimiza-
tion in local parts and discrete optimization in global space.
Firstly, in the local area of each mode, we perform continuous
numerical optimization around the initial parameters (i.e. Œòi)
to obtain the locally optimal parameters (i.e. Œò‚àó
i ). We conduct
this optimization for all modes, including the motion-handling
and transform coding mode, and then we can obtain the locally
optimal parameters of all discrete modes (i.e. Œ®). Secondly, we
perform discrete search optimization among all locally optimal
modes to select the globally best mode (i.e. Œò‚àó). Note that the
online optimization is also end-to-end. As a result, through the
hybrid optimization for the current frame, different content in
the video can be coded adaptively and efÔ¨Åciently.

V.

IMPLEMENTATION

We design a deep network-based video compression frame-
work to implement our proposed hybrid-optimization method.
The implementation is shown in Fig.3 (b). All modules of our
framework are based on deep networks.

At the encoder, we use the optical Ô¨Çow network to conduct
motion estimation between the current frame and reference

frames and take the optical Ô¨Çow as the initial motion infor-
mation. Then we design various partition modes and multi
motion modes to describe the optical Ô¨Çow efÔ¨Åciently so that the
expensive pixel-level Ô¨Çow is changed into low-cost partition
and motion parameters. We employ the motion compensation
network to obtained the predicted frame from the reference
frames and motion parameters. The transform module is used
to converts the residual into coefÔ¨Åcients that are easier to
code. The transformed coefÔ¨Åcients are determined whether to
code or not by residual skip mode. After that, we reÔ¨Åne the
reconstructed frame by the learned in-loop Ô¨Ålter and output the
Ô¨Ånal reconstruction. The entropy encoding network generates
the probability distribution of quantized motion parameters
and transformed coefÔ¨Åcients, and then codes them into the
bitstream by an arithmetic encoder (AE). At the decoder, with
the help of an arithmetic decoder (AD), the entropy decoding
network decompresses the motion parameters and transformed
coefÔ¨Åcients from the bitstream. Next, we sequentially use
motion compensation, inverse transform, and in-loop Ô¨Ålter to
obtain the decoded reconstruction.

In this section, we will introduce the used coding conÔ¨Ågu-

ration and our proposed coding modules in detail.

A. Coding ConÔ¨Åguration

In this paper, we adopt the random-access (RA) coding
conÔ¨Åguration, and follow the classical hierarchical B layers
coding structure. We refer these hierarchical B layers in the
GOP as layer 0, 1, 2, 3, and so on, respectively in decoding
order. The last B frame in GOP (layer 0) only can perform
uni-directional inter-prediction. However, other B frames can
perform bi-directional inter-prediction by referring to the two
reconstructed frames of the previous layer. Considering this
difference, in this paper, we only code the bi-directional B
frames in the high layers by our designed deep networks, and
the I frame and the B frames of layer 0 are coded by HEVC.
This conÔ¨Åguration can verify the effect of our proposed method
because the layer 0 frames only account for a small part of
the video, that is, one B frame in one GOP and one I frame in

TransformInverseTransformIntra-Picture PredictionIn-LoopFilterMotion EstimationEntropyCodingPredictionResidualMotion DataBitstreamOutput Video Signal DecoderDecoded Picture BufferMotion CompensationCurrentPictureMode DecisionMode DecisionNumericalOptimizationSearch-based OptimizationMEMCDecodedPictureBufferTemporal MergeModeTemporal ScaleModeMotion VectorModeMode DecisionQuadtreeSplitPartition DecisionRDORDOMode DecisionRDOGradient-based Optimization (Online)Search-based Optimization (Online)ResidualEncoderResidualDecoderIn-Loop FilterPartition&MotionParameterEncoderPartition&MotionParameterDecoderGradient-based Optimization (Offline)CurrentPictureone second. Most of the frames are the B frames in the high
layers.

B. Motion Estimation

Motion estimation in block-based hybrid video coding uses
search-based optimization to online search for the best motion
Ô¨Åeld at the encoder side, which usually takes lots of time.
However, learning-based optical Ô¨Çow can infer a motion Ô¨Åeld
directly with the ofÔ¨Çine trained model. In our framework,
we adopt the optical Ô¨Çow network PWC-Net [98] to estimate
an initial motion Ô¨Åeld rapidly. The motion Ô¨Åeld includes the
optical Ô¨Çow between the current frame and reference frames
and the optical Ô¨Çow between reference frames. These optical
Ô¨Çows will be used for later optimization.

C. Block Partition

The correlation between the adjacent pixels in a frame is
usually strong, while the correlation between the far away
pixels may be weak. In order to reduce this redundancy, block
partition is widely used in video coding. The main purpose
of block partition is to group samples that can be processed
efÔ¨Åciently and are coded together. In our framework, we adopt
the variable block partition optimization that provides multiple
block size modes to enable high efÔ¨Åciency and adaptation
to different regions. SpeciÔ¨Åcally, we use quadtree-structured
partitioning and signaling like HEVC. Only square block
partitioning is speciÔ¨Åed, and a block can be recursively split
into four quadrants. Each quadrant is assigned a Ô¨Çag that
indicates whether to split. The partitioned block includes four
sizes: 8√ó8, 16√ó16, 32√ó32, 64√ó64, and the partitioned result
is described efÔ¨Åciently by the split Ô¨Çag. Finally, through the
Ô¨Çexible partition and the parameter sharing within a block, we
avoid the expensive overhead of pixel-level Ô¨Çow and achieve
high-efÔ¨Åciency compression.

D. Motion Mode

The efÔ¨Åcient representation of motion is the key to im-
proving video coding performance. Block-based hybrid video
coding usually describes motion at a high level, which means
the motion model is designed according to the motion type,
such as the block translation model and the afÔ¨Åne model for
rotation and zooming. However, the motion Ô¨Åeld is usually not
as Ô¨Åne as at the pixel level. On the other hand, optical Ô¨Çow in
the learned end-to-end video coding is a more straightforward
way to describe motion at
ignores
high-level motion characteristics. Different motion models are
beneÔ¨Åcial
in our
framework, we introduce a hybrid of different motion models
and propose multi-mode optimization to deal with different
motion efÔ¨Åciently. We design three motion modes: motion vec-
tor (MV) mode, temporal merge (TMerge) mode, and temporal
scale (TScale) mode. We signal a mode index for each block
to decide which mode is used in this block. Note that we
will convert the coded block-level motion parameters into the
pixel-level optical Ô¨Çow Ô¨Åeld for later motion compensation
operation.

to different motion situations. Therefore,

level, but

the pixel

it

13

SpeciÔ¨Åcally, only a MV can describe the translation of
a block in block-based hybrid video coding. Similarly, we
also adopt motion vector mode in our framework to handle
the translational motion. Based on the bi-directional reference
frames at time t ‚àí k and t + k, the bi-directional optical Ô¨Çow
of all pixels in a block at time t are set as ft

ft = {mvt‚Üít‚àík, mvt‚Üít+k}

(32)

where mvt‚Üít‚àík and mvt‚Üít+k denote two motion vectors for
bi-direction, and they are quantized with half-pixel precision
to code. In addition, like learned end-to-end video coding, we
also adopt pixel-level optical Ô¨Çow to deal with Ô¨Åner motion.
However, it‚Äôs not efÔ¨Åcient to transmit the optical Ô¨Çow directly
because the motion information of bi-directional reference
frames provides lots of prior information for the current frame
prediction. To make full use of the prior motion information,
we propose temporal merge mode that can utilize the pixel-
level Ô¨Çow without transmitting any motion information. Since
the time interval between the current frame and reference
frames is usually very short, the motion in most regions can be
considered uniform. Based on the uniform motion assumption
and the prior optical Ô¨Çow between reference frames vt‚àík‚Üít+k,
vt+k‚Üít‚àík, we can obtain the bi-directional pixel-level Ô¨Çow
Ô¨Åeld in a block by

ft = {vt+k‚Üít‚àík √ó 0.5, vt‚àík‚Üít+k √ó 0.5}

(33)

Due to the diversity of motion, the motion in some regions is
not uniform, but it may obey some rules, such as accelerated
motion. Inspired by both coding frameworks, we propose a
hybrid mode of high-level motion parameters and pixel-level
Ô¨Çow to address the problem. This hybrid mode, called temporal
scale mode, combines the high efÔ¨Åciency of the high-level
parameter-based motion model and the Ô¨Åne precision of the
Ô¨Çow-based motion model. In our framework, we use a group
of scaling parameters to describe the temporally characteristic
of motion and then combine the parameters with the prior Ô¨Çow
to generate the bi-directional pixel-level Ô¨Çow Ô¨Åeld:

ft = {vt+k‚Üít‚àík √ó s0, vt‚àík‚Üít+k √ó s1}

(34)

where s0 and s1 denote two two-dimensional scaling vectors
for bi-direction, and they are quantized with one-tenth preci-
sion. Finally, the motion of a block is signaled by four scaling
values.

Our proposed multiple modes can handle most motions
effectively. According to the uniformity of motion in spatio-
temporal domain,
the motion can be classiÔ¨Åed into four
categories: spatially uniform motion, spatially non-uniform
motion,
temporally uniform motion, and temporally non-
uniform motion. Since the proposed block partition mode
can adaptively divide the pixels with the same motion into
one block and the pixels with different motion into different
blocks, it can deal with the spatially uniform and non-uniform
motion to a certain extent. The proposed temporal merge mode
is specially designed for temporally uniform motion. As for
temporally non-uniform motion, if the motion is regular and
the prior Ô¨Çow is estimated accurately, the proposed temporal
scale mode can handle it well. Otherwise, the motion vector

14

Fig. 4. Motion compensation network.

Fig. 5. Residual coding network.

mode by transmitting block-based MV directly will provide a
good approximation for the motion.

E. Motion Compensation

Motion compensation is to retrieve the predicted frame
from the reference frames through the motion Ô¨Åeld. Previous
video coding methods usually use the Ô¨Åxed fractional-pixel
interpolation Ô¨Ålter in MC, such as DCTIF in classical HEVC
and warping operation in learned DVC. In this paper, we
adopt an adaptive kernel-based MC network. The kernel-based
MC that synthesis a pixel by convolving input patches with a
learned kernel [99] can make full use of spatial information and
conduct adaptive prediction for each pixel. We use spatially-
displaced convolution (SDC) [100] which applies the learned
kernel at the location of the motion Ô¨Åeld to combine the given
motion Ô¨Åeld and learned kernel effectively. Besides, we utilize
the contextual information in the feature domain to improve
MC prediction quality further [101], [102]. Our MC network is
shown in Fig.4. We learn a kernel for each pixel and synthesize
a pixel by applying the kernel on the reference frame at the
displaced location of the given motion Ô¨Åeld. We extract the
context from the pre-trained ResNet18 conv1 layer [103] and
also apply the learned SDC kernel to warp the feature. A
reÔ¨Åne network is adopted to fuse them to generate the Ô¨Ånal
MC prediction frame.

F. Transform and Quantization
Transform aims to convert

into coefÔ¨Åcients
that are easier to code, and quantization is used to reduce
the precision of coefÔ¨Åcients to decrease the amount of data.

the residual

Correspondingly,
inverse quantization restores the original
coefÔ¨Åcient range without regaining the precision, and inverse
transform is applied to reconstruct the residual samples from
the restored coefÔ¨Åcients. In our framework, we adopt wavelet
transform/inverse-transform [104] at a frame level and uniform
scalar quantization/inverse-quantization. Our residual coding
transform results in
network is shown in Fig.5. Wavelet
a plurality of subbands of different resolutions. Similar to
JPEG2000 [104], we perform 4 times of pyramid decompo-
sition in wavelet transform, i.e. the residual frame is forward
transformed into four subbands {LL1, HL1, LH1, HH1} at
Ô¨Årst and then we conduct the k-th forward transform from
LLk‚àí1 into {LLk, HLk, LHk, HHk} recursively. Here, LL,
HL, LH, HH are all two dimensional; L stands for low-
frequency and H stands for high-frequency. Finally, we got 13
subbands with coefÔ¨Åcients. At the encoder side, all the forward
transformed coefÔ¨Åcients y are scalar quantized to obtain the
discrete indexes q to be coded:

q = [y/Qstep]

(35)

where [] stands for rounding operation and the unique param-
eter Qstep is used to control the rate and distortion. At the
decoder side, the reconstructed coefÔ¨Åcients ÀÜy are obtained by
uniform scalar inverse-quantization:

ÀÜy = q √ó Qstep

(36)

The inverse transform is strictly the reverse of the forward
transform, which is realized by reversing the order of the
operations and swapping the additions and subtractions.

Our framework adopts wavelet transform mainly because it
has a clearer correspondence between transformed coefÔ¨Åcients
and pixels. The bits of each partitioned block can be estimated
approximatively from coefÔ¨Åcients. Thus,
it‚Äôs very suitable
for block-based rate-distortion optimization among multiple
modes in our framework. Moreover, we can achieve virtually
any reachable bitrate and quality by simply adjusting the quan-
tization step of wavelet transform coefÔ¨Åcients. As shown in
Fig.6, the quality of the reconstructed frames can be controlled
precisely to make it almost the same as HM. Therefore, we not
only can reach hierarchical quality easily but also can solve
the problem of error propagation more effectively compared to
other learned video coding methods [93]. In addition to lossy
compression, since the wavelet transform doesn‚Äôt lose any
information, the coding methods based on wavelet transform
can also support lossless compression by transforming in the
integer domain [104].

G. Residual Mode

The coding performance of different videos may beneÔ¨Åt
from different residual coding methods. For example, when the
prediction is very close to the original signal, coding the sparse
residual will decrease the coding efÔ¨Åciency. To address this
problem, we propose residual skip (ResiSkip) mode that can
skip the coding of transformed coefÔ¨Åcients in some regions.
We divide the residual frame into residual-skip units regularly,
and we signal a Ô¨Çag for each unit to decide whether skip the
residual coefÔ¨Åcients of this unit. The residual-skip unit is set

KernelEstimationKernelWarpRefineContextExtractionKernelWarpOpticalFlowRefFramesSDC KernelContextWarped FrameWarped FeaturePredFrameForwardTransformInverse TransformAEADQIn-Loop FilterEntropyParameters()I-QCoefficientsDecoded CoefficientsContextModelOrigFrameRecoFrameResiFrameResi‚ÄôFramePredFrameto 128 √ó 128 to avoid too much overhead. SpeciÔ¨Åcally, we
transform the residual frame into coefÔ¨Åcients Ô¨Årstly. If the
unit chooses residual skip mode, we set the corresponding
coefÔ¨Åcients of this unit to 0 and skip the coding of those
coefÔ¨Åcients. Otherwise, we code the coefÔ¨Åcients. The residual
skip mode can not only improve the coding efÔ¨Åciency in some
cases, but also avoid the computation of coding coefÔ¨Åcients.

H. In-Loop Filter

After MC and residual compression, we can reconstruct the
current frame from the prediction and residual frame. However,
the reconstruction contains compression error, especially at
low bit rates. It not only reduces the reconstruction quality,
but also affects the inter-prediction of the next coded frame
due to the reference frame mechanism. Therefore, we propose
an in-loop Ô¨Ålter network to compensate for the compression
error of the reconstructed frame. It is operated within the
inter-picture prediction loop. We concatenate the reconstructed
frame, predicted frame, and residual frame as the input to make
full use of the spatio-temporal correlation, then feed them into
a CNN with eight residual blocks [103] to obtain the reÔ¨Åned
reconstructed frame.

I. Entropy Coding

In this paper, we adopt the context-based entropy coding
method similar to [75] and the factorized entropy coding
method. The factorized entropy coding uses the Ô¨Åxed sta-
tistical probability model. The context-based entropy coding
predicts a probability distribution for each coefÔ¨Åcient, where
the probability distribution is constructed by a set of entropy
parameters œà of CNN output, as shown in Fig.5. Arithmetic
coding is used to code each coefÔ¨Åcient according to its
probability distribution. The statistical probability model is
used to code the parameters with weak correlation to reduce
computational complexity, while the context model is used
to code the parameters with strong correlation to improve
coding performance. We use the statistical probability model
to code split Ô¨Çag, motion mode index, residual skip mode
Ô¨Çag, and the scaling parameters of temporal scale mode. The
MVs of motion vector mode are coded directly by the CNN-
based context model. As for the transformed coefÔ¨Åcients, we
adopt the CNN-based context model to exploit the correla-
tion among prediction signal, wavelet coefÔ¨Åcients within and
across subbands. When coding one wavelet coefÔ¨Åcient, CNN
generates its conditional probability distribution by inputting
three context information: the already coded coefÔ¨Åcients in the
current subband, other already coded subbands, and the inter-
prediction signal. Note that we do not perform real arithmetic
encoding during optimization but instead estimate the bits
from the entropy of the corresponding motion and residual
coefÔ¨Åcients.

A. OfÔ¨Çine Hybrid Optimization Procedure

VI. PROCEDURE

15

so it models the optimization problem with a continuous func-
tion. Then we end-to-end train the framework with gradient-
based optimization through lots of samples. The details of the
network training will be provided in Section VII-A1.

Meanwhile, our framework contains multiple modes. The
variable block partition, multiple motion modes, and multiple
residual modes can be combined so that it will produce a large
number of discrete starting points for search optimization. We
use the trained model parameters to initialize those modes.
SpeciÔ¨Åcally, the partition mode is initialized to four sizes,
and the residual coding mode is initialized to whether regular
transform coding or the zero residual. As for motion modes, all
blocks follow the same initialization rules. The coefÔ¨Åcients of
temporal merge mode are initialized to the Ô¨Åxed value 0.5, and
the scale coefÔ¨Åcients of temporal scale mode are initialized by

s0 =

vt‚Üít‚àík
vt+k‚Üít‚àík

, s1 =

vt‚Üít+k
vt‚àík‚Üít+k

(37)

and the MVs of motion vector mode are initialized by

mvt‚Üít‚àík = vt‚Üít‚àík, mvt‚Üít+k = vt‚Üít+k

(38)

where v means to calculate the average of the optical Ô¨Çow in
a block. As a result, we Ô¨Ånish the ofÔ¨Çine optimization of the
coding framework through lots of samples.

B. Online Hybrid Optimization Procedure

Based on our ofÔ¨Çine optimized framework, we conduct
online hybrid optimization for each frame at the encoder to
Ô¨Ånd the better coding parameters. The optimization framework
is shown in Fig.3 (b), and the optimization process is shown in
Algorithm 1. In this paper, we set the number of partition depth
D = 4, minimum partition block size z0 = 8, and the number
of motion mode L = 3, which have already been introduced in
the above sections. As the proposed online hybrid-optimization
method in Section IV-B, we perform continuous optimization
in local parts and discrete optimization in global space in
turn. Firstly,
in the local area of each speciÔ¨Åc mode, we
perform continuous numerical optimization around the initial
mode parameters. SpeciÔ¨Åcally, we traverse the block partition
Ô¨Årst, and then we optimize the parameters of each motion
mode separately under a speciÔ¨Åc partition. The coefÔ¨Åcients
of temporal merge mode are not coded, i.e. Ô¨Åxed value 0.5,
so they don‚Äôt need to be optimized. For other motion modes,
we get the initial coefÔ¨Åcients for each block Ô¨Årstly, where
the initialization rules have been introduced in the ofÔ¨Çine
optimization procedure of Section VI-A. Since not all frames
have signiÔ¨Åcant coding gains through online gradient-based
optimization, such as the frame with little or no motion, we
optimize some frames selectively in a video sequence. If the
current frame needs to be optimized, we online end-to-end
optimize all coefÔ¨Åcients of the current mode at the frame level
with gradient-based optimization by minimizing the RD cost
J:

J = D + ŒªR = d(ÀÜx, x) + Œª(Rm + Rr)

(39)

Based on those designed modules, we build our complete
coding framework. Our framework is on top of deep networks,

where d(ÀÜx, x) denotes the distortion between original frame
x and reconstructed frame ÀÜx, and we use sum square error

16

Algorithm 1 Online Hybrid-Optimization Process
Require: M E(): motion estimation, which inputs original
frame (orig) and reference frames (ref ) and outputs
optical Ô¨Çow (f low); M C(): motion compensation, which
inputs motion coefÔ¨Åcients (c) and reference frames (ref )
and outputs predicted frame (pred); CC(): motion co-
efÔ¨Åcients coding, which inputs motion coefÔ¨Åcients and
outputs coefÔ¨Åcient bits map (cbm); RC(): residual coding,
which inputs residual frame and outputs reconstructed
frame (reco) and residual bits map (rbm); Note that those
modules are all operated at frame level;

Partition frame into blocks with size zd = z0 << d;
for each motion mode l ‚àà [0, L) do

for each block n ‚àà [0, Nd) in frame do

d,l with f low for each block;

end for
predd,l = M C(cd,l, ref ), cbmd,l = CC(cd,l);
recod,l, rbmd,l = RC(orig ‚àí predd,l);
Calculate RD cost Jd,l for each block;
if cd,l need online numerical optimization then
Optimize cd,l online by minimizing Jd,l;

Initialize cn

1: f low = M E(orig, ref );
2: for each depth d ‚àà [0, D) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

end if

else

end for
for each block n ‚àà [0, Nd) do
Search motion mode J n

d = min{J n

d,l, l ‚àà [0, L)};

end for
Update Jd by the selected mode coefÔ¨Åcients cd;
if depth d == 0 then

Take Jd as best RD cost until depth d, i.e. (cid:101)Jd = Jd;

for each block n ‚àà [0, Nd) do

d };

end if

i=1 (cid:101)J ni

d = min{(cid:80)4

d‚àí1, J n

Search partition (cid:101)J n

end for
Update (cid:101)Jd by the selected partition and mode;

23:
24:
25:
26:
27: end for
28: Obtain the best RD cost (cid:101)J = (cid:101)Jd;
29: Calculate RD cost of skipping residual ÀÜJ for each unit;
30: for each unit u ‚àà [0, U ) do
31:
32: end for
33: Update J by the selected residual mode;
34: Code Ô¨Ånal partition, mode, motion&residual coefÔ¨Åcients;

Search residual mode J u = min{ ÀÜJ u, (cid:101)J u};

(SSE) in our implementation. Rm and Rr represent the number
of bits for motion information and residual in this frame,
respectively. Œª of each frame in our framework is set to be
the same as HM. The iteration number is set to 10.

After obtaining the local optimal coefÔ¨Åcients of all modes
in the current partition depth through continuous numerical
optimization, we perform discrete search optimization among
them, i.e. selecting the best mode for each block according
to the RD cost. The block‚Äôs RD cost is calculated with the
distortion between the original block and the reconstructed

block and the bits of motion and residual in this block. Note
that the networks in our framework are all operated at the
frame level. Thus, the distortion and bits of each block are
estimated from the frame result. After all search optimization
at the block level, the reconstructed frame needs to be updated
at
through the networks instead of block
splicing. Then we have Ô¨Ånished the motion mode optimization
in the current partition depth.

the frame level

Subsequently, we perform discrete search optimization for
the partition mode of each region by comparing the RD cost of
the block in the current depth with the best result of this region
before the current depth. We repeat that discrete optimization
until Ô¨Ånishing all partition modes. In the end, we optimize
the residual skip mode for each region by discrete search. As
a result, based on the ofÔ¨Çine model, through the continuous
numerical optimization around the starting points locally and
the discrete search optimization among all local optimums
globally, we have optimized the better parameters for the coded
sample in a hybrid manner.

VII. EXPERIMENTS

A. Experimental Settings

1) Network Training: We use the deep learning software
PyTorch to train our framework on 4 NVIDIA GTX 1080Ti
GPUs. When training our networks, we remove all modes
and code the pixel-level optical Ô¨Çow between the current
frame and reference frames directly so that the whole network
can be trained in an end-to-end manner. The goal of video
compression is to optimize rate-distortion, so we take Eq.
(39) as the loss function to minimize the RD cost. We train
4 basic models for 4 common test QPs in HM. We use Œª
and transform quantization step Qstep to control the bitrate of
different models. In our framework, we set Qstep to 12, 24,
45, 95 to correspond to QP 22, 27, 32, 37 in HM and set
Œª to be the same as HM. Since the quantization operation is
not differential, we adopt the method of forward quantization
and backward pass [105]. Each group of training data consists
of three frames, including the original frame, the reference
frame before and after the original frame. The reference frames
should be reconstructed from our compression network, but
our network has not yet completed training. To address this
problem, we compress the reference frames by HM under
four QPs to approximate them. We use the SJTU-4k dataset
[106] and down-sampling versions of these video sequences.
We extract only the luma component for training. We directly
use the luma models to code the chroma in the test. The frame
is cropped into many non-overlapping 128 √ó 128 blocks, and
the time interval between the original frame and reference
frames is set to 1 or 2 randomly to Ô¨Åt the hierarchical layers
in the test. Finally, we obtain about 500,000 training samples
for each model. We optimize with Adam [63] using 0.9 and
0.999 as hyper-parameters with no weight decay. The learning
rate starts at 0.0001 and then is reduced to 0.00001 when the
loss becomes stable. The batch size is 16.

2) Encoding ConÔ¨Ågurations and Evaluation: We evaluate
our coding framework on the HEVC common test sequences,
including 16 videos of different resolutions known as Classes

TABLE III.

BD-RATE RESULTS OF OUR PROPOSAL COMPARED TO HM

17

Class

Sequence

Class B

Class C

Class D

Class E

Class Summary

Overall

Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
FourPeople
Johnny
KristenAndSara
Class B
Class C
Class D
Class E
Classes B‚ÄìE

RA
Y (%)
‚àí2.4
‚àí4.4
‚àí4.1
2.5
43.4
2.6
4.9
0.4
16.6
‚àí6.4
‚àí14.5
‚àí0.8
3.9
‚àí4.6
2.9
1.2
7.0
6.1
‚àí4.4
‚àí0.2
2.6

TABLE IV.

BD-RATE RESULTS OF OUR PROPOSAL COMPARED TO HM

ON YUV420

Class

Sequence

Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
FourPeople
Johnny
KristenAndSara
Class B
Class C
Class D
Class E
Classes B‚ÄìE

Class B

Class C

Class D

Class E

Summary

Overall

Y (%)
0.9
‚àí3.5
‚àí2.9
4.3
44.9
4.2
6.2
0.6
17.3
‚àí4.4
‚àí13.6
‚àí0.5
6.8
‚àí4.4
2.7
1.3
8.7
7.1
‚àí2.9
‚àí0.1
3.7

RA
U (%)
0.4
1.3
‚àí2.5
12.0
10.5
12.9
13.5
3.4
28.1
3.2
‚àí2.9
3.6
21.2
‚àí2.8
0.2
‚àí0.6
4.3
14.5
6.3
‚àí1.1
6.3

V (%)
1.0
2.0
‚àí0.7
5.1
11.2
9.0
11.1
3.7
24.6
‚àí4.0
‚àí6.2
5.5
14.2
‚àí2.9
2.2
‚àí0.5
3.7
12.1
2.4
‚àí0.4
4.7

Fig. 7.
Rate-distortion comparison in terms of PSNR on HEVC dataset
(Classes B‚ÄìD) using x264, x265, DVC [81], HLVC [87], FVC [91], HM, and
our proposed method. The intra period(IP) of x264, x265, DVC, HLVC, and
FVC reported in [81], [87], [91] is 10. For a fair comparison, the IP of HM
and our proposed method is set to 8 and 16, respectively.

Fig. 6. Quality comparison of each reconstructed frame in the Group Of
Picture (GOP) between HM and our proposal. The PSNR is the average result
of all GOPs of all sequences.

B, C, D, E [107]. Considering that the luma component (Y)
is more important than the chroma components (U and V),
we provide coding results of test sequences with two formats:
YUV400 and YUV420. The YUV400 is the default format in
the following experiments. We compress the Ô¨Årst 1 second
of each sequence when comparing our proposal to HEVC.
We compress the Ô¨Årst 100 frames of each sequence when
comparing our proposal to end-to-end learned methods. PSNR
is used to measure the quality of the reconstructed frames, and
BD-rate [108] is calculated to quantify the coding performance
between different schemes. We compare our coding framework
with the vanilla HEVC reference software HM (version 16.10).
We follow the HEVC common test condition and use the de-
fault Random-Access (RA) encoding conÔ¨Ågurations provided
in HM16.10 without any change, if not otherwise speciÔ¨Åed.
HM is tested under 4 QPs: 22, 27, 32, and 37 [107]. Our
framework is tested with 4 trained Qstep models: 12, 24, 45,
and 95. In a basic model, we can adjust Qstep slightly to easily
achieve the hierarchical quality of different layers. The intra
period (IP) of HM and our framework are both 1 second. The
GOP of HM and our framework are both 8. HM coding is
running in CPU and our framework coding is running in CPU
and a single GPU. The CPU is Intel(R) Xeon(R) CPU E5-2690
v4 @2.60GHz and GPU is NVIDIA TITAN Xp with 12 GB
RAM.

B. Overall Performance

To verify the efÔ¨Åciency of our proposed hybrid optimization,
we compare our framework with both block-based hybrid
video coding and end-to-end learned video coding. The basic
ideas of all designed modes in our framework come from
HEVC, i.e. quadtree partition, temporal merge and scale mode,
motion vector mode, and residual skip mode in our frame-
work corresponding to quadtree partition, temporal merge and
AMVP candidate, motion vector mode, and skip mode in
HEVC. Therefore, we choose the HEVC of block-based hybrid
video coding for comparison. The BD-rate results of our
framework compared to HEVC reference software HM16.10
in YUV400 format are shown in Table III. It can be observed
that our proposed hybrid-optimization video coding method
achieves comparable performance with HM in terms of PSNR,

31333537394143012345678PSNR (dB)POCQP22_HMQP22_OursQP27_HMQP27_OursQP32_HMQP32_OursQP37_HMQP37_Ours2930313233343536373800.10.20.30.40.50.60.7PSNR (dB)Rate (bpp)PSNR on HEVC Dataset (Class B-D)Ours (IP=16)Ours (IP=8)HM (IP=16)HM (IP=8)FVC (IP=10)HLVC (IP=10)x265 (IP=10)DVC (IP=10)x264 (IP=10)18

leading to on average only 2.6% BD-rate increase on Y
component. Our method achieves better performance on half
of the classes and beats HM on 7 sequences among all 16 test
sequences. In addition, we provide the BD-rate comparison
results of YUV420 format in Table IV. Our proposal achieves
on average 3.7%, 6.3%, and 4.7% BD-rate increase for Y, U,
and V, respectively. The performance on the two formats is
consistent. Our method doesn‚Äôt completely outperform HEVC
mainly because our current framework only uses a very little
part of modes in HEVC. In addition to those used modes,
HEVC contains many other advanced modes, such as intra
mode, non-square PU mode, spatial merge and AMVP candi-
date, uni/bi-prediction mode, various transform modes, RDOQ,
etc. Besides, HEVC also uses additional information, such as
more reference frames. Therefore, if we add more modes or
information into our framework, it will have the potential to
outperform HEVC.

The learned end-to-end video coding methods usually code
the video in RGB format. To compare with them, our frame-
work codes the video of YUV420 format Ô¨Årstly and then
converts the reconstruction and the original video into RGB
format
to calculate RGB PSNR. Fig.7 shows the average
performance of some methods in RGB space on the HEVC
dataset (Classes B, C, D), including the typical learned method
DVC [81], the latest bi-directional learned method HLVC [87],
the state-of-the-art learned method FVC [91], x264, x265,
HM under RA conÔ¨Åguration and our method. DVC, HLVC,
and FVC only provide the results with the intra period (IP)
10. Since the IP in HM needs to be set to a multiple of
GOP size (GOP size is 8 in HM16.10) and our framework
adopts a similar hierarchical structure with HM, we provide
the performance of HM and our method with IP 8 and IP 16.
In theory, the performance of IP 10 should be between IP 8 and
IP 16. It can be observed that the coding performance of our
hybrid-optimization coding method is signiÔ¨Åcantly better than
the end-to-end learned methods, leading to more than 45% bits
saving compared to the HLVC and more than 30% bits saving
compared to the state-of-the-art FVC.

C. VeriÔ¨Åcation of Hybrid Optimization

In order to further verify the effectiveness of each opti-
mization in our proposed hybrid-optimization coding method,
we perform ablation studies on Classes B, C, D. First, we
take the popular end-to-end learned method as the baseline,
i.e. ofÔ¨Çine end-to-end gradient-based optimization and online
direct inference, and provide two schemes. The one is called
E2E TMerge, which obtains the predicted frame by interpo-
lating from reference frames without transmitting any motion
parameters and then codes the residual frame. The other one
is called E2E Flow, which obtains the predicted frame by
directly transmitting pixel-level optical Ô¨Çow and then codes
the residual frame. For a fair comparison, we code the pixel-
level Ô¨Çow with the same method as the MV in MV mode,
i.e. coding the vector directly without transform. The BD-
rate results are shown in Table V. It can be observed that
the E2E TMerge performs better than the E2E Flow, leading
to on average 20.3% BD-rate increase compared to HM. It‚Äôs

mainly because it‚Äôs not efÔ¨Åcient to code pixel-level motion
directly in hierarchical bi-directional coding structure. Next,
based on the E2E TMerge, we add discrete multi-mode search-
based optimization gradually and conduct a comprehensive set
of experiments to verify the effectiveness of each proposed
mode. As shown in Table V, when gradually adding TScale
mode, MV mode, variable block partition mode, and ResiSkip
mode, the coding performance improves signiÔ¨Åcantly and the
BD-rate increase decreases to 13.3%, 8.0%, 4.4%, and 4.2%,
respectively. The results show that our method outperforms
the pure end-to-end learned coding method and all proposed
mode are effective. More importantly, it further proves that it is
efÔ¨Åcient to mix discrete multi-mode search-based optimization
into continuous end-to-end gradient-based optimization. In
addition to those optimization methods, our framework also
contains online continuous gradient-based optimization in local
parts, and we conduct experiments to verify its effectiveness.
Since some frames in hierarchical layer structure may have
very small and simple motion, such as the high layers with
short time intervals, it is not necessary to online optimize all
frames. We only perform online gradient-based optimization
on the frames of layer 1. The BD-rate results are shown in
Table VI, where we only calculate the bits and PSNR of the
layer 1 frames. We can observe that the online continuous
gradient-based optimization improves the coding efÔ¨Åciency
signiÔ¨Åcantly, leading to on average 4.1% BD-rate reduction.
Especially,
it reaches very high coding gain on some se-
quences, such as 8.8% in Kimono and 7.7% in BQTerrace.
Through these experiments, we fully veriÔ¨Åed the effectiveness
of the proposed hybrid optimization.

D. Detailed Analysis

1) Performance Analysis:

In particular, our proposal
leading
achieves BD-rate reduction in Classes D and E,
to on average 4.4%, 0.2%, respectively. Class D, a class
with low-resolution sequences, has a small moving range
of pixels correspondingly, and Class E almost has no mo-
tion. Those sequences can be coded efÔ¨Åciently because the
networks can deal with small motion easily. Especially for
the sequence BQSquare, the bits saving is up to 14.5%.
On the other hand, our proposal has a slight BD-rate in-
crease on average in Classes B and C. It‚Äôs mainly due to
BQTerrace and RaceHorsesC. Except for the two se-
quences, our proposal achieves even better performance on
average than HM. BQTerrace features lots of noise, aliasing,
and textures, including dynamic textures on the water surfaces
and the repeated texture on the wall surface and railings.
RaceHorsesC contains the entangled occlusion on the fast
moving horses and the texture on the grass. Texture, complex
object motion, large motion, and entangled occlusion have
always been the huge challenge for optical Ô¨Çow estimation, and
network-based optical Ô¨Çow estimation methods [109], [110]
also have not well solved those problems so far. Our method
uses optical Ô¨Çow as the initial motion information to conduct
online gradient-based optimization. When the initial optical
Ô¨Çow is inaccurate, it is difÔ¨Åcult for our online gradient-based
optimization to obtain precise motion information within few

TABLE V.

ABLATION STUDY: BD-RATE RESULTS OF OUR PROPOSED MODES. ANCHOR IS HM. (1) E2E TMERGE: OUR END-TO-END SCHEME USING

TEMPORAL MERGE MODE. (2) E2E FLOW: OUR END-TO-END SCHEME USING PIXEL-LEVEL FLOW. (3) + TSCALE: ADD TEMPORAL SCALE MODE WITH
FIXED BLOCK PARTITION (32 √ó 32) TO (1). (4) + MV: ADD MOTION VECTOR MODE WITH FIXED BLOCK PARTITION (32 √ó 32) TO (3). (5) + VBLOCK: ADD
VARIABLE BLOCK PARTITION TO (4). (6) + RESISKIP: ADD RESIDUAL SKIP MODE TO (5).

19

E2E TMerge
Y (%)

E2E Flow
Y (%)

+ TScale
Y (%)

Class B

Class C

Class D

Average

29.4

21.6

7.6

20.3

130.1

66.5

33.5

80.8

19.9

15.6

2.8

13.3

+ MV
Y (%)

13.3

10.0

‚àí0.4

8.0

+ VBlock
Y (%)

+ ResiSkip
Y (%)

8.2

7.0

‚àí2.9

4.4

8.3

6.8

‚àí3.5

4.2

TABLE VI.

BD-RATE RESULTS OF OUR PROPOSAL WITH ONLINE

GRADIENT-BASED OPTIMIZATION COMPARED TO OUR PROPOSAL WITHOUT
ONLINE GRADIENT-BASED OPTIMIZATION ON THE LAYER 1 FRAMES

Sequence
Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
Classes B‚ÄìD

Y (%)
‚àí8.8
‚àí0.8
‚àí1.1
‚àí6.5
‚àí7.7
‚àí0.5
‚àí6.9
‚àí2.5
‚àí3.1
‚àí4.1
‚àí2.8
‚àí3.6
‚àí5.4
‚àí4.1

Class B

Class C

Class D

Overall

optimization times. Consequently, our proposal leads to worse
performance on BQTerrace and RaceHorsesC.

2) Mode Selection Analysis: We analyze the results of
mode selection to gain some insights. Particularly, we perform
statistics of the hitting ratio, deÔ¨Åned as the ratio of blocks
choosing a mode over all the partitioned blocks. Here the
ratio is calculated by area rather than by count because blocks
have different sizes in our framework. For the block partition
mode, the hitting ratio statistics are shown in Fig. 8. First,
the larger the block, the higher the hitting ratio. Second, as
QP increases, the hitting ratio of the largest block 64 √ó 64
i.e.
increases, and the hitting ratio of other small blocks,
8 √ó 8, 16 √ó 16, and 32 √ó 32, decreases. Third, as resolution
increases, the hitting ratio of the largest block increases, and
the hitting ratio of other small blocks decreases. One of the
reasons for these phenomenons is that larger block partition
can reduce overhead, especially at lower bit rates. In addition,
since the motion of pixels has a strong spatial correlation,
especially in high resolution, a large block partition can utilize
the correlation more efÔ¨Åciently.

For the motion and residual modes, the hitting ratio statistics
are shown in Fig. 9. First, the motion modes in descending
order of the hitting ratio are TMerge, MV, TScale. It‚Äôs mainly
because a small part of regions can fully meet the requirements
of TScale, and motion in most regions can be considered
uniform in such a short time interval and then choose TMerge.
Second, as QP increases, the hitting ratio of TMerge and
ResiSkip increases, and the hitting ratio of MV and TScale
decreases. The major reason is that TMerge and ResiSkip can
save the bits of motion parameter and residual signiÔ¨Åcantly,

the motion
which is important for lower bit rates. Third,
and residual modes are less related to resolution, because
the motion and residual mainly depend on the video content.
In summary, the large partitioned block, TMerge mode, and
ResiSkip mode can reduce overhead, which is beneÔ¨Åcial for
lower bit rates. The small partitioned block, TScale mode, and
MV mode can provide a Ô¨Åne description for motion, which is
efÔ¨Åcient for higher bit rates. Therefore, there is no remarkable
performance difference between higher and lower bit rates, as
shown in Fig.7.

Fig. 10 presents some visual results of mode selection. We
can observe that large blocks, TMerge mode, ResiSkip mode
are selected for the background and the region with simple
motion, like the Ô¨Çoor in Fig. 10 (a) and (b) and the grassland,
horse bodies in Fig. 10 (c) and (d), whilst small blocks are
selected for the region with complex motion and the edge,
like the moving basketball players in Fig. 10 (a), the edge of
riders, horse heads and legs in Fig. 10 (c). TScale mode is
selected for the region with regular temporally non-uniform
motion, like the basketball with accelerated rotation, the legs
with the accelerated movement of the right-side player in Fig.
10 (a), the horse head from rising to suddenly bowing in Fig.
10 (c). MV mode is selected for the translational region and
the region with the inaccurate prior Ô¨Çow of reference frames,
especially in the picture boundary and the occlusion, like the
player in Fig. 10 (a), the rider legs in Fig. 10 (c). Those visual
results are consistent with our design.

Moreover, we also perform statistics of the inter-prediction
and residual bits in the bitstream between HM and our pro-
posal. The ratio results are shown in Fig. 11. First, the ratio of
residual bits increases in both our proposal and HM as bit rate
increases. Second, the ratio of residual bits in our proposal is
higher than HM. The main reason is that our TMerge mode
can provide an accurate inter-prediction without any bits, and
most regions choose TMerge mode as shown in Fig. 9. Thus,
the ratio of inter-prediction bits in our proposal becomes lower,
and the ratio of residual bits will be higher correspondingly.

E. Computational Complexity

We record the encoding and decoding time of our proposed
framework and the vanilla HM. We take a Class D sequence
(BQSquare 416 √ó 240) as an example and the computational
time results are shown in Table VII. Compared to HM, the
encoder time of our framework increases slightly while the
decoder time increases signiÔ¨Åcantly. In order to know where
the computational complexity comes from, we provide the

20

Fig. 8. Partition selection ratio results.

Fig. 9. Mode selection ratio results.

TABLE VII.

AVERAGE RUNNING TIME PER FRAME OF OUR PROPOSAL AND HM FOR A CLASS D SEQUENCE (BQSQUARE 416 √ó 240). QP IS 27. (1)

HM. (2) OUR PROPOSAL, INCLUDING THE DETAILED TIME OF EACH MODULE.

Time (s)

HM

Enc.
Dec.

2.29
0.004

Optical Flow
0.33
0.168

MC
0.056
0.056

Transform&Quantization
0.014
0.008

Proposal
Context Model
0.835
86.125

AE/AD
0.809
0.799

In-Loop Filter Mode Decision

0.006
0.006

6.143
‚àí

Total
8.193
87.162

detailed results of each module in our framework to gain
some insights. It can be observed from Table VII that the
optical Ô¨Çow, MC, transform and quantization, and in-loop Ô¨Ålter
module are fast. The computational time mainly comes from
the mode decision optimization at the encoder and the context
model at the decoder. At the encoder side, we compare multiple
modes and update the compared results many times through the
networks, which takes lots of computational time. As for the
context model, we can estimate the probability distribution for
all coefÔ¨Åcients in parallel at the encoder side. However, at the
decoder side, we must deal with each coefÔ¨Åcient sequentially
because the probability distribution of the current coefÔ¨Åcient
needs to be based on the previous coefÔ¨Åcients. It is important
to note that if designing a customized algorithm for context
model in deep learning software or using dedicated hardware
for CNN inference, the time can be further reduced. Future
work is required to make the proposed method suitable for
real-time applications.

VIII. DISCUSSION
In addition to rate and distortion, video coding also needs
to consider coding complexity in practical applications. Ac-
cordingly,
the potential of a video coding framework is
mainly judged by two aspects: rate-distortion performance and
computational complexity. Next, we will discuss our hybrid
optimization from the two aspects to show its advantages and
potential.

In the past, researchers study advanced coding methods on
block-based hybrid video coding and end-to-end learned video
coding independently. However, due to the gap between the
two frameworks, the advanced schemes of one framework
often can‚Äôt be applied to the other framework. As a result,
those advanced technologies from the two frameworks can‚Äôt
improve coding performance together, which severely limits
the development of video coding. Our hybrid-optimization
coding framework as the bridge between the two frameworks
can solve this problem well. Since the coding technology is
essentially based on optimization methods, when we carry

02040608010022273237Ratio (%)QPClass B22273237QPClass C22273237QPClass D64√ó6432√ó3216√ó168√ó802040608010022273237Ratio (%)QPClass B22273237QPClass C22273237QPClass DTMergeMVTScaleResiSkip21

(a)

(c)

(b)

(d)

Fig. 10. Left: visual results of block partition and motion mode selection. Green, blue, red indicates blocks that choose Temporal Merge Mode, Temporal Scale
Mode, Motion Vector Mode, respectively. Right: visual results of residual mode selection. Purple indicates blocks that choose Residual Skip Mode. Top: 7-th
frame of BasketballPass at QP 22. Bottom: 21-st frame of RaceHorses at QP 32.

comparison optimization makes our framework easy to add the
more advanced modes from block-based hybrid video coding
to improve coding performance, such as intra mode, multi-
type tree partition, afÔ¨Åne mode, merge mode, uni/bi-prediction
mode, RDOQ, and so on. Our hybrid optimization framework
can attract and combine most of the new technologies from
the two frameworks. Therefore, our hybrid optimization has
signiÔ¨Åcant potential to improve RD performance.

Generally, the search-based optimization in the block-based
hybrid video coding is performed on the CPU, while the
numerical optimization in end-to-end learned video coding is
carried out on the GPU. In practical applications, the devices
that perform video coding by software usually have both CPU
and GPU, such as the widely used modern personal computers
(PC). However, the two frameworks usually only use one of the
CPU and GPU, wasting the computing resources of the other
one. On the other hand, the hybrid of the two optimizations
will lead to the hybrid in the computing architecture. Our hy-
brid optimization can adopt distributed computing to perform
different optimization on different computing devices so that
both CPU and GPU can be fully utilized at the same time.
Therefore, hybrid optimization has great potential in future
practical applications.

Fig. 11. Comparison of the proportion of each component in the bitstream
between HM and our proposal.

the hybrid in optimization,

those technologies can be
out
combined accordingly. SpeciÔ¨Åcally, better models in end-to-
end learned video coding, such as optical Ô¨Çow estimation,
bi-directional prediction, entropy coding, and in-loop Ô¨Ålter,
can be easily plugged into our hybrid-optimization framework
to improve coding performance. In addition, the multi-mode

0%20%40%60%80%100%ResidualNon-ResidualHM      OursQP22QP27QP32QP37HM      OursHM      OursHM      Ours22

IX. CONCLUSION

In this paper, we no longer look at video coding from the
technical point of view, but rethink video coding from its
nature, i.e. optimization problem. Firstly, we have analyzed
and reviewed existing video coding works from the perspective
of optimization, and have concluded that block-based hybrid
coding and end-to-end learned coding represent discrete and
continuous optimization solutions in essence, respectively.
Secondly, based on the theoretical analysis about existing
optimization, we have proposed a hybrid of discrete and
continuous optimization video coding theory, which is more
efÔ¨Åcient, more reÔ¨Åned, and more likely to achieve the globally
optimal solution in theory. Thirdly, guided by the theory, we
have proposed a hybrid-optimization video coding framework
based on deep networks entirely. We have performed extensive
experiments to verify the efÔ¨Åciency of the proposed method.
Results show that the proposed method outperforms the pure
deep learned video coding methods and achieves comparable
performance to HEVC reference software HM16.10 in PSNR.
More importantly, our hybrid optimization can serve as the
bridge of both optimization to promote the improvement of
coding performance together, so it has great potential.

There are several open issues for further study, some of
which have been mentioned before. Firstly, we can add ad-
vanced schemes of the two frameworks into our framework to
further realize the potential of hybrid optimization. Secondly,
we need to Ô¨Ånd out better network structures for both high
compression efÔ¨Åciency and low complexity. Thirdly, we can
study the uni-directional inter-prediction for low-delay conÔ¨Åg-
uration.

REFERENCES

[1] T. Sikora, ‚ÄúTrends and perspectives in image and video coding,‚Äù

Proceedings of the IEEE, vol. 93, no. 1, pp. 6‚Äì17, 2005.

[2] B. Bross, J. Chen, J.-R. Ohm, G. J. Sullivan, and Y.-K. Wang,
‚ÄúDevelopments in international video coding standardization after
AVC, with an overview of versatile video coding (VVC),‚Äù Proceedings
of the IEEE, vol. 109, no. 9, pp. 1463‚Äì1493, 2021.

[3] K. Choi, J. Chen, D. Rusanovskyy, K.-P. Choi, and E. S. Jang, ‚ÄúAn
overview of the MPEG-5 essential video coding standard [standards
in a nutshell],‚Äù IEEE Signal Processing Magazine, vol. 37, no. 3, pp.
160‚Äì167, 2020.

[4] G. Meardi, S. Ferrara, L. Ciccarelli, G. Cobianchi, S. Poularakis,
F. Maurer, S. Battista, and A. Byagowi, ‚ÄúMPEG-5 Part 2: Low
complexity enhancement video coding (LCEVC): Overview and per-
formance evaluation,‚Äù in Applications of Digital Image Processing
XLIII, vol. 11510.
International Society for Optics and Photonics,
2020, p. 115101C.
J. Han, B. Li, D. Mukherjee, C.-H. Chiang, A. Grange, C. Chen, H. Su,
S. Parker, S. Deng, U. Joshi et al., ‚ÄúA technical overview of AV1,‚Äù
Proceedings of the IEEE, vol. 109, no. 9, pp. 1435‚Äì1462, 2021.
J. Zhang, C. Jia, M. Lei, S. Wang, S. Ma, and W. Gao, ‚ÄúRecent
development of AVS video coding standard: AVS3,‚Äù in 2019 Picture
Coding Symposium (PCS).

IEEE, 2019, pp. 1‚Äì5.

[5]

[6]

[7] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù nature, vol.

521, no. 7553, pp. 436‚Äì444, 2015.

[8] A. Krizhevsky, I. Sutskever, and G. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù in NIPS, vol. 25, 2012, pp.
1097‚Äì1105.

[9] C. Dong, Y. Deng, C. C. Loy, and X. Tang, ‚ÄúCompression artifacts
reduction by a deep convolutional network,‚Äù in ICCV, 2015, pp. 576‚Äì
584.

[10] G. Toderici, S. M. O‚ÄôMalley, S. J. Hwang, D. Vincent, D. Min-
nen, S. Baluja, M. Covell, and R. Sukthankar, ‚ÄúVariable rate im-
age compression with recurrent neural networks,‚Äù arXiv preprint
arXiv:1511.06085, 2015.

[11] A. Ortega and K. Ramchandran, ‚ÄúRate-distortion methods for image
and video compression,‚Äù IEEE Signal processing magazine, vol. 15,
no. 6, pp. 23‚Äì50, 1998.
ITU-T (formerly CCITT), ‚ÄúCodec for videoconferencing using primary
digital group transmission,‚Äù ITU-T Recommendation H.120; version 1,
1984; version 2, 1988.

[12]

[14]

[15]

[13] ‚Äî‚Äî, ‚ÄúVideo codec for audiovisual services at p √ó 64 kbitis,‚Äù ITU-T
Recommendation H.261; version 1, Nov. 1990; version 2, Mar. 1993.
ISO/IEC JTCI, ‚ÄúCoding of moving pictures and associated audio for
digital storage media at up to about 1.5 Mbit/s - Part 2: Video,‚Äù
ISO/IEC 11172-2 (MPEG-I), Mar. 1993.
ITU-T (formerly CCITT) and ISO/IEC JTC1, ‚ÄúGeneric coding of
moving pictures and associated audio information - Part 2: Video,‚Äù
ITU-T Recommendation H.262 - ISO/IEC 13818-2 (MPEG-2), Nov.
1994.
ITU-T (formerly CCITT), ‚ÄúVideo coding for low bitrate communica-
tion,‚Äù ITU-T Recommendation H.263; version 1, Nov. 1995; version
2, Jan. 1998.

[16]

[17] G. J. Sullivan and T. Wiegand, ‚ÄúRate-distortion optimization for video
compression,‚Äù IEEE signal processing magazine, vol. 15, no. 6, pp.
74‚Äì90, 1998.

[18] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, ‚ÄúOverview
of the H.264/AVC video coding standard,‚Äù IEEE Transactions on
circuits and systems for video technology, vol. 13, no. 7, pp. 560‚Äì
576, 2003.

[19] G. J. Sullivan, J. Ohm, W.-J. Han, and T. Wiegand, ‚ÄúOverview of the
high efÔ¨Åciency video coding (HEVC) standard,‚Äù IEEE Transactions
on Circuits and Systems for Video Technology, vol. 22, no. 12, pp.
1649‚Äì1668, 2012.

[21]

[20] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. J. Sullivan,
‚ÄúRate-constrained coder control and comparison of video coding
standards,‚Äù IEEE Transactions on circuits and systems for video
technology, vol. 13, no. 7, pp. 688‚Äì703, 2003.
J.-R. Ohm, G. J. Sullivan, H. Schwarz, T. K. Tan, and T. Wiegand,
‚ÄúComparison of the coding efÔ¨Åciency of video coding standards-
including high efÔ¨Åciency video coding (HEVC),‚Äù IEEE Transactions
on circuits and systems for video technology, vol. 22, no. 12, pp.
1669‚Äì1684, 2012.
J. Nocedal and S. Wright, Numerical optimization. Springer Science
& Business Media, 2006.

[22]

[23] L.-K. Liu and E. Feig, ‚ÄúA block-based gradient descent search algo-
rithm for block motion estimation in video coding,‚Äù IEEE Transactions
on circuits and systems for Video Technology, vol. 6, no. 4, pp. 419‚Äì
422, 1996.

[24] F. Dufaux and J. Konrad, ‚ÄúEfÔ¨Åcient, robust, and fast global motion
estimation for video coding,‚Äù IEEE transactions on image processing,
vol. 9, no. 3, pp. 497‚Äì501, 2000.

[25] O.-C. Chen, ‚ÄúMotion estimation using a one-dimensional gradient
descent search,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology, vol. 10, no. 4, pp. 608‚Äì616, 2000.

[26] L.-M. Po, K.-H. Ng, K.-W. Cheung, K.-M. Wong, Y. M. S. Uddin,
and C.-W. Ting, ‚ÄúNovel directional gradient descent searches for fast
block motion estimation,‚Äù IEEE Transactions on Circuits and Systems
for Video Technology, vol. 19, no. 8, pp. 1189‚Äì1195, 2009.

[27] L. Li, H. Li, D. Liu, Z. Li, H. Yang, S. Lin, H. Chen, and F. Wu, ‚ÄúAn
efÔ¨Åcient four-parameter afÔ¨Åne motion model for video coding,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology, vol. 28,
no. 8, pp. 1934‚Äì1948, 2018.

[28] B. Bross, J. Chen, S. Liu, and Y.-K. Wang, ‚ÄúVersatile video coding
(draft 10),‚Äù Joint Video Experts Team (JVET), Tech. Rep. JVET-
S2001, 2020.

[29] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-R.
Ohm, ‚ÄúOverview of the versatile video coding (VVC) standard and its
applications,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology, vol. 31, no. 10, pp. 3736‚Äì3764, 2021.

[30] Y. Wang, J. Ostermann, and Y.-Q. Zhang, Video processing and
communications. Prentice hall Upper Saddle River, NJ, 2002, vol. 1.
[31] X. Wu, E. Barthel, and W. Zhang, ‚ÄúPiecewise 2D autoregression for

predictive image coding,‚Äù in ICIP.

IEEE, 1998, pp. 901‚Äì904.

[32] X. Li and M. T. Orchard, ‚ÄúEdge-directed prediction for lossless com-
pression of natural images,‚Äù IEEE Transactions on image processing,
vol. 10, no. 6, pp. 813‚Äì817, 2001.

[33] H. Ye, G. Deng, and J. C. Devlin, ‚ÄúLeast squares approach for lossless
image coding,‚Äù in International Symposium on Signal Processing and
its Applications (ISSPA), vol. 1.

IEEE, 1999, pp. 63‚Äì66.

[34] H. Liu, Y. Chen, J. Chen, L. Zhang, and M. Karczewicz, ‚ÄúLocal
illumination compensation,‚Äù VCEG, Tech. Rep. VCEG-AZ06, 2015.
[35] K. Zhang, J. Chen, L. Zhang, X. Li, and M. Karczewicz, ‚ÄúEnhanced
cross-component linear model for chroma intra-prediction in video
coding,‚Äù IEEE Transactions on Image Processing, vol. 27, no. 8, pp.
3983‚Äì3997, 2018.

[36] T. Wedi and H. G. Musmann, ‚ÄúMotion-and aliasing-compensated
prediction for hybrid video coding,‚Äù IEEE transactions on circuits and
systems for video technology, vol. 13, no. 7, pp. 577‚Äì586, 2003.
[37] T. Wedi, ‚ÄúAdaptive interpolation Ô¨Ålters and high-resolution displace-
ments for video coding,‚Äù IEEE Transactions on Circuits and Systems
for Video Technology, vol. 16, no. 4, pp. 484‚Äì491, 2006.

[38] Y. Vatis and J. Ostermann, ‚ÄúAdaptive interpolation Ô¨Ålter for H.
264/AVC,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology, vol. 19, no. 2, pp. 179‚Äì192, 2008.

[39] C.-Y. Tsai, C.-Y. Chen, T. Yamakage, I. S. Chong, Y.-W. Huang, C.-M.
Fu, T. Itoh, T. Watanabe, T. Chujoh, M. Karczewicz et al., ‚ÄúAdaptive
loop Ô¨Åltering for video coding,‚Äù IEEE Journal of Selected Topics in
Signal Processing, vol. 7, no. 6, pp. 934‚Äì945, 2013.

[40] M. Karczewicz, N. Hu, J. Taquet, C.-Y. Chen, K. Misra, K. Andersson,
P. Yin, T. Lu, E. Franc¬∏ois, and J. Chen, ‚ÄúVVC in-loop Ô¨Ålters,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology, vol. 31,
no. 10, pp. 3907‚Äì3925, 2021.

[41] A. Alshin, E. Alshina, and T. Lee, ‚ÄúBi-directional optical Ô¨Çow for
improving motion compensation,‚Äù in 28th Picture Coding Symposium.
IEEE, 2010, pp. 422‚Äì425.

[42] D. Liu, Y. Li, J. Lin, H. Li, and F. Wu, ‚ÄúDeep learning-based video
coding: A review and a case study,‚Äù ACM Computing Surveys (CSUR),
vol. 53, no. 1, pp. 1‚Äì35, 2020.

[43] Y. Dai, D. Liu, and F. Wu, ‚ÄúA convolutional neural network approach
for post-processing in HEVC intra coding,‚Äù in International Confer-
ence on Multimedia Modeling. Springer, 2017, pp. 28‚Äì39.

[44] Y. Dai, D. Liu, Z.-J. Zha, and F. Wu, ‚ÄúA CNN-based in-loop Ô¨Ålter

with CU classiÔ¨Åcation for HEVC,‚Äù in VCIP, 2018, pp. 1‚Äì4.

[45] Z. Guan, Q. Xing, M. Xu, R. Yang, T. Liu, and Z. Wang, ‚ÄúMFQE
2.0: A new approach for multi-frame quality enhancement on com-
pressed video,‚Äù IEEE transactions on pattern analysis and machine
intelligence, vol. 43, no. 3, pp. 949‚Äì963, 2019.

[46] C. Jia, S. Wang, X. Zhang, S. Wang, J. Liu, S. Pu, and S. Ma, ‚ÄúContent-
aware convolutional neural network for in-loop Ô¨Åltering in high
efÔ¨Åciency video coding,‚Äù IEEE Transactions on Image Processing,
vol. 28, no. 7, pp. 3343‚Äì3356, 2019.

[47] Y. Zhang, T. Shen, X. Ji, Y. Zhang, R. Xiong, and Q. Dai, ‚ÄúResidual
highway convolutional neural networks for in-loop Ô¨Åltering in HEVC,‚Äù
IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3827‚Äì
3841, 2018.
J. Pfaff, P. Helle, D. Maniry, S. Kaltenstadler, W. Samek, H. Schwarz,
D. Marpe, and T. Wiegand, ‚ÄúNeural network based intra prediction

[48]

23

for video coding,‚Äù in Applications of Digital Image Processing XLI,
vol. 10752.
International Society for Optics and Photonics, 2018, p.
1075213.

[49] Z. Zhao, S. Wang, S. Wang, X. Zhang, S. Ma, and J. Yang, ‚ÄúEnhanced
bi-prediction with convolutional neural network for high efÔ¨Åciency
video coding,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology, vol. 29, no. 11, pp. 3291‚Äì3301, 2019.
J. Liu, S. Xia, W. Yang, M. Li, and D. Liu, ‚ÄúOne-for-all: Grouped
variation network-based fractional
interpolation in video coding,‚Äù
IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2140‚Äì
2151, 2019.

[50]

[51] H. Ma, D. Liu, R. Xiong, and F. Wu, ‚ÄúiWave: CNN-based wavelet-like
transform for image compression,‚Äù IEEE Transactions on Multimedia,
vol. 22, no. 7, pp. 1667‚Äì1679, 2019.

[52] C. Ma, D. Liu, X. Peng, L. Li, and F. Wu, ‚ÄúConvolutional neural
network-based arithmetic coding for HEVC intra-predicted residues,‚Äù
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 30, no. 7, pp. 1901‚Äì1916, 2019.
J. Li, B. Li, J. Xu, R. Xiong, and W. Gao, ‚ÄúFully connected network-
based intra prediction for image coding,‚Äù IEEE Transactions on Image
Processing, vol. 27, no. 7, pp. 3236‚Äì3247, 2018.

[53]

[54] S. Huo, D. Liu, F. Wu, and H. Li, ‚ÄúConvolutional neural network-
based motion compensation reÔ¨Ånement for video coding,‚Äù in ISCAS.
IEEE, 2018, pp. 1‚Äì4.

[55] Y. Li, L. Li, Z. Li, J. Yang, N. Xu, D. Liu, and H. Li, ‚ÄúA hybrid neural
network for chroma intra prediction,‚Äù in ICIP, 2018, pp. 1797‚Äì1801.
[56] N. Yan, D. Liu, H. Li, B. Li, L. Li, and F. Wu, ‚ÄúConvolutional
neural network-based fractional-pixel motion compensation,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology, vol. 29,
no. 3, pp. 840‚Äì853, 2019.

[57] ‚Äî‚Äî, ‚ÄúInvertibility-driven interpolation Ô¨Ålter for video coding,‚Äù IEEE
Transactions on Image Processing, vol. 28, no. 10, pp. 4912‚Äì4925,
2019.

[58] S. Huo, D. Liu, B. Li, S. Ma, F. Wu, and W. Gao, ‚ÄúDeep network-
based frame extrapolation with reference frame alignment,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology, vol. 31,
no. 3, pp. 1178‚Äì1192, 2021.

[59] D. Liu, H. Ma, Z. Xiong, and F. Wu, ‚ÄúCNN-based DCT-like transform
for image compression,‚Äù in International Conference on Multimedia
Modeling. Springer, 2018, pp. 61‚Äì72.

[60] R. Song, D. Liu, H. Li, and F. Wu, ‚ÄúNeural network-based arithmetic
coding of intra prediction modes in HEVC,‚Äù in VCIP, 2017, pp. 1‚Äì4.
[61] Y. Li, D. Liu, H. Li, L. Li, F. Wu, H. Zhang, and H. Yang, ‚ÄúConvolu-
tional neural network-based block up-sampling for intra frame coding,‚Äù
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 28, no. 9, pp. 2316‚Äì2330, 2018.

[62] Y. Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, ‚ÄúLearning a convolutional
neural network for image compact-resolution,‚Äù IEEE Transactions on
Image Processing, vol. 28, no. 3, pp. 1092‚Äì1107, 2019.

[63] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù

arXiv preprint arXiv:1412.6980, 2014.

[64] Z. Liu, X. Yu, Y. Gao, S. Chen, X. Ji, and D. Wang, ‚ÄúCU partition
mode decision for HEVC hardwired intra encoder using convolution
neural network,‚Äù IEEE Transactions on Image Processing, vol. 25,
no. 11, pp. 5088‚Äì5103, 2016.

[65] M. Xu, T. Li, Z. Wang, X. Deng, R. Yang, and Z. Guan, ‚ÄúReducing
complexity of HEVC: A deep learning approach,‚Äù IEEE Transactions
on Image Processing, vol. 27, no. 10, pp. 5044‚Äì5059, 2018.
[66] Z. Jin, P. An, L. Shen, and C. Yang, ‚ÄúCNN oriented fast QTBT
partition algorithm for JVET intra coding,‚Äù in VCIP, 2017, pp. 1‚Äì4.
[67] K. Kim and W. W. Ro, ‚ÄúFast CU depth decision for HEVC using
neural networks,‚Äù IEEE Transactions on Circuits and Systems for Video
Technology, vol. 29, no. 5, pp. 1462‚Äì1473, 2018.

[68] A. Feng, C. Gao, L. Li, D. Liu, and F. Wu, ‚ÄúCNN-based depth
map prediction for fast block partitioning in hevc intra coding,‚Äù in

24

2021 IEEE International Conference on Multimedia and Expo (ICME).
IEEE, 2021, pp. 1‚Äì6.

[69]

[70]

[71]

I. Storch, L. Agostini, B. Zatt, S. Bampi, and D. Palomino, ‚ÄúFastin-
ter360: A fast inter mode decision for HEVC 360 video coding,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology, DOI:
10.1109/TCSVT.2021.3096752, 2021.

J. Ball¬¥e, V. Laparra, and E. P. Simoncelli, ‚ÄúEnd-to-end optimized
image compression,‚Äù arXiv preprint arXiv:1611.01704, 2016.

J. Ball¬¥e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, ‚ÄúVari-
ational image compression with a scale hyperprior,‚Äù arXiv preprint
arXiv:1802.01436, 2018.

[72] D. Minnen, J. Ball¬¥e, and G. Toderici, ‚ÄúJoint autoregressive and
hierarchical priors for learned image compression,‚Äù in NIPS, 2018,
pp. 10 794‚Äì10 803.

[73] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor,
and M. Covell, ‚ÄúFull resolution image compression with recurrent
neural networks,‚Äù in CVPR, 2017, pp. 5306‚Äì5314.

[74] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh, T. Chinen,
S. Jin Hwang, J. Shor, and G. Toderici, ‚ÄúImproved lossy image
compression with priming and spatially adaptive bit rates for recurrent
networks,‚Äù in CVPR, 2018, pp. 4385‚Äì4393.

[75] H. Ma, D. Liu, N. Yan, H. Li, and F. Wu, ‚ÄúEnd-to-end opti-
mized versatile image compression with wavelet-like transform,‚Äù IEEE
Transactions on Pattern Analysis and Machine Intelligence, DOI:
10.1109/TPAMI.2020.3026003, 2020.

[76] Y. Hu, W. Yang, Z. Ma, and J. Liu, ‚ÄúLearning end-to-end lossy image
compression: A benchmark,‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence, DOI: 10.1109/TPAMI.2021.3065339, 2021.

[77] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, ‚ÄúLearned image com-
pression with discretized gaussian mixture likelihoods and attention
modules,‚Äù in CVPR, 2020, pp. 7939‚Äì7948.

[78] T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma, ‚ÄúDeepcoder:
A deep neural network based video compression,‚Äù in VCIP.
IEEE,
2017, pp. 1‚Äì4.

[79] C.-Y. Wu, N. Singhal, and P. Krahenbuhl, ‚ÄúVideo compression through

image interpolation,‚Äù in ECCV, 2018, pp. 416‚Äì431.

[80] Z. Chen, T. He, X. Jin, and F. Wu, ‚ÄúLearning for video compression,‚Äù
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 30, no. 2, pp. 566‚Äì576, 2019.

[81] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, ‚ÄúDVC: An
end-to-end deep video compression framework,‚Äù in CVPR, 2019, pp.
11 006‚Äì11 015.

[82] G. Lu, X. Zhang, W. Ouyang, L. Chen, Z. Gao, and D. Xu, ‚ÄúAn end-to-
end learning framework for video compression,‚Äù IEEE transactions on
pattern analysis and machine intelligence, vol. 43, no. 10, pp. 3292‚Äì
3308, 2021.

[83] A. Habibian, T. v. Rozendaal, J. M. Tomczak, and T. S. Cohen, ‚ÄúVideo
compression with rate-distortion autoencoders,‚Äù in ICCV, 2019, pp.
7033‚Äì7042.

[84] A. Djelouah, J. Campos, S. Schaub-Meyer, and C. Schroers, ‚ÄúNeural
inter-frame compression for video coding,‚Äù in ICCV, 2019, pp. 6421‚Äì
6429.

[85] O. Rippel, S. Nair, C. Lew, S. Branson, A. G. Anderson, and
L. Bourdev, ‚ÄúLearned video compression,‚Äù in ICCV, 2019, pp. 3454‚Äì
3463.

[89] E. Agustsson, D. Minnen, N. Johnston, J. Balle, S. J. Hwang, and
G. Toderici, ‚ÄúScale-space Ô¨Çow for end-to-end optimized video com-
pression,‚Äù in CVPR, 2020, pp. 8503‚Äì8512.

[90] H. Liu, M. Lu, Z. Ma, F. Wang, Z. Xie, X. Cao, and Y. Wang,
‚ÄúNeural video coding using multiscale motion compensation and
spatiotemporal context model,‚Äù IEEE Transactions on Circuits and
Systems for Video Technology, vol. 31, no. 8, pp. 3182‚Äì3196, 2021.

[91] Z. Hu, G. Lu, and D. Xu, ‚ÄúFVC: A new framework towards deep
video compression in feature space,‚Äù in CVPR, 2021, pp. 1502‚Äì1511.
[92] R. Yang, F. Mentzer, L. Van Gool, and R. Timofte, ‚ÄúLearning for video
compression with recurrent auto-encoder and recurrent probability
model,‚Äù IEEE Journal of Selected Topics in Signal Processing, vol. 15,
no. 2, pp. 388‚Äì401, 2020.

[93] G. Lu, C. Cai, X. Zhang, L. Chen, W. Ouyang, D. Xu, and Z. Gao,
‚ÄúContent adaptive and error propagation aware deep video compres-
sion,‚Äù in ECCV. Springer, 2020, pp. 456‚Äì472.

[94] Z. Hu, Z. Chen, D. Xu, G. Lu, W. Ouyang, and S. Gu, ‚ÄúImproving
deep video compression by resolution-adaptive Ô¨Çow coding,‚Äù in ECCV.
Springer, 2020, pp. 193‚Äì209.
[95] C. E. Shannon, ‚ÄúA mathematical

theory of communication,‚Äù Bell

Systems Technical Journal, vol. 27, no. 4, pp. 623‚Äì656, 1948.
[96] ‚Äî‚Äî, ‚ÄúCoding theorems for a discrete source with a Ô¨Ådelity criteria,‚Äù

Ire National Convention Record, 1959.

[97] H. Everett III, ‚ÄúGeneralized lagrange multiplier method for solving
problems of optimum allocation of resources,‚Äù Operations research,
vol. 11, no. 3, pp. 399‚Äì417, 1963.

[98] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, ‚ÄúPWC-Net: CNNs for
optical Ô¨Çow using pyramid, warping, and cost volume,‚Äù in CVPR,
2018, pp. 8934‚Äì8943.

[99] S. Niklaus, L. Mai, and F. Liu, ‚ÄúVideo frame interpolation via adaptive

separable convolution,‚Äù in ICCV, 2017, pp. 261‚Äì270.

[100] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan,
A. Tao, and B. Catanzaro, ‚ÄúSDC-Net: Video prediction using spatially-
displaced convolution,‚Äù in ECCV, 2018, pp. 718‚Äì733.

[101] S. Niklaus and F. Liu, ‚ÄúContext-aware synthesis for video frame

interpolation,‚Äù in CVPR, 2018, pp. 1701‚Äì1710.

[102] W. Bao, W.-S. Lai, X. Zhang, Z. Gao, and M.-H. Yang, ‚ÄúMEMC-Net:
Motion estimation and motion compensation driven neural network for
video interpolation and enhancement,‚Äù IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 43, no. 3, pp. 933‚Äì948, 2019.
[103] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for

image recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778.

[104] C. Christopoulos, A. Skodras, and T. Ebrahimi, ‚ÄúThe JPEG2000 still
image coding system: an overview,‚Äù IEEE transactions on consumer
electronics, vol. 46, no. 4, pp. 1103‚Äì1127, 2000.

[105] L. Theis, W. Shi, A. Cunningham, and F. Husz¬¥ar, ‚ÄúLossy im-
age compression with compressive autoencoders,‚Äù arXiv preprint
arXiv:1703.00395, 2017.

[106] L. Song, X. Tang, W. Zhang, X. Yang, and P. Xia, ‚ÄúThe SJTU 4K

video sequence dataset,‚Äù in QoMEX, 2013, pp. 34‚Äì35.

[107] F. Bossen, ‚ÄúCommon test conditions and software reference conÔ¨Ågu-

rations,‚Äù JCT-VC, Tech. Rep. JCTVC-F900, 2011.

[108] G. Bjontegaard, ‚ÄúCalcuation of average PSNR differences between

RD-curves,‚Äù VCEG, Tech. Rep. VCEG-M33, 2001.

[109] Z. Teed and J. Deng, ‚ÄúRAFT: Recurrent all-pairs Ô¨Åeld transforms for

optical Ô¨Çow,‚Äù in ECCV. Springer, 2020, pp. 402‚Äì419.

[86] M. A. Yilmaz and A. M. Tekalp, ‚ÄúEnd-to-end rate-distortion optimiza-
tion for bi-directional learned video compression,‚Äù in ICIP.
IEEE,
2020, pp. 1311‚Äì1315.

[110] Z. Yin, T. Darrell, and F. Yu, ‚ÄúHierarchical discrete distribution
decomposition for match density estimation,‚Äù in CVPR, 2019, pp.
6044‚Äì6053.

[87] R. Yang, F. Mentzer, L. V. Gool, and R. Timofte, ‚ÄúLearning for video
compression with hierarchical quality and recurrent enhancement,‚Äù in
CVPR, 2020, pp. 6628‚Äì6637.

[88]

J. Lin, D. Liu, H. Li, and F. Wu, ‚ÄúM-LVC: multiple frames prediction
for learned video compression,‚Äù in CVPR, 2020, pp. 3546‚Äì3554.

