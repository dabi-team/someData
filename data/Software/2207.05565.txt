Towards Hybrid-Optimization Video Coding

Shuai Huo, Dong Liu, Li Li, Siwei Ma, Feng Wu, and Wen Gao

1

2
2
0
2

l
u
J

2
1

]

V

I
.
s
s
e
e
[

1
v
5
6
5
5
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Video coding is a mathematical optimization prob-
lem of rate and distortion essentially. To solve this complex
optimization problem in practice, two popular video coding
frameworks have been developed: block-based hybrid video
coding and end-to-end learned video coding. If we rethink video
coding from the perspective of optimization, we ﬁnd that the
existing two frameworks represent two directions of optimization
solutions. Block-based hybrid video coding represents the discrete
optimization solution because those irrelevant coding modes are
discrete in mathematics. The discrete solution provides multiple
starting points (i.e. modes) in global optimization space and then
searches for the best one among them. However, the search-
based optimization algorithm is not efﬁcient enough. On the other
hand, end-to-end learned video coding represents the continuous
optimization solution because the optimization algorithm of deep
learning, gradient descent, is based on a continuous function.
The continuous solution optimizes a group of model parameters
efﬁciently by such a numerical algorithm. However, limited by
only one starting point, it is easy to fall into the local optimum.
To better solve the optimization problem, we propose a hybrid
of discrete and continuous optimization video coding. We regard
video coding as a hybrid of the discrete and continuous optimiza-
tion problem, and use both search and numerical algorithm to
solve it. Our idea is to provide multiple discrete starting points
in the global space and optimize the local optimum around each
point by numerical algorithm efﬁciently. Finally, we search for
the global optimum among those local optimums. Guided by the
hybrid optimization idea, we design a hybrid optimization video
coding framework, which is built on continuous deep networks
entirely and also contains some discrete modes. We conduct a
comprehensive set of experiments to verify the efﬁciency of our
hybrid optimization. Compared to the continuous optimization
framework, our method outperforms pure learned video coding
methods. Meanwhile, compared to the discrete optimization
framework, our method achieves comparable performance to
HEVC reference software HM16.10 in PSNR.

Keywords—Deep neural network, end-to-end optimization, hybrid
optimization, learned video compression, optimization theory, rate-
distortion optimization, video coding.

I.

INTRODUCTION

Video coding is the enabling fundamental for most video
technologies, including computer vision,
image processing,
visual communication, etc. Since the early days of digital
video technology in the 1960s [1], academia and industry have
started to study video coding and developed an outstanding

Date of current version July 13, 2022. (Corresponding author: Dong Liu.)
S. Huo, D. Liu, Li Li, and F. Wu are with the CAS Key Laboratory
of Technology in Geo-Spatial Information Processing and Application Sys-
tem, University of Science and Technology of China, Hefei 230027, China
(e-mail: huoshuai@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; lil1@ustc.edu.cn;
fengwu@ustc.edu.cn).

S. Ma and W. Gao are with the Institute of Digital Media, Peking University,

Beijing 100871, China (email: swma@pku.edu.cn; wgao@pku.edu.cn).

framework called block-based hybrid video coding framework.
The name of block-based hybrid video coding is because it’s a
hybrid of two coding choices or modes (i.e. motion-handling
and picture-coding techniques) at the block level. Under the
two main choices, the framework allows to add various detailed
modes, such as sub-pixel motion mode and merge mode in the
motion-handling choice, skip and DCT mode in the picture-
coding choice. Even if many modes are not very related to
each other, this framework can still integrate them well and
then searches for the best coding mode among them for each
sequence. It is precisely because of the mode expansibility
that an important development direction of video coding is
to add more advanced modes based on the latest framework.
Accordingly, a series of coding standards have been developed
in the past forty years, including H.261, H.263, H.264, H.265,
H.266 [2], EVC [3], LCEVC [4], AV1 [5], AVS3 [6], etc. Now,
H.266/VVC published by ITU-T and ISO/IEC in July 2020 is
the state-of-the-art video coding standard. However, with the
increase of modes, the rate-distortion optimization by mode
search becomes more complex.

Deep learning [7] has brought about great breakthroughs and
revolutionized the paradigm of image/video processing since
2012 [8]. Deep learning uses deep neural networks to convert
the data into a representation at a more abstract level, and
those representations are not hand-designed but learned from
massive data. With the help of the gradient descent algorithm,
the deep networks can be optimized efﬁciently. Inspired by that
success, deep learning for image/video coding has also been
explored since 2015 [9], [10]. Those works can be divided
into two categories: deep tools and end-to-end methods. The
deep tool is to add a deep network-based coding tool into the
traditional coding framework. Those deep tools have improved
the coding efﬁciency signiﬁcantly, which conﬁrmed the feasi-
bility and potential of deep learning-based video coding. On
the other hand, the end-to-end method is built entirely upon
deep networks, and the whole framework optimizes a group
of model parameters by gradient descent in an end-to-end
manner. Although end-to-end works have more potential, due
to the complexity of video coding tasks, the existing end-to-end
works still have a little gap from the best conventional coding.
Thus, the other key development direction of video coding is
to design more effective network modules under the current
end-to-end learned coding framework to realize its potential.
Video coding is to optimize the encoder-decoder parameters
to achieve the best tradeoff performance of bit rate and distor-
tion. Therefore, video coding is a mathematical optimization
problem of rate and distortion essentially. If we look at video
coding from the perspective of optimization, we ﬁnd that the
existing two frameworks represent
two solution directions.
The block-based hybrid video coding represents the discrete
optimization solution. The various coding modes usually are

 
 
 
 
 
 
2

not relevant to each other, so they are discrete in mathematics.
Thus,
this framework can be seen as providing multiple
discrete points (i.e. coding modes) in the coding optimization
space and then globally searching for the best mode for each
coding sequence. Accordingly, the more discrete starting points
will lead to better optimization results. However, the search
usually is not efﬁcient enough for so many modes. On the
other hand, deep learning-based video coding represents the
continuous optimization solution. In mathematics, gradient
descent is based on continuous function. Thus, deep learning-
based video coding uses deep networks to formulate coding
as a continuous optimization problem. Through the numerical
algorithm (e.g. gradient descent),
it optimizes a group of
encoder-decoder parameters efﬁciently in continuous space.
However, limited by the numerical algorithm and only one
starting point, it is easy to fall into the local optimum.

We would like to ask whether discrete and continuous opti-
mization can be combined together. The continuous method
can optimize more efﬁciently by numerical algorithms, but
continuous optimization usually provides only one starting
point. On the other hand, discrete optimization can provide
multiple starting points in the global space. Therefore, we are
going a step further to ask whether we can provide multiple
discrete starting points for continuous optimization to solve
the video coding optimization problem. Based on this idea,
we propose a hybrid of discrete and continuous optimization
video coding. By means of discrete optimization, we provide
various starting points distributed in the global optimization
space. Through continuous optimization, we can optimize the
local optimum around each starting point efﬁciently. Then we
use the discrete search among those local optimums to obtain
the global optimum. In theory, hybrid optimization is not only
efﬁcient but also more possible to achieve the globally optimal
solution.

In this paper, we ﬁrst analyze and summarize existing
video coding works from the perspective of optimization. We
conclude that existing video coding frameworks adopt discrete
or continuous optimization essentially, and we analyze why
they will lead to the coding gain theoretically. Inspired by
the theoretical analysis, we propose a hybrid optimization
video coding theory, and then design a hybrid optimization
video coding framework. Our framework is built on deep
networks entirely to achieve continuous optimization, and we
add multiple starting points (i.e. multiple modes) to achieve
discrete optimization. We conduct a comprehensive set of
experiments to verify the efﬁciency of our hybrid optimization.
Experimental results show that compared to the continuous
optimization framework, our proposed method outperforms the
pure end-to-end learned video coding methods. Meanwhile,
compared to discrete optimization framework, our proposed
method achieves comparable performance to HEVC reference
software HM16.10 in PSNR.

The remainder of this paper is organized as follows. In
Section II, we review the existing video coding works from
the perspective of optimization. In Section III, we theoretically
analyze the optimization in video coding and then proposes a
hybrid-optimization theory. Section IV presents our designed
hybrid-optimization coding framework guided by that theory.

Section V presents the implementation details and Section VI
presents the hybrid optimization procedure. In Section VII,
we show the experimental results. Section VIII discuss the
beneﬁts and potential of proposed framework, and Section IX
concludes this paper.

II. REVIEW OF OPTIMIZATION IN VIDEO CODING

In this section, we review the development of rate-distortion
optimization in video compression. Theoretically, the opti-
mization objective in video coding is: minimize distortion D,
subject to a constraint Rc, on the number of bits used R.
To solve the optimization of video coding, it’s necessary to
give a coding framework or model (i.e mathematical function
expression) ﬁrstly. Based on the model, the solving process
can generally be divided into two steps [11]: (i) ofﬂine step:
optimize initial parameters of the model from a large number
of training sequences; (ii) online step: striving to further
optimize parameters of the model for each test sequence to
obtain the best coding performance. Therefore, ofﬂine step
focuses on how to get the general model parameters as initial
optimization points, while online step concentrates on how
to get the special model parameters for each coding sample
as the ﬁnal optimization solution. Although all video coding
optimization algorithms follow them, those algorithms have
different emphasis on the two steps and then adopt different
optimization ideas correspondingly. According to those differ-
ences, the rate-distortion optimization in video compression
can be divided into four categories.

The ﬁrst is the optimization in classic block-based hybrid
video coding, which is the earliest and most widely used
optimization scheme. On the ofﬂine step, it directly sets the
parameters of modes in the framework without optimization.
Then it searches for the optimal mode online from some
designed modes for a given coding sequence. When the
number of modes is small,
is an effective optimization
it
scheme. However, with the increase of modes, the searching
cost becomes higher and higher, making the coding framework
hard to be optimized.

To address this problem, the second kind of optimization
scheme has appeared, which utilizes some numerical algo-
rithms to online optimize some parts of block-based hybrid
video coding more efﬁciently. The ﬁrst and second classes
of optimization concentrate on the online step, so they can
achieve adaptive coding for each sequence. Unfortunately, the
encoders with too high calculation cost at the online step will
seriously affect the practical application of coding. However,
the computational cost at the ofﬂine step has less effect on the
coding.

Accordingly,

the third kind of optimization scheme has
emerged in block-based hybrid video coding, which adopts
the numerical optimization algorithms at the ofﬂine step to
achieve more efﬁcient optimization for some parts. Those
algorithms usually train a deep network to replace a hand-
optimized module of block-based hybrid video coding, so they
are also called deep tools. Numerical optimization can not only
speed up the entire optimization process but also obtain ﬁner
optimization results, so it shows great potential. However, the

3

TABLE I.

REVIEW OF OPTIMIZATION IN VIDEO CODING

Optimization
Space Formulation

Optimization
Method
(Ofﬂine)

Optimization
Method
(Online)

Optimization
Objective

Representative Works

Discrete

Discrete &

Locally Continuous

Discrete &

N/A

N/A

N/A

N/A

Search

Approximate

H.120, H.261, MPEG-1, MPEG-2,
H.263, H.264, H.265

Approximate

H.266 (Afﬁne ME, ALF, BIO,
CCLM), AIF, LM, LIC

Search

Numerical

Search

Locally Continuous

Numerical

N/A

Approximate

Deep Tools, H.266 (MIP)

Globally Continuous

Numerical

N/A

End-to-End

End-to-End Learned Video Coding

Category

Classic Block-based Hybrid
Video Coding
(1984-now)

Block-based Hybrid Video Coding
with
Online Numerical Optimization
(1996-now)

Block-based Hybrid Video Coding
with
Deep Tools
(2015-now)

End-to-End Learned
Image/Video Coding
(2015-now)

deep tools only use numerical optimization on some parts of
the coding framework. In order to fully explore the potential
of numerical optimization, it is natural to think whether all the
search-based modules in the coding framework can be replaced
with numerical optimization ones.

Then the fourth kind of optimization scheme was proposed,
in which the whole coding framework adopts the numerical
optimization algorithms at the ofﬂine step. Those frameworks
are usually on top of deep networks. What’s more,
those
frameworks are end-to-end trained at
the ofﬂine step and
directly infer the coding results for each sequence at the online
step, so they are also called end-to-end learned video coding.
In the following, we analyze the four categories of rate-
distortion optimization in video compression and correspond-
ing coding technologies in detail. The characteristic of each
category is summarized in Table I, and the representative works
of each category are analyzed in detail in Table II.

A. Classic Block-based Hybrid Video Coding

Motion video data consists essentially of a time-ordered
sequence of pictures. The most straightforward method of
compressing video content is to compress each picture simply
using an image-coding syntax. This method uses only one
image-coding mode to deal with all video content. However,
much of the depicted scene is essentially just repeated in
picture after picture without any signiﬁcant change. Image
coding for video data doesn’t
take advantage of temporal
redundancy.

A simple solution is coding only the changes in a video
scene. Based on this idea, the ﬁrst international digital video
coding standard H.120 [12] was developed by the ITU-T
organization and received ﬁnal approval in 1984. H.120 ofﬂine
designs a conditional replenishment (CR) method to selectively
code the change. CR consists of two coding modes: SKIP and
INTRA mode. The SKIP mode is sending signals to indicate
which areas are repeated, and the INTRA mode is sending new
coded information to replace the changed areas. Accordingly,
the INTRA mode is used to handle the large scene changes,

while the SKIP mode can effectively deal with the repeated
scenes. CR allows searching the best choice between two
modes of representation for each area online. Although CR
is very simple in the algorithm, its optimization ideas lay
the foundation for later video coding standards. That is, CR
introduces multi-mode optimization, which designs various
modes for different situations and searches for the best mode
among those discrete modes. At this time, the optimization
space in CR is still small because it only has two modes.

CR coding only allows exact repetition or complete replace-
ment, but the content of a prior picture often can be a good
approximation. Based on this idea, the ﬁrst practical success
video coding standard H.261 [13] is approved by the ITU-T
in early 1991, which can be capable of operation at affordable
telecom bit rates. Speciﬁcally, the motion estimation (ME) in
the encoder searches for the best integer spatial displacement.
The motion-compensated prediction (MCP) uses the spatial
displacement of the prior picture to form an approximation
prediction. Next, the displaced frame difference (DFD) coding
method codes the resulting difference to reﬁne the MCP
signal. Meanwhile, the processing unit is based on the block
(16 × 16 macroblock in MCP and 8 × 8 block in DCT).
Accordingly, the basic coding structure in H.261 is called
block-based hybrid video coding due to its construction as
a hybrid of motion-handling and picture-coding techniques at
the block level. H.261 was the ﬁrst standard to use the basic
typical structure we ﬁnd still predominant today. Compared to
H.120, H.261 still uses multi-mode optimization but greatly
enlarges the optimization space. Because in addition to CR
mode choices, the ME and MCP modes provide a large amount
of discrete motion vector (MV) choices. With the help of more
mode choices, the representative ability of the ofﬂine designed
model improves signiﬁcantly. Then the better coding choice
is searched for online among those choices, so the coding
performance is improved.

Based on the H.261 hybrid coding framework and multi-
mode optimization, a series of standards were approved sub-
sequently, including MPEG-1 [14] in 1993, MPEG-2 [15] in

4

TABLE II.

OVERVIEW OF REPRESENTATIVE WORKS IN VIDEO CODING OPTIMIZATION

Method

Year

Optimization
Objective

Bisic idea

Classic Block-based Hybrid Video Coding

H.120

H.261

1984

1991

MPEG-1, MPEG-2,
H.263

1993, 1994,
1996

Rate-Distortion
Optimization

1998

H.264, H.265

2003, 2012

D

D

D

RD

RD

Propose the CR scheme with 2-mode searching, and
introduce the multi-modes optimization idea for the ﬁrst time

Introduce the MCP mode with multiply MV choices, and build
the block-based hybrid video coding framework for the ﬁrst time

Introduce more modes, such as half-pixel motion, variable block sizes, etc,
but still consider only distortion in the optimization objective

Consider both rate and distortion in the optimization objective
for the ﬁrst time

Introduce more modes, such as quarter-pixel motion, merge, etc,
and adopt RD optimization but use approximate RD cost in ME

Block-based Hybrid Video Coding with Online Numerical Optimization

Accelerate Classic ME

Accelerate Afﬁne ME

Linear Model
(Linear Prediction,
LIC, CCLM)

Adaptive
Interpolation Filter

Adaptive
Loop Filter

Bi-Directional
Optical Flow

Filtering Tools

Prediction Tools

Transform Tools

Entropy Coding Tools

Down- and Up-
Sampling Tools

MIP

Accelerate Block
Partition/Mode Decision

Variable-Rate RNN-
based Image Coding

RD Optimization-
based Image Coding

Deep Video
Compression (DVC)

Hierarchical
Learned Video
Compression (HLVC)

Feature Video
Compression (FVC)

Online Encoder
Updating

Resolution-adaptive
Flow Coding

1996

2018

1998,
2015,
2018

2003

2008

2010

2015

2018

2018

2017

2018

2018

2016

2015

2017

2019

2020

2021

2020

2020

RD

RD

RD

RD

RD

RD

Search for MV towards the direction of gradient descent

Optimize afﬁne parameters towards the direction of
gradient descent in a high dimensional continuous space

Optimize the linear model by least squares method to predict current pixel
from the neighboring pixels, or the reference pixels, or another component pixels

Optimize the Wiener ﬁlter by least squares method to perform adaptive
sub-pixel interpolation on the reference frame

Optimize the Wiener ﬁlter by least squares method to perform adaptive
loop ﬁltering on the current reconstructed frame

Calculate the analytic solution of optical ﬂow gradient to reﬁne the bi-prediction

Block-based Hybrid Video Coding with Deep Tools

D

RD

RD

R

RD

RD

Optimize DNN to reﬁne the reconstructed frame

Optimize DNN to perform or reﬁne the intra/inter prediction

Optimize DNN to perform the transform

Optimize DNN to predict the probability distribution based on the context

Optimize DNN to achieve the adaptive resolution coding

Optimize simpliﬁed DNN in H.266 to predict the current block from reference pixels.
This is the ﬁrst deep tool adopted by video coding standards.

Accuracy

Optimized DNN to decide the CU partition result directly
or to reduce the mode searching range

End-to-End Learned Image/Video Coding

D

RD

RD

RD

RD

RD

RD

End-to-end optimize DNN for image compression for the
ﬁrst time, but use only distortion in the optimization objective

End-to-end optimize DNN for image compression by RD cost

End-to-end optimize DNN for video compression for the ﬁrst time

End-to-end optimize DNN for B-frame compression with hierarchical quality layers

End-to-end optimize DNN for video compression in the feature space

Online end-to-end optimize the DNN-based encoder in deep video compression
for each test sequence

Online search from multi-resolution modes in deep video compression

1994, and H.263 [16] in early 1996. They add more advanced
modes, such as half-pixel motion mode and bi-prediction mode
in MPEG-1, variable block-size motion compensation mode
(16 × 16 and 8 × 8) in H.263, and so on. More modes expand
the optimization space further, so the coding performance is
improved further. However, in all those standards, the multi-
mode search criterion just considers distortion but not rate, so
the optimization objective is approximate.

To address this problem, the rate-distortion optimization
method [11], [17] was proposed in 1998, which considers both
the rate and distortion in the optimization objective. These
works theoretically analyze that the video coding needs to
optimize both the rate and distortion, and then they introduce
the Lagrangian method to solve the optimization problem ef-
fectively. The Lagrangian optimization changes the constrained
optimization problem into an unconstrained problem so that
the optimization objective becomes minimizing the Lagrangian
rate-distortion cost D + λR. This rate-distortion optimization
has a signiﬁcant impact on video coding. All later video coding
standards adopt this RD optimization, such as H.264 [18],
H.265 [19], and so on. Those standards contain more modes
and use RD optimization to search for the best one effectively.
In the speciﬁc implementation, the optimization objective in
most modes is accurate, where R and D are based on the ﬁnal
stream and reconstructed frame, respectively. However, in ME,
due to the limitation of computational power, the optimization
objective is approximated, i.e., R and D are based on the MV
bits and prediction frame. Therefore, the optimization objective
in those standards is still approximate.

From the above review, we can ﬁnd that the mentioned
block-based hybrid video coding standards have some same
characteristics in optimization. First, all the ofﬂine designed
models in those standards consist of many discrete modes [20],
[21], including INTRA mode, SKIP mode, partition mode,
the
MCP mode with lots of MV choices, etc. Therefore,
optimization parameter space in those standards is discrete.
Second, at the ofﬂine step, the model parameters are almost
not optimized but hand-crafted. It makes efforts to online
search for the optimal mode from the discrete modes for each
sequence. Thus, the optimization method is online searching.
Third, the optimization objective is approximate rather than
actual RD cost.

Although this multi-mode search optimization has achieved
great success, there are still some problems. First, with the
increase of modes, the cost of searching has increased signif-
icantly, making the framework hard to be optimized. Second,
search optimization can only conduct on a ﬁnite number of
discrete modes. Considering the searching complexity, it can’t
perform more reﬁned optimization on the parameters.

B. Block-based Hybrid Video Coding with Online Numerical
Optimization

To address the inefﬁcient problem of search optimization,
some online numerical optimization methods appeared, which
replace part of the search optimization in block-based hybrid
video coding. Compared to search optimization, numerical
optimization [22] not only is more efﬁcient but also makes

5

the optimization parameters more reﬁned. According to the
two advantages, the existing numerical optimization methods
can be divided into two categories. The ﬁrst is for improving
optimization speed, and the second is to increase compression
efﬁciency.

The ﬁrst is mainly to use gradient descent-based method to
accelerate the multi-modes search. In the classical block-based
hybrid coding, it is difﬁcult to establish a simple mathematical
model
to associate different characteristic modes, such as
INTRA mode, SKIP mode, and MCP mode. However, the
different MV choices within MCP mode and RD cost can be
easily modeled with the mathematical function. Based on the
characteristic, in 1996, an online gradient descent optimization
algorithm for ME was proposed [23]. It calculates the gradient
of the objective function to MV, and the search for MV always
moves in the direction of optimal gradient descent. Compared
to the exhaustive search on discrete points, gradient-based
optimization can fully use the continuous characteristics and
exclude many unnecessary search points. Accordingly, it can
conduct ME more efﬁciently. Later on, a series of gradient
descent-based optimization for ME was developed [24]–[26].
However, because the search points of classical MVs are
not many enough, search-based optimization can still handle
ME well. Thus, gradient descent-based optimization has not
attracted widespread attention.

Until recent years, it’s hard for the classical translational
motion model to improve performance further, so the more
advanced afﬁne motion model [27] has been adopted in the
latest video coding standards H.266/VVC [28], [29]. The
afﬁne motion parameters are no longer simple MVs, but more
complex, larger range and higher precision control parameters,
which is more difﬁcult for the classical search-based ME.
To efﬁciently optimize the afﬁne motion parameters, online
gradient descent-based ME plays a key role. The gradient-
based method can not only handle the parameters in a high
dimensional continuous space [30], such as afﬁne parameters,
but also quickly optimize the best parameters with only a
few iterations [27]. However, the optimization objective is still
approximate like the classical ME.

The second category is to calculate the analytic solution by
some numerical methods to perform more reﬁned optimization,
including least-squares optimization and others. The least-
squares optimization is to ﬁt the mapping relationship with the
smallest error for each group of signals. It is mainly applied
in adaptive prediction and ﬁlter in video coding. As for the
adaptive prediction, those works use least squares to online
optimize the linear model (LM) on pixels within or across
the component. Those works build the linear model to predict
the current pixel from the neighboring pixels (called linear
prediction) [31]–[33], or the reference pixels (called local
illumination compensation (LIC)) [34], or another component
pixels (called cross-component linear model (CCLM)) [35].

As for the ﬁlter, the work is to use least squares to optimize
the Wiener ﬁlter to conduct adaptive ﬁltering. But the ﬁlter
are operated on different frames, which is either on the
reference frame [36]–[38] (called adaptive interpolation ﬁlter
(AIF)) or in the current reconstructed frame [39], [40] (called
adaptive loop ﬁlter (ALF)). AIF uses some Wiener ﬁlters

6

to interpolate multiply sub-pixel images adaptively from the
reference frames, and then those frames are used by MC to
generate a more accurate prediction. ALF uses the Wiener
ﬁlters to enhance the current reconstructed frame adaptively.
By efﬁciently and ﬁnely online optimizing the Wiener ﬁlter
with the least squares method, AIF and ALF signiﬁcantly
improve coding performance. Meanwhile, ALF also is adopted
by H.266 standard as a core tool.

In addition to those mentioned technologies, numerical
methods also are applied in other technologies in H.266, such
as the bi-directional optical ﬂow (BDOF) [41]. BDOF uses
the optical ﬂow concept to reﬁne the bi-prediction signal.
It uses numerical methods to online calculate the analytic
solution of the optical ﬂow gradient and then derives the
reﬁnement. Compared to search optimization on some discrete
points, numerical methods can online optimize the parameters
of models (e.g. the linear model, adaptive ﬁlters, and optical
ﬂow model) in the continuous space to achieve more reﬁned
optimization.

From those works, we can ﬁnd that the mentioned methods
have some same characteristics in optimization. First, these
ofﬂine designed models are still based on discrete modes,
that is, following block-based hybrid video coding, but they
choose some highly correlated choices within a mode to make
them locally continuous. Second, the model optimization still
focuses on the online step. It still uses the search-based method
for discrete modes, while it uses the numerical method for
continuous parts to achieve efﬁcient and reﬁned optimization.
Third, the optimization objective is approximate because those
tools only considers the RD cost of their local parts rather than
the whole RD cost, such as the prediction RD cost for gradient
descent-based ME and prediction RD cost for AIF.

Although online numerical optimization is more efﬁcient
than search optimization, the optimization still focuses on the
online step. The high computational cost of online optimization
will limit the wide application of the codec.

C. Block-based Hybrid Video Coding with Deep Tools

Compared to online numerical optimization, the compu-
tational cost of ofﬂine optimization has less effect on the
coding. Correspondingly, some ofﬂine numerical optimization
methods appeared. It not only retains the two advantages of
efﬁcient and reﬁned optimization in numerical optimization,
but also changes the optimization stage from online to ofﬂine.
Those algorithms usually take ofﬂine trained deep networks
as tools of block-based hybrid video coding, and then use the
gradient descent-based numerical method to optimize the net-
work. Similarly, according to the two advantages of numerical
optimization, the existing deep tools can also be categorized
into two groups [42]. The ﬁrst group is to increase compression
efﬁciency, and the second is to improve encoding speed.

The ﬁrst group of deep tools may either replace the cor-
responding hand-optimized module with a deep network, or
newly add a deep network into the scheme. It involves vari-
ous tools, including prediction tools, transform tools, entropy
coding tools, ﬁltering tools, down- and up-sampling tools, and
so on. Since some online numerically optimized tools have

been developed, the straightforward idea is to replace those
online tools with ofﬂine optimized networks. Among those
tools, the ﬁltering tool improves compression efﬁciency most
signiﬁcantly. Correspondingly, the deep network with ofﬂine
numerical optimization was ﬁrst applied in the reconstruction
ﬁltering module in video coding. In 2015, Dong et al. [9]
propose a CNN for compression artifacts reduction on JPEG,
namely ARCNN. They ofﬂine designed the network with 4
convocation layers and optimized the parameters with the
gradient descent method through lots of training samples. In
the test, ARCNN doesn’t need online optimization, which
directly infers the reﬁned image. Experimental results show
that ARCNN achieves incredible performance, i.e. more than
1dB improvement in PSNR than JPEG. ARCNN proved the
feasibility of deep tools with ofﬂine numerical optimization
and demonstrated its great potential. Subsequently, a series of
deep ﬁltering tools are proposed [43]–[47], which occupies the
majority of the deep tools. Although the works have different
network structures, their optimization ideas are the same as
that of ARCNN.

Inspired by the great success of deep ﬁltering tools, other
deep tools have been proposed [48]–[62]. Some of them
replace other online numerically optimized tools with ofﬂine
optimized networks, and some develop new tools. For example,
for the online linear prediction tool, Li et al. [53] propose an
ofﬂine optimized fully connected network for intra prediction,
which also predicts the current pixel from neighboring pixels.
Similar to the idea of LIC, Huo et al. [54] propose an ofﬂine
optimized CNN to reﬁne the inter prediction signal. Analogous
to CCLM, Li et al. [55] propose an ofﬂine optimized CNN to
perform cross-channel prediction. Like AIF, Yan et al. [56],
[57] propose to take ofﬂine optimized CNNs as ﬁlters to
perform sub-pixel interpolation on the reference frames. In
addition to these existing tools with numerical optimization,
there are also many newly developed ofﬂine optimized deep
tools. For example, for the prediction module, Huo et al.
[58] propose a CNN-based new inter prediction mechanism
by extrapolating the current frame from the multiple reference
frames and taking it as another reference frame. For the trans-
form module, Liu et al. [59] propose a CNN-based method
to achieve a DCT-like transform for image coding. For the
entropy coding module, Song et al. [60] optimize a CNN to
predict the probability distribution of the intra prediction mode
based on the context. For the down- and up-sampling coding
module, Li et al. [61], [62] propose a CNN-based down- and
up-sampling method to achieve the adaptive resolution coding.
Due to the high compression efﬁciency, H.266 standard
has adopted an ofﬂine numerical optimization-based tool, i.e.
matrix-based intra-picture prediction (MIP) [48]. MIP uses the
neural network to predict the current block from the reference
samples. The neural network has been simpliﬁed to a matrix-
vector multiplication, with the matrix being selected from a set
of pre-trained matrices and the vector being constructed from
the reference samples. In order to ofﬂine optimize the matrix
coefﬁcients from lots of training samples, a gradient descent-
based numerical algorithm (Adam [63]) has been used.

The second group of deep tools is intended for improving
encoding speed [64]–[68]. Based on the analysis in Section

II-B, the choice within a speciﬁc mode and its RD cost can
be easily modeled with the mathematical function. Among the
core coding modes, the MCP mode and the block partition
mode have the most internal choices, i.e. multiply MV choices
in MCP mode and various block combination choices in
partition mode. However, due to the complexity of motion and
the massive MV choices, it is difﬁcult for the ofﬂine optimized
network to predict MV accurately. Therefore, only a few
ofﬂine numerical optimization methods accelerate MCP mode
decision [69], and most methods focus on accelerating partition
choice selection. In 2016, Liu et al. [64] presented an ofﬂine
optimized CNN to help decide CU partition mode for HEVC
intra encoder for the ﬁrst
time. Without searching among
multiply partition choices, the trained CNN will directly decide
whether to split CU or not based on the content and the
speciﬁed QP.

From those works, we can ﬁnd that the mentioned methods
have some same characteristics in optimization. First, these
ofﬂine designed models are still based on discrete modes, i.e.
following block-based hybrid video coding, but they make
part modes or tools locally continuous. Second, the model
optimization is not only on the online step but also on
the ofﬂine step. The continuous parts are optimized ofﬂine
by the numerical method, while the discrete parts are opti-
mized online by searching. Third, the optimization objective
is approximate, because the optimization objective of those
deep tools is based on either the accuracy (e.g. fast partition
decision), or only D (e.g. ARCNN), or only R (e.g. CNN-
based entropy coding), or part RD (e.g. CNN-based residual
transform), rather than the actual RD cost.

Numerical optimization can not only speed up the opti-
mization process but also perform more reﬁned optimization.
However, the deep tools only use numerical optimization on
some parts of the coding framework. We would like to ask
whether the whole coding framework can be optimized with
numerical methods.

D. End-to-End Learned Image/Video Coding

Based on the idea of the whole coding framework with nu-
merical optimization, end-to-end learned image/video coding
methods were proposed. The coding model is built on top
of deep networks entirely. At the ofﬂine step, one group of
average optimal model parameters are end-to-end optimized
by numerical algorithms through lots of training samples. At
the online step, the ofﬂine optimal model usually directly
infers the coding results for each sequence, which is not
optimized further. Compared to deep tools with local numerical
optimization, end-to-end learned coding can perform more ef-
ﬁcient and more reﬁned optimization for the whole framework.
Meanwhile, the whole framework takes the actual RD cost as
the optimization objective and unites all numerical modules to
optimize in an end-to-end manner. Next, we will review the
end-to-end learned image and video coding in turn.

Since image coding is simpler than video coding, the end-
to-end learned method was initially tried in image coding.
In 2015, Toderici et al. [10] proposed an ofﬂine end-to-
end optimized RNN-based framework for variable rate image

7

compression. They use binary quantization to generate codes,
and do not consider the rate during ofﬂine training, i.e. the
optimization objective is only end-to-end distortion. This work
builds a coding framework with all numerical optimization
modules for the ﬁrst time and initially veriﬁes the feasibility.
However, the optimization objective only based on distortion
limits the improvement of efﬁciency.

To address this problem, many rate-distortion optimized
end-to-end image compression frameworks are proposed [70]–
[77]. The most representative works of CNN-based methods
are from Ball´e et al. [70]–[72]. Ball´e et al. proposed various
end-to-end RD optimized image coding frameworks, using
the factorized-prior [70], hyper-prior [71] and autoregressive
prior [72] models to estimate the entropy of the latent feature
efﬁciently. What’s more, Ma et al. [75] introduce a special end-
to-end framework for both lossy and lossless image compres-
sion, which adopts a trained wavelet-like transform to generate
coefﬁcients and a context-based entropy model to code those
coefﬁcients effectively. Those works take the RD cost as
the objective to end-to-end optimize the whole image coding
frameworks with numerical methods. Experiment results show
that the latest end-to-end image coding method has achieved
competitive performance with H.266, which fully veriﬁed
the feasibility of end-to-end RD optimization with numerical
methods.

Inspired by the successes in end-to-end image coding,
some end-to-end learned video coding methods have been
proposed [78]–[91]. Different from image coding that removes
the spatial redundant, video coding should also remove the
temporal redundant. For this purpose, inter-picture prediction is
a critical issue in video coding. Accordingly, end-to-end video
coding is more complex than image coding. In 2018, Wu et al.
[79] proposed a recurrent neural network (RNN) based video
compression approach. To code the B frame, it performs ME
and then takes the obtained motion information to interpolate
from two reference frames. However, the ME in this work is
not carried out by learned networks, but by classical search
methods. Therefore, this work is not end-to-end optimized.

Later on, a real end-to-end deep video compression frame-
work (DVC) [81], [82] was proposed by Lu et al. for the
ﬁrst time in 2019. All modules in the framework are based
on deep networks, including ME, MC, motion information
compression, and residual compression modules. The entire
network is jointly ofﬂine optimized by numerical methods with
a single loss function, i.e., the joint rate-distortion cost. Based
on this optimization idea, a series of end-to-end optimized
video compression methods were proposed [83]–[92]. For
example, Yang et al. [87] proposed a hierarchical end-to-
end learned video compression (HLVC) for B-frame coding,
which introduces hierarchical quality layers and a recurrent
enhancement network to make full use of the temporal corre-
lation. Hu et al. [91] proposed an end-to-end optimized feature-
space video coding framework (FVC). It performs all major
operations (i.e. ME, MC, motion compression, and residual
compression) in the feature space.

From those works, we can ﬁnd that the mentioned methods
have some same characteristics in optimization. First, these
ofﬂine designed models are based on continuous function

8

Fig. 1.

Illustration of encoder-decoder in video coding.

entirely, so the optimization parameter space is continuous.
Second, the models are optimized only on the ofﬂine step,
and they directly infer the coding results on the online step.
They use the numerical method to optimize the continuous
model ofﬂine. Third, the optimization objective is the actual
RD cost, so the optimization is end-to-end.

Based on all the above analyses in this section, we found that
different kinds of coding frameworks use different optimiza-
tion methods, and different optimization methods also have
different advantages. First, ofﬂine optimization can reduce the
computational cost, and online optimization can achieve adap-
tive coding for each video. Second, numerical optimization in
continuous space is more efﬁcient and reﬁned in local parts,
and search optimization in discrete space can ﬁnd better coding
parameters globally among multiple modes. Third, it is more
efﬁcient to end-to-end optimize with the actual RD cost. We
can ﬁnd that using only one optimization method can not
achieve all the advantages. Therefore, we would like to ask
whether those optimization methods can be combined together
to address this problem.

Based on this idea, a few coding methods have emerged
[93], [94]. To make end-to-end learned coding adaptive for
each video, Lu et al. [93] conduct online end-to-end numerical
optimization on the ofﬂine end-to-end trained DVC model.
It optimizes the motion information (i.e. optical ﬂow) with
gradient-descent methods at the encoder side online without
changing the decoder. However, online gradient-descent opti-
mization starting from only one group of ofﬂine trained model
parameters may fail into the local minima. Hu et al. [94] add
multi-resolution modes search optimization into ofﬂine end-
to-end trained DVC to improve ﬂow coding efﬁciency. Based
on the ofﬂine numerically optimized model, this work online
searches for the mode with the lowest RD cost from a few
resolution ﬂow modes. The optimization in this method is
end-to-end, i.e. with the actual RD cost. However, this work
performs numerical optimization only at the ofﬂine step, but
doesn’t conduct online to achieve more reﬁned optimization.
Meanwhile, the number of modes is too few, making it hard
to search for the globally optimal parameters.

III. THEORETICAL ANALYSIS OF OPTIMIZATION

A. Optimization Problem Formulation

The theoretical foundations of lossy compression are rooted
in Shannon’s seminal work on rate-distortion theory [95],
[96]. Rate-distortion theory analyzes the fundamental tradeoff

between the rate used for representing samples from a data
source variable X, and the expected distortion incurred in
decoding those samples from their compressed representations.
Formally, the relation between the input variable X (original
signal) and output variable ˆX (compressed signal) is a (pos-
sibly stochastic) mapping deﬁned by conditional distribution
P ( ˆX|X). The stochastic mapping P ( ˆX|X) is also called a
test channel. Based on those, Shannon’s theory deﬁned the
classic optimization problem of rate-distortion. That is, for a
data source variable X, if the expected distortion is restricted
to be bounded by Dc, then it optimizes to obtain the lowest
rate

I(X, ˆX) s.t. E[D(X, ˆX)] ≤ Dc

(1)

min
P ( ˆX|X)

where I denotes mutual information. For the video coding
is implemented by an encoder-
problem,
decoder pair F , i.e. ˆX = F (X). The encoder-decoder pair F is
a deterministic mapping (a special case of stochastic mapping)

the test channel

P ( ˆX|X) =

(cid:26)

1, if ˆX = F(X)
0, otherwise

(2)

The input X must be mapped to output ˆX and cannot
be mapped to other outputs, so the conditional probability
P ( ˆX|X) is either 0 or 1. Because the encoder-decoder space
F is huge, it is usually necessary to set the hyper-parameters
of the encoder-decoder ﬁrst, i.e. designing model structure
f , to determine the optimized parameter space. The hyper-
parameters are not optimized, while the parameters Θ in the
model structure need to be optimized in space Ω. Thus, the F
can be converted to a parametrical expression f (Θ)

F (X) (cid:44) f (X|Θ), Θ ∈ Ω

(3)

The model’s parameters Θ consists of the encoder’s parameters
Θenc and the decoder’s parameters Θdec

Θ = Θenc and Θdec

(4)

Accordingly, the optimization problem Eq.(1) can be expressed
as

I(X, ˆX) s.t. E[D(X, ˆX)] ≤ Dc

(5)

min
Θ∈Ω

the mutual

in-
According to Shannon’ information theory,
formation I(X, ˆX) is equal
to the difference between the
entropy H( ˆX) with the conditional entropy H( ˆX|X). Due
to the deﬁnition of entropy and the deterministic mapping in
Eq.(2), the conditional entropy H( ˆX|X) must be equal to 0,
i.e. H( ˆX|X) = 0. Thus, the the mutual information I(X, ˆX)
is equal to the entropy H( ˆX)

I(X, ˆX) = H( ˆX) − H( ˆX|X)
= H( ˆX)

(6)

In the practical coding, it uses the syntax to express the
compressed signal. Speciﬁcally, the original signal X is input
into the encoder to generate the syntax Z, and then the syntax
Z is input into the decoder to obtain the compressed signal
ˆX, as shown in Fig.1.

Z = fenc(X; Θenc); ˆX = fdec(Z; Θdec)

(7)

𝑓𝑒𝑛𝑐(Θ𝑒𝑛𝑐)𝑓𝑑𝑒𝑐(Θ𝑑𝑒𝑐)𝑋𝑍෠𝑋𝑓(Θ)Specially, for the determined decoder fdec(Θdec), one syntax
Z products only one decided output ˆX, but one ˆX may come
from multiple syntax Z. Thus, fdec(Θdec) may be not one-to-
one mapping (not injective function). Accordingly, H(Z) is
the upper bound of H( ˆX)

H( ˆX) ≤ H(Z)

(8)

Since it is hard to measure H( ˆX), to minimize H( ˆX), we
can change to minimize its upper bound H(Z). We denote the
probability distribution of the variable Z as Q, i.e. Z (cid:118) Q,
and the entropy H(Z) is

H(Z) = −

(cid:88)

j

Q(zj)logQ(zj)

(9)

However, the true probability distribution Q usually is un-
known, so it uses an estimated probability distribution B to
approximate Q to code Z.

H(Q, B) = −

(cid:88)

j

≥ H(Z)

Q(zj)logB(zj)

(10)

H(Q, B) is called cross entropy of between two probability
distributions Q and B. We can ﬁnd that H(Q, B) is the upper
bound of H(Z), so H(Z) can be minimized by minimizing
its upper bound H(Q, B). Accordingly, we convert optimizing
mutual information into optimizing entropy. On the other hand,
the entropy represents the average bit rate. Thus, in the coding
problem, optimizing mutual information is equivalent to op-
timizing the bit rate. We donate the calculation of bit rate as
R(), and R(Z) presents the bit rate of coding syntax Z, i.e. the
cross entropy H(Q, B). Based on above formula derivation,
the RD optimization problem Eq.(5) can be expressed as

R(Z) s.t. E[D(X, ˆX)] ≤ Dc

(11)

min
Θ∈Ω

For a data source variable X, if the expected distortion is
restricted to be bounded by Dc, then the encoder-decoder
model parameters Θ are optimized to obtain the lowest bit
rate of coding syntax Z, where syntax Z is the output of the
encoder and used for decoding the compressed signal ˆX.

In the above derivation of Shannon’s theory, the source is
regarded as a random variable, which describes the source as a
probability distribution. However, in practical video coding, the
source is the samples of the distribution. Due to this difference,
to adapt to the practical samples’ coding, some limited condi-
tions are usually added to the theoretical formula. Accordingly,
the optimization problem will be changed. According to the
amount of coding samples, the practical optimization problem
can be categorized into two groups.

The ﬁrst is to optimize the encoder-decoder parameters Θ
for lots of samples. The set of samples can be considered to
represent the distribution of X. To minimize the loss on the set
of samples, an average optimal encoder-decoder parameters Θ
is optimized. Speciﬁcally, a total of K samples x1, · · · , xK
generate K groups of syntax z1, · · · , zK through the encoder,

9

and then the syntax generate K corresponding compressed
samples ˆx1, · · · , ˆxK, respectively

zk = fenc(xk; Θenc); ˆxk = fdec(zk; Θdec); k = 1, · · · , K

(12)
For the set of samples, the rate part becomes the upper bound
of bit rate for coding all samples

R(zk) = −logB(zk); k = 1, · · · , K

(13)

The distortion part becomes

E[D(X, ˆX)]|X=x1,··· ,xK ; ˆX=ˆx1,··· ,ˆxK

=

1
K

K
(cid:88)

D(xk, ˆxk)

k=1

(14)
the RD optimization problem

Based on those equations,
Eq.(11) can be expressed as

min
Θ∈Ω

K
(cid:88)

k=1

R(zk) s.t.

1
K

K
(cid:88)

k=1

D(xk, ˆxk) ≤ Dc

(15)

For lots of source samples, if the average distortion is restricted
to be bounded by Dc, then it optimizes an average optimal
encoder-decoder model parameter group Θ to obtain the lowest
sum of bits of coding all samples’ syntax. The average optimal
parameters can be directly deployed on the encoder/decoder
side for encoding/decoding all the samples.

The second is to optimize for only one given sample. Dif-
ferent from the average optimum for lots of samples, for each
given sample, we can optimize a group of optimal encoder-
decoder parameters Θ. However, as for the optimal decoder
parameters Θdec for each sample, deploying parameters Θdec
for all samples on the decoder side will take up lots of storage,
while transmitting them to the decoder side will pay lots of
bits. Therefore, for the optimization of one given sample, it
usually needs to ﬁx the decoder parameters Θdec. It only opti-
mizes the encoder parameters Θenc for each sample to match
decoder parameters Θdec to achieve the best performance.
Speciﬁcally, a given sample x, the syntax z, compressed signal
ˆx, rate R(z), and distortion can be calculated by

z = fenc(x; Θenc); ˆx = fdec(z; Θdec)
R(z) = −logB(z)
E[D(X, ˆX)]|X=x, ˆX=ˆx = D(x, ˆx)

(16)

Based on those equations,
Eq.(11) can be expressed as

the RD optimization problem

min
Θenc∈Ωenc

R(z) s.t. D(x, ˆx) ≤ Dc

(17)

where the Ωenc is the optimization space of Θenc. For a given
sample, if the distortion is restricted to be bounded by Dc, the
decoder parameters Θdec is ﬁxed and the encoder parameters
Θenc are optimized to obtain the lowest bit rate of coding this
sample’s syntax.

To solve this kind of constrained optimization problem, the
discrete version of Lagrange multipliers [97] can be used to
convert it into an unconstrained problem. The optimized goal

10

is converted to minimizing the rate-distortion cost J of the
optimized parameters Θ. Thus,

At the online step, it searches the best encoder parameters
Θ∗

enc from the set Ψ for the coded sample x

Θ∗ = arg min

J, J = D + λR

(18)

Θ∈Ω

where Θ∗ donates the theoretically optimal parameters.

Although there are two coding optimization problems (i.e.
for lots of samples or a single sample), the ultimate goal is to
optimize the best parameters for each coding sequence. The
optimization for lots of samples also serves for the optimiza-
tion of a single sample. Due to the complexity of coding
problems, although the hyper-model is given, the parameter
space for optimization is by far too large to be evaluated. Thus,
the optimization is non-convex and difﬁcult to be solved. Over
the years, a large number of studies have strived to solve the
video coding optimization problem approximately.

B. Optimization Problem Solution

For given a video sequence x, the solution to the opti-
mization problem of this samples is generally divided into
two steps: ofﬂine determining the optimization space and
optimizing the initial parameters through lots of samples, and
online optimizing the parameters based on the ofﬂine model
for the coded sample. In the speciﬁc implementation, there are
mainly two popular optimization solutions.

The one is still regarding the optimization of the test
channel as a discrete optimization problem, and mainly using
search-based optimization to solve it. The typical example
is block-based hybrid coding. To explain the optimization
for a given data x to be coded more clearly, we take the
model’s parameters Θ as the abscissa and the corresponding
rate-distortion cost J as the ordinate, and draw the simpliﬁed
optimization diagram as shown in Fig.2 (a).

At the ofﬂine step, it designs a model structure to determine
the whole discrete optimization space Γ (Γ ∈ Ω) ﬁrstly.
Discrete optimization space Γ consists of dense parameter
points by discretizing the continuous optimization space Ω.
The discrete optimization space is so large that it is difﬁcult to
search from such a large number of parameter choices directly.
To address this problem, the discrete space usually needs to
be further discretized (or quantized). Speciﬁcally, the whole
discrete optimization space Γ is usually divided into N non-
overlapping subspaces Γ1, · · · , ΓN

Γ = Γ1 ∪ Γ2 ∪ · · · ∪ ΓN

(19)

Each subspace Γi is represented by a discrete optimal point
Θi that searches through lots of training samples x1, · · · , xK,

Θi = arg min

Θ∈Γi

K
(cid:88)

k=1

J(xk; Θ)

(20)

dec.

We take the union of all decoder parameters as the optimal
decoder parameters Θ∗
Θ∗

dec = Θ1 dec ∪ Θ2 dec ∪ · · · ∪ ΘN dec
Then the whole discrete optimization space is expressed by a
set Ψ with N discrete starting points Θ1 enc, · · · , ΘN enc

(21)

Ψ = {Θi enc |i = 1, 2, · · · , N }

(22)

Θ∗

enc = arg min
Θenc∈Ψ

J(x; Θ)

(23)

In the speciﬁc implementation of block-based hybrid coding,
the two optimization steps are realized by ofﬂine designing
multiple coding modes and setting mode parameters, and
online searching. The discrete optimized space Γ is the ﬁnite
range of all the model’s parameters, and is divided into N
subspaces by set N modes. There are still many options for
parameters within subspace, and then it chooses the parameter
Θi as the model’s parameters in i-th subspace (e.g. for a
quarter MV x-dimension value 0.25,
its discrete subspace
contains any discrete value in the interval [0.125, 0.375], and
it ﬁnally chooses the value 0.25 to represent this subspace).
The search-based optimization searches from multiple starting
points globally so that it’s more likely to optimize the best
model parameters. However, the computational efﬁciency of
search optimization is low. In addition, limited by the discrete-
ness of the model’s parameters and the search complexity, the
search point is not dense enough to perform ﬁner optimization,
so it may not achieve better optimization results in local parts.
The other one is formulating the optimization of model
parameters as a continuous optimization problem, and solving
it with the numerical optimization method. End-to-end learned
coding is a typical example. The simpliﬁed optimization dia-
gram is shown in Fig.2 (b). At the ofﬂine step, it designs
continuous functions to modeling the optimization space Ω
and then establishes the continuous relationship between the
optimized parameters Θ and RD cost J. Through minimizing
the cost of lots of samples with the numerical optimization, a
group of average optimal model’s parameters is obtained

Θ1 = arg min

Θ∈Ω

K
(cid:88)

k=1

J(xk; Θ)

(24)

where it only optimizes only one group of average optimal
model parameters Θ1. We take the decoder parameters Θ1 dec
as the optimal decoder parameters Θ∗
dec. At the online step, for
the coded sample x, it directly applies the initial parameters
Θ1 enc or further optimizes a group of better parameters
around Θ1 enc with the numerical optimization

Θ∗

enc = arg min

Θenc∈ΩΘ1 enc

J(x; Θ)

(25)

where ΩΘ1 enc represents the convex space where Θ1 enc is.
In the speciﬁc implementation of end-to-end learned coding,
the two optimization steps are achieved by ofﬂine designing
a network model and training it with the gradient descent
algorithm, and online direct inference or further ﬁne-tuning
around the ofﬂine trained model. The continuous optimized
space Ω is the inﬁnite range of all network parameters, Θ
is the all network parameters, and Θ1 is the ofﬂine optimized
network parameters. The numerical optimization in the contin-
uous space can not only solve the optimization problem more
efﬁciently but also perform ﬁner optimization to make it more
likely to achieve the optimum in the local part. However, most

11

(a)

(b)

(c)

Fig. 2.
hybrid optimization.

Illustration of the optimization solution. (a) Search-based optimization in discrete space. (b) Numerical optimization in continuous space. (c) Proposed

of the numerical optimization are greedy algorithms, such as
the gradient descent algorithm. Those schemes rely heavily on
the starting point since they conduct optimization around it.
Only one starting point will fall into a local optimum.

Based on the two optimization solutions, we propose a
hybrid of discrete and continuous optimization to solve this
problem efﬁciently. We provide multiple discrete start points
in global continuous space and adopt numerical optimization in
the local part around each starting point. Then we use search-
based optimization among the multiple discrete local optimums
to obtain the global optimum. The hybrid optimization diagram
is shown in Fig.2 (c). At the ofﬂine step, we ﬁrst formulate
the problem as continuous optimization and adopt numerical
optimization on the continuous space Ω of the designed model.
We divide the whole continuous optimization space into N
non-overlapping continuous subspaces Ω1, · · · , ΩN

Ω = Ω1 ∪ Ω2 ∪ · · · ∪ ΩN

(26)

We optimize an optimal point Θi for each subspace Ωi through
lots of training samples

Θi = arg min

Θ∈Ωi

K
(cid:88)

k=1

J(xk; Θ)

(27)

dec

We take the union of all decoder parameters as the optimal
decoder parameters Θ∗
Θ∗

dec = Θ1 dec ∪ Θ2 dec ∪ · · · ∪ ΘN dec
At the online step, for the coded sample x, we adopt numerical
optimization to ﬁnd each group of local optimal model’s
parameters Θ∗

(28)

i around each starting point Θi
J(x; Θ)

i enc = arg min

Θ∗

Θenc∈ΩΘi enc
where ΩΘi represents the convex space where Θi is. Then all
N discrete local optimums Θ∗

N form a set Ψ

1, · · · , Θ∗
i enc |i = 1, 2, · · · , N }

Ψ = {Θ∗

(30)

(29)

and search-based optimization is used to search the global
optimal one among all local optimums
Θ∗

J(x; Θ)

(31)

enc = arg min
Θenc∈Ψ

Optimization in the continuous space can optimize the local
optimal parameters more efﬁciently. Meanwhile, the optimiza-
tion in the discrete space can search for the best one from
multiple discrete points globally, which can avoid the ﬁnal
solution falling into the local optimal as much as possible.
Thus, proposed hybrid optimization is a better way to solve
this problem.

IV. PROPOSED METHOD
Guided by the proposed hybrid-optimization theory, we
propose a hybrid-optimization video coding method, as show
in Fig.3 (a).

A. Ofﬂine Hybrid Optimization

As above analysis, ﬁrstly, we ofﬂine set some hyper-
parameters to determine an encoder-decoder model structure.
Speciﬁcally, the hyper-parameters mainly consist of two parts.
is to determine the structure of the continuous
The ﬁrst
part. We adopt
the hybrid framework of motion-handling
and picture-coding techniques, which has proven very ef-
fective. It consists of some modules, including motion esti-
mation (ME), motion compensation (MC), transform/inverse
transform, quantization/inverse quantization, in-loop ﬁlter, and
entropy coding. Then we set the hyper-parameters for each
module to determine its internal structure. We build those
modules with continuously optimized tools (e.g. deep neural
they can be optimized with numerical
networks) so that
methods. The second is to determine the structure of the
discrete part. We design various discrete modes, which can
divide the whole continuous optimization space into multiply
sub-spaces. Due to the hybrid framework of motion-handling
and residual picture-coding, the designed modes focus on the
motion and transform coding accordingly. Through different
motion models and residual coding methods, different content
in the video can be handled efﬁciently.

Next, we ofﬂine optimize the model parameters through
lots of training samples. Speciﬁcally, the optimization can be
divided into two steps. First, we optimize the parameters of
the whole continuous framework. We use numerical methods
to optimize all the modules in an end-to-end manner. Sec-
ond, based on the optimized parameters of the continuous

Θ∗𝐽ΘΘ1Θ𝑁ΘiΘ1Θ∗𝐽ΘΘ𝑁Θ𝑁∗Θ1Θ𝑖Θ1∗𝐽ΘΘ∗12

Fig. 3.

(a) Overview of our hybrid-optimization video coding framework. (b) Implementation of our hybrid-optimization video coding framework.

(a)

(b)

framework, we optimize the initial parameters of each discrete
mode. We convert the parameter format of the continuously
optimized model to the format of each discrete mode, and use
the converted parameters to initialize each mode.

Our proposed method is consistent with the proposed
hybrid-optimization theory. The continuous optimization space
Ω is the range of all module and mode parameters. The modes
divide the continuous optimization space Ω into multiply
sub-spaces Ω1, · · · , ΩN . Through ofﬂine optimization for all
modes, we obtain multiply groups of initial mode parameters
as the discrete starting points Θ1, · · · , ΘN .

B. Online Hybrid Optimization

Based on our ofﬂine optimized framework, we conduct on-
line hybrid optimization for each frame to be coded. Guided by
the proposed hybrid optimization theory, the online optimiza-
tion process is divided into two steps: continuous optimiza-
tion in local parts and discrete optimization in global space.
Firstly, in the local area of each mode, we perform continuous
numerical optimization around the initial parameters (i.e. Θi)
to obtain the locally optimal parameters (i.e. Θ∗
i ). We conduct
this optimization for all modes, including the motion-handling
and transform coding mode, and then we can obtain the locally
optimal parameters of all discrete modes (i.e. Ψ). Secondly, we
perform discrete search optimization among all locally optimal
modes to select the globally best mode (i.e. Θ∗). Note that the
online optimization is also end-to-end. As a result, through the
hybrid optimization for the current frame, different content in
the video can be coded adaptively and efﬁciently.

V.

IMPLEMENTATION

We design a deep network-based video compression frame-
work to implement our proposed hybrid-optimization method.
The implementation is shown in Fig.3 (b). All modules of our
framework are based on deep networks.

At the encoder, we use the optical ﬂow network to conduct
motion estimation between the current frame and reference

frames and take the optical ﬂow as the initial motion infor-
mation. Then we design various partition modes and multi
motion modes to describe the optical ﬂow efﬁciently so that the
expensive pixel-level ﬂow is changed into low-cost partition
and motion parameters. We employ the motion compensation
network to obtained the predicted frame from the reference
frames and motion parameters. The transform module is used
to converts the residual into coefﬁcients that are easier to
code. The transformed coefﬁcients are determined whether to
code or not by residual skip mode. After that, we reﬁne the
reconstructed frame by the learned in-loop ﬁlter and output the
ﬁnal reconstruction. The entropy encoding network generates
the probability distribution of quantized motion parameters
and transformed coefﬁcients, and then codes them into the
bitstream by an arithmetic encoder (AE). At the decoder, with
the help of an arithmetic decoder (AD), the entropy decoding
network decompresses the motion parameters and transformed
coefﬁcients from the bitstream. Next, we sequentially use
motion compensation, inverse transform, and in-loop ﬁlter to
obtain the decoded reconstruction.

In this section, we will introduce the used coding conﬁgu-

ration and our proposed coding modules in detail.

A. Coding Conﬁguration

In this paper, we adopt the random-access (RA) coding
conﬁguration, and follow the classical hierarchical B layers
coding structure. We refer these hierarchical B layers in the
GOP as layer 0, 1, 2, 3, and so on, respectively in decoding
order. The last B frame in GOP (layer 0) only can perform
uni-directional inter-prediction. However, other B frames can
perform bi-directional inter-prediction by referring to the two
reconstructed frames of the previous layer. Considering this
difference, in this paper, we only code the bi-directional B
frames in the high layers by our designed deep networks, and
the I frame and the B frames of layer 0 are coded by HEVC.
This conﬁguration can verify the effect of our proposed method
because the layer 0 frames only account for a small part of
the video, that is, one B frame in one GOP and one I frame in

TransformInverseTransformIntra-Picture PredictionIn-LoopFilterMotion EstimationEntropyCodingPredictionResidualMotion DataBitstreamOutput Video Signal DecoderDecoded Picture BufferMotion CompensationCurrentPictureMode DecisionMode DecisionNumericalOptimizationSearch-based OptimizationMEMCDecodedPictureBufferTemporal MergeModeTemporal ScaleModeMotion VectorModeMode DecisionQuadtreeSplitPartition DecisionRDORDOMode DecisionRDOGradient-based Optimization (Online)Search-based Optimization (Online)ResidualEncoderResidualDecoderIn-Loop FilterPartition&MotionParameterEncoderPartition&MotionParameterDecoderGradient-based Optimization (Offline)CurrentPictureone second. Most of the frames are the B frames in the high
layers.

B. Motion Estimation

Motion estimation in block-based hybrid video coding uses
search-based optimization to online search for the best motion
ﬁeld at the encoder side, which usually takes lots of time.
However, learning-based optical ﬂow can infer a motion ﬁeld
directly with the ofﬂine trained model. In our framework,
we adopt the optical ﬂow network PWC-Net [98] to estimate
an initial motion ﬁeld rapidly. The motion ﬁeld includes the
optical ﬂow between the current frame and reference frames
and the optical ﬂow between reference frames. These optical
ﬂows will be used for later optimization.

C. Block Partition

The correlation between the adjacent pixels in a frame is
usually strong, while the correlation between the far away
pixels may be weak. In order to reduce this redundancy, block
partition is widely used in video coding. The main purpose
of block partition is to group samples that can be processed
efﬁciently and are coded together. In our framework, we adopt
the variable block partition optimization that provides multiple
block size modes to enable high efﬁciency and adaptation
to different regions. Speciﬁcally, we use quadtree-structured
partitioning and signaling like HEVC. Only square block
partitioning is speciﬁed, and a block can be recursively split
into four quadrants. Each quadrant is assigned a ﬂag that
indicates whether to split. The partitioned block includes four
sizes: 8×8, 16×16, 32×32, 64×64, and the partitioned result
is described efﬁciently by the split ﬂag. Finally, through the
ﬂexible partition and the parameter sharing within a block, we
avoid the expensive overhead of pixel-level ﬂow and achieve
high-efﬁciency compression.

D. Motion Mode

The efﬁcient representation of motion is the key to im-
proving video coding performance. Block-based hybrid video
coding usually describes motion at a high level, which means
the motion model is designed according to the motion type,
such as the block translation model and the afﬁne model for
rotation and zooming. However, the motion ﬁeld is usually not
as ﬁne as at the pixel level. On the other hand, optical ﬂow in
the learned end-to-end video coding is a more straightforward
way to describe motion at
ignores
high-level motion characteristics. Different motion models are
beneﬁcial
in our
framework, we introduce a hybrid of different motion models
and propose multi-mode optimization to deal with different
motion efﬁciently. We design three motion modes: motion vec-
tor (MV) mode, temporal merge (TMerge) mode, and temporal
scale (TScale) mode. We signal a mode index for each block
to decide which mode is used in this block. Note that we
will convert the coded block-level motion parameters into the
pixel-level optical ﬂow ﬁeld for later motion compensation
operation.

to different motion situations. Therefore,

level, but

the pixel

it

13

Speciﬁcally, only a MV can describe the translation of
a block in block-based hybrid video coding. Similarly, we
also adopt motion vector mode in our framework to handle
the translational motion. Based on the bi-directional reference
frames at time t − k and t + k, the bi-directional optical ﬂow
of all pixels in a block at time t are set as ft

ft = {mvt→t−k, mvt→t+k}

(32)

where mvt→t−k and mvt→t+k denote two motion vectors for
bi-direction, and they are quantized with half-pixel precision
to code. In addition, like learned end-to-end video coding, we
also adopt pixel-level optical ﬂow to deal with ﬁner motion.
However, it’s not efﬁcient to transmit the optical ﬂow directly
because the motion information of bi-directional reference
frames provides lots of prior information for the current frame
prediction. To make full use of the prior motion information,
we propose temporal merge mode that can utilize the pixel-
level ﬂow without transmitting any motion information. Since
the time interval between the current frame and reference
frames is usually very short, the motion in most regions can be
considered uniform. Based on the uniform motion assumption
and the prior optical ﬂow between reference frames vt−k→t+k,
vt+k→t−k, we can obtain the bi-directional pixel-level ﬂow
ﬁeld in a block by

ft = {vt+k→t−k × 0.5, vt−k→t+k × 0.5}

(33)

Due to the diversity of motion, the motion in some regions is
not uniform, but it may obey some rules, such as accelerated
motion. Inspired by both coding frameworks, we propose a
hybrid mode of high-level motion parameters and pixel-level
ﬂow to address the problem. This hybrid mode, called temporal
scale mode, combines the high efﬁciency of the high-level
parameter-based motion model and the ﬁne precision of the
ﬂow-based motion model. In our framework, we use a group
of scaling parameters to describe the temporally characteristic
of motion and then combine the parameters with the prior ﬂow
to generate the bi-directional pixel-level ﬂow ﬁeld:

ft = {vt+k→t−k × s0, vt−k→t+k × s1}

(34)

where s0 and s1 denote two two-dimensional scaling vectors
for bi-direction, and they are quantized with one-tenth preci-
sion. Finally, the motion of a block is signaled by four scaling
values.

Our proposed multiple modes can handle most motions
effectively. According to the uniformity of motion in spatio-
temporal domain,
the motion can be classiﬁed into four
categories: spatially uniform motion, spatially non-uniform
motion,
temporally uniform motion, and temporally non-
uniform motion. Since the proposed block partition mode
can adaptively divide the pixels with the same motion into
one block and the pixels with different motion into different
blocks, it can deal with the spatially uniform and non-uniform
motion to a certain extent. The proposed temporal merge mode
is specially designed for temporally uniform motion. As for
temporally non-uniform motion, if the motion is regular and
the prior ﬂow is estimated accurately, the proposed temporal
scale mode can handle it well. Otherwise, the motion vector

14

Fig. 4. Motion compensation network.

Fig. 5. Residual coding network.

mode by transmitting block-based MV directly will provide a
good approximation for the motion.

E. Motion Compensation

Motion compensation is to retrieve the predicted frame
from the reference frames through the motion ﬁeld. Previous
video coding methods usually use the ﬁxed fractional-pixel
interpolation ﬁlter in MC, such as DCTIF in classical HEVC
and warping operation in learned DVC. In this paper, we
adopt an adaptive kernel-based MC network. The kernel-based
MC that synthesis a pixel by convolving input patches with a
learned kernel [99] can make full use of spatial information and
conduct adaptive prediction for each pixel. We use spatially-
displaced convolution (SDC) [100] which applies the learned
kernel at the location of the motion ﬁeld to combine the given
motion ﬁeld and learned kernel effectively. Besides, we utilize
the contextual information in the feature domain to improve
MC prediction quality further [101], [102]. Our MC network is
shown in Fig.4. We learn a kernel for each pixel and synthesize
a pixel by applying the kernel on the reference frame at the
displaced location of the given motion ﬁeld. We extract the
context from the pre-trained ResNet18 conv1 layer [103] and
also apply the learned SDC kernel to warp the feature. A
reﬁne network is adopted to fuse them to generate the ﬁnal
MC prediction frame.

F. Transform and Quantization
Transform aims to convert

into coefﬁcients
that are easier to code, and quantization is used to reduce
the precision of coefﬁcients to decrease the amount of data.

the residual

Correspondingly,
inverse quantization restores the original
coefﬁcient range without regaining the precision, and inverse
transform is applied to reconstruct the residual samples from
the restored coefﬁcients. In our framework, we adopt wavelet
transform/inverse-transform [104] at a frame level and uniform
scalar quantization/inverse-quantization. Our residual coding
transform results in
network is shown in Fig.5. Wavelet
a plurality of subbands of different resolutions. Similar to
JPEG2000 [104], we perform 4 times of pyramid decompo-
sition in wavelet transform, i.e. the residual frame is forward
transformed into four subbands {LL1, HL1, LH1, HH1} at
ﬁrst and then we conduct the k-th forward transform from
LLk−1 into {LLk, HLk, LHk, HHk} recursively. Here, LL,
HL, LH, HH are all two dimensional; L stands for low-
frequency and H stands for high-frequency. Finally, we got 13
subbands with coefﬁcients. At the encoder side, all the forward
transformed coefﬁcients y are scalar quantized to obtain the
discrete indexes q to be coded:

q = [y/Qstep]

(35)

where [] stands for rounding operation and the unique param-
eter Qstep is used to control the rate and distortion. At the
decoder side, the reconstructed coefﬁcients ˆy are obtained by
uniform scalar inverse-quantization:

ˆy = q × Qstep

(36)

The inverse transform is strictly the reverse of the forward
transform, which is realized by reversing the order of the
operations and swapping the additions and subtractions.

Our framework adopts wavelet transform mainly because it
has a clearer correspondence between transformed coefﬁcients
and pixels. The bits of each partitioned block can be estimated
approximatively from coefﬁcients. Thus,
it’s very suitable
for block-based rate-distortion optimization among multiple
modes in our framework. Moreover, we can achieve virtually
any reachable bitrate and quality by simply adjusting the quan-
tization step of wavelet transform coefﬁcients. As shown in
Fig.6, the quality of the reconstructed frames can be controlled
precisely to make it almost the same as HM. Therefore, we not
only can reach hierarchical quality easily but also can solve
the problem of error propagation more effectively compared to
other learned video coding methods [93]. In addition to lossy
compression, since the wavelet transform doesn’t lose any
information, the coding methods based on wavelet transform
can also support lossless compression by transforming in the
integer domain [104].

G. Residual Mode

The coding performance of different videos may beneﬁt
from different residual coding methods. For example, when the
prediction is very close to the original signal, coding the sparse
residual will decrease the coding efﬁciency. To address this
problem, we propose residual skip (ResiSkip) mode that can
skip the coding of transformed coefﬁcients in some regions.
We divide the residual frame into residual-skip units regularly,
and we signal a ﬂag for each unit to decide whether skip the
residual coefﬁcients of this unit. The residual-skip unit is set

KernelEstimationKernelWarpRefineContextExtractionKernelWarpOpticalFlowRefFramesSDC KernelContextWarped FrameWarped FeaturePredFrameForwardTransformInverse TransformAEADQIn-Loop FilterEntropyParameters()I-QCoefficientsDecoded CoefficientsContextModelOrigFrameRecoFrameResiFrameResi’FramePredFrameto 128 × 128 to avoid too much overhead. Speciﬁcally, we
transform the residual frame into coefﬁcients ﬁrstly. If the
unit chooses residual skip mode, we set the corresponding
coefﬁcients of this unit to 0 and skip the coding of those
coefﬁcients. Otherwise, we code the coefﬁcients. The residual
skip mode can not only improve the coding efﬁciency in some
cases, but also avoid the computation of coding coefﬁcients.

H. In-Loop Filter

After MC and residual compression, we can reconstruct the
current frame from the prediction and residual frame. However,
the reconstruction contains compression error, especially at
low bit rates. It not only reduces the reconstruction quality,
but also affects the inter-prediction of the next coded frame
due to the reference frame mechanism. Therefore, we propose
an in-loop ﬁlter network to compensate for the compression
error of the reconstructed frame. It is operated within the
inter-picture prediction loop. We concatenate the reconstructed
frame, predicted frame, and residual frame as the input to make
full use of the spatio-temporal correlation, then feed them into
a CNN with eight residual blocks [103] to obtain the reﬁned
reconstructed frame.

I. Entropy Coding

In this paper, we adopt the context-based entropy coding
method similar to [75] and the factorized entropy coding
method. The factorized entropy coding uses the ﬁxed sta-
tistical probability model. The context-based entropy coding
predicts a probability distribution for each coefﬁcient, where
the probability distribution is constructed by a set of entropy
parameters ψ of CNN output, as shown in Fig.5. Arithmetic
coding is used to code each coefﬁcient according to its
probability distribution. The statistical probability model is
used to code the parameters with weak correlation to reduce
computational complexity, while the context model is used
to code the parameters with strong correlation to improve
coding performance. We use the statistical probability model
to code split ﬂag, motion mode index, residual skip mode
ﬂag, and the scaling parameters of temporal scale mode. The
MVs of motion vector mode are coded directly by the CNN-
based context model. As for the transformed coefﬁcients, we
adopt the CNN-based context model to exploit the correla-
tion among prediction signal, wavelet coefﬁcients within and
across subbands. When coding one wavelet coefﬁcient, CNN
generates its conditional probability distribution by inputting
three context information: the already coded coefﬁcients in the
current subband, other already coded subbands, and the inter-
prediction signal. Note that we do not perform real arithmetic
encoding during optimization but instead estimate the bits
from the entropy of the corresponding motion and residual
coefﬁcients.

A. Ofﬂine Hybrid Optimization Procedure

VI. PROCEDURE

15

so it models the optimization problem with a continuous func-
tion. Then we end-to-end train the framework with gradient-
based optimization through lots of samples. The details of the
network training will be provided in Section VII-A1.

Meanwhile, our framework contains multiple modes. The
variable block partition, multiple motion modes, and multiple
residual modes can be combined so that it will produce a large
number of discrete starting points for search optimization. We
use the trained model parameters to initialize those modes.
Speciﬁcally, the partition mode is initialized to four sizes,
and the residual coding mode is initialized to whether regular
transform coding or the zero residual. As for motion modes, all
blocks follow the same initialization rules. The coefﬁcients of
temporal merge mode are initialized to the ﬁxed value 0.5, and
the scale coefﬁcients of temporal scale mode are initialized by

s0 =

vt→t−k
vt+k→t−k

, s1 =

vt→t+k
vt−k→t+k

(37)

and the MVs of motion vector mode are initialized by

mvt→t−k = vt→t−k, mvt→t+k = vt→t+k

(38)

where v means to calculate the average of the optical ﬂow in
a block. As a result, we ﬁnish the ofﬂine optimization of the
coding framework through lots of samples.

B. Online Hybrid Optimization Procedure

Based on our ofﬂine optimized framework, we conduct
online hybrid optimization for each frame at the encoder to
ﬁnd the better coding parameters. The optimization framework
is shown in Fig.3 (b), and the optimization process is shown in
Algorithm 1. In this paper, we set the number of partition depth
D = 4, minimum partition block size z0 = 8, and the number
of motion mode L = 3, which have already been introduced in
the above sections. As the proposed online hybrid-optimization
method in Section IV-B, we perform continuous optimization
in local parts and discrete optimization in global space in
turn. Firstly,
in the local area of each speciﬁc mode, we
perform continuous numerical optimization around the initial
mode parameters. Speciﬁcally, we traverse the block partition
ﬁrst, and then we optimize the parameters of each motion
mode separately under a speciﬁc partition. The coefﬁcients
of temporal merge mode are not coded, i.e. ﬁxed value 0.5,
so they don’t need to be optimized. For other motion modes,
we get the initial coefﬁcients for each block ﬁrstly, where
the initialization rules have been introduced in the ofﬂine
optimization procedure of Section VI-A. Since not all frames
have signiﬁcant coding gains through online gradient-based
optimization, such as the frame with little or no motion, we
optimize some frames selectively in a video sequence. If the
current frame needs to be optimized, we online end-to-end
optimize all coefﬁcients of the current mode at the frame level
with gradient-based optimization by minimizing the RD cost
J:

J = D + λR = d(ˆx, x) + λ(Rm + Rr)

(39)

Based on those designed modules, we build our complete
coding framework. Our framework is on top of deep networks,

where d(ˆx, x) denotes the distortion between original frame
x and reconstructed frame ˆx, and we use sum square error

16

Algorithm 1 Online Hybrid-Optimization Process
Require: M E(): motion estimation, which inputs original
frame (orig) and reference frames (ref ) and outputs
optical ﬂow (f low); M C(): motion compensation, which
inputs motion coefﬁcients (c) and reference frames (ref )
and outputs predicted frame (pred); CC(): motion co-
efﬁcients coding, which inputs motion coefﬁcients and
outputs coefﬁcient bits map (cbm); RC(): residual coding,
which inputs residual frame and outputs reconstructed
frame (reco) and residual bits map (rbm); Note that those
modules are all operated at frame level;

Partition frame into blocks with size zd = z0 << d;
for each motion mode l ∈ [0, L) do

for each block n ∈ [0, Nd) in frame do

d,l with f low for each block;

end for
predd,l = M C(cd,l, ref ), cbmd,l = CC(cd,l);
recod,l, rbmd,l = RC(orig − predd,l);
Calculate RD cost Jd,l for each block;
if cd,l need online numerical optimization then
Optimize cd,l online by minimizing Jd,l;

Initialize cn

1: f low = M E(orig, ref );
2: for each depth d ∈ [0, D) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

end if

else

end for
for each block n ∈ [0, Nd) do
Search motion mode J n

d = min{J n

d,l, l ∈ [0, L)};

end for
Update Jd by the selected mode coefﬁcients cd;
if depth d == 0 then

Take Jd as best RD cost until depth d, i.e. (cid:101)Jd = Jd;

for each block n ∈ [0, Nd) do

d };

end if

i=1 (cid:101)J ni

d = min{(cid:80)4

d−1, J n

Search partition (cid:101)J n

end for
Update (cid:101)Jd by the selected partition and mode;

23:
24:
25:
26:
27: end for
28: Obtain the best RD cost (cid:101)J = (cid:101)Jd;
29: Calculate RD cost of skipping residual ˆJ for each unit;
30: for each unit u ∈ [0, U ) do
31:
32: end for
33: Update J by the selected residual mode;
34: Code ﬁnal partition, mode, motion&residual coefﬁcients;

Search residual mode J u = min{ ˆJ u, (cid:101)J u};

(SSE) in our implementation. Rm and Rr represent the number
of bits for motion information and residual in this frame,
respectively. λ of each frame in our framework is set to be
the same as HM. The iteration number is set to 10.

After obtaining the local optimal coefﬁcients of all modes
in the current partition depth through continuous numerical
optimization, we perform discrete search optimization among
them, i.e. selecting the best mode for each block according
to the RD cost. The block’s RD cost is calculated with the
distortion between the original block and the reconstructed

block and the bits of motion and residual in this block. Note
that the networks in our framework are all operated at the
frame level. Thus, the distortion and bits of each block are
estimated from the frame result. After all search optimization
at the block level, the reconstructed frame needs to be updated
at
through the networks instead of block
splicing. Then we have ﬁnished the motion mode optimization
in the current partition depth.

the frame level

Subsequently, we perform discrete search optimization for
the partition mode of each region by comparing the RD cost of
the block in the current depth with the best result of this region
before the current depth. We repeat that discrete optimization
until ﬁnishing all partition modes. In the end, we optimize
the residual skip mode for each region by discrete search. As
a result, based on the ofﬂine model, through the continuous
numerical optimization around the starting points locally and
the discrete search optimization among all local optimums
globally, we have optimized the better parameters for the coded
sample in a hybrid manner.

VII. EXPERIMENTS

A. Experimental Settings

1) Network Training: We use the deep learning software
PyTorch to train our framework on 4 NVIDIA GTX 1080Ti
GPUs. When training our networks, we remove all modes
and code the pixel-level optical ﬂow between the current
frame and reference frames directly so that the whole network
can be trained in an end-to-end manner. The goal of video
compression is to optimize rate-distortion, so we take Eq.
(39) as the loss function to minimize the RD cost. We train
4 basic models for 4 common test QPs in HM. We use λ
and transform quantization step Qstep to control the bitrate of
different models. In our framework, we set Qstep to 12, 24,
45, 95 to correspond to QP 22, 27, 32, 37 in HM and set
λ to be the same as HM. Since the quantization operation is
not differential, we adopt the method of forward quantization
and backward pass [105]. Each group of training data consists
of three frames, including the original frame, the reference
frame before and after the original frame. The reference frames
should be reconstructed from our compression network, but
our network has not yet completed training. To address this
problem, we compress the reference frames by HM under
four QPs to approximate them. We use the SJTU-4k dataset
[106] and down-sampling versions of these video sequences.
We extract only the luma component for training. We directly
use the luma models to code the chroma in the test. The frame
is cropped into many non-overlapping 128 × 128 blocks, and
the time interval between the original frame and reference
frames is set to 1 or 2 randomly to ﬁt the hierarchical layers
in the test. Finally, we obtain about 500,000 training samples
for each model. We optimize with Adam [63] using 0.9 and
0.999 as hyper-parameters with no weight decay. The learning
rate starts at 0.0001 and then is reduced to 0.00001 when the
loss becomes stable. The batch size is 16.

2) Encoding Conﬁgurations and Evaluation: We evaluate
our coding framework on the HEVC common test sequences,
including 16 videos of different resolutions known as Classes

TABLE III.

BD-RATE RESULTS OF OUR PROPOSAL COMPARED TO HM

17

Class

Sequence

Class B

Class C

Class D

Class E

Class Summary

Overall

Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
FourPeople
Johnny
KristenAndSara
Class B
Class C
Class D
Class E
Classes B–E

RA
Y (%)
−2.4
−4.4
−4.1
2.5
43.4
2.6
4.9
0.4
16.6
−6.4
−14.5
−0.8
3.9
−4.6
2.9
1.2
7.0
6.1
−4.4
−0.2
2.6

TABLE IV.

BD-RATE RESULTS OF OUR PROPOSAL COMPARED TO HM

ON YUV420

Class

Sequence

Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
FourPeople
Johnny
KristenAndSara
Class B
Class C
Class D
Class E
Classes B–E

Class B

Class C

Class D

Class E

Summary

Overall

Y (%)
0.9
−3.5
−2.9
4.3
44.9
4.2
6.2
0.6
17.3
−4.4
−13.6
−0.5
6.8
−4.4
2.7
1.3
8.7
7.1
−2.9
−0.1
3.7

RA
U (%)
0.4
1.3
−2.5
12.0
10.5
12.9
13.5
3.4
28.1
3.2
−2.9
3.6
21.2
−2.8
0.2
−0.6
4.3
14.5
6.3
−1.1
6.3

V (%)
1.0
2.0
−0.7
5.1
11.2
9.0
11.1
3.7
24.6
−4.0
−6.2
5.5
14.2
−2.9
2.2
−0.5
3.7
12.1
2.4
−0.4
4.7

Fig. 7.
Rate-distortion comparison in terms of PSNR on HEVC dataset
(Classes B–D) using x264, x265, DVC [81], HLVC [87], FVC [91], HM, and
our proposed method. The intra period(IP) of x264, x265, DVC, HLVC, and
FVC reported in [81], [87], [91] is 10. For a fair comparison, the IP of HM
and our proposed method is set to 8 and 16, respectively.

Fig. 6. Quality comparison of each reconstructed frame in the Group Of
Picture (GOP) between HM and our proposal. The PSNR is the average result
of all GOPs of all sequences.

B, C, D, E [107]. Considering that the luma component (Y)
is more important than the chroma components (U and V),
we provide coding results of test sequences with two formats:
YUV400 and YUV420. The YUV400 is the default format in
the following experiments. We compress the ﬁrst 1 second
of each sequence when comparing our proposal to HEVC.
We compress the ﬁrst 100 frames of each sequence when
comparing our proposal to end-to-end learned methods. PSNR
is used to measure the quality of the reconstructed frames, and
BD-rate [108] is calculated to quantify the coding performance
between different schemes. We compare our coding framework
with the vanilla HEVC reference software HM (version 16.10).
We follow the HEVC common test condition and use the de-
fault Random-Access (RA) encoding conﬁgurations provided
in HM16.10 without any change, if not otherwise speciﬁed.
HM is tested under 4 QPs: 22, 27, 32, and 37 [107]. Our
framework is tested with 4 trained Qstep models: 12, 24, 45,
and 95. In a basic model, we can adjust Qstep slightly to easily
achieve the hierarchical quality of different layers. The intra
period (IP) of HM and our framework are both 1 second. The
GOP of HM and our framework are both 8. HM coding is
running in CPU and our framework coding is running in CPU
and a single GPU. The CPU is Intel(R) Xeon(R) CPU E5-2690
v4 @2.60GHz and GPU is NVIDIA TITAN Xp with 12 GB
RAM.

B. Overall Performance

To verify the efﬁciency of our proposed hybrid optimization,
we compare our framework with both block-based hybrid
video coding and end-to-end learned video coding. The basic
ideas of all designed modes in our framework come from
HEVC, i.e. quadtree partition, temporal merge and scale mode,
motion vector mode, and residual skip mode in our frame-
work corresponding to quadtree partition, temporal merge and
AMVP candidate, motion vector mode, and skip mode in
HEVC. Therefore, we choose the HEVC of block-based hybrid
video coding for comparison. The BD-rate results of our
framework compared to HEVC reference software HM16.10
in YUV400 format are shown in Table III. It can be observed
that our proposed hybrid-optimization video coding method
achieves comparable performance with HM in terms of PSNR,

31333537394143012345678PSNR (dB)POCQP22_HMQP22_OursQP27_HMQP27_OursQP32_HMQP32_OursQP37_HMQP37_Ours2930313233343536373800.10.20.30.40.50.60.7PSNR (dB)Rate (bpp)PSNR on HEVC Dataset (Class B-D)Ours (IP=16)Ours (IP=8)HM (IP=16)HM (IP=8)FVC (IP=10)HLVC (IP=10)x265 (IP=10)DVC (IP=10)x264 (IP=10)18

leading to on average only 2.6% BD-rate increase on Y
component. Our method achieves better performance on half
of the classes and beats HM on 7 sequences among all 16 test
sequences. In addition, we provide the BD-rate comparison
results of YUV420 format in Table IV. Our proposal achieves
on average 3.7%, 6.3%, and 4.7% BD-rate increase for Y, U,
and V, respectively. The performance on the two formats is
consistent. Our method doesn’t completely outperform HEVC
mainly because our current framework only uses a very little
part of modes in HEVC. In addition to those used modes,
HEVC contains many other advanced modes, such as intra
mode, non-square PU mode, spatial merge and AMVP candi-
date, uni/bi-prediction mode, various transform modes, RDOQ,
etc. Besides, HEVC also uses additional information, such as
more reference frames. Therefore, if we add more modes or
information into our framework, it will have the potential to
outperform HEVC.

The learned end-to-end video coding methods usually code
the video in RGB format. To compare with them, our frame-
work codes the video of YUV420 format ﬁrstly and then
converts the reconstruction and the original video into RGB
format
to calculate RGB PSNR. Fig.7 shows the average
performance of some methods in RGB space on the HEVC
dataset (Classes B, C, D), including the typical learned method
DVC [81], the latest bi-directional learned method HLVC [87],
the state-of-the-art learned method FVC [91], x264, x265,
HM under RA conﬁguration and our method. DVC, HLVC,
and FVC only provide the results with the intra period (IP)
10. Since the IP in HM needs to be set to a multiple of
GOP size (GOP size is 8 in HM16.10) and our framework
adopts a similar hierarchical structure with HM, we provide
the performance of HM and our method with IP 8 and IP 16.
In theory, the performance of IP 10 should be between IP 8 and
IP 16. It can be observed that the coding performance of our
hybrid-optimization coding method is signiﬁcantly better than
the end-to-end learned methods, leading to more than 45% bits
saving compared to the HLVC and more than 30% bits saving
compared to the state-of-the-art FVC.

C. Veriﬁcation of Hybrid Optimization

In order to further verify the effectiveness of each opti-
mization in our proposed hybrid-optimization coding method,
we perform ablation studies on Classes B, C, D. First, we
take the popular end-to-end learned method as the baseline,
i.e. ofﬂine end-to-end gradient-based optimization and online
direct inference, and provide two schemes. The one is called
E2E TMerge, which obtains the predicted frame by interpo-
lating from reference frames without transmitting any motion
parameters and then codes the residual frame. The other one
is called E2E Flow, which obtains the predicted frame by
directly transmitting pixel-level optical ﬂow and then codes
the residual frame. For a fair comparison, we code the pixel-
level ﬂow with the same method as the MV in MV mode,
i.e. coding the vector directly without transform. The BD-
rate results are shown in Table V. It can be observed that
the E2E TMerge performs better than the E2E Flow, leading
to on average 20.3% BD-rate increase compared to HM. It’s

mainly because it’s not efﬁcient to code pixel-level motion
directly in hierarchical bi-directional coding structure. Next,
based on the E2E TMerge, we add discrete multi-mode search-
based optimization gradually and conduct a comprehensive set
of experiments to verify the effectiveness of each proposed
mode. As shown in Table V, when gradually adding TScale
mode, MV mode, variable block partition mode, and ResiSkip
mode, the coding performance improves signiﬁcantly and the
BD-rate increase decreases to 13.3%, 8.0%, 4.4%, and 4.2%,
respectively. The results show that our method outperforms
the pure end-to-end learned coding method and all proposed
mode are effective. More importantly, it further proves that it is
efﬁcient to mix discrete multi-mode search-based optimization
into continuous end-to-end gradient-based optimization. In
addition to those optimization methods, our framework also
contains online continuous gradient-based optimization in local
parts, and we conduct experiments to verify its effectiveness.
Since some frames in hierarchical layer structure may have
very small and simple motion, such as the high layers with
short time intervals, it is not necessary to online optimize all
frames. We only perform online gradient-based optimization
on the frames of layer 1. The BD-rate results are shown in
Table VI, where we only calculate the bits and PSNR of the
layer 1 frames. We can observe that the online continuous
gradient-based optimization improves the coding efﬁciency
signiﬁcantly, leading to on average 4.1% BD-rate reduction.
Especially,
it reaches very high coding gain on some se-
quences, such as 8.8% in Kimono and 7.7% in BQTerrace.
Through these experiments, we fully veriﬁed the effectiveness
of the proposed hybrid optimization.

D. Detailed Analysis

1) Performance Analysis:

In particular, our proposal
leading
achieves BD-rate reduction in Classes D and E,
to on average 4.4%, 0.2%, respectively. Class D, a class
with low-resolution sequences, has a small moving range
of pixels correspondingly, and Class E almost has no mo-
tion. Those sequences can be coded efﬁciently because the
networks can deal with small motion easily. Especially for
the sequence BQSquare, the bits saving is up to 14.5%.
On the other hand, our proposal has a slight BD-rate in-
crease on average in Classes B and C. It’s mainly due to
BQTerrace and RaceHorsesC. Except for the two se-
quences, our proposal achieves even better performance on
average than HM. BQTerrace features lots of noise, aliasing,
and textures, including dynamic textures on the water surfaces
and the repeated texture on the wall surface and railings.
RaceHorsesC contains the entangled occlusion on the fast
moving horses and the texture on the grass. Texture, complex
object motion, large motion, and entangled occlusion have
always been the huge challenge for optical ﬂow estimation, and
network-based optical ﬂow estimation methods [109], [110]
also have not well solved those problems so far. Our method
uses optical ﬂow as the initial motion information to conduct
online gradient-based optimization. When the initial optical
ﬂow is inaccurate, it is difﬁcult for our online gradient-based
optimization to obtain precise motion information within few

TABLE V.

ABLATION STUDY: BD-RATE RESULTS OF OUR PROPOSED MODES. ANCHOR IS HM. (1) E2E TMERGE: OUR END-TO-END SCHEME USING

TEMPORAL MERGE MODE. (2) E2E FLOW: OUR END-TO-END SCHEME USING PIXEL-LEVEL FLOW. (3) + TSCALE: ADD TEMPORAL SCALE MODE WITH
FIXED BLOCK PARTITION (32 × 32) TO (1). (4) + MV: ADD MOTION VECTOR MODE WITH FIXED BLOCK PARTITION (32 × 32) TO (3). (5) + VBLOCK: ADD
VARIABLE BLOCK PARTITION TO (4). (6) + RESISKIP: ADD RESIDUAL SKIP MODE TO (5).

19

E2E TMerge
Y (%)

E2E Flow
Y (%)

+ TScale
Y (%)

Class B

Class C

Class D

Average

29.4

21.6

7.6

20.3

130.1

66.5

33.5

80.8

19.9

15.6

2.8

13.3

+ MV
Y (%)

13.3

10.0

−0.4

8.0

+ VBlock
Y (%)

+ ResiSkip
Y (%)

8.2

7.0

−2.9

4.4

8.3

6.8

−3.5

4.2

TABLE VI.

BD-RATE RESULTS OF OUR PROPOSAL WITH ONLINE

GRADIENT-BASED OPTIMIZATION COMPARED TO OUR PROPOSAL WITHOUT
ONLINE GRADIENT-BASED OPTIMIZATION ON THE LAYER 1 FRAMES

Sequence
Kimono
ParkScene
Cactus
BasketballDrive
BQTerrace
BasketballDrill
BQMall
PartyScene
RaceHorsesC
BasketballPass
BQSquare
BlowingBubbles
RaceHorses
Classes B–D

Y (%)
−8.8
−0.8
−1.1
−6.5
−7.7
−0.5
−6.9
−2.5
−3.1
−4.1
−2.8
−3.6
−5.4
−4.1

Class B

Class C

Class D

Overall

optimization times. Consequently, our proposal leads to worse
performance on BQTerrace and RaceHorsesC.

2) Mode Selection Analysis: We analyze the results of
mode selection to gain some insights. Particularly, we perform
statistics of the hitting ratio, deﬁned as the ratio of blocks
choosing a mode over all the partitioned blocks. Here the
ratio is calculated by area rather than by count because blocks
have different sizes in our framework. For the block partition
mode, the hitting ratio statistics are shown in Fig. 8. First,
the larger the block, the higher the hitting ratio. Second, as
QP increases, the hitting ratio of the largest block 64 × 64
i.e.
increases, and the hitting ratio of other small blocks,
8 × 8, 16 × 16, and 32 × 32, decreases. Third, as resolution
increases, the hitting ratio of the largest block increases, and
the hitting ratio of other small blocks decreases. One of the
reasons for these phenomenons is that larger block partition
can reduce overhead, especially at lower bit rates. In addition,
since the motion of pixels has a strong spatial correlation,
especially in high resolution, a large block partition can utilize
the correlation more efﬁciently.

For the motion and residual modes, the hitting ratio statistics
are shown in Fig. 9. First, the motion modes in descending
order of the hitting ratio are TMerge, MV, TScale. It’s mainly
because a small part of regions can fully meet the requirements
of TScale, and motion in most regions can be considered
uniform in such a short time interval and then choose TMerge.
Second, as QP increases, the hitting ratio of TMerge and
ResiSkip increases, and the hitting ratio of MV and TScale
decreases. The major reason is that TMerge and ResiSkip can
save the bits of motion parameter and residual signiﬁcantly,

the motion
which is important for lower bit rates. Third,
and residual modes are less related to resolution, because
the motion and residual mainly depend on the video content.
In summary, the large partitioned block, TMerge mode, and
ResiSkip mode can reduce overhead, which is beneﬁcial for
lower bit rates. The small partitioned block, TScale mode, and
MV mode can provide a ﬁne description for motion, which is
efﬁcient for higher bit rates. Therefore, there is no remarkable
performance difference between higher and lower bit rates, as
shown in Fig.7.

Fig. 10 presents some visual results of mode selection. We
can observe that large blocks, TMerge mode, ResiSkip mode
are selected for the background and the region with simple
motion, like the ﬂoor in Fig. 10 (a) and (b) and the grassland,
horse bodies in Fig. 10 (c) and (d), whilst small blocks are
selected for the region with complex motion and the edge,
like the moving basketball players in Fig. 10 (a), the edge of
riders, horse heads and legs in Fig. 10 (c). TScale mode is
selected for the region with regular temporally non-uniform
motion, like the basketball with accelerated rotation, the legs
with the accelerated movement of the right-side player in Fig.
10 (a), the horse head from rising to suddenly bowing in Fig.
10 (c). MV mode is selected for the translational region and
the region with the inaccurate prior ﬂow of reference frames,
especially in the picture boundary and the occlusion, like the
player in Fig. 10 (a), the rider legs in Fig. 10 (c). Those visual
results are consistent with our design.

Moreover, we also perform statistics of the inter-prediction
and residual bits in the bitstream between HM and our pro-
posal. The ratio results are shown in Fig. 11. First, the ratio of
residual bits increases in both our proposal and HM as bit rate
increases. Second, the ratio of residual bits in our proposal is
higher than HM. The main reason is that our TMerge mode
can provide an accurate inter-prediction without any bits, and
most regions choose TMerge mode as shown in Fig. 9. Thus,
the ratio of inter-prediction bits in our proposal becomes lower,
and the ratio of residual bits will be higher correspondingly.

E. Computational Complexity

We record the encoding and decoding time of our proposed
framework and the vanilla HM. We take a Class D sequence
(BQSquare 416 × 240) as an example and the computational
time results are shown in Table VII. Compared to HM, the
encoder time of our framework increases slightly while the
decoder time increases signiﬁcantly. In order to know where
the computational complexity comes from, we provide the

20

Fig. 8. Partition selection ratio results.

Fig. 9. Mode selection ratio results.

TABLE VII.

AVERAGE RUNNING TIME PER FRAME OF OUR PROPOSAL AND HM FOR A CLASS D SEQUENCE (BQSQUARE 416 × 240). QP IS 27. (1)

HM. (2) OUR PROPOSAL, INCLUDING THE DETAILED TIME OF EACH MODULE.

Time (s)

HM

Enc.
Dec.

2.29
0.004

Optical Flow
0.33
0.168

MC
0.056
0.056

Transform&Quantization
0.014
0.008

Proposal
Context Model
0.835
86.125

AE/AD
0.809
0.799

In-Loop Filter Mode Decision

0.006
0.006

6.143
−

Total
8.193
87.162

detailed results of each module in our framework to gain
some insights. It can be observed from Table VII that the
optical ﬂow, MC, transform and quantization, and in-loop ﬁlter
module are fast. The computational time mainly comes from
the mode decision optimization at the encoder and the context
model at the decoder. At the encoder side, we compare multiple
modes and update the compared results many times through the
networks, which takes lots of computational time. As for the
context model, we can estimate the probability distribution for
all coefﬁcients in parallel at the encoder side. However, at the
decoder side, we must deal with each coefﬁcient sequentially
because the probability distribution of the current coefﬁcient
needs to be based on the previous coefﬁcients. It is important
to note that if designing a customized algorithm for context
model in deep learning software or using dedicated hardware
for CNN inference, the time can be further reduced. Future
work is required to make the proposed method suitable for
real-time applications.

VIII. DISCUSSION
In addition to rate and distortion, video coding also needs
to consider coding complexity in practical applications. Ac-
cordingly,
the potential of a video coding framework is
mainly judged by two aspects: rate-distortion performance and
computational complexity. Next, we will discuss our hybrid
optimization from the two aspects to show its advantages and
potential.

In the past, researchers study advanced coding methods on
block-based hybrid video coding and end-to-end learned video
coding independently. However, due to the gap between the
two frameworks, the advanced schemes of one framework
often can’t be applied to the other framework. As a result,
those advanced technologies from the two frameworks can’t
improve coding performance together, which severely limits
the development of video coding. Our hybrid-optimization
coding framework as the bridge between the two frameworks
can solve this problem well. Since the coding technology is
essentially based on optimization methods, when we carry

02040608010022273237Ratio (%)QPClass B22273237QPClass C22273237QPClass D64×6432×3216×168×802040608010022273237Ratio (%)QPClass B22273237QPClass C22273237QPClass DTMergeMVTScaleResiSkip21

(a)

(c)

(b)

(d)

Fig. 10. Left: visual results of block partition and motion mode selection. Green, blue, red indicates blocks that choose Temporal Merge Mode, Temporal Scale
Mode, Motion Vector Mode, respectively. Right: visual results of residual mode selection. Purple indicates blocks that choose Residual Skip Mode. Top: 7-th
frame of BasketballPass at QP 22. Bottom: 21-st frame of RaceHorses at QP 32.

comparison optimization makes our framework easy to add the
more advanced modes from block-based hybrid video coding
to improve coding performance, such as intra mode, multi-
type tree partition, afﬁne mode, merge mode, uni/bi-prediction
mode, RDOQ, and so on. Our hybrid optimization framework
can attract and combine most of the new technologies from
the two frameworks. Therefore, our hybrid optimization has
signiﬁcant potential to improve RD performance.

Generally, the search-based optimization in the block-based
hybrid video coding is performed on the CPU, while the
numerical optimization in end-to-end learned video coding is
carried out on the GPU. In practical applications, the devices
that perform video coding by software usually have both CPU
and GPU, such as the widely used modern personal computers
(PC). However, the two frameworks usually only use one of the
CPU and GPU, wasting the computing resources of the other
one. On the other hand, the hybrid of the two optimizations
will lead to the hybrid in the computing architecture. Our hy-
brid optimization can adopt distributed computing to perform
different optimization on different computing devices so that
both CPU and GPU can be fully utilized at the same time.
Therefore, hybrid optimization has great potential in future
practical applications.

Fig. 11. Comparison of the proportion of each component in the bitstream
between HM and our proposal.

the hybrid in optimization,

those technologies can be
out
combined accordingly. Speciﬁcally, better models in end-to-
end learned video coding, such as optical ﬂow estimation,
bi-directional prediction, entropy coding, and in-loop ﬁlter,
can be easily plugged into our hybrid-optimization framework
to improve coding performance. In addition, the multi-mode

0%20%40%60%80%100%ResidualNon-ResidualHM      OursQP22QP27QP32QP37HM      OursHM      OursHM      Ours22

IX. CONCLUSION

In this paper, we no longer look at video coding from the
technical point of view, but rethink video coding from its
nature, i.e. optimization problem. Firstly, we have analyzed
and reviewed existing video coding works from the perspective
of optimization, and have concluded that block-based hybrid
coding and end-to-end learned coding represent discrete and
continuous optimization solutions in essence, respectively.
Secondly, based on the theoretical analysis about existing
optimization, we have proposed a hybrid of discrete and
continuous optimization video coding theory, which is more
efﬁcient, more reﬁned, and more likely to achieve the globally
optimal solution in theory. Thirdly, guided by the theory, we
have proposed a hybrid-optimization video coding framework
based on deep networks entirely. We have performed extensive
experiments to verify the efﬁciency of the proposed method.
Results show that the proposed method outperforms the pure
deep learned video coding methods and achieves comparable
performance to HEVC reference software HM16.10 in PSNR.
More importantly, our hybrid optimization can serve as the
bridge of both optimization to promote the improvement of
coding performance together, so it has great potential.

There are several open issues for further study, some of
which have been mentioned before. Firstly, we can add ad-
vanced schemes of the two frameworks into our framework to
further realize the potential of hybrid optimization. Secondly,
we need to ﬁnd out better network structures for both high
compression efﬁciency and low complexity. Thirdly, we can
study the uni-directional inter-prediction for low-delay conﬁg-
uration.

REFERENCES

[1] T. Sikora, “Trends and perspectives in image and video coding,”

Proceedings of the IEEE, vol. 93, no. 1, pp. 6–17, 2005.

[2] B. Bross, J. Chen, J.-R. Ohm, G. J. Sullivan, and Y.-K. Wang,
“Developments in international video coding standardization after
AVC, with an overview of versatile video coding (VVC),” Proceedings
of the IEEE, vol. 109, no. 9, pp. 1463–1493, 2021.

[3] K. Choi, J. Chen, D. Rusanovskyy, K.-P. Choi, and E. S. Jang, “An
overview of the MPEG-5 essential video coding standard [standards
in a nutshell],” IEEE Signal Processing Magazine, vol. 37, no. 3, pp.
160–167, 2020.

[4] G. Meardi, S. Ferrara, L. Ciccarelli, G. Cobianchi, S. Poularakis,
F. Maurer, S. Battista, and A. Byagowi, “MPEG-5 Part 2: Low
complexity enhancement video coding (LCEVC): Overview and per-
formance evaluation,” in Applications of Digital Image Processing
XLIII, vol. 11510.
International Society for Optics and Photonics,
2020, p. 115101C.
J. Han, B. Li, D. Mukherjee, C.-H. Chiang, A. Grange, C. Chen, H. Su,
S. Parker, S. Deng, U. Joshi et al., “A technical overview of AV1,”
Proceedings of the IEEE, vol. 109, no. 9, pp. 1435–1462, 2021.
J. Zhang, C. Jia, M. Lei, S. Wang, S. Ma, and W. Gao, “Recent
development of AVS video coding standard: AVS3,” in 2019 Picture
Coding Symposium (PCS).

IEEE, 2019, pp. 1–5.

[5]

[6]

[7] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.

521, no. 7553, pp. 436–444, 2015.

[8] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in NIPS, vol. 25, 2012, pp.
1097–1105.

[9] C. Dong, Y. Deng, C. C. Loy, and X. Tang, “Compression artifacts
reduction by a deep convolutional network,” in ICCV, 2015, pp. 576–
584.

[10] G. Toderici, S. M. O’Malley, S. J. Hwang, D. Vincent, D. Min-
nen, S. Baluja, M. Covell, and R. Sukthankar, “Variable rate im-
age compression with recurrent neural networks,” arXiv preprint
arXiv:1511.06085, 2015.

[11] A. Ortega and K. Ramchandran, “Rate-distortion methods for image
and video compression,” IEEE Signal processing magazine, vol. 15,
no. 6, pp. 23–50, 1998.
ITU-T (formerly CCITT), “Codec for videoconferencing using primary
digital group transmission,” ITU-T Recommendation H.120; version 1,
1984; version 2, 1988.

[12]

[14]

[15]

[13] ——, “Video codec for audiovisual services at p × 64 kbitis,” ITU-T
Recommendation H.261; version 1, Nov. 1990; version 2, Mar. 1993.
ISO/IEC JTCI, “Coding of moving pictures and associated audio for
digital storage media at up to about 1.5 Mbit/s - Part 2: Video,”
ISO/IEC 11172-2 (MPEG-I), Mar. 1993.
ITU-T (formerly CCITT) and ISO/IEC JTC1, “Generic coding of
moving pictures and associated audio information - Part 2: Video,”
ITU-T Recommendation H.262 - ISO/IEC 13818-2 (MPEG-2), Nov.
1994.
ITU-T (formerly CCITT), “Video coding for low bitrate communica-
tion,” ITU-T Recommendation H.263; version 1, Nov. 1995; version
2, Jan. 1998.

[16]

[17] G. J. Sullivan and T. Wiegand, “Rate-distortion optimization for video
compression,” IEEE signal processing magazine, vol. 15, no. 6, pp.
74–90, 1998.

[18] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview
of the H.264/AVC video coding standard,” IEEE Transactions on
circuits and systems for video technology, vol. 13, no. 7, pp. 560–
576, 2003.

[19] G. J. Sullivan, J. Ohm, W.-J. Han, and T. Wiegand, “Overview of the
high efﬁciency video coding (HEVC) standard,” IEEE Transactions
on Circuits and Systems for Video Technology, vol. 22, no. 12, pp.
1649–1668, 2012.

[21]

[20] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. J. Sullivan,
“Rate-constrained coder control and comparison of video coding
standards,” IEEE Transactions on circuits and systems for video
technology, vol. 13, no. 7, pp. 688–703, 2003.
J.-R. Ohm, G. J. Sullivan, H. Schwarz, T. K. Tan, and T. Wiegand,
“Comparison of the coding efﬁciency of video coding standards-
including high efﬁciency video coding (HEVC),” IEEE Transactions
on circuits and systems for video technology, vol. 22, no. 12, pp.
1669–1684, 2012.
J. Nocedal and S. Wright, Numerical optimization. Springer Science
& Business Media, 2006.

[22]

[23] L.-K. Liu and E. Feig, “A block-based gradient descent search algo-
rithm for block motion estimation in video coding,” IEEE Transactions
on circuits and systems for Video Technology, vol. 6, no. 4, pp. 419–
422, 1996.

[24] F. Dufaux and J. Konrad, “Efﬁcient, robust, and fast global motion
estimation for video coding,” IEEE transactions on image processing,
vol. 9, no. 3, pp. 497–501, 2000.

[25] O.-C. Chen, “Motion estimation using a one-dimensional gradient
descent search,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 10, no. 4, pp. 608–616, 2000.

[26] L.-M. Po, K.-H. Ng, K.-W. Cheung, K.-M. Wong, Y. M. S. Uddin,
and C.-W. Ting, “Novel directional gradient descent searches for fast
block motion estimation,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 19, no. 8, pp. 1189–1195, 2009.

[27] L. Li, H. Li, D. Liu, Z. Li, H. Yang, S. Lin, H. Chen, and F. Wu, “An
efﬁcient four-parameter afﬁne motion model for video coding,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 28,
no. 8, pp. 1934–1948, 2018.

[28] B. Bross, J. Chen, S. Liu, and Y.-K. Wang, “Versatile video coding
(draft 10),” Joint Video Experts Team (JVET), Tech. Rep. JVET-
S2001, 2020.

[29] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-R.
Ohm, “Overview of the versatile video coding (VVC) standard and its
applications,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 31, no. 10, pp. 3736–3764, 2021.

[30] Y. Wang, J. Ostermann, and Y.-Q. Zhang, Video processing and
communications. Prentice hall Upper Saddle River, NJ, 2002, vol. 1.
[31] X. Wu, E. Barthel, and W. Zhang, “Piecewise 2D autoregression for

predictive image coding,” in ICIP.

IEEE, 1998, pp. 901–904.

[32] X. Li and M. T. Orchard, “Edge-directed prediction for lossless com-
pression of natural images,” IEEE Transactions on image processing,
vol. 10, no. 6, pp. 813–817, 2001.

[33] H. Ye, G. Deng, and J. C. Devlin, “Least squares approach for lossless
image coding,” in International Symposium on Signal Processing and
its Applications (ISSPA), vol. 1.

IEEE, 1999, pp. 63–66.

[34] H. Liu, Y. Chen, J. Chen, L. Zhang, and M. Karczewicz, “Local
illumination compensation,” VCEG, Tech. Rep. VCEG-AZ06, 2015.
[35] K. Zhang, J. Chen, L. Zhang, X. Li, and M. Karczewicz, “Enhanced
cross-component linear model for chroma intra-prediction in video
coding,” IEEE Transactions on Image Processing, vol. 27, no. 8, pp.
3983–3997, 2018.

[36] T. Wedi and H. G. Musmann, “Motion-and aliasing-compensated
prediction for hybrid video coding,” IEEE transactions on circuits and
systems for video technology, vol. 13, no. 7, pp. 577–586, 2003.
[37] T. Wedi, “Adaptive interpolation ﬁlters and high-resolution displace-
ments for video coding,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 16, no. 4, pp. 484–491, 2006.

[38] Y. Vatis and J. Ostermann, “Adaptive interpolation ﬁlter for H.
264/AVC,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 19, no. 2, pp. 179–192, 2008.

[39] C.-Y. Tsai, C.-Y. Chen, T. Yamakage, I. S. Chong, Y.-W. Huang, C.-M.
Fu, T. Itoh, T. Watanabe, T. Chujoh, M. Karczewicz et al., “Adaptive
loop ﬁltering for video coding,” IEEE Journal of Selected Topics in
Signal Processing, vol. 7, no. 6, pp. 934–945, 2013.

[40] M. Karczewicz, N. Hu, J. Taquet, C.-Y. Chen, K. Misra, K. Andersson,
P. Yin, T. Lu, E. Franc¸ois, and J. Chen, “VVC in-loop ﬁlters,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 31,
no. 10, pp. 3907–3925, 2021.

[41] A. Alshin, E. Alshina, and T. Lee, “Bi-directional optical ﬂow for
improving motion compensation,” in 28th Picture Coding Symposium.
IEEE, 2010, pp. 422–425.

[42] D. Liu, Y. Li, J. Lin, H. Li, and F. Wu, “Deep learning-based video
coding: A review and a case study,” ACM Computing Surveys (CSUR),
vol. 53, no. 1, pp. 1–35, 2020.

[43] Y. Dai, D. Liu, and F. Wu, “A convolutional neural network approach
for post-processing in HEVC intra coding,” in International Confer-
ence on Multimedia Modeling. Springer, 2017, pp. 28–39.

[44] Y. Dai, D. Liu, Z.-J. Zha, and F. Wu, “A CNN-based in-loop ﬁlter

with CU classiﬁcation for HEVC,” in VCIP, 2018, pp. 1–4.

[45] Z. Guan, Q. Xing, M. Xu, R. Yang, T. Liu, and Z. Wang, “MFQE
2.0: A new approach for multi-frame quality enhancement on com-
pressed video,” IEEE transactions on pattern analysis and machine
intelligence, vol. 43, no. 3, pp. 949–963, 2019.

[46] C. Jia, S. Wang, X. Zhang, S. Wang, J. Liu, S. Pu, and S. Ma, “Content-
aware convolutional neural network for in-loop ﬁltering in high
efﬁciency video coding,” IEEE Transactions on Image Processing,
vol. 28, no. 7, pp. 3343–3356, 2019.

[47] Y. Zhang, T. Shen, X. Ji, Y. Zhang, R. Xiong, and Q. Dai, “Residual
highway convolutional neural networks for in-loop ﬁltering in HEVC,”
IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3827–
3841, 2018.
J. Pfaff, P. Helle, D. Maniry, S. Kaltenstadler, W. Samek, H. Schwarz,
D. Marpe, and T. Wiegand, “Neural network based intra prediction

[48]

23

for video coding,” in Applications of Digital Image Processing XLI,
vol. 10752.
International Society for Optics and Photonics, 2018, p.
1075213.

[49] Z. Zhao, S. Wang, S. Wang, X. Zhang, S. Ma, and J. Yang, “Enhanced
bi-prediction with convolutional neural network for high efﬁciency
video coding,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 29, no. 11, pp. 3291–3301, 2019.
J. Liu, S. Xia, W. Yang, M. Li, and D. Liu, “One-for-all: Grouped
variation network-based fractional
interpolation in video coding,”
IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2140–
2151, 2019.

[50]

[51] H. Ma, D. Liu, R. Xiong, and F. Wu, “iWave: CNN-based wavelet-like
transform for image compression,” IEEE Transactions on Multimedia,
vol. 22, no. 7, pp. 1667–1679, 2019.

[52] C. Ma, D. Liu, X. Peng, L. Li, and F. Wu, “Convolutional neural
network-based arithmetic coding for HEVC intra-predicted residues,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 30, no. 7, pp. 1901–1916, 2019.
J. Li, B. Li, J. Xu, R. Xiong, and W. Gao, “Fully connected network-
based intra prediction for image coding,” IEEE Transactions on Image
Processing, vol. 27, no. 7, pp. 3236–3247, 2018.

[53]

[54] S. Huo, D. Liu, F. Wu, and H. Li, “Convolutional neural network-
based motion compensation reﬁnement for video coding,” in ISCAS.
IEEE, 2018, pp. 1–4.

[55] Y. Li, L. Li, Z. Li, J. Yang, N. Xu, D. Liu, and H. Li, “A hybrid neural
network for chroma intra prediction,” in ICIP, 2018, pp. 1797–1801.
[56] N. Yan, D. Liu, H. Li, B. Li, L. Li, and F. Wu, “Convolutional
neural network-based fractional-pixel motion compensation,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 29,
no. 3, pp. 840–853, 2019.

[57] ——, “Invertibility-driven interpolation ﬁlter for video coding,” IEEE
Transactions on Image Processing, vol. 28, no. 10, pp. 4912–4925,
2019.

[58] S. Huo, D. Liu, B. Li, S. Ma, F. Wu, and W. Gao, “Deep network-
based frame extrapolation with reference frame alignment,” IEEE
Transactions on Circuits and Systems for Video Technology, vol. 31,
no. 3, pp. 1178–1192, 2021.

[59] D. Liu, H. Ma, Z. Xiong, and F. Wu, “CNN-based DCT-like transform
for image compression,” in International Conference on Multimedia
Modeling. Springer, 2018, pp. 61–72.

[60] R. Song, D. Liu, H. Li, and F. Wu, “Neural network-based arithmetic
coding of intra prediction modes in HEVC,” in VCIP, 2017, pp. 1–4.
[61] Y. Li, D. Liu, H. Li, L. Li, F. Wu, H. Zhang, and H. Yang, “Convolu-
tional neural network-based block up-sampling for intra frame coding,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 28, no. 9, pp. 2316–2330, 2018.

[62] Y. Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, “Learning a convolutional
neural network for image compact-resolution,” IEEE Transactions on
Image Processing, vol. 28, no. 3, pp. 1092–1107, 2019.

[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[64] Z. Liu, X. Yu, Y. Gao, S. Chen, X. Ji, and D. Wang, “CU partition
mode decision for HEVC hardwired intra encoder using convolution
neural network,” IEEE Transactions on Image Processing, vol. 25,
no. 11, pp. 5088–5103, 2016.

[65] M. Xu, T. Li, Z. Wang, X. Deng, R. Yang, and Z. Guan, “Reducing
complexity of HEVC: A deep learning approach,” IEEE Transactions
on Image Processing, vol. 27, no. 10, pp. 5044–5059, 2018.
[66] Z. Jin, P. An, L. Shen, and C. Yang, “CNN oriented fast QTBT
partition algorithm for JVET intra coding,” in VCIP, 2017, pp. 1–4.
[67] K. Kim and W. W. Ro, “Fast CU depth decision for HEVC using
neural networks,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 29, no. 5, pp. 1462–1473, 2018.

[68] A. Feng, C. Gao, L. Li, D. Liu, and F. Wu, “CNN-based depth
map prediction for fast block partitioning in hevc intra coding,” in

24

2021 IEEE International Conference on Multimedia and Expo (ICME).
IEEE, 2021, pp. 1–6.

[69]

[70]

[71]

I. Storch, L. Agostini, B. Zatt, S. Bampi, and D. Palomino, “Fastin-
ter360: A fast inter mode decision for HEVC 360 video coding,” IEEE
Transactions on Circuits and Systems for Video Technology, DOI:
10.1109/TCSVT.2021.3096752, 2021.

J. Ball´e, V. Laparra, and E. P. Simoncelli, “End-to-end optimized
image compression,” arXiv preprint arXiv:1611.01704, 2016.

J. Ball´e, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Vari-
ational image compression with a scale hyperprior,” arXiv preprint
arXiv:1802.01436, 2018.

[72] D. Minnen, J. Ball´e, and G. Toderici, “Joint autoregressive and
hierarchical priors for learned image compression,” in NIPS, 2018,
pp. 10 794–10 803.

[73] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor,
and M. Covell, “Full resolution image compression with recurrent
neural networks,” in CVPR, 2017, pp. 5306–5314.

[74] N. Johnston, D. Vincent, D. Minnen, M. Covell, S. Singh, T. Chinen,
S. Jin Hwang, J. Shor, and G. Toderici, “Improved lossy image
compression with priming and spatially adaptive bit rates for recurrent
networks,” in CVPR, 2018, pp. 4385–4393.

[75] H. Ma, D. Liu, N. Yan, H. Li, and F. Wu, “End-to-end opti-
mized versatile image compression with wavelet-like transform,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, DOI:
10.1109/TPAMI.2020.3026003, 2020.

[76] Y. Hu, W. Yang, Z. Ma, and J. Liu, “Learning end-to-end lossy image
compression: A benchmark,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, DOI: 10.1109/TPAMI.2021.3065339, 2021.

[77] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned image com-
pression with discretized gaussian mixture likelihoods and attention
modules,” in CVPR, 2020, pp. 7939–7948.

[78] T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma, “Deepcoder:
A deep neural network based video compression,” in VCIP.
IEEE,
2017, pp. 1–4.

[79] C.-Y. Wu, N. Singhal, and P. Krahenbuhl, “Video compression through

image interpolation,” in ECCV, 2018, pp. 416–431.

[80] Z. Chen, T. He, X. Jin, and F. Wu, “Learning for video compression,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 30, no. 2, pp. 566–576, 2019.

[81] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, “DVC: An
end-to-end deep video compression framework,” in CVPR, 2019, pp.
11 006–11 015.

[82] G. Lu, X. Zhang, W. Ouyang, L. Chen, Z. Gao, and D. Xu, “An end-to-
end learning framework for video compression,” IEEE transactions on
pattern analysis and machine intelligence, vol. 43, no. 10, pp. 3292–
3308, 2021.

[83] A. Habibian, T. v. Rozendaal, J. M. Tomczak, and T. S. Cohen, “Video
compression with rate-distortion autoencoders,” in ICCV, 2019, pp.
7033–7042.

[84] A. Djelouah, J. Campos, S. Schaub-Meyer, and C. Schroers, “Neural
inter-frame compression for video coding,” in ICCV, 2019, pp. 6421–
6429.

[85] O. Rippel, S. Nair, C. Lew, S. Branson, A. G. Anderson, and
L. Bourdev, “Learned video compression,” in ICCV, 2019, pp. 3454–
3463.

[89] E. Agustsson, D. Minnen, N. Johnston, J. Balle, S. J. Hwang, and
G. Toderici, “Scale-space ﬂow for end-to-end optimized video com-
pression,” in CVPR, 2020, pp. 8503–8512.

[90] H. Liu, M. Lu, Z. Ma, F. Wang, Z. Xie, X. Cao, and Y. Wang,
“Neural video coding using multiscale motion compensation and
spatiotemporal context model,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. 31, no. 8, pp. 3182–3196, 2021.

[91] Z. Hu, G. Lu, and D. Xu, “FVC: A new framework towards deep
video compression in feature space,” in CVPR, 2021, pp. 1502–1511.
[92] R. Yang, F. Mentzer, L. Van Gool, and R. Timofte, “Learning for video
compression with recurrent auto-encoder and recurrent probability
model,” IEEE Journal of Selected Topics in Signal Processing, vol. 15,
no. 2, pp. 388–401, 2020.

[93] G. Lu, C. Cai, X. Zhang, L. Chen, W. Ouyang, D. Xu, and Z. Gao,
“Content adaptive and error propagation aware deep video compres-
sion,” in ECCV. Springer, 2020, pp. 456–472.

[94] Z. Hu, Z. Chen, D. Xu, G. Lu, W. Ouyang, and S. Gu, “Improving
deep video compression by resolution-adaptive ﬂow coding,” in ECCV.
Springer, 2020, pp. 193–209.
[95] C. E. Shannon, “A mathematical

theory of communication,” Bell

Systems Technical Journal, vol. 27, no. 4, pp. 623–656, 1948.
[96] ——, “Coding theorems for a discrete source with a ﬁdelity criteria,”

Ire National Convention Record, 1959.

[97] H. Everett III, “Generalized lagrange multiplier method for solving
problems of optimum allocation of resources,” Operations research,
vol. 11, no. 3, pp. 399–417, 1963.

[98] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “PWC-Net: CNNs for
optical ﬂow using pyramid, warping, and cost volume,” in CVPR,
2018, pp. 8934–8943.

[99] S. Niklaus, L. Mai, and F. Liu, “Video frame interpolation via adaptive

separable convolution,” in ICCV, 2017, pp. 261–270.

[100] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan,
A. Tao, and B. Catanzaro, “SDC-Net: Video prediction using spatially-
displaced convolution,” in ECCV, 2018, pp. 718–733.

[101] S. Niklaus and F. Liu, “Context-aware synthesis for video frame

interpolation,” in CVPR, 2018, pp. 1701–1710.

[102] W. Bao, W.-S. Lai, X. Zhang, Z. Gao, and M.-H. Yang, “MEMC-Net:
Motion estimation and motion compensation driven neural network for
video interpolation and enhancement,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 43, no. 3, pp. 933–948, 2019.
[103] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for

image recognition,” in CVPR, 2016, pp. 770–778.

[104] C. Christopoulos, A. Skodras, and T. Ebrahimi, “The JPEG2000 still
image coding system: an overview,” IEEE transactions on consumer
electronics, vol. 46, no. 4, pp. 1103–1127, 2000.

[105] L. Theis, W. Shi, A. Cunningham, and F. Husz´ar, “Lossy im-
age compression with compressive autoencoders,” arXiv preprint
arXiv:1703.00395, 2017.

[106] L. Song, X. Tang, W. Zhang, X. Yang, and P. Xia, “The SJTU 4K

video sequence dataset,” in QoMEX, 2013, pp. 34–35.

[107] F. Bossen, “Common test conditions and software reference conﬁgu-

rations,” JCT-VC, Tech. Rep. JCTVC-F900, 2011.

[108] G. Bjontegaard, “Calcuation of average PSNR differences between

RD-curves,” VCEG, Tech. Rep. VCEG-M33, 2001.

[109] Z. Teed and J. Deng, “RAFT: Recurrent all-pairs ﬁeld transforms for

optical ﬂow,” in ECCV. Springer, 2020, pp. 402–419.

[86] M. A. Yilmaz and A. M. Tekalp, “End-to-end rate-distortion optimiza-
tion for bi-directional learned video compression,” in ICIP.
IEEE,
2020, pp. 1311–1315.

[110] Z. Yin, T. Darrell, and F. Yu, “Hierarchical discrete distribution
decomposition for match density estimation,” in CVPR, 2019, pp.
6044–6053.

[87] R. Yang, F. Mentzer, L. V. Gool, and R. Timofte, “Learning for video
compression with hierarchical quality and recurrent enhancement,” in
CVPR, 2020, pp. 6628–6637.

[88]

J. Lin, D. Liu, H. Li, and F. Wu, “M-LVC: multiple frames prediction
for learned video compression,” in CVPR, 2020, pp. 3546–3554.

