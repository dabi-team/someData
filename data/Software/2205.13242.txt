Inertially Assisted Semi-Direct Visual Odometry for Fixed Wing

Autonomous Unmanned Air Vehicles

Eduardo Gallo∗†‡and Antonio Barrientos §¶‖

April 2022

2
2
0
2

y
a
M
6
2

]

O
R
.
s
c
[

1
v
2
4
2
3
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

This article proposes a method to diminish the pose (position plus attitude) drift experienced by an SVO (Semi-
Direct Visual Odometry) based visual navigation system installed onboard a UAV (Unmanned Air Vehicle) by
supplementing its pose estimation non linear optimizations with priors based on the outputs of a GNSS (Global
Navigation Satellite System) Denied inertial navigation system. The method is inspired in a PI (Proportional
Integral) control system, in which the attitude, altitude, and rate of climb inertial outputs act as targets to
ensure that the visual estimations do not deviate far from their inertial counterparts. The resulting IA-VNS
(Inertially Assisted Visual Navigation System) achieves major reductions in the horizontal position drift inherent
to the GNSS-Denied navigation of autonomous fixed wing low SWaP (Size, Weight, and Power) UAVs. Additionally,
the IA-VNS can be considered as a virtual incremental position (ground velocity) sensor capable of providing
observations to the inertial filter. Stochastic high fidelity Monte Carlo simulations of two representative scenarios
involving the loss of GNSS signals are employed to evaluate the results and to analyze their sensitivity to the
terrain type overflown by the aircraft as well as to the quality of the onboard sensors on which the priors are
based. The author releases the C++ implementation of both the navigation algorithms and the high fidelity
simulation as open-source software [1].

Keywords: GNSS-Denied, GPS-Denied, visual navigation, autonomous navigation, VO, VIO, UAV, SVO, priors,
non linear optimization

∗Eduardo Gallo holds a MSc in Aerospace Engineering by the Polytechnic University of Madrid and has twenty-three years of
experience working in aircraft trajectory prediction, modeling, and flight simulation. He is currently a Senior Trajectory Prediction
and Aircraft Performance Engineer at Boeing Research & Technology Europe (BR&TE), although he is publishing this article in his
individual capacity and time as part of his PhD thesis titled “Autonomous Unmanned Air Vehicle GNSS-Denied Navigation”, advised
by Dr. Antonio Barrientos within the Centre for Automation and Robotics of the Polytechnic University of Madrid.

†Contact: edugallo@yahoo.com, e.gallo@alumnos.upm.es, https://orcid.org/0000-0002-7397-0425
‡Affiliation: Centro de Automática y Robótica, Universidad Politécnica de Madrid - Consejo Superior de Investigaciones Cientí-

ficas. Centre for Automation and Robotics, Polytechnic University of Madrid.

§Antonio Barrientos received his MSc and PhD degrees from the Polytechnic University of Madrid in 1982 and 1986, respectively.
In 2002 he obtained the MSc Degree in Biomedical Engineering by the National University of Distance Education. Since 1988 he is a
professor at the Polytechnic University of Madrid, where he is presently a full professor teaching robotics and automatic control. He
has worked in robotics for more than 30 years, developing industrial and service robots for different areas. He is a permanent staff
member of the Center for Automation and Robotics (UPM-CSIC) where he is the head of the Applied Robotics Unit. He is a Senior
Member of IEEE and former member of the steering committee of CEA (Spanish Automatics Committee - IFAC Spanish chapter).

¶Contact: antonio.barrientos@upm.es, https://orcid.org/0000-0003-1691-3907
‖Affiliation: Centro de Automática y Robótica, Universidad Politécnica de Madrid - Consejo Superior de Investigaciones Cientí-

ficas. Centre for Automation and Robotics, Polytechnic University of Madrid.

1

 
 
 
 
 
 
Mathematical Notation

Any variable with a hat accent < ˆ· > refers to its (inertial) estimated value, with a circular accent < ∘· > to its
(visual) estimated value, and with a tilde < ̃︀· > to its measured value. In the case of vectors, which are displayed
in bold (e.g., x), other employed symbols include the wide hat < ̂︀· >, which refers to the skew-symmetric form,
the bar < ¯· >, which represents the vector homogeneous coordinates, and the double vertical bars < ‖ · ‖ >, which
refer to the norm. In the case of scalars, the vertical bars < | · | > refer to the absolute value. When employing
quaternions and rigid body poses, which are also displayed in bold (e.g., q and 𝜁), the asterisk superindex < ·* >
refers to the conjugate, their concatenation and multiplication are represented by ∘ and ⊗ respectively, and ⊕
and ⊖ refer to the plus and minus operators.

This article includes various non linear optimizations solved in the spaces of both rigid body rotations and full

motions, instead of Euclidean spaces. It is assumed that the reader is familiar with the Lie algebra of the special
orthogonal group of R3, known as SO(3), and that of the special euclidean group of R3, represented by SE(3), in
particular what refers to the groups actions, concatenations, perturbations, and jacobians, as well as with their
tangent spaces (the rotation vector r and angular velocity 𝜔 for rotations, the transform vector 𝜏 and twist 𝜉 for
motions). [2, 3, 4] are recommended as references.

Position of throttle and control surfaces

Aerodynamic path angle

Error threshold

𝛾TAS
𝛿
𝛿CNTR
𝛿TARGET Control targets
∆

Estimation error, increment

∆p

∆T

𝜃

𝜁

𝜇

𝜉

𝜉

Π
𝜚TUK
𝜎

𝜏

𝜑

𝜒

𝜓

𝜔
Eh
EPO
Eq
ERP
E

f

Atmospheric pressure offset

Atmospheric temperature offset

Body pitch angle

Pose, unit dual quaternion

Mean or expected value

Body bank angle
Motion (SE(3)) velocity or twist
Camera projection

Tukey error function

Standard deviation

Pose, transform vector

Attitude, Euler angles

Bearing

Heading or body yaw angle
Angular (SO(3)) velocity
Altitude adjustment error

Pose optimization error

Attitude adjustment error

Reprojection error

Full sensor error

Focal length

g

h
HHEIGHT
HP
I

J

ℳ

p

q

r

R

ℛ

S

t

T
TE,GDT
v

v
wTUK
x

x
ˆx = xEST
∘x = xIMG
xREF
̃︀x = xSENSED
x = xTRUTH

Lie group action

Geometric altitude

Height, depth over terrain

Pressure altitude

Camera image

Jacobian
SE(3) Lie group element
Point, feature

Attitude, unit quaternion

Attitude, rotation vector

Attitude, rotation matrix
SO(3) Lie group element
Sensor dimension

Time

Displacement

Geodetic coordinates

Speed

Velocity

Tukey weight function

Horizontal distance

Position

Inertial estimated trajectory

Visual estimated trajectory

Reference objectives

Sensed trajectory

Real trajectory

Five different reference frames are used in this article: the ECEF frame (FE), the NED frame (FN), the ground frame
(FX), the body frame (FB), and the camera frame (FC). In addition, the two dimensional image frame (FIMG)
is also employed as introduced in appendix A. Superindexes are employed over vectors to specify the reference
frame in which they are viewed (e.g., vN refers to ground velocity viewed in FN, while vB is the same vector but
viewed in FB). Subindexes may be employed to clarify the meaning of the variable or vector, such as in vTAS

2

for air velocity instead of the ground velocity v, in which case the subindex is either an acronym or its meaning
is clearly explained when first introduced. Subindexes may also refer to a given component of a vector, e.g. vN
refers to the second component of vN. In addition, where two reference frames appear as subindexes to a vector,
it means that the vector goes from the first frame to the second. For example, 𝜔B
refers to the angular velocity
from the FN frame to the FB frame viewed in FB.

NB

2

In addition, there exist various indexes that appear as subindexes: j identifies an specific run within each Monte
Carlo simulation, n identifies a discrete time instant (tn) for the inertial estimations, s (ts) refers to the sensor
outputs, i identifies an image or frame (ti), and k is employed for the keyframes used to generate the map or
terrain structure. Other employed subindexes are l for the steps of the various iteration processes that take place,
and j for the features and associated 3D points. With respect to superindexes, two stars < ·⋆⋆ > represent the
reprojection only solution, while two circles < ·∘∘ > identify a target.

Acronyms

ACC

AOA

AOS

BRIEF

DS

DSO

ECEF

EKF

FAST

FM

FR

FTE

GNSS

GPS

GYR

IA-VNS

ACCelerometers
Angle Of Attack

Angle Of Sideslip

Binary Robust Independent Elementary

Features

DeSert

Direct Sparse Odometry

Earth Centered Earth Fixed

Extended Kalman Filter

Features from Accelerated Segment Test

FarM

FoRest

Flight Technical Error

Global Navigation Satellite System

Global Positioning System
GYRoscopes
Inertially Assisted Visual Navigation

System

IA-VNSE

Inertially Assisted Visual Navigation

System Error

Instrument Landing System
Inertial Measurement Unit

Inertial Navigation System

Inertial Navigation System Error

Incremental Smoothing and Mapping

International Organization for

Standardization

LIght Detection And Ranging

Large scale Semi Direct
MAGnetometers

ILS
IMU

INS

INSE

iSAM

ISO

LIDAR

LSD

MAG

MAV

MEMS

Micro Air Vehicle

Micro Machined Electromechanical

System

MSCKF Multi State Constraint Kalman Filter
MSF

Multi Sensor Fusion

MSL

MX

NDB

NED

NSE

OAT

OKVIS

ORB

OSP

PI

Mean Sea Level

MiX

Non Directional Beacon

North East Down

Navigation System Error

Outside Air Temperature
Open Keyframe Visual Inertial SLAM
Oriented FAST and rotated BRIEF
Outside Static Pressure

Proportional Integral

PRaire

PR
RANSAC RANdom SAmple Consensus
ROC

Rate Of Climb

ROVIO

SAR

SENS
SLAM

SLERP

SoOP

SVO

SWaP

TAS

TSE

UAV

UR

Robust Visual Inertial Odometry

Synthetic Aperture Radar
SENSors
Simultaneous Localization And Mapping

Spherical linear interpolation

Signals of OPportunity

Semi direct Visual Odometry

Size, Weight, and Power

True Air Speed

Total System Error

Unmanned Aerial Vehicle

URban

3

United States of America
USA
VA-INS Visually Assisted Inertial Navigation

System

Very High Frequency

Visual Inertial Navigation System

Visual Inertial Odometry

VHF

VINS

VIO

1

Introduction and Outline

VNS

VNSE

VO

Visual Navigation System

Visual Navigation System Error

Visual Odometry
VHF Omnidirectional Radio Range

VOR
WGS84 World Geodetic System 1984

The extreme dependency of an autonomous UAV (Unmanned Air Vehicle) navigation to the presence of GNSS
(Global Navigation Satellite System) signals, without which it incurs in a slow but unavoidable position drift

that may ultimately lead to the loss of the platform, is discussed in section 2. This dependency does not only

constitute a significant obstacle for the widespread usage of these platforms in civil airspace, but also represents
a threat to military missions. Section 2 also reviews the current approaches to GNSS-Denied navigation, with
emphasis on those based on the images taken by onboard cameras. Figure 1 shows a schematic view of a generic
navigation system, which relies on the sensor outputs
̃︀x = xSENSED and initial estimations ˆx0 and provides its
pose estimations ˆx = xEST to the guidance and control system. Please refer to section 4 and table 1 for further
details on the terms employed in the various flow diagrams below.

̃︀x (ts) = xSENSED (ts)

ˆx0

NAVIGATION

ˆx (tn) = xEST (tn)

Figure 1: Navigation system flow diagram

This article focuses on the need to develop navigation systems capable of diminishing the position drift inherent to
the flight in GNSS-Denied conditions of an autonomous fixed wing low SWaP (Size, Weight, and Power) aircraft so it
has a higher probability of reaching the vicinity of a recovery point, from where it can be landed by remote control.

A previous article by the same authors, [5], showed that it is possible to take advantage of sensors already present

onboard fixed wing aircraft1, the particularities of fixed wing flight, and the atmospheric and wind estimations
that can be obtained before the GNSS signals are lost, to develop an EKF (Extended Kalman Filter) based INS
(Inertial Navigation System) that results in bounded (no drift) estimations for attitude2, altitude3, and ground

velocity4, as well as an unavoidable drift in horizontal position caused by integrating the ground velocity. Figure
2 graphically depicts that the INS inputs include all sensor measurements with the exception of the camera images
I.

̃︀x (ts) ∖ I (ti) = xSENSED (ts) ∖ I (ti)

ˆx0

INERTIAL

NAVIGATION

ˆx (tn) = xEST (tn)

Figure 2: INS flow diagram

This article focuses on visual navigation, this is, that based on the images periodically taken by a down facing

camera rigidly attached to the aircraft structure. The objectives, novelty, and applications of the proposed

1These include accelerometers, gyroscopes, magnetometers, Pitot tube, air vanes, thermometer, and barometer.
2A bounded attitude estimation ensures that the aircraft can remain aloft in GNSS-Denied conditions for as long as there is fuel

available.

3The altitude estimation error depends on the change in atmospheric pressure offset Δp [6] from its value at the time the GNSS

signals are lost, which is bounded by atmospheric physics.

4The ground velocity estimation error depends on the change in wind velocity from its value at the time the GNSS signals are lost,

which is bounded by atmospheric physics.

4

approach are discussed in section 3. Section 4 introduces the stochastic high fidelity simulation employed to

evaluate the navigation results by means of Monte Carlo executions of two scenarios5 representative of the
challenges of GNSS-Denied navigation, which are described in detail in [7]. Its reading is highly recommended in
order to understand the rest of this article, as is that of [8], which describes the different error sources modeled

for each of the onboard sensors.

A VNS (Visual Navigation System) based on an advanced publicly available VO (Visual Odometry) algorithm
known as SVO (Semi-Direct Visual Odometry) is introduced in section 5 and employed as a baseline. Visual
navigation does not rely on absolute references and hence slowly accumulates error (drifts) not only in horizontal
position, but also in attitude and altitude. Figure 3 shows that the VNS relies exclusively on the images I (ti) as
well as the initial visual estimations ∘x0, as explained in section 5.

I (ti) ⊂ xSENSED (ti)

∘x0

VISUAL

NAVIGATION

∘x (ti) = xIMG (ti)

Figure 3: VNS flow diagram

Section 6 describes the proposed IA-VNS (Inertially Assisted VNS), which employs the INS attitude and altitude
estimations to introduce priors into the pose estimation non linear optimizations present in SVO, resulting not only
in significant improvements in the visual attitude and altitude estimations, but specially in a major reduction in
horizontal position drift, which is a full order of magnitude lower than the drifts obtained by either the INS or
the VNS by themselves. The IA-VNS is graphically depicted in figure 4.

I (ti) ⊂ xSENSED (ti)
∘x0

INERTIALLY ASSISTED
VISUAL NAVIGATION

∘x (ti) = xIMG (ti)

ˆx0

INERTIAL

NAVIGATION

ˆx (tn) = xEST (tn)

̃︀x (ts) ∖ I (ti) = xSENSED (ts) ∖ I (ti)

Figure 4: IA-VNS flow diagram

The IA-VNS horizontal position estimations can be considered as the output of a virtual incremental horizontal
displacement sensor6, which are fed back as periodic observations into the EKF, resulting in a VA-INS (Visually
Assisted INS) with not only a major reduction in horizontal position drift compared to the stand alone INS, but
also a significant improvement in attitude estimation accuracy.

The combined VINS (Visual Inertial Navigation System) is hence capable of combining the inertial and visual
sensors and algorithms, employing the inertial attitude and altitude outputs to reduce the visual drifts, as well

as the visual horizontal position output to improve the filter consistency and accuracy, resulting in an accurate
and reliable navigation system capable of consistently leading the low SWaP aircraft to the vicinity of a recovery
point even when faced with an extended flight duration in GNSS-Denied conditions coupled with weather changes
and high turbulence levels. This article however focuses on the IA-VNS algorithms and its results, not on the
fused VINS.

5These are the same scenarios employed to evaluate the INS in [5].
6This is synonymous with a ground velocity sensor.

5

I (ti) ⊂ xSENSED (ti)

VISUAL INERTIAL NAVIGATION SYSTEM

∘x0

ˆx0

̃︀x (ts) ∖ I (ti) = xSENSED (ts) ∖ I (ti)

INERTIALLY ASSISTED

VISUAL NAVIGATION

∘x (ti) = xIMG (ti)

VISUALLY ASSISTED

INERTIAL NAVIGATION

ˆx (tn) = xEST (tn)

Figure 5: VA-INS and VINS flow diagram

The results of both the SVO based VNS as well as the proposed IA-VNS when applied to Monte Carlo simulations
of the two scenarios representative of the challenges of GNSS-Denied conditions are described in section 7. The
analysis of the results continues in section 8, which discusses the sensitivity of the IA-VNS estimations to the type
of terrain overflown by the aircraft, as the terrain texture (or lack of) and its elevation relief are key factors on
the ability of the visual algorithms to detect and track terrain features. Similarly to the analysis performed in [5],

section 9 discusses the sensitivity of the results to the quality or grade of the onboard sensors, on which the priors

are based. By ensuring that the results obtained when employing sensors of inferior quality are qualitatively the
same and quantitatively slightly inferior to those presented in section 7, the benefits of the proposed IA-VNS are
safeguarded from possible errors introduced when modeling the performances of the various sensors. Last, the

results are summarized for convenience in section 10, while section 11 provides a short conclusion.

2 GNSS-Denied Navigation

The number, variety, and applications of UAVs have grown exponentially in the last few years, and the trend
is expected to continue in the future [9, 10]. This is particularly true in the case of low SWaP vehicles because
their reduced cost makes them suitable for a wide range of applications, both civil and military. A significant

percentage of these vehicles are capable of operating autonomously, this is, employing onboard computers to

execute the previously uploaded mission objectives. With small variations, these platforms rely on a suite of

sensors that provide data about the airframe state, a navigation algorithm to estimate the aircraft pose, and

a control system that, based on the navigation outputs, adjusts the aircraft control mechanisms to successfully

execute the preloaded mission.

This article focuses on fixed wing autonomous platforms, which are generally equipped with a GNSS receiver,
accelerometers, gyroscopes, magnetometers, a Pitot tube, air vanes, and one or more cameras. The combination of
triads of accelerometers and gyroscopes is known as the Inertial Measurement Unit (IMU). The errors introduced by
all sensors grow significantly as their SWaP decreases, in particular in the case of the IMU. The recent introduction
of solid state accelerometers and gyroscopes has dramatically improved the performance of low SWaP IMUs, with
new models showing significant improvements when compared to those manufactured only a few years ago. The
problem of noisy measurement readings is compounded in the case of low SWaP vehicles by the low mass of the
platforms, which results in less inertia and hence more high frequency accelerations and rotations caused by
[11] presents a comprehensive review of low SWaP UAV navigation systems and the

the atmospheric turbulence.
problems they face, including the degradation or absence of GNSS signals.

Aircraft navigation traditionally relied on the measurements provided by accelerometers, gyroscopes, and magne-

tometers, incurring in an slow but unbounded position drift that could only be stopped by triangulation with the
use of external navigation (radio) aids. More recently, the introduction of satellite navigation (GNSS) has com-
pletely removed the position drift and enabled autonomous inertial navigation in low SWaP platforms [12, 13, 14].

6

On the negative side, low SWaP inertial navigation exhibits an extreme dependency on the availability of GNSS sig-
nals. If the signals are not present or can not be employed, inertial systems rely on dead reckoning, which results

in velocity and position drift, with the aircraft slowly but steadily deviating from its intended route [15].

The availability of GNSS signals cannot be guaranteed by any means. In addition to the (unlikely) event of one
or various GNSS satellites ceasing to broadcast (voluntarily or not), the GNSS signals can be accidentally blocked
by nearby objects (mountains, buildings, trees), corrupted by their own reflections in those same objects, or

maliciously interfered with by broadcasting a different signal in the same frequency (jamming or spoofing). A
throughout analysis of GNSS threats and reasons for signal degradation is presented in [16]. Any of the above
results in what is known as GNSS -Denied navigation. In that event, the vehicle is unable to fly its intended
route or even return to a safe recovery location, which leads to the uncontrolled loss of the airframe if the GNSS
signals are not recovered before the aircraft runs out of fuel (or battery in case of electric vehicles). Although
there exist various GNSS threat monitoring and reporting systems [17], autonomous aircraft need to continuously
evaluate the quality of the available signals to decide if they can rely on them or not. [18] proposes a continuous
evaluation of GNSS accuracy based on its integration within the INS.

Note that although the term GNSS-Denied is more generic and implies the absence of valid signals from any
navigation satellite system, GPS-Denied is more commonly employed in the literature, particularly in non scientific
circles. Both terms are analogous in the context of navigation, as GPS-Denied does not imply the availability of
navigation signals from non GPS satellite navigation systems; the widespread use of the term GPS-Denied can be
attributed to GPS being the first widely available global navigation satellite system and, at least until recently,
the system on which most GNSS receivers relied. This article however employs the term GNSS-Denied, which is
considered more adequate and technically correct, representing the absence of navigation signals from any GNSS,
GPS included.

The extreme dependency on GNSS availability is not only one of the main impediments for the introduction
of small UAVs in civil airspace, where it is not acceptable to have uncontrolled vehicles causing personal or
material damage, but it also presents a significant drawback for military applications, as a single hull loss may

compromise the onboard technology. At this time there are no comprehensive solutions to the operation of low
SWaP autonomous UAVs in GNSS-Denied scenarios, although the use of onboard cameras seems to be one of the
most promising routes. Bigger and more expensive UAVs, this is, with less stringent SWaP requirements, can rely
to some degree on more accurate IMUs (at the expense of SWaP) and additional communications equipment to
overcome this problem, but for most autonomous UAVs, the permanent loss of the GNSS signals is equivalent to
losing the airframe in an uncontrolled way.

Possible Approaches to GNSS-Denied Navigation

Inertial navigation employs the periodic readings provided by the IMU to estimate the pose of a moving object
by means of dead reckoning or integration. On aircraft, inertial sensors are complemented by magnetometers

and a barometer to add robustness to the inertial solution. Fixed wing aircraft are also equipped with a Pitot

tube and air vanes required by their control system, although their measurements are usually not employed for
navigation. Absolute references such as those provided by navigation radio aids7 or GNSS receivers are required
to remove the position drift inherent to inertial navigation.

Low SWaP autonomous aircraft are too small to incorporate navigation aid receivers, which in any case are not
available over vast regions of the Earth, exhibiting an extreme dependency on the availability of GNSS signals. A
summary of the challenges of GNSS-Denied navigation and the research efforts intended to improve its performance
is provided by [19]. There exist various approaches to mitigate this problem, with detailed reviews provided by

[5, 20].

A promising approach for completely eliminating the position drift is to triangulate using existing signals originally

7The most widely used radio aids for aircraft navigation are VHF Omnidirectional Radio Range (VOR), Non Directional Beacon

(NDB), and Instrument Landing System (ILS).

7

intended for other purposes, such as those of television and cellular networks [21, 22, 23]. Receivers for these
signals, known as Signals of Opportunity (SoOP), have a sufficiently low SWaP to be mounted on most autonomous
aircraft, although the quality and quantity of the available signals varies enormously depending on the location

where the flight takes place.

Georegistration matches landmarks or terrain features as scanned or imaged by vehicles to preloaded data [19, 24],
and can work based on Synthetic Aperture Radar (SAR) [25, 26, 27], Light Detection and Ranging (LIDAR)
[28, 29, 30], or camera visual systems, in what is known as image registration [31, 32, 33, 34]. While SAR suffers
from being memory and computationally expensive and LIDAR is restricted for aviation purposes by its limited
range, image registration is a potentially valid solution for completely removing the position drift in GNSS-Denied
scenarios. Its main challenge is obtaining a high percentage of positive matches when supplied with onboard

generated images that display the terrain not only at different altitudes and attitudes than those of the database

images, but which are also obtained in different seasons, illumination, and weather conditions.

Visual Odometry

Visual Odometry (VO) consists on employing the ground images generated by one or more onboard cameras
(either visual or infrared) without the use of prerecorded image databases or any other sensors, incrementally

estimating the vehicle pose based on the changes that its motion induces on the images [35, 36, 37]. It requires

sufficient illumination, dominance of static scene, enough texture, and scene overlap between consecutive images

or frames. It can rely on a single camera8 (monocular vision), in which case the motion can only be recovered

up to a scale factor, or on various cameras (stereo vision), where the differences among the simultaneous images

taken with the different cameras are employed to determine the scale. It has been employed for navigation of

ground robots, road vehicles, and multirotors flying both indoors and outdoors.

The incremental concatenation of relative poses results in a slow but unbounded pose drift, which can only be
eliminated if aided by Simultaneous Localization and Mapping (SLAM) [38, 39], a particular case of VO in which
the map of the already viewed terrain is stored and employed for loop closure in case it is revisited by the vehicle
during its motion. In this sense, VO only uses the map to improve the local consistency of the solution, while SLAM
is more concerned with its global consistency [35]. The result is that SLAM is potentially more accurate (only if
a certain patch of terrain is visited more than once), but also slower, computationally more expensive, and less

robust.

Modern stand-alone VO algorithms such as Semi Direct Visual Odometry (SVO) [40, 41] and Direct Sparse Odom-
etry (DSO) [42] are robust and exhibit a limited drift, while Large scale Semi Dense SLAM (LSD-SLAM) [43] and
large scale feature based SLAM (ORB-SLAM)9 [44, 45, 46] may be more appropriate if the vehicle revisits an already
imaged area.

A typical VO algorithm includes steps to obtain the images, detect and extract its features, either match or track
those features, estimate the relative motion between consecutive frames, concatenate them to obtain the full
camera pose trajectory, and finally perform some local optimization (bundle adjustment) [35]. VO algorithms can
be generally divided into two broad categories:

• Feature-based methods, also known as matching methods, rely on detecting features (image coordinates plus
descriptor) in every frame and employing the descriptors to match features among consecutive frames.

The camera pose and terrain structure are then recovered by epipolar geometry10 and refined through
minimization of the reprojection error. This enables handling large inter-frame motions, but at the expense
of slow detection algorithms, reliance on detection and matching thresholds, and the use of RANSAC [49] to
identify and remove outliers. The estimated structure and motion relies exclusively on the average of the

8This article relies on monocular vision exclusively.
9ORB stands for Oriented FAST and rotated BRIEF, a type of blob feature.
10Epipolar geometry is that existing between two views of the same object, and only depends on the camera parameters (which
are the same for VO) and their relative pose [47, 48]. When imaging a fixed terrain 3D point, its two images in both camera frames,
the two optical centers, and the 3D point itself are located on the same plane, named the epipolar plane.

8

pixel distance between the positions of each feature in different frames, so many features are required to

average the errors and reduce drift [37].

• Direct or tracking methods avoid the expensive feature extraction and matching process and instead extract
the scene structure and camera motion directly from the luminosity values of selected pixels in the different
frames, based on the local gradient magnitude and direction. Direct methods in turn can be dense (if the
minimization algorithm is applied to every pixel in the image), sparse (if features are extracted and then
tracked instead of matched based on the photometric error at their location), or semi-dense if somewhere
in between (as for example tracking all pixels in the vicinity of each feature). Minimizing the photometric
or luminosity difference between consecutive images requires small inter-frame motions, and although more
precise, the computation of the photometric error is more expensive than the reprojection error employed

in feature-based methods. A final refinement of the structure and motion based on the reprojection error

(bundle adjustment) is possible, although expensive [37].

Visual Inertial Odometry

Estimating the aircraft pose based on both IMUs and cameras represents the most promising solution to GNSS-
Denied navigation, in what is known as Visual Inertial Odometry (VIO) [50, 51], which can also be combined with
image registration to fully eliminate the remaining pose drift. Current VIO implementations are also primarily
intended for ground robots, multirotors, and road vehicles, and hence rely exclusively on the vehicle IMU readings
and the images taken by the onboard cameras, but do not use other sensors commonly found onboard fixed wing
aircraft. VIO has matured significantly in the last few years, with detailed reviews available in [50, 51, 52, 53,
54].

There are currently several open source VIO packages, such as the Multi State Constraint Kalman Filter (MSCKF)
[55], the Open Keyframe Visual Inertial SLAM (OKVIS) [56, 57], the Robust Visual Inertial Odometry (ROVIO)
[58], the monocular Visual Inertial Navigation System (VINS-Mono) [59], SVO combined with Multi Sensor Fusion
(MSF) [40, 41, 60, 61], and SVO combined with Incremental Smoothing and Mapping (iSAM) [40, 41, 62, 63]. All
these open source pipelines are compared in [50], and their results when applied to the EuRoC MAV datasets [64]
are discussed in [65]. There also exist various other published VIO pipelines with implementations that are not
publicly available [66, 67, 68, 69, 70, 71, 72], and there are also others that remain fully proprietary.

The existing VIO schemes to fuse the visual and inertial measurements can be broadly grouped into two paradigms:
loosely coupled pipelines process the measurements separately, resulting in independent visual and inertial pose
estimations, which are then fused to get the final estimate; on the other hand, tightly coupled methods compute
the final pose estimation directly from the tracked image features and the IMU outputs [50, 51]. Tightly coupled
approaches usually result in higher accuracy, as they use all the information available and take advantage of
the IMU integration to predict the feature locations in the next frame. Loosely coupled methods, although less
complex and more computationally efficient, lose information by decoupling the visual and inertial constraints,
and are incapable of correcting the drift present in the visual estimator.

A different classification involves the number of images involved in each estimation [50, 51, 73], which is directly
related with the resulting accuracy and computing demands. Batch algorithms, also known as smoothers, es-
timate multiple states simultaneously by solving a large nonlinear optimization problem or bundle adjustment,

resulting in the highest possible accuracy. Valid techniques to limit the required computing resources include
the reliance on a subset of the available frames (known as keyframes)11, the separation of tracking and mapping
into different threads, and the development of incremental smoothing techniques based on factor graphs [63].
Although employing all available states (full smoothing) is sometimes feasible for very short trajectories, most
pipelines rely on sliding window or fixed lag smoothing, in which the optimization relies exclusively on the mea-
surements associated to the last few keyframes, discarding both the old keyframes as well as all other frames that
have not been cataloged as keyframes. On the other hand, filtering algorithms restrict the estimation process to

11Common criteria to identify keyframes involve thresholds on the time, position, or pose increments from the previous keyframe.

9

the latest state; they require less resources but suffer from permanently dropping all previous information and a

much harder identification and removal of outliers, both of which lead to error accumulation or drift.

The success of any VIO approach relies on an accurate calibration of the pose and time offsets between the IMU
and the camera [50, 51]. Additional challenges applicable to all pipelines include the different working frequencies
of IMUs and cameras, as well the initialization requirements to bootstrap the algorithms.

Additional VIO pipelines are also being developed to take advantage of event cameras [74, 75], which measure
per-pixel luminosity changes asynchronously instead of capturing images at a fixed rate. Event cameras hold

very advantageous properties for visual odometry, including low latency, very high frequency, and excellent

dynamic range, which make them well suited to deal with high speed motion and high dynamic range, which are
problematic for traditional VO and VIO pipelines. On the negative side, new algorithms are required to cope with
the sequence of asynchronous events they generate [50].

3 Objective, Novelty, and Application

Objective

The main objective of this article is to improve the GNSS-Denied navigation capabilities of autonomous aircraft, so
in case GNSS signals become unavailable, they can continue their mission or safely fly to a predetermined recovery
location. To do so, the proposed approach combines two different navigation algorithms: an INS specifically
designed for the flight without GNSS signals of an autonomous fixed wing low SWaP aircraft [5] and a VNS that
relies on an advanced visual odometry pipeline such as SVO [40, 41]. Note that the INS makes use of all onboard
sensors except the camera, while the VNS relies exclusively on the images.

Each of the two systems by itself incurs in unrestricted and excessive horizontal position drift that renders them
inappropriate for long term GNSS-Denied navigation, but for different reasons: while in the INS the drift is
the result of integrating the bounded ground velocity estimations, that of the VNS originates on the slow but
continuous accumulation of estimation errors between consecutive frames. The two systems however differ in
their estimations of the aircraft attitude and altitude, as they are bounded for the INS but also drift in the case
of the VNS.

The proposed approach modifies the VNS so in addition to the images it can also accept as inputs the INS
bounded attitude and altitude outputs, converting it into an Inertially Assisted VNS or IA-VNS. The resulting
vastly improved horizontal position estimation outputs can be considered as the measurements of a virtual ground
velocity or incremental displacement sensor, which can in turn be fed back as observations to the INS.

Novelty

The VIO solutions listed in section 2 are quite generic with respect to the platforms on which they are mounted,
with most applications focused on ground vehicles, indoor robots, and multirotors, as well as with respect to
the employed sensors, which are usually restricted to the IMU (gyroscopes and accelerometers) and one or more
cameras. The existing solutions can also be classified into loosely or tightly coupled based on how they fuse

the incoming data, and into smoothers or filters according to the number of images considered in each estima-

tion.

This article focuses on an specific case (long distance GNSS-Denied turbulent flight of fixed wing low SWaP aircraft),
and as such is simultaneously more restrictive but also takes advantage of the sensors already onboard these
platforms, such as magnetometers, Pitot tube, and air vanes. In addition, and unlike the existing VIO packages,
the proposed solution assumes that GNSS signals are present at the beginning of the flight. As described in detail
in [5], these are key to the obtainment of the bounded attitude and altitude INS outputs on which the proposed
IA-VNS relies.

10

The proposed IA-VNS method represents a novel approach to diminish the pose drift of a VO pipeline by sup-
plementing its pose estimation non linear optimizations with priors based on the bounded attitude and altitude
outputs of a GNSS-Denied inertial filter. The method is inspired in a PI (Proportional Integral) control system, in
which the attitude, altitude, and rate of climb inertial outputs act as targets to ensure that the visual estimations

do not deviate far from their inertial counterparts, resulting in major reductions to not only the visual attitude

and altitude estimation errors, but also to the drift in horizontal position.

When the IA-VNS is treated as a virtual ground velocity or incremental horizontal displacement sensor and
its outputs (measurements) fed back into the inertial filter, the resulting VINS combines the high accuracy
representative of tightly coupled smoothers with the lack of complexity and reduced computing requirements of

filters and loosely coupled approaches. This difficulty in assigning the proposed fusion to either category is a

testament to the novelty of the approach:

• The existence of two independent solutions (inertial and visual) hints to a loosely coupled approach, but

in fact the proposed solution shares most traits with tightly coupled pipelines. First, no information is

discarded because the two solutions are not independent, as each simultaneously feeds and is fed by the
other. The visual estimations (IA-VNS) depend on the inertial attitude and altitude outputs, while the
inertial filter (VA-INS) uses the visual incremental horizontal position estimations as observations. Unlike
loosely coupled solutions, the visual estimations for attitude and altitude are not allowed to deviate from

the inertial ones above a certain threshold, so they do not drift. Additionally, the two estimations are never
fused together, as the final VINS output coincides with that of the VA-INS, as shown in figure 5.

• The proposed solution contains both a filter within the VA-INS (similar to the EKF described in [5]) and
a keyframe based sliding window smoother within the SVO based VA-INS, obtaining the best properties
of both categories. As proven in [5], a filter is sufficient to eliminate the drift in attitude and altitude;
these two outputs can then be provided as priors to the SVO based smoother (section 6), whose nonlinear
optimizations take into account all keyframes that share at least some terrain 3D points with the last frame

viewed by the camera in order to reduce the horizontal position drift. The integrated solution nevertheless
runs several times faster than real time because SVO relies on tracking features instead of matching, while
employing different threads for tracking and mapping.

Application

The main application of this work is the development of a virtual sensor capable of providing the VINS of
autonomous aircraft flying in GNSS-Denied conditions with periodic ground velocity or horizontal incremental
displacement observations, which results in major reductions to the horizontal position drift in the absence of
GNSS signals. In the case that GNSS signals become unavailable in mid flight, GNSS-Denied navigation is required
for the platform to complete its mission or return to base without the position and ground velocity observations
provided by GNSS receivers. The proposed system is not a full replacement of GNSS as it provides incremental
instead of absolute position observations, but as shown in the following sections, these can significantly increase

the possibilities of the aircraft safely reaching the vicinity of the intended recovery location, where it can be

landed by remote control.

4 High Fidelity Stochastic Simulation and Scenarios

To evaluate the performance of the proposed visual navigation algorithms, this article relies on Monte Carlo

simulations consisting of one hundred runs each of two different scenarios based on the high fidelity stochastic

flight simulation graphically depicted in figure 6. The simulation models the flight in varying weather and
turbulent conditions of a low SWaP fixed wing piston engine autonomous UAV, and has been specifically developed to
analyze the performances of different navigation systems in GNSS-Denied conditions. Described in detail in [7], the
simulation models the influence on the resulting aircraft trajectory of many different factors, such as the guidance

objectives that make up the mission, the atmospheric conditions, the wind field, the air turbulence, the local

11

perturbations to the Earth gravity and magnetic fields, the aircraft aerodynamic and propulsive performances,

its onboard sensors and their error sources, the control system that moves the throttle and the aerodynamic

controls so the trajectory conforms to the guidance objectives, and the navigation system that processes the

data obtained by the sensors and feeds the control system. The simulation also considers the asymmetric flight

conditions caused by the single engine torque and cross winds [76, 77], the influence of the fuel consumption

on the aircraft mass and inertial properties, as well as the relative position and orientation of the gyroscopes,

accelerometers, and camera with respect to the body frame and its origin, the aircraft center of mass. The open
source C++ implementation of the high fidelity simulation is available in [1].

xREF

GUIDANCE

𝛿TARGET

CONTROL

𝛿CNTR

AIRCRAFT

EARTH

ˆx = xEST

ˆx0

NAVIGATION

̃︀x = xSENSED

SENSORS

x = xTRUTH

FLIGHT

x = xTRUTH

Figure 6: Components of the high fidelity simulation

The simulation consists on two distinct processes. The first, represented by the yellow blocks on the right of figure

6, focuses on the physics of flight and the interaction between the aircraft and its surroundings that results in the
real aircraft trajectory x = xTRUTH; the second, represented by the green blocks on the left, contains the aircraft
systems in charge of ensuring that the resulting trajectory adheres as much as possible to the mission objectives. It
includes the different sensors whose output comprise the sensed trajectory
̃︀x = xSENSED, the navigation system in
charge of filtering it to obtain the estimated trajectory ˆx = xEST, the guidance system that converts the reference
objectives xREF into the control targets 𝛿TARGET, and the control system that adjusts the position of the throttle
and aerodynamic control surfaces 𝛿CNTR so the estimated trajectory ˆx is as close as possible to the reference
objectives xREF. As shown in the figure, the two parts of the simulation are not independent. The total system
error (TSE) or difference between x and xREF is the combination of the navigation system error (NSE) or difference
between x and ˆx and the flight technical error (FTE) or difference between ˆx and xREF. In the simulation, the
real trajectory is integrated at 500 Hz, the navigation system and sensors operate at 100 Hz (with the exception
of the camera, which works at 10 Hz), while the guidance and control systems work at 50 Hz [7] (table 1). Note
that the inertially estimated trajectory ˆx = xEST shown in figures 1, 2, 4, 5, and 6 has a frequency 10 times faster
than the visual trajectory ∘x shown in figures 3, 4, and 5.

500 Hz

Discrete Time Frequency Period
tt = t · ∆tTRUTH
ts = s · ∆tSENSED
tn = n · ∆tEST
tc = c · ∆tCNTR
ti = i · ∆tIMG

100 Hz

100 Hz

0.002 s

10 Hz

50 Hz

0.02 s

0.01 s

0.01 s

0.1 s

Variables

x = xTRUTH
̃︀x = xSENSED
ˆx = xEST

Systems
Flight physics

Sensors

Inertial navigation
𝛿TARGET, 𝛿CNTR Guidance & control

∘x = xIMG

Visual navigation & camera

Table 1: Working frequencies of the different systems and trajectory representations

All components of the simulation have been modeled with as few simplifications as possible to increase the realism

of the results, as explained in [7]. With the exception of the aircraft performances and its control system, which

are deterministic, all other simulation components are treated as stochastic and hence vary from one execution

to the next, enhancing the significance of the Monte Carlo simulation results. The representation of the different

errors introduced by the onboard sensors is of particular interest for inertial navigation. A detailed description

of the expressions that provide the measurement errors introduced by the various sensors is included in [8], as

well as the sensor performances values employed in the simulation.

12

Camera

This section summarizes the camera model employed on the simulation, but the reader should refer to [8] for

further details. The camera is positioned facing down and rigidly attached to the aircraft structure. Similarly

to the case of the inertial sensors, the navigation system needs to cope with certain uncertainties in the exact

position and attitude of the camera with respect to the aircraft structure.

It is assumed that the shutter speed is sufficiently high that all images are equally sharp, and that the image
generation process is instantaneous. In addition, the camera ISO setting remains constant during the flight, and
all generated images are noise free. The simulation also assumes that the visible spectrum radiation reaching all

patches of the Earth surface remains constant, and the terrain is considered Lambertian [48], so its appearance

at any given time does not vary with the viewing direction. The combined use of these assumptions implies that

a given terrain object is represented with the same luminosity in all images, even as its relative pose (position

and attitude) with respect to the camera varies. Geometrically, the simulation adopts a perspective projection or

pinhole camera model [48], which in addition is perfectly calibrated and hence shows no distortion. The camera
has a focal length of 19 mm and a sensor with 768 by 1024 pixels.

The camera model differs from all other onboard sensor models in that it does not return a sensed variable
̃︀x
consisting of its real value x plus a sensor error E, but instead generates a digital image simulating what a
real camera would record based on the aircraft position and attitude as given by the actual or real trajectory
x = xTRUTH. When provided with the camera pose with respect to the Earth at equally time spaced intervals,
the simulation is capable of generating images that resemble the view of the Earth surface that the camera would
record if located at that particular pose. To do so, it relies on the Earth Viewer library, a modification to
osgEarth [78] (which in turn relies on OpenSceneGraph [79]) capable of generating realistic Earth images as long
as the camera height over the terrain is significantly higher than the vertical relief present in the image. A more

detailed explanation of the image generation process is provided in [8].

Scenarios and Terrain Types

Most VIO packages discussed in section 2 include in their release articles an evaluation when applied to the EuRoC
Micro Air Vehicle (MAV) datasets [64], and so do independent articles such as [65]. These datasets contain perfectly
synchronized stereo images, IMU measurements, and ground truth readings obtained with laser, for eleven different
indoor trajectories flown with a MAV, each with a duration in the order of two minutes and a total distance in
the order of 100 m. This fact by itself indicates that the target application of exiting VIO implementations differs
significantly from the main focus of this article, which is the long term flight of a fixed wing UAV in GNSS-Denied
conditions, as there may exist accumulating errors that are completely non discernible after such short periods

of time, but that grow non linearly and have the capability of inducing significant pose errors when the aircraft

remains aloft for long periods of time.

The algorithms introduced in this article are hence tested through simulation under two different scenarios
designed to analyze the consequences of losing the GNSS signals for long periods of time. Although a short
summary is included below, detailed descriptions of the mission, weather, and wind field employed in each

scenario can be found in [7]. Most parameters comprising the scenario are defined stochastically, resulting in

different values for every execution. Note that all results shown in sections 7, 8, and 9 are based on Monte Carlo

simulations comprising one hundred runs of each scenario, testing the sensitivity of the proposed navigation

algorithms to a wide variety of values in the parameters.

• Scenario #1 has been defined with the objective of adequately representing the challenges faced by an
autonomous fixed wing UAV that suddenly cannot rely on GNSS and hence changes course to reach a pre-
defined recovery location situated at approximately one hour of flight time. In the process, in addition to

executing an altitude and airspeed adjustment, the autonomous aircraft faces significant weather and wind
field changes that make its GNSS-Denied navigation even more challenging.

13

With respect to the mission, the stochastic parameters include the initial airspeed, pressure altitude, and
bearing (vTAS,INI, HP,INI, 𝜒INI), their final values (vTAS,END, HP,END, 𝜒END), and the time at which each of the
three maneuvers is initiated12. The scenario lasts for tEND = 3800 s, while the GNSS signals are lost at
tGNSS = 100 s, which does not involve any loss of generality as the accuracy of the aircraft pose (attitude
and position) estimation does not degrade with time when GNSS signals are available.

The wind field is also defined stochastically, as its two parameters (speed and bearing) are constant both
at the beginning (vWIND,INI, 𝜒WIND,INI) and conclusion (vWIND,END, 𝜒WIND,END) of the scenario, with a linear
transition in between. The specific times at which the wind change starts and concludes also vary stochas-

tically among the different simulation runs. As described in [7], the turbulence remains strong throughout

the whole scenario, but its specific values also vary stochastically from one execution to the next.

A similar linear transition occurs with the temperature and pressure offsets that define the atmospheric
properties [6], as they are constant both at the start (∆TINI, ∆pINI) and end (∆TEND, ∆pEND) of the flight.
In contrast with the wind field, the specific times at which the two transitions start and conclude are not

only stochastic but also different from each other.

• Scenario #2 represents the challenges involved in continuing with the original mission upon the loss of the
GNSS signals, executing a series of continuous turn maneuvers over a relatively short period of time with
no atmospheric or wind variations. As in scenario #1, the GNSS signals are lost at tGNSS = 100 s, but the
scenario duration is shorter (tEND = 500 s). The initial airspeed and pressure altitude (vTAS,INI, HP,INI) are
defined stochastically and do not change throughout the whole scenario; the bearing however changes a

total of eight times between its initial and final values, with all intermediate bearing values as well as the

time for each turn varying stochastically from one execution to the next. Although the same turbulence
is employed as in scenario #1, the wind and atmospheric parameters (vWIND,INI, 𝜒WIND,INI, ∆TINI, ∆pINI)
remain constant throughout scenario #2.

The type of terrain overflown by the aircraft has a significant influence in the performance of the visual navigation

algorithms. In particular, the terrain texture (or lack of) and its elevation relief are the two most important

characteristics in this regard. For this reason, each of the scenario #1 one hundred Monte Carlo simulation runs

can be combined with four different initial geodetic coordinates, so the resulting flights occur over four different

zones or types of terrain. The zones described below are intended to represent a wide array of terrain types13.

Note that changing the area of the world were the flight takes place also modifies the gravitational and magnetic

fields, but as all considered zones are far from the Earth magnetic poles, this does not imply any significant

change in the performance of the inertial algorithms [5]. Images representative of each zone as viewed by the

onboard camera are included below.

• The “desert” (DS) zone is located in the Sonoran desert of southern Arizona (USA) and northern Mexico.
It is characterized by a combination of bajadas (broad slopes of debris) and isolated very steep mountain

ranges. There is virtually no human infrastructure or flat terrain, as the bajadas have sustained slopes of
up to 7∘. The altitude of the bajadas ranges from 300 to 800 m above MSL, and the mountains reach up to
800 m above the surrounding terrain. Texture is abundant because of the cacti and the vegetation along
the dry creeks.

• The “farm” (FM) zone is located in the fertile farmland of southeastern Illinois and southwestern Indiana
(USA). A significant percentage of the terrain is made of regular plots of farmland, but there also exists
some woodland, farm houses, rivers, lots of little towns, and roads. It is mostly flat with an altitude above
MSL between 100 and 200 m, and altitude changes are mostly restricted to the few forested areas. Texture
is nonexistent in the farmlands, where extracting features is often impossible.

12Turns are executed with a bank angle of 𝜉TURN = ±10 ∘, altitude changes employ an aerodynamic path angle of

𝛾TAS,CLIMB = ±2 ∘, and airspeed modifications are automatically executed by the control system as set-point changes.

13Alpine terrain with elevated vertical relief is not included as piston engines aircraft can not fly high enough for the previously

described Earth Viewer tool to generate realistic images in this type of terrain.

14

Figure 7: Typical “desert” (DS) terrain view

Figure 8: Typical “farm” (FM) terrain view

• The “forest” (FR) zone is located in the deciduous forestlands of Vermont and New Hampshire (USA). The
terrain is made up of forests and woodland, with some clearcuts, small towns, and roads. There are virtually

no flat areas, as the land is made up by hills and small to medium size mountains that are never very steep.
The valleys range from 100 to 300 m above MSL, while the tops of the mountains reach 500 to 900 m. Features
are plentiful in the woodlands.

Figure 9: Typical “forest” (FR) terrain view

• The “mix” (MX) zone is located in northern Mississippi and extreme southwestern Tennessee (USA). Approx-
imately half of the land consists of woodland in the hills, and the other half is made up by farmland in the

valleys, with a few small towns and roads. Altitude changes are always presents and the terrain is never
flat, but they are smaller than in the DS and FR zones, with the altitude oscillating between 100 and 200 m
above MSL.

The short duration and continuous maneuvering of scenario #2 enables the use of two additional terrain types.

These two zones are not employed in scenario #1 because the authors could not locate wide enough areas with a

15

Figure 10: Typical “mix” (MX) terrain view

prevalence of this type of terrain (note that scenario #1 trajectories can conclude up to 125 km in any direction
from its initial coordinates, but only 12 km for scenario #2).

• The “prairie” (PR) zone is located in the Everglades floodlands of southern Florida (USA). It consists of flat
grasslands, swamps, and tree islands located a few meters above MSL, with the only human infrastructure
being a few dirt roads and landing strips, but no settlements. Features may be difficult to obtain in some

areas due to the lack of texture.

Figure 11: Typical “prairie” (PR) terrain view

• The “urban” (UR) zone is located in the Los Angeles metropolitan area (California, USA). It is composed by
a combination of single family houses and commercial buildings separated by freeways and streets. There
is some vegetation but no natural landscapes, and the terrain is flat and close to MSL.

Figure 12: Typical “urban” (UR) terrain view

The MX terrain zone is considered the most generic and hence employed to evaluate the visual algorithms in
sections 7 and 9, as well as the inertial ones in [5]; the remaining terrain zones are only used to test the sensitivity

16

of the proposed algorithms to the terrain type, as described in section 8. Although scenario #2 also makes use
of the four terrain types listed for scenario #1 (DS, FM, FR, and MX), it is worth noting that the variability of the
terrain is significantly higher for scenario #1 because of the bigger land extension covered. The altitude relief,

abundance or scarcity of features, land use diversity, and presence of rivers and mountains is hence more varied

when executing a given run of scenario #1 on a certain type of terrain, than when executing the same run for

scenario #2. From the point of view of the influence of the terrain on the visual navigation algorithms, scenario

#1 should theoretically be more challenging than #2.

5 Semi-Direct Visual Odometry

As introduced in section 2, Semi-Direct Visual Odometry (SVO) [40, 41] is a publicly available advanced com-
bination of feature-based and direct VO techniques primarily intended towards the navigation of land robots,
road vehicles, and multirotors, holding various advantages in terms of accuracy and speed over traditional VO
algorithms. By combining the best characteristics of both approaches while avoiding their weaknesses, it obtains

high accuracy and robustness with a limited computational budget. This section provides a short summary of
the SVO pipeline, although the interested reader should refer to [40, 41] for a more detailed description; the pose
optimization phase is however described in depth, as it is the focus of the proposed modifications described in

section 6.

SVO initializes like a feature-based monocular method, requiring the height over the terrain to provide the scale
(initialization), and using feature matching and RANSAC based triangulation (initial homography) to obtain a first
estimation of the terrain 3D position of the identified features. After initialization, the SVO pipeline for each new
image can be divided into two different threads: the mapping thread, which generates terrain 3D points, and the
motion thread, which estimates the camera motion.

Once initialized, the expensive feature detection process (mapping thread) that obtains the features does not

occur in every frame but only once a sufficiently large motion has occurred since the last feature extraction.
When processing each new frame, SVO initially behaves like a direct method, discarding the feature descriptors
and skipping the matching process, and employing the luminosity values of small patches centered around every

feature to (i) obtain a rough estimation of the camera pose (sparse image alignment, motion thread), followed

by (ii) a relaxation of the epipolar restrictions to achieve a better estimation of the different features sub-pixel

location in the new frame (feature alignment, motion thread), which introduces a reprojection residual that is
exploited in the next steps. At this point, SVO once again behaves like a feature-based method, refining (iii)
the camera pose (pose optimization, motion thread) and (iv) the terrain coordinates of the 3D points associated

to each feature (structure optimization, motion thread) based on nonlinear minimization of the reprojection

error.

In this way, SVO is capable of obtaining the accuracy of direct methods at a very high computational speed,
due to only extracting features in selected frames, avoiding (for the most part) robust algorithms when tracking
features, and only reconstructing the structure sparsely. The accuracy of SVO improves if the pixel displacement
between consecutive frames is reduced (high frame rate), which is generally possible as the computational expenses

associated to each frame are low.

None of the motion thread four nonlinear optimization processes listed above makes use of RANSAC, and pose
optimization is the only one that employs a robust M-estimator [80, 81] instead of the traditional mean or

squared error estimator. This has profound benefits in terms of computational speed but leaves the whole

process vulnerable to the presence of outliers in either the features terrain or image positions. To prevent

this, once a feature is detected in a given frame (note that the extraction process obtains pixel coordinates,

not terrain 3D ones), it is immediately assigned with a depth filter (mapping thread) initialized with a large

enough uncertainty around the average depth in the scene; in each subsequent frame, the feature 3D position is

estimated by reprojection and the depth filter uncertainty reduced. Once the feature depth filter has converged,

the detected feature and its associated 3D point become a map candidate, which it is not yet employed in the

17

AUXILIARY

MAPPING THREAD

MOTION THREAD

1. Initialization

1. Feature Detection

1. Sparse Image Alignment

2. Initial Homography

2. Mapping

2. Feature Alignment

3. Depth Filter

3. Pose Optimization

4. Feature Alignment

4. Structure Optimization

Figure 13: SVO threads and processes

motion thread optimizations required to estimate the camera pose. The feature alignment process is however

applied in the background to the map candidates, and it is only after several successful reprojections that a

candidate is upgraded to a map 3D point and hence allowed to influence the motion result. This two step

verification process that requires depth filter convergence and various successful reprojections before a 3D point

is employed in the (mostly) non-robust optimizations is key to prevent outliers from contaminating the solution

and reducing its accuracy.

Pose Optimization

As the focus of the proposed improvements within section 6, the pose optimization phase is the only one described

in detail in this article. Graphically depicted in figure 14, pose optimization is executed for every new frame
i and makes use of the different features and associated 3D points j present in the map to generate the visual
estimation of the pose between the ECEF (FE) and camera (FC) frames ∘

.

𝜁 ECi

The previous feature alignment process improved the projection of the feature points into the current image14

by minimizing the photometric error while ignoring their epipolar constraints, which enables fine tuning

pIMG
ij
the camera pose ∘
𝜁⋆
ECi
different terrain 3D points pE
j
previous frame. This process is known as pose refinement or motion-only bundle adjustment [40].

provided by the sparse image alignment process by minimizing the reprojection error of the

present in the map, which originate at the structure optimization process of the

MOTION THREAD Frame i

SPARSE IMAGE ALIGNMENT

∘

𝜁 ECi0 =

∘

𝜁⋆
ECi

pE
j

FEATURE ALIGNMENT

pIMG
ij

POSE

OPTIMIZATION
Frame i
∑︁

j

∘

𝜁 ECi

Figure 14: Pose optimization flow diagram

The reprojection error ERP is defined in (1) as the sum for each 3D point of the norm of the difference between the
camera projection Π of the ECEF coordinates pE
transformed into the camera frame15 and the image coordinates
j
pIMG
j

.

ERP

)︁

(︁ ∘

𝜁 ECi

=

∑︁

j

⃦
⃦
Π(︀g−1
⃦
⃦

∘

𝜁 ECi

(pE

j ))︀ − pIMG

ij

⃦
⃦
⃦
⃦

(1)

This problem can be solved by means of an iterative Gauss-Newton gradient descent process [82]. Given an initial
pose estimation ∘
), each iteration step l minimizes (2) and

taken from the sparse image alignment result ( ∘
𝜁⋆
ECi

𝜁 ECi0

14The image frame FIMG is a 2D reference frame centered at the sensor corner, as depicted in figure 32.
15g𝜁AB
() represents the SE(3) transformation of a point from frame B to frame A, as described in [2].

18

advances the estimated solution by means of (3) until the step diminution of the reprojection error falls below a
given threshold (ERP,i,l − ERP,i,l+1 < 𝛿RP):

ERPi,l+1 (∆ ∘𝜏 C

ECil) =

∘

𝜁 ECi,l+1 ←−

∑︁

⃦
⃦
Π(︀g−1
⃦
⃦

∘

∘

j
𝜁 ECil ∘ Exp (∆ ∘𝜏 C

𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

ECil) =

∘

𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

(pE

j ))︀ − pIMG

ij

⃦
⃦
⃦
⃦

=

∑︁

⃦
⃦ERPi,l+1,j (∆ ∘𝜏 C
⃦

⃦
⃦
ECil)
⃦

j

(2)

(3)

ECil

Each ∆ ∘𝜏 C
represents the update to the camera pose given by the transform vector [2, 4] viewed in the local
camera frame FCil, which is obtained by following the process described in [82], and results in (4), where JRP,ilj
(5) represents the optical flow obtained in appendix A:

∆ ∘𝜏 C

ECil = −

[︂ ∑︁

JRP,ilj

T JRP,ilj

]︂−1

∑︁

T [︁

JRP,ilj

j

(︃

∘

g−1
𝜁 ECil

+ Π

⊕ ℳ

)︃

)︀

(︀pE

j

j

∈ R2x6

JRP,ilj = J

Π(︀g−1

∘

𝜁 ECil

(pE

j ))︀ − pIMG

ij

]︁

∈ R6

(4)

(5)

In order to protect the resulting pose from the possible presence of outliers in either the 3D points pE
j
image projection pIMG
estimator, such as the bisquare or Tukey estimator [80, 81]. The error to be minimized in each iteration step is
then given by (6), where the Tukey error function 𝜚TUK (x) = 𝜌TUK

, it is better to replace the above squared error or mean estimator by a more robust M-

x)︀ can be found in [81].

or their

(︀√

ij

ERPi,l+1 (∆ ∘𝜏 C

ECil) =

=

∑︁

j
∑︁

j

𝜚TUK

(︃
[︁
Π(︀g−1

∘

𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

(pE

j ))︀ − pIMG

ij

(︁

𝜚TUK

ERPi,l+1,j

T ERPi,l+1,j

)︁

]︁T[︁

Π(︀g−1

∘

𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

(pE

j ))︀ − pIMG

ij

)︃
]︁

(6)

A similar process to that employed above leads to the solution (7), where the Tukey weight function wTUK (x) is
provided by [81]:

∆ ∘𝜏 C

ECil = −

[︂ ∑︁

(︁

wTUK

ERP,ilj

TERP,ilj

)︁

JRP,ilj

TJRP,ilj

]︂−1

j

[︂ ∑︁

j

(︁

wTUK

ERP,ilj

TERP,ilj

)︁

JRP,ilj

TERP,ilj

]︂

∈ R6

ERP,ilj = Π(︀g−1

∘

𝜁 ECil

(pE

j ))︀ − pIMG

ij

∈ R2

(7)

(8)

Initialization

𝜁 EC0

The SVO based VNS depicted in figure 3 requires an estimation of the visual state ∘x0 for initialization, this is,
the ∘
pose between the ECEF and camera FC frames. As in the case of the initialization of the GNSS-Denied
INS described in [5], it is possible to rely exclusively on the output of the GNSS-Based INS before the loss of the
GNSS signals. The only difference between both cases is that the GNSS-Denied INS filters are initialized at the
exact moment that the GNSS signals are lost (tGNSS), while the IA-VNS shall be periodically reinitialized during
GNSS-Based conditions to minimize the inevitable accumulation of errors with time. Note however that during
the simulation scenarios described in section 4, the VNS is only initialized once at t = 0 s.

The

∘

initial pose
𝜁 EC0
( ∘
TE,GDT
0

can be

obtained from the

geodetic

coordinates provided by the posi-

= ˆTGNSS−BASED,LAST),

tion filter
[5]
( ∘qNB0 = ˆqGNSS−BASED,LAST
In addition to ∘
, monocular SVO also requires the initial distance (depth) of the camera optical center with
respect to the terrain ( ˆHHEIGHT0) to provide the scale of the initial homography. The SVO initialization applies
feature detection and matching to two images, considers that all terrain 3D points viewed in both images are

), and the pose between the body (FB) and camera (FC) frames ∘

the body attitude estimated by the attitude filter

𝜁 BC0

𝜁 EC0

[5]

.

located on a flat surface (which does not need to be horizontal), and then solves the corresponding homography

19

employing RANSAC to obtain the direction normal to the terrain plane, the rotation between the two camera
frames, and the (up to scale) translation between the two camera frames.

These two images are not consecutive, as a sufficiently high optical flow between them is required to improve

the conditioning of the homography solution and hence the consistency of the results. The homography hence
is only solved once the aircraft pose has accumulated enough change, and as long as the scale is provided as an
input, it provides the pE
j ECEF terrain coordinates of the features, their pIMG
frames (which become the first two keyframes of the SVO motion thread), and the relative pose ∘
both keyframes. This relative pose is then combined with the input ∘
obtain the absolute pose of the second keyframe, ∘
on SVO proceeds without performing any additional feature matching.

. The initialization is then complete, and from that point

absolute pose of the first keyframe to

image coordinates in both

and pIMG

between

𝜁 C0C1

𝜁 EC1

𝜁 EC0

j1

j0

A key point of the homography solution is that while the normal to the terrain plane and the relative rotation

between the two keyframes are independent of the input depth that provides the scale, the translation between

the two keyframes is proportional to it [83] (the homography solution provides the direction of that translation,

which needs to be multiplied by the scale).

The homography scale, depth, height, or distance from the camera optical center of the first keyframe to the
Earth surface ( ˆHHEIGHT0) can hence be obtained by linear interpolation between any two arbitrary input heights
to ensure that the homography estimation of the distance traveled by the camera between the two keyframes
C0C1‖),
(‖
which is easily obtained from their respective estimated geodetic coordinates.

, coincides with that estimated by the GNSS-Based inertial system (‖ ˆTC0

C0C1‖), which is part of ∘

𝜁 C0C1

TC0

∘

6 Proposed Inertially Assisted Semi-Direct Visual Odometry

Lacking any absolute references, the SVO based VNS described in section 5 gradually accumulates errors in each
of the six dimensions of the vehicle pose ∘
If accurate estimations of attitude and
altitude can be provided by an INS such as that described in [5], these can be employed to ensure that the
and ∘
visual estimations for body attitude and vertical position ( ∘qNB
h) do not deviate in excess from their inertial
and ˆh, so they too can be considered as bounded up to a certain degree. The modified SVO is
counterparts ˆqNB
referred to as IA-SVO (Inertially Assisted SVO).

, as shown in section 7.

𝜁 EB

Note that the inertial estimations should not replace the visual ones within SVO, as this would destabilize the
visual pipeline preventing its convergence, but just act as anchors so the visual estimations oscillate freely as a
result of the multiple SVO optimizations but without drifting too far from the vicinity of the anchors. This section
shows how to modify the cost function within the iterative Gauss-Newton gradient descent pose optimization
phase so it can take advantage of the INS outputs.

MOTION THREAD Frame i

INERTIAL POSE ESTIMATION

∘

𝜁 ECi0 =

∘

𝜁⋆
ECi

pE
j

ˆqNBn

ˆhn

PRIOR BASED

POSE

OPTIMIZATION
Frame i
∑︁

∘

𝜁 ECi

FEATURE ALIGNMENT

pIMG
ij

j

Figure 15: Prior based pose optimization flow diagram

20

Rationale for the Introduction of Priors

The prior based pose optimization process starts by executing exactly the same pose optimization described in
section 5, which seeks to obtain the camera pose ∘
that minimizes the reprojection error ERP (1). The iterative
optimization results in a series of transform vector updates ∆ ∘𝜏 C
(7), where l indicates the iteration step. The
camera pose is then advanced per (3) until the step diminution of the reprojection error falls below a certain

𝜁 ECi

ECil

threshold.

The resulting camera pose, ∘
solution, resulting in ∘
the reprojected body pose16 ∘

𝜁⋆⋆
ECi

∘

𝜁⋆⋆
EBi =

∘

𝜁⋆⋆
ECi ⊗

∘

𝜁*

BC

𝜁 ECi

, is marked with the superindex ⋆⋆ to indicate that it is the reprojection only
obtained in calibration results in

. Its concatenation with the body to camera pose ∘

𝜁 BC

𝜁⋆⋆
EBi

:

(9)

and cartesian coordinates ∘

The reprojected ECEF body attitude ∘q⋆⋆
EBi
[︁ ∘
leads on one hand to the reprojected body attitude ∘q⋆⋆
𝜓⋆⋆
NBi
i
and on the other to the geodetic coordinates ∘
TE,GDT⋆⋆
. Considering a cartesian
reference frame FX with axes parallel to those of FNi but with origin located on the WGS84 ellipsoid surface (h = 0)
at the longitude and latitude given by ∘
is easily derived from

TB⋆⋆
EBi
, equivalent to the Euler angles ∘

can then be readily obtained, which
]︁T

and ECEF to NED rotation ∘q⋆⋆
ENi

, the displacement ∘

NBi =

𝜑⋆⋆

𝜃⋆⋆
i

𝜉⋆⋆
i

]︁T

[︁

,

,

,

∘

∘

∘

i

TX⋆⋆

XBi =

0, 0, −

h⋆⋆
i

TE,GDT⋆⋆

i

∘

. It is also possible to derive the relative pose between FX and the ECEF frame, noted as ∘

TE,GDT⋆⋆
different situations (compatible with each other) can be envisioned:

i

𝜁 XEi

. Two

• Let’s consider that the estimated body attitude provided by the INS (ˆqNBn

) [5] enables the IA-VNS to
conclude that it would be preferred if the optimized body attitude were closer to a certain target attitude

, equivalent to the target Euler angles ∘

[︁ ∘
identified by the superindex ∘∘, ∘q∘∘
𝜓∘∘
𝜃∘∘
,
i
i
NBi
are provided below.
alternatives to obtain the target attitude from the inertially estimated attitude ˆqNBn
The target body attitude is converted into a target camera attitude by means of ˆqBC ≡ ˆ𝜑BC and the original
reprojected ECEF to NED rotation ∘q⋆⋆
, incurring in a negligible error by not considering the attitude change
ENi
of the NED frame as the iteration progresses.

. Various

NBi =

𝜑∘∘

𝜉∘∘
i

]︁T

,

∘

∘

∘q∘∘
ECi = ∘q⋆⋆

ENi ⊗ ∘q∘∘

NBi ⊗ ˆqBC

(10)

Note that the objective is not for the resulting body attitude ∘qNBi
both objectives (minimization of the reprojection error of the different terrain 3D points and minimization of
the incremental rotation between the resulting camera attitude ∘qECi
any hard constraints on the pose (position plus attitude) of the aircraft.

to equal the target ∘q∘∘
NBi

and the target ∘q∘∘
ECi

) without imposing

, but to balance

• Let’s also consider that the estimated geometric altitude provided by the INS (ˆhn) [5] enables the IA-VNS to
conclude that it would be preferred if the optimized aircraft altitude were closer to a certain target altitude
∘

[︁
. The obtainment of the target altitude from
h∘∘
0, 0, −
i
the inertially estimated altitude ˆhn is also discussed below. A negligible error occurs by not considering the
motion of the FX origin over the ellipsoid as the iteration progresses.

, equivalent to a target displacement ∘

XBi =

TX∘∘

h∘∘
i

]︁T

∘

∘

In this case the objective is not to set the resulting geodetic coordinates ∘
TE,GDT∘∘
points, without imposing any hard constraints on the aircraft pose.

i

, but to balance that objective with the minimization of the reprojection error of the different 3D

TE,GDT

i

to equal the target ones

Prior Based Pose Optimization

The following bullets describe how to optimize for the above two objectives independently or concurrently:

16Note that a single asterisk superindex < ·* > applied to a quaternion refers to its conjugate or inverse.

21

• Attitude Adjustment Optimization. The attitude adjustment error Eq is defined in (11) as the norm of the
and the target

Euclidean difference between rotation vectors corresponding to the camera attitude ∘qECi
camera attitude ∘q∘∘
ECi

[2, 4]:

Eq

⃦
(︀Log ( ∘qECi) )︀ = Eq ( ∘rECi) =
⃦Log ( ∘qECi) − Log ( ∘q∘∘
⃦
ECi)

⃦
⃦
∘rECi − ∘r∘∘
⃦
⃦
⃦ =
⃦
ECi

⃦
⃦
⃦

(11)

Its minimization can be solved by means of an iterative Gauss-Newton gradient descent process [82]. Given
an initial attitude estimation ∘rECi,0 = Log (︀ ∘qECi,0
, each iteration step l minimizes
(12) and advances the estimated solution by means of (13) until the step diminution of the attitude adjust-
ment error falls below a given threshold (Eq,i,l − Eq,i,l+1 < 𝛿q):
⃦
)︀ − Log ( ∘q∘∘
⃦Log(︀ ∘qECil ⊕ ∆ ∘rC
⃦
⃦
⃦Log(︀Exp ( ∘rECil) ⊕ ∆ ∘rC
⃦

)︀ taken from ∘

Eq,i,l+1 (∆ ∘rC

ECil) =

𝜁 ECi0 =

𝜁⋆
ECi

(12)

ECil

=

∘

⃦
⃦
ECi)
⃦
⃦
)︀ − ∘r∘∘
⃦
⃦
ECi
ECil
ECil) = ∘qECil ⊕ ∆ ∘rC

ECil

∘qECi,l+1 ←− ∘qECil ∘ Exp (∆ ∘rC

(13)

ECil

Each ∆ ∘rC
represents the update to the camera attitude given by the rotation vector viewed in the local
camera frame FCl, which is obtained by following the process described in [82]17, and results in (14), where
JRil is the SO(3) right jacobian JR (r) provided by [2, 4] 18.

∆ ∘rC

ECil = −

[︁
Ril J−1
J−T

Ril

]︁−1

J−T
Ril

[︀ ∘rECil − ∘r∘∘

ECi

]︀

= −

[︁
Ril J−1
J−T

Ril

]︁−1

= −

[︁
Ril J−1
J−T

Ril

]︁−1

JRil = JR ( ∘rECil) = JR

J−T
Ril

[︀Log(︀ ∘qECil

)︀ − Log(︀ ∘q∘∘

ECi

)︀]︀

J−T
Ril Eq,il ∈ R3
(︁

)︁

Log ( ∘qECil)

∈ R3x3

(14)

(15)

• Altitude Adjustment Optimization. The altitude adjustment error Eh is defined in (16) as the difference
. Note that its obtainment relies

between the aircraft center of mass altitude ∘
on a double transformation of the FC to FB position vector ∘
TC

hi and the target altitude ∘

h∘∘
i

:

CB

)︁

(︁ ∘

𝜁 ECi

=

Eh

∘

TX

XBi,3 −

∘

TX∘∘

XBi,3 = −

∘

hi +

∘

h∘∘
i =

[︂

g ∘
𝜁 XEi

(︁

g ∘
𝜁 ECi

)︀)︁]︂

(︀ ∘
TC

CB

+

∘

h∘∘
i

3

(16)

Its minimization is also solved by means of an iterative Gauss-Newton gradient descent process [82]. Given
an initial pose estimation ∘
, each iteration step l minimizes (17) and advances the estimated
solution by means of (18) until the step diminution of the altitude adjustment error falls below a given
threshold (Eh,i,l − Eh,i,l+1 < 𝛿h). Note that the subindex 3* stands for the corresponding matrix 3rd row.

𝜁 ECi,0 =

𝜁⋆
ECi

∘

Eh,i,l+1 (∆ ∘𝜏 C

ECil) =

=

=

∘

𝜁 ECi,l+1 ←−

[︂

(︁

g ∘
𝜁 XEi

[︂ ∘
TX

XEi +

RXEi

g ∘
𝜁 ECil ⊕ ∆ ∘𝜏 C
[︁

∘

ECil

)︀)︁]︂

(︀ ∘
TC

CB

+

∘

h∘∘
i

3

)︀]︁]︂

(︀ ∘
TC

CB

3
)︀]︁

(︀ ∘
TC

CB

g ∘
𝜁 ECil ⊕ ∆ ∘𝜏 C
[︁
g ∘
𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

ECil

∘

TX

XEi,3 +

∘

RXEi,3*

∘

𝜁 ECil ∘ Exp (∆ ∘𝜏 C

ECil) =

∘

𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

+

∘

h∘∘
i

+

∘

h∘∘
i

(17)

(18)

ECil

represents the update to the camera pose given by the transform vector viewed in the local

Each ∆ ∘𝜏 C
camera frame FCl [82], and results in (19), where Jh,il is the matrix product between the 3rd row of the ∘
rotation matrix and the J

jacobian [2, 4], which represents the derivative of a transformed

RXEi

+ gExp(𝜏 )(p)
+ 𝜏

17Note that in this process the jacobian coincides with the identity matrix because the map f (︀∘rECi

)︀ = ∘rECi coincides with the

rotation vector itself.

18[2, 4] also include an expression for the right jacobian inverse (J−1

Ril = J−1

R (r)).

22

point with respect to perturbations in the Euclidean tangent space (not on the curved manifold) that

generates the motion.

∆ ∘𝜏 C

ECil = −

= −

[︁

[︁

JT
h,il Jh,il

JT
h,il Jh,il

]︁−1

]︁−1

[︂ ∘
TX

XEi,3 +

∘

RXEi,3*

[︁

JT
h,il

)︀]︁

(︀ ∘
TC

CBi

+

∘

h∘∘
i

]︂

g ∘
𝜁 ECil

JT
h,il Eh,il ∈ R6

Jh,il =

∘

RXEi,3* J

+ g ∘

+ 𝜏

𝜁 ECil

∘

TC
(

CBi)

∈ R1x6

(19)

(20)

𝜁 ECi

• Joint Optimization. The prior based pose adjustment algorithm attempts to obtain the ECEF camera
pose ∘
that minimizes the reprojection error ERP discussed in section 5 combined with the weighted
attitude and altitude adjustment errors (Eq and Eh) introduced above. The specific weight values fq and
fh are discussed below. Inspired in [84], the main goal of the optimization algorithm is to minimize the
reprojection error of the different terrain 3D points while simultaneously trying to be close to the attitude

and altitude targets derived from the inertial filter.

EPO

)︁

(︁ ∘

𝜁 ECi

= ERP

)︁

(︁ ∘

𝜁 ECi

+ fq · Eq

(︀ ∘rECi

)︀ + fh · Eh

)︁

(︁ ∘

𝜁 ECi

(21)

Although the rotation vector ∘rECi = Log ( ∘qECi) can be directly obtained from the pose ∘
𝜁 ECi
the three algorithms requires a dimension change in the (15) jacobian, as indicated by (22).

[2, 4], merging

[︁

J−1
RRil =

O3x3 J−1
Ril

]︁

∈ R3x6

(22)

The application of the iterative process described in [84] results in the following solution, which combines

the contributions from the three different optimization targets:

HPO,il =

[︁ ∑︁

(︁

wTUK

ERP,ilj

TERP,ilj

)︁

JRP,ilj

TJRP,ilj

]︁

j

+f 2
q ·

RRil

[︁
RRil J−1
J−T
[︂ ∑︁
⎡
⎢
⎣

j

]︁

+ f 2
h ·

[︁
JT
h,il Jh,il

]︁

∈ R6x6

wTUK

(︁

ERP,ilj

TERP,ilj

)︁

JRP,ilj

TERP,ilj

∆ ∘𝜏 C

ECil = −H−1

PO,il

∘

𝜁 ECi,l+1 ←−

∘

𝜁 ECil ∘ Exp (∆ ∘𝜏 C

ECil) =

+fq · J−T

RRil Eq,il + fh · JT
𝜁 ECil ⊕ ∆ ∘𝜏 C

ECil

∘

h,il Eh,il

]︂

⎤

⎥
⎦

(23)

(24)

(25)

PI Control Inspired Pose Adjustment Activation

The previous paragraphs describe the possible attitude and altitude adjustments and their fusion with the default

reprojection error minimization pose optimization algorithm, but they do not specify the conditions under which
and ∘
the adjustments are activated, how the ∘q∘∘
targets are determined, or the obtainment of their
fq and fh relative weights when applying the (21) joint optimization. These parameters are determined below in
four different cases: an attitude adjustment in which only pitch is controlled, an attitude adjustment in which

NBi ≡

𝜑∘∘
NBi

h∘∘
i

∘

both pitch and bank angles are controlled, a complete attitude adjustment, and an altitude adjustment.

• Pitch Adjustment Activation. The attitude adjustment described in (11) through (15) can be converted
into a pitch only adjustment by forcing the yaw and bank angle targets to coincide in each optimization
step i with the output of the reprojection only optimization:

∘

∘

𝜓∘∘
i
𝜃∘∘
i
𝜉∘∘
i

∘

=

=

=

∘

∘

𝜓⋆⋆
i
𝜃⋆⋆
i + ∆
𝜉⋆⋆
i

∘

∘

𝜃∘∘
i

(26)

(27)

(28)

When activated as explained below, the new body pose target ∘
(the pitch) from the reprojection only ERP optimum pose ∘

𝜁⋆⋆
EBi

only differs in one out of six dimensions

𝜁∘∘
EBi
, and the difference is very small as its effects

23

are intended to accumulate over many successive images. This does not mean however that the other five

components do not vary, as the joint optimization process described in (21) through (25) freely optimizes
within SE (3) with six degrees of freedom to minimize the joint cost function EPO that not only considers
the reprojection error, but also the resulting pitch target.

hi and pitch ∘

The pitch adjustment aims for the visual estimations for altitude ∘
𝜃i (in this order) not to
deviate in excess from their inertially estimated counterparts ˆhn and ˆ𝜃n. It is inspired in a proportional
hi − ˆhn can
integral (PI) control scheme [85, 86, 87, 88] as the geometric altitude adjustment error 19 ∆h =
𝜃i − ˆ𝜃n in the sense that any difference
be considered as the integral of the pitch adjustment error ∆𝜃 =
between adjusted pitch angles (the P control) slowly accumulate over time generating differences in adjusted
altitude (the I control). In addition, the adjustment also depends on the rate of climb (ROC) adjustment
ROCi − ˆROCn, which can be considered a second P control as ROC is the time derivative
error20 ∆ROC =
of the pressure altitude.

∘

∘

∘

Note that the objective is not for the visual estimations to closely track the inertial ones, but only to avoid
excessive deviations, so there exist lower thresholds ∆hLOW, ∆𝜃LOW, and ∆ROCLOW below which the
adjustments are not activated. These thresholds are arbitrary but have been set taking into account the
INS accuracy and its sources of error, as described in [5]. If the absolute value of a certain adjustment
error (difference between the visual and estimated estates) is above its threshold, the IA-VNS can conclude
with a high degree of confidence that the adjustment procedure can be applied; if below the threshold,

the adjustment should not be employed as there is a significant risk that the true visual error (difference

between the visual and actual state) may have the opposite sign, in which case the adjustment would be

counterproductive.

Variable
∆hLOW,GNSS−BASED
∆hLOW,GNSS−DENIED
∆tGNSS
tGNSS

Value Unit

8.0

25.0

1500.0

100.0

m

m

s

s

Variable
∆𝜃LOW
∆ROCLOW
∆

𝜃∘∘
1,MAX
𝜃∘∘
2,MAX

∆

∘

∘

Value Unit

0.2

0.01

0.0005

0.0003

∘

m/s

∘

∘

Table 2: Pitch adjustment settings

As an example, let’s consider a case in which the visual altitude ∘
hi is significantly higher than the observed
one ˆhn, resulting in |∆h| > ∆hLOW; in this case the IA-VNS concludes that the aircraft is “high” and applies
a negative pitch adjustment to slowly decrease the body pitch visual estimation ∘
𝜃 over many images,
with these accumulating over time into a lower altitude ∘
h that what would be the case if no adjustment
were applied. On the other hand, if the absolute value of the adjustment error is below the threshold
(|∆h| < ∆hLOW), the adjustment should not be applied as there exists a significant risk that the aircraft
is in fact “low” instead of “high” (when compared with the true altitude ht, not the the observed one ˆhn),
and a negative pitch adjustment would only exacerbate the situation. A similar reasoning applies for the
adjustment pitch error, in which the IA-VNS reacts or not to correct perceived “nose-up” or “nose-down”
visual estimations.

The applied thresholds are displayed in table 2, in which the altitude threshold ∆hLOW varies per (29) taking
into account that the INS vertical position accuracy varies depending on whether GNSS signals are available
or not, and if they are not the uncertainty increases with time although it is bounded by atmospheric

physics [5].

∆hLOW = ∆hLOW,GNSS−BASED +

t − tGNSS
∆tGNSS

∆hLOW,GNSS−DENIED

(29)

The ∘

𝜃∘∘
i

pitch target to be applied for each image is given by (27), where the obtainment of the pitch

19In this context, adjustment error is understood as the difference between the visual and inertial estimations.
20To avoid noise, this is smoothed over the last 100 images or 10 s.

24

adjustment ∆

∘

𝜃∘∘
i

is explained below based on its three components (30):

∘

∘

∆

𝜃∘∘
i = ∆

𝜃∘∘
h + ∆

∘

𝜃∘∘
𝜃 + ∆

∘

𝜃∘∘
ROC

(30)

– The pitch adjustment due to altitude, ∆

𝜃∘∘
h
below the threshold ∆hLOW to ∆
when the error is twice the threshold, as shown in (31). The
adjustment is bounded at this value to avoid destabilizing SVO with pose adjustments that differ too
much from their reprojection only optimum ∘
(9).

, linearly varies between zero when the adjustment error is

𝜃∘∘
1,MAX

∘

∘

𝜁⋆⋆
EBi

∆

∘

𝜃∘∘
h =

⎧
⎪⎪⎨

⎪⎪⎩

0

when |∆h| < ∆hLOW

− sign (∆h) ∆

∘

− sign (∆h) ∆

𝜃∘∘
1,MAX (|∆h| − ∆hLOW) /∆hLOW ∆hLOW ≤ |∆h| ≤ 2 · ∆hLOW
𝜃∘∘
1,MAX

when |∆h| > 2 · ∆hLOW

∘

(31)

– The pitch adjustment due to pitch, ∆

∘

𝜃∘∘
𝜃

instead of ∆hLOW, while also relying on the same limit ∆
its sign differs from that of ∆
the limit (|∆
𝜃∘∘
𝜃∘∘
𝜃 | ≤ ∆
h + ∆

𝜃∘∘
h
𝜃∘∘
1,MAX

).

∘

∘

∘

∘

, works similarly but employing ∆𝜃 instead of ∆h and ∆𝜃LOW
is set to zero if

. In addition, ∆

∘

∘

𝜃∘∘
1,MAX

𝜃∘∘
𝜃

, and reduced so the combined effect of both targets does not exceed

∘

– The pitch adjustment due to rate of climb, ∆

, also follows a similar scheme but employing ∆ROC
instead of ∆h, ∆ROCLOW instead of ∆hLOW, and ∆
. Additionally, it is
𝜃∘∘
𝜃∘∘
1,MAX
2,MAX
multiplied by the ratio between ∆
to limit its effects when the altitude estimated
𝜃∘∘
1,MAX
error ∆h is small. This adjustment can act in both directions, imposing bigger pitch adjustments if
the altitude error is increasing or lower one if it is already diminishing.

instead of ∆

and ∆

𝜃∘∘
ROC

𝜃∘∘
h

∘

∘

∘

∘

If activated, the weight value fq required for the (21) joint optimization is determined by imposing that
the weighted attitude error fq · Eq
𝜁 ECi0) when evaluated
before the first iteration, this is, it assigns the same weight to the two active components of the joint

)︀ coincides with the reprojection error ERP(

(︀ ∘rECi0

∘

∘

EPO(

𝜁 ECi) cost function (21).

• Pitch and Bank Adjustment Activation. The previous scheme can be modified to also make use of the
observed body bank angle ˆ𝜉n within the framework established by the (11) through (15) attitude adjustment
optimization:

∘

∘

𝜓∘∘
i
𝜃∘∘
i
𝜉∘∘
i

∘

=

=

=

∘

∘

𝜓⋆⋆
i
𝜃⋆⋆
i + ∆
𝜉⋆⋆
i + ∆

∘

∘

𝜃∘∘
i
𝜉∘∘
i

∘

(32)

(33)

(34)

Although the new body pose target ∘
the optimum pose ∘
𝜁⋆⋆
EBi
are allowed to vary when minimizing the joint cost function.

𝜁∘∘
EBi

only differs in two out of six dimensions (pitch and bank) from

obtained by minimizing the reprojection error exclusively, all six degrees of freedom

Variable Value Unit
∆𝜉LOW
𝜉∘∘
∆
1,MAX

0.0003

0.2

∘

∘

∘

Table 3: Additional bank adjustment settings

∘

𝜃∘∘
i

The determination of the pitch adjustment ∆
𝜉∘∘
i
relies on a linear adjustment between two values similar to any of the three components of (30) explained
𝜉i − ˆ𝜉n, as well as a ∆𝜉LOW
in the previous section, but relying on the bank angle adjustment error ∆𝜉 =
maximum adjustment whose values are provided in table 3. Note that the value of
threshold and ∆
the ∆𝜉LOW threshold coincides with that of ∆𝜃LOW as the INS accuracy for both pitch and roll is similar
according to [5].

does not vary, and that of the bank adjustment ∆

𝜉∘∘
1,MAX

∘

∘

∘

It is important to remark that the combined pitch and bank adjustment activation is the one employed to
show the results when applied to the IA-VNS Monte Carlo simulations in sections 7 through 9.

25

• Attitude Adjustment Activation. The use of the observed yaw angle ˆ𝜓 is not recommended as the visual
estimation ∘
𝜓 provided by the VNS (without any inertial inputs) is in general more accurate than its inertial
counterpart ˆ𝜓, as shown in section 7 (table 5). This can be traced on one side to the bigger influence
that a yaw change has on the resulting optical flow when compared with those caused by pitch and bank
changes, which makes the body yaw angle easier to track by the VNS, and on the other to the INS relying
on the gravity pointing down to control pitch and bank adjustments versus the less robust dependence on

the Earth magnetic field and associated magnetometer readings used to estimate the aircraft heading [5].

For this reason, the attitude adjustment process described next has not been implemented, although it is

included here as a suggestion for other applications in which the objective may be to adjust the vehicle
provided by the INS and the initial
attitude as a whole. The process relies on the observed attitude ˆqNBn
Its difference is given by
estimation ∘q⋆⋆
NBi
, where the superindex Bi indicates that it is viewed in the pose optimized body
∆ ∘rBi,⋆⋆ = ˆqNBn ⊖ ∘q⋆⋆
NBi
frame. This perturbation can be decoupled into a rotating direction and an angular displacement [2, 4],
resulting in ∆ ∘rBi,⋆⋆ = ∘nBi,⋆⋆ ∆

provided by the reprojection only pose optimization process.

𝜑⋆⋆.

∘

Let’s now consider that the IA-VNS decides to set an attitude target that differs by ∆
, but rotating about the axis that leads to its inertial estimation ˆqNBn
only solution ∘q⋆⋆
NBi
can then be obtained by SO(3) Spherical Linear Interpolation (SLERP) [2, 3], where t = ∆
∘q∘∘
NBi
is the ratio between the target rotation and the attitude error or estimated angular displacement:

𝜑∘∘ from its reprojection
. The target attitude
∘
𝜑⋆⋆

𝜑∘∘ / ∆

∘

∘

NBi = ∘q⋆⋆
∘q∘∘

NBi ⊗ (︀ ∘q⋆⋆

NBi

* ⊗ ˆqNBn

)︀t

(35)

• Altitude Adjustment Activation. The activation and target determination of the (16) through (20) altitude
adjustment is similar to the attitude adjustments described above. When activated as explained below,
the new body pose target ∘
optimum pose ∘
small as its effects are intended to accumulate over many successive images. This does not mean however
that the other five components do not vary, as the joint optimization process freely optimizes within SE (3)
with six degrees of freedom to minimize the joint cost function that not only considers the reprojection

obtained by minimizing the reprojection error exclusively, and the difference is very

only differs in one out of six dimensions (the geometric altitude) from the

𝜁⋆⋆
EBi

𝜁∘∘
EBi

error, but also the resulting altitude target.

∘

h∘∘
i =

∘

h⋆⋆
i + ∆

∘

h∘∘
i

(36)

The objective is not for the visual altitude estimations ∘
hi to closely track the inertial ones ˆhn, but only to
avoid excessive deviations, so there exists a lower threshold ∆hLOW, which is obtained by means of (29),
below which the adjustment is not activated.

Variable
∆hLOW,GNSS−BASED
∆hLOW,GNSS−DENIED
∆

h∘∘
MAX

∘

Value Unit

Variable Value Unit

8.0

25.0

0.01

m

m

m

∆tGNSS
tGNSS

1500.0

100.0

s

s

Table 4: Altitude adjustment settings

∘

∘

h∘∘
i

linearly varies between zero when the adjustment error is below the threshold

The altitude adjustment ∆
when the error is twice the threshold, as shown in (37). The adjustment is bounded
∆hLOW to ∆
at this value to avoid destabilizing SVO with pose adjustments that differ too much from their reprojection
only optimum ∘

h∘∘
MAX

(9).

𝜁⋆⋆
EBi

∆

∘

𝜃∘∘
h =

⎧
⎪⎪⎨

⎪⎪⎩

0

− sign (∆h) ∆

− sign (∆h) ∆

when |∆h| < ∆hLOW

∘

𝜃∘∘
1,MAX (|∆h| − ∆hLOW) /∆hLOW ∆hLOW ≤ |∆h| ≤ 2 · ∆hLOW
𝜃∘∘
1,MAX

when |∆h| > 2 · ∆hLOW

∘

(37)

26

If activated, the weight value fh required for the (21) joint optimization is determined by imposing that
𝜁 ECi0) when evaluated
the weighted altitude error fh · Eh
before the first iteration, this is, it assigns the same weight to the two active components of the joint

)︀ coincides with the reprojection error ERP(

(︀ ∘
𝜁 ECi0

∘

∘

EPO(

𝜁 ECi) cost function (21).

Note that the altitude adjustment has been implemented and successfully tested, but extensive simulations

have shown that the attitude adjustment (either the pitch only or the pitch and bank adjustment) is capable
by itself of controlling the visually estimated altitude ∘
h by the accumulation over many images of changes
in the aircraft path angle with less negative effects on the remaining components of the aircraft pose. The

results described in section 7 hence do not rely on the altitude adjustment activation, but only on the

previously described pitch and bank adjustment activation.

Additional Modifications to SVO

In addition to the PI inspired introduction of priors into the pose optimization phase of SVO, the proposed IA-VNS
also includes various other modifications to the original SVO pipeline described in section 5:

• Addition of Current Features to the Structure Optimization Phase. The objective of the structure optimiza-
by minimization of the

tion phase (figure 13) is to adjust the ECEF coordinates of the terrain 3D points pE
j
reprojection error into all those keyframes from which each 3D point is visible. For each 3D point pE
j
original algorithm relies on the pose ∘
keyframes feature coordinates pIMG
associated feature coordinates pIMG

𝜁 ECk
corresponding to that 3D point, but the current frame pose ∘
are not employed in the optimization.

of every keyframe from where the 3D point is visible, plus the

𝜁 ECi

, the

and

jk

ji

Such an algorithm hence results in Earth surface 3D points pE
that do not take into consideration the
j
previously obtained camera pose ∘
In the next cycle, corresponding to image Ii+1, these 3D points
constitute a key input to the feature alignment and pose optimization processes, and as they are based on
the keyframe poses and not that of ∘
the adoption of attitude and altitude priors during pose optimization.

, reverting any adjustments gained by

, so is the new pose ∘

𝜁 ECi+1

𝜁 ECi

𝜁 ECi

.

To ensure that the slight pose adjustments induced by the priors with each new image accumulate over time,
, the algorithm
the structure optimization algorithm needs to be modified. For each visible 3D point pE
j
shall rely not only on the pose and image feature coordinates of all keyframes from which the 3D point is

visible, but also on those of the current frame (the one whose pose has just been optimized with the priors).

In this way, the pose adjustments stick and are not reverted in the next cycle.

• Inertial Pose Estimation. The sparse image alignment phase (figure 13) constitutes the most delicate and
error prone phase of the whole SVO pipeline. As a non linear optimization based on the minimization of
the luminosity error in small patches located around the tracked features, whose objective is to obtain a
camera pose approximation ∘
𝜁⋆
ECi
depends on the proximity of the initial estimation ∘
𝜁⋆
ECi,0
relies on the previous image pose for the initial estimation21 ( ∘
𝜁⋆
ECi,0 =

that is later fine tuned by the pose optimization phase, its convergence
to the solution. The stand alone SVO algorithm

𝜁 ECi−1

).

∘

This initial estimation works very well for a big majority of the images being processed, but suffers in

those frames in which the optical flow induced by the camera motion is highest, which for the considered

scenarios corresponds to the roll-in and roll-out maneuvers executed when initiating or concluding a turn.

Short periods of intense turbulence can also generate initial pose estimations that differ too much from their

true values and hence compromise the convergence of the non linear minimization. As the sparse image
alignment output ∘
𝜁⋆
ECi
it is fine tuned to obtain the final pose estimation ∘
sparse image alignment estimations.

is employed in the feature alignment and pose optimization phases (figure 13), where

, these two phases can be compromised by incorrect

𝜁 ECi

21The authors attempted to replace this initial estimation with an extrapolation to the current frame based on the relative visually
estimated pose between the two previous frames. The results were mixed and hence the option discarded, as the accumulation of
turbulence plus maneuvers may induce significant errors.

27

A more accurate and robust pose estimation can however be obtained by discarding the sparse image

𝜁 ECi−1

alignment algorithm and replacing it with a pose estimation based on the output of the previous frame pose
optimization ∘
concatenated with the various inertial incremental pose estimations generated by the
INS since that last frame22. To get an even more accurate estimation ∘
𝜁⋆
in GNSS-Denied conditions, it
ECi
is possible to employ an strategy such as that described in [5], in which only the attitude and air velocity
inertial changes are utilized, but the wind velocity is frozen to its value at the time the GNSS signals were
lost.

• Automated Homography Scale Determination. The initial height ˆHHEIGHT0 estimation described in section
5 suffers from the fact that, although not contiguous, the two keyframes on which it relies are taken

sufficiently close23 that the estimated distance between the two keyframes is significantly influenced by the
GNSS receiver white noise. To fine tune the initial height estimation, it is possible to allow the SVO pipeline
to continue running for an amount of time approximately an order of magnitude longer24 while storing
all images for later use, until reaching a certain image, noted with # in the expressions below, which is
sufficiently separated from the initial one so as to dilute the influence of the GNSS receiver white noise. After
obtaining the visual (‖
C0C#‖) distances up to this frame as indicated in section 5,
the initial height can be linearly approximated as follows:

C0C#‖) and inertial (‖ ˆTC0

TC0

∘

∘

HHEIGHT ≈ ˆHHEIGHT

‖ ˆTC0
TC0
‖

C0C#‖
C0C#‖

∘

(38)

This equivalence can also be used to estimate a correction to the initial body pitch angle based on the

differences in geometric altitude between the visual and inertial estimations:

∘
𝜃 = − arctan

∆

∘

h# − ˆh#
‖ ˆTC0
C0C#‖

(39)

At this point, all the previously stored images are processed again, but this time using ∘
HHEIGHT as the depth
for the homography initialization, and correcting the initial body attitude with ∆
𝜃. Once the execution
catches up with the images being generated in real time, it is not necessary to store any more images, and

∘

each image is processed as soon as it is generated25.

7 Visual Navigation System Error in GNSS-Denied Conditions

This section presents the results obtained with the proposed IA-VNS (section 6) when executing Monte Carlo
simulations of the two GNSS-Denied scenarios over the MX terrain type, each consisting of one hundred executions.
They are compared with the results obtained with the stand alone VNS that relies on the original SVO algorithm
(section 5), and with those of the INS described in [5]. Detailed descriptions of the different modules of the high
fidelity simulation together with the definition of the two scenarios are provided in [7] and summarized in section

4. [8] explains the expressions employed to model the measurement errors introduced by the different sensors, as

well as their “baseline” values employed in the simulation.

The navigation system error (NSE) incurred by the proposed navigation systems (and accordingly denoted as INSE,
VNSE, and IA-VNSE) when estimating the value of the different variables in GNSS-Denied conditions is evaluated in
the tables below according to the aggregated final state metrics defined in [7]. They include the mean, standard
deviation, and maximum values taking as inputs the NSE of the last or final estimation within each simulation

22In the simulation, as the inertial system operates at 100 Hz and the visual one at 10 Hz, the ten last inertial incremental poses

shall be used.

23The first two keyframes are usually separated by one or two seconds.
24Twenty seconds are employed in the simulations.
25Note that the IA-VNS is initialized when GNSS signals are present, so its outputs are not yet required for guidance and control

purposes.

28

run26 .When depicting the NSE, the figures below show the variation with time of the time aggregated metrics
(mean and standard deviation) of the one hundred different executions of each scenario; these are also formally

defined in [7].

Before analyzing the simulation results, it is necessary to remark the following:

• The results obtained with the INS under the same two GNSS-Denied scenarios are described in detail in
[5]. The attitude INSE does not drift and is bounded by the quality of the onboard sensors, ensuring the
aircraft can remain aloft for as long as there is fuel available. The vertical position and ground velocity
INSEs are also bounded by atmospheric physics and do not drift; their estimation errors depend on the
atmospheric pressure offset and wind field changes that occur since the GNSS signals are lost. On the
other hand, the horizontal position INSE drifts as a consequence of integrating the ground velocity without
absolute references. Of the six SE(3) degrees of freedom (three for attitude, two for horizontal position, one
for altitude), the INS is hence capable of successfully estimating four of them in GNSS-Denied conditions.

• Visual navigation systems (either VNS or IA-VNS) are only necessary to reduce the estimation error in the
two remaining degrees of freedom (the horizontal position). Although both of them estimate the complete

six dimensional aircraft pose as described below, their attitude and altitude estimations shall only be

understood as a means to provide an accurate horizontal position estimation, which represents their only

real objective.

• As described below, the SVO based VNS drifts in all six degrees of freedom. The main focus of this article
is on how the addition of INS based priors enables the IA-VNS to reduce the drift in all dimensions. The
resulting horizontal position IA-VNSE is just a fraction of the INSE, and hence can be fed back as additional
observations into the inertial filter, converting it into a fully integrated VINS; this feed back loop however
is not described in this article. The attitude and altitude IA-VNSEs, although improved when compared
to the VNSEs, are qualitatively inferior to the driftless INSEs, but note that their purpose is just to enable
better horizontal position IA-VNS estimations, not to replace the attitude and altitude INS outputs.

Body Attitude Estimation

Table 5 shows the aggregated final state metrics for the three Euler angles representing the body attitude (yaw
𝜓, pitch 𝜃, roll 𝜉), together with the norm of the rotation vector between the real body attitude qNB
estimations, ˆqNB
The errors hence can be formally written as ‖∆ˆrB‖ = ‖ˆqNB ⊖ qNB‖ or ‖∆ ∘rB‖ = ‖ ∘qNB ⊖ qNB‖ [2].

and its
by the IA-VNS, when applied to both scenarios over the MX terrain type.

by the INS and ∘qNB

Scenario MX
[∘] (tEND)

∆ˆ𝜓
mean +0.03

#1

std

max

mean

0.18

-0.61

INSE

∆ˆ𝜃
-0.03

0.05

-0.27

∆ˆ𝜉
-0.00

0.06

-0.23

-0.02 +0.01 +0.00

#2

std

0.13

0.05

0.05

VNSE
∘
𝜃

∆

∘
𝜓

∘
‖∆ˆrB‖
𝜉
∆
0.158 +0.03 +0.08 +0.00
0.21
0.114

0.13

0.23

∆

0.611 +0.63 +0.74 +0.78
0.128 +0.02
0.08
0.078

-0.02 +0.00

0.20

0.21

∆

∘
‖∆ ∘rB‖
𝜓
0.296 +0.03
0.11
0.158

0.791 +0.55
0.253 +0.02
0.08
0.161

IA-VNSE
∘
𝜃
∆
-0.01

∘
𝜉
∆
-0.03

0.16

-0.37

0.14

-0.51

-0.00 +0.01

0.16

0.19

max

+0.33

-0.15 +0.15

0.369 +0.22

-0.65

-0.73

0.730 +0.24 +0.62 +0.74

‖∆ ∘rB‖
0.218
0.103

0.606

0.221
0.137

0.788

Table 5: Aggregated final body attitude INSE, VNSE, and IA-VNSE (100 runs)

The variation with time of the mean and standard deviation of the attitude error for the two MX scenarios is
depicted in figures 16 and 17, respectively. In the case of scenario #1, figure 16 also depicts the worst IA-VNSE
seed (red) among the one hundred executions; this is not shown for scenario #2 in figure 17 because the significant
oscillations hinder the visualization of the remaining lines. In addition, figure 18 shows the estimation of each

26This amounts to 100 points for each scenario.

29

individual Euler angle for the MX scenario #1 exclusively. Note that the INSE (blue lines), which shows no drift
with time in either scenario, coincides with that depicted in [5].

]
∘
[

‖
B
∘r
∆
‖

,
‖
B
ˆr
∆
‖

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

‖Δ∘rB‖END,WORSTj IA-VNSE
𝜇

VNSE

± 𝜎

‖Δ

∘rB‖i

‖Δ

∘rB‖i

𝜇‖Δ^rB‖n ± 𝜎‖Δ^rB‖n INSE
𝜇

± 𝜎

IA-VNSE

‖Δ

∘rB‖i

‖Δ

∘rB‖i

Scenario #1 MX

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 16: Body attitude INSE, VNSE, and IA-VNSE for scenario #1 MX (100 runs)

With respect to the VNSE (pink lines), most of the scenario #1 error is incurred during the turn maneuver at
the beginning of the scenario (refer to tTURN within [7]), with only a slow accumulation during the rest of the
trajectory, composed by a long straight flight with punctual changes in altitude and speed. Additional error

growth would certainly accumulate if more turns were to occur, although this is not tested in the simulation.
This statement seems to contradict the results obtained with scenario #2, in which the error grows with the initial
turns but then stabilizes and even slightly decreases during the rest of the scenario, even though the aircraft is
executing continuous turn maneuvers. This lack of error growth occurs because the scenario #2 trajectories are
so twisted (refer to [7]) that terrain zones previously mapped reappear in the camera field of view during the

consecutive turns, and are hence employed by the pose optimization phase as absolute references, resulting in

a much better attitude estimation that what would occur under more spaced turns. A more detailed analysis

(not shown in the figures) shows that the estimation error does not occur during the whole duration of the turns,

but only during the initial roll-in and final roll-out maneuvers, where the optical flow is highest and hence more
difficult to track by SVO27.

0.5

0.4

0.3

0.2

0.1

]
∘
[

‖
B
∘r
∆
‖

,
‖
B
ˆr
∆
‖

0

0

𝜇‖Δ^rB‖n ± 𝜎‖Δ^rB‖n INSE
𝜇
VNSE
𝜇
IA-VNSE

± 𝜎
± 𝜎

‖Δ

‖Δ

∘rB‖i
∘rB‖i

∘rB‖i
∘rB‖i

‖Δ

‖Δ

Scenario #2 MX

50

100

150

200

250
t [s]

300

350

400

450

500

Figure 17: Body attitude INSE, VNSE, and IA-VNSE for scenario #2 MX (100 runs)

The IA-VNSE results (orange lines) show that the introduction of priors in section 6 works as intended and there
exists a clear benefit for the use of an IA-VNS when compared to the stand alone VNS. In spite of IA-VNSE
values at the beginning of both scenarios that are nearly double those of the VNSE, caused by the initial pitch
adjustment (39) required to improve the fit between the homography output and the initial inertial estimations,
the balance between both errors quickly flips as soon as the aircraft starts maneuvering, resulting in IA-VNSE

27For the two evaluated scenarios, the optical flow during the roll-in and roll-out maneuvers is significantly higher than that induced
by straight flight, pull-up and push-down maneuvers, and even the turning maneuvers themselves (once the bank angle is no longer
changing).

30

values significantly lower than those of the VNSE for the rest of both scenarios. This improvement is more
significant in the case of scenario #1, as the prior based pose optimization is by design a slow adjustment
that requires significant time to slowly correct attitude and altitude deviations between the visual and inertial

estimations.

Qualitatively, the biggest difference between the three estimations resides in the nature of the errors. While
the attitude INSE is bounded by the quality of the sensors and the navigation algorithms [5], drift is present in
the VNS and IA-VNS estimations. The drift resulting from the Monte Carlo simulations may be small, and so is
the attitude estimation error ‖∆ ∘rB‖, but more challenging conditions with more drastic maneuvers and a less
idealized image generation process than that described in section 4 may generate additional drift.

Focusing now on the quantitative results shown in table 5, aggregated errors for each individual Euler angle are
always unbiased and zero mean for each of the three estimations (INS, VNS, IA-VNS), as the means tend to zero
as the number of runs grows, and are much smaller than both the standard deviations and the maximum values.
With respect to the attitude error ‖∆ˆrB‖ and ‖∆ ∘rB‖, their aggregated means are not zero (they are norms), but
are nevertheless quite repetitive in all three cases, as the mean is always significantly higher than the standard

deviation, while the maximum values only represent a small multiple of the means. It is interesting to point
out that while in the case of the INSE the contribution of the yaw error is significantly higher than that of the
pitch and roll errors, the opposite occurs for both the VNSE and the IA-VNSE. This makes sense as the the gravity
direction is employed by the INS as a reference from where the estimated pitch and roll angles can not deviate, but
slow changes in yaw generate larger optical flow variations than those caused by pitch and roll variations.

These results prove that the IA-VNS succeeds when employing the observed pitch and bank angles (ˆ𝜃, ˆ𝜉), whose
errors are bounded, to limit the drift of their visual counterparts ( ∘
28 are significantly
lower for the IA-VNS than for the VNS. Remarkably, this is achieved with no degradation in the body yaw angle,
as 𝜎
remains stable. Note that adjusting the output of certain variables in a minimization algorithm (such
as pose optimization) usually results in a degradation in the accuracy of the remaining variables as the solution

𝜉), as 𝜎

and 𝜎

𝜃, ∘

END

END

END

∘
𝜓

∘
𝜃

∘
𝜉

moves away from the true optimum. In this case, however, the improved fit between the adjusted aircraft pose
and the terrain displayed in the images, results in the SVO pipeline also slightly improving its body yaw estimation
𝜓. The following sections show how the benefits of an improved fit between the displayed terrain and the adjusted
pose also improve the horizontal position estimation.

∘

It is interesting to remark that although qualitatively the IA-VNS estimations of the three Euler angles as well
as that of the total attitude error drift and are hence worse than the bounded estimations obtained by the INS,
quantitatively the IA-VNS estimations are only worse than the INS ones in the case of the pitch, bank, and total
attitude, while the visual yaw angle estimation is consistently superior to the inertial one (refer to the top of

figure 18).

In the case of a real life scenario based on a more realistic image generation process than that described in section
4 and [7], the VNS would likely incur in additional body attitude drift than in the simulations. If this were to
occur, the IA-VNS pose adjustment algorithms described in section 6 would react more aggressively to counteract
the higher pitch and bank deviations, eliminating most of the extra drift, although it is possible that higher pose
adjustment parameters than those listed in tables 2 and 3 would be required. The IA-VNS is hence more resilient
against high drift values than the VNS.

Vertical Position Estimation

Table 6 shows the aggregated final state metrics for vertical position or altitude (∆ˆh = ˆh − h, ∆
h − h) when
applied to both scenarios over the MX terrain type. The results can be considered unbiased or zero mean in all
six cases (two scenarios and three estimation methods), as the mean of the final altitude error 𝜇ENDh is always
significantly lower than both the standard deviation 𝜎ENDh or the maximum value 𝜁END|h|. With respect to the

∘

∘
h =

28As the individual Euler angle metrics are unbiased or zero mean, the benefits of the proposed approach are reflected in the

variation of the remaining metrics, this is, the standard deviation and the maximum value.

31

]
∘
[

∘𝜓
∆

,

ˆ𝜓
∆

]
∘
[

∘𝜃
∆

,
ˆ𝜃
∆

]
∘
[

∘𝜉
∆

,
ˆ𝜉
∆

0.3

0.2

0.1

0

−0.1

−0.2

0.3

0.2

0.1

0

−0.1

−0.2

0.3

0.2

0.1

0

−0.1

−0.2

𝜇Δ ^𝜓n ± 𝜎Δ ^𝜓n INSE
𝜇
VNSE
𝜇
IA-VNSE

± 𝜎
± 𝜎

Δ

Δ

∘
𝜓i
∘
𝜓i

Δ

∘
𝜓i
∘
𝜓i

Δ

Scenario #1 MX

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

𝜇Δ^𝜃n ± 𝜎Δ^𝜃n INSE
𝜇
VNSE
𝜇
IA-VNSE

± 𝜎
± 𝜎

Δ

Δ

∘
𝜃i
∘
𝜃i

Δ

∘
𝜃i
∘
𝜃i

Δ

Scenario #1 MX

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

𝜇Δ^𝜉n ± 𝜎Δ^𝜉n INSE
𝜇
VNSE
𝜇
IA-VNSE

± 𝜎
± 𝜎

Δ

Δ

∘
𝜉i
∘
𝜉i

Δ

∘
𝜉i
∘
𝜉i

Δ

Scenario #1 MX

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 18: Body Euler angles INSE, VNSE, and IA-VNSE for scenario #1 MX (100 runs)

evolution of the geometric altitude estimation error with time, this is shown in figures 19 and 20 in the same

format as the attitude analysis above. Both figures also depict (red lines) the specific seeds corresponding to
those Monte Carlo executions that result in the highest IA-VNS altitude estimation errors. Note that the altitude
INSE (blue lines), which is bounded by atmospheric physics, coincides with that depicted in [5].

Scenario MX

[m]
mean

#1

std

max

mean

#2

std

max

INSE
∆ˆhEND
-4.18

25.78
-70.49

+0.76

7.55
-19.86

VNSE
∘
hEND
∆
+82.91

287.58
+838.32

+3.45

20.56
+72.69

IA-VNSE
∘
hEND
∆
+22.86

49.17
+175.76

+3.59

13.01
+71.64

Table 6: Aggregated final vertical position INSE, VNSE, and IA-VNSE (100 runs)

The VNS estimation of the geometric altitude (pink lines) is worse than that by the INS both qualitatively and
quantitatively, even with the results being optimistic because of the ideal image generation process employed in

the simulation. A continuous drift or error growth with time is present, and results in final errors much higher
than those obtained with the GNSS-Denied inertial filter. These errors are logically bigger for the scenario #1
because of its much longer duration.

A small percentage of this drift can be attributed to the slow accumulation of error inherent to the SVO motion

32

]

m

[

∘h
∆

,
ˆh
∆

300

200

100

0

−100

−200

𝜇Δ^hn ± 𝜎Δ^hn INSE

𝜇

∘
hi

Δ

± 𝜎

∘
hi

Δ

VNSE

𝜇

Δ

∘
hi

± 𝜎

∘
hi

Δ

IA-VNSE

∘
hEND,WORSTj IA-VNSE

Δ

Scenario #1 MX

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 19: Vertical position INSE, VNSE, and IA-VNSE for scenario #1 MX (100 runs)

thread algorithms introduced in section 5, but most of it results from adding the estimated relative pose between

two consecutive images to a pose (that of the previous image) with an attitude that already possesses a small

pitch error (refer to the attitude estimation analysis above). Note that even a fraction of a degree deviation in
pitch can result in hundreds of meters in vertical error when applied to the total distance flown in scenario #1,
as SVO can be very precise when estimating pose changes between consecutive images, but lacks any absolute
reference to avoid slowly accumulating these errors over time. This fact is precisely the reason why the vertical
position VNSE grows more slowly in the second half of scenario #2, as shown in figure 20. As explained in the
attitude analysis above, continuous turn maneuvers cause previously mapped terrain points to reappear in the

camera field of view, stopping the growth in the attitude error (pitch included), which indirectly has the effect

of slowing the growth in altitude estimation error.

The benefits of introducing priors to limit the differences between the visual and inertial altitude estimations are
reflected in the IA-VNSE (orange lines). The error reduction is drastic in the case of the scenario #1, where its
extended duration allows the pose optimization small pitch adjustments to accumulate into significant altitude
corrections over time, and less pronounced but nevertheless significant for scenario #2, where the VNSE (an hence
also the IA-VNSE) results already benefit from previously mapped terrain points reappearing in the aircraft field
of view as a result of the continuous maneuvers. It is necessary to remark the amount of the improvement, as
the final standard deviation 𝜎ENDh diminishes from 287.58 to 49.17 m for scenario #1, and from 20.56 to 13.01 m
for scenario #2.

The benefits of the prior based pose optimization algorithm can be clearly observed in the case of the scenario
#1 execution with the worst final altitude estimation error, whose error variation with time is depicted in figure
19 (red line). After a rapid growth in the first third of the scenario following a particularly negative estimation
during the initial turn, the altitude error reaches a maximum of +233.05 m at 2007.5 s. Attitude adjustment
has become active long before, lowering the estimated pitch angle to first diminish the growth of the altitude
error and then being able to reduce the error itself, reaching a final value of +175.76 m at tEND. As soon as
the differences between the visual pitch, bank, or altitude estimations ( ∘
h) and their inertial counterparts
𝜃,
(ˆ𝜃, ˆ𝜉, ˆh) exceed certain limits (section 6), the attitude adjustment comes into play and slowly adjusts the aircraft
pitch to prevent the visual altitude from deviating too far from the inertial one. This behavior not only improves
the IA-VNS altitude estimation accuracy when compared to that of the VNS, but also its resilience, as the system
actively opposes elevated altitude errors.

∘
𝜉,

∘

Significantly better altitude estimation errors (closer to the inertial ones) could be obtained if more aggressive
settings were employed for ∆
which the pose optimization convergence is compromised. This would result in more aggressive adjustments and

within table 2, as the selected values are far from the level at

𝜃∘∘
1,MAX

𝜃∘∘
2,MAX

and ∆

∘

∘

important accuracy improvements for those cases in which altitude error growth is highest. The setting employed

in this article are modest, as the final objective is not to obtain the smallest possible attitude or vertical position
IA-VNSE (as they are always bigger than their INSE counterparts), but to limit them to acceptable levels so

33

]

m

[

∘h
∆

,
ˆh
∆

40

30

20

10

0

−10

−20

𝜇Δ^hn ± 𝜎Δ^hn INSE

𝜇

Δ

∘
hi

± 𝜎

Δ

∘
hi

VNSE

𝜇

∘
hi

Δ

± 𝜎

∘
hi

Δ

IA-VNSE

∘
hEND,WORSTj IA-VNSE

Δ

Scenario #2 MX

0

50

100

150

200

250
t [s]

300

350

400

450

500

Figure 20: Vertical position INSE, VNSE, and IA-VNSE for scenario #2 MX (100 runs)

SVO can build a more accurate terrain map, improving the fit between the multiple terrain 3D points displayed
in the images and the estimated aircraft pose. To do so it is mandatory to balance the pitch and bank angle

adjustments with the need to stick to solutions close to those that minimize the reprojection error, as explained
in section 6. Higher ∆
It is expected that a better rendition of the real 3D position of the features detected in the keyframes as they

accelerate the adjustments but may decrease the quality of the map.

𝜃∘∘
2,MAX

𝜃∘∘
1,MAX

and ∆

∘

∘

are tracked along successive images will lower the incremental horizontal displacement errors, and hence result in
a lower horizontal position IA-VNSE. This is indeed the real objective for the introduction of the priors, as then
the visual estimation can be more accurate than the inertial one and hence be employed as a virtual sensor to

provide ground velocity or incremental horizontal displacement observations back to the inertial filter.

The IA-VNS altitude estimation improvements over those of the VNS are not only quantitative. Figure 19 shows
no increment in 𝜎∘
(orange lines) in the second half of scenario #1 (once on average the deviation has activated
hn
the attitude adjustment feature). The altitude estimation by the IA-VNS can hence also be described as bounded
and driftless, which represents a qualitative and not only quantitative improvement over that of the VNS. The
bounds are obviously bigger for the IA-VNS than for the INS. In the case of scenario #2, figure 20 shows a slow
growth with time, but this is only because the error amount on average is not yet significant
but steady 𝜎∘
hn
enough to activate the attitude adjustment feature within pose optimization. Note however that it is only on
aggregated terms where drift is eliminated and the visual error bounded to the INSE, as a limited drift may exist
for an individual trajectory, although on the long term the visual estimation tends to return to values not too

far from the inertial ones.

Horizontal Position Estimation

The horizontal position estimation capabilities of the INS, VNS, and IA-VNS share the fact that all of them exhibit
and unrestrained drift or growth with time, as shown in figures 21 and 22. The errors obtained at the end of both
scenarios are shown in table 7, following the same scheme as in previous sections. The approximately linear INS
drift appears when integrating the bounded ground velocity errors; the visual estimation drifts (both VNS and
IA-VNS) originate in the slow accumulation of errors caused by the concatenation of the relative poses between
consecutive images without absolute references, but also show a direct relationship with the scale error committed

when supplying the aircraft height over the terrain during the initial homography.

34

Scenario MX

#1

#2

mean

std

max

mean

std

max

Distance

[m]
107873

19756

4880

INSE
∆ˆxHOR,END
[m]
[%]
7276

7.10
5.69

VNSE
∆ ∘xHOR,END
[m]
[%]
4179

3308

3.82
2.73

IA-VNSE
∆ ∘xHOR,END
[m]
[%]
488

0.46
0.31

350

172842

25288

32.38

21924

14.22

1957

1.48

14198

1176

18253

216

119

586

1.52
0.86

4.38

251

210

954

1.77
1.48

7.08

33

26

130

0.23
0.18

0.98

Table 7: Aggregated final horizontal position INSE, VNSE, and IA-VNSE (100 runs)

In the case of the VNS (pink lines), its scenario #1 horizontal position estimations appear to be significantly more
accurate than those of the INS (blue lines). Note however that the ideal image generation process discussed in
section 4 implies that the simulation results should be treated as a best case only, and that the results obtained
in real world conditions would likely imply a higher horizontal position drift. The drift experienced by the VNS
in figure 22 (scenario #2) also shows the same diminution in its slope in the second half of the scenario discussed
in previous sections, which is attributed to previously mapped terrain points reappearing in the camera field of
view as a consequence of the continuous turns present in scenario #2. Additionally, notice how the VNSE starts
growing at the beginning of the scenario, while the INSE only starts doing so after the GNSS signals are lost at
tGNSS [5].

8,000

6,000

4,000

2,000

]

m

[

R
O
H
∘x
∆

,
R
O
H
ˆx
∆

0

0

∘

𝜇Δ^xHORn ± 𝜎Δ^xHORn INSE
𝜇Δ
xHORi VNSE
𝜇Δ
xHORi IA-VNSE
Δ∘xHOR END,WORSTj IA-VNSE

xHORi ± 𝜎Δ
xHORi ± 𝜎Δ

∘

∘

∘

Scenario #1 MX

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [sec]

Figure 21: Horizontal position INSE, VNSE, and IA-VNSE for scenario #1 MX (100 runs)

The IA-VNS (orange lines) results in major horizontal position estimation improvements over the VNS. The final
horizontal position error mean 𝜇ENDΔ∘xHOR
diminishes from 3.82 to 0.46 % for scenario #1, and from 1.77 to 0.23 %
for scenario #2. The repeatability of the results also improves, as the final standard deviation 𝜎ENDΔ∘xHOR
falls
from 2.73 to 0.31 % and from 1.48 to 0.18 % for both scenarios. Note that although these results may be slightly
optimistic due to the optimized image generation process, they are much more accurate than those obtained with
the INS, for which the error mean and standard deviation amount to 7.10 and 5.69 % for scenario #1, and 1.52
and 0.86 % in case of scenario #2.

It is interesting to remark how the prior based pose optimization described in section 6, an algorithm that adjusts

the aircraft pitch and bank angles based on deviations between the visually estimated pitch angle, bank angle,

and geometric altitude, and their inertially estimated counterparts, is capable of not only improving the visual

estimations of those three variables, but doing so with a minor improvement in the body yaw estimation and an

extreme reduction in the horizontal position error. When the cost function within an optimization algorithm is

modified to adjust certain target components, the expected result is that this can be achieved only at the expense

of the accuracy in the remaining target components, not in addition to it. The reason why in this case all target

components improve lies in that the adjustment creates a better fit between the ground terrain and associated

35

𝜇Δ^xHORn ± 𝜎Δ^xHORn INSE
𝜇Δ

xHORi ± 𝜎Δ

xHORi IA-VNSE

∘

∘

∘

xHORi ± 𝜎Δ

𝜇Δ
Δ∘xHOR END,WORSTj IA-VNSE

xHORi VNSE

∘

Scenario #2 MX

400

300

200

100

]

m

[

R
O
H
∘x
∆

,
R
O
H
ˆx
∆

0

0

50

100

150

200

300

350

400

450

500

250
t [sec]

Figure 22: Horizontal position INSE, VNSE, and IA-VNSE for scenario #2 MX (100 runs)

3D points depicted in the images on one side, and the estimated aircraft pose indicating the position and attitude

from where the images are taken on the other.

The major improvement in the horizontal position estimation capabilities enabled by the introduction of priors
within SVO imply that the the IA-VNS can be considered as a virtual ground velocity or incremental horizontal
displacement sensor, whose outputs can be periodically fed back to the inertial filter to create a fully integrated
VINS that successfully fuses the inertial and visual algorithms.

8

Influence of Terrain Type

This section analyzes the influence of the terrain type overflown by the aircraft on the capability of the IA-VNS
to estimate the aircraft pose when executing Monte Carlo simulations of both scenarios, each composed of 100

runs. The only variation among the different simulations is the terrain type, as all other parameters defining

each scenario (mission, aircraft, sensors, weather, wind, turbulence, geophysics, initial estimations) are exactly

the same for all simulation runs. Note however that each terrain type has an associated initial location and

applicable Earth gravitational and magnetic fields, which slightly impact the inertial estimations as described in

[5].

The SVO algorithms described in section 5 can not operate unless the feature detector can periodically locate
features in the various keyframes, and also requires the depth filter to correctly estimate the 3D terrain coordinates

of each feature. The texture and elevation relief of the overflown terrain hence play a key role in the ability of
the VNS and IA-VNS to estimate the aircraft pose. The use of different types of terrain is intended to provide
a more complete validation of the proposed visual algorithms by evaluating their performance when flying over

regions that differ in both their texture and vertical relief. Refer to section 4 for a detailed description of the

terrain types employed in each of the two scenarios.

Body Attitude Estimation

Tables 8 and 929 show the total attitude error aggregated final state metrics for both scenarios and all terrain

types. The corresponding time aggregated metrics are depicted in figures 2330 and 2431.

29Note that the MX columns of tables 8 and 9 coincide with the results shown in table 5.
30Note that the blue lines of figure 23 (top and bottom plots) are the same as the blue and orange lines of figure 16, respectively.
31Note that the blue lines of figure 24 (top and bottom plots) are the same as the blue and orange lines of figure 17, respectively.

36

Scenario #1 Zone

INSE

‖∆ˆrB‖ [∘] (tEND)

IA-VNSE

‖∆ ∘rB‖ [∘] (tEND)

MX

FR

FM

DS

0.158
0.114

0.163
0.124

0.163
0.123

0.150
0.102

0.611

0.735

0.678

0.600

0.218
0.103

0.240
0.124

0.264
0.116

0.243
0.125

0.606

0.684

0.675

0.692

mean

std

max

mean

std

max

Table 8: Influence of terrain type on final body attitude INSE and IA-VNSE for scenario #1 (100 runs)

These results indicate that although the terrain type impacts the attitude estimation accuracy, its influence is

minor when compared with other factors, such as the specific turbulence and sensor errors employed in each
execution. This assessment is true for the inertially assisted visual estimations, for which scenario #1 runs 35,
79, and 92 are particularly negative independently of the terrain type, as well as runs 16 and 91 for scenario #2,
but also for the inertial estimations, for which #1 runs 23 and 43 as well as #2 executions 16 and 73 are generally
the ones with worst results.

]
∘
[

‖
B
ˆr
∆
‖

]
∘
[

‖
B
∘r
∆
‖

0.4

0.3

0.2

0.1

0

0

0.4

0.3

0.2

0.1

0

0

MX baseline

FR

FM

DS

𝜇‖Δ^rB‖n ± 𝜎‖Δ^rB‖n INSE

Scenario #1

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

𝜇

‖Δ

∘rB‖i

± 𝜎

‖Δ

∘rB‖i

IA-VNSE

Scenario #1

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 23: Influence of terrain type on body attitude INSE and IA-VNSE for scenario #1 (100 runs)

The influence of the terrain type on the attitude INSE is indirect and caused by the Earth magnetic field intensity
and inclination. Each terrain type is associated with a certain Earth location and hence a different magnetic

field, which impacts the tracking of the aircraft attitude by the inertial filter, specially its bearing. With respect
to the IA-VNSE, the influence of the terrain type is very low given the significant variations in texture and vertical
relief among the various zones evaluated. Certain trends can however be identified:

Scenario #2 Zone

INSE

‖∆ˆrB‖ [∘] (tEND)

IA-VNSE

‖∆ ∘rB‖ [∘] (tEND)

MX

FR

FM

DS

UR

PR

0.128
0.078

0.137
0.087

0.136
0.085

0.125
0.073

0.125
0.074

0.119
0.066

0.369

0.401

0.412

0.377

0.383

0.319

0.221
0.137

0.240
0.138

0.227
0.140

0.226
0.138

0.219
0.140

0.215
0.138

0.788

0.858

0.907

0.771

0.763

0.797

mean

std

max

mean

std

max

Table 9: Influence of terrain type on final body attitude INSE and IA-VNSE for scenario #2 (100 runs)
37

• The MX zone provides the best IA-VNSE results for scenario #1 and third best for #2, although the differences
are slim. This is attributed to its benign characteristics, with abundant texture and continuous but slight

terrain altitude variations.

• The FR zone results in significantly worse IA-VNS attitude estimations than all other terrain types at the
beginning of both scenarios, which is attributed to a less accurate initial homography and scale determina-

tion caused by an initial location that differs the most from the flat terrain assumed by the homography.
The FR initial disadvantage slowly dissipates (note that it is still present at tEND = 500 s, when the alternate
scenario concludes) because of its favorable conditions (abundant features and significant but never extreme

vertical relief).

]
∘
[

‖
B
ˆr
∆
‖

]
∘
[

‖
B
∘r
∆
‖

0.4

0.3

0.2

0.1

0

0

0.4

0.3

0.2

0.1

0

0

MX baseline

FR

FM

DS

UR

PR

Scenario #2

𝜇‖Δ^rB‖n ± 𝜎‖Δ^rB‖n INSE

50

100

150

200

250

300

350

400

450

500

𝜇

‖Δ

∘rB‖i

± 𝜎

‖Δ

∘rB‖i

IA-VNSE

Scenario #2

50

100

150

200

250
t [s]

300

350

400

450

500

Figure 24: Influence of terrain type on body attitude INSE and IA-VNSE for scenario #2 (100 runs)

• The FM zone provides the worst IA-VNSE results for scenario #1, as well as second worst for #2, which is
expected as it contains by far the least favorable terrain characteristics from a visual odometry point of

view, with very scarce features and virtually no vertical relief to assist in the estimation of the aircraft pose.

• The DS zone a priori looks like an excellent location for visual navigation, with lots of features and continuous
terrain altitude variations, but it is penalized by the presence of a few very steep mountain ranges, which

challenge the depth filter when overflown, resulting in error growth not present in the other terrain zones.

• The UR and PR zones are only available for scenario #2, resulting in the lowest IA-VNS errors because of
their abundant texture. The lack of altitude relief would likely create some drift on the long term, but it is

not a problem with the continuous turns and short duration of this scenario.

Vertical Position Estimation

Tables 10 and 1132 show the vertical position error aggregated final state metrics for both scenarios and all terrain

types. The corresponding time aggregated metrics are depicted in figures 2533 and 2634.

32Note that the MX columns of tables 10 and 11 coincide with the results shown in table 6.
33Note that the blue lines of figure 25 (top and bottom plots) are the same as the blue and orange lines of figure 19, respectively.
34Note that the blue lines of figure 26 (top and bottom plots) are the same as the blue and orange lines of figure 20, respectively.

38

Scenario #1 Zone

INSE

∆ˆh [m] (tEND)

IA-VNSE ∆

∘

h [m] (tEND)

mean

std

max

MX
-4.18

25.78
-70.49

mean

+22.86

FR
-4.18

25.78
-70.48

+2.36

FM
-4.18

25.78
-70.49

DS
-4.18

25.78
-70.49

+77.66

+10.28

std

max

49.17
+175.76

52.33
59.23
72.00
-138.16 +302.93 +279.14

Table 10: Influence of terrain type on final vertical position INSE and IA-VNSE for scenario #1 (100 runs)

The INSE results are virtually the same for all terrain types because the inertial vertical position error only
depends on the pressure offset change from the time the GNSS signals are lost and on the ionospheric effects, but
not on the terrain type and associated Earth location [5].

MX baseline

FR

FM

DS

Scenario #1

𝜇Δ^hn ± 𝜎Δ^hn INSE

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

Scenario #1

𝜇

Δ

∘
hi

± 𝜎

Δ

∘
hi

IA-VNSE

]

m

[

ˆh
∆

]

m

[

∘h
∆

20

10

0

−10

−20

−30

125
100
75
50
25
0
−25
−50

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 25: Influence of terrain type on vertical position INSE and IA-VNSE for scenario #1 (100 runs)

The altitude IA-VNSE obtained with the various terrain types reinforces the conclusions obtained when discussing
the attitude error. They are more evident for scenario #1 than for #2 because of its longer duration and the fact
that the aircraft does not fly over previously visited areas. The lowest errors, not only for the standard deviation
, are obtained with the FR and DS terrain types, which can be attributed not
𝜎
only to their abundant texture that results in an elevated number of features at all times, but specially to the

but also for the mean 𝜇

END

END

∘
h

∘
h

continuous variation in the terrain altitude (refer to their descriptions in section 4), which in most cases can be

correctly estimated by the depth filter35 but that results in beneficial differences in the distance to the camera of

the different terrain 3D points visible in a given image.

Scenario #2 Zone

INSE

∆ˆh [m] (tEND)

mean

std

max

MX
+0.76

FR
+0.76

FM
+0.76

DS
+0.76

UR
+0.76

PR
+0.76

+7.55 +7.55 +7.55 +7.55 +7.55 +7.55
-19.86
-19.86

-19.86

-19.86

-19.86

-19.86

mean

+3.59

+2.39

+8.16

+2.29

+4.76

+3.44

IA-VNSE ∆

∘

h [m] (tEND)

std

max

+13.01 +15.50 +13.82 +13.17 +13.34 +13.08
+72.25
+71.64

+67.35

+71.90

+69.16

+77.36

35The exception are some small but very steep areas in the DS terrain type.

39

Table 11: Influence of terrain type on final vertical position INSE and IA-VNSE for scenario #2 (100 runs)

The FM zone lies on the opposite side, with significantly worse vertical position estimations for both scenarios, due
to the scarcity of texture in the prevalent rectangular plots of farmland, and the near absence of any vertical relief.
The SVO pipeline hence not only operates with a lower number of features, which increases the negative effects
of any outliers, but also employs features that are very similar to each other from a luminosity point of view and
whose terrain 3D altitude is virtually the same, increasing the chances of inaccuracies in the optimizations.

]

m

[

ˆh
∆

]

m

[

∘h
∆

10

5

0

−5

−10

0

MX baseline

FR

FM

DS

UR

PR

𝜇Δ^hn ± 𝜎Δ^hn INSE

Scenario #2

50

100

150

200

250

300

350

400

450

500

20

10

0

−10

−20

𝜇

∘
hi

Δ

± 𝜎

∘
hi

Δ

IA-VNSE

Scenario #2

0

50

100

150

200

250
t [s]

300

350

400

450

500

Figure 26: Influence of terrain type on vertical position INSE and IA-VNSE for scenario #2 (100 runs)

The MX terrain used as the baseline lies between the two extremes in both texture and vertical relief, and its
results hence are also intermediate. In the case of the UR and PR types, their abundance of features but lack of
vertical relief provides average alternate scenario results, but they would likely be worse for longer flights if it

were possible to locate big enough locations with these types of terrain.

Horizontal Position Estimation

Tables 12 and 1336 show the horizontal position error aggregated final state metrics for both scenarios and all

terrain types. The corresponding time aggregated metrics are depicted in figures 2737 and 2838.

Scenario #1 Zone

INSE

∆ˆxHOR [m,∘ ] (tEND)

IA-VNSE ∆ ∘xHOR [m,∘ ] (tEND)

MX

FR

FM

DS

7276

4880

7.10
5.69

7269

4865

7.09
5.67

7276

4873

7.10
5.67

7278

4892

7.10
5.71

25288

32.38

25281

32.19

25268

32.28

25331

32.65

488

350

1957

0.46
0.31

1.48

566

406

2058

0.53
0.38

1.71

489

322

1783

0.45
0.28

1.34

514

352

1667

0.48
0.31

1.37

mean

std

max

mean

std

max

Table 12: Influence of terrain type on final horizontal position INSE and IA-VNSE for scenario #1 (100 runs)

The horizontal position INSE results are very similar for all terrain types as the error originates when integrating
the horizontal velocity, whose errors are caused by the wind field variations from the time the GNSS signals are

36Note that the MX columns of tables 12 and 13 coincide with the results shown in table 7.
37Note that the blue lines of figure 27 (top and bottom plots) are the same as the blue and orange lines of figure 21, respectively.
38Note that the blue lines of figure 28 (top and bottom plots) are the same as the blue and orange lines of figure 22, respectively.

40

lost [5]. The influence of the terrain type by means of its associated Earth location and magnetic field is by

means of the body attitude estimation and hence very weak.

8,000

6,000

4,000

2,000

]

m

[

R
O
H
ˆx
∆

0

0

1,000

750

500

250

0

0

]

m

[

R
O
H
∘x
∆

MX baseline

FR

FM

DS

𝜇Δ^xHORn ± 𝜎Δ^xHORn INSE

Scenario #1

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

𝜇Δ

∘

xHORi ± 𝜎Δ

∘

xHORi IA-VNSE

Scenario #1

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 27: Influence of terrain type on horizontal position INSE and IA-VNSE for scenario #1 (100 runs)

The influence of the terrain type on the horizontal position IA-VNSE is very small, with slim differences among
the various evaluated terrains. The only terrain type that clearly deviates from the others is the FR, with
slight but consistently worse horizontal position estimations for both scenarios. This behavior stands out as
the abundant texture and continuous smooth vertical relief of the FR terrain is a priori beneficial for the visual
estimations.

Scenario #2 Zone
∆ˆxHOR&∆ ∘xHOR [m,∘ ] (tEND)

INSE

IA-VNSE

mean

std

max

mean

std

max

MX

FR

FM

DS

UR

PR

216

119

586

33

26

1.52
0.86

4.38

0.23
0.18

216

120

587

40

35

1.53
0.86

4.38

0.28
0.24

216

120

586

33

24

1.53
0.86

4.38

0.23
0.17

215

119

586

31

24

1.52
0.86

4.38

0.22
0.17

215

119

586

32

25

1.52
0.86

4.38

0.23
0.18

214

119

588

31

25

130

0.98

188

1.29

117

0.85

114

0.86

128

0.96

119

1.51
0.86

4.39

0.22
0.17

0.90

Table 13: Influence of terrain type on final horizontal position INSE and IA-VNSE for scenario #2 (100 runs)

Although beneficial for the SVO pipeline, the more pronounced vertical relief of the FR terrain type breaches
the flat terrain assumption of the initial homography, hampering its accuracy, and hence results in less precise

initial estimations, including that of the scale. The estimated scale is significantly more important for the
horizontal position estimation than for either the attitude or the vertical position, and the IA-VNS has no means
to compensate the initial errors, which remain approximately equal (percentage wise) for the full duration of

both scenarios.

A similar but opposite reasoning is applicable to the FM type and in a lesser degree to the UR and PR types.
Although a flat terrain in which all terrain features are located at a similar altitude is detrimental to the overall
accuracy of SVO, and results in slightly worse body attitude and vertical position estimations, it is beneficial
for the homography initialization and the scale determination, resulting in consistently more accurate horizontal

position estimations.

41

]

m

[

R
O
H
ˆx
∆

]

m

[

R
O
H
∘x
∆

250

200

150

100

50

0

100

80

60

40

20

0

MX baseline

FR

FM

DS

UR

PR

𝜇Δ^xHORn ± 𝜎Δ^xHORn INSE

0

50

100

150

200

250

300

350

400

450

500

Scenario #2

𝜇Δ

∘

xHORi ± 𝜎Δ

∘

xHORi IA-VNSE

Scenario #2

0

50

100

150

200

250
t [s]

300

350

400

450

500

Figure 28: Influence of terrain type on horizontal position INSE and IA-VNSE for scenario #2 (100 runs)

9

Influence of Sensor Quality

The IA-VNS relies on the inertial estimations provided by the INS to generate the priors needed to ensure that
the visual attitude and altitude estimations do not deviate in excess from their inertial counterparts, as well as

to quickly obtain a rough pose estimation that replaces the expensive and less accurate sparse image alignment

process (section 6).
[5] includes a detailed analysis of the influence of the quality or grade of each individual
sensor on the INSE. This analysis instead varies the quality of all onboard sensors simultaneously and does not
only analyze the influence of these changes on the INSE, but also on the final IA-VNSE. The analysis consists
on repeating the Monte Carlo simulation with the only change of replacing the performance parameters shown

in [8], referred to as the “baseline” configuration, with others representing higher or lower quality sensors, and
then comparing the results. As in the case of [5], such an analysis validates the proposed IA-VNS algorithms
against optimistic assumptions when modeling the errors introduced by the various sensors. By ensuring that the
IA-VNS also works as intended when relying on inertial estimations significantly worse than those obtained in the
baseline configuration of previous sections, the confidence on the results in significantly enhanced. The analysis
relies exclusively on scenario #1; similar conclusions would be obtained if employing scenario #2.

GYR

ACC

MAG
TAS-AOA-AOS
OSP-OAT

SENS Baseline
Baseline

SENS Better
Better

SENS Worse
Worse

Baseline

Baseline

Baseline

Baseline

Better

Better

Better

Baseline

Worse

Worse

Worse

Worse

Table 14: Composition of SENS configurations

Table 14 proposes two different sensor grade combinations in addition to the “SENS Baseline” configuration em-
ployed to obtain the results shown in sections 7 and 8. The specific values of the different sensor parameters
coincide with those employed in [5], and they are not repeated here. They consist of the “SENS Better” configura-
tion, which employs sensors of better quality or grade than those of the baseline39, which is generally associated
to higher SWaP and price, and the “SENS Worse” configuration, composed by sensors with a significantly worse

39With the exception of the barometer and thermometer, which are the same ones employed in the baseline.

42

performance.

In contrast with the sensitivity analyses of section 8 and [5], in which exactly the same algorithms and parameters

are employed with the exception of the subject factor, the sensitivity of the prior based pose optimization
adjustments in case of the “SENS Worse” configuration has been reduced while simultaneously increasing its
strength, to adjust for the expected lower accuracy of the inertial estimations on which it relies. The modified
parameters are displayed in table 15.
In addition, it has been necessary to slightly adjust the configuration
parameters of the “SENS Worse” inertial filter in a few runs to increase its stability.

Parameter

∘

∆𝜃LOW
𝜃∘∘
∆
1,MAX
∆𝜉LOW
𝜉∘∘
∆
1,MAX

∘

SENS Baseline
0.2∘
0.0005∘
0.2∘
0.0003∘

SENS Better
0.2∘
0.0005∘
0.2∘
0.0003∘

SENS Worse
0.3∘
0.0006∘
0.3∘
0.0004∘

Table 15: Pitch and bank adjustment setting for SENS configurations

Body Attitude Estimation

Table 1640 shows the total attitude error aggregated final state metrics for the scenario #1 over the MX terrain type
with all three SENS configurations. The corresponding time aggregated metrics are depicted in figure 2941.

]
∘
[

‖
B
ˆr
∆
‖

]
∘
[

‖
B
∘r
∆
‖

0.5

0.4

0.3

0.2

0.1

0

0

0.4

0.3

0.2

0.1

0

0

baseline

SENS better

SENS worse

𝜇‖Δ^rB‖n ± 𝜎‖Δ^rB‖n INSE

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

Scenario #1 MX

𝜇

‖Δ

∘rB‖i

± 𝜎

‖Δ

∘rB‖i

IA-VNSE

Scenario #1 MX

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 29: Influence of sensors quality on body attitude INSE and IA-VNSE for scenario #1 MX (100 runs)

The attitude INSE results show a clear relationship with the sensors quality or grade, with better sensors con-
sistently resulting in more accurate inertial attitude estimations. Note however that as explained in [5], this

dependency can be traced to the gyroscopes, magnetometers, and air velocity (Pitot tube and air vanes) sensors,

and not to the accelerometers and air data sensors, which do not impact the attitude estimation accuracy.

40Note that the baseline columns of table 16 coincide with the results shown in table 5.
41Note that the blue lines of figure 29 (top and bottom plots) are the same as the blue and orange lines of figure 16, respectively.

43

Scenario #1 MX
SENS Config.
[∘] (tEND)
mean

std

max

INSE
Baseline Better Worse

‖∆ˆrB‖

‖∆ˆrB‖

‖∆ˆrB‖

0.158
0.114

0.611

0.126
0.073

0.416

0.235
0.164

0.812

IA-VNSE
Baseline Better Worse
‖∆ ∘rB‖
‖∆ ∘rB‖
0.281
0.204
0.137
0.109

‖∆ ∘rB‖
0.218
0.103

0.606

0.582

0.803

Table 16: Influence of sensors quality on final body attitude INSE and IA-VNSE for scenario #1 MX (100 runs)

A similar dependency can be observed for the attitude IA-VNSE, although the difference between the “SENS
Better” and baseline configurations is very slim. Although not tested, it is likely that the “SENS Better” results
could be improved if employing different prior based pose optimization parameters to take advantage of the more
accurate inertial estimations, similarly to the changes introduced in table 15 for the “SENS Worse” configuration.
Note also that most of the IA-VNSE is already present at the beginning of the scenario, as the IA-VNS employs
the inertial estimations for its initialization (section 5).

Vertical Position Estimation

Table 1742 shows the vertical position error aggregated final state metrics for the scenario #1 over the MX
terrain type and all three SENS configurations. The corresponding time aggregated metrics are depicted in figure
3043.

]

m

[

ˆh
∆

]

m

[

∘h
∆

20

10

0

−10

−20

−30

100

75

50

25

0
−25

−50

baseline

SENS better

SENS worse

Scenario #1 MX

𝜇Δ^hn ± 𝜎Δ^hn INSE

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

Scenario #1 MX

𝜇

∘
hi

Δ

± 𝜎

Δ

∘
hi

IA-VNSE

0

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 30: Influence of sensors quality on vertical position INSE and IA-VNSE for scenario #1 MX (100 runs)

As indicated in [5], the vertical position INSE is independent of the quality of all sensors, and depends exclusively
on ionospheric effects and the atmospheric pressure offset change since the time the GNSS signals are lost. The
vertical position IA-VNSE however shows a slight dependency with the quality or grade of the onboard sensors
as a consequence of the activation of the prior based pose optimization process (section 6). Note however that

of the three differences between the visual and inertial estimations that control the pitch and bank adjustment

mechanism (pitch, bank, and altitude differences), only the first two are responsible for the impact of the sensors

grade on the vertical position inertially assisted visual estimation.

42Note that the baseline columns of table 17 coincide with the results shown in table 6.
43Note that the blue lines of figure 30 (top and bottom plots) are the same as the blue and orange lines of figure 19, respectively.

44

Scenario #1 MX
SENS Config.
[m] (tEND)
mean

std

max

INSE
Baseline Better Worse
∆ˆh
-4.15

∆ˆh
-4.18

∆ˆh
-4.18

Baseline
∘
h
+22.86

∆

IA-VNSE
Better Worse
∘
h
+28.74

∘
h
+20.74

∆

∆

25.78
-70.49

25.78
-70.50

25.80
-69.74

49.17

59.40
48.36
+175.76 +138.61 +279.11

Table 17: Influence of sensors quality on final vertical position INSE and IA-VNSE for scenario #1 MX (100 runs)

Horizontal Position Estimation

Table 1844 shows the horizontal position error aggregated final state metrics for the MX scenario #1 and all three
SENS configurations. The corresponding time aggregated metrics are depicted in figure 3145.

8,000

6,000

4,000

2,000

]

m

[

R
O
H
ˆx
∆

0

0

1,000

750

500

250

0

0

]

m

[

R
O
H
∘x
∆

baseline

SENS better

SENS worse

𝜇Δ^xHORn ± 𝜎Δ^xHORn INSE

Scenario #1 MX

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

𝜇Δ

∘

xHORi ± 𝜎Δ

∘

xHORi IA-VNSE

Scenario #1 MX

500

1,000

1,500

2,000

2,500

3,000

3,500 3,800

t [s]

Figure 31: Influence of sensors quality on horizontal position INSE and IA-VNSE for scenario #1 MX (100 runs)

The horizontal position INSE results confirm the conclusions obtained in [5]. The error originates when integrating
the ground velocity errors, which can be attributed to the wind velocity change from the time the GNSS signals
are lost, with smaller contributions from the air velocity and body attitude inertial estimation errors. The last
two factors are responsible for the minor variations in horizontal position INSE caused by changes in the quality
of the onboard sensors.

Scenario #1 MX
SENS Config.
[m, %] (tEND)
mean

std

max

Baseline

∆ˆxHOR

INSE
Better

∆ˆxHOR

Worse

∆ˆxHOR

Baseline
∆ ∘xHOR

IA-VNSE
Better
∆ ∘xHOR

Worse
∆ ∘xHOR

7276

4880

7.10
5.69

7149

4806

6.97
5.61

7499

4958

7.33
5.81

488

350

0.46
0.31

476

363

0.45
0.33

578

364

25288

32.38

25116

31.49

25731

33.24

1957

1.48

1934

1.46

2094

0.54
0.32

1.58

Table 18: Influence of sensors quality on final horizontal position INSE and IA-VNSE for scenario #1 (100 runs)

With respect to the horizontal position IA-VNSE, its variation with the quality of the sensors also relies on the

44Note that the baseline columns of table 18 coincide with the results shown in table 7.
45Note that the blue lines of figure 31 (top and bottom plots) are the same as the blue and orange lines of figure 21, respectively.

45

differences between the visual and inertial pitch and bank estimations that determine the activation of the prior

based adjustments within the pose optimization process. As in the case of the attitude and altitude visual

estimations, there are two reasons why the simulation results indicate a significant degradation in accuracy when
employing the “SENS Worse” configuration instead of the baseline, but only a slight improvement when using
the “SENS Better” sensors. The first reason is that, as explained in [5], there exists significantly more room for
deterioration in the body attitude INSE (including the pitch and bank estimations related to the prior based
pose adjustment) than for improvement, which is caused by the characteristics of GNSS-Denied navigation,

and in particular by the multiple inaccuracies present in the attitude filter that prevent the tracking of the full
accelerometer error EACC. The second reason lies in the use of the same pitch and bank adjustment settings for the
“SENS Better” and baseline configurations, but customized ones for “SENS Worse” (table 15). Although not tested,
slightly more sensitive (lower) ∆𝜃LOW and ∆𝜉LOW parameters coupled with slightly weaker (lower) ∆
and
∆

settings would likely result in some improvements for the “SENS Better” configuration.

𝜃∘∘
1,MAX

∘

∘

𝜉∘∘
1,MAX

10 Summary of Results

This article proposes an SVO based IA-VNS installed onboard a fixed wing autonomous UAV that takes advantage of
the GNSS-Denied estimations provided by an inertial filter (INS) to assist the visual pose optimization algorithms.
The method is inspired in a PI control system, in which the attitude, altitude, and rate of climb inertial outputs
act as targets to ensure that the visual estimations do not deviate far from their inertial counterparts, resulting
in major improvements when estimating the aircraft horizontal position without the use of GNSS signals. The
results obtained when applying the proposed algorithms to high fidelity Monte Carlo simulations of two scenarios
representative of the challenges of GNSS-Denied navigation indicate the following:

• The body attitude estimation shows significant quantitative improvements over a stand-alone VNS in both
pitch and bank angle estimations, with no negative influence on the yaw angle estimations. A small amount

of drift with time is present, and can not be fully eliminated. Body pitch and bank angle estimations do not
deviate far from their INS counterparts, while the body yaw angle visual estimation is significantly better
than that obtained by the INS.

• The vertical position estimation shows major improvements over that of a stand-alone VNS, not only
quantitatively but also qualitatively, as drift is fully eliminated. The visual estimation does not deviate in

excess from the inertial one, which is bounded by atmospheric physics.

• The horizontal position estimation, whose improvement is the main objective of the proposed algorithm,
shows major gains when compared to either the stand-alone VNS or the INS, although drift is still present.

The conclusions of sensitivity analyses on the influence of the estimation results with respect to the overflown

terrain type as well as the quality or grade of the onboard sensors on which the pose adjustments rely are the

following:

• The terrain texture (or lack of) and its elevation relief are key factors for the IA-VNS visual odometry
algorithms. Their influence on the aircraft pose estimation results are however slim with the only exception

of regular plots of farmland characterized by a scarcity of features and zero vertical relief, over which the
obtained results suffer some degradation.

• The quality of the onboard sensors influences the accuracy of the inertial estimations on which the
visual targets rely, and although the IA-VNS quantitative pose estimation results vary based on the sensor
grade, the qualitative conclusions listed above are independent from them.

11 Conclusions

The proposed inertially assisted VNS (IA-VNS), which in addition to the images taken by an onboard camera also
relies on the outputs of an INS specifically designed for the challenges faced by autonomous fixed wing aircraft
46

that encounter GNSS-Denied conditions, possesses significant advantages in both accuracy and resilience when
compared with a stand-alone VNS, the most important of which is a major reduction in its horizontal position
drift. The advantages occur independently of the quality of the onboard sensors, and when flying over a variety
of terrain types. The IA-VNS can hence be considered as a virtual incremental horizontal displacement or ground
velocity sensor, whose measurements can be fed back to the inertial filter, resulting in a fully integrated VINS.
The proposed IA-VNS can significantly increase the possibilities of the aircraft safely reaching the vicinity of the
intended recovery location upon the loss of GNSS signals, from where it can be landed by remote control.

A Optical Flow

Let’s consider a pinhole camera [47] (one that adopts an ideal perspective projection) such as that depicted in
figure 32 that is moving with respect to the Earth while maintaining within its field of view a given point p
fixed to the Earth surface. The composition of positions and its time derivation, considering the ECEF as FE, the
camera frame as FC, and a frame FP with its origin in the terrain point p that does not move with respect to FE,
results in the following expression when viewed in FC:

E

TE
˙T

EP = TE
EP = ˙REC TC
EP = REC vC
vE

CP + TE

EC = REC TC

CP + TE

EC

CP + REC

CP + vE

E

C

CP + ˙T
˙T
EC + REC ̂︀𝜔C
EC + vC

CP + ̂︀𝜔C

vC

EP = RCE vE

EP = vC

EC TC

CP = 0

EC = REC ̂︀𝜔C
EC TC
CP = vE

CP + vE

EC TC

CP + REC
EC + ̂︀𝜔E

EC TE

CP

˙T

C

CP + ˙T

E

EC

(40)

(41)

(42)

(43)

pC

Optical Center

iC
1

SV

pIMG

Principal point

Principal axis

OC

iC
2

iC
3

f

iIMG
1

OIMG

SH

iIMG
2

Focal Distance

Focal plane

Figure 32: Frontal pinhole camera model

Note that (43) connects the point coordinates as viewed from the camera TC
CP = ˙pC with the twist 𝜉C
vC
frame, which is composed by its linear and angular velocities vC

CP = pC and their time derivative
of the motion of the camera with respect to the Earth viewed in the FC or local
and 𝜔C

[4].

EC

EC

EC

vC

CP = ˙pC = −vC

EC − ̂︀𝜔C

EC TC

CP = −vC

EC − ̂︀𝜔C

EC pC

(44)

The homogeneous camera coordinates ¯pC are defined as the ratio between the camera coordinates pC and its
, and represent an alternative view to pIMG = Π (pC) of how the point is projected
third coordinate or depth pC
3
in the image. Its time derivative is hence:

¯pC =

pC
pC
3

−→ ˙¯pC =

3 ˙pC − ˙pC
pC
pC 2

3

3 pC

(45)

47

Substituting both pC and pC
within (44) into (45), rearranging terms, and considering the relationship between
3
the image and homogeneous camera coordinates, leads to the following expression for the optical flow [89] or
variation of the point image coordinates:

˙pIMG = J

+ Π (︀g−1
⊕ ℳ

ℳEC

(pE))︀

𝜉C
EC = f

⎡

−

⎢
⎢
⎣

1
pC
3

0

0

−

1
pC
3

¯pC
1
pC
3
¯pC
2
pC
3

1 ¯pC
¯pC
2

−1 − ¯pC 2

1

¯pC
2

1 + ¯pC 2

2

−¯pC

1 ¯pC
2

−¯pC
1

⎤

⎥
⎥
⎦

]︃

[︃

vC
EC
𝜔C
EC

(46)

Considering that the twist 𝜉 is the time derivative of the transform vector 𝜏 [4], the optical flow jacobian is
defined as the derivative of the local frame ideal perspective projection of a point fixed to the spatial frame with
respect to the SE(3) element ℳ caused by a perturbation ∆𝜏 in its local tangent space:

J

+ Π (gℳ(p))
⊕ ℳ

=

lim
Δ𝜏 →0

Π(︀gℳ⊕Δ𝜏 (p) )︀ − Π(︀gℳ (p) )︀
∆𝜏

∈ R2x6

Π(︀gℳ⊕Δ𝜏 (p) )︀ ≈ Π(︀gℳ (p) )︀ +

[︂

J

+ Π(︀gℳ(p))︀
⊕ ℳ

]︂

∆𝜏

∈ R2

(47)

(48)

Less formally, the optical flow jacobian represents how the projection of a fixed point moves within the image as

the camera pose varies. Note that the jacobian only depends on the point camera (local) coordinates and the

camera focal length, and that as all terms multiplying the linear twist component are divided by the image depth

, the effect on the image of a bigger linear velocity can not be distinguished from that of a smaller depth.

pC
3

References

[1] E. Gallo, “High Fidelity Flight Simulation for an Autonomous Low SWaP Fixed Wing UAV in GNSS-Denied
Conditions.” https://github.com/edugallogithub/gnssdenied_flight_simulation, 2020. C++ open
source code.

[2] E. Gallo, “The SO(3) and SE(3) Lie Algebras of Rigid Body Rotations and Motions and their Application
to Discrete Integration, Gradient Descent Optimization, and State Estimation,” 2022. arXiv:2205.12572v1
[cs.RO], https://doi.org/10.48550/arXiv.2205.12572.

[3] J. Sola, “Quaternion Kinematics for the Error-State Kalman Filter,” 2017. arXiv:1711.02508v1 [cs.RO],

https://doi.org/10.48550/arXiv.1711.02508.

[4] J. Sola, J. Deray, and D. Atchuthan, “A Micro Lie Theory for State Estimation in Robotics,” 2018.

arXiv:1812.01537v9 [cs.RO], https://doi.org/10.48550/arXiv.1812.01537.

[5] E. Gallo and A. Barrientos, “Reduction of GNSS-Denied Inertial Navigation Errors for Fixed Wing Au-
tonomous Unmanned Air Vehicles,” Aerospace Science and Technology, 2021. https://doi.org/10.1016/
j.ast.2021.107237.

[6] E. Gallo, “Quasi Static Atmospheric Model for Aircraft Trajectory Prediction and Flight Simulation,” 2021.

arXiv:2101.10744v1 [eess.SY], https://doi.org/10.48550/arXiv.2101.10744.

[7] E. Gallo, “Stochastic High Fidelity Simulation and Scenarios for Testing of Fixed Wing Autonomous
GNSS-Denied Navigation Algorithms,” 2021. arXiv:2102.00883v3 [cs.RO], https://doi.org/10.48550/
arXiv.2102.00883.

[8] E. Gallo, “Customizable Stochastic High Fidelity Model of the Sensors and Camera onboard a Low SWaP
Fixed Wing Autonomous Aircraft,” 2021. arXiv:2102.06492v3 [cs.RO], https://doi.org/10.48550/
arXiv.2102.06492.

[9] M. Hassanalian and A. Abdelkefi, “Classifications, Applications, and Design Challenges of Drones: A Re-
view,” Progress in Aerospace Sciences, 2017. https://doi.org/10.1016/j.paerosci.2017.04.003.

48

[10] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil, N. S. Othman, A. Khreishah,

and M. Guizani, “Unmanned Aerial Vehicles (UAVs): A Survey on Civil Applications and Key Research
Challenges,” IEEE Access, 2019. https://doi.org/10.1109/ACCESS.2019.2909530.

[11] S. Bijjahalli, R. Sabatini, and A. Gardi, “Advances in Intelligent and Autonomous Navigation Systems for
Small UAS,” Progress in Aerospace Sciences, 2020. https://doi.org/10.1016/j.paerosci.2020.100617.

[12] J. A. Farrell, Aided Navigation, GPS with High Rate Sensors. McGraw-Hill, Electronic Engineering Series,

2008.

[13] P. D. Groves, Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems. Artech House,

GNSS Technology and Application Series, 2008.

[14] A. B. Chatfield, Fundamentals of High Accuracy Inertial Navigation. American Institute of Aeronautics and

Astronautics, Progress in Astronautics and Aeronautics, vol 174, 1997.

[15] M. Elbanhawi, A. Mohamed, R. Clothier, J. Palmer, M. Simic, and S. Watkins, “Enabling Technologies
for Autonomous MAV Operations,” Progress in Aerospace Sciences, 2017. https://doi.org/10.1016/j.
paerosci.2017.03.002.

[16] R. Sabatini, T. Moore, and S. Ramasamy, “Global Navigation Satellite Systems Performance Analysis and
Augmentation Strategies in Aviation,” Progress in Aerospace Sciences, 2017. https://doi.org/10.1016/
j.paerosci.2017.10.002.

[17] S. Thombre, M. Z. H. Bhuiyan, P. Eliardsson, B. Gabrielsson, M. Pattison, M. Dumville, D. Fryganiotis,
S. Hill, V. Manikundalam, M. Poloskey, S. Lee, L. Ruotsalainen, S. Soderholm, and H. Kuusniemi, “GNSS
Threat Monitoring and Reporting: Past, Present, and a Proposed Future,” The Journal of Navigation, 2018.
https://doi.org/10.1017/S0373463317000911.

[18] G. Zhang and L. Hsu, “Intelligent GNSS/INS Integrated Navigation System for a Commercial UAV Flight
Control System,” Aerospace Science and Technology, 2018. https://doi.org/10.1016/j.ast.2018.07.
026.

[19] C. Tippitt, A. Schultz, and W. Procino, “Vehicle Navigation: Autonomy Through GPS-Enabled and GPS-

Denied Environments,” state of the art report DSIAC-2020-1328, Defense Systems Information Analysis

Center, December 2020.

[20] N. Gyagenda, J. V. Hatilima, H. Roth, and V. Zhmud, “A Review of GNSS Independent UAV Navigation
Techniques,” Robotics and Autonomous Systems, 2022. https://doi.org/10.1016/j.robot.2022.104069.

[21] R. Kapoor, S. Ramasamy, A. Gardi, and R. Sabatini, “UAV Navigation using Signals of Opportunity in Urban
Environments: A Review,” Energy Procedia, 2017. https://doi.org/10.1016/j.egypro.2017.03.156.

[22] A. Coluccia, F. Ricciato, and G. Ricci, “Positioning Based on Signals of Opportunity,” IEEE Communications

Letters, 2014. https://doi.org/10.1109/LCOMM.2013.123013.132297.

[23] S. T. Goh, O. Abdelkhalik, and S. A. Zekavat, “A Weighted Measurement Fusion Kalman Filter Implemen-
tation for UAV Navigation,” Aerospace Science and Technology, 2013. https://doi.org/10.1016/j.ast.
2012.11.012.

[24] A. Couturier and M. A. Akhloufi, “A Review on Absolute Visual Localization for UAV,” Robotics and

Autonomous Systems, 2020. https://doi.org/10.1016/j.robot.2020.103666.

[25] T. White, J. Wheeler, C. Lindstrom, R. Christensen, and K. R. Moon, “GPS-Denied Navigation Using SAR
Images and Neural Networks,” 2020. arXiv:2010.12108v1 [cs.CV], https://doi.org/10.48550/arXiv.
2010.12108.

[26] Z. Sjanic and F. Gustafsson, “Simultaneous Navigation and Synthetic Aperture Radar Focusing,” IEEE
Transactions on Aerospace and Electronic Systems, 2015. https://doi.org/10.1109/TAES.2015.120820.

49

[27] D. O. Nitti, F. Bovenga, M. T. Chiaradia, M. Greco, and G. Pinelli, “Feasibility of Using Synthetic Aperture

Radar to Aid UAV Navigation,” Sensors, 2015. https://doi.org/10.3390/s150818334.

[28] G. Csaba, L. Somlyai, and Z. Vamossy, “Mobil Robot Navigation using 2D LIDAR,” IEEE 16thWorld Sym-
posium on Applied Machine Intelligence and Informatics (SAMI), 2018. https://doi.org/10.1109/SAMI.
2018.8324002.

[29] G. Hemann, S. Singh, and M. Kaess, “Long Range GPS-Denied Aerial Inertial Navigation with LIDAR
Localization,” IEEE/RSJ International Conference on Intelligent Robots and Systems (iROS), 2016. https:
//doi.org/10.1109/IROS.2016.7759267.

[30] M. M. Miller, A. Soloviev, M. U. de Haag, M. Veth, J. Raquet, T. J. Klausutis, and J. E. Touma, “Navigation
in GPS Denied Environments: Feature-Aided Inertial Systems,” Tech. Rep. RTO-EN-SET-116, NATO, 2010.

[31] H. Goforth and S. Lucey, “GPS-Denied UAV Localization using Pre Existing Satellite Imagery,” in IEEE In-
ternational Conference on Robotics and Automation, IEEE, 2019. https://doi.org/10.1109/ICRA.2019.
8793558.

[32] N. Ziaei, “Geolocation of an Aircraft using Image Registration Coupling Modes for Autonomous Navigation,”

2019. arXiv:1909.02875v1 [eess.IV], https://doi.org/10.48550/arXiv.1909.02875.

[33] T. Wang, “Augmented UAS Navigation in GPS Denied Terrain Environments using Synthetic Vision,”
Iowa State University Graduate Theses and Dissertations, 15835, 2018. https://doi.org/10.31274/
etd-180810-5462.

[34] M. D. Pritt and K. J. LaTourette, “Aircraft Navigation by means of Image Registration,” IEEE Applied
Imagery Pattern Recognition Workshop (AIPR), 2013. https://doi.org/10.1109/AIPR.2013.6749335.

[35] D. Scaramuzza and F. Fraundorfer, “Visual Odometry Part 1: The First 30 Years and Fundamentals,” IEEE

Robotics & Automation Magazine, 2011. https://doi.org/10.1109/MRA.2011.943233.

[36] F. Fraundorfer and D. Scaramuzza, “Visual Odometry Part 2: Matching, Robustness, Optimization, and Ap-
plications,” IEEE Robotics & Automation Magazine, 2012. https://doi.org/10.1109/MRA.2012.2182810.

[37] D. Scaramuzza, “Tutorial on Visual Odometry.” Robotics & Perception Group, University of Zurich, 2012.

[38] D. Scaramuzza, “Visual Odometry and SLAM: Past, Present, and the Robust Perception Age.” Robotics &

Perception Group, University of Zurich, 2017.

[39] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past,
Present, and Future of Simultaneous Localization and Mapping: Towards the Robust Perception Age,” IEEE
Transactions on Robotics, 2016. https://doi.org/10.1109/TRO.2016.2624754.

[40] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast Semi-Direct Monocular Visual Odometry,” in
IEEE International Conference on Robotics and Automation, 2014. https://doi.org/10.1109/ICRA.2014.
6906584.

[41] C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza, “SVO: Semidirect Visual Odometry
for Monocular and Multicamera Systems,” IEEE Transactions on Robotics, 2016. https://doi.org/10.
1109/TRO.2016.2623335.

[42] J. Engel, V. Koltun, and D. Cremers, “Direct Sparse Odometry,” IEEE Transactions on Pattern Analysis

and Machine Intelligence, 2018. https://doi.org/10.1109/TPAMI.2017.2658577.

[43] J. Engel, T. Schops, and D. Cremers, “LSD-SLAM: Large Scale Direct Monocular SLAM,” European Con-

ference on Computer Vision, 2014. https://doi.org/10.1007/978-3-319-10605-2_54.

[44] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “ORB-SLAM: a Versatile and Accurate Monocular SLAM

System,” IEEE Transactions on Robotics, 2015. https://doi.org/10.1109/TRO.2015.2463671.

50

[45] R. Mur-Artal and J. D. Tardos, “ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo, and
RGB-D Cameras,” IEEE Transactions on Robotics, 2017. https://doi.org/10.1109/TRO.2017.2705103.

[46] R. Mur-Artal, Real-Time Accurate Visual SLAM with Place Recognition. PhD thesis, University of Zaragoza,

2017. https://doi.org/10.1109/TRO.2017.2705103.

[47] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. Cambridge University Press,

2nd ed., 2003.

[48] Y. Ma, S. Soatto, J. Kosecka, and S. S. Sastry, An Invitation to 3-D Vision, From Images to Geometric

Models. Imaging, Vision, and Graphics, Springer, 2001.

[49] M. A. Fischler and R. C. Bolles, “RANSAC Sample Consensus: A Paradigm for Model Fitting with Ap-
plications to Image Analysis and Automated Cartography,” Communications of the ACM, 1981. https:
//doi.org/10.1145/358669.358692.

[50] D. Scaramuzza and Z. Zhang, “Visual-Inertial Odometry of Aerial Robots,” 2019. arXiv:1906.03289v2

[cs.RO], https://doi.org/10.48550/arXiv.1906.03289.

[51] G. Huang, “Visual-Inertial Navigation: A Concise Review,” 2019. arXiv:1906.02650v1 [cs.RO], https:

//doi.org/10.48550/arXiv.1906.02650.

[52] L. von Stumberg, V. Usenko, and D. Cremers, “Chapter 7 - A Review and Quantitative Evaluation of Direct
Visual Inertial Odometry,” in Multimodal Scene Understanding (M. Y. Yang, B. Rosenhahn, and V. Murino,
eds.), Academic Press, 2019.

[53] X. Feng, Y. Jiang, X. Yang, M. Du, and X. Li, “Computer Vision Algorithms and Hardware Implementations:
A Survey,” Integration, the VLSI Journal, 2019. https://doi.org/10.1016/j.vlsi.2019.07.005.

[54] A. Al-Kaff, D. Martin, F. Garcia, A. de la Escalera, and J. Maria, “Survey of Computer Vision Algorithms
and Applications for Unmanned Aerial Vehicles,” Expert Systems With Applications, 2017. https://doi.
org/10.1016/j.eswa.2017.09.033.

[55] A. I. Mourikis and S. I. Roumeliotis, “A Multi-State Constraint Kalman Filter for Vision-aided Inertial
Navigation,” Proceedings 2007 IEEE International Conference on Robotics and Automation, 2007. https:
//doi.org/10.1109/ROBOT.2007.364024.

[56] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and R. Siegwart, “Keyframe Based Visual
Inertial SLAM Using Nonlinear Optimization,” Robotics: Science and Systems, 2013. https://doi.org/
10.3929/ethz-b-000236658.

[57] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, “Keyframe Based Visual Inertial SLAM
Using Nonlinear Optimization,” The International Journal of Robotics Research, 2015. https://doi.org/
10.1177/0278364914554813.

[58] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust Visual Inertial Odometry Using a Direct EKF
Based Approach,” International Conference of Intelligent Robot Systems, 2015. https://doi.org/10.3929/
ethz-a-010566547.

[59] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile Monocular Visual Inertial State Estimator,”

IEEE Transactions on Robotics, 2018. https://doi.org/10.1109/TRO.2018.2853729.

[60] S. Lynen, M. W. Achtelik, S. Weiss, M. Chli, and R. Siegwart, “A Robust and Modular Multi Sensor
Fusion Approach Applied to MAV Navigation,” International Conference of Intelligent Robot Systems, 2013.
https://doi.org/10.1109/IROS.2013.6696917.

[61] M. Faessler, F. Fontana, C. Forster, E. Mueggler, M. Pizzoli, and D. Scaramuzza, “Autonomous, Vision Based
Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle,” Journal of Field Robotics, 2015.
https://doi.org/10.1002/rob.21581.

51

[62] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “On Manifold Pre Integration for Real Time Visual
Inertial Odometry,” IEEE Transactions on Robotics, 2017. https://doi.org/10.1109/TRO.2016.2597321.

[63] M. Kaess, H. Johannsson, R. Roberts, V. Ila, J. Leonard, and F. Dellaert, “iSAM2: Incremental Smoothing
and Mapping Using the Bayes Tree,” The International Journal of Robotics Research, 2012. https://doi.
org/10.1177/0278364911430419.

[64] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The
EuRoC MAV Datasets,” IEEE International Journal of Robotics Research, 2016. https://doi.org/10.
1177/0278364915620033.

[65] J. Delmerico and D. Scaramuzza, “A Benchmark Comparison of Monocular Visual-Inertial Odometry Al-
gorithms for Flying Robots,” IEEE International Conference on Robotics and Automation, 2018. https:
//doi.org/10.1109/ICRA.2018.8460664.

[66] R. Mur-Artal and J. M. M. Montiel, “Visual Inertial Monocular SLAM with Map Reuse,” IEEE Robotics

and Automation Letters, 2017. https://doi.org/10.1109/LRA.2017.2653359.

[67] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet: Visual-Inertial Odometry as a Sequence-
to-Sequence Learning Problem,” Proceedings of the AAAI Conference on Artificial Intelligence, 2017. https:
//ojs.aaai.org/index.php/AAAI/article/view/11215.

[68] M. K. Paul, K. Wu, J. A. Hesch, E. D. Nerurkar, and S. I. Roumeliotis, “A Comparative Analysis of
Tightly Coupled Monocular, Binocular, and Stereo VINS,” IEEE International Conference on Robotics and
Automation, 2017. https://doi.org/10.1109/ICRA.2017.7989022.

[69] Y. Song, S. Nuske, and S. Scherer, “A Multi Sensor Fusion MAV State Estimation from Long Range Stereo,

IMU, GPS, and Barometric Sensors,” Sensors, 2017. https://doi.org/10.3390/s17010011.

[70] A. Solin, S. Cortes, E. Rahtu, and J. Kannala, “PIVO: Probabilistic Inertial Visual Odometry for Occlusion
Robust Navigation,” IEEE Winter Conference on Applications of Computer Vision, 2018. https://doi.
org/10.1109/WACV.2018.00073.

[71] S. Houben, J. Quenzel, N. Krombach, and S. Behnke, “Efficient Multi Camera Visual Inertial SLAM for
Micro Aerial Vehicles,” IEEE/RSJ International Conference on Intelligent Robots and Systems, 2016. https:
//doi.org/10.1109/IROS.2016.7759261.

[72] K. Eckenhoff, P. Geneva, and G. Huang, “Direct Visual Inertial Navigation with Analytical Preintegration,”
IEEE International Conference on Robotics and Automation, 2017. https://doi.org/10.1109/ICRA.2017.
7989171.

[73] H. Strasdat, J. M. M. Montiel, and A. J. Davison, “Real Time Monocular SLAM: Why Filter?,” IEEE Inter-
national Conference on Robotics and Automation, 2010. https://doi.org/10.1109/ROBOT.2010.5509636.

[74] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. Davison, J. Con-
radt, K. Daniilidis, and D. Scaramuzza, “Event Based Cameras: A Survey,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2022. https://doi.org/10.1109/TPAMI.2020.3008413.

[75] E. Mueggler, G. Gallego, H. Rebecq, and D. Scaramuzza, “Continuous Time Visual Inertial Odometry for
Event Cameras,” IEEE Transactions on Robotics, 2018. https://doi.org/10.1109/TRO.2018.2858287.

[76] P. Stojakovic and B. Rasuo, “Single Propeller Airplane Minimal Flight Speed based upon the Lateral Maneu-
ver Condition,” Aerospace Science and Technology, 2016. https://doi.org/10.1016/j.ast.2015.12.012.

[77] P. Stojakovic, K. Velimirovic, and B. Rasuo, “Power Optimization of a Single Propeller Airplane Take-
Off Run on the Basis of Lateral Maneuver Limitations,” Aerospace Science and Technology, 2018. https:
//doi.org/10.1016/j.ast.2017.10.015.

[78] “osgEarth.” http://osgearth.org.

52

[79] “Open Scene Graph.” http://openscenegraph.org.

[80] P. J. Huber, Robust Statistics. John Wiley & Sons, 1981.

[81] J. Fox and S. Weisberg, “Robust regression,” 2013.

[82] S. Baker and I. Matthews, “Lucas-Kanade 20 Years On: A Unifying Framework,” International Journal of

Computer Vision, 2004. https://doi.org/10.1023/B:VISI.0000011205.11775.fd.

[83] O. Faugeras and F. Lustman, “Motion and Structure from Motion in a Piecewise Planar Environment,”
International Journal of Pattern Recognition and Artificial Intelligence, 1998. https://doi.org/10.1142/
S0218001488000285.

[84] S. Baker, R. Gross, and I. Matthews, “Lucas-kanade 20 years on: A unifying framework: Part 4,” Tech. Rep.

CMU-RI-TR-04-14, Carnegie Mellon University, 2004.

[85] K. Ogata, Modern Control Engineering. Prentice Hall, 4th ed., 2002.

[86] S. Skogestad and I. Postlethwaite, Multivariable Feedback Control: Analysis and Design. John Wiley & Sons,

2nd ed., 2005.

[87] B. L. Stevens and F. L. Lewis, Aircraft Control and Simulation. John Wiley & Sons, 2nd ed., 2003.

[88] G. F. Franklin, J. D. Powell, and M. Workman, Digital Control of Dynamic Systems. Ellis-Kagle Press,

3rd ed., 1998.

[89] David J. Heeger, “Notes on Motion Estimation,” 1998.

53

