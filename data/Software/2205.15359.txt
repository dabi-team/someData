2
2
0
2

y
a
M
0
3

]

R
C
.
s
c
[

1
v
9
5
3
5
1
.
5
0
2
2
:
v
i
X
r
a

CTR: Checkpoint, Transfer, and Restore
for Secure Enclaves

Yoshimichi Nakatsuka
nakatsuy@uci.edu
University of California, Irvine
USA

Ercan Ozturk
ercano@uci.edu
University of California, Irvine
USA

Alex Shamis
alexsha@microsoft.com
Microsoft Research and
Imperial College London
UK

Andrew Paverd
andrew.paverd@microsoft.com
Microsoft Research and
Microsoft Security Response Center
UK

Peter Pietzuch
prp@imperial.ac.uk
Microsoft Research and
Imperial College London
UK

Abstract
Hardware-based Trusted Execution Environments (TEEs)
are becoming increasingly prevalent in cloud computing,
forming the basis for confidential computing. However, the
security goals of TEEs sometimes conflict with existing cloud
functionality, such as VM or process migration, because TEE
memory cannot be read by the hypervisor, OS, or other soft-
ware on the platform. Whilst some newer TEE architectures
support migration of entire protected VMs, there is currently
no practical solution for migrating individual processes con-
taining in-process TEEs. The inability to migrate such pro-
cesses leads to operational inefficiencies or even data loss if
the host platform must be urgently restarted.

We present CTR, a software-only design to retrofit mi-
gration functionality into existing TEE architectures, whilst
maintaining their expected security guarantees. Our design
allows TEEs to be interrupted and migrated at arbitrary
points in their execution, thus maintaining compatibility
with existing VM and process migration techniques. By co-
operatively involving the TEE in the migration process, our
design also allows application developers to specify state-
ful migration-related policies, such as limiting the number
of times a particular TEE may be migrated. Our prototype
implementation for Intel SGX demonstrates that migration
latency increases linearly with the size of the TEE memory
and is dominated by TEE system operations.

1 Introduction
Confidential computing is an emerging model in cloud com-
puting, which is already offered in some form by each of the
three largest cloud providers [5, 7, 11]. The primary aim of
confidential computing is to protect data in use, e.g., against
insider threats or compromise of the underlying cloud in-
frastructure. Currently, the leading approach for achieving
this is to use hardware-enforced Trusted Execution Environ-
ments (TEEs).

1

Although there are multiple TEE technologies available,
the overarching idea is the same — to create a strong security
boundary between the TEE and other software components.
The misbehavior by any component outside the TEE then
cannot compromise the confidentiality or integrity of the
TEE. Specifically, data within the TEE can only be read or
modified by code within the same TEE. TEEs often also
provide remote attestation functionality, through which a
remote party can ascertain what code runs within the TEE,
and use this information to make security decisions.

Current TEE technologies can be divided into two groups:
those that enable the creation of one or more in-process TEEs
within an application process, and those that protect larger
structures such as containers or entire VMs. An example of
the former is Intel Software Guard Extensions (SGX) [18, 21]
and Keystone [24], which create enclaves within an applica-
tion process and protect the enclave’s data from the host pro-
cess, OS, and hypervisor; examples of the latter are technolo-
gies such as AMD Secure Encrypted Virtualization (SEV) [1],
AMD SEV Secure Nested Paging (SNP) [2], Intel Trust Do-
main Extensions (TDX) [12], and Arm Confidential Compute
Architecture (CCA) [3], which protect entire VMs against
each other and the hypervisor.

However, the hardware-based nature of most modern
TEEs conflicts with VM or process migration. By design,
TEE memory cannot be read by the hypervisor, OS, or other
software on the platform, preventing existing migration
techniques to be used directly on systems containing TEEs.
Whilst some newer TEE architectures support migration,
this is not universally available, and there are no in-process
TEE architectures that support migration.

There are several reasons why migration is important
in cloud computing. From an operational perspective, the
cloud provider may want to move VMs to different physical
machines to reduce the number of active machines. From a
security perspective, the cloud provider may need to restart
specific physical machines to apply firmware security up-
dates, e.g., to defend against newly-identified side-channel

 
 
 
 
 
 
Yoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

attacks against the TEE [19, 23, 26, 27, 33, 34]. Finally, the
tenants (customers) of the cloud provider may want to move
their workloads to a different provider. In these scenarios,
the inability to migrate a process that uses a TEE could lead
to operational inefficiencies or, in the worst case, data loss if
the physical machine must be restarted.

In light of this, several TEE architectures have announced
native support for migrating TEEs, namely AMD SEV [1],
SEV-SNP [2] and Intel TDX [12]. Some prior work [20, 28, 29]
has proposed approaches to support migration of VMs with
Intel SGX enclaves. However, these TEEs are VM-based, not
in-process.

Compared to VM migration, in-process TEEs are harder
to migrate because they are not designed with migration in
mind. There have been several proposals to retrofit migra-
tion functionality into existing in-process TEE architectures.
Guerreiro et al. [21] use Hardware Security Modules (HSMs)
to manage the cryptographic keys needed to securely mi-
grate the data; Alder et al. [16] describe how to migrate the
persistent state (e.g., hardware-based monotonic counter val-
ues) that may be associated with a TEE. However, all of these
require either changes to the hardware (which is likely to be
impractical given the large deployed base), or they assume
the TEE will reach a quiescent state before it is migrated,
which limits the applicability of the technique.

We present CTR, a software-only design to retrofit mi-
gration functionality into existing TEE architectures, whilst
maintaining their expected security guarantees. The core
idea is to add a minimal set of extra functionality to the TEE
and then enlighten the migration tool to make use of this
functionality. Our design makes the following contributions:
(1) It enables migration of existing in-process TEE architec-
tures, without requiring modifications to the hardware or
placing constraints on the software running within the TEE.
This is challenging because it requires a software-only solu-
tion that can operate within the constraints of existing TEE
architectures (e.g., in Intel SGX, some critical data structures
are inaccessible even from software within the enclave). Fur-
thermore, these architectures may not have been designed
with migration in mind.

(2) It allows TEEs to be interrupted, migrated, and resumed
at arbitrary points in their execution, thus matching the par-
adigm of existing process migration tools, such as CRIU [8],
which expect to migrate a process at any point in time. This
allows us to integrate CTR with these existing tools with
minimal changes. Our method for enlightening these tools is
itself extensible, and could be used to enable new types of
process migration behavior.

(3) It involves the TEE in the migration operation, resulting
in a type of cooperative migration. This gives TEE application
developers the ability to specify flexible stateful policies
to govern migration. Examples of such policies may be to

limit the number of times a particular TEE is migrated, or to
migrate only a subset of the TEE’s memory.

As a proof of concept, and for our performance evaluation,
we have implemented CTR for Intel SGX. Through micro
and macro benchmarks, we show that migration latency
increases linearly with the size of the TEE, and that the
overhead is dominated by TEE system operations, mainly
creation and termination of TEEs.

2 Migration is the Cloud
Liveness for a large number of cloud services replies on
the availability of the machines on which the service is de-
ployed [4, 9, 15]. This architecture runs counter to the deploy-
ment philosophy of many cloud providers. Cloud providers
consider a single machine to be expendable and instead de-
fine large availability zones where machines within a single
availability zone may become simultaneously unavailable
but machine in different availability zones will not be unavail-
able at the same time [6, 10, 14]. In these situation migrating
the application from a machine which the cloud provider
will shutdown in the near future allows the services to main-
tain a high level of availability. Therefore, it is important
that TEEs also support migration, especially as confidential
computing are gaining popularity amongst cloud providers.
In this section, we first describe VM-based TEEs that sup-
port migration and their drawbacks, and then discuss the
state-of-the-art solution for in-process TEEs.
TEE VM migration. AMD has enabled its TEE extensions
SEV and SEV-SNP with hardware support for live migra-
tion [1, 2]. The hardware extensions provided by AMD SEV
are called AMD Secure Processor (SP) which manage the
encryption keys required for live migration. During TEE VM
migration, the destination SP attests itself to the source SP
and once the attestation is verified, the source SP sends the
key securely to the destination SP. After the verifications is
completed successfully the source SP sends the encrypted
TEE VM pages to the destination SP, which uses the key sent
by the source SP to decrypt the pages and copy them into
the destination TEE VM.

AMD SEV-SNP improves upon the AMD SEV model by
introducing migration agents. Migration agents oversees the
enforcement of migration policies removing the requirement
for the VM to maintain its own migration policy.

Intel announced live migration support for their unre-
leased TEE extension, TDX [12]. TDX creates TEE VMs,
called trust domains (TDs), and similar to SEV-SNP’s migra-
tion agent, TDX utilizes an entity called migration TD when
migrating a TD. The source and migration TDs conduct a
mutual remote attestation and negotiate a key to encrypt
the contents of the migrating TD.

Although Intel’s current TEE architecture, SGX, does not
natively support migration, several attempts had been made

2

CTR: Checkpoint, Transfer, and Restore for Secure Enclaves

to migrate SGX-enabled VMs in the literature. The first at-
tempt was by Park et al. where they proposed a design
that supports live migration of SGX-enabled VMs [29]. They
pointed out several problems of migrating such VMs, includ-
ing the secure migration of enclave memory. The proposed
idea was to introduce a new set of CPU instructions to en-
able live migration. In their follow-up work [28], the authors
implemented the proposed instruction using OpenSGX [22],
a fully functional Intel SGX emulator based on QEMU.

In parallel work, Gu et al. [20] utilize a control thread
to securely copy an enclave’s state from within the TEE.
The system uses a scheme called “two-phase checkpointing”,
which requires an enclave to reach a checkpoint before mi-
grating, to ensure data consistency of migrated enclaves. The
authors implemented their system a variant of the KVM that
provides support for Intel SGX in applications, guest OSs,
and the KVM itself.

Unfortunately, these attempts cannot migrate a process
running within an operating system without the coordina-
tion of application being aware and adding logic which co-
ordinates with the migration coordinator. This results with
TEEs losing a considerable amount of utility when run on a
machine within a cloud data center. Additionally, the afore-
mentioned systems cannot migrate a process running within
the TEE without migrating the entire VM. This increases the
latency and also adds strain to the cloud provider’s network
bandwidth, as a larger amount of data must be migrated.
This motivates migrating in-process TEEs without the need
of migrating the entire underlying VM.
In-process TEE migration. To the best of our knowledge,
TEEnder [21] is the only work that aims to enable migration
for in-process TEEs, specifically Intel SGX. TEEnder uses
Hardware Security Modules (HSMs) to encrypt and decrypt
SGX enclave data during migration. The main motivation
behind using HSMs is the recent findings of security vulner-
abilities surrounding SGX remote attestation [25, 30, 32, 33].
TEEnder realizes this by integrating enclave applications
with HSMs and implementing an infrastructure that utilizes
HSMs to provide enclaves with migration capabilities. How-
ever, the usage of HSMs, although motivated clearly, will
decrease the deployability of the system as well as the per-
formance of migration.

Reflecting the challenges shown above, a design for mi-
grating processes containing TEEs must fulfil the following
requirements:

R1 The migration functionality must be retrofitted into

existing TEE architectures.

R2 The design must maintain the existing security guar-

antees provided by the TEEs.

R3 The TEE developer must be able to define stateful

policies that govern the migration.

We present an overview of our design in the next section,

and show how it meets these requirements in Section 4.

3

3 System Overview
Figure 1 shows an overview of the main entities involved in
CTR, and the interactions between them.

The migration begins when the migration initiator decides
that a process should be migrated from the node on which it
is currently running (the source) to another node (the destina-
tion). The source process contains at least one TEE that must
be included in the migration. The migration can be broadly
divided into two phases: checkpoint (§3.1) and restore (§3.2).

3.1 Checkpoint phase

The first step in the checkpoint phase is to pause the execu-
tion of the source process by stopping all currently executing
threads, including those within the TEE. With the execution
paused, the state of the source process can be serialized and
saved. There are several existing tools, such as CRIU [8],
that can be used to serialize and save the state of the source
process. We refer to this type of tool as the migration agent
on the source (and destination) node.

However, since the migration agent is running outside
the TEE, it cannot directly read the TEE’s memory, as would
be required to perform the migration. Using an unmodified
migration agent on a process containing a TEE typically
results in an access violation. CTR therefore requires a small
amount of additional functionality to the TEE, through the
inclusion of a software library. This functionality can be
used to request the TEE to serialize its own state, encrypt it,
and write the output to the memory of the source process.
The cryptographic key used to encrypt (and subsequently
decrypt) the TEE’s state cannot be revealed to the source
process, so it is securely transferred to a trusted migration
key service.

The existing migration agent can be enlightened to i) rec-
ognize that the source process contains a TEE, and ii) to use
the additional functionality added to the TEE to perform the
migration. The output of the checkpoint phase is the seri-
alized state of the source process, including the encrypted
state of one or more TEEs. This state is then transferred to
the destination host.

3.2 Restore phase

The restore phase begins when the destination node receives
the state from the checkpoint phase. This state is used to
recreate the saved process (now referred to as the destination
process), using a migration agent on the destination node
(e.g., CRIU). Using information from the saved process state,
the migration agent also creates one or more fresh TEEs in
the destination process, corresponding to the TEEs that were
paused in the source process.

However, the migration agent does not have the decryp-
tion keys for the encrypted TEE state and cannot write di-
rectly to the memory of the destination TEE. Similar to the
checkpoint phase, CTR delegates the task of restoring the

Yoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

Figure 1. Overview of the main components CTR and the interactions between them.

TEEs’ internal state to the TEEs themselves. Specifically, the
additional functionality added to the TEE can also be used to
restore a previously-saved TEE state onto a newly-initialized
TEE.

The destination TEE securely retrieves the decryption
keys from the migration node, using remote attestation to
demonstrate that it is the correct type of TEE running the
expected code. Once the keys have been retrieved, the desti-
nation TEE decrypts the saved state and transforms itself by
overwriting its own heap, stack, and other data structures
with those from the restored state. Once the migration has
been completed, this restored TEE can continue operating
from exactly the same point at which the original source
TEE was paused.

4 Design Challenges & Solutions
This section discusses the main challenges arising from the
requirements in Section 2, and approaches used to overcome
these in CTR.

4.1 Retrofitting migration to existing TEEs

The requirement to support existing TEE architectures, which
may already be widely deployed, precludes the possibility of
modifying TEE hardware (e.g., as done by Park et al. [29])
and necessitates a software-only approach. Since only the
software running within the TEE can read and write TEE
memory, the checkpoint and restore functionality must be
provided from within the TEE. As explained in Section 3,
CTR adds these functionalities by including an additional
software library within the TEE. In future, this library could
be included by default in the TEE development frameworks
(e.g., Open Enclave [13]). Performing the migration from
within the TEE raises specific challenges in both the check-
point and restore phases.

4

Arbitrarily pausing TEEs. Halting a TEE without modify-
ing TEE hardware is a challenge. One way of doing this is to
wait until the TEE reaches a certain point in its execution
and exit (e.g., as done by Gu et al. [20]). However, this re-
quires the TEE to be aware of the migration and thus may
not fit requirements of certain migration agents (e.g., CRIU
requires processes to be arbitrarily paused). This means that
CTR must be able to halt a TEE at any point in time. CTR
realizes this by using interrupts, which is widely supported
across different TEE architectures.
Operating within the TEE. The checkpoint functionality
is invoked once the source TEE has been paused, and there-
fore must carry out its operations without affecting the state
of the source TEE. In addition, the working state of this oper-
ation must not be included in the saved state of the TEE. The
exact techniques used to overcome these challenges will vary
by TEE technology (e.g., we describe our implementation
for Intel SGX in Section 5), but in general will require the
checkpoint operation to use its own stack and heap mem-
ory. The restore functionality faces similar challenges that it
must decrypt and process the saved state within the fresh
destination TEE and then overwrite the state of that TEE
(stack and heap) with the restored state. This likewise means
that the restore operation must use its own reserved memory
within the TEE.
Understanding TEE memory. The next challenge is that
the checkpoint and restore operations within the TEE must
understand the TEE’s memory map. For example, the TEE’s
memory might contain control structures that cannot be read
or written even by software running within the TEE (e.g.,
the Thread Control Structures in Intel SGX). The migration
operations must take care to avoid these memory regions.
Additionally, CTR may need to use architecture-specific tech-
niques to infer the values held in these control structures

Retrieve MigrationKeyUploadMigrationKeyRestoreDestinationTEERestoreDestinationProcessCheckpointSourceTEECheckpointSourceProcessMigrationinitiatorMigrationagentMigrationagentSource ProcessDestination ProcessSource TEEDestination TEEMigration Key Service (MKS)Start CheckpointPhaseStart RestorePhaseSerializedStateEncryptedTEE stateEncryptedTEE stateCreateDestinationTEE124786359CTR: Checkpoint, Transfer, and Restore for Secure Enclaves

in the source TEE and to correctly set these values in the
destination TEE.

Source TEE

Migration Key
Service

Destination TEE

4.2 Maintaining TEE security

When migrating TEEs, CTR needs to ensure that existing TEE
security guarantees still hold. Specifically, the only change
to the security model is that the TEE can be migrated. As
explained in Section 3, the checkpoint operation encrypts
the TEE’s memory before writing it outside the TEE. Specif-
ically, an authenticated encryption scheme, such as AES-
GCM, must be used so that the integrity of the saved state
can be verified by the destination TEE. We refer to the sym-
metric encryption/decryption key as the migration key.
Secure key transfer. The migration key cannot be directly
exported with the saved state — it must instead be securely
transferred to the destination TEE. If the source and des-
tination TEEs were running concurrently, they could use
existing remote attestation functionality to mutually attest
each other, establish a secure channel, and securely transfer
the migration key. However, requiring both TEEs to be run-
ning concurrently would severely limit the applicability of
the approach. For example, existing process migration tools
such as CRIU [8] operate strictly sequentially: the check-
point phase is completed before the restore phase begins.
Requiring concurrently running source and destination TEEs
would also preclude the possibility of self-migration, where
the source and destination are the same physical node. This
would be used to allow the node to be restarted e.g., to install
security firmware updates.

To overcome this challenge, CTR makes use of a new
migration key service (MKS), which serves as a trusted in-
termediary and key escrow service between the source and
destination TEEs. Specifically, once the source TEE has gen-
erated the migration key, it establishes a secure channel with
the MKS and sends the migration key. The restore operation
in the destination TEE retrieves the migration key from the
MKS and uses it to decrypt the TEE state.

The MKS is a relatively simple store-and-forward helper
service, for which there are various possible implementa-
tions. One possibility is to implement the MKS using another
TEE, either on the source, destination, or another node. As
shown in fig. 2, the source TEE attests the MKS to ensure that
the migration key is only transferred to a trustworthy MKS,
and the MKS attests the source TEE to verify the provenance
of this key. Subsequently, the MKS attests the destination
TEE to ascertain that it is the correct type of TEE (e.g., the
same as the source TEE), and the destination TEE attests the
MKS to again verify the provenance of this key. This pro-
vides the same security guarantee as the direct key transfer
between source and destination TEE.
Fork and roll-back attacks. In addition to secure key trans-
fer, the design must also ensure that the migration function-
ality itself cannot be used to mount attacks such as a fork or

generate_MK()

MK

attest()

attest()

MK

attest()

attest()

MK

Figure 2. Secure transfer of the migration key (MK) from
source to destination TEE via the Migration Key Service. All
communication takes place via secure channels.

roll-back attack [16]. Specifically, we need a mechanism to
ensure that the source TEE cannot continue running after
the checkpoint operation has been completed, as this could
lead to a fork attack with multiple copies of the same TEE
running. To prevent this, CTR blocks the source TEE from
being resumed before the output from the checkpoint opera-
tion is released. The precise implementation is architecture-
dependent, but can always be achieved through minor modi-
fications of the TEE’s software. We also require a mechanism
to prevent one saved state being restored to multiple desti-
nation TEEs (another type of fork attack) or being restored
more than once (a roll-back attack). This cannot be prevented
by changes within the TEE, so CTR requires the MKS to only
release the migration key to a single destination TEE.

4.3 Supporting policy-governed migration

The design must provide mechanisms that TEE developers
can use to define and enforce policies to govern the migra-
tion of the TEE. CTR does not define any specific policies,
but aims to provide as much flexibility as possible to TEE
developers. Specifically, CTR enforces policies by calling a
developer-defined function (which call additional functions)
during the restore phase before allowing the TEE to resume
operation. Since the TEE state has already been decrypted
and put into place, this function can be stateful and can
inspect the full state of the TEE. The return value of this

5

Yoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

function indicates whether the TEE should be allowed to
resume operation. This ensures that every restore operation
is visible to the TEE. We sketch two example policies to
illustrate the use of this mechanism.
Limited number of migrations. One example policy could
be to limit the number of times a specific TEE can be mi-
grated. This may be useful in cases where the TEE developer
would like to allow the cloud provider to migrate the TEE a
limited number of time, but may be concerned that an exces-
sive number of migrations might leak information from the
TEE (e.g., through side-channel attacks). To implement this,
the developer would define a counter variable in the TEE’s
memory and decrement this on each successful restore oper-
ation. When the counter reaches zero, the function would
indicate that the TEE should not be permitted to resume.
Clearing caches upon migration. As another example,
the policy might not need to govern whether the TEE can
be resumed, but rather define a set of actions that must be
performed after each migration. For example, the TEE might
contain node-specific state (e.g., some type of cache of local
sessions) that should be invalidated if the TEE is migrated.
Again this can be achieved by calling a developer-defined
function that clears/resets this part of the state whenever
the TEE is restored.

5 Implementation
This section describes CTR-SGX, an implementation of CTR
for Intel Software Guard Extensions. Our implementation is
based on the Open Enclave SDK [13] v0.16.1, but could be
applied to any other SGX SDK. We first describe the specific
steps required for migrating an Intel SGX enclave (§5.1) and
then discuss how we integrated these into the CRIU [8]
process migration tool (§5.2).

5.1 Enclave Migration

In practice, migrating an SGX enclave requires some addi-
tional preparation before the checkpoint phase and some
additional cleanup after the restore phase. We therefore de-
scribe this process in terms of the following four phases:

1. Preparation Phase: Initializes variables used for mi-

gration.

2. Checkpoint Phase: Collects, encrypts, and exports

all necessary data from the source enclave.

3. Restore Phase: Imports, decrypts, and restores all

data to the destination enclave.

4. Cleanup Phase: Cleans up data structures (e.g., buffers)

used for migration.

Below, we describe each phases in detail.

5.1.1 Preparation Phase. The preparation phase is run
asynchronously, any time before the enclave is to be mi-
grated. This can either be integrated into the enclave’s ini-
tialization sequence, or called via a separate ECALL. In this

6

phase, the enclave generates a migration key and initial-
izes the encryption context (i.e., the data structures used
by the cryptographic library). It also retrieves the memory
addresses and sizes of the enclave’s stack, heap, and data
sections, as well as the address and size of the SGX-specific
State Save Area (SSA). These values are stored within the
enclave in preparation for migration.

5.1.2 Checkpoint Phase. To initiate the checkpoint phase,
the host should interrupt all enclave threads and call a newly-
added ECALL (called enclave_export_all). In this ECALL,
the host provides a pointer to a memory buffer outside the
enclave, into which the encrypted state should be written.
The enclave then performs several steps in order to serialize,
encrypt, and save its own state.
Halting enclave threads. CTR-SGX uses interrupts to pause
TEEs. In Intel SGX, interrupting a thread that is within the
enclave causes an event called an Asynchronous Exit (AEX).
When an AEX occurs, the CPU saves the thread’s state in the
enclave’s State Save Area (SSA). This saved state includes
the Thread Local Storage (TLS), current instruction pointer,
stack and base pointers, and other register values (within
a data structure called GPRSGX). The CPU then clears the
registers and jumps to a predefined code location outside the
enclave.

The host process (i.e., the source process) can send a signal
to each thread within the enclave to cause an AEX. However,
the default behavior of most SGX frameworks is for the
thread to immediately re-enter and resume executing within
the enclave after the interrupt has been handled. The host
process therefore needs to take additional steps to prevent
the thread re-entering the enclave. A simple solution would
be to cause the thread to sleep or spin in an infinite loop.
However, all enclave threads jump to the same code address
outside the enclave when they are interrupted. At least one
thread will be required to perform the checkpoint operations
within the enclave, and this thread might also be periodically
interrupted. If this thread is interrupted, it should not be held
outside the enclave.

The host process can overcome the above issues using
three separate flags: IS_HOST_MIGRATING, HAS_ENTERED_-
ENCLAVE, and ALLOW_ERESUME. All three flags start in the
unset state. The host process first sets the IS_HOST_MIGRAT-
ING flag to indicate that it is going to halt the enclave thread.
This flag causes the interrupted enclave threads to be kept in
an infinite loop, which is conditioned to break when the AL-
LOW_ERESUME flag is set. Finally, the HAS_ENTERED_ENCLAVE
flag is set immediately before the migration thread enters
the enclave. This flag prevents any new threads from be-
ing captured in the infinite loop, whilst still holding the
previously-captured threads.

Once the migration thread has entered the enclave (via
the enclave_export_all ECALL), it could set an in-enclave
flag to prevent any other ECALLs being made. It could also

CTR: Checkpoint, Transfer, and Restore for Secure Enclaves

Source
Host Application

Source Enclave

Destination
Host Application

Destination Enclave

ECALL

Interrupt

AEX
enclave_export_all()

Encrypted
enclave data

ERESUME

return

Encrypt
enclave data

Dummy ECALL

Interrupt

AEX
enclave_import_all()

ERESUME

return

Decrypt & layout
enclave data

(a) Checkpoint phase

(b) Restore phase

Figure 3. Enclave migration flow

prevent any threads being resumed while the migration is
in progress by saving and overwriting the saved instruction
pointer values in the SSA. If any thread did resume operation
after this point, it would cause the enclave to crash.
Exporting enclave data. With the enclave paused, the next
step is to encrypt and export the enclave’s data. The data
to be are exported are: (1) data section, (2) heap section,
(3) stack, and (4) SSA. The enclave data are serialized by
concatenating all the above in the order shown. We do not
export code data because we assume both the source and
destination enclaves are running the same code.

Additionally, every SGX enclave includes one or more
Thread Control Structures (TCS), which cannot be read or
written even by code running within the enclave. Each TCS
contains a Current State Save Area (CSSA) value, which
indicates how many threads have been interrupted and are
now outside of the enclave. Since we are using interrupts to
halt enclave threads, we need to infer the CSSA value. We
could employ the same method proposed by Gu et al. [20],
which introduces a software monitor that keeps track of how
many threads have entered and exited the enclave.

Recall that enclave data must be encrypted before it can
be written outside the enclave (Section 4.2). We use AES in
GCM mode (i.e., authenticated encryption) with a 256 bit key
for this purpose. We implemented this using both the cryp-
tographic libraries supported by Open Enclave: mbedTLS
and OpenSSL. Once encrypted, that data is written to the
specified buffer outside the enclave.

5.1.3 Restore Phase. The restore phase also consists of
several steps. First, the destination enclave must be cre-
ated and initialized. Second, CTR-SGX must create one or
more placeholder threads within the destination enclave,
corresponding to the threads that were interrupted from the
source enclave. Third, the host process initiates the restore
process by calling a new ECALL (enclave_import_all). This
ECALL takes the pointer to the encrypted enclave data re-
ceived from the source process. Finally, the restored enclave
threads resume execution from the point at which they were
interrupted.
Creating placeholder threads. Placeholder threads are es-
sentially enclave threads with an infinite loop. There are
several reasons to why we need to create placeholder threads
in the destination enclave. Firstly, we need a pre-allocated
memory region so that we can lay out the exported enclave
data back to its original location. Secondly, this causes the
CPU to increase the CSSA value in the TCS. This technique
overcomes the limitation of not being able to write to the TCS
from software, and can thus be used to set the correct CSSA
values. CTR-SGX therefore creates a placeholder thread in
the destination enclave for each thread that was interrupted
from the source enclave.
Restoring enclave data. The main challenge during the
restoration phase is to ensure that the enclave data is re-
stored to the correct location. For security reasons, the saved
state must be decrypted within the destination enclave. The
data structure used by the cryptographic library (i.e., the
decryption context) is typically allocated on the heap. This
creates a problem when we restore the heap section, because

7

Yoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

we may potentially overwrite the decryption context when
we are decrypting and restoring the subsequent section. To
overcome this issue, CTR-SGX places the decryption con-
text in the global data section and intentionally avoids that
area during the restoration process. This is possible because
the code is identical between the source and destination en-
claves, allowing the encryption and decryption contexts to
be allocated in the same memory location.

5.1.4 Cleanup Phase. Both the source and destination
processes carry out a cleanup phase after they have com-
pleted their respective roles in the migration. On the source
node, after the migration ECALL has returned, the host pro-
cess can tear down the enclave. Alternatively, if allowed by
the enclave developer, the host process may be allowed to
resume the enclave by setting the ALLOW_ERESUME flag. This
is essentially “forking” the enclave, which may be desirable
in some circumstances. On the destination node, after the mi-
gration process has been completed and the enclave resumed,
the buffer that was used to store the encrypted enclave data
can be freed.

5.2 Integrating CTR-SGX into CRIU

In this section, we describe how enclave migration can be
integrated into existing process migration tools. Specifically,
we integrate CTR-SGX into CRIU [8]. The following sub-
sections describe this integration in each of the four phases
discussed above. These descriptions are written with respect
to a source process that contains a single enclave, but would
equally apply if the source process were running multiple
enclaves.

5.2.1 Preparation Phase. In addition to the steps described
in Section 5.1.1, the source process must perform several
steps. First, it must close file descriptors to stdin, stdout,
stderr, and /dev/sgx, as they cannot be migrated by CRIU.
Second, it must allocate a memory buffer to store the en-
crypted enclave memory, the address of which is passed as
a parameter when calling enclave_export_all. Finally, it
creates a file containing code pointers to several functions
within the source process, which will be used in both the
checkpoint and restoration phases.

5.2.2 Checkpoint Phase. This phase begins when the mi-
gration initiator instructs the migration agent (in this case,
CRIU) to migrate a specific process (in this case, the source
process). CRIU process transitions the source process into a
“seized” state and begins to determine what must be migrated.
In order to do this, CRIU injects a piece of code (called the
parasite code) into the source process. The parasite code al-
lows CRIU to gain access to resources held by the source
process, such as file descriptors and threads. However, by
design, even this parasite code cannot read the memory of
the SGX enclave.

8

Extending CRIU. CTR-SGX therefore extends the parasite
code to work with the source enclave in order to achieve
the migration. Specifically, we extend the parasite code to
call the enclave_export_all ECALL. Although it may be
possible to make this ECALL from outside the source process
(e.g., for the migration agent to make this ECALL), this would
have to deal with several address translations. A more natural
approach is for the injected parasite code to make this ECALL,
since the parasite code i) has direct access to variables in
the source process, such as the enclave context, and ii) can
directly invoke functions from the source process’s address
space, such as the ECALL.
Determining function addresses. However, the addresses
of the enclave_export_all ECALL and other enclave func-
tions are not deterministic, due to memory address random-
ization. To overcome this issue, our enlightened version of
CRIU looks up the necessary function pointers in the file
created in the preparation phase (§5.2.1). Specifically, the par-
asite code opens the file from a predefined location, retrieves
the function pointers, and invokes each of the functions
sequentially.
Passing parameters. Since this approach does not allow
CRIU to pass arguments to any of the functions, the function
pointers in the file should point to wrapper functions that do
not take any parameters. These wrapper functions could in
turn call other functions for which the parameters have been
determined in advance. A specific example is the enclave_-
export_all ECALL, which must include the address of the
memory buffer outside the enclave into which the encrypted
state will be written. This parameter can be determined when
the buffer is allocated (i.e., in the preparation phase). CRIU
calls a wrapper function with no parameters, which in turn
calls this ECALL with the pre-prepared parameter value.
Further extensibility. This approach is extensible in that
the application developer can provide additional functions
for CRIU to invoke before starting the migration. Apart from
enabling cooperative migration of TEEs, this could also be
used to support other types of extended functionality e.g.,
if the source process needs to perform some non-standard
clean-up operations or close some resources before being
migrated.
Overall checkpoint process. The interaction between the
source process and the source enclave is shown in Figure 3a.
Putting it all together, the checkpoint process proceeds as
follows: When CRIU seizes the source process, this will inter-
rupt and pause all of the source process’s threads. Specifically,
this will cause an AEX for any threads within the enclave, as
required. Once the process is seized, CRIU injects the para-
site code into the source process and invokes it. The parasite
code then calls the enclave_export_all ECALL, which as
explained above, causes the enclave to serialize, encrypt, and
write its state to the memory of the source process. This
memory will then be included in the saved process image

CTR: Checkpoint, Transfer, and Restore for Secure Enclaves

output by CRIU, alongside with the rest of the source pro-
cess’s memory. Finally, after all the data has been exported,
the parasite code destroys the enclave. The migration agent
can then transfer the saved process image to the destination
node.

5.2.3 Restore Phase. The restore phase begins once the
migration initiator has transferred the process image to the
destination node and invoked the migration agent (CRIU) on
the destination node. The CRIU process on the destination
node uses this process image and essentially transforms itself
into the restored destination process. In order to restore the
process memory, CRIU uses another separate piece of code
called the restorer blob. As with the parasite code in the
checkpoint phase, CTR-SGX extends the CRIU restorer blob
to handle restoration of the enclave. The interaction between
the destination process and the destination enclave is shown
in Figure 3b.
Restoring a process with CRIU. CRIU first restores re-
sources that do not require the restorer blob, such as threads.
This includes the enclave threads that were exported during
the checkpoint phase. CRIU then injects the restorer blob
into its own process memory and uses this blob to restore
the memory from the saved process image.
Restoring an enclave with CRIU. Once the destination
process’s memory has been restored, the restorer blob needs
to create and restore the destination enclave. CTR-SGX uses a
similar mechanism as for the parasite code in the checkpoint
phase in that the restorer blob accesses a file at a predefined
location and reads a list of function pointers to invoke. As
explained above, this file of function pointers was created
during the preparation phase. Since the destination process
does not yet contain an enclave, the restorer blob first in-
vokes the function to create an enclave and the ECALL to ini-
tialize important variables (e.g., encryption context) within
the enclave. It then creates one or more placeholder enclave
threads, corresponding to the number of enclave threads that
were interrupted. Each of these threads is then interrupted
and destroyed once outside the enclave. The restorer blob
then invokes the enclave_import_all ECALL (via the cor-
responding wrapper function). As explained in Section 5.1.3,
this ECALL restores the exported enclave data to the correct
location.

5.2.4 Cleanup Phase. In addition to the steps described
in Section 5.1.4, the injected restorer blob must be removed
from the destination process’s memory. Finally, the restored
process is released from its seized state and starts resuming
its execution. Once the restored process is resumed, the re-
stored enclave thread can re-enter the enclave and continue
execution.

9

6 Evaluation
We evaluated CTR-SGX to understand the overheads and
performance impact of migrating in-process TEEs. In §6.1
we first describe the settings of the environment and bench-
marks used for the evaluation. Next, we report the through-
put of an application running in an enclave and the perfor-
mance impact of when it is migrated (§6.2). Then, we show
the overall latency of migrating both the host application
and enclave (§6.3). Next, we report the latency of migrating
just the enclave (§6.4) and the host application (§6.5). Finally,
we consider the cryptographic operations used in CTR-SGX
(§6.6).

6.1 Experimental Setup

We conducted our experiments using a Microsoft Azure Con-
fidential Computing DC4ds v3 series VM. The VM had 4 Intel
Xeon Platinum 8370C vCPUs (Icelake, 2.80 GHz) with 32 GB
of assigned memory of which 16 GB could be used by the
SGX TEE’s enclave page cache (EPC). We ran Ubuntu 18.04
and installed the Intel SGX DCAP Driver version 1.33.2. To
remove disk I/O latency from the critical path, we used a
RAM disk to store images created by CTR-SGX’s checkpoint
phase.
Implementation. We built our implementation of CTR-
SGX in 913 lines of C/C++ code of which 96 lines were
additions or modifications to the Open Enclave SDK and
204 lines were additions or modifications to CRIU. CTR-SGX
was integrated into CRIU version 3.15 and Open Enclave
version 0.16.1. All cryptography was performed using ei-
ther mbedTLS version 2.16.10 or OpenSSL version 1.1.1k and
CTR-SGX used OpenSSL unless otherwise specified.
Benchmarks. The throughput evaluation used the Small-
Bank benchmark [17] which models a bank with 1, 000 ac-
counts. The clients perform one of five randomly selected
transactions that either: deposit funds; transfer funds; with-
draw funds; check an account’s balance; or amalgamate two
accounts. We implemented the SmallBank benchmark with
SQLite version 3.34.1. We ran the benchmark for 10 seconds
allowing it to reach a steady state before migrating it to a
different process, and then ran it for another 10 seconds
ensuring the migrated process also reached a steady state.

6.2 Throughput

We first explored how the throughput of an application is af-
fected when it is migrated. We created an in-memory SQLite
database inside a 64 MB size enclave and manipulated the
data within the database by executing the SmallBank bench-
mark [17].

Figure 4a shows the result. The blue plot shows the num-
ber of transactions executed per second on a one second
rolling average, which calculates the average of transactions
in the previous second. We plot our results starting after
the first second and advance in 1 millisecond increments.

Yoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

(a) The blue line is the average number of transactions in the
previous second. The vertical lines show when certain migration
operations occurred.

Figure 5. Latency of checkpointing/restoring host applica-
tion and enclave when varying enclave size. Enclave runs a
simple counter application.

(b) The blue plot shows the actual number of transactions that
occurred per second. Vertical lines show the point of time where
certain migration operations occur.

Figure 4. Migration affecting throughput of application with
a 64 MB enclave running SmallBank benchmark.

The vertical blue, green, and purple dashed line shows the
time points where the checkpoint starts, checkpoint ends
and restore starts, and restore ends, respectively. We can see
that the throughput of nearly 1, 300 transactions per second
(txns/s) dips to 750 txns/s as the migration happens, and
recovers after around 1, 400 ms. In this benchmark, when we
consider the number of transaction executed per second, the
rolling average of executed SmallBank transactions never
dips to zero. This shows that most applications – which typ-
ically consider their throughput at a per-second granularity
– will never experience an instance when their application is
unavailable.

We further consider a more course-gained concept of avail-
ability in Figure 4b. This figure shows the case where we
plot the sum of the number of transactions recorded per
second. We see a similar trend as Figure 4a, where the num-
ber of executed transactions in a window is approximately
1, 300 txns/s, and dips to appropriately 750 txns/s, but recov-
ers back the next second. Thus, we observe that a window
of unavailability – where throughput is 0 txns/s – does not
exist and the window of reduced throughput is only one
second.

6.3 Migration Latency

Now, we look the impact of migrating enclaves with an
increasing amount of memory allocated to the enclave and
report all results as the average of 10 measurements. We

Figure 6. Latency of enclave migration operations when
varying enclave size. OpenSSL is used to encrypt/decrypt
enclave data. Checkpoint and Destroy enclave operations
occur during checkpoint phase, while Create and Restore
enclave operations occur during restore.

Figure 7. Latency of checkpointing/restoring only the host
application. The same amount of memory needed to store
encrypted enclave data is allocated by host application and
is migrated with the application.

Figure 8. Latency of checkpointing/restoring only the host
application. No memory buffer is allocated.

first show the overall latency of migrating both the host
application and enclave.

10

123456789101112131415161718192021Time[s]0.751.001.25Throughput[Ktxns/s]CheckpointstartCheckpointend,RestorestartRestoreend1234567891011121314151617181920Time[s]0.751.001.25Throughput[Ktxns/s]CheckpointstartCheckpointend,RestorestartRestoreend8641282565121024Enclavesize[MB]012Operationlatency[s]CheckpointRestore8641282565121024Enclavesize[MB]012Operationlatency[s]DestroyenclaveCheckpointenclaveRestoreenclaveCreateenclave8641282565121024Enclavesize[MB]0.00.20.4Operationlatency[s]CheckpointRestore8641282565121024Enclavesize[MB]0.0000.005Operationlatency[s]CheckpointRestoreCTR: Checkpoint, Transfer, and Restore for Secure Enclaves

We measured the change in migration latency when vary-
ing the enclave size from 8 MB up to 1024 MB, increasing
in two folds. The enclave sizes were chosen based on a sur-
vey conducted by Guerreiro et al. [21] on various projects
that include enclaves, which they concluded that enclave
sizes between 1 MB and 1 GB represent many development
scenarios. The enclave ran an application that incremented
a counter from 0 to 1, 000, 000, 000. For this evaluation, we
used OpenSSL (see § 6.6 for a comparison of cryptographic
libraries). We use this benchmark to understand the over-
all latency of CTR-SGX with the side-effects that a more
complex workload such as SmallBank may introduce.

Figure 5 shows the time required for CTR-SGX to check-
point or restore the host application and the enclave when
the enclave size changes. We can see that the restore phase
takes the most time (2.55 sec for a 1 GB enclave) while check-
point takes less time (0.95 sec). We also observed that the
latency for both phases increases linearly with the enclave
size. Since modern Icelake CPUs have an EPC which is half
the available system memory, we expect this linear relation-
ship between the enclave’s memory and migration time to be
consistent until memory is exhausted. This allows CTR-SGX
users to estimate the time required to migrate their enclave.

6.4 Enclave Migration Latency

Next, we consider the overhead introduced when migrat-
ing just the enclave when varying enclave size. We measure
key enclave operations, namely checkpoint enclave, destroy
enclave, create enclave, and restore enclave. Note that (1)
and (2) occur during the checkpoint phase and (3) and (4) oc-
cur during the restore phase. These operations were chosen
based on preliminary observations conducted prior to this
evaluation.

The results are shown in Figure 6. We can observe that
creating an enclave takes the most time (1.91 sec for 1 GB
enclave), followed by checkpoint (0.40 sec), restore (0.30 sec),
and destroy enclave (0.15 sec). The reason that creating an
enclave takes the longest is because of the allocation and
initialization of enclave pages. We can clearly see that the
restore phase is the largest contributor to the latency we
observed in Figure 5. The other essential enclave migration
operations (checkpoint and restore enclave) introduce mini-
mal latency.

Another point of consideration is that the create enclave
latency is included in the overall latency strictly because
the destination enclave is created during the migration op-
eration. We emphasize that this would change according to
migration policies, e.g., if the policy requires the destina-
tion enclave to be created before or during migration, this
latency would not occur in the critical path. We could further
reduce the latency in Figure 6 by employing multiple threads
when checkpointing/restoring enclave data. Therefore, the
numbers reported here represent the upper bound, where all
migration operations are done sequentially.

Figure 9. Latency of key enclave migration operations when
varying enclave size. mbedTLS is used to encrypt/decrypt
enclave data. Checkpoint and Destroy enclave operations
occur during checkpoint phase, Create and Restore enclave
operations occur during restore.

6.5 Host Migration Latency

Next, we look at the overheads introduced when migrating
only the host application. In this evaluation, we migrated
only the host application and we did not create an enclave
in the application. Since the host application allocates and
initializes a buffer required to store encrypted enclave data
in CTR-SGX, we also wanted to observe whether this has an
impact on latency. Therefore, although the application does
not create an enclave during this evaluation, the application
still allocated and initialized the buffer.

Figure 7 shows the results. We can see that the checkpoint
phase takes the longest (0.4 sec for a 1 GB buffer), followed
by restore (0.35 sec). From this we can draw several conclu-
sions: First, the time required to allocate and initialize the
buffer required to store encrypted enclave data is significant.
This conclusion is further validated by comparing the results
from Figure 7 to the results from Figure 8 where CTR-SGX
does not allocate the buffer. Figure 8 shows that latency of
migrating a process without the memory buffer does not
grow as the enclave’s memory increases. Second, the check-
point/restore latency of a host application is near-identical
to that of an enclave. This shows CTR-SGX can migrate both
a host application and its enclave by doubling the latency
of CRIU and that our implementation of in-process TEE mi-
gration has the same latency overhead as a state-of-the-art
non-TEE process migration utility.

6.6 Latency vs. Crypto library

Finally, we investigate how different cryptographic libraries
affect enclave migration latency. Here we use two different
cryptographic libraries supported by OpenEnclave; mbedTLS
and OpenSSL. Additionally, we measure the latency without
encrypting/decrypting the enclave data during the check-
point/restore phase to show the overhead introduced by
cryptographic operations. The evaluation followed the same
methodology as Section 6.3, where we vary the enclave size
and measure the time taken to migrate an enclave.

Figures 6 and 9 show the latency of enclave migration
when using OpenSSL and mbedTLS, respectively. We see

11

8641282565121024Enclavesize[MB]020Operationlatency[s]DestroyenclaveCheckpointenclaveRestoreenclaveCreateenclaveYoshimichi Nakatsuka, Ercan Ozturk, Alex Shamis, Andrew Paverd, and Peter Pietzuch

TEE persistant state migration. Alder et al. [16] proposed
a design for including persistent state (e.g., sealed data, hard-
ware monotonic counter values) when migrating Intel SGX
enclaves. Support for migrating such data is important, as
migrated enclaves that rely on these persistent state will
not be able to continue normal operation at their migration
destination. Moreover, not being able to migrate hardware
monotonic counter values will undermine the security of
the system, as the enclave will be susceptible against roll-
back attacks. We consider this as complementary work and
envision CTR working in conjunction with this system.

ReplicaTEE [31] considers another aspect of TEE migra-
tion: provisioning of newly instantiated TEEs. Although
ReplicaTEE does not support replicating TEEs with their
internal state intact, it allows server operators to start up
multiple instances of the same enclave and provision them
with the same secret without interacting with the application
owner. The authors have designed ReplicaTEE so that a limit
is imposed on the number TEE instances that can be created.
CTR can utilize ReplicaTEE to limit the number of destina-
tion TEEs which are created while allowing such TEEs to be
provisioned with necessary secrets (e.g., migration key).

8 Conclusion
In this work, we proposed CTR, a software-only design to
enable migration functionality into existing in-process TEE
architectures without the need for hardware modifications.
CTR allows TEEs to be migrated at arbitrary points in their
execution, allowing our method to be integrated into existing
process migration tools. We implemented CTR for Intel SGX
and CRIU, and show that migration latency increases linearly
with the size of the TEE and is primary dependent on time
required to create and initialize the destination TEE.

References
[1] AMD Secure Encrypted Virtualization (SEV).

https://developer.amd.com/sev/. Accessed: 2022-05-14.

[2] AMD SEV-SNP: Strengthening VM Isolation with Integrity Protection
and More. https://www.amd.com/system/files/TechDocs/SEV-SNP-
strengthening-vm-isolation-with-integrity-protection-and-
more.pdf. Accessed: 2022-05-14.

[3] Arm Confidential Compute Architecture.

https://www.arm.com/architecture/security-features/arm-
confidential-compute-architecture. Accessed: 2022-05-14.

[4] Availability.

https://docs.aws.amazon.com/wellarchitected/latest/reliability-
pillar/availability.html. Accessed: 2022-05-18.

[5] AWS Nitro Enclaves.

https://aws.amazon.com/ec2/nitro/nitro-enclaves/. Accessed:
2022-05-14.

[6] Azure availability zones. https://docs.microsoft.com/en-

us/azure/availability-zones/az-overview. Accessed: 2022-05-18.

[7] Azure confidential computing.

https://azure.microsoft.com/en-us/solutions/confidential-compute.
Accessed: 2022-05-14.

[8] CRIU – A project to implement checkpoint/restore functionality for

Linux. https://criu.org/. Accessed: 2021-05-12.

Figure 10. Latency of key enclave migration operations
when varying enclave size. Enclave data is copied outside
without any encryption. Checkpoint and Destroy enclave op-
erations occur during checkpoint phase, Create and Restore
enclave operations occur during restore.

that OpenSSL outperforms mbedTLS by two orders of mag-
nitude, as mbedTLS takes nearly 33 seconds to checkpoint
and restore an enclave, while it takes 0.4 seconds to check-
point and 0.3 seconds to restore an enclave using OpenSSL.
This is validated when comparing Figures 6 and 10, as the
overhead introduced by OpenSSL is in the order of millisec-
onds, not seconds.

We observed from this evaluation it is preferable to use
OpenSSL – which utilizes specialized CPU instruction to
perform the encryption/decryption – whenever possible.
MbedTLS should only be used if there are no specialized
CPU instructions to accelerate cryptographic operations as
mbedTLS introduces less code into the enclaves trusted code
base. This allowed us to conclude that the performance of
enclave migration is dependant on the encryption and de-
cryption performance. Thus, we expect that, as more cryp-
tographic operations are optimized and offloaded to hard-
ware accelerators, the migration latency of CTR-SGX will
decrease.

7 Related Work

TEE migration. Currently, there are only two TEE architec-
tures (AMD SEV [1], SEV-SNP [2], and Intel TDX [12]) that
have publicly announced native support for migrating their
VM-based TEEs. Park et al. and Gu et al. proposed systems
that enable TEEs with Intel SGX enclaves to be migrated. The
drawback of these systems is that they either require native
hardware support or TEEs to be aware and coordinate with
the migration operator. In addition, they target VM-based
TEEs, not in-process ones.

TEEnder [21] is the only TEE migration system which
are aware of that targets in-process TEEs. However, it uses
Hardware Security Modules (HSMs) to encrypt and decrypt
SGX enclave data during migration. This hardware is not
standard on commodity cloud servers and thus this reduces
on which environments their solution can be used as well as
its performance.

12

8641282565121024Enclavesize[MB]012Operationlatency[s]DestroyenclaveCheckpointenclaveRestoreenclaveCreateenclaveCTR: Checkpoint, Transfer, and Restore for Secure Enclaves

[9] Designing resilient systems.

https://cloud.google.com/compute/docs/tutorials/robustsystems.
Accessed: 2022-05-18.

[10] Designing resilient systems. https://cloud.google.com/compute/docs/

tutorials/robustsystems#distribute. Accessed: 2022-05-18.

[11] Google Cloud confidential computing.

https://cloud.google.com/compute/confidential-vm/docs/about-cvm.
Accessed: 2022-05-14.

[12] Intel Trust Domain Extensions.

https://www.intel.com/content/www/us/en/developer/articles/
technical/intel-trust-domain-extensions.html. Accessed: 2022-05-14.

[13] Open Enclave SDK. https://openenclave.io/sdk/. Accessed:

2021-05-19.

[14] Regions and Availability Zones. https://aws.amazon.com/about-

aws/global-infrastructure/regions_az/#Availability_Zones. Accessed:
2022-05-18.

[15] Regions for virtual machines in Azure. https://docs.microsoft.com/en-

us/azure/virtual-machines/regions#feature-availability. Accessed:
2022-05-18.

[16] F. Alder, A. Kurnikov, A. Paverd, and N. Asokan. Migrating SGX
Enclaves with Persistent State. In 2018 48th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN),
2018.

[17] M. Alomari, M. Cahill, A. Fekete, and U. Rohm. The Cost of

Serializability on Platforms That Use Snapshot Isolation. In 2008 IEEE
24th International Conference on Data Engineering, pages 576–585,
2008.

[18] I. Anati, S. Gueron, S. Johnson, and V. Scarlata. Innovative technology
for CPU based attestation and sealing. In Proceedings of the 2nd
international workshop on hardware and architectural support for
security and privacy, volume 13, page 7. ACM New York, NY, USA,
2013.

[19] J. V. Bulck, M. Minkin, O. Weisse, D. Genkin, B. Kasikci, F. Piessens,
M. Silberstein, T. F. Wenisch, Y. Yarom, and R. Strackx. Foreshadow:
Extracting the Keys to the Intel SGX Kingdom with Transient
Out-of-Order Execution. In 27th USENIX Security Symposium
(USENIX Security 18), page 991–1008, Baltimore, MD, Aug. 2018.
USENIX Association.

[20] J. Gu, Z. Hua, Y. Xia, H. Chen, B. Zang, H. Guan, and J. Li. Secure Live
Migration of SGX Enclaves on Untrusted Cloud. In 2017 47th Annual
IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), 2017.

[21] J. Guerreiro, R. Moura, and J. N. Silva. TEEnder: SGX enclave
migration using HSMs. Computers & Security, 96, 2020.

[22] P. Jain, S. J. Desai, M.-W. Shih, T. Kim, S. M. Kim, J.-H. Lee, C. Choi,

Y. Shin, B. B. Kang, and D. Han. OpenSGX: An Open Platform for
SGX Research. In NDSS, volume 16, pages 21–24, 2016.
[23] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas,

M. Hamburg, M. Lipp, S. Mangard, T. Prescher, M. Schwarz, and
Y. Yarom. Spectre Attacks: Exploiting Speculative Execution. In 2019
IEEE Symposium on Security and Privacy (SP), pages 1–19, 2019.
[24] D. Lee, D. Kohlbrenner, S. Shinde, K. Asanović, and D. Song. Keystone:
An open framework for architecting trusted execution environments.
In Proceedings of the Fifteenth European Conference on Computer
Systems, EuroSys ’20, New York, NY, USA, 2020. Association for
Computing Machinery.

[25] J. Lee, J. Jang, Y. Jang, N. Kwak, Y. Choi, C. Choi, T. Kim, M. Peinado,
and B. B. Kang. Hacking in Darkness: Return-oriented Programming
against Secure Enclaves. In 26th USENIX Security Symposium (USENIX
Security 17), pages 523–539, Vancouver, BC, Aug. 2017. USENIX
Association.

[26] M. Lipp, M. Schwarz, D. Gruss, T. Prescher, W. Haas, A. Fogh, J. Horn,
S. Mangard, P. Kocher, D. Genkin, Y. Yarom, and M. Hamburg.
Meltdown: Reading Kernel Memory from User Space. In 27th USENIX
Security Symposium (USENIX Security 18), pages 973–990, Baltimore,
MD, Aug. 2018. USENIX Association.

[27] K. Murdock, D. Oswald, F. D. Garcia, J. Van Bulck, D. Gruss, and
F. Piessens. Plundervolt: Software-based Fault Injection Attacks
against Intel SGX. In 2020 IEEE Symposium on Security and Privacy,
pages 1466–1482, 2020.

[28] J. Park, S. Park, B. B. Kang, and K. Kim. eMotion: An SGX extension
for migrating enclaves. Computers & Security, 80:173–185, 2019.

[29] J. Park, S. Park, J. Oh, and J.-J. Won. Toward Live Migration of

SGX-Enabled Virtual Machines. In 2016 IEEE World Congress on
Services (SERVICES), 2016.

[30] M. Schwarz, M. Lipp, D. Moghimi, J. Van Bulck, J. Stecklina,

T. Prescher, and D. Gruss. ZombieLoad: Cross-Privilege-Boundary
Data Sampling. In Proceedings of the 2019 ACM SIGSAC Conference on
Computer and Communications Security, CCS ’19, pages 753–768, New
York, NY, USA, 2019. Association for Computing Machinery.
[31] C. Soriente, G. Karame, W. Li, and S. Fedorov. ReplicaTEE: Enabling
Seamless Replication of SGX Enclaves in the Cloud. In 2019 IEEE
European Symposium on Security and Privacy (EuroS P), pages
158–171, 2019.

[32] Y. Swami. Intel SGX Remote Attestation is not sufficient. 2017.
[33] S. van Schaik, A. Kwong, and D. Genkin. SGAxe: How SGX Fails in

Practice. 2020.

[34] Y. Xu, W. Cui, and M. Peinado. Controlled-Channel Attacks:

Deterministic Side Channels for Untrusted Operating Systems. In
2015 IEEE Symposium on Security and Privacy, pages 640–656, 2015.

13

