Acquiring a Dynamic Light Field through a Single-Shot Coded Image

Ryoya Mizuno†, Keita Takahashi†, Michitaka Yoshida‡, Chihiro Tsutake†, Toshiaki Fujii†, Hajime Nagahara‡
†Nagoya University, Japan, ‡Osaka University, Japan

2
2
0
2

r
p
A
6
2

]

V

I
.
s
s
e
e
[

1
v
9
8
0
2
1
.
4
0
2
2
:
v
i
X
r
a

Abstract

We propose a method for compressively acquiring a
dynamic light ﬁeld (a 5-D volume) through a single-shot
coded image (a 2-D measurement). We designed an imag-
ing model that synchronously applies aperture coding and
pixel-wise exposure coding within a single exposure time.
This coding scheme enables us to effectively embed the orig-
inal information into a single observed image. The ob-
served image is then fed to a convolutional neural network
(CNN) for light-ﬁeld reconstruction, which is jointly trained
with the camera-side coding patterns. We also developed a
hardware prototype to capture a real 3-D scene moving over
time. We succeeded in acquiring a dynamic light ﬁeld with
5×5 viewpoints over 4 temporal sub-frames (100 views in
total) from a single observed image. Repeating capture and
reconstruction processes over time, we can acquire a dy-
namic light ﬁeld at 4× the frame rate of the camera. To
our knowledge, our method is the ﬁrst to achieve a ﬁner
temporal resolution than the camera itself in compressive
light-ﬁeld acquisition. Our software is available from our
project webpage.1

1. Introduction

A light ﬁeld is represented as a set of multi-view im-
ages, where dozens of views are aligned on a 2-D grid with
tiny viewpoint intervals. This representation contains rich
visual information of a target scene and thus can be used
for various applications such as 3-D display [14, 38], view
synthesis [20,58], depth estimation [34,51], synthetic refo-
cusing [13, 25], and object recognition [17, 45]. The scope
of applications will further expand if the target scene is able
to move over time. However, a light ﬁeld varying over time,
i.e., a dynamic light ﬁeld, is challenging to acquire due to
the huge data rate, which is proportional to both the number
of views and frame rate.

Several approaches to acquire light ﬁelds have been in-
vestigated as summarized in Fig. 1. The most straightfor-
ward approach is to construct an array of cameras [5,37,49],
which requires bulky and costly hardware. The second ap-

1https://www.fujii.nuee.nagoya-u.ac.jp/Research/CompCam2

Figure 1. Our achievement compared with representative previous
works (camera array [49], lens-array camera [24], coded-aperture
camera [12], and coded exposure camera [54]). Axes are in rela-
tive scales w.r.t. camera’s spatial resolution and frame rate.

proach is to insert a micro-lens array in front of an image
sensor [1, 2, 24, 25, 29, 46], which enables us to capture a
light ﬁeld in a single-shot image. However, the spatial reso-
lution of each viewpoint image is sacriﬁced for the angular
resolution (number of views). In the above two approaches,
the frame rate of the acquired light ﬁeld is at most equiva-
lent to that of the cameras. Moreover, the data rate is not
compressed because each light ray is sampled individually.
The third approach aims to acquire a light ﬁeld compres-
sively by using a single camera equipped with a coded mask
or aperture [3, 6, 7, 12, 16, 18, 22, 23, 39, 41, 43]. This kind
of camera was used to obtain a small number of coded im-
ages, from which a light ﬁeld with the full-sensor spatial
resolution can be reconstructed. For static scenes, taking
more images with different coding patterns is beneﬁcial to
achieve higher reconstruction quality. However, for moving
scenes, the use of multiple coded images involves additional
complexities related to scene motions. Hajisharif et al. [8]
used a high dimensional light-ﬁeld dictionary that spanned
several temporal frames. However, their dictionary-based
light-ﬁeld reconstruction required a prohibitively long com-
putation time. Sakai et al. [31] handled scene motions by
alternating two coding patterns over time and by training
their CNN-based algorithm on dynamic scenes. However,
the light ﬁeld was reconstructed only for every two temporal

1

Angular res.Temporal res.Camera arrayLens arraycameraCoded aperturecameraCoded exposurecameraSpatial res.Ours 
 
 
 
 
 
frames (at 0.5× the frame rate of the camera).

In this paper, we advance the compressive approach sev-
eral steps further to innovate the imaging method for a dy-
namic light ﬁeld. As shown in Fig. 1, our method pursues
the full-sensor spatial resolution and a faster frame rate than
the camera itself. To this end, we design an imaging model
that synchronously applies aperture coding [12, 16, 23] and
pixel-wise exposure coding [9,30,48,54] within a single ex-
posure time. This coding scheme enables us to effectively
embed the original information (a 5-D volume of a dynamic
light ﬁeld) into a single coded image (a 2-D measurement).
The coded image is then fed to a CNN for light-ﬁeld re-
construction, which is jointly trained with the camera-side
coding patterns. We also develop a hardware prototype to
capture real 3-D scenes moving over time. As a result, we
succeeded in acquiring the dynamic light ﬁeld with 5×5
viewpoints over 4 temporal sub-frames (100 views in total)
from a single coded image. Repeating capture and recon-
struction processes over time, we acquired a dynamic light
ﬁeld at 4× the frame rate of the camera. To our knowledge,
our method is the ﬁrst to achieve a ﬁner temporal resolution
than the camera itself in compressive light-ﬁeld acquisition.

2. Background

2.1. Computational Photography

In the literature of computational photography, aperture
coding has been used to encode the viewpoint (angular) di-
mension of a light ﬁeld [6, 12, 16, 23], while exposure cod-
ing has been adopted to encode fast temporal changes in a
monocular video [9, 28, 30, 48, 54]. Our method combines
them to encode both the viewpoint (angular) and tempo-
ral dimensions simultaneously. Our method is also con-
sidered as an extreme case of snapshot compressive imag-
ing [44, 56, 57], where a higher dimensional (typically 3-D)
data volume is compressed into a 2-D sensor measurement.
We noticed that Vargas et al. [42] recently proposed an
imaging architecture similar to ours for compressive light
ﬁeld acquisition. However, their method was designed for
static light ﬁelds. Accordingly, their image formation model
implicitly assumed that the target light ﬁeld should be in-
variant during an exposure time (during the period when the
time-varying coding patterns were applied), which is theo-
retically incompatible with moving scenes. Moreover, they
did not report hardware implementation for the pixel-wise
exposure coding.
In contrast, our method is designed to
handle motions during each exposure time, and it is fully
implemented as a hardware prototype.

We model the entire imaging pipeline (coded-image ac-
quisition and light-ﬁeld reconstruction) as a deep neural net-
work, and jointly optimize the camera-side coding patterns
and the reconstruction algorithm. This design aligns with
the recent trend of deep optics [4,11,12,15,26,31,36,52,54]

where optical elements and computational algorithms are
jointly optimized under the framework of deep learning.
However, our method is designed to handle higher dimen-
sional data (dynamic light ﬁelds) than the previous works.

2.2. Light-Field Reconstruction

Reconstruction of a light ﬁeld from a coded/compressed
measurement is considered as an inverse problem, for which
several classes of methods can be used. Traditional meth-
ods [3, 18, 19] formulated this problem as energy mini-
mization with rather simple explicitly-deﬁned prior terms
and solved them using iterative algorithms. These methods
often result in insufﬁcient reconstruction quality and long
computation time. Recently, deep-learning-based meth-
ods [7, 12, 22, 41, 47, 53] have gained more popularity due
to the excellent representation capability of data-driven im-
plicit priors. Trained on a suitable dataset, these meth-
ods can acquire the capability of high-quality reconstruc-
tion. Moreover, reconstruction (inference) on a pre-trained
network does not require much computation time. Hy-
brid approaches have also been investigated. Algorithm
unrolling methods [6, 21] unroll procedures of iterative al-
gorithms into trainable networks, whereas plug-and-play
methods [56, 57] use pre-trained network models as build-
ing blocks of iterative algorithms.

We take a deep-learning-based approach and jointly opti-
mize the entire process (coded-image acquisition and light-
ﬁeld reconstruction) in the spirit of deep optics. For the
reconstruction part, we use a rather plain network architec-
ture to balance the reconstruction quality and the computa-
tional efﬁciency. Further improvement would be expected
with more sophisticated and light-ﬁeld speciﬁc network ar-
chitectures [6, 53]. We leave this as future work, because
the main focus of this paper is the design of the image ac-
quisition process rather than the reconstruction network.

In recent years, view synthesis from a single image [10,
27, 33, 35, 40, 50] has attracted much attention. In principle,
3-D reconstruction/rendering from an ordinary monocular
image (without coding) is an ill-posed problem; the results
are hallucinated by using the implicit scene priors learned
from the training dataset rather than the physical cues. In
contrast, our method aims to recover the 3D and motion
information that is embedded into a single image through
the camera-side coding process.

3. Proposed Method

3.1. Notations and Problem Formulation

A schematic diagram of the camera we assume is shown
in Fig. 2. Each light ray coming into the camera is param-
eterized with ﬁve variables, (u, v, x, y, t), where (u, v) and
(x, y) denote the intersections with the aperture and imag-
ing planes, respectively, and t denotes the time within a sin-

2

Figure 2. Example of dynamic light ﬁeld (left) and schematic dia-
gram of camera (right).

gle exposure time of the camera. We discretize the variable
space into a 5-D integer grid, where the range of each vari-
able is described as Sξ = [0, Nξ) (ξ ∈ {x, y, u, v, t}). By
using these variables, the intensity of a light ray is described
as Lx,y(u, v, t)2. Since (u, v) is associated with the view-
point (angle), Lx,y(u, v, t) is equivalent to a set of multi-
view videos, i.e., a dynamic light ﬁeld.

Our aim is to acquire the latent dynamic light ﬁeld
Lx,y(u, v, t): a 5-D volume with NxNyNuNvNt un-
knowns, from a single coded image Ix,y: a 2-D measure-
ment with NxNy observables. Hereafter, we assume Nu =
Nv = 5 and Nt = 4 unless mentioned otherwise.

3.2. Image Acquisition Model

If the camera has no coding functionalities (in the case

of an ordinary camera), the observed image is given by

Ix,y =

(cid:88)

Lx,y(u, v, t).

(u,v,t)∈Su×Sv×St

(1)

Each pixel value, Ix,y, is the sum of light rays over the
viewpoint (u, v) and temporal (t) dimensions. Therefore,
the variation along u, v, t dimensions is simply blurred out,
making it difﬁcult to recover.

Meanwhile, we design an imaging method that can ef-
fectively preserve the original 5-D information. We exploit
the combination of aperture coding and pixel-wise exposure
coding that are synchronously varied within a single expo-
sure time. The observed image is given as

Ix,y =

(cid:88)

a(u, v, t) px,y(t) Lx,y(u, v, t).

(2)

(u,v,t)∈Su×Sv×St

where a(u, v, t) ∈ [0, 1] (semi-transparency) and px,y(t) ∈
{0, 1} (on/off) are coding patterns applied on the aperture
and pixel planes, respectively. This imaging process can
be regarded as two-step coding as follows. First, a se-
ries of aperture coding patterns, a(u, v, t), is applied to

2For simplicity, we assume that a light ﬁeld has a single color channel.
When handling a light ﬁeld with RGB colors, we treat each color channel
as an individual light ﬁeld.

3

Figure 3. Coding patterns applied on aperture and pixel planes.

Lx,y(u, v, t) over time, which reduces the original 5-D vol-
ume into a 3-D spatio-temporal tensor, Jx,y(t), as

Jx,y(t) =

(cid:88)

a(u, v, t) Lx,y(u, v, t).

(3)

(u,v)∈Su×Sv

Next, the 3-D tensor, Jx,y(t), is further reduced into a 2-D
measurement, Ix,y, through the pixel-wise exposure coding
over time using px,y(t), as

Ix,y =

(cid:88)

t∈St

px,y(t) Jx,y(t).

(4)

By combining these two steps, we encode both the view-
point (u, v) and temporal (t) dimensions and embed them
into a single 2-D image.

An example of the coding patterns is shown in Fig. 3.
As mentioned later, these patterns are directly linked with
the parameters of a CNN (AcqNet), which is jointly trained
with another CNN for light-ﬁeld reconstruction (RecNet).
Therefore, these coding patterns are optimized for the train-
ing dataset so as to preserve as much of the light-ﬁeld infor-
mation as possible in the observed image.

Figure 4 shows two images (close-ups of the same por-
tion) obtained from a test scene through two imaging mod-
els: the ordinary camera (Eq. (1)) and ours (Eq. (2)). The
ordinary camera obtains a simply blurred observation, while
ours obtains a dappled image due to the coding patterns. To
further analyze the effect of coding, we also used a primitive
scene with a fronto-parallel plane (a primitive plane scene).
As shown in Fig. 5, we prepared an image G(x, y) with nine
bright points as the texture for the plane. We then synthe-
sized a dynamic light ﬁeld using the parameters for the 2-D
lateral velocity (αx, αy) [pixels per unit time] and disparity
d [pixels per viewpoint] (corresponding to the depth) as

Lx,y(u, v, t) = G(x − du − αxt, y − dv − αyt)

(5)

from which we computed an observed image by using
Eq. (2). Some resulting images obtained with different pa-
rameters are shown in Fig. 5 (the brightness is corrected
for visualization). These images can be interpreted as point
spreading functions (PSFs) for various motion and dispar-
ity values. Notably, these PSFs are distinct from each other.
Moreover, even in a single image, the PSFs for the nine

𝐿!,#(𝑢,𝑣,𝑡)	𝑣𝑢𝑦𝑥𝑡ApertureImager𝑝!,#(𝑡)	𝑎(𝑢,𝑣,𝑡)	(𝑢,𝑣)(𝑥,𝑦)𝐿!,#(𝑢,𝑣,𝑡)	Aperturecoding pattern𝑎(𝑢,𝑣,𝑡)Pixel-wise exposure coding pattern𝑝!,#(𝑡)Transmittance1.00.80.60.40.20.0Figure 4. Example images acquired by ordinary camera Eq. (1)
(left) and our imaging model of Eq. (2) (right).

Figure 6. Our camera prototype (left) and optical diagram (right).

Figure 5. Our imaging model yields distinct PSFs for different
motion and disparity values (coding patterns in Fig. 3 were used).

points differ from each other. These results show that both
motions and disparities, which are associated with changes
along the temporal (t) and viewpoint (u, v) dimensions, re-
spectively, are encoded by the various shapes of PSFs de-
pending on the spatial coordinate (x, y). The encoded in-
formation is not human readable, but can be deciphered by
the RecNet that is jointly trained with the coding patterns.

3.3. Hardware Implementation

We developed a prototype camera shown in Fig. 6 that
can apply aperture coding and pixel-wise exposure coding
within a single exposure time.

We used a Nikon Rayfact (25 mm F1.4 SF2514MC) as
the primary lens. The aperture coding was implemented
using a liquid crystal on silicon (LCoS) display (Forth Di-
mension Displays, SXGA-3DM), which had 1280 × 1024
pixels. We divided the central area of the LCoS display into
5 × 5 regions, each with 150 × 150 pixels. Accordingly,
the angular resolution of the light ﬁeld was set to 5 × 5.
The pixel-wise exposure coding was implemented using a
row-column-wise exposure sensor [54] that had 656 × 512
pixels. We synchronized the LCoS display with the image
sensor via an external circuit, so that four sets of coding
patterns were synchronously applied within a single expo-
sure time. The timing chart is shown in Fig. 7. The time
duration assigned for each coding pattern was set to 17 ms.

4

Figure 7. Time chart of our camera. Exposure timing is different
for four vertically divided regions on image sensor.

Accordingly, the unit time for the target light ﬁeld was also
17 ms (58.8 fps). Meanwhile, a single exposure time of the
camera ranged over the 4 time units (temporal sub-frames),
and thus, the interval between the two exposed images was
68 ms (14.7 fps in terms of the camera’s frame rate).

We mention several restrictions resulting from the image
sensor’s hardware. First, the sensor was not equipped with
RGB ﬁlters and was thus incapable of obtaining color in-
formation. Second, the coding patterns were not freely des-
ignable, because they were generated by the column-wise
and row-wise control signals repeating for every 8×8 pix-
els. Therefore, the applicable coding patterns were limited
to binary, 8×8-pixels periodic, and row-column separable
ones. This restriction was considered in our network de-
sign as mentioned later. Finally, due to the timing of the
vertical scan, the time duration covered by a single exposed
image depended on the vertical position. More precisely, as
shown in Fig. 7, the image sensor was vertically divided into
4 regions, each of which had a distinctive exposure timing
with 17 ms differences from the neighbors. Accordingly,
these regions were modulated by the same four sets of cod-
ing patterns but in different orders. To accommodate these
differences, we used a single instance for AcqNet, but per-
muted the order of time units in the input light ﬁeld for the
4 regions, respectively. We prepared 4 instances of RecNet
corresponding to the 4 regions and jointly trained them with
the coding patterns. This extension required four region-
wise reconstruction processes conducted in parallel, but still
maintained ×4 ﬁner temporal resolution than the camera.

(𝛼!,𝛼")(1,0)(1,1)𝐺(𝑥,𝑦)𝑑(0,0)−101Image sensorPrimary lensLCoSdeviceVirtual imager (conjugate to imager)AperturePrimarylensBeamsplitterLCoS(conjugate to aperture)Image sensorw. exposure controlRelay lensTime𝑎(𝑢,𝑣,𝑡)𝑝!,#(𝑡)Exposure time…𝑡$𝑡%𝑡&𝑡’𝑡(𝑡)𝑡*𝑡+Figure 8. Our network architecture consists of AcqNet and RecNet, which correspond to coded image acquisition and light-ﬁeld recon-
struction processes, respectively. Dynamic light-ﬁeld ranging over four temporal units is processed at once.

3.4. Network Design and Training

As shown in Fig. 8, our method was implemented as a
fully convolutional network, consisting of AcqNet and Rec-
Net. AcqNet is a differentiable representation of the image
formation model with trainable coding patterns, where a tar-
get light ﬁeld is compressed into a single observed image.
RecNet was designed to receive the observed image as input
and reconstruct the original light ﬁeld. The entire network
was trained end-to-end using the squared error against the
ground-truth light ﬁeld as the loss function. By doing so, the
image acquisition and light-ﬁeld reconstruction processes
were jointly optimized. When a real camera was used, the
coding patterns for the camera were tuned in accordance
with the trained parameters of AcqNet. Then, image acqui-
sition was conducted physically on the imaging hardware,
and only the reconstruction (inference on RecNet) was per-
formed on the computer.

AcqNet takes as input a dynamic light ﬁeld over 4 con-
secutive time units, which has Nx × Ny pixels and 5 × 5
viewpoints over 4 time units. The viewpoint dimensions
are unfolded into a single channel, resulting in 4 input ten-
sors with the shape of 25 × Nx × Ny. The ﬁrst block of
AcqNet corresponds to the aperture coding (Eq. (3)). To
implement this process, we followed Inagaki et al. [12]; we
used 2-D convolutional layers with 1 × 1 kernels and no
biases, where each kernel weight corresponds to the aper-
tures’ transmittance for each viewpoint. We prepared 4 sep-
arate convolutional layers for the 4 time units, in each of
which 25 channels were reduced into a single channel. The
outputs from these layers are stacked along the channel di-
mension, resulting in a tensor of 4 × Nx × Ny. The sec-
ond block corresponds to the pixel-wise exposure coding
(Eq. (4)), where 8 × 8 repetitive patterns are applied. For
this process, we prepared 64 separate convolutional layers
(1 × 1 kernels without biases), each of which takes a tensor
of 4 × Nx/8 × Ny/8 as input (every 8 × 8 pixels extracted

from the tensor of 4 × Nx × Ny) and reduces 4 channels
into a single channel. To constrain the coding patterns to
be hardware implementable (binary and row-column sepa-
rable), we used the same training technique as Yoshida et
al. [55] (see section 4.1 in [55]). The outputs from these
layers are stacked along the channel dimension, resulting
in a tensor of 64 × Nx/8 × Ny/8, which is equivalent to
a single observed image with Nx × Ny pixels. Finally, to
account for noise during the acquisition process, Gaussian
noise (zero-mean and σ = 0.005 w.r.t. the range of pixel
values [0, 1]) is added to the observed image.

RecNet accepts an output from AcqNet (or an image ac-
quired from a real camera) as a tensor of 64×Nx/8×Ny/8.
The ﬁrst 5 convolutional layers gradually increase the num-
ber of channels to 256, while keeping the spatial size un-
changed. Then, the tensor is reshaped into 4 × Nx × Ny
using a pixel shufﬂing operation [32]. The subsequent two
convolutional layers increase the number of channels to
100, followed by 19 convolutional layers and a residual con-
nection for reﬁnement. The output from RecNet is the latent
dynamic light ﬁeld represented as a tensor of 100×Nx×Ny,
where 100 channels correspond to 5 × 5 views over 4 time
units (temporal sub-frames). As mentioned in 3.3, four in-
stances of RecNet should be used in parallel to handle the
time differences among the four vertical regions.

We ﬁnally mention the training dataset. We ﬁrst col-
lected 223,020 light-ﬁeld patches from 51 static light ﬁelds
with intensity augmentation. Next, following Sakai et
al. [31], we gave 2-D lateral motions (in-plane translations)
to the collected patches to synthesize virtually-moving light-
ﬁeld samples. We used linear motions with constant ve-
locities: (αx, αy) [pixels per unit time], where αx, αy ∈
{−2, 1, 0, 1, 2}; this is equivalent to at most ±8 pixel trans-
lation per frame in terms of the camera’s frame rate. This
motion model was simple and limited, but it would be suf-
ﬁcient for the motions within a single exposure time, which
is short enough. We had 25 motion patterns in total, all

5

64 ch𝑁!/8𝑁"/8Observed imageStack64 ch128 ch128 ch128 ch256 ch1x1 Conv3x3 ConvPixel shuffle4 ch16 ch64 ch5x5 Conv3x3 Conv + ReLUAcqNetRecNetPixel-wise exposure coding1×1filterReshape4 ch88Aperture coding1×1filter25 ch4Reshape25 ch4 ch8 pix8 pix4 ch𝑁!/8𝑁"/8	8881 ch8𝑡𝑁"RearrangeInput dynamic light field𝑁!1×1Conv1×1ConvOutput dynamic light field100 ch100 ch100 ch19Split𝑁"𝑁!+of which were applied to each light-ﬁeld patch. To sum
up, we had 5,575,500 samples of dynamic light ﬁelds, each
with 64 × 64 pixels at 5 × 5 viewpoints over 4 time units.
Note that even a single training sample had a signiﬁcant
size (409,600 elements), which necessitated the network to
be lightweight.

We implemented our software using PyTorch. The net-
work was trained over ﬁve epochs using the Adam opti-
mizer. The training took approximately seven days on a
PC equipped with NVIDIA Geforce RTX 3090. We also
trained our model with 8 × 8 views and different ranges for
the assumed motions (αx, αy). Please refer to the supple-
mentary material for details.

4. Experiments

We conducted several quantitative evaluations using a
computer generated scene and experiments using our pro-
totype camera. To summarize, we succeeded in acquiring a
dynamic light ﬁeld with 4× ﬁner temporal resolution than
the camera itself. Note that there is no baseline to compete
against, because to our knowledge, no prior works have ever
achieved the same goal as ours. Please refer to the supple-
mentary video for better visualization of our results.

4.1. Quantitative Evaluation

Ablation study for the coding method. To validate our
image acquisition model in Eq. (2), we need to analyze
the effect of coding on the aperture (a(u, v, t)) and pixel
(px,y(t)) planes.
In addition to our original method (de-
noted as A+P), we trained three variants of our methods as
follows. Ordinary: no coding was applied (a(u, v, t) =
const, px,y(t) = const), which corresponded to light-ﬁeld
reconstruction from a single uncoded image. A-only: only
the aperture coding was enabled (px,y(t) = const). P-
only: only the pixel-wise exposure coding was enabled
(a(u, v, t) = const). Furthermore, to evaluate the theoreti-
cal upper-bound, we also prepared a free-form coding over
the 5-D space (denoted as Free5D), given by:

Ix,y =

(cid:88)

m(x, y, u, v, t)Lx,y(u, v, t)

(6)

(u,v,t)∈Su×Sv×St

where m(x, y, u, v, t) ∈ [0, 1] was a fully trainable modu-
lating pattern periodic over 8 × 8 pixels. Note that this is
only a software simulation; no hardware realization is avail-
able. The ﬁve methods mentioned so far were different in
the imaging models but aimed for the same goal: recon-
structing a dynamic light ﬁeld (5×5 views over 4 time units)
from a single observed image. For all the methods, RecNets
with the same network structure were jointly trained with
the respective coding patterns on the same training dataset
for the same number of epochs.

For quantitative evaluation, we used a computer gener-
ated light ﬁeld with 5 × 5 viewpoints over 200 temporal

frames, which was rendered from Planets scene provided
by Sakai et al. [31]. 3 Figure 9 visualizes several recon-
structed views (at the top-left viewpoint), horizontal epipo-
lar plane images (EPIs) along the green lines, and the differ-
ences from the ground truth (×3 pixel values). The average
peak signal-to-noise ratio (PSNR) values over the 25 view-
points are plotted along the temporal frames in Fig. 10.

As observed from these results, our method clearly out-
performed the other variants and even achieved quality
close to the ideal Free5D case. Meanwhile, A-only and P-
only resulted in poor reconstruction quality, showing their
insufﬁciency as coding methods. Moreover, the poor result
from Ordinary case indicated that although implicit scene
priors were learned from the training dataset, they alone
were insufﬁcient for high-quality reconstruction.
In con-
trast, the success of our method can be attributed to the elab-
orated coding method that was simultaneously applied on
the aperture and imaging planes, which helped effectively
embed the original 5-D information into a single observed
image. However, the reconstruction quality of our method
exhibited small ﬂuctuations over time. This was closely re-
lated to the fact that four time units (temporal frames) were
processed as a group. Moreover, our method did not include
mechanisms that could explicitly encourage the temporal
consistency, which will be addressed in the future work.

Working range analysis. We also evaluated the ef-
fective working range against motion and disparity using
a primitive plane scene. Following Eq. (5), we synthe-
sized a dynamic light ﬁeld over four time units by using
a natural image in Fig. 11 (left) as the texture. The aver-
age PSNR values obtained with our method (A+P) and the
three variants (A-only, P-only, and Ordinary) are shown in
Fig. 11 (right). Obviously, our method (A+P) can cover a
wider range of motion/disparity values than the others; P-
only performed poorly for d (cid:54)= 0; A-only and Ordinary did
not work well except for d = αx = 0.

In our method (A+P), the reconstruction quality de-
graded gradually as the velocity and disparity values in-
creased. This means that large motions/disparities are chal-
lenging for our method. The working range for the dis-
parity was mainly determined by the 3-D scene structures
contained in the original light-ﬁeld dataset, while the work-
ing range for the velocity was related to the virtual mo-
tions we assumed when synthesizing the dynamic dataset
from static light ﬁelds. Note that our imaging system has
densely-located viewpoints (bounded by the aperture) and a
high temporal resolution (4× the frame-rate of the camera);
therefore, both the motion and disparity are usually limited
within a small range.

Comparison with other methods. We ﬁnally compared
our method against three other methods. The ﬁrst two meth-
ods [6, 31] were based on coded-aperture imaging. From

3https://www.fujii.nuee.nagoya-u.ac.jp/Research/CompCam/

6

A+P (ours)

Free5D

A-only

P-only

Ordinary

Ground truth
(50-th frame)

Ground truth
(100-th frame)

A+P (ours)

Free5D

A-only

P-only

Ordinary

Figure 9. Visual results of our method (A+P), Free5D (ideal case), and three ablation cases (A-only, P-only, and Ordinary). Reconstructed
top-left views are accompanied with horizontal EPIs along green lines and differences from ground truth (×3 brightness).

frames (at 0.5× the frame rate) of the camera. We retrained
Guo et al.’s and Sakai et al.’s on the same dataset as ours
until convergence. In addition, we simulated a Lytro-like
camera, where each of the 5 × 5 views was captured with
the 1/5 × 1/5 spatial resolution at the same frame rate as
the camera. The acquired 5 × 5 views were upsampled to
the original resolution using bicubic interpolation and com-
pared against the ground truth.

For quantitative evaluation, we used Planets assuming
the camera’s frame rate to be the same as ours; accord-
ingly, in these three methods, image acquisition was con-
ducted only at every four temporal frames. Note that only
our method can obtain the light ﬁeld at 4× the frame rate
of the camera, and thus, this comparison only serves as a
reference. The average PSNR values over time are shown
in Fig. 12. The method of Sakai et al. [31] failed to fol-
low the fast scene motions, resulting in poor reconstruction
quality. The method of Guo et al. [6] reconstructed a ﬁnely
textured but geometrically inconsistent result, whereas the
Lytro-like camera produced a geometrically consistent but
blurred result. Our method achieved the best reconstruction
quality with ×4 ﬁner temporal resolution than the camera.

Please refer to the supplementary material for more de-

tailed analysis with different training conditions.

Figure 10. Quantitative reconstruction quality over time for our
method (A+P), Free5D (ideal case), and three ablation cases (A-
only, P-only, and Ordinary).

Guo et al. [6], we adopted a model where a light ﬁeld for
each time unit was reconstructed from a single observed
image, which resulted in frame-by-frame observation and
light-ﬁeld reconstruction at the same frame rate as the cam-
era. The method of Sakai et al. [31] observed three consecu-
tive images over time, and reconstructed a light ﬁeld for the
central time. The light ﬁeld was reconstructed for every two

7

24262830323404080120160200PSNR [dB]FrameOrdinaryP-onlyA-onlyA+P (ours)Free5D (upper-bound)Figure 11. Performance evaluation against various motion
and disparity values on primitive plane scene.

Figure 12. Quantitative quality over time compared against other
methods (Guo et al. [6], Sakai et al. [31], and Lytro-like camera).

Experimental setup

Reconstructed light ﬁeld

Figure 13. Experiment using our prototype camera: experimental setup (left) and reconstructed top-left view accompanied by two EPIs
along green and blue lines (center), and reconstructed light ﬁeld with 5 × 5 views (right).

4.2. Experiment Using Camera Prototype

We prepared a target scene by using several objects
(miniature animals) placed on an electronic turntable, which
produced motions in various directions. Our prototype cam-
era was used to capture the scene at 14.7 fps, from which we
reconstructed the dynamic light ﬁeld at 58.8 fps (4 temporal
frames from each exposed image). The reconstructed light
ﬁeld had 5 × 5 views, each with the full-sensor resolution
(656 × 512 pixels) for each time unit. Our experimental
setup and a part of the results are shown in Fig. 13. The re-
constructed light ﬁeld exhibited natural motions over time
and consistent parallaxes among the viewpoints (refer to the
supplementary video).

5. Conclusions

We proposed a method for compressively acquiring a
dynamic light ﬁeld (a 5-D volume) through a single-shot
coded image (a 2-D measurement). Our method was em-
bodied as a camera that synchronously applied aperture

coding and pixel-wise exposure coding within a single ex-
posure time, combined with a deep-learning-based algo-
rithm for light-ﬁeld reconstruction. The coding patterns
were jointly optimized with the reconstruction algorithm, so
as to embed as much of the original information as possible
in a single observed image. Experimental results showed
that by using a single camera alone, our method can suc-
cessfully acquire a dynamic light ﬁeld with 5 × 5 views at
4× the frame rate of the camera. We believe this is a sig-
niﬁcant advance in the context of compressive light-ﬁeld
acquisition, which will motivate the computational photog-
raphy community to investigate further. Our future work
will include improvement on the network design for better
reconstruction quality and generalization to different con-
ﬁgurations concerning the number of views and the number
of time units included in a single exposure time.

Acknowledgement:
thanks go to Yukinobu
Sugiyama and Kenta Endo at Hamamatsu Photonics K.K.
for providing the image sensor.

Special

8

𝛼!𝐺(𝑥,𝑦)𝑑0123401234A+P (ours)A-onlyP-onlyOrdinaryPSNR [dB]24262830323404080120160200A+P (ours)𝟒×fpsGuo1×fpsSakai0.5×fpsPSNR [dB]FrameLytro-like1×fpsCameraTarget sceneReferences

[1] Edward H Adelson and John YA Wang. Single lens stereo
with a plenoptic camera. IEEE transactions on pattern anal-
ysis and machine intelligence, 14(2):99–106, 1992. 1

[2] Jun Arai, Fumio Okano, Haruo Hoshino, and Ichiro Yuyama.
Gradient-index lens-array method based on real-time integral
photography for three-dimensional images. Applied optics,
37(11):2034–2045, 1998. 1

[3] S. Derin Babacan, Reto Ansorge, Martin Luessi, Pablo Ruiz
Mataran, Rafael Molina, and Aggelos K Katsaggelos. Com-
pressive light ﬁeld sensing. IEEE Transactions on image pro-
cessing, 21(12):4746–4757, 2012. 1, 2

[4] Ayan Chakrabarti. Learning sensor multiplexing design
In International Conference on
through back-propagation.
Neural Information Processing Systems, pages 3089–3097,
2016. 2

[5] Toshiaki Fujii, Kensaku Mori, Kazuya Takeda, Kenji Mase,
Masayuki Tanimoto, and Yasuhito Suenaga. Multipoint mea-
suring system for video and sound - 100-camera and micro-
phone system. In IEEE International Conference on Multi-
media and Expo, pages 437–440, 2006. 1

[6] Mantang Guo, Junhui Hou, Jing Jin, Jie Chen, and Lap-Pui
Chau. Deep spatial-angular regularization for compressive
light ﬁeld reconstruction over coded apertures. In European
Conference on Computer Vision, pages 278–294, 2020. 1, 2,
6, 7, 8

[7] Mayank Gupta, Arjun Jauhari, Kuldeep Kulkarni, Suren
Jayasuriya, Alyosha Molnar, and Pavan Turaga. Compres-
sive light ﬁeld reconstructions using deep learning. In IEEE
Conference on Computer Vision and Pattern Recognition
Workshops, pages 1277–1286, 2017. 1, 2

[8] Saghi Hajisharif, Ehsan Miandji, Christine Guillemot, and
Jonas Unger. Single sensor compressive light ﬁeld video
camera. Computer Graphics Forum, 39(2):463–474, 2020.
1

[9] Yasunobu Hitomi, Jinwei Gu, Mohit Gupta, Tomoo Mit-
sunaga, and Shree K. Nayar. Video from a single coded ex-
posure photograph using a learned over-complete dictionary.
In International Conference on Computer Vision, pages 287–
294, 2011. 2

[10] Ronghang Hu, Nikhila Ravi, Alexander C. Berg, and Deepak
Pathak. Worldsheet: Wrapping the world in a 3D sheet for
view synthesis from a single image. In International Confer-
ence on Computer Vision, 2021. 2

[11] Michael Iliadis, Leonidas Spinoulas, and Aggelos K. Kat-
saggelos. Deepbinarymask: Learning a binary mask for
video compressive sensing, 2016. 2

[12] Yasutaka Inagaki, Yuto Kobayashi, Keita Takahashi, Toshi-
aki Fujii, and Hajime Nagahara. Learning to capture light
ﬁelds through a coded aperture camera. In European Con-
ference on Computer Vision, pages 418–434, 2018. 1, 2, 5

[13] Aaron Isaksen, Leonard McMillan, and Steven J. Gortler.
In ACM SIG-

Dynamically reparameterized light ﬁelds.
GRAPH, pages 297–306, 2000. 1

[14] Seungjae Lee, Changwon Jang, Seokil Moon, Jaebum Cho,
and Byoungho Lee. Additive light ﬁeld displays: realiza-

tion of augmented reality with holographic optical elements.
ACM Transactions on Graphics, 35(4):1–13, 2016. 1
[15] Yuqi Li, Miao Qi, Rahul Gulve, Mian Wei, Roman Genov,
Kiriakos N. Kutulakos, and Wolfgang Heidrich. End-to-end
video compressive sensing using anderson-accelerated un-
rolled networks. In International Conference on Computa-
tional Photography, pages 137–148, 2020. 2

[16] Chia-Kai Liang, Tai-Hsu Lin, Bing-Yi Wong, Chi Liu, and
Homer H Chen. Programmable aperture photography: mul-
tiplexed light ﬁeld acquisition. ACM Transactions on Graph-
ics, 27(3):1–10, 2008. 1, 2

[17] Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, and
Rin-Ichiro Taniguchi. Light ﬁeld distortion feature for trans-
parent object recognition. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 2786–2793, 2013. 1

[18] Kshitij Marwah, Gordon Wetzstein, Yosuke Bando, and
Ramesh Raskar. Compressive light ﬁeld photography using
overcomplete dictionaries and optimized projections. ACM
Transactions on Graphics, 32(4):1–12, 2013. 1, 2

[19] Ehsan Miandji, Saghi Hajisharif, and Jonas Unger. A uniﬁed
framework for compression and compressed sensing of light
ﬁelds and light ﬁeld videos. ACM Transactions on Graphics,
38(3):1–18, 2019. 2

[20] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light ﬁeld fusion: Practical view syn-
thesis with prescriptive sampling guidelines. ACM Transac-
tions on Graphics, 38:1–14, 2019. 1

[21] Vishal Monga, Yuelong Li, and Yonina C. Eldar. Algorithm
Interpretable, efﬁcient deep learning for signal
IEEE Signal Processing Magazine,

unrolling:
and image processing.
38(2):18–44, 2021. 2

[22] Oﬁr Nabati, David Mendlovic, and Raja Giryes. Fast and
accurate reconstruction of compressed color light ﬁeld.
In
International Conference on Computational Photography,
pages 1–11, 2018. 1, 2

[23] Hajime Nagahara, Changyin Zhou, Takuya Watanabe, Hi-
roshi Ishiguro, and Shree K Nayar. Programmable aperture
camera using LCoS. In European Conference on Computer
Vision, pages 337–350, 2010. 1, 2

[24] Ren Ng. Digital light ﬁeld photography. PhD thesis, Stan-

ford University, 2006. 1

[25] Ren Ng, Marc Levoy, Mathieu Br´edif, Gene Duval, Mark
Horowitz, and Pat Hanrahan. Light ﬁeld photography with
a hand-held plenoptic camera. Computer Science Technical
Report CSTR, 2(11):1–11, 2005. 1

[26] Shijie Nie, Lin Gu, Yinqiang Zheng, Antony Lam, Nobutaka
Ono, and Imari Sato. Deeply learned ﬁlter response functions
for hyperspectral reconstruction. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 4767–
4776, 2018. 2

[27] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3D
ken burns effect from a single image. ACM Transactions on
Graphics, 38(6):1–15, 2019. 2

[28] Ramesh Raskar, Amit Agrawal, and Jack Tumblin. Coded
exposure photography: Motion deblurring using ﬂuttered
shutter. ACM Transactions on Graphics, 25(3):795–804,
2006. 2

9

[29] Raytrix:. 3D light ﬁeld camera technology, 2021. https:

//www.raytrix.de/. 1

[30] Dikpal Reddy, Ashok Veeraraghavan, and Rama Chellappa.
P2C2: Programmable pixel compressive camera for high
speed imaging. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 329–336, 2011. 2

[31] Kohei Sakai, Keita Takahashi, Toshiaki Fujii, and Hajime
Nagahara. Acquiring dynamic light ﬁelds through coded
aperture camera. In European Conference on Computer Vi-
sion, pages 368–385, 2020. 1, 2, 5, 6, 7, 8

[32] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Johannes Totz,
Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efﬁcient sub-pixel convolutional neural network. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1874–1883, 2016. 5

[33] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin
Huang. 3D photography using context-aware layered depth
In IEEE/CVF Conference on Computer Vision
inpainting.
and Pattern Recognition, 2020. 2

[34] Changha Shin, Hae-Gon Jeon, Youngjin Yoon, In So Kweon,
and Seon Joo Kim. EPINET: A fully-convolutional neural
network using epipolar geometry for depth from light ﬁeld
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 4748–4757, 2018. 1

[35] Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi
Ramamoorthi, and Ren Ng. Learning to synthesize a 4D
RGBD light ﬁeld from a single image. In IEEE International
Conference on Computer Vision, pages 2262–2270, 2017. 2
[36] He Sun, Adrian V. Dalca, and Katherine L. Bouman. Learn-
ing a probabilistic strategy for computational imaging sen-
sor selection. In International Conference on Computational
Photography, pages 81–92, 2020. 2

[37] Yuichi Taguchi, Takafumi Koike, Keita Takahashi, and
Takeshi Naemura. TransCAIP: A live 3D TV system using a
camera array and an integral photography display with inter-
active control of viewing parameters. IEEE Transactions on
Visualization and Computer Graphics, 15(5):841–852, 2009.
1

[38] Keita Takahashi, Yuto Kobayashi, and Toshiaki Fujii. From
focal stack to tensor light-ﬁeld display. IEEE Transactions
on Image Processing, 27(9):4571–4584, 2018. 1

[39] Salil Tambe, Ashok Veeraraghavan, and Amit Agrawal. To-
wards motion aware light ﬁeld video for dynamic scenes. In
IEEE International Conference on Computer Vision, pages
1009–1016, 2013. 1

[40] Richard Tucker and Noah Snavely. Single-view view syn-
thesis with multiplane images. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2020. 2

[41] Anil Kumar Vadathya, Sharath Girish, and Kaushik Mitra. A
uniﬁed learning based framework for light ﬁeld reconstruc-
tion from coded projections. IEEE Transactions on Compu-
tational Imaging, 6:304–316, 2019. 1, 2

[42] Edwin Vargas, Julien N. P. Martel, Gordon Wetzstein, and
Henry Arguello. Time-multiplexed coded aperture imaging:
Learned coded aperture and pixel exposures for compressive
imaging systems. In International Conference on Computer
Vision, 2021. 2

[43] Ashok Veeraraghavan, Ramesh Raskar, Amit Agrawal,
Ankit Mohan, and Jack Tumblin. Dappled photography:
Mask enhanced cameras for heterodyned light ﬁelds and
coded aperture refocusing. ACM Transactions on Graphics,
26(3):69, 2007. 1

[44] Ashwin Wagadarikar, Renu John, Rebecca Willett, and
David Brady. Single disperser design for coded aperture
snapshot spectral imaging. Appl. Opt., 47(10):B44–B51,
2008. 2

[45] Ting-Chun Wang, Jun-Yan Zhu, Ebi Hiroaki, Manmohan
Chandraker, Alexei Efros, and Ravi Ramamoorthi. A 4D
light-ﬁeld dataset and cnn architectures for material recogni-
tion. In European Conference on Computer Vision, volume
9907, pages 121–138, 2016. 1

[46] Ting-Chun Wang, Jun-Yan Zhu, Nima Khademi Kalantari,
Alexei A. Efros, and Ravi Ramamoorthi. Light ﬁeld video
capture using a learning-based hybrid imaging system. ACM
Transactions on Graphics, 36(4):133:1–133:13, 2017. 1
[47] Yunlong Wang, Fei Liu, Zilei Wang, Guangqi Hou, Zhenan
Sun, and Tieniu Tan. End-to-end view synthesis for light
ﬁeld imaging with pseudo 4DCNN. In European Conference
on Computer Vision, 2018. 2

[48] Mian Wei, Navid Sarhangnejad, Zhengfan Xia, Nikita Gu-
sev, Nikola Katic, Roman Genov, and Kiriakos N. Kutulakos.
Coded two-bucket cameras for computer vision. In European
Conference on Computer Vision, pages 55–73, 2018. 2
[49] Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Tal-
vala, Emilio Antunez, Adam Barth, Andrew Adams, Mark
Horowitz, and Marc Levoy. High performance imaging us-
ing large camera arrays. ACM Transactions on Graphics,
24(3):765–776, 2005. 1

[50] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin
Johnson. Synsin: End-to-end view synthesis from a single
image. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020. 2

[51] W. Williem, In Kyu Park, and Kyoung Mu Lee. Robust
light ﬁeld depth estimation using occlusion-noise aware data
costs. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(10):2484–2497, 2018. 1

[52] Yicheng Wu, Vivek Boominathan, Huaijin Chen, Aswin
Sankaranarayanan, and Ashok Veeraraghavan. Phasecam3D
— learning phase masks for passive single view depth esti-
mation. In International Conference on Computational Pho-
tography, pages 1–12, 2019. 2

[53] Henry Wing Fung Yeung, Junhui Hou, Xiaoming Chen, Jie
Chen, Zhibo Chen, and Yuk Ying Chung. Light ﬁeld spatial
super-resolution using deep efﬁcient spatial-angular separa-
IEEE Transactions on Image Processing,
ble convolution.
28(5):2319–2330, 2019. 2

[54] Michitaka Yoshida, Toshiki Sonoda, Hajime Nagahara,
Kenta Endo, Yukinobu Sugiyama, and Rin-ichiro Taniguchi.
High-speed imaging using CMOS image sensor with quasi
pixel-wise exposure. IEEE Transactions on Computational
Imaging, 6:463–476, 2020. 1, 2, 4

[55] Michitaka Yoshida, Akihiko Torii, Masatoshi Okutomi,
Kenta Endo, Yukinobu Sugiyama, Rin-ichiro Taniguchi, and
Hajime Nagahara. Joint optimization for compressive video

10

sensing and reconstruction under hardware constraints.
European Conference on Computer Vision, 2018. 5

In

[56] Xin Yuan, David J. Brady, and Aggelos K. Katsaggelos.
Snapshot compressive imaging: Theory, algorithms, and ap-
plications. IEEE Signal Processing Magazine, 38(2):65–88,
2021. 2

[57] Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai. Plug-and-
play algorithms for large-scale snapshot compressive imag-
ing. In IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 1444–1454, 2020. 2

[58] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo magniﬁcation: Learning view
synthesis using multiplane images. ACM Transactions on
Graphics, 37:1–12, 2018. 1

11

