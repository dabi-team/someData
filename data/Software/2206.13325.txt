BASHEXPLAINER: Retrieval-Augmented Bash Code
Comment Generation based on Fine-tuned
CodeBERT

Chi Yu†, Guang Yang†, Xiang Chen†∗, Ke Liu†, Yanlin Zhou†
†School of Information Science and Technology, Nantong University, China
Email: yc struggle@163.com, novelyg@outlook.com, xchencs@ntu.edu.cn, aurora.ke.liu@outlook.com, 1159615215@qq.com

2
2
0
2

n
u
J

7
2

]
E
S
.
s
c
[

1
v
5
2
3
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Developers use shell commands for many tasks,
such as ﬁle system management, network control, and process
management. Bash is one of the most commonly used shells
and plays an important role in Linux system development
and maintenance. Due to the language ﬂexibility of Bash code,
developers who are not familiar with Bash often have difﬁculty
understanding the purpose and functionality of Bash code. In
this study, we study Bash code comment generation problem
and proposed an automatic method BASHEXPLAINER based on
two-stage training strategy. In the ﬁrst stage, we train a Bash
encoder by ﬁne-tuning CodeBERT on our constructed Bash code
corpus. In the second stage, we ﬁrst retrieve the most similar code
from the code repository for the target code based on semantic
and lexical similarity. Then we use the trained Bash encoder
to generate two vector representations. Finally, we fuse these
two vector representations via the fusion layer and generate the
code comment through the decoder. To show the competitiveness
of our proposed method, we construct a high-quality corpus by
combining the corpus shared in the previous NL2Bash study and
the corpus shared in the NLC2CMD competition. This corpus
contains 10,592 Bash codes and corresponding comments. Then
we selected ten baselines from previous studies on automatic
code comment generation, which cover information retrieval
methods, deep learning methods, and hybrid methods. The
experimental results show that in terms of the performance mea-
sures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER
can outperform all baselines by at least 8.75%, 9.29%, 4.77%
and 3.86%. Then we design ablation experiments to show the
component setting rationality of BASHEXPLAINER. Later, we
conduct a human study to further show the competitiveness of
BASHEXPLAINER. Finally, we develop a browser plug-in based
on BASHEXPLAINER to facilitate the understanding of the Bash
code for developers.

Index Terms—Technological, Bash Command, Code comment

generation, Deep learning, Information retrieval

I. INTRODUCTION

Shell is the interface between the developer and the Linux
operating system. Nowadays, Linux operating system supports
different types of shells. Bash is the default shell command
language for Linux and has been widely used in Linux
development and maintenance. Compared with the advanced
programming languages (such as Java, Python), Bash language
is used in a few scenarios, but the role of the Bash language
during the Linux system development and maintenance still
cannot be ignored.

TABLE I
A POST RELATED TO BASH CODE UNDERSTANDING ON STACK
OVERFLOW

Post Title

what does “ set – ${@ : 2} ” mean in Bash shell?

Content

I have a shell script,
and a line in that is “set – ${@ : 2}”,
could you please tell me what does it mean?
I have tried in my test script,
it seems to remove the $1 param in args,
could you help me with more details?

Tags

Bash, shell

As a script language, the Bash language has the feature of
language ﬂexibility [1]. Therefore, for developers who are not
familiar with the Bash language, it is still challenging for them
to understand Bash code during the system development and
maintenance. Based on our statistical analysis, there are 86,846
Q&A posts related to the keyword “shell” and 143,743 Q&A
posts related to the keyword “Bash” on Stack Overﬂow until
March 2022. In Table I, we use a post1 from Stack Overﬂow
to show the necessity of automatically generating comments
for Bash codes. According to the content of this post, we ﬁnd
that this user could not understand the meaning of this Bash
code. Therefore, it is challenging for developers who are not
familiar with the Bash language to understand the purpose and
functionality of Bash code.

High-quality code comments can improve the readability
and comprehensibility of the code and play an important
role during software development and maintenance [2], [3].
However, developers often forget to write comments or do not
keep the code comments up to date, which can result in missed
or outdated code comments. Previous study [4] found that
understanding source code may take more than half of the time
in software development. Therefore, automatic code comment
generation is an important research topic in current software
engineering research. However, to our best knowledge, most of
previous studies [3], [5]–[11] mainly focused on source code
comment generation for popular programming languages (such
as Java and Python), while less attention has been paid to the

∗ Xiang Chen is the corresponding author.

1https://stackoverﬂow.com/questions/56229939/what-does-set-2-mean-in-

bash-shell

 
 
 
 
 
 
script language Bash.

code for developers.

In this study, we are the ﬁrst to study this problem and
propose a novel method BASHEXPLAINER for Bash code
comment generation, which uses both an information retrieval
method and a deep learning method. Speciﬁcally, BASH-
EXPLAINER includes two-stage training strategy: Bash code
encoder construction stage and Bash code comment genera-
tion stage. In the ﬁrst stage, to better generate code vector
representation, we use our constructed corpus to train Bash
encoder by ﬁne-tuning CodeBERT [12]. In the second stage,
we ﬁrst utilize an information retrieval module to retrieve the
most similar code for the target code. The information retrieval
module is designed based on semantic similarity and lexical
similarity. Then we input the target code and similar code into
the Bash encoder trained in the ﬁrst stage, which will output
two code vector representations. Later, we use normalization
operation to process these two vector representations and fuse
these two vectors via the fusion layer. Finally, the fused vectors
are passed through the decoder to generate the corresponding
code comment.

To evaluate the effectiveness of our proposed method BASH-
EXPLAINER, we constructed a high-quality corpus based on
the corpus shared in the previous study NL2Bash [1] and the
ofﬁcial data from the NLC2CMD competition2. We merged
these two corpora and removed duplicate data from them. No-
tice we only focused on the 135 most useful utilities identiﬁed
by Linux users by following the study of NL2Bash [1].

Since we are the ﬁrst to study the problem of automatic
Bash code comment generation, we select the state-of-the-
art methods from the previous studies on automatic code
comment generation as the baselines for BASHEXPLAINER.
These baselines cover deep learning-based methods [12]–[15],
information retrieval-based methods [3], [5], [16], [17] and
hybrid methods [18], [19]. The results of the automated eval-
uation and the human study show that our proposed method
BASHEXPLAINER can achieve better performance than all
the baselines. Finally, we also designed ablation experiments
to verify the component setting rationality in our proposed
method BASHEXPLAINER.

• We shared our corpus and scripts in our project home-
page3 to facilitate follow-up studies on the Bash code
comment generation problem.

II. RELATED WORK

In this section, we ﬁrst analyze the related studies on
generating Bash code from natural language descriptions. Then
we analyze the previous studies on automatic code comment
generation. Finally, we emphasize the novelty of our study.

A. Generating Bash Code From Natural Language Descrip-
tion

Due to the irregular syntax of Bash language, mapping
natural
language to corresponding Bash commands (i.e.,
NL2Bash) is a challenging task and has attracted the attention
of researchers. For example, Lin et al. [1] were the ﬁrst to
address this problem and constructed a corpus of Bash code
with natural language descriptions. Speciﬁcally, they collected
the corpus related to Bash code from related repositories
(such as developer Q&A forums, technical tutorials, technical
websites, and course materials). After data preprocessing,
they collected over 9,000 pairs of data covering over 100
Bash utilities. Finally, they considered three neural machine
translation models (i.e., Seq2Seq model [20], [21], CopyNet
model [13], and Tellina model [22]) and evaluated this task
at three different granularities. Kan et al. [23] proposed a
new GSAM (Grid Structure Attention Mechanism) mechanism
as part of the Seq2Seq model based on the study of Lin et
al. [22]. Speciﬁcally, the GSAM mechanism uses Bi-GRU
(Bidirectional GRU) [24], [25] as a feature extraction model,
maps these hidden states into a grid structure through a
neural network, and uses a convolutional neural network to
compute the adjacency features. They further used a copy
mechanism [13] to alleviate the OOV (out of vocabulary)
problem. Compared with the method proposed by Lin et
al. [22], this method can improve the performance by 7.0
and 7.3 percentage points in terms of BLEU-1 and BLEU-
3 respectively.

The main contributions of our study can be summarized as

B. Automatic Code Comment Generation

follows.

• We are the ﬁrst to study the automated Bash code com-
ment generation problem. Then we propose a retrieval-
augmented Bash code comment generation method
BASHEXPLAINER based on ﬁne-tuned CodeBERT.

• We constructed and shared a high-quality corpus for Bash
code comment generation. Then we conducted extensive
experiments on this corpus to evaluate the effectiveness
and the component setting rationality of our proposed
method. Both automated evaluation and human study
show that BASHEXPLAINER can outperform state-of-the-
art baselines.

• Based on BASHEXPLAINER, we developed a Chrome-
based plug-in to facilitate the understanding of the Bash

The information retrieval method was one of the earliest
kinds of methods since code reuse [26], [27] is more common
in large-scale code repositories. For corpora with high code
reuse, the use of information retrieval methods can achieve
better performance. For example, Haiduc et al. [16] ﬁrst used
VSM (Vector Space Model) and LSI (Latent Semantic Index)
to retrieve relevant terms from the corpus to construct code
comments. Eddy et al. [28] proposed the topic model hPAM
to select relevant terms from the corpus to construct code com-
ments. Wong et al. [29] used code snippets and corresponding
descriptions from the developer Q&A site Stack Overﬂow.
Speciﬁcally, they used the token-based code clone detection
tool SIM to detect similar code and used the comments of the
detected similar code as the ﬁnal comments. Later, Wong et

2https://eval.ai/web/challenges/challenge-page/674/leaderboard/1831

3https://github.com/NTDXYG/BASHEXPLAINER

al. [30] proposed the method CloCom. Speciﬁcally, they used a
token-based code clone detection tool to retrieve similar code
from GitHub and used the information from the reviews to
generate comments. Liu et al. [5] proposed a nearest neighbor-
based method NNGen, where they used a bag-of-words model
to convert the code commits in the training set into vectors,
and subsequently, selected the k code commits that are most
similar to the code commits based on the cosine similarity. The
BLEU-4 metric scores of the new code commit with these k
code commits are calculated, and the commit message of the
code commit with the highest BLEU-4 score is used as the
commit message for the new code commit.

To improve the generalization ability of the models, deep
learning methods have been used in recent studies. These
studies treat the code comment generation task as the neural
machine translation (NMT) task [31]. Then the source code
is used as the input and the code comment as the output,
and deep learning methods are used to train the model. Iyer
et al. [15] ﬁrst proposed CODE-NN, which used LSTM and
the attention mechanism to construct encoders and decoders.
Hu et al. [6] proposed the method DeepCom, which used the
abstract syntax tree to analyze the semantic and structural in-
formation of Java code and then proposed the SBT (structure-
based traversal) method to traverse the abstract syntax tree
and convert the abstract syntax tree into an AST sequence.
Ahmad et al. [32] used the Transformer model for code
comment generation. Transformer model [14] is based on the
Multi-headed Self-attention mechanism, which can efﬁciently
capture long-range dependencies. Yang et al. [7] proposed a
Transformer-based method ComFormer, which uses the lexical
and syntactic information of code snippets to effectively learn
the code semantics.

Some recent studies proposed hybrid methods (such as
the combination of information retrieval methods and deep
learning methods) and achieved promising performances. Wei
et al. [3] proposed the method Re2com, which used the
comment of the similar code snippet as an exemplar. This
method used the information of exemplar, target code, AST
information, and similar codes to help the neural network
model to generate code comments. Zhang et al. [19] proposed
the method Rencos, which retrieved the two most similar
code snippets from the code repository by considering both
syntactic similarity and semantic similarity. Then the target
code was encoded and these two codes retrieved similar
code snippets and generated comments by fusing them in the
decoding phase. Li et al. [33] considered the code comment
obtained by information retrieval as prototypes, and combined
the pattern information in the prototypes with the semantic
information of the input code, and then automatically edited
the prototypes by the trained neural network to generate code
comment.

C. Novelty of Our Study

language Bash. In this
comment generation for the script
study, we are the ﬁrst
to study the Bash code comment
generation problem and propose a novel retrieval-augmented
comment generation method based on ﬁne-tuned CodeBERT.
Our method employs the two-stage training strategy. In the ﬁrst
stage, we train the Bash encoder by ﬁne-tuning CodeBERT
on our constructed Bash corpus, which can generate higher-
quality code vector representation for Bash. In the second
stage, we ﬁrst retrieval
the most similar code based on
semantic and lexical similarity. Then we generate the code
vector representations of the target code and the most similar
code by the Bash encoder. Finally, we use the fusion layer
proposed by Yang et al. [34] to fuse these two vectors and
ﬁnally generate comment based on the decoder.

III. OUR PROPOSED METHOD BASHEXPLAINER

The framework of BASHEXPLAINER is shown in Fig-
ure 1. Speciﬁcally, BASHEXPLAINER utilizes two-stage train-
ing strategy: Bash code encoder construction stage and Bash
code comment generation stage. In the ﬁrst stage, we want to
train the encoder for the Bash code, which can generate higher-
quality vector representation of the Bash code. In this stage,
we ﬁne-tune the pre-trained model CodeBERT [12], [35] on
our constructed Bash corpus. In the second stage, given the
target code, we ﬁrst use the information retrieval module to
retrieval the most similar code from the code repository. Then
we generate vector representations for these two codes via the
Bash code encoder. Later we perform normalization operation
and then fuse these two vectors via the fusion layer [34].
Finally, we generate the comment via the decoder module.

A. Bash Code Encoder Construction Stage

CodeBert [12] is a bi-modal pre-training model for program-
ming languages (PL) and natural languages (NL) based on the
encoder in Transformer. CodeBert is pre-trained on a large-
scale general-purpose corpus by using two tasks: Masked Lan-
guage Model (MLM) and Replaced Token Detection (RTD).
Speciﬁcally, the MLM task targets bi-modal data by simul-
taneously feeding the code with the corresponding comments
and randomly selecting positions for masking, then replacing
the token with a special token [MASK]. The goal of the MLM
task is to predict the original token. The RTD task targets uni-
modal data with separate codes and comments and randomly
replaces the token, which aims to learn whether the token
is the original word via a discriminator. Speciﬁcally, in this
stage, we ﬁrst build a standard Seq2Seq model via CodeBERT
as the encoder. Then we ﬁne-tune the CodeBert using our
constructed Bash code corpus. In the ﬁne-tuning process, we
freeze the model parameters in CodeBERT. Finally, we train
the Bash code encoder, which can generate better semantic
representational information for the Bash code, and this Bash
code encoder can be used in the second stage.

To our best knowledge, previous studies [3], [5]–[11] mainly
focused on popular advanced programming languages (such
as Java and Python) and few studies focused on the code

B. Bash Code Comment Generation Stage

1) Information Retrieval Module: Code reuse [3], [36] is a
common practice in software development and similar codes

Fig. 1. The framework of our proposed method BASHEXPLAINER

usually have similar code comments. For the target code, we
can try to retrieve a similar code from the code repository.
However, the syntax of Bash commands with the command
options is irregular and it is difﬁcult to represent the Bash
code in the abstract syntax tree [1]. Therefore, we construct an
information retrieval module by considering both the semantic
similarity and the lexical similarity. Here, we ﬁrst retrieve top-
k similar codes from the code repository according to the
semantic similarity and then further retrieve the most similar
code from these k codes according to the lexical similarity.
The rationality of this retrieval order setting is evaluated in
Section V-C.
Semantic similarity-based retrieval. Considering the promis-
ing performance of CodeBERT in natural language search and
code document generation tasks [12], we use the ﬁne-tuned
encoder based on CodeBERT to obtain the semantic vector
representations for Bash codes. Then, the semantic similarity is
calculated by the Euclidean distance. Later, the top-k candidate
codes are retrieved from the code repository based on the
semantic similarity.

the
The detailed process is illustrated as follows. First
Bash code Xtar is input into the ﬁne-tuned encoder based on
CodeBERT. Then, the ﬁrst layer h0 and the last layer hn of the
hidden state are extracted, summed, and averaged to obtain the
semantic feature vector Vtar, where n is the number of hidden
layers.

Vtar = avg(h0 + hn)

(1)

Later, we can represent the Bash codes in the code repos-
itory into a vector set {Vi}N
i=1, where N is the number of
Bash codes in the code repository. Finally, we can compute
semantic-similarity between the target code vector Vtar and
the historical code vector Vi via the Euclidean distance.

semantic-similarity(Vtar, Vi) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

n
(cid:88)

j=1

(Vtar[j] − Vi[j])2

(2)
Based on the computed semantic-similarity, top-k can-
i=1 can be retrieved in the code

didate Bash codes {X}k
repository.
Lexical similarity-based retrieval. We follow the study of
Liu et al. [37] to calculate the lexical similarity by using
text edit distance. First, the set of candidate codes {X}k
i=1
is traversed, and we treat the traversed code Xi and the target
code Xtar as the set of tokens. Then we can calculate the
lexical-similarity between them via the text edit distance.

lexical-similarity(Xtar, Xi) = 1 −

dis(Xtar, Xi)
max(|Xtar|, |Xi|)

(3)

where dis(Xtar, Xi) is the text edit distance and |Xtar| is the
length of token sequence of Xtar. Finally the most similar
code Xsim is retrieved from the candidate codes based on
lexical-similarity.

2) Normalization Operation: After retrieving the similar
code Xsim, we use the ﬁne-tuned encoder based on Code-
BERT to obtain the semantic vector representations Mtar and
Msim (M ∈ Rbatch×N ×dmodel ), where the ﬁrst dimension is
the size of the batch, the second dimension is the length of
the input sequence, and the third dimension is the vector of
embedding representations corresponding to the words from
the target code Xtar and the similar code Xsim. Then we
want to normalize the third dimension of two representation
vectors, which can speed up the convergence of model training
and improve the performance of the trained model. Here we
follow the previous studies [38] and use the L2 Normalization
method to process the representation vectors, which can result
in the normalized vectors Vtar and Vsim.

Vtar =

Mtar
(cid:107)Mtar(cid:107)

=

Vsim =

Msim
(cid:107)Msim(cid:107)

=

Mtar

Mtar

2
i

Msim

(cid:115) t
(cid:80)
i=1

(cid:115) t
(cid:80)
i=1

are repeated N times, where N represents the number of layers
of the decoder. Finally, the output of the decoder ht is sent to
the fully connected neural network, which is then passed to
the softmax layer to predict the probability of the next token.

P (yt+1 | y1, · · · , yt) = softmax (htW + b)

(10)

(4)

(5)

Msim

2
i

where y denotes the predicted token. We train our model
parameters θ by the loss function L based on cross entropy:

where t is the dimension of the vector M .

3) Fusion Layer: The fusion layer is used to efﬁciently fuse
the two vectors Vtar and Vsim. In this study, we utilize the
fusion layer designed by Yang et al. [34] due to its simplicity
and effectiveness, which include three splicing methods.

G1 = F1([Vtar; Vsim])

G2 = F2([Vtar; Vtar − Vsim])

G3 = F3([Vtar; Vtar ◦ Vsim])

(6)

(7)

(8)

Where F1, F2, and F3 are three single-layer feedforward
neural networks with mutually independent parameters. [; ]
means the splicing of two vectors. − denotes the subtraction
of two vectors, which can highlight the difference between
the two vectors. ◦ means the dot product of two vectors,
which can highlight the similarity between the two vectors.
Then we directly splice the three obtained vectors, input them
into another feedforward neural network and then compute the
output G of the fusion layer as follows:

G = F ([G1; G2; G3])

(9)

4) Decoder Module: In the decoder module, we use Trans-
former’s decoder pair as the framework’s decoder. The au-
toregressive mechanism is used in the decoder, which can
predict the next possible word based on the previous content.
The speciﬁc implementation is that only the left part of
the current word is known when decoding. Therefore, the
masking mechanism is used to shield the inﬂuence of the right
part of the current word and maintain the characteristics of
autoregression. Each layer of the decoder performs additional
cross-attention calculations on the ﬁnal hidden layer of the
encoder and is connected by cross attention. Each decoder
layer performs an attention operation on the ﬁnal hidden state
of the encoder output, which can make the output of the
generated model closer to the original output.

Speciﬁcally,

the decoder uses the sentence sequence to
predict the next word. That is the previous output is input
to the Mask Multi-Head Attention layer. The function of
Mask Multi-Head Attention is to block the following words
to prevent information leakage. The next steps are almost the
same as those in the encoder. The output vector is ﬁrst passed
to the Residual Connection and Layer Normalization layer,
then is entered into the Feed-forward layer and is performed
residual connection and layer normalization. The above steps

L = −

|y|
(cid:88)

i=1

log Pθ(yi|y<i, x)

(11)

Previous studies [6], [39], [40] showed that using neural
networks’ maximum probability distribution to generate text
often leads to low-quality results. Recently, most studies [41]–
[44] used beam search to achieve better performance on
text generation tasks. Therefore,
in our proposed method
BASHEXPLAINER, we also use the beam search to generate
comments for Bash codes. Speciﬁcally, the beam search is a
compromise between the greedy strategy and the exhaustive
strategy. It retains top-k high-probability words at each step
of the prediction as to the input for the next time step, where
k denotes the beam size. The larger the value of k, the greater
the possibility of achieving better performance, but at the cost
of more computational cost.

IV. EXPERIMENTAL SETUP

A. Experimental Subject

To construct our corpus, we ﬁrst consider the corpus shared
by NL2Bash [1]. NL2Bash [1] was the ﬁrst study to map
natural language descriptions to Bash commands in Linux
and shared a high-quality corpus. Then the follow-up stud-
ies [23], [45] used this corpus as their experimental subject.
To better evaluate the performance of our proposed method,
we also augmented our corpus with the corpus shared by
the NLC2CMD competition. After augmentation, we removed
the duplicated samples from them. Finally, we constructed a
larger-scale corpus, which contains a total of 10,592 samples
and the composition of each sample in the corpus is <Bash
code, code comment>.

Table II shows the length statistics of the Bash code and
the corresponding comments in the corpus. In these tables,
we can ﬁnd that the length of most Bash codes in the corpus
is less than 20 and mainly around 8, and the length of the
corresponding comments is mainly around 11. In our empirical
study, we use a random sampling method to split the dataset
into the training set, the validation set, and the test set in the
ratio of 80%: 10%: 10% by following previous studies [37],
[46], [47].

B. Performance Measures

To compare the performance between BASHEXPLAINER
and baselines, we consider three performance measures (i.e.,
BLEU [48], METEOR [49], and ROUGE-L [50]). These
performance measures have been widely used in previous

TABLE II
LENGTH STATISTICS OF SAMPLES IN THE CORPUS

Code length statistics

Average Mode Median
4

8.528

7

<16
0.908

<32
0.997

<48
0.999

Code comment length statistics

Average Mode Median
10
11.874

11

<16
0.803

<32
0.995

<48
0.999

The second group contains deep learning methods and we

consider the following four baselines.

• CopyNet [13] is the model from the NL2Bash study [1]
with the best performance, which invokes the copy mech-
anism into the encoder-decoder structure.

• Transformer [32]

is an encoder-decoder

framework
based on Multi-head-Self-attention Mechanism and Po-
sition Encoding. Transformer have been widely used in
different NLP understanding and generation tasks with
promising results.

studies for neural machine translation and automatic code
comment generation [3], [6], [18], [19], [46], [51], [52]. We
illustrate the details of these performance measures as follows.
BLEU. BLEU (Bilingual Evaluation Understudy) [48] is one
of the ﬁrst proposed measures for machine translation quality
evaluation. BLEU is a precision-based similarity measure for
analyzing the degree of simultaneous occurrence of n-grams
between the candidate text and the reference text. The common
metrics are BLEU-1, BLEU-2, BLEU-3, and BLEU-4, where
n-gram refers to the number of consecutive words of n.
METEOR. METEOR (Metric for Evaluation of Translation
with Explicit Ordering) [49] is based on a single-precision
weighted summed average and single-word recall. METEOR
is designed to address the shortcomings in the BLEU measure.
ROUGE-L. ROUGE-L (Recall-Oriented Understudy for Gist-
ing Evaluation) [50] is a measure based on Recall. It is used
to calculate the length of the longest common subsequence
between the candidate text and the reference text. The longer
the length of this sequence, the higher the score of ROUGE-L.
To avoid result differences due to different implementations,
we use the implementations provided by the nlg-eval library4
for three performance measures, which can mitigate the threat
to the internal validity.

C. Baselines

Since there are no previous studies on Bash code comment
generation,
to show the competitiveness of our proposed
method BASHEXPLAINER, we compare our proposed method
with state-of-the-art baselines from previous studies on code
comment generation. Speciﬁcally, our chosen baselines can
cover three different groups. The ﬁrst group contains infor-
mation retrieval methods and we consider the following four
baselines.

• LSI [17] retrieves similar codes by calculating the dis-
tance between the text and the words in the corpus.
• VSM [16] uses the feature vector of codes for retrieval
and uses cosine similarity to retrieve similar codes from
the training set.

• BM25 [19] is a bag-of-words retrieval function for es-
timating the correlation between documents and a given
search query.

• NNGen [5] generates commit messages based on nearest
it sorts the code in terms of both cosine

neighbors,
similarity and BLEU value.

4https://github.com/Maluuba/nlg-eval

• CODE-NN [15] is the ﬁrst deep learning model to be
used for the comment generation task, which uses LSTM
and attention mechanisms to generate code comments.
• CodeBERT [12] uses the pre-trained CodeBERT model
as an encoder to build the encoder-decoder models.
The third group contains hybrid methods and we consider

the following two baselines.

• Hybrid-Deepcom [18] combines the lexical and struc-
information of Java methods and considers the
tural
syntactic information of the code by performing structural
traversal of the AST of the code.

• Rencos [19] is a retrieval-based neural source code
summary generation method that uses retrieved similar
codes to augment the neural model.

For the baselines (i.e., CopyNet [13], Transformer [32], and
CodeBERT [12]) that do not share code, we implemented
them according to the method description and the results of
our implementations are very close to the results reported in
the original studies. For the remaining baselines, we directly
utilized the scripts shared by original studies. To ensure a fair
comparison between BASHEXPLAINER and these baselines,
we tuned and optimized the hyper-parameters in these meth-
ods.

D. Experimental Settings

Our proposed method and some baselines were imple-
mented based on the PyTorch framework. We use the pack-
ages Faiss5, Textdistance6 and Transformers7 to implement
our proposed method. Speciﬁcally, we use the Levenshtein
edit distance [53] from the Textdistance package to calculate
lexical similarity, and use the Adamw optimizer for 30 epochs
with a learning rate of 2e-4 for the model training. The values
of the hyperparameters are optimized according to our best
practices and are shown in Table III.

We run all the experiments on a computer with an Intel(R)
Xeon(R) Silver 4210 CPU and a GeForce RTX3090 GPU with
24 GB memory. The running OS platform is Windows OS.

E. Tool implementation

To facilitate the developers to understand Bash codes, we
implemented a tool based on BASHEXPLAINER and integrated
it into the Chrome browser. The screenshot of our developed

5https://github.com/facebookresearch/faiss
6https://github.com/life4/textdistance
7https://github.com/huggingface/transformers

TABLE III
HYPER-PARAMETERS OF BASHEXPLAINER AND THEIR VALUES

Category

Hyper-parameter

Value

Deep Learning Part

decoder layers
hidden size
max input length
max output length
beam search size

Information Retrieval Part

top-k
CodeBERT hidden size

6
768
64
32
10

8
768

is shown in Figure 2. After calling out

tool
the plug-in,
developers can copy the Bash code into the input box and
click the “Generate Comment” button to quickly generate the
corresponding comment.

method. These results show that using simple information
retrieval methods is not suitable for the Bash comment gen-
eration task. The NNGen method uses cosine similarity as
well as BLEU values to retrieve similar code, and this method
can retrieve code that is more similar to the target code, so
the NNGen method can achieve a signiﬁcant improvement in
BLEU values. However, compared with the NNGen method,
BASHEXPLAINER can achieve higher performance in all per-
formance measures. Speciﬁcally, in terms of BLEU-1/2/3/4
measure, BASHEXPLAINER can improve the performance
by at least 3.93%, 7.20%, 8.75%, and 9.29%. In terms of
METEOR and ROUGE-L measures, BASHEXPLAINER can
also improve 4.77% and 7.21%. The possible reason is that
it
the retrieved similar code
and the target code can be perfectly matched by only using
the information retrieval method for our studied Bash code
comment generation problem.

to guarantee that

is difﬁcult

Secondly, we compare BASHEXPLAINER with the deep
learning baselines. We select CopyNet [13] as the baseline,
since CopyNet can achieve the best performance in the
NL2Bash study. Here we want to investigate whether this
method is suitable for Bash code comment generation and the
comparison results show the unsatisfactory performance of the
CopyNet method. Then, we ﬁnd that the CODE-NN method
and the CodeBERT method can achieve the best performance
among all deep learning baselines. However, our proposed
method BASHEXPLAINER can improve the performance by
at least 6.07%, 11.73%, 17.02%, and 22.59% in terms of the
BLEU-1/2/3/4 measures, while in terms of the METEOR and
ROUGE-L measures, BASHEXPLAINER can also improve the
performance by at least 6.81% and 3.86%. Compared with the
information retrieval method NNGen, the CODE-NN method
and the CodeBERT method perform worse than the NNGen
method in most performance measures. This indicates that
for Bash code comment generation, only using deep learning
methods cannot help to achieve promising performance. Since
most deep learning models require a large amount of training
data, the size of our constructed corpus for Bash code is still
small when compared to the corpus for Java or Python [6],
[54]. We conjecture this is one possible reason why the
deep learning method does not perform well in our study.
In addition, according to the statistical information of the
corpus shown in Section IV-A, the average length of Bash
code is 8.528. Due to the short length of most Bash code
snippets, it is challenging for deep learning methods to learn
semantic information in short texts, which may also lead to
poor performance of deep learning methods.

Finally, we compare our proposed method with the hybrid
baselines. Final comparison results show that our proposed
method BASHEXPLAINER can improve the performance by at
least 10.10%, 17.18%, 21.84%, and 24.80% in terms of the
BLEU-1/2/3/4 measures, while the method BASHEXPLAINER
can also improve the performance by at least 10.43% and
8.44% in terms of the METEOR and ROUGE-L measures.

Fig. 2. The screenshot of our developed Chrome browser plug-in

V. RESULT ANALYSIS

A. Result Analysis for RQ1

RQ1: Can our proposed method BASHEXPLAINER out-
perform the state-of-the-art baselines in the task of Bash
code comment generation?

In this RQ, we want to evaluate the performance of BASH-
EXPLAINER in an automated way. Since Bash code comment
generation has not been investigated in previous studies, we
consider 10 state-of-the-art baselines from similar problems
(such as code comment generation). These baselines can
be divided into three groups: information retrieval methods,
deep learning methods, and hybrid methods. Since measur-
ing the similarity between human-generated code comments
and model-generated comments is a challenging task, we
used three performance measures [48]–[50], which have been
widely used in previous studies of neural machine translation
and code comment generation.

Table IV shows the automatic evaluation results of our
proposed method BASHEXPLAINER and our considered base-
lines. In this table, we can ﬁnd that our proposed method
BASHEXPLAINER can achieve the best performance in terms
of all performance measures.

Firstly, we compare BASHEXPLAINER with the information
retrieval baselines. We ﬁnd that the performance of the infor-
mation retrieval methods is not high except for the NNGen

TABLE IV
COMPARISON RESULTS BETWEEN OUR PROPOSED METHOD BASHEXPLAINER AND BASELINES

Method Type

Method Name BLEU -1 (%) BLEU -2 (%) BLEU -3 (%) BLEU -4 (%) METEOR (%) ROUGE -L (%)

Information Retrieval

Deep Learning

LSI
VSM
BM25
NNGen

CopyNet
Transformer
CODE-NN
CodeBERT

Hybrid Method

Hybrid-DeepCom
Rencos

Our Method

BASHEXPLAINER

30.18
36.16
42.08
50.62

38.11
46.39
49.60
48.65

47.78
46.27

52.61

18.07
24.47
30.41
38.75

27.06
33.37
37.18
37.02

35.45
35.11

41.54

12.48
18.62
23.58
32.11

20.67
25.42
29.53
29.84

27.91
28.66

34.92

9.40
15.25
19.24
27.85

16.43
19.97
24.17
24.83

22.75
24.39

30.44

18.30
22.04
26.35
27.69

22.06
25.22
26.85
27.16

26.27
25.82

29.01

28.82
34.58
38.49
45.88

40.18
44.01
47.21
47.36

45.36
45.06

49.19

B. Result Analysis for RQ2

RQ2: Can two-stage training strategy help for improv-
ing the performance of our proposed method BASHEX-
PLAINER?

In this RQ, we want to investigate whether using the two-
stage training strategy is help for improve the performance.
Therefore, we design a control method w/o two-stage training
for comparison. In this control method, the hyper-parameters’
values of the Bash code encoder, the fusion layer, and decoder
are optimized through a single-stage training strategy.

Table V shows the ﬁnal comparison results. In this table,
we can ﬁnd that BASHEXPLAINER can signiﬁcantly improve
the performance in terms of all performance measures by
considering the two-stage training strategy. Speciﬁcally, using
the two-stage training strategy can improve the performance
13.5%, 16.1%, 19.3%, and 22.9% in terms of the BLEU-
1/2/3/4 measures, respectively. In terms of the METEOR and
ROUGE-L measures, using the two-stage training strategy
can improve the performance 8.1% and 2.3% respectively.
Therefore, construction Bash code encoder by ﬁne-tuning
CodeBERT can help to signiﬁcantly improve the performance
of BASHEXPLAINER and the two-stage training strategy is
necessary for our proposed method BASHEXPLAINER.

C. Result Analysis for RQ3

RQ3: How effective are the component settings of our
proposed method BASHEXPLAINER?

In this RQ, we aim to design a set of ablation experiments
to verify the component setting rationality of our proposed
method BASHEXPLAINER. To show the component setting
rationality of our proposed method BASHEXPLAINER, we
design a set of control methods. In particular, to analyze the
rationality of the information retrieval module in BASHEX-
PLAINER, we replace our information retrieval module with
the best information retrieval baseline NNGen and keep the
remaining modules unchanged (denoted as with NNGen). To
show the rationality of the similarity retrieval order in our
information retrieval module, we switched the retrieval order

in the similarity retrieval module (denoted as with Reverse
Retrieval). To show the setting rationality of the normaliza-
tion operation, we remove the normalization operation from
BASHEXPLAINER (denoted as w/o Normalization). To show
the setting rationality of the fusion layer, we replaced our con-
sidered fusion layer that contains three splicing methods with
the simple fusion method that directly splices two semantic
vectors (denoted as with Simple Fusion). Finally, to show
the competitiveness of using the encoder-decoder structure in
BASHEXPLAINER, we remove the encoder-decoder structure
and only use the information retrieval module (denoted as w/o
NMT).

Table VI shows the results of the ablation experiments.
After comparing these control methods, our proposed method
can achieve the best performance. Speciﬁcally, compared with
with NNGen and with Reverse Retrieval, we can ﬁnd that
our proposed information retrieval module is reasonable and
the order setting of ﬁrst retrieval by semantic similarity and
second retrieval by lexical similarity can help to retrieve
similar codes more effectively than other methods. Compared
with w/o Normalization, we can verify the rationality of the
normalization operation (i.e., normalizing the representation
vector can improve the performance of BASHEXPLAINER).
Compared with with Simple Fusion, the result shows that the
fusion layer can effectively fuse the representational informa-
tion of the similar code and the target code, which can ﬁnally
improve the quality of the generated code comments. This also
shows the fusion layer designed by Yang et al. [34] is still
effective for Bash code comment generation task. Compared
with w/o NMT, we can ﬁnd that when only using the
information retrieval method, it is difﬁcult to ensure that the
reused code comment can perfectly match the target code in
terms of code semantics.

VI. DISCUSSION

A. Comment Quality Comparison by Human Study

Using automatic measures can only assess the lexical gap
between the generated comments and the human-written com-

TABLE V
COMPARISON RESULTS FOR BASHEXPLAINER WITH OR WITHOUT THE TWO-STAGE TRAINING STRATEGY

Setting

BLEU -1 (%) BLEU -2 (%) BLEU -3 (%) BLEU -4 (%) METEOR (%) ROUGE -L (%)

w/o two-stage training
BASHEXPLAINER

46.35
52.61

35.77
41.54

29.26
34.92

24.75
30.44

26.83
29.01

48.06
49.19

TABLE VI
ABLATION STUDY RESULTS FOR OUR PROPOSED METHOD BASHEXPLAINER

Setting

BLEU -1 (%) BLEU -2 (%) BLEU -3 (%) BLEU -4 (%) METEOR (%) ROUGE -L (%)

with NNGen
with Reverse Retrieve
w/o Normalization
with Simple Fusion
w/o NMT
BASHEXPLAINER

50.74
51.85
51.13
51.51
51.81
52.61

39.22
40.86
40.38
40.44
40.52
41.54

32.37
34.35
34.05
33.84
33.96
34.92

27.75
30.02
29.69
29.01
29.62
30.44

28.26
28.72
28.78
28.02
28.45
29.01

48.41
48.41
49.10
48.21
47.76
49.19

ments. However, it does not truly reﬂect the semantic gap
between the generated code comments and the human-written
comments [3], [8], [15], [19]. Therefore, we conducted a
human study to measure the quality of comments generated
by NNGen [5], CodeBERT [12], Hybrid-DeepCom [18] and
BASHEXPLAINER, since these selected three baselines have
the best performance in each group of baselines. In our human
study, we followed the methodology used by previous studies
on source code comment generation [3], [33].

In our human study, we measure the quality of the com-

ments in three different perspectives:

• Similarity. This perspective concerns the similarity be-
tween the generated comment and the human-written
comment.

• Naturalness. This perspective concerns the grammatical-

ity and ﬂuency of the generated comment.

• Informativeness. This perspective concerns the amount
of content carried over from the Bash code to the gener-
ated comment, which ignores the ﬂuency of the comment.

To measure the quality of the generated comments, we ﬁrst
hired ﬁve master students, who have extensive experience in
using Bash for Linux system development and maintenance.
Then we randomly selected 100 Bash codes from the test
set, which include human-written comments and the com-
ments generated by four methods. We provide the human-
written comment since the hired students can score generated
comments by referring this ground truth. Each student was
asked to rate the four comments for each Bash code based
on similarity, naturalness, and informativeness on a scale
from 0 to 4, with higher scores indicating that
the code
comments can better meet the requirements. A sample of our
used questionnaire can be found in our project homepage.
During the review process, students can search the Internet
for relevant information and unfamiliar concepts. To guarantee

a fair comparison, students do not know which comment is
generated by which method, and the order of questionnaires is
different for different students. To guarantee the label quality,
we need each student to review only 50 Bash codes in half a
day.

TABLE VII
THE COMPARISON RESULTS (STANDARD DEVIATION IN PARENTHESES) OF
OUR HUMAN STUDY

Method

Informativeness

Naturalness

Similarity

NNGen
CodeBERT
Hybrid-DeepCom
BASHEXPLAINER

1.78 (1.32)
2.11 (1.22)
2.37 (1.54)
2.74 (1.35)

3.61 (0.89)
3.10 (1.18)
3.32 (1.02)
3.54 (0.76)

1.44 (1.65)
1.66 (1.63)
2.01 (1.25)
2.41 (1.30)

Table VII shows the the human study results between
BASHEXPLAINER and three representative baselines. In this
table, we can ﬁnd BASHEXPLAINER can outperform the other
three baselines by at least 15.6% in terms of Informativeness
perspective and 19.9% in terms of Similarity perspective,
respectively. It is worth noting that the performance in terms
of Naturalness perspective is only 0.07 points lower than the
NNGen method because the code comments generated by
the NNGen method are retrieved from the code repository.
The code comments in the code repository are all written
by developers, so they mainly conform to the natural lan-
guage syntax and have higher naturalness. The performance
of BASHEXPLAINER is slightly lower than that of the NNGen
method, which shows that BASHEXPLAINER can also generate
comments with higher naturalness. Similar to the results of
previous studies [3], [8], we used standard deviation and p-
values to further evaluate the experimental comparison re-
sults. The differences in the standard deviations of the four
methods are small, which means there are relatively few
differences in scoring among students. All p-values were less

TABLE VIII
GENERATED COMMENTS OF DIFFERENT METHODS FOR THREE BASH
CODES

ID

Example

1

2

3

Bash Code: ﬁnd . -type f -name *.php.
Ground Truth: ﬁnd all php ﬁles under current directory
Hybrid-DeepCom: search current directory tree for regular ﬁle
whose name end php
CodeBERT: ﬁnd all php ﬁle under current directory
NNGen: list all ﬁles/directories under current directory using
comma (,) as the delimiter for different ﬁelds in the output
BASHEXPLAINER: ﬁnd all php ﬁles under current directory

Bash Code: ﬁnd . -type l -! -exec test -e \; -print.
Ground Truth: ﬁnd all broken symlinks under current directory
Hybrid-DeepCom: convert all broken symlink under current directory
CodeBERT: ﬁnd broken symbol link
NNGen: search the ‘images’ directory tree for regular ﬁles
BASHEXPLAINER: ﬁnd all broken symlinks under current directory

Bash Code: unset array[‘shuf -i 0-3 -n1’].
Ground Truth: unsets random one from ﬁrst four array members.
Hybrid-DeepCom: unset random one in ﬁrst
CodeBERT: unset random one from ﬁrst four array member
NNGen:search directory /users/david/desktop/ recursively for
regular ﬁles with extensions .txt, .mpg, .jpg
BASHEXPLAINER: unset random one from ﬁrst four array members

than 0.05, which means the comparison result difference exists
statistically signiﬁcant. Finally, we used Fleiss Kappa [55] to
measure the consistency of the scoring results between these
ﬁve students. The ﬁnal Fleiss Kappa value is 0.723, which
indicate that there exists consistency in the scoring results of
these students.

B. Qualitative Analysis

Table VIII shows the code comments generated by four
methods for three Bash codes in the test set. In our empirical
study, we can ﬁnd that
the information retrieval method
NNGen can perform well in automatic evaluation. However,
directly reusing the comment from the corpus may not be
suitable for Bash code. For example, due to the language
ﬂexibility of Bash code, similar Bash codes may have different
semantics. For the last two Bash codes, the retrieved Bash
code by NNGen has a large semantic difference from the
target code. Moreover, we ﬁnd the CodeBERT method can
generate higher-quality comments than the Hybrid-DeepCom
method. However, the generated comment still has a certain
distance from the ground truth. For example, for the second
Bash code, the comment generated by CodeBERT misses the
phrase “under current directory” while the comment generated
by BASHEXPLAINER can match the ground-truth comment.
The reason is that BASHEXPLAINER can effectively fuse
information from the similar code and the target code.

VII. THREATS TO VALIDITY

In this section, we analyze potential threats to the validity

of empirical study.
Threats to internal validity. To avoid faults in the implemen-
tations, we check our implementation carefully and use mature
libraries. For example, we use the Faiss library for informa-
tion retrieval and the PyTorch framework for implementing

BASHEXPLAINER and baselines. The second threat
is the
considered baselines. Since the Bash code comment generation
problem has not been investigated in previous studies, we
mainly choose the state-of-the-art baselines from the source
code comment generation domain and aim to cover different
types of methods, such as information retrieval methods, deep
learning methods, and hybrid methods.
Threats to external validity. The main external threat is the
representativeness of the choice corpus. To alleviate this threat,
we ﬁrst consider the corpus provided by NL2Bash [1] and
the competition data of the NLC2CMD Challenge. Then we
merged these two corpora to construct a larger-scale corpus
and removed the duplicated Bash codes in these two corpora.
Threats to construct validity. The structural threats con-
cern the performance measures used to evaluate the perfor-
mance of BASHEXPLAINER and baselines. To mitigate these
threats, we chose three performance measures BLEU [48],
METEOR [49], and ROUGE-L [50]. These measures have
been widely used in previous studies on neural machine
translation and source code comment generation. Moreover,
we also conducted a human study to analyze the generated
comment quality in the manual way.
Threats to conclusion validity. Due to the high computational
cost of deep learning and the sufﬁcient number of samples in
our constructed corpus, we only split the dataset once. This
setting is consistent with the previous studies on source code
summarization [3], [19], [37]. To alleviate the threat to the
conclusion validity, we also randomly split our datasets three
times with different random seeds. Due to the limitation of
paper length, we show the detailed comparison result on our
project homepage and the comparison results also conﬁrm the
effectiveness of our proposed method.

VIII. CONCLUSION AND FUTURE WORK
In this study, we are the ﬁrst to investigate the problem of
automatic Bash code comment generation and then propose the
retrieval-augmented neural source code comment generation
method BASHEXPLAINER based on the ﬁne-tuned CodeBERT.
The results of the automated evaluation and human study show
that our proposed method BASHEXPLAINER can outperform
the state-of-the-art baselines from the previous studies of
automatic code comment generation on our constructed Bash
code corpus. Moreover, we also verify the component setting
rationality of BASHEXPLAINER by designing a set of ablation
experiments.

In the future, we ﬁrst want

to augment our corpus by
gathering more Bash code from GitHub and Stack Overﬂow,
which can help to cover more types of Bash commands. We
second want to improving the quality of generated comments
by designing more effective fusion methods.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their insightful comments and suggestions. Chi Yu and
Guang Yang have contributed equally to this work and they are
co-ﬁrst authors. This work is supported in part by the National
Natural Science Foundation of China (Grant no. 61872263).

REFERENCES

[1] X. V. Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst, “Nl2bash:
A corpus and semantic parser for natural language interface to the
linux operating system,” in Proceedings of the Eleventh International
Conference on Language Resources and Evaluation (LREC 2018), 2018.
[2] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker,
“Towards automatically generating summary comments for java meth-
ods,” in Proceedings of the IEEE/ACM international conference on
Automated software engineering, 2010, pp. 43–52.

[3] B. Wei, Y. Li, G. Li, X. Xia, and Z. Jin, “Retrieve and reﬁne:
exemplar-based neural comment generation,” in 2020 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE).
IEEE, 2020, pp. 349–360.

[4] X. Xia, L. Bao, D. Lo, Z. Xing, A. E. Hassan, and S. Li, “Measuring
program comprehension: A large-scale ﬁeld study with professionals,”
IEEE Transactions on Software Engineering, vol. 44, no. 10, pp. 951–
976, 2017.

[5] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, “Neural-
machine-translation-based commit message generation: how far are we?”
in Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, 2018, pp. 373–384.

[6] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment gener-
ation,” in 2018 IEEE/ACM 26th International Conference on Program
Comprehension (ICPC).

IEEE, 2018, pp. 200–20 010.

[7] G. Yang, X. Chen, J. Cao, S. Xu, Z. Cui, C. Yu, and K. Liu, “Comformer:
Code comment generation via transformer and fusion method-based
hybrid code representation,” in 2021 8th International Conference on
Dependable Systems and Their Applications (DSA).
IEEE, 2021, pp.
30–41.

[8] X. Hu, Z. Gao, X. Xia, D. Lo, and X. Yang, “Automating user notice
generation for smart contract functions,” in 2021 36th IEEE/ACM
International Conference on Automated Software Engineering (ASE).
IEEE, 2021, pp. 5–17.

[9] Z. Li, Y. Wu, B. Peng, X. Chen, Z. Sun, Y. Liu, and D. Paul,
“Setransformer: A transformer-based code semantic parser for code
comment generation,” IEEE Transactions on Reliability, 2022.

[10] G. Yang, K. Liu, X. Chen, Y. Zhou, C. Yu, and H. Lin, “Ccgir:
Information retrieval-based code comment generation method for smart
contracts,” Knowledge-Based Systems, vol. 237, p. 107858, 2022.
[11] Z. Li, Y. Wu, B. Peng, X. Chen, Z. Sun, Y. Liu, and D. Yu, “Secnn: A
semantic cnn parser for code comment generation,” Journal of Systems
and Software, vol. 181, p. 111036, 2021.

[12] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming
and natural languages,” in Findings of the Association for Computational
Linguistics: EMNLP 2020, 2020, pp. 1536–1547.

[13] J. Gu, Z. Lu, H. Li, and V. O. Li, “Incorporating copying mechanism
in sequence-to-sequence learning,” in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), 2016, pp. 1631–1640.

[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems, vol. 30, 2017.

[15] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing source
code using a neural attention model,” in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), 2016, pp. 2073–2083.

[16] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, “On the use of
automated text summarization techniques for summarizing source code,”
in 2010 17th Working Conference on Reverse Engineering.
IEEE, 2010,
pp. 35–44.

[17] S. Haiduc, J. Aponte, and A. Marcus, “Supporting program com-
prehension with source code summarization,” in 2010 acm/ieee 32nd
international conference on software engineering, vol. 2.
IEEE, 2010,
pp. 223–226.

[18] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation
with hybrid lexical and syntactical information,” Empirical Software
Engineering, vol. 25, no. 3, pp. 2179–2217, 2020.

[19] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, “Retrieval-based
neural source code summarization,” in 2020 IEEE/ACM 42nd Interna-
tional Conference on Software Engineering (ICSE).
IEEE, 2020, pp.
1385–1397.

[20] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[21] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
information processing

with neural networks,” Advances in neural
systems, vol. 27, 2014.

[22] X. V. Lin, C. Wang, D. Pang, K. Vu, and M. D. Ernst, “Program synthesis
from natural language using recurrent neural networks,” University of
Washington Department of Computer Science and Engineering, Seattle,
WA, USA, Tech. Rep. UW-CSE-17-03-01, 2017.

[23] J.-W. Kan, W.-C. Chien, and S.-D. Wang, “Grid structure attention for
natural language interface to bash commands,” in 2020 International
Computer Symposium (ICS).

IEEE, 2020, pp. 67–72.

[24] L. C. Jain and L. R. Medsker, Recurrent neural networks: design and

applications. CRC Press, Inc., 1999.

[25] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-
works,” IEEE transactions on Signal Processing, vol. 45, no. 11, pp.
2673–2681, 1997.

[26] M. Kim, V. Sazawal, D. Notkin, and G. Murphy, “An empirical study
the 10th European
of code clone genealogies,” in Proceedings of
software engineering conference held jointly with 13th ACM SIGSOFT
international symposium on Foundations of software engineering, 2005,
pp. 187–196.

[27] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccﬁnder: A multilinguistic
token-based code clone detection system for large scale source code,”
IEEE transactions on software engineering, vol. 28, no. 7, pp. 654–670,
2002.

[28] B. P. Eddy, J. A. Robinson, N. A. Kraft, and J. C. Carver, “Evaluating
source code summarization techniques: Replication and expansion,” in
2013 21st International Conference on Program Comprehension (ICPC).
IEEE, 2013, pp. 13–22.

[29] E. Wong, J. Yang, and L. Tan, “Autocomment: Mining question and an-
swer sites for automatic comment generation,” in 2013 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE).
IEEE, 2013, pp. 562–567.

[30] E. Wong, T. Liu, and L. Tan, “Clocom: Mining existing source code for
automatic comment generation,” in 2015 IEEE 22nd International Con-
ference on Software Analysis, Evolution, and Reengineering (SANER).
IEEE, 2015, pp. 380–389.

[31] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473,
2014.

[32] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “A transformer-
based approach for source code summarization,” in Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics,
2020, pp. 4998–5007.

[33] J. Li, Y. Li, G. Li, X. Hu, X. Xia, and Z. Jin, “Editsum: A retrieve-
and-edit framework for source code summarization,” in 2021 36th
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE).

IEEE, 2021, pp. 155–166.

[34] R. Yang, J. Zhang, X. Gao, F. Ji, and H. Chen, “Simple and effective
text matching with richer alignment features,” in Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, 2019,
pp. 4699–4709.

[35] X. Zhou, D. Han, and D. Lo, “Assessing generalizability of codebert,”
in 2021 IEEE International Conference on Software Maintenance and
Evolution (ICSME).
IEEE, 2021, pp. 425–436.

[36] M. Gharehyazie, B. Ray, and V. Filkov, “Some from here, some
from there: Cross-project code reuse in github,” in 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR).
IEEE, 2017, pp. 291–301.

[37] S. Liu, Y. Chen, X. Xie, J. K. Siow, and Y. Liu, “Retrieval-augmented
generation for code summarization via hybrid gnn,” in International
Conference on Learning Representations, 2020.

[38] A. Henry, P. R. Dachapally, S. S. Pawar, and Y. Chen, “Query-key
the Association for

normalization for transformers,” in Findings of
Computational Linguistics: EMNLP 2020, 2020, pp. 4246–4253.
[39] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, “Summarizing source
code with transferred api knowledge,” in Proceedings of
the 27th
International Joint Conference on Artiﬁcial Intelligence, 2018, pp. 2269–
2275.

[40] B. Liu, T. Wang, X. Zhang, Q. Fan, G. Yin, and J. Deng, “A neural-
network based code summarization approach by using source code

and its call dependencies,” in Proceedings of
Symposium on Internetware, 2019, pp. 1–10.

the 11th Asia-Paciﬁc

[41] S. Wiseman and A. M. Rush, “Sequence-to-sequence learning as beam-
search optimization,” in Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing, 2016, pp. 1296–1306.

[42] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee,
D. Crandall, and D. Batra, “Diverse beam search: Decoding diverse so-
lutions from neural sequence models,” arXiv preprint arXiv:1610.02424,
2016.

[43] M. Freitag and Y. Al-Onaizan, “Beam search strategies for neural
machine translation,” in Proceedings of the First Workshop on Neural
Machine Translation, 2017, pp. 56–60.

[44] K. Cao, C. Chen, S. Baltes, C. Treude, and X. Chen, “Automated
query reformulation for efﬁcient search based on query logs from
stack overﬂow,” in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE).

IEEE, 2021, pp. 1273–1285.
language processing: Unix command parsing for

[45] D. Trizna, “Shell

machine learning,” arXiv preprint arXiv:2107.02438, 2021.

[46] G. Yang, Y. Zhou, X. Chen, and C. Yu, “Fine-grained pseudo-code
generation method via code feature extraction and transformer,” in 2021
28th Asia-Paciﬁc Software Engineering Conference (APSEC).
IEEE,
2021, pp. 213–222.

[47] P. Liguori, E. Al-Hossami, D. Cotroneo, R. Natella, B. Cukic, and
S. Shaikh, “Shellcode ia32: A dataset for automatic shellcode gen-
eration,” in Proceedings of the 1st Workshop on Natural Language
Processing for Programming (NLP4Prog 2021), 2021, pp. 58–64.
[48] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics, 2002,
pp. 311–318.

[49] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evalua-
tion with improved correlation with human judgments,” in Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for
machine translation and/or summarization, 2005, pp. 65–72.

[50] L. C. ROUGE, “A package for automatic evaluation of summaries,” in
Proceedings of Workshop on Text Summarization of ACL, Spain, 2004.
[51] G. Yang, X. Chen, Y. Zhou, and C. Yu, “Dualsc: Automatic generation
and summarization of shellcode via transformer and dual learning,” in
Proceedings of The 29th IEEE International Conference on Software
Analysis, Evolution and Reengineering (SANER 2022), 2022.

[52] K. Liu, G. Yang, X. Chen, and C. Yu, “Sotitle: A transformer-based
post title generation approach for stack overﬂow,” in Proceedings of The
29th IEEE International Conference on Software Analysis, Evolution and
Reengineering (SANER 2022), 2022.

[53] V. I. Levenshtein et al., “Binary codes capable of correcting deletions,
insertions, and reversals,” in Soviet physics doklady, vol. 10, no. 8.
Soviet Union, 1966, pp. 707–710.

[54] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
“Improving automatic source code summarization via deep reinforce-
ment learning,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 397–407.

[55] J. L. Fleiss, “Measuring nominal scale agreement among many raters.”

Psychological bulletin, vol. 76, no. 5, p. 378, 1971.

