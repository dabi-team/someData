2
2
0
2

g
u
A
5
1

]
E
S
.
s
c
[

1
v
7
3
0
7
0
.
8
0
2
2
:
v
i
X
r
a

A Drift Handling Approach for Self-Adaptive ML Software in
Scalable Industrial Processes

Firas Bayram
Department of Mathematics and Computer Science,
Karlstad University, Universitetsgatan 2, Karlstad, 651 88,
Sweden
Firas.Bayram@kau.se

Bestoun S. Ahmed
Department of Mathematics and Computer Science,
Karlstad University, Universitetsgatan 2, Karlstad, 651 88,
Sweden
bestoun@kau.se

Erik Hallin
Uddeholms AB, UvedsvÃ¤gen, Hagfors, 683 33, VÃ¤rmlands
lÃ¤n, Sweden
erik.hallin@uddeholm.com

Anton Engman
Uddeholms AB, UvedsvÃ¤gen, Hagfors, 683 33, VÃ¤rmlands
lÃ¤n, Sweden
anton.engman@uddeholm.com

ABSTRACT
Most industrial processes in real-world manufacturing applications
are characterized by the scalability property, which requires an
automated strategy to self-adapt machine learning (ML) software
systems to the new conditions. In this paper, we investigate an Elec-
troslag Remelting (ESR) use case process from the Uddeholms AB
steel company. The use case involves predicting the minimum pres-
sure value for a vacuum pumping event. Taking into account the
long time required to collect new records and efficiently integrate
the new machines with the built ML software system. Addition-
ally, to accommodate the changes and satisfy the non-functional
requirement of the software system, namely adaptability, we pro-
pose an automated and adaptive approach based on a drift handling
technique called importance weighting. The aim is to address the
problem of adding a new furnace to production and enable the adapt-
ability attribute of the ML software. The overall results demonstrate
the improvements in ML software performance achieved by im-
plementing the proposed approach over the classical non-adaptive
approach.

KEYWORDS
ML software adaptability, automated industrial process, Electroslag
Remelting (ESR), non-functional requirements, changing environ-
ments, concept drift

ACM Reference Format:
Firas Bayram, Bestoun S. Ahmed, Erik Hallin, and Anton Engman. 2022. A
Drift Handling Approach for Self-Adaptive ML Software in Scalable Indus-
trial Processes. In Proceedings of 37th IEEE/ACM International Conference
on Automated Software Engineering (ASE 2022). ACM, City, State, Country,
5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASE 2022, xx - xx Month, 2022, United States
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $xx.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
In production systems, machine learning (ML) is an integral part of
a more extensive software system that performs the overall desired
functionalities [11]. With its powerful forecasting tools, the ML
system has been widely used in real-world industrial processes,
providing valuable insights for running businesses [4]. Modern
industries are usually characterized by different levels of scalability.
Scalable contexts include the upscaling of business machinery and
equipment, the acquisition of new customers, the change in market
demand, or the expansion of new products [1]. Therefore, industries
are imposed with a significant challenge that requires performing
a reasonable adaptation to work efficiently and cope with the new
conditions in non-stationary environments.

Adaptability is one of the essential non-functional requirements
(NFRs), also known as extra-functional requirements [14], of any
software deployed in a changing system. A general definition of
NFR software adaptability is the ability of the software to deal
with new conditions in response to changes in the environment
[17]. From an industrial perspective, adaptability is an NFR quality
factor that represents the extent to which the software can exhibit
business-level expandability. ML software is heavily relies on the
conditions under which the system was constructed. Therefore,
industrial ML software systems must inherit their characteristics
from the NFR in the design phase.

Traditional ML systems accumulate their knowledge based on
historical observations and trends. Thus, such systems would re-
quire their learned knowledge to be adjusted to the new conditions
to maintain high performance. Moreover, due to the increased com-
plexity of industrial systems, it would be preferable to perform
such adaptations in an automated manner and minimize human
intervention. The adaptation would reduce the time required to
install a new software system using a similar system already built.
At the same time, automation facilitates improving the overall per-
formance of the ML software and reducing costs by optimizing the
efforts needed to integrate the new conditions [9].

Modern industries in this competitive world always aim to evolve
their business. A common extension for industries is the need to
add new machinery for some industrial processes. Typically, this
new hardware is either an upgrade to existing similar machines
that perform the same functionality or introduces novel services
for the industry. From an ML perspective, both situations involve

 
 
 
 
 
 
ASE 2022, xx - xx Month, 2022, United States

Bayram F, et al.

the challenge of addressing the new conditions and getting the
predictive system up and running as quickly as possible.

Concept drift is the machine learning situation where training
and test conditions differ, which can lead to poor predictive per-
formance in production, that is, when ğ‘ƒğ‘¡ğ‘Ÿ (ğ‘‹, ğ‘Œ ) â‰  ğ‘ƒğ‘¡ğ‘’ (ğ‘‹, ğ‘Œ ) [2].
One type of concept drift is covariate shift, also known as domain
adaptation, is the setting when we want to transform a pre-learned
knowledge to a new condition in related domains [13], that is, when
ğ‘ƒğ‘¡ğ‘Ÿ (ğ‘‹ ) â‰  ğ‘ƒğ‘¡ğ‘’ (ğ‘‹ ) and ğ‘ƒğ‘¡ğ‘Ÿ (ğ‘Œ |ğ‘‹ ) = ğ‘ƒğ‘¡ğ‘’ (ğ‘Œ |ğ‘‹ ). The discrepancy in the
data distribution reflects the new conditions, whereas the identical-
ity in the conditional distributions reflects that we are learning the
same task.

The use case of this paper is carried out at Uddeholms AB steel
company in Sweden1. The use case investigates the Electroslag
Remelting (ESR) process in the steel manufacturing industry. The
ESR is a continuous process that is used to remelt, refine, and solidify
steel to produce ingots. The ESR process includes a sub-step, which
is the vacuum pumping that extracts the oxygen from the furnace.
The pumping process, which takes about 20 minutes, is stopped
when the vacuum chamber reaches a certain pressure threshold.
ML software predicts whether the desired threshold will be met
in each pumping event. However, on average, the pumping event
takes place once a day, which limits the amount of data that can be
collected and used for prediction. Early identification of pumping
events that will not meet expected quality is of great value to the
industry; it would help save the costs of inappropriate pumping
events by calling the maintenance team beforehand.

To address the aforementioned issue, we implement a covariate
shift solution that enables the use of data collected from old furnaces
and adapts it to be incorporated into the corresponding learning
procedure of a new furnace. In other words, using drift adaptation
techniques, we transfer the knowledge learned from the old furnace,
which represents the source domain in our use case, to the new
furnace, which represents the target domain. In this way, the scaling
of the industrial process will be handled very quickly and efficiently
rather than waiting for data collection from the new furnace. In
this paper, we show how drift adaptation techniques improve the
performance of predictive ML software when installed under new
conditions.

2 INDUSTRIAL CASE OVERVIEW
The use case investigated in this paper is the industrial process of
Electroslag Remelting (ESR) vacuum pumping. The vacuum pro-
cess aims to extract air oxygen from the vacuum chamber. This is
done for quality assurance purposes to ensure that the steel deliv-
ered is free of contaminants and gas and in an ideal condition for
subsequent procedures. A schematic sketch of the ESR process is
presented in Figure 1.

The Uddeholm steel company ESR process takes place approx-
imately once a day. And each pumping event lasts 20 minutes.
Therefore, it will take almost a year to collect data for around 300
events. Pressure observations are recorded from sensors connected
to the vacuum chambers. Sensors read pressure values and send

1https://www.uddeholm.com/

Figure 1: Electroslag Remelting Process Sketch.

them to a data storage system once every second. The storage sys-
tem stores both the pressure values of the current and historical
pumping events.

At the end of each pumping event, the minimum pressure value
in the vacuum chamber is evaluated. Suppose that the minimum
value does not reach the desired threshold. In that case, the furnace
will be checked by experts for maintenance purposes, and the pump-
ing event is re-initiated for an additional twenty minutes. Since the
pressure value is proportional to the amount of gas leakage and the
surface area of the vacuum chamber, the pressure value is used as
an indicator to inspect the quality of the furnaces.

ML software is used to forecast the minimum pressure value
based on the first observations of the pressure records that are
the input of the ML model. The learning task is implemented in an
automated setup for early detection of pumping issues and saves the
cost of improper pumping events. Given the limited amount of data
that can be collected and the use-case specifications, we developed
a self-adaptation solution using drift handling approaches. The
solution can be used in scalable processes and satisfy the NFR
adaptability, such as adding a new furnace where there are very
few (and possibly none) historical data records exist.

3 WORKFLOW
In this section, we present the overall workflow implemented to
address our ESR use case. This self-adaptive workflow aims to au-
tomatically address scalable processes in industries. The overall
workflow is illustrated in Figure 2. The evaluation is carried out
on the scenario of adding new furnace(s) to the company for the
vacuum pumping process. The issue arises from the lack of histori-
cal data for the new furnace, namely furnace B (the target domain),
and the relatively long time required to collect the data to train the
ML model. Therefore, the datasets collected from the old furnace(s),
namely furnace A (the source domain), are adjusted to the new
conditions and used to train the ML model.

Initially, the data were collected from furnace A and stored in
the system. Once a new furnace is introduced to the ESR process,
an automated process is started to re-weight the data of furnace B
according to the data distribution of the pressure values registered
from furnace A. The weight information is used to adapt the target

Soldified IngotMetal PoolSlag PoolStubElectrodeRamPowerSupplyA Drift Handling Approach for Self-Adaptive ML Software in Scalable Industrial Processes ASE 2022, xx - xx Month, 2022, United States

Figure 2: Automated drift-adaptive workflow for the ESR use case.

data to the source data and will be used as input to the ML model
for the new furnace. As illustrated in Figure 2, the weight values
are incorporated into the ML models that perform the predictive
task. For the latter, we have implemented decision tree algorithms
to predict the minimum pressure value of each vacuum pumping
event. Despite the fact that importance weighting is a general con-
cept and can be exploited in various ML algorithms by adjusting
the corresponding loss function [18], we implemented ensemble
algorithms based on decision trees which are Random Forests (RF)
[3] and XGBoost [6] for our solution due to their superiority in
providing good results in small run time with low computational
cost [10]. Furthermore, adding additional constraints to more com-
plex algorithms would result in a longer average prediction time,
which may violate the deadline required for the task. For the model
evaluation, we used the mean absolute percentage error (MAPE)
measure to monitor the performance of the ML model. MAPE is
one of the most commonly used performance metrics by industrial
practitioners for regression tasks due to its interpretability and
independence of the scale of the variables [21].

This weighting mechanism is called importance weighting, which
solves the problem of covariate shift by ensuring that the data dis-
tributions of the source and target domains are aligned [19]. The
weights are going to be incorporated by introducing a function ğ‘¤ (ğ’™)
that represents the density ratio between the target and source do-
ğ‘te (ğ’™)
ğ‘tr (ğ’™) , where ğ‘te is the data distribution
main labelled data: ğ‘¤ (ğ’™) =
(cid:9)ğ‘›te
of the target domain inputs (cid:8)ğ’™te
ğ‘–=1 and ğ‘tr is the data distribu-
ğ‘–
(cid:9)ğ‘›tr
tion of the source domain inputs (cid:8)ğ’™tr
ğ‘–=1. The main objective of
ğ‘–
importance weighting-based covariate shift corrections is to find
the optimal density ratio function ğ‘¤ (ğ’™) such that the empirical risk
under the source data distribution ğ‘…ğ‘¡ğ‘Ÿ
ğ‘¤ (â„“) matches the empirical
risk under the target data distribution ğ‘…ğ‘¡ğ‘’

ğ‘¤ (â„“) [16].

For covariate shift adaptation, we use the kernel mean matching
method (KMM) [8] that directly finds importance weights based
on the density ratio without estimating the densities for the source
and target data separately. Specifically, KMM finds the importance
estimates by minimizing the Maximum Mean Discrepancy (MMD)
between ğ‘te (ğ‘¥) and ğ‘¤ (ğ’™)ğ‘tr(ğ‘¥) in a Reproducing Kernel Hilbert
Space (RKHS) Î¦(ğ‘¥). KMM finds the importance weights by solving

the following optimization problem [20]:

min
ğ‘›tr
{ğ‘¤ğ‘– }
ğ‘–=1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

1
2

ğ‘›trâˆ‘ï¸
ğ‘–,ğ‘—=1

ğ‘¤ğ‘–ğ‘¤ ğ‘— ğ¾ğœ

(cid:16)

ğ’™tr
ğ‘– , ğ’™tr
ğ‘—

(cid:17)

âˆ’

ğ‘›trâˆ‘ï¸
ğ‘–=1

ğ‘¤ğ‘–ğœ…ğ‘–

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»
ğ‘¤ğ‘– âˆ’ ğ‘›tr

(cid:16)

(cid:16)

(cid:17)

ğ‘›tr
ğ‘›te

(cid:12)
(cid:12) â‰¤ ğ‘›trğœ–

subject to ğ‘¤ğ‘– âˆˆ [0, ğµ], ğ‘– = 1, . . . , ğ‘›ğ‘¡ğ‘Ÿ and (cid:12)
where ğ¾ğœ

(cid:12)(cid:205)ğ‘›tr
ğ‘–=1
is the radial basis function (RBF) kernel, or
(cid:205)ğ‘›te
ğ‘—=1

ğ‘– , ğ’™tr
ğ’™tr
ğ‘—
, ğµ and
Gaussian kernel, with width ğœ, ğœ…ğ‘– :=
ğœ– are tuning parameters that control the constraints on the mini-
mization problem. The first constraint restricts the weight interval
between 0 and ğµ to limit the influence of each input point. The sec-
ond constraint ensures that the estimated function ğ‘ƒğ‘¡ğ‘’ (ğ’™) is close to
a probability density function (PDF) [12]. For parameter selection,
we use the same value settings as suggested by Huang et al. in the
original paper [8], ğµ = 1000, and ğœ– = (cid:0)âˆš
ğ‘›ğ‘¡ğ‘Ÿ , while the
kernel width can be found using a cross-validation procedure.

ğ‘›ğ‘¡ğ‘Ÿ âˆ’ 1(cid:1) /

ğ‘– , ğ’™te
ğ’™tr
ğ‘—

ğ¾ğœ

âˆš

(cid:17)

4 EVALUATION RESULTS
In this section, we present the evaluation results after applying
the self-adaptation technique introduced in the previous section.
Comparisons were conducted separately on the old and new fur-
naces using adaptive and non-adaptive approaches. The results
were monitored and recorded using different time intervals with
respect to the pressure value of the vacuum pumping events. The
minimum threshold is forecast every 30 seconds between the first
and third minutes.

Figures 3a and 3b compare the results of applying the XGBoost
algorithm to the ESR data from the different furnaces. Figures 3c and
3d compare the results of applying the RF algorithm. The algorithms
are trained on data collected from historical pumping events of
furnace A and are evaluated on pumping events of both furnaces A
and B. The results show the MAPE rates using different temporal
intervals. We can see an improvement in the error rate when the
importance weighting is used in the four cases. More specifically,
the error rate is lower when the models are evaluated in the furnace
from where the pressure data were originally recorded. However,
the error is slightly higher when we evaluate the model on new

Furnace AFurnace BPressureRecordsÂ Furnace ADatasetImportanceWeightingML ModelResultsÂ Furnace BIndustrialprocessML softwareadaptationML softwaredeploymentMonitoring andobservabilityDataacquisitionDatastorageIndustry-basedactionASE 2022, xx - xx Month, 2022, United States

Bayram F, et al.

Table 1: Furnace A - XGBoost

Table 3: Furnace B - XGBoost

60

Without IW 30.926
Using IW
26.991
Improvement
12.726% 21.697% 32.248% 45.788% 61.702%

90
17.636
13.810

120
9.575
6.487

150
5.074
2.751

180
2.475
0.948

60

Without IW 30.926
With IW
26.991
Improvement
12.726% 14.830% 24.258% 35.268% 49.318%

90
16.733
14.252

120
9.381
7.105

150
5.176
3.351

180
2.783
1.410

Table 2: Furnace A - RF

Table 4: Furnace B - RF

60

Without IW 31.369
Using IW
27.061
Improvement
13.732% 28.204% 44.998% 62.818% 77.723%

90
19.190
13.778

120
11.718
6.445

150
7.314
2.720

180
4.209
0.938

60

Without IW 30.677
With IW
26.033
Improvement
15.138% 19.350% 26.985% 56.787% 71.909%

90
19.329
15.589

120
11.994
8.757

150
7.460
3.224

180
4.712
1.324

furnace datasets. This is because the data distribution of the new
furnaces may not be as similar to the data distribution of the old
furnace. Hence, fewer data points will have high-importance weight
estimates following the KMM method. Usually, the error rates of the
target domain are lower with the acquisition of more data points
as the estimated PDF approaches the actual one.

We can also see a steady decline in the error rate as we enlarge
the time window frame. As summarized in Tables 1 and 2, for fur-
nace A and without using the importance weighting technique, the
MAPE rate starts at more than 30% when using the pressure values
of the first minute. Gradually, it decreases when the pressure values
are used after the third minute to approximately 2.5% when apply-
ing the XGBoost algorithm and approximately 4.2% when applying
the RF algorithm. However, when using the importance weighting
technique, the MAPE rate starts approximately 27%, reflecting an
improvement of approximately 13%, and decreases to less than 1%
after using the pressure values after the third minute. Approxi-
mately 61% MAPE improvements for the XGBoost algorithm and
approximately 78% for the RF algorithm.

Analogously for furnace B and as revealed in Tables 3 and 4, the
MAPE rate starts at more than 30% after the first minute without
using the importance weighting technique. It reaches approximately
2.8% for the XGBoost algorithm and 4.7% for the RF algorithm. Using
the importance weighting technique, the error starts approximately
27% after the first minute and reaches around 1.4% after the third
minute for XGBoost, achieving approximately 13% improvement.
However, the error starts approximately 26% with the RF algorithm
after the first minute, reflecting an improvement of approximately
15% and decreases to approximately 1.32% after the third minute,
reflecting an improvement of approximately 72% of the MAPE
evaluation metric.

Observations from the experimental evaluation showed that
the application of an automated and adaptive solution yielded high
improvement rates and solved the problem of lack of data when new
conditions are introduced in industrial processes. The improvement
is proportional to the data size in both ML algorithms used; i.e.,
the importance weighting methods perform better when fed more
data points. Thus, the results demonstrate a trade-off between the
predictive performance of the ML software and the decision-making
time. Industry experts could determine the threshold that controls
the trade-off by setting a minimum value of the performance metric
to be achieved before signaling a maintenance alert.

5 CONCLUSION AND RELATED WORK
Our paper shows an industrial use case of applying an automated
approach that addresses scalable industrial processes. The approach
is based on ML software that is able to satisfy the adaptability char-
acteristic, which is considered a key non-functional requirement in
evolving systems. We discussed the problem of upscaling the ESR
process with a new furnace(s) in the Uddeholm steel company. The
adaptive ML software uses a drift handling technique called im-
portance weighting. The evaluation results show the improvement
in the error rate of the adaptive solution over the non-adaptive
solution. The adaptability of the proposed approach can open new
opportunities for industries to integrate expandable solutions in
scalable processes by reusing the knowledge extracted from previ-
ous similar tasks.

In recent studies, importance weighting has been used in various
domains to improve the performance of the predictive system. Rako-
tomamonjy et al. [15] has proposed a method that uses importance
weighting based on the Wasserstein distance between the source
and target domains and is applied to real-world visual domain adap-
tation problems. Similarly, in [22], the authors have incorporated
importance weighting techniques into deep active learning tasks
and evaluated the proposed approach on image datasets. For the
vacuum pumping problem, a recent study [7] has been presented
to predict the minimum value of a vacuum pumping event. The
exponentially weighted moving average (EWMA) chart has been in-
tegrated with the random forest algorithm in the predictive system.
In a different approach [5], a data augmentation approach has been
developed to generate more training samples using the principles
of vacuum pumping to improve prediction performance.

In future work, it would be interesting to utilize the importance
weights and incorporate them as one factor of the quality assurance
mechanism, specifically the data quality component, of the overall
MLOps system, in which the ML solution is eventually deployed. It
would also be beneficial to test other importance weighting strate-
gies and compare the results.

ACKNOWLEDGMENTS
This work has been funded by the Knowledge Foundation of Swe-
den (KKS) through the Synergy Project AIDA - A Holistic AI-
driven Networking and Processing Framework for Industrial IoT
(Rek:20200067).

A Drift Handling Approach for Self-Adaptive ML Software in Scalable Industrial Processes ASE 2022, xx - xx Month, 2022, United States

(a) XGBoost results- Furnace A.

(b) XGBoost results-Furnace B.

(c) RF results- Furnace A.

(d) RF results-Furnace B.

Figure 3: Evaluation results of the ESR use case

REFERENCES
[1] Riccardo Accorsi, Marco Bortolini, Francesco Gabriele Galizia, Francesco Gualano,
and Marcella Oliani. 2021. Scalability analysis in industry 4.0 manufacturing. In
Sustainable Design and Manufacturing 2020. Springer, 161â€“171.

[2] Firas Bayram, Bestoun S Ahmed, and Andreas Kassler. 2022. From concept
drift to model degradation: An overview on performance-aware drift detectors.
Knowledge-Based Systems (2022), 108632.

[3] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5â€“32.
[4] Ana Isabel Canhoto and Fintan Clear. 2020. Artificial intelligence and machine
learning as business tools: A framework for diagnosing value destruction poten-
tial. Business Horizons 63, 2 (2020), 183â€“193.

[5] Ayan Chatterjee, Bestoun S Ahmed, Erik Hallin, and Anton Engman. 2022. Test-
ing of Machine Learning Models with Limited Samples: An Industrial Vacuum
Pumping Application. arXiv preprint arXiv:2208.04062 (2022).

[6] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.

[7] Paras Garg, Amitkumar Patil, Gunjan Soni, Arvind Keprate, and Seemant Arora.
2021. Machine Learning-Based Abnormality Detection Approach for Vacuum
Pump Assembly Line. Reliability: Theory & Applications SI 2 (64) (2021), 176â€“187.
[8] Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard SchÃ¶lkopf, and
Alex Smola. 2006. Correcting sample selection bias by unlabeled data. Advances
in neural information processing systems 19 (2006).

[9] James Lee and Inc. Lilburn, GA: Fairmont Press. 2020. Bringing building automa-
tion systems under control. Web Based Enterprise Energy and Building Automation
Systems: Design and Installation (2020).

[10] Weiwei Lin, Ziming Wu, Longxin Lin, Angzhan Wen, and Jin Li. 2017. An
ensemble random forest algorithm for insurance big data analysis. Ieee access 5
(2017), 16568â€“16575.

[11] Lucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, Helena HolmstrÃ¶m Olsson, and
Ivica Crnkovic. 2019. A taxonomy of software engineering challenges for machine
learning systems: An empirical investigation. In International Conference on Agile
Software Development. Springer, Cham, 227â€“243.

[12] Yun-Qian Miao, Ahmed K Farahat, and Mohamed S Kamel. 2015. Ensemble
kernel mean matching. In 2015 IEEE International Conference on Data Mining.
IEEE, 330â€“338.

[13] Jose G Moreno-Torres, Troy Raeder, RocÃ­o Alaiz-RodrÃ­guez, Nitesh V Chawla,
and Francisco Herrera. 2012. A unifying view on dataset shift in classification.
Pattern recognition 45, 1 (2012), 521â€“530.

[14] Marco Panunzio and Tullio Vardanega. 2014. An architectural approach with
separation of concerns to address extra-functional requirements in the develop-
ment of embedded real-time software systems. Journal of Systems Architecture
60, 9 (2014), 770â€“781.

[15] Alain Rakotomamonjy, RÃ©mi Flamary, Gilles Gasso, M El Alaya, Maxime Berar,
and Nicolas Courty. 2022. Optimal transport for conditional domain matching
and label shift. Machine Learning 111, 5 (2022), 1651â€“1670.

[16] Petar Stojanov, Mingming Gong, Jaime Carbonell, and Kun Zhang. 2019. Low-
dimensional density ratio estimation for covariate shift correction. In The 22nd
International Conference on Artificial Intelligence and Statistics. PMLR, 3449â€“3458.
[17] Nary Subramanian and Lawrence Chung. 2001. Metrics for software adaptability.

Proc. Software Quality Management (SQM 2001) 158 (2001).

[18] Masashi Sugiyama and Motoaki Kawanabe. 2012. Machine learning in non-

stationary environments: Introduction to covariate shift adaptation. MIT press.

[19] Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Mo-
toaki Kawanabe. 2007. Direct importance estimation with model selection and
its application to covariate shift adaptation. Advances in neural information
processing systems 20 (2007).

[20] Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von
BÃ¼nau, and Motoaki Kawanabe. 2008. Direct importance estimation for covariate
shift adaptation. Annals of the Institute of Statistical Mathematics 60, 4 (2008),
699â€“746.

[21] Yaoli Wang, Lipo Wang, Fangjun Yang, Wenxia Di, and Qing Chang. 2021. Ad-
vantages of direct input-to-output connections in neural networks: The Elman
network for stock index forecasting. Information Sciences 547 (2021), 1066â€“1079.
[22] Eric Zhao, Anqi Liu, Animashree Anandkumar, and Yisong Yue. 2021. Active
learning under label shift. In International Conference on Artificial Intelligence
and Statistics. PMLR, 3412â€“3420.

MAPE010203040Seconds6090120150180Without Using IWUsing IWMAPE010203040Seconds6090120150180Without Using IWUsing IWMAPE010203040Seconds6090120150180Without Using IWUsing IWMAPE010203040Seconds6090120150180Without Using IWUsing IW