Multi-Frames Temporal Abnormal Clues
Learning Method for Face Anti-Spooï¬ng

Heng Cong*, Rongyu Zhang, Jiarong He, Jin Gao
Interactive Entertainment Group of Netease Inc, Guangzhou, China
{congheng, zhangrongyu, gzhejiarong, jgao}@corp.netease.com

2
2
0
2

g
u
A
8

]

V
C
.
s
c
[

1
v
6
7
0
4
0
.
8
0
2
2
:
v
i
X
r
a

Abstractâ€”Face anti-spooï¬ng researches are widely used in
face recognition and has received more attention from industry
and academics. In this paper, we propose the EulerNet, a new
temporal feature fusion network in which the differential ï¬lter
and residual pyramid are used to extract and amplify abnormal
clues from continuous frames, respectively. A lightweight sample
labeling method based on face landmarks is designed to label
large-scale samples at a lower cost and has better results than
other methods such as 3D camera. Finally, we collect 30,000
live and spooï¬ng samples using various mobile ends to create a
dataset that replicates various forms of attacks in a real-world
setting. Extensive experiments on public OULU-NPU show that
our algorithm is superior to the state of art and our solution has
already been deployed in real-world systems servicing millions
of users.

I. INTRODUCTION

Face anti-spooï¬ng (FAS) plays an important role in interac-
tive AI systems and is also a challenging task without speciï¬c
hardware equipped in the industry. The existing methods based
on multi-modal
infrared light, structured
light, and light ï¬eld) are widely used in scenarios, where there
are specialized hardware devices. Although these methods [1]â€“
[3] perform well in classiï¬cation, they cannot be used on
mobile devices on a broad scale.

information (e.g.

Initially, the traditional manual feature is used to recognize
face spooï¬ng [4]â€“[6]. With the development of convolution
neural network (CNN), classiï¬cation techniques are excelled
in the FAS task [7]â€“[9]. Recently, the application of face
depth information has further improved the FAS effectiveness.
The previous method performed excellently, but
there are
many shortcomings. In [10]â€“[13], single-frame information
is used to estimate the face depth for video prediction, but
the inter-frame information of the video is discarded. [14]â€“
[17] achieved better effects by considering the inter-frame
information, whereas the feature of face abnormal clues is not
be utilized. [18] label face depth with the 3D camera equipped
to focus on face detail, which is difï¬cult to promote. By
contrast, [19]â€“[21] convert 2D faces to 3D faces using PRNet,
which has a lower label cost but a higher error rate. Especially
in the prediction of large-angle side faces, [19]â€“[21] is not
effective. The binary mask label is obtained by the middle part
of the face, in [10], [22]â€“[25], where live and attack is marked
as 1 and 0, respectively. However, [10], [22]â€“[25] ignores the
face depth and edge texture information, which degrades the
effect of the model. Moreover, many datasets such as NUAA

* denotes corresponding author

[26], CASIA-MFSD [27], Replay-Attack [28], MSU-USSA
[29], and OULU-NPU [30] released several FAS data that
are collected in the laboratory state. However, because of the
uncertainty of lighting, scenarios, device, etc., the samples
in the laboratory varies greatly from the samples in the real
world.

To address the above problems, we propose a temporal
feature fusion network named EulerNet. The eulerian video
magniï¬cation is introduced to extract temporal clues and fuse
the inter-frame feature from the video. Residual Pyramid is
designed in EulerNet to fuse features of different resolutions.
We propose a feature-compressed attention module (FCAM)
to process features in the temporal dimension. To balance the
cost and accuracy, we propose a lightweight face labeling
method based on face landmarks, which is faster than [18]
and more accurate than [19]â€“[21]. The face position map
obtained by the proposed labeling method is easier to model
at ï¬ne-grain than depth map. Compared with binary mask,
more gradient information of the face edges is retained by
face position map. Finally, a large and comprehensive FAS
dataset is built to simulate the real usage background under
the industry environment. According to the real proportion of
cell phone models and application scenarios in real-world, we
collect live and attack by different types of mobile ends in
various scenarios for our dataset.

In summary, the main contributions of this work as follows:

â€¢ We propose a novel network architecture to extract abnor-
mal clues and process multi-frame temporal information.
The feature-compressed attention module (FCAM) and
Residual Pyramid are designed to improve the compu-
tational efï¬ciency, focus on prominent face information,
and amplify the weak signal.

â€¢ A lightweight face labeling method based on face land-
marks is presented to balance the labeling cost and
accuracy. Compared with binary mask and depth map,
face position map is accurate and fast.

â€¢ We build a large and comprehensive dataset, in which
live and attack samples are collected by different types
of mobile ends in various scenarios.

â€¢ The effectiveness, performance, and advantages of the
proposed method are numerically and experimentally
veriï¬ed.

DOI reference number: 10.18293/SEKE2022-076

 
 
 
 
 
 
II. THE PROPOSED METHOD

In this section, we ï¬rst introduce a temporal feature fusion
network based on eulerian magniï¬cation (EulerNet), including
feature-compressed attention module (FCAM) and Residual
Pyramid. Following EulerNet, a lightweight face labeling
method based on face landmarks is described and the super-
vision method based on face location map is presented.

A. EulerNet

We propose a temporal feature fusion network based on
eulerian magniï¬cation, namely EulerNet, to extract the ab-
normal clues from live and spooï¬ng. By applying eulerian
video magniï¬cation [31] to live and spooï¬ng faces, the import
clues for face anti-spooï¬ng are discovered. As illustrated
in Fig. 1, the result of print attack sample, lack a natural
motion transition has obvious distinguishability with other
attacks. Although replay attack and live sample have the
same motion transition, the pattern characteristics are different,
which provides reliable information for the network. Fig. 2
shows EulerNet architecture includes a Residual Pyramid and
a series of feature-compressed attention modules (FCAM) and
max pooling. Instead of processing the whole video, we extract
a sequence (length 4 and frame interval 3) from the video as
input to avoid the problems of excessive memory usage and
high derivation time overhead. The attention structure was
designed in FCAM to process the temporal features. Using
differential inï¬nite impulse response ï¬ltering, FCAM amplify
the subtle changes in faces between different frames. In Fig.
2, the feature level is divided from 256Ã—256 into 128Ã—128,
64Ã—64, and 32Ã—32 in EulerNet, which is fused by Residual
Pyramid. Different levels of features (128Ã—128, 64Ã—64, and
32Ã—32) are sampled for residuals to obtain effective frequency
for face anti-spooï¬ng.

Fig. 2: The proposed EulerNet.

y[n] = b0x[n] + h1[n âˆ’ 1]

h1[n] = b1x[n] + h2[n âˆ’ 1] âˆ’ a1y[n]

h2[n] = b2x[n] âˆ’ a2y[n]

(1)

(2)

(3)

where y[n] and x[n] present output and input at nth timestamp,
respectively. hi is the state matrix with 0. bi and aj is the
training parameters of the ï¬lter layer, i âˆˆ [0, 2], j âˆˆ [1, 2].
The sequential feature is utilized to ï¬lter video signals in
DIIRF and the effective frequency for face anti-spooï¬ng is
reserved. The sigmoid function as the gate control unit process
the feature from DIIRF. Attentional structure is built by
multiplying the feature map obtained by sigmoid back to the
original input. FCAM architecture is shown in Fig. 3.

(a) Live

(b) Print

(c) Replay

Fig. 1: Results of Applying Eulerian Video Magniï¬cation

Feature-compressed Attention Module. In FCAM, the
feature map channels are adjusted to 1 by the 3Ã—3 convolution
(Conv3Ã—3), which synthesizes information from each channel.
Inspired by the realization of speech signal [32], we transform
the implementation of an inï¬nite impulse response (IIR) ï¬lter
on the image feature map to construct a differential inï¬nite
impulse response ï¬lter (DIIRF). By initializing a state matrix
at the feature map size with 0, the output and the update for
state in time sequence can be described as

Fig. 3: Feature-compressed attention module.

Residual Pyramid. To amplify the weak signal for face
anti-spooï¬ng, we design Residual Pyramid to fuse feature.
As shown in Fig. 2, 3-level outputs from backbone network
are collected as the input of Residual Pyramid. In Residual
Pyramid, the 32Ã—32 (F32Ã—32) and 64Ã—64 (F64Ã—64) features
are upsampled to generate 64Ã—64 (F (cid:48)
64Ã—64) and 128Ã—128

InputOutputFCAM Ã—4Max PooingFCAM Ã—3Max PooingFCAM Ã—3Max PooingFCAMResidualPyramidFCAM Ã—2Labelst + âˆ†t t + 2âˆ†t t + 3âˆ†t t + 4âˆ†t L2 Loss128Ã—12864Ã—6432Ã—32DIIRFSigmoidConv3Ã—3inputoutputFeature compressedSignal captureAttention multiplyF (cid:48)
128Ã—128) feature. Then, the F (cid:48)
128Ã—128 feature
are subtracted by F64Ã—64 and F128Ã—128, respectively. After
the output S64Ã—64 and S128Ã—128 are
Conv3Ã—3 extracting,
obtained.

64Ã—64 and F (cid:48)

S128Ã—128 = Conv (F128Ã—128 âˆ’ U psample (F64Ã—64))

S64Ã—64 = Conv (F64Ã—64 âˆ’ U psample (F32Ã—32))

Ouput32Ã—32 = Concat(Donwsample(S128Ã—128),

Donwsample(S64Ã—64),
F32Ã—32)

(4)

(5)

(6)

The Residual Pyramid can help the network directly learn use-
ful information from features of different depths. In particular,
upsampling and downsampling are introduced to calculate the
residual between different resolutions.

Fig. 4: Residual Pyramid for feature fusing of different depths.

B. A lightweight face labeling method based on face land-
marks

Binary mask-based and Depth map-based labeling methods
are effective, but both have drawbacks. Binary mask [22] does
not distinguish between the face and background, and discards
the depth information, which reduces the network efï¬ciency.
For depth map-based labeling methods, additional labeling
resources are required. Compared with binary mask-based and
Depth map-based labeling methods, we connect the key points
of the face contour to form a closed face region, namely
face position map. In particular, the pixel points within the
face region are labeled as 1 and 0 on live face and spooï¬ng,
respectively. Face position map is used to reduce the labeling
time, preserve the gradient information of the face edges, and
improve accuracy. Fig. 5 shows the binary mask, depth map,
and face position map.

Fig. 5: Comparison of different supervision for FAS.

III. DATASET COLLECTION

The difference of data distribution in different application
scenarios determines the model performance in a speciï¬c
scenario. To address this problem, we simulate the real-world
scenario to establish the face anti-spooï¬ng dataset, where each
data sample is captured by the mobile end camera. In our
dataset, the majority of mobile ends currently on the market
are used for data collection. The spooï¬ng type is divided
into three parts: complex attack, certiï¬cate attack, and image
dynamic generation attack. In particular, it is difï¬cult to create
and conduct 3D attacks in practical scenarios, so we mainly
collect 2D attacks used in [33].

In our dataset, the complex attack includes various paper
attacks, projection attacks, high-deï¬nition quality shots, and
low-quality shots. Certiï¬cate attack is common in real-world
scenarios and different from other attacks, which consists of ID
card attacks, passport document attacks, etc. Image dynamic
generation attack is an AI attack method, where dynamic
images are generated from single images for face spooï¬ng.

Our dataset outperforms previous public datasets [27], [28],
[34] in three points: 1) Our dataset is the largest one that
includes 30,000 live and spooï¬ng videos (average duration to
be 2s), collected from 10000 subjects, compared to 12,000
videos from 200 subjects in [18]. 2) As shown in Fig. 6, our
dataset covers dozens of commonly used mobile ends. 3) The
data distribution of our dataset matches the real-world mobile
veriï¬cation scenarios.

Fig. 6: Statistics of electronic devices and scenes of our
dataset.

IV. EXPERIMENTS

A. Implementation Details

To verify the effectiveness of our proposed network archi-
tecture and supervision method, we conduct experiments on
the collected dataset, dividing the training, development, and
test sets according to the ratio of 3:1:1. The learning rate
is initialized as 1e-4 and the batchsize is set to 16 for a
synergistic optimizer Ranger [35]. The input to the model are

F128Ã—128F128Ã—128F64Ã—64F32Ã—32F64Ã—64LegendUp-SampleDown-SampleSubtractConcatenationConvConvï¼‡ï¼‡LiveBinary MaskDepth MapLocation MapEnvironmentOutdoorIndoorAndroid system device25%20%15%10%5%0016%14%12%10%8%6%4%2%IOS system deviceDarkBacklightExposureNew WindowNear LightRoadsideGardenSquareCourtIndoorOutdoor60%40%23%15%23%20%21%23%40%7%30%sequences of 4 consecutive frames taken from the same video
at intervals of 3. Thus we compute 64 images in one gradient
update. The model is trained with Nvidia Tesla V100s. Pytorch
is utilized as the backend for the network architecture. L2 loss
is utilized to guide the network predict the location map.

Average Classiï¬cation Error Rate (ACER) is the mean value
of Attack Presentation Classiï¬cation Error Rate (APCER) and
Bona Fide Presentation Classiï¬cation Error Rate (BPCER),
given by

AP CER =

F P
T N + F P

, BP CER =

F N
T P + F N

ACER =

AP CER + BP CER
2

(7)

(8)

In the next subsections, ACER is uniformly utilized for evalu-
ation. For each video, we extracted multiple non-overlapping
sequences,
then calculated the live score individually and
ï¬nally average scores for video.

B. Ablation Study

To investigate the behavior of EulerNet, we conducted
several ablation studies. Table I shows the ablation study
on our dataset. The baseline model is the proposed method
using all improvements. Compare 1 (C1) and Compare 5 (C5)
discuss the inï¬‚uence of different labels, including binary mask
and depth map. In Compare 3 (C3), the Residual Pyramid is
replaced by channels concatenation. Compare 4 (C4) removes
the FCAM. Compare 2 (C2) discards all structure improve-
ments and uses the common depth map for global comparison.
Effectiveness of FCAM and Residual Pyramid. After
adding FCAM and d Residual Pyramid, ACER decreased by
0.34 percent and 0.38 percent, respectively, indicating that they
were beneï¬cial in FAS. The results suggest that extracting
the effective frequency information of the input and fusing
different resolution features together in a residual manner is
meaningful for face anti-spooï¬ng. Table I demonstrate that we
must guide the network to mine as many anomalous patterns as
possible in the attack video. The ACER of the proposed model
is reduced by 0.7% concerning Compare 2 in Table I. We
optimize the combination of models and labels to maximize
the beneï¬ts and obtain excellent accuracy on our dataset.

Effectiveness of Face Location Map Supervision. The
optimal model on the development set is utilized to check the
test set. Our proposed model yields the best ACER, achieving
0.18% lower than the model supervised with depth map and
0.96% lower than the model supervised with binary mask. This
indicates that the features with binary mask supervision are not
robust and discriminative, which are learned from labeling all
pixels of living images as 1. The practice of regression on face
location map reduces the difï¬culty of pixel modeling task and
makes the network focus more on the color texture of face
region and edge gradient information.

As shown in the Fig. 7, we plot the training performance
curves of different models. At the beginning, the curve without
FCAM drops the fastest because the model does not extract
frequency information. Although the early iteration is fast, the

TABLE I: Ablation study on our dataset.

Structure

ACER(%)â†“

Tag

Compare 1
Compare 2
Compare 3
Compare 4
Compare 5
Baseline

Label

Binary Mask
Depth Map
Face Location Map
Face Location Map
Depth Map
Face Location Map

FCAM
âˆš

Residual
Pyramid
âˆš

Ã—
âˆš

Ã—
âˆš
âˆš

Ã—
Ã—
âˆš
âˆš
âˆš

Dev

3.95
3.62
2.85
3.13
2.74
2.48

Test

2.84
2.57
2.26
2.22
2.06
1.88

Fig. 7: Performance comparison curve on the development set
in training process.

accuracy in the later stage is low, which suggests that it is sig-
niï¬cant to mine the speciï¬c frequency information. In the late
training phase (epochs 45-50), the ï¬‚uctuating curves are sorted
by mean value size as C2>C1>C4>C3>C5>our method.
Especially,
the proposed method curve shows a smoother
decreasing trend during training with less ï¬‚uctuation.

C. Visualization

The Grad-cam [38] is used to visualize the output of the
neurons in the model to compare the changes brought by
changing the structure or the supervision method. Fig. 8 details

Fig. 8: Visualization study of neuron outputs at different
depths: (a) our method, (b) w/o FCAM, (c) w/o Residual
Pyramid. From left to right are the results with increasing
downsampling multiplier as well as the multi-resolution fusion
results and the ï¬nal prediction maps. Their resolutions are
128Ã—128, 64Ã—64, 32Ã—32, 32Ã—32, 32Ã—32 in order.

                    ( S R F K V        $ & ( 5    E L Q D U \  P D V N  &   U H P R Y H  D O O  L P S U R Y H P H Q W V  &   Z  R  5 H V L G X D O  3 \ U D P L G  &   Z  R  ) & $ 0  &   G H S W K  P D S  &   R X U  P H W K R G(a)(b)(c)Max Pooling 1Max Pooling 2Max Pooling 3Fusion ResultPredicted Map0.364Score0.4530.4850.485, which is better than the model removed FCAM and
Residual Pyramid. In addition, as the network deepened, the
extracted features mined more abnormal clues and produced
more accurate face location maps.

Fig. 9 shows the visualization results for the three different
labels (face location map, binary map, and depth map) are
studied in this paper. For the live face, the heat map supervised
by the face localization map is sharper and ï¬ner at the edges
of the face region. As shown in Fig. 9, the binary mask-
based supervised method ignores the gradient information of
face edges. Compared with the depth map, the prediction
map based on the face location map has higher contrast
in distinguishing faces and backgrounds. When the input is
spooï¬ng face, the predictions obtained by the face location
map-based and depth map-based methods are similar and still
bias the face region, which demonstrates that the network does
not need to simulate depth information in practice. As shown
in Fig. 9, the results of the binary mask have less distinct
semantic features. The data distribution on the heat map is
more uniform and scattered with the binary mask supervision.
Compared with the above three supervision methods, the loss
of generalization of the features extracted by the network
without the drive of depth and location information.

D. Comparison on Public Dataset

We compare the proposed EulerNet with recent deep learn-
ing methods on OULU-NPU [30]. Protocol 1-3 respectively
evaluate the generalization of the algorithm under previously
unseen environmental conditions, attacks created with different
printers or displays, and input camera variation. Protocol 4
considers all the above three factors. Generally, protocols 3
and 4 are more difï¬cult than other protocols.

The Comparison results on OULU-NPU are shown in Table
II. The best performance obtained by the proposed method in
protocols 3 and 4 demonstrates that our method can maintain
accuracy under complex conditions. Table II proves that the
structures we have designed are capable of extracting abnormal
clues, which has a stronger generalization in facing changes
of the shooting device. The complexity of protocols 3 and 4
is similar to the realistic scenario where electronic products
are changing rapidly, there are demonstrates the practicality
of our proposed method.

V. CONCLUSION

In this paper, we propose a novel face anti-spooï¬ng method,
which effectively recognize the subtle differences between real
face and spooï¬ng in the video. The novel network architecture,
namely EulerNet, is designed to fuse temporal information
and extract abnormal clues. We propose a lightweight label-
ing method based on face landmarks to reduce the labeling
cost and improve the labeling speed. Extensive experimental
results on our datasets and public OULU-NPU validate the
effectiveness of our method.

Fig. 9: Model output and heatmap visualization under different
supervision methods. The same column indicates the results
of the same face, marked with ID 1-3.

TABLE II: The results of intra testing on four protocols of
OULU-NPU compared with deep learning methods

Prot.

1

2

3

4

Method
Disentangled [36]
FAS-SGTD [14]
DeepPixBiS [22]
CDCN [37]
EulerNet(Ours)
DeepPixBiS [22]
Disentangled [36]
FAS-SGTD [14]
CDCN [37]
EulerNet(Ours)
DeepPixBiS [22]
FAS-SGTD [14]
CDCN [37]
Disentangled [36]
EulerNet(Ours)
DeepPixBiS [22]
CDCN [37]
FAS-SGTD [14]
Disentangled [36]
EulerNet(Ours)

APCER(%)
1.7
2.0
0.8
0.4
0.4
11.4
1.1
2.5
1.5
2.1
11.7Â±19.6
3.2Â±2.0
2.4Â±1.3
2.8Â±2.2
2.6Â±1.3
36.7Â±29.7
4.6Â±4.6
6.7Â±7.5
5.4Â±2.9
1.8Â±1.9

BPCER(%)
0.8
0.0
0.0
1.7
3.3
0.6
3.6
1.3
1.4
1.4
10.6Â±14.1
2.2Â±1.4
2.2Â±2.0
1.7Â±2.6
1.6Â±0.8
13.3Â±14.1
9.2Â±8.0
3.3Â±4.1
3.3Â±6.0
4.3Â±2.4

ACER(%)
1.3
1.0
0.4
1.0
1.9
6.0
2.4
1.9
1.5
1.7
11.1Â±9.4
2.7Â±0.6
2.3Â±1.4
2.2Â±2.2
2.1Â±0.5
25.0Â±12.7
6.9Â±2.9
5.0Â±2.2
4.4Â±3.0
3.1Â±0.9

the effect of FCAM and Residual Pyramid on the features
extracted by the network. Max Pooling 1-3 indicates the
results using different downsampling multipliers. Fusion result
is the multi-resolution fusion results. The ï¬nal prediction are
shown in Predicted Map. Scores are presented in the last
column of Fig. 8. The higher the score on the far right,
the higher the likelihood of being judged as live faces. The
comparison between Max Pooling 1 and Max Pooling 2
shows that the visualization features are clearly distinct under
different downsampling scales. The model with FCAM pays
more attention to the parts where the action occurs, so there
are higher activation values at pixels of the contour, eyes,
and nose. Finally, the face score obtained by our method is

ID 1ID 2ID 3Live  SampleFaceLocationMapDepthMapBinaryMaskSpoofing  SampleFaceLocationMapDepthMapBinaryMask[21] Z. Wang, C. Zhao, Y. Qin, Q. Zhou, G. Qi, J. Wan, and Z. Lei,
â€œExploiting temporal and depth information for multi-frame face anti-
spooï¬ng,â€ arXiv preprint arXiv:1811.05118, 2018.

[22] A. George and S. Marcel, â€œDeep pixel-wise binary supervision for
face presentation attack detection,â€ in 2019 International Conference
on Biometrics (ICB).

IEEE, 2019, pp. 1â€“8.
[23] M. S. Hossain, L. Rupty, K. Roy, M. Hasan, S. Sengupta, and
N. Mohammed, â€œA-deeppixbis: Attentional angular margin for face anti-
spooï¬ng,â€ in 2020 Digital Image Computing: Techniques and Applica-
tions (DICTA).

IEEE, 2020, pp. 1â€“8.

[24] Y. Ma, L. Wu, Z. Li et al., â€œA novel face presentation attack detection
scheme based on multi-regional convolutional neural networks,â€ Pattern
Recognition Letters, vol. 131, pp. 261â€“267, 2020.

[25] W. Sun, Y. Song, H. Zhao, and Z. Jin, â€œA face spooï¬ng detection method
based on domain adaptation and lossless size adaptation,â€ IEEE access,
vol. 8, pp. 66 553â€“66 563, 2020.

[26] X. Tan, Y. Li, J. Liu, and L. Jiang, â€œFace liveness detection from a single
image with sparse low rank bilinear discriminative model,â€ in European
Conference on Computer Vision. Springer, 2010, pp. 504â€“517.
[27] Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Z. Li, â€œA face anti-
spooï¬ng database with diverse attacks,â€ in 2012 5th IAPR international
conference on Biometrics (ICB).

IEEE, 2012, pp. 26â€“31.

[28] I. Chingovska, A. Anjos, and S. Marcel, â€œOn the effectiveness of local
binary patterns in face anti-spooï¬ng,â€ in 2012 BIOSIG-proceedings
of
interest group
(BIOSIG).

the international conference of biometrics special

IEEE, 2012, pp. 1â€“7.

[29] K. Patel, H. Han, and A. K. Jain, â€œSecure face unlock: Spoof detection on
smartphones,â€ IEEE transactions on information forensics and security,
vol. 11, no. 10, pp. 2268â€“2283, 2016.

[30] Z. Boulkenafet, J. Komulainen, L. Li, X. Feng, and A. Hadid, â€œOulu-npu:
A mobile face presentation attack database with real-world variations,â€
in 2017 12th IEEE international conference on automatic face & gesture
recognition (FG 2017).

IEEE, 2017, pp. 612â€“618.

[31] H.-Y. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman,
â€œEulerian video magniï¬cation for revealing subtle changes in the world,â€
ACM transactions on graphics (TOG), vol. 31, no. 4, pp. 1â€“8, 2012.

[32] B. Kuznetsov, J. D. Parker, and F. Esqueda, â€œDifferentiable iir ï¬lters for
machine learning applications,â€ in Proc. Int. Conf. Digital Audio Effects
(eDAFx-20), 2020, pp. 297â€“303.

[33] L. Li and X. Feng, â€œFace anti-spooï¬ng via deep local binary pattern,â€ in
Deep Learning in Object Detection and Recognition. Springer, 2019,
pp. 91â€“111.

[34] P. P. Chan, W. Liu, D. Chen, D. S. Yeung, F. Zhang, X. Wang, and C.-C.
Hsu, â€œFace liveness detection using a ï¬‚ash against 2d spooï¬ng attack,â€
IEEE Transactions on Information Forensics and Security, vol. 13, no. 2,
pp. 521â€“534, 2017.

[35] L. Wright, â€œRanger

- a synergistic optimizer.â€ https://github.com/

lessw2020/Ranger-Deep-Learning-Optimizer, 2019.

[36] K.-Y. Zhang, T. Yao, J. Zhang, Y. Tai, S. Ding, J. Li, F. Huang, H. Song,
and L. Ma, â€œFace anti-spooï¬ng via disentangled representation learning,â€
in European Conference on Computer Vision. Springer, 2020, pp. 641â€“
657.

[37] Z. Yu, C. Zhao, Z. Wang, Y. Qin, Z. Su, X. Li, F. Zhou, and G. Zhao,
â€œSearching central difference convolutional networks for face anti-
spooï¬ng,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 5295â€“5305.

[38] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, â€œGrad-cam: Visual explanations from deep networks via
gradient-based localization,â€ in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618â€“626.

REFERENCES

[1] Y. Liu, A. Jourabloo, and X. Liu, â€œLearning deep models for face anti-
spooï¬ng: Binary or auxiliary supervision,â€ in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2018, pp. 389â€“
398.

[2] X. Xie, Y. Gao, W.-S. Zheng, J. Lai, and J. Zhu, â€œOne-snapshot face anti-
spooï¬ng using a light ï¬eld camera,â€ in Chinese Conference on Biometric
Recognition. Springer, 2017, pp. 108â€“117.

[3] D. Yi, Z. Lei, Z. Zhang, and S. Z. Li, â€œFace anti-spooï¬ng: Multi-spectral
Springer, 2014,

approach,â€ in Handbook of Biometric Anti-Spooï¬ng.
pp. 83â€“102.

[4] Z. Boulkenafet, J. Komulainen, and A. Hadid, â€œFace anti-spooï¬ng based
on color texture analysis,â€ in 2015 IEEE international conference on
image processing (ICIP).

IEEE, 2015, pp. 2636â€“2640.

[5] T. A. Siddiqui, S. Bharadwaj, T. I. Dhamecha, A. Agarwal, M. Vatsa,
R. Singh, and N. Ratha, â€œFace anti-spooï¬ng with multifeature videolet
aggregation,â€ in 2016 23rd International Conference on Pattern Recog-
nition (ICPR).

IEEE, 2016, pp. 1035â€“1040.

[6] X. Li, J. Komulainen, G. Zhao, P.-C. Yuen, and M. PietikÂ¨ainen, â€œGener-
alized face anti-spooï¬ng by detecting pulse from face videos,â€ in 2016
23rd International Conference on Pattern Recognition (ICPR).
IEEE,
2016, pp. 4244â€“4249.

[7] J. Stehouwer, A. Jourabloo, Y. Liu, and X. Liu, â€œNoise modeling, synthe-
sis and classiï¬cation for generic object anti-spooï¬ng,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 7294â€“7303.

[8] O. Lucena, A. Junior, V. Moia, R. Souza, E. Valle, and R. Lotufo,
â€œTransfer learning using convolutional neural networks for face anti-
spooï¬ng,â€ in International conference image analysis and recognition.
Springer, 2017, pp. 27â€“34.

[9] Y. A. U. Rehman, L. M. Po, and M. Liu, â€œDeep learning for face anti-
spooï¬ng: An end-to-end approach,â€ in 2017 Signal Processing: Algo-
rithms, Architectures, Arrangements, and Applications (SPA).
IEEE,
2017, pp. 195â€“200.

[10] Z. Yu, Y. Qin, X. Xu, C. Zhao, Z. Wang, Z. Lei, and G. Zhao, â€œAuto-fas:
Searching lightweight networks for face anti-spooï¬ng,â€ in ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2020, pp. 996â€“1000.

[11] Z. Yu, J. Wan, Y. Qin, X. Li, S. Z. Li, and G. Zhao, â€œNas-fas: Static-
dynamic central difference network search for face anti-spooï¬ng,â€ IEEE
transactions on pattern analysis and machine intelligence, vol. 43, no. 9,
pp. 3005â€“3023, 2020.

[12] Y. Qin, W. Zhang, J. Shi, Z. Wang, and L. Yan, â€œOne-class adaptation
face anti-spooï¬ng with loss function search,â€ Neurocomputing, vol. 417,
pp. 384â€“395, 2020.

[13] Y. Qin, Z. Yu, L. Yan, Z. Wang, C. Zhao, and Z. Lei, â€œMeta-teacher for
face anti-spooï¬ng,â€ IEEE transactions on pattern analysis and machine
intelligence, 2021.

[14] Z. Wang, Z. Yu, C. Zhao, X. Zhu, Y. Qin, Q. Zhou, F. Zhou, and
Z. Lei, â€œDeep spatial gradient and temporal depth learning for face anti-
spooï¬ng,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2020, pp. 5042â€“5051.

[15] M. Asim, Z. Ming, and M. Y. Javed, â€œCnn based spatio-temporal feature
extraction for face anti-spooï¬ng,â€ in 2017 2nd International Conference
on Image, Vision and Computing (ICIVC).
IEEE, 2017, pp. 234â€“238.
[16] Z. Yu, X. Li, P. Wang, and G. Zhao, â€œTransrppg: Remote photoplethys-
mography transformer for 3d mask face presentation attack detection,â€
IEEE Signal Processing Letters, vol. 28, pp. 1290â€“1294, 2021.
[17] X. Yang, W. Luo, L. Bao, Y. Gao, D. Gong, S. Zheng, Z. Li, and W. Liu,
â€œFace anti-spooï¬ng: Model matters, so does data,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 3507â€“3516.

[18] Y. Liu, Y. Tai, J. Li, S. Ding, C. Wang, F. Huang, D. Li, W. Qi, and
R. Ji, â€œAurora guard: Real-time face anti-spooï¬ng via light reï¬‚ection,â€
arXiv preprint arXiv:1902.10311, 2019.

[19] Z. Yu, X. Li, X. Niu, J. Shi, and G. Zhao, â€œFace anti-spooï¬ng with
human material perception,â€ in European Conference on Computer
Vision. Springer, 2020, pp. 557â€“575.

[20] T. Kim, Y. Kim, I. Kim, and D. Kim, â€œBasn: Enriching feature repre-
sentation using bipartite auxiliary supervisions for face anti-spooï¬ng,â€
in Proceedings of the IEEE/CVF International Conference on Computer
Vision Workshops, 2019, pp. 0â€“0.

