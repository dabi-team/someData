2
2
0
2

r
p
A
8

]
E
S
.
s
c
[

2
v
3
9
2
3
0
.
4
0
2
2
:
v
i
X
r
a

Enhancing Semantic Code Search with Multimodal Contrastive
Learning and Soft Data Augmentation

Ensheng Shia,â€  Wenchao Gub,â€  Yanlin Wangc,Â§ Lun Duc
Hongyu Zhangd Shi Hanc Dongmei Zhangc Hongbin Suna,Â§

aXiâ€™an Jiaotong University

bThe Chinese University of Hong Kong

dThe University of Newcastle
cMicrosoft Research
s1530129650@stu.xjtu.edu.cn, wcgu@cse.cuhk.edu.hk
{yanlwang, lun.du, shihan, dongmeiz}@microsoft.com
hongyu.zhang@newcastle.edu.au, hsun@mail.xjtu.edu.cn

ABSTRACT
Code search aims to retrieve the most semantically relevant code
snippet for a given natural language query. Recently, large-scale
code pre-trained models such as CodeBERT and GraphCodeBERT
learn generic representations of source code and have achieved
substantial improvement on code search task. However, the high-
quality sequence-level representations of code snippets have not
been sufficiently explored. In this paper, we propose a new approach
with multimodal contrastive learning and soft data augmentation
for code search. Multimodal contrastive learning is used to pull
together the representations of code-query pairs and push apart the
unpaired code snippets and queries. Moreover, data augmentation
is critical in contrastive learning for learning high-quality repre-
sentations. However, only semantic-preserving augmentations for
source code are considered in existing work. In this work, we pro-
pose to do soft data augmentation by dynamically masking and
replacing some tokens in code sequences to generate code snippets
that are similar but not necessarily semantic-preserving as positive
samples for paired queries. We conduct extensive experiments to
evaluate the effectiveness of our approach on a large-scale dataset
with six programming languages. The experimental results show
that our approach significantly outperforms the state-of-the-art
methods. We also adapt our techniques to several pre-trained mod-
els such as RoBERTa and CodeBERT, and significantly boost their
performance on the code search task.

1 INTRODUCTION

Code search plays an important role in software development
and maintenance [55, 64]. To implement a certain functionality,
developers often search and reuse previously-written relevant code

Â§Yanlin Wang and Hongbin Sun are the corresponding authors.
â€ Work is done during internship at Microsoft Research Asia

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Â© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

1

from open source repositories such as Github or from a large local
codebase [53, 60, 79]. With the advent of a large corpus of open
source code, finding the semantic relevant code snippets among
massive code snippets based on natural language queries has be-
come one of the key challenges in this field [1].

Early studies [46, 51â€“53] on code search mainly focus on the
lexical information of the code snippets and leverage information
retrieval methods to find the relevant code snippets for given a
search query. Deep end-to-end code search approaches [6, 14, 23,
26, 29, 42, 44, 45, 63, 70, 77, 82] leverage the neural networks to
embed code snippets and queries into a shared high-dimensional
vector space and measure their semantic similarity through vector
distances. Recently, pre-trained source code models [17, 27], which
are pre-trained on a large multi-programming-language dataset,
improve the understanding of code semantics and achieve better
code search performance. For example, GraphCodeBERT [27] im-
proves the performance of end-to-end supervised approaches [33]
by about 70% on MRR and achieves an average MRR of 0.713 on the
CodeSearchNet [33] dataset. Nevertheless, code search performance
still needs to be further improved.

Contrastive learning approaches [28], which pull together the
representations of relevant (positive) samples and push apart the
irrelevant (negative) samples, are well suitable for solving the code
search problem, which expects the paired code snippet and query
to have close representations and unpaired code snippet and query
to have different representations. Recently, some studies [5, 37]
apply contrastive learning approaches for unsupervised code repre-
sentation learning and achieve good performance on downstream
tasks. Their core idea is to generate the semantic-preserving code
transformations as a form of program augmentation and pre-train a
model to identify semantically similar and dissimilar code snippets
through the approaches related to contrastive learning. At the same
time, some studies [78, 80] also apply contrastive learning to learn
the cross-modal representations for videos/images and texts.

For contrastive learning, data augmentation is critical for learn-
ing high-quality representations [7, 8, 38, 67]. Previous stud-
ies [5, 37] on unsupervised code representation learning mainly use
the semantic-preserving program transformations (Â§ 5.2.2) to aug-
ment source code. However, some program transformations require
certain constraints and such constraints may not be always satisfied
in one code snippet. For example, the Statements Permutation [5]
requires that the swapped two statements have no data dependency
on each other in a basic block and Loop Exchange [5] requires that

 
 
 
 
 
 
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Shi, et al.

there is at least one loop statement in code snippets. Some simple
semantic-preserving program transformations such as Variable
Renaming have a limited benefit for code search task [5, 37].

In this work, we adapt the multimodal contrastive learning frame-
work [78] and soft data augmentation (SoDa) for code search. Our
proposed model CoCoSoDa (stand for semantic Code search with
multimodal Contrastive learning and Soft Data augmentation) aims
to minimize the distance between the representations of code-
query pair and maximize the distance between representations
of the query (code snippet) and other many unpaired code snip-
pets (queries). To learn the better sequence-level representation
of the code snippet and query instead of focusing on token-level
semantic modeling1, we propose SoDa adopting dynamic masking
technique [48] to generate similar code snippets or queries (shown
in the left of Figure 1) as a data augmentation approach. SoDa is
easy to build and scale to any programming language. The over-
all framework of CoCoSoDa are shown in Figure 1. Specifically,
first, we sample randomly ğ‘Ÿ (ğ‘Ÿ is stable in 5% - 25%) of the tokens
from the code snippet or query. These tokens are pre-processed
as follows â€” 80% are replaced with a [MASK] token, 10% with a
random token from the vocabulary built from the training dataset
in advance, and 10% left unchanged. Second, we use a large-scale
code pre-trained model GraphCodeBERT to initialize the code/-
query encoder and momentum code/query encoder, and feed the
original and augmented samples (codes or queries) to the encoders
and momentum encoders, respectively, to obtain the representation
of samples. Third, a multimodal contrastive learning loss is used
to pull close semantically similar representations and push apart
dissimilar representations.

We evaluate the effectiveness of CoCoSoDa on a large-scale
dataset CodeSearchNet [33] with six programming language (Ruby,
JavaScript, Go, Python, Java, PHP) and compare CoCoSoDa with
eight state-of-the-art approaches. We also apply CoCoSoDa to three
large-scale pre-trained models, including natural language pre-
trained model RoBERTa [48], code pre-trained models RoBERTa
(code) [17] and CodeBERT [17]. In addition, we conduct the ablation
study to study the effectiveness of each component of CoCoSoDa.
We also assign different hyperparameters to check their impact on
code search. Experimental results show that: (1) CoCoSoDa signifi-
cantly outperforms existing SOTA approaches on the code search
task (Â§ 5.1). (2) SoDa is a simple yet effective data augmentation
approach to learn the sequence-level representation compared with
the semantic-preserving transformations (Â§ 5.2.2). (3) CoCoSoDa
can be easily adapted to other pre-trained models and significantly
boost their performance (Â§ 5.3). (4) CoCoSoDa and GraphCodeBERT
can benefit from larger queue size and mini-batch size, respectively
(Â§ 5.4). (5) CoCoSoDa performs stably over a range of hyperparam-
eters: learning rate is from 5ğ‘’âˆ’6 to 5ğ‘’âˆ’5, momentum coefficient ğ‘š
is between 0.930 and 0.999, masked ratio ğ‘Ÿ is from 5% to 25%, and
temperature hyperparameter ğœ varies from 0.01 to 0.1 (Â§ 5.4).
We summarize the contributions of this paper as follows:
â€¢ We propose a new approach incorporating multimodal con-
trastive learning for code search task. It can pull together the

1Sequence-level representation and token-level representation are relative. The former
mainly represents the overall semantic of the code//query, while the latter usually
represents the semantic of each token according the surrounding context.

2

representations of matched code-query pairs and push apart
the representations of unmatched code-query pairs.

â€¢ We propose a simple yet effective soft data augmentation
method SoDa that utilizes sequence dynamic masking for data
augmentation. Unlike existing approaches that use hard data
augmentation with semantic-preserving code transformations,
we demonstrate that soft data augmentation can boost code
search performance by learning richer sequence-level repre-
sentations of codes and queries.

â€¢ We conduct extensive experiments to evaluate the effectiveness
of our approach on a large-scale multi-programming-language
dataset. The results show that our approach significantly out-
performs baselines.

2 RELATED WORK
2.1 Code Search
Learning the representation of code is an emerging topic and has
been found to be useful in many software engineering tasks, such
as code summarization [35, 40, 61, 62, 81], code search [15, 25, 26,
29, 82], code completion [4, 57, 59, 65, 73], commit message gen-
eration [10, 49, 66, 68, 76]. Among them, code search plays an
important role in software development and maintenance [55, 64].
Code search aims to find the most semantically matching code snip-
pet for the given query. Traditional approaches [46, 51â€“53] based
on retrieval information mainly focus on the lexical information of
the source code and apply keywords matching methods to search
the relevant code snippets for the given query. In recent years, deep
learning-based approaches leverage the neural network to learn the
semantic representations of the source code and natural language
to improve the understanding of code snippets and queries. Gu et
al. [26] is the first to use the deep neural network to embed the code
and query into a shared vector space and measure the similarity of
them using vector distance. Subsequently, various types of model
structures are applied to code search, including sequential mod-
els [23, 29, 63, 70, 77], convolutional neural network [42, 44, 82],
tree neural network [70], graph models [45, 70], and transform-
ers [14, 82].

Recently, large-scale code pre-trained models [17, 27], which are
pre-trained on a massive source code dataset, improve the under-
standing of code semantics and achieve significant improvements
in code search task. RoBERTa (code) [17] , CodeBERT [17] and
GraphCodeBERT [27] are all based on RoBERTa architecture but
pre-trained with different pre-trained tasks. RoBERTa (code) is pre-
trained with masked language modeling (MLM), which is to predict
the original tokens which are masked. CodeBERT is pre-trained
with MLM and replaced token detection (RTD), which uses a dis-
criminator to identify the replaced token. GraphCodeBERT takes
source code paired with summarization and the corresponding data
flow as the input and is pre-trained with MLM, data flow edge
prediction, and node alignment tasks. Our approach can be easily
applied to these pre-trained models and boost their performance.

2.2 Contrastive Learning for Code
Representation Learning

Contrastive learning approaches [28], which pull close the similar
representations and push apart different representations, have been

Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Figure 1: The framework of CoCoSoDa.

successfully used in self-supervised representation learning on
images [7, 30] and natural language texts [16, 19, 21]. To generate
individual augmentations, images usually use spatial [12, 20] and
appearance transformation [31, 34], and natural language texts
mostly use back-translation approach [16] and spans technique [21].
Then, a large-scale model is pre-trained to identify whether the
augmented samples are from the same original sample.

Inspired by these, some studies [5, 13, 37, 71] try to use con-
trastive learning approaches on unsupervised code representation
learning. Different from the above self-supervised code represen-
tations such as CodeBERT [17] and GraphCodeBERT [27], which
are mainly pre-trained with token-level training objects, these ap-
proaches based on contrastive learning mainly focus on learning
the sequence-level code representation. For example, Jain et al. [37]
and Bui et al. [5] mainly use semantic-preserving program transfor-
mations to generate the functionally equivalent code snippets and
pre-train the model to recognize semantically equivalent and non-
equivalent code snippets through contrastive learning techniques.
Ding et al. [13] use data augmentation to generate functionally
equivalent code as a positive sample and inject the bug into code
snippet as a negative sample. The model is trained by contrastive
learning loss and has some improvement in downstream tasks like
vulnerability detection, clone detection, and code summarization.
Unlike the above-mentioned pre-trained technique, our model is
based on multimodal contrastive learning with momentum en-
coders, which allows the model to learn the good representation

based on samples in the current mini-batch and previous mini-
batches. Furthermore, previous data augmentation requires pre-
serving the semantics of source code, whereas we use a simple yet
effective dynamic masking technique that allows more flexible soft
data augmentation.

3 PROPOSED APPROACH
In this section, we illustrate our model CoCoSoDa for code search.
The overall architecture is shown in Figure 1. In general, our ap-
proach adopts a pre-trained model as the base code/query encoder
and finetunes it based on multimodal contrastive learning and soft
data augmentation. To avoid using a large mini-batch size that
requires expensive computation resources and could bring general-
ization issues [22, 43], we adopt the momentum contrastive learning
algorithm [30] as our main framework. CoCoSoDa is comprised of
the following four components:

â€¢ Pre-trained code/query encoder captures the semantic in-
formation of a code snippet or natural language query and
maps it into a high-dimensional embedding. We use the Graph-
CodeBERT, a bimodal pre-trained model for both programming
and natural languages, as the code/query encoder.

â€¢ Momentum code/query encoder encodes the samples (code
snippets or queries) of current and previous mini-batches to
enrich the negative samples. To keep the consistency of rep-
resentation [30], the momentum encoder is updated by linear
interpolation of the encoder and momentum encoder.

3

 Original Source Code c:def save_file(dataframe, filename):    df = dataframe    df.to_csv(filename ,sep=',', encoding='utf-8',     index=False) Masked Source Code c*:def save_file(dataframe, filename):    df = dataframe    df.to_csv(<MASK> ,sep=',', encoding=<MASK>,     index=False)Dynamic Maskingğœ™qğ‘’ğœ™ğ‘šqğ‘’ğœƒğ‘šğ‘ğ‘’ğœƒğ‘ğ‘’Mini-batchQueue(q1, c1), (q2, c2),â€¦,(q1000, c1000),â€¦q1, q2q1 ,  q2q1,* q2*Query EncoderMomentumQuery Encoderq3*,â€¦, q4096*QueueVq1*Vq2*Mini-batchc1, c2c1, c2c1*, c2*Code EncoderMomentumCode Encoderc3*,â€¦,c4096*MultiModal Contrastive Learning LossMini-batchSoft data augmentationSoft data augmentation...Vq4096*Vq1Vq2Vc1*Vc2*...Vc4096*Vc1Vc2Soft data augmentation Masked Query q*:<MASK>  to Write to CSV Files in PythonOriginal Query q:How to Write to CSV Files in PythonDynamic MaskingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Shi, et al.

â€¢ Soft data augmentation is to dynamically mask or replace
some tokens in a sample (code/query) to generate a similar sam-
ple as a form of data augmentation. This is a simple yet effective
augmentation approach compared to static masking [11, 48] or
other semantic-preserving program transformations [5, 37].
â€¢ Multimodal contrastive learning loss function is defined
for contrastive learning. The optimized objective is to minimize
the distance of the representations of the paired code snippet
and query and maximize the distance between the query (code
snippet) and other unpaired code snippets (queries).

3.1 Pre-trained Encoder and Momentum

Encoder

In this section, we introduce the base model, input samples, output
representation and update mechanism of encoder and momentum
encoder. As the pre-trained models such as GraphCodeBERT [17]
have achieved substantial improvement in code search, we take
GraphCodeBERT as the base code/query encoder. Following previ-
ous study [27], to obtain the whole sequence representations of the
query/code, we insert a special token [CLS] at the beginning of the
input code/query sequence and using the embedding of [CLS] at
the last layer as the whole sequence-level representation. Then we
use a 2-layer Multi-Layer Perception (MLP) projector to map the
sequence-level representation of code/query to shared latent space.
In the MoCo [30] framework, there is a momentum encoder
encoding the samples of the current and previous mini-batches.
Specifically, the momentum encoder maintains a queue by enqueu-
ing the samples in the current mini-batch and dequeuing the sam-
ples in the oldest mini-batch. Here, we also take GraphCodeBERT
as the momentum code/query encoder. The difference of the up-
date mechanism between the encoder and momentum encoder is
that the encoder is updated by the back-propagation algorithm
while the momentum encoder is updated by linear interpolation
of the encoder and the momentum encoder. Thus, compared with
the memory bank approach [75], which fixes and saves the rep-
resentations of all samples of the training dataset in advance, the
momentum encoder can generate consistent representations and
has been demonstrated to be effective [30]. For the end-to-end ap-
proach [23, 27], it has one encoder and takes other samples in the
current mini-batch as negative samples. Thus, it requires a large
mini-batch size in order to expand the number of negative sam-
ples [30]. Therefore, end-to-end approaches require larger memory
computational resources than our approach.

We denote the parameters of the code encoder as ğœƒğ‘ğ‘’ and the mo-
mentum code encoder as ğœƒğ‘šğ‘ğ‘’ , with parameters being the weights
of GraphCodeBERT and 2-layer MLP projector. Therefore, ğœƒğ‘šğ‘ğ‘’ is
updated by:

ğœƒğ‘šğ‘ğ‘’ = ğ‘šğœƒğ‘šğ‘ğ‘’ + (1 âˆ’ ğ‘š)ğœƒğ‘ğ‘’

(1)

where ğ‘š âˆˆ [0, 1) is a momentum coefficient. Similarly, we denote
the parameters of the query encoder and moment query encoder
as ğœ™ğ‘ğ‘’ and ğœ™ğ‘šğ‘ğ‘’ . Then ğœ™ğ‘šğ‘ğ‘’ is updated by:

ğœ™ğ‘šğ‘ğ‘’ = ğ‘šğœ™ğ‘šğ‘ğ‘’ + (1 âˆ’ ğ‘š)ğœ™ğ‘ğ‘’

(2)

3.2 Soft Data Augmentation
In this section, we introduce soft data augmentation (SoDa), which
is a simple data augmentation approach without external con-
straints for source code or queries. We first introduce how to obtain
soft data augmentation and then introduce how to use the aug-
mented data.

As shown in the left of Figure 1, we adopt the dynamic mask-
ing technique [48] to implement SoDa. Specifically, we randomly
select ğ‘Ÿ (default value is 15%) of the tokens from code snippets or
queries. Then the tokens are pre-processed as follows: 80% of them
are replaced with a [MASK] token, 10% with a random token, and
10% left unchanged. Here, dynamic means that in data processing,
the masking and replacement operations are performed at each
iteration rather than only performed once as in static making [48].
We will further study the impact of these two strategies in Â§ 5.2.2.
We denote the SoDa module as ğºğ‘ ğ‘œğ‘‘ğ‘ which performs the dy-
namic masking operation for the given input sequence. Specifically,
we first perform the dynamic masking operation for the code snip-
pets ğ¶ = (ğ‘1, ..., ğ‘ğ‘ğ‘  ) and queries ğ‘„ = (ğ‘1, ..., ğ‘ğ‘ğ‘  ) in a mini-batch
with batch size ğ‘ğ‘  by:
ğ‘âˆ—
ğ‘– = ğºğ‘ ğ‘œğ‘‘ğ‘ (ğ‘ğ‘– ),
ğ‘– and ğ‘âˆ—

(3)
where ğ‘âˆ—
ğ‘– are the augmented samples of the code snippet ğ‘ğ‘–
and query ğ‘ğ‘– , respectively. Then code snippet ğ‘ğ‘– and query ğ‘ğ‘– are
fed into the code/query encoder and augmented samples ğ‘âˆ—
ğ‘˜ and ğ‘âˆ—
ğ‘˜
( ğ‘˜ = 1, ..., ğ¾ and ğ¾ is the queue size) in the current and previous
mini-batches are fed to the momentum code/query encoder by:

ğ‘âˆ—
ğ‘– = ğºğ‘ ğ‘œğ‘‘ğ‘ (ğ‘ğ‘– )

(ğ‘– = 1, ..., ğ‘ğ‘ )

vci = ğ‘“ğœƒğ‘ğ‘’ (ğ‘ğ‘– ),
vqi = ğ‘“ğœƒğ‘ğ‘’ (ğ‘ğ‘– ),
, and vqâˆ—
k

= ğ‘“ğœƒğ‘šğ‘ğ‘’ (ğ‘âˆ—
ğ‘˜ )
= ğ‘“ğœƒğ‘šğ‘ğ‘’ (ğ‘âˆ—
ğ‘˜ )

vcâˆ—
k
vqâˆ—
k

(4)

, vqi

, vcâˆ—

where, vci
of the code snippet ğ‘ğ‘– , query ğ‘ğ‘– , augmented code snippet ğ‘âˆ—
augmented query ğ‘âˆ—

are final sequence-level representations
ğ‘˜ , and

ğ‘˜ , respectively.

k

3.3 Multimodal Contrastive Learning Loss

Function

Multimodal contrastive learning loss function is used to optimize
the parameters of the model. Specifically, given a query ğ‘ğ‘– , we
denote the paired ğ‘ğ‘– or ğ‘âˆ—
ğ‘˜ (ğ‘– = 1, ..., ğ‘ğ‘ 
and ğ‘˜ = 1, ..., ğ¾). For the query ğ‘ğ‘– , with similarity measured by
dot product [27], we define the multimodal contrastive learning
loss [56, 70] as:

ğ‘– and unpaired ğ‘âˆ—

ğ‘˜ as ğ‘âˆ’

ğ‘– as ğ‘+

ğ¿ğ‘ğ‘– = âˆ’ log

exp(vqi Â· v+
ci

exp(vqi Â· v+
ci
)/ğœ) + (cid:205)ğ¾

/ğœ)

ğ‘˜=1 exp(vqi Â· vâˆ’
ck

(5)

/ğœ)

where ğœ is the temperature hyperparameter [30, 75] and is set to
0.07 following previous works [30, 37]. Intuitively, this optimization
objective is to maximize the semantic similarity of the query and
its paired code snippet and minimize the semantic similarity of the
query and its unpaired code snippets. In the same way, for a code
snippet ğ‘ğ‘– , we define multimodal contrastive learning loss as:

ğ¿ğ‘ğ‘– = âˆ’ log

exp(vci Â· v+
qi

exp(vci Â· v+
qi
)/ğœ) + (cid:205)ğ¾

/ğœ)

ğ‘˜=1 exp(vci Â· vâˆ’
qk

(6)

/ğœ)

Both ğœƒğ‘ğ‘’ and ğœ™ğ‘ğ‘’ are learnable parameters and updated by the
back-propagation algorithm.

where ğ‘ğ‘– + is the paired query of input code snippet ğ‘ğ‘– , and ğ‘ğ‘˜
denotes the unpaired query. To this end, the overall multimodal

âˆ’

4

Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 1: Dataset statistics.

Language

Training Validation

Test Candidate Codes

Ruby
JavaScript
Java
Go
PHP
Python

24,927
58,025
164,923
167,288
241,241
251,820

1,400
3,885
5,183
7,325
12,982
13,914

1,261
3,291
10,955
8,122
14,014
14,918

4,360
13,981
40,347
28,120
52,660
43,827

contrastive learning loss function for a mini-batch is:

ğ¿ =

ğ‘ğ‘ 
âˆ‘ï¸

ğ‘–=1

(ğ¿ğ‘ğ‘– + ğ¿ğ‘ğ‘– )

(7)

We apply AdamW [50] algorithm to optimize the loss functions.

4 EXPERIMENTAL DESIGN
4.1 Datasets
We conduct experiments on a large-scale benchmark dataset Code-
SearchNet [33] as used in Guo et al. [27]. It contains six program-
ming languages - Ruby, JavaScript, Go, Python, Java, and PHP. This
dataset is widely used in previous studies [14, 17, 24, 27, 33, 72, 74].
The statistics of the dataset are shown in Table 1. The training data
contains code-query pairs, while valid and test set only have natural
language queries. Following previous studies [26, 27, 32], the model
is to retrieve the correct code snippets from the Candidate Codes
for the given queries when performing the evaluation.

4.2 Baselines
To evaluate the effectiveness of our approach, we compare Co-
CoSoDa with four deep end-to-end approaches: NBow, CNN,
BiRNN, and SelfAtnn proposed by Husains et al. [33] and four pre-
training-based approaches: RoBERTa [48], RoBERTa (code) [17],
CodeBERT [17], and GraphCodeBERT [27].

â€¢ NBow, CNN, BiRNN and SelfAtnn use various encoding
models such as neural bag-of-words [36], 1D convolultional
neural network [39] , bi-directional GRU [9], and multi-head
attention [69] to obtain the representation of code snippets and
queries. Then they use inner product of the representations of
code snippets and queries to measure their similarity.

â€¢ RoBERTa and RoBERTa (code) are built on a multi-layer
bidirectional Transformer [69] encoder and pre-trained with
masked language modeling (MLM) task, which is to predict
the original tokens of the masked positions. The formal is pre-
trained on natural language corpus [48], while the latter is
pre-trained on source code corpus [33].

â€¢ CodeBERT is a bimodal pre-trained model pre-trained with
large-scale code-text pairs on two tasks: MLM and replaced
token detection (RTD), which uses a discriminator to identify
the replaced token.

â€¢ GraphCodeBERT considers the code structure information
for pre-training and achieves the state-of-art results among
code search baselines. The pre-training tasks include MLM,
data flow edge prediction, and node alignment.

5

In our experiments, we train the four deep end-to-end ap-
proaches from scratch, and for the four pre-trained approaches,
we initialize them with the pre-trained models and fine-tune them
according to their original paper descriptions and the avaiable
source code [17, 27, 33].

4.3 Experimental Settings
Following GraphCodeBERT [17], we use Transformer with 12 lay-
ers, 768 dimensional hidden states, and 12 attention heads. The
vocabulary sizes of code and queries are set to 50,265. Max sequence
lengths of code snippets and queries are 128 and 256, respectively.
For optimizer, we use AdamW with the learning rate 2e-5. Follow-
ing previous studies [27, 32], the code encoder and query encoder
share parameters to reduce the number of total parameters. Fol-
lowing MoCo [30], the temperature hyperparameter ğœ is set as 0.07
and momentum coefficient ğ‘š is 0.999. The queue size is set to 4096
for relatively larger datasets (Go, Python, Java, and PHP) and 1024
for relatively smaller datasets (Ruby and JavaScript). The batch size
is 64 and the maximum number of epochs is 20. In addition, we
run the experiments 3 times with random seeds 0,1,2 and display
the mean value in the paper. All experiments are conducted on a
machine with 252 GB main memory and 4 Tesla V100 32GB GPUs.

4.4 Evaluation Metrics
We measure the performance of our approach using four metrics:
MRR (Mean Reciprocal Rank) and top-k recall (R@k, k=1,5,10),
which are widely used in previous studies [14, 17, 23, 26, 27, 29, 33,
42, 44, 45, 63, 70, 70, 70, 77, 82].

MRR is the average of reciprocal ranks of the correct code snip-
pets for given queries ğ‘„. R@k measures the percentage of queries
that the paired code snippets exist in the top-k returned ranked
lists. They are calculated as follows:

ğ‘€ğ‘…ğ‘… =

1
|ğ‘„ |

|ğ‘„ |
âˆ‘ï¸

ğ‘–=1

1
ğ‘…ğ‘ğ‘›ğ‘˜ğ‘–

,

R@k =

1
|ğ‘„ |

|ğ‘„ |
âˆ‘ï¸

ğ‘–=1

ğ›¿ (ğ‘…ğ‘ğ‘›ğ‘˜ğ‘– â‰¤ ğ‘˜)

(8)

where ğ‘…ğ‘ğ‘›ğ‘˜ğ‘– is the rank of the paired code snippet related to the
i-th query. ğ›¿ is an indicator function that returns 1 if ğ‘…ğ‘ğ‘›ğ‘˜ğ‘– â‰¤ ğ‘˜
otherwise returns 0.

5 EXPERIMENTAL RESULTS
5.1 RQ1: What is the Effectiveness of

CoCoSoDa?

We evaluate the effectiveness of our model CoCoSoDa by compar-
ing it to four recent deep end-to-end code search models (NBow,
CNN, BiRNN, and SelfAtt) and four pre-trained models (RoBERTa,
RoBERTa (code), CodeBERT, and GraphCodeBERT) introduced in
Â§ 4.2 on the CodeSearchNet dataset with six programming lan-
guages. The experimental results are shown in Table 2. We display
the result with MRR metric due to space limitation. Results under
other metrics are put on the anonymous replication package. The
following conclusion hold for other metrics

We can see that the four pre-trained models perform better
than the four deep end-to-end models trained from scratch, which
shows the effectiveness of the pre-training technique. Since Graph-
CodeBERT considers the data flow information of source code, it

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Shi, et al.

Table 2: The performance of different approaches on MRR.
JS is short for JavaScript. CoCoSoDa outperforms signifi-
cantly (statistical significance ğ‘ < 0.01).

Table 3: The gain of multimodal contrastive learning.

Model

MRR R@1

R@5 R@10

Ruby

JS

Go Python Java PHP Avg.

Model

NBow
CNN
BiRNN
SelfAtt

0.162 0.157 0.330
0.276 0.224 0.680
0.213 0.193 0.688
0.275 0.287 0.723

0.587 0.523 0.855
RoBERTa
0.631 0.57 0.864
RoBERTa (code)
CodeBERT
0.679 0.621 0.885
GraphCodeBERT 0.703 0.644 0.897

0.161
0.242
0.290
0.398

0.590
0.621
0.672
0.692

0.171 0.152 0.189
0.263 0.260 0.324
0.304 0.338 0.338
0.404 0.426 0.419

0.605 0.561 0.620
0.636 0.581 0.650
0.677 0.626 0.693
0.691 0.649 0.713

CoCoSoDa

0.712 0.666 0.913 0.717 0.726 0.669 0.734

performs the best among four pre-trained models. CoCoSoDa takes
GraphCodeBERT as the base code/query encoder and outperforms
GraphCodeBERT2 because we apply multimodal contrastive learn-
ing and soft data augmentation. Both of them contribute to learning
a good sequence-level representation.

5.2 RQ2: How Much do Different Components

Contribute?

In this section, we study the contributions of the two main compo-
nents: multimodal contrastive learning framework and soft data
augmentation. We take the Java dataset as an example since Java is
the most studied language in the recent 81 studies [47].

5.2.1 Multimodal contrastive learning ablation study. To study the
effectiveness of the multimodal contrastive learning framework
(short for MultiMoCo), we remove the soft data augmentation com-
ponent from Figure 1 and feed the original samples (code snippets
and queries) to the code/query encoder and momentum code/query
encoder. We evaluate the pre-trained GraphCodeBERT, fine-tuned
GraphCodeBERT and MultiMoCo in terms of the four metrics: MRR,
R@1, R@5, and R@10.

The experimental results are shown in Table 3, we can see that
pre-trained GraphCodeBERT without fine-tuning performs poorly
due to the representation degeneration problem [18, 41]. That is, the
high-frequent tokens dominate the sequence representation [41],
resulting in the poor sequence-level semantic representation of the
code snippet and query.

For GraphCodeBERT (fine-tuned) and MultiMoCo, they are all
optimized by minimizing the distance of the representation of
paired code snippet and query. However, MultiMoCo aims to maxi-
mize the distance between representations of the queries and other
unpaired code snippets in the queue (including the current and
previous mini-batches), while GraphCodeBERT only aims to maxi-
mize the distance between representations of the queries and other
unpaired code snippets in the current mini-batch. We discuss the
impact of the queue size and mini-batch size on the model per-
formance in Â§ 5.4. Since the queue is decoupled with mini-batch,

2We conducted t-test between our model and four pre-trained models, and the results
show the improvements are statistically significant with ğ‘ < 0.01.

6

0.006
GraphCodeBERT (Pre-trained) 0.004
0.691
GraphCodeBERT (Fine-tuned)
0.865
0.710 0.616 0.829 0.874
MultiMoCo

0.004
0.814

0.003
0.593

Table 4: The gain of data augmentation technique for Graph-
CodeBERT and MultiMoCo.

Base Model Data Augmentation MRR R@1 R@5 R@10

GraphCode
BERT

6-SP Transformations 0.707 0.597 0.817
0.704 0.608 0.824
SoDa (static)
0.715 0.622 0.833
SoDa (dynamic)

0.869
0.871
0.879

MultiMoCo

6-SP Transformations 0.710 0.615 0.832
0.879
0.875
0.707 0.612 0.828
SoDa (static)
0.726 0.630 0.845 0.887
SoDa (dynamic)

MultiMoCo

Variable Renaming
Unused Statement
Permute Statement
Loop Exchange
Switch to If
Boolean Exchange

0.709 0.614 0.614
0.708 0.611 0.830
0.711 0.616 0.834
0.713 0.618 0.832
0.711 0.615 0.834
0.712 0.617 0.835

0.876
0.878
0.877
0.877
0.879
0.879

MultiMoCo can use the large queue size with more negative sam-
ples, learn the good representations, and perform well in the code
search task.

Soft data augmentation ablation study. In this section, we
5.2.2
study the impact of different data augmentations. We first briefly
describe these augmentations including dynamic masking (denoted
as SoDa (dynamic)), static masking (denoted as SoDa (static)),
and six semantic-preserving program transformations (denoted
as 6-SP Transformations) and then analyze them based on the
experimental results.

SoDa (dynamic) and SoDa (static) introduced in Â§ 3.2 are to obtain
the similar but not semantically equivalent code snippets, while
previous studies [5, 37] try to use the semantic-preserving trans-
formations. The following are the popular 6-SP Transformations:
â€¢ Variable Renaming is to rename a variable with a random
token selected from variable vocabulary built from training set.
â€¢ Permute Statement is to swap two statements, which have

no data dependency on each other in a basic block

â€¢ Unused Statement is to insert an dead code snippet such as

an unused declaration statement.

â€¢ Loop Exchange is to replace for statements with while state-

ments or vice versa.

â€¢ Switch to If is to replace a switch statement with an equiva-

lent if statement.

â€¢ Boolean Exchange is to switch the value of a boolean vari-
able and propagates this change in the method. For example,
changing â€œboolean removed = false; if (!removed) log.warn(...)â€
to â€œboolean removed = true; if (removed) log.warn(...)â€

Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

We use the tnpa tool [58] to obtain the above semantic-preserving
program transformations.

Next, we introduce how to use augmented samples. One way is
to directly treat augmentation code snippets (queries) paired their
original queries (code snippets) as augmented training samples and
feed them to GraphCodeBERT. This way ignores the relationship
between the augmented samples and original samples. Another
way is to integrate different data augmentation approaches into
MultiMoCo. Specifically, as shown in Figure 1, we replace the soft
data augmentation module with different data augmentation ap-
proaches to generate the positive samples ğ‘âˆ—
ğ‘˜ and feed them
to momentum code encoder and momentum query encoder, respec-
tively.

ğ‘˜ and ğ‘âˆ—

All experimental results are shown in Table 4. For GraphCode-
BERT (first row of Table 4), all data augmentation can improve the
performance of the model in terms of all the four metrics: MRR,
R@1, R@5, and R@10 because they can enrich the data diversity.
However, 6-SP Transformations and SoDa (static) have a limited
benefit for the model MultiMoCo (second row of Table 4). For
example, compared with MultiMoCo, MultiMoCo based on 6-SP
Transformations improves the performance on R@5 and R@10,
and MultiMoCo based on SoDa (static) improves the performance
on R@10. However, MultiMoCo based on SoDa (dynamic) boosts
the performance of the MultiMoCo on four metrics. These results
demonstrate the effectiveness of the SoDa (dynamic).

Furthermore, we conduct the ablation study on the effectiveness
of each semantic-preserving program transformation individually
when integrated to MultiMoCo. The six semantic-preserving pro-
gram transformations can be generally divided into three groups
according to the changed information. Variable Renaming and
Unused Statement change the lexical information of the code snip-
pets; Statement Permutation, Loop Exchange and Switch to If
change the structure information; and Boolean Exchange changes
the logic of source code. The experimental results are shown in
the third row of Table 4, we can see that each semantic-preserving
program transformation has a slight impact for MultiMoCo. Specif-
ically, the simple program transformations- Variable Renaming and
Unused Statement only change the lexical information of source
code and degrade the performance of the MultiMoCo. Other trans-
formations (such as Switch to If and Boolean Exchange) are to change
the structure and logic information of source code and improve the
performance of the MultiMoCo. Thus, increasing the diversity of
source code data structures and logic is beneficial for learning a
good representation of source code.

In summary, we can conclude that SoDa (dynamic) is a simple yet
effective data augmentation and is better than SoDa (static) and 6-
SP Transformations for code search. For each semantic-preserving
program transformation, increasing the diversity of code structures
and logic information has more gains on code search than changing
the lexical information of source code.

5.3 RQ3: What is the Performance of Our

Approach on other Pre-trained Models?
We further study the performance of our approach on the other
three pre-trained models introduced in Â§ 4.2, including a natural

7

Figure 2: The performance of CoCoSoDa under different
queue sizes and GraphCodeBERT under mini-batch sizes.

language pre-trained model RoBERTa and two source code pre-
trained models RoBERTa (code) and CodeBERT. Specifically, we use
these pre-trained models as the code/query encoders and momen-
tum code/query encoders in Figure 1. For the input code snippet
and query sequence, we insert a special token [CLS] and take the
embedding of [CLS] as the sequence-level representations of the
code snippet or query. Experimental settings are same as in Â§ 4.3.
The results are shown in Table 5. RoBERTaCoCoSoDa means
using RoBERTa as the code/query encoders and momen-
tum code/query encoders in our framework. Overall, we
can see that RoBERTaCoCoSoDa, RoBERTa (code)CoCoSoDa and
CodeBERTCoCoSoDa significantly outperform RoBERTa, RoBERTa
(code) and CodeBERT respectively on all the six programming
languages in terms of four metrics. These results demonstrate
that our approach can be generalized to other pre-trained mod-
els and boost their performance. Besides, RoBERTa, which is pre-
trained on the natural language corpus and fine-tuned with our
method, achieves comparable performance with GraphCodeBERT
on Go dataset. RoBERTa (code)CoCoSoDa and CodeBERTCoCoSoDa
also slightly outperform GraphCodeBERT on Go, Python and Java
datasets. These indicate that our approach is orthogonal to the
pre-trained technique and can lead to independent improvements.

5.4 RQ4 :What is the Impact of Different

Hyperparameters?

In this section, we study the impact of different hyperparameters:
queue size ğ¾, learning rate, momentum coefficient ğ‘š, masking ratio
ğ‘Ÿ , and temperature hyperparameter ğœ.

We show the performance of CoCoSoDa on different queue sizes
and GraphCodeBERT (fine-tuned) on different mini-batch sizes in
Figure 2. We can see that CoCoSoDa and GraphCodeBERT benefit
from larger queue size and mini-batch size respectively, which is
consistent with previous studies [7, 30]. However, GraphCodeBERT
is limited by the larger mini-batch size because large mini-batch
optimization could bring some new challenges [22, 43], especially
on the generalization issue. Besides, a larger mini-batch requires
more memory resources. Here, the max mini-batch size of 32GB
GPU can afford is 64 for GraphCodeBERT.

We also evaluate our model CoCoSoDa on different learning rate,
momentum coefficient ğ‘š masking ratio ğ‘Ÿ , and temperature hyper-
parameter ğœ. The experimental results are shown in Figure 3. From
the results of varying learning rate (the top left of Figure 3), we can

81632641282565121024204840968192Queue/Mini-batch size0.620.640.660.680.700.720.740.76MRR0.6530.6710.6910.699OOM0.6710.6940.6990.7040.7060.7080.7110.7150.7210.7260.726CoCoSoDaGraphCodeBERTConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Shi, et al.

Table 5: Results on other pre-trained models. RoBERTa-C is short for RoBERTa (code). CoCoSoDa improves the performance
of the pre-trained models significantly (statistical significance ğ‘ < 0.01).

PL

Metric

RoBERTa RoBERTaCoCoSoDa RoBERTa-C RoBERTa-CCoCoSoDa CodeBERT CodeBERTCoCoSoDa

Ruby

JavaScript

Go

Python

Java

PHP

MRR
R@1
R@5
R@10
MRR
R@1
R@5
R@10
MRR
R@1
R@5
R@10
MRR
R@1
R@5
R@10
MRR
R@1
R@5
R@10
MRR
R@1
R@5
R@10

0.587
0.469
0.717
0.785
0.523
0.413
0.652
0.730
0.855
0.800
0.926
0.949
0.590
0.480
0.727
0.793
0.605
0.499
0.737
0.796
0.561
0.450
0.694
0.764

0.619
0.511
0.756
0.819
0.562
0.448
0.696
0.774
0.893
0.850
0.946
0.962
0.646
0.539
0.775
0.835
0.657
0.553
0.785
0.842
0.614
0.507
0.750
0.813

0.631
0.524
0.761
0.821
0.570
0.452
0.716
0.794
0.864
0.811
0.930
0.952
0.621
0.511
0.756
0.819
0.636
0.528
0.770
0.831
0.581
0.467
0.715
0.783

0.695
0.598
0.819
0.869
0.635
0.528
0.770
0.831
0.906
0.863
0.958
0.972
0.697
0.601
0.818
0.869
0.705
0.608
0.825
0.873
0.636
0.528
0.770
0.831

0.679
0.583
0.800
0.853
0.621
0.514
0.752
0.814
0.885
0.837
0.944
0.962
0.672
0.574
0.792
0.850
0.677
0.580
0.796
0.852
0.626
0.521
0.753
0.814

0.688
0.587
0.808
0.862
0.630
0.524
0.761
0.821
0.903
0.859
0.955
0.971
0.695
0.595
0.814
0.867
0.702
0.607
0.821
0.868
0.638
0.532
0.768
0.823

see that performance is generally stable for small learning rate [54]
(from 5ğ‘’âˆ’6 to 5ğ‘’âˆ’5). The learning rates that are larger than 7ğ‘’âˆ’5
have obvious impacts on the model performance. The results of
different momentum coefficient ğ‘š are shown in the top right of Fig-
ure 3. We can see that performance increases when the momentum
coefficient ğ‘š becomes larger. This is because a large momentum
coefficient is beneficial to obtain the consistent representation for
the queue [30]. The momentum coefficient that is smaller than 0.9
has a significant impact on performance. These findings are con-
sistent with the previous work [30]. From the results of varying
masked ratio ğ‘Ÿ (the bottom left of Figure 3), we can see that the
performance is insensitive to the masked ratio ğ‘Ÿ when the masked
ratio ğ‘Ÿ is between 5% and 25%. A larger masked ratio such as 50%
brings considerable performance degradation. It is reasonable be-
cause the larger masked ratio causes the code snippet to lose too
much information. The results of different temperature hyperpa-
rameter ğœ are shown in the bottom right of Figure 3. We can see
that performance is stable when the temperature hyperparameter
ğœ varies from 0.01 to 0.1.

In general, our model is stably over a range of these hyperpa-
rameters (learning rate is from 5ğ‘’âˆ’6 to 5ğ‘’âˆ’5, momentum coefficient
is between 0.930 and 0.999, masked ratio ğ‘Ÿ is from 5% to 25%, and
temperature hyperparameter ğœ varies from 0.01 to 0.1).

Figure 3: The impact of different hyperparameters.

5.5 Case Study
In this section, we show some cases to demonstrate the effectiveness
of our model CoCoSoDa.

Figure 4 shows the results returned by CoCoSoDa and Graph-
CodeBERT for the query â€œTransform a hexadecimal String to a
byte array.â€ from the Java dataset. The query includes two oper-
ation objects: hexadecimal String and byte array and one ac-
tion Transform. To implement the functionality of the query, we
usually take the hexadecimal String as an input parameter and
use â€œtoXXX(...)â€ to perform the â€œTransformâ€ action. Our model Co-
CoSoDa can successfully understand the semantics of the whole

8

5e61e53e55e57e51e4Learning rate0.600.650.700.750.800.850.900.95MRRR@1R@5R@100.8000.9100.9300.9500.9700.9900.999Momentum coefficient m0.600.650.700.750.800.850%5%10%15%20%25%30%50%Masking ratio r0.550.600.650.700.750.800.850.900.010.030.050.070.090.1Temperature hyperparameter 0.650.700.750.800.850.90Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

code snippet and query and return the correct result, while Graph-
CodeBERT cannot. This is because the code representation obtained
by GraphCodeBERT is affected by token-level semantics such as
String and byte, thereby, returning the code snippets with similar
tokens but completely opposite semantics: â€œConverts bytes to hex
string.".

Listing 1: The first result returned by CoCoSoDa

public static byte [] hexStringToByte ( String

hexString ) {
try {

return Hex . decodeHex ( hexString .

toCharArray () );

catch ( DecoderException e) {

throw new UnexpectedException (e);
}

}

}

Listing 2: The first result returned by GraphCodeBERT

public static String toHexString ( final byte []

bytes ) {
char [] chars = new char [ bytes . length * 2];
int i = 0;
for ( byte b : bytes ) {

chars [i ++] = CharUtil . int2hex (( b & 0 xF0 )

>> 4) ;

chars [i ++] = CharUtil . int2hex (b & 0 x0F );

}
return new String ( chars );
}

Figure 4: The first code snippets returned by CoCoSoDa and
GraphCodeBERT for the query â€œTransform a hexadecimal
String to a byte array.â€ on Java language.

In Figure 5, we compare the results returned by CoCoSoDa and
GraphCodeBERT for the query â€œReturns a sorted list of the arcs in
the code not executed.â€ in the Python dataset. CoCoSoDa returns
the correct code snippet, while GraphCodeBERT returns the code
snippet with another semantics â€œReturns a sorted list of the exe-
cuted arcs missing from the code.â€. These two code snippets have
very similar tokens but easily confused semantics. Specifically, the
conditions of the for statement and if statement are totally inverse.
The code snippet of Listing 3 looks through possible and checks
if the item p in the executed, while the code snippet of Listing
4 looks through executed and checks whether the item e in the
possible. Our model can understand the sequence-level semantics
of the code snippet and query instead of the token-level semantic,
thereby, returning the correct result.

The above concrete cases demonstrate the advantages of Co-

CoSoDa in understanding the whole code snippets.

6 DISCUSSION
6.1 Why does Our Model CoCoSoDa Work?
The advantages of CoCoSoDa mainly come from soft data augmen-
tation and multimodal contrastive learning. The soft data augmen-
tation destroys a sequence within a certain range and generates
a similar sample as the â€œpositiveâ€ example. It can help the model

9

Listing 3: The first result returned by CoCoSoDa

def arcs_missing ( self ):

possible = self . arc_possibilities ()
executed = self . arcs_executed ()
missing = [

p for p in possible

if p not in executed

and p [0] not in self . no_branch

return sorted ( missing )

Listing 4: The first result returned by GraphCodeBERT

def arcs_unpredicted ( self ):

possible = self . arc_possibilities ()
executed = self . arcs_executed ()
unpredicted = [

e for e in executed

if e not in possible
and e [0] != e [1]

return sorted ( unpredicted )

]

]

Figure 5: The first code snippets returned by CoCoSoDa and
GraphCodeBERT for the query â€œReturns a sorted list of the
arcs in the code not executed.â€ on Python language.

learn a representation of the code snippet and query from a global
(sequence-level) view rather than simply aggregate the token-level
semantic. Thus, our model tends to return code snippets related to
the sequence-level functionality rather than the token-level similar-
ities. For multimodal contrastive learning, it can pull together the
representations of the code-query pair and push apart the repre-
sentations of queries and many unpaired code snippets. Therefore,
CoCoSoDa can learn the better representations of code and query
and perform well on code search task.

6.2 Limitations & Threats to Validity
Although CoCoSoDa has an overall advantage, our model could
still return inaccurate results, especially for the code snippets that
use the third-library API or self-defined methods. This is because
CoCoSoDa only considers the information of the code snippet itself
rather than other contexts such as other methods in the enclosing
class or project [2, 72]. In our future work, more contextual informa-
tion (such as enclosing class/project and called API/methods) could
be considered in our model to further improve the performance of
CoCoSoDa.

We also have identified the following threats to our approach:
Programming Languages. Due to the heavy effort to evaluate
the model on all programming languages, we conduct our exper-
iment with as many programming languages as possible on the
existing build datasets. Our model on different programming lan-
guages would have different results. In the future, we will evaluate
the effectiveness of our approach with more other programming
languages.

Pre-trained models. To demonstrate that our approach is orthog-
onal to the pre-trained technique and can lead to independent
improvements for code search tasks, we have adopted and evalu-
ated our approach on four pre-trained models including a natural

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Shi, et al.

language pre-trained model RoBERTa and three source code pre-
trained models RoBERTa (code), CodeBERT, and GraphCodeBERT.
It remains to be verified whether or not the proposed approach is
applicable to other pre-trained models such as GPT [3] and T5 [74].
Evaluated benchmark. The paired code snippet is usually used
as the correct result for the given query. In fact, some unpaired
code snippets also answer the given query. In the future, we will
invite some developers to manually score the semantical correlation
between the arbitrary code snippet and query and build a high-
quality code search benchmark.

7 CONCLUSION
In this paper, we present CoCoSoDa, which leverages multimodal
constractive learning and soft data augmentation for code search.
Soft data augmentation helps CoCoSoDa learn the representation
of the code snippets from the sequence-level point of view. Multi-
modal contrastive learning can pull together the representations
of code-query pair and push apart the unpaired code snippets and
queries. Thus, CoCoSoDa can learn sequence-level representations
of code snippets and queries. We conduct extensive experiments
on large-scale benchmark dataset with six programming languages
(Ruby, JavaScript, Go, Python, Java, PHP) and the results confirm
its effectiveness. We also apply CoCoSoDa on other pre-trained
models RoBERTa RoBERTa (code), and CodeBERT and boost their
performance on code search. We also assign different hyperparam-
eters to check their impact on code search and find that CoCoSoDa
performs stably over a range of hyperparameters. In our future
work, more contextual information (such as enclosing class/pro-
ject and called API/methods) could be considered in our model to
further improve the performance of CoCoSoDa.

REFERENCES
[1] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton.
2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput.
Surv. 51, 4 (2018), 81:1â€“81:37.

[2] Aakash Bansal, Sakib Haque, and Collin McMillan. 2021. Project-Level Encoding
for Neural Source Code Summarization of Subroutines. In ICPC. IEEE, 253â€“264.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In
NeurIPS.

[4] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples

to improve code completion systems. In ESEC/FSE. 213â€“222.

[5] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. In SIGIR. ACM, 511â€“521.

[6] JosÃ© Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.
2019. When deep learning met code search. In Proceedings of the ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
August 26-30, 2019, Marlon Dumas, Dietmar Pfahl, Sven Apel, and Alessandra
Russo (Eds.). ACM, 964â€“974. https://doi.org/10.1145/3338906.3340458

[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In ICML
(Proceedings of Machine Learning Research, Vol. 119). PMLR, 1597â€“1607.

[8] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. 2020.

Improved
Baselines with Momentum Contrastive Learning. CoRR abs/2003.04297 (2020).
[9] Kyunghyun Cho, Bart van Merrienboer, Ã‡aglar GÃ¼lÃ§ehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder-Decoder for Statistical Machine Translation.
In EMNLP. ACL, 1724â€“1734.

10

[10] Luis Fernando Cortes-Coy, Mario Linares VÃ¡squez, Jairo Aponte, and Denys
Poshyvanyk. 2014. On Automatically Generating Commit Messages via Summa-
rization of Source Code Changes. In SCAM. IEEE Computer Society, 275â€“284.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL-HLT (1). Association for Computational Linguistics, 4171â€“4186.

[12] Terrance Devries and Graham W. Taylor. 2017.

Improved Regularization of

Convolutional Neural Networks with Cutout. CoRR abs/1708.04552 (2017).
[13] Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi
Ray, and Saikat Chakraborty. 2021. Contrastive Learning for Source Code with
Structural and Functional Properties. CoRR abs/2110.03868 (2021).

[14] Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and Dongmei Zhang.
2021. Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning
Approach for Semantic Code Search. In CIKM. ACM, 2994â€“2998.

[15] Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and Dongmei Zhang.
2021. Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning
Approach for Semantic Code Search. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management. 2994â€“2998.

[16] Hongchao Fang and Pengtao Xie. 2020. CERT: Contrastive Self-supervised

Learning for Language Understanding. CoRR abs/2005.12766 (2020).

[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
November 2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and
Yang Liu (Eds.). Association for Computational Linguistics, 1536â€“1547. https:
//doi.org/10.18653/v1/2020.findings-emnlp.139

[18] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019. Represen-
tation Degeneration Problem in Training Natural Language Generation Models.
In ICLR (Poster). OpenReview.net.

[19] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive
Learning of Sentence Embeddings. In EMNLP (1). Association for Computational
Linguistics, 6894â€“6910.

[20] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. 2018. Unsupervised Rep-
resentation Learning by Predicting Image Rotations. In ICLR (Poster). OpenRe-
view.net.

[21] John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. 2021. DeCLUTR: Deep
Contrastive Learning for Unsupervised Textual Representations. In ACL/IJCNLP
(1). Association for Computational Linguistics, 879â€“895.

[22] Priya Goyal, Piotr DollÃ¡r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate,
Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR abs/1706.02677.
[23] Jian Gu, Zimin Chen, and Martin Monperrus. 2021. Multimodal Representation
for Neural Code Search. In IEEE International Conference on Software Maintenance
and Evolution, ICSME 2021, Luxembourg, September 27 - October 1, 2021. IEEE,
483â€“494. https://doi.org/10.1109/ICSME52107.2021.00049

[24] Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu Zhang, Zenglin
Xu, and Michael R. Lyu. 2021. CRaDLe: Deep code retrieval based on semantic
Dependency Learning. Neural Networks 141 (2021), 385â€“394.

[25] Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang,
and Michael R Lyu. 2022. Accelerating Code Search with Deep Hashing and
Code Classification. In ACL.

[26] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In

ICSE. ACM, 933â€“944.

[27] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/
forum?id=jLoC4ez43PZ

[28] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality Reduction
by Learning an Invariant Mapping. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 2006), 17-22 June 2006, New York,
NY, USA. IEEE Computer Society, 1735â€“1742. https://doi.org/10.1109/CVPR.2006.
100

[29] Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia Hockenmaier. 2020. A

Multi-Perspective Architecture for Semantic Code Search. In ACL.

[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020.
Momentum Contrast for Unsupervised Visual Representation Learning. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020. Computer Vision Foundation / IEEE, 9726â€“9735.
https://doi.org/10.1109/CVPR42600.2020.00975

[31] Andrew G. Howard. 2014. Some Improvements on Deep Convolutional Neural

Network Based Image Classification. In ICLR (Poster).

[32] Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming
Zhou, and Nan Duan. 2021. CoSQA: 20, 000+ Web Queries for Code Search and
Question Answering. In ACL.

Enhancing Semantic Code Search with Multimodal Contrastive Learning and Soft Data Augmentation

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

[33] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http://arxiv.org/abs/
1909.09436

[34] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate Shift. In ICML (JMLR
Workshop and Conference Proceedings, Vol. 37). JMLR.org, 448â€“456.

[35] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In ACL, Vol. 1. The
Association for Computer Linguistics.

[36] Mohit Iyyer, Varun Manjunatha, Jordan L. Boyd-Graber, and Hal DaumÃ© III. 2015.
Deep Unordered Composition Rivals Syntactic Methods for Text Classification.
In ACL (1). The Association for Computer Linguistics, 1681â€“1691.

[37] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Sto-
ica. 2021. Contrastive Code Representation Learning. In EMNLP (1). Association
for Computational Linguistics, 5954â€“5971.

[38] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Baner-
jee, and Fillia Makedon. 2020. A Survey on Contrastive Self-supervised Learning.
CoRR abs/2011.00362 (2020).

[39] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In

EMNLP. ACL, 1746â€“1751.

[40] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In ICSE.
795â€“806.

[41] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020.
On the Sentence Embeddings from Pre-trained Language Models. In EMNLP (1).
Association for Computational Linguistics, 9119â€“9130.

[42] Wei Li, Haozhe Qin, Shuhan Yan, Beijun Shen, and Yuting Chen. 2020. Learning
Code-Query Interaction for Enhancing Code Searches. In ICSME. IEEE, 115â€“126.
[43] Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. 2020. Donâ€™t

Use Large Mini-batches, Use Local SGD. In ICLR. OpenReview.net.

[44] Chunyang Ling, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2020. Adaptive Deep Code
Search. In ICPC â€™20: 28th International Conference on Program Comprehension,
Seoul, Republic of Korea, July 13-15, 2020. ACM, 48â€“59. https://doi.org/10.1145/
3387904.3389278

[45] Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu,
Alex X. Liu, Chunming Wu, and Shouling Ji. 2021. Deep Graph Matching and
Searching for Semantic Code Retrieval. ACM Trans. Knowl. Discov. Data 15, 5
(2021), 88:1â€“88:21. https://doi.org/10.1145/3447571

[46] Erik Linstead, Sushil Krishna Bajracharya, Trung Chi Ngo, Paul Rigor,
Cristina Videira Lopes, and Pierre Baldi. 2009. Sourcerer: mining and search-
ing internet-scale software repositories. Data Min. Knowl. Discov. 18, 2 (2009),
300â€“336.

[47] Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John C. Grundy.
2022. Opportunities and Challenges in Code Search Tools. ACM Comput. Surv.
54, 9 (2022), 196:1â€“196:40.

[48] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).
[49] Zhongxin Liu, Xin Xia, Ahmed E. Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation: how
far are we?. In ASE. ACM, 373â€“384.

[50] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.

In ICLR.

[51] Meili Lu, Xiaobing Sun, Shaowei Wang, David Lo, and Yucong Duan. 2015. Query
expansion via WordNet for effective code search. In SANER. IEEE Computer
Society, 545â€“549.

[52] Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei Zhang, and
Jianjun Zhao. 2015. CodeHow: Effective Code Search Based on API Understanding
and Extended Boolean Model (E). In ASE. IEEE Computer Society, 260â€“270.
[53] Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing Xie, and Chen
Fu. 2011. Portfolio: finding relevant functions and their usage. In ICSE. ACM,
111â€“120.

[54] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On
the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong
Baselines. In ICLR. OpenReview.net.

[55] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query
Expansion Based on Crowd Knowledge for Code Search. IEEE Trans. Serv. Comput.
9, 5 (2016), 771â€“783.

[56] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning

with contrastive predictive coding. arXiv preprint arXiv:1807.03748.

[57] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2015.

Intelligent Code
Completion with Bayesian Networks. ACM Trans. Softw. Eng. Methodol. 25, 1
(2015), 3:1â€“3:31.

[58] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour. 2019. Testing
Neural Program Analyzers. In 34th IEEE/ACM International Conference on Auto-
mated Software Engineering (Late Breaking Results-Track). https://arxiv.org/abs/
1908.10711

11

[59] Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014. Code completion with

statistical language models. In PLDI. 419â€“428.

[60] Steven P. Reiss. 2009. Semantics-based code search. In ICSE. IEEE, 243â€“253.
[61] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dong-
mei Zhang, and Hongbin Sun. 2022. On the Evaluation of Neural Code Summa-
rization. In ICSE.

[62] Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang,
and Hongbin Sun. 2021. CAST: Enhancing Code Summarization with Hierarchical
Splitting and Reconstruction of Abstract Syntax Trees. In EMNLP.

[63] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Im-
proving Code Search with Co-Attentive Representation Learning. In ICPC â€™20:
28th International Conference on Program Comprehension, Seoul, Republic of Korea,
July 13-15, 2020. ACM, 196â€“207. https://doi.org/10.1145/3387904.3389269
[64] Janice Singer, Timothy C. Lethbridge, Norman G. Vinson, and Nicolas Anquetil.
1997. An examination of software engineering work practices. In CASCON. IBM,
21.

[65] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana
Franco, and Miltiadis Allamanis. 2020. Fast and Memory-Efficient Neural Code
Completion. Arxiv Preprint (2020). https://arxiv.org/abs/1611.08307

[66] Wei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Shi Han, Hongyu Zhang, Dongmei
Zhang, and Wenqiang Zhang. 2021. On the Evaluation of Commit Message
Generation Models: An Experimental Study. In 2021 IEEE International Conference
on Software Maintenance and Evolution (ICSME). IEEE, 126â€“136.

[67] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip
Isola. 2020. What Makes for Good Views for Contrastive Learning?. In NeurIPS.
[68] Mario Linares VÃ¡squez, Luis Fernando Cortes-Coy, Jairo Aponte, and Denys
Poshyvanyk. 2015. ChangeScribe: A Tool for Automatically Generating Commit
Messages. In ICSE (2). IEEE Computer Society, 709â€“712.

[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In NIPS. 5998â€“6008.

[70] Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, and
Philip S. Yu. 2019. Multi-modal Attention Network Learning for Semantic Source
Code Retrieval. In ASE. IEEE, 13â€“25.

[71] Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu,
Jin Liu, and Xin Jiang. 2021. SynCoBERT: Syntax-Guided Multi-Modal Contrastive
Pre-Training for Code Representation. arXiv preprint arXiv:2108.04556 (2021).

[72] Yanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, and Dongmei Zhang.
2020. Cocogum: Contextual code summarization with multi-relational gnn
on umls. Technical Report. Microsoft, MSR-TR-2020-16. [Online]. Available:
https://www.microsoft.com/en-us/research/publication/cocogum-contextual-
code-summarization-with-multi-relational-gnn-on-umls.

[73] Yanlin Wang and Hui Li. 2021. Code completion by modeling flattened abstract

syntax trees as graphs. In AAAI.

[74] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5:
Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Under-
standing and Generation. In EMNLP (1). Association for Computational Linguis-
tics, 8696â€“8708.

[75] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised Fea-
ture Learning via Non-Parametric Instance Discrimination. In CVPR. Computer
Vision Foundation / IEEE Computer Society, 3733â€“3742.

[76] Shengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, and Jian Lu.
2019. Commit Message Generation for Source Code Changes. In IJCAI. ijcai.org,
3975â€“3981.

[77] Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang.
2020. Leveraging Code Generation to Improve Code Retrieval and Summarization
via Dual Learning. In WWW â€™20: The Web Conference 2020, Taipei, Taiwan, April
20-24, 2020, Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen
(Eds.). ACM / IW3C2, 2309â€“2319. https://doi.org/10.1145/3366423.3380295
[78] Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire,
Ajinkya Kale, and Baldo Faieta. 2021. Multimodal Contrastive Training for
Visual Representation Learning. In CVPR. Computer Vision Foundation / IEEE,
6995â€“7004.

[79] Feng Zhang, Haoran Niu, Iman Keivanloo, and Ying Zou. 2018. Expanding
Queries for Code Search Using Semantically Related API Class-names.
IEEE
Trans. Software Eng. 44, 11 (2018), 1070â€“1082.

[80] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang.
2021. Cross-Modal Contrastive Learning for Text-to-Image Generation. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,
virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 833â€“842.
https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Cross-Modal_
Contrastive_Learning_for_Text-to-Image_Generation_CVPR_2021_paper.html
[81] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based Neural Source Code Summarization. In ICSE. IEEE / ACM.
[82] Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong, and Lu Zhang. 2020. OCoR:

An Overlapping-Aware Code Retriever. In ASE.

