2
2
0
2

g
u
A
1

]

V

I
.
s
s
e
e
[

1
v
0
2
6
0
0
.
8
0
2
2
:
v
i
X
r
a

SPAALUV: Software Package for Automated Analysis
of Lung Ultrasound Videos

Anito Anto1, Linda Rose Jimson1, Tanya Rose1, Mohammed Jafrin1,
Mahesh Raveendranatha Panicker2
Cochin University of Science and Technology, Cochin, India1

Indian Institute of Technology Palakkad, India2

Abstract

In the recent past with the rapid surge of COVID-19 infections, lung
ultrasound has emerged as a fast and powerful diagnostic tool particularly
for continuous and periodic monitoring of the lung. There have been many
attempts towards severity classiﬁcation, segmentation and detection of key
landmarks in the lung. Leveraging the progress, an automated lung ultra-
sound video analysis package is presented in this work, which can provide
summary of key frames in the video, ﬂagging of the key frames with lung in-
fection and options to automatically detect and segment the lung landmarks.
The integrated package is implemented as an open-source web application
and available in the link https://github.com/anitoanto/alus-package.

Keywords: Lung ultrasound, Video summarisation, Segmentation, Object
tagging, Software Package

1. Motivation and signiﬁcance

Over the last decade, lung ultrasound (LUS) has evolved remarkably as
a potent diagnostic tool for a variety of lung disorders [1]. With portabil-
ity, possibility of repeated and dynamic bedside scanning and non-invasive
radiation-free nature, LUS is expected to be a defacto scanning methodol-
ogy in emergency medicine and ambulatory scenarios [2]. In the recent past
with COVID-19 outbreak, there have been many developments towards intro-
ducing artiﬁcial intelligence enabled ultrasound image/video analysis [3]-[6],
thereby easing the workload of the clinicians involved. In this work, we pro-
pose a software package for automated real-time analysis of lung ultrasound
videos that can potentially be utilized by even by naive clinicians. The pur-
pose of this package is two fold: 1) to summarize the lung ultrasound videos

Preprint submitted to SoftwareX

August 2, 2022

 
 
 
 
 
 
Figure 1: An overview of the proposed package

into a short video of only key frames which helps fast triaging and also en-
ables tele-medicine and 2) to detect and segment the key landmarks in the
summarized key frames for easy interpretation.

For a large number of ultrasound videos with a massive amount of frames
that are diﬃcult to annotate manually, video summarisation relieves the clin-
icians from the time-consuming burden of selecting the frames that are most
relevant to the diagnosis of the patient [11], [12]. The automated package is
delivered as an open-source web application that summarizes the ultrasound
videos extracting only the signiﬁcant frames to generate a compressed ver-
sion with non-redundant abnormal data. The non-redundant frames can be
employed for detection and segmentation of the key landmarks in the sum-
marized key frames, which can be subsequently downloaded by the clinicians
for reporting [13]. A brief overview of the proposed framework is as shown
in Fig. 1 and more details of the algorithms are available at [12],[16].

2. Software description

2.1. Software Architecture

The web application consists of a responsive user interface rendered using
the React framework in the front-end with a high level descriptive overview of
the project.The back end, which encompasses all the non-interface resources,
is implemented using the high-performance FastAPI framework.This is inte-
grated with the machine learning algorithms for video summarisation, classi-
ﬁcation, segmentation, and object tagging [12],[16].An overview of the front
end and back end are shown in Figures 2 and 3 respectively.

The user interface of the web application includes the input controls,
navigational components, and informational containers that enable users to
easily navigate through the web application and create a positive user ex-
perience.The tool comprises of six pages: Home, Upload, Segmentation and
Object Tagging, Report Page, About, and Contact Page.Users can navigate
between the Home, About, and Contact pages using the navigation bar at
the top of the web page.The upload page interface contains an upload button

2

Video  SummarizationSegmentationObject DetectionFigure 2: Detailed overview of the front-end ﬂow diagram

that allows the user to upload multiple videos at once. After completing
the upload procedure, the user will be acknowledged with the number of
videos that have been uploaded to the application. When the user clicks the
Next button, the uploaded videos are sent for processing as an input to the
model for video summarization.Once the videos are summarised with the key
frames, each of the key frames are employed for segmentation and detection
of the key landmarks.On the third page, the segmented video is displayed on
the left-hand side of the screen, with its corresponding key frames extracted
from it on the right.The segmentation and object tagging checkboxes can be
used to toggle between both choices as desired by the user.The segmentation
option will be selected by default, and the subsequent web page will show a
summarised segmented video with a few random keyframes.The pleura and
the abnormalities from the summarised video will be highlighted in a distinct
colour from the rest of the image, thus helping in making clinical diagnosis
easier.By toggling the segmentation checkbox, we can switch between the
normal summarised video and the segmented video with their corresponding
frames in order to highlight the visual diﬀerences between the original ultra-
sound video and the summary.The object tagging checkbox will display the
object-tagged video with their labelled artefacts enclosed in bounding boxes,

3

accompanied by their conﬁdence scores. The videos can be toggled individ-
ually according to the user’s preference, and when the choice gets switched,
the model does not need to be processed again, allowing the output to be
displayed immediately.The random option beneath the frames allows the user
to generate random frames each time it is clicked, allowing this feature to
be used for each video separately.The tool also allows the user to download
the segmented and object-tagged output, allowing them to retrieve valuable
information in order to make crucial clinical judgements.

2.2. Software Functionalities

The lung videos acquired using the obliquely positioned ultrasound probe
[14] along the posterior, anterior, and lateral chest walls are uploaded via a
POST web request from the front-end interface as shown in Fig. 2. The Fast
API on the Uvicorn server handles the responses according to the type of
client-side requests as shown in Fig. 3. In response to the uploaded video,
the back end server generates a unique identiﬁer key, which is returned to
the client-side. This speciﬁc key acts as a conduit for future client-server
communication [15]. The uploaded videos are processed by triggering a web
request by the front end using the unique key identiﬁer to the API endpoint
of the server. This ultrasound scan of diﬀerent areas of a person’s lungs
serves as an input to the machine learning model.

Figure 3: Detailed overview of the back end architecture

4

The uploaded ultrasound scan videos with redundant frames may be quite
large to process, so the pathologically important key frames are identiﬁed
using a video summarisation framework. The video summarisation architec-
ture is modelled in the form of ensemble encoders with an LSTM decoder
as described in [12]. This unsupervised video summarisation model sum-
marises the uploaded videos by selecting diagnostically important frames
crucial for the clinician’s use, thus enhancing its utility in the emergency
department as well as in telemedicine. During the pandemic, segmentation
of key landmarks and automatic classiﬁcation of lung video frames into nor-
mal and abnormal classes may be useful for a variety of clinical applications
and add value in a limited-resource clinical environment. This application
also has an underlying AI-driven object detection model with enabled ac-
tive learning that automatically identiﬁes the important landmarks in the
lung. Object tagging is implemented using the YOLOv5 algorithm, which
treats the summarized video as a multi-class problem [16]. The algorithm
displays bounding boxes with their conﬁdence scores for various ultrasound
artefacts like A-lines, B-lines, consolidations and landmarks such as pleura,
rib and shadow. The segmented and object-tagged videos are saved on the
server-side under a unique key. Random frames are extracted from each sum-
marised video. Consequently, the back end sends a response to the client-side
request by transmitting a set of URLs for each video and keyframes to be
displayed in the front end according to the user preference. This results in
the desired summarized video and 8 random keyframes extracted from each
of the videos.

3. Illustrative Examples

A step by step guide on the usage of the software tool depicting the pro-
cessing of lung ultrasound videos by the automated lung ultrasound package
is illustrated below.

3.1. Upload and Process LUS videos

The software application begins with a basic overview of the mission of
the project. The user must click the “Get Started” button to enter the
upload page shown in Fig. 4. The ﬁrst step is to upload multiple ultrasound
videos obtained using the probe by clicking the “Upload Videos” button.
Once the user has uploaded videos, it is sent for processing by clicking the
“NEXT” button. Although we will only upload one video in this example,
the procedure is the same for multiple simultaneous analysis of ultrasound
scan videos. By refreshing the page, a user can make any necessary changes
to the uploaded videos.

5

Figure 4: Overview of uploading LUS videos

3.2. Generate and Download Results

The uploaded LUS videos are subjected to segmentation and object tag-
ging algorithms by the automated LUS tool. The segmented and object
tagged results are displayed for each ultrasound video separately along with
their extracted keyframes as in the Fig.5. Depending on the user’s prefer-
ence, the abnormal areas and the key landmarks in the lung are identiﬁed by
the SPAALUV package. Clinicians are able to conduct a thorough analysis
of the report by being able to view each key frame closely in a variety of ori-
entations. For each LUS video, the arrow button on the right side generates
an additional set of random keyframes. This software tool also provides the
users with a “Download all” button to download all the summarised videos
and their processed frames as a zip ﬁle for reference purposes in future ex-
aminations. The entire demonstration on the usage of the software tool is
also provided for user awareness in the About page.

6

(a) Segmentation Output

(b) Object-tagging Output

Figure 5: Overview of Result Page

4. Impact

Providing clinicians with a web based easy to use open source software
package for automated analysis of lung ultrasound videos is extremely ben-
eﬁcial in the scenarios of pandemic like COVID-19. The package has the
capability of summarizing the acquired ultrasound videos in terms of the
key frames and also detection and segmentation of the key landmarks in the
lung. The tool will enable faster triaging and also become an enabler for
telemedicine. Even though, the developed framework employs speciﬁc mod-
els trained for lung ultrasound diagnostics, the software framework can be
easily extended to other ultrasound applications.

7

5. Conclusions

In this work a web based open source software package is developed for
automated analysis of lung ultrasound videos. The ultrasound scan videos
are summarised using a robust ultrasound video summarisation pipeline,
extracting relevant frames and aiding clinicians to interpret the scan data
more accurately. The summarised video is further segmented, classiﬁed, and
object-tagged to help analyse the severity of the lungs eﬃciently. The pro-
posed automated lung ultrasound tool has been developed and deployed on
the cloud for piloting in collaborated hospitals.

6. Conﬂict of Interest

We wish to conﬁrm that there are no known conﬂicts of interest associated
with this publication and there has been no signiﬁcant ﬁnancial support for
this work that could have inﬂuenced its outcome.

Acknowledgements

The authors would like to acknowledge the technical contributions from
Mr. Roshan P Mathews, Dr. Abhilash R Hareendranathan and Ms. Jinu
Joseph in contributing to the development of diﬀerent machine learning mod-
els which are reemployed in the proposed work. The authors are also ex-
tremely thankful to the clinical team consisting of Dr. Yale Tung Chen, Dr.
Jacob Jaremko, Dr. Kesavadas C and Dr. Kiran Vishnu Narayan for their
support in providing with relevant data and clinical support as and when
needed. We would like to acknowledge the funding from the Department of
Science and Technology - Science and Engineering Research Board (DST-
SERB (CVD/2020/000221)) for the CRG COVID-19 funding. The authors
are grateful to Compute Canada and the NVIDIA and CDAC for providing
the computing resources for the project.

References

[1] Marini, Thomas J., Deborah J. Rubens, Yu T. Zhao, Justin Weis, Tim-
othy P. O’Connor, William H. Novak, and Katherine A. Kaproth-Joslin.
”Lung ultrasound: the essentials.” Radiology: Cardiothoracic Imaging 3,
no. 2 (2021).

[2] Jackson, Karl, Robert Butler, and Avinash Aujayeb. ”Lung ultrasound
in the COVID-19 pandemic.” Postgraduate medical journal 97, no. 1143
(2021): 34-39.

8

[3] Wang, Xi, Hao Chen, Huiling Xiang, Huangjing Lin, Xi Lin, and Pheng-
Ann Heng. ”Deep virtual adversarial self-training with consistency regu-
larization for semi-supervised medical image classiﬁcation.” Medical im-
age analysis 70 (2021): 102010.

[4] Tsai, Chung-Han, Jeroen van der Burgt, Damjan Vukovic, Nancy Kaur,
Libertario Demi, David Canty, Andrew Wang et al. ”Automatic deep
learning-based pleural eﬀusion classiﬁcation in lung ultrasound images
for respiratory pathology diagnosis.” Physica Medica 83 (2021): 38-45.

[5] Mugasa, Hatwib, Sumeet Dua, Joel EW Koh, Yuki Hagiwara, Oh Shu
Lih, Chakri Madla, Pailin Kongmebhol, Kwan Hoong Ng, and U. Rajen-
dra Acharya. ”An adaptive feature extraction model for classiﬁcation of
thyroid lesions in ultrasound images.” Pattern Recognition Letters 131
(2020): 463-473.

[6] Lee, Francis Chun Yue. ”Lung ultrasound—a primary survey of the
acutely dyspneic patient.” Journal of intensive care 4, no. 1 (2016): 1-13.

[7] Ouahabi, Abdeldjalil, and Abdelmalik Taleb-Ahmed. ”Deep learning for
real-time semantic segmentation: Application in ultrasound imaging.”
Pattern Recognition Letters 144 (2021): 27-34.

[8] Zhou, Yue, Houjin Chen, Yanfeng Li, Qin Liu, Xuanang Xu, Shu Wang,
Pew-Thian Yap, and Dinggang Shen. ”Multi-task learning for segmen-
tation and classiﬁcation of tumors in 3D automated breast ultrasound
images.” Medical Image Analysis 70 (2021): 101918.

[9] Wang, Yu, Xinke Ge, He Ma, Shouliang Qi, Guanjing Zhang, and Yudong
Yao. ”Deep learning in medical ultrasound image analysis: a review.”
IEEE Access 9 (2021): 54310-54324.

[10] Litjens, Geert, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra
Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van
Der Laak, Bram Van Ginneken, and Clara I. S´anchez. ”A survey on deep
learning in medical image analysis.” Medical image analysis 42 (2017):
60-88.

[11] Zhou, Kaiyang, Yu Qiao, and Tao Xiang. ”Deep reinforcement learning
for unsupervised video summarization with diversity-representativeness
reward.” In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 32, no. 1. 2018.

9

[12] Mathews, Roshan P., Mahesh Raveendranatha Panicker, Abhilash R.
Hareendranathan, Yale Tung Chen, Jacob L. Jaremko, Brian Buchanan,
Kiran Vishnu Narayan, and Greeta Mathews. ”Unsupervised multi-latent
space reinforcement learning framework for video summarization in ul-
trasound imaging.” arXiv preprint arXiv:2109.01309 (2021).

[13] Mojoli, Francesco, B´elaid Bouhemad, Silvia Mongodi, and Daniel Licht-
enstein. ”Lung ultrasound for critically ill patients.” American journal of
respiratory and critical care medicine 199, no. 6 (2019): 701-714.

[14] Gargani, Luna, and Giovanni Volpicelli. ”How I do it: lung ultrasound.”

Cardiovascular ultrasound 12, no. 1 (2014): 1-10.

[15] Northwood, Chris. The Full Stack Developer: Your Essential Guide to
the Everyday Skills Expected of a Modern Full Stack Web Developer.
Apress, 2018.

[16] Joseph, Jinu, Mahesh Raveendranatha Panicker, Yale Tung Chen, Ke-
savadas Chandrasekharan, Vimal Chacko Mondy, Anoop Ayyappan, Ji-
neesh Valakkada, and Kiran Vishnu Narayan. ”covEcho Resource con-
strained lung ultrasound image analysis tool for faster triaging and active
learning.” arXiv preprint arXiv:2206.10183 (2022).

10

Current executable software version

link

used

to
for

Current software version
Permanent
code/repository
this version
Legal Software License
Software code languages, tools,
and services used
Installation requirements &
dependencies

- If available Link to developer
documentation/manual
Support email for questions

v1.0
https : //github.com/anitoanto/alus −
package

MIT License
Python 3.9, FastAPI, React, Node LTS

backend:python 3.9.x, packages in require-
ments.txt ﬁle
frontend:node 16.16.0 LTS, npm, packages
in package.json ﬁle, browser
speciﬁcation: good hardware for ML pro-
cessing, preferably intel i5 8gen or above
https : //github.com/anitoanto/alus −
package/blob/main/READM E.md
anitoanto07@gmail.com

11

