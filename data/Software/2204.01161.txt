A Modern Theory for High-dimensional Cox
Regression Models

Xianyang Zhang∗1, Huijuan Zhou2, and Hanxuan Ye1

1Texas A&M University
2Shanghai University of Finance and Economics

Abstract: The proportional hazards model has been extensively used in many ﬁelds such as
biomedicine to estimate and perform statistical signiﬁcance testing on the eﬀects of covariates inﬂu-
encing the survival time of patients. The classical theory of maximum partial-likelihood estimation
(MPLE) is used by most software packages to produce inference, e.g., the coxph function in R and the
PHREG procedure in SAS. In this paper, we investigate the asymptotic behavior of the MPLE in the
regime in which the number of parameters p is of the same order as the number of samples n. The
main results are (i) existence of the MPLE undergoes a sharp ‘phase transition’; (ii) the classical MPLE
theory leads to invalid inference in the high-dimensional regime. We show that the asymptotic behavior
of the MPLE is governed by a new asymptotic theory. These ﬁndings are further corroborated through
numerical studies. The main technical tool in our proofs is the Convex Gaussian Min-max Theorem
(CGMT), which has not been previously used in the analysis of partial likelihood. Our results thus
extend the scope of CGMT and shed new light on the use of CGMT for examining the existence of
MPLE and non-separable objective functions.
Keywords: Convex Gaussian Min-max Theorem, Cox Regression, High-dimensionality, Likelihood-ratio
Test, Wald Test.

1

Introduction

1.1 Background

Since the ﬁrst introduction in 1972 by D. R. Cox, the proportional hazards model has been routinely
used in many applied ﬁelds such as biomedicine in order to investigate the association between the
survival time of patients and predictor variables. In the proportional hazards model, the hazard for an
individual, i, with covariates Xi = (Xi1, . . . , Xip)(cid:62) is speciﬁed as a product

λ(t|Xi) = λ0(t) exp(X(cid:62)

i β∗),

2
2
0
2

r
p
A
3

]
T
S
.
h
t
a
m

[

1
v
1
6
1
1
0
.
4
0
2
2
:
v
i
X
r
a

i β∗) in which the
of an unknown baseline hazard function λ0(·) and a relative risk function exp(X(cid:62)
individual covariate values enter linearly via the regression coeﬃcients β∗ = (β∗
p )(cid:62). When no
prior knowledge is available regarding the structure of the parameters, the proportional hazards model is
often ﬁtted via maximizing the partial log-likelihood function. Classical theory of the maximum partial
likelihood estimation (MPLE) states that when the dimension of variables p is ﬁxed and the sample size
n → +∞,

1 , . . . , β∗

√

n((cid:98)β − β∗) d→ N (0, I−1
β∗ ),

∗Zhang and Zhou acknowledge partial support from NSF DMS-1811747, NSF DMS-2113359 and NIH

1R21 HG011662. Address correspondence to Xianyang Zhang (zhangxiany@stat.tamu.edu).

1

 
 
 
 
 
 
where (cid:98)β denotes the maximum partial likelihood estimator, Iβ∗ is the p × p Fisher information matrix
evaluated at the true value β∗ and d→ stands for convergence in distribution. This result has been
adopted by many software packages to produce signiﬁcance testing and conﬁdence intervals e.g., the
coxph function in R and the PHREG procedure in SAS.

1.2 Motivation

In modern clinical studies, it is often of interest to understand the association between patients’
survival times and a set of high-dimensional covariates such as genomics features and medical images.
The use of proportional hazards model to large data sets thus raises the following questions:

(A) does the classical theory of MPLE provide a good approximation to the ﬁnite sample behaviors

when the number of variables p is a non-negligible proportion of the sample size n?

(B) If the classical theory fails in the high-dimension paradigm, is there a new theory characterizing

the asymptotic properties of MPLE?

1.3 Prior works and our contribution

Previous works in the survival analysis literature have focused on the sparse regime where the number
of relevant predictors is much smaller than the sample size, and employed the penalized partial likelihood
approach to perform simultaneous estimation and variable selection (Tibshirani, 1997; Fan and Li, 2002;
Gui and Li, 2005; Zhang and Lu, 2007; Bradic et al., 2011). Oracle inequalities for the penalized MPLE
have been obtained in Ga¨ıﬀas and Guilloux (2012); Huang et al. (2013); Kong and Nan (2014). A more
recent line of research studies hypothesis testing and conﬁdence interval construction for high-dimensional
Cox regression using the debiasing approach (Fang et al., 2017; Yu et al., 2018; Kong et al., 2021).

In this work, with the aim to answer questions (A) and (B), we study the original MPLE in the
high-dimensional setting where p and n diverge to inﬁnity simultaneously with p/n → δ ∈ (0, 1). To the
best of our knowledge, the asymptotic properties of the original MPLE have not been studied under this
asymptotic regime in the literature. Our main results are summarized as follows.

(i) Under the Gaussian assumption on the covariates, the existence of the MPLE undergoes a sharp
‘phase transition’. The MPLE exists asymptotically (with probability approaching one) only when
δ is below a quantity h(λ0, κ, PC) that is determined by the base line hazard function λ0, the signal
strength κ2 := lim var(X(cid:62)

i β) and the distribution of the censoring time PC.

(ii) The classical MPLE theory leads to invalid inference in the high-dimensional regime where p/n →
δ ∈ (0, 1). We show that the asymptotic behaviors of the MPLE and the Wald test formed by
the sum of squares of the MPLE are governed by a new asymptotic theory. In particular, the
asymptotic bias and variance of the MPLE are precisely characterized by the new theory. The
Wald test is shown to converge to a scaled chi-square distribution.

1.4 Technical tools

There have been several recent works on understanding the asymptotic behaviors of statistical estima-
tors derived from minimizing a convex loss function in the high-dimensional setting. Examples include the
regularized linear regression (Thrampoulidis et al., 2015), M-estimation (El Karoui et al., 2013; Donoho
and Montanari, 2016), penalized M-estimation (Thrampoulidis et al., 2018), logistic regression (Sur and
Cand`es, 2019), reguralized logistic regression (Salehi et al., 2019), high-dimensional classiﬁcation (Liang
and Sur, 2020; Thrampoulidis et al., 2020), adversarial training (Javanmard and Soltanolkotabi, 2020)
among others. All the above results are derived under the assumptions that p/n → δ > 0 and most
of the works assumed that the covariates follow a Gaussian distribution. The technical tools employed
in these studies can be roughly classiﬁed into three categories: (a) the leave-one-out argument; (b) the
approximate message passing (AMP) algorithm and the associated state evolution equations; (c) the
Convex Gaussian Min-max Theorem (CGMT). In El Karoui et al. (2013), the authors developed the
leave-one-out technique to heuristically derive a nonlinear system of two deterministic equations that
characterizes the asymptotic square errors of the M-estimator. A rigorous proof of these results based on

2

the leave-one-out argument was provided in El Karoui (2013). The AMP algorithm was ﬁrst introduced
in Donoho et al. (2009) as an eﬃcient reconstruction scheme in compressed sensing. The authors further
derived a system of state evolution equations to accurately predict the dynamical behavior of several
observables involved in the AMP algorithm. The AMP technique was later on adopted by Donoho and
Montanari (2016) and Sur and Cand`es (2019) to study the high-dimensional M-estimation and logistic
regression respectively. Along a diﬀerent line, Thrampoulidis et al. (2015, 2018) introduced the CGMT
as a stronger version of the classical Gaussian Min-max Theorem due to Gordon (1988). The usefulness
of the CGMT lies on that it associates the original primary optimization (PO) with an auxiliary opti-
mization (AO) problem from which one can infer the asymptotic properties regarding the original PO.
In many applications, the AO problem can be reduced to an optimization problem involving only scalar
variables. The Karush-Kuhn-Tucker conditions with respect to the scalar variables in the AO problem
induce a set of equations that characterizes the asymptotic properties of the optimal solution to the
PO. The CGMT has proved useful in several contexts arising from high-dimensional statistics, machine
learning and information theory, see e.g., Dhifallah et al. (2018); Salehi et al. (2019); Hu and Lu (2019);
Liang and Sur (2020); Thrampoulidis et al. (2020); Javanmard and Soltanolkotabi (2020).

The main results (i) and (ii) in this paper are also built upon the CGMT. To obtain (i), we observe
that the existence of MPLE is related to the optimal value of a convex optimization problem. Using
the CGMT and some results from convex geometry, we prove the phase transition phenomenon for the
existence of MPLE and obtain the corresponding phase transition curve. Result (ii) are derived using the
CGMT by relating the MPLE to the solution of an AO problem. However, due to the non-separability
of the partial likelihood function, our analysis is more involved than those for M-estimation and logistic
regression, and extra eﬀort is needed to deal with the AO problem and derive the optimality conditions,
see Section S5. Finally, we emphasize that our arguments are diﬀerent from those in Sur and Cand`es
(2019) which is built on the AMP technique that does not seem directly applicable to our setting.

The rest of the paper is organized as follows. Section 2 introduces the setups and discusses the
failures of the classical large sample theories for MPLE in high-dimension. We study the existence of
MPLE and derive the phase transition curve in Section 3. We develop a new asymptotic theory in Section
4, which is used to perform asymptotic exact error analysis on the MPLE and to derive the asymptotic
distributions of the MPLE. We further present some numerical results to corroborate our theoretical
ﬁndings within each section. Section 5 concludes and discusses a few future research directions.

2 Preliminaries

2.1 Basic setup

Consider a sequence of i.i.d samples {(Xi, Ti)}n

i=1 generated from the population (X, T ), where
Xi = (Xi1, . . . , Xip)(cid:62) is a p-dimensional covariate associated with the ith individual. In practice, not all
the survival times are fully observable. We consider a sequence of right censoring times {Ci}n
i=1 that are
independent of the survival times {Ti}n
i=1 (see Remark S4.1 for a relaxation of this assumption). Thus
we work with the i.i.d. observations (Yi, Xi, ∆i), where Yi = Ti ∧ Ci := min(Ti, Ci) and ∆i = 1{Ti ≤ Ci}
are event time and censoring indicator, respectively. The Cox proportional hazards model speciﬁes the
hazard function for the ith individual as

where β∗ ∈ Rp is the parameter of interest and λ0(t) is the unknown baseline hazard function. The
maximum partial likelihood estimator (MPLE) is deﬁned as

λ(t|Xi) = λ0(t) exp(X(cid:62)

i β∗),

(1)

(cid:98)β = argmax

β

L(β), L(β) =

1
n




n
(cid:88)

i=1



X(cid:62)

i β − log





1
n

n
(cid:88)

j=1

1{Yj ≥ Yi} exp(X(cid:62)

j β)










∆i,

(2)

where L(β) is the log partial likelihood function evaluated at β. Compared to M-estimation and logistic
regression, the log partial likelihood is a sum of non-i.i.d random variables which complicates the analysis.

3

(a) The true and estimated values of the re-
gression coeﬃcients. The dark line segments
represent the values of β∗ and blue points rep-
resent the values of (cid:98)β for the corresponding
coordinates.

(b) The blue points correspond to the pairs
(β∗
j , (cid:98)βj) for j = 1, 2, . . . , p. The dark line
has slope one, and the red line is the ﬁtted
least squares regression line based on the blue
points.

Figure 1: The biasness of the MPLE.

2.2 Failures of classical large sample theories

In classical large sample theories, we assume p is ﬁxed and let n → ∞. Under mild regularity

conditions, the MPLE behaves similarly as the ordinary MLE (Murphy and van der Vaart, 2000)

√

n((cid:98)β − β∗) d→ N (0, I−1
β∗ ),

where Iβ∗ = −E[∂2L(β)/∂β∂β(cid:62)|β=β∗ ] is the p × p Fisher information matrix evaluated at the truth.
However, in the comparable setting where p goes to inﬁnity with the same rate as n, the classical theories
can lead to invalid inference. We use numerical examples to illustrate this point. Through the numerical
studies below, we set n = 4, 000 and p = 800 (so that δ = 0.2). Suppose the entries of the design matrix
(X1, . . . , Xn)(cid:62) follow N (0, 1/p) independently. We set λ0(t) = λ = 1 for the baseline hazard function
and Ci ∼i.i.d. Unif(1, 2) for the censoring times. We have the following observations which are in general
similar to those in Sur and Cand`es (2019) for high-dimensional logistic regression.

√

1. MPLE is biased. In the ﬁrst experiment, we set the ﬁrst hundred entries of β∗ to be 2

5, the next
hundred entries to be −2
5 and the remaining entries to be 0. It can be clearly seen from Figure
1(a) that MPLE is not unbiased. The absolute values of the estimates tend to be larger than the
true values. In the second experiment, we generate the entries of β∗ from N (1, 4) independently.
j , ˆβj) do not scatter around the 45 degree line but rather a
Figure 1(b) shows that the pairs of (β∗
diﬀerent line with a larger slope, which indicates an upward bias in the estimation.

√

2. The standard deviation (std.) of (cid:98)β from the Fisher information matrix (abbreviated as Fisher
std.) is smaller than the true std. To see this, we generate half of the entries of β∗ independently
from N (3, 1) and let the remaining be zeros. We conduct 1,000 simulation runs, estimate the
Fisher std. by the square root of the average of 1,000 diagonals of the inverse of the matrix
−∂2L(β)/∂β∂β(cid:62)|β=β∗ , and estimate the true std. by the std. of 1,000 estimates of β∗. Figure
(2) shows the mean of the 400 estimates of the Fisher stds of the null coeﬃcients and the histogram
of the estimates of the true stds of the null coeﬃcients. Apparently, the Fisher std. underestimates
the true std.

3. The partial log-likelihood ratio test does not converge to a chi-square distribution, and the Wald
z-test does not converge to a standard normal distribution. Again we let half of the entries
of β∗ be generated independently from N (3, 1) and the rest be zeros. We use the partial log-

4

−8−40480200400600800IndexTrue and Fitted Coefficients−505−3036True CoefficientsMPLEFigure 2: Comparison between the Fisher std. and true std. The red line represents the
Fisher std. The blue histogram depicts the empirical distribution of the std’s of the 400
null coeﬃcients.

likelihood ratio test to examine the signiﬁcance of the ﬁrst null coeﬃcient (i.e., the 401 entry of
the coeﬃcient vector). According to the classical large sample theory, the partial log-likelihood
ratio test converges in distribution to χ2
1 (Wilks, 1938). We conduct 50,000 simulation runs,
and calculate the p-values based on the χ2 approximation. From Figure 3(a), we see that the
distribution of the p-values deviates signiﬁcantly from the uniform distribution. Using the outputs
from the previous simulation (for the second bullet point), we can calculate the Wald z-statistics
by the ratio between the 1,000 estimates of the 400 null coeﬃcients and their Fisher stds, and
then obtain the p-values, see Figure 3(b). Again the p-values are not uniformly distributed in this
case.

3 Existence of the MPLE

3.1 Phase transition boundary curve

As the ﬁrst step toward understanding the behaviors of the MPLE in high-dimension, we characterize
the conditions for the existence of the MPLE. For high-dimensional logistic regression, Cand`es and Sur
(2018) established that the existence of the MLE undergoes a phase transition phenomenon, and obtained
the explicit form of the boundary curve. However, their argument is not directly applicable to the Cox
regression model due to the more complicated characterization of the existence of the MPLE and the
model structure. To overcome the diﬃculty, we present a new argument based on the CGMT technique.
The basic idea is to relate the existence of the MPLE to the optimal value of a convex optimization
problem (the PO problem). Using the CGMT, we can associate the PO problem with an AO problem.
By analyzing the corresponding AO problem, we ﬁnd the condition under which the MPLE exists with
probability approaching one. Using similar arguments, we manage to recover some of the results in
Cand`es and Sur (2018). The readers are referred to Section S3 for the details.

Throughout the section, we shall assume that Xi ∼i.i.d N (0, Σ) for a non-singular covariance matrix

5

02040600.700.750.800.850.90Std. of Coefficients of Null CoordinatesCounts(a) P-values of the partial log-likelihood ratio
tests for the null coeﬃcients using the χ2
1 ap-
proximation.

(b) P-values of the Wald-z tests for the null
coeﬃcients using the standard normal approx-
imation.

Figure 3: Invalid inferences based on the classical theory for MPLE.

Σ. We ﬁrst present the general conditions for the existence of the MPLE. Deﬁne the set

B := span {∆i(Xj − Xi) : 1 ≤ i ≤ n, j ∈ R(Yi) \ {i}} ,

where R(t) = {j : Yj ≥ t}. By Jacobsen (1989), the MPLE exists if and only if the following two
conditions are satisﬁed:

1. dim(B) = p;
2. There does not exist a nonzero vector b ∈ Rp such that

b(cid:62)(Xj − Xi) ≤ 0,

for all 1 ≤ i ≤ n with ∆i = 1 and j ∈ R(Yi) \ {i}.

Suppose Ci ≥ cL and P (Ti < cL) > c > 0. Then with probability tending to one, there exists a Yi with
Yi < cL and ∆i = 1.
In this case, Condition 1 holds with probability approaching one. By writing
Xi = Σ1/2Zi for Zi ∼i.i.d N (0, Ip), Condition 2 can be equivalently expressed as: there does not exist a
nonzero vector b ∈ Rp such that b(cid:62)(Zj − Zi) ≤ 0, for all 1 ≤ i ≤ n with ∆i = 1 and j ∈ R(Yi) \ {i}.
Therefore, without loss of generality, we may assume that Σ = Ip in the following discussions. Deﬁne
the set

D := {(i, j) : 1 ≤ i ≤ n, ∆i = 1, j ∈ R(Yi) \ {i}},

and let

κ2 = var(X(cid:62)

i β∗).

By the rotational invariance of the Gaussian distribution, we can show that the joint distribution of
(Yi, X(cid:62)

i ) = (Yi, Xi1, . . . , Xip) is the same as that of

(yi, q(cid:62)

i ) = (yi, qi1, . . . , qip),

where yi = ti ∧ Ci with ti having the hazard function

λ(t|qi1) = λ0(t) exp(κqi1)

and

qi = (qi1, . . . , qip)(cid:62) ∼ N (0, Ip),

(qi2, . . . , qip) ⊥ (yi, qi1),

6

01000200030000.000.250.500.751.00P valuesCounts01000020000300000.000.250.500.751.00P valuesCountsfor 1 ≤ i ≤ n. To examine the existence of the MPLE, we consider the convex optimization problem

max
−1≤b≤1

(cid:88)

(i,j)∈D

aijb(cid:62)(qi − qj)

subject to b(cid:62)(qi − qj) ≥ 0 for all (i, j) ∈ D,

(3)

where aij > 0 is prespeciﬁed and ﬁxed, b = (b1, . . . , bp)(cid:62) and −1 ≤ b ≤ 1 means −1 ≤ bi ≤ 1 for all
i. Clearly, the MPLE does not exist if and only if the optimal value of the above problem is greater
than zero. Before presenting the main result regarding the existence of the MPLE, we introduce some
quantities. Without loss of generality, we assume that

y1 ≥ y2 ≥ · · · ≥ yn,

and the indices of censored observations is smaller than the indices of uncensored observations that have
the same value. Let {2 ≤ i ≤ n : ∆i = 1} = {i1, . . . , ik}. Deﬁne the set

(cid:26)

M =

m = (m1, . . . , mn) ∈ Rn : min
s<il

ms ≥ mil , max
j∈Dil

mj ≤ mil , l = 1, . . . , k

,

(cid:27)

where Dil = {1 ≤ j < il : yj = yil , ∆j = 1}. We are now in position to present the main result of this
section.

Theorem 3.1. Deﬁne the quantities

hU (λ0, κ, PC) = lim sup

min
t∈R,m∈M

(cid:107)h − t(cid:101)q − m(cid:107)2,

hL(λ0, κ, PC) = lim inf

min
t∈R,m∈M

(cid:107)h − t(cid:101)q − m(cid:107)2,

1
n
1
n

(4)

(5)

where (cid:101)q = (q11, . . . , qn1)(cid:62) and h ∼ N (0, In) is independent of {(yi, qi1)}n
i=1. The MPLE exists (with
probability tending to one) if δ < hL(λ0, κ, PC) and the MLE does not exist (with probability tending to
one) if δ > hU (λ0, κ, PC). When hU (λ0, κ, PC) = hL(λ0, κ, PC) = h(λ0, κ, PC), the MPLE undergoes a
phase transition with h(λ0, κ, PC) being the boundary curve.

Remark 3.1. The restriction in M can be equivalently expressed as the following pairwise constraints





mi ≤ mj
mi ≥ mj
mi = mj
no restriction

∆i1{yj ≥ yi} = 1, ∆j1{yi ≥ yj} = 0,
∆i1{yj ≥ yi} = 0, ∆j1{yi ≥ yj} = 1,
∆i1{yj ≥ yi} = 1, ∆j1{yi ≥ yj} = 1,
∆i1{yj ≥ yi} = 0, ∆j1{yi ≥ yj} = 0.

Under the assumption that y1 > y2 > · · · > yn (i.e., there is no tie), we argue that the optimization with
respect to m ∈ M in the deﬁnitions of hU and hL can be translated into a quadratic programming (QP)
with at most n − 1 inequality constraints. For each 1 ≤ i ≤ n − 1, let ki be the smallest index such that
ki > i and ∆ki = 1. Let G be the set of indices i such that the corresponding ki exists. Then for ﬁxed
t ∈ R, the optimization with respect to m ∈ M in (4) and (5) can be formulated as

Gn(t) :=

min
m=(m1,...,mn)∈M

(cid:107)(cid:101)ht − m(cid:107)2,

subject to mi ≥ mki for i ∈ G,

with (cid:101)ht = h − t(cid:101)q, which can be solved eﬃciently using existing QP solvers. By performing an one-
dimensional optimization, we can ﬁnd mint∈R Gn(t) = mint∈R,m∈M (cid:107)h − t(cid:101)q − m(cid:107)2.

7

3.2 Checking the existence of MPLE in ﬁnite sample

Next, we discuss how to solve the convex optimization problem (3) by reducing the number of
constraints and conduct a numerical study to compare the phase transition boundary curve with the
empirical results. Indeed we can infer that the number of constraints is no more than 2(n − 1). More
precisely, the number of constraints is equal to ik − 1 + (cid:80)s
l=1(ml − 1), where ik is the maximum index
of uncensored observations, s is the number of tie values that have at least two uncensored observations,
and ml is the number of uncensored observations that are equal to the lth tie value. In fact, we can
write down the constraints explicitly. Let {i1, . . . , ik} = {1 ≤ i ≤ n : ∆i = 1} with i1 < · · · < ik. Let
{jl,1, . . . , jl,ml } ⊆ {i1, . . . , ik} with jl,1 < · · · < jl,ml be the index set of the uncensored observations that
are equal to the lth tie value (with at least two uncensored observations) for l = 1, . . . , s. Then the full
set of constraints are given by

b(cid:62)(qil − qil−1 ) ≥ 0, b(cid:62)(qil − qil−1+1) ≥ 0, . . . , b(cid:62)(qil − qil−1) ≥ 0,
b(cid:62)(qjl,1 − qjl,2 ) ≥ 0, b(cid:62)(qjl,2 − qjl,3) ≥ 0, . . . , b(cid:62)(qjl,ml−1 − qjl,ml

l = 1, . . . , k,

) ≥ 0,

l = 1, . . . , s,

(6)

(7)

where i0 = 1 and (7) is the additional set of constraints due to the existence of ties. When there is
no tie, we only need the ik − 1 constraints in (6). Under the constraints in (6) and (7) and using the
simple fact that b(cid:62)(qi − qj) ≥ 0 and b(cid:62)(qj − qk) ≥ 0 imply that b(cid:62)(qi − qk) ≥ 0, one can recover all
the constraints in (3). Therefore, the existence of the MPLE can be solved eﬃciently through the linear
programming (3) with ik − 1 + (cid:80)s

l=1(ml − 1) constraints.

We empirically verify that the existence of the MPLE undergoes a phase transition and the ﬁnite
sample transition boundary matches well with the theoretical boundary curve derived in Theorem 3.1. To
generate the data, we assume that each β∗
i /cκ is independently generated from the uniform distribution
on [κ − 1, κ + 1], where the scaling parameter cκ = (cid:112)κ2/(1/3 + κ2) ensures that (cid:107)β∗(cid:107)/
p = κ. The
i β∗), and
survival time Ti follows the exponential distribution with the rate parameter λi = exp(X(cid:62)
the censoring time Ci follows the uniform distribution on [1, 2] which is independent of Ti. As there
is no tie in {Yi}, we can examine the existence of the MPLE by solving problem (3) with the im − 1
constraints given in (6). Figure 4 (a) summarizes the results based on n = 500 and 500 replications. The
red theoretical boundary curve that separates the δ − κ plane into two regions is obtained by solving
the constrained quadratic programming described in Remark 3.1. While the white and black regions
obtained by solving the problem (3) indicate the probability that the MPLE exists (black is zero, and
white is one). Overall, the ﬁnite sample transition boundary is consistent with the theoretical boundary,
which demonstrates the practical relevance of the theoretical ﬁnding. In addition, we explore the change
of the phase transition boundary with diﬀerent censoring time distributions. Consider Ci ∼ U [1, b + 1],
where b ∈ {0.5, 1, 7, 30}. The phase transition boundary in Figure 4 (b) shifts from the right to the left
as b decreases, which makes intuitive sense as for a higher censoring rate (i.e., smaller b), the existence
of the MPLE requires a smaller δ.

√

4 A New Asymptotic Theory

4.1 Error analysis

We develop a new asymptotic theory to describe the asymptotic behavior of the MPLE in the high-
dimensional setting. The core of our theory is a set of nonlinear equations derived using CGMT that
characterize the behavior of the MPLE. Built upon these equations, we perform an asymptotic exact error
analysis on the MPLE and study the asymptotic distributions of the MPLE. Throughout the discussions
below, we assume that

A1 Xij ∼i.i.d N (0, 1/p) for 1 ≤ i ≤ n and 1 ≤ j ≤ p;

A2 p/n → δ ∈ (0, 1).

8

(a) Empirical probability that the MPLE ex-
ists (black is zero, and white is one) estimated
based on n = 500 samples and 500 replica-
tions. The red curve indicates the theoretical
transition boundary.

(b) The theoretical transition boundary curves
when the censoring time Ci ∼ U [1, b + 1] with
b ∈ {0.5, 1, 7, 30}.

Figure 4: Theoretical transition boundary and the empirical probability that the MPLE
exists.

Recall that under the proportional hazards model (1), the survival function of the survival time Ti is
given by

S(x|X(cid:62)

i β∗) = exp

(cid:26)

− exp(X(cid:62)

i β∗)

(cid:27)

λ0(t)dt

,

(cid:90) x

0

As κ2 = var(X(cid:62)
The next assumption can be justiﬁed under mild conditions using the law of large numbers.

i β∗ =d κZ for Z ∼ N (0, 1), where “=d” means equal in distribution.

i β∗) = (cid:107)β∗(cid:107)2/p, X(cid:62)

A3 Assume that

1
n

n
(cid:88)

1{Ti ≤ Ci} →p 1 − E[S(C|κZ)],

i=1
n
(cid:88)

i=1

1
κn

1{Ti ≤ Ci}X(cid:62)

i β∗ →p −E[S(C|κZ)Z].

(8)

(9)

where (C, κZ) =d (Ci, X(cid:62)

i β∗).

Let a, b and r be three scalar quantities that are used to describe the asymptotic behavior of MPLE.
i β∗/κ and
√
δb/r,

The roles of a and b will be made clear later. Further let q = (q1, . . . , qn)(cid:62) with qi = X(cid:62)
h ∼ N (0, In) that is independent with (Yi, Xi, ∆i, Ci). Set ξ = (ξ1, . . . , ξn)(cid:62) = κaq + bh + ∆
where ∆ = (∆1, . . . , ∆n)(cid:62). We write Gn(u) := (cid:80)n
(u1, . . . , un). Deﬁne the proximal operator of Gn(u) at ξ as

(cid:17)
j=1 1{Yj ≥ Yi} exp(uj)

i=1 ∆i log

n−1 (cid:80)n

for u =

(cid:16)

u∗ = (u∗

1, . . . , u∗

n) = arg min
u∈Rn

Gn(u) +

r
√
δb
2

(cid:107)u − ξ(cid:107)2 .

To introduce the main result, we require convergence of some counting processes at u∗. Let Yi(t) ∈ {0, 1}
be a predictable at risk indicator process which takes the value one when the ith subject is under
observation (Andersen and Gill, 1982). We make the following weak convergence assumption.

9

01020300.00.20.40.60.8deltaKappa0.000.250.500.751.00probn=50001020300.50.60.70.80.91.0deltakappab0.51730A4 There exist processes S(s, t), S(t) and R(t) such that

1
n

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

Yi(s)Yi(t) exp(2u∗

i ) →p S(s, t),

Yi(t) exp(u∗

i ) →p S(t),

Yi(t) exp(X(cid:62)

i β∗)λ0(t) →p R(t).

For b1, b2, b3 ∈ R, consider the nonlinear equation b3{log(u) − b1} = −b2u with respect to u > 0. Let
K(b1, b2, b3) be the solution to the equation, i.e., b3[log{K(b1, b2, b3)} − b1] = −b2K(b1, b2, b3). We
introduce the following function

(cid:32)

M

κa, b,

√
b

δ

(cid:33)

r

=

(cid:90) 1

0

log(S(s))R(s)ds +

b
√
2r

δ

(cid:90) 1

(cid:90) 1

E

(cid:104)

Y (s)Y (t)K 2 (cid:16)

ξ, (cid:82) 1
0

Y (u)
S(u) R(u)du, r/(b

√

(cid:17)(cid:105)

δ)

0

0

S(s)S(t)

R(s)R(t)dsdt,

where S(·) is the solution to the equation

(cid:20)

(cid:18)

S(s) = E

Y (s)K

ξ,

(cid:90) 1

0

Y (u)
S(u)

R(u)du,

(cid:19)(cid:21)

r
√

b

δ

and (Y (t), ξ) =d (Yi(t), ξi). Denote the partial derivative of M (·, ·, ·) by

Mi(a1, a2, a3) =

∂M (a1, a2, a3)
∂ai

,

1 ≤ i ≤ 3.

Theorem 4.1. Under Assumptions A1-A4, the asymptotic behavior of the MPLE is governed by the
following three nonlinear equations:

(cid:32)

M1

κa, b,

(cid:32)

M2

κa, b,

(cid:32)

M3

κa, b,

(cid:33)

δ

(cid:33)

δ

(cid:33)

δ

√

r
√

r
√

b

b

b

r

= −E[S(C|κZ)Z],

√

δr,

=

= −

r2
2

+

1
2

(1 − E[S(C|κZ)]) .

(10)

(11)

(12)

Let (a∗, b∗, r∗) be the solution to the nonlinear equations (10)-(12). We have the following result

connecting (a∗, b∗) with the asymptotic error of the MPLE.

Theorem 4.2. Under Assumptions A1-A4, we have

(cid:107)(cid:98)β − β∗(cid:107)2

(cid:107)β∗(cid:107)2 →p (a∗ − 1)2 +

(b∗)2
κ2 ,

(cid:107)(cid:98)β − a∗β∗(cid:107)2
p

→p (b∗)2.

The proof of Theorem 4.2 relies on showing that

(cid:62)

(cid:98)β

β∗/(cid:107)β∗(cid:107)2 →p a∗,

(cid:107)P⊥β∗(cid:107)/

√

p →p b∗.

(13)

10

√

In other words, a∗(cid:107)β∗(cid:107) measures the projection of the MPLE onto the direction of the true parameter
β∗, and
pb∗ is approximately the norm of the projection of the MPLE onto the space spanned by
the columns of P⊥. We conduct a numerical study to verify (13) by following the same data generating
mechanism considered in Section 3.2. Fixing n = 500, we vary δ from 0.1 to 0.4 and κ from 1 to 6. Denote
p. We obtain (a∗, b∗) by ﬁnding an approximate solution to
by ˆa = (cid:98)β
the nonlinear equations (10)-(12). See Section S5 in the supplementary material for the details. As seen
from Figure 5. ˆa and ˆb are quite consistent with their theoretical values a∗ and b∗ in all cases.

β∗/(cid:107)β∗(cid:107)2 and ˆb = (cid:107)(cid:98)β − ˆaβ∗(cid:107)/

√

(cid:62)

Figure 5: Comparison between (ˆa, ˆb) and (a∗, b∗) for various values of δ and κ, where
n = 500 and the number of replications is 100.

Remark 4.1. Let g ∼ N (0, Ip) be independent of other random quantities. Deﬁne P = β∗β∗(cid:62)/(cid:107)β∗(cid:107)2
and P⊥ = Ip − P. From the derivations in the analysis of the AO in Section S5, we know that

(cid:98)β = P(cid:98)β + P⊥ (cid:98)β ≈ a∗β∗ +

P⊥ (cid:98)β
(cid:107)P⊥ (cid:98)β(cid:107)

(cid:107)P⊥ (cid:98)β(cid:107) ≈ a∗β∗ +

P⊥g
(cid:107)P⊥g(cid:107)

(cid:107)P⊥ (cid:98)β(cid:107) ≈ a∗β∗ + b∗P⊥g,

where the ﬁrst approximation is due to (cid:98)β
P⊥ (cid:98)β/(cid:107)P⊥ (cid:98)β(cid:107) ≈ P⊥g/(cid:107)P⊥g(cid:107) and the third approximation is from the fact that (cid:107)P⊥ (cid:98)β(cid:107)/
(cid:107)P⊥g(cid:107)/
continuous bivariate function ψ(·, ·), we expect that

β∗/(cid:107)β∗(cid:107)2 ≈ a∗, the second approximation is because of
p ≈ b∗ and
p →p 1. Suppose the entries of β∗ are drawn independently from a distribution P0. For a

√

√

(cid:62)

1
p

p
(cid:88)

j=1

ψ( (cid:98)βj − a∗β∗

j , β∗

j ) ≈

1
p

p
(cid:88)

j=1

ψ((b∗P⊥g)j, β∗

j ) →p E[ψ(b∗Z, β0)],

where (b∗P⊥g)j denotes the jth component of b∗P⊥g, Z ∼ N (0, 1) and β0 ∼ P0.

11

kappa: 4kappa: 5kappa: 6kappa: 1kappa: 2kappa: 30.10.20.30.40.10.20.30.40.10.20.30.4123123deltaValuecurvea^b^a * b * 4.2 Asymptotic distributions

In this section, we derive the asymptotic distribution of the MPLE. Let S0 be the set of the null

components, i.e., S0 = {1 ≤ j ≤ p : β∗

j = 0}.

Theorem 4.3. Suppose S ⊆ S0 := {1 ≤ j ≤ p : β∗
Assumptions A1-A4, we have

j = 0} and |S| = l is ﬁxed in the asymptotics. Under

where (cid:98)βS = { (cid:98)βj : j ∈ S}. As a consequence,

(cid:98)βS
b∗ →d N (0, Il),

(cid:33)2

(cid:32) ˆβj
b∗

(cid:88)

j∈S

→d χ2
l .

The above theorem shows that the MPLE of the null coeﬃcients scaled by the constant b∗ converges
to a multivariate normal distribution with the identity covariance matrix and hence the Wald test formed
by the sum of squares of the MPLE converges to a chi-square distribution.

j /c(cid:48)

Below we conduct a simulation study to demonstrate the practical relevance of the ﬁnding in Theorem
4.3. Consider n = 500, p = 200 and half of the coordinates of β are non-zero with κ = 1. Each non-zero
component β∗
κ is independently generated from the uniform distribution on [κ − 1, κ + 1], where the
κ is set to (cid:112)2κ2/(1/3 + κ2) to keep the signal strength (cid:107)β(cid:107)/
scaling parameter c(cid:48)
p equal to κ. We
generate 50, 000 independent data sets and ﬁt the Cox regression model to each data set. Figure 6
(a) presents the two sided p-value pi = 2Φ(−| ˆβj/b∗|) for the ﬁrst 50 null coordinates of β (combined
over the 50, 000 simulation runs). We also show the empirical cumulative distribution function (cdf) of
Φ( ˆβj/b∗) for a particular null coordinate of β in Figure 6 (b). We observe that the p-values are uniformly
distributed and there is a perfect agreement between the empirical cdf and the 45 degree line.

√

(a) Histogram of the p-values for the ﬁrst ﬁfty
null coordinates of β.

(b) Empirical cdf of Φ( ˆβj/b∗) for a particular
null coordinate.

Figure 6: Histogram and empirical cdf, where n = 500, p = 200 and half of the coordinates
of β are non-zero with κ = 1. The results are based on 50, 000 replications.

Next we examine the chi-square approximation for the quantity (cid:80)

j∈S ( ˆβj/b∗)2. Figure 7 depicts
the histograms for the p-value pi = Fl((cid:80)
j = 0} with
|S| = l = 2, 5 and Fl denotes the cdf for the chi-square distribution with l degrees of freedom. The
results suggest that the chi-square approximation is quite accurate.

j∈S ( ˆβj/b∗)2), where S ⊆ S0 := {1 ≤ j ≤ p : β∗

12

02000040000600000.000.250.500.751.00P−Valuescount0.000.250.500.751.000.000.250.500.751.00tEmpirical cdf(a) l = 2

(b) l = 5

Figure 7: Histograms for pi = Fl((cid:80)
j∈S( ˆβj/b∗)2) based on the chi-square approximation,
where n = 500, p = 200 and half of the coordinates of β are non-zero with κ = 1. The
results are based on 50, 000 replications.

5 Conclusion

In this paper, we studied the asymptotic behavior of the MPLE in a high-dimensional Cox regression
model with Gaussian covariates. We showed that the extence of the MPLE undergoes a sharp phase
transition and we derived the explicit expression for phase transition boundary. In addition, we developed
a new theory which gives the asymptotic distributions of the MPLE in the Cox regression model with
independent Gaussian covariates. As a byproduct, we also obtained the limiting distribution for the
Wald test. Our methods are built on some elements from convex geometry and the CGMT which is a
modern version of the Gaussian comparison inequalities.

Finally we mention two future research directions. First, it would be interesting to investigate if the
results derived in this paper hold for more general covariate distributions. Second, it is of interest to
study the penalized regression problem argmin β L(β) + ρ(β), for some partial likelihood function L(·)
and penalty function ρ(·). We leave these problems as future research topics.

References

Sur, P., Chen, Y., and Cand`es, E. J. (2019). The likelihood ratio test in high-dimensional logistic regres-

sion is asymptotically a rescaled chi-square. Probability theory and related ﬁelds, 175, 487-558.

Andersen, P. K., and Gill, R. D. (1982). Cox’s regression model for counting processes: a large sample

study. Annals of Statistics, 10, 1100-1120.

Moreau, J. J. (1962). D´ecomposition orthogonale d’un espace hilbertien selon deux cˆones mutuellement

polaires. Comptes rendus hebdomadaires des s´eances de l’Acad´emie des sciences, 238–240.

Dhifallah, O., Thrampoulidis, C., and Lu, Y. M. (2018). Phase retrieval via polytope optimization:

Geometry, phase transitions, and new algorithms. arXiv preprint arXiv:1805.09555.

Hu, H., and Lu, Y. M. (2019, July). Asymptotics and optimal designs of SLOPE for sparse linear
regression. In 2019 IEEE International Symposium on Information Theory (ISIT) (pp. 375-379). IEEE.

Gordon, Y. (1988). On Milman’s inequality and random subspaces which escape through a mesh in Rn.

In Geometric aspects of functional analysis (pp. 84-106). Springer, Berlin, Heidelberg.

Donoho, D. L., Maleki, A., and Montanari, A. (2009). Message-passing algorithms for compressed sensing.

Proceedings of the National Academy of Sciences, 106, 18914-18919.

13

02000040000600000.000.250.500.751.00P−Valuescount010000200000.000.250.500.751.00P−ValuescountThrampoulidis, C., Oymak, S., and Soltanolkotabi, M. (2020). Theoretical insights into multiclass clas-

siﬁcation: A high-dimensional asymptotic view. arXiv preprint arXiv:2011.07729.

Javanmard, A., and Soltanolkotabi, M. (2020). Precise statistical analysis of classiﬁcation accuracies for

adversarial training. arXiv preprint arXiv:2010.11213.

Liang, T., and Sur, P. (2020). A Precise High-Dimensional Asymptotic Theory for Boosting and

Minimum-L1-Norm Interpolated Classiﬁers. arXiv preprint arXiv:2002.01586.

Fang, E. X., Ning, Y., and Liu, H. (2017). Testing and conﬁdence intervals for high dimensional pro-
portional hazards models. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
79, 1415-1437.

Kong, S., Yu, Z., Zhang, X., and Cheng, G. (2021). High-dimensional robust inference for Cox regression

models using desparsiﬁed Lasso. Scandinavian Journal of Statistics, 48, 1068-1095.

Yu, Y., Bradic, J., and Samworth, R. J. (2018). Conﬁdence intervals for high-dimensional Cox models.

arXiv preprint arXiv:1803.01150.

Ga¨ıﬀas, S., and Guilloux, A. (2012). High-dimensional additive hazards models and the Lasso. Electronic

Journal of Statistics, 6, 522-546.

Kong, S., and Nan, B. (2014). Non-asymptotic oracle inequalities for the high-dimensional Cox regression

via Lasso. Statistica Sinica, 24, 25-42.

Gui, J., and Li, H. (2005). Penalized Cox regression analysis in the high-dimensional and low-sample size

settings, with applications to microarray gene expression data. Bioinformatics, 21, 3001-3008.

Zhang, H. H., and Lu, W. (2007). Adaptive Lasso for Cox’s proportional hazards model. Biometrika, 94,

691-703.

Huang, J., Sun, T., Ying, Z., Yu, Y., and Zhang, C. H. (2013). Oracle inequalities for the lasso in the

Cox model. Annals of statistics, 41, 1142.

Bradic, J., Fan, J., and Jiang, J. (2011). Regularization for Cox’s proportional hazards model with

NP-dimensionality. The Annals of Statistics 39, 3092–3120.

Cand`es, E. J. and Sur, P. (2018) . The phase transition for the existence of the maximum likelihood

estimate in high-dimensional logistic regression. arXiv preprint arXiv:1804.09753.

Donoho, D. and Montanari A. (2016). High dimensional robust M-estimation: Asymptotic variance via

approximate message passing. Probability Theory and Related Fields 166, 935–969.

El Karoui, N. (2013). Asymptotic behavior of unregularized and ridge-regularized high-dimensional ro-

bust regression estimators: rigorous results. arXiv preprint arXiv:1311.2445.

El Karoui, N., Bean, D., Bickel, P. J., Lim, C., and Yu, B. (2013). On robust regression with high-

dimensional predictors. Proceedings of the National Academy of Sciences 110, 14557–14562.

Fan, J. and Li, R. (2002). Variable selection for Cox’s proportional hazards model and frailty model. The

Annals of Statistics 30, 74–99.

Huber , P. J. and Ronchetti, E. (2009). Robust statistics (second edition). John Wiley and Sons.

Jacobsen M. (1989). Existence and Unicity of MLEs in Discrete Exponential Family Distributions. Scan-

dinavian Journal of Statistics 16, 335–349.

Murphy, S. A. and van der Vaart, A. W. (2000). On proﬁle likelihood. Journal of American Statistical

Association 95, 449–465.

14

Salehi, F., Abbasi, E., and Hassibi, B. (2019) The impact of regulation on high-dimensional logistic

regression. Neural Information Processing Systems.

Sur, P. and Cand`es, E. J. (2019) . A modern maximum-likelihood theory for high-dimensional logistic

regression. Proceedings of the National Academy of Sciences 116 , 14516–14525.

Thrampoulidis, C., Abbasi, E., and Hassibi, B. (2018). Precise error analysis of regularized m-estimators

in high dimensions. IEEE Transactions on Information Theory 64, 5592–5628.

Thrampoulidis, C., Oymak, S., and Hassibi, B. (2015). Regularized linear regression: A precise analysis

of the estimation error. In Conference on Learning Theory, 1683–1709.

Tibshirani, R. (1997). The LASSO method for variable selection in the Cox model. Statistics in Medicine

16, 385–395.

Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses.

The annals of mathematical statistics, 9, 60–62.

15

Supplementary Material

S1 Convex Gaussian Min-max Theorem
Deﬁnition S1.1 (GMT admissible sequence (Thrampoulidis et al., 2015)). Let G ∈ Rn×p, h ∈ Rn,
g ∈ Rp, Sw ⊂ Rp, Su ⊂ Rn, ψ : Sw × Su → R, all indexed by p (n = n(p)). The sequence
{G, g, h, Sw, Su, ψ}p∈N, where N denotes the set of positive integers, is said to be admissible if for each
p ∈ N, Sw and Su are compact sets and ψ is continuous on its domain.

A sequence {G, g, h, Sw, Su, ψ}p∈N deﬁnes a sequence of min-max problems:

Φ(G) := min
w∈Sw
φ(g, h) := min
w∈Sw

max
u∈Su
max
u∈Su

u(cid:62)Gw + ψ(w, u),

(cid:107)u(cid:107)2g(cid:62)w + (cid:107)w(cid:107)2h(cid:62)u + ψ(w, u).

(S1)

(S2)

They are referred to as the Primary Optimization (PO) and Auxiliary Optimization (AO) problems,
respectively. Denote the optimal minimizer of (S1) as wΦ(G). Then the CGMT can be stated as follows.

Theorem S1.2 (CGMT (Thrampoulidis et al., 2015)). Let {G, g, h, Sw, Su, ψ}p∈N be a GMT admis-
sible sequence, for which additionally the entries of G, g and h are i.i.d. N (0, 1). The following four
statements hold.
(i) For any p ∈ N and c ∈ R,

P{Φ(G) < c} ≤ 2P{φ(g, h) < c}.

(ii) Fix any p ∈ N. If Sw, Su are convex sets, and ψ(·, ·) is convex-concave (i.e., convex on its ﬁrst
argument and concave on its second argument) on Sw × Su, then, for any µ ∈ R and t > 0,

P {|Φ(G) − µ| > t} ≤ 2P {|φ(g, h) − µ| > t} .

(iii) Let S be an arbitrary open subset of Sw and S c = Sw \ S. Denote ΦS c(G) and φS c(g, h) be the
optimal costs of the optimizations in (S1) and (S2), respectively, when the minimization over w is now
constrained over w ∈ S c. If there exist constants ¯φ, ¯φS c, and η > 0 such that,

a ¯φS c ≥ ¯φ + 3η;
b φ(g, h) < ¯φ + η with probability at least 1 − p;
c φS c (g, h) > ¯φS c − η with probability at least 1 − p;

Then we have P (wΦ(G) ∈ S) ≥ 1 − 4p. Here the probabilities are taken with respect to the randomness
in G, g and h.
(iv) Following the notation in (iii), suppose there exist constants ¯φ < ¯φS c such that φ(g, h) →p ¯φ and
φS c(g, h) →p ¯φS c. Then

P (wΦ(G) ∈ S) → 1.

S2 A useful result from convex analysis

Let C be a non-empty subset of Rn. The polar cone of C, denoted by C∗, is deﬁned as

C∗ = {c∗ ∈ Rn : (cid:104)c∗, c(cid:105) ≤ 0 for all c ∈ C}.

We state the following result from the classical convex analysis.

Proposition S2.1. (Moreau, 1962) Suppose C is a closed convex cone. For h ∈ Rn, let ΠC(h) be the
projection of h onto C. Then we have the decomposition

h = ΠC(h) + ΠC∗ (h),

(cid:104)ΠC(h), ΠC∗ (h)(cid:105) = 0.

16

As a consequence, (cid:104)h − ΠC(h), ΠC(h)(cid:105) = 0 and hence (cid:107)h(cid:107)2 = (cid:107)ΠC(h)(cid:107)2 + (cid:107)h − ΠC(h)(cid:107)2.

Proposition S2.2 (Polar cone theorem). Suppose C is a closed convex cone. Then

Lemma S2.3. For two sets C1 and C2, we have (C1 + C2)∗ = C∗

1 ∩ C∗
2 .

(C∗)∗ = C.

S3 Existence of the MLE in logistic regression: a

revisit

We revisit the logistic regression and reproduce the results in Cand`es and Sur (2018) using a new
argument based on the CGMT. The new argument will be generalized to the Cox model in the next
section. Cand`es and Sur (2018) considered the model

P (Yi = 1|Xi) = 1 − P (Yi = −1|Xi) = σ(β∗

0 + X(cid:62)

i β∗

1),

σ(x) :=

1
1 + exp(−x)

, Xi ∼ N (0, Σ),

where β∗

0 ∈ R and β∗

1 ∈ Rp. Following their arguments, we have the equivalent model

where

(Yi, Xi) =d (yi, qi),

P (yi = 1|qi1) = 1 − P (yi = −1|qi1) = σ(β0 + γ0qi1),
(qi2, . . . , qip) ∼ N (0, Ip−1),
(qi2, . . . , qip) ⊥ (yi, qi1).

The MLE does not exist if and only if there exist b0 ∈ R and b1 ∈ Rp such that (b0, b(cid:62)

1 )(cid:62) (cid:54)= 0 and

yi(b0 + q(cid:62)

i b1) ≥ 0,

for all 1 ≤ i ≤ n. Due to the independence between (yi, qi1) and (qi2, . . . , qip), we have yi(qi2, . . . , qip) ∼
N (0, Ip−1). Let V be a n × 2 matrix with the ith row being (yi, yiqi1) and Q be a n × (p − 1) matrix with
the ith row being yi(qi2, . . . , qip) ∼ N (0, Ip−1), which is independent of V. For a vector a = (a1, . . . , ap)(cid:62),
we write a ≥ c (or a ≤ c) if ai ≥ c (or ai ≤ c) for all 1 ≤ i ≤ p. To examine the existence of the MLE,
we ﬁx any u > 0 and consider the convex optimization problem

max
−1≤b1≤1,−1≤b2≤1

u(cid:62)(Vb1 + Qb2)

subject to Vb1 + Qb2 ≥ 0,

where b1 ∈ R2 and b2 ∈ Rp−1. Notice that the MLE exists if and only if the optimal value of the
objective function is equal to zero. We rewrite the problem in the Lagrangian form as

Φ(V, Q) =

max
−1≤b1≤1,−1≤b2≤1

min
v≥0

u(cid:62)(Vb1 + Qb2) + v(cid:62)(Vb1 + Qb2)

(S3)

= min
v≥0

max
−1≤b1≤1,−1≤b2≤1

(u + v)(cid:62)Vb1 + (u + v)(cid:62)Qb2,

where we can switch the order of the maximization and minimization as the objective function in (S3)
is concave-convex in its arguments. By the CGMT, we consider an asymptotically equivalent problem
of the form

φ(V, g, h) = min
v≥0

max
−1≤b1≤1,−1≤b2≤1

(u + v)(cid:62)Vb1 + (cid:107)u + v(cid:107)g(cid:62)b2 − (cid:107)b2(cid:107)h(cid:62)(u + v),

17

where g ∈ Rp−1 and h ∈ Rn both have i.i.d N (0, 1) components. Taking maximization with respect to
the directions of b1 and b2, we obtain

min
v≥0

max
(cid:107)b1(cid:107),(cid:107)b2(cid:107)

(cid:107)V(cid:62)(u + v)(cid:107)(cid:107)b1(cid:107) + (cid:107)u + v(cid:107)(cid:107)g(cid:107)(cid:107)b2(cid:107) − (cid:107)b2(cid:107)h(cid:62)(u + v).

(S4)

We observe two facts:

a. Φ(V, Q) ≥ 0 and φ(V, g, h) ≥ 0;

b. P (Φ(V, Q) > 0) = P (MLE does not exist).

Setting µ = 0 in (ii) of Theorem S1.2 and using fact (a), we have

P (Φ(V, Q) > t) ≤ 2P (φ(V, g, h) > t).

Letting t ↓ 0 and using fact (b), we get

P (MLE does not exist) = P (Φ(V, Q) > 0) ≤ 2P (φ(V, g, h) > 0).

On the other hand, letting c ↓ 0 in (i) of Theorem S1.2, we obtain

P (MLE exists) = P (Φ(V, Q) = 0) ≤ 2P (φ(V, g, h) = 0).

Next we introduce some notation. Deﬁne

C = span(V) + {b : b ≤ 0} = {a + b : a ∈ span(V), b ≤ 0},

where span(V) is the space spanned by the columns of V. Let

C∗ = span⊥(V) ∩ {b : b ≥ 0},

(S5)

(S6)

where span⊥(V) denotes the orthogonal complement of span(V). Note that span⊥(V) is also the polar
cone of span(V) and {b : b ≥ 0} is the polar cone of {b : b ≤ 0}. Using Lemma S2.3, C∗ is the polar
cone of C. By Proposition S2.1, we have

(cid:107)ΠC∗ (h)(cid:107)2 = (cid:107)h − ΠC(h)(cid:107)2 =

min
a∈span(V),b≤0

(cid:107)h − a − b(cid:107)2 = min

a∈span(V)

(cid:107)(h − a)+(cid:107)2.

As p/n → δ, by the laws of large numbers, we have

(cid:107)ΠC∗ (h)(cid:107)2 =

1
n

(cid:107)g(cid:107)2 →p δ,

1
n
1
n

min
a∈span(V)

(cid:107)(h − a)+(cid:107)2 →p min
t1,t2∈R

E[(h − t1y − t2yq)2

+],

where (y, q) has the same distribution as that of (yi, qi1). Below we consider two cases.

Case 1: Assuming δ > mint1,t2∈R E[(h−t1y−t2yq)2

+], we aim to show that P (MLE does not exist) →

1. In this case, we have

1
√
n

(cid:107)ΠC∗ (h)(cid:107) <

1
√
n

(cid:107)g(cid:107)

with probability tending to one. For the objective function in (S4) to be zero, we need to ﬁnd a vector
v ≥ 0 such that

V(cid:62)(u + v) = 0

and

h(cid:62)(u + v)
(cid:107)u + v(cid:107)

≥ (cid:107)g(cid:107).

However, this event happens with probability tending to zero as when V(cid:62)(u+v) = 0, we have u+v ∈ C∗

18

and

Therefore, we must have

P

(cid:18) 1
√
n

(cid:107)g(cid:107) >

1
√
n

(cid:107)ΠC∗ (h)(cid:107) ≥

1
√
n

h(cid:62)(u + v)
(cid:107)u + v(cid:107)

(cid:19)

→ 1.

P (MLE exists) = P (Φ(V, Q) = 0) ≤ 2P (φ(V, g, h) = 0) → 0,

which implies that P (MLE does not exist) → 1.

Case 2: Assuming δ < mint1,t2∈R E[(h − t1y − t2yq)2

+ = {b ∈ Rn : b ≥ 0} and (cid:101)Rn

Rn
and Sur (2018), we have

+ be the interior of Rn

+], we show that P (MLE exists) → 1. Let
+. By similar arguments as in Lemma 2 of Cand`es

We note that for any u ∈ (cid:101)Rn
+,

P (span⊥(V) ∩ (cid:101)Rn

+ (cid:54)= ∅) → 1.

(cid:26) u + v
(cid:107)u + v(cid:107)

(cid:27)

: v ≥ 0

=

(cid:26) v
(cid:107)v(cid:107)

: v ∈ (cid:101)Rn
+

(cid:27)

.

(S7)

(S8)

With probability converging to one, the projection of h onto C∗ is in (cid:101)Rn
bility, we can ﬁnd (cid:101)v ∈ span⊥(V) ∩ (cid:101)Rn
a v∗ ≥ 0 satisfying that u + v∗ ∈ span⊥(V) ∩ (cid:101)Rn

+ such that (cid:107)ΠC∗ (h)(cid:107) = h(cid:62)
+ and

+. Using (S7), with high proba-
(cid:101)v/(cid:107)(cid:101)v(cid:107). By (S7) and (S8), there exists

(cid:107)ΠC∗ (h)(cid:107) =

h(cid:62)(u + v∗)
(cid:107)u + v∗(cid:107)

.

Under the assumption that δ < mint1,t2∈R E[(h − t1y − t2yq)2

+],

1
√
n

(cid:107)ΠC∗ (h)(cid:107) >

1
√
n

(cid:107)g(cid:107)

with probability tending to one, which implies that φ(V, g, h) = 0 when v in (S4) is chosen to be v∗.
Therefore, we obtain

P (MLE does not exist) = P (Φ(V, Q) > 0) ≤ 2P (φ(V, g, h) > 0) → 0,

or equivalently P (MLE exists) → 1.

S4 Existence of the MPLE in Cox regression

Under the Cox model (1), the conditional distribution of Yi given Xi depends on Xi only through a
i β∗. By the rotational invariance of the Gaussian distribution, we can show that

linear combination X(cid:62)
the joint distribution of (Yi, X(cid:62)

i ) = (Yi, Xi1, . . . , Xip) is the same as that of

where yi = ti ∧ Ci with ti having the hazard function

(yi, q(cid:62)

i ) = (yi, qi1, . . . , qip),

and

λ(t|qi1) = λ0(t) exp(κqi1)

(qi2, . . . , qip) ∼ N (0, Ip−1),
(qi2, . . . , qip) ⊥ (yi, qi1),

19

i β∗). Thus we only need to study
for 1 ≤ i ≤ n. Here (yi, κqi1) has the same distribution as that of (Yi, X(cid:62)
the existence of the MPLE in the equivalent model. To this end, we consider the convex optimization
problem

max
−1≤b≤1

(cid:88)

(i,j)∈D

aijb(cid:62)(qi − qj)

subject to b(cid:62)(qi − qj) ≥ 0 for all (i, j) ∈ D,

where aij > 0 is ﬁxed throughout the arguments and b = (b1, . . . , bp)(cid:62). The MPLE exists if and
only if the optimal value of the above objective function is equal to zero. Deﬁne (cid:101)a = ((cid:101)a1, . . . , (cid:101)an)(cid:62)
with (cid:101)ai = (cid:80)n
i=1 aij∆i1{Yj ≥ Yi}. Let Q =
(q1, . . . , qn)(cid:62) = ((cid:101)q1, Q2) ∈ Rn×p, where (cid:101)q1 = (q11, . . . , qn1)(cid:62) ∈ Rn and Q2 ∈ Rn×(p−1). Note that

j=1 aij∆i1{Yj ≥ Yi} and ˘a = (˘a1, . . . , ˘an)(cid:62) with ˘aj = (cid:80)n

(cid:88)

(i,j)∈D

aijb(cid:62)(qi − qj) =

n
(cid:88)

n
(cid:88)

i=1

j=1

aij∆i1{Yj ≥ Yi}b(cid:62)(qi − qj) = ((cid:101)a − ˘a)(cid:62)

(cid:101)q1b1 + ((cid:101)a − ˘a)(cid:62)Q2b2,

(S9)

for b2 = (b2, . . . , bp)(cid:62). By introducing the Lagrangian {vij}(i,j)∈D and using (S9), we can rewrite the
optimization problem as

Φ((cid:101)q1, Q2) = max
−1≤b≤1

min
vij ≥0

(cid:88)

(i,j)∈D

aijb(cid:62)(qi − qj) +

(cid:88)

(i,j)∈D

vijb(cid:62)(qi − qj)

=

max
−1≤b1≤1,−1≤b2≤1

min
vij ≥0

= min
vij ≥0

max
−1≤b1≤1,−1≤b2≤1

((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)
((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)

(cid:101)q1b1 + ((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)Q2b2
(cid:101)q1b1 + ((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)Q2b2,

(S10)

where (cid:101)v and ˘v are deﬁned in a similar way as (cid:101)a and ˘a with aij replaced by vij. Here we switch the order
of the maximization and minimization as the objective function in (S10) is concave-convex. Conditional
on {yi, qi1, ∆i}n
i=1 and using the CGMT, we consider an asymptotically equivalent AO problem deﬁned
as

φ((cid:101)q1, g, h) = min
vij ≥0

max
−1≤b1≤1,−1≤b2≤1

((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)

(cid:101)q1b1 + (cid:107)(cid:101)a − ˘a + (cid:101)v − ˘v(cid:107)g(cid:62)b2

− (cid:107)b2(cid:107)h(cid:62)((cid:101)a − ˘a + (cid:101)v − ˘v),

(S11)

where g ∈ Rp−1 and h ∈ Rn both have i.i.d N (0, 1) components that are independent of other random
quantities. Taking maximization with respect to the directions of b1 and b2, the optimization problem
becomes

min
vij ≥0

max
√

(cid:107)b2(cid:107)≤

p−1

|((cid:101)a − ˘a + (cid:101)v − ˘v)(cid:62)

(cid:101)q1| + (cid:107)(cid:101)a − ˘a + (cid:101)v − ˘v(cid:107)(cid:107)g(cid:107)(cid:107)b2(cid:107) − (cid:107)b2(cid:107)h(cid:62)((cid:101)a − ˘a + (cid:101)v − ˘v).

Recall that

(cid:26)

M =

m = (m1, . . . , mn) ∈ Rn : min
s<il

ms ≥ mil , max
j∈Dil

mj ≤ mil , l = 1, . . . , k

,

(cid:27)

where Dil = {1 ≤ j < il : yj = yil , ∆j = 1}. Deﬁne the set

M∗ =






m∗ = (m∗

1, . . . , m∗

n) ∈ Rn : m∗

i =

n
(cid:88)

j=1

(cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj}) for some cij, cji ≥ 0






,

which will be shown to be the polar cone of M. Further let

C = span((cid:101)q1) + M = {t(cid:101)q1 + m : t ∈ R, m ∈ M} and C∗ = span⊥((cid:101)q1) ∩ M∗.
It is not hard to see that M∗ is a cone. Next we show that M is the polar cone of M∗. For any

20

m = (m1, ..., mn)(cid:62) in the polar cone of M∗ and m∗ ∈ M∗, we must have

(cid:104)m, m∗(cid:105) =

(cid:88)

mi (cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj})

=

1≤i(cid:54)=j≤n
(cid:88)

1≤i<j≤n

(mi − mj) (cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj}) ≤ 0,

for any cij ≥ 0. Set ckl = clk = 0 if {k, l} (cid:54)= {i, j}. We have

(mi − mj) (cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj}) ≤ 0.

We see that


mi ≤ mj

mi ≥ mj
mi = mj

no restriction

∆i1{yj ≥ yi} = 1, ∆j1{yi ≥ yj} = 0,
∆i1{yj ≥ yi} = 0, ∆j1{yi ≥ yj} = 1,
∆i1{yj ≥ yi} = 1, ∆j1{yi ≥ yj} = 1,
∆i1{yj ≥ yi} = 0, ∆j1{yi ≥ yj} = 0,

which implies that m ∈ M. On the other hand, it is not hard to see that for any m ∈ M and m∗ ∈ M∗,
(cid:104)m, m∗(cid:105) ≤ 0. Thus M is the polar cone of M∗. As M∗ is closed and convex, by Proposition S2.2, we
obtain that M∗ is the polar cone of M. As span⊥((cid:101)q1) is the polar cone of span((cid:101)q1), using Lemma S2.3,
we have that C∗ is the polar cone of C. Similar to the discussions for logistic regression, we consider two
cases.

Case 1: Suppose δ > hU (λ0, κ, PC). We show that P (MPLE does not exist) → 1. By the assump-

tion, we have

1
√
n

(cid:107)h − ΠC(h)(cid:107) =

1
√
n

(cid:107)ΠC∗ (h)(cid:107) <

1
√
n

(cid:107)g(cid:107)

(S12)

with probability tending to one. For the objective function in (S11) to be zero, we need to ﬁnd {v∗
such that

ij}(i,j)∈D

((cid:101)a − ˘a + (cid:101)v∗ − ˘v∗)(cid:62)

(cid:101)q1 = 0

and

h(cid:62)((cid:101)a − ˘a + (cid:101)v∗ − ˘v∗)
(cid:107)(cid:101)a − ˘a + (cid:101)v∗ − ˘v∗(cid:107)

≥ (cid:107)g(cid:107).

(S13)

The deﬁnitions of (cid:101)a, ˘a, (cid:101)v∗ and ˘v∗ imply that (cid:101)a − ˘a + (cid:101)v∗ − ˘v∗ ∈ M∗. Moreover, from the ﬁrst condition
in (S13), we have (cid:101)a − ˘a + (cid:101)v∗ − ˘v∗ ∈ span⊥((cid:101)q1) and thus (cid:101)a − ˘a + (cid:101)v − ˘v ∈ C∗. By (S12), we get

P

(cid:18) 1
√
n

(cid:107)g(cid:107) >

1
√
n

(cid:107)ΠC∗ (h)(cid:107) ≥

1
√
n

h(cid:62)((cid:101)a − ˘a + (cid:101)v∗ − ˘v∗)
(cid:107)(cid:101)a − ˘a + (cid:101)v∗ − ˘v∗(cid:107)

(cid:19)

→ 1.

Therefore, we must have

P (MLE exists) = P (Φ((cid:101)q1, Q) = 0) ≤ 2P (φ((cid:101)q1, g, h) = 0) → 0,

which implies that P (MLE does not exist) → 1.

Case 2: Assuming that δ < hL(λ0, κ, PC), we argue that P (MPLE exists) → 1. Let (cid:102)M∗ denote the

interior of M∗. We ﬁrst claim that

P (span⊥((cid:101)q1) ∩ (cid:102)M∗ (cid:54)= {0}) → 1.

(S14)

21

To see this, we note that for any m∗ ∈ (cid:102)M∗,

n
(cid:88)

n
(cid:88)

(cid:104)m∗, (cid:101)q1(cid:105) =

qi (cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj})

=

=

j=1

(qi − qj) (cij∆i1{yj ≥ yi} − cji∆j1{yi ≥ yj})

i=1
(cid:88)

j<i

(cid:88)

j<i,yi(cid:54)=yj

(qi − qj)cij∆i +

(cid:88)

j<i,yi=yj

(qi − qj) (cij∆i − cji∆j) .

We note that the event that all qi − qj with j < i, yi (cid:54)= yj and ∆i = 1 have the same sign happens with
exponentially small probability. As cij’s are arbitrarily positive, the ﬁrst term in the last equality can
be equal to any value including the negative of the second term with appropriate choice of cij’s. Hence,
with high probability, there exists m∗ ∈ (cid:102)M∗ such that (cid:104)m∗, (cid:101)q1(cid:105) = 0, which justiﬁes claim (S14). Also,
it is not hard to verify that for any a ∈ (cid:101)Rn
+,

(cid:26)

(cid:101)a − ˘a + (cid:101)v − ˘v
(cid:107)(cid:101)a − ˘a + (cid:101)v − ˘v(cid:107)

(cid:27)

(cid:26)

: v ≥ 0

=

(cid:101)v − ˘v
(cid:107)(cid:101)v − ˘v(cid:107)

: v ∈ (cid:101)Rn
+

(cid:27)

.

(S15)

With high probability, the projection of h onto C∗ is in (cid:102)M∗. Hence by (S14) and (S15), we can ﬁnd
v∗ ≥ 0 such that (cid:101)a − ˘a + (cid:101)v∗ − ˘v∗ ∈ span⊥((cid:101)q1), (cid:107)(cid:101)a − ˘a + (cid:101)v∗ − ˘v∗(cid:107) (cid:54)= 0, and

(cid:107)ΠC∗ (h)(cid:107) =

h(cid:62)((cid:101)a − ˘a + (cid:101)v∗ − ˘v∗)
(cid:107)(cid:101)a − ˘a + (cid:101)v∗ − ˘v∗(cid:107)

.

Under the assumption that δ < hL(λ0, κ, PC),

1
√
n

(cid:107)ΠC∗ (h)(cid:107) >

1
√
n

(cid:107)g(cid:107)

with probability tending to one. With the v∗ chosen above, φ((cid:101)q1, g, h) = 0. Therefore, we obtain

P (MLE does not exist) = P (Φ((cid:101)q1, Q) > 0) ≤ 2P (φ((cid:101)q1, g, h) > 0) → 0,

which implies that P (MLE exists) → 1.

Remark S4.1. Suppose the censoring time Ci depends on the covariate Xi ∼ N (0, Ip) through X(cid:62)
i θ
for some θ ∈ Rp, and Ci is conditionally independent of the survival time Ti given Xi. Let A be
an orthogonal matrix with ﬁrst row being (β∗/(cid:107)β∗(cid:107))(cid:62) and second row being θ(cid:62)P⊥/(cid:107)P⊥θ(cid:107), where
P = β∗β∗(cid:62)/(cid:107)β∗(cid:107)2. Let AXi = qi = (qi1, . . . , qip)(cid:62). We have X(cid:62)

i β∗ = (cid:107)β∗(cid:107)qi1 and

X(cid:62)

i θ = X(cid:62)

i P⊥θ + X(cid:62)

i Pθ = (cid:107)P⊥θ(cid:107)qi2 + X(cid:62)

i β∗θ(cid:62)β∗/(cid:107)β∗(cid:107)2 = (cid:107)P⊥θ(cid:107)qi2 + (cid:107)Pθ(cid:107)qi1.

Thus we have found the equivalent model:

PYi,Xi(y, x) = PYi|Xi(y|X(cid:62)

i β∗, X(cid:62)

i θ)PXi(x)

which can be expressed equivalently as

Pyi,qi(y, q) = Pyi|qi (y|qi1, (cid:107)P⊥θ(cid:107)qi2 + (cid:107)Pθ(cid:107)qi1)Pqi(q)

through the rotation A, where

qi = (qi1, . . . , qip)(cid:62) ∼ N (0, Ip),

(qi3, . . . , qip) ⊥ (yi, qi1, qi2).

Analogy to the case where Ci is independent of Xi, we can derive similar results by replacing (cid:101)q1, Q2 with
Q1 ∈ Rn×2 and Q2 ∈ Rn×(p−2). The argument is alike if Ci depends on multiple linear combinations of
Xi.

22

S5 Error analysis of the MPLE

Reformulating the PO

Let H =

√

p(X1, . . . , Xn)(cid:62) and κ = (cid:107)β∗(cid:107)/

√

in (2). We can express it as

p. Recall the deﬁnition of the partial log-likelihood L

where

L(β) =

1
n

∆(cid:62)u −

1
n

∆(cid:62) log (A exp(u)) ,

u =

1
√
p

Hβ,

∆ = (∆1, . . . , ∆n)(cid:62) and A = n−1(a1, . . . , an)(cid:62) with ai = (1{Y1 ≥ Yi}, . . . , 1{Yn ≥ Yi})(cid:62). By introduc-
ing a Lagrange multiplier v, we can write the optimization problem in (2) as a min-max optimization

min
β∈Rp,u∈Rn

max
v∈Rn

−

1
n

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u)) +

v(cid:62)
n

(cid:18)

u −

(cid:19)

Hβ

.

1
√
p

(S16)

The bilinear form v(cid:62)Hβ depends on ∆ and A. Deﬁne P = β∗β∗(cid:62)/(cid:107)β∗(cid:107)2 and P⊥ = I − P. We have

H = H1 + H2, H1 = HP, H2 = HP⊥.

With the above decomposition, (S16) can be rewritten as

min
β∈Rp,u∈Rn

max
v∈Rn

−

1
n

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u)) +

(cid:18)

u −

1
n

v(cid:62)

1
√
p

(cid:19)

H1β

−

1
√

p

n

v(cid:62)H2P⊥β.

We note that the objective function above is convex with respect to β and u and concave with respect
to v. Using the CGMT, we consider the AO problem deﬁned as

min
β∈Rp,u∈Rn

max
v∈Rn

−

−

1
n

n

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u)) +

1
n

v(cid:62)

(cid:18)

u −

(cid:19)

1
√
p

H1β

1
√

p

(v(cid:62)h(cid:107)P⊥β(cid:107) + (cid:107)v(cid:107)g(cid:62)P⊥β),

(S17)

where h ∈ Rn and g ∈ Rp both have i.i.d. standard normal entries that are independent with the other
random quantities.

Analysis of AO

Next we analyze the AO in (S17). The goal here is to turn the vector optimization problem into an
equivalent form of a scalar optimization problem. We ﬁrst perform the maximization with respect to the
direction of v. The terms that are related to v induce the following maximization with respect to v

max
v∈Rn

1
n

v(cid:62)

(cid:18)

u −

1
√
p

H1β −

(cid:19)

h(cid:107)P⊥β(cid:107)

1
√
p

−

1
√

p

n

(cid:107)v(cid:107)g(cid:62)P⊥β.

(S18)

The direction of the optimizer v∗ must satisfy that

v∗
(cid:107)v∗(cid:107)

=

Thus we can write (S18) as

u − 1√
(cid:13)
(cid:13)u − 1√
(cid:13)

p H1β − 1√
p H1β − 1√

p h(cid:107)P⊥β(cid:107)
(cid:13)
(cid:13)
p h(cid:107)P⊥β(cid:107)
(cid:13)

.

max
r≥0

r

(cid:18) 1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u −

1
√
p

H1β −

1
√
p

h(cid:107)P⊥β(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

1
√

p

n

g(cid:62)P⊥β

(cid:19)

,

(S19)

23

where r = (cid:107)v∗(cid:107). Plugging the above expression into (S17), we obtain

min
β∈Rp,u∈Rn

max
r≥0

−

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u))

1
n
(cid:18) 1
n

+ r

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u −

1
√
p

H1β −

1
√
p

(cid:13)
(cid:13)
h(cid:107)P⊥β(cid:107)
(cid:13)
(cid:13)

−

1
√

p

n

g(cid:62)P⊥β

(cid:19)

.

(S20)

As the original optimization problem is convex with respect to β and u and concave with respect to v,
in an asymptotic sense, we can ﬂip the maximization with the minimization. We consider the following
problem,

min
β∈Rp

= min
β∈Rp

1
n

1
n

= min

a∈R,b≥0

where

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
n

u −

u −

1
√
p

1
√
p

H1β −

1
√
p

(cid:13)
(cid:13)
h(cid:107)P⊥β(cid:107)
(cid:13)
(cid:13)

−

1
√

p

n

g(cid:62)P⊥β

Hβ∗a −

1
√
p

(cid:13)
(cid:13)
h(cid:107)P⊥β(cid:107)
(cid:13)
(cid:13)

−

1
√

p

g(cid:62)P⊥β
(cid:107)P⊥β(cid:107)

n

(cid:107)P⊥β(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u −

1
√
p

Hβ∗a −

1
√
p

hb

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

1
√

p

n

(cid:107)P⊥g(cid:107)b,

a =

β(cid:62)β∗
(cid:107)β∗(cid:107)2

and b = (cid:107)P⊥β(cid:107).

Plugging (S21) into (S20) yields that

min
a∈R,b≥0,u∈Rn

max
r≥0

−

1
n
(cid:18) 1
n

+ r

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u))

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u −

1
√
p

Hβ∗a −

1
√
p

hb

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

1
√

p

n

(cid:19)

(cid:107)P⊥g(cid:107)b

.

We notice that for any s0 > 0,

1
2v

+

vs2
0
2

min
v≥0

= s0.

Using this fact, we can reformulate (S22) as

min
a∈R,b,v≥0,u∈Rn

max
r≥0

−

1
n

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u)) +

+

rv
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u
n

−

1
√

p

n

Hβ∗a −

1
√

p

n

hb

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

r
2v

−

r
√

p

n

(cid:107)P⊥g(cid:107)b.

Replacing v, r and b by

√

nv,

√

nr and

√

pb respectively, we obtain

min
a∈R,b,v≥0,u∈Rn

max
r≥0

−

1
n

∆(cid:62)u +

1
n

∆(cid:62) log (A exp(u)) +

+

rv
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u
√
n

−

1
√
np

Hβ∗a −

1
√
n

Some algebra yields that

r
2v
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

−

hb

r
√
n

(cid:107)P⊥g(cid:107)b.

(S21)

(S22)

(S23)

(S24)

∆(cid:62)u +

rv
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u
√
n

−

1
√
np

Hβ∗a −

−

=

rv
2

1
n
(cid:13)
(cid:13)
(cid:13)
(cid:13)

u
√
n

−

1
√
np

Hβ∗a −

1
√
n

hb −

∆
√

rv

−

(cid:107)∆(cid:107)2
2nrv

−

∆(cid:62)Hβ∗a
√

n

p

−

∆(cid:62)hb
n

.

hb

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

1
√
n
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

n

24

As g ∈ Rp has i.i.d standard normal entries, we have

(cid:107)P⊥g(cid:107)
√
p

→p 1,

by the law of large numbers. Using Assumption A3, we have

→p 1 − E[S(C|κZ)],

∆
n
∆(cid:62)Hβ∗
√

p

n
∆(cid:62)h
n

→p 0.

=

∆(cid:62)Hβ∗
n(cid:107)β∗(cid:107)

(cid:107)β∗(cid:107)
√
p

→p −κE[S(C|κZ)Z],

Combining the above arguments leads to the following optimization problem

min
a∈R,b,v≥0,u∈Rn

max
r≥0

1
n

∆(cid:62) log (A exp(u)) +

r
2v

+

rv
2n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

u − κaq − bh −

∆
rv

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

−

1 − E[S(C|κZ)]
2rv

+ κaE[S(C|κZ)Z] − r

√

δb,

(S25)

where q = (q1, . . . , qn)(cid:62) = Hβ∗/(κ

√

p). Let

Gn(u) := ∆(cid:62) log (A exp(u)) =



∆i log



n
(cid:88)

i=1

1
n

n
(cid:88)

j=1



1{Yj ≥ Yi} exp(uj)



and ξ = (ξ1, . . . , ξn)(cid:62) = κaq + bh + ∆

rv . Deﬁne the Moreau envelope function

min
u∈Rn

Gn(u) +

rv
2

(cid:107)u − ξ(cid:107)2 = MGn

(cid:18)

ξ;

(cid:19)

.

1
rv

Then (S25) becomes

min
a∈R,b,v≥0

max
r≥0

(cid:18)

MGn

ξ;

1
n

(cid:19)

1
rv

+

r
2v

−

1 − E[S(C|κZ)]
2rv

+ κaE[S(C|κZ)Z] − r

√

δb.

(S26)

Analysis of the Moreau envelope function

Our goal here is to show that as n → ∞,

(cid:18)

MGn

ξ;

1
n

(cid:19)

1
rv

(cid:18)

→ M

κa, b,

(cid:19)

1
rv

for some limiting function M (·, ·, ·). To facilitate the derivations, we introduce some stochastic processes
that are useful in the survival analysis (Andersen and Gill, 1982). Consider an n-dimensional counting
process N(n)(t) = (N1(t), . . . , Nn(t)) for t ≥ 0, where Ni(t) counts the number of observed events for
the ith individual in the time interval [0, 1]. The sample paths of N1, . . . , Nn are step functions, zero
at t = 0, with jumps of size +1 only. Furthermore, no two components jump at the same time. Let
Yi(t) ∈ {0, 1} be a predictable at risk indicator process that can be constructed from data. Note that

25

Ni(t) is a counting process with the intensity process Yi(t) exp(X(cid:62)

i β∗)λ0(t). We can write

1
n

Gn(u) =

1
n

n
(cid:88)

i=1



∆i log



1
n

n
(cid:88)

j=1

1{Yj ≥ Yi} exp(uj)





=

=

(cid:90) 1

0

(cid:90) 1

0



log





log



1
n

1
n

n
(cid:88)

j=1

n
(cid:88)

j=1

Yj(s) exp(uj)


 d ¯Nn(s)

Yj(s) exp(uj)


 Rn(s; β∗)λ0(s)ds

where ¯Nn(t) = n−1 (cid:80)n

i=1 Ni(t) and

Rn(s; β∗) =

1
n

n
(cid:88)

i=1

Yi(s) exp(X(cid:62)

i β∗) =

1
n

n
(cid:88)

i=1

Yi(s) exp(κqi).

Thus we obtain

min
u∈Rn

1
n

Gn(u) +

rv
2n

(cid:107)u − ξ(cid:107)2

= min
u∈Rn

(cid:90) 1

0



log



1
n

n
(cid:88)

j=1



Yj(s) exp(uj)



1
n

n
(cid:88)

i=1

Yi(s) exp(κqi)λ0(s)ds

+

rv
2n

n
(cid:88)

j=1

(uj − ξj)2.

The ﬁrst order condition implies that at the optimal u∗ = (u∗

1, . . . , u∗
n)

rv(u∗

k − ξk) = −

(cid:90) 1

0

1
n

Yk(s) exp(u∗
k)
j=1 Yj(s) exp(u∗
j )

(cid:80)n

Rn(s; β∗)λ0(s)ds.

Squaring both sides, summing over k and scaling both sides by 1/n, we obtain

r2v2
n

n
(cid:88)

(u∗

k − ξk)2 =

k=1

(cid:90) 1

(cid:90) 1

0

0

1
n2

Under Assumption A4, we have

(cid:80)n

1
n
(cid:80)n

k=1 Yk(s)Yk(t) exp(2u∗
k)
i + u∗
j )

i,j=1 Yi(s)Yj(t) exp(u∗

Rn(s; β∗)Rn(t; β∗)λ0(s)λ0(t)dsdt.

r2v2
n

n
(cid:88)

(u∗

k − ξk)2 →p

k=1

(cid:90) 1

(cid:90) 1

0

0

S(s, t)
S(s)S(t)

R(s)R(t)dsdt,

and



log



(cid:90) 1

0

1
n

n
(cid:88)

j=1



Yj(s) exp(u∗
j )



1
n

n
(cid:88)

i=1

Yi(s) exp(κqi)λ0(s)ds →p

(cid:90) 1

0

log(S(s))R(s)ds.

Therefore, we get

(cid:18)

MGn

ξ;

1
n

(cid:19)

1
rv

→p

(cid:90) 1

0

log(S(s))R(s)ds +

1
2rv

(cid:90) 1

(cid:90) 1

0

0

S(s, t)
S(s)S(t)

R(s)R(t)dsdt.

26

Next we show how S(s, t) and S(t) depend on ξi. Note that

rv(u∗

k − ξk)
exp(u∗
k)

= −

(cid:90) 1

0

1
n

Yk(s)

(cid:80)n

j=1 Yj(s) exp(u∗
j )

Rn(s; β∗)λ0(s)ds →p −

(cid:90) 1

0

Yk(s)
S(s)

R(s)ds.

We can solve this nonlinear equation for u∗
satisﬁes the nonlinear equation

k in terms of rv, ξk and (cid:82) 1

0

Yk(s)
S(s) R(s)ds. Asymptotically, u∗

k

rv(u∗

k − ξk)
exp(u∗
k)

= −

(cid:90) 1

0

Yk(u)
S(u)

R(u)du.

u∗
k = log

(cid:26)

(cid:18)

K

ξk,

(cid:90) 1

0

Yk(u)
S(u)

(cid:19)(cid:27)

R(u)du, rv

.

We write the solution as

Then we have

1
n

n
(cid:88)

k=1

Yk(s) exp(u∗

k) =

1
n

n
(cid:88)

k=1

(cid:18)

Yk(s)K

ξk,

(cid:90) 1

0

Yk(u)
S(u)

(cid:19)

R(u)du, rv

.

Letting n → +∞, we have S(·) being the solution to the following equation

(cid:20)

(cid:18)

S(s) = E

Y (s)K

ξ,

(cid:90) 1

0

Y (u)
S(u)

(cid:19)(cid:21)

R(u)du, rv

.

Similarly, we have

1
n

n
(cid:88)

k=1

Yk(s)Yk(t) exp(2u∗

k) =

1
n

n
(cid:88)

k=1

Yk(s)Yk(t)K 2

(cid:90) 1

(cid:18)

ξk,

0

Y (u)
S(u)

(cid:19)

R(u)du, rv

which implies

S(s, t) = E

(cid:20)
Y (s)Y (t)K 2

(cid:18)

ξ,

(cid:90) 1

0

(cid:19)(cid:21)

R(u)du, rv

.

Y (u)
S(u)

Combining the above results, we have shown that

(cid:18)

M

κa, b,

(cid:19)

1
rv

=

(cid:90) 1

0

log(S(s))R(s)ds +

(cid:104)

(cid:90) 1

(cid:90) 1

E

0

0

1
2rv

where S(·) is the solution to the equation

Y (s)Y (t)K 2 (cid:16)

ξ, (cid:82) 1
0
S(s)S(t)

Y (u)
S(u) R(u)du, rv

(cid:17)(cid:105)

R(s)R(t)dsdt,

(cid:20)

(cid:18)

S(s) = E

Y (s)K

ξ,

(cid:90) 1

0

Y (u)
S(u)

(cid:19)(cid:21)

R(u)du, rv

with ξ = κaq + bh + ∆

rv and R(s) = λ0(s)E [Y (s) exp(κq)] .

Optimality conditions

Since the objective function is smooth, when the optimal values are all non-zero, they should satisfy

the ﬁrst order optimality condition. We derive the conditions for a, b, v and r for the problem

(cid:18)

min
a∈R,b,v≥0

max
r≥0

M

κa, b,

(cid:19)

1
rv

+

r
2v

−

1 − E[S(C|κZ)]
2rv

+ κaE[S(C|κZ)Z] − r

√

δb.

(S27)

27

separately below. Let

• Condition for a

• Condition for b

• Condition for v

• Condition for r

Mi(a1, a2, a3) =

∂M (a1, a2, a3)
∂ai

,

1 ≤ i ≤ 3.

(cid:18)

M1

κa, b,

(cid:19)

1
rv

+ E[S(C|κZ)Z] = 0.

(cid:18)

M2

κa, b,

(cid:19)

√

δ.

= r

1
rv

−

1
rv2 M3

(cid:18)

κa, b,

(cid:19)

−

1
rv

r
2v2 +

1 − E[S(C|κZ)]
2rv2

= 0.

−

1
r2v

(cid:18)

M3

κa, b,

(cid:19)

1
rv

+

1
2v

+

1 − E[S(C|κZ)]
2r2v

√

−

δb = 0.

The last two equations imply that

Therefore, we obtain the following set of equations

b =

1
√

.

δ

v

(cid:32)

M1

κa, b,

(cid:32)

M2

κa, b,

(cid:32)

M3

κa, b,

(cid:33)

δ

(cid:33)

δ

(cid:33)

δ

√

r
√

r
√

b

b

b

r

= −E[S(C|κZ)Z],

√

δ,

= r

= −

r2
2

+

1
2

(1 − E[S(C|κZ)]) .

Approximate solution
√

Recall that b

δ = 1/v. Consider the problem

min
a∈R,b≥0

max
r≥0

(cid:32)

MGn

ξ;

1
n

√

(cid:33)

δ

b

r

√

b
δ
2r

−

(1 − E[S(C|κZ)]) + κaE[S(C|κZ)Z] −

r

√

δb
2

,

(cid:32)

MGn

ξ;

√
b

δ

(cid:33)

r

= min
u∈Rn



∆i log



n
(cid:88)

i=1

1
n

n
(cid:88)

j=1



1{Yj ≥ Yi} exp(uj)

 +

r
√
2b

δ

(cid:107)u − ξ(cid:107)2 ,

ξ = κa

Hβ∗
(cid:107)β∗(cid:107)

+ bh +

√

δb∆
r

where

with

√

for H =
solution (a∗, b∗, r∗) to the set of nonlinear equations.

p(X1, . . . , Xn)(cid:62). We solve the above min-max problem numerically to obtain the approximate

28

Proof of Theorem 4.2

We provide a sketch of the proof. Inspecting the derivations in Section S5, we know that the scalar

quantity a results from the transformation

and the quantity b is related to β through

a =

β(cid:62)β∗
(cid:107)β∗(cid:107)2 ,

b =

(cid:107)P⊥β(cid:107)
√
p

.

Let (cid:98)βAO be the solution to the AO in (S17). As (S17) and (S27) are asymptotically equivalent, we have

(cid:62)
AOβ∗
(cid:98)β
(cid:107)β∗(cid:107)2 → a∗

and

(cid:107)P⊥ (cid:98)βAO(cid:107)
√
p

→ b∗.

Therefore, we have

=

(cid:107)(cid:98)βAO − β∗(cid:107)2
(cid:107)β∗(cid:107)2
(cid:107)(cid:98)βAO − a∗β∗(cid:107)2
p

Now consider the event

(cid:107)P(cid:98)βAO(cid:107)2 + (cid:107)P⊥ (cid:98)βAO(cid:107)2 − 2(cid:98)β

(cid:107)β∗(cid:107)2

(cid:62)
AOβ∗ + (cid:107)β∗(cid:107)2

→p (a∗ − 1)2 +

(b∗)2
κ2 ,

→p (b∗)2.

(cid:26)

S =

β ∈ Rp :

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)β − β∗(cid:107)2

(cid:107)β∗(cid:107)2 − (a∗ − 1)2 −

(b∗)2
κ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

≤ (cid:15)

for any (cid:15) > 0. We have P ((cid:98)βAO ∈ S) → 1. Using (iv) of Theorem S1.2, we have P ((cid:98)β ∈ S) → 1. The other
result can be proved similarly.

Proof of Theorem 4.3

From the analysis of the AO problem, we have P⊥ (cid:98)βAO/(cid:107)P⊥ (cid:98)βAO(cid:107) = P⊥g/(cid:107)P⊥g(cid:107). For any ﬁxed

d ∈ Rp with d(cid:62)β∗ = O(1) and (cid:107)d(cid:107)2 = O(1), we have

d(cid:62) (cid:98)βAO =d(cid:62)P(cid:98)βAO + d(cid:62)P⊥ (cid:98)βAO

(cid:62)
AOβ∗
(cid:107)β∗(cid:107)2 +

(cid:107)P⊥ (cid:98)βAO(cid:107)

=d(cid:62)β∗ (cid:98)β

d(cid:62)P⊥ (cid:98)βAO
(cid:107)P⊥ (cid:98)βAO(cid:107)
d(cid:62)P⊥g
(cid:107)P⊥g(cid:107)
=(a∗ + op(1))d(cid:62)β∗ + (b∗ + op(1))d(cid:62)P⊥g.

(cid:62)
AOβ∗
(cid:107)β∗(cid:107)2 +

=d(cid:62)β∗ (cid:98)β

(cid:107)P⊥ (cid:98)βAO(cid:107)

where the orders for the op(1) terms are uniform over all d with d(cid:62)β∗ = O(1) and (cid:107)d(cid:107)2 = O(1). Choosing
d to be the standard basis vector corresponding to any j ∈ S0 gives (cid:98)βAO,j = (b∗ + op(1))(P⊥g)j. Thus

Consider the event

1
|S0|

(cid:88)

j∈S0

AO,j = (b∗ + op(1))2 1
(cid:98)β2
|S0|

(cid:88)

j∈S0

(P⊥g)2

j →p (b∗)2.




β ∈ Rp :

S =



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
|S0|

(cid:88)

j∈S0

j − (b∗)2
β2






≤ (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

29

for any (cid:15) > 0. Using (iv) of Theorem S1.2, we have P ((cid:98)β ∈ S) → 1. Therefore,

1
|S0|

(cid:88)

j∈S0

j →p (b∗)2.
(cid:98)β2

Let (cid:98)βS0 = ( (cid:98)βj)j∈S0. Following similar arguments as in the proof of Theorem 3 in Sur and Cand`es (2019),
we know that for (cid:98)βS /(cid:107)(cid:98)βS0 (cid:107) has the same distribution as that of ZS /(cid:107)Z(cid:107), where Z = (Zj)j∈S0 ∈ R|S0|
has i.i.d N (0, 1) entries and ZS = (Zj)j∈S . As (cid:107)(cid:98)βS0(cid:107)/(cid:107)Z(cid:107) → b∗, we obtain (cid:98)βS /b∗ →d N (0, Il).

30

