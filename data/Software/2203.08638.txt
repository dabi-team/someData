Noname manuscript No.
(will be inserted by the editor)

Building Domain-Speciﬁc Machine Learning Workﬂows: A
Conceptual Framework for the State-of-the-Practice

Bentley James Oakes · Michalis Famelis · Houari Sahraoui

2
2
0
2

r
a

M
6
1

]
E
S
.
s
c
[

1
v
8
3
6
8
0
.
3
0
2
2
:
v
i
X
r
a

Abstract Domain experts are increasingly employing
machine learning to solve their domain-speciﬁc prob-
lems. This article presents six key challenges that a do-
main expert faces in transforming their problem into a
computational workﬂow, and then into an executable
implementation. These challenges arise out of our con-
ceptual framework which presents the “route” of op-
tions that a domain expert may choose to take while
developing their solution.

To ground our conceptual framework in the state-
of-the-practice, this article discusses a selection of avail-
able textual and graphical workﬂow systems and their
support for these six challenges. Case studies from the
literature in various domains are also examined to high-
light the tools used by the domain experts as well as
a classiﬁcation of the domain-speciﬁcity and machine
learning usage of their problem, workﬂow, and imple-
mentation.

The state-of-the-practice informs our discussion of
the six key challenges, where we identify which chal-
lenges are not suﬃciently addressed by available tools.
We also suggest possible research directions for soft-
ware engineering researchers to increase the automation
of these tools and disseminate best-practice techniques
between software engineering and various scientiﬁc do-
mains.

B. Oakes
DIRO, Universit´e de Montr´eal, Montreal, Canada
E-mail: bentley.oakes@umontreal.ca

M. Famelis
DIRO, Universit´e de Montr´eal, Montreal, Canada
E-mail: famelis@iro.umontreal.ca

H. Sahraoui
DIRO, Universit´e de Montr´eal, Montreal, Canada
E-mail: sahraouh@iro.umontreal.ca

1 Introduction

The past two decades have seen the learning algorithms,
especially deep learning, permeate throughout every
scientiﬁc, engineering, and business domain to enable
new techniques and solve complex challenges. One ex-
ample of many is the recent solving of the long-standing
protein folding problem, which focuses on how a pro-
tein will fold in three-dimensional space given it’s one-
dimensional representation [58].

The enormous power oﬀered by current machine
learning techniques is therefore of great interest to stake-
holders across all domains. However, utilising these tech-
niques often requires a user who is an expert in their
own domain to gain proﬁciency in not only the concepts
of machine learning but also the programming abilities
used to call the low-level libraries. This is undesirable as
the domain expert would like to reason about concepts
and terms that are in their own domain. For example,
the literature on domain-speciﬁc languages shows that
solving a problem in the problem domain is more eﬃ-
cient than solving it in the solution domain [59,110].

Recently, the rise of low-code platforms partially
addresses this issue by raising the level of abstraction
from writing code to interactively deﬁning procedures
and GUIs [14,11]. A domain expert (or “citizen devel-
oper”) is thus assisted to build applications or computa-
tions using an easy-to-use and easy-to-deploy interface.
The domain expert may also be able to select domain-
speciﬁc components which directly address concerns in
their domain such as IoT [53], or machine learning com-
ponents to simplify machine learning tasks.

Research Problem

In this article, we focus on this intersection of domain
experts, low-code solutions, and machine learning. Specif-

 
 
 
 
 
 
2

Bentley James Oakes et al.

ically, we are interested in the ﬂow-based [80] nature of
workﬂows, which are the typical presentation of com-
putations in low-code platforms and scientiﬁc comput-
ing frameworks. That is, these workﬂows are composed
of computational blocks arranged in a graph structure
with the edges denoting data or control dependencies.
We note that this representation perfectly matches with
the common notion of a machine learning pipeline where
data is ingested, cleaned, and trained upon to produce
a machine learning model.

The research question that naturally arises is thus:
Given a domain expert and a domain-speciﬁc (DS) prob-
lem, what are the tools and techniques needed such that
this problem can be: a) mapped to a machine learn-
ing (ML) representation, b) constructed as a workﬂow
which utilises ML techniques and libraries, and c) de-
ployed from that workﬂow into an executable implemen-
tation?

For instance, to address the ﬁrst sub-question, we
have identiﬁed that a domain-speciﬁc problem can be
mapped to a ML representation either formal reason-
ing or expert mapping. Huang et al. describe such an
expert mapping in the domain of genomics [52]. Their
work assists genomics researchers by mapping drug and
tissue structures from the domain into forms suitable
for ML, deﬁning the problem as a ML problem, and
recommending suitable ML techniques for solving the
ML problem.

Contributions

words, we attempt to organise a) the meaning of, and
b) tools and techniques such that a domain expert can:

– Map a DS problem to a form suitable for ML
– Obtain a solution workﬂow for a DS and/or ML

problem

– Experiment with ML tools and techniques within a

workﬂow

– Add DS knowledge to improve ML performance (e.g.,

feature engineering)

– Produce an implementation from a workﬂow which
is well-suited for a domain expert (in terms of scal-
ability, DS tooling, etc.)

– Extract a workﬂow from an existing implementation

(code, Jupyter notebook)

To ground the above framework and challenges, this
article discusses some state-of-the-practice tools and
techniques which address these questions. A selection
of case studies also provides recent examples of domain
experts utilising machine learning as framed in the con-
text of our framework. We also provide a detailed dis-
cussion of the beneﬁts of structuring the research prob-
lem into our conceptual framework, along with an ex-
amination of challenges and research directions.

Overall, our intention with this article is to provide
a starting point for those readers who are interested in
overcoming these challenges such that domain experts
can more eﬃciently utilise machine learning to solve
their problems. This involves cross-discipline eﬀorts to
apply best practices and insights across software engi-
neering, machine learning, and other scientiﬁc ﬁelds.

Our main contribution is a conceptual framework to
map out the tools and techniques for solving a DS
problem using a workﬂow which involves ML, where
that workﬂow is then deployed into an executable im-
plementation. This framework illustrates that there are
multiple choices or routes that a domain expert may
choose from to obtain an executable implementation
from their problem. For example, a domain expert may
wish to ﬁrst consult an expert mapping to determine
how the problem should be structured in a ML repre-
sentation, before they construct a workﬂow by manually
selecting suitable components from a repository.

These three layers are divided into regions, where
problems, workﬂows, and implementations are organ-
ised by the dimensions of domain-speciﬁcity and pres-
ence of machine learning. The choices of the domain
expert can then be seen as transformations between
diﬀerent regions of our conceptual framework. These
transformations are phrased as interesting software en-
gineering challenges which directly impact a domain
expert who wishes to use machine learning. In other

Article Structure

As our framework relates topics from diﬀerent disci-
plines, Section 2 provides background on the topics of
domain-speciﬁcity, machine learning, and workﬂows.

Our three-layer conceptual framework relating prob-
lems, solution workﬂows, and implementations is pre-
sented in an overview in Section 3. Section 4 discusses
each layer of this framework in turn including the trans-
formations within each layer. Section 5 then presents
the transformations between layers. These inter-layer
transformations involve turning problems into work-
ﬂows, and from workﬂows into an implementation.

Section 6 provides a brief selection of state-of-the-
practice tools which implement the transformations found
in our framework. Representative case studies are pre-
sented in Section 7 to detail how practitioners from var-
ious ﬁelds are employing these tools and executing these
transformations on their own problems.

Section 8 then uses the framework as a basis for a
discussion of the opportunities and challenges for im-

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

3

plementing and automating the transformations in the
framework. Section 9 presents our concluding thoughts.

2 Background

This section provides a brief background in three core
topics of this article: the concept of domain-speciﬁcity,
machine learning, and computational workﬂows.

2.1 Domain-Speciﬁcity

In this article, we discuss the idea of domain-speciﬁcity
as relating to the concepts of a particular domain. This
is relevant throughout our framework as a) a domain
expert had speciﬁc experience and insights into that
domain and is less familiar with concepts outside that
domain, b) the problem the domain expert has is (par-
tially) expressed using those domain concepts, and c)
the technical considerations for a solution such as com-
puting platforms and tools may be speciﬁc to that do-
main. In this article, domain-speciﬁcity is manifested as
a cross-cutting dimension of the framework as discussed
in Section 3.1. In particular, we specify that problems,
workﬂows, and implementations can be more or less
domain-speciﬁc.

For example, consider the domain of neuroscience
which is the study of the nervous system. Relevant con-
cepts to a neuroscientist include neurons, signals, be-
haviour, activation, brain hemispheres, neural circuits,
and brain damage, with more according to the sub-ﬁeld
of the neuroscientist. These concepts may be formalised
in an ontology 1, allowing for more precise or (semi-)
automated reasoning about the concepts and their con-
nections.

Along with these concepts, the data examined by
this expert is highly speciﬁc to the domain. In neuro-
science, this can include processing of functional Mag-
netic Resonance Imaging (fMRI) ﬁles requiring spe-
cialised techniques to handle motion correction (cor-
recting the movement of the subject within the scan-
ner) and smoothing of the data (to average out the noise
present in the measurement). Finally, the datasets, tools,
and computational platforms available to a neuroscien-
tist are highly domain-speciﬁc such as repositories of
mouse brain scans2.

As a domain expert is most knowledgeable about
concepts from their domains, we follow the approach of
domain-speciﬁc modelling in declaring that problems
should be solved at a high level of abstraction using

1 See https://github.com/SciCrunch/NIF-Ontology for

an example.

2 For example: https://neuinfo.org/.

domain-speciﬁc concepts [59,110]. That is, the domain
expert should use domain-speciﬁc languages instead of
general programming languages, and there should be a
layered approach when possible to hide technical and
implementation details. This approach has been shown
to lower the cognitive workload of learning new con-
cepts and resulting in increased productivity [110]. In
the context of this article, we are interested in improv-
ing support such that the domain expert can describe
their problem and workﬂow using domain-speciﬁc con-
cepts.

2.2 Machine Learning

Machine learning (ML) can be summarised as “pro-
gramming computers to optimise a performance crite-
rion using example data or past experience” [4]. That
is, based on a collection of data and experiences, ML
seeks to automatically create models capable of relat-
ing output for particular inputs without requiring a pro-
grammer to directly implement the steps for computing
such outputs. These deﬁnitions therefore capture many
applications ranging from interpolating based on a sim-
ple linear regression up to automated driving by using
visual data interpreted using neural networks.

One way of categorising ML approaches is into three
approaches: supervised learning, unsupervised learning,
and reinforcement learning. In supervised learning, the
supervisor provides inputs and labelled outputs, and
the technique must learn the mapping between inputs
and outputs. An example would be to train a linear re-
gression of variables, producing a classiﬁer which can
predict whether a tissue sample is a tumor or not [39],
or constructing a similarity function such that related
objects can be found. In unsupervised learning, the tech-
nique learns without provided labels. This can oﬀer in-
sights into the structure of the domain such as uncover-
ing related clusters of data or outliers. In reinforcement
learning, the intelligent agent is rewarded with a de-
ﬁned reward function. This allows the agent to explore
possible actions and receive automated feedback.

A ML model must be trained before it can be used
for reasoning. For example, consider the scenario of
training a model for a “line-of-best-ﬁt” (linear regres-
sion). Once the data points have been loaded and any
cleaning necessary performed, the data is then com-
monly divided into training and test sets. This allows
for an unbiased measure of error when testing, as the
model has not “seen” the testing data beforehand. The
linear regression is then learned by a suitable algorithm,
and the linear regression is the produced ML model
ready for use. When this model is used it is fed a new

4

Bentley James Oakes et al.

piece of data as input, and it will predict a particular
output.

Many libraries are available which implement some
form of ML. For example, scikit-learn3 is a popular
Python module which oﬀers a high-level API for ML
techniques, while Keras4 provides an API for directly
creating neural networks by constructing each layer into
a model. ML techniques are also available directly within
academic and scientiﬁc tools such as the MATLAB Statis-
tics and Machine Learning toolbox5.

2.3 Computational Workﬂows

General workﬂows have existed for many years, espe-
cially in the manufacturing domain. In this article, we
focus on computational workﬂows which deﬁne the steps
that a computational device follows to produce results.
In particular, we are interested in workﬂows which
represent ﬂow-based programming by containing dis-
crete steps representing computational steps [80]. This
can be represented by a directed-acyclic graph (DAG)
of computational nodes connected by control and data
dependencies. Note that in current workﬂow systems
(Section 6) this restriction on acyclic graphs is relaxed,
as tools may wish to encode control ﬂow structures such
as loops over input ﬁles.

This concept of data “ﬂowing” through a workﬂow
is quite a natural structure for many computations we
examine in this article. Indeed, some domains may lean
into the plumbing metaphor and refer to the workﬂow
as a “pipeline”. Lamprecht et al. state that, “Another
common, more diﬀerentiating view is that pipelines are
purely computational and as such a subset of the more
general notion of workﬂows, which can also involve a
human element” [69]. To clarify the terminology in this
article, we only discuss computational workﬂows. That
is, the term ‘workﬂow’ in this article do not contain hu-
man elements and can therefore be equated to pipelines.

Scientiﬁc Workﬂow Management

The large scale of scientiﬁc data and the reproducibility
requirements of scientiﬁc processes have encouraged the
growth of workﬂow management systems within vari-
ous scientiﬁc domains. For example, ensuring that data
sources and computations follow the well-known prin-
ciples of Findable, Accessible, Interoperable, Reusable
(FAIR) [114] demands a comprehensive management

3 https://scikit-learn.org
4 https://keras.io
5 https://www.mathworks.com/products/statistics.

html

system. The life sciences in particular have a rich his-
tory of workﬂow systems [73,115, 88, 69], a few of which
are discussed in our sections on tools (Section 6) and
case studies (Section 7).

As workﬂows explicitly declare the sequence of com-
putations they perform, they assist in the production
of reproducible results [105,115]. That is, they assist
to reproduce the results of another experiment [54].
For example, the discrete nature of workﬂow compo-
nents means that components can be tagged with prove-
nance information [94] and placed into containers such
as Docker containers 6. This allows for easily accessi-
ble, self-contained units which can be accessed through
repositories and placed into dependency management
systems [26].

Many factors can inﬂuence whether a computational
process is reproducible, and workﬂows are only one step
towards full reproducibility. For example, Digan et al.
discuss 40 reproducibility features sourced from rec-
ommendations and clinical usages of workﬂows in the
natural language processing (NLP) domain [28]. Mora-
Cantallops et al. discuss reproduciblity in the context
of artiﬁcal intelligence/ML [79].

Machine Learning Pipelines

In ML, the term pipeline is commonly used to denote
the steps involved in the training and reasoning pro-
cesses. For example, data can be involved in a linear
ﬂow of loading, ﬁltering, cleaning, splitting (into a train-
ing and testing set), and trained upon.

A similar linear ﬂow is also present for the neural
networks used in deep learning. The input data passes
through layers in this network which recognise patterns
in the input data, store relationships in the weights be-
tween nodes in inner layers, and then produce output
values or categories.

Thus, ML pipelines are a one-to-one match to the
workﬂow formalism we examine in this article. This uni-
ﬁcation of structure is important when we discuss work-
ﬂows which involve components from both a particular
scientiﬁc domain as well as ML components.

3 Overview of Our Framework

This section will provide an overview of our conceptual
framework by discussing its structure, two dimensions,
and relating the framework to our challenge questions
introduced in Section 1.

6 https://www.docker.com

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

5

Fig. 1 Our three-layer framework with the problem, solution workﬂow, and implementation spaces.

As a summary, our conceptual framework maps out
the options for a domain expert to choose from to de-
velop a workﬂow-based solution to their problem using
machine learning, and ultimately obtain a usable im-
plementation. The framework is seen in Figure 1, sepa-
rated into twelve regions through the division of three
layers and two dimensions. The three horizontal lay-
ers deﬁne the problem space, workﬂow solution space,
and the implementation space. The problem space layer
contains the problem of the domain expert. For exam-
ple, a neuroscientist may wish to classify brain scan
data as belonging to a depressed patient or not. The
solution workﬂow space layer contains workﬂows which
solve the problems from the upper layer. The implemen-
tation space contains the implementation details for the
workﬂows in the middle layer. Section 4 addresses these
layers in detail.

Each layer is decomposed into four regions, deﬁned
by the domain-speciﬁc and machine learning dimen-
sions. Section 4 also discusses the mappings and trans-
formations between the regions in a layer. For exam-
ple, one transformation would map a problem in the
domain-speciﬁc problem space (DSP) to the machine
learning problem space (MLP). This represents all tech-
niques to transform the domain-speciﬁc problem to a
machine learning problem such as an expert mapping [52].
Section 5 will then discuss the mappings and trans-
formations between layers. That is, from a problem to
a workﬂow, and then from a workﬂow to an implemen-
tation.

3.1 Dimensions of the Space

This framework deﬁnes two dimensions for each layer.
First is the notion of domain speciﬁcity which captures
how many domain-speciﬁc concepts an artefact con-

tains. Second, the machine learning dimension captures
how many machine learning (ML) concepts the artefact
contains.

For example, consider an artefact from the ﬁrst layer:
a problem to be solved. This problem could be very
generic such as “ﬁnding the clusters for a table of data”.
Adding domain speciﬁcity comes from adding domain
knowledge to the problem, such as clustering related
genes or molecules which may change the solutions avail-
able for the problem. Similarly, ML concepts such as
clustering, unsupervised/supervised learning, or convo-
lutional ﬁlters may be present in the problem descrip-
tion or not, aﬀecting how far the problem is along the
machine learning dimension.

In the problem space layer, the problems are cate-
gorised based on the domain-speciﬁc or ML concepts
present. For the solution workﬂow layer, this categori-
sation depends on the proportion of components in the
workﬂow. In the implementation space layer, this cate-
gorisation depends on the use of domain-speciﬁc or ML
APIs or libraries. Section 4 further discusses how the
dimensions divide up each layer in the framework.

Note that by necessity, these categorisations are fuzzy
and we intentionally do not provide boundaries to these
regions. Instead, we provide these dimensions to pro-
voke reﬂection about the mappings and transformations
along these dimensions. That is, what does it mean
to make a problem, workﬂow, or implementation more
domain-speciﬁc or involve more machine learning, and
what are the techniques to do so?

Let us also note that these two dimensions are cer-
tainly not orthogonal. Speciﬁcally, machine learning is
itself a domain of interest and these two dimensions
may overlap signiﬁcantly depending on the problem of
interest. Therefore, let us state here that when this ar-

6

Bentley James Oakes et al.

ticle refers to domains or domain experts, we refer to a
non-machine learning domain.

3.2 Relation to Software Engineering Challenges

As expressed in our framework, the domain expert wishes
to transform their problem from their domain (the DSP
region on the top layer in Figure 1) all the way into an
implementation using machine learning libraries (the
BI or MLI regions on the bottom layer). Through these
transformations, the domain expert is able to move
around through diﬀerent regions as shown by the case
studies in Section 7, though support may be lacking for
some operations in various tools as discussed in Sec-
tion 8. In particular, we are interested in determining
tool support for transformations enabling the most di-
rect route from the domain-speciﬁc problem (DSP) to a
blended workﬂow (BW) down to a blended implemen-
tation (BI).

Here we recall the challenge questions from Section 1
and relate them to speciﬁc transformations. Note that
other transformations are indeed possible and may be
relevant to a domain expert. However, we have chosen
these transformations to focus on as they seem most
relevant to the tools and case studies discussed in this
article.

Mapping a DS problem to a form suitable for ML : This
challenge refers to how domain-speciﬁc problems in the
problem space can be mapped or transformed to include
more ML concepts. That is, moving the problems along
the ML dimension in the problem space.

Providing a solution workﬂow for a DS and/or ML
problem : This challenge refers to the mapping between
a problem in the problem space and a workﬂow in the
solution workﬂow space. That is, moving from the top
to the middle layer in the workﬂow.

Allowing the domain expert to experiment with appro-
priate ML components in a DS workﬂow : This chal-
lenge refers to moving solution workﬂows on the middle
layer along the ML dimension through the integration
of more ML components.

Adding DS knowledge to improve ML performance :
This challenge refers to moving solution workﬂows on
the middle layer along the DS dimension by adding new
DS information or components.

Producing an implementation from a workﬂow which is
well-suited for a domain expert : This transformation is
between the middle and bottom layers, as the workﬂow
of the domain expert is mapped or transformed to an
executable implementation.

Extracting a workﬂow from an existing implementation
(code or notebook : This challenge is an inverse to the
previous one, as the implementation on the bottom
layer of the framework is instead transformed into a
solution workﬂow on the middle layer.

4 Layers and Intra-Layer Transformations

This section details the three layers of our framework
as shown in Figure 3: the problem space, the solution
workﬂow space, and the implementation space. The re-
gions and relevant transformations within each layer
are then presented.

4.1 Problem Space

The problem space is where the problem of the domain
expert is formulated. For example, the running example
presented in Section 1 can be expressed in two ways. In
the domain-speciﬁc space (DSP), the problem is to pre-
dict whether a drug will work well with a sample of tis-
sue [52]. As a machine learning representation (MLP),
this problem becomes a numerical prediction for eﬃ-
cacy of the graph structure (for the drug) versus a lin-
ear string of characters (for the genes present in the
tissue). A blended version of this problem (BP) is vi-
sualised in Figure 2 where the structures for the drug
and gene are fed into ML encoders and used to make
a numerical prediction related to how the drug aﬀects
the gene.

Fig. 2 The Drug Response Prediction problem from Huang
et al. [52].

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

7

4.1.1 Artefact Representation

The artefact for this layer is a problem composed of con-
cepts. This problem may be speciﬁed in multiple ways
ranging from a simple informal statement to a com-
plex formal representation. For example, the problem
could be speciﬁed as natural language research ques-
tions, in an ontological manner, in a textual or graph-
ical domain-speciﬁc language (DSL), or in a template-
based, requirement-like manner.

4.1.2 Layer Regions

In our framework, we deﬁne four regions for the prob-
lem space: General Problems (GP), Domain-Speciﬁc
Problems (DSP), Machine Learning Problems (MLP),
and Blended Problems (BP) as shown in Figure 3. These
regions are deﬁned by the domain-speciﬁcity and ma-
chine learning dimensions as discussed in Section 3.1.

Fig. 3 A representation of the problem space, with intra-
layer transformations.

A general problem is one which does not refer to
any domain-speciﬁc concepts, nor does it refer to any
machine learning concepts. This region is provided for
completeness as this article focuses on the other three
regions.

Domain-speciﬁc problems are those that are spec-
iﬁed in the domain of interest by experts. They in-
volve concepts which are specialised to that domain.
In this article, the domain-speciﬁc problem is the start-
ing point from which the expert wishes to solve with a
workﬂow and ﬁnally an executable implementation.

A machine learning problem is a problem which in-
volves a high proportion of machine learning concepts.
For example, learning how to classify objects by ingest-
ing data from a table. The machine learning problem
may also delve into high-level workﬂow steps, such as
referring to concepts such as convolutional layers or op-
timising of meta-parameters in training.

A blended problem is one that contains both domain-
speciﬁc and machine learning concepts. For example,
the problem represented in Figure 2 contains both DS
and ML concepts. This type of problem may be ideal for
the domain expert to begin with instead of a domain-
speciﬁc one, as the ML concepts can be directly concep-
tualised and/or operationalised in the workﬂow layer.

However, it already implies that the practitioners has
access to both domain and machine learning knowledge,
and has studied the problem from both domains.

4.1.3 Layer Transformations

As discussed in Section 3.1, it is interesting to reason
about how to transform an artefact on each layer to
make it more domain speciﬁc or involve more machine
learning components. In other words, to make these
problems more DS/ML speciﬁc and less general.

To increase domain speciﬁcity, a domain expert will
have to encode more domain knowledge into the prob-
lem. This could be in the form of an ontology represent-
ing formal knowledge, or by directly specifying features
of interest in the domain and their interaction with
other features. For example, consider the drug molecule
and gene running example. As this molecule is a chem-
ical structure, this restricts the possible forms it may
take, which may make the solving of this problem eas-
ier or enable the use of domain-speciﬁc tools for the
workﬂow.

Increasing the proportion of ML concepts is similar.
A ML domain expert will identify structures and tech-
niques from the ML domain to cast the problem into.
This may be to specify the technique used for classiﬁca-
tion or learning, or the representation that the problem
should take. This is the transformation which repre-
sents our challenge question: mapping a DS problem to
a form suitable for ML.

Direct Mapping Transformations are also possible di-
rectly between the domain-speciﬁc and machine learn-
ing problem regions. This transformation is how to take
a problem which exists in the domain and map it to a
problem in the machine learning space, or to create a
many-to-many mapping. This opens up further possi-
bilities for insights and implementation.

Here we identify two possible techniques for map-
ping: expert mapping and ontological/formal mapping.
In expert mapping, a group of experts from both the
domain of interest and the machine learning domain
issue recommendations about how to map the prob-
lem. This is shown by Huang et al. where problems in
genomics are mapped to machine learning representa-
tions [52]. This is a high eﬀort technique, but could
greatly assist with workﬂow and implementation cre-
ation.

This expert mapping is of course possible in other
domains. Aneja et al. provide a table mapping the prob-
lems addressed in the neuro-oncology literature with
the ML approach used in those papers [6]. Jablonka et

8

Bentley James Oakes et al.

al. provide another detailed review with mention of how
problems in material science were mapped to ML [55].
Another technique is to perform formal or ontologi-
cal reasoning. In this technique, the two domains would
be modelled and a reasoner would perform the map-
ping. An example would be performing ontological rea-
soning. This may oﬀer more potential for automation.
However, the domains themselves would have to be ex-
plicitly modelled [100].

Reversing the Transformation In principle, these trans-
formations could be reversed. That is, a problem could
be made more general with the same techniques, and a
machine learning problem could be mapped to a domain-
speciﬁc one. Improving the generalisation and mapping
possibilities for problems would increase the workﬂow
and implementations options available to practitioners.

4.2 Solution Workﬂow Space

In the second layer of our framework, we deﬁne a space
of solution workﬂows. These workﬂows detail the ac-
tions required to solve the problem deﬁned on the upper
layer.

4.2.1 Artefact Representation

The representation of these workﬂows is based on those
described in Section 2.3 with some available standards
presented in Section 6.1. The core aspect is that these
workﬂows consist of a directed sequence of components
with explicit control and/or data ﬂow. These compo-
nents represent a step or action in the workﬂow, and
they can be typed to enforce structure on the accepted
inputs and outputs and deﬁne their semantics.

Figure 4 shows a few components from a workﬂow
deﬁned in the Galaxy workﬂow management tool [57].
Galaxy is further discussed in both Section 6.2.2 and
Section 7.3.3.

The idea with this representation is to have a uni-
ﬁed, graph-like structure amenable for modularity, re-
use, modiﬁcation, and sharing. As mentioned in Sec-
tion 2.3, workﬂows are already common in scientiﬁc
domains and structured “pipelines” are in place in the
data science/machine learning world. Therefore our con-
ceptual framework solely focuses on graph-like work-
ﬂows as the representation for this middle layer.

4.2.2 Layer Regions

In this space, we again roughly classify workﬂows into
four regions as shown in Figure 5.

A general workﬂow is one where there are very few
domain-speciﬁc or machine learning components within
the workﬂow. Thus it is a catch-all category comprising
those workﬂows not discussed below.

In our framework, a domain-speciﬁc workﬂow is one
that has many components from the domain of inter-
est. These components must address a domain-speciﬁc
concept which is not of interest to a general user out-
side of that domain. Examples include loading a ﬁle
or database with a domain-accepted structure, a com-
putation performing a speciﬁc task of interest to the
domain, or communication with domain entities such
as robotic arms.

We also consider a rough spectrum where some work-
ﬂows have more domain-speciﬁc components than oth-
ers. For example, one workﬂow may simply load data
from an EEG ﬁle and visualise it, while another may
perform ﬁltering or spectrum analysis on the data. This
second workﬂow is thus more domain-speciﬁc than the
ﬁrst.

Similar to our category of domain-speciﬁc workﬂows,
a machine learning workﬂow is one that contains ML
components. Again, we state that there is a rough spec-
trum of these workﬂows from not including machine
learning components to heavily relying on these com-
ponents.

The blended workﬂow is one that contains numerous
domain-speciﬁc and machine learning components. As
well, a component itself may be labelled as blended, as
it may address a domain speciﬁc concern but heavily
utilise machine learning “within” the execution of the
component. We propose that this blended type of work-
ﬂow is desired for a domain expert to arrive at, since
the presence of these components should raise the level
of abstraction and increase the modularity and reuse
of the workﬂow. However, it may be preferable for a
domain expert not familiar with ML to begin working
with a DS workﬂow, and have the workﬂow ‘adjusted’
to become more blended over time as they gain familiar-
ity with ML concepts and components. This oﬀers the
domain expert further experimentation, customisation,
and optimisation possibilities.

4.2.3 Layer Transformations

We have posed two interesting questions in the intro-
duction of this article of how to transform a workﬂow
along these two dimensions. That is, to identify the
techniques to: a) increase the domain-speciﬁcity, and/or
b) usage of machine learning in a workﬂow. These cor-
respond to our challenge questions of: a) adding DS
knowledge to improve ML performance, and b) exper-

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

9

Fig. 4 A selection of a workﬂow described in [39], in the workﬂow management tool Galaxy [57].

Fig. 5 A representation of the solution workﬂow space, with
intra-layer transformations.

imenting with ML tools and techniques within a work-
ﬂow.

A ﬁrst approach is to ask a domain or machine learn-
ing expert to study the workﬂow and identify where
components should be added or improved. Their knowl-
edge could then be modelled and implemented for au-
tomated approaches to detect and suggest components
for use, such as that implemented by Kumar et al. [65].
These recommender systems could therefore shift the
workﬂow along the domain-speciﬁc or machine learn-
ing dimensions.

For example, a workﬂow may load genomic data
from a table provided in a spreadsheet for further pro-
cessing. Depending on the requirements of the user, the
loading components may be better replaced with a com-
ponent which is able to download up-to-date genomic
data directly from a cloud repository as is available in
tools like Galaxy.

Domain-speciﬁc workﬂows can also be utilised as
a sub-workﬂow. For example, Sections 6.2.1 and 7.3.1
discuss the fMRIPrep workﬂow which is for performing
speciﬁc pre-processing for neuroscience data. Thus, a
domain expert’s workﬂow becomes more DS when fM-
RIPrep is used.

from the old to reduce the error of machine learning
algorithms such as deep learning [12]. For example, Fan
et al. study the problem of marking reported software
bugs as ‘valid’ or ‘invalid’ [37]. From the bug data, they
extract new features such as recent number of bugs by
reporter, does the bug have a code patch attached, and
bug text readability scores.

Reversing the Transformation Just as with the layer
transformations for the problem space (Section 4.1.3),
these transformations could indeed be reversed to in-
crease the generality of the workﬂows. That is, to make
a workﬂow less DS and involve fewer ML components.
This may help to increase the applicability of the work-
ﬂow across domains, however we do not consider it fur-
ther.

4.3 Implementation Space

Similar to the above layers, the implementation space is
also a rough categorisation of possibilities. In our con-
ceptual framework, the implementation space deﬁnes
the low-level result which runs on a computational de-
vice, such as produced code or the execution of a work-
ﬂow engine. For example, some workﬂow management
tools are directly implemented in Python, or the Orange
editor (Section 6) can directly execute the workﬂow in-
side the tool.

4.3.1 Artefact Representation

The error of ML techniques may also be lowered
when DS information is used. One aspect of this is the
ﬁeld of feature engineering where new data is extracted

In this space, the artefacts are represented as code or
another machine executable format. This may be Python
code which contains calls to ML APIs, or may be the

10

Bentley James Oakes et al.

machine code executed by a workﬂow engine which is
directly interpreting and executing the instructions within
a workﬂow.

These two examples are not at the same level of
abstraction. However, the intention with this represen-
tation is that: a) this code directly calls upon domain-
speciﬁc and machine learning APIs, and b) this code
should not be written directly by a domain expert as
the level of abstraction is too low.

Fig. 6 A representation of the implementation space, with
intra-layer transformations.

4.3.2 Layer Regions

Similar to the above layers, the implementation space
layer is deﬁned by the same two dimensions. The domain-
speciﬁc dimension deﬁnes the proportion of the code
which calls upon domain-speciﬁc APIs or libraries. The
machine learning dimension deﬁnes the proportion for
machine learning APIs or libraries.

A general implementation contains a small propor-
tion of calls to DS or ML APIs or libraries. Thus it is
general code.

In a domain-speciﬁc implementation the code makes
calls into an API or library which provides domain-
speciﬁc computation. For example, the nipype library
(Section 6) oﬀers a neuroscience-speciﬁc Python library.
Thus the more calls to libraries like these, the more
domain-speciﬁc the implementation.

Likewise, a machine learning implementation has
a high proportion of machine learning API or library
calls. An example would be directly calling Tensorﬂow
or other ML library from Python.

A blended implementation is one which uses both
domain-speciﬁc and machine learning APIs and libraries.
Thus the ML learning libraries are wrapped in a domain-
speciﬁc interface, and potentially optimised for each
domain-speciﬁc task. Clearly it would be desirable for
the domain expert to produce this form of implementa-
tion to achieve the most speciﬁc implementation. How-
ever, the domain expert should not write the implemen-
tation by hand, and instead generate an implementa-
tion from their workﬂow.

4.3.3 Layer Transformations

Again, the same transformations as the solution work-
ﬂow layer occur in this layer. Code can be mapped
manually or automatically to either a domain-speciﬁc
library call or a machine learning one.

These tasks could be useful for addressing legacy
code and updating it (called “cogniﬁcation”). However,
as argued in Section 8, it is not be eﬃcient to focus on
mapping and recommendations at the implementation
level. Instead, a more eﬃcient approach would be to
extract the workﬂow from the code and apply analyses
at the workﬂow level.

5 Inter-layer Transformations

This section deﬁnes the possible transformations used
to transform an artefact in one of the layers in our
framework to another layer. Section 6 then provides
examples of state-of-the-practice tools which support
these transformations.

The transformations discussed here are: a) from the
problem space to the solution workﬂow space, and b)
from the solution workﬂow space to the implementa-
tion space. Note that transforming a problem from the
problem space to the implementation space is classical
programming, which we do not elaborate further in this
section but appears in case studies in Section 7.

5.1 Problem Space to Solution Workﬂow Space
Transformations

The transformation between the ﬁrst two layers of our
framework transforms problems from the problem space
into workﬂows in the workﬂow space. This transforma-
tion corresponds to our challenge question of obtaining
a solution workﬂow for a DS and/or ML problem. Prac-
tically, this transformation is most likely a combination
of a domain expert building and/or ﬁnding workﬂows
and workﬂow components.

That is, a domain expert could: a) ﬁnd an exist-
ing workﬂow or components in a repository, b) rely on
formal or informal mappings or recommendations to as-
semble a workﬂow, or c) build the workﬂow themselves
either from a component library.

Here we will summarise a few of the techniques
available in the tools from Section 6 to assist a domain
expert in obtaining or building a workﬂow.

Component Libraries Many of the graphical workﬂow
tools (see Section 6.2.2) use a library of components for
the domain expert to select from when building their

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

11

workﬂow. Plugins or extensions can extend this compo-
nent palette with domain-speciﬁc components, allowing
for easier selection of these components. For example,
the Galaxy tool oﬀers workﬂow components which di-
rectly obtain data from biology databases. This aids
biologists to eﬃciently obtain up-to-date data directly
into their workﬂows.

Domain-speciﬁc Tutorials and Sample Workﬂows The
documentation surrounding a workﬂow tool often pro-
vides numerous examples for using the tool and for solv-
ing real-world problems. For example, the Nipype tool
discussed in Section 6.2.1 oﬀers over 30 neuroscience-
speciﬁc examples on its website. This allows users to get
started quickly to solve their domain-speciﬁc problems.

Workﬂow Repositories Some of the workﬂow tools dis-
cussed in Section 6 have explicit repositories for search-
ing and obtaining workﬂows, or have multiple tuto-
rial workﬂows for demonstrating the use of their com-
ponents. These repositories allow for experts to select
whole workﬂows and their component parts for use in
solving their problem.

However, a brief glance at these repositories sug-
gests usability issues. For example, one Galaxy work-
ﬂow repository7 contains hundreds of workﬂows, yet
the only search options available cover free text, user
rating, and keyword search. This may make it quite dif-
ﬁcult for a domain expert to ﬁnd a suitable workﬂow
unless techniques are used to further recover semantic
information [27]. Reproducibility issues may also ham-
per this search as computations may not be consistent
between runs or user machines [105].

Automated Recommendations Recent work by Wen et
al. suggests that it may be possible to automatically
determine similarity of workﬂows in repositories [113].
This could allow for enhanced discoverability of work-
ﬂows.

Workﬂow tools can also recommend next compo-
nents to be placed automatically. This allows for do-
main experts to be assisted by the tool to build their
workﬂow. For example, the excellent article of Kumar
et al. presents a recommendation engine for the Galaxy
framework [65]. This engine integrates into Galaxy it-
self to provide component recommendations based on
the existing components and the recent usage of that
component in the data set. Figure 7 shows the tool pro-
viding a list of recommended next components for the
user to select from.

7 https://usegalaxy.eu/workflows/list_published

Fig. 7 Automated recommendation of a component. Modi-
ﬁed from [65].

Automated Creation An interesting technique to create
the whole workﬂow at once is to use machine learning
techniques themselves to create workﬂows. This is the
ﬁeld of AutoML [51], which uses accuracy metrics to
create a machine learning pipeline for the domain ex-
pert’s data. A partial or full workﬂow for the expert can
also be provided based on their problem and/or their
data [45,44]. As a recent example of this AutoML ap-
proach, we point to Dunn et al. who use a benchmarking
set in materials science as the basis for creating material
science-speciﬁc workﬂows [29].

5.1.1 Region Transformation

Figure 8 diagrams some of the possible transformations
between regions on the problem space layer into the so-
lution workﬂow layer. That is, a domain-speciﬁc prob-
lem (DSP) could be solved with a machine learning
workﬂow (MLW). As indicated in the ﬁgure with bolded
arrows, the two preferable transformations are to take
the original domain-speciﬁc problem and directly cre-
ate a domain-speciﬁc or blended workﬂow. This retains
the domain-speciﬁc nature of the problem and adds the
required machine learning components.

Fig. 8 Transformations between the problem space (inner
boxes) and the solution workﬂow space (outer boxes).

12

Bentley James Oakes et al.

5.1.2 Reversing the Transformation

As mentioned above, it can be challenging for domain
experts to search a workﬂow repository and ﬁnd suit-
able workﬂows for their problem. One direction to ad-
dress this issue is to automatically ‘mine’ the workﬂow
itself and extract the problem that workﬂow solves, or
at least extract some tags and other semantic informa-
tion [27].

5.2 Solution Workﬂow Space to Implementation Space
Transformations

The transformations between the middle and bottom
layers of the framework transform a workﬂow in the
solution workﬂow space into some form of code in the
implementation space (Section 4.3). This corresponds
to our challenge question of producing an implemen-
tation from a workﬂow which is usable for a domain
expert.

The techniques examined here are: a) re-implement
the workﬂow manually (not discussed below), b) code
generation / Model-Driven Engineering (MDE) tech-
niques, or c) workﬂow execution directly performed by
the workﬂow tool.

Fig. 9 Transformations between the solution workﬂow space
(inner boxes) and the implementation space (outer boxes.

5.2.1 Model-Driven Engineering Techniques

From a workﬂow deﬁned in a DSL or as components, it
can be a straightforward process to perform MDE tech-
niques such as code generation [7]. Due to the modular
nature of the workﬂows, executable code could be gen-
erated for each component.

This code generation may also take place over a
number of intermediate languages, such as using work-
ﬂow middleware to handle concerns such as scalability
(Section 6).

5.2.2 Direct Execution

Another way of executing the workﬂow is to run it in-
side the workﬂow tool itself. This relieves the domain

expert from running the ﬁnal code themselves, though
it may be more diﬃcult to optimise if needed. Many
of the tools in Section 6 execute the workﬂow in this
way, either through the host language such as Python
or within the tool itself such as Galaxy. This execution
may also be local on the domain expert’s machine or
run on another machine.

5.2.3 Reversing the Transformation

An interesting direction of research is to consider tak-
ing legacy code and extracting the workﬂow from it as
in our challenge question: extract a workﬂow from an
existing implementation. This could be a manual in-
spection or an automatic process. Once completed, the
workﬂow could then become the source of truth and
could be moved into a workﬂow repository for further
dissemination and development [15].

6 Workﬂow Standards and Tools

This section provides another contribution of this pa-
per: an introductory (and therefore non-comprehensive)
selection of workﬂow standards and tools available to
experts from various domains. The intention is for read-
ers to gain a quick understanding of the diversity and
features of what is available today.

Limitations The primary limitation of this section is
that no delve into the literature can reasonably cover
the overwhelming number of workﬂow systems avail-
able. As a lower bound, Amstutz et al. maintain an in-
complete listing of more than 300 workﬂow systems [5].
Instead, the selection in this report is to focus on
open-source scientiﬁc workﬂow tools reported in the
literature. That is, the standards and tools selected
are not targeted towards business, application devel-
opment, or those workﬂows which are built within a
tool or platform8. In this article, the ﬁeld of biologi-
cal sciences is very well-represented. This is due to the
combination of big data and reproducibility concerns
which motivated the creation of such tools [115,91].

This report also aims to be a general introduction
and does not touch on many of the “ilities” relevant
for domain experts to use these tools. For example,
Wratten et al. evaluate twelve bioinformatic workﬂow

example,

8 These workﬂows are commonly referred to as “visual
scripting”. For
https://unity.com/products/
unity-visual-scripting, https://www.blackmagicdesign.
com/ca/products/davinciresolve/fusion,
https:
//lensstudio.snapchat.com/guides/visual-scripting/.
Lens Studio is of particular interest with the integration of
ML algorithms within the workﬂow.

and

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

13

managers using the categories of ease of use, expres-
siveness, portability, scalability, learning resources, and
pipeline initiatives [115]. Admed et al mention modu-
larity and reproducibility amongst others [2], while Ko-
rtelainen adds the important characteristics of licens-
ing and maturity [63]. Other factors may be connec-
tion to specialised tools or computing platforms such
as the Hermes middleware platform for increased scal-
ability [61].

6.1 Workﬂow Formalisms and Standards

Workﬂows can be represented in the simplest form as
a connected and directed graph, where nodes in the
graph are computations and the edges are dependencies
of data or control. Extending beyond this representa-
tion are well-known formalisms which can also represent
workﬂows.

For example, Petri Nets [95] can allow for formal
veriﬁcation of properties such as liveness for the system,
or a Formalism Transformation Graph + Process Model
(FTG+PM) can record the formalisms and transfor-
mations employed in the workﬂow [17]. Another well-
known workﬂow standard is the Business Process Model
and Notation (BPMN) [18] to formalise both automatic
and manual workﬂows within an organisation.

Bringing together both Petri Nets and BPMN is Yet
Another Workﬂow Language (YAWL)9. This workﬂow
language from the 2000’s takes Petri Nets as a starting
point and adds extensions for commonly-seen workﬂow
patterns [108]. The formal semantics of YAWL allow
for veriﬁcation of workﬂow properties such as soundness
(ensuring an option to complete, proper completion, and
no dead transitions) [116].

More recently, a number of workﬂow standards have
been developed in various sub-ﬁelds but none has yet
established dominance over the others [97]. This may
soon change with convergence on the Common Work-
ﬂow Language (CWL)10.

CWL originated in the bioinformatics community
and oﬀers a declarative workﬂow deﬁnition language
(a DSL) that can be written in JSON or YAML to be
executed by a workﬂow execution engine [21]. Of partic-
ular interest to this report is that DS attributes can be
added to workﬂows and their steps as needed by users,
allowing for a great deal of ﬂexibility and discoverability
for domain experts. The standard is becoming estab-
lished throughout multiple domains and has a number
of implementing tools, including upcoming support in
the graphical Galaxy workﬂow tool discussed below.

6.2 Workﬂow Tools and Management Systems

Workﬂow tools and management systems can be related
and considered as descendants from programming build
systems [73] such as make [101] and SCons [62]. The ob-
jective is to record and manage the dependencies of each
component, such that the computation can be correctly
executed.

However, the wide variety of workﬂow management
tools and systems available today have an expanded set
of concerns beyond compilation steps [98, 88,91]. This
can include automatic versioning and provenance con-
cerns, deployment to computational resources, and pro-
viding components for use in workﬂows. In particular,
there is a strong usage of these systems on containerisa-
tion and package management to ensure that workﬂows
can be re-executed in the same context that they were
ﬁrst developed in. The strong focus of these systems on
the scientiﬁc requirement for reusability and reprodu-
cability is further discussed in Section 8.

This section will touch upon some workﬂow systems
found in use today and report some of the interesting
features and considerations implemented.

6.2.1 Text-Based

Current text-based workﬂow systems seem to follow two
approaches: either the system is implemented as a mod-
ule/library for a general programming language such as
Python, or the system ingests a standard markup lan-
guage/DSL.

Language Module A common implementation strategy
for workﬂow tools is to leverage the user’s knowledge
of a general programming language. Commonly, this is
Python due to its widespread usage.

For example, luigi is a tool from Spotify11 which al-
lows a user to build up a dependency graph of Tasks
which interact with Target ﬁles. These concepts are
deﬁned within Python code and the Luigi API oﬀers
access to common database/cloud tools. A web-based
scheduler and visualiser is also available for monitoring
long-running workﬂows.

Luigi was extended by Lampa et al. into SciLuigi [67]
for scientiﬁc workﬂow requirements such as a separation
of the workﬂow and the tasks, audit support, and sup-
port for high-performance computing. The authors then
developed SciPipe 12 in the Go programming language
for enhanced type-safety and performance [68].

Workﬂow systems deﬁned as language modules can
also be tailored to particular domains, further reducing

9 https://yawlfoundation.github.io/
10 https://www.commonwl.org

11 https://github.com/spotify/luigi
12 https://github.com/scipipe/scipipe

14

Bentley James Oakes et al.

the amount of code a domain expert must write to use
the workﬂows.

For example, the automate tool13 for computational
materials science [76] oﬀers workﬂows to copy and cus-
tomise based on speciﬁc analysis of materials, and an
API to the analysis tools themselves. atomate uses the
FireWorks workﬂow software which provides provence
and reporting support for high-throughput computa-
tions [56].

Another example of a domain-speciﬁc library for
workﬂows is the nipype Python software package14 to
deﬁne workﬂows in neuroimaging [48]. The intention
here is to deﬁne components commonly used in neu-
roscience and have them as part of the same work-
ﬂow. This allows domain experts who know Python to
quickly build a workﬂow of neuroscience-speciﬁc tools.
Moving one level of abstraction higher, fMRIPrep15
is an automated workﬂow built on top of nipype [32,
31]. fMRIPrep adapts to the input data automatically
to performing the appropriate preprocessing steps for
functional magnetic resonance imaging (fMRI), such as
head motion correction and skull stripping. This assists
in providing replicable results for neuroimaging studies
both in terms of computation and by providing “boil-
erplate” natural language text for insertion into a re-
search article’s method section.

Replicable results are also important when consider-
ing the development of ML models. The emerging ﬁeld
of Machine Learning Ops (MLOps) tackles the automa-
tion, provenance, performance, and other aspects of ML
in a workﬂow-based form [92]. The ZenML Python li-
brary16 provides a high-level API to machine learning
tasks and tools, while oﬀering workﬂow management
features such as versioning, scheduling, and visualisa-
tion.

Markup or Domain-Speciﬁc Language The other work-
ﬂow speciﬁcation commonly seen in workﬂow manage-
ment systems is to have a deﬁnition written in either
a markup language (such as XML YAML) or a custom
DSL for the workﬂow itself.

Compi 17 is a framework to not only build and run
workﬂows, but also deploy the workﬂows as command-
line applications or containerised as Docker contain-
ers [74]. That is, once a domain expert has built a work-
ﬂow, Compi packages the workﬂow and its dependencies
can be easily shared to other domain experts to use as
a command-line application. Compi uses the markup

13 https://atomate.org
14 https://nipype.readthedocs.io/en/latest/
15 https://fmriprep.org/en/stable/
16 https://zenml.io/
17 http://sing-group.org/compi/

language XML to deﬁne the workﬂows as the creators
L´opez-Fern´andez et al. argue that a DSL for deﬁning
workﬂows is “less interoperable, being diﬃcult to pro-
duce or consume from languages other than the one on
which the DSL is based”, and that XML is “easy to vali-
date syntactically and semantically through schemas” [74].
A repository of Compi workﬂows is available through
the Compi hub project18 which aims to provide commu-
nity exploration of the workﬂow, including automatic
visualisation of the workﬂow tasks and links to sample
input data [84].

Nextﬂow 19 is a workﬂow management system “de-
signed speciﬁcally for bioinformaticians familiar with
programming” [26]. Workﬂows are designed in a Bash
script-like DSL to manage data ﬂow between diﬀerent
workﬂow components. The Nextﬂow tool itself has sup-
port for obtaining and setting up Docker containers to
allow for greater reproducibility of workﬂows.

Nextﬂow also has an active ecosystem providing val-
idated open-source pipelines. In particular, the nf-core
eﬀort20 is a community-maintained eﬀort to develop
“collaborative, peer-reviewed, best-practice analysis pipelines”[34].
Only one pipeline per data type/analysis is allowed, and
all pipelines require quality checks such as a common
structure, MIT licensing, continuous integration tests,
linting, and appropriate documentation.

6.2.2 Graphical

With the rise of “low-code” platforms, there are an in-
creasing number of graphical workﬂow systems avail-
able [11]. A prominent example of this is the domain
of business applications, where providers such as out-
systems21 and Mendix22 provide graphical interfaces to
create applications which can involve ML.

A workﬂow system straddling the domain-speciﬁc
and business domains is the Konstanz Information Miner
(KNIME) [10]. From the University of Konstanz circa
2007, this framework originally focused on pharmaceu-
tical applications23 but has now scaled up for use within
large-scale enterprises. KNIME is based on the Eclipse
platform and oﬀers a component library and canvas for
drag-and-drop connection of nodes. KNIME also oﬀers
a repository for hundreds of components and workﬂows
available for use with a selection of curated compo-
nents available24. Also relevant to this article is a fea-
ture within KNIME called the “Workﬂow Coach”. From

18 https://sing-group.org/compihub/explore
19 https://www.nextflow.io/
20 https://nf-co.re/
21 https://www.outsystems.com/
22 https://www.mendix.com
23 https://www.knime.com/knime-open-source-story
24 https://hub.knime.com/

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

15

KNIME community usage statistics, this panel is able
to recommend the next component to use in the work-
ﬂow25.

The Workﬂow Instance Generation and Selection
tool (WINGS) 26 focuses on semantic workﬂows, where
each input and component has semantic information at-
tached [45]. This information is represented in the form
of triples which allows for domain-speciﬁc information
to be used to select workﬂow components. WINGS can
use this semantic information to select components, in-
put datasets, and parameter values [44].

Focusing on more domain-speciﬁc workﬂow systems,
we select three commonly-used graphical platforms: Node-
RED, Orange, and Galaxy.

Node-RED The Node-RED tool is a web-based editor
for creating Internet of Things (IoT) workﬂows [20].
Nodes provide built-in functionality or can be customised
by adding Javascript code. Figure 10 shows an example
ﬂow to remind a user to exercise. A node communicates
with a weather service to obtain the temperature. If this
value is above 15, then the ﬂow communicates with an
activity tracker. If this reports that a user’s step count
is low, then an email is sent. Flows can be deployed ei-
ther to the user’s local machine or many other embed-
ded devices such as Raspberry PIs27. Nodes and work-
ﬂows can be shared in an online repository28, allowing
users to enhance their workﬂows with new nodes29 and
sub-workﬂows.

Orange The Orange tool oﬀers visual scripting of data
mining techniques including machine learning opera-
tors and visualisation capabilities [25,24]. Originating
out of a bioinformatics research group, Orange focuses
on providing an easy-to-learn data science tool. A can-
vas is provided for users to drag-and-drop nodes from
a node library, where typed connections then aid users
in assembling a workﬂow. Figure 11 shows an exam-
ple workﬂow where data is visualised before clustering
occurs with K-Means [87]. Note the option pane for
K-Means allowing a user to tune the parameters. The
clustering is then visualised.

Orange has two features which improve the reuse
of the tool and its workﬂows. First is the selection of

workﬂows available on the Orange website30. This of-
fers 20 sample workﬂows for performing common data
science tasks such as performing principal component
analysis. The second feature is the robust add-on sup-
port which provides new nodes for workﬂows (which
are called “widgets” in Orange). For example, currently
there are add-ons for bioinformatics, education, and ex-
plainable AI available from within Orange itself. Users
can also create their own nodes to create DS work-
ﬂows [46,107].

Galaxy The Galaxy project is a web-based entire com-
putational workbench for developing biomedical work-
ﬂows [57]. It has spread to other ﬁelds as well with
over 5000 publications citing Galaxy31. The workbench
heavily focuses on concerns such as reproducability, prove-
nance of data and tools, and sharing of workﬂows and
learnings.

Figure 12 shows the Galaxy interface from an on-
line tutorial [19]. On the left is a pane for selecting tools
which in this example focus on genomics. The centre
pane is for displaying website, tool, or dataset informa-
tion. The right pane details the history of the analysis.
These histories correspond to inputs and outputs of the
tools to be recorded and shared. These histories can also
be visualised on a canvas.

An interesting aspect of the Galaxy project is a
focus on community and specialisation. For example,
there is a main publicly-available Galaxy instance on-
line32. However, to spread out computation costs and
oﬀer the possibility of specialising the datasets and tools
available, Galaxy can run on a user’s local or cloud ma-
chine. Furthermore, these other Galaxy instances can
be customised into diﬀerent workspaces.

We highlight three recent DS specialisations of Galaxy

which are publicly available online. Vandenbrouck et al.
developed a Galaxy instance for proteomics research [109],
Tekman et al. provide one for “single cell omics” [104],
and Gu et al. oﬀer a ML focus [49]. These specialisations
each oﬀer targeted DS tools, workﬂows, and compu-
tational resources, allowing domain experts to quickly
develop workﬂows.

6.3 Relation to Framework

https://www.knime.com/blog/

25 See
the-wisdom-of-the-knime-crowd-the-knime-workflow-coach
26 https://www.wings-workflows.org/
27 https://projects.raspberrypi.org/en/projects/
getting-started-with-node-red/
28 https://flows.nodered.org/
29 For example, ML nodes: https://flows.nodered.org/
node/node-red-contrib-machine-learning.

The above tools focus on managing workﬂows for a user.
In this section, we provide general comments relating
these tools to the six questions speciﬁed in Section 1.

30 https://orangedatamining.com/workflows/
31 See https://galaxyproject.eu/citations for publica-
tions focused on just one online instance of the workbench.
32 https://usegalaxy.org/

16

Bentley James Oakes et al.

Fig. 10 Example Node-RED ﬂow to send an email when the weather is nice and a step counter is low. Figure modiﬁed
from [70].

Fig. 11 Example Orange workﬂow to visualise data, cluster it with K-Means, and then show the clustering [87].

Fig. 12 Example Galaxy interface with tool, info, and history panes [8, 19].

This is not intended to be a comprehensive analysis
and comparison of the tools, but instead suggest which
challenges are currently addressed, and which are not
yet addressed by these tools.

Mapping a DS problem to a form suitable for ML : For
the ﬁrst question, none of the tools or their documen-
tation address the issue of mapping a domain-speciﬁc
problem to a ML one. This is understandable, as the
workﬂow management systems focus on the workﬂow
and implementation layers of our framework. However,

further integration of problem speciﬁcation and map-
ping into these tools may assist domain experts.

Providing a solution workﬂow for a DS and/or ML
problem :

This challenge is the core focus of these workﬂow
management systems as they provide the domain ex-
pert with the formalisms and assistance to build up the
workﬂow. However, it is clear that tools have diﬀerent
ways of assisting the user, as described in Section 5.1.
This includes assisted workﬂow composition, domain-
speciﬁc examples, component libraries, and workﬂow

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

17

repositories. Not yet fully addressed in all tools is au-
tomation techniques for constructing workﬂows such as
recommending components.

workﬂows from problems, this is not yet an automated
process. Thus there are ample opportunities for improv-
ing the experience of domain experts to create solution
workﬂows as discussed in Section 8.

Allowing the domain expert to experiment with appro-
priate ML components in a DS workﬂow :

This challenge relates to the ease of which a domain
expert can modify their workﬂow to include ML compo-
nents. The workﬂow tools described here do not oﬀer
automated support for such a modiﬁcation. However,
tools such as Orange attempt to ease the experimen-
tation process for domain experts through its use of
typed component connectors, rich component library,
and visualisation support.

Adding DS knowledge to improve ML performance :

Only the WINGS tool was seen to improve the per-
formance of ML techniques by utilising DS knowledge.
This is possible due to semantic reasoning of domain
knowledge which is used to select appropriate com-
ponents and parameters. More ML-speciﬁc techniques
such as feature extraction are possible but there does
not seem to be integrated support for this challenge n
the tools examined here.

Producing an implementation from a workﬂow which is
well-suited for a domain expert :

This challenge is very well addressed by the tools
mentioned here. In particular, Galaxy oﬀers powerful
computational resources in a web-based platform. This
allows bioinformatics experts to run their workﬂows on
specialised platforms.

Extracting a workﬂow from an existing implementation
(code or notebook :

None of the tools oﬀer support for extracting work-
ﬂows from existing code. However, the light-weight na-
ture of the Python module-based tools could be seen
as an easy way to “lift” existing code into an explicit
workﬂow.

Summary Table 1 oﬀers a general analysis on whether
the tools discussed in this section addressed the chal-
lenge questions. For each question, a summary of the
techniques from Sections 4 and 5 is presented. Symbols
then provide an indication whether the challenge is not,
partially, or more fully addressed by the tools. The last
column then highlights the best examples which address
each challenge.

From Table 1 it is clear that there are a number
of challenge questions which are not addressed in cur-
rent workﬂow tools or their ecosystems. We also iden-
tify that while most tools oﬀer support for constructing

7 Case Studies

This section examines the use of the tools from Sec-
tion 6 in various domains. Cases studies selected from
the scientiﬁc literature provide a sample of how experts
in each domain are building domain-speciﬁc workﬂows
utilising machine learning. This section thus serves to:
a) ground our framework in the state-of-the-practice,
and b) highlight research challenges and opportunities
where the modelling community can assist domain ex-
perts.

As a caveat, these case studies have been selected
in an ad-hoc and non-systematic manner. Instead, the
informal criteria was based on recent publication, avail-
able artefacts, and variety of domain. The intention is
to provide a ﬂavour of the heterogeneity of the domains
and the recent use of tools for discussion, not an exten-
sive literature survey.

We focus on three sets of case studies in this section.
The ﬁrst set is where the case study has an implicit
workﬂow. That is, there is no explicit workﬂow graph
in one of the standards or tools reported in Section 6,
and the workﬂow is expressed in code. The second set of
case studies contains explicit workﬂow artefacts, where
the workﬂow is explicitly deﬁned using a workﬂow stan-
dard/tool. The third set is one case study where the
high-level workﬂow itself is implicit, but it relies on a
sub-workﬂow deﬁned using a workﬂow framework.

7.1 Case Study Overview

Table 2 lists the case studies examined in this report
and discussed throughout this section. Each case study
is provided an identiﬁer based on the primary tool used
and a short description. The second column in Table 2
denotes whether the case study has an implicit, explicit,
or hybrid workﬂow, and the third column lists whether
the workﬂow is created textually or graphically. The last
column reports the regions through our framework that
the case study has artefacts in. That is, what “path”
the authors took through our framework from Section 3.
This can be ML, DS, or B for blended. The Kaggle case
also shows that a blended workﬂow can have a strong
lean towards one dimension. In this case the lean is
towards the ML dimension, represented by BML.

For each case study, the domain-speciﬁc problem is
described. Then, the regions and transformations from

18

Question
Mapping DS → ML
Problem → Workﬂow

Increasing workﬂow ML
Increasing workﬂow DS

Techniques
Expert mapping, ontologies
Workﬂow composition, examples, re-
pos, libraries
Suggestions, experimentation
Knowledge representation,
extraction

feature

Workﬂow → implementation Containerisation, deployment, run

Implementation → workﬂow

within tool
Code mining, language integration

Table 1 Broad analysis of tool support for answering challenge questions.

Bentley James Oakes et al.

Addressed Best Supporting Tool(s)

None
WINGS, Nextﬂow, Nipype, KNIME,
Node-RED, Orange, Galaxy
KNIME, Orange
WINGS

automate, nipype, Compi, KNIME,
Node-RED, Orange, Galaxy
Nipype

Label

Description

Repres. Workﬂow

Regions

CS PyTorch
CS MATLAB Classifying rock origin based on molecular structure (geo-

Detecting peaks in metabolomic data

Textual
Textual

Expression
Implicit
Implicit

B-B-B
B-B-B

CS Kaggle
CS nipype
CS Orange
CS Galaxy

chemistry)
Crowd-sourcing ML solution to marine biology challenge
Detecting depression from MRI images (neuroscience)
Data mining analysis for smart school Internet traﬃc.
Classiﬁcation of urothelial cancer (bioinformatics)

Implicit
Textual
Textual
Hybrid
Graphical Explicit
Graphical Explicit

ML-BML-BML
B-DS-DS
ML-ML-ML
DS-DS-DS

Table 2 Summary of the case studies.

our framework (Section 3) which are relevant are pre-
sented.

Python, basic statistics, exploratory data analysis, clas-
siﬁcation, and regression.

7.2 Implicit Workﬂow Case Studies

The ﬁrst four case studies presented focus on implicit
workﬂows where there is not a workﬂow explicitly de-
ﬁned in one of the standards or tools from Section 6.
These case studies presented therefore cover cases where
the domain expert directly writes an implementation of
their problem, skipping the workﬂow layer of our frame-
work (Section 3). A discussion of these implicit versus
explicit workﬂows is found in Section 8.

7.2.1 CS PyTorch

The ﬁrst case study we examine represents the situation
where a domain expert encodes their problem directly
upon a ML library such as PyTorch or sklearn.

As a Suggested Practice Directly writing code on an
ML library is at a low-level of abstraction requiring a
great deal of ML knowledge. However, we have found
two recent publications from 2020 and 2021 where this
approach is suggested.

For chemistry students, Laﬂuente et al. present an
introductory workshop focusing on utilising Python and
visualisation/ML libraries [66]. The example Jupyter
notebooks 33 lead students through an introduction to

In the ﬁeld of materials science, Wang et al. suggest
that utilising Python code and the PyTorch library is
considered ‘best practice’ [111]. The example Jupyter
notebooks34 walk a domain expert through an exam-
ple application to highlight ML techniques and consid-
erations at a very granular level of detail. For exam-
ple, the reader is taken through constructing the lay-
ers of a neural network in PyTorch, along with calling
the prediction/back-propagation functions. While this
work is very comprehensive and suggests many useful
and concrete suggestions for utilising ML in materials
science, we (kindly) suggest that this is the wrong level
of abstraction for a domain expert to utilise ML at.
While the libraries themselves already abstract the low-
level details, raising the level of abstraction further may
be more appropriate for non-programmers.

Many factors may require utilising ML techniques at
this low level of abstraction may be required for func-
tional properties. For example, obtaining high degrees
of parallelism is cited by Zhou et al. as one reason for
building a material science library upon PyTorch [117].
The PYSEQM library35 implements functions applica-
ble for semi-empirical quantum mechanics models, of-
fering a high-level and domain-speciﬁc interface which
is able to compute over the wide variety of GPUS which
PyTorch supports, oﬀering a speedup over other tools.

33 https://github.com/ML4chemArg/
Intro-to-Machine-Learning-in-Chemistry

34 https://github.com/anthony-wang/BestPractices
35 https://github.com/lanl/PYSEQM

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

19

Case Study As the publications from Laﬂuente et al.
and Wang et al. are targeted toward chemical science
researchers just beginning to utilise ML, we also present
here a case study of a chemical analysis tool peakonly
utilising ML.

Melnikov et al. present an application of deep learn-
ing to classifying and integrating peaks in raw liquid
chromatography-mass spectrometry (LC-MS) data [78].
The problem studied in the work is how to detect re-
gions of interest (peaks) which occur in the noisy LC-
MS data. Figure 13 shows how the data is ﬁrst clas-
siﬁed by a convolutional neural network (CNN) as a)
noise, b) one or more peaks, or c) needs manual classi-
ﬁcation36. A second CNN then provides the integration
boundaries. The results from the CNN are validated
against another tool. The authors provide a graphical
tool peakonly to perform these actions and visualise the
output.

Problem Layer The DS problem speciﬁed in the case
study is to detect and integrate the peaks in the data.
The authors have transformed this into a blended prob-
lem by utilising the expert mapping given by prior work,
where deep learning is used to perform peak detection
and noise ﬁltering.

Solution Workﬂow Layer Figure 13 displays a high-
level view of the paper’s approach to peak classiﬁca-
tion and integration. This forms the basis of the work-
ﬂow components which we extract in Table 3. Note that
these steps are performed by a user interacting with the
peakonly graphical interface, though a runner script is
available.

Table 3 is our attempt at extracting the steps in the
workﬂow from the publication and tool of Melnikov et
al. The ﬁrst column is the high-level step which sum-
marises the individual steps in the second column. The
description column details what the step performs, with
the quoted text copied from the code to explain the DS
steps. The last column in Table 4 is our rough clas-
siﬁcation of whether the step is general (generic pro-
gramming code), domain-speciﬁc (DS), machine learn-
ing (ML), or blended.

First, the DS ﬁle format is loaded into the tool.
Then an algorithm runs to detect the regions of inter-
est (ROIs). These two steps are highly domain-speciﬁc.
The two CNNs are executed to ﬁrst classify the ROIs
and then to integrate the ones with detected peaks. We
classify these as ML-intensive steps. Finally, the user is
presented with the ROIs in a list where they are able to

visually inspect the plot. The data can then be exported
to a CSV ﬁle.

From our classiﬁcation of these steps, it is fair to
say that this is a blended implicit workﬂow. There is a
balance of DS and ML components.

Implementation Layer The implementation for the peakonly
tool is in the Python language, with usage of common
data science/ML libraries (matplotlib, numpy, pandas,
scipy, PyTorch) as well as the DS library pymzML for
mass spectrometry data. This code can be classiﬁed as
blended due to the extensive mixed use of these libraries
within the tool.

Remarks The peakonly tool is an excellent example of
domain experts utilising ML to solve a DS problem and
providing an easy-to-use graphical interface to it. This
lowers the barriers to entry for other domain experts to
utilise ML on their similar problems.

7.2.2 CS MATLAB

For the second case study which focuses on using the
common MATLAB tool which combines data process-
ing and ML techniques, we have selected a publication
from Hasterok et al. in the ﬁeld of geochemistry [50].
The studied problem is to use the chemical composition
of metamorphic rocks to classify whether the origins of
the rocks were sedimentary or igneous. The resulting
MATLAB code is available on GitHub37.

Problem Layer On the problem layer, there is a clear
DS problem of classifying rocks on their chemical struc-
tures.

However, the authors venture further into the ML
domain to determine which available ML classiﬁers are
best suited for their problem. They describe investi-
gation of principal component analysis (PCA) to ﬁlter
the data before classiﬁcation, as well as comparing K-
nearest neighbour, decision trees, ensemble trees, and
testing various parameters within these classiﬁers.

A robust knowledge about applying ML to their DS
problem is shown. Thus we can classify this paper as
addressing a blended problem. The method of trans-
forming the DS problem into a blended problem is not
detailed, but based on the extensive related work cited
we surmise that the authors gained this knowledge by
reading past work which is a form of expert mapping.

36 Figure reprinted (adapted) with permission from [78].
Copyright 2020 American Chemical Society.

37 https://github.com/dhasterok/global_geochemistry/
tree/master/protolith/

20

Bentley James Oakes et al.

Fig. 13 Visual abstract from Melnikov et al. demonstrating classiﬁcation and integration of peaks [78].

High-level Step
Obtain Data

Calculate

Analysis

Description
Load .mzML input data into tool

Step
Load ﬁle
ROI detection Run algorithm to detect ROIs
Classify
Integrate
Plot
Write out

Run classiﬁcation with CNN
Run integration with CNN
Plot integrated ROIs
Write output CSV ﬁle

Classiﬁcation
DS
DS
ML
ML
DS
General

Table 3 Implicit workﬂow components in peakonly tool (Melnikov et al.) [78]

Solution Workﬂow Layer As mentioned, the solution
provided by the authors does not contain an explicit
workﬂow. Instead, the MATLAB code forms an implicit
workﬂow operating on the input data and resulting in
a classiﬁcation or prediction. The authors provide their
code in separate MATLAB scripts with comments, al-
lowing us to reconstruct the predictor workﬂow and di-
vide the scripts into DS and ML. Note that this table
contains workﬂows for both the training and prediction
processes.

From this overview of the workﬂow, we can conclude
that there is a mix of both DS and ML components.
Thus this is a blended workﬂow.

Implementation Layer Following the classiﬁcation of
the implicit workﬂow as blended, it is clear that the
implementation is also blended.

This code was (presumably) hand-written by the au-
thors. The exception is the code in the training step38.
This code seems to have been generated by the MAT-
LAB Classiﬁcation Learner app39.

Remarks From reading the publication of Hasterok et
al, it is clear that the authors have obtained a great deal
of ML knowledge in addition to their domain expertise.
They discuss pre-processing the data through princi-
pal component analysis (PCA) and perform a compari-
son between multiple ML classiﬁcation techniques. This

38 train RUSBoost Classifier 30l 1000s 20190222.m.
39 https://www.mathworks.com/help/stats/
classificationlearner-app.html

knowledge of both domains is reﬂected by the classiﬁca-
tions given by our framework, where all of the problem,
solution workﬂow, and implementation are blended.

The authors also leverage the ML functions built-
into MATLAB for performing the training and classiﬁ-
cation, including the use of a MATLAB app to generate
the appropriate training code.

The code provided contains an implicit workﬂow de-
ﬁned in MATLAB code. However, the authors have
taken the time to modularise this code into various
functions. This improves the usability and reproducibil-
ity of the code amongst other researchers.

7.2.3 CS Kaggle

Kaggle 40 is an online data science platform allowing
data scientists to share models and code. Kaggle is well-
known for its “challenges”, where an organisation posts
their data and an evaluation metric, and asks the Kag-
gle community to come up with a solution to that met-
ric. For example, the “NFL Big Data Bowl” challenge
asked for a prediction of how far one team will advance
on the ﬁeld during one play [47]. The data provided
contained position, speed, and rotation information for
each player on the ﬁeld, weather information such as
temperature, humidity, and wind velocity, and other
data points. Once the competition has ended, the or-
ganisation sometimes contact the winners to explain
their solution.

There are challenges in utilising Kaggle as a crowd-
sourcing tool to provide ML solutions for DS prob-

40 https://kaggle.com/

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

21

High-level Step
Obtain Data

Training
Analysis

Prediction

Analysis

Description
Data loaded into MATLAB

prep for cluster.m selects data for training and testing. Can se-
lect to use meta-igneous/meta-sedimentary rocks or not
Using code generated from MATLAB’s Classiﬁcation Learner app
Write out model ﬁles
Plot training performance
Converting XLS data to CSV, and reading

Step
Load learning
data
Treat learning
data
Training
Write out
Plot
Convert
read
data
FEFIX
CAT2OX
OXIDE NORM “Computes the oxide norm for a set of given oxides”
Prediction
Write out
Analyse

Use MATLAB predict function with the classiﬁer and input data
Write to CSV ﬁle
Plot classiﬁcation performance

“Convert all Fe to FeO and calculate Fe2+/Fe total ratio”
“Convert cations to oxide data when missing”

and
predict

Classiﬁcation
General

Blended

ML
General
General
General

DS
DS
DS
ML
General
General

Table 4 Implicit workﬂow components in MATLAB scripts for predicting protoliths (Hasterok et al.) [50]

lems [13]. Brieﬂy, the beneﬁts of this approach is that
domain experts can directly interface with ML experts
on Kaggle-hosted forums to share best practices and
domain-speciﬁc information. The hope is that this will
lead to knowledge transfer and provide high-quality in-
sights for the domain experts, and oﬀer money, prestige,
and valuable skill training for the ML experts [103].

There are a number of drawbacks, however. The do-
main experts have to spend eﬀort to set up the contest
by providing prize money and accurate problem data.
Also, the solution provided by the ML experts may
not be immediately applicable to the DS problem and
lessons learned must be transferred back to the domain
experts.

For example, in the Killer Shrimp Challenge a triv-
ial solution was found by participants. The data was
organised such that all data points with an index value
greater than or equal to 2917769 had the presence of the
killer shrimp. Thus the solutions of participants could
simply test the index of each data point to obtain per-
fect accuracy on the predictions.

As an example of the eﬀort required to transfer the
ML lessons learned back to the domain experts, we
point to the excellent article of Sutton et al. [102]. This
article examines the top three solutions of a materials
science challenge to determine the impact of the repre-
sentation versus the learning method on the ﬁnal accu-
racy. They describe how the ﬁrst-place solution was a
novel representation for material property ML.

Problem Layer Returning to the Killer Shrimp Chal-
lenge, the underlying problem is to detect the presence
of the species Dikerogammarus villosus (also known as
the “Killer Shrimp”) which causes environmental dam-
age and is invasive in Europe. The speciﬁc Kaggle chal-
lenge is to take data points on water salinity, temper-

ature, depth, wave exposure, and the presence of sand,
and predict whether the Killer Shrimp will be present
or not 41. We refer readers to the article of Bumann et
al. [13] for a full description of the challenge set-up and
interactions between the domain experts and challenge
participants.

It is interesting to note that the DS problem of pre-
dicting the presence of Killer Shrimp was eﬀectively
turned into a problem of predicting 1 or 0 in a particu-
lar column of a spreadsheet. Thus we classify this as a
DS problem being mapped into a ML problem due to
the lack of DS concepts in the problem statement.

Solution Workﬂow Layer In this report, we will at-
tempt to extract the (implicit) workﬂow from the publically-
available second-place solution [106]. The solution is
available as a Jupyter notebook which aids in the re-
construction of the workﬂow.

Table 5 displays our extraction of the workﬂow in
the solution. Note that similar to other workﬂows, we
deﬁne the loading and saving of CSV ﬁles as rather
general steps. Most of the remaining steps are solely
ML-focused. However, the author of the notebook has
also added two DS columns. The ﬁrst added feature is
water density which is calculated from the temperature,
salinity, and depth values. The second column is a clas-
siﬁcation of the wave exposure value to record whether
the point is extremely or very exposed to waves.

In summary, this workﬂow is mostly comprised of
ML components. However, due to the presence of these
added DS features, one could say it is a blended work-
ﬂow with a heavy focus towards the ML side.

Implementation Layer The Jupyter notebook solution
contains Python code and imports the expected data

41 https://www.kaggle.com/c/killer-shrimp-invasion

22

High-level Step
Obtain Data

Feature Adding

Train

Predict
Analysis

Step
Load ﬁles
Clean data
Add Density
Classify Expo-
sure
Add Tempera-
ture Feature
Add Outlier
Feature
Train Classi-
ﬁer
Prediction
Write out
Plot Feature

Bentley James Oakes et al.

Description
Load .csv ﬁles
Fill in missing data with sklearn.impute.IterativeImputer
Calculate ocean density based on other columns
Classify wave exposure into categories

Classiﬁcation
General
ML
DS
DS

Add column based on trained polynomial of temperature

Add column to determine if data point is outlier

Train a classiﬁer

Predict the presence
Write output CSV ﬁle
Plot the importance of features

ML

ML

ML

ML
General
ML

Table 5 Workﬂow components in a Jupyter notebook for the Killer Shrimp Challenge [106].

science/ML libraries (numpy, pandas, matplotlib, sklearn,
xgboost). As the workﬂow is blended (though tilted
towards ML), the implementation can be said to be
blended as well.

Remarks It was surprising to see features added to the
data which were DS. Our expectation was that this sort
of DS knowledge would not be as present in Kaggle so-
lutions, based on the expertise focuses of the challenge
organisers and the ML experts. For example, we wish
to highlight this quote from the analysis of Gordeev
and Singer on their winning entry for the football chal-
lenge [47]:

Don’t worry about having domain knowledge to
attempt a speciﬁc problem. The main thing we
learned in this competition is that you don’t nec-
essarily need domain knowledge or industry [sic]
to successfully tackle the data science challenge.
Sometimes it even can be an advantage, as you
go in blindly without many prior assumptions
that might wrongly steer your exploratory anal-
yses.

Thus, providing a transformation from the DS prob-
lem to a ML representation may have to be balanced
between prior DS knowledge and ML analyses.

7.3 Hybrid and Explicit Workﬂow Case Studies

The remaining case studies presented here are those
which explicitly represent the workﬂow (or a sub-workﬂow)
in one of the standards or tools from Section 6. As dis-
cussed in Section 8, an explicit workﬂow aids with re-
producibility, modularisation, collaboration, re-use, etc.
We also note that the explicit workﬂows tend to
have a strong focus on enabling plugins or extensions for
domains. This means that users are able to customise
the workﬂows for their domains easily.

7.3.1 CS nipype

Practitioners may use workﬂow management systems
such as nipype to develop reproducible workﬂows focus-
ing on particular domain processing tasks. For example,
Celestine et al. present a Python module42 for perform-
ing pre-processing workﬂows such as DS ﬁle conver-
sion and skull stripping for small mammal MRI brain
data [16].

These pre-processing tasks are essential for trans-
forming the data such that it can be treated with ML.
In this case study, we analyse a workﬂow which uses
fMRIPrep as a sub-workﬂow to process MRI data be-
fore it is used to predict whether the person in ques-
tion has depression [81]. As mentioned in Section 6.2.1,
this fMRIPrep automated workﬂow is built on top of
nipype to perform pre-processing of fMRI data [32, 31].
As such, the fMRIPrep tool itself is an explicitly de-
ﬁned workﬂow. However, the authors of Mousavian et
al. have deﬁned an implicit workﬂow in Python to or-
chestrate the usage of the fMRIPrep tool. Thus this is
an interesting hybrid workﬂow case study which utilises
an explicit tool workﬂow.

Problem Layer The speciﬁed problem of Mousavian et
al. is to classify MRI images on whether the subject
has Major Depression Disorder (MDD) or not. There
are three major challenges addressed in the article. The
ﬁrst is to investigate diﬀerent correlation measures of
the voxels (essentially three-dimensional pixels) of the
MRI data. These correlation measures relate diﬀerent
areas of the brain together, and are used as features
for the ML classiﬁcation task. The second challenge is
to handle imbalanced data sets where many subjects
within the set do not have depression which can cause
issues with classiﬁcation. The third major challenge is

42 https://github.com/sammba-mri/sammba-mri

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

23

High-level Step
Pre-Process Data

Step
Load data
Convert format
fMRIPrep
FSL FEAT

Deﬁning Features Data Preparation

Description
Load data into Python
Convert into BIDS format
fMRIPrep workﬂow for alignment/timing correction
Smoothing/ﬁltering steps
“Background removement, normalise, resize, & cube deﬁ-
nition”

Classiﬁcation
General
DS
DS
DS
DS

Classiﬁcation

Prep. Initial Features Correlate cubes
Feature Selection
Classiﬁcation

Rank correlations with stat. tests
Run classiﬁers on data

DS
ML
ML

Table 6 Workﬂow components for predicting depression from MRI images (Mousavian et al.) [81]

to determine which of 14 ML classiﬁers performs best
on the dataset.

month, along with the previous day’s traﬃc and an av-
erage of the previous two days.

From the problem and these speciﬁed challenges, it
is clear that this is a blended problem which combines
DS and ML concepts. Speciﬁcally, the correlation chal-
lenge is DS, while the imbalanced datasets and choice
of classiﬁer challenges are ML-speciﬁc.

Multiple ML classiﬁers are used and compared in
this analysis from both the Orange and KNIME tools
mentioned in Section 6.2.2. As the intention was to com-
pare ML classiﬁers of two diﬀerent workﬂow tools, we
classify this problem as a blended problem.

Solution Workﬂow Layer The case study implicitly de-
ﬁnes a workﬂow through its use of Python scripts43.
However, Mousavian et al. represent the workﬂow as ex-
plicit blocks in the article [81]. Therefore it is straight-
forward to classify each task in the workﬂow as DS or
ML as done for the other case studies.

Table 6 shows the workﬂow as deﬁned in the arti-
cle of Mousavian et al.. The steps present in the article
clearly identify that the majority of the workﬂow is DS.
In particular, only the feature selection and actual clas-
siﬁcation steps are ML.

Implementation Layer As the workﬂow of this case study
is mostly DS, it follows that the implementation is also
very DS. In particular, heavy use of DS libraries and
tools are used such as dcm2niix for format conversion,
PyDeface for removing facial structure from the images,
and the fMRIPrep implementation itself44.

7.3.2 CS Orange

The case study for the Orange tool focuses on data min-
ing analysis for Internet traﬃc from a smart school [1].

Problem Layer The article from Adekitan et al. is an
exploratory analysis on using machine learning tech-
niques for prediction of Internet traﬃc at an educa-
tional institution. The speciﬁc problem is to predict a
classiﬁcation for both download traﬃc and upload traf-
ﬁc among low, slight, moderate, and heavy data traﬃc.
The input information is a numerical day, week, and

43 https://github.com/moosavianmz/
DetectingDepression
44 https://github.com/nipreps/fmriprep

Solution Workﬂow Layer Figure 14 shows the work-
ﬂow of Adekitan et al. in the Orange tool, while the
workﬂow for KNIME is found in their article [1]. The
three general components at the bottom (File, Data
Table, and Box Plot) are used to load the data and vi-
sualise it. The Test & Score component takes the ﬁve
ML learners and the loaded data, and performs the ML
classiﬁcation task. The results are then passed to the
four evaluation components on the right. We classify the
Test & Score component, the learners, and the evalua-
tion components as all ML components. There are no
DS components in this workﬂow, however the feature
engineering described in the article means that domain
knowledge concerning the academic calendar of the in-
stitution has been encoded into the source data. There-
fore, this workﬂow can be classiﬁed as mostly ML-based
with a DS feature engineering step.

Implementation Layer The execution for the Orange
tool can be performed within the editor itself, or by call-
ing the underlying Python code deﬁned for each compo-
nent45. In Orange, the code for the components used is
built upon the scipy and scikit-learn modules. Therefore
we classify this implementation as mostly ML-based.

Remarks This case study represents an exploratory us-
age of ML techniques within a workﬂow. The authors
performed some DS feature extraction on the data and
then applied diﬀerent classiﬁers to determine the per-
formance. The classiﬁcation performance found was quite
low (55 to 63% accuracy), indicating that further DS

45 Orange is currently unable to generate orchestration
Python code from a workﬂow. See https://github.com/
biolab/orange3/issues/1341

24

Bentley James Oakes et al.

Fig. 14 Solution workﬂow in the Orange tool. Adapted from [1].

feature extraction may be required to improve classiﬁ-
cation performance.

7.3.3 CS Galaxy

The last case study for this report details a workﬂow for
the Galaxy framework. The article of F¨oll et al. tackles
supervised classiﬁcation of urothelial (bladder) cancer
using mass spectrometry imaging (MSI) [39].

Problem Layer The input data studied by F¨oll et al.
is obtained using MSI. This imaging technique is per-
formed on a slice of tissue. For each region in the sam-
ple, the instrument provides a spectrum of the masses
of present biomolecules. That is, for each “pixel” of the
sample image a one-dimensional spectrum is created
where peaks correspond to a particular biocompound.
This can be used to visualise and classify regions of the
sample where a particular biomolecule is present.

In the problem of F¨oll et al., this MSI data is manu-
ally labelled by an expert as containing either tumor tis-
sue or stroma (connecting tissue). Tumor tissue is then
further classiﬁed into invasive or non-invasive. There-
fore the problem statement is to develop a classiﬁcation
workﬂow which can pre-process and classify this unique
spectral data. This problem can be said to be DS as it
does not refer to the classiﬁcation techniques used.

Solution Workﬂow Layer An extract from the Galaxy
workﬂow of F¨oll et al. is seen in Figure 4 on page 9. In
particular, these components are in the workﬂow which
classiﬁes tissue as a tumor or stroma. The MSI classi-
ﬁcation component on the right-hand side of Figure 4
takes the MSI data and parameters and outputs a clas-
siﬁcation.

Similar to the other case studies, we present a broad
analysis of the case study workﬂows46 in Table 7. For
each workﬂow created by the authors, we classify each
component within as general, domain-speciﬁc without
ML concepts, or domain-speciﬁc with ML concepts 47. A
percentage of the components which are DS is reported
in a column on the right-hand side of Table 7 along
with a classiﬁcation of the workﬂow.

Table 7 indicates that the workﬂows for this case
study range from moderately DS to strongly DS. Gen-
eral components are used for dataset loading and ma-
nipulation while the DS components perform the non-
trivial work. There are no non-domain-speciﬁc ML com-
ponents in these workﬂows, and the component MSI
Classiﬁcation performs the domain-speciﬁc classiﬁca-
tion computations. Thus it is clear that this is a DS
workﬂow.

Implementation Layer Workﬂows are run by Galaxy
through the web-based tool on either a public or private
server. The individual components are deﬁned through
XML wrappers which deﬁne how to run the underlying
tool.

For these particular workﬂows, the majority of the
DS components are speciﬁc to MSI as they have been
created by the authors in a previous work [38]. These
components are wrappers around the Cardinal tool,
which is a R language module speciﬁcally for analysing
mass spectrometry-based imaging [9]. Thus the imple-
mentation of this workﬂow is mostly DS.

46 https://github.com/foellmelanie/Bladder_MSI_
Manuscript_Galaxy_links
47 Note that ﬁle reading and writing were counted as gen-
eral components, which somewhat inﬂates their number. As
well, in WF3 and WF4 repeated components for splitting
a dataset ten times were counted only once to avoid over-
representation.

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

25

Workﬂow Name
WF1: Co-registration and ROI Extraction
WF2: Data Handling and Preprocessing
WF3: Classiﬁcation tumor vs. stroma
WF4: Classiﬁcation Inﬁltrating vs. Non-inﬁltrating
WF5: Visualization
WF6: Annotating Potential Identities

Num. General Num. DS Num. DS % DS Classiﬁcation

Comps.

22
10
17
15
11
11

Comps.
(non-ML)
6
22
2
3
13
0

Comps.
(ML)
0
0
3
4
0
0

21
69
23
32
54
0

Mod. DS
DS
Mod. DS
Mod. DS
DS
General

Table 7 Classiﬁcation of Galaxy workﬂows for analysing urothelial cancer (F¨oll et al.) [39]

8 Discussion

This section provides discussion for the main research
topic of this article: what are the ways in which domain
experts can use workﬂow-based tools and techniques to
to solve their domain-speciﬁc problems using machine
learning. For this discussion, we ﬁrst present the ben-
eﬁts and drawbacks for structuring this research topic
using our three-layer framework organised into two di-
mensions. Then we examine each of the challenge ques-
tions introduced in Section 1 and present what we see
to be remaining research and integration challenges for
the software engineering community.

8.1 Beneﬁts and Drawbacks of the Three-Layer
Framework

This section discusses some beneﬁts and drawbacks of
organising this research topic as a three-layer frame-
work with inter- and intra-layer transformations.

8.1.1 Beneﬁts

Separation of Concerns The main beneﬁt of our frame-
work is the separation of concerns into the three lay-
ers: problem, workﬂow, and implementation. Similar to
domain-speciﬁc languages, this ensures that the domain
expert ﬁrst addresses the problem space which they are
familiar with, rather than dealing with the accidental
complexity of the workﬂow and implementation spaces.
The framework deﬁnes transformations between these
layers, oﬀering the domain expert a structured way of
progressing their solution.

This separation of concerns is also present in the
workﬂow literature. For example, Lamprecht et al. [69]
deﬁne six stages of workﬂows over time: question or
hypothesis, conceptual workﬂow, abstract workﬂow (se-
quences of tools but not fully conﬁgured), concrete work-
ﬂow (ready to run), production workﬂow (ready for
reuse), and scientiﬁc results. The ﬁrst two stages of
question/hypothesis and conceptual workﬂow thus map
onto our notion of domain-speciﬁc problem.

Implicit versus Explicit Workﬂows Explicit workﬂows
are both conceptually (and literally) at the centre of
our framework. This is because we see numerous ben-
eﬁts with this formalism for domain experts to use in
combination with ML.

First, it is obvious that there is compatibility be-
tween the use of scientiﬁc workﬂows and ML pipelines.
They share the same underlying formalism due to the
same concept of control and data ﬂow, as well as con-
cerns about modularity and reuse. A workﬂow-based
approach also seems to be very amenable to visualisa-
tion and manipulation in graphical tools, allowing non-
experts to quickly build a workﬂow for their problem.

Second, these standalone components are a useful
abstraction over the technical details and complexity
of ML approaches. The domain expert does not have
to become familiar with the ML libraries or in some
cases even a programming language to orchestrate the
workﬂow. Again, this is in concordance with the prin-
ciples of domain-speciﬁc engineering where the domain
expert should focus on manipulating concepts within
their domain.

Third, providing explicit components to the domain
expert allows for enhanced traceability, reuse, scientiﬁc
replication. As seen with the Galaxy tool, input and
output history can be kept for every component in a
workﬂow, along with explicit versioning and supporting
information.

Fourth, a standardised workﬂow system can oﬀer
enhanced beneﬁts for deployment on cloud or high-
performance systems. For example, Lehmann et al. dis-
cuss the scalability beneﬁts gained when porting an
implicit workﬂow orchestrated with the Bash shell lan-
guage to the Nextﬂow workﬂow system [72].

In some domains, the use of explicit workﬂows is a
best practice. For example, Poldrack et al. discuss the
use of nipype for reproducible and scalable workﬂows in
neuroscience [89], while Reiter et al. provide a detailed
article of techniques for biology experts to get started
with workﬂows[91]. However, other ﬁelds may not have
such a strong culture of workﬂows and still recommend
coding for problem solving. As an illustrative example,

26

Bentley James Oakes et al.

Wang et al. recently suggest for material scientists to
use PyTorch in Python for ML purposes [111].

Focus on Transformations Another beneﬁt of our frame-
work is the focus on transformations between regions of
layers, as well as between the layers themselves. These
transformations can be phrased as challenge questions
which are relevant to both software engineers and the
domain experts who must use these transformations.
This provides a clear intent for each transformation
and allows for analysis and identiﬁcation of current ap-
proaches and new techniques.

8.1.2 Drawbacks

There are a number of drawbacks to our organisation of
the research problem onto this three-layer framework.

The ﬁrst is that the two dimensions selected of domain-

speciﬁc and employing machine learning are not orthog-
onal as mentioned in Section 3.1. The divisions of these
dimensions into regions is also crude as we are unable
to provide speciﬁc metrics to divide problems, work-
ﬂows, or implementations due to the qualitative nature
of these dimensions. In particular, the classiﬁcations of
the problem, workﬂows, and implementations for the
case studies in Section 7 are very ad-hoc.

Second, restricting these complex systems onto three
layers is a gross simpliﬁcation. In particular, we ac-
knowledge that the implementation layer is most likely
made up of numerous layers of domain-speciﬁc or gen-
eral programming languages. We have classiﬁed imple-
mentation code in some case studies as domain-speciﬁc
when this is just the top layer of what may be machine
learning or general code at the lowest layer.

Lastly, we also acknowledge the incompleteness of
this article to cover the research topic. It is impossi-
ble to fairly cover all domains or to give an impression
of how prevalent the usage of any tool or technique is
within a domain. This lack of comprehensiveness may
render our framework less applicable when applied to a
particular domain.

8.2 Challenges and Future Research Directions

In this section, we again present the challenge questions
from the introduction in Section 1. For each question we
then present our thoughts on how this challenge ques-
tion has been addressed by the tools and case studies
seen in this article. We present the challenges and po-
tential research directions for each question.

8.2.1 Mapping a DS problem to a form suitable for ML

The ﬁrst challenge we have selected focuses on the prob-
lem layer. That is, how to assist the domain expert to
choose the machine learning techniques which may as-
sist them.

Our analysis in Section 6.3 indicates that none of
the workﬂow frameworks discussed in this article tackle
this challenge. This may be expected as the challenge
is at the problem level, not the workﬂow layer.

This challenge is however very relevant to a domain
expert. For example, most of the case studies exam-
ined in Section 7 directly deﬁne a ML or blended prob-
lem. This could indicate that domain experts are hav-
ing to gain suﬃcient machine learning knowledge to be-
gin their study, instead of leaving the problem as a DS
problem. In particular, the case study of Kaggle (Sec-
tion 7.2.3) shows that domain experts will go to great
lengths to obtain ML expertise on their problem.

Addressing this challenge involves bringing together
semantic information from the DS and ML domains.
In particular, ontological information could be used to
match problems in a particular domain with a ML spec-
iﬁcation [69].

8.2.2 Providing a solution workﬂow for a DS and/or
ML problem

The second challenge we observe is for the domain ex-
pert to eﬃciently discover and/or build a workﬂow which
solves their problem.

Many frameworks use a repository approach to im-
prove the discoverability of workﬂows. That is, they
provide a website for domain experts to search for a
workﬂow which suits their needs 48. We also point to
the impressive Collective Knowledge framework [40] for
a repository focusing on AI, ML, and system research49.
However there still remains a challenge to connect
the workﬂow solutions present in the repository with
the problem faced by the domain expert [41]. The lit-
erature discusses manual and automatic semantic ex-
traction techniques which can assist domain experts in
ﬁnding workﬂows [27, 88]. However, enabling this at-
scale across multiple domains and tools will continue
to be a challenge.

As an example of the rich information to extract
from workﬂows, we point to the work of Lamprecht
et al. who discuss automatic discovery and the static
analysis of workﬂows [69]. This includes technical pa-
rameters (versioning, FAIRness metrics, usage, etc.),

48 Examples include https://workflowhub.eu/, httpsL/
/hub.knime.com, and https://nf-co.re.
49 https://cknowledge.io

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

27

domain-speciﬁc considerations (relevance of components
to a domain, similarity to existing workﬂows, type and
format of results, etc.), and community inﬂuence (cita-
tions, comments, ratings, etc.).

Automated workﬂow composition can also assist do-
main experts in building their workﬂows. A promising
approach is to combine the techniques of AutoML [51]
with domain knowledge about required domain-speciﬁc
components [60,43].

Whether a workﬂow is found or not, the domain
expert is likely to want to add their own components.
Thus another sub-challenge is to improve the suggestion
possibilities for domain experts. Recommendations are
found in the KNIME, Galaxy, and low-code tools [3,
65], but we see further potential in this research area.
For example, suggesting larger pieces of workﬂows, im-
proved semantic reasoning such that components re-
late directly to the domain [77], and employing ma-
chine learning techniques themselves to suggest com-
ponents [85].

8.2.3 Allowing the domain expert to experiment with
appropriate ML components in a DS workﬂow

An interesting challenge is to encourage and assist a
domain expert with experimenting with ML techniques
within a workﬂow. For example, the Orange tool (Sec-
tion 7.3.2 makes it simple to add ML components to
a workﬂow and visualise the results. This sort of vi-
sual experimentation ﬁts perfectly with the component-
based nature of workﬂows and ties in well with the chal-
lenge of improving the automatic recommendation sys-
tems. This experimentation step can also be partially
automated by integrating AutoML techniques [51] or
recommender systems [65] into the workﬂow tool to dy-
namically react to workﬂow changes.

8.2.5 Producing an implementation from a workﬂow
which is well-suited for a domain expert

Once a domain expert has created their workﬂow they
must be able to run it in a scalable manner. This is ad-
dressed in multiple tools from Section 6, but there will
always be further challenges to ensure that a domain ex-
pert can deploy their solution on the correct infrastruc-
ture. For example, code could be generated or param-
eterised based on the domain-speciﬁc tasks or data op-
erated on, such as image- or voxel-based datasets [23].
The rise of containerisation also opens up many chal-
lenges to ensure that these containers are distributed
for optimal scalability and security [89].

8.2.6 Extracting a workﬂow from an existing
implementation (code or notebook)

The ﬁnal challenge we highlight in our article is to con-
vert the existing implementations a domain expert may
have into workﬂows. Amongst other beneﬁts, this would
improve the modularisation and dissemination of these
solutions [15].

For example, Jupyter notebooks 50 are a well-known
paradigm for storage, dissemination, and reproduction
of experimental results [86]. Each cell
in a notebook
contains text or executable code, where the results of
code are shown directly underneath. This format thus
provides a narrative to provide context for the code,
which is useful for disseminating results or tutorials on
a topic.

Rule et al. suggest that scientists spend time to
make Jupyter notebooks themselves form part of a work-
ﬂow [93]. An interesting line of research is therefore to
develop tooling and techniques to automate this pro-
cess, such that the legacy notebooks of domain experts
or machine learning experts51 can be automatically pro-
moted to explicit workﬂows [15].

8.2.4 Adding DS knowledge to improve ML
performance

8.2.7 External Challenges

Another research challenge is how to utilise the DS
knowledge of a domain expert to directly improve the
performance of ML techniques. This is seen in some
case studies (such as in Section 7.2.3 and Section 7.3.1)
where the features themselves were modiﬁed to take
into domain knowledge. As with other challenges de-
scribed here, one approach may be to combine domain
knowledge represented in an ontology with suggestions
for features to extract, such as provided by unsuper-
vised feature extraction [99] or ontology embeddings [64].

Beyond the challenges related to the framework itself,
we also identify two other challenges which are impor-
tant to increase the impact of addressing the prob-
lems speciﬁed in this article. These challenges are: a)
strengthening the workﬂow community as a whole, and
b) proposing tools and techniques to guide a domain
expert in solving their problems.

50 https://jupyter.org/
51 See Quaranta et al. for a dataset of Kaggle notebooks [90].

28

Bentley James Oakes et al.

Strengthening the Workﬂow Community For domain
experts to be able to eﬀectively use workﬂows to solve
their problems using ML, there must be a strong cross-
domain workﬂow community. This community will then
be able to pool knowledge and resources to best solve
domain problems.

The excellent article of da Silva et al. suggests cur-
rent challenges and proposed activities in a workﬂow
community context [97]. The challenges they see are:
FAIR computational workﬂows, AI workﬂows, exascale
challenges and beyond, APIs, reuse, interoperability and
standards, training and education, and building a work-
ﬂows community.

We also see other avenues to strengthen the work-
ﬂow community. In particular, we note the recent re-
search and commercial interest of low-code platforms
which are in some cases workﬂow management tools [53,
11]. It may be possible to leverage this interest into
further developing workﬂow management systems by
providing support for commercial domains. For exam-
ple, the KNIME tool (Section 6.2.2 started development
for solving pharmaceutical applications, but has now
evolved to oﬀer a commercial solution.

Crowdsourcing knowledge is also another possibil-
ity to build up the workﬂow community. For example,
Paul-Gilloteaux et al. suggest the organisation of reg-
ular “taggathons” to annotate tools, workﬂows, com-
ponents, databases, and training materials with terms
from an ontology [88].

We also point towards Kaggle (Section 7.2.3) as an
interesting community of domain and ML experts. De-
spite the issues with crowdsourcing [103,13], it may be
possible to further utilise this pool of knowledge. In
particular, we suggest that oﬀering incentives for com-
petition participants to include explicit workﬂows and
reusable components in their solution may assist with
reuse of their eﬀorts.

Bringing in further expertise from software engi-
neering sub-ﬁelds could also bring beneﬁts to the work-
ﬂow community. In particular, we draw from our own
expertise in model-driven engineering to suggest that
there are many research avenues to explore.

For instance, the multi- view/formalism/level of ab-
straction approach of multi-paradigm modelling may
assist in reducing the cognitive complexity of domain
experts [42]. A concrete example is providing views on
a workﬂow such that the domain expert can focus on
diﬀerent aspects of the workﬂow as needed.

Another research avenue would be integration of
model management approaches such as modelling vari-
ability and uncertainty techniques into workﬂow man-
agement tools [36]. The last research avenue we pro-
pose would be the integration of veriﬁcation and va-

lidity techniques such as recording performance met-
rics [22], enhancing type safety [33], and checking for
formal properties such as reachability [35].

8.2.8 Guiding the Domain Expert

The last challenge we mention in this article is how
to guide the domain expert in both ﬁnding the best
practices and tools for their domain, as well as their
path in the framework.

Recently there have been articles in multiple do-
mains walking a domain expert through the best tools
and techniques available to employ ML [52, 75, 111,66,
92]. For example, Nakhle and Harfouche provide four
detailed Jupyter notebooks52 walking domain experts
in phenomics (plant sciences) through four steps of a
ML task [82].

These four steps ([image] dataset selection, data pre-
processing, data analysis, and performance analysis and
explanation) are representative of most ML workﬂows.
Therefore we suggest that similar dissemination eﬀorts
in diﬀerent domains may assist domain experts. In par-
ticular, collaborative knowledge bases for a domain ex-
pert to navigate the tools and resources available in
their domain may be useful.

Another tooling eﬀort could be to dynamically as-
sist the domain expert in producing template workﬂows
based on their domain-speciﬁc problem [71, 83]. How-
ever, this raises the question of how to assist the do-
main expert through the regions of our framework in
Section 3.

For example, consider three approaches to take a DS
problem and arrive at a blended workﬂow. The ﬁrst ap-
proach is on the problem level, where the domain expert
is provided with basic ML knowledge to assist them in
reﬁning the DS problem to include ML concepts. The
second approach is to immediately build a workﬂow,
and then use AutoML [51] techniques, assist the user
in experimentation (as in the Orange tool), or use onto-
logical recommendations [77] to complete the workﬂow.
The last approach is to follow principles from human-
guided machine learning to iteratively build out the
workﬂow [96,43, 30]. In these three approaches, more
or less automation may be appropriate depending on
the task and user [112].

A further consideration is whether to hide or ex-
pose the ML concepts and components based on the
ML knowledge of the domain expert. This could allow
a user to work with a mostly DS workﬂow, and over
time adjust the workﬂow towards a blended workﬂow

52 Available
here:
Ready-Steady-Go-AI

https://github.com/HarfoucheLab/

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

29

as they gain insight and familiarity with the ML com-
ponents.

unlock new possibilities in their application to pressing
scientiﬁc issues.

9 Conclusion

This article presents a conceptual framework to struc-
ture the process and tools whereby domain experts can
utilise machine learning to solve their problems. In par-
ticular, we focus on the computational workﬂow repre-
sentation of solutions where executable components are
connected by control and data ﬂow edges. Examining
the state-of-the-practice, we identify six key challenges
that a domain expert may face in developing an exe-
cutable workﬂow:

– Map a DS problem to a form suitable for ML
– Obtain a solution workﬂow for a DS and/or ML

problem

– Experiment with ML tools and techniques within a

workﬂow

– Add DS knowledge to improve ML performance (e.g.,

feature engineering)

– Produce an implementation from a workﬂow which
is well-suited for a domain expert (in terms of scal-
ability, DS tooling, etc.)

– Extract a workﬂow from an existing implementation

(code, Jupyter notebook)

These challenges are represented by transformation
within regions of our conceptual framework. This frame-
work has three layers, consisting of the problem layer,
workﬂow solution layer, and implementation layer. Each
layer is further structured with two dimensions repre-
senting the domain speciﬁcity and machine learning us-
age of the artefacts on that layer.

This conceptual framework structures our investiga-
tion of the state-of-the-practice of how domain experts
are employing machine learning. In particular, a selec-
tion of textual and graphical workﬂow tools are pre-
sented to illustrate tool support for the challenges we
have identiﬁed. Case studies selected from recent works
in various domains further explore how the problems,
workﬂows, and implementations created by domain ex-
perts are heterogeneous in terms of the amount of do-
main speciﬁcity and machine learning usage. We also
provide a short discussion on each challenge to indicate
possible research directions.

This article thus forms a basis for further discus-
sion and research into assisting domain experts with
developing workﬂow solutions which employ machine
learning. Integrating best practices from software engi-
neering and across tools will reduce the friction for do-
main experts to utilise these powerful techniques and

Acknowledgements The authors would like to thank our
colleagues Jessie Galasso-Carbonnel and Istv´an D´avid for their
insightful discussions on this article.

Conﬂict of interest

The authors declare that they have no conﬂict of inter-
est.

References

1. Adekitan, A.I., Abolade, J., Shobayo, O.: Data min-
ing approach for predicting the daily internet data
traﬃc of a smart university.
Journal of Big Data
6(1), 1–23 (2019). Figure adapted under the Cre-
ative Commons Attribution 4.0 International License
http://creativecommons.org/licenses/by/4.0/

2. Ahmed, A.E., Allen, J.M., Bhat, T., Burra, P., Fliege,
C.E., Hart, S.N., Heldenbrand, J.R., Hudson, M.E., Is-
tanto, D.D., Kalmbach, M.T., et al.: Design considera-
tions for workﬂow management systems use in produc-
tion genomics research and the clinic. Scientiﬁc reports
11(1), 1–18 (2021)

3. Almonte, L., Cantador, I., Guerra, E., de Lara, J.: To-
wards automating the construction of recommender sys-
tems for low-code development platforms. In: Proceed-
ings of the 23rd ACM/IEEE International Conference
on Model Driven Engineering Languages and Systems:
Companion Proceedings, pp. 1–10 (2020)

4. Alpaydin, E.: Introduction to machine learning. MIT

press (2020)

5. Amstutz, P., Mikheev, M., Crusoe, M.R., Tijani´c, N.,
Lampa, S., et al.: Existing workﬂow systems. Common
Workﬂow Language wiki, GitHub (2021). URL https:
//s.apache.org/existing-workflow-systems.
Up-
dated 2021-12-14, accessed 2022-01-06

6. Aneja, S., Chang, E., Omuro, A.: Applications of arti-
ﬁcial intelligence in neuro-oncology. Current opinion in
neurology 32(6), 850–856 (2019)

7. Banoo, S.N.: Flow-based programming for machine
learning. Master’s thesis, Technical University of Mu-
nich (2020)

8. Batut, B., Hiltemann, S., Bagnacani, A., Baker, D.,
Bhardwaj, V., Blank, C., Bretaudeau, A., Brillet-
Gu´eguen, L., ˇCech, M., Chilton, J., Clements, D.,
Doppelt-Azeroual, O., Erxleben, A., Freeberg, M.A.,
Gladman, S., Hoogstrate, Y., Hotz, H.R., Houwaart, T.,
Jagtap, P., Larivi`ere, D., Corguill´e, G.L., Manke, T.,
Mareuil, F., Ram´ırez, F., Ryan, D., Sigloch, F.C., So-
ranzo, N., Wolﬀ, J., Videm, P., Wolﬁen, M., Wubuli,
A., Yusuf, D., Taylor, J., Backofen, R., Nekrutenko, A.,
Gr¨uning, B.: Community-driven data analysis training
for biology. Cell Systems 6(6), 752–758.e1 (2018). DOI
10.1016/j.cels.2018.05.012. URL https://doi.org/10.
1016/j.cels.2018.05.012

9. Bemis, K.D., Harry, A., Eberlin, L.S., Ferreira, C.,
van de Ven, S.M., Mallick, P., Stolowitz, M., Vitek, O.:
Cardinal: an R package for statistical analysis of mass
spectrometry-based imaging experiments. Bioinformat-
ics 31(14), 2418–2420 (2015)

30

Bentley James Oakes et al.

10. Berthold, M.R., Cebron, N., Dill, F., Gabriel, T.R.,
K¨otter, T., Meinl, T., Ohl, P., Sieb, C., Thiel, K.,
Wiswedel, B.: Knime: The Konstanz Information Miner.
In: C. Preisach, H. Burkhardt, L. Schmidt-Thieme,
R. Decker (eds.) Data Analysis, Machine Learning and
Applications, pp. 319–326. Springer Berlin Heidelberg,
Berlin, Heidelberg (2008)

11. Bock, A.C., Frank, U.: In search of the essence of low-
code: an exploratory study of seven development plat-
In: 2021 ACM/IEEE International Conference
forms.
on Model Driven Engineering Languages and Systems
Companion (MODELS-C), pp. 57–66. IEEE (2021)
12. Borghesi, A., Baldo, F., Milano, M.: Improving deep
learning models via constraint-based domain knowledge:
a brief survey. arXiv preprint arXiv:2005.10691 (2020)
13. Bumann, A., Teigland, R.: The challenges of knowledge
combination in ML-based crowdsourcing–the ODF killer
shrimp challenge using ML and Kaggle. In: Proceedings
of the 54th Hawaii International Conference on System
Sciences, p. 4930 (2021)

14. Cabot, J.: Positioning of the low-code movement within
the ﬁeld of model-driven engineering.
In: Proceed-
ings of the 23rd ACM/IEEE International Conference
on Model Driven Engineering Languages and Systems:
Companion Proceedings, pp. 1–3 (2020)

15. Carvalho, L.A., Wang, R., Gil, Y., Garijo, D.:
NiW: Converting notebooks into workﬂows to capture
dataﬂow and provenance. In: K-CAP Workshops (2017)
16. Celestine, M., Nadkarni, N.A., Garin, C.M., Bougacha,
S., Dhenain, M.: Sammba-MRI: A library for processing
SmAll-MaMmal BrAin MRI data in Python. Frontiers
in neuroinformatics 14, 24 (2020)

17. Challenger, M., Vanherpen, K., Denil, J., Vangheluwe,
H.: FTG+PM: Describing engineering processes in
multi-paradigm modelling.
In: Foundations of Multi-
Paradigm Modelling for Cyber-Physical Systems, pp.
259–271. Springer, Cham (2020)

18. Chinosi, M., Trombetta, A.: BPMN: An introduction to
the standard. Computer Standards & Interfaces 34(1),
124–134 (2012)

19. Clements, D., Gallardo, C.:

Introduction to ge-
nomics
and Galaxy (Galaxy training materials)
(2021). URL https://training.galaxyproject.org/
training-material/topics/introduction/tutorials/
galaxy-intro-strands/tutorial.html.
Online;
accessed Thu Jan 27 2022

20. Contributors, O.F..: Node-RED. https://nodered.org
Iosup, A., Amstutz, P.,
21. Crusoe, M.R., Abeln, S.,
Chilton, J., Tijani´c, N., M´enager, H., Soiland-Reyes,
S., Gavrilovic, B., Goble, C.: Methods included: Stan-
dardizing computational reuse and portability with
the common workﬂow language.
arXiv preprint
arXiv:2105.07028 (2021)

22. D´avid, I., Vangheluwe, H., Van Tendeloo, Y.: Trans-
lating engineering workﬂow models to devs for perfor-
mance evaluation. In: 2018 Winter Simulation Confer-
ence (WSC), pp. 616–627. IEEE (2018)

23. Deelman, E., Mandal, A., Jiang, M., Sakellariou, R.:
The role of machine learning in scientiﬁc workﬂows. The
International Journal of High Performance Computing
Applications 33(6), 1128–1139 (2019)

24. Demˇsar, J., Curk, T., Erjavec, A., Gorup, ˇC., Hoˇcevar,
T., Milutinoviˇc, M., Moˇzina, M., Polajnar, M., Toplak,
M., Stariˇc, A., et al.: Orange: data mining toolbox in
python. the Journal of machine Learning research 14(1),
2349–2353 (2013)

25. Demˇsar, J., Zupan, B.: From experimental machine
learning to interactive data mining. White Paper (www.
ailab. si/orange), Faculty of Computer and Information
science, University of Ljubljana (2005)

26. Di Tommaso, P., Chatzou, M., Floden, E.W., Barja,
P.P., Palumbo, E., Notredame, C.: Nextﬂow enables re-
producible computational workﬂows. Nature biotech-
nology 35(4), 316–319 (2017)

27. Diaz, J.S.B., Medeiros, C.B.: Workﬂowhunt: combin-
ing keyword and semantic search in scientiﬁc workﬂow
repositories. In: 2017 IEEE 13th International Confer-
ence on e-Science (e-Science), pp. 138–147. IEEE (2017)
28. Digan, W., N´ev´eol, A., Neuraz, A., Wack, M., Baudoin,
D., Burgun, A., Rance, B.: Can reproducibility be im-
proved in clinical natural language processing? a study
of 7 clinical nlp suites. Journal of the American Medical
Informatics Association 28(3), 504–515 (2021)

29. Dunn, A., Wang, Q., Ganose, A., Dopp, D., Jain, A.:
Benchmarking materials property prediction methods:
the Matbench test set and Automatminer reference al-
gorithm. npj Computational Materials 6(1), 1–10 (2020)
30. D’Orazio, V., Honaker, J., Prasady, R., Shoemate, M.:
Modeling and forecasting armed conﬂict: Automl with
human-guided machine learning. In: 2019 IEEE Inter-
national Conference on Big Data (Big Data), pp. 4714–
4723. IEEE (2019)

31. Esteban, O., Ciric, R., Finc, K., Blair, R.W.,
Markiewicz, C.J., Moodie, C.A., Kent, J.D., Goncalves,
M., DuPre, E., Gomez, D.E., et al.: Analysis of task-
based functional MRI data preprocessed with fM-
RIPrep. Nature protocols 15(7), 2186–2202 (2020)
32. Esteban, O., Markiewicz, C.J., Blair, R.W., Moodie,
C.A., Isik, A.I., Erramuzpe, A., Kent, J.D., Goncalves,
M., DuPre, E., Snyder, M., et al.: fMRIPrep: a robust
preprocessing pipeline for functional MRI. Nature meth-
ods 16(1), 111–116 (2019)

33. Evans, R., Frohlich, S., Wang, M.: Circuitﬂow: A
domain speciﬁc language for dataﬂow programming.
In: International Symposium on Practical Aspects of
Declarative Languages, pp. 79–98. Springer (2022)
34. Ewels, P.A., Peltzer, A., Fillinger, S., Patel, H., Al-
neberg, J., Wilm, A., Garcia, M.U., Di Tommaso, P.,
Nahnsen, S.: The nf-core framework for community-
curated bioinformatics pipelines. Nature biotechnology
38(3), 276–278 (2020)

35. Fabra, J., Ib´a˜nez, M.J., Ezpeleta, J., et al.: Behavioral
analysis of scientiﬁc workﬂows with semantic informa-
tion. IEEE Access 6, 66030–66046 (2018)

36. Famelis, M., Chechik, M.: Managing design-time uncer-
tainty. Software & Systems Modeling 18(2), 1249–1284
(2019)

37. Fan, Y., Xia, X., Lo, D., Hassan, A.E.: Chaﬀ from the
wheat: Characterizing and determining valid bug re-
ports. IEEE transactions on software engineering 46(5),
495–525 (2018)

38. F¨oll, M.C., Moritz, L., Wollmann, T., Stillger, M.N.,
Vockert, N., Werner, M., Bronsert, P., Rohr, K.,
Gr¨uning, B.A., Schilling, O.: Accessible and repro-
ducible mass spectrometry imaging data analysis in
Galaxy. GigaScience 8(12), giz143 (2019)

39. F¨oll, M.C., Volkmann, V., Enderle-Ammour, K., Wil-
helm, K., Guo, D., Vitek, O., Bronsert, P.G.C.,
Schilling, O.: Moving translational mass spectrome-
try imaging towards transparent and reproducible data
analyses: A case study of an urothelial cancer cohort
analyzed in the Galaxy framework. bioRxiv (2021)

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

31

40. Fursin, G.: Collective knowledge: organizing research
projects as a database of reusable components and
portable workﬂows with common interfaces. Philosoph-
ical Transactions of the Royal Society A 379(2197),
20200211 (2021)

41. Garijo, D., Gil, Y., Corcho, O.: Abstract, link, publish,
exploit: An end to end framework for workﬂow shar-
ing. Future Generation Computer Systems 75, 271–283
(2017)

42. Giese, H., Levendovszky, T., Vangheluwe, H.: Summary
of the workshop on multi-paradigm modeling: Con-
cepts and tools. In: International Conference on Model
Driven Engineering Languages and Systems, pp. 252–
262. Springer (2006)

43. Gil, Y., Honaker, J., Gupta, S., Ma, Y., D’Orazio, V.,
Garijo, D., Gadewar, S., Yang, Q., Jahanshad, N.: To-
wards human-guided machine learning. In: Proceedings
of the 24th International Conference on Intelligent User
Interfaces, pp. 614–624 (2019)

44. Gil, Y., Ratnakar, V., Fritz, C.: Assisting scientists with
complex data analysis tasks through semantic work-
ﬂows. In: 2010 AAAI Fall Symposium Series (2010)
45. Gil, Y., Ratnakar, V., Kim, J., Gonzalez-Calero, P.,
Groth, P., Moody, J., Deelman, E.: Wings: Intelligent
workﬂow-based design of computational experiments.
IEEE Intelligent Systems 26(1), 62–72 (2010)

46. Godec, P., Panˇcur, M., Ileniˇc, N., ˇCopar, A., Straˇzar,
M., Erjavec, A., Pretnar, A., Demˇsar, J., Stariˇc, A.,
Toplak, M., et al.: Democratized image analytics by vi-
sual programming through integration of deep models
and small-scale machine learning. Nature communica-
tions 10(1), 1–7 (2019)

48. Gorgolewski, K., Burns, C.D., Madison, C., Clark, D.,
Halchenko, Y.O., Waskom, M.L., Ghosh, S.S.: Nipype:
a ﬂexible, lightweight and extensible neuroimaging data
processing framework in python. Frontiers in neuroin-
formatics 5, 13 (2011)

49. Gu, Q., Kumar, A., Bray, S., Creason, A., Khantey-
moori, A., Jalili, V., Gr¨uning, B., Goecks, J.: Galaxy-
ML: An accessible, reproducible, and scalable machine
learning toolkit for biomedicine. PLOS Computational
Biology 17(6), e1009014 (2021)

50. Hasterok, D., Gard, M., Bishop, C., Kelsey, D.: Chem-
ical identiﬁcation of metamorphic protoliths using ma-
chine learning methods. Computers & Geosciences 132,
56–68 (2019)

51. He, X., Zhao, K., Chu, X.: AutoML: A survey of the
state-of-the-art. Knowledge-Based Systems 212, 106622
(2021)

52. Huang, K., Xiao, C., Glass, L.M., Critchlow, C.W., Gib-
son, G., Sun, J.: Machine learning applications for thera-
peutic tasks with genomics data. Patterns 2(10), 100328
(2021). DOI 10.1016/j.patter.2021.100328

53. Ihirwe, F., Di Ruscio, D., Mazzini, S., Pierini, P.,
Pierantonio, A.: Low-code engineering for Internet of
Things: A state of research. In: Proceedings of the 23rd
ACM/IEEE International Conference on Model Driven
Engineering Languages and Systems: Companion Pro-
ceedings, pp. 1–8 (2020)

54. Ivie, P., Thain, D.: Reproducibility in scientiﬁc com-
puting. ACM Computing Surveys (CSUR) 51(3), 1–36
(2018)

55. Jablonka, K.M., Ongari, D., Moosavi, S.M., Smit, B.:
Big-data science in porous materials: materials genomics
and machine learning. Chemical reviews 120(16), 8066–
8129 (2020)

56. Jain, A., Ong, S.P., Chen, W., Medasani, B., Qu, X.,
Kocher, M., Brafman, M., Petretto, G., Rignanese,
G.M., Hautier, G., et al.: Fireworks: A dynamic work-
ﬂow system designed for high-throughput applications.
Concurrency and Computation: Practice and Experi-
ence 27(17), 5037–5059 (2015)

57. Jalili, V., Afgan, E., Gu, Q., Clements, D., Blankenberg,
D., Goecks, J., Taylor, J., Nekrutenko, A.: The Galaxy
platform for accessible, reproducible and collaborative
biomedical analyses: 2020 update. Nucleic acids research
48(W1), W395–W402 (2020)

58. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov,
M., Ronneberger, O., Tunyasuvunakool, K., Bates, R.,
ˇZ´ıdek, A., Potapenko, A., et al.: Highly accurate protein
structure prediction with AlphaFold. Nature 596(7873),
583–589 (2021)

59. K¨arn¨a, J., Tolvanen, J.P., Kelly, S.: Evaluating the use
of domain-speciﬁc modeling in practice. In: Proceedings
of the Object-Oriented Programming, Systems, Lan-
guages and Applications workshop on Domain-Speciﬁc
Modeling (2009)

60. Kasalica, V., Lamprecht, A.L.: Ape: A command-line
tool and api for automated workﬂow composition. In:
International Conference on Computational Science, pp.
464–476. Springer (2020)

61. Kintsakis, A.M., Psomopoulos, F.E., Symeonidis, A.L.,
Mitkas, P.A.: Hermes: Seamless delivery of container-
ized bioinformatics workﬂows in hybrid cloud (HTC)
environments. SoftwareX 6, 217–224 (2017)

63. Kortelainen, P.: Manage your workﬂows: A classiﬁcation
framework and technology review of workﬂow manage-
ment systems. Ph.D. thesis, Tampere University (2021)
64. Kulmanov, M., Smaili, F.Z., Gao, X., Hoehndorf, R.: Se-
mantic similarity and machine learning with ontologies.
Brieﬁngs in bioinformatics 22(4), 1–18 (2021)

65. Kumar, A., Rasche, H., Gr¨uning, B., Backofen, R.: Tool
recommender system in Galaxy using deep learning. Gi-
gaScience 10(1), giaa152 (2021). Figure adapted under
the Creative Commons Attribution 4.0 International Li-
cense http://creativecommons.org/licenses/by/4.0/
66. Lafuente, D., Cohen, B., Fiorini, G., Garc´ıa, A.A.,
Bringas, M., Morzan, E., Onna, D.: A gentle introduc-
tion to machine learning for chemists: An undergraduate
workshop using python notebooks for visualization, data
processing, analysis, and modeling. Journal of Chemical
Education 98(9), 2892–2898 (2021)

67. Lampa, S., Alvarsson, J., Spjuth, O.: Towards agile
large-scale predictive modelling in drug discovery with
ﬂow-based programming design principles. Journal of
cheminformatics 8(1), 1–12 (2016)

68. Lampa, S., Dahl¨o, M., Alvarsson, J., Spjuth, O.: Scip-
ipe: A workﬂow library for agile development of complex
and dynamic bioinformatics pipelines. GigaScience 8(5),
giz044 (2019)

69. Lamprecht, A.L., Palmblad, M., Ison, J., Schw¨ammle,
V., Al Manir, M.S., Altintas, I., Baker, C.J., Amor,
A.B.H., Capella-Gutierrez, S., Charonyktakis, P., et al.:
Perspectives on automated composition of workﬂows in
the life sciences. F1000Research 10 (2021)

70. Lea, R.: Node-RED programming guide.
noderedguide.com/. Accessed January 2022.

http://

47. Gordeev, D., Singer, P.: From football newbies to
NFL (data) champions: A winner’s interview with The
Zoo (2020). URL https://medium.com/kaggle-blog/
from-football-newbies-to-nfl-data-champions-a-winners-interview-with-the-zoo-391793168714

Science & Engineering 7(1), 79–88 (2005)

62. Knight, S.: Building software with scons. Computing in

32

Bentley James Oakes et al.

71. Lee, D.J.L., Macke, S.: A human-in-the-loop perspective
on AutoML: Milestones and the road ahead. IEEE Data
Engineering Bulletin (2020)

72. Lehmann, F., Frantz, D., Becker, S., Leser, U., Hostert,
P.: FORCE on Nextﬂow: Scalable analysis of earth ob-
servation data on commodity clusters.
In: Int. Work-
shop on Complex Data Challenges in Earth Observation
(2021)

73. Leipzig, J.: A review of bioinformatic pipeline frame-
works. Brieﬁngs in bioinformatics 18(3), 530–536 (2017)
74. L´opez-Fern´andez, H., Gra˜na-Castro, O., Nogueira-
Rodr´ıguez, A., Reboiro-Jato, M., Glez-Pe˜na, D.: Compi:
a framework for portable and reproducible pipelines.
PeerJ Computer Science 7, e593 (2021)

75. Marx, V.: When computational pipelines go ‘clank’. Na-

ture Methods 17(7), 659–662 (2020)

76. Mathew, K., Montoya,

J.H., Faghaninia, A.,
Dwarakanath, S., Aykol, M., Tang, H., Chu, I.h.,
Smidt, T., Bocklund, B., Horton, M., et al.: Atom-
interface to generate, execute, and
ate: A high-level
analyze computational materials science workﬂows.
Computational Materials Science 139, 140–152 (2017)
77. McIver, R.P.: A knowledge-based approach to scientiﬁc
workﬂow composition. Ph.D. thesis, Cardiﬀ University
(2015)

78. Melnikov, A.D., Tsentalovich, Y.P., Yanshole, V.V.:
Deep learning for the precise peak detection in high-
resolution LC–MS data. Analytical chemistry 92(1),
588–592 (2019)

79. Mora-Cantallops, M., S´anchez-Alonso, S., Garc´ıa-
Barriocanal, E., Sicilia, M.A.: Traceability for trustwor-
thy AI: A review of models and tools. Big Data and
Cognitive Computing 5(2), 20 (2021)
80. Morrison, J.P.: Flow-based programming.

In: Proc.
1st International Workshop on Software Engineering for
Parallel and Distributed Systems, pp. 25–29 (1994)
81. Mousavian, M., Chen, J., Traylor, Z., Greening, S.: De-
pression detection from sMRI and rs-fMRI images us-
ing machine learning. Journal of Intelligent Information
Systems 57(2), 395–418 (2021)

82. Nakhle, F., Harfouche, A.L.: Ready, steady, go AI:
A practical
tutorial on fundamentals of artiﬁcial
intelligence and its applications in phenomics image
analysis.
Patterns 2(9), 100323 (2021).
https://doi.org/10.1016/j.patter.2021.100323.
https://www.sciencedirect.com/science/article/
pii/S2666389921001719

URL

DOI

83. Nalchigar, S.: From business goals to analytics and ma-
chine learning solutions: a conceptual modeling frame-
work. Ph.D. thesis, University of Toronto (Canada)
(2020)

84. Nogueira-Rodr´ıguez, A., L´opez-Fern´andez, H., Gra˜na-
Castro, O., Reboiro-Jato, M., Glez-Pe˜na, D.: Compi
hub: a public repository for sharing and discovering
compi pipelines. In: International Conference on Practi-
cal Applications of Computational Biology & Bioinfor-
matics, pp. 51–59. Springer (2020)

85. Nouri, A., Davis, P.E., Subedi, P., Parashar, M.: Ex-
ploring the role of machine learning in scientiﬁc work-
ﬂows: Opportunities and challenges.
arXiv preprint
arXiv:2110.13999 (2021)

86. Oakes, B.J., Franceschini, R., Van Mierlo, S.,
Vangheluwe, H.: The computational notebook paradigm
for multi-paradigm modeling.
In: 2019 ACM/IEEE
22nd International Conference on Model Driven Engi-
neering Languages and Systems Companion (MODELS-
C), pp. 449–454 (2019). DOI 10.1109/MODELS-C.
2019.00072

87. Orange, D.M.F..F.: Orange website. https://orange.

biolab.si/

88. Paul-Gilloteaux, P., Tosi, S., H´erich´e, J.K., Gaignard,
A., M´enager, H., Mar´ee, R., Baecker, V., Klemm, A.,
Kalaˇs, M., Zhang, C., et al.: Bioimage analysis work-
ﬂows: community resources to navigate through a com-
plex ecosystem. F1000Research 10 (2021)

89. Poldrack, R.A., Gorgolewski, K.J., Varoquaux, G.:
Computational and informatics advances for repro-
ducible data analysis in neuroimaging. arXiv preprint
arXiv:1809.10024 (2018)

90. Quaranta, L., Calefato, F., Lanubile, F.: KGTorrent:
A dataset of Python Jupyter Notebooks from Kag-
gle. In: 2021 IEEE/ACM 18th International Conference
on Mining Software Repositories (MSR), pp. 550–554.
IEEE (2021)

91. Reiter, T., Brooks, P.T., Irber, L., Joslin, S.E., Reid,
C.M., Scott, C., Brown, C.T., Pierce-Ward, N.T.:
Streamlining data-intensive biology with workﬂow sys-
tems. GigaScience 10(1), giaa140 (2021)

92. Ruf, P., Madan, M., Reich, C., Ould-Abdeslam, D.: De-
mystifying MLOps and presenting a recipe for the se-
lection of open-source tools. Applied Sciences 11(19),
8861 (2021)

93. Rule, A., Birmingham, A., Zuniga, C., Altintas, I.,
Huang, S.C., Knight, R., Moshiri, N., Nguyen, M.H.,
Rosenthal, S.B., P´erez, F., et al.: Ten simple rules for
writing and sharing computational analyses in Jupyter
Notebooks (2019)

94. Salazar, V.W., Cavalcante, J.V.F., de Oliveira, D.,
Thompson, F., Mattoso, M.: Bioprov-a provenance li-
brary for bioinformatics workﬂows. Journal of Open
Source Software 6(67), 3622 (2021)

95. Salimifard, K., Wright, M.: Petri Net-based modelling
of workﬂow systems: An overview. European journal of
operational research 134(3), 664–676 (2001)

96. Santos, A., Castelo, S., Felix, C., Ono, J.P., Yu, B.,
Hong, S.R., Silva, C.T., Bertini, E., Freire, J.: Visus:
An interactive system for automatic machine learning
model building and curation.
In: Proceedings of the
Workshop on Human-In-the-Loop Data Analytics, pp.
1–7 (2019)

97. da Silva, R.F., Casanova, H., Chard, K., Altintas, I., Ba-
dia, R.M., Balis, B., Coleman, T., Coppens, F., Di Na-
tale, F., Enders, B., et al.: A community roadmap for
scientiﬁc workﬂows research and development. In: 2021
IEEE Workshop on Workﬂows in Support of Large-Scale
Science (WORKS), pp. 81–90. IEEE (2021)

98. Singh, R., Graves, J.A., Anantharaj, V., Sukumar, S.R.:
Evaluating scientiﬁc workﬂow engines for data and com-
pute intensive discoveries. In: 2019 IEEE International
Conference on Big Data (Big Data), pp. 4553–4560.
IEEE (2019)

99. Solorio-Fern´andez, S., Carrasco-Ochoa, J.A., Mart´ınez-
Trinidad, J.F.: A review of unsupervised feature selec-
tion methods. Artiﬁcial Intelligence Review 53(2), 907–
948 (2020)

100. Soto, P.C., Ramzy, N., Ocker, F., Vogel-Heuser, B.: An
ontology-based approach for preprocessing in machine
learning. In: 2021 IEEE 25th International Conference
on Intelligent Engineering Systems (INES), pp. 000133–
000138. IEEE (2021)

101. Stallman, R.M., McGrath, R., Smith, P.D.: GNU
recompilation
a program for directing
Make
(2020). URL https://www.gnu.org/software/make/
manual/make.pdf

-

Building Domain-Speciﬁc Machine Learning Workﬂows: A Conceptual Framework for the State-of-the-Practice

33

ﬁnally a reality! Business Process Management Journal
(2009)

117. Zhou, G., Nebgen, B., Lubbers, N., Malone, W.,
Niklasson, A.M., Tretiak, S.: Graphics processing unit-
accelerated semiempirical Born Oppenheimer molecular
dynamics using PyTorch. Journal of Chemical Theory
and Computation 16(8), 4951–4962 (2020)

102. Sutton, C., Ghiringhelli, L.M., Yamamoto, T., Lyso-
gorskiy, Y., Blumenthal, L., Hammerschmidt, T.,
Golebiowski, J.R., Liu, X., Ziletti, A., Scheﬄer, M.:
Crowd-sourcing materials-science challenges with the
NOMAD 2018 Kaggle competition. npj Computational
Materials 5(1), 1–11 (2019)

103. Tauchert, C., Buxmann, P., Lambinus, J.: Crowdsourc-
ing data science: A qualitative analysis of organizations’
usage of Kaggle competitions.
In: Proceedings of the
53rd Hawaii international conference on system sciences
(2020)

104. Tekman, M., Batut, B., Ostrovsky, A., Antoniewski,
C., Clements, D., Ramirez, F., Etherington, G.J., Hotz,
H.R., Scholtalbers, J., Manning, J.R., et al.: A single-cell
RNA-seq training and analysis suite using the Galaxy
framework. bioRxiv (2020)

105. Theaud, G., Houde, J.C., Bor´e, A., Rheault, F.,
Morency, F., Descoteaux, M.: TractoFlow: A robust, ef-
ﬁcient and reproducible diﬀusion MRI pipeline leverag-
ing Nextﬂow & Singularity. NeuroImage 218, 116889
(2020)

106. Thompson, C.: Killer

lution.
killer-shrimp-2nd-place-solution (2020)

so-
https://www.kaggle.com/cwthompson/

Shrimp

place

2nd

107. Toplak, M., Read, S.T., Sandt, C., Borondics, F.:
Quasar: Easy machine learning for biospectroscopy.
Cells 10(9), 2300 (2021)

108. Van Der Aalst, W.M., Ter Hofstede, A.H.: Yawl: yet
another workﬂow language. Information systems 30(4),
245–275 (2005)

109. Vandenbrouck, Y., Christiany, D., Combes, F., Loux,
V., Brun, V.: Bioinformatics tools and workﬂow to se-
lect blood biomarkers for early cancer diagnosis: an ap-
plication to pancreatic cancer. Proteomics 19(21-22),
1800489 (2019)

110. Voelter, M., Kolb, B., Birken, K., Tomassetti, F., Alﬀ,
P., Wiart, L., Wortmann, A., Nordmann, A.: Using lan-
guage workbenches and domain-speciﬁc languages for
safety-critical software development. Software & Sys-
tems Modeling 18(4), 2507–2530 (2019)

111. Wang, A.Y.T., Murdock, R.J., Kauwe, S.K., Oliynyk,
A.O., Gurlo, A., Brgoch, J., Persson, K.A., Sparks,
T.D.: Machine learning for materials scientists: An in-
troductory guide toward best practices. Chemistry of
Materials 32(12), 4954–4965 (2020). DOI 10.1021/acs.
chemmater.0c01907. URL https://doi.org/10.1021/
acs.chemmater.0c01907

112. Wang, D., Liao, Q.V., Zhang, Y., Khurana, U., Samu-
lowitz, H., Park, S., Muller, M., Amini, L.: How much
automation does a data scientist want? arXiv preprint
arXiv:2101.03970 (2021)

113. Wen, Y., Hou, J., Yuan, Z., Zhou, D.: Heterogeneous
information network-based scientiﬁc workﬂow recom-
mendation for complex applications. Complexity 2020
(2020)

114. Wilkinson, M.D., Dumontier, M., Aalbersberg, I.J., Ap-
pleton, G., Axton, M., Baak, A., Blomberg, N., Boiten,
J.W., da Silva Santos, L.B., Bourne, P.E., et al.: The
fair guiding principles for scientiﬁc data management
and stewardship. Scientiﬁc data 3(1), 1–9 (2016)
115. Wratten, L., Wilm, A., G¨oke, J.: Reproducible, scal-
able, and shareable analysis pipelines with bioinformat-
ics workﬂow managers. Nature methods 18(10), 1161–
1168 (2021)

116. Wynn, M.T., Verbeek, H., van der Aalst, W.M., ter Hof-
stede, A.H., Edmond, D.: Business process veriﬁcation–

