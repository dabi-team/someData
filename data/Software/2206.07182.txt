Automated Detection of Typed Links
in Issue Trackers

Clara Marie L¨uders, Tim Pietz, and Walid Maalej
Universit¨at Hamburg, Hamburg, Germany
E-Mail: clara.marie.lueders@uni-hamburg.de, tim.pietz@uni-hamburg.de, walid.maalej@uni-hamburg.de

2
2
0
2

n
u
J

4
1

]
E
S
.
s
c
[

1
v
2
8
1
7
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Stakeholders in software projects use issue trackers
like JIRA to capture and manage issues, including requirements
and bugs. To ease issue navigation and structure project knowl-
edge, stakeholders manually connect issues via links of certain
types that reﬂect different dependencies, such as Epic-, Block-,
Duplicate-, or Relate- links. Based on a large dataset of 15
JIRA repositories, we study how well state-of-the-art machine
learning models can automatically detect common link types. We
found that a pure BERT model trained on titles and descriptions
of linked issues signiﬁcantly outperforms other optimized deep
learning models, achieving an encouraging average macro F1-
score of 0.64 for detecting 9 popular link types across all
repositories (weighted F1-score of 0.73). For the speciﬁc Subtask-
and Epic-
the model achieved top F1-scores of 0.89
and 0.97, respectively. Our model does not simply learn the
textual similarity of the issues. In general, shorter issue text
seems to improve the prediction accuracy with a strong negative
correlation of -0.70. We found that Relate-links often get confused
with the other links, which suggests that they are likely used
as default links in unclear cases. We also observed signiﬁcant
differences across the repositories, depending on how they are
used and by whom.

links,

Index Terms—Issue Management, Issue Tracking Systems,
Dependency Management, Duplicate Detection, Mining Issue
Tracker, JIRA, BERT.

I. INTRODUCTION

Software development teams use issue trackers to manage
and maintain software products and their requirements. Popu-
lar issue trackers in practice are Bugzilla1, GitHub Issues2,
and JIRA3, with common features like issue creation and
tracking, communication around issues, release planning, issue
triaging, and issue dependency management. As a central
entity, issues have many properties including their type [24]
(e.g. requirement, feature request, bug report, or task), priority
(e.g. low, high, critical), and assignee. Issue trackers and issue
in general, have been the focus of software
management,
engineering and requirements engineering research for over
a decade, with promising automation approaches, e.g., on
issue type prediction (and correction) [12], [13], [21], [27],
[37], priority (and escalation) prediction [7], [8], [20], [23], or
triaging and assignment [14], [33].

Issues are often interconnected via links [16], [24], which
enables stakeholders to capture the relationships between the
issues and structure the overall project knowledge. Depending

1https://www.bugzilla.org
2https://github.com
3https://www.atlassian.com/software/jira

on the issue tracker and the project, these links can have differ-
ent types. Popular link types are Relate for capturing a general
relation; Subtask and Epic for capturing issue hierarchies; as
well as Depend or Block links for capturing causal or workﬂow
dependencies. Also, Duplicate links are particularly popular
in open source projects, where many stakeholders and users
independently report issues that might be a duplication. This
speciﬁc link type has attracted much attention from Software
Engineering research in recent years [4], [11], [15], [36].

Issue linking is an important part of issue management
and software engineering in general. Research has found that
linking helps reduce issue resolution time [16] and prevent
software defects [28]. Missing or incorrect links can partic-
ularly be problematic for requirements analysis and release
planning [34]. For instance, missing Depend or Block links to
issues assigned to a speciﬁc release might be crucial for that
release. Similarly, missing duplicate links might lead to miss-
ing additional context information [1], which can particularly
be relevant for testing.

As issue trackers are getting more central for documenting
almost all project activities, particularly in agile projects, and
with the evolution of software products over time, a project can
easily get thousands of issues. Each new issue might thus have
hundreds of thousands of potentially relevant links. Correctly
identifying and connecting issues quickly become difﬁcult,
time-consuming, and error-prone [10], [18]. This is even worse
in popular issue trackers like those of Apache, RedHat, or Qt,
which are open to users and other stakeholders and which
may contain hundreds of thousands of issues. The problem
becomes also more complex when link types are taken into
account: the issue creator or maintainer does not only have to
decide if a link between two issues exists but also what the
correct link type is.

To tackle this problem, we report on a novel study, which
aims at automatically predicting the issue links including their
speciﬁc type. Our work is enabled by the advances in deep
learning technology (particularly for natural language texts)
as well as by the availability of large issue datasets. Our
work has two main contributions. First, we compare multiple
state-of-the-art, end-to-end deep-learning models to predict
and classify the issue links. Our evaluation shows that a BERT
model using the description and title of the two involved issues
is by far the most accurate to classify typed links. There are
barely any limitations to the applicability of the model, as the
issue types are not predeﬁned but learned from the data. The

 
 
 
 
 
 
results are very encouraging, particularly since up to 13 link
types are predicted. Second, we compare the properties of the
various repositories and link types and we analyze potential
correlations with the performance of the model. The results
reveal insights on link types confusion as well as on how to
design and apply link prediction tools in practice. In addition
to the recently published dataset of 15 JIRA repositories [24],
we share our code and analysis scripts to ease replication.4

The remainder of the paper is structured as follows. Sec-
tion II outlines our research questions, method, data, and
the various classiﬁcation models used. Section III presents
the performance results of the BERT model for typed link
prediction across 15 different issue repositories. In Section IV,
we compare the repositories and the link types with the factors
that inﬂuence the prediction performance. In Section V, we
discuss our ﬁndings, their limitations, possible link prediction
optimization strategies, and implications for different stake-
holders. Finally, we discuss related work in Section VI and
conclude the paper in Section VII.

II. RESEARCH SETTING

We outline our research questions and data as well as the

research method and machine learning models investigated.

A. Research Questions and Data

Motivated by a) the importance of the typed link prediction
in issue management [16], [28], [34], b) recent work on issue
duplicate prediction [11], c) recent advances of transformer-
based machine learning technique for natural language pro-
cessing BERT [6], as well as d) the availability of large
issue datasets [24], we studied typed links prediction in issue
trackers focusing on two main questions:

• RQ1. How well can state-of-the-art machine learning

predict the different link types in issue trackers?

• RQ2. What are possible explanations for the performance
differences between repositories and link types and what
can we learn from the differences?

For answering the research questions, we used a large
dataset originally consisting of 16 public JIRA reposito-
ries [24]. This dataset perfectly matches our research goals.
Not only is JIRA one of the most popular tools for issue man-
agement in practice567; it is also well-known for its support for
various link types, which can be customized depending on the
project needs. Other available datasets, e.g. of Bugzilla [15],
only focus on the speciﬁc link type Duplicate and have inten-
sively been used in software engineering literature (particularly
Mining Software Repositories) so far [4], [11], [38]. Bugzilla
also supports multiple link types, such as Relate and Depend,
but JIRA repositories usually include a larger variety of link
types.

4https://github.com/RegenKordel/LYNX-TypedLinkDetection
5https://enlyft.com/tech/products/atlassian-jira
6https://www.datanyze.com/market-share/project-management--217/

jira-market-share

7https://www.slintel.com/tech/bug-and-issue-tracking

TABLE I: Studied JIRA repositories in alphabetical order.
Columns: Documented Link Types (#Types);
percentage of issues with a link (%Cov.);
the percentage of links for issues from two different subprojects. (%CP)

Repo.
Apache
Hyperledger
IntelDAOS
JFrog
Jira
JiraEcosystem
MariaDB
Mindville
Mojang
MongoDB
Qt
RedHat
Sakai
SecondLife
Sonatype
Spring
Total
Median

Year
2000 1014926 255767
2016
16304
28146
2016
2599
9474
3229
2006
15535
99819
274545
2002
11398
41866
2004
14618
31229
2009
44
2015
2134
420819 215527
2012
63821
137172
2009
148579
2005
40105
353000 119669
2001
19803
2004
631
2007
4465
2008
14462
2003
- 2686282 882261
15461
-

#Issues #Links #Types #Pro. %Cov. %CP
646
16
28.5% 5.2%
32 54.9% 4.6%
8
2
30.8% 3.8%
11
28.6% 8.2%
10
10
46.7% 43.2%
16
30
33.0% 6.8%
101
14
44.5% 2.5%
11
8
4.0% 4.6%
4
7
53.7% 5.4%
8
5
45.2% 19.1%
27
14
30.2% 6.9%
21
11
39.2% 23.5%
241
15
42.4% 1.4%
53
8
2
39.9% 2.4%
6
7.0% 1.5%
5
11
25.6% 10.0%
80
7
164 1276
-
36.1% 5.3%
24
10.5

50550
1867
87284
69156

59853

-

Table I summarizes the analyzed issue repositories. The
table shows the year of creation, number of issues, number of
links, unique link types, coverage, number of projects, and the
share of cross-project links. Coverage represents the number of
issues having at least one link. The share of cross-project links
is the share of links that connect issues of different projects in
a repository. We bolded the minimum and maximum for each
metric across all repositories in the table.

The investigated repositories are heterogeneous in the re-
ported properties. The number of reported issues ranges
from 1,867 to 1,014,926, while the number of links ranges
from 44 links in Mindville to 255,767 links in Apache. The
repositories also vary in terms of link types: Mindville uses 4
unique link types while Jira (corresponding to the development
of the JIRA issue tracker) and Apache use 16 unique link
types. As 44 training points are too low for a stratiﬁed split,
we excluded Mindville from the analysis. On average, the
coverage is about 36% meaning that 36% of all issues are
part of a link. The coverage ranges from 4.0% in Mindville
to 54.9% and 53.7% in Hyperledger and Mojang respectively.
in Jira, RedHat, and MongoDB links rarely cross
Except
project boundaries. The majority (95% on average) of links
are between issues of the same project. The Jira repository
shows a high coverage among its projects (46.7%).

When analyzing the data, we observed that a link might
point to a private issue. As we have no further information
about private issues, we excluded these links from the analysis.
We also removed multi-links, i.e. when two public issues were
part of multiple links with different types. This affected 1.1%
of all public links. We also simplify the analysis by ignoring
link direction.

Out of the 31 unique link types found in the dataset, most
appear in less than half of all repositories. Uncommon link
types only represent a small share of the links. Table II shows
the frequencies of link types that are used by at
least 7
repositories. The highest variance can be observed for Relate,

TABLE II: Popular link types and their usage shares in percent across the studied JIRA repositories.

Repository
Apache
Hyperledger
IntelDAOS
JFrog
Jira
JiraEcosystem
MariaDB
Mindville
Mojang
MongoDB
Qt
RedHat
Sakai
SecondLife
Sonatype
Spring
Mean
Standard Deviation

Relate Duplicate Subtask Clone Block Depend Epic Split
0.0
1.7
0.5
2.9
/
1.5
/
0.8
0.2
2.9
1.2
1.8
0.2
/
/
15.9
/
0.3
1.2
0.3
6.7
0.1
0.1
15.4
/
4.8
/
7.6
0.1
/
/
0.1
1.1
4.0
2.1
5.4

5.1
/
/
7.9
0.2
1.1
/
/
/
22.9
15.6
/
13.0
4.4
3.6
12.1
8.6
7.2

6.1
8.2
25.5
/
1.0
5.9
13.0
2.3
0.1
/
0.03
15.2
0.03
/
/
/
7.0
8.1

4.9
39.6
/
/
/
24.2
6.4
/
/
15.9
13.5
/
/
/
0.2
11.3
14.5
12.5

10.1
3.9
9.7
19.9
21.7
15.3
9.4
38.6
90.0
13.5
10.6
4.9
9.3
/
7.7
12.1
18.4
21.5

28.3
17.2
39.3
27.4
63.8
22.9
51.1
43.2
9.5
39.9
22.4
25.9
49.0
29.5
40.0
47.7
34.8
14.3

32.8
27.6
10.5
36.0
2.5
20.0
6.1
/
/
1.4
24.4
20.8
17.0
49.8
30.1
13.4
20.9
13.8

Incorporate Bonﬁre Testing Cause Coverage
94.3
100.0
86.6
93.5
96.6
99.1
100.0
100.0
100.0
96.7
93.4
94.0
100.0
93.5
95.0
96.7
96.2
3.7

0.0
0.01
/
/
0.2
0.9
/
/
0.1
/
/
/
0.01
/
8.1
/
1.3
3.0

4.1
/
/
1.4
2.5
1.8
7.9
/
/
/
/
8.9
6.7
2.2
/
/
4.4
3.0

1.2
/
/
/
1.8
3.9
6.0
/
/
1.7
/
2.6
/
/
5.3
/
3.2
1.9

Duplicate, Subtask, and Epic. We focus our study on link
types that have a share greater than 1% in the respective
repository. For instance, Qt’s Clone makes only 0.1% in all
of Qt’s links and thus will not appear in the results table
in Section III. Additionally, to ensure a minimum amount of
comparability and generalizability across the repositories, we
exclude link types from the analysis if their total share across
all repositories is less than 2% (which leads to excluding
Split and Bonﬁre Testing from the analysis). As a result, our
analysis focuses on the following common link types: Relate,
Duplicate, Subtask, Depend, Epic, Clone, Incorporate, Cause,
and Block. A detailed description of the link types can be
found in [24]. We particularly note the difference between
Duplicate and Clone authors: Duplicate links represent acci-
dentally created reports of the same issue whereas Clone links
are automatically created when a user uses the “clone” feature
of JIRA. The link types Block and Depend are similar. But as
some issue trackers, such as Apache and JiraEcosystems use
both types in parallel, we refrained from merging them.

B. Research Method and Machine Learning Models

For answering RQ1, we built, trained, and compared multi-
ple deep learning end-to-end models that predict the link type
or the absence of a link for an issue pair. This includes a
BERT model, which recently attracted much attention in the
NLP community. For RQ2, we focused on the top-performing
model and looked for factors that correlate with the prediction
performance, in particular, properties of the repositories stud-
ied, the link types, as well as properties of the linked issues.
We focused our research on a minimal model input, con-
sisting of the title and description. These are rather universal
issue properties (independently of the tracker and project at
hand) and are usually available at the issue creation time. This
means that we did not study the usage of additional features
for the prediction, such as the issue type or status which might
inﬂuence the prediction performance. Such features might also
contain information about the label (link types) which could
create a bias in the model (e.g., the resolution for duplicate
issues could be “duplicate”).

Our labels consist of the link types as used in the repository
by the stakeholders. We also added non-links as a label
by randomly selecting closed issue pairs that do not have
the resolution property “duplicate”. For instance, the model
trained on Qt had 8 labels Relate, Subtask, Depend, Epic,
Duplicate, Split, Replace, and non-link. We kept the number
of non-links in the experiment always equal to the mean counts
of the other labels: to get enough data points so that the
model can learn but also not too many data points which could
create an imbalance. We then randomly split the data for each
repository into a training (80%) and a test set (20%). For the
model validation, we took 20% of the training data, resulting
in 64/16/20 train-validate-test sets. We also stratiﬁed the data
by link type. We used the test set only for the evaluation.

For selecting the model’s architecture, we checked the
Software Engineering and the Natural Language Processing
(NLP) literature. In the NLP community, BERT [6] and Dis-
tilBERT [30] have recently attracted much attention for various
NLP tasks. In Software engineering, different approaches for
duplicate detection have been recently suggested, with two
main model architectures. Single-Channel architectures [4]
take the word representation (word2vec, fasttext, or GLOVE)
and encode each issue separately with a siamese network,
consisting of a CNN, LSTM, and a feed-forward neural
network. Then, the two encoded issues are fed into another
forward neural network to determine if one issue is a duplicate
of the other. Dual-Channel architectures [11] use the word
representations of two issues (which are two matrices of word
embeddings) to construct a tensor by putting the two matrices
on top of each other. Thus, in this architecture, the two issues
are encoded jointly.

We experimented with BERT, DistilBERT,

the Single-
Channel, and Dual-Channel architectures using FastText and
Word2Vec. DistilBERT is a smaller transformer model trained
to imitate BERT through knowledge distillation. It retains most
of BERT’s general language understanding capabilities while
using 40% fewer model weights. We included DistilBERT
in our comparison because we thought a model with fewer

parameters might perform better on smaller repositories.

In preliminary experiments, we found that BERT outper-
formed all other models in all setups: BERT outperformed
DistilBERT by an average of 0.05 F1-score,
the Single-
Channel models by an average of 0.21, and the Dual-Channel
models by an average of 0.26. The code and results of all
evaluated models are included in the replication package. In
the remainder of the paper, we focus on discussing the results
of BERT as best performing model.

We concatenated the title and description of both issues
into one string and used this as input for the BERT model.
Then, we tokenized this input string with the tokenizer of
bert-base-uncased / distilbert-base-uncased, which is a trained
WordPiece tokenizer. We truncated the token sequence at 192
tokens with a longest ﬁrst strategy. That is, if possible, we
kept all tokens and otherwise truncate the longer of the two
issues ﬁrst. The [CLS] token output of the BERT model was
then fed into a dense layer classiﬁcation head, which predicts
the label of the link. For the training, we chose AdamW and
use the default learning rate of 5e−5 and weight decay of 0.1.
We ran the training on NVIDIA Tesla K80 GPUs using the
largest possible batch size that ﬁt into the GPU memory, which
was 96 with BERT and 128 with DistilBERT. We trained
for 30 epochs and evaluated the model on the validation set
after every epoch, reporting only on the model with the lowest
validation F1-score.

We report the F1-score per common link type and the macro
and weighted averages of the model per repository. Detailed
tables for recall and precision are included in the replication
package, without speciﬁc results which are worth discussing
in the paper. We also present the normalized confusion matrix
for each repository. We chose the conservative macro F1-
score as our primary metric to evaluate performance. Weighted
averages tend to overestimate model performance, as models
tend to predict instances of majority classes better since there
are more data points to learn from. Furthermore, neither using
class weights nor SMOTE as strategies to counter the class
imbalance showed any improvements. As we are unaware of
any baselines of link type detection in JIRA, we created a
baseline by using TF-IDF with Random Forest and SVM and
compared this against our results. We balanced both baseline
classiﬁers with class-weights and report the macro F1-score
of a 5-fold cross-validation.

III. PREDICTION PERFORMANCE

A. Overall Prediction Results

Table III shows the common link types and their respective
F1-scores for each repository. We also present the mean and
standard deviation per link type as well as the macro and
weighted F1-score per repository. Overall, the model macro
F1-score achieves 0.64 on a multi-class problem with a median
of 7 classes per repository. The weighted F1-score goes up
to 0.73. The recall goes from 0.47 for Sonatype up to 0.89 for
Mojang. The precision ranged from 0.46 for Sonatype up to
0.86 for Mojang. The results are far better for all repositories

than the Random Forest and SVM baseline with TF-IDF. Both
baselines achieve an average F1-score of 0.27.

As expected, the results differ across repositories. JFrog
only has a macro F1-score of 0.48, while Mojang has a macro
F1-score of 0.88. Some of the variances across the repositories
can be explained by the size of the training set. For instance,
JFrog, SecondLife, and Sonatype each contain less than 5k
links in total, whereas Mojang contains roughly 200k links. We
also observe that the performance of the model differs per link
type. The model detects Subtask and Epic links consistently
with a top performance, while the prediction of Duplicate,
Depend, and Cause seem to be less accurate. The other link
types Depend, Clone, Incorporate show mixed results. Non-
Link shows top performance for all but one repository.

Figure 1 plots the macro F1-score against

the standard
deviation of the F1-scores across the link types. A higher
standard deviation means that the model performs well for
some but not for all link types. A low standard deviation
means that the model performs similarly for all types. Mojang,
with a lot of available training data and only three different
predicted link types (Duplicate, Relate, and non-link) performs
best (highest macro F1-score with lowest standard devia-
tion). The next cluster consists of Hyperledger, IntelDAOS,
Jira, MariaDB, and MongoDB: with macro F1-scores ranging
from 0.70 to 0.74 and a standard deviation around 0.19. Then,
Qt, RedHat, and Sakai perform slightly worse: their macro
F1-scores lie between 0.63 and 0.66, while their standard
deviation is less than 0.225. The last cluster consists of
Apache, JFrog, JiraEcosystems, Secondlife, Sonatype, and
Spring. These repositories, all with lower coverage, either have
a macro F1-score less than 0.60 or a higher standard deviation.
The case of Apache is particularly interesting as it has the
highest number of links and issues, and one of the highest
number of predicted classes. It also contains the largest number
of projects (646), which indicates some internal heterogeneity.

Finding 1. A general BERT model applied to issue titles
and descriptions predicts the typed links with a promising
mean macro F1-score of 0.64 across 15 repositories. Some
repositories show a top performance while the model
achieved moderate performance for others. The key dif-
ference seems to be the link coverage.

B. Individual Link Types and Confusion Analysis

In the next step, we took a closer look at

the various
link types. Figure 2 shows the confusion matrices for each
repository, ordered by the number of data points per link type
in the test set. We identiﬁed which link types were confusing to
the model and thus consequently mislabeled. Overall, Relate is
often the majority class and commonly predicted by the model
for other link types. This sometimes happens for Duplicate
but on a much smaller scale. We also observe that Clone and
Duplicate are well distinguished by the model.

1) Relate Links: These links are provided by default in a
JIRA installation. They are thus used in all repositories. They
are usually one of the largest classes. The model often mistakes

TABLE III: F1-scores for predicting issues links and their types across the studied JIRA repositories. A bluer color indicates
that the F1-score is closer to 1 while a reddish color indicates a result closer to 0.

RF: Random Forest Baseline (Macro F1), SVM: Support Vector Machine Baseline (Macro F1)

Repository
Apache
Hyperledger
IntelDAOS
JFrog
Jira
JiraEcosystem
MariaDB
Mojang
MongoDB
Qt
RedHat
Sakai
SecondLife
Sonatype
Spring
Mean
Standard Dev.

Relate Duplicate Subtask Depend
0.46
/
/
0.45
/
0.19
/
/
0.69
0.75
/
0.48
0.40
0.42
0.48
0.48
0.15

0.91
0.89
0.82
0.95
0.93
0.89
0.86
/
0.85
0.93
0.90
0.88
0.87
0.91
0.82
0.89
0.04

0.49
0.37
0.44
0.51
0.71
0.57
0.37
0.97
0.46
0.43
0.29
0.39
/
0.33
0.38
0.48
0.17

0.64
0.70
0.69
0.57
0.88
0.62
0.76
0.70
0.73
0.56
0.63
0.72
0.73
0.68
0.73
0.69
0.08

Clone Incorporate
0.55
/
/
0.0
0.64
0.29
0.68
/
/
/
0.77
0.57
0.0
/
/
0.44
0.28

0.61
0.80
0.86
/
0.61
0.59
/
/
/
/
0.83
0.65
0.40
/
/
0.67
0.14

Epic Block Cause Non-Link Macro F1 Weight. F1
0.70
0.97
0.85
0.97
0.68
/
0.66
/
0.82
/
0.71
0.98
0.72
0.97
0.95
/
0.72
0.97
0.71
0.96
0.70
/
0.68
/
0.74
/
0.65
/
0.69
0.99
0.73
0.97
0.08
0.01

0.52
0.62
0.7
/
/
0.41
0.66
/
/
/
0.63
/
/
/
/
0.59
0.10

0.76
0.85
0.48
0.72
0.86
0.60
0.76
0.96
0.72
0.81
0.72
0.78
0.85
0.73
0.74
0.76
0.11

0.34
/
/
/
0.37
0.38
0.52
/
0.36
/
0.40
/
/
0.26
/
0.38
0.07

0.56
0.74
0.72
0.48
0.73
0.53
0.7
0.88
0.72
0.66
0.63
0.64
0.52
0.46
0.62
0.64
0.11

RF SVM
0.13
0.12
0.27
0.30
0.35
0.37
0.27
0.25
0.23
0.27
0.17
0.17
0.33
0.27
0.48
0.48
0.25
0.31
0.29
0.27
0.21
0.25
0.22
0.20
0.30
0.37
0.21
0.21
0.28
0.24
0.27
0.27

in Jira and Mojang Duplicate links were better classiﬁed
with 0.71 and 0.97 F1-scores, respectively. The other reposi-
tories achieved a macro F1-score up to 0.50. In contrast to
Duplicate issues that are usually created independently by
different stakeholders, Clone links are usually intentionally
created via the “Clone Issue” feature of JIRA. These links
seem to be easier detectable by the model with F1-scores
ranging from 0.40 to 0.86.

3) Subtask & Epic Links: The model showed top scores for
classifying Subtask and Epic links, with an average F1-score
of 0.89 and 0.97 respectively. Subtask links ranging from 0.82
up to 0.95 and Epic links ranging from 0.97 up to 0.99, both
also have a low standard deviation. Subtask and Epic links
have an extra section or ﬁeld in an issue view in JIRA and are
not grouped under the ”Issue Links“ section as for the other
link types. Surprisingly, the model was able to differentiate
Subtask links from Epic links, although the performance for
Subtask was slightly lower if the repository includes the type
Epic.

4) Depend, Incorporate, Block, & Cause Links: All of these
link types show either varying or rather low performances.
Depend links’ performance ranged from 0.19 to 0.75. This
could be explained by the link type share. Qt, with the best
performance, has approx. 16% of its links as Depend link
types, followed by MongoDB, with approx. 23% Depend
links. When the performance is low, either Depend is not used
much in the repository (i.e. the absolute count is less than 300)
or it is confused with Relate (for Spring).

The performance for predicting Incorporate links also var-
ied with a standard deviation of 0.28, and macro F1-scores
ranging from 0.00 up to 0.77. In JFrog and SecondLife, the
number of training examples was less than 50 (44 and 14
respectively), explaining the 0.00 in both rows. In cases where
the performance is high, there were at least 1000 training
examples in the repository.

Block links had a moderate performance with a standard
deviation of 0.10 and macro F1-scored from 0.41 up to 0.70.

Fig. 1: Performance per repository according to macro F1-
score and standard deviation for the studied link types.

other link types as Relate links. The F1-score ranges from 0.56
up to 0.88. It is predicted fairly well with an average macro
F1-score of 0.69.

2) Duplicate & Clone Links: We originally assumed that
the model will struggle to distinguish Clone from Duplicate
links and vice versa. Surprisingly, the existence of the class
Clone did not confuse the model. They were rarely mistaken
for each other, which can be observed in the confusion
matrices of Apache, Hyperledger, IndelDAOS, Jira, JiraE-
cosystem, RedHat, and Sakai. This suggests that they exhibit
a difference that the model can recognize. Duplicate links
have rather mixed F1-scores, ranging from 0.29 to 0.97. Only

Fig. 2: Normalized confusion matrices for each repository. Rows are sorted by the support of link type, meaning the majority
class is always the ﬁrst row. The columns are in the same order as the rows.

Even though three repositories frequently use this link type,
it only has an average of 4.8% share across all repositories.
It seems that the model only struggles to distinguish this link
type from Relate but not from other – semantically rather simi-
lar – link types, such as Depend or Cause. We can also observe
that Block and Depend are used separately: either Block is used
or Depend– except in Apache and JiraEcosystem. Finally, we
observe that the performance of Block is high if the repository
uses this type in more than 10% of its links.

Cause links have a very low average share and low perfor-
mance, ranging from 0.26 up to 0.52. MariaDB had the best
performance, but even there the model was unable to discern
this link type from Relate. The reason for this might simply be
that a causal relation is harder to detect while simultaneously
not having enough example data.

5) Non-Links: We observed an encouraging to top perfor-
mance for predicting non-links, up to 0.96 in Mojang, except
for IntelDAOS. Overall the model did not struggle to identify
links from non-links. The case of Mojang, which only has 3

link types to predict, is an indicator of how well the model can
perform as a “linked issues” vs. “non-linked issues” detector.

Finding 2. The BERT model is overall accurate at dis-
tinguishing non-links from links. The structural link types
Subtask and Epic perform very well across all repositories.
Duplicate does not perform well overall, but the model
is able to distinguish Clone links from Duplicate links.
Depend, Incorporate, and Block can be distinguished when
they are frequently used. Cause corresponds to the lowest
prediction performance.
Relate seems to be the most confusing for the model as it
is likely a jack-of-all-trades type used to label a link when
a stakeholder is unsure which type ﬁts.

IV. ANALYSIS OF DIFFERENCES

In this section, we explore possible causes for the model
performance differences across the repositories and link types.

TABLE IV: Correlation coefﬁcient and p-Value of macro F1-
scores of the classiﬁer to properties of the repositories.

Properties
#Issues
#Links
#Projects
#PredictedTypes
%Coverage
%CrossProject
#Total Users
#Assignees
#Creators
#Reporters
Assignee-Issue-Ratio
#Age

Correlation
0.0937
0.3523
-0.2109
-0.5039
0.7500
0.2439
0.4419
-0.1567
0.4374
0.4435
0.5208
-0.4525

p-Value
0.7397
0.1978
0.4505
0.0554
0.0013
0.3811
0.0991
0.5771
0.1030
0.0978
0.0465
0.0903

A. Repository Analysis

We calculated the Pearson correlation and p-value of dif-
ferent repository properties listed in Table I to ﬁnd out what
properties correlate with the macro F1-scores of the model. We
also correlated the number of unique issue creators, reporters,
assignees, and the total number of unique users.

Table IV shows the correlation of the properties with the
macro F1-score. The model’s performance seems independent
of the number of issues in the issue tracker. As expected, we
observe a strong positive correlation with the coverage, an
average positive correlation with the number of links, and a
fairly strong negative correlation with the number of link types
and the age of the issue tracker. A small negative correlation
also exists for the number of projects in the repository. Only
the correlation with coverage is signiﬁcant.

it

is interesting that

Moreover, we observe positive correlations with the number
of total users, creators, and reporters, and a negative correlation
with the number of assignees. Although none of them are
the number of assignees
signiﬁcant,
correlates negatively with the prediction performance. We thus
calculated the number of issues per assignee in a repository,
which turns out to have a signiﬁcant, fairly strong, positive
correlation with the prediction performance. It is worth noting
that, given the rather limited statistical power of the 15 studied
repositories, it is not surprising that most correlations are not
signiﬁcant.

Finding 3. Coverage and number of issues per assignee
show strong and statistically signiﬁcant correlations with
the macro F1-score to predict typed links. A higher cov-
erage can indicate that stakeholders place more value on
linking. The more issues an assignee is responsible for, the
more homogeneous a repository (and the linking) is likely
to become.

B. Link Type Analysis

/ 0.95
/ 0.99

TABLE V: Median cosine similarity scores of the texts of
linked issues on text-token-level across the JIRA repositories.
Repo
Rel. Dup. Sub. Dep. Clo.
Inc. Epic Blo. Cau. NL.
0.14 0.29 0.00 0.08 0.90 0.07 0.00 0.03 0.09 0.00
Apache
/ 0.00
/ 0.06 0.31
0.33 0.44 0.21
Hyperledger
/ 0.00
/ 0.11
0.18 0.29 0.07
IntelDAOS
/
/
0.37 0.38 0.00 0.28 1.00 0.27
JFrog
/ 0.02
/
Jira
/ 0.29 0.34 0.03
0.89 0.44 0.06 0.49 0.92 0.40
JiraEcosystem 0.25 0.48 0.00 0.15 0.94 0.13 0.00 0.13 0.24 0.00
/ 0.13 0.00 0.22 0.13 0.04
MariaDB
/
0.25 0.32 0.06
/ 0.00
Mojang
/ 0.19
0.24 0.19
/ 0.37
/
/ 0.13 0.00
MongoDB
0.17 0.31 0.17 0.24 1.00
/ 0.00
Qt
0.24 0.30 0.21 0.12 0.91
/ 0.10 0.16 0.00
RedHat
0.21 0.36 0.17
/ 1.00 0.00
/ 0.00
/ 0.12
Sakai
0.24 0.39 0.15 0.18 0.89 0.19
/
SecondLife
/ 0.03
/
0.25
/ 0.17 0.15 0.58 0.00
/ 0.21 0.00
/ 0.34
Sonatype
0.27 0.40 0.00 0.14
/
0.20 0.30 0.00 0.16 0.30
Spring
/ 0.00
/
/ 0.00
0.28 0.35 0.09 0.20 0.83 0.15 0.05 0.17 0.19 0.01
Mean

/
/ 0.00
/ 0.00 0.20

Table V shows the median of the cosine similarity of two
issues that make up a link. We see that Clone is the “most
similar” link type, with an average of 0.83. This makes sense
as these links are created by a feature in JIRA that clones the
text with only small changes made by the issue creators. We
think that it might be interesting to investigate Clone links
that have dissimilar issues, particularly if this was due to
incremental changes over time. Duplicate links are the second
most similar, with around 0.35 cosine similarity score, on
average, and a large gap to the similarity score of the Clone
links. Relate links are the third most similar with an average
score of 0.28.

Interestingly, Epic and Subtask linked issues are very dis-
similar. Considering that these are hierarchical relationships,
one would assume that one of the issue’s texts is somewhat
contained in the corresponding other issues, which would
result in a higher similarity. This consistently low similarity
together with the consistently high performance of these link
types indicates that the model is not only learning textual
similarities. Otherwise, the performance of non-links would
have been similarly high. Additionally, the model is able to
clearly differentiate Subtask from Epic.

Finding 4. Clone and Duplicate pairs differ a lot in their
textual similarity. Clone pairs are the most similar, followed
by a wide gap and then by Duplicate. Structural links
Epic and Subtask tend to connect token-dissimilar issues,
as for non-links. Since Epic and Subtask have the highest
prediction performance and can get differentiated from
each other as well as non-links, we can conclude that the
model seems to be learning beyond the lexical similarity
of issues.

We calculated the TF-IDF vectors of the title and description
of the involved issues and used cosine similarity to calculate
the similarity of the issue pairs on a word basis. We also looked
at the length of the textual descriptions of linked issues (word
count of title and description of issue pairs) as well as the
difference in length of two linked issues. We report the results
for the common types in the dataset.

Table VI presents the median issue text length per reposi-
tory, while Table VII shows the median text length differences
per repository. The Table shows the corresponding values for
the studied link types. We observe that Epic and Subtask tend
to connect shorter issues than other link types, while issues
connected via Cause, Relate, Duplicate, and Block are longer.
Moreover, in line with the similarity results, we can observe

TABLE VI: Median length of texts of linked issue on text-
token-level across the JIRA repositories.
Repo.
Apache
Hyperledger
IntelDAOS
JFrog
Jira
JiraEcosystem
MariaDB
Mojang
MongoDB
Qt
RedHat
Sakai
SecondLife
Sonatype
Spring
Mean

Rel. Dupl. Sub. Dep. Clo. Inc. Epic Blo. Cau. NL
93 109 172 133
157
98 120
114
/ 105
96 131
176
/
/
92
/ 142
/ 297
268
/
/ 123
/
152
/ 131
/
70 260 195
/ 161 244 168
192
158 180 183
88
88 142
119
51
93
105 108
70 341 439 350
433
/ 188
/
/ 112
155
/
/ 126
/ 174 119
165
/
96 100
/ 151
187
/
106 134
97 153 110
135
/
/ 112
92
/ 143
174
/ 192
150 123 127
/ 160
163
/
/
120 130 102
50
162
/ 184
/ 116
107
/
53
107 124
173
/ 127
/
/
80 176 215 139
113 132 138
187

87
159
79
159
90
398
9
172
76
182
36
104
85
432
/
146
68
154
45
181
136
74
153 106
79
68
58
69

69
89 216

/
185
179
196

/ 132

44
48

TABLE VII: Median difference of text length of two linked
issues on text-token-level across the JIRA repositories.
Repo.
Apache
Hyperledger
IntelDAOS
JFrog
Jira
JiraEcosystem
MariaDB
Mojang
MongoDB
Qt
RedHat
Sakai
SecondLife
Sonatype
Spring
Mean

Rel. Dupl. Sub. Dep. Clo. Inc. Epic Blo. Cau. NL
52
3
36
46
2
46
59
2
/ 147
60
2
/
/
54
8
43
/
28
4
36
24
38 122 166 162
/
35
24
49
1
55
9
46
0
48
3
54
17
23
/
53
44
9
55

41
/
33
/
40 143
35
18
/
/
/
66

50
58
91
44
0
32
144
42
58
56
42
52
51
45
52
54

43
50
146
49
43
24
124
49
47
51
37
38
/
42
50
57

34
35
30
5
33
12
37
/
22
16
27
36
31
30
23
26

37
/
/
18
39
36
/
/
36
44
/
48
25
46
38
37

41
/
/
61
49
31
98
/
/
/
44
41
70
/
/
54

/
63
/
52
/
/
64
/
75

/
/
/
53
23
38

62
/
/
/
68
51

that two issues linked by a Clone link are very similar in
length, while other link types tend to have a higher difference
in the word count.

The link type Cause, performing worst, bears the highest
difference in text lengths and the longest texts. One possible
explanation is that corresponding descriptions might contain
a lot of information. The BERT model cuts the text after a
number of words and will likely not see the full text of all
links. Particularly information of long stack traces might get
lost.

Table VIII shows the correlation of the analyzed link types
properties, averaged across repositories, with the correspond-
ing macro F1-score. We observe that
the only signiﬁcant
correlation is the text length. We also see cosine similarity
has a small correlation and is not signiﬁcant, conﬁrming

TABLE VIII: Correlation coefﬁcient and p-Value of macro
F1-scores of the classiﬁer to properties of the link types.

Properties
#Counts in Repos
#Difference
#Length
#Cosine Similarity

Correlation
0.2587
-0.4999
-0.6994
-0.1989

p-Value
0.4704
0.1412
0.0244
0.5817

that the model is not a simple similarity comparison model.
Additionally, we observe that links that are mistaken by the
model as Relate links often connect
lengthier issues than
issues of correctly classiﬁed links as well as issues of other
mislabeled links (aside from Relate).

Next, we explored individual link types in depth. Table IX
shows the individual correlations of each link type and its
properties in the repositories. For this, we excluded Mojang,
as it is an outlier with three types and very good performance.
Unsurprisingly, we observe that the share of a link type
in the training data correlates positively with the achieved
performance, except for Epic. This observation is signiﬁ-
cant for Relate, Duplicate, Depend, and Incorporate. This
is aligned with our previous observation that Depend and
Incorporate require more data to be classiﬁed well. The link
type Epic depends on the length, the shorter a text, the better
the classiﬁcation. The link type Clone correlates negatively
with the text length difference and positively with the cosine
similarity. This is the striking difference from other link types.
Last, Cause seems to depend on the difference in text lengths.
Finally, we calculated the Pearson correlation of the perfor-
mance of link types to each other. That is, the existence of a
link type might impact the performance of another link type,
like Depend and Block. We found that Subtask is harder to
detect if the Epic is present in the repository with a correlation
of -0.81. Furthermore, the link type pairs Clone and Block,
Clone and Incorporate, and Block and Incorporate strongly
correlate positively (≥ 0.9) with each other.

Finding 5. Text length of linked issues correlates neg-
atively (-0.70) with the model performance. Epic and
Subtask links correspond to the shortest texts, while Cause,
Relate, and Duplicate present
longer texts. Links mis-
labeled as Relate links tend to connect lengthier issues
than a) correctly classiﬁed links and b) other mislabeled
links. Unsurprisingly, the share of the link type correlates
positively for the link types Relate, Duplicate, Depend, and
Incorporate.

V. DISCUSSION

We summarize our ﬁndings with possible interpretations and
implications for practice. We then discuss the importance of
issue and link quality and outline further research directions.
Finally, we outline possible threats to validity.

A. Applicability of Typed Link Prediction in Practice

While previous work has intensively studied duplication
links (details in Section VI), this work is among the ﬁrst to
study the reliable prediction of issue links having different
types (i.e.
typed links). The ﬁrst resulting use case is to
recommend to stakeholders missing issue links or to highlight
incorrect link types. Our state-of-the-art BERT model achieved
an average macro F1-score of 0.64 and a weighted F1-score
of 0.73 across the 15 studied repositories. These are promising
results considering that the median amount of predicted classes
is 7 (as opposed to, e.g., the simpler binary classiﬁcation).

TABLE IX: Correlation coefﬁcients of the F1-scores to properties of the link types. Signiﬁcant correlations are in bold.

Property

#Support
Share in Training Data
Length
Difference
Cosine Similarity

Link Type

Relate
0.3086
0.7561
0.3514
-0.0165
0.5779

Duplicate
0.5531
0.8765
-0.1507
-0.1547
0.3253

Subtask
0.2399
0.4211
-0.4255
-0.2916
-0.0222

Depend
0.5938
0.8938
-0.0631
0.2407
0.1122

Clone
0.3900
0.1561
-0.2878
-0.8466
0.8451

Incorporate
0.5787
0.7795
-0.0004
-0.0744
0.0269

Epic
-0.3888
-0.0856
-0.7852
-0.7344
-0.1673

Block
-0.0333
0.7133
0.6875
0.7381
0.2922

Cause
0.0821
0.1525
0.7447
0.7617
-0.2032

Non-Link
0.2539
0.3486
0.1927
0.0793
0.3891

Our model is fairly general as it is end-to-end (without
feature engineering) and it only uses the title and description of
the issues as input features. Additionally, even smaller reposi-
tories, like IntelDAOS (2599 links) showed a fair performance.
Indeed, we found no signiﬁcant correlation with the number
of links (the training data). The model works with mostly
untouched data, only with as little basic preprocessing as pos-
sible. Stakeholders with more knowledge about the repository
might ﬁne-tune features based on internal workﬂows. Our
model can further be optimized depending on the individual
repository and project context.

The model was quite precise at predicting the hierarchical
links Epic and Subtask. It also showed similarly high accuracy
for non-links. However, the model was not as good at identi-
fying Duplicate links. Counter-intuitively, it did not struggle
to distinguish Clone from Duplicate links. This is likely due
to the way Clone links are created. JIRA offers a feature to
stakeholders to clone an issue. They create a new issue and
start with all the properties of the cloned issue as default. This
explains the high cosine similarity of issues linked by clone.
In contrast, issues of a Duplicate link are usually created by
two different contributors, who are often unaware of the other
issue. As Clone links are the only ones created automatically,
a link prediction tool could also ignore them.

Other link types, Depend, Incorporate, and Block seem to
need a critical mass to achieve good results. The link type
Cause has the lowest accuracy, which might be due to the
fact that it is barely used or due to its length. We truncated the
texts, meaning that longer texts were cut. This affected link
types which tend to be longer. One reason for long texts is
non-natural language fragments, such as stack traces or code.
It is also possible that stakeholders incorrectly link these issues
with the Relate link type. While manually reviewing the data,
we observed that Relate links often contain stack traces or
other pieces of non-natural language.

There are three strategies that will

likely improve the
prediction performance and further increase the applicability
in practice. First, a tool could ﬁrst detect the existence of a link
and then its type with two different models. The link detection
model is trained on the dataset where all linked issues are in
one class and randomly created non-links in the other class.
The type detection model is trained on all links except the
Relate links. This alleviates the ensuing prediction problems
of likely bad data quality for the Relate link type.

Second, a tool might predict categories of link types instead
of a speciﬁc type. With a median of 10.5 link types per
repository, it is likely that some types are semantically related
or partly redundant. Stakeholders in a project are better aware

of which link types are similar. As our model is general to
various types, no additional changes are needed, except for
grouping the labels from certain types to a category. Then, a
stakeholder with knowledge about the project can choose the
correct link type from the predicted category.

Third, to increase the model’s applicability, the top k predic-
tions can be presented to the stakeholder with multiple possible
hits. With k = 3, we calculated that the average prediction
performance of our results would increase by almost 18%.

B. Data Quality as Prerequisite for Correct Link Prediction

Issue quality, in our case, text quality (missing information,
ambiguity), directly impacts prediction quality as we only
use the textual descriptions in linked issues. For instance, the
issues BE-2138 and BE-949 in Hyperledger are linked as Clone
and mislabeled as Epic by our model. Both of them only have
a title (“Footer Components” and “Header Components”) but
only an empty description.

Issue quality might be affected by the stakeholder creating
the issue or the link (user, developer, analyst, product manager,
etc.). Zimmerman et al. [39] found that most issues reported by
users miss important information for the developers, such as
steps to reproduce or stack traces. As users are often unaware
of what makes good issue reports, they are likely to create
lower-quality issues. In contrast, analysts or developers are
more aware that certain details are important to implement a
requirement, ﬁx a bug, or complete a certain task.

The link types Subtask and Epic can be distinguished quite
well with the text alone, likely due to the unique way they
are treated in JIRA. The main difference to other link types is
that they have their own sections in the issue details. Thus,
stakeholders might treat them more carefully. Furthermore,
Epic and Subtask are often used to structure the issue tracking
system. It is likely that Epic and Subtask links are created with
intent and care, usually by stakeholders deeply involved with
the repository with a planning role. The link is likely created
at issue creation time in an analysis and planning task. This
all might improve the quality of the issue text as well as the
correctness of the links.

In general, we think that the quality of the issues and the
linking (i.e. the correctness of the link and its type) is only as
good as the carefulness of the people who create them. This is
apparent in the seemingly rather average/low quality of Relate
links, which is the most popular link type and most confused
by the model at the same time. We noticed, that other link

8https://jira.hyperledger.org/browse/BE-213
9https://jira.hyperledger.org/browse/BE-94

types are often mislabeled as Relate links. One explanation
might be that the typed link prediction might suffer from low
quality of Relate links. Due to its general nature, Relate links
are likely misused by stakeholders when they are uncertain
whether other speciﬁc types are correct or more ﬁtting. Thus,
Relate might contain a lot of data points that should have
another label. For instance, the issue ZOOKEEPER-392010
in Apache has a Relate link to ZOOKEEPER-3466 and
ZOOKEEPER-3828, while the comments discuss that
this
should be a Duplicate.

C. Further Implications for Research

We found repository and issue attributes that correlate with
the prediction performance. Those were the coverage,
the
issue-to-assignee ratio, as well as the text length of a link.
Furthermore, individual link type performance in repositories
differs. Clone’s performance depends on the textual simi-
larity while other link types, such as Incorporate, Depend,
Duplicate, and Relate depend on their share in the training
data. We think that two important factors, which require more
extensive research are the link quality and the heterogeneity
of a repository.

Coverage, the share of issues that are part of a link, corre-
lates positively with the performance of the model and could
be an indicator of higher data quality, making it easier for any
model to learn. A higher coverage implies that stakeholders
try to link as many issues as possible or place a higher value
on linking as a “best practice” to structure and manage the
project knowledge.

The heterogeneity of an issue tracker is also an interesting
factor in typed link prediction. The issue-to-assignee ratio also
correlates positively with the model performance, the more
issues are assigned to one person, the better the model. This
makes sense: the fewer people involved, the less heterogeneity
and more standardization can likely be observed in the repos-
itory. We did not ﬁnd any further signiﬁcant correlation with
the number of creators, reporters, or overall users.

Another indicator of heterogeneity is the number of projects
in a repository, up to 646 in the case of Apache. We did not
ﬁnd a signiﬁcant correlation between the number of cross-
project links or the number of projects and the performance
of the model on a repository. However, it might make sense to
evaluate the prediction model per project or cluster of projects
per repository instead, as links and their types inside a single
project should be more homogeneous than in the whole repos-
itory. Additionally, the usage of semantically overlapping link
types (such as Depend and Block) in a repository might point
towards a certain heterogeneity. This could be an indicator
for which issues or projects are “similarly managed”. For
instance, one group might mainly use Block for their issues
while another group mainly Depend.

Finally, an interesting aspect of link and typed link predic-
tion is identifying orphans, loners, or phantoms. As coverage
seems to be a factor for good typed link prediction, issues

10https://issues.apache.org/jira/browse/ZOOKEEPER-3920

without any links to other issues or commits should be
investigated. Schermann et al. [31] already examined and
created a heuristic model to deal with links between issues
and commits.

D. Threats to Validity

The data used for training and testing is largely representa-
tive of the way stakeholders use links and their types in prac-
tice. The only restriction we placed was the 1% share lower
bound, as it is very conservative it should not introduce any
bias that overestimates the quality of the model. Practitioners
with knowledge of the underlying processes and context can
group their link types as they see ﬁt, and the same model can
still be applied.

As the labels, the link types, are made by humans, they can
contain false positives. We discuss likely quality issues and
differences between the repositories that inﬂuence the quality
of the model’s prediction. Additionally, we removed 1.1%
of the data as they contained multiple links with different
types between two issues. After reviewing them manually,
we noticed that they were often conﬂicting. Moreover, as
the percentage of multi-links was small, we chose to remove
them since they do not warrant to use a multi-class multi-
label classiﬁer. We also evaluated the approach on 15 different
repositories and thus the approach is generalizable for repos-
itories that use JIRA. We did not evaluate the approach for
Bugzilla and GitHub, which use fewer link types, thus the
approach might produce different results for repositories that
use other issue trackers. As Bugzilla’s default types (Relate,
Duplicate, and Block) are a subset of JIRA’s default types,
the model should achieve a similar performance in Bugzilla.
GitHub does not offer a speciﬁc link functionality. Thus, the
model might be harder to apply there.

We added randomly created non-links to the dataset. We
chose to add as many as the mean number of other link types
present to avoid a majority class which might bias the results.
With a higher amount of non-links performance can change.
Finally, another possible threat is the evolution and changes
over time in issue trackers. If a repository has been in use for
a long time, the implicit deﬁnition of link types can change
too. Issues themselves change over time and the links might
not be updated accordingly and certain link types might fall
out of favor over time.

VI. RELATED WORK

With the rise of agile, requirements are often collected and
tracked in issue trackers. This rise also led to a generation
of large amounts of data in issue trackers, which are often
too much to handle manually. In a case study, Fucci et
al. [10] interviewed JIRA users and found that information
overload is one of their biggest challenges. Interviewees of
the study expressed the need for a requirements dependency
identiﬁcation functionality to reduce the overhead of discov-
ering and documenting dependencies manually. This paper
provides a solid model that tackles this requirements depen-
dency identiﬁcation functionality. Franch et al. [9] tackle the

ensuing problems of agile practices in requirements elicitation
and management with situational method engineering. The
collection and maintenance of requirements interdependencies
(i.e. issue links) also face similar challenges.

Requirements Engineering research has largely studied an-
other kind of dependency between issues/requirements and
software artifacts: a topic known as traceability. Deep learning
has also been used for traceability. Lin et al. [17] found that the
traceability links between issue description to source code can
be found with BERT which outperforms the traditional infor-
mation retrieval methods, achieving an F1-score of 0.612 and
0.729. Typed link prediction has similar application problems
as traceability, such as poor quality in issue trackers, found by
Merten et al. [22]. Additionally, Seiler et al. [32] conducted
an interview study about the problems of feature requests
in issue trackers and found that unclear feature descriptions
and insufﬁcient traceability are among the major problems
in practice. For typed link prediction we also found that
heterogeneity of the repository is a problem.

Concerning issue link prediction, Duplicate is the most
widely researched type, as duplication detection is a tedious
task [1] when curating issue trackers. Deshmukh et al. [4]
proposed a single-channel siamese network approach with
triplet loss which achieves an accuracy close to 90% and a
recall rate close to 80%. He et al. [11] proposed a dual-channel
approach and achieved an accuracy of up to 97%. Rocha
et al. [29] created a model using all “Duplicate” issues as
different descriptions of the same issue and report a Recall@25
of 85% for retrieval and an 84% AUROC for classiﬁcation.
All
three works [4], [11], [29] use the data set provided
by Lazar et al. [15], containing data mined from the four
open-source Bugzilla systems: Eclipse, Mozilla, NetBeans,
and OpenOfﬁce.

There also exist studies researching other link types between
issues and link usage. Thompson et al. [34] studied three
open-source systems and analyzed how software developers
use work breakdown relationships between issues in JIRA.
They observed little consistency in the types and naming
of the supported relationships. Li et al. [16] examined the
issue linking practices in GitHub and extracted emerging
linking patterns. They cateogorized link types into 6 link type
categories, namely: “Dependent”, “Duplicate”, “Relevant”,
“Referenced”, “Fixed”, “Enhanced”, all the rarer link types
were assigned the category “Other”. They discovered patterns
for automatic classiﬁcation; “Referenced” links usually refer
to historic comments with important knowledge and that
“Duplicate” links are usually marked within the day. Tomova
et al. [35] studied seven open-source systems and reported
that the rationale behind the choice of a speciﬁc link type is
not always obvious. The authors also found that Clone links
are indicative of textual similarity, issues linked through a
Relate link presented varying degrees of textual similarity and
thus require further contextual information to be accurately
identiﬁed. From the varying degrees of textual similarity,
we hypothesized that Relate links might be a jack-of-all-
trades type. Furthermore, while we found that some link types

have distinct textual similarities, they are not unequivocally
identiﬁable only based on the textual similarity.

“Requires” and “Reﬁnes” links were examined by Desh-
pande et al. [5], who extracted dependencies on two industrial
data sets achieving an F1-score of at least 75% in both training
sets. Block links were examined by Cheng et al. [2] on the
projects mined by Lazar et al. [15]. They predicted the Block
link type with an F1-Score of 81% and AUC of 97.5%.

Most previous works view link types in isolation. Recently,
we evaluated state-of-the-art duplicate detection models on
these
the same dataset used in this work and found that
models struggle with distinguishing Duplicate from other link
types [19]. In this work, we focus on predicting multiple user-
deﬁned link types. We present and evaluate a BERT model for
typed links prediction.

Nicholson et al. [25], [26] also researched typed link pre-
diction between issues of projects in Apache. The ﬁrst [25]
analyzes the link types and tries to ﬁnd patterns to predict
missing links. The second [26] evaluates several traditional
machine learning approaches and achieves a weighted F1-
score of about 0.563 up to 0.692 across the three projects
HIVE, FLEX, and AMBARI.

Data quality, in this case, issue texts and link quality are
essential for well-performing tools and smooth-running work-
ﬂows. We found that data quality affects typed link prediction
as well. In a similar line, Dalpiaz et al. [3] created an approach
to improve the quality of user stories, a type of issue, by
removing linguistic defects. Link quality is directly affected by
requirements quality. If issue descriptions are vague or contain
other defects, links are harder to classify.

VII. CONCLUSION

Using BERT on the titles and descriptions of issue pairs,
we achieved good performances to predict typed issue links
on most studied repositories; and consistently excellent per-
formance for predicting Epic and Subtask links. Our detailed
analysis revealed that by better understanding the data and im-
proving the issue quality (for the training and prediction) and
the link quality (for the training) the prediction performance
will likely get improved further – particularly for correctly
predicting the general purpose Relate links. We discussed
strategies for increasing the performance and thus the model’s
applicability, particularly where data quality is limited or
heterogeneity in the issue tracker is high. Future work should
focus on the underlying data as well as the repository-,
project-, and stakeholder-speciﬁc factors that might impact the
performance. For this user studies and qualitative research is
needed to understand how and why stakeholders use the links
and link types.

ACKNOWLEDGMENT

We thank Lloyd Montgomery for collecting the dataset.
This work has been partly conducted within the Horizon 2020
project OpenReq, which is supported by the European Union
under the Grant Nr. 732463.

REFERENCES

[1] Y. C. Cavalcanti, E. S. d. Almeida, C. E. A. d. Cunha, D. Lucr´edio,
and S. R. d. L. Meira. An initial study on the bug report duplication
problem. In 2010 14th European Conference on Software Maintenance
and Reengineering, pages 264–267, 2010.

[2] X. Cheng, N. Liu, L. Guo, Z. Xu, and T. Zhang. Blocking bug prediction
based on xgboost with enhanced features. In 2020 IEEE 44th Annual
Computers, Software, and Applications Conference (COMPSAC), pages
902–911, July 2020.

[3] F. Dalpiaz and S. Brinkkemper. Agile requirements engineering: From
user stories to software architectures. In 2021 IEEE 29th International
Requirements Engineering Conference (RE), pages 504–505, 2021.
[4] J. Deshmukh, K. M. Annervaz, S. Podder, S. Sengupta, and N. Dubash.
Towards accurate duplicate bug retrieval using deep learning techniques.
In 2017 IEEE International Conference on Software Maintenance and
Evolution (ICSME), pages 115–124, 2017.

[5] G. Deshpande, Q. Motger, C. Palomares, I. Kamra, K. Biesialska,
X. Franch, G. Ruhe, and J. Ho. Requirements dependency extraction by
integrating active learning with ontology-based retrieval. In 2020 IEEE
28th International Requirements Engineering Conference (RE), pages
78–89, Aug 2020.

[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805, 2018.

[7] S. Fang, Y.-s. Tan, T. Zhang, Z. Xu, and H. Liu. Effective prediction
of bug-ﬁxing priority via weighted graph convolutional networks. IEEE
Transactions on Reliability, 70(2):563–574, 2021.

[8] C. Fitzgerald, E. Letier, and A. Finkelstein. Early failure prediction in
feature request management systems. In 2011 IEEE 19th International
Requirements Engineering Conference, pages 229–238, 2011.

[9] X. Franch, A. Henriksson, J. Ralyt´e, and J. Zdravkovic. Data-driven
agile requirements elicitation through the lenses of situational method
engineering. In 2021 IEEE 29th International Requirements Engineering
Conference (RE), pages 402–407, 2021.

[10] D. Fucci, C. Palomares, X. Franch, D. Costal, M. Raatikainen, M. Stet-
tinger, Z. Kurtanovic, T. Kojo, L. Koenig, A. Falkner, G. Schenner,
F. Brasca, T. M¨annist¨o, A. Felfernig, and W. Maalej. Needs and chal-
lenges for a platform to support large-scale requirements engineering:
A multiple-case study. In Proceedings of the 12th ACM/IEEE Interna-
tional Symposium on Empirical Software Engineering and Measurement,
ESEM ’18, New York, NY, USA, 2018. Association for Computing
Machinery.

[11] J. He, L. Xu, M. Yan, X. Xia, and Y. Lei. Duplicate bug report detection
using dual-channel convolutional neural networks. In Proceedings of the
28th International Conference on Program Comprehension, ICPC ’20,
page 117–127, New York, NY, USA, 2020. Association for Computing
Machinery.

[12] K. Herzig, S. Just, and A. Zeller.

It’s not a bug, it’s a feature: How
In 2013 35th International

misclassiﬁcation impacts bug prediction.
Conference on Software Engineering (ICSE), pages 392–401, 2013.
[13] T. Hey, J. Keim, A. Koziolek, and W. F. Tichy. Norbert: Transfer
learning for requirements classiﬁcation. In 2020 IEEE 28th International
Requirements Engineering Conference (RE), pages 169–179, 2020.
[14] G. Jeong, S. Kim, and T. Zimmermann. Improving bug triage with bug
tossing graphs. ESEC/FSE ’09, page 111–120, New York, NY, USA,
2009. Association for Computing Machinery.

[15] A. Lazar, S. Ritchey, and B. Sharif. Generating duplicate bug datasets.
In Proceedings of the 11th working conference on mining software
repositories, pages 392–395, 2014.

[16] L. Li, Z. Ren, X. Li, W. Zou, and H. Jiang. How are issue units
linked? empirical study on the linking behavior in github. In 2018 25th
Asia-Paciﬁc Software Engineering Conference (APSEC), pages 386–395,
2018.

[17] J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland-Huang. Traceabil-
ity transformed: Generating more accurate links with pre-trained bert
models. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE), pages 324–335, 2021.

[18] G. Lucassen, F. Dalpiaz, J. M. E. van der Werf, S. Brinkkemper, and
D. Zowghi. Behavior-driven requirements traceability via automated
acceptance tests. In 2017 IEEE 25th International Requirements Engi-
neering Conference Workshops (REW), pages 431–434, 2017.

[19] C. M. L¨uders, A. Bouraffa, and W. Maalej. Beyond duplicates: Towards
understanding and predicting link types in issue tracking systems.

In 2022 IEEE/ACM 19th Working Conference on Mining Software
Repositories (MSR). IEEE, 2022.

[20] R. Malhotra, A. Dabas, H. A. S, and M. Pant. A study on machine
learning applied to software bug priority prediction. In 2021 11th In-
ternational Conference on Cloud Computing, Data Science Engineering
(Conﬂuence), pages 965–970, 2021.

[21] T. Merten, M. Falis, P. H¨ubner, T. Quirchmayr, S. B¨ursner, and B. Paech.
Software feature request detection in issue tracking systems. In 2016
IEEE 24th International Requirements Engineering Conference (RE),
pages 166–175, 2016.

[22] T. Merten, D. Kr¨amer, B. Mager, P. Schell, S. B¨ursner, and B. Paech.
Do information retrieval algorithms for automated traceability perform
In International Working
effectively on issue tracking system data?
Conference on Requirements Engineering: Foundation for Software
Quality, pages 45–62. Springer, 2016.

[23] L. Montgomery and D. Damian. What do support analysts know about
their customers? on the study and prediction of support ticket escalations
In 2017 IEEE 25th International
in large software organizations.
Requirements Engineering Conference (RE), pages 362–371, 2017.
[24] L. Montgomery, C. L¨uders, and W. Maalej. An alternative issue tracking
In 2022 IEEE/ACM 19th Working

dataset of public jira repositories.
Conference on Mining Software Repositories (MSR). IEEE, 2022.

[25] A. Nicholson, D. M. Arya, and J. L. Guo.

Traceability network
In 2020
analysis: A case study of links in issue tracking systems.
IEEE Seventh International Workshop on Artiﬁcial Intelligence for
Requirements Engineering (AIRE), pages 39–47, 2020.

[26] A. Nicholson and G. Jin L.C. Issue link label recovery and prediction for
open source software. In 2021 IEEE 29th International Requirements
Engineering Conference Workshops (REW), pages 126–135, 2021.
[27] Q. Perez, P.-A. Jean, C. Urtado, and S. Vauttier. Bug or not bug? that
is the question. In 2021 IEEE/ACM 29th International Conference on
Program Comprehension (ICPC), pages 47–58, 2021.

[28] P. Rempel and P. M¨ader. Preventing defects: The impact of requirements
IEEE Transactions on

traceability completeness on software quality.
Software Engineering, 43(8):777–797, 2017.

[29] T. M. Rocha and A. L. D. C. Carvalho. Siameseqat: A semantic context-
based duplicate bug report detection using replicated cluster information.
IEEE Access, 9:44610–44630, 2021.

[30] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled

version of bert: smaller, faster, cheaper and lighter, 2019.

[31] G. Schermann, M. Brandtner, S. Panichella, P. Leitner, and H. Gall.
In 2015
Discovering loners and phantoms in commit and issue data.
IEEE 23rd International Conference on Program Comprehension, pages
4–14, 2015.

[32] M. Seiler and B. Paech. Using tags to support feature management across
In International
issue tracking systems and version control systems.
Working Conference on Requirements Engineering: Foundation for
Software Quality, pages 174–180. Springer, 2017.

[33] C. Stanik, L. Montgomery, D. Martens, D. Fucci, and W. Maalej.
A simple nlp-based approach to support onboarding and retention in
open source communities. In 2018 IEEE International Conference on
Software Maintenance and Evolution (ICSME), pages 172–182, 2018.
[34] C. A. Thompson, G. C. Murphy, M. Palyart, and M. Gaˇsparic. How
software developers use work breakdown relationships in issue reposi-
tories. In 2016 IEEE/ACM 13th Working Conference on Mining Software
Repositories (MSR), pages 281–285. IEEE, 2016.

[35] M. T. Tomova, M. Rath, and P. M¨ader. Use of trace link types in issue
tracking systems. In Proceedings of the 40th International Conference
on Software Engineering: Companion Proceeedings, ICSE ’18, page
181–182, New York, NY, USA, 2018. Association for Computing
Machinery.

[36] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. An approach to
detecting duplicate bug reports using natural language and execution
In Proceedings of the 30th International Conference on
information.
Software Engineering, ICSE ’08, page 461–470, New York, NY, USA,
2008. Association for Computing Machinery.

[37] J. P. Winkler, J. Gr¨onberg, and A. Vogelsang. Optimizing for recall
In 2019
in automatic requirements classiﬁcation: An empirical study.
IEEE 27th International Requirements Engineering Conference (RE),
pages 40–50, 2019.

[38] G. Xiao, X. Du, Y. Sui, and T. Yue. Hindbr: Heterogeneous information
In 2020 IEEE 31st
network based duplicate bug report prediction.
International Symposium on Software Reliability Engineering (ISSRE),
pages 195–206, Oct 2020.

[39] T. Zimmermann, R. Premraj, N. Bettenburg, S. Just, A. Schroter, and
IEEE Transactions on

C. Weiss. What makes a good bug report?

Software Engineering, 36(5):618–643, 2010.

