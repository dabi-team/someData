Hierarchical Reinforcement Learning Based Video
Semantic Coding for Segmentation

1nd Guangqi Xie, Xin Li, Shiqi Lin, Zhibo Chen*
Unviersity of Science and Technology of China, Hefei, China
{jszjxgq, lixin666, linsq047}@mail.ustc.edu.cn,
chenzhibo@ustc.edu.cn

2nd Li Zhang, Kai Zhang, Yue Li
ByteDance Inc.
{lizhang.idm, yue.li}@bytedance.com
kzhang1981@hotmail.com

2
2
0
2

g
u
A
4
2

]

V

I
.
s
s
e
e
[

1
v
9
2
5
1
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—The rapid development of intelligent tasks, e.g.,
segmentation, detection, and classiﬁcation, etc, has brought an
urgent need for semantic compression, which aims to reduce
the compression cost while maintaining the original semantic
information. However, it is impractical to directly integrate the
semantic metric into the traditional codecs since they cannot
be optimized in an end-to-end manner. To solve this problem,
some pioneering works have applied reinforcement learning to
implement image-wise semantic compression. Nevertheless, the
video semantic compression has not been explored since its
complex reference architectures and compression modes. In this
paper, we take a step forward to video semantic compression and
propose the Hierarchical Reinforcement Learning based task-
driven Video Semantic Coding, named as HRLVSC. Speciﬁcally,
to simplify the complex mode decision of video semantic coding,
we divided the action space into frame-level and CTU-level
spaces in a hierarchical manner, and then explore the best mode
selection for them progressively with the cooperation of frame-
level and CTU-level agents. Moreover, since the modes of video
semantic coding will exponentially increase with the number of
frames in a Group of Pictures (GOP), we carefully investigate
the effects of different mode selections for video semantic coding,
and design a simple but effective mode simpliﬁcation strategy for
it. We have validated our HRLVSC on video segmentation task
with HEVC reference software HM16.19. Extensive experimental
results demonstrated that our HRLVSC can achieve over 39%
BD-rate saving for video semantic coding under the Low Delay
P conﬁguration.

Index Terms—semantic video coding for segmentation, rate-

distortion optimization, hierarchical reinforcement learning

I. INTRODUCTION

intelligent

Deep learning has brought a technological revolution in
visual
tasks, such as image/video object detec-
tion [1]–[4], segmentation [5]–[8], face detection [9], [10],
and person reidentiﬁcation [11]–[13], etc, which enables the
intelligent tasks to be greatly developed and broadly applied
in people’s lives. Meanwhile, billions of images/videos for
intelligent tasks have caused a heavy burden for transmis-
sion and storage. However, the commonly-used image/video
codecs, such as JPEG, AVC [14], HEVC [15], VVC [16],
and related techniques [17], [18] are usually designed and
optimized with pixel-level metrics, such as PSNR [19], and
perceptual metrics [20], [21], which are not optimal for seman-
tic compression. It is crucial to investigate how to reduce the
compression cost while maintaining the semantic information

*Corresponding author

of images/videos according to speciﬁc intelligent tasks (i.e.,
task-driven semantic coding).

Different from the pixel-wise ﬁdelity (e.g., PSNR ) and per-
ceptual ﬁdelity (e.g., MS-SSIM), task-driven semantic ﬁdelity
is hard to be integrated into traditional codecs directly. The
reason lies in that traditional codecs cannot be optimized in an
end-to-end manner such as learning-based coding(e.g., LBHIC
[22]). Furthermore, traditional codecs conduct image/video
compression on pixel level, while semantic information cannot
be represented with simple pixel-wise metrics like PSNR. To
tackle the above challenges, some works [23], [24] take a step
forward to task-driven semantic coding, and then introduce
the reinforcement learning to implement the Rate-Distortion
Optimization (RDO) of semantic coding. Despite that these
studies have achieved great performances on many image
intelligent tasks, (including image segmentation, detection and
classiﬁcation), their methods are only designed for the all-intra
mode of semantic coding instead of uniﬁed video semantic
coding. Therefore, task-driven video semantic coding is still
under-explored.

Compared with image coding, where the compression of
different coding tree units (CTUs) are relatively independent
[25], video compression contains amounts of compression
modes and reference architectures. It is complex and time-
consuming to ﬁnd the optimal mode selection for task-driven
video semantic coding. To simplify the mode decision for
video compression, previous works [26]–[36] have inves-
tigated the effects of the coding modes of current frame
on the following frames in one GOP, and then, model the
distortion or rate propagation with linear function [26]–[32],
deep learning model [33] or reinforcement
learning agent
[34]–[36]. However, the above methods only explored the
relationship between different mode selections and pixel-wise
ﬁdelity, which is not suitable for task-driven semantic ﬁdelity.
In this paper, we take a step forward to task-driven video
semantic coding. To integrate the video semantic ﬁdelity
to the rate-distortion optimization of traditional codecs and
solve the challenges brought by tremendous amounts of mode
selections, we propose the hierarchical reinforcement learning
based task-driven video semantic coding, named as HRLVSC.
Speciﬁcally, we divide the mode selections of video com-
pression into frame level and CTU level in a hierarchical
manner, and then ﬁnd the best mode selection for task-driven

 
 
 
 
 
 
Fig. 1.
Illustration of our proposed HRLVSC. The hierarchically structured network generates optimal task-driven compression modes from parent level to
child level progressively. In the ﬁrst stage, the parent-level agent takes three consecutive frames and corresponding masks as input, extracts deep features and
then predicts the quantization parameters(QP) for each frame. Finer control is at the next stage. The child-level agent extracts corresponding features and
outputs the relative QP around the central frame-level QP for each CTU. Finally, traditional codec takes selected modes from both levels and conducts the
optimal task-driven compression. The actual rate and semantic ﬁdelity of each level are used as rewards to train the agents in a coarse-to-ﬁne manner.

video semantic coding progressively with the cooperation of
frame-level and CTU-level agents. Moreover, to simplify the
mode decision for video semantic coding, we also carefully
investigate the effects of different compression modes for task-
driven video semantic coding. Based on our exploration, we
design an efﬁcient but effective mode simpliﬁcation strategy
for video semantic coding. To validate the effectiveness of
our HRLVSC, we select video segmentation as target task.
Extensive experiments under the Low Delay P conﬁguration
have demonstrated that our HRLVSC can achieve over 39%
BD-rate saving for video semantic coding.

The main contributions of our work can be summarized as

follows:

• As the pioneering work, we propose the hierarchical
reinforcement learning based video semantic coding(i.e.,
HRLVSC) for segmentation, where the complex mode
space is simpliﬁed into frame-level and CTU-level, and
the optimal mode selection is learned with the cooper-
ation of frame- and CTU-level agents in a progressive
manner.

• We carefully explore the correlation between different
mode selections and the semantic ﬁdelity, and propose
an efﬁcient but effective mode simpliﬁcation strategy for
task-driven video semantic coding.

• Extensive experiments on video segmentation task under
low-delay P conﬁguration have demonstrated the supe-
riority of our proposed HRLVSC, which exceeds the
standard software HM16.191 by a BD-rate saving of 39%.
The rest of the paper is organized as follows. In sec. II,
we clarify our task-driven video semantic coding scheme
HRLVSC in detail. Sec. III describes our experimental setting

1Available:https://hevc.hhi.fraunhofer.de/svn/svn HEVCSoftware/tags/HM-

16.19/

and validates the effectiveness of our proposed HRLVSC by
comparing it with the state-of-the-art codecs and a series of
ablation studies. Finally, we conclude this paper in Section IV.

II. PROPOSED METHOD

In this section, we will introduce our hierarchical rein-
forcement learning based video semantic coding scheme (i.e.,
HRLVSC) from the perspective of problem formulation, tech-
nique details and mode simpliﬁcation.

A. Problem formulation

Task-driven video semantic coding aims to reduce the
computation cost while maintaining the semantic information
existed in videos, which can be formulated as:

min Js, Js = D(M) + λs

Tf
(cid:88)

N
(cid:88)

t=1

i=1

Rt,i(M),

(1)

where Js, D(M) and Rt,i(M) are rate-distortion perfor-
mance, the semantic distortion of whole video and the rate of
the ith CTU in the tth frame, respectively. M represents the
selected mode for compression, and λs is the hyperparameter
to adjust the importance of rate and distortion. Tf and N are
the number of frames and CTUs in one frame, respectively. To
ﬁnd the best mode M∗ for Eq. 1, a straightforward method is
to utilize a reinforcement learning agent to explore the optimal
mode adaptively like the work [23]. However, the optional
modes for video semantic compression will exponentially in-
crease with the number of frames and CTUs, which inevitably
prevents the learning of RL agent. To simplify the exploration
space and enables the RL agent to learn the optimal mode
effectively and efﬁciently, we divide the exploration space into
frame level and CTU level, and then, introduce the hierarchical
reinforcement learning to solve the Eq. 1. Speciﬁcally, as

Generate masksGenerate masksFeature extractor……Parent CriticParent ActorChild ActorChild CriticQ-Value2429282928…QP of each frame-2+4Delta QP of CTUs from two regionsParent-level agentChild-levelagentHEVC LowDelayPDownstreamTaskReconstructed videoCorresponding masksTraditionalCodecFeature extractorQ-ValueParent-level rewardCTU-level rewardshown in Eq. 2, we set two-step goals respectively for parent-
level agent and child-level agent. The parent-level agent RLp
∗ in pursuit of
aims to explore the best frame-level mode Mf
minimizing the frame-level rate-distortion Jsf . And the child-
level agent RLc is devoted to minimizing the ﬁnal semantic
rate-distortion performance Js by ﬁnding the best CTU-level
mode Mc

∗ while cooperating with parent-level agent.

Jsf = D(Mf ) + λs

Tf
(cid:88)

t=1

Rt(Mf )

Js = D(Mc|M∗

f ) + λs

Tf
(cid:88)

N
(cid:88)

Rt,i(Mc|M∗
f )

(2)

t=1
f = RLp(min Jsf ), M∗
c = RLc(min Js)

i=1

M∗

With hierarchical reinforcement learning in Eq. 2, we can ob-
∗} for task-driven semantic
∗, Mc
tain the best mode pair {Mf
coding effectively and efﬁciently. In this paper, the frame-
level mode Mf and CTU-level mode Mc are the Quantization
Parameter (QP) of one frame and the relative QP (i.e., ∆QP )
around the central frame-level QP for each CTU.

B. Hierarchical Reinforcement Learning Based Video Seman-
tic Coding for Segmentation

We aim to utilize the parent-level agent and child-level
agent to explore the optimal frame-level mode and CTU-level
mode based on the former, respectively. Therefore, we model
this decision-making problem as a hierarchical MDP process
that aligns well with the nature of hierarchical reinforcement
learning (HRL).

A one-level RL agent commonly models the policy learning
problem as a Markov decision process (MDP) represented with
(S, A, P, R, γ, T ). The RL agent observes the environment
state s ∈ S and relies on the learnable policy π(a |s) : S ×
A → [0, 1] to take an action a ∈ A. Then, the RL agent
receives a step-wise reward r : S × A → R. The environment
moves to next state with a transition function denoted as P :
S × A × S → [0, 1]. γ ∈ (0, 1] is a discount factor and T is
a time horizon. We aim to learn an optimal policy π∗ which
can maximize the accumulative reward R.

Furthermore, HRL contains two-level RL agents, i.e., the
parent-level (frame-level) agent and the child-level (CTU-
level) agent, so as to learn a parent policy πP (aP (cid:12)
(cid:12)sP ) and
a child policy πC(aC (cid:12)
(cid:12)sC, aP ). The parent policy outputs a
parent action aP ∈ AP , which is taken as the condition for the
following decision by the child policy. Thanks to the parent-
level agent which ﬁrst makes decisions at frame-level, the
action space of child-level agent can be effectively reduced.
With a smaller action space, it’s easier for the CTU-level
policy to ﬁnd a more effective strategy.

Here, we give the detailed design of HRL for task-driven
video semantic coding. State: Both the parent-level policy
and the child-level policy need to perceive the content of
the frames. Therefore, given three consecutive frames and
corresponding masks, we concatenate them and utilize the

Fig. 2. Comparison of proposed HRLVSC(red) against HEVC anchor without
rate control(black), HEVC anchor with rate control(blue), RL(green) and
hand-crafted scheme(scatter).

deep features extracted by convolution network as the parent-
level state and the child-level state. Action: The parent-level
agent is responsible for assigning QP for a frame in a coarse
way. QP is a central value within a range. Based on the
decision of the parent-level agent, the child-level agent further
determines (cid:52)QP around the central value for each CTU in
the frame. Reward: The parent-level agent and the child-level
agent target at minimizing the impact of compression on the
performance of downstream network. We deﬁne the reward
functions of parent-level and child-level policies as:

Rwf = Ms (Mf ) − λs

Tf
(cid:88)

t=1

Rt (Mf ) − αf

Rwc = Ms

(cid:0)Mc | P ∗

f

(cid:1) − λs

Tf
(cid:88)

N
(cid:88)

t=1

i=1

Rt,i

(cid:0)Mc | M∗

f

(cid:1) − αc

(3)

where the frame-level reward Rwf is expressed as the sum
of two terms: the task-related ﬁdelity Ms(Mf ), which is
measured by the Mean Intersection over Union (mIOU) for
video segmentation, and the negative sum of rate Rt(Mf )
of each frame. αf and αc are hyperparameters to keep the
initial reward close to zero, and then stimulus the agent to
explore more optimal actions. λs is an adjustable semantic
coding parameter that balances the semantic distortion against
rate. The CTU-level reward Rwc is similar to the frame-level
reward except for the following difference. First, the distortion
term is estimated by the task accuracy Ms after all of the CTU-
level parameters are selected, and the rate term is measured
by the negative sum of the bitrate Rt,i of all CTUs. Second,
considering that the CTU-level action Mc is restricted by
frame-level action Mf , the CTU-level reward is consequently
conditioned on Mf . The two reward functions are consistent
with the objective of the hierarchical policy in Eq. 2.

Given that the actions/modes QP and (cid:52)QP subject to
discrete distribution,
the widely used
therefore, we adopt
Advantage Actor-Critic (A2C) algorithm [37] where the actor

network aims to learn a discrete control policy while the critic
network focuses on estimating the value of state V πθ
ϕ (s). The
parent-level and child-level agents are detailed in Fig. 1.

C. Mode simpliﬁcation

Although the HRL has reduced the action space greatly in
some content, the optional modes for video semantic coding
are still severely unaffordable for training since the expensive
coding time cost. To further cut down the training cost, we aim
to ﬁnd an effective but efﬁcient mode simpliﬁcation strategy
for task-driven video semantic coding. Speciﬁcally, we design
the mode simpliﬁcation strategy from two perspectives, i.e.,
frame-level simpliﬁcation and CTU-level simpliﬁcation.

For frame-level actions, deciding one speciﬁc QP value for
each frame in one GOP is impractical. The complexity will
increase exponentially with the number of frames in one GOP.
To reduce the complexity while keeping the characteristic of
original traditional codecs, we simplify the action space by
only deciding the QPs for the ﬁrst two frames in one GOP. The
QPs of other frames in this GOP are set with the offset used in
traditional codecs. For CTU-level action space, we simplify the
action space based on the characteristic of semantic task, i.e.,
the accuracy of semantic task are mainly associated with the
semantic-related region in one frame. Therefore, we utilize the
semantic mask, which is generated with corresponding task, to
divide the region of one frame into two parts, i.e., semantic-
related region and semantic-unrelated region. Then, we can
allocate two CTU-level QPs respectively for these two regions,
without requiring to decide one QP value for each CTU.

III. EXPERIMENTS

A. Dataset and Implementation Details

To validate the effectiveness of our proposed HRLVSC, we
conduct experiments on video segmentation task under the
commonly-used low-delay P conﬁguration with the reference
software HM 16.19.
Dataset: We construct a video semantic coding dataset to
optimize and validate our HRLVSC framework, named as
TVSC dataset. The dataset contains one video semantic task,
i.e., video segmentation task. Our dataset is based on the
commonly-used dataset DAVIS2017 [38] for video segmen-
tation task. For each video, we resized them as 960 × 544,
and compressed them with the modes used in our optional
action spaces.
Implementation Details: The HRL model is implemented
with Pytorch platform. We train our HRL model with one
NVIDIA 1080Ti GPU for 10000 iterations. The batchsize is
30 and the learning rate is 1e-3 for parent-level agent and 1e-4
for child-level agent.

B. Performance Analysis

1) Compared with HEVC Anchor: To verify the effective-
ness of our HRLVSC scheme, we compare our HRLVSC
with standard HEVC codec HM 16.19 under Low-delay P
conﬁguration. For HEVC, we select the QP from 22 to 37 for
compression as our baseline. For our proposed HRLVSC, we

set λ as 0, 0.05, 0.1, 0.15, 0.2, 0.4, 0.6, 0.8, 1.0, respectively.
The result is shown in Table I. From the table, we can observe
that our proposed HRLVSC scheme outperforms HEVC an-
chor by a bit saving of 39.93%. Even compared with HEVC
anchor with rate control, our HRLVSC can still achieve a bit
rate saving of 20.47%, which demonstrates the effectiveness
of our HRLVSC for task-driven video semantic coding.

TABLE I
BD-RATE AND BD-MIOU COMPARED WITH ANCHOR HM16.19 WITH
FIXED QP

Method
BD-Rate
BD-mIOU

HEVC w/ rate control
-19.46%
0.67%

Proposed HRLVSC
-39.93%
1.50%

RL
-35.54%
1.31%

2) Ablation Study: We conduct the ablation studies from
two perspectives: 1) Replacing the hierarchical reinforcement
learning agents with one reinforcement learning agent used in
[23]. 2) Substituting our HRLVSC with hand-crafted methods
i.e., assigning the higher QP value for semantic-unrelated
CTUs and lower QP value for semantic-related CTUs. The
experimental results are as follows:

Ablation on HRL: As shown in 2, the performance of
applying only one reinforcement learning agent i.e., RL in
Fig. 2, for both frame-level and CTU-level QPs decision, is
lower than employing hierarchical reinforcement learning. The
reasons for that are: 1) The hierarchical structure restricts the
child-level action space, which makes the training procedure
more stable. 2) The action spaces are complex, which is hard
for RL to make decision.

Run time

Proposed

TABLE II
TIME COMPLEXITY

Encoder

Decoder

HRL agent
coding

0.17s
324.25s

1.23s

1.23s

HEVC w/o Rate Control

318.45s

Comparison with hand-crafted schemes In this part, we
will discuss the overwhelming advantage of proposed HRL
against hand-crafted schemes. For video segmentation task,
we can obtain the segmentation masks for frames in a video.
Therefore, the hand-crafted methods can assign different QP
values for each CTU based on the semantic importance i.e.,
the segmentation mask ratio S for each CTU. In other words,
when the mask ratio is higher, the CTU will be assigned
lower QP value for better coding quality. For frame-level
QP i.e., QPf , we keep the original QP value in standard
software HM 16.19 for hand-crafted scheme. For CTU-level
QP value in hand-crafted scheme, we attempt to adopt differ-
ent functions to establish the relationship between QP value
and semantic importance i.e., mask ration S, respectively as
linear, exponential, square, log, and square root functions. The
experimental results are shown in Fig 2, we can ﬁnd that hand-
crafted schemes are far from our proposed HRLVSC, since our
scheme can capture the optimal relationship between QP value
and semantic importance.

3) Complexity Analysis: In this section, we compare our
HRLVSC with standard software HM 16.19 from the perspec-
tive of time complexity. As shown in Table II, our HRLVSC

does not increase any decoding time. For encoding time, our
HRL agents only take about 1.52 seconds for the QP decision
of the whole video with the size of 960x544. It is efﬁcient
and effective to apply our algorithm in the task-driven video
semantic coding.

IV. CONCLUSION

In this paper, we are the ﬁrst

to investigate the task-
driven video semantic coding, and propose the hierarchical
reinforcement learning based scheme HRLVSC for it. Unlike
the image semantic coding, task-driven video semantic cod-
ing contains tremendous reference architectures and coding
modes, which inevitably prevents its development. To tackle
this challenge, we divide the complex coding modes into frame
level and CTU level, and then, introduce the hierarchical RL
agents for them. To further reduce the time complexity for
training, we carefully design a simple but effective mode
simpliﬁcation strategy for task-driven video semantic coding.
Extensive experiments on video segmentation task under low-
delay P conﬁguration have validated the effectiveness of our
scheme. We will extend our HRLVSC to more video semantic
tasks and more conﬁguration of video coding in future work.

REFERENCES

[1] L. Jiao, R. Zhang, F. Liu, S. Yang, B. Hou, L. Li, and X. Tang, “New
generation deep learning for video object detection: A survey,” IEEE
Transactions on Neural Networks and Learning Systems, 2021.

[2] X. Zhu, J. Dai, L. Yuan, and Y. Wei, “Towards high performance video
object detection,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018, pp. 7210–7218.

[3] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from
video tubelets with convolutional neural networks,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016,
pp. 817–825.

[4] S. Wang, Y. Zhou, J. Yan, and Z. Deng, “Fully motion-aware network
for video object detection,” in Proceedings of the European conference
on computer vision (ECCV), 2018, pp. 542–557.

[5] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, “Modular interactive video
object segmentation: Interaction-to-mask, propagation and difference-
aware fusion,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021, pp. 5559–5568.

[6] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix´e, D. Cremers, and
L. Van Gool, “One-shot video object segmentation,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2017,
pp. 221–230.

[7] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and
A. Sorkine-Hornung, “A benchmark dataset and evaluation methodology
for video object segmentation,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 724–732.
[8] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. Sorkine-
Hornung, “Learning video object segmentation from static images,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 2663–2672.

[9] A. Kumar, A. Kaur, and M. Kumar, “Face detection techniques: a
review,” Artiﬁcial Intelligence Review, vol. 52, no. 2, pp. 927–948, 2019.
[10] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “Wider face: A face detection
benchmark,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 5525–5533.

[11] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable
person re-identiﬁcation: A benchmark,” in Proceedings of the IEEE
international conference on computer vision, 2015, pp. 1116–1124.
[12] E. Ahmed, M. Jones, and T. K. Marks, “An improved deep learning
architecture for person re-identiﬁcation,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2015, pp. 3908–
3916.

[13] X. Jin, C. Lan, W. Zeng, G. Wei, and Z. Chen, “Semantics-aligned
representation learning for person re-identiﬁcation,” in Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, vol. 34, no. 07, 2020,
pp. 11 173–11 180.

[14] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview
of the h. 264/avc video coding standard,” IEEE Transactions on circuits
and systems for video technology, vol. 13, no. 7, pp. 560–576, 2003.

[15] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of
the high efﬁciency video coding (hevc) standard,” IEEE Transactions
on circuits and systems for video technology, vol. 22, no. 12, pp. 1649–
1668, 2012.

[16] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-
R. Ohm, “Overview of the versatile video coding (vvc) standard and
its applications,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 31, no. 10, pp. 3736–3764, 2021.

[17] X. Li, S. Sun, Z. Zhang, and Z. Chen, “Multi-scale grouped dense net-
work for vvc intra coding,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops, 2020, pp. 158–
159.

[18] B. Li, X. Li, Y. Lu, S. Liu, R. Feng, and Z. Chen, “Hst: Hierarchical swin
transformer for compressed image super-resolution,” in Proceedings of
the European Conference on Computer Vision (ECCV) Workshops, 2022.
[19] N. Instruments, “Peak signal-to-noise ratio as an image quality metric,”

2013.

[20] J. Liu, X. Li, Y. Peng, T. Yu, and Z. Chen, “Swiniqa: Learned swin
distance for compressed image quality assessment,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 1795–1799.

[21] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The
unreasonable effectiveness of deep features as a perceptual metric,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 586–595.

[22] Y. Wu, X. Li, Z. Zhang, X. Jin, and Z. Chen, “Learned block-based
hybrid image compression,” IEEE Transactions on Circuits and Systems
for Video Technology, 2021.

[23] X. Li, J. Shi, and Z. Chen, “Task-driven semantic coding via reinforce-
ment learning,” IEEE Transactions on Image Processing, vol. 30, pp.
6307–6320, 2021.

[24] J. Shi and Z. Chen, “Reinforced bit allocation under task-driven semantic
distortion metrics,” in 2020 IEEE international symposium on circuits
and systems (ISCAS).
IEEE, 2020, pp. 1–5.

[25] Y. Li, B. Li, D. Liu, and Z. Chen, “A convolutional neural network-
based approach to rate control in hevc intra coding,” in 2017 IEEE
Visual Communications and Image Processing (VCIP).
IEEE, 2017,
pp. 1–4.

[26] L. Li, B. Li, H. Li, and C. W. Chen, “λ-domain optimal bit allocation
algorithm for high efﬁciency video coding,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 28, no. 1, pp. 130–142,
2016.

[27] Y. Mao, M. Wang, S. Wang, and S. Kwong, “High efﬁciency rate control
for versatile video coding based on composite cauchy distribution,” IEEE
Transactions on Circuits and Systems for Video Technology, 2021.
[28] J. He, E.-H. Yang, F. Yang, and K. Yang, “Adaptive quantization param-
eter selection for h. 265/hevc by employing inter-frame dependency,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 28, no. 12, pp. 3424–3436, 2017.

[29] S. Li, C. Zhu, Y. Gao, Y. Zhou, F. Dufaux, and M.-T. Sun, “Lagrangian
multiplier adaptation for rate-distortion optimization with inter-frame
dependency,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 26, no. 1, pp. 117–129, 2015.

[30] A. Fiengo, G. Chierchia, M. Cagnazzo, and B. Pesquet-Popescu, “Rate
allocation in predictive video coding using a convex optimization
framework,” IEEE Transactions on Image Processing, vol. 26, no. 1,
pp. 479–489, 2016.

[31] Y. Li, Z. Liu, Z. Chen, and S. Liu, “Rate control for versatile video
coding,” in 2020 IEEE International Conference on Image Processing
(ICIP).

IEEE, 2020, pp. 1176–1180.

[32] Y. Gao, C. Zhu, S. Li, and T. Yang, “Temporally dependent rate-
distortion optimization for low-delay hierarchical video coding,” IEEE
Transactions on Image Processing, vol. 26, no. 9, pp. 4457–4470, 2017.
[33] F. Liu, G. Cao, D. Yang, Y. Zha, Y. Zhang, and X. Liu, “An lstm based
rate and distortion prediction method for low-delay video coding,” in
Proceedings of the ACM Multimedia Asia, 2019, pp. 1–6.

[34] M. Zhou, X. Wei, S. Kwong, W. Jia, and B. Fang, “Rate control method
based on deep reinforcement learning for dynamic video sequences in
hevc,” IEEE Transactions on Multimedia, vol. 23, pp. 1106–1121, 2020.
learning
for hevc/h. 265 intra-frame rate control,” in 2018 IEEE International
Symposium on Circuits and Systems (ISCAS).

[35] J.-H. Hu, W.-H. Peng, and C.-H. Chung, “Reinforcement

IEEE, 2018, pp. 1–5.

[36] H. Guo, C. Zhu, S. Li, and Y. Gao, “Optimal bit allocation at frame level
for rate control in hevc,” IEEE Transactions on Broadcasting, vol. 65,
no. 2, pp. 270–281, 2018.

[37] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” in ICML, 2016.

[38] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel´aez, A. Sorkine-Hornung,
and L. Van Gool, “The 2017 davis challenge on video object segmen-
tation,” arXiv preprint arXiv:1704.00675, 2017.

