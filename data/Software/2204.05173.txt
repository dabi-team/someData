2
2
0
2

r
p
A
4
1

]

G
L
.
s
c
[

2
v
3
7
1
5
0
.
4
0
2
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2022

MACHINE LEARNING STATE-OF-THE-ART WITH UN-
CERTAINTIES

Peter Steinbach, Felicita Gernhardt, Mahnoor Tanveer, Steve Schmerler, Sebastian Starke ∗
Department of Computational Science
Core Facility for Information Services and Computing
Helmholtz-Zentrum Dresden-Rossendorf
{p.steinbach, f.gernhardt, m.tanveer, s.schmerler, s.starke}@hzdr.de

ABSTRACT

With the availability of data, hardware, software ecosystem and relevant skill sets,
the machine learning community is undergoing a rapid development with new
architectures and approaches appearing at high frequency every year. In this arti-
cle, we conduct an exemplary image classiﬁcation study in order to demonstrate
how conﬁdence intervals around accuracy measurements can greatly enhance the
communication of research results as well as impact the reviewing process. In ad-
dition, we explore the hallmarks and limitations of this approximation. We discuss
the relevance of this approach reﬂecting on a spotlight publication of ICLR22. A
reproducible workﬂow1 is made available as an open-source adjoint to this publi-
cation. Based on our discussion, we make suggestions for improving the authoring
and reviewing process of machine learning articles.

1

INTRODUCTION

The machine learning (ML) community has established a routine during the publication life cycle
in which new approaches are ﬁrst validated on benchmark datasets in experiments. During paper
review, the viability of propositions is reasoned based on these experiments and compared to cur-
rent state-of-the-art results. Model comparison is henceforth a common task during the review and
authoring process.

With the availability of training data – like ImageNet by Russakovsky et al. (2015), qoca by Reddy
et al. (2018), bbbc by Lehmussola et al. (2008) – in conjunction with available hardware and hu-
man software skills, established conferences are ﬂooded by submissions. Studies like Xin report
submission numbers above 1000 every year for the major conferences in the ﬁeld. This demand has
called for more reviewing capacity provided by the conferences in turn. Given the acceptance rates
of submitted articles, new approaches for common ML tasks are available every year.

In parallel, the spread of ML techniques is still ongoing in all domains of society, industry and
academia. In these application domains, new challenges of dataset availability and heterogeneity
await any data scientist or ML practitioner who wants to exploit ML in her or his ﬁeld. These prac-
titioners need to decide where to put their time. Having publications being released with publicly
available code reduces the barrier to try out new architectures. Model comparison is henceforth a
common task for practitioners as well.

In the following discussion, we offer our suggestions for evaluation of ML architectures to eventually
augment the process of model comparison:

• We suggest a minimal standard to report ML experiments (based on an analysis available

as an open-source repository in Steinbach (2022)).

• When evaluating claims based on global performance metrics, we suggest uncertainty (or

variance) and its approximation as a ﬁrst-class citizen in the review process.

∗ML Evaluation Standards Workshop at ICLR 2022.
1Our workﬂow is available on github.com/psteinb/sota on uncertainties.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2022

2 EXPERIMENTS

Rather than providing a meta-analysis of already published results and discussions, we provide a
representative example analysis at ﬁrst. We use this to put the discussion not only in the perspective
of the reviewer but also in the perspective of the submitting authors. Given our motivation from
Section 1, we believe that both sides should be heard in this discussion.

We base our argumentation on the task of image classiﬁcation as an example. We use the popular
timm library (Wightman, 2019) to train and run image classiﬁcation models.
In order to allow
reproducible results, all steps in our analysis are encoded in an automated snakemake (M¨older et al.,
2021) workﬂow available as open-source (Steinbach, 2022). All details on our experiments are given
in the Appendix Section 5.1. Our reproducible workﬂow is illustrated in Appendix Section 5.3.

As accuracy is the predominant metric in use in image classiﬁcation published at ML conferences,
we use it here as a representative example and report it after training for 80 epochs.

To illustrate our point we set out to compare the performance of three different architectures. We
report their measured accuracy values (computed on the holdout validation set) side by side to learn
which architecture offers the highest performance as is often done in publications. To obtain a more
comprehensive view, we take the discussion in section 1.7 of Raschka (2018) and compute a conﬁ-
dence interval around the accuracy point estimate under the normal approximation. The core idea
behind this approach considers accuracy as a random variable which complies to a binomial distri-
bution over the holdout folds available. The derivation of Raschka (2018) concludes the estimated
conﬁdence interval to be

ˆσ = z

(cid:114) 1

nholdout

ACCholdout (1 − ACCholdout)

(1)

In eq. (1), z denotes the 1 − α
2 error quantile of a standard normal distribution with α being the
error quantile itself. We chose a one-sigma conﬁdence interval of 68.1% and used z = 1 (a 95%
conﬁdence interval requires z = 1.96). nholdout refers to the number of samples in the holdout
validation set and ACCholdout to the accuracy calculated therein.

(a) Point estimates

(b) Point estimates and conﬁdence intervals

Figure 1: Accuracy estimates on 10-class image classiﬁcation for three different ML architectures.
Figure 1a shows point estimates only. Figure 1b shows point estimates and conﬁdence intervals
computed from sample standard deviations of accuracies in 20-fold cross-validation.

From ﬁg. 1 we learn that the conﬁdence estimates of both ResNe(X)T architectures are very close
to each other. One-sigma conﬁdence intervals of both models almost contact each other. Given the
fact that these are one-sigma conﬁdence intervals for accuracy, the performance difference between
the two ResNe(X)T models can and should never be reported as signiﬁcant. In contrast, the Vision
Transformer implementation offers distinctly inferior accuracy. Note, a rigorous conclusion whether
reported accuracies are signiﬁcantly different from each other requires mathematical tools such as
McNemar’s test (McNemar, 1947) or similar which are beyond the reach of this article.

To empirically conﬁrm that the approximation of uncertainties stated above does hold, we created
20 stratiﬁed folds of the imagenette dataset and repeated training for 80 epochs again on all
three model architectures as suggested in Raschka (2018). Each fold offered a hold-out dataset
of 670 images of equal proportion for each class. In addition, we performed the training with six
different seeds on all available folds in order to include effects from initialization of the network
weights as reported in Zhuang et al. (2021). The results of this analysis are available in Appendix

2

Published as a conference paper at ICLR 2022

Section 5.2.2. The latter study provides us conﬁdence that Equation (1) is a valid approximation to
describe variability of obtained accuracy measurements in practice. At the same time, we highlight
limitations of this approach.

Uncertainty estimates enhance display of ML results and help judge the degree of progress offered.
The approximation technique illustrated in this section provides an author with a time and hardware
runtime effective technique to quantify uncertainty estimates of ML metrics perceived as random
variables. Finally, having uncertainties displayed provides an intuition to readers and reviewers on
how robust the offered ﬁndings are compared to the established state-of-the-art in the ﬁeld.

3 APPLICATION

In order to support our argumentation, we would like to reﬂect on a peer-reviewed paper published
in ICLR22. It was awarded as a spotlight paper. The authors have provided their code on a public
platform (Park & Kim, 2022b) and all review comments have been made available (Park & Kim,
2022c). We like to point out that this is a singular piece of evidence and hence should not serve to
represent the entire ensemble of ML articles submitted to any ML conference.

In the paper “How Do Vision Transformers Work?” (Park & Kim, 2022a), the main discussion re-
volves around the observation that vision transformer networks (exploiting multi-head self-attention,
MSA) for image classiﬁcation appear to smooth the loss landscape. The article proposes a new net-
work design based on this (AlterNet) and claims that AlterNet outperforms CNNs.

(a) No uncertainties

(b) Including uncertainties

Figure 2: Reproduction of ﬁgure 12a from Park & Kim (2022a) (left). Augmentation of the same
ﬁgure with estimated accuracy calculated using eq. (1) using a one-sigma 68.2% (colored) and two-
sigma 95% (grey) conﬁdence interval (right). Data to reproduce these ﬁgures was obtained by using
Rohatgi (2021) on the ﬁgures from the preprint PDF.

Figure 2 reproduces and augments Figure 12a from Park & Kim (2022a). In the original text, the
article concludes:

Figure 12a reports the accuracy of Alter-ResNet-50, which replaces the Conv
blocks in ResNet-50 with local MSAs . . . As expected, MSAs in the last stage
(c4) signiﬁcantly improve the accuracy. Surprisingly, an MSA in 2nd stage (c2)
improves the accuracy, while two or more MSAs in the 3rd stage (c3) reduce it.
In conclusion, MSAs at the end of a stage play an important role in prediction.

Augmenting the results with approximated conﬁdence intervals uncovers and underlines the intrinsic
uncertainty of the presented results in visual form. As these accuracies were obtained based on a
single holdout set, the assumptions of the normal approximation of eq. (1) hold.

Further, the performance gain using MSA elements (any entry with MSA > 0) versus a classical
network design without them (MSA = 0) is reduced when taking the uncertainties into account. It

3

Published as a conference paper at ICLR 2022

is surprising to note that none of the four published reviews commented on this shortcoming of the
discussion in the paper. This may indicate that AlterNet may not offer distinct performance gains as
suggested in the text above. When considering the 95% conﬁdence intervals (grey shaded intervals
in ﬁg. 2b), all conﬁdence intervals for MSA > 0 overlap with the one for MSA = 0. From our point
of view, this has a considerable impact on the conclusions brought forward by this article.

4 CONCLUSION

Reporting known uncertainties on metrics obtained in ML experiments is an important but challeng-
ing task. Nonetheless it is indispensable for the evaluation of ML experiments. Uncertainties are
essential especially as the progress of improvements in the ﬁeld has slowed down in recent years
(Bouthillier et al., 2021). The core challenge does not only reside in the computational expense
required (see multi-fold, multi-seed experiments performed and summarized in ﬁg. 5), but also in
the statistical treatment of correlations (which we did not discuss in this article).

In this spirit, we showed that approximations to uncertainties can be applied to standard use cases
often seen in the community. In this way, they provide an important tool for reviewers and readers
alike to judge the presented results and relate their degree of progress. Uncertainty estimates help to
establish trustworthiness in any author’s results.

We emphasize that reporting uncertainties is only a ﬁrst step towards rigorous model comparison
using hypothesis testing based on McNemar’s test (McNemar, 1947) or similar approaches. While
we believe that hypothesis testing based model comparison with high statistical power should be
the norm, we acknowledge its practical implications. Hence, we strongly encourage the use of
uncertainties and their approximations as an intermittent and essential step towards this goal in
order to support reviewers and authors alike.

We suggest that conferences and journals invest more effort in on-boarding reviewers to be aware
of these statistical tools. Moreover, journals and conferences should make sure that authors report
the central parameters required for computing such approximations. In an extreme case, articles
could also be desk rejected if they fall short of delivering parameters for quantifying uncertainties.
In the best of all worlds, the community could suggest conferences and journals to make reporting
uncertainties a requirement for submission. Along this line of thought, the community could deﬁne
a single or tiered levels of standards with degrees of conﬁdence such intervals should encode. With
such measures, we hope that our community can promote uncertainty to a ﬁrst class citizen in the
evaluation of ML models and experiments.

Last but not least, we have demonstrated that such investigations are possible with today’s soft-
ware tools to an extent that offers a scalable software environment allowing authors to use cloud
or HPC infrastructures (e.g. for model training). At the same time, this technology yields prove-
nance information of results from the training data to the ﬁnal plot of a publication. Publishing
software alongside articles is a good ﬁrst step towards reproducibility. We believe that the use of
such workﬂow software should be encouraged in the submission process to ensure reproducibility
and transparency at the same time.

ACKNOWLEDGMENTS

The authors gratefully acknowledge the computing time granted on the supercomputer JUSUF (Vi-
eth, 2021) at Forschungszentrum J¨ulich. This work is supported by the Helmholtz Association
Initiative and Networking Fund under the Helmholtz AI platform grant.

REFERENCES

Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Troﬁmov, Brennan Nichyporuk, Justin
Szeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vin-
cent Michalski, Dmitriy Serdyuk, Tal Arbel, Chris Pal, Ga¨el Varoquaux, and Pascal Vincent.
Accounting for variance in machine learning benchmarks. CoRR, abs/2103.03098, 2021. URL
https://arxiv.org/abs/2103.03098.

4

Published as a conference paper at ICLR 2022

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale, 2020. URL https://arxiv.org/abs/2010.11929.

Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classiﬁcation with convolutional neural networks, 2018. URL https://arxiv.org/
abs/1812.01187.

Jeremy Howard et al.

Imagenette. URL https://github.com/fastai/imagenette.

Commit at time of writing.

Antti Lehmussola, Pekka Ruusuvuori, Jyrki Selinummi, Tiina Rajala, and Olli Yli-Harja. Synthetic
images of high-throughput microscopy for validation of image analysis methods. Proceedings of
the IEEE, 96(8):1348–1360, 2008. doi: 10.1109/JPROC.2008.925490.

Quinn McNemar. Note on the sampling error of the difference between correlated proportions or

percentages. Psychometrika, 12(2):153–157, 1947.

Felix M¨older, Kim Philipp Jablonski, Brice Letcher, Michael B Hall, Christopher H Tomkins-Tinch,
Vanessa Sochat, Jan Forster, Soohyun Lee, Sven O Twardziok, Alexander Kanitz, et al. Sus-
tainable data analysis with snakemake. F1000Research, 10, 2021. doi: 10.12688/f1000research.
29032.2.

Namuk Park and Songkuk Kim. How do vision transformers work?, 2022a. URL https://

arxiv.org/abs/2202.06709.

Namuk Park and Songkuk Kim. Code for the preprint ”how do vision transformers work?”, 2022b.
URL https://github.com/xxxnell/how-do-vits-work. Commit at time of writ-
ing.

Namuk Park and Songkuk Kim. Review of the preprint ”how do vision transformers work?”, 2022c.

URL https://openreview.net/forum?id=D78Go4hVcxO.

Sebastian Raschka. Model evaluation, model selection, and algorithm selection in machine learning.

2018. URL http://arxiv.org/abs/2103.03098.

Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering
challenge. CoRR, abs/1808.07042, 2018. URL http://arxiv.org/abs/1808.07042.

Ankit Rohatgi. Webplotdigitizer: Version 4.5, 2021. URL https://automeris.io/

WebPlotDigitizer.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Peter Steinbach. Sota with uncertainties repository, 2022. URL https://github.com/

psteinb/sota_on_uncertainties/.

Benedikt von St Vieth. Jusuf: Modular tier-2 supercomputing and cloud infrastructure at J¨ulich
supercomputing centre. Journal of large-scale research facilities JLSRF, 7:179, 2021. doi: 10.
17815/jlsrf-7-179.

Ross Wightman.

Pytorch image models.

pytorch-image-models, 2019. version 0.5.4.

https://github.com/rwightman/

Saining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated resid-
ual transformations for deep neural networks. CoRR, abs/1611.05431, 2016. URL http:
//arxiv.org/abs/1611.05431.

Li Xin. Statistics of acceptance rate for the main AI conferences. URL https://github.com/

lixin4ever/Conference-Acceptance-Rate. Commit at time of writing.

5

Published as a conference paper at ICLR 2022

Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. Randomness in neural
network training: Characterizing the impact of tooling. CoRR, abs/2106.11872, 2021. URL
https://arxiv.org/abs/2106.11872.

5 APPENDIX

5.1 EXPERIMENTAL DETAILS

As a training dataset, we use imagenette (Howard et al.), a subset of ImageNet (Russakovsky
et al., 2015) which amounts to ntrain = 9469 training images of 320 × 320 pixels encoded as
RGBA JPEG ﬁles available in 10 distinct object classes. The holdout validation set sums up to
nholdout = 3925 images.

We use the popular timm library (Wightman, 2019) to train and run three image classiﬁcation mod-
els. In order to allow reproducible results, all steps in the analysis are encoded in an automated
snakemake (M¨older et al., 2021) workﬂow.

The three architectures of choice used in section 2 are as follows:

• restnet is a ResNet architecture (He et al., 2018), timm model: resnet50
• resnext is a ResNext architecture (Xie et al., 2016), timm model: resnext50 32x4d
(Dosovitskiy et al., 2020), timm model:
• vit small is a Vision transformer

vit small patch32 224

All have been trained using the default parameters in timm version 0.5.4 without using pretrained
weights.

To empirically conﬁrm that the approximation of uncertainties stated above does hold, we created 20
stratiﬁed folds of the imagenette dataset and repeated training for 80 epochs again on all three
model architectures as suggested in Raschka (2018). Each fold offered a hold-out dataset of 670
images of equal proportion for each class. In addition, we performed the training with six different
seeds on all available folds.

All code to reproduce experiments is made available (Steinbach, 2022).

5.2 MULTI-SEED MULTI-FOLD EXPERIMENTS

5.2.1 DISTRIBUTION OF ACCURACIES

In this section, we provide more insights into the distribution of accuracy measurements across seeds
and folds.

As suggested in Raschka (2018), the histograms obtained in ﬁg. 3 reveal an asymmetric structure
while being centered at the reported mean values. Especially ﬁg. 3b reveals interesting distribution
shapes unlikely matching a perfect normal distribution. This underlines the nature of the approx-
imation. We suggest to make this aspect the subject of a future study in order to learn if e.g. the
asymmetric shape in ﬁg. 3b is a statistic effect (combination of multiple binomial distributions in-
cluding correlations as the training data in folds overlaps) or systematic with respect to the loss
landscape optimization.

Figure 4 supports the notion expressed earlier, that neither ensembles at the presented size expose
strong similarity to a Gaussian distribution. The quantiles obtained from the samples are apart the
ideal correlation with the expected theoretical percentiles given a Gaussian distribution at the same
mean µ and variance σ.

5.2.2 COMPARISON OF SAMPLE STATISTICS AND APPROXIMATED CONFIDENCE INTERVALS

We consider uncertainties introduced by seeds as a placeholder for more effects discussed by Zhuang
et al. (2021). We highlight at this point that such a procedure is impractical for realistic scenarios
as it would require an insurmountable amount of computing resources, i.e. 360 times the runtime.

6

Published as a conference paper at ICLR 2022

(a) One seed

(b) Six different seeds

Figure 3: Histogram of accuracy estimates for 10-class image classiﬁcation for three different ML
architectures across 20 folds and 6 seeds. Figure 3a shows results for one seed only (15 bins due to
low statistics). Figure 3b shows results across all 6 seeds (25 bins).

However, this provides the unique opportunity to compare a (realistic) single seed case versus a
(unrealistic but desirable) multi-seed analysis.

We repeat the analysis alongside ﬁg. 1 based on the estimates of all available folds and the obtained
conﬁdence intervals in order to motivate a judgment, whether approximate metric uncertainties can
aid the reviewing process and how close they come to a representative estimate.

Figure 5 aspires to compare the uncertainty estimates obtained using from two different methods.
One approach obtains uncertainties from fold samples using the sample standard deviation of accu-
racy measurements. The second approach uses approximation described in eq. (1). In the single seed
case (which is the norm in practice), uncertainty approximations in ﬁg. 5 are inline with fold sample
estimates. This observation reproduces ﬁndings in previous reports (Bouthillier et al., 2021). This
supports our conclusion that such approximations can offer a time and compute efﬁcient method to
obtain uncertainty estimates.

In the case of measurements entering from any seed, the uncertainty approximation from eq. (1)
appears to underestimate consistently the accuracy error estimate from fold samples. This is not
unexpected. From our point of view, this demonstrates existing limitations of the method. We
attribute this shortcoming to a violation of the underlying approximation assumptions of eq. (1). In
this multi-seed multi-fold scenario, the sample of accuracy measurements contain another source of
uncertainty in addition to the one described by eq. (1) - for example stochastic effects introduced
from several differently seeded training runs.

5.3 WORKFLOW AND PROVENANCE OF RESULTS

The design of this analysis using an automated snakemake (M¨older et al., 2021) workﬂow bears
several advantages. This technology allow execution of training and analysis on GPU or CPU and
local or distributed hardware alike.
It also offers the full provenance of a produced ﬁgure with
respect to the training data.

We explicitly highlight this functionality here as we believe that such infrastructure can support the
review of ML workﬂows. Figure 6 depicts the directed acyclic graph that lead to the creation of

7

Published as a conference paper at ICLR 2022

(a) One seed

(b) Six different seeds

Figure 4: Quantile-Quantile plot of accuracy estimates for 10-class image classiﬁcation for three
different ML architectures across 20 folds and 6 seeds. Figure 4a shows results for one seed only
(15 bins due to low statistics). Figure 4b shows results across all 6 seeds (25 bins). The red dashed
line marks the perfect correlation between a theoretical Gaussian distribution at the same µ, σ as
obtained from the measured accuracy ensembles.

Figure 5: Comparison of fold sample based uncertainty with approximated uncertainty using eq. (1).
Each estimate was obtained for one seed (42) or any seed available (total 6 seeds). The uncertainty
plotted for seed 42 was obtained using the approximation in eq. (1). The uncertainty plotted for all
seeds was obtained using the sample standard deviation.

Figure 5 starting from the imagenette2-320 dataset. We believe that jupyter notebooks are
very helpful for practical needs during exploration.

They however often fall short when used as a platform for workﬂows or when sharing such work-
ﬂows. Given tooling and visualization techniques, having automated workﬂows as a standard in
the authoring process can greatly enhance the transparency of studies and therefor make reviewing
easier if not possible at all.

All code to reproduce experiments is made available (Steinbach, 2022).

8

Published as a conference paper at ICLR 2022

.

n
w
o
h
s

e
r
a
0
1
d
l
o
f
n
o
t
n
e
d
n
e
p
e
d

s
k
s
a
t

y
l
n
o

,
y
t
i
l
i
b
i
s
i
v

r
o
F

.
5

e
r
u
g
i

F
f
o

n
o
i
t
a
e
r
c

e
h
t

o
t

d
a
e
l

t
a
h
t

s
k
s
a
t

f
o

h
p
a
r
g
c
i
l
c
y
c
a

d
e
t
c
e
r
i
d

f
o

n
o
i
t
r
o
P

:
6
e
r
u
g
i
F

9

compare_meanstd_seed42_noseed_approx_accuraciesmean_std_across_folds_and_seedsimagenette2_metrics_summarychkpt:lastmean_std_across_foldscreate_tablesdataset:imagenette2-320ﬁll_foldfoldid:10imagenette2_vit_small_defaultfoldstem:fold-09seedval:1332metrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastimagenette2_resnext50_defaultfoldstem:fold-10seedval:42imagenette2_resnet50_defaultfoldstem:fold-10seedval:42imagenette2_vit_small_defaultfoldstem:fold-10seedval:42imagenette2_resnext50_defaultfoldstem:fold-10seedval:1328imagenette2_resnext50_defaultfoldstem:fold-10seedval:1329imagenette2_resnext50_defaultfoldstem:fold-10seedval:1330imagenette2_resnext50_defaultfoldstem:fold-10seedval:1331imagenette2_resnext50_defaultfoldstem:fold-10seedval:1332imagenette2_resnet50_defaultfoldstem:fold-10seedval:1328imagenette2_resnet50_defaultfoldstem:fold-10seedval:1329imagenette2_resnet50_defaultfoldstem:fold-10seedval:1330imagenette2_resnet50_defaultfoldstem:fold-10seedval:1331imagenette2_resnet50_defaultfoldstem:fold-10seedval:1332imagenette2_vit_small_defaultfoldstem:fold-10seedval:1328imagenette2_vit_small_defaultfoldstem:fold-10seedval:1329imagenette2_vit_small_defaultfoldstem:fold-10seedval:1330imagenette2_vit_small_defaultfoldstem:fold-10seedval:1331imagenette2_vit_small_defaultfoldstem:fold-10seedval:1332metrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastmetrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastmetrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastmetrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastmetrics_from_topkidsarch:resnext50chkpttype:lastimagenette2_resnext50_inference_lastmetrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:resnet50chkpttype:lastimagenette2_resnet50_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastmetrics_from_topkidsarch:vit_smallchkpttype:lastimagenette2_vit_small_inference_lastﬁlter_through_wildcardﬁltercol:seedﬁlterval:42