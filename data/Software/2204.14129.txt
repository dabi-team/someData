2
2
0
2

r
p
A
9
2

]

C
D
.
s
c
[

1
v
9
2
1
4
1
.
4
0
2
2
:
v
i
X
r
a

Met: Model Checking-Driven Explorative Testing of CRDT
Designs and Implementations

Yuqi Zhang, Yu Huang∗, Hengfeng Wei∗, Xiaoxing Ma
State Key Laboratory for Novel Software Technology, Nanjing University
cs.yqzhang@gmail.com,{yuhuang,hfwei,xxm}@nju.edu.cn

ABSTRACT

Internet-scale distributed systems often replicate data at multiple
geographic locations to provide low latency and high availability,
despite node and network failures. According to the CAP theorem,
low latency and high availability can only be achieved at the cost
of accepting weak consistency. The Conflict-free Replicated Data
Type (CRDT) is a framework that provides a principled approach to
maintaining eventual consistency among data replicas. CRDTs have
been notoriously difficult to design and implement correctly. Subtle
deep bugs lie in the complex and tedious handling of all possible
cases of conflicting data updates. We argue that the CRDT design
should be formally specified and model-checked, to uncover deep
bugs which are beyond human reasoning. The implementation
further needs to be systematically tested. On the one hand, the
testing needs to inherit the exhaustive nature of the model checking
and ensures the coverage of testing. On the other hand, the testing
is expected to find coding errors which cannot be detected by design
level verification.

Towards the challenges above, we propose the Model Checking-
driven Explorative Testing (Met) framework. At the design level,
Met uses TLA+ to specify and model check CRDT designs. At
the implementation level, Met conducts model checking-driven
explorative testing, in the sense that the test cases are automati-
cally generated from the model checking traces. The system execu-
tion is controlled to proceed deterministically, following the model
checking trace. The explorative testing systematically controls and
permutes all nondeterministic choices of message reorderings.

We apply Met in our practical development of CRDTs. The bugs
in both designs and implementations of CRDTs are found. As for
bugs which can be found by traditional testing techniques, Met
greatly reduces the cost of fixing the bugs. Moreover, Met can find
subtle deep bugs which cannot be found by existing techniques at
a reasonable cost. Based on our practical use of Met, we discuss
how Met provides us with sufficient confidence in the correctness
of our CRDT designs and implementations.

PVLDB Reference Format:
Yuqi Zhang, Yu Huang∗, Hengfeng Wei∗, Xiaoxing Ma. Met: Model
Checking-Driven Explorative Testing of CRDT Designs and
Implementations. PVLDB, 14(1): XXX-XXX, 2020.
doi:XX.XX/XXX.XX

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

∗ Corresponding author.

PVLDB Artifact Availability:

The source code, data, and/or other artifacts have been made available at
https://github.com/elem-azar-unis/CRDT-Redis/tree/master/MET.

1 INTRODUCTION

Large-scale distributed systems often resort to replication tech-
niques to achieve fault-tolerance and load distribution [20, 23, 44].
For a large class of applications, user-perceived latency and overall
service availability are widely regarded as the most critical factors.
Thus, many distributed systems are designed for low latency and
high availability in the first place and resort to eventual consistency
due to the CAP theorem [18, 29]. Eventual consistency allows repli-
cas of some data type to temporarily diverge, and making sure
these replicas will eventually converge to the same state in a deter-
ministic way [44]. The Conflict-free Replicated Data Type (CRDT)
framework provides a principled approach to maintaining eventual
consistency [20, 44]. CRDTs are key components in modern geo-
replicated systems, such as Riak [14], Redis-Enterprise [13], and
Cosmos DB [1].

It has been notoriously difficult to correctly design and imple-
ment CRDTs. CRDT implementations suffer from so-called deep
bugs [36]. From the extensional perspective, deep bugs often appear
in rare situations, but they still manifest themselves, often in critical
situations, in large or longtime deployments. From the intensional
perspective, due to the intrinsic uncertainty in distributed system
execution, deep bugs only appear when the system is faced with
certain subtle combinations of system and environment events.
Though the bug triggering patterns of events usually involve only
a moderate number of events, the space of all potential bug trigger-
ing patterns is exponentially large. This explains why deep bugs
are hard to replay and why they are often beyond the coverage of
standard testing techniques.

It is a great challenge to prevent, detect and fix deep bugs in
the design of a CRDT. The central issue in CRDT design is to re-
solve conflicts between concurrent updates, no matter how the
updates are out of order on different replicas. Design of the conflict
resolution strategy is typically tedious and error-prone, especially
for data types with complex semantics. The designer has to ex-
haustively check all possible interleavings of conflicting updates,
which results in a great amount of metadata. The metadata needs
continuous and consistent maintenance. Existing CRDT designs
are mainly informal. The correctness of the design highly depends
on the experience of the designer. The design also needs intensive
design review. All these factors make the design process quite time-
and energy-consuming.

In order to cope with subtle deep bugs in the design of a CRDT, it
is necessary to have a precise and unambiguous description of the
design. More importantly, the design should be "debuggable". That

 
 
 
 
 
 
is, the design needs to be automatically explored by a machine. All
corner cases can be covered by the exhaustive exploration. More-
over, when a bug is found in the design, the exploration process is
also expected to provide sufficient information for the designer to
find the root cause of the bug and then fix it.

It is also a great challenge to cope with deep bugs in the imple-
mentation of a CRDT, even after the design is deemed correct by
formal verification. First, the implementation has to handle details
which are not covered in the design. The transcription from the
design to the implementation often introduces subtle bugs. More-
over, the design is intended for human reasoning. The developer
may often find the design inefficient when directly translated to the
implementation. Thus the developer often has a strong incentive
to optimize the design. Such intentional deviations often greatly
increase the odds of transcription errors in the implementation.

Second, the design often makes "reasonable" assumptions. In
CRDT implementations, such assumptions have to be carefully en-
sured by extra implementations which are neglected in the design.
The implementations centering around assumptions in the design
are expected to be straightforward and highly dependable. How-
ever, this is often not the case when implementing complex CRDTs.
For example, ensuring an assumption in the design often makes
use of third-party libraries. Such libraries often have many options
for different usage in different scenarios. Misconfiguration or mis-
use of the libraries may introduce subtle bugs. Such bugs often
evade human reasoning and code review since the corresponding
implementations are not specified in detail in the design.

Existing testing techniques are insufficient to cope with deep
bugs in CRDT implementations. Code-level model checking for
distributed systems can handle deep bugs, but usually imposes
a prohibitive cost. What we need is to achieve the best of both
worlds. We need the ability of distributed system model checking
to exhaustively check all corner cases of system execution, but also
need the cost-effectiveness of standard testing techniques.

Toward the challenges above, we propose the Model Checking-
driven Explorative Testing (Met) framework. As indicated by its
name, the Met framework consists of two layers. In the design layer
(also denoted the model layer), Met employs the Temporal Logic of
Actions (TLA+) for the specification of CRDT protocols. TLA+ is a
lightweight formal specification language especially suitable for the
design of distributed and concurrent systems [16]. The specification
in TLA+ precisely describes what is required (i.e., specification of
the correctness properties) and what is to be implemented (i.e.,
specification of the CRDT protocol). More importantly, the design
specified in TLA+ is model-checked to eliminate any violation of
the user-specified correctness conditions. The key to practical and
effective model checking is to tame the state explosion problem.
Met coarsens the specification of the CRDT protocol to prune
unnecessary interleavings of events. Met also directly limits the
scale of the system model to reduce the model checking cost. Met
leverages the designer’s understanding of the CRDT protocol to
ensure that the pruning of the state space will not significantly
hamper the ability of Met to "dig out" deep bugs.

In the implementation layer (also denoted the code layer), Met
conducts explorative testing on CRDT implementations, in the sense

2

that the testing systematically controls and permutes all nondeter-
ministic choices of message reorderings. Met inherits the exhaus-
tive exploration from the design layer to the implementation layer.
The explorative testing contains two key components: generation
of test cases and enhancement of system testability.

As for test case generation, the test cases are automatically ex-
tracted from the model checking trace. The model checking trace
consists of a sequence of system states connected by events trig-
gering the state transitions. The sequence of events, denoted as the
event schedule, is extracted as the test input. The events mainly
include system events, e.g., client requests, synchronization among
replicas, and environment events, e.g., (out-of-order) message de-
livery. The sequence of system states is extracted as the test oracle.
That is, the code-level execution is expected to produce the same
sequence of system states as the model-level execution.

As for enhancement of system testability, Met first enhances
system controllability. That is, Met deterministically replays the
model checking trace in the implementation layer. This is achieved
by hacking the underlying RPC among server replicas of the CRDT
store. The communications among server replicas are all directed to
a central test manager. The test manager discharges the intercepted
events one by one, strictly following the event schedule in the test
input. Note that this hacking is transparent to the upper layer CRDT
implementation.

Besides system controllability, Met further needs to enhance
system observability. Met does not adopt the typical approach,
where the system states are logged and then analyzed offline after
the test case execution. Met first implements dedicated APIs for
the test manager to inspect the internal states of the server replicas.
Given the controllability of the system, the test manager intercepts
all system events and decides the total order of the events. Then the
test manager inserts the system state inspection commands into the
event schedule to record the system state. This approach greatly
simplifies the work of comparing the state sequence of system
execution with that of the model checking, i.e., the test oracle.

The Met framework is applied in the design and implementa-
tion of different types of Replicated Priority Queues (RPQs) and
Replicated Lists (RLists) over the CRDT-Redis data type store [2].
We first discuss how our experiences in testing CRDT implementa-
tions motivate the Met framework. Second, we discuss the bugs
found by Met both in both design and implementation of CRDTs.
The Met framework greatly eases the fixing of bugs which can
be detected by standard testing techniques. Met also finds bugs
which cannot be detected by standard testing techniques. Third, we
discuss how the exhaustive (constrained by the testing budget) and
explorative testing of CRDTs using Met increases our confidence
in the correctness of the design and implementation of CRDTs.

The rest of this work is organized as follows. Section 2 exten-
sively discusses the motivation for Met and overviews of the design
of Met. Section 3 and 4 present the model layer and the code layer
design of Met respectively. Section 5 demonstrates the application
of Met in practical CRDT design and implementation. Section 6
discusses the related work. In Section 7, we conclude this work and
discuss the future work.

2 MOTIVATION AND OVERVIEW

In this section, we first present our practical experiences in CRDT
development, in order to extensively explain the motivation behind
Met. Then we provide an overview of Met.

2.1 Our Experiences Motivating Met

The Met framework is proposed in our practices in developing
different types of CRDTs. Specifically, we propose the Rwf conflict
resolution strategy to ease the design and implementation of data
container CRDTs. Rwf-RPQ and Rwf-List are developed on the
CRDT-Redis data type store [2]. For the purpose of performance
comparison, we also develop the RemoveWin-RPQ and RemoveWin-
List. Rwf and RemoveWin can be viewed as two different types
of conflict resolution strategies. The details of these strategies are
irrelevant to our discussions on the Met framework here. More
details of these CRDTs can be found in [53].

The key challenge in CRDT development is to ensure that the
conflict resolution strategy guarantees eventual consistency, no
matter how the data updates are out of order. Lacking an explo-
rative testing service or tool, we resort to random stress testing.
We keep generating concurrent and conflicting data updates and
randomly dispatch the updates to all the replicas. We further use
traffic control (TC) [19] to add random delay to the messages among
server replicas to make the updates out of order.

The testing proceeds in rounds. In each round, 10,000 update
operations are generated per second, lasting 5 minutes. We check
whether the replicas reach the same state after they have received all
the data updates at the end of each round. A bug is found if any two
replicas do not reach the same state. If no bug is found after 24 hours
of stress testing, the implementation is deemed (sufficiently) correct.
More details of the CRDT development can be found in our previous
work [53]. The experiences in testing our CRDT implementations
extensively motivate us to design the Met framework, as detailed
below.

2.1.1 Why We Need Debuggable CRDT Design. The conflict res-
olution logic in a CRDT protocol is usually quite complex and
error-prone. The informal design of a CRDT protocol is far from
being enough to ensure the correctness of the design. Informal
software designs, e.g., textual descriptions, flow charts and pseudo-
codes, are dominant in current software development practices.
The correctness of informal software design is mainly guaranteed
by manual reasoning and intensive design review among a team of
developers. However, it is widely accepted that human intuition is
poor at estimating the true probability of supposedly extremely rare
combinations of events in real deployments of complex distributed
systems [36, 41].

For example when developing the Rwf-List [53], we find vio-
lations of eventual consistency through stress testing in a small
number of experiments, as shown in Issue #1 below.

Issue #1. List replicas may not converge in the stress testing [3].

To find the root cause of this bug, we first look at the final states of
two replicas, and identify the list elements with divergent positions.
As specified in the Rwf-List protocol, the position of one element is
decided when the element is added to the list for the first time. Thus
we go backtracking through the execution trace, in order to find

the point when the list replicas first diverge, and the point when
the elements with divergent positions are first added to the list. In
this way, we find that the bug is caused by the element which is in
the list but has an undefined position.

A minimized example of this bug is shown in Figure 1. According
to our design of the Rwf-List, the location of list element 𝑒 is decided
when it is first added on 𝑠𝑒𝑟𝑣𝑒𝑟0. If we remove and then re-add 𝑒
(such as undoing the removal of words in collaborative editing),
its position will not change. The client may send the 𝑟𝑒-𝑎𝑑𝑑 (𝑒)
operation to 𝑠𝑒𝑟𝑣𝑒𝑟1, which has not yet gotten the previous 𝑎𝑑𝑑 (𝑒)
operation from 𝑠𝑒𝑟𝑣𝑒𝑟0. 𝑆𝑒𝑟𝑣𝑒𝑟1 should reject this 𝑟𝑒-𝑎𝑑𝑑 operation
or buffer this operation for processing later. But in our buggy design,
𝑠𝑒𝑟𝑣𝑒𝑟1 accepts this request and assigns an illegal position to 𝑒.
When 𝑠𝑒𝑟𝑣𝑒𝑟1 receives the delayed 𝑎𝑑𝑑 (𝑒) operation, it will ignore
this operation, which causes the divergence among replicas.

Figure 1: Buggy design of the re-add operation.

Though the debugging process above is methodologically sim-
ple, it is quite time- and energy-consuming. The stress testing runs
for hours or even days. The error trace is quite long when the di-
vergence among replicas is identified. Both trace analysis scrips
and manual inspection are needed to find when the divergence
first occurred and when the elements corresponding to the diver-
gence were first added to the list. Given the information about the
divergence, we then need to manually reason how the elements
are handled by the replicas, in order to understand how the bug is
introduced in the design and the implementation. We spent several
days to find the root cause and fix this bug.

Note that in our design, the 𝑟𝑒-𝑎𝑑𝑑 operation is treated as a
special case of the add operation. To fix the bug in Issue #1, we
add one more case in the conflict resolution logic of the add oper-
ation. The resulted implementation then passes the stress testing.
However, we highly suspect that there are still deep bugs hidden
in our design. It is mainly because the conflict resolution logic in
the add operation is quite complex. Further adding more cases in
the already-quite-complex logic in add makes the design highly
unreliable. Manual reasoning is powerless to ensure the correctness
of this complex design.

The experiences above convince us to add formal specification
and verification as one obligatory step in our CRDT design. Though
formal specification also requires non-trivial human efforts, the
specification process will force the designer to be unambiguous
about the details in the design. The specification can further be
exhaustively checked by a model checker. We expect the increase

3

server0server1clientadd ein some position  remove eundo the removeand re-add ethesynchronization of e’s actual positionarrives lateeacquires an undefined positionin design quality and decrease in testing cost can well compensate
for the human efforts in the formal specification process.

2.1.2 Why We Need Code-level Testing. Even though the design has
been deemed correct after the formal verification, we still need code-
level testing. It is mainly because, at the model level, we verify a
system model, not the actual system implementation. The obtained
results are thus as good as the system model. Different types of
coding errors can be introduced in the implementation. We mainly
discuss two salient types of coding errors, namely the transcription
error and the assumption error.

We first discuss the transcription errors, i.e., errors introduced
when transcribing the design into the code. As we know, software
design is intentionally abstract. The developer must instantiate
the abstract design and transform it to correct executable code.
The developer will meet numerous details which are not covered
in the design. This process is often error-prone. Moreover, the
developer often has the incentive to optimize the design in the
implementation. This is mainly because that the informal design is
intended for human reasoning. Thus directly translating the design
to executable code is often a feasible but inefficient choice. Even
for the formal design intended for machine exploration, there is
still a significant gap in semantics between a formal specification
and an executable program. Thus the programmer may often try
to find an equivalent but more efficient implementation from the
abstract design. This process often increases the odds of introducing
transcription errors.

Though our random stress testing did not find bugs due to tran-
scription errors (denoted the transcription bugs), we highly suspect
that there must be such bugs in our implementation. We "borrow" a
transcription bug found by the Met framework from Section 5.3.1
for the purpose of illustration here.

Issue #4.
In CRDTs developed using the Rwf framework, the ex-
istence of one element is protected by a precondition in the design.
However, the implementation deviates from the design in the case
when the precondition does not hold [6].

Issue #4 can be illustrated by the example in Figure 2. In this ex-
ample, 𝑐𝑙𝑖𝑒𝑛𝑡0 first adds element "a" and then changes the value
of "a" (say, it changes the font of character "a" in a string in a col-
laborative editing scenario). Then 𝑐𝑙𝑖𝑒𝑛𝑡1 queries the value of the
string and obtains string "[a]". After reading the value of the string,
𝑐𝑙𝑖𝑒𝑛𝑡1 inserts "b" after "a" into the string. However, in our buggy
implementation, 𝑐𝑙𝑖𝑒𝑛𝑡0 may read string "[b,a]".

This bug can be triggered by the following adversarial pattern
of events. 𝑆𝑒𝑟𝑣𝑒𝑟0 first receives the operation "changing the font
of 𝑎" (without adding 𝑎 first). Then 𝑠𝑒𝑟𝑣𝑒𝑟0 will receive the oper-
ation "adding 𝑏 after 𝑎". At the design level, this "adding 𝑏 after
𝑎" operation is quite safe, since it is protected by the precondition
that "before adding 𝑏 after 𝑎, 𝑎 must exist". The design does not
explicitly state what to do if the precondition does not hold. The
precondition implicitly states that, if the element does not exist,
nothing should be done. However, in our implementation, an illegal
position is assigned to 𝑏, and then to 𝑎. In this way, 𝑐𝑙𝑖𝑒𝑛𝑡0 gets
"[𝑏, 𝑎]". This bug is beyond the coverage of model checking in the
design layer, and we discuss in Section 5.3.1 how we detect this bug
with the help of Met.

Figure 2: An exemplar transcription bug.

The second type of coding errors, namely the assumption er-
ror, pertains to assumptions in the design. In a broader sense, the
assumption error is also a kind of transcription error. However,
as various assumptions are widely used in designs of distributed
protocols and systems, we explicitly separate this type of errors
out.

In designs of distributed protocols and systems, many details are
often intentionally omitted, via the form of "reasonable" assump-
tions. The assumptions are usually assumed to be straightforward
to implement or to be readily provided by existing libraries/tools.
Thus details of how the assumptions should be guaranteed are omit-
ted in the design. However, when different modules of a system
have different or even conflicting assumptions, the developer needs
to put more effort into managing the implementations pertaining to
a number of various assumptions. Moreover, third-party libraries or
tools can have complex semantics and subtle configurations. Even if
the library implementation is perfectly correct, the developer may
still use it with a wrong configuration, thus failing to provide the
intended guarantee required by the assumption. In such situations,
the developers may introduce assumption errors unwittingly. As
the number of assumptions in the design increases, the odds of
assumption bugs, i.e., bugs due to the assumption errors, quickly
increase to an extent that cannot be neglected.

Issue #2 is an example of the assumption bug in our implementa-
tion. The RemoveWin CRDTs assume that the underlying network
provides the causal delivery semantics, but the networking primi-
tives of the CRDT-Redis data store do not provide such semantics. In
contrast, the Rwf CRDTs do not need the causal delivery network.
In the example shown in Figure 3, the 𝑐𝑙𝑖𝑒𝑛𝑡 sends requests 𝑎, 𝑏
and 𝑐 to different servers. 𝑆𝑒𝑟𝑣𝑒𝑟1 receives 𝑏 and 𝑐 directly from the
𝑐𝑙𝑖𝑒𝑛𝑡, and receives the synchronization of 𝑎 from 𝑠𝑒𝑟𝑣𝑒𝑟0. Since the
synchronization of 𝑎 arrives at 𝑠𝑒𝑟𝑣𝑒𝑟1 before that of 𝑏 and 𝑐, 𝑠𝑒𝑟𝑣𝑒𝑟1
works correctly. In contrast, 𝑠𝑒𝑟𝑣𝑒𝑟2 gets 𝑏 and 𝑐 from 𝑠𝑒𝑟𝑣𝑒𝑟1 first,
without getting 𝑎. The upper layer RemoveWin CRDT protocol
assumes that the underlying network provides causal delivery of
messages. Thus the CRDT protocol does not consider the case in
our example and 𝑏 and 𝑐 are erroneously processed on 𝑠𝑒𝑟𝑣𝑒𝑟2.

The root cause behind this bug is that the Rwf CRDTs and the
RemoveWin CRDTs have different assumptions on the underlying
network. The CRDT-Redis platform aims to host different CRDT
protocols, which may have different assumptions on the network,
the persistent storage, etc. Thus we need to "align" the assumptions
of different protocols on the CRDT-Redis platform. This situation

4

server0server1client0add ato an empty list  change the font of aclient1read the listadd bafter areturn list [a]read the listreturn list [b, a]is similar to the misconfiguration problem in cloud and data center
platforms [47]. The implementations pertaining to such alignments
are error-prone and are not sufficiently documented in the design.
Also note that, even when we find the root cause of an assump-
tion bug, the fixing of this type of bugs is often non-trivial. In our
first patch to fix Issue #2, 𝑠𝑒𝑟𝑣𝑒𝑟2 does not synchronize 𝑏 and 𝑐
correctly when it receives 𝑎. Then when 𝑠𝑒𝑟𝑣𝑒𝑟2 later receives 𝑑, it
gets stuck since the required delivery of 𝑏 and 𝑐 are not available.
We argue that not only the original implementation, but also the
patches fixing the assumption bugs need extensive testing.
Issue #2. The RemoveWin CRDT protocols assume that the underly-
ing network provides causal delivery of messages. The CRDT-Redis
platform does not provide such semantics of network communication
[4].

deterministically covering the most important ones. Met automates
the most tedious part of the manual testing and enables efficient
explorative testing.

2.2 Overview of the Met Framework

The Met framework consists of two layers. The architecture of
Met is illustrated in Figure 4.

Figure 4: Architecture of the MET framework.

In the model layer, we have:
• (Section 3.1) Specification. The CRDT protocol and the correctness

conditions are formally specified in TLA+.

• (Section 3.2) Model Checking. The specifications are automatically
explored by the TLC model checker. Reduction techniques are
proposed to tame the state explosion problem.

In the code layer, we have:
• (Section 4.1) Test Case Generation. Test cases are automatically
generated from the model checking trace. Test case generation
includes generation of the test inputs and that of the test oracle.
• (Section 4.2) Testability Enhancement. The test manager replays
the model checking trace in the code layer, which requires the
enhancement of system controllability and observability.

3 FORMAL SPECIFICATION AND MODEL

CHECKING OF CRDT DESIGNS

In this section we introduce the formal verification of CRDT design
in Met, including formal modeling in TLA+ and taming of the state
explosion problem in model checking.

3.1 Formal Modeling in TLA+

The formal modeling includes the specification of the CRDT proto-
col and that of the correctness properties. The formal specification
language we use is TLA+.

3.1.1 TLA+ Basics. In TLA+ we model a distributed system in terms
of one single global state. A distributed system is specified as a state
machine by describing the possible initial states and the allowed
state transitions called 𝑁 𝑒𝑥𝑡. The system specification contains
a set of (global) system variables 𝑉 . A state is an assignment to
the system variables. 𝑁 𝑒𝑥𝑡 is the disjunction of a set of actions
𝑎1 ∨ 𝑎2 ∨ · · · ∨ 𝑎𝑝 , where an action is a conjunction of several
clauses 𝑐1 ∧ 𝑐2 ∧ · · · ∧ 𝑐𝑞. A clause is either an enabling condition,
or a next-state update.

An enabling condition is a state predicate which describes the
constraints the current state must satisfy, while the next-state up-
date describes how variables can change in a step (by "step" we

Figure 3: An exemplar assumption bug.

Besides the underlying network, the CRDT protocols may also
have assumptions about other modules in the system. More assump-
tions bugs will be discussed in Section 5.3.2.

2.1.3 Why We Need Automatic Explorative Testing. The Issue #2
discussed above is discovered by manual explorative testing. Given
the effectiveness of this manual explorative testing, we decide to
automate this process. This automation gives us the Met framework.
Specifically, we, as the designer of the CRDT protocol, know which
part of the conflict resolution is most tricky and unreliable. Thus
we manually construct adversarial executions which target at the
most tricky part of our design. Given the adversarial executions,
we then extend our unit testing to replay such executions for one
server replica. The inputs are given to the replica following the
adversarial execution. The replica then executes its CRDT protocol.
When the replica interacts with the outside world, we manually
calculate what should be returned to the replica. The server replica
is provided with the illusion that it obtains the feedback from other
peer replicas.

Issue #2 is out of reach of stress testing and is found by this
manual explorative testing. We can easily see the advantages and
disadvantages of the manual explorative testing. The main advan-
tage is that, the test cases are significantly more effective than
the randomly generated ones. The main disadvantage is that, the
whole process is manual and imposes a prohibitive cost. Our CRDT
implementations cannot be sufficiently tested using this manual
testing.

The Met framework is directly shaped by this manual explo-
rative testing practice. Met enumerates all possible test cases, thus

5

server0server1clientserver2requiring aabcdreceiving arequiring b, cState Space PruningSafety Condition SpecificationCRDT ProtocolSpecificationModelCheckerTest Input GenerationTest OracleGenerationTest ManagerObservabilityControllabilityModel Checking of CRDT designExplorative Testing of CRDT implementationmean successive states). Whenever every enabling condition 𝜙𝑎 of
an action 𝑎 is satisfied in a given "current" state, the system can
transfer to the "next" state by executing 𝑎, assigning to each variable
𝑎
the value specified by 𝑎. We use "𝑠1
→ 𝑠2" to denote that the system
state goes from 𝑠1 to 𝑠2 by executing action 𝑎, and 𝑎 can be omitted
if it is obvious from the context. Such execution keeps going and
the sequence of system states forms a trace of system execution.
Exemplar TLA+ specifications can be found in the following Figure
5 and Figure 6.

One salient feature of TLA+ is that correctness properties and
system designs are just steps on a ladder of abstraction, with cor-
rectness properties occupying higher levels, system designs and
algorithms in the middle, and executable code and hardware at the
lower levels [41]. This ladder of abstraction helps designers manage
the complexity of real-world distributed systems. Designers may
choose to describe the system at several "middle" levels of abstrac-
tion, with each lower level serving a different purpose, such as to
understand the consequences of finer-grain concurrency or more
detailed behavior of a communication medium. The designer can
then verify that each level is correct with respect to a higher level.
The freedom to choose and adjust levels of abstraction makes TLA+
extremely flexible.

the replicas can crash or the messages can be dropped, the data
type store does not need to guarantee anything and still provides
eventual consistency. Thus such cases are usually not covered in
the design of a CRDT protocol.

Given the modeling of the network channel, we do not explicitly
model the clients. All client requests are directly modeled as data
access messages in the network channel, which will be delivered to
the replicas some time in the future. Note that the specification of
system behavior intentionally omits certain details. We will justify
these simplifications in Section 3.2. Figure 5 is a skeletal example of
the specification of a CRDT protocol. The global variables record
status of the server replica. The history variable records the state
transitions (the history variable is introduced for the purpose of
test case generation, as detailed in Section 4.1.1). The action records
all possible state transitions. The model checker simply applies
all possible state transitions and enumerates all reachable system
states.

Specification of CRDT Protocols. A replicated data type store
3.1.2
consists of a group of replicas and clients, as well as the network
channel that connects them. The state of the replica consists of:
i) the concrete data of the data type, ii) the meta-data for conflict
resolution, and iii) the input / output message buffer for network
communication. The essential issue of specifying a CRDT protocol
is to specify how the replica updates its state. The state transfer
can be driven by three kinds of events, as defined in the generic
framework of CRDT design [43, 44]:
• send : the send event can be triggered when the output buffer of
the server replica is not empty. It will broadcast one message in
the output buffer to all other replicas.

• receive : the receive event can be triggered when there are mes-
sages to be delivered from the network channel. It will deliver
one message from the channel to the input buffer of the server
replica.

• do : the do event can be triggered when the input buffer is not
empty. It consumes one message from the input buffer and up-
dates the state of the server replica. If the message is a client
request, the do event will further put a synchronization message
into the output buffer of the replica.

Detailed design of the replica update logic is specified in the CRDT
protocol. The event-driven design of a CRDT protocol can be read-
ily transformed to TLA+ specifications. More examples of CRDT
protocol specifications can be found in our online repository [2].

The underlying network is modeled as a shared global channel,
described by a global variable in TLA+. The channel buffers all the
messages in transmission. We assume that the network channel
provides the eventual-delivery-once semantics. The messages can
be out-of-order, but cannot be dropped, duplicated or forged. We
also assume that the replicas will not crash. Note that this assump-
tion is indicated in the definition of eventual consistency [44] and
is widely adopted in CRDT designs. Put it in another way, when

6

Figure 5: A skeletal example of the TLA+ specification for
Rwf-RPQ.

Specification of Correctness Properties. The correctness con-
3.1.3
dition we adopt for CRDT design is eventual consistency1. A CRDT
protocol guarantees eventual consistency if any two replicas always
converge to the same state as long as they have received the same
set of updates, regardless of the order of updates received. The
convergence among replicas means that they reach the same state
in regard to both the data type itself and the metadata for conflict
resolution.

Besides eventual consistency, we may further specify correctness
conditions pertaining to the semantics of the data type under con-
cern. For example in the design of the Rwf-List, we check whether
the position identifiers of all elements in the string are unique and
totally ordered. Figure 6 is an example of specifying the correctness

1The term eventual consistency may have slightly different meanings in different
contexts. In this work, by eventual consistency we mean strong eventual consistency
defined in [44].

modulerwfrpqInit∆=Globalvariables∧ops=[j∈Procs7→{}]networkbuﬀer∧history=hihistoryvariableLocaldataandmetadataofreplicas∧eset=[self∈Procs7→{}]∧tset=[self∈Procs7→{}]Next∆=∃self∈Procs:chooseonereplicatoactGenerateaclientrequest,handleit∨letrequest∆=generateClientRequest(...)in∧history0=Append(history,hrequest,...i)∧∨tryToHandleAsAdd(request,...)∨tryToHandleAsRemove(request,...)∨tryToHandleAsIncrease(request,...)Handleasynchronizationmessage∨ifops[self]6={}then∃msg∈ops[self]:∧history0=Append(history,hmsg,...i)∧syncAndUpdate(msg,...)conditions for Rwf-List. The correctness condition includes both
specification of eventual consistency and that of the constraints on
the position identifiers.

Figure 6: Correctness condition of Rwf-List in TLA+.

3.2 Model Checking of CRDT Designs

Given the CRDT protocol specified in TLA+, we can now explore the
design using the TLC model checker. Though the model checking
process is fully automatic, it is intrinsically constrained by the state
explosion problem. Full checking of the system specification of
arbitrary scale is usually impractical.

In order to conduct practical model checking of the design and
obtain sufficient confidence in the correctness of the design, we
prune the state space in advance, from two orthogonal dimensions.
First, the specification is coarsened to omit unnecessary details.
Second, we leverage the small scope hypothesis to check a small
scale system while not significantly sacrificing the effectiveness of
finding deep bugs.

3.2.1 Coarsening the Specification. We utilize the feature that TLA+
enables the developer to flexibly adjust the level of abstraction. In
the TLA+ specification of a CRDT protocol, the replica mainly pro-
cesses three types of events. The send and receive events model the
network communication. The do event mainly updates the meta-
data for conflict replication and updates the data type itself. In light
of the state explosion problem, we coarsen the TLA+ specification
and focus on the do event.

We bind the send event after the do event. This means that we
let a replica broadcast the synchronization message to all other
peer replicas right after it has handled a data update from some
client, rather than handle other events first and broadcast the data
update in the future. Similarly, we bind the receive event before the
do event. This means that we let a replica handle the update request
in a message right after the message is received.

This coarsening is mainly based on the observation that the
subtle and deep bugs in CRDT designs and implementations mainly

result from the complex (and often tedious) processing of diverse
patterns of conflicting data updates. When we "glue together" the
receive , do and send events, unnecessary interleavings of events are
pruned from the state space, while all possible cases of conflicting
resolution are preserved. As further evidenced by our experimental
evaluation on Met, our coarsening of the specification effectively
limits the cost for model checking, while not significantly sacrificing
the ability to find deep bugs.

Limiting the Scale of the Model. In theory, we need to set the
3.2.2
scale of the model to a number larger than any possible instance of
the system deployed in a real scenario. Then the model checking
can guarantee that the design is always correct in all possible ap-
plication scenarios. However, due to the state explosion problem,
the checking cost is exponential to the scale of the model. We have
to limit the scale to achieve practical checking.

We argue that the small scale model checking can still find most
deep bugs. Our argument is backed by the observation known as
the small scope hypothesis [32, 33, 37, 50]. The hypothesis states
that analyzing small system instances suffices in practice to ensure
sufficient correctness of large scale systems. Empirical studies sup-
port this hypothesis in different settings [17, 42, 50]. For example
in the setting of distributed systems, this hypothesis is proved for a
family of consensus algorithms targeting the benign asynchronous
setting [37]. An empirical study [50] of 198 bug reports for several
popular distributed systems found that 98% of those bugs could be
triggered by three or fewer processes.

We leverage our understanding of the CRDT protocol to tune
the key parameters of the system configuration in order to control
the scale of the model. Let 𝑛 denote the number of server replicas,
𝑞 the number of client requests and 𝑄 the number of all possible
client requests, i.e. the size of the space of all possible requests.

As for the server number 𝑛, we set 𝑛 to a small number. In our
application of the Met framework (Section 5), 𝑛 is up to 3. In most
cases, 3 replicas are sufficient to manifest the subtle interleavings
of concurrent data updates which result in the deep bugs [45, 48].
As 𝑛 goes beyond 3, the checking cost quickly becomes prohibitive.
As for 𝑞, we set the 𝑞 ≥ 𝑛. This is to make sure that every replica
is covered. Besides, the client requests also need to cover all APIs
the CRDT provides. Given the limit on the testing budget, we can
have bigger 𝑞 when 𝑛 is small, but when 𝑛 is relatively large, we
can only have 𝑞 close to or equal to 𝑛.

As for 𝑄, we let it be a little larger than 𝑞. This enables each
client request to possibly have different parameters while limiting
the state space. The value of 𝑄 is affected by multiple factors. The
Met framework is mainly targeted at data container types. We
first need to consider whether the data elements in the container
are independent. If so, we can consider each element separately,
significantly reducing the size of the state space. But we may also
face the data types where the data elements cannot be handled
separately. For example elements in the Rwf-RPQ can be viewed as
independent2, while elements in the Rwf-List are not independent.
When the model checker needs to execute a client request, we set

2When considering the conflict resolution, the elements in the Rwf-RPQ can be
considered independent. When the server replica locally maintains the priority queue
data structure, the data elements are dependent. But this dependence is not relevant
to the conflict resolution we focus on in this work.

7

modulerwflistIftworeplicashavereceivedthesamesetofupdates,theywillconvergetothesamestatewithregardtoallCRDTdataandmetadata.StrongEventualConsistency∆=∀p1,p2∈Procs:(p16=p2∧ops[p1]=ops[p2])⇒(eset[p1]=eset[p2]∧tset[p1]=tset[p2]∧lset[p1]=lset[p2]∧ltset[p1]=ltset[p2])Theelementsinthelistarealwaystotallyordered,despitethattheymaybeinsertedconcurrently.ListElementTotalOrdered∆=∀p∈Procs:∀i,j∈Elmts:(i6=j)⇒(lset[p][i]=Dftleid∨lset[p][j]=Dftleid∨lset[p][i]6=lset[p][j])CheckingInvariant∆=StrongEventualConsistency∧ListElementTotalOrderedthe parameters of the request by choosing a uniformly random re-
quest from all possible requests. When the operations have multiple
equally important parameters, we choose one to represent all the
parameters.

Detailed configurations of the model and the results of model

checking are described in Section 5.4.2.

4 EXPLORATIVE TESTING OF CRDT

IMPLEMENTATIONS

The Met framework provides an explorative testing service to guar-
antee sufficient correctness of CRDT implementations. The key of
the explorative testing service is to utilize the model checking re-
sults at the model level, and guide the explorative and exhaustive
(constrained by the testing budget) testing at the code level. The two
key components of the testing service are: automatic test case gen-
eration from the model checking trace and testability enhancement
of the system under test, as detailed below.

4.1 Test Case Generation

The salient feature of explorative testing is that it systematically
controls and traverses the non-deterministic choices of message
reorderings for CRDT implementations. Given that the exhaustive
checking of message reorderings has been conducted in the model
layer, we automatically generate test cases from the model checking
traces. The test case generation consists of two parts: the generation
of test inputs and that of the test oracle, as shown in Figure 7.

execution without interfering with the execution. The test inputs
can thus be extracted from the history variable.

4.1.2 Generation of the Test Oracle. In Met, the executable code
and the TLA+ specification of the CRDT protocol are viewed as
just two steps on a ladder of abstraction. Given that the protocol
design has been verified by model checking, our objective is to
verify that the code-level execution is correct with respect to the
model-level execution. Thus, the sequence of system states in every
model checking trace is extracted to serve as the test oracle.

In explorative testing, we pick one test case and control the sys-
tem execution to follow the event schedule in the test input. We
check whether the code-level sequence of system states is equiva-
lent to the model-level one. Here, the criteria of being equivalent is
that, the meta-data for conflict resolution and the concrete data of
the data type are the same, while certain details in the code-level
states can be ignored.

4.2 Testability Enhancement

The critical challenge in explorative testing is to control the system
execution to deterministically follow the event schedule in the test
input. Moreover, the test manager also needs to inspect the internal
state of the server replica, in order to tell whether the system passes
certain test cases. The rationale of testability enhancement in Met
is shown in Figure 8. We discuss the two primary issues in testability
enhancement in detail below.

Figure 8: System controllability and observability.

4.2.1 Controllability: Manipulating Code-Level Execution. In TLA+
we model a distributed system in terms of one single global state.
Thus the event schedule in the model checking trace is a total order
of events. The test manager needs to deterministically control the
system execution to follow this total order of events.

The key to achieving system controllability is to control the tim-
ing of each event. In the Met framework, we hack the networking
component of each server replica. The hacking is transparent to the
upper layer protocol. The replicas are provided with the illusion
that they communicate with each other via sending and receiving
messages. In fact, all the sending and receiving of messages are
redirected to the central test manager. The test manager is in full
charge of deciding the timing of every event. The system execution
makes one step forward when the test manager "fires" one event.
Given the system architecture explained above, we directly inte-
grate the client request generation module into the test manager,
and do not explicitly model the clients. The test manager will dis-
patch client request events to the replicas as specified in the test
input.

Figure 7: From model-level traces to code-level test cases.

4.1.1 Generation of Test Inputs. The test input is merely the event
schedule, i.e., a sequence of events that take place in the system.
The test input consists of the system events and the environment
events. According to our modeling in Section 3.1, the system events
mainly include data update requests from the clients and the data
synchronization requests among replicas. The environment events
are the delivery of data updates from the network channel to the
replicas.

To enable automatic extraction of test inputs from the model
checking trace, we add the history variable [35] in the TLA+ speci-
fication of the CRDT protocol. The history variable is an auxiliary
variable which passively records the state transitions in system

8

...History (event schedule):[...]CRDT replica state: [...]Model-level State GraphCode-Level Test Casesmodel checking traceTest case #40 : { ... }Input :s1 rcvrequest a;s2 rcvrequest b;s2 sync m3to s1;s1 sync m1to s2;...Oracle :The final CRDT state of --s1 : {...}s2 : {...}.........Test case #41 : { ... }Input :s1 rcvrequest a;s2 rcvrequest b;s2 sync m3to s1;s1 sync m1to s2;...Oracle :The final CRDT state of --s1 : {...}s2 : {...}Test case generation•  Test input: extraction of event schedule    •  Test oracle: extraction of state traceTest case #42 : { ... }Input :s1 rcvrequest a;s2 rcvrequest b;s2 rcvsync m2;s1 rcvsync m1;...Oracle :The final CRDT state of --s1 : {...}s2 : {...}CRDT : {...}hacked networking CRDT : {...}hacked networking m1m3m2m4Test case #42 : { ... }Input :s1 rcvrequest a;s2 rcvrequest b;s2 rcvsync m2;s1 rcvsync m1;...Oracle :The final CRDT state of --s1 : {...}s2 : {...}TestManagerServer 1Server 2controlobserveIn our application of Met, we instantiate the general approach
above over the CRDT-Redis data store [2]. Details of our implemen-
tation can be found in Section 5.1.

4.2.2 Observability: Inspecting Internal State of the Replica. Though
we can obtain part of the replica state via its public API, the Met
framework requires much more than that. As indicated by the test
oracle, the test manager requires detailed metadata for conflict
resolution. It also requires the internal representation of the data
structure.

The commonly used approach is to obtain the required data
through extensive logging. However, this logging will require a
detailed description of the timing information. It then requires a
non-trivial offline analysis of the log. The Met framework requires
the data type store to be enhanced with replica state inspection
APIs. Such APIs are dedicated to the test manager whenever it
needs to inspect and record the internal state of the server replica.
Given that any progress of the system execution is under the
control of the test manager, the test manager can use the APIs for
replica state inspection and obtain the system state after every event
is dispatched to the replicas. Thus the state sequence of system
execution directly corresponds to the test oracle. This approach
greatly eases the conformance checking against the test oracle.

5 APPLICATION OF MET IN PRACTICAL

CRDT DEVELOPMENT

In this section, we present how we apply the Met framework to
cope with deep bugs in our CRDT design and implementation. We
first present the implementation of Met. Then we discuss how the
deep bugs are found and fixed in both the model layer and the code
layer. In the end, we discuss how Met improves our confidence in
the correctness of our CRDT design and implementation.

5.1 Implementation of the Met Framework

We specify our CRDT protocols in PlusCal [34], which is automati-
cally translated to TLA+ specifications. The Rwf-RPQ protocol has
about 200 lines of PlusCal code, and Rwf-List about 300. We model
check and revise the design until no violations can be detected.
Every model checking trace of the final design is parsed to generate
the test case. The test case generation tool has about 100 lines of
Python code.

Our implementation of the Rwf-RPQ and Rwf-List are over
the CRDT-Redis data type store we developed [2]. To enhance the
testability of CRDT-Redis, we first hack the networking module
of the server replica. Then we augment each data type with APIs
dedicated to the test manager for replica state inspection. The
testability enhancement is implemented in about 200 lines of C
code.

The test manager is implemented in about 1000 lines of C++
code. The test manager is in charge of launching the server replica
groups. It will intercept all communications among the servers and
strictly control the system execution according to the test input.
During the (controlled) system execution, the test manager inspects
the internal states of the replicas at its will. In our practice, we
just inspect the final states of the replicas when all messages are
delivered to reduce the testing cost. This simplification is reasonable
because in the system state, we record all the metadata for conflict

9

resolution and all data of the data type itself. Our experiences in
CRDT development convince us that, any divergence in the previous
states will be reflected in the final state, and the divergence will be
amplified by more data updates. The probability that the divergence
is counteracted by following updates is negligible.

The experiment is conducted on a PC with an Intel I7-6700 quad-
core CPU (3.40GHz) and 16GB RAM. We use VMware Workstation
16 Pro to run the virtual machines. Each VM is set to have a quad-
core CPU with 8GB RAM, running Manjaro with Linux kernel 5.10.
The model checking is conducted with TLA+ Toolbox v1.7.1 [15].
All implementations discussed above are available in our GitHub
repository online [2].

With the help from Met, we find and fix seven bugs in our
CRDT design and implementation. Note that we mainly focus on
relatively deep bugs here. Bugs which can be detected by standard
testing techniques are omitted. The bugs found in our application
of Met are listed as Issue #1 ~#7 in the issue page of the online
repository [2]. As discussed in Section 2.1.1, Issue #1 can be found
by stress testing, but with Met, we find and fix it at a much lower
cost. When we use Met to fix Issue #1, we further raise Issue #3,
which requires the revision of our CRDT design. Issue #2 can be
detected by manual explorative testing. Met principally automate
this process, at a much lower cost. For Issue #4 ~ #7, they can only
be detected by Met. We discuss these bugs in detail below.

5.2 Model Checking of Our CRDT Design

We detect Issue #1 by hours of stress testing. We then take days to
fix the issue. Using the Met framework, we first precisely specify
the CRDT protocol in TLA+. The formal specification and model
checking help us find bugs deep in subtle corner cases in our design.
The fixing of Issue #1 using Met is much more efficient. The model
checking provides the trace which leads to the violation of the
correctness condition. This is the shortest trace since the TLC model
checker traverses the state space in the BFS manner. We use this
trace to help revise the design. We keep revising the design until
the design passes the model checking.

The bugs found by the model checking have the following char-
acteristics. Such bugs can only be triggered by specific interleavings
of client requests and replica synchronization operations. The bug-
triggering event pattern may not be quite long, but it hides in an
exponential number of all possible such patterns. More importantly,
the bugs are often latent, in the sense that they may not cause ex-
ternally observable symptoms for quite a long period of time even
after the divergence has appeared. Thus finding such bugs often
requires long time random stress testing. All these characteristics
of the bugs necessitate the formal specification and verification of
our CRDT design.

The formal specification process in the fixing of Issue #1 drives
us to further improve our design, as shown in Issue #3. In our
original design, we treat the 𝑟𝑒-𝑎𝑑𝑑 operation as a special case in the
𝑎𝑑𝑑 operation. The semantics of the add operation becomes quite
complex. Issue #3 basically says that the design should be revised.
The 𝑟𝑒-𝑎𝑑𝑑 logic should be treated as a separate operation. The
conflict handling logic involves the new operation 𝑟𝑒-𝑎𝑑𝑑 should
be supplemented. Though we put more efforts in designing the new
operation 𝑟𝑒-𝑎𝑑𝑑 as well as the conflict resolution logic involving

𝑟𝑒-𝑎𝑑𝑑 , the overall complexity of our CRDT design is reduced and
the resulting specification is less complex. Separating the 𝑟𝑒-𝑎𝑑𝑑
operation out also helps reduce the model checking cost.
Issue #3. The 𝑟𝑒-𝑎𝑑𝑑 operation should be separated from the add
operation and treated as an independent operation [5].

Summarizing the process of fixing Issue #1 and #3, the formal
specification forced us to eliminate the ambiguity in semantics of
element existence corresponding to the 𝑟𝑒-𝑎𝑑𝑑 operation. We ex-
plicitly model the existence of one element into three cases: existent,
non-existent and once-existent. The meanings of the "existent" and
"non-existent" states are as usual. The state "once-existent" pertains
to the 𝑟𝑒-𝑎𝑑𝑑 operation. When we 𝑟𝑒-𝑎𝑑𝑑 an element "a", this ele-
ment should not be treated as a new element. The 𝑟𝑒-𝑎𝑑𝑑 operation
adds an element which once existed in the data container but is
now not in the container. We model this status as "once-existent"
to correctly design the semantics of the 𝑟𝑒-𝑎𝑑𝑑 operation.

5.3 Model Checking-driven Explorative

Testing of Our CRDT Implementation

Having verified the CRDT protocol in the model layer, we further
conduct model checking-driven explorative testing in the code layer.
We aim at finding bugs due to transcription errors, named tran-
scription bugs, and bugs due to assumptions made in the protocol,
named assumption bugs.

5.3.1 Transcription Bugs. We use Issue #4 as an example of tran-
scription bugs in Section 2. This bug is beyond the coverage of
stress testing, and can only be found by Met. When transforming
the design, both formal and informal, into codes, the developers
inevitably needs to handle details which are not covered in the
design. Issue #4 is a bug caused by the deviation between our de-
sign and implementation, corresponding to the "dummy-existence"
semantics of data elements.

Specifically, at the design level, the element is either exiting or
not-existing. The handling is protected by the precondition that
“the element must exist". This implicitly states that, if the element
does not exist, nothing should be done. However, at the code level,
we must model the intermediate state that the element exists as
a dummy. This intermediate state is mainly caused by the late
arrival of an operation adding some element into the data container.
In our buggy implementation, the dummy state is not explicitly
modeled and illegal positions are assigned to the dummy elements
(see details of the example in Section 2.1.2).

Issue #5 [7] and Issue #6 [8] are similar to Issue #4 in the sense
that they are also due to the misinterpretation of the design, con-
cerning the existence of elements. We omit the detailed discussions
on these two issues due to the limit of space.

5.3.2 Assumption Bugs. The assumption bug Issue #2 is intro-
duced when we integrate different types of CRDT protocols in the
CRDT-Redis platform. Different protocols have different assump-
tions about the underlying network. There is often a gap between
what the protocol assumes and what the platform provides. This
significantly increases the odds of assumption bugs in our CRDT
implementations. Moreover, implementing a patch fixing such bugs
is also error-prone and needs extensive testing (see details of this
issue in Section 2.1.2).

10

Issue #7 is another example of an assumption bug. The Rwf-List
and the RemoveWin-List protocols assume that there is a position
ID generator. It generates list element IDs which are totally ordered,
dense, and consistent with the element order resulting from the
𝑎𝑑𝑑 operations. Our design of list element organization (borrowed
from the existing design in [46]) does not specify how such an ID
generator can be implemented. The implementation of such an ID
generator is non-trivial, and the bug in our implementation of the
ID generator is found by Met. Given the test case execution trace,
as well as how the trace violates the test oracle, we quickly find the
root cause and revise our implementation of the ID generator.

Issue #7. The order of elements in the RemoveWin List and the
Rwf-List is sometimes not consistent with the order induced by 𝑎𝑑𝑑
operations [9].

5.4 How Met Improves Our Confidence in

CRDT Implementations

Our primary goal of introducing the Met framework is to obtain
correct CRDT designs and implementations. In the ideal case, the
design has been verified to be correct by model checking. As for
the implementation, the testing is both explorative and exhaustive,
driven by model checking. Thus the implementation is also correct.
However, in practice, the design and implementation obtained
using Met still potentially have bugs. There are mainly two sources
of threats to the correctness of CRDT designs and implementations
ensured by Met. One is that any details in the implementation
which are not modeled in the formal design can still cause bugs.
The other is that the state space is not really traversed. A significant
portion of the state space is pruned due to the state space explosion
problem. This pruning can potentially miss bugs in the design and
implementation.

We discuss in detail below how Met provides us with sufficient
correctness of CRDT designs and implementations, in spite of the
threats discussed above.

5.4.1 Threats from System Modeling. There are always certain as-
pects of a system operating in realistic scenarios which are not
modeled in (both informal and formal) system design. Thus bugs
corresponding to the unmodeled part of the system cannot be de-
tected by Met. The unmodeled part of the system is principally the
"unknown unknowns" [22, 25, 28]. We argue that such bugs will
not hamper the usefulness of Met. Met is mainly targeted at the
subtle deep bugs in CRDT design and implementations. The deep
bugs we focus on are primarily "known unknowns" [22, 25, 28] as
detailed below.

According to our experiences in CRDT design and implemen-
tation, deep bugs are mainly known unknowns in the sense that
we know or highly suspect their existence. We also know the high-
level pattern and mechanism of how the deep bugs may manifest
themselves. In theory, the developer can find and fix all the deep
bugs through manual reasoning and testing. This approach just
requires too much human efforts and time. We lack the tool sup-
port to automate and accelerate this manual process, effectively
reducing the testing cost.

Our Met framework fills this gap. The gist of proposing the
Met framework is thus to automate the most tedious and repetitive

part of human reasoning and manual explorative testing. Given the
automation provided by Met, human knowledge of the deep bugs
can be efficiently leveraged, to dig out the deep bugs at a reasonable
cost.

In summary, though Met is no better than the human knowledge
of deep bugs, it efficiently leverages knowledge of the known un-
knowns and pragmatically finds and fixes deep bugs. In this regard,
Met can provide us with sufficient confidence in the correctness of
CRDT designs and implementations.

5.4.2 Threats from State Space Pruning. The total cost for model
checking-driven explorative testing is mainly decided by the cost
for the model checking. It is because the cost for the explorative
testing is proportional to the number of test cases, which is the
same as the number of model checking traces. We have to limit the
scale of the model to finish the testing within the testing budget.
The limit on the model scale is based on the small scope hypothesis,
as discussed in Section 3.2. Table 1 shows the different parts of
the cost for testing via the Met framework. The configurations
shown in the table are the largest affordable scale of models. The
Met configuration tuple stands for "number of servers – number of
client requests". In our experiment, we limit the testing time by 12
hours and limit the disk space used for storing the model checking
traces by 100GB.

Table 1: The cost for explorative testing using Met.

Space cost

Time cost

Met
config

Num of
traces

TLC
output

Test
case

Checking Testing

Rwf-
RPQ

Rwf-
List

1-10
2-5
3-3

1-5
2-4
3-3

9,765,625 2.87GB 1.22GB 6:30s
9:26s
7,202,312 1.80GB 884MB
1:53s
105MB
209MB
883,413

3:10:46s
2:29:03s
20:27s

69,343,957 9.64GB 5.47GB 2:43:22s 11:50:55s
2:12:38s 5:09:29s
23,639,180 4.09GB 2.1GB
1:46:18s
51:05s
5,085,147 1.43GB 731MB

The model checking will not complete if we increase any dimen-
sion of the configuration. For example, we tried to increase one
client request to the RWF-RPQ model configuration 3-3, which is 3
servers + 4 client requests. The model checking will not finish after
24 hours. Looking at the number of pending states in the queue, we
estimate that it needed at least another 24 hours. It has used about
150GB of disk space when we manually stopped the checking.

In the model checking, we have at most three server replicas.
The case of one server is used to exercise the basic logic of the
protocol without any concurrent updates. The case of two servers
is used to test the commutative properties of possible concurrent
operations. The case of three servers is used to test the subtle corner
cases in conflict resolution. As proved in conflict resolution with
the operational transformation technique in collaborative editing,
three operations are enough to express all the properties required
for eventual convergence [45, 48]. Thus we think that the model
checking with three replicas can find most deep bugs pertaining to

11

the conflict resolution. Also note that we have three types of opera-
tions for each CRDT. Thus we have at least three client requests for
each test case, where all types of CRDT operations can be tested.
As shown in Table 1, the cost for Rwf-RPQ and that for Rwf-List
differ quite a lot. This is mainly due to the fact that the Rwf-RPQ
is data-independent, while the Rwf-List is not. Our checking for
the Rwf-RPQ focuses on one data element. We choose {10, 20}
to be the set of possible initial values of 𝑎𝑑𝑑 operations, and {-3,
4} the set of possible value increases for 𝑖𝑛𝑐𝑟𝑒𝑎𝑠𝑒 operations. All
values are picked uniformly at random. The Rwf-List does not have
the property of data independence. The order of the elements is
important for the list. Since each client request may add a new
element to the list, we set the size of the element space to the total
number of client requests. As we target at collaborative editing
scenarios, each data element can have up to 6 attributes (e.g., the
font, being italic and being bold face). In our testing via Met, we
consider only one attribute since we mainly focus on the conflict
resolution logic. We choose {10, 20} to be the set of possible initial
values and update values for the Rwf-List.

We also propose an optimization concerning the model checking
trace. We did not let the TLC model checker output the whole state
transition graph like the work in [21]. Instead we let the CRDT
models output only the final states. This makes the output file one
or two orders of magnitude smaller. The extracted test case files
are about half the size of the TLC output files. The optimization
prevents the space usage from being the bottleneck. To get a rough
idea, it takes about one hour to test 1GB of test case data on average.
Comparing the time cost for the model checking and that for the
explorative testing, we find that the model checking is faster than
the explorative testing. This is mainly because the model checker
can prune the redundant checking of system states, while in Met
execution of each test case is stateless.

According to the discussions above, the cost for testing via Met
is reasonable. Now the critical issue is to discuss whether Met can
help us effectively find deep bugs in our CRDT design and imple-
mentation. The effectiveness of the Met framework depends on the
small scope hypothesis. We argue that the hypothesis works well
in the testing of CRDT implementations. In collaborative editing
scenarios, convergence properties involving at most three concur-
rent operations are sufficient to ensure eventual convergence. Since
operational transformation techniques also focus on ensuring even-
tual convergence among replicas, this result indicates that with
three replicas we can construct the triggering pattern of most deep
bugs in CRDT development. This result is in accordance with our
experiences in CRDT testing. As discussed in our design of Met,
we focus on the conflict resolution logic of CRDTs. We leverage our
understanding of the CRDT design to carefully prune parts of the
state space which are not relevant or less relevant to the conflict
resolution logic.

In summary, most deep bugs concerning the conflict resolution
logic can be reproduced in small scale systems. The pruning in our
model checking in Met carefully avoids erroneously pruning traces
which can trigger deep bugs. In this regard, we believe that Met en-
sures sufficient correctness of CRDT designs and implementations.

6 RELATED WORK

Distributed systems are notoriously difficult to implement correctly,
mainly due to the intrinsic uncertainty in their executions [26].
This uncertainty is created by asynchrony, the absence of global
time, various types of failures, etc. From the perspective of adver-
sary argument [24], the developer needs to enumerate all possible
combinations of uncertain events and reason out the invariable
correctness of system behavior. This type of exhaustive reasoning
is often impractical for the developer. This explains why model
checking techniques are often employed to verify the correctness
of distributed system designs [39, 41]. For example, the Chord pro-
tocol is formally specified in Alloy and is model-checked using the
Alloy Analyzer [51, 52]. Distributed consensus protocols, such as
Paxos [10] and Raft [11] are specified in TLA+ and model-checked
using the TLC model checker. TLA+ is also widely used to verify the
design of distributed systems in industrial scenarios, e.g. in Ama-
zon S3 and DynamoDB [40, 41], CosmosDB [12] and TaurusDB
[27]. The techniques discussed above mainly focus on design at
the model level. From a pragmatic perspective, we need to lever-
age model checking to further improve the reliability of code-level
implementations.

Distributed System Model Checking (DMCK) treats the code-
level implementation as the model, and directly conducts model
checking on the code [31, 36, 38, 49]. DMCK precisely controls
the execution of the distributed system and exhaustively explores
all possible execution paths. DMCK has shown the potential for
detecting deep bugs which are extremely difficult to detect through
traditional testing techniques. However, DMCK usually imposes
a prohibitive cost, and state reduction techniques are essential to
the practical application of DMCK. Partial order reduction exploits
the independence among events, and establishes the equivalence
among interleavings of independent events [49]. Thus the state
space is reduced by checking only one interleaving on behalf of
all equivalent ones. Dynamic interface reduction [31] is based on
the observation that different interleavings of local events (within
one system component) often result in the same global interaction
(among multiple components). Thus the redundant enumeration
of global events can be saved. To better reduce the state space,
semantic knowledge of the target system can further be utilized.
The Semantic-Aware Model Checking (SAMC) [36] presents four
semantic-aware reduction policies that enable the model checker to
define fine-grained event dependency/independency and symmetry
beyond what black-box approaches can do. The Failure Testing
Service (Fate) focuses on recovery testing [30]. Fate exhaustively
exercises as many combinations of failures as possible. Semantic
knowledge of the failure patterns is utilized to prioritize the failure
patterns. The limited testing budget is then directed to the testing
of the high-priority patterns.

Our Met framework shares the same goal of DMCK techniques.
We all aim to inherit the feature of exhaustive exploration of model
checking to improve code-level testing. However, the Met frame-
work takes a methodologically different approach. Met does not
aim at fully automatic checking of the code-level implementation
as the model. Met is designed to be semi-automatic. Human efforts
are required to obtain the formal specification of the system design.
The developer is also required to enhance the testability of the

system under test. After the manual preparations above, the model
checking, the test case generation and the explorative testing are
automatic.

We opt for this semi-automatic approach mainly based on the
observation that, fully automatic DMCK approaches still imposes
a prohibitive cost on distributed systems in the field. We leverage
moderate human efforts to reduce the overall testing cost, while still
inheriting the exhaustive exploration of model checking to code-
level testing. We argue that this tradeoff is effective and efficient
for subtle deep bugs in CRDT implementations, and hopefully in
more distributed systems.

MongoDB uses Model-Based Test Case Generation (MBTCG) to
ensure the equivalence between the C++ and the Golang versions of
the operational transformation (OT) implementations in MongoDB
Realm Sync [21]. Met is inspired by the MBTCG technique of
MongoDB. Moreover, the OT technique also focuses on ensuring
eventual convergences among different replicas, which motivates
us to first apply Met on CRDT designs and implementations. The
MBTCG technique is used at the unit testing level, and focuses on
the OT module in the Realm Sync system. The Met framework is
targeted at subtle deep bugs in CRDT data stores and the explorative
testing is conducted at the system level.

7 CONCLUSION

In this work we present the Met framework to cope with deep
bugs in CRDT designs and implementations. The developer first
specifies the CRDT design in TLA+, and model checks the design
to eliminate model level bugs. Then test cases are automatically
generated from the model checking traces to achieve explorative
and exhaustive testing at the code level. We demonstrate a practical
application of the Met framework in the design and implemen-
tation of various data types over the CRDT-Redis data store. The
Met framework not only eases the fixing of bugs which can be
detected by standard testing techniques. It also finds bugs which
are out of reach of existing techniques. We also discuss how Met
increases our confidence in the correctness of CRDT designs and
implementations.

In our future work, we will apply Met to more different types
of distributed systems, e.g., distributed consensus components and
atomic commit components in cloud-native distributed databases.
We will also explore heuristic exploration strategies as well as
parallel checking frameworks, to further tame the state explosion
problem in Met.

ACKNOWLEDGMENTS

This work was supported by the Cooperation Fund of Huawei-
Nanjing University Next Generation Programming Innovation Lab
(No. YBN2019105178SW38).

REFERENCES
[1] 2022. Azure Cosmos DB – NoSQL Database, Microsoft Azure. Retrieved Feb. 20,

2022 from https://azure.microsoft.com/en-us/services/cosmos-db/

[2] 2022. Conflict-Free Replicated Data Types Based on Redis. Retrieved Feb. 22,

2022 from https://github.com/elem-azar-unis/CRDT-Redis

[3] 2022. CRDT-Redis: Issue #1. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/1

[4] 2022. CRDT-Redis: Issue #2. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/2

12

[5] 2022. CRDT-Redis: Issue #3. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/3

[6] 2022. CRDT-Redis: Issue #4. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/4

[7] 2022. CRDT-Redis: Issue #5. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/5

[8] 2022. CRDT-Redis: Issue #6. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/6

[9] 2022. CRDT-Redis: Issue #7. Retrieved Apr. 19, 2022 from https://github.com/

elem-azar-unis/CRDT-Redis/issues/7

[10] 2022. Dr. TLA+ Series – Paxos (Andrew Helwer). Retrieved Apr. 17, 2022 from

https://github.com/tlaplus/DrTLAPlus/tree/master/Paxos

[11] 2022. Formal TLA+ specification of the Raft consensus algorithm. Retrieved

Apr. 17, 2022 from https://github.com/ongardie/raft.tla

[12] 2022. High-level TLA+ specifications of the five consistency levels offered by

Azure Cosmos DB. https://github.com/Azure/azure-cosmos-tla

[13] 2022. Redis Enterprise, Database built for hybrid applications. Retrieved Feb. 20,

2022 from https://redis.com/redis-enterprise/

[14] 2022. Riak Enterprise NoSQL Database, Scalable Database Solutions. Retrieved

Feb. 20, 2022 from https://riak.com/
[15] 2022. TLA+ – The Brontinus release.

Retrieved Apr. 21, 2022 from https:

//github.com/tlaplus/tlaplus/releases/tag/v1.7.1

[16] 2022. The TLA+ Home Page. Retrieved Feb. 20, 2022 from https://lamport.

azurewebsites.net/tla/tla.html

[17] Alexandr Andoni, Dumitru Daniliuc, Sarfraz Khurshid, and Darko Marinov. 2003.

Evaluating the "Small Scope Hypothesis" (POPL’03, Vol.2).

[18] E. Brewer. 2012. CAP twelve years later: How the "rules" have changed. Computer

45, 2 (2012), 23–29.

[19] Martin A. Brown. 2020. Traffic Control HOWTO. http://tldp.org/HOWTO/Traffic-

Control-HOWTO/index.html. Accessed: 09-30-2020.

[20] Sebastian Burckhardt, Alexey Gotsman, Hongseok Yang, and Marek Zawirski.
2014. Replicated Data Types: Specification, Verification, Optimality. In Proceedings
of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages (San Diego, California, USA) (POPL ’14). ACM, New York, NY, USA,
271–284. https://doi.org/10.1145/2535838.2535848

[21] A. Jesse Jiryu Davis, Max Hirschhorn, and Judah Schvimer. 2020. Extreme
Modelling in Practice. Proc. VLDB Endow. 13, 9 (May 2020), 1346–1358. https:
//doi.org/10.14778/3397230.3397233

[22] Sebastian Elbaum and David S. Rosenblum. 2014. Known Unknowns: Testing in
the Presence of Uncertainty. In Proceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering (Hong Kong, China) (FSE
2014). Association for Computing Machinery, New York, NY, USA, 833–836.
https://doi.org/10.1145/2635868.2666608

[23] Vitor Enes, Paulo Sergio Almeida, Carlos Baquero, and Joao Leitao. 2019. Efficient
Synchronization of State-Based CRDTs. In 2019 IEEE 35th International Conference
on Data Engineering (ICDE). 148–159. https://doi.org/10.1109/ICDE.2019.00022
[24] Jeff Erickson. 2022. More Algorithms Lecture Notes. Retrieved Mar. 27, 2022

from http://jeffe.cs.illinois.edu/teaching/algorithms/

[25] Naeem Esfahani, Ehsan Kouroshfar, and Sam Malek. 2011. Taming Uncertainty
in Self-Adaptive Software. In Proceedings of the 19th ACM SIGSOFT Symposium
and the 13th European Conference on Foundations of Software Engineering (Szeged,
Hungary) (ESEC/FSE ’11). Association for Computing Machinery, New York, NY,
USA, 234–244. https://doi.org/10.1145/2025113.2025147

[26] Pedro Fonseca, Kaiyuan Zhang, Xi Wang, and Arvind Krishnamurthy. 2017. An
Empirical Study on the Correctness of Formally Verified Distributed Systems. In
Proceedings of the Twelfth European Conference on Computer Systems (Belgrade,
Serbia) (EuroSys ’17). Association for Computing Machinery, New York, NY, USA,
328–343. https://doi.org/10.1145/3064176.3064183

[27] Song Gao, Bohua Zhan, Depeng Liu, Xuechao Sun, Yanan Zhi, David N. Jansen,
and Lijun Zhang. 2021. Formal Verification of Consensus in the Taurus Distributed
Database. In Formal Methods, Marieke Huisman, Corina Păsăreanu, and Naijun
Zhan (Eds.). Springer International Publishing, Cham, 741–751.

[28] David Garlan. 2010. Software Engineering in an Uncertain World. In Proceedings
of the FSE/SDP Workshop on Future of Software Engineering Research (Santa Fe,
New Mexico, USA) (FoSER ’10). Association for Computing Machinery, New York,
NY, USA, 125–128. https://doi.org/10.1145/1882362.1882389

[29] Seth Gilbert and Nancy A. Lynch. 2012. Perspectives on the CAP Theorem.

Computer 45, 2 (2012), 30–36. https://doi.org/10.1109/MC.2011.389

[30] Haryadi S. Gunawi, Thanh Do, Pallavi Joshi, Peter Alvaro, Joseph M. Hellerstein,
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Koushik Sen, and Dhruba
Borthakur. 2011. FATE and DESTINI: A Framework for Cloud Recovery Testing.
In Proceedings of the 8th USENIX Conference on Networked Systems Design and
Implementation (Boston, MA) (NSDI’11). USENIX Association, USA, 238–252.
[31] Huayang Guo, Ming Wu, Lidong Zhou, Gang Hu, Junfeng Yang, and Lintao Zhang.
2011. Practical Software Model Checking via Dynamic Interface Reduction. In
Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles
(Cascais, Portugal) (SOSP ’11). ACM, New York, NY, USA, 265–278. https://doi.
org/10.1145/2043556.2043582

13

[32] Travis Hance, Marijn Heule, Ruben Martins, and Bryan Parno. 2021. Finding
Invariants of Distributed Systems: It’s a Small (Enough) World After All. In 18th
USENIX Symposium on Networked Systems Design and Implementation (NSDI
21). USENIX Association, 115–131. https://www.usenix.org/conference/nsdi21/
presentation/hance

[33] Daniel Jackson. 2012. Software Abstractions: Logic, Language, and Analysis. The

MIT Press.

[34] Leslie Lamport. 2009. The PlusCal Algorithm Language. In Theoretical Aspects of
Computing - ICTAC 2009, Martin Leucker and Carroll Morgan (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 36–60.

[35] Leslie Lamport and Stephan Merz. 2017. Auxiliary Variables in TLA+. CoRR
abs/1703.05121 (2017). arXiv:1703.05121 http://arxiv.org/abs/1703.05121
[36] Tanakorn Leesatapornwongsa, Mingzhe Hao, Pallavi Joshi, Jeffrey F. Lukman,
and Haryadi S. Gunawi. 2014. SAMC: Semantic-aware Model Checking for Fast
Discovery of Deep Bugs in Cloud Systems. In Proceedings of the 11th USENIX
Conference on Operating Systems Design and Implementation (Broomfield, CO)
(OSDI’14). USENIX Association, Berkeley, CA, USA, 399–414. http://dl.acm.org/
citation.cfm?id=2685048.2685080

[37] Ognjen Marić, Christoph Sprenger, and David Basin. 2017. Cutoff Bounds for
Consensus Algorithms. In Computer Aided Verification, Rupak Majumdar and
Viktor Kunčak (Eds.). Springer International Publishing, Cham, 217–237.
[38] Madanlal Musuvathi, David Y. W. Park, Andy Chou, Dawson R. Engler, and
David L. Dill. 2002. CMC: A Pragmatic Approach to Model Checking Real
Code. In Proceedings of the 5th Symposium on Operating Systems Design and
Implementation (Copyright Restrictions Prevent ACM from Being Able to Make
the PDFs for This Conference Available for Downloading) (Boston, Massachusetts)
(OSDI ’02). USENIX Association, USA, 75–88.

[39] Chris Newcombe. 2011. Debugging Designs using exhaustively testable pseudo-
code. In Proceedings of the International Workshop on High Performance Transaction
Systems (California) (HPTS’11). http://hpts.ws/papers/2011/agenda.html
[40] Chris Newcombe. 2014. Why Amazon Chose TLA + . In Abstract State Machines,
Alloy, B, TLA, VDM, and Z, Yamine Ait Ameur and Klaus-Dieter Schewe (Eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg, 25–39.

[41] Chris Newcombe, Tim Rath, Fan Zhang, Bogdan Munteanu, Marc Brooker, and
Michael Deardeuff. 2015. How Amazon Web Services Uses Formal Methods.
Commun. ACM 58, 4 (March 2015), 66–73. https://doi.org/10.1145/2699417
[42] Johannes Oetsch, Michael Prischink, Jörg Pührer, Martin Schwengerer, and Hans
Tompits. 2012. On the Small-Scope Hypothesis for Testing Answer-Set Programs.
In Proceedings of the Thirteenth International Conference on Principles of Knowledge
Representation and Reasoning (Rome, Italy) (KR’12). AAAI Press, 43–53.
[43] Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski. 2011. A
comprehensive study of Convergent and Commutative Replicated Data Types. Re-
search Report RR-7506. Inria – Centre Paris-Rocquencourt ; INRIA. 50 pages.
https://hal.inria.fr/inria-00555588

[44] Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski. 2011.
Conflict-free Replicated Data Types. In Proceedings of the 13th International
Conference on Stabilization, Safety, and Security of Distributed Systems (Grenoble,
France) (SSS’11). Springer-Verlag, Berlin, Heidelberg, 386–400. http://dl.acm.org/
citation.cfm?id=2050613.2050642

[45] Chengzheng Sun, Yi Xu, and Agustina Agustina. 2014. Exhaustive Search of
Puzzles in Operational Transformation. In Proceedings of the 17th ACM Confer-
ence on Computer Supported Cooperative Work and Social Computing (Baltimore,
Maryland, USA) (CSCW ’14). Association for Computing Machinery, New York,
NY, USA, 519–529. https://doi.org/10.1145/2531602.2531630

[46] Stephane Weiss, Pascal Urso, and Pascal Molli. 2010. Logoot-Undo: Distributed
Collaborative Editing System on P2P Networks. IEEE Transactions on Parallel
and Distributed Systems 21, 8 (2010), 1162–1174. https://doi.org/10.1109/TPDS.
2009.173

[47] Tianyin Xu, Xinxin Jin, Peng Huang, Yuanyuan Zhou, Shan Lu, Long Jin, and
Shankar Pasupathy. 2016. Early Detection of Configuration Errors to Reduce
Failure Damage. In Proceedings of the 12th USENIX Conference on Operating
Systems Design and Implementation (Savannah, GA, USA) (OSDI’16). USENIX
Association, USA, 619–634.

[48] Yi Xu, Chengzheng Sun, and Mo Li. 2014. Achieving Convergence in Operational
Transformation: Conditions, Mechanisms and Systems. In Proceedings of the 17th
ACM Conference on Computer Supported Cooperative Work and Social Computing
(Baltimore, Maryland, USA) (CSCW ’14). Association for Computing Machinery,
New York, NY, USA, 505–518. https://doi.org/10.1145/2531602.2531629
[49] Junfeng Yang, Tisheng Chen, Ming Wu, Zhilei Xu, Xuezheng Liu, Haoxiang
Lin, Mao Yang, Fan Long, Lintao Zhang, and Lidong Zhou. 2009. MODIST:
Transparent Model Checking of Unmodified Distributed Systems. In NSDI’09.
USENIX, 213–228.

[50] Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Yongle
Zhang, Pranay U. Jain, and Michael Stumm. 2014. Simple Testing Can Prevent
Most Critical Failures: An Analysis of Production Failures in Distributed Data-
Intensive Systems. In Proceedings of the 11th USENIX Conference on Operating
Systems Design and Implementation (Broomfield, CO) (OSDI’14). USENIX Associ-
ation, USA, 249–265.

[51] Pamela Zave. 2015. How to Make Chord Correct (Using a Stable Base). CoRR
abs/1502.06461 (2015). arXiv:1502.06461 http://arxiv.org/abs/1502.06461
[52] Pamela Zave. 2017. Reasoning About Identifier Spaces: How to Make Chord
IEEE Transactions on Software Engineering 43, 12 (2017), 1144–1156.

Correct.
https://doi.org/10.1109/TSE.2017.2655056

[53] Yuqi Zhang, Hengfeng Wei, and Yu Huang. 2021. Remove-Win: a Design Frame-
work for Conflict-free Replicated Data Types. In Proceedings of the 27th IEEE
International Conference on Parallel and Distributed Systems (Beijing, China) (IC-
PADS’21). IEEE.

14

