Going From Molecules to Genomic Variations to Scientific Discovery:

Intelligent Algorithms and Architectures for Intelligent Genome Analysis

Mohammed Alser1,*, Joel Lindegger, Can Firtina, Nour Almadhoun, Haiyu Mao, Gagandeep
Singh, Juan Gomez-Luna, Onur Mutlu2,*

ETH Zurich, Gloriastrasse 35, 8092 Zürich, Switzerland
1alserm@ethz.ch, 2omutlu@gmail.com
*Corresponding authors

Abstract

We now need more than ever to make genome analysis more intelligent. We need to read,
analyze, and interpret our genomes not only quickly, but also accurately and efficiently enough
to scale the analysis to population level. There currently exist major computational bottlenecks
and inefficiencies throughout
the entire genome analysis pipeline, because state-of-the-art
genome sequencing technologies are still not able to read a genome in its entirety. We describe
the ongoing journey in significantly improving the performance, accuracy, and efficiency of
genome analysis using intelligent algorithms and hardware architectures. We explain
state-of-the-art algorithmic methods and hardware-based acceleration approaches for each step
of the genome analysis pipeline and provide experimental evaluations. Algorithmic approaches
exploit
the genome as well as the structure of the underlying hardware.
Hardware-based acceleration approaches exploit specialized microarchitectures or various
execution paradigms (e.g., processing inside or near memory) along with algorithmic changes,
leading to new hardware/software co-designed systems. We conclude with a foreshadowing of
future challenges, benefits, and research directions triggered by the development of both very
low cost yet highly error prone new sequencing technologies and specialized hardware chips for
genomics. We hope that these efforts and the challenges we discuss provide a foundation for
future work in making genome analysis more intelligent.
The analysis script and data used in our experimental evaluation are available at:
https://github.com/CMU-SAFARI/Molecules2Variations

the structure of

Keywords: Genome analysis; read mapping; hardware acceleration; hardware/software
co-design

1. Introduction

Sequencing genomic molecules stimulates research and development in biomedicine and other
life sciences through its rapidly growing presence in clinical medicine [1–5], outbreak tracing
[6–10], and understanding of pathogens and urban microbial communities [11–15]. These
developments are driven in part by the successful sequencing of the human genome [16] and in
part by the introduction of high-throughput sequencing technologies that have dramatically
reduced the cost of DNA sequencing [17]. The bioinformatics community has developed a

1

multitude of software tools to leverage increasingly large and complex sequencing datasets
[18–20]. These tools have reshaped the landscape of modern biology and become an essential
component of life sciences [21]. The increasing dependence of biomedical scientists on these
powerful tools creates a critical need for faster and more efficient computational tools. Our
understanding of genomic molecules today is affected by the ability of modern computing
technology to quickly and accurately determine an individual’s entire genome. In order to
computationally analyze an organism’s genome, the DNA molecule must first be converted to
digital data in the form of a string over an alphabet of four letters or base-pairs (bp), commonly
denoted by A, C, G, and T. The four letters in the DNA alphabet correspond to four chemical
bases, adenine, cytosine, guanine, and thymine, respectively, which make up a DNA molecule.
After more than 7 decades of continuous attempts (since 1945 [22]), there is still no sequencing
technology that can read a DNA molecule in its entirety. As a workaround, sequencing machines
generate randomly sampled subsequences of the original genome sequence, called reads [23].
The resulting reads lack information about their order and corresponding locations in the
complete genome. Software tools, collectively known as genome analysis tools, are used to
reassemble read fragments back into an entire genome sequence and infer genomic variations
that make an individual genetically different from another.

There are five key initial steps in a standard genome sequencing and analysis workflow
[18], as we show in Figure 1. The first step is obtaining the genomic data either through
sequencing a DNA molecule, downloading real data from publicly available databases, or
computer simulation. Sequencing requires the collection and preparation of the sample in the
laboratory or on-site. The second step, known as basecalling, is to process the raw sequencing
data as each sequencing technology generates different representations of the sequencing
data. The basecalling step must convert the raw sequencing data into standard format of
sequences of A, C, G, and T in the DNA alphabet. The third step, known as quality control,
examines the quality of each sequenced base and decides which bases of a read to trim, as
sequencing machines introduce different
types and rates of sequencing errors leading to
inaccurate end results. The fourth step is a process known as read mapping, which matches
each read sequence to one or more possible locations within the reference genome (i.e., a
representative genome sequence for a particular species), based on the similarity between the
read and the reference sequence segment at that location. Unfortunately, the bases in a read
may not be identical to the bases in the reference genome at the location that the read actually
comes from. These differences may be due to (1) sequencing errors (with a rate in the range of
the read length, depending on the sequencing technology), and (2) genetic
0.1-20% of
differences that are specific to the individual organism’s DNA and may not exist in the reference
genome. A read mapping step must tolerate such differences during similarity check, which
makes read mapping even more challenging. The fifth step, known as variant calling, aims to
identify the possible genetic differences between the reference genome and the sequenced
genome. Genetic differences include small variations [24] that are less than 50 bp, such as
single nucleotide polymorphisms (SNPs) and small
insertions and deletion (indels). Genetic
differences can also be larger than 50 bp variations, known as structural variations [25], which
are caused by chromosome-scale changes in a genome. For example, insertion of an about
600,000-base long region has been observed in some chromosomes [26].

2

3

Figure 1: Overview of a typical genome analysis pipeline. 1) Genomic sequencing data is
first obtained through sequencing a new sample, downloading from publicly-available
databases, or computer simulation. Sequencing starts with (A) extracting the DNA, (B)
fragmenting it, (C) preparing its library, and (D) using sequencing machines for providing raw
sequencing data. 2) Raw sequencing data needs to be converted into read sequences of A,
C, G, T in the DNA alphabet using basecalling techniques. Basecalling techniques are
sequencing technology dependent. 3) To ensure high quality sequencing reads, a quality
control step is performed to filter out low quality subsequences of a read or an entire read
sequence. 4) Read mapping step is performed to locate each read sequence in a reference
genome. Read mapping is four steps: (A) indexing the reference genome, (B) extracting
seeds from each read and locating common seeds with the index, (C) pre-alignment filtering
dissimilar sequence pairs, and (D) performing sequence alignment for every sequence pair
that passes the filtering. 5) Detecting and inferring genomic variations usually consists of
three steps: (A) processing the read mapping data for increasing its quality, (B) classifying
variations between mapped reads and a reference genome, and (C) identifying genomic
variations.

We define intelligent genome analysis as the ability to read, analyze, and interpret
genomes not only quickly, but also accurately and efficiently enough to scale the analysis to
population level. Some existing works on accelerating one or more steps in genome analysis
sacrifice the optimality of the analysis results in order to reduce execution time (as described in
[18,27]). Genome analysis is currently a first-tier diagnostic test for critically ill patients with rare
genetic disorders, which necessitates the need for making the analysis much faster while
maintaining the same or providing better analysis accuracy for successful clinical practice
[1,2,28,29]. For example, it is observed that at least 10% of read sequences simulated from the
human reference genome remain unmapped across 14 state-of-the-art aligners [30] due to
potential mapping artifacts [31], which demonstrates that accuracy is still an issue even in read
mapping that is extensively studied. On the other hand, the vast majority of genome analysis
tools are implemented as software running on general-purpose computers, while sequencing is
performed using extremely specialized, high-throughput machines [18]. This introduces two
main problems. 1) Modern sequencing machines generate read fragments at an exponentially
higher rate than prior sequencing technologies, with their growth far outpacing the growth in
computational power in recent years [32]. 2) Genome analysis generates a large amount of data
movement between the sequencing machine and the computer performing the analysis and
between different components (e.g., compute units and main memory) of computers. The data
movement across power-hungry buses is extremely costly in terms of both execution time and
energy [33–41]. Increasing the number of CPUs used for genome analysis may decrease the
overall analysis time, but significantly increases energy consumption and hardware costs, and
potentially worsens the data movement bottleneck (more cores competing for memory access)
[42,43]. Cloud computing platforms still suffer from similar issues along with additional concerns
due to genomic data protection guidelines in many countries [44–49].

4

These costs and challenges are a significant barrier to enabling intelligent genome
analysis that can keep up with sequencing technologies. As a result, there is a dire need for
new computational techniques that can quickly process and analyze a tremendous number of
extracted reads in order to drive cutting-edge advances in applications of genome analysis
[27,35,36,50,51]. There exists a large body of work trying to tackle this problem by using
intelligent algorithms,
intelligent hardware accelerators, and intelligent hardware/software
co-design [33–35,52–54]. Computer algorithms and hardware architectures are called intelligent
they are able to efficiently satisfy three principles, data-centric, data-driven, and
if
architecture/algorithm/data-aware [55]. First, we would like to process genomic data efficiently
by minimizing data movement and maximizing the efficiency with which data is handled, i.e.,
stored, accessed, and processed. Second, we would like to take advantage of the vast amounts
of genomic data and metadata to continuously improve decision making (  self-optimizing
decisions) for many different use cases in science, medicine, and technology. Third, we would
like to orchestrate the multiple components across the entire analysis system and adapt
algorithms by understanding the structure of the underlying hardware, understanding analysis
algorithms, and understanding various properties (i.e., the structure of the genome, type of
sequencing data, quality of sequencing data) of each piece of data. Our goal in this work is to
survey a prominent set of these three types of intelligent acceleration efforts for guiding the
design of new highly-efficient tools for intelligent genome analysis. To this end, we (1) discuss
various state-of-the-art mechanisms and techniques that improve the execution time of one or
more steps of the genome analysis pipeline using different modern high-performance computing
architectures, and (2) highlight the challenges, that system designers and programmers must
address to enable the widespread adoption of hardware-accelerated intelligent genome
analysis.

2. Obtaining Genomic Sequencing Data

Genomic sequencing data (reads) can be obtained by 1) sequencing a DNA sample, 2)
downloading real sequencing data from publicly available databases, or 3) simulating
sequencing data, as we show in Figure 1.1.

2.1. Generating Sequencing Data
One of the earliest successful protein sequencing attempts was made in the 1950s by Frederick
Sanger who devoted his scientific life to the determination of the chemical structure of biological
molecules, especially that of insulin [22,23,56,57]. The first DNA sequencing was successful
only after two decades, in 1977, as introduced in two different sequencing methods by Sanger
and Coulson [56], and by Maxam and Gilbert [58]. Though there was constant progress in
sequencing attempts for different small genomes, we could obtain the first draft of the human
genome sequence only in June 2000 as a result of a large international consortium, costing
more than USD 3 billion (USD 1 for each DNA base) and more than 10 years of research
[16,59]. This sequencing era was referred to as first generation sequencing. Since then, DNA
sequencing has evolved at a fast pace with increasing sequencing throughput and decreasing
cost, which lead to frequent updates and major improvements to the human genome sequence

5

[60–62] and very recently have resulted in a near-complete human genome sequence [63].
These advances reshaped the landscape of modern biology and sequencing became an
essential component of biomedical research.

Generating sequencing data includes three key steps: sample collection, preparation
(known as library preparation [64]), and sequencing (Figure 1.1). Sample collection and library
preparation significantly depend on the protocol, preparation kit, minimum amount of DNA in the
sample, and the sequencing machine, which require meeting rigorous requirements by the
manufacturer of sequencing machines for the successful sequencing. The sample collection
and library preparation steps are performed using non-computational methods in the “wet”
laboratory or on-site, especially when the used sequencing machine is portable, prior to
performing the actual sequencing. Each sequencing machine has different properties such as
sequencing throughput, read length, sequencing error rate, type of raw sequencing data,
sequencing machine size, and cost. Sequencing throughput is defined as the number of bases
generated by the sequencing machine per second. Reads can have different lengths and they
can be categorized into three types: 1) short reads (up to a few hundred bp), 2) ultra-long reads
(ranging from hundreds to millions of bp), and 3) accurate long reads (up to a few thousands of
bp). Though most of the existing sequencing machines are fundamentally different, they also
share common properties, such as requiring library preparation, generating only fragments (i.e.,
reads) of the DNA sequence, introducing errors in the output sequencing data, and requiring
converting the raw sequencing data into sequences of nucleotides (i.e., A, C, G, and T in the
DNA alphabet). Most existing sequencing technologies start with DNA fragmentation as part of
the library preparation protocol. DNA fragmentation refers to intentionally breaking DNA strands
into fragments using, for example, resonance vibration [64,65]. DNA fragmentation helps to
exploit a large number of DNA fragments for higher sequencing yield, as sequencing quality
usually degrades towards the end of long DNA fragments [66] due to for example the limited
lifetime of polymerase enzymes used for sequencing [67].

laboratories, and authors of

2.2. Downloading Real Sequencing Data
Research groups,
research papers normally release their
sequencing data on publicly available repositories to meet requirements of both reproducibility
in biomedical and life science research and journals for data sharing [68]. There are currently
more than 29 peta (1015) bases of sequencing data publicly available as FASTQ files on the
Sequence Read Archive (SRA) database [69], which is doubling in the number of bases every 2
years [70]. Other databases are also available, such as the European Nucleotide Archive (ENA)
[71]. There are also a large number of reference genome sequences, as FASTA files, for more
than 108,257 distinct organisms publicly available in the NCBI Reference Sequence Database
(RefSeq) database [72], which is doubling in the number of organisms every 3 years [73].

2.3. Simulating Sequencing Data
Sequencing data can be generated using computer simulation [74–77]. Many computational
laboratories use simulated sequencing data, as they lack adequate resources to generate their
sequencing data using sequencing machines [19] or lack access to gold standard experimental
data when self-assessing a newly developed tool [12,78]. Existing sequencing technologies use

6

result

in sequencing data with different
different mechanisms and chemistries that
characteristics. Read simulators take into account many of these characteristics by modeling
them according to each technology. These simulators differ in target sequencing technology,
input requirements, and output format. They also have several aspects in common, such as
requiring a reference genome, the minimum and maximum read lengths, sequencing error
distribution, and type of genetic variations (e.g., substitutions, insertions, or/and deletions) that
make read sequences different from the reference genome sequence. Most read simulators
generate different standard file formats, such as FASTQ, FASTA, or BAM, which can be used to
locate potential mapping locations of each read in the reference genome.

Examples of read simulators for these three types include ART [79], Mason [77], and
ReSeq [80] for short reads, PBSIM2 [75] and NanoSim [76] for ultra-long reads, and PBSIM [81]
for accurate long reads (known as HiFi reads). However, the use of read simulations poses
several
limitations, as simulated data recapitulates the important features of real data and
oversimplifies/biases the challenge for tested methods [19]. To avoid such biases, a common
practice and more-comprehensive approach is to complement the simulated data with real
experimental data. Hence, most journals require making the new sequencing data publicly
(a unique identifier given to
accessible and explicitly reporting the accession number
sequencing data) of existing sequencing data used in the study.

3. Types of Genomic Sequencing Data

There currently exist three different sequencing technologies that are widely-used to sequence
DNA samples around the globe and in space [82]. Current prominent sequencing technologies
and their output data can be categorized into three types: 1) short reads, 2) ultra-long reads,
and 3) accurate long reads. In Table 1, we summarize the main differences between these three
types of prominent sequencing technologies. We provide more details on the sequencing
machine size, cost, throughput, maximum library preparation time, and maximum sequencing
time for the most capable instrument for each sequencing technology.

Table 1. Summary of
the main differences between the three types of prominent
sequencing technologies. We choose the most capable instrument as a representative of
each sequencing technology.

Short Reads

Ultra-long Reads

Accurate Long
Reads

Leading company

Illumina
(https://www.illumina.
com)

Oxford Nanopore
Technologies
(https://nanoporetec
h.com)

Pacific Biosciences
(https://www.pacb.co
m)

Representative
instrument

Illumina NovaSeq
6000

ONT PromethION 48

PacBio Sequel IIe

7

Instrument picture

Instrument weight

481 kg

28 kg

362 kg

Instrument
dimension (W×H×D
in cm)

Instrument cost1

Read length

Read length in a
single data file

Accuracy

Maximum
sequencing
throughput per run

Number of flow
cells operating
simultaneously

Hands-on library
preparation time

Turnaround library
preparation time4

Sequencing run
time
(including
basecalling)

80 × 94.5 × 165.6

59 × 19 × 43

92.7 x 167.6 x 86.4

3Y

100-300

Fixed

99.9%

6 Tb

Y

100-2M

1.6Y

10K-30K

Variable

Modest Variability

90%-98%

99.9%2

14 Tb

35 Gb (160 Gb of raw
sequencing data)3

2

48

1

45 minutes - 2 hours

10 minutes-3 hours

6 hours

1.5-6.5 hours

24 hours

24 hours

44 hours

72 hours

30 hours

1Y can be as low as USD 300,000. The cost does not include consumables (e.g., flow cells and
reagents) needed for each sequencing run.
2Consensus accuracy.

8

3Per SMRT cell. Sequel
IIe can process up to 8 SMRT cells sequentially, slide 25
https://www.pacb.com/wp-content/uploads/HiFi_Sequencing_and_Software_v10.1_Release_Techni
cal_Overview_for_Sequel_II_System_and_Sequel_IIe_System_Users-Customer-Training-01.pdf.
4Includes both hands-on library preparation time and waiting time.

3.1. Short Reads
Short read sequencing (sometimes called second generation sequencing) technologies, such as
Illumina [17,83,84] and Singular Genomics [85], generate subsequences of length 100-300 bp.
Illumina is currently the dominant supplier of sequencing instruments. The key advantage of
Illumina short reads is the significantly low sequencing error rate (as low as 0.1% of the read
length) introduced by the sequencing machine [86]. Over 6 terabases can be generated in a
single sequencing run in about two days using a single instrument (Illumina NovaSeq 6000).
Illumina sequencing technology, called sequencing by synthesis (SBS), is very similar to that of
the first generation sequencing (i.e., Sanger) [87,88]. A key procedural difference in comparison
to Sanger sequencing is in the preparation of
the sequencing library and the degree of
parallelism and throughput during sequencing. Sanger sequencing libraries require multiple
steps that require growth in culture and DNA isolation before sequencing [88]. This multistep
process can be completed in approximately one week, at which point the processed DNAs are
ready for sequencing.

Illumina sequencing requires a hands-on time of less than two hours (or a turnaround
time of up to 6.5 hours) to prepare the sample for sequencing. Library preparation in Illumina
sequencing starts with fragmenting the DNA into short fragments and adding adapters (short
single strands of synthetic DNA, called oligonucleotides [89]) to both fragment ends. These
adapters enable binding each fragment to the flow cell. Fragments can then be amplified to
create clusters of up to 1,000 identical copies of each single fragment. Sequencing is then
performed base-by-base using a recent technique called 2-channel sequencing [90]. During
each sequencing cycle (3.5-6.75 minutes [91]), a mixture of two nucleotides, adenine and
thymine, labeled with the same fluorescent dye (i.e., green color) is added to each cluster in the
flow cell. Images are taken of the light emitted from each DNA cluster using a CMOS sensor.
Next, a mixture of two nucleotides, adenine and cytosine, labeled with another fluorescent dye
(i.e., red color) is added to each cluster in the flow cell. Another image is taken of the light
emitted from each DNA cluster. During basecalling, the combinations of “light observed” and “no
light observed” in the two images are interpreted. E.g., if the light is only observed in the first
image, it is interpreted as a thymine base. If the light is observed in the second image, it is
interpreted as a cytosine base. Clusters with light in both images are flagged as adenine bases,
while clusters with no light in both images represent guanine bases. This process is repeated on
each nucleotide for the length of the DNA fragment (read). The large number of clusters and the
straightforward basecalling process make Illumina sequencing provide the highest throughput of
accurate bases compared to other sequencing techniques.

9

3.2. Ultra-long Reads
Ultra-long read (or nanopore) sequencing, called third generation sequencing, is more recent
than Illumina sequencing. The first nanopore sequencing machine, MinION, was introduced in
2014 and made commercially available in 2015. The main concept behind nanopore sequencing
was brainstormed much earlier in the 1990s [92]. The first MinION sequencing machine was
promising as it was incredibly small in size (smaller than the palm of a hand and weighing only
87 g) compared to existing sequencing machines. However, its sequencing accuracy rate was
65% (i.e., one out of every three bases is erroneous, which can significantly degrade the
accuracy of downstream analyses) [93].

The nanopore sequencing technology requires first preparing the sequencing library by
fragmenting the DNA sequence and adding a sequencing adapter and a motor protein at each
end of
the fragment. The sequencing starts with passing each DNA fragment through a
nanoscale protein pore (nanopore in short) that has an electrical current passing through it. The
sequencer measures changes to an electrical current as nucleic acids, each with different
electrical resistance, are passed through the nanopore. Using the computational basecalling
step, the electrical signals are decoded into a specific DNA sequence. The motor protein helps
control the translocation speed of the DNA fragment through the nanopore. Over 14 terabases
can be generated in a single sequencing run in about 3 days using a single ONT PromethION
48 instrument (Table 1). Recent nanopore machines use dual electrical current sensors (called
reader heads) to increase the accuracy of sensing and improve the detection resolution [93,94].
The accuracy of nanopore sequencing technology has been constantly improving from 65% to
above 90% and can reach 98% for some reads due to both dual sensing and improved
basecalling [95–98]. However, this comes at the cost of computationally-expensive basecalling,
as we discuss in Section 5.

3.3. Accurate Long Reads
The latest sequencing technology, referred to as third or fourth sequencing technology [99], is
from Pacific Biosciences (PacBio). It generates high-fidelity (HiFi) reads that are relatively long
(10-30K) and highly accurate (99.9%)
[95,100,101]. The PacBio sequencing requires
fragmenting the DNA molecule and adding double-stranded adapters (called SMRTbell) to both
ends of the fragment. The DNA fragment has a DNA subsequence binding to its reverse
complement sequence. This creates a circular DNA fragment for sequencing. The PacBio
sequencing leverages multiple pass circular consensus sequencing (CCS) by sequencing the
same circular DNA fragment at least 30 times and then correcting errors by calculating a
consensus sequence [101]. Each sequencing pass through the circular DNA fragment produces
a subread, which is used to calculate the consensus sequence by overlapping all resulting
subreads of a single DNA fragment. The PacBio sequencing uses a polymerase that passes
through the circular DNA fragment and incorporates fluorescently labeled nucleotides. As a
base is held by the polymerase, fluorescent light is produced and recorded by a camera in
real-time. The camera provides a movie of up to 30 hours of continuous fluorescent light pulses
that can be interpreted into bases. The PacBio sequencing provides the least sequencing
throughput compared to Illumina and ONT. Over 35 gigabases can be generated in a single
sequencing run in about 30 hours (Table 1).

10

(number of basecalled bases). Another property of short

3.4. Discussion on Types of Sequencing Reads
Short reads have the advantages of both low sequencing error rate and high sequencing
reads that can be
throughput
considered as an advantage is the equivalent length of all reads stored in the same FASTQ file,
which helps in achieving, for example,   load balancing between several CPU threads. Repetitive
regions in genomes pose challenges for constructing assembly (de novo assembly [102]) using
short reads. De novo assembly is an alternative to read mapping, in which it constructs the
sequence of a genome from overlapping read sequences without comparison to a
reference-genome sequence. Both ultra-long reads and accurate long reads in general offer
better opportunities for genome assembly and detecting complex structural variant calling.

Ultra-long reads provide two main advantages: 1) providing more contiguous assembly
than that of short reads, where each contig can read up to 3 Mb (covering the typical length of
bacteria genomes) [103], and 2) its read length is theoretically limited only by the length of the
DNA fragment
translocating through the pore [104] as it does not require enzyme-based
nucleotide incorporation, amplification for cluster generation, nor detection of fluorescence
signals [93,105]. However, nanopore sequencing data suffers from high sequencing error rate
and some of its computational analysis steps require longer executing time and higher memory
footprint compared to both short reads and accurate long reads. For example, performing de
novo assembly (using Canu [106]) using ultra-long reads is at least fourfold slower than that
when using accurate long reads, which is mainly because of the errors in the raw sequencing
data [103]. This also leads to introducing new computational steps with the goal of polishing
errors in the assembly [96,98,107]. Such new steps normally increase the computation
overhead of analyzing ultra-long reads as the new steps are computationally expensive [107].
Another example of expensive steps for analyzing ultra-long reads is basecalling, which is
based on neural networks. For this, ONT PromethION 48 includes 4 A100 GPU boards for
accelerating basecalling and coping with its sequencing throughput.

The high accuracy of accurate long reads is a key enabler of the recent improvements in
human genome assembly and unlocking complex regions of repetitive DNA [63]. More than
50% of the regions previously inaccessible with Illumina short reads for the GRCh37 human
reference genome are now accessible with HiFi reads (supplemented with ultra-along reads) for
the GRCh38 human reference genome [101]. For other applications, such as profiling
microbiomes through metagenomics analysis, the use of accurate long reads leads to detecting
the same numbers of species as with the use of short reads [108]. However, the sequencing
cost and computational expensive basecalling required to generate HiFi reads currently
challenge widespread adoption.

This suggests that there is no preferable sequencing data type for all applications and
use cases. Each sequencing technology has its own unique advantages and disadvantages.
The short reads will continue to be widely used due to their very high accuracy and low cost.
With increases in read lengths of Illumina sequencing technology [109], there will be a growing
demand for adjusting existing algorithms or introducing new algorithms that leverage new

11

properties of anticipated Illumina sequencing data. With increases in accuracy of long and
ultra-long reads,
their
computational steps (e.g., basecalling) and reducing overall sequencing cost
to enable
widespread adoption.

there will be a growing demand for

improving execution time of

4. Genome Analysis Using Different Types of Sequencing Reads

We evaluate the performance of three typical genome analysis pipelines (Figure 1) for the three
prominent sequencing data types, 1) short reads, 2) ultra-long reads, and 3) accurate long
reads, as we show in Figure 2. We report the throughput of each step using a single CPU
thread, running on a 2.3 GHz Intel Xeon Gold 5118 CPU with up to 48 threads and 192 GB
DDR4 RAM. Note that nanopore basecalling is based on the throughput of Guppy running on a
CPU [110]. We report the sequencing throughput using a single flow cell. We calculate the
throughput of each step by dividing the number of bases (outputted by sequencing and
basecalling or taken by quality control, read mapping, and variant calling) over the total
execution time in hours. We provide the data and exact command lines used to run each tool in
the GitHub repository of this paper.

Figure 2: Performance comparison of the five main steps of genome analysis pipeline.

We make five key observations based on Figure 2. 1) Short read sequencing provides
the highest throughput per flow cell compared to other long read sequencing technologies. 2)
Genome analysis of both ultra-long reads and accurate long reads suffers from long execution
time of their basecalling step. This is expected because both technologies use computationally
expensive basecalling steps, as we explain in detail in Section 5. 3) Read mapping is the most
computationally expensive step, followed by variant calling, in the genome analysis pipeline for
short reads. 4) The first major bottleneck in the genome analysis pipelines for both accurate
long reads and ultra-long reads is variant calling. Read mapping is the second and the third
bottleneck in the genome analysis pipelines for accurate long reads and ultra-long reads,

12

respectively. 5) Sequencing throughput is 341⨉ and 56.8⨉ higher than the throughput of read
mapping and variant calling of short reads, respectively. Sequencing throughput is 2.4⨉ and
93.2⨉ (3.8⨉ and 4.8⨉) higher than the throughput of read mapping and variant calling of
ultra-long reads (accurate long reads), respectively.

We conclude that each type of sequencing data imposes different acceleration
challenges and creates its own computational bottlenecks. There is also a dire need for
techniques that can overcome the existing computational
developing new computational
bottlenecks and building new hardware architectures that can reduce data movements between
different steps of genome analysis and improve overall analysis time and energy efficiency. In
the next sections, we survey various state-of-the-art mechanisms and techniques that improve
the execution time of one or more steps of the genome analysis pipelines for different types of
sequencing data.

5. Basecalling

regardless of

Most existing sequencing machines do not provide read sequences in the DNA alphabets.
Instead, they provide a native format that is sequencing technology dependent. Thus, existing
sequencing technologies require a basecalling step (Figure 1.2)
the native
sequencing data format into a standard format that can be understood by all existing genome
the sequencing technology used. Basecalling is the first
analysis tools,
computational step in the genome sequencing pipeline that converts raw sequencing data
(images for
for ONT) into sequences of
nucleotides (i.e., A, C, G, and T in the DNA alphabet). We provide in this section a brief
description of basecalling for the three prominent sequencing technologies, Illumina short reads,
Oxford Nanopore ultra-long reads, and PacBio HiFi accurate long reads. We summarize the
main differences between their basecalling techniques in Table 2.

Illumina, movie for PacBio, or electric current

to convert

Illumina sequencing,

5.1. Illumina
During each cycle of
two images are taken to identify the possible
chemical reactions occurring in DNA clusters. The light intensities captured in one or both
images directly represent the type of the subject nucleotide, as we explain in Section 3.1. This
information is stored as binary data in the CBCL file format. Each sequencing run provides a
large number of CBCL files that need to be converted into a single FASTQ file. The basecalling
per CBCL file can take up to 0.21 minutes (50% of it is spent on reading and writing from/to
files) [111], which is significantly less than the time for a single sequencing cycle (3.5-6.75
minutes [91]).

13

the main differences between the three types of prominent
Table 2. Summary of
sequencing technologies. We choose the most capable instrument as a representative of
each sequencing technology.

Type of raw
sequencing data
(before basecalling)

Input file format for
basecalling

Expected size of
basecalling input
file

Short Reads

Ultra-long Reads

Accurate Long
Reads

Multiple images of
fluorescence
intensities for each
sequencing cycle

Electrical signal for
each DNA segment

Fluorescence traces
captured continuously
into a 30-hour movie

BCL or CBCL

FAST5

BAM

One CBCL file of
size 350MB per
cycle, lane, and
surface

10⨉ the size of the
corresponding
FASTQ file

Subreads.BAM of
size 0.5-1.5 TB

Basecalling
algorithm

BCL2FASTQ

Guppy/Bonito (deep
neural networks)

CCS

Basecalling time

48 minutes1

142 minutes2

24 hours3

Number of
basecalled bases

83.5 Gb1

20 Gb2

200 Gb of HiFi yield3

1BCL2FASTQ based on SRR2890933, 1.67 billion reads (full 8 lanes) at 50bp read length [112].
2Using a single V100 GPU, adapted from [110].
3Using CCS v6.0 for a 25 KBases library on 2x64 cores at 2.4 GHz, adapted from [113].

5.2. ONT
For nanopore sequencing, the conversion of the raw electrical current signal or squiggle into a
sequence of nucleotides is challenging because: (1) signals have stochastic behavior and low
signal-to-noise ratio (SNR) due to thermal noise and the lack of statistically significant current
signals triggered by DNA motions, (2) electrical signals can have long dependencies of event
data on neighboring nucleotides, and (3) the sensors (reader heads) used cannot measure the
changes in electrical current due to a single nucleotide, but rather measuring the effect of
multiple nucleotides together. Nanopore raw data are current intensity values measured at 4
kHz, saved in the FAST5 format
(a modified HDF5 format). Nanopore basecalling is
computationally expensive and its algorithm is quickly evolving. Neural networks have
supplanted hidden Markov model (HMM) based basecallers for their better accuracy, and
various neural network structures are being tested [95,114]. Another important advantage of
nanopore basecalling is its ability to improve read accuracy (by 10% [115]) by correcting
possible sequencing errors.

14

ONT provides a production basecaller, called Guppy, and its development version,
Bonito. Guppy is optimized for performance and accelerated using modern GPUs. Modern
neural network basecallers such as Guppy and Bonito are typically composed of: (1) residual
layers, and/or (2) long short-term memory-based recurrent layers.
network with convolutional
RNN-based models are used to model the temporal dependencies present in basecalling data.
Other than Guppy, which is a GPU-based basecaller, most of the existing basecallers are only
implemented for CPU execution. Thus, basecallers lack dedicated hardware acceleration
implementations, which could greatly reduce the basecalling time. Even Guppy takes 25 hours
to basecall a 3 Gb human genome on a powerful server-grade GPU [116]. Potential alternatives
to further accelerate Guppy and other basecallers are processing-in-memory techniques
inside
[33–36,50,52–54,117–119], which consist of placing compute capabilities near or
memory. PIM techniques are particularly well suited for workloads with memory-bound behavior,
as
that, algorithm/architecture co-designed
processing-in-memory (PIM) can accelerate Guppy by 6⨉ [116]. ONT also provides Guppy-lite,
a throughput-optimized version of Guppy that provides higher performance at the expense of
the basecalling accuracy. Therefore, basecallers need to balance the tradeoff between speed
and accuracy depending on the requirements of a particular application [121].

show [39,120]. For

for example RNNs

Instead of bascalling followed by analysis in basepair space, several works propose
analyzing reads directly in their raw signal space, obviating or alleviating the need for expensive
basecalling [122,123]. One motivation for such proposals is the Read-Until feature [124] of ONT
sequencing machines, which allows to physically eject reads from each nanopore in real time if
they are deemed not interesting for the application (e.g., do not belong to target species).
Ejecting a DNA segment requires analyzing the partial squiggle signal much earlier than
completing sequencing the complete DNA segment, which the computationally expensive
basecalling cannot satisfy. SquiggleNet [125] is a deep-learning-based approach that classifies
DNA sequences directly from electrical signals. SquiggleNet provides 90% accuracy with
real-time processing, which means that SquiggleNet wrongly identifies, on average, 10% of the
DNA segments as irrelevant. UNCALLED [126] is another approach that segments raw signals
into possible k-mers and uses Ferragina-Manzini (FM) index along with a probabilistic model to
search for k-mer matches with the target reference genome. UNCALLED achieves an accuracy
of 93.7% with an average detection speed of <50ms per DNA segment. Sigmap [127] has
comparable performance to UNCALLED, but
it overcomes the applicability limitations of
UNCALLED to large genomes by optimizing the raw signal mapping pipeline. This optimization
leads to a 4.4⨉ improvement in detection performance. SquiggleFilter [125] matches reads
against a target genome using hardware-accelerated dynamic time warping algorithm [119,128].
SquiggleFilter needs fewer signal measurements to identify irrelevant DNA segments compared
to UNCALLED, allowing for earlier ejection from the pores. It is computationally cheaper and
particularly suited for sequencing in the field, where the computational resources are limited.
The computational step of SquiggleFilter can be as fast as 0.027ms per DNA segment,
depending on the size of the to-be-compared-with reference genome. Analysis in raw signal
space can also provide information that would otherwise be lost during basecalling, such as
chemical modifications of nucleotides, or estimating the length of the poly(A) tail of mRNA [122].

15

5.3. PacBio
For PacBio sequencing, 30 hours of continuous fluorescent light pulses are recorded as a
movie. The movie can be directly interpreted into bases, resulting in multiple subreads (stored in
BAM format), where each subread corresponds to a single pass over the circular DNA fragment.
The current PacBio basecalling workflow is CCS [129], which includes aligning the subreads to
each other using computationally expensive sequencing alignment tools, such as KSW2 [130]
and Edlib [131], and other polishing steps. We observe that there are currently neither hardware
acceleration nor alternative more efficient (more customized than KSW2 and Edlib) algorithms
for improving the runtime of PacBio basecalling.

6. Quality Control

The goal of the quality control (QC) step (Figure 1.3) is to examine the quality of some regions
in the read sequence or the entire read sequence and trim them if they are of low quality or not
needed anymore (as in adapters for sequencing). Some of the causes for low quality regions
are library preparation (e.g., fragmenting the DNA into very short fragments) and sequencing
(e.g., low base quality during sequencing) [95,132]. The QC step ensures high quality of the
reads and hence high quality downstream analysis.

The QC step evaluates the quality of the entire read sequence by (1) examining the
intrinsic quality of the read sequence, that is the average quality score generated by the
sequencing machine before or after basecalling, (2) assessing the length of the read, and (3)
evaluating the number of ambiguous bases (N) in a read. The QC step also evaluates the
quality of individual bases for (1) masking out an untrustworthy region in a read if the average
quality score of the region’s bases is low and (2) trimming beginning or/and trailing regions of a
read if it is. For example, Illumina sequences have degraded quality towards the ends of the
reads, while adapter sequences are added to both head and tail of the DNA fragment for
sequencing. Looking at quality distribution over base positions can help decide the trimming
sites. There are also other quality control steps that can be applied during or after read
mapping, such as those provided by Picard [133], and can be hardware accelerated as in [134].
Some other quality control steps can be carried out directly after sequencing and before
basecalling, such as trimming barcode sequences used for labeling different samples to be
sequenced within the same sequencing run [135,136].

There exists a number of software tools for performing QC. FastQC [137] is the most
popular tool for controlling the quality of Illumina sequencing data. FastQC takes a FASTQ file
as its input and performs ten different quality analyses. A given FASTQ file may pass or fail
each analysis and a FASTQ file is usually accepted when it passes all quality analyses. There
are also a large number of QC software tools for long read (both nanopore and HiFi), such as
LongQC [138], which uses expensive read mapping, minimap2 [130],
for low coverage
detection. RabbitQC [139] exploits modern multicore CPUs to parallelize the QC computations
and provides an order of magnitude faster performance for the three prominent types of
sequencing technologies. We observe that there are currently a large number of software tools
for QC, but we still lack efficient hardware accelerators for improving the QC step.

16

7. Read Mapping

The goal of read mapping is to locate possible subsequences of the reference genome
sequence that are similar to the read sequence while allowing at most E edits, where E is the
edit distance threshold. Tolerating a number of differences is essential for correctly finding
possible locations of each read due to sequencing errors and genetic variations. Mapping
billions of reads to the reference genome is still computationally expensive [27,35,36,53,140].
Therefore, most existing read mapping algorithms exploit two key heuristic steps, indexing and
filtering, to reduce the search space for each read sequence in the reference genome. Read
mapping includes four computational steps (Figure 1.4),
indexing, seeding, pre-alignment
filtering, and sequence alignment. First, a read mapper starts with building a large index
database using subsequences (called seeds) extracted from a reference genome to enable
quick and efficient querying of the reference genome. Second, the mapper uses the prepared
index database to determine one or more possible regions of the reference genome that are
likely to be similar to each read sequence by matching subsequences extracted from each read
with the subsequences stored in the index database.

Third, the read mapper uses filtering heuristics to quickly examine the similarity for every
read sequence and one potential matching segment in the reference genome identified during
seeding. As only a few short subsequences are matched between each read sequence and
each reference genome segment, there can be a large number of differences between the two
sequences. Hence filtering heuristics aim to eliminate most of the dissimilar sequence pairs by
performing minimal computations. Fourth, the mapper performs sequence alignment to check
whether or not the remaining sequence pairs that pass the filter are actually similar. Due to
potential differences, the similarity between a read and a reference sequence segment must be
identified using an approximate string matching (ASM) algorithm. The ASM typically uses a
computationally-expensive dynamic programming (DP) algorithm to optimally (1) examines all
possible prefixes of two sequences and tracks the prefixes that provide the highest possible
alignment score (known as optimal alignment), (2) identify the type of each difference (i.e.,
insertion, deletion, or substitution), and (3) locate each difference in one of the two given
sequences. Such alignment information is typically output by read mapping into a sequence
alignment/map (SAM, and its compressed representation, BAM) file [141]. The alignment score
is a quantitative representation of the quality of aligning each base of one sequence to a base
from the other sequence. It is calculated as the sum of the scores of all differences and matches
along the alignment implied by a user-defined scoring function. DP-based approaches usually
have quadratic time and space complexity (i.e., (m2) for a sequence length of m), but they avoid
re-examining the same prefixes many times by storing the examination results in a DP table.
The use of DP-based approaches is unavoidable when optimality of the alignment results is
desired [142]. Given the time spent on read mapping, all four steps have been targeted for
acceleration.

17

Table 3: Evaluation analysis of
minimap2, and pbmm2,
ultra-long reads, and accurate long reads, respectively.

for three different

three state-of-the-art

read mappers, BWA-MEM2,
types of sequencing data, short reads,

Short reads

Ultra-long reads

Accurate long reads

Read mapping tool

BWA-MEM2

minimap2

Version

2.2.1

2.24-r1122

pbmm2

1.7.0

Reference genome

Human genome (HG38), GCA_000001405.15, FASTA size 3.2 GB

Indexing time (CPU)

2002 sec

Indexing peak memory

72.3 GB

Indexing size*

17 GB

163 sec

11.4 GB

7.3 GB

144 sec

14.4 GB

5.7 GB

Read set (accession
number)

https://www.ebi.ac
.uk/ena/browser/vi
ew/ERR194147

https://ftp-trace.ncbi.n
lm.nih.gov/Reference
Samples/giab/data/A
shkenazimTrio/HG00
3_NA24149_father/U
CSC_Ultralong_Oxfor
dNanopore_Promethi
on/GM24149_1.fastq.
gz

https://ftp-trace.ncbi.n
lm.nih.gov/Reference
Samples/giab/data/A
shkenazimTrio/HG00
3_NA24149_father/P
acBio_CCS_15kb_20
kb_chemistry2/reads/
PBmixSequel729_1_
A01_PBTH_30hours
_19kbV2PD_70pM_H
umanHG003.fastq.gz

Number of reads

1,430,362,384

6,724,033

1,289,591

Number of bases

144,466,600,784

82,196,263,791

24,260,611,730

Read mapping time

868.9 hours1

48.6 hours

17.5 hours

Mapping peak memory

131.2 GB1

9.6 GB

20 GB

Number of mapped
reads2

Mapping throughput (input
bases/mapping time)

Number of output
mappings (SAM)

File size of output
mappings (SAM)

2,842,576,947

3,854,572

1,286,256

46,185 bases/sec

469,801 bases/sec

385,090 bases/sec

2,875,143,231

8,322,218

1,396,899

222.6 GB

190.2 GB

54.4 GB

1For paired-end read mapping
2After excluding secondary, supplementary, and unmapped alignments using SAMtools and
SAM flag 2308.

18

7.1. Accelerating Indexing and Seeding
Indexing and seeding fundamentally use the same algorithm to extract the subsequences from
the reference genome or read sequences. The only difference is that the indexing step stores
the seeds extracted from the reference genome in an indexing database, while the seeding step
uses the extracted seeds to query the indexing database. The indexing step populates a lookup
data structure that is indexed by the contents of a seed (e.g., its hash value), and identifies all
locations where a seed exists in the reference genome (Figure 1b). Indexing needs to be done
only once for a reference genome, thus it is not on the critical path for most bioinformatics
applications. Seeding is performed for every read sequence and thus it contributes to the
execution time of read mapping. However,
the number of extracted seeds in both steps
(indexing and seeding),
the length of each seed, and the frequency of each seed can
significantly impact the overall memory footprint, performance, and accuracy of read mapping
[130,143,144]. For example, querying very short seeds leads to a large number of mapping
locations that need to be checked for a sequence alignment, which makes later steps more
computationally costly. In contrast, querying very long seeds may prevent identifying some
mapping locations. This is because the querying process usually requires the entire seed to
exactly appear in the indexing database, and longer seeds have a higher probability of
containing mismatches. This can lead to missing some mapping locations and causing a low
accuracy (defined in this context as sensitivity, the ability of a read mapper to find the location of
a read sequence that already exists in the reference genome). The properties of the data affect
the tradeoffs between these choices, for example long reads tend to have a higher error rate,
thus shorter seed lengths are appropriate for good sensitivity. There are three major directions
for improving the indexing and seeding steps: (1) better seed sampling techniques, (2) better
indexing data structures, and (3) accelerating the task and minimizing its data movement
through specialized hardware.

7.1.1. Sampling Seeds
The goal of sampling the seeds is to reduce redundant information that can be inferred from
extracted seeds. For example, choosing all possible overlapping subsequences of length k,
called k-mers, as seeds causes each base to appear in k seeds, causing unnecessarily high
memory footprint and inefficient querying due to large number of seed hits. Thus, state-of-the-art
read mapping algorithms (e.g., minimap2 [130]) typically aim to reduce the number of seeds that
are extracted for the index structure by sampling all possible k-mers into a smaller set of k-mers.
A common strategy to choose such a smaller set is to impose an ordering (e.g. lexicographically
or by hash value) on every group of w overlapping k-mers and choosing only the k-mer with the
smallest order as a seed, known as the minimizer k-mers [145]. Minimizer-based approach
guarantees finding at least one seed in a group of k-mers, known as windowing guarantee,
ensuring low information loss depending on the values of k and w. Additionally, the frequency
(i.e., the size of the location list) of each minimizer seed can be restricted up to a certain
threshold to reduce the workload for querying and filtering the seed hits [143,146,147]. Similar
to minimizers, the syncmer approach [148] is a more recent type of sampling method with a
different selection criteria than minimizers that has shown to provide more uniform distribution of

19

seeds to achieve better sensitivity. The syncmer approach chooses a seed as a minimizer
whose substring located at a fixed location achieves the smallest order compared to that of
substrings of other seeds. This strategy ensures a certain gap between any two consecutive
minimizers, which enables read mappers to report mapping locations for reads that are
unmapped using minimizers-based read mappers [149].

To increase the sensitivity of read mappers, other approaches can be applied, such as
spaced [150] and strobemer [151] seeds. Spaced seeds [150] exclude some bases from each
seed following a predetermined pattern, where the resulting seeds are composed of multiple
shorter substrings [152]. Spaced seeds can achieve high sensitivity when finding seed matches
by allowing the excluded bases to mismatch (substitute) with their corresponding bases.
S-conLSH [153] is a recent approach that applies the locality-sensitive hashing idea and uses
multiple patterns on the same sequence to enable excluding different sets of characters
belonging to a sequence. A recent approach, known as strobemers [151], improves on spaced
seeds by varying the sizes of the spaces dynamically based on selection criterias. These
strategies for joining/linking seeds are orthogonal to minimizers and syncmers, and recent work
shows such approaches can be combined for additional sensitivity improvements [154].

There are also other works that use the minimizer sampling strategy without providing a
windowing guarantee, such as MinHash [155]. MinHash finds a single minimizer k-mer from an
entire sequence (i.e., w equals the sequence length). To find many minimizer k-mers from the
same sequence, the idea is to use many hash functions and find the minimizer k-mer from each
hash function. The goal is to find a minimizer k-mer at n-many regions of a sequence using
n-many different hash functions, providing a sampled set of k-mers. Although the MinHash
approach is effective when comparing sequences of similar lengths, it uses many redundant
minimizers when comparing sequences of varying length to ensure high accuracy, which comes
with a high cost of performance and memory overhead [156].

7.1.2. Improving Data Structures for Seed Lookups
After choosing the appropriate method for extracting seeds, the goal is to store or query them
using the index. The straightforward data structure to find seed matches is a hash table that
stores the hash value of each seed as a key, and location lists of each seed as values. Hash
tables have been used since 1988 in read mapping [18] and are still used even by the
state-of-the-art read mappers as they show good performance in practice [130]. Choosing a
hash function is an important design choice for hash tables. It is desired to use hash functions
with low collision rates so that different seeds are not assigned to the same hash value. It is also
desired to increase the collision rate for highly similar seeds to improve the overall sensitivity. A
recent approach, BLEND [144], aims to generate hash values such that highly similar seeds can
have the same hash value while dissimilar seeds are still assigned to different hash values with
low collision rates. Such an approach can find approximate (i.e., fuzzy) matches of seeds
directly using their hash values, which can be applied to other seeding approaches that enable
finding inexact matching, such as spaced seeds and strobemers.

20

There are also other data structures that can be efficiently used with the aim of reduced
memory footprint and improved querying time. One example of such data structures is FM-index
(implemented by Langarita et al. [157]), which provides a compressed representation of the
full-text index, while allowing for querying the index without the need for decompression. This
approach has two main advantages. 1) We can query seeds of arbitrary lengths, which helps to
typically has less (by 3.8⨉) memory footprint
reduce the number of queried seeds. 2) It
compared to that of the indexing step of minimap2 [18]. However, there is no significant
difference in read mapping runtime due to the use of either indexing data structure [18]. One
major bottleneck of FM-indexes is that locating the exact matches by querying the FM-index is
significantly slower than that of classical indexes [157,158]. The FM-index can be accelerated
by at least 2⨉ using SIMD-capable CPUs [159]. BWA-MEM2 [158] proposes an uncompressed
version of the FM-index that is at least 10⨉ larger than the compressed FM-index to speed up
the querying step by 2⨉. The seeding step of BWA-MEM2 can be further accelerated by 2⨉ by
using enumerated radix trees on recent CPUs to reduce the number of memory accesses and
improve the access patterns [160]. Hash-based minimizer lookup can be replaced with learned
indexes [161]. The learned indexes use machine learning models to predict the locations of the
queried minimizers. The expected benefit of such a machine learning-based index is the
reduced size of the index as it does not store the locations of the seeds. However, it is shown
that a learned-index based read mapper has the same memory footprint as the hash-table
based read mapper [162].

7.1.3. Reducing Data Movement During Indexing
Indexing and seeding remain memory-intensive tasks [53], and hence do not fit modern
processor centric systems well. An alternative approach is PIM, where processing happens
either inside the memory chip itself, or close to it [34]. This approach can improve both energy
efficiency, by moving data a shorter distance, as well as throughput, by providing more total
memory bandwidth [34]. MEDAL [163] proposes integrating small ASIC accelerators for seeding
close to off-the-shelf DRAM chips on standard LRDIMM memory modules. GenStore [53]
proposes a seeding and filtering accelerator inside SSDs, providing comparable advantages to
PIM approaches, but with the key difference that the index and reads do not have to be moved
outside of the storage device for seeding. The higher internal bandwidth of SSDs provides
increased throughput, and the reduced data movement improves energy efficiency. RADAR
[164] implements the search for exact matches in an index database by storing the database in
3D Resistive Random Access Memory (ReRAM) based Content Addressable memory
(ReCAM). The database can be directly queried without offloading it, leading to a small amount
of data movement and highly energy efficient operation.

7.2. Accelerating Pre-Alignment Filtering
After finding one or more potential mapping locations of the read in the reference genome, the
read mapper checks the similarity between each read and each segment extracted at these
mapping locations in the reference genome. These segments can be similar or dissimilar to the
though they share common seeds. To avoid examining dissimilar sequences using
read,
computationally-expensive sequence alignment algorithms, read mappers typically use filtering
heuristics that are called pre-alignment filters. The key idea of pre-alignment filtering is to quickly

21

estimate the number of edits between two given sequences and use this estimation to decide
whether or not the computationally-expensive DP-based alignment calculation is needed — if
not, a significant amount of time is saved by avoiding DP-based alignment. If two genomic
sequences differ by more than the edit distance threshold, then the two sequences are identified
as dissimilar sequences and hence DP calculation is not needed. Edit distance is defined as the
minimum number of single character changes needed to convert a sequence into the other
sequence [165]. In practice, only genomic sequence pairs with an edit distance less than or
to a user-defined threshold (i.e., E) provide useful data for most genomic studies
equal
[18,51,52,140,166,167]. Pre-alignment filters use one of four major approaches to quickly filter
out the dissimilar sequence pairs: (1) the pigeonhole principle, (2) base counting, (3) q-gram
filtering, or (4) sparse DP. Long read mappers typically use q-gram filtering or sparse DP, as
their performance scales linearly with read length and independently of the edit distance.

7.2.1. Pigeonhole Principle
The pigeonhole principle states that if E items are put into E+1 boxes, then one or more boxes
would be empty. This principle can be applied to detect dissimilar sequences and discard them
from the candidate sequence pairs used for ASM. If two sequences differ by E edits, then they
least a single subsequence (free of edits) among any set of E+1
should share at
non-overlapping subsequences [140], where E is the edit distance threshold. Pigeonhole-based
pre-alignment filtering can accelerate read mappers even without specialized hardware. For
example,
the Adjacency Filter [147] accelerates sequence alignment by up to 19×. The
accuracy and speed of pre-alignment filtering with the pigeonhole principle have been rapidly
improved over the last seven years. Shifted Hamming Distance (SHD) [167] uses SIMD-capable
CPUs to provide high filtering speed, but supports a sequence length up to only 128 base pairs
due to the SIMD register widths. GateKeeper [166] utilizes the large amounts of parallelism
offered by FPGA architectures to accelerate SHD and overcome such sequence length
limitations. MAGNET [168] provides a comprehensive analysis of all sources of
filtering
inaccuracy of GateKeeper and SHD. Shouji
leverages this analysis to improve the
filtering by up to two orders of magnitude compared to both
accuracy of pre-alignment
GateKeeper and SHD, using a new algorithm and a new FPGA architecture.

[140]

SneakySnake [51] achieves up to four orders of magnitude higher filtering accuracy
compared to GateKeeper and SHD by mapping the pre-alignment filtering problem to the single
net routing (SNR) problem in VLSI chip layout. SNR finds the shortest routing path that
interconnects two terminals on the boundaries of a VLSI chip layout in the presence of
obstacles. SneakySnake is the only pre-alignment filter that efficiently works on CPUs, GPUs,
and FPGAs. To further reduce data movements, SneakySnake is redesigned to exploit the
near-memory computation capability on modern FPGA boards equipped with high-bandwidth
memory (HBM) [54]. Near-memory pre-alignment filtering improves performance and energy
efficiency by 27.4⨉ and 133⨉, respectively, over SneakySnake running on a 16-core (64
hardware threads)
IBM POWER9 CPU [54]. GenCache [169] proposes to perform
highly-parallel pre-alignment filtering inside the CPU cache to reduce data movement and
improve energy efficiency, with about 20% cache area overhead. GenCache shows that using
different existing pre-alignment filters together (a similar approach to [170]), each of which

22

operates only for a given edit distance threshold (e.g., using SHD only when is between 1 and
5), provides a 2.5⨉ speedup over GenCache with a single pre-alignment
filter. Several
pigeonhole principle based pre-alignment filters are evaluated for wide-range FPGA platforms
[171].

7.2.2. Base Counting
The base counting filter compares the numbers of bases (A, C, G, T) in the read with the
corresponding base counts in the reference segment. The sum of absolute differences of the
base counts provides an upper bound on the edit distance of the read and reference segment. If
one sequence has, for example, three more Ts than another sequence, then their alignment has
at most three edits. If half of the sum of absolute differences between the four base counts is
greater than E, then the two sequences are dissimilar and the reference segment is discarded.
The base counting filter is used in mrsFAST-Ultra [172] and GASSST [170]. Such a simple
filtering approach rejects a significant fraction of dissimilar sequences (e.g., 49.8%–80.4% of
sequences, as shown in GASSST [170]) and thus avoids a large fraction of expensive
verification computations required by sequence alignment algorithms. A PIM implementation of
base counting can improve filtering time by 100⨉ compared to its CPU implementation [173].

7.2.3. q-gram Filtering Approach
The q-gram filtering approach considers all of the sequence’s possible overlapping substrings of
length q (known as q-grams). Given a sequence of length m, there are m−q+1 overlapping
q-grams that are obtained by sliding a window of length q over the sequence. A single difference
in one of the sequences can affect at most q overlapping q-grams. Thus, differences can affect
no more than q⋅E q-grams, where E is the edit distance threshold. The minimum number of
shared q-grams between two similar sequences is therefore (m−q+1)−(q⋅E). This filtering
approach requires very simple operations (e.g., sums and comparisons), which makes it
attractive for hardware acceleration, such as in GRIM-Filter [52]. GRIM-Filter exploits the high
memory bandwidth and computation capability in the logic layer of 3D-stacked memory to
accelerate q-gram filtering in the DRAM chip itself, using a new representation of reference
genome that is friendly to in-memory processing. q-gram filtering is generally robust in handling
only a small number of edits, as the presence of edits in any q-gram is significantly
underestimated (e.g., counted as a single edit) [174]. The data reuse in GRIM-Filter can be
exploited for improving both performance and energy efficiency of filtering [175].

7.2.4. Sparse Dynamic Programming
Sparse DP algorithms exploit
the exact matches (seeds) shared between a read and a
reference segment to reduce execution time. These algorithms exclude the corresponding
locations of these seeds from estimating the number of edits between the two sequences, as
they were already detected as exact matches during indexing. Sparse DP filtering techniques
link the overlapping seeds together to build longer chains and use the total
length of the
calculated chains as a metric for filtering the sequence pairs. This approach is also known as
chaining, and is used in minimap2 [130] and rHAT [176]. GPU and FPGA accelerators [177] can
achieve 7⨉ and 28⨉ acceleration, respectively, compared to the sequential
implementation
(executed with 14 CPU threads) of the chaining algorithm used in minimap2. mm2-fast [162]

23

accelerates minimap2’s chaining step by up to 3.1⨉ with SIMD instructions. mm2-ax [178]
accelerates mm2-fast’s chaining step by up to 12.6⨉ using a GPU. Modular Aligner [179]
introduces an alternative to chaining based on two seed filtering techniques, achieving better
performance than minimap2 in terms of both accuracy and runtime.

7.3. Accelerating Sequence Alignment
After filtering out most of the mapping locations that lead to dissimilar sequence pairs, read
mapping calculates the sequence alignment information for every read and reference segment
extracted at each mapping location. Sequence alignment calculation is typically accelerated
using one of two approaches: (1) accelerating optimal affine gap scoring DP-based algorithms
using hardware accelerators, and (2) developing heuristics that sacrifice the optimality of the
alignment score solution in order to reduce alignment time. Affine gap scores are typically
calculated using the Smith-Waterman-Gotoh algorithm [180], allowing for linear integer scores
for matches/substitutions, and affine integer scores for gaps. Affine gap scores are more
general than linear or unit (edit distance) costs, but are more costly to compute by a constant
factor. Despite more than three decades of attempts to accelerate sequence alignment, the
fastest known edit distance algorithm [181] has a nearly quadratic running time, O(m2/log2m) for
a sequence of length m, which is proven to be a tight bound, assuming the strong exponential
time hypothesis holds [142]. A common approach to reducing the algorithmic work without
sacrificing optimality is to define an edit distance threshold, limiting the maximum number of
allowed single-character edits in the alignment. In this case, only a subset of the entries of the
DP table is computed, called diagonal vectors, as first proposed in Ukkonen’s banded algorithm
[182]. The number of diagonal vectors required for computing the DP matrix is 2E+1, where E is
the edit distance threshold. This reduces the runtime complexity to O(m*E). This approach is
effective for short reads, where the typical sequencing error rates are low, thus a low edit
distance threshold can be chosen. Unfortunately, as long reads have high sequencing error
rates (up to 20% of the read length), the edit distance threshold for long reads has to be high,
which results in calculating more entries in the DP matrix compared to that of short reads. The
use of heuristics (i.e., the second approach) helps to reduce the number of calculated entries in
the DP matrix and hence allows both the execution time and memory footprint to grow only
linearly or less with read length (as opposed to quadratically with classical DP). Next, we
describe the two approaches in detail.

7.3.1. Accurate Alignment Accelerators
From a hardware perspective, sequence alignment acceleration has five directions: (1) using
SIMD-capable CPUs, (2) using multicore CPUs and GPUs, (3) using FPGAs, (4) using ASICs,
and (5) using processing-in-memory architectures. Parasail [183] and KSW2 (used in minimap2
[130]) exploit both Ukkonen’s banded algorithm and SIMD-capable CPUs to compute banded
alignment for a sequence pair with a configurable scoring function. SIMD instructions offer
significant parallelism to the matrix computation by executing the same vector operation on
multiple operands at once. mm2-fast [162] accelerates KSW2 by up to 2.2⨉ by matching its
SIMD capability to recent CPU architectures. KSW2 is nearly as fast as Parasail when KSW2
does not use heuristics (explained in Section 7.3.2). The wavefront algorithm (WFA) [184]
reformulates the classic Smith-Waterman-Gotoh recursion such that the runtime is reduced to

24

O(m*s) for a sequence pair of length m and affine gap cost of s without fixing the vale of s
ahead of time. It is SIMD-friendly and shows significant speedups for sequence pairs that have
high similarity. The memory footprint and runtime complexity of WFA can be improved to O(s2)
[185]. However, this improved runtime is not practical due to a large constant factor. The
memory footprint of the WFA algorithm can be improved to O(s) at the expense of an increase in
runtime complexity to O(m*s) [186]. LEAP [187] formulates what can be considered a more
general version of WFA, which is applicable to any convex penalty scores.

The multicore architecture of CPUs and GPUs provides the ability to compute
alignments of many independent sequence pairs concurrently. GASAL2 [188] exploits the
multicore architecture of both CPUs and GPUs for highly-parallel computation of sequence
alignment with a user-defined scoring function. Unlike other GPU-accelerated tools, GASAL2
transfers the bases to the GPU, without encoding them into binary format, and hides the data
transfer time by overlapping GPU and CPU execution. GASAL2 is up to 20⨉ faster than Parasail
(when executed with 56 CPU threads). BWA-MEM2 [158] accelerates the banded sequence
alignment of its predecessor (BWA-MEM [189]) by up to 11.6⨉, by leveraging multicore and
SIMD parallelism. A GPU implementation [190] of the WFA algorithm improves the original CPU
implementation by 1.5-7.7⨉ using long reads.

Other designs, such as FPGASW [191], exploit the very large number of hardware
execution units in FPGAs to form a linear systolic array [192]. Each execution unit in the systolic
array is responsible for computing the value of a single entry of the DP matrix. The systolic array
computes a single vector of the matrix at a time. The data dependency between the entries
restricts the systolic array to computing the vectors sequentially (e.g., top-to-bottom, left-to-right,
in an anti-diagonal manner). FPGASW has a similar execution time as its GPU
or
implementation, but is 4⨉ more power efficient. SeedEx [193] designs an FPGA accelerator
similar to FPGASW, but improves hardware utilization by speculatively computing fewer than
2E+1 diagonal bands, and then applying optimality checking heuristics to guarantee correct
results. An FPGA accelerator [194] can accelerate the WFA algorithm by up to 8.8⨉ and
improve its energy efficiency by 9.7⨉ for only short reads.

Specialized hardware accelerators (i.e., ASIC designs) provide application-specific,
power- and area-efficient solutions to accelerate sequence alignment. For example, GenAx
[195] is composed of SillaX, a sequence alignment accelerator, and a second accelerator for
finding seeds. SillaX supports both a configurable scoring function and traceback operations.
SillaX is more efficient for short reads than for long reads, as it consists of an automata
processor whose performance scales quadratically with the edit distance. GenAx is 31.7⨉ faster
than the predecessor of BWA-MEM2 (i.e., BWA-MEM [189]) for short reads. Recent PIM
architectures such as RAPID [196] exploit the ability to perform computation inside or near the
memory chip to enable efficient sequence alignment. RAPID modifies the DP-based alignment
algorithm to make it friendly to in-memory parallel computation by calculating two DP matrices
(similar to Smith-Waterman-Gotoh [180]): one for calculating substitutions and exact matches
and another for calculating insertions and deletions. RAPID claims that this approach efficiently
enables higher levels of parallelism compared to traditional DP algorithms. The main two

25

benefits of RAPID and such PIM-based architectures are higher performance and higher energy
efficiency [33,34], as they alleviate the need to transfer data between the main memory and the
CPU cores through slow and energy hungry buses, while providing high degree of parallelism
with the help of PIM. RAPID is on average 11.8⨉ faster and 212.7⨉ more power efficient than
384-GPU cluster implementation of sequence alignment, known as CUDAlign [197]. A recent
PIM architecture of WFA algorithm implemented in real hardware provides up to 4.87⨉ higher
throughput than the 56-thread CPU implementation using short reads [198].

7.3.2. Alignment Accelerators with Limited Functionality
The second direction is to limit the functionality of the alignment algorithm or sacrifice the
optimality of the alignment solution in order to reduce execution time. The use of restrictive
functionality and heuristics limits the possible applications of the algorithms that utilize this
direction. Examples of limiting functionality include limiting the scoring function (e.g. allowing
only linear gap or unit scores), and calculating only the alignment score without performing the
backtracking step [199]. There are several existing algorithms and corresponding hardware
accelerators that limit scoring function flexibility. An example of limiting the scoring function is
Myers’ bit-vector algorithm [200], where the scoring function is limited to edit distance [165]. In
this case, all types of edits are penalized equally when calculating the total alignment score.
Restrictive scoring functions enable computation with smaller bit-widths, such that either smaller
registers can be used, or multiple DP entries fit into a single SIMD register. This reduces the
total execution time of the alignment algorithm by operating on multiple DP entries in parallel in
a SIMD fashion.In the case of Myer’s bit-vector algorithm a single 64-bit register can hold the
values of 64 entries of the DP matrix. BitPAl [201] expands on the idea by limiting the scoring
function to linear gap scores and achieves speedups through bit-parallel execution. ASAP [202]
accelerates edit distance calculation by up to 63.3⨉ using FPGAs compared to its CPU
implementation. The use of a fixed scoring function as in Edlib [131], which is the
state-of-the-art
implementation of Myers’ bit-vector algorithm, helps to outperform Parasail
(which uses a flexible scoring function) by 12–1000⨉. One downside of a limited scoring function
is that it may lead to the selection of a suboptimal sequence alignment, relative to an affine gap
scoring function as in the Smith-Waterman-Gotoh algorithm. There are also a large number of
edit distance approximation algorithms that provide a reduction in time complexity (e.g., m1.647
instead of m2), but they suffer from providing overestimated edit distance [203–206].

There are other algorithms and hardware architectures that provide low alignment time
by trading off accuracy. Darwin [36] builds a customized hardware architecture to speed up the
alignment process, by dividing the DP matrix into overlapping submatrices and greedily
processing each submatrix using systolic arrays. Darwin provides three orders of magnitude
speedup compared to Edlib [131]. Greedily processing each submatrix reduces the number of
calculated entries of the full DP matrix and hence reduces the memory footprint and algorithmic
workload, but it leads to suboptimal alignment calculation [170]. Darwin claims that choosing a
large submatrix size (≥ 320 × 320) and ensuring sufficient overlap (≥128 entries) between
adjacent submatrices may provide optimal alignment calculation for some datasets. GenASM
[35]
is a framework that uses bit-vector-based ASM and a similar strategy as Darwin to
accelerate multiple steps of the genome analysis pipeline, and is designed to be implemented

26

inside 3D-stacked memory. Through a combination of hardware–software co-design to unlock
parallelism, and processing-in-memory to reduce data movement, GenASM achieves 111⨉/116⨉
speedup over state-of-the-art software read mappers while reducing power consumption by
33⨉/37⨉.

There are other proposals that limit the number of calculated entries of the DP matrix
based on one of two approaches: (1) using sparse DP or (2) using a greedy approach to
maintain a high alignment score. Both approaches suffer from producing possibly suboptimal
alignments [207,208]. The first approach uses the same sparse DP algorithm used for
pre-alignment filtering but as an alignment step, as done in the exonerate tool [207]. This
includes applying DP-based alignment algorithms only between every two non-overlapping
chains to quickly estimate the total number of edits. The second approach is employed in
X-drop [208], which (1) avoids calculating entries (and their neighbors) whose alignment scores
are more than below the highest score seen so far (where is a user-specified parameter), and
(2) stops early when a high alignment score is not possible. The X-drop algorithm is guaranteed
to find the optimal alignment between relatively-similar sequences for only some scoring
functions [208]. A similar algorithm (known as Z-drop) makes KSW2 at least 2.6⨉ faster than
Parasail. A recent GPU implementation [209] of the X-drop algorithm is 3.1–120.4⨉ faster than
KSW2. A related approach is adaptive banding [210] (and its improved algorithm Block aligner
[211]), where the band of calculated diagonals is shifted up or down depending on the highest
score in the last calculated anti-diagonal.

8. Variant Calling

The goal of variant calling (Figure 1.5) is to find the differences between an individual or a
group of individuals (i.e., population) compared to a reference genome of a species. Calling the
variants is an essential step in genome analysis because the attributes of an individual or a
population (e.g., phenotypes or diseases) are determined from these variations. To determine
these variants, there are several steps that need to be performed as outlined by tools such as
GATK’s best practices [212] and DeepVariant’s workflow [213]. Variant calling usually iterates
over the read mapping information to identify the variants such as SNPs, short INDELs and
SVs. Although there are several algorithms to find SNPs and short INDELs, the main idea of
most of these tools is to find the locations in a genome where the reference genome and the
sequencing reads have different bases. To find such regions, several mapped reads should
consistently show the same edit at the same position of a reference genome to call the variant
with a high quality.

Calling the variants with high quality is essential for performing accurate downstream
analysis (e.g., validating a genetic disease) [214]. There are several parameters that contribute
to calling high quality variants such as high sequencing depth of coverage, highly accurate
sequencing reads (i.e., Illumina and PacBio HiFi), long reads, accurate and deterministic read
mappers. The sequencing depth of coverage refers to the average number of reads mapped to
each location in the reference genome. This helps variant callers to better distinguish the
genetic mutations from errors (both sequencing errors and read mapping artifacts [31]). It is also

27

known that variant calling tools may also be nondeterministic such that running the same tool
multiple times may result in different results [31]. Thus, it is essential for the community to
provide the best practices to achieve high accuracy due to many parameters involved in high
quality variant calling.

There are several efforts in the field to provide best practices when performing genomic
analysis that includes variant calling. One of the efforts is to provide benchmarking studies to
evaluate the accuracy of the variant calling output [215]. Such comparisons are usually done by
comparing the output from a variant caller with a ground truth dataset (e.g., GiAB [216]).
Another effort is to suggest the best pipeline of tools to achieve the best accuracy for variant
calling [214]. Another effort focuses on identifying the computational bottlenecks in the best
practices for variant calling and accelerates these bottlenecks to achieve high performance in
variant calling [217,218]. Compared to the number of proposed software and hardware
accelerators for read mapping, there are only a limited number of hardware accelerators for
variant calling. Given the large execution time (Table 4) of this key step in genome analysis,
there is still a huge need for accelerating state-of-the-art variant calling tools, such as
DeepVariant.

DeepVariant has 3 key steps: 1) processing mapping data (called making examples), 2)
variant classification, and 3) generating variant calls. The first step prepares read mapping
output data (e.g., read bases, base quality, edit information, strand information) for the neural
network. The output of read mapping (i.e., SAM file) has also some irrelevant information that
needs to be cleaned. This includes identifying and removing read duplicates that can be a result
of library preparation using PCR, as they do not lead to any useful information for variant calling.
The second step is when the deep neural network does the classification. The last step
interprets the classification output by the neural network as variant calls stored in VCF format.
Information about each step can be found in [219]. Based on Table 4, we make three key
observations. 1) The first step of DeepVariant consumes about 50% of the execution time of the
second step, variant classification. The first stage can be accelerated by 2⨉ using modern
FPGAs as in [134]. 2) Using nearly 10⨉ more bases of short read mapping data compared to
that of accurate long reads leads to nearly the same number of variant calls (PASS) and half of
the number of called variants (RefCALL). 3) Variant calling using ultra-long reads is
computationally very expensive, which can be mainly because of
the errors in the raw
sequencing data, as we discuss in Section 3.4.

28

Table 4: Evaluation analysis of variant calling, using DeepVariant tool, using read
mapping results of three different types of sequencing data, short reads, ultra-long
reads, and accurate long reads.

Short reads

Ultra-long reads

Accurate long reads

Variant calling tool

DeepVariant

PEPPER +
DeepVariant

DeepVariant

Version

1.3.0

0.8

1.3.0

Total number of bases in
input SAM file

Phase 1 (making
examples) CPU Time
(sec)

Phase 2 (calling
variants) CPU Time (sec)

Phase 3
(post-processing
variants) CPU Time (sec)

250,103,434,512

56,958,985,752

23,944,354,059

250,359

1,136,356

230,066

473,962

3,480,419

549,272

2,193

2,765

6,201

Total Run Time (sec)

746,514

4,644,980

4,619,540

6,054,168

785,539

4,589,024

1,313,292

6,722,265

2,603,968

7.9 variants/sec

2.77 variants/sec

9.2 variants/sec

Number of Called
Variants (PASS)

Number of Called
Variants (RefCall)

Variant Calling
Throughput (Number of
called variants / sec)

9. Discussion and Future Opportunities

Despite more than three decades of attempts, bridging the performance gap between
sequencing machines and computational analysis is still challenging. We summarize six main
challenges below.

First, we need to accelerate the entire genome analysis process rather than its individual
steps. Accelerating only a single step of genome analysis limits the overall achieved speedup
according to Amdahl’s Law. However, some of the computational steps included in genome
analysis pipeline are also included in other genomics pipelines. For example, improving read
mapping performance positively impacts almost all genomic analyses that use sequencing data
[13,27,35,36,53]. The contribution of read mapping to the entire analysis pipeline varies

29

depending on the application. For example, read mapping takes up to 1) 45% of the execution
time when discovering sequence variants in cancer genomics studies [220], and 2) 60% of the
execution time when profiling the taxonomy of a multi-species (i.e., metagenomic) sample [13].
Illumina and NVIDIA started following a more holistic approach, and they claim to accelerate
genome analysis by more than 48⨉, mainly by using specialization and hardware/software
co-design. Illumina has built an FPGA-based platform, called DRAGEN [221], that accelerates
all steps of genome analysis, including read mapping and variant calling. DRAGEN reduces the
overall analysis time from 32 CPU hours to only 37 minutes [222]. NVIDIA has built Parabricks,
a software suite accelerated using the company’s latest GPUs. Parabricks [223] can analyze
whole human genomes at 30⨉ coverage in about 45 minutes.

Second, we need to reduce the high amount of data movement that takes place during
genome analysis. Moving data (1) between compute units and main memory, (2) between
multiple hardware accelerators, and (3) between the sequencing machine and the computer
performing the analysis incurs high costs in terms of execution time and energy. These costs
are a significant barrier to enabling efficient analysis that can keep up with sequencing
technologies, and some recent works try to tackle this problem [33,34,52]. DRAGEN reduces
data movement between the sequencing machine and the computer performing analysis by
adding specialized hardware support inside the sequencing machine for data compression.
However,
this still requires movement of compressed data. GenStore [53] mitigates data
movement from the storage devices to the rest of the system (processors and main memory) by
processing more than 80% of the input read set inside the storage device.

Third, we need to build more specialized hardware accelerators that are mainly
developed for genomics. Computer programs are currently widely used for analyzing genomic
data, which limits their scaling capability and efficiency to handle population-level analyses. We
are witnessing a paradigm shift
to near-data computing with more specialized hardware
accelerators for other key applications such as self-driving cars [224], and artificial intelligence
with the largest chip ever [225]. This already fuelled huge interest in genomics especially from
large companies, such as NVIDIA, which introduces GPU H100 boards that are equipped with
HBM3 and hardware support for building and calculating DP matrix for sequence alignment.
UPMEM also shows significant benefits for using their PIM-capable memory devices for genome
analysis [226]. We envision that performing genome analysis inside the sequencing machine
itself using emerging technologies (e.g., PIM-enabled systems) can significantly improve
efficiency by eliminating sequencer-to-computer movement, and embedding a single specialized
chip for genome analysis within a portable sequencing device can potentially be a key enabler
for new applications of genome sequencing (e.g., rapid surveillance of diseases such as Ebola
[7] and COVID-19 [6,227], near-patient testing, bringing precision medicine to remote locations).
Unfortunately, efforts in this direction remain very limited.

Fourth, an emerging problem with using a single reference genome for an entire species
is the reference genome bias. The use of a single reference genome can bias the mapping
process and downstream analysis towards the DNA composition and variations present in the
reference organism due to population-specific genetic variations, individual’s genetic variations,

30

and sequencing errors [228,229]. An emerging technique to overcome reference bias is the use
of graph-based representations of a species’ genome, known as genome graphs [230]. A
genome graph represents the reference genome and known genetic variations in the population
as a graph-based data structure. Genome graphs are growing in popularity for genome
analysis, which requires modifying existing tools or introducing new tools and hardware
accelerators for supporting genome graphs instead of
linear representations of reference
genomes. Hardware acceleration is demonstrated to greatly benefit sequence mapping to
genome graphs. SeGraM is the first hardware acceleration framework for sequence-to-graph
mapping and alignment, where it provides an order of magnitude faster and more energy
efficient performance compared to software sequence-to-graph mapping tools [231]. A new
direction to alleviate the computation overhead of using different reference genomes is to
update the existing results of one step of the genome analysis pipeline for the new reference
genome without re-running the step’s algorithm again. The efforts in this direction are still limited
to only read mapping [61,62,232].

Fifth, we need to develop flexible hardware architectures that do not conservatively limit
the range of supported parameter values at design time. Commonly-used read mappers (e.g.,
minimap2) have different input parameters, each of which has a wide range of input values. For
example, the edit distance threshold is typically user defined and can be very high (15-20% of
the read length) for recent long reads. A configurable scoring function is another example, as it
determines the number of bits needed to store each entry of the DP matrix (e.g., DRAGEN
imposes a restriction on the maximum frequency of seed occurrence). Due to rapid changes in
sequencing technologies (e.g., high sequencing error rate and longer read lengths) [96,98],
these design restrictions can quickly make specialized hardware obsolete. Thus, read mappers
need to adapt their algorithms and their hardware architectures to be modular and scalable so
that they can be implemented for any sequence length and edit distance threshold based on the
sequencing technology.

Sixth, we need to adapt existing genomic data formats for hardware accelerators or
develop more efficient file formats. Most sequencing data is stored in the FASTQ/FASTA format,
where each base takes a single byte (8 bits) of memory. This encoding is inefficient, as only 2
bits (3 bits when the ambiguous base, N, is included) are needed to encode each DNA base.
The sequencing machine converts sequenced bases into FASTQ/FASTA format, and hardware
accelerators convert
the file contents into unique (for each accelerator) compact binary
representations for efficient processing. This process that requires multiple format conversions
wastes time. For example, only 43% of the sequence alignment time in BWA-MEM2 [158] is
spent on calculating the DP matrix, while 33% of the sequence alignment time is spent on
pre-processing the input sequences for loading into SIMD registers, as provided in [158]. To
address this inefficiency, we need to widely adopt efficient hardware-friendly formats, such as
UCSC’s 2bit format (https://genome.ucsc.edu/goldenPath/help/twoBit), to maximize the benefits
of hardware accelerators and reduce resource utilization. We are not aware of any recent read
mapper that uses such formats.

31

Looking into the late future, even if accurately sequencing the entire genome as a single
string might be possible, we believe that most of the tools and hardware accelerators involved in
the intelligent genome analysis pipeline will continue to remain a crucial component in analyzing
and comparing the sequencing data. For example, we still need to quickly and efficiently
compare complete genomes together for inferring variations and identifying metagenomic
taxonomy profiles. The acceleration efforts we highlight in this work represent state-of-the-art
efforts to reduce current bottlenecks in the genome analysis pipeline. We hope that these efforts
and the challenges we discuss provide a foundation for future work in making genome analysis
faster, more accurate, privacy-preserving, more energy-efficient, and cost-effective; simply more
intelligent.

Acknowledgments

We thank the SAFARI group members for feedback and the stimulating intellectual environment.
We acknowledge the generous gifts and support provided by our industrial partners: Google,
Huawei,
Intel, Microsoft, VMware, and the Semiconductor Research Corporation. M.A.
dedicates this paper to the memory of his father, who passed away on 9th March 2022.

References

[1] G.S. Ginsburg, K.A. Phillips, Precision Medicine: From Science To Value, Health Aff. . 37

(2018) 694–701.

[2] L. Farnaes, A. Hildreth, N.M. Sweeney, M.M. Clark, S. Chowdhury, S. Nahas, J.A. Cakici,
W. Benson, R.H. Kaplan, R. Kronick, M.N. Bainbridge, J. Friedman, J.J. Gold, Y. Ding, N.
Veeraraghavan, D. Dimmock, S.F. Kingsmore, Rapid whole-genome sequencing decreases
infant morbidity and cost of hospitalization, NPJ Genom Med. 3 (2018) 10.

[3] M.M. Clark, A. Hildreth, S. Batalov, Y. Ding, S. Chowdhury, K. Watkins, K. Ellsworth, B.
Camp, C.I. Kint, C. Yacoubian, L. Farnaes, M.N. Bainbridge, C. Beebe, J.J.A. Braun, M.
Bray, J. Carroll, J.A. Cakici, S.A. Caylor, C. Clarke, M.P. Creed, J. Friedman, A. Frith, R.
Gain, M. Gaughran, S. George, S. Gilmer, J. Gleeson, J. Gore, H. Grunenwald, R.L. Hovey,
M.L. Janes, K. Lin, P.D. McDonagh, K. McBride, P. Mulrooney, S. Nahas, D. Oh, A. Oriol, L.
Puckett, Z. Rady, M.G. Reese, J. Ryu, L. Salz, E. Sanford, L. Stewart, N. Sweeney, M.
Tokita, L. Van Der Kraan, S. White, K. Wigby, B. Williams, T. Wong, M.S. Wright, C.
Yamada, P. Schols, J. Reynders, K. Hall, D. Dimmock, N. Veeraraghavan, T. Defay, S.F.
Kingsmore, Diagnosis of genetic diseases in seriously ill children by rapid whole-genome
sequencing and automated phenotyping and interpretation, Science Translational Medicine.
11 (2019). https://doi.org/10.1126/scitranslmed.aat6177.

[4] N.M. Sweeney, S.A. Nahas, S. Chowdhury, S. Batalov, M. Clark, S. Caylor, J. Cakici, J.J.
Nigro, Y. Ding, N. Veeraraghavan, C. Hobbs, D. Dimmock, S.F. Kingsmore, Rapid whole
genome sequencing impacts care and resource utilization in infants with congenital heart
disease, NPJ Genom Med. 6 (2021) 29.

[5] G.S. Ginsburg, H.F. Willard, Genomic and personalized medicine:

foundations and

applications, Transl. Res. 154 (2009) 277–287.

[6] J.S. Bloom, L. Sathe, C. Munugala, E.M. Jones, M. Gasperini, N.B. Lubock, F. Yarza, E.M.
Thompson, K.M. Kovary, J. Park, D. Marquette, S. Kay, M. Lucas, T. Love, A. Sina
Booeshaghi, O.F. Brandenberg, L. Guo, J. Boocock, M. Hochman, S.W. Simpkins, I. Lin, N.
LaPierre, D. Hong, Y. Zhang, G. Oland, B.J. Choe, S. Chandrasekaran, E.E. Hilt, M.J.

32

Butte, R. Damoiseaux, C. Kravit, A.R. Cooper, Y. Yin, L. Pachter, O.B. Garner, J. Flint, E.
Eskin, C. Luo, S. Kosuri, L. Kruglyak, V.A. Arboleda, Massively scaled-up testing for
SARS-CoV-2 RNA via next-generation sequencing of pooled and barcoded nasal and
saliva samples, Nat Biomed Eng. 5 (2021) 657–665.

[7] J. Quick, N.J. Loman, S. Duraffour, J.T. Simpson, E. Severi, L. Cowley, J.A. Bore, R.
Koundouno, G. Dudas, A. Mikhail, N. Ouédraogo, B. Afrough, A. Bah, J.H. Baum, B.
Becker-Ziaja, J.-P. Boettcher, M. Cabeza-Cabrerizo, A. Camino-Sanchez, L.L. Carter, J.
Doerrbecker, T. Enkirch,
I.G.G. Dorival, N. Hetzelt, J. Hinzmann, T. Holm, L.E.
Kafetzopoulou, M. Koropogui, A. Kosgey, E. Kuisma, C.H. Logue, A. Mazzarelli, S. Meisel,
M. Mertens, J. Michel, D. Ngabo, K. Nitzsche, E. Pallash, L.V. Patrono, J. Portmann, J.G.
Repits, N.Y. Rickett, A. Sachse, K. Singethan, I. Vitoriano, R.L. Yemanaberhan, E.G.
Zekeng, R. Trina, A. Bello, A.A. Sall, O. Faye, O. Faye, N. ’faly Magassouba, C.V. Williams,
V. Amburgey, L. Winona, E. Davis, J. Gerlach, F. Washington, V. Monteil, M. Jourdain, M.
Bererd, A. Camara, H. Somlare, A. Camara, M. Gerard, G. Bado, B. Baillet, D. Delaune,
K.Y. Nebie, A. Diarra, Y. Savane, R.B. Pallawo, G.J. Gutierrez, N. Milhano, I. Roger, C.J.
Williams, F. Yattara, K. Lewandowski, J. Taylor, P. Rachwal, D. Turner, G. Pollakis, J.A.
Hiscox, D.A. Matthews, M.K. O’Shea, A.M. Johnston, D. Wilson, E. Hutley, E. Smit, A. Di
Caro, R. Woelfel, K. Stoecker, E. Fleischmann, M. Gabriel, S.A. Weller, L. Koivogui, B.
Diallo, S. Keita, A. Rambaut, P. Formenty, S. Gunther, M.W. Carroll, Real-time, portable
genome sequencing for Ebola surveillance, Nature. 530 (2016) 228–232.

[8] R. Yelagandula, A. Bykov, A. Vogt, R. Heinen, E. Özkan, M.M. Strobl, J.C. Baar, K.
Uzunova, B. Hajdusits, D. Kordic, E. Suljic, A. Kurtovic-Kozaric, S.
Izetbegovic, J.
Schaeffer, P. Hufnagl, A. Zoufaly, T. Seitz, VCDI, M. Födinger, F. Allerberger, A. Stark, L.
Cochella, U. Elling, Multiplexed detection of SARS-CoV-2 and other respiratory infections in
high throughput by SARSeq, Nat. Commun. 12 (2021) 3132.

[9] V.T.M. Le, B.A. Diep, Selected insights from application of whole-genome sequencing for

outbreak investigations, Curr. Opin. Crit. Care. 19 (2013) 432–439.

[10] V. Nikolayevskyy, K. Kranzer, S. Niemann, F. Drobniewski, Whole genome sequencing of
Mycobacterium tuberculosis for detection of recent transmission and tracing outbreaks: A
systematic review, Tuberculosis . 98 (2016) 77–85.

[11] D. Danko, D. Bezdan, E.E. Afshin, S. Ahsanuddin, C. Bhattacharya, D.J. Butler, K.R. Chng,
D. Donnellan, J. Hecht, K. Jackson, K. Kuchin, M. Karasikov, A. Lyons, L. Mak, D.
Meleshko, H. Mustafa, B. Mutai, R.Y. Neches, A. Ng, O. Nikolayeva, T. Nikolayeva, E. Png,
K.A. Ryon, J.L. Sanchez, H. Shaaban, M.A. Sierra, D. Thomas, B. Young, O.O. Abudayyeh,
J. Alicea, M. Bhattacharyya, R. Blekhman, E. Castro-Nallar, A.M. Cañas, A.D.
Chatziefthimiou, R.W. Crawford, F. De Filippis, Y. Deng, C. Desnues, E. Dias-Neto, M.
Dybwad, E. Elhaik, D. Ercolini, A. Frolova, D. Gankin, J.S. Gootenberg, A.B. Graf, D.C.
Green, I. Hajirasouliha, J.J.A. Hastings, M. Hernandez, G. Iraola, S. Jang, A. Kahles, F.J.
Kelly, K. Knights, N.C. Kyrpides, P.P. Łabaj, P.K.H. Lee, M.H.Y. Leung, P.O. Ljungdahl, G.
Mason-Buck, K. McGrath, C. Meydan, E.F. Mongodin, M.O. Moraes, N. Nagarajan, M.
Nieto-Caballero, H. Noushmehr, M. Oliveira, S. Ossowski, O.O. Osuolale, O. Özcan, D.
Paez-Espino, N. Rascovan, H. Richard, G. Rätsch, L.M. Schriml, T. Semmler, O.U.
Sezerman, L. Shi, T. Shi, R. Siam, L.H. Song, H. Suzuki, D.S. Court, S.W. Tighe, X. Tong,
K.I. Udekwu, J.A. Ugalde, B. Valentine, D.I. Vassilev, E.M. Vayndorf, T.P. Velavan, J. Wu,
M.M. Zambrano, J. Zhu, S. Zhu, C.E. Mason, International MetaSUB Consortium, A global
metagenomic map of urban microbiomes and antimicrobial resistance, Cell. 184 (2021)
3376–3393.e17.

[12] F. Meyer, A. Fritz, Z.L. Deng, D. Koslicki, A. Gurevich, Critical Assessment of Metagenome
(2021).

Interpretation-the
https://www.biorxiv.org/content/10.1101/2021.07.12.451567.abstract.

challenges,

BioRxiv.

second

round

of

[13] N. LaPierre, M. Alser, E. Eskin, D. Koslicki, S. Mangul, Metalign: efficient alignment-based

33

metagenomic profiling via containment min hash, Genome Biol. 21 (2020) 242.

[14] N. LaPierre, S. Mangul, M. Alser, I. Mandric, N.C. Wu, D. Koslicki, E. Eskin, MiCoP:
Microbial Community Profiling method for detecting viral and fungal organisms in
metagenomic samples, bioRxiv. (2018) 243188. https://doi.org/10.1101/243188.

[15] F. Meyer, A. Fritz, Z.-L. Deng, D. Koslicki, T.R. Lesker, A. Gurevich, G. Robertson, M. Alser,
D. Antipov, F. Beghini, D. Bertrand, J.J. Brito, C.T. Brown, J. Buchmann, A. Buluç, B. Chen,
R. Chikhi, P.T.L.C. Clausen, A. Cristian, P.W. Dabrowski, A.E. Darling, R. Egan, E. Eskin, E.
Georganas, E. Goltsman, M.A. Gray, L.H. Hansen, S. Hofmeyr, P. Huang, L. Irber, H. Jia,
T.S. Jørgensen, S.D. Kieser, T. Klemetsen, A. Kola, M. Kolmogorov, A. Korobeynikov, J.
Kwan, N. LaPierre, C. Lemaitre, C. Li, A. Limasset, F. Malcher-Miranda, S. Mangul, V.R.
Marcelino, C. Marchet, P. Marijon, D. Meleshko, D.R. Mende, A. Milanese, N. Nagarajan, J.
Nissen, S. Nurk, L. Oliker, L. Paoli, P. Peterlongo, V.C. Piro, J.S. Porter, S. Rasmussen,
E.R. Rees, K. Reinert, B. Renard, E.M. Robertsen, G.L. Rosen, H.-J. Ruscheweyh, V.
Sarwal, N. Segata, E. Seiler, L. Shi, F. Sun, S. Sunagawa, S.J. Sørensen, A. Thomas, C.
Tong, M. Trajkovski, J. Tremblay, G. Uritskiy, R. Vicedomini, Z. Wang, Z. Wang, Z. Wang, A.
Warren, N.P. Willassen, K. Yelick, R. You, G. Zeller, Z. Zhao, S. Zhu, J. Zhu, R.
Garrido-Oter, P. Gastmeier, S. Hacquard, S. Häußler, A. Khaledi, F. Maechler, F. Mesny, S.
Radutoiu, P. Schulze-Lefert, N. Smit, T. Strowig, A. Bremges, A. Sczyrba, A.C. McHardy,
Critical Assessment of Metagenome Interpretation: the second round of challenges, Nat.
Methods. (2022). https://doi.org/10.1038/s41592-022-01431-4.

[16] E.S. Lander, L.M. Linton, B. Birren, C. Nusbaum, M.C. Zody, J. Baldwin, K. Devon, K.
Dewar, M. Doyle, W. FitzHugh, R. Funke, D. Gage, K. Harris, A. Heaford, J. Howland, L.
Kann, J. Lehoczky, R. LeVine, P. McEwan, K. McKernan, J. Meldrim, J.P. Mesirov, C.
Miranda, W. Morris, J. Naylor, C. Raymond, M. Rosetti, R. Santos, A. Sheridan, C.
Sougnez, Y. Stange-Thomann, N. Stojanovic, A. Subramanian, D. Wyman, J. Rogers, J.
Sulston, R. Ainscough, S. Beck, D. Bentley, J. Burton, C. Clee, N. Carter, A. Coulson, R.
Deadman, P. Deloukas, A. Dunham, I. Dunham, R. Durbin, L. French, D. Grafham, S.
Gregory, T. Hubbard, S. Humphray, A. Hunt, M. Jones, C. Lloyd, A. McMurray, L. Matthews,
S. Mercer, S. Milne, J.C. Mullikin, A. Mungall, R. Plumb, M. Ross, R. Shownkeen, S. Sims,
R.H. Waterston, R.K. Wilson, L.W. Hillier, J.D. McPherson, M.A. Marra, E.R. Mardis, L.A.
Fulton, A.T. Chinwalla, K.H. Pepin, W.R. Gish, S.L. Chissoe, M.C. Wendl, K.D. Delehaunty,
T.L. Miner, A. Delehaunty, J.B. Kramer, L.L. Cook, R.S. Fulton, D.L. Johnson, P.J. Minx,
S.W. Clifton, T. Hawkins, E. Branscomb, P. Predki, P. Richardson, S. Wenning, T. Slezak, N.
Doggett, J.F. Cheng, A. Olsen, S. Lucas, C. Elkin, E. Uberbacher, M. Frazier, R.A. Gibbs,
D.M. Muzny, S.E. Scherer, J.B. Bouck, E.J. Sodergren, K.C. Worley, C.M. Rives, J.H.
Gorrell, M.L. Metzker, S.L. Naylor, R.S. Kucherlapati, D.L. Nelson, G.M. Weinstock, Y.
Sakaki, A. Fujiyama, M. Hattori, T. Yada, A. Toyoda, T. Itoh, C. Kawagoe, H. Watanabe, Y.
Totoki, T. Taylor, J. Weissenbach, R. Heilig, W. Saurin, F. Artiguenave, P. Brottier, T. Bruls,
E. Pelletier, C. Robert, P. Wincker, D.R. Smith, L. Doucette-Stamm, M. Rubenfield, K.
Weinstock, H.M. Lee, J. Dubois, A. Rosenthal, M. Platzer, G. Nyakatura, S. Taudien, A.
Rump, H. Yang, J. Yu, J. Wang, G. Huang, J. Gu, L. Hood, L. Rowen, A. Madan, S. Qin,
R.W. Davis, N.A. Federspiel, A.P. Abola, M.J. Proctor, R.M. Myers, J. Schmutz, M. Dickson,
J. Grimwood, D.R. Cox, M.V. Olson, R. Kaul, C. Raymond, N. Shimizu, K. Kawasaki, S.
Minoshima, G.A. Evans, M. Athanasiou, R. Schultz, B.A. Roe, F. Chen, H. Pan, J. Ramser,
H. Lehrach, R. Reinhardt, W.R. McCombie, M. de la Bastide, N. Dedhia, H. Blöcker, K.
Hornischer, G. Nordsiek, R. Agarwala, L. Aravind, J.A. Bailey, A. Bateman, S. Batzoglou, E.
Birney, P. Bork, D.G. Brown, C.B. Burge, L. Cerutti, H.C. Chen, D. Church, M. Clamp, R.R.
Copley, T. Doerks, S.R. Eddy, E.E. Eichler, T.S. Furey, J. Galagan, J.G. Gilbert, C. Harmon,
Y. Hayashizaki, D. Haussler, H. Hermjakob, K. Hokamp, W. Jang, L.S. Johnson, T.A. Jones,
S. Kasif, A. Kaspryzk, S. Kennedy, W.J. Kent, P. Kitts, E.V. Koonin, I. Korf, D. Kulp, D.
Lancet, T.M. Lowe, A. McLysaght, T. Mikkelsen, J.V. Moran, N. Mulder, V.J. Pollara, C.P.

34

Ponting, G. Schuler, J. Schultz, G. Slater, A.F. Smit, E. Stupka, J. Szustakowki, D.
Thierry-Mieg, J. Thierry-Mieg, L. Wagner, J. Wallis, R. Wheeler, A. Williams, Y.I. Wolf, K.H.
Wolfe, S.P. Yang, R.F. Yeh, F. Collins, M.S. Guyer, J. Peterson, A. Felsenfeld, K.A.
Wetterstrand, A. Patrinos, M.J. Morgan, P. de Jong, J.J. Catanese, K. Osoegawa, H.
Shizuya, S. Choi, Y.J. Chen, J. Szustakowki, International Human Genome Sequencing
Consortium, Initial sequencing and analysis of the human genome, Nature. 409 (2001)
860–921.

[17] J.A. Reuter, D.V. Spacek, M.P. Snyder, High-throughput sequencing technologies, Mol. Cell.

58 (2015) 586–597.

[18] M. Alser, J. Rotman, D. Deshpande, K. Taraszka, H. Shi, P.I. Baykal, H.T. Yang, V. Xue, S.
Knyazev, B.D. Singer, B. Balliu, D. Koslicki, P. Skums, A. Zelikovsky, C. Alkan, O. Mutlu, S.
Mangul, Technology dictates algorithms: recent developments in read alignment, Genome
Biol. 22 (2021) 249.

[19] S. Mangul, L.S. Martin, B.L. Hill, A.K.-M. Lam, M.G. Distler, A. Zelikovsky, E. Eskin, J. Flint,
Systematic benchmarking of omics computational tools, Nat. Commun. 10 (2019) 1393.
[20] B.B. Misra, C.D. Langefeld, M. Olivier, L.A. Cox, Integrated Omics: Tools, Advances, and

Future Approaches, J. Mol. Endocrinol. (2018). https://doi.org/10.1530/JME-18-0055.

[21] F. Markowetz, All biology is computational biology, PLoS Biol. 15 (2017) e2002050.
[22] F. Sanger, The free amino groups of insulin, 1945.
[23] J. Shendure, S. Balasubramanian, G.M. Church, W. Gilbert, J. Rogers, J.A. Schloss, R.H.
Waterston, DNA sequencing at 40: past, present and future, Nature. 550 (2017) 345–353.
[24] R. Nielsen, J.S. Paul, A. Albrechtsen, Y.S. Song, Genotype and SNP calling from

next-generation sequencing data, Nat. Rev. Genet. 12 (2011) 443–451.

[25] S.S. Ho, A.E. Urban, R.E. Mills, Structural variation in the sequencing era, Nat. Rev. Genet.

21 (2020) 171–189.

[26] S. Jacquemont, A. Reymond, F. Zufferey, L. Harewood, R.G. Walters, Z. Kutalik, D.
Martinet, Y. Shen, A. Valsesia, N.D. Beckmann, G. Thorleifsson, M. Belfiore, S. Bouquillon,
D. Campion, N. de Leeuw, B.B.A. de Vries, T. Esko, B.A. Fernandez, F. Fernández-Aranda,
J.M. Fernández-Real, M. Gratacòs, A. Guilmatre, J. Hoyer, M.-R. Jarvelin, R.F. Kooy, A.
Kurg, C. Le Caignec, K. Männik, O.S. Platt, D. Sanlaville, M.M. Van Haelst, S. Villatoro
Gomez, F. Walha, B.-L. Wu, Y. Yu, A. Aboura, M.-C. Addor, Y. Alembik, S.E. Antonarakis, B.
Arveiler, M. Barth, N. Bednarek, F. Béna, S. Bergmann, M. Beri, L. Bernardini, B.
Blaumeiser, D. Bonneau, A. Bottani, O. Boute, H.G. Brunner, D. Cailley, P. Callier, J.
Chiesa, J. Chrast, L. Coin, C. Coutton, J.-M. Cuisset, J.-C. Cuvellier, A. David, B. de
Freminville, B. Delobel, M.-A. Delrue, B. Demeer, D. Descamps, G. Didelot, K. Dieterich, V.
Disciglio, M. Doco-Fenzy, S. Drunat, B. Duban-Bedu, C. Dubourg, J.S. El-Sayed Moustafa,
P. Elliott, B.H.W. Faas, L. Faivre, A. Faudet, F. Fellmann, A. Ferrarini, R. Fisher, E. Flori, L.
Forer, D. Gaillard, M. Gerard, C. Gieger, S. Gimelli, G. Gimelli, H.J. Grabe, A. Guichet, O.
Guillin, A.-L. Hartikainen, D. Heron, L. Hippolyte, M. Holder, G. Homuth, B. Isidor, S.
Jaillard, Z. Jaros, S. Jiménez-Murcia, G.J. Helas, P. Jonveaux, S. Kaksonen, B. Keren, A.
Kloss-Brandstätter, N.V.A.M. Knoers, D.A. Koolen, P.M. Kroisel, F. Kronenberg, A. Labalme,
E. Landais, E. Lapi, V. Layet, S. Legallic, B. Leheup, B. Leube, S. Lewis, J. Lucas, K.D.
MacDermot, P. Magnusson, C. Marshall, M. Mathieu-Dramard, M.I. McCarthy, T. Meitinger,
M.A. Mencarelli, G. Merla, A. Moerman, V. Mooser, F. Morice-Picard, M. Mucciolo, M.
Nauck, N.C. Ndiaye, A. Nordgren, L. Pasquier, F. Petit, R. Pfundt, G. Plessis, E.
Rajcan-Separovic, G.P. Ramelli, A. Rauch, R. Ravazzolo, A. Reis, A. Renieri, C. Richart,
J.S. Ried, C. Rieubland, W. Roberts, K.M. Roetzer, C. Rooryck, M. Rossi, E. Saemundsen,
V. Satre, C. Schurmann, E. Sigurdsson, D.J. Stavropoulos, H. Stefansson, C. Tengström, U.
Thorsteinsdóttir, F.J. Tinahones, R. Touraine, L. Vallée, E. van Binsbergen, N. Van der Aa,
C. Vincent-Delorme, S. Visvikis-Siest, P. Vollenweider, H. Völzke, A.T. Vulto-van Silfhout, G.
Waeber, C. Wallgren-Pettersson, R.M. Witwicki, S. Zwolinksi, J. Andrieux, X. Estivill, J.F.

35

Gusella, O. Gustafsson, A. Metspalu, S.W. Scherer, K. Stefansson, A.I.F. Blakemore, J.S.
Beckmann, P. Froguel, Mirror extreme BMI phenotypes associated with gene dosage at the
chromosome 16p11.2 locus, Nature. 478 (2011) 97–102.

[27] M. Alser, Z. Bingol, D.S. Cali, J. Kim, S. Ghose, C. Alkan, O. Mutlu, Accelerating Genome
IEEE Micro. 40 (2020) 65–75.

Analysis: A Primer on an Ongoing Journey,
https://doi.org/10.1109/mm.2020.3013728.

[28] J.M. Friedman, Y. Bombard, M.C. Cornel, C.V. Fernandez, A.K. Junker, S.E. Plon, Z. Stark,
B.M. Knoppers, Paediatric Task Team of the Global Alliance for Genomics and Health
infants:
Regulatory and Ethics Work Stream, Genome-wide sequencing in acutely ill
genomic medicine’s critical application?, Genet. Med. 21 (2019) 498–504.

[29] C.R. Marshall, S. Chowdhury, R.J. Taft, M.S. Lebo, J.G. Buchan, S.M. Harrison, R. Rowsey,
E.W. Klee, P. Liu, E.A. Worthey, V. Jobanputra, D. Dimmock, H.M. Kearney, D. Bick, S.
Kulkarni, S.L. Taylor, J.W. Belmont, D.J. Stavropoulos, N.J. Lennon, Medical Genome
Initiative, Best practices for the analytical validation of clinical whole-genome sequencing
intended for the diagnosis of germline disease, NPJ Genom Med. 5 (2020) 47.

[30] G. Baruzzo, K.E. Hayer, E.J. Kim, B. Di Camillo, G.A. FitzGerald, G.R. Grant,
Simulation-based comprehensive benchmarking of RNA-seq aligners, Nat. Methods. 14
(2017) 135–139.

[31] C. Firtina, C. Alkan, On genomic repeats and reproducibility, Bioinformatics. 32 (2016)

2243–2247.

[32] Z.D. Stephens, S.Y. Lee, F. Faghri, R.H. Campbell, C. Zhai, M.J. Efron, R. Iyer, M.C.
Schatz, S. Sinha, G.E. Robinson, Big Data: Astronomical or Genomical?, PLoS Biol. 13
(2015) e1002195.

[33] O. Mutlu, S. Ghose, J. Gómez-Luna, R. Ausavarungnirun, Processing data where it makes

sense: Enabling in-memory computation, Microprocess. Microsyst. 67 (2019) 28–41.

[34] S. Ghose, A. Boroumand, J.S. Kim, J. Gómez-Luna, O. Mutlu, Processing-in-memory: A

workload-driven perspective, IBM J. Res. Dev. 63 (2019) 3:1–3:19.

[35] D.S. Cali, G.S. Kalsi, Z. Bingöl, C. Firtina, L. Subramanian, J.S. Kim, R. Ausavarungnirun,
M. Alser, J. Gomez-Luna, A. Boroumand, A. Norion, A. Scibisz, S. Subramoneyon, C.
Alkan, S. Ghose, O. Mutlu, GenASM: A High-Performance, Low-Power Approximate String
Matching Acceleration Framework for Genome Sequence Analysis, in: 2020 53rd Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO), 2020: pp. 951–966.
[36] Y. Turakhia, G. Bejerano, W.J. Dally, Darwin, Proceedings of the Twenty-Third International
Conference on Architectural Support for Programming Languages and Operating Systems.
(2018). https://doi.org/10.1145/3173162.3173193.

[37] O. Mutlu, S. Ghose, J. Gómez-Luna, R. Ausavarungnirun, A Modern Primer on Processing

in Memory, arXiv [cs.AR]. (2020). http://arxiv.org/abs/2012.03112.

[38] A. Boroumand, S. Ghose, Y. Kim, R. Ausavarungnirun, E. Shiu, R. Thakur, D. Kim, A.
Kuusela, A. Knies, P. Ranganathan, O. Mutlu, Google Workloads for Consumer Devices:
Mitigating Data Movement Bottlenecks, in: Proceedings of the Twenty-Third International
Conference on Architectural Support for Programming Languages and Operating Systems,
Association for Computing Machinery, New York, NY, USA, 2018: pp. 316–331.

[39] A. Boroumand, S. Ghose, B. Akin, R. Narayanaswami, G.F. Oliveira, X. Ma, E. Shiu, O.
Mutlu, Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine
in: 2021 30th International Conference on Parallel
Learning Inference Bottlenecks,
Architectures and Compilation Techniques (PACT), 2021: pp. 159–172.

[40] M. Horowitz, 1.1 computing’s energy problem (and what we can do about it), in: 2014 IEEE
International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), IEEE,
2014: pp. 10–14.

[41] G.F. Oliveira, J. Gómez-Luna, L. Orosa, S. Ghose, N. Vijaykumar,

I. Fernandez, M.
Sadrosadati, O. Mutlu, DAMOV: A New Methodology and Benchmark Suite for Evaluating

36

Data Movement Bottlenecks, IEEE Access. 9 (undefined 2021) 134457–134502.

[42] O. Mutlu, L. Subramanian, Research problems and opportunities in memory systems,

Supercomputing Frontiers and Innovations. 1 (2014) 19–55.

[43] O. Mutlu, Memory scaling: A systems architecture perspective,

in: 2013 5th IEEE

International Memory Workshop, 2013: pp. 21–25.

[44] B. Langmead, A. Nellore, Cloud computing for genomic data analysis and collaboration,

Nat. Rev. Genet. 19 (2018) 325.

[45] N. Almadhoun, E. Ayday, Ö. Ulusoy, Differential privacy under dependent tuples—the case

of genomic privacy, Bioinformatics. 36 (2019) 1696–1703.

[46] N. Almadhoun, E. Ayday, Ö. Ulusoy, Inference attacks against differentially private query
tuples, Bioinformatics. 36 (2020)

results from genomic datasets including dependent
i136–i145.

[47] M. Alser, N. Almadhoun, A. Nouri, C. Alkan, E. Ayday, Can you really anonymize the donors
of genomic data in today’s digital world?, in: Lecture Notes in Computer Science, Springer
International Publishing, Cham, 2016: pp. 237–244.

[48] N.A. Alserr, O. Ulusoy, E. Ayday, O. Mutlu, GenShare: Sharing Accurate
Differentially-Private Statistics for Genomic Datasets with Dependent Tuples, arXiv
[q-bio.GN]. (2021). http://arxiv.org/abs/2112.15109.

[49] N.A. Alserr, G. Kale, O. Mutlu, O. Tastan, E. Ayday, Near-Optimal Privacy-Utility Tradeoff in
(2021).

Selective

[cs.CR].

Hiding,

Using

arXiv

SNP

Genomic
Studies
http://arxiv.org/abs/2106.05211.

[50] N.M. Ghiasi, J. Park, H. Mustafa, J. Kim, A. Olgun, A. Gollwitzer, D.S. Cali, C. Firtina, H.
Mao, N.A. Alserr, R. Ausavarungnirun, N. Vijaykumar, M. Alser, O. Mutlu, GenStore: A
High-Performance and Energy-Efficient
In-Storage Computing System for Genome
Sequence Analysis, arXiv [cs.AR]. (2022). http://arxiv.org/abs/2202.10400.

[51] M. Alser, T. Shahroodi, J. Gómez-Luna, C. Alkan, O. Mutlu, SneakySnake: A Fast and
for CPUs, GPUs, and FPGAs,

Accurate Universal Genome Pre-Alignment Filter
Bioinformatics. (2020). https://doi.org/10.1093/bioinformatics/btaa1015.

[52] J.S. Kim, D. Senol Cali, H. Xin, D. Lee, S. Ghose, M. Alser, H. Hassan, O. Ergin, C. Alkan,
O. Mutlu, GRIM-Filter: Fast seed location filtering in DNA read mapping using
processing-in-memory technologies, BMC Genomics. 19 (2018) 89.

[53] N. Mansouri Ghiasi, J. Park, H. Mustafa, J. Kim, A. Olgun, A. Gollwitzer, D. Senol Cali, C.
Firtina, H. Mao, N. Almadhoun Alserr, R. Ausavarungnirun, N. Vijaykumar, M. Alser, O.
Mutlu, GenStore: a high-performance in-storage processing system for genome sequence
analysis,
the 27th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems, Association for Computing
Machinery, New York, NY, USA, 2022: pp. 635–654.

in: Proceedings of

[54] G. Singh, M. Alser, D.S. Cali, D. Diamantopoulos, J. Gómez-Luna, H. Corporaal, O. Mutlu,
FPGA-Based Near-Memory Acceleration of Modern Data-Intensive Applications,
IEEE
Micro. 41 (2021) 39–48.

[55] O. Mutlu,

Intelligent Architectures for

Intelligent Machines,

in: 2020 International

Symposium on VLSI Design, Automation and Test (VLSI-DAT), 2020: pp. 1–4.

[56] F. Sanger, S. Nicklen, A.R. Coulson, DNA sequencing with chain-terminating inhibitors,

Proc. Natl. Acad. Sci. U. S. A. 74 (1977) 5463–5467.

[57] The

Nobel

in
https://www.nobelprize.org/prizes/chemistry/1958/sanger/lecture/
2022).

Chemistry

1958,

Prize

NobelPrize.org.

(accessed March

(n.d.).
2,

[58] A.M. Maxam, W. Gilbert, A new method for sequencing DNA, Proc. Natl. Acad. Sci. U. S. A.

74 (1977) 560–564.

[59] White

House

press

release,

https://web.ornl.gov/sci/techresources/Human_Genome/project/clinton1.shtml

(n.d.).
(accessed

37

March 2, 2022).

[60] Introduction to Patches, (n.d.). https://www.ncbi.nlm.nih.gov/grc/help/patches/ (accessed

March 2, 2022).

[61] J.S. Kim, C. Firtina, D.S. Cali, M. Alser, N. Hajinazar, C. Alkan, O. Mutlu, AirLift: A Fast and
Comprehensive Technique for Translating Alignments between Reference Genomes, arXiv
Preprint
(2019).
https://www.researchgate.net/profile/Damla-Senol-Cali/publication/338036201_AirLift_A_Fa
st_and_Comprehensive_Technique_for_Translating_Alignments_between_Reference_Gen
omes/links/5f7382e692851c14bc9ff96e/AirLift-A-Fast-and-Comprehensive-Technique-for-Tr
anslating-Alignments-between-Reference-Genomes.pdf.

arXiv:1912.

08735.

[62] T. Mun, N.-C. Chen, B. Langmead, LevioSAM: Fast lift-over of variant-aware reference

alignments, Bioinformatics. (2021). https://doi.org/10.1093/bioinformatics/btab396.

[63] S. Nurk, S. Koren, A. Rhie, M. Rautiainen, A.V. Bzikadze, The complete sequence of a
(2021).

human
https://www.biorxiv.org/content/10.1101/2021.05.26.445798v1.abstract.

genome,

bioRxiv.

[64] F. Syed, H. Grunenwald, N. Caruccio, Next-generation sequencing library preparation:
simultaneous fragmentation and tagging using in vitro transposition, Nature Methods. 6
(2009) i–ii. https://doi.org/10.1038/nmeth.f.272.

[65] E.L. van Dijk, Y. Jaszczyszyn, C. Thermes, Library preparation methods for next-generation

sequencing: tone down the bias, Exp. Cell Res. 322 (2014) 12–20.

[66] D.R. Kelley, M.C. Schatz, S.L. Salzberg, Quake: quality-aware detection and correction of

sequencing errors, Genome Biol. 11 (2010) R116.

[67] H.A. Erlich, D. Gelfand, J.J. Sninsky, Recent advances in the polymerase chain reaction,

Science. 252 (1991) 1643–1651.

[68] M. Alser, S. Waymost, R. Ayyala, B. Lawlor, R.J. Abdill, N. Rajkumar, N. LaPierre, J. Brito,
A.M. Ribeiro-dos-Santos, C. Firtina, N. Almadhoun, V. Sarwal, E. Eskin, Q. Hu, D. Strong,
Byoung-Do, Kim, M.S. Abedalthagafi, O. Mutlu, S. Mangul, Packaging, containerization,
and virtualization of
challenges, and
computational omics methods: Advances,
opportunities, arXiv [q-bio.GN]. (2022). http://arxiv.org/abs/2203.16261.

[69] Home - SRA - NCBI, (n.d.). https://www.ncbi.nlm.nih.gov/sra (accessed March 27, 2022).
[70] Overview

NCBI/NLM/NIH,

Sequence

: Main

archive

read

:

:

(n.d.).

https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi (accessed March 3, 2022).

[71] EMBL-EBI, ENA browser, (n.d.). https://www.ebi.ac.uk/ena (accessed March 27, 2022).
[72] RefSeq: NCBI Reference Sequence Database, (n.d.). https://www.ncbi.nlm.nih.gov/refseq

(accessed March 27, 2022).

[73] D.J. Nasko, S. Koren, A.M. Phillippy, T.J. Treangen, RefSeq database growth influences the
accuracy of k-mer-based lowest common ancestor species identification, Genome Biol. 19
(2018) 165.

[74] M. Escalona, S. Rocha, D. Posada, A comparison of tools for the simulation of genomic

next-generation sequencing data, Nat. Rev. Genet. 17 (2016) 459–469.

[75] Y. Ono, K. Asai, M. Hamada, PBSIM2: a simulator for long-read sequencers with a novel

generative model of quality scores, Bioinformatics. 37 (2021) 589–595.

[76] C. Yang, J. Chu, R.L. Warren, I. Birol, NanoSim: nanopore sequence read simulator based
(2017).

characterization,

GigaScience.

statistical

6

on
https://doi.org/10.1093/gigascience/gix010.

[77] M. Holtgrewe, Mason: a read simulator for second generation sequencing data, (2010).

https://refubium.fu-berlin.de/handle/fub188/18686.

[78] D.M. Portik, C. Titus Brown, N. Tessa Pierce-Ward, Evaluation of taxonomic profiling
(2022)

long-read shotgun metagenomic sequencing datasets, bioRxiv.

methods for
2022.01.31.478527. https://doi.org/10.1101/2022.01.31.478527.

[79] W. Huang, L. Li, J.R. Myers, G.T. Marth, ART: a next-generation sequencing read simulator,

38

Bioinformatics. 28 (2012) 593–594.

[80] S. Schmeing, M.D. Robinson, ReSeq simulates realistic Illumina high-throughput

sequencing data, Genome Biol. 22 (2021) 67.

[81] Y. Ono, K. Asai, M. Hamada, PBSIM: PacBio reads simulator—toward accurate genome

assembly, Bioinformatics. 29 (2012) 119–121.

[82] S.L. Castro-Wallace, C.Y. Chiu, K.K. John, S.E. Stahl, K.H. Rubins, A.B.R. McIntyre, J.P.
Dworkin, M.L. Lupisella, D.J. Smith, D.J. Botkin, T.A. Stephenson, S. Juul, D.J. Turner, F.
Izquierdo, S. Federman, D. Stryke, S. Somasekar, N. Alexander, G. Yu, C.E. Mason, A.S.
Burton, Nanopore DNA Sequencing and Genome Assembly on the International Space
Station, (n.d.). https://doi.org/10.1101/077651.

[83] E.L. van Dijk, H. Auger, Y. Jaszczyszyn, C. Thermes, Ten years of next-generation

sequencing technology, Trends Genet. 30 (2014) 418–426.

[84] M.A. Quail, I. Kozarewa, F. Smith, A. Scally, P.J. Stephens, R. Durbin, H. Swerdlow, D.J.
Turner, A large genome center’s improvements to the Illumina sequencing system, Nat.
Methods. 5 (2008) 1005–1010.

[85] Singular Genomics, Singular Genomics. (2020). https://singulargenomics.com (accessed

March 4, 2022).

[86] T.C. Glenn, Field guide to next-generation DNA sequencers, Mol. Ecol. Resour. 11 (2011)

759–769.

[87] NGS

(n.d.).
https://emea.illumina.com/science/technology/next-generation-sequencing/ngs-vs-sanger-s
equencing.html (accessed March 4, 2022).

sequencing,

Sanger

vs.

[88] E.R. Mardis, DNA sequencing technologies: 2006–2016, Nat. Protoc. 12 (2017) 213–218.
[89] J. Medžiūnė, Ž. Kapustina, S. Žeimytė, J. Jakubovska, R. Sindikevičienė, I. Čikotienė, A.
Lubys, Advanced preparation of fragment libraries enabled by oligonucleotide-modified
2′,3′-dideoxynucleotides, Communications Chemistry. 5 (2022) 1–8.

[90] 2-channel

(n.d.).
https://emea.illumina.com/science/technology/next-generation-sequencing/sequencing-tech
nology/2-channel-sbs.html (accessed March 5, 2022).

technology,

SBS

[91] Run time estimates for each sequencing step on Illumina sequencing platforms, (n.d.).
https://emea.support.illumina.com/bulletins/2017/02/run-time-estimates-for-each-sequencin
g-step-on-illumina-sequenci.html (accessed March 6, 2022).

[92] Company

Oxford
https://nanoporetech.com/about-us/history (accessed March 7, 2022).

Nanopore

history,

Technologies.

(2021).

[93] Y. Wang, Y. Zhao, A. Bollas, Y. Wang, K.F. Au, Nanopore sequencing technology,

bioinformatics and applications, Nat. Biotechnol. 39 (2021) 1348–1365.

[94] Y.-T. Huang, P.-Y. Liu, P.-W. Shih, Homopolish: a method for the removal of systematic
errors in nanopore sequencing by homologous polishing, Genome Biol. 22 (2021) 95.
[95] S.L. Amarasinghe, S. Su, X. Dong, L. Zappia, M.E. Ritchie, Q. Gouil, Opportunities and

challenges in long-read sequencing data analysis, Genome Biol. 21 (2020) 30.

[96] C. Firtina, J.S. Kim, M. Alser, D.S. Cali, A. Ercument Cicek, C. Alkan, O. Mutlu, Apollo: a
sequencing-technology-independent, scalable and accurate assembly polishing algorithm,
Bioinformatics. 36 (2020) 3669–3679. https://doi.org/10.1093/bioinformatics/btaa179.

[97] G.A. Logsdon, M.R. Vollger, E.E. Eichler, Long-read human genome sequencing and its

applications, Nat. Rev. Genet. 21 (2020) 597–614.

[98] D. Senol Cali, J.S. Kim, S. Ghose, C. Alkan, O. Mutlu, Nanopore sequencing technology
and tools for genome assembly: computational analysis of the current state, bottlenecks
and future directions, Brief. Bioinform. 20 (2019) 1542–1559.

[99] Y. Suzuki, Advent of a new sequencing era: long-read and on-site sequencing, J. Hum.

Genet. 65 (2020) 1.

[100] T. Hon, K. Mars, G. Young, Y.-C. Tsai, J.W. Karalius, J.M. Landolin, N. Maurer, D.

39

Kudrna, M.A. Hardigan, C.C. Steiner, S.J. Knapp, D. Ware, B. Shapiro, P. Peluso, D.R.
Rank, Highly accurate long-read HiFi sequencing data for five complex genomes, Sci Data.
7 (2020) 399.

[101] A.M. Wenger, P. Peluso, W.J. Rowell, P.-C. Chang, R.J. Hall, G.T. Concepcion, J. Ebler,
A. Fungtammasan, A. Kolesnikov, N.D. Olson, A. Töpfer, M. Alonge, M. Mahmoud, Y. Qian,
C.-S. Chin, A.M. Phillippy, M.C. Schatz, G. Myers, M.A. DePristo, J. Ruan, T. Marschall, F.J.
Sedlazeck, J.M. Zook, H. Li, S. Koren, A. Carroll, D.R. Rank, M.W. Hunkapiller, Accurate
circular consensus long-read sequencing improves variant detection and assembly of a
human genome, Nat. Biotechnol. 37 (2019) 1155–1162.

[102] M.J.P. Chaisson, R.K. Wilson, E.E. Eichler, Genetic variation and the de novo assembly

of human genomes, Nat. Rev. Genet. 16 (2015) 627–640.

[103] M. Jain, S. Koren, K.H. Miga, J. Quick, A.C. Rand, T.A. Sasani, J.R. Tyson, A.D. Beggs,
I.T. Fiddes, S. Malla, H. Marriott, T. Nieto, J. O’Grady, H.E. Olsen, B.S.
A.T. Dilthey,
Pedersen, A. Rhie, H. Richardson, A.R. Quinlan, T.P. Snutch, L. Tee, B. Paten, A.M.
Phillippy, J.T. Simpson, N.J. Loman, M. Loose, Nanopore sequencing and assembly of a
human genome with ultra-long reads, Nat. Biotechnol. 36 (2018) 338–345.

[104] L. Gong, C.-H. Wong, J. Idol, C.Y. Ngan, C.-L. Wei, Ultra-long Read Sequencing for

Whole Genomic DNA Analysis, J. Vis. Exp. (2019). https://doi.org/ 10.3791/58954.

[105] S. Deschamps, Y. Zhang, V. Llaca, L. Ye, A. Sanyal, M. King, G. May, H. Lin, A
chromosome-scale assembly of the sorghum genome using nanopore sequencing and
optical mapping, Nat. Commun. 9 (2018) 4844.

[106] S. Koren, B.P. Walenz, K. Berlin, J.R. Miller, N.H. Bergman, A.M. Phillippy, Canu:
scalable and accurate long-read assembly via adaptive k-mer weighting and repeat
separation, Genome Res. 27 (2017) 722–736.

[107] Y. Chen, F. Nie, S.-Q. Xie, Y.-F. Zheng, Q. Dai, T. Bray, Y.-X. Wang, J.-F. Xing, Z.-J.
Huang, D.-P. Wang, L.-J. He, F. Luo, J.-X. Wang, Y.-Z. Liu, C.-L. Xiao, Efficient assembly of
nanopore reads via highly accurate and intact error correction, Nature Communications. 12
(2021). https://doi.org/10.1038/s41467-020-20236-7.

[108]

J.L. Gehrig, D.M. Portik, M.D. Driscoll, E. Jackson, S. Chakraborty, D. Gratalo, M. Ashby,
R. Valladares, Finding the right fit: evaluation of short-read and long-read sequencing
approaches to maximize the utility of clinical microbiome data, Microb Genom. 8 (2022).
https://doi.org/10.1099/mgen.0.000794.

[109] High performance long read assay enables contiguous data up to 10Kb on existing
illumina
(n.d.).
platforms,
https://www.illumina.com/science/genomics-research/articles/infinity-high-performance-long
-read-assay.html (accessed April 5, 2022).
Benton, Guppy GPU

benchmarking

basecalling),

(nanopore

[110] M.

https://esr-nz.github.io/gpu_basecalling_testing/gpu_benchmarking.html
3, 2022).

[111] A. Cacho, E. Smirnova, S. Huzurbazar, X. Cui, A Comparison of Base-calling Algorithms

for Illumina Sequencing Technology, Brief. Bioinform. 17 (2016) 786–795.

[112] M.S. Lindner, B. Strauch, J.M. Schulze, S. Tausch, P.W. Dabrowski, A. Nitsche, B.Y.
Renard, HiLive – Real-Time Mapping of Illumina Reads while Sequencing, Bioinformatics.
(2016) btw659. https://doi.org/10.1093/bioinformatics/btw659.

[113] Performance, CCS Docs. (n.d.). https://ccs.how/faq/performance (accessed April 5,

2022).

[114] F.J. Rang, W.P. Kloosterman, J. de Ridder, From squiggle to basepair: computational
approaches for improving nanopore sequencing read accuracy, Genome Biol. 19 (2018) 90.
[115] R.R. Wick, L.M. Judd, K.E. Holt, Performance of neural network basecalling tools for

Oxford Nanopore sequencing, Genome Biol. 20 (2019) 129.

[116] Q. Lou, S.C. Janga, L. Jiang, Helix: Algorithm/Architecture Co-design for Accelerating

40

(n.d.).
(accessed March

Nanopore Genome Base-calling, in: Proceedings of the ACM International Conference on
Parallel Architectures and Compilation Techniques, Association for Computing Machinery,
New York, NY, USA, 2020: pp. 293–304.

[117]

J.D. Ferreira, G. Falcao, J. Gómez-Luna, M. Alser, L. Orosa, M. Sadrosadati, J.S. Kim,
G.F. Oliveira, T. Shahroodi, A. Nori, O. Mutlu, PLUTo: Enabling massively parallel
computation
(2021).
http://arxiv.org/abs/2104.07699.

[cs.AR].

lookup

tables,

DRAM

arXiv

via

in

[118] N. Hajinazar, G.F. Oliveira, S. Gregorio, J.D. Ferreira, N.M. Ghiasi, M. Patel, M. Alser, S.
Ghose, J. Gómez-Luna, O. Mutlu, SIMDRAM: a framework for bit-serial SIMD processing
using DRAM, in: Proceedings of the 26th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems, Association for Computing
Machinery, New York, NY, USA, 2021: pp. 329–345.

[119]

I. Fernandez, R. Quislant, E. Gutiérrez, O. Plata, C. Giannoula, M. Alser, J.
Gómez-Luna, O. Mutlu, NATSA: A Near-Data Processing Accelerator for Time Series
Analysis, in: 2020 IEEE 38th International Conference on Computer Design (ICCD), 2020:
pp. 120–129.

[120] F. Schuiki, M. Schaffner, F.K. Gürkaynak, L. Benini, A Scalable Near-Memory
Architecture for Training Deep Neural Networks on Large In-Memory Datasets, IEEE Trans.
Comput. 68 (2019) 484–497.

[121] Z. Xu, Y. Mai, D. Liu, W. He, X. Lin, C. Xu, L. Zhang, X. Meng, J. Mafofo, W.A. Zaher, A.
Koshy, Y. Li, N. Qiao, Fast-bonito: A faster deep learning based basecaller for nanopore
sequencing, Artificial Intelligence in the Life Sciences. 1 (2021) 100011.

[122] Y.K. Wan, C. Hendra, P.N. Pratanwanich, J. Göke, Beyond sequencing: machine
learning algorithms extract biology hidden in Nanopore signal data, Trends Genet. 38
(2022) 246–257.

[123] H. Gamaarachchi, C.W. Lam, G. Jayatilaka, H. Samarakoon, J.T. Simpson, M.A. Smith,
rapid

S. Parameswaran, GPU accelerated adaptive banded event alignment
comparative nanopore signal analysis, BMC Bioinformatics. 21 (2020) 343.

for

[124] M. Loose, S. Malla, M. Stout, Real-time selective sequencing using nanopore

technology, Nat. Methods. 13 (2016) 751–754.

[125] T. Dunn, H. Sadasivan, J. Wadden, K. Goliya, K.-Y. Chen, D. Blaauw, R. Das, S.
Narayanasamy, SquiggleFilter: An Accelerator for Portable Virus Detection, in: MICRO-54:
IEEE/ACM International Symposium on Microarchitecture, Association for
54th Annual
Computing Machinery, New York, NY, USA, 2021: pp. 535–549.

[126] S. Kovaka, Y. Fan, B. Ni, W. Timp, M.C. Schatz, Targeted nanopore sequencing by
real-time mapping of raw electrical signal with UNCALLED, Nat. Biotechnol. 39 (2021)
431–441.

[127] H. Zhang, H. Li, C. Jain, H. Cheng, K.F. Au, H. Li, S. Aluru, Real-time mapping of

nanopore raw signals, Bioinformatics. 37 (2021) i477–i483.

[128] Using Dynamic

Time Warping

to Find Patterns

in Time Series,

(n.d.).

https://www.aaai.org/Library/Workshops/1994/ws94-03-031.php (accessed April 5, 2022).

[129] How does CCS work, CCS Docs.

(n.d.). https://ccs.how/how-does-ccs-work.html

(accessed March 25, 2022).

[130] H. Li, Minimap2: pairwise alignment for nucleotide sequences, Bioinformatics. 34 (2018)

3094–3100.

[131] M. Šošić, M. Šikić, Edlib: a C/C++ library for fast, exact sequence alignment using edit

distance, Bioinformatics. 33 (2017) 1394–1395.

[132] U.H. Trivedi, T. CÃ©zard, S. Bridgett, A. Montazam, J. Nichols, M. Blaxter, K. Gharbi,
Quality control of next-generation sequencing data without a reference, Frontiers in
Genetics. 5 (2014). https://doi.org/10.3389/fgene.2014.00111.

[133] Picard, (n.d.). https://broadinstitute.github.io/picard (accessed March 27, 2022).

41

[134] T.J. Ham, D. Bruns-Smith, B. Sweeney, Y. Lee, S.H. Seo, U. Gyeong Song, Y.H. Oh, K.
Asanovic, J.W. Lee, L.W. Wills, Genesis: A Hardware Acceleration Framework for Genomic
Data Analysis, 2020 ACM/IEEE 47th Annual
International Symposium on Computer
Architecture (ISCA). (2020). https://doi.org/10.1109/isca45697.2020.00031.

[135] P.D.N. Hebert, T.R. Gregory, The promise of DNA barcoding for taxonomy, Syst. Biol. 54

(2005) 852–859.

[136] A. Baccaro, A.-L. Steck, A. Marx, Barcoded nucleotides, Angew. Chem. Int. Ed Engl. 51

(2012) 254–257.

[137] S. Andrews, Others, FastQC: a quality control tool for high throughput sequence data,

(2010).

[138] Y. Fukasawa, L. Ermini, H. Wang, K. Carty, M.-S. Cheung, LongQC: A Quality Control

Tool for Third Generation Sequencing Long Read Data, G3 . 10 (2020) 1193–1196.

[139] Z. Yin, H. Zhang, M. Liu, W. Zhang, H. Song, H. Lan, Y. Wei, B. Niu, B. Schmidt, W. Liu,
RabbitQC: high-speed scalable quality control for sequencing data, Bioinformatics. 37
(2021) 573–574.

[140] M. Alser, H. Hassan, A. Kumar, O. Mutlu, C. Alkan, Shouji: a fast and efficient

pre-alignment filter for sequence alignment, Bioinformatics. 35 (2019) 4255–4263.

[141] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan, N. Homer, G. Marth, G. Abecasis,
R. Durbin, 1000 Genome Project Data Processing Subgroup, The Sequence
Alignment/Map format and SAMtools, Bioinformatics. 25 (2009) 2078–2079.

[142] A. Backurs, P. Indyk, Edit Distance Cannot Be Computed in Strongly Subquadratic Time
(unless SETH is false), in: Proceedings of the Forty-Seventh Annual ACM Symposium on
Theory of Computing, Association for Computing Machinery, New York, NY, USA, 2015: pp.
51–58.

[143] H. Xin, S. Nahar, R. Zhu, J. Emmons, G. Pekhimenko, C. Kingsford, C. Alkan, O. Mutlu,
Optimal seed solver: optimizing seed selection in read mapping, Bioinformatics. 32 (2016)
1632–1642.

[144] C. Firtina, J. Park, J.S. Kim, M. Alser, D.S. Cali, T. Shahroodi, N.M. Ghiasi, G. Singh, K.
Kanellopoulos, C. Alkan, O. Mutlu, BLEND: A Fast, Memory-Efficient, and Accurate
Mechanism
(2021).
to
http://arxiv.org/abs/2112.08687.

Seed Matches,

[q-bio.GN].

Fuzzy

arXiv

Find

[145] S. Schleimer, D.S. Wilkerson, A. Aiken, Winnowing: local algorithms for document
fingerprinting, in: Proceedings of the 2003 ACM SIGMOD International Conference on
Management of Data, Association for Computing Machinery, New York, NY, USA, 2003: pp.
76–85.

[146] C. Jain, A. Rhie, H. Zhang, C. Chu, B.P. Walenz, S. Koren, A.M. Phillippy, Weighted
minimizer sampling improves long read mapping, Bioinformatics. 36 (2020) i111–i118.
[147] H. Xin, D. Lee, F. Hormozdiari, S. Yedkar, O. Mutlu, C. Alkan, Accelerating read mapping

with FastHASH, BMC Genomics. 14 Suppl 1 (2013) S13.

[148] R. Edgar, Syncmers are more sensitive than minimizers for selecting conserved k‑mers

in biological sequences, PeerJ. 9 (2021) e10805.

[149] D. Pellow, A. Dutta, R. Shamir, Using syncmers improves long-read mapping, bioRxiv.

(2022) 2022.01.10.475696. https://doi.org/10.1101/2022.01.10.475696.

[150] B. Ma, J. Tromp, M. Li, PatternHunter: faster and more sensitive homology search,

Bioinformatics. 18 (2002) 440–445.

[151] K. Sahlin, Effective sequence similarity detection with strobemers, Genome Res. 31

(2021) 2080–2094.

[152] S. Girotto, M. Comin, C. Pizzi, Efficient computation of spaced seed hashing with block

indexing, BMC Bioinformatics. 19 (2018) 441.

[153] A. Chakraborty, B. Morgenstern, S. Bandyopadhyay, S-conLSH: alignment-free gapped

mapping of noisy long reads, BMC Bioinformatics. 22 (2021) 64.

42

[154] K. Sahlin, Flexible seed size enables ultra-fast and accurate read alignment, (n.d.).

https://doi.org/10.1101/2021.06.18.449070.

[155] K. Berlin, S. Koren, C.-S. Chin, J.P. Drake, J.M. Landolin, A.M. Phillippy, Assembling
large genomes with single-molecule sequencing and locality-sensitive hashing, Nat.
Biotechnol. 33 (2015) 623–630.
[156] H. Li, Minimap and miniasm:

fast mapping and de novo assembly for noisy long

sequences, Bioinformatics. 32 (2016) 2103–2110.

[157] R. Langarita, A. Armejach, J. Setoain, P.E.I. Marin, J. Alastruey-Benedé, M.M. Planas,
Compressed sparse FM-index: Fast sequence alignment using large k-steps, IEEE/ACM
Trans.
(2020).
https://ieeexplore.ieee.org/abstract/document/9109660/.

Bioinform.

Comput.

Biol.

[158] M. Vasimuddin, S. Misra, H. Li, S. Aluru, Efficient Architecture-Aware Acceleration of
BWA-MEM for Multicore Systems, in: 2019 IEEE International Parallel and Distributed
Processing Symposium (IPDPS), 2019: pp. 314–324.

[159] T. Anderson, T.J. Wheeler, An optimized FM-index library for nucleotide and amino acid

search, Algorithms Mol. Biol. 16 (2021) 25.

[160] A. Subramaniyan, J. Wadden, K. Goliya, N. Ozog, X. Wu, S. Narayanasamy, D. Blaauw,
R. Das, Accelerated Seeding for Genome Sequence Alignment with Enumerated Radix
Trees, in: 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture
(ISCA), 2021: pp. 388–401.

[161] D. Ho, J. Ding, S. Misra, N. Tatbul, V. Nathan, Vasimuddin, T. Kraska, LISA: Towards
Learned DNA Sequence Search, arXiv [cs.DB]. (2019). http://arxiv.org/abs/1910.04728.

[162] S. Kalikar, C. Jain, Vasimuddin, S. Misra, Accelerating minimap2 for

long-read
sequencing applications on modern CPUs, Nature Computational Science. 2 (2022) 78–83.
[163] W. Huangfu, X. Li, S. Li, X. Hu, P. Gu, Y. Xie, MEDAL: Scalable DIMM based Near Data
Processing Accelerator for DNA Seeding Algorithm, in: Proceedings of the 52nd Annual
IEEE/ACM International Symposium on Microarchitecture, Association for Computing
Machinery, New York, NY, USA, 2019: pp. 587–599.

[164] W. Huangfu, S. Li, X. Hu, Y. Xie, RADAR: A 3D-ReRAM based DNA Alignment
Accelerator Architecture, in: 2018 55th ACM/ESDA/IEEE Design Automation Conference
(DAC), 2018: pp. 1–6.

[165] V.I. Levenshtein, Others, Binary codes capable of correcting deletions, insertions, and

reversals, in: Soviet Physics Doklady, Soviet Union, 1966: pp. 707–710.

[166] M. Alser, H. Hassan, H. Xin, O. Ergin, O. Mutlu, C. Alkan, GateKeeper: a new hardware
architecture for accelerating pre-alignment in DNA short read mapping, Bioinformatics. 33
(2017) 3355–3363.

[167] H. Xin, J. Greth, J. Emmons, G. Pekhimenko, C. Kingsford, C. Alkan, O. Mutlu, Shifted
to accelerate alignment
1553–1560.

Hamming distance: a fast and accurate SIMD-friendly filter
verification
31
read
https://doi.org/10.1093/bioinformatics/btu856.

Bioinformatics.

mapping,

(2015)

in

[168] M. Alser, O. Mutlu, C. Alkan, MAGNET: Understanding and Improving the Accuracy of

Genome Pre-Alignment Filtering, arXiv [q-bio.GN]. (2017). http://arxiv.org/abs/1707.01631.

[169] A. Nag, C.N. Ramachandra, R. Balasubramonian, R. Stutsman, E. Giacomin, H.
Kambalasubramanyam, P.-E. Gaillardon, GenCache: Leveraging In-Cache Operators for
Efficient Sequence Alignment, in: Proceedings of the 52nd Annual IEEE/ACM International
Symposium on Microarchitecture, Association for Computing Machinery, New York, NY,
USA, 2019: pp. 334–346.

[170] G. Rizk, D. Lavenier, GASSST: global alignment short sequence search tool,

Bioinformatics. 26 (2010) 2534–2540.

[171] D. Castells-Rufas, S. Marco-Sola, J.C. Moure, Q. Aguado, A. Espinosa, FPGA
Acceleration of Pre-Alignment Filters for Short Read Mapping With HLS, IEEE Access. 10

43

(undefined 2022) 22079–22100.

[172] F. Hach, I. Sarrafi, F. Hormozdiari, C. Alkan, E.E. Eichler, S.C. Sahinalp, mrsFAST-Ultra:
a compact, SNP-aware mapper for high performance sequencing applications, Nucleic
Acids Res. 42 (2014) W494–500.

[173] M. Khalifa, R. Ben-Hur, R. Ronen, O. Leitersdorf, L. Yavits, S. Kvatinsky, FiltPIM:
In-Memory Filter for DNA Sequencing, in: 2021 28th IEEE International Conference on
Electronics, Circuits, and Systems (ICECS), 2021: pp. 1–4.

[174] D. Weese, M. Holtgrewe, K. Reinert, RazerS 3: faster, fully sensitive read mapping,

Bioinformatics. 28 (2012) 2592–2599.

[175] F. Hameed, A.A. Khan, J. Castrillon, ALPHA: A Novel Algorithm-Hardware Co-design for
IEEE Transactions on Emerging Topics in

Accelerating DNA Seed Location Filtering,
Computing. (undefined 2021) 1–1.

[176] B. Liu, D. Guan, M. Teng, Y. Wang, rHAT: fast alignment of noisy long reads with regional

hashing, Bioinformatics. 32 (2016) 1625–1631.

[177] L. Guo, J. Lau, Z. Ruan, P. Wei, J. Cong, Hardware Acceleration of Long Read Pairwise
Overlapping in Genome Sequencing: A Race Between FPGA and GPU, in: 2019 IEEE 27th
International Symposium on Field-Programmable Custom Computing Machines
Annual
(FCCM), 2019: pp. 127–135.

[178] H. Sadasivan, M. Maric, E. Dawson, V. Iyer, J. Israeli, S. Narayanasamy, Accelerating
Minimap2 for accurate long read alignment on GPUs, bioRxiv. (2022) 2022.03.09.483575.
https://doi.org/10.1101/2022.03.09.483575.

[179] M. Schmidt, K. Heese, A. Kutzner, Accurate high throughput alignment via line

sweep-based seed processing, Nat. Commun. 10 (2019) 1939.

[180] O. Gotoh, An improved algorithm for matching biological sequences, J. Mol. Biol. 162

(1982) 705–708.

[181] W.J. Masek, M.S. Paterson, A faster algorithm computing string edit distances, J.

Comput. System Sci. 20 (1980) 18–31.

[182] E. Ukkonen, Algorithms for approximate string matching, Information and Control. 64

(1985) 100–118.

[183]

J. Daily, Parasail: SIMD C library for global, semi-global, and local pairwise sequence

alignments, BMC Bioinformatics. 17 (2016) 81.

[184] S. Marco-Sola, J.C. Moure, M. Moreto, A. Espinosa, Fast gap-affine pairwise alignment

using the wavefront algorithm, Bioinformatics. 37 (2021) 456–463.

[185]

J.M. Eizenga, B. Paten, Improving the time and space complexity of the WFA algorithm
2022.01.12.476087.

scoring,

bioRxiv.

(2022)

its

and
https://doi.org/10.1101/2022.01.12.476087.

generalizing

[186] S. Marco-Sola, J.M. Eizenga, A. Guarracino, B. Paten, E. Garrison, M. Moreto, Optimal
2022.04.14.488380.
space,

alignment

bioRxiv.

(2022)

O(s)

gap-affine
https://doi.org/10.1101/2022.04.14.488380.

in

[187] H. Xin, J. Kim, S. Nahar, C. Kingsford, C. Alkan, O. Mutlu, LEAP: A Generalization of the
(2017) 133157.

Landau-Vishkin Algorithm with Custom Gap Penalties, bioRxiv.
https://doi.org/10.1101/133157.

[188] N. Ahmed, J. Lévy, S. Ren, H. Mushtaq, K. Bertels, Z. Al-Ars, GASAL2: a GPU
accelerated sequence alignment library for high-throughput NGS data, BMC Bioinformatics.
20 (2019) 520.

[189] H. Li, Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM,

arXiv [q-bio.GN]. (2013). http://arxiv.org/abs/1303.3997.

[190] Q. Aguado-Puig, S. Marco-Sola, J.C. Moure, C. Matzoros, D. Castells-Rufas, A.
Espinosa, M. Moreto, WFA-GPU: Gap-affine pairwise alignment using GPUs, bioRxiv.
(2022) 2022.04.18.488374. https://doi.org/10.1101/2022.04.18.488374.

[191] X. Fei, Z. Dan, L. Lina, M. Xin, Z. Chunlei, FPGASW: Accelerating large-scale

44

smith–Waterman sequence alignment application with backtracking on FPGA linear systolic
array, Interdiscip. Sci. 10 (2018) 176–188.

[192] Kung, Why systolic architectures?, Computer . 15 (1982) 37–46.
[193] D. Fujiki, S. Wu, N. Ozog, K. Goliya, D. Blaauw, S. Narayanasamy, R. Das, SeedEx: A
Genome Sequencing Accelerator for Optimal Alignments in Subminimal Space, in: 2020
53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 2020: pp.
937–950.

[194] A. Haghi, S. Marco-Sola, L. Alvarez, D. Diamantopoulos, C. Hagleitner, M. Moreto, An
FPGA Accelerator of the Wavefront Algorithm for Genomics Pairwise Alignment, in: 2021
31st International Conference on Field-Programmable Logic and Applications (FPL), 2021:
pp. 151–159.

[195] D. Fujiki, A. Subramaniyan, T. Zhang, Y. Zeng, R. Das, D. Blaauw, S. Narayanasamy,
GenAx: A Genome Sequencing Accelerator, in: 2018 ACM/IEEE 45th Annual International
Symposium on Computer Architecture (ISCA), 2018: pp. 69–82.

[196] S. Gupta, M. Imani, B. Khaleghi, V. Kumar, T. Rosing, RAPID: A ReRAM Processing
in-Memory Architecture for DNA Sequence Alignment, in: 2019 IEEE/ACM International
Symposium on Low Power Electronics and Design (ISLPED), 2019: pp. 1–6.

[197] E.F. de O. Sandes, E.F. de Oliveira Sandes, G. Miranda, X. Martorell, E. Ayguade, G.
Incremental Speculative Traceback for Exact
IEEE Transactions on Parallel and

Teodoro, A.C.M. Melo, CUDAlign 4.0:
Chromosome-Wide Alignment
Distributed Systems. 27 (2016) 2838–2850. https://doi.org/10.1109/tpds.2016.2515597.
[198] S. Diab, A. Nassereldine, M. Alser, J.G. Luna, O. Mutlu, I. El Hajj, High-throughput
Pairwise Alignment with the Wavefront Algorithm using Processing-in-Memory, arXiv
[cs.AR]. (2022). http://arxiv.org/abs/2204.02085.

in GPU Clusters,

[199] P. Chen, C. Wang, X. Li, X. Zhou, Accelerating the Next Generation Long Read Mapping
IEEE/ACM Trans. Comput. Biol. Bioinform. 11 (2014)

with the FPGA-Based System,
840–852.

[200] G. Myers, A fast bit-vector algorithm for approximate string matching based on dynamic

programming, J. ACM. 46 (1999) 395–415.

[201]

J. Loving, Y. Hernandez, G. Benson, BitPAl: a bit-parallel, general

integer-scoring

sequence alignment algorithm, Bioinformatics. 30 (2014) 3166–3173.

[202] S.S. Banerjee, M. El-Hadedy, J.B. Lim, Z.T. Kalbarczyk, D. Chen, S.S. Lumetta, R.K.
Iyer, ASAP: Accelerated Short-Read Alignment on Programmable Hardware, IEEE Trans.
Comput. 68 (2019) 331–346.

[203] M. Charikar, O. Geri, M.P. Kim, W. Kuszmaul, On Estimating Edit Distance: Alignment,
(2018).

Embeddings,

Reduction,

[cs.DS].

arXiv

and

Dimension
http://arxiv.org/abs/1804.09907.

[204] T. Batu, F. Ergun, C. Sahinalp, Oblivious string embeddings and edit distance
the Seventeenth Annual ACM-SIAM Symposium on

approximations, Proceedings of
Discrete Algorithm - SODA ’06. (2006). https://doi.org/ 10.1145/1109557.1109644.

[205] A. Andoni, K. Onak, Approximating Edit Distance in Near-Linear Time, SIAM Journal on

Computing. 41 (2012) 1635–1648. https://doi.org/10.1137/090767182.

[206] D. Chakraborty, D. Das, E. Goldenberg, M. Koucky, M. Saks, Approximating Edit
Distance within Constant Factor in Truly Sub-Quadratic Time, 2018 IEEE 59th Annual
(2018).
Symposium
https://doi.org/10.1109/focs.2018.00096.

Foundations

Computer

(FOCS).

Science

on

of

[207] G.S.C. Slater, E. Birney, Automated generation of heuristics for biological sequence

comparison, BMC Bioinformatics. 6 (2005) 31.

[208] Z. Zhang, S. Schwartz, L. Wagner, W. Miller, A greedy algorithm for aligning DNA

sequences, J. Comput. Biol. 7 (2000) 203–214.

[209] A. Zeni, G. Guidi, M. Ellis, N. Ding, M.D. Santambrogio, S. Hofmeyr, A. Buluc, L. Oliker,

45

K. Yelick, LOGAN: High-Performance GPU-Based X-Drop Long-Read Alignment, 2020
IEEE International Parallel and Distributed Processing Symposium (IPDPS).
(2020).
https://doi.org/10.1109/ipdps47924.2020.00055.

[210] H. Suzuki, M. Kasahara, Acceleration of Nucleotide Semi-Global Alignment with

Adaptive Banded Dynamic Programming, (n.d.). https://doi.org/10.1101/130633.

[211] D. Liu, M. Steinegger, Block aligner: fast and flexible pairwise sequence alignment with
2021.11.08.467651.

bioRxiv.

(2021)

SIMD-accelerated
blocks,
https://doi.org/10.1101/2021.11.08.467651.
Practices

adaptive

[212] Best

Workflows

–

GATK,

(n.d.).

https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows
(accessed March 25, 2022).

[213] R. Poplin, P.-C. Chang, D. Alexander, S. Schwartz, T. Colthurst, A. Ku, D. Newburger, J.
Dijamco, N. Nguyen, P.T. Afshar, S.S. Gross, L. Dorfman, C.Y. McLean, M.A. DePristo, A
universal SNP and small-indel variant caller using deep neural networks, Nat. Biotechnol.
36 (2018) 983–987.

[214] D.C. Koboldt, Best practices for variant calling in clinical sequencing, Genome Med. 12

(2020) 91.

[215]

J.M. Zook, N.F. Hansen, N.D. Olson, L. Chapman, J.C. Mullikin, C. Xiao, S. Sherry, S.
Koren, A.M. Phillippy, P.C. Boutros, S.M.E. Sahraeian, V. Huang, A. Rouette, N. Alexander,
C.E. Mason, I. Hajirasouliha, C. Ricketts, J. Lee, R. Tearle, I.T. Fiddes, A.M. Barrio, J. Wala,
A. Carroll, N. Ghaffari, O.L. Rodriguez, A. Bashir, S. Jackman, J.J. Farrell, A.M. Wenger, C.
Alkan, A. Soylev, M.C. Schatz, S. Garg, G. Church, T. Marschall, K. Chen, X. Fan, A.C.
English, J.A. Rosenfeld, W. Zhou, R.E. Mills, J.M. Sage, J.R. Davis, M.D. Kaiser, J.S.
Oliver, A.P. Catalano, M.J.P. Chaisson, N. Spies, F.J. Sedlazeck, M. Salit, A robust
benchmark for detection of germline large deletions and insertions, Nat. Biotechnol. 38
(2020) 1347–1355.

[216] Genome in a bottle, NIST. (n.d.). https://www.nist.gov/programs-projects/genome-bottle

(accessed March 25, 2022).

[217] D. Sampietro, C. Crippa, L. Di Tucci, E. Del Sozzo, M.D. Santambrogio, FPGA-based
PairHMM Forward Algorithm for DNA Variant Calling, 2018 IEEE 29th International
Conference on Application-Specific Systems, Architectures and Processors (ASAP).
(2018). https://doi.org/10.1109/asap.2018.8445119.

[218] D. Freed, R. Aldana, J.A. Weber, J.S. Edwards, The Sentieon Genomics Tools – A fast
and accurate solution to variant calling from next-generation sequence data, bioRxiv. (2017)
115717. https://doi.org/10.1101/115717.

[219] deepvariant-details.md

at
https://github.com/google/deepvariant (accessed April 5, 2022).

r1.3

google/deepvariant,

·

Github,

n.d.

[220] R. Luo, Y.-L. Wong, W.-C. Law, L.-K. Lee, J. Cheung, C.-M. Liu, T.-W. Lam, BALSA:
for whole-genome and whole-exome sequencing,

integrated secondary analysis
accelerated by GPU, PeerJ. 2 (2014) e421.

[221]

Illumina

DRAGEN

Bio-IT

Platform,

(n.d.).

https://www.illumina.com/products/by-type/informatics-products/dragen-bio-it-platform.html
(accessed March 26, 2022).

[222] A. Goyal, H.J. Kwon, K. Lee, R. Garg, S.Y. Yun, Y.H. Kim, S. Lee, M.S. Lee, Ultra-fast
next generation human genome sequencing data processing using DRAGENTM bio-IT
processor for precision medicine, Open Journal of Genetics. 7 (2017) 9–19.

[223] NVIDIA

Genome

Sequencing

Analysis,

NVIDIA.

(n.d.).

https://developer.nvidia.com/clara-parabricks (accessed March 26, 2022).

[224] E. Talpes, D.D. Sarma, G. Venkataramanan, P. Bannon, B. McGee, B. Floering, A.
Jalote, C. Hsiong, S. Arora, A. Gorti, G.S. Sachdev, Compute Solution for Tesla’s Full
Self-Driving Computer, IEEE Micro. 40 (2020) 25–35.

46

[225] G. Lauterbach, The Path to Successful Wafer-Scale Integration: The Cerebras Story,

IEEE Micro. 41 (2021) 52–57.

[226] D.

Lavenier, R. Cimadomo, R.
in:

Processor-in-Memory Architecture,
Bioinformatics and Biomedicine (BIBM), 2020: pp. 204–207.

Jodin, Variant Calling Parallelization
2020

on
IEEE International Conference on

[227] M. Alser, J.S. Kim, N.A. Alserr, S.W. Tell, O. Mutlu, COVIDHunter: An Accurate, Flexible,
and Environment-Aware Open-Source COVID-19 Outbreak Simulation Model, arXiv
[q-bio.PE]. (2021). http://arxiv.org/abs/2102.03667.

[228] R.M. Sherman, J. Forman, V. Antonescu, D. Puiu, M. Daya, N. Rafaels, M.P. Boorgula,
S. Chavan, C. Vergara, V.E. Ortega, A.M. Levin, C. Eng, M. Yazdanbakhsh, J.G. Wilson, J.
Marrugo, L.A. Lange, L.K. Williams, H. Watson, L.B. Ware, C.O. Olopade, O. Olopade, R.R.
Oliveira, C. Ober, D.L. Nicolae, D.A. Meyers, A. Mayorga, J. Knight-Madden, T. Hartert,
N.N. Hansel, M.G. Foreman, J.G. Ford, M.U. Faruque, G.M. Dunston, L. Caraballo, E.G.
Burchard, E.R. Bleecker, M.I. Araujo, E.F. Herrera-Paz, M. Campbell, C. Foster, M.A. Taub,
T.H. Beaty,
I. Ruczinski, R.A. Mathias, K.C. Barnes, S.L. Salzberg, Assembly of a
pan-genome from deep sequencing of 910 humans of African descent, Nat. Genet. 51
(2019) 30–35.

[229] S. Ballouz, A. Dobin, J.A. Gillis, Is it time to change the reference genome?, Genome

Biol. 20 (2019) 159.

[230] B. Paten, A.M. Novak, J.M. Eizenga, E. Garrison, Genome graphs and the evolution of

genome inference, Genome Res. 27 (2017) 665–676.

[231] D.S. Cali, K. Kanellopoulos, J. Lindegger, Z. Bingöl, G.S. Kalsi, Z. Zuo, C. Firtina, M.B.
Cavlak, J. Kim, N.M. Ghiasi, G. Singh, J. Gómez-Luna, N.A. Alserr, M. Alser, S.
Subramoney, C. Alkan, S. Ghose, O. Mutlu, SeGraM: A Universal Hardware Accelerator for
Genomic Sequence-to-Graph and Sequence-to-Sequence Mapping, arXiv [cs.AR]. (2022).
http://arxiv.org/abs/2205.05883.

[232]

J.S. Kim, C. Firtina, M.B. Cavlak, D.S. Cali, C. Alkan, O. Mutlu, FastRemap: A Tool for
(2022).

Quickly Remapping Reads between Genome Assemblies, arXiv [q-bio.GN].
http://arxiv.org/abs/2201.06255.

47

