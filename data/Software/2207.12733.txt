2
2
0
2

l
u
J

6
2

]
E
S
.
s
c
[

1
v
3
3
7
2
1
.
7
0
2
2
:
v
i
X
r
a

On the Interaction between Test-Suite Reduction and
Regression-Test Selection Strategies

Sebastian Ruland1 and Malte Lochau2

1Technical University Darmstadt, Darmstadt, Germany
2University of Siegen, Siegen, Germany

July 27, 2022

Abstract

Unit testing is one of the most established quality-assurance techniques for software
development. One major advantage of unit testing is the adjustable trade-oï¬€ between
eï¬ƒciency (i.e., testing eï¬€ort) and eï¬€ectiveness (i.e., fault-detection probability). To this
end, various strategies have been proposed to exploit this trade-oï¬€.
In particular, test-
suite reduction (TSR) reduces the number of (presumably redundant) test cases while
testing a single program version. Regression-test selection (RTS) selects test cases for
testing consecutive program revisions. However, both TSR and RTS may inï¬‚uenceâ€”or
even obstructâ€” each othersâ€™ performance when used in combination. For instance, test
cases discarded during TSR for a particular program version may become relevant again
for RTS. However, ï¬nding a combination of both strategies leading to a reasonable trade-
oï¬€ throughout the version history of a program is an open question. The goal of this
paper is to gain a better understanding of the interactions between TSR and RTS with
respect to eï¬ƒciency and eï¬€ectiveness. To this end, we present a conï¬gurable framework
called RegreTS for automated unit-testing of C programs. The framework comprises
diï¬€erent strategies for TSR and RTS and possible combinations thereof. We apply this
framework to a collection of subject systems, delivering several crucial insights. First, TSR
has almost always a negative impact on the eï¬€ectiveness of RTS, yet a positive impact
on eï¬ƒciency. Second, test cases revealing to testers the eï¬€ect of program modiï¬cations
between consecutive program versions are far more eï¬€ective than test cases simply covering
modiï¬ed code parts, yet causing much more testing eï¬€ort.

1 Introduction

Background and Motivation. Software testing is concerned with revealing as many bugs
as possible in a program within aâ€”usually strictly limitedâ€”amount of time [1]. In particular,
unit testing is one of the most important innovations in the recent past for pro-actively ensuring
software quality in an eï¬€ective, yet tractable and agile manner. Bugs revealed by unit tests
include program crashes caused by programming errors as well as faulty input-output value
pairs contradicting a given speciï¬cation of the expected behavior (e.g., functionally incorrect
program logics or ambiguous requirements). To this end, test cases are deï¬ned in terms of
exemplary input values and expected output values for experimentally executing the program
unit under test in a systematic manner.

One advantage of software testing as compared to other quality-assurance techniques is
theâ€”more or lessâ€”freely adjustable trade-oï¬€ between two (generally conï¬‚icting) optimization
goals, namely eï¬€ectiveness (i.e., maximizing fault-detection probability) and eï¬ƒciency (mini-
mizing testing eï¬€ort) [2â€“4]. In an idealistic setting, a perfect trade-oï¬€ between both goals would

 
 
 
 
 
 
RegreTS

Page 2 / 39

be a test suite that ï¬nds all bugs with minimum eï¬€ort (e.g., requiring a minimum number
of test cases). In reality, however, the number and exact location of bugs are, unfortunately,
a-priori unknown; and even if all bugs would be known in advance (which would make testing
obsolete), then ï¬nding a minimum set of test cases for revealing them is NP-hard [5]. Vari-
ous heuristics have been proposed to control the selection of suï¬ƒciently eï¬€ective, yet eï¬ƒcient
sets of test cases for a program unit under test. Heuristics for measuring eï¬€ectiveness often
rely on structural code-coverage metrics (e.g., branch coverage) to be optimized by a selected
test suite [6], whereas heuristics for eï¬ƒciency apply test-suite reduction (TSR) techniques to
decrease the number of redundant test cases, again, with respect to that given code-coverage
criterion [2].

Moreover, modern software development is faced with the challenge of ever-shortened release
cycles leading to increasing frequencies of consecutive program revisions. The goal of regression
testing is to reveal emerging bugs introduced by erroneous program modiï¬cations between
subsequent program versions [2]. RTS strategies deï¬ne criteria for updating an existing test
suite of a previous program version, by removing outdated test cases, by keeping still relevant
test cases, and by adding further test cases for newly added functionality. Many RTS and
regression-test generation strategies are further concerned with prioritizing test cases to apply
the presumably more eï¬€ective test cases ï¬rst. The aforementioned trade-oï¬€ between eï¬ƒciency
and eï¬€ectiveness is therefore also the primary goal of regression testing, but now starting from
an existing test suite inherited from previous program revisions.

Problem Statement and Research Challenges. Both TSR strategies for testing a single
program version as well as regression-testing strategies for testing consecutive program revisions
are concerned with the same problem:

How to achieve a reasonable trade-oï¬€ between eï¬ƒciency and eï¬€ectiveness?

Nevertheless, both kinds of strategies may potentially inï¬‚uenceâ€”or even obstructâ€” each
other in various ways. For instance, a test case being considered redundant for one program
version (thus being removed during TSR) may become relevant, again, for later program revi-
sions. As a result, expensive re-selections of eventually â€œlostâ€ test cases may become necessary
during regression testing. Hence, excessive TSR, although potentially improving testing eï¬ƒ-
ciency for one program version, may, eventually, have a negative impact on regression testing.
On the other hand, too reluctant TSR may lead to an ever-growing test suite thus also increas-
ing test-selection eï¬€ort during regression testing. Those subtle interactions between TSR and
regression testing are neither obvious nor fully predictable in advance. Finding a feasible com-
bination of both strategies yielding a suitable eï¬ƒciency/eï¬€ectiveness trade-oï¬€ throughout the
entire version history of a program is an open challenge involving further ï¬ne-grained details
to be taken into account (e.g., code-coverage criteria, TSR heuristics, RTS criteria etc.).

Contributions. We present a conceptual framework for an in-depth investigation of interac-
tions between strategies for TSR and RTS and the resulting impact on (mutually contradicting)
eï¬ƒciency and eï¬€ectiveness measures. We focus on unit testing of C programs and use practi-
cally established control-ï¬‚ow coverage criteria as eï¬€ectiveness measure for RTS and TSR [4].
As eï¬€ectiveness heuristics for RTS, we consider modiï¬cation-traversing as well as modiï¬cation-
revealing test cases [2].

In our experimental comparison of the diï¬€erent strategies, we additionally consider fault-
detection rate as a-posteriori eï¬€ectiveness measure. Concerning testing eï¬ƒciency, we measure
sizes of test suites (i.e., number of test cases) as well as the computational eï¬€ort (i.e., CPU-time)
for (regression) test-case generation and/or selection as well as TSR.

To summarize, we make the following contributions.

RegreTS

Page 3 / 39

â€¢ Conï¬gurable framework for unit-test generation and selection, integrating recent TSR

strategies, as well as existing and novel RTS strategies.

â€¢ Tool support for automatically applying diï¬€erent strategies to a version history of C

program units.

â€¢ Experimental evaluation results gained from applying our tool to a collection of C

program units with available version history. The evaluation results show that:

(1) TSR based on a coverage-based notion of test-case redundancy almost always de-
creases eï¬€ectiveness of RTS, yet obviously having a positive impact on testing eï¬ƒciency,

(2) modiï¬cation-revealing test cases are far more eï¬€ective than modiï¬cation-traversing
test cases for RTS, yet modiï¬cation-revealing test-case generation requires much more
computational eï¬€ort and

(3) the number of test cases and the number of previous program versions considered for
RTS only has a low impact on the eï¬€ectiveness of regression testing.

Outline. The remainder of this paper is structured as follows.
Section 2 contains an overview on the necessary background and terminology used in the
remainder of the paper. We ï¬rst introduce an illustrative example to introduce essential notions
and concepts and also use this example to derive the motivation for the methodology proposed
in the main part of the paper.
Section 3 contains a conceptual description of the proposed evaluation framework for investi-
gating the interaction between TSR and RTS strategies. After a general overview, the diï¬€erent
possible strategies are then described in more detail using the illustrative example.
Section 4 provides a detailed description of our tool support for the envisioned methodology
together with experimental evaluation results gained from applying the tool to a collection of
subject systems.
Section 5 provides an overview about related work from the ï¬elds of regression testing strate-
gies, automated test-case generation, as well as related approaches for regression veriï¬cation.
Section 6 concludes the paper and provides a brief outlook on possible future work.

Veriï¬ability. To make our results reproducible, we provide the tool implementation and all
experimental results as well as raw data on a supplementary web page1.

2 Background and Motivation

We ï¬rst describe the necessary background for the rest of this paper. We introduce an illustra-
tive running example by means of a small, evolving C program unit with corresponding unit
test cases. Based on this example, we describe basic notions of unit testing, test-suite reduction
and regression testing. We conclude by summarizing the research challenges addressed in the
remainder of this paper.

Program Units. Let us consider the sample C program unit in Fig. 1a. This source code 2
constitutes the initial version P0 of an evolving program developed and (re-)tested in an in-
cremental manner. Function find last receives as inputs an integer-array x[] and an integer
value y and is supposed to return the index of the last occurrence of the value of y in x[]. The
ï¬rst element of the array is supposed to carry as meta-information the length (i.e., the number
of elements) of the array. For this reason, the ï¬rst element should be ignored during the search

1https://www.es.tu-darmstadt.de/regrets
2We will use the terms program, source code and program unit interchangeably for convenience

RegreTS

Page 4 / 39

for the value of y. The search itself is supposed to be performed by iterating over the array
and by keeping track of the latest occurrence of y 3. Additionally, the function should return
predeï¬ned error codes:

â€¢ If the size of the array (i.e., the value of the ï¬rst element) is less or equal to zero, the

returned error-code is -1.

â€¢ If the value of y does not occur in the array, the returned error-code is -2.

Program Bugs. The initial program version P0 in Fig. 1a does, however, not satisfy the
speciï¬ed functionality as it contains the following three bugs.

1. Bug 1 : The search index starts at 0, thus incorrectly including the value of the meta-

information into the search (see line 5).

2. Bug 2 : The search index stops at x[0] âˆ’ 2 thus incorrectly excluding the last element

from the search (see line 5).

3. Bug 3 : The search incorrectly matches all values smaller than, or equal to, y instead of

solely considering values equal to y (see line 6).

To ï¬x those bugs, assume the developer to consecutively creates program revisions by correcting
erroneous code parts.

Program Revisions. Let us assume that three consecutive revisions, P1, P2 and P3, of
the initial program version P0 have been created, where P3 ï¬nally has all three bugs ï¬xed as
described above. A program revision denotes a new program version as a result of modiï¬cations
made to some parts of the previous program version. We represent program revisions using the
established diff-syntax frequently used in patch ï¬les. Line numbers marked with post-ï¬x --
refer to the respective lines in the previous program version being removed in the new version,
and those with post-ï¬x ++ refer to lines in the new version being added to the previous version:

â€¢ Patch 1 in Fig. 1b provides a ï¬x for Bug 1.

â€¢ Patch 2 in Fig. 1d provides a ï¬x for Bug 2.

â€¢ Patch 3 in Fig. 1f provides a ï¬x for Bug 3.

After the ï¬rst bug ï¬x, we obtain the improved (yet still faulty) program version P1 (cf. Fig. 1c).
The second bug ï¬x to P1 (yielding program version P2, cf. Fig. 1e) and the third bug ï¬x to
P2 ï¬nally yield the bug-free version P3 (cf. Fig. 1g). Such an incremental process is often
interleaved by consecutive unit-testing steps to assure correctness of new versions and/or to
reveal further bugs potentially emerging during revisions.

Unit Testing. Test cases for unit testing by means of program inputs are usually selected
with respect to (structural) code coverage criteria. For instance, we require at least two diï¬€erent
test cases for branch coverage of program version P0. First, we require a test case for cover-
ing (reaching) the true-branch of the if-statement in line 2 (i.e., the input-array is empty).
Additionally, we require at least one further test case for covering the remaining branches as
follows.

3Of course, a more reasonable implementation may perform a reversed iterative search and return the ï¬rst
occurrence. The purpose of the example is to demonstrate essential testing concepts in a condensed way thus
requiring some simpliï¬cations which do, however, not threaten the validity of the overall approach.

RegreTS

Page 5 / 39

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =0; i <= x [0] -2; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(a) Initial Version P0 of a Program Unit

5--:
5++:

for ( int i =0; i <= x [0] -2; i ++
for ( int i =1; i <= x [0] -2; i ++

(b) First Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -2; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(c) New Program Version P1 After Applying the First Bug Fix

5--:
5++:

for ( int i =1; i <= x [0] -2; i ++
for ( int i =1; i <= x [0] -1; i ++

(d) Second Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -1; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(e) New Program Version P2 After Applying the Second Bug Fix

6--:
6++:

if ( x [ i ] <= y )
if ( x [ i ] == y )

(f) Third Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -1; i ++)
if ( x [ i ] == y )
last = i ;
return last ;

}

(g) New Program Version P3 After Applying the Third Bug Fix

Figure 1: Program Versions P0, P1, P2 and P3 with their corresponding Bug Fixes

RegreTS

Page 6 / 39

â€¢ The false-branch of the if-statement in line 2 (i.e., requiring the input-array to have at

least one element).

â€¢ The true-branch of the for-loop in line 5 (i.e., requiring the input-array to have at least

two elements).

â€¢ The false-branch of the for-loop in line 5 (i.e., requiring the input-array to have at least

one element).

â€¢ The true-branch of the if-statement in line 6 (i.e., requiring the input-array to have at

least one element being less or equal to y).

â€¢ The false-branch of the if-statement in line 6 (i.e., requiring the input-array to have at

least one element not being less or equal to y).

To satisfy branch coverage on P0, a developer/tester may select a test suite consisting of

the following two test cases 4:
â€¢ t1= (x=[0], y=0),

â€¢ t2= (x=[3,5,5,3], y=4).

We denote test cases t as collections of input-value assignments (i.e., an array x and a value for
y). Test-case speciï¬cations are often further equipped with the expected output values (i.e.,
last=-1 for t1 and last=-2 for t2). If applied to P0, t1 would pass, whereas t2 would indeed
fail as it produces the erroneous return value 0 instead of the expected value -2. Hence, this
test suite, although satisfying branch coverage, only reveals Bug 1 and Bug 3, but not Bug 2.
In contrast, a test suite containing as test cases:

â€¢ t1= (x=[0], y=0),

â€¢ t3= (x=[1,1,1], y=2),

â€¢ t4= (x=[1,2,2], y=0).

also satisï¬es branch coverage and reveals Bug 1 and Bug 2, but not Bug 3. Hence, this test
suite is similarly eï¬€ective as the ï¬rst (revealing two out of three bugs), yet being less eï¬ƒcient
as it causes more testing eï¬€ort due to the additional test-case execution. The test suite:

â€¢ t1= (x=[0], y=0),

â€¢ t2= (x=[3,5,5,3], y=4),

â€¢ t3= (x=[1,1,1], y=2).

satisï¬es branch coverage and reveals all three bugs, since t2 detects Bug 1 and Bug 3, and t3
detects Bug 1 and Bug 2. In contrast, the test suite

â€¢ t1= (x=[0], y=0),

â€¢ t5= (x=[3,1,1,2], y=1).

also satisï¬es branch coverage, but reveals none of the three bugs, since t1 is unable to reach
any bug and the execution of t5 (accidentally) produces a correct output for all versions. These
examples illustrate the well-known dilemma of unit-test selection that (1) adherence to (purely
syntactic) code-coverage criteria does not guarantee eï¬€ective test-suite selections [7, 8], and (2)
using more test cases does not necessary increase eï¬€ectiveness, yet obviously decreases testing
eï¬ƒciency. Concerning (2), TSR aims at removing redundant test cases from a test suite without
decreasing code coverage [4].

4More precisely, this test suite is only able to reach Line 8 due to Bug 3

RegreTS

Page 7 / 39

Test-Suite Reduction. Let us now assume that a developer/tester ï¬rst selects the test cases
t1 and t2 for testing program version P0, where t2 fails due to Bugs 1 and 3. After applying the
ï¬rst bug ï¬x to remove Bug 1, the initial test suite consisting of t1 and t2 does no more satisfy
branch coverage. This is due to the fact, that after the ï¬rst bug ï¬x is applied, the ï¬rst element
(i.e., the meta-information) of the input array is no more included into the search. As the last
element is also not included (due to Bug 2), execution of t2 will not enter the true-branch of
the if-statement in line 6, and is, therefore, unable to reach 100% branch coverage. Hence,
the developer/tester has to select a further test case, for instance:

â€¢ t6= (x=[3,0,1,0], y=0)

to cover the missing branch. Thus, the existing test case t2 becomes redundant and might be
removed from the test suite as t1 and t6 are suï¬ƒcient for 100% branch coverage.

More generally, TSR is concerned with selecting from an existing set of test cases a suï¬ƒcient,
yet preferably small number of test cases for a program under test. Corresponding strategies
for TSR have to address several challenges:

â€¢ Finding a minimal set of test cases from an existing test suite satisfying a given coverage

criterion is NP-hard, being reducible to the minimum set-cover problem [5].

â€¢ As illustrated by the running example, deï¬ning redundancy of test cases only with respect
to a code-coverage criterion might be misleading thus obstructing testing eï¬€ectiveness [7,
8].

A further challenge arises in the context of evolving programs. Let us next assume that bug
ï¬x 2 is applied to remove Bug 2. After that, the new test case t6 is no more able to reveal the
remaining Bug 3, as the last element of the array is equal to y and, therefore, the test output
(accidentally) satisï¬es the speciï¬cation. In contrast, the previously removed test case t2 would
have revealed Bug 3, since the last element is smaller than y, thus leading to the output of 3
instead of âˆ’2. This example shows that TSR solely based on structural coverage criteria deï¬ned
on a current program version might be problematic in case of evolving programs. Test cases
being redundant for a current program version might become relevant again after a program
In particular, regression testing 5 is concerned with selecting a suitable set of test
revision.
cases after program revisions [2, 9].

Regression Testing. As illustrated by the previous example, after revealing a bug due to
a failed test-case execution, a developer/tester consecutively creates program revisions to ï¬x
the bugs 6. After creating a program revision, the current test suite is re-executed to assure
modiï¬cations made during the revision (1) successfully ï¬x a discovered bug (i.e., test cases pre-
viously failing now pass) and (2) do not introduce new bugs (i.e., test cases previously passing
still pass). Those test cases from the existing test suite should be re-executed investigating
program parts aï¬€ected by modiï¬cations made during the revision. New test cases might be
required to investigate existing and/or newly introduced program parts not yet and/or no more
covered by existing test cases. In both cases, one distinguishes between modiï¬cation-traversing
test cases and modiï¬cation-revealing test cases [2]. Executions of modiï¬cation-traversing test
cases at least reach a program modiï¬cation, but may, however, not reveal the modiï¬cation to
a tester (i.e., the program versions before and after the revision may produce the same output

5The term regression testing often summarizes a wide range of diï¬€erent disciplines of testing evolving programs
not only including RTS, but also test-case prioritization, test-history analysis, test-artifact storage etc. We will
focus on to the core problem of selecting test cases for regression testing.

6For the sake of clarity, we assume in the following that the granularity of each individual modiï¬cation made
during a revision is limited to one particular line of code. However, this assumption does not possess any threat
to the validity of our approach.

RegreTS

Page 8 / 39

values for the same test inputs). In contrast, executions of modiï¬cation-revealing test cases not
only reach a program modiï¬cation, but also yield diï¬€erent output values when applied before
and after the revision.

For instance, recall the case in which the initial test suite consisting of t1 and t2 is applied
to P0. As t2 fails on P0, a developer might perform the ï¬rst bug ï¬x, leading to P1. Next assume
that t1 and t2 are simply reused for testing P1. Both test cases would pass now although the
second and third bug are still present in P1. This is because t1 is unable to reach the bug and t2
is unable to detect the bug as the last element which would reveal Bug 3 is not included in the
search due to Bug 2. The output yielded by t2, therefore, also conforms to the speciï¬cation.
In contrast, selecting t3 in addition to, or instead of, t2 would also reveal the second bug on
P1, thus enabling the developer to perform a further bug ï¬x leading to P2. However, after this
bug ï¬x, t3 becomes unable to detect the last bug as there are no values contained in x having
a smaller value than y. In contrast, t2 is now able to detect the last bug.

The previous example shows that eï¬€ectiveness of a test suite not only depends on the
particular test cases, but also on the current program version under test. Hence, fault-detection
probability of test cases may both decay as well as improve over time.

To generalize, RTS is concerned with selecting a suï¬ƒcient, yet preferably small number of
existing/new test cases to be (re-)executed on a revised program. Strategies for RTS have to
address several challenges:

â€¢ Finding a minimal modiï¬cation-traversing set of test cases from an existing test suite is
NP-hard and does, by deï¬nition, not guarantee eï¬€ective assurance of program revisions.

â€¢ Finding any modiï¬cation-revealing test case for a program modiï¬cation corresponds to

the program-equivalence problem which is undecidable.

â€¢ In a practical setting, the aforementioned idealized assumptions on program revisions do
usually not hold: units may contain several bugs which inï¬‚uence or even obfuscate each
other and not all modiï¬cations applied during a program revision are actually bug ï¬xes.

Test-Suite Reduction vs. Regression-Test Selection Both techniques aim at improving
testing eï¬ƒciency by reducing the number of test cases to be executed without presumably
harming eï¬€ectiveness:

â€¢ TSR strategies aim at selecting from an existing test suite of one program version a
presumably small subset being suï¬ƒcient to satisfy the given code-coverage criterion.

â€¢ RTS strategies aim at selecting from an existing test suite of previous program versions
and/or new test cases a presumably small subset being suï¬ƒcient to traverse/reveal critical
program modiï¬cations in the current program revision.

The previous examples illustrate the subtle interplay between both strategies aï¬€ecting eï¬ƒciency
and eï¬€ectiveness in non-obvious ways:

â€¢ Removing too few test cases during TSR might obstruct the gain in eï¬ƒciency of subse-
quent RTS as well as TSR steps due to critically increasing overhead required for those
strategies in case of ever-growing test suites.

â€¢ Removing too many test cases during TSR might obstruct eï¬€ectiveness of subsequent
RTS as well as TSR steps as currently redundant test cases might become relevant again.

We next present a novel evaluation framework to systematically investigate these interac-

tions.

RegreTS

Page 9 / 39

Figure 2: Overview of the Evaluation Methodology

3 Evaluation Methodology

We present a conï¬gurable framework for systematically addressing the challenges described
in the previous section. The methodology allows us to evaluate interactions between diï¬€erent
strategies for TSR and RTS with respect to practical eï¬ƒciency and eï¬€ectiveness measures. We
ï¬rst present a conceptual overview and then describe the provided parameters for adjusting
the strategies.

3.1 Overview

Figure 2 provides an overview of our methodology using the running example introduced in
the previous section.

Starting from the initial program version P0, the subsequent program versions, P1, P2,
P3, . . ., result from applying consecutive revisions, given as patches Patch1, Patch2, Patch3.
For each program version Pi, we consider a corresponding test suite Ti containing the test
cases selected for this particular program version. Based on the initial test suite T0 created
for P0 (e.g., using either a coverage criterion like branch coverage, or randomly generated test
cases, or other criteria), the regression test suites Ti for testing subsequent program versions
Pi, i > 0, result from applying a RTS strategy. As a complementing step, a TSR strategy may
be applied to test suites Ti of program versions Pi to remove redundant test cases.

Our methodology comprises strategies for both TSR as well as RTS. The parameters for
ï¬ne-tuning the strategies are denoted as circled numbers in Fig. 2. We brieï¬‚y describe these
parameters which will be explained in more detail below.

1â—‹ Reduction Strategy (RS): Technique used for TSR. Possible strategies: None, ILP,

Patch 1 Patch 2 Patch 3  ğ‘»ğŸ ğ’•ğŸ: ([0],0)  ğ’•ğŸ: ([3,5,5,3],4)  ğ‘»ğŸ ğ’•ğŸ: ([0],0)  ğ’•ğŸ: ([3,5,5,3],4) ğ’•ğŸ”: ([3,0,1,0],0)     ğ‘»â€²ğŸ ğ’•ğŸ”: ([3,0,1,0],0)    5 1  ğ‘»â€²â€²â€²ğŸ‘ ğ’•ğŸ–: ([2,0,0],1)  1 â‰  2 â‰  2  ğ‘»ğŸ‘ ğ’•ğŸ: ([0],0)  ğ’•ğŸ: ([3,5,5,3],4) ğ’•ğŸ”: ([3,0,1,0],0)  ğ’•ğŸ•: ([1,0],1) ğ’•ğŸ–: ([2,0,0],1)  ğ’•ğŸ—: ([2,1,1],2) ğ’•ğŸğŸ: ([3,2,1,0],3)   ğ‘»â€²â€²ğŸ‘ ğ’•ğŸ”: ([3,0,1,0],0)  ğ’•ğŸ•: ([1,0],1) ğ’•ğŸ–: ([2,0,0],1)  ğ’•ğŸ—: ([2,1,1],2) ğ’•ğŸğŸ: ([3,2,1,0],3 â‰  2  ğ‘»â€²ğŸ‘ ğ’•ğŸ•: ([1,0],1)   1 ğ‘·ğŸ ğ‘·ğŸ ğ‘·ğŸ ğ‘·ğŸ‘ 4 3 ğ‘»ğŸ ğ’•ğŸ: ([0],0)  ğ’•ğŸ: ([3,5,5,3],4)   â€¦  ğ‘»â€²ğŸ ğ’•ğŸ: ([3,5,5,3],4)   1 4 3 RegreTS

Page 10 / 39

FAST++, and DIFF.

2â—‹ Regression Test-Case Selection Criterion (RTC): New regression test cases for
a program version Pi may be added to test suite Ti either by means of modiï¬cation-
traversing test cases (i.e., at least reaching the lines of code modiï¬ed from Piâˆ’1 to Pi) or
by modiï¬cation-revealing test cases (i.e., yielding diï¬€erent outputs if applied to Pi and
Piâˆ’1).

3â—‹ Number of Regression Test Cases (NRT): The number of diï¬€erent regression test
cases added into Ti satisfying RTC for each previous program version Tj, 0 â‰¤ j < i.

4â—‹ Number of Previous Program Revisions (NPR): The (maximum) number of pre-
vious program versions Pj (i âˆ’ NPR â‰¤ j < i) for all of which NRT diï¬€erent test cases
satisfying RTC are added to Ti.

5â—‹ Continuous Reduction (CR): Controls whether the non-reduced test suite Tiâˆ’1 (No-
CR) or the reduced test suite T (cid:48)
iâˆ’1 (CR) of the previous program Piâˆ’1 version is (re-)used
for the next program version Pi or if the previous test cases are ignored (None). This
test suite is extended by new test cases according to the previously described parameters.

Note that parameter CR can be applied for all test suites of all versions. However, for clarity
and space reasons, it is only depicted from program version P2 on in Fig. 2.

3.2 Test-Suite Reduction Strategies

We now describe the diï¬€erent TSR strategies supported by our framework. We, again, use an
illustrative example to explain the impact of the strategies on the trade-oï¬€ between precision
and computational eï¬€ort. We ï¬rst give a general characterization of the test-suite minimization
problem for one single program P , a given test suite T and a code-coverage criterion on P [4].

Input: Program P , Test Suite T , where

â€¢ P contains of a set of test goals G = {g1, g2, . . . , gn} according to the given code-coverage

criterion and

â€¢ test suite T = t1, t2, . . . , tn consists of a set of test cases, where each tj âˆˆ T covers a
subset Gj âŠ† G of test goals on P such that for each gi âˆˆ G there exists at least one test
case tj âˆˆ T with gi âˆˆ Gj

7.

Output: Minimal Test Suite T (cid:48) âŠ† T , where

â€¢ for each gi âˆˆ G there exists at least one test case tj âˆˆ T (cid:48) with gi âˆˆ Gj and

â€¢ for each T (cid:48)(cid:48) âŠ† T also satisfying the ï¬rst property, it holds that |T (cid:48)| â‰¤ |T (cid:48)(cid:48)|.

The test-suite minimization problem is NP-hard being reducible to the minimum set cover
problem [4] such that ï¬nding exact minimal solutions is computational infeasible for realis-
tic programs. Various TSR heuristics have been proposed for approximating minimal test
suites constituting diï¬€erent trade-oï¬€s between precision (deviation from exact solutions) and
computational eï¬€ort for performing the reduction.

To illustrate the diï¬€erent approaches in our framework, P1 from our running example

together with T3 containing four test cases selected for branch coverage:

â€¢ t1= (x=[0], y=0),

7Set G actually contains the subsets of test goals covered by at least one test case in T .

RegreTS

Page 11 / 39

Figure 3: Comparision of Test-Suite Reduction Strategies

â€¢ t2= (x=[3,5,5,3], y=4),

â€¢ t3= (x=[1,1,1], y=2), and

â€¢ t4= (x=[1,2,2], y=0).

Figure 3 (on the left) illustrates the test-suite minimization problem. Program version P1
contains three conditional branches (i.e., if-statement in line 2, loop-head in line 6 and if-
statement in line 7), leading to six goals G = {g1, g2, . . . , g6}. A line between test case ti and
test goal gj indicates that gj âˆˆ Gi (i.e., ti satisï¬es gj). Test suite T3 indeed satisï¬es branch
coverage on P1 as for each g âˆˆ G, there is at least one test case in T3. However, T3 is not
minimal as, for instance, t2 and t4 cover exactly the same test goals, g2, g3, g4 and g6, and this
set is further subsumed by the test goals satisï¬ed by t3. In fact, t3 is the only test case covering
g5 and t1 is also indispensable as it is the only test case satisfying g1. Hence, T3 = {t1, t3} is
the (unique) minimal test suite as shown on the left.

Computing the minimal test suite requires, in the worst case, to enumerate all 2|T | subsets of
T which is infeasible in case of realistic programs. Practical approaches usually compute reduced
test suites approximating the minimal solution. We now describe the strategies considered in
our framework.

ILP Strategy. The test-suite minimization problem can be encoded as Integer linear op-
timization problem which can be precisely solved using Integer Linear Programming (ILP)
solvers [5]. Our ILP encoding uses for each test case ti âˆˆ T a decision variable xi either having
value 1 if ti is selected, or value 0 if ti is not selected. The ILP formula contains for each
test goal gj âˆˆ G a clause building the sum of the decision variables xi of all test cases ti âˆˆ T
for which gj âˆˆ Gi holds. By requiring the value of each such sum to be greater than 0, we
ensure each test goal to be covered by the minimal test suite. To enforce minimality, the overall
optimization objective is to minimize the sum over all values of decision variables xi.

Applied to our example, this encoding introduces the variables x1, x2, x3, x3 for the test

cases in T and adds the following clauses for the test goals:

x1 >= 1
x2 + x3 + x4 >= 1
x2 + x3 + x4 >= 1
x2 + x3 + x4 >= 1
x3 >= 1
x2 + x3 + x4 >= 1

(1)

(2)

(3)

(4)

(5)

(6)

(7)

Minimal ILP FAST++ DIFF Program Version ğ‘ƒ1 Test Goals ğ‘”1 ğ‘”2 ğ‘”3 ğ‘”4 ğ‘”5 ğ‘”6 Test Suite T ğ‘¡1 ğ‘¡2 ğ‘¡3 ğ‘¡4 ğ‘¡1 ğ‘¡1 ğ‘¡1 ğ‘¡1 ğ‘¡2 ğ‘¡3 ğ‘¡3 ğ‘¡3 ğ‘¡3 RegreTS

Page 12 / 39

Table 1: Vector Encoding of Test Cases
6
Test Case \ Value
0
(t1)
2
(t2)
0
(t3)
0
(t4)

1
0
0
3
1

0
2
0
0
1

5
0
1
0
0

2
0
0
1
2

3
0
2
0
0

Table 2: Vector Encoding of Test Cases after Random Projection

Test Case \ Value
(t1)
(t2)
(t3)
(t4)

x
0
0
0
0

y
0
0
3
2

z
0
5
2
0

The optimization objectives is deï¬ned as:

min(x1 + x2 + x3 + x4).

As illustrated in Fig.3, recent ILP solvers are able to deliver an exact minimal solution. How-
ever, large amount of test cases naturally lead to high computational eï¬€ort. We discuss two
TSR heuristics to avoid intractable computational eï¬€ort in case of larger-scaled problems, yet
providing an acceptable approximation of the exact (minimal) solution.

FAST++ Strategy. The FAST++ approach encodes the selection of test cases into a (re-
duced) test suite using the Euclidean distance of a reduced vector-space model computed by
random projection [10]. We also illustrate this approach using our example. Test cases are
encoded as vectors, where the number of elements corresponds to the number of diï¬€erent input
values of all test cases. The ï¬rst element of each vector denotes the number of occurrences of
the lowest input value (e.g., for our running example the number â€™0â€™). The next element of
each vector is the next lowest input value (e.g., for our running example the number â€™1â€™). The
encoding of all test cases is shown in Tab. 1 and the result of reducing the dimensions of the
vector-space model by random projection is shown in Tab. 2.

Based on this encoding, TSR works as follows. First, a random test case (e.g., t2) is selected
into the (initially empty) reduced test suite. For the selected test case, the Euclidean distances
to all other remaining test cases is computed based on the reduced vector-space model. The
probability of selecting a particular remaining test case next into the reduced test suite increases
with the distance value (thus preferring test cases covering test goals being dissimilar to those
covered by previously selected ones). In our example, this would be, for instance, test case t1.
This iterative step is repeated until all test goals are covered. For instance, the next test case
might be t3 which suï¬ƒces to achieve full branch coverage thus leading to termination. This
technique is, on average, more eï¬ƒcient than ILP, yet potentially leading to less precise results
as demonstrated by the example. There is not even a guarantee to ï¬nd local optima as random
projection may obfuscate necessary information. Additionally, the ordering of input values is
ignored.

DIFF Strategy. This strategy also incrementally selects test cases until all test goals are
covered [11]. In contrast to FAST++, DIFF is a purely greedy-based approach which always
selects as next test case one that covers a maximum number of uncovered test goals. Applied
to our example, DIFF would, for instance, perform the following selections:

1. t3 (covering 5 uncovered test goals),

RegreTS

Page 13 / 39

2. t1 (covering 1 uncovered test goal).

This technique is, in general, similarly eï¬ƒcient and (im-)precise as FAST++, where this par-
ticular example is well-suited for DIFF as the local optimum is the same as the global one.
However, if local and global optima diï¬€er, there is no guarantee about the optimality of the
result.

The diï¬€erent TSR technique might lead to diï¬€erent results in terms of the number of test
cases of the reduced test suite. Although, this clearly leads to diï¬€erent results in terms of
eï¬ƒciency (in terms of test-suite size) it also might aï¬€ect eï¬€ectiveness. For example, test case
t2 does not cover additional test goals and also does not detect any bugs in program version P1.
However, in program version P2 it will again detect Bug 3. We chose those three TSR strategies
as they provide diï¬€erent trade-oï¬€s between the eï¬€ectiveness of reduction (i.e., how small the
resulting test-suite will be) and eï¬ƒciency in terms of CPU time needed for reduction. [10,
12] Since ILP provides an optimal solution, the resulting test-suite will be minimal, however,
leading to much more computational eï¬€ort. The greedy approach (DIFF) is more eï¬ƒcient,
however, the resulting test-suite might be larger compared to ILP. FAST++ is even more
eï¬ƒcient compared to the greedy approach, however, leading to to even larger test-suites.

3.3 Regression-Test Selection Strategies

The TSR strategies considered so far are concerned with selecting from an existing test suite of
one single program version a reduced subset preserving test coverage on that program version.
In contrast, RTS strategies are concerned with selecting from an existing test suite of a previ-
ous program version a suï¬ƒcient set of test cases to investigate modiï¬cations applied to that
program version leading to a subsequent version. If no such test case(s) can be found among
the existing ones, new test cases must be created and added to the test suite of the subsequent
program version. We next describe diï¬€erent RTS strategies supported by our framework. We
use an illustrative example to explain the impact of the strategies on the trade-oï¬€s between pre-
cision and computational eï¬€ort. We start with a general characterization of the regression-test
selection problem, consisting of a program version P with existing test suite T and subsequent
version P (cid:48) for which a regression test suite T (cid:48) is created [2].

Input: Program version Piâˆ’1 with Test Suite Tiâˆ’1 and subsequent program version Pi.
Output: Regression Test Suite Ti = T (cid:48)
iâˆ’1 âˆª T (cid:48)
selected for reuse from Tiâˆ’1 and T (cid:48)

i are newly added test cases.

iâˆ’1 âŠ† Tiâˆ’1 are existing test cases

i , where T (cid:48)

Consider P3 from our running example together with test suite T3. Figure 4 provides an
illustration of the RTS problem. For P3, the selected test suite contains three modiï¬cation-
traversing test cases (i.e., t2, t3 and t4) being able to at least traverse the modiï¬ed line of
code. However, only t2 is modiï¬cation revealing as it not only reaches the modiï¬cation, but
also disposes a diï¬€erence in the output behavior of P3 as compared to P2. Hence, depending
on the RTS strategy, also new test cases have to be generated for T (cid:48)
i .

Regression-Test-Case Generation. We now describe how we automatically generate re-
gression test cases utilizing oï¬€-the-shelf test-generation tools originally designed for covering
test goals encoded as reachability properties. We utilize a software model-checker for C pro-
grams to automatically derive test cases from counterexamples for (non-) reachability queries
of test goals [13]. We encode the problem of generating regression test cases for diï¬€erent ver-
sions of a program in a generic comparator-function. The shape of the comparator-function
Pi,j for the program versions Pi and Pj depends on parameter RTC. We illustrate this using
the comparator-function P2,3 for our running example.

RegreTS

Page 14 / 39

Figure 4: Comparison of Regression-Test Selection Strategies

1
2
3
4
5
6
7
8
9
10
11
12

int find_last_p2_3 ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;

int last = -2;
for ( int i =1; i <= x [0] -1; i ++) {

test_goal :
if ( x [ i ] == y )
last = i ;

}
return last ;

}

(a) Modiï¬cation-Traversing Comparator-Function for P2 and P3

void find_last_p2_3 ( int x [] , int y ) {

if ( find_last_p2 (x , y ) != find_last_p3 (x , y )
test_goal : printf ( " differencing (cid:32) test (cid:32) case (cid:32) found " ) ;

}

(b) Modiï¬cation-Revealing Comparator-Function for P2 and P3

Figure 5: Comparator-Functions

Program Version ğ‘ƒ3 Test Suite ğ‘‡2 ğ‘¡1 ğ‘¡2 ğ‘¡4 ğ‘¡5 ğ‘”1 ğ‘”2 ğ‘”3 ğ‘”4 ğ‘”5 ğ‘”6 Test Goals ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ Modification Traversing ğ‘¡1 ğ‘¡2 ğ‘¡4 ğ‘¡5 Modification Revealing ğ‘¡1 ğ‘¡2 ğ‘¡4 ğ‘¡5 Program Version ğ‘ƒ2 Bug Fix B3 Test Suite ğ‘‡2 ğ‘¡1 ğ‘¡2 ğ‘¡4 ğ‘¡5 ğ‘”1 ğ‘”2 ğ‘”3 ğ‘”4 ğ‘”5 ğ‘”6 Test Goals ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ X X X X RegreTS

Page 15 / 39

1
2
3
4
5
6
7
8

int find_last_p3 ( int x [] , int y ) {...}
struct intStruct find_last_p4 ( int x [] , int y ) {...}

void find_last_p3_4 ( int x [] , int y ) {

if ( find_last_p3 (x , y ) != find_last_p4 (x , y )
test_goal : printf ( " differencing (cid:32) test (cid:32) case (cid:32) found " ) ;

}

Figure 6: Invalid Comparator Program

â€¢ RTC = MT: The comparator-function P2,3 for generating modiï¬cation-traversing test
cases between P2 and P3 is shown in Figure 5a. The program location aï¬€ected by the
program modiï¬cation leading from P2 to P3 is marked by a special program label test goal.
A test-case generator can be used to generate a program input reaching this program label
(and therefore at least traversing this modiï¬cation).

â€¢ RTC = MR: The comparator-program P2,3 for generating modiï¬cation-revealing test
cases between P2 and P3 is shown in Fig. 5b. Here, the program input is delegated to
P2 and P3 and the output is compared by an equals-function (or != in case of basic
types) based on the return-type. Hence, the special program label test goal in line 3 is
only reachable by a test case if the test input generated by a test-case generator yields
diï¬€erent output values for P2 and P3 (thus not only reaching, but also revealing this
modiï¬cation).

Remarks. The problem of generating modiï¬cation-revealing test cases obviously subsumes
the generation of modiï¬cation-traversing test cases and therefore, presumably, causes (much)
more computational eï¬€ort. In fact, both problems are undecidable as reachability of program
locations is reducible to the halting problem. Hence, the test-generation approach described
above is inherently incomplete 8. Our framework further comprises the parameter NRT to
control the number of multiple diï¬€erent test cases distinguishing the output behavior of two
consecutive program versions (which may, however, fail due to the theoretical limitations de-
scribed before). Lastly, the technique based on the comparator-function suï¬€ers from some
technical limitations. For example, if the data type of the return value changes the compar-
ison of the return values is, in general, no more possible (see Fig. 6, where the return types
of function f ind last p3 and f ind last p4 diï¬€er such that the comparator-program would not
compile). Additionally, as both program versions need to be compiled into one single program,
merging of those versions can become technical challenging (e.g., if the data type of global
variables or functions changes or the members of structs are modiï¬ed).

Multiple Regression Test-Case Generation. Most test-case generators transform the
input program into an intermediate representation like a control-ï¬‚ow automaton (CFA). To
keep the following examples graspable, we use the basic example shown in Figure 7a instead of
our running example. This CFA corresponds to a small program with input value x returning
0 if x is smaller than zero and the value of x, otherwise. A CFA can be used to identify
all program paths leading to test goals (e.g., all CFA-edges in case of branch coverage) and
to control automated test-suite generation by performing reachability queries for uncovered
test goal [14]. To support parameter NRT, we have to generate multiple diï¬€erent test cases
reaching the same test goal (i.e., a modiï¬ed program location). We start by generating the
ï¬rst test case based on the original CFA. For instance, to generate a test case reaching the

8This fact, however, is already true for the problem of generating a test case simply reaching a test goal in

one version of a program in a non-regression scenario.

RegreTS

Page 16 / 39

(a) Original CFA

(b) Instrumented CFA

Figure 7: On-the-ï¬‚y CFA Instrumentation

return-edge in Fig. 7a, any input is feasible (e.g., input -5 would traverse the left path). For
further test cases reaching the return-edge, we modify the CFA as shown in Fig. 7b to exclude
already covered paths. For this, we add for each path for which we already created a test case
a fresh variable with initial value 0 (i.e., variable w1 introduced by the new edge from node 2
to e1 ). We assign value 1 to this variable after traversing the associated branch of the path
(i.e., the new edge from node 4 to e2 ). By further requiring the value of at least one of the
fresh variables before reaching the test goal must not be 1 (i.e., the new edge from node 6 to
e3 ), we exclude all previous test cases reaching that test goal. In the example, assumption
w1!=1 enforces a further test case to assign a positive value to x, whereas a third run of the
test-case generator will ï¬nd no further path reaching this goal.

For example, to generate two test cases reaching line 7 in program version P3, we ï¬rst
generate a test case as usual (e.g., t7= (x=[1,0], y=1)). For the second test case, we introduce
a fresh variable into the CFA which is incremented when traversing a branch of the path already
taken by the previous test case (i.e., the true-branch of the if-statement, the true-branch of
the for-loop, the false-branch of the for-loop and the false-branch of the last if-statement).
As there are four branches, the ï¬nal check will ascertain that the fresh variable is not equal
to 4. When iterating through the for-loop more than once (as opposed to the ï¬rst test case)
or taking the true-branch of the last if-statement, the value will diï¬€er from 4 and, therefore,
the path can be taken for a further test case being diï¬€erent from all previous ones (e.g., t8=
(x=[2,0,0], y=1)).

Remarks. Again, this methodology is inherently incomplete as it is unknown whether further
test cases exist if no more test cases are found by the test-case generator (e.g., due to time-outs).
In addition, each incrementally added fresh variable presumably increases program complexity
thus potentially increasing the computational eï¬€ort required for every further test case.

Regression Test-Suite Generation. We conclude by summarizing the description of our
framework. Algorithm 1 depicts a generic procedure for generating regression test suites Ti for
program version Pi meeting all possible instantiations of the parameters RTC, NRT, NPR,
RS and CR. Parameter RTC is either set to modiï¬cation-traversing (MT) or modiï¬cation-
revealing (MR), whereas NRT and NPR can be any number greater than zero (NPR is
naturally restricted to be at most i). Besides the current version Pi, the algorithm receives
as inputs the sequence of patches applied since version P0 as well as the test suite Tiâˆ’1 of the
preceding version Piâˆ’1 based on parameter CR. Lastly, RS is set to ILP, DIFF, FAST++

1 2 4 3 5 ğ‘¥<0 !(ğ‘¥<0) ğ‘–ğ‘›ğ‘¡ ğ‘¥ ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘› a ğ‘–ğ‘›ğ‘¡ ğ‘ 6 7 ğ‘= 0 ğ‘=x 1 2 3 4 5 ğ‘¥<0 !(ğ‘¥<0) ğ‘–ğ‘›ğ‘¡ ğ‘¥ ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘› a ğ‘–ğ‘›ğ‘¡ ğ‘ e3 ğ‘=0 ğ‘=x 6 e2 ğ’˜ğŸ=ğŸ e1 7 ğ’Šğ’ğ’• ğ’˜ğŸ=ğŸ w1!=1 RegreTS

Page 17 / 39

Algorithm 1 Regression Test-Suite Generation
Input: RTC âˆˆ {MT, MR}, NRT âˆˆ N+, NPR âˆˆ {1, . . . , i}
Input: Pi, Patch1, . . . , Patchi
Input: Tiâˆ’1
Output: Ti

iâˆ’k to Pj

Pj â† apply Patchâˆ’1
Pi,j â† Comparator(cid:104)Pi, Pj(cid:105)(RTC)
l â† 0
while l < NRT do

t â† NewTestGen(Pi,j, Ti,j)
if t = null then

1: procedureMain
2: Ti â† Tiâˆ’1(CR)
3: Ti,j â† âˆ…
4: Pj â† Pi
5: for k = 0 to NPR âˆ’ 1 do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: Ti â† Ti âˆª Ti,j
19: Ti â† Reduction(cid:104)Ti(cid:105)(RS)

end if
Ti,j â† Ti,j âˆª {t}
l â† l + 1

end while

continue in line 5

or None.

j

Test suite Ti is initialized with the test cases from the existing test suite Tiâˆ’1 (line 2), and
Pj (initialized in line 4 by Pi) refers to the previous version for which diï¬€erentiating test cases
are generated in the next iteration. The outer loop from line 5 to 15 performs a descending
traversal over NPR previous versions by reversely applying the corresponding patches to Pj
(i.e., applying Patchâˆ’1
to Pj yields version Pjâˆ’1). In line 7, the comparator-function template
is instantiated with Pi and Pj to synthesize a comparator -function Pi,j as input for a coverage-
based test-case generator in line 10 [13]. The inner loop (lines 9â€“15) repeatedly generates test
cases for Pi,j until at most NRT diï¬€erent test cases have been generated (cf. Sect. 3.3). The
test-case generator receives as additional input the set Ti,j of already generated test cases.
If the test-case generator fails to ï¬nd further test cases (either due to a time-out or due to
exhaustive search), the current iteration of the inner loop is terminated and the next iteration
of the outer loop starts (lines 11â€“12). Otherwise, the new test case is added to Ti,j (line 14)
and ï¬nally to Ti (line 18). Finally, the test-suite is reduced depending on parameter RS in
line 19.

Remarks. For generating multiple diï¬€erent test cases for the same modiï¬cation, only the
set Ti,j of already generated diï¬€erentiating test cases for the current pair Pi, Pj of program
versions is taken into account. However, the approach may be generalized by blocking the
whole set Ti of already generated test cases in subsequent runs of the test-case generator. This
may, however, lead to drastically increased computational eï¬€ort and is, therefore, currently not
supported.

3.4 Integration and Discussion of Strategies

The parameters RS, RTC, NRT, NPR and CR allow for adjustments of eï¬ƒciency and
eï¬€ectiveness achievable by a regression-testing strategy concerning the detection of regression
bugs.

RegreTS

Page 18 / 39

Eï¬ƒciency. Eï¬ƒciency of regression-testing strategies may be measured in terms of (1) the
computational eï¬€ort (e.g., CPU time) for selecting/generating regression test cases, together
with (2) the number of regression test cases being selected. Concerning (1), strategies with
RTC = MT are presumably more eï¬ƒcient than those with RTC = MR as ï¬nding modiï¬cation-
revealing test cases is, on average, computationally more complicated than only reaching a
modiï¬ed program location. Concerning (2), the number of regression test cases to be selected
for each new program version is (at most) NRT Â· NPR thus presumably growing with increas-
ing values of NRT and NPR. Additionally, strategies with RS (cid:54)= None are presumably less
eï¬ƒcient in terms of CPU time, but presumably much more eï¬ƒcient in terms of the number of
test cases. Finally, CR also presumably decreases eï¬ƒciency in terms of CPU time, however
presumably increases eï¬ƒciency in terms of number of test cases.

Eï¬€ectiveness. Eï¬€ectiveness of regression-testing strategies may be measured in terms of the
number of regression bugs detected by the selected regression test cases. Test cases selected
for RTC = MR are presumably more eï¬€ective than those for RTC = MT. Similarly, fault-
detection rates of regression test suites presumably increase with higher values for NRT (i.e.,
leading to more diï¬€erent ways of testing modiï¬cations between program versions) as well as
NPR (i.e., taking more previous program versions into account). For instance, concerning
the revision from P2 to P3 in our example, revealing bug 3 in P2 requires a test case such as
t2. This can be ensured by strategies with RTC = MR, NRT > 0, and NPR > 0 which
are, however, presumably less eï¬ƒcient than strategies with RTC = MT. Finally, RS and
CR might decrease eï¬€ectiveness as reducing the test suite might remove test cases that would
succeed in ï¬nding the bug (see Sect. 3.2).

In the next section, we empirically investigate these assumptions by presenting the results
of an experimental evaluation gained from applying a tool for our methodology to a collection
of subject systems.

4 Experimental Evaluation

The conï¬gurable framework presented in the previous section allows us to investigate the impact
of the parameters 1â—‹â€“ 5â—‹ (see Sect. 3) on eï¬ƒciency and eï¬€ectiveness of regression testing. In
our experimental evaluation, we consider version histories of C program units (i.e., function
level). We use real-world systems obtained from GitHub and the change history provided by
git. To systematically compare eï¬€ectiveness of diï¬€erent regression-testing strategies, we further
employ simulated bugs throughout program-version histories and measure the corresponding
fault-detection ratio of the selected test suites. We do this by repeatedly applying standard
mutation operators for C programs. Correspondingly, we measure eï¬ƒciency in terms of the
computational eï¬€ort for generating regression test cases as well as in terms of the number of
test cases to be (re-)executed throughout the version history.

4.1 Research Questions

We consider the following research questions.

(RQ1) How does the regression-testing strategy impact testing eï¬€ectiveness?

(RQ2) How does the regression-testing strategy impact testing eï¬ƒciency?

(RQ3) Which regression-testing strategy constitutes the best trade-oï¬€ between eï¬€ectiveness and

eï¬ƒciency?

RegreTS

Page 19 / 39

4.2 Experimental Setup

We next describe the evaluation methods and experimental design used in our experiments to
address these research questions.

Methods and Experimental Design To compare diï¬€erent strategies for regression-testing,
we instantiate the ï¬ve parameters RTC, NRT, NPR, RS and CR of our methodology as
described in Sect. 3 as follows:

â€¢ RTC âˆˆ {MT, MR} (RTS Generation Criterion).

â€¢ NRT âˆˆ {1, 2, 3} (Number of Regression Test Cases per Revision).

â€¢ NPR âˆˆ {1, 2, 3} (Number of Previous Program Revisions).

â€¢ RS âˆˆ {None, ILP, FAST++, DIFF} (TSR Strategy).

â€¢ CR âˆˆ {CR, No-CR, None} (Continuous Reduction).

To answer RQ1-RQ3, each instantiation of all ï¬ve parameters, therefore, corresponds to
one particular regression-testing strategy under consideration, where we denote a strategy
by [RTC, NRT, NPR, RS, CR] for short. We thus obtain 144 reasonable regression-testing
strategies (since the combination of RS = None and CR = CR is meaningless and we do
not take into account strategies having CR = None and RS! = None as reduction of non-
accumulated test-suites is also meaningless). Two of these strategies may be considered as
proper baselines:

â€¢ Baseline 1 (basic regression testing strategy):[MT, 1, 1, None, No-CR].

â€¢ Baseline 2 (basic regression testing without initial test suite):[MT, 1, 1, None, None].

We limit our considerations to 3 as maximum value for NRT and NPR as the increase of
eï¬€ectiveness diminishes for higher values of NRT and NPR. Additionally, we do not consider
0 as value for NRT and NPR, as this would obviously result in an empty test-suite.

We further divide RQ2 into the following sub-questions.

â€¢ (RQ2.1) How does the regression-testing strategy impact eï¬ƒciency in terms of CPU

time?

â€¢ (RQ2.2) How does the regression-testing strategy impact eï¬ƒciency in terms of number

of test-cases?

Note that we consider test-case generation time for CPU time and not test-case execution time.
This is due to the fact, that test-case execution time was negligible during our evaluation.

Subject Systems. We consider program units as our subject systems in terms of testable
functions extracted from real-world C programs for which we further require available version
history. We therefore focus on open-source projects from GitHub. We selected program units
in terms of preprocessed c-ï¬les consisting of an entry-function (i.e., the function-under-test)
as well as all functions within the same compilation unit having (direct or indirect) callee-
dependencies to the function-under-test. To sum up, our subject systems have to fulï¬ll the
following requirements to be useable for our evaluation purposes.

â€¢ The unit must be processable by the test-case generator used in our framework (e.g.,

syntax must be ANSI C, no multi-threading, etc.).

RegreTS

Page 20 / 39

â€¢ The functions-under-test (as well as all callees) must have undergone a version-history of

at least 3 modiï¬cations.

â€¢ The signature of the functions-under-test must provide input parameters for which dif-
ferent values will yield diï¬€erent return values (or aï¬€ect the values of at least one global
variable). However, this needs not to be known in advance as the existence of such param-
eters is actually checked during test generation (which may timeout if this requirement
is not met).

â€¢ Calling the function-under-test multiple times with the same parameter values produces
the same return value or global-variable values (i.e., no non-deterministic behavior or
external system-function calls are considered).

The resulting collection of subject systems comprises program units from open-source

projects published in GitHub:

â€¢ betaï¬‚ight9 contains six program units from the ï¬‚ight controller software betaï¬‚ight.

â€¢ netdata10 contains six program units from the infrastructure monitoring and trou-

bleshooting software netdata.

â€¢ wrk11 contains one program unit from the HTTP benchmarking tool wrk.

The size of subject systems ranges from 270 to 950 lines of code (after removing unnecessary
code). The number of changes (i.e., commits) for each subject system is between 4 and 18
(only counting commits changing code inside the unit).

Simulating Bugs. Although software evolution and regression testing become more and
more important, properly documented and practically usable histories of program versions
combined with information about real bugs in software projects are still barely available. Hence,
for in-depth investigations of the interactions between program revisions, program bugs and
their detection by regression test cases as required for answering our research questions, we
have to rely on synthetically generated bugs. Mutation testing is a well-established approach
for evaluating eï¬€ectiveness of testing techniques by simulating common program faults [15].
In particular, mutation-testing tools provide collections of syntactic program-transformation
operations such that the resulting modiï¬ed program potentially shows diï¬€erent (i.e., erroneous)
output behaviors.

Fortunately, it has been recently shown that mutations provide a reliable substitute for
realistic bugs in testing experiments [15]. We, therefore, utilize concepts from mutation testing
to simulate bugs in our subject systems. Figure 8 provides an overview of our approach. For
each program version other than program version P0, we create mutants containing simulated
bugs. The regression-test generation is then executed on this bugged version and eï¬€ectiveness
of the resulting test suite is measured by executing the generated/selected regression-test suite
on both the bugged and the bug-ï¬xed (i.e., the original) version and by comparing the return
values (see Sect. 3). If the return values diï¬€er, the bug is successfully detected by the test-suite.
We used 62 mutation operators in total (the names of the mutation operators are provided on
our supplementary web page 12), which can be clustered into three diï¬€erent types. The ï¬rst
group consists of mutation operators replacing constants and variables (e.g., replace variable a
by variable b, replace constant 5 by constant 6 etc.). The second group consists of mutation
operators replacing operators (e.g., replacing + by âˆ’). The last group consists of mutation
operators replacing pointers and array references (e.g., replacing pointer pt1 by pointer pt2).

9https://github.com/betaï¬‚ight/betaï¬‚ight
10https://github.com/netdata/netdata
11https://github.com/wg/wrk
12https://www.es.tu-darmstadt.de/regrets

RegreTS

Page 21 / 39

Figure 8: Bug Simulation using Mutation Testing

Data Collection. For comparing diï¬€erent regression-testing strategies, we ï¬rst generate a
faulty revisions Bi from all revisions Pi of each subject system by simulating bugs as described
before. Next, we introduce program labels into each faulty revision Bi marking each line of
code which has been modiï¬ed since the last version Piâˆ’1. These labels are used as test goals to
generate modiï¬cation-traversing test cases (similarly for Piâˆ’2 and Piâˆ’3 depending on parameter
NPR). In addition, we generate the comparator -programs described in Sect. 3 for generating
modiï¬cation-revealing test-cases. The comparator -program is further used for comparing test-
case executions on the faulty version Bi and the previous versions Piâˆ’1, Piâˆ’2 and Piâˆ’3. If no
comparator -program can be created (e.g., due to the limitations explained in Sect. 3), no test
suite will be generated for all strategies targeting this speciï¬c comparator-program.

We applied Algorithm 1 to generate regression-test suites T s
i

for every bugged revision Bi

using all 144 strategies.

To answer RQ1, we measure eï¬€ectiveness of the generated test suites such that test suite
T s
i detects the bug in Bi if it contains at least one test case t whose execution yields diï¬€erent
output values when executed on Pi and Bi (denoted Pi(tc) (cid:54)= Bi(tc)). Hence, eï¬€ectiveness with
respect to a test suite T s
i

is calculated as

where detects(T s
generated for Pi. Eï¬€ectiveness of a strategy s is calculated as

i , i) = 1 if âˆƒt âˆˆ T s
i

eï¬€ectiveness(T s

(8)
: Pi(t) (cid:54)= Bi(t), or 0 otherwise, and Bi is the faulty revisions

i ) = detects(T s

i , i)

where n is the number of revisions.

eï¬€ectiveness(s) =

(cid:80)n

i=0 eï¬€ectiveness(T s
i )
n

(9)

To answer RQ2, we measure eï¬ƒciency in two ways. First, eï¬ƒciency of regression test suites
generated by strategy s for program unit Pi is calculated as the average size of test suites T s
i
in terms of the number of test cases

i ) = |T s
i |
such that the overall eï¬ƒciency of strategy s is calculated as

eï¬ƒciencysize(T s

eï¬ƒciencysize(s) =

(cid:80)n

i=0 eï¬ƒciencysize(T s
i )
n

.

(10)

(11)

Second, eï¬ƒciency of the regression test-suite generation is given as the aggregated CPU time
eï¬ƒciencyCPU consisting of the initialization phase of the software model-checker, the reacha-
bility analysis, and the test-case extraction. While the ï¬rst phase is only executed once for
each program revision under test, the other phases are executed by the number of test goals
multiplied by parameter NRT. Additionally, for practical reasons, the whole process is limited
by a global time-out parameter after which the process terminates without providing further
test cases thus potentially leading to less test cases than speciï¬ed by parameter NRT. The

ğ‘·ğŸ Patch 1 Patch 2 Patch 3 ğ‘·ğŸ ğ‘©ğŸ Bug 2 ğ‘·ğŸ ğ‘©ğŸ Bug 1 ğ‘©ğŸ‘ Bug 3 ğ‘·ğŸ‘ â€¦ RegreTS

Page 22 / 39

equation for calculating eï¬ƒciencyCPU of the diï¬€erent strategies is
i=0 eï¬ƒciencycpu(T s
i )
n

eï¬ƒciencycpu(s) =

(cid:80)n

.

(12)

Finally, to answer RQ3, we calculate the mean values of each strategy for each program unit
in our subject system for eï¬€ectiveness and eï¬ƒciency and, thereupon, calculate the trade-oï¬€ as

trade-oï¬€(s) =

eï¬€ectiveness(s)
eï¬ƒciencysize(s)

.

(13)

Tool Support. We implemented Algorithm 1 in a tool, called RegreTS (Regression Testing
Strategies). RegreTS extends the software model-checker CPAchecker for C programs to
generate regression test cases. This is achieved by performing reachability-analysis runs for
program locations marked as test goals (see Sect. 3). Additionally, we use MUSIC [16], a
mutation tool for C programs, to generate synthetic bugs as described above. For measuring
test coverage, we utilize TestCov13, a test-suite executor for C programs. TestCov further
supports TSR with a greedy algorithm, as explained in Sect. 3, called DIFF. Furthermore,
we extended TestCov to also support FAST++ and ILP. All tools are available on our
website14.

Measurement Setup. RegreTS as well as MUSIC and TestCov have been executed on
an Ubuntu 18.04 machine, equipped with an Intel Core i7-7700k CPU and 64 GB of RAM.
The CPU time for test-suite generation is limited to 900 s per revision, and the CPU time for
the execution of the test cases to detect bugs is limited to 30 s per test case. The TSR is not
limited by CPU time as the CPU time needed for reduction was negligible. We executed our
evaluation with Java 1.8.0-171 and limited the Java heap to 15 GB.

4.3 Results

The measurements for RQ1 are depicted in Fig. 9a for all strategies with RTC = MR and in
Fig. 9b for all strategies with RTC = MT. Results for RQ2 are shown in Figs. 10a and 11a
for all strategies with RTC = MR and Figs. 10b and 11b for all strategies with RTC = MT.
The results for RQ3 are shown in Fig. 12a for all strategies with RTC = MR and Fig. 12b for
all strategies with RTC = MT. The box plots in Figs. 10a, 10b, 11a and 11b aggregate the
results after applying the formulas 11 and 12 (see above) for all subject systems. The boxes
depict the range of results while the black dashes depict the mean values. The whiskers show
the minimum and maximum values (excluding outliers).

RQ1 (Eï¬€ectiveness). The best eï¬€ectiveness for our subject systems is achieved by the
strategies [MR, 3, 3, None, No-CR], [MR, 2, 3, None, No-CR], [MR, 2, 3, None,
No-CR], [MR, 3, 3, FAST++, CR] and [MR, 3, 3, FAST++, No-CR] with an average bug
detection rate of 0.3659. Compared to the baseline 1 [MT, 1, 1, None, No-CR] with an average
detection rate of 0.306 and baseline 2 [MT, 1, 1, None, None] with an average detection rate of
0.224. The worst performing strategy is [MT, 3, 3, FAST++, CR] with an average detection
rate of 0.119.

RQ2 (Eï¬ƒciency). The best eï¬ƒciency measure concerning CPU time is obtained by strat-
egy [MT, 1, 1, None, No-CR] and [MT, 1, 1, None, None] with an average amount of 6.058s.
The worst performing strategy, [MR, 3, 3, ILP, No-CR], requires 993.229s on average. Con-
cerning test-suite sizes, [MT, 1, 1, ILP, CR], [MT, 2, 1, ILP, CR], and [MT, 3, 1, ILP, CR]

13https://gitlab.com/sosy-lab/software/test-suite-validator
14https://www.es.tu-darmstadt.de/regrets

RegreTS

Page 23 / 39

perform best with an average measure of 0.225 test cases, whereas strategy [MT, 3, 3, None,
No-CR] leads to the largest test-suite sizes with an average measure of 115.44 test cases.

RQ3 (Trade-Oï¬€ ). The base trade-oï¬€ between eï¬€ectiveness and CPU time is obtained by
strategy [MT, 1, 1, None, No-CR] with an average of 0.0503 bugs found per second. The
worst performing strategy is [MT, 3, 3, FAST++, CR] with an average of 0.0015 bugs found
per second.

4.4 Discussion and Summary

RQ1 (Eï¬€ectiveness). Setting parameter RTC to MR increases eï¬€ectiveness for all strate-
gies. In fact, almost all strategies using MR are more eï¬€ective than even the most eï¬€ective
strategy using MT. Indeed, parameter RTC has the highest impact on eï¬€ectiveness. While
the impact of other parameters is smaller, it is nonetheless also observable. Choosing None
for parameter RS increases eï¬€ectiveness, as the other parameter value reduces eï¬€ectiveness by
reducing the number of test cases. Choosing CR or No-CR for parameter CR has nearly no
impact on eï¬€ectiveness. Strategies with MR and ILP cause a signiï¬cant loss in eï¬€ectiveness
if CR is selected. Choosing None for parameter CR has a negative impact on strategies with
MR selected, however, only having a small impact if MT is selected. Lastly, parameters NRT
and NPR also increase eï¬€ectiveness with increasing values (even in case of small increases).

Answer RQ1
The best eï¬€ectiveness measure is reached by strategy [MR, 3, 3, None, No-CR] (i.e., mod-
iï¬cation revealing test cases, three test cases per test goal, up to three previous revi-
sions and no test-suite reduction) which improves eï¬€ectiveness compared to the baseline
[MT, 1, 1, None, No-CR] by 19% and compared to the baseline [MT, 1, 1, None, None]
by 61%. The highest impact on eï¬€ectiveness is caused by parameter RTC

RQ2.1 (CPU Time). Parameter RTC has by far the highest impact on CPU time. When
choosing MT, CPU time increases nearly 20 fold. As expected, parameter NPR increases
the CPU time nearly linearly to its respective value. Unexpectedly, parameter NRT does
not aï¬€ect CPU time by a large margin. This is most likely due to the fact, that reachability
information computed during the ï¬rst run of the test generator can be re-used for the next test
cases. Parameters CR and RS are negligible in terms of CPU time if MR is selected, since
the number of test-cases remains small. If MT is selected, both parameters impact CPU time,
however still only by a small margin.

Answer RQ2.1
The best eï¬ƒciency measure in terms of CPU time is reached by the baseline strategies
[MT, 1, 1, None, No-CR] and [MT, 1, 1, None, None] (i.e., modiï¬cation traversing test
cases, one test case per test goal, one previous revision and no test-suite reduction, either
ignoring or using the previous test-suite (as this makes no diï¬€erent in CPU time)). The
highest impact on CPU time is caused by parameter RTC.

RQ2.2 (Test-Suite Size). The highest impact on eï¬ƒciency in terms of test-suite size is
caused by parameter RS.
If None is selected, no reduction is enabled, and therefore, the
test-suite grows with each version. However, the choice of the technique used for test-suite
reduction only has a small impact on the test-suite size.

Parameter RTC also has a high impact on the test-suite size. This is due to the fact, that
a patch might contain multiple modiï¬cations. Therefore, the number of test cases is higher in
case of MT (i.e., requiring one test-case per modiï¬ed line) as compared to MR, where only one
test case is required. Parameter CR also aï¬€ects test-suite sizes, but only by a small amount.

RegreTS

Page 24 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 9: Results Eï¬€ectiveness

Strategy0.000.050.100.150.200.250.300.35Strategy0.000.050.100.150.200.250.30RegreTS

Page 25 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 10: Results Generation Time

Strategy05001,0001,5002,0002,500Strategy020406080100120RegreTS

Page 26 / 39

(a) Strategies with RTC = MR

Figure 11: Results Test-Suite Size

(b) Strategies with RTC = MT

Test-Suite SizeStrategy05101520253035404550Strategy020406080100120140160180200RegreTS

Page 27 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 12: Results Bugs per Second

Strategy0.00000.00020.00040.00060.00080.00100.0012Strategy0.0000.0050.0100.0150.0200.0250.0300.0350.0400.0450.050RegreTS

Page 28 / 39

Lastly, parameters NRT and NPR aï¬€ect the test-suite size almost linearly to their respective
values.

Answer RQ2.2
The best eï¬ƒciency measure in terms of test-suite size is reached by the strategy
[MR, 1, 1, ILP, CR] (i.e., modiï¬cation revealing test cases, one test case per test goal,
one previous revision, ILP as test-suite reduction strategy and using the reduced test-
suite of the previous revision) which improves eï¬€ectiveness compared to the baseline
[MT, 1, 1, None, No-CR] by 6200% and compared to the baseline [MT, 1, 1, None, None]
by 970%. The highest impact on the test-suite size is caused by parameter RS.

RQ3 (Trade-Oï¬€ ). We observe that all parameters have a large impact on the trade-oï¬€ be-
tween eï¬€ectiveness and CPU time. Therefore, the interactions between the diï¬€erent parameters
is the main driver aï¬€ecting the trade-oï¬€.

Answer RQ3
The strategy yielding the best trade-oï¬€ is [MT, 1, 1, None, No-CR] (i.e., modiï¬cation
traversing test cases, one test case per test goal, one previous revisions, no test-suite reduc-
tion and using the test-suite of the previous revision as well) for which the fault-detection
capability is acceptable, but the eï¬ƒciency in terms of CPU time is very high. Compared to
the second baseline [MT, 1, 1, None, None] the trade-oï¬€ is increased by 36%.

Remarks. Strategy [MT, 3, 3, None, No-CR] is less eï¬€ective than strategy [MT, 3, 2,
None, No-CR] which, by deï¬nition, is counter-intuitive. This is due to the fact, that some
comparator -programs for comparing a faulty version Bi to a program revision Piâˆ’2 are invalid
(e.g., due to diï¬€erent return types), whereas the comparator -program for Bi and program
revisions Piâˆ’3 are actually valid. However, no test cases could be found by the test-case gen-
erator revealing the bug. Hence, the number of test suites for [MT, 3, 3, None, No-CR] and
[MT, 3, 2, None, No-CR] diï¬€er slightly, which is the reason for the discrepancy between the
factual results and the theoretical speciï¬cation of the technique. In our subject systems, the
probability that no valid comparator -program can be generated is approximately 8%. There-
fore, while this technical limitation is present, it should not aï¬€ect the results by a large margin.

4.5 Threats to Validity

Internal Validity. Our regression-testing methodology relies on the assumption that when
diï¬€erent program versions are tested with the same test case, all factors (e.g., platform, en-
vironment) that might inï¬‚uence the output except for the source code itself remain constant.
This so-called controlled-regression-testing assumption is commonly used in regression-testing
experiments and does, therefore, not harm validity of the results [2].

Concerning the soundness of our methodology, we tested the test-generation loop by manu-
ally checking results for selected subject systems. However, due to undecidability of reachability
of program locations, if no more test cases can be found (e.g., due to time-outs or imprecise
counter-examples), it is unknown if further test cases exist. Nevertheless, we expect precision
improvements to not substantially obstruct the (relative) results of our evaluation.

Another threat to validity might arise from our selection of mutation operators and their
applications to our subject systems. However, our selection comprises those mutations leading
to useful results w.r.t. our experimental setting, namely aï¬€ecting one line of code, performing
no code deletions and producing a compilable result.

Limiting our considerations to (functional) unit testing may also threaten internal validity.
As unit testing constitutes the most established and relevant testing technique in practice, it
is particularly interesting and relevant to investigate our methodology at this level ï¬rst. In
addition, the proposed concepts might be likewise applicable at integration- and system-level.

RegreTS

Page 29 / 39

Additionally, our methodology does not incorporate systematic reusability-checks of existing
test cases for revealing modiï¬cations also in later revisions which may aï¬€ect eï¬ƒciency measures.
We plan to extend our approach, accordingly, in a future work but we expect similar results as
in our current setting.

Finally, we expect our current focus on C programs to also not seriously harm validity as
we expect similar results for other programming languages, at least for those relying on an
imperative core (e.g., most OO languages).

External Validity. We are not aware of any competitive tools with similar functionality
as RegreTS, especially concerning the generation of a conï¬gurable number of modiï¬cation-
revealing test cases. Surprisingly, it was not possible for us to use other recent test-case
generators for strategies with RTC = MR, which, by design, should have been possible.
This might be due to the fact, that the subject systems are real-world programs, which where
explicitly selected to be processable by our test-case generator. However, test-case generators
are (usually) limited in supporting certain constructs of the C language. We tried to use other
test-case generators (i.e., Klee, FuSEBMC, Symbiotic and PRTest) from the international
testing competition 2021 [17]. However, these test-case generators were barely able to generate
any test cases at all. In addition, successfully generated test cases were actually unable to reveal
modiï¬cations between diï¬€erent program version and were thus immediately removed from the
test-suites during TSR (leading to empty test suites). Only PRTest was able to generate some
meaningful test-cases but was, however, also not able to generate any test case for more than
half of the subject systems thus being unusable for a proper comparison. However, the main
focus of this paper is to compare diï¬€erent regression-testing strategies, and not to compare
diï¬€erent test-case generators for regression-test generation. We thus assume the results to
be very similar for other test-case generation techniques (even though eï¬€ectiveness in terms
of CPU time might change, the ratio of the CPU time of diï¬€erent strategy presumably stay
similar).

Another threat might arise from the selection of subject systems and the usage of simulated
bugs. Unfortunately, real-world systems with suï¬ƒcient information about revisions and bugs
as required for our experiments are barely available. We evaluated three prominent candidates
for potential candidates. First, CoreBench only provides a very short version history (often
only 1â€“2 versions) and incorporates many bugs being undetectable at unit level (e.g., involving
ï¬les and global errors like overï¬‚ows) [18]. Second, the regression-veriï¬cation tasks from the
SV-Benchmarks [19] also have a small version history and the diï¬€erent tasks cannot be executed
in a self-sustained manner as needed to reveal those bugs. However, we spend a lot of time in
searching for other freely available community benchmarks including version history and known
bugs. However, suitable benchmarks are still very rare. Amongst others, we had a look into
further subject systems from the SIR Repository including programs like gzip and make [20],
but either our test-case generator was not able to handle those programs for mostly technical
reasons, or the programs were not suitable for our purposes (see descriptions above). As also
already discussed above, mutation testing is a reliable fault-injection technique for measuring
eï¬€ectiveness in testing experiments [15].

Finally, our tool relies on third-party software, namely CPAchecker, a software model-
checker, and the mutation tool MUSIC for C programs. However, both tools are established
and have been extensively used for other experiments in the recent past, so we expect them to
produce sound results.

RegreTS

Page 30 / 39

5 Related Work

5.1 Regression-Testing

A comprehensive overview about regression testing is provided by [2], describing three cate-
gories: (1) minimization of test suites as well as (2) selection and (3) prioritization of test
cases for regression testing.

Test-suite minimization is concerned with selecting from an existing test suite a subset
of test cases to reduce the number of redundant test executions during regression testing.
Many works propose heuristics for approximating near-optimal solutions [4, 21â€“23] for this
NP-complete optimization problem, requiring as inputs an existing test suite and a-priori
deï¬ned metrics for measuring eï¬€ectiveness of test cases. The approaches used in this paper for
TSR have been proposed before. The greedy algorithm as used by DIFF has been introduced
in [11]. To use ILP solving for TSR was initially proposed by [5] and FAST++ has been
introduced by [10]. However, none of these works investigate the interactions between RTS
and TSR techniques as done in this paper.

[24] evaluate existing test-suite-reduction techniques on real-word projects based on their
failed builds. [25] proposed a new technique to reduce test suites based on assertions instead
of structural code coverage to improve eï¬€ectiveness of the resulting test suite. In an earlier
work, [26] compare and combine TSR and test-case selection to further increase eï¬ƒciency
of regression testing. Our methodology goes beyond their approach as we consider further
strategic parameters which turned out to be very relevant.

Regression-test selection is concerned with selecting from an existing test suite of an
evolving program a subset of test cases to be (re-)executed on a new program version. A
variety of diï¬€erent techniques has been applied (e.g., control-ï¬‚ow analysis [27â€“29] and/or data-
ï¬‚ow analysis [30â€“33]). Other works take behavior-preserving modiï¬cations (e.g., refactorings)
into account [34], apply RTS to highly-conï¬gurable software [35], and try to ï¬nd pareto-optimal
solutions for multi-objective RTS [36]. However, none of these works aim at generating new
modiï¬cation-revealing test cases to enhance eï¬€ectiveness of regression testing as done in our
work. In particular, most recent works only guarantee test cases to be modiï¬cation-traversing.

Test-case prioritization is concerned with selecting from an existing test suite a (re-)test-
execution order such that eï¬€ectiveness of testing increases as quickly as possible over time (e.g.,
to ï¬nd as many faults as fast as possible) [37]. The underlying problem is very similar to mini-
mization/selection problems. Most existing approaches consider code coverage as eï¬€ectiveness
criterion to statically compute an (a-priori) ordering among test cases [38â€“43]. In a recent work,
Wang and Zeng propose a dynamic prioritization technique based on fault-detection history
and other properties [44]. In contrast, in our methodology, prioritization is currently out of
scope, but may be easily incorporated during test-case generation using recent approaches.

5.2 Test-Case Generation

A wide range of technique exist for (automatically) generating test cases which we will describe
in the following grouped by the test-case generation technique applied. However, we are not
aware of related works in terms of the multiple test-cases per test goal to increase eï¬€ectiveness
of the resulting test suite.

RegreTS

Page 31 / 39

5.2.1 Coverage-Based Test-Case Generation

Fuzzing. Fuzzing is currently very popular both in research and practice. The idea is to
quickly generate a large number of test cases by generating (semi-)random input values. In
some approaches, the input values of existing test cases are reused and modiï¬ed to generate
new test cases. [45] Recent fuzzing techniques are based on evolutionary algorithms [46] or
context-free grammars of the input data [47]. In addition, grey-box fuzzing [48] and whitebox
fuzzing [47, 49] have been proposed (i.e., fuzzers also considering the source code of the program
under test). However, the primary goal of fuzzing is not to generate test cases for regression
testing systematically traversing / revealing particular program modiï¬cations through diï¬€erent
possible paths and/or program versions as done in our approach.

Plain Random. A test-case generation technique that is similar to, yet simpler than, is
plain random test-case generation. [50]. Test cases are randomly generated, and afterwards,
the achieved coverage is measured. This approach is clearly more eï¬ƒcient than test-goal guided
techniques. On the other hand, more complicated test goals are often not reached, as the
chance to generate valid input values reaching those goals is small (e.g., to generate a test
1
case for input value x which evaluates true for x == 1 is
232 for a 32-bit system). Therefore,
generating (multiple diï¬€erent) modiï¬cation-revealing test cases is usually extremely expensive
and ineï¬€ective using purely random approaches.

Symbolic Execution. Symbolic execution employs a symbolic reachability graph cope with
the reachable state space of input programs during test-case generation. One prominent ex-
ample is Klee [51]. Based on symbolic execution, it might be also possible to generate multiple
test cases covering the same goal through diï¬€erent paths as done in our work. However, we are
not aware of any recent work going into this direction.

Bounded Model Checking. Another technique to scale test-case generation to larger pro-
grams is bounded model checking [52]. A bounded model checkers also computes the reachable
state space of programs (either in an abstract or concrete representation), where loops are only
explored up to maximum number of iterations k (bound). This enables the model checker
to prove program properties with certainty only within that bound. Such a tool can be also
used for test cases generation similar to symbolic model checking (i.e., by encoding test goals
as reachbility problems, see below). Again, we are not aware of any works using bounded
model checking to generate multiple test cases for the same test goal or program modiï¬cation,
respectively.

Symbolic Model Checking. Another approach to handle larger input programs is symbolic
model checking as applied, for instance, by the CPAchecker framework, which is also used by
our approach [13]. CoVeriTest is another recent test-case generator based on the CPAchecker
framework [53]. Again, these and other tools do currently not support generation of multiple
test-cases per test goal or program modiï¬cation as required in our approach. However, encoding
the underlying problem as reachability query as done in our approach would also enable the
usage of these other tools for regression-test generation.

5.2.2 Regression-Test-Case Generation.

We next discuss related work on generating test cases for systematically investigating semantic
diï¬€erences between similar programs. Diï¬€erential testing [54] is concerned with the following
problem: Given two comparable programs and an set of diï¬€erential test cases, the systems can
be checked for bugs by running the test cases. If the outputs diï¬€er or the test loops indeï¬nitely

RegreTS

Page 32 / 39

or crashes, the test case is a candidate for a bug-revealing test. Thereupon, Evans and Savioa
proposed an approach for detecting program changes by comparing test-execution results of
two diï¬€erent program versions [55]. The tool CSmith combines diï¬€erential testing with fuzzing
(i.e., C programs) to ï¬nd bugs in C-compiler implementations [56]. The work being presumably
most closely related to our methodology is DiffGen [57] for generating test cases comparing
two versions of a Java program. This is achieved by instrumenting programs with equality-
assert-statements and generating test suites for statement coverage on (failed) assertions. This
work diï¬€ers from ours as it does not support multiple test cases ï¬nding diï¬€erences between
program versions, and also does not take multiple prior versions into account. Additionally,
they do not take TSR into account to increase eï¬ƒciency of regression testing.

The goal of mutation testing is to measure eï¬€ectiveness of test suites or test-generation
techniques, respectively, by deriving from an original program a set of syntactically slightly
changed mutants (simulating faults) [58]. A test case detects a mutant if its test-execution
results for the original program diï¬€er from those for the mutant. Based on this principle, [59]
pursue mutation-driven generation of test cases for Java programs by using genetic algorithms
to ï¬nd test cases that detect mutants. In contrast, [60] use a combination of symbolic execution
and a search-based heuristic to identify test cases that are likely to reveal mutants.
[61] also
propose a heuristic for generating strong mutation-detecting test cases using hill climbing.
As these works are mainly based on heuristics, the generated test cases do not guarantee to
traverse/reveal program modiï¬cations. Additionally, those approaches do not allow to conï¬gure
the number of test cases or the number of diï¬€erent programs as our parameters NPR and NRT,
nor do they consider TSR.

Automatically generating test cases for diï¬€erentiating two program versions for regression
testing has been initially proposed by [62]. Their white-box testing tool automatically compares
output values of two given program versions to derive input values leading to diï¬€erent outputs.
Their approach is applicable to Pascal programs only and does not support multiple test cases
and/or program versions as in our work.

5.3 Regression Veriï¬cation

The goal of regression veriï¬cation is to analyze diï¬€erent versions of a system or program to
check whether the speciï¬cation is still fulï¬lled after modiï¬cations. To this end, intermediate
veriï¬cation results are re-used between versions to increase eï¬ƒciency [63â€“65]. For instance,
intermediate results (so-called abstraction precisions) of veriï¬cation runs enable reuse for later
version [66], and regression veriï¬cation may be applied to restrict the set of program inputs
manually [67] or by syntactic checks [68]. Furthermore, there is work on lifting principles
of regression veriï¬cation to multi-threaded programs [69] and re-checking evolving software
of automated production systems [70]. Moreover, eï¬ƒciency of regression veriï¬cation may be
improved, for instance, by applying state-space partitioning-techniques [71] and by improved
encodings of reuse-information [72]. Other works in this area reuse ï¬nal veriï¬cation results in
case of a subsequent change in the program [14, 73â€“76]. However, none of these approaches
further utilizes the information collected for regression analysis to derive modiï¬cation-revealing
test cases.

Similarly, conditional model checking aims at reusing results of veriï¬cation runs to perform
collaborative veriï¬cation [77, 78]. To this end, there are exchange formats for veriï¬cation
witnesses for property violations [79] as well as for correctness proofs [80]. Furthermore, there
is work on keeping track of unveriï¬ed parts of a program to apply test-case generation for these
parts [81, 82] and reusing veriï¬cation results for hybrid systems [83]. However, none of these
approaches aim at ï¬nding diï¬€erences between versions of the same program as done in our
approach.

RegreTS

Page 33 / 39

6 Conclusion and Future Work

We presented a conï¬gurable regression-testing methodology for automating the selection of
regression test-cases with a particular focus on revealing regression bugs in evolving programs.
RegreTS currently supports regression testing of C programs at unit level. Our experimental
results show that eï¬€ectiveness and the eï¬ƒciency highly depend on the selected regression
test-case generation strategy, where the parameter RTC (i.e., either generating modiï¬cation-
traversing or modiï¬cation-revealing test cases) has the strongest impact on the eï¬€ectiveness
and eï¬ƒciency. To conclude, our experimental results show that for obtaining the best eï¬ƒciency
in terms of CPU time and the best trade-oï¬€ modiï¬cation traversing test cases should be used
without test-suite reduction. However, for the best eï¬€ectiveness, modiï¬cation revealing test
cases should be used with multiple test cases per test goal and multiple previous revisions taken
into account. Additionally, test-suite reduction obstructs eï¬€ectiveness, therefore, for optimal
eï¬€ectiveness test-suite reduction should be disabled.

As a future work, we plan to extend our approach. First, we plan to further improve
our test-generation technique to support additional subject systems as well as in utilizing
alternative test-case generation techniques within our methodology (e.g., symbolic execution)
and to compare the outcome with our current results. Furthermore, we plan to investigate
other kinds of regression errors and to identify suitable regression strategies for eï¬€ectively
revealing those bugs. Finally, we plan to adapt RegreTS to other testing scenarios including,
for instance, other input languages besides C and other testing levels beyond unit testing which
enables us to conduct experiments on a richer set of subject systems.

Acknowledgments This work was funded by the Hessian LOEWE initiative within the
Software-Factory 4.0 project.

References

[1] Paul Ammann and Jeï¬€ Oï¬€utt. Introduction to Software Testing. Cambridge University

Press, 2016. isbn: 978-1-107-17201-2.

[2] Shin Yoo and Mark Harman. â€œRegression Testing Minimization, Selection and Prioritiza-
tion: A Surveyâ€. In: Softw. Test. Verif. Reliab. 22.2 (2012), pp. 67â€“120. issn: 0960-0833.
doi: 10.1002/stv.430.

[3] Emelie EngstrÂ¨om, Mats Skoglund, and Per Runeson. â€œEmpirical Evaluations of Regres-
sion Test Selection Techniques: A Systematic Reviewâ€. In: Proceedings of the Second
ACM-IEEE International Symposium on Empirical Software Engineering and Measure-
ment. ACM, 2008, pp. 22â€“31. isbn: 978-1-59593-971-5. doi: 10.1145/1414004.1414011.

[4] Mary Jean Harrold, Rajiv Gupta, and Mary Lou Soï¬€a. â€œA Methodology for Controlling
the Size of a Test Suiteâ€. In: ACM Trans. Softw. Eng. Methodol. 2.3 (1993), pp. 270â€“285.
issn: 1049-331X. doi: 10.1145/152388.152391.

[5] Alireza Khalilian and Saeed Parsa. â€œBi-criteria Test Suite Reduction by Cluster Analysis
of Execution Proï¬lesâ€. In: Advances in Software Engineering Techniques. Springer Berlin
Heidelberg, 2012, pp. 243â€“256. isbn: 978-3-642-28038-2.

[6] Saswat Anand et al. â€œAn orchestrated survey of methodologies for automated software
test case generationâ€. In: Journal of Systems and Software 86.8 (2013), pp. 1978â€“2001.
issn: 0164-1212. doi: https://doi.org/10.1016/j.jss.2013.02.061.

[7] Fabiano Pecorelli, Fabio Palomba, and Andrea De Lucia. â€œThe Relation of Test-Related
Factors to Software Quality: A Case Study on Apache Systemsâ€. In: Empirical Software
Engineering 26.2 (2021). doi: 10.1007/s10664-020-09891-y.

RegreTS

Page 34 / 39

[8] Laura Inozemtseva and Reid Holmes. â€œCoverage is Not Strongly Correlated with Test
Suite Eï¬€ectivenessâ€. In: Proceedings of the 36th International Conference on Software En-
gineering. Association for Computing Machinery, 2014, pp. 435â€“445. isbn: 9781450327565.
doi: 10.1145/2568225.2568271.

[9] Jung-Min Kim and A. Porter. â€œA history-based test prioritization technique for regression
testing in resource constrained environmentsâ€. In: Proceedings of the 24th International
Conference on Software Engineering. ICSE 2002. Association for Computing Machinery,
2002, pp. 119â€“129. isbn: 158113472X. doi: 10.1109/ICSE.2002.1007961.

[10] Emilio Cruciani, Breno Miranda, Roberto Verdecchia, and Antonia Bertolino. â€œScalable
Approaches for Test Suite Reductionâ€. In: 2019 IEEE/ACM 41st International Confer-
ence on Software Engineering (ICSE). IEEE Press, 2019, pp. 419â€“429. doi: 10.1109/
ICSE.2019.00055.

[11] Dirk Beyer and Thomas Lemberger. â€œTestCov: Robust Test-Suite Execution and Cov-
erage Measurementâ€. In: Proceedings of the 34th IEEE/ACM International Conference
on Automated Software Engineering. IEEE Press, 2019, 1074â€“1077. isbn: 9781728125084.
doi: 10.1109/ASE.2019.00105.

[12] Zhenyu Chen, Xiaofang Zhang, and Baowen Xu. â€œA Degraded ILP Approach for Test

Suite Reduction.â€ In: Jan. 2008, pp. 494â€“499.

[13] Dirk Beyer, Adam J. Chlipala, Thomas A. Henzinger, Ranjit Jhala, and Rupak Majum-
dar. â€œGenerating Tests from Counterexamplesâ€. In: Proceedings of the 26th International
Conference on Software Engineering. IEEE Computer Society, 2004, pp. 326â€“335. isbn:
978-0769521633. doi: 10.5555/998675.999437.

[14] Dirk Beyer, Andreas Holzer, Michael Tautschnig, and Helmut Veith. â€œInformation Reuse
for Multi-goal Reachability Analysesâ€. In: ESOP â€™13. Vol. 7792. Springer, 2013, pp. 472â€“
491. isbn: 978-3-642-37036-6. doi: 10.1007/978-3-642-37036-6_26.

[15] James H. Andrews, Lionel C. Briand, and Yvan Labiche. â€œIs mutation an appropri-
ate tool for testing experiments? [software testing]â€. In: Proceedings. 27th International
Conference on Software Engineering, 2005. ICSE 2005. IEEE, 2005, pp. 402â€“411. doi:
10.1109/ICSE.2005.1553583.

[16] Duy Loc Phan, Yunho Kim, and Moonzoo Kim. â€œMUSIC: Mutation Analysis Tool with
High Conï¬gurability and Extensibilityâ€. In: 2018 IEEE International Conference on
Software Testing, Veriï¬cation and Validation Workshops. IEEE, 2018, pp. 40â€“46. doi:
10.1109/ICSTW.2018.00026.

[17] Dirk Beyer. â€œStatus Report on Software Testing: Test-Comp 2021â€. In: Fundamental
Approaches to Software Engineering. Springer International Publishing, 2021, pp. 341â€“
357. isbn: 978-3-030-71500-7. doi: 10.1007/978-3-030-71500-7_17.

[18] Marcel BÂ¨ohme and Abhik Roychoudhury. â€œCoREBench: Studying Complexity of Regres-
sion Errorsâ€. In: Proceedings of the 23rd ACM/SIGSOFT International Symposium on
Software Testing and Analysis. ACM, 2014, pp. 105â€“115.

[19] Dirk Beyer. SV-Benchmarks: Benchmark Set of 8th Intl. Competition on Software Veri-

ï¬cation (SV-COMP 2019). 2019. doi: 10.5281/zenodo.2598728.

[20] Hyunsook Do, Sebastian G. Elbaum, and Gregg Rothermel. â€œSupporting Controlled Ex-
perimentation with Testing Techniquesâ€. In: Empirical Software Engineering: An Inter-
national Journal 10.4 (2005), pp. 405â€“435.

[21] Tsong Yueh Chen and Man Fai Lau. â€œDividing Strategies for the Optimization of a Test
Suiteâ€. In: Inf. Process. Lett. 60.3 (1996), pp. 135â€“141. issn: 0020-0190. doi: 10.1016/
S0020-0190(96)00135-4.

RegreTS

Page 35 / 39

[22] Joseph R. Horgan and Saul London. â€œA data ï¬‚ow coverage testing tool for Câ€. In: [1992]
Proceedings of the Second Symposium on Assessment of Quality Software Development
Tools. IEEE, 1992, pp. 2â€“10. doi: 10.1109/AQSDT.1992.205829.

[23] A. Jeï¬€erson Oï¬€utt, Jie Pan, and Jeï¬€rey M. Voas. â€œProcedures for reducing the size
of coverage-based test setsâ€. In: Twelfth International Conference on Testing Computer
Software. 1995, pp. 111â€“123.

[24] August Shi, Alex Gyori, Suleman Mahmood, Peiyuan Zhao, and Darko Marinov. â€œEvalu-
ating Test-Suite Reduction in Real Software Evolutionâ€. In: Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis. Association for
Computing Machinery, 2018, pp. 84â€“94. isbn: 9781450356992. doi: 10.1145/3213846.
3213875.

[25] Junjie Chen, Yanwei Bai, Dan Hao, Lingming Zhang, Lu Zhang, and Bing Xie. â€œHow Do
Assertions Impact Coverage-Based Test-Suite Reduction?â€ In: 2017 IEEE International
Conference on Software Testing, Veriï¬cation and Validation (ICST). IEEE, 2017. doi:
10.1109/icst.2017.45.

[26] August Shi, Tifany Yung, Alex Gyori, and Darko Marinov. â€œComparing and Combin-
ing Test-Suite Reduction and Regression Test Selectionâ€. In: Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering. Association for Computing
Machinery, 2015, pp. 237â€“247. isbn: 9781450336758. doi: 10.1145/2786805.2786878.

[27] J. Hartmann and David J. Robson. â€œRevalidation during the software maintenance
phaseâ€. In: Proceedings. Conference on Software Maintenance - 1989. IEEE, 1989, pp. 70â€“
80. doi: 10.1109/ICSM.1989.65195.

[28] J. Hartmann and David J. Robson. â€œRETEST-development of a selective revalidation pro-
totype environment for use in software maintenanceâ€. In: Twenty-Third Annual Hawaii
International Conference on System Sciences. Vol. 2. IEEE, 1990, 92â€“101 vol.2. doi:
10.1109/HICSS.1990.205179.

[29] J. Hartmann and David J. Robson. â€œTechniques for selective revalidationâ€. In: IEEE

Software 7.1 (1990), pp. 31â€“36. issn: 0740-7459. doi: 10.1109/52.43047.

[30] Rajiv Gupta, Mary Jean Harrold, and Mary Lou Soï¬€a. â€œAn approach to regression testing
using slicingâ€. In: Proceedings Conference on Software Maintenance 1992. IEEE Com-
puter Society, 1992, pp. 299â€“308. doi: 10.1109/ICSM.1992.242531.

[31] Mary Jean Harrold and M. L. Souï¬€a. â€œAn incremental approach to unit testing during
maintenanceâ€. In: Proceedings. Conference on Software Maintenance, 1988. IEEE, 1988,
pp. 362â€“367. doi: 10.1109/ICSM.1988.10188.

[32] Mary Jean Harrold and Mary Lou Soï¬€a. â€œInterprocedual Data Flow Testingâ€. In: Pro-
ceedings of the ACM SIGSOFT â€™89 Third Symposium on Software Testing, Analysis, and
Veriï¬cation. ACM, 1989, pp. 158â€“167. isbn: 0-89791-342-6. doi: 10.1145/75308.75327.

[33] Abu-Bakr Taha, Stephen M. Thebaut, and Sying-Syang Liu. â€œAn approach to software
fault localization and revalidation based on incremental data ï¬‚ow analysisâ€. In: [1989]
Proceedings of the Thirteenth Annual International Computer Software Applications Con-
ference. IEEE, 1989, pp. 527â€“534. doi: 10.1109/CMPSAC.1989.65142.

[34] Kaiyuan Wang, Chenguang Zhu, Ahmet Celik, Jongwook Kim, Don Batory, and Milos
Gligoric. â€œTowards Refactoring-aware Regression Test Selectionâ€. In: Proceedings of the
40th International Conference on Software Engineering. ACM, 2018, pp. 233â€“244. isbn:
978-1-4503-5638-1. doi: 10.1145/3180155.3180254.

RegreTS

Page 36 / 39

[35] Dusica Marijan and Marius Liaaen. â€œPractical Selective Regression Testing with Eï¬€ective
Redundancy in Interleaved Testsâ€. In: Proceedings of the 40th International Conference
on Software Engineering: Software Engineering in Practice. Association for Computing
Machinery, 2018, 153â€“162. isbn: 9781450356596. doi: 10.1145/3183519.3183532.

[36] Ankur Choudhary, Arun Prakash Agrawal, and Arvinder Kaur. â€œAn Eï¬€ective Approach
for Regression Test Case Selection Using Pareto Based Multi-objective Harmony Searchâ€.
In: Proceedings of the 11th International Workshop on Search-Based Software Testing.
ACM, 2018, pp. 13â€“20. isbn: 978-1-4503-5741-8. doi: 10.1145/3194718.3194722.

[37] Weichen E. Wong, Joseph R. Horgan, Saul London, and Aditya P. Mathur. â€œEï¬€ect of
Test Set Minimization on Fault Detection Eï¬€ectivenessâ€. In: 1995 17th International
Conference on Software Engineering. ACM, 1995, pp. 41â€“41. doi: 10 . 1145 / 225014 .
225018.

[38] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary Jean Harrold. â€œTest case
prioritization: an empirical studyâ€. In: Proceedings IEEE International Conference on
Software Maintenance - 1999 (ICSMâ€™99). â€™Software Maintenance for Business Changeâ€™
(Cat. No.99CB36360). IEEE, 1999, pp. 179â€“188. doi: 10.1109/ICSM.1999.792604.

[39] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary J. Harrold. â€œPrioritizing
test cases for regression testingâ€. In: IEEE Transactions on Software Engineering 27.10
(2001), pp. 929â€“948. issn: 0098-5589. doi: 10.1109/32.962562.

[40] Sebastian Elbaum, David Gable, and Gregg Rothermel. â€œUnderstanding and measuring
the sources of variation in the prioritization of regression test suitesâ€. In: Proceedings Sev-
enth International Software Metrics Symposium. IEEE Computer Society, 2001, pp. 169â€“
179. doi: 10.1109/METRIC.2001.915525.

[41] Sebastian Elbaum, Alexey Malishevsky, and Gregg Rothermel. â€œIncorporating varying
test costs and fault severities into test case prioritizationâ€. In: Proceedings of the 23rd
International Conference on Software Engineering. ICSE 2001. IEEE Computer Society,
2001, pp. 329â€“338. doi: 10.1109/ICSE.2001.919106.

[42] Alexey Malishevsky, Gregg Rothermel, and Sebastian Elbaum. â€œModeling the cost-beneï¬ts
tradeoï¬€s for regression testing techniquesâ€. In: International Conference on Software
Maintenance, 2002. Proceedings. IEEE, 2002, pp. 204â€“213. doi: 10.1109/ICSM.2002.
1167767.

[43] Gregg Rothermel, Sebastian Elbaum, Alexey Malishevsky, Praveen Kallakuri, and Brian
Davia. â€œThe impact of test suite granularity on the cost-eï¬€ectiveness of regression test-
ingâ€. In: Proceedings of the 24th International Conference on Software Engineering. ICSE
2002. ACM, 2002, pp. 130â€“140. doi: 10.1145/581356.581358.

[44] Xiaolin Wang and Hongwei Zeng. â€œHistory-Based Dynamic Test Case Prioritization
for Requirement Properties in Regression Testingâ€. In: 2016 IEEE/ACM International
Workshop on Continuous Software Evolution and Delivery (CSED). IEEE, 2016, pp. 41â€“
47. doi: 10.1109/CSED.2016.016.

[45] Jun Li, Bodong Zhao, and Chao Zhang. â€œFuzzing: a surveyâ€. In: Cybersecurity 1.1 (2018).

doi: 10.1186/s42400-018-0002-y.

[46] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuï¬€rida, and Her-
bert Bos. â€œVUzzer: Application-aware Evolutionary Fuzzingâ€. In: Proceedings 2017 Net-
work and Distributed System Security Symposium. Internet Society, 2017. doi: 10.14722/
ndss.2017.23404.

RegreTS

Page 37 / 39

[47] Patrice Godefroid, Adam Kiezun, and Michael Y. Levin. â€œGrammar-Based Whitebox
Fuzzingâ€. In: Proceedings of the 29th ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation. Association for Computing Machinery, 2008, 206â€“215.
isbn: 9781595938602. doi: 10.1145/1375581.1375607.

[48] Micha(cid:32)l Zalewski. Technical â€whitepaperâ€ for aï¬‚-fuzz. Tech. rep. 2006. url: https : / /

lcamtuf.coredump.cx/afl/technical_details.txt.

[49] Patrice Godefroid, Michael Y. Levin, and David Molnar. â€œAutomated Whitebox Fuzz

Testingâ€. In: Network Distributed Security Symposium (NDSS). 2008.

[50] Thomas Lemberger. â€œPlain random test generation with PRTestâ€. In: International Jour-
nal on Software Tools for Technology Transfer (2020). doi: 10 . 1007 / s10009 - 020 -
00568-x.

[51] Cristian Cadar, Daniel Dunbar, and Dawson Engler. â€œKLEE: Unassisted and Automatic
Generation of High-Coverage Tests for Complex Systems Programsâ€. In: 8th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 08). Vol. 8. USENIX
Association, 2008, 209â€“224.

[52] Mikhail R. Gadelha, Rafael Menezes, Felipe R. Monteiro, Lucas C. Cordeiro, and Denis
Nicole. â€œESBMC: Scalable and Precise Test Generation based on the Floating-Point
Theoryâ€. In: Fundamental Approaches to Software Engineering. Springer International
Publishing, 2020, pp. 525â€“529. isbn: 978-3-030-45234-6. doi: 10 . 1007 / 978 - 3 - 030 -
45234-6_27.

[53] Marie-Christine Jakobs. â€œCoVeriTest with Dynamic Partitioning of the Iteration Time
Limit (Competition Contribution)â€. In: Fundamental Approaches to Software Engineer-
ing. Vol. 12076. Springer International Publishing, 2020, pp. 540â€“544. isbn: 978-3-030-
45234-6. doi: 10.1007/978-3-030-45234-6\_30.

[54] William M. McKeeman. â€œDiï¬€erential Testing for Softwareâ€. In: Digital Technical Journal

10 (1998), pp. 100â€“107.

[55] Robert B. Evans and Alberto Savoia. â€œDiï¬€erential Testing: A New Approach to Change
Detectionâ€. In: The 6th Joint Meeting on European Software Engineering Conference and
the ACM SIGSOFT Symposium on the Foundations of Software Engineering: Companion
Papers. ACM, 2007, pp. 549â€“552. isbn: 978-1-59593-812-1. doi: 10 . 1145 / 1295014 .
1295038.

[56] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. â€œFinding and Understanding
Bugs in C Compilersâ€. In: Proceedings of the 32Nd ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation. ACM, 2011, pp. 283â€“294. isbn: 978-1-
4503-0663-8. doi: 10.1145/1993498.1993532.

[57] Kunal Taneja and Tao Xie. â€œDiï¬€Gen: Automated Regression Unit-Test Generationâ€. In:
2008 23rd IEEE/ACM International Conference on Automated Software Engineering.
IEEE, 2008, pp. 407â€“410. doi: 10.1109/ASE.2008.60.

[58] Yue Jia and Mark Harman. â€œAn Analysis and Survey of the Development of Mutation
Testingâ€. In: IEEE Transactions on Software Engineering 37.5 (2011), pp. 649â€“678. issn:
0098-5589. doi: 10.1109/TSE.2010.62.

[59] Gordon Fraser and Andreas Zeller. â€œMutation-driven Generation of Unit Tests and Ora-
clesâ€. In: Proceedings of the 19th International Symposium on Software Testing and Anal-
ysis. ACM, 2010, pp. 147â€“158. isbn: 978-1-60558-823-0. doi: 10.1145/1831708.1831728.
[60] Mark Harman, Yue Jia, and William B. Langdon. â€œStrong Higher Order Mutation-based
Test Data Generationâ€. In: Proceedings of the 19th ACM SIGSOFT Symposium and the
13th European Conference on Foundations of Software Engineering. ACM, 2011, pp. 212â€“
222. isbn: 978-1-4503-0443-6. doi: 10.1145/2025113.2025144.

RegreTS

Page 38 / 39

[61] Francisco Carlos M. Souza, Mike Papadakis, Yves Le Traon, and MÂ´arcio E. Delamaro.
â€œStrong Mutation-based Test Data Generation Using Hill Climbingâ€. In: Proceedings of
the 9th International Workshop on Search-Based Software Testing. ACM, 2016, pp. 45â€“
54. isbn: 978-1-4503-4166-0. doi: 10.1145/2897010.2897012.

[62] Bogdan Korel and Ali M. Al-Yami. â€œAutomated Regression Test Generationâ€. In: Pro-
ceedings of the 1998 ACM SIGSOFT International Symposium on Software Testing and
Analysis. ACM, 1998, pp. 143â€“152. isbn: 0-89791-971-8. doi: 10.1145/271771.271803.

[63] R. H. Hardin, R. P. Kurshan, K. L. McMillan, J. A. Reeds, and N. J. A. Sloane. â€œEï¬ƒcient

regression veriï¬cationâ€. In: WODES â€™96. IEE, 1996, pp. 157â€“150.

[64] Thomas A. Henzinger, Ranjit Jhala, Rupak Majumdar, and Marco A. A. Sanvido. â€œEx-
treme Model Checkingâ€. In: Veriï¬cation: Theory and Practice. Vol. 2772. LNCS. Springer,
2003, pp. 332â€“358. isbn: 978-3-540-39910-0. doi: 10.1007/978-3-540-39910-0_16.

[65] Ofer Strichman and Benny Godlin. â€œRegression Veriï¬cation - A Practical Way to Verify
Programsâ€. In: VSTTE â€™05. Vol. 4171. LNCS. Springer, 2008, pp. 496â€“501. isbn: 978-3-
540-69149-5. doi: 10.1007/978-3-540-69149-5_54.

[66] Dirk Beyer, Stefan LÂ¨owe, Evgeny Novikov, Andreas Stahlbauer, and Philipp Wendler.
â€œPrecision Reuse for Eï¬ƒcient Regression Veriï¬cationâ€. In: ESEC/FSE â€™13. ACM, 2013,
pp. 389â€“399. isbn: 978-1-4503-2237-9. doi: 10.1145/2491411.2491429.

[67] Marcel BÂ¨ohme, Bruno C. d. S. Oliveira, and Abhik Roychoudhury. â€œPartition-based
Regression Veriï¬cationâ€. In: ICSE â€™13. IEEE Press, 2013, pp. 302â€“311. isbn: 978-1-4673-
3076-3. doi: 10.5555/2486788.2486829.

[68] Benny Godlin and Ofer Strichman. â€œRegression veriï¬cation: proving the equivalence of
similar programsâ€. In: Software Testing, Veriï¬cation and Reliability 23.3 (2013), pp. 241â€“
258. doi: 10.1002/stvr.1472.

[69] Sagar Chaki, Arie Gurï¬nkel, and Ofer Strichman. â€œRegression Veriï¬cation for Multi-
threaded Programsâ€. In: VMCAI â€™12. Vol. 7148. Springer, 2012, pp. 119â€“135. isbn: 978-
3-642-27940-9. doi: 10.1007/978-3-642-27940-9_9.

[70] Bernhard Beckert, Mattias Ulbrich, Birgit Vogel-Heuser, and Alexander Weigl. â€œRegres-
sion Veriï¬cation for Programmable Logic Controller Softwareâ€. In: ICFEM â€™15. Vol. 9407.
Springer International Publishing, 2015, pp. 234â€“251. isbn: 978-3-319-25423-4. doi: 10.
1007/978-3-319-25423-4_15.

[71] John Backes, Suzette Person, Neha Rungta, and Oksana Tkachuk. â€œRegression Veriï¬-
cation Using Impact Summariesâ€. In: SPIN â€™13. Vol. 7976. Springer, 2013, pp. 99â€“116.
isbn: 978-3-642-39176-7. doi: 10.1007/978-3-642-39176-7_7.

[72] Dennis Felsing, Sarah Grebing, Vladimir Klebanov, Philipp RÂ¨ummer, and Mattias Ul-
brich. â€œAutomating Regression Veriï¬cationâ€. In: ASE â€™14. ACM, 2014, pp. 349â€“360. isbn:
978-1-4503-3013-8. doi: 10.1145/2642937.2642987.

[73] Willem Visser, Jaco Geldenhuys, and Matthew B. Dwyer. â€œGreen: Reducing, Reusing
and Recycling Constraints in Program Analysisâ€. In: FSE â€™12. ACM, 2012, 58:1â€“58:11.
isbn: 978-1-4503-1614-9. doi: 10.1145/2393596.2393665.

[74] Ondrej Sery, Grigory Fedyukovich, and Natasha Sharygina. â€œIncremental Upgrade Check-
ing by Means of Interpolation-based Function Summariesâ€. In: FMCAD â€™12. IEEE, 2012,
pp. 114â€“121.

[75] Guowei Yang, Matthew B. Dwyer, and Gregg Rothermel. â€œRegression Model Checkingâ€.

In: ICSM â€™19. IEEE, 2009, pp. 115â€“124. doi: 10.1109/ICSM.2009.5306334.

RegreTS

Page 39 / 39

[76] Steven Lauterburg, Ahmed Sobeih, Darko Marinov, and Mahesh Viswanathan. â€œIncre-
mental State-Space Exploration for Programs with Dynamically Allocated Dataâ€. In:
ICSE â€™08. ACM, 2008, pp. 291â€“300. doi: 10.1145/1368088.1368128.

[77] Dirk Beyer, Thomas A. Henzinger, M. Erkan Keremoglu, and Philipp Wendler. â€œCon-
ditional Model Checking: A Technique to Pass Information between Veriï¬ersâ€. In: Pro-
ceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of
Software Engineering. Association for Computing Machinery, 2012. isbn: 9781450316149.
doi: 10.1145/2393596.2393664.

[78] Maria Christakis, Peter MÂ¨uller, and Valentin WÂ¨ustholz. â€œCollaborative Veriï¬cation and
Testing with Explicit Assumptionsâ€. In: FM â€™12. Vol. 7436. Springer, 2012, pp. 132â€“146.
isbn: 978-3-642-32759-9. doi: 10.1007/978-3-642-32759-9_13.

[79] Dirk Beyer, Matthias Dangl, Daniel Dietsch, Matthias Heizmann, and Andreas Stahlbauer.
â€œWitness Validation and Stepwise Testiï¬cation Across Software Veriï¬ersâ€. In: ESEC/FSE
â€™15. ACM, 2015, pp. 721â€“733. isbn: 978-1-4503-3675-8. doi: 10.1145/2786805.2786867.

[80] Dirk Beyer, Matthias Dangl, Daniel Dietsch, and Matthias Heizmann. â€œCorrectness Wit-
nesses: Exchanging Veriï¬cation Results Between Veriï¬ersâ€. In: FSE â€™16. ACM, 2016,
pp. 326â€“337. isbn: 978-1-4503-4218-6. doi: 10.1145/2950290.2950351.

[81] Mike Czech, Marie-Christine Jakobs, and Heike Wehrheim. â€œJust Test What You Cannot
Verify!â€ In: FASE â€™15. Vol. 9033. Springer, 2015, pp. 100â€“114. isbn: 978-3-662-46675-9.
doi: 10.1007/978-3-662-46675-9_7.

[82] Maria Christakis, Peter MÂ¨uller, and Valentin WÂ¨ustholz. â€œGuiding Dynamic Symbolic
Execution Toward Unveriï¬ed Program Executionsâ€. In: ICSE â€™16. ACM, 2016, pp. 144â€“
155. isbn: 978-1-4503-3900-1. doi: 10.1145/2884781.2884843.

[83] Stefan Mitsch, Grant Olney Passmore, and AndrÂ´e Platzer. â€œCollaborative Veriï¬cation-
Driven Engineering of Hybrid Systemsâ€. In: Mathematics in Computer Science 8.1 (2014),
pp. 71â€“97. issn: 1661-8289. doi: 10.1007/s11786-014-0176-y.

