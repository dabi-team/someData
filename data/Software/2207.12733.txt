2
2
0
2

l
u
J

6
2

]
E
S
.
s
c
[

1
v
3
3
7
2
1
.
7
0
2
2
:
v
i
X
r
a

On the Interaction between Test-Suite Reduction and
Regression-Test Selection Strategies

Sebastian Ruland1 and Malte Lochau2

1Technical University Darmstadt, Darmstadt, Germany
2University of Siegen, Siegen, Germany

July 27, 2022

Abstract

Unit testing is one of the most established quality-assurance techniques for software
development. One major advantage of unit testing is the adjustable trade-oﬀ between
eﬃciency (i.e., testing eﬀort) and eﬀectiveness (i.e., fault-detection probability). To this
end, various strategies have been proposed to exploit this trade-oﬀ.
In particular, test-
suite reduction (TSR) reduces the number of (presumably redundant) test cases while
testing a single program version. Regression-test selection (RTS) selects test cases for
testing consecutive program revisions. However, both TSR and RTS may inﬂuence—or
even obstruct— each others’ performance when used in combination. For instance, test
cases discarded during TSR for a particular program version may become relevant again
for RTS. However, ﬁnding a combination of both strategies leading to a reasonable trade-
oﬀ throughout the version history of a program is an open question. The goal of this
paper is to gain a better understanding of the interactions between TSR and RTS with
respect to eﬃciency and eﬀectiveness. To this end, we present a conﬁgurable framework
called RegreTS for automated unit-testing of C programs. The framework comprises
diﬀerent strategies for TSR and RTS and possible combinations thereof. We apply this
framework to a collection of subject systems, delivering several crucial insights. First, TSR
has almost always a negative impact on the eﬀectiveness of RTS, yet a positive impact
on eﬃciency. Second, test cases revealing to testers the eﬀect of program modiﬁcations
between consecutive program versions are far more eﬀective than test cases simply covering
modiﬁed code parts, yet causing much more testing eﬀort.

1 Introduction

Background and Motivation. Software testing is concerned with revealing as many bugs
as possible in a program within a—usually strictly limited—amount of time [1]. In particular,
unit testing is one of the most important innovations in the recent past for pro-actively ensuring
software quality in an eﬀective, yet tractable and agile manner. Bugs revealed by unit tests
include program crashes caused by programming errors as well as faulty input-output value
pairs contradicting a given speciﬁcation of the expected behavior (e.g., functionally incorrect
program logics or ambiguous requirements). To this end, test cases are deﬁned in terms of
exemplary input values and expected output values for experimentally executing the program
unit under test in a systematic manner.

One advantage of software testing as compared to other quality-assurance techniques is
the—more or less—freely adjustable trade-oﬀ between two (generally conﬂicting) optimization
goals, namely eﬀectiveness (i.e., maximizing fault-detection probability) and eﬃciency (mini-
mizing testing eﬀort) [2–4]. In an idealistic setting, a perfect trade-oﬀ between both goals would

 
 
 
 
 
 
RegreTS

Page 2 / 39

be a test suite that ﬁnds all bugs with minimum eﬀort (e.g., requiring a minimum number
of test cases). In reality, however, the number and exact location of bugs are, unfortunately,
a-priori unknown; and even if all bugs would be known in advance (which would make testing
obsolete), then ﬁnding a minimum set of test cases for revealing them is NP-hard [5]. Vari-
ous heuristics have been proposed to control the selection of suﬃciently eﬀective, yet eﬃcient
sets of test cases for a program unit under test. Heuristics for measuring eﬀectiveness often
rely on structural code-coverage metrics (e.g., branch coverage) to be optimized by a selected
test suite [6], whereas heuristics for eﬃciency apply test-suite reduction (TSR) techniques to
decrease the number of redundant test cases, again, with respect to that given code-coverage
criterion [2].

Moreover, modern software development is faced with the challenge of ever-shortened release
cycles leading to increasing frequencies of consecutive program revisions. The goal of regression
testing is to reveal emerging bugs introduced by erroneous program modiﬁcations between
subsequent program versions [2]. RTS strategies deﬁne criteria for updating an existing test
suite of a previous program version, by removing outdated test cases, by keeping still relevant
test cases, and by adding further test cases for newly added functionality. Many RTS and
regression-test generation strategies are further concerned with prioritizing test cases to apply
the presumably more eﬀective test cases ﬁrst. The aforementioned trade-oﬀ between eﬃciency
and eﬀectiveness is therefore also the primary goal of regression testing, but now starting from
an existing test suite inherited from previous program revisions.

Problem Statement and Research Challenges. Both TSR strategies for testing a single
program version as well as regression-testing strategies for testing consecutive program revisions
are concerned with the same problem:

How to achieve a reasonable trade-oﬀ between eﬃciency and eﬀectiveness?

Nevertheless, both kinds of strategies may potentially inﬂuence—or even obstruct— each
other in various ways. For instance, a test case being considered redundant for one program
version (thus being removed during TSR) may become relevant, again, for later program revi-
sions. As a result, expensive re-selections of eventually “lost” test cases may become necessary
during regression testing. Hence, excessive TSR, although potentially improving testing eﬃ-
ciency for one program version, may, eventually, have a negative impact on regression testing.
On the other hand, too reluctant TSR may lead to an ever-growing test suite thus also increas-
ing test-selection eﬀort during regression testing. Those subtle interactions between TSR and
regression testing are neither obvious nor fully predictable in advance. Finding a feasible com-
bination of both strategies yielding a suitable eﬃciency/eﬀectiveness trade-oﬀ throughout the
entire version history of a program is an open challenge involving further ﬁne-grained details
to be taken into account (e.g., code-coverage criteria, TSR heuristics, RTS criteria etc.).

Contributions. We present a conceptual framework for an in-depth investigation of interac-
tions between strategies for TSR and RTS and the resulting impact on (mutually contradicting)
eﬃciency and eﬀectiveness measures. We focus on unit testing of C programs and use practi-
cally established control-ﬂow coverage criteria as eﬀectiveness measure for RTS and TSR [4].
As eﬀectiveness heuristics for RTS, we consider modiﬁcation-traversing as well as modiﬁcation-
revealing test cases [2].

In our experimental comparison of the diﬀerent strategies, we additionally consider fault-
detection rate as a-posteriori eﬀectiveness measure. Concerning testing eﬃciency, we measure
sizes of test suites (i.e., number of test cases) as well as the computational eﬀort (i.e., CPU-time)
for (regression) test-case generation and/or selection as well as TSR.

To summarize, we make the following contributions.

RegreTS

Page 3 / 39

• Conﬁgurable framework for unit-test generation and selection, integrating recent TSR

strategies, as well as existing and novel RTS strategies.

• Tool support for automatically applying diﬀerent strategies to a version history of C

program units.

• Experimental evaluation results gained from applying our tool to a collection of C

program units with available version history. The evaluation results show that:

(1) TSR based on a coverage-based notion of test-case redundancy almost always de-
creases eﬀectiveness of RTS, yet obviously having a positive impact on testing eﬃciency,

(2) modiﬁcation-revealing test cases are far more eﬀective than modiﬁcation-traversing
test cases for RTS, yet modiﬁcation-revealing test-case generation requires much more
computational eﬀort and

(3) the number of test cases and the number of previous program versions considered for
RTS only has a low impact on the eﬀectiveness of regression testing.

Outline. The remainder of this paper is structured as follows.
Section 2 contains an overview on the necessary background and terminology used in the
remainder of the paper. We ﬁrst introduce an illustrative example to introduce essential notions
and concepts and also use this example to derive the motivation for the methodology proposed
in the main part of the paper.
Section 3 contains a conceptual description of the proposed evaluation framework for investi-
gating the interaction between TSR and RTS strategies. After a general overview, the diﬀerent
possible strategies are then described in more detail using the illustrative example.
Section 4 provides a detailed description of our tool support for the envisioned methodology
together with experimental evaluation results gained from applying the tool to a collection of
subject systems.
Section 5 provides an overview about related work from the ﬁelds of regression testing strate-
gies, automated test-case generation, as well as related approaches for regression veriﬁcation.
Section 6 concludes the paper and provides a brief outlook on possible future work.

Veriﬁability. To make our results reproducible, we provide the tool implementation and all
experimental results as well as raw data on a supplementary web page1.

2 Background and Motivation

We ﬁrst describe the necessary background for the rest of this paper. We introduce an illustra-
tive running example by means of a small, evolving C program unit with corresponding unit
test cases. Based on this example, we describe basic notions of unit testing, test-suite reduction
and regression testing. We conclude by summarizing the research challenges addressed in the
remainder of this paper.

Program Units. Let us consider the sample C program unit in Fig. 1a. This source code 2
constitutes the initial version P0 of an evolving program developed and (re-)tested in an in-
cremental manner. Function find last receives as inputs an integer-array x[] and an integer
value y and is supposed to return the index of the last occurrence of the value of y in x[]. The
ﬁrst element of the array is supposed to carry as meta-information the length (i.e., the number
of elements) of the array. For this reason, the ﬁrst element should be ignored during the search

1https://www.es.tu-darmstadt.de/regrets
2We will use the terms program, source code and program unit interchangeably for convenience

RegreTS

Page 4 / 39

for the value of y. The search itself is supposed to be performed by iterating over the array
and by keeping track of the latest occurrence of y 3. Additionally, the function should return
predeﬁned error codes:

• If the size of the array (i.e., the value of the ﬁrst element) is less or equal to zero, the

returned error-code is -1.

• If the value of y does not occur in the array, the returned error-code is -2.

Program Bugs. The initial program version P0 in Fig. 1a does, however, not satisfy the
speciﬁed functionality as it contains the following three bugs.

1. Bug 1 : The search index starts at 0, thus incorrectly including the value of the meta-

information into the search (see line 5).

2. Bug 2 : The search index stops at x[0] − 2 thus incorrectly excluding the last element

from the search (see line 5).

3. Bug 3 : The search incorrectly matches all values smaller than, or equal to, y instead of

solely considering values equal to y (see line 6).

To ﬁx those bugs, assume the developer to consecutively creates program revisions by correcting
erroneous code parts.

Program Revisions. Let us assume that three consecutive revisions, P1, P2 and P3, of
the initial program version P0 have been created, where P3 ﬁnally has all three bugs ﬁxed as
described above. A program revision denotes a new program version as a result of modiﬁcations
made to some parts of the previous program version. We represent program revisions using the
established diff-syntax frequently used in patch ﬁles. Line numbers marked with post-ﬁx --
refer to the respective lines in the previous program version being removed in the new version,
and those with post-ﬁx ++ refer to lines in the new version being added to the previous version:

• Patch 1 in Fig. 1b provides a ﬁx for Bug 1.

• Patch 2 in Fig. 1d provides a ﬁx for Bug 2.

• Patch 3 in Fig. 1f provides a ﬁx for Bug 3.

After the ﬁrst bug ﬁx, we obtain the improved (yet still faulty) program version P1 (cf. Fig. 1c).
The second bug ﬁx to P1 (yielding program version P2, cf. Fig. 1e) and the third bug ﬁx to
P2 ﬁnally yield the bug-free version P3 (cf. Fig. 1g). Such an incremental process is often
interleaved by consecutive unit-testing steps to assure correctness of new versions and/or to
reveal further bugs potentially emerging during revisions.

Unit Testing. Test cases for unit testing by means of program inputs are usually selected
with respect to (structural) code coverage criteria. For instance, we require at least two diﬀerent
test cases for branch coverage of program version P0. First, we require a test case for cover-
ing (reaching) the true-branch of the if-statement in line 2 (i.e., the input-array is empty).
Additionally, we require at least one further test case for covering the remaining branches as
follows.

3Of course, a more reasonable implementation may perform a reversed iterative search and return the ﬁrst
occurrence. The purpose of the example is to demonstrate essential testing concepts in a condensed way thus
requiring some simpliﬁcations which do, however, not threaten the validity of the overall approach.

RegreTS

Page 5 / 39

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

1
2
3
4
5
6
7
8
9

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =0; i <= x [0] -2; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(a) Initial Version P0 of a Program Unit

5--:
5++:

for ( int i =0; i <= x [0] -2; i ++
for ( int i =1; i <= x [0] -2; i ++

(b) First Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -2; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(c) New Program Version P1 After Applying the First Bug Fix

5--:
5++:

for ( int i =1; i <= x [0] -2; i ++
for ( int i =1; i <= x [0] -1; i ++

(d) Second Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -1; i ++)
if ( x [ i ] <= y )
last = i ;
return last ;

}

(e) New Program Version P2 After Applying the Second Bug Fix

6--:
6++:

if ( x [ i ] <= y )
if ( x [ i ] == y )

(f) Third Bug Fix

int find_last ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;
int last = -2;
for ( int i =1; i <= x [0] -1; i ++)
if ( x [ i ] == y )
last = i ;
return last ;

}

(g) New Program Version P3 After Applying the Third Bug Fix

Figure 1: Program Versions P0, P1, P2 and P3 with their corresponding Bug Fixes

RegreTS

Page 6 / 39

• The false-branch of the if-statement in line 2 (i.e., requiring the input-array to have at

least one element).

• The true-branch of the for-loop in line 5 (i.e., requiring the input-array to have at least

two elements).

• The false-branch of the for-loop in line 5 (i.e., requiring the input-array to have at least

one element).

• The true-branch of the if-statement in line 6 (i.e., requiring the input-array to have at

least one element being less or equal to y).

• The false-branch of the if-statement in line 6 (i.e., requiring the input-array to have at

least one element not being less or equal to y).

To satisfy branch coverage on P0, a developer/tester may select a test suite consisting of

the following two test cases 4:
• t1= (x=[0], y=0),

• t2= (x=[3,5,5,3], y=4).

We denote test cases t as collections of input-value assignments (i.e., an array x and a value for
y). Test-case speciﬁcations are often further equipped with the expected output values (i.e.,
last=-1 for t1 and last=-2 for t2). If applied to P0, t1 would pass, whereas t2 would indeed
fail as it produces the erroneous return value 0 instead of the expected value -2. Hence, this
test suite, although satisfying branch coverage, only reveals Bug 1 and Bug 3, but not Bug 2.
In contrast, a test suite containing as test cases:

• t1= (x=[0], y=0),

• t3= (x=[1,1,1], y=2),

• t4= (x=[1,2,2], y=0).

also satisﬁes branch coverage and reveals Bug 1 and Bug 2, but not Bug 3. Hence, this test
suite is similarly eﬀective as the ﬁrst (revealing two out of three bugs), yet being less eﬃcient
as it causes more testing eﬀort due to the additional test-case execution. The test suite:

• t1= (x=[0], y=0),

• t2= (x=[3,5,5,3], y=4),

• t3= (x=[1,1,1], y=2).

satisﬁes branch coverage and reveals all three bugs, since t2 detects Bug 1 and Bug 3, and t3
detects Bug 1 and Bug 2. In contrast, the test suite

• t1= (x=[0], y=0),

• t5= (x=[3,1,1,2], y=1).

also satisﬁes branch coverage, but reveals none of the three bugs, since t1 is unable to reach
any bug and the execution of t5 (accidentally) produces a correct output for all versions. These
examples illustrate the well-known dilemma of unit-test selection that (1) adherence to (purely
syntactic) code-coverage criteria does not guarantee eﬀective test-suite selections [7, 8], and (2)
using more test cases does not necessary increase eﬀectiveness, yet obviously decreases testing
eﬃciency. Concerning (2), TSR aims at removing redundant test cases from a test suite without
decreasing code coverage [4].

4More precisely, this test suite is only able to reach Line 8 due to Bug 3

RegreTS

Page 7 / 39

Test-Suite Reduction. Let us now assume that a developer/tester ﬁrst selects the test cases
t1 and t2 for testing program version P0, where t2 fails due to Bugs 1 and 3. After applying the
ﬁrst bug ﬁx to remove Bug 1, the initial test suite consisting of t1 and t2 does no more satisfy
branch coverage. This is due to the fact, that after the ﬁrst bug ﬁx is applied, the ﬁrst element
(i.e., the meta-information) of the input array is no more included into the search. As the last
element is also not included (due to Bug 2), execution of t2 will not enter the true-branch of
the if-statement in line 6, and is, therefore, unable to reach 100% branch coverage. Hence,
the developer/tester has to select a further test case, for instance:

• t6= (x=[3,0,1,0], y=0)

to cover the missing branch. Thus, the existing test case t2 becomes redundant and might be
removed from the test suite as t1 and t6 are suﬃcient for 100% branch coverage.

More generally, TSR is concerned with selecting from an existing set of test cases a suﬃcient,
yet preferably small number of test cases for a program under test. Corresponding strategies
for TSR have to address several challenges:

• Finding a minimal set of test cases from an existing test suite satisfying a given coverage

criterion is NP-hard, being reducible to the minimum set-cover problem [5].

• As illustrated by the running example, deﬁning redundancy of test cases only with respect
to a code-coverage criterion might be misleading thus obstructing testing eﬀectiveness [7,
8].

A further challenge arises in the context of evolving programs. Let us next assume that bug
ﬁx 2 is applied to remove Bug 2. After that, the new test case t6 is no more able to reveal the
remaining Bug 3, as the last element of the array is equal to y and, therefore, the test output
(accidentally) satisﬁes the speciﬁcation. In contrast, the previously removed test case t2 would
have revealed Bug 3, since the last element is smaller than y, thus leading to the output of 3
instead of −2. This example shows that TSR solely based on structural coverage criteria deﬁned
on a current program version might be problematic in case of evolving programs. Test cases
being redundant for a current program version might become relevant again after a program
In particular, regression testing 5 is concerned with selecting a suitable set of test
revision.
cases after program revisions [2, 9].

Regression Testing. As illustrated by the previous example, after revealing a bug due to
a failed test-case execution, a developer/tester consecutively creates program revisions to ﬁx
the bugs 6. After creating a program revision, the current test suite is re-executed to assure
modiﬁcations made during the revision (1) successfully ﬁx a discovered bug (i.e., test cases pre-
viously failing now pass) and (2) do not introduce new bugs (i.e., test cases previously passing
still pass). Those test cases from the existing test suite should be re-executed investigating
program parts aﬀected by modiﬁcations made during the revision. New test cases might be
required to investigate existing and/or newly introduced program parts not yet and/or no more
covered by existing test cases. In both cases, one distinguishes between modiﬁcation-traversing
test cases and modiﬁcation-revealing test cases [2]. Executions of modiﬁcation-traversing test
cases at least reach a program modiﬁcation, but may, however, not reveal the modiﬁcation to
a tester (i.e., the program versions before and after the revision may produce the same output

5The term regression testing often summarizes a wide range of diﬀerent disciplines of testing evolving programs
not only including RTS, but also test-case prioritization, test-history analysis, test-artifact storage etc. We will
focus on to the core problem of selecting test cases for regression testing.

6For the sake of clarity, we assume in the following that the granularity of each individual modiﬁcation made
during a revision is limited to one particular line of code. However, this assumption does not possess any threat
to the validity of our approach.

RegreTS

Page 8 / 39

values for the same test inputs). In contrast, executions of modiﬁcation-revealing test cases not
only reach a program modiﬁcation, but also yield diﬀerent output values when applied before
and after the revision.

For instance, recall the case in which the initial test suite consisting of t1 and t2 is applied
to P0. As t2 fails on P0, a developer might perform the ﬁrst bug ﬁx, leading to P1. Next assume
that t1 and t2 are simply reused for testing P1. Both test cases would pass now although the
second and third bug are still present in P1. This is because t1 is unable to reach the bug and t2
is unable to detect the bug as the last element which would reveal Bug 3 is not included in the
search due to Bug 2. The output yielded by t2, therefore, also conforms to the speciﬁcation.
In contrast, selecting t3 in addition to, or instead of, t2 would also reveal the second bug on
P1, thus enabling the developer to perform a further bug ﬁx leading to P2. However, after this
bug ﬁx, t3 becomes unable to detect the last bug as there are no values contained in x having
a smaller value than y. In contrast, t2 is now able to detect the last bug.

The previous example shows that eﬀectiveness of a test suite not only depends on the
particular test cases, but also on the current program version under test. Hence, fault-detection
probability of test cases may both decay as well as improve over time.

To generalize, RTS is concerned with selecting a suﬃcient, yet preferably small number of
existing/new test cases to be (re-)executed on a revised program. Strategies for RTS have to
address several challenges:

• Finding a minimal modiﬁcation-traversing set of test cases from an existing test suite is
NP-hard and does, by deﬁnition, not guarantee eﬀective assurance of program revisions.

• Finding any modiﬁcation-revealing test case for a program modiﬁcation corresponds to

the program-equivalence problem which is undecidable.

• In a practical setting, the aforementioned idealized assumptions on program revisions do
usually not hold: units may contain several bugs which inﬂuence or even obfuscate each
other and not all modiﬁcations applied during a program revision are actually bug ﬁxes.

Test-Suite Reduction vs. Regression-Test Selection Both techniques aim at improving
testing eﬃciency by reducing the number of test cases to be executed without presumably
harming eﬀectiveness:

• TSR strategies aim at selecting from an existing test suite of one program version a
presumably small subset being suﬃcient to satisfy the given code-coverage criterion.

• RTS strategies aim at selecting from an existing test suite of previous program versions
and/or new test cases a presumably small subset being suﬃcient to traverse/reveal critical
program modiﬁcations in the current program revision.

The previous examples illustrate the subtle interplay between both strategies aﬀecting eﬃciency
and eﬀectiveness in non-obvious ways:

• Removing too few test cases during TSR might obstruct the gain in eﬃciency of subse-
quent RTS as well as TSR steps due to critically increasing overhead required for those
strategies in case of ever-growing test suites.

• Removing too many test cases during TSR might obstruct eﬀectiveness of subsequent
RTS as well as TSR steps as currently redundant test cases might become relevant again.

We next present a novel evaluation framework to systematically investigate these interac-

tions.

RegreTS

Page 9 / 39

Figure 2: Overview of the Evaluation Methodology

3 Evaluation Methodology

We present a conﬁgurable framework for systematically addressing the challenges described
in the previous section. The methodology allows us to evaluate interactions between diﬀerent
strategies for TSR and RTS with respect to practical eﬃciency and eﬀectiveness measures. We
ﬁrst present a conceptual overview and then describe the provided parameters for adjusting
the strategies.

3.1 Overview

Figure 2 provides an overview of our methodology using the running example introduced in
the previous section.

Starting from the initial program version P0, the subsequent program versions, P1, P2,
P3, . . ., result from applying consecutive revisions, given as patches Patch1, Patch2, Patch3.
For each program version Pi, we consider a corresponding test suite Ti containing the test
cases selected for this particular program version. Based on the initial test suite T0 created
for P0 (e.g., using either a coverage criterion like branch coverage, or randomly generated test
cases, or other criteria), the regression test suites Ti for testing subsequent program versions
Pi, i > 0, result from applying a RTS strategy. As a complementing step, a TSR strategy may
be applied to test suites Ti of program versions Pi to remove redundant test cases.

Our methodology comprises strategies for both TSR as well as RTS. The parameters for
ﬁne-tuning the strategies are denoted as circled numbers in Fig. 2. We brieﬂy describe these
parameters which will be explained in more detail below.

1○ Reduction Strategy (RS): Technique used for TSR. Possible strategies: None, ILP,

Patch 1 Patch 2 Patch 3  𝑻𝟏 𝒕𝟏: ([0],0)  𝒕𝟐: ([3,5,5,3],4)  𝑻𝟐 𝒕𝟏: ([0],0)  𝒕𝟐: ([3,5,5,3],4) 𝒕𝟔: ([3,0,1,0],0)     𝑻′𝟐 𝒕𝟔: ([3,0,1,0],0)    5 1  𝑻′′′𝟑 𝒕𝟖: ([2,0,0],1)  1 ≠ 2 ≠ 2  𝑻𝟑 𝒕𝟏: ([0],0)  𝒕𝟐: ([3,5,5,3],4) 𝒕𝟔: ([3,0,1,0],0)  𝒕𝟕: ([1,0],1) 𝒕𝟖: ([2,0,0],1)  𝒕𝟗: ([2,1,1],2) 𝒕𝟏𝟎: ([3,2,1,0],3)   𝑻′′𝟑 𝒕𝟔: ([3,0,1,0],0)  𝒕𝟕: ([1,0],1) 𝒕𝟖: ([2,0,0],1)  𝒕𝟗: ([2,1,1],2) 𝒕𝟏𝟎: ([3,2,1,0],3 ≠ 2  𝑻′𝟑 𝒕𝟕: ([1,0],1)   1 𝑷𝟎 𝑷𝟏 𝑷𝟐 𝑷𝟑 4 3 𝑻𝟎 𝒕𝟏: ([0],0)  𝒕𝟐: ([3,5,5,3],4)   …  𝑻′𝟏 𝒕𝟐: ([3,5,5,3],4)   1 4 3 RegreTS

Page 10 / 39

FAST++, and DIFF.

2○ Regression Test-Case Selection Criterion (RTC): New regression test cases for
a program version Pi may be added to test suite Ti either by means of modiﬁcation-
traversing test cases (i.e., at least reaching the lines of code modiﬁed from Pi−1 to Pi) or
by modiﬁcation-revealing test cases (i.e., yielding diﬀerent outputs if applied to Pi and
Pi−1).

3○ Number of Regression Test Cases (NRT): The number of diﬀerent regression test
cases added into Ti satisfying RTC for each previous program version Tj, 0 ≤ j < i.

4○ Number of Previous Program Revisions (NPR): The (maximum) number of pre-
vious program versions Pj (i − NPR ≤ j < i) for all of which NRT diﬀerent test cases
satisfying RTC are added to Ti.

5○ Continuous Reduction (CR): Controls whether the non-reduced test suite Ti−1 (No-
CR) or the reduced test suite T (cid:48)
i−1 (CR) of the previous program Pi−1 version is (re-)used
for the next program version Pi or if the previous test cases are ignored (None). This
test suite is extended by new test cases according to the previously described parameters.

Note that parameter CR can be applied for all test suites of all versions. However, for clarity
and space reasons, it is only depicted from program version P2 on in Fig. 2.

3.2 Test-Suite Reduction Strategies

We now describe the diﬀerent TSR strategies supported by our framework. We, again, use an
illustrative example to explain the impact of the strategies on the trade-oﬀ between precision
and computational eﬀort. We ﬁrst give a general characterization of the test-suite minimization
problem for one single program P , a given test suite T and a code-coverage criterion on P [4].

Input: Program P , Test Suite T , where

• P contains of a set of test goals G = {g1, g2, . . . , gn} according to the given code-coverage

criterion and

• test suite T = t1, t2, . . . , tn consists of a set of test cases, where each tj ∈ T covers a
subset Gj ⊆ G of test goals on P such that for each gi ∈ G there exists at least one test
case tj ∈ T with gi ∈ Gj

7.

Output: Minimal Test Suite T (cid:48) ⊆ T , where

• for each gi ∈ G there exists at least one test case tj ∈ T (cid:48) with gi ∈ Gj and

• for each T (cid:48)(cid:48) ⊆ T also satisfying the ﬁrst property, it holds that |T (cid:48)| ≤ |T (cid:48)(cid:48)|.

The test-suite minimization problem is NP-hard being reducible to the minimum set cover
problem [4] such that ﬁnding exact minimal solutions is computational infeasible for realis-
tic programs. Various TSR heuristics have been proposed for approximating minimal test
suites constituting diﬀerent trade-oﬀs between precision (deviation from exact solutions) and
computational eﬀort for performing the reduction.

To illustrate the diﬀerent approaches in our framework, P1 from our running example

together with T3 containing four test cases selected for branch coverage:

• t1= (x=[0], y=0),

7Set G actually contains the subsets of test goals covered by at least one test case in T .

RegreTS

Page 11 / 39

Figure 3: Comparision of Test-Suite Reduction Strategies

• t2= (x=[3,5,5,3], y=4),

• t3= (x=[1,1,1], y=2), and

• t4= (x=[1,2,2], y=0).

Figure 3 (on the left) illustrates the test-suite minimization problem. Program version P1
contains three conditional branches (i.e., if-statement in line 2, loop-head in line 6 and if-
statement in line 7), leading to six goals G = {g1, g2, . . . , g6}. A line between test case ti and
test goal gj indicates that gj ∈ Gi (i.e., ti satisﬁes gj). Test suite T3 indeed satisﬁes branch
coverage on P1 as for each g ∈ G, there is at least one test case in T3. However, T3 is not
minimal as, for instance, t2 and t4 cover exactly the same test goals, g2, g3, g4 and g6, and this
set is further subsumed by the test goals satisﬁed by t3. In fact, t3 is the only test case covering
g5 and t1 is also indispensable as it is the only test case satisfying g1. Hence, T3 = {t1, t3} is
the (unique) minimal test suite as shown on the left.

Computing the minimal test suite requires, in the worst case, to enumerate all 2|T | subsets of
T which is infeasible in case of realistic programs. Practical approaches usually compute reduced
test suites approximating the minimal solution. We now describe the strategies considered in
our framework.

ILP Strategy. The test-suite minimization problem can be encoded as Integer linear op-
timization problem which can be precisely solved using Integer Linear Programming (ILP)
solvers [5]. Our ILP encoding uses for each test case ti ∈ T a decision variable xi either having
value 1 if ti is selected, or value 0 if ti is not selected. The ILP formula contains for each
test goal gj ∈ G a clause building the sum of the decision variables xi of all test cases ti ∈ T
for which gj ∈ Gi holds. By requiring the value of each such sum to be greater than 0, we
ensure each test goal to be covered by the minimal test suite. To enforce minimality, the overall
optimization objective is to minimize the sum over all values of decision variables xi.

Applied to our example, this encoding introduces the variables x1, x2, x3, x3 for the test

cases in T and adds the following clauses for the test goals:

x1 >= 1
x2 + x3 + x4 >= 1
x2 + x3 + x4 >= 1
x2 + x3 + x4 >= 1
x3 >= 1
x2 + x3 + x4 >= 1

(1)

(2)

(3)

(4)

(5)

(6)

(7)

Minimal ILP FAST++ DIFF Program Version 𝑃1 Test Goals 𝑔1 𝑔2 𝑔3 𝑔4 𝑔5 𝑔6 Test Suite T 𝑡1 𝑡2 𝑡3 𝑡4 𝑡1 𝑡1 𝑡1 𝑡1 𝑡2 𝑡3 𝑡3 𝑡3 𝑡3 RegreTS

Page 12 / 39

Table 1: Vector Encoding of Test Cases
6
Test Case \ Value
0
(t1)
2
(t2)
0
(t3)
0
(t4)

1
0
0
3
1

0
2
0
0
1

5
0
1
0
0

2
0
0
1
2

3
0
2
0
0

Table 2: Vector Encoding of Test Cases after Random Projection

Test Case \ Value
(t1)
(t2)
(t3)
(t4)

x
0
0
0
0

y
0
0
3
2

z
0
5
2
0

The optimization objectives is deﬁned as:

min(x1 + x2 + x3 + x4).

As illustrated in Fig.3, recent ILP solvers are able to deliver an exact minimal solution. How-
ever, large amount of test cases naturally lead to high computational eﬀort. We discuss two
TSR heuristics to avoid intractable computational eﬀort in case of larger-scaled problems, yet
providing an acceptable approximation of the exact (minimal) solution.

FAST++ Strategy. The FAST++ approach encodes the selection of test cases into a (re-
duced) test suite using the Euclidean distance of a reduced vector-space model computed by
random projection [10]. We also illustrate this approach using our example. Test cases are
encoded as vectors, where the number of elements corresponds to the number of diﬀerent input
values of all test cases. The ﬁrst element of each vector denotes the number of occurrences of
the lowest input value (e.g., for our running example the number ’0’). The next element of
each vector is the next lowest input value (e.g., for our running example the number ’1’). The
encoding of all test cases is shown in Tab. 1 and the result of reducing the dimensions of the
vector-space model by random projection is shown in Tab. 2.

Based on this encoding, TSR works as follows. First, a random test case (e.g., t2) is selected
into the (initially empty) reduced test suite. For the selected test case, the Euclidean distances
to all other remaining test cases is computed based on the reduced vector-space model. The
probability of selecting a particular remaining test case next into the reduced test suite increases
with the distance value (thus preferring test cases covering test goals being dissimilar to those
covered by previously selected ones). In our example, this would be, for instance, test case t1.
This iterative step is repeated until all test goals are covered. For instance, the next test case
might be t3 which suﬃces to achieve full branch coverage thus leading to termination. This
technique is, on average, more eﬃcient than ILP, yet potentially leading to less precise results
as demonstrated by the example. There is not even a guarantee to ﬁnd local optima as random
projection may obfuscate necessary information. Additionally, the ordering of input values is
ignored.

DIFF Strategy. This strategy also incrementally selects test cases until all test goals are
covered [11]. In contrast to FAST++, DIFF is a purely greedy-based approach which always
selects as next test case one that covers a maximum number of uncovered test goals. Applied
to our example, DIFF would, for instance, perform the following selections:

1. t3 (covering 5 uncovered test goals),

RegreTS

Page 13 / 39

2. t1 (covering 1 uncovered test goal).

This technique is, in general, similarly eﬃcient and (im-)precise as FAST++, where this par-
ticular example is well-suited for DIFF as the local optimum is the same as the global one.
However, if local and global optima diﬀer, there is no guarantee about the optimality of the
result.

The diﬀerent TSR technique might lead to diﬀerent results in terms of the number of test
cases of the reduced test suite. Although, this clearly leads to diﬀerent results in terms of
eﬃciency (in terms of test-suite size) it also might aﬀect eﬀectiveness. For example, test case
t2 does not cover additional test goals and also does not detect any bugs in program version P1.
However, in program version P2 it will again detect Bug 3. We chose those three TSR strategies
as they provide diﬀerent trade-oﬀs between the eﬀectiveness of reduction (i.e., how small the
resulting test-suite will be) and eﬃciency in terms of CPU time needed for reduction. [10,
12] Since ILP provides an optimal solution, the resulting test-suite will be minimal, however,
leading to much more computational eﬀort. The greedy approach (DIFF) is more eﬃcient,
however, the resulting test-suite might be larger compared to ILP. FAST++ is even more
eﬃcient compared to the greedy approach, however, leading to to even larger test-suites.

3.3 Regression-Test Selection Strategies

The TSR strategies considered so far are concerned with selecting from an existing test suite of
one single program version a reduced subset preserving test coverage on that program version.
In contrast, RTS strategies are concerned with selecting from an existing test suite of a previ-
ous program version a suﬃcient set of test cases to investigate modiﬁcations applied to that
program version leading to a subsequent version. If no such test case(s) can be found among
the existing ones, new test cases must be created and added to the test suite of the subsequent
program version. We next describe diﬀerent RTS strategies supported by our framework. We
use an illustrative example to explain the impact of the strategies on the trade-oﬀs between pre-
cision and computational eﬀort. We start with a general characterization of the regression-test
selection problem, consisting of a program version P with existing test suite T and subsequent
version P (cid:48) for which a regression test suite T (cid:48) is created [2].

Input: Program version Pi−1 with Test Suite Ti−1 and subsequent program version Pi.
Output: Regression Test Suite Ti = T (cid:48)
i−1 ∪ T (cid:48)
selected for reuse from Ti−1 and T (cid:48)

i are newly added test cases.

i−1 ⊆ Ti−1 are existing test cases

i , where T (cid:48)

Consider P3 from our running example together with test suite T3. Figure 4 provides an
illustration of the RTS problem. For P3, the selected test suite contains three modiﬁcation-
traversing test cases (i.e., t2, t3 and t4) being able to at least traverse the modiﬁed line of
code. However, only t2 is modiﬁcation revealing as it not only reaches the modiﬁcation, but
also disposes a diﬀerence in the output behavior of P3 as compared to P2. Hence, depending
on the RTS strategy, also new test cases have to be generated for T (cid:48)
i .

Regression-Test-Case Generation. We now describe how we automatically generate re-
gression test cases utilizing oﬀ-the-shelf test-generation tools originally designed for covering
test goals encoded as reachability properties. We utilize a software model-checker for C pro-
grams to automatically derive test cases from counterexamples for (non-) reachability queries
of test goals [13]. We encode the problem of generating regression test cases for diﬀerent ver-
sions of a program in a generic comparator-function. The shape of the comparator-function
Pi,j for the program versions Pi and Pj depends on parameter RTC. We illustrate this using
the comparator-function P2,3 for our running example.

RegreTS

Page 14 / 39

Figure 4: Comparison of Regression-Test Selection Strategies

1
2
3
4
5
6
7
8
9
10
11
12

int find_last_p2_3 ( int x [] , int y ) {

if ( x [0] <= 0)
return -1;

int last = -2;
for ( int i =1; i <= x [0] -1; i ++) {

test_goal :
if ( x [ i ] == y )
last = i ;

}
return last ;

}

(a) Modiﬁcation-Traversing Comparator-Function for P2 and P3

void find_last_p2_3 ( int x [] , int y ) {

if ( find_last_p2 (x , y ) != find_last_p3 (x , y )
test_goal : printf ( " differencing (cid:32) test (cid:32) case (cid:32) found " ) ;

}

(b) Modiﬁcation-Revealing Comparator-Function for P2 and P3

Figure 5: Comparator-Functions

Program Version 𝑃3 Test Suite 𝑇2 𝑡1 𝑡2 𝑡4 𝑡5 𝑔1 𝑔2 𝑔3 𝑔4 𝑔5 𝑔6 Test Goals 𝑡𝑟𝑢𝑒 𝑡𝑟𝑢𝑒 𝑡𝑟𝑢𝑒 𝑓𝑎𝑙𝑠𝑒 𝑓𝑎𝑙𝑠𝑒 𝑓𝑎𝑙𝑠𝑒 Modification Traversing 𝑡1 𝑡2 𝑡4 𝑡5 Modification Revealing 𝑡1 𝑡2 𝑡4 𝑡5 Program Version 𝑃2 Bug Fix B3 Test Suite 𝑇2 𝑡1 𝑡2 𝑡4 𝑡5 𝑔1 𝑔2 𝑔3 𝑔4 𝑔5 𝑔6 Test Goals 𝑡𝑟𝑢𝑒 𝑡𝑟𝑢𝑒 𝑡𝑟𝑢𝑒 𝑓𝑎𝑙𝑠𝑒 𝑓𝑎𝑙𝑠𝑒 𝑓𝑎𝑙𝑠𝑒 X X X X RegreTS

Page 15 / 39

1
2
3
4
5
6
7
8

int find_last_p3 ( int x [] , int y ) {...}
struct intStruct find_last_p4 ( int x [] , int y ) {...}

void find_last_p3_4 ( int x [] , int y ) {

if ( find_last_p3 (x , y ) != find_last_p4 (x , y )
test_goal : printf ( " differencing (cid:32) test (cid:32) case (cid:32) found " ) ;

}

Figure 6: Invalid Comparator Program

• RTC = MT: The comparator-function P2,3 for generating modiﬁcation-traversing test
cases between P2 and P3 is shown in Figure 5a. The program location aﬀected by the
program modiﬁcation leading from P2 to P3 is marked by a special program label test goal.
A test-case generator can be used to generate a program input reaching this program label
(and therefore at least traversing this modiﬁcation).

• RTC = MR: The comparator-program P2,3 for generating modiﬁcation-revealing test
cases between P2 and P3 is shown in Fig. 5b. Here, the program input is delegated to
P2 and P3 and the output is compared by an equals-function (or != in case of basic
types) based on the return-type. Hence, the special program label test goal in line 3 is
only reachable by a test case if the test input generated by a test-case generator yields
diﬀerent output values for P2 and P3 (thus not only reaching, but also revealing this
modiﬁcation).

Remarks. The problem of generating modiﬁcation-revealing test cases obviously subsumes
the generation of modiﬁcation-traversing test cases and therefore, presumably, causes (much)
more computational eﬀort. In fact, both problems are undecidable as reachability of program
locations is reducible to the halting problem. Hence, the test-generation approach described
above is inherently incomplete 8. Our framework further comprises the parameter NRT to
control the number of multiple diﬀerent test cases distinguishing the output behavior of two
consecutive program versions (which may, however, fail due to the theoretical limitations de-
scribed before). Lastly, the technique based on the comparator-function suﬀers from some
technical limitations. For example, if the data type of the return value changes the compar-
ison of the return values is, in general, no more possible (see Fig. 6, where the return types
of function f ind last p3 and f ind last p4 diﬀer such that the comparator-program would not
compile). Additionally, as both program versions need to be compiled into one single program,
merging of those versions can become technical challenging (e.g., if the data type of global
variables or functions changes or the members of structs are modiﬁed).

Multiple Regression Test-Case Generation. Most test-case generators transform the
input program into an intermediate representation like a control-ﬂow automaton (CFA). To
keep the following examples graspable, we use the basic example shown in Figure 7a instead of
our running example. This CFA corresponds to a small program with input value x returning
0 if x is smaller than zero and the value of x, otherwise. A CFA can be used to identify
all program paths leading to test goals (e.g., all CFA-edges in case of branch coverage) and
to control automated test-suite generation by performing reachability queries for uncovered
test goal [14]. To support parameter NRT, we have to generate multiple diﬀerent test cases
reaching the same test goal (i.e., a modiﬁed program location). We start by generating the
ﬁrst test case based on the original CFA. For instance, to generate a test case reaching the

8This fact, however, is already true for the problem of generating a test case simply reaching a test goal in

one version of a program in a non-regression scenario.

RegreTS

Page 16 / 39

(a) Original CFA

(b) Instrumented CFA

Figure 7: On-the-ﬂy CFA Instrumentation

return-edge in Fig. 7a, any input is feasible (e.g., input -5 would traverse the left path). For
further test cases reaching the return-edge, we modify the CFA as shown in Fig. 7b to exclude
already covered paths. For this, we add for each path for which we already created a test case
a fresh variable with initial value 0 (i.e., variable w1 introduced by the new edge from node 2
to e1 ). We assign value 1 to this variable after traversing the associated branch of the path
(i.e., the new edge from node 4 to e2 ). By further requiring the value of at least one of the
fresh variables before reaching the test goal must not be 1 (i.e., the new edge from node 6 to
e3 ), we exclude all previous test cases reaching that test goal. In the example, assumption
w1!=1 enforces a further test case to assign a positive value to x, whereas a third run of the
test-case generator will ﬁnd no further path reaching this goal.

For example, to generate two test cases reaching line 7 in program version P3, we ﬁrst
generate a test case as usual (e.g., t7= (x=[1,0], y=1)). For the second test case, we introduce
a fresh variable into the CFA which is incremented when traversing a branch of the path already
taken by the previous test case (i.e., the true-branch of the if-statement, the true-branch of
the for-loop, the false-branch of the for-loop and the false-branch of the last if-statement).
As there are four branches, the ﬁnal check will ascertain that the fresh variable is not equal
to 4. When iterating through the for-loop more than once (as opposed to the ﬁrst test case)
or taking the true-branch of the last if-statement, the value will diﬀer from 4 and, therefore,
the path can be taken for a further test case being diﬀerent from all previous ones (e.g., t8=
(x=[2,0,0], y=1)).

Remarks. Again, this methodology is inherently incomplete as it is unknown whether further
test cases exist if no more test cases are found by the test-case generator (e.g., due to time-outs).
In addition, each incrementally added fresh variable presumably increases program complexity
thus potentially increasing the computational eﬀort required for every further test case.

Regression Test-Suite Generation. We conclude by summarizing the description of our
framework. Algorithm 1 depicts a generic procedure for generating regression test suites Ti for
program version Pi meeting all possible instantiations of the parameters RTC, NRT, NPR,
RS and CR. Parameter RTC is either set to modiﬁcation-traversing (MT) or modiﬁcation-
revealing (MR), whereas NRT and NPR can be any number greater than zero (NPR is
naturally restricted to be at most i). Besides the current version Pi, the algorithm receives
as inputs the sequence of patches applied since version P0 as well as the test suite Ti−1 of the
preceding version Pi−1 based on parameter CR. Lastly, RS is set to ILP, DIFF, FAST++

1 2 4 3 5 𝑥<0 !(𝑥<0) 𝑖𝑛𝑡 𝑥 𝑟𝑒𝑡𝑢𝑟𝑛 a 𝑖𝑛𝑡 𝑎 6 7 𝑎= 0 𝑎=x 1 2 3 4 5 𝑥<0 !(𝑥<0) 𝑖𝑛𝑡 𝑥 𝑟𝑒𝑡𝑢𝑟𝑛 a 𝑖𝑛𝑡 𝑎 e3 𝑎=0 𝑎=x 6 e2 𝒘𝟏=𝟏 e1 7 𝒊𝒏𝒕 𝒘𝟏=𝟎 w1!=1 RegreTS

Page 17 / 39

Algorithm 1 Regression Test-Suite Generation
Input: RTC ∈ {MT, MR}, NRT ∈ N+, NPR ∈ {1, . . . , i}
Input: Pi, Patch1, . . . , Patchi
Input: Ti−1
Output: Ti

i−k to Pj

Pj ← apply Patch−1
Pi,j ← Comparator(cid:104)Pi, Pj(cid:105)(RTC)
l ← 0
while l < NRT do

t ← NewTestGen(Pi,j, Ti,j)
if t = null then

1: procedureMain
2: Ti ← Ti−1(CR)
3: Ti,j ← ∅
4: Pj ← Pi
5: for k = 0 to NPR − 1 do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: Ti ← Ti ∪ Ti,j
19: Ti ← Reduction(cid:104)Ti(cid:105)(RS)

end if
Ti,j ← Ti,j ∪ {t}
l ← l + 1

end while

continue in line 5

or None.

j

Test suite Ti is initialized with the test cases from the existing test suite Ti−1 (line 2), and
Pj (initialized in line 4 by Pi) refers to the previous version for which diﬀerentiating test cases
are generated in the next iteration. The outer loop from line 5 to 15 performs a descending
traversal over NPR previous versions by reversely applying the corresponding patches to Pj
(i.e., applying Patch−1
to Pj yields version Pj−1). In line 7, the comparator-function template
is instantiated with Pi and Pj to synthesize a comparator -function Pi,j as input for a coverage-
based test-case generator in line 10 [13]. The inner loop (lines 9–15) repeatedly generates test
cases for Pi,j until at most NRT diﬀerent test cases have been generated (cf. Sect. 3.3). The
test-case generator receives as additional input the set Ti,j of already generated test cases.
If the test-case generator fails to ﬁnd further test cases (either due to a time-out or due to
exhaustive search), the current iteration of the inner loop is terminated and the next iteration
of the outer loop starts (lines 11–12). Otherwise, the new test case is added to Ti,j (line 14)
and ﬁnally to Ti (line 18). Finally, the test-suite is reduced depending on parameter RS in
line 19.

Remarks. For generating multiple diﬀerent test cases for the same modiﬁcation, only the
set Ti,j of already generated diﬀerentiating test cases for the current pair Pi, Pj of program
versions is taken into account. However, the approach may be generalized by blocking the
whole set Ti of already generated test cases in subsequent runs of the test-case generator. This
may, however, lead to drastically increased computational eﬀort and is, therefore, currently not
supported.

3.4 Integration and Discussion of Strategies

The parameters RS, RTC, NRT, NPR and CR allow for adjustments of eﬃciency and
eﬀectiveness achievable by a regression-testing strategy concerning the detection of regression
bugs.

RegreTS

Page 18 / 39

Eﬃciency. Eﬃciency of regression-testing strategies may be measured in terms of (1) the
computational eﬀort (e.g., CPU time) for selecting/generating regression test cases, together
with (2) the number of regression test cases being selected. Concerning (1), strategies with
RTC = MT are presumably more eﬃcient than those with RTC = MR as ﬁnding modiﬁcation-
revealing test cases is, on average, computationally more complicated than only reaching a
modiﬁed program location. Concerning (2), the number of regression test cases to be selected
for each new program version is (at most) NRT · NPR thus presumably growing with increas-
ing values of NRT and NPR. Additionally, strategies with RS (cid:54)= None are presumably less
eﬃcient in terms of CPU time, but presumably much more eﬃcient in terms of the number of
test cases. Finally, CR also presumably decreases eﬃciency in terms of CPU time, however
presumably increases eﬃciency in terms of number of test cases.

Eﬀectiveness. Eﬀectiveness of regression-testing strategies may be measured in terms of the
number of regression bugs detected by the selected regression test cases. Test cases selected
for RTC = MR are presumably more eﬀective than those for RTC = MT. Similarly, fault-
detection rates of regression test suites presumably increase with higher values for NRT (i.e.,
leading to more diﬀerent ways of testing modiﬁcations between program versions) as well as
NPR (i.e., taking more previous program versions into account). For instance, concerning
the revision from P2 to P3 in our example, revealing bug 3 in P2 requires a test case such as
t2. This can be ensured by strategies with RTC = MR, NRT > 0, and NPR > 0 which
are, however, presumably less eﬃcient than strategies with RTC = MT. Finally, RS and
CR might decrease eﬀectiveness as reducing the test suite might remove test cases that would
succeed in ﬁnding the bug (see Sect. 3.2).

In the next section, we empirically investigate these assumptions by presenting the results
of an experimental evaluation gained from applying a tool for our methodology to a collection
of subject systems.

4 Experimental Evaluation

The conﬁgurable framework presented in the previous section allows us to investigate the impact
of the parameters 1○– 5○ (see Sect. 3) on eﬃciency and eﬀectiveness of regression testing. In
our experimental evaluation, we consider version histories of C program units (i.e., function
level). We use real-world systems obtained from GitHub and the change history provided by
git. To systematically compare eﬀectiveness of diﬀerent regression-testing strategies, we further
employ simulated bugs throughout program-version histories and measure the corresponding
fault-detection ratio of the selected test suites. We do this by repeatedly applying standard
mutation operators for C programs. Correspondingly, we measure eﬃciency in terms of the
computational eﬀort for generating regression test cases as well as in terms of the number of
test cases to be (re-)executed throughout the version history.

4.1 Research Questions

We consider the following research questions.

(RQ1) How does the regression-testing strategy impact testing eﬀectiveness?

(RQ2) How does the regression-testing strategy impact testing eﬃciency?

(RQ3) Which regression-testing strategy constitutes the best trade-oﬀ between eﬀectiveness and

eﬃciency?

RegreTS

Page 19 / 39

4.2 Experimental Setup

We next describe the evaluation methods and experimental design used in our experiments to
address these research questions.

Methods and Experimental Design To compare diﬀerent strategies for regression-testing,
we instantiate the ﬁve parameters RTC, NRT, NPR, RS and CR of our methodology as
described in Sect. 3 as follows:

• RTC ∈ {MT, MR} (RTS Generation Criterion).

• NRT ∈ {1, 2, 3} (Number of Regression Test Cases per Revision).

• NPR ∈ {1, 2, 3} (Number of Previous Program Revisions).

• RS ∈ {None, ILP, FAST++, DIFF} (TSR Strategy).

• CR ∈ {CR, No-CR, None} (Continuous Reduction).

To answer RQ1-RQ3, each instantiation of all ﬁve parameters, therefore, corresponds to
one particular regression-testing strategy under consideration, where we denote a strategy
by [RTC, NRT, NPR, RS, CR] for short. We thus obtain 144 reasonable regression-testing
strategies (since the combination of RS = None and CR = CR is meaningless and we do
not take into account strategies having CR = None and RS! = None as reduction of non-
accumulated test-suites is also meaningless). Two of these strategies may be considered as
proper baselines:

• Baseline 1 (basic regression testing strategy):[MT, 1, 1, None, No-CR].

• Baseline 2 (basic regression testing without initial test suite):[MT, 1, 1, None, None].

We limit our considerations to 3 as maximum value for NRT and NPR as the increase of
eﬀectiveness diminishes for higher values of NRT and NPR. Additionally, we do not consider
0 as value for NRT and NPR, as this would obviously result in an empty test-suite.

We further divide RQ2 into the following sub-questions.

• (RQ2.1) How does the regression-testing strategy impact eﬃciency in terms of CPU

time?

• (RQ2.2) How does the regression-testing strategy impact eﬃciency in terms of number

of test-cases?

Note that we consider test-case generation time for CPU time and not test-case execution time.
This is due to the fact, that test-case execution time was negligible during our evaluation.

Subject Systems. We consider program units as our subject systems in terms of testable
functions extracted from real-world C programs for which we further require available version
history. We therefore focus on open-source projects from GitHub. We selected program units
in terms of preprocessed c-ﬁles consisting of an entry-function (i.e., the function-under-test)
as well as all functions within the same compilation unit having (direct or indirect) callee-
dependencies to the function-under-test. To sum up, our subject systems have to fulﬁll the
following requirements to be useable for our evaluation purposes.

• The unit must be processable by the test-case generator used in our framework (e.g.,

syntax must be ANSI C, no multi-threading, etc.).

RegreTS

Page 20 / 39

• The functions-under-test (as well as all callees) must have undergone a version-history of

at least 3 modiﬁcations.

• The signature of the functions-under-test must provide input parameters for which dif-
ferent values will yield diﬀerent return values (or aﬀect the values of at least one global
variable). However, this needs not to be known in advance as the existence of such param-
eters is actually checked during test generation (which may timeout if this requirement
is not met).

• Calling the function-under-test multiple times with the same parameter values produces
the same return value or global-variable values (i.e., no non-deterministic behavior or
external system-function calls are considered).

The resulting collection of subject systems comprises program units from open-source

projects published in GitHub:

• betaﬂight9 contains six program units from the ﬂight controller software betaﬂight.

• netdata10 contains six program units from the infrastructure monitoring and trou-

bleshooting software netdata.

• wrk11 contains one program unit from the HTTP benchmarking tool wrk.

The size of subject systems ranges from 270 to 950 lines of code (after removing unnecessary
code). The number of changes (i.e., commits) for each subject system is between 4 and 18
(only counting commits changing code inside the unit).

Simulating Bugs. Although software evolution and regression testing become more and
more important, properly documented and practically usable histories of program versions
combined with information about real bugs in software projects are still barely available. Hence,
for in-depth investigations of the interactions between program revisions, program bugs and
their detection by regression test cases as required for answering our research questions, we
have to rely on synthetically generated bugs. Mutation testing is a well-established approach
for evaluating eﬀectiveness of testing techniques by simulating common program faults [15].
In particular, mutation-testing tools provide collections of syntactic program-transformation
operations such that the resulting modiﬁed program potentially shows diﬀerent (i.e., erroneous)
output behaviors.

Fortunately, it has been recently shown that mutations provide a reliable substitute for
realistic bugs in testing experiments [15]. We, therefore, utilize concepts from mutation testing
to simulate bugs in our subject systems. Figure 8 provides an overview of our approach. For
each program version other than program version P0, we create mutants containing simulated
bugs. The regression-test generation is then executed on this bugged version and eﬀectiveness
of the resulting test suite is measured by executing the generated/selected regression-test suite
on both the bugged and the bug-ﬁxed (i.e., the original) version and by comparing the return
values (see Sect. 3). If the return values diﬀer, the bug is successfully detected by the test-suite.
We used 62 mutation operators in total (the names of the mutation operators are provided on
our supplementary web page 12), which can be clustered into three diﬀerent types. The ﬁrst
group consists of mutation operators replacing constants and variables (e.g., replace variable a
by variable b, replace constant 5 by constant 6 etc.). The second group consists of mutation
operators replacing operators (e.g., replacing + by −). The last group consists of mutation
operators replacing pointers and array references (e.g., replacing pointer pt1 by pointer pt2).

9https://github.com/betaﬂight/betaﬂight
10https://github.com/netdata/netdata
11https://github.com/wg/wrk
12https://www.es.tu-darmstadt.de/regrets

RegreTS

Page 21 / 39

Figure 8: Bug Simulation using Mutation Testing

Data Collection. For comparing diﬀerent regression-testing strategies, we ﬁrst generate a
faulty revisions Bi from all revisions Pi of each subject system by simulating bugs as described
before. Next, we introduce program labels into each faulty revision Bi marking each line of
code which has been modiﬁed since the last version Pi−1. These labels are used as test goals to
generate modiﬁcation-traversing test cases (similarly for Pi−2 and Pi−3 depending on parameter
NPR). In addition, we generate the comparator -programs described in Sect. 3 for generating
modiﬁcation-revealing test-cases. The comparator -program is further used for comparing test-
case executions on the faulty version Bi and the previous versions Pi−1, Pi−2 and Pi−3. If no
comparator -program can be created (e.g., due to the limitations explained in Sect. 3), no test
suite will be generated for all strategies targeting this speciﬁc comparator-program.

We applied Algorithm 1 to generate regression-test suites T s
i

for every bugged revision Bi

using all 144 strategies.

To answer RQ1, we measure eﬀectiveness of the generated test suites such that test suite
T s
i detects the bug in Bi if it contains at least one test case t whose execution yields diﬀerent
output values when executed on Pi and Bi (denoted Pi(tc) (cid:54)= Bi(tc)). Hence, eﬀectiveness with
respect to a test suite T s
i

is calculated as

where detects(T s
generated for Pi. Eﬀectiveness of a strategy s is calculated as

i , i) = 1 if ∃t ∈ T s
i

eﬀectiveness(T s

(8)
: Pi(t) (cid:54)= Bi(t), or 0 otherwise, and Bi is the faulty revisions

i ) = detects(T s

i , i)

where n is the number of revisions.

eﬀectiveness(s) =

(cid:80)n

i=0 eﬀectiveness(T s
i )
n

(9)

To answer RQ2, we measure eﬃciency in two ways. First, eﬃciency of regression test suites
generated by strategy s for program unit Pi is calculated as the average size of test suites T s
i
in terms of the number of test cases

i ) = |T s
i |
such that the overall eﬃciency of strategy s is calculated as

eﬃciencysize(T s

eﬃciencysize(s) =

(cid:80)n

i=0 eﬃciencysize(T s
i )
n

.

(10)

(11)

Second, eﬃciency of the regression test-suite generation is given as the aggregated CPU time
eﬃciencyCPU consisting of the initialization phase of the software model-checker, the reacha-
bility analysis, and the test-case extraction. While the ﬁrst phase is only executed once for
each program revision under test, the other phases are executed by the number of test goals
multiplied by parameter NRT. Additionally, for practical reasons, the whole process is limited
by a global time-out parameter after which the process terminates without providing further
test cases thus potentially leading to less test cases than speciﬁed by parameter NRT. The

𝑷𝟎 Patch 1 Patch 2 Patch 3 𝑷𝟐 𝑩𝟐 Bug 2 𝑷𝟏 𝑩𝟏 Bug 1 𝑩𝟑 Bug 3 𝑷𝟑 … RegreTS

Page 22 / 39

equation for calculating eﬃciencyCPU of the diﬀerent strategies is
i=0 eﬃciencycpu(T s
i )
n

eﬃciencycpu(s) =

(cid:80)n

.

(12)

Finally, to answer RQ3, we calculate the mean values of each strategy for each program unit
in our subject system for eﬀectiveness and eﬃciency and, thereupon, calculate the trade-oﬀ as

trade-oﬀ(s) =

eﬀectiveness(s)
eﬃciencysize(s)

.

(13)

Tool Support. We implemented Algorithm 1 in a tool, called RegreTS (Regression Testing
Strategies). RegreTS extends the software model-checker CPAchecker for C programs to
generate regression test cases. This is achieved by performing reachability-analysis runs for
program locations marked as test goals (see Sect. 3). Additionally, we use MUSIC [16], a
mutation tool for C programs, to generate synthetic bugs as described above. For measuring
test coverage, we utilize TestCov13, a test-suite executor for C programs. TestCov further
supports TSR with a greedy algorithm, as explained in Sect. 3, called DIFF. Furthermore,
we extended TestCov to also support FAST++ and ILP. All tools are available on our
website14.

Measurement Setup. RegreTS as well as MUSIC and TestCov have been executed on
an Ubuntu 18.04 machine, equipped with an Intel Core i7-7700k CPU and 64 GB of RAM.
The CPU time for test-suite generation is limited to 900 s per revision, and the CPU time for
the execution of the test cases to detect bugs is limited to 30 s per test case. The TSR is not
limited by CPU time as the CPU time needed for reduction was negligible. We executed our
evaluation with Java 1.8.0-171 and limited the Java heap to 15 GB.

4.3 Results

The measurements for RQ1 are depicted in Fig. 9a for all strategies with RTC = MR and in
Fig. 9b for all strategies with RTC = MT. Results for RQ2 are shown in Figs. 10a and 11a
for all strategies with RTC = MR and Figs. 10b and 11b for all strategies with RTC = MT.
The results for RQ3 are shown in Fig. 12a for all strategies with RTC = MR and Fig. 12b for
all strategies with RTC = MT. The box plots in Figs. 10a, 10b, 11a and 11b aggregate the
results after applying the formulas 11 and 12 (see above) for all subject systems. The boxes
depict the range of results while the black dashes depict the mean values. The whiskers show
the minimum and maximum values (excluding outliers).

RQ1 (Eﬀectiveness). The best eﬀectiveness for our subject systems is achieved by the
strategies [MR, 3, 3, None, No-CR], [MR, 2, 3, None, No-CR], [MR, 2, 3, None,
No-CR], [MR, 3, 3, FAST++, CR] and [MR, 3, 3, FAST++, No-CR] with an average bug
detection rate of 0.3659. Compared to the baseline 1 [MT, 1, 1, None, No-CR] with an average
detection rate of 0.306 and baseline 2 [MT, 1, 1, None, None] with an average detection rate of
0.224. The worst performing strategy is [MT, 3, 3, FAST++, CR] with an average detection
rate of 0.119.

RQ2 (Eﬃciency). The best eﬃciency measure concerning CPU time is obtained by strat-
egy [MT, 1, 1, None, No-CR] and [MT, 1, 1, None, None] with an average amount of 6.058s.
The worst performing strategy, [MR, 3, 3, ILP, No-CR], requires 993.229s on average. Con-
cerning test-suite sizes, [MT, 1, 1, ILP, CR], [MT, 2, 1, ILP, CR], and [MT, 3, 1, ILP, CR]

13https://gitlab.com/sosy-lab/software/test-suite-validator
14https://www.es.tu-darmstadt.de/regrets

RegreTS

Page 23 / 39

perform best with an average measure of 0.225 test cases, whereas strategy [MT, 3, 3, None,
No-CR] leads to the largest test-suite sizes with an average measure of 115.44 test cases.

RQ3 (Trade-Oﬀ ). The base trade-oﬀ between eﬀectiveness and CPU time is obtained by
strategy [MT, 1, 1, None, No-CR] with an average of 0.0503 bugs found per second. The
worst performing strategy is [MT, 3, 3, FAST++, CR] with an average of 0.0015 bugs found
per second.

4.4 Discussion and Summary

RQ1 (Eﬀectiveness). Setting parameter RTC to MR increases eﬀectiveness for all strate-
gies. In fact, almost all strategies using MR are more eﬀective than even the most eﬀective
strategy using MT. Indeed, parameter RTC has the highest impact on eﬀectiveness. While
the impact of other parameters is smaller, it is nonetheless also observable. Choosing None
for parameter RS increases eﬀectiveness, as the other parameter value reduces eﬀectiveness by
reducing the number of test cases. Choosing CR or No-CR for parameter CR has nearly no
impact on eﬀectiveness. Strategies with MR and ILP cause a signiﬁcant loss in eﬀectiveness
if CR is selected. Choosing None for parameter CR has a negative impact on strategies with
MR selected, however, only having a small impact if MT is selected. Lastly, parameters NRT
and NPR also increase eﬀectiveness with increasing values (even in case of small increases).

Answer RQ1
The best eﬀectiveness measure is reached by strategy [MR, 3, 3, None, No-CR] (i.e., mod-
iﬁcation revealing test cases, three test cases per test goal, up to three previous revi-
sions and no test-suite reduction) which improves eﬀectiveness compared to the baseline
[MT, 1, 1, None, No-CR] by 19% and compared to the baseline [MT, 1, 1, None, None]
by 61%. The highest impact on eﬀectiveness is caused by parameter RTC

RQ2.1 (CPU Time). Parameter RTC has by far the highest impact on CPU time. When
choosing MT, CPU time increases nearly 20 fold. As expected, parameter NPR increases
the CPU time nearly linearly to its respective value. Unexpectedly, parameter NRT does
not aﬀect CPU time by a large margin. This is most likely due to the fact, that reachability
information computed during the ﬁrst run of the test generator can be re-used for the next test
cases. Parameters CR and RS are negligible in terms of CPU time if MR is selected, since
the number of test-cases remains small. If MT is selected, both parameters impact CPU time,
however still only by a small margin.

Answer RQ2.1
The best eﬃciency measure in terms of CPU time is reached by the baseline strategies
[MT, 1, 1, None, No-CR] and [MT, 1, 1, None, None] (i.e., modiﬁcation traversing test
cases, one test case per test goal, one previous revision and no test-suite reduction, either
ignoring or using the previous test-suite (as this makes no diﬀerent in CPU time)). The
highest impact on CPU time is caused by parameter RTC.

RQ2.2 (Test-Suite Size). The highest impact on eﬃciency in terms of test-suite size is
caused by parameter RS.
If None is selected, no reduction is enabled, and therefore, the
test-suite grows with each version. However, the choice of the technique used for test-suite
reduction only has a small impact on the test-suite size.

Parameter RTC also has a high impact on the test-suite size. This is due to the fact, that
a patch might contain multiple modiﬁcations. Therefore, the number of test cases is higher in
case of MT (i.e., requiring one test-case per modiﬁed line) as compared to MR, where only one
test case is required. Parameter CR also aﬀects test-suite sizes, but only by a small amount.

RegreTS

Page 24 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 9: Results Eﬀectiveness

Strategy0.000.050.100.150.200.250.300.35Strategy0.000.050.100.150.200.250.30RegreTS

Page 25 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 10: Results Generation Time

Strategy05001,0001,5002,0002,500Strategy020406080100120RegreTS

Page 26 / 39

(a) Strategies with RTC = MR

Figure 11: Results Test-Suite Size

(b) Strategies with RTC = MT

Test-Suite SizeStrategy05101520253035404550Strategy020406080100120140160180200RegreTS

Page 27 / 39

(a) Strategies with RTC = MR

(b) Strategies with RTC = MT

Figure 12: Results Bugs per Second

Strategy0.00000.00020.00040.00060.00080.00100.0012Strategy0.0000.0050.0100.0150.0200.0250.0300.0350.0400.0450.050RegreTS

Page 28 / 39

Lastly, parameters NRT and NPR aﬀect the test-suite size almost linearly to their respective
values.

Answer RQ2.2
The best eﬃciency measure in terms of test-suite size is reached by the strategy
[MR, 1, 1, ILP, CR] (i.e., modiﬁcation revealing test cases, one test case per test goal,
one previous revision, ILP as test-suite reduction strategy and using the reduced test-
suite of the previous revision) which improves eﬀectiveness compared to the baseline
[MT, 1, 1, None, No-CR] by 6200% and compared to the baseline [MT, 1, 1, None, None]
by 970%. The highest impact on the test-suite size is caused by parameter RS.

RQ3 (Trade-Oﬀ ). We observe that all parameters have a large impact on the trade-oﬀ be-
tween eﬀectiveness and CPU time. Therefore, the interactions between the diﬀerent parameters
is the main driver aﬀecting the trade-oﬀ.

Answer RQ3
The strategy yielding the best trade-oﬀ is [MT, 1, 1, None, No-CR] (i.e., modiﬁcation
traversing test cases, one test case per test goal, one previous revisions, no test-suite reduc-
tion and using the test-suite of the previous revision as well) for which the fault-detection
capability is acceptable, but the eﬃciency in terms of CPU time is very high. Compared to
the second baseline [MT, 1, 1, None, None] the trade-oﬀ is increased by 36%.

Remarks. Strategy [MT, 3, 3, None, No-CR] is less eﬀective than strategy [MT, 3, 2,
None, No-CR] which, by deﬁnition, is counter-intuitive. This is due to the fact, that some
comparator -programs for comparing a faulty version Bi to a program revision Pi−2 are invalid
(e.g., due to diﬀerent return types), whereas the comparator -program for Bi and program
revisions Pi−3 are actually valid. However, no test cases could be found by the test-case gen-
erator revealing the bug. Hence, the number of test suites for [MT, 3, 3, None, No-CR] and
[MT, 3, 2, None, No-CR] diﬀer slightly, which is the reason for the discrepancy between the
factual results and the theoretical speciﬁcation of the technique. In our subject systems, the
probability that no valid comparator -program can be generated is approximately 8%. There-
fore, while this technical limitation is present, it should not aﬀect the results by a large margin.

4.5 Threats to Validity

Internal Validity. Our regression-testing methodology relies on the assumption that when
diﬀerent program versions are tested with the same test case, all factors (e.g., platform, en-
vironment) that might inﬂuence the output except for the source code itself remain constant.
This so-called controlled-regression-testing assumption is commonly used in regression-testing
experiments and does, therefore, not harm validity of the results [2].

Concerning the soundness of our methodology, we tested the test-generation loop by manu-
ally checking results for selected subject systems. However, due to undecidability of reachability
of program locations, if no more test cases can be found (e.g., due to time-outs or imprecise
counter-examples), it is unknown if further test cases exist. Nevertheless, we expect precision
improvements to not substantially obstruct the (relative) results of our evaluation.

Another threat to validity might arise from our selection of mutation operators and their
applications to our subject systems. However, our selection comprises those mutations leading
to useful results w.r.t. our experimental setting, namely aﬀecting one line of code, performing
no code deletions and producing a compilable result.

Limiting our considerations to (functional) unit testing may also threaten internal validity.
As unit testing constitutes the most established and relevant testing technique in practice, it
is particularly interesting and relevant to investigate our methodology at this level ﬁrst. In
addition, the proposed concepts might be likewise applicable at integration- and system-level.

RegreTS

Page 29 / 39

Additionally, our methodology does not incorporate systematic reusability-checks of existing
test cases for revealing modiﬁcations also in later revisions which may aﬀect eﬃciency measures.
We plan to extend our approach, accordingly, in a future work but we expect similar results as
in our current setting.

Finally, we expect our current focus on C programs to also not seriously harm validity as
we expect similar results for other programming languages, at least for those relying on an
imperative core (e.g., most OO languages).

External Validity. We are not aware of any competitive tools with similar functionality
as RegreTS, especially concerning the generation of a conﬁgurable number of modiﬁcation-
revealing test cases. Surprisingly, it was not possible for us to use other recent test-case
generators for strategies with RTC = MR, which, by design, should have been possible.
This might be due to the fact, that the subject systems are real-world programs, which where
explicitly selected to be processable by our test-case generator. However, test-case generators
are (usually) limited in supporting certain constructs of the C language. We tried to use other
test-case generators (i.e., Klee, FuSEBMC, Symbiotic and PRTest) from the international
testing competition 2021 [17]. However, these test-case generators were barely able to generate
any test cases at all. In addition, successfully generated test cases were actually unable to reveal
modiﬁcations between diﬀerent program version and were thus immediately removed from the
test-suites during TSR (leading to empty test suites). Only PRTest was able to generate some
meaningful test-cases but was, however, also not able to generate any test case for more than
half of the subject systems thus being unusable for a proper comparison. However, the main
focus of this paper is to compare diﬀerent regression-testing strategies, and not to compare
diﬀerent test-case generators for regression-test generation. We thus assume the results to
be very similar for other test-case generation techniques (even though eﬀectiveness in terms
of CPU time might change, the ratio of the CPU time of diﬀerent strategy presumably stay
similar).

Another threat might arise from the selection of subject systems and the usage of simulated
bugs. Unfortunately, real-world systems with suﬃcient information about revisions and bugs
as required for our experiments are barely available. We evaluated three prominent candidates
for potential candidates. First, CoreBench only provides a very short version history (often
only 1–2 versions) and incorporates many bugs being undetectable at unit level (e.g., involving
ﬁles and global errors like overﬂows) [18]. Second, the regression-veriﬁcation tasks from the
SV-Benchmarks [19] also have a small version history and the diﬀerent tasks cannot be executed
in a self-sustained manner as needed to reveal those bugs. However, we spend a lot of time in
searching for other freely available community benchmarks including version history and known
bugs. However, suitable benchmarks are still very rare. Amongst others, we had a look into
further subject systems from the SIR Repository including programs like gzip and make [20],
but either our test-case generator was not able to handle those programs for mostly technical
reasons, or the programs were not suitable for our purposes (see descriptions above). As also
already discussed above, mutation testing is a reliable fault-injection technique for measuring
eﬀectiveness in testing experiments [15].

Finally, our tool relies on third-party software, namely CPAchecker, a software model-
checker, and the mutation tool MUSIC for C programs. However, both tools are established
and have been extensively used for other experiments in the recent past, so we expect them to
produce sound results.

RegreTS

Page 30 / 39

5 Related Work

5.1 Regression-Testing

A comprehensive overview about regression testing is provided by [2], describing three cate-
gories: (1) minimization of test suites as well as (2) selection and (3) prioritization of test
cases for regression testing.

Test-suite minimization is concerned with selecting from an existing test suite a subset
of test cases to reduce the number of redundant test executions during regression testing.
Many works propose heuristics for approximating near-optimal solutions [4, 21–23] for this
NP-complete optimization problem, requiring as inputs an existing test suite and a-priori
deﬁned metrics for measuring eﬀectiveness of test cases. The approaches used in this paper for
TSR have been proposed before. The greedy algorithm as used by DIFF has been introduced
in [11]. To use ILP solving for TSR was initially proposed by [5] and FAST++ has been
introduced by [10]. However, none of these works investigate the interactions between RTS
and TSR techniques as done in this paper.

[24] evaluate existing test-suite-reduction techniques on real-word projects based on their
failed builds. [25] proposed a new technique to reduce test suites based on assertions instead
of structural code coverage to improve eﬀectiveness of the resulting test suite. In an earlier
work, [26] compare and combine TSR and test-case selection to further increase eﬃciency
of regression testing. Our methodology goes beyond their approach as we consider further
strategic parameters which turned out to be very relevant.

Regression-test selection is concerned with selecting from an existing test suite of an
evolving program a subset of test cases to be (re-)executed on a new program version. A
variety of diﬀerent techniques has been applied (e.g., control-ﬂow analysis [27–29] and/or data-
ﬂow analysis [30–33]). Other works take behavior-preserving modiﬁcations (e.g., refactorings)
into account [34], apply RTS to highly-conﬁgurable software [35], and try to ﬁnd pareto-optimal
solutions for multi-objective RTS [36]. However, none of these works aim at generating new
modiﬁcation-revealing test cases to enhance eﬀectiveness of regression testing as done in our
work. In particular, most recent works only guarantee test cases to be modiﬁcation-traversing.

Test-case prioritization is concerned with selecting from an existing test suite a (re-)test-
execution order such that eﬀectiveness of testing increases as quickly as possible over time (e.g.,
to ﬁnd as many faults as fast as possible) [37]. The underlying problem is very similar to mini-
mization/selection problems. Most existing approaches consider code coverage as eﬀectiveness
criterion to statically compute an (a-priori) ordering among test cases [38–43]. In a recent work,
Wang and Zeng propose a dynamic prioritization technique based on fault-detection history
and other properties [44]. In contrast, in our methodology, prioritization is currently out of
scope, but may be easily incorporated during test-case generation using recent approaches.

5.2 Test-Case Generation

A wide range of technique exist for (automatically) generating test cases which we will describe
in the following grouped by the test-case generation technique applied. However, we are not
aware of related works in terms of the multiple test-cases per test goal to increase eﬀectiveness
of the resulting test suite.

RegreTS

Page 31 / 39

5.2.1 Coverage-Based Test-Case Generation

Fuzzing. Fuzzing is currently very popular both in research and practice. The idea is to
quickly generate a large number of test cases by generating (semi-)random input values. In
some approaches, the input values of existing test cases are reused and modiﬁed to generate
new test cases. [45] Recent fuzzing techniques are based on evolutionary algorithms [46] or
context-free grammars of the input data [47]. In addition, grey-box fuzzing [48] and whitebox
fuzzing [47, 49] have been proposed (i.e., fuzzers also considering the source code of the program
under test). However, the primary goal of fuzzing is not to generate test cases for regression
testing systematically traversing / revealing particular program modiﬁcations through diﬀerent
possible paths and/or program versions as done in our approach.

Plain Random. A test-case generation technique that is similar to, yet simpler than, is
plain random test-case generation. [50]. Test cases are randomly generated, and afterwards,
the achieved coverage is measured. This approach is clearly more eﬃcient than test-goal guided
techniques. On the other hand, more complicated test goals are often not reached, as the
chance to generate valid input values reaching those goals is small (e.g., to generate a test
1
case for input value x which evaluates true for x == 1 is
232 for a 32-bit system). Therefore,
generating (multiple diﬀerent) modiﬁcation-revealing test cases is usually extremely expensive
and ineﬀective using purely random approaches.

Symbolic Execution. Symbolic execution employs a symbolic reachability graph cope with
the reachable state space of input programs during test-case generation. One prominent ex-
ample is Klee [51]. Based on symbolic execution, it might be also possible to generate multiple
test cases covering the same goal through diﬀerent paths as done in our work. However, we are
not aware of any recent work going into this direction.

Bounded Model Checking. Another technique to scale test-case generation to larger pro-
grams is bounded model checking [52]. A bounded model checkers also computes the reachable
state space of programs (either in an abstract or concrete representation), where loops are only
explored up to maximum number of iterations k (bound). This enables the model checker
to prove program properties with certainty only within that bound. Such a tool can be also
used for test cases generation similar to symbolic model checking (i.e., by encoding test goals
as reachbility problems, see below). Again, we are not aware of any works using bounded
model checking to generate multiple test cases for the same test goal or program modiﬁcation,
respectively.

Symbolic Model Checking. Another approach to handle larger input programs is symbolic
model checking as applied, for instance, by the CPAchecker framework, which is also used by
our approach [13]. CoVeriTest is another recent test-case generator based on the CPAchecker
framework [53]. Again, these and other tools do currently not support generation of multiple
test-cases per test goal or program modiﬁcation as required in our approach. However, encoding
the underlying problem as reachability query as done in our approach would also enable the
usage of these other tools for regression-test generation.

5.2.2 Regression-Test-Case Generation.

We next discuss related work on generating test cases for systematically investigating semantic
diﬀerences between similar programs. Diﬀerential testing [54] is concerned with the following
problem: Given two comparable programs and an set of diﬀerential test cases, the systems can
be checked for bugs by running the test cases. If the outputs diﬀer or the test loops indeﬁnitely

RegreTS

Page 32 / 39

or crashes, the test case is a candidate for a bug-revealing test. Thereupon, Evans and Savioa
proposed an approach for detecting program changes by comparing test-execution results of
two diﬀerent program versions [55]. The tool CSmith combines diﬀerential testing with fuzzing
(i.e., C programs) to ﬁnd bugs in C-compiler implementations [56]. The work being presumably
most closely related to our methodology is DiffGen [57] for generating test cases comparing
two versions of a Java program. This is achieved by instrumenting programs with equality-
assert-statements and generating test suites for statement coverage on (failed) assertions. This
work diﬀers from ours as it does not support multiple test cases ﬁnding diﬀerences between
program versions, and also does not take multiple prior versions into account. Additionally,
they do not take TSR into account to increase eﬃciency of regression testing.

The goal of mutation testing is to measure eﬀectiveness of test suites or test-generation
techniques, respectively, by deriving from an original program a set of syntactically slightly
changed mutants (simulating faults) [58]. A test case detects a mutant if its test-execution
results for the original program diﬀer from those for the mutant. Based on this principle, [59]
pursue mutation-driven generation of test cases for Java programs by using genetic algorithms
to ﬁnd test cases that detect mutants. In contrast, [60] use a combination of symbolic execution
and a search-based heuristic to identify test cases that are likely to reveal mutants.
[61] also
propose a heuristic for generating strong mutation-detecting test cases using hill climbing.
As these works are mainly based on heuristics, the generated test cases do not guarantee to
traverse/reveal program modiﬁcations. Additionally, those approaches do not allow to conﬁgure
the number of test cases or the number of diﬀerent programs as our parameters NPR and NRT,
nor do they consider TSR.

Automatically generating test cases for diﬀerentiating two program versions for regression
testing has been initially proposed by [62]. Their white-box testing tool automatically compares
output values of two given program versions to derive input values leading to diﬀerent outputs.
Their approach is applicable to Pascal programs only and does not support multiple test cases
and/or program versions as in our work.

5.3 Regression Veriﬁcation

The goal of regression veriﬁcation is to analyze diﬀerent versions of a system or program to
check whether the speciﬁcation is still fulﬁlled after modiﬁcations. To this end, intermediate
veriﬁcation results are re-used between versions to increase eﬃciency [63–65]. For instance,
intermediate results (so-called abstraction precisions) of veriﬁcation runs enable reuse for later
version [66], and regression veriﬁcation may be applied to restrict the set of program inputs
manually [67] or by syntactic checks [68]. Furthermore, there is work on lifting principles
of regression veriﬁcation to multi-threaded programs [69] and re-checking evolving software
of automated production systems [70]. Moreover, eﬃciency of regression veriﬁcation may be
improved, for instance, by applying state-space partitioning-techniques [71] and by improved
encodings of reuse-information [72]. Other works in this area reuse ﬁnal veriﬁcation results in
case of a subsequent change in the program [14, 73–76]. However, none of these approaches
further utilizes the information collected for regression analysis to derive modiﬁcation-revealing
test cases.

Similarly, conditional model checking aims at reusing results of veriﬁcation runs to perform
collaborative veriﬁcation [77, 78]. To this end, there are exchange formats for veriﬁcation
witnesses for property violations [79] as well as for correctness proofs [80]. Furthermore, there
is work on keeping track of unveriﬁed parts of a program to apply test-case generation for these
parts [81, 82] and reusing veriﬁcation results for hybrid systems [83]. However, none of these
approaches aim at ﬁnding diﬀerences between versions of the same program as done in our
approach.

RegreTS

Page 33 / 39

6 Conclusion and Future Work

We presented a conﬁgurable regression-testing methodology for automating the selection of
regression test-cases with a particular focus on revealing regression bugs in evolving programs.
RegreTS currently supports regression testing of C programs at unit level. Our experimental
results show that eﬀectiveness and the eﬃciency highly depend on the selected regression
test-case generation strategy, where the parameter RTC (i.e., either generating modiﬁcation-
traversing or modiﬁcation-revealing test cases) has the strongest impact on the eﬀectiveness
and eﬃciency. To conclude, our experimental results show that for obtaining the best eﬃciency
in terms of CPU time and the best trade-oﬀ modiﬁcation traversing test cases should be used
without test-suite reduction. However, for the best eﬀectiveness, modiﬁcation revealing test
cases should be used with multiple test cases per test goal and multiple previous revisions taken
into account. Additionally, test-suite reduction obstructs eﬀectiveness, therefore, for optimal
eﬀectiveness test-suite reduction should be disabled.

As a future work, we plan to extend our approach. First, we plan to further improve
our test-generation technique to support additional subject systems as well as in utilizing
alternative test-case generation techniques within our methodology (e.g., symbolic execution)
and to compare the outcome with our current results. Furthermore, we plan to investigate
other kinds of regression errors and to identify suitable regression strategies for eﬀectively
revealing those bugs. Finally, we plan to adapt RegreTS to other testing scenarios including,
for instance, other input languages besides C and other testing levels beyond unit testing which
enables us to conduct experiments on a richer set of subject systems.

Acknowledgments This work was funded by the Hessian LOEWE initiative within the
Software-Factory 4.0 project.

References

[1] Paul Ammann and Jeﬀ Oﬀutt. Introduction to Software Testing. Cambridge University

Press, 2016. isbn: 978-1-107-17201-2.

[2] Shin Yoo and Mark Harman. “Regression Testing Minimization, Selection and Prioritiza-
tion: A Survey”. In: Softw. Test. Verif. Reliab. 22.2 (2012), pp. 67–120. issn: 0960-0833.
doi: 10.1002/stv.430.

[3] Emelie Engstr¨om, Mats Skoglund, and Per Runeson. “Empirical Evaluations of Regres-
sion Test Selection Techniques: A Systematic Review”. In: Proceedings of the Second
ACM-IEEE International Symposium on Empirical Software Engineering and Measure-
ment. ACM, 2008, pp. 22–31. isbn: 978-1-59593-971-5. doi: 10.1145/1414004.1414011.

[4] Mary Jean Harrold, Rajiv Gupta, and Mary Lou Soﬀa. “A Methodology for Controlling
the Size of a Test Suite”. In: ACM Trans. Softw. Eng. Methodol. 2.3 (1993), pp. 270–285.
issn: 1049-331X. doi: 10.1145/152388.152391.

[5] Alireza Khalilian and Saeed Parsa. “Bi-criteria Test Suite Reduction by Cluster Analysis
of Execution Proﬁles”. In: Advances in Software Engineering Techniques. Springer Berlin
Heidelberg, 2012, pp. 243–256. isbn: 978-3-642-28038-2.

[6] Saswat Anand et al. “An orchestrated survey of methodologies for automated software
test case generation”. In: Journal of Systems and Software 86.8 (2013), pp. 1978–2001.
issn: 0164-1212. doi: https://doi.org/10.1016/j.jss.2013.02.061.

[7] Fabiano Pecorelli, Fabio Palomba, and Andrea De Lucia. “The Relation of Test-Related
Factors to Software Quality: A Case Study on Apache Systems”. In: Empirical Software
Engineering 26.2 (2021). doi: 10.1007/s10664-020-09891-y.

RegreTS

Page 34 / 39

[8] Laura Inozemtseva and Reid Holmes. “Coverage is Not Strongly Correlated with Test
Suite Eﬀectiveness”. In: Proceedings of the 36th International Conference on Software En-
gineering. Association for Computing Machinery, 2014, pp. 435–445. isbn: 9781450327565.
doi: 10.1145/2568225.2568271.

[9] Jung-Min Kim and A. Porter. “A history-based test prioritization technique for regression
testing in resource constrained environments”. In: Proceedings of the 24th International
Conference on Software Engineering. ICSE 2002. Association for Computing Machinery,
2002, pp. 119–129. isbn: 158113472X. doi: 10.1109/ICSE.2002.1007961.

[10] Emilio Cruciani, Breno Miranda, Roberto Verdecchia, and Antonia Bertolino. “Scalable
Approaches for Test Suite Reduction”. In: 2019 IEEE/ACM 41st International Confer-
ence on Software Engineering (ICSE). IEEE Press, 2019, pp. 419–429. doi: 10.1109/
ICSE.2019.00055.

[11] Dirk Beyer and Thomas Lemberger. “TestCov: Robust Test-Suite Execution and Cov-
erage Measurement”. In: Proceedings of the 34th IEEE/ACM International Conference
on Automated Software Engineering. IEEE Press, 2019, 1074–1077. isbn: 9781728125084.
doi: 10.1109/ASE.2019.00105.

[12] Zhenyu Chen, Xiaofang Zhang, and Baowen Xu. “A Degraded ILP Approach for Test

Suite Reduction.” In: Jan. 2008, pp. 494–499.

[13] Dirk Beyer, Adam J. Chlipala, Thomas A. Henzinger, Ranjit Jhala, and Rupak Majum-
dar. “Generating Tests from Counterexamples”. In: Proceedings of the 26th International
Conference on Software Engineering. IEEE Computer Society, 2004, pp. 326–335. isbn:
978-0769521633. doi: 10.5555/998675.999437.

[14] Dirk Beyer, Andreas Holzer, Michael Tautschnig, and Helmut Veith. “Information Reuse
for Multi-goal Reachability Analyses”. In: ESOP ’13. Vol. 7792. Springer, 2013, pp. 472–
491. isbn: 978-3-642-37036-6. doi: 10.1007/978-3-642-37036-6_26.

[15] James H. Andrews, Lionel C. Briand, and Yvan Labiche. “Is mutation an appropri-
ate tool for testing experiments? [software testing]”. In: Proceedings. 27th International
Conference on Software Engineering, 2005. ICSE 2005. IEEE, 2005, pp. 402–411. doi:
10.1109/ICSE.2005.1553583.

[16] Duy Loc Phan, Yunho Kim, and Moonzoo Kim. “MUSIC: Mutation Analysis Tool with
High Conﬁgurability and Extensibility”. In: 2018 IEEE International Conference on
Software Testing, Veriﬁcation and Validation Workshops. IEEE, 2018, pp. 40–46. doi:
10.1109/ICSTW.2018.00026.

[17] Dirk Beyer. “Status Report on Software Testing: Test-Comp 2021”. In: Fundamental
Approaches to Software Engineering. Springer International Publishing, 2021, pp. 341–
357. isbn: 978-3-030-71500-7. doi: 10.1007/978-3-030-71500-7_17.

[18] Marcel B¨ohme and Abhik Roychoudhury. “CoREBench: Studying Complexity of Regres-
sion Errors”. In: Proceedings of the 23rd ACM/SIGSOFT International Symposium on
Software Testing and Analysis. ACM, 2014, pp. 105–115.

[19] Dirk Beyer. SV-Benchmarks: Benchmark Set of 8th Intl. Competition on Software Veri-

ﬁcation (SV-COMP 2019). 2019. doi: 10.5281/zenodo.2598728.

[20] Hyunsook Do, Sebastian G. Elbaum, and Gregg Rothermel. “Supporting Controlled Ex-
perimentation with Testing Techniques”. In: Empirical Software Engineering: An Inter-
national Journal 10.4 (2005), pp. 405–435.

[21] Tsong Yueh Chen and Man Fai Lau. “Dividing Strategies for the Optimization of a Test
Suite”. In: Inf. Process. Lett. 60.3 (1996), pp. 135–141. issn: 0020-0190. doi: 10.1016/
S0020-0190(96)00135-4.

RegreTS

Page 35 / 39

[22] Joseph R. Horgan and Saul London. “A data ﬂow coverage testing tool for C”. In: [1992]
Proceedings of the Second Symposium on Assessment of Quality Software Development
Tools. IEEE, 1992, pp. 2–10. doi: 10.1109/AQSDT.1992.205829.

[23] A. Jeﬀerson Oﬀutt, Jie Pan, and Jeﬀrey M. Voas. “Procedures for reducing the size
of coverage-based test sets”. In: Twelfth International Conference on Testing Computer
Software. 1995, pp. 111–123.

[24] August Shi, Alex Gyori, Suleman Mahmood, Peiyuan Zhao, and Darko Marinov. “Evalu-
ating Test-Suite Reduction in Real Software Evolution”. In: Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis. Association for
Computing Machinery, 2018, pp. 84–94. isbn: 9781450356992. doi: 10.1145/3213846.
3213875.

[25] Junjie Chen, Yanwei Bai, Dan Hao, Lingming Zhang, Lu Zhang, and Bing Xie. “How Do
Assertions Impact Coverage-Based Test-Suite Reduction?” In: 2017 IEEE International
Conference on Software Testing, Veriﬁcation and Validation (ICST). IEEE, 2017. doi:
10.1109/icst.2017.45.

[26] August Shi, Tifany Yung, Alex Gyori, and Darko Marinov. “Comparing and Combin-
ing Test-Suite Reduction and Regression Test Selection”. In: Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering. Association for Computing
Machinery, 2015, pp. 237–247. isbn: 9781450336758. doi: 10.1145/2786805.2786878.

[27] J. Hartmann and David J. Robson. “Revalidation during the software maintenance
phase”. In: Proceedings. Conference on Software Maintenance - 1989. IEEE, 1989, pp. 70–
80. doi: 10.1109/ICSM.1989.65195.

[28] J. Hartmann and David J. Robson. “RETEST-development of a selective revalidation pro-
totype environment for use in software maintenance”. In: Twenty-Third Annual Hawaii
International Conference on System Sciences. Vol. 2. IEEE, 1990, 92–101 vol.2. doi:
10.1109/HICSS.1990.205179.

[29] J. Hartmann and David J. Robson. “Techniques for selective revalidation”. In: IEEE

Software 7.1 (1990), pp. 31–36. issn: 0740-7459. doi: 10.1109/52.43047.

[30] Rajiv Gupta, Mary Jean Harrold, and Mary Lou Soﬀa. “An approach to regression testing
using slicing”. In: Proceedings Conference on Software Maintenance 1992. IEEE Com-
puter Society, 1992, pp. 299–308. doi: 10.1109/ICSM.1992.242531.

[31] Mary Jean Harrold and M. L. Souﬀa. “An incremental approach to unit testing during
maintenance”. In: Proceedings. Conference on Software Maintenance, 1988. IEEE, 1988,
pp. 362–367. doi: 10.1109/ICSM.1988.10188.

[32] Mary Jean Harrold and Mary Lou Soﬀa. “Interprocedual Data Flow Testing”. In: Pro-
ceedings of the ACM SIGSOFT ’89 Third Symposium on Software Testing, Analysis, and
Veriﬁcation. ACM, 1989, pp. 158–167. isbn: 0-89791-342-6. doi: 10.1145/75308.75327.

[33] Abu-Bakr Taha, Stephen M. Thebaut, and Sying-Syang Liu. “An approach to software
fault localization and revalidation based on incremental data ﬂow analysis”. In: [1989]
Proceedings of the Thirteenth Annual International Computer Software Applications Con-
ference. IEEE, 1989, pp. 527–534. doi: 10.1109/CMPSAC.1989.65142.

[34] Kaiyuan Wang, Chenguang Zhu, Ahmet Celik, Jongwook Kim, Don Batory, and Milos
Gligoric. “Towards Refactoring-aware Regression Test Selection”. In: Proceedings of the
40th International Conference on Software Engineering. ACM, 2018, pp. 233–244. isbn:
978-1-4503-5638-1. doi: 10.1145/3180155.3180254.

RegreTS

Page 36 / 39

[35] Dusica Marijan and Marius Liaaen. “Practical Selective Regression Testing with Eﬀective
Redundancy in Interleaved Tests”. In: Proceedings of the 40th International Conference
on Software Engineering: Software Engineering in Practice. Association for Computing
Machinery, 2018, 153–162. isbn: 9781450356596. doi: 10.1145/3183519.3183532.

[36] Ankur Choudhary, Arun Prakash Agrawal, and Arvinder Kaur. “An Eﬀective Approach
for Regression Test Case Selection Using Pareto Based Multi-objective Harmony Search”.
In: Proceedings of the 11th International Workshop on Search-Based Software Testing.
ACM, 2018, pp. 13–20. isbn: 978-1-4503-5741-8. doi: 10.1145/3194718.3194722.

[37] Weichen E. Wong, Joseph R. Horgan, Saul London, and Aditya P. Mathur. “Eﬀect of
Test Set Minimization on Fault Detection Eﬀectiveness”. In: 1995 17th International
Conference on Software Engineering. ACM, 1995, pp. 41–41. doi: 10 . 1145 / 225014 .
225018.

[38] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary Jean Harrold. “Test case
prioritization: an empirical study”. In: Proceedings IEEE International Conference on
Software Maintenance - 1999 (ICSM’99). ’Software Maintenance for Business Change’
(Cat. No.99CB36360). IEEE, 1999, pp. 179–188. doi: 10.1109/ICSM.1999.792604.

[39] Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary J. Harrold. “Prioritizing
test cases for regression testing”. In: IEEE Transactions on Software Engineering 27.10
(2001), pp. 929–948. issn: 0098-5589. doi: 10.1109/32.962562.

[40] Sebastian Elbaum, David Gable, and Gregg Rothermel. “Understanding and measuring
the sources of variation in the prioritization of regression test suites”. In: Proceedings Sev-
enth International Software Metrics Symposium. IEEE Computer Society, 2001, pp. 169–
179. doi: 10.1109/METRIC.2001.915525.

[41] Sebastian Elbaum, Alexey Malishevsky, and Gregg Rothermel. “Incorporating varying
test costs and fault severities into test case prioritization”. In: Proceedings of the 23rd
International Conference on Software Engineering. ICSE 2001. IEEE Computer Society,
2001, pp. 329–338. doi: 10.1109/ICSE.2001.919106.

[42] Alexey Malishevsky, Gregg Rothermel, and Sebastian Elbaum. “Modeling the cost-beneﬁts
tradeoﬀs for regression testing techniques”. In: International Conference on Software
Maintenance, 2002. Proceedings. IEEE, 2002, pp. 204–213. doi: 10.1109/ICSM.2002.
1167767.

[43] Gregg Rothermel, Sebastian Elbaum, Alexey Malishevsky, Praveen Kallakuri, and Brian
Davia. “The impact of test suite granularity on the cost-eﬀectiveness of regression test-
ing”. In: Proceedings of the 24th International Conference on Software Engineering. ICSE
2002. ACM, 2002, pp. 130–140. doi: 10.1145/581356.581358.

[44] Xiaolin Wang and Hongwei Zeng. “History-Based Dynamic Test Case Prioritization
for Requirement Properties in Regression Testing”. In: 2016 IEEE/ACM International
Workshop on Continuous Software Evolution and Delivery (CSED). IEEE, 2016, pp. 41–
47. doi: 10.1109/CSED.2016.016.

[45] Jun Li, Bodong Zhao, and Chao Zhang. “Fuzzing: a survey”. In: Cybersecurity 1.1 (2018).

doi: 10.1186/s42400-018-0002-y.

[46] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuﬀrida, and Her-
bert Bos. “VUzzer: Application-aware Evolutionary Fuzzing”. In: Proceedings 2017 Net-
work and Distributed System Security Symposium. Internet Society, 2017. doi: 10.14722/
ndss.2017.23404.

RegreTS

Page 37 / 39

[47] Patrice Godefroid, Adam Kiezun, and Michael Y. Levin. “Grammar-Based Whitebox
Fuzzing”. In: Proceedings of the 29th ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation. Association for Computing Machinery, 2008, 206–215.
isbn: 9781595938602. doi: 10.1145/1375581.1375607.

[48] Micha(cid:32)l Zalewski. Technical ”whitepaper” for aﬂ-fuzz. Tech. rep. 2006. url: https : / /

lcamtuf.coredump.cx/afl/technical_details.txt.

[49] Patrice Godefroid, Michael Y. Levin, and David Molnar. “Automated Whitebox Fuzz

Testing”. In: Network Distributed Security Symposium (NDSS). 2008.

[50] Thomas Lemberger. “Plain random test generation with PRTest”. In: International Jour-
nal on Software Tools for Technology Transfer (2020). doi: 10 . 1007 / s10009 - 020 -
00568-x.

[51] Cristian Cadar, Daniel Dunbar, and Dawson Engler. “KLEE: Unassisted and Automatic
Generation of High-Coverage Tests for Complex Systems Programs”. In: 8th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 08). Vol. 8. USENIX
Association, 2008, 209–224.

[52] Mikhail R. Gadelha, Rafael Menezes, Felipe R. Monteiro, Lucas C. Cordeiro, and Denis
Nicole. “ESBMC: Scalable and Precise Test Generation based on the Floating-Point
Theory”. In: Fundamental Approaches to Software Engineering. Springer International
Publishing, 2020, pp. 525–529. isbn: 978-3-030-45234-6. doi: 10 . 1007 / 978 - 3 - 030 -
45234-6_27.

[53] Marie-Christine Jakobs. “CoVeriTest with Dynamic Partitioning of the Iteration Time
Limit (Competition Contribution)”. In: Fundamental Approaches to Software Engineer-
ing. Vol. 12076. Springer International Publishing, 2020, pp. 540–544. isbn: 978-3-030-
45234-6. doi: 10.1007/978-3-030-45234-6\_30.

[54] William M. McKeeman. “Diﬀerential Testing for Software”. In: Digital Technical Journal

10 (1998), pp. 100–107.

[55] Robert B. Evans and Alberto Savoia. “Diﬀerential Testing: A New Approach to Change
Detection”. In: The 6th Joint Meeting on European Software Engineering Conference and
the ACM SIGSOFT Symposium on the Foundations of Software Engineering: Companion
Papers. ACM, 2007, pp. 549–552. isbn: 978-1-59593-812-1. doi: 10 . 1145 / 1295014 .
1295038.

[56] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. “Finding and Understanding
Bugs in C Compilers”. In: Proceedings of the 32Nd ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation. ACM, 2011, pp. 283–294. isbn: 978-1-
4503-0663-8. doi: 10.1145/1993498.1993532.

[57] Kunal Taneja and Tao Xie. “DiﬀGen: Automated Regression Unit-Test Generation”. In:
2008 23rd IEEE/ACM International Conference on Automated Software Engineering.
IEEE, 2008, pp. 407–410. doi: 10.1109/ASE.2008.60.

[58] Yue Jia and Mark Harman. “An Analysis and Survey of the Development of Mutation
Testing”. In: IEEE Transactions on Software Engineering 37.5 (2011), pp. 649–678. issn:
0098-5589. doi: 10.1109/TSE.2010.62.

[59] Gordon Fraser and Andreas Zeller. “Mutation-driven Generation of Unit Tests and Ora-
cles”. In: Proceedings of the 19th International Symposium on Software Testing and Anal-
ysis. ACM, 2010, pp. 147–158. isbn: 978-1-60558-823-0. doi: 10.1145/1831708.1831728.
[60] Mark Harman, Yue Jia, and William B. Langdon. “Strong Higher Order Mutation-based
Test Data Generation”. In: Proceedings of the 19th ACM SIGSOFT Symposium and the
13th European Conference on Foundations of Software Engineering. ACM, 2011, pp. 212–
222. isbn: 978-1-4503-0443-6. doi: 10.1145/2025113.2025144.

RegreTS

Page 38 / 39

[61] Francisco Carlos M. Souza, Mike Papadakis, Yves Le Traon, and M´arcio E. Delamaro.
“Strong Mutation-based Test Data Generation Using Hill Climbing”. In: Proceedings of
the 9th International Workshop on Search-Based Software Testing. ACM, 2016, pp. 45–
54. isbn: 978-1-4503-4166-0. doi: 10.1145/2897010.2897012.

[62] Bogdan Korel and Ali M. Al-Yami. “Automated Regression Test Generation”. In: Pro-
ceedings of the 1998 ACM SIGSOFT International Symposium on Software Testing and
Analysis. ACM, 1998, pp. 143–152. isbn: 0-89791-971-8. doi: 10.1145/271771.271803.

[63] R. H. Hardin, R. P. Kurshan, K. L. McMillan, J. A. Reeds, and N. J. A. Sloane. “Eﬃcient

regression veriﬁcation”. In: WODES ’96. IEE, 1996, pp. 157–150.

[64] Thomas A. Henzinger, Ranjit Jhala, Rupak Majumdar, and Marco A. A. Sanvido. “Ex-
treme Model Checking”. In: Veriﬁcation: Theory and Practice. Vol. 2772. LNCS. Springer,
2003, pp. 332–358. isbn: 978-3-540-39910-0. doi: 10.1007/978-3-540-39910-0_16.

[65] Ofer Strichman and Benny Godlin. “Regression Veriﬁcation - A Practical Way to Verify
Programs”. In: VSTTE ’05. Vol. 4171. LNCS. Springer, 2008, pp. 496–501. isbn: 978-3-
540-69149-5. doi: 10.1007/978-3-540-69149-5_54.

[66] Dirk Beyer, Stefan L¨owe, Evgeny Novikov, Andreas Stahlbauer, and Philipp Wendler.
“Precision Reuse for Eﬃcient Regression Veriﬁcation”. In: ESEC/FSE ’13. ACM, 2013,
pp. 389–399. isbn: 978-1-4503-2237-9. doi: 10.1145/2491411.2491429.

[67] Marcel B¨ohme, Bruno C. d. S. Oliveira, and Abhik Roychoudhury. “Partition-based
Regression Veriﬁcation”. In: ICSE ’13. IEEE Press, 2013, pp. 302–311. isbn: 978-1-4673-
3076-3. doi: 10.5555/2486788.2486829.

[68] Benny Godlin and Ofer Strichman. “Regression veriﬁcation: proving the equivalence of
similar programs”. In: Software Testing, Veriﬁcation and Reliability 23.3 (2013), pp. 241–
258. doi: 10.1002/stvr.1472.

[69] Sagar Chaki, Arie Gurﬁnkel, and Ofer Strichman. “Regression Veriﬁcation for Multi-
threaded Programs”. In: VMCAI ’12. Vol. 7148. Springer, 2012, pp. 119–135. isbn: 978-
3-642-27940-9. doi: 10.1007/978-3-642-27940-9_9.

[70] Bernhard Beckert, Mattias Ulbrich, Birgit Vogel-Heuser, and Alexander Weigl. “Regres-
sion Veriﬁcation for Programmable Logic Controller Software”. In: ICFEM ’15. Vol. 9407.
Springer International Publishing, 2015, pp. 234–251. isbn: 978-3-319-25423-4. doi: 10.
1007/978-3-319-25423-4_15.

[71] John Backes, Suzette Person, Neha Rungta, and Oksana Tkachuk. “Regression Veriﬁ-
cation Using Impact Summaries”. In: SPIN ’13. Vol. 7976. Springer, 2013, pp. 99–116.
isbn: 978-3-642-39176-7. doi: 10.1007/978-3-642-39176-7_7.

[72] Dennis Felsing, Sarah Grebing, Vladimir Klebanov, Philipp R¨ummer, and Mattias Ul-
brich. “Automating Regression Veriﬁcation”. In: ASE ’14. ACM, 2014, pp. 349–360. isbn:
978-1-4503-3013-8. doi: 10.1145/2642937.2642987.

[73] Willem Visser, Jaco Geldenhuys, and Matthew B. Dwyer. “Green: Reducing, Reusing
and Recycling Constraints in Program Analysis”. In: FSE ’12. ACM, 2012, 58:1–58:11.
isbn: 978-1-4503-1614-9. doi: 10.1145/2393596.2393665.

[74] Ondrej Sery, Grigory Fedyukovich, and Natasha Sharygina. “Incremental Upgrade Check-
ing by Means of Interpolation-based Function Summaries”. In: FMCAD ’12. IEEE, 2012,
pp. 114–121.

[75] Guowei Yang, Matthew B. Dwyer, and Gregg Rothermel. “Regression Model Checking”.

In: ICSM ’19. IEEE, 2009, pp. 115–124. doi: 10.1109/ICSM.2009.5306334.

RegreTS

Page 39 / 39

[76] Steven Lauterburg, Ahmed Sobeih, Darko Marinov, and Mahesh Viswanathan. “Incre-
mental State-Space Exploration for Programs with Dynamically Allocated Data”. In:
ICSE ’08. ACM, 2008, pp. 291–300. doi: 10.1145/1368088.1368128.

[77] Dirk Beyer, Thomas A. Henzinger, M. Erkan Keremoglu, and Philipp Wendler. “Con-
ditional Model Checking: A Technique to Pass Information between Veriﬁers”. In: Pro-
ceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of
Software Engineering. Association for Computing Machinery, 2012. isbn: 9781450316149.
doi: 10.1145/2393596.2393664.

[78] Maria Christakis, Peter M¨uller, and Valentin W¨ustholz. “Collaborative Veriﬁcation and
Testing with Explicit Assumptions”. In: FM ’12. Vol. 7436. Springer, 2012, pp. 132–146.
isbn: 978-3-642-32759-9. doi: 10.1007/978-3-642-32759-9_13.

[79] Dirk Beyer, Matthias Dangl, Daniel Dietsch, Matthias Heizmann, and Andreas Stahlbauer.
“Witness Validation and Stepwise Testiﬁcation Across Software Veriﬁers”. In: ESEC/FSE
’15. ACM, 2015, pp. 721–733. isbn: 978-1-4503-3675-8. doi: 10.1145/2786805.2786867.

[80] Dirk Beyer, Matthias Dangl, Daniel Dietsch, and Matthias Heizmann. “Correctness Wit-
nesses: Exchanging Veriﬁcation Results Between Veriﬁers”. In: FSE ’16. ACM, 2016,
pp. 326–337. isbn: 978-1-4503-4218-6. doi: 10.1145/2950290.2950351.

[81] Mike Czech, Marie-Christine Jakobs, and Heike Wehrheim. “Just Test What You Cannot
Verify!” In: FASE ’15. Vol. 9033. Springer, 2015, pp. 100–114. isbn: 978-3-662-46675-9.
doi: 10.1007/978-3-662-46675-9_7.

[82] Maria Christakis, Peter M¨uller, and Valentin W¨ustholz. “Guiding Dynamic Symbolic
Execution Toward Unveriﬁed Program Executions”. In: ICSE ’16. ACM, 2016, pp. 144–
155. isbn: 978-1-4503-3900-1. doi: 10.1145/2884781.2884843.

[83] Stefan Mitsch, Grant Olney Passmore, and Andr´e Platzer. “Collaborative Veriﬁcation-
Driven Engineering of Hybrid Systems”. In: Mathematics in Computer Science 8.1 (2014),
pp. 71–97. issn: 1661-8289. doi: 10.1007/s11786-014-0176-y.

