An Investigation on Non-Invasive Brain-Computer
Interfaces: Emotiv Epoc+ Neuroheadset and Its
Effectiveness

Md Jobair Hossain Faruk
Dept. of Software Engineering and Game Dev.
Kennesaw State University
Marietta, USA
mhossa21@students.kennesaw.edu

Maria Valero
Dept. of Information Technology
Kennesaw State University
Marietta, USA
mvalero2@kennesaw.edu

Hossain Shahriar
Dept. of Information Technology
Kennesaw State University
Marietta, USA
hshahria@kennesaw.edu

Abstract—Neurotechnology describes as one of the focal points
of today’s research of hundreds of academicians and researchers
globally; particularly around the domain of Brain-Computer
Interfaces (BCI) that was introduced in the 1970s and has been
at the forefront of many neurotechnological discoveries. The
primary attempts of BCI research are to decoding human speech
from brain signals, implementing creativity by imagination, and
controlling neuro-psychological patterns by utilizing billion of
neural activities that would signiﬁcantly beneﬁt people suffering
from neurological disorders. In this study, we illustrate the
progress of BCI research and present scores of unveiled contem-
porary approaches. First, we explore a decoding natural speech
approach that is designed to decode human speech directly from
the human brain onto a digital screen introduced by Facebook
Reality Lab and University of California San Francisco. Then, we
study a recently presented visionary project to control the human
brain using Brain-Machine Interfaces (BMI) approach. We
also investigate well-known electroencephalography (EEG) based
Emotiv Epoc+ Neuroheadset to identify six emotional parameters
including engagement, excitement, focus, stress, relaxation, and
interest using brain signals by experimenting the neuroheadset
among three human subjects where we utilize two two super-
vised learning classiﬁers, Na¨ıve Bayes and Linear Regression to
show the accuracy and competency of the Epoc+ device and
its associated applications in neurotechnological research. We
present experimental studies and the demonstration indicates
69 % and 62 % improved accuracy for the aforementioned
classiﬁers respectively in reading the performance matrices of
the participants. We envision that non-invasive, insertable, and
low-cost BCI approaches shall be the focal point for not only
an alternative for patients with physical paralysis but also
understanding the brain that would pave us to access and control
the memories and brain somewhere very near.

Index Terms—Brain-computer interface, brain activities, elec-

troencephalography, neuroheadset technology

I. INTRODUCTION
The past decade has seen a tremendous explosion of re-
search in neurotechnology by adopting state-of-the-art technol-
ogy, AI, and machine learning that helps to improve the ability
of researchers to conceptualize the Brain-Computer Interfaces
(BCI) [1]–[4]. One of the ambitions of the scientists is to pour

the way of technology in conjunction with the brain that would
lead them translating the speech by utilizing brain activities
in real-time application [5]. The human brain contains about
100 billion neurons or nerve cells and responsible to receive
sensory input from outer environments for transmitting infor-
mation to other organs of the human body to control from
simple survival to complex cognitive functions [6]–[8]. Such
functions make the brain a foremost entity of the human body
for every thought, action, memory, feeling, and everything
related to innermost activities.

Despite the wealth of modern technology, no individual
or mechanism can control people’s desires, actions, feelings,
thoughts, and values [9]. However, the goal of futuristic tech-
nologies is to control one’s brain activities using not only smart
devices, but also emotional parameters of the human being
[10]–[12]. The idea of BCI was introduced by Professor Vidal
[13] in 1973 with a project that presented the ﬁrst systematic
attempt to conceptualize the baselines of direct brain-computer
communication using electroencephalography (EEG). EEG
research can be described as one of the oldest neuro-scientiﬁc
techniques, with the ﬁrst human brain recordings published
by Hans Berger in 1929 [14]. The EEG approach has matured
over the decades, particularly in the 1960s when BCIs research
was developed [15].

Due to the pioneering work in BCIs, the academia and
researchers have started to realize the signiﬁcance of reading
and controlling the brain; consequently, conducting research
and proposing new approaches around this domain has been
growing rapidly in recent years. Particularly since 2001, efforts
have been presented towards materializing the interest of mod-
ern science [16]. Furthermore, tech-giants like Facebook or
even high-proﬁle entrepreneurs like Elon Musk are intensiﬁed
new futuristic startups that seek to enhance human capabilities
through BCI technology despite its limitations and hindrance.
Besides, plenty of neurotechnology-based applications are
available now on the market, so they can be used for various
non-clinical and research purposes [17], [18].

This paper pursues to present a synopsis of BCI technology,

history, research potential, and review of various approaches.
The paper also explores a Mind-Machine Interfaces (MMI)
approach called Emotiv Epoc+ Neuroheadset and discusses
the experimental ﬁndings that aim to identify the emotional
parameters of human subjects. The primary contributions of
the paper as follows:

• We provide a comprehensive review on three areas of
BCI including (i) Brain-Reading Computer, (ii) Brain-
Machine Interfaces, and (iii) Mind-Machine Interfaces
and different approaches for each area.

• We study potential approaches for each one of the BCI
areas and investigate the effectiveness in neurotechnolog-
ical research.

• We discuss a case study to study six performance metrics
of the human brain using Emotiv Epoc+ Neuroheadset
and demonstrate the experiments in a lab environment.
The rest of the paper is structured as follows: Section II
reviews the relevant literature on BCI and portraits an overview
of three research directions. Section III elaborates our contri-
bution by introducing an experimental presentation of Epoc+
Neuroheadsets and evaluations of its performance metrics. The
section also draw a discussion on both experimental and survey
results. Finally, Section IV concludes the paper.

II. BRAIN-COMPUTER INTERFACES (BCI)

Brain-Computer Interfaces (BCI) technology is a rapidly
developing research ﬁeld, recognized as an emerging tech-
nology, that has attracted researcher’s attention from different
ﬁelds in the last two decades [19]–[21]. BCIs is also known as
a class of neurotechnology originally developed for medical
assisting applications that may be described as the most
sensorial research ﬁeld due to its connectivity with the human
brain [22]–[24]. Crawford et al. [16] addressed BCI as a
measurement of the central nervous systems’ (CNS) activities,
which translate into new CNS outputs without using the brain’s
normal output channels of peripheral nerves and muscles [4],
[25]. Nowadays, researchers are optimistic to develop new
augmentative communication and control technology for hu-
man patients with severe neuromuscular disorders by adopting
BCI technology [26].

The fundamental structure of the modern BCI system is
based on four basic components including signal acquisi-
tion, signal preprocessing, feature extraction, and classiﬁca-
tion [28]. Shih et. al. [29] emphasizes the importance of
signal-acquisition hardware which must be multi-functional,
convenient, portable, safe, adaptable in all environments, and
capable to communicate with the brain using electrical signals.
According to Guy et al. [30], the ﬁrst process of BCIs is
acquiring brain signals, followed by analyzing them towards
decoding the neural activities. Finally, translates the extracting
data into commands that are relayed to an output device
that carries out the desired action. Instead to use the brain’s
normal output pathways, these devices measure and use signals
produced by the CNS [29]. Fig. 1 Illustrates the low-level

Fig. 1. BCI low-level architecture [27]

architecture of BCI structured introduced by Pfurtscheller et al.
[27] that has been widely accepted by many scholars includes
[31] and [32]. The ﬁrst step is to acquire the brain signals by
utilizing different methods, for instance, chip-set or electrodes
to extract the speciﬁc signal features towards classifying the
type of intends [33]. Classiﬁed features help the operating
device to translate into speciﬁed commands [27].

Fig. 2. BCIs based research approaches.

According to a recent update [34], Facebook’s Reality
Lab (FRL) announced initiation on Brain-Reading Computer
(BRC). BRC is a speech decoding approach that allows people
to type throughout electrical brain activities without conven-
tional text entry or voice dictation [35]. Besides, Neuralink
has unveiled a chip-based Brain-Machine Interface approach,
called ‘The Link’ [36]. The Link aims to introduce a futuristic
technology ‘Brain-Machine Interface (BMI)’ that will enable
patients with neurological conditions to control phones or
computers with their minds [37]. Other than BRC and BMI, a
bio-informatics company, Emotiv, is developing an EEG-based
neuroheadset that is capable to use electroencephalography
(EEG) and electrodes [38] to read the brain. In this section, we
discuss the aforementioned three potential BCIs approaches as
shown in Fig. 2. First, we provide an overview of BRC based
approach followed by Neuralink’s futuristic project, The Link.
Finally, we discuss a well know EEG based Mind-Machine
model.

A. Brain-Reading Computer

Facebook’s Brain-Computer Interface (BCI) development
research team lead by Regina Dugan [39] initiated a hands-
free communication (without saying a word) project in 2017
in collaboration with the University of California, San Fran-
cisco (UCSF). The research, known as ‘Project Steno’ mainly
focuses on the development of real-time applications based on
a brain-reading computer that is capable of decoding speech
directly from the human brain onto a screen using machine
learning techniques [18]. The primary aim of Project Steno
is to let people, particularly patients who have shortness in
speaking, type phrases in a computer by thinking about what
they wanted to say using a non-invasive, wearable device. This
goal pairs with UCSF’s research because it intends to improve
the lives of people suffering from paralysis and other forms
of speech impairment [40].

The model exploits a set of machine learning techniques
equipped with reﬁned phonological speech models that can
learn and decode the speech using brain activities [41]. It
also utilizes recurrent neural networks inspired by state-of-
the-art speech recognition and language translation algorithms.
[40], [42]. According to a report, [43], the proposed approach,
which is still in its infancy, may not decode every thought
of a human brain into text, rather it will decode only those
speech that would be decided by the person. The decoding
process is divided into two segments, ’high-extraction’ that is
responsible to record the neural activity of brain signals, and
’neural network’ that consists of a temporal convolution, an
RNN encoder, and a decoder process. The hypothesis has been
demonstrated using 30-50 unique sentences. The result shows
the promising potential of decoding selective thought of the
human brain that is expected to be used for futuristic mind-
controlled technology. Researchers from both teams expecting
to decode 100 words per minute from 1,000 vocabulary words
and an expected decoding error of less than 17 %.

Fig. 3. Process of speech decoding by adopting the neural network approach
presented by Moses et al. in 2020 [44].

In a separate study by the Center for Integrative Neuro-
science, UCSF, [44] introduced a novel approach that shows
robust accuracy of decoding natural speech or typing using
electrocorticogram (ECoG) displayed in Fig. 3. Regardless
of the demonstration within a limited environment, the re-
searchers show the accuracy of the proposed approach higher
than the previous attempts of state-of-the-art phoneme-based
classiﬁers [35]. Approximately 30-50 sentences were used
to conduct the experiments of the method, called decoding

pipeline. The ﬁrst step of the method is to record the high-
density neural activities of participants. To acquiring data, the
researchers utilize Deep Neural Networks (DNNs) techniques
where temporal convolutional sequence modeling is used for
extracting feature sequences, and the encoding and decoding
process used a recurrent neural network (RNN) that predicts
the speech and the next word in the sequence respectively.

B. Brain-Machine Interface

The Brain-Machine Interface (BMI) is intimately related
to the effort of developing new electrophysiological methods
to record the extracellular electrical activity of large neu-
ronal populations using multi-electrode conﬁgurations [45].
For many decades, research in BMI got
the attention of
researchers globally, especially John Cunningham Lilly [45]
who introduced the initial architectural design of BMI in
1950 when he successfully implanted 25–610 electrodes in
brain adult rhesus monkeys’ brains (See Fig. 4). Modern BMI
architecture is designed for both experimental and clinical
studies and can translate raw neuronal signals into motor
commands [46] as demonstrated by Chapin, J.K. et al. [47]. In
this study, researchers proposed an approach that can directly
control the cortical neurons using a robotic manipulator.

Fig. 4. John Lilly’s parts of the electrode implant (left); X-ray of 20 implanted
sleeves and one inserted electrode of monkey (right) [45]

A newly established, futuristic company ‘Neuralink’ an-
nounced its visionary project called ‘The Link’ with the
mission of helping people with paralysis to regain indepen-
dence through the control of computers and mobile devices
[48]. Neuralink hopes to use an implantable device to record
brain signals and allow people to control computers and other
machines with just their thoughts [49]. To describe Neuralink’s
ﬁrst steps toward a scalable high-bandwidth BMI system, Elon
Musk and Neuralink share [50] the endeavor to develop a
visionary product for brain’ reading composed of arrays of
small and ﬂexible electrode “threads”, with as many as 3,072
electrodes per array distributed across 96 threads. In Fig 5 the
prototype of the Link is given where letter A refers to the
256 channels data processing unit; B indicates the polymer
threads on parylene-c substrate; C is titanium cases that are
coated with parylene-c, and D is the power and data connector.
The approach of Neuralink, according to its website, [51] is to
understand the brain followed by interfacing, and engineering
the brain.

time, researchers and labs around the world have made impres-
sive progress towards bringing the imagination to reality. Like
many others, Emotiv, a bioinformatics company, is focusing
to develop varieties of electroencephalography (EEG) based
BCIs products with the mission of empowering individuals to
understand their brain and accelerate brain research globally
since its foundation in 2011 [56]. EEG, according to Williams
et al. [14], is one of the oldest neuroscientiﬁc techniques and
refer to a continuous recording of the electrical activity gener-
ated by groups of neurons in the brain. The device calibrates
itself by having the subject closing their eyes and remaining
in a neutral state of mind without movement. This calibration
method provides a relatively clean reading to measure the EEG
properly [57]. EEG is capable to read the outer layer of the
brain and measure the oscillations of electric impulses in the
brain using sensors on the scalp. Emotiv Epoc+ neuroheadset,
shown in Fig. 7, is considered as a popular alternative of
medical-grade EEG recording devices due to its novelty and
affordable commercialization [58], [59]. The Epoc+ headset is
considered as an EEG reader that connects with a processing
application through Bluetooth technology [60]. Epoc+ headset
is capable to detect, read, and acquire data from the user’s
emotional state, facial expressions using EEG. The device
also provides support applications to display the output of
the signals as variables [61]. In this sub-section II-C, we
review such non-invasive device called Epoc+ Neuroheadset
that provides wireless EEG data acquisition and processing.

Fig. 5. Architecture of the links’ sensor device called Neuralink application-
speciﬁc integrated circuit (ASIC) [50].

In addition, Neuralink has developed a neurosurgical robot
‘robotic electrode inserter’ as scratched in Fig. 6 that has the
capacity of inserting six threads (192 electrodes) per minute
into the brain in an automated mode [50]. In the ﬁgure, A
indicates the loaded needle pincher cartridge; B shows the
brain position sensors while C refers to the light modules
consist of multiple independent wavelengths. D, E, F, and G
indicate the motor, needle camera, wide-angle view camera,
and stereoscopic cameras, respectively. The primary objectives
of this robot are to avoid surgery time, vasculature and record
from dispersed brain regions. According to Neuralink, the
chip must be installed using the automated ’electrode inserter
robot’ and the electrodes, which will read those impulses, will
amplify the signal in the processing unit [52]. In the recent
announcement, Elon Musk presents the chips that already
implanted in three pig’s brain [53]. During the demonstration,
Elon Musk shows the reading neural signals of one of the pigs
named Gertrude in real-time [54]. The developed approach has
exhibited results that achieve a spiking yield of up to 70 % in
chronically implanted electrodes [50].

Fig. 7. Physical layout of the Emotiv Epoc+ neuroheadset.

Emotiv neuroheadset (version: Epoc+) consist of 14 Silver-
Silver Chloride Electrodes (Ag/AgCl) [62] located at F3, F4,
AF3, AF4, F7, and F8 for imaging the lobus frontalis neural
activity of the subject’s brain; T7, T8, FC5 and FC6 for lobus
temporalis while the lobus parietalis is scanned by P8 and P7
electrodes. The other two electrodes O2 and O1 are responsible
for the lobus occipitalis in two different arms within 3-axis
navigations [38]. Epoc+ reads the data between 128 Hz and
256 Hz and connects with a parent app ”Emotiv App” to access
the offered features using three other different software appli-
cations independently named ‘EmotivBCI’, ‘EmotivPRO’, and
‘EmotivBrainVIZ’ [14]. To evaluate the neuroheadset and its

Fig. 6. Robotic electrode Inserter introduced by [50].

C. Mind-Machine Interface

Professor Eberhard E. Fetz [55] was one of the earliest
pioneers on work that connects machines to minds. Since that

applications, it is necessary to prepare the headset and the
applications and adjust the headset with the skull. We found
the physical design of the Epoc+ neuroheadset has deﬁciencies
in terms of maintaining the contact quality above 98%, which
is required for accessing various features and getting expected
readings. Fig. 8 presents the contact quality between the skull
and the electrodes during the evaluation that reaches 100 %
(left) and 41 % (right). Ensuring contact quality need to be
higher in order to access some of features including extraction
of ‘mental commands’ and ‘facial expressions’.

Fig. 8. Overview and percentage of contact quality of Epoc+ Neuroheadset.

According to the researcher Deitz et al. [63], EEG is
efﬁcient for measuring rates of decision-making and reading
neuro-engagement with dimension. By adopting the concept of
EEG, Epoc+ neuroheadset offers scores of features for students
and researchers. Besides, Emotiv also offers two exciting
features ‘Mental Commands’ and ‘Facial Expression’. Fig. 9
represents the Emotiv training interface of its mental com-
mands and facial expressions. Using the mental commands,
the user is allowed to control an object by providing training
for different directions, such as ‘push’, ‘lift’, and ‘rotate
right’. Besides, facial expression enables users to repeat the
parallel facial expression such as ‘smile’, ‘clench teeth’, ‘raise
brows’, etc. However, we demonstrate performance metrics
exclusively to investigate and validate the effectiveness of
Epoc+ neuroheadset in reading emotions of human subjects.
In section III, we discuss the analyzing result of emotional
parameters of the participants including (i) engagement, (ii)
focus, (iii) excitement, (iv) stress, (v) relaxation, and (vi)
interest.

Fig. 9. Emotiv training interface for mental commands (left) and facial
expressions (right).

III. EXPERIMENTS AND EVALUATIONS

A. Real-Time Datasets

A total of three participants with self-reported healthy con-
ditions have participated in the following experiment. Subject-
I and Subject-II is a male of 25 and 62 years old while
Subject-III is a female of 48. We ﬁrst attempted to achieve
a connection quality of 100% to investigate the 14 Ag-AgCl
electrodes in terms of channel spacing (uV). We suggest the
participants instilling their emotions according to the screening
video towards detecting their feelings accurately. To prepare
the aforementioned participants for the ﬁnal experiments, we
provide necessary training in the lab environment including a
brieﬁng about the neuroheadset and an experiment followed
by short demo practices align to real-time experiments. To
is
ensure the required connection and reliable values,
necessary to prepare the headset by hydrating the sensors
using multipurpose contact lens saline solution followed by
installing the sensors with the headset (each sensor should
be inserted into black plastic arms of the headset) that lead
to connecting the headset with software applications using
Bluetooth. We calibrate the subject using Emotiv App towards
getting real-time readings on Emotiv applications. To achieve
the expected readings, we ’calibrate’ the subject
to get a
strong signal, we asked the participants to close their eyes
and remain in a neutral state followed by keeping their
eyes open without blinking. Our implementation commenced
by screening different kinds of videos to help participants
controlling their emotions by watching given scenarios within
the aforementioned categories. The initial result generated by
Emotiv app depicted in Fig. 10.

it

Fig. 10.
diagram for 14 electrodes in 100 % contact quality.

Illustration of the 200 uV RAW electroencephalography (EEG)

We begin the experiments with subject-I by displaying
a movie clip (category-action) on a TV screen for ﬁfteen
minutes while we collected the readings for the last eleven
minutes and thirty seconds. For the subject-II, we play a math
competition video while subject-III (category-attention) was
shown a movie clip (category-comedy). We record the readings
for an entire segment of experiments and collect a wide range
of datasets in CSV format. We extract about 6,086 events
(2,066 “subject-I”, 2,100 “subject-II”, and 1,920 “subject-III”)
related to the performance metrics from hundred of datasets

generated as experimental output by Epoc applications. We
split each group of extract datasets into ﬁve different categories
to monitoring and yielding the emotional states. We also
surveillance the ﬂow of each categorized dataset for assessing
and validating the output of the performance metrics. Our
primary effort is to produce the graphical interfaces using the
datasets where the X-axis represents the ﬂow of the dataset and
the Y-axis represents the value of the event. We obtain a max-
imum value of 1.00, 9.016, 131.23, 1.00, 16.672, and 1.049
for Engagement, Excitement, Stress, Relaxation, Interest, and
Focus respectively while minimum values are -0.999, 0.00, -
2.945, 0.00, 0.00, and 0.00 for the same parameters illustrate
(See Figures 11-14 where PM refers to Performance Metrics).
During the experiments, Emotiv App allows to monitor the
real-time reading of all the parameters on a graphical interface.

Fig. 11. Overall parameters of performance metrics in the lab environment
(100% contact qualities) using internal datasets, subject-I.

Fig. 12. Overall parameters of performance metrics in the lab environment
(100% contact qualities) using internal datasets, subject-II.

Fig. 14. Parameter (stress) of performance metrics in the lab environment
(100% contact qualities) using experimental datasets.

the parameters is not having required contact quality. For
instance, Figures 11-13 illustrate the paucity of the parameter
“Focus”, while “Excitement” and “Interest” is seen as the most
reading parameters by the electrodes as shown in Fig. 11-12.
In subsection III-D, we evaluate the readings to validate the
datasets with the preliminary survey in subsection III-C. We
attempt to assess possible correlations between the events to
determine the viability of EEG’s readings.

B. External Datasets

We also collected a vast number of data from Emotiv
by adopting quantitative research approach, approximately
202,140 datasets consisting of about 2,170 events of emotional
parameters. Our primary goal is to evaluate the events gradu-
ally with the real-time events generated from neuroheadset. We
identify the minimum values of -1.182, 5.427, 0.886, 3.650,
and -1.182 for Engagement, Excitement, Relaxation, Interest,
and Focus respectively while the maximum values are -0.130,
16.105, 0.348, 15.549, and 0.844. However, designed values
(maximum) for Engagement, Excitement, Relaxation, Interest
and Focus are sequentially -0.144, 16.528, 0.462, 17.034, and
1.000 while minimum acceptable values are -1.258, 4.90, -
0.010, -5.270, and 0.000 for the aforementioned parameters
displayed in Fig. 15-16; X-axis represents the events and Y-
axis represents the value of the variables and PM refers to
Performance Metrics.

Fig. 15. Overall parameters of performance metrics using external datasets
from Emotiv.

Fig. 13. Overall parameters of performance metrics in the lab environment
(100% contact qualities) using internal datasets, subject-III.

C. Preliminary Survey

During the analysis of hundreds of data, we found missing
readings of brain signals for certain times and we assume
that the primary cause of absence data of a speciﬁc part of

We conduct a preliminary survey among all participants of
the experimental study to evaluate and validate the readings
data. A self-administered questionnaire utilizes to collect the

Fig. 16. Parameter (stress) of performance metrics using external datasets
from Emotiv.

data related to the subject’s thoughts where they were asked to
answer and complete lists of questionnaires online. We design
the survey with a variety of questions including “Rate the
speciﬁc parameters” on a scale of 1-10 for a particular time-
frame when we record the readings.

TABLE I
PRELIMINARY SURVEY: A GENERIC ILLUSTRATION OF THE RATINGS FOR
PARAMETERS

Parameters
Engagement
Excitement
Stress
Relaxation
Focus
Interest

Experiment (First Half)
Subject-II
0.30
0.70
0.60
0.30
0.40
0.90

Subject-I
0.60
0.30
0.20
0.60
0.30
0.70

Experiment (Second Half)

Parameters
Engagement
Excitement
Stress
Relaxation
Focus
Interest

Subject-I
0.90
0.70
0.70
0.70
0.50
0.90

Subject-II
0.60
0.90
0.60
0.50
0.60
0.10

Subject-II
0.40
0.80
0.30
0.50
0.20
0.80

Subject-II
0.50
1.00
0.10
0.60
0.10
1.00

For the questionnaires of the survey, we split the time-frame
of the experiment into two; (i) ﬁrst six minutes and (ii) second
six minutes of the experiments. We survey the ratings of the
subjects for each performance metric during both time-frame
as shown in Table I. We also consider asking the subject’s basis
about the higher rate in two particular performance metrics
which are ‘Excitement’ and ‘Interest’.

D. Evaluation

After conducting training and experiments within a con-
trolled session, we evaluate the ﬁndings of performance
metrics of Emotiv applications. We aggregate the data to
conduct statistical analysis, introduce different but informative
diagonals that presents two primary output, (i) correlations
between performance metrics (top-right corner) and (ii) his-
togram of experimental datasets and external datasets (middle)
as illustrated in Fig. 17 and Fig. 18 respectively. Both diagrams
are unique and we can identify the linear dependency between
events (bottom-left corner). The charts also illustrate the
correlation between each performance metric.

Fig. 17. Probability chart of performance metrics using raw data generated
in the lab environment (100% contact qualities).

Fig. 18. Probability chart of performance metrics using external datasets
collected from Emotiv.

We train and test experimental datasets by adopting two
supervised learning algorithms: Na¨ıve Bayes and Linear Re-
gression. We use 70% events for training and 30% for testing
purposes from 1,242 events. All of the events splits into six
speciﬁc parameters in accordance with the Emotiv neurohead-
set’s readings. For Na¨ıve Bayes, we calculate the probability
to obtain the prediction. We ﬁrst identify the percentage of
accurately predict values which is 48% followed by using
a confusion matrix that gave an accuracy of 57%. Besides,
we identify another statistical accuracy include skewed data
measurement named Kappa that gives a primary exactness
32%. Our effort was to increase the exactitude of Kappa that
leads the classiﬁer to improve overall accuracy. In our third
attempts, we ﬁnd a considerable realistic accuracy of 69% and
Kappa slightly increased to 41%, Table II(a).

In Table II(b), we apply 70% and 30% of overall events for
training and testing respectively. Linear regression classiﬁer
classiﬁed about 800 events and identify 45% and 62% accu-
racy for two attempts. However, the measured standard error
was 15% and the t-Stat was 12%. Besides, demonstration on
individual parameters also indicates close accuracy.

To give a comparative representation between the perfor-
mance metrics of three subjects and their survey report, we
conduct a generalized contrastive study. First, we evaluate

TABLE II
ANALOGY BETWEEN NA¨IVE BAYES AND LINEAR REGRESSION USING
CONFUSION MATRIX (70% TRAINING, 30% TESTING)

Initial Accuracy
57%
32% (Kappa)

Na¨ıve Bayes (a)

Improved Accuracy
69%
41% (Kappa)
Linear Regression (b)

N=1,242
Training: 70%
Testing: 30%

Initial/Improved Accuracy
45%
62%

Standard Error/t-Stat
15%
12%

N=1,242
Training: 70%
Testing: 30%

the associate dataset from experiments, subsequent survey,
and extend to external datasets. In a generalizing analysis
illustrated in Fig. 19-25 (X-axis and Y-axis represent
the
event and value of the event respectively), two parameters
‘Engagement’ and ‘Relaxation’ of subject-II show gradual
steadiness between events. Our participant (Subject-II) afﬁrms
possible engagement during the experiments since he was keen
to understand the mathematical logic that leads the parameter
‘Focus’ uncertain reading but higher than other subjects.
On the contrary, the other two parameters ‘Excitement’ and
‘Interest’ of subject-I show the higher reading and the survey
indicates her total interest and excitement in watching movies
(action). However, both subject I and II indicates vicissitudes
orientation in stress and focus respectively. Subjects agreed
that unique experimental techniques, the content of the video
clip, and the latest technological equipment used to conduct
the experiments were the primary reason for the higher reading
of ‘Excitement’ and ‘Interest’ parameters. We also evaluated
the external datasets, and conclude identical and robust but
static ﬂow in each parameters comparing to experimental
datasets Fig. 17-22. Although our illustration between three
similar datasets, records in different lab environments, partic-
ipants, and periods, show signiﬁcant similarity within perfor-
mance metrics. Moreover, slight differences may be occurred
due to disparity between aforesaid datasets of the subject’s
activities during the experiments.

E. Discussion

Utilizing our experimental datasets of Epoc+ Neuroheadset,
we measure the classiﬁcation of emotions accuracy 69% and
62% for Na¨ıve Bayes and Linear Regression respectively
illustrated in Table II. To optimize the accuracy of Na¨ıve
Bayes classiﬁcation, we adopt the Kappa metric that measures
the agreement between classiﬁcation and truth values; As
expected, it increase the classiﬁcation accuracy by about 9%
compare to the ﬁrst attempt. Within a comparative study
between survey and experimental data draw a parallel implica-
tion. Besides, we present diagrams for the linear dependency
between events of each dataset and complete analogical illus-
trations. The study also found that preparatory training helps
the subjects to complete the experiments intensively. Overall,
all dataset (including external) demonstrate the compatibility
of the Emotiv approach by reading the EEG signals, con-
verting the data into variables, displaying on the user inter-

Fig. 19. Graphical illustration between Emotiv and survey datasets for linear
regression, X-axis consist of survey data while Y-axis consists of experimental
events.

Fig. 20. Analogical illustration based on the parameter Engagement between
subjects, 75% experimental and 25% external.

Fig. 21. Analogical illustration based on the parameter Excitement between
four subjects, 75% experimental and 25% external.

Fig. 22. Analogical
subjects, 75% experimental and 25% external.

illustration based on the parameter Focus between

face, and giving promising accuracy that can be compatible

accuracy of the neuroheadset and its applications. So far,
experimental ﬁndings appear a promising research tool around
the ﬁeld of neurotechnology. Signiﬁcantly, other approaches
revealed their progressional promises in BCIs research towards
a technological wonderland. To draw a conclusion, we have
not yet considered high-level synthesis, but we aim to con-
tinue our effort explicitly towards signiﬁcant accomplishments
around the domain of BCIs; following the vision 2050 when
BCI could become a magic wand for developing men control
objects with the mind.

ACKNOWLEDGMENT

This work was supported in part by research computing
resources and technical expertise via a partnership between
Kennesaw State University’s Ofﬁce of the Vice President for
Research and the Ofﬁce of the CIO and Vice President for
Information Technology [64].

REFERENCES

[1] M. Ienca and R. Andorno, “Towards new human rights in the age of
neuroscience and neurotechnology,” Life Sciences, Society and Policy,
vol. 13, no. 1, pp. 1–27, 2017.

[2] H. C. e. a. Swetnam M, McBride D, “Neurotechnology futures technol-

ogy,” POTOMAC INSTITUTE FOR POLICY STUDIES, 2013.

[3] C. I. Bargmann and W. T. Newsome, “The brain research through ad-
vancing innovative neurotechnologies (brain) initiative and neurology,”
JAMA neurology, vol. 71, no. 6, pp. 675–676, 2014.

[4] J. R. WOLPAW, “Brain-computer interfaces,” Neurological Rehabilita-

tion, vol. 110, 2013.

[5] J. N. Mak and J. R. Wolpaw, “Clinical applications of brain-computer
interfaces: Current state and future prospects,” IEEE Reviews in Biomed-
ical Engineering, vol. 2, pp. 187–199, 2009.

[6] M. A. Hofman, “Evolution of the human brain: When bigger is better,”

Frontiers in neuroanatomy, vol. 8, p. 15, 03 2014.

[7] K. Sidiropoulou, E. Pissadaki, and P. Poirazi, “Inside the brain of a

neuron,” EMBO reports, vol. 7, pp. 886–92, 10 2006.

[8] P. Janetius, “Human brain: Mystery revealed?” vol. 7, pp. 748–750, 11

2018.

[9] N. Rose, “Reading the human brain: How the mind became legible,”

Body Society, vol. 22, 01 2016.

[10] M. T. Review.

(2020) The us military is trying to read minds.

[11] R.

D.

https://www.technologyreview.com/2019/10/16/132269/us-military-
super-soldiers-control-drones-brain-computer-interfaces/.
(2020)
are

mind
Avail-
https://blogs.scientiﬁcamerican.com/observations/mind-reading-

control
able:
and-mind-control-technologies-are-coming/

Fields.
technologies

[Online].

coming.

reading

Mind

and

[12] L. Golembiewski. (2020) Are you ready for tech that connects to
your brain? [Online]. Available: https://hbr.org/2020/09/are-you-ready-
for-tech-that-connects-to-your-brain

[13] J. Vidal, “Toward direct brain-computer communication.” Annual review

of biophysics and bioengineering, vol. 2, pp. 157–80, 1973.

[14] N. S. Williams, G. McArthur, and N. Badcock, “10 years of epoc: A
scoping review of emotiv’s portable eeg device,” bioRxiv, 2020.
[15] A. K¨ubler, “The history of bci: From a vision for the future to real sup-
port for personhood in people with locked-in syndrome,” Neuroethics,
vol. 13, p. 163–180, 2019.

[16] C. Crawford, M. Andujar, S. Remy, and J. Gilbert, “Cloud infrastructure

for mind-machine interface,” 07 2014.

[17] E. Waltz. (2019) Elon musk announces neuralink advance toward sync-
ing our brains with ai. [Online]. Available: https://spectrum.ieee.org/the-
human-os/biomedical/devices/elon-musk-neuralink-advance-brains-ai

Palmer.
its

[18] A.
ing
able:
its-brain-reading-computer.html

closer
to mak-
[Online]. Avail-
a
https://www.cnbc.com/2019/07/30/facebook-is-still-working-on-

(2019)
brain-reading

getting
reality.

Facebook

computer

is

[19] B. He, “Brain, computer and machine interfacing,” Neuromodulation

(Second Edition), 2018.

Fig. 23. Analogical illustration based on the parameter Interest between
subjects, 75% experimental and 25% external.

Fig. 24. Analogical illustration based on the parameter Relaxation between
subjects, 75% experimental and 25% external.

Fig. 25. Analogical
subjects, 75% experimental and 25% external.

illustration based on the parameter Stress between

for researchers, neuroscientists, psychologists, rehabilitation
specialists around the context of detecting and measuring
performance metrics.

IV. CONCLUSION

Neurotechnology is an untenable research ﬁeld that has been
entrenched to the scholars in academia and BCI research is
now a multidisciplinary effort. In this study, we considered
investigating three unveiled potential Brain-Computer Inter-
faces (BCI) research approaches. The paper discussed the
FRL and Neuralink’s projects in detail and their recently
presented demonstration results. Besides, we demonstrated
Emotiv Epoc+ Neuroheadset using raw data from three differ-
ent sources and illustrated its effectiveness by adopting Na¨ıve
Bayes and Linear Regression technique towards analyzing and
understanding brain activities, electroencephalography (EEG),
and Brain-Computer Interfaces (BCI) extensively for future
endeavor. We also evaluated the efﬁciency of the Epoc+
device in terms of reading the performance metrics of human
beings with a presentation of ﬁndings that indicates acceptance

[20] J. Luaut´e and I. Laffont, “Edito: Bcis and physical medicine and
rehabilitation: The future is now,” Annals of physical and rehabilitation
medicine, vol. 58, 01 2015.

[21] F. Mousa, R. El-Khoribi, and M. Shoman, “A novel brain computer
interface based on principle component analysis,” Procedia Computer
Science, vol. 82, pp. 49–56, 12 2016.

[22] C. Zuckerman, “The human brain explained,” Natl. Geogr. Mag., 2019.
[Online]. Available: https://www.nationalgeographic.com/science/health-
and-human-body/human-body/brain/

[23] T. H. B. et al., “Introduction: The human brain,” Newscientist, p. 11–14,
2006. [Online]. Available: https://www.newscientist.com/article/dn9969-
introduction-the-human-brain/

[24] B. J. Lance, S. E. Kerick, A. J. Ries, K. S. Oie, and K. McDowell,
“Brain–computer interface technologies in the coming decades,” Pro-
ceedings of the IEEE, vol. 100, no. Special Centennial Issue, pp. 1585–
1599, 2012.

[25] J. Wolpaw and E. Wolpaw, “Brain-computer interfaces: Something new
under the sun,” Brain-Computer Interfaces: Principles and Practice, 01
2012.

[26] D.

J. McFarland
for
no.

R. Wolpaw,
and
interfaces
vol.
60–66, May
54,
https://doi.org/10.1145/1941487.1941506

J.
communication
5,

“Brain-computer
control,” Commun. ACM,
[Online]. Available:
2011.

and

p.

[27] G. Pfurtscheller, B. Graimann, and C. Neuper, “Eeg-based brain-
computer interface systems and signal processing,” Encyclopedia of
Biomedical Engineering, vol. 2, pp. 1156–1166, 01 2006.

[28] S. N. Abdulkader, A. Atia, and M.-S. M. Mostafa, “Brain computer
interfacing: Applications and challenges,” Egyptian Informatics Journal,
vol. 16, no. 2, pp. 213–230, 2015.

[29] J. Shih, D. Krusienski, and J. Wolpaw, “Brain-computer interfaces in
medicine,” Mayo Clinic proceedings. Mayo Clinic, vol. 87, pp. 268–79,
03 2012.

[30] Cuntai Guan, M. Thulasidas, and Jiankang Wu, “High performance p300
speller for brain-computer interface,” in IEEE International Workshop
on Biomedical Circuits and Systems, 2004., 2004, pp. S3/5/INV–S3/13.
[31] S. Machado, L. Almada, and R. N. Annavarapu, “Progress and prospects
in eeg-based brain-computer interface: Clinical applications in neurore-
habilitation,” Journal of Rehabilitation Robotics, vol. 2013, pp. 28–41,
01 2013.

[32] V. Kaiser, A. Kreilinger, G. M¨uller-Putz, and C. Neuper, “First steps
toward a motor imagery based stroke bci: New strategy to set up a
classiﬁer,” Frontiers in neuroscience, vol. 5, p. 86, 07 2011.

[34] A.

futuristic

Facebook

just
brain-typing

[33] B. N. M. D. J. P. G. . V. T. M. Wolpaw, J. R., “Brain-computer interfaces
for communication and control,” Clinical neurophysiology : ofﬁcial
journal of the International Federation of Clinical Neurophysiology, vol.
113(6), p. 767–791, 2002.
(2019)

Robertson.
an
up-
[Online].
its
https://www.theverge.com/2019/7/30/20747483/facebook-

on
date
Available:
ucsf-brain-computer-interface-typing-speech-recognition-experiment
[35] J. G. M. E. F. C. David A. Moses, Matthew K. Leonard, “Real-time
decoding of question-and-answer speech dialogue using human cortical
activity,” Nature Communication, 2019.
(2020)
between

Elon musk
[36] J. Musto.
link
demos
[Online]. Avail-
able:
https://www.foxbusiness.com/technology/elon-musk-neuralink-
shows-pig-with-successful-computer-chip-implant

published
project.

in
computer

‘ﬁtbit
brain,

skull’:

chips.

your

[37] BBC. (2020) Neuralink: Elon musk unveils pig with chip in its
brain. [Online]. Available: https://www.bbc.com/news/world-us-canada-
53956683

[38] I. Fouad and F. Mohammed, “Using emotiv epoc neuroheadset to acquire
data in brain-computer interface,” International Journal of Advanced
Research, vol. 3, pp. 1012–1017, 11 2015.
dugan.

[39] AllGov.

Available:

[Online].

Regina

elvira

http://www.allgov.com/ofﬁcials/dugan-regina?ofﬁcialid=29324

[40] Tech@facebook.

(2019)

new interface: Hands-
free
[On-
word.
line]. Available: https://tech.fb.com/imagining-a-new-interface-hands-
free-communication-without-saying-a-word/

Imagining
without

communication

a
saying

a

[41] N. Weiler.

real

(2019) Team ids
brain’s

phrases
in
[Online].
Available: https://www.ucsf.edu/news/2019/07/415046/team-ids-spoken-
words-and-phrases-real-time-brains-speech-signals

spoken words

signals.

speech

from

time

and

[42] Facebook.
technology
https://about.fb.com/news/2017/04/f8-2017-day-2/

(2017)
updates

from day

2017: Ai,

two.

F8

building

8

and more
Available:

[Online].

[43] J.

Constine.
for

Facebook
interfaces
and
https://techcrunch.com/2017/04/19/facebook-brain-interface/

is
skin-hearing.

(2017)
typing

building

brain-computer
[Online]. Available:

[44] E. F. C. Joseph G. Makin, David A. Moses, “Machine translation
of cortical activity to text with an encoder–decoder framework,” Nat
Neurosci, vol. 23, p. 575–582, 2020.

[45] M. A. L. N. Mikhail A Lebedev, “rain-machine interfaces: From
basic science to neuroprostheses and neurorehabilitation,” Physiol. Rev,
vol. 97, p. 767–837, 2016.

[46] N. M. Lebedev MA, “Brain-machine interfaces: past, present and future,”

Trends Neurosci, vol. 29, pp. 536–46, 2006.

[47] J. Chapin, K. Moxon, R. Markowitz, and M. Nicolelis, “Real-time
control of a robot arm using simultaneous recorded neurons in the motor
cortex,” Nature neuroscience, vol. 2, pp. 664–70, 07 1999.

[48] Neuralink. (2020) A direct link between the brain everyday technology.

[49] A. Nippert.
bacteria

[Online]. Available: https://neuralink.com/applications/
(2019) Connecting
to machines may
[Online]. Avail-
for
let
along
come
able: https://massivesci.com/articles/brain-machine-interface-bmi-elon-
mulk-neuralink-mind-control/

brains
the

ride.

[50] N. Musk E, “An integrated brain-machine interface platform with
thousands of channels,” J Med Internet Res, vol. 21, pp. 536–46, 2019.
Available:
Approach.

[51] Neuralink.

[Online].

(2020)

https://neuralink.com/approach/

[52] A. Kulshreshth, A. Anand, and A. Lakanpal, “Neuralink- an elon musk
start-up achieve symbiosis with artiﬁcial intelligence,” 10 2019, pp. 105–
109.

[53] L.

Crane.

Elon

brain

(2020)

a
neu-
[Online].
https://www.newscientist.com/article/2253274-elon-musk-

demonstrated
pig.
live

ralink
Available:
demonstrated-a-neuralink-brain-implant-in-a-live-pig/
(2020)

demonstrates

AREVALO.

musk
a

implant

in

[54] E.

Elon
on

musk
pig.

neu-
Available:

ralink
https://www.tesmanian.com/blogs/tesmanian-blog/neuralink-pigs

brain-chip

[Online].

implant

Rajesh

[55] J. W.
and
able:
machine-how-close-are-we/.

P.
How

mind
Avail-
https://www.scientiﬁcamerican.com/article/melding-mind-and-

(2017) Melding
[Online].
we?

N.
close

machine:

Rao.

are

[56] Emotiv.

Emotiv.

(2011)
https://www.emotiv.com/about-emotiv/.
[On-
emg
line]. Available: https://www.chestercountyhospital.org/news/medical-
columns/2012/april/eeg-emg-omg-what-is-that

(2012) Eeg

J. Placido.

omg what

Available:

[Online].

that?

is

[57] C.

[58] S. K. Josef Parvizi, “Promises and limitations of human intracranial
electroencephalography,” Nature neuroscience, vol. 21(4), p. 474–483,
2018. [Online]. Available: doi.org/10.1038/s41593-018-0108-2

[59] M. Duvinage, T. Castermans, M. Petieau, T. Hoellinger, G. Cheron,
and T. Dutoit, “Performance of the emotiv epoc headset for p300-based
applications,” BioMedical Engineering OnLine, vol. 12, pp. 56 – 56,
2013.

[60] A. Telpaz, R. Webb, and D. J. Levy, “Using eeg to predict consumers’
future choices,” Journal of Marketing Research, vol. 52, no. 4, pp. 511–
529, 2015.

[61] D. Prince, M. Edmonds, A. Sutter, M. Cusumano, W. Lu, and V. Asari,
“Brain machine interface using emotiv epoc to control robai cyton
robotic arm,” in 2015 National Aerospace and Electronics Conference
(NAECON), 2015, pp. 263–266.

[62] D. R. S. Kelly, “Analytical electrochemistry: The basic concepts,” The
Analytical Sciences Digital Library, vol. 9, 2018. [Online]. Avail-
able:
https://community.asdlib.org/activelearningmaterials/analytical-
electrochemistry-the-basic-concepts/

[63] M. C. P. J. H. J. T. C. George D. Deitz, Marla B. Royne, “Eeg-based
measures versus panel ratings,” Journal of Advertising Research, vol. 56,
2016.

[64] T.

Boyle

and

R.

facilities

hpc
https://digitalcommons.kennesaw.edu/training/10

2021.

and

Aygun,
resources,”

“Kennesaw

state

university
[Online]. Available:

