An Automated Deployment and Testing Framework
for Resilient Distributed Smart Grid Applications

Purboday Ghosh ∗, Hao Tu †, Timothy Krentz∗, Gabor Karsai∗ and Srdjan Lukic†
∗Institute for Software-Integrated Systems, Vanderbilt University, Nashville, TN
†North Carolina State University, Raleigh, NC

2
2
0
2

n
u
J

7
2

]

C
D
.
s
c
[

1
v
0
0
0
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Executing distributed cyber-physical software pro-
cesses on edge devices that maintains the resiliency of the
overall system while adhering to resource constraints is quite
a challenging trade-off
to consider for developers. Current
approaches do not solve this problem of deploying software
components to devices in a way that satisﬁes different resilience
requirements that can be encoded by developers at design
time. This paper introduces a resilient deployment framework
that can achieve that by accepting user-deﬁned constraints to
optimize redundancy or cost for a given application deployment.
Experiments with a microgrid energy management application
developed using a decentralized software platform show that
the deployment conﬁguration can play an important role in
enhancing the resilience capabilities of distributed applications
as well as reducing the resource demands on individual nodes
even without modifying the control logic.

I. INTRODUCTION

The modern electric grid is a complex Cyber-Physical
System (CPS) [1] consisting of several software applications
running simultaneously. They perform various control and
regulatory functions associated with the system, for exam-
ple, energy management [2], protection [3], and automatic
reconﬁguration [4]. The various functions or roles required
for these applications which are themselves distributed, can,
in turn, be modularized into distributed computing processes
or actors, like a relay actor for communicating with a relay
between two sub-networks, a computation actor for calculating
set points, etc. At run-time, the various actor processes are
assigned to remote, embedded computing devices to execute
their functions. This process is known as deployment.

Existing deployment solutions do not allow for the consid-
eration of the resilience requirements of such critical CPS and
how they can be combined with the resource requirements of
the software along with hardware limitations of the embedded
nodes. This paper proposes a resilient deployment solver for
calculating a valid deployment conﬁguration that can be used
to automate the deployment process such that incorporates
resilience in terms of resilient operating constraints.

We implement our solution to the deployment and testing
problems on a decentralized software platform for distributed
software development called Resilient Information Architec-
ture Platform for Smart Grids (RIAPS) [5], to leverage its
the
inherent resilience features and ease of use. We test
deployment solution using a real-world application from the

978-1-6654-8356-8/22/$31.00 ©2022 IEEE

power systems domain and run it using using a virtual network
emulator called Mininet [6].

Thus, the contributions of the paper are (1) A resilient
deployment solver that generates a valid deployment con-
ﬁguration based on deﬁned resilience constraints and (2)
showcasing the effectiveness of the deployment in a practical
energy management application. As a minor contribution, we
also present the implementation of two fault tolerance patterns
in the application using a RIAPS feature called groups.

II. RELATED WORK

Over the years,

there has been substantial research on
distributed systems deployment and synthesis that has, in turn,
shaped our work. However, they are either restricted to only
hardware reliability [7], [8] , require ﬁne-grained information
[9], or require enforcing constraints in the design itself [10].
Cloud-fog computing architectures for Internet of things
(IoT) applications also aim for the allocation of distributed
processes or services to computing hardware while trying to
optimize some criteria [11], [12], [13],[14], [15]. However,
no evaluation of resilience performance is performed after
deployment. They mainly aim to reduce latency, response
times, and manage node resources.

Among other constraint-based works, Chariot [16] employs
a run-time self-reconﬁguration algorithm for deploying IoT
applications to edge devices, but it relies on commands from
a continuously running manager component in all nodes and
can not be used in design time. Zephyrus2 [17] also introduces
replication and separation constraints similar to ours and uses
a speciﬁcation language, but the platform is not designed
to consider cyber-physical systems. The system architecture
considered in [18] assumes a ﬁxed number of nodes, while
[19] only considers resource allocation.

III. BACKGROUND

RIAPS Overview: The Resilient Information Architecture
Platform for Smart Grids (RIAPS) [5] is an integrated and
decentralized software framework that can be used for the
development, deployment, and monitoring of distributed ap-
plications on remote hardware nodes, called RIAPS nodes. It
supports a distributed computational model where a user can
deﬁne actors containing a number of components that
run different computations and can communicate with each
other via message passing through interfaces called ports.

 
 
 
 
 
 
Actors
Building
and Charger
actors

Grid actor

BESS actor

Fig. 1: RIAPS nodes, actors, components and services

Aggregator

Functions
1. Get power consumption readings from sensors
2. Estimate power demands for a future time
horizon (e.g. a day or 12 hours)
3. Send power request to the coordinator, re-
ceive setpoint commands from the coordinator
1. In grid-connected mode, send information to the
coordinator about the power available from the main
grid for the time horizon
1. Act as an energy buffer when the microgrid operates
in islanded mode or if the power demand can not be
met by the grid
1. Produce an optimal allocation strategy for each
future time step based on the power available
2. Based on the power available and BESS
state of charge, send discharging/ charging
commands to it.
3. Provide allocated power to the loads

On top of it,
it also provides a number of platform ser-
vices for remote node control, coordination, fault monitoring,
time synchronization and so on [20]. Figure 1 shows the
RIAPS infrastructure. A RIAPS application is represented
by an app.riaps ﬁle that describes the actor-component
relationships along with the various message types and com-
munication ports that each component uses. It also has an
associated deployment ﬁle, app.depl, where the assignment
of the actors to target computing nodes is speciﬁed.
RIAPS Energy Management Application (REMApp): To
show the effectiveness of our deployment solver and the
resilience testing setup, we demonstrated prototyping a simple
microgrid [21] energy management application for a small
ofﬁce campus using RIAPS. It consisted of a building, an
electric vehicle (EV) charging station, a battery energy storage
system (BESS), and a centralized regulatory actor Aggregator,
with functions as described in Table I. A dispatch algorithm
is designed to transfer available power to the loads and BESS
while maintaining the battery limits.

The dispatch problem is formulated as an optimization
problem and the constraints are described in Equation 1. Here
xij denotes the power granted to load i for time slot j, K is
the prediction horizon, N number of electrical loads, P gridj
denotes the total power available from the grid in time slot j
and Preq ij denotes the power required for unit i at time slot
j. wi is a weight assigned to each load which signiﬁes its
priority. Thus, the coordinator functions as a model predictive
controller. Both the building and the charger actors consist of a
predictive component, a manager component, and an interface
device component. The predictive component incorporates a
deep neural network based forecasting algorithm that is trained
using prerecorded consumption data. We simulate building and
charger loads using actual measurement data from a microgrid.
We omit the implementation details of the REMApp applica-
tion and the control algorithms, due to space constraints.

min
xij

N
(cid:88)

K
(cid:88)

i=1

j=1

wi × 0.5 × (Preq ij − xij)2

N
(cid:88)

i=1

xij ≤ Pgridj ∀j

(1)

xij ≥ 0
xij ≤ Preq ij

TABLE I: Actors and their functions in REMApp

Fig. 2: RIAPS Energy Management Application. The translu-
cent rectangles represent actors and the solid rectangles rep-
resent components. The arrows indicate messages exchanged.
This is the logical architecture prior to including any redun-
dancy for fault tolerance.

Apart from these actors, we also implement a Data Logger
actor with a Logger component to log measurement and event-
related data and also run a remote dashboard for visualizing
the results. The application model is shown in Figure 2.
RIAPS Groups: In RIAPS, A group deﬁnes a virtual
connection bus between components on the actual physical
network. It implements a pub-sub pattern among members
providing features like intra- group communication, dynamic
joining and leaving, leader election (based on Raft [22]), and
member voting on a value or an action. While the primary
application of groups is to create a hierarchical architecture
within a system for multilevel coordination and control, we
demonstrate here that some of those features can be leveraged

BuildingQry / BuildingAnsBuilding InterfaceBuildingUpdateData / BuildingPredictionPowerQry / PowerAnsBuilding ManagerBuildingPredictiveChargerQry / ChargerAnsCharger InterfaceChargerUpdateData / ChargerPredictionPowerQry / PowerAnsCharger ManagerChargerPredictiveLogDataCoordinatorGridPowerGrid InterfacePowerAvailableLoggerInfluxDBPowerQry / PowerAnsBESSQry / BESSAnsBESS ManagerBESSBuilding ActorCharger ActorUser Interface DashboardBESS ActorUtility GridData LoggerAggregatorto implement robust fault tolerance design patterns.

Considering the REMApp application, its principal perfor-

mance and safety speciﬁcations can be described as:
1. The system should not dispatch more power than is available
at the current time step.
2. The power allocated to the loads should be close to the
predicted demand, provided the ﬁrst condition is not violated.
For the ﬁnal application design, we consider two kinds of
threats for the application: (1) Node drop out caused by either
a communication fault or a hardware crash and (2) False
Data Injection security Attack (FDIA) occurring at any of the
sensors in the building or the EV charger leading to erroneous
power allocation.

In order to make the system resilient to these two types of
threats, we instantiate redundant copies of actors and introduce
two types of group behavior protocols for the components
within those actors, as described next.
Redundancy groups: Provides an automated failover mecha-
nism. The behavior is the following: Elect a leader, detect if
leader is out, and then replace with a new leader.
Consensus groups: It is a distributed agreement pattern on
top of leader election. Each member initiates a vote on a
value; if leader’s vote is successful, then it uses its own value.
Otherwise, it selects a group member with a veriﬁed vote to
send their value.

For the REMApp application, we apply the redundancy
groups pattern to the coordinator component and the consensus
groups pattern to the building, EV charger and BESS manager
components.

IV. RESILIENT DEPLOYMENT SOLVER

The deployment solver was implemented in Python and con-
sists of four parts: a deployment speciﬁcation language dspec
using which users can deﬁne fault tolerance as well as resource
speciﬁcations, parsed using a model parser; a constraint solver
implemented using Z3 [23] that converts them into constraints
and computes a valid deployment matrix satisfying them; and
ﬁnally a deployment generator that converts the solution into a
RIAPS deployment ﬁle. It takes as input a RIAPS model ﬁle
app.riaps, a deployment speciﬁcation ﬁle app.dspec,
a hardware conﬁguration ﬁle hardware-spec.conf for
resource limits and produces as output a RIAPS deployment
ﬁle app.depl.

A. Constraint formulation

Before formalizing the constraints used in the solver, some

basic deﬁnitions are required.
Actor: An actor is a tuple consisting of an identiﬁer ID,
redundancy D, host dependency H and a set of resource
constraints R. Aj := (cid:104)ID, D, H, R(cid:105).
Node: A node is a tuple consisting of an identiﬁer ID and a
set of resource constraints R. N odei := (cid:104)ID, R(cid:105)
Redundancy: It is an integer that implies the number of copies
of an actor that needs to be deployed. D(Aj) ∈ N
Host dependency: The host dependency for an actor is a
mapping between an actor and a node that the actor must be

# Hardware Specifications
#Beaglebone Black
[bbb]
# no. of CPU cores
cores = 1
max_cpu = 0.7 # percentage
# Internal memory (MB)
mem = 512
max_mem = 0.7
# disk space (MB)
spc = 4096
max_spc = 0.7

Fig. 3: Hardware speciﬁcation ﬁle. The name of the hardware
device is used as the key. Thus, the same ﬁle can be used to
deploy to a network of heterogeneous devices.

actor Aggregator(configfile, id, grptype) {

uses {

cpu max 35 % over 1;
mem 200 mb;
space 2048 mb;
net rate 40 kbps ceil 60 kbps; // Net limits

// File space limit

// Mem limit

// cpu limit over 1 second

}
...

}

Fig. 4: The resource limits placed on the actor can be deﬁned
in the RIAPS model under the uses block.

is a set of actors that must be

deployed to. H(Aj) = N odei.ID. This captures the scenario
of actors tied to a particular host, say, for example edge located
devices that must be accessed from that particular location.
Colocation condition: It
deployed to the same node. C = {Aj|j ∈ N}.
Separation condition: It is the opposite of colocation. It is a
set of actors that can not be placed on the same node. S =
{Aj|j ∈ N ∧ (Aj, Ak ∈ S =⇒ Aj, Ak /∈ C. ∀i (cid:54)= j)}
Resource limits: They are a set R() specifying the worst
case resource usage metrics deﬁned for the actors that must
be limited by the hardware resource speciﬁcations in case of
nodes, including, CPU utilization (cpu, cpumax for worst-
case) per core up to total cores, expressed as a percentage over
interval, memory (mem) expressed in MB, and disk space
(spc) expressed in MB. For network bandwidth two quantities
are used, an average rate, (rate) and a maximum ceiling (ceil),
expressed in kilobits per second (kbps).

The various resource limits for the hardware are speciﬁed
using a conﬁguration ﬁle with the name of the hardware device
acting as the key. An example is shown in Figure 3. The
network speciﬁcations for the node are taken from the RIAPS
conﬁguration ﬁle included with RIAPS installation, which
contains values speciﬁed for the network interface card rate
(NIC RATE) and ceiling (NIC CEIL). The worst case resource
usage for the various actors are speciﬁed in the RIAPS model
under the individual actor deﬁnitions. An example is shown
in ﬁgure 4.

The ﬁnal deployment solution space is encoded as a 2D
deployment matrix where each element is a binary variable,
X deﬁned as equation 2.

app REMApp {

Aggregator copies 2;
colocate (UtilityGrid,DataLogger);
separate (BESSActor, BuildingActor, ChargerActor);
deploy (UtilityGrid) on (h1);
// use limits for bbb on all;
}

Fig. 5: REMApp.dspec ﬁle. The last line (commented here) is
used for resource constraints.

(cid:40)

Xi,j =

if Aj assigned to N odei.

1,
0, otherwise.

(2)

Based on these deﬁnitions the different constraints used by

the solver can be formulated as:
Redundancy

(cid:88)

∀j(

Xi,j = D(Aj))

(3)

Host-actor dependency

i

∀j(N odei ∈ H(Aj)) =⇒ (∀p (cid:54)= i, Xp,j = 0)

(4)

Colocation constraints

∀Am, An ∈ C, (Xi,m = 1) =⇒ (Xi,n = 1)

(5)

Separation constraints

∀Am, An ∈ S, (Xi,m = 1) =⇒ (Xi,n = 0)

(6)

CPU

(cid:88)

∀i

j

(Xi,j × cpu(Aj))/100 ∗ interval

(7)

< cpumax(N odei)/100 ∗ cores ∗ interval

Memory

(cid:88)

∀i

j

Disk space

(Xi,j × mem(Aj)) < mem(N odei)

(8)

(cid:88)

∀i

j

(Xi,j × spc(Aj)) < spc(N odei)

(9)

Network bandwidth

(cid:88)

(Xi,j × rate(Aj)) + M ax(ceilj − ratej)

∀i

j

(10)

< 0.95 ∗ N IC RAT E(N odei)

We applied the solver to the REMApp model to validate its
correctness, as well as evaluate its performance in terms of
scalability and timing.
Basic Solver: We use the speciﬁcations in Figure 5 to run
the solver for 12 nodes. The solution produced by the solver
deploys 1 copy of the DataLogger and UtilityGrid actors on
host h1, 1 copy of the Aggregator actor each on hosts h5 and
h8, 1 copy of BESSActor on host h5, 1 copy of ChargerActor
on host h7 and 1 copy of BuildingActor on host h10. Thus, it
utilizes only 5 out of the 12 nodes.

Aggre-
gator
0
1
0
0
0
0
0
1
0

h1
h2
h3
h4
h5
h6
h7
h8
h9

BESS
Actor
0
0
1
1
0
1
0
0
0

Building
Actor
1
1
0
0
0
0
0
1
0

Charger
Actor
0
0
0
0
1
0
1
0
1

Data
Logger
1
0
0
0
0
0
0
0
0

Utility
Grid
1
0
0
0
0
0
0
0
0

TABLE II: Solver solution for the fault
conﬁguration

tolerance testing

Maximum Redundancy: By tweaking the solver conﬁgura-
tion, it is possible to search for a solution that maximizes
the copies of actors given a number of nodes. In this case,
there were 12 copies of Aggregator, 1 per node; 3 copies of
the BESS actor on h7, h8, h9; 3 copies of BuildingActor on
h1, h5, h11; and 6 copies of ChargerActor on h2, h3, h4, h6,
h10, h12. Since the DataLogger and UtilityGrid actors are co-
located and tied to one host, there was only 1 copy of them
on h1.
Minimum Cost: The solver can also be conﬁgured to solve
for a deployment conﬁguration that uses the minimum number
of nodes for the given constraints.

This setting was used to generate the deployment for our
test run by modifying the speciﬁcations of ﬁgure 5 in the
following way:

• Include 3 copies of the building, charger and BESS actors
to tolerate a single actor failure due to FDIA attack as
per the 2n + 1 redundancy rule [24].

• Separate the charger and the aggregator actor. This was
done purely for experimental purposes to demonstrate the
effects of the faults on the aggregator and the charger
independently.

The solution indicates a minimum of 9 nodes to deploy the
application as shown in table II.

The solver was tested on an Intel desktop PC with 8 GB
RAM, on a number of problems and the computation time
stayed below 5 seconds for up to 100 nodes and below 9
seconds for up to 1600 assertions added to the solver. This
is comparable to values reported in [16], which reports an
average of 1.74 seconds and a maximum of 48 seconds.

V. FAULT TOLERANCE TESTING

We tested the application using a virtual network emulator
Mininet [6]. Mininet allows for rapid prototyping of large
networks on a single computer. It also provides a command-
line interface (CLI) that can be used to control link parameters,
bring links up or down, and send commands to hosts.

A. Experimental Results

A 60 minute resilience test was designed and tested with the
deployment generated in Table II for the constraints described
in Section IV.
1. At t0, start test.

Aggre-
gator
0
1
0
0
0
1
0
0
0
0
0
0

h1
h2
h3
h4
h5
h6
h7
h8
h9
h10
h11
h12

BESS
Actor
0
0
0
0
0
0
1
1
0
0
1
0

Building
Actor
0
0
0
1
0
0
0
0
1
0
0
1

Charger
Actor
0
0
1
0
1
0
1
0
0
1
0
0

Data
Logger
1
0
0
0
0
0
0
0
0
0
0
0

Utility
Grid
1
0
0
0
0
0
0
0
0
0
0
0

TABLE III: Solver solution for 12 nodes with network limits

Condition 2 was also satisﬁed as observed in the plots that the
allocated power closely followed the predicted power and only
deviated in case the available power was insufﬁcient, showing
that the application functionality was resilient to both threats.

B. Deployment with Network Bandwidth Constraints

This section demonstrates how the solver evaluated the
deployment conﬁguration taking into account
the resource
limiting constraints for the hardware devices and resource
requirements for the distributed application actors. Network
bandwidth constraints were used to conduct the experiments
since the data can be easily captured and isolated for each
mininet host.

(a) Network trafﬁc for deployment without using resource constraints

(b) Network trafﬁc for deployment after using resource constraints

Fig. 7: Network trafﬁc generated by the hosts while running
REMApp. The vertical axis represents data in bits.

The network interface speciﬁcations chosen to run the ex-
periment were N IC RAT E = 118kbps and N IC CEIL =

Fig. 6: Snapshot of run 3 showing the energy proﬁles of the
different loads and the power from the grid. For the loads, The
light green curve represents the predicted power, the light blue
curve represents the power dispatched by the coordinator. The
green line is the measurment data.

2. At t0 + 20, bring down host h2 (Aggregator).
3. At t0 + 40, start the FDIA attack on host h7 (EV Charger).
4. At t0 + 55 stop the attack.
5. At t0 + 60, end test.

The experiments were repeated three times. For two of the
three runs, the ﬁrst fault targeted a leader node (leader election
is dynamic). This triggered a new leader election, which took
about 1s on average. The average time interval between the
start of a round of voting and the obtaining of its result was
9 ms before and 19 ms after FDIA. Both of these times are
negligible when compared to the controller time-step of 15
minutes real time that was used in the application.

Figure 6 shows how the system performed during run
3 of the experiment. The black vertical line indicates the
end time of the simulation. The right of that line indicates
the future time horizon. Validating the plots in light of the
performance criteria discussed in section III, it can be observed
that Condiiton 1 was satisﬁed, indicated in the pre-fault period
(left of Figure 6) when the power dispatched to both loads was
less than the predicted or requested power. This was because
the available power was not adequate, even after the battery
was discharged, as seen in the state-of-charge (SoC) plot. This
condition was also satisﬁed during the post-fault period where
the power dispatched for the building was capped to 1000 KW,
while the chargers encountered almost total load curtailment.
This was due to the fact the the battery SoC was down to its
minimum speciﬁcation of 0.2 and hence unable to discharge.

Wireshark	IO	Graphs:	capturefile_9.pcapng20304050607080Time	(s)20000400006000080000100000120000140000h1TCP	errors1	sec	Intervals	h2h3h4h5h6h7h8h9Wireshark	IO	Graphs:	capturefile_12.pcapng2832364044485256Time	(s)3000045000600007500090000105000h1TCP	errors1	sec	Intervals	h2h3h4h5h6h7h8h9h10h11h12131kbps (Sec IV: Resource limits). The network limits the
REMApp actors application were speciﬁed as rate = 40kbps
and ceil = 60kbps for each actor. These values were chosen in
this experiment mainly for demonstration purposes. In a pro-
duction deployment, typically, each actor will have different
values which need to be calculated by prior bench marking or
building faithful models and simulation. The solver was run
using the same dspec ﬁle described in ﬁgure 5 but with the
last line uncommented, allowing resource limits. The solution
obtained is shown in Table III, which uses 3 additional nodes
compared to Table II without using resource limits.

Figure 7 shows the tcp network trafﬁc for each host (node)
while running the application captured over 1 second intervals.
Figure 7a corresponds to the conﬁguration of Table II, while
ﬁgure 7b corresponds to Table III). The tcp packets include
the application messages sent via RIAPS along with some
overhead. From the graphs, it can be seen that the average
bandwidth consumed by the hosts is 84k bits per second in
Figure 7a and 71k bits per second in Figure 7b, indicating
a reduction of 15%. Looking at
the maximum bandwidth
consumed, it can be seen that it remained almost entirely
within 100k bits for all hosts, when solved with network
limits, with a few overshoots. However, they were within the
N IC RAT E of 118kbps set for the calculations. On the
other hand, the bandwidth consumed by the host h1 regularly
exceeded 120kbps when deployed without using limits.

VI. CONCLUSION AND FUTURE WORK

This paper introduced a design time framework for pro-
totyping and deploying resilient distributed applications for
cyber-physical systems such as the Smart Grid. The resilient
deployment solver maps distributed application entities or
actors to remote nodes while adhering to both hardware and
resilience speciﬁcations. The deployment was tested using a
real world application of a microgrid energy management
system and successfully demonstrated how the deployment
decision can affect the behavior of a distributed application
under faults and can also help maintain the resource limitations
imposed by the hosting platform.

Future work would include incorporating latency and timing
constraints like [18], and implementing more fault tolerance
patterns.

ACKNOWLEDGMENT

This work was funded in part by the Advanced Research
Projects Agency-Energy (ARPA-E), U.S. Department of En-
ergy, under Award Number DE-AR0000666. The views and
opinions of the authors expressed herein do not necessarily
state or reﬂect those of the US Government or any agency
thereof.

REFERENCES

[1] E. A. Lee and S. A. Seshia, “Introduction to embedded systems - a

cyber-physical systems approach. http://leeseshia.org,” 2011.

[2] O. Boqtob, H. El Moussaoui, H. El Markhi, and T. Lamhamdi, “Micro-
grid energy management system: a state-of-the-art review,” Journal of
Electrical Systems, vol. 15, no. 1, pp. 53–67, 2019.

[3] Y. Li, L. Fu, K. Meng, Z. Y. Dong, K. Muttaqi, and W. Du, “Autonomous
control strategy for microgrid operating modes smooth transition,” IEEE
access, vol. 8, pp. 142 159–142 172, 2020.

[4] S. Thakar, A. Vijay, and S. Doolla, “System reconﬁguration in micro-
grids,” Sustainable energy, grids and networks, vol. 17, p. 100191, 2019.
[5] S. Eisele, I. Mardari, A. Dubey, and G. Karsai, “Riaps: Resilient
information architecture platform for decentralized smart systems,” in
2017 IEEE 20th International Symposium on Real-Time Distributed
Computing (ISORC).
IEEE, 2017, pp. 125–132.

[6] B. Lantz, B. Heller, and N. McKeown, “A network in a laptop: rapid
prototyping for software-deﬁned networks,” in Proceedings of the 9th
ACM SIGCOMM Workshop on Hot Topics in Networks, 2010, pp. 1–6.
[7] J. Onishi, S. Kimura, R. J. James, and Y. Nakagawa, “Solving the
redundancy allocation problem with a mix of components using the
improved surrogate constraint method,” IEEE transactions on reliability,
vol. 56, no. 1, pp. 94–101, 2007.

[8] M. Glaß, M. Lukasiewycz, T. Streichert, C. Haubelt, and J. Teich,
“Reliability-aware system synthesis,” in 2007 Design, Automation &
Test in Europe Conference & Exhibition.

IEEE, 2007, pp. 1–6.

[9] Z. Gao and Z. Wu, “Component assignment for large distributed
embedded software development,” in International Conference on Grid
and Pervasive Computing. Springer, 2007, pp. 642–654.

[10] F. Maticu, P. Pop, C. Axbrink, and M. Islam, “Automatic functionality
assignment to autosar multicore distributed architectures,” SAE Techni-
cal Paper, Tech. Rep., 2016.

[11] S. Smolka and Z. ´A. Mann, “Evaluation of fog application placement

algorithms: a survey,” Computing, feb 2022.

[12] N. Roy, A. Dubey, A. Gokhale, and L. Dowdy, “A capacity planning
process for performance assurance of component-based distributed sys-
tems,” in Proceeding of the second joint WOSP/SIPEW international
conference on Performance engineering - ICPE '11. ACM Press, 2011.
[13] Y. Xia, X. Etchevers, L. Letondeur, T. Coupaye, and F. Desprez,
“Combining hardware nodes and software components ordering-based
heuristics for optimizing the placement of distributed IoT applications
in the fog,” in Proceedings of the 33rd Annual ACM Symposium on
Applied Computing. ACM, apr 2018.

[14] Z. Zhao, G. Min, W. Gao, Y. Wu, H. Duan, and Q. Ni, “Deploying edge
computing nodes for large-scale IoT: A diversity aware approach,” IEEE
Internet of Things Journal, vol. 5, no. 5, pp. 3606–3614, oct 2018.
[15] J. L. Herrera, J. Galan-Jimenez, P. Bellavista, L. Foschini, J. Garcia-
Alonso, J. M. Murillo, and J. Berrocal, “Optimal deployment of fog
nodes, microservices and SDN controllers in time-sensitive IoT scenar-
ios,” in 2021 IEEE Global Communications Conference (GLOBECOM).
IEEE, dec 2021.

[16] S. Pradhan, A. Dubey, S. Khare, S. Nannapaneni, A. Gokhale, S. Ma-
hadevan, D. C. Schmidt, and M. Lehofer, “CHARIOT,” ACM Transac-
tions on Cyber-Physical Systems, vol. 2, no. 3, pp. 1–37, jul 2018.

[17] E.

´Abrah´am, F. Corzilius, E. B. Johnsen, G. Kremer, and J. Mauro,
“Zephyrus2: On the ﬂy deployment optimization using smt and cp
technologies,” in International Symposium on Dependable Software
Engineering: Theories, Tools, and Applications.
Springer, 2016, pp.
229–245.

[18] P. Nuzzo, N. Bajaj, M. Masin, D. Kirov, R. Passerone, and A. L.
Sangiovanni-Vincentelli, “Optimized selection of reliable and cost-
effective safety-critical system architectures,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 39,
no. 10, pp. 2109–2123, 2020.

[19] P. Manolios and V. Papavasileiou, “Virtual integration of cyber-physical
systems by veriﬁcation,” in Proc. AVICPS. Citeseer, 2010, p. 65.
[20] P. Ghosh, S. Eisele, A. Dubey, M. Metelko, I. Madari, P. Volgyesi,
and G. Karsai, “On the design of fault-tolerance in a decentralized
software platform for power systems,” in 2019 IEEE 22nd International
Symposium on Real-Time Distributed Computing (ISORC). IEEE, 2019,
pp. 52–60.

[21] D. T. Ton and M. A. Smith, “The us department of energy’s microgrid
initiative,” The Electricity Journal, vol. 25, no. 8, pp. 84–94, 2012.
[22] D. Ongaro and J. Ousterhout, “In search of an understandable con-
sensus algorithm,” in 2014 {USENIX} Annual Technical Conference
({USENIX}{ATC} 14), 2014, pp. 305–319.

[23] L. de Moura and N. Bjørner, “Z3: An efﬁcient SMT solver,” in Tools
and Algorithms for the Construction and Analysis of Systems. Springer
Berlin Heidelberg, 2008, pp. 337–340.

[24] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in

OSDI, vol. 99, no. 1999, 1999, pp. 173–186.

