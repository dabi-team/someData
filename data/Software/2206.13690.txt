Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

ORIGINAL ARTICLE

Identifying the requirement conﬂicts in SRS documents using
transformer-based sentence embeddings

Garima Malik1 | Mucahit Cevik*1 | Devang Parikh2 | Ayse Basar1

1Data Science Lab, Mechanical Industrial
Engineering Department, Toronto
Metropolitan University, Ontario, Canada
2IBM, North Carolina, USA

Correspondence
*Mucahit Cevik, Data Science Lab,
Mechanical Industrial Engineering
Department, Toronto Metropolitan
University, Toronto, Ontario, M5B 2K3,
Canada. Email: mcevik@ryerson.ca

2
2
0
2

n
u
J

8
2

]
E
S
.
s
c
[

1
v
0
9
6
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

High quality software systems typically require a set of clear, complete and compre-

hensive requirements. In the process of software development life cycle, a software
requirement speciﬁcation (SRS) document lays the foundation of product devel-

opment by deﬁning the set of functional and nonfunctional requirements. It also

improves the quality of software products and ensure timely delivery of the projects.

These requirements are typically documented in natural language which might lead

to misinterpretations and conﬂicts between the requirements. In this study, we aim to

identify the conﬂicts in requirements by analyzing their semantic compositions and

contextual meanings. We propose an approach for automatic conﬂict detection, which

consists of two phases: identifying conﬂict candidates based on textual similarity, and

using semantic analysis to ﬁlter the conﬂicts. The similarity-based conﬂict detection

strategy involves ﬁnding the appropriate candidate requirements with the help of sen-

tence embeddings and cosine similarity measures. Semantic conﬂict detection is an

additional step applied over all the candidates identiﬁed in the ﬁrst phase, where the

useful information is extracted in the form of entities to be used for determining the

overlapping portions of texts between the requirements. We test the generalizability

of our approach using ﬁve SRS documents from diﬀerent domains. Our experiments

show that the proposed conﬂict detection strategy can capture the conﬂicts with high

accuracy, and help automate the entire conﬂict detection process.

KEYWORDS:
Software Requirement Speciﬁcations, Conﬂict Detection, Sentence Similarity, Sentence Embeddings,

Named Entity Recognition

1

INTRODUCTION

Requirement Engineering (RE) is the process of deﬁning, documenting, and maintaining the software requirements 1. RE process
involves four main activities, namely, requirements elicitation, requirements speciﬁcation, requirements veriﬁcation and valida-
tion, and requirements management. In the requirement speciﬁcation process, the deliverable is termed as software requirement
speciﬁcation (SRS) document which is a highly important document in software development life cycle (SDLC) 2. SRS docu-
ments describe the functionality and expected performance for software products, naturally aﬀecting all the subsequent phases
in the process. The requirement set deﬁned in SRS documents are analyzed and reﬁned in the design phase, which results in
various design documents. Then, the developers proceed with these documents to build the code for the software system 3.

 
 
 
 
 
 
2

Malik et al.

SRS documents are mostly written in natural language to improve the comprehensibility of requirements. The success
of any software system is largely dependent on the clarity, transparency, and comprehensibility of software requirements 4.
Unclear, ambiguous, conﬂicting and incomprehensible software requirements might lead to increased project completion times,
ineﬃciency in software systems, and increase in the project budget.

Two requirements are said to be conﬂicting if the implementation of the ﬁrst negatively impacts the second requirement 3,5.
Detection of conﬂicts in the earlier development phase is very important, however, the manual identiﬁcation of these conﬂicts
could be tedious and time-consuming. It is necessary to develop semi-automated or automated approaches for conﬂict detection
in SRS documents. Considering the structure of software requirements, natural language processing (NLP) methods can help
in analyzing and understanding the software requirements semantically. We can use various information extraction techniques
such as named entity recognition (NER), and Parts of Speech (POS) tagging, alongside the semantic similarity of the natural
language text to interpret the context and syntactic nature of the software requirements.

In order to provide an automated approach for generalised conﬂict identiﬁcation, we propose a two-stage framework which
elicits the conﬂict criteria from the provided software requirements, and outputs the conﬂicting requirements. In the ﬁrst phase,
we convert the software requirements into high dimensional vectors using various sentence embeddings, and then identify the
conﬂict candidates using cosine similarity. Then, in the second phase, candidate conﬂict set is further reﬁned by measuring the
overlapping entities in the requirement texts, with high level of overlaps pointing to a conﬂict.

Research Goal
The main focus of this study is to create a NLP-based framework to automatically detect the conﬂicting requirements in SRS
documents. We assume that structure of requirements is based on natural language and it can have a variety of forms such as
modal verb style (include ‘shall’, ‘will’, ‘should’, and ‘must’), user stories, functional, and non-functional requirements. We
leverage deep learning-based techniques to understand the semantics of the requirements. This helps designing a framework
based on text similarity and understanding with the objective of reducing the manual interventions and increasing the eﬃciency
of the automated conﬂict detection. In our analysis, we do not distinguish duplicates from the conﬂicts, as they are both undesired
in software development processes, and refer duplicates as conﬂicts.

Contribution
The main contributions of our study can be summarized as follows:

1. We propose an unsupervised learning-based conﬂict detection algorithm to identify conﬂicts in a given set of requirements.
Previous studies typically converts the requirement texts into formal structures whereas our approach works with raw
requirement text (i.e., natural language text). We use BERT and USE sentence embeddings to capture the semantics
and contextual meaning of the software requirements which generates a set of candidate conﬂicts. Then, we apply an
additional ﬁlter over the candidate set that labels two requirements as conﬂicting if they have a high degree of overlap in
their software-speciﬁc entities.

2. We employ software-speciﬁc entity extraction models to extract the entities from the requirements with the help of manu-
ally annotated corpus of SRS documents and custom feature set. We show how a highly accurate entity recognition model
can be employed for an important practical problem by employing this model within a semantic conﬂict detection method.

3. We conduct comprehensive numerical study using open source requirement sets associated with multiple disciplines
such as healthcare, automobile, aerospace, and transportation which contain annotated conﬂicts. Our analysis contributes
to a better understanding for the capabilities of NLP techniques for the conﬂict detection task in software requirement
engineering.

Structure of the Paper
The remainder of the paper is organized as follows. Section 2 provides the background on the problem of conﬂict identiﬁcation
in software requirement datasets. We also review relevant techniques for conﬂict identiﬁcation in requirement engineering and
software development, particularly over SRS documents. Section 3 introduces our proposed method for automated conﬂict
detection, and provides a detailed discussion over dataset characteristics, sentence embeddings, semantic similarity, and NER.
In Section 4, we present the results from our numerical study, which includes a performance comparison over diﬀerent conﬂict
identiﬁcation strategies for various requirement datasets. Lastly, Section 5 provides concluding remarks and future research
directions.

Malik et al.

2

BACKGROUND

3

Previous studies suggest the use of NLP-based techniques to solve various software requirement related problems such as
requirement classiﬁcation 6,7, ambiguity detection 8, bug report classiﬁcation 9, duplicate bug report prediction 10, conﬂict iden-
tiﬁcation 5, and mapping of natural language-based requirements into formal structures 11. Conﬂict detection is one of the most
diﬃcult problems in requirement engineering 3. Inability in identifying the conﬂicts in software requirements might lead to
uncertainties and cost overrun in software development. Several papers discussed conﬂict identiﬁcation in various domains,
however, an autonomous, reliable and generalizable approach for detecting conﬂicting requirements is yet to be achieved. Below,
we review the relevant literature for automated conﬂict detection over SRS documents.

The terms ‘Ambiguity’ and ‘Conﬂict’ can be misconstrued in the requirement engineering context. Researchers have provided
formal deﬁnitions for the requirement ambiguity as a requirement having more than one meaning, and provided various tech-
niques to detect the requirement ambiguities in SRS documents 1,12. On the other hand, requirement conﬂict detection remains
as a challenging problem, lacking a well-accepted formal deﬁnition and structure. An ambiguity in SRS documents can be either
language-based (lexical, syntactic, semantic, pragmatic, vagueness, generality, and language error ambiguity) 13 or RE-speciﬁc
ambiguities (conceptual translational, requirements document, application domain, system domain, and development domain) 1.
Several studies deﬁne the requirement conﬂict depending upon the domain of requirements. However, the term ‘conﬂict’ can be
deﬁned more broadly as the presence of interference, interdependency, or inconsistency between requirements 14. Kim et al. 15
proposed the deﬁnition of requirement conﬂict as interaction and dependencies present between requirements which results into
negative or undesired operation of the software systems.

Butt et al. 16 deﬁned requirement conﬂicts based on the categorization of requirements to mandatory, essential, and optional
requirements. Kim et al. 15 described the requirement structure as Actor (Noun) + Action (verb) + Object (object) + Resource
(resource). An activity conﬂict can arise when two requirements achieve the same actions through diﬀerent object and a resource
conﬂict may arise when diﬀerent components try to share the same resources. Moser et al. 17 categorized the conﬂicts as simple
(if exists between two requirements) and complex (if exists between three or more requirements). Recently, Guo et al. 5 proposed
a comprehensive deﬁnition for semantic conﬂicts amongst diﬀerent functional requirements. They stated that if two requirements
having inferential, interdependent, and inclusive relationship then it may lead to inconsistent behaviour in software system.

Shah and Jinwala 1 conducted a survey of natural language-based software requirements ambiguity resolution techniques.
They also provided insights into diﬀerent web-based tools for ambiguity detection. They categorize ambiguity detection into
three groups, namely, manual glossaries, rule-based and automatic ontology-based approaches. Given that natural language
requirements are complex in their nature, such heuristic or rule-based parser-dependent methods are prone to errors if semantic
elements of the requirements are not extracted with signiﬁcant accuracy. In another study, Aldekhail et al. 3 reviewed various
papers on the conﬂict detection in software requirements. They concluded that semi-automated methods require a signiﬁcant
manual process, while fully automated models are still based on human analysis of the requirement sets. That is, the models
aiming full automation are still diﬃcult to generalise, resulting in large errors when applied to diﬀerent requirement sets.

Viana et al. 18 presented a system that detected conﬂicting requirements in systems of systems. The paper simulated a smart
home system and focused on conﬂict identiﬁcation in a nutrition system called Feed Me Feed Me. The process assume that
system requirements are written using RELAX framework, which are deﬁned in fuzzy branching temporal logic 19. Such frame-
work assert that requirements have modal components (e.g., SHALL, SHOULD, and MAY), temporal (e.g. EVENTUALLY,
UNTIL, and AS CLOSE AS POSSIBLE TO), and ordinal (e.g., AS MANY, AS FEW AS POSSIBLE) operators. The conﬂict
identiﬁcation system implemented a detection system that had three main steps: overlap detection, conﬂict detection, and con-
ﬂict resolution. With the help of RELAX framework, the algorithm was able to detect overlapping requirements whenever they
used or blocked the same resources, or when they over-used or under-used other resources of the smart home. However, as noted
in the study, the framework is entirely based on a pure simulation, and it is diﬃcult to claim generalizability and scalability of
the proposed approach other datasets and domains.

Guo et al. 5 attempted a deeper dive into the semantics of software requirements using NLP methods coupled with heuristic
rules that helped identify requirement conﬂicts using a divide-and-conquer inspired design. Finer Semantic Analysis-based
Requirements Conﬂict Detector (FSARC) attempted to automate this process through a seven step procedure that relies on
Standord’s CoreNLP library 20. The ﬁrst step employs POS tagging and Stanford’s Dependency Parser (SDP) which convert
each requirement into novel eight tuple: (𝑖𝑑, 𝑔𝑟𝑜𝑢𝑝_𝑖𝑑, 𝑒𝑣𝑒𝑛𝑡, 𝑎𝑔𝑒𝑛𝑡, 𝑜𝑝𝑒𝑟𝑎𝑡𝑖𝑜𝑛, 𝑖𝑛𝑝𝑢𝑡, 𝑜𝑢𝑡𝑝𝑢𝑡, 𝑟𝑒𝑠𝑡𝑟𝑖𝑐𝑡𝑖𝑜𝑛). This eight-tuple is
later passed through several rule-based routines which perform the task of identifying the conﬂicts. The proposed algorithm
performed well and showed promise to be generalizable. On the other hand, the approach relies on CoreNLP library’s ability

4

Malik et al.

to correctly generate the eight-tuple, which is the main input of the algorithm. This also implies that the model would expect
a certain structure to be followed in writing the requirements that are analyzed. In a similar fashion, Ghosh et al. 21 proposed
a method that converts natural language requirements to formal software speciﬁcations by implementing the ‘ARSENAL’, a
method that combine heuristic rules on extracted NLP components using Stanford’s Typed Dependency Parser library (STDP) 22.
Speciﬁcally, they broke down each requirement into ﬁve components: Term Type, Negated Or Not, Quantiﬁer Type, Relations /
Attributions, and Lists. Once their parts were tagged in the natural language requirements, they were passed into heuristic rules
to output a formal requirement set. Sabriye and Zainon 12 also developed a simple rule-based algorithm that detect software
requirement ambiguities by using POS tagging.

Many other studies explored the use of supervised machine-learning techniques in identifying conﬂicting requirements, major-
ity of which oﬀering a semi-automated approach (i.e., requiring manual inspection of the identiﬁed conﬂicting requirements).
Polpinij and Ghose 23 suggested a 4-step fuzzy decision process for conﬂict detection. The ﬁrst step involved manual labeling of
training data which was fed into a fuzzy decision-tree classiﬁer. The second step trained this model, while the third one predicted
the unlabeled instances using the trained decision-tree classiﬁer. In the ﬁnal step, the most ambiguous instances were ﬂagged for
potential removal or treatment. A Support Vector Machines (SVM) was employed to detect patterns of conﬂicting requirements
with the aim of increasing separation between the two classes of positive and negative elements of the requirements.

Subha and Palaniswami 24 trained a naive Bayes classiﬁer for conﬂict detection and extraction of software ontologies in
requirement engineering domain. It was based on word probability and word count method. The algorithm relied on the prob-
abilities of words coming from the text of the training data. Sharma et al. 25 modiﬁed the same algorithm as they added a
pre-processing step where the training data was tokenized using bag of words (BoW) method which discarded the order of words
in a text while keeping track of the frequency of each word. This was followed by the removal of stop words that did not add
contextual meaning to sentences. As a ﬁnal pre-processing step prior to feeding the training data into the naive Bayes Classiﬁer,
POS tags were also added to the data. Sardinha et al. 26 developed a tool called EA-Analyser for aspect-oriented requirements.
They applied Bayesian learning method to identify the conﬂicting dependencies in the requirements. Oo et al. 27 implemented
various machine-learning models and found that naive Bayes classiﬁer produced more consistent results in detecting conﬂicts
compared to SVM and decision-tree classiﬁers. Osman and Zaharin 2, experimented with similar models on Malay software
requirement datasets, and concluded that the naive Bayes classiﬁer generated best results.

Our work diﬀers from these existing studies in multiple ways. First, we propose a two phase generalised approach for auto-
mated conﬂict detection using unsupervised learning. To this end, Das et al. 28 introduced the idea of sentence embeddings
for similarity detection in software requirements. We extend this idea and combine the two sentence embeddings (BERT and
TFIDF) to calculate the cosine similarity between the requirements. Second, our approach directly works with software require-
ments as opposed to Guo et al. 5, which converts the software requirements into formal representations and apply rule-based
procedures to detect the conﬂicts. Third, diﬀerent from ﬁner semantic analysis enabled by Guo et al. 5’s rule-based approach, we
deﬁne the set of software-speciﬁc entities, and train machine learning models with handcrafted features dedicated to software
requirements. These entities provide an additional way of verifying the conﬂicts semantically.

3 METHODOLOGY

In this section, we ﬁrst describe the SRS datasets used in our numerical study, which is followed by a brief discussion on sentence-
embeddings used in the experiments. Then, we provide speciﬁc details of the building blocks of our conﬂict detection algorithm
and the experimental setup. Figure 1 shows our proposed framework for automated conﬂict detection in SRS documents. Our
technique comprises two phases, which are similarity-based conﬂict detection (Phase I) and semantic conﬂict detection (Phase
II).

Datasets

3.1
We consider ﬁve SRS datasets that belong to various domains such as software, healthcare, transportation, and hardware. Three
of these are open-source SRS datasets (OpenCoss, WorldVista, and UAV), and the other two are extracted from public SRS
documents. Generally, requirements are documented in a structured format and we retain the original structure of requirements
for conﬂict detection process. To maintain the consistency in requirement structure, we converted complex requirements (para-
graphs or compound sentences) into a simple sentences. Originally, these datasets included very few conﬂicts. As such, we

Malik et al.

5

FIGURE 1 Proposed framework for identifying the conﬂicting requirements in SRS documents.

added synthetic conﬂicts in each SRS dataset to augment the data for our analysis. Table 1 provides summary information on
the SRS datasets.

TABLE 1 Dataset characteristics

Dataset

Domain

# of requirements

# of original conﬂicts

# of synthetic conﬂicts

Transportation

OpenCoss
WorldVista Medical
UAV
PURE
IBM-UAV

Aerospace
Thermodynamics
Hardware

115
140
116
125
105

6
5
4
0
2

5
23
16
21
13

Table 2 shows sample data instances, demonstrating the general structure and format of the requirements used in the conﬂict
detection process. Each requirement dataset consists of requirement id, requirement text, and ‘Conﬂict’ column indicating the
presence of conﬂict in ‘Yes’ or ‘No’ format. We also included the ‘Conﬂict-Label’ column to indicate the pair of conﬂicts. For
instance, requirements 1 and 2 are conﬂicting due to the diﬀerent instruction for charging the same ‘UAV’.

Below, we brieﬂy describe all the SRS datasets used in the numerical study.

• OpenCoss: OPENCOSS1 refers to Open Platform for Evolutionary Certiﬁcation Of Safety-critical Systems for the rail-
way, avionics, and automotive markets. This is a challenging dataset to identify the conﬂicts as the samples from the

1http://www.opencoss-project.eu


Requirements Sentence embeddingsGenerate  cosine similarity matrixGenerate ROC       curveR1 to RnCandidate  conflict setIdentify similar requirement set for each conflict candidateNamed entity recognition (NER)NLTK tagger (Noun and Verb)Software tagger (Actor and Action)Compute the overlapping tags ratioConflict  requirementsNon-conflict  requirementsRatio > T0  Ratio < T0  Phase I: Similarity-based Conflict DetectionPhase II: Semantic Conflict DetectionDetermine cosine similarity cutoff6

Malik et al.

TABLE 2 Sample data instances from UAV dataset with requirement id, text, and conﬂict label (Yes/No). Requirements 1 and
2 are conﬂicting.

Req. Id Requirement text

Conﬂict Conﬂict-Label

1.
2.
3.
4.

The UAV shall charge to 50 % in less than 3 hours.
Yes
Yes
The UAV shall fully charge in less than 3 hours.
The Aviary shall provide a remote surveillance from the air of a location within 32.19 Km of the origin. No
No
Remote surveillance shall include video streaming for manual navigation of the surveillance platform.

Yes (2)
Yes (1)
No
No

OpenCoss dataset indicates a lot of similar or duplicate requirements with repeating words. Initially, this set included 110
requirements and we added 5 more synthetic conﬂicts.

• WorldVista: WorldVista2 is a health management system that records patient information starting from the hospital
admission to discharge procedures. The requirement structure is basic, and written in natural language with health care
terminologies. It originally consisted of 117 requirements and we added 23 synthetic conﬂicts.

• UAV: The UAV (Unmanned Aerial Vehicle) 5,29 dataset is created by the University of Notre Dame and it includes all the
functional requirements which deﬁne the functions of the UAV control system. The requirement syntax is based on the
template of EARS (Easy Approach to Requirements Syntax) 30. Originally, this dataset had 99 requirements and we added
16 conﬂicting requirements to the set, which resulted in a conﬂict proportion of 30%.

• PURE: PURE (Public Requirements dataset), contains 79 publicly available SRS documents collected from the web 31.
We manually extracted set of requirements from two SRS documents, namely, THEMAS (Thermodynamic System) and
Mashbot (web interface for managing a company’s presence on social networks). In total, we collected 83 requirements
and induced synthetic 21 conﬂicts to maintain consistency with the other datasets.

• IBM-UAV: This dataset is proprietary, and provided by IBM. It consists of software requirements used in various projects
related to the aerospace and automobile industry. We sampled 75 requirements from the original set, and introduced 13
synthetic conﬂicts. The requirement text follows a certain format speciﬁed by IBM’s RQA (Requirement Quality Analysis)
system.

Embeddings and Cosine Similarity

3.2
Word embeddings can be used to convert the text into vectors of real numbers to prepare the data for any kind of modeling. For
the conﬂict detection problem, we applied sentence embeddings to encode the meaning or context of software requirements.
Sentence embeddings use the weighted average of the embeddings of each word present in the sentence, and apply dimension-
ality reduction techniques to extract the embeddings 32,28. For the requirement datasets, we considered TFIDF as the baseline
embedding. We also experimented with deep learning-based USE embedding and transformer-based BERT embeddings. Below,
we brieﬂy summarize diﬀerent embeddings used in our analysis.

• TFIDF: TFIDF stands for Term Frequency Inverse Document Frequency, a commonly-used method in information
retrieval tasks 33. It is a numerical measure to determine the importance of words based on their frequency. TFIDF creates
the vectors by weighing the terms according to their prevalence across the documents. The TFIDF value for each word
‘𝑖’ in the document can be expressed as the multiplication of TF
values. TF denotes the number of times term
‘𝑖’ appears in the document ‘𝑑’, and IDF denotes the inverse of document frequency which measures the signiﬁcance of
term ‘𝑖’.

and IDF

𝑖,𝑑

𝑖

• USE: USE stands for Universal Sentence Encoder 32. It encodes natural language text into high-dimensional vectors. These
vectors can be used in text classiﬁcation, document clustering, semantic similarity detection, and other natural language-
speciﬁc tasks. The pre-trained USE consists of two variations, one trained with transformer encoder and the other trained
with Deep Averaging Network (DAN). USE encoder is trained and optimized for inputs such as sentences, phrases, and

2http://coest.org/datasets

Malik et al.

7

short paragraphs, and they are suitable for applying sentence level embeddings over the requirements. It is also trained on a
diverse set of transfer learning tasks such as sentence-based classiﬁcation, multilingual tasks, and ﬁne-grained questions-
answer retrieval. We applied the DAN version of the USE embeddings for the software requirements, and generated the
512-dimensional vector for each input requirement.

• BERT-TFIDF: BERT stands for Bidirectional Encoder Representation from Transformers 34. In this embedding, we
combine the vector obtained from BERT text embedding with TFIDF embedding. The former harnesses state-of-the-art
technology that can vectorize and detect context and semantics, while the latter is a probabilistic method that gives more
importance to the words that occur less often in a given document. The merging process involve two steps. In the ﬁrst
step, we simply concatenate the embedding vectors, and in the second step we perform dimensionality reduction. This
step help reduce the initial resulting dimension of the embedded vectors. BERT embeddings are comprised of 768 features
while TFIDF embeddings can span into thousands of features (depending on the data) hence the need for dimensionality
reduction. To maintain the uniform vector size, we employ Uniform Manifold Approximation and Projection (UMAP) 35,
which is an eﬀective dimensionality reduction tool.

Cosine similarity is a distance measure that can be used to calculate the similarity between two words, sentences, paragraphs,
or the whole document 36. It is an eﬀective measure to estimate the similarity of vectors in high-dimensional space 37. This metric
models language-based input text as a vector of real-valued terms and the similarity between two texts is derived from the cosine
angle between two texts term vectors as follows:

cos(𝐫𝟏, 𝐫𝟐) =

𝐫𝟏𝐫𝟐
‖𝐫𝟏‖‖𝐫𝟐‖

=

√∑𝑛

∑𝑛

𝑖=1 𝐫𝟏𝑖𝐫𝟐𝑖
√∑𝑛

𝑖=1 (𝐫𝟏𝑖)2

𝑖=1 (𝐫𝟐𝑖)2

(1)

The values for cosine similarity ranges from -1 to 1 where -1 signiﬁes dissimilarity and 1 signiﬁes similarity. To better demon-
strate how cosine similarity can be used over embedding vectors, we provide an illustrative example with three sample software
requirements, 𝑟1

, which are deﬁned as follows:

, and 𝑟3

, 𝑟2

• 𝑟1

= ‘The OPENCOSS platform shall be able to export evidence traceability links of an assurance project to external

tools.’

• 𝑟2

= ‘The OPENCOSS platform must be able to send out evidence traceability links of an assurance project to external

tools and internal tools.’

• 𝑟3

= ‘The OPENCOSS platform shall provide users with the ability to specify evidence traceability links in traceability

matrices.’

We calculate the cosine similarity between these requirement vectors when embedded with TFIDF, BERT-TFIDF, and USE.
Table 3 shows the cosine similarity values between these requirements. BERT-TFIDF is able to capture the frequency based
with a cosine similarity value of 0.88. USE being the second highest with
features and semantic similarity between 𝑟1
the value of 0.81. Requirement text for 𝑟3
is not similar to those of the other two requirements, and all the sentence embeddings
indicate low values of cosine similarity with 𝑟3

and 𝑟2

.

TABLE 3 Cosine similarity between 𝑟1

, 𝑟2

, and 𝑟3

with diﬀerent sentence embeddings.

TFIDF BERT-TFIDF USE

cos(𝑟1
cos(𝑟1
cos(𝑟2

, 𝑟2
, 𝑟3
, 𝑟3

)
)
)

0.44
0.16
0.09

0.88
0.50
0.47

0.81
0.55
0.40

8

Malik et al.

Phase I: Similarity-based Conﬂict Detection

3.3
Algorithm 1 summarize the process provided in the left panel of Figure 1. We employ a similarity-based conﬂict detection
approach to automatically identify the conﬂicts in a given requirement set. We also use the resulting set of conﬂicts as a candidate
conﬂict set for Phase II of our framework. We ﬁrst create the sentence embedding vector for each requirement 𝑟 ∈  using
SentenceEmbedding() procedure. It basically converts the requirements into numerical vector using one of the embeddings
deﬁned in Section 3.2.

We next calculate the pairwise distance matrix (Δ), which measures the cosine similarity value between each pair of require-
ments. This matrix can act as a look up table to extract the most similar requirement and corresponding cosine similarity value
for a given requirement. Then, we use ROC curve (receiver operating characteristic curve) to identify the cosine similarity
threshold (𝛿), which speciﬁes the minimum similarity value after which requirements are labeled as conﬂicting. The cutoﬀ value
(𝛿) is selected as the value that maximizes {TPR(Δ, 𝑘) − (1 − FPR(Δ, 𝑘))} over threshold values 𝑘 ∈ {0.01, … , 1.00} and the
distance matrix Δ. This way, we balance the false positives and true positives rates, conﬂict having the positive label. Lastly,
we assign labels of conﬂict or no-conﬂict to the requirements using 𝛿 as threshold value. The candidate conﬂict set () contains
all the requirements with conﬂict label. Note that conﬂict property is symmetric, i.e., if 𝑟1
is also
conﬂicting with 𝑟1

is conﬂicting with 𝑟2

, and, 𝑟1, 𝑟2 ∈ .

, then 𝑟2

Algorithm 1 Similarity-based Conﬂict Detection

Input:

Requirement set:  = {𝑟1, 𝑟2, … , 𝑟𝑛}

Output:

Candidate conﬂict set: 
⃗ ← SentenceEmbedding()
Δ ← PairwiseDistance( ⃗)
𝛿 ← arg max𝑘∈{0.01,…,1.00}
 ← AssignLabels( ⃗,𝛿)

{TPR(Δ, 𝑘) − (1 − FPR(Δ, 𝑘))

}

// Generate requirement vectors
// Calculate the cosine similarity matrix
// Determine the cosine similarity cutoﬀ
// Label the requirements

Phase II: Semantic Conﬂict Detection

3.4
Algorithm 2 describes the process of semantic conﬂict detection as presented in right panel of Figure 1. This algorithm serves
as a second ﬁlter on the candidate conﬂicts generated in Phase I. Speciﬁcally, any candidate conﬂict 𝑐 ∈  is semantically
compared against top 𝑚 most similar requirements from . That is, by focusing on only 𝑚 most similar requirements, we reduce
the computational burden, and also make use of the cosine similarity between the requirements. This semantic comparison is
performed based on overlap ratio between the entities present in the requirements. For a given candidate conﬂict 𝑐 ∈ , overlap
ratio is calculated as

{

max𝑟∈

𝑐

Overlap(𝑐, 𝑟)

}

UniqueEntities(𝑐)

𝑣𝑐 =

(2)

𝑐

where 
represents the set of 𝑚 most similar requirements to candidate conﬂict 𝑐. The function Overlap(𝑐, 𝑟) calculates the
number of overlapping entities between 𝑐 and 𝑟, and function UniqueEntities(𝑐) calculates the number of unique entities in
candidate conﬂict (i.e., a requirement text) 𝑐. The calculated overlap ratio 𝑣𝑐
for 𝑐 ∈  is then compared against a pre-determined
. In our analysis, we set 𝑚 = 5 and 𝑇𝑜 = 1, which
≥ 𝑇𝑜
overlap threshold value, 𝑇𝑜
are determined based on preliminary experiments. In Algorithm 2, GetSimilarRequirements(𝑐, , 𝑚) returns the list  of
𝑚 most similar requirements from  for candidate conﬂict 𝑐, and GetMaxOverlapRatio(𝑐, ) returns the maximum value for
the overlaps between requirements from set  and candidate conﬂict 𝑐.

, and 𝑐 is added to ﬁnal conﬂict set ̄ if 𝑣𝑐

We also provide sample calculations in Table 4 to better illustrate the process in Algorithm 2. We show the overlapping
software-speciﬁc entities present in candidate requirement (𝑐) and similar requirement (𝑟 ∈ ) with diﬀerent color codes. For
instance, entity ‘UAV’ is represented by blue color. Requirements 𝑟1
both return high overlap ratios, indicating that they
are conﬂicting with 𝑐.

and 𝑟2

Malik et al.

Algorithm 2 Semantic Conﬂict Detection

Input:

Candidate conﬂict set:  = {𝑐1, 𝑐2, … , 𝑐𝑡}
Requirement set:  = {𝑟1, 𝑟2, … , 𝑟𝑛}
# of similar requirements: 𝑚

Initialization:
̄ = ∅
𝑇𝑜 = 1
𝑚 = 5
For 𝑐 ∈  do:

 ← GetSimilarRequirements(𝑐, , 𝑚)
𝑣 ← GetMaxOverlapRatio(𝑐, )
If 𝑣 ≥ 𝑇𝑜

:
̄ ← ̄ ∪ {𝑐}

return ̄

9

// Initialise conﬂict set to an empty set
// Set overlapping threshold as 1
// Set number of similar requirements as 5

// Get the 𝑚 similar requirements
// Calculate the maximum overlap ratio using Equation (2)
// Compare with threshold
// Augment the ﬁnal conﬂict set

TABLE 4 Sample candidate (𝑐) and set of similar requirements () to calculate the overlapping ratio (𝑣). The maximum count
of overlap is 7 which resulted in overlap ratio value 𝑣 as 1.

Candidate Requirement

Similar Requirements

Overlap(𝑐, 𝑟) Ratio (𝑣)

= ’The UAV ﬂight range shall

𝑟1
be no less than 20 miles.’

= ‘The UAV ﬂight range shall

𝑟2
be a minimum of 20 miles.’

𝑐 = ‘The UAV ﬂight range shall
be no less than 20 kilometers.’

= ‘The UAV shall be able to autonomously a

𝑟3
ﬂight plan consisting of a set of waypoints
within its range and ﬂight capabilities.’

= ‘The UAV shall be able to autonomously plan

𝑟4
a ﬂight consisting of a set of waypoints
within its range and ﬂight capabilities.’

= ‘The Pilot controller shall be able to download

𝑟5
a ﬂight plan from the Hummingbird consisting
of a set of waypoints within the ﬂight range of the
Hummingbird.’

7

5

2

2

2

1.00

0.71

0.28

0.28

0.28

Named Entity Recognition

3.5
We employ NER to verify the conﬂict candidates semantically and then relabeling it as conﬂict or non-conﬂict. The purpose of
this step is to extract the relevant entities from requirements which can contribute in identifying the conﬂicting requirements. For
Phase II, we employ two NER approaches, namely, general NER and software-speciﬁc NER, and use Machine Learning-based
Conditional Random Fields (ML-CRF) algorithm for NER model training. We brieﬂy describe the details of these methods
below.

• General NER: Rupp et al. 38 suggest that a software requirement should follow the structure as Actor (Noun) + Object
+ Action (Verb) + Resource. The generic NER method extracts ‘Noun’ and ‘Verb’ tags from the requirements based on
this structure. We employed POS tagger provided in NLTK (Natural Language Toolkit) library in Python. NLTK unigram
tagger works best with all the standard corpora such as Brown, Penn Treebank and CoNLL2000 39.

• Software-speciﬁc NER: To create a software-speciﬁc NER model, we sampled some requirements from each of the
software requirement datasets described in Section 3.1. Software-speciﬁc entities can provide more useful information

10

Malik et al.

about the software requirements and help in the semantic conﬂict detection process. We annotated a total of 378 software
requirements to train the ML-CRF model. That is, as we have small number of annotated requirements, we chose to use
a probabilistic ML model to perform the software-speciﬁc entity extraction. There are four main software entities (Actor,
Action, Object, and Property) which are typically used in literature to convert the software requirements into formal
representations 11. We included two additional relevant entities (Metric and Operator) speciﬁc to our IBM-UAV dataset
which account for numerical values (units, digits, and operators) in the requirements. The software-speciﬁc entities are
described as follows:

– Actor: Actor refers to the actors or nouns present in a requirement. It also includes intended users and external

systems interacting with the software, e.g., UAV, Aviary, and Themas System.

– Action: Action can be interpreted as the main verb in a requirement or an operation performed by an actor on some

object 11.

– Object: Object involves any input/output and resource of the system that is mentioned in the software requirement 11.
– Property: Property includes all adjectives mentioned for objects/actions that explains the properties of the objec-
t/action involved. Consider this requirement ‘The aviary shall support autonomous mission plans as well as
pilot-directed ﬂight’. Here, ‘autonomous mission plans’ and ‘pilot-directed ﬂight’ represents the property tag.

– Metric: Standard unit measures such as ‘𝑘𝑚’, ‘𝑚2’, and ‘𝑚∕𝑠2’.
– Operator: Relational operators such as ‘>=’, ‘<=’, ‘less than’, and ‘greater than’.

Table 5 shows POS tags and software-speciﬁc tags obtained for sample requirements from UAV dataset.

• Machine Learning-based Conditional Random Fields: This method provides a probabilistic approach for NER and it
is also known as discriminative classiﬁer. It models the conditional probability of assigning appropriate tag sequence ̂𝑦
with the help of input text 𝑥 as ̂𝑦 = arg max𝑦 𝑃 (𝑦|𝑥). In our NER task, the input data is sequential, and we have to take
the context into account while predicting the tags for a particular word. We use customised feature set as an additional
input to the model to incorporate this behaviour. In software-speciﬁc entity extraction, we create a comprehensive list of
features which includes lexical (input tokens), contextual (tokens in the locality of input text with window size of [−1, 1]),
character-level (digits and alphabets), and orthographic (capitalization and abbreviations) features.

TABLE 5 Sample NER results for the UAV dataset

NER method

Requirements

Description

General NER

The aviary shall support autonomous mission
plans as well as pilot-directed ﬂight.
The UAV shall fully charge in less than 3 hours. Noun - UAV, hours

Noun - mission, plans, ﬂight
Verb - support

Verb - charge

The aviary shall support autonomous mission
plans as well as pilot-directed ﬂight.

Software-speciﬁc NER

The UAV shall fully charge in less than 3 hours.

Actor - aviary
Action - support
Property - autonomous mission plans,
pilot-directed ﬂight

Actor - UAV
Action - charge
Operator - less than
Metric - 3 hours

Malik et al.

11

Experimental Setup

3.6
In the numerical analysis, we consider various sentence embeddings from simple transformers library3. That is, we use
general purpose pretrained models which have been evaluated on various NLP tasks. Speciﬁcally, we conduct experiments
using several BERT variants by instantiating pre-trained transformer models (i.e., distill-base-multilingual, all-MiniLM-L12-
V25, distillRoberta-v1, and mpnet-base-v24). Our preliminary experiments suggest diﬀerent sentence embeddings for each
requirement datasets. In BERT-TFIDF embeddings, we use all-distillRoberta-v1 for Opencoss, UAV and PURE, Distiluse-base-
multilingual-cased-v1 for WorldVista and all-mpnet-base-v2 for IBM-UAV datasets.

In Phase I of our conﬂict identiﬁcation process, the main objective is to extract all the possible requirements which can be
analyzed semantically to determine the ﬁnal set of conﬂicts in the SRS documents. We perform 3-fold cross validation over all the
requirement datasets. That is, considering the distribution of conﬂicting requirements and the limited number of requirements,
we divide each dataset into 3 diﬀerent folds. Each fold includes some conﬂicting and non-conﬂicting requirements, however,
we make sure that each conﬂict present in the fold should have its conﬂict pair present in the same fold. For our techniques, we
use training set to determine the cosine similarity cut-oﬀ value, and apply this value on the corresponding test set. We employ
standard classiﬁcation metrics such as macro averaged version of F1-score, precision and recall in performance evaluation.

In Phase II, we train a software-speciﬁc entity extraction model (ML-CRF). We use grid-search method to ﬁnd the best
parameters for ML-CRF model. The main hyperparameters used in the ﬁnal ML-CRF model are ‘lbfgs’ for the algorithm, 0.1
for c1 and c2 (i.e., regularization parameters), and 100 for max_iterations. To evaluate the performance of the ML-CRF model,
we employ 5-fold cross validation, and consider standard NER metrics such as accuracy, precision, recall and F1-score.

4

RESULTS

In this section, we ﬁrst assess the performance of the similarity-based conﬂict detection algorithm (i.e., Phase I), and present the
results from our comparative analysis with various sentence embeddings for each of the requirement datasets. Next, we report
the results for entity extraction methods that are used to understand the semantics of the requirements. Lastly, we discuss the
performance of semantic conﬂict detection algorithm for all the requirement datasets.

Similarity-based Conﬂict Detection Performance

4.1
The ﬁrst step that comes after calculating the similarity matrix between the requirements is the generation of ROC curves, which
are used to obtain the similarity cut-oﬀ for conﬂict detection based on TPR and FPR values. Figure 2 shows the ROC curves for
the UAV dataset with BERT-TFIDF embedding in each fold of the 3-fold cross validation. We prepare similar ROC curves for
all the other requirement sets which are provided in the appendix (see Section 6).

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 2 ROC curves for UAV requirement set with BERT-TFIDF embedding across 3 folds

3https://www.sbert.net/docs/pretrained_models.html
5https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
4https://huggingface.co/sentence-transformers/all-mpnet-base-v2

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC12

Malik et al.

Table 6 shows the summary results of similarity-based conﬂict detection approach with diﬀerent sentence embeddings. We
report the average values obtained from 3-fold cross validation for cosine similarity cutoﬀ and standard classiﬁcation metrics
along with conﬂict class support values. In terms of the embeddings, BERT-TFIDF provides the best performance for all the
datasets except for IBM-UAV dataset where USE performs slightly better than BERT-TFIDF. As expected, TFIDF embedding
performs worse than all the other embedding because it only considers the frequency based features to create the requirement
vectors. We also observe that USE generates high cutoﬀ value for each dataset in comparison to other sentence embeddings.

TABLE 6 Evaluation results for the similarity-based conﬂict detection approach on all the requirement datasets. Highlighted text
represents the best sentence embedding for each dataset with its respective performance values. Reported values are averaged
over 3 folds and results are shown as “mean ± standard deviation”.

Dataset

Embeddings

Cosine cutoﬀ

F1-score

Recall

Precision

Support

OpenCoss

0.67
TFIDF
USE
0.91
BERT-TFIDF 0.79

WorldVista TFIDF

0.24
USE
0.66
BERT-TFIDF 0.42

UAV

PURE

IBM-UAV

TFIDF

0.44
0.79
USE
BERT-TFIDF 0.64

0.49
TFIDF
0.88
USE
BERT-TFIDF 0.70

TFIDF

0.59
0.89
USE
BERT-TFIDF 0.76

0.43 ± 0.04
0.41 ± 0.05
0.43 ± 0.04
0.85 ± 0.03
0.82 ± 0.01
0.86 ± 0.05
0.84 ± 0.10
0.90 ± 0.03
0.90 ± 0.04
0.79 ± 0.08
0.86 ± 0.11
0.88 ± 0.11
0.60 ± 0.19
0.68 ± 0.24
0.64 ± 0.22

0.68 ± 0.23
0.68 ± 0.18
0.68 ± 0.23
0.86 ± 0.04
0.83 ± 0.08
0.86 ± 0.04
0.85 ± 0.11
0.97 ± 0.04
0.91 ± 0.06
0.71 ± 0.11
0.80 ± 0.17
0.80 ± 0.17
0.55 ± 0.22
0.65 ± 0.28
0.65 ± 0.28

0.34 ± 0.06
0.30 ± 0.02
0.34 ± 0.06
0.84 ± 0.06
0.84 ± 0.11
0.86 ± 0.12
0.83 ± 0.11
0.85 ± 0.05
0.89 ± 0.04
0.92 ± 0.10
0.95 ± 0.06
1.00 ± 0.00
0.72 ± 0.20
0.73 ± 0.16
0.73 ± 0.12

7.33
7.33
7.33

18.66
18.66
18.66

11.66
11.66
11.66

14.00
14.00
14.00

9.33
9.33
9.33

In general, we ﬁnd that the performance of the similarity-based conﬂict detection approach varies with the datasets. With
BERT-TFIDF embedding, we achieve F1-score of 90%, 88%, and 86% for UAV, PURE and WorldVista datasets, respectively.
We also obtain better recall values for these datasets as 91%, 80%, and 86%, respectively, which shows the capability of our
approach in identifying the conﬂicting requirements. For OpenCoss dataset, our approach shows relatively poor performance
with an F1-score of 43% and recall value of 68%. In this dataset, the structure/text of the requirements are very similar to each
other, and algorithm labels them as conﬂict based on the higher value of cosine similarity. For IBM-UAV dataset, we note that
USE and BERT-TFIDF embeddings provides F1-score of 68% and 64%, respectively. Higher standard deviation values for this
dataset can be explained by the smaller support for conﬂict class.

Semantic Conﬂict Detection Results

4.2
In Phase II of the conﬂict detection approach, we validate the presence of conﬂicts semantically by extracting the useful entities
from the requirements. It is also an additional step over the similarity-based conﬂict detection approach. Table 7 shows the entity-
speciﬁc performance of the software-speciﬁc NER model with ‘Actor’ being the best performing entity with an F1-score of
85%. Other entities (Metric, Object, Operator, and Property) have lower support values in the test set which results into average
performance. Because we directly use the NLTK package for general NER, we do not assess the entity extraction performance
(e.g., to accurately identify nouns or verbs) for this method.

We choose the best performing embedding from Phase I to apply the semantic detection approach, and implement the software-
speciﬁc NER and general NER techniques. Table 8 shows the comparative results for both NER models for the candidate conﬂict
sets obtained from Phase I. For WorldVista, PURE, and IBM-UAV dataset, software-speciﬁc NER model works better than

Malik et al.

13

TABLE 7 Entity speciﬁc performance of software-speciﬁc NER model (ML-CRF) used in semantic conﬂict detection approach.
Reported values are averaged over 5 folds and results are shown as “mean ± standard deviation”.

Precision

Recall

F1-score

Support

Action
Actor
Metric
Object
Operator
Property

Micro avg
Macro avg
Weighted avg

0.80 ± 0.02
0.87 ± 0.04
0.68 ± 0.20
0.76 ± 0.09
0.80 ± 0.18
0.69 ± 0.04
0.77 ± 0.04
0.77 ± 0.04
0.77 ± 0.03

0.75 ± 0.03
0.83 ± 0.03
0.70 ± 0.11
0.63 ± 0.09
0.58 ± 0.28
0.63 ± 0.09
0.71 ± 0.03
0.69 ± 0.06
0.71 ± 0.03

0.78 ± 0.02
0.85 ± 0.03
0.68 ± 0.14
0.68 ± 0.08
0.62 ± 0.17
0.65 ± 0.03
0.74 ± 0.03
0.71 ± 0.05
0.74 ± 0.03

91.0
86.0
7.6
38.6
6.0
130.0

359.2
359.2
359.2

general NER with the F1-score of 82%, 88%, and 69%, respectively. For OpenCoss and UAV datasets, general entity extraction
of nouns and verbs outperforms software NER with the F1-score of 47% and 87%, respectively.

TABLE 8 Performance of semantic conﬂict detection approach with NER methods and best sentence embedding (BERT-TFIDF)
from Phase I. Reported values are averaged over 3 folds and results are shown as “mean ± standard deviation”. F1-score shows
the relative and absolute percentage change in comparison to Phase I results.

Dataset

NER Method

OpenCoss

General NER
Software NER

WorldVista General NER

UAV

Software NER

General NER
Software NER

PURE

General NER

Software NER

IBM-UAV

General NER

Software NER

F1-score

Recall

Precision

Support

0.47 ± 0.04 (↑ 0.04 / 9.30%)
0.43 ± 0.04
0.70 ± 0.12
0.82 ± 0.03 (↓ 0.04 / -4.65%)

0.87 ± 0.04 (↓ 0.03 / -3.33%)
0.80 ± 0.02
0.84 ± 0.17
0.88 ± 0.11 (0.00 / 0.00%)
0.63 ± 0.19
0.69 ± 0.25 (↑ 0.05 / 7.93%)

0.68 ± 0.23
0.68 ± 0.23
0.57 ± 0.16
0.74 ± 0.08

0.86 ± 0.07
0.74 ± 0.00
0.76 ± 0.24
0.80 ± 0.17
0.55 ± 0.22
0.65 ± 0.30

0.41 ± 0.13
0.34 ± 0.06
0.96 ± 0.05
0.87 ± 0.06

0.88 ± 0.03
0.86 ± 0.04
1.00 ± 0.00
1.00 ± 0.00
0.82 ± 0.13
0.77 ± 0.20

7.33
7.33

18.66
18.66

11.66
11.66

14.00
14.00

9.33
9.33

The added step of semantic conﬂict detection aims to reduce the uncertainty in labeling a particular requirement as conﬂict
solely based on the similarity measure. Table 8 also reports the absolute and relative percentage change in F1-score with respect
to Phase I for all the requirement sets. We observe an absolute improvement in F1-score for OpenCoss and IBM-UAV datasets
by 4% and 5%, respectively. There is also a drop of 3% and 4% F1-score for WorldVista and UAV dataset compared to the results
presented in Table 6. For PURE, Phase II does not lead to any change in performance.

Figure 3 shows the binary classiﬁcation results in the form of normalized confusion matrices averaged over 3 folds, where
the ‘conﬂict’ class is taken as the positive label. In general, our approach gives good performance in identifying the non-conﬂict
class, and, for the conﬂict class, the performance is highly impacted by the distribution of the class labels. We manage to improve
the number of true positives in OpenCoss and IBM-UAV datasets. Alongside, we notice an increase in false positives and false
negatives for all the datasets. We also present the detailed confusion matrices for each dataset across 3-folds in appendix (see
Section 6).

14

Malik et al.

(a) OpenCoss

(b) WorldVista

(c) UAV

(d) PURE

(e) IBM-UAV

FIGURE 3 Classiﬁcation results in the form of normalised confusion matrices (‘Conﬂict’ as positive class) with semantic
conﬂict detection approach.

5

CONCLUSION

Software project development typically requires precise and unambiguous requirements to ensure timely delivery of software
projects. Previous studies suggest converting the software requirements into formal representations before the detection of ambi-
guities and conﬂicts in the requirement set. On the other hand, the majority of SRS documents are written in natural language.
We develop a two-phase process for automatic conﬂict detection from SRS documents which works directly on natural language-
based requirements. In Phase I, with the help of transformer-based sentence embeddings, we convert the software requirements
into numeric vectors, and determine the similarity between the requirements using cosine similarity. Next, we employ the ROC
curves to determine the cosine similarity cutoﬀ, which help obtaining the candidate conﬂict set. In Phase II, we apply the gen-
eral and software-speciﬁc NER to extract the meaningful entities from the candidate requirements and calculate the overlapping
entity ratio to determine the ﬁnal set of conﬂicts.

In general, our conﬂict detection technique works well with four requirement datasets. In similarity-based conﬂict detection,
BERT-TFIDF achieves better results for all the datasets and provides higher overall F1-score values. In semantic conﬂict detec-
tion, we manage to improve the eﬃcacy of Phase I in identifying the conﬂicts by 4% - 5% in terms of F1-score in certain datasets.
However, for some other datasets, we notice that F1-score remains the same or there is a drop compared to Phase I results.
Regardless, we consider Phase II as a technique to verify the embedding-based similarity with semantic-based similarity.

We concur that NLP domain is highly dynamic and new methods (e.g., embeddings) are developed at a fast pace. In this
regard, we aim to extend our analysis by using other transformer-based sentence embeddings. Similarly, transformer-based NER
models can be explored to improve the entity extraction performance. Furthermore, we plan to expand our work to identify the
duplicate requirements in the SRS documents, which require a more nuanced algorithm to distinguish conﬂicts from duplicates.

ACKNOWLEDGEMENTS

The authors would like to thank Raad Al-Husban for his valuable help in data preparation and conceptualization steps of this
work.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.70.30.320.680.350.400.450.500.550.600.65Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.040.260.740.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual0.950.0490.140.860.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual100.290.710.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.950.050.350.650.10.20.30.40.50.60.70.80.9Malik et al.

DATA AVAILABILITY STATEMENT

Full research data are not shared due to conﬁdentiality reasons.

References

15

1. Shah US, Jinwala DC. Resolving ambiguities in natural language software requirements: a comprehensive survey. ACM

SIGSOFT Software Engineering Notes 2015; 40(5): 1–7.

2. Osman MH, Zaharin MF. Ambiguous software requirement speciﬁcation detection: An automated approach. In: IEEE. ;

2018: 33–40.

3. Aldekhail M, Chikh A, Ziani D. Software requirements conﬂict identiﬁcation: review and recommendations. Int J Adv

Comput Sci Appl (IJACSA) 2016; 7(10): 326.

4. Egyed A, Grunbacher P. Identifying requirements conﬂicts and cooperation: How quality attributes and automated

traceability can help. IEEE software 2004; 21(6): 50–58.

5. Guo W, Zhang L, Lian X. Automatically detecting the conﬂicts between software requirements based on ﬁner semantic

analysis. arXiv preprint arXiv:2103.02255 2021.

6. Merugu R, Chinnam SR. Automated cloud service based quality requirement classiﬁcation for software requirement

speciﬁcation. Evolutionary Intelligence 2021; 14(2): 389–394.

7. Zhao L, Alhoshan W, Ferrari A, et al. Natural Language Processing for Requirements Engineering: A Systematic Mapping

Study. ACM Computing Surveys (CSUR) 2021; 54(3): 1–41.

8. Ezzini S, Abualhaija S, Arora C, Sabetzadeh M, Briand LC. Using domain-speciﬁc corpora for improved handling of

ambiguity in requirements. In: IEEE. ; 2021: 1485–1497.

9. Zhou Y, Tong Y, Gu R, Gall H. Combining text mining and data mining for bug report classiﬁcation. Journal of Software:

Evolution and Process 2016; 28(3): 150–176.

10. Aggarwal K, Timbers F, Rutgers T, Hindle A, Stroulia E, Greiner R. Detecting duplicate bug reports with software

engineering domain knowledge. Journal of Software: Evolution and Process 2017; 29(3): e1821.

11. Diamantopoulos T, Roth M, Symeonidis A, Klein E. Software requirements as an application domain for natural language

processing. Language Resources and Evaluation 2017; 51(2): 495–524.

12. Sabriye AOJ, Zainon WMNW. A framework for detecting ambiguity in software requirement speciﬁcation. In: IEEE. ;

2017: 209–213.

13. Handbook A. From Contract Drafting to Software Speciﬁcation: Linguistic Sources of Ambiguity. 2003.

14. Mairiza D, Zowghi D, Nurmuliani N. Managing conﬂicts among non-functional requirements. In: University of Technology,

Sydney. ; 2009.

15. Kim M, Park S, Sugumaran V, Yang H. Managing requirements conﬂicts in software product lines: A goal and scenario

based approach. Data & Knowledge Engineering 2007; 61(3): 417–432.

16. Butt WH, Amjad S, Azam F. Requirement conﬂicts resolution: using requirement ﬁltering and analysis. In: Springer. ; 2011:

383–397.

17. Moser T, Winkler D, Heindl M, Biﬄ S. Requirements management with semantic technology: An empirical study on

automated requirements categorization and conﬂict analysis. In: Springer. ; 2011: 3–17.

18. Viana T, Zisman A, Bandara AK. Identifying conﬂicting requirements in systems of systems. In: IEEE. ; 2017: 436–441.

16

Malik et al.

19. Whittle J, Sawyer P, Bencomo N, Cheng BH, Bruel JM. RELAX: a language to address uncertainty in self-adaptive systems

requirement. Requirements Engineering 2010; 15(2): 177–196.

20. Manning CD, Surdeanu M, Bauer J, Finkel JR, Bethard S, McClosky D. The Stanford CoreNLP natural language processing

toolkit. In: stanford. ; 2014: 55–60.

21. Ghosh S, Elenius D, Li W, Lincoln P, Shankar N, Steiner W. Automatically extracting requirements speciﬁcations from

natural language. arXiv preprint arXiv:1403.3142 2014.

22. Li W. Speciﬁcation mining: New formalisms, algorithms and applications. University of California, Berkeley . 2013.

23. Polpinij J, Ghose A. An automatic elaborate requirement speciﬁcation by using hierarchical text classiﬁcation. In: . 1. IEEE.

; 2008: 706–709.

24. Subha R, Palaniswami S. Ontology extraction and semantic ranking of unambiguous requirements. Life Science Journal

2013; 10(2): 131–8.

25. Sharma R, Bhatia J, Biswas K. Machine learning for constituency test of coordinating conjunctions in requirements

speciﬁcations. In: ACM. ; 2014: 25–31.

26. Sardinha A, Chitchyan R, Weston N, Greenwood P, Rashid A. EA-Analyzer: automating conﬂict detection in a large set of

textual aspect-oriented requirements. Automated Software Engineering 2013; 20(1): 111–135.

27. Oo KH, Nordin A, Ismail AR, Sulaiman S. An Analysis of Ambiguity Detection Techniques for Software Requirements

Speciﬁcation (SRS). International Journal of Engineering & Technology 2018; 7(2.29): 501–505.

28. Das S, Deb N, Cortesi A, Chaki N. Sentence embedding models for similarity detection of software requirements. SN

Computer Science 2021; 2(2): 1–11.

29. Cleland-Huang J, Vierhauser M, Bayley S. Dronology: An incubator for cyber-physical system research. arXiv preprint

arXiv:1804.02423 2018.

30. Mavin A, Wilkinson P, Harwood A, Novak M. Easy approach to requirements syntax (EARS). In: IEEE. ; 2009: 317–322.

31. Ferrari A, Spagnolo GO, Gnesi S. Pure: A dataset of public requirements documents. In: IEEE. ; 2017: 502–505.

32. Cer D, Yang Y, Kong Sy, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175 2018.

33. Aizawa A. An information-theoretic perspective of tf–idf measures. Information Processing & Management 2003; 39(1):

45–65.

34. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding.

arXiv preprint arXiv:1810.04805 2018.

35. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv

preprint arXiv:1802.03426 2018.

36. Corley CD, Mihalcea R. Measuring the semantic similarity of texts. In: ACL. ; 2005: 13–18.

37. Gao X, Wu S. Hierarchical clustering algorithm for binary data based on cosine similarity. In: IEEE. ; 2018: 1–6.

38. Rupp C, Simon M, Hocker F. Requirements engineering und management. HMD Praxis der Wirtschaftsinformatik 2009;

46(3): 94–103.

39. Khin NPP, Aung TN. Analyzing tagging accuracy of part-of-speech taggers. In: Springer. ; 2015: 347–354.

Malik et al.

6

APPENDIX

17

This section is structured into two parts. We ﬁrst present the detailed results for each dataset for similarity-based conﬂict detection
technique. Then, we present the normalized confusion matrices for each dataset for the semantic conﬂict detection technique.

ROC Curves for Threshold Detection

6.1
Figure 4,5,6, and 7 shows the ROC curves obtained from the 3-fold cross validation over all the requirement sets. These curves
facilitate the process of ﬁnding the cosine similarity threshold in Phase I.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 4 ROC curves for OpenCoss requirement set with BERT-TFIDF embedding across 3 folds

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 5 ROC curves for WorldVista requirement set with BERT-TFIDF embedding across 3 folds

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC18

Malik et al.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 6 ROC curves for PURE requirement set with BERT-TFIDF embedding across 3 folds

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 7 ROC curves for IBM-UAV requirement set with BERT-TFIDF embedding across 3 folds

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROCMalik et al.

19

Detailed Classiﬁcation Results for Phase II

6.2
Figure 8 shows the performance of the semantic conﬂict detection technique over OpenCoss dataset. We observe variability
in the results in each fold as higher number of FPs is contributing towards lower F1-scores. Figure 9 and 10 for WorldVista
and UAV datasets, respectively, shows consistent results over all the folds, and we are able to capture the requirement conﬂicts
with signiﬁcant accuracy. Figure 11 shows the dominance of the FN values in the PURE dataset in the ﬁrst fold, which distorts
the overall F1-score for the conﬂict class. Figure 12 for IBM-UAV dataset shows great performance in second and third fold,
however, ﬁrst fold shows an increase in FN which contributes to lower F1-scores for the conﬂict class.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 8 Normalized confusion matrices for OpenCoss dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 9 Normalized confusion matrices for WorldVista dataset in each fold.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.610.390.380.620.400.450.500.550.60Non-ConflictConflictPredictedNon-ConflictConflictActual0.940.0650.570.430.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.550.45010.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.350.650.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.070.250.750.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.070.150.850.10.20.30.40.50.60.70.80.920

Malik et al.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 10 Normalized confusion matrices for UAV dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 11 Normalized confusion matrices for PURE dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 12 Normalized confusion matrices for IBM-UAV dataset in each fold.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.0740.0910.910.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.0370.250.750.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.0370.0830.920.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual100.430.570.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.140.860.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.290.710.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.920.080.750.250.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.920.08010.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.30.70.00.20.40.60.81.0