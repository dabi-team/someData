Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

ORIGINAL ARTICLE

Identifying the requirement conï¬‚icts in SRS documents using
transformer-based sentence embeddings

Garima Malik1 | Mucahit Cevik*1 | Devang Parikh2 | Ayse Basar1

1Data Science Lab, Mechanical Industrial
Engineering Department, Toronto
Metropolitan University, Ontario, Canada
2IBM, North Carolina, USA

Correspondence
*Mucahit Cevik, Data Science Lab,
Mechanical Industrial Engineering
Department, Toronto Metropolitan
University, Toronto, Ontario, M5B 2K3,
Canada. Email: mcevik@ryerson.ca

2
2
0
2

n
u
J

8
2

]
E
S
.
s
c
[

1
v
0
9
6
3
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

High quality software systems typically require a set of clear, complete and compre-

hensive requirements. In the process of software development life cycle, a software
requirement speciï¬cation (SRS) document lays the foundation of product devel-

opment by deï¬ning the set of functional and nonfunctional requirements. It also

improves the quality of software products and ensure timely delivery of the projects.

These requirements are typically documented in natural language which might lead

to misinterpretations and conï¬‚icts between the requirements. In this study, we aim to

identify the conï¬‚icts in requirements by analyzing their semantic compositions and

contextual meanings. We propose an approach for automatic conï¬‚ict detection, which

consists of two phases: identifying conï¬‚ict candidates based on textual similarity, and

using semantic analysis to ï¬lter the conï¬‚icts. The similarity-based conï¬‚ict detection

strategy involves ï¬nding the appropriate candidate requirements with the help of sen-

tence embeddings and cosine similarity measures. Semantic conï¬‚ict detection is an

additional step applied over all the candidates identiï¬ed in the ï¬rst phase, where the

useful information is extracted in the form of entities to be used for determining the

overlapping portions of texts between the requirements. We test the generalizability

of our approach using ï¬ve SRS documents from diï¬€erent domains. Our experiments

show that the proposed conï¬‚ict detection strategy can capture the conï¬‚icts with high

accuracy, and help automate the entire conï¬‚ict detection process.

KEYWORDS:
Software Requirement Speciï¬cations, Conï¬‚ict Detection, Sentence Similarity, Sentence Embeddings,

Named Entity Recognition

1

INTRODUCTION

Requirement Engineering (RE) is the process of deï¬ning, documenting, and maintaining the software requirements 1. RE process
involves four main activities, namely, requirements elicitation, requirements speciï¬cation, requirements veriï¬cation and valida-
tion, and requirements management. In the requirement speciï¬cation process, the deliverable is termed as software requirement
speciï¬cation (SRS) document which is a highly important document in software development life cycle (SDLC) 2. SRS docu-
ments describe the functionality and expected performance for software products, naturally aï¬€ecting all the subsequent phases
in the process. The requirement set deï¬ned in SRS documents are analyzed and reï¬ned in the design phase, which results in
various design documents. Then, the developers proceed with these documents to build the code for the software system 3.

 
 
 
 
 
 
2

Malik et al.

SRS documents are mostly written in natural language to improve the comprehensibility of requirements. The success
of any software system is largely dependent on the clarity, transparency, and comprehensibility of software requirements 4.
Unclear, ambiguous, conï¬‚icting and incomprehensible software requirements might lead to increased project completion times,
ineï¬ƒciency in software systems, and increase in the project budget.

Two requirements are said to be conï¬‚icting if the implementation of the ï¬rst negatively impacts the second requirement 3,5.
Detection of conï¬‚icts in the earlier development phase is very important, however, the manual identiï¬cation of these conï¬‚icts
could be tedious and time-consuming. It is necessary to develop semi-automated or automated approaches for conï¬‚ict detection
in SRS documents. Considering the structure of software requirements, natural language processing (NLP) methods can help
in analyzing and understanding the software requirements semantically. We can use various information extraction techniques
such as named entity recognition (NER), and Parts of Speech (POS) tagging, alongside the semantic similarity of the natural
language text to interpret the context and syntactic nature of the software requirements.

In order to provide an automated approach for generalised conï¬‚ict identiï¬cation, we propose a two-stage framework which
elicits the conï¬‚ict criteria from the provided software requirements, and outputs the conï¬‚icting requirements. In the ï¬rst phase,
we convert the software requirements into high dimensional vectors using various sentence embeddings, and then identify the
conï¬‚ict candidates using cosine similarity. Then, in the second phase, candidate conï¬‚ict set is further reï¬ned by measuring the
overlapping entities in the requirement texts, with high level of overlaps pointing to a conï¬‚ict.

Research Goal
The main focus of this study is to create a NLP-based framework to automatically detect the conï¬‚icting requirements in SRS
documents. We assume that structure of requirements is based on natural language and it can have a variety of forms such as
modal verb style (include â€˜shallâ€™, â€˜willâ€™, â€˜shouldâ€™, and â€˜mustâ€™), user stories, functional, and non-functional requirements. We
leverage deep learning-based techniques to understand the semantics of the requirements. This helps designing a framework
based on text similarity and understanding with the objective of reducing the manual interventions and increasing the eï¬ƒciency
of the automated conï¬‚ict detection. In our analysis, we do not distinguish duplicates from the conï¬‚icts, as they are both undesired
in software development processes, and refer duplicates as conï¬‚icts.

Contribution
The main contributions of our study can be summarized as follows:

1. We propose an unsupervised learning-based conï¬‚ict detection algorithm to identify conï¬‚icts in a given set of requirements.
Previous studies typically converts the requirement texts into formal structures whereas our approach works with raw
requirement text (i.e., natural language text). We use BERT and USE sentence embeddings to capture the semantics
and contextual meaning of the software requirements which generates a set of candidate conï¬‚icts. Then, we apply an
additional ï¬lter over the candidate set that labels two requirements as conï¬‚icting if they have a high degree of overlap in
their software-speciï¬c entities.

2. We employ software-speciï¬c entity extraction models to extract the entities from the requirements with the help of manu-
ally annotated corpus of SRS documents and custom feature set. We show how a highly accurate entity recognition model
can be employed for an important practical problem by employing this model within a semantic conï¬‚ict detection method.

3. We conduct comprehensive numerical study using open source requirement sets associated with multiple disciplines
such as healthcare, automobile, aerospace, and transportation which contain annotated conï¬‚icts. Our analysis contributes
to a better understanding for the capabilities of NLP techniques for the conï¬‚ict detection task in software requirement
engineering.

Structure of the Paper
The remainder of the paper is organized as follows. Section 2 provides the background on the problem of conï¬‚ict identiï¬cation
in software requirement datasets. We also review relevant techniques for conï¬‚ict identiï¬cation in requirement engineering and
software development, particularly over SRS documents. Section 3 introduces our proposed method for automated conï¬‚ict
detection, and provides a detailed discussion over dataset characteristics, sentence embeddings, semantic similarity, and NER.
In Section 4, we present the results from our numerical study, which includes a performance comparison over diï¬€erent conï¬‚ict
identiï¬cation strategies for various requirement datasets. Lastly, Section 5 provides concluding remarks and future research
directions.

Malik et al.

2

BACKGROUND

3

Previous studies suggest the use of NLP-based techniques to solve various software requirement related problems such as
requirement classiï¬cation 6,7, ambiguity detection 8, bug report classiï¬cation 9, duplicate bug report prediction 10, conï¬‚ict iden-
tiï¬cation 5, and mapping of natural language-based requirements into formal structures 11. Conï¬‚ict detection is one of the most
diï¬ƒcult problems in requirement engineering 3. Inability in identifying the conï¬‚icts in software requirements might lead to
uncertainties and cost overrun in software development. Several papers discussed conï¬‚ict identiï¬cation in various domains,
however, an autonomous, reliable and generalizable approach for detecting conï¬‚icting requirements is yet to be achieved. Below,
we review the relevant literature for automated conï¬‚ict detection over SRS documents.

The terms â€˜Ambiguityâ€™ and â€˜Conï¬‚ictâ€™ can be misconstrued in the requirement engineering context. Researchers have provided
formal deï¬nitions for the requirement ambiguity as a requirement having more than one meaning, and provided various tech-
niques to detect the requirement ambiguities in SRS documents 1,12. On the other hand, requirement conï¬‚ict detection remains
as a challenging problem, lacking a well-accepted formal deï¬nition and structure. An ambiguity in SRS documents can be either
language-based (lexical, syntactic, semantic, pragmatic, vagueness, generality, and language error ambiguity) 13 or RE-speciï¬c
ambiguities (conceptual translational, requirements document, application domain, system domain, and development domain) 1.
Several studies deï¬ne the requirement conï¬‚ict depending upon the domain of requirements. However, the term â€˜conï¬‚ictâ€™ can be
deï¬ned more broadly as the presence of interference, interdependency, or inconsistency between requirements 14. Kim et al. 15
proposed the deï¬nition of requirement conï¬‚ict as interaction and dependencies present between requirements which results into
negative or undesired operation of the software systems.

Butt et al. 16 deï¬ned requirement conï¬‚icts based on the categorization of requirements to mandatory, essential, and optional
requirements. Kim et al. 15 described the requirement structure as Actor (Noun) + Action (verb) + Object (object) + Resource
(resource). An activity conï¬‚ict can arise when two requirements achieve the same actions through diï¬€erent object and a resource
conï¬‚ict may arise when diï¬€erent components try to share the same resources. Moser et al. 17 categorized the conï¬‚icts as simple
(if exists between two requirements) and complex (if exists between three or more requirements). Recently, Guo et al. 5 proposed
a comprehensive deï¬nition for semantic conï¬‚icts amongst diï¬€erent functional requirements. They stated that if two requirements
having inferential, interdependent, and inclusive relationship then it may lead to inconsistent behaviour in software system.

Shah and Jinwala 1 conducted a survey of natural language-based software requirements ambiguity resolution techniques.
They also provided insights into diï¬€erent web-based tools for ambiguity detection. They categorize ambiguity detection into
three groups, namely, manual glossaries, rule-based and automatic ontology-based approaches. Given that natural language
requirements are complex in their nature, such heuristic or rule-based parser-dependent methods are prone to errors if semantic
elements of the requirements are not extracted with signiï¬cant accuracy. In another study, Aldekhail et al. 3 reviewed various
papers on the conï¬‚ict detection in software requirements. They concluded that semi-automated methods require a signiï¬cant
manual process, while fully automated models are still based on human analysis of the requirement sets. That is, the models
aiming full automation are still diï¬ƒcult to generalise, resulting in large errors when applied to diï¬€erent requirement sets.

Viana et al. 18 presented a system that detected conï¬‚icting requirements in systems of systems. The paper simulated a smart
home system and focused on conï¬‚ict identiï¬cation in a nutrition system called Feed Me Feed Me. The process assume that
system requirements are written using RELAX framework, which are deï¬ned in fuzzy branching temporal logic 19. Such frame-
work assert that requirements have modal components (e.g., SHALL, SHOULD, and MAY), temporal (e.g. EVENTUALLY,
UNTIL, and AS CLOSE AS POSSIBLE TO), and ordinal (e.g., AS MANY, AS FEW AS POSSIBLE) operators. The conï¬‚ict
identiï¬cation system implemented a detection system that had three main steps: overlap detection, conï¬‚ict detection, and con-
ï¬‚ict resolution. With the help of RELAX framework, the algorithm was able to detect overlapping requirements whenever they
used or blocked the same resources, or when they over-used or under-used other resources of the smart home. However, as noted
in the study, the framework is entirely based on a pure simulation, and it is diï¬ƒcult to claim generalizability and scalability of
the proposed approach other datasets and domains.

Guo et al. 5 attempted a deeper dive into the semantics of software requirements using NLP methods coupled with heuristic
rules that helped identify requirement conï¬‚icts using a divide-and-conquer inspired design. Finer Semantic Analysis-based
Requirements Conï¬‚ict Detector (FSARC) attempted to automate this process through a seven step procedure that relies on
Standordâ€™s CoreNLP library 20. The ï¬rst step employs POS tagging and Stanfordâ€™s Dependency Parser (SDP) which convert
each requirement into novel eight tuple: (ð‘–ð‘‘, ð‘”ð‘Ÿð‘œð‘¢ð‘_ð‘–ð‘‘, ð‘’ð‘£ð‘’ð‘›ð‘¡, ð‘Žð‘”ð‘’ð‘›ð‘¡, ð‘œð‘ð‘’ð‘Ÿð‘Žð‘¡ð‘–ð‘œð‘›, ð‘–ð‘›ð‘ð‘¢ð‘¡, ð‘œð‘¢ð‘¡ð‘ð‘¢ð‘¡, ð‘Ÿð‘’ð‘ ð‘¡ð‘Ÿð‘–ð‘ð‘¡ð‘–ð‘œð‘›). This eight-tuple is
later passed through several rule-based routines which perform the task of identifying the conï¬‚icts. The proposed algorithm
performed well and showed promise to be generalizable. On the other hand, the approach relies on CoreNLP libraryâ€™s ability

4

Malik et al.

to correctly generate the eight-tuple, which is the main input of the algorithm. This also implies that the model would expect
a certain structure to be followed in writing the requirements that are analyzed. In a similar fashion, Ghosh et al. 21 proposed
a method that converts natural language requirements to formal software speciï¬cations by implementing the â€˜ARSENALâ€™, a
method that combine heuristic rules on extracted NLP components using Stanfordâ€™s Typed Dependency Parser library (STDP) 22.
Speciï¬cally, they broke down each requirement into ï¬ve components: Term Type, Negated Or Not, Quantiï¬er Type, Relations /
Attributions, and Lists. Once their parts were tagged in the natural language requirements, they were passed into heuristic rules
to output a formal requirement set. Sabriye and Zainon 12 also developed a simple rule-based algorithm that detect software
requirement ambiguities by using POS tagging.

Many other studies explored the use of supervised machine-learning techniques in identifying conï¬‚icting requirements, major-
ity of which oï¬€ering a semi-automated approach (i.e., requiring manual inspection of the identiï¬ed conï¬‚icting requirements).
Polpinij and Ghose 23 suggested a 4-step fuzzy decision process for conï¬‚ict detection. The ï¬rst step involved manual labeling of
training data which was fed into a fuzzy decision-tree classiï¬er. The second step trained this model, while the third one predicted
the unlabeled instances using the trained decision-tree classiï¬er. In the ï¬nal step, the most ambiguous instances were ï¬‚agged for
potential removal or treatment. A Support Vector Machines (SVM) was employed to detect patterns of conï¬‚icting requirements
with the aim of increasing separation between the two classes of positive and negative elements of the requirements.

Subha and Palaniswami 24 trained a naive Bayes classiï¬er for conï¬‚ict detection and extraction of software ontologies in
requirement engineering domain. It was based on word probability and word count method. The algorithm relied on the prob-
abilities of words coming from the text of the training data. Sharma et al. 25 modiï¬ed the same algorithm as they added a
pre-processing step where the training data was tokenized using bag of words (BoW) method which discarded the order of words
in a text while keeping track of the frequency of each word. This was followed by the removal of stop words that did not add
contextual meaning to sentences. As a ï¬nal pre-processing step prior to feeding the training data into the naive Bayes Classiï¬er,
POS tags were also added to the data. Sardinha et al. 26 developed a tool called EA-Analyser for aspect-oriented requirements.
They applied Bayesian learning method to identify the conï¬‚icting dependencies in the requirements. Oo et al. 27 implemented
various machine-learning models and found that naive Bayes classiï¬er produced more consistent results in detecting conï¬‚icts
compared to SVM and decision-tree classiï¬ers. Osman and Zaharin 2, experimented with similar models on Malay software
requirement datasets, and concluded that the naive Bayes classiï¬er generated best results.

Our work diï¬€ers from these existing studies in multiple ways. First, we propose a two phase generalised approach for auto-
mated conï¬‚ict detection using unsupervised learning. To this end, Das et al. 28 introduced the idea of sentence embeddings
for similarity detection in software requirements. We extend this idea and combine the two sentence embeddings (BERT and
TFIDF) to calculate the cosine similarity between the requirements. Second, our approach directly works with software require-
ments as opposed to Guo et al. 5, which converts the software requirements into formal representations and apply rule-based
procedures to detect the conï¬‚icts. Third, diï¬€erent from ï¬ner semantic analysis enabled by Guo et al. 5â€™s rule-based approach, we
deï¬ne the set of software-speciï¬c entities, and train machine learning models with handcrafted features dedicated to software
requirements. These entities provide an additional way of verifying the conï¬‚icts semantically.

3 METHODOLOGY

In this section, we ï¬rst describe the SRS datasets used in our numerical study, which is followed by a brief discussion on sentence-
embeddings used in the experiments. Then, we provide speciï¬c details of the building blocks of our conï¬‚ict detection algorithm
and the experimental setup. Figure 1 shows our proposed framework for automated conï¬‚ict detection in SRS documents. Our
technique comprises two phases, which are similarity-based conï¬‚ict detection (Phase I) and semantic conï¬‚ict detection (Phase
II).

Datasets

3.1
We consider ï¬ve SRS datasets that belong to various domains such as software, healthcare, transportation, and hardware. Three
of these are open-source SRS datasets (OpenCoss, WorldVista, and UAV), and the other two are extracted from public SRS
documents. Generally, requirements are documented in a structured format and we retain the original structure of requirements
for conï¬‚ict detection process. To maintain the consistency in requirement structure, we converted complex requirements (para-
graphs or compound sentences) into a simple sentences. Originally, these datasets included very few conï¬‚icts. As such, we

Malik et al.

5

FIGURE 1 Proposed framework for identifying the conï¬‚icting requirements in SRS documents.

added synthetic conï¬‚icts in each SRS dataset to augment the data for our analysis. Table 1 provides summary information on
the SRS datasets.

TABLE 1 Dataset characteristics

Dataset

Domain

# of requirements

# of original conï¬‚icts

# of synthetic conï¬‚icts

Transportation

OpenCoss
WorldVista Medical
UAV
PURE
IBM-UAV

Aerospace
Thermodynamics
Hardware

115
140
116
125
105

6
5
4
0
2

5
23
16
21
13

Table 2 shows sample data instances, demonstrating the general structure and format of the requirements used in the conï¬‚ict
detection process. Each requirement dataset consists of requirement id, requirement text, and â€˜Conï¬‚ictâ€™ column indicating the
presence of conï¬‚ict in â€˜Yesâ€™ or â€˜Noâ€™ format. We also included the â€˜Conï¬‚ict-Labelâ€™ column to indicate the pair of conï¬‚icts. For
instance, requirements 1 and 2 are conï¬‚icting due to the diï¬€erent instruction for charging the same â€˜UAVâ€™.

Below, we brieï¬‚y describe all the SRS datasets used in the numerical study.

â€¢ OpenCoss: OPENCOSS1 refers to Open Platform for Evolutionary Certiï¬cation Of Safety-critical Systems for the rail-
way, avionics, and automotive markets. This is a challenging dataset to identify the conï¬‚icts as the samples from the

1http://www.opencoss-project.eu


Requirements Sentence embeddingsGenerate  cosine similarity matrixGenerate ROC       curveR1 to RnCandidate  conflict setIdentify similar requirement set for each conflict candidateNamed entity recognition (NER)NLTK tagger (Noun and Verb)Software tagger (Actor and Action)Compute the overlapping tags ratioConflict  requirementsNon-conflict  requirementsRatio > T0  Ratio < T0  Phase I: Similarity-based Conflict DetectionPhase II: Semantic Conflict DetectionDetermine cosine similarity cutoff6

Malik et al.

TABLE 2 Sample data instances from UAV dataset with requirement id, text, and conï¬‚ict label (Yes/No). Requirements 1 and
2 are conï¬‚icting.

Req. Id Requirement text

Conï¬‚ict Conï¬‚ict-Label

1.
2.
3.
4.

The UAV shall charge to 50 % in less than 3 hours.
Yes
Yes
The UAV shall fully charge in less than 3 hours.
The Aviary shall provide a remote surveillance from the air of a location within 32.19 Km of the origin. No
No
Remote surveillance shall include video streaming for manual navigation of the surveillance platform.

Yes (2)
Yes (1)
No
No

OpenCoss dataset indicates a lot of similar or duplicate requirements with repeating words. Initially, this set included 110
requirements and we added 5 more synthetic conï¬‚icts.

â€¢ WorldVista: WorldVista2 is a health management system that records patient information starting from the hospital
admission to discharge procedures. The requirement structure is basic, and written in natural language with health care
terminologies. It originally consisted of 117 requirements and we added 23 synthetic conï¬‚icts.

â€¢ UAV: The UAV (Unmanned Aerial Vehicle) 5,29 dataset is created by the University of Notre Dame and it includes all the
functional requirements which deï¬ne the functions of the UAV control system. The requirement syntax is based on the
template of EARS (Easy Approach to Requirements Syntax) 30. Originally, this dataset had 99 requirements and we added
16 conï¬‚icting requirements to the set, which resulted in a conï¬‚ict proportion of 30%.

â€¢ PURE: PURE (Public Requirements dataset), contains 79 publicly available SRS documents collected from the web 31.
We manually extracted set of requirements from two SRS documents, namely, THEMAS (Thermodynamic System) and
Mashbot (web interface for managing a companyâ€™s presence on social networks). In total, we collected 83 requirements
and induced synthetic 21 conï¬‚icts to maintain consistency with the other datasets.

â€¢ IBM-UAV: This dataset is proprietary, and provided by IBM. It consists of software requirements used in various projects
related to the aerospace and automobile industry. We sampled 75 requirements from the original set, and introduced 13
synthetic conï¬‚icts. The requirement text follows a certain format speciï¬ed by IBMâ€™s RQA (Requirement Quality Analysis)
system.

Embeddings and Cosine Similarity

3.2
Word embeddings can be used to convert the text into vectors of real numbers to prepare the data for any kind of modeling. For
the conï¬‚ict detection problem, we applied sentence embeddings to encode the meaning or context of software requirements.
Sentence embeddings use the weighted average of the embeddings of each word present in the sentence, and apply dimension-
ality reduction techniques to extract the embeddings 32,28. For the requirement datasets, we considered TFIDF as the baseline
embedding. We also experimented with deep learning-based USE embedding and transformer-based BERT embeddings. Below,
we brieï¬‚y summarize diï¬€erent embeddings used in our analysis.

â€¢ TFIDF: TFIDF stands for Term Frequency Inverse Document Frequency, a commonly-used method in information
retrieval tasks 33. It is a numerical measure to determine the importance of words based on their frequency. TFIDF creates
the vectors by weighing the terms according to their prevalence across the documents. The TFIDF value for each word
â€˜ð‘–â€™ in the document can be expressed as the multiplication of TF
values. TF denotes the number of times term
â€˜ð‘–â€™ appears in the document â€˜ð‘‘â€™, and IDF denotes the inverse of document frequency which measures the signiï¬cance of
term â€˜ð‘–â€™.

and IDF

ð‘–,ð‘‘

ð‘–

â€¢ USE: USE stands for Universal Sentence Encoder 32. It encodes natural language text into high-dimensional vectors. These
vectors can be used in text classiï¬cation, document clustering, semantic similarity detection, and other natural language-
speciï¬c tasks. The pre-trained USE consists of two variations, one trained with transformer encoder and the other trained
with Deep Averaging Network (DAN). USE encoder is trained and optimized for inputs such as sentences, phrases, and

2http://coest.org/datasets

Malik et al.

7

short paragraphs, and they are suitable for applying sentence level embeddings over the requirements. It is also trained on a
diverse set of transfer learning tasks such as sentence-based classiï¬cation, multilingual tasks, and ï¬ne-grained questions-
answer retrieval. We applied the DAN version of the USE embeddings for the software requirements, and generated the
512-dimensional vector for each input requirement.

â€¢ BERT-TFIDF: BERT stands for Bidirectional Encoder Representation from Transformers 34. In this embedding, we
combine the vector obtained from BERT text embedding with TFIDF embedding. The former harnesses state-of-the-art
technology that can vectorize and detect context and semantics, while the latter is a probabilistic method that gives more
importance to the words that occur less often in a given document. The merging process involve two steps. In the ï¬rst
step, we simply concatenate the embedding vectors, and in the second step we perform dimensionality reduction. This
step help reduce the initial resulting dimension of the embedded vectors. BERT embeddings are comprised of 768 features
while TFIDF embeddings can span into thousands of features (depending on the data) hence the need for dimensionality
reduction. To maintain the uniform vector size, we employ Uniform Manifold Approximation and Projection (UMAP) 35,
which is an eï¬€ective dimensionality reduction tool.

Cosine similarity is a distance measure that can be used to calculate the similarity between two words, sentences, paragraphs,
or the whole document 36. It is an eï¬€ective measure to estimate the similarity of vectors in high-dimensional space 37. This metric
models language-based input text as a vector of real-valued terms and the similarity between two texts is derived from the cosine
angle between two texts term vectors as follows:

cos(ð«ðŸ, ð«ðŸ) =

ð«ðŸð«ðŸ
â€–ð«ðŸâ€–â€–ð«ðŸâ€–

=

âˆšâˆ‘ð‘›

âˆ‘ð‘›

ð‘–=1 ð«ðŸð‘–ð«ðŸð‘–
âˆšâˆ‘ð‘›

ð‘–=1 (ð«ðŸð‘–)2

ð‘–=1 (ð«ðŸð‘–)2

(1)

The values for cosine similarity ranges from -1 to 1 where -1 signiï¬es dissimilarity and 1 signiï¬es similarity. To better demon-
strate how cosine similarity can be used over embedding vectors, we provide an illustrative example with three sample software
requirements, ð‘Ÿ1

, which are deï¬ned as follows:

, and ð‘Ÿ3

, ð‘Ÿ2

â€¢ ð‘Ÿ1

= â€˜The OPENCOSS platform shall be able to export evidence traceability links of an assurance project to external

tools.â€™

â€¢ ð‘Ÿ2

= â€˜The OPENCOSS platform must be able to send out evidence traceability links of an assurance project to external

tools and internal tools.â€™

â€¢ ð‘Ÿ3

= â€˜The OPENCOSS platform shall provide users with the ability to specify evidence traceability links in traceability

matrices.â€™

We calculate the cosine similarity between these requirement vectors when embedded with TFIDF, BERT-TFIDF, and USE.
Table 3 shows the cosine similarity values between these requirements. BERT-TFIDF is able to capture the frequency based
with a cosine similarity value of 0.88. USE being the second highest with
features and semantic similarity between ð‘Ÿ1
the value of 0.81. Requirement text for ð‘Ÿ3
is not similar to those of the other two requirements, and all the sentence embeddings
indicate low values of cosine similarity with ð‘Ÿ3

and ð‘Ÿ2

.

TABLE 3 Cosine similarity between ð‘Ÿ1

, ð‘Ÿ2

, and ð‘Ÿ3

with diï¬€erent sentence embeddings.

TFIDF BERT-TFIDF USE

cos(ð‘Ÿ1
cos(ð‘Ÿ1
cos(ð‘Ÿ2

, ð‘Ÿ2
, ð‘Ÿ3
, ð‘Ÿ3

)
)
)

0.44
0.16
0.09

0.88
0.50
0.47

0.81
0.55
0.40

8

Malik et al.

Phase I: Similarity-based Conï¬‚ict Detection

3.3
Algorithm 1 summarize the process provided in the left panel of Figure 1. We employ a similarity-based conï¬‚ict detection
approach to automatically identify the conï¬‚icts in a given requirement set. We also use the resulting set of conï¬‚icts as a candidate
conï¬‚ict set for Phase II of our framework. We ï¬rst create the sentence embedding vector for each requirement ð‘Ÿ âˆˆ îˆ¾ using
SentenceEmbedding(îˆ¾) procedure. It basically converts the requirements into numerical vector using one of the embeddings
deï¬ned in Section 3.2.

We next calculate the pairwise distance matrix (Î”), which measures the cosine similarity value between each pair of require-
ments. This matrix can act as a look up table to extract the most similar requirement and corresponding cosine similarity value
for a given requirement. Then, we use ROC curve (receiver operating characteristic curve) to identify the cosine similarity
threshold (ð›¿), which speciï¬es the minimum similarity value after which requirements are labeled as conï¬‚icting. The cutoï¬€ value
(ð›¿) is selected as the value that maximizes {TPR(Î”, ð‘˜) âˆ’ (1 âˆ’ FPR(Î”, ð‘˜))} over threshold values ð‘˜ âˆˆ {0.01, â€¦ , 1.00} and the
distance matrix Î”. This way, we balance the false positives and true positives rates, conï¬‚ict having the positive label. Lastly,
we assign labels of conï¬‚ict or no-conï¬‚ict to the requirements using ð›¿ as threshold value. The candidate conï¬‚ict set (îˆ¯) contains
all the requirements with conï¬‚ict label. Note that conï¬‚ict property is symmetric, i.e., if ð‘Ÿ1
is also
conï¬‚icting with ð‘Ÿ1

is conï¬‚icting with ð‘Ÿ2

, and, ð‘Ÿ1, ð‘Ÿ2 âˆˆ îˆ¯.

, then ð‘Ÿ2

Algorithm 1 Similarity-based Conï¬‚ict Detection

Input:

Requirement set: îˆ¾ = {ð‘Ÿ1, ð‘Ÿ2, â€¦ , ð‘Ÿð‘›}

Output:

Candidate conï¬‚ict set: îˆ¯
âƒ—îˆ¾ â† SentenceEmbedding(îˆ¾)
Î” â† PairwiseDistance( âƒ—îˆ¾)
ð›¿ â† arg maxð‘˜âˆˆ{0.01,â€¦,1.00}
îˆ¯ â† AssignLabels( âƒ—îˆ¾,ð›¿)

{TPR(Î”, ð‘˜) âˆ’ (1 âˆ’ FPR(Î”, ð‘˜))

}

// Generate requirement vectors
// Calculate the cosine similarity matrix
// Determine the cosine similarity cutoï¬€
// Label the requirements

Phase II: Semantic Conï¬‚ict Detection

3.4
Algorithm 2 describes the process of semantic conï¬‚ict detection as presented in right panel of Figure 1. This algorithm serves
as a second ï¬lter on the candidate conï¬‚icts generated in Phase I. Speciï¬cally, any candidate conï¬‚ict ð‘ âˆˆ îˆ¯ is semantically
compared against top ð‘š most similar requirements from îˆ¾. That is, by focusing on only ð‘š most similar requirements, we reduce
the computational burden, and also make use of the cosine similarity between the requirements. This semantic comparison is
performed based on overlap ratio between the entities present in the requirements. For a given candidate conï¬‚ict ð‘ âˆˆ îˆ¯, overlap
ratio is calculated as

{

maxð‘Ÿâˆˆîˆ¸

ð‘

Overlap(ð‘, ð‘Ÿ)

}

UniqueEntities(ð‘)

ð‘£ð‘ =

(2)

ð‘

where îˆ¸
represents the set of ð‘š most similar requirements to candidate conï¬‚ict ð‘. The function Overlap(ð‘, ð‘Ÿ) calculates the
number of overlapping entities between ð‘ and ð‘Ÿ, and function UniqueEntities(ð‘) calculates the number of unique entities in
candidate conï¬‚ict (i.e., a requirement text) ð‘. The calculated overlap ratio ð‘£ð‘
for ð‘ âˆˆ îˆ¯ is then compared against a pre-determined
. In our analysis, we set ð‘š = 5 and ð‘‡ð‘œ = 1, which
â‰¥ ð‘‡ð‘œ
overlap threshold value, ð‘‡ð‘œ
are determined based on preliminary experiments. In Algorithm 2, GetSimilarRequirements(ð‘, îˆ¾, ð‘š) returns the list îˆ¸ of
ð‘š most similar requirements from îˆ¾ for candidate conï¬‚ict ð‘, and GetMaxOverlapRatio(ð‘, îˆ¸) returns the maximum value for
the overlaps between requirements from set îˆ¸ and candidate conï¬‚ict ð‘.

, and ð‘ is added to ï¬nal conï¬‚ict set Ì„îˆ¯ if ð‘£ð‘

We also provide sample calculations in Table 4 to better illustrate the process in Algorithm 2. We show the overlapping
software-speciï¬c entities present in candidate requirement (ð‘) and similar requirement (ð‘Ÿ âˆˆ îˆ¸) with diï¬€erent color codes. For
instance, entity â€˜UAVâ€™ is represented by blue color. Requirements ð‘Ÿ1
both return high overlap ratios, indicating that they
are conï¬‚icting with ð‘.

and ð‘Ÿ2

Malik et al.

Algorithm 2 Semantic Conï¬‚ict Detection

Input:

Candidate conï¬‚ict set: îˆ¯ = {ð‘1, ð‘2, â€¦ , ð‘ð‘¡}
Requirement set: îˆ¾ = {ð‘Ÿ1, ð‘Ÿ2, â€¦ , ð‘Ÿð‘›}
# of similar requirements: ð‘š

Initialization:
Ì„îˆ¯ = âˆ…
ð‘‡ð‘œ = 1
ð‘š = 5
For ð‘ âˆˆ îˆ¯ do:

îˆ¸ â† GetSimilarRequirements(ð‘, îˆ¾, ð‘š)
ð‘£ â† GetMaxOverlapRatio(ð‘, îˆ¸)
If ð‘£ â‰¥ ð‘‡ð‘œ

:
Ì„îˆ¯ â† Ì„îˆ¯ âˆª {ð‘}

return Ì„îˆ¯

9

// Initialise conï¬‚ict set to an empty set
// Set overlapping threshold as 1
// Set number of similar requirements as 5

// Get the ð‘š similar requirements
// Calculate the maximum overlap ratio using Equation (2)
// Compare with threshold
// Augment the ï¬nal conï¬‚ict set

TABLE 4 Sample candidate (ð‘) and set of similar requirements (îˆ¸) to calculate the overlapping ratio (ð‘£). The maximum count
of overlap is 7 which resulted in overlap ratio value ð‘£ as 1.

Candidate Requirement

Similar Requirements

Overlap(ð‘, ð‘Ÿ) Ratio (ð‘£)

= â€™The UAV ï¬‚ight range shall

ð‘Ÿ1
be no less than 20 miles.â€™

= â€˜The UAV ï¬‚ight range shall

ð‘Ÿ2
be a minimum of 20 miles.â€™

ð‘ = â€˜The UAV ï¬‚ight range shall
be no less than 20 kilometers.â€™

= â€˜The UAV shall be able to autonomously a

ð‘Ÿ3
ï¬‚ight plan consisting of a set of waypoints
within its range and ï¬‚ight capabilities.â€™

= â€˜The UAV shall be able to autonomously plan

ð‘Ÿ4
a ï¬‚ight consisting of a set of waypoints
within its range and ï¬‚ight capabilities.â€™

= â€˜The Pilot controller shall be able to download

ð‘Ÿ5
a ï¬‚ight plan from the Hummingbird consisting
of a set of waypoints within the ï¬‚ight range of the
Hummingbird.â€™

7

5

2

2

2

1.00

0.71

0.28

0.28

0.28

Named Entity Recognition

3.5
We employ NER to verify the conï¬‚ict candidates semantically and then relabeling it as conï¬‚ict or non-conï¬‚ict. The purpose of
this step is to extract the relevant entities from requirements which can contribute in identifying the conï¬‚icting requirements. For
Phase II, we employ two NER approaches, namely, general NER and software-speciï¬c NER, and use Machine Learning-based
Conditional Random Fields (ML-CRF) algorithm for NER model training. We brieï¬‚y describe the details of these methods
below.

â€¢ General NER: Rupp et al. 38 suggest that a software requirement should follow the structure as Actor (Noun) + Object
+ Action (Verb) + Resource. The generic NER method extracts â€˜Nounâ€™ and â€˜Verbâ€™ tags from the requirements based on
this structure. We employed POS tagger provided in NLTK (Natural Language Toolkit) library in Python. NLTK unigram
tagger works best with all the standard corpora such as Brown, Penn Treebank and CoNLL2000 39.

â€¢ Software-speciï¬c NER: To create a software-speciï¬c NER model, we sampled some requirements from each of the
software requirement datasets described in Section 3.1. Software-speciï¬c entities can provide more useful information

10

Malik et al.

about the software requirements and help in the semantic conï¬‚ict detection process. We annotated a total of 378 software
requirements to train the ML-CRF model. That is, as we have small number of annotated requirements, we chose to use
a probabilistic ML model to perform the software-speciï¬c entity extraction. There are four main software entities (Actor,
Action, Object, and Property) which are typically used in literature to convert the software requirements into formal
representations 11. We included two additional relevant entities (Metric and Operator) speciï¬c to our IBM-UAV dataset
which account for numerical values (units, digits, and operators) in the requirements. The software-speciï¬c entities are
described as follows:

â€“ Actor: Actor refers to the actors or nouns present in a requirement. It also includes intended users and external

systems interacting with the software, e.g., UAV, Aviary, and Themas System.

â€“ Action: Action can be interpreted as the main verb in a requirement or an operation performed by an actor on some

object 11.

â€“ Object: Object involves any input/output and resource of the system that is mentioned in the software requirement 11.
â€“ Property: Property includes all adjectives mentioned for objects/actions that explains the properties of the objec-
t/action involved. Consider this requirement â€˜The aviary shall support autonomous mission plans as well as
pilot-directed ï¬‚ightâ€™. Here, â€˜autonomous mission plansâ€™ and â€˜pilot-directed ï¬‚ightâ€™ represents the property tag.

â€“ Metric: Standard unit measures such as â€˜ð‘˜ð‘šâ€™, â€˜ð‘š2â€™, and â€˜ð‘šâˆ•ð‘ 2â€™.
â€“ Operator: Relational operators such as â€˜>=â€™, â€˜<=â€™, â€˜less thanâ€™, and â€˜greater thanâ€™.

Table 5 shows POS tags and software-speciï¬c tags obtained for sample requirements from UAV dataset.

â€¢ Machine Learning-based Conditional Random Fields: This method provides a probabilistic approach for NER and it
is also known as discriminative classiï¬er. It models the conditional probability of assigning appropriate tag sequence Ì‚ð‘¦
with the help of input text ð‘¥ as Ì‚ð‘¦ = arg maxð‘¦ ð‘ƒ (ð‘¦|ð‘¥). In our NER task, the input data is sequential, and we have to take
the context into account while predicting the tags for a particular word. We use customised feature set as an additional
input to the model to incorporate this behaviour. In software-speciï¬c entity extraction, we create a comprehensive list of
features which includes lexical (input tokens), contextual (tokens in the locality of input text with window size of [âˆ’1, 1]),
character-level (digits and alphabets), and orthographic (capitalization and abbreviations) features.

TABLE 5 Sample NER results for the UAV dataset

NER method

Requirements

Description

General NER

The aviary shall support autonomous mission
plans as well as pilot-directed ï¬‚ight.
The UAV shall fully charge in less than 3 hours. Noun - UAV, hours

Noun - mission, plans, ï¬‚ight
Verb - support

Verb - charge

The aviary shall support autonomous mission
plans as well as pilot-directed ï¬‚ight.

Software-speciï¬c NER

The UAV shall fully charge in less than 3 hours.

Actor - aviary
Action - support
Property - autonomous mission plans,
pilot-directed ï¬‚ight

Actor - UAV
Action - charge
Operator - less than
Metric - 3 hours

Malik et al.

11

Experimental Setup

3.6
In the numerical analysis, we consider various sentence embeddings from simple transformers library3. That is, we use
general purpose pretrained models which have been evaluated on various NLP tasks. Speciï¬cally, we conduct experiments
using several BERT variants by instantiating pre-trained transformer models (i.e., distill-base-multilingual, all-MiniLM-L12-
V25, distillRoberta-v1, and mpnet-base-v24). Our preliminary experiments suggest diï¬€erent sentence embeddings for each
requirement datasets. In BERT-TFIDF embeddings, we use all-distillRoberta-v1 for Opencoss, UAV and PURE, Distiluse-base-
multilingual-cased-v1 for WorldVista and all-mpnet-base-v2 for IBM-UAV datasets.

In Phase I of our conï¬‚ict identiï¬cation process, the main objective is to extract all the possible requirements which can be
analyzed semantically to determine the ï¬nal set of conï¬‚icts in the SRS documents. We perform 3-fold cross validation over all the
requirement datasets. That is, considering the distribution of conï¬‚icting requirements and the limited number of requirements,
we divide each dataset into 3 diï¬€erent folds. Each fold includes some conï¬‚icting and non-conï¬‚icting requirements, however,
we make sure that each conï¬‚ict present in the fold should have its conï¬‚ict pair present in the same fold. For our techniques, we
use training set to determine the cosine similarity cut-oï¬€ value, and apply this value on the corresponding test set. We employ
standard classiï¬cation metrics such as macro averaged version of F1-score, precision and recall in performance evaluation.

In Phase II, we train a software-speciï¬c entity extraction model (ML-CRF). We use grid-search method to ï¬nd the best
parameters for ML-CRF model. The main hyperparameters used in the ï¬nal ML-CRF model are â€˜lbfgsâ€™ for the algorithm, 0.1
for c1 and c2 (i.e., regularization parameters), and 100 for max_iterations. To evaluate the performance of the ML-CRF model,
we employ 5-fold cross validation, and consider standard NER metrics such as accuracy, precision, recall and F1-score.

4

RESULTS

In this section, we ï¬rst assess the performance of the similarity-based conï¬‚ict detection algorithm (i.e., Phase I), and present the
results from our comparative analysis with various sentence embeddings for each of the requirement datasets. Next, we report
the results for entity extraction methods that are used to understand the semantics of the requirements. Lastly, we discuss the
performance of semantic conï¬‚ict detection algorithm for all the requirement datasets.

Similarity-based Conï¬‚ict Detection Performance

4.1
The ï¬rst step that comes after calculating the similarity matrix between the requirements is the generation of ROC curves, which
are used to obtain the similarity cut-oï¬€ for conï¬‚ict detection based on TPR and FPR values. Figure 2 shows the ROC curves for
the UAV dataset with BERT-TFIDF embedding in each fold of the 3-fold cross validation. We prepare similar ROC curves for
all the other requirement sets which are provided in the appendix (see Section 6).

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 2 ROC curves for UAV requirement set with BERT-TFIDF embedding across 3 folds

3https://www.sbert.net/docs/pretrained_models.html
5https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
4https://huggingface.co/sentence-transformers/all-mpnet-base-v2

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC12

Malik et al.

Table 6 shows the summary results of similarity-based conï¬‚ict detection approach with diï¬€erent sentence embeddings. We
report the average values obtained from 3-fold cross validation for cosine similarity cutoï¬€ and standard classiï¬cation metrics
along with conï¬‚ict class support values. In terms of the embeddings, BERT-TFIDF provides the best performance for all the
datasets except for IBM-UAV dataset where USE performs slightly better than BERT-TFIDF. As expected, TFIDF embedding
performs worse than all the other embedding because it only considers the frequency based features to create the requirement
vectors. We also observe that USE generates high cutoï¬€ value for each dataset in comparison to other sentence embeddings.

TABLE 6 Evaluation results for the similarity-based conï¬‚ict detection approach on all the requirement datasets. Highlighted text
represents the best sentence embedding for each dataset with its respective performance values. Reported values are averaged
over 3 folds and results are shown as â€œmean Â± standard deviationâ€.

Dataset

Embeddings

Cosine cutoï¬€

F1-score

Recall

Precision

Support

OpenCoss

0.67
TFIDF
USE
0.91
BERT-TFIDF 0.79

WorldVista TFIDF

0.24
USE
0.66
BERT-TFIDF 0.42

UAV

PURE

IBM-UAV

TFIDF

0.44
0.79
USE
BERT-TFIDF 0.64

0.49
TFIDF
0.88
USE
BERT-TFIDF 0.70

TFIDF

0.59
0.89
USE
BERT-TFIDF 0.76

0.43 Â± 0.04
0.41 Â± 0.05
0.43 Â± 0.04
0.85 Â± 0.03
0.82 Â± 0.01
0.86 Â± 0.05
0.84 Â± 0.10
0.90 Â± 0.03
0.90 Â± 0.04
0.79 Â± 0.08
0.86 Â± 0.11
0.88 Â± 0.11
0.60 Â± 0.19
0.68 Â± 0.24
0.64 Â± 0.22

0.68 Â± 0.23
0.68 Â± 0.18
0.68 Â± 0.23
0.86 Â± 0.04
0.83 Â± 0.08
0.86 Â± 0.04
0.85 Â± 0.11
0.97 Â± 0.04
0.91 Â± 0.06
0.71 Â± 0.11
0.80 Â± 0.17
0.80 Â± 0.17
0.55 Â± 0.22
0.65 Â± 0.28
0.65 Â± 0.28

0.34 Â± 0.06
0.30 Â± 0.02
0.34 Â± 0.06
0.84 Â± 0.06
0.84 Â± 0.11
0.86 Â± 0.12
0.83 Â± 0.11
0.85 Â± 0.05
0.89 Â± 0.04
0.92 Â± 0.10
0.95 Â± 0.06
1.00 Â± 0.00
0.72 Â± 0.20
0.73 Â± 0.16
0.73 Â± 0.12

7.33
7.33
7.33

18.66
18.66
18.66

11.66
11.66
11.66

14.00
14.00
14.00

9.33
9.33
9.33

In general, we ï¬nd that the performance of the similarity-based conï¬‚ict detection approach varies with the datasets. With
BERT-TFIDF embedding, we achieve F1-score of 90%, 88%, and 86% for UAV, PURE and WorldVista datasets, respectively.
We also obtain better recall values for these datasets as 91%, 80%, and 86%, respectively, which shows the capability of our
approach in identifying the conï¬‚icting requirements. For OpenCoss dataset, our approach shows relatively poor performance
with an F1-score of 43% and recall value of 68%. In this dataset, the structure/text of the requirements are very similar to each
other, and algorithm labels them as conï¬‚ict based on the higher value of cosine similarity. For IBM-UAV dataset, we note that
USE and BERT-TFIDF embeddings provides F1-score of 68% and 64%, respectively. Higher standard deviation values for this
dataset can be explained by the smaller support for conï¬‚ict class.

Semantic Conï¬‚ict Detection Results

4.2
In Phase II of the conï¬‚ict detection approach, we validate the presence of conï¬‚icts semantically by extracting the useful entities
from the requirements. It is also an additional step over the similarity-based conï¬‚ict detection approach. Table 7 shows the entity-
speciï¬c performance of the software-speciï¬c NER model with â€˜Actorâ€™ being the best performing entity with an F1-score of
85%. Other entities (Metric, Object, Operator, and Property) have lower support values in the test set which results into average
performance. Because we directly use the NLTK package for general NER, we do not assess the entity extraction performance
(e.g., to accurately identify nouns or verbs) for this method.

We choose the best performing embedding from Phase I to apply the semantic detection approach, and implement the software-
speciï¬c NER and general NER techniques. Table 8 shows the comparative results for both NER models for the candidate conï¬‚ict
sets obtained from Phase I. For WorldVista, PURE, and IBM-UAV dataset, software-speciï¬c NER model works better than

Malik et al.

13

TABLE 7 Entity speciï¬c performance of software-speciï¬c NER model (ML-CRF) used in semantic conï¬‚ict detection approach.
Reported values are averaged over 5 folds and results are shown as â€œmean Â± standard deviationâ€.

Precision

Recall

F1-score

Support

Action
Actor
Metric
Object
Operator
Property

Micro avg
Macro avg
Weighted avg

0.80 Â± 0.02
0.87 Â± 0.04
0.68 Â± 0.20
0.76 Â± 0.09
0.80 Â± 0.18
0.69 Â± 0.04
0.77 Â± 0.04
0.77 Â± 0.04
0.77 Â± 0.03

0.75 Â± 0.03
0.83 Â± 0.03
0.70 Â± 0.11
0.63 Â± 0.09
0.58 Â± 0.28
0.63 Â± 0.09
0.71 Â± 0.03
0.69 Â± 0.06
0.71 Â± 0.03

0.78 Â± 0.02
0.85 Â± 0.03
0.68 Â± 0.14
0.68 Â± 0.08
0.62 Â± 0.17
0.65 Â± 0.03
0.74 Â± 0.03
0.71 Â± 0.05
0.74 Â± 0.03

91.0
86.0
7.6
38.6
6.0
130.0

359.2
359.2
359.2

general NER with the F1-score of 82%, 88%, and 69%, respectively. For OpenCoss and UAV datasets, general entity extraction
of nouns and verbs outperforms software NER with the F1-score of 47% and 87%, respectively.

TABLE 8 Performance of semantic conï¬‚ict detection approach with NER methods and best sentence embedding (BERT-TFIDF)
from Phase I. Reported values are averaged over 3 folds and results are shown as â€œmean Â± standard deviationâ€. F1-score shows
the relative and absolute percentage change in comparison to Phase I results.

Dataset

NER Method

OpenCoss

General NER
Software NER

WorldVista General NER

UAV

Software NER

General NER
Software NER

PURE

General NER

Software NER

IBM-UAV

General NER

Software NER

F1-score

Recall

Precision

Support

0.47 Â± 0.04 (â†‘ 0.04 / 9.30%)
0.43 Â± 0.04
0.70 Â± 0.12
0.82 Â± 0.03 (â†“ 0.04 / -4.65%)

0.87 Â± 0.04 (â†“ 0.03 / -3.33%)
0.80 Â± 0.02
0.84 Â± 0.17
0.88 Â± 0.11 (0.00 / 0.00%)
0.63 Â± 0.19
0.69 Â± 0.25 (â†‘ 0.05 / 7.93%)

0.68 Â± 0.23
0.68 Â± 0.23
0.57 Â± 0.16
0.74 Â± 0.08

0.86 Â± 0.07
0.74 Â± 0.00
0.76 Â± 0.24
0.80 Â± 0.17
0.55 Â± 0.22
0.65 Â± 0.30

0.41 Â± 0.13
0.34 Â± 0.06
0.96 Â± 0.05
0.87 Â± 0.06

0.88 Â± 0.03
0.86 Â± 0.04
1.00 Â± 0.00
1.00 Â± 0.00
0.82 Â± 0.13
0.77 Â± 0.20

7.33
7.33

18.66
18.66

11.66
11.66

14.00
14.00

9.33
9.33

The added step of semantic conï¬‚ict detection aims to reduce the uncertainty in labeling a particular requirement as conï¬‚ict
solely based on the similarity measure. Table 8 also reports the absolute and relative percentage change in F1-score with respect
to Phase I for all the requirement sets. We observe an absolute improvement in F1-score for OpenCoss and IBM-UAV datasets
by 4% and 5%, respectively. There is also a drop of 3% and 4% F1-score for WorldVista and UAV dataset compared to the results
presented in Table 6. For PURE, Phase II does not lead to any change in performance.

Figure 3 shows the binary classiï¬cation results in the form of normalized confusion matrices averaged over 3 folds, where
the â€˜conï¬‚ictâ€™ class is taken as the positive label. In general, our approach gives good performance in identifying the non-conï¬‚ict
class, and, for the conï¬‚ict class, the performance is highly impacted by the distribution of the class labels. We manage to improve
the number of true positives in OpenCoss and IBM-UAV datasets. Alongside, we notice an increase in false positives and false
negatives for all the datasets. We also present the detailed confusion matrices for each dataset across 3-folds in appendix (see
Section 6).

14

Malik et al.

(a) OpenCoss

(b) WorldVista

(c) UAV

(d) PURE

(e) IBM-UAV

FIGURE 3 Classiï¬cation results in the form of normalised confusion matrices (â€˜Conï¬‚ictâ€™ as positive class) with semantic
conï¬‚ict detection approach.

5

CONCLUSION

Software project development typically requires precise and unambiguous requirements to ensure timely delivery of software
projects. Previous studies suggest converting the software requirements into formal representations before the detection of ambi-
guities and conï¬‚icts in the requirement set. On the other hand, the majority of SRS documents are written in natural language.
We develop a two-phase process for automatic conï¬‚ict detection from SRS documents which works directly on natural language-
based requirements. In Phase I, with the help of transformer-based sentence embeddings, we convert the software requirements
into numeric vectors, and determine the similarity between the requirements using cosine similarity. Next, we employ the ROC
curves to determine the cosine similarity cutoï¬€, which help obtaining the candidate conï¬‚ict set. In Phase II, we apply the gen-
eral and software-speciï¬c NER to extract the meaningful entities from the candidate requirements and calculate the overlapping
entity ratio to determine the ï¬nal set of conï¬‚icts.

In general, our conï¬‚ict detection technique works well with four requirement datasets. In similarity-based conï¬‚ict detection,
BERT-TFIDF achieves better results for all the datasets and provides higher overall F1-score values. In semantic conï¬‚ict detec-
tion, we manage to improve the eï¬ƒcacy of Phase I in identifying the conï¬‚icts by 4% - 5% in terms of F1-score in certain datasets.
However, for some other datasets, we notice that F1-score remains the same or there is a drop compared to Phase I results.
Regardless, we consider Phase II as a technique to verify the embedding-based similarity with semantic-based similarity.

We concur that NLP domain is highly dynamic and new methods (e.g., embeddings) are developed at a fast pace. In this
regard, we aim to extend our analysis by using other transformer-based sentence embeddings. Similarly, transformer-based NER
models can be explored to improve the entity extraction performance. Furthermore, we plan to expand our work to identify the
duplicate requirements in the SRS documents, which require a more nuanced algorithm to distinguish conï¬‚icts from duplicates.

ACKNOWLEDGEMENTS

The authors would like to thank Raad Al-Husban for his valuable help in data preparation and conceptualization steps of this
work.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.70.30.320.680.350.400.450.500.550.600.65Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.040.260.740.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual0.950.0490.140.860.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual100.290.710.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.950.050.350.650.10.20.30.40.50.60.70.80.9Malik et al.

DATA AVAILABILITY STATEMENT

Full research data are not shared due to conï¬dentiality reasons.

References

15

1. Shah US, Jinwala DC. Resolving ambiguities in natural language software requirements: a comprehensive survey. ACM

SIGSOFT Software Engineering Notes 2015; 40(5): 1â€“7.

2. Osman MH, Zaharin MF. Ambiguous software requirement speciï¬cation detection: An automated approach. In: IEEE. ;

2018: 33â€“40.

3. Aldekhail M, Chikh A, Ziani D. Software requirements conï¬‚ict identiï¬cation: review and recommendations. Int J Adv

Comput Sci Appl (IJACSA) 2016; 7(10): 326.

4. Egyed A, Grunbacher P. Identifying requirements conï¬‚icts and cooperation: How quality attributes and automated

traceability can help. IEEE software 2004; 21(6): 50â€“58.

5. Guo W, Zhang L, Lian X. Automatically detecting the conï¬‚icts between software requirements based on ï¬ner semantic

analysis. arXiv preprint arXiv:2103.02255 2021.

6. Merugu R, Chinnam SR. Automated cloud service based quality requirement classiï¬cation for software requirement

speciï¬cation. Evolutionary Intelligence 2021; 14(2): 389â€“394.

7. Zhao L, Alhoshan W, Ferrari A, et al. Natural Language Processing for Requirements Engineering: A Systematic Mapping

Study. ACM Computing Surveys (CSUR) 2021; 54(3): 1â€“41.

8. Ezzini S, Abualhaija S, Arora C, Sabetzadeh M, Briand LC. Using domain-speciï¬c corpora for improved handling of

ambiguity in requirements. In: IEEE. ; 2021: 1485â€“1497.

9. Zhou Y, Tong Y, Gu R, Gall H. Combining text mining and data mining for bug report classiï¬cation. Journal of Software:

Evolution and Process 2016; 28(3): 150â€“176.

10. Aggarwal K, Timbers F, Rutgers T, Hindle A, Stroulia E, Greiner R. Detecting duplicate bug reports with software

engineering domain knowledge. Journal of Software: Evolution and Process 2017; 29(3): e1821.

11. Diamantopoulos T, Roth M, Symeonidis A, Klein E. Software requirements as an application domain for natural language

processing. Language Resources and Evaluation 2017; 51(2): 495â€“524.

12. Sabriye AOJ, Zainon WMNW. A framework for detecting ambiguity in software requirement speciï¬cation. In: IEEE. ;

2017: 209â€“213.

13. Handbook A. From Contract Drafting to Software Speciï¬cation: Linguistic Sources of Ambiguity. 2003.

14. Mairiza D, Zowghi D, Nurmuliani N. Managing conï¬‚icts among non-functional requirements. In: University of Technology,

Sydney. ; 2009.

15. Kim M, Park S, Sugumaran V, Yang H. Managing requirements conï¬‚icts in software product lines: A goal and scenario

based approach. Data & Knowledge Engineering 2007; 61(3): 417â€“432.

16. Butt WH, Amjad S, Azam F. Requirement conï¬‚icts resolution: using requirement ï¬ltering and analysis. In: Springer. ; 2011:

383â€“397.

17. Moser T, Winkler D, Heindl M, Biï¬„ S. Requirements management with semantic technology: An empirical study on

automated requirements categorization and conï¬‚ict analysis. In: Springer. ; 2011: 3â€“17.

18. Viana T, Zisman A, Bandara AK. Identifying conï¬‚icting requirements in systems of systems. In: IEEE. ; 2017: 436â€“441.

16

Malik et al.

19. Whittle J, Sawyer P, Bencomo N, Cheng BH, Bruel JM. RELAX: a language to address uncertainty in self-adaptive systems

requirement. Requirements Engineering 2010; 15(2): 177â€“196.

20. Manning CD, Surdeanu M, Bauer J, Finkel JR, Bethard S, McClosky D. The Stanford CoreNLP natural language processing

toolkit. In: stanford. ; 2014: 55â€“60.

21. Ghosh S, Elenius D, Li W, Lincoln P, Shankar N, Steiner W. Automatically extracting requirements speciï¬cations from

natural language. arXiv preprint arXiv:1403.3142 2014.

22. Li W. Speciï¬cation mining: New formalisms, algorithms and applications. University of California, Berkeley . 2013.

23. Polpinij J, Ghose A. An automatic elaborate requirement speciï¬cation by using hierarchical text classiï¬cation. In: . 1. IEEE.

; 2008: 706â€“709.

24. Subha R, Palaniswami S. Ontology extraction and semantic ranking of unambiguous requirements. Life Science Journal

2013; 10(2): 131â€“8.

25. Sharma R, Bhatia J, Biswas K. Machine learning for constituency test of coordinating conjunctions in requirements

speciï¬cations. In: ACM. ; 2014: 25â€“31.

26. Sardinha A, Chitchyan R, Weston N, Greenwood P, Rashid A. EA-Analyzer: automating conï¬‚ict detection in a large set of

textual aspect-oriented requirements. Automated Software Engineering 2013; 20(1): 111â€“135.

27. Oo KH, Nordin A, Ismail AR, Sulaiman S. An Analysis of Ambiguity Detection Techniques for Software Requirements

Speciï¬cation (SRS). International Journal of Engineering & Technology 2018; 7(2.29): 501â€“505.

28. Das S, Deb N, Cortesi A, Chaki N. Sentence embedding models for similarity detection of software requirements. SN

Computer Science 2021; 2(2): 1â€“11.

29. Cleland-Huang J, Vierhauser M, Bayley S. Dronology: An incubator for cyber-physical system research. arXiv preprint

arXiv:1804.02423 2018.

30. Mavin A, Wilkinson P, Harwood A, Novak M. Easy approach to requirements syntax (EARS). In: IEEE. ; 2009: 317â€“322.

31. Ferrari A, Spagnolo GO, Gnesi S. Pure: A dataset of public requirements documents. In: IEEE. ; 2017: 502â€“505.

32. Cer D, Yang Y, Kong Sy, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175 2018.

33. Aizawa A. An information-theoretic perspective of tfâ€“idf measures. Information Processing & Management 2003; 39(1):

45â€“65.

34. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding.

arXiv preprint arXiv:1810.04805 2018.

35. McInnes L, Healy J, Melville J. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv

preprint arXiv:1802.03426 2018.

36. Corley CD, Mihalcea R. Measuring the semantic similarity of texts. In: ACL. ; 2005: 13â€“18.

37. Gao X, Wu S. Hierarchical clustering algorithm for binary data based on cosine similarity. In: IEEE. ; 2018: 1â€“6.

38. Rupp C, Simon M, Hocker F. Requirements engineering und management. HMD Praxis der Wirtschaftsinformatik 2009;

46(3): 94â€“103.

39. Khin NPP, Aung TN. Analyzing tagging accuracy of part-of-speech taggers. In: Springer. ; 2015: 347â€“354.

Malik et al.

6

APPENDIX

17

This section is structured into two parts. We ï¬rst present the detailed results for each dataset for similarity-based conï¬‚ict detection
technique. Then, we present the normalized confusion matrices for each dataset for the semantic conï¬‚ict detection technique.

ROC Curves for Threshold Detection

6.1
Figure 4,5,6, and 7 shows the ROC curves obtained from the 3-fold cross validation over all the requirement sets. These curves
facilitate the process of ï¬nding the cosine similarity threshold in Phase I.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 4 ROC curves for OpenCoss requirement set with BERT-TFIDF embedding across 3 folds

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 5 ROC curves for WorldVista requirement set with BERT-TFIDF embedding across 3 folds

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC18

Malik et al.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 6 ROC curves for PURE requirement set with BERT-TFIDF embedding across 3 folds

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 7 ROC curves for IBM-UAV requirement set with BERT-TFIDF embedding across 3 folds

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROC0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateROCMalik et al.

19

Detailed Classiï¬cation Results for Phase II

6.2
Figure 8 shows the performance of the semantic conï¬‚ict detection technique over OpenCoss dataset. We observe variability
in the results in each fold as higher number of FPs is contributing towards lower F1-scores. Figure 9 and 10 for WorldVista
and UAV datasets, respectively, shows consistent results over all the folds, and we are able to capture the requirement conï¬‚icts
with signiï¬cant accuracy. Figure 11 shows the dominance of the FN values in the PURE dataset in the ï¬rst fold, which distorts
the overall F1-score for the conï¬‚ict class. Figure 12 for IBM-UAV dataset shows great performance in second and third fold,
however, ï¬rst fold shows an increase in FN which contributes to lower F1-scores for the conï¬‚ict class.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 8 Normalized confusion matrices for OpenCoss dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 9 Normalized confusion matrices for WorldVista dataset in each fold.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.610.390.380.620.400.450.500.550.60Non-ConflictConflictPredictedNon-ConflictConflictActual0.940.0650.570.430.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.550.45010.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.350.650.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.070.250.750.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.070.150.850.10.20.30.40.50.60.70.80.920

Malik et al.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 10 Normalized confusion matrices for UAV dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 11 Normalized confusion matrices for PURE dataset in each fold.

(a) Fold - 1

(b) Fold - 2

(c) Fold - 3

FIGURE 12 Normalized confusion matrices for IBM-UAV dataset in each fold.

Non-ConflictConflictPredictedNon-ConflictConflictActual0.930.0740.0910.910.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.0370.250.750.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual0.960.0370.0830.920.20.40.60.8Non-ConflictConflictPredictedNon-ConflictConflictActual100.430.570.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.140.860.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.290.710.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual0.920.080.750.250.10.20.30.40.50.60.70.80.9Non-ConflictConflictPredictedNon-ConflictConflictActual0.920.08010.00.20.40.60.81.0Non-ConflictConflictPredictedNon-ConflictConflictActual100.30.70.00.20.40.60.81.0