CARBON: A Counterfactual Reasoning based Framework for
Neural Code Comprehension Debiasing

Shuzheng Gao1, Cuiyun Gao1,2,3âˆ—, Chaozheng Wang1, Jun Sun4, David Lo4
1 Harbin Institute of Technology, Shenzhen, China
2 Peng Cheng Laboratory, Shenzhen, China
3 Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies, China
4 Singapore Management University, Singapore
szgao98@gmail.com,gaocuiyun@hit.edu.cn,wangchaozheng@stu.hit.edu.cn,junsun@smu.edu.sg,davidlo@smu.edu.sg

2
2
0
2

l
u
J

2
2

]
E
S
.
s
c
[

1
v
4
0
1
1
1
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
Previous studies have demonstrated that code intelligence models
are sensitive to program transformation among which identifier
renaming is particularly easy to apply and effective. By simply
renaming one identifier in source code, the models would output
completely different results. The prior research generally mitigates
the problem by generating more training samples. Such an approach
is less than ideal since its effectiveness depends on the quantity
and quality of the generated samples. Different from these studies,
we are devoted to adjusting models for explicitly distinguishing
the influence of identifier names on the results, called naming bias
in this paper, and thereby making the models robust to identifier
renaming. Specifically, we formulate the naming bias with a struc-
tural causal model (SCM), and propose a counterfactual reasoning
based framework named CARBON for eliminating the naming bias
in neural code comprehension. CARBON explicitly captures the
naming bias through multi-task learning in the training stage, and
reduces the bias by counterfactual inference in the inference stage.
We evaluate CARBON on three neural code comprehension tasks,
including function naming, defect detection and code classification.
Experiment results show that CARBON achieves relatively better
performance (e.g., +0.5% on the function naming task at F1 score)
than the baseline models on the original benchmark datasets, and
significantly improvement (e.g., +37.9% on the function naming task
at F1 score) on the datasets with identifiers renamed. The proposed
framework provides a causal view for improving the robustness of
code intelligence models.

1 INTRODUCTION
With the rapid development of artificial intelligence techniques,
automated code comprehension has drawn more and more attention
in the software engineering community [18, 23, 32, 40, 50]. Recently,
many neural code comprehension models have been proposed to
learn the code semantics with deep neural networks (DNNs), and
achieved state-of-the-art performance on various tasks, such as code
summarization [40], function naming [3], and defect detection [54].
Despite the great success of the code intelligence models, recent
studies have shown that the models are not robust to semantic-
preserving program transformation especially the modification
on identifier names [29, 45, 48]. As illustrated in Figure 1 (a), the
neural function naming model NCS [1] can correctly predict the
function name as â€œsortâ€ given the original code. However, when we

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
2022. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

simply substitute the name of the identifier â€œarrâ€ with â€œf â€, the model
provides a completely different function name which is irrelevant
to the program semantics.

(a) Prediction result of original code.

(b) Prediction result when the identifier substituted.

Figure 1: A motivating example.

Existing research mainly mitigate this problem by generating
more training samples [30, 45, 48]. Although the results seem more
robust to identifier renaming, due to their data-driven nature, the
performance of these methods highly relies on the quantity and
quality of generated samples [33, 47]. Another effort [45] explore to
relieve the issue by code abstraction which replaces the identifier
names with a specific symbol, e.g., â€œVARâ€. Although the technique
reduces the modelsâ€™ dependency on identifier names, several stud-
ies [7, 45] have demonstrated that it leads to obvious performance
degradation on tasks such as function naming and code completion,
which also indicates that identifier names are meaningful for pro-
gram comprehension. In this paper, we keep the data unchanged
for exploiting the naming information, and aim at modifying the
model to explicitly prevent it from being severely misled by the
identifier names.

1  public static void(int[] arr) {2 for(int i = 1; i < arr.length; i++){3 int x = arr[i];4 int j = i -1;5 while (j >= 0 && arr[j] > x) {6 arr[j + 1] = arr[j];7 j--;8 }9 arr[j + 1] = x;10 }11 }Prediction: sort1  public static void(int[] f) {2 for (int i = 1; i < f.length; i++){3 int x = f[i];4 int j = i -1;5 while (j >= 0 && f[j] > x) {6 f[j + 1] = f[j];7 j--;8 }9 f[j + 1] = x;10 }11 }Prediction: open 
 
 
 
 
 
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

comprehensioN (CARBON) framework to eliminate the naming
bias. CARBON involves a counterfactual model, as shown in Fig-
ure 2 (b), which computes the direct effect by estimating â€œwhat the
name of this function would be if only its identifier names were givenâ€.
In this work, we obtain this counterfactual model through multi-
task learning. With the counterfactual model, CARBON conducts
counterfactual inference to eliminate the naming bias by subtract-
ing the direct effect from total effect in the inference stage. With
counterfactual reasoning, CARBON can return accurate and robust
prediction, e.g., accurately predicts the function name â€œsortâ€ even
if the identifier name is changed, as illustrated in Figure 2 (c).

We evaluate the ability of CARBON in debiasing the identifier
names on three popular code comprehension tasks, including func-
tion naming, software defect detection and code classification. For
each task, we choose at least three popular conventional models
and equip them with the proposed framework CARBON. For eval-
uating the diabiasing ability, we establish a transformed test set
for each dataset by randomly substituting each identifier name in
the original test set with another one in the dataset. Experimental
results demonstrate that the models extended with CARBON not
only significantly improves robustness on transformed test sets
but also achieves relatively better performance on the original test
sets. Specifically, for the function naming task, CARBON improves
the F1 score of NCS on average by 37.9% and 0.5% on transformed
test set and original test set, respectively; for defect detection it
achieves improvement on accuracy of CodeBERT by 8.3% and 0.9%
on the transformed test set and original test set, respectively; while
for code classification, it improves the accuracy of CodeBERT by
1.9% and 0.3% on the transformed test set and original test set,
respectively.

In summary, the main contributions of this work are as follows:
(1) To the best of our knowledge, we are the first to focus on
the model bias caused by identifier names, and formulate
the bias from a causal view.

(2) We propose a counterfactual reasoning-based framework
CARBON for capturing the naming bias in current neural
code comprehension models through multi-task learning,
and conducting debiasing via counterfactual inference.
(3) Extensive experiments show that the proposed framework
CARBON is more robust to identifier renaming than con-
ventional models on various code comprehension tasks. The
debiasing ability of CARBON also improves the overall per-
formance on the original datasets.

2 PRELIMINARIES
In this section, we review the key concepts we used in causal in-
ference. In the following, we use the capital letter (e.g., T ) and
lowercase letter (e.g., t) to denote a variable and a specific value
respectively.

Structural Causal Model. The Structural Causal Model (SCM)
reflects the causal relations between variables through a directed
acyclic graph ğº = {ğ‘‰ , ğ¸}, where ğ‘‰ denotes the set of variables
and ğ¸ denotes the set of edges which describe the direct causal
relationship between variables. These causal relationships can be
further parameterized with structural equation [25]. Figure 3 is a
simple example of SCM involving three variables, treatment variable

Figure 2: Our causal view on the naming bias in neural code
comprehension. The conventional model and counterfac-
tual model estimate the result of corresponding question,
respectively. The prediction results are from the example
in Figure 1. The length of the bar represents the predicted
probability of the corresponding label.

Inspired by recent research in causal inference [46, 53], we pro-
pose a novel view for the problem: The incorrect prediction when
substituting identifier names, e.g., Figure 1 (b), is induced by the
spurious correlation [26] between the identifier names and predic-
tion results, e.g., â€œf â€ and â€œopenâ€. For the example in Figure 1, a large
number of functions named â€œopenâ€ in the training set use â€œf â€ as an
abbreviation of a file handling object, e.g., â€œFile f = new File()â€ in
Java, biasing the model to learn the strong yet spurious correlation
between the identifier name â€œf â€ and prediction result â€œopenâ€. In
this paper, we define the bias caused by the spurious correlation
between identifier names and results as naming bias. While names
are useful for code comprehension tasks, they can be misleading
since technically they are irrelevant to the programming behav-
ior and developers may choose certain names for various reasons.
Thus, mitigating the naming bias is beneficial for improving both
the accuracy and robustness of model predictions. However, due to
the black-box nature of deep learning models, it is challenging to
explicitly isolate the naming bias from the models.

The recent counterfactual reasoning-based approaches have
proven effective on debiasing in fields such as computer vision [24]
and recommendation [37, 41]. Motivated by this, we revisit the pre-
diction process of neural code comprehension from a causal view,
as shown in Figure 2. By formulating the process with a structural
causal model (SCM) [26], we identify that the naming bias is attrib-
uted to the direct effect of identifier names on the model prediction.
Conventional neural code comprehension models, as shown in Fig-
ure 2 (a), perform prediction by estimating the total effect of the
input code snippet on results, and are incapable to distinguish the
direct effect from the total effect. Thus, the conventional models
are sensitive to identifier naming, e.g., outputting the prediction
â€œopenâ€ after we substitute the identifier name â€œarrâ€ with â€œf â€. In this
work, we propose a CounterfActual Reasoning Based neural cOde

Counterfactual ModelCounterfactual Inference (CARBON)-=Total EffectDirect EffectTotal EffectDirect EffectConventional ModelopensortWhat is the name of this function if the whole code was given?What is the name of this function if only its identifier names were given?opensortopensortopensortopensortDebiased PredictionabcCARBON: A Counterfactual Reasoning based Framework for Neural Code Comprehension Debiasing

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Figure 3: An example of SCM. White and gray nodes denote
the variables are at factual and counterfactual status respec-
tively. Four graphs on the right represent counterfactual no-
tations under different situations.

medicine (ğ‘€), mediator variable placebo effect1(ğ‘ƒ) and outcome vari-
able disease (ğ·) [26]. There causal relationships can be presented
as follows:

(1)

ğ‘ƒğ‘š = ğ‘“ğ‘ƒ (ğ‘€ = ğ‘š),
ğ·ğ‘š,ğ‘ = ğ‘“ğ· (ğ‘€ = ğ‘š, ğ‘ƒ = ğ‘),
(2)
where ğ‘“ğ‘ƒ and ğ‘“ğ· denote the structural equation of corresponding
variable respectively. In this case, the causal effect of medicine on
disease exists in two paths. The first path M â†’ D denotes that
medicine has a direct effect on disease through the biological mech-
anism. The other path M â†’ P â†’ D shows that taking medicines can
also alleviate the disease through the mediator, placebo effect. Thus
when estimating to what extent the medicine affects the disease
through the placebo effect, we need to exclude the bias caused by
the biological mechanism of medicine, i.e., M â†’ D.

Counterfactual Inference. Counterfactual inference is used to
estimate for the same individual what the outcome variable would
be if the value of some variables were different from the value in the
factual world. As shown in Figure 3, counterfactual inference can
answer the following question: whether the disease of the patient
could be alleviated if he didnâ€™t receive the placebo effect but took
medicines. Specifically, it estimates the value of ğ· when ğ‘ƒ receives
ğ‘€ = ğ‘š through ğ‘€ â†’ ğ‘ƒ, while ğ· receives ğ‘€ = ğ‘šâˆ— through ğ‘€ â†’
ğ·, where ğ‘šâˆ— denotes the value of ğ‘€ in a counterfactual world, i.e.,
the patient did not take medicine. This estimation can be achieved
by using ğ‘‘ğ‘œ (ğ‘ƒ = ğ‘âˆ—):

ğ·ğ‘š,ğ‘âˆ— = ğ‘“ğ· (ğ‘€ = ğ‘š, ğ‘‘ğ‘œ (ğ‘ƒ = ğ‘âˆ—)),
(3)
where ğ‘‘ğ‘œ (.) operator denotes the intervention defined by SCM. It
forcibly substitutes ğ‘ = ğ‘“ğ‘ƒ (ğ‘€ = ğ‘š) with ğ‘âˆ— = ğ‘“ğ‘ƒ (ğ‘€ = ğ‘šâˆ—) in the
structural equation ğ‘“ğ· . Note that ğ‘‘ğ‘œ (ğ‘ƒ = ğ‘âˆ—) does not affect the
ascendant variable of ğ‘ƒ, i.e., ğ‘€ retains its value ğ‘š on the direct
path M â†’ D. In clinical trials, it represents that the patient takes
medicine without being informed.

Causal effects. The causal effects measure to what extent value
change of the treatment variable (e.g., the value of ğ‘€ change from
ğ‘šâˆ— to ğ‘š) affects the value of the outcome variable (e.g., ğ·). For
example in Figure 3, the total effect (TE) [26] of ğ‘€ on ğ· is defined
as:

ğ‘‡ ğ¸ = ğ·ğ‘š,ğ‘ âˆ’ ğ·ğ‘šâˆ—,ğ‘âˆ—,

(4)

1https://en.wikipedia.org/wiki/Placebo#Effects

(a) The SCM of conventional neural code comprehension
model.

(b) The SCM of our counterfactual reasoning-based neu-
ral code comprehension model.

Figure 4: Illustration for the SCM of conventional neu-
ral code comprehension model (a), and our counterfactual
reasoning-based neural code comprehension model (b).

where ğ·ğ‘šâˆ—,ğ‘âˆ— denotes the situation that the patient neither took
medicines nor received the placebo effect. We can see that TE calcu-
lates the causal effect of ğ‘€ to ğ· from both the direct causal path M
â†’ D and the indirect causal path M â†’ P â†’ D. For a detailed anal-
ysis, existing work often decomposes TE into natural direct effect
(NDE) and total indirect effect (TIE) through TE = NDE+TIE [24, 34].
NDE represents the value change of the outcome variable when
value change of the treatment variable only affects it through the
direct path M â†’ D. Formally, NDE is defined as follows:

ğ‘ ğ·ğ¸ = ğ·ğ‘š,ğ‘âˆ— âˆ’ ğ·ğ‘šâˆ—,ğ‘âˆ—,

(5)

Accordingly, the TIE can be obtained by subtracting NDE from TE:
ğ‘‡ ğ¼ ğ¸ = ğ‘‡ ğ¸ âˆ’ ğ‘ ğ·ğ¸ = ğ·ğ‘š,ğ‘ âˆ’ ğ·ğ‘š,ğ‘âˆ—,

(6)

TIE measures value change the outcome variable when value change
of the treatment variable only affects it through the indirect path M
â†’ P â†’ D. Equipped with the above causality concepts, we solve the
problem of estimating the influence of placebo effect via calculating
TIE of ğ‘€ on ğ·.

3 METHODOLOGY
In this section, we first present our causal view of the prediction
process for neural code comprehension models, among which the
naming bias is formulated with an SCM. Then we describe our
proposed CARBON framework for eliminating the naming bias via
counterfactual reasoning.

3.1 A Causal View on Neural Code

Comprehension

As shown in Figure 4 (a), we abstract the prediction process of neural
code comprehension by defining four variables: 1) naming infor-
mation T , which denotes the information introducing naming bias
in source code, e.g., user-defined identifier name; 2) non-naming

mDppm*Dp*m, m*MDPp*M:  MedicineP:   Placebo EffectD:  DiseaseDDâˆ—, âˆ—, âˆ—, âˆ—FTKRT:  Naming informationF:  Non-naming informationK:  Combined knowledgeR:  Model predictionftkRf*k*-tRt*, , âˆ—, âˆ—, Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

(a) Conventional model.

(b) CARBON.

Figure 5: The overall workflow of conventional neural code comprehension model and CARBON. The basic model denotes
the model used by conventional code comprehension models.

information F , which denotes the code properties without nam-
ing bias, e.g., the remaining tokens other than the identifier names;
3) combined knowledge K, which serves as a mediator exploit-
ing both the naming information and non-naming information for
the model prediction; 4) model prediction R, which denotes the
prediction results of code comprehension tasks, e.g., classification
scores for the code classification task.

Based on the above variable definitions, we formulate the causal
structure of conventional models, as illustrated in Figure 4 (a), with
causal relationships shown as follows:

(1) F â†’ R represents the causal relationship from non-naming
information ğ¹ to the model prediction ğ‘…, e.g., predicting the
function name of given code snippet as â€œsortâ€ (Figure 1 (a))
since the two loops for implementing data swap operations
in the code are related to data sorting. We regard this path
as a â€œbeneficialâ€ path because the prediction only based on
non-naming information is robust to identifier renaming.
(2) T â†’ R represents the causal relationship from naming in-
formation ğ‘‡ to the model prediction ğ‘…, e.g., predicting the
function name of the given code snippet as â€œopen â€ (Figure 1
(b)) only since the code involves an identifier â€œf â€. Consider-
ing that the naming information only cannot determine code
semantics, naming-based prediction will be severely misled
by the spurious correlation under identifier renaming. Since
the path introduces naming bias to the model prediction, we
regard this path as a â€œbadâ€ path.

(3) F, T â†’ K â†’ R represents the process that models predict
based on the combined knowledge ğ¾ which exploits both
the naming information ğ‘‡ and non-naming information ğ¹ .

We also regard this path as a â€œbeneficialâ€ path since it not
only leverages the robust non-naming information F and
also uses naming information T as supplementary. Although
technically the names are irrelevant to the program behav-
iors, it still contributes to the code comprehension process.
The path represents that we leverage the useful informa-
tion of identifier names through the combined knowledge ğ¾
instead of discarding them entirely, which is beneficial for
model accuracy.

As shown in Figure 4 (a), conventional neural code comprehen-
sion models predict through the total effect, i.e., ğ‘…ğ‘“ ,ğ‘˜,ğ‘¡ - ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ— ,
which integrates the direct effect from all the three paths. The in-
clusion of the â€œbadâ€ path T â†’ R inevitably introduces naming bias
to the conventional models. Thus, to improve the robustness and
accuracy of the models, the direct effect of ğ‘‡ â†’ ğ‘… from the total
effect should be excluded during model prediction. Based on the
causal analysis, we introduce the counterfactual reasoning-based
neural code comprehension model which estimates the causal effect
of ğ‘‡ and ğ¹ on the prediction ğ‘… with direct effect ğ‘‡ â†’ ğ‘… blocked, as
shown in Figure 4 (b). Specifically, the model estimates the direct
effect of ğ‘‡ on ğ‘… with the natural direct effect, i.e., ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ - ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ— ,
and then subtracts the direct effect from the total effect. We describe
how we implement the idea and details of the proposed general
framework CARBON for eliminating the naming bias in the next
section.

3.2 Details of CARBON
In this section, we elaborate on the details of CARBON. Figure 5 (a)
and (b) illustrate the overall workflow of the conventional model

Source codeBasic Modelğ‘ğ¿Inference Stage:ğ‘Training  Stage:ğ‘LossClassification scoreParsepublic static void (int[]){for(int =1; < .length; ++){int = [];...f i i f i x f i ...public static void (int[]f){for(int i=1;i<f.length;i++){int x = f[i];...Basic Model   Basic ModelBasic ModelTraining  Stage:=(++)++=   Fusion  1  public static void   (int[] f) { 2      for (int i = 1; i < f.length; i++){ 3          int x = f[i]; 4          int j = i - 1; 5          while (j >= 0 && f[j] > x) { 6              f[j + 1] = f[j]; 7              j--; 8          } 9          f[j + 1] = x;10      }11  }Naming branchCombinedbranchNon-namingbranchInference Stage:++Â (1âˆ’)âˆ—=sharingsharing Source codeAn example of source codeâ€™=++(âˆ’)âˆ—CARBON: A Counterfactual Reasoning based Framework for Neural Code Comprehension Debiasing

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

and proposed CARBON, respectively. For conventional models, the
training and inference stages are same; while for CARBON, the cal-
culation of classification scores2 in the training and inference stage
are different. Specifically, in the training stage, CARBON captures
each direct effect in SCM through multi-task learning; while in the
inference stage, it eliminates the naming bias by counterfactual
inference.

3.2.1 Task formulation. In this work, we broadly divide neural
code comprehension tasks into two paradigms, i.e., classification-
based and generation-based. Considering that the generation-based
task can be viewed as a successive classification task, where the
models output classification scores over the vocabulary at each
time step, we unify the workflow of the neural code comprehension
tasks as follows. Assume that we have a source code database ğ‘‹ =
{ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ‘› } and corresponding ground truth ğ‘Œ = {ğ‘¦1, ğ‘¦2, ..., ğ‘¦ğ‘› },
where ğ‘› is the number of training data and ğ‘¦ ğ‘— is a class label or
target sequence of the ğ‘—-th training instance for classification and
generation task, respectively. In the training stage, our goal is to
train a neural model F to minimize the prediction error on training
set. Formally, F can be formulated as:

Â¯F = arg min

âˆ‘ï¸

ğ¿(F (ğ‘¥), ğ‘¦),

(7)

F

(ğ‘¥,ğ‘¦) âˆˆ {ğ‘‹ ,ğ‘Œ }
where ğ¿(Â·) denotes the loss function such as cross entropy and F
can be a large variety of existing neural models like Long Short-
Term Memory (LSTM) [11] and Transformer [35]. In the inference
stage, given a sample ğ‘¥ â€² in the test set, the model first calculates the
classification score, i.e., ğ‘ and ğ‘ â€²
ğ‘Ÿ for the conventional model and
CARBON, respectively (as shown in Figure 5). Following the widely-
used greedy search strategy, the class with the highest classification
score is selected as prediction result:

ğ‘¦ğ‘Ÿ = arg max

ğ‘

(ğ‘§ â€²),

(8)

where ğ‘ is the set of candidate classes and ğ‘¦ğ‘Ÿ is the prediction result.

Framework Design. As shown in Figure 5 (b), our proposed
3.2.2
CARBON framework follows the SCM illustrated in Figure 4 (b).
Inspired by the studies in computer vision [24], we distinguish
the three causal paths including F â†’ R, K â†’ R and T â†’ R by
designing three branches in CARBON, i.e., non-naming branch,
combined branch and naming branch, respectively. To avoid the
increase of model size, the parameters of basic models for the three
branches are shared. Specifically, CARBON first parses the input
source code into three types of input, including non-naming in-
put ğ‘“ , combined input ğ‘˜, naming input ğ‘¡ for the three branches,
respectively. CARBON then estimates the direct causal effect of
each path by calculating classification score for each branch based
on the basic model, formulated as:

ğ‘ ğ‘“ = F (ğ‘“ ), ğ‘ğ‘˜ = F (ğ‘˜), ğ‘ğ‘¡ = F (ğ‘¡).

(9)

To obtain the total effect of the source code on prediction, we
combine the direct causal effect from three branches and fuse their

2It is also called logits in many machine learning papers [10, 20].

Algorithm 1 Algorithm of CARBON framework
Input: training set {ğ‘‹ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, ğ‘Œğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› }, test set {ğ‘‹ğ‘¡ğ‘’ğ‘ ğ‘¡ }, fusion itera-

tion threshold ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, total iteration ğ¼

Output: neural model F , prediction result ğ‘¦ğ‘Ÿ

Multi-task Training:
1: for ğ‘– âˆˆ {1, ..., ğ¼ } do
2:

Extract ğ‘“ , ğ‘¡, ğ‘˜ from ğ‘‹ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘ ğ‘“ = F (ğ‘“ ), ğ‘ğ‘¡ = F (ğ‘¡), ğ‘ğ‘˜ = F (ğ‘˜)
if ğ‘–<ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› then

ğ‘ğ‘Ÿ = ğ‘ğ‘˜

else

ğ‘ğ‘Ÿ = 1

3 (ğ‘ ğ‘“ +ğ‘ğ‘˜ +ğ‘ğ‘¡ )

end if
Calculating ğ¿ğ‘Ÿ , ğ¿ğ‘¡ , ğ¿ğ‘“ with ğ‘Œğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
Update model F with ğ¿ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = ğ¿ğ‘Ÿ +ğ¿ğ‘¡ +ğ¿ğ‘“

10:
11: end for
12: return model F

3:

4:

5:

6:

7:

8:

9:

Counterfactual Inference:
1: Extract ğ‘“ , ğ‘¡, ğ‘˜ from ğ‘‹ğ‘¡ğ‘’ğ‘ ğ‘¡
2: ğ‘ ğ‘“ = F (ğ‘“ ), ğ‘ğ‘¡ = F (ğ‘¡), ğ‘ğ‘˜ = F (ğ‘˜)
3: ğ‘ğ‘Ÿ = ğ‘ ğ‘“ + ğ‘ğ‘˜ + (1 âˆ’ ğ›¼) âˆ— ğ‘ğ‘¡
4: ğ‘¦ğ‘Ÿ = arg max(ğ‘ğ‘Ÿ )
5: return Prediction result ğ‘¦ğ‘Ÿ

classification score into ğ‘ğ‘Ÿ which is corresponding to the variable
ğ‘… in SCM. Here we compute ğ‘ğ‘Ÿ by using a simple fusion method:

ğ‘ğ‘Ÿ =

1
3

(ğ‘ ğ‘“ + ğ‘ğ‘˜ + ğ‘ğ‘¡ ),

(10)

According to the definition of structural equation in Section 2, the
fusion method also indicates that the structural equation of the
variable ğ‘… is parameterized as follows:

ğ‘…ğ‘“ ,ğ‘˜,ğ‘¡ =

1
3

(F (ğ‘“ ) + F (ğ‘˜) + F (ğ‘¡)).

(11)

Based on the framework designed following SCM, we intro-
duce the stages of model training and inference in Section 3.2.3
and Section 3.2.4, respectively. The overall computation process of
CARBON is illustrated in Algorithm 1.

3.2.3 Multi-task Training. In the training stage, CARBON needs
to realize multiple goals: 1) to accurately estimate the total effect
of source code on prediction results; and 2) to capture the direct
effect of each path and distinguish it from total effect. To achieve the
goals, we adopt the multi-task learning strategy [37, 41] for model
training:

ğ¿ğ‘“ = ğ¿(ğ‘ ğ‘“ , ğ‘¦), ğ¿ğ‘Ÿ = ğ¿(ğ‘ğ‘Ÿ , ğ‘¦), ğ¿ğ‘¡ = ğ¿(ğ‘ğ‘¡ , ğ‘¦),

(12)

ğ¿ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = ğ¿ğ‘“ + ğ¿ğ‘Ÿ + ğ¿ğ‘¡ ,
(13)
Here, ğ¿ğ‘Ÿ is used to train the model to accurately estimate the total
effect (i.e., the first goal); while ğ¿ğ‘“ and ğ¿ğ‘¡ are involved to capture the
direct effect and distinguish it from the total effect (i.e., the second
goal). Besides, to reduce the difficulty of capturing the direct effect
and distinguishing it from total effect simultaneously, we also adopt
the deferred training strategy [6]. Specifically, we first train the
three branches separately to well capture each direct effect, and

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

then fuse their classification scores after ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› iterations (Line 4-7
of the multi-task training stage in Algorithm 1).

3.2.4 Counterfactual Inference. As aforementioned, the key to elim-
inate the naming bias is to remove the direct effect T â†’ R from the
total effect. CARBON first estimates the total effect of the input code
on model prediction ğ‘… as follows:

ğ‘‡ ğ¸ = ğ‘…ğ‘“ ,ğ‘˜,ğ‘¡ âˆ’ ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ—,

(14)

where ğ‘“ âˆ—, ğ‘˜âˆ— and ğ‘¡ âˆ— denote the corresponding empty input. We fol-
low [24, 41] and set the classification score to a uniform distribution
if the input is empty, which is formulated as:

F (ğ‘“ âˆ—) = F (ğ‘˜âˆ—) = F (ğ‘¡ âˆ—) = ğ‘¢

(15)

where ğ‘¢ is the classification score with uniform distribution. Then
CARBON estimates the naming bias by calculating the NDE of
naming input on model prediction, i.e., the direct effect of ğ‘‡ = ğ‘¡ on
prediction ğ‘… under the situation ğ¹ = ğ‘“ âˆ— and ğ¾ = ğ‘˜âˆ—:

ğ‘ ğ·ğ¸ = ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ’ ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ—,

(16)

Furthermore, eliminating naming bias can be realized by subtracting
NDE from TE. Here we further introduce a hyper-parameter ğ›¼ to
control the degree of debiasing in the following equation:

ğ‘‡ ğ¸ âˆ’ ğ›¼ âˆ— ğ‘ ğ·ğ¸ = ğ‘…ğ‘“ ,ğ‘˜,ğ‘¡ âˆ’ ğ›¼ âˆ— ğ‘…ğ‘“ ,ğ‘˜ âˆ—,ğ‘¡ âˆ—,

(17)

where ğ›¼ is ranged from 0 to 1. Since we only need the class index
with maximum classification score in the inference stage, after
removing the constant, the final classification score ğ‘ â€²
ğ‘Ÿ is calculated
as follows:
ğ‘ â€²
ğ‘Ÿ = ğ‘…ğ‘“ ,ğ‘˜,ğ‘¡ âˆ’ ğ›¼ âˆ— ğ‘…ğ‘“ âˆ—,ğ‘˜ âˆ—,ğ‘¡ âˆ—

=

(F (ğ‘“ ) + F (ğ‘˜) + F (ğ‘¡)) âˆ’

(F (ğ‘“ âˆ—) + F (ğ‘˜âˆ—) + F (ğ‘¡))

ğ›¼

3
ğ›¼

(ğ‘¢ + ğ‘¢ + F (ğ‘¡))

(18)

1
3
1
3

=

(F (ğ‘“ ) + F (ğ‘˜) + F (ğ‘¡)) âˆ’

3
âˆ F (ğ‘“ ) + F (ğ‘˜) + (1 âˆ’ ğ›¼) âˆ— F (ğ‘¡)
= ğ‘ ğ‘“ + ğ‘ğ‘˜ + (1 âˆ’ ğ›¼) âˆ— ğ‘ğ‘¡ .

4 EXPERIMENTAL SETUP
In this section, we detail the experimental settings for three pop-
ular evaluation tasks including function naming, defect detection
and code classifications, which have been active area of software
engineering research for years.

4.1 Evaluation Tasks
Function naming: Function naming aims to automatically
4.1.1
generate a meaningful and succinct name for a function. In software
industry, it can help engineers correct the inconsistent method and
API name for program readability and maintainability [15, 17, 22].
In this work, we formulated it as a generation task with greedy
search strategy and cross-entropy loss function.

4.1.2 Defect detection: Given a code snippet, defect detection aims
to detect whether there are vulnerabilities in it, which is crucial to
defend a software system from cyberattacks [54, 55]. In the previous
work, it is formulated as a binary classification task and the binary
cross-entropy is always used as the loss function.

Table 1: Statistics of the benchmark datasets.

Datasets
CSN-Java
CSN-Python
CSN-Go
CSN-PHP
CSN-Ruby
CSN-JavaScript
Defect Detection
Code Classification

Train Validation
5,183
164,923
13,914
251,820
7,325
167,288
12,982
241,241
1,400
24,927
2,745
38,499
2,732
21,854
10,400
31,200

Test
10,955
14,014
8,122
14,014
1,261
2,232
2,732
10,400

4.1.3 Code classification: Code classification is the task of clas-
sifying a code snippet by its functionality, which is helpful for
program comprehension and maintenance [21, 50]. It is formulated
as a multi-class classification task which utilize cross-entropy as
the loss function.

4.2 Baselines
Function naming: For function naming, we adopt three basic
4.2.1
models for evaluation. CodeNN [13] is a classical code to text model
which generates source code summaries with an LSTM network
and attention mechanism. NCS [1] is a recent state-of-art-model on
code summarization. CodeBERT [8] is a widely-used pre-trained
model for source code. We fine tune CodeBERT with the pre-trained
encoder with an additional decoder training from scratch.

4.2.2 Defect detection: For defect detection, we adopt the follow-
ing models. TextCNN [14] and BiLSTM Att [11] are two widely
used methods for text classification in NLP. Here, BiLSTM Att is
the combination of BiLSTM and attention mechanism [44]. Many
defect detection work [16, 31] employ them to construct their model.
Devign [54] is proposed to learn the various vulnerability charac-
teristics with a composite code property graph and graph neural
network. We also use CodeBERT as the basic model since it has
shown promising result on defect detection [19].

4.2.3 Code classification: We adopt three representative work in
this field as the basic model for CARBON. TBCNN [21] is a classical
code classification model which captures structural information
of the Abstract Syntax Tree (AST) with a tree-based convolution
neural network. ASTNN [54] learns the code representation by
splitting the large AST into a sequence of small statement trees. We
also involve the pre-trained model CodeBERT for evaluation.

4.3 Datasets and Metrics
Function naming: For function naming, we use the widely
4.3.1
used CodeSearchNet (CSN) [12] dataset which contains six pro-
gramming languages including Java, Python, Go, PHP, JavaScript
and Ruby. Specifically, we use the cleaned dataset which is pre-
processed and open sourced in CodeBERT [8]. For JavaScript, we
further filter the samples without a function name. As for the met-
ric, we evaluate the results by Precision, Recall and F1, which are
used to measure the similarity between generated function name
and the reference name.

CARBON: A Counterfactual Reasoning based Framework for Neural Code Comprehension Debiasing

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 2: Experimental results (F1 score) on function naming. Percentages listed within parantheses are computed improvement
or reduction in F1 score as compared with the results of the corresponding basic model.

Approach

Java

Python

JavaScript

PHP
Transformed test set

Go

Ruby

Average

23.05

CodeNN
+CARBON 28.35(â†‘22.99%)
NCS
+CARBON 27.28(â†‘26.24%)
CodeBERT 25.90
+CARBON 36.72(â†‘41.78%)

21.61

33.18

CodeNN
+CARBON 33.69(â†‘1.54%)
NCS
+CARBON 35.63(â†‘1.59%)
CodeBERT 46.38
+CARBON 47.04(â†‘1.42%)

35.07

4.00
5.44(â†‘36.00%)
3.65
4.67(â†‘27.95%)
5.05
7.29(â†‘44.36%)

4.72
6.00(â†‘27.12%)
3.32
7.20(â†‘116.87%)
4.61
10.54(â†‘128.63%)

17.61
22.44(â†‘27.43%)
17.56
24.38(â†‘38.84%)
23.43
35.80(â†‘52.80%)
Original test set

17.79
24.91(â†‘40.02%)
21.68
27.33(â†‘26.06%)
27.72
38.62(â†‘39.32%)

5.98
7.25(â†‘21.24%)
5.70
10.47(â†‘83.68%)
20.72
27.49(â†‘32.67%)

12.19
15.73(â†‘29.04%)
12.25
16.89(â†‘37.88%)
17.91
26.08(â†‘45.62%)

22.64
22.69(â†‘0.22%)
25.68
25.40(â†“1.09%)
37.64
37.66(â†‘0.05%)

14.40
14.16(â†“1.67%)
16.31
16.46(â†‘0.92%)
29.55
27.11(â†“8.26%)

36.83
36.87(â†‘0.11%)
40.19
40.25(â†‘0.04%)
49.36
50.35(â†‘2.01%)

34.64
34.79(â†‘0.43%)
40.52
40.21(â†“0.77%)
49.88
50.28(â†‘0.80%)

12.24
12.36(â†‘0.98%)
12.93
13.64(â†‘5.5%)
32.04
30.49(â†“4.84%)

25.66
25.76(â†‘0.39%)
28.45
28.60(â†‘0.53%)
40.81
40.49(â†“0.78%)

Table 3: Experimental results (ACC) on defect detection.

Approach
TextCNN
+CARBON
BiLSTM Att
+CARBON
Devign
+CARBON
CodeBERT
+CARBON

Original test set Transformed test set
58.57
59.96(â†‘2.37%)
62.08
62.34(â†‘0.42%)
55.77
56.74(â†‘1.74%)
63.47
64.05(â†‘0.91%)

54.36
55.78(â†‘2.61%)
54.90
56.55(â†‘3.01%)
52.03
53.98(â†‘3.75%)
57.24
62.01(â†‘8.33%)

Table 4: Experimental results (ACC) on code classification.

96.76

Approach Original test set Transformed test set
TBCNN
+CARBON 97.00(â†‘0.24%)
ASTNN
+CARBON 98.18(â†‘0.14%)
CodeBERT 97.96
+CARBON 98.28(â†‘0.33%)

68.45
83.08(â†‘21.37%)
89.43
96.06(â†‘7.41%)
95.76
97.56(â†‘1.88%)

98.04

4.3.2 Defect detection: The defect detection dataset we used is first
released in Devign [54]. It contains 27,318 C code snippet collected
from the QEMU and FFmpeg projects. As for the dataset split, we
use the benchmark open sourced by CodeXGLUE [19], in which
the dataset is split into training set, validation set and test set in a
proportion of 8:1:1. For defect detection, following [38, 54], we use
accuracy (ACC) as the evaluation metric.

4.3.3 Code classification: For code classification, we use the POJ
dataset [21] which contains 52,000 code snippets of C language
with 104 classes. It is collected from Online Judge (OJ) and code
snippets in the same class are used to solve the same programming
problem. We follow ASTNN [54] and split the dataset into training

set, validation set and test set in a proportion of 3:1:1. We follow [21,
50] and use accuracy (ACC) as the evaluation metric.

We list the statistics of the datasets used in our experiments in
Table 1. To explore how naming bias affects model performance
and evaluate the debiasing ability of CARBON, we create an extra
transformed test set for each dataset. Specifically, we follow [4, 51]
and randomly substitute the identifier names in the test set with
another identifier name appeared in the dataset.

4.4 Implementation Details
During experiment, we reproduce each model either directly using
the code released by the author or strictly following the steps de-
scribed in their paper. For a fair comparison, we make sure that the
hyperparameters for model with and without CARBON are same
such as training epochs and learning rate. The value of ğ›¼ in Equ.
(17) is set to be 0.4, 0.5, 0.6, 0.7 or 0.8 for different datasets. The
ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› is set as 10% of total training iterations. We will discuss how
we select parameters for each dataset in Section 5.4.

When applying CARBON to each baseline, we parse the source
code into three sequences (Figure 5) for models that treat source as
plain text such as NCS and CodeBERT for the function naming task.
For models that treat the source code as a tree [50] or graph [54],
we divide the tree or graph into two parts that only contain the
nodes with and without the naming information, respectively. To
extract the identifiers in source code, we first parse it into AST with
tree-sitter, and filter the leaf node according to its type and the type
of its parent. For example, we extract the identifiers for C/C++ by
selecting the leaf node whose type is â€œidentifierâ€ and parentâ€™s type
is not â€œcall_expressionâ€.

All the experiments are conducted on a server with 4 Nvidia
Tesla V100 GPUs which has 32 GB graphic memory. We run each
baseline and CARBON for three times and report the best results.

5 EXPERIMENTAL RESULTS
In this section, we evaluate the performance of CARBON by an-
swering the following research questions:

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

RQ1: Does CARBON improve the robustness of existing mod-

els?

RQ2: Is CARBON beneficial for improving the accuracy of ex-

isting models?

RQ3: What is the impact of multi-task learning and counter-
factual inference on the performance of CARBON?
RQ4: How do different parameter settings affect the perfor-

mance of CARBON?

5.1 RQ1: Evaluation on the Robustness of

CARBON

We evaluate the robustness of CARBON on the transformed datasets
for the three tasks, with results illustrated in Table 2-4, respectively.
Due to the page limit, we only present the F1 scores of different
models for the function naming task in Table 2. The scores for the
precision and recall metrics are presented on the GitHub repository.
Based on the results, we achieve the following observations:

Conventional models severely suffer from naming bias.
By comparing the results of conventional models on the original
test set and transformed test set, we find that identifier renaming
leads to significant performance degradation on the three tasks.
For example, the average performance of the models for function
naming, defect detection, and code classification drop by 55.4%,
8.9% and 12.3%, respectively. The results indicate that conventional
neural code comprehension models are easily affected by naming
bias.

CARBON consistently and significantly improve the model
robustness. As can be seen in Table 2-4, CARBON consistently out-
performs the conventional models on all the task datasets, demon-
strating its capability and generalizability in improving the robust-
ness of neural code comprehension models. Besides, the improve-
ment over the conventional models is substantial, for example, for
function naming CARBON improves the performance of CodeNN,
NCS and CodeEBRT on transformed test set by 29.0%, 37.9%, and
45.6%, respectively; for defect detection and code classification CAR-
BON also improves the performance of CodeBERT on transformed
test set by 8.3% and 1.9%, respectively. The results suggest that the
design in CARBON can eliminate naming bias, enable the models
reliable to identifier renaming.

The robustness improvement on smaller datasets are more
obvious. As shown in Table 2, by analyzing the results on the
datasets with different sizes for the function naming task, we find
that CARBON achieves higher improvement on the smaller datasets
with identifier renaming. For example, CARBON boosts NCS by
116.9% and 83.7% on JavaScript and Ruby, respectively. This may
be attributed to that neural models are prone to overfitting on the
datasets with small sizes, and thus easier to be biased by identifier
names. Our proposed CARBON can render the models less biased
by identifier names.

5.2 RQ2: Performance Evaluation
In this section, we evaluate the accuracy of CARBON on the code
comprehension tasks. From Table 2-4, we observe that CARBON
improves the performance of conventional models in most cases.
Function Naming. For function naming, as shown in Table 2,
we can find that CARBON improves the performance of each basic

model in the vast majority of cases. Specifically, the average im-
provement of CARBON over CodeNN and NCS is 0.4% and 0.5%,
respectively, regarding the F1 score. Although the averaged F1 score
for CodeBERT+CARBON drops slightly, the performance on most
languages still increases. This results show the effectiveness of
CARBON on function naming.

Defect Detection. As shown in Table 3, we can observe that
CARBON improves the accuracy of all the basic models with an
average improvement of 1.4%. Specifically, CodeBERT+CARBON
and Devign+CARBON outperforms their corresponding baselines
by 0.9% and 1.7% respectively, which indicates that CARBON can
facilitate conventional models to capture the patterns of vulnerable
code snippets.

Code Classification. For code classification, as shown in Ta-
ble 4, we can observe consistent improvement of CARBON on
different basic models. For example, although the performance of
ASTNN and CodeBERT are strong enough, i.e., achieving 98.04%
and 97.96% accuracy respectively, CARBON can further boost them
by 0.1% and 0.3%, respectively. This indicates that the debiasing
ability of CARBON is also benefit to comprehend the code func-
tionality.

5.3 RQ3: Ablation Study
We further perform ablation studies to verify the effectiveness of
two key components of CARBON, i.e., multi-task learning and
counterfactual inference. We select CodeBERT as the basic model
as it is used for evaluation on all the tasks. For function naming,
we use the PHP dataset for evaluation.

Multi-task learning: ğ¿ğ‘“ and ğ¿ğ‘¡ are introduced to distinguish
the direct causal effect from total effect. From Table 5, we can observe
that model without them suffer from different degree of loss on
the original test set or transformed test set. Specifically, removing
ğ¿ğ‘“ leads to a significant decrease on transformed test set, with the
decrease rate at 8.4%, 5.9% and 7.9% for function naming, defect
detection and code classification, respectively; while without ğ¿ğ‘¡ ,
the framework performance drops consistently on the original test
set. This indicates that removing ğ¿ğ‘“ prevents the model from fully
capturing the naming bias, which is harmful to model robustness;
while removing ğ¿ğ‘¡ makes the model unable to distinguish T â†’
R and K â†’ R well, resulting in poorly exploiting the beneficial
knowledge brought by K â†’ R. Moreover, removing both ğ¿ğ‘“ and
ğ¿ğ‘¡ leads to worse performance, e.g., a drop of 0.5% and 0.8% on the
original and transformed test set of code classification, respectively.
Counterfactual inference: We validate the effectiveness of
counterfactual inference by setting the ğ›¼ in Equ. (11) to zero. As
shown in the last row in Table 5, without counterfactual infer-
ence, the frameworkâ€™s performance decreases on all tasks except
a slightly improvement on the original test set of code classifica-
tion. Specifically, the robustness on transformed test set drop by
12.7%, 1.1%, and 0.4% on function naming, defect detection and
code classification, respectively. The results show that the debias-
ing by counterfactual inference improves both the robustness and
accuracy of CARBON.

CARBON: A Counterfactual Reasoning based Framework for Neural Code Comprehension Debiasing

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Table 5: Ablation study. Best and second best results are marked in bold and underline respectively.

Approach

CodeBERT
+CARBON
-w/o ğ¿ğ‘“
-w/o ğ¿ğ‘¡
-w/o ğ¿ğ‘¡ and ğ¿ğ‘“
-w/o Counterfactual Inference

Function Naming (F1) Defect Detection (ACC) Code Classification (ACC)
Original Transformed Original
23.43
35.80
32.80
36.68
34.52
31.26

Transformed
57.24
62.01
58.38
61.42
59.08
61.31

Transformed
95.76
97.56
89.90
98.02
96.81
97.18

Original
97.96
98.28
98.12
98.16
97.74
98.36

49.36
50.35
49.67
49.79
48.69
50.16

63.47
64.05
64.20
63.65
63.69
63.87

first increases and achieves its peak, and then descends obviously
with a larger ğ›¼. The optimal ğ›¼ value is around 0.6 for the tasks.
However, for the transformed dataset, the performance increases
monotonically as ğ›¼ grows from 0 to 1. Recall that ğ›¼ in Equ. (17)
is designed to control the degree of debiasing. Since the bias on
the original dataset is not serious, a small ğ›¼ is suitable. However,
on transformed dataset with identifier renaming, the naming bias
misleads the model prediction severely; thus, a large degree of
debiasing helps improve the model robustness. During the experi-
mentation, to balance the performance of CARBON on both original
and transformed datasets, we choose the value of ğ›¼ from the set
{0.4, 0.5, 0.6, 0.7, 0.8}. Specifically, we test the model with ğ›¼ set from
0.4 to 0.8, and select the ğ›¼ with best robustness under the condition
that the performance on the original dataset is not sacrificed too
much.

The parameter ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›. We study the effect of ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, as intro-
duced in Section 3.2.3, by varying it from 0% to 30% of the total
training iterations for the tasks. From figure 6 (b), (d) and (f), we can
observe that involving ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› benefits the performance on both
original and transformed datasets for all the tasks. However, for
function naming, we also observe that when increasing ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› to
30% of total training iterations the F1 score on the original dataset is
lower than that when ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› is set to zero. This may be attributed
to that models need more training epochs to well distinguish T â†’
R and K â†’ R for the complex generation task. In this work, we set
ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› as 10% of the total training iterations due to the relatively
better results on all tasks.

6 DISCUSSION
6.1 Threats to Validity
We identify three main treats to validity of our study:

(1) The selection of code comprehension tasks. In this work,
we select three popular code comprehension tasks to evalu-
ate CARBON, including function naming, defect detection
and code classification. Although CARBON shows superior
performance on the these tasks, other tasks such as code
search and code summarization are also important and not
involved in our experiment. In the future, we will validate
CARBON on more code comprehension tasks.

(2) The selection of basic models. For each task, we select
as least three basic models to validate the effectiveness of
CARBON. The selected basic models are representative for
the corresponding task. To comprehensively evaluate the

(a) ğ›¼ on function naming.

(b) ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› on function naming.

(c) ğ›¼ on defect detection.

(d) ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› on defect detection.

(e) ğ›¼ on code classification.

(f) ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› on code classification.

Figure 6: Parameter analysis on ğ›¼ and ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›. The left and
right vertical axes indicate results on the original and trans-
formed dataset, respectively. This figure is better viewed
with color.

5.4 RQ4: Parameter Analysis
In this section, we analyze how the two key hyper-parameters ğ›¼
and ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› affect the performance of CARBON. Specifically, we
illustrate the experimentation with CodeBERT on the Go dataset,
TextCNN and ASTNN as basic models for function naming, defect
detection, and code classification, respectively.

The parameter ğ›¼. As shown in Figure 6 (a), (c) and (e), the
model performance shows similar trend along with the increase of
ğ›¼ on the original dataset for all the tasks. CARBONâ€™ performance

323538414449.0 49.4 49.8 50.2 50.6 00.20.40.60.81OriginalTransformed37.5 38.0 38.5 39.0 39.5 49.2 49.5 49.8 50.1 50.4 0%10%20%30%OriginalTransformed53.2 54.0 54.8 55.6 56.4 58.8 59.1 59.4 59.7 60.0 00.20.40.60.8155.2 55.4 55.6 55.8 56.0 59.2 59.4 59.6 59.8 60.0 0%10%20%30%95.0 95.4 95.8 96.2 96.6 98.00 98.05 98.10 98.15 98.20 00.20.40.60.8195.8 96.1 96.4 96.7 97.0 97.6 97.8 98.0 98.2 98.4 0%10%20%30%Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

performance of CARBON, more basic models should be con-
sidered. In the future, we will experiment with more basic
models to evaluate the generality of CARBON.

(3) The selection of datasets. For each task, we select one
popular dataset for evaluation. However, there are other
datasets like [2] for function naming. In the future, we will
conduct experiments on more datasets to evaluate CARBON.

7 RELATED WORK
7.1 Code Comprehension
In this section, we focus on deep-learning-based methods on three
tasks that are covered in our work including function naming,
defect detection and code classification. Besides, the related work
on pre-trained models for code are also discussed.

Function Naming: Alon et al. [3] present Code2seq that rep-
resents the code snippets by sampling certain paths from the ASTs.
Another work proposed by ZÃ¼gner et al. [56] focuses on multilin-
gual code summarization and proposes to build upon language-
agnostic features such as source code and AST-based features. A
recent work [27] propose to encodes tree paths into transformer.
Defect Detection: Russell et al. [31] empirically evaluate the
ML techniques on defect detection and find that TextCNN with an
ensemble tree algorithm achieves the best performance. Another
work [16] proposes the first deep learning-based vulnerability de-
tection system VulDeePecker. Devign [54] is proposed to learn the
various vulnerability characteristics with a composite code property
graph and graph neural network.

Code classification: TBCNN [21] is a classical code classifica-
tion model which captures structural information of the AST with
a tree-based convolutional neural network. ASTNN [54] learns the
code representation by splitting the large AST into a sequence of
small statement trees. Another recent work [5] propose to capture
the tree structure of code with a capsule network.

Pre-trained models for code: Recently, a number of pre-trained
models for source code have been proposed [8, 9, 38]. CodeBERT [8]
is an encoder-only pre-trained model based on Masked language
modeling and replaced token detection. GraphCodeBERT [9] fur-
ther leverage code structure information by data flow graph. An-
other recent work [23] proposes a sequence-to-sequence pre-trained
model with the encoder-decoder architecture.

7.2 Causal Inference
Causal inference has attracted increasing attention in fields includ-
ing computer vision [24, 36], natural language processing [28, 39]
and recommendation [37, 41]. The general purpose of causal infer-
ence is to help model pursue causal effect rather than correlation
effect. In the work [24], the authors propose a counterfactual frame-
work to remove the language bias in visual question answering.
In recommendation, [37] and [41] also employ similar methods to
eliminate the popularity bias. Some works [42, 52] also consider to
build the causal graph from data generation view and remove the
confounder with back-door adjustment. Different from the above
studies, we are devoted to extracting and eliminating the code-
specific bias in neural code models. To the best of our knowledge,
we are the first to introduce the causal inference into code intelli-
gence.

7.3 Adversarial Robustness of Code
Recently, a series of work attempt to improve the robustness of
the neural models for source code. [30, 43, 45, 49]. Yefet et al. [45]
propose discrete adversarial manipulation algorithm to mine per-
turbations for code snippets. Authors in [49] introduce metropolis-
hastings modifier to generate adversarial samples for classification
models. In the work [30], authors employ sequences of paramet-
ric while semantic-preserving program transformation operations
as an adversary. Another recent work [43] propose to generate
adversarial examples that are more natural to human judge. The
performance of the studies highly relies on the quantity and quality
of generated samples. In this work, to alleviate the burden of data
generation, we focus on designing a novel and general model to
eliminate the naming bias.

8 CONCLUSION AND FUTURE WORK
In this paper, we present CARBON, a counterfactual reasoning
based framework to eliminate the naming bias in neural code com-
prehension. CARBON captures the naming bias in the training
stage through multi-task learning and reduces it by counterfactual
inference in the inference stage. CARBON is flexible and easy to
be applied to various tasks and basic models. The evaluation on
three popular tasks demonstrates the effectiveness of CARBON on
original test sets and its robustness to identifier renaming. In the
future, we will explore to apply more causal inference techniques
to solve the challenges in other code intelligence tasks.

REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020. Association for Computational Linguistics, 4998â€“5007.

[2] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
Attention Network for Extreme Summarization of Source Code. In Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016 (JMLR
Workshop and Conference Proceedings, Vol. 48). JMLR.org, 2091â€“2100.

[3] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Gener-
ating Sequences from Structured Representations of Code. In 7th International
Conference on Learning Representations, ICLR 2019. OpenReview.net.

[4] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. In SIGIR â€™21: The 44th International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM, 511â€“521.

[5] Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. TreeCaps: Tree-Based Cap-
sule Networks for Source Code Processing. In Thirty-Fifth AAAI Conference on
Artificial Intelligence, AAAI 2021. AAAI Press, 30â€“38.

[6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos ArÃ©chiga, and Tengyu Ma. 2019.
Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss. In
Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019. 1565â€“1576.

[7] Nadezhda Chirkova and Sergey Troshin. 2021. Empirical study of transformers
for source code. In ESEC/FSE â€™21: 29th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. ACM,
703â€“715.

[8] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
the Association for Computational Linguistics: EMNLP 2020 (Findings of ACL,
Vol. EMNLP 2020). Association for Computational Linguistics, 1536â€“1547.
[9] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In 9th International Conference on Learning Representations, ICLR 2021.
OpenReview.net.

[10] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowl-

edge in a Neural Network. CoRR abs/1503.02531 (2015).

CARBON: A Counterfactual Reasoning based Framework for Neural Code Comprehension Debiasing

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

[11] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural

OpenReview.net.

computation 9, 8 (1997), 1735â€“1780.

[12] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019).

[13] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics, ACL 2016.
The Association for Computer Linguistics.

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2014. ACL, 1746â€“1751.

[15] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. A Context-based Automated
Approach for Method Name Consistency Checking and Suggestion. In 43rd
IEEE/ACM International Conference on Software Engineering, ICSE 2021. IEEE,
574â€“586.

[16] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-Based System for
Vulnerability Detection. In 25th Annual Network and Distributed System Security
Symposium, NDSS 2018. The Internet Society.

[17] Fang Liu, Ge Li, Zhiyi Fu, Shuai Lu, Yiyang Hao, and Zhi Jin. 2022. Learning to
Recommend Method Names with Global Context. CoRR abs/2201.10705 (2022).
[18] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based Pre-
trained Language Model for Code Completion. In 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2020. IEEE, 473â€“485.
[19] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. CoRR abs/2102.04664 (2021).

[20] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,
Andreas Veit, and Sanjiv Kumar. 2021. Long-tail learning via logit adjustment.
In 9th International Conference on Learning Representations, ICLR 2021. OpenRe-
view.net.

[21] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural
Networks over Tree Structures for Programming Language Processing. In Pro-
ceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press,
1287â€“1293.

[22] Son Nguyen, Hung Phan, Trinh Le, and Tien N. Nguyen. 2020. Suggesting
natural method names to check name consistencies. In ICSE â€™20: 42nd International
Conference on Software Engineering. ACM, 1372â€“1384.

[23] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo.
2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code
Representations. CoRR abs/2201.01549 (2022).

[24] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-
Rong Wen. 2021. Counterfactual VQA: A Cause-Effect Look at Language Bias. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021. Computer
Vision Foundation / IEEE, 12700â€“12710.

[25] Judea Pearl. 1998. Graphs, causality, and structural equation models. Sociological

Methods & Research 27, 2 (1998), 226â€“284.

[26] Judea Pearl. 2009. Causality. Cambridge university press.
[27] Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. 2021. Integrating Tree
Path in Transformer for Code Representation. Advances in Neural Information
Processing Systems 34 (2021).

[28] Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and Pengjun Xie. 2021. Coun-
terfactual Inference for Text Classification Debiasing. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021.
Association for Computational Linguistics, 5434â€“5445.

[29] Md. Rafiqul Islam Rabin, Nghi D. Q. Bui, Ke Wang, Yijun Yu, Lingxiao Jiang, and
Mohammad Amin Alipour. 2021. On the generalizability of Neural Program
Models with respect to semantic-preserving program transformations. Inf. Softw.
Technol. 135 (2021), 106552.

[30] Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh
Jha, and Thomas W. Reps. 2020. Semantic Robustness of Models of Source Code.
CoRR abs/2002.03043 (2020).

[31] Rebecca L. Russell, Louis Y. Kim, Lei H. Hamilton, Tomo Lazovich, Jacob Harer,
Onur Ozdemir, Paul M. Ellingwood, and Marc W. McConley. 2018. Automated
Vulnerability Detection in Source Code Using Deep Representation Learning. In
17th IEEE International Conference on Machine Learning and Applications, ICMLA
2018. IEEE, 757â€“762.

[32] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Im-
proving Code Search with Co-Attentive Representation Learning. In ICPC â€™20:
28th International Conference on Program Comprehension. ACM, 196â€“207.
[33] Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh,
and Patrick D. McDaniel. 2018. Ensemble Adversarial Training: Attacks and
Defenses. In 6th International Conference on Learning Representations, ICLR 2018.

[34] Tyler J VanderWeele. 2013. A three-way decomposition of a total effect into
direct, indirect, and interactive effects. Epidemiology (Cambridge, Mass.) 24, 2
(2013), 224.

[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017. 5998â€“6008.

[36] Tan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang. 2021. Causal Atten-
tion for Unbiased Visual Recognition. In 2021 IEEE/CVF International Conference
on Computer Vision, ICCV 2021. IEEE, 3071â€“3080.

[37] Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua.
2021. Clicks can be Cheating: Counterfactual Recommendation for Mitigating
Clickbait Issue. In SIGIR â€™21: The 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval. ACM, 1288â€“1297.

[38] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5:
Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Under-
standing and Generation. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2021. Association for Computa-
tional Linguistics, 8696â€“8708.

[39] Zhao Wang and Aron Culotta. 2021. Robustness to Spurious Correlations in Text
Classification via Automatically Generated Counterfactuals. In Thirty-Fifth AAAI
Conference on Artificial Intelligence, AAAI 2021. AAAI Press, 14024â€“14031.
[40] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine:
Exemplar-based Neural Comment Generation. In 35th IEEE/ACM International
Conference on Automated Software Engineering, ASE 2020. IEEE, 349â€“360.
[41] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.
2021. Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias
in Recommender System. In KDD â€™21: The 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. ACM, 1791â€“1800.

[42] Xun Yang, Fuli Feng, Wei Ji, Meng Wang, and Tat-Seng Chua. 2021. Decon-
founded Video Moment Retrieval with Causal Intervention. In SIGIR â€™21: The 44th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. ACM, 1â€“10.

[43] Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022. Natural Attack for Pre-trained

Models of Code. CoRR abs/2201.08698 (2022).

[44] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and
Eduard H. Hovy. 2016. Hierarchical Attention Networks for Document Classifi-
cation. In NAACL HLT 2016, The 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies.
The Association for Computational Linguistics, 1480â€“1489.

[45] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of

code. Proc. ACM Program. Lang. 4, OOPSLA (2020), 162:1â€“162:30.

[46] Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. 2020. Interven-
tional Few-Shot Learning. In Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020.

[47] Huan Zhang, Hongge Chen, Zhao Song, Duane S. Boning, Inderjit S. Dhillon, and
Cho-Jui Hsieh. 2019. The Limitations of Adversarial Training and the Blind-Spot
Attack. In 7th International Conference on Learning Representations, ICLR 2019.
OpenReview.net.

[48] Huangzhao Zhang, Zhiyi Fu, Ge Li, Lei Ma, Zhehao Zhao, Huaâ€™an Yang, Yizhe Sun,
Yang Liu, and Zhi Jin. 2022. Towards Robustness of Deep Program Processing
Modelsâ€“Detection, Estimation and Enhancement. ACM Transactions on Software
Engineering and Methodology (2022).

[49] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating
Adversarial Examples for Holding Robustness of Source Code Processing Models.
In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020. AAAI
Press, 1169â€“1176.

[50] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. 2019. A novel neural source code representation based on abstract syntax
tree. In Proceedings of the 41st International Conference on Software Engineering,
ICSE 2019. IEEE / ACM, 783â€“794.

[51] Weiwei Zhang, Shengjian Guo, Hongyu Zhang, Yulei Sui, Yinxing Xue, and Yun
Xu. 2021. Challenging Machine Learning-based Clone Detectors via Semantic-
preserving Code Transformations. CoRR abs/2111.10793 (2021).

[52] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling,
and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias
in Recommendation. In SIGIR â€™21: The 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval. ACM, 11â€“20.

[53] Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo
Han, Bernhard SchÃ¶lkopf, and Kun Zhang. 2021. Adversarial Robustness through
the Lens of Causality. CoRR abs/2106.06196 (2021).

[54] Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks. In Advances in Neural Information Pro-
cessing Systems 32: Annual Conference on Neural Information Processing Systems
2019, NeurIPS 2019. 10197â€“10207.

Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY

Gao, et al.

[55] Deqing Zou, Yawei Zhu, Shouhuai Xu, Zhen Li, Hai Jin, and Hengkai Ye. 2021.
Interpreting Deep Learning-based Vulnerability Detector Predictions Based on
Heuristic Searching. ACM Trans. Softw. Eng. Methodol. 30, 2 (2021), 23:1â€“23:31.

[56] Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of Source Code
from Structure and Context. In 9th International Conference on Learning Repre-
sentations, ICLR 2021. OpenReview.net.

