Federated Random Reshufﬂing with Compression and Variance Reduction

Grigory Malinovsky 1 Peter Richt´arik 1

Abstract

Random Reshufﬂing (RR), which is a variant of
Stochastic Gradient Descent (SGD) employing
sampling without replacement, is an immensely
popular method for training supervised machine
learning models via empirical risk minimization.
Due to its superior practical performance, it is
embedded and often set as default in standard
machine learning software. Under the name Fe-
dRR, this method was recently shown to be ap-
plicable to federated learning (Mishchenko et al.,
2021), with superior performance when compared
to common baselines such as Local SGD. Inspired
by this development, we design three new algo-
rithms to improve FedRR further: compressed
FedRR and two variance reduced extensions: one
for taming the variance coming from shufﬂing
and the other for taming the variance due to com-
pression. The variance reduction mechanism for
compression allows us to eliminate dependence
on the compression parameter, and applying ad-
ditional controlled linear perturbations for Ran-
dom Reshufﬂing, introduced by Malinovsky et al.
(2021) helps to eliminate variance at the optimum.
We provide the ﬁrst analysis of compressed lo-
cal methods under standard assumptions without
bounded gradient assumptions and for heteroge-
neous data, overcoming the limitations of the com-
pression operator. We corroborate our theoretical
results with experiments on synthetic and real data
sets.

2
2
0
2

y
a
M
0
1

]

G
L
.
s
c
[

2
v
4
1
9
3
0
.
5
0
2
2
:
v
i
X
r
a

1. Introduction

The primary approach for training supervised machine learn-
ing models in the modern machine learning world is Em-
pirical Risk Minimization. While the ultimate goal of su-
pervised learning is to train models that generalize well to
unseen data, in practice, only a ﬁnite data set is available

1King Abdullah University of Science and Technology
KAUST. Correspondence to: Grigory Malinovsky <grig-
orii.malinovskii@kaust.edu.sa>.

during training. ERM formulation leads to the following
ﬁnite-sum optimization problem:

(cid:34)

min
x∈Rd

f (x) =

(cid:35)

gm(x)

,

1
M

M
(cid:88)

m=1

(1)

where each function we also have ﬁnite-sum structure

gm =

1
n

n
(cid:88)

i=1

fm,i(x).

Big machine learning models are typically trained in a dis-
tributed setting. The training data is distributed across
several workers, which compute local updates and then
communicate them to the server. We are particularly inter-
ested in the Federated Learning setting. Federated Learn-
ing (Koneˇcn`y et al., 2016) is a subarea of distributed ma-
chine learning, where the number of devices n is enormous.
Usually, millions of local devices are heterogeneous to local
data and computational and memory resources. Also, users
want to keep their privacy, so the algorithm should do train-
ing locally. Moreover, communication between workers
should be conducted via a trusted aggregation server, which
is very expensive.

Communication as the bottleneck. In literature, we have
two strategies to overcome communication issues in fed-
erated learning. The ﬁrst one is communication compres-
sion, where our goal is to reduce the number of communi-
cated bits using gradient compression scheme (Mishchenko
et al., 2019; Gorbunov et al., 2020) and compressed iterates
(Khaled & Richt´arik, 2019; Chraibi et al., 2019). There are
many compression techniques such as quantization (Alis-
tarh et al., 2017; Bernstein et al., 2018; Ramezani-Kebrya
et al., 2019), sparsiﬁcation (Aji & Heaﬁeld, 2017; Lin et al.,
2017; Wangni et al., 2017; Alistarh et al., 2018) and other ap-
proaches (Shamir et al., 2014; Vogels et al., 2019; Wu et al.,
2018). The second strategy to tackle this issue is increas-
ing the number of local steps between the communication
rounds. The most popular algorithm — FedAvg (McMahan
et al., 2017)— is based on this idea. Many papers provide
theoretical justiﬁcations for special cases of FedAvg such
as local GD (Khaled et al., 2019) and local SGD (Khaled
et al., 2020; Gorbunov et al., 2020; Stich, 2018; Lin et al.,
2018). The natural union of communication compression
and local computations is presented in Basu et al. (2020);

 
 
 
 
 
 
Federated Random Reshufﬂing with Compression and Variance Reduction

Haddadpour et al. (2021). However, the theory provided in
these papers is limited due to unrealistic assumptions.

Sampling without replacement. Stochastic ﬁrst-order al-
gorithms, in particular, have attracted much attention in the
machine learning world. Of these, stochastic gradient de-
scent (SGD) is perhaps the best known and the most basic.
SGD has a long history (Robbins & Monro, 1951) and is
therefore well-studied and well-understood (Gower et al.,
2019). However, methods based on data permutations (Bot-
tou, 2009), when data points are shufﬂed randomly and
processed in order, without replacement, show better perfor-
mance than SGD (Recht & R´e, 2013). Also, this method has
software implementation advantages since these methods
are friendly to cache locality (Bengio, 2012). The most
popular model in this class is Random Reshufﬂing (Recht
& R´e, 2012). Shufﬂe-Once (Safran & Shamir, 2020) uses a
similar approach, but shufﬂing occurs only once, at the very
beginning, before the training begins. Random Reshufﬂing
and Shufﬂe-Once have a long history, and many theoreti-
cal works try to show the advantages of Random Reshuf-
ﬂing (G¨urb¨uzbalaban et al., 2019; Haochen & Sra, 2019;
Nagaraj et al., 2019) and Shufﬂe Once (Rajput et al., 2020).

Federated Random Reshufﬂing. Recent advances of
Mishchenko et al. (2020) and providing extension for Feder-
ated Learning (Mishchenko et al., 2021) allow us to consider
this technique as a particular variant of FedAvg with a ﬁxed
number of local computations and sampling without replace-
ment.

Variance Reduction. Compression operators help to re-
duce the number of transmitted bits. However, at the same
time, it starts to be a source of variance, which increases
the neighborhood of the optimal solution. This variance can
sufﬁciently slow down the algorithm. In order to overcome
this challenge, we need to use a variance reduction mecha-
nism. The idea of this approach is based on shifted compres-
sion operator, and ﬁrstly it was proposed for compressed
gradients (Mishchenko et al., 2019). For compressed it-
erates, the variance reduction mechanism was proposed
in Chraibi et al. (2019). Moreover, stochastic ﬁrst-order
methods become a source of variance due to their random
nature. Hopefully, another variance reduction mechanism
can help with this type of variance. There are many variance-
reduced methods which use sampling with replacement such
as SVRG (Johnson & Zhang, 2013), L-SVRG (Kovalev
et al., 2020),SAGA (Defazio et al., 2014a),SAG (Roux et al.,
2012),Finito(Defazio et al., 2014b) etc. For permutation-
based algorithms, we have only a few variance-reduced
methods (Ying et al., 2019; Park & Ryu, 2020; Mokhtari
et al., 2018). A recent paper of Malinovsky et al. (2021)
introduced linear perturbation reformulation that allows get-
ting better rates for variance reduced Random Reshufﬂing.

2. Contributions

This section outlines our work’s key contributions and of-
fers explanations and clariﬁcations regarding some of the
development.

Compressed FedRR. We propose the ﬁrst method, which
combines three ideas: compression, local steps, and sam-
pling without replacement. This method is compressed
federated random reshufﬂing. The basic approach is to ap-
ply the compression operator to the iterates after each epoch
and then aggregate compressed updates. Applying compres-
sion to the iterates can signiﬁcantly worsen convergence
properties. We prove the following rate:

E(cid:107)xT − x∗(cid:107)2 ≤ (1 − γµ)

nT

2 E(cid:107)x0 − x∗(cid:107)2

+

2ω
M

1
γµ

1
M

M
(cid:88)

(cid:107)xn

∗,m(cid:107)2

m=1
(cid:19)

(cid:18)

1 +

+

2
µ

2ω
M

γ2L

1
M

M
(cid:88)

(cid:16)

m=1

(cid:107)∇Fm(x∗)(cid:107)2 +

(cid:17)

σ2
∗,m

n
4

M

(cid:80)M

(cid:80)M

1
γµ
It

(cid:1) γ2L 1

(cid:0)1 + 2ω

m=1 (cid:107)xn

1
M
is equal

As we can see, we have a part with linear rate: (1 −
γµ) nT
2 E(cid:107)x0−x∗(cid:107)2. Also there are three sources of variance
in the optimum. The ﬁrst one is 2ω
∗,m(cid:107)2
M
to zero
and it caused by compression.
only if ω = 0.
This term cannot be elimated by
decreasing step-sizes strategies.
The second term is
2
4 σ2
∗,m. This source of variance
µ
is caused by stochasticity of Random Reshufﬂing method.
This variance can be decreased by decreasing step-sizes.
The third term is 2
m=1 (cid:107)∇Fm(x∗)(cid:107)2.
µ
This source of variance is caused by heterogeneity of data.
In other words, if we have the same optimum for all func-
tions Fm(x), we can get rid of this term. In heterogenious
regime we need to use additional mechanism for controlling
client drift such as SCAFFOLD (Karimireddy et al., 2019).

(cid:0)1 + 2ω

(cid:1) γ2L 1

(cid:80)M

m=1

M

M

M

n

Variance-reduced Compressed FedRR. In the previous
section, it was shown that using compressed iterates causes
additional variance, which cannot be vanished by decreasing
step-sizes strategies. Moreover, it forces us to have an ad-
ditional assumption on the compression operator. We need
to have very small compression parameter ω ≤ M γµε
∗,m(cid:107)2 .
In order to ﬁx it, we propose Variance-reduced compressed
FedRR (FedCRR-VR) that utilizes shifted compressed up-
dates with learning shifts. The similar mechanism is used in
Mishchenko et al. (2019) and Gorbunov et al. (2020).

2
M (cid:107)xn

Double Variance-reduced Compressed FedRR. We pro-
pose a modiﬁcation of Variance-reduced Compressed Fed-
erated Random Reshufﬂing, which allows eliminating vari-
ance caused by stochasticity. We armed Variance-reduced
Compressed FedRR with the linear permutation approach

Federated Random Reshufﬂing with Compression and Variance Reduction

proposed by Malinovsky et al. (2021). Now we get both
variance reduction mechanisms in one algorithm.

3. Preliminaries

3.1. L-smooth and µ-strongly convex functions

Before introducing our convergence results, let us ﬁrst for-
mulate all concepts that we use throughout the paper. Firstly,
we consider a class of mu-strongly convex and L-smooth
functions.

Deﬁnition 1. A differentiable function f is µ-strongly con-
vex if

f (y) ≥ f (x) + ∇f (x)T (y − x) +

µ
2

(cid:107)y − x(cid:107)2

for µ > 0 and all x, y.

Deﬁnition 2. A differentiable function f is L-smooth if

f (y) ≤ f (x) + ∇f (x)T (y − x) +

L
2

(cid:107)y − x(cid:107)2

for some L > 0 and all x, y.

There is the ﬁrst assumption that we use in all theorems.

Assumption 1. Each fm,i is µ-strongly convex and L-
smooth.

We also need to deﬁne Bregman divergence, which is often
used in the analysis.

Deﬁnition 3. The Bregman divergence with respect to f is
the mapping Df : Rd × Rd → R deﬁned as follows:

Df (x, y)

def
= f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105).

3.2. Compression operator

In order to overcome communication issues, we apply a
compression operator to the iterates. Now we are going to
extend Federated Random Reshufﬂing using compression.
Let us deﬁne the concept of compressors.
Deﬁnition 4. We say that a randomized map C : Rd → Rd
is in class Bd(ω) if there exists a constant ω ≥ 0 such that
the following relations hold for all x ∈ Rd:

E [C(x)] = x, E (cid:2)(cid:107)C(x)(cid:107)2(cid:3) ≤ (ω + 1)(cid:107)x(cid:107)2.

Assumption 2. All compression operators are in class
Bd(ω).

This class of compressor operators is classical in litera-
ture (Mishchenko et al., 2019; Horvath et al., 2019; Basu
et al., 2020).

3.3. Random Reshufﬂing, Shufﬂe Once

In order to conduct analysis for sampling without replace-
ment we need to establish speciﬁc notions. We sam-
ple a random permutation {π0, π1, . . . , πn−1} of the set
{1, 2, . . . , n}, and proceed with n iterates of the form
(cid:1) at each machine locally.
(cid:0)xi
xi+1
t,m = xi
t,m
We also consider option when we have only one random
permutation, at the very beginning, and then algorithm uses
this permutation during the whole process.

t,m − γ∇fm,πi

For a constant stepsize and a ﬁxed permutation, we deﬁne
intermediate limit point:

xi
∗

def= x∗ − γ

i−1
(cid:88)

j=0

∇fπj (x∗) ,

i = 1, . . . , n − 1.

To measure the closeness between x∗ and xn
tion from Mishchenko et al. (2021) of Shufﬂing radius.
Deﬁnition 5. For given a stepsize γ > 0 and a random
permutation π of {1, 2, . . . , n} shufﬂing radius is deﬁned by

∗ we use deﬁni-

σ2
rad

def
= max

i=1,...,n−1

(cid:20) 1
γ2

Eπ

(cid:2)Dfπi

(cid:0)xi

∗, x∗

(cid:1)(cid:3)

(cid:21)

.

We also need to deﬁne the most popular parameter for
method’s stochastisity.
Deﬁnition 6. Variance at the optimum:

σ2
∗

def
=

1
n

n
(cid:88)

i=1

(cid:107)∇fi (x∗) − ∇f (x∗)(cid:107)2 .

The shufﬂing radius for permutation-based algorithms is
natural, and it is more convenient to work with this concept.
However, we need to have an upper bound in terms of σ2
∗
to compare different methods. To get an upper bound for
shufﬂing radius, we need to use a lemma in Mishchenko
et al. (2020) that bounds variance of sampling without re-
placement.
Theorem 1. For any stepsize γ > 0 and any random per-
mutation π of {1, 2 . . . , n} we have

σ2
rad ≤

(cid:18)

Lmax
2

n

n (cid:107)∇f (x∗)(cid:107)2 +

(cid:19)

.

1
2

σ2
∗

In case when we have only one node we obtain that
(cid:107)∇f (x∗)(cid:107)2 = 0. However, in multuple node case we will
need this term.

3.4. Lifted problem reformulation

Let us consider a bigger product space by introducing
dummy variables and the constraint x1 = x2 = . . . = xM .
We need to deﬁne regularizer for this reformulation:
(cid:26) 0,

x1 = · · · = xM
otherwise .

+∞,

ψ (x1, . . . , xM ) =

Federated Random Reshufﬂing with Compression and Variance Reduction

Table 1. Comparison of the main features of our algorithms and results with other methods with compression.

Feature

Variance
reduction for
compression

Variance
reduction for
stochasticity

Variance
reduction for
client-drift

RR(1)

No additional
assumption

Communication complexity(2)

QSPARSE
et al., 2020)

(Basu

FedCOMGATE
(Haddadpour et al.,
2021)

DIANA(Mishchenko
et al., 2019)

FedCRR

FedCRR-VR

FedCRR-VR-2

(cid:55)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)(3)

(cid:55)(4)

(cid:51)

(cid:55)(5)

(cid:51)

(cid:55)(7)

O

(cid:16) κ(ω+1)
√

ε

(cid:17)

˜O

(cid:16)

κ( ω

(cid:17)
M + 1)

O

˜O

˜O

˜O












(cid:18)

κ max

(cid:26)

V 0,

(1−αp)σ2
µLnαp

(cid:27)

(cid:19)

1
ε

(cid:19)(cid:19)

(6)

(cid:18)(cid:18)

κ +

√

κ(σcl+σst)
√
µ
ε
(cid:16)
(cid:17)n
1− 1
κ
(cid:17)n(cid:17)2 +

(ω+1)
(cid:16)

1−

(cid:16)
1− 1
κ

√

κ(σcl+σst)
√
ε

µ





(cid:18)

(ω+1)

1− 1
√
κ

(cid:32)

(cid:18)

1−

1− 1
√
κ

κn

(cid:19) n
2
(cid:33)2 +

κn
(cid:19) n
2

√

κ(σcl )
√
ε
µ








(1) Sampling without replacement (Random Reshufﬂing).
(2) ˜O notation ignores logarithmic factors.
(3) Bounded second moment: E (cid:2)(cid:107)∇fi (x)(cid:107)2
γ ∈ (0, 1].
(4) Assumption that for all x1, . . . , xn ∈ Rd the compressor C satisﬁes E
≤ G2.
Classical compression operators like RandK and l2-quantization on Rd does not satisfy this condition. As counterexample we can set n = 2
and x2 = −x1 = t · (1, 1, . . . , 1)(cid:62) with arbitrary large t > 0.
(5) Small compression operator: ω ≤ M γµε

1. Contractive compressor assumption: E (cid:2)(cid:107)x − C(x)(cid:107)2

(cid:3) ≤ (1 − γ)(cid:107)x(cid:107)2

i=1 C (xj )(cid:13)
(cid:13)

(cid:3) ≤ G2

(cid:13)C (cid:0) 1

(cid:104)(cid:13)
(cid:13) 1
n

2 − (cid:13)

i=1 xj

2 for

(cid:1)(cid:13)
(cid:13)

(cid:80)n

(cid:80)n

2(cid:105)

n

2

2

(cid:107)xn

∗,m (cid:107)2 .

2
M

(6) Client drift: σcl = 1
M

(7) Big data regime: n > log

(cid:80)M

m=1 (cid:107)Fm(x∗)(cid:107), sum of local variances: (cid:80)M
(cid:16) 1

(cid:16) 1

(cid:17)(cid:17)−1

(cid:17) (cid:16)

1

1−δ2

log

1−γµ

, where 0 < δ < 1.

√
n
2 σ∗,m

Using this regularizer, we can establish the reformulated
problem:

min
x1,...,xM ∈Rd

1
nM

Fm(x) =

M
(cid:88)

Fm (xm) + ψ (x1, . . . , xM )

m=1
n
(cid:88)

fmj(x).

j=1

We need to have an upper bound for the shufﬂing radius
for this reformulated problem. First, we need to deﬁne the
variance of the method’s stochasticity in distributed case.
Deﬁnition 7. The variance of local gradients:

σ2
m,∗

def
=

1
n

n
(cid:88)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇fmj (x∗) −

1
n

∇Fm (x∗)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

.

Now we need to use a lemma from (Mishchenko et al., 2021)
to bound shufﬂed radius for the reformulated problem:
Lemma 1. The shufﬂing radius σ2
upper bounded by

rad of lifted problem is

σ2
rad ≤ L

M
(cid:88)

(cid:16)

m=1

(cid:107)∇Fm (x∗)(cid:107)2 +

(cid:17)

.

σ2
m,∗

n
4

The second part depends on sum of local gradient norms
(cid:80)M
m=1 (cid:107)∇Fm (x∗)(cid:107)2. Both of these terms appear in analy-

sis of local SGD (Khaled et al., 2020).

4. Compressed Federated Random

Reshufﬂing

In this section, we propose a direct application of com-
pressed iterates to federated Random Reshuﬂling. In this
procedure server distributes the current point to workers,
then each worker computes the full epoch according to its
sampled permutation locally. After that, the ﬁnal iterate
xn
t,m is compressed and transmitted to the server, where all
updates are aggregated by taking the average.

Theorem 2. Suppose that Assumption 2 and Assumption 1
hold. Additionally assume that compression parameter is
sufﬁciently small: ω ≤ M
2
ﬁes γ ≤ 1
(Algorithm 1) satisfy

L , the iterates generated by FedCRR or FedCSO

. If the stepsize satis-

1−(1−γµ)
n
(1−γµ)
2

n
2

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) ≤ (1 − γµ)

nT

2 (cid:107)x0 − x∗(cid:107)2

+

2
µ

γ2Lmax

1
M

M
(cid:88)

m=1

(cid:16)
(cid:107)Fm(x∗)(cid:107)2 +

(cid:17)

σ2
∗,m

n
4

We can see that there are two parts of variance. First
one depends on the sum of local variances (cid:80)M
m,∗.

m=1 σ2

+

2ω
M

1
γµ

1
M

M
(cid:88)

m=1

(cid:107)xn

∗,m(cid:107)2.

Federated Random Reshufﬂing with Compression and Variance Reduction

sample

{1, 2, . . . , n}

permutation
(Only

Rd, number of epochs T
each

m,

2: For

Algorithm 1 Federated Compressed Random Reshufﬂing
(FedCRR) and Shufﬂe-Once (FedCSO)
1: Parameters: Stepsize γ > 0, initial vector x0 = x0

0 ∈

Rd, number of epochs T
each

m,

2: For

π0,m, π1,m, . . . , πn−1,m of
FedCSO)

for m = 1, . . . , M locally in parallel do

3: for epochs t = 0, 1, . . . , T − 1 do
4:
5:
6:

x0
t,m = xt
Sample permutation π0,m, π1,m, . . . , πn−1,m of
{1, 2, . . . , n} (Only FedCRR)
for i = 0, 1, . . . , n − 1 do

t,m − γ∇fπi,m(xi

t,m)

7:
8:
9:
10:
11:
12:
13: end for

xi+1
t,m = xi
end for
qt,m = C(xn

t,m)

end for
xt+1 = 1
M

(cid:80)M

m=1 qt,m;

We can see that the last term makes the largest contribution
to the size of the neighborhood, and decreasing stepsizes
cannot help. Now we establish communication complexity.
Corollary 1. Let the assumptions in the Theorem 2 hold.
Also assume that ω ≤ M γµε
∗,m(cid:107)2 . Then the communication
2
M (cid:107)xn
complexity of Algorithm 1 is

T = ˜O

(cid:16)(cid:16)

κ +

(cid:17)

√
κ
ε ∆
√

µ

log (cid:0) 1

(cid:1)(cid:17)

,

ε
√

where ∆ = 1
M

(cid:80)M

m=1 ((cid:107)∇Fm(x∗)(cid:107) +

nσ∗,m)

Algorithm 2 Variance Reduced Federated Compressed
Random Reshufﬂing (FedCRR-VR) and Shufﬂe-Once
(FedCSO-VR)
1: Parameters: Stepsize γ > 0, initial vector x0 = x0

0 ∈

π0,m, π1,m, . . . , πn−1,m of
FedCSO-VR)

sample

{1, 2, . . . , n}

permutation
(Only

for m = 1, . . . , M locally in parallel do

3: for epochs t = 0, 1, . . . , T − 1 do
4:
5:
6:

x0
t,m = xt
Sample permutation π0,m, π1,m, . . . , πn−1,m of
{1, 2, . . . , n} (Only FedCRR-VR)
for i = 0, 1, . . . , n − 1 do

7:
8:
9:
10:
11:
12:
13:
14: end for

t,m − γ∇fπi,m(xi

t,m)

xi+1
t,m = xi
end for
qt,m = C(xn
t,m − ht,m)
ht+1,m = ht,m + αqt,m

end for
xt+1 = (1 − η)xt + η 1
M

(cid:80)M

m=1 (qt,m + ht,m);

FedCRR-VR or FedCSO-VR (Algorithm 2) satisfy

EΨT ≤

(cid:18)

1 −

min (α, η(1 − (1 − γµ)n))
2

(cid:19)T

Ψ0

(cid:16)

(cid:17)

2

α + η + 2η2ω
γ3Lmax
M
M (α, η(1 − (1 − γµ)n))

+

M
(cid:88)

m=1

δm,

5. Variance Reduced Compressed Federated

Random Reshufﬂing

In this section, we introduce a variance reduction mech-
anism for compression in order to upgrade Algorithm 1.
The main part of the algorithm remains the same. How-
ever, after each epoch, we apply the compression operator
to the difference between local iterates and learning shifts.
After that, at each node, we compute updates of learning
shifts. To control the learning process of shifts, we use
additional parameter α. To get convergence we need to
satisfy α ≤ 1
ω+1 . After that server aggregates updates by
using a convex combination of previous iterate and average
of updates. To control this convex combination, we use
additional parameter η. The next theorem shows that this
mechanism helps to get rid of compression variance and
the additional assumptions. To get the convergence rate, we
introduce the Lyapunov function.
Theorem 3. Suppose that Assumption 1 and Assumption 2
hold. Then provided the stepsize satisﬁes γ ≤ 1
L , α ≤ 1
ω+1
(cid:16)
1, (1−(1−γµ)n)M
the iterates generated by
12ω(1−γµ)n

and η ≤ min

(cid:17)

where Lyapunov function is deﬁned as Ψt = (cid:107)xt −
x∗(cid:107)2 + 4η2ω
and δm =
(cid:16)
(cid:107)∇Fm (x∗)(cid:107)2 + n

(cid:13)
(cid:13)ht,m − xn
(cid:17)

(cid:80)M

(cid:13)
2
(cid:13)

m=1

1
M

∗,m

αM

.

4 σ2

m,∗

Now there is no compression variance term anymore. Next
corollary demonstrates communication complexity.

Corollary 2. Let the assumptions in the Theorem 3 hold.
Then the communication complexity of Algorithm 2 is

T = O

(cid:18)(cid:18) (ω+1)(1− 1
(1−(1− 1

κ )n
κ )n)2 +

(cid:19)

√
κ
ε ∆
√

µ

log (cid:0) 1

ε

(cid:1)

(cid:19)

,

where ∆ = 1
M

(cid:80)M

m=1 ((cid:107)∇Fm(x∗)(cid:107) +

√

nσ∗,m) .

We have the same second term, which depends on the sum
of local gradients and local variances. Also, the linear rate
is slightly worse since we can use any compression oper-
ator, and we also need to learn shifts. However, the main
advantage of this method is the possibility of using any
compression parameter ω.

Federated Random Reshufﬂing with Compression and Variance Reduction

Algorithm 3 Double Variance Reduced Federated Com-
pressed Random Reshufﬂing (FedCRR-VR-2) and Shufﬂe-
Once (FedCSO-VR-2)
1: Parameters: Stepsize γ > 0, initial vector x0 = x0

0 ∈

Rd, number of epochs T
each

m,

2: For

π0,m, π1,m, . . . , πn−1,m of
VR-FedCSO)

sample

{1, 2, . . . , n}

permutation
(Only

for m = 1, . . . , M locally in parallel do

3: for epochs t = 0, 1, . . . , T − 1 do
4:
5:
6:
7:

x0
t,m = xt
yt = xt
Sample permutation π0,m, π1,m, . . . , πn−1,m of
{1, 2, . . . , n} (Only D-VR-FedCRR)
for i = 0, 1, . . . , n − 1 do
(cid:1) = ∇fπi,m

(cid:1) − ∇fπi,m (yt) +

(cid:0)xi

8:
9:

t,m

g (cid:0)xi
t,m, yt
1
n ∇Fm (yt)
xi+1
t,m = xi
end for
qt,m = C(xn
t,m − ht,m)
ht+1,m = ht,m + αqt,m

t,m − γg (cid:0)xi

end for
xt+1 = (1 − η)xt + η 1
M

10:
11:
12:
13:
14:
15:
16: end for

(cid:1)

t,m, yt

2 (cid:0)1 − (1 − γµ) n

γµ) n
VR-2 or FedCSO-VR-2 (Algorithm 3) satisfy

2 (cid:1) , the iterates generated by FedCRR-

EΨT ≤

(cid:18)

1 −

1
2

min (cid:0)α, η(1 − (1 − γµ)

n

2 )(cid:1)

(cid:19)T

Ψ0

(cid:16)

2

+

(cid:17)

α + η + 2η2ω
M

γ3L (cid:80)M
M min (cid:0)α, η(1 − (1 − γµ) n

(cid:0)(cid:107)∇Fm(x∗)(cid:107)2(cid:1)
2 )(cid:1)

m=1

,

where Lyapunov function is deﬁned as Ψt = (cid:107)xt − x∗(cid:107)2 +
(cid:13)
2
4η2ω
(cid:13)
αM

(cid:13)
(cid:13)ht,m − xn

(cid:80)M

m=1

1
M

∗,m

.

We need to use smaller stepsize since we applied variance
reduction mechanism. However, we managed to vanish sum
of local variances. The next theorem shows the communica-
tion complexity of Algorithm 3.

Corollary 3. Let the assumptions in the Theorem 3 hold.
Then the communication complexity of Algorithm 3 is





T = O





(ω+1)
(cid:18)

(cid:16)

1−

(cid:16)

1− 1
√
κ

1− 1
√
κ

κn

(cid:17) n
2
(cid:19)2 +

κn
(cid:17) n
2

√
κ
√

µ

ε ∆(cid:48)


 log (cid:0) 1

ε



(cid:1)

 ,

(cid:80)M

m=1 (qt,m + ht,m);

where ∆(cid:48) = 1
M

(cid:80)M

m=1 ((cid:107)∇Fm(x∗)(cid:107)) .

6. Double Variance Reduced Compressed

Federated Random Reshufﬂing

This section proposes another variance reduction mecha-
nism to eliminate local variances caused by the method’s
stochasticity. To achieve this goal, we need to use inner prod-
uct reformulation introduced by Malinovsky et al. (2021).
We can get an equivalent form of the local function. Let
ai, . . . , an ∈ Rd are vectors that sum to zero (cid:80)n
i=1 ai = 0:

Fm(x) =

n
(cid:88)

i=1

(fi,m + (cid:104)ai,m, x(cid:105)) =

n
(cid:88)

i=1

˜fi,m.

(2)

Let us consider the following gradient estimate:

g (cid:0)xi

t, yt

(cid:1) = ∇fπi,m

(cid:0)xi

t

(cid:1) − ∇fπi,m (yt) +

1
n

∇Fm (yt) .

Obviously, the sum of these vectors is equal to zero:

n
(cid:88)

i=1

ai,m = −

n
(cid:88)

i=1

∇fπi,m (yt) +

1
n

n
(cid:88)

i=1

∇Fm (yt) = 0.

Now we are ready to formulate the theorem of convergence
guarantees.
Theorem 4. Suppose that Assumption 2 and Assumption 1
hold. Then provided the stepsize satisﬁes γ ≤ 1
nL ,
8L
(cid:16)

(cid:112) µ

(cid:17)

1−(1−γµ)

n
2

(cid:19)

M

α ≤ 1

ω+1 , η ≤ min

12ω(1−γµ)

n
2

and 1

8 ≤ (1 −

(cid:18)

1,

7. Experiments

(cid:13)
2
(cid:13)

+ λ

(cid:13)
(cid:13)Am

i,:x − ym
i

Model. In our experiments we solve the regularized ridge
regression problem, which has the form 1 with fim(x) =
2 (cid:107)x(cid:107)2, where Am ∈ Rn×d, ym ∈ Rn
1
2
and λ > 0 is regularization parameter. Consider concate-
nated matrix A ∈ Rmn×d. This problem satisﬁes Assump-
tion 1 for L = maxi (cid:107)Ai,:(cid:107)2 + λ and µ =
+ λ,
where ρmin is the smallest eigenvalue. In our experiments
we set λ = 1
n . In all plots x-axis is the number of commu-
nicated bits, and y-axis is the squared norm of difference
between current iterate and solution.

ρmin(A(cid:62)A)
n

Compression operator. In all experiments we used ran-
dom sparsiﬁcation as compression operator:

C(x) =

d
k

(cid:88)

i∈S

xiei,

where S is a random subset of {1, 2, . . . , d(cid:105) of cardinality k
chosen uniformly at random, and ei is the i-th standard unit
basis vector in Rd.

Hardware and software. We use real datasets from open
LIBSVM corpus (Chang & Lin, 2011) (Modiﬁed BSD
License www.csie.ntu.edu.tw/ cjlin/libsvm/) and synthetic
datasets from scikit-learn.datasets (Pedregosa et al., 2011)
(BSD License https://scikit-learn.org). We implemented all
algorithms in Python. All methods were evaluated on a com-
puter with an Intel(R) Xeon(R) Gold 6146 CPU at 3.20GHz,

Federated Random Reshufﬂing with Compression and Variance Reduction

Figure 1. Comparison of distributed methods and versions of Compressed Federated Random Reshufﬂing: FedCRR, FedCRR-VR,
FedCRR-VR-2 on synthetic datasets with the different data points. In order to have fair competition, we used the same parameter ω for all
methods with compression.

having 24 cores. You can ﬁnd more details and additional
experiments in supplementary materials.

Results. We have a very tight match between our theory and
the numerical results. As we can see, Compressed Federated
Random Reshufﬂing cannot get appropriate accuracy since
we have a huge compression variance term. It means that
this method can be used only if the required accuracy is not
high. However, we can see that variance-reduced methods
show better convergence. While FedRR-VR has the same
linear rate, the solution’s neighborhood is smaller in compar-
ison to other methods. FedRR-VR-2 has a slower linear rate
because of stepsize requirements, but the neighborhood of
the solution is the smallest and allows to get a much better
solution to the problem.

8. Conclusion

In this work, we propose three new algorithms: Compressed
Federated Random Reshufﬂing and two variance-reduced
variants. These methods are ﬁrst-of-its-kind algorithms
that include three popular approaches: periodic aggregation,
compressed updates, and sampling without replacement.
Moreover, we sequentially applied variance reduction mech-
anisms for compression and then for Random Reshufﬂing.
We provide the ﬁrst analysis under general assumptions.
Experimental results conﬁrm our theoretical ﬁndings. Thus,
we gain a deeper theoretical understanding of how these al-

gorithms work and hope that this will inspire researchers to
develop further and analyze methods for Federated learning.
In future work, we desire to get rid of the client drift term in
the neighborhood of the solution and get an algorithm that
will converge linearly to the exact solution. We also want to
allow a method to have partial participation of clients since
it is essential for a Federated Learning setting. We also be-
lieve that our theoretical and practical results can be applied
to other aspects of machine learning and federated learning,
leading to improvements in current and future applications.

References

Aji, A. F. and Heaﬁeld, K.

Sparse communica-
tion for distributed gradient descent. arXiv preprint
arXiv:1704.05021, 2017.

Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic,
M. Qsgd: Communication-efﬁcient sgd via gradient quan-
tization and encoding. Advances in Neural Information
Processing Systems, 30:1709–1720, 2017.

Alistarh, D., Hoeﬂer, T., Johansson, M., Khirirat, S., Kon-
stantinov, N., and Renggli, C. The convergence of sparsi-
ﬁed gradient methods. arXiv preprint arXiv:1809.10505,
2018.

Basu, D., Data, D., Karakus, C., and Diggavi, S. Qsparse-
local-sgd: Distributed sgd with quantization, sparsiﬁca-

012345Communicated bits1e81011109107105103101xkx*2DCGDDIANAlocal_SGDFedCRR-VR-2FedCRR-VRFedCRR0.00.20.40.60.81.01.21.4Communicated bits1e8103102101100|xkx*|2FedCRRFedRRFedCRR-VRFederated Random Reshufﬂing with Compression and Variance Reduction

tion, and local computations. IEEE Journal on Selected
Areas in Information Theory, 1:217–226, 2020.

Bengio, Y. Practical recommendations for gradient-based
training of deep architectures. Neural Networks: Tricks
of the Trade, pp. 437–478, 2012. ISSN 1611-3349. doi:
10.1007/978-3-642-35289-8 26.

Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anand-
kumar, A. signsgd: Compressed optimisation for non-
convex problems. In International Conference on Ma-
chine Learning, pp. 560–569. PMLR, 2018.

Bottou, L. Curiously fast convergence of some stochastic

gradient descent algorithms. 2009.

Chang, C.-C. and Lin, C.-J. Libsvm: a library for sup-
port vector machines. ACM Transactions on Intelligent
Systems and Technology (TIST), 2(3):1–27, 2011.

Chraibi, S., Khaled, A., Kovalev, D., Richt´arik, P., Salim,
A., and Tak´aˇc, M. Distributed ﬁxed point methods with
compressed iterates. arXiv preprint arXiv:1912.09925,
2019.

Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA : A
fast incremental gradient method with support for non-
strongly convex composite objectives. arXiv preprint
arXiv:1407.0202, 2014a.

Defazio, A., Domke, J., and Caetano. Finito: A faster, per-
mutable incremental gradient method for big data prob-
lems. In Xing, E. P. and Jebara, T. (eds.), Proceedings of
the 31st International Conference on Machine Learning,
volume 32 of Proceedings of Machine Learning Research,
pp. 1125–1133, Bejing, China, 22–24 Jun 2014b. PMLR.

Gorbunov, E., Hanzely, F., and Richt´arik, P. A uniﬁed
theory of sgd: Variance reduction, sampling, quantization
and coordinate descent. In International Conference on
Artiﬁcial Intelligence and Statistics, pp. 680–690. PMLR,
2020.

Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A.,
Shulgin, E., and Richt´arik, P. SGD: General analysis
and improved rates. In International Conference on Ma-
chine Learning, pp. 5200–5209. PMLR, 2019.

G¨urb¨uzbalaban, M., Ozdaglar, A., and Parrilo, P. A. Why
random reshufﬂing beats stochastic gradient descent.
Mathematical Programming, pp. 1–36, 2019.

Haddadpour, F., Kamani, M. M., Mokhtari, A., and Mah-
davi, M. Federated learning with compression: Uniﬁed
analysis and sharp guarantees. In International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 2350–
2358. PMLR, 2021.

Haochen, J. and Sra, S. Random shufﬂing beats SGD after
ﬁnite epochs. In Chaudhuri, K. and Salakhutdinov, R.
(eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Ma-
chine Learning Research, pp. 2624–2633, Long Beach,
California, USA, 09–15 Jun 2019. PMLR.

Horvath, S., Ho, C.-Y., Horvath, L., Sahu, A. N., Canini,
M., and Richtarik, P. Natural compression for distributed
deep learning. arXiv preprint arXiv:1905.10988, 2019.

Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. Advances
in Neural Information Processing Systems, 26:315–323,
2013.

Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich,
S. U., and Suresh, A. T. SCAFFOLD: Stochastic Con-
trolled Averaging for On-Device Federated Learning.
arXiv preprint arXiv:1910.06378, 2019.

Khaled, A. and Richt´arik, P. Gradient descent with com-
pressed iterates. arXiv preprint arXiv:1909.04716, 2019.

Khaled, A., Mishchenko, K., and Richt´arik, P. First anal-
ysis of local gd on heterogeneous data. arXiv preprint
arXiv:1909.04715, 2019.

Khaled, A., Mishchenko, K., and Richt´arik, P. Tighter
theory for local sgd on identical and heterogeneous data.
In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 4519–4529. PMLR, 2020.

Koneˇcn`y, J., McMahan, H. B., Yu, F. X., Richt´arik, P.,
Suresh, A. T., and Bacon, D. Federated learning: Strate-
gies for improving communication efﬁciency. arXiv
preprint arXiv:1610.05492, 2016.

Kovalev, D., Horv´ath, S., and Richt´arik, P. Don’t jump
through hoops and remove those loops: SVRG and
katyusha are better without the outer loop. In Algorithmic
Learning Theory, pp. 451–467. PMLR, 2020.

Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. Don’t
use large mini-batches, use local sgd. arXiv preprint
arXiv:1808.07217, 2018.

Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, W. J.
Deep gradient compression: Reducing the communica-
tion bandwidth for distributed training. arXiv preprint
arXiv:1712.01887, 2017.

Malinovsky, G., Sailanbayev, A., and Richt´arik, P. Random
reshufﬂing with variance reduction: New analysis and
better rates. arXiv preprint arXiv:2104.09342, 2021.

McMahan, B., Moore, E., Ramage, D., Hampson, S., and
y Arcas, B. A. Communication-efﬁcient learning of deep

Federated Random Reshufﬂing with Compression and Variance Reduction

Robbins, H. and Monro, S. A stochastic approximation
method. The annals of mathematical statistics, pp. 400–
407, 1951.

Roux, N. L., Schmidt, M., and Bach, F. A stochastic gradient
method with an exponential convergence rate for ﬁnite
training sets. arXiv preprint arXiv:1202.6258, 2012.

Safran, I. and Shamir, O. How good is SGD with random
shufﬂing? In Conference on Learning Theory, pp. 3250–
3284. PMLR, 2020.

Shamir, O., Srebro, N., and Zhang, T. Communication-
efﬁcient distributed optimization using an approximate
In International conference on
newton-type method.
machine learning, pp. 1000–1008. PMLR, 2014.

Stich, S. U. Local sgd converges fast and communicates

little. arXiv preprint arXiv:1805.09767, 2018.

Vogels, T., Karimireddy, S. P., and Jaggi, M. Powersgd:
Practical low-rank gradient compression for distributed
optimization. arXiv preprint arXiv:1905.13727, 2019.

Wangni, J., Wang, J., Liu, J., and Zhang, T. Gradient sparsi-
ﬁcation for communication-efﬁcient distributed optimiza-
tion. arXiv preprint arXiv:1710.09854, 2017.

Wu, J., Huang, W., Huang, J., and Zhang, T. Error com-
pensated quantized sgd and its applications to large-scale
distributed optimization. In International Conference on
Machine Learning, pp. 5325–5333. PMLR, 2018.

Ying, B., Yuan, K., Vlaski, S., and Sayed, A. H. Stochastic
learning under random reshufﬂing with constant step-
sizes. In IEEE Transactions on Signal Processing, vol-
ume 67, pp. 474–489, 2019.

networks from decentralized data. In Artiﬁcial Intelli-
gence and Statistics, pp. 1273–1282. PMLR, 2017.

Mishchenko, K., Gorbunov, E., Tak´aˇc, M., and Richt´arik,
P. Distributed learning with compressed gradient differ-
ences. arXiv preprint arXiv:1901.09269, 2019.

Mishchenko, K., Khaled, A., and Richt´arik, P. Random
reshufﬂing: Simple analysis with vast improvements. Ad-
vances in Neural Information Processing Systems, 33,
2020.

Mishchenko, K., Khaled, A., and Richt´arik, P. Proxi-
mal and federated random reshufﬂing. arXiv preprint
arXiv:2102.06704, 2021.

Mokhtari, A., G¨urb¨uzbalaban, M., and Ribeiro, A. Sur-
passing gradient descent provably: A cyclic incremental
method with linear convergence rate. SIAM Journal on
Optimization, 28(2):1420–1447, 2018.

Nagaraj, D., Jain, P., and Netrapalli, P. SGD without replace-
ment: sharper rates for general smooth convex functions.
In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed-
ings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learn-
ing Research, pp. 4703–4711, Long Beach, California,
USA, 09–15 Jun 2019. PMLR.

Park, Y. and Ryu, E. K. Linear convergence of cyclic SAGA.

Optimization Letters, 14(6):1583–1598, 2020.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Rajput, S., Gupta, A., and Papailiopoulos, D. Closing the
convergence gap of SGD without replacement. arXiv
preprint arXiv:2002.10400, 2020.

Ramezani-Kebrya, A., Faghri, F., and Roy, D. M. Nuqsgd:
Improved communication efﬁciency for data-parallel
arXiv preprint
sgd via nonuniform quantization.
arXiv:1908.06077, 2019.

Recht, B. and R´e, C. Toward a noncommutative arithmetic-
geometric mean inequality: conjectures, case-studies, and
consequences. In Conference on Learning Theory, pp. 11–
1. JMLR Workshop and Conference Proceedings, 2012.

Recht, B. and R´e, C. Parallel stochastic gradient algorithms
for large-scale matrix completion. Mathematical Pro-
gramming Computation, 5(2):201–226, 2013.

Federated Random Reshufﬂing with Compression and Variance Reduction

Supplementary materials

A. Basic Facts

Proposition 1. Let f : Rd → R be continuously differentiable and let L ≥ 0. Then the following statements are equivalent:

• f is L-smooth

• 2Df (x, y) ≤ L(cid:107)x − y(cid:107)2 for all x, y ∈ Rd

• (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ≤ L(cid:107)x − y(cid:107)2 for all x, y ∈ Rd

Proposition 2. Let f : Rd → R be continuously differentiable and let µ ≥ 0. Then the following statements are equivalent:

• f is µ-strongly convex

• 2Df (x, y) ≥ µ(cid:107)x − y(cid:107)2 for all x, y ∈ Rd

• (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ≥ µ(cid:107)x − y(cid:107)2 for all x, y ∈ Rd

Note that the µ = 0 case reduces to convexity.
Proposition 3. Let f : Rd → R be continuously differentiable and L > 0. Then the following statements are equivalent:

• f is convex and L-smooth

• 0 ≤ 2Df (x, y) ≤ L(cid:107)x − y(cid:107)2 for all x, y ∈ Rd

• 1

• 1

L (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ 2Df (x, y) for all x, y ∈ Rd
L (cid:107)∇f (x) − ∇f (y)(cid:107)2 ≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) for all x, y ∈ Rd

Proposition 4 (Jensen’s inequality). Let f : Rd → R be a convex function, x1, . . . , xm ∈ Rd and λ1, . . . , λm be
nonnegative real numbers adding up to 1. Then

(cid:32) m
(cid:88)

f

i=1

(cid:33)

λixi

≤

m
(cid:88)

i=1

λif (xi)

Proposition 5. For all a, b ∈ Rd and t > 0 the following inequalities holds:

(cid:104)a, b(cid:105) ≤

(cid:107)a(cid:107)2
2t
(cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2

t(cid:107)b(cid:107)2
2

+

1
2

(cid:107)a(cid:107)2 − (cid:107)b(cid:107)2 ≤ (cid:107)a + b(cid:107)2

B. General lemmas

B.1. Proposition 1

We need to prove a basic fact which will be used later.

Proposition 6. Let us consider

then

xn
∗,m = x∗ − γ

n−1
(cid:88)

i=0

∇fπi,m(x∗),

1
M

M
(cid:88)

m=1

xn
∗,m = x∗.

Federated Random Reshufﬂing with Compression and Variance Reduction

Proof. We start from the deﬁnition:

1
M

M
(cid:88)

m=1

xn
∗,m =

1
M

(cid:32)

M
(cid:88)

x∗ − γ

(cid:33)

∇fπi,m(x∗)

n−1
(cid:88)

i=0

m=1

M
(cid:88)

x∗ −

=

1
M

m=1
= x∗ − ∇f (x∗)
= x∗.

1
M

M
(cid:88)

n−1
(cid:88)

m=1

i=0

∇fπi,m(x∗)

B.2. Proof of Theorem 1

For completeness we include the proof of important theorem introduced in Mishchenko et al. (2020).

Proof.

E (cid:2)Dfπi

(cid:0)xi

∗, x∗

(cid:1)(cid:3) ≤ E

(cid:20) L
2

(cid:13)
(cid:13)xi

∗ − x∗

2(cid:21)
(cid:13)
(cid:13)

≤

Lmax
2

E

∗ − x∗

2(cid:105)

(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)xi


=

γ2Lmax
2

E




γ2Lmaxi2
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
∇fπj (x∗)
(cid:13)
(cid:13)
(cid:13)

2




i−1
(cid:88)

j=0

1
i

i−1
(cid:88)

j=0

∇fπj (x∗)

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

γ2Lmaxi2
2

E

(cid:104)(cid:13)
(cid:13) ¯Xπ

2(cid:105)

(cid:13)
(cid:13)

,

=

=

where ¯Xπ = 1
j
(Mishchenko et al., 2020).

(cid:80)i−1

j=0 Xπj with Xj

def= ∇fj (x∗) for j = 1, 2, . . . , n . Since ¯X = ∇f (x∗) , by applying Lemma 1 in

E

(cid:104)(cid:13)
(cid:13) ¯Xπ

2(cid:105)

(cid:13)
(cid:13)

= (cid:107) ¯X(cid:107)2 + E

(cid:104)(cid:13)
(cid:13) ¯Xπ − ¯X(cid:13)
(cid:13)

2(cid:105)

= (cid:107)∇f (x∗)(cid:107)2 +

n − i
i(n − 1)

σ2
∗.

It remains to combine both terms and use the bounds i2 ≤ n2 and i(n − i) ≤ n(n−1)
i ∈ {1, 2, . . . , n − 1}, and divide both sides of the resulting inequality by γ2.

2

, which holds for all

B.3. Proof of Lemma 1

Proof. We start from Theorem 1. Then for reformulated problem we have

nσ2
∗

def=

n
(cid:88)

i=1

(cid:107)∇fi (x∗) − ∇f (x∗)(cid:107)2 =

n
(cid:88)

M
(cid:88)

i=1

m=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇fmi (x∗) −

∇Fm (x∗)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

1
n

.

For inner sum we have a bound from Mishchenko et al. (2021):
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

∇fmi (x∗) −

∇Fm (x∗)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

i=1

Also, we have

n2 (cid:107)∇f (x∗)(cid:107)2 = n2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

∇fi (x∗)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

M
(cid:88)

m=1

≤ nσ2

m,∗ + (cid:107)∇Fm (x∗)(cid:107)2 .

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

∇fmi (x∗)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

M
(cid:88)

m=1

(cid:107)∇Fm (x∗)(cid:107)2 .

Plugging the last two inequalities back inside the ﬁrst bound on σ2

rad, we get the lemma’s statement.

Federated Random Reshufﬂing with Compression and Variance Reduction

C. Analysis of Algorithm 1

C.1. Proof of Theorem 1

Proof. We start from conditional expectation

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2 | xn

t,m

(cid:3) = E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

C(xn

t,m) − x∗



xn
t,m



(cid:13)
2 (cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)
(cid:12)
(cid:13)

≤ E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

C(xn

t,m) −

1
M

M
(cid:88)

m=1

xn
t,m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

xn
t,m

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

xn
t,m − x∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

≤

ω
M 2

2ω
M 2

M
(cid:88)

m=1

M
(cid:88)

m=1

(cid:107)xn

t,m(cid:107)2 +

1
M

M
(cid:88)

m=1

(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

(cid:107)xn

t,m − xn

∗,m(cid:107)2

1
M

M
(cid:88)

m=1

(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

2ω
M 2

M
(cid:88)

m=1

(cid:107)xn

∗,m(cid:107)2

≤ (1 − γµ)n

(cid:18)

1 +

(cid:19)

2ω
M

(cid:107)xt − x∗(cid:107)2 +

(cid:18)

+ 2

1 +

(cid:19)

2ω
M

γ3σ2

rad





n−1
(cid:88)

j=0

(1 − γµ)j

1
M

M
(cid:88)

m=1

(cid:107)xn

∗,m(cid:107)2

2ω
M


 .

Using tower property we get

Utilizing this property we have

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) = E (cid:2)E (cid:2)(cid:107)xt+1 − x∗(cid:107)2 | xn

t,m

(cid:3)(cid:3) .

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) ≤ (1 − γµ)n

(cid:18)

1 +

(cid:19)

2ω
M

E (cid:2)(cid:107)xt − x∗(cid:107)2(cid:3) +

2ω
M

1
M

M
(cid:88)

m=1

E(cid:107)xn

∗,m(cid:107)2

(cid:18)

+ 2

1 +

(cid:19)

2ω
M



γ3σ2

rad



n−1
(cid:88)

j=0



(1 − γµ)j

 .

Unrolling this recursion we get

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) ≤

(cid:18)

(1 − γµ)n

(cid:19)(cid:19)T

(cid:18)

1 +

2ω
M

(cid:107)x0 − x∗(cid:107)2

+

T −1
(cid:88)

i=0

(cid:18)

(1 − γµ)n

(cid:18)

1 +

2ω
M

(cid:19)(cid:19)i 2ω
M

1
M

M
(cid:88)

m=1

E(cid:107)xn

∗,m(cid:107)2

T −1
(cid:88)

(cid:18)

+ 2

(1 − γµ)n

i=0

(cid:18)

1 +

2ω
M

(cid:19)(cid:19)i (cid:18)

1 +

(cid:19)

2ω
M



γ3σ2

rad



n−1
(cid:88)

j=0



(1 − γµ)j

 .

Using assumption of compression operator we have

(1 − γµ)n

(cid:18)

1 +

(cid:19)

2ω
M

≤ (1 − γµ)

n

2 .

Federated Random Reshufﬂing with Compression and Variance Reduction

Also let us look at last term:

(cid:18)

1 +

2ω
M

(cid:19) (cid:32)T −1
(cid:88)

(1 − γµ)i

(cid:33)

i=0

≤

≤

≤

i=0

T −1
(cid:88)

j=0

T −1
(cid:88)

(1 − γµ)i

(cid:18)

1 +

(cid:19)i

2ω
M

(cid:18)

(cid:18)

(1 − γµ)

1 +

(cid:19)(cid:19)i

2ω
M

T −1
(cid:88)

(1 − γµ)

nj
2 .

i=0

Moreover, we have this bound for geometric sequence:

(cid:32)T −1
(cid:88)

(1 − γµ)

(cid:33) 


nj
2

n−1
(cid:88)

(1 − γµ)j



 ≤

T −1
(cid:88)

n−1
(cid:88)

(1 − γµ)

i=0

j=0

i=0

j=0

ni

2 +j ≤

1
γµ

.

The same bound we have for the second sum:

(cid:32)T −1
(cid:88)

(1 − γµ)

i=0

(cid:33)

nj
2

≤

1
γµ

.

Finally, we have the following:

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) ≤ (1 − γµ)

nT

2 (cid:107)x0 − x∗(cid:107)2 +

2
µ

γ2σ2

rad +

2ω
M

1
γµ

1
M

M
(cid:88)

m=1

E(cid:107)xn

∗,m(cid:107)2.

Using Lemma we have

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2(cid:3) ≤ (1 − γµ)

nT

2 (cid:107)x0 − x∗(cid:107)2 +

2
µ

γ2Lmax

1
M

M
(cid:88)

(cid:16)

m=1

(cid:107)Fm(x∗)(cid:107)2 +

(cid:17)

σ2
∗,m

n
4

+

2ω
M

1
γµ

1
M

M
(cid:88)

m=1

E(cid:107)xn

∗,m(cid:107)2.

D. Analysis of Algorithm 2 and Algorithm 3

D.1. Lemma 2

For Algorithm 2 and Algorithm 3 the following inequality holds:

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2 | xt, ht,m

(cid:3) ≤

η2
M 2 ω

M
(cid:88)

m=1

(cid:107)xn

t,m − ht,m(cid:107)2 + (1 − η)(cid:107)xt − x∗(cid:107)2 + η

1
M

M
(cid:88)

m=1

(cid:107)xn

t,m − xn

∗,m(cid:107)2

Federated Random Reshufﬂing with Compression and Variance Reduction

Proof. Let us use property of compression operator:

E (cid:2)(cid:107)xt+1 − x∗(cid:107)2 | xt, ht,m

(cid:3) = E

= E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1 − η)xt + η

1
M

M
(cid:88)

m=1

(cid:0)C(xn

t,m − ht,m) + ht,m



xt, ht,m



(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





(cid:13)
(cid:13)
(cid:13)
η
(cid:13)
(cid:13)

1
M

M
(cid:88)

m=1

C(xn

t,m − ht,m) − η

1
M

M
(cid:88)

m=1

(xn

(cid:13)
(cid:13)
(cid:13)
t,m − ht,m)
(cid:13)
(cid:13)

2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



xt, ht,m



+

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(1 − η)xt + η

1
M

M
(cid:88)

m=1

xn
t,m − x∗

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

η2
M 2 ω

M
(cid:88)

m=1

(cid:107)xn

t,m − ht,m(cid:107)2 + (1 − η)(cid:107)xt − x∗(cid:107)2 + η

1
M

M
(cid:88)

m=1

D.2. Lemma 3

(cid:107)xn

t,m − xn

∗,m(cid:107)2.

For Algorithm 2 and Algorithm 3 the following inequality holds:
E (cid:2)(cid:107)ht+1,m − xn

∗,m(cid:107)2(cid:3) ≤ (1 − α)E(cid:107)ht,m − xn

∗,m(cid:107)2 + αE(cid:107)xn

t,m − xn

∗,m(cid:107)2.

Proof. Let us start from conditional expectation:

E (cid:2)(cid:107)ht+1,m − xn

∗,m(cid:107)2 | xn

t,m, ht,m

(cid:3) = E

(cid:104)(cid:13)
(cid:13)ht,m + αqt,m − xn

(cid:13)
2
(cid:13)

(cid:105)

∗,m

| xn
∗,m(cid:107)2 + 2α (cid:10)ht,m − xn
∗,m(cid:107)2 + 2α (cid:10)ht,m − xn

t,m, ht,m
∗,m, E[qt,m](cid:11) + α2E (cid:2)(cid:107)qt,m(cid:107)2(cid:3)
∗,m, xn

t,m − ht,m

(cid:11)

t,m − ht,m(cid:107)2

∗,m(cid:107)2 + 2α (cid:10)ht,m − xn

∗,m, xn

t,m − ht,m

(cid:11)

≤ (cid:107)ht,m − xn
≤ (cid:107)ht,m − xn
+ α2(ω + 1)(cid:107)xn
≤ (cid:107)ht,m − xn
+ α(cid:107)xn
= (cid:107)ht,m − xn

t,m − ht,m(cid:107)2

∗,m(cid:107)2 + α (cid:10)2ht,m − 2xn

∗,m + xn

t,m − ht,m, xn

t,m − ht,m

(cid:11) .

Let us consider last term:

(cid:10)2ht,m − 2xn
= (cid:10)ht,m − xn
= −(cid:107)ht,m − xn

∗,m + xn
∗,m + xn

t,m − ht,m, xn
∗,m, xn
t,m − xn
t,m − xn

t,m − ht,m
t,m − xn
∗,m(cid:107)2.

∗,m(cid:107)2 + (cid:107)xn

(cid:11)

∗,m − (cid:0)ht,m − xn

∗,m

(cid:1)(cid:11)

Using this result and previous inequlaity we get th following:

E (cid:2)(cid:107)ht+1,m − xn

∗,m(cid:107)2 | xn

t,m, ht,m

(cid:3) ≤ (1 − α)(cid:107)ht,m − xn

∗,m(cid:107)2 + α(cid:107)xn

t,m − xn

∗,m(cid:107)2.

Taking full expectation we ﬁnish the proof.

D.3. Lemma 6

For completeness we include the proof of important theorem introduced in Mishchenko et al. (2021). Suppose that each fi
is L-smooth and µ-strongly convex. Then the inner iterates satisfy

E

(cid:104)(cid:13)
(cid:13)xi+1

t − xi+1

∗

2(cid:105)

(cid:13)
(cid:13)

≤ (1 − γµ)E

(cid:104)(cid:13)
(cid:13)xi

t − xi
∗

2(cid:105)

(cid:13)
(cid:13)

− 2γ (1 − γL) E (cid:2)Dfπi

(cid:0)xi

t, x∗

(cid:1)(cid:3) + 2γ3σ2

rad.

Federated Random Reshufﬂing with Compression and Variance Reduction

Proof. By deﬁnition of xi+1

t

and xi+1

∗

, we have

E

(cid:104)(cid:13)
(cid:13)xi+1

t − xi+1

∗

2(cid:105)

(cid:13)
(cid:13)

= E

(cid:104)(cid:13)
(cid:13)xi

2(cid:105)

(cid:13)
(cid:13)

t − xi
∗
(cid:104)(cid:13)
(cid:13)∇fπi

− 2γE (cid:2)(cid:10)∇fπi
(cid:0)xi

(cid:0)xi
(cid:1) − ∇fπi (x∗)(cid:13)
(cid:13)

t

t

+ γ2E

.

(cid:1) − ∇fπi (x∗) , xi
2(cid:105)

t − xi
∗

(cid:11)(cid:3)

Note that the third term can be bounded as

(cid:13)
(cid:13)∇fπi

(cid:0)xi

t

(cid:1) − ∇fπi

(cid:0)xi

t

(cid:1)(cid:13)
2
(cid:13)

≤ 2L · Dfπi

(cid:0)xi

t, x∗

(cid:1) .

Using the three-point identity we get
(cid:10)∇fπi

(cid:0)xi

(cid:1) − ∇fπi (x∗) , xi

t

t − xi
∗

(cid:11) = Dfπi

(cid:0)xi

∗, xi
t

(cid:1) + Dfπi

(cid:0)xi

t, x∗

(cid:1) − Dfπi

(cid:0)xi

∗, x∗

(cid:1) .

Combining these bounds we have
(cid:104)(cid:13)
(cid:13)xi+1

t − xi+1

(cid:13)
(cid:13)

E

∗

2(cid:105)

≤ E

(cid:104)(cid:13)
(cid:13)xi

t − xi
∗

2(cid:105)

(cid:13)
(cid:13)

− 2γ · E (cid:2)Dfπi
∗, xi
t
− 2γ (1 − γL) E (cid:2)Dfπi

(cid:0)xi

(cid:1)(cid:3) + 2γ · E (cid:2)Dfπi
(cid:0)xi
(cid:1)(cid:3) .

t, x∗

(cid:0)xi

∗, x∗

(cid:1)(cid:3)

Using µ-strong convexity of fπi , we derive

µ
2

(cid:13)
(cid:13)xi

t − xi
∗

(cid:13)
2
(cid:13)

≤ Dfπi

(cid:0)xi

∗, xi
t

(cid:1) .

Using deﬁnition of shufﬂing radius we have
(cid:0)xi

E (cid:2)Dfπi

∗, x∗

(cid:1)(cid:3) ≤ max

i=1,...,n−1

E (cid:2)Dfπi

(cid:0)xi

∗, x∗

(cid:1)(cid:3) = γ2σ2

rad.

Putting all bounds together we get result.

D.4. Proof of Theorem 3

Proof. Let us deﬁne the Lyapunov function:

Ψt = (cid:107)xt − x∗(cid:107)2 +

4η2ω
αM

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)ht,m − xn

∗,m

(cid:13)
2
(cid:13)

.

Now we use Lemma 2 and Lemma 3:

E [Ψt+1] = E(cid:107)xt+1 − x∗(cid:107)2 +

4η2ω
αM

1
M

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht+1,m − xn

∗,m

(cid:13)
2
(cid:13)

≤

2η2
M 2 ω

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

2η2
M 2 ω

M
(cid:88)

m=1

E(cid:107)ht,m − xn

∗,m(cid:107)2 + (1 − η)E(cid:107)xt − x∗(cid:107)2

+ η

1
M

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

4η2ω
αM

(1 − α)

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht,m − xn

∗,m

(cid:13)
2
(cid:13)

+

4η2ω
αM

α

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2.

Using Lemma 6 and Theorem 2 from Mishchenko et al. (2021) we have

E [Ψt+1] ≤

4η2ω
αM

(cid:16)

1 −

α
2

(cid:17) 1
M

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht,m − xn

∗,m

(cid:18)

(cid:13)
2
(cid:13)

+

1 − η + η(1 − γµ)n +

(cid:19)

(1 − γµ)n

6η2ω
M

E(cid:107)xt − x∗(cid:107)2

(cid:18)

+

α + η +

(cid:19)

2η2ω
M

2γ3σ2

rad





n−1
(cid:88)

(1 − γµ)j





j=0

Federated Random Reshufﬂing with Compression and Variance Reduction

Using the condition η ≤ min

(cid:16)

1, (1−(1−γµ)n)M
12ω(1−γµ)n

(cid:17)

we have

E [Ψt+1] ≤ max

(cid:18)

1 −

α
2

, 1 −

η (1 − (1 − γµ)n)
2

(cid:19)

E [Ψt] +

(cid:18)

α + η +

(cid:19)

2η2ω
M

2γ3σ2

rad

Note that we have the following inequality:





n−1
(cid:88)

j=0

(1 − γµ)j





1 − γµ ≤ 1 −

−γµ ≤ −

η (1 − (1 − γµ)n)

1
2
1
2
1
η (1 − (1 − γµ)n) .
2

η (1 − (1 − γµ)n)

We have it since 0 < η ≤ 1 and n > 1, so we have

γµ ≥

γµ ≥

γµ ≥

1 ≥

1
2
1
2
1
2

(1 − (1 − γµ)n)

(1 − (1 − γµ))

.

Unrolling this recursion ﬁnishes the proof.

Lemma 4

For completeness we include the proof of important theorem introduced in Malinovsky et al. (2021). Suppose that the
functions f1, . . . , fn are µ-strongly convex and L-smooth. Fix constant 0 < δ < 1. If the stepsize satisﬁes γ ≤ δ
L
and if number of functions is sufﬁciently big:

(cid:112) µ
2nL

n > log

(cid:18) 1

1 − δ2

(cid:19)

(cid:18)

·

log

(cid:18) 1

(cid:19)(cid:19)−1

1 − γµ

and

then we have

(cid:104)

E

δ2 ≤ (1 − γµ)
∗ (cid:107)2(cid:105)

t − xn

(cid:104)

E

(cid:107)xn

≤ (1 − γµ)

(cid:104)

n
2 E

n

2 (cid:0)1 − (1 − γµ)

n

2 (cid:1) .
(cid:107)xt − x∗(cid:107)2(cid:105)

,

(cid:107)xn

t − xn

∗ (cid:107)2 | xt

(cid:105)

≤ (1 − γµ)n (cid:107)xt − x∗(cid:107)2 +

γ3Ln
2

σ2
∗

(cid:32)n−1
(cid:88)

i=0

(1 − γµ)i

Proof. We start from Theorem 1 in Mishchenko et al. (2020):

(cid:104)

E

(cid:107)xn

t − xn

∗ (cid:107)2 | xt

(cid:105)

≤ (1 − γµ)n (cid:107)xt − x∗(cid:107)2 +

γ3Ln
2

σ2
∗

(cid:32)n−1
(cid:88)

i=0

(1 − γµ)i

Using property of geometric progression we can have an upper bound (cid:80)n−1

i=0 (1 − γµ)i ≤ 1

γµ :

(cid:104)

E

(cid:107)xn

t − xn

∗ (cid:107)2 | xt

(cid:105)

≤ (1 − γµ)n (cid:107)xt − x∗(cid:107)2 +

γ2Ln
2µ

σ2
∗.

(cid:33)

(cid:33)

Federated Random Reshufﬂing with Compression and Variance Reduction

Using Lemma 1 in Malinovsky et al. (2021) we get

E

(cid:104)
(cid:107)xn

t − xn

∗ (cid:107)2 | xt

(cid:18)

(cid:105)

≤

(1 − γµ)n +

(cid:19)

2γ2L3n
µ

(cid:107)xt − x∗(cid:107)2 .

Let us use γ ≤ δ
L

(cid:112) µ

2nL . To get convergence we need

(1 − γµ)n + δ2 < 1.

This leads to the following inequality:

Also assume

n > log

(cid:18) 1

1 − δ2

(cid:19)

(cid:18)

·

log

(cid:18) 1

(cid:19)(cid:19)−1

1 − γµ

.

δ2 ≤ (1 − γµ)

n

2 (cid:0)1 − (1 − γµ)

n

2 (cid:1) .

Putting this into bound ﬁnishes the proof.

D.5. Lemma 5

For completeness we include the proof of important lemma introduced in Malinovsky et al. (2021).

Assume that each fi is L-smooth and convex. If we apply the linear perturbation reformulation 6, then the variance of
reformulated problem satisﬁes the following inequality:

∗ ≤ 4L2(cid:107)yt − x∗(cid:107)2.
˜σ2

Proof.

˜σ2
∗ =

1
n

n
(cid:88)

i=1

(cid:107)∇fi (x∗) − ∇fi (yt) + ∇f (yt) − ∇f (x∗)(cid:107)2

Using Young’s inequality we have

˜σ2
∗ ≤

1
n

n
(cid:88)

i=1
n
(cid:88)

≤

1
n

(cid:16)

2 (cid:107)∇fi (yt) − ∇fi (x∗)(cid:107)2 + 2 (cid:107)∇f (yt) − ∇f (x∗)(cid:107)2(cid:17)

n
(cid:88)

4LDf (yt, x∗)

1
n

4LiDfi (yt, x∗) +

i=1

i=1
≤ 4LDf (yt, x∗) + 4LDf (yt, x∗)
= 8LDf (yt, x∗)
≤ 4L2 (cid:107)yt − x∗(cid:107)2

D.6. Proof of Theorem 4

The proof is similar to the proof of 3.

Proof. Let us deﬁne the Lyapunov function:

Ψt = (cid:107)xt − x∗(cid:107)2 +

4η2ω
αM

1
M

M
(cid:88)

m=1

(cid:13)
(cid:13)ht,m − xn

∗,m

(cid:13)
2
(cid:13)

.

Federated Random Reshufﬂing with Compression and Variance Reduction

Now we use Lemma 2 and Lemma 3:

E [Ψt+1] = E(cid:107)xt+1 − x∗(cid:107)2 +

4η2ω
αM

1
M

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht+1,m − xn

∗,m

(cid:13)
2
(cid:13)

≤

2η2
M 2 ω

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

2η2
M 2 ω

M
(cid:88)

m=1

E(cid:107)ht,m − xn

∗,m(cid:107)2 + (1 − η)E(cid:107)xt − x∗(cid:107)2

+ η

1
M

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2 +

4η2ω
αM

(1 − α)

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht,m − xn

∗,m

(cid:13)
2
(cid:13)

+

4η2ω
αM

α

M
(cid:88)

m=1

E(cid:107)xn

t,m − xn

∗,m(cid:107)2.

Using Lemma 5 and Theorem 3 from Malinovsky et al. (2021) we have

E [Ψt+1] ≤

4η2ω
αM

(cid:16)

1 −

α
2

(cid:17) 1
M

M
(cid:88)

m=1

E (cid:13)

(cid:13)ht,m − xn

∗,m

(cid:13)
2
(cid:13)

+

(cid:18)

1 − η + η(1 − γµ)

n
2 +

6η2ω
M

(cid:19)

n
2

(1 − γµ)

E(cid:107)xt − x∗(cid:107)2

(cid:18)

+

α + η +

(cid:19)

2η2ω
M

2γ3L

M
(cid:88)

m=1

(cid:107)∇Fm (x∗)(cid:107)2





n−1
(cid:88)

j=0

(1 − γµ)j





Using the condition η ≤ min

1,

(cid:18)

(cid:16)

1−(1−γµ)

(cid:17)

n
2

M

(cid:19)

12ω(1−γµ)

n
2

we have

(cid:32)

E [Ψt+1] ≤ max

1 −

α
2

, 1 −

η (cid:0)1 − (1 − γµ) n
2 (cid:1)
2

(cid:33)

E [Ψt]

(cid:18)

+

α + η +

(cid:19)

2η2ω
M

2γ3L

M
(cid:88)

m=1

(cid:107)∇Fm (x∗)(cid:107)2

(1 − γµ)j









n−1
(cid:88)

j=0

Unrolling this recursion as we did previously ﬁnishes the proof.

