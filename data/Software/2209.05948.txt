Learning to Prevent Proﬁtless Neural Code
Completion

Zhensu Sun∗, Xiaoning Du†, Fu Song∗, Shangwen Wang‡, Mingze Ni§, Li Li†
∗ ShanghaiTech University, Shanghai, China
† Monash University, Melbourne, Australia
‡ National University of Defense Technology, Changsha, China
§ University of Technology Sydney, Sydney, Australia
{sunzhs, songfu}@shanghaitech.edu.cn, {xiaoning.du, li.li}@monash.edu,
wangshangwen13@nudt.edu.cn, Mingze.Ni@student.uts.edu.au

2
2
0
2

p
e
S
3
1

]
E
S
.
s
c
[

1
v
8
4
9
5
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Currently, large pre-trained models are widely ap-
plied in neural code completion systems, such as Github Copi-
lot, aiXcoder, and TabNine. Though large models signiﬁcantly
outperform their smaller counterparts, a survey with 2,631
participants reveals that around 70% displayed code completions
from Copilot are not accepted by developers. Being reviewed but
not accepted, these completions bring a threat to productivity.
Besides, considering the high cost of the large models, it is a huge
waste of computing resources and energy, which severely goes
against the sustainable development principle of AI technologies.
Additionally, in code completion systems, the completion requests
are automatically and actively issued to the models as developers
type out, which signiﬁcantly aggravates the workload. However,
to the best of our knowledge, such waste has never been
realized, not to mention effectively addressed, in the context of
neural code completion. Hence, preventing such proﬁtless code
completions from happening in a cost-friendly way is of urgent
need. To ﬁll this gap, we ﬁrst investigate the prompts of these
completions and ﬁnd four observable prompt patterns, which
demonstrate the feasibility of identifying such prompts based
on prompts themselves. Motivated by this ﬁnding, we propose
an early-rejection mechanism to turn down low-return prompts
by foretelling the completion qualities without sending them to
the LCM. Further, we propose a lightweight Transformer-based
estimator to demonstrate the feasibility of the mechanism. The
experimental results show that the estimator rejects low-return
prompts with a promising accuracy of 83.2%.

I. INTRODUCTION

Beneﬁting from the large-scale pre-trained code models,
automated code completion systems powered by deep learning
have achieved unprecedented superior performance. A large
number of commercial applications based on Large Code
Models (LCMs) are recently released, such as Github Copi-
lot [1], aiXcoder [2], TabNine [3], and CodeWhisperer [4].
Without relying on data annotation, the pre-training of LCMs
happens in a self-supervised manner, which signiﬁcantly
promotes the exploitation of widely existing code corpus.
However, many empirical studies have found that a large
proportion of code completions, though generated by state-of-
the-art LCM-based code completion systems, have no or even
negative effects on improving the productivity of developers.
Ziegler et al. [5] collected developers’ feedback on Copilot
code completions and surprisingly found that around 70%
displayed code completions are not accepted by develop-

to understand and/or debug. Consequently,

ers (according to 2,631 survey responses). Multiple other
studies [6], [7] also revealed that developers might get rid
of the entire code completions and start over the coding
themselves, because the completed code requires substantial
it poses
effort
a threat to the productivity and leads to a huge waste of
computing resources and energy, which severely goes against
the sustainable development principle of AI technologies [8].
To the best of our knowledge, the energy waste problem has
never been realized in the context of neural code completion.
A quantitative analysis of the ﬁnancial and environmental
damage has yet to be done. Here, we illustrate the potential
waste with the cost analysis results on GPT-3, a popular
pre-trained large model (175 billion parameters) for natural
language. Training GPT-3 with a basic V100 NVIDIA GPU
requires about 288 years theoretically [9], which costs millions
of dollars on cloud service and produces CO2 emissions
approximating to three direct round trips of a whole passenger
jet between San Francisco and New York [10]. Moreover,
the inference cost after their deployment can be even higher
than the training cost since the models have to continuously
fulﬁll the user requests. Both NVIDIA [11] and Amazon [12]
estimate that up to 90% workload of neural models is for
inference rather than the model training.

LCMs currently used in commercial code completion sys-
tems are smaller than GPT-3, but only by an order of mag-
nitude. For example, Github Copilot and aiXcoder contain
12 billion and 10 billion parameters, respectively. The costs
are still considerable, especially in the context where the
scale of the large models keeps expanding. The number of
trainable parameters of the latest large models has reached
one trillion [13]. Additionally, aiming for seamless assistance
to developers, code completion systems are designed to auto-
matically and actively issue code completion requests to their
central servers when developers are typing out code or com-
ments. Such intensive interaction with the backend LCMs will
make their inference workload signiﬁcantly heavier. According
to a survey [5] on GitHub Copilot, an average number of
22.5 code completions are displayed to a user per hour. In
fact, if new characters are keyed in before a code completion
is received, that code completion will be discarded without

 
 
 
 
 
 
being shown to the user, which means that the number of the
actual code completions produced by GitHub Copilot within
one hour could be higher than 22.5. Given that almost 70%
of these code completions are not accepted, enormous energy
and resources could have been wasted. Hence, preventing such
proﬁtless code completions from happening in a cost-friendly
way is of urgent need.

Energy gets burnt when an LCM is ﬁred to conduct an
inference over the code prompt in a code completion request.
To avoid wasting energy on completing code prompts that
cannot anticipate a proﬁtable completion, measures have to
be taken without ﬁring LCMs. That is to say, those low-
return prompts shall be precluded before being fed into LCMs.
This is feasible to achieve if recognizable patterns exist
among prompts leading to proﬁtless code completions; rule-
based or learning-based algorithms can be devised to identify
these patterns. Thus, a manual inspection to these prompts is
necessary. The prompts of these unaccepted code completions
reported in [5] form an ideal dataset, but they are currently
not publicly available. Therefore, as an alternative, we crafted
a small group of prompts for the manual review by partially
extracting from Java code snippets. Speciﬁcally, two authors
manually reviewed the group of 383 code prompts paired with
corresponding code completions produced by Github Copilot
and determined if a code prompt is helpful or not. Finally,
we successfully summarized four frequently observed prompt
patterns leading to proﬁtless code completions. Besides, we
experimentally revealed that the quality of code completions
produced by Copilot drops a lot when code prompts contain
these patterns.

Motivated by the ﬁndings of the above empirical study,
we propose to equip the LCM-based code completion sys-
tems with an early-rejection mechanism to turn down low-
return prompts based on quality estimation before completion
(QEBC). The workﬂow is illustrated in Figure 1. The core
idea of QEBC is to have a lightweight code completion quality
estimator for the code completion system, which guards the
prompts sent to the LCM. It foretells the completion qualities
of the LCM solely based on prompts. Whenever the estimated
completion quality is not up to a certain threshold, it suggests
to give up the completion request. To guarantee the user
experience and cost reduction simultaneously, the mechanism
must be effective and cost-friendly. The former requires the
estimator to accurately estimate the completion quality for
prompts, especially the low-return prompts, and the latter is
to create and utilize the estimator in a cost-friendly way, such
that the overall cost of the code completion system can be
signiﬁcantly reduced. As a newly identiﬁed research problem
in the context of neural code completion systems, how to
design such an estimator is yet to be investigated.

There exists a trade-off between the effectiveness and cost-
friendliness of an estimator. The optimum completion quality
estimator is the LCM itself, however, no energy could be saved
this way. With a lighter but less accurate estimator, it may
underestimate or overestimate the actual completion qualities,
where the former may hinder the usability of the system and

the latter may leave some low-return prompts unattended.
Thus, a systematic investigation of the estimator design is
valuable but challenging. In this work, we simply look into
deep-learning-based estimators, and, for the ﬁrst time, attempt
to train a lightweight Transformer-based completion quality
estimator (TCQE) with prompts and their completion quality
indicators from the LCMs. We mainly aim to demonstrate the
feasibility of QEBC, and test how well TCQE can fulﬁll the
goals of QEBC.

To sufﬁciently evaluate the effectiveness of TCQE and the
feasibility of QEBC, we conduct a comprehensive evaluation
with eight experimental settings, including two programming
languages (Java and Python), two LCMs (GPT-2 and CodeT5),
and two metrics (BLEU and CrystalBLEU). Firstly, we mea-
sure whether TCQE can accurately reject low-return prompts
under different threshold settings. TCQE achieves an accuracy
of 83.2% on average of all the experimental settings. More
importantly, it only takes 0.9 GFLOPs for TCQE to run an
inference, while GPT-2 and CodeT5 respectively cost 72.6 and
290.7 GFLOPs. Further, a human evaluation is conducted to
investigate the prompts, where more negative patterns can be
observed in prompts with low estimation scores. Finally, we
release the code of TCQE to facilitate future research and
industrial practices, which are available on our website [14].
To the best of our knowledge, we are the ﬁrst to propose
an effective mechanism to prevent proﬁtless code completions
for LCM-based code completion systems. This work sheds
light on the serious energy waste problem in neural code
completion and opens a new research direction in this ﬁeld.
Our contribution can be summarized as follow:

• A human study that reveals the existence of the observ-
able patterns in low-return prompts, which demonstrates
the feasibility of inventing a corresponding recognizer.
• A simple yet cost-friendly prompt early-rejection mech-
anism, Performance Estimation Before Completion
(PEBC), to effectively save the ﬁnancial and environmen-
tal costs and improve the productivity of LCM-based code
completion systems.

• An estimator for QEBC, Transformer-based completion
quality estimator(TCQE), which can accurately estimate
the performance of LCMs on a given code snippet.

• A comprehensive evaluation that demonstrates the fea-
sibility of QEBC by evaluating the effectiveness and
efﬁciency of TCQE.

II. PROFITLESS CODE COMPLETIONS AND LOW-RETURN
PROMPTS

Before introducing the approach, we clarify the important
terminology used throughout the article, namely, proﬁtless
code completions and low-return prompts. Basically,
low-
return prompts lead to proﬁtless code completions. Therefore,
our work exactly aims at preventing the completion of low-
return prompts. We also empirically summarized four types of
low-return prompts as enlightenment to our approach.

2

Fig. 1: The workﬂow of an LCM-based code completion system with QEBC.

A. Proﬁtless Code Completions

Being proﬁtless means that the completions generated by
code completion systems fail to boost, sometimes even inhibit,
developers’ productivity, for example, when they do not match
developers’ intention or demand non-negligible efforts for
in a scenario
understanding and/or debugging. Intuitively,
where a completion is generated by the system according
to an input prompt, there are two possible reasons for the
ineffectiveness of the completion, notably, the system is not
doing a good job or the prompt itself is not informative enough
to derive a proper recommendation.

Researchers have been looking into the helpfulness of
Github Copilot ever since its birth, by surveying user feedback
on their experience with Copilot [6], [7] or collecting statistics
about the portion of Copilot completions that get discarded
without adoption [5]. The human study in [6] revealed that
three of the ﬁve cases, where participants failed to accom-
plish the programming task, were caused by the misleading
recommendations from Copilot. According to another study,
among the Copilot completions displayed to users, around
70% were not accepted [5]. Such proﬁtless completions waste
both the developers’ effort to review them and the valuable
computing resources and energy. This work intends to prevent
such completions from happening and raises attention on the
cost-effectiveness of designed solutions.

B. Low-return Prompts

We call the prompts leading to proﬁtless code completions
as low-return prompts. If there is a way to recognize these low-
return prompts in advance and preclude them before igniting
LCMs for the inference, the huge waste coming along with
proﬁtless completions can be avoided. The recognizer must
be effective in distinguishing such low-return prompts and
work in a cost-friendly manner. Its feasibility highly depends
on whether such low-return prompts share some recognizable
characteristics in common, such that they could be captured
with either rule-based or learning-based algorithms.

To this end, we conduct a small-scale empirical study and
if any,
to extract patterns of low-return prompts,
attempt
through manual inspection. A desired dataset is the production
data of the unaccepted completions as studied in [5], but it is
not publicly accessible. As an alternative, we crafted a group
of prompts in pair with their corresponding expected code
completions by randomly cutting code snippets into two parts,
where the preﬁx works as the prompt and the sufﬁx is the
expected completion. The code snippets are randomly sampled

Fig. 2: The simple examples of the prompts following each
summarized patterns. The example following the pattern is
is in green. The
in red, and the slightly shaped variant
completions are marked with gray background.

from a Java code dataset. Finally, we obtained 383 prompt-
and-expectation pairs. The prompts are fed into a commercial
code completion application, Copilot, to get the completion
recommendations. Note that Copilot automatically triggers a
completion request when the coding suspends for a while, so
that every prompt used here can trigger a completion in real
world.

Two authors were appointed to separately decide whether
a code completion is helpful, i.e., semantically close to the
expected code completion. Afterward, they had a discussion to
reconcile all disagreements. According to the observed prof-
itless completions, they further examined the corresponding
prompts, i.e., the low-return prompts, and successfully sum-
marized four most observable patterns: Meaningless Names,
Vague Intention, Unopen Context, and Self-unexplainable
Context, which appeared in 154, 262, 7, and 318 prompts,
respectively. Please note that this cannot be seen as a complete
list of the features of low-return prompts, but just a demonstra-
tion to exemplify the existence of such characteristics. Further,
we compare the quality of received completions between
the prompts following and unfollowing these patterns. The
BLEU scores of the completions from Copilot for prompts
following these patterns, except Unopen Context, are averagely
20.3% lower than the ones without patterns. Besides, Copilot
generates blank completions for all the prompts following

3

def sum(nums):    sum_num = 0    for num in _____Estimator Large Code ModelYesYesNoCode Promptdef sum(nums):    sum_num = 0    for num in nums:Code Completion> ?notify userNosubmit ?String ss = s.toString();Meaningless NamesString lowerCased = camelCased.toLowerCase();str = str.toString();Vague Intention// remove all spacestr = str.replace(“ ”,“”);public void isValid(){return true;}Unopen Contextpublic void isValid(){return true;}Room myRoom = new Room();Int roomId = myRoom.getId();Self-unexplainable Contextpublic class Room{int roomID(){return 10;}}Room myRoom = new Room();Int roomId = myRoom.roomID();Unopen Context. It experimentally demonstrates the effects
of these empirically summarized patterns. The overall results
are released on our website. We present the details of the
four patterns as follows. Besides, as listed in Figure 2, each
pattern is accompanied by a simple prompt example following
that pattern, and a slightly shaped variant of that prompt to be
an opposite pattern.

• Meaningless Names: Identiﬁer names, such as variable
names and method names, play an important role in com-
pleting code prompts, either for human developers or auto-
mated tools. Java naming convention [15] also recommends
using mnemonic names which indicate the intent of their
usage to casual observers. Meaningless names offer little
information about the type, intention, or other critical prop-
erties of the identiﬁers to the code completion systems,
making it difﬁcult, sometimes even impossible, to strike
completions desired by the developers. As shown in the
example, when to get the lower-cased version of a string,
the variable name lowerCased provides a much clearer
hint, than ss, to the inference model on recommending the
toLowerCase() API.

• Vague Intention: In addition to identiﬁer names, code
intention can also be conveyed through comments. When the
intention is not indicated in either way, completion models
can hardly generate a beneﬁcial completion for a prompt,
since there are too many plausible options. The example
demonstrates a case where the developer intends to remove
spaces in a string. When neither meaningful names nor
comments are presented to reveal the coding intention, the
completion model tends to ﬁnd a popular String API instead
of going directly to the helpful one.

• Unopen Context: For prompts ending with enclosed code
structures, e.g., ﬁnished class or function deﬁnitions, little
can be done on the completer side, since the code develop-
ment has come to an intermission. Even if the completion
model properly generates a blank completion for such
prompts, this is still unnecessary consumption of energy
and computing resources. The example shows a prompt of
an enclosed function deﬁnition, where no completions are
needed.

• Self-unexplainable Context: As opposed to built-in li-
braries and popular third-party libraries of a programming
language, project-speciﬁc code, such as global variables,
self-deﬁned functions, and customized classes, is rarely seen
when training code completion models. If completing a
prompt relies on the understanding of some project-speciﬁc
code, the reasoning is difﬁcult to proceed if such information
is missing from the prompt. In the example,
the class
myRoom is a self-deﬁned Java class, and it would be hard
to recommend the invocation to roomID() without knowing
the list of methods deﬁned in it.

III. QUALITY ESTIMATION BEFORE COMPLETION

Currently, low-return prompts are handled by LCM-based
code completion systems indiscriminately, which leads to

enormous energy and resource waste and threatens their sus-
tainable development. To save the ﬁnancial and environmental
costs, we look into this new research topic: how to selectively
handle code completion requests in a cost-friendly manner
without signiﬁcantly affecting the user experience with the
system. Our empirical study illustrates the feasibility of de-
signing an effective recognizer for such low-return prompts by
identifying their observable patterns. With such a recognizer,
we can avoid the recognized low-return prompts being sent to
the code completion systems to reduce the energy and resource
costs. Therefore, without loss of generality, we propose to
have a lightweight completion quality estimator which foretells
how well a prompt can be completed by an LCM without
proceeding the actual inference, and call the mechanism QEBC
(quality estimation before completion). Various methods can
be adopted to implement the estimator, including machine
learning models and heuristic rules. Prompts will be turned
down without getting completed if the completion quality
is estimated to be lower than a pre-deﬁned threshold. The
strictness of QEBC can be ﬂexibly controlled by adjusting the
threshold. To be speciﬁc, we deﬁne QEBC as follows:

Deﬁnition 1
Given an LCM-based code completion system M and a
code completion prompt p, a completion quality estimator
E will estimate the quality of the completions generated
by M for p before p is actually processed. If the estima-
tion score s does not pass a conﬁgurable threshold t, p
will be rejected.

We demonstrate the workﬂow of a code completion system
equipped with QEBC in Figure 1. Each code completion
prompts sent to the system will be ﬁrst measured by the
estimator. Whenever the estimated completion quality is not
up to a certain threshold, the users will be notiﬁed. They can
choose to further revise the prompts to make them potentially
high-return, or issue the completion request anyway.

A. Technical Challenges

The effectiveness of QEBC depends on how much waste
can be saved. In fact, QEBC is not required to perfectly reject
all of the low-return prompts. It can be applied in practice
as long as it achieves a positive gain, i.e. the waste it saves
is higher than its consumption which mainly comes from its
estimator. Formally, the expected gain for handling one prompt
can be roughly regulated as:

∆E = α × β × ELCM − EQEBC

(1)

where α is the rejection possibility of the prompt, β represents
the possibility that the rejected prompt is not resubmitted by
the user, EQEBC and ELCM are respectively the running cost
of QEBC and the LCM for one prompt. In the context of
QEBC, α is decided by the conﬁgurable threshold, and ELCM
is regarded as a constant since QEBC is orthogonal to the
LCM. Thus, to save more cost, QEBC seeks to obtain higher
β and lower EQEBC, respectively mapping to two challenges:
accuracy and cost-friendliness.

4

Fig. 3: The training process of TCQE.

Accuracy of the estimator. The estimator should be accurate,
i.e., estimating as close as possible to the actual completion
qualities. Estimators with poor accuracy will lead to more re-
submitted completion requests, which harm the productivity
brought by auto-completions and the gain of waste saving.
However, as an emerging problem, the estimator is not yet
well investigated by the research community. It is still unclear
how to design and implement an effective estimator that can
accurately estimate the completion quality of the LCM on a
given code prompt, which calls for more attention from the
research community.
Cost-friendliness of the estimator. Considering the purpose
of QEBC, i.e., reducing the cost of LCM-based code comple-
tion systems, the estimator is required to be cost-friendly in
computing. The cost of the estimator itself is directly related to
the gain of QEBC. Thus, we cannot inﬁnitely enlarge the scale
of the estimator to obtain more accurate completion qualities.
How to balance the trade-off between the effectiveness and
efﬁciency of the estimator requires further consideration.

IV. TRANSFORMER-BASED COMPLETION QUALITY
ESTIMATOR

The estimator is the core of QEBC. Currently, although
no mechanism like ours has been proposed, commercial code
completion systems deﬁne some simplistic rules to block out
extremely uninformative prompts, e.g., Github Copilot refuses
to complete code ﬁles with less than ten characters. Such
rule-based heuristics can be seen as estimators who give
the minimum score to prompts matching the rules. However,
a large population of low-return prompts still escape from
the ﬁltering, such as those presented in Section II-B. There
could be other low-return prompts following more complicated
patterns that are not observed in our small-scale study. Thus,
we desire techniques that could automatically capture the code
semantics of low-return prompts. Considering the outstanding
performance of deep learning (DL) in the semantic under-
standing of code, we seek to design a lightweight DL model
to estimate the performance of a target LCM. Speciﬁcally,
we propose a Transformer-based completion quality estimator
(TCQE) to fulﬁll this task.

TCQE learns to predict an estimation score for a given
code prompt, which is a regression task. The estimation score
should indicate the quality of the completions generated by

the target LCM for the prompt. However, out of privacy and
business considerations, there is no public dataset on the help-
fulness proﬁle of in-production LCM-based code completion
systems.
In this work, we resort to accuracy-based metrics,
e.g., BLEU, as the replacement. They are widely used to
measure the accuracy of model-generated completions as to
the ground truth. Intuitively, completions are less likely to be
helpful if coming with bad accuracy. The accuracy scores can
be conveniently acquired by querying the target LCM. In the
next, we elaborate on the model architecture of TCQE and the
preparation of its training dataset.

A. Training Dataset Preparation

We need a training dataset for TCQE where each data point
contains a prompt and the accuracy score of its completion
generated by the target LCM. We also need the ground truth
completions for the prompts to calculate the accuracy. Similar
to the dataset preparation in the empirical study in section II-B,
we randomly split a code snippet C into two parts, where the
former part Cx works as a prompt and the latter part serves as
its ground truth completion Cy. For the code snippet dataset,
we can reuse the training dataset of the target LCM, which
usually consists of function deﬁnitions collected from open-
source repositories.

(cid:48)

To obtain the completion accuracy for each Cx, we execute
the target LCM M on Cx, obtain its prediction C
y, and cal-
culate its accuracy against the ground truth Cy with respect to
an accuracy metric as s. Besides, for Cx whose corresponding
ground truth Cy is null, i.e., the prompt is in a ﬁnished status,
we directly set its corresponding score to 0 instead of querying
the target LCM. It helps TCQE to recognize prompts in the
Unopen Context pattern as low-return. Finally, by pairing each
Cx with the score s, we obtain a code-score dataset, with
which we train the TCQE model.

B. The Design of TCQE

Transformer [16] has shown outstanding performance on
various code understanding tasks [17]. In this work, we
adopt the Transformer architecture of GPT-2 as the backbone
of TCQE since such Decoder-only architecture is widely
used in current LCM-based code completion systems, such
as TabNine, aiXcoder, and Github Copilot. Adapted for the
regression task, TCQE further attaches a linear head layer to
the Transformer-based backbone. Besides, we limit the number

5

def sum(nums):    sum_num = 0    for num in nums:        sum_num += num    return sum_numdef sum(nums):    sum_num = 0    for num in nums:        sum_num += num    return sum_numCode SnippetCode PromptGround Truth        sum_num += num    return sum_numCompletionTarget LCMScoreMSE LossTCQETransformerLinear LayerGenerating Training DataTraining Estimator0.821.00Estimateof trainable parameters in TCQE to 16 million, a lightweight
scale with promising accuracy shown in Section VI-A.

During the forward propagation, for a training sample
(Cx, s), the code prompt (Cx is ﬁrst tokenized into tokens
w0, w1, ....wn by the Byte-Pair Encoding (BPE) tokenizer [18],
where wi is the i-th token and n is the number of tokens.
Then, the Transformer-based module encodes the tokens into
a sequence of hidden states h0, h1, ....hn. Finally, the linear
head layer yields a prediction on the quality score, s(cid:48), based
on the hidden state of the last token hn, which contains the
semantic information of the whole sequence. The goal of the
training is to minimize the difference between s and s(cid:48), and we
calculate the loss with the Mean Squared Error (MSE). MSE
is a commonly used loss function for regression problems,
calculated as:

MSE(s, s(cid:48)) =

1
N

N
(cid:88)

i=1

(s(cid:48)

i − si)2

(2)

, where N is the number of training samples. The gradients
calculated from the loss function are back-propagated to
update the parameters of the models.

V. EXPERIMENTAL SETUP

This section introduces the research questions, large code
models, baseline estimators, datasets, evaluation metrics, and
implementation details of our experiments. The goal of our
experiments is to evaluate the accuracy and efﬁciency of
TCQE, which further demonstrates the feasibility of QEBC.
Therefore, we propose three research questions and design
several experiments to answer these research questions:
• RQ1: How accurate is the TCQE in estimating the prompts

when applied in QEBC?

• RQ2: How cost-friendly is the TCQE for inference com-

pared with other large models?

• RQ3: What kinds of prompts are more likely to be regarded

as low-return prompts by TCQE?

A. Large Code Models

There are many large code models involved in our experi-
ments, including the target LCMs for training and evaluating
TCQE and the cost baseline large models for measuring the
cost-friendliness of TCQE.

1) Target LCMs: In this work, we adopt two popular LCMs
in our research community, GPT-2 and CodeT5, to serve as
the candidate for the target LCM.
• GPT-2: GPT-2 is a large language model pre-trained on a
large corpus of general texts, like Wikipedia, and has been
used by Tabnine [3], a commercial code completion system.
We ﬁne-tune this model with our code datasets to make it
work on the code completion task. It is also a commonly
used baseline model for code completion in many research
papers [19], [20].

• CodeT5: CodeT5 [21] is an encoder-decoder Transformer
model which employs a uniﬁed framework to seamlessly
support both code understanding and generation tasks. It
is pre-trained on CodeSearchNet dataset which consists of

six programming languages including Python and Java. We
use its off-the-shelf pre-trained model without any further
training.
2) Cost baseline large models:

In our experiments, we
compare the inference cost of TCQE with various large
models. To comprehensively observe the efﬁciency diversity
between TCQE and large models, the models are selected
based on their popularity and scale. Limited by the number
of publicly available LCMs, not all the selected models are
code-related since the inference cost of large models is mainly
decided by their scale and architecture. Thus, the selected
large models for the cost comparison are: GPT-2 [22], Code-
Bert [23], CodeT5 [21], GPT-Neo [24], Megatron [25], T-
NLG [26], and GPT-3 [27]. GPT-2, GPT-Neo, and GPT-3
share the same architecture but differ in some technical details
and training datasets. CodeBert and CodeT5 are two widely
used LCMs in many code-related tasks, while T-NLG and
Megatron are two popular large language models in the ﬁeld
of natural language processing.

B. Baseline Estimators

To the best of our knowledge, as the ﬁrst work for the esti-
mator of QEBC, TCQE does not have comparable counterparts
in related studies. Thus, we design three rule-based baseline
estimators to demonstrate the effectiveness of TCQE better.
• Random (RAND): RAND generates a random ﬂoating
point between 0 and 1 as the estimation score, no matter
what a code prompt is. It works as the control group to
demonstrate the performance of the worst estimator.

• Cyclomatic Complexity (CC): Cyclomatic complexity [28]
is a classic metric for code quality. CC computes the
cyclomatic complexity of a code prompt by v(G) = e−n+2
to be the estimation score, where e and n respectively refer
to the number of edges and nodes in the control ﬂow graph
of the code prompt.

• Characters of Code (COC): Similar to LOC, COC counts
the number of characters in a code prompt to serve as the
estimation score of the code prompt.

To better compare with TCQE, the values returned by the
last two estimators are re-scaled into the internal [0,1]: each
value is divided by the maximum value from its corresponding
estimator on the whole test set.

C. Datasets

We focus on the Java and Python programming languages in
our experiments which have been extensively studied in code-
related DL tasks. Theoretically, TCQE is applicable to general
programming languages. The datasets for Java and Python are
listed as follows.
• Java: We use COFIC [29] as our Java dataset. It is collected
by extracting functions from Java repositories on Github,
containing 1,048,519 code snippets. We split the dataset into
two proportions: 90% for training and 10% for testing.
• Python: The Python dataset is CodeSearchNet (CSN) [30].
It has been pre-split where the train and test split respec-
tively contain 453,772 and 22,175 code snippets.

6

Language LCM Metric Estimator

Java

Python

GPT2

CodeT5

GPT2

CodeT5

BLEU

CBLEU

BLEU

CBLEU

BLEU

CBLEU

BLEU

CBLEU

TABLE I: Results of TCQE and its baselines.
t=0.05

t=0.01

0

0

0

0

t=0.2

t=0.5

1,047
1,404
0

t=0.1
Acc@t #Rejected Acc@t #Rejected Acc@t #Rejected Acc@t #Rejected Acc@t #Rejected
8.1% 10,469
7.1%
RAND
5.3% 32,211
2.8%
COC
8.5% 71,232
0.0%
CC
92.6% 3,920
TCQE
98.2% 1,191
38.1% 10,363
RAND 37.2% 1,049
42.7% 32,211
78.6% 1,404
COC
38.1% 71,232
0.0%
CC
87.5% 16,857
97.3% 4,712
TCQE
19.5% 10,528
1,083
7.8%
RAND
15.3% 32,211
1,404
0.0%
COC
19.9% 71,232
0
0.0%
CC
66.9% 4,320
TCQE
471
98.1%
48.0% 10,534
RAND 43.8% 1,077
48.0% 32,211
70.4% 1,404
COC
48.0% 71,232
0.0%
CC
71.9% 44,088
93.7% 3,420
TCQE
10.2% 2,161
213
6.1%
RAND
10.5% 4,030
296
0.0%
COC
0
0.0%
0
0.0%
CC
66.7%
55
TCQE
171
74.5%
56.5% 2,219
226
RAND 54.0%
71.0% 4,030
296
90.5%
COC
0.0%
0.0%
CC
0
79.1% 5,544
90.3% 1,196
TCQE
27.1% 2,144
229
9.2%
RAND
10.2% 4,030
296
0.0%
COC
0.0%
0
0.0%
CC
65.3% 2,412
119
TCQE
65.5%
76.6% 2,177
216
RAND 73.6%
80.3% 4,030
296
77.4%
COC
0.0%
0.0%
CC
0
84.3% 15,416
95.8% 1,482
TCQE

46.0% 52,466
46.4% 99,436
46.4% 94,637
77.7% 37,130
58.0% 52,146
58.1% 99,436
58.2% 94,637
77.9% 60,870
89.2% 52,596
89.2% 99,436
89.2% 94,637
91.8% 100,045
92.5% 52,609
92.6% 99,436
92.5% 94,637
93.4% 103,147
70.4% 11,109
70.8% 18,245
70.3% 21,965
78.1% 14,768
81.5% 11,132
82.5% 18,245
81.6% 21,965
82.9% 20,969
93.6% 11,023
94.2% 18,245
93.6% 21,965
93.6% 22,106
96.2% 11,063
96.6% 18,245
96.2% 21,965
96.2% 22,154

5,139
7.1%
2.6% 13,580
0.0%
96.6% 2,906
37.6% 5,100
49.2% 13,580
0.0%
93.2% 10,594
9.9%
5,346
0.6% 13,580
0.0%
89.8% 1,245
43.3% 5,295
43.0% 13,580
0.0%
79.7% 21,562
1,029
8.3%
2,143
4.9%
0
0.0%
76.7%
90
55.7% 1,071
78.5% 2,143
0.0%
85.4% 2,727
17.4% 1,060
2,143
0.0%
0
0.0%
69.9%
677
75.3% 1,036
82.0% 2,143
0.0%
88.7% 8,031

26.2% 20,905
28.0% 62,392
26.3% 85,405
87.7% 7,909
45.0% 20,754
46.2% 62,392
44.8% 85,405
81.9% 29,089
60.4% 20,984
61.4% 62,392
60.5% 85,405
73.8% 61,453
72.3% 21,237
73.1% 62,392
72.1% 85,405
81.7% 78,052
39.1% 4,297
43.4% 8,334
39.9% 18,128
70.7% 1,143
65.3% 4,468
74.2% 8,334
67.1% 18,128
77.6% 11,935
47.1% 4,401
27.8% 8,334
45.0% 18,128
71.6% 8,501
78.6% 4,415
78.2% 8,334
79.9% 18,128
81.6% 20,535

0

0

0

0

0

0

0

The training parts of the two datasets are used to train the
TCQE and ﬁne-tune the GPT-2 model, and the test parts are
used to evaluate the performance of TCQE. It is noteworthy
that each code snippet in the test datasets is randomly split
into two parts: the ﬁrst part is the completion prompts, and
the second part serves as the ground truth.

D. Evaluation Metrics

Five popular metrics are adopted in our experiments:

• BLEU: BLEU [31] is widely adopted to approximate the
accuracy of LCMs. It counts the matched n-grams between
an LCM-generated code and its reference. Following Lin et
al. [32], we smooth the BLEU to give non-zero credits to
short completions.

• CristalBLEU (CBLEU): CristalBLEU [33] is a variant of
the BLEU metric, speciﬁcally designed for measuring the
similarity between source code. It ignores the trivially shared
n-grams in the code corpus, hence reducing the noises of
BLEU.

• Accuracy@t (Acc@t): Accuracy@t represents the possi-
bility that the actual completion quality score is also lower
than the threshold t when the estimation score is lower than
t. The threshold set by users reﬂects their expectations of
the low-return prompts. Thus, a high Acc@t suggests that
the estimator can be effectively applied in QEBC.

• Floating point operations (FLOPs): FLOPs is the num-
ber of ﬂoating-point operations needed for executing an
instance. Correspondingly, 1 gitaFLOPs (GFLOPs) indicates
one billion (109) FLOPs. It is a popular metric to measure
the efﬁciency of DL models [34], [35].

• SHAP value: SHAP (SHapley Additive exPlanations) [36]
is a game theoretic approach to explain the output of any
machine learning model. The SHAP value is widely used
to measure the impact of the features on the ﬁnal output
of the model. The positive value represents positive impact
and negative value represents negative impact, where higher
absolute values indicate greater impact.

E. Implementation Details

GPT-2 is ﬁnetuned on a pre-trained version (124M parame-
ters) with our training datasets and CodeT5 is the off-the-shelf
pre-trained model with 223M parameters. We respectively set
the maximum number of input tokens and generated tokens for
one completion to 256 and 10, which is enough for LCMs to
understand the code prompt and provide a full-line suggestion.
The ﬁnetuning of the GPT-2 model and the training of TCQE
respectively take 10 and 30 epochs under a learning rate
of 1e-4 and 5e-5. All the code scripts in our experiments
are available on our website and will be open-sourced once
accepted (cf. Section X).

7

VI. EXPERIMENTAL RESULTS

A. RQ1: Accuracy of TCQE

In this experiment, we evaluate the accuracy of TCQE on
estimating the completion quality of different LCMs when
applied in QEBC. As introduced in Section IV, training a
TCQE model requires a code dataset, a target LCM, and
a metric. Since we have two datasets (Python and Java),
two LCMs (GPT-2 and CodeT5), and two metrics (BLEU
and CBLEU), in total there are eight experimental settings
for evaluating TCQE. Correspondingly, we train eight TCQE
models under the settings and apply them, together with the
three rule-based baseline estimators (RAND, CC, and COC),
to estimate the completion quality of the two LCMs (GPT-
2 and CodeT5). In the evaluation, we follow the workﬂow of
QEBC, i.e., using the estimators to produce an estimation score
for each prompt in the test set and rejecting the prompt if its
score is lower than a speciﬁc threshold. The evaluation results
are demonstrated by the Accuracy@t (Acc@t), which reﬂects
how accurate QEBC can reject low-return prompts according
to the setting of the threshold with the help of TCQE. In this
study, ﬁve threshold values are considered (0.01, 0.05, 0.1, 0.2,
and 0.5) and we also record the number of rejected prompts
(#Rejected) during the evaluations.

The results of each estimator under different settings are
reported in Table I, where the results of TCQE are high-
the eight experimental settings,
lighted in gray. Under all
TCQE rejects the prompts with signiﬁcantly higher accu-
racy compared with the three rule-based baseline estima-
tors. On average of all settings, TCQE respectively rejects
1,581/5,979/11,591/27,327/47,649 prompts with an accuracy
of 89.2%/85.0%/76.8%/78.3%/86.5% at the threshold value
of 0.01/0.05/0.1/0.2/0.5. Note that among the 1,191 prompts
for GPT2 in Java rejected by TCQE with respect to BLEU,
only 1.8% are wrongly rejected, which is an extraordinary
estimation. In contrast, most of the baselines can not achieve
comparable performance. Among the three baselines, the per-
formance of CC is close to the RAND estimator, which sug-
gests that cyclomatic complexity is not capable of completion
quality estimation. COC can produce valid estimations under
speciﬁc settings. However, even under its best-performing
setting, i.e., estimating the GPT2 in Python with respect to
CBLEU, TCQE still rejects more prompts with similar or
higher accuracy, e.g., three times more rejected prompts at
a threshold of 0.01.

Besides, we can observe some diversity in the performance
of TCQE caused by different settings. For example, to reject
around 8,000 prompts for CodeT5 in Python, TCQE achieves
with 88.7% accuracy at t=0.05 for CBLEU while 78.2% at
t=0.2 for BLEU. Similarly, under the same threshold t=0.05,
language Python and metric BLEU, TCQE can reject 171
prompts for GPT2 but 2,421 prompts for CodeT5 with accu-
racy higher than 65%. Such diversity suggests that there is no
silver bullet for the threshold. It should be ﬂexibly conﬁgured
according to the performance of the LCM.

Answer to RQ1: TCQE can accurately reject the prompts
according to the conﬁgurable threshold, which demon-
strates the feasibility of QEBC. Besides, we reveal that
the threshold of TCQE should be set according to the
performance of the LCM.

B. RQ2: Cost-friendliness of TCQE

In this experiment, we evaluate the cost-friendliness of
TCQE and compare it with various large models, including
GPT-2, CodeBert, CodeT5, GPT-Neo, Megatron, T-NLG, and
GPT-3. The number of their trainable parameters ranges from
124M to 175,000M. We measure the cost-friendliness with the
number of ﬂoating point operations (represented by GFLOPs)
cost in the inference. We respectively run one inference, using
128 tokens, with each model in the same environment and
record its GFLOPs and inference time using the Flops Proﬁler
of DeepSpeed [37]. Due to our limited computing resources,
the results of Megatron, T-NLG, and GPT-3 are collected
from the experiments reported by [38]. Their results are still
comparable with ours since the number of parameters and the
GFLOPs for one inference are independent of the experimental
environment. By comparing the GFLOPs and inference time
of the models, we can estimate the computing resources that
TCQE can save.

We report the scale and inference cost of TCQE and each
model in Table II. TCQE has only 16M parameters, which is
signiﬁcantly less than those of the other models, the lowest
of which is 124M. Compared with the other models, TCQE
costs the fewest computing resources (i.e., 0.9 GFLOPs) and
runs fastest (i.e., 9.69 ms) for one inference. In contrast, the
other models require much more GFLOPs, ranging from 72.6
to 740,000 GFLOPs, and more time, at least 23.44ms, for one
inference. These large models consume around 80 times or
even 800,000 times more computational resources than TCQE
does for inference. Thus, according to Equation (1) introduced
in Section III-A, the α×β are respectively 0.012 and 0.003, to
achieve positive gains in computational resources. Speciﬁcally,
in our experiments, α can be represented by the proportion
of rejected prompts and β can refer to the accuracy of the
estimator. Thus, among all the settings in RQ1, we can observe
at least two threshold settings that satisfy this requirement for
obtaining positive gains in computing resources for GPT-2 and
CodeT5. Though the ﬁgure may not be precise considering
the cost of other factors such as communications, it sufﬁces
to demonstrate that TCQE is lightweight, which satisﬁes our
expectation for a feasible estimator.

Answer to RQ2: Compared with large models, TCQE
runs at a very low cost, respectively 1.2% and 0.3% of
the cost of GPT-2 and CodeT5, on computing resources.

C. RQ3: Low-return Prompts

In this experiment, we seek to analyze the prompts that are
regarded as low-return, i.e., being assigned with low estimation
scores, by TCQE. To be speciﬁc, we select three groups of

8

TABLE II: Scale and inference cost of TCQE and other
baseline large models.

Models

Parameters (M) GFLOPs Time (ms)

TCQE
GPT-2
CodeBert
CodeT5
GPT-Neo
Megatron1
T-NLG1
GPT-31
1 Collected from [38]

16
124
125
223
1,316
8,300
17,000
175,000

0.9
72.6
88.2
290.7
1994.9
18,000
36,000
740,000

9.69
33.12
23.44
92.02
112.58
-
-
-

TABLE III: Statistics of each pattern in the reviewed prompts.

Patterns

Low-rated High-rated
#w/ #w/o #w/ #w/o #w/ #w/o ∆s

Random

Meaningless Names 173 210 104 279
165 218
324
Vague Intention
0
Unopen Context
383
12
247 136
Self-unexp. Context 296

59
371
87

154 229 12.7%↓
262 121 19.5%↓
7 376 ∞ ↓
318 65 19.7%↓

prompts from the Java testing dataset according to their estima-
tion scores and review each group to count the appearance of
the four features introduced in Section II-B for subsequent data
analysis. The three groups are respectively rejected prompts
at threshold t=0.1, retained prompts at threshold t=0.5, and
random prompts. Since it
the
prompts from each group, we respectively sample a subset.
The sample size is determined by a statistical formula [39],
i.e.,

to review all

is non-trivial

ss =

z2 ∗ p ∗ (1 − p)/c2
1 + z2∗p∗(1−p)/(c2−1)

population

(3)

, where population is the size of the group (set
to the
size of the entire test dataset for adequate sampling), p is
the standard deviation of the population, c is the conﬁdence
interval (margin of error), z is the Z-Score determined by
the conﬁdence level. In this experiment, we choose to work
with a 95% conﬁdence level (i.e., 1.96 Z-Score according
to [39]), a standard deviation of 0.5 (0.5 is the maximum
standard deviation, which is a safe choice given the exact
ﬁgure unknown), and a conﬁdence interval of 5%. Finally,
we have three subsets, respectively containing 383 completion
prompts. During the review, we follow a similar review process
introduced in Section III-A, i.e., two authors separately judge
whether each prompt meets the deﬁnition of each pattern and
discuss to reconcile all disagreements. The review results are
recorded with four binary values corresponding to the four
patterns, 1 if the pattern is observed, otherwise, 0. Thus,
after the review, every prompt is assigned four binary values
corresponding to the four patterns, i.e., 1 if the pattern is
observed, otherwise, 0.

With the review results, we ﬁrst investigate the random
group by measuring the difference between the estimation
scores of prompts with or without each pattern, i.e., ∆s. As
shown in Table III, a signiﬁcant margin in the estimation
least 12.7%, can be observed in every feature.
scores, at
the average estimation score for prompts with
Especially,

9

Fig. 4: SHAP values of the patterns over all the reviewed
prompts. Each point represents one reviewed prompt, the color
of which indicates whether the prompt follows the pattern (red
for true, blue for false). Limited by space, we use the acronyms
for feature names.

Unopen Context is only 0.005, leading to an extremely high
∆s. Thus, we can draw a similar conclusion to Section II-B,
i.e., the four patterns are also related to the estimation results
of TCQE. Furthermore, compared with the high-rated group,
the number of prompts in the low-rated group that follow any
of the patterns is signiﬁcantly (p-value of the t-test less than
0.001) higher. For example, 165 prompts in the high-rated
group show vague intention while the ﬁgure for the low-
rated group is 324. Therefore, prompts that are regarded as
low-return prompts by TCQE are more likely to follow these
patterns.

impactful

Finally, we seek to quantitatively measure the impact of
each pattern on the ﬁnal estimation score using the SHAP
value. Since SHAP value is designed for machine learning
models, we train a simple XGBoost Classiﬁer with the review
results of the low-rated group and high-rated group, where
the four binary values are the features and the group is the
label. The SHAP values of every pattern for every prompt
are plotted in Figure 4, which shows the distribution of the
the patterns, Vague
impacts each pattern has. Among all
Intention is the most
to the estimation results,
where the SHAP values for following or unfollowing are
signiﬁcantly higher than other patterns. The SHAP values of
both Meaningless Names and Self-unexplainable Context are
lower than 0.5. Interestingly, we notice that in some prompts,
these patterns show a positive impact. It suggests that TCQE
is not sufﬁciently aware of these two patterns, especially Self-
unexplainable Context. Besides,
the prompts with Unopen
Context show a high impact while the ones without it do not
affect the estimation results, which demonstrates the ability of
TCQE to identify Unopen Context. Based on the importance
of these features, developers can optimize their coding to better
cooperate with TCQE.

Answer to RQ3: Prompts with low estimation scores are
more likely to follow the negative patterns summarized
in Section II-B, where Vague Intention is the most im-
pactful one to the estimation result.

1.51.00.50.00.51.0SHAP valueUCSCMNVIVII. RELATED WORK

Neural Code Completion. The deep learning techniques used
for neural code completion are constantly evolving. At the very
beginning, the models used in code completion studies were
dominated by train-from-scratch models, such as Recurrent
Neural Networks (RNN) and their variants [40], [41]. For ex-
ample, Li et al. [40] propose to enhance the LSTM model with
a pointer component to solve the out-of-vocabulary problem
in code completion systems. Such models are lightweight and
can be easily deployed on users’ devices. With the rise of a
new paradigm in deep learning, i.e., pre-train and ﬁne-tune, the
focus of code completion studies has shifted to transformer-
based pre-trained models, including the encoder-decoder [42]
and decoder-only [17], [43] transformer architecture. The pre-
trained models with a large number of trainable parameters,
usually over hundreds of millions, are known as Large Lan-
guage Models (LLM), where LCM is the variant of LLM in
the domain of source code. LCM refers to the large model
that is speciﬁcally trained with a large-scale code corpus, e.g.,
by pre-training a base LLM [23], [21], or ﬁne-tuning a pre-
trained LLM [17], to generate code. Recent studies [44], [17]
have empirically demonstrated the state-of-the-art performance
of LCMs across multiple code-related tasks. For example, a
recently released LCM, AlphaCode [42], can win a ranking
of top 54.3% in programming competitions with more than
5,000 human participants. Motivated by their surprisingly high
performance, LCMs have been widely applied in commercial
code completion systems, including TabNine [3], aiXcoder [2],
Github Copilot [1] and Amazon CodeWhisperer [4]. Generally
speaking, the more parameters in a model the greater the
computational resources required for inference. Small models
are lightweight enough to run on local devices, while large
models have to be deployed on a large number of professional
GPU servers, providing services to clients via APIs. As a
result, using LCMs is costly, both ﬁnancially, due to the cost of
professional hardware, and environmentally, due to the carbon
footprint required to fuel the hardware[34], [35]. This greatly
limits the commercialization and further research of LCMs.
Researchers have proposed many methods to reduce such
costs, such as weight sharing [45], pruning [46] and knowledge
distillation [47]. However, it is still a challenging problem that
calls for community efforts to build an environment-friendly
and sustainable future for LCMs.

Performance Estimation for Neural Models. Estimating the
performance of a neural model without actually training or
testing the model is an important task for many research
ﬁelds. For example, Xia et al. [48] predict the performance of
translation models given their experimental settings to bypass
the computation restriction. Wei et al. [49] estimates the per-
formance of the hyperparameters of neural models to expedite
the automated hyperparameter optimization for meta-learning.
Alshubaily [50] accelerate the ﬁtness evaluation during the
neural architecture search with an end-to-end performance
predictor. These studies illustrate the feasibility of estimating
the performance of neural models, paving the way for our

studies. Different from their work, in this work, QEBC is
proposed for LCM-based code completion systems which
is a brand-new and important ﬁeld with a number of new
challenges to address.

VIII. THREATS TO VALIDITY

Metrics for TCQE. Since manually constructing a dataset for
training a TCQE model is unfeasible, we use automatic evalu-
ation metrics, BLEU and CristalBLEU, to construct a dataset
for training TCQE models in our experiments. However, such
exact-match-based metrics do not consider the code semantics,
so may not be very reliable in measuring the quality of the
code completions. The shortcomings of such metrics have been
investigated by the research community [51], [52], but still
remain a signiﬁcant challenge. For TCQE, the accept rate of
the code completion is the most desirable metric. However, it
needs to be collected from a production environment, which is
beyond our capabilities. We, therefore, call for more industrial
participation to facilitate further research on this topic.

Generalization. Limited by our computing resources, we only
conduct experiments on LCMs with parameters less than one
billion. In theory, our approach is capable of any LCMs.
Yet, the effectiveness of TCQE in larger LCMs has not been
experimentally veriﬁed. Besides, we only evaluate TCQE on
two programming languages, Java and Python, which is also
a threat to the generalizability of our model.

Threshold. QEBC rejects the completion prompts according
to their estimation scores with a pre-set threshold. Though
we have explored several threshold settings in Section VI-A,
it is still a non-trivial task to set a proper threshold for an
LCM-based code completion system considering its complex
production environment. Luckily, TCQE is lightweight enough
for a conservative threshold in practice, i.e., positive gains can
be achieved by rejecting only a small percentage of requests
(estimated to be as small as about 1.2% for GPT-2 and 0.3%
for CodeT5 in Section VI-B). Further studies on the settings
of the threshold for the optimal saving effects are desired.

IX. CONCLUSION

We pointed out

the proﬁtless code completion problem
in the context of neural code completion due to low-return
prompts, which not only poses a threat to the productivity but
only leads to a huge waste of computing resources and energy.
We conducted an empirical study via manual inspection to
analyze the patterns of low-return prompts, resulting in four
patterns that are frequently observed to cause proﬁtless code
completions. We proposed the ﬁrst early-rejection mechanism
to turn down lower-return prompts based on quality estimation
before completion (QEBC). QEBC allows to prevent proﬁtless
code completions by estimating the completion quality of the
LCM before a query is actually processed and rejecting the
query if the estimation cannot pass a threshold. Furthermore,
we propose a lightweight Transformer-based estimator, TCQE,
to estimate the completion quality for a given prompt. The
experimental results show that TCQE can accurately reject

10

low-return prompts based on various threshold settings, which
demonstrates the feasibility of the mechanism. In the future,
we seek to design estimators that are more effective and
lightweight
to facilitate the industrial applications of our
mechanism.

X. DATA AVAILABILITY

We have released the code scripts for each experiment,

dataset, and experimental results on our website [14].

11

REFERENCES

[1] G. Copilot,

“Github
https://copilot.github.com/, june 10, 2022.

copilot

·

your

ai

pair

programmer.”

[2] aiXcoder, “aixcoder,” https://www.aixcoder.com/en/, june 10, 2022.
[3] TabNine,

completions — tabnine,”

faster with

“Code

ai

https://www.tabnine.com/, june 10, 2022.

CodeWhisperer,

[4] A.
–
https://aws.amazon.com/codewhisperer/, july 1, 2022.

“Ml-powered
–

codewhisperer

amazon

amazon

coding

web

companion
services,”

[5] A. Ziegler, E. Kalliamvakou, S. Simister, G. Sittampalam, A. Li, A. S.
Rice, D. Rifkin, and E. Aftandilian, “Productivity assessment of neural
code completion,” ArXiv, vol. abs/2205.06537, 2022.

[6] P. Vaithilingam, T. Zhang, and E. L. Glassman, “Expectation vs. experi-
ence: Evaluating the usability of code generation tools powered by large
language models,” in CHI Conference on Human Factors in Computing
Systems Extended Abstracts, 2022, pp. 1–7.

[7] S. Barke, M. B. James, and N. Polikarpova, “Grounded copilot:
How programmers interact with code-generating models,” ArXiv, vol.
abs/2206.15000, 2022.

[8] A. R.-V. Wynsberghe, “Sustainable ai: Ai for sustainability and the

sustainability of ai,” AI Ethics, vol. 1, pp. 213–218, 2021.

[9] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. A. Patwary, V. A.
Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro,
A. Phanishayee, and M. A. Zaharia, “Efﬁcient large-scale language
model training on gpu clusters using megatron-lm,” Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, 2021.

[10] D. A. Patterson, J. Gonzalez, Q. V. Le, C. Liang, L.-M. Mungu´ıa,
D. Rothchild, D. R. So, M. Texier, and J. Dean, “Carbon emissions
and large neural network training,” ArXiv, vol. abs/2104.10350, 2021.

[11] G. Leopold, “Aws to offer nvidia´s t4 gpus for ai

inferencing,”

[12] J. Barr,

https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-
inference-platform/, june 10, 2022.
update
“Amazon

aws
inf1
for high performance cost-effective inferencing,”

inferentia chips
https://aws.amazon.com/cn/blogs/aws/amazon-ec2-update-inf1-
instances-with-aws-inferentia-chips-for-high-performance-cost-
effective-inferencing/, june 10, 2022.

instances with

ec2

–

[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, “All
nlp tasks are generation tasks: A general pretraining framework,” ArXiv,
vol. abs/2103.10360, 2021.

[14] Anonymous, “Source code,” https://anonymous.4open.science/r/TCQE-

0FC7, sep 2, 2022.

“Code

[15] ORACLE,
gramming
https://www.oracle.com/java/technologies/javase/codeconventions-
namingconventions.html, june 10, 2022.

for
naming

conventions

language:

the

java
pro-
conventions,”

9.

[16] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” ArXiv,
vol. abs/1706.03762, 2017.

[17] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Kaplan, H. Edwards,
Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger,
M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder,
M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. W. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-
Voss, W. H. Guss, A. Nichol, I. Babuschkin, S. A. Balaji, S. Jain,
A. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. M.
Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating
large language models trained on code,” ArXiv, vol. abs/2107.03374,
2021.

[18] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of
rare words with subword units,” ArXiv, vol. abs/1508.07909, 2016.
[19] Z. Sun, X. Du, F. Song, M. Ni, and L. Li, “Coprotector: Protect open-
source code against unauthorized training usage with data poisoning,”
Proceedings of the ACM Web Conference 2022, 2022.

[20] R. Schuster, C. Song, E. Tromer, and V. Shmatikov, “You autocomplete
me: Poisoning vulnerabilities in neural code completion,” USENIX
Security Symposium, 2021.

[21] Y. Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, “Codet5: Identiﬁer-
aware uniﬁed pre-trained encoder-decoder models for code understand-
ing and generation,” ArXiv, vol. abs/2109.00859, 2021.

[22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,

“Language models are unsupervised multitask learners,” 2019.

12

[23] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained model for
programming and natural languages,” ArXiv, vol. abs/2002.08155, 2020.
[24] L. Gao, S. R. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,
J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy,
“The pile: An 800gb dataset of diverse text for language modeling,”
ArXiv, vol. abs/2101.00027, 2021.

[25] M. Shoeybi, M. A. Patwary, R. Puri, P. LeGresley, J. Casper, and
B. Catanzaro, “Megatron-lm: Training multi-billion parameter language
models using model parallelism,” ArXiv, vol. abs/1909.08053, 2019.

[26] M. Research, “Turing-nlg: A 17-billion-parameter

- microsoft

by microsoft
us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-
microsoft/, july 1, 2022.

language model
research,” https://www.microsoft.com/en-

[27] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
Voss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei, “Language models are few-shot learners,” ArXiv, vol.
abs/2005.14165, 2020.

[28] T. J. McCabe, “A complexity measure,” IEEE Transactions on Software

Engineering, vol. SE-2, pp. 308–320, 1976.

[29] Z. Sun, L. Li, Y. Liu, and X. Du, “On the importance of build-
ing high-quality training datasets for neural code search,” ArXiv, vol.
abs/2202.06649, 2022.

[30] H. Husain, H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code search,”
ArXiv, vol. abs/1909.09436, 2019.

[31] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for

automatic evaluation of machine translation,” in ACL, 2002.

[32] C.-Y. Lin and F. J. Och, “ORANGE: a method for evaluating automatic
for machine translation,” in COLING 2004:
evaluation metrics
Proceedings of the 20th International Conference on Computational
Linguistics. Geneva, Switzerland: COLING, aug 23–aug 27 2004,
pp. 501–507. [Online]. Available: https://www.aclweb.org/anthology/
C04-1072

[33] A. Eghbali and M. Pradel, “Crystalbleu: Precisely and efﬁciently mea-
suring the similarity of code,” 2022 IEEE/ACM 44th International
Conference on Software Engineering: Companion Proceedings (ICSE-
Companion), pp. 341–342, 2022.

[34] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-

erations for deep learning in nlp,” ArXiv, vol. abs/1906.02243, 2019.

[35] R. Desislavov, F. Mart’inez-Plumed, and J. Hern’andez-Orallo, “Com-
pute and energy consumption trends in deep learning inference,” ArXiv,
vol. abs/2109.05472, 2021.

[36] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting model

predictions,” ArXiv, vol. abs/1705.07874, 2017.

[37] DeepSpeed, “microsoft/deepspeed: Deepspeed is a deep learning op-
timization library that makes distributed training and inference easy,
efﬁcient, and effective.” https://github.com/microsoft/DeepSpeed, july 1,
2022.

[38] A. Gholami, Z. Yao, S. Kim, M. W. Mahoney, and K. Keutzer, “Ai and

memory wall,” RiseLab Medium Post, 2021.

[39] R. V. Krejcie and D. W. Morgan, “Determining sample size for research
activities,” Educational and Psychological Measurement, vol. 30, pp.
607 – 610, 1970.

[40] J. Li, Y. Wang, M. R. Lyu, and I. King, “Code completion with neural
attention and pointer networks,” ArXiv, vol. abs/1711.09573, 2018.
[41] A. Svyatkovskiy, Y. Zhao, S. Fu, and N. Sundaresan, “Pythia: Ai-assisted
code completion system,” Proceedings of
the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining,
2019.

[42] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
Tom, Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy,
C. de, M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl,
S. Gowal, Alexey, Cherepanov, J. Molloy, D. J. Mankowitz, E. S.
Robson, P. Kohli, N. de, Freitas, K. Kavukcuoglu, and O. Vinyals,
“Competition-level code generation with alphacode,” ArXiv, vol.
abs/2203.07814, 2022.

[43] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
and C. Xiong, “A conversational paradigm for program synthesis,”
ArXiv, vol. abs/2203.13474, 2022.

[44] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B.
Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou,
M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng,
S. Fu, and S. Liu, “Codexglue: A machine learning benchmark dataset
for code understanding and generation,” ArXiv, vol. abs/2102.04664,
2021.

[45] S. Rothe, S. Narayan, and A. Severyn, “Leveraging pre-trained check-
points for sequence generation tasks,” Transactions of the Association
for Computational Linguistics, vol. 8, pp. 264–280, 2020.

[46] A. See, M.-T. Luong, and C. D. Manning, “Compression of neural

machine translation models via pruning,” in CoNLL, 2016.

[47] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A

survey,” ArXiv, vol. abs/2006.05525, 2021.

[48] M. Xia, A. Anastasopoulos, R. Xu, Y. Yang, and G. Neubig, “Pre-

dicting performance for natural language processing tasks,” ArXiv, vol.
abs/2005.00870, 2020.

[49] Y. Wei, P. Zhao, and J. Huang, “Meta-learning hyperparameter perfor-

mance prediction with neural processes,” ICML, 2021.

[50] I. Alshubaily, “Efﬁcient neural architecture search with performance

prediction,” ArXiv, vol. abs/2108.01854, 2021.

[51] M. Evtikhiev, E. Bogomolov, Y. Sokolov, and T. Bryksin, “Out of the
bleu: how should we assess quality of the code generation models?”
ArXiv, vol. abs/2208.03133, 2022.

[52] D. Roy, S. Fakhoury, and V. Arnaoudova, “Reassessing automatic
evaluation metrics for code summarization tasks,” Proceedings of the
29th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering, 2021.

13

