Assessing the Quality of Computational Notebooks
for a Frictionless Transition from Exploration to Production
Luigi Quaranta
luigi.quaranta@uniba.it
University of Bari
Bari, Italy

2
2
0
2

y
a
M
4
2

]
E
S
.
s
c
[

1
v
1
4
9
1
1
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
The massive trend of integrating data-driven AI capabilities into tra-
ditional software systems is rising new intriguing challenges. One
of such challenges is achieving a smooth transition from the explo-
rative phase of Machine Learning projects – in which data scientists
build prototypical models in the lab – to their production phase –
in which software engineers translate prototypes into production-
ready AI components. To narrow down the gap between these
two phases, tools and practices adopted by data scientists might
be improved by incorporating consolidated software engineering
solutions. In particular, computational notebooks have a promi-
nent role in determining the quality of data science prototypes.
In my research project, I address this challenge by studying the
best practices for collaboration with computational notebooks and
proposing proof-of-concept tools to foster guidelines compliance.

KEYWORDS
Software Engineering, Artificial Intelligence, Data Science, Machine
Learning, computational notebooks, static analysis tools, linters

1 RESEARCH PROBLEM
Over the past few years, Artificial Intelligence (AI) and Machine
Learning (ML) have seen unprecedented development, thus en-
abling the design of intelligent software solutions in a large and
growing number of application domains. As a direct consequence,
the integration of AI-powered capabilities in traditional software
systems has become a massive trend [1].

Such an integration, however, poses a large number of complex
challenges [2, 8, 13–15], in part inherited from the fields of Software
Engineering (SE) and Data Science (DS), in part completely new
[31], arising from the need to translate lab model prototypes into
production-ready AI/ML components [10–12].

Despite the availability of high-level libraries and frameworks
that make the development of AI/ML prototypes a relatively easy
task to achieve – even for professionals lacking a computer science
background – building reliable and scalable AI systems remains
a complex endeavor. Uninformed misuses of low-code ML frame-
works typically end up being the source of new kinds of technical
debt [3, 27, 29].

A possible way to approach the multifaceted challenge of devel-
oping production-ready AI components is to apply – early in the
ML model building process – suitable adaptations of consolidated
Software Engineering solutions [28, 30, 34]. For instance, practi-
tioners have already started leveraging Continuous Integration (CI)

and Delivery (CD) in ML [26]. As a doctoral student, I have em-
braced this perspective and tried to address – through the lens of a
software engineer – the research problem of achieving a smoother
translation of AI/ML prototypes to production.

So far, the focus of my research has been on the workflow and
tooling employed by DS teams in the early stages of AI/ML projects,
with a particular emphasis on one of the most popular tools among
data scientists: the computational notebook [25].

2 HYPOTHESIS
Having had the opportunity to complement a preliminary literature
review with qualitative research conducted at an Italian Data Sci-
ence company, early in my PhD studies I have been able to detect
and characterize a tension between the initial, exploitative phase
of an AI/ML project and the final, production phase. This tension,
mainly caused by a shift in the goals of the two phases, is exacer-
bated by the different educational backgrounds of the stakeholders
involved and the different tools in use. This idea is illustrated in
Figure 1, depicting a typical ML workflow.

In the initial phase of an AI/ML project – which we will call
the Exploration phase – data scientists generally build prototypical
models using computational notebooks, i.e., interactive documents
combining code and natural language text in a cohesive narrative
of experimental trials. Their main goal is to timely deliver the best-
performing models to the rest of their team.

Later, in the final phase of a project – hereafter referred to as
the Production phase – the best ML prototypes are handed off to
software engineers, who are typically in charge of bringing them
to production as live services; to achieve this, software engineers
leverage consolidated SE solutions (e.g., VCS, testing frameworks,
static code analyzers, etc.) and practices (e.g., agile practices, CI/CD).
Their goal is to build robust and scalable AI components out of ML
prototypes and to consolidate the resulting process into a repro-
ducible pipeline.

Transitioning from Exploration to Production is not easy, even
for companies that rely on well-established teams and procedures.
Typically, the two phases have no clear boundaries: it is not easy
to define when, in an ML workflow like the one in Figure 1, the
transition should happen and who is accountable for it: should data
scientists be in charge of raising the quality of their code to the
production standards? Or should software engineers be account-
able for organizing and improving prototypical code, making it
production-ready?

My research work is based on the following assumption: incorpo-
rating software engineering solutions into the tools and practices

 
 
 
 
 
 
Luigi Quaranta

Figure 1: Tension between exploration and production in the typical ML workflow.

adopted by data scientists in the early stages of AI/ML projects
can smooth out the friction that is currently experienced by practi-
tioners in the transition between the aforementioned two phases
and reduce the “cultural” mismatch between DS and SE profession-
als. Furthermore, given the widespread adoption of computational
notebooks within the data science community, I hypothesize that
engineering-aware enhancements of popular notebook platforms
like Jupyter Notebook [19] would produce the most tangible effects
on the production-readiness of prototypical ML artifacts.

3 EXPECTED CONTRIBUTIONS
With my PhD research work, I expect to contribute to the devel-
opment of better, SE-aware collaborative practices in data science
teams, with a focus on the use of computational notebooks.

Collaboration in data science teams happens across a wide range
of tools and practices [35]. Among these, computational notebooks
– and in particular Jupyter Notebook – have seen a steadily growing
adoption by data scientists worldwide as they offer a fast and inter-
active prototyping environment, a lightweight form of experiment
documentation, and innovative collaboration capabilities, powered
by modern web technologies [16]. Nonetheless, the notebook for-
mat has also been criticized for inducing bad programming practices
(e.g., code fragmentation and non-linear execution of code) as well
as for offering scarce-to-null native support for Software Engineer-
ing best practices (even basic ones, such as code modularization,
testing, and versioning) [5]. Consequently, many researchers have
started investigating the quality of Jupyter notebooks and proved
that they are too often (i) inundated by poor-quality code [9, 17, 33],
(ii) scarcely reproducible [17], and (iii) superficially documented
[25] – notwithstanding their support for richly formatted text and
multimedia output.

To counter these phenomena, some scholars have already pro-
posed best practices for better use of computational notebooks
[17, 23], although not providing scientific validation for them. Build-
ing on their work, I have already contributed by collecting and val-
idating a comprehensive list of best practices for the collaborative
use of computational notebooks in a professional context.

Other researchers have built their own extensions for the Jupyter
ecosystem, aimed at supporting the creation of better notebooks.
The contributions range from notebook-specific versioning systems

[7], to tools that improve the readability [24] and the communi-
cation capabilities of notebooks [32]. Head et al. also proposed
an extension for the extraction of clean code from notebook cells,
based on the desired output [6]. Adding on these efforts, I plan
to contribute by (i) reviewing the currently available software so-
lutions for the reproducibility of AI/ML experiments – with an
emphasis on notebook support – and (ii) designing, developing,
and validating a linter for Jupyter Notebook, aimed at fostering the
adoption of validated best practices for the collaborative use of this
tool.

Ultimately, I will further investigate the problem of notebook
reproducibility, in an original attempt to link it with the adop-
tion/violation of best practices.

4 EVALUATION PLAN
As stated in the previous section, one of the major expected contri-
butions of my research is the collection and validation of a catalog
of best practices for the collaborative use of computational note-
books. At present, I have already collected the best practices by
means of a multivocal literature review (see Study 3 in Sect. 5.3).
For their validation, I followed a mixed approach: first, I qualita-
tively assessed the appropriateness and completeness of the catalog
by conducting interviews with expert data scientists; then, I quan-
titatively assessed the adoption of the best practices by analyzing a
selection of notebooks collaboratively developed by experts.

As for the second major contribution of my research, i.e., the de-
velopment of a linter for Jupyter Notebook documents (see Study 5
in Sect. 6.1), I plan to validate it with a field study, to be performed
in a professional context with professional data science teams.

Finally, to investigate the relationship between the reproducibil-
ity of computational notebooks and the adoption of best practices
for their development, I plan to perform a further archival study
(see Study 6 in Sect. 6.2). This time, by leveraging the validated
linter, I will seek for best practice violations, while also trying to
reproduce the assessed notebooks.

5 PRELIMINARY RESULTS
In the following, I present the results achieved so far in my research
work, going through the studies completed in the last two years.

Problem UnderstandingModelDeploymentDataPreparationData Analysis /Model BuildingExplorationData ScientistsComputational NotebooksTimelinessProductionSoftware EngineersVersioned codeReproducibilityFeat. EngineeringModel TrainingModel EvaluationModel ServingModel MonitoringData CollectionData CleaningData LabelingAssessing the Quality of Computational Notebooks

5.1 Study 1: Bringing ML models to

production: an industry perspective from
data scientists

I started my research project by studying the challenges that lie
in the integration of data-driven AI components into traditional
software systems.

Upon completing a preliminary literature review on the subject,
just a few weeks before the pandemic outbreak, my research group
had the chance to get in touch with an Italian consulting company
focused on data science, operating in the financial domain. There,
we organized a workshop on the reproducibility of AI/ML experi-
ments. The workshop was attended by 18 data scientists with varied
educational backgrounds. After an ice-breaking presentation on the
existing software solutions for achieving reproducibility in AI/ML
experiments, the event culminated in an inspiring brainstorming
session on the topic of ML models productization. After the work-
shop, I performed a thematic analysis of the transcripts from the
event, extracting the most relevant themes.

One of the topics that soon catalyzed the debate was the role
of computational notebooks in the typical ML workflow. Some of
the attendees deemed notebooks as indispensable tools, mainly
because of their storytelling capability; when a data scientist needs
to reconstruct the history and reasoning behind an experiment,
having a well-written computational narrative is – in practice –
more useful than VCS or other tracking mechanisms. Others pro-
moted the use of computational notebooks alongside traditional
IDEs: consolidated code fragments should be regularly transferred
from notebook cells to a versioned and structured codebase. On
the other end of the spectrum, a few attendees claimed that note-
books should be dropped as soon as possible during the workflow,
immediately after the data exploration stage. Interestingly though,
none of the attendees questioned the importance of computational
notebooks: the discussion dealt, instead, with the right time in the
workflow in which notebooks should be dismissed to give way to
standard code.

Besides discussing the notebook lifecycle, some of the data scien-
tists also expressed the desire for specific notebook features: e.g., the
native support for testing and versioning (since the data exploration
stage), along with concrete aids to improve the reproducibility of
computations within the Jupyter ecosystem.

Lastly, some of the attendees underlined the importance of defin-
ing a set of validated best practices, to be shared across the team, and
of fostering knowledge-sharing opportunities, to smooth out the
cultural differences between data scientists and software engineers.
This study was accepted and presented at WAIN 2021 [10]. A few
weeks after the workshop, I also administered a couple of surveys
and performed some follow-up interviews with the members of
the data science team of the company, with the specific aim of
investigating their typical workflow. Overall, the insights gained
in this phase constitute the foundation of the research hypothesis
stated in Sect. 2 and have inspired the focus of the rest of my work.

5.2 Study 2: Software solutions for the

reproducibility of AI/ML experiments
To gain a better understanding of the theme of reproducibility in
data-driven AI experiments, I performed an exploratory review

of the available software solutions that support its achievement.
I tested each of the examined solutions against a predefined case
study, taking notes about specific tool capabilities and their impact
on the workflow. From this practical experience, I derived a taxon-
omy1 that can be leveraged by practitioners to orient themselves
in the vast and growing offer of tools in this category. The details
about the taxonomy are reported in an article that was accepted
and will be presented at AIxIA 2021 [21].

5.3 Study 3: Best practices for collaboration

with computational notebooks

Inspired by the results of the workshop described in Sect. 5.1, I
started focusing my attention on computational notebooks. In par-
ticular, to meet the need for a set of validated best practices, I
investigated the following research questions: (RQ1) “What are
the best practices for collaboration with computational notebooks?”
and (RQ2) “What collaboration-specific best practices do experienced
data scientists actually follow when working with computational
notebooks?”

To answer RQ1, I conducted a multivocal literature review [4],
i.e., a systematic review in which sources can belong to both white
literature (e.g., peer-reviewed papers published in journals or con-
ference proceedings) or grey literature (e.g., informal documents
produced by professionals, as technical blog posts). Upon perform-
ing a thematic analysis of the retrieved articles, I was able to collect
a catalog of 17 best practices, organized in 6 main themes, for the
collaborative use of computational notebooks.

To answer RQ5, I performed a qualitative and quantitative as-
sessment of the adoption of the best practices from the catalog. The
former was based on semi-structured interviews with 22 experi-
enced data scientists from different companies; all interviewees
had at least one year of experience with Jupyter Notebook. At the
beginning of each interview, to provide some context, I broadly
introduced the main themes that emerged in the multivocal litera-
ture review, although not revealing any specific guideline. Then,
I asked participants to share their personal best practices for the
collaborative use of computational notebooks.

As for the quantitative assessment, it was performed by means
of an archival study on a dataset of 1,380 Python Juyter notebooks
taken from kaggle.com. Kaggle is a Google-owned platform hosting
ML competitions for data scientists of all levels of expertise and
offering a cloud-based data science environment in which users can
write scripts and notebooks in the Jupyter format. A daily dump of
the platform metadata is available as a Kaggle dataset named “Meta
Kaggle2.” From this dataset, I assembled the collection of notebooks
used in this study by filtering and downloading notebooks that
were collaboratively authored by expert Kaggle users.

Overall, I found that practitioners are generally aware of the best
practices available in the literature, although they do not always
apply them. Indeed, the recommended behaviors are often deemed
unfeasible or even counterproductive in definite contexts, mainly
due to the lack of time and proper tool support. By reasoning
on these results, I was able to identify possible enhancements for

1Available at https://github.com/collab-uniba/Software-Solutions-for-Reproducible-
ML-Experiments.
2https://www.kaggle.com/kaggle/meta-kaggle

notebook development environments like Jupyter Notebook. This
study is currently under minor revision at CSCW 2022 [22].

5.4 Study 4: A dataset of Python Jupyter

notebooks from Kaggle

Having personally experienced the effort of putting together a
dataset of computational notebooks with rich metadata for my
archival studies, I decided to leverage the familiarity gained with the
Kaggle platform to build KGTorrent, a dataset of Python Jupyter
notebooks that I made publicly available to the research community.
KGTorrent consists of two main components: (1) the actual
corpus – comprising 248,761 Python Jupyter notebooks – and (2) a
companion MySQL database, derived from Meta Kaggle. Besides
notebook metadata, the companion database stores comprehensive
information about Kaggle users and competitions. The scripts de-
veloped to assemble KGTorrent, available on GitHub,3 can be used
to replicate the collection as well as to update it according to newer
versions of Meta Kaggle.

The paper on KGTorrent was accepted and presented at MSR
2021 [20]. At present, I am also finalizing the development of
an RPC-style web API for KGTorrent, to provide interested re-
searchers with means to easily access the collection and filter it
according to their specific research questions.

6 COMPLETION PLAN
As clarified next, I plan to complete my research project with the
development and validation of a proof-of-concept tool, aimed at
improving the collaborative practices around computational note-
books, with the ultimate goal to smooth out the transition from ex-
ploration to production in AI/ML projects (see Study 5 in Sect. 6.1).
Moreover, I plan to perform an archival study to assess the impact
of the adoption of best practices on notebook reproducibility (see
Study 6 in Sect. 6.2).

6.1 Study 5: pynblint, a linter for Python

Jupyter notebooks

At present, I am designing and developing a linting library for com-
putational notebooks, named pynblint. Specifically, the library will
perform the analysis of data science projects containing Python
Jupyter notebooks: each notebook in a project repository will be
checked against a predefined set of rules. Such rules will be based
on operationalizations of the best practices from the catalog in-
troduced in Study 3 (see Sect. 5.3): pynblint will warn its users
when they do not comply with the guidelines and promptly suggest
corrective actions. Julynter, a similar contribution by Pimentel et
al. [18], is implemented as a plugin for a specific notebook platform:
JupyterLab. Differently, pynblint will be a static code analyzer for
all notebooks in the Jupyter format, regardless of the platform used
to write them. It will work both as a standard CLI application – to
be seamlessly integrated into CI/CD pipelines – and as a modern
web app – to be easily accessed by beginners. I plan to conclude
this activity by February 2022.

In the following three months, after a pilot test of the tool with
student volunteers from a data science course of my university,

3https://github.com/collab-uniba/KGTorrent

Luigi Quaranta

I plan to conduct a thorough validation of pynblint in the field,
by involving one or more professional data science teams. The ex-
pected output of this validation process is two-fold: (i), the collected
feedback will enable a refinement of the linting library; (ii) I will be
able to gather further statistics on the adoption of the guidelines,
this time enriched with contextual information (i.e., the educational
background of the notebook author, the application domain of the
project, the time-span allotted to the project, etc.); consequently, I
will be enabled to determine, in each context, which of the proposed
fixes are typically adopted and which ignored.

6.2 Study 6: Sharing reproducible notebooks –

are best practices enough?

Non-reproducible notebooks hinder collaboration and hamper the
transition from exploration to production. To address this further
concern, I will conclude my research project by investigating the
following research questions: (RQ3) “Is there a relationship between
the reproducibility of computational notebooks and the adoption of
the best practices for their collaborative development?” and (RQ4)
“What are the guidelines that are most beneficial to the reproducibility
of computational notebooks?”. I plan to accomplish this investigation
by the end of June 2022.

To answer the questions, upon completion of the pynblint vali-
dation process, I will leverage the linter and build a record of the
guideline violations found in a dataset of Python Jupyter notebooks,
while also testing the reproducibility of the related computations.
Then, I will be able to statistically determine the existence of a
relationship between the adoption of the best practices and the
reproducibility of computational notebooks.

7 CONCLUSION
In my research project, I address the challenging translation of
AI/ML prototypes into production-ready AI components. In par-
ticular, given their widespread adoption among data scientists, I
have investigated the best practices for the collaborative use of
computational notebooks. Furthermore, in the last year of my PhD,
I will conclude the development and validation of a dedicated linter
to check and foster compliance of data science repositories with
the guidelines. I plan to defend my doctoral thesis in early 2023.

REFERENCES
[1] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
Software Engineering for Machine Learning: a Case Study. In Proceedings of the
41st International Conference on Software Engineering: Software Engineering in
Practice. IEEE Press, 291–300.

[2] Anders Arpteg, Björn Brinne, Luka Crnkovic-Friis, and Jan Bosch. 2018. Software
engineering challenges of deep learning. In 2018 44th euromicro conference on
software engineering and advanced applications (SEAA). 50–59. tex.organization:
IEEE.

[3] Justus Bogner, Roberto Verdecchia, and Ilias Gerostathopoulos. 2021. Characteriz-
ing Technical Debt and Antipatterns in AI-Based Systems: A Systematic Mapping
Study. In 2021 IEEE/ACM International Conference on Technical Debt (TechDebt).
IEEE, Madrid, Spain. https://doi.org/10.1109/TechDebt52882.2021.00016 arXiv:
2103.09783.

[4] Vahid Garousi, Michael Felderer, and Mika V. Mäntylä. 2019. Guidelines for
including grey literature and conducting multivocal literature reviews in software
engineering. Information and Software Technology 106 (2019), 101 – 121. https:
//doi.org/10.1016/j.infsof.2018.09.006

[5] Joel Grus. 2018. I don’t like notebooks. https://conferences.oreilly.com/jupyter/

jup-ny/public/schedule/detail/68282.html

applications. https://martinfowler.com/articles/cd4ml.html

[27] David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-
mar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan
Dennison. 2015. Hidden technical debt in machine learning systems. In Advances
in neural information processing systems. 2503–2511.

[28] Alex Serban, Koen van der Blom, Holger Hoos, and Joost Visser. 2020. Adoption
and Effects of Software Engineering Best Practices in Machine Learning. In
Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software
Engineering and Measurement (ESEM) (ESEM ’20). Association for Computing
Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3382494.3410681
tex.ids= serbanAdoptionEffectsSoftware2020.

[29] Yiming Tang, Raffi Khatchadourian, Mehdi Bagherzadeh, Rhia Singh, Ajani Stew-
art, and Anita Raja. 2021. An empirical study of refactorings and technical
debt in machine learning systems. In 2021 IEEE/ACM 43rd international con-
ference on software engineering (ICSE). IEEE, Madrid, Spain, 238–250. https:
//doi.org/10.1109/ICSE43902.2021.00033 tex.organization: IEEE.

[30] Bart van Oort, Luís Cruz, Maurício Aniche, and Arie van Deursen. 2021. The
Prevalence of Code Smells in Machine Learning projects. In 2021 IEEE/ACM 1st
Workshop on AI Engineering - Software Engineering for AI (WAIN). IEEE, Madrid,
Spain. https://doi.org/10.1109/WAIN52551.2021.00011 arXiv: 2103.04146.
[31] Zhiyuan Wan, Xin Xia, David Lo, and Gail C. Murphy. 2019. How does Machine
Learning Change Software Development Practices? IEEE Transactions on Software
Engineering (2019), 1–1. https://doi.org/10.1109/TSE.2019.2937083

[32] April Yi Wang, Zihan Wu, Christopher Brooks, and Steve Oney. 2020. Callisto:
Capturing the "Why" by Connecting Conversations with Computational Narra-
tives. In Proceedings of the 2020 CHI Conference on Human Factors in Computing
Systems. ACM, Honolulu HI USA, 1–13. https://doi.org/10.1145/3313831.3376740
[33] Jiawei Wang, Li Li, and Andreas Zeller. 2020. Better code, better sharing: On the
need of analyzing jupyter notebooks. In Proc. of the ACM/IEEE 42nd International
Conference on Software Engineering: New Ideas and Emerging Results. ACM, 53–56.
https://doi.org/10.1145/3377816.3381724

[34] Hironori Washizaki, Hiromu Uchida, Foutse Khomh, and Yann-Gael Gueheneuc.
2019. Studying Software Engineering Patterns for Designing Machine Learning
Systems. 2019 10th International Workshop on Empirical Software Engineering in
Practice (IWESEP) (2019), 49–495. https://doi.org/10.1109/IWESEP49350.2019.
00017 arXiv: 1910.04736.

[35] Amy X. Zhang, Michael Muller, and Dakuo Wang. 2020. How do Data Science
Workers Collaborate? Roles, Workflows, and Tools. Proceedings of the ACM
on Human-Computer Interaction 4, CSCW (May 2020), 022:1–022:23.
https:
//doi.org/10.1145/3392826

Assessing the Quality of Computational Notebooks

[6] Andrew Head, Fred Hohman, Titus Barik, Steven M. Drucker, and Robert DeLine.
2019. Managing Messes in Computational Notebooks. In Proceedings of the 2019
CHI Conference on Human Factors in Computing Systems - CHI ’19. ACM Press,
Glasgow, Scotland Uk, 1–12. https://doi.org/10.1145/3290605.3300500

[7] Mary Beth Kery and Brad A Myers. 2018. Interactions for untangling messy
history in a computational notebook. In 2018 IEEE symposium on visual languages
and human-centric computing (VL/HCC). 147–155. tex.organization: IEEE.
[8] Miryung Kim, Thomas Zimmermann, Robert DeLine, and Andrew Begel. 2018.
Data scientists in software teams: State of the art and challenges. IEEE Transac-
tions on Software Engineering 44, 11 (2018), 1024–1038. https://doi.org/10.1109/
TSE.2017.2754374 Publisher: IEEE.

[9] Andreas Koenzen, Neil Ernst, and Margaret-Anne Storey. 2020. Code Duplication
and Reuse in Jupyter Notebooks. In Proc. of the 2020 Symposium on Visual Lan-
guages and Human-Centric Computing. https://doi.org/10.1109/VL/HCC50065.
2020.9127202

[10] Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Amoruso, Fabio
Fumarola, and Michele Filannino. 2021. Towards Productizing AI/ML Models: An
Industry Perspective from Data Scientists. In 2021 IEEE/ACM 1st Workshop on AI
Engineering - Software Engineering for AI (WAIN). IEEE, Madrid, Spain, 129–132.
https://doi.org/10.1109/WAIN52551.2021.00027

[11] Grace A. Lewis, Stephany Bellomo, and April Galyardt. 2019. Component Mis-
matches Are a Critical Bottleneck to Fielding AI-Enabled Systems in the Public
Sector. In arXiv:1910.06136 [cs]. Arlington, Virginia, USA. http://arxiv.org/abs/
1910.06136 arXiv: 1910.06136.

[12] Grace A. Lewis, Stephany Bellomo, and Ipek Ozkaya. 2021. Characterizing and
Detecting Mismatch in Machine-Learning-Enabled Systems. In 2021 IEEE/ACM 1st
Workshop on AI Engineering - Software Engineering for AI (WAIN). IEEE, Madrid,
Spain. https://doi.org/10.1109/WAIN52551.2021.00028

[13] Lucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, Helena Holmström Olsson,
and Ivica Crnkovic. 2019. A Taxonomy of Software Engineering Challenges
for Machine Learning Systems: An Empirical Investigation. In Agile Processes
in Software Engineering and Extreme Programming, Philippe Kruchten, Steven
Fraser, and François Coallier (Eds.). Springer International Publishing, 227–243.
[14] Lucy Ellen Lwakatare, Aiswarya Raj, Ivica Crnkovic, Jan Bosch, and Helena Holm-
ström Olsson. 2020. Large-scale machine learning systems in real-world industrial
settings: A review of challenges and solutions. Information and Software Technol-
ogy 127 (2020), 106368. https://doi.org/10.1016/j.infsof.2020.106368

[15] E Nascimento, I Ahmed, E Oliveira, MP Palheta, I Steinmacher, and T Conte. 2019.
Understanding Development Process of Machine Learning Systems: Challenges
and Solutions. In 2019 ACM/IEEE International Symposium on Empirical Software
Engineering and Measurement (ESEM). 1–6. https://doi.org/10.1109/ESEM.2019.
8870157

[16] Jeffrey M Perkel. 2018. Why Jupyter is data scientists’ computational notebook

of choice. Nature 563, 7732 (2018), 145–147.

[17] Joao Felipe Pimentel, Leonardo Murta, Vanessa Braganholo, and Juliana Freire.
2019. A Large-Scale Study About Quality and Reproducibility of Jupyter Note-
books. In Proc. of the 16th International Conference on Mining Software Repositories.
507–517. https://doi.org/10.1109/MSR.2019.00077

[18] João Felipe Pimentel, Leonardo Murta, Vanessa Braganholo, and Juliana Freire.
2021. Understanding and improving the quality and reproducibility of Jupyter
notebooks. Empirical Software Engineering 26, 4 (July 2021), 65. https://doi.org/
10.1007/s10664-021-09961-9

[19] Fernando Pérez and Brian E. Granger. 2015. Project Jupyter: Computational Nar-
ratives as the Engine of Collaborative Data Science. Technical Report. UC Berke-
ley and Cal Poly. 24 pages. http://archive.ipython.org/JupyterGrantNarrative-
2015.pdf

[20] Luigi Quaranta, Fabio Calefato, and Filippo Lanubile. 2021. KGTorrent: A Dataset
of Python Jupyter Notebooks from Kaggle. In 2021 IEEE/ACM 18th International
Conference on Mining Software Repositories (MSR). IEEE, Madrid, Spain, 550–554.
https://doi.org/10.1109/MSR52588.2021.00072

[21] Luigi Quaranta, Fabio Calefato, and Filippo Lanubile. 2021. A Taxonomy of Tools

for Reproducible Machine Learning Experiments. AIxIA 2021.

[22] Luigi Quaranta, Fabio Calefato, and Filippo Lanubile. 2022. <Title omitted for

double blind review>. Under minor revision at CSCW 2022.

[23] Adam Rule, Amanda Birmingham, Cristal Zuniga, Ilkay Altintas, Shih-Cheng
Huang, Rob Knight, Niema Moshiri, Mai H. Nguyen, Sara Brin Rosenthal, Fer-
nando Pérez, and Peter W. Rose. 2018. Ten Simple Rules for Reproducible
Research in Jupyter Notebooks.
http:
//arxiv.org/abs/1810.08055 arXiv: 1810.08055.

arXiv:1810.08055 [cs] (Oct. 2018).

[24] Adam Rule, Ian Drosos, Aurélien Tabard, and James D. Hollan. 2018. Aiding
Collaborative Reuse of Computational Notebooks with Annotated Cell Folding.
Proceedings of the ACM on Human-Computer Interaction 2, CSCW (Nov. 2018),
1–12. https://doi.org/10.1145/3274419

[25] Adam Rule, Aurélien Tabard, and James D. Hollan. 2018. Exploration and Ex-
planation in Computational Notebooks. In Proc. of the 2018 CHI Conference on
Human Factors in Computing Systems. https://doi.org/10.1145/3173574.3173606
[26] Danilo Sato, Arif Wider, and Christoph Windheuser. 2019. Continuous Delivery
for Machine Learning - Automating the end-to-end lifecycle of Machine Learning

