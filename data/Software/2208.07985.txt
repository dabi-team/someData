2
2
0
2

g
u
A
6
1

]
I

N
.
s
c
[

1
v
5
8
9
7
0
.
8
0
2
2
:
v
i
X
r
a

1

Federated Multi-Discriminator BiWGAN-GP
based Collaborative Anomaly Detection for
Virtualized Network Slicing

Weili Wang, Chengchao Liang, Lun Tang, Halim Yanikomeroglu, Fellow, IEEE, Qianbin Chen, Senior
Member, IEEE

Abstract—Virtualized network slicing allows a multitude of logical networks to be created on a common substrate infrastructure to
support diverse services. A virtualized network slice is a logical combination of multiple virtual network functions, which run on virtual
machines (VMs) as software applications by virtualization techniques. As the performance of network slices hinges on the normal
running of VMs, detecting and analyzing anomalies in VMs are critical. Based on the three-tier management framework of virtualized
network slicing, we ﬁrst develop a federated learning (FL) based three-tier distributed VM anomaly detection framework, which enables
distributed network slice managers to collaboratively train a global VM anomaly detection model while keeping metrics data locally. The
high-dimensional, imbalanced, and distributed data features in virtualized network slicing scenarios invalidate the existing anomaly
detection models. Considering the powerful ability of generative adversarial network (GAN) in capturing the distribution from complex
data, we design a new multi-discriminator Bidirectional Wasserstein GAN with Gradient Penalty (BiWGAN-GP) model to learn the
normal data distribution from high-dimensional resource metrics datasets that are spread on multiple VM monitors. The
multi-discriminator BiWGAN-GP model can be trained over distributed data sources, which avoids high communication and
computation overhead caused by the centralized collection and processing of local data. We deﬁne an anomaly score as the
discriminant criterion to quantify the deviation of new metrics data from the learned normal distribution to detect abnormal behaviors
arising in VMs. The efﬁciency and effectiveness of the proposed collaborative anomaly detection algorithm are validated through
extensive experimental evaluation on a real-world dataset.

Index Terms—Virtualized network slicing, virtual machines, collaborative anomaly detection, federated learning (FL), bidirectional
Wasserstein generative adversarial network with gradient penalty (BiWGAN-GP).

(cid:70)

1 INTRODUCTION

N ETWORK slicing allows network operators to build

multiple self-contained logical networks over a com-
mon underlying network infrastructure to accommodate
the wide range of service requirements [1], [2]. The rise of
network function virtualization (NFV) and software deﬁned
networks (SDN) facilitates the deployment of network slices
through running classical network functions as software
applications in virtual machines (VMs) instead of dedicated
hardware [3]. However, with the increasingly extensive
application of VMs, their security and stability issues have
drawn a wide attention [4]. As the performance of network
slices hinges on the normal running of VMs, it is crucial to
detect anomalies in VMs in a timely manner to ensure the
service quality of network slices.

Anomaly detection is necessary to help ﬁnd and identify
abnormal behaviors in VMs before serious failures occur.
The abnormal behaviors of a VM are deﬁned as any
performance degradation deviating from normal behaviors

W. Wang, C. Liang, L. Tang and Q. Chen are with the School of
Communication and Information Engineering and the Key Laboratory
of Mobile Communication, Chongqing University of Posts and Telecom-
munications, Chongqing 400065, China (e-mail: 1961797154@qq.com;
liangcc@cqupt.edu.cn; tangluncq@163.com; cqb@cqupt.edu.cn).
H. Yanikomeroglu is with the Department
and Com-
of Systems
puter Engineering, Carleton University, Ottawa, ON, Canada (e-mail:
halim@sce.carleton.ca).

[5]. Existing researches [6], [7], [8] have shown that the
abnormal behaviors of VMs usually come with a signiﬁcant
change in resource metrics, so it is a good way to implement
anomaly detection for VMs by collecting and analyzing its
multi-dimensional resource metrics data. Although there
have been many interesting researches for anomaly detec-
tion, including statistical and probability methods [9], [10],
distance-based methods [11], [12], domain-based methods
[13], [14], reconstruction-based methods [15], [16], [17], and
information theory based methods [18], as classiﬁed in [19],
detecting anomalies of VMs in virtualized network slicing
environment still faces many challenges:

Firstly, new and emerging user cases in virtualized
network slicing environment have resulted in diverse types
of metrics data and highly complex data structures, which
are hard to accurately capture with traditional anomaly
detection methods.

Secondly, the boundary between normal metrics data
and abnormal ones is hard to get for three reasons: 1)
Abnormal events rarely occur in real networks, so available
normal and abnormal data are extremely imbalanced; 2) It is
too costly to obtain enough and various kinds of abnormal
metrics data in the environment with multiple demand-
diverging virtual networks; 3) Novel anomalies often arise
in virtualized network slicing environment.

Thirdly, substrate networks consist of multiple regions
and VMs that constitute a network slice can cross multiple

 
 
 
 
 
 
regions, so resource metrics data of VMs are distributed in
the networks. Existing works commonly use a centralized
database to aggregate all local metrics data, such as Gnocchi
[20] and Prometheus [21], [22], which will compromise the
communication and computation efﬁciency when learning
a global VM anomaly detection model.

Generative adversarial network (GAN) has drawn sub-
stantial attention for its powerful ability in capturing the
distribution from high-dimensional real-world data [23]. To
conquer the difﬁculties in capturing complex data struc-
tures, ﬁnding boundaries between normal and abnormal
data and high cost of centralized training, we design a new
multi-discriminator Bidirectional Wasserstein GAN with
Gradient Penalty (BiWGAN-GP) to capture the distribu-
tion of normal metrics data in VMs. Compared to GAN,
the multi-discriminator BiWGAN-GP includes four main
improvements: 1) Replace Jensen-Shannon (JS) divergence
with Wasserstein-1 distance to avoid gradient vanishing
happened in GAN commonly (Wasserstein GAN (WGAN)
[24]); 2) Besides a generator and a discriminator, an encoder
is added to realize the reverse mapping of generator to facil-
itate the establishment of discriminant criterion for anomaly
detection (Bidirectional GAN (BiGAN) [25]); 3) Gradient
penalty is used to enforce the Lipschitz constraint in WGAN
to solve the optimization difﬁculties of weight clipping
(WGAN-GP [26]); 4) Instead of a centralized discriminator,
multiple discriminators are running locally to avoid high
communication and computation overhead caused by the
centralized collection and processing of local data.

The management framework for virtualized network
slicing is a three-tier structure, including virtualized in-
frastructure managers (VIMs) for VMs, network slice man-
agers for network slices, and network controller for the
whole networks. For adapting to the three-tier management
framework of virtualized network slicing, we develop a
federated learning (FL) based three-tier distributed anomaly
detection framework to identify abnormal behaviors arising
in VMs. The introduction of FL enables to collaboratively
train a global VM anomaly detection model over multiple
distributed datasets [27]. The novelty of the paper lies
in the combination of multi-discriminator BiWGAN-GP
and FL to detect anomalies in VMs in the context of
virtualized network slicing environment. Speciﬁcally, the
main contributions of the paper are summarized as follows:

• To lower the latency when collecting data, separate
databases are deployed for different regions. Besides,
to ensure the isolation among different network
slices, in each region, we deploy a VM monitor for
a network slice to collect and store metrics data of
VMs in the network slice as a local database.

• Considering the three-tier management framework
of virtualized network slicing, we develop an
FL-based three-tier distributed anomaly detection
framework to identify abnormal behaviors arising
in VMs. The developed framework enables to
collaboratively train a global VM anomaly detection
model while keeping metrics data locally through
the hierarchical cooperation among VM monitors,
network slice managers and the network controller.
In view of the high-dimensional, imbalanced, and

•

2

distributed data features in virtualized network slic-
ing scenarios, we propose a new multi-discriminator
BiWGAN-GP algorithm to learn the normal data
distribution from high-dimensional metrics data-
sets that are spread on multiple VM monitors. The
proposed algorithm deploys multiple discriminators
running locally in VM monitors to avoid the central-
ized collection and processing of local data. In addi-
tion, an anomaly score is deﬁned as the discriminant
criterion to quantify the deviation of new metrics
data from the learned normal distribution to detect
abnormal behaviors arising in VMs.

• We implement extensive simulations on a real-world
dataset to evaluate the efﬁciency and effectiveness
of
the proposed federated multi-discriminator
BiWGAN-GP based collaborative anomaly detection
algorithm. Experimental results demonstrate that the
proposed algorithm can accurately detect abnormal
behaviors in VMs with low communication and
computation cost.

The rest of the paper is organized as follows. Section 2
presents the system model of virtualized network slicing
management and the corresponding distributed anomaly
detection framework. Section 3 introduces different GAN
algorithms and proposes the multi-discriminator BiWGAN-
GP algorithm. In Section 4, we present the training and
detection processes of the three-tier FL-based distributed
anomaly detection framework. The performance of the
developed framework is evaluated in Section 5. Then, we
provide a discussion on important related works in Section
6 and ﬁnally Section 7 concludes the paper.

2 SYSTEM MODEL

2.1 Three-tier management framework

The network slice management and orchestration (MANO)
is responsible for the control of virtualized network slicing
[3]. The system model of virtualized network slicing man-
agement is shown in Fig. 1, which is a three-tier structure.
The speciﬁc roles of each tier are described as follows:

Substrate networks: Substrate networks are divided into
N regions, where each region consists of various physical
nodes including servers, switches and gateways. Physical
nodes provide basic storage, computing and network re-
sources required by the creation and operation of VMs.
One physical node can host multiple VMs supported by the
virtualization layer. VIM is responsible for the management
and maintenance of each region, whose implementations
include VIM connector developed in Open-Source MANO
project [29], OpenStack and Docker.

Network slice instances: Network slices are deﬁned
as logical networks running on a common underlying
infrastructure, mutually isolated, created on demand and
with independent control and management [1], which is
realized by distributed network slice managers. Network
slice managers can be implemented by Juju charms [29],
Tacker [30] or Puppet [3] solutions. A virtualized network
slice consists of a set of virtual network functions and virtual
links connecting these functions. Virtual network functions
are often running on VMs as software applications [31].

3

Fig. 1. System model of virtualized network slicing management.

Fig. 2. FL-based three-tier anomaly detection framework. This
framework’s workﬂow consists of seven steps as follows: 1(cid:13) The VM
monitor trains its own local discriminator model based on the local
training dataset; 2(cid:13) The VM monitor uploads error feedbacks for
generator and encoder to its network slice manager; 3(cid:13) The network
slice manager updates its generator and encoder models according to
the error feedbacks from all connected VM monitors, then generates
fake data based on the updated generator and encoder; 4(cid:13) The network
slice manager sends the fake data to update its local discriminators
on VM monitors; 5(cid:13) After some iterations, all distributed network slice
managers upload their generator and encoder models to the network
controller for global aggregation; 6(cid:13) The network controller obtains
global generator and encoder models by the FedAVG algorithm [28]; 7(cid:13)
The network controller sends new global generator and encoder models
to distributed network slice managers for their further updates. The
above steps are implemented repeatedly until global models converge.

functionalities can be provided by OSM [29], OpenStack
Tacker [30] and OpenBaton [32].

2.2 Three-tier distributed anomaly detection framework

Based on the system model of virtualized network slicing
management, we develop a three-tier distributed anomaly
detection framework, as illustrated in Fig. 2, to detect
abnormal behaviors of VMs in virtualized network slicing
environment. As the abnormal behaviors of VMs usually
come with signiﬁcant changes in resource metrics, it is a
good way to implement anomaly detection by collecting and
analyzing their resource metrics data. We design a multi-
discriminator BiWGAN-GP algorithm as the basic method
for anomaly detection, which is elaborated in Section 3.

To lower the latency when collecting data and to ensure
the isolation among network slices,
in each region, we
deploy a VM monitor for a network slice to collect and store
metrics data of VMs in the network slice as a local database.
VM monitors resort to the virtualization layer to capture
resource metrics of VMs [4], [6]. The resource metrics are
related with the CPU usage, memory consumption, disk
read/write and network input/output. To train a global
VM detection model, the resource metrics data from all
VMs are required. To avoid the centralized collection and
processing of local data, we design an FL-based three-tier
distributed anomaly detection framework to collaboratively
train a global VM anomaly detection model over datasets
that are spread on multiple VM monitors. The framework’s
workﬂow is illustrated in Fig. 2.

The detailed training and detection processes for the FL-
based three-tier distributed anomaly detection framework
are presented in Section 4.

Service instances:

In service instance layer, service
requests are received and translated into required network
functions. The network controller is responsible for the
orchestration of different network slices through coordinat-
ing the available resources of the whole networks, whose

3 MULTI-DISCRIMINATOR BIWGAN-GP

The multi-discriminator BiWGAN-GP algorithm, which is a
distributed form of BiWGAN-GP, trains local discriminators
on local VM monitors using their own training datasets. The

EmbeddingEmbeddingNetwork Slice Instance Layer...Service Instance LayerNetwork Slice MANOService Instance 1Service Instance SService Instance 2RequestRequestNetwork ControllerNetwork SliceManager SNetwork SliceManager 2Network SliceManager 1...Network slice SNetwork slice 2Network slice 1Substrate Networks...Virtualization layer......Region 1Region NVirtual Machines...Virtual Machines...Virtualized Infrastructure Manager NVirtualized Infrastructure Manager 1......Network Slice Manager 1Network Slice Manager 2Network Slice Manager S............⑦②③②③②③④④④⑤⑤⑤⑦⑦...VIM 1VIM 2VIM NVM Monitor SLocal databaseVM Monitor 2Local databaseVM Monitor 1Local database...VM Monitor SLocal databaseVM Monitor 2Local databaseVM Monitor 1Local database...VM Monitor SLocal databaseVM Monitor 2Local databaseVM Monitor 1Local database...①①①Network Controller⑥4

multi-discriminator BiWGAN-GP is designed to capture the
distribution of normal metrics data in VMs.

3.1 Existing models: GAN, BiGAN, WGAN-GP

GAN: GAN [33] consists of two models: generator G and
discriminator D. Given the real metrics data {X r}Ntd
r=1 as
the training dataset, G tries to transform a low-dimensional
random noise z extracted from a known distribution Pz into
a fake data ¯X (namely ¯X = G(z)), such that D cannot
distinguish ¯X from real data {X r}Ntd
r=1. Let PX and P ¯X
denote the distributions of real and fake data, and D(X)
denote the probability that X obeys the distribution of real
data. The training objective for GAN is deﬁned as [33]

[log(1 − D(G(z)))]

(1)

Fig. 3. Multi-discriminator BiWGAN-GP framework.

max
D

min
G
= E

X∼PX

= E

X∼PX

V (G, D)

[log(D(X))] + E

z∼Pz
[log(D(X))] + E

¯X∼P ¯X

[log(1 − D( ¯X))].

G and D are updated alternately. If an optimal D is
obtained, the training process of G theoretically leads to
minimizing the JS divergence between PX and P ¯X , namely,

min
G

V (G) = 2DJS(PX ||P ¯X ) − log 4,

(2)

where DJS(·||·) represents the JS divergence. When G
and D are both trained into optimality, PX = P ¯X can
be achieved, which indicates that G has captured the
distribution of real data.

BiGAN: Besides G and D in classical GAN, BiGAN
also contains an encoder E. GAN has realized a mapping
from low-dimensional random noise z in latent space to
fake data ¯X in data space ( ¯X = G(z)), but the reverse
process is unavailable. The encoder E in BiGAN completes
the mapping from the real data X in data space to a low-
dimensional representation f in latent space (f = E(X)).
D in BiGAN tries to distinguish between the joint pairs
(X, E(X)) and (G(z), z) in both data space and latent
space. The training objective for BiGAN is given by

V (G, E, D)

max
D

min
G,E
= E

=

X∼PX
E
(X,f )∼PXf

[log(D(X, E(X)))] + E

[log(1 − D(G(z), z))]

[log(D(X, f )] +

[log(1 − D( ¯X, z))],

z∼Pz
E
( ¯X,z)∼P ¯Xz

(3)
where PXf and P ¯Xz represent the joint distributions of
(X, f ) and ( ¯X, z), respectively. Given an optimal D, the
training objective of G and E is also to minimize the JS
divergence between PXf and P ¯Xz, namely,

The global minimum of (4) can be achieved when PXf =
P ¯Xz is satisﬁed. By this time, E and G are each other’s
inverse, namely G(E(X)) = X and E(G(z)) = z [25].

WGAN-GP: In the early phase of GAN training, the
very little overlap between PX and P ¯X will cause their
JS divergence being a constant and the gradient vanishing,
which will terminate the training exceptionally. To solve it,

WGAN [24] replaces the JS divergence with Wasserstein-1
distance, which is computed by

W1(PX ||P ¯X ) = sup
f ∈L1

{ E
X∼PX

[f (X)] − E

¯X∼P ¯X

[f ( ¯X)]},

(5)

where L1 is the set of 1-Lipschitz functions, which is
satisﬁed through weight clipping [24]. Due to the optimiza-
tion difﬁculties of weight clipping, [26] proposed WGAN
with gradient penalty (WGAN-GP) to enforce the Lipschitz
constraint. WGAN-GP uses D with the norm of its gradient
penalty to approximate the function f ∈ L1 in WGAN.
Therefore, D is trained based on the following objective:

max
D

V (D) = E

X∼PX

[D(X)] − E

[D( ¯X)]

¯X∼P ¯X
(cid:99)X D( (cid:99)X)||2 − 1)2],

(6)

+η E
(cid:99)X∼P

(cid:99)X

[(||∇

where η is the penalty coefﬁcient and (cid:99)X is deﬁned to uni-
formly sample along straight lines between pairs of points
extracted from the distributions PX and P ¯X . Therefore, (cid:99)X
is the weighted sum of X and ¯X: (cid:99)X = εX + (1 − ε) ¯X,
where ε ∈ U [0, 1]. G is trained through

min
G

V (G) = − E

¯X∼P ¯X

[D( ¯X)] = − E

z∼Pz

[D(G(z))].

(7)

3.2 BiWGAN-GP and its distributed form

We propose BiWGAN-GP to combine the strengths of
BiGAN and WGAN-GP. After obtaining (X, f ) and ( ¯X, z),
( (cid:99)X, (cid:98)z) is calculated through ( (cid:99)X, (cid:98)z) = ε(X, f ) + (1 −
ε)( ¯X, z) similar to the calculation of (cid:99)X. In BiWGAN-GP,
D is trained through
E
(X,f )∼PXf

[D( ¯X, z)]

[D(X, f )] −

E
( ¯X,z)∼P ¯Xz

V (D) =

max
D

[(||∇( (cid:99)X, (cid:98)z)D(( (cid:99)X, (cid:98)z))||2 − 1)2],

( (cid:99)X, (cid:98)z)∼P

(cid:99)X (cid:98)z

(cid:99)X (cid:98)z is similar to P

(8)
where the deﬁnition of P
(cid:99)X . After training
D, the Wasserstein-1 distance W1(PXf ||P ¯Xz) between PXf
and P ¯Xz can be expressed as
W1(PXf ||P ¯Xz) =
E
(X,f )∼PXf

[D( ¯X, z)].

[D(X, f )] −

E
( ¯X,z)∼P ¯Xz

(9)

min
G,E

V (G, E) = 2DJS(PXf ||P ¯Xz) − log 4.

(4)

+η

E

VM Monitor  1VM Monitor 2VM Monitor  NNetwork slice managerMerge...G(z), zGeneratornoise zEncoderdata XX, E(X)Local databaseDiscriminatorDiscriminatorDiscriminatorSend dataUpdate parametersSend feedbackLocal databaseLocal databaseThe training objective of G and E is to minimize

W1(PXf ||P ¯Xz), namely,

min
G,E

V (G, E) = W1(PXf ||P ¯Xz).

(10)

A multi-discriminator BiWGAN-GP is designed to learn
the distribution of all metrics data within a network slice.
In multi-discriminator BiWGAN-GP, a pair of generator
and encoder is trained on a network slice manager and N
discriminators are trained on VM monitors.

The multi-discriminator BiWGAN-GP framework is il-
lustrated in Fig. 3. The network slice manager selects and
sends the generated data to each VM monitor, and the
discriminators in VM monitors try to distinguish generated
data from local real data. Therefore, the optimization objec-
tive of multi-discriminator BiWGAN-GP is deﬁned as

min
G,E

max
D

V (G, E, D) =

1
N

+

N
(cid:88)

n=1

E
{
(X,f )∼PXf

[Dn(X, f )] −

E
( ¯X,z)∼P ¯Xz

[Dn( ¯X, z)]

E

( (cid:99)X, (cid:98)z)∼P

(cid:99)X (cid:98)z

[(||∇( (cid:99)X, (cid:98)z)Dn(( (cid:99)X, (cid:98)z))||2 − 1)2]}.

(11)

3.3 Modeling the multi-discriminator BiWGAN-GP

As the available training dataset of metrics is time series, we
choose Vanilla Long Short-Term Memory Network (VLSTM)
as the basic model of generator G and encoder E, which has
shown superior performance on time-series-related tasks as
a kind of recurrent neural networks [34]. Given [y1, ..., yt]
as input, then for each time epoch τ (τ = 1, ..., t), we have

iτ = σ(wi.[yτ , hτ −1, cτ −1] + bi),
fτ = σ(wf .[yτ , hτ −1, cτ −1] + bf ),
cτ = fτ (cid:12) cτ −1 + iτ (cid:12) tanh(wz.[yτ , hτ −1] + bz),
oτ = σ(wo.[yτ , hτ −1, cτ ] + bo),
hτ = oτ (cid:12) tanh(cτ ),

(12)

where iτ , fτ and oτ are the input gate, forget gate and
output gate, respectively. cτ denotes the memory unit and
hτ denotes the hidden state. w∗ and b∗ represent weights
and bias, respectively. The symbol σ(·) represents the
sigmoid function and (cid:12) denotes the operation of element-
wise multiplication.

Build G: We build the generator G into a three-layer
structure, including two VLSTM layers and one fully con-
nected layer activated by linear. In network slice managers,
after extracting z from Pz, G maps z into the fake data ¯X
¯X = G(z; [wG, bG]),

(13)

where wG and bG denote weights and biases of G, which
compose model parameters θG.

Build E: As encoder E is the inverse function of G, it
is also a three-layer structure with two VLSTM layers and
one fully connected layer activated by linear, but they are
grouped in a fully reverse order to G. E maps the real data
X to a low-dimensional representation f

5

where wE and bE denote weights and biases of E, which
compose model parameters θE.

Build Dn: We build the discriminator Dn (n ∈ N )
with three fully connected layer, where the last layer is
sigmoid activated to get a probability in range (0, 1). The
discriminator Dn in each VM monitor maps (Xn, fn) and
( ¯Xn, zn) to a scalar d

d = Dn

(cid:8)(cid:2)(Xn, fn), ( ¯Xn, zn)(cid:3) ; [wDn , bDn ](cid:9) ,

(15)

where wDn and bDn denote weights and biases of Dn,
which compose model parameters θDn .

The training of multi-discriminator BiWGAN-GP al-
gorithm for a network slice is iteratively performed by
VM monitors and the network slice manager. Next, we
separately specify the training of Dn (n ∈ N ) on each VM
monitor and the training of G and E on the network slice
manager.

Training on VM monitors: Each VM monitor hosts a
discriminator Dn with its parameters θDn . In each iteration,
a VM monitor contains two functions: 1) Receive the
generated data to update its discriminator; 2) Calculate the
error feedbacks for generator and encoder updates.

1) Discriminator Update: Each VM monitor receives the
generated data pairs ( ¯Xn, zn) and the low-dimensional
representation fn of Xn from the network slice manager,
whose batch sizes are all M . Then, the VM monitor performs
K training iterations on Dn. In the kth local iteration,
n ) and ( ¯X m
for each batch (X m
n ) in (Xn, fn) and
( ¯Xn, zn) , the loss function of Dn is calculated by
n )+

= −{Dn(X m

n , zm

n , f m

n , zm

Lm
Dn
η[||∆( (cid:99)Xm

n , f m
n )Dn( (cid:99)X m

n ) − Dn( ¯X m
n )||2 − 1]2}.
n , (cid:98)zm

(16)

n , (cid:98)zm

Accordingly, the gradients ∆θDn are calculated by

∆θDn =

1
M

M
(cid:88)

∂Lm
Dn
∂θDn

.

(17)

m=1
We update the parameters θDn of discriminator Dn

using the Adam optimizer method

θDn ← Adam(∆θDn , θDn , α, β1, β2),

(18)

where α, β1 and β2 are hyperparameters of Adam optimizer.
2) Error Feedbacks Calculation: To update the parameters
of G and E in the network slice manager, each VM monitor
calculates the error feedbacks F n
G and F n
E based on the local
loss function Ln
EG and Dn after K training iterations. The
local loss function Ln
EG for G and E in each VM monitor is
computed as

Ln

EG =

1
M

M
(cid:88)

{Dn(X m

n , f m

n ) − Dn( ¯X m

n , zm

n )}.

(19)

m=1
The error feedback F n
n }, where em
n, ..., eM

{e1

n is deﬁned as

E of E consists of M vectors

em
n =

∂Ln
∂(X m

EG
n , f m
n )

.

(20)

Similarly, the error feedback F n

G of G also consists of M

vectors {g1

n, ..., gM

n }, where gm

n is deﬁned as
∂Ln
∂( ¯X m

EG
n , zm
n )

.

f = E(X; [wE, bE]),

(14)

gm
n =

(21)

E and F n
F n

G obtained in each VM monitor will be sent to

the network slice manager for G and E updates.

Training on the network slice manager: The network slice
manager hosts G and E, which contains two functions
in each iteration: 1) Generate and encode data using the
generator and encoder; 2) Receive error feedbacks to update
the generator and encoder.

1) Data Generation: The network slice manager ﬁrst
receives the set of training batches {Xn}N
n=1 from N VM
monitors. E turns {Xn}N
n=1 into a set of low-dimensional
representations {fn}N
n=1. In addition, G transforms a set
of low-dimensional random noise {zn}N
n=1 extracted from
a known distribution Pz into a set of fake data { ¯Xn}N
n=1.
Then, the network slice manager sends fn, zn and ¯Xn to
each VM monitor to train Dn (n ∈ N ).

2) G and E Updates: After receiving error feedbacks F n
E
and F n
G from each VM monitor, the network slice manager
updates parameters θG and θE of G and E. We use the
averaging operation to aggregate error feedbacks from all
VM monitors as it is the most common method to merge
feedbacks updated in parallel [35]. Speciﬁcally, the gradients
∆θG of θG are deduced from feedbacks F n
N
(cid:88)

G (n ∈ N ) by

M
(cid:88)

∆θl

G =

1
M N

EG

∂Ln
∂θl
G
∂Ln
∂( ¯X m

=

=

1
M N

1
M N

n=1
N
(cid:88)

m=1
M
(cid:88)

n=1
N
(cid:88)

m=1
M
(cid:88)

n=1

m=1

gm
n

EG
n , zm
n )
∂( ¯X m
n , zm
n )
∂θl
G

,

∂( ¯X m
n , zm
n )
∂θl
G

(22)

where ∆θl
G is the l-th (l ∈ L) element of gradients
∆θG. After the gradients are calculated, the network slice
manager updates parameters θG using Adam optimizer

θG ← Adam{(∆θ1

G, ..., ∆θL

G), θG, α, β1., β2}.

(23)

Similarly,

deduced from all feedbacks F n

the gradients ∆θE of parameters θE are
E (n ∈ N ) by

Algorithm 1 Multi-discriminator BiWGAN-GP algorithm
Hyperparameters: The number of iterations K between
two E and G iterations. Batch size M . Adam
hyperparameters α, β1, β2. The prior distribution Pz.
The number of training iterations I.

6

Input: Initialized model parameters: θG, θE, θDn (n ∈ N ),

and distributed training datasets ∪N

n=1{X r

n}Ntd
r=1.

Output: Converged model parameters: θG, θE, θDn (n ∈

N ).

1: for i = 1 : I do
2: (cid:66) Update Dn (n ∈ N ) on VM monitors
3:

Sample Xn from its own training dataset {X r
and send it to the network slice manager
Receive ( ¯Xn, zn) and fn from the network slice
manager to obtain the data pairs (Xn, fn) and
( ¯Xn, zn)
for k = 1 : K do

n}Ntd
r=1

n , f m

n , (cid:98)zm

n ) = ε(X m

for m = 1 : M do
Calculate ( (cid:99)X m
ε)( ¯X m
n , zm
n )
Calculate Lm
if k == K then
Calculate Ln
and gm
Calculate elements em
n
n
feedbacks according to (20) and (21)

Dn according to (16)

EG according to (19)

n ) + (1 −

of error

end if
end for
Update gradients ∆θDn and Dn’s parameters θDn
according to (17) and (18)

end for
Send the error feedbacks F n
E = {e1
F n

n, ..., gM
n } to the network slice manager

G = {g1

n, ..., eM

n } and

17: (cid:66) Update E and G on the network slice manager
18:

n=1 from N VM

Receive a set of training batches {Xn}N
monitors
Sample a set of noise {zn}N

19:
20: Obtain {fn}N
G({zn}N
Send fn, zn and ¯Xn to each VM monitor

n=1 ← E({Xn}N

n=1 ∼ Pz

n=1)

21:
22: Deduce gradients ∆θG of G from all feedbacks F n
G
(n ∈ N ) based on (22), then update G’s parameters
θG according to (23)

23: Deduce gradients ∆θE of E from all feedbacks F n
E
(n ∈ N ) based on (24), then update E’s parameters
θE according to (25)

n=1) and { ¯Xn}N

n=1 ←

4:

5:
6:
7:

8:
9:
10:
11:

12:
13:
14:

15:
16:

∆θl

E =

1
M N

=

=

1
M N

1
M N

N
(cid:88)

M
(cid:88)

n=1
N
(cid:88)

m=1
M
(cid:88)

EG

∂Ln
∂θl
E
∂Ln
∂(X m

n=1
N
(cid:88)

m=1
M
(cid:88)

n=1

m=1

em
n

EG
n , f m
n )
∂(X m
n , f m
n )
∂θl
E

n , f m
∂(X m
n )
∂θl
E

(24)

,

24: end for

where ∆θl
E is the l-th (l ∈ L) element of gradients ∆θE. As
G and E are each other’s inverse, ∆θE has the same number
of elements with ∆θG. Then, the network slice manager
updates parameters θE using Adam optimizer

θE ← Adam{(∆θ1

E, ..., ∆θL
The detailed steps of multi-discriminator BiWGAN-GP

E), θE, α, β1, β2}.

(25)

algorithm are presented in Algorithm 1.

4 FL-BASED THREE-TIER DISTRIBUTED ANOMALY
DETECTION FRAMEWORK

The proposed multi-discriminator BiWGAN-GP algorithm
is adapted to the management of VMs within a network

slice. To extend it to the entire network, FL is introduced
to the multi-discriminator BiWGAN-GP algorithm to de-
velop an FL-based three-tier distributed anomaly detection
framework as shown in Fig. 4. From the perspective of
a network slice, local training data in each VM monitor
are utilized to train the multi-discriminator BiWGAN-GP
model. After some iterations, network slice managers will
send the local models of generators and encoders to the
network controller for global aggregation, such that we can
achieve a global VM anomaly detection model through the
hierarchical cooperation among VM monitors, network slice
managers and the network controller.

7

Fig. 4. FL-based three-tier distributed anomaly detection framework.

4.1 Training

The training of FL-based multi-discriminator BiWGAN-GP
algorithm on network slices and the network controller are
separately speciﬁed as follows:

Network

slices: A multi-discriminator BiWGAN-GP
model
is implemented within a network slice, which
contains one pair of generator and encoder and N
discriminators. N Discriminators are trained on VM
monitors and local generators Gs and encoders Es (s ∈ S)
are trained on network slice managers, where S represents
both the number and set of network slices. Network
slice managers ﬁrst receive initial global models Gglobal
and Eglobal of generator and encoder from the network
controller as the initial local ones. In a network slice s,
the network slice manager and VM monitors exchange
data and parameters every K iterations for the training of
Dn (n ∈ N ), Gs and Es. After L iterations, each network
slice manager sends model parameters θGs and θEs to
the network controller for global aggregation. After the
averaging operation in network controller, it sends new
global models Gglobal and Eglobal to network slice managers
for updating local Gs and Es (s ∈ S) using the Adam
optimizer method

received parameters and send global models Gglobal and
Eglobal back to network slice managers. Gglobal and Eglobal
are updated using averaging operation (i.e., FedAVG algo-
rithm [28]), which is given by

θglobal
G

=

(cid:80)S

s=1 QsθGs
Q

, θglobal
E

=

(cid:80)S

s=1 QsθEs
Q

,

(27)

where Qs is the number of training data in network slice s
(s ∈ S), and Q = (cid:80)S
s=1 Qs is the total number of training
data in all network slices.

The above steps are implemented repeatedly until the
global models Gglobal and Eglobal reach optimal conver-
gence. In addition, the network environment is not static
but time-varying. The trained model may become inaccu-
rate with time. Therefore, the proposed FL-based multi-
discriminator BiWGAN-GP algorithm still needs to be
retrained periodically on the newly collected data.

The proposed algorithm can be extended to the dynamic
network environment. If there are new network slices
joining the FL, we can transfer the well-trained model as
their initial models to detect possible abnormal behaviors
in current period, and then update their anomaly detection
model in the next retraining period.

θGs ← Adam(∆θGs, θglobal
θEs ← Adam(∆θEs, θglobal

G

E

, α, β1, β2),
, α, β1, β2),

(26)

4.2 Detection

where θglobal
G
Gglobal and Eglobal.

and θglobal
E

are global model parameters of

Network controller: Network controller is responsible for
the global aggregation of Gs and Es (s ∈ S). Network
controller contains two main functions: (1) Initialize global
models and send them to network slice managers; (2)
(s ∈ S)
Aggregate all model parameters θGs and θEs
uploaded by network slice managers, then average the

trained,

After the generator and encoder are well
the
copy of them will be sent to each VM monitor for VM
anomaly detection. To measure the abnormality of VM v
(v ∈ V ), we introduce an anomaly score A(Xv) for the new
metrics data Xv. The anomaly score A(Xv) is deﬁned as a
weighted combination of the reconstruction loss LEG and
the discriminator loss LD, which is calculated by

A(Xv) = γLEG(Xv) + (1 − γ)LD(Xv),

(28)

...Network slice SVM monitor 2VM monitor NNetwork slice 1Discriminator...GeneratorEncoderNetwork slice managerDiscriminatorMergeVM monitor 1Discriminator...EncoderGeneratorNetwork slice managerMergeVM monitor 1DiscriminatorVM monitor 2DiscriminatorVM monitor NDiscriminatorNetwork controllerMergeMerge.........FedAVGGenerator 1Generator S...Global generator Global encoder FedAVGEncoder 1Encoder S......Send dataUpdate parametersSend feedbackDistribute modelsUpload modelsAlgorithm 2 FL-based multi-discriminator BiWGAN-GP
Hyperparameters: The number of iterations K between
two E and G iterations. Batch size M . Adam
hyperparameters α, β1, β2. The number of training
iterations I. The number of local iterations L before
aggregation. The value of threshold for anomaly score.
θglobal
,
E
θDn (n ∈ N ), and distributed training datasets
n}Ntd
∪N
n=1{X r
r=1 for all s ∈ S, new metrics data Xv of
VM v (v ∈ V ).

Input: Initialized model parameters:

θglobal
G

,

Output: Converged model parameters: θglobal

, θglobal
E
θDn (n ∈ N ) for all s ∈ S, the abnormality of VM v .

G

and

1: for i = 1 : I do {(cid:66) Training process}
2: (cid:66) Update Dn (n ∈ N ) for all s ∈ S on VM monitors
3:
4: (cid:66) Update Es and Gs (s ∈ S) on network slice

Implement steps (3)-(16) in Algorithm 1

5:

6:
7:
8:

9:

managers
and θglobal
Receive initial global parameters θglobal
E
from the network controller as initial local parameters
θGs and θEs
Implement steps (18)-(24) in Algorithm 1
if i mod L == 0 then

G

Send local parameters θGs and θEs to the network
controller for aggregation
Receive updated global parameters θglobal
θglobal
E
in each network slice s according to (26)

and
and use them update local Gs and Es models

G

end if

10:
11: (cid:66) Update global models Gglobal and Eglobal on the

12:

network controller
Initialize global model parameters θglobal
and sends them to all network slice managers

G

and θglobal
E

13: Update global model parameters θglobal

by
averaging θGs and θEs (s ∈ S) sent from network slice
managers every L iterations according to (27)

and θglobal
E

G

14: end for
15: For new metrics data Xv of VM v, calculate its anomaly
score A(Xv) based on its corresponding G, E and D
according to (28)

{(cid:66) Detection process}

VM v is determined as abnormal

16: if A(Xv) > threshold then
17:
18: else
19:
20: end if

VM v is determined as normal

where γ is
the weighted coefﬁcient, LEG(Xv) =
||Xv − G(E(Xv))||1 and LD(Xv) = σ(D(Xv), E(Xv), 1).
In LD(Xv), σ is the sigmoid cross entropy, which deﬁnes
the discriminator’s conﬁdence that Xv obeys the real data
distribution [36]. For new metrics data Xv of VM v, its VM
monitor calculates the anomaly score A(Xv) based on its
own discriminator and the received generator and encoder.
If A(Xv) is larger than a preset threshold, VM v will be
determined as abnormal.

8

5 PERFORMANCE EVALUATION

In this section, we evaluate the performance of the proposed
FL-based multi-discriminator BiWGAN-GP algorithm in
terms of efﬁciency and effectiveness through extensive ex-
perimental evaluation, and compare it with GAN, BiGAN,
WGAN-GP, centralized BiWGAN-GP, standalone BiWGAN-
GP, and multi-discriminator BiWGAN-GP algorithms.

5.1 Experiment setup

We implement the centralized, standalone, distributed and
the developed FL-based distributed anomaly detection
framework in Python language using Pytorch package
on Intel(R) Core(TM) i7-6700H CPU with 16GB RAM. In
our implementations, we emulate 1 network controller, 3
network slice managers, and 9 VM monitors, and use socket
to realize the information exchange among them.

1) Datasets: We choose the VNFDataset (virtual IP Mul-
timedia IP system) 1 as the experimental dataset for perfor-
mance evaluation. The entire dataset consists of 6 indepen-
dent subsets of resource metrics data belonging to 6 different
VMs. Each subset contains more than 170 thousand records
where each record is described by 26 metric features (1st-
4th are CPU-based, 5th-12th are disk-based, 13th-19th are
memory-based, and 20th-26th are network-based), which
are speciﬁed in Table 1. We partition each subset into
three disjoint datasets: training dataset for model training,
validation dataset for threshold setting, and test dataset for
performance evaluation.

2) Evaluation metrics: To evaluate the performance of the
proposed FL-based multi-discriminator BiWGAN-GP algo-
rithm and the contrast algorithms on anomaly detection,
the following metrics are considered as the comparison
indicators, which can be categorized into two aspects: (1)
Efﬁciency in terms of the communication, computation and
memory cost on VM monitors, network slice managers and
network controller in the training process; (2) Effectiveness
in terms of Precision, Recall, F1-socre and Accuracy, which
are formulated based on four fundamental indicators: TN
(true negative) represents the number of correctly identiﬁed
normal behaviors, FN (false negative) indicates the number
of incorrectly identiﬁed abnormal behaviors as normal ones,
FP (false positive) represents the number of incorrectly
identiﬁed normal behaviors as abnormal ones, and TP
(true positive) indicates the number of correctly identiﬁed
abnormal behaviors.

Precision refers to the ratio of correctly identiﬁed ab-
normal behaviors to the total identiﬁed abnormal ones,
calculated by

P recision =

T P
T P + F P

.

(29)

Recall refers to the ratio of correctly identiﬁed abnormal

behaviors to the total actual abnormal ones, calculated by

Recall =

T P
T P + F N

.

(30)

The detailed steps of FL-based multi-discriminator

BiWGAN-GP algorithm are presented in Algorithm 2.

1. VNFDataset: virtual IP Multimedia IP system, https://www.kagg
le.com/imenbenyahia/clearwatervnf-virtual-ip-multimedia-ip-system.

TABLE 1
Metric Features in VNFDataset

9

Feature types

Feature numbers

Detailed description

CPU-based
Disk-based
Memory-based
Network-based

4
8
7
7

Including CPU idle percent, wait percent, system percent, stolen percent, etc.
Including disk used percent, read/write requests, read/write frequencies (times/s), read/write rates (kbytes/s), etc.
Including average loads, usable percent, usable size (MB), free size (MB), total size (MB), etc.
Including in/out size (bytes/s), in/out errors (/s), in/out packets (/s), dropped packets (/s), etc.

F1-score is the weighted average of the precision and

recall, calculated by

F 1-score = 2 ×

precison × recall
precison + recall

.

(31)

Accuracy refers to the ratio of correctly identiﬁed behav-

iors to the total ones, calculated by

Accuracy =

T P + T N
T P + T N + F P + F N

.

(32)

5.2 Efﬁciency

Computational complexity: We analyze the computational
complexities of the proposed FL-based multi-discriminator
BiWGAN-GP algorithm from three different sides, including
VM monitors, network slice managers and the network con-
troller, and compare them with the standalone BiWGAN-GP
algorithm (only runs in VM monitors), multi-discriminator
BiWGAN-GP algorithm (runs in VM monitors and network
slice managers), and centralized BiWGAN-GP algorithm
(only runs in the network controller), respectively. In the
proposed algorithm, the computational complexity on VM
monitors mainly relies on the architecture of discriminator
D, and the computational complexities on network slice
managers and the network controller mainly rely on the
architectures of encoder E and generator G. As D, E and
G are all built using neural networks, their computational
complexities are hard to accurately describe due to many
free variables [37]. For simplicity, we assume that the
number of ﬂoating point operations to process one-batch
data in D, E and G is directly proportional to the cardinality
of model parameters |θD|,
|θE| and |θG|, respectively.
Besides, the number of ﬂoating point operations to update
parameters of D, E and G is also assumed to be directly
proportional to |θD|, |θE| and |θG|, respectively. Therefore,
the detailed computational complexities on VM monitors,
network slice managers and the network controller are
analyzed as follows:

feedbacks

computing error

• VM monitors. On each VM monitor, the complexities
of
and updating
model parameters of D are O(4M |θD|) and
O(4KM |θD|) per training iteration. Therefore, the
total computational complexity on each VM monitor
is O(4I(1 + K)M |θD|), which is much less than
O(4IKM |θD| + 2IM (|θE| + |θG|)) of the standalone
BiWGAN-GP algorithm on each VM monitor. As
the multi-discriminator BiWGAN-GP algorithm is
part of the proposed algorithm, they have the same
computational complexity on VM monitors.

• Network slice managers. On each network slice man-
ager, the complexity of encoding and generating one-
batch data using E and G is O(M N (|θE| + |θG|)),

and the complexity of updating model parameters
of E and G is also O(M N (|θE| + |θG|)). Therefore,
the total computational complexity on each network
slice manager is O(2IM N (|θE| + |θG|)). Similarly,
the proposed algorithm and the multi-discriminator
BiWGAN-GP algorithm have the same computa-
tional complexity on network slice managers.

• Network controller. On the network controller, the
proposed algorithm only needs to aggregate model
parameters of E and G from all network slice
managers every L training iterations. Hence the
computational complexity on the network controller
is O(S(|θE| + |θG|)I/L), which is much less than
O(2SIM N (2K|θD| + |θE| + |θG|)) of the centralized
BiWGAN-GP algorithm on the network controller.

To validate the convergence of BiWGAN-GP model in
different training modes, including centralized, standalone,
distributed, and the proposed FL-based distributed modes
as shown in Fig. 5, the averaged values of discriminator
loss and encoder-generator (EG) loss in each iteration are
recorded and depicted in Fig. 6. In centralized training
mode, the network controller is responsible for collecting
metrics data of all VMs and trains the centralized BiWGAN-
GP model for anomaly detection. In standalone training
mode, the BiWGAN-GP model is trained inside each VM
monitor without any data or parameters exchange. In dis-
tributed training mode, the multi-discriminator BiWGAN-
GP algorithm presented in Algorithm 1 is used for anomaly
detection, where the network slice manager is responsible
for the training of generator and encoder and VM monitors
are responsible for the training of discriminators. The
proposed FL-based distributed training mode is the com-
bination of multiple distributed modes, which is elaborated
in Algorithm 2. Fig. 6(a), (b), (c) and (d) shows the training
processes of BiWGAN-GP model in centralized, standalone,
distributed and FL-based distributed modes, respectively.
The discriminator loss and EG loss both converge to stable
values after about 300 training iterations, which indicates
that the generators, encoders and discriminators all can be
successfully trained in all four training modes. By contrast,
the proposed FL-based distributed training mode enables
the discriminator loss and EG loss to converge to the most
stable values and achieve the best convergence performance
because of its collaboration feature.

Fig. 7(a) shows the computation cost of the above four
training modes under different number of iterations. The
computation cost is measured by the consumed time for
training. By contrast, the centralized training mode needs
the most training time because it requires the network
controller to collect metrics data from all VM monitors
for the centralized processing. The standalone training

10

Fig. 5. The diagram of different training modes.

(a)

(b)

(c)

(d)

Fig. 6. Training processes of BiWGAN-GP model in different training modes. (a) Centralized training mode; (b) Standalone training mode; (c) Multi-
discriminator BiWGAN-GP model in distributed training mode; (d) Multi-discriminator BiWGAN-GP model in FL-based distributed training mode.

training time because each VM
mode needs the least
monitor trains the BiWGAN-GP model individually using
its own local metrics data. Besides, the distributed and FL-
based distributed training modes for multi-discriminator
BiWGAN-GP model are more efﬁcient than the centralized
mode, but slightly inferior to the standalone mode.

In standalone mode, there is no communication among
VM monitors, network slice managers and the network
controller. In centralized mode, VM monitors send their all
metrics data to the corresponding network slice managers
and assemble on the network controller at the beginning.
As the distributed and FL-based distributed training modes
decentralize the discriminator training tasks to VM mon-
itors by leaving generator and encoder training tasks on
network slice managers, the communication cost between
network slice managers and VM monitors is reasonable
compared with the centralized mode. Fig. 7(b) shows the
communication cost between network slice managers and
VM monitors with the variation of batch sizes. The larger
batch size brings a higher communication cost due to the
increased data exchanged between network slice managers
and VM monitors. Fig. 7(c) shows the communication cost
between network slice managers and the network controller
when increasing the number of local iterations L before
aggregation. The communication cost decreases with the
increase of L because the bigger L means less interactions
between network slice managers and the network controller.
Fig. 7(d) shows the memory cost for the above four

training modes as the size of total training dataset increases.
It shows that the memory cost on the network controller
under distributed and FL-based distributed training modes
is much less than that under the centralized training mode.
it also indicates that the memory cost on the
Besides,
VM monitors under distributed and FL-based distributed
training modes is also slightly less than that under the
standalone training mode.

5.3 Effectiveness

After the FL-based multi-discriminator BiWGAN-GP model
is trained well, the abnormality of new metrics data can
be measured through its anomaly score calculated by Eq.
(28). If the anomaly score is larger than a preset threshold,
new metrics data will be determined as abnormal. The
value of threshold will affect the detection performance of
the FL-based multi-discriminator BiWGAN-GP model. The
relationship between precision, recall and F1-score under
different values of threshold is presented in Fig. 8, where
each point is obtained by setting a new value of threshold.
From Fig. 8, we can see that different threshold values
bring different detection performance, so we design a
simple method to obtain an effective threshold value using
the validation dataset. To simulate anomalies in VMs,
four error types are injected to the validation dataset,
including endless loop in CPU, memory leak, Disk I/O fault
and network congestion. Then, we calculate the average

Network controllerNetwork slice manager 1Network slice manager 2Network slice manager 3EGEGEGVM 1VM 2VM 3Slice  1VM monitorDVM monitorDVM monitorDVM 1VM 2VM 3Slice  2VM monitorDVM monitorDVM monitorDVM 1VM 2VM 3Slice  3VM monitorVM monitorVM monitorEGDEGDEGDEGDCentralized modeDistributed modeStandalone modeFL-based distributed mode Data collection for Centralized modeParameters exchange for two distributed modes050010001500Number of Iterations-50510Discriminator Loss-0.500.51EG Loss050010001500Number of Iterations-50510Discriminator Loss-0.500.51EG Loss050010001500Number of Iterations-50510Discriminator Loss-0.500.51EG Loss050010001500Number of Iterations-50510Discriminator Loss-0.500.51EG Loss11

(a)

(b)

(c)

(d)

Fig. 7. Efﬁciency of different training modes. (a) Computation cost; (b) Communication cost between network slice managers (NSM) and VM
monitors; (c) Communication cost between NSM and the network controller; (d) Memory cost.

GAN algorithms, including GAN [20], WGAN [21], WGAN-
GP [23] and BiGAN [22]. For convenience, the standalone
training mode is used for all the ﬁve algorithms in this
simulation. Because the BiWGAN-GP algorithm combines
both the strengths of BiGAN and WGAN-GP, it has ob-
viously improved the detection performance compared to
the state-of-the-art GAN algorithms. Speciﬁcally, compared
to GAN, WGAN, WGAN-GP and BiGAN, the precision of
BiWGAN-GP algorithm has increased by 4.5%, 2.9%, 2.2%,
and 1.8%, respectively, the recall of BiWGAN-GP algorithm
has increased by 4.8%, 2.2%, 2.1%, and 2.7%, respectively,
and the F1-score of BiWGAN-GP algorithm has increased
by 4.6%, 2.5%, 2.0%, and 2.2%, respectively.

the standalone BiWGAN-GP,

Fig. 10 shows the detection performance of the cen-
the
tralized BiWGAN-GP,
multi-discriminator BiWGAN-GP and the FL-based multi-
discriminator BiWGAN-GP algorithms. We use the preci-
sion, recall and F1-socre as the evaluation metrics. We can
see that the values of three metrics using any algorithm
are nearly stable after 300 iterations, which are consistent
with the convergence trends of discriminator and EG losses
presented in Fig. 6. It indicates that the four algorithms
converge through enough iterations and can capture the
distribution from metrics data of VMs. As shown in Fig. 10,
the FL-based multi-discriminator BiWGAN-GP algorithm
is superior to the standalone BiWGAN-GP and multi-
discriminator BiWGAN-GP algorithms, and slightly infe-
rior to the centralized BiWGAN-GP algorithm. Besides,
according to the variation ranges of recall, precision and
F1-score after 300 iterations, the detection performance of
the FL-based multi-discriminator BiWGAN-GP algorithm is
more stable than that of the standalone BiWGAN-GP and
multi-discriminator BiWGAN-GP algorithms because of its
collaboration feature.

the standalone BiWGAN-GP,

The detection performance of four anomaly detection
algorithms on four anomaly cases is summarized in Ta-
bles 2-5. Four anomaly detection algorithms are the cen-
tralized BiWGAN-GP,
the
multi-discriminator BiWGAN-GP and the FL-based multi-
discriminator BiWGAN-GP algorithms. Four anomaly cases
include endless loop in CPU, memory leak, Disk I/O fault
and network congestion. By contrast, the performance of
the FL-based multi-discriminator BiWGAN-GP algorithm
is superior to the standalone BiWGAN-GP and multi-
discriminator BiWGAN-GP algorithms, and slightly infe-
rior to the centralized BiWGAN-GP algorithm on all four

Fig. 8. The relationship between precision, recall and F1-score.

Fig. 9. The detection performance comparison between BiWGAN-GP
and the state-of-the-art GAN algorithms.

anomaly scores Anormal and Aabnormal of normal and
abnormal metrics in the modiﬁed validation dataset. The
threshold is set as

threshold =

Anormal + Aabnormal
2

.

(33)

BiWGAN-GP is the basic model used in the devel-
oped FL-based three-tier distributed anomaly detection
framework. To validate the detection performance of the
BiWGAN-GP algorithm and the state-of-the-art GAN al-
gorithms in the presence of anomalies, four error types
are injected to the test dataset randomly to simulate the
anomalies in resource metrics data, including endless loop
in CPU, memory leak, Disk I/O fault and network conges-
tion. Fig. 9 shows the detection performance comparison
between the BiWGAN-GP algorithm and the state-of-the-art

400600800100012001400Number of Iterations0100200300400500600Time Cost (sec)StandaloneCentralizedDistributedFL-based distributed1020304050Batch Size51525354555Communication Cost (KB)VM monitor->NSMNSM->VM monitor1020304050Number of Iterations  L before Aggregation0100200300400Communication Cost (KB)NSM<->Controller12345Size of Total Training Dataset (8*105)10-210-1100101102Memory Cost (MB)StandaloneCentralizedDistributed-VM MonitorDistributed-NSMDistributed-Controller0.210.410.6F1-score0.8Recall0.50.8Precision10.600.4PrecisionRecallF1-score0.80.850.90.951ValueGANWGANWGAN-GPBiGANBiWGAN-GPTABLE 4
Detection performance on Disk I/O fault

12

Algorithms

Accuracy

Precision

Recall

F1-score

Centralized
Standalone
Multi-discriminator
FL-based multi-discriminator

0.9874
0.9559
0.9667
0.9740

0.9835
0.9544
0.9654
0.9718

0.9916
0.9570
0.9672
0.9754

0.9873
0.9557
0.9662
0.9736

TABLE 5
Detection performance on network congestion

Algorithms

Accuracy

Precision

Recall

F1-score

Centralized
Standalone
Multi-discriminator
FL-based multi-discriminator

0.9726
0.9540
0.9635
0.9740

0.9713
0.9501
0.9609
0.9698

0.9735
0.9582
0.9659
0.9754

0.9726
0.9542
0.9634
0.9736

6 RELATED WORK
A substantial part of the paper lies at the intersection of
the three following research domains: 1) Monitoring frame-
work for network virtualization environment; 2) GAN-
based anomaly detection algorithm; 3) FL-based distributed
learning paradigm.

6.1 Monitoring framework for network virtualization en-
vironment

it

Network virtualization environment can reduce the de-
ployment cost and simplify the life-cycle management of
introduces new management
network functions, but
challenges and issues [38]. Therefore, building the effective
monitoring framework for network virtualization environ-
ment is vital to ensure its high availability and reliability. To
satisfy the security requirements of VMs, a comprehensive
protection mechanism with the capacity of defense-in-depth
for VMs was proposed in [39], where the anomaly detection
was realized by building the normal trafﬁc model and
determining the abnormal behaviors according to their
degree of deviation from the normal behaviors. Mishra
et al.
[40] designed a multi-level security architecture,
namely VMGuard, for VM monitoring at the process level
and system call level to detect known attacks and their
variants. VMGuard applied the random forest classiﬁer to
produce a generic behavior for different categories of attacks
in monitored VMs. As container-based virtualization has
become a key solution in present virtualized networks, Zou
et al. [4] developed an online container anomaly detection
framework by monitoring and analyzing the resource-
relevant metrics of containers with the optimized isolation
forest algorithm. The considered metrics included CPU
usage, memory usage, disk read/write speed and network
speed. However, the monitoring framework designed in [4],
[39], [40] used centralized database to collect and process
relevant data of VMs distributed in different regions of
substrate networks, which will cause high communication
and computation overhead when learning a global VM
anomaly detection model. Our previous work [41] designed
a distributed monitoring mechanism from the perspective of
physical nodes and links, not from the entire networks. In
this paper, for each region of the entire networks, we deploy
a VM monitor for a network slice to collect and store metrics

(a)

(b)

(c)

Fig. 10. The detection performance comparison among four algorithms.
(a) Precision; (b) Recall; (c) F1-score.

TABLE 2
Detection performance on endless loop in CPU

Algorithms

Accuracy

Precision

Recall

F1-score

Centralized
Standalone
Multi-discriminator
FL-based multi-discriminator

0.9842
0.9554
0.9651
0.9740

0.9833
0.9549
0.9650
0.9710

0.9848
0.9549
0.9645
0.9774

0.9841
0.9549
0.9648
0.9742

TABLE 3
Detection performance on memory leak

Algorithms

Accuracy

Precision

Recall

F1-score

Centralized
Standalone
Multi-discriminator
FL-based multi-discriminator

0.9860
0.9557
0.9673
0.9743

0.9769
0.9563
0.9664
0.9702

0.9892
0.9590
0.9683
0.9783

0.9853
0.9560
0.9674
0.9742

anomaly cases.

30060090012001500Number of Iterations0.850.90.951PrecisionStandalone BiWGAN-GPCentralized BiWGAN-GPMulti-discriminator BiWGAN-GPFL-based multi-discriminator BiWGAN-GP100400700100013001500Number of Iterations0.850.90.951RecallStandalone BiWGAN-GPCentralized BiWGAN-GPMulti-discriminator BiWGAN-GPFL-based multi-discriminator BiWGAN-GP100400700100013001500Number of Iterations0.850.90.951F1-scoreStandalone BiWGAN-GPCentralized BiWGAN-GPMulti-discriminator BiWGAN-GPFL-based multi-discriminator BiWGAN-GPdata of VMs in the network slice as a local database. The
distributed anomaly detection can be trained over datasets
that are spread across the entire networks.

6.2 GAN-based anomaly detection algorithm

GAN is a promising generative model proposed by Good-
fellow et al. [33], which is superior in capturing the distribu-
tion from high-dimensional complex real-world data. GAN
consists of two models: the generator and the discriminator,
which are usually built with neural networks. By training
the generator and discriminator through the adversarial
learning, the Nash equilibrium can be achieved theoretically
[42], where the generator can generate fake samples sharing
the same distribution with real ones that the discriminator
is unable to distinguish between them. From this, GAN
and its improved modiﬁcations have been widely used to
implement anomaly detection in various ﬁelds, such as in
distribution systems [14], [43], in vehicular ad hoc networks
[44], in high-speed railways [45] and in movie reviews [37].
The rationale behind the GAN-based anomaly detection
is capturing the distribution from normal data and then
ﬁnding an effective discriminant criterion to distinguish
abnormal data from normal ones. In [14], BiWGAN was
used as a feature extractor to map normal samples back
to latent space and support vector data description was
used to establish the discriminant criterion for nontechnical
losses detection in power system. Using smart meters’ data,
a GAN was designed in [43] to extract the normal temporal-
spatial behavior of distribution systems. When abnormal
behaviors occurred, the smart meters’ data were expected
to have anomaly scores out of the normal range. Shu
et al. [44] proposed a distributed BiGAN framework to
detect abnormal network behaviors for the entire vehicular
ad hoc networks without exchanging data belonging to
local sub-networks. In [45], a deep convolutional GAN
was constructed to map the images of catenary support
components into high-dimensional feature spaces, and an
anomaly rating criterion was deﬁned to detect abnormal
images of high-speed railways. Gao et al. [37] developed a
novel attention-driven conditional GAN to capture the cor-
relations of movie reviews and identify spammed reviews.
The effectiveness of GAN and its improved modiﬁcations in
anomaly detection has been fully veriﬁed through extensive
researches in these ﬁelds. In this paper, we design a new
multi-discriminator BiWGAN-GP algorithm with multiple
discriminators running locally in view of the distributed
data features in virtualized network slicing environment.

6.3 FL-based distributed learning paradigm

FL is developed as a distributed leaning framework where
training data are decentralized across learners, rather than
being centralized [46], [47], [48], [49]. FL allows each learner
to train a model from local data and send its local model
to a center periodically. The center is responsible for model
aggregation and sharing the global model with all learners.
In [46], to model reliability in vehicular communications
with low communication cost, FL was used to enable
vehicular users to learn the tail distribution of network-wide
queue lengths locally without sharing the actual queue data.

13

As anomalies in edge devices seriously affect the manufac-
turing process in industrial IoT, a communication-efﬁcient
FL based deep anomaly detection framework was proposed
in [47] to enable distributed edge devices to collaboratively
train a global detection model with improved generalization
ability. Darwish et al. [48] extended FL to achieve the
self-evolving network management for future intelligent
vertical HetNet. In [49], based on the three-layer association
relationship in radio access network slicing (device-base
station-network slicing), a hybrid FL reinforcement learning
framework was exploited to solve the device association
problem while enforcing data security and device privacy,
which included a horizontal aggregation on base stations
for the same service types and a vertical aggregation on an
encrypted center for different service types. Inspired by the
existing researches, FL-based distributed learning paradigm
can well adapt to the hierarchical control of virtualized
network slicing, which facilitates the establishment of a
global anomaly detection model without compromising the
communication and computation efﬁciency.

7 CONCLUSIONS

The novelty of the paper lies in the combination of multi-
discriminator BiWGAN-GP and FL to detect anomalies in
VMs in the context of virtualized network slicing envi-
ronment. Within a network slice, the multi-discriminator
BiWGAN-GP algorithm implements discriminators on VM
monitors, and the generator and encoder on its network
slice manager to monitor and analyze the resource metrics
data of VMs in a distributed way. Then, the updated
parameters of generators and encoders trained on network
slice managers are sent to the network controller for ag-
gregation using FL framework. We achieve a global VM
anomaly detection model through the hierarchical cooper-
ation among VM monitors, network slice managers and
the network controller. We deﬁne an anomaly score using
the reconstruction loss and discriminator loss of the multi-
discriminator BiWGAN-GP algorithm, and then adopt the
validation dataset to obtain the effective threshold for the
anomaly score to distinguish between normal and abnormal
behaviors. The experiment results on a real-world dataset
validate the efﬁciency and effectiveness of the proposed
FL-based multi-discriminator BiWGAN-GP algorithm on
detecting typical anomalies of VMs in virtualized network
slicing environment.

REFERENCES

[1]

[2]

Khamse-Ashari, G.

J. Ordonez-Lucena et al., “Network slicing for 5G with SDN/NFV:
Concepts, architectures, and challenges,” IEEE Commun. Mag.,
vol. 55, no. 5, pp. 80–87, May 2017.
J.
and
H. Yanikomeroglu, “An agile and distributed mechanism
for inter-domain network slicing in next-generation mobile
networks,” IEEE Trans. Mobile Comput., early access, Feb. 23, 2021,
doi: 10.1109/TMC.2021.3061613.

Bor-Yaliniz,

Senarath,

I.

[3] F. Z. Yousaf, M. Bredel, S. Schaller, and F. Schneider, “NFV and
SDN-Key technology enablers for 5G networks,” IEEE J. Sel. Areas
Commun., vol. 35, no. 11, pp. 2468–2478, Nov. 2017.

[4] Z. Zou et al., “A docker container anomaly monitoring system
based on optimized isolation forest,” IEEE Trans. Cloud Comput.,
vol. 10, no. 1, pp. 134–145, Jan.-Mar. 2022.

14

[29] “OSM: Open-Source MANO,” 2016. [Online]. Available: https:

//osm.etsi.org/.

[30] “OpenStack Tacker,” 2017.
openstack.org/wiki/Tacker.

[Online]. Available: https://wiki.

[31] A. Laghrissi and T. Taleb, “A survey on the placement of virtual
resources and virtual network functions,” IEEE Commun. Surveys
Tuts., vol. 21, no. 2, pp. 1409–1434, 2nd Quart. 2019.

[32] “OpenBaton,” 2015.

[Online]. Available: https://openbaton.

github.io/.

[33] I. J. Goodfellow et al., “Generative adversarial nets,” in Proc. Adv.

Neural Inf. Process. Syst., Jun. 2014, pp. 2672–2680.

[34] K. Greff et al., “LSTM: A search space odyssey,” IEEE Trans. Neural

Netw. Learn. Syst., vol. 28, no. 10, pp. 2222–2232, Oct. 2017.

[35] C. Hardy, E. Le Merrer, and B. Sericola, “MD-GAN: Multi-
discriminator generative adversarial networks for distributed
datasets,” in Proc. IEEE Int. Parallel Distrib. Process. Symp. (IPDPS),
May 2019, pp. 866–877.

[36] T. Schlegl et al., “Unsupervised anomaly detection with generative
adversarial networks to guide marker discovery,” in Proc. Int. Conf.
Inf. Process. Med. Imag., May 2017, pp. 146–157.

[37] Y. Gao, M. Gong, Y. Xie, and A. K. Qin, “An attention-
based unsupervised adversarial model for movie review spam
detection,” IEEE Trans. Multimedia, vol. 23, pp. 784–796, Jan. 2021.
[38] S. Cherrared et al., “A survey of fault management in network
virtualization environments: Challenges and solutions,” IEEE
Trans. Netw. Service Manag., vol. 16, no. 4, pp. 1537–1551, Dec. 2019.
[39] X. Yin et al., “Research of security as a service for VMs in IaaS

platform,” IEEE Access, vol. 6, pp. 29 158–29 172, Jun. 2018.

[40] P. Mishra, V. Varadharajan, E. S. Pilli, and U. Tupakula,
“VMGuard: A VMI-based security architecture for intrusion
detection in cloud environment,” IEEE Trans. Cloud Comput., vol. 8,
no. 3, pp. 957–971, Jul.-Sep. 2020.

[41] W. Wang et al., “Distributed online anomaly detection for virtual-
ized network slicing environment,” IEEE Trans. Veh. Technol., pp.
1–15, early access, Jul. 2022, doi: 10.1109/TVT.2022.3193074.
[42] K. Wang et al., “Generative adversarial networks: Introduction and
outlook,” IEEE/CAA J. Autom. Sinica, vol. 4, no. 4, pp. 588–598, Sep.
2017.

[43] Y. Yuan, K. Dehghanpour, F. Bu, and Z. Wang, “Outage detection
in partially observable distribution systems using smart meters
and generative adversarial networks,” IEEE Trans. Smart Grid,
vol. 11, no. 6, pp. 5418–5430, Nov. 2020.

[44] J. Shu et al., “Collaborative intrusion detection for VANETs: A deep
learning-based distributed SDN approach,” IEEE Trans. Intell.
Transp. Syst., vol. 22, no. 7, pp. 4519–4530, Jul. 2021.

[45] Y. Lyu et al., “A generic anomaly detection of catenary support
components based on generative adversarial networks,” IEEE
Trans. Instrum. Meas., vol. 69, no. 5, pp. 2439–2448, May 2020.
[46] S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Distributed
federated learning for ultra-reliable low-latency vehicular commu-
nications,” IEEE Trans. Commun., vol. 68, no. 2, pp. 1146–1159, Feb.
2020.

[47] Y. Liu et al., “Deep anomaly detection for time-series data in
industrial IoT: A communication-efﬁcient on-device federated
learning approach,” IEEE Internet Things J., vol. 8, no. 8, pp. 6348–
6358, Apr. 2021.

[48] T. Darwish et al., “A vision of self-evolving network management
for future intelligent vertical hetnet,” IEEE Wireless Commun.,
vol. 28, no. 4, pp. 96–105, Aug. 2021.

[49] Y.-J. Liu et al., “Device association for RAN slicing based on hybrid
federated deep reinforcement learning,” IEEE Trans. Veh. Technol.,
vol. 69, no. 12, pp. 15 731–15 745, Dec. 2020.

[5] H. Zhang, J. Liu, and T. Wu, “Adaptive and incremental-clustering
anomaly detection algorithm for VMs under cloud platform
runtime environment,” IEEE Access, vol. 6, pp. 76 984–76 992, Dec.
2018.

[6] R. Cogranne, G. Doyen, N. Ghadban, and B. Hammi, “Detecting
botclouds at large scale: A decentralized and robust detection
method for multi-tenant virtualized environments,” IEEE Trans.
Netw. Service Manag., vol. 15, no. 1, pp. 68–82, Mar. 2018.

[7] C. Sauvanaud et al., “Anomaly detection and diagnosis for cloud
services: Practical experiments and lessons learned,” J. Syst. Softw.,
vol. 139, pp. 84–106, May 2018.

[8] H. Liu et al., “Rain: Towards real-time core devices anomaly
detection through session data in cloud network,” in Proc. NOMS-
IEEE/IFIP Netw. Operations Manage. Symp., Apr. 2020, pp. 1–6.
[9] W. Wang, Q. Chen, X. He, and L. Tang, “Cooperative anomaly
detection with transfer learning-based hidden markov model in
virtualized network slicing,” IEEE Commun. Lett., vol. 23, no. 9,
pp. 1534–1537, Sep. 2019.

[10] Q. Jiang and X. Yan, “Multimode process monitoring using
variational bayesian inference and canonical correlation analysis,”
IEEE Trans. Autom. Sci. Eng., vol. 16, no. 4, pp. 1814–1824, Oct.
2019.

[11] O. Onireti et al., “A cell outage management framework for dense
heterogeneous networks,” IEEE Trans. Veh. Technol., vol. 65, no. 4,
pp. 2097–2113, Apr. 2016.

[12] Q. Liao and S. Stanczak, “Network state awareness and proactive
anomaly detection in self-organizing networks,” in Proc. IEEE
Globecom Workshops (GC Wkshps), Dec. 2015, pp. 1–6.

[13] X. Miao, Y. Liu, H. Zhao, and C. Li, “Distributed online one-class
support vector machine for anomaly detection over networks,”
IEEE Trans. Cybern., vol. 49, no. 4, pp. 1475–1488, Apr. 2019.
[14] T. Hu et al., “Nontechnical losses detection through coordinated
BiWGAN and SVDD,” IEEE Trans. Neural Netw. Learn. Syst.,
vol. 32, no. 5, pp. 1866–1880, May 2021.

[15] J. Chen, M. Chen, X. Wei, and B. Chen, “Matrix differential
decomposition-based anomaly detection and localization in NFV
networks,” IEEE Access, vol. 7, pp. 29 320–29 331, Jan. 2019.
[16] Y. Cheng, H. Zhu, J. Wu, and X. Shao, “Machine health monitoring
using adaptive kernel spectral clustering and deep long short-
term memory recurrent neural networks,” IEEE Trans. Ind. Inform.,
vol. 15, no. 2, pp. 987–997, Feb. 2019.

[17] Z. Li, A. L. G. Rios, and L. Trajkovi´c, “Machine learning for
detecting anomalies and intrusions in communication networks,”
IEEE J. Sel. Areas Commun., vol. 39, no. 7, pp. 2254–2264, Jul. 2021.
[18] I. Nevat et al., “Anomaly detection and attribution in networks
with temporally correlated trafﬁc,” IEEE/ACM Trans. Netw.,
vol. 26, no. 1, pp. 131–144, Feb. 2018.

[19] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, “A
review of novelty detection,” Signal Process., vol. 99, no. 6, pp.
215–249, Jun. 2014.

[20] D. Giannopoulos, P. Papaioannou, C. Tranoris, and S. Denazis,
“Monitoring as a service over a 5G network slice,” in Proc. Joint
Eur. Conf. Netw. Commun. 6G Summit (EuCNC/6G Summit), Jun.
2021, pp. 329–334.

[21] V. A. Cunha et al., “5 Growth: Secure and reliable network slicing
for verticals,” in Proc. Joint Eur. Conf. Netw. Commun. 6G Summit
(EuCNC/6G Summit), Jun. 2021, pp. 347–352.

[22] C. Papagianni et al., “5Growth: AI-driven 5G for automation in
vertical industries,” in Proc. Eur. Conf. Netw. Commun. (EuCNC),
Jun. 2020, pp. 17–22.

[23] A. Creswell

et

al., “Generative adversarial networks: An
overview,” IEEE Signal Process. Mag., vol. 35, no. 1, pp. 53–65, Jan.
2018.

[24] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” Dec.
2017, arXiv:1701.07875. [Online]. Available: http://arxiv.org/abs/
1701.07875.

[25] J. Donahue, P. Kr¨ahenb ¨uhl, and T. Darrell, “Adversarial feature
[Online]. Available:

learning,” Apr. 2017, arXiv:1605.09782.
http://arxiv.org/abs/1605.09782.

[26] I. Gulrajani et al., “Improved training of Wasserstein GANs,” in
Proc. Adv. Neural Inf. Process. Syst., May 2017, pp. 5769–5779.
[27] Y. Liu et al., “Privacy-preserving trafﬁc ﬂow prediction: A
federated learning approach,” IEEE Internet Things J., vol. 7, no. 8,
pp. 7751–7763, Aug. 2020.

[28] J. Kone ˇon ´y et al., “Federated learning: Strategies for improving
communication efﬁciency,” Oct. 2017, arXiv:1610.05492. [Online].
Available: https://arxiv.org/abs/1610.05492.

