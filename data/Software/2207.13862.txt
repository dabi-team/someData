2
2
0
2

l
u
J

8
2

]
S
M

.
s
c
[

1
v
2
6
8
3
1
.
7
0
2
2
:
v
i
X
r
a

HDSDP: Software for Semideﬁnite Programming

Wenzhi Gao∗ Dongdong Ge† Yinyu Ye‡

July 29, 2022

Abstract

HDSDP is a numerical software solving the semideﬁnite programming problems. The main frame-
work of HDSDP resembles the dual-scaling interior point solver DSDP [2] and several new features,
especially a dual method based on the simpliﬁed homogeneous self-dual embedding, have been
implemented. The embedding enhances stability of dual method and several new heuristics
and computational techniques are designed to accelerate its convergence. HDSDP aims to
show how dual-scaling algorithms beneﬁt from the self-dual embedding and it is developed
in parallel to DSDP5.8. Numerical experiments over several classical benchmark datasets ex-
hibit its robustness and eﬃciency, and particularly its advantages on SDP instances featuring
low-rank structure and sparsity. The pre-built binary of HDSDP is currently freely available at
https://github.com/COPT-Public/HDSDP.

1 Introduction

Semideﬁnite programming (SDP) is a mathematical programming problem deﬁned by

min
X

(cid:104)C, X(cid:105)

subject to AX = b
X ∈ Sn
+,

(1)

where we study linear optimization subject to aﬃne constraints over the cone of positive-semideﬁnite
matrices. Due to its extensive modeling capability, SDP has been employed in various communi-
ties including combinatorial optimization [9, 14], dynamic systems [25], sums of squares optimiza-
tion [13], quantum information [10], and distance geometry [5, 22]. We refer the interested readers
to [26] for a more comprehensive review of SDP applications.

∗gwz@163.shufe.edu.cn
†ge.dongdong@mail.shufe.edu.cn
‡yyye@stanford.edu

1

 
 
 
 
 
 
While SDP proves useful in many applications, a fundamental issue is how to numerically solve
them. Theoretically, SDP is a convex conic problem which admits eﬃcient polynomial-time al-
gorithms and for general SDPs, the interior point method (IPM) is known as a most robust and
eﬃcient approach. Since the 1990s, high-performance SDPs softwares based on the IPM have been
developed, including DSDP [2], COPT [17], Mosek [1], Sedumi [18], SDPT3 [24], CSDP [7] and SDPA [28].
While most SDP codes implement the IPM, there also exist a number of successful attempts adopt-
ing other algorithms including [11, 12, 29] and a list of such SDP software is available from [15].

SDP solvers based on diﬀerent IPM variants enjoy nice convergence behavior both theoretically and
practically. Most SDP solvers implement a path-following primal-dual approach either using infea-
sible start [20] or the homogeneous self-dual embedding [19] with DSDP being an exception. DSDP
implements a dual IPM method based on the potential reduction framework proposed in [4]. Since
the initial release [3], DSDP has gone a long way through several major updates and evolved into an
eﬃcient and robust solver for general SDPs [2]. To further enhance the eﬃciency and robustness of
DSDP, we make another extension by incorporating the well-known homogeneous self-dual (HSD)
embedding into the dual algorithm. This new implementation, named HDSDP, is presented in this
manuscript.

The rest of the manuscript is organized as follows. Section 2 describes the SDP formulation of
interest and basic notations. Section 3 reviews the dual-scaling algorithm for SDP and describes
how to combine it with the simpliﬁed HSD embedding.
In Section 4 and 5, we introduce the
practical aspects for HDSDP. Last we present the computational results of various SDP problems.

2 Formulation and Notations

HDSDP is interested in the standard form primal SDP and its dual

(P )

minX

(cid:104)C, X(cid:105)

(D) maxy,S

subject to (cid:104)Ai, X(cid:105) = b,

i = 1, . . . , m

X ∈ Sn
+

subject to (cid:80)m

b(cid:62)y
i=1 Aiyi + S = C

S ∈ Sn
+,

where the problem data {Ai}, C are n × n symmetric matrices (Sn) and b ∈ Rm is a real vec-
tor. Matrix inner product (cid:104)·, ·(cid:105) is deﬁned by (cid:104)A, X(cid:105) := (cid:80)
+ denotes the cone
of positive semideﬁnite matrices. For brevity we use X (cid:23) 0 to denote the relation X ∈ Sn
+
and the linear map A : Sn → Rm and its adjoint A∗ : Rm → Sn are respectively deﬁned by
AX := ((cid:104)A, X1(cid:105) , . . . , (cid:104)Am, Xm(cid:105))(cid:62) and A∗y := (cid:80)m
ij denotes matrix
Frobenius norm. With the above notations, we rewrite the primal and dual problems by

i=1 Aiyi. (cid:107)A(cid:107)F :=

k,l cijxij and Sn

(cid:113)(cid:80)

ij a2

2

(P )

minX

(cid:104)C, X(cid:105)
subject to AX = b,

X (cid:23) 0

(D) maxy,S

b(cid:62)y

i = 1, . . . , m

subject to A∗y + S = C

S (cid:23) 0.

and the feasible regions for (P ) and (D) are respectively F(P ) := {X : AX = b, X (cid:23) 0} and
F(D) := {(y, S) : A∗y + S = C, S (cid:23) 0}. Any X ∈ F(P ) is called primal feasible and (y, S) ∈ F(D)
is dual feasible. The interior of the feasible regions are denoted by F 0(P ), F 0(D) and any point in
F 0 is called an interior point solution.

Remark 1 Although in this manuscript we focus on the SDP of single-block for ease of exposition,
a natural generalization to multi-block SDPs applies rather straightforward.

HDSDP implements a dual method that solves both (P ) and (D) through the simpliﬁed HSD and in
the next section we get down to the underlying theoretical details.

3 Homogeneous Dual Scaling Algorithm

In this section we review the dual-scaling algorithm in DSDP and its interpretations through Newton’s
method. Then we show how the simpliﬁed HSD embedding applies to the dual method leveraging
this interpretation.

3.1 Dual-scaling Algorithm

The dual-scaling algorithm for SDP is initially proposed in [4] and works under three conditions. 1)
the data {Ai} are linearly independent. 2) (P ) and (D) both admit interior point solution. 3) an in-
terior dual feasible solution (cid:0)y0, S0(cid:1) ∈ F 0(D) is known. The ﬁrst two conditions imply strong dual-
ity and the existence of a primal-dual optimal pair (X∗, y∗, S∗) satisfying complementarity condition
(cid:104)X∗, S∗(cid:105) = 0 and XS = 0. Also, the central path C(µ) := (cid:8)(X, y, S) ∈ F 0(P ) × F 0(D) : XS = µI(cid:9)
is guaranteed to exist, which serves as the foundation of the path-following IPMs. Given barrier
parameter µ, by the last condition, dual-scaling starts from a dual-feasible solution (y, S) and takes
Newton’s step towards AX = b, A∗y + S = C and XS = µI by solving

A (X + ∆X) = b
A∗∆y + ∆S = 0
µS−1∆SS−1 + ∆X = µS−1 − X,

(2)

where the last relation linearizes X = µS−1 instead of XS = µI and S−1 is called a scaling matrix.
By carefully driving µ to 0, dual-scaling eliminates infeasibility, approaches optimality, and ﬁnally
solves the problem.
One feature of dual-scaling is that X and ∆X vanish in the Schur complement system of (2)

3







(cid:10)A1, S−1A1S−1(cid:11)
...
(cid:10)Am, S−1A1S−1(cid:11)

· · ·
. . .
· · ·

(cid:10)A1, S−1AmS−1(cid:11)
...
(cid:10)Am, S−1AmS−1(cid:11)







∆y =

1
µ

b − AS−1

(3)

and the dual algorithm thereby avoids explicit reference to the primal variable X. For brevity we
sometimes denote the left-hand side matrix in (3) by M. When {Ai} , C are sparse, the dual vari-
able S = C − A∗y inherits the sparsity pattern from the data and it is therefore cheaper to iterate
in the dual space to exploit sparsity. Another desirable feature of dual-scaling is the availablity
of primal solution by solving a projection subproblem at the end of the algorithm [2]. The above
properties characterize the behavior of dual-scaling.

However, the nice theoretical properties of the dual method is not free. First, an initial dual feasible
solution is needed but obtaining such a solution is often as diﬃcult as solving the original problem.
Second, due to a lack of information from the primal space, the dual-scaling has to be property
guided to avoid deviating too far away from the central path. Last, dual-scaling linearizes the highly
nonlinear relation X = µS−1 and this imposes strict constraint on the damping factor towards the
Newton’s direction. To overcome the aforementioned diﬃculties, DSDP introduces slack variables
with big-M penalty to ensure nonempty interior and a trivial dual feasible solution. Moreover, a
potential function is introduced to guide the dual iterations. These attempts works well in practice
and makes DSDP an eﬃcient general SDP solver. For a complete description of the theoretical as-
pects of DSDP and its delicate implementation, we refer the interested readers to [2, 3].

Although DSDP proves eﬃcient in real practice, the big-M methods requires prior estimation of the
optimal solution to retain optimality. Also, a large penalty might lead to numerical diﬃculties and
misclassiﬁcation of infeasible problems when the problem is ill-conditioned. It is natural to ask if
there exists an alternative to big-M method to address these issues and in this work, we propose
to leverage the well-known HSD embedding.

3.2 Homogeneous Dual-scaling Algorithm

In this section, we introduce the theoretical idea behind HDSDP. HSD embedding is a skew symmetric
system whose non-trivial interior point solution certiﬁcates primal-dual feasibility. HDSDP adopts
the simpliﬁed HSD embedding [27] and its extension to SDP [19].

4

HSD embedding for SDP

AX − bτ = 0
−A∗y + Cτ − S = 0
b(cid:62)y − (cid:104)C, X(cid:105) − κ = 0

X, S (cid:23) 0,

κ, τ ≥ 0,

(4)

where κ, τ ≥ 0 are homogenizing variables for infeasibility detection. Given parameter µ, we
similarly deﬁne the central path

AX − bτ = 0
−A∗y + Cτ − S = 0
b(cid:62)y − (cid:104)C, X(cid:105) − κ = 0

XS = µI,

κτ = µ

X, S (cid:23) 0,

κ, τ ≥ 0.

(5)

Here (y, S, τ ) are jointly considered as dual variables. Given a dual point (y, S, τ ) such that
−A∗y + Cτ − S = R, HDSDP chooses a damping factor γ ∈ (0, 1] and takes Newton’s step towards

A (X + ∆X) − b(τ + ∆τ ) = 0

−A∗ (y + ∆y) + C(τ + ∆τ ) − (S + ∆S) = −γR

µS−1∆SS−1 + ∆X = µS−1 − X,
µτ −2∆τ + ∆κ = µτ −1 − κ,

where, as in DSDP, we modify (5) and linearize X = µS−1 and κ = µτ −1. We note that the damping
factor can be chosen after the directions are computed and for simplicity we set γ = 0. Then the
Newton’s direction (∆y, ∆τ ) is computed through the following Schur complement.

(cid:32)

(cid:32)

=

µM

−b − µAS−1CS−1

−b + µAS−1CS−1 −µ (cid:0)(cid:10)C, S−1CS−1(cid:11) + τ −2(cid:1)

(cid:33) (cid:32)

∆y
∆τ

bτ
b(cid:62)y − µτ −1

(cid:33)

(cid:32)

− µ

AS−1
(cid:10)C, S−1(cid:11)

(cid:33)

(cid:32)

+ µ

AS−1RS−1
(cid:10)C, S−1RS−1(cid:11)

(cid:33)

(cid:33)

(6)

In practice, HDSDP solves ∆y1 := M−1b, ∆y2 := M−1AS−1, ∆y3 := M−1AS−1RS−1, ∆y4 =
M−1AS−1CS−1, plugs ∆y = τ +∆τ
µ ∆y1 − ∆y2 + ∆y3 + ∆y4∆τ into the second equation to
solve for ∆τ and ﬁnally assembles ∆y using τ, ∆τ and µ. With the above relations, X(µ) :=
µS−1 (S − R + A∗∆y − C∆τ ) S−1 satisﬁes AX(µ)−b(τ +∆τ ) = 0 and X(µ) (cid:23) 0 iﬀ. −A∗ (y − ∆y)+

5

C(τ − ∆τ ) − 2R (cid:23) 0. When −A∗ (y − ∆y) + C(τ − ∆τ ) − 2R is positive deﬁnite, an objective
bound follows by

¯z = (cid:104)Cτ, X(µ)(cid:105)

= (cid:10)R + S + A∗y, µS−1 (S − R + A∗∆y − C∆τ ) S−1(cid:11)
= (τ + ∆τ )b(cid:62)y + nµ + (cid:0)AS−1 + AS−1RS−1(cid:1)(cid:62)

∆y + µ (cid:0)(cid:10)C, S−1(cid:11) + (cid:10)C, S−1CS−1(cid:11)(cid:1) ∆τ

and dividing both sides by τ . Alternatively HDSDP extracts a bound from the projection problem

min
X
subject to

(cid:13)S1/2XS1/2 − µI(cid:13)
(cid:13)
2
(cid:13)
F
AX = bτ,

µ ∆y1 − ∆y2

whose optimal solution is X(cid:48)(µ) := µS−1 (Cτ − A∗ (y − ∆(cid:48)y) − R) S−1, where ∆y(cid:48) = τ
and

(cid:111)

z(cid:48) = (cid:104)Cτ, X(cid:48)(µ)(cid:105) = µ

(cid:110)(cid:10)R, S−1(cid:11) + (cid:0)AS−1RyS−1 + AS−1(cid:1)(cid:62)
In practice, when the explicit solution X(µ) or X(cid:48)(µ) is needed, we can decompose S and solve two
sets of linear systems to obtain them.
One major computationally intensive part in HDSDP is the setup of the Schur complement matrix
M. Since the objective C often does not enjoy the same structure as {Ai}, the additional cost
from AS−1CS−1 and (cid:10)C, S−1CS−1(cid:11) calls for more eﬃcient techniques to set up M. In HDSDP, a
permutation σ(m) of the rows of M is generated heuristically and M is then set up using one of
the following ﬁve techniques row-by-row.

+ τ b(cid:62)y.

∆(cid:48)y + n

(cid:10)Aσ(1), S−1Aσ(1)S−1(cid:11)







· · ·
. . .

(cid:10)Aσ(1), S−1Aσ(m)S−1(cid:11)
...
(cid:10)Aσ(m), S−1Aσ(m)S−1(cid:11)







Technique M1 and M2 are inherited from DSDP. They exploit the low-rank structure of the problem
data and an eigen-decomposition of the problem data

Ai =

rank(Ai)
(cid:88)

r=1

λirai,ra(cid:62)
i,r

has to be computed at the beginning of the algorithm.

6

Technique M1

1. Setup Bσ(i) = (cid:80)rank(Aσ(i))
2. Compute Mσ(i)σ(j) = (cid:10)Bσ(i), Aσ(j)

λr

r=1

(cid:11) , ∀j ≥ i.

(cid:0)S−1aσ(i),r

(cid:1) (cid:0)S−1aσ(i),r

(cid:1)(cid:62)

.

Technique M2

1. Setup S−1aσ(i),r, r = 1, . . . , rσ(i).

2. Compute Mσ(i)σ(j) = (cid:80)rank(Aσ(i))

r=1

(cid:0)S−1aσ(i),r

(cid:1)(cid:62)

Aσ(j)

(cid:0)S−1aσ(i),r

(cid:1).

Technique M3, M4 and M5 exploit the sparsity of the constraints and needs evaluation of S−1. [8].

Technique M3

1. Setup Bσ(i) = S−1Aσ(i)S−1.
2. Compute Mσ(i)σ(j) = (cid:10)Bσ(i), Aσ(j)

(cid:11) , ∀j ≥ i.

Technique M4

1. Setup Bσ(i) = S−1Aσ(i).
2. Compute Mσ(i)σ(j) = (cid:10)Bσ(i)S−1, Aσ(j)

(cid:11) , ∀j ≥ i.

Technique M5

1. Compute Mσ(i)σ(j) = (cid:10)S−1Aσ(i)S−1, Aσ(j)

(cid:11) , ∀j ≥ i directly.

At the beginning of the algorithm, HDSDP estimates the number of ﬂops using each technique and
each row of M is associated with the cheapest technique to minimize the overall computation cost.
This is a major improvement over DSDP5.8 in the computational aspect and we leave the details to
the later sections.

After setting up M, conjugate gradient (CG) method is employed to solve the linear systems. The
maximum number of iterations is chosen around 50/m and is heuristically adjusted. Either the
diagonal of M or its Cholesky decomposition is chosen as pre-conditioner and after a Cholesky
pre-conditioner is computed, HDSDP reuses it for the consecutive solves till a heuristic determines
that the current pre-conditioner is outdated. When the algorithm approaches optimality, M might
become ill-conditioned and HDSDP switches to LDLT decomposition in case Cholesky fails.

Using the Newton’s direction, HDSDP computes the maximum stepsize

α = max {α ∈ [0, 1] : S + α∆S (cid:23) 0, τ + α∆τ ≥ 0}

7

via a Lanczos procedure [23]. Then to determine a proper stepsize, one line-search determines αc
such that the barrier satisﬁes − log det (S + αc∆S) − log det(τ + αc∆τ ) ≤ − log det (S + αc∆S) −
log det(τ +αc∆τ ) and a second line-search chooses γ such that S+αcA∗∆y2+αcγ (R − A∗∆y3) (cid:23) 0.
Next a full direction (∆yγ, ∆Sγ, ∆τ γ) is assembled from the Newton system with damping factor
γ and a third Lanczos procedure computes α(cid:48) = max {α ∈ [0, 1] : S + α∆Sr (cid:23) 0, τ + α∆τ r ≥ 0}.
Last HDSDP updates y ← y + 0.95α(cid:48)∆yr and τ ← τ + 0.95α∆τ r. To make further use of the Schur
matrix and to maintain centrality, the above procedure is repeated several times in an iteration.

In HDSDP, the update of the barrier parameter µ is another critical factor. At the end of each
iteration, HDSDP updates the barrier parameter µ by (cid:0)¯z − b(cid:62)y + θ (cid:107)R(cid:107)F
(cid:1) /ρn, where ρ and θ are
pre-deﬁned parameters. Heuristics also adjust µ using previously computed αc, α(cid:48) and γ.

To get the best of the two worlds, HDSDP implements the same dual-scaling algorithm as in DSDP5.8
assuming a dual feasible solution. When the dual infeasibility (cid:107)A∗y + S − C(cid:107)F ≤ ετ and µ are
suﬃciently small, HDSDP ﬁxes τ = 1, re-starts with (y/τ, S/τ ) and applies dual-scaling to guide
convergence. For the detailed implementation of dual-scaling in DSDP5.8 refer to [2]. To sum up,
HDSDP implements strategies and computational tricks tailored for the embedding and can switch
to DSDP5.8 once a dual feasible solution is available.

4 Initialization and Feasibility Certiﬁcate

Internally HDSDP deals with the following problem

min
X
subject to

(cid:104)C, X(cid:105) + u(cid:62)xu + l(cid:62)xl

AX + xu − xl = b

X (cid:23) 0, xu ≥ 0, xl ≥ 0

and together with its dual, the HSD embedding is

AX + xu − xl − bτ = 0
−A∗y + Cτ − S = 0
b(cid:62)y − (cid:104)C, X(cid:105) − u(cid:62)xu − l(cid:62)xl − κ = 0

X, S (cid:23) 0,

κ, τ ≥ 0,

where the primal problem is relaxed by two slack variables with penalty l, u to prevent y going too
large. Using the embedding, HDSDP needs no big-M initialization in the dual variable and starts from
arbitrary S (cid:31) 0, τ > 0. By default default S is initialized by a multiple of the identity matrix. One
feature of the HSD embedding is its capability to detect infeasibility. Given infeasibility tolerance
εf , HDSDP classiﬁes the problem as primal unbounded, dual infeasible if (cid:107)R(cid:107)F > εf τ, τ /κ < εf

8

and µ/µ0 ≤ ε2
b(cid:62)y > ε−1
Once a ray is detected, the problem is classiﬁed as primal infeasible dual unbounded.

f . If R is eliminated by HDSDP, then the embedding certiﬁcates dual feasibility. If
f , then HDSDP begins checking if the Newton’s step (∆y, ∆S) is a dual improving ray.

5 HDSDP Software

HDSDP is written in ANSI C and provides a self-contained user interface. While DSDP serves as a
sub-routine library, HDSDP is designed to be a stand-alone SDP solver and re-written to accommo-
date the new computational tricks and third-party packages. After the user inputs the data and
invokes the optimization routine, HDSDP goes through several modules including input check, pre-
solving, two-phase optimization, solution recovery and post-solving. The modules are implemented
independently and are sequentially organized in a pipeline by HDSDP.

5.1 Pre-solver

One important feature of HDSDP is a special pre-solving module designed for SDPs.
strategies from DSDP5.8 and adds new tricks to work jointly with the optimization module.

It inherits

When the pre-solver is invoked, it ﬁrst goes through the problem data {Ai} two rounds to detect
the possible low-rank structure. The ﬁrst round uses Gaussian elimination for rank-one structure
and the second round applies eigenvalue decomposition from Lapack. Two exceptions are when
the data matrix is too dense or sparse. If a matrix looks too dense to be decomposed eﬃciently,
it is skipped and marked as full-rank; if it has very few entries, an elegant solution from DSDP5.8
is applied: 1) a permutation gathers the non-zeros to a much smaller dense block. 2) dense eigen
routines from Lapack applies. 3) the inverse permutation recovers the decomposition.

After detecting the hidden low-rank structures, the pre-solver moves on to the analysis of the Schur
matrix M: 1). the sparsity and rank information of the matrices are collected. 2). a permutation
of {Ai} is generated in descending order of sparsity. 3). for each row of M, the ﬂops using each of
M1 to M5 technique is computed and the cheapest technique is recorded. The recorded techniques
reduces the ﬂops to set up M and accelerates the convergence.

Last the pre-solver scales the coeﬃcients and goes on to detect the following structures. 1). Implied
trace: constraints implying tr (X) = θ. 2). Implied dual bound: constraints implying l ≤ y ≤ u. 3).
Empty primal interior: constraints implying tr (cid:0)Xaa(cid:62)(cid:1) ≈ 0. 4). Empty dual interior: constraints
implying A(cid:62)y = C. 5). Feasibility problem. C = 0. For each of the cases above the solver adjusts
its internal strategies to enhance the numerical stability and convergence.

9

5.2 Two-phase Optimization

HDSDP implements two phase-algorithm which integrates HSD embedding (Phase A) and dual-
scaling (Phase B). Phase A targets feasibility certiﬁcate and numerical stability, while Phase B
aims to eﬃciently drive a dual-feasible solution to optimality using potential function.

Figure 1: Pipeline of HDSDP

In a word, the two phases share the same backend computation routines but are associated with
diﬀerent goals and strategies. HDSDP decides which strategy to use based on the solution behavior.

5.3 Iteration Monitor

To help the users capture the progress of the solver, HDSDP prints related information to the screen
at diﬀerent stages of optimization.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Start presolving
| - XXX completes in 0.001 seconds
| - Matrix statistics ready in 0.000 seconds
|
| - Special structures found
tr ( X ) = 3.00 e +03 : Bound of X fixed
|
| Presolve Ends . Elapsed Time : 0.371 seconds
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

Schur Re - ordering : M1 : 0

M2 : 3000

M5 : 0

M3 : 0

M4 : 1

While pre-solving, HDSDP prints time statistics when certain operation is done. Specially, the row

Schur Re-ordering: M1: 0

M2: 3000

M3: 0

M4: 1

M5: 0

indicates how many times each Schur complement technique is applied and if special SDP structures
are detected, they are also printed to the screen.

tr(X) = 3.00e+03 : Bound of X fixed.

10

Pre-solveInputDual FeasiblePhase B Dual-scalingDual-ray detectionPrimal FeasibleSolution recoveryPost-solvingOutputYesNoPhase A Dual Feasibility CertiﬁcateYesNoWhen the pre-solving ends, HDSDP prints matrix statistics including type and sum of norm. Also,
the solver internally adjusts its parameters based on the collected information and display them to
the user.

|

0 |

| b |

| A |

1 |

0 |

Zero |

3000 |

Dense |

Sparse |

Rank -1 |

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Matrix statistics [ Including C ]:
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -| - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Parameter Summary :
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Rhon [1.0 , 10.0]: 5
| Golden linesearch {0 , 1}: 0
| Primal relaxation penalty (0.0 , inf ): 1 e +07
| Time limit (0.0 , inf ): 15000
| Corrector A : 4
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP is initialized with Ry = -1.225 e +05 * I
| DSDP Phase A starts . Eliminating dual infeasibility
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

Corrector B : 0

3.0000 e +03 |

3.0000 e +03 |

1.2000 e +04

| C |

|

After pre-solving, HDSDP enters optimization, invokes HSD embedding (Phase A), and prints logs.

pObj |

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Iter |
E |
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
|
* |
|
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

1.0 e +05 |
0.0 e +00 | 6.71 e +06 | 1.0 e +00 | 4.0 e +04 |
2.4 e +08 | -7.3 e +08 | 0.00 e +00 | 1.0 e +00 | 3.8 e +04 |

0.00 | 1.0 e +20 |
1.00 | 1.1 e +02 |

Pnorm |

1 |
2 |

dObj |

dInf |

step |

k / t |

mu |

Iter
pObj
dObj
k/t
mu
step
Pnorm
E

the iteration number
the primal objective bound
the dual objective
κ/τ from the embedding
the current barrier parameter
stepsize α taken
the proximity to the central path
event monitor

Table 1: Iteration monitor of Phase A

When Phase A ﬁnds a dual-feasible solution, HDSDP collection the solution statistics to adjust the
parameters for dual-scaling in Phase B, prints related information and performs re-start.

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP Phase A ends with status : D S D P _ P R I M A L _ D U A L _ F E A S I B L E
| Elapsed Time : 0.535 seconds
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP Phase A certificates primal - dual feasibility
| Primal relaxation penalty is set to

2.449 e +06

11

| Perturbing dual iterations by
| DSDP Phase B starts . Restarting dual - scaling
| Heuristic start : mu :
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

dObj : -7.346 e +08

2.177 e +04 pObj :

2.450 e +08

0.000 e +00

The log from Phase B is similar to Phase A but dInf is now replaced by pInf to characterize primal
infeasibility. Also, k/t is dropped from log since the embedding is not applied.

pInf |

dObj |

pObj |

1 |
2 |
3 |

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Iter |
E |
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
|
|
|
P |
|
...
P |
|
|
F |
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

2.4500 e +08 | -7.3462 e +08 |
2.4500 e +08 | -3.6742 e +07 |
1.3061 e +08 | -1.8487 e +06 |

3.001 e +03 | 2.18 e +04 |
3.001 e +03 | 2.18 e +04 |
8.915 e -03 | 3.72 e +03 |

14 | -1.1997 e +04 | -1.2000 e +04 |
15 | -1.1999 e +04 | -1.2000 e +04 |

9.510 e -11 | 4.66 e -06 |
1.902 e -12 | 9.32 e -07 |

1.1 e +02 |
5.6 e +02 |
1.3 e +02 |

1.00 |
0.09 |
0.41 |

9.4 e +00 |
5.1 e +01 |

0.00 |
0.02 |

Pnorm |

step |

mu |

When Phase B converges, the solver extracts the solution status, recovers primal feasible solution
(if available) and brieﬂy prints solution and time statistics.

dObj : -1.20000 e +04

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP Phase B ends with status : D S D P _ I NT E R N A L _ E R R OR
| Elapsed Time : 5.960 seconds
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP Ends .
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| Primal solution is extracted .
| Final pObj : -1.19993 e +04
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
| DSDP Time Summary :
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
|
|
|
|
|
|
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

Presolve |
Phase A ( iter ) |
Phase B ( iter ) |
Get X |
Postsolve |
All |

0.371 |
0.535 | (2)
5.960 | (15)
0.809 |
0.000 |
7.675 | (17)

Time ( s ) |

Event |

Compared to other IPM solvers, HDSDP tends to display more information so that the users better
understand structures and features of the SDP instance. We believe that knowledge of the problem
structure would assist the users when customzing the solver for their applications.

6 Computational results

The eﬃciency of robustness of DSDP5.8 has been proven through years of computational experience
and HDSDP aims to achieve further improvement on a special class of SDPs where dual method has
advantage over the primal-dual solvers. In this section, we introduce several classes of SDPs suitable

12

for the dual method and we compare the performance of HDSDP, DSDP5.8 and COPT 5.0 (fastest
solver on Mittelmann’s benchmark implementing both primal-dual and dual method) on several
benchmark datasets to verify the performance improvement of HDSDP. For each problem, we only de-
scribe the mathematical model and refer the readers to [6,16] for the detailed background and formu-
lation. The time statistics in this section are obtained using Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz
with 64GB memory.

6.1 Maximum-cut

The SDP relaxation of the max-cut problem is represented by

min
X

(cid:104)C, X(cid:105)

subject to diag (X) = 1

X (cid:23) 0.

Let ei be the i-th column of the identity matrix and the constraint diag (X) = e is decomposed
into (cid:10)X, eie(cid:62)
is rank-one and has only one non-zero entry, M2
and M5 can greatly reduce the computation of the Schur matrix.

(cid:11) = 1, i = 1, . . . , n. Note that eie(cid:62)

i

i

Instance

mcp100
mcp124-1
mcp124-2
mcp124-3
mcp124-4
mcp250-1
mcp250-2
mcp250-3
mcp250-4
mcp500-1
mcp500-2
mcp500-3
mcp500-4
maxG11
maxG32

HDSDP
0.03
0.05
0.05
0.05
0.06
0.15
0.11
0.17
0.19
0.60
0.69
0.82
1.11
1.27
5.13

DSDP5.8
0.03
0.03
0.02
0.04
0.05
0.10
0.15
0.21
0.29
0.32
0.74
1.12
1.84
0.82
8.57

COPT v5.0
0.12
0.15
0.17
0.16
0.15
0.66
0.66
0.62
0.69
0.48
0.72
1.11
2.35
1.07
10.77

Instance

maxG51
maxG55
maxG60
G40 mb
G40 mc
G48 mb
G48mc
G55mc
G59mc
G60 mb
G60mc
torusg3-8
torusg3-15
toruspm3-8-50
toruspm3-15-50

HDSDP
1.46
146.92
323.62
12.76
8.09
16.70
4.42
118.11
181.20
261.50
257.10
0.85
23.77
0.76
19.27

DSDP5.8
4.65
606.66
1485.00
17.08
38.54
29.71
18.10
344.80
774.70
650.00
600.08
1.42
178.8
0.93
91.67

COPT v5.0
5.97
520.05
1269.83
25.76
49.07
50.49
33.19
505.18
727.89
964.20
962.79
1.04
137.60
0.76
117.92

Table 2: Max-cut problems

Computational experience suggests that on large-scale max-cut instances, HDSDP is 2 ∼ 3 times
faster than DSDP5.8.

13

6.2 Graph Partitioning

The SDP relaxation of the graph partitioning problem is given by

min
X
subject to

(cid:104)C, X(cid:105)

diag (X) = 1
(cid:10)11(cid:62), X(cid:11) = β
kX − 11(cid:62) (cid:23) 0

X ≥ 0,

where 1 denotes the all-one vector and k, β are the problem parameters. Although the dual S no
longer enjoys sparsity, the low-rank structure is still available to accelerate convergence.

Instance

gpp100
gpp124-1
gpp124-2
gpp124-3
gpp124-4
gpp250-1
gpp250-2
gpp250-3

HDSDP
0.03
0.04
0.05
0.05
0.06
0.15
0.11
0.17

DSDP5.8
0.04
0.09
0.05
0.05
0.05
0.16
0.15
0.14

COPT v5.0
0.19
0.28
0.28
0.34
0.36
1.58
1.33
1.46

Instance

gpp250-4
gpp500-1
gpp500-2
gpp500-3
gpp500-4
bm1
biomedP
industry2

HDSDP
0.19
0.60
0.69
0.82
1.11
2.28
221.2
Failed

DSDP5.8
0.16
0.63
0.60
0.55
0.58
2.36
Failed
Failed

COPT v5.0
1.21
0.64
0.56
0.56
0.51
1.74
Failed
Failed

Table 3: Graph partitioning problems

On graph partitioning, we see that HDSDP has comparable performance to DSDP but is more robust
on some problems.

6.3 Optimal Diagonal Pre-conditioning

The optimal diagonal pre-conditioning problem originates from [21], where given a matrix B (cid:31) 0,
ﬁnding a diagonal matrix D to minimize the condition number κ (cid:0)D−1/2BD−1/2(cid:1) can be modeled
as an SDP. The formulation for optimal diagonal pre-conditioning is given by

max
τ,D
subject to

τ

D (cid:22) B

τ B − D (cid:22) 0.

Expressing D = (cid:80)
i di, the problem is exactly in the SDP dual form. If B is also sparse, the
problem can be eﬃciently solved using the dual method. When the matrix B is large and sparse,
we see that HDSDP dominates the performance of DSDP due to the Schur complement tricks.

i eie(cid:62)

14

Instance

diag-bench-500-0.1
diag-bench-1000-0.01
diag-bench-2000-0.05
diag-bench-west0989
diag-bench-DK01R
diag-bench-micromass 10NN

HDSDP
6.70
44.50
34.30
6.72
13.18
9.35

DSDP5.8
7.50
55.20
340.70
113.23
Failed
60.127

COPT v5.0
6.80
53.90
307.10
108.20
Failed
51.71

Table 4: Optimal diagonal pre-conditioning problems

Remark 2 For optimal pre-conditioning, we start HDSDP from a non-default trivial dual feasible
solution τ = −106, D = 0.

6.4 Other Problems

So far HDSDP is tested and tuned over a large set of benchmarks including SDPLIB [6] and Hans
Mittelmann’s sparse SDP benchmark [16]. By the time this manuscript is written, HDSDP is the
fourth fastest among all the benchmarked solvers.

Scaled shifted geometric means of runtimes ("1" is fastest solver )

1

2.30

4.61

2.28

12.3

3.72

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
count of " a "
solved of 75
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
problem
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =

SeDuMi

SDPT3

HDSDP

5
70

2
64

8
74

11
70

13
69

17
61

COPT

CSDP

SDPA

We report the solution accuracy and local CPU time of HDSDP on Mittlelmann’s benchmark in
the appendix. Readers can refer to [16] for a detailed explanation of the error measures and the
criterion of a successful solve. Among all of 75 problems, 70 are successfully solved; 3 problems fail
due to insuﬃcient memory, 1 fails due to failure to ﬁnd a feasible dual solution and 1 fails due to
error in primal solution recovery. The benchmark test proves the eﬃciency and robustness of HDSDP
as a general purpose SDP solver. Below we present some benchmark datasets with nice structure
for HDSDP. They enjoy at least one of sparsity and low-rank structure.

6.5 Summary

HDSDP implements data structures and computational techniques specially optimized for sparse
SDPs with low-rank constraint structure. Vast computational experiments suggests that on these
problems HDSDP outperforms both primal-dual and conventional dual solvers in terms of eﬃciency
and robustness on these instances.

15

Instance

checker 1.5
foot
hand
ice 2.0
p auss2 3.0
rendl1 2000 1e-6
trto3
trto4
trto5
sensor 500b
sensor 1000b

Background
unknown
unknown
unknown
unknown
unknown
unknown
topology design
topology design
topology design
sensor network localization
sensor network localization

Feature
sparse, low-rank
sparse, low-rank
low-rank
low-rank
sparse, low-rank
low-rank
sparse, low-rank
sparse, low-rank
sparse, low-rank
sparse, low-rank
sparse, low-rank

HDSDP
55.38
25.76
5.60
706.16
739.40
15.74
1.67
12.28
111.59
86.73
232.05

DSDP5.8
146.80
23.83
5.57
1106.00
1066.00
22.31
1.73
12.40
151.00
37.87
143.32

COPT v5.0
137.15
262.54
50.70
1542.96
1111.72
231.80
3.04
30.09
233.16
7.01
32.27

Table 5: Feature of several benchmark problems

7 When (not) to use DSDP/HDSDP

While HDSDP is designed for general SDPs, it targets the problems more tractable in the dual form
than by the primal-dual methods. This is the principle for the techniques implemented by HDSDP.
Here are some rules in mind when deciding whether to use the dual method (or HDSDP).

1. Does the problem enjoys nice dual structure?

Many combinatorial problems have formulations friendly to the dual methods. Some typical
features include (aggregated) sparsity and low-rank structure. Dual methods eﬀectively ex-
ploit these features by iterating in the dual space and using eﬃcient computational tricks. If
the problem is dense and most constraints are full-rank, dual method has no advantage over
the primal-dual solvers due to 1) comparable iteration cost to primal-dual methods. 2) more
iterations for convergence.

2. Do we need the primal optimal solution or just the optimal value?

For some applications dual method fails to recover a correct primal solution due to numerical
diﬃculties. If the optimal value is suﬃcient, there is no problem. But if an accurate primal
optimal solution is always necessary, it is better to be more careful and to test the recovery
procedure in case of failure at the last step.

3. Do we need to certiﬁcate infeasibility strictly?

One weakness of the dual method is the diﬃculty in infeasibility certiﬁcate. Although on the
dual side this issue is addressed by HDSDP using the embedding, dual methods still suﬀer from
failure to identify primal infeasibility.

16

4. Is dual-feasibility hard to attain?

The ﬁrst phase of HDSDP adopts the infeasible Newton’s method and focuses on eliminating
the dual infeasibility. This principle works well if the dual constraints are relatively easy to
satisfy, but if this condition fails to hold (e.g., empty dual interior), experiments suggest the
embedding spend a long time deciding feasibility. In this case it is suggested using DSDP5.8
or supply an initial dual solution.

To conclude, HDSDP solves SDPs but it solves certain SDPs eﬃciently.

8 Conclusions

In this manuscript we propose an extension of the dual-scaling algorithm based on the HSD embed-
ding. The resultant solver, HDSDP, is presented to demonstrate how dual method can be eﬀectively
integrated with the embedding trick. HDSDP is developed in parallel to DSDP5.8 and is entailed
with several newly added computational techniques. The solver exhibits promising performance on
several benchmark datasets and is under active development. Users are welcome to try the solver
and provide valuable suggestions.

9 Acknowledgement

We thank Dr. Qi Huangfu from COPT development team for his constructive ideas in the solver
design and implementation. We also appreciate Hans Mittelmann’s eﬀorts in benchmarking the
solver. Finally, we sincerely respect the developers of DSDP for their precious suggestions [2] and
It is the eﬃcient and elegant
their invaluable eﬀorts getting DSDP through all along the way.
implementation from DSDP5.8 that guides HDSDP to where it is.

References

[1] Mosek ApS. Mosek optimization toolbox for matlab. User’s Guide and Reference Manual,

Version, 4, 2019.

[2] Steven J Benson and Yinyu Ye. Algorithm 875: Dsdp5–software for semideﬁnite programming.

ACM Transactions on Mathematical Software (TOMS), 34(3):1–20, 2008.

[3] Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large-scale sparse semideﬁnite programs

for combinatorial optimization. SIAM Journal on Optimization, 10(2):443–461, 2000.

[4] Steven J Benson, Yinyu Yeb, and Xiong Zhang. Mixed linear and semideﬁnite programming for
combinatorial and quadratic optimization. Optimization Methods and Software, 11(1-4):515–
544, 1999.

17

[5] Pratik Biswas and Yinyu Ye. Semideﬁnite programming for ad hoc wireless sensor network
localization. In Proceedings of the 3rd international symposium on Information processing in
sensor networks, pages 46–54, 2004.

[6] Brian Borchers. Sdplib 1.2, a library of semideﬁnite programming test problems. Optimization

Methods and Software, 11(1-4):683–690, 1999.

[7] Brian Borchers. Csdp user’s guide, 2006.

[8] Katsuki Fujisawa, Masakazu Kojima, and Kazuhide Nakata. Exploiting sparsity in primal-dual
interior-point methods for semideﬁnite programming. Mathematical Programming, 79(1):235–
253, 1997.

[9] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM),
42(6):1115–1145, 1995.

[10] Masahito Hayashi. Quantum information theory. Springer, 2016.

[11] Michal Kocvara, Michael Stingl, and PENOPT GbR. Pensdp users guide (version 2.2).

PENOPT GbR, 1435:1436, 2006.

[12] Tomasz Kwasniewicz and Fran¸cois Glineur.

Implementation of a semideﬁnite optimization

solver in the julia programming language. 2021.

[13] Monique Laurent. Sums of squares, moment matrices and optimization over polynomials. In

Emerging applications of algebraic geometry, pages 157–270. Springer, 2009.

[14] Monique Laurent and Franz Rendl. Semideﬁnite programming and integer programming.

Handbooks in Operations Research and Management Science, 12:393–514, 2005.

[15] Anirudha Majumdar, Georgina Hall, and Amir Ali Ahmadi. Recent scalability improvements
for semideﬁnite programming with applications in machine learning, control, and robotics.
Annual Review of Control, Robotics, and Autonomous Systems, 3:331–360, 2020.

[16] Hans D Mittelmann. An independent benchmarking of sdp and socp solvers. Mathematical

Programming, 95(2):407–430, 2003.

[17] Cardinal Operations. Cardinal optimizer, 2022.

[18] Imre Polik, Tamas Terlaky, and Yuriy Zinchenko. Sedumi: a package for conic optimization.
In IMA workshop on Optimization and Control, Univ. Minnesota, Minneapolis. Citeseer, 2007.

[19] Florian A Potra and Rongqin Sheng. On homogeneous interrior-point algorithms for semidef-

inite programming. Optimization Methods and Software, 9(1-3):161–184, 1998.

18

[20] Florian A Potra and Rongqin Sheng. A superlinearly convergent primal-dual infeasible-interior-
point algorithm for semideﬁnite programming. SIAM Journal on Optimization, 8(4):1007–1028,
1998.

[21] Zhaonan Qu, Yinyu Ye, and Zhengyuan Zhou. Diagonal preconditioning: Theory and algo-

rithms. arXiv preprint arXiv:2003.07545, 2020.

[22] Anthony Man-Cho So and Yinyu Ye. Theory of semideﬁnite programming for sensor network

localization. Mathematical Programming, 109(2):367–384, 2007.

[23] Kim-Chuan Toh. A note on the calculation of step-lengths in interior-point methods for
semideﬁnite programming. Computational Optimization and Applications, 21(3):301–310,
2002.

[24] Kim-Chuan Toh, Michael J Todd, and Reha H T¨ut¨unc¨u. On the implementation and usage of
sdpt3–a matlab software package for semideﬁnite-quadratic-linear programming, version 4.0. In
Handbook on semideﬁnite, conic and polynomial optimization, pages 715–754. Springer, 2012.

[25] Lieven Vandenberghe and Stephen Boyd. Semideﬁnite programming. SIAM review, 38(1):49–

95, 1996.

[26] Henry Wolkowicz. Semideﬁnite and cone programming bibliography/comments. http://orion.

uwaterloo. ca/˜ hwolkowi/henry/book/fronthandbk. d/sdpbibliog. pdf, 2005.

[27] Xiaojie Xu, Pi-Fang Hung, and Yinyu Ye. A simpliﬁed homogeneous and self-dual linear
programming algorithm and its implementation. Annals of Operations Research, 62(1):151–
171, 1996.

[28] Makoto Yamashita, Katsuki Fujisawa, Mituhiro Fukuda, Kazuhiro Kobayashi, Kazuhide
Nakata, and Maho Nakata. Latest developments in the sdpa family for solving large-scale
sdps. In Handbook on semideﬁnite, conic and polynomial optimization, pages 687–713. Springer,
2012.

[29] Liuqin Yang, Defeng Sun, and Kim-Chuan Toh. Sdpnal++: a majorized semismooth newton-
cg augmented lagrangian method for semideﬁnite programming with nonnegative constraints.
Mathematical Programming Computation, 7(3):331–366, 2015.

19

A Mittlelmann’s Benchmark Test

The following test of the benchmark dataset is run on an intel i11700K with 128GB memory.

Instance
1dc.1024
1et.1024
1tc.1024
1zc.1024

Error 1
3.440000e-08
7.660000e-09
6.790000e-09
1.230000e-08
AlH 1.510000e-09
2.850000e-11
BH2
1.130000e-08
Bex2 1 5
1.410000e-12
Bst jcbpaf2
1.370000e-11
CH2
1.170000e-06
G40 mb
1.310000e-04
G48 mb
7.080000e-11
G48mc
1.390000e-10
G55mc
1.220000e-10
G59mc
1.030000e-09
G60 mb
G60mc
1.030000e-09
H3O 1.090000e-09
1.820000e-09
NH2
1.360000e-09
NH3
4.670000e-10
NH4
3.620000e-09
biggs
2.320000e-09
broyden25
1.390000e-11
buck4
4.920000e-09
buck5
5.250000e-13
cancer 100
2.830000e-09
checker 1.5
1.210000e-17
chs 5000
3.980000e-08
cnhil10
8.550000e-10
cphil12
5.000000e-01
diamond patch
2.370000e-10
e moment quad
2.180000e-09
e moment stable
1.130000e-04
foot
2.740000e-09
hamming 8 3 4
1.00000e+00
hamming 9 5 6

Error 2
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
1.200000e-17
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
-0.00000e+00
0.00000e+00
7.700000e-19
0.00000e+00
0.00000e+00
1.00000e+00

Error 3
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
3.800000e-09
3.300000e-12
5.000000e-11
2.300000e-09
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
3.700000e-09
2.300000e-09
2.700000e-09
3.400000e-09
1.900000e-12
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
5.400000e-09
3.500000e-07
0.00000e+00
0.00000e+00
1.00000e+00

Error 4
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
1.100000e-12
0.00000e+00
0.00000e+00
0.00000e+00
2.000000e-12
2.000000e-12
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
9.100000e-13
0.00000e+00
0.00000e+00
0.00000e+00
3.400000e-15
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
8.500000e-10
6.700000e-11
0.00000e+00
1.00000e+00

Error 5
5.400000e-06
3.000000e-06
2.600000e-06
1.400000e-06
1.100000e-04
1.300000e-07
5.000000e-07
2.800000e-07
3.300000e-07
1.800000e-05
1.900000e-03
2.400000e-06
7.300000e-07
4.900000e-07
2.400000e-03
2.400000e-03
3.100000e-07
3.800000e-07
3.200000e-07
2.300000e-07
3.500000e-08
7.000000e-09
4.400000e-07
5.400000e-04
1.700000e-08
1.300000e-06
1.500000e-07
8.500000e-09
9.300000e-09
-9.400000e-01
-1.900000e-06
-1.100000e-05
-2.300000e-03
1.600000e-08
1.00000e+00

Error 6
5.600000e-06
2.900000e-06
2.600000e-06
1.300000e-06
1.100000e-04
3.900000e-07
1.200000e-07
1.700000e-07
4.000000e-07
6.000000e-07
6.800000e-07
2.400000e-06
7.300000e-07
4.900000e-07
2.400000e-03
2.400000e-03
3.600000e-07
4.300000e-07
3.500000e-07
3.900000e-07
4.200000e-08
7.300000e-09
2.400000e-07
2.800000e-04
3.400000e-08
1.200000e-06
1.500000e-07
2.000000e-08
9.300000e-09
0.00000e+00
1.100000e-08
4.900000e-08
1.900000e-04
1.600000e-08
1.00000e+00

Time
2500.186
175.211
142.422
469.066
10162.681
315.241
273.331
419.132
315.060
7.025
8.489
2.681
179.720
264.597
213.472
212.088
1344.764
290.156
1367.722
5202.509
14.316
1774.592
21.185
194.738
396.231
41.693
36.825
63.071
259.832
Failed
334.162
256.368
12.878
38.635
Failed

Table 6: Mittelmann’s Benchmark Test Part 1

20

Instance
hand
ice 2.0
inc 1200
mater-5
mater-6
neosfbr25
neosfbr30e8
neu1
neu1g
neu2
neu2c
neu2g
neu3
neu3g
p auss2 3.0
prob 2 4 0
prob 2 4 1
rabmo
reimer5
rendl
ros 2000
rose15
sensor 1000b
shmup4
shmup5
spar060-020-1 LS
swissroll
taha1a
taha1b
taha1c
theta12
theta102
theta123
tiger texture
torusg3-15
trto4
trto5
vibra4
vibra5
yalsdp

Error 1
4.500000e-08
5.330000e-09
9.920000e-06
4.830000e-11
1.670000e-10
3.640000e-09
2.650000e-08
3.650000e-06
7.850000e-10
2.480000e-08
1.00000e+00
4.280000e-10
8.110000e-09
9.090000e-09
6.030000e-08
2.380000e-15
9.810000e-15
9.160000e-10
1.740000e-09
1.530000e-10
5.510000e-16
8.550000e-08
6.940000e-10
1.770000e-08
1.070000e-05
7.770000e-08
9.480000e-06
1.800000e-10
8.660000e-09
9.900000e-09
4.920000e-08
7.370000e-11
1.00000e+00
9.230000e-04
1.860000e-11
8.580000e-08
1.430000e-06
6.030000e-09
3.690000e-06
5.290000e-09

Error 2
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
9.200000e-18
0.00000e+00
2.700000e-17
1.00000e+00
0.00000e+00
0.00000e+00
1.600000e-17
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
7.900000e-18
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
1.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00

Error 3
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
4.800000e-07
0.00000e+00
3.900000e-07
1.00000e+00
0.00000e+00
2.600000e-07
0.00000e+00
0.00000e+00
4.400000e-10
0.00000e+00
1.600000e-07
1.100000e-07
0.00000e+00
0.00000e+00
1.300000e-07
0.00000e+00
1.400000e-13
2.400000e-10
0.00000e+00
0.00000e+00
2.400000e-07
2.100000e-10
4.100000e-07
0.00000e+00
0.00000e+00
1.00000e+00
1.200000e-12
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
1.600000e-06
0.00000e+00

Error 4
5.200000e-14
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
5.700000e-10
0.00000e+00
8.300000e-10
1.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
3.000000e-10
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
1.00000e+00
8.700000e-11
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00
0.00000e+00

Error 5
1.100000e-06
1.400000e-06
-2.300000e-04
8.400000e-07
1.100000e-06
2.100000e-07
4.000000e-07
-1.300000e-05
4.000000e-09
-1.100000e-07
1.00000e+00
9.900000e-09
-8.000000e-07
6.500000e-09
-2.200000e-06
-7.100000e-07
4.100000e-07
-4.300000e-05
-3.600000e-05
4.100000e-07
1.400000e-07
-7.200000e-05
6.400000e-07
9.700000e-07
6.300000e-05
9.100000e-07
-7.300000e-03
-1.900000e-07
7.200000e-08
1.800000e-05
2.500000e-07
6.600000e-07
1.00000e+00
1.600000e-03
3.300000e-07
-1.400000e-06
-4.700000e-05
2.000000e-03
5.600000e-05
2.400000e-06

Error 6
5.200000e-07
1.200000e-06
2.500000e-07
8.400000e-07
1.100000e-06
2.100000e-07
4.200000e-07
5.000000e-06
6.400000e-08
4.600000e-08
1.00000e+00
4.100000e-08
3.000000e-08
2.500000e-08
3.600000e-07
6.200000e-07
4.100000e-07
1.500000e-08
4.600000e-09
4.100000e-07
1.400000e-07
1.500000e-08
1.400000e-07
5.000000e-07
5.100000e-07
9.900000e-09
2.100000e-07
2.400000e-07
9.100000e-08
9.900000e-04
2.900000e-07
6.600000e-07
1.00000e+00
2.000000e-03
3.300000e-07
1.800000e-07
2.400000e-07
1.300000e-03
1.100000e-04
2.400000e-06

Time
2.431
372.376
128.205
23.988
61.504
1084.074
5924.588
109.041
89.921
108.879
Failed
90.972
1731.641
1807.446
451.407
221.371
98.066
182.872
1862.190
7.351
3.760
104.258
203.969
63.314
770.997
779.882
37.456
196.912
670.662
2199.454
1095.150
5378.786
Failed
53.752
24.455
7.633
82.491
33.061
326.107
125.136

Table 7: Mittelmann’s Benchmark Test Part 2

21

