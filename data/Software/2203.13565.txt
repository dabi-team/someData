2
2
0
2

r
a

M
5
2

]
E
S
.
s
c
[

1
v
5
6
5
3
1
.
3
0
2
2
:
v
i
X
r
a

Self-Assessing Creative Problem Solving for Aspiring Software
Developers: A Pilot Study

Wouter Groeneveld
KU Leuven
Leuven, Belgium
wouter.groeneveld@kuleuven.be

Joost Vennekens
KU Leuven
Leuven, Belgium
joost.vennekens@kuleuven.be

Lynn Van den Broeck
KU Leuven
Leuven, Belgium
lynn.vandenbroeck@kuleuven.be

Kris Aerts
KU Leuven
Leuven, Belgium
kris.aerts@kuleuven.be

ABSTRACT
We developed a self-assessment tool for computing students in
higher education to measure their Creative Problem Solving skills.
Our survey encompasses 7 dimensions of creativity, based on exist-
ing validated scales and conducted focus groups. These are: techni-
cal knowledge, communication, constraints, critical thinking, curiosity,
creative state of mind, and creative techniques. Principal axis factor
analysis groups the dimensions into three overarching constructs:
ability, mindset, and interaction. The results of a pilot study (ùëõ = 269)
provide evidence for its psychometric qualities, making it a use-
ful instrument for educational researchers to investigate students‚Äô
creative skills.

CCS CONCEPTS
‚Ä¢ Social and professional topics ‚Üí Student assessment; Soft-
ware engineering education.

KEYWORDS
creativity; self-assessment; creative problem solving

ACM Reference Format:
Wouter Groeneveld, Lynn Van den Broeck, Joost Vennekens, and Kris Aerts.
2022. Self-Assessing Creative Problem Solving for Aspiring Software Devel-
opers: A Pilot Study. In Proceedings of ACM Conference (Conference‚Äô17). ACM,
New York, NY, USA, 7 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Industry experts perceive creativity as an important non-technical
skill for software engineers [17]. Nevertheless, current curricula
pay relatively little explicit attention to this skill [16]. This may
in part be due to the fact that creativity is a complex and broad
concept [11], which has not yet been widely researched in the field
of Software Engineering (SE) [3].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference‚Äô17, July 2017, Washington, DC, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

In this paper, we are interested in the concept of creative problem
solving (CPS) , i.e., the use of creativity to solve day-to-day pro-
gramming problems. We build on previous work, that examined
this concept by means of a focus group study [18]. This work, which
we will further refer to simply as the FGS, identified 39 themes that
are relevant to CPS, grouped in seven domains: technical knowledge,
communication, constraints, critical thinking, curiosity, creative state
of mind, and creative techniques. Due to page limits, we cannot give
more details about the FGS here, but they are available in [18]. Our
goal is to make the insights from the FGS actionable for comput-
ing education research. In particular, we derive from this study a
creativity self-assessment tool for SE students, called the Creative
Programming Problem Solving Test (CPPST). It allows students to
measure their skills in each of the seven creativity domains. This
may be useful for students themselves, e.g., as a means of tracking
their progress throughout a curriculum, but it is mainly intended as
an instrument for educational researchers who wish to investigate
the effect that certain changes to the curriculum or to a particular
course have on the students‚Äô creative skills.

The remainder of this paper is divided as follows. Section 2
outlines related work, while Section 3 and 4 describe the used
methodology to design and analyze the survey. Next, in Section
5 and 6, we present and discuss the results, followed by possible
limitations in Section 7. Finally, we conclude this work in Section 8.

2 BACKGROUND
In general, creativity is often measured through divergent thinking,
as is the case for the popular Torrance‚Äôs Test of Creative Think-
ing (TTCT) [32]. While the TTCT can be used to assess creative
potential [25], it does not attempt to measure the creative process
itself [29]. To address this, Miller [29] developed the Cognitive Pro-
cesses Associated with Creativity (CPAC) scale, focusing on the
processes of brainstorming, metaphorical and analogical thinking,
perspective-taking, imagery, incubation, and flow. The FGS also
identified these topics, but, in addition, the industry professionals
also stressed topics such as constraint-based thinking and com-
munication (see Section 3). In the SE context, the CPAC scale is
therefore only part of the story.

The same can be said for similar efforts of measuring creativity
in general, such as the Creativity Support Index (CSI) of Carroll
et al. [9]. It consists of these components: exploration, expressive-
ness, immersion, effort/results, enjoyment, collaboration. This list

 
 
 
 
 
 
Conference‚Äô17, July 2017, Washington, DC, USA

Pre-print of accepted ITiCSE 2022 paper.

also lacks several important topics from the FGS. However, the
expressiveness component was not mentioned in the FGS: while
self-expression is often thought to be a key part of creativity in gen-
eral, industry experts do not appear to consider it as an important
component of CPS in software engineering.

Scales have also been developed to measure creativity specifi-
cally in engineering students. Amato-Henderson et al. [2] based
their scale on three existing scales: the Resistance to Change scale,
the Curiosity and Exploration Inventory, and the Zampetakis &
Moustakis Scale that assesses entrepreneurial intentions. The FGS
did not mention entrepreneurial intentions, and while general en-
gineering and SE share many conceptual ideas, they differ radically
in execution. There is no ability to apply a bugfix patch to a badly
engineered bridge, for example. Their exploratory factor analysis
revealed four main factors: cognitive approaches (the usage of prob-
lem solving tactics, mapping to our CPS domain creative techniques),
cognitive challenges (the enjoyment of experiences that challenge
viewpoints; this somewhat maps to curiosity), cognitive prepared-
ness, and impulsivity in problem solving. The last two components
are personality-based, which our CPS model does not focus on.

There also exist multiple scales that focus on computational
thinking. The Computational Thinking Scale (CTS) of Korkmaz
et al. [28] contains the following five factors: creativity, coopera-
tivity, algoritmic thinking, critical thinking and problem solving.
Although the CTS scale confirms the importance of a few domains
that our CPS framework shares, such as the presence of creative
and communicative skills, it lacks important aspects from the FGS,
such as the influence of constraints and the usage of practical cre-
ative techniques. To measure their single ‚Äúcreativity‚Äù factor, they
selected suitable items from the ‚ÄúHow Creative Are You?‚Äù scale,
the ‚ÄúProblem Solving Scale‚Äù, the ‚ÄúCooperative Learning Attitude
Scale‚Äù, ‚ÄúThe Scale of California Critical Thinking Tendency‚Äù, and
the ‚ÄúLogical-Mathematical Thinking‚Äù scale.

In summary, although self-assessment metrics exist for creativity
in general, in an engineering context and in the context of compu-
tational thinking, none fully suits our needs. Indeed, existing scales
focus on other aspects of creativity than CPS and do not cover all
of the aspects identified by industry experts in the FGS.

3 SURVEY DESIGN
For each of the seven domains identified in the FGS, we composed
eight questions, based on existing surveys. An overview of the
full questionnaire is available in Appendix A. Our intention is to
reduce the item set after determining the internal consistency by
inspecting Cronbach‚Äôs ùõº values. Bad questions will be omitted in
future versions of the survey. The analysis is described in Section 4.
Since we are interested in CPS in the context of SE students, we
designed the scale to be as domain-specific as possible, as recom-
mended by Barbot et al. [6]. Therefore, we replace generic questions
such as ‚ÄúI like variety‚Äù by, e.g.: ‚ÄúI liked varied aspects of the pro-
gramming assignment‚Äù.

Assessment tools for creativity can be divided into those mea-
suring creative potential, performance, and achievement [5]. Our
CPPST measures achievements. It does not gauge creative potential,
nor does it measure performance, as Torrance‚Äôs TTCT or Amabile‚Äôs
Consensual Assessment Technique (CAT) [1] metrics do. The items

are self-assessing in nature and ask about the usage of various CPS
skills on a recent programming achievement.

All questions were sampled on a Likert-5 scale, which is the most
common in creativity research [24]. Some surveys on which we
based our questions use a Likert-6 scale with ‚Äúnot applicable‚Äù as the
sixth option, but we left this out as all items are deemed relevant.
The answer categories are: (1) completely disagree, (2) disagree, (3)
neutral, (4) agree, (5) completely agree. Two questions per domain
are reverse-coded to check the consistency of the answers. For each
survey, questions are served in random order. Students were not
able to see the results after finishing the questionnaire.

We now describe the origins of the questions for each of the

seven FGS domains.

3.1 Technical Knowledge (KNW)
To be able to creatively solve programming problems, one needs to
have technical knowledge and to invest in keeping this knowledge
up-to-date, by continuously learning and improving oneself. In
the FGS, the following themes emerged w.r.t. technical knowledge:
continuous learning, domain models, seeking out different inputs.

To assess these themes, we adapt the following 8 questions from
the 14-item lifelong learning measurement scale of Kirby et al. [26]:
1. I have gained little knowledge during the project. (inverted)
2. I learned and applied new practical programming techniques.
3. I have gained insight into the problem domain.
4. The technical aspect of programming appealed to me.
5. I thought about my learning process and how to improve it.
6. I felt uncomfortable with this project because many aspects

were unknown. (inverted)

7. I tried to relate the new knowledge to something I know.
8. Thanks to the project I also gained knowledge of other things

outside of programming.

3.2 Communication (COM)
The FGS stresses that modern SE is a team-based activity. Engaging
in discussions helps tremendously in figuring out how to solve diffi-
cult programming problems. According to the FGS, communication
is therefore an important aspect of creative work in SE.

To assess this, we combined results from the FGS with ideas from
the Collaboration Self Assessment Tool [30]. The latter survey is
based on the following concepts: (1) Contributions, (2) Quality of
Work, (3) Time management, (4) Team Support, (5) Preparedness, (6)
Problem solving, (7) Team Dynamics, (8) Interactions with Others, (9)
Role Flexibility, and (10) Reflection. Since some of these concepts
are already present in the other domains of our CPPST scale, we
focus here on communication, collaboration, and team efforts.

Related themes from the FGS are: rubber ducking (to explain a
problem to oneself or others to gain new insights), drawing on a
whiteboard, getting fast feedback, working with peers, responsibility.
The following questions assess communication in the CPPST:

1. I hardly asked for feedback from my fellow students. (inverted)
2. I visualized the problem on a whiteboard or on paper.
3. I regularly asked feedback from my teachers.
4. I supported my teammates by helping them with their tasks.
5. My own tasks were not completed on time so teammates ran

into problems with the deadline. (inverted)

Self-Assessing Creative Problem Solving for Aspiring Software Developers: A Pilot Study

Conference‚Äô17, July 2017, Washington, DC, USA

6. I supported the ideas and efforts of my teammates.
7. I was so proud of our result that I showed it to everyone.
8. I thoroughly thought suggestions by others through.

3.3 Constraints (CTR)
Constraints outline the context of the problem. They keep you from
working on it forever, and they keep the solution relevant to the
issue at hand. Most software projects are ‚Äúbrownfield‚Äù projects, in
which an existing code base comes with many external constraints.
Thus, both designing and thinking in context of constraints are
very important in software engineering.

Biskjaer et al. [8] recently demonstrated that there is a sweet
spot for accelerating (the potential for) creativity with constraints.
Too few constraints cause one to lose interest, while too many
constraints may cause stress and hamper creative freedom. Ac-
cording to Biskjaer [7], self-imposed creativity constraints can also
play an important role. For our survey, however, we view such
self-imposed constraints as a creative technique, and focus here on
external constraints.

Themes that emerged from the FGS are: client-oriented designing,
efficiency, relevance, context, performance, time-constraints. The fol-
lowing questions assess the ability to effectively handle constraints:
1. I regularly thought about the correctness of my solution.
2. Due to time pressure, I performed less well. (inverted)
3. I tried to make my code as elegant as possible.
4. I tried to identify the constraints of the assignment.
5. I have had the program tested by friends and / or family.
6. There was too much creative freedom for me, so I could not

make a good decision. (inverted)

7. Coding on short notice accelerated my learning process.
8. I regularly tested myself and paid attention to its ease of use.

3.4 Critical Thinking (CRI)
Critical thinking is the ability to question things, to come up with al-
ternatives, and to judge the trustworthiness of information sources.
A classic counterexample in the SE world is mindlessly copy-pasting
code snippets from online sources. Sosu [31] developed and val-
idated a Critical Thinking Disposition Scale. Their scale is split
into two main components after an exploratory factor analysis: (A)
Critical Openness and (B) Reflective Scepticism. We adapted an
equal number of items from both components to our domain.

Themes that emerged from the FGS are: coming up with alterna-
tives, asking ‚Äòwhy?‚Äô. The following questions assess the ability to
think critically in the CPPST:

1. In discussions about problems, I often suggested alternatives.
2. I regularly carefully weighed up the various options we had.
3. I dared to completely rewrite my code when it didn‚Äôt go well.
4. I used multiple sources to find out information myself.
5. I didn‚Äôt think it was important to ask teammates how they

implemented something. (inverted)

6. I always check the credibility of the source when I look

something up.

7. It was more important that it worked than that I 100% un-

derstood why. (inverted)

8. Looking at other projects made me think about my own.

3.5 Curiosity (CUR)
As the FGS states, a ‚Äúhungry‚Äù mind, that is motivated to seek out
new things, is very important in SE. Themes that emerged in the
FGS are: getting out of the comfortzone, motivation, the creative urge,
complexity is fun, admiration. We base our questions about this
domain on the Curiosity Index developed by Fulcher [15]:

1. During the project, I got very much out of my comfort zone.
2. Many parts of the project piqued my curiosity.
3. I enjoyed getting involved in many aspects of the project.
4. I enjoyed really immersing myself in some aspects.
5. I was stimulated by the complexity of the project.
6. I felt the urge to implement extras.
7. I have not had any fun while developing the project. (inverted)
8. I had to commit myself to finish the project. (inverted)

3.6 Creative State of Mind (MND)
Optimal creative work requires a certain state of mind, which Csik-
szentmihalyi [10] calls flow. The FGS mentions that people can tell
whether colleagues are being creative by looking at body language:
Are they happy, and making a lot of jokes? Are they ‚Äòin the zone‚Äô?
Jackson and Marsh [21] developed the Flow State Scale to mea-
sure Optimal Experience, based on the work of Csikszentmihalyi.
It contains 36 items divided into nine factors: (1) Challenge-Skill
Balance, (2) Action-Awareness Merging, (3) Clear Goals, (4) Unam-
biguous Feedback, (5) Concentration on Task, (6) Sense of Control,
(7) Loss of Self-Consciousness, (8) Transformation of Time, and (9)
Autotelic Experience.

Themes that emerged from the FGS are: focus, flow/being in the
zone, flexibility, environment (space/social), productivity tooling. The
common divisor of these themes and the factors of the Flow State
Scale leads to the following questions:

1. I remained focused for a long time on one part of the project.
2. I used productivity tools to focus more on the essentials

(e.g. shortcuts, command line tools, . . . ).
3. I found the experience to be very rewarding.
4. Time seemed to fly while working.
5. I did not know enough to meet the high demands of the

project. (inverted)

6. Programming went almost automatically.
7. I did not know what exactly I wanted to achieve. (inverted)
8. I was not concerned with what others thought of my code.

3.7 Creative Techniques (TCH)
Creative techniques are concrete actions to accelerate CPS. Hunt
[20] describes various creative techniques specifically for software
developers. The FGS also mentioned other simple tricks, such as
taking a coffee break or deliberately going to the toilet.

We were unable to find an existing scale that measures the use
of such techniques. Therefore, we based the questions solely on the
following themes from the FGS: viewing problems from a different
angle (birds-eye view), asking energizer questions, brainstorming,
mapping problems from different domains, combining ideas, and
seeking out input. The following questions assess these:

1. I always used the same method to solve a problem. (inverted)
2. I used knowledge from another domain to solve something.
3. I combined different ideas to tackle a problem.

Conference‚Äô17, July 2017, Washington, DC, USA

Pre-print of accepted ITiCSE 2022 paper.

4. I deliberately took occasional breaks to let things sink in.
5. I brainstormed with others to come up with new ideas.
6. I took a step back now and then to see things as a whole.
7. In case of problems I let myself be inspired by other projects.
8. I was regularly stuck (inverted)

3.8 Face validity
We believe that the face validity of our CPPST is by design already
quite high, since it is based on existing validated scales and the
FGS of [18], in which 33 experts collaborated on the exploration
of creativity in SE. Furthermore, the selected items all resemble
real-life CPS behavior.

Throughout the initial development of the scale, we received
feedback from the department of psychology at a neighboring uni-
versity. One of their recommendations was to use eight items per
domain, instead of five as we had originally planned. Once all of
their feedback had been addressed, we administered the survey to
a small set of volunteering second-year SE students (ùëõ = 9), which
led us to reformulate some sentences that were hard to understand.
The survey was also checked by a number of the FGS participants (ùëõ
= 5), which resulted in rewriting six items and changing the Likert
label for value 3 from ‚Äúmore or less‚Äù to ‚Äúneutral‚Äù. The approval of
the items by these members of the FGS provides further evidence
for the face validity of the survey.

4 SURVEY ANALYSIS
We wanted the data to represent a broad range of SE students.
Therefore, we surveyed both first-year (ùëõ = 141) and last-year (ùëõ
= 128) engineering students. The test was administered as close
as possible to a project deadline as part of a programming course.
The survey was not obligatory, but students were strongly encour-
aged to participate. Because student demographics in the surveyed
group were not very heterogeneous and Miller [29] earlier found
no significant difference in self-assessment creativity scores based
on participant gender, ethnicity or year in school, we decided not
to consider demographics as a separate discriminant.

We follow reliability and validity analysis procedures from Miller
[29] and Korkmaz et al. [28]. However, we will be conservative when
deciding whether to drop an item from the scale when factor analy-
sis shows it to be loaded on multiple factors. Since the CPS domains
are heavily intertwined, as also confirmed by the FGS, we believe
that items loaded on multiple factors cannot be avoided entirely,
without losing key information. We will therefore be hesitant to do
so, especially if the Kaiser-Meyer-Olkin statistic that indicates the
factorability of the item set is high enough [22].

For the reliability analysis, we calculated several Cronbach‚Äôs ùõº
values: an initial ùõº for the entire survey, a separate ùõº after a number
of items were dropped, and ùõº values for each discovered factor.
The items that were dropped were those with doubtful or worse
item-total correlations (ùëÖùëñùë° ), in accordance with the rules of thumb
of Ebel and Frisbie [13]: poor (ùëÖùëñùë° < .20), doubtful (.21 < ùëÖùëñùë° < .29),
good (.30 < ùëÖùëñùë° < .39), and very good (ùëÖùëñùë° > .40).

Construct validity is usually investigated with the help of various
factor analysis methods [13]. Since our survey is based on the seven
domains of the FGS, we first perform a confirmatory factor analysis
(CFA) for these domains. This will answer the question of whether

the original seven domains can be considered independent factors
for our survey. If the answer is no, we will perform an exploratory
factor analysis (EFA) to discover more appropriate factors.

5 RESULTS
In total, 269 out of 336 (80%) students participated. On average,
the survey took 9.5 minutes. Overall, an initial reliability analysis
resulted in a good global ùõº of .89. However, when we look at each
domain separately, several domains contained a number of items
that have a low item-total correlation, according to the rules of
Ebel and Frisbie [13]. After removing the doubtful ùëÖùëñùë° correlations,
reducing the number of items from the initial 56 to 37, the global
ùõº increased to .91. Full reliability item statistics can be found in
Table 2 at the end of this paper.

The Kaiser-Meyer-Olkin statistic for the 37 remaining items is
.90, a value that Kaiser calls ‚Äúmarvelous‚Äù for the factorability [22].
This is also a promising indication for sound construct validity.

We then executed a confirmatory factor analysis with the seven
FGS domains as factors. The Comparative Fit Index (CFI) is .76.
The Root Mean Square Error of Approximation (RMSEA) is .066,
for which the P-value of ùëÖùëÄùëÜùê∏ùê¥ ‚â§ 0.05 was zero. According to
Kline, an acceptable conformity should report values between the
ranges .06 ‚â§ ùëÖùëÄùëÜùê∏ùê¥ ‚â§ .08 and .90 ‚â§ ùê∂ùêπ ùêº ‚â§ .96 [27]. The bad
CFI indicates that the seven-factor model is not a great fit for the
data and that, as may have been expected, the FGS domains are
not sufficiently independent factors. Therefore, we shifted from
confirmatory to exploratory analysis.

A parallel analysis suggests to use between three to five factors,
as visible in the scree plot in Figure 1. Five- and four-factor EFAs
result in unsatisfactory component matrix distributions, while a
three-factor EFA with an oblique rotation reports a CFI of .91 and
a RMSEA of .06. We use the oblique rotation because we recognize
that there is likely to be some correlation between students‚Äô latent
subject matter preference factors. Table 1 shows the three-factor
loadings with principal axis factoring.

Figure 1: Scree plot (eigenvalues according to the factors).

As could be expected from the lower ùëÖùëñùë° correlations in Table 2
for domains such as critical thinking (CRI) and creative techniques
(TCH), items related to those domains result in low loading values in

Self-Assessing Creative Problem Solving for Aspiring Software Developers: A Pilot Study

Conference‚Äô17, July 2017, Washington, DC, USA

Table 1. For instance, only three CRI items remain in the component
matrix when employing a cutoff rate of .30. A reliability analysis
on the three revealed factors results in very good Cronbach‚Äôs ùõº
values: .89, .86 and .83 for F1, F2, and F3 respectively. We further
discuss these results in Section 6.

Table 1: Rotated Component Matrix: reduced 32-item CPPST
set. Loadings below .30 (CRI7, TCH2, TCH5, CTR1, CTR2)
are omitted for brevity.

F2

F3

.41

Item
CUR2
CUR3
CUR4
CUR5
CUR6
MND3
MND4
TCH3
TCH6
KNW1
KNW2
KNW3
KNW4
KNW7
KNW8
COM7
CTR7

F1
.75
.51
.70
.59
.56
.71
.64
.44
.34
.68
.52
.58
.46
.35
.40
.50
.49

F1
.35

F2
.47
.32
.67
.67
.76
.57

Item
CUR7
CUR8
MND5
MND6
TCH8
KNW6
MND7
COM4
COM5
COM8
CTR3
CTR8
CRI1
CRI2
CRI3

F3

.42
.50
.42
.51
.33
.42
.47
.31
.36

6 DISCUSSION
In this section, we will interpret the results of the pilot study in
context of Creative Problem Solving and the FGS, as well as reflect
on possible uses and future work.

6.1 Revealed constructs
The factor analysis in Table 1 shows that, except KNW4 and CUR7,
the items are very evenly distributed across the three factors, which
are deemed reliable thanks to the high ùõº values. Factor F1 con-
tains most Curiosity items, almost all Knowledge, and Creative
Techniques items, and some Creative State of Mind items. These
17 items make up for the majority of the question pool. It is clear
that this group is knowledge oriented. Furthermore, knowledge
and curiosity-related questions show particularly high loadings.
We call this construct Ability. To be able to solve a programming
problem creatively, one first needs to be able to program. Next to
programming skills, knowledge of creative techniques that speed
up the process are a big help. Curiosity may be a key motivation for
investing the time and effort in acquiring all of these skills. KNW4,
‚Äúthe technical aspect of programming appealed to me‚Äù, is loaded on
both F1 and F2. Perhaps some students are mesmerized by the pro-
gramming challenge while others remain indifferent‚Äîindependent
of their technical ability.

Second, factor F2 contains Creative State of Mind items and a
number of related items. For instance, TCH8, the highest loaded
item, states ‚ÄúI was regularly stuck‚Äù, which is also related to one‚Äôs
mindset on how to approach a problem. The feeling of being com-
fortable with unknown aspects (KNW6) is also linked to one‚Äôs belief

in own abilities. CUR7, ‚ÄúI had fun while developing the project‚Äù,
is also loaded on both F1 and F2. This could perhaps indicate that,
as Figueiredo et al. mentioned, while having fun is stimulating
to learning to program, ‚Äúa lack of success on the (introductory)
programming courses, can also be a demotivating factor‚Äù [14]. A
minimum level of ability could increase the coding fun.

As F2 items are related to a way of thinking, we call F2 Mindset.
It seems related to what Duckworth calls ‚ÄúGrit‚Äù or what Dweck
calls a ‚ÄúGrowth Mindset‚Äù [19]. In a recent study on the relation
between mindset in computing students and their study perfor-
mance, Apiola and Sutinen [4] found that mindset on computing
was growth-oriented, but that mindset on creativity was the most
fixed of all scales. This means that students either think they are
creative, or that they are not, but they are not open to the idea of
nurturing creativity. A tool such as our CPPST may be used to track
how students‚Äô creativity increases over the course of a curriculum,
thereby helping to cure students of this misconception.

Lastly, factor F3 contains almost all communication-related items.
Other items, such as ‚Äúin discussions about problems, I often sug-
gested alternatives‚Äù (CRI1) and ‚ÄúI paid attention to its ease of use‚Äù
(CTR8) also involve a interactive element. Therefore, we call F3
Interaction. Social aspects have already been shown to be a critical
component of CPS [2]. External suggestions, support, and criticism
are important to build creative solutions and to build them quickly.
Creative programmers know how and how often to interact, and
how to critically interpret responses. Katz [23] talks about ‚Äúcol-
lective problem solving‚Äù and discusses its many advantages over
individual problem solving, which seem to translate well into the
field of SE.

6.2 First-year versus last-year students
The development of the CPPST is geared towards higher education
students enrolled in a computing study program. We surveyed both
first-year and last-year students and analysed the data as a whole to
better represent the entire curriculum. When inspecting the data of
first-years and last-years separately, we noticed slight differences
in item-total correlation and Cronbach‚Äôs ùõº values for certain FGS
domains. While CUR, MND and KNW items stay the same, many
last-year COM items have lower correlations. For example, COM1
and COM3 are about regularly asking feedback from fellow students
and peers. Perhaps graduates feel too confident and think they don‚Äôt
need feedback. Perhaps feedback is not perceived as something that
can enhance your own creative ideas, but as something obligatory
related to grading. To better interpret the results of our pilot study,
we aim to perform a focus group not unlike the FGS, but with
first-year and last-year students instead of industry experts. Still
too little is known about the perception of creativity in the minds
of SE students. We argue that, next to these quantitative results,
qualitative data can place these differences into context.

6.3 Summary
Our factor analysis yields three major components for CPS in SE:
Ability, Mindset, and Interaction. These factors somewhat overlap
with the aforementioned engineering scale of [2]. They discov-
ered four factors: cognitive approaches (not unlike our factor Abil-
ity), cognitive challenges and cognitive preparedness (somewhat

Conference‚Äô17, July 2017, Washington, DC, USA

Pre-print of accepted ITiCSE 2022 paper.

overlapping with our factor Mindset), and impulsivity. The major
difference is the absence of any interactive component, although
they also do mention the importance of social aspects throughout
their paper. In general, many creativity assessment toolkits lack an
interactive component. Another example is the work of Denson
et al. [12], where creative design in engineering is composed of
technical strength, aesthetic appeal, and originality. It is clear to
us that our first component, Ability, can easily be found in other
metrics. Some metrics take Mindset into consideration, and almost
none Interaction.

7 LIMITATIONS
To reduce treats to validity as much a possible, we closely followed
the recommendations for creative assessment outlined by Barbot et
al. [6]. For example, they indicate that measuring creative achieve-
ment is perhaps the most objective way to assess creativity. We
therefore formulated the questions in the context of a concrete
achievement (a programming project), and administered the survey
as close as possible to the project deadline.

In general, self-rating is bound to introduce some form of bias.
People are usually inaccurate in assessing their own ‚Äúgeneric cre-
ativity‚Äù, as stated by Barbot et al. We therefore chose to avoid the
word ‚Äúcreativity‚Äù altogether in our survey. Instead, we opted for
descriptive, concrete questions, related to programming as much as
possible. This also sidesteps potential issues due to a fixed mindset
on creativity as mentioned by Apiola and Sutinen [4].

We believe that the results of this pilot study show promise. The
Kaiser-Meyer-Olkin statistic indicates a high factorability, and the
Comparative Fit Index and Cronbach‚Äôs ùõº reliability values confirm
that the three-factor model is a good fit. The three major constructs
identified by the exploratory factor analysis are interesting and
explainable, albeit in need of more qualitative context to better
understand the perception of students‚Äô creativity.

8 CONCLUSION
In this paper, we explored the possibility for undergraduate soft-
ware developers to self-assess their CPS skills through a survey
that encompasses seven dimensions of creativity, based on existing
validated scales and conducted focus groups. The Creative Program-
ming Problem Solving Test or CPPST is a survey geared specifically
towards computing education, as we found that existing creativity
metrics are either too broad or too narrow, and do not match well
with expectations of software engineering industry experts.

Principal axis factor analysis grouped the focus group dimen-
sions into three overarching constructs: (1) Ability, (2) Mindset, and
(3) Interaction. We believe that the found constructs can be used
as important building blocks for amplifying students‚Äô creativity in
computing education. Furthermore, this work contributes towards
computing education research by bringing research on creativity
in the field of cognitive psychology closer to the field of computing
education. The test could be used to gauge the creativity levels of
a student assignment. In future work, we plan to conduct a focus
group to gather qualitative data and iterate on the CPPST to further
improve the validity of the self-test. We will also use it as a pre
and post education intervention metric in order to measure the
effectiveness of creativity teaching techniques.

A FULL RELIABILITY ITEM STATISTICS

Table 2: Reliability item statistics, calculated individually
per domain. Items in bold are excluded after inspecting r.cor.

item

raw.r

std.r

r.cor

r.drop mean

sd

invert?

CUR1
CUR2
CUR3
CUR4
CUR5
CUR6
CUR7
CUR8

MND1
MND2
MND3
MND4
MND5
MND6
MND7
MND8

TCH1
TCH2
TCH3
TCH4
TCH5
TCH6
TCH7
TCH8

KNW1
KNW2
KNW3
KNW4
KNW5
KNW6
KNW7
KNW8

COM1
COM2
COM3
COM4
COM5
COM6
COM7
COM8

CTR1
CTR2
CTR3
CTR4
CTR5
CTR6
CTR7
CTR8

CRI1
CRI2
CRI3
CRI4
CRI5
CRI6
CRI7
CRI8

0.018
0.629
0.603
0.677
0.495
0.554
0.636
0.553

0.257
0.263
0.663
0.563
0.626
0.479
0.419
0.011

0.249
0.319
0.493
0.156
0.298
0.296
0.187
0.413

0.582
0.478
0.577
0.613
0.242
0.574
0.373
0.461

0.191
0.324
0.242
0.464
0.372
0.173
0.501
0.301

0.397
0.463
0.491
0.251
0.318
0.276
0.457
0.408

0.412
0.401
0.412
0.259
0.131
0.181
0.322
0.222

0.0232
0.6222
0.6042
0.6693
0.5006
0.5548
0.6171
0.5388

0.2685
0.2475
0.6576
0.5560
0.6105
0.4534
0.4172
0.0014

0.2366
0.3194
0.5116
0.1589
0.3119
0.3136
0.1952
0.3907

0.5903
0.4923
0.5868
0.6016
0.2494
0.5547
0.3781
0.4429

0.2063
0.3066
0.2319
0.4846
0.3724
0.2033
0.4885
0.3333

0.4186
0.4497
0.4939
0.2583
0.2941
0.2680
0.4605
0.4215

0.4363
0.4208
0.4142
0.2659
0.1321
0.1834
0.3154
0.2310

0.007
0.629
0.605
0.678
0.493
0.550
0.622
0.535

0.247
0.224
0.664
0.553
0.616
0.450
0.403
0.040

0.281
0.299
0.502
0.128
0.290
0.290
0.171
0.383

0.593
0.483
0.584
0.606
0.226
0.555
0.360
0.435

0.180
0.286
0.206
0.475
0.359
0.181
0.483
0.316

0.405
0.438
0.487
0.234
0.277
0.244
0.450
0.406

0.426
0.405
0.400
0.244
0.099
0.157
0.296
0.207

0.030
0.599
0.574
0.651
0.459
0.516
0.602
0.515

0.217
0.206
0.635
0.524
0.591
0.434
0.379
0.043

0.285
0.279
0.467
0.109
0.258
0.259
0.140
0.366

0.555
0.450
0.551
0.578
0.199
0.535
0.336
0.418

0.147
0.267
0.191
0.436
0.335
0.143
0.456
0.275

0.369
0.418
0.457
0.214
0.256
0.229
0.418
0.375

0.382
0.370
0.370
0.215
0.079
0.133
0.271
0.178

3.3
3.5
3.7
3.8
3.4
3.4
3.6
3.2

3.6
3.0
3.7
3.6
3.2
2.5
3.5
3.0

2.9
3.4
3.7
3.7
3.8
3.6
3.5
2.6

4.0
3.9
3.7
3.5
3.4
3.2
3.7
3.1

3.7
3.2
2.9
4.0
4.2
4.1
3.3
4.0

3.9
3.3
3.6
3.3
2.7
3.4
3.5
4.0

3.7
3.5
3.8
3.7
3.9
3.2
3.1
3.6

0.96
0.96
0.87
0.94
0.95
1.07
1.09
1.07

0.86
1.21
0.95
1.09
1.10
1.13
0.95
1.09

0.79
0.88
0.68
0.96
0.87
0.80
0.99
1.12

0.80
0.71
0.77
1.10
0.90
1.14
0.83
1.06

0.91
1.25
1.08
0.70
0.87
0.62
1.16
0.56

0.68
1.12
0.89
0.79
1.35
1.01
0.96
0.77

0.73
0.73
0.99
0.94
1.05
0.98
1.12
0.93

‚úì
‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

‚úì

ACKNOWLEDGMENTS
We would like to thank Dr. Barbot‚Äôs creativity research team of
the Psychological Sciences Research Institute from UCLouvan for
their numerous suggestions and critical view to an early draft of
our survey.

Self-Assessing Creative Problem Solving for Aspiring Software Developers: A Pilot Study

Conference‚Äô17, July 2017, Washington, DC, USA

REFERENCES
[1] Teresa M Amabile. 1988. A model of creativity and innovation in organizations.

Research in organizational behavior 10, 1 (1988), 123‚Äì167.

[2] Susan Amato-Henderson, Amber Kemppainen, and Gretchen Hein. 2011. Assess-
ing creativity in engineering students. In 2011 Frontiers in Education Conference
(FIE). IEEE, T4F‚Äì1.

[3] Aamir Amin, Shuib Basri, Mohd Fadzil Hassan, and Mobashar Rehman. 2017.
A snapshot of 26 years of research on creativity in software engineering-A
systematic literature review. In International Conference on Mobile and Wireless
Technology. Springer, 430‚Äì438.

[4] Mikko Apiola and Erkki Sutinen. 2020. Mindset and Study Performance: New
Scales and Research Directions. In Koli Calling‚Äô20: Proceedings of the 20th Koli
Calling International Conference on Computing Education Research. 1‚Äì9.

[5] Baptiste Barbot. 2019. Measuring creativity change and development. Psychology

of Aesthetics, Creativity, and the Arts 13, 2 (2019), 203.

[6] Baptiste Barbot, Richard W Hass, and Roni Reiter-Palmon. 2019. Creativity
assessment in psychological research:(Re) setting the standards. Psychology of
Aesthetics, Creativity, and the Arts 13, 2 (2019), 233.

[7] Michael Mose Biskjaer. 2013. Self-imposed creativity constraints. Ph.D. Disser-
tation. Department of Aesthetics and Communication, Faculty of Arts, Aarhus
University.

[8] Michael Mose Biskjaer, Bo T Christensen, Morten Friis-Olivarius, Sille JJ
Abildgaard, Caroline Lundqvist, and Kim Halskov. 2020. How task constraints
affect inspiration search strategies. International Journal of Technology and Design
Education 30, 1 (2020), 101‚Äì125.

[9] Erin A Carroll, Celine Latulipe, Richard Fung, and Michael Terry. 2009. Creativity
factor evaluation: towards a standardized survey metric for creativity support. In
Proceedings of the seventh ACM conference on Creativity and cognition. 127‚Äì136.
[10] Mihaly Csikszentmihalyi. 1997. Flow and the psychology of discovery and

invention. HarperPerennial, New York 39 (1997).

[11] Gary A Davis. 1999. Barriers to creativity and creative attitudes. Encyclopedia of

creativity 1 (1999), 165‚Äì174.

[12] Cameron D Denson, Jennifer K Buelin, Matthew D Lammi, and Susan D‚ÄôAmico.
2015. Developing Instrumentation for Assessing Creativity in Engineering Design.
Journal of Technology Education 27, 1 (2015), 23‚Äì40.

[13] Robert L Ebel and David A Frisbie. 1972. Essentials of educational measurement.

Prentice-Hall Englewood Cliffs, NJ.

[14] Jos√© Figueiredo, Noel Lopes, and Francisco Jos√© Garc√≠a-Pe√±alvo. 2019. Predicting
student failure in an introductory programming course with multiple back-
propagation. In Proceedings of the Seventh International Conference on Technologi-
cal Ecosystems for Enhancing Multiculturality. 44‚Äì49.

[15] Keston H Fulcher. 2004. Towards measuring lifelong learning: The curiosity index.

Ph.D. Dissertation. ProQuest Information & Learning.

[16] Wouter Groeneveld, Brett A Becker, and Joost Vennekens. 2020. Soft Skills: What
do Computing Program Syllabi Reveal About Non-Technical Expectations of

Undergraduate Students?. In Proceedings of the 2020 ACM Conference on Innovation
and Technology in Computer Science Education. 287‚Äì293.

[17] Wouter Groeneveld, Hans Jacobs, Joost Vennekens, and Kris Aerts. 2020. Non-
cognitive abilities of exceptional software engineers: a Delphi study. In Pro-
ceedings of the 51st ACM Technical Symposium on Computer Science Education.
1096‚Äì1102.

[18] Wouter Groeneveld, Laurens Luyten, Joost Vennekens, and Kris Aerts. 2021.
Exploring the Role of Creativity in Software Engineering. In 2021 IEEE/ACM 43rd
International Conference on Software Engineering: Software Engineering in Society
(ICSE-SEIS). IEEE, 1‚Äì9.

[19] Aaron Hochanadel, Dora Finamore, et al. 2015. Fixed and growth mindset in
education and how grit helps students persist in the face of adversity. Journal of
International Education Research (JIER) 11, 1 (2015), 47‚Äì50.

[20] Andy Hunt. 2008. Pragmatic thinking and learning: Refactor your Wetware. Prag-

matic bookshelf.

[21] Susan A Jackson and Herbert W Marsh. 1996. Development and validation of a
scale to measure optimal experience: The Flow State Scale. Journal of sport and
exercise psychology 18, 1 (1996), 17‚Äì35.

[22] Henry F Kaiser. 1974. An index of factorial simplicity. Psychometrika 39, 1 (1974),

31‚Äì36.

[23] Ruth Katz. 1984. Collective" Problem-Solving" in the History of Music: The Case

of the Camerata. Journal of the History of Ideas (1984), 361‚Äì377.

[24] James C Kaufman. 2012. Counting the muses: development of the Kaufman
domains of creativity scale (K-DOCS). Psychology of Aesthetics, Creativity, and
the Arts 6, 4 (2012), 298.

[25] Jin-Young Kim. 2020. A longitudinal study of the relation between creative po-
tential and academic achievement at an engineering university in Korea. Journal
of Engineering Education 109, 4 (2020), 704‚Äì722.

[26] John R Kirby, Christopher Knapper, Patrick Lamon, and William J Egnatoff. 2010.
Development of a scale to measure lifelong learning. International Journal of
Lifelong Education 29, 3 (2010), 291‚Äì302.

[27] Rex B Kline. 2015. Principles and practice of structural equation modeling. Guilford

publications.

[28] √ñzgen Korkmaz, Recep √áakir, and M Ya≈üar √ñzden. 2017. A validity and reliability
study of the computational thinking scales (CTS). Computers in human behavior
72 (2017), 558‚Äì569.

[29] Angie L Miller. 2014. A self-report measure of cognitive processes associated

with creativity. Creativity Research Journal 26, 2 (2014), 203‚Äì218.

[30] Kathleen Ofstedal and Kathryn Dahlberg. 2009. Collaboration in student teaching:
Introducing the collaboration self-assessment tool. Journal of Early Childhood
Teacher Education 30, 1 (2009), 37‚Äì48.

[31] Edward M Sosu. 2013. The development and psychometric validation of a Critical
Thinking Disposition Scale. Thinking skills and creativity 9 (2013), 107‚Äì119.
[32] E Paul Torrance. 1972. Predictive validity of the torrance tests of creative thinking.

The Journal of creative behavior (1972).

