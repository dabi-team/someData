2
2
0
2

y
a
M
6
1

]
E
M

.
t
a
t
s
[

1
v
0
4
6
7
0
.
5
0
2
2
:
v
i
X
r
a

ecpc: An R-package for generic co-data models for
high-dimensional prediction

Mirrelijn M. van Nee∗1, Lodewyk F.A. Wessels2, and Mark A. van de Wiel1,3

1Epidemiology and Data Science, Amsterdam Public Health research institute, Amsterdam
University Medical Centers, The Netherlands
2Molecular Carcinogenesis, Oncode Institute and Netherlands Cancer Institute
3MRC Biostatistics Unit, Cambridge University, UK

Abstract

High-dimensional prediction considers data with more variables than samples. Generic
research goals are to ﬁnd the best predictor or to select variables. Results may be im-
proved by exploiting prior information in the form of co-data, providing complementary
data not on the samples, but on the variables. We consider adaptive ridge penalised gen-
eralised linear and Cox models, in which the variable speciﬁc ridge penalties are adapted
to the co-data to give a priori more weight to more important variables. The R-package
ecpc originally accommodated various and possibly multiple co-data sources, including
categorical co-data, i.e. groups of variables, and continuous co-data. Continuous co-data,
however, was handled by adaptive discretisation, potentially ineﬃciently modelling and
losing information. Here, we present an extension to the method and software for generic
co-data models, particularly for continuous co-data. At the basis lies a classical linear
regression model, regressing prior variance weights on the co-data. Co-data variables are
then estimated with empirical Bayes moment estimation. After placing the estimation pro-
cedure in the classical regression framework, extension to generalised additive and shape
constrained co-data models is straightforward. Besides, we show how ridge penalties may
be transformed to elastic net penalties with the R-package squeezy. In simulation studies
we ﬁrst compare various co-data models for continuous co-data from the extension to the
original method. Secondly, we compare variable selection performance to other variable
selection methods. Moreover, we demonstrate use of the package in several examples
throughout the paper.

1 Introduction

Generalised linear models (GLMs) (McCullagh and Nelder, 1989) are the cornerstone of many
statistical models for prediction and variable selection purposes, modelling the relation between
outcome data and observed data. When observed data are high-dimensional, with the number
of variables far exceeding the number of samples, these models may be penalised to account
for the high-dimensionality. Well known examples include the ridge (Hoerl and Kennard,
1970), lasso (Tibshirani, 1996) and elastic net penalty (Zou and Hastie, 2005). One of the
main assumptions underlying generalised linear models is that all variables are exchangeable.
In many high-dimensional settings, however, this assumption is questionable (Ignatiadis and
Lolas, 2020). For example, in cancer genomics, variables may be grouped according to some
biological function. Variables within these groups may have a similar eﬀect, while variables
from diﬀerent groups have a diﬀerent eﬀect. Hence, variables are exchangeable within groups,
but not between groups. represent gene expression of genes that have similar eﬀect within

∗The ﬁrst author is supported by ZonMw TOP grant COMPUTE CANCER (40- 00812-98-16012).

1

 
 
 
 
 
 
groups representing some biological function but diﬀerent eﬀect between those groups. To
alleviate the exchangeability assumption, shared information may be modelled explicitly in
the prior distribution of the variables, e.g. by introducing shared group penalties, penalising
more important groups of variables relatively less (as done by van de Wiel et al. (2016)). The
shared prior information may be represented in data matrices, called co-data, to distinguish
the main, observed data with information on the samples from the complementary data with
information on the variables. When the co-data are related to the eﬀect sizes of variables,
these data may be exploited to improve prediction and variable selection in high-dimensional
data settings.

Various R-packages accommodate approaches to incorporate some form of co-data. Early
methods such as grplasso (Meier et al., 2008) and gglasso (Yang and Zou, 2015) allow for cat-
egorical, or grouped, co-data, by using group lasso penalties. As these penalties are governed
by one overall penalty parameter, these types of penalties may be not ﬂexible enough to model
the relation between the eﬀect sizes and grouped co-data. To increase this ﬂexibility, other
methods were developed that estimate multiple, group-speciﬁc penalty (or prior) parameters,
using eﬃcient empirical Bayes approaches. Examples include GRridge (van de Wiel et al.,
2016) for group-adaptive ridge penalties (normal priors), graper (Velten and Huber, 2019) for
group-adaptive spike-and-slab priors and gren (M¨unch et al., 2019) for group-adaptive elas-
tic net priors. The method ecpc (van Nee et al., 2021b) presents a ﬂexible empirical Bayes
approach to extend the use of grouped co-data to various other (and potentially multiple) co-
data types, such as hierarchical groups and continuous co-data, for multi-group adaptive ridge
penalties. For continuous co-data, however, the normal prior variances corresponding to the
ridge penalties are not modelled as a function of the continuous co-data variable, but rather as
a function of groups of variables corresponding to the adaptively discretised co-data variable.
When the relation between the prior variance and continuous co-data is non-constant and/or
“simple”, e.g. linear, the adaptive discretisation may lead to a loss of information and/or inef-
ﬁciently model the relation. The package fwelnet (Tay et al., 2020) develops feature-weighted
elastic net for continuous co-data speciﬁcally (there called “features of features”). Regression
coeﬃcients are estimated jointly with co-data variable weights, modelling the variable-speciﬁc
elastic net penalties by a normalised, exponential function of the co-data. For categorical
co-data, fwelnet boils down to an elastic net penalty on the group level (Tay et al., 2020),
governed by one overall penalty parameter. Hence, it may lack ﬂexibility when compared to
empirical Bayes methods estimating multiple penalties. The package squeezy (van Nee et al.,
2021a) presents fast approximate marginal likelihood estimates for group-adaptive elastic net
penalties, but is available for grouped co-data only.

Here, we present an extension of the R-package ecpc to generic co-data models, in particular
for continuous co-data. First, we show how a classical linear regression model may be used
to regress the (unknown) variable-speciﬁc normal prior variances on the co-data. The co-data
variable weights are estimated with an empirical Bayes moment estimator, slightly modiﬁed
from van Nee et al. (2021b). Then, we present how the estimation procedure may be extended
straightforwardly to model the relation between the prior variances and co-data by generalised
additive models (Hastie and Tibshirani, 1986) for modelling non-linear functions and by shape
constrained additive models (Pya and Wood, 2015) for shape constrained functions, e.g. posi-
tive and monotonically increasing functions. Besides, we use ideas from van Nee et al. (2021a)
to transform the adaptive ridge penalties to elastic net penalties using the package squeezy.
Either this approach or the previously implemented posterior selection approaches (van Nee
et al., 2021b) may be used for variable selection.

1.1 Getting started

The goal of this paper is to provide a stand-alone introduction to the package ecpc and to
provide examples of its use. To get started, one may install the R-package ecpc from CRAN
and load it by running:

2

R> install.packages("ecpc")
R> library("ecpc")

The main function in the package is the eponymous function ecpc(), which ﬁts a ridge penalised
generalised linear model by estimating the co-data variable weights and regression coeﬃcients
subsequently. The function outputs an object of the S3-class ‘ecpc’, for which the methods
summary(), print(), plot(), predict() and coef() have been implemented. See the index
in ?"ecpc-package" for a list of all functions, including functions for preparing and visualising
co-data, or see Figure 1 for a cheat sheet of the main functions and workﬂow of the package.
The remainder of this paper is organised as follows: Section 2 ﬁrst presents the model
and the co-data models for a linear, generalised additive and shape-constrained co-data model.
All types of co-data models are accompanied with short examples of how to use the package.
Next, it is shown how ridge penalties may be transformed to elastic net penalties, again ac-
companied with a short toy example. Section 3 ﬁrst compares various ways of modelling a
continuous co-data variable with the originally proposed adaptive discretisation in a simula-
tion study. Secondly, the method is compared to other methods in terms of variable selection.
Section 4 then demonstrates use of the software on an application to the classiﬁcation of lymph
node metastasis using high-dimensional RNA expression data. Section 5 shortly concludes the
software.

2 Method

Consider response data Y ∈ Rn, observed high-dimensional data X ∈ Rn×p with p (cid:29) n, which
contain information on the n samples of Y , and possibly multiple co-data matrices Z (d) ∈
Rp×Gd , d = 1, .., D, which contain prior information on the p variables of X. Generally, co-
data matrices may include continuous or categorical co-data. For categorical co-data, dummy
variables should be provided. For categorical co-data with overlapping categories, dummy
variables may be weighted accordingly to account for multiplicity (see van Nee et al. (2021b)).
We consider a generalised linear model for the response with canonical link function g(·),
parameterised with regression coeﬃcients β ∈ Rp. Furthermore, we model the regression
coeﬃcients with a normal prior, corresponding to a ridge penalty, in which the prior variance
is regressed on the co-data:

Yi|X i, β ind.∼ π (Yi|X i, β) , EYi|X i,β(Yi) = g−1(X iβ), i = 1, .., n,

βk

ind.∼ N (0, vk), vk = τ 2

global

D
(cid:88)

d=1

wdZ(d)

k γ(d), k = 1, .., p,

(1)

with X i and Zk the ith and kth row of X and Z respectively, γ(d) ∈ RG the co-data variable
weights for co-data matrix d, w the co-data matrix weights and τ 2
global a scaling factor which
may improve numerical computations in practice. When the data X consist of multiple data
modalities, like gene expression data, copy number data and methylation data in genomics,
scaling factors speciﬁc to the data modalities may be used (Boulesteix et al., 2017; van de Wiel
et al., 2021) and estimated with ecpc.

Prior parameters and regression coeﬃcients are estimated with an empirical Bayes ap-
proach, following van Nee et al. (2021b). In short, ﬁrst the global scaling parameter τ 2
global is
estimated, then the co-data variable weights γ(d) for each co-data matrix d separately and then
the co-data weights w. After, given the prior parameter estimates, the regression coeﬃcients
are estimated by maximising the penalised likelihood (equivalent to maximising the posterior).
Multiple co-data matrices may be either provided in diﬀerent matrices Z (1), .., Z (D) or stacked
and provided in one matrix Z := [Z (1), .., Z (D)]. Below, we ﬁrst consider the case in which we
have only one (stacked) co-data matrix Z, for which w = 1, and drop notions of d. Besides,
without loss of generality, we set the scaling parameter τ 2
global to 1. See the remark in Section
2.3 for discussion of the diﬀerences between multiple matrices and one stacked matrix.

3

Figure 1: Cheat sheet for the main functions and work ﬂow of the R-package ecpc, available as pdf-ﬁle
on https://github.com/Mirrelijn/ecpc.

4

- Estimate parameters: fit <- ecpc(Y, X, Z=list(Z_1, Z_2))- Visualise estimates: >Regression coefficients:plot(fit, show="coefficients") >Prior variances:plot(fit, show="priorweights")- Predict for new samples X_2:predictions <- predict(fit, X_2)- Select variables: >A posteriori:fit_post <- postSelect(fit, X, Y) >Transform ridge to elastic net penalties  with parameter alpha:fit_squeezy <- squeezy(Y, X, alpha=alpha, lambdas=fit$penalties)- Create group set:>Categorical:gs_1 <- createGroupset(factor)>Continuous discretised in non-overlapping groups:gs_2 <- createGroupset(values) >Continuous discretised in overlapping groupsfor adaptive discretisation:gs_3 <- splitMedian(values)>Group set on the group level for hierarchical groupsfor adaptive discretisation:gs_grouplvl <- obtainHierarchy(groupset_3)- Choose hypershrinkage (penalty on the    group level):>Few groups: hypershrinkage = "none">Many groups: hypershrinkage="ridge">Select groups: hypershrinkage="lasso">Groups structured in (hierarchical) groups:hypershrinkage="hierLasso,ridge",groupsets.grplvl=list(groupset_grouplvl) - Estimate parameters:fit_gs <- ecpc(Y, X,             groupsets=list(gs_1, gs_2, gs_3),         groupsets.grplvl=list(NULL, NULL,                                 gs_grouplvl),         hypershrinkage=c("none", "ridge",                          "hierLasso,ridge"))- Create co-data (related) matrix:>For group set (make dummy variables):Z_1 <- createZforGroupset(gs_1)>Spline matrix for continuous co-data:Z_2 <- createZforSplines(values)>Difference penalty matrix for splines:S_1 <- createS(G=dim(Z_2)[2])>Constraints for splines:Con_1 <- createCon(G=dim(Z_2)[2]), shape)for shape one of "positive", "monotone.i"    (increasing), "monotone.d" (decreasing), "convex",    "concave", or any combination thereof by    separating with a "+", e.g. "positive+convex"- Choose hypershrinkage (penalty on the co-data    variables):>No penalty/constraints, e.g. linear co-data model:paraPen=NULL, paraCon=NULL>Generalised ridge penalty, e.g. generalised additive     co-data model:paraPen=list(Z2=list(S1=S_1))>Constraints, e.g. shape constrained additive     co-data model:paraCon=list(Z2=Con_1)- Estimate parameters:fit_Z <- ecpc(Y, X, Z=list(Z_1, Z_2),         paraPen=list(Z2=list(S1=S_1)),         paraCon=list(Z2=Con_1))Group 2Group 1Preparing co-dataMain workflowIn group setsIn co-data matricesCo-dataHigh-dimensional dataResponse Cheat sheet for R-package ecpc: Empirical bayes Co-data learnt Prediction and  Covariate selection- Install and load package: install.packages("ecpc") library("ecpc")- Prepare data and co-data (see right):Next, we show how the empirical Bayes approach of estimating γ (van Nee et al., 2021b)
straightforwardly adapts to continuous co-data when the relation is assumed to be linear (as
in Equation (1)). From thereon, we show how the approach naturally ﬁts into the framework
of generalised additive models and shape constrained additive models.

Remark. The ﬁrst version of ecpc, as described in van Nee et al. (2021b), only handles
(possibly overlapping) groups of variables. Co-data is then supplied in a list of group sets, in the
argument groupsets. Continuous co-data variables may be handled by adaptive discretisation.
The new version discussed in this paper allows for both (undiscretised) continuous and grouped
co-data. In the new version of ecpc, one needs to supply co-data as a list of co-data matrices
in the argument Z. The function createZforGroupset() may be used to obtain the co-data
matrix with dummy variables corresponding to a group set for grouped co-data.

2.1 Details for linear co-data models

Consider a linear relation between the prior variances and the co-data:

v = Zγ.

The co-data variable weights γ are estimated in van Nee et al. (2021b) with moment estimation
by equating theoretical moments to empirical moments. For co-data that represent groups of
variables, the empirical moments are averaged over all variables in that group, leading to a
linear system of G equations and G unknowns. For co-data that do not represent groups
of variables, e.g. continuous co-data, we simply form one group per variable, leading to the
following linear system of p equations and G unknowns:

with ◦ representing the Hadamard (element-wise) product. C ∈ Rp×p and b ∈ Rp are derived
in van Nee et al. (2021b) and given by:

(C ◦ C)Zγ = b,

(2)

C = (X T W X + ˜Ω)−1X T W X,
b = ˜β.2 − ˜v,
˜v = diag((X T W X + ˜Ω)−1X T W X(X T W X + ˜Ω)−1),

with ˜β the maximum penalised likelihood estimate given an initial ˜τ 2
global and corresponding
constant diagonal ridge penalty matrix ˜Ω, with W a diagonal weight matrix used in the iterative
weighted least squares algorithm to ﬁt ˜β, and with ˜v an estimate for the variance of ˜β with
respect to the response data Y .

Solving the linear system leads to the following least squares estimate for γ. As the prior

variance has to be positive, the resulting prior variance estimate Zγ is truncated at 0:

ˆγ = argmin

γ

||(C ◦ C)Zγ − b||2
2,

ˆv = (Z ˆγ)+.

(3)

In generalised linear models it is common to use a log-link for the response to enforce positivity,
resulting in positive, multiplicative eﬀects. Note that here, however, Equation (2) is the result
of equating theoretical to empirical moments. Replacing b by log(b) would violate the moment
equalities. Also, if we would enforce positivity instead by, for example, substituting Zγ directly
by v = exp(Zγ(cid:48)), the moment equations would not be linear anymore in γ, nor multiplicative,
e.g. as (C ◦ C) exp(Zγ) (cid:54)= exp((C ◦ C)Zγ) = (cid:81)G
g=1 exp((C ◦ C)Zgγg), with Zg the gth co-
data variable. Hence, the advantage of simply post-hoc truncating Z ˆγ is that the system of
equations in Equation (3) is easily solved. Section 2.3 discusses shape constrained co-data
models which may be used to enforce positivity by including it as a constraint.

5

2.1.1

Interpretation

The interpretation of the co-data weights γ (scaled by τ 2
global) is similar as in regular linear
regression: when co-data variable Zg increases with one unit, while the other co-data variables
are kept ﬁxed, then the prior variance increases with γg (γgτ 2
global). Consequently, when the
prior variance for the eﬀect βk of some variable Xk increases with γg (γgτ 2
global), then the a
priori expected squared eﬀect size, E(β2
global). In other words,
when we would compare the eﬀect of two variables Xk and Xl with the same co-data values,
except for one co-data variable which is one unit higher for Xk than for Xl, then we would a
priori expect β2

k) = vk, increases with γg (γgτ 2

k to be on average γg (γgτ 2

global) larger than β2
l .

2.1.2 Short example in ecpc

For this short example and the ones below, simulate some linear response data:

R> set.seed(1)
R> p <- 300 #number of covariates
R> n <- 100 #sample size training data set
R> n2 <-100 #sample size test data set
R> beta <- rnorm(p, mean=0, sd=0.1) #simulate effects
R> X <- matrix(rnorm(n*p, mean=0, sd=1), n, p) #simulate observed training data
R> Y <- rnorm(n, mean = X%*%beta, sd=1) #simulate response training data
R> X2 <- matrix(rnorm(n2*p, mean=0, sd=1), n, p) #simulate observed test data
R> Y2 <- rnorm(n2, mean = X2%*%beta, sd=1) #simulate response test data

As co-data, suppose that we have two co-data variables; one informative co-data variable
containing the true absolute eﬀect sizes and one non-informative co-data variable containing
random normally distributed values:

R> Z1 <- abs(beta) #informative co-data
R> Z2 <- rnorm(p, mean=0, sd=1) #random, non-informative co-data
R> Z <- cbind(Z1, Z2) #(px2)-dimensional co-data matrix

Then we ﬁt the linear co-data model and test the ﬁt on the test data. Besides, we set
postselection=FALSE to only estimate the dense model, without selecting variables a pos-
teriori (see Section 2.4):

R> fit <- ecpc(Y, X, Z=list(Z), X2=X2, Y2=Y2, postselection=FALSE)

[1] "Estimate global tau^2 (equiv. global ridge penalty lambda)"
[1] "Estimate co-data weights and (if included) hyperpenalties with mgcv"
[1] "Estimate regression coefficients"

Note that the co-data matrix is provided in a list, as it is also possible to provide a list of
multiple co-data matrices. This will be used to explicitly distinguish linear co-data variables
from smooth or constrained ones, as exempliﬁed below. The performance of the ﬁt on the
test data may be given for both the co-data learnt model ﬁt with ecpc() and for the co-data
agnostic model ﬁt with one global ridge penalty:

R> fit$MSEecpc

[1] 2.521757

R> fit$MSEridge

[1] 2.889294

A (summary of) the ﬁtted prior parameters, prior variances and regression coeﬃcients can be
retrieved by the methods print() and summary():

6

R> print(fit)

ecpc fit

Estimated co-data variable weights:
0.2125547 -0.001632461

Estimated co-data weights:
1

R> summary(fit)

Summary estimated prior variances:

Min.

Max.
0.0000000 0.0000000 0.0008287 0.0068645 0.0106837 0.0471067

3rd Qu.

1st Qu.

Median

Mean

Summary estimated regression coefficients:

Min.

1st Qu.
-0.251409 0.000000

Median
0.000000

Mean
0.002368

3rd Qu.
0.002004

Max.
0.269845

Estimated intercept:

0.07294278

Alternatively, the plot() method provides a graph of the regression coeﬃcients and prior
variances.
If the R-packages ggplot2 (Wickham, 2016) and ggpubr (Kassambara, 2020) are
installed, the output looks as shown in Figure 2, else a similar plot will be made with the base
R plot() function.

R> plot(fit, show="coefficients")
R> plot(fit, show="priorweights", Z=list(Z))

Figure 2: Example output for plot() in a linear co-data model for show="coefficients" (left) and
show="priorweights" (right).

Lastly, the regression coeﬃcients may be re-estimated for diﬀerent prior parameters. First,
the function penalties() may be used to change some prior parameters and retrieve the
corresponding ridge penalties. Then, the method coef() re-estimates the regression coeﬃcients
given these penalties. For example, if one would alter the global level of regularisation by
multiplying τ 2

global by 2:

R> new_penalties <- penalties(fit, tauglobal = fit$tauglobal * 2, Z=list(Z))
R> new_coefficients <- coef(fit, penalties=new_penalties, X=X, Y=Y)

Note that in general, however, altering the prior parameters by hand will not be needed as
these parameters are optimised by the function ecpc(). The functions above, however, may

7

0.000.020.040.060.000.010.020.030.04Prior varianceSquared regression coefficients0.000.050.100.150.20Z1Z2Co−data variablePrior variance weightCo−data set 1be used to conveniently skip prior parameter estimation when prior parameters are known, e.g.
when results have been saved and need to be checked quickly. For example, here we just set
all prior parameters to one and only estimate the regression coeﬃcients:

R> new_penalties2 <- penalties(tauglobal = 1, sigmahat = 1, gamma = c(1,1),
+
R> new_coefficients2 <- coef.ecpc(penalties=new_penalties2, X=X, Y=Y)

w = 1, Z=list(Z))

2.2 Details for generalised additive co-data models

Generalised additive models (GAMs), originally proposed in Hastie and Tibshirani (1986),
have been widely applied to model non-linear relations. Applied here, we assume that the
relation between the prior variance and co-data may be modeled by a sum of smooth functions,
s1(·), .., sG(·), of the co-data variables:

v =

G
(cid:88)

g=1

sg(Zg).

In practice, the smooth functions are estimated by using a basis expansion to recast the problem
into a linear model (as originally proposed by, for example, Wahba (1980)). So, for a basis
expansion consisting of Jg basis functions φg,j(·), j = 1, .., Jg, for co-data variable Zg:

sg(Zg) =

Jg
(cid:88)

j=1

φg,j(Zg)γg,j = Φjγg,

v =

G
(cid:88)

g=1

Φjγg = ZGAM γGAM ,

with Φg ∈ Rp×Jg the matrix of co-data variable vector Zg ∈ Rp evaluated in all Jg basis
functions, ZGAM = [Φ1, .., ΦG] and γGAM = (γT

1 , .., γT

G)T .

The type and number of basis functions should in general be chosen such that they are
ﬂexible enough to approximate the underlying function well. To avoid overﬁtting for too many
basis functions, the coeﬃcients may be estimated by optimising the likelihood penalised by a
smoothing penalty. While our software allows the user to supply any basis expansion, we focus
here on the popular p-splines (see Eilers and Marx (2021) for an introduction). This approach
combines ﬂexible spline basis functions with a quadratic smoothing penalty on the diﬀerences
of the spline coeﬃcients. So, the smoothing penalty is of the form γT
γGAM ,
where the diﬀerence penalty matrix Sg smooths the non-linear function of the co-data variable
Zg and where λg is the corresponding smoothing penalty parameter. Hence, the least squares
estimate for the linear co-data model in Equation (3) is extended to the following estimate for
the GAM coeﬃcients in a non-linear co-data model:

g λgSg

(cid:16)(cid:80)

GAM

(cid:17)

(cid:40)

ˆγGAM = argmin

γ

||(C ◦ C)ZGAM γ − b||2

2 +

(cid:41)

λgγT Sgγ

,

G
(cid:88)

g=1

ˆv = (ZGAM ˆγGAM )+.

(4)

This least-squares equation is of a form also known as penalised signal regression (Marx and
Eilers, 1999) and can be solved by the function gam() (or bam() for big data) of the R-package
mgcv, for example. This function also provides fast and stable estimation of the penalties
λg (Wood, 2011). Alternatively, when only one smoothing penalty matrix is provided, the
smoothing penalty may be estimated by using random splits as proposed in van Nee et al.
(2021b).

Remark. Note that grouped co-data may be coded as group sets or as dummies in a co-data
matrix Z. The former option, however, does not allow for a generalised ridge penalty, but for
other penalties including the ordinary ridge and (hierarchical) lasso penalty.

8

2.2.1 Short example in ecpc

We continue with the simulated data from above. First, we use the helper functions
createZforSplines() and createS() to create spline basis matrices and corresponding smooth-
ing penalty matrices respectively. The degree of the spline functions and order of the penalty
matrices are set to 3 and 2 by default, respectively. We set the number of splines to 20 for the
ﬁrst co-data variable and to 30 for the second in this example.

R> Z1.s <- createZforSplines(values=Z1, G=20, bdeg=3)
R> S1.Z1 <- createS(orderPen=2, G=20)
R> Z2.s <- createZforSplines(values=Z2, G=30, bdeg=3)
R> S1.Z2 <- createS(orderPen=2, G=30)

Before we ﬁt the model, we ﬁrst concatenate the two co-data matrices in a list. The variables
of this list are always renamed such that the ith element is named Zi. The smoothing penalty
matrices should be given in a separate argument paraPen, similar to the eponymous argument
in gam(). Each element in this argument’s list should match one of the names Zi, for which the
corresponding smoothing matrix is given in S1 (and optionally S2, S3, et cetera for multiple
smoothing matrices for one co-data matrix).

R> Z.all <- list(Z1=Z1.s, Z2=Z2.s)
R> paraPen.all <- list(Z1=list(S1=S1.Z1), Z2=list(S1=S1.Z2))

Then we ﬁt the model and test it on the test data as follows. Note that an intercept is included
by default:

R> fit.gam <- ecpc(Y, X, Z = Z.all, paraPen = paraPen.all,
+

intrcpt.bam=TRUE, X2=X2, Y2=Y2, postselection=FALSE)

[1] "Estimate global tau^2 (equiv. global ridge penalty lambda)"
[1] "Estimate co-data weights and (if included) hyperpenalties with mgcv"
[1] "Estimate regression coefficients"

R> fit.gam$MSEecpc

[1] 2.472784

The non-linear relation between the prior variance and each co-data source may again be
plotted with the plot() method, either with the co-data spline variables on the x-axis, or the
continuous co-data values on the x-axis. The corresponding output is shown in Figure 3:

R> plot(fit.gam, show="priorweights", Z=Z.all)
R> values <- list(Z1, Z2)
R> plot(fit.gam, show="priorweights", Z=Z.all, values = values)

Figure 3: Example output for plot() in a generalised additive co-data model with the co-data
variables on the x-axis (left two plots) or continuous values on the x-axis (right two plots).

Alternatively, one may plot the non-linear relation directly. The spline variable coeﬃcients are
given in fit$gamma for both co-data matrices and have an attribute codataSource to indicate
for each coeﬃcient to which co-data matrix it belongs. The non-linear relation of one co-data
matrix is then plot as follows (output not shown):

9

−0.050−0.0250.0000.0250.0501234567891011121314151617181920Co−data variablePrior variance weightZ1−0.050−0.0250.0000.0250.050123456789101112131415161718192021222324252627282930Co−data variablePrior variance weightZ2−0.050−0.0250.0000.0250.00.10.20.3Continuous co−data variablePrior variance weightZ1−0.050−0.0250.0000.025−3−2−1012Continuous co−data variablePrior variance weightZ2R> codataNO <- attributes(fit.gam$gamma)$codataSource
R> i <- 2 #1 for informative, 2 for non-informative
R> sk <- as.vector(Z.all[[i]]%*%fit.gam$gamma[codataNO==i])*fit.gam$tauglobal
R> par(mfrow=c(1,1))
R> plot(Z[,i],sk)

2.3 Details for shape-constrained co-data models

Prior assumptions on the shape of the relation between the prior variance and co-data, such as
monotonicity or convexity, may be imposed by constrained optimisation of spline coeﬃcients
(Pya and Wood, 2015). Pya and Wood (2015) develop shape-constrained p-splines to handle
diﬃculties in optimising multiple smoothing penalties due to discontinuous gradients. Their R-
package scam, however, cannot be readily used for signal regression, which diﬀers from regular
regression in that the spline basis matrix is multiplied by the known matrix (C ◦ C). Moreover,
the smoothing parameter estimates are estimated using a generalised cross-validation (GCV)
criterion, which we show below to overﬁt in the unconstrained case. Therefore, we fall back to
the simple approach of directly constraining the spline coeﬃcients.

We use the approach proposed in van Nee et al. (2021b) to estimate the smoothing penalties:
ﬁrst we estimate the smoothing penalties λg separately for each co-data variable Zg using
random splits of the data. As this optimisation is in one dimension only, we can use Brent’s
algorithm from the general purpose optimisation R-package optim, which should be suﬃcient
to handle discontinuous gradients. Then we estimate the spline coeﬃcients γg for each co-data
variable Zg and corresponding spline basis function matrix Φg.
In the general constrained
setting, this estimate is given by subjecting the possible solution to Equation (4) to (in)equality
constraints given in matrix M(in)eq,g and vector b(in)eq,g:

(cid:40) ˆγg = argmin

γ

(cid:8)||(C ◦ C)Φgγ − b||2

2 + λgγT Sgγ(cid:9)

s.t. Mineq,gγ ≤ bineq,g, Meq,gγ = beq,g

(5)

Several shapes may be imposed by choosing Mineq and bineq accordingly (Pya and Wood, 2015):
i) positivity may be imposed by constraining the spline coeﬃcients to be positive; ii) mono-
tonically increasing (decreasing) may be imposed by constraining the ﬁrst order diﬀerences
γi+1 − γi to be positive (negative); iii) convexity (concavity) may be imposed by constraining
second order diﬀerences γi+2 − 2γi+1 + γi to be positive (negative); iv) any combination of the
shapes i-iii may be imposed by combining the corresponding constraints.

Then, given the spline coeﬃcient estimates ˆγg, we combine multiple co-data variables by es-
timating co-data source weights w = (w1, .., wG)T using the same method of moment equation
(van Nee et al., 2021b). For Zw := [Φ1 ˆγ1, .., Φ1 ˆγG]:

ˆw = argmin

w

||(C ◦ C)Zww − b||2
2,

ˆv =

(cid:32) G
(cid:88)

g=1

ˆwgΦg ˆγg

(cid:33)

.

+

(6)

Note that a similar equation is used when multiple co-data matrices Z (1), .., Z (D) are provided.

Remark. Multiple co-data matrices Z (1), .., Z (D) may be provided in a list to the function
ecpc(), or stacked and provided in a list of one co-data matrix Z = [Z (1), .., Z (D)]. When
the function bam() from mgcv is used, multiple smoothing parameters may be used for either
representation, and are estimated jointly. After, the co-data variable weights γ are jointly
estimated for all co-data matrices as well. As a result, the co-data weights w do not need to be
estimated as they are implicitly accounted for in the joint estimate of γ. In contrast, when the
random splitting is used, only one smoothing parameter per co-data matrix may be estimated.
Therefore, the co-data matrix weights are estimated to combine multiple co-data matrices. By
default, the function ecpc() uses bam() when co-data is provided in co-data matrices and no
constraints are provided. This may be changed by setting hypershrinkage="none" when no

10

penalty for the moment estimates is used or to hypershrinkage="ridge" when a generalised
ridge penalty as in Equation (4) is used with random splits for estimating the penalty parameter.
When constraints are provided, the function ecpc() automatically switches to the random splits.

2.3.1 Short example in ecpc

We continue the short example from above for shape-constrained functions. Say we would like
to ﬁnd a positive and monotonically increasing function for the ﬁrst co-data variable, and a
convex function for the second variable. We can use the helper function createCon() to obtain
the constraint matrix Mineq and vector bineq in the desired format for argument paraCon:

R> Con.Z1 <- createCon(G=20, shape="positive+monotone.i")
R> Con.Z2 <- createCon(G=30, shape="convex")
R> paraCon <- list(Z1=Con.Z1, Z2=Con.Z2)

Then we ﬁt the model and plot the estimated shape-constrained functions as follows, with the
output shown in Figure:

R> fit.scam <- ecpc(Y, X, Z = Z.all, paraPen = paraPen.all,
+

paraCon = paraCon, X2=X2, Y2=Y2, postselection=FALSE)

[1] "Estimate global tau^2 (equiv. global ridge penalty lambda)"
[1] "Co-data matrix 1: estimate hyperlambda for ridge+constraints hypershrinkage"
[1] "Estimate weights of co-data source 1"
[1] "Co-data matrix 2: estimate hyperlambda for ridge+constraints hypershrinkage"
[1] "Estimate weights of co-data source 2"
[1] "Estimate co-data source weights"
[1] "Estimate regression coefficients"

R> fit.scam$MSEecpc

[1] 2.490368

R> plot(fit.scam, show="priorweights", Z=Z.all, values=values)

Figure 4: Example output for plot() in a shape constrained additive co-data model.

Note that an intercept is excluded by default, but that it can easily be included by appending

a column of ones to Z.

2.4 Transforming ridge penalties to elastic net penalties

The ﬁrst version of ecpc allows for posterior selection (van Nee et al., 2021b), exempliﬁed in
Section 4. Alternatively, the obtained adaptive ridge penalties may be transformed to elastic
net penalties for simultaneous estimation and variable selection, as explained here.

11

0.000.020.040.060.080.00.10.20.3Continuous co−data variablePrior variance weightZ10.000.020.040.060.08−3−2−1012Continuous co−data variablePrior variance weightZ2In the proposed model in Equation (1), the regression coeﬃcients follow a normal prior
corresponding to a ridge penalty. Now, suppose that each βk independently follows some other
prior distribution π(βk), parameterised by covariate-speciﬁc prior parameter λk and with prior
mean 0 and ﬁnite variance Var(βk) = Zγ = h(λk) for some known monotonic variance function
h(·):

βk

ind.∼ π(βk), E(βk) = 0, Var(βk) = h(λk) = Zkγ.

(7)

As example we consider the elastic net prior, corresponding to the elastic net penalty, with
variable speciﬁc elastic net penalty. Recently, it was shown that when the prior parameters
are group-speciﬁc, the marginal likelihood -as function of λk- is approximately the same as the
marginal likelihood as function of normal prior parameters γ, as the prior distribution of the
linear predictor η = Xβ is asymptotically normally distributed (van Nee et al., 2021a):

π(Y |X, λ) ≈ π(Y |X, γ)

This result also holds for priors with variable speciﬁc, ﬁnite variance Eicker (1966). We may
use this result to obtain approximate method of moment equations for other priors.

Denote by ˆβR(Y ) the ridge penalised maximum likelihood estimate as function of the
observed response data Y . The method of moments equations are given by equating the
theoretical marginal moments to the empirical moments (van Nee et al., 2021b):

EY |λ( ˆβ2

k,R(Y )) = ˆβ2

k,R(Y ), for k = 1, .., p.

Using the normal approximation for the marginal likelihood we obtain:

EY |λ

(cid:16) ˆβ2

k,R(Y )

(cid:17)

=

≈

(cid:90)

Y

(cid:90)

Y

ˆβ2
k,R(Y )π(Y |X, λ)dY

ˆβ2
k,R(Y )π(Y |X, γ)dY = EY |γ

(cid:16) ˆβ2

k,R(Y )

(cid:17)

.

So we may obtain the ridge estimates ˆγ as above to estimate the covariate speciﬁc prior
variances ˆvk = (Zk ˆγ)+, and transform them with the variance function to obtain the covariate
speciﬁc prior parameters:

ˆλk = h−1(ˆvk).

(8)

This transformation could be used to transform the prior variance estimates for the generalised
additive co-data model in Equation (4) and for the shape-constrained co-data model in Equa-
tion (5) too. Note that, however, the penalisation and constraints are applied to γ and not to
λ. So in general, for priors other than the normal prior, the variance function is not linear,
such that the additivity in GAMs is on the prior variance level and not on the transformed
level:

ˆλ = h−1(ˆv) = h−1

(cid:33)

ˆs(Zg)

(cid:54)=

(cid:32)

(cid:88)

g

(cid:88)

g

h−1(ˆs(Zg)),

nor does the transformation of a shape-constrained function, h−1(s(Zk)), necessarily have the
same shape as s(Zk).

2.4.1 Short example for elastic net

We may use the R-package squeezy to transform ridge penalties to elastic net penalties (van
Nee et al., 2021a), in which we use the ﬁt from ecpc() from above. As example, we use the
elastic net parameter α = 0.3, for which we summarise the obtained elastic net penalties and
regression coeﬃcients:

12

R> if(!requireNamespace("squeezy")) install.packages("squeezy")
R> library("squeezy")
R> fit.EN <- squeezy(Y, X, alpha=0.3, X2=X2, Y2=Y2, lambdas=fit$penalties)
R> summary(fit.EN$lambdapApprox) #transformed elastic net penalties

Min. 1st Qu. Median
100.45 520.73

33.19

Mean 3rd Qu.
Inf

Inf

Max.
Inf

R> summary(fit.EN$betaApprox) #fitted elastic net regression coefficients

Min.

1st Qu.
-0.2677508 0.0000000

Median
0.0000000

Mean
0.0008475

3rd Qu.
0.0000000

Max.
0.3351423

3 Simulation study

Estimation and prediction performance have been compared for several methods in van Nee
et al. (2021b). Here, we focus on continuous co-data to exemplify the newly proposed co-data
models. First, we perform a simulation study to compare the estimates of the prior variance
and prediction performance for diﬀerent co-data models proposed here and the adaptive dis-
cretisation proposed in the ﬁrst version of ecpc. Secondly, we perform a simulation study to
compare diﬀerent variable selection methods.

3.1 Estimation and prediction performance of various co-data models

We use the same simulation set-up as in van Nee et al. (2021b) and simulate 50 training and
test data sets for some true vector of regression coeﬃcients β0 ∈ R300. Again, we consider
random and informative co-data, but now continuous versions of it:

1. Random: generate standard normal co-data Zk
2. Informative: use the true regression coeﬃcients to inform the co-data; Zk = |β0
k|.

i.i.d.∼ N (0, 1) for k = 1, .., p.

We compare the following co-data models:

i) ridge: a co-data agnostic, global ridge penalty, corresponding to the co-data intercept
only model. Any co-data method should outperform this baseline method when co-data
is informative, and preferably not lose much when co-data is not informative;

ii) linear: a linear co-data model with an intercept and one (non-)informative co-data

variable;

iii) gam: a generalised additive co-data model using p-splines of degree 3 and with diﬀerence
penalty matrix of second order diﬀerences as suggested in Eilers and Marx (2021). We
use 20 splines and the marginal likelihood method available in bam() from the mgcv
package unless stated otherwise;

iv) scam.p: same as the generalised additive model but with shape constrained to be positive;
v) scam.pmi: same as the generalised additive model but with shape constrained to be

positive and monotonically increasing;

vi) AD: adaptive discretisation of the continuous co-data as proposed in van Nee et al. (2021b).
We use a minimum of 20 variables in the smallest groups, which leads to seven hierarchical
groups.

Figure 5 shows prior variance estimates for diﬀerent co-data models with corresponding
prediction performance shown in Figure 6. As expected, the estimated prior variance is ﬂat
for random co-data and increasing for informative co-data, leading to prediction performance
similar to and better than the co-data agnostic ordinary ridge, respectively. The estimates
of the (constrained) generalised additive co-data models are slightly more non-linear than the
linear estimate, but lead to similar prediction performance. The variance of the estimates
of the constrained generalised additive models in the random co-data reﬂects the eﬀect of
adding constraints, e.g. the estimates vary mostly in the positive direction for the positively
constrained model. The linear and (constrained) generalised additive model slightly outperform
the adaptive discretisation. One advantage of the additive models using p-splines over the

13

Co-data model bam.method G Time (s)
12.7 (4.5)
linear
12.3 (5.6)
gam
13.0 (5.6)
gam
11.6 (5.5)
gam
12.1 (5.8)
gam
12.5 (5.6)
gam
12.9 (5.6)
gam
13.2 (5.5)
gam
16.5 (5.8)
gam
16.5 (11.5)
scam.p
19.8 (10.6)
scam.p
20.1 (17.6)
scam.pmi
26.2 (20.4)
scam.pmi
72.1 (9.8)
AD

none
fREML
fREML
GCV.Cp
GCV.Cp
ML
ML
splits
splits
splits
splits
splits
splits
splits

2
20
50
20
50
20
50
20
50
20
50
20
50
7

Table 1: Results for simulation study based on 50 training and test sets. Average run time and
standard deviation for various co-data models, smoothing parameter estimation methods (as used in
bam() from mgcv or with splits in ecpc) and number of co-data variables.

adaptive discretisation is that the p-splines can estimate local changes on a ﬁner level; while
the adaptive discretisation method is limited to discretisations in which each discretised group
contains at least one variable (in our case, at least 20 variables per discretised group), this is
not needed for p-splines, as they are penalised with a diﬀerence penalty. To illustrate, Figure
A1a in Appendix A shows the generalised additive model estimates in one training data set
when G = 20 or G = 50 splines are used and when the diﬀerence penalty is estimated with
one of the methods provided in the R-function bam() or with the random splits as used in the
ﬁrst version of ecpc. Except for the generalised cross-validation criterion GCV.Cp, the estimates
and corresponding prediction performance seem to be robust for the number of splines (see
also Figures A2 and A1b in Appendix A). Finally, Table 1 shows the average run times of the
methods. The adaptive discretisation is around 3-6 times as slow as the (constrained) additive
co-data models.

3.2 Variable selection compared to other methods

We alter the simulation set-up from above for variable selection. We now set 250 regression
coeﬃcients to 0, leaving 50 non-zero coeﬃcients. We scale the regression coeﬃcients such that
the L2-norm of β0 and the scaled, sparse β0,s are the same. We use the following co-data:

1. Random: as in the simulation study above, so Z (1)
2. Informative+monotone: as in the simulation study above, but with white noise added
ind.∼ N (|β0,s
k |, σ2
0), for
k |2 is (up till some

such that the co-data is not exactly 0 for the zero coeﬃcients, Z (2)
σ0 a tenth of the sample standard deviation of β0,s. The eﬀect size |β0,s
noise) a monotone, quadratic function of the co-data.

i.i.d.∼ N (0, 1) for k = 1, .., p.

k

k

3. Informative+convex: similar to the Informative co-data but distinguishing negative
k ) · Z (2)
k . The eﬀect size |βk|2 is now (up till some

from positive eﬀects by Z (3)
noise) not monotone but a convex, quadratic function of the co-data.

k = sign(β0,s

We compare the following variable selection methods:

i) glmnet: a co-data agnostic elastic net model ﬁtted with glmnet (Friedman et al., 2010).
ii) fwelnet: an elastic net model with continuous co-data, ﬁtted with fwelnet (Tay et al.,
2020). The elastic net penalties are a ﬁxed, exponential function of the co-data weights.
iii) ecpc+squeezy: a GAM for the co-data ﬁtted with ecpc, followed by a transformation of
the ridge penalties to elastic net penalties by squeezy (van Nee et al., 2021a) for variable
selection.

14

Figure 5: Simulation study based on 50 training and test sets and random co-data (left) or informative
co-data (right). Estimated prior variance for various co-data models. The lines indicate the pointwise
median and the inner and outer shaded bands indicate the 25-75% and 5-95% quantiles respectively.
Points indicate the true (β0

k)2.

15

RandomInformativelineargamscam.pscam.pmiAD−20240.000.250.500.751.00−0.50.00.51.01.5−0.50.00.51.01.5−0.50.00.51.01.5−0.50.00.51.01.5−0.50.00.51.01.5Continuous co−data variablePrior varianceFigure 6: Simulation study based on 50 training and test sets and random co-data (left) or informative
co-data (right). Boxplots of the MSE of the predictions on the test sets for various co-data models.

iv) ecpc+postselection: a GAM for the co-data, using the default option for posterior

selection provided in the ecpc software.

The ﬁrst three methods have one additional tuning parameter, the elastic net parameter α ∈
[0, 1], with 0 corresponding to the full model and 1 to the lasso model. The last method has
one tuning parameter, the number of selected covariates (or equivalently, the proportion of
estimated zero eﬀects), ranging from 300 to 0 (0 to 1) for the full model to the most sparse
model. In practice, one may choose one value from a range of values for the tuning parameter
by comparing predictive performances and selecting the sparsest model that performs (nearly)
optimal.

Figure 7 shows the performance of the methods in variable selection and prediction er-
ror on the test data. Note that ecpc+postselection may be tuned to select sparse models
up to a model that is almost empty, reaching a sensitivity and 1-precision of 0.
In con-
trast, the models selected by the other methods still contain more variables, even in the most
sparse models for tuning parameter α = 1. Besides, ecpc+squeezy and ecpc+postselection
do not always select a full model, explaining why the sensitivities do not reach 1. This is
a result from ecpc() truncating estimated negative prior variances to 0, deselecting some
variables a priori. In the sparse setting, the co-data agnostic glmnet outperforms the other
methods both in terms of variable selection and prediction performance when the co-data is
random. This in contrast to the dense simulation setting, in which the prediction performance
of glmnet and gam were on par (Figure 6). For the informative+monotone co-data, fwelnet
slightly outperforms ecpc+squeezy and ecpc+postselection, all outperforming glmnet. For
the informative+convex co-data, however, fwelnet is not able to ﬂexibly adapt to the
convex shape of the co-data, while the ﬂexible GAM for the co-data in ecpc+squeezy and
ecpc+postselection still adequately exploits the co-data.

4 Analysis example

We demonstrate the diﬀerent co-data models by applying the method to an application in
classifying lymph node metastasis (coded 1) from other types of cancer (coded 0). We use the
data readily available from the R-package CoRF, providing high-dimensional RNA expression
training data for p = 12838 probes and n = 133 patients, and validation data for n2 = 97
patients. First we install and load the package. Then we load the data and transform the

16

RandomInformativeridgelineargamscam.pscam.pmiADridgelineargamscam.pscam.pmiAD1015202530Co−data modelMSE(a)

(b)

Figure 7: Simulation study for variable selection based on 50 training and test sets for various types
of co-data. a) Average sensitivity and precision for several methods and various tuning parameters; b)
Mean squared error prediction performance on the test data. The lines indicate the pointwise average
and the inner and outer shaded bands indicate the 25-75% and 5-95% quantiles respectively.

17

RandomInformative+monotoneInformative+convex0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.001−precisionSensitivityMethodglmnetfwelnetecpc+squeezyecpc+postselectionType tuning parameteralphaproportion zerosRandomInformative+monotoneInformative+convex0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000102030Tuning parameterMSEMethodglmnetfwelnetecpc+squeezyecpc+postselectionType tuning parameteralphaproportion zerosresponse of the validation data set to match the format of the training data:

R> if(!requireNamespace("devtools")) install.packages("devtools")
R> library("devtools")
R> install_github("DennisBeest/CoRF")
R> library("CoRF")
R> data("LNM_Example")
R> RespValidationNum <- as.numeric(RespValidation)-1

The data provide three diﬀerent sources of co-data:

1. Signature: a published signature of genes. Probes either match a gene in the signature

or not.

2. Correlation: cis-correlations between RNA expression and copy number.
3. P-values: p-values from an external, similar study, using a diﬀerent technique to measure

RNA expression.

First, we prepare the co-data. The ﬁrst co-data source is categorical. We use the helper
functions createGroupset() and createZforGroupset() to transform the vector of categories
to a group set and co-data matrix:

R> GroupsetSig <- createGroupset(as.factor(CoDataTrain$RoepmanGenes))

[1] "Summary of group sizes:"

0
12324

1
514

R> Z_sig <- createZforGroupset(GroupsetSig)

The second co-data source with correlations is continuous. We use 20 splines to ﬂexibly
model the relation between the prior variance and the correlations. Furthermore, we constrain
the relation to be positively monotonically increasing, as we expect larger correlations to be of
more importance. The co-data spline basis matrix, diﬀerence penalty matrix and constraints
are obtained by:

R> G <- 20 #number of splines
R> Z_cor <- createZforSplines(values=CoDataTrain$Corrs, G=G)
R> S1_cor <- createS(orderPen=2, G=G)
R> Con_cor <- createCon(G=G, shape="positive+monotone.i")

We prepare the co-data with p-values similarly to the correlations, but constrain the re-
lation to be positively monotonically decreasing, as we expect smaller p-values to be of more
importance. We set the p-value of two variables that have missing p-values to the maximum
observed p-value and compute the co-data (related) matrices:

max(CoDataTrain$pvalsVUmc, na.rm=TRUE)

R> CoDataTrain$pvalsVUmc[is.na(CoDataTrain$pvalsVUmc)] <-
+
R> Z_pvals <- createZforSplines(values=CoDataTrain$pvalsVUmc, G=G)
R> S1_pvals <- createS(G=G)
R> Con_pvals <- createCon(G=G, shape="positive+monotone.d")

As the last step of the preparation of the co-data, we save the continous co-data variables in a
separate list for the plot() function that we use below, and concatenate the co-data matrices
in a list:

R> values <- list("Signature" = NULL,
+
+
R> Z_all <- list("Signature" = Z_sig,
+
+

"Correlation" = Z_cor,
"P-values" = Z_pvals)

"Correlation" = CoDataTrain$Corrs,
"P-values" = CoDataTrain$pvalsVUmc)

18

Then we ﬁt the model and select variables a posteriori, with a range of the maximum number
of variables that should be selected deﬁned in the input argument maxsel:

R> set.seed(3)
R> maxSel <- c(2:10,10*(2:10)) #maximum posterior selected variables
R> Res<-ecpc(Y=RespTrain, X=TrainData, Z=Z_all,
+
+
+
+

paraPen = list(Z2=list(S1=S1_cor), Z3=list(S1=S1_pvals)),
paraCon = list(Z2=Con_cor, Z3=Con_pvals),
Y2=RespValidationNum, X2=ValidationData,
maxsel=maxSel)

[1] "Estimate global tau^2 (equiv. global ridge penalty lambda)"
[1] "Co-data matrix 1: estimate weights, hypershrinkage type: none"
[1] "Co-data matrix 2: estimate hyperlambda for ridge+constraints hypershrinkage"
[1] "Estimate weights of co-data source 2"
[1] "Co-data matrix 3: estimate hyperlambda for ridge+constraints hypershrinkage"
[1] "Estimate weights of co-data source 3"
[1] "Estimate co-data source weights"
[1] "Estimate regression coefficients"
[1] "Sparsify model with posterior selection"

We plot the contributions from each co-data source, shown in Figure 8:

R> plot(Res, show="priorweights", Z=Z_all, values=values)

Figure 8: Data analysis example. Figure produced by the plot()-method applied on the ecpc() ﬁt.

The predicted values for the validation data are given in Res$Ypred as X2 was provided to
ecpc(). Alternatively, the predictions may be retrieved with the method predict():

R> Ypred <- predict(Res, X2=ValidationData)

The posterior selected variables are given in Res$betaPost as maxsel was provided to ecpc().
Alternatively, the same posterior selection method may be performed with the function postSelect()
on the ﬁtted ‘ecpc’-object Res:

R> sparseModels <- postSelect(Res, X=TrainData, Y=RespTrain, maxsel=maxSel)

A second approach for variable selection is to use squeezy() to transform the ridge penalties
to elastic net penalties, with elastic net tuning parameter α. As mentioned above, in practice
one may try a range of tuning parameters to choose the sparsest model with close to optimal
performance. For this example, we include the lasso model (α = 1):

R>
+
+

sparseModel2 <- squeezy(Y=RespTrain, X=TrainData, alpha=1,

lambdas=Res$penalties,
X2=ValidationData, Y2=RespValidationNum)

19

0.00000.00250.00500.0075inoutCo−data variablePrior variance weightSignature0.00000.00250.00500.0075−0.250.000.250.50Continuous co−data variablePrior variance weightCorrelation0.00000.00250.00500.00750.000.250.500.751.00Continuous co−data variablePrior variance weightP−valuesInstead of ﬁtting monotone and positively constrained functions for the correlation and
p-values co-data, one could consider other co-data models. Figure 9 shows the results for
three diﬀerent settings: 1) a GAM, i.e. without constraints; 2) a SCAM with positivity con-
straints; 3) a SCAM with positivity and monotonicity constraints, as in the exempliﬁed code
above. The results include the dense models obtained with ecpc() and a co-data agnostic or-
dinary ridge model, and sparse models for a range of posterior selected variables obtained with
postSelect(), the lasso model obtained with transformed penalties from squeezy() and a
co-data agnostic lasso model. The estimated prior variance contribution for the correlation co-
data shows large deviations near the boundaries, which increase when 50 instead of 20 splines
are used (Setting 1). While p-splines have no boundary eﬀects in regular regression models
(Eilers and Marx, 1996), these eﬀects may be the result from the signal regression nature of the
model used in Equation (4). To dampen the boundary eﬀects, it may be beneﬁcial to transform
the co-data values such that the values spread out more evenly, e.g. using the empirical cumu-
lative distribution function. Fitting a co-data model with positive (and monotone) constraints
(Setting 2 and 3) results in smoother functions than when it is ﬁt without constraints. While
adding the constraints stabilises posterior selection for highly sparse models, it generally does
not beneﬁt the prediction performance when compared to the GAM co-data model. The GAM
co-data model results in the best prediction performance among sparse models, though, in
practice, the simpler lasso model may be preferred as it shows competitive performance. The
overall best prediction performance on the test data is retrieved by the full, dense model when
the (unconstrained) generalised additive co-data model is used with 50 splines.

5 Conclusion

We presented an extension to the R-package ecpc that accommodates linear co-data models,
generalised additive co-data models and shape constrained additive co-data models for the
purpose of high-dimensional prediction and variable selection. These co-data models are par-
ticularly useful for continuous co-data, for which an adaptive discretisation was available in the
ﬁrst version. The newly proposed co-data models are shown to run faster and lead to slightly
better prediction performance when compared to the ﬁrst version in a simulation study. More-
over, the estimated variable-speciﬁc ridge penalties may be transformed to elastic net penalties
with the R-package squeezy to allow for variable selection. We showed in a simulation study
that this approach and the previously proposed posterior selection approach lead to similar per-
formance, outperforming other methods when the eﬀect sizes are (non-exponentially) related
to the co-data. We have provided several short examples and one analysis example to a cancer
genomics application to demonstrate the code. Stand-alone R-scripts and other code ﬁles used
for the simulations and examples may be found on https://github.com/Mirrelijn/ecpc.

Acknowledgements

The ﬁrst author is supported by ZonMw TOP grant COMPUTE CANCER (40- 00812-98-
16012). The authors would like to thank Souﬁane Mourragui (Netherlands Cancer Insitute)
for the many worthwhile discussions.

References

Anne-Laure Boulesteix, Riccardo De Bin, Xiaoyu Jiang, and Mathias Fuchs.

Ipf-lasso:
Integrative-penalized regression with penalty factors for prediction based on multi-omics
data. Comput. Math. Method. M., 2017, 2017.

F Eicker. A multivariate central limit theorem for random linear vector forms. Ann. Math.

Stat., pages 1825–1828, 1966.

20

(a)

(b)

Figure 9: Data analysis example: a) Estimated prior variance contributions of each co-data source,
before multiplying with the co-data speciﬁc weight. Note that the p-values are shown on the log-scale
in Settings 2 and 3, to clearly show the non-zero peaks at the smallest p-values; b) corresponding
prediction performance on the validation set for 20 or 50 spline basis functions. The settings corre-
spond to diﬀerent co-data models: 1) no constrains; 2) positive constrained shape; 3) positive and
monotonically constrained shape.

21

P−valuesSetting 1P−valuesSetting 2P−valuesSetting 3CorrelationSetting 1CorrelationSetting 2CorrelationSetting 30.000.250.500.751.001e−061e−041e−021e+001e−061e−041e−021e+00−0.250.000.250.50−0.250.000.250.50−0.250.000.250.500.0000.0010.0020.0030.0000.0010.0020.00000.00250.00500.00750.01000.0000.0010.002−0.050.000.050.100.15−0.010.000.01Continuous co−data variablePrior variance# Splines2050            Setting 1Setting 2Setting 320500255075100025507510002550751000.500.550.600.650.700.750.500.550.600.650.700.75# variablesAUCMethod    ecpcecpc+postSelectecpc+squeezylassoordinary.ridgePaul H. C. Eilers and Brian D. Marx. Flexible smoothing with b-splines and penalties. Statis-
tical Science, 11(2):89–102, 1996. ISSN 08834237. URL http://www.jstor.org/stable/
2246049.

Paul HC Eilers and Brian D Marx. Practical Smoothing: The Joys of P-splines. Cambridge

University Press, 2021.

Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized

linear models via coordinate descent. J. Stat. Softw., 33(1):1, 2010.

Trevor Hastie and Robert Tibshirani. Generalized additive models. Stat. Sci., 1(3):297–318,

1986.

Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthogonal

problems. Technometrics, 12(1):55–67, 1970.

Nikolaos Ignatiadis and Panagiotis Lolas. σ-ridge: group regularized ridge regression via em-

pirical bayes noise level cross-validation. arXiv preprint arXiv:2010.15817, 2020.

Alboukadel Kassambara. Package ggpubr. R package version 0.1, 6, 2020.

Brian D Marx and Paul HC Eilers. Generalized linear regression on sampled signals and curves:

a p-spline approach. Technometrics, 41(1):1–13, 1999.

P McCullagh and JA Nelder. Generalized Linear Models II. Chapman and Hall, London, 1989.

L. Meier, S. van de Geer, and P. B¨uhlmann. The group Lasso for logistic regression. J. R.

Stat. Soc. Ser. B Stat. Methodol., 70(1):53–71, 2008. ISSN 1369-7412.

Magnus M M¨unch, Carel FW Peeters, Aad W van der Vaart, and Mark A van de Wiel. Adaptive
group-regularized logistic elastic net regression. Biostatistics, 12 2019. ISSN 1465-4644. doi:
10.1093/biostatistics/kxz062. kxz062.

Natalya Pya and Simon N Wood. Shape constrained additive models. Stat. Comput., 25(3):

543–559, 2015.

J Kenneth Tay, Nima Aghaeepour, Trevor Hastie, and Robert Tibshirani.

Feature-
weighted elastic net: using “features of features” for better prediction. arXiv preprint
arXiv:2006.01395, 2020.

Robert Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B

Stat. Methodol., pages 267–288, 1996.

M.A. van de Wiel, T.G. Lien, W. Verlaat, W.N. van Wieringen, and S.M. Wilting. Better
prediction by use of co-data: adaptive group-regularized ridge regression. Stat. Med., 35:
368–381, 2016.

Mark A van de Wiel, Mirrelijn M van Nee, and Armin Rauschenberger. Fast cross-validation
for multi-penalty high-dimensional ridge regression. J. Comput. Graph. Stat., pages 1–13,
2021.

Mirrelijn M van Nee, Tim van de Brug, and Mark A van de Wiel. Fast marginal likelihood
estimation of penalties for group-adaptive elastic net. arXiv preprint arXiv:2101.03875,
2021a.

Mirrelijn M van Nee, Lodewyk FA Wessels, and Mark A van de Wiel. Flexible co-data learning

for high-dimensional prediction. Stat. Med., 40(26):5910–5925, 2021b.

Britta Velten and Wolfgang Huber. Adaptive penalization in high-dimensional regression and
classiﬁcation with external covariates using variational bayes. Biostatistics, 10 2019. ISSN
1465-4644. doi: 10.1093/biostatistics/kxz034. kxz034.

22

Grace Wahba. Spline bases, regularization, and generalized cross validation for solving approx-
imation problems with large quantities of noisy data. University of WISCONSIN, 1980.

Hadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York,

2016. ISBN 978-3-319-24277-4. URL https://ggplot2.tidyverse.org.

Simon N Wood. Fast stable restricted maximum likelihood and marginal likelihood estimation
of semiparametric generalized linear models. J. R. Stat. Soc. Ser. B Stat. Methodol., 73(1):
3–36, 2011.

Yi Yang and Hui Zou. A fast uniﬁed algorithm for solving group-lasso penalize learning

problems. Stat. Comput., 25(6):1129–1141, 2015.

Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. J. R.

Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005.

23

A Additional ﬁgures to simulation study

Figures A1 and A2 show the results for generalised additive co-data models when diﬀerent
smoothing parameter methods are used.

24

(a)

(b)

Figure A1: Simulation study based on 50 training and test sets and random co-data (left) or in-
formative co-data (right). a) Example of estimated prior variance for various smoothing parameter
estimation methods in one training data set; b) boxplots of the MSE of the predictions on the test sets
for the ordinary ridge model (G = 1 co-data intercept variable) and for a generalised additive co-data
model using various smoothing parameter estimation methods and G = 20 or 50 splines.

25

RandomInformative−20240.000.250.500.751.000.00.40.81.2Continuous co−data variablePrior variancebam.methodfREMLGCV.CpMLsplitsG2050RandomInformativeridgeMLfREMLGCV.CpsplitsridgeMLfREMLGCV.Cpsplits1015202530MSEG12050Figure A2: Simulation study based on 50 training and test sets and random co-data (left) or in-
formative co-data (right). Estimated prior variance for the generalised additive co-data model using
various smoothing parameter estimation methods. The lines indicate the pointwise median and the
inner and outer shaded bands indicate the 25-75% and 5-95% quantiles respectively. Points indicate
the true (β0

k)2.

26

RandomInformativefREMLGCV.CpMLsplits−20240.000.250.500.751.000.00.51.01.52.00.00.51.01.52.00.00.51.01.52.00.00.51.01.52.0Continuous co−data variablePrior variance