2
2
0
2

y
a
M
3
1

]
E
S
.
s
c
[

1
v
7
3
5
6
0
.
5
0
2
2
:
v
i
X
r
a

Productivity Assessment of Neural Code Completion

Albert Ziegler, Eirini Kalliamvakou, Shawn Simister, Ganesh Sittampalam, Alice Li, Andrew Rice,
Devon Rifkin, and Edward Aftandilian
{wunderalbert,ikaliam,narphorium,hsenag,xalili,acr31,drifkin,eaftan}@github.com
GitHub, Inc.
USA

Abstract
Neural code synthesis has reached a point where snippet gen-
eration is accurate enough to be considered for integration
into human software development workflows. Commercial
products aim to increase programmers’ productivity, with-
out being able to measure it directly. In this case study, we
asked users of GitHub Copilot about its impact on their pro-
ductivity, and sought to find a reflection of their perception
in directly measurable user data. We find that the rate with
which shown suggestions are accepted, rather than more spe-
cific metrics regarding the persistence of completions in the
code over time, drives developers’ perception of productivity.

CCS Concepts: • Software and its engineering → Au-
tomatic programming; • Information systems → Lan-
guage models.

Keywords: code synthesis, code completion, neural networks,
productivity

ACM Reference Format:
Albert Ziegler, Eirini Kalliamvakou, Shawn Simister, Ganesh Sittam-
palam, Alice Li, Andrew Rice, Devon Rifkin, and Edward Aftandil-
ian. 2022. Productivity Assessment of Neural Code Completion. In
Proceedings of the 6th ACM SIGPLAN International Symposium on
Machine Programming (MAPS ’22), June 13, 2022, San Diego, CA,
USA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/
3520312.3534864

1 Introduction
Code completion systems that offer suggestions to a devel-
oper on the basis of contextual information from the IDE
have been shown to be by far the most frequently used kind
of programmer assistance [1]. One common example is that
of proposing a list of method names based on the type of
a variable. Neural code synthesis approaches to code com-
pletion generate suggestions by using a language model to
predict what the user might type next (the completion) from

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
MAPS ’22, June 13, 2022, San Diego, CA, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9273-0/22/06.
https://doi.org/10.1145/3520312.3534864

the context of what they are working on at the moment (the
prompt) [2]. Rather than focusing on a particular task (such
as suggesting a method to call), neural code synthesis pre-
dicts arbitrary sections of code, and rather than generating
single tokens, these systems might predict multiple lines of
code at once.

The potential benefits of generating large sections of code
automatically are huge, but evaluating these systems is chal-
lenging. Offline evaluation where the system is shown a snip-
pet of code with (say) an identifier removed and then asked
to complete it is difficult not least because for longer comple-
tions there are many acceptable alternatives and no straight-
forward mechanism for labeling them automatically [5]. An
additional step taken by some researchers [3, 13, 21] is to
use online evaluation and track the frequency of real users
accepting suggestions, assuming that the more contributions
a system makes to the developer’s code, the higher its ben-
efit. The validity of this assumption is not obvious when
considering issues such as whether two short completions
are more valuable than one long one or other human fac-
tors such as whether reviewing suggestions is detrimental
to programming flow.

Neural synthesis tools such as GitHub Copilot1, Kite2,
and TabNine3 suggest code snippets within an IDE with the
explicitly stated intention to increase a user’s productivity.
Developer productivity has many aspects, and a recent study
has shown that tools like these are helpful in ways that are
only partially reflected by measures like completion times for
standardized tasks [15]. Alternatively, we can leverage the
developers themselves as expert assessors of their own pro-
ductivity. This meshes well with current thinking in software
engineering research which suggests measuring productiv-
ity on multiple dimensions and using self-reported data [6].
We will thus focus on studying perceived productivity.

In this paper we investigate whether usage measurements
of developer interactions with GitHub Copilot can be used
to predict perceived productivity as reported by developers.
We analyze 2,631 survey responses from developers using
GitHub Copilot and match their responses to usage mea-
surements collected from the IDE. We consider acceptance
counts and more detailed measures of contribution such as
the amount of code contributed by GitHub Copilot, and the

1https://copilot.github.com/
2https://www.kite.com
3https://tabnine.com

 
 
 
 
 
 
MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

rate of acceptances which subsequently persist in the code
unchanged. We find that acceptance rate of shown sug-
gestions is a better predictor of perceived productivity
than the alternative measures. We also find that accep-
tance rate varies significantly over our developer population
as well as over time, and present a deeper dive into some of
these variations.

Our results support the principle that acceptance rate can
be used for coarse-grained monitoring of the performance
of a neural code synthesis system. In particular, the ratio
of shown suggestions being accepted correlates better than
more detailed measures of contribution. However, other ap-
proaches remain necessary for fine-grained investigation
due to the many human factors involved.

2 Background
Offline evaluation of code completion can have shortcom-
ings even in tractable circumstances where completions can
be labeled for correctness. For example, a study of 15,000
completions by 66 developers in Visual Studio found signif-
icant differences between synthetic benchmarks used for
model evaluation and real-world usage [7]. The evaluation
of context-aware API completion for Visual Studio Intelli-
Code considered Recall@5—the proportion of completions
for which the correct method call was in the top 5 sugges-
tions. This metric fell from 90% in offline evaluation to 70%
when used online [13].

Due to the diversity of potential solutions to a multi-line
completion task, researchers have used software testing to
evaluate the behaviour of completions. Competitive program-
ming sites have been used as a source of such data [8, 9] as
well as hand-written programming problems [5]. Human-
written tests can be augmented with automatically gener-
ated tests to reduce false-positive rates in which erroneous
programs are accepted [10]. Online evaluation remains im-
portant for general code completion tools because one needs
to understand how well performance on programming com-
petition data generalizes to interactive development in an
IDE.

In this work we define acceptance rate as the fraction of
completions shown to the developer that are subsequently
accepted for inclusion in the source file. The IntelliCode
Compose system uses the term CTR (Click Through Rate)
for this and reports a value of 10% in online trials [12]. An
alternative measure is that of DCPU (Daily Completions ac-
cepted Per User) for which a value of around 20 has been
reported [3, 21]. To calculate acceptance rate one must, of
course, normalize DCPU by the time spent coding each day.
For context, in our study GitHub Copilot has an acceptance
rate of 27% and a mean DCPU in excess of 31. These dif-
ferences are presumably due to differences in the kinds of
completion offered, or perhaps to user interface choices. We

discuss later how developer objectives, choice of program-
ming language and even time of day seem to affect our data.
Such discrepancies highlight the difficulty in using accep-
tance rate to understand the value of a system.

There is some evidence that acceptance rate (and indeed
correctness) might not tell the whole story. One survey of
developers considered the use of AI to support translation
between programming languages and found indications that
developers tolerated, and in some cases valued, erroneous
suggestions from the model [18].

Measuring developer productivity through activity counts
over time (a typical definition of productivity borrowed from
economics) disregards the complexity of software develop-
ment as they account for only a subset of developer out-
puts. A more holistic picture is formed by measuring per-
ceived productivity through self-reported data across various
dimensions [6], and supplementing it with automatically
measured data [4]. In our investigation we used the SPACE
framework [6] to design a survey that captures self-reported
productivity, and paired the self-reported data with usage
telemetry.

To the best of our knowledge, this is the first study of
code suggestion tools establishing a clear link between us-
age measurements and developer productivity or happiness.
A previous study comparing GitHub Copilot against Intelli-
Code with 25 participants found no significant correlation
between task completion times and survey responses [14].
Another study considered the benefits of using a plugin con-
verting natural language prompts to code [20]. It found no
statistically significant improvements in task completion
time or task correctness despite positive qualitative survey
results (possibly due to small sample size).

3 Data and Methodology
3.1 Usage Measurements

GitHub Copilot provides code completions using OpenAI
Codex [5] , which is a version of GPT-3 that has been tuned
on publicly available source code. It runs within the IDE and
at appropriate points sends a completion request to a cloud-
hosted instance of the neural model. Completion requests
contain a prompt drawn from the code currently in the IDE.
GitHub Copilot can generate completions at arbitrary points
in code rather than (say) only being triggered when a de-
veloper types a period for invoking a method on an object.
We use a variety of rules to determine appropriate points to
request a completion; to abandon requests if the developer
has moved on before the model is ready with a completion;
and to determine how much of the response from the model
to surface as a completion.

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

Table 1. Developer usage events collected by GitHub Copilot.

opportunity

shown
accepted
accepted_char
mostly_unchanged_X

unchanged_X
(active) hour

a heuristic-based determination by the IDE and the plugin that a completion might be appropriate
at this point in the code (e.g. the cursor is not in the middle of a word)
completion shown to the developer
completion accepted by the developer for inclusion in the source file
the number of characters in an accepted completion
completion persisting in source code with limited modifications (Levenshtein distance less than
33%) after X seconds, where we consider a duration of 30, 120, 300, and 600 seconds
completion persisting in source code unmodified after X seconds.
an hour during which the developer was using their IDE with the plugin active

Table 2 defines the a core set of metrics which we feel
have a natural interpretation in this context. We note that
there are other alternatives here and we incorporate these
in our discussion where relevant.

3.2 Productivity Survey

To understand users’ experience with GitHub Copilot, we
emailed 17,420 users providing them with a link to complete
an online survey. These were participants of the unpaid
technical preview using GitHub Copilot with their everyday
programming tasks. The only selection criterion was having
previously opted in to receive communications. Between
10th February 2022 and 6th March 2022, we received 2,047
responses we could match to usage measurements during the
4 week period leading up to February 12th March 2022. We
focus on usage data from this period, since the vast majority
(> 80%) of survey users had filled out their survey by then.
The survey questions contained multiple choice questions,
in particular regarding demographic information (shown in
Figure 2) and Likert-style questions about different aspects
of productivity, which were randomized in their order of
appearance to the user. Figure 2 shows the demographic
composition of our respondents. We note the significant
proportion of professional programmers who responded.

The SPACE framework [6] defines 5 dimensions of produc-
tivity: Satisfaction and well-being, Performance, Activity,
Communication and Collaboration, Efficiency and Flow. We
use 4 of these (S,P,C,E) since self reporting on (A) is gener-
ally considered inferior to direct measurement. We included
11 statements covering these 4 dimensions in addition to a
single statement “I am more productive when using GitHub
Copilot”. For each self-reported productivity measure, we
encoded its five ordinal response values to numeric labels (1
= Strongly Disagree, . . ., 5 = Strongly Agree). We include the
full list of questions and their coding to the SPACE frame-
work in Appendix C.

Early in our analysis we found that the usage metrics
we describe in Section 3.1 below corresponded similarly to
each of the measured dimensions of productivity, and in turn
these dimensions were highly correlated to each other (for
details see Figure 3 and Appendix B). We therefore added an

Figure 1. GitHub Copilot’s code completion funnel.

We make usage measurements for each developer by count-
ing the events shown in Table 1, collected for all users of
GitHub Copilot according to our terms of usage.4

Our measures of persistence go further than existing work
which stops at acceptance. The intuition here is that a com-
pletion which is accepted into the source file but then sub-
sequently turns out to be incorrect con be considered to
have wasted developer time both in reviewing it and then
having to go back and delete it again. We also record mostly
unchanged completions, reasoning that a large completion
requiring a few edits might still be a positive contribution. It
is not clear how long after acceptance one should confirm
persistence and so we consider a range of options.

The events pertaining to completions form a funnel which
we show quantitatively in Table 1. We include a summary of
all data in Appendix A.

We normalize these measures against each other and write
X_per_Y to indicate we have normalized metric X by metric Y.
For example: accepted_per_hour is calculated as the total
number of accepted events divided by the total number of
(active) hour events.

4https://docs.github.com/en/github/copilot/github-copilot-telemetry-
terms

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

Table 2. The core set of measurements considered in this paper.

Natural name
Shown rate

Acceptance rate

Persistence rate

Fuzzy persistence rate

Efficiency

Contribution speed

Acceptance frequency

Persistence frequency

Total Volume
Loquaciousness

Eagerness

Explanation
Percentage of completion opportunities that
resulted in a completion being shown to the
user
Percentage of shown completions accepted by
the user
Percentage of accepted completions unchanged
after 30, 120, 300, and 600 seconds
Percentage of accepted completions mostly un-
changed after 30, 120, 300, and 600 seconds
Percentage of completion opportunities that re-
sulted in a completion accepted and unchanged
after 30, 120, 300, and 600 seconds
Number of characters in accepted completions
per distinct, active hour
Number of accepted completions per distinct,
active hour
Number of unchanged completions per distinct,
active hour
Total number of completions shown to the user
Number of shown completions per distinct, ac-
tive hour
Number of shown completions per opportunity

Definition
shown_per_opportunity

accepted_per_shown

unchanged_X_per_accepted

mostly_unchanged_X_per_accepted

accepted_X_per_opportunity,
unchanged_X_per_opportunity

accepted_char_per_hour

accepted_per_hour

unchanged_X_per_hour

shown
shown_per_hour

shown_per_opportunity

measurements considered in this article is available at https:
//github.com/wunderalbert/prod-neural-materials.

4 What Drives Perceived Productivity?
To examine the relationship between objective measure-
ments of user behavior and self-reported perceptions of pro-
ductivity, we used our set of core usage measurements (Ta-
ble 2). We then calculated Pearson’s R correlation coefficient
and the corresponding p-value of the F-statistic between
each pair of usage measurement and perceived productivity
metric. Next, we computed a PLS regression from all us-
age measurements jointly. Finally, we perform incremental
feature selection by analyzing the significance of a univari-
ate model where each usage measurement seeks to predict
the residuals of a model fit with varying numbers of other
metrics; this allows us to more directly rank each metric.

We summarize these results in Figure 3 showing the cor-
relation coefficients between all measures and survey ques-
tions. The full table of all results is included in Appendix B.
Across all three analyses, we find that acceptance
rate (accepted_per_shown) most positively predicts users’
perception of productivity, although, given the con-
founding and human factors, there is still notable un-
explained variance.

Of all usage measurements, acceptance rate correlates
best with aggregate productivity (𝜌 = 0.24, 𝑃 < 0.0001).
This measurement is also the best performing for at least

Figure 2. Demographic composition of survey respondents.

aggregate productivity score calculated as the mean of all
12 individual measures (excluding skipped questions). This
average can only serve as a rough proxy for the much more
complex concept of productivity, but facilitates recognition
of overall trends, which may be less discernible on individual
variables due to higher statistical variation.

For reproducibility and transparency, the full data set of
these aggregate productivity scores together with the usage

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

Figure 3. Correlation between metrics.
Metrics are ordered by similarity based on distance in the correlation matrix, except for manually fixing the aggregate productivity and
acceptance rate at the end for visibility.

one survey question in each of the SPACE dimensions. This
correlation is high confidence but leaves considerable un-
explained variance. Below we explore improvements from
combining multiple usage measurements together.

Looking at the more detailed metrics around persistence,
we see that persistence over shorter time periods is generally
better than over longer periods. This is intuitive in the sense
that shorter periods move the measure closer to acceptance
rate. We also expect that at some point after accepting the
completion it becomes simply part of the code and so any

changes (or not) after that point will not be attributed to
GitHub Copilot. All persistence measures were less well
correlated than acceptance rate.

In order to assess the different metrics in a single model in
a way that is still robust against their strong collinearity and
unaffected by the decision whether or not to include highly
similar metrics, we ran a regression using projection on
latent structures (PLS), which captures the common variation
of these variables as is linearly connected to the aggregate
productivity [19]. The first component, to which every metric

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

Table 3. Incremental benefit of additional metrics.

p-value univ. coef.

accepted_per_shown
+ shown_per_hour
+ accepted_char_per_hour
+ shown_per_opportunity
+ accepted_per_hour
accepted_per_opportunity

+ accepted_char_per_hour
+ accepted_per_hour
+ unchanged_30_per_hour
+ accepted_per_shown

<0.0001
0.04
0.04
0.04
0.05
<0.0001
<0.0001
0.01
0.01
0.02

0.13
+0.03
+0.03
+0.03
+0.02
0.12
+0.04
+0.03
+0.03
+0.03

Figure 4. Different metrics clustering in latent structures
predicting perceived productivity. We color the following
groups: flawless suggestions (anything counting the number
of unchanged suggestions), persistence rate (ratio of accepted
suggestions that are unchanged), and fuzzy persistence rate
(ratio of accepted suggestions that are mostly unchanged).

enjoy higher acceptance rates, possibly hinting at a relative
strength of neural tooling versus deductive tooling for un-
typed languages. Regardless of language, survey participants
had a slightly higher acceptance rate than the whole user
base.

under consideration contributes positively, explains 43.2% of
the variance. The second component captures the acceptance
rate / change rate dichotomy; it explains a further 13.1%.

The results of both the individual correlations as well as
the PLS strongly point to acceptance rate being the most
immediate indicator of perceived productivity.

But what about combinations of metrics? We aim to quan-
tify the extra information provided by one metric over a set
of others. In the vein of incremental feature selection, we
fit an additional predictor to the residual of a model repre-
sented by already selected metrics. Starting from the best
single predictors, Table 3 shows the next most useful predic-
tors that predict the residual of the acceptance rate model
at p < 0.05. Given a model fit to acceptance rate, adding
the shown frequency or rate, as well as either amount of
accepted characters or accepted completions per hour each
further improve predictive ability at statistically significant
levels. No other additions were statistically significant for
further iterations.

So even if acceptance rate may be the best of the metrics
we considered, it is beneficial to combine with others to get
a fuller picture.

5 What Drives Acceptance Rate?
5.1 Language Use

We are aware that there are significant differences for how
GitHub Copilot performs for different programming lan-
guages. The most common languages among our user base
are TypeScript (24.7% of all shown completions in the ob-
served time frame, 21.9% for users in survey), JavaScript
(21.3%, 24.2%), and Python (14.1%, 14.5%). The latter two

Figure 5. Programming language use by survey participants
vs. all users.

This difference in language acceptance can not explain
away the effects from Section 4: when considering the linear
regression for perceived productivity from acceptance rate,
only 17% of the explained variance can be attributed to lan-
guage. On the other hand, however, 38% of the variance of
the different levels for the languages’ average perceived pro-
ductivity can be explained by the acceptance rates in Figure 5
above (using a linear model factoring through acceptance
rate).

5.2 Circadian and Weekly Rhythms

For coherence in the meaning of timestamps and weekdays,
all data in this section was restricted to users from the United
States (whether in the survey or not). We used the same time
frame as for the investigation in Section 4.

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

Figure 6. Average acceptance rate for hour-long time buck-
ets during the week. Each point represents the average for
such a bucket, whereas the shaded ribbon represents the
min-max variation for single hours during the observed 4
week period.

We observe strong regular patterns in overall acceptance
rate (Figure 6). These lead us to distinguish three different
time regimes, all of which are statistically significantly dis-
tinct at p < 0.001% (using a bootstrap test re-sampling the
proposed time regimes):

• The weekend: Saturdays and Sundays (day boundaries
taken from PST), where the average acceptance rate is
comparatively high at 23.5%.

• Typical non-working hours during the week: evenings
after 4 pm PST until mornings 7 am PST, where the
average acceptance rate is also rather high at 23%.
• Typical working hours during the week from 7 am
PST to 4 pm PST, where the average acceptance rate
is much lower at 21.2%.

The border between the second and third regime is fuzzy,
probably partially due to time zone variation within the
United States.

Users’ inclination to accept more suggestions outside of
standard working hours could be attributed either to regular
changes in the users’ behavior (e.g. accepting more solutions
because they are more relaxed), or to changes in the underly-
ing distribution of who is coding and what they are working
on (e.g. personal projects being easier to suggest code for).
To distinguish between these two explanations, we trained
a model to predict a user’s acceptance rate for a particular
time bucket from their usual contribution times (Table 4).
Unexpectedly to us, we found that the actual time bucket
mattered very little – but what did matter was whether it lay
in the user’s usual time regime. That means a user normally
active only during the week accepts fewer solutions on the
rare occasions they do code on a weekend, and a user whose

Figure 7. Acceptance rate depending on whether the user is
mostly active on weekdays / typical work hours (x-axis), and
whether it is actually a weekday / typical office hour (color).

activity is normally restricted to working hours accepts fewer
solutions when they do venture outside that time range.

6 Threats To Validity
The principal challenge to this work is that we have only
been able to investigate correlation and not causation. We
hope to have mitigated this to some extent by selecting “sen-
sible” metrics that we can justifiably believe could capture a
causal relationship with productivity. Nor do we claim that
these metrics themselves directly impact productivity (a de-
veloper with a faulty tab-key that accidentally accepts 80%
of suggestions without user intention will probably not be
extra productive because of it), but only that these metrics
are a good indicator of an underlying quality that predicts
productivity.

We did not set out to accurately predict a survey par-
ticipant’s answers, but merely to find a signal between de-
velopers’ perceived productivity and usage metrics. Still,
we must highlight that our best performing measurement
acceptance_per_shown has a Pearson coefficient of 0.24
and so there is a considerable amount of unexplained vari-
ance remaining.

User-perceived productivity is also not necessarily actual
productivity: seeking to maximise acceptance_per_shown
might satisfy an individual developer without directly re-
ducing the amount of time it takes them to solve a task.
And indeed one study looking at the benefits of GitHub
Copilot over IntelliCode found no measurable impact on
task completion time despite notably positive feedback from
developers [14]. On the other hand, one drawback of task-
oriented studies is of how representative the chosen tasks
are of real workloads whereas online studies (such as ours)
capture authentic activity.

Another substantial caveat is that we only considered a
single completion system, in particular with a single fixed
neural engine. Alternatives could be different in many as-
pects that could affect developer attitudes. Factors might
include average quality and the latency of completions, their
length and even the user interface used to present them.

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

Table 4. Acceptance rate depending on time factors.

coeff p-value

t-value

coeff p-value

t-value

-22.3
weekend user
-0.6
on a weekday
11.5
usual day for user
Results of a linear regression of acceptance rate from: 1. user’s percentile value for of how much their activity is concentrated on weekdays
(during typical work hours), 2. a categorical variable describing whether the suggestion is actually made on a weekday (during typical work
hours), and 3. the proportion of this user’s contributions where that categorical variable would have the same value.

-24.5 work hour user
during work hours
-2.9
11.3 usual time for user

<0.001
0.578
<0.001

<0.001
0.004
<0.001

-0.036
-0.001
0.019

-0.035
-0.004
0.021

7 Conclusion
Neural code completion systems have the potential to hugely
improve developer productivity through their ability to as-
similate contextual information about the developer’s cur-
rent activity and then generate substantial completions in
response. In this paper we investigated ways of connecting
the productivity benefit of GitHub Copilot to usage measure-
ments from developer activity. Our approach was to seek
correlations between our measurements and user-reported
productivity from survey results.

In common with prior work we collected measurements
about the acceptance of completions, but we also developed
measures of persistence. This was based on the idea that
for longer completions a developer might have to take more
action after accepting a completion such as deleting or cor-
recting an erroneous one.

We were surprised to find that acceptance rate (number
of acceptances normalized by the number of shown comple-
tions) was better correlated with reported productivity than
our measures of persistence.

But in hindsight, this makes sense. Coding is not typing,
and GitHub Copilot’s central value lies not in being the way
the user enters the highest possible number of lines of code.
Instead, it lies in helping the user to make the best progress
towards their goals. A suggestion that serves as a useful
template to tinker with may be as good or better than a
perfectly correct (but obvious) line of code that only saves
the user a few keystrokes.

This suggests that a narrow focus on the correctness of
suggestions would not tell the whole story for these kinds of
tooling. Instead one could view code suggestions inside an
IDE to be more akin to a conversation with a chatbot. We see
anecdotal evidence of this in comments posted about GitHub
Copilot online (see Appendix E for examples) in which users
talk about sequences of interactions. A conversation turn in
this context consists of the prompt in the completion request
and the reply as the completion itself. The developer’s re-
sponse to the completion arises from the subsequent changes
which are incorporated in the next prompt to the model.
And there are clear programming parallels to factors such as
specificity and repetition that have been identified to affect

human judgements of conversation quality [11]. Researchers
have already investigated the benefits of natural language
feedback to guide program synthesis [2] and so ours is not a
radical proposal. But neither is it one we have seen followed.
In future work, we wish to further explore this analogy,
borrowing ideas [16] from the evaluation of chatbots and
natural language text generation.

8 Broader Impact
A detailed impact analysis of the model that underlies GitHub
Copilot may be found in the Appendix of [5]. In this section,
we focus more specifically on the potential impact of using
the metrics we have described in this paper to evaluate the
success of neural code completion systems.

First, focusing on a single top-level metric such as ac-
ceptance rate may bias a tool toward the most popular use
cases — the most popular programming languages, natural
languages, IDEs, locations, etc. Users in underrepresented
groups may see lower quality results. We can mitigate this by
slicing our data along the lines described above, and avoid-
ing shipping changes that improve the top-level metric but
degrade performance for other slices of the data.

Second, to compute these metrics, we must collect teleme-
try from users. Collecting these metrics exposes users to
potential security and privacy concerns. We mitigate this by
enacting strict access controls to user data and collaborating
with organization and industry experts at protecting user
data.

Third, blindly optimizing for a proxy (acceptance rate) for
a desired property (usefulness) encourages artificial changes
that improve only that proxy. For example, cutting code sug-
gestions into half and suggesting both parts consecutively
would likely transform one accepted suggestion into two,
while not substantially increasing the number of rejections.
Thus it would likely increase acceptance rate without sub-
stantially increasing, and maybe even while decreasing, user
benefit. We can thus not recommend acceptance rate as sin-
gular and ultimate criterion of quality – it will be useful
for many applications, e.g. comparing incremental changes
to the code generating model, but its validity is limited in

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

other cases, especially those involving significantly changed
operational parameters.

Acknowledgments
We thank the GitHub Copilot team for their help, and in
particular Krzysztof Cieslak and Johan Rosenkilde for im-
plementing the highly complex telemetry of suggestion fate,
including calculating edit distances for fuzzy matches. We
thank the SAINTes team at Microsoft Research for their
advisement; Nicole Forsgren and Denae Ford Robinson for
advising on questions capturing perceived productivity with
the SPACE framework, and Tom Zimmermann and Chris-
tian Bird for recommending more time intervals to consider
for suggestion fate monitoring. We thank Rahul Pandita for
LATEXsupport and proofreading. Finally, we are grateful to
GitHub Incorporated for supporting this research.

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

9 Appendix
A Summary of usage measurements collected
This table shows summary statistics of our core metrics (highlighted in black). We include other possible metrics (arising from
different normalization options) in the table for context.

Metric

opportunity
shown
accepted
unchanged_30
unchanged_120
unchanged_300
unchanged_600
accepted_char
active_hour
mostly_unchanged_30
mostly_unchanged_120
mostly_unchanged_300
mostly_unchanged_600
opportunity_per_active_hour
shown_per_active_hour
accepted_per_active_hour
shown_per_opportunity
accepted_per_opportunity
accepted_per_shown
accepted_char_per_active_hour
accepted_char_per_opportunity
accepted_char_per_shown
accepted_char_per_accepted
mostly_unchanged_30_per_active_hour
mostly_unchanged_30_per_opportunity
mostly_unchanged_30_per_shown
mostly_unchanged_30_per_accepted
mostly_unchanged_120_per_active_hour
mostly_unchanged_120_per_opportunity
mostly_unchanged_120_per_shown
mostly_unchanged_120_per_accepted
mostly_unchanged_300_per_active_hour
mostly_unchanged_300_per_opportunity
mostly_unchanged_300_per_shown
mostly_unchanged_300_per_accepted
mostly_unchanged_600_per_active_hour
mostly_unchanged_600_per_opportunity
mostly_unchanged_600_per_shown
mostly_unchanged_600_per_accepted
unchanged_30_per_active_hour
unchanged_30_per_opportunity
unchanged_30_per_shown
unchanged_30_per_accepted
unchanged_120_per_active_hour
unchanged_120_per_opportunity
unchanged_120_per_shown
unchanged_120_per_accepted
unchanged_300_per_active_hour
unchanged_300_per_opportunity
unchanged_300_per_shown
unchanged_300_per_accepted
unchanged_600_per_active_hour
unchanged_600_per_opportunity
unchanged_600_per_shown
unchanged_600_per_accepted

N

2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,047
2,038
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019
2,047
2,047
2,038
2,019

Mean

Std.

Min.

Median

Max.

13,085.23
1,872.05
503.94
328.96
289.15
262.31
240.69
25,869.83
77.10
434.24
406.51
390.58
382.34
158.07
22.52
6.24
0.15
0.04
0.26
335.15
2.14
13.73
52.84
5.37
0.04
0.22
0.86
5.01
0.03
0.21
0.80
4.81
0.03
0.20
0.77
4.72
0.03
0.20
0.76
4.03
0.03
0.17
0.64
3.51
0.02
0.15
0.56
3.13
0.02
0.13
0.51
2.82
0.02
0.12
0.46

14,493.16
1,922.22
639.20
449.91
400.16
367.78
342.01
33,288.97
55.86
556.89
523.54
503.78
493.05
107.40
13.45
5.76
0.05
0.03
0.12
464.08
1.76
10.00
22.14
5.02
0.02
0.11
0.08
4.66
0.02
0.11
0.10
4.40
0.02
0.10
0.11
4.32
0.02
0.10
0.11
4.06
0.02
0.10
0.17
3.50
0.02
0.09
0.16
3.08
0.02
0.08
0.16
2.76
0.02
0.07
0.16

1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
5.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

8,686.00
1,276.00
293.00
178.00
155.00
140.00
125.00
14,662.00
66.00
254.00
237.00
230.00
222.00
134.40
20.02
4.60
0.15
0.04
0.24
235.00
1.71
11.63
48.85
3.89
0.03
0.21
0.86
3.67
0.03
0.19
0.81
3.54
0.03
0.18
0.77
3.51
0.03
0.18
0.76
2.89
0.02
0.15
0.67
2.52
0.02
0.13
0.59
2.24
0.02
0.12
0.53
2.06
0.02
0.11
0.48

241,033.00
15,832.00
5,851.00
4,253.00
3,937.00
3,737.00
3,487.00
320,080.00
364.00
5,025.00
4,785.00
4,586.00
4,491.00
1,844.00
137.75
58.56
0.37
0.22
1.00
14,064.00
20.87
194.00
494.83
51.92
0.19
1.00
1.00
49.00
0.19
1.00
1.00
46.61
0.19
1.00
1.00
44.85
0.19
1.00
1.00
43.84
0.19
0.64
1.00
40.00
0.19
0.63
1.00
36.77
0.17
0.62
1.00
34.15
0.17
0.61
1.00

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

B Correlations between usage measurements and survey questions
This table shows the correlation of the aggregate productivity score from our survey against our core metrics (highlighted in
black), as well as their PLS scores. We include other possible metrics (arising from different normalization options) in the table
for context.

Metric

accepted_per_shown
mostly_unchanged_30_per_shown
mostly_unchanged_120_per_shown
mostly_unchanged_300_per_shown
accepted_per_opportunity
mostly_unchanged_600_per_shown
unchanged_30_per_shown
mostly_unchanged_30_per_opportunity
mostly_unchanged_120_per_opportunity
mostly_unchanged_30_per_active_hour
accepted_per_active_hour
mostly_unchanged_120_per_active_hour
unchanged_120_per_shown
mostly_unchanged_300_per_active_hour
mostly_unchanged_300_per_opportunity
mostly_unchanged_600_per_active_hour
mostly_unchanged_600_per_opportunity
unchanged_30_per_opportunity
unchanged_30_per_active_hour
unchanged_300_per_shown
unchanged_120_per_active_hour
unchanged_120_per_opportunity
accepted_char_per_opportunity
unchanged_300_per_active_hour
unchanged_300_per_opportunity
unchanged_600_per_active_hour
accepted_char_per_shown
unchanged_600_per_shown
unchanged_600_per_opportunity
accepted_char_per_active_hour
accepted_char
mostly_unchanged_30
shown_per_active_hour
accepted
mostly_unchanged_120
mostly_unchanged_600
shown_per_opportunity
mostly_unchanged_300
unchanged_30
unchanged_120
unchanged_300
unchanged_600
mostly_unchanged_30_per_accepted
unchanged_30_per_accepted
mostly_unchanged_120_per_accepted
unchanged_120_per_accepted
mostly_unchanged_600_per_accepted
mostly_unchanged_300_per_accepted
accepted_char_per_accepted
opportunity_per_active_hour
unchanged_300_per_accepted
shown
unchanged_600_per_accepted
opportunity
active_hour

N

1,780
1,780
1,780
1,780
1,789
1,780
1,780
1,789
1,789
1,789
1,789
1,789
1,780
1,789
1,789
1,789
1,789
1,789
1,789
1,780
1,789
1,789
1,789
1,789
1,789
1,789
1,780
1,780
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,789
1,763
1,763
1,763
1,763
1,763
1,763
1,763
1,789
1,763
1,789
1,763
1,789
1,789

Coefficient

P-Value

0.24
0.23
0.23
0.22
0.22
0.22
0.21
0.21
0.21
0.21
0.21
0.21
0.21
0.21
0.21
0.20
0.20
0.20
0.20
0.19
0.19
0.19
0.19
0.19
0.18
0.18
0.17
0.17
0.16
0.16
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.11
0.10
0.10
0.10
0.07
0.06
0.05
0.04
0.04
0.04
0.03
0.03
0.02
0.01
-0.00
-0.04
-0.05

<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
0.01
0.02
0.04
0.09
0.09
0.09
0.20
0.24
0.40
0.75
0.95
0.08
0.03

PLS-1

0.0133

PLS-2

0.029

0.0122

0.0204

0.0116

0.0164

0.0112
0.0111

0.011
0.0108

0.0107
0.0103
0.01

0.0093
0.0094

0.0122
0.0104

0.0092
0.0099

0.0082
0.0082
0.006

0.0049
0.0155

0.006

0.0025

0.0059

0.0082

0.0036
0.0031
0.0028
0.0023
0.0022
0.0022

0.0011
4e-04
-1e-04

0.0077
-0.0011
0.0064
-0.0047
0.007
0.0053

-0.0094
-0.0149
-0.0138

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

C Aspects of Productivity Measured In The Survey
This table shows the relationship between the survey statements, the metrics and the different dimension of the SPACE
framework [6].

Survey statements

Productivity aspect

Code

Metric name

“I am more productive when using GitHub Copilot”

Perceived productivity

“I feel more fulfilled with my job when using GitHub Copilot.”

“I find myself less frustrated during coding sessions when using GitHub
Copilot.”

Satisfaction
being

and well-

“I can focus on more satisfying work when using GitHub Copilot.”

“While working with an unfamiliar language, I make progress faster
when using GitHub Copilot.”

“The code I write using GitHub Copilot is better than the code I would
have written without GitHub Copilot.”

n/a

“I learn from the suggestions GitHub Copilot shows me.”

“Using GitHub Copilot helps me stay in the flow.”

“I complete tasks faster when using GitHub Copilot.”

“I complete repetitive programming tasks faster when using GitHub
Copilot.”

“I spend less mental effort on repetitive programming tasks when using
GitHub Copilot.”

“I spend less time searching for information or examples when using
GitHub Copilot.”

S

P

A

C

Performance

Activity

Communication and col-
laboration [17]

Efficiency and flow

E

more_productive

more_fulfilled

less_frustrated

focus_satisfying

unfamiliar_progress

better_code

n/a

learn_from

stay_in_flow

tasks_faster

repetitive_faster

less_effort_repetitive

less_time_searching

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

D Incremental Feature Selection
Univariate fit to aggregate_productivity, excluding 120, 300 and 600 second unchanged metrics:

Behavioral Metric
accepted_per_shown
accepted_per_opportunity
accepted_per_hour
unchanged_30_per_opportunity
unchanged_30_per_hour
accepted_char_per_hour
shown_per_opportunity
shown_per_hour
mostly_unchanged_30_per_accepted
unchanged_30_per_accepted
shown

Coefficient P-value
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.01
0.02
0.78

0.13
0.12
0.11
0.11
0.10
0.09
0.06
0.06
0.03
0.03
0.00

SSR
466.26
471.28
473.79
475.43
476.36
482.39
489.60
489.66
493.49
494.04
495.58

Best metric fit to a univariate model modeling residuals of accepted_per_shown univariate model:

Behavioral Metric
shown_per_hour
accepted_char_per_hour
shown_per_opportunity
accepted_per_hour
unchanged_30_per_hour
mostly_unchanged_30_per_accepted
unchanged_30_per_accepted
accepted_per_opportunity
unchanged_30_per_opportunity
shown

Coefficient P-value
0.0363
0.0385
0.0399
0.0465
0.0955
0.2124
0.2461
0.4819
0.5442
0.5539

0.03
0.03
0.03
0.02
0.02
0.02
0.01
0.01
0.01
-0.01

SSR
465.10
465.13
465.15
465.21
465.53
465.85
465.91
466.13
466.17
466.17

MAPS ’22, June 13, 2022, San Diego, CA, USA

Ziegler, Kalliamvakou, Simister, Sittampalam, Li, Rice, Rifkin, and Aftandilian

E Publicly posted comments
Below we include a selection of (unsolicited) publicly posted comments which give a sense that developers are thinking about
engagement with GitHub Copilot rather than solely the immediate correctness of a completion.

It’s a little like pair programming with an incredibly eager junior developer who has read a lot of the documentation
of every popular API in the world.

https://news.ycombinator.com/item?id=30691608

Cycling through GitHub Copilot suggestions and manually editing the suggested code is an amazing flow. What I
really like is that OurTool adapts to my own code style.

https://www.meetup.com/Microsoft-Reactor-Toronto/events/284609940/

It says, ‘How can I facilitate your thinking process?’ rather than, ‘How can I take away your thinking process and
just give you code?’

https://www.protocol.com/workplace/github-copilot-ai-developers

i was writing a function that evaluates a polynomial, in a lambda (‘let eval_polynomial = |‘) and it autofilled a func-
tion that evaluated the polynomial but i wanted horner’s method, so i deleted and typed ‘let eval_polynomial_horner
= |‘it correctly autofilled (with one small error) horner’s method for evaluating polynomials

https://twitter.com/dystopiabreaker/status/1488692230114070529

Just pasted in an AttributeError as a comment in my Python file, and GitHub Copilot began trying to help me
debug my implementation of a @HuggingFace Transformers model. Its advice wasn’t 100% all-knowing, but was
enough to get me to a resolution!

https://twitter.com/DynamicWebPaige/status/1447587091995500546

References
[1] Sven Amann, Sebastian Proksch, Sarah Nadi, and Mira Mezini. 2016. A Study of Visual Studio Usage in Practice. In IEEE 23rd International Conference
on Software Analysis, Evolution, and Reengineering, SANER 2016, Suita, Osaka, Japan, March 14-18, 2016 - Volume 1. IEEE Computer Society, 124–134.
https://doi.org/10.1109/SANER.2016.39

[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,
and Charles Sutton. 2021. Program Synthesis with Large Language Models. CoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732
[3] Gareth Ari Aye, Seohyun Kim, and Hongyu Li. 2021. Learning Autocompletion from Real-World Datasets. In 43rd IEEE/ACM International Conference on
Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2021, Madrid, Spain, May 25-28, 2021. IEEE, 131–139. https://doi.org/10.1109/ICSE-
SEIP52600.2021.00022

[4] Moritz Beller, Vince Orgovan, Spencer Buja, and Thomas Zimmermann. 2020. Mind the gap: on the relationship between automatically measured and

self-reported productivity. IEEE Software 38, 5 (2020), 24–31.

[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,
Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374
https://arxiv.org/abs/2107.03374

[6] Nicole Forsgren, Margaret-Anne Storey, Chandra Maddila, Thomas Zimmermann, Brian Houck, and Jenna Butler. 2021. The SPACE of Developer

Productivity: There’s more to it than you think. Queue 19, 1 (2021), 20–48.

[7] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli. 2019. When code completion fails: a case study on real-world
completions. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, Joanne M.
Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 960–970. https://doi.org/10.1109/ICSE.2019.00101

Productivity Assessment of Neural Code Completion

MAPS ’22, June 13, 2022, San Diego, CA, USA

[8] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob
Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. CoRR abs/2105.09938 (2021). arXiv:2105.09938 https://arxiv.org/abs/2105.09938
[9] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang. 2019. SPoC: Search-based Pseudocode to Code.
In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.).
11883–11894. https://proceedings.neurips.cc/paper/2019/hash/7298332f04ac004a0ca44cc69ecf6f6b-Abstract.html

[10] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,
Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey
Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022.
Competition-Level Code Generation with AlphaCode.

[11] Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? How controllable attributes affect human judgments.
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.).
Association for Computational Linguistics, 1702–1723. https://doi.org/10.18653/v1/n19-1170

[12] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. IntelliCode compose: code generation using transformer. In ESEC/FSE ’20:
28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13,
2020, Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (Eds.). ACM, 1433–1443. https://doi.org/10.1145/3368089.3417058

[13] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vicente Franco, and Miltiadis Allamanis. 2021. Fast and Memory-Efficient
Neural Code Completion. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021, Madrid, Spain, May 17-19, 2021. IEEE,
329–340. https://doi.org/10.1109/MSR52588.2021.00045

[14] Priyan Vaithilingam, Tianyi Zhang, and Elena Glassman. 2022. Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered

by Large Language Models. In CHI ’22 Late-Breaking Work: Proceedings of the 2022 Conference on Human Factors in Computing Systems.

[15] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs. Experience: Evaluating the Usability of Code Generation Tools
Powered by Large Language Models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts (New Orleans, LA, USA) (CHI EA ’22).
Association for Computing Machinery, New York, NY, USA, Article 332, 7 pages. https://doi.org/10.1145/3491101.3519665

[16] Chris van der Lee, Albert Gatt, Emiel van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for the human evaluation of automatically
generated text. In Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019,
Kees van Deemter, Chenghua Lin, and Hiroya Takamura (Eds.). Association for Computational Linguistics, 355–368. https://doi.org/10.18653/v1/W19-8643
[17] Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and Qianying Wang. 2020. From human-human
collaboration to Human-AI collaboration: Designing AI systems that can work together with people. In Extended abstracts of the 2020 CHI conference on
human factors in computing systems. 1–6.

[18] Justin D. Weisz, Michael J. Muller, Stephanie Houde, John T. Richards, Steven I. Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.
2021. Perfection Not Required? Human-AI Partnerships in Code Translation. In IUI ’21: 26th International Conference on Intelligent User Interfaces, College
Station, TX, USA, April 13-17, 2021, Tracy Hammond, Katrien Verbert, Dennis Parra, Bart P. Knijnenburg, John O’Donovan, and Paul Teale (Eds.). ACM,
402–412. https://doi.org/10.1145/3397481.3450656

[19] Svante Wold, Michael Sjöström, and Lennart Eriksson. 2001. PLS-regression: a basic tool of chemometrics. Chemometrics and Intelligent Laboratory

Systems 58, 2 (2001), 109–130. https://doi.org/10.1016/S0169-7439(01)00155-1 PLS Methods.

[20] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2021. In-IDE Code Generation from Natural Language: Promise and Challenges. CoRR abs/2101.11149

(2021). arXiv:2101.11149 https://arxiv.org/abs/2101.11149

[21] Wen Zhou, Seohyun Kim, Vijayaraghavan Murali, and Gareth Ari Aye. 2021.

Improving Code Autocompletion with Transfer Learning. CoRR

abs/2105.05991 (2021). arXiv:2105.05991 https://arxiv.org/abs/2105.05991

