IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

1

PulseDL-II: A System-on-Chip Neural Network
Accelerator for Timing and Energy Extraction of
Nuclear Detector Signals

Pengcheng Ai, Zhi Deng, Senior Member, IEEE, Yi Wang, Hui Gong, Xinchi Ran and Zijian Lang

2
2
0
2

p
e
S
2

]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[

1
v
4
8
8
0
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Front-end electronics equipped with high-speed dig-
itizers are being used and proposed for future nuclear detectors.
Recent literature reveals that deep learning models, especially
one-dimensional convolutional neural networks, are promising
when dealing with digital signals from nuclear detectors. Sim-
ulations and experiments demonstrate the satisfactory accuracy
and additional beneﬁts of neural networks in this area. However,
speciﬁc hardware accelerating such models for online operations
still needs to be studied. In this work, we introduce PulseDL-
II, a system-on-chip (SoC) specially designed for applications of
event feature (time, energy, etc.) extraction from pulses with deep
learning. Based on the previous version, PulseDL-II incorporates
a RISC CPU into the system structure for better functional
ﬂexibility and integrity. The neural network accelerator in the
SoC adopts a three-level (arithmetic unit, processing element,
neural network) hierarchical architecture and facilitates param-
eter optimization of the digital design. Furthermore, we devise
a quantization scheme and associated implementation methods
(rescale & bit-shift) for full compatibility with deep learning
frameworks (e.g., TensorFlow) within a selected subset of layer
types. With the current scheme, the quantization-aware training
of neural networks is supported, and network models are auto-
matically transformed into software of RISC CPU by dedicated
scripts, with nearly no loss of accuracy. We validate the correct
operations of PulseDL-II on ﬁeld programmable gate arrays
(FPGA) and show its improvements compared with the previous
version with respect to the performance, power and area. Finally,
system validation is done with an experimental setup made up of
a direct digital synthesis (DDS) signal generator and an FPGA
development board with analog-to-digital converters (ADC). The
proposed system achieved 60 ps time resolution and 0.40% energy
resolution with online neural network inference at signal to noise
ratio (SNR) of 47.4 dB.

Index Terms—Deep learning, feature extraction, ﬁeld pro-
grammable gate array (FPGA), front-end electronics (FEE),
model quantization, neural network accelerator, system-on-chip
(SoC).

I. INTRODUCTION

A DVANCES in electronic design and manufacturing tech-

nology have greatly inﬂuenced the methodology and
sensitivity of nuclear detectors. High-speed digitizers [1] with
associated processing & storage circuitry revolutionized the

This work was supported in part by the National Key Research and
Development Program of China (under project no. 2020YFE0202001), in part
by the National Key Research and Development Program of China (under
grant no. 2020YFC01220002), and in part by China Postdoctoral Science
Foundation (under grant no. 2021M690088).

The authors are with the Department of Engineering Physics, Tsinghua
University, Beijing 100084, China, and also with the Key Laboratory of
Particle and Radiation Imaging (Tsinghua University), Ministry of Education,
Beijing 100084, China (e-mail: dengz@mail.tsinghua.edu.cn).

front-end electronics (FEE), especially in performance-critical
conditions. For system designers, however, the evolution is
a double-edged sword: for one thing, it provides an abun-
dance of raw data for experimenting various feature extraction
algorithms (both traditional and intelligent); for another, it
poses considerable pressure on the readout system, especially
hardware and software capacities within the data link. It is an
important issue to maximize the beneﬁts of signal sampling
systems while keeping the data throughput controllable and
the overall complexity acceptable.

Deep learning [2], essentially multi-layer neural networks
with delicate structures, developed dramatically in 2010s as
a frontier in machine learning and artiﬁcial intelligence. Big
data from nuclear detectors made it possible to apply this
novel method to many tasks directly related to detector signals.
in high energy physics (HEP), deep neural
For example,
networks have been applied for particle identiﬁcation [3],
particle discrimination [4] and also low-level triggers [5]. In
nuclear medicine, convolutional neural networks have been
in positron
used for accurate estimation of time-of-ﬂight
emission tomography detectors [6], [7]. In plasma physics,
deep variational autoencoders have been utilized to integrate
diagnostic data of soft X-ray images [8]. Recently, additional
merits and understandings about neural networks in nuclear
electronics have been explored, such as estimation of het-
erogeneous uncertainty of nuclear detector signals [9], and
computation of the Cram´er Rao lower bound of timing to ﬁnd
out limits for neural networks [10].

The needs of pushing neural networks to front-end arise
from the trade-off between data transmission and data process-
ing. For HEP experiments, there are some low-level tasks with
online processing demand and stringent latency requirements.
In [11], an application-speciﬁc integrated circuit (ASIC) is
developed for accelerating the inference of a compact autoen-
coder model to ease data compression for the high-granularity
endcap calorimeter. In [5], a dense neural network on an FPGA
is designed for level 1 trigger directly generated upon raw data.
Since the data interfaces and targets in these applications are
well deﬁned, a brute-force approach is adopted to implement
the network model. In this approach, only weights in the
neural network can be adjusted; the network architecture is
unchangeable once the design is consolidated and optimized.
Beyond applications above, there are also some scenarios
where we intend to fulﬁll the advantage of neural networks
while keeping more ﬂexibility. Neural network accelerators
based on the array of processing elements (PE) serve this pur-

 
 
 
 
 
 
IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

2

limits the permissible sampling rate of ADC and prevents the
FEE from fully realizing the potential resolution.

In future upgrade plans, the current FEE made up of discrete
components will be replaced by ASIC. The analog design of
the pre-ampliﬁer and the 200 MHz ADC for each channel are
under development. Besides, in order to reduce the amount of
transmitted data and also to reach theoretical performance of
the FEE, digital logic accelerating neural network inferences
is proposed to be integrated into the front-end system for
extraction of signal features, primarily the time and energy of
incident particles. The work reported in this paper is aimed
at, but not restricted to, application of the neural network
accelerator for ECAL at NICA-MPD.

III. SIGNAL FEATURE EXTRACTION WITH NEURAL
NETWORK (AB)

A. Building Blocks of Network Structure

Deep learning models are evolving at a tremendous pace.
New elements and architectures are frequently coming out.
Here we only focus on one-dimensional (1d) convolutional
neural networks [10], [18], because they not only ﬁt into
the dimensionality of the problem, but also succeed in many
machine learning tasks and facilitate parallel computing.

We select four representative building blocks: 1d convolu-
tion layer, 1d deconvolution (or transpose convolution) layer,
fully-connected layer and nonlinear activation (such as ReLU
[19]). Ref. [10] gives a nice visualization of the former
three layers. Regarding the nonlinear activation, it is the key
for inductive learning (and thus intelligent signal processing)
[20]. With nonlinearity, weights in the mapping function are
selectively turned off/scaled by the nonlinear function. This
subset of neural network layers is supported by mainstream
deep learning frameworks, e.g., TensorFlow [21] which is used
together with Keras [22] in this paper.

B. Autoencoder-Based Network Architecture

We propose to use the autoencoder-based network archi-
tecture [23] as our reference model. It is composed of an
encoder containing convolution layers, a decoder containing
deconvolution layers, an optional bypass between the encoder
and the decoder, and a regression network which can be
located at the far end [18] or at the bottleneck [10]. The
decoder is optional when the regression network is at the
bottleneck. This reference model is the workload targeted by
the neural network accelerator which will be discussed in
Section IV.

C. Quantization-Aware Training and Validation

Conventional artiﬁcial neural networks use ﬂoating-point
numbers to represent weights and intermediate feature maps.
Although it is straightforward for CPUs and GPUs to compute
using ﬂoating-point numbers, it is not friendly to customized
digital ASICs. It
the ﬂoating-point
is possible to convert
model into the ﬁxed-point using 8-bit or 16-bit quantization.
However, directly converting a well-trained model will result
in signiﬁcant degradation of accuracy.

Fig. 1. Three elements of design perspectives within the research.

pose [12], [13]. However, the operation of PE-based accelera-
tors relies on the transactions between accelerators and a host
processor. This is easily available in sophisticated computer
systems, but not in HEP front-end electronics. SoC digital
design with microcontrollers/microprocessors integrated can
efﬁciently schedule transactions related to the neural network
accelerator and thus make the system autonomous and in-
dependent. The SoC neural network accelerator is among a
future upgrade of FEE for the electromagnetic calorimeter in
the NICA-MPD experiment [14], [15].

In the following parts of this paper, we will recursively
discuss three elements of design perspectives, as shown in
Fig. 1. They are:

A Nuclear Electronics: readout system and signal features.
B Neural Network: architectural research, network training.
C Digital Design: neural network accelerator and SoC.
The three independent elements will produce several inter-
sections, i.e. application training, hardware mapping, system
prototype and joint validation. We will use the same notations
(A, B, C) to relate each section to associated topics.

II. ELECTROMAGNETIC CALORIMETER AT NICA-MPD
(AC)

The Multi-Purpose Detector (MPD) at the NICA collider is
designed for protons/heavy ions collisions to study the basic
quantum chromodynamics structure of matter [16]. The elec-
tromagnetic calorimeter (ECAL) in this detector and its FEE
are described in [17]. Silicon photomultipliers coupled to the
shashlik-type calorimeter were used to transform scintillation
light into current pulses, which would later be transmitted
through ﬂat cables. A 64-channel 12-bit commercial ADC
board with 62.5 MHz sampling rate digitized the analog
signals and sent waveform samples to back-end through optical
ﬁbers. The current FEE was expected to reach sub-200 ps
timing resolution and 6% energy resolution.

Although satisfying the basic experimental speciﬁcations,
the current FEE has some major drawbacks. Despite the fact
that data links are reduced by ADC, real-time transmission
of raw ADC samples brings about high data bandwidth and
power consumption (about 250 mW/channel). This in return

NeuralNetworkDigitalDesignNuclearElectronicsHardwareMappingApplicationTrainingSystemPrototypeJointValidationIEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

3

Fig. 2. Three stages of quantization-aware training. In training, labels are used to formulate the loss function. In testing, they are used to compute metrics.

To tackle the problem, a quantization-aware training scheme
[24] natively supported by deep learning frameworks is uti-
lized to gradually transform the original ﬂoating-point model
into the quantized model with nearly no loss of accuracy.
The three stages of the process is shown in Fig. 2. In each
stage, waveform samples coming from detector simulation
or experimental measurement are provided as network input,
and signal features (such as time or energy) are provided as
ground-truth labels to optimize parameters of the network and
validate its performance.

The ofﬁcial code for quantization-aware training is assumed
to be used with CPUs. To make it compatible with the
neural network accelerator here, we post-process the layer-
wise results with rescale and bit-shift on hardware (Section
IV-C) and rewrite the mechanism for quantized-model export
in the hardware-software codesign (Section IV-D).

IV. SYSTEM-ON-CHIP ACCELERATOR DESIGN (BC)

A. A Brief Review of PulseDL

For application of neural networks to process detector
signals at FEE, we developed the ﬁrst version of the neural
network accelerator for pulse processing, named PulseDL [25],
[26]. PulseDL worked as a co-processor and communicated
with a RISC processor through a proprietary bus. The accel-
erator architecture of PulseDL was mainly composed of a 4×4
PE array, fed by dedicated row buffers and column buffers, and
followed by spatial and temporal adder trees. In operation of
each PE, operands from the feature map vector and the kernel
matrix are multiplied and accumulated.

The ﬁrst version of the chip, although a successful practice,
has some notable limitations. A RISC CPU outside the chip
(or accelerator) is needed to schedule transactions, which is
not convenient in use. Dynamic quantization (i.e. deciding
the rounding bit after the feature map is obtained) may bring
about additional overheads of time. The adder tree structure,
especially the temporal adder tree, has yet to be optimized
for area and performance. Finally, only manual conﬁguration
procedure is devised to map neural network models onto the

chip, while extension to deep learning frameworks could be a
more automatic solution. The above limitations motivate us to
develop PulseDL-II, the new version of the digital design.

B. PulseDL-II: SoC Structure

The primary improvement of PulseDL-II is the system
structure. We integrate an RISC CPU,
the ARM Cortex-
M0 microcontroller, and associated AHB/APB buses into the
digital design to form an SoC, as shown in Fig. 3. The Cortex-
M0 core is an intellectual property (IP) distributed freely
as trial by the ARM company [27]. It is a microcontroller
featuring the small footprint and low power.

The SoC has three major parts: the NN Pulse Processor
(PulseDL-II accelerator), the Cortex-M0 SoC and the Dual-
Port AHB RAM. The PulseDL-II neural network accelerator
is mounted on the processor AHB bus as a peripheral. In this
SoC, we have multiple input/output peripheral devices, such
as quad/normal SPIs, UARTs (with or without the internal
buffer), JTAG and GPIOs.

The preset workﬂow is described as follows. The wave-
form samples coming from ADC and self-trigger logic are
transmitted into the double-port buffer, preferably through the
quad SPI interface. The Cortex-M0 core periodically accesses
the double-port buffer and relays the input data to the neural
network accelerator. When the accelerator ﬁnishes computing,
it raises a signal to the Cortex-M0 core and pushes feature
maps into the system RAM under the coordination of the
core. The data transmission between the system RAM and
the accelerator is repeated several times, until the ﬁnal feature
outputs are ready. Finally, the output data are sent out through
a buffered UART and collected by subsequent electronics.

It should be mentioned that transactions on the processor
AHB bus generate unavoidable overheads in the total time
budget. Specially designed direct memory access (DMA)
with data processing abilities have the potential of reducing
these overheads, but also adversely impact on the universal
applicability of the design. At the current stage, we have not
integrated such speciﬁc logic, but will probably do so when

network inputnetwork outputnetwork inputnetwork outputnetwork inputnetwork outputNetworkOptimizerQuantizationOriginalModel(infer with floating number)Quantization-Aware Model(update step and range)Quantized Model（infer with integer/fixednumber）NetworkOptimizerQuantizationNetworkOptimizerQuantizationt0KSamplesSamplesSamplesLabels(t0, K)Labels(t0, K)Labels(t0, K)DetectorSimulationWaveformGenerationTest ResultTest ResultTest ResultTraining and ValidationExperimentDataIEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

4

Fig. 3. The system-on-chip structure of PulseDL-II. Three major parts are surrounded by different polygons and marked with colors.

we gather more information about the performance of different
neural network architectures.

C. PulseDL-II: Accelerator Architecture

Another important improvement is the accelerator archi-
tecture itself. The updated digital design of the PulseDL-II
accelerator is shown in Fig. 4. A new hierarchical level, the
arithmetic unit (AU), is added into the topology. There are
three hierarchical levels in total: AU, PE and NN (neural
network), as shown in the middle of the bottom in this ﬁgure.
In essence, PE plays a role which is more self-contained and
can be regarded as a miniature PulseDL accelerator. Accord-
ingly, AU takes the responsibility of multiply-and-accumulate
(MAC) at a more ﬁne-grained level than the original PE in
PulseDL. By these functional adjustments, both efﬁciency and
ﬂexibility have been improved, and the software mapping
scheme ﬁts better into the hardware, which in turn reduces
power and area.

A template-based methodology is adopted to design logic
elements and to make elements in each level adjustable.
Compared with other methods, such as high level synthesis,
this methodology allows designers to perform cycle-accurate
optimization and ﬁne control of data interfaces, at the expense
of more expertise and manpower. In the example design
referred in the following sections, we use 4 AUs in each PE,
and 15 PEs in each NN. The transactions are assumed to be
directed to a single NN device.

For quantization compatible with TensorFlow or other deep
learning frameworks, a functional block of rescale & bit-shift

[24] is integrated after temporary results are generated by bias
& activation. Rescale & bit-shift is a procedure to adjust the
scale of the output feature map so that the full quantization
bits can be effectively utilized. It serves the purpose in a way
friendly to integer-only digital logic. The actual quantization
bits used in the inference will determine the loss of accuracy
by quantization.

In the PulseDL-II accelerator, other enhancements include:
broadcasting/multicasting input feature maps and kernels to
multiple AUs, optimizing the (temporal) adder tree with the
partial sum accumulator, adding function blocks for bias
addition and activation, and a whole new implementation of
the mapping mode coordinator for different layers, etc.

D. Hardware-Software Codesign

To ensure the proper functioning of the SoC, a framework
for hardware-software codesign is implemented along with
PulseDL-II, as shown in Fig. 5. We use the same names
as Fig. 3 for three major parts in the SoC, placed on the
in this ﬁgure is related to the
left side. The upper part
neural network. Two databases, containing the input/output
transactions and model parameters, are generated from neural
network-related software. The lower part
is related to the
accelerator system. The embedded software for Cortex-M0 is
compiled with the ARM GCC compiler, and converted to the
hexadecimal format. At the bottom, a testbench using VUnit
(a unit test framework for hardware description languages)
is set up to test the function and performance of the SoC
with benchmark programs and also corner cases. Multiple

Quad-SPIAHB MasterNNAcceleratorCortex-M0CorePulseDL-IINeural NetworkAcceleratorProgramMemorySystemRegistersGPIOHigh-SpeedJTAGSystemRAMAPB BusAHB-APBInterconnectQuad-SPIAHB MasterTimerWatchdogUART2ProcessorAHB BusUART3(buffered)Debug Info & TerminationPhysical Feature OutputDouble-PortBufferAuxiliaryAHB BusStatusUART/SPIAHB MasterADC WaveformSamples InputProgramming &MonitoringAlternativeProgrammingCortex-M0SoCNN PulseProcessorDual-PortAHB RAMIEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

5

Fig. 4. The accelerator architecture of PulseDL-II. The feature map memory interface (fmap mem) and kernel memory interface (kernel mem) receive data
and multicast through controllers (multicast ctrl) to multiple arithmetic units (AU). In each AU, the feature map register (fmap reg) and ping-pong kernel
register (kernel reg) store data for multiply-and-accumulate (MAC). After MAC and the adder tree, the partial sum accumulator updates results in the memory
(partial sum mem). Final results are transferred through the result memory interface (result mem). The square in the lower left corner shows the basic structure
of MAC in an AU, and the square in the lower middle shows three hierarchical levels.

programming languages and ﬁle formats are used as indicated
in this ﬁgure.

E. Embedded Software with Weight-Stationary Mapping

Within the SoC structure, different mapping rules are al-
lowed depending on the requirements of application. For
neural networks with small/medium size, a weight-stationary
mapping scheme can be adopted to reduce transactions when
weights are used repeatedly. In this scheme, the operation is
divided into two phases. In the preparation phase, weights
are stored into PEs before waveform samples come in. In the
inference phase, only input data, output data and intermediate
feature maps are transferred in and out of PEs. Besides,
the weight-stationary embedded software enables following
features:

Layer-wise inference pipelining: Weights for different
layers are mapped to different groups of PEs; in inference, PEs
can operate simultaneously as independent computing devices.
Event-level parallelism: Each event (In this paper, we use
the terminology event to mean a series of waveform samples
generated by the detector response to a single initial particle.)
is assigned a unique token and recorded by the Cortex-M0
core; in inference, the token will be traced by the core and
passed in company with feature maps along the pipeline.

F. Evaluation

We compare the performance of PulseDL and PulseDL-II
on an FPGA (Xilinx ZCU104 evaluation board) platform. The

working frequency is set to 100 MHz for both versions. For
fair comparison with PulseDL, the PulseDL-II neural network
accelerator is isolated when measuring power and area.

The evaluation results are shown in Fig. 6. We use a neural
network workload comprising three convolution layers and two
fully-connected layers, and containing approximately 33.4k
MACs and 18.8k trainable parameters, shown in Fig. 7.

In Fig. 6(a) and Fig. 6(b), it can be seen that PulseDL-
II greatly reduces running time and energy consumption. For
compute-intensive layers, such as convolution layer #3 and
fully-connected layer #1, the reduction is very signiﬁcant,
mainly due to the innovation in the accelerator architecture and
mapping scheme. These two layers contribute largely to the
overall improvement in performance (1.83× less) and power
(1.81× less).

For resource utilization in Fig. 6(c), PulseDL-II integrates
more multipliers (64 versus 360), but the average utilization of
FPGA hardware resources is comparable or less. The look-up
tables (LUT) divided by 8-bit multipliers are decreased from
739 to 529 (1.40× less), and the ﬂip-ﬂops (FF) divided by
8-bit multipliers are increased from 920 to 933 (1.01× more).

The preliminary evaluation shows the advancement of
PulseDL-II for common workloads used in feature extraction
of nuclear detector signals. For one thing, the new accelerator
architecture, supported by the SoC structure, boosts the perfor-
mance of inference; for another, power reduction is observed
and resource utilization is acceptable, which makes the ASIC
implementation much more promising.

fmapmemkernel memmapping modecoordinatormulticast ctrlmulticast ctrlping-pongkernel regfmapregMAC+++...+...++++...result mem...OR+......1) multicast fmap& kernel mapping3) multi-stage readout of adder tree4) in situ update of results(1st layer input)(subsequent layers)spatial or temporal iteration****+++AUATPENNAUAUAUPEpartial sum memORrescale &bit-shiftORbias & activation5) final process2) multiply-and-accumulatePulseDL-IINeural Network AcceleratorIEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

6

Fig. 5. The data stream diagram for hardware-software codesign of PulseDL-II. Functional groups are divided by lines with different styles, and programming
languages and ﬁle formats are indicated with colors.

(a) Performance (time consumption).

(b) Power (energy consumption).

(c) Area (resource utilization).

Fig. 6. Comparison of performance, power and area between PulseDL and PulseDL-II on the FPGA platform.

receive and digitize the analog signals. The data acquisition
front-end and the SoC with the neural network accelerator are
implemented as digital logic on the FPGA development board.
Fig. 9 shows the digital logic for data acquisition, a pre-
requisite for feature extraction by the accelerator SoC. We
use self-trigger to get a snapshot of data samples in the
ring buffer and the timestamp. The triggered data is fed to
event data acquisition (DAQ) for both monitoring the current
waveform and preprocessing the waveform for the accelerator
SoC. Multiple integrated logic analyzers (ILA) and the virtual
input/output (VIO) are inserted into the dataﬂow to probe
internal signals and manipulate the data path.

B. Experimental Results

In the experiment, we prestore the CRRC waveform (step
signal ﬁltered by the bandpass capacitance-resistance circuit)
into the DDS signal generator. The waveform function is
shown in Equation 1:

s(t) = K

(cid:19)

(cid:18) t − t0
τ

exp−(t−t0)/τ u(t − t0)

(1)

Fig. 7. Neural network workload used to compare two versions of the design.

V. SYSTEM VALIDATION (ABC)

A. Experimental Setup

To validate the whole system, we establish an experimental
environment without actually connecting the electronics to
the detector, as shown in Fig. 8. A DDS signal generator
(AD9106-ARDZ-EBZ) working at 156.25 MHz produces 4-
channel analog signals which can be programmed by an ARM
microcontroller board (SDP-K1) beforehand. At the other side,
we use an FPGA development board (MZU07A-EV) equipped
with two ADC cards (ADS4225) sampling at 125 MHz to

NN Pulse ProcessorDual-Port AHB RAMCortex-M0 SoCTestbench(VUnit)Testbench(Verilator)TensorFlowNeural Network ModelCPU SoftwareModel Quantization(TensorFlow-Compatible)Model ParametersQuantizedSimulation DataInput/Output TransactionsC HeadersHexadecimalData FilesARM GCC CompilerHexadecimalProgram FileSoC Testbench (VUnit)HardwareSoftwareNeural NetworkRelatedAcceleratorSystemRelatedVerilog/SystemVerilogVHDLPythonC/C++HDF5HEX file & R Q Y B  & R Q Y B  & R Q Y B  ) & B  ) & B                        7 L P H   V  7 L P H  F R Q V X P S W L R Q  E \  O D \ H U V  D Q G  Y H U V L R Q 3 X O V H ' / 3 X O V H ' /  , , & R Q Y B  & R Q Y B  & R Q Y B  ) & B  ) & B                 ( Q H U J \   -  ( Q H U J \  F R Q V X P S W L R Q  E \  O D \ H U V  D Q G  Y H U V L R Q 3 X O V H ' / 3 X O V H ' /  , , 0 8 /  / 8 7  0 8 /  ) )  0 8 /                   & R X Q W                    E L W  P X O W L S O L H U V   0 8 /     / 8 7  0 8 /   D Q G  ) )  0 8 /  3 X O V H ' / 3 X O V H ' /  , ,inputs (12-bit ADC)conv layer 1conv layer 2conv layer 3fc layer 1fc layer 2 (outputs)Param #: 80Param #: 2080Param #: 8256Param #: 8256Param #: 130MAC #: 512MAC #: 8192MAC #: 16384MAC #: 8192MAC #: 128IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

7

(a) Functional diagram.

(b) Photograph.

Fig. 8. The experimental setup for system validation.

Fig. 9. The digital logic on the FPGA development board for data acquisition.
The top-right square shows an example waveform from the oscilloscope.

where u(t) is the step function. We set τ to 40 ns, and set
K = K1K2, where K1 is a constant keeping the SNR relative
to the baseline noise (σbase) to 47.4 dB:

SN R = 20 log10

(cid:19)

(cid:18) K1
σbase

= 47.4 dB

(2)

and K2 is uniformly sampled in the range between 0.5 and
2.0. For time analysis, we produce dual-channel synchronous
waveform and compare the timing results of two channels.
For energy analysis, we ﬁrst vary the waveform amplitude in
a certain range and then use a standard waveform to assess
energy predictions.

The neural network model we use is similar to Fig. 7. It is
comprised of two convolution layers and three fully-connected
layers, and contains 9.8k MACs and 5.8k trainable parameters.
Waveform samples from the monitoring branch in the data
acquisition are used to train the neural networks and to export
network weights for online processing. Identical waveform
samples are used for traditional feature extraction methods.

In Fig. 10, we show experimental results of time/energy
prediction by different methods, including traditional methods
(interpolated constant fraction discrimination [28] for time,
and waveform integration for energy), ofﬂine ﬂoating-point
neural networks and online quantized (8-bit ﬁxed-point) neural
networks. It can be seen that neural networks work better than
traditional methods in the same conditions. Since quantization
effects will inﬂuence the ﬁnal online resolution, we observe
a slight degradation of resolution when comparing the right
two ﬁgures with the middle two. It will be eliminated if more
quantization bits are used (such as 16-bit ﬁxed-point).

Here we report resource utilization, power and performance
of the experimental system. Measured on the Xilinx Zynq
UltraScale+ FPGA, the data acquisition uses 2825 LUTs, 517
FFs, 8 block RAMs (36 kb each) and 8 UltraRAMs (288 kb
each), and consumes 0.371 W dynamic power. The accelerator
SoC uses 89540 LUTs, 75028 FFs and 48 block RAMs, and
consumes 0.541 W dynamic power. The device static power is
0.594 W. Both the dynamic power of the data acquisition and
the static power are expected to improve signiﬁcantly with the
ASIC implementation. The time for on-chip neural network
inference is 113.8 µs at 100 MHz working frequency. With
time consumption from data input/output, the total latency of
a single event is expected to be 165 µs. When working in the
pipelined mode, the throughput is 8.3k events/second.

VI. CONCLUSION

The ability and potential of neural networks in feature
extraction of nuclear detector signals are investigated. Based
on the advantage of neural networks, we prototype a neu-
ral network accelerator-centric FEE for ECAL at NICA-
MPD. Application-speciﬁc neural network architectures are
proposed, and quantization-aware training is extended for use
by customized computing devices.

The major part of our work is to develop an SoC digital
system with the neural network accelerator. The SoC approach
is ﬂexible not only in changing weights of a targeted neural
network, but also in accommodating a variety of network
workloads within the selected subset. We elaborate on the de-
sign of PulseDL-II from both hardware and software aspects.
A comparison between PulseDL-II and its previous version

MZU07A-EVFPGADevBoardADS4225Card125M, 12b, 2VppADS4225Card125M, 12b, 2VppCh1Ch2Ch3Ch4HostComputer1. FPGAFirmware,IntegratedLogicAnalyzer;2. ARMMCUProgram;3. Feature OutputAD9106-ARDZ-EBZDDSSignalGenerator156.25M,12b, 2VppSDP-K1ARMMCUFEP Card WrapperILA 1FEP Card WrapperFEP CardInterfaceSelf TriggerRing BufferTrigger LogicData FIFOTime Stamp FIFOILA 0From ADCFEP CardsEvent DAQMonitor AdapterBRAMCheckPointILA 2Neural Network AdapterBRAMTo AcceleratorSystem-on-ChipVIO 0IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. XX, NO. XX, XXXX 2020

8

(a) Interpolated CFD (time).

(b) Floating-point NN (time).

(c) Quantized NN (time).

(d) Waveform integration (energy).

(e) Floating-point NN (energy).

(f) Quantized NN (energy).

Fig. 10. Experimental results of time/energy prediction by traditional methods, ofﬂine ﬂoating-point neural networks and online quantized neural networks.
For time, the numbers in the parentheses are single-channel standard deviations computed from differences of two channels (divided by

2).

√

demonstrates the advancement of digital design. Finally, sys-
tem validation on an FPGA platform is done.

In the future, we will evaluate the whole system in real-
world nuclear detector dataﬂows. We also plan to tape out with
28/65 nm process after the ASIC layout has been ﬁnished.

REFERENCES

[1] F. Ameli et al., “A low cost, high speed, multichannel analog to digital
converter board,” Nucl. Instrum. Methods Phys. Res. A, Accel. Spectrom.
Detect. Assoc. Equip., vol. 936, pp. 286–287, 2019.

[2] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

no. 7553, pp. 436–444, May 2015.

[3] S. Choudhury, “Tau Identiﬁcation With Deep Neural Networks at the
CMS Experiment,” IEEE Trans. Nucl. Sci., vol. 68, no. 8, pp. 2194–
2200, 2021.

[4] M. Astrain et al., “Real-Time Implementation of the Neutron/Gamma
Discrimination in an FPGA-Based DAQ MTCA Platform Using a
Convolutional Neural Network,” IEEE Trans. Nucl. Sci., vol. 68, no. 8,
pp. 2173–2178, 2021.

[5] B. Clerbaux et al., “Study of Using Machine Learning for Level 1
Trigger Decision in JUNO Experiment,” IEEE Trans. Nucl. Sci., vol. 68,
no. 8, pp. 2187–2193, 2021.

[6] E. Berg and S. R. Cherry, “Using convolutional neural networks to
estimate time-of-ﬂight from PET detector waveforms,” Phys. Med. Biol.,
vol. 63, no. 2, p. 02LT01, Jan 2018.

[7] S. I. Kwon et al., “Ultrafast timing enables reconstruction-free positron
emission imaging,” Nat. Photonics, vol. 15, no. 12, pp. 914–918, Dec
2021.

[8] A. Rigoni Garola et al., “Diagnostic Data Integration Using Deep Neural
Networks for Real-Time Plasma Analysis,” IEEE Trans. Nucl. Sci.,
vol. 68, no. 8, pp. 2165–2172, 2021.

[9] P. Ai, Z. Deng, Y. Wang, and C. Shen, “Universal uncertainty estimation
for nuclear detector signals with neural networks and ensemble learn-
ing,” J. Instrum., vol. 17, no. 02, p. P02032, Feb 2022.

[10] P. Ai, Z. Deng, Y. Wang, and L. Li, “Neural network-featured timing
systems for radiation detectors: performance evaluation based on bound
analysis,” J. Instrum., vol. 16, no. 09, p. P09019, Sep 2021.

[11] G. D. Guglielmo et al., “A Reconﬁgurable Neural Network ASIC for
Detector Front-End Data Compression at the HL-LHC,” IEEE Trans.
Nucl. Sci., vol. 68, no. 8, pp. 2179–2186, 2021.

[12] Y.-H. Chen, T. Krishna et al., “Eyeriss: An Energy-Efﬁcient Reconﬁg-
urable Accelerator for Deep Convolutional Neural Networks,” IEEE J.
Solid-State Circuits, vol. 52, no. 1, pp. 127–138, 2017.

[13] Y.-H. Chen, T.-J. Yang et al., “Eyeriss v2: A Flexible Accelerator for
Emerging Deep Neural Networks on Mobile Devices,” IEEE Trans.
Emerg. Sel. Topics Circuits Syst., vol. 9, no. 2, pp. 292–308, 2019.
[14] C. Shen et al., “Development of shashlik electromagnetic calorimeter
for the NICA/MPD,” J. Instrum., vol. 14, no. 06, p. T06005, Jun 2019.
[15] Y. Li et al., “Beam test results of two shashlyk ECal modules for NICA-
MPD,” Nucl. Instrum. Methods Phys. Res. A, Accel. Spectrom. Detect.
Assoc. Equip., vol. 958, p. 162833, 2020.

[16] “The MultiPurpose Detector – MPD (Conceptual Design Report),”
accessed: 2022-8-4. [Online]. Available: http://mpd.jinr.ru/wp-content/
uploads/2016/04/MPD CDR en.pdf

[17] “MPD NICA technical design report of the electromagnetic calorimeter
(ECal),” accessed: 2021-5-24. [Online]. Available: http://mpd.jinr.ru/
wp-content/uploads/2019/01/TDR ECAL v3.6 2019.pdf

[18] P. Ai et al., “Timing and characterization of shaped pulses with MHz
ADCs in a detector system: a comparative study and deep learning
approach,” J. Instrum., vol. 14, no. 03, p. P03002, Mar 2019.

[19] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee, “Understanding Deep

Neural Networks with Rectiﬁed Linear Units,” in Proc. ICLR, 2018.

[20] J. C. Ye, Geometry of Deep Learning: A Signal Processing Perspective.

Springer Nature Singapore, 2022, pp. 195–226.

[21] M. Abadi et al., “TensorFlow: Large-Scale Machine Learning on
Heterogeneous Distributed Systems,” CoRR, 2016. [Online]. Available:
http://arxiv.org/abs/1603.04467

[22] F. Chollet, “Keras,” https://github.com/keras-team/keras, 2015.
[23] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Net-
works for Biomedical Image Segmentation,” in Proc. MICCAI. Cham:
Springer International Publishing, 2015, pp. 234–241.

[24] B. Jacob et al., “Quantization and Training of Neural Networks for
Efﬁcient Integer-Arithmetic-Only Inference,” in Proc. CVPR, 2018, pp.
2704–2713.

[25] P. Ai et al., “PulseDL: A reconﬁgurable deep learning array processor
dedicated to pulse characterization for high energy physics detectors,”
Nucl. Instrum. Methods Phys. Res. A, Accel. Spectrom. Detect. Assoc.
Equip., vol. 978, p. 164420, 2020.

[26] J.-L. Chen et al., “FPGA implementation of neural network accelerator
for pulse information extraction in high energy physics,” Nucl. Sci. Tech.,
vol. 31, no. 5, p. 46, Apr 2020.
[27] Arm Limited, https://www.arm.com/.
[28] Y. Fan et al., “Research and Veriﬁcation on Real-Time Interpolated
Timing Algorithm Based on Waveform Digitization,” IEEE Trans. Nucl.
Sci., vol. 67, no. 10, pp. 2246–2254, 2020.

0.30.20.10.00.10.20.3time (ns)01234probability densityInterpolated CFD (diff. of two channels)mean = 0.021 ns, std. = 0.094 ns94 ps(66 ps)0.20.10.00.10.2time (ns)012345probability densityFloating-point NN (diff. of two channels)mean = 0.013 ns, std. = 0.083 ns83 ps(59 ps)0.20.10.00.10.2time (ns)012345probability densityQuantized NN (diff. of two channels)mean = -0.009 ns, std. = 0.085 ns85 ps(60 ps)120012201240126012801300integral area0.0000.0050.0100.0150.020probability densityWaveform integration, resolution = 1.355%mean = 1255.293, std. = 17.0061.36%0.9951.0001.005normalized energy050100150probability densityFloating-point NN, resolution = 0.231%mean = 0.999, std. = 0.0020.23%0.9900.9951.0001.0051.0101.015normalized energy020406080100probability densityQuantized NN, resolution = 0.402%mean = 1.001, std. = 0.0040.40%