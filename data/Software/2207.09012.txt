SS-MFAR:SEMI-SUPERVISED MULTI-TASK FACIAL AFFECT
RECOGNITION

2
2
0
2

g
u
A
5

]

V
C
.
s
c
[

2
v
2
1
0
9
0
.
7
0
2
2
:
v
i
X
r
a

Dr. Darshan Gera
SSSIHL, Brindavan Campus
Bengaluru, Karnataka, India
darshangera@sssihl.edu.in

Badveeti Naveen Siva Kumar
SSSIHL, Prasanthi Nilayam Campus
Sri Sathya Sai District, Andhra Pradesh, India
bnaveensivakumar@gmail.com

Bobbili Veerendra Raj Kumar
SSSIHL, Prasanthi Nilayam Campus
Sri Sathya Sai District, Andhra Pradesh, India
veerendra.rajkumar@gmail.com

Dr. S Balasubramanian
SSSIHL, Prasanthi Nilayam Campus
Sri Sathya Sai District, Andhra Pradesh, India
sbalasubramanian@sssihl.edu.in

August 8, 2022

ABSTRACT

Automatic affect recognition has applications in many areas such as education, gaming, software
development, automotives, medical care, etc. but it is non trivial task to achieve appreciable perfor-
mance on in-the-wild data sets. In-the-wild data sets though represent real-world scenarios better
than synthetic data sets, the former ones suffer from the problem of incomplete labels. Inspired by
semi-supervised learning, in this paper, we introduce our submission to the Multi-Task-Learning
Challenge at the 4th Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition. The
three tasks that are considered in this challenge are valence-arousal(VA) estimation, classiﬁcation of
expressions into 6 basic (anger, disgust, fear, happiness, sadness, surprise), neutral, and the ’other’
category and 12 action units(AU) numbered AU-{1,2,4,6,7,10,12,15,23,24,25,26}. Our method
Semi-supervised Multi-task Facial Affect Recognition titled SS-MFAR uses a deep residual net-
work with task speciﬁc classiﬁers for each of the tasks along with adaptive thresholds for each
expression class and semi-supervised learning for the incomplete labels. Source code is available at
https://github.com/1980x/ABAW2022DMACS.

Keywords Multi Task Learning · Semi-supervised Learning · Facial Expression Recognition · AffWild2

1

Introduction

Automatic affect recognition is currently an active area of research and has applications in many areas such as
education, gaming, software development, auto motives [1, 2], medical care, etc. Many works have dealt with
Valence-arousal estimation [3, 4, 5, 6, 7], action unit detection [8, 9, 10], expression classiﬁcation [] tasks individually.
[11] introduced a framework which uses only static images and Multi-Task-Learning (MTL) to learn categorical
representations and use them to estimate dimensional representation, but is limited to AffectNet data set [12]. Aff-wild2
[13, 14, 15, 16, 17, 18, 19, 20, 21, 22] is the ﬁrst dataset with annotations with all three tasks. [17] study uses MTL
for all the three tasks mentioned earlier along with facial attribute detection and face identiﬁcation as case studies to
show that their network FaceBehaviourNet learns all aspects of facial behaviour. It is shown to perform better than the
state-of-the-art models of individual tasks. One of major limitation of Affwild2 is that annotations of valence-arousal,
AU and expression are not available for all the samples. So, this dataset has incomplete labels for different tasks
and further imbalanced for different expression classes. In a recent work, [23] uses semi-supervised learning with
adaptive conﬁdence margin for expression classiﬁcation task for utilizing unlabelled samples. Adaptive conﬁdence
margin is used to deal with inter and intra class difference in predicted probabilities for each expression class. In this
work, we developed a Multi-task Facial Affect Recognition (MFAR) in which a ResNet-18 [24] network pre-trained

 
 
 
 
 
 
A PREPRINT - August 8, 2022

on MS-Celeb-1M [25] weights is used, along with task speciﬁc classiﬁers for the three tasks. To handle the class
imbalance in the dataset, we introduced re-weighting. From the s-Aff-Wild2 [26] dataset, we observed that out of
142383 a total of 51737 images have invalid expression annotations. Motivated from [23], we added semi-supervised
learning (SS-MFAR) to label the images with invalid expression annotations.

2 Method

In this section, we present our solution to the MTL Challenge at the 4th Affective Behavior Analysis in-the-wild
(ABAW) Competition.

2.1 MFAR : Multi-Task Facial Affect Recognition

MFAR architecture is shown in the Figure 1. Weak Augmentations (xw) of input images which have valid annotations
are fed to the network and the outputs from each of the tasks are obtained. In s-Aff-Wild2 dataset which is the training
set used valid annotations for i) expression is any integer value in {0,1,2,. . . ,7}, ii) action unit annotations is either 0 or
1 and iii) valence-arousal is any value in the range [-1, 1], but there are images that have expression and action unit
labels annotated with −1, and valence-arousal values annotated as −5 which are treated as invalid annotations. Outputs
obtained from each of the task speciﬁc classiﬁer is used to ﬁnd task related loss and then combined to give the overall
loss3 the MFAR network minimizes. Losses used for each task are Cross Entropy, Binary Cross Entropy, Concordance
Correlation Coefﬁcient based loss for expression classiﬁcation, action unit detection and valence-arousal estimation
tasks respectively. Re-scaling weight for each class was calculated using Eq.1 and used to counter the imbalance class
problem. If NExp is the total number of valid expression samples and nExp[i] is the number of valid samples for each
expression class, then weights for a class i is denoted by WExp[i] and is deﬁned as

WExp[i] =

NExp
nExp[i]

(1)

Positive weight, which is the ratio of negative samples and positive samples for each action unit as in Eq.2 is used in
binary cross entropy loss to counter the imbalance in the classes that each action unit can take. If N P
AU [i] is the number
of positive samples and N N
AU [i] is the number of negative samples for each action unit, then positive weight for action
unit i is denoted by WAU [i] and is deﬁned as

WAU [i] =

N N
N P

AU [i]
AU [i]

The overall loss is the sum of losses of each task. MFAR network minimizes the overall loss function3.

LM F AR = LV A + LAU + LExp

(2)

(3)

Figure 1: MFAR architecture is shown in this ﬁgure. Weak Augmentations (xw) of input images with valid annotations
are fed to the network and the outputs from each of the tasks are obtained. Losses for each task are added to get the
overall loss function3, that MFAR minimizes. CE is cross entropy5, BCE is binary cross entropy loss6, CCC loss is
Concordance Correlation Coefﬁcient based loss7.

2

A PREPRINT - August 8, 2022

2.2 SS-MFAR : Semi-Supervised Multi-Task Facial Affect Recognition

SS-MFAR architecture is shown in the Figure 2. SS-MFAR is our proposed solution to the MTL challenge. SS-MFAR
adds semi-supervised learning to MFAR on the images that have invalid expression annotations. The images that have
invalid expression annotations i.e., have expression label as -1 are treated as unlabelled data and therefore inspired from
[23] we use semi-supervised learning to predict their labels thereby utilizing more images than MFAR2.1 along with
adaptive threshold to deal with inter and intra class differences in the prediction probabilities that arise due to input
samples that are easy and hard within each expression class and among the expression classes. The overall loss function
SS-MFAR minimizes is the sum of losses of each task as shown in Eq.9. Losses3 with respect to AU detection task and
VA estimation task are same as in MFAR. For loss with respect to expression task, we use weighted combination of CE
on labelled samples, CE on conﬁdent predictions of xs and xw, KL on non-conﬁdent predictions of xw and xs, where
CE is cross entropy loss with class-speciﬁc re-scaling weights1, KL is symmetric KL divergence as given in Eq.8.

Figure 2: SS-MFAR architecture of our proposed solution. Along with obtaining predictions on the input like MFAR,
adaptive threshold for each expression class is learnt. This adaptive threshold is used to mark the predictions on input
images as conﬁdent and non-conﬁdent ones. CE is cross entropy loss5 and is used on the conﬁdent predictions of weak
and strong augmentations, KL is symmetric kl-divergence8 between the probability distributions of non-conﬁdent weak
and strong predictions and the remaining losses are as in MFAR.

3

2.3 Problem formulation

A PREPRINT - August 8, 2022

), action unit annotations yAU

Let D = {(xi, ˜yi)}N
i=1 be the dataset of N samples. Here xi is ith image where ˜yi represents expression class
yExp
) of ith image. The backbone network
i
is parameterized by θ ( ResNet-18 pre-trained on MS-Celeb-1M is used as the backbone). We denote xw as weak
augmented image and xs as strong augmented image, PExp represent probability distribution predicted by Expression
classiﬁer, PAU represent probability of action unit predicted by AU classiﬁer and PV A of valence arousal predicted
by VA classiﬁer. Weak augmentations include standard cropping and horizontal ﬂipping of the input image. Strong
augmentations includes weak augmentations along with Randaugment [27].

) and valence arousal annotations yV A

i

i

2.4 Adaptive Threshold

To tackle inter and intra class differences in prediction probabilities of expression, we use adaptive threshold for each
class. Here we generate the threshold based on the predictions probabilities whose prediction label matches the ground
truth label and threshold is also a function of epoch number since the discriminative ability of the model increases
as epoch number increases.We denote the ground truth labels with ˜yi for image xi ,ep as epoch number, pi denotes
prediction probabilities of the image i and we denote adaptive threshold as T c where c ∈ {1, 2, ..., 8} deﬁned as:

β ∗ ( 1
N s

T c =

i ∗ pi)

(cid:80)N s

i=1 δc
1 + γ−ep
(cid:40) 1

if

˜yi = c,

0

otherwise.

δc
i =

, where

(4)

The values β = 0.95 and γ = e are taken from [23]

2.5 Supervision loss

Supervision loss is computed based on the weak augmented images xw that have valid annotations for each task. We
use Cross Entropy loss for the expression classiﬁcation, binary cross entropy loss for AU detection and Concordance
Correlation Coefﬁcient loss for VA estimation.

Cross Entropy loss denoted by LCE((X, Y ), θ) used for expression classiﬁcation is

Ls

CE = (−

8
(cid:88)

c=1

˜yExpc
i=1 log(pc(xi, θ))))

Binary Cross Entropy loss denoted by LBCE((X, Y ), θ) used for action unit detection is

LBCE = (−

2
(cid:88)

c=1

˜yAU c
i=1 log(pc(xi, θ))))

Concordance Correlation Coefﬁcient loss denoted by LCCC((X, Y ), θ) used for valence arousal estimation is

LCCC = 1 −

2 ∗ sxy
x + (¯x − ¯y)2

x + s2
s2

(5)

(6)

(7)

2.6 Unsupervised and Consistency Loss

Unsupervised Loss and Consistency Loss are used to learn from the images that have invalid expression annotations. We
send weak augmented and strong augmented images to the expression classiﬁer and based on the threshold generated
earlier, we mark all the images that we have taken into conﬁdent and non-conﬁdent predictions. Unsupervised Loss
is Cross Entropy loss denoted by Lu
CE on the conﬁdent logits of strong augmentations and labels predicted on weak
augmentations.Consistency Loss is symmetric KL loss on the predicted probability distributions of weak and strong
augmentations.

Symmetric KL-loss is deﬁned for (p,q) probability distributions as :

Lc

KL = p ∗ log(

p
q

) + q ∗ log(

q
p

)

4

(8)

2.7 Overall Loss

A PREPRINT - August 8, 2022

Overall loss is linear combination of losses from each task, where loss from Expression classiﬁcation is deﬁned as
LExp = λ1 ∗ Ls
KL where λ1 = 0.5, λ2 = 1, λ3 = 0.1 taken from [23] , loss from AU detection
is deﬁned as LAU = LBCE and loss from VA estimation is deﬁned as LV A = LCCC. Overall loss is deﬁned as

CE + λ2 ∗ Lu

CE + λ3 ∗ Lc

LOverall = LExp + LAU + LV A

(9)

for i =1,2,3,...,NB

Algorithm 1: Multi-Task Facial Affect Recognition Using Semi-Supervised Learning
INPUT: dataset(D), parameters(θ), model(ResNet-18) pretrained on MS-Celeb,η(learning rate)
1.for epoch = 1,2,3,...,epochmax
2.
3.
4.
5.
6.
7.
8.
9.

Obtain logits from model which has valid annotations for each task from xw
Obtain Adaptive threshold(Tc) according to Eq. 4
Obtain logits for xw and xs for invalid annotations of expression class
If prediction probability pi > Tc:

Compute overall loss according to Eq. 9 using Eqs. 5,6,7,8
Update θ using Eq. 9

Mark those images predictions as conﬁdent otherwise mark them non conﬁdent.

3 Dataset and Implementation Details

3.1 Dataset

s-AffWild2 [26] database is a static version of Aff-Wild2 database and contains a total of 220419 images. It divided
into training, validation and test sets with 142383, 26877 and 51159 number of images respectively. The following
observations were made with respect to training data:

• 38465, 39066, 51737 images have invalid valence-arousal, action unit, expression annotations respectively.

• for an image if valence has an invalid annotation, then so is arousal.

• for an image if one of the action unit’s has invalid annotation, then so does all the other.

Cropped-aligned images were used for training the network. The dataset contains valence-arousal, expression and
action unit annotations. Values of valence-arousal are in the range[-1,1], expression labels span over eight classes
namely anger, disgust, fear, happiness, sadness, surprise, neutral, and other. 12 action units were considered, speciﬁcally
AU-1,2,4,6,7,10,12,15,23,24,25,26.

3.2

Implementation Details

MFAR model consists of ResNet-18 network loaded with pre-trained weights of MS-celeb-1M, feature maps of the
input were taken out from the last third layer of network which were fed through average pool, dropout and ﬂatten
layers to obtain features of the image which were then normalized. These normalized features were then passed to each
of the task speciﬁc network for further processing. The expression classiﬁer, which is a multi-class classiﬁer consists of
a linear layer with ReLU activation followed by a linear layer with output dimensions equal to the number of expression
classes(8) addressed in this challenge. The action unit classiﬁer consists of 12 binary classiﬁers which are addressed as
part of this challenge. For valence arousal task the normalized features were passed through a fully connected layer with
ReLU activation to obtain the output logits. SS-MFAR model uses the same network as MFAR but gets the predictions
of invalid expression annotations and uses these predictions with appropriate losses to better the overall performance on
the given task. The proposed methods were implemented in PyTorch using GeForce RTX 2080 Ti GPUs with 11GB
memory. The backbone network used is ResNet-18 pre-trained on large scale face dataset MS-Celeb-1M. All the
cropped-aligned of s-Aff-Wild2, provided by organizers were used after resizing them to 224x224. Batch size is set to
256. Optimizer used is Adam. Learning rate (lr) is initialized as 0.001 for base networks and 0.01 for the classiﬁcation
layer.

3.3 Evaluation Metrics

The overall performance score used for MTL challenge consists of :

5

A PREPRINT - August 8, 2022

• sum of the average of Concordance Correlation Coefﬁcient (CCC) of valence and arousal,

• average F1 Score of the 8 expression categories (i.e., macro F1 Score)

• average F1 Score of the 12 action units (i.e., macro F1 Score)

The overall performance for the MTL challenge is given by:

PM T L = PV A + PAU + PExp =

ρa + ρv
2

+

(cid:80)

Exp F Exp
8

1

CCC is deﬁned as

ρc =

2 ∗ sxy
x + (¯x − ¯y)2

x + s2
s2

+

(cid:80)

AU F AU
1
12

(10)

(11)

F1 score is deﬁned as harmonic mean of precision (i.e. Number of positive class images correctly identiﬁed out of
positive predicted) and recall (i.e. Number of positive class images correctly identiﬁed out of true positive class). It can
be written as:

(12)

F1 =

3.4 Experiments

2 ∗ precision ∗ recall
precision + recall

Along with the models presented until now many other approaches to the given challenge were tried out. We list few of
them here and the results obtained in Table 1.

• In order to deal with the expression class imbalance, instead of using the re-weighting technique on MFAR,

re-sampling technique was used. We call this approach MFAR-RS.

• To see the importance of consistency loss we ran a model without KL loss 8. We call this approach SS-MFAR-

NO_KL.

• In order to deal with the expression class imbalance, instead of using the re-weighting technique on SS-MFAR,

re-sampling technique was used. We call this approach SS-MFAR-RS.

• Instead of using the pre-trained weights of MS-Celeb-1M, network pre-trained on AffectNet [12] database was

used. We call this approach SSP-MFAR.
• Instead of just using unsupervised loss Lu

CE on only non-conﬁdent expression predictions, unsupervised loss
on xs and xw of all the images were added to respective task losses. We call this approach SSP-MFAR-SA.
• Instead using a backbone network of ResNet-18 we used a backbone network ResNet-50 on the best performing

model without any pre-trained weights. We call this approach SS-MFAR-50.

4 Results and discussion

We report our results on the ofﬁcial validation set of MTL from the ABAW 2022 Challenge [26] in Table 1 . Our best
performance achieves overall score of 1.125 on validation set which is a signiﬁcant improvement over baseline.

Table 1: Performance comparison on s-Aff-Wild2 validation set

Method
Baseline [26]
MFAR
MFAR-RS
SS-MFAR
SS-MFAR-RS
SS-MFAR-NOKL
SSP-MFAR
SSP-MFAR-SA
SS-MFAR-50

Exp-F1 score AU-F1 score VA-Score Overall

-
0.222
0.191
0.235
0.256
0.205
0.233
0.228
0.168

-
0.493
0.40
0.493
0.461
0.454
0.497
0.484
0.425

-
0.328
0.375
0.397
0.361
0.357
0.378
0.40
0.296

0.30
1.043
0.966
1.125
1.078
1.016
1.108
1.112
0.889

In addition, we present results on validation set of Synthetic Data Challenge using the proposed method in Table 2 for 6
expression classes.

6

A PREPRINT - August 8, 2022

Table 2: Performance comparison on validation set of Synthetic Data Challenge

Method
Baseline [26]
SS-MAFR

Exp-F1 score
0.30
0.587

4.1 Ablation Studies

In this section we present our studies with multiple approaches taken in order to solve the given problem in this
challenge.

4.1.1 Inﬂuence of Resampling

We see from Table 1 that resampling technique used in MFAR-RS doesn’t boost the performance of the model as much
as re-weighting does in SS-MFAR.

4.1.2 Impact of Consistency loss

We can see from the Table 1 the ill effect on model performance when consistency loss 8 is not used. SS-MFAR-NOKL
when compared with SS-MFAR, the overall performance drops by 0.103.

4.1.3 Effect of pretrained weights

SS-MFAR used the pre-trained weights of MS-Celeb-1M, similarly SSP-MFAR used the pre-trained weights of
AffectNet database and obtained relatable performance. We also ran an experiment using the model of SS-MFAR
without pre-trained weights but obtained poor performance, similarly we see SS-MFAR-50 performance is on par with
SS-MFAR model.

4.2 Performance on Test set

The performance of SS-MFAR on ofﬁcial test set of MTL challenge is presented in Table 3 and on ofﬁcial test set
of LSD challenge is presented in Table 4. Clearly, our model performs signiﬁcantly better than Baseline for both the
challenges. In case of MTL challenge, SS-MFAR is able to perform better than methods like ITCNU, USTC-AC
[28], DL-ISIR. Even though its performance is lower compared to methods like HUST-ANT [29], STAR-2022 [30],
CNU-Sclab [31], HSE-NN [32] and Situ-RUCAUM3 [33], our method is ﬁrst of its kind to use all the samples for
representation learning based on semi-supervised learning and it is computationally efﬁcient. In case of LSD challenge,
our method by using a simple residual network is able to outperform methods like USTC-AC [34] and STAR-2022 [30].

Table 3: Performance comparison on s-Aff-Wild2 Test set of MTL challenge (Refer https://ibug.doc.ic.ac.uk/
resources/eccv-2023-4th-abaw/ for *)

Method
Baseline [26]
ITCNU*
USTC-AC [28]
DL_ISIR*
HUST-ANT [29]
STAR-2022 [30]
CNU-Sclab [31]
HSE-NN [32]
Situ-RUCAIM3 [33]
SS-MFAR

Overall
28.00
68.85
93.97
101.87
107.12
108.55
111.35
112.99
143.61
104.06

5 Conclusions

In this paper, we presented our proposed Semi-supervised learning based Multi-task Facial Affect Recognition
framework (SS-MFAR) for ABAW challenge conducted as a part of ECCV 2022. SS-MFAR used all the samples for
learning the features for expression classiﬁcation, valence-arousal estimation and action unit detection by learning

7

Table 4: Performance comparison on Test set of LSD challenge

A PREPRINT - August 8, 2022

Method
Baseline [26]
USTC-AC [28]
STAR-2022 [30]
HUST-ANT [29]
ICT-VIPL*
PPAA*
HSE-NN [32]
SS-MFAR

Overall
30.00
30.92
32.40
34.83
34.83
36.51
37.18
33.64

adaptive conﬁdence margin. This adaptive conﬁdence margin was used to select conﬁdent samples for supervised
learning from different expression classes overcoming inter class difﬁculty and intra class size imbalance. The non-
conﬁdent samples were used by minimizing the unsupervised consistency loss between weak and strong augmented
view of input image. The experiments demonstrate the superiority of proposed method on s-Aff-Wild2 dataset.

Acknowledgments

We dedicate this work to Our Guru and Guide Bhagawan Sri Sathya Sai Baba, Divine Founder Chancellor of Sri Sathya
Sai Institute of Higher Learning, Prasanthi Nilayam, Andhra Pradesh, India.

References

[1] Agata Kołakowska, Agnieszka Landowska, Mariusz Szwoch, Wioleta Szwoch, and Michał Wróbel. Emotion

recognition and its applications. Advances in Intelligent Systems and Computing, 300:51–62, 07 2014.

[2] Khalid Zaman, Zhaoyun Sun, Sayyed Mudassar Shah, Muhammad Shoaib, Lili Pei, and Altaf Hussain. Driver
emotions recognition based on improved faster r-cnn and neural architectural search network. Symmetry, 14(4),
2022.

[3] Geesung Oh, Euiseok Jeong, and Sejoon Lim. Causal affect prediction model using a past facial image sequence.

In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3550–3556, 2021.

[4] Sebastian Handrich, Laslo Dinges, Frerk Saxen, Ayoub Al-Hamadi, and Sven Wachmuth. Simultaneous prediction
of valence / arousal and emotion categories in real-time. In 2019 IEEE International Conference on Signal and
Image Processing Applications (ICSIPA), pages 176–180, 2019.

[5] Geesung Oh, Euiseok Jeong, and Sejoon Lim. Causal affect prediction model using a facial image sequence.

arXiv preprint arXiv:2107.03886, 2021.

[6] Liyu Meng, Yuchen Liu, Xiaolong Liu, Zhaopei Huang, Wenqiang Jiang, Tenggan Zhang, Chuanhe Liu, and
Qin Jin. Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2345–2352, 2022.

[7] Songyou Peng, Le Zhang, Yutong Ban, Meng Fang, and Stefan Winkler. A deep network for arousal-valence

emotion prediction with acoustic-visual cues. arXiv preprint arXiv:1805.00638, 2018.

[8] Geethu Miriam Jacob and Bjorn Stenger. Facial action unit detection with transformers. In Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7680–7689, 2021.

[9] Zhiwen Shao, Zhilei Liu, Jianfei Cai, Yunsheng Wu, and Lizhuang Ma. Facial action unit detection using attention

and relation learning. IEEE transactions on affective computing, 2019.

[10] Chuangao Tang, Wenming Zheng, Jingwei Yan, Qiang Li, Yang Li, Tong Zhang, and Zhen Cui. View-independent
facial action unit detection. In 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition
(FG 2017), pages 878–882. IEEE, 2017.

[11] Wang Xiaohua, Peng Muzi, Pan Lijuan, Hu Min, Jin Chunhua, and Ren Fuji. Two-level attention with two-stage
multi-task learning for facial emotion recognition. Journal of Visual Communication and Image Representation,
62:217–225, 2019.

[12] Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A database for facial expression,

valence, and arousal computing in the wild. IEEE Transactions on Affective Computing, 2017.

8

A PREPRINT - August 8, 2022

[13] Dimitrios Kollias and Stefanos Zafeiriou. Aff-wild2: Extending the aff-wild database for affect recognition. arXiv

preprint arXiv:1811.07770, 2018.

[14] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect, action unit recognition: Aff-wild2, multi-task

learning and arcface. arXiv preprint arXiv:1910.04855, 2019.

[15] Dimitrios Kollias and Stefanos Zafeiriou. A multi-task learning & generation framework: Valence-arousal, action

units & primary expressions. arXiv preprint arXiv:1811.07771, 2018.

[16] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos Zafeiriou. Analysing affective behavior in the ﬁrst

abaw 2020 competition. arXiv preprint arXiv:2001.11409, 2020.

[17] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou. Distribution matching for heterogeneous

multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790, 2021.

[18] Dimitrios Kollias and Stefanos Zafeiriou. Affect analysis in-the-wild: Valence-arousal, expressions, action units

and a uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021.

[19] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou, Athanasios Papaioannou, Guoying Zhao, Björn
Schuller, Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction in-the-wild: Aff-wild database and
challenge, deep architectures, and beyond. International Journal of Computer Vision, pages 1–23, 2019.
[20] Dimitrios Kollias and Stefanos Zafeiriou. Va-stargan: Continuous affect generation. In International Conference

on Advanced Concepts for Intelligent Vision Systems, pages 227–238. Springer, 2020.

[21] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou, Athanasios Papaioannou, Guoying Zhao, and Irene
Kotsia. Aff-wild: Valence and arousal ‘in-the-wild’challenge. In Computer Vision and Pattern Recognition
Workshops (CVPRW), 2017 IEEE Conference on, pages 1980–1987. IEEE, 2017.

[22] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoying Zhao, and Stefanos Zafeiriou. Recognition of affect
in the wild using deep neural networks. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017
IEEE Conference on, pages 1972–1979. IEEE, 2017.

[23] Hangyu Li, Nannan Wang, Xi Yang, Xiaoyu Wang, and Xinbo Gao. Towards semi-supervised deep facial
expression recognition with an adaptive conﬁdence margin. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 4166–4175, 2022.

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[25] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark
for large-scale face recognition. In European conference on computer vision, pages 87–102. Springer, 2016.
[26] Dimitrios Kollias. Abaw: Learning from synthetic data & multi-task learning challenges. arXiv preprint

arXiv:2207.01138, 2022.

[27] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical data augmentation with

no separate search. CoRR, abs/1909.13719, 2019.

[28] Yanan Chang, Yi Wu, Xiangyu Miao, Jiahe Wang, and Shangfei Wang. Multi-task learning for emotion descriptors

estimation at the fourth abaw challenge. arXiv preprint arXiv:2207.09716, 2022.

[29] Siyang Li, Yifan Xu, Huanyu Wu, Dongrui Wu, Yingjie Yin, Jiajiong Cao, and Jingting Ding. Facial affect
analysis: Learning from synthetic data & multi-task learning challenges. arXiv preprint arXiv:2207.09748, 2022.
[30] Lingfeng Wang, Haocheng Li, and Chunyin Liu. Hybrid cnn-transformer model for facial affect recognition in the

abaw4 challenge. arXiv preprint arXiv:2207.10201, 2022.

[31] Dang-Khanh Nguyen, Sudarshan Pant, Ngoc-Huynh Ho, Guee-Sang Lee, Soo-Huyng Kim, and Hyung-Jeong
Yang. Multi-task cross attention network in facial behavior analysis. arXiv preprint arXiv:2207.10293, 2022.
[32] Andrey V Savchenko. Hse-nn team at the 4th abaw competition: Multi-task emotion recognition and learning

from synthetic images. arXiv preprint arXiv:2207.09508, 2022.

[33] Tenggan Zhang, Chuanhe Liu, Xiaolong Liu, Yuchen Liu, Liyu Meng, Lei Sun, Wenqiang Jiang, and Fengyuan
Zhang. Emotion recognition based on multi-task learning framework in the abaw4 challenge. arXiv e-prints,
pages arXiv–2207, 2022.

[34] Xiangyu Miao, Jiahe Wang, Yanan Chang, Yi Wu, and Shangfei Wang. Hand-assisted expression recognition

method from synthetic images at the fourth abaw challenge. arXiv preprint arXiv:2207.09661, 2022.

9

