2
2
0
2

y
a
M
6
1

]

C
D
.
s
c
[

1
v
6
9
6
7
0
.
5
0
2
2
:
v
i
X
r
a

Let‚Äôs Trace It: Fine-Grained Serverless Benchmarking using
Synchronous and Asynchronous Orchestrated Applications

Joel Scheuner
Chalmers | University of Gothenburg
Sweden
scheuner@chalmers.se

Simon Eismann
University of W√ºrzburg
Germany
eismann@uni-wuerzburg.de

Sacheen Talluri
Vrije Universiteit Amsterdam
The Netherlands
S.Talluri@atlarge-research.com

Erwin van Eyk
Vrije Universiteit Amsterdam
The Netherlands
E.vanEyk@atlarge-research.com

Cristina L. Abad
Escuela Superior Politecnica del Litoral
Ecuador
cabadr@espol.edu.ec

Philipp Leitner
Chalmers | University of Gothenburg
Sweden
philipp.leitner@chalmers.se

Alexandru Iosup
Vrije Universiteit Amsterdam
The Netherlands
A.Iosup@atlarge-research.com

Abstract
Making serverless computing widely applicable requires de-
tailed performance understanding. Although contemporary
benchmarking approaches exist, they report only coarse re-
sults, do not apply distributed tracing, do not consider asyn-
chronous applications, and provide limited capabilities for
(root cause) analysis. Addressing this gap, we design and
implement ServiBench, a serverless benchmarking suite.
ServiBench (i) leverages synchronous and asynchronous
serverless applications representative of production usage, (ii)
extrapolates cloud-provider data to generate realistic work-
loads, (iii) conducts comprehensive, end-to-end experiments
to capture application-level performance, (iv) analyzes results
using a novel approach based on (distributed) serverless trac-
ing, and (v) supports comprehensively serverless performance
analysis. With ServiBench, we conduct comprehensive exper-
iments on AWS, covering Ô¨Åve common performance factors:
median latency, cold starts, tail latency, scalability, and dy-
namic workloads. We Ô¨Ånd that the median end-to-end latency
of serverless applications is often dominated not by function
computation but by external service calls, orchestration, or
trigger-based coordination. We release collected experimen-
tal data under FAIR principles and ServiBench as a tested,
extensible open-source tool.

1

Introduction

Establishing computing infrastructure that is easy to manage
yet performs well for all applications is a longstanding goal
of the computer systems community from the 1950s [85].
Emerging in the late 2010s from the integration of mul-
tiple technological breakthroughs [76], serverless comput-
ing [13, 64, 75] aims to abstract away operational concerns
(e.g., autoscaling) from the developer by providing fully
managed cloud platforms through self-serving application
programming interfaces (APIs). Developers can leverage a
rich ecosystem of external services (e.g., message queues,
databases, image recognition), which are glued together by a
Function-as-a-Service (FaaS) platform, such as AWS Lambda.
For the current generation of serverless platforms, ease of
management comes with important performance trade-offs
and issues, including high tail-latency and performance vari-
ability [26,55], and delays introduced by asynchronous use of
external services [55,58]. Thus, understanding and comparing
the performance of serverless platforms is essential. Although
extensive prior work exists in empirical performance evalua-
tion [24, 38, 47] and analysis [37, 81], synthetic [23, 26] and
micro-benchmarking [63], as well as server-side benchmark-
ing [65, 73], there currently exists no serverless benchmark

1

 
 
 
 
 
 
that provides detailed (white-box) analysis at application level,
covering production applications and invocation patterns. Ad-
dressing this gap, in this work we design, implement, and use
ServiBench (sb), an application-level, serverless benchmark-
ing suite based on distributed tracing.

We posit in this paper that a serverless benchmark that
provides performance information at application level is
necessary. Our argument is two-fold. First, a variety of
production-ready serverless applications already exist [13,
22, 39]. These applications have distinctive performance
proÔ¨Åles but alternative approaches insufÔ¨Åciently cover this
diversity (e.g., in external services). Second, application-
level benchmarks have proven useful in related Ô¨Åelds (e.g.,
DeathStarBench [26] for container-based microservices, and
CloudSuite [23] for scale-out workloads). They help identify
architectural bottlenecks, guide application design decisions,
and inspire better programming models.

The challenge of application-level serverless benchmark-
ing is manifold and complex. First, there is a need to design
the benchmarks, and to validate the tools that realize the
benchmark in practice (challenge C1). The majority of ex-
isting work attempts to reverse-engineer commercial server-
less systems [81], by characterizing performance aspects such
as allocated CPU power by memory size [24], cold start over-
head [47], scaling policies [37], or I/O speed [38]. Such
studies can guide cloud users in selecting appropriate services
and conÔ¨Ågurations, but their empirical results are prone to be-
come obsolete quickly. Server-side experimentation [65, 73]
allows to control the entire serverless stack to obtain detailed
proÔ¨Åling data, but lacks tight integration with external ser-
vices, which prevents realistic system-level testing of server-
less applications [40]. Finally, micro-benchmarks (e.g., of
CPU performance) are not representative of real applications.
Summarizing, no benchmark currently: (i) supports a variety
of architectural patterns that appear commonly in serverless
applications, (ii) includes representative, production-grade
applications and invocation patterns, (iii) provides end-to-end
performance data and white-box analysis capabilities, and
(iv) enables reproducible real-world experiments.

Second, conducting white-box analysis requires collect-
ing end-to-end performance data, and extracting Ô¨Åne-
grained latency information (C2). Distributed tracing [45,
61] has been popularized at Google [67] and Facebook [77],
but requires a level of cooperation with the platform that is not
available for serverless applications. Various approaches exist
for this broad class of problems that assume ordered events
and accurate timing, and such approaches can already be use-
ful for synchronous microservice architectures [57]. In con-
trast, many serverless applications use multiple functions and
external services, and invocations can occur asynchronously.
No benchmark in the Ô¨Åeld currently addresses this.

Third, we identify the need to share tools and
data for serverless benchmarking, FAIRly (C3). Releas-
ing software, data, and results in packages that make

Figure 1: System model of a serverless application composed
of multiple functions: (cid:102)1 and (cid:102)2 are user-deÔ¨Åned functions,
and (cid:101)1 is an external service.

them FAIR (‚ÄúÔ¨Åndable, accessible,
reusable‚Äù [83]) is essential in science and engineering.

interoperable, and

We address these challenges with a four-fold contribution:

1. Application benchmark suite (Section 3): Addressing
C1, we present ServiBench, a comprehensive benchmark
suite. In it, 10 realistic open-source applications cover
different forms of orchestration, synchronous and asyn-
chronous triggers, and real-world characteristics such
as programming language, size, and external service us-
age. ServiBench orchestrates reproducible deployments,
automates realistic load generation, collects distributed
traces, and provides detailed white-box analysis.

2. Latency breakdown analysis (Section 4): Addressing
C2, we design novel algorithms and heuristics for de-
tailed latency breakdown analysis of distributed server-
less traces. The key capability over prior work is that
our approach works in a serverless context, across asyn-
chronous call boundaries and external services.

3. Empirical performance study (Section 5): Address-
ing the validation aspects of C1, and the overall chal-
lenge of understanding the performance of serverless ap-
plications, we conduct a comprehensive white-box anal-
ysis of serverless application performance in the AWS
environment (see also Section 6). Our results cover, e.g.,
cold starts, tail latency, and the impact of application
type and invocation patterns on performance.

4. FAIR release of the ServiBench software, data, and
results (Appendix A): Addressing C3, we release
ServiBench on Github, and the conÔ¨Ågurations and full
data (¬±50 GB) on Zenodo (links anonymized).

2 System Model for Serverless Applications

Our work assumes a system model, introduced by the SPEC
Research Group, that represents the operation of tens of ex-
isting serverless platforms [74]. In this model, the server-
less stack provides resources (label L1 in Figure 1), whose

2

Control FlowOrchestratorTrace PointL1 ResourcesL4 Operator ServicesL2 Resource OrchestrationL3 Function ManagementAutoscalerRegistryùëì1ùëì2‚ÑØ1TraceInitializationExecutionL5 FunctionsSync.TriggerAsync.Triggeruse is orchestrated ( L2 ), enabling complex function manage-
ment ( L3 ) such as auto-scaling and image-registry. These
elements have been covered extensively by the commu-
nity [9, 66].

The system model further includes two layers that service
directly the application and are the focus of this work. Each
applications is composed of a single or, more commonly,
multiple user-deÔ¨Åned functions ( L5 ), which can trigger asyn-
chronously external (operator-provided) services ( L4 ). Fig-
ure 1 depicts a constructed example of a serverless application.
There are two user-deÔ¨Åned functions ((cid:102)1 and (cid:102)2). During its
execution, (cid:102)1 triggers synchronously an operator-provided
service, (cid:101)1. At the end of its execution, (cid:102)1 triggers asyn-
chronously an operator-provided orchestrator service.

Production-level serverless applications Ô¨Åt this model well.
The functions can communicate with external services ((cid:101)1),
and with each other, directly via synchronous or asynchronous
triggers. They can also communicate via an orchestrator,
which decides on the control Ô¨Çow based on user-provided
instructions. They can thus construct complex execution
paths, using any of the operator-provided services, e.g., ob-
ject stores, databases, and ML services. A production work-
load can incorporate multiple invocation patterns, leading
to complex performance behavior that incorporates not only
intra-application latency, but also cross-application delays.

We further assume there exists a monitoring service, which
can provide detailed performance information and in particu-
lar trace the upper layers in the serverless stack (i.e., L4 and
L5 ). We do not assume that the other layers are observable
by the application; e.g., there is no server-side tracing. These
assumptions match the operation of common serverless plat-
forms, such as AWS. Although we assume the presence of
distributed tracing, we do not assume its results are consistent
and ordered across multiple components in the system or that
the application triggers can only occur synchronously. This
is consistent with how popular serverless platforms, such as
AWS and Microsoft Azure, operate in practice.

3 Design of ServiBench

In this section, we design ServiBench, an application-level
benchmarking suite. We design around a set of 5 design prin-
ciples, creating a novel architecture and benchmarking Ô¨Çow.
The suite includes 10 representative, open-source applica-
tions. ServiBench includes a process to generate represen-
tative workloads from existing invocation logs provided by
cloud operators such as Microsoft Azure.

In the system model introduced in Section 2, identifying
the latency contribution of each service to the total response
time of the application is challenging. In particular, an ap-
plication can take multiple paths of execution and trigger
asynchronously different methods depending on runtime con-
ditions, which breaks straightforward distributed tracing. We
address this concern through a detailed design, in Section 4.

3.1 Design Principles

Based on guidelines on benchmarking best-practices [29, 78]
and inspired by the microservice benchmark suite DeathStar-
Bench [26], we formulate the following design principles:

1. Representativeness: A representative and relevant
benchmark suite closely matches the characteristics of
real-world applications. We select applications from in-
dustrial workshops and academic studies based on the
most common serverless application types [13, 22], pro-
gramming languages [22, 39], application sizes [22, 39,
66], and external services [22, 39, 66].

2. End-to-end operation: An application-level serverless
benchmark should implement end-to-end functionality
starting from an incoming client request, following into
individual functions, across external services, ending
with a synchronous response or after a chain of asyn-
chronous event-based function triggers. We implement
realistic serverless applications and instrument them us-
ing distributed tracing to track end-to-end operation.

3. Heterogeneity: A heterogeneous benchmark suite
should include diverse applications by different dimen-
sions. Beyond including the most popular choice (e.g.,
most prevalent programming language), we strive for
generalizability by covering additional applications.

4. Reproducibility: A reproducible benchmark suite mit-
igates threats to internal validity that could affect the
ability to obtain the same results with the same method
under changed conditions of measurements [71]. We
provide automated containerized benchmark orchestra-
tion for all applications including their conÔ¨Ågurations
and pinned dependencies.

5. Extensibility: An extensible suite allows for adding ex-
isting serverless applications written in any program-
ming language, using any framework, or cloud service
dependencies with no or only minor code changes. We
demonstrate the extensibility of our plugin-based harness
by integrating existing applications maintaining their di-
verse structure rather than inventing new applications.

3.2 High-Level Design of ServiBench

ServiBench (sb) uses a data-driven benchmarking method
based on a suite of serverless applications described in Sec-
tion 3.3 and serverless invocation patterns derived from real
traces as described in Section 3.4.

Figure 2 illustrates the data Ô¨Çow and main processes of a
benchmark execution with sb. Each serverless application
provides two additional assets in addition to its source code
(Figure 2, label 1 ). First, it deÔ¨Ånes deployment instruc-
tions annotated with OCI-compatible container images for

3

dependency-bundled deployment. This packaging enables re-
producible cross-platform builds of the application and its ser-
vice dependencies, which are deÔ¨Åned through infrastructure-
as-code [30]. Second, a workload scenario deÔ¨Ånes how an ap-
plication is invoked. This can be a single parameterized HTTP
request or a probabilistic state-machine emulating users Ô¨Çow
through a multi-action user story. The prepare process builds
and deploys the application into a serverless platform and per-
forms any preparations for benchmarking such as uploading
datasets or extracting service endpoints ( 2 ).

The invocation patterns for benchmarking are derived
from an invocations dataset ( 3 ). The process step takes
invocation logs and upscales these to Ô¨Åner-grained (second-
level) invocation frequencies, following different trafÔ¨Åc shape
patterns ( 4 ). Subsequently, the invoke process combines
application-speciÔ¨Åc workload scenarios with generic server-
less invocation patterns to invoke the application invocation
endpoints with realistic serverless trafÔ¨Åc patterns ( 5 ). This
workload generation yields client logs, which are mainly used
for validation during experiment analysis as described in Sec-
tion 5. The main results are detailed end-to-end (e2e) traces
collected from individual application components and cor-
related by a trace identiÔ¨Åer passed along inter-service calls.
The analyze traces process ( 6 ) uses critical path analysis to
perform latency breakdown analysis (see Section 4), which
provides a trace breakdown summary to the experiment anal-
ysis ( 7 ).

We implement sb as a Python package that offers a CLI and
SDK to orchestrate serverless application benchmarking. To
integrate with sb, an application needs to provide a Python Ô¨Åle
with three lifecycle methods to prepare, invoke, and cleanup it-
self. Sb supports Docker to package application-speciÔ¨Åc build
and deployment dependencies, and automatically manages
directory mounts and provider credentials.

We use the Azure Function Traces [66] as the invocations
dataset and the k6 [32] load testing tool as the invoker. Though
sb automatically conÔ¨Ågures the trace-based invocation pat-
terns, application-speciÔ¨Åc workload scenarios are written in
JavaScript. For AWS, applications have been instrumented
with X-Ray [4] and the trace analyzer works with AWS ser-
vices supported by X-Ray. Other providers are currently
partially supported, with two applications integrated with
Azure and basic deployment infrastructure for IBM Cloud
and Google Cloud. Expanding support for these providers is
ongoing work.

3.3 Serverless Applications

Table 1 characterizes the 10 serverless applications in the
benchmark suite. We select multiple applications for each of
the most common types identiÔ¨Åed by survey studies [22, 84]
except for the type of operations and monitoring because
such applications are difÔ¨Åcult to test in isolation. In particular,
API refers to synchronously invoked web endpoints (e.g.,

Application

Type

Lang.

A Minimal Baseline [7]
B Thumbnail Gen. [87]
C Event Processing [87]
D Facial Recognition [6]
E Model Training [34]
F Realworld Backend [33]
G Hello Retail! [53]
H Todo API [87]
I Matrix Multipl. [87]
J Video Processing [34]

API
Async
Async
Async
Async
API
API
API
Batch
Batch

JS
Java
JS
JS
Python
JS
JS
Go
C#
Python

#

1
2
8
6
1
21
10
5
6
1

External Services

‚ô¶
‚ô¶ (cid:78)
‚ô¶ (cid:52)(cid:4) (cid:74)
‚ô¶ (cid:78) (cid:70)‚ô†
‚ô¶ (cid:78)
‚ô¶
‚ô¶ (cid:78) (cid:70) (cid:74) (cid:3)
‚ô¶
‚ô¶ (cid:78) (cid:70)
‚ô¶ (cid:78)

(cid:74)

(cid:74)

Table 1: Characteristics of each end-to-end serverless applica-
tion. Abbreviations: Programming language (Lang), number
of functions (#), API gateway (‚ô¶), cloud storage ((cid:78)), cloud
pub/sub ((cid:52)), cloud queue ((cid:4)), cloud orchestration ((cid:70)), cloud
ML (‚ô†), cloud DB ((cid:74)), cloud streaming ((cid:3)).

REST, GraphQL), async processing applications are triggered
through events (e.g., an upload to a storage bucket triggers a
function), and batch refers to larger computation tasks often
simultaneously processed by multiple functions.

For the dominant serverless programming languages
JavaScript (JS) and Python [22, 39], we select multiple ap-
plications. The popular enterprise languages Java, C#, and
increasingly Go are represented by one application each.

Our applications are of representative size, as datasets [66]
and surveys [22,39] show that most applications are composed
of 10 or fewer functions.

We cover the most popular external services used in server-
less applications [22, 39, 66] with a focus on API gateways,
persistency services (e.g., S3 cloud storage, DynamoDB cloud
database), and cloud orchestration (e.g., AWS Step Functions).
Appendix B describes and motivates each application.

3.4 Serverless Invocation Patterns

This section describes how we derive invocation patterns from
the Azure Function Traces [66], a dataset invocation logs from
a commercial cloud platform over two weeks.

3.4.1 Selection and ClassiÔ¨Åcation

From the 74,347 functions in the Azure traces, we selected
528 functions with relevant properties for benchmarking
logs [50]. We removed 45,564 temporary functions not avail-
able over the entire two-week period and skip 15,319 timer
triggers because these follow predictable periodic patterns
and are typically not latency-critical. Knowing that the 18.6%
most popular applications with invocation rates ‚â• 1/min rep-
resent 99.6% of all function invocations [66], we selected the
2.6% most popular functions with average invocation rates
‚â• 1/s as they are relevant for high-volume benchmarking.

We visually identiÔ¨Åed 4 typical invocation patterns by man-
ually classifying two time ranges for 100 of the selected

4

Figure 2: Data Ô¨Çow of a benchmark execution in sb.

granularity of seconds while maintaining the large scale prop-
erties of the Azure traces. We chose a Hurst parameter of 0.8
as empirically determined for web trafÔ¨Åc [17]. This indicates
positive correlation over time, which means an increase in the
workload is likely followed by an increase and a decrease is
likely followed by a decrease. We believe this to be realistic
workload as it generates patterns where workload bursts are
sustained instead of oscillating between peaks and valleys.

Figure 3: Typical serverless invocation patterns over 20 min.

Serverless Architectures

4 Detailed Distributed Trace Analysis for

528 functions. We Ô¨Årst created 200 individual line plots
with invocation counts over 20 minutes1 and grouped simi-
lar trafÔ¨Åc shapes into several clusters. After merging similar
patterns, we identiÔ¨Åed 4 common patterns (see Figure 3):
(i) steady (32.5%) represents stable load with low burstiness,
(ii) Ô¨Çuctuating (37.5%) combines a steady base load with
continuous load Ô¨Çuctuations especially characterized by short
bursts, (iii) spikes (22.5%) represents occasional extreme load
bursts with or without a steady base load, and (iv) jump (7.5%)
represents sudden load changes maintained for several min-
utes before potentially returning to a steady base load.

3.4.2 Trace Upscaler

The trace upscaler generates invocation rates at the granular-
ity of seconds from per-minute invocation logs. The Azure
dataset [66] reports the number of function invocations per
minute. However, bursty serverless invocation rates are not
uniformly distributed over a minute [79]. For example, it
could be that the majority of the invocations in a single minute
all occur in the Ô¨Årst Ô¨Åve seconds.

To generate more realistic patterns than uniformly dis-
tributed or linearly interpolated invocation rates, we use frac-
tional Brownian motion to synthesize perturbations at the

1We explored different time resolutions (2 weeks, 1 day, 4 hours, 1 hour,
30 min, 20 min, 10 min) and found that hourly patterns are similar enough to
20 minutes, which is feasible cost-wise for repeated experimentation with
many different applications under varying conÔ¨Ågurations.

We motivate distributed tracing of serverless applications and
describe how to extract the critical path and a detailed latency
breakdown from distributed traces.

4.1 Challenges and Background

Distributed tracing has been adopted for various use cases [45,
latency analysis),
61] such as distributed proÔ¨Åling (i.e.,
anomaly detection (i.e., identifying and debugging rare prob-
lems), or workload modeling (e.g., identifying representative
workÔ¨Çows). Tracing systems such as Google‚Äôs Dapper [67]
or Facebook‚Äôs Maelstrom [77] help improve performance,
correctness, understanding, testing, and recovery of services.
These insights are even more important for highly distributed
serverless architectures given their ephemeral nature. How-
ever, event-based coordination is inherently asynchronous,
hence hard-to-track background workÔ¨Çows need to be in-
cluded in the tracing and cannot be ignored as for synchronous
microservice architectures [57]. The limited control in server-
less environments makes users dependent on provider tracing
implementations or resort to less detailed third-party or cus-
tom implementations. Further, tracing issues are common at
large scale and trace analysis must detect and handle clock
inaccuracy and incomplete traces.

We represent each request as an execution trace where
the critical path determines the end-to-end latency and the
latency breakdown lists and classiÔ¨Åes each time span along
the critical path as visualized in Figure 4.

5

source  code + deploy scriptsworkload scenariosServerless ApplicationsInvocationsDatasetinvocation endpointsPrepareinvocation patternsProcessinvocation logsclient logsInvoketrace breakdownAnalyze TracesExperimentAnalysisend-to-end traces2345671DataProcessOrder010203040Num. RequestsFluctuatingAzure traceUpscaled trace012345Jump0250500750100012500246Spikes025050075010001250Time [s]0510SteadyFigure 4: SimpliÔ¨Åed depiction of an execution trace (DeÔ¨Ånition 1) with annotated latency breakdown (DeÔ¨Ånition 3).
Data collected from App-A with two cold starts. Values represent time in milliseconds. Labels Sync/Async refer to Figure 5.

DeÔ¨Ånition 1 An execution trace of a serverless application
is a causal-time diagram of the distributed execution of a
request, where a node is a trace span that corresponds to
an individual unit of work (e.g., computation) and an edge
represents a causal relationship through a synchronous or
asynchronous invocation. Each trace span contains a start and
end timestamp and is correlated by a trace id.

DeÔ¨Ånition 2 A critical path in an execution trace is the
longest path weighted by duration, which starts with a client
request and ends with the trace span that has the latest end
time. This deÔ¨Ånition of end-to-end latency includes asyn-
chronous background workÔ¨Çows that do not return to their
parent spans to capture the event-based nature of serverless
systems. Hence, our deÔ¨Ånition differs from a critical path of
a synchronous client response in microservices [57].

DeÔ¨Ånition 3 A latency breakdown of an execution trace is
the most detailed list of time segments along the critical path
without any temporal gaps. This explicitly includes transitions
between trace spans, which are often implicit in an execution
trace. In its aggregated form2, each time segment is classi-
Ô¨Åed and summed up by the following categories common to
serverless applications: (i) computation represents the actual
processing time of serverless functions. (ii) external service
represents the time waiting for the completion of a services
request (e.g., database query, Ô¨Åle upload to a storage services).
(iii) orchestration represents time spent coordinating server-
less function executions by workÔ¨Çow engines (e.g., AWS Step
Functions) or API gateways dispatching requests to functions.
(iv) trigger represents the implicit transition time between an
event and a function bound to this event (e.g., time between

2We use high-level categories for better readability and cross-application
comparison as the full trace breakdown is very detailed. Individual external
services, such as cloud storage, could be classiÔ¨Åed separately if needed.

enqueuing a message until the event is dispatched to a func-
tion). (v) queuing represents the time spent in function worker
queues before it starts executing. (vi) container initialization
represents the time it takes to provision the function execution
environment. (vii) runtime initialization represents the time
it takes to initialize the function language runtime during a
cold start. (viii) Ô¨Ånalization overhead represents cleanup tasks
after function execution and before freezing the sandbox.

4.2 Latency Breakdown Extraction

We Ô¨Årst extract the critical path of an execution trace and
subsequently reÔ¨Åne it into a detailed latency breakdown.

To extract the critical path, we use Algorithm 1, which is
a modiÔ¨Åed version of the weighted longest path algorithm
proposed in the context of microservices [57]. Our modiÔ¨Åca-
tions support asynchronous invocations, unordered traces, and
reÔ¨Åned heuristics to handle timing issues [52] in Ô¨Åne-grained
serverless tracing. A stack with all parent spans of the last
ending span is used to only recurse into child spans connected
to the last ending span. Line 9 deÔ¨Ånes the sorting order for
child spans primarily by the endTime and secondarily by the
startTime. The secondary sort key is required to handle the
special case of a single trace span with a duration of 0 mil-
liseconds. Our heuristic HAPPENSBEFORE detects sequential
relationships and ISASYNC detects asynchronous invocations.
They support a conÔ¨Ågurable error margin (default 1ms) to
gracefully handle minor clock inaccuracies by implementing
temporal comparisons such as t1 < t2 with t2 ‚àí t1 + margin.
We extract the detailed latency breakdown along the critical
path by identifying and categorizing every time segment while
accounting for all gaps between spans. Figure 5 visualizes the
common cases for synchronous and asynchronous invocations
while iterating pairwise (current, next) over the critical path.
For synchronous invocations, we distinguish two different
cases: Sync1 handles a traditional synchronous invocation

6

Gateway 1Function 1 2956Function 1 - Initialization434Function 1 - Execution1479267Bucket 1 - Get Metadata160342Bucket 1 - Upload imageFunction 1 - Unaccounted1010Trigger (not traced)Queueing timeFunction 2337118642001247734Function 2 - InitializationFunction 2 - UnaccountedFunction 2 - ExecutionBucket 1 -  DownloadBucket 2 - Upload37Time [ms]External ServiceOrchestrationTriggerQueueingFinalization OverheadRuntime InitializationComputation2956259323714044Container InitializationOriginal SpanUploadImageStoreImageAsyncTriggerStoreThumb.UserFunction 1: Persist ImageBucket 1: ImagesFunction 2: Make Thumb.Bucket 2: ThumbnailsGateway 1TriggerS1S2A2Algorithm 1 Critical Path Extraction, based on [57].
Require: Serverless execution trace T with

span attributes childSpans, startTime, endTime and
stack S with all parent spans of the last ending span

path ‚Üê [currentSpan]
if S.top() == currentSpan then

1: procedure T .CRITICALPATH(S, currentSpan)
2:
3:
4:
5:
6:
7:

end if
if currentSpan.childSpans == None then

Return path

S.pop()

8:
9:

10:
11:

12:

13:
14:
15:
16:

end if
sortedChildSpans ‚Üê sortAscending(

currentSpan.childSpans, by=[endTime, startTime])

lastChild ‚Üê sortedChildSpans.last
for each child in sortedChildSpans do

if child.HAPPENSBEFORE(lastChild) and

not currentSpan.ISASYNC(path.last) then

path.extend(CRITICALPATH(S, child))

end if

end for
if (currentSpan.ISASYNC(lastChild) and

S.top() == lastChild) or
(not currentSpan.ISASYNC(lastChild) and
not currentSpan.ISASYNC(path.last) then

path.extend(CRITICALPATH(S, lastChild))

17:
end if
18:
Return path
19:
20: end procedure

Return current.endTime < next.startTime

21: procedure current.HAPPENSBEFORE(next)
22:
23: end procedure
24: procedure current.ISASYNC(next)
25:
26: end procedure

Return next.endTime > current.endTime

from a current parent span into a next child span. Sync2
handles a potentially recursive transition from the current
span on a synchronous invocation stack across a common
parent into its next child span. For asynchronous invoca-
tions, we distinguish two cases: Async1 handles if the next
child span overlaps with the current parent span. Async2
handles if there is a gap between the current parent span
and the next child span. This scenario frequently occurs in
serverless systems when triggering a function using a slow
trigger. There is a third case that we can currently not detect,
which is structurally equivalent to Sync1 except that the call
to next is asynchronous. To make this case detectable, trace
speciÔ¨Åcations could deÔ¨Åne labels for synchronous and asyn-
chronous parent-child relationships as discussed for Open
Telemetry [54]. Finally, we assign an activity label (e.g., com-
putation) to each breakdown segment depending on the span

7

Figure 5: Extraction cases of latency breakdown (red seg-
ments) for pairs of current and next nodes on the critical path.

type (e.g., function) as annotated in Figure 4.

5 Experimental Results

We use in this section ServiBench to comprehensively bench-
mark the performance of a popular serverless platform. We
deploy ServiBench on the serverless platform AWS Lambda,
which various reports [22,69] indicate is much used by server-
less applications in production.

ServiBench supports many real-world performance scenar-
ios, from which we focus in this work on (1) latency break-
down to understand the performance of warm invocations and
of cold starts, and on (2) the impact of invocation patterns
on (median) end-to-end latency. We make 6 observations,
and discuss their implications for serverless practitioners and
researchers in Section 5.4.

5.1 Experiment Design

We conduct a performance benchmarking experiment [29]
with an open-loop load generator in the data center region
Northern Virginia (us-east-1) as commonly used by other
serverless studies [10, 14, 16, 81, 82]. We collected over 7.5
million traces, through over 12 months of experimentation in
2021 and 2022.

Application conÔ¨Åguration All functions are conÔ¨Ågured
with the same memory size of 1,024 MB as this provides
a balanced cost-performance ratio [21] between the minimal
memory size of 128 MB (heavy CPU throttling) and the max-
imum memory size for a single CPU core of 1,769 MB [5]
(inefÔ¨Åcient for non-CPU-intensive load). For application-
speciÔ¨Åc memory size tuning, we refer to aws-lambda-power-
tuning [12], systematic literature reviews [59, 63], and many
empirical studies [2,21,24,42,80,81,86]. All supported cloud
services (API Gateway, Lambda, Step Functions) have dis-
tributed X-Ray tracing enabled to trace every request. For
applications with multiple endpoints, we present one repre-

currentAsync1: OverlappingnextcurrentnextAsync2: Trigger gapcurrentnextparentSync1: Parent into childSync2: Transition across common parentcurrentnextFigure 6: Comparison of per-second invocation rates planned
vs. sent vs. executed (left) and validation ratio for pairwise
comparison (right).

sentative endpoint in the paper and refer to the replication
package for detailed results.

Load generator For accurate load generation, we deploy
an over-provisioned EC2 instance of the type t3a.large in the
same region as the serverless applications. We validate per-
second invocation rates for accurate load generation (planned
vs. sent) by correlating the load conÔ¨Åguration with the client
logs and actual load serving (generated vs. executed) by
correlating the client logs with the backend traces. We com-
bine visual comparison (see Figure 6) with FastDTW [60],
an approximate Dynamic Time Warping (DTW) algorithm.
We monitor application error rates client-side by checking
response status codes and server-side by checking for any
exceptions in each trace. Finally, we investigate any invalid
traces due incomplete or invalid trace data following both
logical and time-based validation.

5.2 Latency Breakdown

We Ô¨Årst drill down into the end-to-end latency of serverless ap-
plications to identify critical components using sb (Section 3)
and trace breakdown extraction (Section 4). This application-
level perspective complements existing work, which primarily
focused on micro-benchmarking individual components or
reporting client-side response times for synchronously orches-
trated applications [59, 63]. As a baseline, we focus on warm
invocations and subsequently compare the latency penalty of
cold invocations and tail latency.

Method. For each of the 10 applications from Section B, we
send 4 bursts of 20 concurrent requests with an inter-arrival
time of 60 seconds between each burst. The Ô¨Årst burst trig-
gers up to 20 cold invocations used in Section 5.2.2 and after
the function completes within 60 seconds, the following 3
bursts trigger more warm invocations used for tail-latency
analysis in Section 5.2.3 and as baseline in Section 5.2.1. To
collect enough samples under the same conditions, we con-
duct 10 trials and 14 repetitions resulting in up to 8,400 warm

8

Figure 7: Latency breakdown of warm invocations as median
fraction of end-to-end latency. Values inside the bar-stacks
represent absolute time per activity, in milliseconds.

invocations (3 √ó 20 √ó 10 √ó 14)3. For each of the 10 trials, we
invoke each application using round-robin scheduling with
inter-trial times of 50 minutes to trigger cold invocations in
the Ô¨Årst burst for Section 5.2.2. Before each of the 14 repeti-
tions of trials, we re-deploy each application to ensure a clean
state. We use trace analysis to detect cold invocations through
the presence of Initialization segments [8]. For applications
with chained functions, we ignore ‚Äúpartial cold starts‚Äù and
only consider ‚Äúfull‚Äù warm or cold invocations, where every
function in the critical path shares the same cold start status.

5.2.1 Warm Invocations

Frequently invoked applications often get warm invocations.

Results. The relative latency breakdown in Figure 7 shows
the median latency for each activity introduced in DeÔ¨Åni-
tion 3. App-A exempliÔ¨Åes the orchestration overhead (22 ms)
of a common serverless pattern where an API gateway is
connected to a function. Lightweight applications such as
App-H are similarly dominated by orchestration time (23 ms)
because they do minimal computation work and use fast exter-
nal services (e.g., 6 ms database insert). App-E and App-J are
examples of computation-heavy workloads. External services
like blob storage or computer vision APIs are often the domi-
nating factor, especially for many I/O operations (App-I) or
larger Ô¨Åles (App-D). Asynchronous applications are typically
dominated by transition delays due to trigger and queuing
time as demonstrated by the applications App-B and App-C.

3A related study [72] uses 3,000 samples for individual functions; we
target more per-application samples as requests can be distributed across
endpoints.

01020304050Ti[]200225Reqs per SecondPlannedSentExecutedRatio0.900.9511.051.10RatioExecuted vs. Planned02505007501000Time [s]0.900.9511.051.10Sent vs. Planned01020304050Time [s]200225Reqs per SecondnnedSentExecutedRatioABCDEFGHIJApplication00.250.500.751Latency breakdown1221946851223111054016965311136281863217142227542326236612183101040485ActivityComputationExternal serviceOrchestrationTriggerQueueingFinalization overheadFigure 8: Latency-penalty breakdown for cold invocations
compared to the baseline of warm invocations (Figure 7) as
fraction of the difference between the medians. Values inside
the bar-stacks are absolute, in milliseconds (ms), e.g., for
App-B, Computation takes 4,425 ms longer.

Observation 1: The median end-to-end latency of server-
less applications is often dominated by external service calls
and synchronous orchestration or asynchronous trigger-
based coordination. The actual computation time in
serverless functions is relatively little except for inherently
compute-heavy workloads.

5.2.2 Cold Starts

We now study which time categories contribute to higher
cold start latency using the results from Section 5.2 as a
baseline. Tracing cold starts requires access to timestamps
captured within provider-internal infrastructure. Sb can ex-
tract these internal timestamps from AWS X-Ray traces and
distinguish between container and runtime initialization time,
which would only be possible for self-hosted [73] or provider-
internal [1, 11] systems otherwise. Insights on cold starts
are relevant for applications that are invoked irregularly (e.g.,
inter-arrival times > 10 minutes) or exhibit bursty invocation
patterns and, hence, need to provision new function instances.

Results. Figure 8 shows the latency difference between the
medians for cold invocations compared to warm invocations.
App-A depicts a common initialization overheads for a func-
tion behind an API gateway of 265 ms (98+167) in line with
prior cold start studies for Node.js by Wang et al. [81, Fig-
ure 6] and Maissen et al. [46, Figure 7]. Our results are
more detailed and reveal differences for realistic applications.
Our trace details show that runtime initialization typically
accounts for the majority of cold start overhead compared to
container initialization. For App-A, the container initializa-
tion time of 98 ms is ~20 ms faster than the boot times for the

9

Figure 9: Latency-penalty breakdown for slow invocations
compared to baseline of warm invocations (Figure 7) as frac-
tion of the difference between the median and 99th percentile.
Values inside the bar-stacks are absolute, in milliseconds (ms),
e.g., for App-J, the Cloud storage service adds 2,427 ms of
delay for slow invocations; this accounts for ~90% of the
tail-latency slowdown.

underlying Micro VMs as reported for pre-conÔ¨Ågured Fire-
cracker [1, Figure 6]. In comparison to other applications, the
relative latency penalty remains similar (c.f., App-J). How-
ever, other realistic applications have much higher absolute
initialization times due to large packaged dependencies and
chains of multiple functions in the critical path.

Beyond runtime and container initialization, other cate-
gories can add cold start overhead that is often overlooked.
Computation can contain conditional code executed only upon
cold starts or trigger one-off optimizations such as just-in-
time compilation for interpreted languages [51] exempliÔ¨Åed
by the applications App-B in Java and App-I in C#. Exter-
nal service time can add connection overhead due to extra
authentication upon cold starts (e.g., App-B caches S3 authen-
tication) or database connection setup (e.g., App-H connects
to DynamoDB). Finally, the following categories related
to application coordination remain unaffected by cold starts:
orchestration, trigger, queuing, and instrumentation overhead.

Observation 2: Runtime initialization and container initial-
ization add most overhead for cold invocations but external
service connection initialization and one-off computation
tasks can also contribute.

5.2.3 Tail Latency

Tail latency is increasingly important at scale for cloud
providers [19] and hence particularly challenging for massive
multi-tenant serverless systems. Prior studies [38, 55, 72, 81]
conducted micro-benchmarks to measure tail of individual
serverless components. By leveraging our trace analysis (Sec-
tion 4), we can directly identify which time categories con-
tribute to tail latency (99th percentile) for entire applications.

ABCDEFGHIJApplication00.250.500.751Latency-penalty breakdown1679844252939450951361976303109150205471338107592751326646528160103111406180322212041648275954ActivityComputationExternal serviceOrchestrationTriggerQueueingFinalization overheadRuntime initializationContainer initializationABCDEFGHIJApplication00.250.500.751Latency-penalty breakdown35822261618115112616432991835720284129925054315541257732940853633616033052082427ActivityComputationExternal serviceOrchestrationTriggerQueueingFinalization overheadA

200

B

37

C

50

D

25

E

22

F

G

H

167

154

200

I

10

J

25

Table 2: Invocation rates (in reqs./s) used per application, set
at 50% of the achieved load in the scalability pre-study.

Results. Figure 9 shows that external services cause major
variability. In particular, storing a large Ô¨Åle (+2,429 ms for
App-J) causes massively more tail-latency delay than storing
many chunks of small Ô¨Åles (+602 ms for App-J). Database
services contribute less to tail latency than object storage as
demonstrated by the applications App-C, App-F, App-G, and
App-H with latency penalties between 35 ms and 99 ms.

Another factor of tail latency is the serverless overhead
for orchestrating synchronous applications (i.e., orchestra-
tion time) and asynchronous applications (i.e., trigger and
queuing time). These categories double or triple their latency
in comparison to the baseline in Figure 7. Computation is
inherently variable in a multi-tenant system but contributes at
most 25% to the latency penalty for compute-heavy App-E.

Observation 3: Tail latency is primarily caused by external
services, particularly by object storage.

5.3

Invocation Patterns

Real-world applications exhibit diverse invocation pat-
terns [66] but prior work rarely investigated dynamic work-
loads over time [38] or different invocation patterns [37] and
if so, using artiÔ¨Åcial applications, patterns, and one-time
bursts [10, 72, 81]. It remains unclear how different invo-
cation patterns derived from the Azure Function Traces [66]
(Section 3.4) affect the end-to-end latency of serverless appli-
cations. To address this gap, we investigate the performance
effect of varying invocation rates over time under an equiva-
lent average invocation rate.

Scalability prestudy. We conducted a prestudy to adjust
the average invocation rates to our 10 heterogeneous appli-
cations. Using the same invocation rate or concurrency level
for all applications is inappropriate because it overloads some
applications while others remain close to idle4. Therefore, we
test increasing load levels with constant arrival rates for 90 sec-
onds until an application exceeds a rate of 5% for trace errors
or invalid traces twice in succession. Trace-based root cause
analysis identiÔ¨Åed rate limits and function tracing issues5 as
common. To avoid overloading an application, we select 50%

4We collected over 700K traces for App-B to App-J using the invocation
patterns in Section 3.4 with an average rate of 20 reqs/sec. We tried different
concurrency levels, but noticed that long-running applications were over-
loaded and short-running applications were served by few function instances.
5We reported this and additional issues related to clock drifting and trace

correctness to AWS for further investigation.

Figure 10: End-to-end latency for different invocation patterns
clipped at the 99th percentile due to extreme long tail.

of the achieved load level as target average invocation rate for
parameterizing the invocation patterns (Table 2).

Method. We treat each application from Table 1 with 2 arti-
Ô¨Åcial and 4 realistic workloads derived from real-world traces
as described in Section 3.4. The artiÔ¨Åcial workloads serve
as baseline for fully constant load and maximal burstiness
simulated by on_off alternations with load for 1 second and
idle time of 3 seconds. We scale the average invocation rate
per-application following Table 2. We discard warmup mea-
surements of the Ô¨Årst 60 seconds as the actual invocation rate
can deviate from the target rate in the Ô¨Årst second and initial
cold starts dominate the start of every experiment.

Results. Figure 10 shows the partial CDF of the e2e latency
for applications that accurately followed the target invocation
pattern (<10% deviation from target invocation rate and error
metrics). The median latency is unaffected by invocation pat-
terns as shown by the overlapping CDF curves. Percentiles
up to p99 clipped in the CDF also show no relevant differ-
ence with the exception of App-D, where the peak invocation
rates of the spikes workload reach the rate limit of the facial
recognition service causing external service delays.

The number of initial cold starts differs by invocation pat-
tern but remains very low after the 60 seconds warmup time.
The on_off and spikes patterns have higher peak invocation
rates and trigger more initial cold starts. However, after the
warmup time, additional cold starts are rare (below 10).

Observation 6: Different invocation patterns do not mean-
ingfully affect the median end-to-end latency.

5.4 Discussion

Our results emphasize the importance of serverless bench-
marking that integrates Ô¨Åne-grained latency breakdown anal-
ysis, realistic invocation patterns, and varied benchmark ap-

10

1234500.250.500.751ECDFBWorkload typeconstantfluctuatingjumpon_offspikessteady0.10.20.3C1.622.42.83.200.250.500.751D0.080.120.16Trace duration [s]Fplications. Furthermore, our experiments lead to relevant
implications for serverless practitioners and researchers.

Serverless fulÔ¨Ålls its core promise of stable performance
under bursty workloads. Our results show that serverless
is indeed well-suited for bursty workloads after initial cold
starts and when staying below platform-speciÔ¨Åc rate limits.
Hence, serverless fulÔ¨Ålls its promise of built-in scalability
under the given load levels for our 10 applications. This result
was somewhat unexpected, especially contrasting with prior
research [10, 38, 72]. However, of course, bursty workloads
may still negatively impact performance on different plat-
forms or with even more rapid bursts than what we evaluated
(e.g., per-microsecond bursts rather than per-second bursts).

Slowdowns are caused by control Ô¨Çow and coordination,
not computation. Our results suggest that future research
should go beyond computation-optimization approaches [2,
3, 21], given how little computation time contributes to the
end-to-end latency of many applications. The high fraction of
external service time shows that fast data exchange between
stateless functions remains a key challenge for serverless
applications. Many applications would beneÔ¨Åt from low-
latency storage solutions such as Pocket [35], Shredder [90],
or Locus [56]. Finally, efÔ¨Åcient function coordination through
triggers [44, 55] and workÔ¨Çow orchestration [41] deserves
more research attention given the high transition delays.

Cold start times are best improved by debloating lan-
guage runtimes. Language runtimes should be the primary
focus for optimizing cold start latency given their major im-
pact, adding >500 ms overhead for most applications. Run-
times were not designed for serverless architectures and re-
cent optimizations for Java [25] and .NET [62] achieve large
speedups of up to 10√ó, though sometimes at the cost of
more memory usage or larger deployment sizes. Debloating
system stacks [36] and application dependencies [68] is an-
other promising optimization motivated by large initialization
overheads for applications with large dependency trees (e.g.,
App-E and App-J). Alternatively, serverless developers can
select languages with lower runtime initialization overhead,
such as Golang [46].

The main cause of tail-latency problems for warm invo-
cations are external services and poorly chosen triggers.
Latency-critical applications should carefully choose exter-
nal services and trigger types. Measurement studies cover-
ing different external services can guide the initial selection
process [38, 55, 72, 81]. Our results conÔ¨Årm these Ô¨Åndings
and identify cloud storage as key contributor to performance
variability [72]. Beyond that, sb can provide insights about
alternative application implementations. For example, ap-
plications using database services (App-C, App-F, App-G,

App-H) exhibit better tail latency than those using cloud stor-
age (App-B, App-I, App-J). However, initializing a database
connection can add additional cold start delay (c.f., Figure 8).
For asynchronous orchestration, choosing appropriate trigger
types is crucial as the cloud storage trigger introduces massive
tail latency (e.g., App-B in Figure 7). In contrast, the pub/sub
trigger used in App-C adds minimal tail latency. However,
queuing time may become an issue as non-HTTP-triggered
functions have lower scheduling priority [70].

5.5 Limitations

Despite careful design, we cannot avoid a small number of
limitations in our design and results. First, all results re-
ported in Section 5 are speciÔ¨Åc to the AWS serverless plat-
form. Conceptually, ServiBench enables benchmarking a
wider range of cloud providers. However, cloud providers
may not provide detailed tracing information comparable to
AWS‚Äô X-Ray, preventing in-depth analysis. Following im-
proved tracing capabilities, we are currently adding support
for Microsoft Azure.

Second, we do not tune Lambda memory sizes for in-
dividual benchmark applications or functions. This is a
common decision in benchmark design, to increase fairness of
comparison. Future research should investigate ideal settings
for each application in our benchmark.

Third, the load generator currently supports invocation
patterns on a per-second granularity. For simulating ex-
tremely bursty workloads, more Ô¨Åne-grained conÔ¨Åguration
would be necessary, for example to conÔ¨Ågure a burst that hap-
pens within a few milliseconds. This may explain why we
observed only a limited impact of different invocation patterns
on end-to-end latency in Section 5.3.

Last, we identify but do not address the possible issue of
long-term performance changes in cloud settings. Cloud
providers iterate rapidly and also operational policies can
change, so performance may change or even vary over time.
Future work could address this situation through techniques
such as periodic, long-term measurements.

6 Related Work

This work complements and greatly extends a large body of
work on serverless benchmarks and performance analysis.

Serverless benchmarks and measurement frameworks:
Table 3 compares ServiBench with the most important server-
less benchmarks and performance frameworks. Our study
(i) has a wider scope with more applications, functions, and
services, and in particular with more than one function or
service, (ii) adds realistic applications and invocation patterns
based on real-world characteristics [22, 66, 84], (iii) does not
rely on low-level, server-side tracing, not available for public
serverless platforms, and (iv) enables analysis across a variety
of situations common in production, including synchronous

11

Reference

Focus

Scope

Invocation Patterns

Insights

apps

func/app micro

langs

services

concurrent

trace-based

white box

async

faas-proÔ¨Åler [65]
vHive [73]
ServerlessBench [86]
SeBS [14]
FunctionBench [34]
FaaSDom [46]
BeFaaS [27]

Server-level overheads
Cold-start breakdown
Diverse test cases
Memory size impact
Diverse workloads
Language comparison
Application-centric

ServiBench (this work) White-box analysis

5
9
4
10
8
0
1

10

1
1
1-7
1
1
-
17

1-21

28
0
10
0
6
5
0

0

2
1
4
2
1
4
1

5

0
1
1
1
1
0
1

7

(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:51)
(cid:51)

(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)

(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:51)

Table 3: Summary of related serverless benchmarks.

and asynchronous invocations, end-to-end tracing including
external services, and Ô¨Åne-grained white-box analysis.

Closest to our work, BeFaaS [27] is an application-
centric benchmarking framework, but uses only synchronous
function-chains and a single external service (an external
database). BeFaaS enables cloud-agnostic tracing through
chained functions, however, the language-speciÔ¨Åc architec-
ture does not support the analysis of orchestration, queueing,
trigger, Ô¨Ånalization overheads, and container and runtime
initialization.

Performance analysis of serverless platforms: Perfor-
mance is an important and commonly studied aspect of server-
less computing. Over 100 studies from academia and industry
have already appeared [28,59,63,88]. Commonly investigated
topics include scalability [37, 49], cold starts [48, 81], perfor-
mance variability [15, 38], instance recycling times [43, 81],
and the impact of parameters such as memory size [24,89], or
programming language [20, 31]. These studies tend to rely on
single-purpose micro-benchmarks and rarely utilize tracing
data. Further, reproducibility [71] remains a big challenge in
serverless performance studies, as analyzed recently [63].

7 Conclusion

Due to their compositional nature, serverless applications and
the platforms executing them are challenging to benchmark.
We designed and implemented ServiBench, an open-source,
application-level, serverless benchmarking suite. Unlike ex-
isting approaches, ServiBench: (1) leverages a suite of 10
diverse and realistic applications (importantly, including both
synchronous and asynchronous cases), (2) extracts invoca-
tion patterns from cloud-provider data and generates realistic
workloads, (3) supports end-to-end experiments, capturing
Ô¨Åne-grained application-level performance and enabling re-
producible results, (4) proposes a novel algorithm and heuris-
tics to enable white-box analysis even for asynchronous appli-
cations and data produced by (distributed) serverless tracing,
and (5) supports comprehensive performance analysis for
real-world scenarios such as cold starts and tail latency.

Using ServiBench, we conducted a comprehensive, large-
scale empirical investigation of the AWS serverless platform,

collecting over 7.5 million execution traces. We observe
that median end-to-end latency is most often dominated by
external service calls, orchestration, or by waiting for asyn-
chronous triggers. Excessive tail latency is similarly caused
more by external services (particularly object storage) than
any computation inherent to the serverless applications. Re-
garding cold starts, our results indicate that investment into
simplifying runtime environments or slimming them down
(e.g., as Golang does) is the most promising angle to speed
up scaling. Finally, our experiments conÔ¨Årm the AWS plat-
form can react effectively to workload differences, even to
challenging bursty invocation patterns, for most applications.
In the future, ServiBench, and the general serverless bench-
marking concepts demonstrated by it, can be used by prac-
titioners to evaluate in-detail performance problems in their
own applications or serverless platform of choice. Platform
engineers can use our approach and tooling to further improve
their offerings. We envision ServiBench to become an integral
part of the evaluation of future serverless research contribu-
tions, through its current features, and as an extensible basis.

Acknowledgements

We are grateful to the SPEC Research Group6 for fruitful dis-
cussions and want to thank Johannes Grohmann, Sean Mur-
phy and Jan-Philipp Stegh√∂fer for their contributions. Special
thanks go to our research assistants Simon Trapp and Ranim
Khojah for supporting the integration of applications. We
appreciate the generous support of our industry partners en-
abling large-scale evaluation and detailed investigation of
tracing issues with the AWS service teams. This work was
partially supported by the Wallenberg AI, Autonomous Sys-
tems and Software Program (WASP) funded by the Knut and
Alice Wallenberg Foundation.

A Replication Package

We provide a detailed replication package with two main
goals. First, we want to enable the replication of our results

6https://research.spec.org/

12

by independent researchers to make the results reproducible.
This also allows to track how the reported performance prop-
erties evolve over time. Secondly, we want to enable the use
of ServiBench in further studies as the reported analysis in
Section 5 covers only a small subset of the analysis enabled by
our tool. Further studies could for example analyze different
styles of applications (e.g., scientiÔ¨Åc workÔ¨Çows), different
workload patterns (e.g., scheduled jobs), investigate inÔ¨Çu-
encing factors for different latency components (e.g., what
inÔ¨Çuences the orchestration delays), or use ServiBench to an-
alyze novel approaches that build on top of public serverless
platforms [18, 21, 44].

The key component of our replication package is our open-
source tool ServiBench, which encompasses the benchmark
harness, the trace upscaler for the azure functions traces, and
our latency breakdown analysis. It comes with ten realistic
serverless applications out of the box and instructions for the
integration of additional applications. The ServiBench tool
fully automates the application deployment, load generation,
metric collection, and trace analysis for the performance anal-
ysis of serverless applications. To enable the full replication
of the presented results, we additionally include our scripts
for the visual inspection of the azure trace dataset, the raw
data collected during our measurements, and the scripts to
replicate any analysis and Ô¨Ågure from Section 5. The replica-
tion package is currently available as an anonymous GitHub
repository7 and will be archived to Zenodo with a DOI upon
publication.

Figure 12: Thumbnail Generator (App B) generates a
thumbnail of an image uploaded to a storage bucket. The
Ô¨Årst function implements an HTTP API to upload an image
to a storage bucket. The storage event then triggers a second
function to generate a thumbnail of the image and store it in
another storage bucket.

Figure 13: Event Processing (App C) generates and inserts
events into an input queue. The queue triggers a function
which pre-processes the event and places it in the ingested
queue. The placement of an event in the ingested queue
triggers another function to process the event and store the
results in the database.

B Serverless Application Description

This section describes the architecture and functionality of
each application introduced in Table 1. For further details
on implementation details, such as usage proÔ¨Åle, or service
conÔ¨Åguration, we refer to our replication package.

Figure 14: Facial Recognition (App D) app takes a user
uploaded image, extracts a face from it, and detects if the face
already exists in the database. If the face does not already
exist in the database, the app indexes the face and saves a
thumbnail of the face to object storage.

Figure 11: Minimal Baseline (App A) emulates an HTTP
request sent to the API Gateway which triggers a simple
serverless function, and returns a response.

Figure 15: Model Training (App E) application reads train-
ing datasets from object storage, trains machine learning mod-
els on those datasets, and stores the trained models in another
object storage bucket again. The model is a logistic regression
to predict review sentiment scores on the Amazon Fine Food
Review dataset.

7https://github.com/ServiBench/ReplicationPackage

13

User‚ô¢ GatewayFunctionAsyncTriggerUserFunction1: Persist Image‚äó Bucket1: ImagesFunction2: Generate Thumbnail‚äó Bucket2: Thumbnails‚ô¢ GatewayFunction1:Generate EventFunction2: Ingest Event‚¨õ Queue2: Ingested‚¨õ Queue1: InputFunction3: Process Event‚äô Database1: Results‚ô¢ GatewayUploadPhotoUser‚äó Bucket1: ThumbnailsFunction1: Detect FaceFunction2: Is Duplicate?Function4: MakeThumbnail‚ô† Image Detector 1:Detect Face‚ô† Image Detector 2: Detect DuplicateFunction3: Index FaceFunction5: StoreMetadata‚ô† Image Detector 3: Index Face‚äô Database1: Metadata‚ô¢ Gatewayüüä OrchestrateLogical Control FlowUser‚äó Bucket1: DatasetsFunction1: Train Model‚äó Bucket2: TrainedModels‚ô¢ GatewayFigure 16: RealWorld Backend (App F) uses a functions to
create, read, update, and delete user and article information
stored in a database.

Figure 20: Video Processing (App J) application reads
videos from object storage, applies a greyscale Ô¨Ålter, and
transcodes them. The transcoded videos are stored in object
storage.

Figure 17: Hello Retail! (App G) is a retail inventory catalog
application backed by a database. Users can upload product
information and categorize products into categories. Sup-
ports sending an SMS if a product does not have an image.
Uploaded images are stored in object storage.

Figure 18: Todo API (App H) is a simple to-do app which
uses a FaaS to create, read, update, and delete todos stored in
a database.

Figure 19: Matrix Multiplication (App I) generates a ran-
dom matrix, partitions the matrix, and distributes it for mul-
tiplication. Workers perform the multiplication and write
the results to S3. The results are then combined to get the
Ô¨Ånal result of the multiplication. The app is directed by an
orchestration service.

References

[1] Alexandru Agache, Marc Brooker, Alexandra Iordache,
Anthony Liguori, Rolf Neugebauer, Phil Piwonka, and
Diana-Maria Popa. Firecracker: Lightweight virtual-
In 17th USENIX
ization for serverless applications.
Symposium on Networked Systems Design and Imple-
mentation NSDI, pages 419‚Äì434, 2020.

[2] Nabeel Akhtar, Ali Raza, Vatche Ishakian, and Ibrahim
Matta. Cose: ConÔ¨Åguring serverless functions using
statistical learning. In IEEE INFOCOM 2020 - IEEE
Conference on Computer Communications, 2020.

[3] A. Ali, R. Pinciroli, F. Yan, and E. Smirni. Batch: Ma-
chine learning inference serving on serverless platforms
In SC20: International Con-
with adaptive batching.
ference for High Performance Computing, Networking,
Storage and Analysis, pages 972‚Äì986, 2020.

[4] Amazon Web Services, Inc. Amazon X-Ray. https:
//aws.amazon.com/xray/. Last accessed: Jan 2022.

[5] Amazon Web Services,

Inc.

ConÔ¨Åguring
Lambda docu-
https://docs.aws.amazon.com/

function memory (console).
mentation,
lambda/latest/dg/configuration-function-
common.html#configuration-memory-console.
Last accessed: Jan 2022.

[6] Amazon Web Services, Inc. Facial Recognition. https:
//image-processing.serverlessworkshops.io.
Last accessed: Jan 2022.

[7] Amazon Web Services,

Minimal Base-
https://serverlessland.com/patterns/

line.
apigw-lambda-cdk. Last accessed: Jan 2022.

Inc.

[8] Amazon Web Services, Inc. Using AWS Lambda
with AWS X-Ray. https://docs.aws.amazon.com/
lambda/latest/dg/services-xray.html. Last ac-
cessed: Jan 2022.

[9] Ali Anwar, Mohamed Mohamed, Vasily Tarasov,
Michael Littley, Lukas Rupprecht, Yue Cheng, Nannan
Zhao, Dimitris Skourtis, Amit Warke, Heiko Ludwig,

14

Functions: Handle Different Routes‚äô Database: User/Article DBUser‚ô¢ Gateway:Route Requests‚¨õ Queue1:Event QueueUserFunction2: Process Events‚äô Database1:StoreDatabaseFunction3: Get PhotosSendSMSFunction1: Generate Event‚äó Bucket: PhotosFunction4: Upload Photos‚ô¢ Gatewayüüä OrchestrateFunctions: Handle Different Routes‚äô Database: ToDo DatabaseUser‚ô¢ Gateway:Route RequestsReadResultsUserFunction2: DistributeWorkFunction2: Multiply Sub-matrix‚äó Bucket1: Sub-matrices...Read Sub-matrixWrite resultFunction3: Build ResultFunction4: Generate ReportFunction1: GenerateMatrixüüä Orchestrate‚äó Bucket2: Results‚ô¢ GatewayDataFlowControlFlowUser‚äó Bucket1: VideosFunction1: Process Video‚äó Bucket2: ProcessedVideos‚ô¢ GatewayDean Hildebrand, and Ali Raza Butt. Improving docker
registry design based on production workload analysis.
In Nitin Agrawal and Raju Rangaswami, editors, 16th
USENIX Conference on File and Storage Technologies,
FAST 2018, Oakland, CA, USA, February 12-15, 2018,
pages 265‚Äì278. USENIX Association, 2018.

[10] Daniel Barcelona-Pons and Pedro Garc√≠a-L√≥pez. Bench-
marking parallelism in faas platforms. Future Genera-
tion Computer Systems, 124:268‚Äì284, 2021.

[11] Marc Brooker, Adrian Costin Catangiu, Mike Danilov,
Alexander Graf, Colm MacCarthaigh, and Andrei Sandu.
Restoring uniqueness in microvm snapshots. arXiv
preprint arXiv:2102.12892, 2021.

[12] Alex Casalboni.

AWS Lambda Power Tuning.
Github docs, https://github.com/alexcasalboni/
aws-lambda-power-tuning.
Last accessed: Jan
2022.

[13] Paul Castro, Vatche Ishakian, Vinod Muthusamy, and
Aleksander Slominski. The rise of serverless computing.
Communications of the ACM, 62(12):44‚Äì54, 2019.

[19] Jeffrey Dean and Luiz Andr√© Barroso. The tail at scale.
Communications of the ACM, 56(2):74‚Äì80, 2013.

[20] Karim Djemame, Matthew Parker, and Daniel Datsev.
Open-source serverless architectures: an evaluation of
In 2020 IEEE/ACM 13th Inter-
apache openwhisk.
national Conference on Utility and Cloud Computing
(UCC), pages 329‚Äì335, 2020.

[21] Simon Eismann, Long Bui, Johannes Grohmann,
Cristina L. Abad, Nikolas Herbst, and Samuel Kounev.
Sizeless: predicting the optimal size of serverless func-
tions. In Kaiwen Zhang, Abdelouahed Gherbi, Nalini
Venkatasubramanian, and Lu√≠s Veiga, editors, Middle-
ware ‚Äô21: 22nd International Middleware Conference,
Qu√©bec City, Canada, December 6 - 10, 2021, pages
248‚Äì259. ACM, 2021.

[22] Simon Eismann, Joel Scheuner, Erwin Van Eyk, Maxi-
milian Schwinger, Johannes Grohmann, Nikolas Herbst,
Cristina Abad, and Alexandru Iosup. The state of server-
less applications: Collection, characterization, and com-
IEEE Transactions on Software
munity consensus.
Engineering, 2021.

[14] Marcin Copik, Grzegorz Kwasniewski, Maciej Besta,
Michal Podstawski, and Torsten HoeÔ¨Çer. Sebs: a server-
less benchmark suite for function-as-a-service comput-
ing.
In Kaiwen Zhang, Abdelouahed Gherbi, Nalini
Venkatasubramanian, and Lu√≠s Veiga, editors, Middle-
ware ‚Äô21: 22nd International Middleware Conference,
Qu√©bec City, Canada, December 6 - 10, 2021, pages
64‚Äì78. ACM, 2021.

[23] Michael Ferdman, Almutaz Adileh, Onur Kocberber,
Stavros Volos, Mohammad Alisafaee, Djordje Jevdjic,
Cansu Kaynak, Adrian Daniel Popescu, Anastasia Aila-
maki, and Babak FalsaÔ¨Å. Clearing the clouds: A study
of emerging scale-out workloads on modern hardware.
Proceedings of the 17th International Conference on
Architectural Support for Programming Languages and
Operating Systems (ASPLOS), pages 37‚Äì48, 2012.

[15] Robert Cordingly, Wen Shu, and Wes J. Lloyd. Pre-
dicting performance and cost of serverless computing
functions with SAAF. In IEEE International Confer-
ence on Cloud and Big Data Computing, pages 640‚Äì649,
2020.

[16] Robert Cordingly, Hanfei Yu, Varik Hoang, David Perez,
David Foster, Zohreh Sadeghi, Rashad Hatchett, and
Wes J. Lloyd.
Implications of programming lan-
guage selection for serverless data processing pipelines.
In IEEE DASC/PiCom/CBDCom/CyberSciTech, pages
704‚Äì711, 2020.

[17] Mark Crovella and Azer Bestavros. Self-similarity in
world wide web trafÔ¨Åc: evidence and possible causes.
IEEE/ACM Trans. Netw., 5(6):835‚Äì846, 1997.

[18] J√°nos Czentye, Istv√°n Pelle, Andr√°s Kern, Bal√°zs P√©ter
Gero, L√°szl√≥ Toka, and Bal√°zs Sonkoly. Optimizing
latency sensitive applications for amazon‚Äôs public cloud
platform. In 2019 IEEE Global Communications Con-
ference GLOBECOM, pages 1‚Äì7, 2019.

[24] Kamil Figiela, Adam Gajek, Adam Zima, Beata Obrok,
and Maciej Malawski. Performance evaluation of het-
erogeneous cloud functions. Concurrency and Compu-
tation: Practice and Experience, 30(23), 2018.

[25] Aleksandr Filichkin.
solving

or

GraalVM + AWS
cold start problem.

Java

Lambda
https://filia-aleks.medium.com/graalvm-
aws-lambda-or-solving-java-cold-start-
problem-2655eeee98c6, 2021. Last accessed: Jan
2022.

[26] Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty,
Priyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu,
Brian Ritchken, Brendon Jackson, Kelvin Hu, Meghna
Pancholi, Yuan He, Brett Clancy, Chris Colen, Fukang
Wen, Catherine Leung, Siyuan Wang, Leon Zaruvinsky,
Mateo Espinosa, Rick Lin, Zhongling Liu, Jake Padilla,
and Christina Delimitrou. An open-source benchmark
suite for microservices and their hardware-software im-
plications for cloud & edge systems. In Proceedings
of the 24th International Conference on Architectural

15

Support for Programming Languages and Operating
Systems (ASPLOS), pages 3‚Äì18, 2019.

[27] Martin Grambow, Tobias Pfandzelter, Luk Burchard,
Max Schubert, Carsten Zhao, and David Bermbach. Be-
FaaS: An application-centric benchmarking framework
for faas platforms. In Proceedings of the 9th IEEE In-
ternational Conference on Cloud Engineering (IC2E),
2021.

[28] Hassan B. Hassan, Saman A. Barakat, and Qusay I.
Sarhan. Survey on serverless computing. Journal of
Cloud Computing, 10(1):39, 2021.

[29] Wilhelm Hasselbring. Benchmarking as empirical stan-
dard in software engineering research.
In Ruzanna
Chitchyan, Jingyue Li, Barbara Weber, and Tao Yue,
editors, EASE 2021: Evaluation and Assessment in
Software Engineering, Trondheim, Norway, June 21-24,
2021, pages 365‚Äì372. ACM, 2021.

[30] Michael H√ºttermann. Infrastructure as Code. Apress,

2012.

[31] David Jackson and Gary Clynch. An investigation of
the impact of language runtime on the performance and
cost of serverless functions. In 2018 IEEE/ACM Inter-
national Conference on Utility and Cloud Computing
Companion (UCC Companion), pages 154‚Äì160. IEEE,
2018.

[32] k6.io. https://k6.io/. Last accessed: Jan 2022.

[33] Anish Karandikar.

Real-world Backend.

https://github.com/anishkny/realworld-
dynamodb-lambda. Last accessed: Jan 2022.

[34] Jeongchul Kim and Kyungyong Lee. FunctionBench:
A suite of workloads for serverless cloud function ser-
In Proceedings of the 12th IEEE International
vice.
Conference on Cloud Computing (CLOUD WIP), pages
502‚Äì504, 2019.

[35] Ana Klimovic, Yawen Wang, Patrick Stuedi, Animesh
Trivedi, Jonas Pfefferle, and Christos Kozyrakis. Pocket:
Elastic ephemeral storage for serverless analytics. In
13th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 18), pages 427‚Äì444, 2018.

[36] Simon Kuenzer, Vlad-Andrei Badoiu, Hugo Lefeuvre,
Sharan Santhanam, Alexander Jung, Gaulthier Gain,
Cyril Soldani, Costin Lupu, Stefan Teodorescu, Costi
Raducanu, Cristian Banu, Laurent Mathy, Razvan Dea-
conescu, Costin Raiciu, and Felipe Huici. Unikraft: fast,
In EuroSys ‚Äô21:
specialized unikernels the easy way.
Sixteenth European Conference on Computer Systems,
pages 376‚Äì394, 2021.

[37] J√∂rn Kuhlenkamp, Sebastian Werner, Maria C. Borges,
Dominik Ernst, and Daniel Wenzel. Benchmarking elas-
ticity of FaaS platforms as a foundation for objective-
driven design of serverless applications. In Proceedings
of the 35th ACM/SIGAPP Symposium on Applied Com-
puting (SAC), pages 1576‚Äì1585, 2020.

[38] Hyungro Lee, Kumar Satyam, and Geoffrey C Fox.
Evaluation of production serverless computing envi-
ronments. In Proceedings of the 11th IEEE CLOUD:
3rd International Workshop on Serverless Computing
(WoSC), pages 442‚Äì50, 2018.

[39] Philipp Leitner, Erik Wittern, Josef Spillner, and Walde-
mar Hummer. A mixed-method empirical study of
function-as-a-service software development in industrial
practice. Journal of Systems and Software, 149:340‚Äì
359, 2019.

[40] Valentina Lenarduzzi and Annibale Panichella. Server-
less testing: Tool vendors‚Äô and experts‚Äô point of view.
IEEE Software, 38(1):54‚Äì60, 2020.

[41] Changyuan Lin and Hamzeh Khazaei. Modeling and
optimization of performance and cost of serverless appli-
cations. IEEE Transactions on Parallel and Distributed
Systems, 32(3):615‚Äì632, 2020.

[42] W. Lloyd, M. Vu, B. Zhang, O. David, and G. Leaves-
ley. Improving application migration to serverless com-
puting platforms: Latency mitigation with keep-alive
workloads. In Companion of the 11th IEEE/ACM UCC:
4th International Workshop on Serverless Computing
(WoSC), pages 195‚Äì00, 2018.

[43] Wes Lloyd, Shruti Ramesh, Swetha Chinthalapati, Lan
Ly, and Shrideep Pallickara. Serverless computing: An
investigation of factors inÔ¨Çuencing microservice per-
formance. In 2018 IEEE International Conference on
Cloud Engineering (IC2E), pages 159‚Äì169, 2018.

[44] Pedro Garc√≠a L√≥pez, Aitor Arjona, Josep Samp√©, Alek-
sander Slominski, and Lionel Villard. TriggerÔ¨Çow:
trigger-based orchestration of serverless workÔ¨Çows. In
Proceedings of the 14th ACM International Conference
on Distributed and Event-based Systems, pages 3‚Äì14,
2020.

[45] Jonathan Mace. End-to-End Tracing: Adoption and Use

Cases. Survey, Brown University, 2017.

[46] Pascal Maissen, Pascal Felber, Peter Kropf, and Valerio
Schiavoni. Faasdom: A benchmark suite for serverless
In Proceedings of the 14th ACM Inter-
computing.
national Conference on Distributed and Event-based
Systems, 2020.

16

[47] Johannes Manner, Martin Endre√ü, Tobias Heckel, and
Guido Wirtz. Cold start inÔ¨Çuencing factors in function
as a service. In Companion of the 11th IEEE/ACM UCC:
4th International Workshop on Serverless Computing
(WoSC), pages 181‚Äì88, 2018.

[48] Johannes Manner, Martin Endre√ü, Tobias Heckel, and
Guido Wirtz. Cold start inÔ¨Çuencing factors in function
as a service. In 2018 IEEE/ACM International Confer-
ence on Utility and Cloud Computing Companion (UCC
Companion), pages 181‚Äì188. IEEE, 2018.

[49] Garrett McGrath and Paul R. Brenner.

Serverless
computing: Design, implementation, and performance.
In 2017 IEEE 37th International Conference on Dis-
tributed Computing Systems Workshops (ICDCSW),
pages 405‚Äì410, 2017.

[50] Microsoft, Inc.

Azure Public Dataset.

https:
//github.com/Azure/AzurePublicDataset/blob/
master/AzureFunctionsDataset2019.md.
Last
accessed: Jan 2022.

[51] Branko Minic.

Improving cold start times of Java
AWS Lambda functions using GraalVM and native
https://shinesolutions.com/2021/08/
images.
30/improving-cold-start-times-of-java-aws-
lambda-functions-using-graalvm-and-native-
images/, 2021. Last accessed: Jan 2022.

[52] Ali NajaÔ¨Å, Amy Tai, and Michael Wei. Systems re-
In Workshop on Hot

search is running out of time.
Topics in Operating Systems (HotOS ‚Äô21), 2021.

[53] Nordstrom. Hello Retail! https://web.archive.

org/web/20210119044741/https://acloudguru.
com/blog/engineering/serverless-event-
sourcing-at-nordstrom-ea69bd8fb7cc.
accessed: Jan 2022.

Last

[54] Open Telemetry.

https://github.com/open-

telemetry/opentelemetry-specification/
issues/65. Last accessed: Jan 2022.

[55] Istv√°n Pelle, J√°nos Czentye, J√°nos D√≥ka, and Bal√°zs
Sonkoly. Towards latency sensitive cloud native appli-
cations: A performance study on AWS. In Proceedings
of the 12th IEEE International Conference on Cloud
Computing (CLOUD), pages 272‚Äì280, 2019.

[56] Qifan Pu, Shivaram Venkataraman, and Ion Stoica.
ShufÔ¨Çing, fast and slow: Scalable analytics on server-
less infrastructure. In Proceedings of the 16th USENIX
Symposium on Networked Systems Design and Imple-
mentation (NSDI), pages 193‚Äì206, 2019.

17

[57] Haoran Qiu, Subho S. Banerjee, Saurabh Jha, Zbig-
niew T. Kalbarczyk, and Ravishankar K. Iyer. FIRM:
An intelligent Ô¨Åne-grained resource management frame-
work for slo-oriented microservices. In 14th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 20), pages 805‚Äì825, 2020.

[58] Sterling Quinn, Robert Cordingly, and Wes Lloyd. Im-
plications of alternative serverless application control
In Proceedings of the Seventh Inter-
Ô¨Çow methods.
national Workshop on Serverless Computing (WoSC7)
2021, WoSC ‚Äô21, pages 17‚Äì22. Association for Com-
puting Machinery, 2021.

[59] Ali Raza, Ibrahim Matta, Nabeel Akhtar, Vasiliki
Kalavari, and Vatche Isahagian. Function-as-a-service:
JSys,
From an application developer‚Äôs perspective.
1(1):1‚Äì20, 2021.

[60] Stan Salvador and Philip Chan. Toward accurate dy-
Intell.

namic time warping in linear time and space.
Data Anal., 11(5):561‚Äì580, 2007.

[61] Raja R Sambasivan, Rodrigo Fonseca, Ilari Shafer, and
Gregory R Ganger. So, you want to trace your dis-
tributed system? key design insights from years of prac-
tical experience. Technical Report, 2014.

[62] Bruno Schaatsbergen.

Pre-jitting in AWS Lambda
functions. https://www.bschaatsbergen.com/pre-
jitting-in-lambda, 2021. Last accessed: Jan 2022.

[63] Joel Scheuner and Philipp Leitner.

Function-as-a-
service performance evaluation: A multivocal litera-
ture review. Journal of Systems and Software (JSS),
170:110708, 2020.

[64] Johann Schleier-Smith, Vikram Sreekanti, Anurag
Khandelwal, Joao Carreira, Neeraja Jayant Yadwadkar,
Raluca Ada Popa, Joseph E. Gonzalez, Ion Stoica, and
David A. Patterson. What serverless computing is
and should become: the next phase of cloud computing.
Commun. ACM, 64(5):76‚Äì84, 2021.

[65] Mohammad Shahrad, Jonathan Balkind, and David
Wentzlaff. Architectural implications of function-as-
In Proceedings of the 52nd
a-service computing.
IEEE/ACM International Symposium on Microarchitec-
ture (MICRO), pages 1063‚Äì1075, 2019.

[66] Mohammad Shahrad, Rodrigo Fonseca, I√±igo Goiri,
Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo
Laureano, Colby Tresness, Mark Russinovich, and Ri-
cardo Bianchini. Serverless in the wild: Characterizing
and optimizing the serverless workload at a large cloud
provider. In 2020 USENIX Annual Technical Confer-
ence (ATC), pages 205‚Äì218, 2020.

[67] Benjamin H Sigelman, Luiz Andre Barroso, Mike Bur-
rows, Pat Stephenson, Manoj Plakal, Donald Beaver,
Saul Jaspan, and Chandan Shanbhag. Dapper, a large-
scale distributed systems tracing infrastructure. Techni-
cal report, Google, 2010.

[68] C√©sar Soto-Valero, Thomas Durieux, Nicolas Harrand,
and Benoit Baudry. Trace-based debloat for java byte-
code. arXiv preprint arXiv:2008.08401, 2020.

[69] Josef Spillner and Mohammed Al-Ameen.

Server-
less Literature Dataset. https://doi.org/10.5281/
zenodo.1175423, 2019.

[70] Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric
Sequoia: Enabling
Rozner, and Siddharth Lanka.
In Pro-
quality-of-service in serverless computing.
ceedings of the ACM Symposium on Cloud Computing
(SoCC), 2020.

[71] Barry N Taylor and Chris E Kuyatt. Guidelines for
evaluating and expressing the uncertainty of NIST mea-
surement results. Technical report, National Institute of
Standards and Technology, 1994.

[72] Dmitrii Ustiugov, Theodor Amariucai, and Boris Grot.
Analyzing tail latency in serverless clouds with stellar.
In 2021 IEEE International Symposium on Workload
Characterization (IISWC‚Äô21), 2021.

[73] Dmitrii Ustiugov, Plamen Petrov, Marios Kogias,
Edouard Bugnion, and Boris Grot. Benchmarking, anal-
ysis, and optimization of serverless function snapshots.
In ASPLOS ‚Äô21: 26th ACM International Conference
on Architectural Support for Programming Languages
and Operating Systems, pages 559‚Äì572, 2021.

[74] Erwin van Eyk, Alexandru Iosup, Johannes Grohmann,
Simon Eismann, Andr√© Bauer, Laurens Versluis, Lucian
Toader, Norbert Schmitt, Nikolas Herbst, and Cristina L.
Abad. The SPEC-RG reference architecture for faas:
From microservices and containers to serverless plat-
forms. IEEE Internet Comput., 23(6):7‚Äì18, 2019.

[75] Erwin van Eyk, Alexandru Iosup, Simon Seif, and
Markus Th√∂mmes. The SPEC Cloud group‚Äôs research
vision on FaaS and serverless architectures. In Proceed-
ings of the 2nd International Workshop on Serverless
Computing (WOSC), pages 1‚Äì4, 2017.

[76] Erwin van Eyk, Lucian Toader, Sacheendra Talluri, Lau-
rens Versluis, Alexandru Uta, and Alexandru Iosup.
Serverless is more: From paas to present cloud com-
puting. IEEE Internet Comput., 22(5):8‚Äì17, 2018.

Chou, Sonia Margulis, Daniel Obenshain, Shruti Pad-
manabha, Ashish Shah, Yee Jiun Song, and Tianyin
Xu. Maelstrom: Mitigating datacenter-level disasters by
draining interdependent trafÔ¨Åc safely and efÔ¨Åciently. In
13th USENIX Symposium on Operating Systems Design
and Implementation (OSDI), pages 373‚Äì389, 2018.

[78] J√≥akim von Kistowski, Jeremy A. Arnold, Karl Huppler,
Klaus-Dieter Lange, John L. Henning, and Paul Cao.
How to build a benchmark. In Proceedings of the 6th
ACM/SPEC International Conference on Performance
Engineering (ICPE), pages 333‚Äì336, 2015.

[79] Ao Wang, Shuai Chang, Huangshi Tian, Hongqi Wang,
Haoran Yang, Huiba Li, Rui Du, and Yue Cheng. Faas-
net: Scalable and fast provisioning of custom serverless
container runtimes at alibaba cloud function compute. In
Irina Calciu and Geoff Kuenning, editors, 2021 USENIX
Annual Technical Conference, USENIX ATC 2021, July
14-16, 2021, pages 443‚Äì457. USENIX Association,
2021.

[80] Hao Wang, Di Niu, and Baochun Li. Distributed ma-
chine learning with a serverless architecture. In 2019
IEEE Conference on Computer Communications (IN-
FOCOM), pages 1288‚Äì1296, 2019.

[81] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas
Ristenpart, and Michael Swift. Peeking behind the
curtains of serverless platforms. In Proceedings of the
USENIX Annual Technical Conference (ATC), pages
133‚Äì146, 2018.

[82] Jinfeng Wen, Yi Liu, Zhenpeng Chen, Junkai Chen,
and Yun Ma. Characterizing commodity serverless
computing platforms. Journal of Software: Evolution
and Process, page e2394, 2021.

[83] Wilkinson et al. The FAIR Guiding Principles for
scientiÔ¨Åc data management and stewardship. Nature
SciData, 3, 2016.

[84] Alex Williams. Guide to serverless technologies. Tech-

nical report, The New Stack, 2018.

[85] Jeffrey R. Yost. Making IT Work: A History of the
Computer Services Industry. The MIT Press, 2017.

[86] Tianyi Yu, Qingyuan Liu, Dong Du, Yubin Xia, Binyu
Zang, Ziqian Lu, Pingchao Yang, Chenggang Qin, and
Haibo Chen. Characterizing serverless platforms with
serverlessbench. In Proceedings of the ACM Symposium
on Cloud Computing, pages 30‚Äì44, 2020.

[77] Kaushik Veeraraghavan, Justin Meza, Scott Michel-
son, Sankaralingam Panneerselvam, Alex Gyori, David

[87] Vladimir Yussupov, Uwe Breitenb√ºcher, Frank Ley-
mann, and Christian M√ºller. Facing the unplanned

18

migration of serverless applications: A study on porta-
bility problems, solutions, and dead ends. In Proceed-
ings of the 12th IEEE/ACM International Conference
on Utility and Cloud Computing, pages 273‚Äì283, 2019.

[88] Vladimir Yussupov, Uwe Breitenb√ºcher, Frank Ley-
mann, and Michael Wurster. A systematic mapping
study on engineering function-as-a-service platforms
and tools. In Proceedings of the 12th IEEE/ACM Inter-
national Conference on Utility and Cloud Computing,
pages 229‚Äì240, 2019.

[89] Miao Zhang, Yifei Zhu, Cong Zhang, and Jiangchuan
Liu. Video processing with serverless computing: A
measurement study. In Proceedings of the 29th ACM
workshop on network and operating systems support for
digital audio and video, pages 61‚Äì66, 2019.

[90] Tian Zhang, Dong Xie, Feifei Li, and Ryan Stutsman.
Narrowing the gap between serverless and its state with
storage functions. In Proceedings of the ACM Sympo-
sium on Cloud Computing, pages 1‚Äì12, 2019.

19

