Runtime Prevention of Deserialization Attacks

FranÃ§ois Gauthier
francois.gauthier@oracle.com
Oracle Labs
Brisbane, Queensland, Australia

Sora Bae
sora.bae@oracle.com
Oracle Labs
Brisbane, Queensland, Australia

2
2
0
2

r
p
A
0
2

]

R
C
.
s
c
[

1
v
8
8
3
9
0
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
Untrusted deserialization exploits, where a serialised object graph
is used to achieve denial-of-service or arbitrary code execution,
have become so prominent that they were introduced in the 2017
OWASP Top 10. In this paper, we present a novel and lightweight
approach for runtime prevention of deserialization attacks using
Markov chains. The intuition behind our work is that the features
and ordering of classes in malicious object graphs make them dis-
tinguishable from benign ones. Preliminary results indeed show
that our approach achieves an F1-score of 0.94 on a dataset of 264
serialised payloads, collected from an industrial Java EE application
server and a repository of deserialization exploits.

KEYWORDS
Deserialization, Markov chains, Runtime protection

ACM Reference Format:
FranÃ§ois Gauthier and Sora Bae. 2022. Runtime Prevention of Deserialization
Attacks. In New Ideas and Emerging Results (ICSE-NIERâ€™22), May 21â€“29, 2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.
1145/3510455.3512786

1 INTRODUCTION
In programming languages, serialization is the process of converting
an in-memory object or data structure into a persistent format;
deserialization works the opposite way. An attacker accessing the
serialized form of an object can thus influence the object that will be
created upon deserialization. In recent years, security researchers
discovered various ways of abusing deserialization to achieve denial-
of-service or arbitrary code execution in various languages like Java,
C#, PHP, Python, and Ruby using various serialization formats like
binary, XML, JSON, and YAML [3, 8, 11, 12]. Deserialization issues
are now so prominent that they are now included in the OWASP Top
10 Web Application Security Risks list [2]. In this paper, we focus
on detecting attacks against native Java deserialization that uses
byte streams as the serialization format. To help combat the threat
posed by native Java deserialization vulnerabilities, deserialization
filters were introduced in Java 9 and back-ported to Java 6, 7, and
8 [1]. Upon deserialization of, the filter is invoked after resolving
the class from the byte stream and before creating an object of that
class in memory, giving the filter an opportunity to inspect the
class and stop the deserialization process altogether if a forbidden
class is detected. However, the onus of developing the filters is on
developers and the manual effort involved is not trivial. In this

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
This is the authorâ€™s version of the work. It is posted here for your personal use.
Not for redistribution. The definitive Version of Record was published in New Ideas
and Emerging Results (ICSE-NIERâ€™22), May 21â€“29, 2022, Pittsburgh, PA, USA, https:
//doi.org/10.1145/3510455.3512786.

Figure 1: A simple class diagram and its corresponding byte
stream

paper, we propose an approach to automatically build probabilistic
models from benign and malicious byte streams to detect malicious
deserialization at runtime.

2 BACKGROUND ON JAVA
DESERIALIZATION

In Java, any object from a class that directly or indirectly (i.e.
through inheritance) implements the Serializable interface can
be serialized and deserialized using Java native serialization. During
the serialization process, starting from the root object, references to
other objects (e.g. through class fields) are resolved and serialized
deterministically in a recursive manner until the entire object graph
has been converted to a byte stream. During deserialization, the
serialized object graph is read sequentially from the byte stream,
one object at the time. When an object is deserialized, the first infor-
mation that is extracted from the stream is its class name, at which
point deserialization filters are invoked to give the application an
opportunity to introspect the class and interrupt deserialization if
desired. Figure 1 shows an example of a class diagram (top) with a
simplified representation of its corresponding byte stream (bottom).
Assuming that instances of those classes have been constructed,
serializing the corresponding object graph would result in the byte
stream at the bottom of Figure 1. The first part of the byte stream
contains the name of the class of the first object to be deserialized,
followed by field descriptions. The last part of the stream contains
the field values, which can be objects themselves. If this stream was
deserialized, the deserialization filter would receive the TimerTask
class first, followed by the CommandTask class. Detailing the exact
mechanisms that an attacker can use to exploit Java deserialization
is beyond the scope of this paper, but the interested reader can
refer to [8, 9, 11] for more information. The key takeaway is that
object graphs are serialized, deserialized, and filtered sequentially;
a property we leverage to abstract them as Markov chains.

TimerTask+ task: Runnable+ readObject(ObjectInputStream): voidCommandTask : Runnable+ command: String+ run(): voidTimerTaskLtaskRunnableCommandTaskLcommandString"calc.exe"Class nameField descriptionField value 
 
 
 
 
 
ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

FranÃ§ois Gauthier and Sora Bae

3 BACKGROUND ON MARKOV CHAINS
A Markov chain represents a system that has a finite number of
states: ð‘† = {ð‘ 1, ð‘ 2, . . . , ð‘ ð‘› } and that transitions between states with
some probability ð‘ at each step ð‘¡. The probability of the system
starting in a state ð‘ ð‘– âˆˆ ð‘† is captured by its initial state probability
vector: ð‘ð‘–ð‘›ð‘–ð‘¡ = (ð‘1, ð‘2, . . . , ð‘ð‘›), where each probability ð‘ð‘– corre-
sponds to the probability of the chain starting in state ð‘ ð‘– and where
the probabilities in ð‘ð‘–ð‘›ð‘–ð‘¡ sum to one. In Markov chains, the proba-
bility of transitioning from a state ð‘ ð‘– to another state ð‘  ð‘— depends on
ð‘ ð‘– only, and is captured by a transition probability matrix, where
rows correspond to the state at step ð‘¡, columns correspond to the
state at step ð‘¡ + 1, and each row sums to one:
ð‘1ð‘›
...
ð‘ð‘›ð‘›

ð‘12
...
ð‘ð‘›2
Given a Markov chain and a sequence of states (ð‘¥1, ð‘¥2, . . . , ð‘¥ð‘›), one
can calculate the probability that the chain generated the sequence
with a simple product of probabilities:

ð‘11
...
ð‘ð‘¡ð‘Ÿ = (cid:169)
(cid:173)
(cid:173)
ð‘ð‘›1
(cid:171)

. . .
. . .
. . .

(cid:170)
(cid:174)
(cid:174)
(cid:172)

ð‘ƒ ((ð‘¥1, ð‘¥2, . . . , ð‘¥ð‘›)) = ð‘ð‘–ð‘›ð‘–ð‘¡ (ð‘¥1) Â·

ð‘›
(cid:214)

ð‘–=2

ð‘ð‘¡ð‘Ÿ (ð‘¥ð‘–âˆ’1ð‘¥ð‘– )

(1)

4 MODELLING JAVA DESERIALIZATION

WITH MARKOV CHAINS

In this section, we explain how we model deserialization as a
Markov chain. Our choice of Markov chains over other learning
approaches was motivated by two main factors: 1) previous failed
experiments with a sequence-agnostic classifier, and 2) the need
to make predictions based on small datasets. We indeed experi-
mented with naÃ¯ve Bayes classifiers first and only achieved mar-
ginal improvement over random classification. Then, we chose
Markov chains (MC) over more complex sequence-based learning
approaches that require large datasets like RNN, and LSTM because
in our setup, deserialization is a relatively uncommon operation,
leading to a small dataset. In section 6, we show how MC gen-
erated from Bayesian inference, which accounts for the inherent
uncertainty of small datasets, lead to significantly more precise
predictions than MC that are derived directly from empirical data.

4.1 Abstracting Java classes as states in a

Markov chain

Classes in a stream and their various features determine the code
that is executed during deserialization. For example, in Figure 1,
because the CommandTask class implements the Runnable interface,
the CommandTask constructor will be invoked to create a Runnable
object for the task field. Also, because the TimerTask class over-
rides the readObject method, it can alter default deserialization,
which is a feature that is often exploited in deserialization attacks.
To exploit a deserialization vulnerability, attackers will seek to
craft a byte stream consisting of specific classes with specific fea-
tures in a specific order that will typically lead to denial-of-service
or arbitrary code execution. The key insight here is that the classâ€™s
intended use is largely irrelevant to attackers, who are instead solely
interested in the very specific features that will lead to successful ex-
ploitation. For this reason, we studied all the deserialization exploits

in ysoserial [7] and manually identified some of the features that
make them useful for exploitation purposes. The non-exhaustive
list of features we identified are listed in Table 1.

Given the set of features in Table 1, we can abstract each class
as a Boolean feature vector. Given ð‘› Boolean features, the number
of possible feature vectors is finite and equal to 2ð‘›. In our setup,
the set of states in the Markov chain is thus the set of possible
feature vectors. Because the maximum number of states grows
exponentially with the number of features, in practice, we use the
set of feature vectors that are observed in the training set, which
cardinality tends to be much smaller than 2ð‘›. To account for new
feature vectors in the testing set, we create a generic state to which
all unobserved states map to.

4.2 Estimating probabilities from data
We estimate the various probabilities in our Markov chain directly
from dynamic observations. Specifically, given a set of byte streams,
we deserialize them, extract the resulting sequences of classes,
and abstract all classes to feature vectors. This results in a set
of concrete Markov chain instances (i.e. sequences of states) from
which we can estimate the initial and state transition probabilities.
The most straightforward approach is to directly use empirically
observed frequencies as probabilities. This works well in contexts
where the sample size is large. When the sample size is small,
however, statistical inference methods are generally preferable. In
this work, we use Bayesian inference to estimate the probabilities of
a Markov chain where empirical observations are used to guide the
inference process. Bayesian inference models the variables to infer
as random variables issued from specific probability distributions.
Then, through a guided random process (e.g. Markov Chain Monte
Carlo), it infers the parameters of those probability distributions
that maximise the likelihood of the empirically observed values.
Consider, for example, the transition probability matrix of a Markov
chain. Our goal is to estimate the transition probabilities that best
explain the observed sequences of states. A typical way of modelling
such a matrix is to represent each row as the outcome of a Dirichlet
distribution, which is parameterised with a vector of concentration
parameters (ð›¼1, . . . , ð›¼ð¾ ) where ð›¼ð‘– > 0 and produces as output a
vector of ð¾ real numbers that sum to one:

(ð‘¥1, . . . , ð‘¥ð¾ ), where ð‘¥ð‘– âˆˆ [0, 1], and

ð¾
âˆ‘ï¸

ð‘–=1

ð‘¥ð‘– = 1

The concentration vector is used to initialise the distribution and
captures our prior knowledge about transition probabilities. In our
setup, all the concentration parameters are set to one, to represent
that we have no prior knowledge. By repeatedly adjusting the
concentration parameters, sampling from the per-row Dirichlet
distributions, and evaluating the likelihood of the resulting matrix
against our observations, Bayesian inference eventually converges
to a set of likely transition matrices. It is important to note that
through its inference process, Bayesian inference actually generates
multiple distributions for each row, where the more recent ones are
expected to more precisely capture the real probabilities. In cases
where the observations are few, the inference might not converge
to a single solution, but rather to a set of plausible solutions. We

Runtime Prevention of Deserialization Attacks

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

Table 1: Common class features used in deserialization exploits

Id

Feature

Description

1 Uses reflection

2 Overrides readObject

3 Overrides hashCode

4 Has generic field

Implements Map

5

6

True if the class calls any of the following from java.lang.reflect:
- Constructor.newInstance()
- Field.set()
- Method.invoke()
True if the class overrides the method Object readObject(ObjectInputStream ois)
True if the class overrides the int hashCode() method.
True if the class has a field of any of the following type:
- java.lang.Object
- java.lang.Comparable
- java.util.Comparator
True if the class implements the java.util.Map interface.

Implements Comparator True if the class implements the java.util.Comparator interface.

7 Calls hashCode

8 Calls compare

True if the class calls any of the following methods:
- int java.util.Objects.hash(Object... values)
- int java.util.Objects.hashCode(Object o)
- *.hashCode()
True if the class calls any of the following methods:
- *.compare()
- *.compareTo(...)

later use metrics like standard deviation over the set of generated
solutions to estimate the confidence in our predictions.

5 RUNTIME PREVENTION OF

DESERIALIZATION ATTACKS

We now present our approach to infer Markov chains from benign
and malicious deserialization examples and predict if a given byte
stream is malicious. In our setup, the benign and malicious exam-
ples come from the application under test and from the ysoserial
dataset [7, 11] respectively. To collect the classes and their associ-
ated features, we implemented a custom deserialization filter that
uses ASM [4] to dynamically extract features from deserialised
classes. To enable the classification of byte streams as benign or
malicious, we create two Markov chains during the inference phase:
one from benign examples, and one from malicious examples. Once
the inference phase is complete, we use another custom deserial-
ization filter to detect and prevent deserialization attacks based
on the inferred Markov chains. A simplified filter is illustrated in
algorithm 1. It takes as input the benign (B) and malicious (M)
Markov chains as well as two threshold parameters ð‘¡ and ð‘™. Once
deserialization starts, the Java runtime invokes the filter every time
a new class is read from the serialized stream and passes it the class
and a Boolean flag indicating whether the end of the stream has
been reached. In practice, we must derive the ð‘’ð‘›ð‘‘ flag from other
filter inputs, but we omit these details for clarity. The filter then
uses ASM [4] to abstract the class as a feature vector and appends
it to the current sequence of classes (lines 3-4). It then computes
the mean probability that the sequence has been generated by the
benign or malicious Markov chains (lines 5-6). Then, it computes

confidence intervals of ð‘¡ standard deviations around the means and
checks if the intervals are disjoint (line 7). If the end of the stream
has been reached and the intervals are disjoint, the highest mean
probability determines the outcome (line 9). If the end has been
reached but the intervals are not disjoint, we do not have enough
confidence in the results to reach a decision and conservatively
reject the stream (line 11). If the intervals are disjoint, and at least ð‘™
classes have been read from the stream, deserialization is aborted
early if the stream is malicious (line 13). Otherwise, the filter post-
pones the decision and lets deserialization proceed to the next class
in the stream by returning undecided.

6 PRELIMINARY RESULTS
To validate our approach, we conducted an experiment on the Ora-
cle WebLogic Server1 2 (WLS). In our setup, we collected 227 benign
deserialization chains (avg. length of 38.96) from trusted runs of
WLS. We also collected 37 malicious chains (avg. length of 16.68)
from the deserialization payloads available in [7]. To measure the
precision, recall and F1-score of our approach, we conduct a 5-fold
cross-validation experiment where 80% of the examples are used
for inference and the remaining 20% are used for prediction. Fur-
thermore, to assess the benefits of using statistical inference to
estimate probabilities, we also conduct the same experiment us-
ing Markov chains inferred directly from empirical data. We use
PyMC3 for Bayesian inference [16] and Pomegranate for empirical

1https://www.oracle.com/au/middleware/technologies/fusionmiddleware-
downloads.html
2OracleÂ®WebLogic Server is a registered trademark of Oracle and/or its affiliates.
Other names may be trademarks of their respective owners.

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

FranÃ§ois Gauthier and Sora Bae

Algorithm 1: Deserialization Attack Prevention
Input: B, M, ð‘¡, ð‘™
Output: status âˆˆ {accepted, rejected, undecided}

1 ð‘ ð‘’ð‘ž â† new List()
2 Function MarkovFilter(class, end):
3

ð‘“ ð‘’ð‘Žð‘¡ð‘¢ð‘Ÿð‘’ð‘  â† ExtractFeatures(ð‘ð‘™ð‘Žð‘ ð‘ )
ð‘ ð‘’ð‘ž.append(ð‘“ ð‘’ð‘Žð‘¡ð‘¢ð‘Ÿð‘’ð‘ )
ð‘ƒ B â† ð‘šð‘’ð‘Žð‘›(ð‘ƒ (ð‘ ð‘’ð‘ž | B))
ð‘ƒM â† ð‘šð‘’ð‘Žð‘›(ð‘ƒ (ð‘ ð‘’ð‘ž | M))
ð‘‘ð‘–ð‘  ð‘—ð‘œð‘–ð‘›ð‘¡ â† ((ð‘ƒ B Â± ð‘¡ðœŽ) âˆ© (ð‘ƒM Â± ð‘¡ðœŽ) = âˆ…)
if ð‘’ð‘›ð‘‘ and ð‘‘ð‘–ð‘  ð‘—ð‘œð‘–ð‘›ð‘¡ then

return ð‘ƒM > ð‘ƒ B ? rejected : accepted

else if ð‘’ð‘›ð‘‘ and Â¬ð‘‘ð‘–ð‘  ð‘—ð‘œð‘–ð‘›ð‘¡ then

return rejected

else if ð‘‘ð‘–ð‘  ð‘—ð‘œð‘–ð‘›ð‘¡ and |ð‘ ð‘’ð‘ž| â‰¥ ð‘™ and ð‘ƒM > ð‘ƒ B then

return rejected

else

return undecided

4

5

6

7

8

9

10

11

12

13

14

15

end

16
17 end

Table 2: Precision, recall, F1-score, and inference time of
Bayesian and empirical Markov chains

ð‘¡

0
1
2
3

Precision

Recall

F1-score

91.67Â±6.97
91.67Â±6.97
89.72Â±8.94
88.17Â±11.26

96.67Â±6.67
96.67Â±6.67
100.0Â±0.00
100.0Â±0.00

0.94Â±0.03
0.94Â±0.03
0.94Â±0.05
0.93Â±0.07

Time
(sec)

7163

Bayesian

Empirical â€” 72.95Â±14.27

100.0Â±0.00

0.84Â±0.09

0.7

inference [17]. Table 2 shows the results of our experiment with
ð‘¡ âˆˆ [0, 3] and ð‘™ = âˆž, drawing 5 000 samples from a Metropolis-
Hastings sampler and using the last 500 samples for prediction.
Bayesian inference performs significantly better, at the expense
of inference times that are orders of magnitude larger. Note, how-
ever, that inference can be performed offline and that the actual
runtime overhead, in the order of milliseconds, is similar for both
approaches (i.e. extracting class features and resolving Equation 1).
In algorithm 1, we let the filter stop the deserialization of malicious
streams early if at least ð‘™ classes have been read and the end of
the stream has not been reached. Figure 2 shows the precision and
recall achieved with different values of ð‘™, and ð‘¡ = 2. Reading more
classes is beneficial to precision while recall remains largely unaf-
fected. To our knowledge, this is the first study to use class features
and ordering for defensive purposes against deserialization attacks.
Considering our previous failed experiment with order-agnostic
classifiers and our current F1-score of 0.94 using MC, our results
suggest that class features and ordering do capture the esssence of
malicious chains.

Figure 2: Average precision and recall in function of ð‘™

7 RELATED WORK
In the security community, the classes used in a deserialization
payload are referred to as "gadgets" and the resulting byte stream is
known as a "gadget chain". Many existing work tackled the problem
of detecting gadgets chains in application and libraries [6, 10, 14,
15, 18] using techniques like debugger-assisted manual analysis,
static analysis, and hybrid (i.e. static and dynamic) analysis.

More closely related to our work are approaches aimed at de-
tecting and preventing deserialization attacks [5, 13]. Cristalli et
al. [5], present a two-phase approach that learns trusted execution
paths and sandboxes the native Java deserialization mechanism to
allow deserialization from those paths only. This approach does
not generalise to previously unseen paths and the precision thus
depends on the exhaustiveness of the training phase. To implement
their system, the authors modified a Java Virtual Machine (JVM),
and report overheads in the order of 20%-40%. Pan et al. [13], use
heavyweight instrumentation (e.g. 100x slowdown) to dynamically
collect execution traces and train deep learning models to detect
malicious deserialization at runtime. To achieve an F1-score > 0.90,
authors had to manually generate over 8 000 execution traces, which
is highly unpractical. In contrast, our approach uses a native JVM
and Java deserialization filters, and incurs a very small overhead.

8 FUTURE PLANS
The work presented in this paper is in the very early stages and
warrants several caveats. Our evaluation is currently limited to
one application (WLS), one technology (Java deserialization) and
one sampler (Metropolis-Hastings) only. Other applications, sam-
plers, and languages will have to be investigated. Despite these
limitations, however, we have uncovered several avenues for fur-
ther investigation. First, our results suggest that class features and
ordering capture the essence of a malicious gadget chain. While
attackers can obviously manipulate the stream to evade detection,
successful exploitation requires specific features and ordering. We
believe that this invariant could be key in achieving robustness in
the face of adversarial attacks. Second, while our approach seems
promising on small datasets, it remains unclear how well it scales
up and down and how well it applies in various user scenarios. An
empirical evaluation using different workloads, applications and
user scenarios is needed to assess the practicality of our approach.

01020304050l0.60.70.80.91.0PercentagePrecisionRecallRuntime Prevention of Deserialization Attacks

ICSE-NIERâ€™22, May 21â€“29, 2022, Pittsburgh, PA, USA

REFERENCES
[1] 2021. JEP 290: Filter Incoming Serialization Data. https://openjdk.java.net/jeps/

290.

[2] 2022. Top 10 Web Application Security Risks. https://owasp.org/www-project-

top-ten/.

[3] Moritz Bechler. 2017. Java Unmarshaller Security: Turning your data into code ex-
ecution. https://www.github.com/mbechler/marshalsec/blob/master/marshalsec.
pdf.

[4] Eric Bruneton, Romain Lenglet, and Thierry Coupaye. 2002. ASM: a code manip-
ulation tool to implement adaptable systems. Adaptable and extensible component
systems 30, 19 (2002).

[5] Stefano Cristalli, Edoardo Vignati, Danilo Bruschi, and Andrea Lanzi. 2018.
Trusted execution path for protecting java applications against deserialization
of untrusted data. In International Symposium on Research in Attacks, Intrusions,
and Defenses. Springer, 445â€“464.

[6] Johannes Dahse, Nikolai Krein, and Thorsten Holz. 2014. Code reuse attacks in
php: Automated pop chain generation. In Proceedings of the 2014 ACM SIGSAC
Conference on Computer and Communications Security. 42â€“53.
[7] Chris Frohoff. 2015. ysoserial. https://github.com/frohoff/ysoserial.
[8] Chris Frohoff. 2016. Deserialize My Shorts: Or How I Learned To Start Wor-
rying and Hate Java Object Deserialization. http://frohoff.github.io/owaspsd-
deserialize-my-shorts/.

[9] Brian Goetz. 2019. Towards Better Serialization. https://cr.openjdk.java.net/

~briangoetz/amber/serialization.html.

[10] Ian Haken. 2018. Automated Discovery of Deserialization Gadget Chains.
[11] Gabriel Lawrence and Chris Frohoff. 2015. Marshalling Pickles: How Deserializing
Objects Can Ruin Your Day. https://frohoff.github.io/appseccali-marshalling-
pickles.

[12] Alvaro MuÃ±oz and Oleksandr Mirosh. 2017.

Friday the 13th JSON At-
tacks. https://www.blackhat.com/docs/us-17/thursday/us-17-Munoz-Friday-
The-13th-JSON-Attacks-wp.pdf.

[13] Yao Pan, Fangzhou Sun, Zhongwei Teng, Jules White, Douglas C Schmidt, Jacob
Staples, and Lee Krause. 2019. Detecting web attacks with end-to-end deep
learning. Journal of Internet Services and Applications 10, 1 (2019), 1â€“22.
[14] Or Peles and Roee Hay. 2015. One class to rule them all: 0-day deserialization
vulnerabilities in android. In 9th {USENIX} Workshop on Offensive Technologies
({WOOT} 15).

[15] Shawn Rasheed and Jens Dietrich. 2020. A hybrid analysis to detect Java serialisa-
tion vulnerabilities. In Proceedings of the 35th IEEE/ACM International Conference
on Automated Software Engineering. 1209â€“1213.

[16] John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. 2016. Probabilistic
Programming in Python using PyMC3. PeerJ Computer Science 2 (2016), e55.
[17] Jacob Schreiber. 2017. Pomegranate: Fast and Flexible Probabilistic Modeling in
Python. The Journal of Machine Learning Research 18, 1 (2017), 5992â€“5997.
[18] Mikhail Shcherbakov and Musard Balliu. 2021. SerialDetector: Principled and
Practical Exploration of Object Injection Vulnerabilities for the Web. In Network
and Distributed Systems Security (NDSS) Symposium 202121-24 February 2021.

