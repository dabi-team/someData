XMD: An Expansive Hardware-telemetry based Malware Detector to enhance
Endpoint Detection
In Review.

2
2
0
2

n
u
J

4
2

]

R
C
.
s
c
[

1
v
7
4
4
2
1
.
6
0
2
2
:
v
i
X
r
a

Harshit Kumar, Biswadeep Chakraborty, Sudarshan Sharma, Nikhil Chawla, Saibal Mukhopadhyay
Dept. of Electrical and Computer Engineering
Georgia Institute of Technology, Atlanta, USA
{hkumar64, biswadeep, ssharma497, nikhilchawla}@gatech.edu, saibal.mukhopadhyay@ece.gatech.edu

Abstract

Hardware-based Malware Detectors (HMDs) have shown
promise in detecting malicious workloads. However, the cur-
rent HMDs focus solely on the CPU core of a System-on-
Chip (SoC) and, therefore, do not exploit the full potential
of the hardware telemetry. In this paper, we propose XMD,
an HMD that operates on an expansive set of telemetry chan-
nels extracted from the different subsystems of SoC. Key
innovations in XMD are guided by analytical theorems that
leverage the concept of manifold hypothesis. XMD exploits
the thread-level proﬁling power of the CPU-core telemetry,
and the global proﬁling power of non-core telemetry chan-
nels, to achieve signiﬁcantly better detection performance
and concept drift robustness than currently used Hardware
Performance Counter (HPC) based detectors. We train and
evaluate XMD using hardware telemetries collected from 904
benign applications and 1205 malware samples. XMD im-
proves over currently used HPC-based detectors by 32.91%
for the in-distribution test data and by 67.57% for the concept
drift test data. XMD achieves the best detection performance
of 86.54% with a false positive rate of 2.9%, compared to
the detection rate of 80%, offered by the best performing
software-based Anti-Virus(AV) on VirusTotal, on the same
set of malware samples.

1 Introduction

The previous decade has witnessed an explosive growth of
malicious applications, compromising the security of mod-
ern devices [19]. Consequently, Endpoint security has moved
towards behavior analysis that involves continuous monitor-
ing of sensors across the compute stack, and rigorous data
analysis [14, 73]. As shown in Figure 1, behavior analysis
techniques monitor the program’s execution using semanti-
cally rich information sources like registry keys, network end-
points, system calls, and operating system (OS) hooks. How-
ever, malicious actors can potentially subvert such protection
mechanisms by tampering with the software telemetry [6, 46].

Moreover, such software-level detection approaches result
in signiﬁcant performance overhead. Hence, there has been
a recent thrust in data-driven approaches for detecting mali-
cious workloads using low-level hardware telemetry, which
promises low overheads and better resilience against tamper-
ing compared to software-based telemetry [39].

The application of hardware-level telemetry, like HPC, en-
ergy telemetry channels (e.g., Intel’s RAPL), and Dynamic
Voltage and Frequency Scaling (DVFS), towards malware
detection, has recently gained interest [24, 32, 36, 39, 45, 47,
49, 53–55, 63, 72, 76]. Monitoring the hardware telemetry pro-
vides visibility into active threats, even during the presence
of anti-evasion techniques like obfuscation [47], or cloak-
ing in virtual machines [6]. As shown in Fig 1, HMD along
with software-level detection techniques, form part of the
commercially-deployed collaborative defense model, e.g.,
Endpoint Detection and Response (EDR), that detects and
contains threats [6, 10, 11, 17]. In such a collaborative system,
HMDs can provide an additional layer of detection capability
across the kill chains, complement other detectors, and re-
strict the ability of the adversary to move in the environment
without triggering detection.

Demme et al. demonstrated proof-of-concept of detecting
Android malware, Linux Rootkits, and side-channel attacks
using HPCs [39]. Thereafter, methodologies for increasing
the predictive performance [36, 45, 49, 53, 54], lowering the
overheads of the HMDs [63,72], and alternate CPU-telemetry
sources [32] have been proposed. However, prior works focus
solely on the hardware telemetry from the CPU of the System-
on-Chip (SoC), which captures the partial impact of running
workloads on an SoC, and does not exploit the full potential
of the extensive hardware telemetry. Moreover, these works
do not address the concept drift arising from the behavioral
evolution (adversarial or benign) of applications. The result-
ing change in test data distribution causes severe performance
degradation of the deployed ML models, posing a critical
challenge in deploying models for security applications.

Approach. In this work, we propose XMD, an HMD that
achieves signiﬁcant improvements in detection performance

1

 
 
 
 
 
 
Figure 1: Comprehensive cross-stack monitoring performed by modern Endpoint-Detection : [Left] Software-based malware
detection approaches, [Right] Proposed XMD operating on expansive set of telemetry channels.

and concept drift robustness compared to prior HPC-based
HMDs. To accomplish this, XMD exploits two key innova-
tions. First, to improve detection performance, XMD uses a
comprehensive set of telemetry channels that capture a work-
load’s impact on different SoC sub-systems like GPU, mem-
ory, buses, network, and CPU telemetry, the primary focus
of prior works. XMD achieves this through the use of DVFS
signatures from the non-core devices, and low-level telemetry
from select SYSFS nodes, in addition to the existing CPU
telemetry channels like CPU-DVFS and HPC. Second, XMD
exploits the non-determinism (stochasticity) present in DVFS
and SYSFS channels (collectively referred to as GLOBL chan-
nels in the rest of the paper) to achieve signiﬁcant concept
drift robustness compared to prior HPC-based HMDs.

The two key innovations are grounded in the theorems we
developed by leveraging the concept of manifold hypothesis
(Section 4). Prior works in deep learning have used manifold
hypothesis for studying the geometry of manifolds in Deep
Neural Networks (DNNs) for vision [35], audition [69], and
language modeling [25, 60]. Such analytical frameworks have
explained the generalization performance of DNNs [70]. In
this work, we use manifold hypothesis to connect the stochas-
ticity of the GLOBL channels to concept drift robustness, and
an increase in the solution volume of the fusion-classiﬁer [42]
to the superior classiﬁcation performance of XMD.

To evaluate XMD, we create a robust data-collection frame-
work, using a commodity mobile device (Table 5), that in-
corporates measures to reduce the dataset bias and prevent
over-optimistic results. Using this data-collection framework,
we create three different datasets, which help us empirically
validate our developed theorems. For example, we observe
that while the HPCs can proﬁle threads pretty accurately in a
biased dataset (F1 score 0.99-1.0), their performance degrades
severely under concept-drift scenarios (F1 score 0.46-0.56).
On the other hand, the stochasticity present in the DVFS and
SYSFS-based telemetries results in concept-drift robustness
at the cost of degradation in proﬁling accuracy. Finally, we
compare the detection performance of XMD with commercial
AV engines on VirusTotal. To summarize, our main contribu-
tions are as follows:

• We develop two analytical theorems, using manifold hy-

pothesis, which proves that the stochasticity in hardware
telemetry results in concept drift robustness and using
multiple telemetry channels increases the detection per-
formance of the HMD.

• We design a robust bare-metal data collection framework
to collect expansive hardware-based telemetry that cap-
tures a workload’s impact on different subsystems of the
SoC like CPU, GPU, Memory, Buses, and Network.

• Using three kinds of datasets, we perform empirical ex-
periments to validate the proposed theorems. Specif-
ically, we show that: (1) higher stochasticity in the
GLOBL channels reduces the performance degradation
when tested on a concept drift dataset compared to HPC-
based detectors, and (2) using multiple telemetry chan-
nels from the different subsystems of an SoC results
in better classiﬁcation performance when compared to
classiﬁers that operate on the telemetry from a single
subsystem, e.g., CPU-based telemetry.

• Finally, we develop a fusion-based classiﬁer, called
XMD, that combines the strength of the two classes
of telemetry to provide better performance than prior
HMDs. We benchmark XMD against commercial
software-based AV engines present on VirusTotal and
ﬁnd that XMD provides superior detection rates, with an
acceptable false-positive rate.

2 Background and Threat Model

2.1 Classes of Telemetry

Thread-level Proﬁling : Hardware Performance Coun-
ters (HPCs). HPCs are dedicated physical registers in mod-
ern processors that store the count of the number of microar-
chitectural events in a CPU core during process execution.
They were originally designed to identify performance bot-
tlenecks. To obtain counter information from the registers,
they are conﬁgured to monitor speciﬁc hardware events that
are of interest. Although hundreds of hardware events are
available for monitoring, a limited number of them can be

2

ENDPOINT DETECTION SyscallsHooksRegistryNetworkGPUMemoryMODEMCPUDriverML AgentHardware-based Malware Detection Software-basedFused DecisionHPCConcept Drift robustness& superior detection.Behavioral SignaturesThread-levelDVFSSYSFSGlobalIdentify malicious threads.monitored simultaneously due to the limited number of phys-
ical registers (e.g., 4 in the Snapdragon chipset considered
in this paper). HPCs provide the ability to perform thread-
level proﬁling by saving the register values during context
switches and, therefore, tries to avoid contamination due to
events from other processes [38]. With the adoption of ML
techniques in security, HPCs have recently been repurposed
as a low-level telemetry for identifying malicious workloads
in HMDs [24, 36, 39, 45, 47, 49, 54, 55, 63, 72, 76].

Global Proﬁling : Dynamic Voltage and Frequency
Scaling (DVFS). DVFS is an integral part of all power man-
agement systems. It reduces the power consumption of an
SoC by scaling down the voltage and frequency states of the
different sub-systems (e.g., CPU, GPU, buses, caches, and
memory) based on the targetted performance requirements
of the software workloads. As a result, the DVFS states of a
sub-system capture its activity level, providing insight into
the impact of running workload on that sub-system. Security
implications of the DVFS framework have been studied both
from an offensive perspective [52, 61, 71] and to create de-
fenses [31, 32]. Since Android OS (considered in this paper)
is based on the Linux kernel, the DVFS states of the CPU
and non-CPU devices are accessible through the cpufreq, and
devfreq framework [5, 7].

It should be noted that the DVFS channels (and other
SYSFS nodes used in this work) capture the global state of
the device as compared to HPCs that are used for monitoring
the speciﬁc threads/processes. In mobile devices, numerous
system-level threads and user applications are simultaneously
contending for hardware resources. This makes the DVFS
channels susceptible to noise arising from such background
processes. However, in the case of mobile devices, such as
the one considered in our work, the foreground applications
contribute towards deciding the DVFS states most of the
time [32]. The DVFS channels provide better visibility into
the impact of running a workload on the entire SoC at the
cost of additional non-determinism compared to HPCs that
capture a workload’s impact on the CPU core of the SoC.

2.2 Concept Drift (CD)

A central assumption of training a supervised ML algorithm
is that the training and the test data points are sampled inde-
pendently from the same distribution. However, in the case
of malware detectors deployed in the wild, the underlying
probability distribution of the test data points changes. Such
a change may arise from the evolution of tactics, techniques,
and procedures (TTPs) employed by the malware authors or
can result from the organic behavioral changes in benign ap-
plications. This phenomenon of the shift in the true decision
boundary is called Concept Drift [44, 53].

Current measures for mitigating the performance degrada-
tion arising from CD involve detecting such scenarios and
re-training the ML model. The re-trained model is deployed

to the client devices via security updates. However, if the un-
derlying telemetry for the ML model is sensitive to CD, then
the frequency of periodical re-training increases (expensive).
Therefore, CD robustness can reduce the costs associated with
maintaining an ML model deployed in a security-sensitive
application.

2.3 Threat Model

The threat model of XMD is the same as the HMDs that are
commercially deployed, i.e., we assume that the OS kernel
is not compromised. Since HMDs are a part of the EDR sys-
tem, a compromised kernel undermines the process tracking
capabilities of the EDR and thwarts the trustworthiness of
the different sources of telemetry on which the EDR relies
upon [46]. We note that hardware implementations of HMD
have been explored for providing detection capabilities under
threat models where the OS is compromised [62, 63, 72].

3 Related Works and Motivation

Detection of malicious workloads using hardware telemetry
has been extensively studied in the literature [24, 32, 36, 39,
45, 47, 49, 54, 55, 63, 72, 76]. Due to the variabilities in the
data collection methodology, we do not empirically compare
against prior works, but offer a qualitative comparison against
the surveyed works. As summarized in Table 1, we categorize
the drawbacks of these works as follows:

Restricted scope of hardware telemetry collection.
Prior works on HMD primarily focus on a single modality of
data extracted from the CPU of the SoC [24, 32, 36, 39, 45,
47, 49, 54, 55, 63, 72, 76]. These low-level signatures either
contain thread-level behavior (e.g., HPC) or global behavior
(e.g., CPU-DVFS). While these data-driven approaches have
shown decent test accuracy, they do not use the telemetry
sources available from the different sub-systems of the SoC.
They, therefore, do not realize the full potential of the HMD.
Evaluation on concept drift scenarios. Prior works in
HMDs have only been tested on in-distribution malware and
benign samples that are from the same period as that of the
training data [32,36,39,45,47,49,54,55,63,72,76]. Demme et
al. state in their work that "regardless of how malware writers
change their software, its semantics do not change signiﬁ-
cantly" [39]. A similar argument can be extended to concept
drift scenarios, wherein irrespective of the high-level changes
in TTPs employed by malware authors; the underlying ma-
licious objective remains the same, e.g., ﬁle encryption in
crypto-ransomware. Therefore, in principle, the feature space
on which HMDs operate should exhibit some robustness to
concept drift scenarios. However, none of the prior works
have evaluated their models on a concept drift dataset.

Bias in datasets. Creating a dataset for training and eval-
uating an HMD requires executing the malware and benign

3

Table 1: Comparison with prior works

Ref.

[54]
[72]
[45]
[39]
[63]
[47]
[76]
[32]
[36]
[55]
[24]
[49]

Proposed

Benign
type
Bench
Bench
Bench
Bench
Bench
Regular
Bench/Regular
Bench
Regular
Bench
Bench
Bench/Regular
Bench/Regular
(Diff datasets)

Verify
execution
×
×
×
×
×
(cid:88)
(cid:88)
×
×
×
×
×

(cid:88)

Telemetry
type
HPC
HPC
HPC
HPC
HPC
HPC
HPC
CPU-DVFS
HPC
HPC
HPC
OPCODE, HPC
Expansive DVFS,
SYSFS, HPC

Concept Drift
eval.
×
×
×
×
×
×
×
×
×
×
×
×

(cid:88)

Open-
source
×
×
×
(cid:88)
×
×
(cid:88)
(cid:88)
×
×
×
×

(cid:88)

Comparison against
SW-based AVs
×
×
×
×
×
×
×
×
×
×
×
×

(cid:88)

Table 2: List of collected HPC events

Group

group-1

group-2

group-3

group-4

HPC event
cpu-cycles
instructions
raw-bus-accesses
branch-instructions
branch-misses
raw-mem-access
cache-references
cache-misses
raw-crypto-spec
bus-cycles
raw-mem-access-rd
raw-mem-access-wr

Reference
[55]
[24, 36, 45, 47, 54, 63]
[39, 47]
[24, 36, 39, 45, 54, 55, 63]
[24, 36, 45, 55, 55, 72]
[39, 47, 49]
[24, 36, 76]
[24, 36, 45, 54, 72]
-
[55, 72]
[39, 47]
[39, 47]

applications in a sandbox and collecting the behavioral sig-
natures in the background. Incorrect assumptions while de-
signing the data collection framework can introduce bias in
the dataset. We have identiﬁed sources of bias in the data
collection frameworks of prior works like failure to verify the
execution of the proﬁled applications and the use of bench-
marks as benign applications (Appendix B.1). As we observe
in this paper, a non-trivial percentage of applications do not
have sufﬁcient runtime during automated telemetry collection
(Section 5.4), and benchmarks have the potential to introduce
bias in the dataset (Section 6.1.1).

Comparison against Software-based AVs. Prior works
present a qualitative comparison of how the behavior-based
detection approach of HMDs can outperform the static
analysis-based detection techniques of production AV soft-
ware [24, 32, 36, 39, 45, 47, 49, 54, 55, 63, 72, 76]. However,
they do not present a quantitative comparison of how the
performance of their proposed HMDs compares against the
currently deployed production AV software.

Lack of a uniﬁed open-source dataset. Despite the
plethora of works on HMDs, a handful of publically avail-
able datasets for HMDs restrict the research into developing
and testing novel ML agents. Datasets that have been shared
are lacking in aspects that have been summarized in Table
1 [32, 39, 76]. Due to the huge variability observed in the
behaviors of malware and benign workloads arising from lim-
itations of dynamic analysis studies [28], there is a need for a
standardized dataset.

determinism in the hardware telemetry [38, 76]. However,
adding synthetic noise to the input of an ML model, forcing
the model to learn a more general solution, is a well-known
concept in the ML community [67]. Hence, we make the
following hypotheses: (1.1) the stochasticity present in the
hardware telemetry acts as a regularizer and results in bet-
ter generalization, (1.2) a direct consequence of learning a
general solution will be increased concept drift robustness.

Hypothesis 2 : A better classiﬁcation performance can
be achieved with an expansive set of hardware telemetry
channels. As pointed out by Zhou et al., it is unclear why
the low-level telemetry extracted solely from the CPU of
the SoC can help distinguish high-level behavior between
benign and malicious applications [76]. For example, the
encryption instructions used by a workload can be employed
by ransomware for encrypting user ﬁles or by benignware like
7zip. Using hardware telemetry from solely the CPU does not
provide the complete picture to the ML agents operating on
the telemetry. On the other hand, if the ML agent has access to
the low-level network telemetry, it can potentially identify the
encryption performed by the ransomware since the malware
will additionally communicate with its C2 server. This leads
us to our hypothesis that an ML classiﬁer will perform better
when operating on an expansive set of telemetry channels
extracted from the different sub-systems of an SoC. Next, we
present the necessary background and deﬁnitions based on
which we construct the theorems that back our hypotheses.

4 Theory of XMD

We describe the main hypotheses behind the design of XMD,
followed by the theorems that support our hypotheses.

4.1

Intuition behind XMD

Hypothesis 1: The non-determinism in hardware teleme-
try results in increased concept drift robustness. Recent
works have identiﬁed failure scenarios arising from the non-

4.2 Background and Deﬁnitions

Manifold. Intuitively, a manifold is a topological space that
is locally Euclidean. A topological space is a set of points,
with each point having its own set of neighborhoods [59]. We
have a corresponding set of features for each of the GLOBL
channels and the HPC groups. Using these feature sets, we
can visualize an APK sample as a representative point in a
higher dimensional vector space. We construct a manifold
composed of a set of these representative points for each of
the two classes, i.e., benign and malware. Finally, we end up

4

stochastic process) is the solution to Equation 1. The incre-
ments of B are independent and normally distributed. Hence,
in a small time interval δ, the change in the stochastic process
∆Xt ∼ N (µ (Xt ,t) δ, σ (Xt ,t)2 δ). The drift coefﬁcient (µ) de-
termines the expectation E(Xt ) and the diffusion coefﬁcient
(σ) determines the Variance E((Xt − E(Xt ))2).

4.3 Theorems

Using the formal theory of linear separability of the object
manifolds [33], we perform an analytical study that supports
our hypotheses presented in Section 4.1. Collectively, the
theorems establish that XMD’s superior performance stems
from a higher solution volume. We present an intuitive take on
the theorems in this section and refer the reader to Appendix
A for a mathematically rigorous proof.
Lemma 1: The solution volume for a model trained with
stochastic inputs ( ˜V ) is greater than the solution volume for
a model trained with deterministic inputs (V ), i.e., E[ ˜V ] ≥ V .
Speciﬁcally, we model the stochasticity in the telemetry
channel by adding Gaussian Noise in the solution volume,
i.e., we add a diffusion term in the SDE, which, when solved
using an Ito Integral, gives us the solution volume which
is a stochastic random variable. Finally, we show that the
solution volume calculated by taking the expectation over the
stochastic process is lower bounded by the solution volume
without stochasticity, resulting in better generalizability for
the former. This Lemma supports Hypothesis 1.1.

Theorem 1: Stochasticity in the input training data improves
concept drift robustness of the model.

To prove this, we model concept drift as a change in the
drift term of the stochastic random variable representing the
input feature space. Next, we show that with concept drift,
the volume of the input space increases, which results in a
decrease in the corresponding solution volume. Since the
solution volume in the case of stochasticity is more than
without stochasticity (Lemma 1), this leads to better concept
drift robustness. This theorem supports Hypothesis 1.2.

Theorem 2: Let Vi be the solution volume corresponding to
the classiﬁcation task of the benign and malware applications
using the i-th telemetry channel in the N-dimensional vector
space, where the i-th basis corresponds to the i-th telemetry
channel ∀i ∈ [1, N], and N is the total number of telemetry
channels. We show that the solution volume arising from
the union of different Vis, i.e. ((cid:83)
i Vi), is greater than the
individual Vis considered independently.

To prove this, we assume an N-dimensional vector space,
with one basis for each of the N telemetry channels, and each
Vi is an orthogonal projection of the union of solution vol-
umes ((cid:83)
i Vi) on the i-th basis. Next, we show that the solution
volume arising from (cid:83)
i Vi is lower bounded by the maximum
solution volume Vmax, where Vmax is max{Vi ∀i ∈ [1, N]} .

Figure 2: 2-D visualization of Benign [green] vs. Malware
[red] classiﬁcation using manifolds: (a) candidate hyperplanes
that can separate the benign and malware manifolds, (b) clas-
siﬁcation task with overlapping manifolds, (c) solution space
of the hyperplanes for case-a (easier classiﬁcation problem)
and for case-b (difﬁcult classiﬁcation problem).

with a set of manifolds for each of the benign and malware
classes, for all the GLOBL channels and the HPC groups.
The representative points move around in their respective
manifolds introducing intra-class variabilities (e.g., stochas-
ticity). We use the notation F µ with µ = B or M for the benign
and malware manifolds, respectively, with each point on the
manifold represented by xµ ∈ F µ.

Separability and Hyperplanes. The goal of the classi-
ﬁcation task is to learn the position of the decision hyper-
plane between the two object manifolds. The ability of a
classiﬁer to discriminate between two class manifolds can
be mapped to the separability of class manifolds by a linear
hyperplane [33, 40]. We study the separability of the benign
and malicious manifolds into separate classes, denoted by
binary labels yµ = ±1, by a linear hyperplane. As shown in
Figure 2.(a), a hyperplane is described by a weight vector
w ∈ RN, and separates the manifolds with margin κ such that
yµw · xµ ≥ κ for all µ and xµ ∈ F µ.

Solution Volume. We use solution volume V as a met-
ric to characterize the separability of two manifolds [42]. A
higher solution volume results in better and more generaliz-
able classiﬁcation. An intuitive representation of the solution
volume is shown in Figure 2.(c). We can observe a higher
solution volume for case-a, where the benign and malware
manifolds have higher separability, than case-b, where the
manifolds overlap, resulting in lower separability and, there-
fore, a lower solution volume.

Stochastic Differential Equation (SDE). A SDE is a dif-
ferential equation that contains terms which are stochastic
processes, and is typically of the form

dXt = µ (Xt ,t)
(cid:124) (cid:123)(cid:122) (cid:125)
Drift Coefﬁcient

dt +

σ (Xt ,t)
(cid:124) (cid:123)(cid:122) (cid:125)
Diffusion Coeffecient

dBt

(1)

where B denotes the standard Brownian motion and Xt (a

5

awκw1w2abcLow Solution VolumeSolution SpacebA higher solution volume results in better classiﬁcation per-
formance, hence, supports Hypothesis 2.

4.4 Approach for Experimental Validation

To validate the hypotheses and theorems presented in Sec-
tion 4.1 and 4.3 respectively, we design a robust data col-
lection framework that collects the two classes of telemetry
(HPC and GLOBL) simultaneously, and collectively captures
a workload’s impact on different sub-systems of the SoC like
CPU, GPU, memory, buses, and network (Section 5). The
framework incorporates measures to reduce the datasets’ bias
and prevent over-optimistic results. For example, we devise
a Logcat-based activation checker that ﬁlters out the runs in
which the application doesn’t have sufﬁcient runtime. For
both the HPC data logs and the GLOBL data logs, the data
collection process produces multi-variate time series data.
The feature engineering choices are the same as the ones used
in prior works for DVFS [31, 32] and HPC [39, 76], and are
summarized in Figure 3.(a) and 3.(b). For completeness, we
brieﬂy elaborate on these techniques in Appendix C.2.

Validating the Hypotheses and Theorems. We collect
three different kinds of datasets using which we establish
the concept drift robustness arising from the stochasticity
in the hardware telemetry, i.e., GLOBL channels, validating
Theorem 1 (Section 6.1 and 6.2). Next, using the fusion-
based approach summarized in Figure 3.(c), we demonstrate
the superior classiﬁcation performance of the fusion-based
model used in XMD that incorporates the expansive telemetry,
validating Theorem 2 (Section 6.3).

5 Dataset Collection Framework

5.1 Bare-metal analysis environment

While a Virtual Machine (VM) based sandbox provides bene-
ﬁts like isolation guarantees and checkpointing schemes, prior
work has shown that using a VM-based sandbox introduces
errors into the HPC measurements [76]. Further, malicious
applications do not execute their payload upon detection of
VM, aggravating the problem [27, 30, 50, 51]. To perform an
automated large-scale collection of hardware-level telemetry,
we built a host-client-based bare-metal sandbox environment
as shown in Figure 6.

The client is a Google Pixel 3 mobile device (running An-
droid OS) on which the benign and malicious samples are
executed (Table 5), and the host is a Linux OS-based PC that
orchestrates the data collection. We leveraged Android Debug
Bridge (adb) [1] for transferring data and sending commands
between the host and the client. The network packets from
the client were routed through the host using Gnirehtet [9],
providing unﬁltered Internet access to the Client, which is cru-
cial for the malware to perform essential functionalities like
communicating with their command-and-control (C2) server.

Since we were using a bare-metal analysis environment, we
developed a custom checkpointing scheme to prevent contam-
ination. Details are elaborated in Appendix B.2.

Environment assumptions. During data collection, mul-
tiple system workloads were executing in the background
and were sending and receiving data packets on the network.
Since the experimental Android device has a multi-core CPU,
the processes belonging to the foreground application were
context switching in and out of the cores and migrating to
different cores throughout data collection. Overall, we ensure
that the data collection environment is similar to the environ-
ments observed in real-world scenarios. Therefore, we believe
this work’s observations will apply to a signiﬁcant portion of
mobile devices.

5.2 Selection of Hardware Signals

Choice of HPC channels. The HPC events are stated in
Table 2. We performed a comprehensive literature survey
to identify the performance counter events used in prior
work that resulted in the best detection performance. We con-
sider an additional HPC event, not used in prior work, called
|raw-crypto-spec|,which tracks the number of crypto-
graphic operations. Cryptographic operations are used when
performing SSL or TLS handshake and for deobfuscating
malware payloads [15], and can potentially capture essential
malware functionalities (see Appendix C.4). The collection
of HPC events is limited by the number of HPC available
on the device. On the Snapdragon chipset, there are four
available HPCs. However, one HPC is repurposed for moni-
toring memory latency, so we can collect at most three events
simultaneously. Therefore, all the HPC events are divided
into groups 1-4. Each HPC group has three events that are
collected simultaneously in a single iteration.

Choice of DVFS channels. Prior works have primarily
focused on the impact of running a workload on the CPU by
monitoring the DVFS states of the CPU controller [31, 32];
we expand on this notion by considering an expansive set of
DVFS channels that cover both the CPU and the non-CPU
devices like GPU, memory, buses, and caches. Channel-1
to Channel-11 in Table 3 elaborates on the selected DVFS
channels, their corresponding locations in the Linux device
tree, and the corresponding device’s signature they capture.

Choice of SYSFS channels. To capture the low-level im-
pact of benign or malicious workload on Network devices,
we recorded the number of bytes transmitted and received by
the device. These channels are Channel-12 and Channel-13
in the Table 3. Since switching transistors consume energy,
there is a correlation between power consumption and the
type of workload. Prior works have demonstrated the efﬁ-
cacy of power side channels to detect malicious workloads
on x86, IoT, and embedded systems [29, 34, 41, 43]. We col-
lected telemetry from the |voltage_now|(Channel-14) and
current_now (Channel-15) sysfs nodes. While the channels

6

Figure 3: Creating the classiﬁers: (a) Feature engineering steps for GLOBL channels, (b) feature engineering steps for the HPC
groups, (c) Late stage fusion for merging the decisions of the base detectors

Table 3: List of collected GLOBL (DVFS & SYSFS) channels.

Channel Number
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Location

Description

GPU controller

/sys/class/devfreq/5000000.qcom,kgsl-3d0/gpu_load
/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq CPU controller : lower cluster
/sys/devices/system/cpu/cpu7/cpufreq/scaling_cur_freq CPU controller : higher cluster
CPU bus bandwidth controller
/sys/class/devfreq/soc:qcom,cpubw/cur_freq
GPU bus bandwidth controller
/sys/class/devfreq/soc:qcom,gpubw/cur_freq
GPU bus bandwidth controller
/sys/class/devfreq/soc:qcom,kgsl-busmon/cur-freq
Latency controller L3 cache : lower cluster
/sys/class/devfreq/soc:qcom,l3-cpu0/cur-freq
Latency controller L3 cache : higher cluster
/sys/class/devfreq/soc:qcom,l3-cpu4/cur-freq
Last level cache controller
/sys/class/devfreq/soc:qcom,llccbw/cur-freq
Memory latency controller : lower cluster
/sys/class/devfreq/soc:qcom,memlat-cpu0/cur-freq
Memory latency controller : higher cluster
/sys/class/devfreq/soc:qcom,memlat-cpu4/cur-freq
Network : received bytes
/sys/class/net/tun0/statistics/rx_bytes
Network : transmitted bytes
/sys/class/net/tun0/statistics/tx_bytes
Device current
/sys/class/power_supply/battery/current_now
Device voltage
/sys/class/power_supply/battery/voltage_now

D
V
F
S

SYS
FS

and events described earlier capture the impact of workload on
a speciﬁc subsystem of the SoC, the power channels present
a global telemetry channel capturing the impact on the en-
tire SoC. Overall, the combined information from the HPC,
DVFS, and SYSFS channels present a comprehensive low-
level behavior over all the sub-modules of an SoC like the
CPU, caches, GPU, memory, buses, and network.

5.3 Malware and Benign programs used

In this paper, we use a broad deﬁnition of malware, i.e., any
application that has been ﬂagged as malicious by at least one
AV on VirusTotal [56]. While querying the VirusTotal service,
we observed inconsistencies within the labels of the malware
families, e.g., multiple samples were labeled "a variant of
Android/Packed.Jiagu.<some_variant> potentially unsafe,"
wherein Jiagu is a packer and not a family name. Such la-
beling inconsistencies have been identiﬁed in literature [65],
and prevent us from performing a malware family classiﬁca-
tion study. Therefore, in this work, we focus on benign vs.
malware study where XMD is used as an early-stage detector.

STD-DATASET: The dataset consists of real-world An-
droid benign and malware applications. In particular, we ac-
quired an initial dataset of 1033 samples of malicious and 723
samples of benign Android applications. The samples were
downloaded from AndroZoo and were collected in the period
from Dec 2019 to June 2021 [26]. There are a total of 54
malware families in the malware dataset, as reported by the
ESET-NOD32 AV engine from VirusTotal. The number of
applications and the total number of iterations is summarized
in Table 4 under STD-DATASET.

BENCH-DATASET: To establish the role of non-
determinism in the performance of GLOBL and HPC teleme-
try, we use 21 android benchmark applications [58]. The
benchmark applications comprise CPU-bound benchmarks
like ﬂoating-point operations and memory-bound benchmarks
that test cache and memory. We use the same malware logs
used in the STD-DATASET and ensure that we have a balanced
dataset. As shown in Section 6.1.1, the benchmarks introduce
a bias in the dataset.

7

timechannelstimefrequencyPCA PCA Windowed FFTShape num_channels x feature_sizeΣΣΣΣ(1) Divide timeseries into histogram bins(2) Summation over the histogram bins to get the featureShape1 x feature_sizeFlattentimechannelsBase detector 1 Base detector 2 Base detector N chn-1chn-2chn-NDecision Function Final Decision c   Late-stage fusionb   HPC featuresa   GLOBL features(1) Ensemble(2) LR*channels*LR : Logistic RegressionTable 4: Taxonomy of the dataset

Dataset

Application
type

STD

CD

Benign
Malware
Benign
Malware

# apk
executed once
(% activation)
681 (94%)
776 (75%)
158 (87%)
159 (92%)

# apk
post logcat ﬁlter
(% activation)
448 (62%)
555 (54%)
82 (45%)
104 (60%)

# apk

723
1033
181
172

GLOBL

2120
2143
254
250

# Files

HPC

group-1
408
577
57
75

group-2
602
637
72
78

group-3
582
578
70
59

group-4
582
499
67
53

Table 5: Analysis environment

Google Pixel 3
Android 9.0
Qcom Snapdragon 845
8

Model
OS
Chipset
# Cores
Internet Connected, no-restrictions
Cellular

Not connected

Figure 4: DS for BENCH-DATASET
and STD-DATASET (Metric for
Bias).

Figure 5: Runtime for different iterations of mal-
ware and benign applications (using Logcat)

Figure 6: Overview of the data-collection framework. (1) Host-Client based malware analysis sandbox, (2) start of data collection
process, (3) interaction module and data-collection module running concurrently, (4) system partition integrity checking, (5)
Logcat-based ﬁlter for verfying execution.

5.4 Data Collection Architecture

seconds for each iteration.

The data collection architecture consists of three main com-
ponents: the interaction module, the data-collection module,
and the orchestrator module. The interaction module interacts
with the application while it executes (see Appendix B.3),
the data-collection module collects the data logs in the back-
ground while the application is executing in the foreground
(see Appendix B.4), and the orchestrator is responsible for
synchronizing sub-tasks (Appendix B.5). Figure 6 summa-
rizes the overall ﬂow of proﬁling and collecting the hardware
telemetry logs for a single iteration of data collection.

Method for Running Experiments. For each Android ap-
plication, we perform eight independent iterations of data
collection, where a different sequence of interactions was
used for each iteration. For each iteration of data collection,
we collected all the GLOBL channels, and one group of HPC
events since we are limited by the number of HPC registers.
Overall, each Android application has eight iterations of data
logs for the GLOBL channels and two iterations of data logs
for each HPC group. Considering our limited resources (time
and hardware), we ran data collection for a duration of 40

Ensuring correct execution using Logcat : We designed
a Logcat-based activation checker to ensure that the malware
is executing in the foreground while the hardware-telemetry
logs are collected in the background. In Android OS, logs
from applications are collected in a series of circular buffers,
which can be ﬁltered and viewed using Logcat [2, 12, 22].
We used logcat from the adb shell to ﬁlter the logs using the
process ID (PID) of the foreground application executing on
the device. As shown in Figure 6.(5), the logs contain the
timestamp of the activity, the PID, and the description of the
activity. For every iteration of data collection, we collected
its corresponding logcat and used it to verify the execution of
the foreground application.

Figure 5 shows the distribution of the runtimes of different
iterations of the benign and malware applications, calculated
using the Logcat logs. We can observe a wide variation in
the runtimes of the foreground applications, therefore, for this
work, the logcat-based ﬁlter rejected all the iterations in which
the foreground application executed for less than 15 seconds.
Table 4 summarizes the number of ﬁles post-ﬁltering for the
GLOBL channels and each HPC group. E.g., in the STD-

8

BENCH-DATASETSTD-DATASETDataset type0.0000.0250.0500.0750.1000.1250.1500.175Dissimilarity Score010203040Runtime (in s)050100150200250300350Number of iterationsapk typemalwarebenignStart GnirehtetExtract partition checksum (Pre)• Performance Counter• DVFS• Voltage & Current• rx_bytes & tx_bytes• LogcatExtract partition checksum (Post)Pre == Post ?• Monkey • Broadcast TransferTransferHostClientSandbox SetupHostClientSandbox SetupFlash data partitionFlash ALL partitionsYesNotime-stampPIDActivityActivity CheckerPush data to cloud23451InteractionData CollectionDATASET we observe that 75% of malware executed in at
least one of the eight iterations, and 54% of malware cleared
the logcat ﬁlter. On the other hand, benign applications have
a higher activation rate, with 95% of applications executing at
least once and 62% of applications clearing the logcat ﬁlter.
Sampling overheads. Sampling overhead was experi-
mentally determined by running Android benchmark appli-
cations [58] with and without the sampling scripts running
in the background. In the case of sampling HPC using sim-
pleperf, we observed <2% slowdown in execution times with a
sampling interval of 100 ms. Such a slowdown is observed be-
cause simpleperf interrupts the program’s execution to collect
the HPC values. This overhead is similar to those observed in
prior works (around 0.3-5%) [39, 47].

In the case of sampling the GLOBL states, we did not
observe any increase in the execution time of the benchmark
applications. This is because sampling the GLOBL states does
not interrupt the foreground application and runs in a separate
process. Since the Snapdragon chipset has eight cores, and
none of the benchmarks used all the eight cores, there was no
observed overhead. However, the sampling process is CPU-
bound, which utilizes 90-100% of CPU time when running.

6 Analysis

In this section, we perform empirical analysis aimed at vali-
dating the theorems.

Dataset split. We follow similar splitting process for both
the STD-DATASET and the BENCH-DATASET. We used the en-
tire CD-DATASET as a testing dataset to evaluate the model’s
performance that was trained on the STD-DATASET. The
dataset splits are generated in the context of the following
tasks: (SPLT-case-1) Training and Testing the individual clas-
siﬁers for each GLOBL channel; (SPLT-case-2) Training and
Testing the individual classiﬁers for each of the different HPC
groups; (SPLT-case-3) Training and Testing the Stacked Gen-
eralization models for the late-stage fusion of decisions from
the GLOBL and HPC-based base-classiﬁers (Details in Ap-
pendix C.1).

6.1 Characterizing telemetry channels.

In the following, we ﬁrst show that BENCH-DATASET has
high bias, which is further used to establish the high stochas-
ticity in the GLOBL channels (Takeaway-1). Then, using
STD-DATASET, we demonstrate that the solution volumes of
the different telemetry channels are distinct (Takeaway-2).
We use these as building blocks to validate our theorems.

6.1.1 Quantifying the bias in BENCH-DATASET

Welch’s t-test is a statistical measure to quantify the simi-
larity between two populations (e.g., benign and malware)
using their average statistics. It is a type of hypothesis-testing
where the null hypothesis is accepted or rejected based on the
calculated t-statistic and its corresponding p-value. The null
hypothesis for this study is: "hardware telemetry signatures of
malware and benign applications have similar means." The
t-statistics are calculated using Equation 2.

t-statistic =

µ1 − µ2
(cid:113) s1
2
2
+ s2
N2
N1

(2)

where µ1 and µ2 are the mean of the sample under observation,
s1 and s2 are the variance with N1 and N2 being the total
number of samples. When the calculated t − statistic > |4.5|,
we can reject the null-hypothesis with p-value of 1x10−5 and
conﬁdence score of 99.999% [68].

We perform pairwise t-test analysis on post-processed fea-
tures. We use the t-statistics to estimate a dissimilarity score
(DS) given by Equation 3.

DS =

card({ fi : t-statistic fi > |4.5|})
|F|

(3)

where card(.) denotes the cardinality of the set, fi is the ith
component of the feature vector F, and |.| is dimension of
the corresponding vector space. Intuitively, DS calculates
the fraction of features in the feature vector where the null
hypothesis is rejected, i.e., the telemetry of the malware and
benign applications are distinguishable. A higher DS implies
more distinguishability of malware and benign samples.

We calculated the average DS for the samples present in
the BENCH-DATASET and STD-DATASET. Figure 4 shows the
range of DS calculated for the samples in the dataset using
the different GLOBL telemetry channels. We can observe
that the range of DS for the BENCH-DATASET is higher than
STD-DATASET, signifying the increased distinguishability and
bias arising from the use of benchmarks, considering both
datasets use the same set of malware samples.

6.1.2 Stochasticity in HPC vs. GLOBL channels

Prior works on HMD have reported their best results using
Random Forest (RF) classiﬁers, hence we only consider the
RF classiﬁers for creating base-classiﬁers for each of the
GLOBL channels and the HPC groups. In the following, we
demonstrate the role of stochasticity in the proﬁling power
of the GLOBL and HPC channels, and its corresponding
impact on the classiﬁcation performance for the two differ-
ent datasets: BENCH-DATASET and STD-DATASET. Figure 7
presents the classiﬁcation performance of the different HPC-
based classiﬁers and the GLOBL-based classiﬁers on the two
datasets. We provide a brief discussion highlighting the key
results and takeaways from these experiments.

We study the bias introduced by benchmark applications us-
ing Dissimilarity Scores (DS) derived from the Welch t-test.

Performance on the BENCH-DATASET. We observe high
F1-scores across the GLOBL channels and the different HPC

9

Figure 7: F1-score for HPC-groups and GLOBL channels : (1) BENCH-DATASET, (2) STD-DATASET, and (3) CD-DATASET

groups. The workload behavior of benchmarks is signiﬁcantly
different from the malicious workloads of the dataset, intro-
ducing a strong bias in the dataset. The bias is supported by
the F1-score of the different HPC groups, which can perfectly
classify the benchmark applications from the malicious appli-
cations. HPCs only proﬁle the benchmark applications, and do
not have background noise from the other processes impacting
their signatures. The GLOBL channels, on the other hand, are
susceptible to the background noise arising from the system
workloads running in the background. Therefore, their F1-
scores are not as high as the different HPC-groups.

load. Despite the higher F1 score, relying solely on low-level
telemetry from one subsystem (e.g., network) creates a single
mode of failure that can be easy to bypass [38]. We can ob-
serve that the different channels have different failure modes
when identifying malicious workloads (see Appendix C.3).

There is a wide variation in the F1 scores (0.63-0.89) of ML
models using the same learning algorithm for the different
telemetry channels. This indicates that the solution volume V
of different telemetry channels are distinct, potentially arising
due to the different sources of information captured by the
telemetry channels.

Takeaway-1

Takeaway-2

Thread-level proﬁling performed by HPCs can classify
benchmarks vs. malware more accurately as compared to
the GLOBL channels whose performance is susceptible to
stochasticity arising from background workloads.

Performance on the STD-DATASET. Using real-world be-
nign applications results in a more realistic scenario than us-
ing benchmark applications, making the classiﬁcation task dif-
ﬁcult. The F1 scores for the HPC groups and CPU-telemetry
channels (GLOBL channel-2 and 3) are lower than what has
been reported in prior works that have used benchmark appli-
cations labeled as benign. Compared to the BENCH-DATASET,
the HPC-groups do not have the performance advantage over
the GLOBL channels, signifying that CPU-telemetry is not
enough to separate malicious applications from real-world
benign applications, despite the accurate proﬁling power re-
sulting from the thread-level proﬁling.

The GLOBL channels-12 and 13 that capture the number
of transmitted and received bytes offer signiﬁcantly higher
F1-score compared to the other channels and HPC groups,
demonstrating the role of communication with the C2 server
in differentiating a malicious workload from a benign work-

Telemetry channels from different subsystems in an SoC
have different classiﬁcation scores, i.e., the solution volume
V of different telemetry channels are distinct.

6.2 Theorem 1: Stochasticity vs. CD?

CD-DATASET: To assess the performance of the HMDs
against CD scenarios, we create a CD test dataset in which
the samples were collected from July 2021 to Dec 2021, non-
overlapping from the time interval in which the STD-DATASET
was collected. This dataset is used as a test dataset to test the
performance of the models trained on the STD-DATASET. The
number of applications and the total number of iterations is
summarized in Table 4 under CD-DATASET.

Results. As shown in Figure 7, the GLOBL and HPC-
based base-classiﬁers suffered degradation in accuracy when
evaluated on the CD-DATASET. The average degradation for
the GLOBL channel-based base-classiﬁers was 5.6%, while
it was 24.8% for the HPC-groups-based classiﬁers. While the
degradation in accuracy is insigniﬁcant for certain GLOBL
channels, it is high for the different HPC groups. The non-
determinism in the GLOBL channels (Takeaway-1) acts as a

10

HPC-group-1HPC-group-2HPC-group-3HPC-group-4channel-1channel-2channel-3channel-4channel-5channel-6channel-7channel-8channel-9channel-10channel-11channel-12channel-13channel-14channel-15HPC-groups and GLOBL-channels0.00.20.40.60.81.0F1-score10.99110.870.90.90.860.940.690.850.930.860.840.840.980.930.710.710.670.710.760.670.640.730.630.790.70.650.70.740.720.80.790.860.890.640.650.550.560.540.460.540.660.620.760.60.570.660.690.760.790.730.830.810.640.63BENCH-DATASETSTD-DATASETCD-DATASETregularizer and forces the GLOBL channel-based classiﬁers
to have better generalization, evident by their comparatively
lesser drop in accuracy compared to the HPC channels.

Takeaway-3

The stochasticity in the GLOBL channels results in better
concept drift robustness as compared to the HPC groups,
and therefore, validates Theorem-1 presented in Section 4.

telemetry channels, indicating the increased distinguishability
of malware and benign samples. An increased distinguisha-
bility potentially indicates that the solution volume arising
from ((cid:83)
i Vi) is higher than the solution volumes of the teleme-
try channels (Vi) when considered independently. Next, we
explore the use of late-stage fusion-based detection to real-
ize the potential performance improvements of incorporating
multiple channels.

6.3 Expansive telemetry vs. performance

Figure 8: DS score with respect to number of channels

In the previous section, we observed that each class of
telemetry has its strengths and weaknesses. HPCs can accu-
rately proﬁle the CPU and identify malicious threads but offer
poor generalization and concept drift robustness. On the other
hand, the GLOBL channels capture a global impact of run-
ning a workload on the SoC and offer concept drift robustness
but cannot identify the malicious threads. This motivates us to
develop a fusion-based approach, called XMD, that comple-
ments the thread-level proﬁling provided by the HPCs with
the global proﬁling provided by the GLOBL channels.

6.3.1 Validating Theorem-2: Is fusion-based model the

right approach?

Theorem-2 in Section 4 guides the design of a fusion-based
model where using multiple telemetry channels results in
better solution volume than considering a single telemetry
source, e.g., the CPU-telemetry-based classiﬁers. We evaluate
the impact of incorporating more telemetry channels on the
distinguishability of the benign and malware applications
using DS derived from the t-test presented in Section 6.1.1.

For each pair of benign and malware applications, we incor-
porate more telemetry channels into the feature vector. This
is followed by Principal Component Analysis to reduce the
augmented feature size to the feature size of a single chan-
nel, eliminating the bias arising from increased feature size.
We then calculate the DS. As shown in Figure 8, we see an
increasing trend in the DS upon increasing the number of

11

6.3.2 Approach: Late-stage fusion

As shown in Figure 3.(c), we consider two decision fusion
approaches. In the ﬁrst approach, called ENSEMBLE, we take
a majority vote of decisions of the individual base-classiﬁers,
each of them trained on a different telemetry channel. The
second approach is a stacked generalization approach where
we fuse the decisions of the base-classiﬁers using a second-
stage model, which is a logistic regression model in our case.
Logistic Regression was selected as the second-stage model
as the learned parameters of the model will provide insight
into the importance of each of the GLOBL channels and HPC
groups when arriving at the ﬁnal decision (see Appendix C.4).
Fusing the decisions of the GLOBL channels. Table
6 shows the F1 scores from fusion of the different base-
classiﬁers of the GLOBL and DVFS channels. We can
observe the following trends from these results: First, for
each dataset, the F1 score obtained after fusing the GLOBL
channels has a better F1 score compared to the sub-group
of DVFS. Therefore, considering the impact of running
a workload on all the sub-devices of the SoC is essential.
Second, F1-score from fusion is greater than the F1-scores
from each of the individual channels which agrees with the
statistical analysis in Section 6.3.1 and validates Theorem-2
in Section 4, wherein fusion of distinct solution volumes of
the individual telemetry channels (Takeaway-2) results in
a higher solution volume. An interesting side-effect of an
increased solution volume is the F1 score of 0.98 achieved
by the fused model of DVFS channels. Fusing the decisions
of different telemetry channels offsets the degradation in
proﬁling accuracy arising from the stochasticity. Moreover,
with an F1-score of 0.84 on the CD-DATASET, the enhanced
concept drift robustness arising from the stochasticity is
preserved.

Takeaway-4

The performance of the fused model incorporating all the
GLOBL telemetry channels is greater than the performance
of each channel independently, validating Theorem-2.

XMD: Fusing the decisions of the GLOBL channels
and HPC. Table 7 shows the F1 score of the individual
HPC groups when they were standalone and when used in
conjunction with the DVFS and the GLOBL channels for

1234567891011121314Number of Channels0.000.050.100.150.200.25Dissimilarity ScoreTable 7: F1-scores of different HPC-groups before and after fusion with
the DVFS and GLOBL channels.

Table 6: F1-score of the fused GLOBL channels

HPC

Dataset

Standalone

Fused channel
group
DVFS
GLOBL

Participating
Channels
1-11
1-15

BENCH
DATASET
0.98
0.99

STD
DATASET
0.90
0.92

CD
DATASET
0.82
0.84

group-1

group-2

group-3

group-4

STD
CD
STD
CD
STD
CD
STD
CD

0.66
0.55
0.71
0.56
0.76
0.54
0.67
0.46

With DVFS
(Ensemble)
0.89
0.86
0.89
0.83
0.90
0.86
0.91
0.83

With DVFS
(SG)
0.88
0.85
0.84
0.81
0.91
0.81
0.90
0.81

With GLOBL
(Ensemble)
0.93
0.87
0.89
0.85
0.93
0.87
0.92
0.87

With GLOBL
(SG)
0.90
0.83
0.92
0.87
0.93
0.81
0.93
0.90

Table 8: Detection rates of XMD and Antivirus Scanners on VirusTotal

ensemble
group-1
group-2
group-3
group-4

XMD
84.13 (FPR = 3.70)
87.83 (FPR = 6.89)
86.54 (FPR = 2.90)
85.71 (FPR = 5.23)

AV1
83.87
80.22
80.00
83.56

AV2
82.76
81.93
79.74
76.81

AV3
66.12
53.76
68.23
69.86

AV4
59.64
60.60
48.05
54.54

AV5
53.44
57.47
65.00
48.48

AV6
53.22
52.17
42.86
64.28

AV7
35.48
35.10
43.53
41.09

AV8
32.25
38.46
40.00
34.78

AV9 AV10
27.42
30.50
23.65
23.19
34.11
30.48
29.17
23.88

both the techniques of late-stage fusion, i.e., ensemble and
stacked generalization (SG). We can observe the following
trends from this table: First, the predictive performance of
the GLOBL channels, when fused with the HPCs, is higher
than the predictive performance of the fused GLOBL models
(in Table 6) or the HPC-based base-classiﬁers. Second, for
CD-DATASET, where the performance of the HPC-based base-
classiﬁers suffers signiﬁcant degradation; upon fusing their
decisions with the GLOBL channels, it restores the accuracy
to a signiﬁcant extent. For example, the F1-score of HPC-
group-4 dropped to 0.46 when tested on the CD-DATASET;
however, it increased to 0.90 when fused with the GLOBL
channels. While the performance degradation of the base-
classiﬁer when testing on CD-DATASET was 31.3%, the degra-
dation of the HPC-GLOBL fused model was merely 3.2%.
The receiver operating characteristic (ROC) curve (on the
STD-DATASET) for the Ensemble-fusion models is presented
in Figure 9.

Takeaway-5

The HPC-GLOBL fused model inherits the concept-drift ro-
bustness from the GLOBL channels and exploits the thread-
level proﬁling power of the HPCs, resulting in a robust
malware detector with better performance.

6.4 Comparison with software-based detec-

tors

We compare the performance of the HPC-GLOBL fused mod-
els against the software-based AV engines available on VT.
For the samples present in our test dataset (in STD-DATASET),
we queried VT to get decisions from the AV engines. Not all
of the AV engines produced a decision on the samples present

in our test dataset, as they may not be specialized in Android
malware. Therefore, we selected the top 10 AV engines with
the best detection performance on the test samples and had
produced decisions for at least 85% of the samples present
in our dataset. These AV scanners are ESET-NOD32, Ikarus,
K7GW, Microsoft, CAT-Quickheal, Fortinet, Avira, Cyren,
Kaspersky, Lionic, Trustlook, and ZoneAlarm.

Table 8 shows the detection performance of the different
fused GLOBL-HPC-groups. For example, while the best AV
performance for the test dataset of HPC group-3 is around
80%, the fused model provides a detection rate of 86.54 with
an FPR of 2.9%. Such observations clearly show that the
HMD can play a signiﬁcant role in enhancing endpoint detec-
tion performance in a system of collaborative detectors.

7 Discussion

Theoretical basis of XMD’s superior performance. XMD
performs classiﬁcation in a higher solution volume than prior
HMDs. Theorem-1 indicates that the solution volume of
GLOBL channels is higher than the HPCs, resulting in better

Figure 9: ROC curve for XMD

12

concept drift robustness. Theorem-2 guides the design of a
fusion-based approach that results in a higher solution vol-
ume than the individual telemetry channels. XMD exploits
both the theorems to provide increased concept drift robust-
ness compared to HPC-based HMDs (Theorem-1) and better
classiﬁcation performance than prior CPU-telemetry-based
HMDs (Theorem-2).

Non-determinism in Hardware Telemetry. Recent
works have identiﬁed failure scenarios in HMDs arising from
the non-determinism in the HPC-based telemetry [38, 76].
Non-determinism in HPCs arises due to measurement errors
like overcounting, background OS operations, and multiple
applications running on the system. On the other hand, the
non-determinism in the GLOBL channels arises from global
proﬁling compared to the thread-level proﬁling performed by
the HPCs. This makes the GLOBL channels directly suscep-
tible to the noise arising from background applications, and
therefore, they have much higher stochasticity than HPCs
(Takeaway-1). Hence, we believe our assumption of non-
determinism in GLOBL channels and determinism in HPC
for the theorems is valid and is supported by Takeaway-3.

Compared to prior HPC-based defenses, XMD does not
eschew the non-determinism in the hardware telemetry. XMD
exploits the non-determinism in the hardware telemetry to
provide superior concept drift robustness compared to the
prior HPC-based approaches. Moreover, XMD relies on an
expansive set of telemetry channels that is not restricted to
the CPU core of the SoC, potentially making it robust to such
proof of concept attacks that are aimed at skewing the CPU
telemetry measurements [38, 76]. Finally, we concur with the
opinions of Kazdagli et al., i.e., the goal of HMD is not to
provide 100% true positives and 0% false positives but to
provide an extra layer of protection and complement other
software-based detectors in a collaborative defense system.
Extension to Desktop-class of devices. While we have
considered a mobile-device environment in this work, a simi-
lar approach can be extended to desktop-class of devices. Pro-
ﬁling frameworks provided by hardware vendors can monitor
the trafﬁc activity to storage and network devices, memory ac-
cess patterns like stalls on loads from DRAM and caches, and
the GPU and CPU usage of the proﬁled applications [8]. The
drivers from such tools can be repurposed to potentially en-
hance the detection performance of the deployed HMDs [6].
Limitations and Future Works. Our work has a few lim-
itations. First, while we have taken measures to make our
data-collection environment realistic for mobile devices, there
are corner cases that are not covered by the collection environ-
ment. Our Interaction Module (Appendix B.3) uses the Mon-
key tool, which is a stateless interaction, different from real
user inputs. A better approach for simulating UI interaction
is using a state-based interaction model (e.g., Droidbot [57]).
However, in our framework, Droidbot crashed unpredictably
and required human intervention. Future work can look into
better approaches for simulating human interaction for large-

scale telemetry collection. The analysis performed in this
paper is based on a single Google Pixel 3 mobile device and
is therefore restricted in scope with respect to SoC chipsets
and mobile devices. However, the fundamental theorems and
the corresponding empirical observations presented in this
work do not have architectural or platform dependence and
should potentially extend to other mobile devices. We de-
fer the empirical veriﬁcation of the proposed theorems on
different mobile device platforms as future work.

Second, while our data-collection environment is realis-
tic for mobile devices, the same assumptions may not hold
for desktop-class machines, where the number of workloads
running concurrently is much higher than the mobile-device
environment. In Equation 6 (Appendix A.2) representing the
solution volume with stochastic telemetry, if the diffusion
term (arising from the stochasticity) overpowers the drift
term, then classiﬁcation with a linear hyperplane may not be
feasible. In such a case, we need per-process tracking capabil-
ity for the expansive hardware telemetry, like those provided
by commercial proﬁlers [8]. We defer the extension of XMD
to desktop-class of machines for future work.

Finally, we have used a late-stage fusion approach to incor-
porate the power of multiple telemetry channels. However,
an intermediate fusion-based approach can potentially result
in better detection performance at the cost of increased com-
plexity, e.g., an approach that exploits the interaction between
the different telemetry channels. We leave the exploration of
such novel ML agents as future work.

8 Conclusion

In this paper, we propose XMD, a malware detector that ex-
ploits the expansive set of hardware telemetries from the dif-
ferent sub-systems of an SoC used in a mobile device. XMD
exploits two key innovations grounded in Theorems that we
have developed using the Replica Theory of object manifolds:
superior classiﬁcation performance from expansive telemetry
and concept drift robustness from stochasticity in hardware
telemetry. To evaluate XMD, we create three different datasets
using a robust data collection framework. Our ﬁndings sug-
gest that XMD outperforms the current HPC-based detectors
and the commercial AV software on VirusTotal with accept-
able false-positive rates. Therefore, XMD can complement
other software-based detection approaches in a collaborative
defense system.

References

[1] Android Debug Bridge. https://developer.android.com/studio/

command-line/adb.

[2] The Android Logging Service – A dangerous feature for User
https://blogs.uni-paderborn.de/sse/2013/05/17/

Privacy?
privacy-threatened-by-logging/.

[3] Android Monkey.

https://developer.android.com/studio/

test/other-testing-tools/monkey.

13

[4] Android NDK.

https://developer.android.com/ndk/guides/

other_build_systems.

[5] CPU performance scaling. https://www.kernel.org/doc/html/

v4.14/admin-guide/pm/cpufreq.html.

[6] Detect Ransomware and other Advanced Threats with Intel Threat
https://www.intel.com/content/dam/

Detection Technology.
www/public/us/en/documents/solution-briefs/threat-
detection-technology-solution-brief.pdf.

[7] Device performance scaling. https://www.kernel.org/doc/html/

latest/driver-api/devfreq.html.

[8] Fix

performance

bottlenecks with
https://www.intel.com/content/www/us/en/developer/
tools/oneapi/vtune-profiler.html#gs.ug2nh3.

Intel VTune

Proﬁler.

[9] Gnirehtet. https://github.com/Genymobile/gnirehtet.

[10] Intel

collaborates

with Microsoft

against Cryptojacking.

https://www.intel.com/content/www/us/en/newsroom/
news/intel-microsoft-scale-threat-detection-
cryptojacking.html#gs.1khw0g.

[11] Intel introduces Hardware-based Ransomware Detection for busi-
https://latesthackingnews.com/2021/01/20/intel-

nesses.
introduces-hardware-based-ransomware-detection-for-
businesses.

[12] Logcat.

https://developer.android.com/studio/command-

line/logcat.

[13] Magisk. https://magiskmanager.com/.

[14] Microsoft Defender for Endpoint. https://docs.microsoft.com/

en-us/microsoft-365/security/defender-endpoint/
microsoft-defender-endpoint?view=o365-worldwide.

[15] Obfuscation as a service for Android malware.

https:

//www.gosecure.net/blog/2020/12/02/deep-dive-into-an-
obfuscation-as-a-service-for-android-malware/.

[16] Process
T1055/.

injection.

https://attack.mitre.org/techniques/

[17] Qualcomm Mobile Security.

https://www.qualcomm.com/

products/features/mobile-security/snapdragon-malware-
protection.

[18] Simpleperf.

https://github.com/urho3d/android-ndk/blob/

master/simpleperf/doc/README.md.

[19] Threat Intelligence Reports. https://www.fireeye.com/current-

threats/threat-intelligence-reports.html.

[20] Twrp. https://twrp.me/.

[21] Undeletable adware. https://threatpost.com/android-users-

undeletable-adware/157189/.

[22] Viewing and analyzing Android logs with Logcat.

https:
//securitygrind.com/viewing-and-analyzing-android-logs-
with-logcat/.

[23] xhelper malware. https://www.androidpolice.com/2020/04/19/
months-of-research-finally-crack-android-malware-that-
could-even-survive-factory-resets/.

[24] Manaar Alam, Sarani Bhattacharya, Swastika Dutta, Sayan Sinha, Deb-
deep Mukhopadhyay, and Anupam Chattopadhyay. Rataﬁa: Ran-
somware analysis using time and frequency informed autoencoders. In
2019 IEEE International Symposium on Hardware Oriented Security
and Trust (HOST), pages 218–227, 2019.

[25] Matteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang,
Yoon Kim, and SueYeon Chung. Syntactic perturbations reveal rep-
resentational correlates of hierarchical phrase structure in pretrained
language models. arXiv preprint arXiv:2104.07578, 2021.

14

[26] Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein, and Yves
Le Traon. Androzoo: Collecting millions of android apps for the
research community. In Proceedings of the 13th International Confer-
ence on Mining Software Repositories, MSR ’16, pages 468–471, New
York, NY, USA, 2016. ACM.

[27] Halit Alptekin, Can Yildizli, Erkay Savas, and Albert Levi. Trapdroid:
Bare-metal android malware behavior analysis framework. In 2019 21st
International Conference on Advanced Communication Technology
(ICACT), pages 664–671, 2019.

[28] Erin Avllazagaj, Ziyun Zhu, Leyla Bilge, Davide Balzarotti, and Tudor
Dumitras. When Malware Changed its Mind: An Empirical Study
of Variable Program Behaviors in the Real World. In 30th USENIX
Security Symposium (USENIX Security 21), August 2021.

[29] Robert Bridges, Jarilyn Hernández Jiménez, Jeffrey Nichols, Katerina
Goseva-Popstojanova, and Stacy Prowell. Towards malware detec-
tion via cpu power consumption: Data collection design and analytics.
In 2018 17th IEEE International Conference On Trust, Security And
Privacy In Computing And Communications, pages 1680–1684, 2018.

[30] Alexei Bulazel and Bülent Yener. A survey on Automated Dynamic
Malware Analysis Evasion and Counter-Evasion: PC, mobile, and web.
In Reversing and Offensive-Oriented Trends Symposium, ROOTS. As-
sociation for Computing Machinery, 2017.

[31] Nikhil Chawla, Arvind Singh, Monodeep Kar, and Saibal Mukhopad-
hyay. Application inference using machine learning based side channel
analysis. In International Joint Conference on Neural Networks, 2019.

[32] Nikhil Chawla, Arvind Singh, Harshit Kumar, Monodeep Kar, and
Saibal Mukhopadhyay. Securing IoT devices using dynamic power
management: Machine learning approach. IEEE Internet of Things
Journal, 8(22):16379–16394, 2021.

[33] SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Classiﬁcation
and geometry of general perceptual manifolds. Physical Review X,
8(3):031003, 2018.

[34] Shane S. Clark, Benjamin Ransford, Amir Rahmati, Shane Guineau,
Jacob Sorber, Wenyuan Xu, and Kevin Fu. WattsUpDoc: Power side
channels to nonintrusively discover untargeted malware on embed-
ded medical devices. In 2013 USENIX Workshop on Health Informa-
tion Technologies (HealthTech 13), Washington, D.C., August 2013.
USENIX Association.

[35] Uri Cohen, SueYeon Chung, Daniel D Lee, and Haim Sompolinsky.
Separability and geometry of object manifolds in deep neural networks.
Nature communications, 11(1):1–13, 2020.

[36] Patrick Cronin and Chengmo Yang. Lowering the barrier to online
malware detection through low frequency sampling of HPCs. In 2018
IEEE International Symposium on Hardware Oriented Security and
Trust (HOST), pages 177–180, 2018.

[37] Richard WR Darling. Martingales in manifolds. deﬁnition, examples
and behaviour under maps. Séminaire de probabilités de Strasbourg,
16:217–236, 1982.

[38] Sanjeev Das, Jan Werner, Manos Antonakakis, Michalis Polychronakis,
and Fabian Monrose. Sok: The challenges, pitfalls, and perils of using
hardware performance counters for security. In 2019 IEEE Symposium
on Security and Privacy (SP), pages 20–38, 2019.

[39] John Demme, Matthew Maycock, Jared Schmitz, Adrian Tang, Adam
Waksman, Simha Sethumadhavan, and Salvatore Stolfo. On the fea-
sibility of online malware detection with performance counters. In
Proceedings of the 40th Annual International Symposium on Computer
Architecture, ISCA ’13, page 559–570, New York, NY, USA, 2013.
Association for Computing Machinery.

[40] James J DiCarlo and David D Cox. Untangling invariant object recog-

nition. Trends in cognitive sciences, 11(8):333–341, 2007.

[41] Fei Ding, Hongda Li, Feng Luo, Hongxin Hu, Long Cheng, Hai Xiao,
and Rong Ge. DeepPower: Non-intrusive and deep learning-based

detection of IoT malware using power side channels. In Proceedings
of the 15th ACM Asia Conference on Computer and Communications
Security, ASIA CCS ’20, page 33–46, New York, NY, USA, 2020.
Association for Computing Machinery.

[57] Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. Droid-
bot: a lightweight ui-guided test input generator for android. In 2017
IEEE/ACM 39th International Conference on Software Engineering
Companion (ICSE-C), pages 23–26, 2017.

[42] Elizabeth Gardner. The space of interactions in neural network models.
Journal of physics A: Mathematical and general, 21(1):257, 1988.

[58] Roy Longbottom. Updated android benchmarks for 32 bit and 64 bit

cpus from arm and intel contents, 03 2018.

[43] Jarilyn Hernandez Jimenez and Katerina Goseva-Popstojanova. Mal-
ware detection using power consumption and network trafﬁc data. In
2019 2nd International Conference on Data Intelligence and Security
(ICDIS), pages 53–59, 2019.

[44] Roberto Jordaney, Kumar Sharad, Santanu K. Dash, Zhi Wang, Davide
Papini, Ilia Nouretdinov, and Lorenzo Cavallaro. Transcend: Detecting
concept drift in malware classiﬁcation models. In 26th USENIX Secu-
rity Symposium (USENIX Security 17), pages 625–642, Vancouver, BC,
August 2017. USENIX Association.

[45] Sai Praveen Kadiyala, Pranav Jadhav, Siew-Kei Lam, and Thambip-
illai Srikanthan. Hardware performance counter-based ﬁne-grained
malware detection. ACM Trans. Embed. Comput. Syst., 2020.

[46] George Karantzas and Constantinos Patsakis. An empirical assessment
of Endpoint Detection and Response Systems against Advanced Per-
sistent Threats Attack Vectors. Journal of Cybersecurity and Privacy,
2021.

[47] Mikhail Kazdagli, Vijay Janapa Reddi, and Mohit Tiwari. Quantifying
and improving the efﬁciency of hardware-based mobile malware de-
tectors. In 2016 49th Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO), pages 1–13, 2016.

[48] Amin Kharaz, Sajjad Arshad, Collin Mulliner, William Robertson, and
Engin Kirda. UNVEIL: A Large-Scale, automated approach to de-
tecting ransomware. In 25th USENIX Security Symposium (USENIX
Security 16). USENIX Association, August 2016.

[49] Khaled N. Khasawneh, Meltem Ozsoy, Caleb Donovick, Nael Abu-
Ghazaleh, and Dmitry Ponomarev. EnsembleHMD: Accurate hardware
malware detectors with specialized ensemble classiﬁers. IEEE Trans-
actions on Dependable and Secure Computing, 17(3):620–633, 2020.

[50] Dhilung Kirat, Giovanni Vigna, and Christopher Kruegel. BareCloud:
Bare-metal analysis-based evasive malware detection. In 23rd USENIX
Security Symposium (USENIX Security 14), pages 287–301, San Diego,
CA, August 2014. USENIX Association.

[51] Brian Kondracki, Babak Amin Azad, Najmeh Miramirkhani, and Nick
Nikiforakis. The droid is in the details: Environment-aware evasion of
android sandboxes. 2022.

[52] Harshit Kumar, Nikhil Chawla, and Saibal Mukhopadhyay. BiasP: A
DVFS based exploit to undermine resource allocation fairness in linux
platforms. In Proceedings of the ACM/IEEE International Symposium
on Low Power Electronics and Design, ISLPED ’20, page 223–228,
New York, NY, USA, 2020. Association for Computing Machinery.

[53] Harshit Kumar, Nikhil Chawla, and Saibal Mukhopadhyay. Towards
improving the trustworthiness of hardware based malware detector
using online uncertainty estimation. In 2021 58th ACM/IEEE Design
Automation Conference (DAC), pages 961–966, 2021.

[54] Abraham Peedikayil Kuruvila, Shamik Kundu, and Kanad Basu. Ana-
lyzing the efﬁciency of machine learning classiﬁers in hardware-based
malware detectors. In 2020 IEEE Computer Society Annual Symposium
on VLSI (ISVLSI), pages 452–457, 2020.

[55] Abraham Peedikayil Kuruvila, Xingyu Meng, Shamik Kundu, Gaurav
Pandey, and Kanad Basu. Explainable machine learning for intrusion
detection via hardware performance counters. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, 2022.

[56] Li Li, Daoyuan Li, Tegawendé F. Bissyandé, Jacques Klein, Yves
Le Traon, David Lo, and Lorenzo Cavallaro. Understanding android
app piggybacking: A systematic study of malicious code grafting. IEEE
Transactions on Information Forensics and Security, 12(6), 2017.

[59] Yunqian Ma and Yun Fu. Manifold learning theory and applications,

volume 434. CRC press Boca Raton, 2012.

[60] Jonathan Mamou, Hang Le, Miguel Del Rio, Cory Stephenson, Han-
lin Tang, Yoon Kim, and SueYeon Chung. Emergence of sepa-
rable manifolds in deep language representations. arXiv preprint
arXiv:2006.01095, 2020.

[61] Kit Murdock, David Oswald, Flavio D. Garcia, Jo Van Bulck, Daniel
Gruss, and Frank Piessens. Plundervolt: Software-based fault injection
attacks against intel sgx. In 2020 IEEE Symposium on Security and
Privacy (SP), pages 1466–1482, 2020.

[62] Meltem Ozsoy, Caleb Donovick, Iakov Gorelik, Nael Abu-Ghazaleh,
and Dmitry Ponomarev. Malware-aware processors: A framework for
efﬁcient online malware detection. In 2015 IEEE 21st International
Symposium on High Performance Computer Architecture (HPCA),
pages 651–661, 2015.

[63] Nisarg Patel, Avesta Sasan, and Houman Homayoun. Analyzing hard-
ware based malware detectors. In 2017 54th ACM/EDAC/IEEE Design
Automation Conference (DAC), pages 1–6, 2017.

[64] Zhongmin Qian. A gradient estimate on a manifold with convex bound-
ary. Proceedings of the Royal Society of Edinburgh Section A: Mathe-
matics, 127(1):171–179, 1997.

[65] Mohammed Rashed and Guillermo Suarez-Tangil. An analysis of
android malware classiﬁcation services. Sensors, 21(16), 2021.

[66] Irwin Reyes, Primal Wijesekera, Joel Reardon, Amit Elazari Bar On,
Abbas Razaghpanah, Narseo Vallina-Rodriguez, and Serge Egelman.
“won’t somebody think of the children?” examining COPPA com-
pliance at scale. Proceedings on Privacy Enhancing Technologies,
2018:63 – 83, 2018.

[67] Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding
noise to the input of a model trained with a regularized objective, 2011.

[68] Tobias Schneider and Amir Moradi. Leakage assessment methodology.

Cryptographic Hardware and Embedded Systems, 2015.

[69] Cory Stephenson, Jenelle Feather, Suchismita Padhy, Oguz Elibol, Han-
lin Tang, Josh McDermott, and SueYeon Chung. Untangling in invari-
ant speech recognition. Advances in neural information processing
systems, 32, 2019.

[70] Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin
Tang, and SueYeon Chung. On the geometry of generalization and mem-
orization in deep neural networks. arXiv preprint arXiv:2105.14602.

[71] Adrian Tang, Simha Sethumadhavan, and Salvatore Stolfo.
CLKSCREW: Exposing the perils of Security-Oblivious en-
ergy management. In 26th USENIX Security Symposium (USENIX
Security 17), August 2017.

[72] Sanjeev Tannirkulam Chandrasekaran, Abraham Peedikayil Kuruvila,
Kanad Basu, and Arindam Sanyal. Real-time hardware-based malware
and micro-architectural attack detection utilizing cmos reservoir com-
puting. IEEE Transactions on Circuits and Systems II: Express Briefs,
69(2):349–353, 2022.

[73] R. Vinayakumar, Mamoun Alazab, K. P. Soman, Prabaharan Poor-
nachandran, and Sitalakshmi Venkatraman. Robust intelligent malware
detection using deep learning. IEEE Access, 7:46717–46738, 2019.

[74] Jie Xue, Yuan Li, and Ravi Janardan. On the expected diameter, width,
and complexity of a stochastic convex hull. Computational Geometry,
82:16–31, 2019.

15

[75] Junfeng Yu, Qingfeng Huang, and CheeHoo Yian. Droidscreening: A
practical framework for real-world android malware analysis. Sec. and
Commun. Netw., 9(11):1435–1449, jul 2016.

[76] Boyou Zhou, Anmol Gupta, Rasoul Jahanshahi, Manuel Egele, and
Ajay Joshi. Hardware performance counters can detect malware: Myth
or fact? In Proceedings of the 2018 on Asia Conference on Computer
and Communications Security. Association for Computing Machinery.

[77] Yajin Zhou and Xuxian Jiang. Dissecting android malware: Charac-
terization and evolution. In 2012 IEEE Symposium on Security and
Privacy, pages 95–109, 2012.

A Mathematical proofs

We present the rigorous mathematical proofs in this section,
complementing the intuitive proofs and deﬁnitions presented
in Section 4.

A.1 Additional Deﬁnitions and Background

i=1 Siuµ

i where uµ

Manifold. A point on the manifold consists of the input space
xµ ∈ F µ given as xµ((cid:126)S) = ∑D+1
i are a set of
orthonormal bases of the (D + 1) dimensional linear subspace
containing F µ, the D + 1 components Si represent the coor-
dinates of the manifold point within this subspace and are
constrained to be in the set (cid:126)S ∈ S . S denotes the shape of the
manifolds and encapsulates the afﬁne constraint.

Convex Hull and Polytopes: The convex hull of a set of
points S is the intersection of all half-spaces that contain S . A
half-space is either of the two parts into which a hyperplane
divides an afﬁne space. For example, in a two-dimensional
Euclidean space, a half-space is either of the two parts into
which the space is divided by a line. A convex polytope is an
intersection of a ﬁnite number of half-spaces.

Mathematically, the convex hull is given as: C H (F µ) =
(cid:111)
(cid:110)
xµ((cid:126)S) | (cid:126)S ∈ C H (S )

, where

C H (S ) =

(cid:40)D+1
∑
i=1

αi(cid:126)Si | (cid:126)Si ∈ S , αi ≥ 0,

(cid:41)

αi = 1

(4)

D+1
∑
i=1

Solution Volume. Following Gardner’s replica framework

[42], the volume V of the solution space is deﬁned as

(cid:90)

V =

dNwδ (cid:0)(cid:107)w(cid:107)2 − N(cid:1) ∏

µ,xµ∈F µ

Θ (yµw · xµ − κ)

(5)

where Θ(·) is the Heaviside function to enforce the margin
constraints in the linear separation constraint yµw · xµ ≥ κ,
along with the delta function to ensure (cid:107)w(cid:107)2 = N.

A.2 Theorems and Proofs

Lemma 1: The solution volume for a model trained with
stochastic inputs ( ˜V ) is greater than the solution volume for a
model trained with deterministic inputs (V ) i.e., E[ ˜V ] ≥ V .

Short Proof: To model the stochasticity in the inputs,
we add a stochastic process, which is given by the Gaussian
Noise, to the input data x as a diffusion term. To account
for the Gaussian Noise in the input space, we consider the
induced stochasticity in the manifolds of the convex polytopes
that are obtained from the convex hull of the input data. Hence,
the solution volume is also a stochastic random variable ˜V
and is given by

˜V =

(cid:90)

(cid:124)

dNwδ (cid:0)(cid:107)w(cid:107)2 − N(cid:1) ∏

µ,x∈F

Θ (yw · x − κ)

(cid:123)(cid:122)
drift term=V
(cid:90)

+

(cid:124)

(cid:125)

(6)

G (wα, α) dBα
(cid:123)(cid:122)
(cid:125)
diffusion term

where we model the stochasticity by adding a diffusion term
(cid:82) G (wα, α) dBα. The diffusion term consists of Brownian mo-
tion Bα and its coefﬁcient G is parameterized by unknown
variables wα, α. The diffusion term is an Itô integral and it fol-
lows Gaussian distribution. If we set G (wα, α) = σ, the result
of integration is Bα+s − Bs ∼ N (cid:0)0, ασ2(cid:1), which is consistent
with adding Gaussian noise. Next, we show that E[ ˜V ] ≥ V .
integral
of
(cid:82) G (wα, α) dBα, we assume that G (wα, α) is changed at
discrete time points ti(i ∈ [1, N]), where 0 < t1 < . . . < tN < T .
We deﬁne the integral

calculation

stochastic

For

the

the

S =

(cid:90) T

0

G (wα, α) dBα

as the Riemann sum

SN(w) = lim
N→∞

N
∑
i=1

G (αi−1, w) (cid:0)Bαi − Bαi−1

(cid:1)

.

Since the diffusion is on the manifold of the convex poly-
tope of V , G (wα, α) is a submartingale, which is a process
which increases on average [37, 64]. Hence, E[SN(w)] ≥ 0 ⇒
E[ ˜V ] ≥ V . Therefore, the solution volume with stochastic
inputs is lower bounded by the solution volume with deter-
ministic inputs.(cid:4)

Lemma 2: Let us represent the input with drift as X(cid:48) =
X + ˆX, where X is the input data with stochasticity and ˆX rep-
resents the drift. The modiﬁed convex polytope formed with
drift can be written as C H (X(cid:48)) = C H (X + ˆX) ≥ C H (X).

Proof: We use the expected diameter of stochastic convex
hulls to compare their volumes as discussed by Xue et al. [74].
The diameter of a convex polytope A, denoted by diam(A),
is deﬁned as the maximum Euclidean distance between any
two points in the polytope. Hence, the expected diameter of
the stochastic manifold X(cid:48) is given as

diamX(cid:48) = ∑
R⊆X(cid:48)

Pr[R] · diam(C H (R))

(7)

16

where Pr[R] denotes the probability that R occurs as a real-
ization of X(cid:48). Since we are considering a Gaussian noise as
the diffusion term, without loss of generality, we can consider
that the probability of realization remains constant as we dis-
sect the space X(cid:48) into X, ˆX respectively. Hence, using triangle
inequality for any two points in the convex hulls of the two
sets X, ˆX, we get

and A ∪ C = B. Thus, V (A ∪ C) = V (A) + V (C) = V (B).
Therefore V (A) ≤ V (B) for V (C) ≥ 0 by non-negativity.
Hence, the solution volume of a convex manifold X, given as
V {X}, is a monotonic increasing measure. Therefore,

[V {C H

(cid:32)

(cid:91)

i

(cid:33)

Vi

}] = [V {C H (Vmax ∪ Vi\Vmax)}]

Pr[T ] · diam(C H (T ))

⇒ V [C H

diamX(cid:48) = ∑

R⊆X+ ˆX

Pr[R] · diam(C H (R))

Pr[S] · diam(C H (S)) + ∑
T ⊆ ˆX

Pr[S] · diam(C H (S))

≥ ∑
S⊆X
≥ ∑
S⊆X
= diamX

(8)

(cid:4)

Theorem 1: Stochasticity in the input training data im-

proves concept drift robustness of the model.

Short Proof: We consider the manifold obtained from the
input space using its convex hull. If X is the initial manifold
of the input space, then the input with the drift is represented
as X(cid:48) = X + ˆX, where ˆX represents the drift in the manifold.
Hence, the modiﬁed convex polytope formed with drift can
be written as C H (X(cid:48)) = C H (X + ˆX) ≥ C H (X) (by Lemma
2). Now, we consider X (cid:48),X as the volume of the convex poly-
topes of the input with and without concept drift, respectively.
Now, X (cid:48) ≥ X due to the added volume of the convex hull
formed with the drifted datapoints. Let us denote the solution
volumes with and without concept drift as V (cid:48),V respectively
and the solution volume with stochasticity as ˜V . V is pro-
portional to the complement of the volume of the convex
polytope of the input data X i.e., V ∝ 1
X . Using Lemma-1,
E[ ˜V ] ≥ V n ⇒ ˜V (cid:48) ≥ V (cid:48) i.e., solution volume for conecpt drift
with stochasticity is greater than the solution volume without
stochasticity. Thus, stochasticity improves the robustness to
concept drift. (cid:4)
Lemma 3:

V [C H

(cid:32)

(cid:91)

i

(cid:33)

Vi

] ≥ max{Vi}

(9)

Proof: Let Vi be nonempty, convex sets. We show that
x ∈ C H ((cid:83)
i Vi) if and only if there exist elements vi ∈ Vi
and λi ≥ 0 with ∑i λi = 1 such that x = ∑i λivi. This can be
represented as

C H ((cid:91)

Vi) =

i

(cid:40)

∑
i

λivi | ∑
i

λi = 1, λi ≥ 0, vi ∈ Vi

(cid:41)

Now, let us consider the solution volume V as a measure
deﬁned on a vector space Ω. Then we show that V (A) ≤ V (B)
for all A ⊂ B ∈ Ω. Let A ⊂ B, let C = Ac ∩ B. Then A ∩C = /0

17

≥ [Vmax ∪ Vi\Vmax]
≥ [Vmax] = max{[Vi] ∀i}

(cid:33)

Vi

] ≥ max{(Vi)}

(10)

(cid:32)

(cid:91)

i

(cid:4)

Theorem 2: Let Vi be the solution volume corresponding
to the telemetry channel-i in the N-dimensional vector space
where the i-th basis corresponds to the i-th telemetry channel
∀i ∈ [1, N], and N is the total number of telemetry channels.
Then, V [C H ((cid:83)

i Vi)] ≥ max{Vi} ∀i ∈ [1, N] .
Short Proof: We consider a convex polytope for each of
the solution volume Vi. Without loss of generality, we assume
that each Vi is an orthogonal projection of the union of the so-
lution volumes ((cid:83)
i Vi), which we refer to as the universal con-
vex polytope. The universal convex polytope is constructed by
taking a convex hull over the union of its N orthogonal compo-
nents Vi. From Lemma 3, we get V [C H ((cid:83)
i Vi)] ≥ max{Vi}.
(cid:4)

B Data Collection Framework

B.1 Dataset bias in prior works

Benchmarks used as benign workloads. Prior works on HMD
primarily use benchmark applications for benign workloads
[24,32,39,45,54,55,63,72]. Few works use regular benign ap-
plications (e.g., from Play Store), however, they mix these ap-
plications with benchmark applications [49, 76]. Compared to
regular benign applications, which require interaction with the
device to explore the different threads of operation, running
the benchmark application is straightforward which eases the
process of large-scale data collection. These benchmark ap-
plications are synthetic workloads designed to test a speciﬁc
functionality of the SoC and are not representative of real-
world benign applications, introducing a bias in the dataset.
Execution of the workload. Prior "black-box" approaches
of performing large-scale data collection for hardware teleme-
try do not verify whether the malware is executing [32, 36, 39,
45,54,55,63,72]. The raw hardware signatures do not provide
interpretability, making the veriﬁcation of malware execu-
tion more difﬁcult, as compared to software-based behavioral
signatures [48]. It has been shown that such "black-box" ap-
proaches toward collecting hardware-telemetry are ineffective
and introduce inconsistencies in the dataset. For example, in
the original work using HPC for malware detection [39], 20%

of malware traces are shorter than 1 second, and 56% are
<10s [47]. Kazdagli et al. identiﬁed this issue in their work
and took steps to ensure the correct execution of synthetic
malware payloads that they had devised [47]. However, for
off-the-shelf malware, they did not elaborate on how they
"checked the validity of performance counter readings". Fur-
ther, prior DVFS works have not tackled this issue in malware
detection approaches [32]. For such global signatures, it is
important to ensure the successful execution of workloads of
interest, because these channels capture signatures irrespec-
tive of the execution of the workload of interest. Finally, vital
functionalities of malware like communication with the C2
server and subsequent payload activation often rely on net-
work access. Prior works do not specify access to the internet
in their data collection setup [32, 36, 45, 54, 55, 63, 72, 76].

B.2 Checkpointing scheme

We devised a custom checkpointing scheme to restore the
client to its clean state after data collection was performed
on a particular sample. Since performing a factory reset on
the phone requires manual intervention to set up the phone
again, we used a custom recovery tool (TWRP [20]) to create
an image of the userdata partition. We assumed that the appli-
cations under analysis can read/write to the userdata partition.
Since the client was rooted (using Magisk [13]) and we ran
applications with full permissions, we used a checksum-based
scheme to determine whether the non-userdata partitions are
altered or not, handling cases where malware modify system
partitions to gain persistence [21, 23]. After the malicious
application ﬁnishes its execution, we verify the integrity of
the non-userdata partitions using the checksum. Upon detect-
ing modiﬁcations to the system partitions, we ﬂash all the
partitions of the device, otherwise, we only ﬂash the userdata
partition. As we can conclude from Table 9, the checksum-
based integrity checking saves a lot of data-collection time,
considering the fact that we did not detect any malware in our
dataset that modiﬁed the non-userdata partition.

Table 9: Checkpointing procedure timings

Step
Reboot
Restore USERDATA partition
Restore ALL partitions

Time (s)
28
195
435

B.3

Interaction Module

The activation of malicious payloads in the Android ecosys-
tem is often contingent on events that require user interactions.
This makes performing large-scale automated data collection
for Android malware more difﬁcult as compared to desktop-
based malware [47]. Therefore, we devised an interaction

module to simulate human interaction with the goal of trigger-
ing the malware payloads. The Interaction-module consists of
two sub-modules: Monkey-based interaction and Broadcast-
event-based interaction.

Monkey-based

interaction. We
use Android’s
UI/Application Exerciser Monkey [3], a popular tool
in Android SDK, to automate the interaction with applica-
tions by simulating user inputs. Monkey simulates random UI
interactions by injecting a pseudorandom stream of simulated
user input events into the app. Prior works have shown that
Monkey matches or exceeds adult human coverage 61%
of the time [66]. However, we note that Monkey provides
sub-optimal code coverage considering its random nature,
and is unable to react to visual UI elements like popup dialog
boxes. Therefore, we added functionalities (like Broadcast
Event-based interaction) to increase the code coverage.
Furthermore, our Logcat based activation checker ﬁlters out
the iterations in which the Interaction module fails to achieve
sufﬁcient activation ensuring the cleanliness of the dataset.

Broadcast-event-based interaction. Malware applica-
tions observe system-level broadcast events, an essential
component of Inter-Process communication, for activating
their payload [75, 77]. For example, the malware explained
in the motivating example monitors the broadcast event
BOOT_COMPLETED to trigger as soon as the device completes
the boot process. Moreover, there are multiple broadcast
events that the malicious application registers statically in
its manifest ﬁle for triggering actions, as described in [77].
Therefore, the Interaction-module uses adb commands to
send different broadcast receiver events to the targeted mal-
ware application for activating the malicious payload.

B.4 Data Collection Module

GLOBL : We used a script, written in C, that reads the oper-
ating states of all the GLOBL channels mentioned in Table
3, logs the timestamp, and saves these values to a ﬁle. The
script was cross-compiled using the NDK toolchain [4] and
runs natively in the client. This binary runs in the background
while the proﬁled application is running in the foreground.
The average sampling frequency for collecting the GLOBL
data was 3.5 kHz.

HPC : The HPC events are monitored using Android’s
simpleperf tool which is a native CPU proﬁling tool for An-
droid [18]. Using simpleperf, we can repurpose the hardware
counters offered by Snapdragon’s Performance Monitoring
Unit, to record the HPC events that are stated in Table 2. We
used the stat command of simpleperf to sample the accumu-
lated counter data for the proﬁled application every 100 ms.
We monitor the entire application which includes monitoring
all the threads of the application. We note that the HPC mea-
surement setup doesn’t account for evasion techniques like
process injection, where the malicious application runs in the
context of other benign processes [16].

18

B.5 Orchestrator module : High-level ﬂow

Orchestrator module. The Orchestrator module is a Python
script executing on the Host, responsible for invocation
and synchronization of the execution of application-under-
analysis, the Data-Collection module, and the Interaction mod-
ule. The orchestrator also performs the checksum extraction
to verify the integrity of the userdata and system partitions
and pushes the collected data logs to the cloud. A high-level
ﬂow of the orchestrator module is as follows:

(1) In the host-client sandbox setup, the client starts with
a clean image of the rooted OS. (2) The application-under-
analysis is transferred to the Android device and the reverse
tethering process is started for providing internet access to
the device. A pre-infection checksum of all the partitions is
obtained for integrity veriﬁcation. (3) The application is in-
stalled on the device followed by the simultaneous invocation
of the application execution, Data-collection module, and the
Interaction module. The application is run for 40 seconds
with the Monkey interacting with the application for the ﬁrst
30 seconds followed by sending broadcast events to the appli-
cation for the next 10 seconds. At the end of the execution, the
malicious process is killed. (4) Once the hardware telemetry
collection is ﬁnished, the application is killed and the gen-
erated data logs are pulled from the analysis device to the
host machine. A post-infection checksum of the partitions
is obtained and the entire OS is ﬂashed if there is any viola-
tion in the integrity of non-userdata partitions, else only the
userdata partition is ﬂashed. (5) Post-ﬁltering is performed
using the Logcat-based ﬁlter to ensure that we only use the
hardware telemetry logs for iterations in which the applica-
tion is executed. For the benign applications, we uninstalled
the application between different iterations and rebooted the
device between iterations, considering such applications do
not affect non-userdata partitions.

C Analysis

C.1 Dataset Split

(SPLT-case-1) Training and Testing the individual classiﬁers
for each GLOBL channel: The complete dataset is split into
Training and Testing with a 70-30 split. (SPLT-case-2) Train-
ing and Testing the individual classiﬁers for each of the differ-
ent HPC groups: The HPC logs for the respective groups are
split into Training and Testing with a 70-30 split. (SPLT-case-
3) Training and Testing the Stacked Generalization models
for the late-stage fusion of decisions from the GLOBL classi-
ﬁers and the HPC classiﬁers: We perform an initial scan of
the testing dataset used in case-1 and case-2 to identify all
the iterations of data collections for which both the HPC and
the GLOBL data logs were present, allowing us to fuse the
decisions from both HPC and DVFS. The resulting dataset
is further split into training (SG-train) and testing (SG-test),

for training the second-stage model and testing the stacked
models. Through this process, we ensure that for training the
second-stage model, we are not using the same training data
that was used for training the base classiﬁers. Finally, we form
a new dataset for the second-stage model by collecting the
output of the individual base classiﬁers using SG-train. We
now treat training the second-stage model as a new problem
and employ a learning algorithm to solve it.

C.2 Feature Engineering

GLOBL. The GLOBL channels consist of multivariate time-
series signatures (Figure 3.(a)), with each channel capturing
behavioral information from a different sub-device. Unlike
the HPC channels, each sample at a particular time stamp is a
state instead of event counts. Therefore, while the individual
states are important, the variation of states with the passage
of time contains valuable information as well. Windowed
FFT is performed on each channel, resulting in a spectro-
gram where the x-axis is time, the y-axis is frequency, with
the corresponding entries as the amplitude of the Fourier
Transform. This is followed by performing dimensionality
reduction on both the time axis and the frequency axis using
Principal Component Analysis. Next, we select the PCA com-
ponents that capture >95% variance in the features. Finally,
the features from all the channels are stacked resulting in a
tensor of dimension num_channels x feature_size. The
high sampling frequency of the GLOBL channels warrants
the conversion of the time domain signals into the frequency
domain, as windowed FFT captures a compact representation
of the highly sampled time series.

HPC. For the HPC channels, each sample is a vector made
up of event counts at the time of sampling. Figure 3.(b) shows
the feature engineering steps for one HPC-group with three
different channels. First, the multivariate time series is divided
into 32 equal parts. Next, the raw values are summed over in
each interval to form a feature vector of size 32 for each vari-
ate of the time series. Finally, the feature vectors are ﬂattened
to get a single feature vector of size 1 x feature_size,
which represents the information captured by a single HPC
group. The feature engineering choices for the HPC are made
by considering the jitter and noise introduced due to the limi-
tations in the measurements. The process of binning preserves
the raw values, reduces the effects of jitter [76], and evens out
the noise [39], resulting in better-trained classiﬁers.

C.3 Failure modes of different GLOBL chan-

nels

Figure 10 presents the decisions of the different GLOBL
channels on a set of samples (subset of 25 samples selected
for visualization). Each square represents a decision, where
a black square is a correct decision, and a white square is an
incorrect decision.

19

Figure 11: Weights assigned to the decisions of the base-
classiﬁers (one model per channel) by the second-stage model
(i.e. Logistic Regression) in Stacked Generalization, where
decisions from each HPC-group is fused with the 15 GLOBL
channels resulting in corresponding fused-group.

Figure 10: Decisions of GLOBL channels on a subset of
samples. [White square] is incorrect decision, [Black square]
is correct decision.

C.4

Interpretation of second-stage model pa-
rameters

Figure 11 presents the learned weights assigned to the ﬁrst
stage models by the second stage model when the decisions
are fused using Stacked Generalization. A higher absolute
weight corresponds to higher importance being assigned to
the corresponding channel, when arriving at the ﬁnal decision.
We can observe that chn-13 (#received-bytes) and chn-14
(#transmitted-bytes) have a higher weight underscoring the
importance of network interaction when identifying malicious
workloads. For the fused-group-3, we can also observe that
the HPC telemetry has a signiﬁcantly higher weight as com-
pared to the other fused-groups. This can be attributed to
the incorporation of raw-crypto-spec in the HPC-group-3
which is able to potentially capture a subset of the network
interaction.

20

chn-1chn-2chn-3chn-4chn-5chn-6chn-7chn-8chn-9chn-10chn-11chn-12chn-13chn-14chn-15GLOBL channels05101520Sampleschn-1chn-2chn-3chn-4chn-5chn-6chn-7chn-8chn-9chn-10chn-11chn-12chn-13chn-14chn-15HPCfused-group-1fused-group-2fused-group-3fused-group-40.63.30.02.90.40.80.93.74.2-0.41.94.06.83.00.31.40.01.60.80.20.71.1-0.60.61.11.31.61.22.91.6-0.11.00.93.01.01.90.50.7-1.72.1-2.01.0-0.23.53.6-0.92.83.50.21.61.80.31.82.81.8-0.51.91.01.34.33.91.4-1.70.442024