Diet Code Is Healthy: Simplifying Programs for Pre-trained
Models of Code
Zhaowei Zhang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1âˆ—
1School of Software, Shanghai Jiao Tong University, China
2The University of Newcastle, Australia
{andy_zhangzw,bjshen,xiaodong.gu}@sjtu.edu.cn,hongyu.zhang@newcastle.edu.au

2
2
0
2

p
e
S
0
2

]
E
S
.
s
c
[

4
v
0
9
3
4
1
.
6
0
2
2
:
v
i
X
r
a

ABSTRACT
Pre-trained code representation models such as CodeBERT have
demonstrated superior performance in a variety of software engi-
neering tasks, yet they are often heavy in complexity, quadratically
with the length of the input sequence. Our empirical analysis of
CodeBERTâ€™s attention reveals that CodeBERT pays more attention
to certain types of tokens and statements such as keywords and data-
relevant statements. Based on these findings, we propose DietCode,
which aims at lightweight leverage of large pre-trained models for
source code. DietCode simplifies the input program of CodeBERT
with three strategies, namely, word dropout, frequency filtering,
and an attention-based strategy that selects statements and tokens
that receive the most attention weights during pre-training. Hence,
it gives a substantial reduction in the computational cost without
hampering the model performance. Experimental results on two
downstream tasks show that DietCode provides comparable results
to CodeBERT with 40% less computational cost in fine-tuning and
testing.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Natural language processing.

KEYWORDS
Program simplification, Pre-trained models, Learning program rep-
resentations, Code intelligence

ACM Reference Format:
Zhaowei Zhang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1. 2022. Diet
Code Is Healthy: Simplifying Programs for Pre-trained Models of Code. In
Proceedings of the 30th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE â€™22),
November 14â€“18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3540250.3549094

1 INTRODUCTION
Pre-trained models of code such as CodeBERT [15] have been the
cutting-edge program representation technology, yielding remark-
able performance on a variety of software engineering tasks such

âˆ—Xiaodong Gu is the corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3549094

as code completion [11], code search [15], and clone detection [29].
Having pre-trained on large-scale code corpora, they demonstrate a
better understanding of the semantics of source code than previous
deep learning models such as code2vec [3] and ASTNN [50].

Despite making a giant leap in accuracy, pre-trained models are
often heavy in computation, which significantly hinders their appli-
cations in practice. For example, the standard CodeBERT contains
125 million parameters and takes 28 hours to pre-train on 8.5M code.
More critically, pre-trained models usually need to be fine-tuned
before use, which is cost inefficient due to the large scale of param-
eters and training data. As such, it is highly desirable to identify
the critical feature learned by pre-trained models and reduce the
required computational cost by focusing only on the important
information from model inputs [34].

To understand critical information learned by pre-trained mod-
els, we conduct an empirical analysis of CodeBERT â€“ a pre-trained
model for programming and natural languages. Our study aims to
find out (i) what kinds of tokens CodeBERT pays the most attention
to; and (ii) what kinds of statements are most important to Code-
BERT when learning code representations. To answer these two
questions, we categorize tokens and statements into a few classes
and summarize the attention weights of each class that the pre-
trained CodeBERT assigns to. Our results reveal that keywords and
data types are the most critical tokens that CodeBERT focuses on. In
terms of statements, CodeBERT pays more attention to method sig-
natures and return statements, which show the overall functionality
of a method.

Based on these empirical findings, we propose DietCode, a novel
method that aims at lightweight leverage of large pre-trained mod-
els for source code. DietCode reduces the computational complexity
of pre-trained models by pruning unimportant tokens and state-
ments from the input programs. Three pruning strategies are pro-
posed, including word dropout, frequency filtering, and attention-
based pruning. In particular, the attention-based pruning strategy
selects tokens and statements that receive the highest attention
from pre-trained models. The program simplification algorithm
formulates statement selection as a 0-1 knapsack problem where
statements are regarded as items, and their attention weights are
regarded as values. The algorithm selects statements (items) under
the constraint of a given target length (capacity).

We apply DietCode to two downstream tasks, namely, code
search and code summarization. We measure the performance with
relative length, FLOPs, and time cost, and compare our approach
with baseline models, including the original pre-trained code model
and SIVAND [34]. Experimental results show that DietCode pro-
vides comparable results as RoBERTa (code), with nearly 40% less
computational cost in fine-tuning and testing.

 
 
 
 
 
 
ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

Our contributions can be summarized as follows:

â€¢ We conduct an in-depth empirical analysis of critical tokens

and statements learned by CodeBERT.

â€¢ We propose a novel program simplification approach for pre-
trained programming language models that can significantly
reduce the computation cost while retaining comparable
performance.

â€¢ We extensively evaluate the proposed approach in two down-
stream tasks and show the effectiveness of our approach.

2 BACKGROUND
2.1 Pre-Trained Language Models
Pre-trained language models such as BERT [14], GPT-2 [35], and
T5 [36] have achieved remarkable success in a variety of NLP
tasks [12, 13, 47]. They refer to neural network models trained
on large text corpora and can be fine-tuned to low-resource down-
stream tasks.

State-of-the-art pre-trained models are mainly built upon the
Transformer architecture [43]. The Transformer is a sequence-to-
sequence learning model using the attention mechanism [4]. It
is based on the encoder-decoder architecture, where a source se-
quence is encoded into hidden states and then taken as input to
the decoder for generating a target sequence. Both the encoder and
decoder contain multiple identical layers, and each layer consists
of a multi-head self-attention network followed by a feed-forward
network. Both of the outputs will be normalized before entering
the next layer.

The key component of Transformer is the self-attention mech-
anism that represents a sequence by relating tokens in different
positions [43]. The goal of self-attention is to learn the important
regions in the input sequence. Unlike traditional recurrent neural
networks [10], self-attention networks can learn the dependencies
from distant tokens in parallel. For an input sequence (ğ‘¥1,. . . , ğ‘¥ğ‘›)
of length ğ‘›, the self-attention produces its representation (ğ‘§1, ğ‘§2 ...
ğ‘§ğ‘›) as

ğ‘§ğ‘– =

ğ‘›
âˆ‘ï¸

ğ‘—=1

Softmax(

(ğ‘¥ğ‘–ğ‘Š ğ‘„ ) Â· (ğ‘¥ ğ‘—ğ‘Š ğ¾ )ğ‘‡
âˆš
ğ‘‘

) Â· ğ‘¥ ğ‘—ğ‘Š ğ‘‰

(1)

where ğ‘Š ğ‘„ , ğ‘Š ğ¾ , and ğ‘Š ğ‘‰ denote the modelâ€™s parameters. ğ‘‘ is the
dimension of the input matrix. The model involves several attention
heads, and each headâ€™s output is concatenated into the final result.
Pre-trained models usually involve large-scale parameters and
consume huge computational resources. For example, BERT-base
and BERT-large contain 110M and 340M parameters, respectively.
As such, the lightweight leverage of pre-trained models is greatly
desirable for research and practitioners [39].

2.2 CodeBERT
Recently, researchers have adopted BERT for software engineer-
ing tasks and proposed CodeBERT [15]. CodeBERT is a bimodal
pre-trained model for natural and programming languages, captur-
ing semantic representations from programming languages [15].
The program representations learned by CodeBERT can be fur-
ther utilized for downstream tasks such as code search and code
summarization.

CodeBERT is built on top of a Transformer encoder [43]. The opti-
mization involves two tasks: masked language modeling (MLM) and
replaced token detection (RTD). MLM masks two random tokens
from the input pair of code and natural language comments, and
aims to predict the original token in an extensive vocabulary. RTD
involves two generators and a discriminator. The generators predict
the original token for the masked token while the discriminator
predicts whether the tokens are original or not. After pre-training,
CodeBERT can be adapted to downstream tasks through fine-tuning
on the target dataset.

3 EMPIRICAL ANALYSIS
3.1 Study Design
In this section, we describe our study methodology and experi-
mental setup. There are many levels of granularity for code, such
as tokens, statements, and functions. As an in-depth study, we
begin by investigating the atomic unit of source code, namely, to-
kens. We next investigate the statement-level knowledge learned
by CodeBERT, which contains basic structures and semantic units.
We finally explore the function-level knowledge learned by Code-
BERT through downstream tasks. In summary, we design our study
methodology by addressing the following research questions:

â€¢ RQ1: What critical tokens does CodeBERT learn about?
We study the critical information CodeBERT learned at the
token level by analyzing the attention weights assigned to
these tokens and visualizing their relative importance.
â€¢ RQ2: What critical statements does CodeBERT learn
about? We further study the statements that CodeBERT
assigns the most weights to. We classify code statements
into common categories such as initialization, assignment,
and return, and present the attention weight of each category
assigned by CodeBERT.

To answer these questions, we first need to know how to rep-
resent the key information learned by CodeBERT. In other words,
how to measure the importance of each token and statement? Since
the heart of CodeBERT is the self-attention network, where hidden
states of each token are calculated layer-by-layer according to the
self-attention weights, we measure the importance of each token
using the attention weights in the Transformer layers in Code-
BERT after pre-training. Next, we will present the details of how to
measure the importance of tokens and statements, respectively.

3.1.1 Measuring Token Importance Using Attention Weights. As
introduced in Section 2.2, CodeBERT takes a sequence of source
code tokens as input and produces a self-attention weight for each
input token. Each weight measures how the corresponding token
gains attention from other tokens in the input sequence. The higher
the attention weights, the more attention that are paid by other
tokens. Therefore, in our study, we measure the importance of
each token using the attention weight. CodeBERT has multiple
self-attention layers and heads, each producing an attention weight
for the same token. We average the attention weights of all layers
and heads for each token. In our experiments, we go through the
CodeSearchNet corpus [20] and take as input each code snippet to
the pre-trained CodeBERT. Then, we calculate the average attention
weight that CodeBERT assigns to each token.

Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Table 1: Statistics of statements. Arithmetic means state-
ments with only mathematical operations. Function Invoca-
tion represents statements that invoke other functions.

Category

Quantity Category

Quantity

Function Invocation
Method Signature
Variable Declaration
If Condition
Annotation
Return
Getter
For
Try
Logging
Setter

16,558
11,755
11,701
11,646
8,980
8,331
3,092
2,190
1,797
1,763
1,721

Throw
Catch
Arithmetic
Case
While
Break
Finally
Continue
Switch
Synchronized

1,460
1,309
628
577
459
341
297
142
210
73

3.1.2 Computing Attentions of Statements. Having obtained the
importance of each token, we compute the attention weight for
each statement. Intuitively, the attention weight for a statement
can simply be obtained using the average attention weights of all
its tokens. However, different tokens in a statement have different
importance. Without considering the global importance of each
token, statements that consist of unimportant tokens could obtain
even higher attention. Based on this concern, we regularize state-
ment attention by penalizing unimportant tokens that gain fewer
attention weights across the whole corpus. More specifically, we
calculate the attention weight for a statement ğ‘† using a weighted
average of attention weights of its tokens:

ğ‘(ğ‘†) =

âˆ‘ï¸

ğ‘¡ âˆˆğ‘†

ğ‘¤ (ğ‘¡) Â· ğ‘(ğ‘¡)

(2)

where ğ‘(ğ‘¡) represents the attention weight of token ğ‘¡ in statement ğ‘†;
ğ‘¤ (ğ‘¡) denotes the normalized attention weight of token ğ‘¡ in the
whole corpus described in the previous section. The normalization
is implemented using the Softmax function, namely,

ğ‘¤ (ğ‘¡) = Softmaxğ‘¡ âˆˆğ‘† (ğ‘(ğ‘¡))

(3)

Unlike tokens, statements are often unique in the corpus and
cannot be included in a dictionary. To efficiently analyze statements,
we classify statements into 21 categories such as method signature,
variable declaration, and if condition, and only present the average
attention weight for each category. These categories are mainly
guided by Java specification [16]. In particular, the logger category
contains statements with standard logging functionalities such
as log4j, Logger, and println. Table 1 shows the categories of
statements we have summarized and the corresponding quantities
in the CodeSearchNet corpus. We can see that function invocation
and method signature are the most common statements, while loop
statements such as while are relatively rare.

3.2 Results and Analysis
In this section, we present the results of our experiments for each
research question.

3.2.1 What does CodeBERT learn about code tokens? (RQ1). Fig-
ure 2 highlights the critical tokens in Java that gain the most atten-
tion by CodeBERT. The graph is summarized from the CodeSearch-
Net [20] corpus which contains more than 100,000 Java functions.
For tokens in each function, we calculate their attention weights
and insert them into a <token, attention> map until the keys of
the map are stable. For ease of visualization, we remove tokens
that appear less than 50 times in the corpus. Among all the Java
tokens, coverage gains the lowest attention weight (5.43ğ‘’-5) while
boolean has the highest weight (2.94ğ‘’-2). The standard deviation
of these weights is 5.97ğ‘’-3.

The results show that CodeBERT assigns more attention to Java
keywords such as public and boolean. This is expected since Java
keywords frequently appear in Java code and play a dominant role
in representing code semantics. Another category of tokens that
receives high attention is the data-oriented identifiers such as Map,
List, and String, probably because they define key data structures
in a function.

Motivated by this finding, we further study the attention of
Java keywords. Figure 1 shows the attention weight of each Java
keyword. As we can see, the keyword private obtains the highest
attention weight of 1.15ğ‘’-2 among all keywords, while interface
and transient gain the lowest attention weights of 7.14ğ‘’-4 and
9.62ğ‘’-4, respectively. The standard deviation of these weights is
1.513ğ‘’-3.

Among all Java keywords, method modifiers such as public,
private, and static receive the highest attention weights We con-
jecture that these modifiers often denote the beginning of a method
signature, which is essential to understanding the entire code. Next
to these modifiers, finally and return also receive high attention
weights. Finally usually represents the end of a function, and the
code inside the finally block will definitely be executed. Return
represents the methodâ€™s output, which may show the methodâ€™s
functionality. Surprisingly, branching-related keywords such as if
and switch receive lower attention, presumably because CodeBERT
considers them unimportant for program understanding despite
their frequent occurrence in code.

A similar pattern can be observed in Python. For example, Code-
BERT prefers keywords such as return and def that represent the
general structure of a function, just like return and public in Java.
In particular, among the branching and loop keywords, while ranks
higher than if and for.

Figure 3 shows a heatmap of attentions for a Java function. The
code is about reading content from a file. The tokens with higher
attention weights are marked in deeper colors. The heatmap shows
a similar result as discussed above. public and finally receive the
highest attention weights in this function, while new and if obtain
the lowest attentions. Tokens that are related to the core function-
ality, such as File and read, have shown high attention weights.
By contrast, tokens that are auxiliary to the main functionality,
such as null and Exception have received much lower attention.
Another interesting observation is that grammatical symbols such
as } and; are considered necessary by CodeBERT probably because
they mark the end of statements and blocks. Overall, the heatmap
shows that CodeBERT mainly pays attention to functional tokens
that reflect the overall functionality and does not care much about
notional tokens about grammar and special branches.

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

Figure 1: Attention weights of Java keywords learned by CodeBERT

Figure 2: Java tokens highlighted by CodeBERT. The size of
each token indicates the attention weight assigned by Code-
BERT.

Figure 3: A heatmap of attention weights for Java statements
and tokens. The background color of a token is proportional
to the average attention weight assigned to it).

3.2.2 What does CodeBERT learn about code statements? (RQ2).
Figure 4 shows the attention weights assigned by CodeBERT for
each type of Java statement. As seen, method signature has gained
the highest attention (4.11ğ‘’-3), while case statement obtains the
lowest attention (2.32ğ‘’-3). The standard deviation of the attention
weights over all categories is 4.88ğ‘’-4. Broadly speaking, CodeBERT
focuses more on statements indicating the overall functionality of a
method, such as method signature and return, probably because they
contain dense information of the whole function, such as names
and targets.

Interestingly, arithmetic expressions (expressions with mathemat-
ical operations) also receive more attention than other statements.
Like natural language, arithmetic expression can be viewed as a
sequence of operations that tell the functionality of a statement.

Function invocations has also been shown to be essential for Code-
BERT to understand code. Like arithmetic expressions, function in-
vocations can be regarded as identifiers of other method signatures,
which can be literally understood through the function names and
parameters.

Surprisingly, CodeBERT does not pay much attention to state-
ments related to control flow structures, such as while, for and case.
This indicates that CodeBERT can effectively learn representations
of plain texts while being limited in learning structures.

Figure 3 also shows the heatmap of statement attentions for a
sample function of Java. Similar to the conclusions above, method
signature is the most important, which provides a general intro-
duction to the methodâ€™s functionality. At the same time, state-
ments about variable declaration and initialization, such as â€œString

00.0020.0040.0060.0080.010.0120.0140.016protectedprivatepublicstaticwhileassertfinallyreturnvoidbooleansynchronizednativedoubleforclasstrysupershortthisswitchcatchelsefloatthrowthrowsenumcontinuecasedoimportlongbreaknewdefaultcharextendsifinstanceofintfinalbytepackagetransientinterfaceDiet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

code to meet a specified sequence length. For each token ğ‘¤ âˆˆ ğ‘†, we
define a binary variable ğ‘Ÿğ‘¤ âˆˆ {0, 1} to indicate whether the token
will be kept (ğ‘Ÿğ‘¤ = 1) or pruned (ğ‘Ÿğ‘¤ = 0):

ğ‘Ÿğ‘¤ âˆ¼ Bernoulli(ğ‘)
ğ¶ğ‘ = {ğ‘¤ |ğ‘¤ âˆˆ ğ¶ and ğ‘Ÿğ‘¤ > 0}
where ğ‘ = ğ¿/|ğ¶ | denotes the probability of selecting a token. In
addition to reducing computational complexity, word dropout has
also been shown to improve the robustness of neural networks [21].
Hence, it enhances the performance of many tasks.

(4)

4.2 Frequency Filtering
A key concern with the word dropout strategy is that it can also
prune important tokens randomly from the input. Hence, we intro-
duce another frequency-based token pruning strategy that removes
uncommon tokens while keeping only the most frequently occurred
tokens.

Specifically, for each input code snippet ğ¶ = {ğ‘¤1, ..., ğ‘¤ğ‘› }, we
keep tokens if their frequency is among the top ğ‘˜ (ğ‘˜ = |ğ¶ğ‘ |) of all
tokens in the input, namely,

ğ¶ğ‘ = {ğ‘¤ |ğ‘¤ âˆˆ ğ¶, ğ‘“ (ğ‘¤) âˆˆ topâˆ’ğ‘˜ (ğ‘“ (ğ‘¤1), ..., ğ‘“ (ğ‘¤ğ‘›)), ğ‘˜ = |ğ¶ğ‘ |)

(5)

where ğ‘“ (ğ‘¤) denotes the frequency of ğ‘¤ in the whole code corpora.

4.3 Attention-based Code Pruning
The frequency filtering strategy can roughly distinguish important
tokens, but may only bias common tokens. According to our em-
pirical findings above, CodeBERT pays attention to certain types
of tokens and statements. Tokens that receive high attention do
not coincide with common tokens. In order to provide a more fine-
grained selection of important tokens and statements, we propose
an attention-based code pruning strategy that selects tokens and
statements based on their attention weights.

The simplification procedure is summarized in Algorithm 1. We
begin by summarizing the category of statements for a given pro-
gramming language according to the methodology in our empirical
study. Then, we create attention dictionaries for both statements
and tokens. The algorithm runs code pruning in two phases: select-
ing statements and pruning tokens.

In the statement selection phase, we select pivot statements that
belong to a category that obtain the highest attention weights in
our empirical study. Meanwhile, we want the selected statements
not to exceed the length constraint (i.e., ğ¿ tokens). This can be
formulated as a 0-1 knapsack problem [33], where statements can
be regarded as the items to be collected into a knapsack, with the
attention weights being the values, and the statement lengths being
the weights. The length constraint can be regarded as the capacity
of the knapsack. It is worth noting that we enlarge the capacity by
the maximum length of statements for tolerance of selecting one
more statement so that the algorithm can further perform token
pruning in the next phase.

In the token pruning phase, we greedily remove the tokens that
have the lowest attention weights in the lowest weighed statements
iteratively until satisfying the maximum number of tokens.

One potential problem is the different scales of statement at-
tentions and token sizes. We find that some shorter sequences are
much more likely to be chosen during our experiments even if they

Figure 4: Attention weights of various types of Java state-
ments learned by CodeBERT

content == nullâ€ receive lower attentions, probably because they
only create new variables with initial value null without actual
operations to them. The attention weight of the if statement in the
finally block is also low (2.92ğ‘’-3). This function only closes the
file reader and may have little effect on CodeBERTâ€™s understanding
of the whole method.

We observe a similar trend for Python statements. Method sig-
natures also receive the highest attention weight (6.86ğ‘’-3), while
branching statements such as break and continue have the lowest
weights. The standard deviation of attention weights for Python
statements is 1.66ğ‘’-3, which is a bit larger than that of Java state-
ments.

Compared to Java, the distributions of Python statements and
tokens are a bit more sparse. For example, the method signature
statement which ranks the first owns a much higher weight than the
second category of statements (i.e., return). The top keyword def
also obtains a much higher attention weight than other keywords.

4 DIETCODE: PROGRAM SIMPLIFICATION

FOR CODEBERT

As Equation 1 indicates, the computational complexity for com-
puting the attention matrix is O (ğ‘‘2ğ‘› + ğ‘›2ğ‘‘), which quadratically
scales with sequence length. As such, the attention operation be-
comes a bottleneck when applied to long sequences such as source
code [25].

We wonder whether we can remove unimportant tokens or state-
ments from the input programs of CodeBERT. In this way, the time
and space costs for the pre-trained model can be substantially re-
duced. Based on this idea, we propose DietCode, a lightweight
pre-trained model for source code. DietCode reduces the computa-
tional complexity of CodeBERT by simplifying input programs.

More specifically, our problem formulation is as follows: given
a code snippet ğ¶ = {ğ‘ 1; . . . ; ğ‘  |ğ¶ | } which consists of |ğ¶ | statements,
we want to output a pruned code snippet ğ¶ğ‘ which contains a
maximum length of ğ¿ tokens. The goal of program simplification is
to prune as many tokens as possible without a significant impact
on the accuracy of downstream tasks.

4.1 Word Dropout
One straightforward strategy for program simplification is the word
dropout [21], namely, randomly dropping tokens from the input

0.0020.00250.0030.00350.0040.0045Func. SignatureFinallyReturnExpressionTryAnnotationFunctionWhileSetterContinueGetterSwitchIfForCatchBreakLogVariableThrowSyncronizedCaseESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

Algorithm 1 The Attention-based Pruning Algorithm
Require:
1: C=ğ‘ 1; . . . ; ğ‘  |ğ¶ |: an input code snippet,
2: Ağ‘‡ : attention dictionary of tokens,
3: Ağ‘† : attention dictionary of statements,
4: L: target length

Table 2: Statistics of Datasets

Corpus

Training

Test

CodeSearchNet (Java)
CodeSearchNet (Python)

908,886
824,343

1,000,000
1,000,000

Ensure:

5: Reduced code snippet Cğ‘ = ğ‘  â€²
6: for s âˆˆ C do
7:

1; . . . ; ğ‘  â€²

|ğ¶ğ‘ |

a(s) = Ağ‘† (s)
for t âˆˆ s do

a(t) = Ağ‘‡ (t)

8:

9:

end for

10:
11: end for
12: ğ¶0 â† 0-1

âŠ² obtain the statement attention

âŠ² obtain the token attention

Knapsack (items={s}ğ‘  âˆˆğ¶ , values={ğ‘(ğ‘ )}ğ‘  âˆˆğ¶ ,
âŠ² candidate

weights= {|s|}ğ‘  âˆˆğ¶ , capacity = ğ¿+maxğ‘  âˆˆğ¶ (|s|)
statements
13: for i = 1,..., (cid:205)ğ‘  âˆˆğ¶0
14:

(|s|) - L do
ğ‘  â€² â† ğ‘  â€² \ {t} where ğ‘  â€² = argminğ‘  âˆˆğ¶0

ğ‘(ğ‘ ),
ğ‘¡ = argminğ‘¡0 âˆˆğ‘ â€²ğ‘(ğ‘¡0)

15: end for
16: ğ¶ğ‘ â† ğ‘  â€²

statements

1; . . . ; ğ‘  â€²

|ğ¶0 |

where ğ¶0 = {ğ‘  â€²

1, ğ‘  â€²

2, ..., ğ‘  â€²

|ğ¶0 |

} âŠ² concatenate

receive low attention weights. This is probably because the value
ratio (the attention weight divided by the sequence length) of a
shorter sequence is much larger than the longer one. We amplify
the attention weights using min-max normalization and multiply
with the number of tokens in the statement, that is,

ğ‘(ğ‘ ) =

ğ‘(ğ‘ ) âˆ’ minğ‘  âˆˆğ‘† (ğ‘(ğ‘ ))
maxğ‘  âˆˆğ‘† (ğ‘(ğ‘ )) âˆ’ minğ‘  âˆˆğ‘† (ğ‘(ğ‘ ))

Ã— ğ‘

(6)

where ğ‘ denotes the number of tokens in the statement.

The time complexity of the simplification algorithm is O(ğ‘ Â·
(ğ¿ğ‘‡ + ğ¿ğ‘€ )), where ğ¿ğ‘‡ denotes the target number of tokens, and
ğ¿ğ‘€ denotes the maximum length of statements. The time cost
mainly comes from the 0-1 knapsack algorithm. Furthermore, the
knapsack algorithm requires a two-dimensional array for dynamic
programming, which requires space complexity.

Figure 5 shows an example of program simplification by Diet-
Code. The original code (Figure 5(a)) aims to read content from a
file with a given filename. Our goal is to reduce the code to contain
only 60 tokens without considerably losing the semantics. In the
statement selection phase (Figure 5(b)), the simplification algorithm
solves a 0-1 knapsack problem by setting the capacity to 78 (includ-
ing the target length of 60 and the longest statement length 18).
Then, DietCode selects trunk statements such as method signature
and return, and removes trivial statements such as variable assign-
ments and catch. The number of tokens in the code is reduced from
118 to 77. In the token pruning phase, DietCode further removes
tokens such as variable initialization (Figure 5(c)), and reduces the
number of tokens from 77 to 60. Although reduced by around 50%,
the main semantic of this function is not destroyed significantly.
CodeBERT can still understand the function through key statements
such as the method name readFile, the key variable content, and
the invoked function read. Although the remaining code is never

runnable, the selected critical information can be used for down-
stream tasks such as code search and summarization.

5 EVALUATION
This section introduces the evaluation of DietCode in two down-
stream tasks. We aim to answer the following research questions
through experiments:

â€¢ RQ3: How effective is DietCode in program simplifica-
tion? We evaluate the performance and computation cost
of DietCode in two downstream tasks and compare it with
baseline models.

â€¢ RQ4: How effective is DietCode under different rela-
tive lengths? We study the effect of different relative lengths
on the evaluation scores. Our goal is to find the relative
length that leads to the best trade-off between model perfor-
mance and computational expense.

â€¢ RQ5: What is the effect of different pruning strategies?
The attention-based DietCode performs both statement and
token pruning. We conduct an ablation study to identify the
effect of pruning each of them.

5.1 Downstream Tasks
We test our algorithm in two downstream tasks, namely, code search
and code summarization. They are the most widely used software
engineering tasks to demonstrate the capacity of NL-PL understand-
ing [17, 22, 23, 31, 41, 46].

Code Search. Code search is a typical testbed for pre-trained
code models such as CodeBERT [15]. The task aims to find code
snippets from a codebase that is most relevant to a given query.

Code Summarization. Code summarization aims to generate a
natural language summary for an input code snippet. It has also
been widely used for evaluating pre-trained models for source
code [15, 45]. Through this task, we want to verify whether program
simplification by DietCode is effective in generation tasks.

5.2 Datasets
We use the CodeSearchNet to fine-tune and test our model [20],
which contains <comment, code> pairs collected from open source
projects. code means the code snippet of a method, while comment
is the description of the code that was mainly collected from com-
ments for the whole functions such as Javadocs or docstrings in
Python. The statistics of CodeSearchNet is shown in Table 2.

The original data of CodeSearchNet are in the format of token
sequences. We preprocess the data by parsing them into statements.
We parse statements by splitting brackets and semicolons according
to the common guidance of Java [16]. For Python code, there are
no delimiters because the indentation is missing in the dataset.
Besides, Python does not use brackets and semicolons. Hence, we

Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

(a) The original source code

(b) Code after statement selection

(c) Code after token pruning

Figure 5: An example of program simplification by DietCode (attention).

split statements using other hint symbols such as â€œdefâ€, â€œ=â€, â€œ:â€, and
â€œ)â€.

To reduce the vocabulary size of code tokens, we delexicalize
all string constants to a generic token string and all numerical
constants to the same token 10. We conserve special numbers such
as 0, 1, and -1, which can belong to boolean values.

5.4 Experimental Setup
The demonstrate the strength of our approach, we simplify the
input programs for pre-training based models and compare the
performance to that of the original pre-trained model. Specifically,
we compare our technique to the vanilla versions of three pre-
trained models:

5.3 Evaluation Metrics
First, we want to measure the maximum extent of code reduction
that DietCode can achieve without losing much accuracy. We de-
fine Relative Length (RL) to measure how much of the code is
remained after simplification. It is computed as the percentage of
the length of the simplified code snippet |ğ¶ğ‘ | with respect to the
length of the original code |ğ¶ |:

ğ‘…ğ¿ =

|ğ¶ğ‘ |
|ğ¶ |

Ã— 100%

(7)

where |Â·| denotes the length of a code snippet (i.e., number of tokens).
The smaller the relative length, the greater the simplification to the
original code.

We also use FLOPs (floating point operations) [19] to measure
the effect of model reduction. FLOPs is a widely used metric to
measure the complexity of a machine learning model. The higher
the FLOPs, the slower the processing of the model.

The third metric is the time cost, including fine-tuning time
(FT Time) and testing time. We calculate the execution time of
DietCode for fine-tuning and testing, measured by the number of
hours from the program starting to the termination.

Besides the metrics for complexity, we use two standard metrics
to evaluate the effectiveness of DietCode in two downstream tasks
respectively. We measure the performance of code search using
MRR (mean reciprocal rank), which refers to the average of the
multiplicative inverse of the position for the first correct answer
for the query [20].

We measure the performance of code summarization using the
BLEU-4 (bilingual evaluation understudy) score [27], which cal-
culates the average of n-gram precision on a couple of sequences
with a penalty for short sequences.

â€¢ CodeBERT [15]: the original CodeBERT without code sim-
plification. We follow the experimental setup in the Code-
BERT paper.

â€¢ CodeT5 [45]: a widely used pre-trained programming lan-
guage model based on the sequence-to-sequence architec-
ture. CodeT5 has demonstrated better performance on gen-
erative tasks [45].

â€¢ RoBERTa [28]: a popular extension of BERT that has demon-
strated significant improvement. The original RoBERTa was
pre-trained in natural languages. So we also report the re-
sults of RoBERTa (code) [15], a variant that was pre-trained
on source code.

Besides the pre-trained models, we also compare the accuracy
of DietCode with that of traditional deep learning approaches,
including BiRNN [10], SelfAttn [43], Seq2Seq [10], and Trans-
former [43]. As they are also baseline models for CodeBERT, we
directly take the results from the CodeBERT paper [15].

Finally, we compare DietCode with SIVAND [34], which is also
a program simplification method proposed by Rabin et al. [34].
SIVAND 1 is a machine-learning-based algorithm that recursively
splits source code into fragments and measures its importance
through the method name prediction task. The algorithm selects
important fragments as the reduced code. In our experiments, we
directly use the code provided by the authors.

We implement our model based on the GitHub repository of
CodeBERT 2 and CodeT5 3 using the default hyperparameter set-
tings. All models are optimized with the Adam algorithm [26] with

1https://github.com/mdrafiqulrabin/SIVAND
2https://github.com/microsoft/CodeBERT
3https://github.com/salesforce/CodeT5

1publicStringreadFile(Stringfilename) {2Stringcontent=null;3File file =newFile(filename);4FileReaderreader =null;5try{6reader=newFileReader(file);7char[] chars=newchar[(int)file.length()];8reader.read(chars);9content=newString(chars);10reader.close();11}catch(IOExceptione) {12e.printStackTrace();13}finally{14if(reader!=null){15reader.close();16}17}18returncontent;19}1publicStringreadFile(Stringfilename){2Stringcontent=null;3File file =newFile(filename);4FileReaderreader =null;5try{6reader=newFileReader(file);7reader.read(chars);8content=newString(chars);9reader.close();10} catch (IOExceptione) {11}finally{}12returncontent;13}1publicStringreadFile(Stringfilename){2File file =newFile(filename);3FileReader4try{5reader=newFileReader(file);6reader.read(chars);7content=newString(chars);8reader.close();9}finally{} 10returncontent;11}ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

Table 3: Performance of various code simplification methods on the code search task (ğ¿in=input length; RL=relative length;
FT=fine-tuning).

Model

BiRNN [10]
SelfAttn [43]
RoBERTa [28]
RoBERTa (code)
CodeBERT [15]
DietCode
- Attention
- Dropout
- Frequency

CodeT5 [45]
DietCode
- Attention
- Dropout
- Frequency

ğ¿in

200
200
200
200
200

120
120
120

200

120
120
120

Java

RL

FT Time Test Time

FLOPs MRR

100%
100%
100%
100%
100%

60%
60%
60%

9.88h
22.83h
23.85h
21.02h
20.82h

11.08h
10.73h
10.32h

1.13h
4.00h
4.78h
4.53h
3.17h

1.83h
1.95h
1.8h

8.34G
0.29
16.99G 0.59
16.99G 0.67
16.99G 0.72
16.99G 0.74

10.19G 0.71
10.19G 0.68
10.19G 0.66

100%

16.83h

2.97h

16.99G 0.72

60%
60%
60%

9.3h
9.25h
8.97h

1.74h
1.67h
1.58h

10.19G 0.71
10.19G 0.68
10.19G 0.66

Python

RL

FT Time Test Time

FLOPs MRR

100%
100%
100%
100%
100%

60%
60%
60%

7.07h
17.78h
19.32h
20.78h
17.92h

9.62h
9.33h
8.67h

1.25h
2.67h
3.53h
4.07h
2.82h

1.82h
1.93h
1.79h

8.34G
16.99G
16.99G
16.99G
16.99G

10.19G
10.19G
10.19G

0.32
0.69
0.81
0.84
0.84

0.81
0.80
0.78

100%

17.61h

2.83h

16.99G 0.837

60%
60%
60%

8.31h
9.33h
8.67h

1.81h
1.93h
1.79h

10.19G 0.813
10.19G 0.799
10.19G 0.785

ğ¿in

200
200
200
200
200

120
120
120

200

120
120
120

Table 4: Performance of various code simplification methods on the code summarization task.

Model

Seq2Seq [10]
Transformer [43]
RoBERTa [28]
RoBERTa (code)
CodeBERT [15]
DietCode
- Attention
- Dropout
- Frequency

CodeT5 [45]
DietCode
- Attention
- Dropout
- Frequency

ğ¿in

256
256
256
256
256

150
150
150

256

150
150
150

Java

RL FT Time Test Time FLOPs

BLEU-4

100% 6.26h
100% 13.28h
100% 15.42h
100% 14.5h
100% 13.82h

60% 8.18h
60% 7.95h
60% 8.62h

1.26h
5.43h
5.22h
6.02h
4.8h

1.62h
1.40h
1.33h

12.750G
24.328G
24.328G
24.328G
24.328G

15.325G
15.325G
15.325G

100% 16.91h

4.88h

54.01G

60% 10.25h
60% 9.95h
60% 10.23h

1.63h
1.68h
1.35h

37.81G
37.81G
37.81G

15.09
16.26
16.47
17.50
18.95

17.29
15.63
16.13

20.46

19.25
17.65
18.74

Python

RL FT Time Test Time FLOPs

BLEU-4

100% 4.43h
100% 9.70h
100% 10.63h
100% 10.45h
100% 8.32h

60% 5.35h
60% 6.81h
60% 6.12h

1.02h
2.43h
1.88h
2.05h
1.87h

1.16h
1.15h
1.42h

12.750G
24.328G
24.328G
24.328G
24.328G

15.325G
15.325G
15.325G

100% 9.68h

1.48h

54.01G

60% 6.57h
60% 6.67h
60% 6.33h

0.90h
0.82h
0.85h

37.81G
37.81G
37.81G

15.93
15.81
18.14
18.58
19.04

17.08
16.04
16.55

20.37

18.53
17.07
17.77

ğ¿in

256
256
256
256
256

150
150
150

256

150
150
150

learning rates of 1ğ‘’-5 and 5ğ‘’-5 for the code search and code sum-
marization tasks respectively.

it still significantly outperforms traditional non-pretraining based
methods such as BiRNN (MRR=0.29) and SelfAttn (MRR=0.59).

We run all models on a machine with a CPU of Intel(R) Xeon(R)

Silver 4214R 2.40GHz and a GPU of Nvidia Tesla P40.

5.5 Experimental Results (RQ3)
Table 3 and 4 show the performance of DietCode in two downstream
tasks. Overall, DietCode reduces the computational cost by around
40% while keeping a comparable accuracy to the original pre-trained
models. The original CodeBERT takes nearly one day for fine-tuning
and more than 3 hours for code search on the Java test set to
achieve an MRR score of 0.74. In contrast, by reducing the length
of the input code to 60%, the fine-tuning and testing time can be
reduced to 11.08 hours and 1.83 hours, respectively. The FLOPs
also drops from 16.99G to 10.19G. At the same time, the accuracy
(MRR = 0.71) is comparable to the original CodeBERT (MRR=0.74).
It is worth noting that although the performance drops slightly,

DietCode on CodeT5 demonstrates a similar efficacy. The vanilla
CodeT5 requires more than 16 hours for fine-tuning and around
3 hours for testing on the Java code search task. By dropping out
40% input tokens using DietCode, the fine-tuning and test time is
reduced to around 9 and 1.6 hours, respectively. Meanwhile, Diet-
Code (attention) achieves a comparable performance (MRR=0.71)
to the original CodeT5 (MRR=0.72) with only a reduction of 1.5%.
DietCode is also effective in the code summarization task. Taking
Java as an example, the vanilla CodeBERT achieves a BLEU-4 score
of 18.95 at the expense of 13.82 hours of fine-tuning and 4.8 hours
of testing. When the target length is set to 150 tokens (i.e., 60% of
the original length), the fine-tuning time is reduced to about 60%
and the FLOPs drops from 24.328G to only 15.325G. This does not
sacrifice accuracy (BLEU-4=17.29).

Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

(a) results on code search

(b) results on code summarization

(a) results on code search

(b) results on code summarization

Figure 6: Performance under different relative lengths on
Java.

Figure 7: Ablation results of token and statement pruning
on Java.

Comparing the three pruning strategies, DietCode with attention
shows strength over the two other strategies. For example, Diet-
Code with attention-based pruning achieves an MRR of 0.71 in the
Java code search task, which is better than dropout (MRR=0.68) and
frequency filtering (MRR=0.66). The results suggest that by finer-
grained pruning of tokens based on attention weights, DietCode
can achieve more effective program simplification. DietCode on
the CodeT5 demonstrates a similar trend in both tasks, where the
attention-based pruning achieves better performance (BLEU=19.25)
than dropout (BLEU=17.65) and frequency filtering (BLEU=18.74).
We observe that this strength becomes less significant in the Python
language. We conjecture that the Python language contains fewer
auxiliary tokens (such as brackets), resulting in a more uniform
distribution of attention weights over all tokens. Therefore, remov-
ing tokens with smaller attention is closer to removing random or
frequent tokens in the Python language.

As a noteworthy point, it is inapplicable to test the baseline
model SIVAND in our datasets, as it takes a massive amount of
time (>30,000 hours according to our estimation) to process more
than 1 million functions in the CodeSearchNet benchmarks. This is
mainly because the ddmin algorithm used by SIVAND is based on
backtracking, where each step the algorithm should invoke the deep
learning model for a prediction task. For this reason, we estimate the
time cost of SIVAND by sampling 50 functions from CodeSearchNet.
Results show that it costs 104 and 77 minutes respectively in the
CodeSearchNet (Java) and CodeSearchNet (Python) datasets. Based
on this observation, we estimate the testing time in the entire
dataset. Since the data we sample is small in size, it is insufficient
to fine-tune the model to validate the accuracy.

5.6 Effect of Relative Length (RQ4)
Figure 6 shows the performance of DietCode under different relative
lengths and pruning strategies. We can see that the performance of
all pruning strategies drops sharply as the relative length decreases.
In both two tasks, DietCode with attention performs better than
the other two pruning strategies under all relative lengths. When
the relative length is above 70%, the performance is comparable to
that of the original CodeBERT but the reduction of computational
cost can be modest. When the relative length drops below 50%, the
performance is deficient since most of the code has been pruned.
A relative length of around 60% is probably the best trade-off for
these two tasks.

5.7 Effect of Token and Statement Pruning

(RQ5)

Figure 7 shows the performance of DietCode with token and state-
ment pruning on the two tasks. In order to verify the effect of
statement pruning, we only run the statement selection phase in
Algorithm 1. To test the effect of token pruning, we drop the lowest
attention tokens directly until meeting the target length. As we can
see from the results, a significant drop in performance is observed
for both strategies as the relative length decreases. When 90% code
remains, the MRR scores of the two pruning strategies are close. As
the relative length decreases, the score of the token pruning strat-
egy drops much faster than that of the statement pruning strategy.
When the relative length decreases further, their MRR scores drop
dramatically. The results suggest that statements are more critical
than tokens in learning code representations by CodeBERT. One
possible reason could be that token attention usually measures local
information, while statement attention considers the relationship
between tokens. Therefore, statement attention could keep more
semantic of the functions. The code summarization task shares a
similar result as the code search task.

6 DISCUSSION
6.1 How Can Pre-Trained Models Understand

Simplified Code?

One debatable question is how can pre-trained models understand
simplified code which is not compilable and runnable. Source code
involves two channels of information: formal & natural [6, 7]. It
can either be considered as programs for computers to execute or
as a specific natural language for humans to communicate [18].
The former requires the source code to be strictly compilable and
runnable, while the latter focuses more on conveying information
to humans. We believe that CodeBERT considers more about the
natural channel of source code [7, 18]. As such, it pays more at-
tention to keywords without necessarily having to know how the
program actually behaves during execution.

Our empirical findings are in agreement with a previous human
study on how programmers summarize source code [37], which
shows that developers usually start by reading the function name.
They may skip some structural information such as For and If
conditions while paying special attention to literal information such
as method invocations and variable names, which literally show

40506070Relative Length(%)0.550.600.650.70MRRattentionrandomfrequency40506070Relative Length(%)1415161718BLEU4attentionrandomfrequency5060708090Relative Length(%)0.580.600.620.640.660.680.700.720.74MRRstatementtoken5060708090Relative Length(%)15.015.516.016.517.017.518.018.5BLEU4statementtokenESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

the intention of the code [37]. The pruning algorithms described in
our paper make use of this finding and simplify source code by only
selecting critical information (e.g., reflected by the attentions of
CodeBERT) in the source code. Therefore, they do not have much
impact on the understanding of source code.

6.2 What users may adopt the techniques?
One debatable question is what users may adopt the technique since
it has a precision loss. From the results in Table 3 and 4, although
the performance drops slightly, it still significantly outperforms
traditional non-pretraining based methods such as BiRNN and Self-
Attn. Meanwhile, DietCode shows an advantage of computational
efficiency (e.g., saving 40% of fine-tuning time). Hence, DietCode
provides a practical option for users with limited computational
resources but still want quick leverage of large pre-trained models.

6.3 The role of the 0-1 knapsack formulation
One may question whether we really need to transform the problem
into the 0-1 knapsack. In our preliminary studies, simply pruning
statements often results in sequences that are either too long or
too short. Token pruning provides a finer-grained pruning to the
input program. This amounts to selecting statements and tokens to
obtain the maximum attention while satisfying the length restric-
tion. Hence, we formulate it as a 0-1 knapsack problem and solve it
using a greedy strategy. According to the experimental results in
Figure 7, a simple statement pruning archives an MRR of around
0.68 on Java code search when the relative length is 60%, which
is evidently lower than that of DietCode (MRR=0.71). The same
comparison can be observed on the Java code summarization task
(i.e., 16.8 vs. 17.3 in terms of BLEU). This suggests the effectiveness
of the 0-1 knapsack formulation.

6.4 Implications for Future Research
Through our empirical study, we find that CodeBERT does not
recognize grammar and structural information very well. For ex-
ample, the if condition, for condition, and while condition
are three typical statements representing code structures, but they
are rarely considered by pre-trained models when learning pro-
gram representations. This motivates future research on better
incorporating code structures into pre-trained code models, for
example, converting structural symbols (e.g., the bracket } ) to a
more verbal style (e.g., â€˜the end of a for statementâ€™) before feeding
into pre-trained models.

Besides DietCode, there is a broader class of techniques, includ-
ing model compression [40], model distillation [39], and data re-
duction [48], which aim to reduce the computational complexity
of large pre-trained models at the cost of a slight decrease in per-
formance. DietCode provides an easy, alternative solution by only
pruning input programs. We believe this is more practical for de-
velopers as they only need to process the data without modifying
the model. In the future, we can investigate more techniques such
as model compression and distillation.

6.5 Threats to Validity
Internal Threats. When calculating the attention weights for state-
ments, we encounter the out-of-vocabulary (OOV) issue. We resolve

this issue by excluding rare categories of statements in our experi-
ments. Nevertheless, some categories of statements could still be
necessary though having a low quantity in the corpus. Further-
more, the raw data provided by CodeSearchNet is in the format of
plain text. We parse statements using text processing rules designed
by ourselves. However, there could be irregular patterns that our
parser can not recognize. This could cause noise in our dataset and
affect the results of our research.

External Threats. We have just conducted our experiments in
Java and Python. Though the conclusions from both languages are
similar, other programming languages such as LISP and Erlang
could have different attention patterns. In addition, DietCode is
evaluated in two downstream tasks: code search and code summa-
rization. However, these tasks rely much on the literal information
provided in the source code. Thus, it remains to be verified whether
or not the proposed simplification algorithm is applicable to other
software engineering tasks.

7 RELATED WORK
7.1 Understanding Pre-trained Models of Code
Besides our work, there have been other studies that also try to
explain the mechanisms of pre-trained models for code [1, 30, 32,
38]. Karmakar and Robbes [24] applied four probing tasks on pre-
trained code models to investigate whether pre-trained models
can learn different aspects of source code such as syntactic, struc-
tural, surface-level, and semantic information. Different from our
work, they only empirically study the whole pre-trained model,
whereas we focus on more specific (critical tokens and statements)
knowledge learned by pre-trained models.

There have also been many works that investigate the attention
weights of pre-trained models for source code. For example, Wan
et al. [44] performed a structural analysis of pre-trained language
models for source code. They analyzed the self-attention weights
and found that the Transformer attention can capture high-level
structural information of source code. AutoFocus [5] aims to reveal
the most relevant part of the code to programmers. They mea-
sure the relevance of statements using attention weights from a
GGNN [2]. Different from our approach, they capture structural
knowledge learned by pre-trained models, while we dig more into
critical statements and tokens learned by pre-trained models. Fur-
thermore, we propose simplifying programs for pre-trained lan-
guage models based on the findings.

7.2 Program Simplification
Program simplification has gained increasing attention recently.
The state-of-the-art methods such as SIVAND [34] and P2IM [42] a
based on the delta debugging prototype [49]. The delta debugging
mechanism requires an input code snippet and an auxiliary deep
learning model such as the code2vec [3]. The deep learning model
takes as input the code snippet and splits it into fragments. Each
fragment is then taken as input to the neural network model to
perform testing tasks such as method name prediction and mis-
used variables detection. If a fragment obtains a satisfying score,
the program will split it further. The process continues until the
performance of the subset does not satisfy the target score. Finally,

Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

the algorithm produces the smallest code snippet that satisfies the
objective of the deep model.

Compared to DietCode, their methods are computationally inef-
ficient because they need to run an auxiliary deep learning model
and evaluate the performance at each iteration. For example, it
costs hundreds of hours for SIVAND to process 10,000 functions,
while DietCode can complete it within two minutes.

8 CONCLUSION
This paper empirically analyzes the critical information learned by
CodeBERT, including important tokens and statements in both Java
and Python. Our results show that CodeBERT focuses on keywords
and data-relevant statements such as method signatures and return
statements. Based on our empirical findings, we propose a novel
approach named DietCode for lightweight leverage of pre-trained
models. DietCode simplifies the input code to a target length by
selecting important statements and tokens based on their atten-
tion weights using a 0-1 Knapsack algorithm. Experiments on two
tasks have shown that DietCode provides comparable results as
CodeBERT, with the advantage of computational efficiency.

In the future, we will consider more aspects that CodeBERT
learns about code, such as syntax rules and semantic relations, to
further reduce the size of source code for pre-trained models. We
will also investigate model compression [8, 9] and distillation [39]
techniques to further reduce the size of pre-trained programming
language models.

Our source code and experimental data are publicly available at

https://github.com/zhangzwwww/DietCode

9 ACKNOWLEDGE
This work was sponsored by NSFC No. 62102244, CCF-Tencent
Open Research Fund (RAGR20220129), and CCF-Baidu Open Fund
(NO. 2021PP15002000).

REFERENCES
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-
fied Pre-training for Program Understanding and Generation. In Proceedings of
the 2021 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies. 2655â€“2668.

[2] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations.

[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning
distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29.

[4] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate. In Proceedings of the 3rd
International Conference on Learning Representations (ICLR).

[5] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2019. Autofocus: interpreting
attention-based neural networks by code perturbation. In Proceedings of 34th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 38â€“41.

[6] Casey Casalnuovo, E Morgan, and P Devanbu. 2020. Does surprisal predict
code comprehension difficulty. In Proceedings of the 42nd Annual Meeting of the
Cognitive Science Society. Cognitive Science Society Toronto, Canada.

[7] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu,
and Baishakhi Ray. 2020. NatGen: Generative pre-training by" Naturalizing"
source code. In Proceedings of the 30th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE).

[8] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey of model
arXiv preprint

compression and acceleration for deep neural networks.
arXiv:1710.09282 (2017).

[9] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2018. Model compression and
acceleration for deep neural networks: The principles, progress, and challenges.
IEEE Signal Processing Magazine 35, 1 (2018), 126â€“136.

[10] Kyunghyun Cho, Bart van Merrienboer, Ã‡aglar GÃ¼lÃ§ehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP).

[11] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimil-
iano Di Penta, and Gabriele Bavota. 2021. An empirical study on the usage of
BERT models for code completion. In 2021 IEEE/ACM 18th International Confer-
ence on Mining Software Repositories (MSR). IEEE, 108â€“119.

[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
In International Conference on Learning Representations.

[13] Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model
pretraining. Advances in neural information processing systems 32 (2019).
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). 4171â€“4186.

[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: a pre-Trained
model for programming and natural languages. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP): Findings. 1536â€“1547.
[16] James Gosling, Bill Joy, Guy Steele, and Gilad Bracha. 2000. The Java language

specification. Addison-Wesley Professional.

[17] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program
comprehension with source code summarization. In Proceedings of ACM/IEEE
32nd international conference on software engineering (ICSE), Vol. 2. IEEE, 223â€“226.
[18] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837â€“847.

[19] Raphael Hunger. 2005. Floating point operations in matrix-vector calculus. Munich

University of Technology, Inst. for Circuit Theory and Signal.

[20] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).

[21] Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal DaumÃ© III. 2015.
Deep unordered composition rivals syntactic methods for text classification.
In Proceedings of the 53rd annual meeting of the association for computational
linguistics and the 7th international joint conference on natural language processing
(volume 1: Long papers). 1681â€“1691.

[22] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In Proceedings
of 32nd IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 135â€“146.

[23] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. 2002. CCFinder: A
multilinguistic token-based code clone detection system for large scale source
code. IEEE Transactions on Software Engineering (TSE) 28, 7 (2002), 654â€“670.
[24] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models
know about code?. In 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE). IEEE, 1332â€“1336.

[25] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph
Hassoun, and Kurt Keutzer. 2021. Learned token pruning for transformers. arXiv
preprint arXiv:2107.00910 (2021).

[26] Diederik P. Kingma and Jimmy Ba. 2017. Adam: a method for stochastic opti-

mization. arXiv:1412.6980 [cs.LG]

[27] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics. 7871â€“7880.

[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: a
robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[29] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understand-
ing and Generation. In Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1).

[30] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the usage
of text-to-text transfer transformer to support code-related tasks. In Proceedings
of IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
336â€“347.

ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore

Zhang, et al.

[31] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li. 2016. Query
IEEE Transactions on

expansion based on crowd knowledge for code search.
Services Computing 9, 5 (2016), 771â€“783.

[32] Matteo Paltenghi and Michael Pradel. 2021. Thinking Like a Developer? Compar-
ing the Attention of Humans with Neural Models of Code. In 2021 36th IEEE/ACM
International Conference on Automated Software Engineering (ASE). IEEE, 867â€“879.

[33] David Pisinger. 1995. Algorithms for knapsack problems. (1995).
[34] Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin Alipour.
2021. Understanding neural code intelligence through program simplification.
In Proceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE).
Association for Computing Machinery, 441â€“452.

[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. 2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.

[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text transformer. Journal of Machine
Learning Research 21 (2020), 1â€“67.

[37] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, and Sidney
Dâ€™Mello. 2014. Improving automated source code summarization via an eye-
tracking study of programmers. In Proceedings of the 36th international conference
on Software engineering. 390â€“401.

[38] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology:
what we know about how BERT works. Transactions of the Association for
Computational Linguistics 8 (2020), 842â€“866.

[39] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 (2019).

[40] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. 2020. Q-BERT: Hessian based ultra
low precision quantization of BERT. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 34. 8815â€“8821.

[41] Kathryn T Stolee, Sebastian Elbaum, and Daniel Dobos. 2014. Solving the search
for source code. ACM Transactions on Software Engineering and Methodology

(TOSEM) 23, 3 (2014), 1â€“45.

[42] Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim A Laredo, and Alessandro Morari.
2021. Probing model signal-awareness via prediction-preserving input minimiza-
tion. In Proceedings of the 29th ACM Joint Meeting on European Software Engi-
neering Conference and Symposium on the Foundations of Software Engineering.
945â€“955.

[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems (NeurlPS 2017).
5998â€“6008.

[44] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022.
What do they capture? â€“ a structural analysis of pre-trained language models
for source code. In In Proceedings of the 44th International Conference on Software
Engineering (ICSE).

[45] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-
aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and
Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing. 8696â€“8708.

[46] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk.
2016. Deep learning code fragments for code clone detection. In Proceedings of
the 31st IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE, 87â€“98.

[47] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
and Quoc V Le. 2019. Xlnet: generalized autoregressive pretraining for language
understanding. Advances in neural information processing systems 32 (2019).
[48] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. 2021. TR-BERT: Dynamic
Token Reduction for Accelerating BERT Inference. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. 5798â€“5809.

[49] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-
inducing input. IEEE Transactions on Software Engineering 28, 2 (2002), 183â€“200.
[50] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong
Liu. 2019. A novel neural source code representation based on abstract syntax tree.
In Proceedings of IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 783â€“794.

